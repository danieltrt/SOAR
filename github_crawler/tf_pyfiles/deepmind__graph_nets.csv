file_path,api_count,code
setup.py,0,"b'# Copyright 2018 The GraphNets Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Setuptools installation script.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom setuptools import find_packages\nfrom setuptools import setup\n\ndescription = """"""Graph Nets is DeepMind\'s library for building graph networks in\nTensorflow and Sonnet.\n""""""\n\nsetup(\n    name=""graph_nets"",\n    version=""1.1.1.dev"",\n    description=""Library for building graph networks in Tensorflow and Sonnet."",\n    long_description=description,\n    author=""DeepMind"",\n    license=""Apache License, Version 2.0"",\n    keywords=[""graph networks"", ""tensorflow"", ""sonnet"", ""machine learning""],\n    url=""https://github.com/deepmind/graph-nets"",\n    packages=find_packages(),\n    # Additional ""tensorflow"" and ""tensorflow_probability"" requirements should\n    # be installed separately (See README).\n    install_requires=[\n        ""absl-py"",\n        ""dm-sonnet"",\n        ""dm-tree"",\n        ""future"",\n        ""networkx"",\n        ""numpy"",\n        ""setuptools"",\n        ""six"",\n    ],\n    classifiers=[\n        ""Development Status :: 4 - Beta"",\n        ""Intended Audience :: Developers"",\n        ""Intended Audience :: Education"",\n        ""Intended Audience :: Science/Research"",\n        ""License :: OSI Approved :: Apache Software License"",\n        ""Operating System :: POSIX :: Linux"",\n        ""Operating System :: Microsoft :: Windows"",\n        ""Operating System :: MacOS :: MacOS X"",\n        ""Programming Language :: Python :: 2.7"",\n        ""Programming Language :: Python :: 3.4"",\n        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",\n    ],\n)\n'"
graph_nets/__init__.py,0,"b'# Copyright 2018 The GraphNets Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Graph networks library.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom graph_nets import blocks\nfrom graph_nets import graphs\nfrom graph_nets import modules\nfrom graph_nets import utils_np\nfrom graph_nets import utils_tf\n'"
graph_nets/_base.py,0,"b'# Lint as: python2, python3\n# Copyright 2018 The GraphNets Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n\n""""""Base classes for modules, defined depending on the Sonnet version.\n\nStrategy to be compatible with both Sonnet 1 and Sonnet 2 works as follows:\n - Dynamically decide which version we are using.\n - Create an adapter base class, with a unified API, that would allow child\n   classes to implement interfaces similar to Sonnet 1.\n - All GraphNet modules Networks inherit for that same base class, and work with\n   either Sonnet 1 or Sonnet 2, depending on how the library is configured.\n\nWe do not recommmend users to inherit from this main class, as we only adapt the\nfunctionality for the GraphNets use cases.\n\nWe also define a `WrappedModelFnModule`. This is similar to `sonnet.v1.Module`,\nexcept that is receives a callable that returns the build method, rather than\nreceiving the build method directly. We need this because:\n - There is no analogous to `sonnet.v1.Module` in Sonnet 2.\n - `sonnet.v1.Module` relies on `get_variable` to return always the same\n   variables in subsequent calls to the Sonnet module. This means that passing\n   a single build method that builds submodules inside of it, yields the right\n   variable sharing when called multiple times, thanks to custom variable\n   getters. This mechanism does not work in Sonnet 2, and it would lead to\n   separate varaibles/submodules being isntantiated every time the module is\n   connected. This is why our `WrappedModelFnModule` instead, takes a callable\n   that can be called in the `__init__` similarly to how `*_model_fn` arguments\n   work in `blocks.py` and `modules.py`.\n\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\nimport contextlib\n\nimport six\nimport sonnet as snt\n\n_sonnet_version = snt.__version__\n_sonnet_major_version = int(_sonnet_version.split(""."")[0])\n\nif _sonnet_major_version == 1:\n\n  AbstractModule = snt.AbstractModule\n\nelif _sonnet_major_version == 2:\n\n  @six.add_metaclass(abc.ABCMeta)\n  class AbstractModule(snt.Module):\n    """"""Makes Sonnet1-style childs from this look like a Sonnet2 module.""""""\n\n    def __init__(self, *args, **kwargs):\n      super(AbstractModule, self).__init__(*args, **kwargs)\n      self.__call__.__func__.__doc__ = self._build.__doc__\n\n    # In snt2 calls to `_enter_variable_scope` are ignored.\n    @contextlib.contextmanager\n    def _enter_variable_scope(self, *args, **kwargs):\n      yield None\n\n    def __call__(self, *args, **kwargs):\n      return self._build(*args, **kwargs)\n\n    @abc.abstractmethod\n    def _build(self, *args, **kwargs):\n      """"""Similar to Sonnet 1 ._build method.""""""\n\n\nelse:\n  raise RuntimeError(\n      ""Unexpected sonnet major version %d"" % (_sonnet_major_version))\n\n\nclass WrappedModelFnModule(AbstractModule):\n  """"""Wraps a model_fn as a Sonnet module with a name.\n\n  Following `blocks.py` convention, a `model_fn` is a callable that, when called\n  with no arguments, returns a callable similar to a Sonnet module instance.\n\n  """"""\n\n  def __init__(self, model_fn, name):\n    """"""Inits the module.\n\n    Args:\n      model_fn: callable that, when called with no arguments, returns a callable\n          similar to a Sonnet module instance.\n      name: Name for the wrapper module.\n\n    """"""\n    super(WrappedModelFnModule, self).__init__(name=name)\n    with self._enter_variable_scope():\n      self._model = model_fn()\n\n  def _build(self, *args, **kwargs):\n    return self._model(*args, **kwargs)\n'"
graph_nets/blocks.py,48,"b'# Copyright 2018 The GraphNets Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n\n""""""Building blocks for Graph Networks.\n\nThis module contains elementary building blocks of graph networks:\n\n  - `broadcast_{field_1}_to_{field_2}` propagates the features from `field_1`\n    onto the relevant elements of `field_2`;\n\n  - `{field_1}To{field_2}Aggregator` propagates and then reduces the features\n    from `field_1` onto the relevant elements of `field_2`;\n\n  - the `EdgeBlock`, `NodeBlock` and `GlobalBlock` are elementary graph networks\n    that only update the edges (resp. the nodes, the globals) of their input\n    graph (as described in https://arxiv.org/abs/1806.01261).\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom graph_nets import _base\nfrom graph_nets import graphs\nfrom graph_nets import utils_tf\n\nimport tensorflow as tf\n\n\nNODES = graphs.NODES\nEDGES = graphs.EDGES\nGLOBALS = graphs.GLOBALS\nRECEIVERS = graphs.RECEIVERS\nSENDERS = graphs.SENDERS\nGLOBALS = graphs.GLOBALS\nN_NODE = graphs.N_NODE\nN_EDGE = graphs.N_EDGE\n\n\ndef _validate_graph(graph, mandatory_fields, additional_message=None):\n  for field in mandatory_fields:\n    if getattr(graph, field) is None:\n      message = ""`{}` field cannot be None"".format(field)\n      if additional_message:\n        message += "" "" + format(additional_message)\n      message += "".""\n      raise ValueError(message)\n\n\ndef _validate_broadcasted_graph(graph, from_field, to_field):\n  additional_message = ""when broadcasting {} to {}"".format(from_field, to_field)\n  _validate_graph(graph, [from_field, to_field], additional_message)\n\n\ndef _get_static_num_nodes(graph):\n  """"""Returns the static total number of nodes in a batch or None.""""""\n  return None if graph.nodes is None else graph.nodes.shape.as_list()[0]\n\n\ndef _get_static_num_edges(graph):\n  """"""Returns the static total number of edges in a batch or None.""""""\n  return None if graph.senders is None else graph.senders.shape.as_list()[0]\n\n\ndef broadcast_globals_to_edges(graph, name=""broadcast_globals_to_edges"",\n                               num_edges_hint=None):\n  """"""Broadcasts the global features to the edges of a graph.\n\n  Args:\n    graph: A `graphs.GraphsTuple` containing `Tensor`s, with globals features of\n      shape `[n_graphs] + global_shape`, and `N_EDGE` field of shape\n      `[n_graphs]`.\n    name: (string, optional) A name for the operation.\n    num_edges_hint: Integer indicating the total number of edges, if known.\n\n  Returns:\n    A tensor of shape `[n_edges] + global_shape`, where\n    `n_edges = sum(graph.n_edge)`. The i-th element of this tensor is given by\n    `globals[j]`, where j is the index of the graph the i-th edge belongs to\n    (i.e. is such that\n    `sum_{k < j} graphs.n_edge[k] <= i < sum_{k <= j} graphs.n_edge[k]`).\n\n  Raises:\n    ValueError: If either `graph.globals` or `graph.n_edge` is `None`.\n  """"""\n  _validate_broadcasted_graph(graph, GLOBALS, N_EDGE)\n  with tf.name_scope(name):\n    return utils_tf.repeat(graph.globals, graph.n_edge, axis=0,\n                           sum_repeats_hint=num_edges_hint)\n\n\ndef broadcast_globals_to_nodes(graph, name=""broadcast_globals_to_nodes"",\n                               num_nodes_hint=None):\n  """"""Broadcasts the global features to the nodes of a graph.\n\n  Args:\n    graph: A `graphs.GraphsTuple` containing `Tensor`s, with globals features of\n      shape `[n_graphs] + global_shape`, and `N_NODE` field of shape\n      `[n_graphs]`.\n    name: (string, optional) A name for the operation.\n    num_nodes_hint: Integer indicating the total number of nodes, if known.\n\n  Returns:\n    A tensor of shape `[n_nodes] + global_shape`, where\n    `n_nodes = sum(graph.n_node)`. The i-th element of this tensor is given by\n    `globals[j]`, where j is the index of the graph the i-th node belongs to\n    (i.e. is such that\n    `sum_{k < j} graphs.n_node[k] <= i < sum_{k <= j} graphs.n_node[k]`).\n\n  Raises:\n    ValueError: If either `graph.globals` or `graph.n_node` is `None`.\n  """"""\n  _validate_broadcasted_graph(graph, GLOBALS, N_NODE)\n  with tf.name_scope(name):\n    return utils_tf.repeat(graph.globals, graph.n_node, axis=0,\n                           sum_repeats_hint=num_nodes_hint)\n\n\ndef broadcast_sender_nodes_to_edges(\n    graph, name=""broadcast_sender_nodes_to_edges""):\n  """"""Broadcasts the node features to the edges they are sending into.\n\n  Args:\n    graph: A `graphs.GraphsTuple` containing `Tensor`s, with nodes features of\n      shape `[n_nodes] + node_shape`, and `senders` field of shape\n      `[n_edges]`.\n    name: (string, optional) A name for the operation.\n\n  Returns:\n    A tensor of shape `[n_edges] + node_shape`, where\n    `n_edges = sum(graph.n_edge)`. The i-th element is given by\n    `graph.nodes[graph.senders[i]]`.\n\n  Raises:\n    ValueError: If either `graph.nodes` or `graph.senders` is `None`.\n  """"""\n  _validate_broadcasted_graph(graph, NODES, SENDERS)\n  with tf.name_scope(name):\n    return tf.gather(graph.nodes, graph.senders)\n\n\ndef broadcast_receiver_nodes_to_edges(\n    graph, name=""broadcast_receiver_nodes_to_edges""):\n  """"""Broadcasts the node features to the edges they are receiving from.\n\n  Args:\n    graph: A `graphs.GraphsTuple` containing `Tensor`s, with nodes features of\n      shape `[n_nodes] + node_shape`, and receivers of shape `[n_edges]`.\n    name: (string, optional) A name for the operation.\n\n  Returns:\n    A tensor of shape `[n_edges] + node_shape`, where\n    `n_edges = sum(graph.n_edge)`. The i-th element is given by\n    `graph.nodes[graph.receivers[i]]`.\n\n  Raises:\n    ValueError: If either `graph.nodes` or `graph.receivers` is `None`.\n  """"""\n  _validate_broadcasted_graph(graph, NODES, RECEIVERS)\n  with tf.name_scope(name):\n    return tf.gather(graph.nodes, graph.receivers)\n\n\nclass EdgesToGlobalsAggregator(_base.AbstractModule):\n  """"""Aggregates all edges into globals.""""""\n\n  def __init__(self, reducer, name=""edges_to_globals_aggregator""):\n    """"""Initializes the EdgesToGlobalsAggregator module.\n\n    The reducer is used for combining per-edge features (one set of edge\n    feature vectors per graph) to give per-graph features (one feature\n    vector per graph). The reducer should take a `Tensor` of edge features, a\n    `Tensor` of segment indices, and a number of graphs. It should be invariant\n    under permutation of edge features within each graph.\n\n    Examples of compatible reducers are:\n    * tf.math.unsorted_segment_sum\n    * tf.math.unsorted_segment_mean\n    * tf.math.unsorted_segment_prod\n    * unsorted_segment_min_or_zero\n    * unsorted_segment_max_or_zero\n\n    Args:\n      reducer: A function for reducing sets of per-edge features to individual\n        per-graph features.\n      name: The module name.\n    """"""\n    super(EdgesToGlobalsAggregator, self).__init__(name=name)\n    self._reducer = reducer\n\n  def _build(self, graph):\n    _validate_graph(graph, (EDGES,),\n                    additional_message=""when aggregating from edges."")\n    num_graphs = utils_tf.get_num_graphs(graph)\n    graph_index = tf.range(num_graphs)\n    indices = utils_tf.repeat(graph_index, graph.n_edge, axis=0,\n                              sum_repeats_hint=_get_static_num_edges(graph))\n    return self._reducer(graph.edges, indices, num_graphs)\n\n\nclass NodesToGlobalsAggregator(_base.AbstractModule):\n  """"""Aggregates all nodes into globals.""""""\n\n  def __init__(self, reducer, name=""nodes_to_globals_aggregator""):\n    """"""Initializes the NodesToGlobalsAggregator module.\n\n    The reducer is used for combining per-node features (one set of node\n    feature vectors per graph) to give per-graph features (one feature\n    vector per graph). The reducer should take a `Tensor` of node features, a\n    `Tensor` of segment indices, and a number of graphs. It should be invariant\n    under permutation of node features within each graph.\n\n    Examples of compatible reducers are:\n    * tf.math.unsorted_segment_sum\n    * tf.math.unsorted_segment_mean\n    * tf.math.unsorted_segment_prod\n    * unsorted_segment_min_or_zero\n    * unsorted_segment_max_or_zero\n\n    Args:\n      reducer: A function for reducing sets of per-node features to individual\n        per-graph features.\n      name: The module name.\n    """"""\n    super(NodesToGlobalsAggregator, self).__init__(name=name)\n    self._reducer = reducer\n\n  def _build(self, graph):\n    _validate_graph(graph, (NODES,),\n                    additional_message=""when aggregating from nodes."")\n    num_graphs = utils_tf.get_num_graphs(graph)\n    graph_index = tf.range(num_graphs)\n    indices = utils_tf.repeat(graph_index, graph.n_node, axis=0,\n                              sum_repeats_hint=_get_static_num_nodes(graph))\n    return self._reducer(graph.nodes, indices, num_graphs)\n\n\nclass _EdgesToNodesAggregator(_base.AbstractModule):\n  """"""Agregates sent or received edges into the corresponding nodes.""""""\n\n  def __init__(self, reducer, use_sent_edges=False,\n               name=""edges_to_nodes_aggregator""):\n    super(_EdgesToNodesAggregator, self).__init__(name=name)\n    self._reducer = reducer\n    self._use_sent_edges = use_sent_edges\n\n  def _build(self, graph):\n    _validate_graph(graph, (EDGES, SENDERS, RECEIVERS,),\n                    additional_message=""when aggregating from edges."")\n    # If the number of nodes are known at graph construction time (based on the\n    # shape) then use that value to make the model compatible with XLA/TPU.\n    if graph.nodes is not None and graph.nodes.shape.as_list()[0] is not None:\n      num_nodes = graph.nodes.shape.as_list()[0]\n    else:\n      num_nodes = tf.reduce_sum(graph.n_node)\n    indices = graph.senders if self._use_sent_edges else graph.receivers\n    return self._reducer(graph.edges, indices, num_nodes)\n\n\nclass SentEdgesToNodesAggregator(_EdgesToNodesAggregator):\n  """"""Agregates sent edges into the corresponding sender nodes.""""""\n\n  def __init__(self, reducer, name=""sent_edges_to_nodes_aggregator""):\n    """"""Constructor.\n\n    The reducer is used for combining per-edge features (one set of edge\n    feature vectors per node) to give per-node features (one feature\n    vector per node). The reducer should take a `Tensor` of edge features, a\n    `Tensor` of segment indices, and a number of nodes. It should be invariant\n    under permutation of edge features within each segment.\n\n    Examples of compatible reducers are:\n    * tf.math.unsorted_segment_sum\n    * tf.math.unsorted_segment_mean\n    * tf.math.unsorted_segment_prod\n    * unsorted_segment_min_or_zero\n    * unsorted_segment_max_or_zero\n\n    Args:\n      reducer: A function for reducing sets of per-edge features to individual\n        per-node features.\n      name: The module name.\n    """"""\n    super(SentEdgesToNodesAggregator, self).__init__(\n        use_sent_edges=True,\n        reducer=reducer,\n        name=name)\n\n\nclass ReceivedEdgesToNodesAggregator(_EdgesToNodesAggregator):\n  """"""Agregates received edges into the corresponding receiver nodes.""""""\n\n  def __init__(self, reducer, name=""received_edges_to_nodes_aggregator""):\n    """"""Constructor.\n\n    The reducer is used for combining per-edge features (one set of edge\n    feature vectors per node) to give per-node features (one feature\n    vector per node). The reducer should take a `Tensor` of edge features, a\n    `Tensor` of segment indices, and a number of nodes. It should be invariant\n    under permutation of edge features within each segment.\n\n    Examples of compatible reducers are:\n    * tf.math.unsorted_segment_sum\n    * tf.math.unsorted_segment_mean\n    * tf.math.unsorted_segment_prod\n    * unsorted_segment_min_or_zero\n    * unsorted_segment_max_or_zero\n\n    Args:\n      reducer: A function for reducing sets of per-edge features to individual\n        per-node features.\n      name: The module name.\n    """"""\n    super(ReceivedEdgesToNodesAggregator, self).__init__(\n        use_sent_edges=False, reducer=reducer, name=name)\n\n\ndef _unsorted_segment_reduction_or_zero(reducer, values, indices, num_groups):\n  """"""Common code for unsorted_segment_{min,max}_or_zero (below).""""""\n  reduced = reducer(values, indices, num_groups)\n  present_indices = tf.math.unsorted_segment_max(\n      tf.ones_like(indices, dtype=reduced.dtype), indices, num_groups)\n  present_indices = tf.clip_by_value(present_indices, 0, 1)\n  present_indices = tf.reshape(\n      present_indices, [num_groups] + [1] * (reduced.shape.ndims - 1))\n  reduced *= present_indices\n  return reduced\n\n\ndef unsorted_segment_min_or_zero(values, indices, num_groups,\n                                 name=""unsorted_segment_min_or_zero""):\n  """"""Aggregates information using elementwise min.\n\n  Segments with no elements are given a ""min"" of zero instead of the most\n  positive finite value possible (which is what `tf.math.unsorted_segment_min`\n  would do).\n\n  Args:\n    values: A `Tensor` of per-element features.\n    indices: A 1-D `Tensor` whose length is equal to `values`\' first dimension.\n    num_groups: A `Tensor`.\n    name: (string, optional) A name for the operation.\n\n  Returns:\n    A `Tensor` of the same type as `values`.\n  """"""\n  with tf.name_scope(name):\n    return _unsorted_segment_reduction_or_zero(\n        tf.math.unsorted_segment_min, values, indices, num_groups)\n\n\ndef unsorted_segment_max_or_zero(values, indices, num_groups,\n                                 name=""unsorted_segment_max_or_zero""):\n  """"""Aggregates information using elementwise max.\n\n  Segments with no elements are given a ""max"" of zero instead of the most\n  negative finite value possible (which is what `tf.math.unsorted_segment_max`\n  would do).\n\n  Args:\n    values: A `Tensor` of per-element features.\n    indices: A 1-D `Tensor` whose length is equal to `values`\' first dimension.\n    num_groups: A `Tensor`.\n    name: (string, optional) A name for the operation.\n\n  Returns:\n    A `Tensor` of the same type as `values`.\n  """"""\n  with tf.name_scope(name):\n    return _unsorted_segment_reduction_or_zero(\n        tf.math.unsorted_segment_max, values, indices, num_groups)\n\n\nclass EdgeBlock(_base.AbstractModule):\n  """"""Edge block.\n\n  A block that updates the features of each edge in a batch of graphs based on\n  (a subset of) the previous edge features, the features of the adjacent nodes,\n  and the global features of the corresponding graph.\n\n  See https://arxiv.org/abs/1806.01261 for more details.\n  """"""\n\n  def __init__(self,\n               edge_model_fn,\n               use_edges=True,\n               use_receiver_nodes=True,\n               use_sender_nodes=True,\n               use_globals=True,\n               name=""edge_block""):\n    """"""Initializes the EdgeBlock module.\n\n    Args:\n      edge_model_fn: A callable that will be called in the variable scope of\n        this EdgeBlock and should return a Sonnet module (or equivalent\n        callable) to be used as the edge model. The returned module should take\n        a `Tensor` (of concatenated input features for each edge) and return a\n        `Tensor` (of output features for each edge). Typically, this module\n        would input and output `Tensor`s of rank 2, but it may also be input or\n        output larger ranks. See the `_build` method documentation for more\n        details on the acceptable inputs to this module in that case.\n      use_edges: (bool, default=True). Whether to condition on edge attributes.\n      use_receiver_nodes: (bool, default=True). Whether to condition on receiver\n        node attributes.\n      use_sender_nodes: (bool, default=True). Whether to condition on sender\n        node attributes.\n      use_globals: (bool, default=True). Whether to condition on global\n        attributes.\n      name: The module name.\n\n    Raises:\n      ValueError: When fields that are required are missing.\n    """"""\n    super(EdgeBlock, self).__init__(name=name)\n\n    if not (use_edges or use_sender_nodes or use_receiver_nodes or use_globals):\n      raise ValueError(""At least one of use_edges, use_sender_nodes, ""\n                       ""use_receiver_nodes or use_globals must be True."")\n\n    self._use_edges = use_edges\n    self._use_receiver_nodes = use_receiver_nodes\n    self._use_sender_nodes = use_sender_nodes\n    self._use_globals = use_globals\n\n    with self._enter_variable_scope():\n      self._edge_model = edge_model_fn()\n\n  def _build(self, graph):\n    """"""Connects the edge block.\n\n    Args:\n      graph: A `graphs.GraphsTuple` containing `Tensor`s, whose individual edges\n        features (if `use_edges` is `True`), individual nodes features (if\n        `use_receiver_nodes` or `use_sender_nodes` is `True`) and per graph\n        globals (if `use_globals` is `True`) should be concatenable on the last\n        axis.\n\n    Returns:\n      An output `graphs.GraphsTuple` with updated edges.\n\n    Raises:\n      ValueError: If `graph` does not have non-`None` receivers and senders, or\n        if `graph` has `None` fields incompatible with the selected `use_edges`,\n        `use_receiver_nodes`, `use_sender_nodes`, or `use_globals` options.\n    """"""\n    _validate_graph(\n        graph, (SENDERS, RECEIVERS, N_EDGE), "" when using an EdgeBlock"")\n\n    edges_to_collect = []\n\n    if self._use_edges:\n      _validate_graph(graph, (EDGES,), ""when use_edges == True"")\n      edges_to_collect.append(graph.edges)\n\n    if self._use_receiver_nodes:\n      edges_to_collect.append(broadcast_receiver_nodes_to_edges(graph))\n\n    if self._use_sender_nodes:\n      edges_to_collect.append(broadcast_sender_nodes_to_edges(graph))\n\n    if self._use_globals:\n      num_edges_hint = _get_static_num_edges(graph)\n      edges_to_collect.append(\n          broadcast_globals_to_edges(graph, num_edges_hint=num_edges_hint))\n\n    collected_edges = tf.concat(edges_to_collect, axis=-1)\n    updated_edges = self._edge_model(collected_edges)\n    return graph.replace(edges=updated_edges)\n\n\nclass NodeBlock(_base.AbstractModule):\n  """"""Node block.\n\n  A block that updates the features of each node in batch of graphs based on\n  (a subset of) the previous node features, the aggregated features of the\n  adjacent edges, and the global features of the corresponding graph.\n\n  See https://arxiv.org/abs/1806.01261 for more details.\n  """"""\n\n  def __init__(self,\n               node_model_fn,\n               use_received_edges=True,\n               use_sent_edges=False,\n               use_nodes=True,\n               use_globals=True,\n               received_edges_reducer=tf.math.unsorted_segment_sum,\n               sent_edges_reducer=tf.math.unsorted_segment_sum,\n               name=""node_block""):\n    """"""Initializes the NodeBlock module.\n\n    Args:\n      node_model_fn: A callable that will be called in the variable scope of\n        this NodeBlock and should return a Sonnet module (or equivalent\n        callable) to be used as the node model. The returned module should take\n        a `Tensor` (of concatenated input features for each node) and return a\n        `Tensor` (of output features for each node). Typically, this module\n        would input and output `Tensor`s of rank 2, but it may also be input or\n        output larger ranks. See the `_build` method documentation for more\n        details on the acceptable inputs to this module in that case.\n      use_received_edges: (bool, default=True) Whether to condition on\n        aggregated edges received by each node.\n      use_sent_edges: (bool, default=False) Whether to condition on aggregated\n        edges sent by each node.\n      use_nodes: (bool, default=True) Whether to condition on node attributes.\n      use_globals: (bool, default=True) Whether to condition on global\n        attributes.\n      received_edges_reducer: Reduction to be used when aggregating received\n        edges. This should be a callable whose signature matches\n        `tf.math.unsorted_segment_sum`.\n      sent_edges_reducer: Reduction to be used when aggregating sent edges.\n        This should be a callable whose signature matches\n        `tf.math.unsorted_segment_sum`.\n      name: The module name.\n\n    Raises:\n      ValueError: When fields that are required are missing.\n    """"""\n\n    super(NodeBlock, self).__init__(name=name)\n\n    if not (use_nodes or use_sent_edges or use_received_edges or use_globals):\n      raise ValueError(""At least one of use_received_edges, use_sent_edges, ""\n                       ""use_nodes or use_globals must be True."")\n\n    self._use_received_edges = use_received_edges\n    self._use_sent_edges = use_sent_edges\n    self._use_nodes = use_nodes\n    self._use_globals = use_globals\n\n    with self._enter_variable_scope():\n      self._node_model = node_model_fn()\n      if self._use_received_edges:\n        if received_edges_reducer is None:\n          raise ValueError(\n              ""If `use_received_edges==True`, `received_edges_reducer` ""\n              ""should not be None."")\n        self._received_edges_aggregator = ReceivedEdgesToNodesAggregator(\n            received_edges_reducer)\n      if self._use_sent_edges:\n        if sent_edges_reducer is None:\n          raise ValueError(\n              ""If `use_sent_edges==True`, `sent_edges_reducer` ""\n              ""should not be None."")\n        self._sent_edges_aggregator = SentEdgesToNodesAggregator(\n            sent_edges_reducer)\n\n  def _build(self, graph):\n    """"""Connects the node block.\n\n    Args:\n      graph: A `graphs.GraphsTuple` containing `Tensor`s, whose individual edges\n        features (if `use_received_edges` or `use_sent_edges` is `True`),\n        individual nodes features (if `use_nodes` is True) and per graph globals\n        (if `use_globals` is `True`) should be concatenable on the last axis.\n\n    Returns:\n      An output `graphs.GraphsTuple` with updated nodes.\n    """"""\n\n    nodes_to_collect = []\n\n    if self._use_received_edges:\n      nodes_to_collect.append(self._received_edges_aggregator(graph))\n\n    if self._use_sent_edges:\n      nodes_to_collect.append(self._sent_edges_aggregator(graph))\n\n    if self._use_nodes:\n      _validate_graph(graph, (NODES,), ""when use_nodes == True"")\n      nodes_to_collect.append(graph.nodes)\n\n    if self._use_globals:\n      # The hint will be an integer if the graph has node features and the total\n      # number of nodes is known at tensorflow graph definition time, or None\n      # otherwise.\n      num_nodes_hint = _get_static_num_nodes(graph)\n      nodes_to_collect.append(\n          broadcast_globals_to_nodes(graph, num_nodes_hint=num_nodes_hint))\n\n    collected_nodes = tf.concat(nodes_to_collect, axis=-1)\n    updated_nodes = self._node_model(collected_nodes)\n    return graph.replace(nodes=updated_nodes)\n\n\nclass GlobalBlock(_base.AbstractModule):\n  """"""Global block.\n\n  A block that updates the global features of each graph in a batch based on\n  (a subset of) the previous global features, the aggregated features of the\n  edges of the graph, and the aggregated features of the nodes of the graph.\n\n  See https://arxiv.org/abs/1806.01261 for more details.\n  """"""\n\n  def __init__(self,\n               global_model_fn,\n               use_edges=True,\n               use_nodes=True,\n               use_globals=True,\n               nodes_reducer=tf.math.unsorted_segment_sum,\n               edges_reducer=tf.math.unsorted_segment_sum,\n               name=""global_block""):\n    """"""Initializes the GlobalBlock module.\n\n    Args:\n      global_model_fn: A callable that will be called in the variable scope of\n        this GlobalBlock and should return a Sonnet module (or equivalent\n        callable) to be used as the global model. The returned module should\n        take a `Tensor` (of concatenated input features) and return a `Tensor`\n        (the global output features). Typically, this module would input and\n        output `Tensor`s of rank 2, but it may also input or output larger\n        ranks. See the `_build` method documentation for more details on the\n        acceptable inputs to this module in that case.\n      use_edges: (bool, default=True) Whether to condition on aggregated edges.\n      use_nodes: (bool, default=True) Whether to condition on node attributes.\n      use_globals: (bool, default=True) Whether to condition on global\n        attributes.\n      nodes_reducer: Reduction to be used when aggregating nodes. This should\n        be a callable whose signature matches tf.math.unsorted_segment_sum.\n      edges_reducer: Reduction to be used when aggregating edges. This should\n        be a callable whose signature matches tf.math.unsorted_segment_sum.\n      name: The module name.\n\n    Raises:\n      ValueError: When fields that are required are missing.\n    """"""\n\n    super(GlobalBlock, self).__init__(name=name)\n\n    if not (use_nodes or use_edges or use_globals):\n      raise ValueError(""At least one of use_edges, ""\n                       ""use_nodes or use_globals must be True."")\n\n    self._use_edges = use_edges\n    self._use_nodes = use_nodes\n    self._use_globals = use_globals\n\n    with self._enter_variable_scope():\n      self._global_model = global_model_fn()\n      if self._use_edges:\n        if edges_reducer is None:\n          raise ValueError(\n              ""If `use_edges==True`, `edges_reducer` should not be None."")\n        self._edges_aggregator = EdgesToGlobalsAggregator(\n            edges_reducer)\n      if self._use_nodes:\n        if nodes_reducer is None:\n          raise ValueError(\n              ""If `use_nodes==True`, `nodes_reducer` should not be None."")\n        self._nodes_aggregator = NodesToGlobalsAggregator(\n            nodes_reducer)\n\n  def _build(self, graph):\n    """"""Connects the global block.\n\n    Args:\n      graph: A `graphs.GraphsTuple` containing `Tensor`s, whose individual edges\n        (if `use_edges` is `True`), individual nodes (if `use_nodes` is True)\n        and per graph globals (if `use_globals` is `True`) should be\n        concatenable on the last axis.\n\n    Returns:\n      An output `graphs.GraphsTuple` with updated globals.\n    """"""\n    globals_to_collect = []\n\n    if self._use_edges:\n      _validate_graph(graph, (EDGES,), ""when use_edges == True"")\n      globals_to_collect.append(self._edges_aggregator(graph))\n\n    if self._use_nodes:\n      _validate_graph(graph, (NODES,), ""when use_nodes == True"")\n      globals_to_collect.append(self._nodes_aggregator(graph))\n\n    if self._use_globals:\n      _validate_graph(graph, (GLOBALS,), ""when use_globals == True"")\n      globals_to_collect.append(graph.globals)\n\n    collected_globals = tf.concat(globals_to_collect, axis=-1)\n    updated_globals = self._global_model(collected_globals)\n    return graph.replace(globals=updated_globals)\n'"
graph_nets/graphs.py,0,"b'# Copyright 2018 The GraphNets Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n\n""""""A class that defines graph-structured data.\n\nThe main purpose of the `GraphsTuple` is to represent multiple graphs with\ndifferent shapes and sizes in a way that supports batched processing.\n\nThis module first defines the string constants which are used to represent\ngraph(s) as tuples or dictionaries: `N_NODE, N_EDGE, NODES, EDGES, RECEIVERS,\nSENDERS, GLOBALS`.\n\nThis representation could typically take the following form, for a batch of\n`n_graphs` graphs stored in a `GraphsTuple` called graph:\n\n  - N_NODE: The number of nodes per graph. It is a vector of integers with shape\n    `[n_graphs]`, such that `graph.N_NODE[i]` is the number of nodes in the i-th\n    graph.\n\n  - N_EDGE: The number of edges per graph. It is a vector of integers with shape\n    `[n_graphs]`, such that `graph.N_EDGE[i]` is the number of edges in the i-th\n    graph.\n\n  - NODES: The nodes features. It is either `None` (the graph has no node\n    features), or a vector of shape `[n_nodes] + node_shape`, where\n    `n_nodes = sum(graph.N_NODE)` is the total number of nodes in the batch of\n    graphs, and `node_shape` represents the shape of the features of each node.\n    The relative index of a node from the batched version can be recovered from\n    the `graph.N_NODE` property. For instance, the second node of the third\n    graph will have its features in the\n    `1 + graph.N_NODE[0] + graph.N_NODE[1]`-th slot of graph.NODES.\n    Observe that having a `None` value for this field does not mean that the\n    graphs have no nodes, only that they do not have node features.\n\n  - EDGES: The edges features. It is either `None` (the graph has no edge\n    features), or a vector of shape `[n_edges] + edge_shape`, where\n    `n_edges = sum(graph.N_EDGE)` is the total number of edges in the batch of\n    graphs, and `edge_shape` represents the shape of the features of each edge.\n    The relative index of an edge from the batched version can be recovered from\n    the `graph.N_EDGE` property. For instance, the third edge of the third\n    graph will have its features in the `2 + graph.N_EDGE[0] + graph.N_EDGE[1]`-\n    th slot of graph.EDGES.\n    Observe that having a `None` value for this field does not necessarily mean\n    that the graph has no edges, only that they do not have edge features.\n\n  - RECEIVERS: The indices of the receiver nodes, for each edge. It is either\n    `None` (if the graph has no edges), or a vector of integers of shape\n    `[n_edges]`, such that `graph.RECEIVERS[i]` is the index of the node\n    receiving from the i-th edge.\n    Observe that the index is absolute (in other words, cumulative), i.e.\n    `graphs.RECEIVERS` take value in `[0, n_nodes]`. For instance, an edge\n    connecting the vertices with relative indices 2 and 3 in the second graph of\n    the batch would have a `RECEIVERS` value of `3 + graph.N_NODE[0]`.\n    If `graphs.RECEIVERS` is `None`, then `graphs.EDGES` and `graphs.SENDERS`\n    should also be `None`.\n\n  - SENDERS: The indices of the sender nodes, for each edge. It is either\n    `None` (if the graph has no edges), or a vector of integers of shape\n    `[n_edges]`, such that `graph.SENDERS[i]` is the index of the node\n    sending from the i-th edge.\n    Observe that the index is absolute, i.e. `graphs.RECEIVERS` take value in\n    `[0, n_nodes]`. For instance, an edge connecting the vertices with relative\n    indices 1 and 3 in the third graph of the batch would have a `SENDERS` value\n    of `1 + graph.N_NODE[0] + graph.N_NODE[1]`.\n    If `graphs.SENDERS` is `None`, then `graphs.EDGES` and `graphs.RECEIVERS`\n    should also be `None`.\n\n  - GLOBALS: The global features of the graph. It is either `None` (the graph\n    has no global features), or a vector of shape `[n_graphs] + global_shape`\n    representing graph level features.\n\nThe `utils_np` and `utils_tf` modules provide convenience methods to work with\ngraph that contain numpy and tensorflow data, respectively: conversion,\nbatching, unbatching, indexing, among others.\n\nThe `GraphsTuple` class, however, is not restricted to storing vectors, and can\nbe used to store attributes of graphs as well (for instance, types or shapes).\n\nThe only assertions it makes are that the `None` fields are compatible with the\ndefinition of a graph given above, namely:\n\n  - the N_NODE and N_EDGE fields cannot be `None`;\n\n  - if RECEIVERS is None, then SENDERS must be `None` (and vice-versa);\n\n  - if RECEIVERS and SENDERS are `None`, then `EDGES` must be `None`.\n\nThose assumptions are checked both upon initialization and when replacing a\nfield by calling the `replace` or `map` method.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nNODES = ""nodes""\nEDGES = ""edges""\nRECEIVERS = ""receivers""\nSENDERS = ""senders""\nGLOBALS = ""globals""\nN_NODE = ""n_node""\nN_EDGE = ""n_edge""\n\nGRAPH_FEATURE_FIELDS = (NODES, EDGES, GLOBALS)\nGRAPH_INDEX_FIELDS = (RECEIVERS, SENDERS)\nGRAPH_DATA_FIELDS = (NODES, EDGES, RECEIVERS, SENDERS, GLOBALS)\nGRAPH_NUMBER_FIELDS = (N_NODE, N_EDGE)\nALL_FIELDS = (NODES, EDGES, RECEIVERS, SENDERS, GLOBALS, N_NODE, N_EDGE)\n\n\nclass GraphsTuple(\n    collections.namedtuple(""GraphsTuple"",\n                           GRAPH_DATA_FIELDS + GRAPH_NUMBER_FIELDS)):\n  """"""Default namedtuple describing `Graphs`s.\n\n  A children of `collections.namedtuple`s, which allows it to be directly input\n  and output from `tensorflow.Session.run()` calls.\n\n  An instance of this class can be constructed as\n  ```\n  GraphsTuple(nodes=nodes,\n              edges=edges,\n              globals=globals,\n              receivers=receivers,\n              senders=senders,\n              n_node=n_node,\n              n_edge=n_edge)\n  ```\n  where `nodes`, `edges`, `globals`, `receivers`, `senders`, `n_node` and\n  `n_edge` are arbitrary, but are typically numpy arrays, tensors, or `None`;\n  see module\'s documentation for a more detailed description of which fields\n  can be left `None`.\n  """"""\n\n  def _validate_none_fields(self):\n    """"""Asserts that the set of `None` fields in the instance is valid.""""""\n    if self.n_node is None:\n      raise ValueError(""Field `n_node` cannot be None"")\n    if self.n_edge is None:\n      raise ValueError(""Field `n_edge` cannot be None"")\n    if self.receivers is None and self.senders is not None:\n      raise ValueError(\n          ""Field `senders` must be None as field `receivers` is None"")\n    if self.senders is None and self.receivers is not None:\n      raise ValueError(\n          ""Field `receivers` must be None as field `senders` is None"")\n    if self.receivers is None and self.edges is not None:\n      raise ValueError(\n          ""Field `edges` must be None as field `receivers` and `senders` are ""\n          ""None"")\n\n  def __init__(self, *args, **kwargs):\n    del args, kwargs\n    # The fields of a `namedtuple` are filled in the `__new__` method.\n    # `__init__` does not accept parameters.\n    super(GraphsTuple, self).__init__()\n    self._validate_none_fields()\n\n  def replace(self, **kwargs):\n    output = self._replace(**kwargs)\n    output._validate_none_fields()  # pylint: disable=protected-access\n    return output\n\n  def map(self, field_fn, fields=GRAPH_FEATURE_FIELDS):\n    """"""Applies `field_fn` to the fields `fields` of the instance.\n\n    `field_fn` is applied exactly once per field in `fields`. The result must\n    satisfy the `GraphsTuple` requirement w.r.t. `None` fields, i.e. the\n    `SENDERS` cannot be `None` if the `EDGES` or `RECEIVERS` are not `None`,\n    etc.\n\n    Args:\n      field_fn: A callable that take a single argument.\n      fields: (iterable of `str`). An iterable of the fields to apply\n        `field_fn` to.\n\n    Returns:\n      A copy of the instance, with the fields in `fields` replaced by the result\n      of applying `field_fn` to them.\n    """"""\n    return self.replace(**{k: field_fn(getattr(self, k)) for k in fields})\n'"
graph_nets/modules.py,22,"b'# Copyright 2018 The GraphNets Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Common Graph Network architectures.\n\nThe modules in this files are Sonnet modules that:\n\n  - take a `graphs.GraphsTuple` containing `Tensor`s as input, with possibly\n    `None` fields (depending on the module);\n\n  - return a `graphs.GraphsTuple` with updated values for some fields\n    (depending on the module).\n\n\nThe provided modules are:\n\n  - `GraphNetwork`: a general purpose Graph Network composed of configurable\n    `EdgeBlock`, `NodeBlock` and `GlobalBlock` from `blocks.py`;\n\n  - `GraphIndependent`: a Graph Network producing updated edges (resp. nodes,\n    globals) based on the input\'s edges (resp. nodes, globals) only;\n\n  - `InteractionNetwork` (from https://arxiv.org/abs/1612.00222): a\n    network propagating information on the edges and nodes of a graph;\n\n  - RelationNetwork (from https://arxiv.org/abs/1706.01427): a network\n    updating the global property based on the relation between the input\'s\n    nodes properties;\n\n  - DeepSets (from https://arxiv.org/abs/1703.06114): a network that operates on\n    sets (graphs without edges);\n\n  - CommNet (from https://arxiv.org/abs/1605.07736 and\n    https://arxiv.org/abs/1706.06122): a network updating nodes based on their\n    previous features and the features of the adjacent nodes.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom graph_nets import _base\nfrom graph_nets import blocks\nimport tensorflow as tf\n\n_DEFAULT_EDGE_BLOCK_OPT = {\n    ""use_edges"": True,\n    ""use_receiver_nodes"": True,\n    ""use_sender_nodes"": True,\n    ""use_globals"": True,\n}\n\n_DEFAULT_NODE_BLOCK_OPT = {\n    ""use_received_edges"": True,\n    ""use_sent_edges"": False,\n    ""use_nodes"": True,\n    ""use_globals"": True,\n}\n\n_DEFAULT_GLOBAL_BLOCK_OPT = {\n    ""use_edges"": True,\n    ""use_nodes"": True,\n    ""use_globals"": True,\n}\n\n\nclass InteractionNetwork(_base.AbstractModule):\n  """"""Implementation of an Interaction Network.\n\n  An interaction networks computes interactions on the edges based on the\n  previous edges features, and on the features of the nodes sending into those\n  edges. It then updates the nodes based on the incomming updated edges.\n  See https://arxiv.org/abs/1612.00222 for more details.\n\n  This model does not update the graph globals, and they are allowed to be\n  `None`.\n  """"""\n\n  def __init__(self,\n               edge_model_fn,\n               node_model_fn,\n               reducer=tf.math.unsorted_segment_sum,\n               name=""interaction_network""):\n    """"""Initializes the InteractionNetwork module.\n\n    Args:\n      edge_model_fn: A callable that will be passed to `EdgeBlock` to perform\n        per-edge computations. The callable must return a Sonnet module (or\n        equivalent; see `blocks.EdgeBlock` for details), and the shape of the\n        output of this module must match the one of the input nodes, but for the\n        first and last axis.\n      node_model_fn: A callable that will be passed to `NodeBlock` to perform\n        per-node computations. The callable must return a Sonnet module (or\n        equivalent; see `blocks.NodeBlock` for details).\n      reducer: Reducer to be used by NodeBlock to aggregate edges. Defaults to\n        tf.math.unsorted_segment_sum.\n      name: The module name.\n    """"""\n    super(InteractionNetwork, self).__init__(name=name)\n\n    with self._enter_variable_scope():\n      self._edge_block = blocks.EdgeBlock(\n          edge_model_fn=edge_model_fn, use_globals=False)\n      self._node_block = blocks.NodeBlock(\n          node_model_fn=node_model_fn,\n          use_sent_edges=False,\n          use_globals=False,\n          received_edges_reducer=reducer)\n\n  def _build(self, graph):\n    """"""Connects the InterationNetwork.\n\n    Args:\n      graph: A `graphs.GraphsTuple` containing `Tensor`s. `graph.globals` can be\n        `None`. The features of each node and edge of `graph` must be\n        concatenable on the last axis (i.e., the shapes of `graph.nodes` and\n        `graph.edges` must match but for their first and last axis).\n\n    Returns:\n      An output `graphs.GraphsTuple` with updated edges and nodes.\n\n    Raises:\n      ValueError: If any of `graph.nodes`, `graph.edges`, `graph.receivers` or\n        `graph.senders` is `None`.\n    """"""\n    return self._node_block(self._edge_block(graph))\n\n\nclass RelationNetwork(_base.AbstractModule):\n  """"""Implementation of a Relation Network.\n\n  See https://arxiv.org/abs/1706.01427 for more details.\n\n  The global and edges features of the input graph are not used, and are\n  allowed to be `None` (the receivers and senders properties must be present).\n  The output graph has updated, non-`None`, globals.\n  """"""\n\n  def __init__(self,\n               edge_model_fn,\n               global_model_fn,\n               reducer=tf.math.unsorted_segment_sum,\n               name=""relation_network""):\n    """"""Initializes the RelationNetwork module.\n\n    Args:\n      edge_model_fn: A callable that will be passed to EdgeBlock to perform\n        per-edge computations. The callable must return a Sonnet module (or\n        equivalent; see EdgeBlock for details).\n      global_model_fn: A callable that will be passed to GlobalBlock to perform\n        per-global computations. The callable must return a Sonnet module (or\n        equivalent; see GlobalBlock for details).\n      reducer: Reducer to be used by GlobalBlock to aggregate edges. Defaults to\n        tf.math.unsorted_segment_sum.\n      name: The module name.\n    """"""\n    super(RelationNetwork, self).__init__(name=name)\n\n    with self._enter_variable_scope():\n      self._edge_block = blocks.EdgeBlock(\n          edge_model_fn=edge_model_fn,\n          use_edges=False,\n          use_receiver_nodes=True,\n          use_sender_nodes=True,\n          use_globals=False)\n\n      self._global_block = blocks.GlobalBlock(\n          global_model_fn=global_model_fn,\n          use_edges=True,\n          use_nodes=False,\n          use_globals=False,\n          edges_reducer=reducer)\n\n  def _build(self, graph):\n    """"""Connects the RelationNetwork.\n\n    Args:\n      graph: A `graphs.GraphsTuple` containing `Tensor`s, except for the edges\n        and global properties which may be `None`.\n\n    Returns:\n      A `graphs.GraphsTuple` with updated globals.\n\n    Raises:\n      ValueError: If any of `graph.nodes`, `graph.receivers` or `graph.senders`\n        is `None`.\n    """"""\n    output_graph = self._global_block(self._edge_block(graph))\n    return graph.replace(globals=output_graph.globals)\n\n\ndef _make_default_edge_block_opt(edge_block_opt):\n  """"""Default options to be used in the EdgeBlock of a generic GraphNetwork.""""""\n  edge_block_opt = dict(edge_block_opt.items()) if edge_block_opt else {}\n  for k, v in _DEFAULT_EDGE_BLOCK_OPT.items():\n    edge_block_opt[k] = edge_block_opt.get(k, v)\n  return edge_block_opt\n\n\ndef _make_default_node_block_opt(node_block_opt, default_reducer):\n  """"""Default options to be used in the NodeBlock of a generic GraphNetwork.""""""\n  node_block_opt = dict(node_block_opt.items()) if node_block_opt else {}\n  for k, v in _DEFAULT_NODE_BLOCK_OPT.items():\n    node_block_opt[k] = node_block_opt.get(k, v)\n  for key in [""received_edges_reducer"", ""sent_edges_reducer""]:\n    node_block_opt[key] = node_block_opt.get(key, default_reducer)\n  return node_block_opt\n\n\ndef _make_default_global_block_opt(global_block_opt, default_reducer):\n  """"""Default options to be used in the GlobalBlock of a generic GraphNetwork.""""""\n  global_block_opt = dict(global_block_opt.items()) if global_block_opt else {}\n  for k, v in _DEFAULT_GLOBAL_BLOCK_OPT.items():\n    global_block_opt[k] = global_block_opt.get(k, v)\n  for key in [""edges_reducer"", ""nodes_reducer""]:\n    global_block_opt[key] = global_block_opt.get(key, default_reducer)\n  return global_block_opt\n\n\nclass GraphNetwork(_base.AbstractModule):\n  """"""Implementation of a Graph Network.\n\n  See https://arxiv.org/abs/1806.01261 for more details.\n  """"""\n\n  def __init__(self,\n               edge_model_fn,\n               node_model_fn,\n               global_model_fn,\n               reducer=tf.math.unsorted_segment_sum,\n               edge_block_opt=None,\n               node_block_opt=None,\n               global_block_opt=None,\n               name=""graph_network""):\n    """"""Initializes the GraphNetwork module.\n\n    Args:\n      edge_model_fn: A callable that will be passed to EdgeBlock to perform\n        per-edge computations. The callable must return a Sonnet module (or\n        equivalent; see EdgeBlock for details).\n      node_model_fn: A callable that will be passed to NodeBlock to perform\n        per-node computations. The callable must return a Sonnet module (or\n        equivalent; see NodeBlock for details).\n      global_model_fn: A callable that will be passed to GlobalBlock to perform\n        per-global computations. The callable must return a Sonnet module (or\n        equivalent; see GlobalBlock for details).\n      reducer: Reducer to be used by NodeBlock and GlobalBlock to aggregate\n        nodes and edges. Defaults to tf.math.unsorted_segment_sum. This will be\n        overridden by the reducers specified in `node_block_opt` and\n        `global_block_opt`, if any.\n      edge_block_opt: Additional options to be passed to the EdgeBlock. Can\n        contain keys `use_edges`, `use_receiver_nodes`, `use_sender_nodes`,\n        `use_globals`. By default, these are all True.\n      node_block_opt: Additional options to be passed to the NodeBlock. Can\n        contain the keys `use_received_edges`, `use_nodes`, `use_globals` (all\n        set to True by default), `use_sent_edges` (defaults to False), and\n        `received_edges_reducer`, `sent_edges_reducer` (default to `reducer`).\n      global_block_opt: Additional options to be passed to the GlobalBlock. Can\n        contain the keys `use_edges`, `use_nodes`, `use_globals` (all set to\n        True by default), and `edges_reducer`, `nodes_reducer` (defaults to\n        `reducer`).\n      name: The module name.\n    """"""\n    super(GraphNetwork, self).__init__(name=name)\n    edge_block_opt = _make_default_edge_block_opt(edge_block_opt)\n    node_block_opt = _make_default_node_block_opt(node_block_opt, reducer)\n    global_block_opt = _make_default_global_block_opt(global_block_opt, reducer)\n\n    with self._enter_variable_scope():\n      self._edge_block = blocks.EdgeBlock(\n          edge_model_fn=edge_model_fn, **edge_block_opt)\n      self._node_block = blocks.NodeBlock(\n          node_model_fn=node_model_fn, **node_block_opt)\n      self._global_block = blocks.GlobalBlock(\n          global_model_fn=global_model_fn, **global_block_opt)\n\n  def _build(self, graph):\n    """"""Connects the GraphNetwork.\n\n    Args:\n      graph: A `graphs.GraphsTuple` containing `Tensor`s. Depending on the block\n        options, `graph` may contain `None` fields; but with the default\n        configuration, no `None` field is allowed. Moreover, when using the\n        default configuration, the features of each nodes, edges and globals of\n        `graph` should be concatenable on the last dimension.\n\n    Returns:\n      An output `graphs.GraphsTuple` with updated edges, nodes and globals.\n    """"""\n    return self._global_block(self._node_block(self._edge_block(graph)))\n\n\nclass GraphIndependent(_base.AbstractModule):\n  """"""A graph block that applies models to the graph elements independently.\n\n  The inputs and outputs are graphs. The corresponding models are applied to\n  each element of the graph (edges, nodes and globals) in parallel and\n  independently of the other elements. It can be used to encode or\n  decode the elements of a graph.\n  """"""\n\n  def __init__(self,\n               edge_model_fn=None,\n               node_model_fn=None,\n               global_model_fn=None,\n               name=""graph_independent""):\n    """"""Initializes the GraphIndependent module.\n\n    Args:\n      edge_model_fn: A callable that returns an edge model function. The\n        callable must return a Sonnet module (or equivalent). If passed `None`,\n        will pass through inputs (the default).\n      node_model_fn: A callable that returns a node model function. The callable\n        must return a Sonnet module (or equivalent). If passed `None`, will pass\n        through inputs (the default).\n      global_model_fn: A callable that returns a global model function. The\n        callable must return a Sonnet module (or equivalent). If passed `None`,\n        will pass through inputs (the default).\n      name: The module name.\n    """"""\n    super(GraphIndependent, self).__init__(name=name)\n\n    with self._enter_variable_scope():\n      # The use of snt.Module below is to ensure the ops and variables that\n      # result from the edge/node/global_model_fns are scoped analogous to how\n      # the Edge/Node/GlobalBlock classes do.\n      if edge_model_fn is None:\n        self._edge_model = lambda x: x\n      else:\n        self._edge_model = _base.WrappedModelFnModule(\n            edge_model_fn, name=""edge_model"")\n      if node_model_fn is None:\n        self._node_model = lambda x: x\n      else:\n        self._node_model = _base.WrappedModelFnModule(\n            node_model_fn, name=""node_model"")\n      if global_model_fn is None:\n        self._global_model = lambda x: x\n      else:\n        self._global_model = _base.WrappedModelFnModule(\n            global_model_fn, name=""global_model"")\n\n  def _build(self, graph):\n    """"""Connects the GraphIndependent.\n\n    Args:\n      graph: A `graphs.GraphsTuple` containing non-`None` edges, nodes and\n        globals.\n\n    Returns:\n      An output `graphs.GraphsTuple` with updated edges, nodes and globals.\n\n    """"""\n    return graph.replace(\n        edges=self._edge_model(graph.edges),\n        nodes=self._node_model(graph.nodes),\n        globals=self._global_model(graph.globals))\n\n\nclass DeepSets(_base.AbstractModule):\n  """"""DeepSets module.\n\n  Implementation for the model described in https://arxiv.org/abs/1703.06114\n  (M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. Salakhutdinov, A. Smola).\n  See also PointNet (https://arxiv.org/abs/1612.00593, C. Qi, H. Su, K. Mo,\n  L. J. Guibas) for a related model.\n\n  This module operates on sets, which can be thought of as graphs without\n  edges. The nodes features are first updated based on their value and the\n  globals features, and new globals features are then computed based on the\n  updated nodes features.\n\n  Note that in the original model, only the globals are updated in the returned\n  graph, while this implementation also returns updated nodes.\n  The original model can be reproduced by writing:\n  ```\n  deep_sets = DeepSets()\n  output = deep_sets(input)\n  output = input.replace(globals=output.globals)\n  ```\n\n  This module does not use the edges data or the information contained in the\n  receivers or senders; the output graph has the same value in those fields as\n  the input graph. Those fields can also have `None` values in the input\n  `graphs.GraphsTuple`.\n  """"""\n\n  def __init__(self,\n               node_model_fn,\n               global_model_fn,\n               reducer=tf.math.unsorted_segment_sum,\n               name=""deep_sets""):\n    """"""Initializes the DeepSets module.\n\n    Args:\n      node_model_fn: A callable to be passed to NodeBlock. The callable must\n        return a Sonnet module (or equivalent; see NodeBlock for details). The\n        shape of this module\'s output must equal the shape of the input graph\'s\n        global features, but for the first and last axis.\n      global_model_fn: A callable to be passed to GlobalBlock. The callable must\n        return a Sonnet module (or equivalent; see GlobalBlock for details).\n      reducer: Reduction to be used when aggregating the nodes in the globals.\n        This should be a callable whose signature matches\n        tf.math.unsorted_segment_sum.\n      name: The module name.\n    """"""\n    super(DeepSets, self).__init__(name=name)\n\n    with self._enter_variable_scope():\n      self._node_block = blocks.NodeBlock(\n          node_model_fn=node_model_fn,\n          use_received_edges=False,\n          use_sent_edges=False,\n          use_nodes=True,\n          use_globals=True)\n      self._global_block = blocks.GlobalBlock(\n          global_model_fn=global_model_fn,\n          use_edges=False,\n          use_nodes=True,\n          use_globals=False,\n          nodes_reducer=reducer)\n\n  def _build(self, graph):\n    """"""Connects the DeepSets network.\n\n    Args:\n      graph: A `graphs.GraphsTuple` containing `Tensor`s, whose edges, senders\n        or receivers properties may be `None`. The features of every node and\n        global of `graph` should be concatenable on the last axis (i.e. the\n        shapes of `graph.nodes` and `graph.globals` must match but for their\n        first and last axis).\n\n    Returns:\n      An output `graphs.GraphsTuple` with updated globals.\n    """"""\n    return self._global_block(self._node_block(graph))\n\n\nclass CommNet(_base.AbstractModule):\n  """"""CommNet module.\n\n  Implementation for the model originally described in\n  https://arxiv.org/abs/1605.07736 (S. Sukhbaatar, A. Szlam, R. Fergus), in the\n  version presented in https://arxiv.org/abs/1706.06122 (Y. Hoshen).\n\n  This module internally creates edge features based on the features from the\n  nodes sending to that edge, and independently learns an embedding for each\n  node. It then uses these edges and nodes features to compute updated node\n  features.\n\n  This module does not use the global nor the edges features of the input, but\n  uses its receivers and senders information. The output graph has the same\n  value in edge and global fields as the input graph. The edge and global\n  features fields may have a `None` value in the input `gn_graphs.GraphsTuple`.\n  """"""\n\n  def __init__(self,\n               edge_model_fn,\n               node_encoder_model_fn,\n               node_model_fn,\n               reducer=tf.math.unsorted_segment_sum,\n               name=""comm_net""):\n    """"""Initializes the CommNet module.\n\n    Args:\n      edge_model_fn: A callable to be passed to EdgeBlock. The callable must\n        return a Sonnet module (or equivalent; see EdgeBlock for details).\n      node_encoder_model_fn: A callable to be passed to the NodeBlock\n        responsible for the first encoding of the nodes. The callable must\n        return a Sonnet module (or equivalent; see NodeBlock for details). The\n        shape of this module\'s output should match the shape of the module built\n        by `edge_model_fn`, but for the first and last dimension.\n      node_model_fn: A callable to be passed to NodeBlock. The callable must\n        return a Sonnet module (or equivalent; see NodeBlock for details).\n      reducer: Reduction to be used when aggregating the edges in the nodes.\n        This should be a callable whose signature matches\n        tf.math.unsorted_segment_sum.\n      name: The module name.\n    """"""\n    super(CommNet, self).__init__(name=name)\n\n    with self._enter_variable_scope():\n      # Computes $\\Psi_{com}(x_j)$ in Eq. (2) of 1706.06122\n      self._edge_block = blocks.EdgeBlock(\n          edge_model_fn=edge_model_fn,\n          use_edges=False,\n          use_receiver_nodes=False,\n          use_sender_nodes=True,\n          use_globals=False)\n      # Computes $\\Phi(x_i)$ in Eq. (2) of 1706.06122\n      self._node_encoder_block = blocks.NodeBlock(\n          node_model_fn=node_encoder_model_fn,\n          use_received_edges=False,\n          use_sent_edges=False,\n          use_nodes=True,\n          use_globals=False,\n          received_edges_reducer=reducer,\n          name=""node_encoder_block"")\n      # Computes $\\Theta(..)$ in Eq.(2) of 1706.06122\n      self._node_block = blocks.NodeBlock(\n          node_model_fn=node_model_fn,\n          use_received_edges=True,\n          use_sent_edges=False,\n          use_nodes=True,\n          use_globals=False,\n          received_edges_reducer=reducer)\n\n  def _build(self, graph):\n    """"""Connects the CommNet network.\n\n    Args:\n      graph: A `graphs.GraphsTuple` containing `Tensor`s, with non-`None` nodes,\n        receivers and senders.\n\n    Returns:\n      An output `graphs.GraphsTuple` with updated nodes.\n\n    Raises:\n      ValueError: if any of `graph.nodes`, `graph.receivers` or `graph.senders`\n      is `None`.\n    """"""\n    node_input = self._node_encoder_block(self._edge_block(graph))\n    return graph.replace(nodes=self._node_block(node_input).nodes)\n\n\ndef _unsorted_segment_softmax(data,\n                              segment_ids,\n                              num_segments,\n                              name=""unsorted_segment_softmax""):\n  """"""Performs an elementwise softmax operation along segments of a tensor.\n\n  The input parameters are analogous to `tf.math.unsorted_segment_sum`. It\n  produces an output of the same shape as the input data, after performing an\n  elementwise sofmax operation between all of the rows with common segment id.\n\n  Args:\n    data: A tensor with at least one dimension.\n    segment_ids: A tensor of indices segmenting `data` across the first\n      dimension.\n    num_segments: A scalar tensor indicating the number of segments. It should\n      be at least `max(segment_ids) + 1`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A tensor with the same shape as `data` after applying the softmax operation.\n\n  """"""\n  with tf.name_scope(name):\n    segment_maxes = tf.math.unsorted_segment_max(data, segment_ids,\n                                                 num_segments)\n    maxes = tf.gather(segment_maxes, segment_ids)\n    # Possibly refactor to `tf.stop_gradient(maxes)` for better performance.\n    data -= maxes\n    exp_data = tf.exp(data)\n    segment_sum_exp_data = tf.math.unsorted_segment_sum(exp_data, segment_ids,\n                                                        num_segments)\n    sum_exp_data = tf.gather(segment_sum_exp_data, segment_ids)\n    return exp_data / sum_exp_data\n\n\ndef _received_edges_normalizer(graph,\n                               normalizer,\n                               name=""received_edges_normalizer""):\n  """"""Performs elementwise normalization for all received edges by a given node.\n\n  Args:\n    graph: A graph containing edge information.\n    normalizer: A normalizer function following the signature of\n      `modules._unsorted_segment_softmax`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A tensor with the resulting normalized edges.\n\n  """"""\n  with tf.name_scope(name):\n    return normalizer(\n        data=graph.edges,\n        segment_ids=graph.receivers,\n        num_segments=tf.reduce_sum(graph.n_node))\n\n\nclass SelfAttention(_base.AbstractModule):\n  """"""Multi-head self-attention module.\n\n  The module is based on the following three papers:\n   * A simple neural network module for relational reasoning (RNs):\n       https://arxiv.org/abs/1706.01427\n   * Non-local Neural Networks: https://arxiv.org/abs/1711.07971.\n   * Attention Is All You Need (AIAYN): https://arxiv.org/abs/1706.03762.\n\n  The input to the modules consists of a graph containing values for each node\n  and connectivity between them, a tensor containing keys for each node\n  and a tensor containing queries for each node.\n\n  The self-attention step consist of updating the node values, with each new\n  node value computed in a two step process:\n  - Computing the attention weights between each node and all of its senders\n   nodes, by calculating sum(sender_key*receiver_query) and using the softmax\n   operation on all attention weights for each node.\n  - For each receiver node, compute the new node value as the weighted average\n   of the values of the sender nodes, according to the attention weights.\n  - Nodes with no received edges, get an updated value of 0.\n\n  Values, keys and queries contain a ""head"" axis to compute independent\n  self-attention for each of the heads.\n\n  """"""\n\n  def __init__(self, name=""self_attention""):\n    """"""Inits the module.\n\n    Args:\n      name: The module name.\n    """"""\n    super(SelfAttention, self).__init__(name=name)\n    self._normalizer = _unsorted_segment_softmax\n\n  def _build(self, node_values, node_keys, node_queries, attention_graph):\n    """"""Connects the multi-head self-attention module.\n\n    The self-attention is only computed according to the connectivity of the\n    input graphs, with receiver nodes attending to sender nodes.\n\n    Args:\n      node_values: Tensor containing the values associated to each of the nodes.\n        The expected shape is [total_num_nodes, num_heads, key_size].\n      node_keys: Tensor containing the key associated to each of the nodes. The\n        expected shape is [total_num_nodes, num_heads, key_size].\n      node_queries: Tensor containing the query associated to each of the nodes.\n        The expected shape is [total_num_nodes, num_heads, query_size]. The\n        query size must be equal to the key size.\n      attention_graph: Graph containing connectivity information between nodes\n        via the senders and receivers fields. Node A will only attempt to attend\n        to Node B if `attention_graph` contains an edge sent by Node A and\n        received by Node B.\n\n    Returns:\n      An output `graphs.GraphsTuple` with updated nodes containing the\n      aggregated attended value for each of the nodes with shape\n      [total_num_nodes, num_heads, value_size].\n\n    Raises:\n      ValueError: if the input graph does not have edges.\n    """"""\n\n    # Sender nodes put their keys and values in the edges.\n    # [total_num_edges, num_heads, query_size]\n    sender_keys = blocks.broadcast_sender_nodes_to_edges(\n        attention_graph.replace(nodes=node_keys))\n    # [total_num_edges, num_heads, value_size]\n    sender_values = blocks.broadcast_sender_nodes_to_edges(\n        attention_graph.replace(nodes=node_values))\n\n    # Receiver nodes put their queries in the edges.\n    # [total_num_edges, num_heads, key_size]\n    receiver_queries = blocks.broadcast_receiver_nodes_to_edges(\n        attention_graph.replace(nodes=node_queries))\n\n    # Attention weight for each edge.\n    # [total_num_edges, num_heads]\n    attention_weights_logits = tf.reduce_sum(\n        sender_keys * receiver_queries, axis=-1)\n    normalized_attention_weights = _received_edges_normalizer(\n        attention_graph.replace(edges=attention_weights_logits),\n        normalizer=self._normalizer)\n\n    # Attending to sender values according to the weights.\n    # [total_num_edges, num_heads, embedding_size]\n    attented_edges = sender_values * normalized_attention_weights[..., None]\n\n    # Summing all of the attended values from each node.\n    # [total_num_nodes, num_heads, embedding_size]\n    received_edges_aggregator = blocks.ReceivedEdgesToNodesAggregator(\n        reducer=tf.math.unsorted_segment_sum)\n    aggregated_attended_values = received_edges_aggregator(\n        attention_graph.replace(edges=attented_edges))\n\n    return attention_graph.replace(nodes=aggregated_attended_values)\n'"
graph_nets/utils_np.py,1,"b'# Copyright 2018 The GraphNets Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Auxiliary methods that operate on graph structured data.\n\nThis modules contains functions to convert between python data structures\nrepresenting graphs and `graphs.GraphsTuple` containing numpy arrays.\nIn particular:\n\n  - `networkx_to_data_dict` and `data_dict_to_networkx` convert from/to an\n    instance of `networkx.OrderedMultiDiGraph` from/to a data dictionary;\n\n  - `networkxs_to_graphs_tuple` and `graphs_tuple_to_networkxs` convert\n    from instances of `networkx.OrderedMultiDiGraph` to `graphs.GraphsTuple`;\n\n  - `data_dicts_to_graphs_tuple` and `graphs_tuple_to_data_dicts` convert to and\n    from lists of data dictionaries and `graphs.GraphsTuple`;\n\n  - `get_graph` allows to index or slice a `graphs.GraphsTuple` to extract a\n    subgraph or a subbatch of graphs.\n\nThe functions in these modules are able to deal with graphs containing `None`\nfields (e.g. featureless nodes, featureless edges, or no edges).\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nfrom graph_nets import graphs\nimport networkx as nx\nimport numpy as np\nfrom six.moves import range\nfrom six.moves import zip  # pylint: disable=redefined-builtin\n\nNODES = graphs.NODES\nEDGES = graphs.EDGES\nGLOBALS = graphs.GLOBALS\nRECEIVERS = graphs.RECEIVERS\nSENDERS = graphs.SENDERS\nGLOBALS = graphs.GLOBALS\nN_NODE = graphs.N_NODE\nN_EDGE = graphs.N_EDGE\n\nGRAPH_DATA_FIELDS = graphs.GRAPH_DATA_FIELDS\nGRAPH_NUMBER_FIELDS = graphs.GRAPH_NUMBER_FIELDS\nALL_FIELDS = graphs.ALL_FIELDS\n\nGRAPH_NX_FEATURES_KEY = ""features""\n\n\ndef _check_valid_keys(keys):\n  if any([x in keys for x in [EDGES, RECEIVERS, SENDERS]]):\n    if not (RECEIVERS in keys and SENDERS in keys):\n      raise ValueError(""If edges are present, senders and receivers should ""\n                       ""both be defined."")\n\n\ndef _defined_keys(dict_):\n  return {k for k, v in dict_.items() if v is not None}\n\n\ndef _check_valid_sets_of_keys(dicts):\n  """"""Checks that all dictionaries have exactly the same valid key sets.""""""\n  prev_keys = None\n  for dict_ in dicts:\n    current_keys = _defined_keys(dict_)\n    _check_valid_keys(current_keys)\n    if prev_keys and current_keys != prev_keys:\n      raise ValueError(\n          ""Different set of keys found when iterating over data dictionaries ""\n          ""({} vs {})"".format(prev_keys, current_keys))\n    prev_keys = current_keys\n\n\ndef _compute_stacked_offsets(sizes, repeats):\n  """"""Computes offsets to add to indices of stacked np arrays.\n\n  When a set of np arrays are stacked, the indices of those from the second on\n  must be offset in order to be able to index into the stacked np array. This\n  computes those offsets.\n\n  Args:\n    sizes: A 1D sequence of np arrays of the sizes per graph.\n    repeats: A 1D sequence of np arrays of the number of repeats per graph.\n\n  Returns:\n    The index offset per graph.\n  """"""\n  return np.repeat(np.cumsum(np.hstack([0, sizes[:-1]])), repeats)\n\n\ndef _check_key(node_index, key):\n  if node_index != key:\n    raise ValueError(\n        ""Nodes of the networkx.OrderedMultiDiGraph must have sequential ""\n        ""integer keys consistent with the order of the nodes (e.g. ""\n        ""`list(graph_nx.nodes)[i] == i`), found node with index {} and key {}""\n        .format(node_index, key))\n\n  return True\n\n\ndef networkx_to_data_dict(graph_nx,\n                          node_shape_hint=None,\n                          edge_shape_hint=None,\n                          data_type_hint=np.float32):\n  """"""Returns a data dict of Numpy data from a networkx graph.\n\n  The networkx graph should be set up such that, for fixed shapes `node_shape`,\n   `edge_shape` and `global_shape`:\n    - `graph_nx.nodes(data=True)[i][-1][""features""]` is, for any node index i, a\n      tensor of shape `node_shape`, or `None`;\n    - `graph_nx.edges(data=True)[i][-1][""features""]` is, for any edge index i, a\n      tensor of shape `edge_shape`, or `None`;\n    - `graph_nx.edges(data=True)[i][-1][""index""]`, if present, defines the order\n      in which the edges will be sorted in the resulting `data_dict`;\n    - `graph_nx.graph[""features""] is a tensor of shape `global_shape`, or\n      `None`.\n\n  The dictionary `type_hints` can provide hints of the ""float"" and ""int"" types\n  for missing values.\n\n  The output data is a sequence of data dicts with fields:\n    NODES, EDGES, RECEIVERS, SENDERS, GLOBALS, N_NODE, N_EDGE.\n\n  Args:\n    graph_nx: A `networkx.OrderedMultiDiGraph`. The node keys must be sequential\n      integer values following the order in which nodes are added to the graph\n      starting from zero. That is `list(graph_nx.nodes)[i] == i`.\n    node_shape_hint: (iterable of `int` or `None`, default=`None`) If the graph\n      does not contain nodes, the trailing shape for the created `NODES` field.\n      If `None` (the default), this field is left `None`. This is not used if\n      `graph_nx` contains at least one node.\n    edge_shape_hint: (iterable of `int` or `None`, default=`None`) If the graph\n      does not contain edges, the trailing shape for the created `EDGES` field.\n      If `None` (the default), this field is left `None`. This is not used if\n      `graph_nx` contains at least one edge.\n    data_type_hint: (numpy dtype, default=`np.float32`) If the `NODES` or\n      `EDGES` fields are autocompleted, their type.\n\n  Returns:\n    The data `dict` of Numpy data.\n\n  Raises:\n    TypeError: If `graph_nx` is not an instance of networkx.\n    KeyError: If `graph_nx` contains at least one node without the ""features""\n      key in its attribute dictionary, or at least one edge without the\n      ""features"" key in its attribute dictionary.\n    ValueError: If `graph_nx` contains at least one node with a `None`\n      ""features"" attribute and one least one node with a non-`None` ""features""\n      attribute; or if `graph_nx` contains at least one edge with a `None`\n      ""features"" attribute and one least one edge with a non-`None` ""features""\n      attribute.\n    ValueError: If the nodes have keys that are not consistent with the order\n      of the nodes.\n  """"""\n  nodes = None\n  try:\n    number_of_nodes = graph_nx.number_of_nodes()\n  except ValueError:\n    raise TypeError(""Argument `graph_nx` of wrong type {}"".format(\n        type(graph_nx)))\n  if number_of_nodes == 0:\n    if node_shape_hint is not None:\n      nodes = np.zeros([0] + list(node_shape_hint), dtype=data_type_hint)\n  else:\n    try:\n      nodes_data = [\n          data[GRAPH_NX_FEATURES_KEY]\n          for node_i, (key, data) in enumerate(graph_nx.nodes(data=True))\n          if _check_key(node_i, key) and data[GRAPH_NX_FEATURES_KEY] is not None\n      ]\n      if nodes_data:\n        if len(nodes_data) != number_of_nodes:\n          raise ValueError(\n              ""Either all the nodes should have features, or none of them"")\n        nodes = np.array(nodes_data)\n    except KeyError:\n      raise KeyError(""Missing \'features\' field from the graph nodes. ""\n                     ""This could be due to the node having been silently added ""\n                     ""as a consequence of an edge addition when creating the ""\n                     ""networkx instance"")\n\n  edges = None\n  number_of_edges = graph_nx.number_of_edges()\n  if number_of_edges == 0:\n    senders = np.zeros(0, dtype=np.int32)\n    receivers = np.zeros(0, dtype=np.int32)\n    if edge_shape_hint is not None:\n      edges = np.zeros([0] + list(edge_shape_hint), dtype=data_type_hint)\n  else:\n    if ""index"" in list(graph_nx.edges(data=True))[0][2]:\n      senders, receivers, edge_attr_dicts = zip(\n          *sorted(graph_nx.edges(data=True), key=lambda x: x[2][""index""]))\n    else:\n      senders, receivers, edge_attr_dicts = zip(*graph_nx.edges(data=True))\n    senders = np.array(senders, dtype=np.int32)\n    receivers = np.array(receivers, dtype=np.int32)\n    edges_data = [\n        x[GRAPH_NX_FEATURES_KEY]\n        for x in edge_attr_dicts\n        if x[GRAPH_NX_FEATURES_KEY] is not None\n    ]\n    if edges_data:\n      if len(edges_data) != number_of_edges:\n        raise ValueError(\n            ""Either all the edges should have features, or none of them"")\n      edges = np.array(edges_data)\n\n  globals_ = None\n  if GRAPH_NX_FEATURES_KEY in graph_nx.graph:\n    globals_ = graph_nx.graph[GRAPH_NX_FEATURES_KEY]\n\n  return {\n      NODES: nodes,\n      EDGES: edges,\n      RECEIVERS: receivers,\n      SENDERS: senders,\n      GLOBALS: globals_,\n      N_NODE: number_of_nodes,\n      N_EDGE: number_of_edges,\n  }\n\n\ndef _unstack(array):\n  """"""Similar to `tf.unstack`.""""""\n  num_splits = int(array.shape[0])\n  return [np.squeeze(x, 0) for x in np.split(array, num_splits, axis=0)]\n\n\ndef data_dict_to_networkx(data_dict):\n  """"""Returns a networkx graph that contains the stored data.\n\n  Depending on the content of `data_dict`, the returned `networkx` instance has\n  the following properties:\n\n  - The nodes feature are placed in the nodes attribute dictionary under the\n    ""features"" key. If the `NODES` fields is `None`, a `None` value is placed\n    here;\n\n  - If the `RECEIVERS` field is `None`, no edges are added to the graph.\n    Otherwise, edges are added with the order in which they appeared in\n    `data_dict` stored in the ""index"" field of their attributes dictionary;\n\n  - The edges features are placed in the edges attribute dictionary under the\n    ""features"" key. If the `EDGES` field is `None`, a `None` value is placed;\n\n  - The global feature are placed under the key ""features"" of the graph\n    property of the returned instance. If the `GLOBALS` field is `None`, a\n    `None` global property is created.\n\n  Args:\n    data_dict: A graph `dict` of Numpy data.\n\n  Returns:\n    The `networkx.OrderedMultiDiGraph`. The node keys will be the data_dict\n    integer node indices.\n\n  Raises:\n    ValueError: If the `NODES` field of `data_dict` contains `None`, and\n      `data_dict` does not have a `N_NODE` field.\n  """"""\n  graph_nx = nx.OrderedMultiDiGraph()\n  data_dict = _populate_number_fields(data_dict)\n  graph_nx.graph[GRAPH_NX_FEATURES_KEY] = data_dict[GLOBALS]\n\n  if data_dict[NODES] is not None:\n    if data_dict[NODES].shape[0] > 0:\n      nodes_list = _unstack(data_dict[NODES])\n      for i, x in enumerate(nodes_list):\n        graph_nx.add_node(i, **{GRAPH_NX_FEATURES_KEY: x})\n  elif data_dict[N_NODE] is not None:\n    for i in range(data_dict[N_NODE]):\n      graph_nx.add_node(i, **{GRAPH_NX_FEATURES_KEY: None})\n  else:\n    raise ValueError(""Cannot create a graph with unspecified number of nodes"")\n\n  if data_dict[EDGES] is not None and data_dict[EDGES].shape[0] > 0:\n    edges_features = [{  # pylint: disable=g-complex-comprehension\n        ""index"": i,\n        GRAPH_NX_FEATURES_KEY: x\n    } for i, x in enumerate(_unstack(data_dict[EDGES]))]\n    edges_data = zip(data_dict[SENDERS], data_dict[RECEIVERS], edges_features)\n    graph_nx.add_edges_from(edges_data)\n  elif data_dict[RECEIVERS] is not None and data_dict[RECEIVERS].shape[0] > 0:\n    edges_features = [{  # pylint: disable=g-complex-comprehension\n        ""index"": i,\n        GRAPH_NX_FEATURES_KEY: None\n    } for i in range(data_dict[RECEIVERS].shape[0])]\n    edges_data = zip(data_dict[SENDERS], data_dict[RECEIVERS], edges_features)\n    graph_nx.add_edges_from(edges_data)\n\n  return graph_nx\n\n\ndef networkxs_to_graphs_tuple(graph_nxs,\n                              node_shape_hint=None,\n                              edge_shape_hint=None,\n                              data_type_hint=np.float32):\n  """"""Constructs an instance from an iterable of networkx graphs.\n\n   The networkx graph should be set up such that, for fixed shapes `node_shape`,\n   `edge_shape` and `global_shape`:\n    - `graph_nx.nodes(data=True)[i][-1][""features""]` is, for any node index i, a\n      tensor of shape `node_shape`, or `None`;\n    - `graph_nx.edges(data=True)[i][-1][""features""]` is, for any edge index i, a\n      tensor of shape `edge_shape`, or `None`;\n    - `graph_nx.edges(data=True)[i][-1][""index""]`, if present, defines the order\n      in which the edges will be sorted in the resulting `data_dict`;\n    - `graph_nx.graph[""features""] is a tensor of shape `global_shape`, or\n      `None`.\n\n  The output data is a sequence of data dicts with fields:\n    NODES, EDGES, RECEIVERS, SENDERS, GLOBALS, N_NODE, N_EDGE.\n\n  Args:\n    graph_nxs: A container of `networkx.OrderedMultiDiGraph`s. The node keys\n      must be sequential integer values following the order in which nodes are\n      added to the graph starting from zero. That is\n      `list(graph_nx.nodes)[i] == i`.\n    node_shape_hint: (iterable of `int` or `None`, default=`None`) If the graph\n      does not contain nodes, the trailing shape for the created `NODES` field.\n      If `None` (the default), this field is left `None`. This is not used if\n      `graph_nx` contains at least one node.\n    edge_shape_hint: (iterable of `int` or `None`, default=`None`) If the graph\n      does not contain edges, the trailing shape for the created `EDGES` field.\n      If `None` (the default), this field is left `None`. This is not used if\n      `graph_nx` contains at least one edge.\n    data_type_hint: (numpy dtype, default=`np.float32`) If the `NODES` or\n      `EDGES` fields are autocompleted, their type.\n\n  Returns:\n    The instance.\n\n  Raises:\n    ValueError: If `graph_nxs` is not an iterable of networkx instances.\n  """"""\n  data_dicts = []\n  try:\n    for graph_nx in graph_nxs:\n      data_dict = networkx_to_data_dict(graph_nx, node_shape_hint,\n                                        edge_shape_hint, data_type_hint)\n      data_dicts.append(data_dict)\n  except TypeError:\n    raise ValueError(""Could not convert some elements of `graph_nxs`. ""\n                     ""Did you pass an iterable of networkx instances?"")\n\n  return data_dicts_to_graphs_tuple(data_dicts)\n\n\ndef graphs_tuple_to_networkxs(graphs_tuple):\n  """"""Converts a `graphs.GraphsTuple` to a sequence of networkx graphs.\n\n  Args:\n    graphs_tuple: A `graphs.GraphsTuple` instance containing numpy arrays.\n\n  Returns:\n    The list of `networkx.OrderedMultiDiGraph`s. The node keys will be the data\n    dict integer node indices.\n  """"""\n  return [\n      data_dict_to_networkx(x) for x in graphs_tuple_to_data_dicts(graphs_tuple)\n  ]\n\n\ndef data_dicts_to_graphs_tuple(data_dicts):\n  """"""Constructs a `graphs.GraphsTuple` from an iterable of data dicts.\n\n  The graphs represented by the `data_dicts` argument are batched to form a\n  single instance of `graphs.GraphsTuple` containing numpy arrays.\n\n  Args:\n    data_dicts: An iterable of dictionaries with keys `GRAPH_DATA_FIELDS`, plus,\n      potentially, a subset of `GRAPH_NUMBER_FIELDS`. The NODES and EDGES fields\n      should be numpy arrays of rank at least 2, while the RECEIVERS, SENDERS\n      are numpy arrays of rank 1 and same dimension as the EDGES field first\n      dimension. The GLOBALS field is a numpy array of rank at least 1.\n\n  Returns:\n    An instance of `graphs.GraphsTuple` containing numpy arrays. The\n    `RECEIVERS`, `SENDERS`, `N_NODE` and `N_EDGE` fields are cast to `np.int32`\n    type.\n  """"""\n  data_dicts = [dict(d) for d in data_dicts]\n  for key in graphs.GRAPH_DATA_FIELDS:\n    for data_dict in data_dicts:\n      data_dict.setdefault(key, None)\n  _check_valid_sets_of_keys(data_dicts)\n  data_dicts = _to_compatible_data_dicts(data_dicts)\n  return graphs.GraphsTuple(**_concatenate_data_dicts(data_dicts))\n\n\ndef graphs_tuple_to_data_dicts(graph):\n  """"""Splits the stored data into a list of individual data dicts.\n\n  Each list is a dictionary with fields NODES, EDGES, GLOBALS, RECEIVERS,\n  SENDERS.\n\n  Args:\n    graph: A `graphs.GraphsTuple` instance containing numpy arrays.\n\n  Returns:\n    A list of the graph data dictionaries. The GLOBALS field is a tensor of\n      rank at least 1, as the RECEIVERS and SENDERS field (which have integer\n      values). The NODES and EDGES fields have rank at least 2.\n  """"""\n  offset = _compute_stacked_offsets(graph.n_node, graph.n_edge)\n\n  nodes_splits = np.cumsum(graph.n_node[:-1])\n  edges_splits = np.cumsum(graph.n_edge[:-1])\n  graph_of_lists = collections.defaultdict(lambda: [])\n  if graph.nodes is not None:\n    graph_of_lists[NODES] = np.split(graph.nodes, nodes_splits)\n  if graph.edges is not None:\n    graph_of_lists[EDGES] = np.split(graph.edges, edges_splits)\n  if graph.receivers is not None:\n    graph_of_lists[RECEIVERS] = np.split(graph.receivers - offset, edges_splits)\n    graph_of_lists[SENDERS] = np.split(graph.senders - offset, edges_splits)\n  if graph.globals is not None:\n    graph_of_lists[GLOBALS] = _unstack(graph.globals)\n\n  n_graphs = graph.n_node.shape[0]\n  # Make all fields the same length.\n  for k in GRAPH_DATA_FIELDS:\n    graph_of_lists[k] += [None] * (n_graphs - len(graph_of_lists[k]))\n  graph_of_lists[N_NODE] = graph.n_node\n  graph_of_lists[N_EDGE] = graph.n_edge\n\n  result = []\n  for index in range(n_graphs):\n    result.append({field: graph_of_lists[field][index] for field in ALL_FIELDS})\n  return result\n\n\ndef _to_compatible_data_dicts(data_dicts):\n  """"""Converts the content of `data_dicts` to arrays of the right type.\n\n  All fields are converted to numpy arrays. The index fields (`SENDERS` and\n  `RECEIVERS`) and number fields (`N_NODE`, `N_EDGE`) are cast to `np.int32`.\n\n  Args:\n    data_dicts: An iterable of dictionaries with keys `ALL_KEYS` and values\n      either `None`s, or quantities that can be converted to numpy arrays.\n\n  Returns:\n    A list of dictionaries containing numpy arrays or `None`s.\n  """"""\n  results = []\n  for data_dict in data_dicts:\n    result = {}\n    for k, v in data_dict.items():\n      if v is None:\n        result[k] = None\n      else:\n        dtype = np.int32 if k in [SENDERS, RECEIVERS, N_NODE, N_EDGE] else None\n        result[k] = np.asarray(v, dtype)\n    results.append(result)\n  return results\n\n\ndef _populate_number_fields(data_dict):\n  """"""Returns a dict with the number fields N_NODE, N_EDGE filled in.\n\n  The N_NODE field is filled if the graph contains a non-None NODES field;\n  otherwise, it is set to 0.\n  The N_EDGE field is filled if the graph contains a non-None RECEIVERS field;\n  otherwise, it is set to 0.\n\n  Args:\n    data_dict: An input `dict`.\n\n  Returns:\n    The data `dict` with number fields.\n  """"""\n  dct = data_dict.copy()\n  for number_field, data_field in [[N_NODE, NODES], [N_EDGE, RECEIVERS]]:\n    if dct.get(number_field) is None:\n      if dct[data_field] is not None:\n        dct[number_field] = np.array(\n            np.shape(dct[data_field])[0], dtype=np.int32)\n      else:\n        dct[number_field] = np.array(0, dtype=np.int32)\n  return dct\n\n\ndef _concatenate_data_dicts(data_dicts):\n  """"""Concatenate a list of data dicts to create the equivalent batched graph.\n\n  Args:\n    data_dicts: An iterable of data dictionaries with keys `GRAPH_DATA_FIELDS`,\n      plus, potentially, a subset of `GRAPH_NUMBER_FIELDS`. Each dictionary is\n      representing a single graph.\n\n  Returns:\n    A data dictionary with the keys `GRAPH_DATA_FIELDS + GRAPH_NUMBER_FIELDS`,\n    representing the concatenated graphs.\n  """"""\n  # Create a single dict with fields that contain sequences of graph tensors.\n  concatenated_dicts = collections.defaultdict(lambda: [])\n  for data_dict in data_dicts:\n    data_dict = _populate_number_fields(data_dict)\n    for k, v in data_dict.items():\n      if v is not None:\n        concatenated_dicts[k].append(v)\n      else:\n        concatenated_dicts[k] = None\n\n  concatenated_dicts = dict(concatenated_dicts)\n\n  for field, arrays in concatenated_dicts.items():\n    if arrays is None:\n      concatenated_dicts[field] = None\n    elif field in list(GRAPH_NUMBER_FIELDS) + [GLOBALS]:\n      concatenated_dicts[field] = np.stack(arrays)\n    else:\n      concatenated_dicts[field] = np.concatenate(arrays, axis=0)\n\n  if concatenated_dicts[RECEIVERS] is not None:\n    offset = _compute_stacked_offsets(concatenated_dicts[N_NODE],\n                                      concatenated_dicts[N_EDGE])\n    for field in (RECEIVERS, SENDERS):\n      concatenated_dicts[field] += offset\n\n  return concatenated_dicts\n\n\ndef get_graph(input_graphs, index):\n  """"""Indexes into a graph.\n\n  Given a `graphs.GraphsTuple` containing arrays and an index (either\n  an `int` or a `slice`), index into the nodes, edges and globals to extract the\n  graphs specified by the slice, and returns them into an another instance of a\n  `graphs.GraphsTuple` containing `Tensor`s.\n\n  Args:\n    input_graphs: A `graphs.GraphsTuple` containing numpy arrays.\n    index: An `int` or a `slice`, to index into `graph`. `index` should be\n      compatible with the number of graphs in `graphs`.\n\n  Returns:\n    A `graphs.GraphsTuple` containing numpy arrays, made of the extracted\n      graph(s).\n\n  Raises:\n    TypeError: if `index` is not an `int` or a `slice`.\n  """"""\n  if isinstance(index, int):\n    graph_slice = slice(index, index + 1)\n  elif isinstance(index, slice):\n    graph_slice = index\n  else:\n    raise TypeError(""unsupported type: %s"" % type(index))\n  data_dicts = graphs_tuple_to_data_dicts(input_graphs)[graph_slice]\n  return graphs.GraphsTuple(**_concatenate_data_dicts(data_dicts))\n'"
graph_nets/utils_tf.py,114,"b'# Copyright 2018 The GraphNets Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n\n""""""Tensorflow ops and helpers useful to manipulate graphs.\n\nThis module contains utility functions to operate with `Tensor`s representations\nof graphs, in particular:\n\n  - `build_placeholders_from_data_dicts` and `build_placeholders_from_networkx`\n     create placeholder structures to represent graphs;\n\n  - `get_feed_dict` allow to create a `feed_dict` from a `graphs.GraphsTuple`\n    containing numpy arrays and potentially, `None` values;\n\n  - `data_dicts_to_graphs_tuple` converts between data dictionaries and\n    `graphs.GraphsTuple`;\n\n  - `fully_connect_graph_static` (resp. `fully_connect_graph_dynamic`) adds\n    edges to a `graphs.GraphsTuple` in a fully-connected manner, in the case\n    where the number of nodes per graph is known at graph construction time and\n    is the same for all graphs (resp. only known at runtime and may depend on\n    the graph);\n\n  - `set_zero_node_features`, `set_zero_edge_features` and\n    `set_zero_global_features` complete a `graphs.GraphsTuple` with a `Tensor`\n    of zeros for the nodes, edges and globals;\n\n  - `concat` batches `graphs.GraphsTuple` together (when using `axis=0`), or\n    concatenates them along their data dimension;\n\n  - `repeat` is a utility convenient to broadcast globals to edges or nodes of\n    a graph;\n\n  - `get_graph` indexes or slices a `graphs.GraphsTuple` to extract a subgraph\n    or a subbatch of graphs;\n\n  - `stop_gradients` stops the gradients flowing through a graph;\n\n  - `identity` applies a `tf.identity` to every field of a graph;\n\n  - `make_runnable_in_session` allows to run a graph containing `None` fields\n    through a Tensorflow session.\n\nThe functions in these modules are able to deal with graphs containing `None`\nfields (e.g. featureless nodes, featureless edges, or no edges).\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport functools\n\nfrom graph_nets import graphs\nfrom graph_nets import utils_np\nimport six\nfrom six.moves import range\nimport tensorflow as tf\nimport tree\n\n\nNODES = graphs.NODES\nEDGES = graphs.EDGES\nRECEIVERS = graphs.RECEIVERS\nSENDERS = graphs.SENDERS\nGLOBALS = graphs.GLOBALS\nN_NODE = graphs.N_NODE\nN_EDGE = graphs.N_EDGE\n\nGRAPH_DATA_FIELDS = graphs.GRAPH_DATA_FIELDS\nGRAPH_NUMBER_FIELDS = graphs.GRAPH_NUMBER_FIELDS\nALL_FIELDS = graphs.ALL_FIELDS\n\n\ndef _get_shape(tensor):\n  """"""Returns the tensor\'s shape.\n\n   Each shape element is either:\n   - an `int`, when static shape values are available, or\n   - a `tf.Tensor`, when the shape is dynamic.\n\n  Args:\n    tensor: A `tf.Tensor` to get the shape of.\n\n  Returns:\n    The `list` which contains the tensor\'s shape.\n  """"""\n\n  shape_list = tensor.shape.as_list()\n  if all(s is not None for s in shape_list):\n    return shape_list\n  shape_tensor = tf.shape(tensor)\n  return [shape_tensor[i] if s is None else s for i, s in enumerate(shape_list)]\n\n\ndef _build_placeholders_from_specs(dtypes,\n                                   shapes,\n                                   force_dynamic_num_graphs=True):\n  """"""Creates a `graphs.GraphsTuple` of placeholders with `dtypes` and `shapes`.\n\n  The dtypes and shapes arguments are instances of `graphs.GraphsTuple` that\n  contain dtypes and shapes, or `None` values for the fields for which no\n  placeholder should be created. The leading dimension the nodes and edges are\n  dynamic because the numbers of nodes and edges can vary.\n  If `force_dynamic_num_graphs` is True, then the number of graphs is assumed to\n  be dynamic and all fields leading dimensions are set to `None`.\n  If `force_dynamic_num_graphs` is False, then `N_NODE`, `N_EDGE` and `GLOBALS`\n  leading dimensions are statically defined.\n\n  Args:\n    dtypes: A `graphs.GraphsTuple` that contains `tf.dtype`s or `None`s.\n    shapes: A `graphs.GraphsTuple` that contains `list`s of integers,\n      `tf.TensorShape`s, or `None`s.\n    force_dynamic_num_graphs: A `bool` that forces the batch dimension to be\n      dynamic. Defaults to `True`.\n\n  Returns:\n    A `graphs.GraphsTuple` containing placeholders.\n\n  Raises:\n    ValueError: The `None` fields in `dtypes` and `shapes` do not match.\n  """"""\n  dct = {}\n  for field in ALL_FIELDS:\n    dtype = getattr(dtypes, field)\n    shape = getattr(shapes, field)\n    if dtype is None or shape is None:\n      if not (shape is None and dtype is None):\n        raise ValueError(\n            ""only one of dtype and shape are None for field {}"".format(field))\n      dct[field] = None\n    elif not shape:\n      raise ValueError(""Shapes must have at least rank 1"")\n    else:\n      shape = list(shape)\n      if field not in [N_NODE, N_EDGE, GLOBALS] or force_dynamic_num_graphs:\n        shape[0] = None\n\n      dct[field] = tf.placeholder(dtype, shape=shape, name=field)\n\n  return graphs.GraphsTuple(**dct)\n\n\ndef _placeholders_from_graphs_tuple(graph, force_dynamic_num_graphs=True):\n  """"""Creates a `graphs.GraphsTuple` of placeholders that matches a numpy graph.\n\n  Args:\n    graph: A `graphs.GraphsTuple` that contains numpy data.\n    force_dynamic_num_graphs: A `bool` that forces the batch dimension to be\n      dynamic. Defaults to `True`.\n\n  Returns:\n    A `graphs.GraphsTuple` containing placeholders.\n  """"""\n  graph_dtypes = graph.map(\n      lambda v: tf.as_dtype(v.dtype) if v is not None else None, ALL_FIELDS)\n  graph_shapes = graph.map(lambda v: list(v.shape) if v is not None else None,\n                           ALL_FIELDS)\n  return _build_placeholders_from_specs(\n      graph_dtypes,\n      graph_shapes,\n      force_dynamic_num_graphs=force_dynamic_num_graphs)\n\n\ndef get_feed_dict(placeholders, graph):\n  """"""Feeds a `graphs.GraphsTuple` of numpy arrays or `None` into `placeholders`.\n\n  When feeding a fully defined graph (no `None` field) into a session, this\n  method is not necessary as one can directly do:\n\n  ```\n  _ = sess.run(_, {placeholders: graph})\n  ```\n\n  However, if the placeholders contain `None`, the above construction would\n  fail. This method allows to replace the above call by\n\n  ```\n  _ = sess.run(_, get_feed_dict(placeholders: graph))\n  ```\n\n  restoring the correct behavior.\n\n  Args:\n    placeholders: A `graphs.GraphsTuple` containing placeholders.\n    graph: A `graphs.GraphsTuple` containing placeholder compatibale values,\n      or `None`s.\n\n  Returns:\n    A dictionary with key placeholders and values the fed in values.\n\n  Raises:\n    ValueError: If the `None` fields in placeholders and `graph` do not exactly\n      match.\n  """"""\n  feed_dict = {}\n  for field in ALL_FIELDS:\n    placeholder = getattr(placeholders, field)\n    feed_value = getattr(graph, field)\n    if placeholder is None or feed_value is None:\n      if not (placeholder is None and feed_value is None):\n        raise ValueError(""Field {} should be `None` in either none or both of ""\n                         ""the placeholders and feed values."".format(field))\n    else:\n      feed_dict[placeholder] = feed_value\n  return feed_dict\n\n\ndef placeholders_from_data_dicts(data_dicts,\n                                 force_dynamic_num_graphs=True,\n                                 name=""placeholders_from_data_dicts""):\n  """"""Constructs placeholders compatible with a list of data dicts.\n\n  Args:\n    data_dicts: An iterable of data dicts containing numpy arrays.\n    force_dynamic_num_graphs: A `bool` that forces the batch dimension to be\n      dynamic. Defaults to `True`.\n    name: (string, optional) A name for the operation.\n\n  Returns:\n    An instance of `graphs.GraphTuple` placeholders compatible with the\n      dimensions of the dictionaries in `data_dicts`.\n  """"""\n  with tf.name_scope(name):\n    graph = data_dicts_to_graphs_tuple(data_dicts)\n    return _placeholders_from_graphs_tuple(\n        graph, force_dynamic_num_graphs=force_dynamic_num_graphs)\n\n\ndef placeholders_from_networkxs(graph_nxs,\n                                node_shape_hint=None,\n                                edge_shape_hint=None,\n                                data_type_hint=tf.float32,\n                                force_dynamic_num_graphs=True,\n                                name=""placeholders_from_networkxs""):\n  """"""Constructs placeholders compatible with a list of networkx instances.\n\n  Given a list of networkxs instances, constructs placeholders compatible with\n  the shape of those graphs.\n\n  The networkx graph should be set up such that, for fixed shapes `node_shape`,\n   `edge_shape` and `global_shape`:\n    - `graph_nx.nodes(data=True)[i][-1][""features""]` is, for any node index i, a\n      tensor of shape `node_shape`, or `None`;\n    - `graph_nx.edges(data=True)[i][-1][""features""]` is, for any edge index i, a\n      tensor of shape `edge_shape`, or `None`;\n    - `graph_nx.edges(data=True)[i][-1][""index""]`, if present, defines the order\n      in which the edges will be sorted in the resulting `data_dict`;\n    - `graph_nx.graph[""features""] is a tensor of shape `global_shape` or `None`.\n\n  Args:\n    graph_nxs: A container of `networkx.MultiDiGraph`s.\n    node_shape_hint: (iterable of `int` or `None`, default=`None`) If the graph\n      does not contain nodes, the trailing shape for the created `NODES` field.\n      If `None` (the default), this field is left `None`. This is not used if\n      `graph_nx` contains at least one node.\n    edge_shape_hint: (iterable of `int` or `None`, default=`None`) If the graph\n      does not contain edges, the trailing shape for the created `EDGES` field.\n      If `None` (the default), this field is left `None`. This is not used if\n      `graph_nx` contains at least one edge.\n    data_type_hint: (numpy dtype, default=`np.float32`) If the `NODES` or\n      `EDGES` fields are autocompleted, their type.\n    force_dynamic_num_graphs: A `bool` that forces the batch dimension to be\n      dynamic. Defaults to `True`.\n    name: (string, optional) A name for the operation.\n\n  Returns:\n    An instance of `graphs.GraphTuple` placeholders compatible with the\n      dimensions of the graph_nxs.\n  """"""\n  with tf.name_scope(name):\n    graph = utils_np.networkxs_to_graphs_tuple(graph_nxs, node_shape_hint,\n                                               edge_shape_hint,\n                                               data_type_hint.as_numpy_dtype)\n    return _placeholders_from_graphs_tuple(\n        graph, force_dynamic_num_graphs=force_dynamic_num_graphs)\n\n\ndef _compute_stacked_offsets(sizes, repeats):\n  """"""Computes offsets to add to indices of stacked tensors (Tensorflow).\n\n  When a set of tensors are stacked, the indices of those from the second on\n  must be offset in order to be able to index into the stacked tensor. This\n  computes those offsets.\n\n  Args:\n    sizes: A 1D `Tensor` of the sizes per graph.\n    repeats: A 1D `Tensor` of the number of repeats per graph.\n\n  Returns:\n    A 1D `Tensor` containing the index offset per graph.\n  """"""\n  sizes = tf.cast(tf.convert_to_tensor(sizes[:-1]), tf.int32)\n  offset_values = tf.cumsum(tf.concat([[0], sizes], 0))\n  return repeat(offset_values, repeats)\n\n\ndef concat(input_graphs, axis, name=""graph_concat""):\n  """"""Returns an op that concatenates graphs along a given axis.\n\n  In all cases, the NODES, EDGES and GLOBALS dimension are concatenated\n  along `axis` (if a fields is `None`, the concatenation is just a `None`).\n  If `axis` == 0, then the graphs are concatenated along the (underlying) batch\n  dimension, i.e. the RECEIVERS, SENDERS, N_NODE and N_EDGE fields of the tuples\n  are also concatenated together.\n  If `axis` != 0, then there is an underlying assumption that the receivers,\n  SENDERS, N_NODE and N_EDGE fields of the graphs in `values` should all match,\n  but this is not checked by this op.\n  The graphs in `input_graphs` should have the same set of keys for which the\n  corresponding fields is not `None`.\n\n  Args:\n    input_graphs: A list of `graphs.GraphsTuple` objects containing `Tensor`s\n      and satisfying the constraints outlined above.\n    axis: An axis to concatenate on.\n    name: (string, optional) A name for the operation.\n\n  Returns: An op that returns the concatenated graphs.\n\n  Raises:\n    ValueError: If `values` is an empty list, or if the fields which are `None`\n      in `input_graphs` are not the same for all the graphs.\n  """"""\n  if not input_graphs:\n    raise ValueError(""List argument `input_graphs` is empty"")\n  utils_np._check_valid_sets_of_keys([gr._asdict() for gr in input_graphs])  # pylint: disable=protected-access\n  if len(input_graphs) == 1:\n    return input_graphs[0]\n  nodes = [gr.nodes for gr in input_graphs if gr.nodes is not None]\n  edges = [gr.edges for gr in input_graphs if gr.edges is not None]\n  globals_ = [gr.globals for gr in input_graphs if gr.globals is not None]\n\n  with tf.name_scope(name):\n    nodes = tf.concat(nodes, axis, name=""concat_nodes"") if nodes else None\n    edges = tf.concat(edges, axis, name=""concat_edges"") if edges else None\n    if globals_:\n      globals_ = tf.concat(globals_, axis, name=""concat_globals"")\n    else:\n      globals_ = None\n    output = input_graphs[0].replace(nodes=nodes, edges=edges, globals=globals_)\n    if axis != 0:\n      return output\n    n_node_per_tuple = tf.stack(\n        [tf.reduce_sum(gr.n_node) for gr in input_graphs])\n    n_edge_per_tuple = tf.stack(\n        [tf.reduce_sum(gr.n_edge) for gr in input_graphs])\n    offsets = _compute_stacked_offsets(n_node_per_tuple, n_edge_per_tuple)\n    n_node = tf.concat(\n        [gr.n_node for gr in input_graphs], axis=0, name=""concat_n_node"")\n    n_edge = tf.concat(\n        [gr.n_edge for gr in input_graphs], axis=0, name=""concat_n_edge"")\n    receivers = [\n        gr.receivers for gr in input_graphs if gr.receivers is not None\n    ]\n    receivers = receivers or None\n    if receivers:\n      receivers = tf.concat(receivers, axis, name=""concat_receivers"") + offsets\n    senders = [gr.senders for gr in input_graphs if gr.senders is not None]\n    senders = senders or None\n    if senders:\n      senders = tf.concat(senders, axis, name=""concat_senders"") + offsets\n    return output.replace(\n        receivers=receivers, senders=senders, n_node=n_node, n_edge=n_edge)\n\n\ndef stop_gradient(graph,\n                  stop_edges=True,\n                  stop_nodes=True,\n                  stop_globals=True,\n                  name=""graph_stop_gradient""):\n  """"""Stops the gradient flow through a graph.\n\n  Args:\n    graph: An instance of `graphs.GraphsTuple` containing `Tensor`s.\n    stop_edges: (bool, default=True) indicates whether to stop gradients for\n      the edges.\n    stop_nodes: (bool, default=True) indicates whether to stop gradients for\n      the nodes.\n    stop_globals: (bool, default=True) indicates whether to stop gradients for\n      the globals.\n    name: (string, optional) A name for the operation.\n\n  Returns:\n    GraphsTuple after stopping the gradients according to the provided\n    parameters.\n\n  Raises:\n    ValueError: If attempting to stop gradients through a field which has a\n      `None` value in `graph`.\n  """"""\n\n  base_err_msg = ""Cannot stop gradient through {0} if {0} are None""\n  fields_to_stop = []\n  if stop_globals:\n    if graph.globals is None:\n      raise ValueError(base_err_msg.format(GLOBALS))\n    fields_to_stop.append(GLOBALS)\n  if stop_nodes:\n    if graph.nodes is None:\n      raise ValueError(base_err_msg.format(NODES))\n    fields_to_stop.append(NODES)\n  if stop_edges:\n    if graph.edges is None:\n      raise ValueError(base_err_msg.format(EDGES))\n    fields_to_stop.append(EDGES)\n\n  with tf.name_scope(name):\n    return graph.map(tf.stop_gradient, fields_to_stop)\n\n\ndef identity(graph, name=""graph_identity""):\n  """"""Pass each element of a graph through a `tf.identity`.\n\n  This allows, for instance, to push a name scope on the graph by writing:\n  ```\n  with tf.name_scope(""encoder""):\n    graph = utils_tf.identity(graph)\n  ```\n\n  Args:\n    graph: A `graphs.GraphsTuple` containing `Tensor`s. `None` values are passed\n      through.\n    name: (string, optional) A name for the operation.\n\n  Returns:\n    A `graphs.GraphsTuple` `graphs_output` such that for any field `x` in NODES,\n    EDGES, GLOBALS, RECEIVERS, SENDERS, N_NODE, N_EDGE, if `graph.x` was\n    `None`, `graph_output.x` is `None`, and otherwise\n    `graph_output.x = tf.identity(graph.x)`\n  """"""\n  non_none_fields = [k for k in ALL_FIELDS if getattr(graph, k) is not None]\n  with tf.name_scope(name):\n    return graph.map(tf.identity, non_none_fields)\n\n\ndef make_runnable_in_session(graph, name=""make_graph_runnable_in_session""):\n  """"""Allows a graph containing `None` fields to be run in a `tf.Session`.\n\n  The `None` values of `graph` are replaced by `tf.no_op()`. This function is\n  meant to be called just before a call to `sess.run` on a Tensorflow session\n  `sess`, as `None` values currently cannot be run through a session.\n\n  Args:\n    graph: A `graphs.GraphsTuple` containing `Tensor`s or `None` values.\n    name: (string, optional) A name for the operation.\n\n  Returns:\n    A `graphs.GraphsTuple` `graph_output` such that, for any field `x` in NODES,\n    EDGES, GLOBALS, RECEIVERS, SENDERS, N_NODE, N_EDGE, and a Tensorflow session\n    `sess`, if `graph.x` was `None`, `sess.run(graph_output)` is `None`, and\n    otherwise\n  """"""\n  none_fields = [k for k in ALL_FIELDS if getattr(graph, k) is None]\n  with tf.name_scope(name):\n    return graph.map(lambda _: tf.no_op(), none_fields)\n\n\ndef repeat(tensor, repeats, axis=0, name=""repeat"", sum_repeats_hint=None):\n  """"""Repeats a `tf.Tensor`\'s elements along an axis by custom amounts.\n\n  Equivalent to Numpy\'s `np.repeat`.\n  `tensor and `repeats` must have the same numbers of elements along `axis`.\n\n  Args:\n    tensor: A `tf.Tensor` to repeat.\n    repeats: A 1D sequence of the number of repeats per element.\n    axis: An axis to repeat along. Defaults to 0.\n    name: (string, optional) A name for the operation.\n    sum_repeats_hint: Integer with the total sum of repeats in case it is\n      known at graph definition time.\n\n  Returns:\n    The `tf.Tensor` with repeated values.\n  """"""\n  with tf.name_scope(name):\n    if sum_repeats_hint is not None:\n      sum_repeats = sum_repeats_hint\n    else:\n      sum_repeats = tf.reduce_sum(repeats)\n\n    # This is TPU compatible.\n    # Create a tensor consistent with output size indicating where the splits\n    # between the different repeats are. For example:\n    #   repeats = [2, 3, 6]\n    # with cumsum(exclusive=True):\n    #   scatter_indices = [0, 2, 5]\n    # with scatter_nd:\n    #   block_split_indicators = [1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0]\n    # cumsum(exclusive=False) - 1\n    #   gather_indices =         [0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 2]\n\n    # Note that scatter_nd accumulates for duplicated indices, so for\n    # repeats = [2, 0, 6]\n    # scatter_indices = [0, 2, 2]\n    # block_split_indicators = [1, 0, 2, 0, 0, 0, 0, 0]\n    # gather_indices =         [0, 0, 2, 2, 2, 2, 2, 2]\n\n    # Sometimes repeats may have zeros in the last groups. E.g.\n    # for repeats = [2, 3, 0]\n    # scatter_indices = [0, 2, 5]\n    # However, the `gather_nd` only goes up to (sum_repeats - 1) index. (4 in\n    # the example). And this would throw an error due to trying to index\n    # outside the shape. Instead we let the scatter_nd have one more element\n    # and we trim it from the output.\n    scatter_indices = tf.cumsum(repeats, exclusive=True)\n    block_split_indicators = tf.scatter_nd(\n        indices=tf.expand_dims(scatter_indices, axis=1),\n        updates=tf.ones_like(scatter_indices),\n        shape=[sum_repeats + 1])[:-1]\n    gather_indices = tf.cumsum(block_split_indicators, exclusive=False) - 1\n\n    # An alternative implementation of the same, where block split indicators\n    # does not have an indicator for the first group, and requires less ops\n    # but requires creating a matrix of size [len(repeats), sum_repeats] is:\n    # cumsum_repeats = tf.cumsum(repeats, exclusive=False)\n    # block_split_indicators = tf.reduce_sum(\n    #     tf.one_hot(cumsum_repeats, sum_repeats, dtype=tf.int32), axis=0)\n    # gather_indices = tf.cumsum(block_split_indicators, exclusive=False)\n\n    # Now simply gather the tensor along the correct axis.\n    repeated_tensor = tf.gather(tensor, gather_indices, axis=axis)\n\n    shape = tensor.shape.as_list()\n    shape[axis] = sum_repeats_hint\n    repeated_tensor.set_shape(shape)\n    return repeated_tensor\n\n\ndef _populate_number_fields(data_dict):\n  """"""Returns a dict with the number fields N_NODE, N_EDGE filled in.\n\n  The N_NODE field is filled if the graph contains a non-`None` NODES field;\n  otherwise, it is set to 0.\n  The N_EDGE field is filled if the graph contains a non-`None` RECEIVERS field;\n  otherwise, it is set to 0.\n\n  Args:\n    data_dict: An input `dict`.\n\n  Returns:\n    The data `dict` with number fields.\n  """"""\n  dct = data_dict.copy()\n  for number_field, data_field in [[N_NODE, NODES], [N_EDGE, RECEIVERS]]:\n    if dct.get(number_field) is None:\n      if dct[data_field] is not None:\n        dct[number_field] = tf.shape(dct[data_field])[0]\n      else:\n        dct[number_field] = tf.constant(0, dtype=tf.int32)\n  return dct\n\n\ndef _to_compatible_data_dicts(data_dicts):\n  """"""Convert the content of `data_dicts` to tensors of the right type.\n\n  All fields are converted to `Tensor`s. The index fields (`SENDERS` and\n  `RECEIVERS`) and number fields (`N_NODE`, `N_EDGE`) are cast to `tf.int32`.\n\n  Args:\n    data_dicts: An iterable of dictionaries with keys `ALL_KEYS` and\n      values either `None`s, or quantities that can be converted to `Tensor`s.\n\n  Returns:\n    A list of dictionaries containing `Tensor`s or `None`s.\n  """"""\n  results = []\n  for data_dict in data_dicts:\n    result = {}\n    for k, v in data_dict.items():\n      if v is None:\n        result[k] = None\n      else:\n        dtype = tf.int32 if k in [SENDERS, RECEIVERS, N_NODE, N_EDGE] else None\n        result[k] = tf.convert_to_tensor(v, dtype)\n    results.append(result)\n  return results\n\n\ndef _concatenate_data_dicts(data_dicts):\n  """"""Concatenate a list of data dicts to create the equivalent batched graph.\n\n  Args:\n    data_dicts: An iterable of data dictionaries with keys a subset of\n      `GRAPH_DATA_FIELDS`, plus, potentially, a subset of `GRAPH_NUMBER_FIELDS`.\n      Every element of `data_dicts` has to contain the same set of keys.\n      Moreover, the key `NODES` or `N_NODE` must be present in every element of\n      `data_dicts`.\n\n  Returns:\n    A data dictionary with the keys `GRAPH_DATA_FIELDS + GRAPH_NUMBER_FIELDS`,\n    representing the concatenated graphs.\n\n  Raises:\n    ValueError: If two dictionaries in `data_dicts` have a different set of\n      keys.\n  """"""\n  # Go from a list of dict to a dict of lists\n  dct = collections.defaultdict(lambda: [])\n  for data_dict in data_dicts:\n    data_dict = _populate_number_fields(data_dict)\n    for k, v in data_dict.items():\n      if v is not None:\n        dct[k].append(v)\n      elif k not in dct:\n        dct[k] = None\n  dct = dict(dct)\n\n  # Concatenate the graphs.\n  for field, tensors in dct.items():\n    if tensors is None:\n      dct[field] = None\n    elif field in list(GRAPH_NUMBER_FIELDS) + [GLOBALS]:\n      dct[field] = tf.stack(tensors)\n    else:\n      dct[field] = tf.concat(tensors, axis=0)\n\n  # Add offsets to the receiver and sender indices.\n  if dct[RECEIVERS] is not None:\n    offset = _compute_stacked_offsets(dct[N_NODE], dct[N_EDGE])\n    dct[RECEIVERS] += offset\n    dct[SENDERS] += offset\n\n  return dct\n\n\ndef _create_complete_edges_from_nodes_static(n_node, exclude_self_edges):\n  """"""Creates complete edges for a graph with `n_node`.\n\n  Args:\n    n_node: (python integer) The number of nodes.\n    exclude_self_edges: (bool) Excludes self-connected edges.\n\n  Returns:\n    A dict of RECEIVERS, SENDERS and N_EDGE data (`Tensor`s of rank 1).\n  """"""\n  receivers = []\n  senders = []\n  n_edges = 0\n  for node_1 in range(n_node):\n    for node_2 in range(n_node):\n      if not exclude_self_edges or node_1 != node_2:\n        receivers.append(node_1)\n        senders.append(node_2)\n        n_edges += 1\n\n  return {\n      RECEIVERS: tf.constant(receivers, dtype=tf.int32),\n      SENDERS: tf.constant(senders, dtype=tf.int32),\n      N_EDGE: tf.constant([n_edges], dtype=tf.int32)\n  }\n\n\ndef _create_complete_edges_from_nodes_dynamic(n_node, exclude_self_edges):\n  """"""Creates complete edges for a graph with `n_node`.\n\n  Args:\n    n_node: (integer scalar `Tensor`) The number of nodes.\n    exclude_self_edges: (bool) Excludes self-connected edges.\n\n  Returns:\n    A dict of RECEIVERS, SENDERS and N_EDGE data (`Tensor`s of rank 1).\n  """"""\n  rng = tf.range(n_node)\n  receivers, senders = tf.meshgrid(rng, rng)\n  n_edge = n_node * n_node\n\n  if exclude_self_edges:\n    ind = tf.cast(1 - tf.eye(n_node), bool)\n    receivers = tf.boolean_mask(receivers, ind)\n    senders = tf.boolean_mask(senders, ind)\n    n_edge -= n_node\n\n  receivers = tf.reshape(tf.cast(receivers, tf.int32), [n_edge])\n  senders = tf.reshape(tf.cast(senders, tf.int32), [n_edge])\n  n_edge = tf.reshape(n_edge, [1])\n\n  return {RECEIVERS: receivers, SENDERS: senders, N_EDGE: n_edge}\n\n\ndef _validate_edge_fields_are_all_none(graph):\n  if not all(getattr(graph, x) is None for x in [EDGES, RECEIVERS, SENDERS]):\n    raise ValueError(""Can only add fully connected a graph with `None`""\n                     ""edges, receivers and senders"")\n\n\ndef fully_connect_graph_static(graph,\n                               exclude_self_edges=False,\n                               name=""fully_connect_graph_static""):\n  """"""Adds edges to a graph by fully-connecting the nodes.\n\n  This method can be used if the number of nodes for each graph in `graph` is\n  constant and known at graph building time: it will be inferred by dividing\n  the number of nodes in the batch(the length of `graph.nodes`) by the number of\n  graphs in the batch (the length of `graph.n_node`). It is an error to call\n  this method with batches of graphs with dynamic or uneven sizes; in the latter\n  case, the method may silently yield an incorrect result.\n\n  Args:\n    graph: A `graphs.GraphsTuple` with `None` values for the edges, senders and\n      receivers.\n    exclude_self_edges (default=False): Excludes self-connected edges.\n    name: (string, optional) A name for the operation.\n\n  Returns:\n    A `graphs.GraphsTuple` containing `Tensor`s with fully-connected edges.\n\n  Raises:\n    ValueError: If any of the `EDGES`, `RECEIVERS` or `SENDERS` field is not\n      `None` in `graph`.\n    ValueError: If the number of graphs (extracted from `graph.n_node` leading\n      dimension) or number of nodes (extracted from `graph.nodes` leading\n      dimension) is not known at construction time, or if the latter does not\n      divide the former (observe that this is only a necessary condition for\n      the constantness of the number of nodes per graph).\n  """"""\n  _validate_edge_fields_are_all_none(graph)\n\n  num_graphs = graph.n_node.shape.as_list()[0]\n  if num_graphs is None:\n    raise ValueError(""Number of graphs must be known at construction time when ""\n                     ""using `fully_connect_graph_static`. Did you mean to use ""\n                     ""`fully_connect_graph_dynamic`?"")\n  num_nodes = graph.nodes.shape.as_list()[0]\n  if num_nodes is None:\n    raise ValueError(""Number of nodes must be known at construction time when ""\n                     ""using `fully_connect_graph_static`. Did you mean to use ""\n                     ""`fully_connect_graph_dynamic`?"")\n  if num_nodes % num_graphs != 0:\n    raise ValueError(""Number of nodes must be the same in all graphs when ""\n                     ""using `fully_connect_graph_static`. Did you mean to use ""\n                     ""`fully_connect_graph_dynamic`?"")\n  num_nodes_per_graph = num_nodes // num_graphs\n\n  with tf.name_scope(name):\n    one_graph_edges = _create_complete_edges_from_nodes_static(\n        num_nodes_per_graph, exclude_self_edges)\n    n_edges = num_nodes_per_graph * (num_nodes_per_graph - 1)\n    if not exclude_self_edges:\n      n_edges += num_nodes_per_graph\n\n    all_graph_edges = {\n        k: tf.tile(v, [num_graphs]) for k, v in six.iteritems(one_graph_edges)\n    }\n    offsets = [\n        num_nodes_per_graph * i  # pylint: disable=g-complex-comprehension\n        for i in range(num_graphs)\n        for _ in range(n_edges)\n    ]\n    all_graph_edges[RECEIVERS] += offsets\n    all_graph_edges[SENDERS] += offsets\n    return graph.replace(**all_graph_edges)\n\n\ndef fully_connect_graph_dynamic(graph,\n                                exclude_self_edges=False,\n                                name=""fully_connect_graph_dynamic""):\n  """"""Adds edges to a graph by fully-connecting the nodes.\n\n  This method does not require the number of nodes per graph to be constant,\n  or to be known at graph building time.\n\n  Args:\n    graph: A `graphs.GraphsTuple` with `None` values for the edges, senders and\n      receivers.\n    exclude_self_edges (default=False): Excludes self-connected edges.\n    name: (string, optional) A name for the operation.\n\n  Returns:\n    A `graphs.GraphsTuple` containing `Tensor`s with fully-connected edges.\n\n  Raises:\n    ValueError: if any of the `EDGES`, `RECEIVERS` or `SENDERS` field is not\n      `None` in `graph`.\n  """"""\n  _validate_edge_fields_are_all_none(graph)\n\n  with tf.name_scope(name):\n\n    def body(i, senders, receivers, n_edge):\n      edges = _create_complete_edges_from_nodes_dynamic(graph.n_node[i],\n                                                        exclude_self_edges)\n      return (i + 1, senders.write(i, edges[SENDERS]),\n              receivers.write(i, edges[RECEIVERS]),\n              n_edge.write(i, edges[N_EDGE]))\n\n    num_graphs = get_num_graphs(graph)\n    loop_condition = lambda i, *_: tf.less(i, num_graphs)\n    initial_loop_vars = [0] + [\n        tf.TensorArray(dtype=tf.int32, size=num_graphs, infer_shape=False)\n        for _ in range(3)  # senders, receivers, n_edge\n    ]\n    _, senders_array, receivers_array, n_edge_array = tf.while_loop(\n        loop_condition, body, initial_loop_vars, back_prop=False)\n\n    n_edge = n_edge_array.concat()\n    offsets = _compute_stacked_offsets(graph.n_node, n_edge)\n    senders = senders_array.concat() + offsets\n    receivers = receivers_array.concat() + offsets\n    senders.set_shape(offsets.shape)\n    receivers.set_shape(offsets.shape)\n\n    receivers.set_shape([None])\n    senders.set_shape([None])\n\n    num_graphs = graph.n_node.get_shape().as_list()[0]\n    n_edge.set_shape([num_graphs])\n\n    return graph._replace(senders=senders, receivers=receivers, n_edge=n_edge)\n\n\ndef set_zero_node_features(graph,\n                           node_size,\n                           dtype=tf.float32,\n                           name=""set_zero_node_features""):\n  """"""Completes the node state of a graph.\n\n  Args:\n    graph: A `graphs.GraphsTuple` with a `None` edge state.\n    node_size: (int) the dimension for the created node features.\n    dtype: (tensorflow type) the type for the created nodes features.\n    name: (string, optional) A name for the operation.\n\n  Returns:\n    The same graph but for the node field, which is a `Tensor` of shape\n    `[number_of_nodes, node_size]`  where `number_of_nodes = sum(graph.n_node)`,\n    with type `dtype`, filled with zeros.\n\n  Raises:\n    ValueError: If the `NODES` field is not None in `graph`.\n    ValueError: If `node_size` is None.\n  """"""\n  if graph.nodes is not None:\n    raise ValueError(\n        ""Cannot complete node state if the graph already has node features."")\n  if node_size is None:\n    raise ValueError(""Cannot complete nodes with None node_size"")\n  with tf.name_scope(name):\n    n_nodes = tf.reduce_sum(graph.n_node)\n    return graph._replace(\n        nodes=tf.zeros(shape=[n_nodes, node_size], dtype=dtype))\n\n\ndef set_zero_edge_features(graph,\n                           edge_size,\n                           dtype=tf.float32,\n                           name=""set_zero_edge_features""):\n  """"""Completes the edge state of a graph.\n\n  Args:\n    graph: A `graphs.GraphsTuple` with a `None` edge state.\n    edge_size: (int) the dimension for the created edge features.\n    dtype: (tensorflow type) the type for the created edge features.\n    name: (string, optional) A name for the operation.\n\n  Returns:\n    The same graph but for the edge field, which is a `Tensor` of shape\n    `[number_of_edges, edge_size]`, where `number_of_edges = sum(graph.n_edge)`,\n    with type `dtype` and filled with zeros.\n\n  Raises:\n    ValueError: If the `EDGES` field is not None in `graph`.\n    ValueError: If the `RECEIVERS` or `SENDERS` field are None in `graph`.\n    ValueError: If `edge_size` is None.\n  """"""\n  if graph.edges is not None:\n    raise ValueError(\n        ""Cannot complete edge state if the graph already has edge features."")\n  if graph.receivers is None or graph.senders is None:\n    raise ValueError(\n        ""Cannot complete edge state if the receivers or senders are None."")\n  if edge_size is None:\n    raise ValueError(""Cannot complete edges with None edge_size"")\n  with tf.name_scope(name):\n    senders_leading_size = graph.senders.shape.as_list()[0]\n    if senders_leading_size is not None:\n      n_edges = senders_leading_size\n    else:\n      n_edges = tf.reduce_sum(graph.n_edge)\n    return graph._replace(\n        edges=tf.zeros(shape=[n_edges, edge_size], dtype=dtype))\n\n\ndef set_zero_global_features(graph,\n                             global_size,\n                             dtype=tf.float32,\n                             name=""set_zero_global_features""):\n  """"""Completes the global state of a graph.\n\n  Args:\n    graph: A `graphs.GraphsTuple` with a `None` global state.\n    global_size: (int) the dimension for the created global features.\n    dtype: (tensorflow type) the type for the created global features.\n    name: (string, optional) A name for the operation.\n\n  Returns:\n    The same graph but for the global field, which is a `Tensor` of shape\n    `[num_graphs, global_size]`, type `dtype` and filled with zeros.\n\n  Raises:\n    ValueError: If the `GLOBALS` field of `graph` is not `None`.\n    ValueError: If `global_size` is not `None`.\n  """"""\n  if graph.globals is not None:\n    raise ValueError(\n        ""Cannot complete global state if graph already has global features."")\n  if global_size is None:\n    raise ValueError(""Cannot complete globals with None global_size"")\n  with tf.name_scope(name):\n    n_graphs = get_num_graphs(graph)\n    return graph._replace(\n        globals=tf.zeros(shape=[n_graphs, global_size], dtype=dtype))\n\n\ndef data_dicts_to_graphs_tuple(data_dicts, name=""data_dicts_to_graphs_tuple""):\n  """"""Creates a `graphs.GraphsTuple` containing tensors from data dicts.\n\n   All dictionaries must have exactly the same set of keys with non-`None`\n   values associated to them. Moreover, this set of this key must define a valid\n   graph (i.e. if the `EDGES` are `None`, the `SENDERS` and `RECEIVERS` must be\n   `None`, and `SENDERS` and `RECEIVERS` can only be `None` both at the same\n   time). The values associated with a key must be convertible to `Tensor`s,\n   for instance python lists, numpy arrays, or Tensorflow `Tensor`s.\n\n   This method may perform a memory copy.\n\n   The `RECEIVERS`, `SENDERS`, `N_NODE` and `N_EDGE` fields are cast to\n   `np.int32` type.\n\n  Args:\n    data_dicts: An iterable of data dictionaries with keys in `ALL_FIELDS`.\n    name: (string, optional) A name for the operation.\n\n  Returns:\n    A `graphs.GraphTuple` representing the graphs in `data_dicts`.\n  """"""\n  data_dicts = [dict(d) for d in data_dicts]\n  for key in ALL_FIELDS:\n    for data_dict in data_dicts:\n      data_dict.setdefault(key, None)\n  utils_np._check_valid_sets_of_keys(data_dicts)  # pylint: disable=protected-access\n  with tf.name_scope(name):\n    data_dicts = _to_compatible_data_dicts(data_dicts)\n    return graphs.GraphsTuple(**_concatenate_data_dicts(data_dicts))\n\n\ndef _check_valid_index(index, element_name):\n  """"""Verifies if a value with `element_name` is a valid index.""""""\n  if isinstance(index, int):\n    return True\n  elif isinstance(index, tf.Tensor):\n    if index.dtype != tf.int32 and index.dtype != tf.int64:\n      raise TypeError(\n          ""Invalid tensor `{}` parameter. Valid tensor indices must have ""\n          ""types tf.int32 or tf.int64, got {}.""\n          .format(element_name, index.dtype))\n    if index.shape.as_list():\n      raise TypeError(\n          ""Invalid tensor `{}` parameter. Valid tensor indices must be scalars ""\n          ""with shape [], got{}""\n          .format(element_name, index.shape.as_list()))\n    return True\n  else:\n    raise TypeError(\n        ""Invalid `{}` parameter. Valid tensor indices must be integers ""\n        ""or tensors, got {}.""\n        .format(element_name, type(index)))\n\n\ndef get_graph(input_graphs, index, name=""get_graph""):\n  """"""Indexes into a graph.\n\n  Given a `graphs.graphsTuple` containing `Tensor`s and an index (either\n  an `int` or a `slice`), index into the nodes, edges and globals to extract the\n  graphs specified by the slice, and returns them into an another instance of a\n  `graphs.graphsTuple` containing `Tensor`s.\n\n  Args:\n    input_graphs: A `graphs.GraphsTuple` containing `Tensor`s.\n    index: An `int`, a `slice`, a tensor `int` or a tensor `slice`, to index\n      into `graph`. `index` should be compatible with the number of graphs in\n      `graphs`. The `step` parameter of the `slice` objects must be None.\n    name: (string, optional) A name for the operation.\n\n  Returns:\n    A `graphs.GraphsTuple` containing `Tensor`s, made of the extracted\n      graph(s).\n\n  Raises:\n    TypeError: if `index` is not an `int`, a `slice`, or corresponding tensor\n      types.\n    ValueError: if `index` is a slice and `index.step` if not None.\n  """"""\n\n  def safe_slice_none(value, slice_):\n    if value is None:\n      return value\n    return value[slice_]\n\n  if isinstance(index, (int, tf.Tensor)):\n    _check_valid_index(index, ""index"")\n    graph_slice = slice(index, index + 1)\n  elif (isinstance(index, slice) and\n        _check_valid_index(index.stop, ""index.stop"") and\n        (index.start is None or _check_valid_index(\n            index.start, ""index.start""))):\n    if index.step is not None:\n      raise ValueError(""slices with step/stride are not supported, got {}""\n                       .format(index))\n    graph_slice = index\n  else:\n    raise TypeError(\n        ""unsupported index type got {} with type {}. Index must be a valid ""\n        ""scalar integer (tensor or int) or a slice of such values.""\n        .format(index, type(index)))\n\n  start_slice = slice(0, graph_slice.start)\n\n  with tf.name_scope(name):\n    start_node_index = tf.reduce_sum(\n        input_graphs.n_node[start_slice], name=""start_node_index"")\n    start_edge_index = tf.reduce_sum(\n        input_graphs.n_edge[start_slice], name=""start_edge_index"")\n    end_node_index = start_node_index + tf.reduce_sum(\n        input_graphs.n_node[graph_slice], name=""end_node_index"")\n    end_edge_index = start_edge_index + tf.reduce_sum(\n        input_graphs.n_edge[graph_slice], name=""end_edge_index"")\n    nodes_slice = slice(start_node_index, end_node_index)\n    edges_slice = slice(start_edge_index, end_edge_index)\n\n    sliced_graphs_dict = {}\n\n    for field in set(GRAPH_NUMBER_FIELDS) | {""globals""}:\n      sliced_graphs_dict[field] = safe_slice_none(\n          getattr(input_graphs, field), graph_slice)\n\n    field = ""nodes""\n    sliced_graphs_dict[field] = safe_slice_none(\n        getattr(input_graphs, field), nodes_slice)\n\n    for field in {""edges"", ""senders"", ""receivers""}:\n      sliced_graphs_dict[field] = safe_slice_none(\n          getattr(input_graphs, field), edges_slice)\n      if (field in {""senders"", ""receivers""} and\n          sliced_graphs_dict[field] is not None):\n        sliced_graphs_dict[field] = sliced_graphs_dict[field] - start_node_index\n\n    return graphs.GraphsTuple(**sliced_graphs_dict)\n\n\ndef get_num_graphs(input_graphs, name=""get_num_graphs""):\n  """"""Returns the number of graphs (i.e. the batch size) in `input_graphs`.\n\n  Args:\n    input_graphs: A `graphs.GraphsTuple` containing tensors.\n    name: (string, optional) A name for the operation.\n\n  Returns:\n    An `int` (if a static number of graphs is defined) or a `tf.Tensor` (if the\n      number of graphs is dynamic).\n  """"""\n  with tf.name_scope(name):\n    return _get_shape(input_graphs.n_node)[0]\n\n\ndef nest_to_numpy(nest_of_tensors):\n  """"""Converts a nest of eager tensors to a nest of numpy arrays.\n\n  Leaves non-`tf.Tensor` elements untouched.\n\n  A common use case for this method is to transform a `graphs.GraphsTuple` of\n  tensors into a `graphs.GraphsTuple` of arrays, or nests containing\n  `graphs.GraphsTuple`s.\n\n  Args:\n    nest_of_tensors: Nest containing `tf.Tensor`s.\n\n  Returns:\n    A nest with the same structure where `tf.Tensor`s are replaced by numpy\n    arrays and all other elements are kept the same.\n  """"""\n  return tree.map_structure(\n      lambda x: x.numpy() if isinstance(x, tf.Tensor) else x,\n      nest_of_tensors)\n\n\ndef specs_from_graphs_tuple(\n    graphs_tuple_sample,\n    dynamic_num_graphs=False,\n    dynamic_num_nodes=True,\n    dynamic_num_edges=True,\n    description_fn=tf.TensorSpec,\n    ):\n  """"""Returns the `TensorSpec` specification for a given `GraphsTuple`.\n\n  This method is often used with `tf.function` in Tensorflow 2 to obtain\n  improved speed and performance of eager code. For example:\n\n  ```\n  example_graphs_tuple = get_graphs_tuple(...)\n\n  @tf.function(input_signature=[specs_from_graphs_tuple(example_graphs_tuple)])\n  def forward_pass(graphs_tuple_input):\n    graphs_tuple_output = graph_network(graphs_tuple_input)\n    return graphs_tuple_output\n\n  for i in range(num_training_steps):\n    input = get_graphs_tuple(...)\n    with tf.GradientTape() as tape:\n      output = forward_pass(input)\n      loss = compute_loss(output)\n    grads = tape.gradient(loss, graph_network.trainable_variables)\n    optimizer.apply(grads, graph_network.trainable_variables)\n  ```\n\n  Args:\n    graphs_tuple_sample: A `graphs.GraphsTuple` with sample data. `GraphsTuple`s\n      that have fields with `None` are not accepted since they will create an\n      invalid signature specification for `tf.function`. If your graph has\n      `None`s use `utils_tf.set_zero_edge_features`,\n      `utils_tf.set_zero_node_features` or `utils_tf.set_zero_global_features`.\n      This method also returns the signature for `GraphTuple`s with nests of\n      tensors in the feature fields (`nodes`, `edges`, `globals`), including\n      empty nests (e.g. empty list, dict, or tuple). Nested node, edge and\n      global feature tensors, should usually have the same leading dimension as\n      all other node, edge and global feature tensors respectively.\n    dynamic_num_graphs: Boolean indicating if the number of graphs in each\n      `GraphsTuple` will be variable across examples.\n    dynamic_num_nodes: Boolean indicating if number of nodes per graph will be\n      variable across examples. Not used if `dynamic_num_graphs` is True, as the\n      size of the first axis of all `GraphsTuple` fields will be variable, due\n      to the variable number of graphs.\n    dynamic_num_edges: Boolean indicating if number of edges per graph will be\n      variable across examples. Not used if dynamic_num_graphs is True, as the\n      size of the first axis of all `GraphsTuple` fields will be variable, due\n      to the variable number of graphs.\n    description_fn: A callable which accepts the dtype and shape arguments to\n      describe the shapes and types of tensors. By default uses `tf.TensorSpec`.\n\n  Returns:\n    A `GraphsTuple` with tensors replaced by `TensorSpec` with shape and dtype\n    of the field contents.\n\n  Raises:\n    ValueError: If a `GraphsTuple` has a field with `None`.\n  """"""\n  graphs_tuple_description_fields = {}\n  edge_dim_fields = [graphs.EDGES, graphs.SENDERS, graphs.RECEIVERS]\n\n  # Method to get the spec for a single tensor.\n  def get_tensor_spec(tensor, field_name):\n    """"""Returns the spec of an array or a tensor in the field of a graph.""""""\n\n    shape = list(tensor.shape)\n    dtype = tensor.dtype\n\n    # If the field is not None but has no field shape (i.e. it is a constant)\n    # then we consider this to be a replaced `None`.\n    # If dynamic_num_graphs, then all fields have a None first dimension.\n    # If dynamic_num_nodes, then the ""nodes"" field needs None first dimension.\n    # If dynamic_num_edges, then the ""edges"", ""senders"" and ""receivers"" need\n    # a None first dimension.\n    if (shape and (\n        dynamic_num_graphs or\n        (dynamic_num_nodes and field_name == graphs.NODES) or\n        (dynamic_num_edges and field_name in edge_dim_fields))):\n      shape[0] = None\n    return description_fn(shape=shape, dtype=dtype)\n\n  for field_name in graphs.ALL_FIELDS:\n    field_sample = getattr(graphs_tuple_sample, field_name)\n    if field_sample is None:\n      raise ValueError(\n          ""The `GraphsTuple` field `{}` was `None`. All fields of the ""\n          ""`GraphsTuple` must be specified to create valid signatures that""\n          ""work with `tf.function`. This can be achieved with `input_graph = ""\n          ""utils_tf.set_zero_{{node,edge,global}}_features(input_graph, 0)`""\n          ""to replace None\'s by empty features in your graph. Alternatively""\n          ""`None`s can be replaced by empty lists by doing `input_graph = ""\n          ""input_graph.replace({{nodes,edges,globals}}=[]). To ensure ""\n          ""correct execution of the program, it is recommended to restore ""\n          ""the None\'s once inside of the `tf.function` by doing ""\n          ""`input_graph = input_graph.replace({{nodes,edges,globals}}=None)""\n          """".format(field_name))\n\n    if field_name in graphs.GRAPH_FEATURE_FIELDS:\n      field_spec = tree.map_structure(\n          functools.partial(get_tensor_spec, field_name=field_name),\n          field_sample)\n    else:\n      field_spec = get_tensor_spec(field_sample, field_name=field_name)\n\n    graphs_tuple_description_fields[field_name] = field_spec\n\n  return graphs.GraphsTuple(**graphs_tuple_description_fields)\n'"
graph_nets/demos/__init__.py,0,"b'# Copyright 2018 The GraphNets Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n\n""""""Graph networks library demos.""""""\n'"
graph_nets/demos/models.py,1,"b'# Copyright 2018 The GraphNets Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Model architectures for the demos.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom graph_nets import modules\nfrom graph_nets import utils_tf\nimport sonnet as snt\n\nNUM_LAYERS = 2  # Hard-code number of layers in the edge/node/global models.\nLATENT_SIZE = 16  # Hard-code latent layer sizes for demos.\n\n\ndef make_mlp_model():\n  """"""Instantiates a new MLP, followed by LayerNorm.\n\n  The parameters of each new MLP are not shared with others generated by\n  this function.\n\n  Returns:\n    A Sonnet module which contains the MLP and LayerNorm.\n  """"""\n  return snt.Sequential([\n      snt.nets.MLP([LATENT_SIZE] * NUM_LAYERS, activate_final=True),\n      snt.LayerNorm()\n  ])\n\n\nclass MLPGraphIndependent(snt.AbstractModule):\n  """"""GraphIndependent with MLP edge, node, and global models.""""""\n\n  def __init__(self, name=""MLPGraphIndependent""):\n    super(MLPGraphIndependent, self).__init__(name=name)\n    with self._enter_variable_scope():\n      self._network = modules.GraphIndependent(\n          edge_model_fn=make_mlp_model,\n          node_model_fn=make_mlp_model,\n          global_model_fn=make_mlp_model)\n\n  def _build(self, inputs):\n    return self._network(inputs)\n\n\nclass MLPGraphNetwork(snt.AbstractModule):\n  """"""GraphNetwork with MLP edge, node, and global models.""""""\n\n  def __init__(self, name=""MLPGraphNetwork""):\n    super(MLPGraphNetwork, self).__init__(name=name)\n    with self._enter_variable_scope():\n      self._network = modules.GraphNetwork(make_mlp_model, make_mlp_model,\n                                           make_mlp_model)\n\n  def _build(self, inputs):\n    return self._network(inputs)\n\n\nclass EncodeProcessDecode(snt.AbstractModule):\n  """"""Full encode-process-decode model.\n\n  The model we explore includes three components:\n  - An ""Encoder"" graph net, which independently encodes the edge, node, and\n    global attributes (does not compute relations etc.).\n  - A ""Core"" graph net, which performs N rounds of processing (message-passing)\n    steps. The input to the Core is the concatenation of the Encoder\'s output\n    and the previous output of the Core (labeled ""Hidden(t)"" below, where ""t"" is\n    the processing step).\n  - A ""Decoder"" graph net, which independently decodes the edge, node, and\n    global attributes (does not compute relations etc.), on each message-passing\n    step.\n\n                      Hidden(t)   Hidden(t+1)\n                         |            ^\n            *---------*  |  *------*  |  *---------*\n            |         |  |  |      |  |  |         |\n  Input --->| Encoder |  *->| Core |--*->| Decoder |---> Output(t)\n            |         |---->|      |     |         |\n            *---------*     *------*     *---------*\n  """"""\n\n  def __init__(self,\n               edge_output_size=None,\n               node_output_size=None,\n               global_output_size=None,\n               name=""EncodeProcessDecode""):\n    super(EncodeProcessDecode, self).__init__(name=name)\n    self._encoder = MLPGraphIndependent()\n    self._core = MLPGraphNetwork()\n    self._decoder = MLPGraphIndependent()\n    # Transforms the outputs into the appropriate shapes.\n    if edge_output_size is None:\n      edge_fn = None\n    else:\n      edge_fn = lambda: snt.Linear(edge_output_size, name=""edge_output"")\n    if node_output_size is None:\n      node_fn = None\n    else:\n      node_fn = lambda: snt.Linear(node_output_size, name=""node_output"")\n    if global_output_size is None:\n      global_fn = None\n    else:\n      global_fn = lambda: snt.Linear(global_output_size, name=""global_output"")\n    with self._enter_variable_scope():\n      self._output_transform = modules.GraphIndependent(edge_fn, node_fn,\n                                                        global_fn)\n\n  def _build(self, input_op, num_processing_steps):\n    latent = self._encoder(input_op)\n    latent0 = latent\n    output_ops = []\n    for _ in range(num_processing_steps):\n      core_input = utils_tf.concat([latent0, latent], axis=1)\n      latent = self._core(core_input)\n      decoded_op = self._decoder(latent)\n      output_ops.append(self._output_transform(decoded_op))\n    return output_ops\n'"
graph_nets/demos_tf2/__init__.py,0,"b'# Copyright 2018 The GraphNets Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n\n""""""Graph networks library demos in TensorFlow 2.""""""\n'"
graph_nets/demos_tf2/models.py,1,"b'# Copyright 2018 The GraphNets Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Model architectures for the demos in TensorFlow 2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom graph_nets import modules\nfrom graph_nets import utils_tf\nimport sonnet as snt\n\n\nNUM_LAYERS = 2  # Hard-code number of layers in the edge/node/global models.\nLATENT_SIZE = 16  # Hard-code latent layer sizes for demos.\n\n\ndef make_mlp_model():\n  """"""Instantiates a new MLP, followed by LayerNorm.\n\n  The parameters of each new MLP are not shared with others generated by\n  this function.\n\n  Returns:\n    A Sonnet module which contains the MLP and LayerNorm.\n  """"""\n  return snt.Sequential([\n      snt.nets.MLP([LATENT_SIZE] * NUM_LAYERS, activate_final=True),\n      snt.LayerNorm(axis=-1, create_offset=True, create_scale=True)\n  ])\n\n\nclass MLPGraphIndependent(snt.Module):\n  """"""GraphIndependent with MLP edge, node, and global models.""""""\n\n  def __init__(self, name=""MLPGraphIndependent""):\n    super(MLPGraphIndependent, self).__init__(name=name)\n    self._network = modules.GraphIndependent(\n        edge_model_fn=make_mlp_model,\n        node_model_fn=make_mlp_model,\n        global_model_fn=make_mlp_model)\n\n  def __call__(self, inputs):\n    return self._network(inputs)\n\n\nclass MLPGraphNetwork(snt.Module):\n  """"""GraphNetwork with MLP edge, node, and global models.""""""\n\n  def __init__(self, name=""MLPGraphNetwork""):\n    super(MLPGraphNetwork, self).__init__(name=name)\n    self._network = modules.GraphNetwork(make_mlp_model, make_mlp_model,\n                                         make_mlp_model)\n\n  def __call__(self, inputs):\n    return self._network(inputs)\n\n\nclass EncodeProcessDecode(snt.Module):\n  """"""Full encode-process-decode model.\n\n  The model we explore includes three components:\n  - An ""Encoder"" graph net, which independently encodes the edge, node, and\n    global attributes (does not compute relations etc.).\n  - A ""Core"" graph net, which performs N rounds of processing (message-passing)\n    steps. The input to the Core is the concatenation of the Encoder\'s output\n    and the previous output of the Core (labeled ""Hidden(t)"" below, where ""t"" is\n    the processing step).\n  - A ""Decoder"" graph net, which independently decodes the edge, node, and\n    global attributes (does not compute relations etc.), on each message-passing\n    step.\n\n                      Hidden(t)   Hidden(t+1)\n                         |            ^\n            *---------*  |  *------*  |  *---------*\n            |         |  |  |      |  |  |         |\n  Input --->| Encoder |  *->| Core |--*->| Decoder |---> Output(t)\n            |         |---->|      |     |         |\n            *---------*     *------*     *---------*\n  """"""\n\n  def __init__(self,\n               edge_output_size=None,\n               node_output_size=None,\n               global_output_size=None,\n               name=""EncodeProcessDecode""):\n    super(EncodeProcessDecode, self).__init__(name=name)\n    self._encoder = MLPGraphIndependent()\n    self._core = MLPGraphNetwork()\n    self._decoder = MLPGraphIndependent()\n    # Transforms the outputs into the appropriate shapes.\n    if edge_output_size is None:\n      edge_fn = None\n    else:\n      edge_fn = lambda: snt.Linear(edge_output_size, name=""edge_output"")\n    if node_output_size is None:\n      node_fn = None\n    else:\n      node_fn = lambda: snt.Linear(node_output_size, name=""node_output"")\n    if global_output_size is None:\n      global_fn = None\n    else:\n      global_fn = lambda: snt.Linear(global_output_size, name=""global_output"")\n    self._output_transform = modules.GraphIndependent(\n        edge_fn, node_fn, global_fn)\n\n  def __call__(self, input_op, num_processing_steps):\n    latent = self._encoder(input_op)\n    latent0 = latent\n    output_ops = []\n    for _ in range(num_processing_steps):\n      core_input = utils_tf.concat([latent0, latent], axis=1)\n      latent = self._core(core_input)\n      decoded_op = self._decoder(latent)\n      output_ops.append(self._output_transform(decoded_op))\n    return output_ops\n'"
graph_nets/tests/__init__.py,0,"b'# Copyright 2018 The GraphNets Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Graph networks library tests.""""""\n'"
graph_nets/tests/blocks_test.py,81,"b'# Copyright 2018 The GraphNets Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n\n""""""Tests for blocks.py.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\n\nfrom absl.testing import parameterized\nfrom graph_nets import blocks\nfrom graph_nets import graphs\nfrom graph_nets import utils_np\nfrom graph_nets import utils_tf\nimport numpy as np\nimport sonnet as snt\nimport tensorflow as tf\n\n\n\nSMALL_GRAPH_1 = {\n    ""globals"": [1.1, 1.2, 1.3, 1.4],\n    ""nodes"": [[10.1, 10.2], [20.1, 20.2], [30.1, 30.2]],\n    ""edges"": [[101., 102., 103., 104.], [201., 202., 203., 204.]],\n    ""senders"": [0, 1],\n    ""receivers"": [1, 2],\n}\n\nSMALL_GRAPH_2 = {\n    ""globals"": [-1.1, -1.2, -1.3, -1.4],\n    ""nodes"": [[-10.1, -10.2], [-20.1, -20.2], [-30.1, -30.2]],\n    ""edges"": [[-101., -102., -103., -104.]],\n    ""senders"": [1,],\n    ""receivers"": [2,],\n}\n\nSMALL_GRAPH_3 = {\n    ""globals"": [1.1, 1.2, 1.3, 1.4],\n    ""nodes"": [[10.1, 10.2], [20.1, 20.2], [30.1, 30.2]],\n    ""edges"": [[101., 102., 103., 104.], [201., 202., 203., 204.]],\n    ""senders"": [1, 1],\n    ""receivers"": [0, 2],\n}\n\nSMALL_GRAPH_4 = {\n    ""globals"": [1.1, 1.2, 1.3, 1.4],\n    ""nodes"": [[10.1, 10.2], [20.1, 20.2], [30.1, 30.2]],\n    ""edges"": [[101., 102., 103., 104.], [201., 202., 203., 204.]],\n    ""senders"": [0, 2],\n    ""receivers"": [1, 1],\n}\n\n\nclass GraphModuleTest(tf.test.TestCase, parameterized.TestCase):\n  """"""Base class for all the tests in this file.""""""\n\n  def setUp(self):\n    super(GraphModuleTest, self).setUp()\n    tf.set_random_seed(0)\n\n  def _get_input_graph(self, none_fields=None):\n    if none_fields is None:\n      none_fields = []\n    input_graph = utils_tf.data_dicts_to_graphs_tuple(\n        [SMALL_GRAPH_1, SMALL_GRAPH_2, SMALL_GRAPH_3, SMALL_GRAPH_4])\n    input_graph = input_graph.map(lambda _: None, none_fields)\n    return input_graph\n\n  def _get_shaped_input_graph(self):\n    return graphs.GraphsTuple(\n        nodes=tf.zeros([3, 4, 5, 11], dtype=tf.float32),\n        edges=tf.zeros([5, 4, 5, 12], dtype=tf.float32),\n        globals=tf.zeros([2, 4, 5, 13], dtype=tf.float32),\n        receivers=tf.range(5, dtype=tf.int32) // 3,\n        senders=tf.range(5, dtype=tf.int32) % 3,\n        n_node=tf.constant([2, 1], dtype=tf.int32),\n        n_edge=tf.constant([3, 2], dtype=tf.int32),\n    )\n\n  def _assert_build_and_run(self, network, input_graph):\n    # No error at construction time.\n    output = network(input_graph)\n    # No error at runtime.\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n      sess.run(output)\n\n\nBROADCAST_GLOBAL_TO_EDGES = [\n    [1.1, 1.2, 1.3, 1.4],\n    [1.1, 1.2, 1.3, 1.4],\n    [-1.1, -1.2, -1.3, -1.4],\n]\n\nBROADCAST_GLOBAL_TO_NODES = [\n    [1.1, 1.2, 1.3, 1.4],\n    [1.1, 1.2, 1.3, 1.4],\n    [1.1, 1.2, 1.3, 1.4],\n    [-1.1, -1.2, -1.3, -1.4],\n    [-1.1, -1.2, -1.3, -1.4],\n    [-1.1, -1.2, -1.3, -1.4],\n]\n\nSENDER_NODES_TO_EDGES = [\n    [10.1, 10.2],\n    [20.1, 20.2],\n    [-20.1, -20.2],\n]\n\nRECEIVER_NODES_TO_EDGES = [\n    [20.1, 20.2],\n    [30.1, 30.2],\n    [-30.1, -30.2],\n]\n\n\nclass BroadcastersTest(GraphModuleTest):\n  """"""Tests for the broadcasters.""""""\n\n  @parameterized.named_parameters(\n      (""globals_to_edges"",\n       blocks.broadcast_globals_to_edges, BROADCAST_GLOBAL_TO_EDGES),\n      (""globals_to_nodes"",\n       blocks.broadcast_globals_to_nodes, BROADCAST_GLOBAL_TO_NODES),\n      (""sender_nodes_to_edges"",\n       blocks.broadcast_sender_nodes_to_edges, SENDER_NODES_TO_EDGES),\n      (""receiver_nodes_to_edges"",\n       blocks.broadcast_receiver_nodes_to_edges, RECEIVER_NODES_TO_EDGES),\n  )\n  def test_output_values(self, broadcaster, expected):\n    """"""Test the broadcasted output value.""""""\n    input_graph = utils_tf.data_dicts_to_graphs_tuple(\n        [SMALL_GRAPH_1, SMALL_GRAPH_2])\n    broadcasted = broadcaster(input_graph)\n    with tf.Session() as sess:\n      broadcasted_out = sess.run(broadcasted)\n    self.assertNDArrayNear(\n        np.array(expected, dtype=np.float32), broadcasted_out, err=1e-4)\n\n  @parameterized.named_parameters(\n      (""globals_to_edges"",\n       blocks.broadcast_globals_to_edges, BROADCAST_GLOBAL_TO_EDGES),\n      (""globals_to_nodes"",\n       blocks.broadcast_globals_to_nodes, BROADCAST_GLOBAL_TO_NODES),\n      (""sender_nodes_to_edges"",\n       blocks.broadcast_sender_nodes_to_edges, SENDER_NODES_TO_EDGES),\n      (""receiver_nodes_to_edges"",\n       blocks.broadcast_receiver_nodes_to_edges, RECEIVER_NODES_TO_EDGES),\n  )\n  def test_output_values_larger_rank(self, broadcaster, expected):\n    """"""Test the broadcasted output value.""""""\n    input_graph = utils_tf.data_dicts_to_graphs_tuple(\n        [SMALL_GRAPH_1, SMALL_GRAPH_2])\n    input_graph = input_graph.map(\n        lambda v: tf.reshape(v, [v.get_shape().as_list()[0]] + [2, -1]))\n    broadcasted = broadcaster(input_graph)\n    with tf.Session() as sess:\n      broadcasted_out = sess.run(broadcasted)\n    self.assertNDArrayNear(\n        np.reshape(np.array(expected, dtype=np.float32),\n                   [len(expected)] + [2, -1]),\n        broadcasted_out,\n        err=1e-4)\n\n  @parameterized.named_parameters(\n      (""globals_to_edges_no_globals"",\n       blocks.broadcast_globals_to_edges, (""globals"",)),\n      (""globals_to_nodes_no_globals"",\n       blocks.broadcast_globals_to_nodes, (""globals"",)),\n      (""sender_nodes_to_edges_none_nodes"",\n       blocks.broadcast_sender_nodes_to_edges, (""nodes"",)),\n      (""sender_nodes_to_edges_none_senders"",\n       blocks.broadcast_sender_nodes_to_edges,\n       (""edges"", ""senders"", ""receivers"")),\n      (""receiver_nodes_to_edges_none_nodes"",\n       blocks.broadcast_receiver_nodes_to_edges, (""nodes"",)),\n  )\n  def test_missing_field_raises_exception(self, broadcaster, none_fields):\n    """"""Test that an error is raised if a required field is `None`.""""""\n    input_graph = self._get_input_graph(none_fields)\n    with self.assertRaisesRegexp(\n        ValueError, ""field cannot be None when broadcasting""):\n      broadcaster(input_graph)\n\n\nclass ReducersTest(GraphModuleTest):\n  """"""Tests for the reducers.""""""\n\n  @parameterized.parameters(\n      (blocks.unsorted_segment_min_or_zero,\n       [[0., 0.],\n        [0.1, -0.1],\n        [0.2, -0.3],\n        [0.4, -0.6],\n        [0.7, -1.],\n        [0.9, -0.9],\n        [0., 0.]]),\n      (blocks.unsorted_segment_max_or_zero,\n       [[0., 0.],\n        [0.1, -0.1],\n        [0.3, -0.2],\n        [0.6, -0.4],\n        [1., -0.7],\n        [0.9, -0.9],\n        [0., 0.]]),\n  )\n  def test_output_values(self, reducer, expected_values):\n    input_values_np = np.array([[0.1, -0.1],\n                                [0.2, -0.2],\n                                [0.3, -0.3],\n                                [0.4, -0.4],\n                                [0.5, -0.5],\n                                [0.6, -0.6],\n                                [0.7, -0.7],\n                                [0.8, -0.8],\n                                [0.9, -0.9],\n                                [1., -1.]], dtype=np.float32)\n    input_indices_np = np.array([1, 2, 2, 3, 3, 3, 4, 4, 5, 4], dtype=np.int32)\n    num_groups_np = np.array(7, dtype=np.int32)\n\n    input_indices = tf.constant(input_indices_np, dtype=tf.int32)\n    input_values = tf.constant(input_values_np, dtype=tf.float32)\n    num_groups = tf.constant(num_groups_np, dtype=tf.int32)\n\n    reduced = reducer(input_values, input_indices, num_groups)\n\n    with tf.Session() as sess:\n      reduced_out = sess.run(reduced)\n\n    self.assertNDArrayNear(\n        np.array(expected_values, dtype=np.float32), reduced_out, err=1e-4)\n\n\nSEGMENT_SUM_EDGES_TO_GLOBALS = [\n    [302., 304., 306., 308.],\n    [-101., -102., -103., -104.],\n    [302., 304., 306., 308.],\n    [302., 304., 306., 308.],\n]\n\nSEGMENT_SUM_NODES_TO_GLOBALS = [\n    [60.3, 60.6],\n    [-60.3, -60.6],\n    [60.3, 60.6],\n    [60.3, 60.6],\n]\n\nSEGMENT_SUM_SENT_EDGES_TO_NODES = [\n    [101., 102., 103., 104.],\n    [201., 202., 203., 204.],\n    [0., 0., 0., 0.],\n    [0., 0., 0., 0.],\n    [-101., -102., -103., -104.],\n    [0., 0., 0., 0.],\n    [0., 0., 0., 0.],\n    [302., 304., 306., 308.],\n    [0., 0., 0., 0.,],\n    [101., 102., 103., 104.],\n    [0., 0., 0., 0.],\n    [201., 202., 203., 204.],\n]\n\nSEGMENT_SUM_RECEIVED_EDGES_TO_NODES = [\n    [0., 0., 0., 0.],\n    [101., 102., 103., 104.],\n    [201., 202., 203., 204.],\n    [0., 0., 0., 0.],\n    [0., 0., 0., 0.],\n    [-101., -102., -103., -104.],\n    [101., 102., 103., 104.],\n    [0., 0., 0., 0.],\n    [201., 202., 203., 204.],\n    [0., 0., 0., 0.],\n    [302., 304., 306., 308,],\n    [0., 0., 0., 0.],\n]\n\n\nclass FieldAggregatorsTest(GraphModuleTest):\n\n  @parameterized.named_parameters(\n      (""edges_to_globals"",\n       blocks.EdgesToGlobalsAggregator(tf.unsorted_segment_sum),\n       SEGMENT_SUM_EDGES_TO_GLOBALS,),\n      (""nodes_to_globals"",\n       blocks.NodesToGlobalsAggregator(tf.unsorted_segment_sum),\n       SEGMENT_SUM_NODES_TO_GLOBALS,),\n      (""sent_edges_to_nodes"",\n       blocks.SentEdgesToNodesAggregator(tf.unsorted_segment_sum),\n       SEGMENT_SUM_SENT_EDGES_TO_NODES,),\n      (""received_edges_to_nodes"",\n       blocks.ReceivedEdgesToNodesAggregator(tf.unsorted_segment_sum),\n       SEGMENT_SUM_RECEIVED_EDGES_TO_NODES),\n  )\n  def test_output_values(self, aggregator, expected):\n    input_graph = self._get_input_graph()\n    aggregated = aggregator(input_graph)\n    with tf.Session() as sess:\n      aggregated_out = sess.run(aggregated)\n    self.assertNDArrayNear(\n        np.array(expected, dtype=np.float32), aggregated_out, err=1e-4)\n\n  @parameterized.named_parameters(\n      (""edges_to_globals"",\n       blocks.EdgesToGlobalsAggregator(tf.unsorted_segment_sum),\n       SEGMENT_SUM_EDGES_TO_GLOBALS,),\n      (""nodes_to_globals"",\n       blocks.NodesToGlobalsAggregator(tf.unsorted_segment_sum),\n       SEGMENT_SUM_NODES_TO_GLOBALS,),\n      (""sent_edges_to_nodes"",\n       blocks.SentEdgesToNodesAggregator(tf.unsorted_segment_sum),\n       SEGMENT_SUM_SENT_EDGES_TO_NODES,),\n      (""received_edges_to_nodes"",\n       blocks.ReceivedEdgesToNodesAggregator(tf.unsorted_segment_sum),\n       SEGMENT_SUM_RECEIVED_EDGES_TO_NODES),\n  )\n  def test_output_values_larger_rank(self, aggregator, expected):\n    input_graph = self._get_input_graph()\n    input_graph = input_graph.map(\n        lambda v: tf.reshape(v, [v.get_shape().as_list()[0]] + [2, -1]))\n    aggregated = aggregator(input_graph)\n    with tf.Session() as sess:\n      aggregated_out = sess.run(aggregated)\n    self.assertNDArrayNear(\n        np.reshape(np.array(expected, dtype=np.float32),\n                   [len(expected)] + [2, -1]),\n        aggregated_out,\n        err=1e-4)\n\n  @parameterized.named_parameters(\n      (""received edges to nodes missing edges"",\n       blocks.ReceivedEdgesToNodesAggregator, ""edges""),\n      (""sent edges to nodes missing edges"",\n       blocks.SentEdgesToNodesAggregator, ""edges""),\n      (""nodes to globals missing nodes"",\n       blocks.NodesToGlobalsAggregator, ""nodes""),\n      (""edges to globals missing nodes"",\n       blocks.EdgesToGlobalsAggregator, ""edges""),)\n  def test_missing_field_raises_exception(self, constructor, none_field):\n    """"""Tests that aggregator fail if a required field is missing.""""""\n    input_graph = self._get_input_graph([none_field])\n    with self.assertRaisesRegexp(ValueError, none_field):\n      constructor(tf.unsorted_segment_sum)(input_graph)\n\n  @parameterized.named_parameters(\n      (""received edges to nodes missing nodes and globals"",\n       blocks.ReceivedEdgesToNodesAggregator, [""nodes"", ""globals""]),\n      (""sent edges to nodes missing nodes and globals"",\n       blocks.SentEdgesToNodesAggregator, [""nodes"", ""globals""]),\n      (""nodes to globals missing edges and globals"",\n       blocks.NodesToGlobalsAggregator,\n       [""edges"", ""receivers"", ""senders"", ""globals""]),\n      (""edges to globals missing globals"",\n       blocks.EdgesToGlobalsAggregator, [""globals""]),\n  )\n  def test_unused_field_can_be_none(self, constructor, none_fields):\n    """"""Tests that aggregator fail if a required field is missing.""""""\n    input_graph = self._get_input_graph(none_fields)\n    constructor(tf.unsorted_segment_sum)(input_graph)\n\n\nclass EdgeBlockTest(GraphModuleTest):\n\n  def setUp(self):\n    super(EdgeBlockTest, self).setUp()\n    self._scale = 10.\n    self._edge_model_fn = lambda: lambda features: features * self._scale\n\n  @parameterized.named_parameters(\n      (""all inputs"", True, True, True, True),\n      (""edges nodes only"", True, False, False, False),\n      (""receiver nodes only"", False, True, False, False),\n      (""sender nodes only"", False, False, True, False),\n      (""globals only"", False, False, False, True),\n      (""edges and sender nodes"", True, False, True, False),\n      (""receiver nodes and globals"", False, True, False, True),\n  )\n  def test_output_values(\n      self, use_edges, use_receiver_nodes, use_sender_nodes, use_globals):\n    """"""Compares the output of an EdgeBlock to an explicit computation.""""""\n    input_graph = self._get_input_graph()\n    edge_block = blocks.EdgeBlock(\n        edge_model_fn=self._edge_model_fn,\n        use_edges=use_edges,\n        use_receiver_nodes=use_receiver_nodes,\n        use_sender_nodes=use_sender_nodes,\n        use_globals=use_globals)\n    output_graph = edge_block(input_graph)\n\n    model_inputs = []\n    if use_edges:\n      model_inputs.append(input_graph.edges)\n    if use_receiver_nodes:\n      model_inputs.append(blocks.broadcast_receiver_nodes_to_edges(input_graph))\n    if use_sender_nodes:\n      model_inputs.append(blocks.broadcast_sender_nodes_to_edges(input_graph))\n    if use_globals:\n      model_inputs.append(blocks.broadcast_globals_to_edges(input_graph))\n\n    model_inputs = tf.concat(model_inputs, axis=-1)\n    self.assertEqual(input_graph.nodes, output_graph.nodes)\n    self.assertEqual(input_graph.globals, output_graph.globals)\n\n    with tf.Session() as sess:\n      output_graph_out, model_inputs_out = sess.run(\n          (output_graph, model_inputs))\n\n    expected_output_edges = model_inputs_out * self._scale\n    self.assertNDArrayNear(\n        expected_output_edges, output_graph_out.edges, err=1e-4)\n\n  @parameterized.named_parameters(\n      (""all inputs"", True, True, True, True, 12),\n      (""edges only"", True, False, False, False, 4),\n      (""receivers only"", False, True, False, False, 2),\n      (""senders only"", False, False, True, False, 2),\n      (""globals only"", False, False, False, True, 4),\n  )\n  def test_created_variables(self,\n                             use_edges, use_receiver_nodes, use_sender_nodes,\n                             use_globals, expected_first_dim_w):\n    """"""Verifies the variable names and shapes created by an EdgeBlock.""""""\n    output_size = 10\n    expected_var_shapes_dict = {\n        ""edge_block/mlp/linear_0/b:0"": [output_size],\n        ""edge_block/mlp/linear_0/w:0"": [expected_first_dim_w, output_size]}\n\n    input_graph = self._get_input_graph()\n\n    edge_block = blocks.EdgeBlock(\n        edge_model_fn=functools.partial(snt.nets.MLP,\n                                        output_sizes=[output_size]),\n        use_edges=use_edges,\n        use_receiver_nodes=use_receiver_nodes,\n        use_sender_nodes=use_sender_nodes,\n        use_globals=use_globals)\n    edge_block(input_graph)\n\n    variables = edge_block.get_variables()\n    var_shapes_dict = {var.name: var.get_shape().as_list() for var in variables}\n    self.assertDictEqual(expected_var_shapes_dict, var_shapes_dict)\n\n  @parameterized.named_parameters(\n      (""missing node (receivers only)"", False, True, False, False, (""nodes"",)),\n      (""missing node (senders only)"", False, False, True, False, (""nodes"",)),\n      (""missing edge data"", True, False, False, False, (""edges"",)),\n      (""missing edges (but no edge consumption)"", False, True, True, False,\n       (""edges"", ""senders"", ""receivers"")),\n      (""missing globals"", False, False, False, True, (""globals"",)),\n  )\n  def test_missing_field_raises_exception(\n      self, use_edges, use_receiver_nodes, use_sender_nodes, use_globals,\n      none_fields):\n    """"""Checks that missing a required field raises an exception.""""""\n    input_graph = self._get_input_graph(none_fields)\n    edge_block = blocks.EdgeBlock(\n        edge_model_fn=self._edge_model_fn,\n        use_edges=use_edges,\n        use_receiver_nodes=use_receiver_nodes,\n        use_sender_nodes=use_sender_nodes,\n        use_globals=use_globals)\n    with self.assertRaisesRegexp(ValueError, ""field cannot be None""):\n      edge_block(input_graph)\n\n  def test_compatible_higher_rank_no_raise(self):\n    """"""No exception should occur with higher ranks tensors.""""""\n    input_graph = self._get_shaped_input_graph()\n    input_graph = input_graph.map(lambda v: tf.transpose(v, [0, 2, 1, 3]))\n    network = blocks.EdgeBlock(\n        functools.partial(snt.Conv2D, output_channels=10, kernel_shape=[3, 3]))\n    self._assert_build_and_run(network, input_graph)\n\n  @parameterized.named_parameters(\n      (""mismatched edges and r. nodes"", True, True, False, False, ""nodes""),\n      (""mismatched edges and s. nodes"", True, False, True, False, ""nodes""),\n      (""mismatched edges and globals"", True, False, False, True, ""edges""),\n      (""mismatched nodes and globals"", False, True, True, True, ""globals""),\n  )\n  def test_incompatible_higher_rank_inputs_raises(self,\n                                                  use_edges,\n                                                  use_receiver_nodes,\n                                                  use_sender_nodes,\n                                                  use_globals,\n                                                  field):\n    """"""A exception should be raised if the inputs have incompatible shapes.""""""\n    input_graph = self._get_shaped_input_graph()\n    input_graph = input_graph.replace(\n        **{field: tf.transpose(getattr(input_graph, field), [0, 2, 1, 3])})\n    network = blocks.EdgeBlock(\n        functools.partial(snt.Conv2D, output_channels=10, kernel_shape=[3, 3]),\n        use_edges=use_edges,\n        use_receiver_nodes=use_receiver_nodes,\n        use_sender_nodes=use_sender_nodes,\n        use_globals=use_globals\n    )\n    with self.assertRaisesRegexp(ValueError, ""in both shapes must be equal""):\n      network(input_graph)\n\n  @parameterized.named_parameters(\n      (""mismatched nodes"", True, False, False, True, ""nodes""),\n      (""mismatched edges"", False, True, True, True, ""edges""),\n      (""mismatched globals"", True, True, True, False, ""globals""),\n  )\n  def test_incompatible_higher_rank_inputs_no_raise(self,\n                                                    use_edges,\n                                                    use_receiver_nodes,\n                                                    use_sender_nodes,\n                                                    use_globals,\n                                                    field):\n    """"""No exception should occur if a differently shapped field is not used.""""""\n    input_graph = self._get_shaped_input_graph()\n    input_graph = input_graph.replace(\n        **{field: tf.transpose(getattr(input_graph, field), [0, 2, 1, 3])})\n    network = blocks.EdgeBlock(\n        functools.partial(snt.Conv2D, output_channels=10, kernel_shape=[3, 3]),\n        use_edges=use_edges,\n        use_receiver_nodes=use_receiver_nodes,\n        use_sender_nodes=use_sender_nodes,\n        use_globals=use_globals\n    )\n    self._assert_build_and_run(network, input_graph)\n\n  @parameterized.named_parameters(\n      (""no edges"", False, True, True, ""edges""),\n      (""no nodes"", True, False, True, ""nodes""),\n      (""no globals"", True, True, False, ""globals""),\n  )\n  def test_unused_field_can_be_none(\n      self, use_edges, use_nodes, use_globals, none_field):\n    """"""Checks that computation can handle non-necessary fields left None.""""""\n    input_graph = self._get_input_graph([none_field])\n    edge_block = blocks.EdgeBlock(\n        edge_model_fn=self._edge_model_fn,\n        use_edges=use_edges,\n        use_receiver_nodes=use_nodes,\n        use_sender_nodes=use_nodes,\n        use_globals=use_globals)\n    output_graph = edge_block(input_graph)\n\n    model_inputs = []\n    if use_edges:\n      model_inputs.append(input_graph.edges)\n    if use_nodes:\n      model_inputs.append(blocks.broadcast_receiver_nodes_to_edges(input_graph))\n      model_inputs.append(blocks.broadcast_sender_nodes_to_edges(input_graph))\n    if use_globals:\n      model_inputs.append(blocks.broadcast_globals_to_edges(input_graph))\n\n    model_inputs = tf.concat(model_inputs, axis=-1)\n    self.assertEqual(input_graph.nodes, output_graph.nodes)\n    self.assertEqual(input_graph.globals, output_graph.globals)\n\n    with tf.Session() as sess:\n      actual_edges, model_inputs_out = sess.run(\n          (output_graph.edges, model_inputs))\n\n    expected_output_edges = model_inputs_out * self._scale\n    self.assertNDArrayNear(expected_output_edges, actual_edges, err=1e-4)\n\n  def test_no_input_raises_exception(self):\n    """"""Checks that receiving no input raises an exception.""""""\n    with self.assertRaisesRegexp(ValueError, ""At least one of ""):\n      blocks.EdgeBlock(\n          edge_model_fn=self._edge_model_fn,\n          use_edges=False,\n          use_receiver_nodes=False,\n          use_sender_nodes=False,\n          use_globals=False)\n\n\nclass NodeBlockTest(GraphModuleTest):\n\n  def setUp(self):\n    super(NodeBlockTest, self).setUp()\n    self._scale = 10.\n    self._node_model_fn = lambda: lambda features: features * self._scale\n\n  @parameterized.named_parameters(\n      (""all inputs, custom reductions"", True, True, True, True,\n       tf.unsorted_segment_sum, tf.unsorted_segment_mean),\n      (""received edges only, blocks reducer"",\n       True, False, False, False, blocks.unsorted_segment_max_or_zero, None),\n      (""sent edges only, custom reduction"",\n       False, True, False, False, None, tf.unsorted_segment_prod),\n      (""nodes only"",\n       False, False, True, False, None, None),\n      (""globals only"",\n       False, False, False, True, None, None),\n      (""received edges and nodes, custom reductions"",\n       True, False, True, False,\n       blocks.unsorted_segment_min_or_zero, tf.unsorted_segment_prod),\n      (""sent edges and globals, custom reduction"",\n       False, True, False, True, None, blocks.unsorted_segment_min_or_zero),\n  )\n  def test_output_values(\n      self, use_received_edges, use_sent_edges, use_nodes,\n      use_globals, received_edges_reducer, sent_edges_reducer):\n    """"""Compares the output of a NodeBlock to an explicit computation.""""""\n    input_graph = self._get_input_graph()\n    node_block = blocks.NodeBlock(\n        node_model_fn=self._node_model_fn,\n        use_received_edges=use_received_edges,\n        use_sent_edges=use_sent_edges,\n        use_nodes=use_nodes,\n        use_globals=use_globals,\n        received_edges_reducer=received_edges_reducer,\n        sent_edges_reducer=sent_edges_reducer)\n    output_graph = node_block(input_graph)\n\n    model_inputs = []\n    if use_received_edges:\n      model_inputs.append(\n          blocks.ReceivedEdgesToNodesAggregator(\n              received_edges_reducer)(input_graph))\n    if use_sent_edges:\n      model_inputs.append(\n          blocks.SentEdgesToNodesAggregator(sent_edges_reducer)(input_graph))\n    if use_nodes:\n      model_inputs.append(input_graph.nodes)\n    if use_globals:\n      model_inputs.append(blocks.broadcast_globals_to_nodes(input_graph))\n\n    model_inputs = tf.concat(model_inputs, axis=-1)\n    self.assertEqual(input_graph.edges, output_graph.edges)\n    self.assertEqual(input_graph.globals, output_graph.globals)\n\n    with tf.Session() as sess:\n      output_graph_out, model_inputs_out = sess.run(\n          (output_graph, model_inputs))\n\n    expected_output_nodes = model_inputs_out * self._scale\n    self.assertNDArrayNear(\n        expected_output_nodes, output_graph_out.nodes, err=1e-4)\n\n  @parameterized.named_parameters(\n      (""all inputs"", True, True, True, True, 14),\n      (""received edges only"", True, False, False, False, 4),\n      (""sent edges only"", False, True, False, False, 4),\n      (""nodes only"", False, False, True, False, 2),\n      (""globals only"", False, False, False, True, 4),\n  )\n  def test_created_variables(self,\n                             use_received_edges, use_sent_edges, use_nodes,\n                             use_globals, expected_first_dim_w):\n    """"""Verifies the variable names and shapes created by a NodeBlock.""""""\n    output_size = 10\n    expected_var_shapes_dict = {\n        ""node_block/mlp/linear_0/b:0"": [output_size],\n        ""node_block/mlp/linear_0/w:0"": [expected_first_dim_w, output_size]}\n\n    input_graph = self._get_input_graph()\n\n    node_block = blocks.NodeBlock(\n        node_model_fn=functools.partial(snt.nets.MLP,\n                                        output_sizes=[output_size]),\n        use_received_edges=use_received_edges,\n        use_sent_edges=use_sent_edges,\n        use_nodes=use_nodes,\n        use_globals=use_globals)\n\n    node_block(input_graph)\n\n    variables = node_block.get_variables()\n    var_shapes_dict = {var.name: var.get_shape().as_list() for var in variables}\n    self.assertDictEqual(expected_var_shapes_dict, var_shapes_dict)\n\n  @parameterized.named_parameters(\n      (""missing nodes"", False, False, True, False, (""nodes"",)),\n      (""missing edge data (receivers only)"",\n       True, False, False, False, (""edges"",)),\n      (""missing edge data (senders only)"",\n       False, True, False, False, (""edges"",)),\n      (""missing globals"", False, False, False, True, (""globals"",)),\n  )\n  def test_missing_field_raises_exception(\n      self, use_received_edges, use_sent_edges, use_nodes, use_globals,\n      none_fields):\n    """"""Checks that missing a required field raises an exception.""""""\n    input_graph = self._get_input_graph(none_fields)\n    node_block = blocks.NodeBlock(\n        node_model_fn=self._node_model_fn,\n        use_received_edges=use_received_edges,\n        use_sent_edges=use_sent_edges,\n        use_nodes=use_nodes,\n        use_globals=use_globals)\n    with self.assertRaisesRegexp(ValueError, ""field cannot be None""):\n      node_block(input_graph)\n\n  @parameterized.named_parameters(\n      (""no received edges reducer"", True, False, None, tf.unsorted_segment_sum),\n      (""no sent edges reducer"", False, True, tf.unsorted_segment_sum, None),\n  )\n  def test_missing_aggregation_raises_exception(\n      self, use_received_edges, use_sent_edges,\n      received_edges_reducer, sent_edges_reducer):\n    """"""Checks that missing a required aggregation argument raises an error.""""""\n    with self.assertRaisesRegexp(ValueError, ""should not be None""):\n      blocks.NodeBlock(\n          node_model_fn=self._node_model_fn,\n          use_received_edges=use_received_edges,\n          use_sent_edges=use_sent_edges,\n          use_nodes=False,\n          use_globals=False,\n          received_edges_reducer=received_edges_reducer,\n          sent_edges_reducer=sent_edges_reducer)\n\n  def test_compatible_higher_rank_no_raise(self):\n    """"""No exception should occur with higher ranks tensors.""""""\n    input_graph = self._get_shaped_input_graph()\n    input_graph = input_graph.map(lambda v: tf.transpose(v, [0, 2, 1, 3]))\n    network = blocks.NodeBlock(\n        functools.partial(snt.Conv2D, output_channels=10, kernel_shape=[3, 3]))\n    self._assert_build_and_run(network, input_graph)\n\n  @parameterized.named_parameters(\n      (""mismatched nodes and r. edges"", True, False, True, False, ""edges""),\n      (""mismatched nodes and s. edges"", True, False, True, False, ""edges""),\n      (""mismatched edges and globals"", True, False, False, True, ""globals""),\n      (""mismatched nodes and globals"", False, True, True, True, ""globals""),\n  )\n  def test_incompatible_higher_rank_inputs_raises(self,\n                                                  use_received_edges,\n                                                  use_sent_edges,\n                                                  use_nodes,\n                                                  use_globals,\n                                                  field):\n    """"""A exception should be raised if the inputs have incompatible shapes.""""""\n    input_graph = self._get_shaped_input_graph()\n    input_graph = input_graph.replace(\n        **{field: tf.transpose(getattr(input_graph, field), [0, 2, 1, 3])})\n    network = blocks.NodeBlock(\n        functools.partial(snt.Conv2D, output_channels=10, kernel_shape=[3, 3]),\n        use_received_edges=use_received_edges,\n        use_sent_edges=use_sent_edges,\n        use_nodes=use_nodes,\n        use_globals=use_globals\n    )\n    with self.assertRaisesRegexp(ValueError, ""in both shapes must be equal""):\n      network(input_graph)\n\n  @parameterized.named_parameters(\n      (""mismatched nodes"", True, True, False, True, ""nodes""),\n      (""mismatched edges"", False, False, True, True, ""edges""),\n      (""mismatched globals"", True, True, True, False, ""globals""),\n  )\n  def test_incompatible_higher_rank_inputs_no_raise(self,\n                                                    use_received_edges,\n                                                    use_sent_edges,\n                                                    use_nodes,\n                                                    use_globals,\n                                                    field):\n    """"""No exception should occur if a differently shapped field is not used.""""""\n    input_graph = self._get_shaped_input_graph()\n    input_graph = input_graph.replace(\n        **{field: tf.transpose(getattr(input_graph, field), [0, 2, 1, 3])})\n    network = blocks.NodeBlock(\n        functools.partial(snt.Conv2D, output_channels=10, kernel_shape=[3, 3]),\n        use_received_edges=use_received_edges,\n        use_sent_edges=use_sent_edges,\n        use_nodes=use_nodes,\n        use_globals=use_globals\n    )\n    self._assert_build_and_run(network, input_graph)\n\n  @parameterized.named_parameters(\n      (""no edges"", False, True, True, ""edges""),\n      (""no nodes"", True, False, True, ""nodes""),\n      (""no globals"", True, True, False, ""globals""),\n  )\n  def test_unused_field_can_be_none(\n      self, use_edges, use_nodes, use_globals, none_field):\n    """"""Checks that computation can handle non-necessary fields left None.""""""\n    input_graph = self._get_input_graph([none_field])\n    node_block = blocks.NodeBlock(\n        node_model_fn=self._node_model_fn,\n        use_received_edges=use_edges,\n        use_sent_edges=use_edges,\n        use_nodes=use_nodes,\n        use_globals=use_globals)\n    output_graph = node_block(input_graph)\n\n    model_inputs = []\n    if use_edges:\n      model_inputs.append(\n          blocks.ReceivedEdgesToNodesAggregator(\n              tf.unsorted_segment_sum)(input_graph))\n      model_inputs.append(\n          blocks.SentEdgesToNodesAggregator(\n              tf.unsorted_segment_sum)(input_graph))\n    if use_nodes:\n      model_inputs.append(input_graph.nodes)\n    if use_globals:\n      model_inputs.append(blocks.broadcast_globals_to_nodes(input_graph))\n\n    model_inputs = tf.concat(model_inputs, axis=-1)\n    self.assertEqual(input_graph.edges, output_graph.edges)\n    self.assertEqual(input_graph.globals, output_graph.globals)\n\n    with tf.Session() as sess:\n      actual_nodes, model_inputs_out = sess.run(\n          (output_graph.nodes, model_inputs))\n\n    expected_output_nodes = model_inputs_out * self._scale\n    self.assertNDArrayNear(expected_output_nodes, actual_nodes, err=1e-4)\n\n  def test_no_input_raises_exception(self):\n    """"""Checks that receiving no input raises an exception.""""""\n    with self.assertRaisesRegexp(ValueError, ""At least one of ""):\n      blocks.NodeBlock(\n          node_model_fn=self._node_model_fn,\n          use_received_edges=False,\n          use_sent_edges=False,\n          use_nodes=False,\n          use_globals=False)\n\n\nclass GlobalBlockTest(GraphModuleTest):\n  """"""Tests for the GlobalBlock.""""""\n\n  def setUp(self):\n    super(GlobalBlockTest, self).setUp()\n    self._scale = 10.\n    self._global_model_fn = lambda: lambda features: features * self._scale\n\n  @parameterized.named_parameters(\n      (""all_inputs, custom reductions"",\n       True, True, True, tf.unsorted_segment_sum, tf.unsorted_segment_mean),\n      (""edges only, blocks reducer"",\n       True, False, False, blocks.unsorted_segment_max_or_zero, None),\n      (""nodes only, custom reduction"",\n       False, True, False, None, tf.unsorted_segment_prod),\n      (""globals only"",\n       False, False, True, None, None),\n      (""edges and nodes, blocks reducer"",\n       True, True, False, blocks.unsorted_segment_min_or_zero,\n       tf.unsorted_segment_prod),\n      (""nodes and globals, blocks reducer"",\n       False, True, True, None, blocks.unsorted_segment_min_or_zero),\n  )\n  def test_output_values(\n      self, use_edges, use_nodes, use_globals, edges_reducer, nodes_reducer):\n    """"""Compares the output of a GlobalBlock to an explicit computation.""""""\n    input_graph = self._get_input_graph()\n    global_block = blocks.GlobalBlock(\n        global_model_fn=self._global_model_fn,\n        use_edges=use_edges,\n        use_nodes=use_nodes,\n        use_globals=use_globals,\n        edges_reducer=edges_reducer,\n        nodes_reducer=nodes_reducer)\n    output_graph = global_block(input_graph)\n\n    model_inputs = []\n    if use_edges:\n      model_inputs.append(\n          blocks.EdgesToGlobalsAggregator(edges_reducer)(input_graph))\n    if use_nodes:\n      model_inputs.append(\n          blocks.NodesToGlobalsAggregator(nodes_reducer)(input_graph))\n    if use_globals:\n      model_inputs.append(input_graph.globals)\n\n    model_inputs = tf.concat(model_inputs, axis=-1)\n    self.assertEqual(input_graph.edges, output_graph.edges)\n    self.assertEqual(input_graph.nodes, output_graph.nodes)\n\n    with tf.Session() as sess:\n      output_graph_out, model_inputs_out = sess.run(\n          (output_graph, model_inputs))\n\n    expected_output_globals = model_inputs_out * self._scale\n    self.assertNDArrayNear(\n        expected_output_globals, output_graph_out.globals, err=1e-4)\n\n  @parameterized.named_parameters(\n      (""default"", True, True, True, 10),\n      (""use edges only"", True, False, False, 4),\n      (""use nodes only"", False, True, False, 2),\n      (""use globals only"", False, False, True, 4),\n  )\n  def test_created_variables(self, use_edges, use_nodes,\n                             use_globals, expected_first_dim_w):\n    """"""Verifies the variable names and shapes created by a GlobalBlock.""""""\n    output_size = 10\n    expected_var_shapes_dict = {\n        ""global_block/mlp/linear_0/b:0"": [output_size],\n        ""global_block/mlp/linear_0/w:0"": [expected_first_dim_w, output_size]}\n\n    input_graph = self._get_input_graph()\n\n    global_block = blocks.GlobalBlock(\n        global_model_fn=functools.partial(snt.nets.MLP,\n                                          output_sizes=[output_size]),\n        use_edges=use_edges,\n        use_nodes=use_nodes,\n        use_globals=use_globals)\n\n    global_block(input_graph)\n\n    variables = global_block.get_variables()\n    var_shapes_dict = {var.name: var.get_shape().as_list() for var in variables}\n    self.assertDictEqual(expected_var_shapes_dict, var_shapes_dict)\n\n  @parameterized.named_parameters(\n      (""missing edges"", True, False, False, ""edges""),\n      (""missing nodes"", False, True, False, ""nodes""),\n      (""missing globals"", False, False, True, ""globals""),\n  )\n  def test_missing_field_raises_exception(\n      self, use_edges, use_nodes, use_globals, none_field):\n    """"""Checks that missing a required field raises an exception.""""""\n    input_graph = self._get_input_graph([none_field])\n    global_block = blocks.GlobalBlock(\n        global_model_fn=self._global_model_fn,\n        use_edges=use_edges,\n        use_nodes=use_nodes,\n        use_globals=use_globals)\n    with self.assertRaisesRegexp(ValueError, ""field cannot be None""):\n      global_block(input_graph)\n\n  @parameterized.named_parameters(\n      (""no edges"", False, True, True, ""edges""),\n      (""no nodes"", True, False, True, ""nodes""),\n      (""no globals"", True, True, False, ""globals""),\n  )\n  def test_unused_field_can_be_none(\n      self, use_edges, use_nodes, use_globals, none_field):\n    """"""Checks that computation can handle non-necessary fields left None.""""""\n    input_graph = self._get_input_graph([none_field])\n    global_block = blocks.GlobalBlock(\n        global_model_fn=self._global_model_fn,\n        use_edges=use_edges,\n        use_nodes=use_nodes,\n        use_globals=use_globals)\n    output_graph = global_block(input_graph)\n\n    model_inputs = []\n    if use_edges:\n      model_inputs.append(\n          blocks.EdgesToGlobalsAggregator(tf.unsorted_segment_sum)(input_graph))\n    if use_nodes:\n      model_inputs.append(\n          blocks.NodesToGlobalsAggregator(tf.unsorted_segment_sum)(input_graph))\n    if use_globals:\n      model_inputs.append(input_graph.globals)\n\n    model_inputs = tf.concat(model_inputs, axis=-1)\n    self.assertEqual(input_graph.edges, output_graph.edges)\n    self.assertEqual(input_graph.nodes, output_graph.nodes)\n\n    with tf.Session() as sess:\n      actual_globals, model_inputs_out = sess.run(\n          (output_graph.globals, model_inputs))\n\n    expected_output_globals = model_inputs_out * self._scale\n    self.assertNDArrayNear(expected_output_globals, actual_globals, err=1e-4)\n\n  def test_compatible_higher_rank_no_raise(self):\n    """"""No exception should occur with higher ranks tensors.""""""\n    input_graph = self._get_shaped_input_graph()\n    input_graph = input_graph.map(lambda v: tf.transpose(v, [0, 2, 1, 3]))\n    network = blocks.GlobalBlock(\n        functools.partial(snt.Conv2D, output_channels=10, kernel_shape=[3, 3]))\n    self._assert_build_and_run(network, input_graph)\n\n  @parameterized.named_parameters(\n      (""mismatched nodes and edges"", True, True, False, ""edges""),\n      (""mismatched edges and globals"", True, False, True, ""globals""),\n      (""mismatched nodes and globals"", False, True, True, ""globals""),\n  )\n  def test_incompatible_higher_rank_inputs_raises(self,\n                                                  use_edges,\n                                                  use_nodes,\n                                                  use_globals,\n                                                  field):\n    """"""A exception should be raised if the inputs have incompatible shapes.""""""\n    input_graph = self._get_shaped_input_graph()\n    input_graph = input_graph.replace(\n        **{field: tf.transpose(getattr(input_graph, field), [0, 2, 1, 3])})\n    network = blocks.GlobalBlock(\n        functools.partial(snt.Conv2D, output_channels=10, kernel_shape=[3, 3]),\n        use_edges=use_edges,\n        use_nodes=use_nodes,\n        use_globals=use_globals\n    )\n    with self.assertRaisesRegexp(ValueError, ""in both shapes must be equal""):\n      network(input_graph)\n\n  @parameterized.named_parameters(\n      (""mismatched nodes"", True, False, True, ""nodes""),\n      (""mismatched edges"", False, True, True, ""edges""),\n      (""mismatched globals"", True, True, False, ""globals""),\n  )\n  def test_incompatible_higher_rank_inputs_no_raise(self,\n                                                    use_edges,\n                                                    use_nodes,\n                                                    use_globals,\n                                                    field):\n    """"""No exception should occur if a differently shapped field is not used.""""""\n    input_graph = self._get_shaped_input_graph()\n    input_graph = input_graph.replace(\n        **{field: tf.transpose(getattr(input_graph, field), [0, 2, 1, 3])})\n    network = blocks.GlobalBlock(\n        functools.partial(snt.Conv2D, output_channels=10, kernel_shape=[3, 3]),\n        use_edges=use_edges,\n        use_nodes=use_nodes,\n        use_globals=use_globals\n    )\n    self._assert_build_and_run(network, input_graph)\n\n  def test_no_input_raises_exception(self):\n    """"""Checks that receiving no input raises an exception.""""""\n    with self.assertRaisesRegexp(ValueError, ""At least one of ""):\n      blocks.GlobalBlock(\n          global_model_fn=self._global_model_fn,\n          use_edges=False,\n          use_nodes=False,\n          use_globals=False)\n\n  @parameterized.named_parameters(\n      (""missing edges reducer"", True, False, None, tf.unsorted_segment_sum),\n      (""missing nodes reducer"", False, True, tf.unsorted_segment_sum, None),\n  )\n  def test_missing_aggregation_raises_exception(\n      self, use_edges, use_nodes, edges_reducer,\n      nodes_reducer):\n    """"""Checks that missing a required aggregation argument raises an error.""""""\n    with self.assertRaisesRegexp(ValueError, ""should not be None""):\n      blocks.GlobalBlock(\n          global_model_fn=self._global_model_fn,\n          use_edges=use_edges,\n          use_nodes=use_nodes,\n          use_globals=False,\n          edges_reducer=edges_reducer,\n          nodes_reducer=nodes_reducer)\n\n\ndef _mask_leading_dimension(tensor):\n  return tf.placeholder_with_default(tensor,\n                                     [None] + tensor.get_shape().as_list()[1:])\n\n\nclass CommonBlockTests(GraphModuleTest):\n  """"""Tests that are common to the EdgeBlock, NodeBlock and GlobalBlock.""""""\n\n  @parameterized.named_parameters(\n      (""edge block"", blocks.EdgeBlock),\n      (""node block"", blocks.NodeBlock),\n      (""global block"", blocks.GlobalBlock),\n  )\n  def test_dynamic_batch_sizes(self, block_constructor):\n    """"""Checks that all batch sizes are as expected through a GraphNetwork.""""""\n    input_graph = self._get_input_graph()\n    placeholders = input_graph.map(_mask_leading_dimension, graphs.ALL_FIELDS)\n    model = block_constructor(\n        functools.partial(snt.nets.MLP, output_sizes=[10]))\n    output = model(placeholders)\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n      other_input_graph = utils_np.data_dicts_to_graphs_tuple(\n          [SMALL_GRAPH_1, SMALL_GRAPH_2])\n      actual = sess.run(output, {placeholders: other_input_graph})\n    for k, v in other_input_graph._asdict().items():\n      self.assertEqual(v.shape[0], getattr(actual, k).shape[0])\n\n  @parameterized.named_parameters(\n      (""float64 data, edge block"", tf.float64, tf.int32, blocks.EdgeBlock),\n      (""int64 indices, edge block"", tf.float32, tf.int64, blocks.EdgeBlock),\n      (""float64 data, node block"", tf.float64, tf.int32, blocks.NodeBlock),\n      (""int64 indices, node block"", tf.float32, tf.int64, blocks.NodeBlock),\n      (""float64 data, global block"", tf.float64, tf.int32, blocks.GlobalBlock),\n      (""int64 indices, global block"", tf.float32, tf.int64, blocks.GlobalBlock),\n  )\n  def test_dtypes(self, data_dtype, indices_dtype, block_constructor):\n    """"""Checks that all the output types are as expected for blocks.""""""\n    input_graph = self._get_input_graph()\n    input_graph = input_graph.map(lambda v: tf.cast(v, data_dtype),\n                                  [""nodes"", ""edges"", ""globals""])\n    input_graph = input_graph.map(lambda v: tf.cast(v, indices_dtype),\n                                  [""receivers"", ""senders""])\n    model = block_constructor(\n        functools.partial(snt.nets.MLP, output_sizes=[10]))\n    output = model(input_graph)\n    for field in [""nodes"", ""globals"", ""edges""]:\n      self.assertEqual(data_dtype, getattr(output, field).dtype)\n    for field in [""receivers"", ""senders""]:\n      self.assertEqual(indices_dtype, getattr(output, field).dtype)\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
graph_nets/tests/graphs_test.py,2,"b'# Copyright 2018 The GraphNets Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n\n""""""Tests for `graphs.py`.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nfrom absl.testing import parameterized\nfrom graph_nets import graphs\nimport tensorflow as tf\n\n\nclass GraphsTest(tf.test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(GraphsTest, self).setUp()\n    all_fields = graphs.GRAPH_DATA_FIELDS + graphs.GRAPH_NUMBER_FIELDS\n    self.graph = {k: k for k in all_fields}\n\n  @parameterized.named_parameters(\n      (""no n_node"", [""n_node""],),\n      (""no n_edge"", [""n_edge""],),\n      (""receivers but no senders"", [""edges"", ""senders""],),\n      (""senders but no receivers"", [""edges"", ""receivers""],),\n      (""edges but no senders/receivers"", [""receivers"", ""senders""],),\n  )\n  def test_inconsistent_none_fields_raise_error_on_creation(self, none_fields):\n    for none_field in none_fields:\n      self.graph[none_field] = None\n    with self.assertRaisesRegexp(ValueError, none_fields[-1]):\n      graphs.GraphsTuple(**self.graph)\n\n  @parameterized.named_parameters(\n      (""no n_node"", [""n_node""],),\n      (""no n_edge"", [""n_edge""],),\n      (""receivers but no senders"", [""edges"", ""senders""],),\n      (""senders but no receivers"", [""edges"", ""receivers""],),\n      (""edges but no senders/receivers"", [""receivers"", ""senders""],),\n  )\n  def test_inconsistent_none_fields_raise_error_on_replace(self, none_fields):\n    graph = graphs.GraphsTuple(**self.graph)\n    with self.assertRaisesRegexp(ValueError, none_fields[-1]):\n      graph.replace(**{none_field: None for none_field in none_fields})\n\n  @parameterized.named_parameters(\n      (""all fields defined"", [],),\n      (""no node state"", [""nodes""],),\n      (""no edge state"", [""edges""],),\n      (""no global state"", [""globals""],),\n      (""no state"", [""nodes"", ""edges"", ""globals""],),\n      (""no graph"", [""nodes"", ""edges"", ""globals"", ""receivers"", ""senders""],),\n      (""no edges"", [""edges"", ""receivers"", ""senders""],),\n  )\n  def test_creation_with_valid_none_fields(self, none_fields):\n    for none_field in none_fields:\n      self.graph[none_field] = None\n    graph = graphs.GraphsTuple(**self.graph)\n    for k, v in self.graph.items():\n      self.assertEqual(v, getattr(graph, k))\n\n  @parameterized.named_parameters(\n      (""all fields defined"", [],),\n      (""no node state"", [""nodes""],),\n      (""no edge state"", [""edges""],),\n      (""no global state"", [""globals""],),\n      (""no state"", [""nodes"", ""edges"", ""globals""],),\n      (""no graph"", [""nodes"", ""edges"", ""globals"", ""receivers"", ""senders""],),\n      (""no edges"", [""edges"", ""receivers"", ""senders""],),\n  )\n  def test_replace_with_valid_none_fields(self, none_fields):\n    # Create a graph with different values.\n    graph = graphs.GraphsTuple(**{k: v + v for k, v in self.graph.items()})\n    # Update with a graph containing the initial values, or Nones.\n    for none_field in none_fields:\n      self.graph[none_field] = None\n    graph = graph.replace(**self.graph)\n    for k, v in self.graph.items():\n      self.assertEqual(v, getattr(graph, k))\n\n  @parameterized.parameters(\n      ([],),\n      ([""nodes""],),\n      ([""edges""],),\n      ([""globals""],),\n      ([""receivers""],),\n      ([""senders""],),\n      ([""n_node""],),\n      ([""n_edge""],),\n      ([""receivers"", ""senders""],),\n      ([""nodes"", ""edges"", ""globals""],),\n      ([""nodes"", ""edges"", ""globals"", ""receivers"", ""senders"",\n        ""n_node"", ""n_edge""],),\n  )\n  def test_map_fields_as_expected(self, fields_to_map):\n    """"""Tests that the fields are mapped are as expected.""""""\n    graph = graphs.GraphsTuple(**self.graph)\n    graph = graph.map(lambda v: v + v, fields_to_map)\n    for field in graphs.ALL_FIELDS:\n      if field in fields_to_map:\n        self.assertEqual(field + field, getattr(graph, field))\n      else:\n        self.assertEqual(field, getattr(graph, field))\n\n  def test_map_field_called_only_once(self):\n    """"""Tests that the mapping function is called exactly once per field.""""""\n    graph = graphs.GraphsTuple(**self.graph)\n    mapped_fields = []\n    def map_fn(v):\n      mapped_fields.append(v)\n      return v\n    graph = graph.map(map_fn, graphs.ALL_FIELDS)\n    self.assertCountEqual(mapped_fields, graphs.ALL_FIELDS)\n\n  def test_map_field_default_value(self):\n    """"""Tests the default value for the `fields` argument.""""""\n    graph = graphs.GraphsTuple(**self.graph)\n    mapped_fields = []\n    graph = graph.map(mapped_fields.append)\n    self.assertCountEqual(\n        mapped_fields, [graphs.EDGES, graphs.GLOBALS, graphs.NODES])\n\n  def test_map_field_is_parallel(self):\n    """"""Tests that fields are mapped parallelaly, not sequentially.""""""\n    graph = graphs.GraphsTuple(**self.graph)\n    graph = graph.map(lambda v: None, [""edges"", ""receivers"", ""senders""])\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
graph_nets/tests/modules_test.py,97,"b'# Copyright 2018 The GraphNets Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n\n""""""Tests for modules.py.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\n\nfrom absl.testing import parameterized\nfrom graph_nets import blocks\nfrom graph_nets import graphs\nfrom graph_nets import modules\nfrom graph_nets import utils_np\nfrom graph_nets import utils_tf\nimport numpy as np\nimport sonnet as snt\nimport tensorflow as tf\n\n\n\nSMALL_GRAPH_1 = {\n    ""globals"": [1.1, 1.2, 1.3],\n    ""nodes"": [[10.1, 10.2], [20.1, 20.2], [30.1, 30.2]],\n    ""edges"": [[101., 102., 103., 104.], [201., 202., 203., 204.]],\n    ""senders"": [0, 1],\n    ""receivers"": [1, 2],\n}\n\nSMALL_GRAPH_2 = {\n    ""globals"": [-1.1, -1.2, -1.3],\n    ""nodes"": [[-10.1, -10.2], [-20.1, -20.2], [-30.1, -30.2]],\n    ""edges"": [[-101., -102., -103., -104.]],\n    ""senders"": [1,],\n    ""receivers"": [2,],\n}\n\nSMALL_GRAPH_3 = {\n    ""globals"": [1.1, 1.2, 1.3],\n    ""nodes"": [[10.1, 10.2], [20.1, 20.2], [30.1, 30.2]],\n    ""edges"": [[101., 102., 103., 104.], [201., 202., 203., 204.]],\n    ""senders"": [1, 1],\n    ""receivers"": [0, 2],\n}\n\nSMALL_GRAPH_4 = {\n    ""globals"": [1.1, 1.2, 1.3],\n    ""nodes"": [[10.1, 10.2], [20.1, 20.2], [30.1, 30.2]],\n    ""edges"": [[101., 102., 103., 104.], [201., 202., 203., 204.]],\n    ""senders"": [0, 2],\n    ""receivers"": [1, 1],\n}\n\n\ndef _mask_leading_dimension(tensor):\n  return tf.placeholder_with_default(tensor,\n                                     [None] + tensor.get_shape().as_list()[1:])\n\n\nclass GraphModuleTest(tf.test.TestCase, parameterized.TestCase):\n  """"""Base class for all the tests in this file.""""""\n\n  def setUp(self):\n    super(GraphModuleTest, self).setUp()\n    tf.set_random_seed(0)\n\n  def _assert_all_none_or_all_close(self, expected, actual, *args, **kwargs):\n    if expected is None:\n      return self.assertAllEqual(expected, actual)\n    return self.assertAllClose(expected, actual, *args, **kwargs)\n\n  def _get_input_graph(self, none_field=None):\n    input_graph = utils_tf.data_dicts_to_graphs_tuple(\n        [SMALL_GRAPH_1, SMALL_GRAPH_2, SMALL_GRAPH_3, SMALL_GRAPH_4])\n    if none_field:\n      input_graph = input_graph.replace(**{none_field: None})\n    return input_graph\n\n  def _get_shaped_input_graph(self):\n    return graphs.GraphsTuple(\n        nodes=tf.zeros([3, 4, 5, 11], dtype=tf.float32),\n        edges=tf.zeros([5, 4, 5, 12], dtype=tf.float32),\n        globals=tf.zeros([2, 4, 5, 13], dtype=tf.float32),\n        receivers=tf.range(5, dtype=tf.int32) // 3,\n        senders=tf.range(5, dtype=tf.int32) % 3,\n        n_node=tf.constant([2, 1], dtype=tf.int32),\n        n_edge=tf.constant([3, 2], dtype=tf.int32),\n    )\n\n  def _get_shaped_model_fns(self):\n    edge_model_fn = functools.partial(\n        snt.Conv2D, output_channels=10, kernel_shape=[3, 3])\n    node_model_fn = functools.partial(\n        snt.Conv2D, output_channels=8, kernel_shape=[3, 3])\n    global_model_fn = functools.partial(\n        snt.Conv2D, output_channels=7, kernel_shape=[3, 3])\n    return edge_model_fn, node_model_fn, global_model_fn\n\n  def _assert_build_and_run(self, network, input_graph):\n    # No error at construction time.\n    output = network(input_graph)\n    # No error at runtime.\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n      sess.run(output)\n\n\nclass GraphIndependentTest(GraphModuleTest):\n\n  def _get_model(self, name=None):\n    kwargs = {\n        ""edge_model_fn"": functools.partial(snt.nets.MLP, output_sizes=[5]),\n        ""node_model_fn"": functools.partial(snt.nets.MLP, output_sizes=[10]),\n        ""global_model_fn"": functools.partial(snt.nets.MLP, output_sizes=[15]),\n    }\n    if name:\n      kwargs[""name""] = name\n    return modules.GraphIndependent(**kwargs)\n\n  def test_same_as_subblocks(self):\n    """"""Compares the output to explicit subblocks output.""""""\n    input_graph = self._get_input_graph()\n    model = self._get_model()\n    output_graph = model(input_graph)\n\n    expected_output_edges = model._edge_model(input_graph.edges)\n    expected_output_nodes = model._node_model(input_graph.nodes)\n    expected_output_globals = model._global_model(input_graph.globals)\n\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n      (output_graph_out,\n       expected_edges_out, expected_nodes_out, expected_globals_out) = sess.run(\n           (output_graph,\n            expected_output_edges,\n            expected_output_nodes,\n            expected_output_globals))\n\n    self._assert_all_none_or_all_close(expected_edges_out,\n                                       output_graph_out.edges)\n    self._assert_all_none_or_all_close(expected_nodes_out,\n                                       output_graph_out.nodes)\n    self._assert_all_none_or_all_close(expected_globals_out,\n                                       output_graph_out.globals)\n\n  @parameterized.named_parameters(\n      (""default name"", None), (""custom name"", ""custom_name""))\n  def test_created_variables(self, name=None):\n    """"""Verifies variable names and shapes created by a GraphIndependent.""""""\n    name = name if name is not None else ""graph_independent""\n    expected_var_shapes_dict = {\n        name + ""/edge_model/mlp/linear_0/b:0"": [5],\n        name + ""/edge_model/mlp/linear_0/w:0"": [4, 5],\n        name + ""/node_model/mlp/linear_0/b:0"": [10],\n        name + ""/node_model/mlp/linear_0/w:0"": [2, 10],\n        name + ""/global_model/mlp/linear_0/b:0"": [15],\n        name + ""/global_model/mlp/linear_0/w:0"": [3, 15],\n    }\n    input_graph = self._get_input_graph()\n    model = self._get_model(name=name)\n\n    model(input_graph)\n    variables = model.get_variables()\n    var_shapes_dict = {var.name: var.get_shape().as_list() for var in variables}\n    self.assertDictEqual(expected_var_shapes_dict, var_shapes_dict)\n\n  def test_gradient_flow(self):\n    """"""Verifies that gradient flow is as expected.""""""\n    input_graph = self._get_input_graph()\n    model = self._get_model()\n    output_graph = model(input_graph)\n\n    for input_field in [""nodes"", ""edges"", ""globals""]:\n      input_tensor = getattr(input_graph, input_field)\n      for output_field in [""nodes"", ""edges"", ""globals""]:\n        output_tensor = getattr(output_graph, output_field)\n        gradients = tf.gradients(output_tensor, input_tensor)\n        if input_field == output_field:\n          self.assertNotEqual(None, gradients[0])\n        else:\n          self.assertListEqual([None], gradients)\n\n  @parameterized.named_parameters(\n      (""differently shaped edges"", ""edges""),\n      (""differently shaped nodes"", ""nodes""),\n      (""differently shaped globals"", ""globals""),)\n  def test_incompatible_higher_rank_inputs_no_raise(self, field_to_reshape):\n    """"""A GraphIndependent does not make assumptions on its inputs shapes.""""""\n    input_graph = self._get_shaped_input_graph()\n    edge_model_fn, node_model_fn, global_model_fn = self._get_shaped_model_fns()\n    input_graph = input_graph.map(\n        lambda v: tf.transpose(v, [0, 2, 1, 3]), [field_to_reshape])\n    network = modules.GraphIndependent(\n        edge_model_fn, node_model_fn, global_model_fn)\n    self._assert_build_and_run(network, input_graph)\n\n\nclass GraphNetworkTest(GraphModuleTest):\n\n  def _get_model(self):\n    edge_model_fn = functools.partial(snt.Linear, output_size=5)\n    node_model_fn = functools.partial(snt.Linear, output_size=10)\n    global_model_fn = functools.partial(snt.Linear, output_size=15)\n    return modules.GraphNetwork(\n        edge_model_fn=edge_model_fn,\n        node_model_fn=node_model_fn,\n        global_model_fn=global_model_fn)\n\n  @parameterized.named_parameters(\n      (""default name"", None), (""custom name"", ""custom_name""))\n  def test_created_variables(self, name=None):\n    """"""Verifies variable names and shapes created by a GraphNetwork.""""""\n    name = name if name is not None else ""graph_network""\n    expected_var_shapes_dict = {\n        name + ""/edge_block/mlp/linear_0/b:0"": [5],\n        name + ""/edge_block/mlp/linear_0/w:0"": [4 + 4 + 3, 5],\n        name + ""/node_block/mlp/linear_0/b:0"": [10],\n        name + ""/node_block/mlp/linear_0/w:0"": [5 + 2 + 3, 10],\n        name + ""/global_block/mlp/linear_0/b:0"": [15],\n        name + ""/global_block/mlp/linear_0/w:0"": [10 + 5 + 3, 15],\n    }\n    input_graph = self._get_input_graph()\n    extra_kwargs = {""name"": name} if name else {}\n    model = modules.GraphNetwork(\n        edge_model_fn=functools.partial(snt.nets.MLP, output_sizes=[5]),\n        node_model_fn=functools.partial(snt.nets.MLP, output_sizes=[10]),\n        global_model_fn=functools.partial(snt.nets.MLP, output_sizes=[15]),\n        **extra_kwargs)\n\n    model(input_graph)\n    variables = model.get_variables()\n    var_shapes_dict = {var.name: var.get_shape().as_list() for var in variables}\n    self.assertDictEqual(expected_var_shapes_dict, var_shapes_dict)\n\n  @parameterized.named_parameters(\n      (""reduce sum reduction"", tf.unsorted_segment_sum,),\n      (""reduce max or zero reduction"", blocks.unsorted_segment_max_or_zero,),)\n  def test_same_as_subblocks(self, reducer):\n    """"""Compares the output to explicit subblocks output.\n\n    Args:\n      reducer: The reducer used in the `NodeBlock` and `GlobalBlock`.\n    """"""\n    input_graph = self._get_input_graph()\n\n    edge_model_fn = functools.partial(snt.Linear, output_size=5)\n    node_model_fn = functools.partial(snt.Linear, output_size=10)\n    global_model_fn = functools.partial(snt.Linear, output_size=15)\n\n    graph_network = modules.GraphNetwork(\n        edge_model_fn=edge_model_fn,\n        node_model_fn=node_model_fn,\n        global_model_fn=global_model_fn,\n        reducer=reducer)\n\n    output_graph = graph_network(input_graph)\n\n    edge_block = blocks.EdgeBlock(\n        edge_model_fn=lambda: graph_network._edge_block._edge_model,\n        use_sender_nodes=True,\n        use_edges=True,\n        use_receiver_nodes=True,\n        use_globals=True)\n    node_block = blocks.NodeBlock(\n        node_model_fn=lambda: graph_network._node_block._node_model,\n        use_nodes=True,\n        use_sent_edges=False,\n        use_received_edges=True,\n        use_globals=True,\n        received_edges_reducer=reducer)\n    global_block = blocks.GlobalBlock(\n        global_model_fn=lambda: graph_network._global_block._global_model,\n        use_nodes=True,\n        use_edges=True,\n        use_globals=True,\n        edges_reducer=reducer,\n        nodes_reducer=reducer)\n\n    expected_output_edge_block = edge_block(input_graph)\n    expected_output_node_block = node_block(expected_output_edge_block)\n    expected_output_global_block = global_block(expected_output_node_block)\n    expected_edges = expected_output_edge_block.edges\n    expected_nodes = expected_output_node_block.nodes\n    expected_globals = expected_output_global_block.globals\n\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n      (output_graph_out,\n       expected_edges_out, expected_nodes_out, expected_globals_out) = sess.run(\n           (output_graph, expected_edges, expected_nodes, expected_globals))\n\n    self._assert_all_none_or_all_close(expected_edges_out,\n                                       output_graph_out.edges)\n    self._assert_all_none_or_all_close(expected_nodes_out,\n                                       output_graph_out.nodes)\n    self._assert_all_none_or_all_close(expected_globals_out,\n                                       output_graph_out.globals)\n\n  def test_dynamic_batch_sizes(self):\n    """"""Checks that all batch sizes are as expected through a GraphNetwork.""""""\n    input_graph = self._get_input_graph()\n    placeholders = input_graph.map(_mask_leading_dimension, graphs.ALL_FIELDS)\n    model = self._get_model()\n    output = model(placeholders)\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n      other_input_graph = utils_np.data_dicts_to_graphs_tuple(\n          [SMALL_GRAPH_1, SMALL_GRAPH_2])\n      actual = sess.run(output, {placeholders: other_input_graph})\n    for k, v in other_input_graph._asdict().items():\n      self.assertEqual(v.shape[0], getattr(actual, k).shape[0])\n\n  @parameterized.named_parameters(\n      (""float64 data"", tf.float64, tf.int32),\n      (""int64 indices"", tf.float32, tf.int64),)\n  def test_dtypes(self, data_dtype, indices_dtype):\n    """"""Checks that all the output types are as expected in a GraphNetwork.""""""\n    input_graph = self._get_input_graph()\n    input_graph = input_graph.map(lambda v: tf.cast(v, data_dtype),\n                                  [""nodes"", ""globals"", ""edges""])\n    input_graph = input_graph.map(lambda v: tf.cast(v, indices_dtype),\n                                  [""senders"", ""receivers""])\n    model = self._get_model()\n    output = model(input_graph)\n    for field in [""nodes"", ""globals"", ""edges""]:\n      self.assertEqual(data_dtype, getattr(output, field).dtype)\n    for field in [""receivers"", ""senders""]:\n      self.assertEqual(indices_dtype, getattr(output, field).dtype)\n\n  @parameterized.named_parameters(\n      (""edges only"", True, False, False, False),\n      (""receivers only"", False, True, False, False),\n      (""senders only"", False, False, True, False),\n      (""globals only"", False, False, False, True),)\n  def test_edge_block_options(self,\n                              use_edges,\n                              use_receiver_nodes,\n                              use_sender_nodes,\n                              use_globals):\n    """"""Test for configuring the EdgeBlock options.""""""\n    reducer = tf.unsorted_segment_sum\n    input_graph = self._get_input_graph()\n    edge_model_fn = functools.partial(snt.Linear, output_size=10)\n    edge_block_opt = {""use_edges"": use_edges,\n                      ""use_receiver_nodes"": use_receiver_nodes,\n                      ""use_sender_nodes"": use_sender_nodes,\n                      ""use_globals"": use_globals}\n    # Identity node model\n    node_model_fn = lambda: tf.identity\n    node_block_opt = {""use_received_edges"": False,\n                      ""use_sent_edges"": False,\n                      ""use_nodes"": True,\n                      ""use_globals"": False}\n    # Identity global model\n    global_model_fn = lambda: tf.identity\n    global_block_opt = {""use_globals"": True,\n                        ""use_nodes"": False,\n                        ""use_edges"": False}\n\n    graph_network = modules.GraphNetwork(\n        edge_model_fn=edge_model_fn,\n        edge_block_opt=edge_block_opt,\n        node_model_fn=node_model_fn,\n        node_block_opt=node_block_opt,\n        global_model_fn=global_model_fn,\n        global_block_opt=global_block_opt,\n        reducer=reducer)\n\n    output_graph = graph_network(input_graph)\n\n    edge_block = blocks.EdgeBlock(\n        edge_model_fn=lambda: graph_network._edge_block._edge_model,\n        use_edges=use_edges,\n        use_receiver_nodes=use_receiver_nodes,\n        use_sender_nodes=use_sender_nodes,\n        use_globals=use_globals)\n\n    expected_output_edge_block = edge_block(input_graph)\n    expected_output_node_block = expected_output_edge_block\n    expected_output_global_block = expected_output_node_block\n    expected_edges = expected_output_edge_block.edges\n    expected_nodes = expected_output_node_block.nodes\n    expected_globals = expected_output_global_block.globals\n\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n      (output_graph_out,\n       expected_edges_out, expected_nodes_out, expected_globals_out) = sess.run(\n           (output_graph, expected_edges, expected_nodes, expected_globals))\n\n    self._assert_all_none_or_all_close(expected_edges_out,\n                                       output_graph_out.edges)\n    self._assert_all_none_or_all_close(expected_nodes_out,\n                                       output_graph_out.nodes)\n    self._assert_all_none_or_all_close(expected_globals_out,\n                                       output_graph_out.globals)\n\n  @parameterized.named_parameters(\n      (""received edges only"", True, False, False, False, None, None),\n      (""received edges, max reduction"",\n       True, False, False, False, tf.unsorted_segment_max, None),\n      (""sent edges only"", False, True, False, False, None, None),\n      (""sent edges, max reduction"",\n       False, True, False, False, None, tf.unsorted_segment_max),\n      (""nodes only"", False, False, True, False, None, None),\n      (""globals only"", False, False, False, True, None, None),\n  )\n  def test_node_block_options(self,\n                              use_received_edges,\n                              use_sent_edges,\n                              use_nodes,\n                              use_globals,\n                              received_edges_reducer,\n                              sent_edges_reducer):\n    """"""Test for configuring the NodeBlock options.""""""\n    input_graph = self._get_input_graph()\n\n    if use_received_edges:\n      received_edges_reducer = received_edges_reducer or tf.unsorted_segment_sum\n    if use_sent_edges:\n      sent_edges_reducer = sent_edges_reducer or tf.unsorted_segment_sum\n\n    # Identity edge model.\n    edge_model_fn = lambda: tf.identity\n    edge_block_opt = {""use_edges"": True,\n                      ""use_receiver_nodes"": False,\n                      ""use_sender_nodes"": False,\n                      ""use_globals"": False}\n    node_model_fn = functools.partial(snt.Linear, output_size=10)\n    node_block_opt = {""use_received_edges"": use_received_edges,\n                      ""use_sent_edges"": use_sent_edges,\n                      ""use_nodes"": use_nodes,\n                      ""use_globals"": use_globals,\n                      ""received_edges_reducer"": received_edges_reducer,\n                      ""sent_edges_reducer"": sent_edges_reducer}\n    # Identity global model\n    global_model_fn = lambda: tf.identity\n    global_block_opt = {""use_globals"": True,\n                        ""use_nodes"": False,\n                        ""use_edges"": False}\n\n    graph_network = modules.GraphNetwork(\n        edge_model_fn=edge_model_fn,\n        edge_block_opt=edge_block_opt,\n        node_model_fn=node_model_fn,\n        node_block_opt=node_block_opt,\n        global_model_fn=global_model_fn,\n        global_block_opt=global_block_opt)\n\n    output_graph = graph_network(input_graph)\n\n    node_block = blocks.NodeBlock(\n        node_model_fn=lambda: graph_network._node_block._node_model,\n        use_nodes=use_nodes,\n        use_sent_edges=use_sent_edges,\n        use_received_edges=use_received_edges,\n        use_globals=use_globals,\n        received_edges_reducer=received_edges_reducer,\n        sent_edges_reducer=sent_edges_reducer)\n\n    expected_output_edge_block = input_graph\n    expected_output_node_block = node_block(input_graph)\n    expected_output_global_block = expected_output_node_block\n    expected_edges = expected_output_edge_block.edges\n    expected_nodes = expected_output_node_block.nodes\n    expected_globals = expected_output_global_block.globals\n\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n      (output_graph_out,\n       expected_edges_out, expected_nodes_out, expected_globals_out) = sess.run(\n           (output_graph, expected_edges, expected_nodes, expected_globals))\n\n    self._assert_all_none_or_all_close(expected_edges_out,\n                                       output_graph_out.edges)\n    self._assert_all_none_or_all_close(expected_nodes_out,\n                                       output_graph_out.nodes)\n    self._assert_all_none_or_all_close(expected_globals_out,\n                                       output_graph_out.globals)\n\n  @parameterized.named_parameters(\n      (""edges only"", True, False, False, None, None),\n      (""edges only, max"", True, False, False, tf.unsorted_segment_max, None),\n      (""nodes only"", False, True, False, None, None),\n      (""nodes only, max"", False, True, False, None, tf.unsorted_segment_max),\n      (""globals only"", False, False, True, None, None),\n  )\n  def test_global_block_options(self,\n                                use_edges,\n                                use_nodes,\n                                use_globals,\n                                edges_reducer,\n                                nodes_reducer):\n    """"""Test for configuring the NodeBlock options.""""""\n    input_graph = self._get_input_graph()\n\n    if use_edges:\n      edges_reducer = edges_reducer or tf.unsorted_segment_sum\n    if use_nodes:\n      nodes_reducer = nodes_reducer or tf.unsorted_segment_sum\n\n    # Identity edge model.\n    edge_model_fn = lambda: tf.identity\n    edge_block_opt = {""use_edges"": True,\n                      ""use_receiver_nodes"": False,\n                      ""use_sender_nodes"": False,\n                      ""use_globals"": False}\n    # Identity node model\n    node_model_fn = lambda: tf.identity\n    node_block_opt = {""use_received_edges"": False,\n                      ""use_sent_edges"": False,\n                      ""use_nodes"": True,\n                      ""use_globals"": False}\n    global_model_fn = functools.partial(snt.Linear, output_size=10)\n    global_block_opt = {""use_globals"": use_globals,\n                        ""use_nodes"": use_nodes,\n                        ""use_edges"": use_edges,\n                        ""edges_reducer"": edges_reducer,\n                        ""nodes_reducer"": nodes_reducer}\n\n    graph_network = modules.GraphNetwork(\n        edge_model_fn=edge_model_fn,\n        edge_block_opt=edge_block_opt,\n        node_model_fn=node_model_fn,\n        node_block_opt=node_block_opt,\n        global_model_fn=global_model_fn,\n        global_block_opt=global_block_opt)\n\n    output_graph = graph_network(input_graph)\n\n    global_block = blocks.GlobalBlock(\n        global_model_fn=lambda: graph_network._global_block._global_model,\n        use_edges=use_edges,\n        use_nodes=use_nodes,\n        use_globals=use_globals,\n        edges_reducer=edges_reducer,\n        nodes_reducer=nodes_reducer)\n\n    expected_output_edge_block = input_graph\n    expected_output_node_block = expected_output_edge_block\n    expected_output_global_block = global_block(expected_output_node_block)\n    expected_edges = expected_output_edge_block.edges\n    expected_nodes = expected_output_node_block.nodes\n    expected_globals = expected_output_global_block.globals\n\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n      (output_graph_out,\n       expected_edges_out, expected_nodes_out, expected_globals_out) = sess.run(\n           (output_graph, expected_edges, expected_nodes, expected_globals))\n\n    self._assert_all_none_or_all_close(expected_edges_out,\n                                       output_graph_out.edges)\n    self._assert_all_none_or_all_close(expected_nodes_out,\n                                       output_graph_out.nodes)\n    self._assert_all_none_or_all_close(expected_globals_out,\n                                       output_graph_out.globals)\n\n  def test_higher_rank_outputs(self):\n    """"""Tests that a graph net can be build with higher rank inputs/outputs.""""""\n    input_graph = self._get_shaped_input_graph()\n    network = modules.GraphNetwork(*self._get_shaped_model_fns())\n    self._assert_build_and_run(network, input_graph)\n\n  @parameterized.named_parameters(\n      (""wrongly shaped edges"", ""edges""),\n      (""wrongly shaped nodes"", ""nodes""),\n      (""wrongly shaped globals"", ""globals""),)\n  def test_incompatible_higher_rank_inputs_raises(self, field_to_reshape):\n    """"""A exception should be raised if the inputs have incompatible shapes.""""""\n    input_graph = self._get_shaped_input_graph()\n    edge_model_fn, node_model_fn, global_model_fn = self._get_shaped_model_fns()\n    input_graph = input_graph.map(\n        lambda v: tf.transpose(v, [0, 2, 1, 3]), [field_to_reshape])\n    graph_network = modules.GraphNetwork(\n        edge_model_fn, node_model_fn, global_model_fn)\n    with self.assertRaisesRegexp(ValueError, ""in both shapes must be equal""):\n      graph_network(input_graph)\n\n  def test_incompatible_higher_rank_partial_outputs_raises(self):\n    """"""A error should be raised if partial outputs have incompatible shapes.""""""\n    input_graph = self._get_shaped_input_graph()\n    edge_model_fn, node_model_fn, global_model_fn = self._get_shaped_model_fns()\n    edge_model_fn_2 = functools.partial(\n        snt.Conv2D, output_channels=10, kernel_shape=[3, 3], stride=[1, 2])\n    graph_network = modules.GraphNetwork(\n        edge_model_fn_2, node_model_fn, global_model_fn)\n    with self.assertRaisesRegexp(ValueError, ""in both shapes must be equal""):\n      graph_network(input_graph)\n    node_model_fn_2 = functools.partial(\n        snt.Conv2D, output_channels=10, kernel_shape=[3, 3], stride=[1, 2])\n    graph_network = modules.GraphNetwork(\n        edge_model_fn, node_model_fn_2, global_model_fn)\n    with self.assertRaisesRegexp(ValueError, ""in both shapes must be equal""):\n      graph_network(input_graph)\n\n\nclass InteractionNetworkTest(GraphModuleTest):\n\n  def _get_model(self, reducer=None, name=None):\n    kwargs = {\n        ""edge_model_fn"": functools.partial(snt.Linear, output_size=5),\n        ""node_model_fn"": functools.partial(snt.Linear, output_size=10)\n    }\n    if reducer:\n      kwargs[""reducer""] = reducer\n    if name:\n      kwargs[""name""] = name\n    return modules.InteractionNetwork(**kwargs)\n\n  @parameterized.named_parameters(\n      (""default name"", None), (""custom name"", ""custom_name""))\n  def test_created_variables(self, name=None):\n    """"""Verifies variable names and shapes created by an InteractionNetwork.""""""\n    name = name if name is not None else ""interaction_network""\n    expected_var_shapes_dict = {\n        name + ""/edge_block/linear/b:0"": [5],\n        name + ""/edge_block/linear/w:0"": [2 + 2 + 4, 5],\n        name + ""/node_block/linear/b:0"": [10],\n        name + ""/node_block/linear/w:0"": [5 + 2, 10],\n    }\n    input_graph = utils_tf.data_dicts_to_graphs_tuple([SMALL_GRAPH_1])\n    model = self._get_model(name=name)\n\n    model(input_graph)\n    variables = model.get_variables()\n    var_shapes_dict = {var.name: var.get_shape().as_list() for var in variables}\n    self.assertDictEqual(expected_var_shapes_dict, var_shapes_dict)\n\n  @parameterized.named_parameters(\n      (""default"", tf.unsorted_segment_sum,),\n      (""max or zero reduction"", blocks.unsorted_segment_max_or_zero,),\n      (""no globals"", tf.unsorted_segment_sum, ""globals""),\n  )\n  def test_same_as_subblocks(self, reducer, none_field=None):\n    """"""Compares the output to explicit subblocks output.\n\n    Args:\n      reducer: The reducer used in the `NodeBlock`s.\n      none_field: (string, default=None) If not None, the corresponding field\n        is removed from the input graph.\n    """"""\n    input_graph = self._get_input_graph(none_field)\n\n    interaction_network = self._get_model(reducer)\n    output_graph = interaction_network(input_graph)\n    edges_out = output_graph.edges\n    nodes_out = output_graph.nodes\n    self.assertAllEqual(input_graph.globals, output_graph.globals)\n\n    edge_block = blocks.EdgeBlock(\n        edge_model_fn=lambda: interaction_network._edge_block._edge_model,\n        use_sender_nodes=True,\n        use_edges=True,\n        use_receiver_nodes=True,\n        use_globals=False)\n    node_block = blocks.NodeBlock(\n        node_model_fn=lambda: interaction_network._node_block._node_model,\n        use_nodes=True,\n        use_sent_edges=False,\n        use_received_edges=True,\n        use_globals=False,\n        received_edges_reducer=reducer)\n\n    expected_output_edge_block = edge_block(input_graph)\n    expected_output_node_block = node_block(expected_output_edge_block)\n    expected_edges = expected_output_edge_block.edges\n    expected_nodes = expected_output_node_block.nodes\n\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n      (actual_edges_out, actual_nodes_out,\n       expected_edges_out, expected_nodes_out) = sess.run(\n           [edges_out, nodes_out, expected_edges, expected_nodes])\n\n    self._assert_all_none_or_all_close(expected_edges_out, actual_edges_out)\n    self._assert_all_none_or_all_close(expected_nodes_out, actual_nodes_out)\n\n  @parameterized.named_parameters(\n      (""no nodes"", [""nodes""],),\n      (""no edge data"", [""edges""],),\n      (""no edges"", [""edges"", ""receivers"", ""senders""],),\n  )\n  def test_field_must_not_be_none(self, none_fields):\n    """"""Tests that the model cannot be built if required fields are missing.""""""\n    input_graph = utils_tf.data_dicts_to_graphs_tuple([SMALL_GRAPH_1])\n    input_graph = input_graph.map(lambda _: None, none_fields)\n    interaction_network = self._get_model()\n    with self.assertRaises(ValueError):\n      interaction_network(input_graph)\n\n  def test_higher_rank_outputs(self):\n    """"""Tests that an IN can be build with higher rank inputs/outputs.""""""\n    input_graph = self._get_shaped_input_graph()\n    edge_model_fn, node_model_fn, _ = self._get_shaped_model_fns()\n    graph_network = modules.InteractionNetwork(edge_model_fn, node_model_fn)\n    self._assert_build_and_run(graph_network, input_graph)\n\n  @parameterized.named_parameters(\n      (""wrongly shaped edges"", ""edges""),\n      (""wrongly shaped nodes"", ""nodes""),)\n  def test_incompatible_higher_rank_inputs_raises(self, field_to_reshape):\n    """"""Am exception should be raised if the inputs have incompatible shapes.""""""\n    input_graph = self._get_shaped_input_graph()\n    edge_model_fn, node_model_fn, _ = self._get_shaped_model_fns()\n    input_graph = input_graph.map(\n        lambda v: tf.transpose(v, [0, 2, 1, 3]), [field_to_reshape])\n    graph_network = modules.InteractionNetwork(edge_model_fn, node_model_fn)\n    with self.assertRaisesRegexp(ValueError, ""in both shapes must be equal""):\n      graph_network(input_graph)\n\n  def test_incompatible_higher_rank_inputs_no_raise(self):\n    """"""The globals can have an arbitrary shape in the input.""""""\n    input_graph = self._get_shaped_input_graph()\n    edge_model_fn, node_model_fn, _ = self._get_shaped_model_fns()\n    input_graph = input_graph.replace(\n        globals=tf.transpose(input_graph.globals, [0, 2, 1, 3]))\n    graph_network = modules.InteractionNetwork(edge_model_fn, node_model_fn)\n    self._assert_build_and_run(graph_network, input_graph)\n\n\nclass RelationNetworkTest(GraphModuleTest):\n\n  def _get_model(self, reducer=tf.unsorted_segment_sum, name=None):\n    kwargs = {\n        ""edge_model_fn"": functools.partial(snt.Linear, output_size=5),\n        ""global_model_fn"": functools.partial(snt.Linear, output_size=15)\n    }\n    if reducer:\n      kwargs[""reducer""] = reducer\n    if name:\n      kwargs[""name""] = name\n    return modules.RelationNetwork(**kwargs)\n\n  @parameterized.named_parameters(\n      (""default name"", None), (""custom name"", ""custom_name""))\n  def test_created_variables(self, name=None):\n    """"""Verifies variable names and shapes created by a RelationNetwork.""""""\n    name = name if name is not None else ""relation_network""\n    expected_var_shapes_dict = {\n        name + ""/edge_block/linear/b:0"": [5],\n        name + ""/edge_block/linear/w:0"": [2 + 2, 5],\n        name + ""/global_block/linear/b:0"": [15],\n        name + ""/global_block/linear/w:0"": [5, 15],\n    }\n    input_graph = utils_tf.data_dicts_to_graphs_tuple([SMALL_GRAPH_1])\n    model = self._get_model(name=name)\n\n    model(input_graph)\n    variables = model.get_variables()\n    var_shapes_dict = {var.name: var.get_shape().as_list() for var in variables}\n    self.assertDictEqual(expected_var_shapes_dict, var_shapes_dict)\n\n  @parameterized.named_parameters(\n      (""default"", tf.unsorted_segment_sum, None),\n      (""max or zero reduction"", blocks.unsorted_segment_max_or_zero, None),\n      (""no edges"", tf.unsorted_segment_sum, ""edges""),\n      (""no globals"", tf.unsorted_segment_sum, ""globals""),\n  )\n  def test_same_as_subblocks(self, reducer, none_field=None):\n    """"""Compares the output to explicit subblocks output.\n\n    Args:\n      reducer: The reducer used in the `GlobalBlock`.\n      none_field: (string, default=None) If not None, the corresponding field\n        is removed from the input graph.\n    """"""\n    input_graph = self._get_input_graph(none_field)\n    relation_network = self._get_model(reducer)\n    output_graph = relation_network(input_graph)\n\n    edge_block = blocks.EdgeBlock(\n        edge_model_fn=lambda: relation_network._edge_block._edge_model,\n        use_edges=False,\n        use_receiver_nodes=True,\n        use_sender_nodes=True,\n        use_globals=False)\n    global_block = blocks.GlobalBlock(\n        global_model_fn=lambda: relation_network._global_block._global_model,\n        use_edges=True,\n        use_nodes=False,\n        use_globals=False,\n        edges_reducer=reducer,\n        nodes_reducer=reducer)\n\n    expected_output_edge_block = edge_block(input_graph)\n    expected_output_global_block = global_block(expected_output_edge_block)\n\n    self.assertEqual(input_graph.edges, output_graph.edges)\n    self.assertEqual(input_graph.nodes, output_graph.nodes)\n\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n      (actual_globals_out, expected_globals_out) = sess.run(\n          (output_graph.globals, expected_output_global_block.globals))\n\n    self._assert_all_none_or_all_close(expected_globals_out, actual_globals_out)\n\n  @parameterized.named_parameters(\n      (""no nodes"", [""nodes""],), (""no edges"", [""edges"", ""receivers"", ""senders""],)\n  )\n  def test_field_must_not_be_none(self, none_fields):\n    """"""Tests that the model cannot be built if required fields are missing.""""""\n    input_graph = utils_tf.data_dicts_to_graphs_tuple([SMALL_GRAPH_1])\n    input_graph = input_graph.map(lambda _: None, none_fields)\n    relation_network = self._get_model()\n    with self.assertRaises(ValueError):\n      relation_network(input_graph)\n\n  @parameterized.named_parameters(\n      (""differently shaped edges"", ""edges""),\n      (""differently shaped nodes"", ""nodes""),\n      (""differently shaped globals"", ""globals""),)\n  def test_incompatible_higher_rank_inputs_no_raise(self, field_to_reshape):\n    """"""A RelationNetwork does not make assumptions on its inputs shapes.""""""\n    input_graph = self._get_shaped_input_graph()\n    edge_model_fn, _, global_model_fn = self._get_shaped_model_fns()\n    input_graph = input_graph.map(\n        lambda v: tf.transpose(v, [0, 2, 1, 3]), [field_to_reshape])\n    network = modules.RelationNetwork(edge_model_fn, global_model_fn)\n    self._assert_build_and_run(network, input_graph)\n\n\nclass DeepSetsTest(GraphModuleTest):\n\n  def _get_model(self, reducer=None, name=None):\n    kwargs = {\n        ""node_model_fn"": functools.partial(snt.Linear, output_size=5),\n        ""global_model_fn"": functools.partial(snt.Linear, output_size=15)\n    }\n    if reducer:\n      kwargs[""reducer""] = reducer\n    if name:\n      kwargs[""name""] = name\n    return modules.DeepSets(**kwargs)\n\n  @parameterized.named_parameters(\n      (""default name"", None), (""custom name"", ""custom_name""))\n  def test_created_variables(self, name=None):\n    """"""Verifies variable names and shapes created by a DeepSets network.""""""\n    name = name if name is not None else ""deep_sets""\n    expected_var_shapes_dict = {\n        name + ""/node_block/linear/b:0"": [5],\n        name + ""/node_block/linear/w:0"": [2 + 3, 5],\n        name + ""/global_block/linear/b:0"": [15],\n        name + ""/global_block/linear/w:0"": [5, 15],\n    }\n    input_graph = self._get_input_graph()\n    model = self._get_model(name=name)\n\n    model(input_graph)\n    variables = model.get_variables()\n    var_shapes_dict = {var.name: var.get_shape().as_list() for var in variables}\n    self.assertDictEqual(expected_var_shapes_dict, var_shapes_dict)\n\n  @parameterized.named_parameters(\n      (""default"", tf.unsorted_segment_sum, []),\n      (""no edge data"", tf.unsorted_segment_sum, [""edges""]),\n      (""no edges"", tf.unsorted_segment_sum, [""edges"", ""receivers"", ""senders""]),\n      (""max or zero reduction"", blocks.unsorted_segment_max_or_zero, []),\n  )\n  def test_same_as_subblocks(self, reducer, none_fields):\n    """"""Compares the output to explicit subblocks output.\n\n    Args:\n      reducer: The reducer used in the NodeBlock.\n      none_fields: (list of strings) The corresponding fields are removed from\n        the input graph.\n    """"""\n    input_graph = self._get_input_graph()\n    input_graph = input_graph.map(lambda _: None, none_fields)\n\n    deep_sets = self._get_model(reducer)\n\n    output_graph = deep_sets(input_graph)\n    output_nodes = output_graph.nodes\n    output_globals = output_graph.globals\n\n    node_block = blocks.NodeBlock(\n        node_model_fn=lambda: deep_sets._node_block._node_model,\n        use_received_edges=False,\n        use_sent_edges=False,\n        use_nodes=True,\n        use_globals=True)\n    global_block = blocks.GlobalBlock(\n        global_model_fn=lambda: deep_sets._global_block._global_model,\n        use_edges=False,\n        use_nodes=True,\n        use_globals=False,\n        nodes_reducer=reducer)\n\n    node_block_out = node_block(input_graph)\n    expected_nodes = node_block_out.nodes\n    expected_globals = global_block(node_block_out).globals\n\n    self.assertAllEqual(input_graph.edges, output_graph.edges)\n    self.assertAllEqual(input_graph.receivers, output_graph.receivers)\n    self.assertAllEqual(input_graph.senders, output_graph.senders)\n\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n      (output_nodes_, output_globals_, expected_nodes_,\n       expected_globals_) = sess.run(\n           [output_nodes, output_globals, expected_nodes, expected_globals])\n\n    self._assert_all_none_or_all_close(expected_nodes_, output_nodes_)\n    self._assert_all_none_or_all_close(expected_globals_, output_globals_)\n\n  @parameterized.parameters(\n      (""nodes"",), (""globals"",),\n  )\n  def test_field_must_not_be_none(self, none_field):\n    """"""Tests that the model cannot be built if required fields are missing.""""""\n    input_graph = utils_tf.data_dicts_to_graphs_tuple([SMALL_GRAPH_1])\n    input_graph = input_graph.replace(**{none_field: None})\n    deep_sets = self._get_model()\n    with self.assertRaises(ValueError):\n      deep_sets(input_graph)\n\n  def test_incompatible_higher_rank_inputs_raises(self):\n    """"""A exception should be raised if the inputs have incompatible shapes.""""""\n    input_graph = self._get_shaped_input_graph()\n    _, node_model_fn, global_model_fn = self._get_shaped_model_fns()\n    input_graph = input_graph.replace(\n        nodes=tf.transpose(input_graph.nodes, [0, 2, 1, 3]))\n    graph_network = modules.DeepSets(node_model_fn, global_model_fn)\n    with self.assertRaisesRegexp(ValueError, ""in both shapes must be equal""):\n      graph_network(input_graph)\n\n  def test_incompatible_higher_rank_partial_outputs_no_raise(self):\n    """"""There is no constraint on the size of the partial outputs.""""""\n    input_graph = self._get_shaped_input_graph()\n    node_model_fn = functools.partial(\n        snt.Conv2D, output_channels=10, kernel_shape=[3, 3], stride=[1, 2])\n    global_model_fn = functools.partial(\n        snt.Conv2D, output_channels=10, kernel_shape=[3, 3])\n    network = modules.DeepSets(node_model_fn, global_model_fn)\n    self._assert_build_and_run(network, input_graph)\n\n  def test_incompatible_higher_rank_inputs_no_raise(self):\n    """"""A DeepSets does not make assumptions on the shape if its input edges.""""""\n    input_graph = self._get_shaped_input_graph()\n    _, node_model_fn, global_model_fn = self._get_shaped_model_fns()\n    input_graph = input_graph.replace(\n        edges=tf.transpose(input_graph.edges, [0, 2, 1, 3]))\n    network = modules.DeepSets(node_model_fn, global_model_fn)\n    self._assert_build_and_run(network, input_graph)\n\n\nclass CommNetTest(GraphModuleTest):\n\n  def _get_model(self, reducer=None, name=None):\n    kwargs = {\n        ""edge_model_fn"": functools.partial(snt.Linear, output_size=15),\n        ""node_encoder_model_fn"": functools.partial(snt.Linear, output_size=8),\n        ""node_model_fn"": functools.partial(snt.Linear, output_size=5),\n    }\n    if reducer is not None:\n      kwargs[""reducer""] = reducer\n    if name:\n      kwargs[""name""] = name\n    return modules.CommNet(**kwargs)\n\n  @parameterized.named_parameters(\n      (""default name"", None), (""custom name"", ""custom_name""))\n  def test_created_variables(self, name=None):\n    """"""Verifies variable names and shapes created by a DeepSets network.""""""\n    name = name if name is not None else ""comm_net""\n    expected_var_shapes_dict = {\n        name + ""/edge_block/linear/b:0"": [15],\n        name + ""/edge_block/linear/w:0"": [2, 15],\n        name + ""/node_encoder_block/linear/b:0"": [8],\n        name + ""/node_encoder_block/linear/w:0"": [2, 8],\n        name + ""/node_block/linear/b:0"": [5],\n        name + ""/node_block/linear/w:0"": [15 + 8, 5],\n    }\n    input_graph = self._get_input_graph()\n    model = self._get_model(name=name)\n\n    model(input_graph)\n    variables = model.get_variables()\n    var_shapes_dict = {var.name: var.get_shape().as_list() for var in variables}\n    self.assertDictEqual(expected_var_shapes_dict, var_shapes_dict)\n\n  @parameterized.named_parameters(\n      (""default"", tf.unsorted_segment_sum,),\n      (""no edges"", tf.unsorted_segment_sum, ""edges""),\n      (""no globals"", tf.unsorted_segment_sum, ""globals""),\n      (""max or zero reduction"", blocks.unsorted_segment_max_or_zero,),\n  )\n  def test_same_as_subblocks(self, reducer, none_field=None):\n    """"""Compares the output to explicit subblocks output.\n\n    Args:\n      reducer: The reducer used in the `NodeBlock`s.\n      none_field: (string, default=None) If not None, the corresponding field\n        is removed from the input graph.\n    """"""\n    input_graph = self._get_input_graph(none_field)\n\n    comm_net = self._get_model(reducer)\n    output_graph = comm_net(input_graph)\n    output_nodes = output_graph.nodes\n\n    edge_subblock = blocks.EdgeBlock(\n        edge_model_fn=lambda: comm_net._edge_block._edge_model,\n        use_edges=False,\n        use_receiver_nodes=False,\n        use_sender_nodes=True,\n        use_globals=False)\n    node_encoder_subblock = blocks.NodeBlock(\n        node_model_fn=lambda: comm_net._node_encoder_block._node_model,\n        use_received_edges=False,\n        use_sent_edges=False,\n        use_nodes=True,\n        use_globals=False,\n        received_edges_reducer=reducer)\n    node_subblock = blocks.NodeBlock(\n        node_model_fn=lambda: comm_net._node_block._node_model,\n        use_received_edges=True,\n        use_sent_edges=False,\n        use_nodes=True,\n        use_globals=False,\n        received_edges_reducer=reducer)\n\n    edge_block_out = edge_subblock(input_graph)\n    encoded_nodes = node_encoder_subblock(input_graph).nodes\n    node_input_graph = input_graph.replace(\n        edges=edge_block_out.edges, nodes=encoded_nodes)\n    node_block_out = node_subblock(node_input_graph)\n    expected_nodes = node_block_out.nodes\n\n    self.assertAllEqual(input_graph.globals, output_graph.globals)\n    self.assertAllEqual(input_graph.edges, output_graph.edges)\n    self.assertAllEqual(input_graph.receivers, output_graph.receivers,)\n    self.assertAllEqual(input_graph.senders, output_graph.senders)\n\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n      actual_nodes_output, expected_nodes_output = sess.run(\n          [output_nodes, expected_nodes])\n\n    self._assert_all_none_or_all_close(expected_nodes_output,\n                                       actual_nodes_output)\n\n  @parameterized.named_parameters(\n      (""no nodes"", [""nodes""],), (""no edges"", [""edges"", ""receivers"", ""senders""],)\n  )\n  def test_field_must_not_be_none(self, none_fields):\n    """"""Tests that the model cannot be built if required fields are missing.""""""\n    input_graph = utils_tf.data_dicts_to_graphs_tuple([SMALL_GRAPH_1])\n    input_graph = input_graph.map(lambda _: None, none_fields)\n    comm_net = self._get_model()\n    with self.assertRaises(ValueError):\n      comm_net(input_graph)\n\n  def test_higher_rank_outputs(self):\n    """"""Tests that a CommNet can be build with higher rank inputs/outputs.""""""\n    input_graph = self._get_shaped_input_graph()\n    graph_network = modules.CommNet(*self._get_shaped_model_fns())\n    self._assert_build_and_run(graph_network, input_graph)\n\n\nclass SelfAttentionTest(GraphModuleTest):\n\n  def _get_model(self, reducer=None, name=None):\n    kwargs = {\n        ""edge_model_fn"": functools.partial(snt.Linear, output_size=15),\n        ""node_encoder_model_fn"": functools.partial(snt.Linear, output_size=8),\n        ""node_model_fn"": functools.partial(snt.Linear, output_size=5),\n    }\n    if reducer is not None:\n      kwargs[""reducer""] = reducer\n    if name:\n      kwargs[""name""] = name\n    return modules.CommNet(**kwargs)\n\n  LOGITS_1D = [np.log(2), np.log(2), np.log(2), 0., 0., 0.]\n  SOFTMAX_1D = [1., 2/3, 0.5, 0.25, 0.25, 1/3]\n  LOGITS_2D = [[np.log(2), 1.], [np.log(2), 1.], [np.log(2), 1.],\n               [0., 1.], [0., 1.], [0., 1.]]\n  SOFTMAX_2D = [[1., 1.], [2/3, 0.5], [1/2, 1/3],\n                [1/4, 1/3], [1/4, 1/3], [1/3, 0.5]]\n  SENDERS = [0, 2, 2, 3, 4, 3]\n  RECEIVERS = [1, 5, 6, 6, 6, 5]\n  N_NODE = [2, 5]\n  N_EDGE = [1, 5]\n\n  @parameterized.named_parameters(\n      (""one dimensional"", LOGITS_1D, SOFTMAX_1D),\n      (""two dimensional"", LOGITS_2D, SOFTMAX_2D),)\n  def test_unsorted_segment_softmax(self, data, expected_softmax):\n    """"""Verifies variable names and shapes created by a DeepSets network.""""""\n\n    data = tf.constant(data, dtype=tf.float32)\n    segment_ids = tf.constant(self.RECEIVERS, dtype=tf.int32)\n    num_segments = tf.constant(sum(self.N_NODE), dtype=tf.int32)\n\n    actual_softmax = modules._unsorted_segment_softmax(\n        data, segment_ids, num_segments)\n\n    with tf.Session() as sess:\n      actual_softmax_output = sess.run(actual_softmax)\n\n    self.assertAllClose(expected_softmax, actual_softmax_output)\n\n  @parameterized.named_parameters(\n      (""one dimensional"", LOGITS_1D, SOFTMAX_1D,\n       modules._unsorted_segment_softmax),\n      (""two dimensional"", LOGITS_2D, SOFTMAX_2D,\n       modules._unsorted_segment_softmax),)\n  def test_received_edges_normalizer(self, logits,\n                                     expected_normalized, normalizer):\n    graph = graphs.GraphsTuple(\n        nodes=None,\n        edges=logits,\n        globals=None,\n        receivers=tf.constant(self.RECEIVERS, dtype=tf.int32),\n        senders=tf.constant(self.SENDERS, dtype=tf.int32),\n        n_node=tf.constant(self.N_NODE, dtype=tf.int32),\n        n_edge=tf.constant(self.N_EDGE, dtype=tf.int32),\n    )\n    actual_normalized_edges = modules._received_edges_normalizer(\n        graph, normalizer)\n\n    with tf.Session() as sess:\n      actual_normalized_edges_output = sess.run(actual_normalized_edges)\n\n    self.assertAllClose(expected_normalized, actual_normalized_edges_output)\n\n  def test_self_attention(self):\n    # Just one feature per node.\n    values_np = np.arange(sum(self.N_NODE)) + 1.\n    # Multiple heads, one positive values, one negative values.\n    values_np = np.stack([values_np, values_np*-1.], axis=-1)\n    # Multiple features per node, per head, at different scales.\n    values_np = np.stack([values_np, values_np*0.1], axis=-1)\n    values = tf.constant(values_np, dtype=tf.float32)\n\n    keys_np = [\n        [[0.3, 0.4]]*2,  # Irrelevant (only sender to one node)\n        [[0.1, 0.5]]*2,  # Not used (is not a sender)\n        [[1, 0], [0, 1]],\n        [[0, 1], [1, 0]],\n        [[1, 1], [1, 1]],\n        [[0.4, 0.3]]*2,  # Not used (is not a sender)\n        [[0.3, 0.2]]*2]  # Not used (is not a sender)\n    keys = tf.constant(keys_np, dtype=tf.float32)\n\n    queries_np = [\n        [[0.2, 0.7]]*2,  # Not used (is not a receiver)\n        [[0.3, 0.2]]*2,  # Irrelevant (only receives from one node)\n        [[0.2, 0.8]]*2,  # Not used (is not a receiver)\n        [[0.2, 0.4]]*2,  # Not used (is not a receiver)\n        [[0.3, 0.9]]*2,  # Not used (is not a receiver)\n        [[0, np.log(2)], [np.log(3), 0]],\n        [[np.log(2), 0], [0, np.log(3)]]]\n    queries = tf.constant(queries_np, dtype=tf.float32)\n\n    attention_graph = graphs.GraphsTuple(\n        nodes=None,\n        edges=None,\n        globals=None,\n        receivers=tf.constant(self.RECEIVERS, dtype=tf.int32),\n        senders=tf.constant(self.SENDERS, dtype=tf.int32),\n        n_node=tf.constant(self.N_NODE, dtype=tf.int32),\n        n_edge=tf.constant(self.N_EDGE, dtype=tf.int32),)\n\n    self_attention = modules.SelfAttention()\n    output_graph = self_attention(values, keys, queries, attention_graph)\n    mixed_nodes = output_graph.nodes\n\n    with tf.Session() as sess:\n      mixed_nodes_output = sess.run(mixed_nodes)\n\n    expected_mixed_nodes = [\n        [[0., 0.], [0., 0.]],  # Does not receive any edges\n        [[1., 0.1], [-1., -0.1]],  # Only receives from n0.\n        [[0., 0.], [0., 0.]],  # Does not receive any edges\n        [[0., 0.], [0., 0.]],  # Does not receive any edges\n        [[0., 0.], [0., 0.]],  # Does not receive any edges\n        [[11/3, 11/3*0.1],  # Head one, receives from n2(1/3) n3(2/3)\n         [-15/4, -15/4*0.1]],  # Head two, receives from n2(1/4) n3(3/4)\n        [[20/5, 20/5*0.1],   # Head one, receives from n2(2/5) n3(1/5) n4(2/5)\n         [-28/7, -28/7*0.1]],  # Head two, receives from n2(3/7) n3(1/7) n4(3/7)\n    ]\n\n    self.assertAllClose(expected_mixed_nodes, mixed_nodes_output)\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
graph_nets/tests/test_utils.py,5,"b'# Copyright 2018 The GraphNets Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n\n""""""Utilities for `utils_np_test` and `utils_tf_test`.\n\nThis provides a base class for tests involving `graphs.GraphsTuple`\ncontaining either numpy or tensorflow data. This base class is populated with\ntest data and also provides a convenience method for asserting graph equality.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\nimport itertools\n\nfrom graph_nets import graphs\nfrom graph_nets import utils_np\nimport numpy as np\nimport tensorflow as tf\n\n\n@contextlib.contextmanager\ndef assert_new_op_prefixes(test, expected_prefix, assert_some_new_ops=True):\n  """"""Asserts the namescope of tf ops created within the context manager.""""""\n  ops_before = [n.name for n in tf.get_default_graph().as_graph_def().node]\n  yield\n  ops_after = [n.name for n in tf.get_default_graph().as_graph_def().node]\n  new_ops = set(ops_after) - set(ops_before)\n  prefix_length = len(expected_prefix)\n  if assert_some_new_ops:\n    test.assertNotEqual(0, len(new_ops))\n  for op_name in new_ops:\n    test.assertEqual(expected_prefix, op_name[:prefix_length])\n\n\ndef mask_leading_dimension(tensor):\n  return tf.placeholder_with_default(tensor,\n                                     [None] + tensor.get_shape().as_list()[1:])\n\n\nNODES_DIMS = [7, 11]\nEDGES_DIMS = [13, 14]\nGLOBALS_DIMS = [5, 3]\n\n\nclass GraphsTest(tf.test.TestCase):\n  """"""A base class for tests that operate on GraphsNP or GraphsTF.""""""\n\n  def _populate_test_data(self, max_size):\n    """"""Populates the class fields with data used for the tests.\n\n    This creates a batch of graphs with number of nodes from 0 to `num`,\n    number of edges ranging from 1 to `num`, plus an empty graph with no nodes\n    and no edges (so that the total number of graphs is 1 + (num ** (num + 1)).\n\n    The nodes states, edges states and global states of the graphs are\n    created to have different types and shapes.\n\n    Those graphs are stored both as dictionaries (in `self.graphs_dicts_in`,\n    without `n_node` and `n_edge` information, and in `self.graphs_dicts_out`\n    with these two fields filled), and a corresponding numpy\n    `graphs.GraphsTuple` is stored in `self.reference_graph`.\n\n    Args:\n      max_size: The maximum number of nodes and edges (inclusive).\n    """"""\n    filt = lambda x: (x[0] > 0) or (x[1] == 0)\n    n_node, n_edge = zip(*list(\n        filter(filt, itertools.product(\n            range(max_size + 1), range(max_size + 1)))))\n\n    graphs_dicts = []\n    nodes = []\n    edges = []\n    receivers = []\n    senders = []\n    globals_ = []\n\n    def _make_default_state(shape, dtype):\n      return np.arange(np.prod(shape)).reshape(shape).astype(dtype)\n\n    for i, (n_node_, n_edge_) in enumerate(zip(n_node, n_edge)):\n      n = _make_default_state([n_node_,] + NODES_DIMS, ""f4"") + i * 100.\n      e = _make_default_state(\n          [n_edge_,] + EDGES_DIMS, np.float64) + i * 100. + 1000.\n      r = _make_default_state([n_edge_], np.int32) % n_node[i]\n      s = (_make_default_state([n_edge_], np.int32) + 1) % n_node[i]\n      g = _make_default_state(GLOBALS_DIMS, ""f4"") - i * 100. - 1000.\n\n      nodes.append(n)\n      edges.append(e)\n      receivers.append(r)\n      senders.append(s)\n      globals_.append(g)\n      graphs_dict = dict(nodes=n, edges=e, receivers=r, senders=s, globals=g)\n      graphs_dicts.append(graphs_dict)\n\n    # Graphs dicts without n_node / n_edge (to be used as inputs).\n    self.graphs_dicts_in = graphs_dicts\n    # Graphs dicts with n_node / n_node (to be checked against outputs).\n    self.graphs_dicts_out = []\n    for dict_ in self.graphs_dicts_in:\n      completed_dict = dict_.copy()\n      completed_dict[""n_node""] = completed_dict[""nodes""].shape[0]\n      completed_dict[""n_edge""] = completed_dict[""edges""].shape[0]\n      self.graphs_dicts_out.append(completed_dict)\n\n    # pylint: disable=protected-access\n    offset = utils_np._compute_stacked_offsets(n_node, n_edge)\n    # pylint: enable=protected-access\n    self.reference_graph = graphs.GraphsTuple(**dict(\n        nodes=np.concatenate(nodes, axis=0),\n        edges=np.concatenate(edges, axis=0),\n        receivers=np.concatenate(receivers, axis=0) + offset,\n        senders=np.concatenate(senders, axis=0) + offset,\n        globals=np.stack(globals_),\n        n_node=np.array(n_node),\n        n_edge=np.array(n_edge)))\n    self.graphs_dicts = graphs_dicts\n\n  def _assert_graph_equals_np(self, graph0, graph, force_edges_ordering=False):\n    """"""Asserts that all the graph fields of graph0 and graph match.""""""\n    if graph0.nodes is None:\n      self.assertEqual(None, graph.nodes)\n    else:\n      self.assertAllClose(graph0.nodes, graph.nodes)\n    if graph0.globals is None:\n      self.assertEqual(None, graph.globals)\n    else:\n      self.assertAllClose(graph0.globals, graph.globals)\n    self.assertAllClose(graph0.n_node, graph.n_node.tolist())\n    if graph0.receivers is None:\n      self.assertEqual(None, graph.receivers)\n      self.assertEqual(None, graph.senders)\n      self.assertEqual(None, graph.edges)\n      self.assertAllEqual(graph0.n_edge, graph.n_edge)\n      return\n    self.assertAllClose(graph0.n_edge, graph.n_edge.tolist())\n\n    if not force_edges_ordering:\n      self.assertAllClose(graph0.receivers, graph.receivers)\n      self.assertAllClose(graph0.senders, graph.senders)\n      if graph0.edges is not None:\n        self.assertAllClose(graph0.edges, graph.edges)\n      else:\n        self.assertEqual(None, graph.edges)\n      return\n    # To compare edges content, we need to make sure they appear in the same\n    # order\n    if graph0.edges is not None:\n      sorted_receivers0, sorted_senders0, sorted_content0 = zip(\n          *sorted(zip(graph0.receivers, graph0.senders, graph0.edges.tolist())))\n      sorted_receivers, sorted_senders, sorted_content = zip(\n          *sorted(zip(graph.receivers, graph.senders, graph.edges.tolist())))\n      self.assertAllClose(sorted_content0, sorted_content)\n    elif graph.receivers is not None:\n      sorted_receivers0, sorted_senders0 = zip(\n          *sorted(zip(graph0.receivers, graph0.senders)))\n      sorted_receivers, sorted_senders = zip(\n          *sorted(zip(graph.receivers, graph.senders)))\n    else:\n      return\n    self.assertAllClose(sorted_receivers0, sorted_receivers)\n    self.assertAllClose(sorted_senders0, sorted_senders)\n\n  def setUp(self):\n    self._populate_test_data(max_size=2)\n    tf.reset_default_graph()\n'"
graph_nets/tests/utils_np_test.py,1,"b'# Copyright 2018 The GraphNets Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n\n""""""Test for utils_np.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\n\nfrom absl.testing import parameterized\nfrom graph_nets import utils_np\nfrom graph_nets.tests import test_utils\nimport networkx as nx\nimport numpy as np\nfrom six.moves import range\nimport tensorflow as tf\n\n\nclass ConcatenationTest(test_utils.GraphsTest, parameterized.TestCase):\n\n  def test_compute_stacked_offsets(self):\n    sizes = np.array([5, 4, 3, 1, 2, 0, 3, 0, 4, 7])\n    repeats = [2, 2, 0, 2, 1, 3, 2, 0, 3, 2]\n    offsets0 = utils_np._compute_stacked_offsets(sizes, repeats)\n    offsets1 = utils_np._compute_stacked_offsets(sizes, np.array(repeats))\n    expected_offsets = [\n        0, 0, 5, 5, 12, 12, 13, 15, 15, 15, 15, 15, 18, 18, 18, 22, 22\n    ]\n    self.assertAllEqual(expected_offsets, offsets0.tolist())\n    self.assertAllEqual(expected_offsets, offsets1.tolist())\n\n  def test_concatenate_data_dicts(self):\n    cat = utils_np._concatenate_data_dicts(self.graphs_dicts_in)\n    for k, v in cat.items():\n      self.assertAllEqual(getattr(self.reference_graph, k), v)\n\n\nclass DataDictsConversionTest(test_utils.GraphsTest, parameterized.TestCase):\n\n  @parameterized.parameters(([],),\n                            ([""edges""],),\n                            ([""globals""],),\n                            ([""edges"", ""receivers"", ""senders""],))\n  def test_data_dicts_to_graphs_tuple(self, none_fields):\n    """"""Fields in `none_fields` will be cleared out.""""""\n    for field in none_fields:\n      for graph_dict in self.graphs_dicts_in:\n        if field in graph_dict:\n          if field == ""nodes"":\n            graph_dict[""n_node""] = graph_dict[""nodes""].shape[0]\n          graph_dict[field] = None\n          self.reference_graph = self.reference_graph._replace(**{field: None})\n        if field == ""senders"":\n          self.reference_graph = self.reference_graph._replace(\n              n_edge=np.zeros_like(self.reference_graph.n_edge))\n    graphs = utils_np.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    for field in none_fields:\n      self.assertEqual(None, getattr(graphs, field))\n    self._assert_graph_equals_np(self.reference_graph, graphs)\n\n  @parameterized.parameters((""receivers"",), (""senders"",))\n  def test_data_dicts_to_graphs_tuple_missing_field_raises(self, none_field):\n    """"""Fields that cannot be missing.""""""\n    for graph_dict in self.graphs_dicts_in:\n      graph_dict[none_field] = None\n    with self.assertRaisesRegexp(ValueError, none_field):\n      utils_np.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n\n  def test_data_dicts_to_graphs_tuple_infer_n_node(self):\n    """"""Not having nodes is fine if providing the number of nodes.""""""\n    for graph_dict in self.graphs_dicts_in:\n      graph_dict[""n_node""] = graph_dict[""nodes""].shape[0]\n      graph_dict[""nodes""] = None\n    out = utils_np.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    self.assertAllEqual([0, 1, 1, 1, 2, 2, 2], out.n_node)\n\n  def test_data_dicts_to_graphs_tuple_cast_types(self):\n    """"""Index and number fields should be cast to numpy arrays.""""""\n    for graph_dict in self.graphs_dicts_in:\n      graph_dict[""n_node""] = np.array(\n          graph_dict[""nodes""].shape[0], dtype=np.int64)\n      graph_dict[""receivers""] = graph_dict[""receivers""].astype(np.int16)\n      graph_dict[""senders""] = graph_dict[""senders""].astype(np.float64)\n      graph_dict[""nodes""] = graph_dict[""nodes""].astype(np.float64)\n      graph_dict[""edges""] = graph_dict[""edges""].astype(np.float64)\n    out = utils_np.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    for key in [""n_node"", ""n_edge"", ""receivers"", ""senders""]:\n      self.assertEqual(np.int32, getattr(out, key).dtype)\n    for key in [""nodes"", ""edges""]:\n      self.assertEqual(np.float64, getattr(out, key).dtype)\n\n  def test_data_dicts_to_graphs_tuple_from_lists(self):\n    """"""Tests creatings a GraphsTuple from python lists.""""""\n    for graph_dict in self.graphs_dicts_in:\n      graph_dict[""receivers""] = graph_dict[""receivers""].tolist()\n      graph_dict[""senders""] = graph_dict[""senders""].tolist()\n    graphs = utils_np.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    self._assert_graph_equals_np(self.reference_graph, graphs)\n\n  @parameterized.named_parameters(\n      (""all_fields"", []),\n      (""no_data"", [""nodes"", ""edges"", ""globals""]),\n      (""no_edges"", [""edges"", ""receivers"", ""senders""]))\n  def test_graphs_tuple_to_data_dicts(self, none_fields):\n    graphs_tuple = utils_np.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    graphs_tuple = graphs_tuple.map(lambda _: None, none_fields)\n    data_dicts = utils_np.graphs_tuple_to_data_dicts(graphs_tuple)\n    for none_field, data_dict in itertools.product(none_fields, data_dicts):\n      self.assertEqual(None, data_dict[none_field])\n    for expected_data_dict, data_dict in zip(self.graphs_dicts_out, data_dicts):\n      for k, v in expected_data_dict.items():\n        if k not in none_fields:\n          self.assertAllClose(v, data_dict[k])\n\n\ndef _single_data_dict_to_networkx(data_dict):\n  graph_nx = nx.OrderedMultiDiGraph()\n  if data_dict[""nodes""].size > 0:\n    for i, x in enumerate(data_dict[""nodes""]):\n      graph_nx.add_node(i, features=x)\n\n  if data_dict[""edges""].size > 0:\n    edge_data = zip(data_dict[""senders""], data_dict[""receivers""], [{\n        ""features"": x\n    } for x in data_dict[""edges""]])\n    graph_nx.add_edges_from(edge_data)\n  graph_nx.graph[""features""] = data_dict[""globals""]\n\n  return graph_nx\n\n\nclass NetworkxConversionTest(test_utils.GraphsTest, parameterized.TestCase):\n\n  def test_order_preserving(self):\n    """"""Tests that edges order can be preserved when importing from networks.""""""\n    graph = nx.DiGraph()\n    for node_index in range(4):\n      graph.add_node(node_index, features=np.array([node_index]))\n    receivers = [0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]\n    senders = [1, 2, 3, 0, 2, 3, 0, 1, 3, 0, 1, 2]\n    for edge_index, (receiver, sender) in enumerate(zip(receivers, senders)):\n      # Removing the ""index"" key makes this test fail 100%.\n      edge_data = {""features"": np.array([edge_index]), ""index"": edge_index}\n      graph.add_edge(sender, receiver, **edge_data)\n    graph.graph[""features""] = np.array([0.])\n    graphs_graph = utils_np.networkx_to_data_dict(graph)\n    self.assertAllEqual(receivers, graphs_graph[""receivers""])\n    self.assertAllEqual(senders, graphs_graph[""senders""])\n    self.assertAllEqual([[x] for x in range(4)], graphs_graph[""nodes""])\n    self.assertAllEqual([[x] for x in range(12)], graphs_graph[""edges""])\n\n  def test_networkxs_to_graphs_tuple_with_none_fields(self):\n    graph_nx = nx.OrderedMultiDiGraph()\n    data_dict = utils_np.networkx_to_data_dict(\n        graph_nx,\n        node_shape_hint=None,\n        edge_shape_hint=None)\n    self.assertEqual(None, data_dict[""edges""])\n    self.assertEqual(None, data_dict[""globals""])\n    self.assertEqual(None, data_dict[""nodes""])\n    graph_nx.add_node(0, features=None)\n    data_dict = utils_np.networkx_to_data_dict(\n        graph_nx,\n        node_shape_hint=1,\n        edge_shape_hint=None)\n    self.assertEqual(None, data_dict[""nodes""])\n    graph_nx.add_edge(0, 0, features=None)\n    data_dict = utils_np.networkx_to_data_dict(\n        graph_nx,\n        node_shape_hint=[1],\n        edge_shape_hint=[1])\n    self.assertEqual(None, data_dict[""edges""])\n    graph_nx.graph[""features""] = None\n    utils_np.networkx_to_data_dict(graph_nx)\n    self.assertEqual(None, data_dict[""globals""])\n\n  def test_networkxs_to_graphs_tuple(self):\n    graph0 = utils_np.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    graph_nxs = []\n    for data_dict in self.graphs_dicts_in:\n      graph_nxs.append(_single_data_dict_to_networkx(data_dict))\n      hints = {\n          ""edge_shape_hint"": data_dict[""edges""].shape[1:],\n          ""node_shape_hint"": data_dict[""nodes""].shape[1:],\n          ""data_type_hint"": data_dict[""nodes""].dtype,\n      }\n    graph = utils_np.networkxs_to_graphs_tuple(graph_nxs, **hints)\n    self._assert_graph_equals_np(graph0, graph, force_edges_ordering=True)\n\n  def test_networkxs_to_data_dict_raises_node_key_error(self):\n    """"""If the nodes have keys not consistent with the order they were added.""""""\n    graph_nx = nx.OrderedMultiDiGraph()\n    graph_nx.add_node(0, features=None)\n    graph_nx.add_node(1, features=None)\n    graph_nx.add_node(3, features=None)\n\n    with self.assertRaisesRegexp(\n        ValueError, ""found node with index 2 and key 3""):\n      utils_np.networkx_to_data_dict(graph_nx)\n\n    # Check that it is still raised even if there is a node with each key,\n    # and only the order is wrong.\n    graph_nx.add_node(2, features=None)\n    with self.assertRaisesRegexp(\n        ValueError, ""found node with index 2 and key 3""):\n      utils_np.networkx_to_data_dict(graph_nx)\n\n  def test_networkxs_to_graphs_tuple_raises_key_error(self):\n    """"""If the ""features"" field is not present in the nodes or edges.""""""\n    graph_nx = _single_data_dict_to_networkx(self.graphs_dicts_in[-1])\n    first_node = list(graph_nx.nodes(data=True))[0]\n    del first_node[1][""features""]\n    with self.assertRaisesRegexp(\n        KeyError, ""This could be due to the node having been silently added""):\n      utils_np.networkxs_to_graphs_tuple([graph_nx])\n    graph_nx = _single_data_dict_to_networkx(self.graphs_dicts_in[-1])\n    first_edge = list(graph_nx.edges(data=True))[0]\n    del first_edge[2][""features""]\n    with self.assertRaises(KeyError):\n      utils_np.networkxs_to_graphs_tuple([graph_nx])\n\n  def test_networkxs_to_graphs_tuple_raises_assertion_error(self):\n    """"""Either all nodes (resp. edges) should have features, or none of them.""""""\n    graph_nx = _single_data_dict_to_networkx(self.graphs_dicts_in[-1])\n    first_node = list(graph_nx.nodes(data=True))[0]\n    first_node[1][""features""] = None\n    with self.assertRaisesRegexp(\n        ValueError, ""Either all the nodes should have features""):\n      utils_np.networkxs_to_graphs_tuple([graph_nx])\n    graph_nx = _single_data_dict_to_networkx(self.graphs_dicts_in[-1])\n    first_edge = list(graph_nx.edges(data=True))[0]\n    first_edge[2][""features""] = None\n    with self.assertRaisesRegexp(\n        ValueError, ""Either all the edges should have features""):\n      utils_np.networkxs_to_graphs_tuple([graph_nx])\n\n  @parameterized.named_parameters(\n      (""all fields defined"", []),\n      (""stateless"", [""nodes"", ""edges"", ""globals""]))\n  def test_graphs_tuple_to_networkxs(self, none_fields):\n    if ""nodes"" in none_fields:\n      for graph in self.graphs_dicts_in:\n        graph[""n_node""] = graph[""nodes""].shape[0]\n    graphs = utils_np.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    graphs = graphs.map(lambda _: None, none_fields)\n    graph_nxs = utils_np.graphs_tuple_to_networkxs(graphs)\n    for data_dict, graph_nx in zip(self.graphs_dicts_out, graph_nxs):\n      if ""globals"" in none_fields:\n        self.assertEqual(None, graph_nx.graph[""features""])\n      else:\n        self.assertAllClose(data_dict[""globals""], graph_nx.graph[""features""])\n      nodes_data = graph_nx.nodes(data=True)\n      for i, (v, (j, n)) in enumerate(zip(data_dict[""nodes""], nodes_data)):\n        self.assertEqual(i, j)\n        if ""nodes"" in none_fields:\n          self.assertEqual(None, n[""features""])\n        else:\n          self.assertAllClose(v, n[""features""])\n      edges_data = sorted(\n          graph_nx.edges(data=True), key=lambda x: x[2][""index""])\n      for v, (_, _, e) in zip(data_dict[""edges""], edges_data):\n        if ""edges"" in none_fields:\n          self.assertEqual(None, e[""features""])\n        else:\n          self.assertAllClose(v, e[""features""])\n      for r, s, (i, j, _) in zip(\n          data_dict[""receivers""], data_dict[""senders""], edges_data):\n        self.assertEqual(s, i)\n        self.assertEqual(r, j)\n\n\nclass GetItemTest(test_utils.GraphsTest, parameterized.TestCase):\n\n  def test_get_single_item(self):\n    index = 2\n    expected = self.graphs_dicts_out[index]\n\n    graphs = utils_np.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    graph = utils_np.get_graph(graphs, index)\n    actual, = utils_np.graphs_tuple_to_data_dicts(graph)\n\n    for k, v in expected.items():\n      self.assertAllClose(v, actual[k])\n\n  def test_get_many_items(self):\n    index = slice(1, 3)\n    expected = self.graphs_dicts_out[index]\n\n    graphs = utils_np.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    graphs2 = utils_np.get_graph(graphs, index)\n    actual = utils_np.graphs_tuple_to_data_dicts(graphs2)\n\n    for ex, ac in zip(expected, actual):\n      for k, v in ex.items():\n        self.assertAllClose(v, ac[k])\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
graph_nets/tests/utils_tf_test.py,201,"b'# Copyright 2018 The GraphNets Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n\n""""""Tests for utils_tf.py.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom graph_nets import graphs\nfrom graph_nets import utils_np\nfrom graph_nets import utils_tf\nfrom graph_nets.tests import test_utils\nimport networkx as nx\nimport numpy as np\nfrom six.moves import range\nimport tensorflow as tf\nimport tree\n\n\n\n\nclass RepeatTest(tf.test.TestCase, parameterized.TestCase):\n  """"""Tests for `repeat`.""""""\n\n  @parameterized.named_parameters(\n      (""base"", (3,), [2, 3, 4], 0),\n      (""empty_group_first"", (3,), [0, 3, 4], 0),\n      (""empty_group_middle"", (3,), [2, 0, 4], 0),\n      (""double_empty_group_middle"", (4,), [2, 0, 0, 4], 0),\n      (""empty_group_last"", (3,), [2, 3, 0], 0),\n      (""just_one_group"", (1,), [2], 0),\n      (""zero_groups"", (0,), [], 0),\n      (""axis 0"", (2, 3, 4), [2, 3], 0),\n      (""axis 1"", (3, 2, 4), [2, 3], 1),\n      (""axis 2"", (4, 3, 2), [2, 3], 2),\n      (""zero_groups_with_shape"", (2, 0, 4), [], 1),\n      )\n  def test_repeat(self, shape, repeats, axis):\n    num_elements = np.prod(shape)\n    t = np.arange(num_elements).reshape(*shape)\n    expected = np.repeat(t, repeats, axis=axis)\n    tensor = tf.constant(t)\n    repeats = tf.constant(repeats, dtype=tf.int32)\n    op = utils_tf.repeat(tensor, repeats, axis=axis)\n    with tf.Session() as sess:\n      actual = sess.run(op)\n    self.assertAllEqual(expected, actual)\n\n  @parameterized.named_parameters((""default"", ""custom_name"", None),\n                                  (""custom"", None, ""repeat""))\n  def test_name_scope(self, name, expected_name):\n    kwargs = {""name"": name} if name else {}\n    expected_name = expected_name if expected_name else name\n\n    t = tf.zeros([3, 2, 4])\n    indices = tf.constant([2, 3])\n    with test_utils.assert_new_op_prefixes(self, expected_name + ""/""):\n      utils_tf.repeat(t, indices, axis=1, **kwargs)\n\n\ndef _generate_graph(batch_index, n_nodes=4, add_edges=True):\n  graph = nx.DiGraph()\n  for node in range(n_nodes):\n    node_data = {""features"": np.array([node, batch_index], dtype=np.float32)}\n    graph.add_node(node, **node_data)\n  if add_edges:\n    for edge, (receiver, sender) in enumerate(zip([0, 0, 1], [1, 2, 3])):\n      if sender < n_nodes and receiver < n_nodes:\n        edge_data = np.array([edge, edge + 1, batch_index], dtype=np.float64)\n        graph.add_edge(sender, receiver, features=edge_data, index=edge)\n  graph.graph[""features""] = np.array([batch_index], dtype=np.float32)\n  return graph\n\n\nclass ConcatTest(tf.test.TestCase, parameterized.TestCase):\n  """"""Tests for `concat`, along various axis.""""""\n\n  @parameterized.named_parameters(\n      (""no nones"", []), (""stateless graph"", [""nodes"", ""edges"", ""globals""]),\n      (""no edges"", [""edges"", ""receivers"", ""senders""]))\n  def test_concat_first_axis(self, none_fields):\n    graph_0 = utils_np.networkxs_to_graphs_tuple(\n        [_generate_graph(0, 3), _generate_graph(1, 2)])\n    graph_1 = utils_np.networkxs_to_graphs_tuple([_generate_graph(2, 2)])\n    graph_2 = utils_np.networkxs_to_graphs_tuple([_generate_graph(3, 3)])\n    graphs_ = [\n        gr.map(tf.convert_to_tensor, graphs.ALL_FIELDS)\n        for gr in [graph_0, graph_1, graph_2]\n    ]\n    graphs_ = [gr.map(lambda _: None, none_fields) for gr in graphs_]\n    concat_graph = utils_tf.concat(graphs_, axis=0)\n    for none_field in none_fields:\n      self.assertEqual(None, getattr(concat_graph, none_field))\n    concat_graph = concat_graph.map(tf.no_op, none_fields)\n    with tf.Session() as sess:\n      concat_graph = sess.run(concat_graph)\n    if ""nodes"" not in none_fields:\n      self.assertAllEqual(\n          np.array([0, 1, 2, 0, 1, 0, 1, 0, 1, 2]),\n          [x[0] for x in concat_graph.nodes])\n      self.assertAllEqual(\n          np.array([0, 0, 0, 1, 1, 2, 2, 3, 3, 3]),\n          [x[1] for x in concat_graph.nodes])\n    if ""edges"" not in none_fields:\n      self.assertAllEqual(\n          np.array([0, 1, 0, 0, 0, 1]), [x[0] for x in concat_graph.edges])\n      self.assertAllEqual(\n          np.array([0, 0, 1, 2, 3, 3]), [x[2] for x in concat_graph.edges])\n    self.assertAllEqual(np.array([3, 2, 2, 3]), concat_graph.n_node)\n    self.assertAllEqual(np.array([2, 1, 1, 2]), concat_graph.n_edge)\n    if ""senders"" not in none_fields:\n      # [1, 2], [1], [1], [1, 2] and 3, 2, 2, 3 nodes\n      # So we are summing [1, 2, 1, 1, 2] with [0, 0, 3, 5, 7, 7]\n      self.assertAllEqual(np.array([1, 2, 4, 6, 8, 9]), concat_graph.senders)\n    if ""receivers"" not in none_fields:\n      # [0, 0], [0], [0], [0, 0] and 3, 2, 2, 3 nodes\n      # So we are summing [0, 0, 0, 0, 0, 0] with [0, 0, 3, 5, 7, 7]\n      self.assertAllEqual(np.array([0, 0, 3, 5, 7, 7]), concat_graph.receivers)\n    if ""globals"" not in none_fields:\n      self.assertAllEqual(np.array([[0], [1], [2], [3]]), concat_graph.globals)\n\n  def test_concat_last_axis(self):\n    graph0 = utils_np.networkxs_to_graphs_tuple(\n        [_generate_graph(0, 3), _generate_graph(1, 2)])\n    graph1 = utils_np.networkxs_to_graphs_tuple(\n        [_generate_graph(2, 3), _generate_graph(3, 2)])\n    graph0 = graph0.map(tf.convert_to_tensor, graphs.ALL_FIELDS)\n    graph1 = graph1.map(tf.convert_to_tensor, graphs.ALL_FIELDS)\n    with tf.Session() as sess:\n      concat_graph = sess.run(utils_tf.concat([graph0, graph1], axis=-1))\n    self.assertAllEqual(\n        np.array([[0, 0, 0, 2], [1, 0, 1, 2], [2, 0, 2, 2], [0, 1, 0, 3],\n                  [1, 1, 1, 3]]), concat_graph.nodes)\n    self.assertAllEqual(\n        np.array([[0, 1, 0, 0, 1, 2], [1, 2, 0, 1, 2, 2], [0, 1, 1, 0, 1, 3]]),\n        concat_graph.edges)\n    self.assertAllEqual(np.array([3, 2]), concat_graph.n_node)\n    self.assertAllEqual(np.array([2, 1]), concat_graph.n_edge)\n    self.assertAllEqual(np.array([1, 2, 4]), concat_graph.senders)\n    self.assertAllEqual(np.array([0, 0, 3]), concat_graph.receivers)\n    self.assertAllEqual(np.array([[0, 2], [1, 3]]), concat_graph.globals)\n\n\nclass BuildPlaceholdersTest(test_utils.GraphsTest, parameterized.TestCase):\n\n  def _assert_expected_shapes(self, placeholders, but_for=None,\n                              num_graphs=None):\n    if but_for is None:\n      but_for = []\n    if ""nodes"" not in but_for:\n      self.assertAllEqual([None, 2], placeholders.nodes.shape.as_list())\n    if ""edges"" not in but_for:\n      self.assertAllEqual([None, 3], placeholders.edges.shape.as_list())\n    if ""globals"" not in but_for:\n      self.assertAllEqual([num_graphs, 1], placeholders.globals.shape.as_list())\n    for key in [""receivers"", ""senders""]:\n      if key not in but_for:\n        self.assertAllEqual([None], getattr(placeholders, key).shape.as_list())\n    for key in [""n_node"", ""n_edge""]:\n      if key not in but_for:\n        self.assertAllEqual([num_graphs],\n                            getattr(placeholders, key).shape.as_list())\n\n  @parameterized.named_parameters(\n      (""all_field_defined"", [], False),\n      (""no features"", [""nodes"", ""edges"", ""globals""], False),\n      (""no edges"", [""edges"", ""receivers"", ""senders""], False),\n      (""dynamic"", [], True))\n  def test_build_placeholders_from_specs(self,\n                                         none_fields,\n                                         force_dynamic_num_graphs=False):\n    num_graphs = 3\n    shapes = graphs.GraphsTuple(\n        nodes=[3, 4],\n        edges=[2],\n        globals=[num_graphs, 4, 6],\n        receivers=[None],\n        senders=[18],\n        n_node=[num_graphs],\n        n_edge=[num_graphs],\n    )\n    dtypes = graphs.GraphsTuple(\n        nodes=tf.float64,\n        edges=tf.int32,\n        globals=tf.float32,\n        receivers=tf.int64,\n        senders=tf.int64,\n        n_node=tf.int32,\n        n_edge=tf.int64)\n    dtypes = dtypes.map(lambda _: None, none_fields)\n    shapes = shapes.map(lambda _: None, none_fields)\n    placeholders = utils_tf._build_placeholders_from_specs(\n        dtypes, shapes, force_dynamic_num_graphs=force_dynamic_num_graphs)\n    for k in graphs.ALL_FIELDS:\n      placeholder = getattr(placeholders, k)\n      if k in none_fields:\n        self.assertEqual(None, placeholder)\n      else:\n        self.assertEqual(getattr(dtypes, k), placeholder.dtype)\n        if k not in [""n_node"", ""n_edge"", ""globals""] or force_dynamic_num_graphs:\n          self.assertAllEqual([None] + getattr(shapes, k)[1:],\n                              placeholder.shape.as_list())\n        else:\n          self.assertAllEqual([num_graphs] + getattr(shapes, k)[1:],\n                              placeholder.shape.as_list())\n\n  @parameterized.named_parameters((""static_num_graphs"", False),\n                                  (""dynamic_num_graphs"", True))\n  def test_placeholders_from_data_dicts(self, force_dynamic_num_graphs):\n    num_graphs = len(self.graphs_dicts_in)\n    placeholders = utils_tf.placeholders_from_data_dicts(\n        self.graphs_dicts_in, force_dynamic_num_graphs=force_dynamic_num_graphs)\n    self.assertAllEqual([None, 7, 11], placeholders.nodes.shape.as_list())\n    self.assertAllEqual([None, 13, 14], placeholders.edges.shape.as_list())\n\n    global_shape = placeholders.globals.shape.as_list()\n    if force_dynamic_num_graphs:\n      self.assertAllEqual([None, 5, 3], global_shape)\n    else:\n      self.assertAllEqual([num_graphs, 5, 3], global_shape)\n    for key in [""receivers"", ""senders""]:\n      self.assertAllEqual([None], getattr(placeholders, key).shape.as_list())\n    for key in [""n_node"", ""n_edge""]:\n      shape = getattr(placeholders, key).shape.as_list()\n      if force_dynamic_num_graphs:\n        self.assertAllEqual([None], shape)\n      else:\n        self.assertAllEqual([num_graphs], shape)\n\n  def test_placeholders_from_networkxs(self):\n    num_graphs = 16\n    networkxs = [\n        _generate_graph(batch_index) for batch_index in range(num_graphs)\n    ]\n    placeholders = utils_tf.placeholders_from_networkxs(\n        networkxs, force_dynamic_num_graphs=False)\n    self._assert_expected_shapes(placeholders, num_graphs=num_graphs)\n    self.assertEqual(tf.float32, placeholders.nodes.dtype)\n    self.assertEqual(tf.float64, placeholders.edges.dtype)\n\n  def test_placeholders_from_networkxs_missing_nodes(self):\n    num_graphs = 16\n    networkxs = [\n        _generate_graph(batch_index, n_nodes=0, add_edges=False)\n        for batch_index in range(num_graphs)\n    ]\n    placeholders = utils_tf.placeholders_from_networkxs(\n        networkxs, force_dynamic_num_graphs=False)\n    self.assertEqual(None, placeholders.nodes)\n    self.assertEqual(None, placeholders.edges)\n    self._assert_expected_shapes(\n        placeholders, but_for=[""nodes"", ""edges""], num_graphs=num_graphs)\n\n  def test_placeholders_from_networkxs_hints(self):\n    num_graphs = 16\n    networkxs = [\n        _generate_graph(batch_index, n_nodes=0, add_edges=False)\n        for batch_index in range(num_graphs)\n    ]\n    placeholders = utils_tf.placeholders_from_networkxs(\n        networkxs,\n        node_shape_hint=[14],\n        edge_shape_hint=[17],\n        data_type_hint=tf.float64,\n        force_dynamic_num_graphs=False)\n    self.assertAllEqual([None, 14], placeholders.nodes.shape.as_list())\n    self.assertAllEqual([None, 17], placeholders.edges.shape.as_list())\n    self._assert_expected_shapes(\n        placeholders, but_for=[""nodes"", ""edges""], num_graphs=num_graphs)\n    self.assertEqual(tf.float64, placeholders.nodes.dtype)\n    self.assertEqual(tf.float64, placeholders.edges.dtype)\n\n  def test_placeholders_from_networkxs_missing_edges(self):\n    num_graphs = 16\n    networkxs = [\n        _generate_graph(batch_index, add_edges=False)\n        for batch_index in range(num_graphs)\n    ]\n    placeholders = utils_tf.placeholders_from_networkxs(\n        networkxs, force_dynamic_num_graphs=False)\n    self.assertEqual(None, placeholders.edges)\n    self._assert_expected_shapes(\n        placeholders, but_for=[""edges""], num_graphs=num_graphs)\n\n  def test_feed_data(self):\n    networkx = [_generate_graph(batch_index) for batch_index in range(16)]\n    placeholders = utils_tf.placeholders_from_networkxs(\n        networkx, force_dynamic_num_graphs=True)\n    # Does not need to be the same size\n    networkxs = [_generate_graph(batch_index) for batch_index in range(2)]\n    with tf.Session() as sess:\n      output = sess.run(\n          placeholders,\n          utils_tf.get_feed_dict(placeholders,\n                                 utils_np.networkxs_to_graphs_tuple(networkxs)))\n    self.assertAllEqual(\n        np.array([[0, 0], [1, 0], [2, 0], [3, 0], [0, 1], [1, 1], [2, 1],\n                  [3, 1]]), output.nodes)\n    self.assertEqual(np.float32, output.nodes.dtype)\n    self.assertAllEqual(np.array([[0], [1]]), output.globals)\n    self.assertEqual(np.float32, output.globals.dtype)\n    sorted_edges_content = sorted(\n        [(x, y, z)\n         for x, y, z in zip(output.receivers, output.senders, output.edges)])\n    self.assertAllEqual([0, 0, 1, 4, 4, 5],\n                        [x[0] for x in sorted_edges_content])\n    self.assertAllEqual([1, 2, 3, 5, 6, 7],\n                        [x[1] for x in sorted_edges_content])\n    self.assertEqual(np.float64, output.edges.dtype)\n    self.assertAllEqual(\n        np.array([[0, 1, 0], [1, 2, 0], [2, 3, 0], [0, 1, 1], [1, 2, 1],\n                  [2, 3, 1]]), [x[2] for x in sorted_edges_content])\n\n  @parameterized.named_parameters((\n      ""no features"",\n      [""nodes"", ""edges"", ""globals""],\n  ), (\n      ""no edges"",\n      [""edges"", ""receivers"", ""senders""],\n  ))\n  def test_get_feed_dict_raises(self, none_fields):\n    networkxs = [_generate_graph(batch_index) for batch_index in range(16)]\n    placeholders = utils_tf.placeholders_from_networkxs(networkxs)\n    feed_values = utils_np.networkxs_to_graphs_tuple(networkxs)\n    with self.assertRaisesRegexp(ValueError, """"):\n      utils_tf.get_feed_dict(\n          placeholders.map(lambda _: None, none_fields), feed_values)\n    with self.assertRaisesRegexp(ValueError, """"):\n      utils_tf.get_feed_dict(placeholders,\n                             feed_values.map(lambda _: None, none_fields))\n\n  def test_feed_data_no_nodes(self):\n    networkx = [\n        _generate_graph(batch_index, n_nodes=0, add_edges=False)\n        for batch_index in range(16)\n    ]\n    placeholders = utils_tf.placeholders_from_networkxs(\n        networkx, force_dynamic_num_graphs=True)\n    # Does not need to be the same size\n    networkxs = [\n        _generate_graph(batch_index, n_nodes=0, add_edges=False)\n        for batch_index in range(2)\n    ]\n    self.assertEqual(None, placeholders.nodes)\n    self.assertEqual(None, placeholders.edges)\n    with tf.Session() as sess:\n      output = sess.run(\n          placeholders.replace(nodes=tf.no_op(), edges=tf.no_op()),\n          utils_tf.get_feed_dict(placeholders,\n                                 utils_np.networkxs_to_graphs_tuple(networkxs)))\n    self.assertAllEqual(np.array([[0], [1]]), output.globals)\n    self.assertEqual(np.float32, output.globals.dtype)\n\n  def test_feed_data_no_edges(self):\n    networkx = [\n        _generate_graph(batch_index, add_edges=False)\n        for batch_index in range(16)\n    ]\n    placeholders = utils_tf.placeholders_from_networkxs(\n        networkx, force_dynamic_num_graphs=True)\n    # Does not need to be the same size\n    networkxs = [\n        _generate_graph(batch_index, add_edges=False)\n        for batch_index in range(2)\n    ]\n    self.assertEqual(None, placeholders.edges)\n    with tf.Session() as sess:\n      output = sess.run(\n          placeholders.replace(edges=tf.no_op()),\n          utils_tf.get_feed_dict(placeholders,\n                                 utils_np.networkxs_to_graphs_tuple(networkxs)))\n    self.assertAllEqual(\n        np.array([[0, 0], [1, 0], [2, 0], [3, 0], [0, 1], [1, 1], [2, 1],\n                  [3, 1]]), output.nodes)\n    self.assertAllEqual(np.array([[0], [1]]), output.globals)\n    self.assertEqual(np.float32, output.globals.dtype)\n\n\nclass StopGradientsGraphTest(tf.test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(StopGradientsGraphTest, self).setUp()\n    self._graph = utils_tf.data_dicts_to_graphs_tuple([{\n        ""senders"": tf.zeros([10], dtype=tf.int32),\n        ""receivers"": tf.zeros([10], dtype=tf.int32),\n        ""nodes"": tf.ones([5, 7]),\n        ""edges"": tf.zeros([10, 6]),\n        ""globals"": tf.zeros([1, 8])\n    }])\n\n  def _check_if_gradients_exist(self, stopped_gradients_graph):\n    gradients = []\n    for field in [""globals"", ""nodes"", ""edges""]:\n      xs = getattr(self._graph, field)\n      ys = getattr(stopped_gradients_graph, field)\n      gradient = tf.gradients(ys, xs)[0] if ys is not None else ys\n      gradients.append(gradient)\n    return [True if grad is not None else False for grad in gradients]\n\n  @parameterized.named_parameters(\n      (""stop_all_fields"", True, True, True),\n      (""stop_globals"", True, False, False), (""stop_nodes"", False, True, False),\n      (""stop_edges"", False, False, True), (""stop_none"", False, False, False))\n  def test_stop_gradients_outputs(self, stop_globals, stop_nodes, stop_edges):\n    stopped_gradients_graph = utils_tf.stop_gradient(\n        self._graph,\n        stop_globals=stop_globals,\n        stop_nodes=stop_nodes,\n        stop_edges=stop_edges)\n\n    gradients_exist = self._check_if_gradients_exist(stopped_gradients_graph)\n    expected_gradients_exist = [\n        not stop_globals, not stop_nodes, not stop_edges\n    ]\n    self.assertAllEqual(expected_gradients_exist, gradients_exist)\n\n  @parameterized.named_parameters((""no_nodes"", ""nodes""), (""no_edges"", ""edges""),\n                                  (""no_globals"", ""globals""))\n  def test_stop_gradients_with_missing_field_raises(self, none_field):\n    self._graph = self._graph.map(lambda _: None, [none_field])\n    with self.assertRaisesRegexp(ValueError, none_field):\n      utils_tf.stop_gradient(self._graph)\n\n  def test_stop_gradients_default_params(self):\n    """"""Tests for the default params of `utils_tf.stop_gradient`.""""""\n    stopped_gradients_graph = utils_tf.stop_gradient(self._graph)\n    gradients_exist = self._check_if_gradients_exist(stopped_gradients_graph)\n    expected_gradients_exist = [False, False, False]\n    self.assertAllEqual(expected_gradients_exist, gradients_exist)\n\n\nclass IdentityTest(tf.test.TestCase, parameterized.TestCase):\n  """"""Tests for the `identity` method.""""""\n\n  def setUp(self):\n    super(IdentityTest, self).setUp()\n    self._graph = utils_tf.data_dicts_to_graphs_tuple([{\n        ""senders"": tf.random_uniform([10], maxval=10, dtype=tf.int32),\n        ""receivers"": tf.random_uniform([10], maxval=10, dtype=tf.int32),\n        ""nodes"": tf.random_uniform([5, 7]),\n        ""edges"": tf.random_uniform([10, 6]),\n        ""globals"": tf.random_uniform([1, 8])\n    }])\n\n  def test_name_scope(self):\n    """"""Tests that the name scope are correctly pushed through this function.""""""\n    graph = self._graph\n    with tf.name_scope(""test""):\n      graph_id = utils_tf.identity(graph)\n    for field in [\n        ""nodes"", ""edges"", ""globals"", ""receivers"", ""senders"", ""n_node"", ""n_edge""\n    ]:\n      self.assertEqual(""test"", getattr(graph_id, field).name.split(""/"")[0])\n\n  @parameterized.named_parameters(\n      (""all fields defined"", []), (""no node features"", [""nodes""]),\n      (""no edge features"", [""edges""]), (""no global features"", [""globals""]),\n      (""no edges"", [""edges"", ""receivers"", ""senders""]))\n  def test_output(self, none_fields):\n    """"""Tests that this function produces the identity.""""""\n    graph = self._graph.map(lambda _: None, none_fields)\n    with tf.name_scope(""test""):\n      graph_id = utils_tf.identity(graph)\n    graph = utils_tf.make_runnable_in_session(graph)\n    graph_id = utils_tf.make_runnable_in_session(graph_id)\n    with tf.Session() as sess:\n      expected_out, actual_out = sess.run([graph, graph_id])\n    for field in [\n        ""nodes"", ""edges"", ""globals"", ""receivers"", ""senders"", ""n_node"", ""n_edge""\n    ]:\n      if field in none_fields:\n        self.assertEqual(None, getattr(actual_out, field))\n      else:\n        self.assertNDArrayNear(\n            getattr(expected_out, field), getattr(actual_out, field), err=1e-4)\n\n\nclass RunGraphWithNoneInSessionTest(tf.test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(RunGraphWithNoneInSessionTest, self).setUp()\n    self._graph = utils_tf.data_dicts_to_graphs_tuple([{\n        ""senders"": tf.random_uniform([10], maxval=10, dtype=tf.int32),\n        ""receivers"": tf.random_uniform([10], maxval=10, dtype=tf.int32),\n        ""nodes"": tf.random_uniform([5, 7]),\n        ""edges"": tf.random_uniform([10, 6]),\n        ""globals"": tf.random_uniform([1, 8])\n    }])\n\n  @parameterized.named_parameters(\n      (""all fields defined"", []), (""no node features"", [""nodes""]),\n      (""no edge features"", [""edges""]), (""no global features"", [""globals""]),\n      (""no edges"", [""edges"", ""receivers"", ""senders""]))\n  def test_output(self, none_fields):\n    """"""Tests that this function produces the identity.""""""\n    graph = self._graph.map(lambda _: None, none_fields)\n    with tf.name_scope(""test""):\n      graph_id = utils_tf.make_runnable_in_session(graph)\n    graph = graph.map(tf.no_op, none_fields)\n    with tf.Session() as sess:\n      expected_out, actual_out = sess.run([graph, graph_id])\n    for field in [\n        ""nodes"", ""edges"", ""globals"", ""receivers"", ""senders"", ""n_node"", ""n_edge""\n    ]:\n      if field in none_fields:\n        self.assertEqual(None, getattr(actual_out, field))\n      else:\n        self.assertNDArrayNear(\n            getattr(expected_out, field), getattr(actual_out, field), err=1e-4)\n\n\nclass ComputeOffsetTest(tf.test.TestCase):\n  """"""Tests for the `compute_stacked_offsets` method.""""""\n\n  def setUp(self):\n    super(ComputeOffsetTest, self).setUp()\n    tf.reset_default_graph()\n    self.sizes = [5, 4, 3, 1, 2, 0, 3, 0, 4, 7]\n    self.repeats = [2, 2, 0, 2, 1, 3, 2, 0, 3, 2]\n    self.offset = [\n        0, 0, 5, 5, 12, 12, 13, 15, 15, 15, 15, 15, 18, 18, 18, 22, 22\n    ]\n\n  def test_compute_stacked_offsets(self):\n    offset0 = utils_tf._compute_stacked_offsets(self.sizes, self.repeats)\n    offset1 = utils_tf._compute_stacked_offsets(\n        np.array(self.sizes), np.array(self.repeats))\n    offset2 = utils_tf._compute_stacked_offsets(\n        tf.constant(self.sizes, dtype=tf.int32),\n        tf.constant(self.repeats, dtype=tf.int32))\n\n    with tf.Session() as sess:\n      o0, o1, o2 = sess.run([offset0, offset1, offset2])\n\n    self.assertAllEqual(self.offset, o0.tolist())\n    self.assertAllEqual(self.offset, o1.tolist())\n    self.assertAllEqual(self.offset, o2.tolist())\n\n\nclass DataDictsCompletionTests(test_utils.GraphsTest, parameterized.TestCase):\n  """"""Tests for the methods creating complete graphs from partial graphs.""""""\n\n  def _assert_indices_sizes(self, dict_, n_relation):\n    for key in [""receivers"", ""senders""]:\n      self.assertAllEqual((n_relation,), dict_[key].get_shape().as_list())\n\n  @parameterized.named_parameters(\n      (""static"", utils_tf._create_complete_edges_from_nodes_static),\n      (""dynamic"", utils_tf._create_complete_edges_from_nodes_dynamic),\n  )\n  def test_create_complete_edges_from_nodes_include_self_edges(self, method):\n    for graph_dict in self.graphs_dicts_in:\n      n_node = graph_dict[""nodes""].shape[0]\n      edges_dict = method(n_node, exclude_self_edges=False)\n      self._assert_indices_sizes(edges_dict, n_node**2)\n\n  @parameterized.named_parameters(\n      (""static"", utils_tf._create_complete_edges_from_nodes_static),\n      (""dynamic"", utils_tf._create_complete_edges_from_nodes_dynamic),\n  )\n  def test_create_complete_edges_from_nodes_exclude_self_edges(self, method):\n    for graph_dict in self.graphs_dicts_in:\n      n_node = graph_dict[""nodes""].shape[0]\n      edges_dict = method(n_node, exclude_self_edges=True)\n      self._assert_indices_sizes(edges_dict, n_node * (n_node - 1))\n\n  def test_create_complete_edges_from_nodes_dynamic_number_of_nodes(self):\n    for graph_dict in self.graphs_dicts_in:\n      n_node = tf.shape(tf.constant(graph_dict[""nodes""]))[0]\n      edges_dict = utils_tf._create_complete_edges_from_nodes_dynamic(\n          n_node, exclude_self_edges=False)\n      n_relation_op = n_node**2\n      with tf.Session() as sess:\n        n_relation, receivers, senders, n_edge = sess.run([\n            n_relation_op, edges_dict[""receivers""], edges_dict[""senders""],\n            edges_dict[""n_edge""]\n        ])\n      self.assertAllEqual((n_relation,), receivers.shape)\n      self.assertAllEqual((n_relation,), senders.shape)\n      self.assertEqual(n_relation, n_edge)\n\n\nclass GraphsCompletionTests(test_utils.GraphsTest, parameterized.TestCase):\n  """"""Tests for completing partial GraphsTuple.""""""\n\n  def _assert_indices_sizes(self, graph, n_relation):\n    for key in [""receivers"", ""senders""]:\n      self.assertAllEqual((n_relation,),\n                          getattr(graph, key).get_shape().as_list())\n\n  @parameterized.named_parameters((""edge size 0"", 0), (""edge size 1"", 1))\n  def test_fill_edge_state(self, edge_size):\n    """"""Tests for filling the edge state with a constant content.""""""\n    for g in self.graphs_dicts_in:\n      g.pop(""edges"")\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    n_edges = np.sum(self.reference_graph.n_edge)\n    graphs_tuple = utils_tf.set_zero_edge_features(graphs_tuple, edge_size)\n    self.assertAllEqual((n_edges, edge_size),\n                        graphs_tuple.edges.get_shape().as_list())\n\n  @parameterized.named_parameters((""edge size 0"", 0), (""edge size 1"", 1))\n  def test_fill_edge_state_dynamic(self, edge_size):\n    """"""Tests for filling the edge state with a constant content.""""""\n    for g in self.graphs_dicts_in:\n      g.pop(""edges"")\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    graphs_tuple = graphs_tuple._replace(\n        n_edge=tf.placeholder_with_default(\n            graphs_tuple.n_edge, shape=graphs_tuple.n_edge.get_shape()))\n    n_edges = np.sum(self.reference_graph.n_edge)\n    graphs_tuple = utils_tf.set_zero_edge_features(graphs_tuple, edge_size)\n    with tf.Session() as sess:\n      actual_edges = sess.run(graphs_tuple.edges)\n    self.assertNDArrayNear(\n        np.zeros((n_edges, edge_size)), actual_edges, err=1e-4)\n\n  @parameterized.named_parameters((""global size 0"", 0), (""global size 1"", 1))\n  def test_fill_global_state(self, global_size):\n    """"""Tests for filling the global state with a constant content.""""""\n    for g in self.graphs_dicts_in:\n      g.pop(""globals"")\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    n_graphs = self.reference_graph.n_edge.shape[0]\n    graphs_tuple = utils_tf.set_zero_global_features(graphs_tuple, global_size)\n    self.assertAllEqual((n_graphs, global_size),\n                        graphs_tuple.globals.get_shape().as_list())\n\n  @parameterized.named_parameters((""global size 0"", 0), (""global size 1"", 1))\n  def test_fill_global_state_dynamic(self, global_size):\n    """"""Tests for filling the global state with a constant content.""""""\n    for g in self.graphs_dicts_in:\n      g.pop(""globals"")\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    # Hide global shape information\n    graphs_tuple = graphs_tuple._replace(\n        n_node=tf.placeholder_with_default(graphs_tuple.n_node, shape=[None]))\n    n_graphs = self.reference_graph.n_edge.shape[0]\n    graphs_tuple = utils_tf.set_zero_global_features(graphs_tuple, global_size)\n    with tf.Session() as sess:\n      actual_globals = sess.run(graphs_tuple.globals)\n    self.assertNDArrayNear(\n        np.zeros((n_graphs, global_size)), actual_globals, err=1e-4)\n\n  @parameterized.named_parameters((""node size 0"", 0), (""node size 1"", 1))\n  def test_fill_node_state(self, node_size):\n    """"""Tests for filling the node state with a constant content.""""""\n    for g in self.graphs_dicts_in:\n      g[""n_node""] = g[""nodes""].shape[0]\n      g.pop(""nodes"")\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    n_nodes = np.sum(self.reference_graph.n_node)\n    graphs_tuple = utils_tf.set_zero_node_features(graphs_tuple, node_size)\n    self.assertAllEqual((n_nodes, node_size),\n                        graphs_tuple.nodes.get_shape().as_list())\n\n  @parameterized.named_parameters((""node size 0"", 0), (""node size 1"", 1))\n  def test_fill_node_state_dynamic(self, node_size):\n    """"""Tests for filling the node state with a constant content.""""""\n    for g in self.graphs_dicts_in:\n      g[""n_node""] = g[""nodes""].shape[0]\n      g.pop(""nodes"")\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    graphs_tuple = graphs_tuple._replace(\n        n_node=tf.placeholder_with_default(\n            graphs_tuple.n_node, shape=graphs_tuple.n_node.get_shape()))\n    n_nodes = np.sum(self.reference_graph.n_node)\n    graphs_tuple = utils_tf.set_zero_node_features(graphs_tuple, node_size)\n    with tf.Session() as sess:\n      actual_nodes = sess.run(graphs_tuple.nodes)\n    self.assertNDArrayNear(\n        np.zeros((n_nodes, node_size)), actual_nodes, err=1e-4)\n\n  def test_fill_edge_state_with_missing_fields_raises(self):\n    """"""Edge field cannot be filled if receivers or senders are missing.""""""\n    for g in self.graphs_dicts_in:\n      g.pop(""receivers"")\n      g.pop(""senders"")\n      g.pop(""edges"")\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    with self.assertRaisesRegexp(ValueError, ""receivers""):\n      graphs_tuple = utils_tf.set_zero_edge_features(graphs_tuple, edge_size=1)\n\n  def test_fill_state_default_types(self):\n    """"""Tests that the features are created with the correct default type.""""""\n    for g in self.graphs_dicts_in:\n      g.pop(""nodes"")\n      g.pop(""globals"")\n      g.pop(""edges"")\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    graphs_tuple = utils_tf.set_zero_edge_features(graphs_tuple, edge_size=1)\n    graphs_tuple = utils_tf.set_zero_node_features(graphs_tuple, node_size=1)\n    graphs_tuple = utils_tf.set_zero_global_features(\n        graphs_tuple, global_size=1)\n    self.assertEqual(tf.float32, graphs_tuple.edges.dtype)\n    self.assertEqual(tf.float32, graphs_tuple.nodes.dtype)\n    self.assertEqual(tf.float32, graphs_tuple.globals.dtype)\n\n  @parameterized.parameters(\n      (tf.float64,),\n      (tf.int32,),\n  )\n  def test_fill_state_user_specified_types(self, dtype):\n    """"""Tests that the features are created with the correct default type.""""""\n    for g in self.graphs_dicts_in:\n      g.pop(""nodes"")\n      g.pop(""globals"")\n      g.pop(""edges"")\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    graphs_tuple = utils_tf.set_zero_edge_features(graphs_tuple, 1, dtype)\n    graphs_tuple = utils_tf.set_zero_node_features(graphs_tuple, 1, dtype)\n    graphs_tuple = utils_tf.set_zero_global_features(graphs_tuple, 1, dtype)\n    self.assertEqual(dtype, graphs_tuple.edges.dtype)\n    self.assertEqual(dtype, graphs_tuple.nodes.dtype)\n    self.assertEqual(dtype, graphs_tuple.globals.dtype)\n\n  @parameterized.named_parameters(\n      (""no self edges"", False),\n      (""self edges"", True),\n  )\n  def test_fully_connect_graph_dynamic(self, exclude_self_edges):\n    for g in self.graphs_dicts_in:\n      g.pop(""edges"")\n      g.pop(""receivers"")\n      g.pop(""senders"")\n    n_relation = 0\n    for g in self.graphs_dicts_in:\n      n_node = g[""nodes""].shape[0]\n      if exclude_self_edges:\n        n_relation += n_node * (n_node - 1)\n      else:\n        n_relation += n_node * n_node\n\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    graphs_tuple = utils_tf.fully_connect_graph_dynamic(graphs_tuple,\n                                                        exclude_self_edges)\n    with tf.Session() as sess:\n      actual_receivers, actual_senders = sess.run(\n          [graphs_tuple.receivers, graphs_tuple.senders])\n    self.assertAllEqual((n_relation,), actual_receivers.shape)\n    self.assertAllEqual((n_relation,), actual_senders.shape)\n    self.assertAllEqual((len(self.graphs_dicts_in),),\n                        graphs_tuple.n_edge.get_shape().as_list())\n\n  @parameterized.named_parameters(\n      (""no self edges"", False),\n      (""self edges"", True),\n  )\n  def test_fully_connect_graph_dynamic_with_dynamic_sizes(\n      self, exclude_self_edges):\n    for g in self.graphs_dicts_in:\n      g.pop(""edges"")\n      g.pop(""receivers"")\n      g.pop(""senders"")\n    n_relation = 0\n    for g in self.graphs_dicts_in:\n      n_node = g[""nodes""].shape[0]\n      if exclude_self_edges:\n        n_relation += n_node * (n_node - 1)\n      else:\n        n_relation += n_node * n_node\n\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    graphs_tuple = graphs_tuple.map(test_utils.mask_leading_dimension,\n                                    [""nodes"", ""globals"", ""n_node"", ""n_edge""])\n    graphs_tuple = utils_tf.fully_connect_graph_dynamic(graphs_tuple,\n                                                        exclude_self_edges)\n    with tf.Session() as sess:\n      actual_receivers, actual_senders, actual_n_edge = sess.run(\n          [graphs_tuple.receivers, graphs_tuple.senders, graphs_tuple.n_edge])\n    self.assertAllEqual((n_relation,), actual_receivers.shape)\n    self.assertAllEqual((n_relation,), actual_senders.shape)\n    self.assertAllEqual((len(self.graphs_dicts_in),), actual_n_edge.shape)\n    expected_edges = []\n    offset = 0\n    for graph in self.graphs_dicts_in:\n      n_node = graph[""nodes""].shape[0]\n      for e1 in range(n_node):\n        for e2 in range(n_node):\n          if not exclude_self_edges or e1 != e2:\n            expected_edges.append((e1 + offset, e2 + offset))\n      offset += n_node\n    actual_edges = zip(actual_receivers, actual_senders)\n    self.assertSetEqual(set(actual_edges), set(expected_edges))\n\n  @parameterized.named_parameters(\n      (""no self edges"", False),\n      (""self edges"", True),\n  )\n  def test_fully_connect_graph_static(self, exclude_self_edges):\n    for g in self.graphs_dicts_in:\n      g.pop(""edges"")\n      g.pop(""receivers"")\n      g.pop(""senders"")\n    num_graphs = 2\n    num_nodes = 3\n    if exclude_self_edges:\n      n_edges = num_nodes * (num_nodes - 1)\n    else:\n      n_edges = num_nodes * num_nodes\n    n_relation = num_graphs * n_edges\n    graphs_dicts = [{\n        ""nodes"": tf.zeros([num_nodes, 1])\n    } for _ in range(num_graphs)]\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(graphs_dicts)\n    graphs_tuple = utils_tf.fully_connect_graph_static(graphs_tuple,\n                                                       exclude_self_edges)\n    self.assertAllEqual((n_relation,),\n                        graphs_tuple.receivers.get_shape().as_list())\n    self.assertAllEqual((n_relation,),\n                        graphs_tuple.senders.get_shape().as_list())\n    self.assertAllEqual((num_graphs,),\n                        graphs_tuple.n_edge.get_shape().as_list())\n    with tf.Session() as sess:\n      actual_receivers, actual_senders, actual_n_edge = sess.run(\n          [graphs_tuple.receivers, graphs_tuple.senders, graphs_tuple.n_edge])\n    expected_edges = []\n    offset = 0\n    for _ in range(num_graphs):\n      for v1 in range(num_nodes):\n        for v2 in range(num_nodes):\n          if not exclude_self_edges or v1 != v2:\n            expected_edges.append((v1 + offset, v2 + offset))\n      offset += num_nodes\n    actual_edges = zip(actual_receivers, actual_senders)\n    self.assertNDArrayNear(\n        np.array([n_edges] * num_graphs), actual_n_edge, 1e-4)\n    self.assertSetEqual(set(actual_edges), set(expected_edges))\n\n  def test_fully_connect_graph_static_with_dynamic_sizes_raises(self):\n    for g in self.graphs_dicts_in:\n      g.pop(""edges"")\n      g.pop(""receivers"")\n      g.pop(""senders"")\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    graphs_tuple_1 = graphs_tuple.map(test_utils.mask_leading_dimension,\n                                      [""n_node""])\n    with self.assertRaisesRegexp(ValueError, ""known at construction time""):\n      utils_tf.fully_connect_graph_static(graphs_tuple_1)\n    graphs_tuple_2 = graphs_tuple.map(test_utils.mask_leading_dimension,\n                                      [""nodes""])\n    with self.assertRaisesRegexp(ValueError, ""known at construction time""):\n      utils_tf.fully_connect_graph_static(graphs_tuple_2)\n    with self.assertRaisesRegexp(ValueError, ""the same in all graphs""):\n      utils_tf.fully_connect_graph_static(graphs_tuple)\n\n\nclass GraphsTupleConversionTests(test_utils.GraphsTest, parameterized.TestCase):\n  """"""Tests for the method converting between data dicts and GraphsTuple.""""""\n\n  @parameterized.named_parameters((""all fields defined"", []), (\n      ""no edge features"",\n      [""edges""],\n  ), (\n      ""no node features"",\n      [""nodes""],\n  ), (\n      ""no globals"",\n      [""globals""],\n  ), (\n      ""no edges"",\n      [""edges"", ""receivers"", ""senders""],\n  ))\n  def test_data_dicts_to_graphs_tuple(self, none_fields):\n    """"""Fields in `none_fields` will be cleared out.""""""\n    for field in none_fields:\n      for graph_dict in self.graphs_dicts_in:\n        if field in graph_dict:\n          if field == ""nodes"":\n            graph_dict[""n_node""] = graph_dict[""nodes""].shape[0]\n          graph_dict[field] = None\n        self.reference_graph = self.reference_graph._replace(**{field: None})\n      if field == ""senders"":\n        self.reference_graph = self.reference_graph._replace(\n            n_edge=np.zeros_like(self.reference_graph.n_edge))\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    for field in none_fields:\n      self.assertEqual(None, getattr(graphs_tuple, field))\n    graphs_tuple = graphs_tuple.map(tf.no_op, none_fields)\n    with tf.Session() as sess:\n      self._assert_graph_equals_np(self.reference_graph, sess.run(graphs_tuple))\n\n  @parameterized.parameters((""receivers"",), (""senders"",))\n  def test_data_dicts_to_graphs_tuple_raises(self, none_field):\n    """"""Fields that cannot be missing.""""""\n    for graph_dict in self.graphs_dicts_in:\n      graph_dict[none_field] = None\n    with self.assertRaisesRegexp(ValueError, none_field):\n      utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n\n  def test_data_dicts_to_graphs_tuple_no_raise(self):\n    """"""Not having nodes is fine, if the number of nodes is provided.""""""\n    for graph_dict in self.graphs_dicts_in:\n      graph_dict[""n_node""] = graph_dict[""nodes""].shape[0]\n      graph_dict[""nodes""] = None\n    utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n\n  def test_data_dicts_to_graphs_tuple_cast_types(self):\n    """"""Index and number fields should be cast to tensors of the right type.""""""\n    for graph_dict in self.graphs_dicts_in:\n      graph_dict[""n_node""] = np.array(\n          graph_dict[""nodes""].shape[0], dtype=np.int64)\n      graph_dict[""receivers""] = graph_dict[""receivers""].astype(np.int16)\n      graph_dict[""senders""] = graph_dict[""senders""].astype(np.float64)\n      graph_dict[""nodes""] = graph_dict[""nodes""].astype(np.float64)\n      graph_dict[""edges""] = tf.constant(graph_dict[""edges""], dtype=tf.float64)\n    out = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    for key in [""n_node"", ""n_edge"", ""receivers"", ""senders""]:\n      self.assertEqual(tf.int32, getattr(out, key).dtype)\n      self.assertEqual(type(tf.int32), type(getattr(out, key).dtype))\n    for key in [""nodes"", ""edges""]:\n      self.assertEqual(type(tf.float64), type(getattr(out, key).dtype))\n      self.assertEqual(tf.float64, getattr(out, key).dtype)\n\n\nclass GraphsIndexingTests(test_utils.GraphsTest, parameterized.TestCase):\n  """"""Tests for the `get_graph` method.""""""\n\n  @parameterized.named_parameters((""int_index"", False),\n                                  (""tensor_index"", True))\n  def test_getitem_one(self, use_tensor_index):\n    index = 2\n    expected = self.graphs_dicts_out[index]\n\n    if use_tensor_index:\n      index = tf.constant(index)\n\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    graph_op = utils_tf.get_graph(graphs_tuple, index)\n    graph_op = utils_tf.make_runnable_in_session(graph_op)\n\n    with tf.Session() as sess:\n      graph = sess.run(graph_op)\n    actual, = utils_np.graphs_tuple_to_data_dicts(graph)\n\n    for k, v in expected.items():\n      self.assertAllClose(v, actual[k])\n    self.assertEqual(expected[""nodes""].shape[0], actual[""n_node""])\n    self.assertEqual(expected[""edges""].shape[0], actual[""n_edge""])\n\n  @parameterized.named_parameters((""int_slice"", False),\n                                  (""tensor_slice"", True))\n  def test_getitem(self, use_tensor_slice):\n    index = slice(1, 3)\n    expected = self.graphs_dicts_out[index]\n\n    if use_tensor_slice:\n      index = slice(tf.constant(index.start), tf.constant(index.stop))\n\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    graphs2_op = utils_tf.get_graph(graphs_tuple, index)\n    graphs2_op = utils_tf.make_runnable_in_session(graphs2_op)\n\n    with tf.Session() as sess:\n      graphs2 = sess.run(graphs2_op)\n    actual = utils_np.graphs_tuple_to_data_dicts(graphs2)\n\n    for ex, ac in zip(expected, actual):\n      for k, v in ex.items():\n        self.assertAllClose(v, ac[k])\n      self.assertEqual(ex[""nodes""].shape[0], ac[""n_node""])\n      self.assertEqual(ex[""edges""].shape[0], ac[""n_edge""])\n\n  @parameterized.named_parameters(\n      (""index_bad_type"", 1.,\n       TypeError, ""Index must be a valid scalar integer""),\n      (""index_bad_shape"", tf.constant([0, 1]),\n       TypeError, ""Valid tensor indices must be scalars""),\n      (""index_bad_dtype"", tf.constant(1.),\n       TypeError, ""Valid tensor indices must have types""),\n      (""slice_bad_type_stop"", slice(1.),\n       TypeError, ""Valid tensor indices must be integers""),\n      (""slice_bad_shape_stop"", slice(tf.constant([0, 1])),\n       TypeError, ""Valid tensor indices must be scalars""),\n      (""slice_bad_dtype_stop"", slice(tf.constant(1.)),\n       TypeError, ""Valid tensor indices must have types""),\n      (""slice_bad_type_start"", slice(0., 1),\n       TypeError, ""Valid tensor indices must be integers""),\n      (""slice_bad_shape_start"", slice(tf.constant([0, 1]), 1),\n       TypeError, ""Valid tensor indices must be scalars""),\n      (""slice_bad_dtype_start"", slice(tf.constant(0.), 1),\n       TypeError, ""Valid tensor indices must have types""),\n      (""slice_with_step"", slice(0, 1, 1),\n       ValueError, ""slices with step/stride are not supported""),\n  )\n  def test_raises(self, index, error_type, message):\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    with self.assertRaisesRegexp(error_type, message):\n      utils_tf.get_graph(graphs_tuple, index)\n\n\nclass TestNumGraphs(test_utils.GraphsTest):\n  """"""Tests for the `get_num_graphs` function.""""""\n\n  def setUp(self):\n    super(TestNumGraphs, self).setUp()\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    self.empty_graph = graphs_tuple.map(lambda _: None,\n                                        graphs.GRAPH_DATA_FIELDS)\n\n  def test_num_graphs_static(self):\n    graph = self.empty_graph.replace(n_node=tf.zeros([3], dtype=tf.int32))\n    self.assertEqual(3, utils_tf.get_num_graphs(graph))\n\n  def test_num_graphs_dynamic(self):\n    n_node_placeholder = tf.placeholder(tf.int32, [None])\n    graph = self.empty_graph.replace(n_node=n_node_placeholder)\n    num_graphs = utils_tf.get_num_graphs(graph)\n    with tf.Session() as sess:\n      actual_num_graphs = sess.run(\n          num_graphs, {n_node_placeholder: np.zeros([3], dtype=np.int32)})\n    self.assertEqual(3, actual_num_graphs)\n\n\nclass TestSpecsFromGraphsTuple(test_utils.GraphsTest, parameterized.TestCase):\n  """"""Tests for the `spec_from_graphs_tuple` function.""""""\n\n  @parameterized.named_parameters(\n      (""dynamic_nodes_edges_not_batched_without_constants"",\n       True, True, False, False, False, None),\n      (""dynamic_num_graphs_not_batched_without_constants"",\n       False, False, True, False, False, None),\n      (""static_num_graphs_not_batched_without_constants"",\n       False, False, True, False, False, None),\n      (""static_num_graphs_batched_without_constants"",\n       False, False, True, True, False, None),\n      (""dynamic_nodes_edges_not_batched_with_constants"",\n       True, True, False, False, True, None),\n      (""dynamic_num_graphs_not_batched_with_constants"",\n       False, False, True, False, True, None),\n      (""static_num_graphs_not_batched_with_constants"",\n       False, False, True, False, True, None),\n      (""static_num_graphs_batched_with_constants"",\n       False, False, True, True, True, None),\n      (""dynamic_graphs_batched_empty_nested_features"",\n       False, False, True, False, False, ""emtpy_nests""),\n      (""dynamic_graphs_batched_deep_nested_features"",\n       False, False, True, False, False, ""deep_nests""),\n      )\n  def test_correct_signature(\n      self,\n      dynamic_num_nodes,\n      dynamic_num_edges,\n      dynamic_num_graphs,\n      batched,\n      replace_globals_with_constant,\n      nested_features_type):\n    """"""Tests that the correct spec is created when using different options.""""""\n\n    if batched:\n      input_data_dicts = [self.graphs_dicts[1], self.graphs_dicts[2]]\n    else:\n      input_data_dicts = [self.graphs_dicts[1]]\n\n    graph = utils_np.data_dicts_to_graphs_tuple(input_data_dicts)\n    num_graphs = len(input_data_dicts)\n    num_edges = sum(graph.n_edge).item()\n    num_nodes = sum(graph.n_node).item()\n\n    # Manually setting edges and globals fields to give some variety in\n    # testing situations.\n    # Making edges have rank 1 to .\n    graph = graph.replace(edges=np.zeros(num_edges))\n\n    # Make a constant field.\n    if replace_globals_with_constant:\n      graph = graph.replace(globals=np.array(0.0, dtype=np.float32))\n\n    if nested_features_type is not None:\n      graph = self._add_nested_features(graph, nested_features_type)\n\n    spec_signature = utils_tf.specs_from_graphs_tuple(\n        graph, dynamic_num_graphs, dynamic_num_nodes, dynamic_num_edges)\n\n    # Captures if nodes/edges will be dynamic either due to dynamic nodes/edges\n    # or dynamic graphs.\n    dynamic_nodes_or_graphs = dynamic_num_nodes or dynamic_num_graphs\n    dynamic_edges_or_graphs = dynamic_num_edges or dynamic_num_graphs\n\n    num_edges = None if dynamic_edges_or_graphs else num_edges\n    num_nodes = None if dynamic_nodes_or_graphs else num_nodes\n    num_graphs = None if dynamic_num_graphs else num_graphs\n\n    if replace_globals_with_constant:\n      expected_globals_shape = []\n    else:\n      expected_globals_shape = [num_graphs,] + test_utils.GLOBALS_DIMS\n\n    expected_answer = graphs.GraphsTuple(\n        nodes=tf.TensorSpec(\n            shape=[num_nodes,] + test_utils.NODES_DIMS,\n            dtype=tf.float32),\n        edges=tf.TensorSpec(\n            shape=[num_edges],  # Edges were manually replaced to have dim 1.\n            dtype=tf.float64),\n        n_node=tf.TensorSpec(\n            shape=[num_graphs],\n            dtype=tf.int32),\n        n_edge=tf.TensorSpec(\n            shape=[num_graphs],\n            dtype=tf.int32),\n        globals=tf.TensorSpec(\n            shape=expected_globals_shape,\n            dtype=tf.float32),\n        receivers=tf.TensorSpec(\n            shape=[num_edges],\n            dtype=tf.int32),\n        senders=tf.TensorSpec(\n            shape=[num_edges],\n            dtype=tf.int32),\n        )\n\n    if nested_features_type is not None:\n      expected_answer = self._add_nested_features(\n          expected_answer, nested_features_type)\n\n    with self.subTest(name=""Correct Type.""):\n      self.assertIsInstance(spec_signature, graphs.GraphsTuple)\n\n    with self.subTest(name=""Correct Structure.""):\n      tree.assert_same_structure(spec_signature, expected_answer)\n\n    with self.subTest(name=""Correct Signature.""):\n      def assert_equal(actual, expected):\n        self.assertEqual(actual, expected)\n        return True\n      tree.map_structure(assert_equal, spec_signature, expected_answer)\n\n  def _add_nested_features(self, graphs_tuple, nested_feature_type):\n\n    if nested_feature_type == ""emtpy_nests"":\n      return graphs_tuple.replace(\n          nodes={},\n          edges=tuple([]),\n          globals=[])\n    else:\n      return graphs_tuple.replace(\n          nodes={""a"": graphs_tuple.nodes, ""b"": [graphs_tuple.nodes]},\n          edges=(graphs_tuple.edges, {""c"": graphs_tuple.edges}),\n          globals=[graphs_tuple.globals, {""d"": graphs_tuple.globals}])\n\n  @parameterized.parameters(\n      (graphs.GLOBALS,), (graphs.EDGES,), (graphs.NODES,))\n  def test_none_throws_error(self, none_field):\n    """"""Tests that an error is thrown if a GraphsTuple field is None.""""""\n    graphs_tuple = utils_np.data_dicts_to_graphs_tuple([self.graphs_dicts[1]])\n    graphs_tuple = graphs_tuple.replace(**{none_field: None})\n    with self.assertRaisesRegex(\n        ValueError, ""`{}` was `None`. All fields of the `G"".format(none_field)):\n      utils_tf.specs_from_graphs_tuple(graphs_tuple)\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
graph_nets/tests_tf2/__init__.py,0,"b'# Copyright 2018 The GraphNets Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Graph networks library tests for TF2.""""""\n'"
graph_nets/tests_tf2/blocks_test.py,73,"b'# Copyright 2018 The GraphNets Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for blocks.py in Tensorflow 2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\n\nfrom absl.testing import parameterized\nfrom graph_nets import blocks\n\nfrom graph_nets import graphs\nfrom graph_nets import utils_np\nfrom graph_nets import utils_tf\nimport numpy as np\nimport sonnet as snt\nimport tensorflow as tf\n\n\n\n\nSMALL_GRAPH_1 = {\n    ""globals"": [1.1, 1.2, 1.3, 1.4],\n    ""nodes"": [[10.1, 10.2], [20.1, 20.2], [30.1, 30.2]],\n    ""edges"": [[101., 102., 103., 104.], [201., 202., 203., 204.]],\n    ""senders"": [0, 1],\n    ""receivers"": [1, 2],\n}\n\nSMALL_GRAPH_2 = {\n    ""globals"": [-1.1, -1.2, -1.3, -1.4],\n    ""nodes"": [[-10.1, -10.2], [-20.1, -20.2], [-30.1, -30.2]],\n    ""edges"": [[-101., -102., -103., -104.]],\n    ""senders"": [1,],\n    ""receivers"": [2,],\n}\n\nSMALL_GRAPH_3 = {\n    ""globals"": [1.1, 1.2, 1.3, 1.4],\n    ""nodes"": [[10.1, 10.2], [20.1, 20.2], [30.1, 30.2]],\n    ""edges"": [[101., 102., 103., 104.], [201., 202., 203., 204.]],\n    ""senders"": [1, 1],\n    ""receivers"": [0, 2],\n}\n\nSMALL_GRAPH_4 = {\n    ""globals"": [1.1, 1.2, 1.3, 1.4],\n    ""nodes"": [[10.1, 10.2], [20.1, 20.2], [30.1, 30.2]],\n    ""edges"": [[101., 102., 103., 104.], [201., 202., 203., 204.]],\n    ""senders"": [0, 2],\n    ""receivers"": [1, 1],\n}\n\n\nclass GraphModuleTest(tf.test.TestCase, parameterized.TestCase):\n  """"""Base class for all the tests in this file.""""""\n\n  def setUp(self):\n    super(GraphModuleTest, self).setUp()\n    tf.random.set_seed(0)\n\n  def _get_input_graph(self, none_fields=None):\n    if none_fields is None:\n      none_fields = []\n    input_graph = utils_tf.data_dicts_to_graphs_tuple(\n        [SMALL_GRAPH_1, SMALL_GRAPH_2, SMALL_GRAPH_3, SMALL_GRAPH_4])\n    input_graph = input_graph.map(lambda _: None, none_fields)\n    return input_graph\n\n  def _get_shaped_input_graph(self):\n    return graphs.GraphsTuple(\n        nodes=tf.zeros([3, 4, 5, 11], dtype=tf.float32),\n        edges=tf.zeros([5, 4, 5, 12], dtype=tf.float32),\n        globals=tf.zeros([2, 4, 5, 13], dtype=tf.float32),\n        receivers=tf.range(5, dtype=tf.int32) // 3,\n        senders=tf.range(5, dtype=tf.int32) % 3,\n        n_node=tf.constant([2, 1], dtype=tf.int32),\n        n_edge=tf.constant([3, 2], dtype=tf.int32),\n    )\n\n  def _assert_build_and_run(self, network, input_graph):\n    # No error at construction time.\n    _ = network(input_graph)\n\n\nBROADCAST_GLOBAL_TO_EDGES = [\n    [1.1, 1.2, 1.3, 1.4],\n    [1.1, 1.2, 1.3, 1.4],\n    [-1.1, -1.2, -1.3, -1.4],\n]\n\nBROADCAST_GLOBAL_TO_NODES = [\n    [1.1, 1.2, 1.3, 1.4],\n    [1.1, 1.2, 1.3, 1.4],\n    [1.1, 1.2, 1.3, 1.4],\n    [-1.1, -1.2, -1.3, -1.4],\n    [-1.1, -1.2, -1.3, -1.4],\n    [-1.1, -1.2, -1.3, -1.4],\n]\n\nSENDER_NODES_TO_EDGES = [\n    [10.1, 10.2],\n    [20.1, 20.2],\n    [-20.1, -20.2],\n]\n\nRECEIVER_NODES_TO_EDGES = [\n    [20.1, 20.2],\n    [30.1, 30.2],\n    [-30.1, -30.2],\n]\n\n\nclass BroadcastersTest(GraphModuleTest):\n  """"""Tests for the broadcasters.""""""\n\n  @parameterized.named_parameters(\n      (""globals_to_edges"",\n       blocks.broadcast_globals_to_edges, BROADCAST_GLOBAL_TO_EDGES),\n      (""globals_to_nodes"",\n       blocks.broadcast_globals_to_nodes, BROADCAST_GLOBAL_TO_NODES),\n      (""sender_nodes_to_edges"",\n       blocks.broadcast_sender_nodes_to_edges, SENDER_NODES_TO_EDGES),\n      (""receiver_nodes_to_edges"",\n       blocks.broadcast_receiver_nodes_to_edges, RECEIVER_NODES_TO_EDGES),\n  )\n  def test_output_values(self, broadcaster, expected):\n    """"""Test the broadcasted output value.""""""\n    input_graph = utils_tf.data_dicts_to_graphs_tuple(\n        [SMALL_GRAPH_1, SMALL_GRAPH_2])\n    broadcasted_out = broadcaster(input_graph)\n    self.assertNDArrayNear(\n        np.array(expected, dtype=np.float32), broadcasted_out, err=1e-4)\n\n  @parameterized.named_parameters(\n      (""globals_to_edges"",\n       blocks.broadcast_globals_to_edges, BROADCAST_GLOBAL_TO_EDGES),\n      (""globals_to_nodes"",\n       blocks.broadcast_globals_to_nodes, BROADCAST_GLOBAL_TO_NODES),\n      (""sender_nodes_to_edges"",\n       blocks.broadcast_sender_nodes_to_edges, SENDER_NODES_TO_EDGES),\n      (""receiver_nodes_to_edges"",\n       blocks.broadcast_receiver_nodes_to_edges, RECEIVER_NODES_TO_EDGES),\n  )\n  def test_output_values_larger_rank(self, broadcaster, expected):\n    """"""Test the broadcasted output value.""""""\n    input_graph = utils_tf.data_dicts_to_graphs_tuple(\n        [SMALL_GRAPH_1, SMALL_GRAPH_2])\n    input_graph = input_graph.map(\n        lambda v: tf.reshape(v, [v.get_shape().as_list()[0]] + [2, -1]))\n    broadcasted_out = broadcaster(input_graph)\n    self.assertNDArrayNear(\n        np.reshape(np.array(expected, dtype=np.float32),\n                   [len(expected)] + [2, -1]),\n        broadcasted_out,\n        err=1e-4)\n\n  @parameterized.named_parameters(\n      (""globals_to_edges_no_globals"",\n       blocks.broadcast_globals_to_edges, (""globals"",)),\n      (""globals_to_nodes_no_globals"",\n       blocks.broadcast_globals_to_nodes, (""globals"",)),\n      (""sender_nodes_to_edges_none_nodes"",\n       blocks.broadcast_sender_nodes_to_edges, (""nodes"",)),\n      (""sender_nodes_to_edges_none_senders"",\n       blocks.broadcast_sender_nodes_to_edges,\n       (""edges"", ""senders"", ""receivers"")),\n      (""receiver_nodes_to_edges_none_nodes"",\n       blocks.broadcast_receiver_nodes_to_edges, (""nodes"",)),\n  )\n  def test_missing_field_raises_exception(self, broadcaster, none_fields):\n    """"""Test that an error is raised if a required field is `None`.""""""\n    input_graph = self._get_input_graph(none_fields)\n    with self.assertRaisesRegexp(\n        ValueError, ""field cannot be None when broadcasting""):\n      broadcaster(input_graph)\n\n\nclass ReducersTest(GraphModuleTest):\n  """"""Tests for the reducers.""""""\n\n  @parameterized.parameters(\n      (blocks.unsorted_segment_min_or_zero,\n       [[0., 0.],\n        [0.1, -0.1],\n        [0.2, -0.3],\n        [0.4, -0.6],\n        [0.7, -1.],\n        [0.9, -0.9],\n        [0., 0.]]),\n      (blocks.unsorted_segment_max_or_zero,\n       [[0., 0.],\n        [0.1, -0.1],\n        [0.3, -0.2],\n        [0.6, -0.4],\n        [1., -0.7],\n        [0.9, -0.9],\n        [0., 0.]]),\n  )\n  def test_output_values(self, reducer, expected_values):\n    input_values_np = np.array([[0.1, -0.1],\n                                [0.2, -0.2],\n                                [0.3, -0.3],\n                                [0.4, -0.4],\n                                [0.5, -0.5],\n                                [0.6, -0.6],\n                                [0.7, -0.7],\n                                [0.8, -0.8],\n                                [0.9, -0.9],\n                                [1., -1.]], dtype=np.float32)\n    input_indices_np = np.array([1, 2, 2, 3, 3, 3, 4, 4, 5, 4], dtype=np.int32)\n    num_groups_np = np.array(7, dtype=np.int32)\n\n    input_indices = tf.constant(input_indices_np, dtype=tf.int32)\n    input_values = tf.constant(input_values_np, dtype=tf.float32)\n    num_groups = tf.constant(num_groups_np, dtype=tf.int32)\n\n    reduced_out = reducer(input_values, input_indices, num_groups)\n\n    self.assertNDArrayNear(\n        np.array(expected_values, dtype=np.float32), reduced_out, err=1e-4)\n\n\nSEGMENT_SUM_EDGES_TO_GLOBALS = [\n    [302., 304., 306., 308.],\n    [-101., -102., -103., -104.],\n    [302., 304., 306., 308.],\n    [302., 304., 306., 308.],\n]\n\nSEGMENT_SUM_NODES_TO_GLOBALS = [\n    [60.3, 60.6],\n    [-60.3, -60.6],\n    [60.3, 60.6],\n    [60.3, 60.6],\n]\n\nSEGMENT_SUM_SENT_EDGES_TO_NODES = [\n    [101., 102., 103., 104.],\n    [201., 202., 203., 204.],\n    [0., 0., 0., 0.],\n    [0., 0., 0., 0.],\n    [-101., -102., -103., -104.],\n    [0., 0., 0., 0.],\n    [0., 0., 0., 0.],\n    [302., 304., 306., 308.],\n    [0., 0., 0., 0.,],\n    [101., 102., 103., 104.],\n    [0., 0., 0., 0.],\n    [201., 202., 203., 204.],\n]\n\nSEGMENT_SUM_RECEIVED_EDGES_TO_NODES = [\n    [0., 0., 0., 0.],\n    [101., 102., 103., 104.],\n    [201., 202., 203., 204.],\n    [0., 0., 0., 0.],\n    [0., 0., 0., 0.],\n    [-101., -102., -103., -104.],\n    [101., 102., 103., 104.],\n    [0., 0., 0., 0.],\n    [201., 202., 203., 204.],\n    [0., 0., 0., 0.],\n    [302., 304., 306., 308,],\n    [0., 0., 0., 0.],\n]\n\n\nclass FieldAggregatorsTest(GraphModuleTest):\n\n  @parameterized.named_parameters(\n      (""edges_to_globals"",\n       blocks.EdgesToGlobalsAggregator(tf.math.unsorted_segment_sum),\n       SEGMENT_SUM_EDGES_TO_GLOBALS,),\n      (""nodes_to_globals"",\n       blocks.NodesToGlobalsAggregator(tf.math.unsorted_segment_sum),\n       SEGMENT_SUM_NODES_TO_GLOBALS,),\n      (""sent_edges_to_nodes"",\n       blocks.SentEdgesToNodesAggregator(tf.math.unsorted_segment_sum),\n       SEGMENT_SUM_SENT_EDGES_TO_NODES,),\n      (""received_edges_to_nodes"",\n       blocks.ReceivedEdgesToNodesAggregator(tf.math.unsorted_segment_sum),\n       SEGMENT_SUM_RECEIVED_EDGES_TO_NODES),\n  )\n  def test_output_values(self, aggregator, expected):\n    input_graph = self._get_input_graph()\n    aggregated_out = aggregator(input_graph)\n    self.assertNDArrayNear(\n        np.array(expected, dtype=np.float32), aggregated_out, err=1e-4)\n\n  @parameterized.named_parameters(\n      (""edges_to_globals"",\n       blocks.EdgesToGlobalsAggregator(tf.math.unsorted_segment_sum),\n       SEGMENT_SUM_EDGES_TO_GLOBALS,),\n      (""nodes_to_globals"",\n       blocks.NodesToGlobalsAggregator(tf.math.unsorted_segment_sum),\n       SEGMENT_SUM_NODES_TO_GLOBALS,),\n      (""sent_edges_to_nodes"",\n       blocks.SentEdgesToNodesAggregator(tf.math.unsorted_segment_sum),\n       SEGMENT_SUM_SENT_EDGES_TO_NODES,),\n      (""received_edges_to_nodes"",\n       blocks.ReceivedEdgesToNodesAggregator(tf.math.unsorted_segment_sum),\n       SEGMENT_SUM_RECEIVED_EDGES_TO_NODES),\n  )\n  def test_output_values_larger_rank(self, aggregator, expected):\n    input_graph = self._get_input_graph()\n    input_graph = input_graph.map(\n        lambda v: tf.reshape(v, [v.get_shape().as_list()[0]] + [2, -1]))\n    aggregated_out = aggregator(input_graph)\n    self.assertNDArrayNear(\n        np.reshape(np.array(expected, dtype=np.float32),\n                   [len(expected)] + [2, -1]),\n        aggregated_out,\n        err=1e-4)\n\n  @parameterized.named_parameters(\n      (""received edges to nodes missing edges"",\n       blocks.ReceivedEdgesToNodesAggregator, ""edges""),\n      (""sent edges to nodes missing edges"",\n       blocks.SentEdgesToNodesAggregator, ""edges""),\n      (""nodes to globals missing nodes"",\n       blocks.NodesToGlobalsAggregator, ""nodes""),\n      (""edges to globals missing nodes"",\n       blocks.EdgesToGlobalsAggregator, ""edges""),)\n  def test_missing_field_raises_exception(self, constructor, none_field):\n    """"""Tests that aggregator fail if a required field is missing.""""""\n    input_graph = self._get_input_graph([none_field])\n    with self.assertRaisesRegexp(ValueError, none_field):\n      constructor(tf.math.unsorted_segment_sum)(input_graph)\n\n  @parameterized.named_parameters(\n      (""received edges to nodes missing nodes and globals"",\n       blocks.ReceivedEdgesToNodesAggregator, [""nodes"", ""globals""]),\n      (""sent edges to nodes missing nodes and globals"",\n       blocks.SentEdgesToNodesAggregator, [""nodes"", ""globals""]),\n      (""nodes to globals missing edges and globals"",\n       blocks.NodesToGlobalsAggregator,\n       [""edges"", ""receivers"", ""senders"", ""globals""]),\n      (""edges to globals missing globals"",\n       blocks.EdgesToGlobalsAggregator, [""globals""]),\n  )\n  def test_unused_field_can_be_none(self, constructor, none_fields):\n    """"""Tests that aggregator fail if a required field is missing.""""""\n    input_graph = self._get_input_graph(none_fields)\n    constructor(tf.math.unsorted_segment_sum)(input_graph)\n\n\nclass EdgeBlockTest(GraphModuleTest):\n\n  def setUp(self):\n    super(EdgeBlockTest, self).setUp()\n    self._scale = 10.\n    self._edge_model_fn = lambda: lambda features: features * self._scale\n\n  @parameterized.named_parameters(\n      (""all inputs"", True, True, True, True),\n      (""edges nodes only"", True, False, False, False),\n      (""receiver nodes only"", False, True, False, False),\n      (""sender nodes only"", False, False, True, False),\n      (""globals only"", False, False, False, True),\n      (""edges and sender nodes"", True, False, True, False),\n      (""receiver nodes and globals"", False, True, False, True),\n  )\n  def test_output_values(\n      self, use_edges, use_receiver_nodes, use_sender_nodes, use_globals):\n    """"""Compares the output of an EdgeBlock to an explicit computation.""""""\n    input_graph = self._get_input_graph()\n    edge_block = blocks.EdgeBlock(\n        edge_model_fn=self._edge_model_fn,\n        use_edges=use_edges,\n        use_receiver_nodes=use_receiver_nodes,\n        use_sender_nodes=use_sender_nodes,\n        use_globals=use_globals)\n    output_graph_out = edge_block(input_graph)\n\n    model_inputs = []\n    if use_edges:\n      model_inputs.append(input_graph.edges)\n    if use_receiver_nodes:\n      model_inputs.append(blocks.broadcast_receiver_nodes_to_edges(input_graph))\n    if use_sender_nodes:\n      model_inputs.append(blocks.broadcast_sender_nodes_to_edges(input_graph))\n    if use_globals:\n      model_inputs.append(blocks.broadcast_globals_to_edges(input_graph))\n\n    model_inputs = tf.concat(model_inputs, axis=-1)\n    self.assertIs(input_graph.nodes, output_graph_out.nodes)\n    self.assertIs(input_graph.globals, output_graph_out.globals)\n\n    expected_output_edges = model_inputs * self._scale\n    self.assertNDArrayNear(\n        expected_output_edges.numpy(), output_graph_out.edges.numpy(), err=1e-4)\n\n  @parameterized.named_parameters(\n      (""all inputs"", True, True, True, True, 12),\n      (""edges only"", True, False, False, False, 4),\n      (""receivers only"", False, True, False, False, 2),\n      (""senders only"", False, False, True, False, 2),\n      (""globals only"", False, False, False, True, 4),\n  )\n  def test_created_variables(self,\n                             use_edges, use_receiver_nodes, use_sender_nodes,\n                             use_globals, expected_first_dim_w):\n    """"""Verifies the variable names and shapes created by an EdgeBlock.""""""\n    output_size = 10\n    expected_var_shapes_dict = {\n        ""edge_block/mlp/linear_0/b:0"": [output_size],\n        ""edge_block/mlp/linear_0/w:0"": [expected_first_dim_w, output_size]}\n\n    input_graph = self._get_input_graph()\n\n    edge_block = blocks.EdgeBlock(\n        edge_model_fn=functools.partial(snt.nets.MLP,\n                                        output_sizes=[output_size]),\n        use_edges=use_edges,\n        use_receiver_nodes=use_receiver_nodes,\n        use_sender_nodes=use_sender_nodes,\n        use_globals=use_globals)\n    edge_block(input_graph)\n\n    variables = edge_block.variables\n    var_shapes_dict = {var.name: var.get_shape().as_list() for var in variables}\n    self.assertDictEqual(expected_var_shapes_dict, var_shapes_dict)\n\n  @parameterized.named_parameters(\n      (""missing node (receivers only)"", False, True, False, False, (""nodes"",)),\n      (""missing node (senders only)"", False, False, True, False, (""nodes"",)),\n      (""missing edge data"", True, False, False, False, (""edges"",)),\n      (""missing edges (but no edge consumption)"", False, True, True, False,\n       (""edges"", ""senders"", ""receivers"")),\n      (""missing globals"", False, False, False, True, (""globals"",)),\n  )\n  def test_missing_field_raises_exception(\n      self, use_edges, use_receiver_nodes, use_sender_nodes, use_globals,\n      none_fields):\n    """"""Checks that missing a required field raises an exception.""""""\n    input_graph = self._get_input_graph(none_fields)\n    edge_block = blocks.EdgeBlock(\n        edge_model_fn=self._edge_model_fn,\n        use_edges=use_edges,\n        use_receiver_nodes=use_receiver_nodes,\n        use_sender_nodes=use_sender_nodes,\n        use_globals=use_globals)\n    with self.assertRaisesRegexp(ValueError, ""field cannot be None""):\n      edge_block(input_graph)\n\n  def test_compatible_higher_rank_no_raise(self):\n    """"""No exception should occur with higher ranks tensors.""""""\n    input_graph = self._get_shaped_input_graph()\n    input_graph = input_graph.map(lambda v: tf.transpose(v, [0, 2, 1, 3]))\n    network = blocks.EdgeBlock(\n        functools.partial(snt.Conv2D, output_channels=10, kernel_shape=[3, 3]))\n    self._assert_build_and_run(network, input_graph)\n\n  @parameterized.named_parameters(\n      (""mismatched edges and r. nodes"", True, True, False, False, ""nodes""),\n      (""mismatched edges and s. nodes"", True, False, True, False, ""nodes""),\n      (""mismatched edges and globals"", True, False, False, True, ""edges""),\n      (""mismatched nodes and globals"", False, True, True, True, ""globals""),\n  )\n  def test_incompatible_higher_rank_inputs_raises(self,\n                                                  use_edges,\n                                                  use_receiver_nodes,\n                                                  use_sender_nodes,\n                                                  use_globals,\n                                                  field):\n    """"""A exception should be raised if the inputs have incompatible shapes.""""""\n    input_graph = self._get_shaped_input_graph()\n    input_graph = input_graph.replace(\n        **{field: tf.transpose(getattr(input_graph, field), [0, 2, 1, 3])})\n    network = blocks.EdgeBlock(\n        functools.partial(snt.Conv2D, output_channels=10, kernel_shape=[3, 3]),\n        use_edges=use_edges,\n        use_receiver_nodes=use_receiver_nodes,\n        use_sender_nodes=use_sender_nodes,\n        use_globals=use_globals\n    )\n    with self.assertRaisesRegexp(\n        tf.errors.InvalidArgumentError, ""Dimensions of inputs should match""):\n      network(input_graph)\n\n  @parameterized.named_parameters(\n      (""mismatched nodes"", True, False, False, True, ""nodes""),\n      (""mismatched edges"", False, True, True, True, ""edges""),\n      (""mismatched globals"", True, True, True, False, ""globals""),\n  )\n  def test_incompatible_higher_rank_inputs_no_raise(self,\n                                                    use_edges,\n                                                    use_receiver_nodes,\n                                                    use_sender_nodes,\n                                                    use_globals,\n                                                    field):\n    """"""No exception should occur if a differently shapped field is not used.""""""\n    input_graph = self._get_shaped_input_graph()\n    input_graph = input_graph.replace(\n        **{field: tf.transpose(getattr(input_graph, field), [0, 2, 1, 3])})\n    network = blocks.EdgeBlock(\n        functools.partial(snt.Conv2D, output_channels=10, kernel_shape=[3, 3]),\n        use_edges=use_edges,\n        use_receiver_nodes=use_receiver_nodes,\n        use_sender_nodes=use_sender_nodes,\n        use_globals=use_globals\n    )\n    self._assert_build_and_run(network, input_graph)\n\n  @parameterized.named_parameters(\n      (""no edges"", False, True, True, ""edges""),\n      (""no nodes"", True, False, True, ""nodes""),\n      (""no globals"", True, True, False, ""globals""),\n  )\n  def test_unused_field_can_be_none(\n      self, use_edges, use_nodes, use_globals, none_field):\n    """"""Checks that computation can handle non-necessary fields left None.""""""\n    input_graph = self._get_input_graph([none_field])\n    edge_block = blocks.EdgeBlock(\n        edge_model_fn=self._edge_model_fn,\n        use_edges=use_edges,\n        use_receiver_nodes=use_nodes,\n        use_sender_nodes=use_nodes,\n        use_globals=use_globals)\n    output_graph = edge_block(input_graph)\n\n    model_inputs = []\n    if use_edges:\n      model_inputs.append(input_graph.edges)\n    if use_nodes:\n      model_inputs.append(blocks.broadcast_receiver_nodes_to_edges(input_graph))\n      model_inputs.append(blocks.broadcast_sender_nodes_to_edges(input_graph))\n    if use_globals:\n      model_inputs.append(blocks.broadcast_globals_to_edges(input_graph))\n\n    model_inputs = tf.concat(model_inputs, axis=-1)\n    self.assertIs(input_graph.nodes, output_graph.nodes)\n    self.assertIs(input_graph.globals, output_graph.globals)\n\n    actual_edges = output_graph.edges.numpy()\n    model_inputs_out = model_inputs.numpy()\n\n    expected_output_edges = model_inputs_out * self._scale\n    self.assertNDArrayNear(expected_output_edges, actual_edges, err=1e-4)\n\n  def test_no_input_raises_exception(self):\n    """"""Checks that receiving no input raises an exception.""""""\n    with self.assertRaisesRegexp(ValueError, ""At least one of ""):\n      blocks.EdgeBlock(\n          edge_model_fn=self._edge_model_fn,\n          use_edges=False,\n          use_receiver_nodes=False,\n          use_sender_nodes=False,\n          use_globals=False)\n\n\nclass NodeBlockTest(GraphModuleTest):\n\n  def setUp(self):\n    super(NodeBlockTest, self).setUp()\n    self._scale = 10.\n    self._node_model_fn = lambda: lambda features: features * self._scale\n\n  @parameterized.named_parameters(\n      (""all inputs, custom reductions"", True, True, True, True,\n       tf.math.unsorted_segment_sum, tf.math.unsorted_segment_mean),\n      (""received edges only, blocks reducer"",\n       True, False, False, False, blocks.unsorted_segment_max_or_zero, None),\n      (""sent edges only, custom reduction"",\n       False, True, False, False, None, tf.math.unsorted_segment_prod),\n      (""nodes only"",\n       False, False, True, False, None, None),\n      (""globals only"",\n       False, False, False, True, None, None),\n      (""received edges and nodes, custom reductions"",\n       True, False, True, False,\n       blocks.unsorted_segment_min_or_zero, tf.math.unsorted_segment_prod),\n      (""sent edges and globals, custom reduction"",\n       False, True, False, True, None, blocks.unsorted_segment_min_or_zero),\n  )\n  def test_output_values(\n      self, use_received_edges, use_sent_edges, use_nodes,\n      use_globals, received_edges_reducer, sent_edges_reducer):\n    """"""Compares the output of a NodeBlock to an explicit computation.""""""\n    input_graph = self._get_input_graph()\n    node_block = blocks.NodeBlock(\n        node_model_fn=self._node_model_fn,\n        use_received_edges=use_received_edges,\n        use_sent_edges=use_sent_edges,\n        use_nodes=use_nodes,\n        use_globals=use_globals,\n        received_edges_reducer=received_edges_reducer,\n        sent_edges_reducer=sent_edges_reducer)\n    output_graph = node_block(input_graph)\n\n    model_inputs = []\n    if use_received_edges:\n      model_inputs.append(\n          blocks.ReceivedEdgesToNodesAggregator(\n              received_edges_reducer)(input_graph))\n    if use_sent_edges:\n      model_inputs.append(\n          blocks.SentEdgesToNodesAggregator(sent_edges_reducer)(input_graph))\n    if use_nodes:\n      model_inputs.append(input_graph.nodes)\n    if use_globals:\n      model_inputs.append(blocks.broadcast_globals_to_nodes(input_graph))\n\n    model_inputs = tf.concat(model_inputs, axis=-1)\n    self.assertIs(input_graph.edges, output_graph.edges)\n    self.assertIs(input_graph.globals, output_graph.globals)\n\n    output_graph_out = utils_tf.nest_to_numpy(output_graph)\n    model_inputs_out = model_inputs\n\n    expected_output_nodes = model_inputs_out * self._scale\n    self.assertNDArrayNear(\n        expected_output_nodes, output_graph_out.nodes, err=1e-4)\n\n  @parameterized.named_parameters(\n      (""all inputs"", True, True, True, True, 14),\n      (""received edges only"", True, False, False, False, 4),\n      (""sent edges only"", False, True, False, False, 4),\n      (""nodes only"", False, False, True, False, 2),\n      (""globals only"", False, False, False, True, 4),\n  )\n  def test_created_variables(self,\n                             use_received_edges, use_sent_edges, use_nodes,\n                             use_globals, expected_first_dim_w):\n    """"""Verifies the variable names and shapes created by a NodeBlock.""""""\n    output_size = 10\n    expected_var_shapes_dict = {\n        ""node_block/mlp/linear_0/b:0"": [output_size],\n        ""node_block/mlp/linear_0/w:0"": [expected_first_dim_w, output_size]}\n\n    input_graph = self._get_input_graph()\n\n    node_block = blocks.NodeBlock(\n        node_model_fn=functools.partial(snt.nets.MLP,\n                                        output_sizes=[output_size]),\n        use_received_edges=use_received_edges,\n        use_sent_edges=use_sent_edges,\n        use_nodes=use_nodes,\n        use_globals=use_globals)\n\n    node_block(input_graph)\n\n    variables = node_block.variables\n    var_shapes_dict = {var.name: var.get_shape().as_list() for var in variables}\n    self.assertDictEqual(expected_var_shapes_dict, var_shapes_dict)\n\n  @parameterized.named_parameters(\n      (""missing nodes"", False, False, True, False, (""nodes"",)),\n      (""missing edge data (receivers only)"",\n       True, False, False, False, (""edges"",)),\n      (""missing edge data (senders only)"",\n       False, True, False, False, (""edges"",)),\n      (""missing globals"", False, False, False, True, (""globals"",)),\n  )\n  def test_missing_field_raises_exception(\n      self, use_received_edges, use_sent_edges, use_nodes, use_globals,\n      none_fields):\n    """"""Checks that missing a required field raises an exception.""""""\n    input_graph = self._get_input_graph(none_fields)\n    node_block = blocks.NodeBlock(\n        node_model_fn=self._node_model_fn,\n        use_received_edges=use_received_edges,\n        use_sent_edges=use_sent_edges,\n        use_nodes=use_nodes,\n        use_globals=use_globals)\n    with self.assertRaisesRegexp(ValueError, ""field cannot be None""):\n      node_block(input_graph)\n\n  @parameterized.named_parameters(\n      (""no received edges reducer"", True, False, None,\n       tf.math.unsorted_segment_sum),\n      (""no sent edges reducer"", False, True, tf.math.unsorted_segment_sum,\n       None),\n  )\n  def test_missing_aggregation_raises_exception(\n      self, use_received_edges, use_sent_edges,\n      received_edges_reducer, sent_edges_reducer):\n    """"""Checks that missing a required aggregation argument raises an error.""""""\n    with self.assertRaisesRegexp(ValueError, ""should not be None""):\n      blocks.NodeBlock(\n          node_model_fn=self._node_model_fn,\n          use_received_edges=use_received_edges,\n          use_sent_edges=use_sent_edges,\n          use_nodes=False,\n          use_globals=False,\n          received_edges_reducer=received_edges_reducer,\n          sent_edges_reducer=sent_edges_reducer)\n\n  def test_compatible_higher_rank_no_raise(self):\n    """"""No exception should occur with higher ranks tensors.""""""\n    input_graph = self._get_shaped_input_graph()\n    input_graph = input_graph.map(lambda v: tf.transpose(v, [0, 2, 1, 3]))\n    network = blocks.NodeBlock(\n        functools.partial(snt.Conv2D, output_channels=10, kernel_shape=[3, 3]))\n    self._assert_build_and_run(network, input_graph)\n\n  @parameterized.named_parameters(\n      (""mismatched nodes and r. edges"", True, False, True, False, ""edges""),\n      (""mismatched nodes and s. edges"", True, False, True, False, ""edges""),\n      (""mismatched edges and globals"", True, False, False, True, ""globals""),\n      (""mismatched nodes and globals"", False, True, True, True, ""globals""),\n  )\n  def test_incompatible_higher_rank_inputs_raises(self,\n                                                  use_received_edges,\n                                                  use_sent_edges,\n                                                  use_nodes,\n                                                  use_globals,\n                                                  field):\n    """"""A exception should be raised if the inputs have incompatible shapes.""""""\n    input_graph = self._get_shaped_input_graph()\n    input_graph = input_graph.replace(\n        **{field: tf.transpose(getattr(input_graph, field), [0, 2, 1, 3])})\n    network = blocks.NodeBlock(\n        functools.partial(snt.Conv2D, output_channels=10, kernel_shape=[3, 3]),\n        use_received_edges=use_received_edges,\n        use_sent_edges=use_sent_edges,\n        use_nodes=use_nodes,\n        use_globals=use_globals\n    )\n    with self.assertRaisesRegexp(\n        tf.errors.InvalidArgumentError,\n        ""Dimensions of inputs should match""):\n      network(input_graph)\n\n  @parameterized.named_parameters(\n      (""mismatched nodes"", True, True, False, True, ""nodes""),\n      (""mismatched edges"", False, False, True, True, ""edges""),\n      (""mismatched globals"", True, True, True, False, ""globals""),\n  )\n  def test_incompatible_higher_rank_inputs_no_raise(self,\n                                                    use_received_edges,\n                                                    use_sent_edges,\n                                                    use_nodes,\n                                                    use_globals,\n                                                    field):\n    """"""No exception should occur if a differently shapped field is not used.""""""\n    input_graph = self._get_shaped_input_graph()\n    input_graph = input_graph.replace(\n        **{field: tf.transpose(getattr(input_graph, field), [0, 2, 1, 3])})\n    network = blocks.NodeBlock(\n        functools.partial(snt.Conv2D, output_channels=10, kernel_shape=[3, 3]),\n        use_received_edges=use_received_edges,\n        use_sent_edges=use_sent_edges,\n        use_nodes=use_nodes,\n        use_globals=use_globals\n    )\n    self._assert_build_and_run(network, input_graph)\n\n  @parameterized.named_parameters(\n      (""no edges"", False, True, True, ""edges""),\n      (""no nodes"", True, False, True, ""nodes""),\n      (""no globals"", True, True, False, ""globals""),\n  )\n  def test_unused_field_can_be_none(\n      self, use_edges, use_nodes, use_globals, none_field):\n    """"""Checks that computation can handle non-necessary fields left None.""""""\n    input_graph = self._get_input_graph([none_field])\n    node_block = blocks.NodeBlock(\n        node_model_fn=self._node_model_fn,\n        use_received_edges=use_edges,\n        use_sent_edges=use_edges,\n        use_nodes=use_nodes,\n        use_globals=use_globals)\n    output_graph = node_block(input_graph)\n\n    model_inputs = []\n    if use_edges:\n      model_inputs.append(\n          blocks.ReceivedEdgesToNodesAggregator(\n              tf.math.unsorted_segment_sum)(input_graph))\n      model_inputs.append(\n          blocks.SentEdgesToNodesAggregator(\n              tf.math.unsorted_segment_sum)(input_graph))\n    if use_nodes:\n      model_inputs.append(input_graph.nodes)\n    if use_globals:\n      model_inputs.append(blocks.broadcast_globals_to_nodes(input_graph))\n\n    model_inputs = tf.concat(model_inputs, axis=-1)\n    self.assertIs(input_graph.edges, output_graph.edges)\n    self.assertIs(input_graph.globals, output_graph.globals)\n\n    actual_nodes = output_graph.nodes.numpy()\n    model_inputs_out = model_inputs.numpy()\n\n    expected_output_nodes = model_inputs_out * self._scale\n    self.assertNDArrayNear(expected_output_nodes, actual_nodes, err=1e-4)\n\n  def test_no_input_raises_exception(self):\n    """"""Checks that receiving no input raises an exception.""""""\n    with self.assertRaisesRegexp(ValueError, ""At least one of ""):\n      blocks.NodeBlock(\n          node_model_fn=self._node_model_fn,\n          use_received_edges=False,\n          use_sent_edges=False,\n          use_nodes=False,\n          use_globals=False)\n\n\nclass GlobalBlockTest(GraphModuleTest):\n  """"""Tests for the GlobalBlock.""""""\n\n  def setUp(self):\n    super(GlobalBlockTest, self).setUp()\n    self._scale = 10.\n    self._global_model_fn = lambda: lambda features: features * self._scale\n\n  @parameterized.named_parameters(\n      (""all_inputs, custom reductions"",\n       True, True, True, tf.math.unsorted_segment_sum,\n       tf.math.unsorted_segment_mean),\n      (""edges only, blocks reducer"",\n       True, False, False, blocks.unsorted_segment_max_or_zero, None),\n      (""nodes only, custom reduction"",\n       False, True, False, None, tf.math.unsorted_segment_prod),\n      (""globals only"",\n       False, False, True, None, None),\n      (""edges and nodes, blocks reducer"",\n       True, True, False, blocks.unsorted_segment_min_or_zero,\n       tf.math.unsorted_segment_prod),\n      (""nodes and globals, blocks reducer"",\n       False, True, True, None, blocks.unsorted_segment_min_or_zero),\n  )\n  def test_output_values(\n      self, use_edges, use_nodes, use_globals, edges_reducer, nodes_reducer):\n    """"""Compares the output of a GlobalBlock to an explicit computation.""""""\n    input_graph = self._get_input_graph()\n    global_block = blocks.GlobalBlock(\n        global_model_fn=self._global_model_fn,\n        use_edges=use_edges,\n        use_nodes=use_nodes,\n        use_globals=use_globals,\n        edges_reducer=edges_reducer,\n        nodes_reducer=nodes_reducer)\n    output_graph = global_block(input_graph)\n\n    model_inputs = []\n    if use_edges:\n      model_inputs.append(\n          blocks.EdgesToGlobalsAggregator(edges_reducer)(input_graph))\n    if use_nodes:\n      model_inputs.append(\n          blocks.NodesToGlobalsAggregator(nodes_reducer)(input_graph))\n    if use_globals:\n      model_inputs.append(input_graph.globals)\n\n    model_inputs = tf.concat(model_inputs, axis=-1)\n    self.assertIs(input_graph.edges, output_graph.edges)\n    self.assertIs(input_graph.nodes, output_graph.nodes)\n\n    output_graph_out = utils_tf.nest_to_numpy(output_graph)\n    model_inputs_out = model_inputs\n\n    expected_output_globals = model_inputs_out * self._scale\n    self.assertNDArrayNear(\n        expected_output_globals, output_graph_out.globals, err=1e-4)\n\n  @parameterized.named_parameters(\n      (""default"", True, True, True, 10),\n      (""use edges only"", True, False, False, 4),\n      (""use nodes only"", False, True, False, 2),\n      (""use globals only"", False, False, True, 4),\n  )\n  def test_created_variables(self, use_edges, use_nodes,\n                             use_globals, expected_first_dim_w):\n    """"""Verifies the variable names and shapes created by a GlobalBlock.""""""\n    output_size = 10\n    expected_var_shapes_dict = {\n        ""global_block/mlp/linear_0/b:0"": [output_size],\n        ""global_block/mlp/linear_0/w:0"": [expected_first_dim_w, output_size]}\n\n    input_graph = self._get_input_graph()\n\n    global_block = blocks.GlobalBlock(\n        global_model_fn=functools.partial(snt.nets.MLP,\n                                          output_sizes=[output_size]),\n        use_edges=use_edges,\n        use_nodes=use_nodes,\n        use_globals=use_globals)\n\n    global_block(input_graph)\n\n    variables = global_block.variables\n    var_shapes_dict = {var.name: var.get_shape().as_list() for var in variables}\n    self.assertDictEqual(expected_var_shapes_dict, var_shapes_dict)\n\n  @parameterized.named_parameters(\n      (""missing edges"", True, False, False, ""edges""),\n      (""missing nodes"", False, True, False, ""nodes""),\n      (""missing globals"", False, False, True, ""globals""),\n  )\n  def test_missing_field_raises_exception(\n      self, use_edges, use_nodes, use_globals, none_field):\n    """"""Checks that missing a required field raises an exception.""""""\n    input_graph = self._get_input_graph([none_field])\n    global_block = blocks.GlobalBlock(\n        global_model_fn=self._global_model_fn,\n        use_edges=use_edges,\n        use_nodes=use_nodes,\n        use_globals=use_globals)\n    with self.assertRaisesRegexp(ValueError, ""field cannot be None""):\n      global_block(input_graph)\n\n  @parameterized.named_parameters(\n      (""no edges"", False, True, True, ""edges""),\n      (""no nodes"", True, False, True, ""nodes""),\n      (""no globals"", True, True, False, ""globals""),\n  )\n  def test_unused_field_can_be_none(\n      self, use_edges, use_nodes, use_globals, none_field):\n    """"""Checks that computation can handle non-necessary fields left None.""""""\n    input_graph = self._get_input_graph([none_field])\n    global_block = blocks.GlobalBlock(\n        global_model_fn=self._global_model_fn,\n        use_edges=use_edges,\n        use_nodes=use_nodes,\n        use_globals=use_globals)\n    output_graph = global_block(input_graph)\n\n    model_inputs = []\n    if use_edges:\n      model_inputs.append(\n          blocks.EdgesToGlobalsAggregator(\n              tf.math.unsorted_segment_sum)(input_graph))\n    if use_nodes:\n      model_inputs.append(\n          blocks.NodesToGlobalsAggregator(\n              tf.math.unsorted_segment_sum)(input_graph))\n    if use_globals:\n      model_inputs.append(input_graph.globals)\n\n    model_inputs = tf.concat(model_inputs, axis=-1)\n    self.assertIs(input_graph.edges, output_graph.edges)\n    self.assertIs(input_graph.nodes, output_graph.nodes)\n\n    actual_globals = output_graph.globals.numpy()\n    model_inputs_out = model_inputs\n\n    expected_output_globals = model_inputs_out * self._scale\n    self.assertNDArrayNear(expected_output_globals, actual_globals, err=1e-4)\n\n  def test_compatible_higher_rank_no_raise(self):\n    """"""No exception should occur with higher ranks tensors.""""""\n    input_graph = self._get_shaped_input_graph()\n    input_graph = input_graph.map(lambda v: tf.transpose(v, [0, 2, 1, 3]))\n    network = blocks.GlobalBlock(\n        functools.partial(snt.Conv2D, output_channels=10, kernel_shape=[3, 3]))\n    self._assert_build_and_run(network, input_graph)\n\n  @parameterized.named_parameters(\n      (""mismatched nodes and edges"", True, True, False, ""edges""),\n      (""mismatched edges and globals"", True, False, True, ""globals""),\n      (""mismatched nodes and globals"", False, True, True, ""globals""),\n  )\n  def test_incompatible_higher_rank_inputs_raises(self,\n                                                  use_edges,\n                                                  use_nodes,\n                                                  use_globals,\n                                                  field):\n    """"""A exception should be raised if the inputs have incompatible shapes.""""""\n    input_graph = self._get_shaped_input_graph()\n    input_graph = input_graph.replace(\n        **{field: tf.transpose(getattr(input_graph, field), [0, 2, 1, 3])})\n    network = blocks.GlobalBlock(\n        functools.partial(snt.Conv2D, output_channels=10, kernel_shape=[3, 3]),\n        use_edges=use_edges,\n        use_nodes=use_nodes,\n        use_globals=use_globals\n    )\n    with self.assertRaisesRegexp(\n        tf.errors.InvalidArgumentError,\n        ""Dimensions of inputs should match""):\n      network(input_graph)\n\n  @parameterized.named_parameters(\n      (""mismatched nodes"", True, False, True, ""nodes""),\n      (""mismatched edges"", False, True, True, ""edges""),\n      (""mismatched globals"", True, True, False, ""globals""),\n  )\n  def test_incompatible_higher_rank_inputs_no_raise(self,\n                                                    use_edges,\n                                                    use_nodes,\n                                                    use_globals,\n                                                    field):\n    """"""No exception should occur if a differently shapped field is not used.""""""\n    input_graph = self._get_shaped_input_graph()\n    input_graph = input_graph.replace(\n        **{field: tf.transpose(getattr(input_graph, field), [0, 2, 1, 3])})\n    network = blocks.GlobalBlock(\n        functools.partial(snt.Conv2D, output_channels=10, kernel_shape=[3, 3]),\n        use_edges=use_edges,\n        use_nodes=use_nodes,\n        use_globals=use_globals\n    )\n    self._assert_build_and_run(network, input_graph)\n\n  def test_no_input_raises_exception(self):\n    """"""Checks that receiving no input raises an exception.""""""\n    with self.assertRaisesRegexp(ValueError, ""At least one of ""):\n      blocks.GlobalBlock(\n          global_model_fn=self._global_model_fn,\n          use_edges=False,\n          use_nodes=False,\n          use_globals=False)\n\n  @parameterized.named_parameters(\n      (""missing edges reducer"", True, False, None,\n       tf.math.unsorted_segment_sum),\n      (""missing nodes reducer"", False, True, tf.math.unsorted_segment_sum,\n       None),\n  )\n  def test_missing_aggregation_raises_exception(\n      self, use_edges, use_nodes, edges_reducer,\n      nodes_reducer):\n    """"""Checks that missing a required aggregation argument raises an error.""""""\n    with self.assertRaisesRegexp(ValueError, ""should not be None""):\n      blocks.GlobalBlock(\n          global_model_fn=self._global_model_fn,\n          use_edges=use_edges,\n          use_nodes=use_nodes,\n          use_globals=False,\n          edges_reducer=edges_reducer,\n          nodes_reducer=nodes_reducer)\n\n\nclass CommonBlockTests(GraphModuleTest):\n  """"""Tests that are common to the EdgeBlock, NodeBlock and GlobalBlock.""""""\n\n  @parameterized.named_parameters(\n      (""edge block"", blocks.EdgeBlock),\n      (""node block"", blocks.NodeBlock),\n      (""global block"", blocks.GlobalBlock),\n  )\n  def test_dynamic_batch_sizes(self, block_constructor):\n    """"""Checks that all batch sizes are as expected through a GraphNetwork.""""""\n    # Remove all placeholders from here, these are unnecessary in tf2.\n    input_graph = utils_np.data_dicts_to_graphs_tuple(\n        [SMALL_GRAPH_1, SMALL_GRAPH_2])\n    input_graph = input_graph.map(tf.constant, fields=graphs.ALL_FIELDS)\n    model = block_constructor(\n        functools.partial(snt.nets.MLP, output_sizes=[10]))\n    output = model(input_graph)\n    actual = utils_tf.nest_to_numpy(output)\n\n    for k, v in input_graph._asdict().items():\n      self.assertEqual(v.shape[0], getattr(actual, k).shape[0])\n\n  @parameterized.named_parameters(\n      (""float64 data, edge block"", tf.float64, tf.int32, blocks.EdgeBlock),\n      (""int64 indices, edge block"", tf.float32, tf.int64, blocks.EdgeBlock),\n      (""float64 data, node block"", tf.float64, tf.int32, blocks.NodeBlock),\n      (""int64 indices, node block"", tf.float32, tf.int64, blocks.NodeBlock),\n      (""float64 data, global block"", tf.float64, tf.int32, blocks.GlobalBlock),\n      (""int64 indices, global block"", tf.float32, tf.int64, blocks.GlobalBlock),\n  )\n  def test_dtypes(self, data_dtype, indices_dtype, block_constructor):\n    """"""Checks that all the output types are as expected for blocks.""""""\n    input_graph = self._get_input_graph()\n    input_graph = input_graph.map(lambda v: tf.cast(v, data_dtype),\n                                  [""nodes"", ""edges"", ""globals""])\n    input_graph = input_graph.map(lambda v: tf.cast(v, indices_dtype),\n                                  [""receivers"", ""senders""])\n    model = block_constructor(\n        functools.partial(snt.nets.MLP, output_sizes=[10]))\n    output = model(input_graph)\n    for field in [""nodes"", ""globals"", ""edges""]:\n      self.assertEqual(data_dtype, getattr(output, field).dtype)\n    for field in [""receivers"", ""senders""]:\n      self.assertEqual(indices_dtype, getattr(output, field).dtype)\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
graph_nets/tests_tf2/modules_test.py,86,"b'# Copyright 2018 The GraphNets Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for modules.py in Tensorflow 2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\n\nfrom absl.testing import parameterized\nfrom graph_nets import blocks\n\nfrom graph_nets import graphs\nfrom graph_nets import modules\nfrom graph_nets import utils_np\nfrom graph_nets import utils_tf\nimport numpy as np\nimport sonnet as snt\nimport tensorflow as tf\n\n\n\n\nSMALL_GRAPH_1 = {\n    ""globals"": [1.1, 1.2, 1.3],\n    ""nodes"": [[10.1, 10.2], [20.1, 20.2], [30.1, 30.2]],\n    ""edges"": [[101., 102., 103., 104.], [201., 202., 203., 204.]],\n    ""senders"": [0, 1],\n    ""receivers"": [1, 2],\n}\n\nSMALL_GRAPH_2 = {\n    ""globals"": [-1.1, -1.2, -1.3],\n    ""nodes"": [[-10.1, -10.2], [-20.1, -20.2], [-30.1, -30.2]],\n    ""edges"": [[-101., -102., -103., -104.]],\n    ""senders"": [1,],\n    ""receivers"": [2,],\n}\n\nSMALL_GRAPH_3 = {\n    ""globals"": [1.1, 1.2, 1.3],\n    ""nodes"": [[10.1, 10.2], [20.1, 20.2], [30.1, 30.2]],\n    ""edges"": [[101., 102., 103., 104.], [201., 202., 203., 204.]],\n    ""senders"": [1, 1],\n    ""receivers"": [0, 2],\n}\n\nSMALL_GRAPH_4 = {\n    ""globals"": [1.1, 1.2, 1.3],\n    ""nodes"": [[10.1, 10.2], [20.1, 20.2], [30.1, 30.2]],\n    ""edges"": [[101., 102., 103., 104.], [201., 202., 203., 204.]],\n    ""senders"": [0, 2],\n    ""receivers"": [1, 1],\n}\n\n\nclass GraphModuleTest(tf.test.TestCase, parameterized.TestCase):\n  """"""Base class for all the tests in this file.""""""\n\n  def setUp(self):\n    super(GraphModuleTest, self).setUp()\n    tf.random.set_seed(0)\n\n  def _assert_all_none_or_all_close(self, expected, actual, *args, **kwargs):\n    if expected is None:\n      return self.assertAllEqual(expected, actual)\n    return self.assertAllClose(expected, actual, *args, **kwargs)\n\n  def _get_input_graph(self, none_field=None):\n    input_graph = utils_tf.data_dicts_to_graphs_tuple(\n        [SMALL_GRAPH_1, SMALL_GRAPH_2, SMALL_GRAPH_3, SMALL_GRAPH_4])\n    if none_field:\n      input_graph = input_graph.replace(**{none_field: None})\n    return input_graph\n\n  def _get_shaped_input_graph(self):\n    return graphs.GraphsTuple(\n        nodes=tf.zeros([3, 4, 5, 11], dtype=tf.float32),\n        edges=tf.zeros([5, 4, 5, 12], dtype=tf.float32),\n        globals=tf.zeros([2, 4, 5, 13], dtype=tf.float32),\n        receivers=tf.range(5, dtype=tf.int32) // 3,\n        senders=tf.range(5, dtype=tf.int32) % 3,\n        n_node=tf.constant([2, 1], dtype=tf.int32),\n        n_edge=tf.constant([3, 2], dtype=tf.int32),\n    )\n\n  def _get_shaped_model_fns(self):\n    edge_model_fn = functools.partial(\n        snt.Conv2D, output_channels=10, kernel_shape=[3, 3])\n    node_model_fn = functools.partial(\n        snt.Conv2D, output_channels=8, kernel_shape=[3, 3])\n    global_model_fn = functools.partial(\n        snt.Conv2D, output_channels=7, kernel_shape=[3, 3])\n    return edge_model_fn, node_model_fn, global_model_fn\n\n  def _assert_build_and_run(self, network, input_graph):\n    # No error at construction time.\n    _ = network(input_graph)\n\n\nclass GraphIndependentTest(GraphModuleTest):\n\n  def _get_model(self, name=None):\n    kwargs = {\n        ""edge_model_fn"": functools.partial(snt.nets.MLP, output_sizes=[5]),\n        ""node_model_fn"": functools.partial(snt.nets.MLP, output_sizes=[10]),\n        ""global_model_fn"": functools.partial(snt.nets.MLP, output_sizes=[15]),\n    }\n    if name:\n      kwargs[""name""] = name\n    return modules.GraphIndependent(**kwargs)\n\n  def test_same_as_subblocks(self):\n    """"""Compares the output to explicit subblocks output.""""""\n    input_graph = self._get_input_graph()\n    model = self._get_model()\n    output_graph = utils_tf.nest_to_numpy(model(input_graph))\n\n    expected_output_edges = model._edge_model(input_graph.edges).numpy()\n    expected_output_nodes = model._node_model(input_graph.nodes).numpy()\n    expected_output_globals = model._global_model(input_graph.globals).numpy()\n\n    self._assert_all_none_or_all_close(expected_output_edges,\n                                       output_graph.edges)\n    self._assert_all_none_or_all_close(expected_output_nodes,\n                                       output_graph.nodes)\n    self._assert_all_none_or_all_close(expected_output_globals,\n                                       output_graph.globals)\n\n  @parameterized.named_parameters(\n      (""default name"", None), (""custom name"", ""custom_name""))\n  def test_created_variables(self, name=None):\n    """"""Verifies variable names and shapes created by a GraphIndependent.""""""\n    name = name if name is not None else ""graph_independent""\n    expected_var_shapes_dict = {\n        name + ""/edge_model/mlp/linear_0/b:0"": [5],\n        name + ""/edge_model/mlp/linear_0/w:0"": [4, 5],\n        name + ""/node_model/mlp/linear_0/b:0"": [10],\n        name + ""/node_model/mlp/linear_0/w:0"": [2, 10],\n        name + ""/global_model/mlp/linear_0/b:0"": [15],\n        name + ""/global_model/mlp/linear_0/w:0"": [3, 15],\n    }\n    input_graph = self._get_input_graph()\n    model = self._get_model(name=name)\n\n    model(input_graph)\n    variables = model.variables\n    var_shapes_dict = {var.name: var.get_shape().as_list() for var in variables}\n    self.assertDictEqual(expected_var_shapes_dict, var_shapes_dict)\n\n  @parameterized.named_parameters(\n      (""with tffunction"", True),\n      (""without tffunction"", False),)\n  def test_gradient_flow(self, use_autograd):\n    """"""Verifies that gradient flow is as expected.""""""\n    input_graph = self._get_input_graph()\n\n    for input_field in [""nodes"", ""edges"", ""globals""]:\n      input_tensor = getattr(input_graph, input_field)\n      for output_field in [""nodes"", ""edges"", ""globals""]:\n        model = self._get_model()\n        if use_autograd:\n          model.__call__ = tf.function(model.__call__)\n        with tf.GradientTape() as tape:\n          tape.watch(input_tensor)\n          output_graph = model(input_graph)\n        output_tensor = getattr(output_graph, output_field)\n        gradient = tape.gradient(output_tensor, input_tensor)\n        if input_field == output_field:\n          self.assertNotEqual(\n              None, gradient,\n              msg=""gradient should flow from {} to {}"".format(\n                  output_field, input_field))\n        else:\n          self.assertEqual(None, gradient,\n                           msg=""gradient should not flow from {} to {}"".format(\n                               output_field, input_field))\n\n  @parameterized.named_parameters(\n      (""differently shaped edges"", ""edges""),\n      (""differently shaped nodes"", ""nodes""),\n      (""differently shaped globals"", ""globals""),)\n  def test_incompatible_higher_rank_inputs_no_raise(self, field_to_reshape):\n    """"""A GraphIndependent does not make assumptions on its inputs shapes.""""""\n    input_graph = self._get_shaped_input_graph()\n    edge_model_fn, node_model_fn, global_model_fn = self._get_shaped_model_fns()\n    input_graph = input_graph.map(\n        lambda v: tf.transpose(v, [0, 2, 1, 3]), [field_to_reshape])\n    network = modules.GraphIndependent(\n        edge_model_fn, node_model_fn, global_model_fn)\n    self._assert_build_and_run(network, input_graph)\n\n\nclass GraphNetworkTest(GraphModuleTest):\n\n  def _get_model(self):\n    edge_model_fn = functools.partial(snt.Linear, output_size=5)\n    node_model_fn = functools.partial(snt.Linear, output_size=10)\n    global_model_fn = functools.partial(snt.Linear, output_size=15)\n    return modules.GraphNetwork(\n        edge_model_fn=edge_model_fn,\n        node_model_fn=node_model_fn,\n        global_model_fn=global_model_fn)\n\n  @parameterized.named_parameters(\n      (""default name"", None), (""custom name"", ""custom_name""))\n  def test_created_variables(self, name=None):\n    """"""Verifies variable names and shapes created by a GraphNetwork.""""""\n    name = name if name is not None else ""graph_network""\n    expected_var_shapes_dict = {\n        name + ""/edge_block/mlp/linear_0/b:0"": [5],\n        name + ""/edge_block/mlp/linear_0/w:0"": [4 + 4 + 3, 5],\n        name + ""/node_block/mlp/linear_0/b:0"": [10],\n        name + ""/node_block/mlp/linear_0/w:0"": [5 + 2 + 3, 10],\n        name + ""/global_block/mlp/linear_0/b:0"": [15],\n        name + ""/global_block/mlp/linear_0/w:0"": [10 + 5 + 3, 15],\n    }\n    input_graph = self._get_input_graph()\n    extra_kwargs = {""name"": name} if name else {}\n    model = modules.GraphNetwork(\n        edge_model_fn=functools.partial(snt.nets.MLP, output_sizes=[5]),\n        node_model_fn=functools.partial(snt.nets.MLP, output_sizes=[10]),\n        global_model_fn=functools.partial(snt.nets.MLP, output_sizes=[15]),\n        **extra_kwargs)\n\n    model(input_graph)\n    variables = model.variables\n    var_shapes_dict = {var.name: var.get_shape().as_list() for var in variables}\n    self.assertDictEqual(expected_var_shapes_dict, var_shapes_dict)\n\n  @parameterized.named_parameters(\n      (""reduce sum reduction"", tf.math.unsorted_segment_sum, False),\n      (""reduce sum reduction with tf.function"", tf.math.unsorted_segment_sum,\n       True),\n      (""reduce max or zero reduction"", blocks.unsorted_segment_max_or_zero,\n       True),)\n  def test_same_as_subblocks(self, reducer, use_tf_function):\n    """"""Compares the output to explicit subblocks output.\n\n    Args:\n      reducer: The reducer used in the `NodeBlock` and `GlobalBlock`.\n      use_tf_function: Whether to compile the model with `tf.function`.\n    """"""\n    input_graph = self._get_input_graph()\n\n    edge_model_fn = functools.partial(snt.Linear, output_size=5)\n    node_model_fn = functools.partial(snt.Linear, output_size=10)\n    global_model_fn = functools.partial(snt.Linear, output_size=15)\n\n    graph_network = modules.GraphNetwork(\n        edge_model_fn=edge_model_fn,\n        node_model_fn=node_model_fn,\n        global_model_fn=global_model_fn,\n        reducer=reducer)\n\n    if use_tf_function:\n      input_signature = [utils_tf.specs_from_graphs_tuple(input_graph)]\n      graph_network_fn = tf.function(graph_network, input_signature)\n    else:\n      graph_network_fn = graph_network\n\n    output_graph = graph_network_fn(input_graph)\n\n    edge_block = blocks.EdgeBlock(\n        edge_model_fn=lambda: graph_network._edge_block._edge_model,\n        use_sender_nodes=True,\n        use_edges=True,\n        use_receiver_nodes=True,\n        use_globals=True)\n    node_block = blocks.NodeBlock(\n        node_model_fn=lambda: graph_network._node_block._node_model,\n        use_nodes=True,\n        use_sent_edges=False,\n        use_received_edges=True,\n        use_globals=True,\n        received_edges_reducer=reducer)\n    global_block = blocks.GlobalBlock(\n        global_model_fn=lambda: graph_network._global_block._global_model,\n        use_nodes=True,\n        use_edges=True,\n        use_globals=True,\n        edges_reducer=reducer,\n        nodes_reducer=reducer)\n\n    expected_output_edge_block = edge_block(input_graph)\n    expected_output_node_block = node_block(expected_output_edge_block)\n    expected_output_global_block = global_block(expected_output_node_block)\n    expected_edges = expected_output_edge_block.edges.numpy()\n    expected_nodes = expected_output_node_block.nodes.numpy()\n    expected_globals = expected_output_global_block.globals.numpy()\n\n    self._assert_all_none_or_all_close(expected_edges,\n                                       output_graph.edges.numpy())\n    self._assert_all_none_or_all_close(expected_nodes,\n                                       output_graph.nodes.numpy())\n    self._assert_all_none_or_all_close(expected_globals,\n                                       output_graph.globals.numpy())\n\n  def test_dynamic_batch_sizes(self):\n    """"""Checks that all batch sizes are as expected through a GraphNetwork.""""""\n    input_graph = utils_np.data_dicts_to_graphs_tuple(\n        [SMALL_GRAPH_1, SMALL_GRAPH_2])\n    input_graph = input_graph.map(tf.constant, fields=graphs.ALL_FIELDS)\n    model = self._get_model()\n    output = model(input_graph)\n    for k, v in input_graph._asdict().items():\n      self.assertEqual(v.shape[0], getattr(output, k).shape[0])\n\n  @parameterized.named_parameters(\n      (""float64 data"", tf.float64, tf.int32),\n      (""int64 indices"", tf.float32, tf.int64),)\n  def test_dtypes(self, data_dtype, indices_dtype):\n    """"""Checks that all the output types are as expected in a GraphNetwork.""""""\n    input_graph = self._get_input_graph()\n    input_graph = input_graph.map(lambda v: tf.cast(v, data_dtype),\n                                  [""nodes"", ""globals"", ""edges""])\n    input_graph = input_graph.map(lambda v: tf.cast(v, indices_dtype),\n                                  [""senders"", ""receivers""])\n    model = self._get_model()\n    output = model(input_graph)\n    for field in [""nodes"", ""globals"", ""edges""]:\n      self.assertEqual(data_dtype, getattr(output, field).dtype)\n    for field in [""receivers"", ""senders""]:\n      self.assertEqual(indices_dtype, getattr(output, field).dtype)\n\n  @parameterized.named_parameters(\n      (""edges only"", True, False, False, False),\n      (""receivers only"", False, True, False, False),\n      (""senders only"", False, False, True, False),\n      (""globals only"", False, False, False, True),)\n  def test_edge_block_options(self,\n                              use_edges,\n                              use_receiver_nodes,\n                              use_sender_nodes,\n                              use_globals):\n    """"""Test for configuring the EdgeBlock options.""""""\n    reducer = tf.math.unsorted_segment_sum\n    input_graph = self._get_input_graph()\n    edge_model_fn = functools.partial(snt.Linear, output_size=10)\n    edge_block_opt = {""use_edges"": use_edges,\n                      ""use_receiver_nodes"": use_receiver_nodes,\n                      ""use_sender_nodes"": use_sender_nodes,\n                      ""use_globals"": use_globals}\n    # Identity node model\n    node_model_fn = lambda: tf.identity\n    node_block_opt = {""use_received_edges"": False,\n                      ""use_sent_edges"": False,\n                      ""use_nodes"": True,\n                      ""use_globals"": False}\n    # Identity global model\n    global_model_fn = lambda: tf.identity\n    global_block_opt = {""use_globals"": True,\n                        ""use_nodes"": False,\n                        ""use_edges"": False}\n\n    graph_network = modules.GraphNetwork(\n        edge_model_fn=edge_model_fn,\n        edge_block_opt=edge_block_opt,\n        node_model_fn=node_model_fn,\n        node_block_opt=node_block_opt,\n        global_model_fn=global_model_fn,\n        global_block_opt=global_block_opt,\n        reducer=reducer)\n\n    output_graph = utils_tf.nest_to_numpy(graph_network(input_graph))\n\n    edge_block = blocks.EdgeBlock(\n        edge_model_fn=lambda: graph_network._edge_block._edge_model,\n        use_edges=use_edges,\n        use_receiver_nodes=use_receiver_nodes,\n        use_sender_nodes=use_sender_nodes,\n        use_globals=use_globals)\n\n    expected_output_edge_block = edge_block(input_graph)\n    expected_output_node_block = expected_output_edge_block\n    expected_output_global_block = expected_output_node_block\n    expected_edges = expected_output_edge_block.edges.numpy()\n    expected_nodes = expected_output_node_block.nodes.numpy()\n    expected_globals = expected_output_global_block.globals.numpy()\n\n    self._assert_all_none_or_all_close(expected_edges,\n                                       output_graph.edges)\n    self._assert_all_none_or_all_close(expected_nodes,\n                                       output_graph.nodes)\n    self._assert_all_none_or_all_close(expected_globals,\n                                       output_graph.globals)\n\n  @parameterized.named_parameters(\n      (""received edges only"", True, False, False, False, None, None),\n      (""received edges, max reduction"",\n       True, False, False, False, tf.math.unsorted_segment_max, None),\n      (""sent edges only"", False, True, False, False, None, None),\n      (""sent edges, max reduction"",\n       False, True, False, False, None, tf.math.unsorted_segment_max),\n      (""nodes only"", False, False, True, False, None, None),\n      (""globals only"", False, False, False, True, None, None),\n  )\n  def test_node_block_options(self,\n                              use_received_edges,\n                              use_sent_edges,\n                              use_nodes,\n                              use_globals,\n                              received_edges_reducer,\n                              sent_edges_reducer):\n    """"""Test for configuring the NodeBlock options.""""""\n    input_graph = self._get_input_graph()\n\n    if use_received_edges:\n      received_edges_reducer = (\n          received_edges_reducer or tf.math.unsorted_segment_sum)\n    if use_sent_edges:\n      sent_edges_reducer = (\n          sent_edges_reducer or tf.math.unsorted_segment_sum)\n\n    # Identity edge model.\n    edge_model_fn = lambda: tf.identity\n    edge_block_opt = {""use_edges"": True,\n                      ""use_receiver_nodes"": False,\n                      ""use_sender_nodes"": False,\n                      ""use_globals"": False}\n    node_model_fn = functools.partial(snt.Linear, output_size=10)\n    node_block_opt = {""use_received_edges"": use_received_edges,\n                      ""use_sent_edges"": use_sent_edges,\n                      ""use_nodes"": use_nodes,\n                      ""use_globals"": use_globals,\n                      ""received_edges_reducer"": received_edges_reducer,\n                      ""sent_edges_reducer"": sent_edges_reducer}\n    # Identity global model\n    global_model_fn = lambda: tf.identity\n    global_block_opt = {""use_globals"": True,\n                        ""use_nodes"": False,\n                        ""use_edges"": False}\n\n    graph_network = modules.GraphNetwork(\n        edge_model_fn=edge_model_fn,\n        edge_block_opt=edge_block_opt,\n        node_model_fn=node_model_fn,\n        node_block_opt=node_block_opt,\n        global_model_fn=global_model_fn,\n        global_block_opt=global_block_opt)\n\n    output_graph = utils_tf.nest_to_numpy(graph_network(input_graph))\n\n    node_block = blocks.NodeBlock(\n        node_model_fn=lambda: graph_network._node_block._node_model,\n        use_nodes=use_nodes,\n        use_sent_edges=use_sent_edges,\n        use_received_edges=use_received_edges,\n        use_globals=use_globals,\n        received_edges_reducer=received_edges_reducer,\n        sent_edges_reducer=sent_edges_reducer)\n\n    expected_output_edge_block = input_graph\n    expected_output_node_block = node_block(input_graph)\n    expected_output_global_block = expected_output_node_block\n    expected_edges = expected_output_edge_block.edges.numpy()\n    expected_nodes = expected_output_node_block.nodes.numpy()\n    expected_globals = expected_output_global_block.globals.numpy()\n\n    self._assert_all_none_or_all_close(expected_edges,\n                                       output_graph.edges)\n    self._assert_all_none_or_all_close(expected_nodes,\n                                       output_graph.nodes)\n    self._assert_all_none_or_all_close(expected_globals,\n                                       output_graph.globals)\n\n  @parameterized.named_parameters(\n      (""edges only"", True, False, False, None, None),\n      (""edges only, max"", True, False, False, tf.math.unsorted_segment_max,\n       None),\n      (""nodes only"", False, True, False, None, None),\n      (""nodes only, max"", False, True, False, None,\n       tf.math.unsorted_segment_max),\n      (""globals only"", False, False, True, None, None),\n  )\n  def test_global_block_options(self,\n                                use_edges,\n                                use_nodes,\n                                use_globals,\n                                edges_reducer,\n                                nodes_reducer):\n    """"""Test for configuring the NodeBlock options.""""""\n    input_graph = self._get_input_graph()\n\n    if use_edges:\n      edges_reducer = edges_reducer or tf.math.unsorted_segment_sum\n    if use_nodes:\n      nodes_reducer = nodes_reducer or tf.math.unsorted_segment_sum\n\n    # Identity edge model.\n    edge_model_fn = lambda: tf.identity\n    edge_block_opt = {""use_edges"": True,\n                      ""use_receiver_nodes"": False,\n                      ""use_sender_nodes"": False,\n                      ""use_globals"": False}\n    # Identity node model\n    node_model_fn = lambda: tf.identity\n    node_block_opt = {""use_received_edges"": False,\n                      ""use_sent_edges"": False,\n                      ""use_nodes"": True,\n                      ""use_globals"": False}\n    global_model_fn = functools.partial(snt.Linear, output_size=10)\n    global_block_opt = {""use_globals"": use_globals,\n                        ""use_nodes"": use_nodes,\n                        ""use_edges"": use_edges,\n                        ""edges_reducer"": edges_reducer,\n                        ""nodes_reducer"": nodes_reducer}\n\n    graph_network = modules.GraphNetwork(\n        edge_model_fn=edge_model_fn,\n        edge_block_opt=edge_block_opt,\n        node_model_fn=node_model_fn,\n        node_block_opt=node_block_opt,\n        global_model_fn=global_model_fn,\n        global_block_opt=global_block_opt)\n\n    output_graph = utils_tf.nest_to_numpy(graph_network(input_graph))\n\n    global_block = blocks.GlobalBlock(\n        global_model_fn=lambda: graph_network._global_block._global_model,\n        use_edges=use_edges,\n        use_nodes=use_nodes,\n        use_globals=use_globals,\n        edges_reducer=edges_reducer,\n        nodes_reducer=nodes_reducer)\n\n    expected_output_edge_block = input_graph\n    expected_output_node_block = expected_output_edge_block\n    expected_output_global_block = global_block(expected_output_node_block)\n    expected_edges = expected_output_edge_block.edges.numpy()\n    expected_nodes = expected_output_node_block.nodes.numpy()\n    expected_globals = expected_output_global_block.globals.numpy()\n\n    self._assert_all_none_or_all_close(expected_edges,\n                                       output_graph.edges)\n    self._assert_all_none_or_all_close(expected_nodes,\n                                       output_graph.nodes)\n    self._assert_all_none_or_all_close(expected_globals,\n                                       output_graph.globals)\n\n  def test_higher_rank_outputs(self):\n    """"""Tests that a graph net can be build with higher rank inputs/outputs.""""""\n    input_graph = self._get_shaped_input_graph()\n    network = modules.GraphNetwork(*self._get_shaped_model_fns())\n    self._assert_build_and_run(network, input_graph)\n\n  @parameterized.named_parameters(\n      (""wrongly shaped edges"", ""edges""),\n      (""wrongly shaped nodes"", ""nodes""),\n      (""wrongly shaped globals"", ""globals""),)\n  def test_incompatible_higher_rank_inputs_raises(self, field_to_reshape):\n    """"""A exception should be raised if the inputs have incompatible shapes.""""""\n    input_graph = self._get_shaped_input_graph()\n    edge_model_fn, node_model_fn, global_model_fn = self._get_shaped_model_fns()\n    input_graph = input_graph.map(\n        lambda v: tf.transpose(v, [0, 2, 1, 3]), [field_to_reshape])\n    graph_network = modules.GraphNetwork(\n        edge_model_fn, node_model_fn, global_model_fn)\n    with self.assertRaisesRegexp(\n        tf.errors.InvalidArgumentError,\n        ""Dimensions of inputs should match""):\n      graph_network(input_graph)\n\n  def test_incompatible_higher_rank_partial_outputs_raises(self):\n    """"""A error should be raised if partial outputs have incompatible shapes.""""""\n    input_graph = self._get_shaped_input_graph()\n    edge_model_fn, node_model_fn, global_model_fn = self._get_shaped_model_fns()\n    edge_model_fn_2 = functools.partial(\n        snt.Conv2D, output_channels=10, kernel_shape=[3, 3], stride=[1, 2])\n    graph_network = modules.GraphNetwork(\n        edge_model_fn_2, node_model_fn, global_model_fn)\n    with self.assertRaisesRegexp(\n        tf.errors.InvalidArgumentError, ""Dimensions of inputs should match""):\n      graph_network(input_graph)\n    node_model_fn_2 = functools.partial(\n        snt.Conv2D, output_channels=10, kernel_shape=[3, 3], stride=[1, 2])\n    graph_network = modules.GraphNetwork(\n        edge_model_fn, node_model_fn_2, global_model_fn)\n    with self.assertRaisesRegexp(\n        tf.errors.InvalidArgumentError, ""Dimensions of inputs should match""):\n      graph_network(input_graph)\n\n\nclass InteractionNetworkTest(GraphModuleTest):\n\n  def _get_model(self, reducer=None, name=None):\n    kwargs = {\n        ""edge_model_fn"": functools.partial(snt.Linear, output_size=5),\n        ""node_model_fn"": functools.partial(snt.Linear, output_size=10)\n    }\n    if reducer:\n      kwargs[""reducer""] = reducer\n    if name:\n      kwargs[""name""] = name\n    return modules.InteractionNetwork(**kwargs)\n\n  @parameterized.named_parameters(\n      (""default name"", None), (""custom name"", ""custom_name""))\n  def test_created_variables(self, name=None):\n    """"""Verifies variable names and shapes created by an InteractionNetwork.""""""\n    name = name if name is not None else ""interaction_network""\n    expected_var_shapes_dict = {\n        name + ""/edge_block/linear/b:0"": [5],\n        name + ""/edge_block/linear/w:0"": [2 + 2 + 4, 5],\n        name + ""/node_block/linear/b:0"": [10],\n        name + ""/node_block/linear/w:0"": [5 + 2, 10],\n    }\n    input_graph = utils_tf.data_dicts_to_graphs_tuple([SMALL_GRAPH_1])\n    model = self._get_model(name=name)\n\n    model(input_graph)\n    variables = model.variables\n    var_shapes_dict = {var.name: var.get_shape().as_list() for var in variables}\n    self.assertDictEqual(expected_var_shapes_dict, var_shapes_dict)\n\n  @parameterized.named_parameters(\n      (""default"", tf.math.unsorted_segment_sum,),\n      (""max or zero reduction"", blocks.unsorted_segment_max_or_zero,),\n      (""no globals"", tf.math.unsorted_segment_sum, ""globals""),\n  )\n  def test_same_as_subblocks(self, reducer, none_field=None):\n    """"""Compares the output to explicit subblocks output.\n\n    Args:\n      reducer: The reducer used in the `NodeBlock`s.\n      none_field: (string, default=None) If not None, the corresponding field\n        is removed from the input graph.\n    """"""\n    input_graph = self._get_input_graph(none_field)\n\n    interaction_network = self._get_model(reducer)\n    output_graph = interaction_network(input_graph)\n    edges_out = output_graph.edges.numpy()\n    nodes_out = output_graph.nodes.numpy()\n    self.assertAllEqual(input_graph.globals, output_graph.globals)\n\n    edge_block = blocks.EdgeBlock(\n        edge_model_fn=lambda: interaction_network._edge_block._edge_model,\n        use_sender_nodes=True,\n        use_edges=True,\n        use_receiver_nodes=True,\n        use_globals=False)\n    node_block = blocks.NodeBlock(\n        node_model_fn=lambda: interaction_network._node_block._node_model,\n        use_nodes=True,\n        use_sent_edges=False,\n        use_received_edges=True,\n        use_globals=False,\n        received_edges_reducer=reducer)\n\n    expected_output_edge_block = edge_block(input_graph)\n    expected_output_node_block = node_block(expected_output_edge_block)\n    expected_edges = expected_output_edge_block.edges.numpy()\n    expected_nodes = expected_output_node_block.nodes.numpy()\n\n    self._assert_all_none_or_all_close(expected_edges, edges_out)\n    self._assert_all_none_or_all_close(expected_nodes, nodes_out)\n\n  @parameterized.named_parameters(\n      (""no nodes"", [""nodes""],),\n      (""no edge data"", [""edges""],),\n      (""no edges"", [""edges"", ""receivers"", ""senders""],),\n  )\n  def test_field_must_not_be_none(self, none_fields):\n    """"""Tests that the model cannot be built if required fields are missing.""""""\n    input_graph = utils_tf.data_dicts_to_graphs_tuple([SMALL_GRAPH_1])\n    input_graph = input_graph.map(lambda _: None, none_fields)\n    interaction_network = self._get_model()\n    with self.assertRaises(ValueError):\n      interaction_network(input_graph)\n\n  def test_higher_rank_outputs(self):\n    """"""Tests that an IN can be build with higher rank inputs/outputs.""""""\n    input_graph = self._get_shaped_input_graph()\n    edge_model_fn, node_model_fn, _ = self._get_shaped_model_fns()\n    graph_network = modules.InteractionNetwork(edge_model_fn, node_model_fn)\n    self._assert_build_and_run(graph_network, input_graph)\n\n  @parameterized.named_parameters(\n      (""wrongly shaped edges"", ""edges""),\n      (""wrongly shaped nodes"", ""nodes""),)\n  def test_incompatible_higher_rank_inputs_raises(self, field_to_reshape):\n    """"""Am exception should be raised if the inputs have incompatible shapes.""""""\n    input_graph = self._get_shaped_input_graph()\n    edge_model_fn, node_model_fn, _ = self._get_shaped_model_fns()\n    input_graph = input_graph.map(\n        lambda v: tf.transpose(v, [0, 2, 1, 3]), [field_to_reshape])\n    graph_network = modules.InteractionNetwork(edge_model_fn, node_model_fn)\n    with self.assertRaisesRegexp(\n        tf.errors.InvalidArgumentError, ""Dimensions of inputs should match""):\n      graph_network(input_graph)\n\n  def test_incompatible_higher_rank_inputs_no_raise(self):\n    """"""The globals can have an arbitrary shape in the input.""""""\n    input_graph = self._get_shaped_input_graph()\n    edge_model_fn, node_model_fn, _ = self._get_shaped_model_fns()\n    input_graph = input_graph.replace(\n        globals=tf.transpose(input_graph.globals, [0, 2, 1, 3]))\n    graph_network = modules.InteractionNetwork(edge_model_fn, node_model_fn)\n    self._assert_build_and_run(graph_network, input_graph)\n\n\nclass RelationNetworkTest(GraphModuleTest):\n\n  def _get_model(self, reducer=tf.math.unsorted_segment_sum, name=None):\n    kwargs = {\n        ""edge_model_fn"": functools.partial(snt.Linear, output_size=5),\n        ""global_model_fn"": functools.partial(snt.Linear, output_size=15)\n    }\n    if reducer:\n      kwargs[""reducer""] = reducer\n    if name:\n      kwargs[""name""] = name\n    return modules.RelationNetwork(**kwargs)\n\n  @parameterized.named_parameters(\n      (""default name"", None), (""custom name"", ""custom_name""))\n  def test_created_variables(self, name=None):\n    """"""Verifies variable names and shapes created by a RelationNetwork.""""""\n    name = name if name is not None else ""relation_network""\n    expected_var_shapes_dict = {\n        name + ""/edge_block/linear/b:0"": [5],\n        name + ""/edge_block/linear/w:0"": [2 + 2, 5],\n        name + ""/global_block/linear/b:0"": [15],\n        name + ""/global_block/linear/w:0"": [5, 15],\n    }\n    input_graph = utils_tf.data_dicts_to_graphs_tuple([SMALL_GRAPH_1])\n    model = self._get_model(name=name)\n\n    model(input_graph)\n    variables = model.variables\n    var_shapes_dict = {var.name: var.get_shape().as_list() for var in variables}\n    self.assertDictEqual(expected_var_shapes_dict, var_shapes_dict)\n\n  @parameterized.named_parameters(\n      (""default"", tf.math.unsorted_segment_sum, None),\n      (""max or zero reduction"", blocks.unsorted_segment_max_or_zero, None),\n      (""no edges"", tf.math.unsorted_segment_sum, ""edges""),\n      (""no globals"", tf.math.unsorted_segment_sum, ""globals""),\n  )\n  def test_same_as_subblocks(self, reducer, none_field=None):\n    """"""Compares the output to explicit subblocks output.\n\n    Args:\n      reducer: The reducer used in the `GlobalBlock`.\n      none_field: (string, default=None) If not None, the corresponding field\n        is removed from the input graph.\n    """"""\n    input_graph = self._get_input_graph(none_field)\n    relation_network = self._get_model(reducer)\n    output_graph = relation_network(input_graph)\n\n    edge_block = blocks.EdgeBlock(\n        edge_model_fn=lambda: relation_network._edge_block._edge_model,\n        use_edges=False,\n        use_receiver_nodes=True,\n        use_sender_nodes=True,\n        use_globals=False)\n    global_block = blocks.GlobalBlock(\n        global_model_fn=lambda: relation_network._global_block._global_model,\n        use_edges=True,\n        use_nodes=False,\n        use_globals=False,\n        edges_reducer=reducer,\n        nodes_reducer=reducer)\n\n    expected_output_edge_block = edge_block(input_graph)\n    expected_output_global_block = global_block(expected_output_edge_block)\n\n    self.assertIs(input_graph.edges, output_graph.edges)\n    self.assertIs(input_graph.nodes, output_graph.nodes)\n\n    self._assert_all_none_or_all_close(\n        output_graph.globals.numpy(),\n        expected_output_global_block.globals.numpy())\n\n  @parameterized.named_parameters(\n      (""no nodes"", [""nodes""],), (""no edges"", [""edges"", ""receivers"", ""senders""],)\n  )\n  def test_field_must_not_be_none(self, none_fields):\n    """"""Tests that the model cannot be built if required fields are missing.""""""\n    input_graph = utils_tf.data_dicts_to_graphs_tuple([SMALL_GRAPH_1])\n    input_graph = input_graph.map(lambda _: None, none_fields)\n    relation_network = self._get_model()\n    with self.assertRaises(ValueError):\n      relation_network(input_graph)\n\n  @parameterized.named_parameters(\n      (""differently shaped edges"", ""edges""),\n      (""differently shaped nodes"", ""nodes""),\n      (""differently shaped globals"", ""globals""),)\n  def test_incompatible_higher_rank_inputs_no_raise(self, field_to_reshape):\n    """"""A RelationNetwork does not make assumptions on its inputs shapes.""""""\n    input_graph = self._get_shaped_input_graph()\n    edge_model_fn, _, global_model_fn = self._get_shaped_model_fns()\n    input_graph = input_graph.map(\n        lambda v: tf.transpose(v, [0, 2, 1, 3]), [field_to_reshape])\n    network = modules.RelationNetwork(edge_model_fn, global_model_fn)\n    self._assert_build_and_run(network, input_graph)\n\n\nclass DeepSetsTest(GraphModuleTest):\n\n  def _get_model(self, reducer=None, name=None):\n    kwargs = {\n        ""node_model_fn"": functools.partial(snt.Linear, output_size=5),\n        ""global_model_fn"": functools.partial(snt.Linear, output_size=15)\n    }\n    if reducer:\n      kwargs[""reducer""] = reducer\n    if name:\n      kwargs[""name""] = name\n    return modules.DeepSets(**kwargs)\n\n  @parameterized.named_parameters(\n      (""default name"", None), (""custom name"", ""custom_name""))\n  def test_created_variables(self, name=None):\n    """"""Verifies variable names and shapes created by a DeepSets network.""""""\n    name = name if name is not None else ""deep_sets""\n    expected_var_shapes_dict = {\n        name + ""/node_block/linear/b:0"": [5],\n        name + ""/node_block/linear/w:0"": [2 + 3, 5],\n        name + ""/global_block/linear/b:0"": [15],\n        name + ""/global_block/linear/w:0"": [5, 15],\n    }\n    input_graph = self._get_input_graph()\n    model = self._get_model(name=name)\n\n    model(input_graph)\n    variables = model.variables\n    var_shapes_dict = {var.name: var.get_shape().as_list() for var in variables}\n    self.assertDictEqual(expected_var_shapes_dict, var_shapes_dict)\n\n  @parameterized.named_parameters(\n      (""default"", tf.math.unsorted_segment_sum, []),\n      (""no edge data"", tf.math.unsorted_segment_sum, [""edges""]),\n      (""no edges"", tf.math.unsorted_segment_sum,\n       [""edges"", ""receivers"", ""senders""]),\n      (""max or zero reduction"", blocks.unsorted_segment_max_or_zero, []),\n  )\n  def test_same_as_subblocks(self, reducer, none_fields):\n    """"""Compares the output to explicit subblocks output.\n\n    Args:\n      reducer: The reducer used in the NodeBlock.\n      none_fields: (list of strings) The corresponding fields are removed from\n        the input graph.\n    """"""\n    input_graph = self._get_input_graph()\n    input_graph = input_graph.map(lambda _: None, none_fields)\n\n    deep_sets = self._get_model(reducer)\n\n    output_graph = deep_sets(input_graph)\n    output_nodes = output_graph.nodes.numpy()\n    output_globals = output_graph.globals.numpy()\n\n    node_block = blocks.NodeBlock(\n        node_model_fn=lambda: deep_sets._node_block._node_model,\n        use_received_edges=False,\n        use_sent_edges=False,\n        use_nodes=True,\n        use_globals=True)\n    global_block = blocks.GlobalBlock(\n        global_model_fn=lambda: deep_sets._global_block._global_model,\n        use_edges=False,\n        use_nodes=True,\n        use_globals=False,\n        nodes_reducer=reducer)\n\n    node_block_out = node_block(input_graph)\n    expected_nodes = node_block_out.nodes.numpy()\n    expected_globals = global_block(node_block_out).globals.numpy()\n\n    self.assertAllEqual(input_graph.edges, output_graph.edges)\n    self.assertAllEqual(input_graph.receivers, output_graph.receivers)\n    self.assertAllEqual(input_graph.senders, output_graph.senders)\n\n    self._assert_all_none_or_all_close(expected_nodes, output_nodes)\n    self._assert_all_none_or_all_close(expected_globals, output_globals)\n\n  @parameterized.parameters(\n      (""nodes"",), (""globals"",),\n  )\n  def test_field_must_not_be_none(self, none_field):\n    """"""Tests that the model cannot be built if required fields are missing.""""""\n    input_graph = utils_tf.data_dicts_to_graphs_tuple([SMALL_GRAPH_1])\n    input_graph = input_graph.replace(**{none_field: None})\n    deep_sets = self._get_model()\n    with self.assertRaises(ValueError):\n      deep_sets(input_graph)\n\n  def test_incompatible_higher_rank_inputs_raises(self):\n    """"""A exception should be raised if the inputs have incompatible shapes.""""""\n    input_graph = self._get_shaped_input_graph()\n    _, node_model_fn, global_model_fn = self._get_shaped_model_fns()\n    input_graph = input_graph.replace(\n        nodes=tf.transpose(input_graph.nodes, [0, 2, 1, 3]))\n    graph_network = modules.DeepSets(node_model_fn, global_model_fn)\n    with self.assertRaisesRegexp(\n        tf.errors.InvalidArgumentError,\n        ""Dimensions of inputs should match""):\n      graph_network(input_graph)\n\n  def test_incompatible_higher_rank_partial_outputs_no_raise(self):\n    """"""There is no constraint on the size of the partial outputs.""""""\n    input_graph = self._get_shaped_input_graph()\n    node_model_fn = functools.partial(\n        snt.Conv2D, output_channels=10, kernel_shape=[3, 3], stride=[1, 2])\n    global_model_fn = functools.partial(\n        snt.Conv2D, output_channels=10, kernel_shape=[3, 3])\n    network = modules.DeepSets(node_model_fn, global_model_fn)\n    self._assert_build_and_run(network, input_graph)\n\n  def test_incompatible_higher_rank_inputs_no_raise(self):\n    """"""A DeepSets does not make assumptions on the shape if its input edges.""""""\n    input_graph = self._get_shaped_input_graph()\n    _, node_model_fn, global_model_fn = self._get_shaped_model_fns()\n    input_graph = input_graph.replace(\n        edges=tf.transpose(input_graph.edges, [0, 2, 1, 3]))\n    network = modules.DeepSets(node_model_fn, global_model_fn)\n    self._assert_build_and_run(network, input_graph)\n\n\nclass CommNetTest(GraphModuleTest):\n\n  def _get_model(self, reducer=None, name=None):\n    kwargs = {\n        ""edge_model_fn"": functools.partial(snt.Linear, output_size=15),\n        ""node_encoder_model_fn"": functools.partial(snt.Linear, output_size=8),\n        ""node_model_fn"": functools.partial(snt.Linear, output_size=5),\n    }\n    if reducer is not None:\n      kwargs[""reducer""] = reducer\n    if name:\n      kwargs[""name""] = name\n    return modules.CommNet(**kwargs)\n\n  @parameterized.named_parameters(\n      (""default name"", None), (""custom name"", ""custom_name""))\n  def test_created_variables(self, name=None):\n    """"""Verifies variable names and shapes created by a DeepSets network.""""""\n    name = name if name is not None else ""comm_net""\n    expected_var_shapes_dict = {\n        name + ""/edge_block/linear/b:0"": [15],\n        name + ""/edge_block/linear/w:0"": [2, 15],\n        name + ""/node_encoder_block/linear/b:0"": [8],\n        name + ""/node_encoder_block/linear/w:0"": [2, 8],\n        name + ""/node_block/linear/b:0"": [5],\n        name + ""/node_block/linear/w:0"": [15 + 8, 5],\n    }\n    input_graph = self._get_input_graph()\n    model = self._get_model(name=name)\n\n    model(input_graph)\n    variables = model.variables\n    var_shapes_dict = {var.name: var.get_shape().as_list() for var in variables}\n    self.assertDictEqual(expected_var_shapes_dict, var_shapes_dict)\n\n  @parameterized.named_parameters(\n      (""default"", tf.math.unsorted_segment_sum,),\n      (""no edges"", tf.math.unsorted_segment_sum, ""edges""),\n      (""no globals"", tf.math.unsorted_segment_sum, ""globals""),\n      (""max or zero reduction"", blocks.unsorted_segment_max_or_zero,),\n  )\n  def test_same_as_subblocks(self, reducer, none_field=None):\n    """"""Compares the output to explicit subblocks output.\n\n    Args:\n      reducer: The reducer used in the `NodeBlock`s.\n      none_field: (string, default=None) If not None, the corresponding field\n        is removed from the input graph.\n    """"""\n    input_graph = self._get_input_graph(none_field)\n\n    comm_net = self._get_model(reducer)\n    output_graph = comm_net(input_graph)\n    output_nodes = output_graph.nodes\n\n    edge_subblock = blocks.EdgeBlock(\n        edge_model_fn=lambda: comm_net._edge_block._edge_model,\n        use_edges=False,\n        use_receiver_nodes=False,\n        use_sender_nodes=True,\n        use_globals=False)\n    node_encoder_subblock = blocks.NodeBlock(\n        node_model_fn=lambda: comm_net._node_encoder_block._node_model,\n        use_received_edges=False,\n        use_sent_edges=False,\n        use_nodes=True,\n        use_globals=False,\n        received_edges_reducer=reducer)\n    node_subblock = blocks.NodeBlock(\n        node_model_fn=lambda: comm_net._node_block._node_model,\n        use_received_edges=True,\n        use_sent_edges=False,\n        use_nodes=True,\n        use_globals=False,\n        received_edges_reducer=reducer)\n\n    edge_block_out = edge_subblock(input_graph)\n    encoded_nodes = node_encoder_subblock(input_graph).nodes\n    node_input_graph = input_graph.replace(\n        edges=edge_block_out.edges, nodes=encoded_nodes)\n    node_block_out = node_subblock(node_input_graph)\n    expected_nodes = node_block_out.nodes\n\n    self.assertAllEqual(input_graph.globals, output_graph.globals)\n    self.assertAllEqual(input_graph.edges, output_graph.edges)\n    self.assertAllEqual(input_graph.receivers, output_graph.receivers,)\n    self.assertAllEqual(input_graph.senders, output_graph.senders)\n\n    self._assert_all_none_or_all_close(\n        expected_nodes.numpy(), output_nodes.numpy())\n\n  @parameterized.named_parameters(\n      (""no nodes"", [""nodes""],), (""no edges"", [""edges"", ""receivers"", ""senders""],)\n  )\n  def test_field_must_not_be_none(self, none_fields):\n    """"""Tests that the model cannot be built if required fields are missing.""""""\n    input_graph = utils_tf.data_dicts_to_graphs_tuple([SMALL_GRAPH_1])\n    input_graph = input_graph.map(lambda _: None, none_fields)\n    comm_net = self._get_model()\n    with self.assertRaises(ValueError):\n      comm_net(input_graph)\n\n  def test_higher_rank_outputs(self):\n    """"""Tests that a CommNet can be build with higher rank inputs/outputs.""""""\n    input_graph = self._get_shaped_input_graph()\n    graph_network = modules.CommNet(*self._get_shaped_model_fns())\n    self._assert_build_and_run(graph_network, input_graph)\n\n\nclass SelfAttentionTest(GraphModuleTest):\n\n  def _get_model(self, reducer=None, name=None):\n    kwargs = {\n        ""edge_model_fn"": functools.partial(snt.Linear, output_size=15),\n        ""node_encoder_model_fn"": functools.partial(snt.Linear, output_size=8),\n        ""node_model_fn"": functools.partial(snt.Linear, output_size=5),\n    }\n    if reducer is not None:\n      kwargs[""reducer""] = reducer\n    if name:\n      kwargs[""name""] = name\n    return modules.CommNet(**kwargs)\n\n  LOGITS_1D = [np.log(2), np.log(2), np.log(2), 0., 0., 0.]\n  SOFTMAX_1D = [1., 2/3, 0.5, 0.25, 0.25, 1/3]\n  LOGITS_2D = [[np.log(2), 1.], [np.log(2), 1.], [np.log(2), 1.],\n               [0., 1.], [0., 1.], [0., 1.]]\n  SOFTMAX_2D = [[1., 1.], [2/3, 0.5], [1/2, 1/3],\n                [1/4, 1/3], [1/4, 1/3], [1/3, 0.5]]\n  SENDERS = [0, 2, 2, 3, 4, 3]\n  RECEIVERS = [1, 5, 6, 6, 6, 5]\n  N_NODE = [2, 5]\n  N_EDGE = [1, 5]\n\n  @parameterized.named_parameters(\n      (""one dimensional"", LOGITS_1D, SOFTMAX_1D),\n      (""two dimensional"", LOGITS_2D, SOFTMAX_2D),)\n  def test_unsorted_segment_softmax(self, data, expected_softmax):\n    """"""Verifies variable names and shapes created by a DeepSets network.""""""\n\n    data = tf.constant(data, dtype=tf.float32)\n    segment_ids = tf.constant(self.RECEIVERS, dtype=tf.int32)\n    num_segments = tf.constant(sum(self.N_NODE), dtype=tf.int32)\n\n    actual_softmax = modules._unsorted_segment_softmax(\n        data, segment_ids, num_segments)\n\n    self.assertAllClose(expected_softmax, actual_softmax.numpy())\n\n  @parameterized.named_parameters(\n      (""one dimensional"", LOGITS_1D, SOFTMAX_1D,\n       modules._unsorted_segment_softmax),\n      (""two dimensional"", LOGITS_2D, SOFTMAX_2D,\n       modules._unsorted_segment_softmax),)\n  def test_received_edges_normalizer(self, logits,\n                                     expected_normalized, normalizer):\n    graph = graphs.GraphsTuple(\n        nodes=None,\n        edges=logits,\n        globals=None,\n        receivers=tf.constant(self.RECEIVERS, dtype=tf.int32),\n        senders=tf.constant(self.SENDERS, dtype=tf.int32),\n        n_node=tf.constant(self.N_NODE, dtype=tf.int32),\n        n_edge=tf.constant(self.N_EDGE, dtype=tf.int32),\n    )\n    actual_normalized_edges = modules._received_edges_normalizer(\n        graph, normalizer)\n\n    self.assertAllClose(expected_normalized, actual_normalized_edges.numpy())\n\n  def test_self_attention(self):\n    # Just one feature per node.\n    values_np = np.arange(sum(self.N_NODE)) + 1.\n    # Multiple heads, one positive values, one negative values.\n    values_np = np.stack([values_np, values_np*-1.], axis=-1)\n    # Multiple features per node, per head, at different scales.\n    values_np = np.stack([values_np, values_np*0.1], axis=-1)\n    values = tf.constant(values_np, dtype=tf.float32)\n\n    keys_np = [\n        [[0.3, 0.4]]*2,  # Irrelevant (only sender to one node)\n        [[0.1, 0.5]]*2,  # Not used (is not a sender)\n        [[1, 0], [0, 1]],\n        [[0, 1], [1, 0]],\n        [[1, 1], [1, 1]],\n        [[0.4, 0.3]]*2,  # Not used (is not a sender)\n        [[0.3, 0.2]]*2]  # Not used (is not a sender)\n    keys = tf.constant(keys_np, dtype=tf.float32)\n\n    queries_np = [\n        [[0.2, 0.7]]*2,  # Not used (is not a receiver)\n        [[0.3, 0.2]]*2,  # Irrelevant (only receives from one node)\n        [[0.2, 0.8]]*2,  # Not used (is not a receiver)\n        [[0.2, 0.4]]*2,  # Not used (is not a receiver)\n        [[0.3, 0.9]]*2,  # Not used (is not a receiver)\n        [[0, np.log(2)], [np.log(3), 0]],\n        [[np.log(2), 0], [0, np.log(3)]]]\n    queries = tf.constant(queries_np, dtype=tf.float32)\n\n    attention_graph = graphs.GraphsTuple(\n        nodes=None,\n        edges=None,\n        globals=None,\n        receivers=tf.constant(self.RECEIVERS, dtype=tf.int32),\n        senders=tf.constant(self.SENDERS, dtype=tf.int32),\n        n_node=tf.constant(self.N_NODE, dtype=tf.int32),\n        n_edge=tf.constant(self.N_EDGE, dtype=tf.int32),)\n\n    self_attention = modules.SelfAttention()\n    output_graph = self_attention(values, keys, queries, attention_graph)\n\n    expected_mixed_nodes = [\n        [[0., 0.], [0., 0.]],  # Does not receive any edges\n        [[1., 0.1], [-1., -0.1]],  # Only receives from n0.\n        [[0., 0.], [0., 0.]],  # Does not receive any edges\n        [[0., 0.], [0., 0.]],  # Does not receive any edges\n        [[0., 0.], [0., 0.]],  # Does not receive any edges\n        [[11/3, 11/3*0.1],  # Head one, receives from n2(1/3) n3(2/3)\n         [-15/4, -15/4*0.1]],  # Head two, receives from n2(1/4) n3(3/4)\n        [[20/5, 20/5*0.1],   # Head one, receives from n2(2/5) n3(1/5) n4(2/5)\n         [-28/7, -28/7*0.1]],  # Head two, receives from n2(3/7) n3(1/7) n4(3/7)\n    ]\n\n    self.assertAllClose(expected_mixed_nodes, output_graph.nodes.numpy())\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
graph_nets/tests_tf2/test_utils.py,5,"b'# Copyright 2018 The GraphNets Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Utilities for `utils_tf_test` in Tensorflow 2.\n\nThis provides a base class for tests involving `graphs.GraphsTuple`\ncontaining either numpy or tensorflow data. This base class is populated with\ntest data and also provides a convenience method for asserting graph equality.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport contextlib\nimport itertools\n\nfrom graph_nets import graphs\nfrom graph_nets import utils_np\nimport numpy as np\nimport tensorflow as tf\n\n\n\n@contextlib.contextmanager\ndef assert_new_op_prefixes(test, expected_prefix, assert_some_new_ops=True):\n  """"""Asserts the namescope of tf ops created within the context manager.""""""\n  ops_before = [n.name for n in tf.get_default_graph().as_graph_def().node]\n  yield\n  ops_after = [n.name for n in tf.get_default_graph().as_graph_def().node]\n  new_ops = set(ops_after) - set(ops_before)\n  prefix_length = len(expected_prefix)\n  if assert_some_new_ops:\n    test.assertNotEqual(0, len(new_ops))\n  for op_name in new_ops:\n    test.assertEqual(expected_prefix, op_name[:prefix_length])\n\n\ndef mask_leading_dimension(tensor):\n  # Place holder masking does not work anymore. Just use the tensor as is.\n  return tf.constant(tensor, shape=tensor.get_shape().as_list())\n\n\nclass GraphsTest(tf.test.TestCase):\n  """"""A base class for tests that operate on GraphsNP or GraphsTF.""""""\n\n  def _populate_test_data(self, max_size):\n    """"""Populates the class fields with data used for the tests.\n\n    This creates a batch of graphs with number of nodes from 0 to `num`,\n    number of edges ranging from 1 to `num`, plus an empty graph with no nodes\n    and no edges (so that the total number of graphs is 1 + (num ** (num + 1)).\n\n    The nodes states, edges states and global states of the graphs are\n    created to have different types and shapes.\n\n    Those graphs are stored both as dictionaries (in `self.graphs_dicts_in`,\n    without `n_node` and `n_edge` information, and in `self.graphs_dicts_out`\n    with these two fields filled), and a corresponding numpy\n    `graphs.GraphsTuple` is stored in `self.reference_graph`.\n\n    Args:\n      max_size: The maximum number of nodes and edges (inclusive).\n    """"""\n    filt = lambda x: (x[0] > 0) or (x[1] == 0)\n    n_node, n_edge = zip(*list(\n        filter(filt, itertools.product(\n            range(max_size + 1), range(max_size + 1)))))\n\n    graphs_dicts = []\n    nodes = []\n    edges = []\n    receivers = []\n    senders = []\n    globals_ = []\n\n    def _make_default_state(shape, dtype):\n      return np.arange(np.prod(shape)).reshape(shape).astype(dtype)\n\n    for i, (n_node_, n_edge_) in enumerate(zip(n_node, n_edge)):\n      n = _make_default_state([n_node_, 7, 11], ""f4"") + i * 100.\n      e = _make_default_state([n_edge_, 13, 14], np.float64) + i * 100. + 1000.\n      r = _make_default_state([n_edge_], np.int32) % n_node[i]\n      s = (_make_default_state([n_edge_], np.int32) + 1) % n_node[i]\n      g = _make_default_state([5, 3], ""f4"") - i * 100. - 1000.\n\n      nodes.append(n)\n      edges.append(e)\n      receivers.append(r)\n      senders.append(s)\n      globals_.append(g)\n      graphs_dict = dict(nodes=n, edges=e, receivers=r, senders=s, globals=g)\n      graphs_dicts.append(graphs_dict)\n\n    # Graphs dicts without n_node / n_edge (to be used as inputs).\n    self.graphs_dicts_in = graphs_dicts\n    # Graphs dicts with n_node / n_node (to be checked against outputs).\n    self.graphs_dicts_out = []\n    for dict_ in self.graphs_dicts_in:\n      completed_dict = dict_.copy()\n      completed_dict[""n_node""] = completed_dict[""nodes""].shape[0]\n      completed_dict[""n_edge""] = completed_dict[""edges""].shape[0]\n      self.graphs_dicts_out.append(completed_dict)\n\n    # pylint: disable=protected-access\n    offset = utils_np._compute_stacked_offsets(n_node, n_edge)\n    # pylint: enable=protected-access\n    self.reference_graph = graphs.GraphsTuple(**dict(\n        nodes=np.concatenate(nodes, axis=0),\n        edges=np.concatenate(edges, axis=0),\n        receivers=np.concatenate(receivers, axis=0) + offset,\n        senders=np.concatenate(senders, axis=0) + offset,\n        globals=np.stack(globals_),\n        n_node=np.array(n_node),\n        n_edge=np.array(n_edge)))\n\n  def _assert_graph_equals_np(self, graph0, graph, force_edges_ordering=False):\n    """"""Asserts that all the graph fields of graph0 and graph match.""""""\n\n    def silent_convert_to_numpy(tensor):\n      if isinstance(tensor, tf.Tensor):\n        return tensor.numpy()\n      else:\n        return tensor\n\n    graph = graph.map(silent_convert_to_numpy, graphs.ALL_FIELDS)\n    graph0 = graph0.map(silent_convert_to_numpy, graphs.ALL_FIELDS)\n\n    if graph0.nodes is None:\n      self.assertEqual(None, graph.nodes)\n    else:\n      self.assertAllClose(graph0.nodes, graph.nodes)\n    if graph0.globals is None:\n      self.assertEqual(None, graph.globals)\n    else:\n      self.assertAllClose(graph0.globals, graph.globals)\n    self.assertAllClose(graph0.n_node, graph.n_node.tolist())\n    if graph0.receivers is None:\n      self.assertEqual(None, graph.receivers)\n      self.assertEqual(None, graph.senders)\n      self.assertEqual(None, graph.edges)\n      self.assertAllEqual(graph0.n_edge, graph.n_edge)\n      return\n    self.assertAllClose(graph0.n_edge, graph.n_edge.tolist())\n\n    if not force_edges_ordering:\n      self.assertAllClose(graph0.receivers, graph.receivers)\n      self.assertAllClose(graph0.senders, graph.senders)\n      if graph0.edges is not None:\n        self.assertAllClose(graph0.edges, graph.edges)\n      else:\n        self.assertEqual(None, graph.edges)\n      return\n    # To compare edges content, we need to make sure they appear in the same\n    # order\n    if graph0.edges is not None:\n      sorted_receivers0, sorted_senders0, sorted_content0 = zip(\n          *sorted(zip(graph0.receivers, graph0.senders, graph0.edges.tolist())))\n      sorted_receivers, sorted_senders, sorted_content = zip(\n          *sorted(zip(graph.receivers, graph.senders, graph.edges.tolist())))\n      self.assertAllClose(sorted_content0, sorted_content)\n    elif graph.receivers is not None:\n      sorted_receivers0, sorted_senders0 = zip(\n          *sorted(zip(graph0.receivers, graph0.senders)))\n      sorted_receivers, sorted_senders = zip(\n          *sorted(zip(graph.receivers, graph.senders)))\n    else:\n      return\n    self.assertAllClose(sorted_receivers0, sorted_receivers)\n    self.assertAllClose(sorted_senders0, sorted_senders)\n\n  def setUp(self):\n    self._populate_test_data(max_size=2)\n'"
graph_nets/tests_tf2/utils_tf_test.py,129,"b'# Copyright 2018 The GraphNets Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for utils_tf.py in Tensorflow 2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom graph_nets import graphs\nfrom graph_nets import utils_np\nfrom graph_nets import utils_tf\nfrom graph_nets.tests_tf2 import test_utils\nimport networkx as nx\nimport numpy as np\nfrom six.moves import range\nimport tensorflow as tf\nimport tree\n\n\n\n\nclass RepeatTest(tf.test.TestCase, parameterized.TestCase):\n  """"""Tests for `repeat`.""""""\n\n  @parameterized.named_parameters(\n      (""base"", (3,), [2, 3, 4], 0),\n      (""empty_group_first"", (3,), [0, 3, 4], 0),\n      (""empty_group_middle"", (3,), [2, 0, 4], 0),\n      (""double_empty_group_middle"", (4,), [2, 0, 0, 4], 0),\n      (""empty_group_last"", (3,), [2, 3, 0], 0),\n      (""just_one_group"", (1,), [2], 0),\n      (""zero_groups"", (0,), [], 0),\n      (""axis 0"", (2, 3, 4), [2, 3], 0),\n      (""axis 1"", (3, 2, 4), [2, 3], 1),\n      (""axis 2"", (4, 3, 2), [2, 3], 2),\n      (""zero_groups_with_shape"", (2, 0, 4), [], 1),\n      )\n  def test_repeat(self, shape, repeats, axis):\n    num_elements = np.prod(shape)\n    t = np.arange(num_elements).reshape(*shape)\n    expected = np.repeat(t, repeats, axis=axis)\n    tensor = tf.constant(t)\n    repeats = tf.constant(repeats, dtype=tf.int32)\n    actual = utils_tf.repeat(tensor, repeats, axis=axis)\n    self.assertAllEqual(expected, actual)\n\n  @parameterized.named_parameters((""default"", ""custom_name"", None),\n                                  (""custom"", None, ""repeat""))\n  def test_name_scope(self, name, expected_name):\n    self.skipTest(""Uses get_default_graph."")\n    kwargs = {""name"": name} if name else {}\n    expected_name = expected_name if expected_name else name\n\n    t = tf.zeros([3, 2, 4])\n    indices = tf.constant([2, 3])\n    with test_utils.assert_new_op_prefixes(self, expected_name + ""/""):\n      utils_tf.repeat(t, indices, axis=1, **kwargs)\n\n\ndef _generate_graph(batch_index, n_nodes=4, add_edges=True):\n  graph = nx.DiGraph()\n  for node in range(n_nodes):\n    node_data = {""features"": np.array([node, batch_index], dtype=np.float32)}\n    graph.add_node(node, **node_data)\n  if add_edges:\n    for edge, (receiver, sender) in enumerate(zip([0, 0, 1], [1, 2, 3])):\n      if sender < n_nodes and receiver < n_nodes:\n        edge_data = np.array([edge, edge + 1, batch_index], dtype=np.float64)\n        graph.add_edge(sender, receiver, features=edge_data, index=edge)\n  graph.graph[""features""] = np.array([batch_index], dtype=np.float32)\n  return graph\n\n\nclass ConcatTest(tf.test.TestCase, parameterized.TestCase):\n  """"""Tests for `concat`, along various axis.""""""\n\n  @parameterized.named_parameters(\n      (""no nones"", []), (""stateless graph"", [""nodes"", ""edges"", ""globals""]),\n      (""no edges"", [""edges"", ""receivers"", ""senders""]))\n  def test_concat_first_axis(self, none_fields):\n    graph_0 = utils_np.networkxs_to_graphs_tuple(\n        [_generate_graph(0, 3), _generate_graph(1, 2)])\n    graph_1 = utils_np.networkxs_to_graphs_tuple([_generate_graph(2, 2)])\n    graph_2 = utils_np.networkxs_to_graphs_tuple([_generate_graph(3, 3)])\n    graphs_ = [\n        gr.map(tf.convert_to_tensor, graphs.ALL_FIELDS)\n        for gr in [graph_0, graph_1, graph_2]\n    ]\n    graphs_ = [gr.map(lambda _: None, none_fields) for gr in graphs_]\n    concat_graph = utils_tf.concat(graphs_, axis=0)\n    for none_field in none_fields:\n      self.assertEqual(None, getattr(concat_graph, none_field))\n    concat_graph = concat_graph.map(tf.no_op, none_fields)\n    if ""nodes"" not in none_fields:\n      self.assertAllEqual(\n          np.array([0, 1, 2, 0, 1, 0, 1, 0, 1, 2]),\n          [x[0] for x in concat_graph.nodes])\n      self.assertAllEqual(\n          np.array([0, 0, 0, 1, 1, 2, 2, 3, 3, 3]),\n          [x[1] for x in concat_graph.nodes])\n    if ""edges"" not in none_fields:\n      self.assertAllEqual(\n          np.array([0, 1, 0, 0, 0, 1]), [x[0] for x in concat_graph.edges])\n      self.assertAllEqual(\n          np.array([0, 0, 1, 2, 3, 3]), [x[2] for x in concat_graph.edges])\n    self.assertAllEqual(np.array([3, 2, 2, 3]), concat_graph.n_node)\n    self.assertAllEqual(np.array([2, 1, 1, 2]), concat_graph.n_edge)\n    if ""senders"" not in none_fields:\n      # [1, 2], [1], [1], [1, 2] and 3, 2, 2, 3 nodes\n      # So we are summing [1, 2, 1, 1, 2] with [0, 0, 3, 5, 7, 7]\n      self.assertAllEqual(np.array([1, 2, 4, 6, 8, 9]), concat_graph.senders)\n    if ""receivers"" not in none_fields:\n      # [0, 0], [0], [0], [0, 0] and 3, 2, 2, 3 nodes\n      # So we are summing [0, 0, 0, 0, 0, 0] with [0, 0, 3, 5, 7, 7]\n      self.assertAllEqual(np.array([0, 0, 3, 5, 7, 7]), concat_graph.receivers)\n    if ""globals"" not in none_fields:\n      self.assertAllEqual(np.array([[0], [1], [2], [3]]), concat_graph.globals)\n\n  def test_concat_last_axis(self):\n    graph0 = utils_np.networkxs_to_graphs_tuple(\n        [_generate_graph(0, 3), _generate_graph(1, 2)])\n    graph1 = utils_np.networkxs_to_graphs_tuple(\n        [_generate_graph(2, 3), _generate_graph(3, 2)])\n    graph0 = graph0.map(tf.convert_to_tensor, graphs.ALL_FIELDS)\n    graph1 = graph1.map(tf.convert_to_tensor, graphs.ALL_FIELDS)\n    concat_graph = utils_tf.concat([graph0, graph1], axis=-1)\n    self.assertAllEqual(\n        np.array([[0, 0, 0, 2], [1, 0, 1, 2], [2, 0, 2, 2], [0, 1, 0, 3],\n                  [1, 1, 1, 3]]), concat_graph.nodes)\n    self.assertAllEqual(\n        np.array([[0, 1, 0, 0, 1, 2], [1, 2, 0, 1, 2, 2], [0, 1, 1, 0, 1, 3]]),\n        concat_graph.edges)\n    self.assertAllEqual(np.array([3, 2]), concat_graph.n_node)\n    self.assertAllEqual(np.array([2, 1]), concat_graph.n_edge)\n    self.assertAllEqual(np.array([1, 2, 4]), concat_graph.senders)\n    self.assertAllEqual(np.array([0, 0, 3]), concat_graph.receivers)\n    self.assertAllEqual(np.array([[0, 2], [1, 3]]), concat_graph.globals)\n\n\nclass StopGradientsGraphTest(tf.test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(StopGradientsGraphTest, self).setUp()\n    self._graph = utils_tf.data_dicts_to_graphs_tuple([{\n        ""senders"": tf.zeros([10], dtype=tf.int32),\n        ""receivers"": tf.zeros([10], dtype=tf.int32),\n        ""nodes"": tf.ones([5, 7]),\n        ""edges"": tf.zeros([10, 6]),\n        ""globals"": tf.zeros([1, 8])\n    }])\n\n  def _check_if_gradients_exist(self, stopped_gradients_graph):\n    gradients = []\n    for field in [""globals"", ""nodes"", ""edges""]:\n      with tf.GradientTape() as tape:\n        xs = getattr(self._graph, field)\n        ys = getattr(stopped_gradients_graph, field)\n      gradient = tape.gradient(ys, xs) if ys is not None else ys\n      gradients.append(gradient)\n    return [True if grad is not None else False for grad in gradients]\n\n  @parameterized.named_parameters(\n      (""stop_all_fields"", True, True, True),\n      (""stop_globals"", True, False, False), (""stop_nodes"", False, True, False),\n      (""stop_edges"", False, False, True), (""stop_none"", False, False, False))\n  def test_stop_gradients_outputs(self, stop_globals, stop_nodes, stop_edges):\n    stopped_gradients_graph = utils_tf.stop_gradient(\n        self._graph,\n        stop_globals=stop_globals,\n        stop_nodes=stop_nodes,\n        stop_edges=stop_edges)\n\n    gradients_exist = self._check_if_gradients_exist(stopped_gradients_graph)\n    expected_gradients_exist = [\n        not stop_globals, not stop_nodes, not stop_edges\n    ]\n    self.assertAllEqual(expected_gradients_exist, gradients_exist)\n\n  @parameterized.named_parameters((""no_nodes"", ""nodes""), (""no_edges"", ""edges""),\n                                  (""no_globals"", ""globals""))\n  def test_stop_gradients_with_missing_field_raises(self, none_field):\n    self._graph = self._graph.map(lambda _: None, [none_field])\n    with self.assertRaisesRegexp(ValueError, none_field):\n      utils_tf.stop_gradient(self._graph)\n\n  def test_stop_gradients_default_params(self):\n    """"""Tests for the default params of `utils_tf.stop_gradient`.""""""\n    stopped_gradients_graph = utils_tf.stop_gradient(self._graph)\n    gradients_exist = self._check_if_gradients_exist(stopped_gradients_graph)\n    expected_gradients_exist = [False, False, False]\n    self.assertAllEqual(expected_gradients_exist, gradients_exist)\n\n\nclass IdentityTest(tf.test.TestCase, parameterized.TestCase):\n  """"""Tests for the `identity` method.""""""\n\n  def setUp(self):\n    super(IdentityTest, self).setUp()\n    self._graph = utils_tf.data_dicts_to_graphs_tuple([{\n        ""senders"": tf.random.uniform([10], maxval=10, dtype=tf.int32),\n        ""receivers"": tf.random.uniform([10], maxval=10, dtype=tf.int32),\n        ""nodes"": tf.random.uniform([5, 7]),\n        ""edges"": tf.random.uniform([10, 6]),\n        ""globals"": tf.random.uniform([1, 8])\n    }])\n\n  def test_name_scope(self):\n    """"""Tests that the name scope are correctly pushed through this function.""""""\n    self.skipTest(""Tensor.name is meaningless when eager execution is enabled"")\n\n  @parameterized.named_parameters(\n      (""all fields defined"", []), (""no node features"", [""nodes""]),\n      (""no edge features"", [""edges""]), (""no global features"", [""globals""]),\n      (""no edges"", [""edges"", ""receivers"", ""senders""]))\n  def test_output(self, none_fields):\n    """"""Tests that this function produces the identity.""""""\n    graph = self._graph.map(lambda _: None, none_fields)\n    with tf.name_scope(""test""):\n      graph_id = utils_tf.identity(graph)\n    expected_out = utils_tf.nest_to_numpy(graph)\n    actual_out = utils_tf.nest_to_numpy(graph_id)\n    for field in [\n        ""nodes"", ""edges"", ""globals"", ""receivers"", ""senders"", ""n_node"", ""n_edge""\n    ]:\n      if field in none_fields:\n        self.assertEqual(None, getattr(actual_out, field))\n      else:\n        self.assertNDArrayNear(\n            getattr(expected_out, field), getattr(actual_out, field), err=1e-4)\n\n\nclass RunGraphWithNoneTest(tf.test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(RunGraphWithNoneTest, self).setUp()\n    self._graph = utils_tf.data_dicts_to_graphs_tuple([{\n        ""senders"": tf.random.uniform([10], maxval=10, dtype=tf.int32),\n        ""receivers"": tf.random.uniform([10], maxval=10, dtype=tf.int32),\n        ""nodes"": tf.random.uniform([5, 7]),\n        ""edges"": tf.random.uniform([10, 6]),\n        ""globals"": tf.random.uniform([1, 8])\n    }])\n\n  @parameterized.named_parameters(\n      (""all fields defined"", []), (""no node features"", [""nodes""]),\n      (""no edge features"", [""edges""]), (""no global features"", [""globals""]),\n      (""no edges"", [""edges"", ""receivers"", ""senders""]))\n  def test_output(self, none_fields):\n    """"""Tests that this function produces the identity.""""""\n    graph_id = self._graph.map(lambda _: None, none_fields)\n    graph = graph_id.map(tf.no_op, none_fields)\n    expected_out = graph\n    actual_out = graph_id\n    for field in [\n        ""nodes"", ""edges"", ""globals"", ""receivers"", ""senders"", ""n_node"", ""n_edge""\n    ]:\n      if field in none_fields:\n        self.assertEqual(None, getattr(actual_out, field))\n      else:\n        self.assertNDArrayNear(\n            getattr(expected_out, field), getattr(actual_out, field), err=1e-4)\n\n\nclass ComputeOffsetTest(tf.test.TestCase):\n  """"""Tests for the `compute_stacked_offsets` method.""""""\n\n  def setUp(self):\n    super(ComputeOffsetTest, self).setUp()\n    self.sizes = [5, 4, 3, 1, 2, 0, 3, 0, 4, 7]\n    self.repeats = [2, 2, 0, 2, 1, 3, 2, 0, 3, 2]\n    self.offset = [\n        0, 0, 5, 5, 12, 12, 13, 15, 15, 15, 15, 15, 18, 18, 18, 22, 22\n    ]\n\n  def test_compute_stacked_offsets(self):\n    offset0 = utils_tf._compute_stacked_offsets(\n        self.sizes, self.repeats)\n    offset1 = utils_tf._compute_stacked_offsets(\n        np.array(self.sizes), np.array(self.repeats))\n    offset2 = utils_tf._compute_stacked_offsets(\n        tf.constant(self.sizes, dtype=tf.int32),\n        tf.constant(self.repeats, dtype=tf.int32))\n\n    self.assertAllEqual(self.offset, offset0.numpy().tolist())\n    self.assertAllEqual(self.offset, offset1.numpy().tolist())\n    self.assertAllEqual(self.offset, offset2.numpy().tolist())\n\n\nclass DataDictsCompletionTests(test_utils.GraphsTest, parameterized.TestCase):\n  """"""Tests for the methods creating complete graphs from partial graphs.""""""\n\n  def _assert_indices_sizes(self, dict_, n_relation):\n    for key in [""receivers"", ""senders""]:\n      self.assertAllEqual((n_relation,), dict_[key].get_shape().as_list())\n\n  @parameterized.named_parameters(\n      (""static"", utils_tf._create_complete_edges_from_nodes_static),\n      (""dynamic"", utils_tf._create_complete_edges_from_nodes_dynamic),\n  )\n  def test_create_complete_edges_from_nodes_include_self_edges(self, method):\n    for graph_dict in self.graphs_dicts_in:\n      n_node = graph_dict[""nodes""].shape[0]\n      edges_dict = method(n_node, exclude_self_edges=False)\n      self._assert_indices_sizes(edges_dict, n_node**2)\n\n  @parameterized.named_parameters(\n      (""static"", utils_tf._create_complete_edges_from_nodes_static),\n      (""dynamic"", utils_tf._create_complete_edges_from_nodes_dynamic),\n  )\n  def test_create_complete_edges_from_nodes_exclude_self_edges(self, method):\n    for graph_dict in self.graphs_dicts_in:\n      n_node = graph_dict[""nodes""].shape[0]\n      edges_dict = method(n_node, exclude_self_edges=True)\n      self._assert_indices_sizes(edges_dict, n_node * (n_node - 1))\n\n  def test_create_complete_edges_from_nodes_dynamic_number_of_nodes(self):\n    for graph_dict in self.graphs_dicts_in:\n      n_node = tf.shape(tf.constant(graph_dict[""nodes""]))[0]\n      edges_dict = utils_tf._create_complete_edges_from_nodes_dynamic(\n          n_node, exclude_self_edges=False)\n      n_relation = n_node**2\n      receivers = edges_dict[""receivers""].numpy()\n      senders = edges_dict[""senders""].numpy()\n      n_edge = edges_dict[""n_edge""].numpy()\n      self.assertAllEqual((n_relation,), receivers.shape)\n      self.assertAllEqual((n_relation,), senders.shape)\n      self.assertEqual(n_relation, n_edge)\n\n\nclass GraphsCompletionTests(test_utils.GraphsTest, parameterized.TestCase):\n  """"""Tests for completing partial GraphsTuple.""""""\n\n  def _assert_indices_sizes(self, graph, n_relation):\n    for key in [""receivers"", ""senders""]:\n      self.assertAllEqual((n_relation,),\n                          getattr(graph, key).get_shape().as_list())\n\n  @parameterized.named_parameters((""edge size 0"", 0), (""edge size 1"", 1))\n  def test_fill_edge_state(self, edge_size):\n    """"""Tests for filling the edge state with a constant content.""""""\n    for g in self.graphs_dicts_in:\n      g.pop(""edges"")\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    n_edges = np.sum(self.reference_graph.n_edge)\n    graphs_tuple = utils_tf.set_zero_edge_features(graphs_tuple, edge_size)\n    self.assertAllEqual((n_edges, edge_size),\n                        graphs_tuple.edges.get_shape().as_list())\n\n  @parameterized.named_parameters((""edge size 0"", 0), (""edge size 1"", 1))\n  def test_fill_edge_state_dynamic(self, edge_size):\n    """"""Tests for filling the edge state with a constant content.""""""\n    for g in self.graphs_dicts_in:\n      g.pop(""edges"")\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    graphs_tuple = graphs_tuple._replace(\n        n_edge=tf.constant(\n            graphs_tuple.n_edge, shape=graphs_tuple.n_edge.get_shape()))\n    n_edges = np.sum(self.reference_graph.n_edge)\n    graphs_tuple = utils_tf.set_zero_edge_features(graphs_tuple, edge_size)\n    actual_edges = graphs_tuple.edges\n    self.assertNDArrayNear(\n        np.zeros((n_edges, edge_size)), actual_edges, err=1e-4)\n\n  @parameterized.named_parameters((""global size 0"", 0), (""global size 1"", 1))\n  def test_fill_global_state(self, global_size):\n    """"""Tests for filling the global state with a constant content.""""""\n    for g in self.graphs_dicts_in:\n      g.pop(""globals"")\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    n_graphs = self.reference_graph.n_edge.shape[0]\n    graphs_tuple = utils_tf.set_zero_global_features(graphs_tuple, global_size)\n    self.assertAllEqual((n_graphs, global_size),\n                        graphs_tuple.globals.get_shape().as_list())\n\n  @parameterized.named_parameters((""global size 0"", 0), (""global size 1"", 1))\n  def test_fill_global_state_dynamic(self, global_size):\n    """"""Tests for filling the global state with a constant content.""""""\n    for g in self.graphs_dicts_in:\n      g.pop(""globals"")\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    # Hide global shape information\n    graphs_tuple = graphs_tuple._replace(\n        n_node=tf.constant(\n            graphs_tuple.n_node, shape=graphs_tuple.n_edge.get_shape()))\n    n_graphs = self.reference_graph.n_edge.shape[0]\n    graphs_tuple = utils_tf.set_zero_global_features(graphs_tuple, global_size)\n    actual_globals = graphs_tuple.globals.numpy()\n    self.assertNDArrayNear(\n        np.zeros((n_graphs, global_size)), actual_globals, err=1e-4)\n\n  @parameterized.named_parameters((""node size 0"", 0), (""node size 1"", 1))\n  def test_fill_node_state(self, node_size):\n    """"""Tests for filling the node state with a constant content.""""""\n    for g in self.graphs_dicts_in:\n      g[""n_node""] = g[""nodes""].shape[0]\n      g.pop(""nodes"")\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    n_nodes = np.sum(self.reference_graph.n_node)\n    graphs_tuple = utils_tf.set_zero_node_features(graphs_tuple, node_size)\n    self.assertAllEqual((n_nodes, node_size),\n                        graphs_tuple.nodes.get_shape().as_list())\n\n  @parameterized.named_parameters((""node size 0"", 0), (""node size 1"", 1))\n  def test_fill_node_state_dynamic(self, node_size):\n    """"""Tests for filling the node state with a constant content.""""""\n    for g in self.graphs_dicts_in:\n      g[""n_node""] = g[""nodes""].shape[0]\n      g.pop(""nodes"")\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    graphs_tuple = graphs_tuple._replace(\n        n_node=tf.constant(\n            graphs_tuple.n_node, shape=graphs_tuple.n_node.get_shape()))\n    n_nodes = np.sum(self.reference_graph.n_node)\n    graphs_tuple = utils_tf.set_zero_node_features(graphs_tuple, node_size)\n    actual_nodes = graphs_tuple.nodes.numpy()\n    self.assertNDArrayNear(\n        np.zeros((n_nodes, node_size)), actual_nodes, err=1e-4)\n\n  def test_fill_edge_state_with_missing_fields_raises(self):\n    """"""Edge field cannot be filled if receivers or senders are missing.""""""\n    for g in self.graphs_dicts_in:\n      g.pop(""receivers"")\n      g.pop(""senders"")\n      g.pop(""edges"")\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    with self.assertRaisesRegexp(ValueError, ""receivers""):\n      graphs_tuple = utils_tf.set_zero_edge_features(graphs_tuple, edge_size=1)\n\n  def test_fill_state_default_types(self):\n    """"""Tests that the features are created with the correct default type.""""""\n    for g in self.graphs_dicts_in:\n      g.pop(""nodes"")\n      g.pop(""globals"")\n      g.pop(""edges"")\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    graphs_tuple = utils_tf.set_zero_edge_features(graphs_tuple, edge_size=1)\n    graphs_tuple = utils_tf.set_zero_node_features(graphs_tuple, node_size=1)\n    graphs_tuple = utils_tf.set_zero_global_features(\n        graphs_tuple, global_size=1)\n    self.assertEqual(tf.float32, graphs_tuple.edges.dtype)\n    self.assertEqual(tf.float32, graphs_tuple.nodes.dtype)\n    self.assertEqual(tf.float32, graphs_tuple.globals.dtype)\n\n  @parameterized.parameters(\n      (tf.float64,),\n      (tf.int32,),\n  )\n  def test_fill_state_user_specified_types(self, dtype):\n    """"""Tests that the features are created with the correct default type.""""""\n    for g in self.graphs_dicts_in:\n      g.pop(""nodes"")\n      g.pop(""globals"")\n      g.pop(""edges"")\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    graphs_tuple = utils_tf.set_zero_edge_features(graphs_tuple, 1, dtype)\n    graphs_tuple = utils_tf.set_zero_node_features(graphs_tuple, 1, dtype)\n    graphs_tuple = utils_tf.set_zero_global_features(graphs_tuple, 1, dtype)\n    self.assertEqual(dtype, graphs_tuple.edges.dtype)\n    self.assertEqual(dtype, graphs_tuple.nodes.dtype)\n    self.assertEqual(dtype, graphs_tuple.globals.dtype)\n\n  @parameterized.named_parameters(\n      (""no self edges"", False),\n      (""self edges"", True),\n  )\n  def test_fully_connect_graph_dynamic(self, exclude_self_edges):\n    for g in self.graphs_dicts_in:\n      g.pop(""edges"")\n      g.pop(""receivers"")\n      g.pop(""senders"")\n    n_relation = 0\n    for g in self.graphs_dicts_in:\n      n_node = g[""nodes""].shape[0]\n      if exclude_self_edges:\n        n_relation += n_node * (n_node - 1)\n      else:\n        n_relation += n_node * n_node\n\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    graphs_tuple = utils_tf.fully_connect_graph_dynamic(graphs_tuple,\n                                                        exclude_self_edges)\n    actual_receivers = graphs_tuple.receivers.numpy()\n    actual_senders = graphs_tuple.senders.numpy()\n\n    self.assertAllEqual((n_relation,), actual_receivers.shape)\n    self.assertAllEqual((n_relation,), actual_senders.shape)\n    self.assertAllEqual((len(self.graphs_dicts_in),),\n                        graphs_tuple.n_edge.get_shape().as_list())\n\n  @parameterized.named_parameters(\n      (""no self edges"", False),\n      (""self edges"", True),\n  )\n  def test_fully_connect_graph_dynamic_with_dynamic_sizes(\n      self, exclude_self_edges):\n    for g in self.graphs_dicts_in:\n      g.pop(""edges"")\n      g.pop(""receivers"")\n      g.pop(""senders"")\n    n_relation = 0\n    for g in self.graphs_dicts_in:\n      n_node = g[""nodes""].shape[0]\n      if exclude_self_edges:\n        n_relation += n_node * (n_node - 1)\n      else:\n        n_relation += n_node * n_node\n\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    graphs_tuple = graphs_tuple.map(test_utils.mask_leading_dimension,\n                                    [""nodes"", ""globals"", ""n_node"", ""n_edge""])\n    graphs_tuple = utils_tf.fully_connect_graph_dynamic(graphs_tuple,\n                                                        exclude_self_edges)\n\n    actual_receivers = graphs_tuple.receivers.numpy()\n    actual_senders = graphs_tuple.senders.numpy()\n    actual_n_edge = graphs_tuple.n_edge.numpy()\n    self.assertAllEqual((n_relation,), actual_receivers.shape)\n    self.assertAllEqual((n_relation,), actual_senders.shape)\n    self.assertAllEqual((len(self.graphs_dicts_in),), actual_n_edge.shape)\n    expected_edges = []\n    offset = 0\n    for graph in self.graphs_dicts_in:\n      n_node = graph[""nodes""].shape[0]\n      for e1 in range(n_node):\n        for e2 in range(n_node):\n          if not exclude_self_edges or e1 != e2:\n            expected_edges.append((e1 + offset, e2 + offset))\n      offset += n_node\n    actual_edges = zip(actual_receivers, actual_senders)\n    self.assertSetEqual(set(actual_edges), set(expected_edges))\n\n\nclass GraphsTupleConversionTests(test_utils.GraphsTest, parameterized.TestCase):\n  """"""Tests for the method converting between data dicts and GraphsTuple.""""""\n\n  @parameterized.named_parameters((""all fields defined"", []), (\n      ""no edge features"",\n      [""edges""],\n  ), (\n      ""no node features"",\n      [""nodes""],\n  ), (\n      ""no globals"",\n      [""globals""],\n  ), (\n      ""no edges"",\n      [""edges"", ""receivers"", ""senders""],\n  ))\n  def test_data_dicts_to_graphs_tuple(self, none_fields):\n    """"""Fields in `none_fields` will be cleared out.""""""\n    for field in none_fields:\n      for graph_dict in self.graphs_dicts_in:\n        if field in graph_dict:\n          if field == ""nodes"":\n            graph_dict[""n_node""] = graph_dict[""nodes""].shape[0]\n          graph_dict[field] = None\n        self.reference_graph = self.reference_graph._replace(**{field: None})\n      if field == ""senders"":\n        self.reference_graph = self.reference_graph._replace(\n            n_edge=np.zeros_like(self.reference_graph.n_edge))\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    for field in none_fields:\n      self.assertEqual(None, getattr(graphs_tuple, field))\n    graphs_tuple = graphs_tuple.map(tf.no_op, none_fields)\n    self._assert_graph_equals_np(self.reference_graph, graphs_tuple)\n\n  @parameterized.parameters((""receivers"",), (""senders"",))\n  def test_data_dicts_to_graphs_tuple_raises(self, none_field):\n    """"""Fields that cannot be missing.""""""\n    for graph_dict in self.graphs_dicts_in:\n      graph_dict[none_field] = None\n    with self.assertRaisesRegexp(ValueError, none_field):\n      utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n\n  def test_data_dicts_to_graphs_tuple_no_raise(self):\n    """"""Not having nodes is fine, if the number of nodes is provided.""""""\n    for graph_dict in self.graphs_dicts_in:\n      graph_dict[""n_node""] = graph_dict[""nodes""].shape[0]\n      graph_dict[""nodes""] = None\n    utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n\n  def test_data_dicts_to_graphs_tuple_cast_types(self):\n    """"""Index and number fields should be cast to tensors of the right type.""""""\n    for graph_dict in self.graphs_dicts_in:\n      graph_dict[""n_node""] = np.array(\n          graph_dict[""nodes""].shape[0], dtype=np.int64)\n      graph_dict[""receivers""] = graph_dict[""receivers""].astype(np.int16)\n      graph_dict[""senders""] = graph_dict[""senders""].astype(np.float64)\n      graph_dict[""nodes""] = graph_dict[""nodes""].astype(np.float64)\n      graph_dict[""edges""] = tf.constant(graph_dict[""edges""], dtype=tf.float64)\n    out = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    for key in [""n_node"", ""n_edge"", ""receivers"", ""senders""]:\n      self.assertEqual(tf.int32, getattr(out, key).dtype)\n      self.assertEqual(type(tf.int32), type(getattr(out, key).dtype))\n    for key in [""nodes"", ""edges""]:\n      self.assertEqual(type(tf.float64), type(getattr(out, key).dtype))\n      self.assertEqual(tf.float64, getattr(out, key).dtype)\n\n\nclass GraphsIndexingTests(test_utils.GraphsTest, parameterized.TestCase):\n  """"""Tests for the `get_graph` method.""""""\n\n  @parameterized.named_parameters((""int_index"", False),\n                                  (""tensor_index"", True))\n  def test_getitem_one(self, use_tensor_index):\n    index = 2\n    expected = self.graphs_dicts_out[index]\n\n    if use_tensor_index:\n      index = tf.constant(index)\n\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    graph = utils_tf.get_graph(graphs_tuple, index)\n\n    graph = utils_tf.nest_to_numpy(graph)\n    actual, = utils_np.graphs_tuple_to_data_dicts(graph)\n\n    for k, v in expected.items():\n      self.assertAllClose(v, actual[k])\n    self.assertEqual(expected[""nodes""].shape[0], actual[""n_node""])\n    self.assertEqual(expected[""edges""].shape[0], actual[""n_edge""])\n\n  @parameterized.named_parameters((""int_slice"", False),\n                                  (""tensor_slice"", True))\n  def test_getitem(self, use_tensor_slice):\n    index = slice(1, 3)\n    expected = self.graphs_dicts_out[index]\n\n    if use_tensor_slice:\n      index = slice(tf.constant(index.start), tf.constant(index.stop))\n\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    graphs2 = utils_tf.get_graph(graphs_tuple, index)\n\n    graphs2 = utils_tf.nest_to_numpy(graphs2)\n    actual = utils_np.graphs_tuple_to_data_dicts(graphs2)\n\n    for ex, ac in zip(expected, actual):\n      for k, v in ex.items():\n        self.assertAllClose(v, ac[k])\n      self.assertEqual(ex[""nodes""].shape[0], ac[""n_node""])\n      self.assertEqual(ex[""edges""].shape[0], ac[""n_edge""])\n\n  @parameterized.named_parameters(\n      (""index_bad_type"", 1.,\n       TypeError, ""Index must be a valid scalar integer"", False, False),\n      (""index_bad_shape"", [0, 1],\n       TypeError, ""Valid tensor indices must be scalars"", True, False),\n      (""index_bad_dtype"", 1.,\n       TypeError, ""Valid tensor indices must have types"", True, False),\n      (""slice_bad_type_stop"", 1.,\n       TypeError, ""Valid tensor indices must be integers"", False, True),\n      (""slice_bad_shape_stop"", [0, 1],\n       TypeError, ""Valid tensor indices must be scalars"", True, True),\n      (""slice_bad_dtype_stop"", 1.,\n       TypeError, ""Valid tensor indices must have types"", True, True),\n      (""slice_bad_type_start"", slice(0., 1),\n       TypeError, ""Valid tensor indices must be integers"", False, False),\n      (""slice_with_step"", slice(0, 1, 1),\n       ValueError, ""slices with step/stride are not supported"", False, False),\n  )\n  def test_raises(self, index, error_type, message, use_constant, use_slice):\n    if use_constant:\n      index = tf.constant(index)\n    if use_slice:\n      index = slice(index)\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    with self.assertRaisesRegexp(error_type, message):\n      utils_tf.get_graph(graphs_tuple, index)\n\n\nclass TestNumGraphs(test_utils.GraphsTest):\n  """"""Tests for the `get_num_graphs` function.""""""\n\n  def setUp(self):\n    super(TestNumGraphs, self).setUp()\n    graphs_tuple = utils_tf.data_dicts_to_graphs_tuple(self.graphs_dicts_in)\n    self.empty_graph = graphs_tuple.map(lambda _: None,\n                                        graphs.GRAPH_DATA_FIELDS)\n\n  def test_num_graphs(self):\n    graph = self.empty_graph.replace(n_node=tf.zeros([3], dtype=tf.int32))\n    self.assertEqual(3, utils_tf.get_num_graphs(graph))\n\n\nclass TestNestToNumpy(test_utils.GraphsTest):\n  """"""Test that graph with tf.Tensor fields get converted to numpy.""""""\n\n  def setUp(self):\n    super(TestNestToNumpy, self).setUp()\n    self._graph = utils_tf.data_dicts_to_graphs_tuple([{\n        ""senders"": tf.random.uniform([10], maxval=10, dtype=tf.int32),\n        ""receivers"": tf.random.uniform([10], maxval=10, dtype=tf.int32),\n        ""nodes"": tf.random.uniform([5, 7]),\n        ""edges"": tf.random.uniform([10, 6]),\n        ""globals"": tf.random.uniform([1, 8])\n    }])\n\n  def test_single_graph(self):\n    numpy_graph = utils_tf.nest_to_numpy(self._graph)\n    for field in graphs.ALL_FIELDS:\n      self.assertIsInstance(getattr(numpy_graph, field), np.ndarray)\n      self.assertNDArrayNear(\n          getattr(self._graph, field).numpy(),\n          getattr(numpy_graph, field), 1e-8)\n\n  def test_mixed_graph_conversion(self):\n    graph = self._graph.replace(nodes=None)\n    graph = graph.map(lambda x: x.numpy(), [""edges""])\n\n    converted_graph = utils_tf.nest_to_numpy(graph)\n    self.assertIsNone(converted_graph.nodes)\n    self.assertIsInstance(converted_graph.edges, np.ndarray)\n\n  def test_nested_structure(self):\n    regular_graph = self._graph\n    graph_with_nested_fields = regular_graph.map(\n        lambda x: {""a"": x, ""b"": tf.random.uniform([4, 6])})\n\n    nested_structure = [\n        None,\n        regular_graph,\n        (graph_with_nested_fields,),\n        tf.random.uniform([10, 6])]\n    nested_structure_numpy = utils_tf.nest_to_numpy(nested_structure)\n\n    tree.assert_same_structure(nested_structure, nested_structure_numpy)\n\n    for tensor_or_none, array_or_none in zip(\n        tree.flatten(nested_structure),\n        tree.flatten(nested_structure_numpy)):\n      if tensor_or_none is None:\n        self.assertIsNone(array_or_none)\n        continue\n\n      self.assertIsNotNone(array_or_none)\n      self.assertNDArrayNear(\n          tensor_or_none.numpy(),\n          array_or_none, 1e-8)\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
