file_path,api_count,code
setup.py,0,"b'""""""Setup for pip package.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom setuptools import find_namespace_packages\nfrom setuptools import setup\n\n\ndef _get_sonnet_version():\n  with open(\'sonnet/__init__.py\') as fp:\n    for line in fp:\n      if line.startswith(\'__version__\'):\n        g = {}\n        exec(line, g)  # pylint: disable=exec-used\n        return g[\'__version__\']\n    raise ValueError(\'`__version__` not defined in `sonnet/__init__.py`\')\n\n\ndef _parse_requirements(requirements_txt_path):\n  with open(requirements_txt_path) as fp:\n    return fp.read().splitlines()\n\n\n_VERSION = _get_sonnet_version()\n\nEXTRA_PACKAGES = {\n    \'tensorflow\': [\'tensorflow>=2\'],\n    \'tensorflow with gpu\': [\'tensorflow-gpu>=2\'],\n}\n\nsetup(\n    name=\'dm-sonnet\',\n    version=_VERSION,\n    url=\'https://github.com/deepmind/sonnet\',\n    license=\'Apache 2.0\',\n    author=\'DeepMind\',\n    description=(\n        \'Sonnet is a library for building neural networks in TensorFlow.\'),\n    long_description=open(\'README.md\').read(),\n    long_description_content_type=\'text/markdown\',\n    author_email=\'sonnet-dev-os@google.com\',\n    # Contained modules and scripts.\n    packages=find_namespace_packages(exclude=[\'*_test.py\']),\n    install_requires=_parse_requirements(\'requirements.txt\'),\n    extras_require=EXTRA_PACKAGES,\n    tests_require=_parse_requirements(\'requirements-test.txt\'),\n    requires_python=\'>=3.6\',\n    include_package_data=True,\n    zip_safe=False,\n    # PyPI package information.\n    classifiers=[\n        \'Development Status :: 3 - Alpha\',\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Education\',\n        \'Intended Audience :: Science/Research\',\n        \'License :: OSI Approved :: Apache Software License\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'Topic :: Scientific/Engineering :: Mathematics\',\n        \'Topic :: Software Development :: Libraries :: Python Modules\',\n        \'Topic :: Software Development :: Libraries\',\n    ],\n)\n'"
docs/conf.py,0,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Configuration file for the Sphinx documentation builder.""""""\n\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n\n# pylint: disable=g-bad-import-order\n# pylint: disable=g-import-not-at-top\nimport doctest\nimport inspect\nimport os\nimport sys\n\nsys.path.insert(0, os.path.abspath(\'../\'))\nsys.path.append(os.path.abspath(\'ext\'))\n\nimport sonnet as snt\nimport sphinxcontrib.katex as katex\n\n# -- Project information -----------------------------------------------------\n\nproject = \'Sonnet\'\ncopyright = \'2019, DeepMind\'  # pylint: disable=redefined-builtin\nauthor = \'Sonnet Contributors\'\n\n# -- General configuration ---------------------------------------------------\n\nmaster_doc = \'index\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'link_tf_api\',\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.autosummary\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.inheritance_diagram\',\n    \'sphinx.ext.linkcode\',\n    \'sphinx.ext.napoleon\',\n    \'sphinxcontrib.bibtex\',\n    \'sphinxcontrib.katex\',\n    \'sphinx_autodoc_typehints\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# -- Options for autodoc -----------------------------------------------------\n\nautodoc_default_options = {\n    \'member-order\': \'bysource\',\n    \'special-members\': True,\n    \'exclude-members\': \'__repr__, __str__, __weakref__\',\n}\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\nhtml_favicon = \'_static/favicon.ico\'\n\n# -- Options for doctest -----------------------------------------------------\n\ndoctest_test_doctest_blocks = \'true\'\ndoctest_global_setup = """"""\nimport tensorflow as tf\nimport sonnet as snt\n\n# `TpuReplicator` cannot be constructed without a TPU, however it has exactly\n# the same API as `Replicator` so we can run doctests using that instead.\nsnt.distribute.TpuReplicator = snt.distribute.Replicator\n""""""\ndoctest_default_flags = (\n    doctest.ELLIPSIS\n    | doctest.IGNORE_EXCEPTION_DETAIL\n    | doctest.DONT_ACCEPT_TRUE_FOR_1\n    | doctest.NORMALIZE_WHITESPACE)\n\n# -- Options for katex ------------------------------------------------------\n\n# See: https://sphinxcontrib-katex.readthedocs.io/en/0.4.1/macros.html\nlatex_macros = r""""""\n    \\def \\d              #1{\\operatorname{#1}}\n""""""\n\n# Translate LaTeX macros to KaTeX and add to options for HTML builder\nkatex_macros = katex.latex_defs_to_katex_macros(latex_macros)\nkatex_options = \'macros: {\' + katex_macros + \'}\'\n\n# Add LaTeX macros for LATEX builder\nlatex_elements = {\'preamble\': latex_macros}\n\n# -- Source code links -------------------------------------------------------\n\n\ndef linkcode_resolve(domain, info):\n  """"""Resolve a GitHub URL corresponding to Python object.""""""\n  if domain != \'py\':\n    return None\n\n  try:\n    mod = sys.modules[info[\'module\']]\n  except ImportError:\n    return None\n\n  obj = mod\n  try:\n    for attr in info[\'fullname\'].split(\'.\'):\n      obj = getattr(obj, attr)\n  except AttributeError:\n    return None\n  else:\n    obj = inspect.unwrap(obj)\n\n  try:\n    filename = inspect.getsourcefile(obj)\n  except TypeError:\n    return None\n\n  try:\n    source, lineno = inspect.getsourcelines(obj)\n  except OSError:\n    return None\n\n  # TODO(slebedev): support tags after we release an initial version.\n  return \'https://github.com/deepmind/sonnet/blob/v2/sonnet/%s#L%d#L%d\' % (\n      os.path.relpath(filename, start=os.path.dirname(\n          snt.__file__)), lineno, lineno + len(source) - 1)\n'"
examples/simple_mnist.py,23,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Trivial convnet learning MNIST.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nfrom absl import app\nimport sonnet as snt\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom typing import Dict, Text\n\n\ndef mnist(split: Text, batch_size: int) -> tf.data.Dataset:\n  """"""Returns a tf.data.Dataset with MNIST image/label pairs.""""""\n\n  def preprocess_dataset(images, labels):\n    # Mnist images are int8 [0, 255], we cast and rescale to float32 [-1, 1].\n    images = ((tf.cast(images, tf.float32) / 255.) - .5) * 2.\n    return images, labels\n\n  dataset = tfds.load(\n      name=""mnist"",\n      split=split,\n      shuffle_files=split == ""train"",\n      as_supervised=True)\n  dataset = dataset.map(preprocess_dataset)\n  dataset = dataset.shuffle(buffer_size=4 * batch_size)\n  dataset = dataset.batch(batch_size)\n  # Cache the result of the data pipeline to avoid recomputation. The pipeline\n  # is only ~100MB so this should not be a significant cost and will afford a\n  # decent speedup.\n  dataset = dataset.cache()\n  # Prefetching batches onto the GPU will help avoid us being too input bound.\n  # We allow tf.data to determine how much to prefetch since this will vary\n  # between GPUs.\n  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n  return dataset\n\n\ndef train_step(\n    model: snt.Module,\n    optimizer: snt.Optimizer,\n    images: tf.Tensor,\n    labels: tf.Tensor,\n) -> tf.Tensor:\n  """"""Runs a single training step of the model on the given input.""""""\n  with tf.GradientTape() as tape:\n    logits = model(images)\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        labels=labels, logits=logits)\n    loss = tf.reduce_mean(loss)\n  variables = model.trainable_variables\n  gradients = tape.gradient(loss, variables)\n  optimizer.apply(gradients, variables)\n  return loss\n\n\n@tf.function\ndef train_epoch(\n    model: snt.Module,\n    optimizer: snt.Optimizer,\n    dataset: tf.data.Dataset,\n) -> tf.Tensor:\n  loss = 0.\n  for images, labels in dataset:\n    loss = train_step(model, optimizer, images, labels)\n  return loss\n\n\n@tf.function\ndef test_accuracy(\n    model: snt.Module,\n    dataset: tf.data.Dataset,\n) -> Dict[Text, tf.Tensor]:\n  """"""Computes accuracy on the test set.""""""\n  correct, total = 0, 0\n  for images, labels in dataset:\n    preds = tf.argmax(model(images), axis=1)\n    correct += tf.math.count_nonzero(tf.equal(preds, labels), dtype=tf.int32)\n    total += tf.shape(labels)[0]\n  accuracy = (correct / tf.cast(total, tf.int32)) * 100.\n  return {""accuracy"": accuracy, ""incorrect"": total - correct}\n\n\ndef main(unused_argv):\n  del unused_argv\n\n  model = snt.Sequential([\n      snt.Conv2D(32, 3, 1),\n      tf.nn.relu,\n      snt.Conv2D(32, 3, 1),\n      tf.nn.relu,\n      snt.Flatten(),\n      snt.Linear(10),\n  ])\n\n  optimizer = snt.optimizers.SGD(0.1)\n\n  train_data = mnist(""train"", batch_size=128)\n  test_data = mnist(""test"", batch_size=1000)\n\n  for epoch in range(5):\n    train_loss = train_epoch(model, optimizer, train_data)\n    test_metrics = test_accuracy(model, test_data)\n    print(""[Epoch %d] train loss: %.05f, test acc: %.02f%% (%d wrong)"" %\n          (epoch, train_loss, test_metrics[""accuracy""],\n           test_metrics[""incorrect""]))\n\n\nif __name__ == ""__main__"":\n  app.run(main)\n'"
examples/simple_mnist_test.py,8,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.examples.simple_mnist.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sonnet as snt\nfrom examples import simple_mnist\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\n\nclass SimpleMnistTest(test_utils.TestCase):\n\n  def setUp(self):\n    self.ENTER_PRIMARY_DEVICE = False  # pylint: disable=invalid-name\n    super(SimpleMnistTest, self).setUp()\n\n  def test_train_epoch(self):\n    model = snt.Sequential([\n        snt.Flatten(),\n        snt.Linear(10),\n    ])\n\n    optimizer = snt.optimizers.SGD(0.1)\n\n    dataset = tf.data.Dataset.from_tensor_slices(\n        (tf.random.normal([2, 8, 8, 1]),\n         tf.ones([2], dtype=tf.int64))).batch(2).repeat(4)\n\n    for _ in range(3):\n      loss = simple_mnist.train_epoch(model, optimizer, dataset)\n    self.assertEqual(loss.shape, [])\n    self.assertEqual(loss.dtype, tf.float32)\n\n  def test_test_accuracy(self):\n    model = snt.Sequential([\n        snt.Flatten(),\n        snt.Linear(10),\n    ])\n    dataset = tf.data.Dataset.from_tensor_slices(\n        (tf.random.normal([2, 8, 8, 1]),\n         tf.ones([2], dtype=tf.int64))).batch(2).repeat(4)\n\n    outputs = simple_mnist.test_accuracy(model, dataset)\n    self.assertEqual(len(outputs), 2)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/__init__.py,0,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Sonnet built for TensorFlow 2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom sonnet import distribute\nfrom sonnet import initializers\nfrom sonnet import mixed_precision\nfrom sonnet import nets\nfrom sonnet import optimizers\nfrom sonnet import pad\nfrom sonnet import regularizers\nfrom sonnet.src.axis_norm import InstanceNorm\nfrom sonnet.src.axis_norm import LayerNorm\nfrom sonnet.src.base import allow_empty_variables\nfrom sonnet.src.base import Module\nfrom sonnet.src.base import no_name_scope\nfrom sonnet.src.base import Optimizer\nfrom sonnet.src.batch_apply import BatchApply\nfrom sonnet.src.batch_apply import merge_leading_dims\nfrom sonnet.src.batch_apply import split_leading_dim\nfrom sonnet.src.batch_norm import BaseBatchNorm\nfrom sonnet.src.batch_norm import BatchNorm\nfrom sonnet.src.bias import Bias\nfrom sonnet.src.build import build\nfrom sonnet.src.conv import Conv1D\nfrom sonnet.src.conv import Conv2D\nfrom sonnet.src.conv import Conv3D\nfrom sonnet.src.conv_transpose import Conv1DTranspose\nfrom sonnet.src.conv_transpose import Conv2DTranspose\nfrom sonnet.src.conv_transpose import Conv3DTranspose\nfrom sonnet.src.custom_getter import custom_variable_getter\nfrom sonnet.src.deferred import Deferred\nfrom sonnet.src.depthwise_conv import DepthwiseConv2D\nfrom sonnet.src.dropout import Dropout\nfrom sonnet.src.embed import Embed\nfrom sonnet.src.group_norm import GroupNorm\nfrom sonnet.src.leaky_clip_by_value import leaky_clip_by_value\nfrom sonnet.src.linear import Linear\nfrom sonnet.src.metrics import Mean\nfrom sonnet.src.metrics import Metric\nfrom sonnet.src.metrics import Sum\nfrom sonnet.src.moving_averages import ExponentialMovingAverage\nfrom sonnet.src.once import once\nfrom sonnet.src.recurrent import Conv1DLSTM\nfrom sonnet.src.recurrent import Conv2DLSTM\nfrom sonnet.src.recurrent import Conv3DLSTM\nfrom sonnet.src.recurrent import deep_rnn_with_residual_connections\nfrom sonnet.src.recurrent import deep_rnn_with_skip_connections\nfrom sonnet.src.recurrent import DeepRNN\nfrom sonnet.src.recurrent import dynamic_unroll\nfrom sonnet.src.recurrent import GRU\nfrom sonnet.src.recurrent import LSTM\nfrom sonnet.src.recurrent import lstm_with_recurrent_dropout\nfrom sonnet.src.recurrent import LSTMState\nfrom sonnet.src.recurrent import RNNCore\nfrom sonnet.src.recurrent import static_unroll\nfrom sonnet.src.recurrent import TrainableState\nfrom sonnet.src.recurrent import UnrolledLSTM\nfrom sonnet.src.recurrent import UnrolledRNN\nfrom sonnet.src.recurrent import VanillaRNN\nfrom sonnet.src.reshape import flatten\nfrom sonnet.src.reshape import Flatten\nfrom sonnet.src.reshape import reshape\nfrom sonnet.src.reshape import Reshape\nfrom sonnet.src.scale_gradient import scale_gradient\nfrom sonnet.src.sequential import Sequential\nfrom sonnet.src.utils import format_variables\nfrom sonnet.src.utils import log_variables\n\n__all__ = (\n    ""BaseBatchNorm"",\n    ""BatchApply"",\n    ""BatchNorm"",\n    ""Bias"",\n    ""Conv1D"",\n    ""Conv1DLSTM"",\n    ""Conv1DTranspose"",\n    ""Conv2D"",\n    ""Conv2DLSTM"",\n    ""Conv2DTranspose"",\n    ""Conv3D"",\n    ""Conv3DLSTM"",\n    ""Conv3DTranspose"",\n    ""DeepRNN"",\n    ""Deferred"",\n    ""DepthwiseConv2D"",\n    ""Dropout"",\n    ""Embed"",\n    ""ExponentialMovingAverage"",\n    ""flatten"",\n    ""Flatten"",\n    ""GroupNorm"",\n    ""InstanceNorm"",\n    ""GRU"",\n    ""LSTM"",\n    ""LSTMState"",\n    ""LayerNorm"",\n    ""Linear"",\n    ""Mean"",\n    ""Metric"",\n    ""Module"",\n    ""Optimizer"",\n    ""reshape"",\n    ""Reshape"",\n    ""RNNCore"",\n    ""Sequential"",\n    ""Sum"",\n    ""TrainableState"",\n    ""UnrolledLSTM"",\n    ""UnrolledRNN"",\n    ""VanillaRNN"",\n    ""allow_empty_variables"",\n    ""build"",\n    ""custom_variable_getter"",\n    ""deep_rnn_with_residual_connections"",\n    ""deep_rnn_with_skip_connections"",\n    ""distribute"",\n    ""dynamic_unroll"",\n    ""format_variables"",\n    ""initializers"",\n    ""log_variables"",\n    ""lstm_with_recurrent_dropout"",\n    ""merge_leading_dims"",\n    ""no_name_scope"",\n    ""nets"",\n    ""once"",\n    ""leaky_clip_by_value"",\n    ""optimizers"",\n    ""pad"",\n    ""regularizers"",\n    ""scale_gradient"",\n    ""split_leading_dim"",\n    ""static_unroll"",\n)\n\n__version__ = ""2.0.0""\n\n#  ________________________________________\n# / Please don\'t use symbols in `src` they \\\n# \\ are not part of the Sonnet public API. /\n#  ----------------------------------------\n#         \\   ^__^\n#          \\  (oo)\\_______\n#             (__)\\       )\\/\\\n#                 ||----w |\n#                 ||     ||\n#\ntry:\n  del src  # pylint: disable=undefined-variable\nexcept NameError:\n  pass\n'"
sonnet/distribute.py,0,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Utilities for using Sonnet with TensorFlow Distribution Strategy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom sonnet.src.distribute.batch_norm import CrossReplicaBatchNorm\nfrom sonnet.src.distribute.replicator import create_variables_eagerly\nfrom sonnet.src.distribute.replicator import Replicator\nfrom sonnet.src.distribute.replicator import TpuReplicator\n\n__all__ = (\n    ""create_variables_eagerly"",\n    ""Replicator"",\n    ""TpuReplicator"",\n    ""CrossReplicaBatchNorm"",\n)\n'"
sonnet/initializers.py,0,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Initializers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom sonnet.src.initializers import Constant\nfrom sonnet.src.initializers import Identity\nfrom sonnet.src.initializers import Initializer\nfrom sonnet.src.initializers import Ones\nfrom sonnet.src.initializers import Orthogonal\nfrom sonnet.src.initializers import RandomNormal\nfrom sonnet.src.initializers import RandomUniform\nfrom sonnet.src.initializers import TruncatedNormal\nfrom sonnet.src.initializers import VarianceScaling\nfrom sonnet.src.initializers import Zeros\n\n__all__ = (\n    ""Constant"",\n    ""Identity"",\n    ""Initializer"",\n    ""Ones"",\n    ""Orthogonal"",\n    ""RandomNormal"",\n    ""RandomUniform"",\n    ""TruncatedNormal"",\n    ""VarianceScaling"",\n    ""Zeros"",\n)\n'"
sonnet/mixed_precision.py,0,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Sonnet mixed precision built for TensorFlow 2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom sonnet.src.mixed_precision import disable\nfrom sonnet.src.mixed_precision import enable\nfrom sonnet.src.mixed_precision import modes\nfrom sonnet.src.mixed_precision import scope\n\n__all__ = (\n    ""disable"",\n    ""enable"",\n    ""modes"",\n    ""scope"",\n)\n'"
sonnet/optimizers.py,0,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Sonnet optimizers built for TensorFlow 2.\n\nAll optimizers implement the `snt.Optimizer` interface.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom sonnet.src.optimizers.adam import Adam\nfrom sonnet.src.optimizers.momentum import Momentum\nfrom sonnet.src.optimizers.rmsprop import RMSProp\nfrom sonnet.src.optimizers.sgd import SGD\n\n__all__ = (\n    ""Adam"",\n    ""Momentum"",\n    ""RMSProp"",\n    ""SGD"",\n)\n'"
sonnet/pad.py,0,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Paddings.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom sonnet.src.pad import causal\nfrom sonnet.src.pad import create\nfrom sonnet.src.pad import full\nfrom sonnet.src.pad import reverse_causal\nfrom sonnet.src.pad import same\nfrom sonnet.src.pad import valid\n\n__all__ = (\n    ""causal"",\n    ""create"",\n    ""full"",\n    ""reverse_causal"",\n    ""same"",\n    ""valid"",\n)\n'"
sonnet/regularizers.py,0,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Regularizers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom sonnet.src.regularizers import L1\nfrom sonnet.src.regularizers import L2\nfrom sonnet.src.regularizers import OffDiagonalOrthogonal\nfrom sonnet.src.regularizers import Regularizer\n\n__all__ = [\n    ""L1"",\n    ""L2"",\n    ""OffDiagonalOrthogonal"",\n    ""Regularizer"",\n]\n'"
docs/ext/link_tf_api.py,1,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Reference TensorFlow API symbols.\n\nThis extension allows to reference TensorFlow API symbols using the\n``:tf:`` role. For example, the following::\n\n    Sonnet :py:`~base.Module` is based on :tf:`Module`.\n\ngenerates a link to ``tf.Module``.\n""""""\n\n# from __future__ import google_type_annotations\n\nimport functools\n\nfrom docutils import nodes\nfrom docutils.parsers.rst import states\nfrom six.moves.urllib import parse as urlparse\nimport tensorflow as tf\nfrom typing import Any\nfrom typing import List\nfrom typing import Text\nfrom typing import Tuple\n\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.util import tf_export\n\n__version__ = ""0.1""\n\n# TODO(slebedev): make the version configurable or infer from ``tf``?\nTF_VERSION = ""2.0""\nTF_API_BASE_URL = (\n    ""https://www.tensorflow.org/versions/r%s/api_docs/python/tf/"" % TF_VERSION)\n\n\ndef tf_role_fn(\n    typ: Text,\n    rawtext: Text,\n    text: Text,\n    lineno: int,\n    inliner: states.Inliner,\n    options: Any = None,\n    content: Any = None) -> Tuple[List[nodes.Node], List[nodes.system_message]]:\n  """"""Generates a reference to a given TensorFlow API symbol.\n\n  Only exported API symbols can be referenced. For example, non-exported\n  :tf:`float32` will not produce a reference and will be rendered as\n  plain-text.\n\n  Args:\n    typ: Type of the role. Fixed to ``""tf""``.\n    rawtext: Raw contents of the role, e.g. ``"":tf:`Module``""`.\n    text: The `contents` of the role e.g. ``""Module""``.\n    lineno: Line number of the parsed role.\n    inliner: Inline reST markup parser. Used for error reporting.\n    options: Unused.\n    content: Unused.\n\n  Returns:\n    Generated reST nodes and system messages.\n  """"""\n  del options, content  # Unused.\n\n  canonical_url = tf_doc_url(text)\n  xref = nodes.literal(rawtext, typ + ""."" + text, classes=[""xref""])\n  if not canonical_url:\n    warning = (\n        ""unable to expand :%s:`%s`; symbol is not exported by TensorFlow."" %\n        (typ, text))\n    inliner.reporter.warning(warning, line=lineno)\n    return [xref], []\n  else:\n    node = nodes.reference(\n        rawtext, """", xref, internal=False, refuri=canonical_url)\n    return [node], []\n\n\ndef tf_doc_url(text):\n  """"""Retrieves the TensorFlow doc URL for the given symbol.\n\n  Args:\n    text: A string for a symbol inside TF (e.g. ``""optimizers.Adam""``).\n\n  Returns:\n    A string URL linking to the TensorFlow doc site or ``None`` if a URL could\n    not be resolved.\n  """"""\n  get_tf_name = functools.partial(\n      tf_export.get_canonical_name_for_symbol, add_prefix_to_v1_names=True)\n\n  try:\n    prev_symbol = None\n    symbol = tf\n    for chunk in text.split("".""):\n      prev_symbol = symbol\n      symbol = getattr(prev_symbol, chunk)\n  except AttributeError:\n    return None\n\n  canonical_name = get_tf_name(symbol)\n\n  # Check if we\'re looking at a method reference (e.g. ""TensorArray.read"").\n  if prev_symbol and not canonical_name:\n    prev_canonical_name = get_tf_name(prev_symbol)\n    if prev_canonical_name:\n      canonical_name = prev_canonical_name + ""#"" + text.split(""."")[-1]\n\n  if not canonical_name:\n    return None\n\n  return urlparse.urljoin(TF_API_BASE_URL, canonical_name.replace(""."", ""/""))\n\n\ndef setup(app):\n  app.add_role(""tf"", tf_role_fn)\n\n  return {\n      ""version"": __version__,\n      ""parallel_read_safe"": True,\n      ""parallel_write_safe"": True,\n  }\n'"
docs/ext/link_tf_api_test.py,0,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for ``:tf:`` Sphinx role.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import absltest\nfrom docs.ext import link_tf_api\n\nDOC_BASE_URL = ""https://www.tensorflow.org/versions/r2.0/api_docs/python/tf""\n\n\nclass LinkTfApiTest(absltest.TestCase):\n\n  def test_non_existent(self):\n    self.assertIsNone(link_tf_api.tf_doc_url(""tomhennigan""))\n    self.assertIsNone(link_tf_api.tf_doc_url(""autograph.1""))\n\n  def test_link_to_top_level(self):\n    self.assertEqual(\n        link_tf_api.tf_doc_url(""function""), DOC_BASE_URL + ""/function"")\n    self.assertEqual(link_tf_api.tf_doc_url(""Module""), DOC_BASE_URL + ""/Module"")\n\n  def test_link_to_nested_package(self):\n    self.assertEqual(\n        link_tf_api.tf_doc_url(""autograph.to_code""),\n        DOC_BASE_URL + ""/autograph/to_code"")\n\n  def test_link_to_method_of_exported_class(self):\n    self.assertEqual(\n        link_tf_api.tf_doc_url(""TensorArray.read""),\n        DOC_BASE_URL + ""/TensorArray#read"")\n\n  def test_link_to_non_existent_method_of_exported_class(self):\n    self.assertIsNone(link_tf_api.tf_doc_url(""TensorArray.tomhennigan""))\n\n\nif __name__ == ""__main__"":\n  absltest.main()\n'"
sonnet/nets/__init__.py,0,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Common network architectures implemented as Sonnet modules.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom sonnet.nets import resnet\nfrom sonnet.src.nets.cifar10_convnet import Cifar10ConvNet\nfrom sonnet.src.nets.mlp import MLP\nfrom sonnet.src.nets.resnet import ResNet\nfrom sonnet.src.nets.resnet import ResNet50\nfrom sonnet.src.nets.vqvae import VectorQuantizer\nfrom sonnet.src.nets.vqvae import VectorQuantizerEMA\n\n__all__ = (\n    ""MLP"",\n    ""Cifar10ConvNet"",\n    ""resnet"",\n    ""ResNet"",\n    ""ResNet50"",\n    ""VectorQuantizer"",\n    ""VectorQuantizerEMA"",\n)\n'"
sonnet/nets/resnet.py,0,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""ResNet components.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom sonnet.src.nets.resnet import BlockGroup\nfrom sonnet.src.nets.resnet import BottleNeckBlockV1\nfrom sonnet.src.nets.resnet import BottleNeckBlockV2\n\n__all__ = (\n    ""BlockGroup"",\n    ""BottleNeckBlockV1"",\n    ""BottleNeckBlockV2"",\n)\n'"
sonnet/src/axis_norm.py,8,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Generic axis normalization module.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport collections\nimport six\n\nfrom sonnet.src import base\nfrom sonnet.src import initializers\nfrom sonnet.src import once\nfrom sonnet.src import types\nfrom sonnet.src import utils\n\nimport tensorflow as tf\nfrom typing import Optional, Text\n\n\nclass LayerNorm(base.Module):\n  r""""""Normalizes inputs along the given axes.\n\n  This is a generic implementation of normalization along specific axes of the\n  input. :class:`InstanceNorm` is a subclass of this module, it normalizes over\n  the spatial dimensions.\n\n  It transforms the input ``x`` into:\n\n  .. math::\n\n     \\d{outputs} = \\d{scale} \\dfrac{x - \\mu}{\\sigma + \\epsilon} + \\d{offset}\n\n  Where :math:`\\mu` and :math:`\\sigma` are respectively the mean and standard\n  deviation of ``x``.\n\n  There are many different variations for how users want to manage scale and\n  offset if they require them at all. These are:\n\n    - No ``scale``/``offset`` in which case ``create_*`` should be set to\n      ``False`` and ``scale``/``offset`` aren\'t passed when the module is\n      called.\n    - Trainable ``scale``/``offset`` in which case create_* should be set to\n      ``True`` and again ``scale``/``offset`` aren\'t passed when the module is\n      called. In this case this module creates and owns the scale/offset\n      variables.\n    - Externally generated ``scale``/``offset``, such as for conditional\n      normalization, in which case ``create_*`` should be set to ``False`` and\n      then the values fed in at call time.\n\n  Attributes:\n    scale: If ``create_scale=True``, a trainable :tf:`Variable` holding the\n      current scale.\n    offset: If ``create_offset=True``, a trainable :tf:`Variable` holding the\n      current offset.\n  """"""\n\n  def __init__(self,\n               axis: types.Axis,\n               create_scale: bool,\n               create_offset: bool,\n               eps: types.FloatLike = 1e-5,\n               scale_init: Optional[initializers.Initializer] = None,\n               offset_init: Optional[initializers.Initializer] = None,\n               data_format: Text = ""channels_last"",\n               name: Optional[Text] = None):\n    r""""""Constructs an ``LayerNorm`` module.\n\n    Args:\n      axis: An ``int``, ``slice`` or sequence of ``int``\\s representing the axes\n        which should be normalized across. Typical usages are: ``1`` or ``-1``\n        for normalization over just the channels and ``slice(1, None)``,\n        ``slice(2, None)`` for normalization over the spatial and channel\n        dimensions whilst avoiding the batch and/or time dimensions.\n      create_scale: ``bool`` representing whether to create a trainable scale\n        per channel applied after the normalization.\n      create_offset: ``bool`` representing whether to create a trainable offset\n        per channel applied after normalization and scaling.\n      eps: Small epsilon to avoid division by zero variance. Defaults to\n        ``1e-5``.\n      scale_init: Optional initializer for the scale variable. Can only be set\n        if ``create_scale=True``. By default scale is initialized to ``1``.\n      offset_init: Optional initializer for the offset variable. Can only be set\n        if ``create_offset=True``. By default offset is initialized to ``0``.\n      data_format: The data format of the input. Can be either\n        ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By\n        default it is ``channels_last``.\n      name: Name of the module.\n    """"""\n    super(LayerNorm, self).__init__(name=name)\n\n    if isinstance(axis, slice):\n      self._axis = axis\n    elif isinstance(axis, six.integer_types):\n      self._axis = (axis,)\n    elif (isinstance(axis, collections.Iterable) and\n          all(isinstance(ax, six.integer_types) for ax in axis)):\n      self._axis = axis\n    else:\n      raise ValueError(""`axis` should be an int, slice or iterable of ints."")\n\n    self._eps = eps\n\n    self._data_format = data_format\n    self._channel_index = utils.get_channel_index(data_format)\n\n    self._rank = None\n\n    self._create_scale = create_scale\n    self._create_offset = create_offset\n\n    if self._create_scale:\n      self._scale_init = (\n          scale_init if scale_init is not None else initializers.Ones())\n    elif scale_init is not None:\n      raise ValueError(""Cannot set `scale_init` if `create_scale=False`."")\n    if self._create_offset:\n      self._offset_init = (\n          offset_init if offset_init is not None else initializers.Zeros())\n    elif offset_init is not None:\n      raise ValueError(""Cannot set `offset_init` if `create_offset=False`."")\n\n  def __call__(self,\n               inputs: tf.Tensor,\n               scale: Optional[tf.Tensor] = None,\n               offset: Optional[tf.Tensor] = None) -> tf.Tensor:\n    """"""Returns normalized inputs.\n\n    Args:\n      inputs: An n-D tensor of the ``data_format`` specified in the constructor\n        on which the transformation is performed.\n      scale: A tensor up to n-D. The shape of this tensor must be broadcastable\n        to the shape of ``inputs``. This is the scale applied to the normalized\n        inputs. This cannot be passed in if the module was constructed with\n        ``create_scale=True``.\n      offset: A tensor up to n-D. The shape of this tensor must be broadcastable\n        to the shape of ``inputs``. This is the offset applied to the normalized\n        ``inputs``. This cannot be passed in if the module was constructed with\n        ``create_offset=True``.\n\n    Returns:\n      An n-d tensor of the same shape as inputs that has been normalized.\n    """"""\n    self._initialize(inputs)\n    if self._create_scale:\n      if scale is not None:\n        raise ValueError(\n            ""Cannot pass `scale` at call time if `create_scale=True`."")\n      scale = self.scale\n\n    if self._create_offset:\n      if offset is not None:\n        raise ValueError(\n            ""Cannot pass `offset` at call time if `create_offset=True`."")\n      offset = self.offset\n\n    if len(inputs.shape) != self._rank:\n      raise ValueError(\n          ""The rank of the inputs cannot change between calls, the""\n          "" original call was rank={} but this call was rank={}."".format(\n              self._rank, len(inputs.shape)))\n\n    mean, var = tf.nn.moments(inputs, self._axis, keepdims=True)\n\n    normalized = tf.nn.batch_normalization(\n        inputs,\n        mean=mean,\n        variance=var,\n        scale=scale,\n        offset=offset,\n        variance_epsilon=self._eps)\n    return normalized\n\n  @once.once\n  def _initialize(self, inputs: tf.Tensor):\n    """"""Setup of rank specific values.""""""\n    self._rank = len(inputs.shape)\n\n    # Turns slice into list of axis\n    if isinstance(self._axis, slice):\n      axes = tuple(range(self._rank))\n      self._axis = axes[self._axis]\n\n    # Create scale and offset variables\n    dtype = inputs.dtype\n    if self._channel_index == -1:\n      params_shape = [inputs.shape[-1]]\n    else:  # self._channel_index == 1\n      params_shape = [inputs.shape[1]] + [1] * (self._rank - 2)\n\n    if self._create_scale:\n      self.scale = tf.Variable(\n          self._scale_init(params_shape, dtype), name=""scale"")\n    else:\n      self.scale = None\n\n    if self._create_offset:\n      self.offset = tf.Variable(\n          self._offset_init(params_shape, dtype), name=""offset"")\n    else:\n      self.offset = None\n\n\nclass InstanceNorm(LayerNorm):\n  """"""Normalizes inputs along the spatial dimensions.\n\n  See :class:`LayerNorm` for more details.\n\n  Attributes:\n    scale: If ``create_scale=True``, a trainable :tf:`Variable` holding the\n      current scale.\n    offset: If ``create_offset=True``, a trainable :tf:`Variable` holding the\n      current offset.\n  """"""\n\n  def __init__(self,\n               create_scale: bool,\n               create_offset: bool,\n               eps: types.FloatLike = 1e-5,\n               scale_init: Optional[initializers.Initializer] = None,\n               offset_init: Optional[initializers.Initializer] = None,\n               data_format: Text = ""channels_last"",\n               name: Optional[Text] = None):\n    """"""Constructs an ``InstanceNorm`` module.\n\n    This method creates a module which normalizes over the spatial dimensions.\n\n    Args:\n      create_scale: ``bool`` representing whether to create a trainable scale\n        per channel applied after the normalization.\n      create_offset: ``bool`` representing whether to create a trainable offset\n        per channel applied after normalization and scaling.\n      eps: Small epsilon to avoid division by zero variance. Defaults to\n        ``1e-5``.\n      scale_init: Optional initializer for the scale variable. Can only be set\n        if ``create_scale=True``. By default scale is initialized to ``1``.\n      offset_init: Optional initializer for the offset variable. Can only be set\n        if ``create_offset=True``. By default offset is initialized to ``0``.\n      data_format: The data format of the input. Can be either\n        ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By\n        default it is ``channels_last``.\n      name: Name of the module.\n    """"""\n    if utils.get_channel_index(data_format) == 1:\n      axis = slice(2, None)\n    else:  # channel_index = -1\n      axis = slice(1, -1)\n    super(InstanceNorm, self).__init__(\n        axis=axis,\n        create_scale=create_scale,\n        create_offset=create_offset,\n        eps=eps,\n        scale_init=scale_init,\n        offset_init=offset_init,\n        data_format=data_format,\n        name=name)\n'"
sonnet/src/axis_norm_test.py,58,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.axis_norm.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom sonnet.src import axis_norm\nfrom sonnet.src import initializers\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\n\nclass LayerNormTest(test_utils.TestCase, parameterized.TestCase):\n\n  def testSimpleCase(self):\n    layer = axis_norm.LayerNorm([1, 2], create_scale=False, create_offset=False)\n    inputs = tf.ones([2, 3, 3, 5])\n\n    outputs = layer(inputs).numpy()\n    for x in np.nditer(outputs):\n      self.assertEqual(x, 0.0)\n\n  def testSimpleCaseVar(self):\n    layer = axis_norm.LayerNorm([1, 2],\n                                create_scale=True,\n                                create_offset=True,\n                                scale_init=initializers.Constant(0.5),\n                                offset_init=initializers.Constant(2.0))\n\n    inputs = tf.ones([2, 3, 3, 5])\n\n    outputs = layer(inputs).numpy()\n    for x in np.nditer(outputs):\n      self.assertEqual(x, 2.0)\n\n  def testSimpleCaseNCHWVar(self):\n    layer = axis_norm.LayerNorm([1, 2],\n                                create_scale=True,\n                                create_offset=True,\n                                scale_init=initializers.Constant(0.5),\n                                offset_init=initializers.Constant(2.0),\n                                data_format=""NCHW"")\n\n    inputs = tf.ones([2, 5, 3, 3])\n\n    outputs = layer(inputs).numpy()\n    for x in np.nditer(outputs):\n      self.assertEqual(x, 2.0)\n\n  def testDataFormatAgnosticVar(self):\n    c_last_layer = axis_norm.LayerNorm([1, 2],\n                                       create_scale=True,\n                                       create_offset=True)\n    c_first_layer = axis_norm.LayerNorm([2, 3],\n                                        create_scale=True,\n                                        create_offset=True,\n                                        data_format=""NCHW"")\n\n    inputs = tf.random.uniform([3, 4, 4, 5], 0, 10)\n\n    c_last_output = c_last_layer(inputs)\n    inputs = tf.transpose(inputs, [0, 3, 1, 2])\n    c_first_output = c_first_layer(inputs)\n    c_first_output = tf.transpose(c_first_output, [0, 2, 3, 1])\n\n    self.assertAllClose(c_last_output.numpy(), c_first_output.numpy())\n\n  def testSimpleCaseTensor(self):\n    layer = axis_norm.LayerNorm([1, 2], create_scale=False, create_offset=False)\n\n    inputs = tf.ones([2, 3, 3, 5])\n    scale = tf.constant(0.5, shape=(5,))\n    offset = tf.constant(2.0, shape=(5,))\n\n    outputs = layer(inputs, scale, offset).numpy()\n    for x in np.nditer(outputs):\n      self.assertEqual(x, 2.0)\n\n  def testSimpleCaseNCHWTensor(self):\n    layer = axis_norm.LayerNorm([1, 2],\n                                data_format=""NCHW"",\n                                create_scale=False,\n                                create_offset=False)\n\n    inputs = tf.ones([2, 5, 3, 3])\n    scale = tf.constant(0.5, shape=(5, 1, 1))\n    offset = tf.constant(2.0, shape=(5, 1, 1))\n\n    outputs = layer(inputs, scale, offset).numpy()\n    for x in np.nditer(outputs):\n      self.assertEqual(x, 2.0)\n\n  def testDataFormatAgnosticTensor(self):\n    c_last_layer = axis_norm.LayerNorm([1, 2],\n                                       create_scale=False,\n                                       create_offset=False)\n    c_first_layer = axis_norm.LayerNorm([2, 3],\n                                        data_format=""NCHW"",\n                                        create_scale=False,\n                                        create_offset=False)\n\n    inputs = tf.random.uniform([3, 4, 4, 5], 0, 10)\n    scale = tf.random.normal((5,), mean=1.0)\n    offset = tf.random.normal((5,))\n\n    c_last_output = c_last_layer(inputs, scale, offset)\n    inputs = tf.transpose(inputs, [0, 3, 1, 2])\n    scale = tf.reshape(scale, (5, 1, 1))\n    offset = tf.reshape(offset, (5, 1, 1))\n    c_first_output = c_first_layer(inputs, scale, offset)\n    c_first_output = tf.transpose(c_first_output, [0, 2, 3, 1])\n\n    self.assertAllClose(c_last_output.numpy(), c_first_output.numpy())\n\n  @parameterized.parameters(""NHW"", ""HWC"", ""channel_last"")\n  def testInvalidDataFormat(self, data_format):\n    with self.assertRaisesRegexp(\n        ValueError,\n        ""Unable to extract channel information from \'{}\'."".format(data_format)):\n      axis_norm.LayerNorm(\n          3, data_format=data_format, create_scale=False, create_offset=False)\n\n  @parameterized.parameters(""NCHW"", ""NCW"", ""channels_first"")\n  def testValidDataFormatChannelsFirst(self, data_format):\n    test = axis_norm.LayerNorm(\n        3, data_format=data_format, create_scale=False, create_offset=False)\n\n    self.assertEqual(test._channel_index, 1)\n\n  @parameterized.parameters(""NHWC"", ""NWC"", ""channels_last"")\n  def testValidDataFormatChannelsLast(self, data_format):\n    test = axis_norm.LayerNorm(\n        3, data_format=data_format, create_scale=False, create_offset=False)\n\n    self.assertEqual(test._channel_index, -1)\n\n  @parameterized.named_parameters((""String"", ""foo""), (""ListString"", [""foo""]))\n  def testInvalidAxis(self, axis):\n    with self.assertRaisesRegexp(\n        ValueError, ""`axis` should be an int, slice or iterable of ints.""):\n      axis_norm.LayerNorm(axis, create_scale=False, create_offset=False)\n\n  def testNoScaleAndInitProvided(self):\n    with self.assertRaisesRegexp(\n        ValueError, ""Cannot set `scale_init` if `create_scale=False`.""):\n      axis_norm.LayerNorm(\n          3,\n          create_scale=False,\n          create_offset=True,\n          scale_init=initializers.Ones())\n\n  def testNoOffsetBetaInitProvided(self):\n    with self.assertRaisesRegexp(\n        ValueError, ""Cannot set `offset_init` if `create_offset=False`.""):\n      axis_norm.LayerNorm(\n          3,\n          create_scale=True,\n          create_offset=False,\n          offset_init=initializers.Zeros())\n\n  def testCreateScaleAndScaleProvided(self):\n    layer = axis_norm.LayerNorm([2], create_scale=True, create_offset=False)\n\n    with self.assertRaisesRegexp(\n        ValueError, ""Cannot pass `scale` at call time if `create_scale=True`.""):\n      layer(tf.ones([2, 3, 4]), scale=tf.ones([4]))\n\n  def testCreateOffsetAndOffsetProvided(self):\n    layer = axis_norm.LayerNorm([2], create_offset=True, create_scale=False)\n\n    with self.assertRaisesRegexp(\n        ValueError,\n        ""Cannot pass `offset` at call time if `create_offset=True`.""):\n      layer(tf.ones([2, 3, 4]), offset=tf.ones([4]))\n\n  def testSliceAxis(self):\n    slice_layer = axis_norm.LayerNorm(\n        slice(1, -1), create_scale=False, create_offset=False)\n    axis_layer = axis_norm.LayerNorm((1, 2),\n                                     create_scale=False,\n                                     create_offset=False)\n\n    inputs = tf.random.uniform([3, 4, 4, 5], 0, 10)\n    scale = tf.random.normal((5,), mean=1.0)\n    offset = tf.random.normal((5,))\n\n    slice_outputs = slice_layer(inputs, scale, offset)\n    axis_outputs = axis_layer(inputs, scale, offset)\n\n    self.assertAllEqual(slice_outputs.numpy(), axis_outputs.numpy())\n\n  def testRankChanges(self):\n    layer = axis_norm.LayerNorm((1, 2), create_scale=False, create_offset=False)\n\n    inputs = tf.ones([2, 3, 3, 5])\n    scale = tf.constant(0.5, shape=(5,))\n    offset = tf.constant(2.0, shape=(5,))\n\n    layer(inputs, scale, offset)\n\n    with self.assertRaisesRegexp(\n        ValueError,\n        ""The rank of the inputs cannot change between calls, the original""):\n      layer(tf.ones([2, 3, 3, 4, 5]), scale, offset)\n\n  def testWorksWithFunction(self):\n    layer = axis_norm.LayerNorm((1, 2), create_scale=False, create_offset=False)\n    function_layer = tf.function(layer)\n\n    inputs = tf.ones([2, 3, 3, 5])\n    scale = tf.constant(0.5, shape=(5,))\n    offset = tf.constant(2.0, shape=(5,))\n\n    outputs = layer(inputs, scale, offset)\n    function_outputs = function_layer(inputs, scale, offset)\n\n    self.assertAllEqual(outputs.numpy(), function_outputs.numpy())\n\n  def testShapeAgnostic(self):\n    layer = axis_norm.LayerNorm((1, 2), create_scale=False, create_offset=False)\n    inputs_spec = tf.TensorSpec([None, None, None, None], dtype=tf.float32)\n    params_spec = tf.TensorSpec([None], dtype=tf.float32)\n    function_layer = tf.function(layer).get_concrete_function(\n        inputs_spec, params_spec, params_spec)\n\n    scale = tf.constant(0.5, shape=(5,))\n    offset = tf.constant(2.0, shape=(5,))\n\n    outputs = function_layer(tf.ones([2, 3, 3, 5]), scale, offset)\n    self.assertEqual(outputs.shape, [2, 3, 3, 5])\n    for x in np.nditer(outputs):\n      self.assertEqual(x, 2.0)\n\n    scale = tf.constant(0.5, shape=(3,))\n    offset = tf.constant(2.0, shape=(3,))\n\n    outputs = function_layer(tf.ones([3, 4, 6, 3]), scale, offset)\n    self.assertEqual(outputs.shape, [3, 4, 6, 3])\n    for x in np.nditer(outputs):\n      self.assertEqual(x, 2.0)\n\n  def test5DDataFormatAgnostic(self):\n    c_last_layer = axis_norm.LayerNorm([1, 2, 3],\n                                       create_scale=False,\n                                       create_offset=False)\n    c_first_layer = axis_norm.LayerNorm([2, 3, 4],\n                                        create_scale=False,\n                                        create_offset=False,\n                                        data_format=""NCDHW"")\n\n    inputs = tf.random.uniform([3, 4, 4, 4, 5], 0, 10)\n    scale = tf.random.normal((5,), mean=1.0)\n    offset = tf.random.normal((5,))\n\n    c_last_output = c_last_layer(inputs, scale, offset)\n    inputs = tf.transpose(inputs, [0, 4, 1, 2, 3])\n    scale = tf.reshape(scale, [-1, 1, 1, 1])\n    offset = tf.reshape(offset, [-1, 1, 1, 1])\n    c_first_output = c_first_layer(inputs, scale, offset)\n    c_first_output = tf.transpose(c_first_output, [0, 2, 3, 4, 1])\n\n    self.assertAllClose(\n        c_last_output.numpy(), c_first_output.numpy(), atol=1e-5, rtol=1e-5)\n\n  def test3DDataFormatAgnostic(self):\n    c_last_layer = axis_norm.LayerNorm([1],\n                                       create_scale=False,\n                                       create_offset=False)\n    c_first_layer = axis_norm.LayerNorm([2],\n                                        create_scale=False,\n                                        create_offset=False,\n                                        data_format=""NCW"")\n\n    inputs = tf.random.uniform([3, 4, 5], 0, 10)\n    scale = tf.random.normal((5,), mean=1.0)\n    offset = tf.random.normal((5,))\n\n    c_last_output = c_last_layer(inputs, scale, offset)\n    inputs = tf.transpose(inputs, [0, 2, 1])\n    scale = tf.reshape(scale, [-1, 1])\n    offset = tf.reshape(offset, [-1, 1])\n    c_first_output = c_first_layer(inputs, scale, offset)\n    c_first_output = tf.transpose(c_first_output, [0, 2, 1])\n\n    self.assertAllClose(\n        c_last_output.numpy(), c_first_output.numpy(), atol=1e-5, rtol=1e-5)\n\n  def testInstanceNormCorrectAxis(self):\n    layer = axis_norm.InstanceNorm(create_scale=True, create_offset=True)\n\n    inputs = tf.ones([3, 4, 5, 6])\n    layer(inputs)\n\n    self.assertEqual(layer._axis, (1, 2))\n\n  def testInstanceNormCorrectNCW(self):\n    layer = axis_norm.InstanceNorm(\n        create_scale=True, create_offset=True, data_format=""channels_first"")\n\n    inputs = tf.ones([3, 4, 5, 6])\n    layer(inputs)\n\n    self.assertEqual(layer._axis, (2, 3))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/base.py,8,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Base Sonnet module.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport abc\nimport functools\nimport inspect\nimport os\nimport pprint\nimport sys\n\nimport six\nfrom sonnet.src import once\nfrom sonnet.src import types\nfrom sonnet.src import utils\nimport tensorflow as tf\nfrom typing import Any, Callable, Dict, Optional, Sequence, Text, Tuple, Type, TypeVar\n\nT = TypeVar(""T"")\nTFFunctionType = type(tf.function(lambda: None, autograph=False))  # pylint: disable=invalid-name\nAPPLY_NAME_SCOPE = ""__snt_with_name_scope""\nALLOW_EMPTY_RESULT = ""__snt_allow_empty_result""\n\n\ndef no_name_scope(method: T) -> T:\n  """"""Decorator to wrap a method, preventing automatic name scope wrapping.\n\n  By default, any method on a module is considered as a forwards function, and\n  so any variables / modules created by the method will be scoped as belonging\n  to the module. In some cases this is undesirable, for example when\n  implementing ``.clone()`` / ``.transpose()``, as in those cases we want the\n  new module to have the scope of wherever the ``.transpose()`` call is made. To\n  allow this, decorate any methods with ``no_name_scope``.\n\n  Args:\n    method: the method to wrap.\n\n  Returns:\n    The method, with a flag indicating no name scope wrapping should occur.\n  """"""\n  # NOTE: This logic is tied to ModuleMetaclass.__new__, if anything is\n  # changed here corresponding changes will be needed there.\n  setattr(method, APPLY_NAME_SCOPE, False)\n  return method\n\n\nclass ModuleMetaclass(abc.ABCMeta):\n  """"""Metaclass for `Module`.""""""\n\n  def __new__(\n      mcs: Type[Type[T]],\n      name: Text,\n      bases: Tuple[Type[Any], ...],\n      clsdict: Dict[Text, Any],\n  ) -> Type[T]:\n    methods = []\n\n    for key, value in clsdict.items():\n      if key == ""name_scope"":\n        continue\n\n      elif key.startswith(""__"") and key != ""__call__"":\n        # Don\'t patch methods like `__getattr__` or `__del__`.\n        continue\n\n      elif isinstance(value, property):\n        # TODO(tomhennigan) Preserve the type of property subclasses.\n        clsdict[key] = property(\n            value.fget if not value.fget else with_name_scope(value.fget),\n            value.fset if not value.fset else with_name_scope(value.fset),\n            value.fdel if not value.fdel else with_name_scope(value.fdel),\n            doc=value.__doc__)\n\n      elif inspect.isfunction(value) or isinstance(value, TFFunctionType):\n        # We defer patching methods until after the type is created such that we\n        # can trigger the descriptor binding them to the class.\n        methods.append(key)\n\n    clsdict.setdefault(""__repr__"", lambda module: module._auto_repr)  # pylint: disable=protected-access\n\n    cls = super(ModuleMetaclass, mcs).__new__(mcs, name, bases, clsdict)\n\n    for method_name in methods:\n      # Note: the below is quite subtle, we need to ensure that we\'re wrapping\n      # the method bound to the class. In some cases (e.g. `wrapt`) this is\n      # important since the method can trigger different behavior when it is\n      # bound (e.g. in wrapt `FunctionWrapper.__get__(None, cls)` produces a\n      # `BoundFunctionWrapper` which in turn populates the `instance` argument\n      # to decorator functions using args[0]).\n      # Equivalent to: `cls.__dict__[method_name].__get__(None, cls)`\n      method = getattr(cls, method_name)\n      method = with_name_scope(method)\n      setattr(cls, method_name, method)\n\n    return cls\n\n  def __call__(cls: Type[T], *args, **kwargs) -> T:\n    # Call new such that we have an un-initialized module instance that we can\n    # still reference even if there is an exception during __init__. This is\n    # needed such that we can make sure the name_scope constructed in __init__\n    # is closed even if there is an exception.\n\n    # NOTE: We disable pytype since (somewhat surprisingly) this method is bound\n    # with the new class and not the metaclass.\n    module = cls.__new__(cls, *args, **kwargs)  # pytype: disable=wrong-arg-types\n\n    # Now attempt to initialize the object.\n    try:\n      module.__init__(*args, **kwargs)\n    except:\n      # We must explicitly catch so that in Python 2 sys.exc_info() is populated\n      # before entering the finally block.\n      raise\n    else:\n      module._auto_repr = auto_repr(cls, *args, **kwargs)  # pylint: disable=protected-access\n    finally:\n      exc_info = sys.exc_info()\n\n      # The base Module constructor enters the modules name scope before\n      # returning such that other functionality in the ctor happens within the\n      # modules name scope.\n      ctor_name_scope = getattr(module, ""_ctor_name_scope"", None)\n      if ctor_name_scope is not None:\n        ctor_name_scope.__exit__(*exc_info)\n        del module._ctor_name_scope\n\n      # TODO(tomhennigan) Remove `_scope_name` after next TF release.\n      ran_super_ctor = (\n          hasattr(module, ""_name_scope"") or hasattr(module, ""_scope_name""))\n\n      if exc_info[0] is None and not ran_super_ctor:\n        raise ValueError(\n            ""Constructing a snt.Module without calling the super constructor ""\n            ""is not supported. Add the following as the first line in your ""\n            ""__init__ method:\\n\\nsuper(%s, self).__init__()"" % cls.__name__)\n\n    return module\n\n\ndef safe_compare(a, b) -> bool:\n  try:\n    return bool(a == b)\n  except:  # pylint: disable=bare-except\n    # Some equality checks might be buggy (e.g. `tf.Tensor == None`), in those\n    # cases be defensive and assume `a != b`. Note that an exception is also\n    # thrown when a and b are ndarrays of >1 element.\n    # TODO(tomhennigan) We could be smarter about comparing ndarrays.\n    return False\n\n\ndef auto_repr(cls: Type[Any], *args, **kwargs) -> Text:\n  """"""Derives a `__repr__` from constructor arguments of a given class.\n\n      >>> class Foo(object):\n      ...   def __init__(self, x=None, y=42):\n      ...      pass\n      ...\n\n      >>> auto_repr(Foo, ""x"")\n      ""Foo(x=\'x\')""\n\n      >>> auto_repr(Foo, ""x"", y=21)\n      ""Foo(x=\'x\', y=21)""\n\n      >>> auto_repr(Foo, None, 42)\n      Foo()\n\n  Args:\n    cls: a class to derive `__repr__` for.\n    *args: positional arguments.\n    **kwargs: keyword arguments.\n\n  Returns:\n    A string representing a call equivalent to `cls(*args, **kwargs)`.\n  """"""\n  argspec = utils.getfullargspec(cls.__init__)\n  arg_names = argspec.args\n  # Keep used positionals minus self.\n  arg_names = arg_names[1:(len(args) + 1)]\n  # Keep used kwargs in the order they appear in argspec.\n  arg_names.extend(n for n in argspec.args if n in kwargs)\n  arg_values = inspect.getcallargs(cls.__init__, None, *args, **kwargs)\n\n  # Extract default parameter values.\n  defaults = argspec.defaults or ()\n  defaults = dict(zip(argspec.args[-len(defaults):], defaults))\n  is_default = lambda n, v: (n in defaults and safe_compare(v, defaults[n]))\n\n  names_and_values = [(name + ""="", arg_values[name]) for name in arg_names\n                      if not is_default(name, arg_values[name])]\n  # Add varargs.\n  names_and_values.extend(("""", arg) for arg in args[len(argspec.args) - 1:])\n  # Add varkwargs.\n  names_and_values.extend(\n      (name + ""="", kwargs[name]) for name in kwargs if name not in argspec.args)\n\n  single_line = cls.__name__ + ""({})"".format("", "".join(\n      name + repr(value) for name, value in names_and_values))\n  if len(single_line) <= 80:\n    return single_line\n  else:\n    return ""{}(\\n{},\\n)"".format(\n        cls.__name__,\n        indent(4, "",\\n"".join(fancy_repr(n, v) for n, v in names_and_values)))\n\n\ndef fancy_repr(name: Text, value: Any) -> Text:\n  repr_value = pprint.pformat(value)\n  if name:\n    repr_value = indent(len(name), repr_value).strip()\n  return name + repr_value\n\n\ndef indent(amount: int, s: Text) -> Text:\n  """"""Indents `s` with `amount` spaces.""""""\n  prefix = amount * "" ""\n  return ""\\n"".join(prefix + line for line in s.splitlines())\n\n\n@utils.decorator\ndef wrap_with_name_scope(\n    method: Callable[..., T],\n    instance: Any,\n    args: Sequence[Any],\n    kwargs: Dict[Text, Any],\n) -> T:\n  """"""Decorator that calls the given function in the module name scope.\n\n  Args:\n    method: The bound method to call.\n    instance: `Module` instance.\n    args: Positional arguments to `method`.\n    kwargs: Keyword arguments to `method`.\n\n  Returns:\n    `with instance.name_scope: return method(*args, **kwargs)`\n  """"""\n  if instance is None:\n    instance = args[0]\n    args = args[1:]\n    method = functools.partial(method, instance)\n\n  try:\n    module_name_scope = instance.name_scope\n  except AttributeError as exc_value_from:\n    exc_value = AttributeError(\n        ""The super constructor must be called before any other methods in ""\n        ""your constructor. If this is not possible then annotate all the ""\n        ""methods called with `@snt.no_name_scope`."")\n    six.raise_from(exc_value, exc_value_from)\n\n  with module_name_scope:\n    # snt.Module enters the module name scope for all methods. To disable this\n    # for a particular method annotate it with `@snt.no_name_scope`.\n    return method(*args, **kwargs)\n\n\n@utils.decorator\ndef wrap_with_name_scope_no_exception(\n    method: Callable[..., T],\n    instance: Any,\n    args: Sequence[Any],\n    kwargs: Dict[Text, Any],\n) -> T:\n  """"""Patches the given method so it enters the modules name scope.""""""\n  if instance is None:\n    instance = args[0]\n    args = args[1:]\n    method = functools.partial(method, instance)\n\n  with instance.name_scope:\n    # snt.Module enters the module name scope for all methods. To disable this\n    # for a particular method annotate it with `@snt.no_name_scope`.\n    return method(*args, **kwargs)\n\n\ndef with_name_scope(method: T) -> T:\n  """"""Patches the given method so it enters the modules name scope.""""""\n  if os.environ.get(""SNT_MODULE_NAME_SCOPES"", """").lower() in (""0"", ""false""):\n    # For debugging purposes name scoping can be disabled using the environment\n    # variable `SNT_MODULE_NAME_SCOPES` (note: this does not apply to __init__).\n    # This can help to make stack traces shallower and should have no\n    # behavioural effect (unless your code relies on string variable names).\n    return method\n  elif not getattr(method, APPLY_NAME_SCOPE, True):\n    # The function has been annotated to say that no autoscoping should be\n    # applied, so do not patch it.\n    return method\n  elif isinstance(method, TFFunctionType):\n    # Autograph cannot convert functions that have try/catch.\n    method._decorate(wrap_with_name_scope_no_exception)  # pylint: disable=protected-access\n    return method\n  elif hasattr(method, ""__snt_once_wrapped__""):\n    # Special case methods decorated with @snt.once so the name scope is pushed\n    # inside the function body rather than outside. This removes the overhead of\n    # entering/exiting the name_scope just to do nothing.\n    return once.once(wrap_with_name_scope(method.__snt_once_wrapped__))  # pylint: disable=no-value-for-parameter\n  else:\n    return wrap_with_name_scope(method)  # pylint: disable=no-value-for-parameter\n\n\nNO_VARIABLES_ERROR = """"""\n{module!r} does not currently contain any {property}.\n\nMost Sonnet modules create variables the first time they are called with an\ninput and requesting variables before this typically indicates a coding error.\n\nYou should refactor your code such that you request module variables after you\npass an example input to the module. For example:\n\n    module = {module!r}\n    output = module(input)\n    params = module.{property}\n\nIf the module is stateless consider using `snt.allow_empty_variables(module)` to\nsuppress this error:\n\n    module = {module!r}\n    snt.allow_empty_variables(module)\n    params = module.{property}\n\nYou can annotate your own subclasses directly if you prefer:\n\n    @snt.allow_empty_variables\n    class MyStatelessModule(snt.Module):\n      pass\n"""""".strip()\n\n\ndef allow_empty_variables(module_or_cls: T) -> T:\n  """"""Allows ``{trainable_,}variables`` to return empty results.\n\n  >>> mod = snt.Module()\n  >>> mod.variables\n  Traceback (most recent call last):\n    ...\n  ValueError: ... pass an example input to the module...\n  >>> mod = snt.allow_empty_variables(mod)\n  >>> mod.variables\n  ()\n\n  Args:\n    module_or_cls: A :class:`Module` instance or subclass to decorate.\n\n  Returns:\n    The input module or class.\n  """"""\n  setattr(module_or_cls, ALLOW_EMPTY_RESULT, True)\n  return module_or_cls\n\n\ndef assert_tf2():\n  if not assert_tf2.checked:\n    with tf.init_scope():\n      assert tf.executing_eagerly(), ""Sonnet v2 requires TensorFlow 2""\n    assert_tf2.checked = True\n\nassert_tf2.checked = False\n\n\nclass Module(six.with_metaclass(ModuleMetaclass, tf.Module)):\n  """"""Base class for Sonnet modules.\n\n  A Sonnet module is a lightweight container for variables and other modules.\n  Modules typically define one or more ""forward"" methods (e.g. ``__call__``)\n  which apply operations combining user input and module parameters. For\n  example::\n\n      >>> class MultiplyModule(snt.Module):\n      ...   def __call__(self, x):\n      ...     if not hasattr(self, \'w\'):\n      ...       self.w = tf.Variable(2., name=\'w\')\n      ...     return x * self.w\n\n      >>> mod = MultiplyModule()\n      >>> mod(1.)\n      <tf.Tensor: ... numpy=2.0>\n\n  Sonnet modules are a layer on top of :tf:`Module`, implementing automatic name\n  scoping as described in the original RFC :cite:`agarwal2019stateful`.\n  """"""\n\n  def __init__(self, name: Optional[Text] = None):\n    """"""Initializes the current module with the given name.\n\n    Subclasses should call this constructor before creating other modules or\n    variables such that those modules are named correctly.\n\n    Args:\n      name: An optional string name for the class. Must be a valid Python\n        identifier. If ``name`` is not provided then the class name for the\n        current instance is converted to ``lower_snake_case`` and used instead.\n    """"""\n    assert_tf2()\n\n    super(Module, self).__init__(name=name)\n\n    if getattr(self.__init__, APPLY_NAME_SCOPE, True):\n      # Enter the name scope so subsequent code in the contructor (e.g. creating\n      # submodules) happens inside the modules name scope. This is exited when\n      # the subclass __init__ returns (this is implemented in ModuleMetaclass).\n      self._ctor_name_scope = self.name_scope\n      self._ctor_name_scope.__enter__()\n\n  @property\n  def variables(self):\n    r""""""Sequence of :tf:`Variable`\\ s owned by this module and it\'s submodules.\n\n    See :tf:`Module.variables` for implementation details.\n\n    NOTE: Most Sonnet modules create variables lazily (e.g. the first time they\n    are called). As such just after construction there are typically no\n    variables. To mitigate a common error (calling ``.variables`` or\n    ``.trainable_variables`` before any variables are created) these properties\n    will raise an exception if their result is empty. See\n    :func:`allow_empty_variables` if you want to suppress this error.\n\n    Returns:\n      A sequence of variables for the current module (sorted by attribute\n      name) followed by variables from all submodules recursively (breadth\n      first).\n    """"""\n    variables = super(Module, self).variables\n    if not variables and not getattr(self, ALLOW_EMPTY_RESULT, False):\n      # Raise a useful error if the collection is empty. Typically this\n      # indicates that the user has requested the property before the module has\n      # been connected. In many situations this can cause hard to diagnose\n      # problems (eg. if you are trying to copy the initial state from one\n      # module to another by zipping both module variables and assigning one to\n      # the other).\n      raise ValueError(\n          NO_VARIABLES_ERROR.format(module=self, property=""variables""))\n    return variables\n\n  @property\n  def trainable_variables(self):\n    r""""""Sequence of :tf:`Variable`\\ s owned by this module and it\'s submodules.\n\n    See :tf:`Module.trainable_variables` for implementation details.\n\n    NOTE: Most Sonnet modules create variables lazily (e.g. the first time they\n    are called). As such just after construction there are typically no\n    variables. To mitigate a common error (calling ``.variables`` or\n    ``.trainable_variables`` before any variables are created) these properties\n    will raise an exception if their result is empty. See\n    :func:`allow_empty_variables` if you want to suppress this error.\n\n    Returns:\n      A sequence of variables for the current module (sorted by attribute\n      name) followed by variables from all submodules recursively (breadth\n      first).\n    """"""\n    trainable_variables = super(Module, self).trainable_variables\n    if not trainable_variables and not getattr(self, ALLOW_EMPTY_RESULT, False):\n      # Raise a useful error if the collection is empty. Typically this\n      # indicates that the user has requested the property before the module has\n      # been connected. In many situations this can cause hard to diagnose\n      # problems (eg. if you are trying to copy the initial state from one\n      # module to another by zipping both module variables and assigning one to\n      # the other).\n      raise ValueError(\n          NO_VARIABLES_ERROR.format(module=self,\n                                    property=""trainable_variables""))\n    return trainable_variables\n\n\nclass Optimizer(Module):\n  """"""Base class for Sonnet optimizers.""""""\n\n  @abc.abstractmethod\n  def apply(self, updates: Sequence[types.ParameterUpdate],\n            parameters: Sequence[tf.Variable]):\n    """"""Applies `updates` to `parameters`.""""""\n    pass\n'"
sonnet/src/base_test.py,26,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.base.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport six\nfrom sonnet.src import base\nfrom sonnet.src import test_utils\nimport tensorflow as tf\nimport wrapt\n\n\nclass BaseTest(test_utils.TestCase):\n\n  def test_basic(self):\n    m = LambdaModule()\n    self.assertIsNone(m(None))\n\n  def testWrappedMethod(self):\n    mod = WraptModule()\n    scope_name, y = mod(3)\n    self.assertEqual(scope_name, ""wrapt_module/"")\n    self.assertEqual(y, (3**2)**2)\n\n  def testControlFlow(self):\n    mod = ControlFlowModule()\n    f = tf.function(mod).get_concrete_function(tf.TensorSpec([]))\n    self.assertEqual(f(tf.constant(1.)).numpy(), 1.)\n    self.assertEqual(f(tf.constant(11.)).numpy(), 11.**2)\n\n\nclass TestModuleNaming(tf.test.TestCase):\n\n  def test_single_name(self):\n    mod = base.Module(name=""simple"")\n    self.assertEqual(mod.name, ""simple"")\n    self.assertEqual(mod.name_scope.name, ""simple/"")\n\n  def test_construct_in_scope(self):\n    with tf.name_scope(""foo""):\n      mod = base.Module(name=""bar"")\n    self.assertEqual(mod.name, ""bar"")\n    self.assertEqual(mod.name_scope.name, ""foo/bar/"")\n\n  def test_enters_name_scope_in_call(self):\n    mod = ReturnsNameScopeModule()\n    for _ in range(3):\n      self.assertEqual(mod(), mod.name_scope.name)\n\n  def test_enters_name_scope_in_other_method(self):\n    mod = ReturnsNameScopeModule()\n    for _ in range(3):\n      self.assertEqual(mod.alternative_forward(), mod.name_scope.name)\n\n  def test_subclassed_module(self):\n    mod = SubclassedReturnsNameScopeModule()\n    for _ in range(3):\n      self.assertEqual(mod.alternative_forward(), mod.name_scope.name)\n      self.assertEqual(mod.alternative_alternative_forward(),\n                       mod.name_scope.name)\n\n  def test_submodule_created_late(self):\n    m = TreeModule()\n    self.assertEqual(m.name, ""tree_module"")\n    self.assertEqual(m.name_scope.name, ""tree_module/"")\n    leaf1 = m.new_leaf()\n    self.assertEqual(leaf1.name, ""tree_module"")\n    self.assertEqual(leaf1.name_scope.name, ""tree_module/tree_module/"")\n\n  def test_does_not_evaluate_property_methods(self):\n    mod = PropertyThrowsWhenCalledModule()\n    with self.assertRaises(AssertionError):\n      mod.raise_assertion_error  # pylint: disable=pointless-statement\n\n  def test_overridden_name_scope(self):\n    mod = ModuleOverridingNameScope()\n    self.assertEqual(mod(), mod.name_scope.name)\n    self.assertEqual(mod.alternative_forward(), mod.name_scope.name)\n\n  def test_patched_callable(self):\n    with tf.name_scope(""foo""):\n      mod = base.Module(name=""bar"")\n    mod.foo = get_name_scope\n    # `foo` is not a method so we do not re-enter the name scope.\n    self.assertEqual(mod.foo(), """")\n\n  def test_property(self):\n    mod = PropertyModule()\n    mod.some_property = None, None  # None, None for the linter.\n    getter_scope_name, setter_scope_name = mod.some_property\n    self.assertEqual(getter_scope_name, ""property_module/"")\n    self.assertEqual(setter_scope_name, ""property_module/"")\n\n  def test_property_no_name_scope(self):\n    mod = PropertyModule()\n    mod.no_name_scope_property = None, None  # None, None for the linter.\n    getter_scope_name, setter_scope_name = mod.no_name_scope_property\n    self.assertEqual(getter_scope_name, """")\n    self.assertEqual(setter_scope_name, """")\n\n  def test_ctor_no_name_scope(self):\n    mod = CtorNoNameScope()\n    self.assertEqual(mod.ctor_name_scope, """")\n    self.assertEqual(mod.w.name, ""w:0"")\n\n  def test_ctor_no_name_scope_no_super(self):\n    msg = (""Constructing a snt.Module without calling the super constructor is ""\n           ""not supported"")\n    with self.assertRaisesRegexp(ValueError, msg):\n      CtorNoNameScopeNoSuper()\n\n  def test_invalid_name(self):\n    msg = "".* is not a valid module name""\n    with self.assertRaisesRegexp(ValueError, msg):\n      base.Module(name=""$Foo"")\n\n  def test_modules_not_numbered_in_eager(self):\n    mod = RecursiveModule(2)\n    self.assertEqual(mod.name_scope.name, ""badger/"")\n    self.assertEqual(mod.child.name_scope.name, ""badger/badger/"")\n\n    mod = RecursiveModule(2)\n    self.assertEqual(mod.name_scope.name, ""badger/"")\n    self.assertEqual(mod.child.name_scope.name, ""badger/badger/"")\n\n  def test_module_numbering_in_graph(self):\n    with tf.Graph().as_default():\n      mod = RecursiveModule(2)\n      self.assertEqual(mod.name_scope.name, ""badger/"")\n      self.assertEqual(mod.child.name_scope.name, ""badger/badger/"")\n\n      mod = RecursiveModule(2)\n      self.assertEqual(mod.name_scope.name, ""badger_1/"")\n      self.assertEqual(mod.child.name_scope.name, ""badger_1/badger/"")\n\n  def test_ctor_error_closes_name_scope(self):\n    with self.assertRaises(ErrorModuleError):\n      # If super constructor is called then a name scope is opened then an error\n      # is thrown. The metaclass should handle this and close the namescope\n      # before re-throwing the exception.\n      ErrorModule(call_super=True)\n\n    self.assertEqual("""", get_name_scope())\n\n  def test_ctor_error_handles_ctor_not_opening_name_scope(self):\n    with self.assertRaises(ErrorModuleError):\n      # If super ctor is not called then the name scope isn\'t opened. We need to\n      # ensure that this doesn\'t trigger an exception (e.g. the metaclass trying\n      # to __exit__ a non-existent name scope).\n      ErrorModule(call_super=False)\n\n    self.assertEqual("""", get_name_scope())\n\n  def test_forward_method_closes_name_scope(self):\n    mod = ErrorModule(call_super=True, raise_in_constructor=False)\n    with self.assertRaises(ErrorModuleError):\n      mod()\n\n    self.assertEqual("""", get_name_scope())\n\n  def test_get_attr_doesnt_enter_name_scope(self):\n    scope_names = []\n\n    class GetAttrModule(base.Module):\n\n      def __getattr__(self, name):\n        scope_names.append((name, get_name_scope()))\n        return super(GetAttrModule, self).__getattr__(name)\n\n    mod = GetAttrModule()\n    with self.assertRaises(AttributeError):\n      mod.does_not_exist  # pylint: disable=pointless-statement\n    self.assertIn((""does_not_exist"", """"), scope_names)\n\n  def test_get_attribute_doesnt_enter_name_scope(self):\n    scope_names = []\n\n    class GetAttributeModule(base.Module):\n\n      def __getattribute__(self, name):\n        scope_names.append((name, get_name_scope()))\n        return super(GetAttributeModule, self).__getattribute__(name)\n\n    mod = GetAttributeModule()\n    with self.assertRaises(AttributeError):\n      mod.does_not_exist  # pylint: disable=pointless-statement\n    self.assertIn((""does_not_exist"", """"), scope_names)\n\n\nclass VariableNamingTest(tf.test.TestCase):\n\n  def test_variable_names(self):\n    mod = RecursiveModule(3)\n    self.assertEqual(mod.w.name, ""badger/mushroom:0"")\n    self.assertEqual(mod.child.w.name, ""badger/badger/mushroom:0"")\n    self.assertEqual(mod.child.child.w.name, ""badger/badger/badger/mushroom:0"")\n\n\nclass AutoReprTest(tf.test.TestCase):\n\n  def test_order_matches_argspec(self):\n    module = RecursiveModule(trainable=False, depth=2)\n    self.assertEqual(repr(module), ""RecursiveModule(depth=2, trainable=False)"")\n\n  def test_defaults_ignored(self):\n    module = RecursiveModule(1)\n    self.assertEqual(repr(module), ""RecursiveModule(depth=1)"")\n\n  def test_does_not_fail_with_hostile_input(self):\n    r = RaisesOnEquality()\n    self.assertFalse(r.equality_checked)\n    module = NoopModule(r)\n    self.assertEqual(repr(module), ""NoopModule(a=hostile)"")\n    self.assertTrue(r.equality_checked)\n\n  def test_args_are_repred(self):\n    module = TreeModule(name=""TreeModule"")\n    self.assertEqual(repr(module), ""TreeModule(name=\'TreeModule\')"")\n    module = TreeModule(""TreeModule"")\n    self.assertEqual(repr(module), ""TreeModule(name=\'TreeModule\')"")\n\n  def test_long_repr_multi_line(self):\n    module = TakesSubmodules([TreeModule() for _ in range(6)], name=""hai"")\n    self.assertEqual(\n        repr(module), ""\\n"".join([\n            ""TakesSubmodules("",\n            ""    submodules=[TreeModule(),"",\n            ""                TreeModule(),"",\n            ""                TreeModule(),"",\n            ""                TreeModule(),"",\n            ""                TreeModule(),"",\n            ""                TreeModule()],"",\n            ""    name=\'hai\',"",\n            "")"",\n        ]))\n\n  def test_repr_wildcard(self):\n    module = WildcardInit(1, 2, 3, foo=""bar"")\n    # NOTE: This is not a valid piece of Python, but it is unambiguous and\n    # probably the most helpful thing we can do. An alternative would be to\n    # special case `__init__(a, *args)` and not render names preceding *args\n    # but this is unlikely to be common in the ctor.\n    self.assertEqual(repr(module), ""WildcardInit(a=1, b=2, 3, foo=\'bar\')"")\n\n  def test_repr_non_bool_equality(self):\n    class FooModule(base.Module):\n\n      def __init__(self, a=((-1., -1.))):\n        super(FooModule, self).__init__()\n\n    # auto_repr tests default values for equality. In numpy (and TF2) equality\n    # is tested elementwise so the return value of `==` is an ndarray which we\n    # then attempt to reduce to a boolean.\n    foo = FooModule(a=np.array([[2., 2.]]))\n    self.assertEqual(repr(foo), ""FooModule(a=array([[2., 2.]]))"")\n    foo = FooModule(a=np.array([[-1., -1.]]))\n    self.assertEqual(repr(foo), ""FooModule(a=array([[-1., -1.]]))"")\n\n\nclass ForwardMethodsTest(tf.test.TestCase):\n\n  def testFunctionType(self):\n    mod = ModuleWithFunctionAnnotatedCall()\n    self.assertIsInstance(mod.forward, base.TFFunctionType)\n    self.assertIsInstance(mod.forward_ag, base.TFFunctionType)\n\n  def testEntersNameScope_call(self):\n    mod = ModuleWithFunctionAnnotatedCall()\n    self.assertEqual(mod.forward().numpy(),\n                     b""module_with_function_annotated_call/"")\n    # TODO(b/122265385) Re-enable this assertion.\n    # self.assertEqual(mod.forward_ag().numpy(),\n    #                  b""module_with_function_annotated_call/"")\n\n  def testEntersNameScope_concreteFunction(self):\n    mod = ModuleWithFunctionAnnotatedCall()\n    self.assertEqual(mod.forward.get_concrete_function()().numpy(),\n                     b""module_with_function_annotated_call/"")\n    # TODO(b/122265385) Re-enable this assertion.\n    # self.assertEqual(mod.forward_ag.get_concrete_function()().numpy(),\n    #                  b""module_with_function_annotated_call/"")\n\n\nclass AbcTest(tf.test.TestCase):\n\n  def testAbstract(self):\n    msg = ""Can\'t instantiate .* abstract methods""\n    with self.assertRaisesRegexp(TypeError, msg):\n      AbstractModule()  # pylint: disable=abstract-class-instantiated\n\n  def testConcrete(self):\n    mod = ConcreteModule()\n    x, scope_name = mod(2.)\n    self.assertEqual(x, 4.)\n    self.assertEqual(scope_name, ""concrete_module/"")\n    self.assertEqual(get_name_scope(), """")\n\n  def testCallMethodsOnParent(self):\n    mod = ConcreteModule()\n    self.assertEqual(mod.foo(), True)\n\n\nclass CustomGradientTest(test_utils.TestCase):\n\n  def test_custom_gradient(self):\n    if tf.version.GIT_VERSION != ""unknown"":\n      # TODO(tomhennigan) Enable this once TF 2.0.1 comes out.\n      self.skipTest(""Requires TF > 2.0.0"")\n\n    mod = ZeroGradModule()\n    with tf.GradientTape() as tape:\n      y = mod(2.)\n    g = tape.gradient(y, mod.w)\n    self.assertAllEqual(g, tf.zeros([2, 2]))\n\n\nclass ZeroGradModule(base.Module):\n\n  @tf.custom_gradient\n  def __call__(self, x):\n    if not hasattr(self, ""w""):\n      self.w = tf.Variable(tf.ones([2, 2]), name=""w"")\n\n    with tf.GradientTape() as tape:\n      y = tf.reduce_sum(self.w ** x)\n    dw = tape.gradient(y, self.w)\n\n    def grad(dy, variables=None):\n      assert variables\n      return dy * 0, [dw * 0]\n\n    return y, grad\n\n\nclass LambdaModule(base.Module):\n\n  def __call__(self, x):\n    return x\n\n\ndef get_name_scope():\n  with tf.name_scope(""x"") as scope_name:\n    return scope_name[:-2]\n\n\n@wrapt.decorator\ndef wrapt_decorator(method, instance, args, kwargs):\n  if instance is None:\n    raise ValueError(""Expected instance to be non-null."")\n\n  scope_name, y = method(*args, **kwargs)\n  return scope_name, y**2\n\n\nclass WraptModule(base.Module):\n\n  @wrapt_decorator\n  def __call__(self, x):\n    return get_name_scope(), x**2\n\n\nclass ControlFlowModule(base.Module):\n\n  def __call__(self, x):\n    if x < 10:\n      return x\n    else:\n      return x**2\n\n\nclass ErrorModuleError(Exception):\n  pass\n\n\nclass ErrorModule(base.Module):\n\n  def __init__(self, call_super, raise_in_constructor=True):\n    if call_super:\n      super(ErrorModule, self).__init__()\n    if raise_in_constructor:\n      raise ErrorModuleError(""Deliberate error!"")\n\n  def __call__(self):\n    raise ErrorModuleError(""Deliberate error!"")\n\n\nclass RecursiveModule(base.Module):\n\n  def __init__(self, depth, trainable=True):\n    super(RecursiveModule, self).__init__(name=""badger"")\n    self.child = None\n    if depth > 1:\n      self.child = RecursiveModule(depth - 1, trainable=trainable)\n    self.w = tf.Variable(1.0, trainable=trainable, name=""mushroom"")\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass AbstractModule(base.Module):\n\n  @abc.abstractmethod\n  def __call__(self, x):\n    pass\n\n  def foo(self):\n    return True\n\n\nclass ConcreteModule(AbstractModule):\n\n  def __call__(self, x):\n    return x**2, get_name_scope()\n\n\nclass TreeModule(base.Module):\n\n  def __init__(self, name=None):\n    super(TreeModule, self).__init__(name=name)\n    self._leaves = []\n\n  def new_leaf(self, name=None):\n    leaf = TreeModule(name=name)\n    self._leaves.append(leaf)\n    return leaf\n\n\nclass ReturnsNameScopeModule(base.Module):\n\n  def alternative_forward(self):\n    return get_name_scope()\n\n  def __call__(self):\n    return get_name_scope()\n\n\nclass SubclassedReturnsNameScopeModule(ReturnsNameScopeModule):\n\n  def alternative_alternative_forward(self):\n    return get_name_scope()\n\n\nclass PropertyThrowsWhenCalledModule(base.Module):\n\n  @property\n  def raise_assertion_error(self):\n    raise AssertionError\n\n\nclass ModuleOverridingNameScope(ReturnsNameScopeModule):\n\n  @property\n  def name_scope(self):\n    return tf.name_scope(""yolo/"")\n\n\nclass CommonErrorsTest(test_utils.TestCase, parameterized.TestCase):\n\n  def test_not_calling_super_constructor(self):\n    msg = (""Constructing a snt.Module without calling the super constructor is ""\n           ""not supported"")\n    with self.assertRaisesRegexp(ValueError, msg):\n      DoesNotCallSuperConstructorModule()\n\n  def test_calls_method_before_super(self):\n    msg = ""super constructor must be called before any other methods""\n    with self.assertRaisesRegexp(AttributeError, msg):\n      CallsMethodBeforeSuperConstructorModule(allowed_method=False)\n\n  def test_annotated_method_is_allowed(self):\n    self.assertIsNotNone(\n        CallsMethodBeforeSuperConstructorModule(allowed_method=True))\n\n  @parameterized.parameters(""trainable_variables"", ""variables"")\n  def test_requests_variables_before_they_exist(self, property_name):\n    class MyModule(base.Module):\n      pass\n\n    mod = MyModule()\n    err = ""MyModule.* does not currently contain any {}"".format(property_name)\n    with self.assertRaisesRegexp(ValueError, err):\n      getattr(mod, property_name)\n\n  @parameterized.parameters(""trainable_variables"", ""variables"")\n  def test_allow_empty_variables_instance(self, property_name):\n    mod = base.Module()\n    mod = base.allow_empty_variables(mod)\n    self.assertEmpty(getattr(mod, property_name))\n\n  @parameterized.parameters(""trainable_variables"", ""variables"")\n  def test_allow_empty_variables_class(self, property_name):\n    mod = NeverCreatesVariables()\n    self.assertEmpty(getattr(mod, property_name))\n\n\nclass NoopModule(base.Module):\n\n  def __init__(self, a=None):\n    super(NoopModule, self).__init__()\n    self.a = a\n\n\nclass RaisesOnEquality(object):\n\n  equality_checked = False\n\n  def __repr__(self):\n    return ""hostile""\n\n  def __eq__(self, other):\n    self.equality_checked = True\n    raise ValueError(""== not supported"")\n\n  def __ne__(self, other):\n    self.equality_checked = True\n    raise ValueError(""!= not supported"")\n\n\n@base.allow_empty_variables\nclass NeverCreatesVariables(base.Module):\n  pass\n\n\nclass ModuleWithFunctionAnnotatedCall(base.Module):\n\n  @tf.function(autograph=False)\n  def forward(self):\n    return get_name_scope()\n\n  @tf.function(autograph=True)\n  def forward_ag(self):\n    return get_name_scope()\n\n\nclass CtorNoNameScope(base.Module):\n\n  @base.no_name_scope\n  def __init__(self):\n    super(CtorNoNameScope, self).__init__()\n    self.ctor_name_scope = get_name_scope()\n    self.w = tf.Variable(1., name=""w"")\n\n\nclass CtorNoNameScopeNoSuper(base.Module):\n\n  @base.no_name_scope\n  def __init__(self):\n    pass\n\n\nclass PropertyModule(base.Module):\n\n  def __init__(self):\n    super(PropertyModule, self).__init__()\n    self._setter_scope_name = None\n\n  @property\n  def some_property(self):\n    getter_scope_name = get_name_scope()\n    return getter_scope_name, self._setter_scope_name\n\n  @some_property.setter\n  def some_property(self, my_property):\n    self._setter_scope_name = get_name_scope()\n\n  @property\n  @base.no_name_scope\n  def no_name_scope_property(self):\n    getter_scope_name = get_name_scope()\n    return getter_scope_name, self._setter_scope_name\n\n  @no_name_scope_property.setter\n  @base.no_name_scope\n  def no_name_scope_property(self, my_property):\n    self._setter_scope_name = get_name_scope()\n\n\nclass DoesNotCallSuperConstructorModule(base.Module):\n\n  def __init__(self):\n    # NOTE: Intentionally does not call super constructor.\n    pass\n\n\nclass CallsMethodBeforeSuperConstructorModule(base.Module):\n\n  def __init__(self, allowed_method):\n    if allowed_method:\n      self.no_name_scope()\n    else:\n      self.with_name_scope()\n    super(CallsMethodBeforeSuperConstructorModule, self).__init__()\n\n  @base.no_name_scope\n  def no_name_scope(self):\n    pass\n\n  def with_name_scope(self):\n    pass\n\n\nclass CustomMetaclass(type):\n\n  TAG = ""__custom_metaclass__""\n\n  def __new__(mcs, name, bases, clsdict):\n    new_type = super(CustomMetaclass, mcs).__new__(mcs, name, bases, clsdict)\n    setattr(new_type, CustomMetaclass.TAG, True)\n    return new_type\n\n\nclass CombiningMetaclass(base.ModuleMetaclass, CustomMetaclass):\n\n  TAG = ""__combining_metaclass__""\n\n  def __new__(mcs, name, bases, clsdict):\n    new_type = super(CombiningMetaclass, mcs).__new__(mcs, name, bases, clsdict)\n    setattr(new_type, CombiningMetaclass.TAG, True)\n    return new_type\n\n\n@six.add_metaclass(CombiningMetaclass)\nclass ModuleWithCustomMetaclass(base.Module):\n\n  def __init__(self):\n    super(ModuleWithCustomMetaclass, self).__init__()\n    self.init_name_scope = get_name_scope()\n\n\nclass CustomMetaclassTest(tf.test.TestCase):\n\n  def testSupportsCustomMetaclass(self):\n    m = ModuleWithCustomMetaclass()\n    self.assertEqual(m.init_name_scope, ""module_with_custom_metaclass/"")\n    self.assertTrue(getattr(ModuleWithCustomMetaclass, CombiningMetaclass.TAG))\n    self.assertTrue(getattr(ModuleWithCustomMetaclass, CustomMetaclass.TAG))\n\n\nclass TakesSubmodules(base.Module):\n\n  def __init__(self, submodules, name=None):\n    super(TakesSubmodules, self).__init__(name=name)\n\n\nclass WildcardInit(base.Module):\n\n  def __init__(self, a, b, *args, **kwargs):\n    super(WildcardInit, self).__init__()\n    del args, kwargs\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/batch_apply.py,24,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Merges a number of leading dimensions of an input tensor to manipulate it.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport numpy as np\nfrom sonnet.src import base\nimport tensorflow as tf\nimport tree\nfrom typing import Any, Callable, Optional, Sequence, Text, Union\n\n\nclass BatchApply(base.Module):\n  """"""Merges a number of leading dimensions of an input tensor to manipulate it.\n\n  Merges a number of leading dimensions of a tensor into a single dimension,\n  connects the provided module, then splits the leading dimension of the\n  result to match the input.\n\n  Input tensors whose rank is smaller than the number of dimensions to collapse\n  (e.g. all scalar values, which are tensors of rank 0), are passed unaltered to\n  the provided module.\n\n  This is useful for applying some module to each timestep of a Time x Batch x N\n  tensor. If a module is hard coded to only support 2D (Batch x N) then the\n  full 3D Tensor cannot be provided. BatchApply will \'merge\' the first two\n  dimensions of the sequence tensor by reshaping to a (Time * Batch) x N Tensor,\n  and then the internal module can be applied. The result of that operation is\n  reshaped such that its first dimensions are split to match the leading\n  dimensions of the input.\n  """"""\n\n  def __init__(self,\n               module: Callable[..., tf.Tensor],\n               num_dims: int = 2,\n               name: Optional[Text] = None):\n    super(BatchApply, self).__init__(name=name)\n    self.module = module\n    self.num_dims = num_dims\n\n  def __call__(self, *args, **kwargs):\n    example = first_leaf(args, kwargs)\n    if example is None:\n      raise ValueError(""BatchApply requires at least one tensor input."")\n\n    num_dims = self.num_dims\n    merge = lambda x: merge_leading_dims(x, num_dims=num_dims)\n    split = lambda x: split_leading_dim(x, num_dims=num_dims, example=example)\n\n    # Merge leading dimensions of inputs.\n    # Example: [T, B, N] -> [T*B, N]\n    args = tree.map_structure(merge, args)\n    kwargs = tree.map_structure(merge, kwargs)\n\n    # Compute merged output.\n    # Example: [T*B, O]\n    outputs = self.module(*args, **kwargs)\n\n    # Split leading dimensions of output to match input.\n    # Example: [T*B, O] -> [T, B, O]\n    return tree.map_structure(split, outputs)\n\n\ndef first_leaf(args, kwargs) -> Optional[Any]:\n  flat_args = tree.flatten(args)\n  if flat_args:\n    return flat_args[0]\n  flat_kwargs = tree.flatten(kwargs)\n  if flat_kwargs:\n    return flat_kwargs[0]\n  return None\n\n\ndef split_leading_dim(\n    x: Optional[tf.Tensor],\n    example: tf.Tensor,\n    num_dims: int,\n) -> Optional[tf.Tensor]:\n  """"""Split the first dimension of a tensor to match an example.\n\n  See :func:`merge_leading_dims`.\n\n  >>> x = tf.ones([6, 1])\n  >>> example = tf.ones([3, 2, 1])\n  >>> snt.split_leading_dim(x, example, 2)\n  <tf.Tensor: ...shape=(3, 2, 1), ...>\n\n  If ``x`` is not a :tf:`Tensor` or :tf:`Variable` then is is returned\n  unchanged:\n\n  >>> snt.split_leading_dim(\'not a tensor\', example, 2)\n  \'not a tensor\'\n\n  Args:\n    x: A tensor with leading dim merged.\n    example: An Tensor with leading dim not merged.\n    num_dims: The number of leading dimensions of example to use.\n\n  Returns:\n    A tensor with leading dim split, or the input unchanged.\n  """"""\n  if x is None or not isinstance(x, (tf.Tensor, tf.Variable)):\n    return x\n\n  static_shape = example.shape[:num_dims] + x.shape[1:]\n  if static_shape.is_fully_defined():  # pytype: disable=attribute-error\n    return tf.reshape(x, static_shape)\n\n  # Shape can\'t be inferred statically.\n  leading_dims = tf.shape(example)[:num_dims]\n  other_dims = tf.shape(x)[1:]\n  dynamic_shape = tf.concat([leading_dims, other_dims], axis=0)\n  return tf.reshape(x, dynamic_shape)\n\n\ndef maybe_prod(s: Sequence[Union[int, None]]) -> Optional[int]:\n  try:\n    return np.prod(s)\n  except TypeError:\n    # Can happen if the input contains `None`.\n    return None\n\n\ndef merge_leading_dims(\n    x: Optional[tf.Tensor],\n    num_dims: int,\n) -> Optional[tf.Tensor]:\n  """"""Merges leading dimensions of a tensor.\n\n  See :func:`split_leading_dim`.\n\n  >>> x = tf.ones([3, 2, 1])\n  >>> snt.merge_leading_dims(x, num_dims=2)\n  <tf.Tensor: ...shape=(6, 1), ...>\n\n  If the rank of ``x`` is less than ``num_dims`` it is returned unchanged:\n\n  >>> snt.merge_leading_dims(x, 4)\n  <tf.Tensor: ...shape=(3, 2, 1), ...>\n\n  If ``x`` is not a :tf:`Tensor` or :tf:`Variable` then is is returned\n  unchanged:\n\n  >>> snt.merge_leading_dims(\'not a tensor\', 1)\n  \'not a tensor\'\n\n  Args:\n    x: A :tf:`Tensor` to merge.\n    num_dims: The number of leading dimensions to merge.\n\n  Returns:\n    A :tf:`Tensor` with merged leading dimensions or the input unchanged.\n  """"""\n  if x is None or not isinstance(x, (tf.Tensor, tf.Variable)):\n    return x\n\n  # Check if the rank of the input tensor is well-defined.\n  if x.shape.dims is None:\n    raise ValueError(\n        ""Can\'t merge leading dimensions of tensor of unknown rank."")\n\n  # We can only merge the num_dims leading dimensions if the rank of the given\n  # tensor is sufficiently large.\n  if num_dims > x.shape.rank:\n    return x\n\n  static_shape = [maybe_prod(x.shape[:num_dims])] + x.shape[num_dims:]\n  if static_shape.is_fully_defined():  # pytype: disable=attribute-error\n    return tf.reshape(x, static_shape)\n\n  # Shape can\'t be inferred statically.\n  tensor_shape = tf.shape(x)\n  leading_dim = tf.reduce_prod(tensor_shape[:num_dims], keepdims=True)\n  other_dims = tensor_shape[num_dims:]\n  dynamic_shape = tf.concat([leading_dim, other_dims], axis=0)\n  result = tf.reshape(x, dynamic_shape)\n  # We lose some static shape information from the above reduce/slice/concat\n  # dance, so we explicitly pass it in from what we computed earlier.\n  result.set_shape(static_shape)\n  return result\n'"
sonnet/src/batch_apply_test.py,21,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.batch_apply.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom sonnet.src import base\nfrom sonnet.src import batch_apply\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\nEXAMPLE_INPUTS = (\n    ((1, 2, 3), 1),\n    ((1, 2, 3), 2),\n    ((1, 2, 3, 4), 3),\n    ((1, 2, 3, 4, 5, 6), 4),\n)\n\n\nclass BatchApplyTest(test_utils.TestCase):\n\n  def test_simple(self):\n    m = batch_apply.BatchApply(AddOne())\n    x = tf.zeros([2, 3, 4])\n    y = m(x)\n    self.assertAllEqual(y, tf.ones([2, 3, 4]))\n\n  def test_no_output(self):\n    m = batch_apply.BatchApply(NoOutputModule())\n    y = m(tf.ones([1, 1, 1]))\n    self.assertIsNone(y)\n\n  def test_kwargs(self):\n    m = batch_apply.BatchApply(KwargsModule())\n    y = m(tf.ones([1, 1, 1]), is_training=True)\n    self.assertIsNone(y)\n\n\nclass MergeLeadingDimsTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters(object(), (np.ones([]),), 1, None)\n  def test_x_not_tensor(self, x):\n    self.assertIs(x, batch_apply.merge_leading_dims(x, 1))\n\n  @parameterized.parameters(*EXAMPLE_INPUTS)\n  def test_static_shape(self, x_shape, num_dims):\n    x = tf.ones(x_shape)\n    y = batch_apply.merge_leading_dims(x, num_dims)\n    y_shape = (np.prod(x_shape[:num_dims]),) + x_shape[num_dims:]\n    self.assertEqual(y.shape, y_shape)\n\n  @parameterized.parameters(*EXAMPLE_INPUTS)\n  def test_dynamic_shape(self, x_shape, num_dims):\n    merge = tf.function(batch_apply.merge_leading_dims)\n\n    x = tf.TensorSpec([None for _ in x_shape])\n    cf = merge.get_concrete_function(x, num_dims)\n    y_shape = (np.prod(x_shape[:num_dims]),) + x_shape[num_dims:]\n    y_shape_dynamic = cf.output_shapes\n    y_shape_dynamic.assert_is_compatible_with(y_shape)\n\n    x = tf.ones(x_shape)\n    y = cf(x)\n    self.assertEqual(y.shape, y_shape)\n\n  @parameterized.parameters(*EXAMPLE_INPUTS)\n  def test_dynamic_shape_has_static_info_in_graph(self, x_shape, num_dims):\n    y_shape = (np.prod(x_shape[:num_dims]),) + x_shape[num_dims:]\n\n    @tf.function\n    def merge(x, num_dims):\n      y = batch_apply.merge_leading_dims(x, num_dims)\n      self.assertIsNotNone(y.shape.dims)\n      y.shape.assert_is_compatible_with(y_shape)\n      # Make sure we have static shape info except for the trailing None.\n      self.assertNotIn(None, y.shape[:-1])\n      self.assertIsNone(y.shape[-1])\n      return y\n\n    # Fill `None` in the last dimension (which won\'t be merged).\n    x = tf.TensorSpec(x_shape[:-1] + (None,))\n    cf = merge.get_concrete_function(x, num_dims)\n    y_shape_dynamic = cf.output_shapes\n    y_shape_dynamic.assert_is_compatible_with(y_shape)\n\n\nclass SplitLeadingDimTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters(object(), (np.ones([]),), 1, None)\n  def test_x_not_tensor(self, x):\n    self.assertIs(x, batch_apply.split_leading_dim(x, None, 1))\n\n  @parameterized.parameters(*EXAMPLE_INPUTS)\n  def test_static_shape(self, i_shape, num_dims):\n    x_shape = (np.prod(i_shape[:num_dims]),) + (2, 2)\n    y_shape = i_shape[:num_dims] + x_shape[1:]\n    x = tf.ones(x_shape)\n    i = tf.ones(i_shape)\n    y = batch_apply.split_leading_dim(x, i, num_dims)\n    self.assertEqual(y.shape, y_shape)\n\n  @parameterized.parameters(*EXAMPLE_INPUTS)\n  def test_dynamic_shape(self, i_shape, num_dims):\n    x_shape = (np.prod(i_shape[:num_dims]),) + (2, 2)\n    y_shape = i_shape[:num_dims] + x_shape[1:]\n\n    # Build a concrete function with fully dynamic input dimensions.\n    x = tf.TensorSpec([None for _ in x_shape])\n    i = tf.TensorSpec([None for _ in i_shape])\n    split = tf.function(batch_apply.split_leading_dim)\n    cf = split.get_concrete_function(x, i, num_dims)\n    y_shape_dynamic = cf.output_shapes\n    y_shape_dynamic.assert_is_compatible_with(y_shape)\n\n    # Make use of the concrete function with fully specified inputs.\n    x = tf.ones(x_shape)\n    i = tf.ones(i_shape)\n    y = cf(x, i)\n    self.assertEqual(y.shape, y_shape)\n\n  @parameterized.parameters(*EXAMPLE_INPUTS)\n  def test_dynamic_shape_has_static_info_in_graph(self, i_shape, num_dims):\n    x_shape = (np.prod(i_shape[:num_dims]),) + (2, 2)\n    y_shape = i_shape[:num_dims] + x_shape[1:]\n\n    @tf.function\n    def split(x, i, num_dims):\n      y = batch_apply.split_leading_dim(x, i, num_dims)\n      self.assertIsNotNone(y.shape.dims)\n      y.shape.assert_is_compatible_with(y_shape)\n      self.assertNotIn(None, y.shape[1:])\n      return y\n\n    # Build a concrete function with fully dynamic input dimensions.\n    x = tf.TensorSpec((None,) + x_shape[1:])\n    i = tf.TensorSpec((None,) + i_shape[1:])\n    cf = split.get_concrete_function(x, i, num_dims)\n    y_shape_dynamic = cf.output_shapes\n    y_shape_dynamic.assert_is_compatible_with(y_shape)\n\n\nclass NoOutputModule(base.Module):\n\n  def __call__(self, x):\n    return None\n\n\nclass KwargsModule(base.Module):\n\n  def __call__(self, x, is_training=None):\n    if is_training:\n      return None\n\n\nclass AddOne(base.Module):\n\n  def __call__(self, x):\n    assert len(x.shape) == 2, ""Requires rank 2 input.""\n    return x + 1.\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/batch_norm.py,19,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Batch normalization module.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nfrom sonnet.src import base\nfrom sonnet.src import initializers\nfrom sonnet.src import metrics\nfrom sonnet.src import moving_averages\nfrom sonnet.src import once\nfrom sonnet.src import types\nfrom sonnet.src import utils\n\nimport tensorflow as tf\nfrom typing import Optional, Text, Tuple\n\n\nclass BaseBatchNorm(base.Module):\n  r""""""Batch normalization module.\n\n  This implements normalization across the batch and spatial dimensions.\n  It maintains moving averages of the mean and variance which can be\n  used to normalize at test time. The constructor is generic and\n  requires the user to pass in objects to compute these.\n\n  At training time we use the batch statistics for that batch and these are then\n  used to update the moving averages.\n\n  At test time we can either use the moving averages of the batch statistics\n  (``test_local_stats=False``) or we can use the local statistics\n  (``test_local_stats=True``).\n\n  It transforms the input ``x`` into:\n\n  .. math::\n\n      \\d{outputs} = \\d{scale} \\dfrac{x - \\mu}{\\sigma + \\epsilon} + \\d{offset}\n\n  Where :math:`\\mu` and :math:`\\sigma` are respectively the mean and standard\n  deviation of ``x``. Note that this module automatically uses the fused batch\n  norm op if the data format is ``NHWC``.\n\n  There are many different variations for how users want to manage scale and\n  offset if they require them at all. These are:\n\n    - No scale/offset in which case ``create_*`` should be set to ``False`` and\n      ``scale``/``offset`` aren\'t passed when the module is called.\n    - Trainable scale/offset in which case ``create_*`` should be set to\n      ``True`` and again ``scale``/``offset`` aren\'t passed when the module is\n      called. In this case this module creates and owns the ``scale``/``offset``\n      variables.\n    - Externally generated ``scale``/``offset``, such as for conditional\n      normalization, in which case ``create_*`` should be set to ``False`` and\n      then the values fed in at call time.\n\n  Attributes:\n    scale: If ``create_scale``, a trainable :tf:`Variable` holding the current\n      scale after the module is connected for the first time.\n    offset: If ``create_offset``, a trainable :tf:`Variable` holding the current\n      offset after the module is connected for the first time.\n  """"""\n\n  def __init__(self,\n               create_scale: bool,\n               create_offset: bool,\n               moving_mean: metrics.Metric,\n               moving_variance: metrics.Metric,\n               eps: types.FloatLike = 1e-5,\n               scale_init: Optional[initializers.Initializer] = None,\n               offset_init: Optional[initializers.Initializer] = None,\n               data_format: Text = ""channels_last"",\n               name: Optional[Text] = None):\n    """"""Constructs a ``BaseBatchNorm`` module.\n\n    Args:\n      create_scale: whether to create a trainable scale per channel applied\n        after the normalization.\n      create_offset: whether to create a trainable offset per channel applied\n        after normalization and scaling.\n      moving_mean: A metric which tracks the moving average of the mean which\n        can be used to normalize at test time.\n      moving_variance: A metric which tracks the moving average of the variance\n        which can be used to normalize at test time.\n      eps: Small epsilon to avoid division by zero variance. Defaults to\n        ``1e-5``.\n      scale_init: Optional initializer for the scale variable. Can only be set\n        if ``create_scale=True``. By default scale is initialized to ``1``.\n      offset_init: Optional initializer for the offset variable. Can only be set\n        if ``create_offset=True``. By default offset is initialized to ``0``.\n      data_format: The data format of the input. Can be either\n        ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By\n        default it is ``channels_last``.\n      name: Name of the module.\n    """"""\n    super(BaseBatchNorm, self).__init__(name=name)\n\n    self._eps = eps\n\n    self.moving_mean = moving_mean\n    self.moving_variance = moving_variance\n\n    self._data_format = data_format\n    self._channel_index = utils.get_channel_index(data_format)\n\n    self._create_scale = create_scale\n    self._create_offset = create_offset\n\n    if not self._create_scale and scale_init is not None:\n      raise ValueError(""Cannot set `scale_init` if `create_scale=False`"")\n    self._scale_init = scale_init or initializers.Ones()\n    if not self._create_offset and offset_init is not None:\n      raise ValueError(""Cannot set `offset_init` if `create_offset=False`"")\n    self._offset_init = offset_init or initializers.Zeros()\n\n  @utils.smart_autograph\n  def __call__(self,\n               inputs: tf.Tensor,\n               is_training: types.BoolLike,\n               test_local_stats: types.BoolLike = False,\n               scale: Optional[tf.Tensor] = None,\n               offset: Optional[tf.Tensor] = None):\n    """"""Returns normalized inputs.\n\n    Args:\n      inputs: An n-D tensor of the data_format specified above on which the\n        transformation is performed.\n      is_training: Whether the module should be connected in training mode,\n        meaning the moving averages are updated.\n      test_local_stats: Whether local batch statistics should be used when\n        ``is_training=False``. If not, moving averages are used. By default\n        ``False``.\n      scale: A tensor up to n-D. The shape of this tensor must be broadcastable\n        to the shape of ``inputs``. This is the scale applied to the normalized\n        inputs. This cannot be passed in if the module was constructed with\n        ``create_scale=True``.\n      offset: A tensor up to n-D. The shape of this tensor must be broadcastable\n        to the shape of ``inputs``. This is the offset applied to the normalized\n        inputs. This cannot be passed in if the module was constructed with\n        ``create_offset=True``.\n\n    Returns:\n      An n-d tensor of the same shape as inputs that has been normalized.\n    """"""\n    use_batch_stats = is_training or test_local_stats\n    if self._create_scale:\n      if scale is not None:\n        raise ValueError(\n            ""Cannot pass `scale` at call time if `create_scale=True`."")\n\n    if self._create_offset:\n      if offset is not None:\n        raise ValueError(\n            ""Cannot pass `offset` at call time if `create_offset=True`."")\n\n    self._initialize(inputs)\n    if scale is None:\n      scale = self.scale\n    if offset is None:\n      offset = self.offset\n\n    mean, variance = self._moments(inputs, use_batch_stats)\n\n    if self._fused:\n      out, mean, variance, _, _ = tf.raw_ops.FusedBatchNormV2(\n          x=inputs,\n          mean=mean,\n          variance=variance,\n          scale=scale,\n          offset=offset,\n          is_training=use_batch_stats,\n          epsilon=self._eps,\n          data_format=self._fused_data_format)\n\n    else:\n      out = tf.nn.batch_normalization(\n          inputs,\n          mean=mean,\n          variance=variance,\n          scale=scale,\n          offset=offset,\n          variance_epsilon=self._eps)\n\n    if is_training:\n      self._update_statistics(mean, variance)\n\n    return out\n\n  @once.once\n  def _initialize(self, inputs: tf.Tensor):\n    input_shape = inputs.shape\n    rank = len(input_shape)\n    self._fused = (rank == 4 and self._channel_index == -1)\n    self._fused_data_format = ""NHWC"" if self._channel_index == -1 else ""NCHW""\n    if self._channel_index < 0:\n      channel_index = self._channel_index + rank\n    else:\n      channel_index = self._channel_index\n    self._axis = tuple(i for i in range(rank) if i != channel_index)\n\n    # Ensure all the variables are created on the first call\n    mean, variance = tf.nn.moments(inputs, self._axis, keepdims=True)\n    self.shape = mean.shape\n    self.moving_mean.initialize(mean)\n    self.moving_variance.initialize(variance)\n\n    dtype = inputs.dtype\n\n    if self._channel_index == -1:\n      params_shape = [inputs.shape[-1]]\n    else:  # self._channel_index == 1\n      params_shape = [inputs.shape[1]] + [1] * (rank - 2)\n    # Creates scale and offset parameters - required for fused_batch_norm\n    # trainable set to with_scale and with_offset which gives no-op if false\n    self.scale = tf.Variable(\n        self._scale_init(params_shape, dtype),\n        name=""scale"",\n        trainable=self._create_scale)\n\n    self.offset = tf.Variable(\n        self._offset_init(params_shape, dtype),\n        name=""offset"",\n        trainable=self._create_offset)\n\n    if self._fused:\n      with tf.init_scope():\n        self._fused_constant = tf.constant([])\n\n  def _moments(self, inputs: tf.Tensor,\n               use_batch_stats: types.BoolLike) -> Tuple[tf.Tensor, tf.Tensor]:\n    if use_batch_stats:\n      if self._fused:\n        # The raw ops version of fused batch norm calculates the mean and\n        # variance internally but requires tensors to be passed in.\n        mean = self._fused_constant\n        variance = self._fused_constant\n      else:\n        mean, variance = tf.nn.moments(inputs, self._axis, keepdims=True)\n    else:  # use moving stats\n      mean = self.moving_mean.value\n      variance = self.moving_variance.value\n      if self._fused:\n        mean = tf.squeeze(mean, self._axis)\n        variance = tf.squeeze(variance, self._axis)\n    return mean, variance\n\n  def _update_statistics(self, mean, variance):\n    if self._fused:\n      mean = tf.reshape(mean, self.shape)\n      variance = tf.reshape(variance, self.shape)\n    self.moving_mean.update(mean)\n    self.moving_variance.update(variance)\n\n\nclass BatchNorm(BaseBatchNorm):\n  """"""Batch normalization with exponential moving average for test statistics.\n\n  See :class:`BaseBatchNorm` for details.\n\n  Attributes:\n    scale: If ``create_scale=True``, a trainable :tf:`Variable` holding the\n      current scale after the module is connected for the first time.\n    offset: If ``create_offset``, a trainable :tf:`Variable` holding the current\n      offset after the module is connected for the first time.\n  """"""\n\n  def __init__(self,\n               create_scale: bool,\n               create_offset: bool,\n               decay_rate: float = 0.999,\n               eps: types.FloatLike = 1e-5,\n               scale_init: Optional[initializers.Initializer] = None,\n               offset_init: Optional[initializers.Initializer] = None,\n               data_format: Text = ""channels_last"",\n               name: Optional[Text] = None):\n    """"""Constructs a ``BatchNorm`` module.\n\n    Args:\n      create_scale: whether to create a trainable scale per channel applied\n        after the normalization.\n      create_offset: whether to create a trainable offset per channel applied\n        after normalization and scaling.\n      decay_rate: Decay rate of the exponential moving averages of the mean and\n        variance.\n      eps: Small epsilon to avoid division by zero variance. Defaults to\n        ``1e-5``.\n      scale_init: Optional initializer for the scale variable. Can only be set\n        if ``create_scale=True``. By default scale is initialized to ``1``.\n      offset_init: Optional initializer for the offset variable. Can only be set\n        if ``create_offset=True``. By default offset is initialized to ``0``.\n      data_format: The data format of the input. Can be either\n        ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By\n        default it is ``channels_last``.\n      name: Name of the module.\n    """"""\n    with tf.name_scope(name or ""batch_norm""):\n      moving_mean = moving_averages.ExponentialMovingAverage(\n          decay_rate, name=""moving_mean"")\n      moving_variance = moving_averages.ExponentialMovingAverage(\n          decay_rate, name=""moving_variance"")\n\n    super(BatchNorm, self).__init__(\n        create_scale=create_scale,\n        create_offset=create_offset,\n        moving_mean=moving_mean,\n        moving_variance=moving_variance,\n        eps=eps,\n        scale_init=scale_init,\n        offset_init=offset_init,\n        data_format=data_format,\n        name=name)\n'"
sonnet/src/batch_norm_test.py,45,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.batch_norm.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom sonnet.src import batch_norm\nfrom sonnet.src import initializers\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\n\nclass BaseBatchNormTest(test_utils.TestCase, parameterized.TestCase):\n\n  def testSimpleTraining(self):\n    layer = batch_norm.BaseBatchNorm(\n        moving_mean=TestMetric(),\n        moving_variance=TestMetric(),\n        create_scale=False,\n        create_offset=False)\n\n    inputs = tf.ones([2, 3, 3, 5])\n    scale = tf.constant(0.5, shape=(5,))\n    offset = tf.constant(2.0, shape=(5,))\n\n    outputs = layer(inputs, True, scale=scale, offset=offset).numpy()\n    self.assertAllEqual(outputs, tf.fill(inputs.shape, 2.0))\n    self.assertEqual((0, 1, 2), layer._axis)\n\n  def testSimpleTrainingNCHW(self):\n    layer = batch_norm.BaseBatchNorm(\n        moving_mean=TestMetric(),\n        moving_variance=TestMetric(),\n        create_scale=False,\n        create_offset=False,\n        data_format=""NCHW"")\n\n    inputs = tf.ones([2, 5, 3, 3])\n    scale = tf.constant(0.5, shape=(5, 1, 1))\n    offset = tf.constant(2.0, shape=(5, 1, 1))\n\n    outputs = layer(inputs, True, scale=scale, offset=offset).numpy()\n    self.assertAllEqual(outputs, tf.fill(inputs.shape, 2.0))\n    self.assertEqual((0, 2, 3), layer._axis)\n\n  def testSimpleTraining3D(self):\n    layer = batch_norm.BaseBatchNorm(\n        moving_mean=TestMetric(),\n        moving_variance=TestMetric(),\n        create_scale=False,\n        create_offset=False)\n\n    inputs = tf.ones([2, 3, 3, 3, 5])\n    scale = tf.constant(0.5, shape=(5,))\n    offset = tf.constant(2.0, shape=(5,))\n\n    outputs = layer(inputs, True, scale=scale, offset=offset).numpy()\n    self.assertAllEqual(outputs, tf.fill(inputs.shape, 2.0))\n    self.assertEqual((0, 1, 2, 3), layer._axis)\n\n  def testSimpleTraining3DNCDHW(self):\n    layer = batch_norm.BaseBatchNorm(\n        moving_mean=TestMetric(),\n        moving_variance=TestMetric(),\n        create_scale=False,\n        create_offset=False,\n        data_format=""NCDHW"")\n\n    inputs = tf.ones([2, 5, 3, 3, 3])\n    scale = tf.constant(0.5, shape=(5, 1, 1, 1))\n    offset = tf.constant(2.0, shape=(5, 1, 1, 1))\n\n    outputs = layer(inputs, True, scale=scale, offset=offset).numpy()\n    self.assertAllEqual(outputs, tf.fill(inputs.shape, 2.0))\n    self.assertEqual((0, 2, 3, 4), layer._axis)\n\n  def testNoScaleAndOffset(self):\n    layer = batch_norm.BaseBatchNorm(\n        moving_mean=TestMetric(),\n        moving_variance=TestMetric(),\n        create_scale=False,\n        create_offset=False,\n        data_format=""NHWC"")\n\n    inputs = tf.ones([2, 5, 3, 3, 3])\n    outputs = layer(inputs, True)\n    self.assertAllEqual(outputs, tf.zeros_like(inputs))\n\n  def testSingleBatchInference(self):\n    layer = batch_norm.BaseBatchNorm(\n        moving_mean=TestMetric(),\n        moving_variance=TestMetric(),\n        create_scale=True,\n        create_offset=True)\n    inputs = tf.ones([1, 1, 1, 1])\n    outputs = layer(inputs, False)\n    self.assertAllEqual(outputs, tf.zeros_like(inputs))\n\n  @parameterized.parameters(True, False)\n  def testWithTfFunction(self, autograph):\n    if ""TPU"" in self.device_types:\n      self.skipTest(""Test not working on TPU"")\n      # TODO(tamaranorman) enable on TPU\n\n    layer = batch_norm.BaseBatchNorm(\n        moving_mean=TestMetric(),\n        moving_variance=TestMetric(),\n        create_scale=False,\n        create_offset=False,\n        data_format=""NHWC"")\n    layer = tf.function(layer, autograph=autograph)\n\n    inputs = tf.ones([2, 5, 3, 3, 3])\n    scale = tf.constant(0.5, shape=(5, 1, 1, 1))\n    offset = tf.constant(2.0, shape=(5, 1, 1, 1))\n    expected1 = tf.zeros_like(inputs)\n    expected2 = tf.fill(inputs.shape, 2.0)\n\n    for is_training, use_batch_stats in itertools.product((True, False),\n                                                          (True, False)):\n      outputs = layer(inputs, is_training, use_batch_stats)\n      self.assertAllEqual(outputs, expected1)\n\n      outputs = layer(\n          inputs, is_training, use_batch_stats, scale=scale, offset=offset)\n      self.assertAllEqual(outputs, expected2)\n\n  @parameterized.parameters(True, False)\n  def testWithTfFunctionTfArgs(self, autograph):\n    if ""TPU"" in self.device_types:\n      self.skipTest(""Test not working on TPU"")\n      # TODO(tamaranorman) enable on TPU\n\n    layer = batch_norm.BaseBatchNorm(\n        moving_mean=TestMetric(),\n        moving_variance=TestMetric(),\n        create_scale=False,\n        create_offset=False,\n        data_format=""NHWC"")\n    layer = tf.function(layer, autograph=autograph)\n\n    inputs = tf.ones([2, 5, 3, 3, 3])\n    expected = tf.zeros_like(inputs)\n\n    for is_training, use_batch_stats in itertools.product((True, False),\n                                                          (True, False)):\n      # NOTE: The use of `tf.constant` means we require graph control flow\n      outputs = layer(inputs, tf.constant(is_training),\n                      tf.constant(use_batch_stats))\n      self.assertAllEqual(outputs, expected)\n\n  def testUsingTestStats(self):\n    layer = batch_norm.BaseBatchNorm(\n        moving_mean=TestMetric(),\n        moving_variance=TestMetric(),\n        create_scale=False,\n        create_offset=False)\n\n    inputs = tf.ones([2, 3, 3, 5])\n    scale = tf.constant(0.5, shape=(5,))\n    offset = tf.constant(2.0, shape=(5,))\n\n    outputs = layer(inputs, True, scale=scale, offset=offset).numpy()\n    self.assertAllEqual(outputs, tf.fill(inputs.shape, 2.0))\n    outputs = layer(inputs, False, scale=scale, offset=offset).numpy()\n    for x in np.nditer(outputs):\n      self.assertAllClose(x, 2.0, rtol=1e-5, atol=1e-3)\n\n  def testIsTrainingFalseFirstCall(self):\n    layer = batch_norm.BaseBatchNorm(\n        moving_mean=TestMetric(),\n        moving_variance=TestMetric(),\n        create_scale=False,\n        create_offset=False)\n    inputs = tf.ones([2, 3, 3, 5])\n    outputs = layer(inputs, False)\n    self.assertAllEqual(outputs, tf.fill(inputs.shape, 0.0))\n\n  @parameterized.parameters(""NHW"", ""HWC"", ""channel_last"")\n  def testInvalidDataFormat(self, data_format):\n    with self.assertRaisesRegexp(\n        ValueError,\n        ""Unable to extract channel information from \'{}\'"".format(data_format)):\n      batch_norm.BaseBatchNorm(\n          moving_mean=TestMetric(),\n          moving_variance=TestMetric(),\n          create_scale=False,\n          create_offset=False,\n          data_format=data_format)\n\n  @parameterized.parameters(""NCHW"", ""NCW"", ""channels_first"")\n  def testValidDataFormatChannelsFirst(self, data_format):\n    test = batch_norm.BaseBatchNorm(\n        moving_mean=TestMetric(),\n        moving_variance=TestMetric(),\n        create_scale=False,\n        create_offset=False,\n        data_format=data_format)\n\n    self.assertEqual(test._channel_index, 1)\n\n  @parameterized.parameters(""NHWC"", ""NWC"", ""channels_last"")\n  def testValidDataFormatChannelsLast(self, data_format):\n    test = batch_norm.BaseBatchNorm(\n        moving_mean=TestMetric(),\n        moving_variance=TestMetric(),\n        create_scale=False,\n        create_offset=False,\n        data_format=data_format)\n\n    self.assertEqual(test._channel_index, -1)\n\n  def testNoScaleAndInitProvided(self):\n    with self.assertRaisesRegexp(\n        ValueError, ""Cannot set `scale_init` if `create_scale=False`""):\n      batch_norm.BaseBatchNorm(\n          moving_mean=TestMetric(),\n          moving_variance=TestMetric(),\n          create_scale=False,\n          create_offset=True,\n          scale_init=initializers.Ones())\n\n  def testNoOffsetBetaInitProvided(self):\n    with self.assertRaisesRegexp(\n        ValueError, ""Cannot set `offset_init` if `create_offset=False`""):\n      batch_norm.BaseBatchNorm(\n          moving_mean=TestMetric(),\n          moving_variance=TestMetric(),\n          create_scale=True,\n          create_offset=False,\n          offset_init=initializers.Zeros())\n\n\nclass BatchNormTest(test_utils.TestCase, parameterized.TestCase):\n\n  def testSimple(self):\n    layer = batch_norm.BatchNorm(False, False)\n\n    inputs = tf.ones([2, 3, 3, 5])\n    scale = tf.constant(0.5, shape=(5,))\n    offset = tf.constant(2.0, shape=(5,))\n\n    outputs = layer(inputs, True, scale=scale, offset=offset).numpy()\n    self.assertAllEqual(outputs, tf.fill(inputs.shape, 2.0))\n\n\nclass TestMetric(object):\n\n  def __init__(self):\n    self._foo = None\n    self._built = False\n\n  def update(self, x):\n    if self._built:\n      self._foo.assign(x)\n    else:\n      self._foo = tf.Variable(x)\n      self._built = True\n\n  @property\n  def value(self):\n    return self._foo\n\n  def initialize(self, x):\n    self._foo = tf.Variable(x)\n    self._built = True\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/bias.py,4,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Bias module.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nfrom sonnet.src import base\nfrom sonnet.src import initializers\nfrom sonnet.src import once\nfrom sonnet.src import types\nfrom sonnet.src import utils\n\nimport tensorflow as tf\nfrom typing import Optional, Sequence, Text\n\n\nclass Bias(base.Module):\n  """"""Bias module.\n\n  Example Usage:\n\n      >>> N, H, W, C = 1, 2, 3, 4\n      >>> x = tf.random.normal([N, H, W, C])\n\n      >>> scalar_bias = snt.Bias(bias_dims=[])\n      >>> scalar_bias_output = scalar_bias(x)\n      >>> assert scalar_bias.b.shape == []\n\n  Create a bias over all non-minibatch dimensions:\n\n      >>> all_bias = snt.Bias()\n      >>> all_bias_output = all_bias(x)\n      >>> assert all_bias.b.shape == [H, W, C]\n\n  Create a bias over the last non-minibatch dimension:\n\n      >>> last_bias = snt.Bias(bias_dims=[-1])\n      >>> last_bias_output = last_bias(x)\n      >>> assert last_bias.b.shape == [C]\n\n  Create a bias over the first non-minibatch dimension:\n\n      >>> first_bias = snt.Bias(bias_dims=[1])\n      >>> first_bias_output = first_bias(x)\n      >>> assert first_bias.b.shape == [H, 1, 1]\n\n  Subtract and later add the same learned bias:\n\n      >>> bias = snt.Bias()\n      >>> h1 = bias(x, multiplier=-1)\n      >>> h2 = bias(x)\n      >>> h3 = bias(x, multiplier=-1)\n      >>> reconstructed_x = bias(h3)\n      >>> assert tf.reduce_all(tf.equal(x, reconstructed_x))\n  """"""\n\n  def __init__(self,\n               output_size: Optional[int] = None,\n               bias_dims: Optional[Sequence[int]] = None,\n               b_init: Optional[initializers.Initializer] = None,\n               name: Optional[Text] = None):\n    """"""Constructs a `Bias` module that supports broadcasting.\n\n    Args:\n      output_size: Output size (output shape without batch dimension). If\n        `output_size` is left as `None`, the size will be directly inferred by\n        the input.\n      bias_dims: Sequence of which dimensions to retain from the input shape\n        when constructing the bias. The remaining dimensions will be broadcast\n        over (given size of 1), and leading dimensions will be removed\n        completely. See class doc for examples.\n      b_init: Optional initializer for the bias. Default to zeros.\n      name: Name of the module.\n    """"""\n    super(Bias, self).__init__(name=name)\n    self.output_size = output_size\n    self.bias_dims = bias_dims\n    self.b_init = initializers.Zeros() if b_init is None else b_init\n\n  @once.once\n  def _initialize(self, inputs):\n    utils.assert_minimum_rank(inputs, 2)\n\n    input_shape = inputs.shape\n    bias_shape = calculate_bias_shape(input_shape, self.bias_dims)\n\n    input_size = input_shape[1:]\n    if self.output_size is not None:\n      if self.output_size != input_size:\n        raise ValueError(""Input shape must be {} not {}"".format(\n            (-1,) + self.output_size, input_shape))\n\n    self.input_size = input_size\n    self.b = tf.Variable(self.b_init(bias_shape, inputs.dtype), name=""b"")\n\n  def __call__(self, inputs: tf.Tensor, multiplier: types.FloatLike = None):\n    """"""Adds bias to `inputs` and optionally multiplies by `multiplier`.\n\n    Args:\n      inputs: A Tensor of size `[batch_size, input_size1, ...]`.\n      multiplier: A scalar or Tensor which the bias term is multiplied by before\n        adding it to `inputs`. Anything which works in the expression `bias *\n        multiplier` is acceptable here. This may be useful if you want to add a\n        bias in one place and subtract the same bias in another place via\n        `multiplier=-1`.\n\n    Returns:\n      A Tensor of size `[batch_size, input_size1, ...]`.\n    """"""\n    self._initialize(inputs)\n    if multiplier is not None:\n      return inputs + (self.b * multiplier)\n    else:\n      return inputs + self.b\n\n\ndef calculate_bias_shape(input_shape: types.ShapeLike,\n                         bias_dims: Sequence[int]):\n  """"""Calculate `bias_shape` based on the `input_shape` and `bias_dims`.\n\n  Args:\n    input_shape: Shape of the input being passed into the module. The leading\n      dimension is the mini-batch size.\n    bias_dims: The dimensions that bias should be applied over. The remaining\n      dimensions will be broadcast over.\n\n  Returns:\n    bias_shape: Tuple corresponding to the shape of bias Variable to create.\n\n  Raises:\n    ValueError: If the user attempts to add bias over the mini-batch dimension,\n        e.g. `bias_dims=[0]`.\n  """"""\n  input_rank = len(input_shape)\n  if bias_dims is None:\n    # If None, default is to use all dimensions.\n    return input_shape[1:]\n\n  elif not bias_dims:\n    # If empty list, use a scalar bias.\n    return ()\n\n  else:\n    # Otherwise, calculate bias_shape from bias_dims.\n    bias_shape = [1] * input_rank\n    # Populate bias dimensions.\n    for dim in bias_dims:\n      if dim < 0:\n        dim %= input_rank\n\n      if dim == 0:\n        raise ValueError(""Cannot apply bias across the minibatch dimension."")\n      elif dim >= input_rank:\n        raise ValueError(\n            ""Dimension %d (bias_dims=%r) out of range for input of rank %r."" %\n            (dim, tuple(bias_dims), input_rank))\n\n      bias_shape[dim] = input_shape[dim]\n    # Strip leading unit dimensions.\n    start = input_rank\n    for dim in range(1, input_rank):\n      if bias_shape[dim] != 1:\n        start = dim\n        break\n    return tuple(bias_shape[start:])  # Do not apply across minibatch dimension.\n'"
sonnet/src/bias_test.py,16,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.bias.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom sonnet.src import bias\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\n\nclass BiasTest(test_utils.TestCase):\n\n  def test_output_shape(self):\n    mod = bias.Bias(output_size=(2 * 2,))\n    with self.assertRaisesRegexp(ValueError, ""Input shape must be [(]-1, 4[)]""):\n      mod(tf.ones([2, 2, 2]))\n\n  def test_output_size_valid(self):\n    mod = bias.Bias(output_size=(2 * 2,))\n    mod(tf.ones([2, 2 * 2]))\n\n  def test_bias_dims_scalar(self):\n    mod = bias.Bias(bias_dims=())\n    mod(tf.ones([1, 2, 3, 4]))\n    self.assertEmpty(mod.b.shape)\n\n  def test_bias_dims_custom(self):\n    b, d1, d2, d3 = range(1, 5)\n    mod = bias.Bias(bias_dims=[1, 3])\n    out = mod(tf.ones([b, d1, d2, d3]))\n    self.assertEqual(mod.b.shape, [d1, 1, d3])\n    self.assertEqual(out.shape, [b, d1, d2, d3])\n\n  def test_bias_dims_negative_out_of_order(self):\n    mod = bias.Bias(bias_dims=[-1, -2])\n    mod(tf.ones([1, 2, 3]))\n    self.assertEqual(mod.b.shape, [2, 3])\n\n  def test_bias_dims_invalid(self):\n    mod = bias.Bias(bias_dims=[1, 5])\n    with self.assertRaisesRegexp(ValueError,\n                                 ""5 .* out of range for input of rank 3""):\n      mod(tf.ones([1, 2, 3]))\n\n  def test_b_init_defaults_to_zeros(self):\n    mod = bias.Bias()\n    mod(tf.ones([1, 1]))\n    self.assertAllEqual(mod.b.read_value(), tf.zeros_like(mod.b))\n\n  def test_b_init_custom(self):\n    ones_initializer = lambda s, d: tf.ones(s, dtype=d)\n    mod = bias.Bias(b_init=ones_initializer)\n    mod(tf.ones([1, 1]))\n    self.assertAllEqual(mod.b.read_value(), tf.ones_like(mod.b))\n\n  def test_name(self):\n    mod = bias.Bias(name=""foo"")\n    self.assertEqual(mod.name, ""foo"")\n    mod(tf.ones([1, 1]))\n    self.assertEqual(mod.b.name, ""foo/b:0"")\n\n  def test_multiplier(self):\n    ones_initializer = lambda s, d: tf.ones(s, dtype=d)\n    mod = bias.Bias(b_init=ones_initializer)\n    out = mod(tf.ones([1, 1]), multiplier=-1)\n    self.assertAllEqual(tf.reduce_sum(out), 0)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/build.py,8,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Utility function to build Sonnet modules.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tree\nfrom typing import Any, Callable\n\n\ndef _int_or_none(o):\n  return isinstance(o, (int, type(None)))\n\n\ndef _promote_shapes(o):\n  """"""Promotes lists of ints/Nones to :tf:`TensorSpec` instances.""""""\n  if isinstance(o, (list, tuple)) and all(_int_or_none(e) for e in o):\n    return tf.TensorSpec(o)\n  return o\n\n\ndef _maybe_tensor_spec(shape, dtype):\n  return tf.TensorSpec(shape, dtype) if dtype is not None else None\n\n\n# TODO(tomhennigan) Use TensorNest in types here.\ndef build(\n    f: Callable[..., Any],\n    *args,\n    **kwargs\n):\n  r""""""Builds a module by creating all parameters but not computing any output.\n\n      >>> mod = snt.nets.MLP([1000, 10])\n      >>> snt.build(mod, [None, 28 * 28])\n      TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n      >>> mod.variables\n      (<tf.Variable \'mlp/linear_0/b:0\' shape=(1000,) ...>,\n       <tf.Variable \'mlp/linear_0/w:0\' shape=(784, 1000) ...>,\n       <tf.Variable \'mlp/linear_1/b:0\' shape=(10,) ...>,\n       <tf.Variable \'mlp/linear_1/w:0\' shape=(1000, 10) ...>)\n\n  Args:\n    f: A function or callable :class:`Module` that will create variables.\n    *args: Positional arguments to supply to ``f``. Note that positional\n      arguments that are sequences of None/ints are converted to\n      :tf:`TensorSpec` instances.\n    **kwargs: Keyword arguments to pass to the module.\n\n  Returns:\n    The output of ``f`` with any :tf:`Tensor`\\ s replaced by :tf:`TensorSpec`.\n  """"""\n  f = tf.function(f)\n  args = map(_promote_shapes, args)\n  # NOTE: We use a concrete function to ensure that weights are created and\n  # initialized, but other stateful ops (e.g. updating weights) are not.\n  cf = f.get_concrete_function(*args, **kwargs)\n  return tree.map_structure(_maybe_tensor_spec, cf.output_shapes,\n                            cf.output_dtypes)\n'"
sonnet/src/build_test.py,8,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.build.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom sonnet.src import build\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\n\nclass BuildTest(test_utils.TestCase):\n\n  def test_call_with_shape_lke_object(self):\n    output_spec = build.build(tensor_identity, [1, None, 3])\n    self.assertEqual(output_spec, tf.TensorSpec([1, None, 3]))\n\n  def test_output_spec(self):\n    dtype = tf.float32 if self.primary_device == ""TPU"" else tf.float16\n    inputs = {""foo"": [tf.ones([], dtype), None]}\n    output_spec = build.build(lambda x: x, inputs)\n    self.assertEqual(output_spec,\n                     {""foo"": [tf.TensorSpec([], dtype), None]})\n\n  def test_does_not_trigger_sideeffects(self):\n    mod = IncrementsCounter()\n    output_spec = build.build(mod)\n    self.assertIsNone(output_spec)\n    self.assertEqual(mod.counter.numpy(), 0)\n\n\ndef tensor_identity(x):\n  assert isinstance(x, tf.Tensor)\n  return x\n\n\nclass IncrementsCounter(tf.Module):\n\n  def __call__(self):\n    if not hasattr(self, ""counter""):\n      self.counter = tf.Variable(0)\n    self.counter.assign_add(1)\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/conv.py,8,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Convolutional modules.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport numpy as np\nimport six\n\nfrom sonnet.src import base\nfrom sonnet.src import initializers\nfrom sonnet.src import once\nfrom sonnet.src import pad\nfrom sonnet.src import utils\nimport tensorflow as tf\nfrom typing import Optional, Sequence, Text, Union\n\n\nclass ConvND(base.Module):\n  """"""A general N-dimensional convolutional module.""""""\n\n  def __init__(self,\n               num_spatial_dims: int,\n               output_channels: int,\n               kernel_shape: Union[int, Sequence[int]],\n               stride: Union[int, Sequence[int]] = 1,\n               rate: Union[int, Sequence[int]] = 1,\n               padding: Union[Text, pad.Paddings] = ""SAME"",\n               with_bias: bool = True,\n               w_init: Optional[initializers.Initializer] = None,\n               b_init: Optional[initializers.Initializer] = None,\n               data_format: Optional[Text] = None,\n               name: Optional[Text] = None):\n    """"""Constructs a `ConvND` module.\n\n    Args:\n      num_spatial_dims: The number of spatial dimensions of the input.\n      output_channels: The number of output channels.\n      kernel_shape: Sequence of kernel sizes (of length num_spatial_dims), or an\n        integer. `kernel_shape` will be expanded to define a kernel size in all\n        dimensions.\n      stride: Sequence of strides (of length num_spatial_dims), or an integer.\n        `stride` will be expanded to define stride in all dimensions.\n      rate: Sequence of dilation rates (of length num_spatial_dims), or integer\n        that is used to define dilation rate in all dimensions. 1 corresponds to\n        standard ND convolution, `rate > 1` corresponds to dilated convolution.\n      padding: Padding to apply to the input. This can either ""SAME"", ""VALID"" or\n        a callable or sequence of callables up to size N. Any callables must\n        take a single integer argument equal to the effective kernel size and\n        return a list of two integers representing the padding before and after.\n        See snt.pad.* for more details and example functions.\n      with_bias: Whether to include bias parameters. Default `True`.\n      w_init: Optional initializer for the weights. By default the weights are\n        initialized truncated random normal values with a standard deviation of\n        `1 / sqrt(input_feature_size)`, which is commonly used when the inputs\n        are zero centered (see https://arxiv.org/abs/1502.03167v3).\n      b_init: Optional initializer for the bias. By default the bias is\n        initialized to zero.\n      data_format: The data format of the input.\n      name: Name of the module.\n    """"""\n    super(ConvND, self).__init__(name=name)\n\n    if not 1 <= num_spatial_dims <= 3:\n      raise ValueError(\n          ""We only support convoltion operations for num_spatial_dims=1, 2 or ""\n          ""3, received num_spatial_dims={}."".format(num_spatial_dims))\n    self._num_spatial_dims = num_spatial_dims\n    self.output_channels = output_channels\n    self.kernel_shape = kernel_shape\n    self.stride = stride\n    self.rate = rate\n\n    if isinstance(padding, six.string_types):\n      self.conv_padding = padding.upper()\n      self.padding_func = None\n    else:\n      self.conv_padding = ""VALID""\n      self.padding_func = padding\n\n    self.data_format = data_format\n    self._channel_index = utils.get_channel_index(data_format)\n    self.with_bias = with_bias\n\n    self.w_init = w_init\n    if with_bias:\n      self.b_init = b_init if b_init is not None else initializers.Zeros()\n    elif b_init is not None:\n      raise ValueError(""When not using a bias the b_init must be None."")\n\n  def __call__(self, inputs: tf.Tensor) -> tf.Tensor:\n    """"""Applies the defined convolution to the inputs.\n\n    Args:\n      inputs: An ``N + 2`` rank :tf:`Tensor` of dtype :tf:`float16`,\n        :tf:`bfloat16` or `tf.float32` to which the convolution is applied.\n\n    Returns:\n      An ``N + 2`` dimensional :tf:`Tensor` of shape\n        ``[batch_size, output_dim_1, output_dim_2, ..., output_channels]``.\n    """"""\n    self._initialize(inputs)\n\n    if self.padding_func:\n      inputs = tf.pad(inputs, self._padding)\n\n    outputs = tf.nn.convolution(\n        inputs,\n        self.w,\n        strides=self.stride,\n        padding=self.conv_padding,\n        dilations=self.rate,\n        data_format=self.data_format)\n    if self.with_bias:\n      outputs = tf.nn.bias_add(outputs, self.b, data_format=self.data_format)\n\n    return outputs\n\n  @once.once\n  def _initialize(self, inputs: tf.Tensor):\n    """"""Constructs parameters used by this module.""""""\n    utils.assert_rank(inputs, self._num_spatial_dims + 2)\n    self.input_channels = inputs.shape[self._channel_index]\n    if self.input_channels is None:\n      raise ValueError(""The number of input channels must be known."")\n    self._dtype = inputs.dtype\n\n    self.w = self._make_w()\n    if self.with_bias:\n      self.b = tf.Variable(\n          self.b_init((self.output_channels,), self._dtype), name=""b"")\n\n    if self.padding_func:\n      self._padding = pad.create(\n          padding=self.padding_func,\n          kernel=self.kernel_shape,\n          rate=self.rate,\n          n=self._num_spatial_dims,\n          channel_index=self._channel_index)\n\n  def _make_w(self):\n    weight_shape = utils.replicate(self.kernel_shape, self._num_spatial_dims,\n                                   ""kernel_shape"")\n    weight_shape = weight_shape + (self.input_channels, self.output_channels)\n\n    if self.w_init is None:\n      # See https://arxiv.org/abs/1502.03167v3.\n      fan_in_shape = weight_shape[:-1]\n      stddev = 1 / np.sqrt(np.prod(fan_in_shape))\n      self.w_init = initializers.TruncatedNormal(stddev=stddev)\n\n    return tf.Variable(self.w_init(weight_shape, self._dtype), name=""w"")\n\n\nclass Conv1D(ConvND):\n  """"""``Conv1D`` module.""""""\n\n  def __init__(self,\n               output_channels: int,\n               kernel_shape: Union[int, Sequence[int]],\n               stride: Union[int, Sequence[int]] = 1,\n               rate: Union[int, Sequence[int]] = 1,\n               padding: Union[Text, pad.Paddings] = ""SAME"",\n               with_bias: bool = True,\n               w_init: Optional[initializers.Initializer] = None,\n               b_init: Optional[initializers.Initializer] = None,\n               data_format: Text = ""NWC"",\n               name: Optional[Text] = None):\n    """"""Constructs a ``Conv1D`` module.\n\n    Args:\n      output_channels: The number of output channels.\n      kernel_shape: Sequence of length 1, or an integer. ``kernel_shape`` will\n        be expanded to define a kernel size in all dimensions.\n      stride: Sequence of strides of length 1, or an integer. ``stride`` will be\n        expanded to define stride in all dimensions.\n      rate: Sequence of dilation rates of length 1, or integer that is used to\n        define dilation rate in all dimensions. 1 corresponds to standard\n        convolution, ``rate > 1`` corresponds to dilated convolution.\n      padding: Padding to apply to the input. This can be either ``SAME``,\n        ``VALID`` or a callable or sequence of callables of size 1. Any\n        callables must take a single integer argument equal to the effective\n        kernel size and return a list of two integers representing the padding\n        before and after. See snt.pad.* for more details and example functions.\n      with_bias: Whether to include bias parameters. Default ``True``.\n      w_init: Optional initializer for the weights. By default the weights are\n        initialized truncated random normal values with a standard deviation of\n        ``1``/``sqrt(input_feature_size)``, which is commonly used when the\n        inputs are zero centered (see https://arxiv.org/abs/1502.03167v3).\n      b_init: Optional initializer for the bias. By default the bias is\n        initialized to zero.\n      data_format: The data format of the input.\n      name: Name of the module.\n    """"""\n    super(Conv1D, self).__init__(\n        num_spatial_dims=1,\n        output_channels=output_channels,\n        kernel_shape=kernel_shape,\n        stride=stride,\n        rate=rate,\n        padding=padding,\n        with_bias=with_bias,\n        w_init=w_init,\n        b_init=b_init,\n        data_format=data_format,\n        name=name)\n\n\nclass Conv2D(ConvND):\n  """"""`Conv2D` module.""""""\n\n  def __init__(self,\n               output_channels: int,\n               kernel_shape: Union[int, Sequence[int]],\n               stride: Union[int, Sequence[int]] = 1,\n               rate: Union[int, Sequence[int]] = 1,\n               padding: Union[Text, pad.Paddings] = ""SAME"",\n               with_bias: bool = True,\n               w_init: Optional[initializers.Initializer] = None,\n               b_init: Optional[initializers.Initializer] = None,\n               data_format: Text = ""NHWC"",\n               name: Optional[Text] = None):\n    """"""Constructs a ``Conv2D`` module.\n\n    Args:\n      output_channels: The number of output channels.\n      kernel_shape: Sequence of kernel sizes (of length 2), or an integer.\n        ``kernel_shape`` will be expanded to define a kernel size in all\n        dimensions.\n      stride: Sequence of strides (of length 2), or an integer. ``stride`` will\n        be expanded to define stride in all dimensions.\n      rate: Sequence of dilation rates (of length 2), or integer that is used to\n        define dilation rate in all dimensions. 1 corresponds to standard\n        convolution, ``rate > 1`` corresponds to dilated convolution.\n      padding: Padding to apply to the input. This can either ``SAME``,\n        ``VALID`` or a callable or sequence of callables of size 2. Any\n        callables must take a single integer argument equal to the effective\n        kernel size and return a list of two integers representing the padding\n        before and after. See snt.pad.* for more details and example functions.\n      with_bias: Whether to include bias parameters. Default ``True``.\n      w_init: Optional initializer for the weights. By default the weights are\n        initialized truncated random normal values with a standard deviation of\n        ``1 / sqrt(input_feature_size)``, which is commonly used when the inputs\n        are zero centered (see https://arxiv.org/abs/1502.03167v3).\n      b_init: Optional initializer for the bias. By default the bias is\n        initialized to zero.\n      data_format: The data format of the input.\n      name: Name of the module.\n    """"""\n    super(Conv2D, self).__init__(\n        num_spatial_dims=2,\n        output_channels=output_channels,\n        kernel_shape=kernel_shape,\n        stride=stride,\n        rate=rate,\n        padding=padding,\n        with_bias=with_bias,\n        w_init=w_init,\n        b_init=b_init,\n        data_format=data_format,\n        name=name)\n\n\nclass Conv3D(ConvND):\n  """"""`Conv3D` module.""""""\n\n  def __init__(self,\n               output_channels: int,\n               kernel_shape: Union[int, Sequence[int]],\n               stride: Union[int, Sequence[int]] = 1,\n               rate: Union[int, Sequence[int]] = 1,\n               padding: Union[Text, pad.Paddings] = ""SAME"",\n               with_bias: bool = True,\n               w_init: Optional[initializers.Initializer] = None,\n               b_init: Optional[initializers.Initializer] = None,\n               data_format: Text = ""NDHWC"",\n               name: Optional[Text] = None):\n    """"""Constructs a ``Conv3D`` module.\n\n    Args:\n      output_channels: The number of output channels.\n      kernel_shape: Sequence of kernel sizes (of length 3), or an integer.\n        ``kernel_shape`` will be expanded to define a kernel size in all\n        dimensions.\n      stride: Sequence of strides (of length 3), or an integer. `stride` will be\n        expanded to define stride in all dimensions.\n      rate: Sequence of dilation rates (of length 3), or integer that is used to\n        define dilation rate in all dimensions. 1 corresponds to standard\n        convolution, ``rate > 1`` corresponds to dilated convolution.\n      padding: Padding to apply to the input. This can either ``SAME``,\n        ``VALID`` or a callable or sequence of callables up to size N. Any\n        callables must take a single integer argument equal to the effective\n        kernel size and return a list of two integers representing the padding\n        before and after. See snt.pad.* for more details and example functions.\n      with_bias: Whether to include bias parameters. Default ``True``.\n      w_init: Optional initializer for the weights. By default the weights are\n        initialized truncated random normal values with a standard deviation of\n        ``1 / sqrt(input_feature_size)``, which is commonly used when the inputs\n        are zero centered (see https://arxiv.org/abs/1502.03167v3).\n      b_init: Optional initializer for the bias. By default the bias is\n        initialized to zero.\n      data_format: The data format of the input.\n      name: Name of the module.\n    """"""\n    super(Conv3D, self).__init__(\n        num_spatial_dims=3,\n        output_channels=output_channels,\n        kernel_shape=kernel_shape,\n        stride=stride,\n        rate=rate,\n        padding=padding,\n        with_bias=with_bias,\n        w_init=w_init,\n        b_init=b_init,\n        data_format=data_format,\n        name=name)\n'"
sonnet/src/conv_test.py,37,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.conv.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport mock\nimport numpy as np\nfrom sonnet.src import conv\nfrom sonnet.src import initializers\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\n\ndef create_constant_initializers(w, b, with_bias):\n  if with_bias:\n    return {\n        ""w_init"": initializers.Constant(w),\n        ""b_init"": initializers.Constant(b)\n    }\n  else:\n    return {""w_init"": initializers.Constant(w)}\n\n\ndef foo(i):\n  return [i, i]\n\n\nclass ConvTest(test_utils.TestCase, parameterized.TestCase):\n\n  @mock.patch(""__main__.foo"", mock.MagicMock(return_value=[0, 0]))\n  def testPaddingFunctionReached(self):\n    self.reached = False\n\n    conv1 = conv.ConvND(\n        num_spatial_dims=2,\n        output_channels=1,\n        kernel_shape=3,\n        stride=1,\n        padding=foo,\n        data_format=""NHWC"",\n        **create_constant_initializers(1.0, 1.0, True))\n\n    conv1(tf.ones([1, 5, 5, 1], dtype=tf.float32))\n\n    self.assertEqual(conv1.conv_padding, ""VALID"")\n    self.assertEqual(conv1.padding_func, foo)\n    self.assertTrue(foo.called)\n\n  @parameterized.parameters(0, 4)\n  def testIncorrectN(self, n):\n    with self.assertRaisesRegexp(\n        ValueError,\n        ""We only support convoltion operations for num_spatial_dims=1, 2 or 3""):\n      conv.ConvND(\n          num_spatial_dims=n,\n          output_channels=1,\n          kernel_shape=3,\n          data_format=""NHWC"")\n\n  def testInitializerKeysInvalidWithoutBias(self):\n    with self.assertRaisesRegexp(ValueError, ""b_init must be None""):\n      conv.ConvND(\n          num_spatial_dims=2,\n          output_channels=1,\n          kernel_shape=3,\n          data_format=""NHWC"",\n          with_bias=False,\n          b_init=tf.zeros_initializer())\n\n  def testIncorrectRankInput(self):\n    c = conv.ConvND(\n        num_spatial_dims=2,\n        output_channels=1,\n        kernel_shape=3,\n        data_format=""NHWC"")\n    with self.assertRaisesRegex(ValueError, ""Shape .* must have rank 4""):\n      c(tf.ones([2, 4, 4]))\n\n  @parameterized.parameters(tf.float32, tf.float64)\n  def testDefaultInitializers(self, dtype):\n    if ""TPU"" in self.device_types and dtype == tf.float64:\n      self.skipTest(""Double precision not supported on TPU."")\n\n    conv1 = conv.ConvND(\n        num_spatial_dims=2,\n        output_channels=1,\n        kernel_shape=16,\n        stride=1,\n        padding=""VALID"",\n        data_format=""NHWC"")\n\n    out = conv1(tf.random.normal([8, 64, 64, 1], dtype=dtype))\n\n    self.assertAllEqual(out.shape, [8, 49, 49, 1])\n    self.assertEqual(out.dtype, dtype)\n\n    # Note that for unit variance inputs the output is below unit variance\n    # because of the use of the truncated normal initalizer\n    err = 0.2 if self.primary_device == ""TPU"" else 0.1\n    self.assertNear(out.numpy().std(), 0.87, err=err)\n\n  @parameterized.named_parameters(\n      (""SamePaddingUseBias"", True, ""SAME""),\n      (""SamePaddingNoBias"", False, ""SAME""),\n      (""samePaddingUseBias"", True, ""same""),\n      (""samePaddingNoBias"", False, ""same""),\n      (""ValidPaddingNoBias"", False, ""VALID""),\n      (""ValidPaddingUseBias"", True, ""VALID""),\n      (""validPaddingNoBias"", False, ""valid""),\n      (""validPaddingUseBias"", True, ""valid""),\n  )\n  def testFunction(self, with_bias, padding):\n    conv1 = conv.ConvND(\n        num_spatial_dims=2,\n        output_channels=1,\n        kernel_shape=3,\n        stride=1,\n        padding=padding,\n        with_bias=with_bias,\n        data_format=""NHWC"",\n        **create_constant_initializers(1.0, 1.0, with_bias))\n    conv2 = conv.ConvND(\n        num_spatial_dims=2,\n        output_channels=1,\n        kernel_shape=3,\n        stride=1,\n        padding=padding,\n        with_bias=with_bias,\n        data_format=""NHWC"",\n        **create_constant_initializers(1.0, 1.0, with_bias))\n    defun_conv = tf.function(conv2)\n\n    iterations = 5\n\n    for _ in range(iterations):\n      x = tf.random.uniform([1, 5, 5, 1])\n      y1 = conv1(x)\n      y2 = defun_conv(x)\n\n      self.assertAllClose(self.evaluate(y1), self.evaluate(y2), atol=1e-4)\n\n  def testUnknownBatchSizeNHWC(self):\n    x = tf.TensorSpec([None, 5, 5, 3], dtype=tf.float32)\n\n    c = conv.ConvND(\n        num_spatial_dims=2,\n        output_channels=2,\n        kernel_shape=3,\n        data_format=""NHWC"")\n    defun_conv = tf.function(c).get_concrete_function(x)\n\n    out1 = defun_conv(tf.ones([3, 5, 5, 3]))\n    self.assertEqual(out1.shape, [3, 5, 5, 2])\n\n    out2 = defun_conv(tf.ones([5, 5, 5, 3]))\n    self.assertEqual(out2.shape, [5, 5, 5, 2])\n\n  def testUnknownBatchSizeNCHW(self):\n    if self.primary_device == ""CPU"":\n      self.skipTest(""NCHW not supported on CPU"")\n\n    x = tf.TensorSpec([None, 3, 5, 5], dtype=tf.float32)\n    c = conv.ConvND(\n        num_spatial_dims=2,\n        output_channels=2,\n        kernel_shape=3,\n        data_format=""NCHW"")\n    defun_conv = tf.function(c).get_concrete_function(x)\n\n    out1 = defun_conv(tf.ones([3, 3, 5, 5]))\n    self.assertEqual(out1.shape, [3, 2, 5, 5])\n\n    out2 = defun_conv(tf.ones([5, 3, 5, 5]))\n    self.assertEqual(out2.shape, [5, 2, 5, 5])\n\n  @parameterized.parameters(True, False)\n  def testUnknownChannels(self, autograph):\n    x = tf.TensorSpec([3, 3, 3, None], dtype=tf.float32)\n\n    c = conv.ConvND(\n        num_spatial_dims=2,\n        output_channels=1,\n        kernel_shape=3,\n        data_format=""NHWC"")\n    defun_conv = tf.function(c, autograph=autograph)\n\n    with self.assertRaisesRegex(ValueError,\n                                ""The number of input channels must be known""):\n      defun_conv.get_concrete_function(x)\n\n  def testUnknownSpatialDims(self):\n    x = tf.TensorSpec([3, None, None, 3], dtype=tf.float32)\n\n    c = conv.ConvND(\n        num_spatial_dims=2,\n        output_channels=1,\n        kernel_shape=3,\n        data_format=""NHWC"")\n    defun_conv = tf.function(c).get_concrete_function(x)\n\n    out = defun_conv(tf.ones([3, 5, 5, 3]))\n    expected_out = c(tf.ones([3, 5, 5, 3]))\n    self.assertEqual(out.shape, [3, 5, 5, 1])\n    self.assertAllEqual(self.evaluate(out), self.evaluate(expected_out))\n\n    out = defun_conv(tf.ones([3, 4, 4, 3]))\n    expected_out = c(tf.ones([3, 4, 4, 3]))\n    self.assertEqual(out.shape, [3, 4, 4, 1])\n    self.assertAllEqual(self.evaluate(out), self.evaluate(expected_out))\n\n\nclass Conv2DTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters(True, False)\n  def testComputationPaddingSame(self, with_bias):\n    expected_out = [[4, 6, 6, 6, 4], [6, 9, 9, 9, 6], [6, 9, 9, 9, 6],\n                    [6, 9, 9, 9, 6], [4, 6, 6, 6, 4]]\n    conv1 = conv.Conv2D(\n        output_channels=1,\n        kernel_shape=3,\n        stride=1,\n        padding=""SAME"",\n        with_bias=with_bias,\n        **create_constant_initializers(1.0, 1.0, with_bias))\n\n    out = conv1(tf.ones([1, 5, 5, 1], dtype=tf.float32))\n    self.assertEqual(out.shape, [1, 5, 5, 1])\n    out = tf.squeeze(out, axis=(0, 3))\n\n    expected_out = np.asarray(expected_out, dtype=np.float32)\n    if with_bias:\n      expected_out += 1\n\n    self.assertAllClose(self.evaluate(out), expected_out)\n\n  @parameterized.parameters(True, False)\n  def testComputationPaddingValid(self, with_bias):\n    expected_out = [[9, 9, 9], [9, 9, 9], [9, 9, 9]]\n    conv1 = conv.Conv2D(\n        output_channels=1,\n        kernel_shape=3,\n        stride=1,\n        padding=""VALID"",\n        with_bias=with_bias,\n        **create_constant_initializers(1.0, 1.0, with_bias))\n\n    out = conv1(tf.ones([1, 5, 5, 1], dtype=tf.float32))\n    self.assertEqual(out.shape, [1, 3, 3, 1])\n    out = tf.squeeze(out, axis=(0, 3))\n\n    expected_out = np.asarray(expected_out, dtype=np.float32)\n    if with_bias:\n      expected_out += 1\n\n    self.assertAllClose(self.evaluate(out), expected_out)\n\n\nclass Conv1DTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters(True, False)\n  def testComputationPaddingSame(self, with_bias):\n    expected_out = [2, 3, 3, 3, 2]\n    conv1 = conv.Conv1D(\n        output_channels=1,\n        kernel_shape=3,\n        stride=1,\n        padding=""SAME"",\n        with_bias=with_bias,\n        **create_constant_initializers(1.0, 1.0, with_bias))\n\n    out = conv1(tf.ones([1, 5, 1], dtype=tf.float32))\n    self.assertEqual(out.shape, [1, 5, 1])\n    out = tf.squeeze(out, axis=(0, 2))\n\n    expected_out = np.asarray(expected_out, dtype=np.float32)\n    if with_bias:\n      expected_out += 1\n\n    self.assertAllClose(self.evaluate(out), expected_out)\n\n  @parameterized.parameters(True, False)\n  def testComputationPaddingValid(self, with_bias):\n    expected_out = [3, 3, 3]\n    expected_out = np.asarray(expected_out, dtype=np.float32)\n    if with_bias:\n      expected_out += 1\n\n    conv1 = conv.Conv1D(\n        output_channels=1,\n        kernel_shape=3,\n        stride=1,\n        padding=""VALID"",\n        with_bias=with_bias,\n        **create_constant_initializers(1.0, 1.0, with_bias))\n\n    out = conv1(tf.ones([1, 5, 1], dtype=tf.float32))\n    self.assertEqual(out.shape, [1, 3, 1])\n    out = tf.squeeze(out, axis=(0, 2))\n\n    self.assertAllClose(self.evaluate(out), expected_out)\n\n\nclass Conv3DTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters(True, False)\n  def testComputationPaddingSame(self, with_bias):\n    expected_out = np.asarray([\n        9, 13, 13, 13, 9, 13, 19, 19, 19, 13, 13, 19, 19, 19, 13, 13, 19, 19,\n        19, 13, 9, 13, 13, 13, 9, 13, 19, 19, 19, 13, 19, 28, 28, 28, 19, 19,\n        28, 28, 28, 19, 19, 28, 28, 28, 19, 13, 19, 19, 19, 13, 13, 19, 19, 19,\n        13, 19, 28, 28, 28, 19, 19, 28, 28, 28, 19, 19, 28, 28, 28, 19, 13, 19,\n        19, 19, 13, 13, 19, 19, 19, 13, 19, 28, 28, 28, 19, 19, 28, 28, 28, 19,\n        19, 28, 28, 28, 19, 13, 19, 19, 19, 13, 9, 13, 13, 13, 9, 13, 19, 19,\n        19, 13, 13, 19, 19, 19, 13, 13, 19, 19, 19, 13, 9, 13, 13, 13, 9\n    ]).reshape((5, 5, 5))\n    if not with_bias:\n      expected_out -= 1\n\n    conv1 = conv.Conv3D(\n        output_channels=1,\n        kernel_shape=3,\n        stride=1,\n        padding=""SAME"",\n        with_bias=with_bias,\n        **create_constant_initializers(1.0, 1.0, with_bias))\n\n    out = conv1(tf.ones([1, 5, 5, 5, 1], dtype=tf.float32))\n    self.assertEqual(out.shape, [1, 5, 5, 5, 1])\n    out = tf.squeeze(out, axis=(0, 4))\n\n    self.assertAllClose(self.evaluate(out), expected_out)\n\n  @parameterized.parameters(True, False)\n  def testComputationPaddingValid(self, with_bias):\n    expected_out = np.asarray([\n        28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n        28, 28, 28, 28, 28, 28, 28, 28, 28\n    ]).reshape((3, 3, 3))\n    if not with_bias:\n      expected_out -= 1\n\n    conv1 = conv.Conv3D(\n        output_channels=1,\n        kernel_shape=3,\n        stride=1,\n        padding=""VALID"",\n        with_bias=with_bias,\n        **create_constant_initializers(1.0, 1.0, with_bias))\n\n    out = conv1(tf.ones([1, 5, 5, 5, 1], dtype=tf.float32))\n    self.assertEqual(out.shape, [1, 3, 3, 3, 1])\n    out = tf.squeeze(out, axis=(0, 4))\n\n    self.assertAllClose(self.evaluate(out), expected_out)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/conv_transpose.py,9,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Transpose convolutional module.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom sonnet.src import base\nfrom sonnet.src import initializers\nfrom sonnet.src import once\nfrom sonnet.src import types\nfrom sonnet.src import utils\n\nimport tensorflow as tf\nfrom typing import Optional, Sequence, Text, Union\n\n\ndef smart_concat(v1, v2):\n  if isinstance(v1, tf.Tensor) or isinstance(v2, tf.Tensor):\n    return tf.concat([v1, v2], 0)\n  else:\n    return v1 + v2\n\n\ndef smart_lambda(func, v1, v2):\n  if isinstance(v1, tf.Tensor) or isinstance(v2, tf.Tensor):\n    return func(v1, v2)\n  else:\n    return [func(x, y) for (x, y) in zip(v1, v2)]\n\n\nclass ConvNDTranspose(base.Module):\n  """"""An N-dimensional transpose convolutional module.\n\n  Attributes:\n     w: Weight variable. Note is `None` until module is connected.\n     b: Biases variable. Note is `None` until module is connected.\n     input_shape: The input shape of the first set of inputs. Note is `None`\n       until module is connected.\n  """"""\n\n  def __init__(self,\n               num_spatial_dims: int,\n               output_channels: int,\n               kernel_shape: Union[int, Sequence[int]],\n               output_shape: Optional[types.ShapeLike] = None,\n               stride: Union[int, Sequence[int]] = 1,\n               rate: Union[int, Sequence[int]] = 1,\n               padding: Text = ""SAME"",\n               with_bias: bool = True,\n               w_init: Optional[initializers.Initializer] = None,\n               b_init: Optional[initializers.Initializer] = None,\n               data_format: Optional[Text] = None,\n               name: Optional[Text] = None):\n    """"""Constructs a `ConvNDTranspose` module.\n\n    Args:\n      num_spatial_dims: Number of spatial dimensions of the input.\n      output_channels: Number of output channels.\n      kernel_shape: Sequence of integers (of length num_spatial_dims), or an\n        integer representing kernel shape. `kernel_shape` will be expanded to\n        define a kernel size in all dimensions.\n      output_shape: Output shape of the spatial dimensions of a transpose\n        convolution. Can be either an iterable of integers or a\n        `TensorShape` of length `num_spatial_dims`. If a `None` value is given,\n        a default shape is automatically calculated.\n      stride: Sequence of integers (of length num_spatial_dims), or an integer.\n        `stride` will be expanded to define stride in all dimensions.\n      rate: Sequence of integers (of length num_spatial_dims), or integer that\n        is used to define dilation rate in all dimensions. 1 corresponds to\n        standard ND convolution, `rate > 1` corresponds to dilated convolution.\n      padding: Padding algorithm, either ""SAME"" or ""VALID"".\n      with_bias: Boolean, whether to include bias parameters. Default `True`.\n      w_init: Optional initializer for the weights. By default the weights are\n        initialized truncated random normal values with a standard deviation of\n        `1 / sqrt(input_feature_size)`, which is commonly used when the\n        inputs are zero centered (see https://arxiv.org/abs/1502.03167v3).\n      b_init: Optional initializer for the bias. By default the bias is\n        initialized to zero.\n      data_format: The data format of the input.\n      name: Name of the module.\n    """"""\n    super(ConvNDTranspose, self).__init__(name=name)\n\n    if not 1 <= num_spatial_dims <= 3:\n      raise ValueError(\n          ""We only support transpose convolution operations for ""\n          ""num_spatial_dims=1, 2 or 3, received num_spatial_dims={}."".format(\n              num_spatial_dims))\n    self._num_spatial_dims = num_spatial_dims\n    self._output_channels = output_channels\n    self._kernel_shape = kernel_shape\n    self._output_shape = output_shape\n    self._stride = stride\n    self._rate = rate\n\n    if padding == ""SAME"" or padding == ""VALID"":\n      self._padding = padding\n    else:\n      raise TypeError(""ConvNDTranspose only takes string padding, please ""\n                      ""provide either `SAME` or `VALID`."")\n    self._data_format = data_format\n    self._channel_index = utils.get_channel_index(data_format)\n    self._with_bias = with_bias\n\n    self._w_init = w_init\n    if with_bias:\n      self._b_init = b_init if b_init is not None else initializers.Zeros()\n    elif b_init is not None:\n      raise ValueError(""When not using a bias the b_init must be None."")\n\n  def __call__(self, inputs):\n    self._initialize(inputs)\n\n    if self._output_shape is None:\n      output_shape = self._get_output_shape(inputs)\n      if self._channel_index == 1:\n        output_shape = smart_concat([self._output_channels], output_shape)\n      else:\n        output_shape = smart_concat(output_shape, [self._output_channels])\n    else:\n      output_shape = self._output_shape\n    output_shape = smart_concat([tf.shape(inputs)[0]], output_shape)\n\n    outputs = tf.nn.conv_transpose(\n        input=inputs,\n        filters=self.w,\n        output_shape=output_shape,\n        strides=self._stride,\n        padding=self._padding,\n        data_format=self._data_format,\n        dilations=self._rate,\n        name=None)\n    if self._with_bias:\n      outputs = tf.nn.bias_add(outputs, self.b, data_format=self._data_format)\n    return outputs\n\n  @once.once\n  def _initialize(self, inputs):\n    utils.assert_rank(inputs, self._num_spatial_dims + 2)\n    self.input_channels = inputs.shape[self._channel_index]\n    if self.input_channels is None:\n      raise ValueError(""The number of input channels must be known"")\n    self._dtype = inputs.dtype\n\n    if self._output_shape is not None:\n      if len(self._output_shape) != self._num_spatial_dims:\n        raise ValueError(\n            ""The output_shape must be of length {} but instead was {}."".format(\n                self._num_spatial_dims, len(self._output_shape)))\n      if self._channel_index == 1:\n        self._output_shape = [self._output_channels] + list(self._output_shape)\n      else:\n        self._output_shape = list(self._output_shape) + [self._output_channels]\n\n    self.w = self._make_w()\n    if self._with_bias:\n      self.b = tf.Variable(\n          self._b_init((self._output_channels,), self._dtype), name=""b"")\n\n  def _make_w(self):\n    """"""Makes and returns the variable representing the weight.""""""\n    kernel_shape = utils.replicate(self._kernel_shape, self._num_spatial_dims,\n                                   ""kernel_shape"")\n    weight_shape = kernel_shape + (self._output_channels, self.input_channels)\n\n    if self._w_init is None:\n      # See https://arxiv.org/abs/1502.03167v3.\n      fan_in_shape = kernel_shape + (self.input_channels,)\n      stddev = 1 / np.sqrt(np.prod(fan_in_shape))\n      self._w_init = initializers.TruncatedNormal(stddev=stddev)\n\n    return tf.Variable(self._w_init(weight_shape, self._dtype), name=""w"")\n\n  def _get_output_shape(self, inputs):\n    input_shape = inputs.shape if inputs.shape.is_fully_defined() else tf.shape(\n        inputs)\n\n    if self._channel_index == 1:\n      input_size = input_shape[2:]\n    else:\n      input_size = input_shape[1:-1]\n    stride = utils.replicate(self._stride, self._num_spatial_dims, ""stride"")\n\n    output_shape = smart_lambda(lambda x, y: x * y, input_size, stride)\n\n    if self._padding == ""VALID"":\n      kernel_shape = utils.replicate(self._kernel_shape, self._num_spatial_dims,\n                                     ""kernel_shape"")\n      rate = utils.replicate(self._rate, self._num_spatial_dims, ""rate"")\n      effective_kernel_shape = [\n          (shape - 1) * rate + 1 for (shape, rate) in zip(kernel_shape, rate)\n      ]\n      output_shape = smart_lambda(lambda x, y: x + y - 1, output_shape,\n                                  effective_kernel_shape)\n\n    return output_shape\n\n\nclass Conv1DTranspose(ConvNDTranspose):\n  """"""A 1D transpose convolutional module.""""""\n\n  def __init__(self,\n               output_channels: int,\n               kernel_shape: Union[int, Sequence[int]],\n               output_shape: Optional[types.ShapeLike] = None,\n               stride: Union[int, Sequence[int]] = 1,\n               rate: Union[int, Sequence[int]] = 1,\n               padding: Text = ""SAME"",\n               with_bias: bool = True,\n               w_init: Optional[initializers.Initializer] = None,\n               b_init: Optional[initializers.Initializer] = None,\n               data_format: Text = ""NWC"",\n               name: Optional[Text] = None):\n    """"""Constructs a `Conv1DTranspose` module.\n\n    Args:\n      output_channels: Number of output channels.\n      kernel_shape: Sequence of integers (of length 1), or an integer\n        representing kernel shape. `kernel_shape` will be expanded to define a\n        kernel size in all dimensions.\n      output_shape: Output shape of the spatial dimensions of a transpose\n        convolution. Can be either an integer or an iterable of integers or\n        `Dimension`s, or a `TensorShape` (of length 1). If a `None` value is\n        given, a default shape is automatically calculated.\n      stride: Sequence of integers (of length 1), or an integer. `stride` will\n        be expanded to define stride in all dimensions.\n      rate: Sequence of integers (of length 1), or integer that is used to\n        define dilation rate in all dimensions. 1 corresponds to standard 1D\n        convolution, `rate > 1` corresponds to dilated convolution.\n      padding: Padding algorithm, either ""SAME"" or ""VALID"".\n      with_bias: Boolean, whether to include bias parameters. Default `True`.\n      w_init: Optional initializer for the weights. By default the weights are\n        initialized truncated random normal values with a standard deviation of\n        `1 / sqrt(input_feature_size)`, which is commonly used when the\n        inputs are zero centered (see https://arxiv.org/abs/1502.03167v3).\n      b_init: Optional initializer for the bias. By default the bias is\n        initialized to zero.\n      data_format: The data format of the input.\n      name: Name of the module.\n    """"""\n    super(Conv1DTranspose, self).__init__(\n        num_spatial_dims=1,\n        output_channels=output_channels,\n        kernel_shape=kernel_shape,\n        output_shape=output_shape,\n        stride=stride,\n        rate=rate,\n        padding=padding,\n        with_bias=with_bias,\n        w_init=w_init,\n        b_init=b_init,\n        data_format=data_format,\n        name=name)\n\n\nclass Conv2DTranspose(ConvNDTranspose):\n  """"""A 2D transpose convolutional module.""""""\n\n  def __init__(self,\n               output_channels: int,\n               kernel_shape: Union[int, Sequence[int]],\n               output_shape: Optional[types.ShapeLike] = None,\n               stride: Union[int, Sequence[int]] = 1,\n               rate: Union[int, Sequence[int]] = 1,\n               padding: Text = ""SAME"",\n               with_bias: bool = True,\n               w_init: Optional[initializers.Initializer] = None,\n               b_init: Optional[initializers.Initializer] = None,\n               data_format: Text = ""NHWC"",\n               name: Optional[Text] = None):\n    """"""Constructs a `Conv2DTranspose` module.\n\n    Args:\n      output_channels: An integer, The number of output channels.\n      kernel_shape: Sequence of integers (of length 2), or an integer\n        representing kernel shape. `kernel_shape` will be expanded to define a\n        kernel size in all dimensions.\n      output_shape: Output shape of the spatial dimensions of a transpose\n        convolution. Can be either an integer or an iterable of integers or\n        `Dimension`s, or a `TensorShape` (of length 2). If a `None` value is\n        given, a default shape is automatically calculated.\n      stride: Sequence of integers (of length 2), or an integer. `stride` will\n        be expanded to define stride in all dimensions.\n      rate: Sequence of integers (of length 2), or integer that is used to\n        define dilation rate in all dimensions. 1 corresponds to standard 2D\n        convolution, `rate > 1` corresponds to dilated convolution.\n      padding: Padding algorithm, either ""SAME"" or ""VALID"".\n      with_bias: Boolean, whether to include bias parameters. Default `True`.\n      w_init: Optional initializer for the weights. By default the weights are\n        initialized truncated random normal values with a standard deviation of\n        `1 / sqrt(input_feature_size)`, which is commonly used when the\n        inputs are zero centered (see https://arxiv.org/abs/1502.03167v3).\n      b_init: Optional initializer for the bias. By default the bias is\n        initialized to zero.\n      data_format: The data format of the input.\n      name: Name of the module.\n    """"""\n    super(Conv2DTranspose, self).__init__(\n        num_spatial_dims=2,\n        output_channels=output_channels,\n        kernel_shape=kernel_shape,\n        output_shape=output_shape,\n        stride=stride,\n        rate=rate,\n        padding=padding,\n        with_bias=with_bias,\n        w_init=w_init,\n        b_init=b_init,\n        data_format=data_format,\n        name=name)\n\n\nclass Conv3DTranspose(ConvNDTranspose):\n  """"""A 3D transpose convolutional module.""""""\n\n  def __init__(self,\n               output_channels: int,\n               kernel_shape: Union[int, Sequence[int]],\n               output_shape: Optional[types.ShapeLike] = None,\n               stride: Union[int, Sequence[int]] = 1,\n               rate: Union[int, Sequence[int]] = 1,\n               padding: Text = ""SAME"",\n               with_bias: bool = True,\n               w_init: Optional[initializers.Initializer] = None,\n               b_init: Optional[initializers.Initializer] = None,\n               data_format: Text = ""NDHWC"",\n               name: Optional[Text] = None):\n    """"""Constructs a `Conv3DTranspose` module.\n\n    Args:\n      output_channels: An integer, The number of output channels.\n      kernel_shape: Sequence of integers (of length 3), or an integer\n        representing kernel shape. `kernel_shape` will be expanded to define a\n        kernel size in all dimensions.\n      output_shape: Output shape of the spatial dimensions of a transpose\n        convolution. Can be either an integer or an iterable of integers or\n        `Dimension`s, or a `TensorShape` (of length 3). If a None value is\n        given, a default shape is automatically calculated.\n      stride: Sequence of integers (of length 3), or an integer. `stride` will\n        be expanded to define stride in all dimensions.\n      rate: Sequence of integers (of length 3), or integer that is used to\n        define dilation rate in all dimensions. 1 corresponds to standard 3D\n        convolution, `rate > 1` corresponds to dilated convolution.\n      padding: Padding algorithm, either ""SAME"" or ""VALID"".\n      with_bias: Boolean, whether to include bias parameters. Default `True`.\n      w_init: Optional initializer for the weights. By default the weights are\n        initialized truncated random normal values with a standard deviation of\n        `1 / sqrt(input_feature_size)`, which is commonly used when the\n        inputs are zero centered (see https://arxiv.org/abs/1502.03167v3).\n      b_init: Optional initializer for the bias. By default the bias is\n        initialized to zero.\n      data_format: The data format of the input.\n      name: Name of the module.\n    """"""\n    super(Conv3DTranspose, self).__init__(\n        num_spatial_dims=3,\n        output_channels=output_channels,\n        kernel_shape=kernel_shape,\n        output_shape=output_shape,\n        stride=stride,\n        rate=rate,\n        padding=padding,\n        with_bias=with_bias,\n        w_init=w_init,\n        b_init=b_init,\n        data_format=data_format,\n        name=name)\n'"
sonnet/src/conv_transpose_test.py,33,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.conv_transpose.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom sonnet.src import conv_transpose\nfrom sonnet.src import initializers as lib_initializers\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\n\ndef create_constant_initializers(w, b, with_bias):\n  if with_bias:\n    return {\n        ""w_init"": lib_initializers.Constant(w),\n        ""b_init"": lib_initializers.Constant(b)\n    }\n  else:\n    return {""w_init"": lib_initializers.Constant(w)}\n\n\nclass ConvTransposeTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters(0, 4)\n  def testIncorrectN(self, n):\n    with self.assertRaisesRegexp(\n        ValueError,\n        ""only support transpose convolution operations for num_spatial_dims""):\n      conv_transpose.ConvNDTranspose(\n          num_spatial_dims=n,\n          output_channels=1,\n          output_shape=None,\n          kernel_shape=3,\n          data_format=""NHWC"")\n\n  def testIncorrectPadding(self):\n    with self.assertRaisesRegexp(\n        TypeError,\n        ""ConvNDTranspose only takes string padding, please provide either""):\n      conv_transpose.ConvNDTranspose(\n          2, output_channels=1, kernel_shape=3, padding=None)\n\n  def testBiasInitNoBias(self):\n    with self.assertRaisesRegexp(\n        ValueError,\n        ""When not using a bias the b_init must be None.""):\n      conv_transpose.ConvNDTranspose(\n          2, output_channels=1, kernel_shape=3, with_bias=False,\n          b_init=lib_initializers.Ones(), data_format=""NHWC"")\n\n  def testIncorrectOutputShape(self):\n    c = conv_transpose.ConvNDTranspose(\n        num_spatial_dims=2,\n        output_channels=3,\n        kernel_shape=2,\n        output_shape=[1],\n        data_format=""NHWC"")\n    with self.assertRaisesRegexp(\n        ValueError,\n        ""The output_shape must be of length 2 but instead was 1.""):\n      c(tf.ones([3, 5, 5, 3]))\n\n  @parameterized.parameters(*itertools.product(\n      [True, False],  # with_bias\n      [""SAME"", ""VALID""]))  # padding\n  def testGraphConv(self, with_bias, padding):\n    conv1 = conv_transpose.ConvNDTranspose(\n        num_spatial_dims=2,\n        output_channels=1,\n        output_shape=None,\n        kernel_shape=3,\n        stride=1,\n        padding=padding,\n        with_bias=with_bias,\n        data_format=""NHWC"",\n        **create_constant_initializers(1.0, 1.0, with_bias))\n    conv2 = conv_transpose.ConvNDTranspose(\n        num_spatial_dims=2,\n        output_channels=1,\n        output_shape=None,\n        kernel_shape=3,\n        stride=1,\n        padding=padding,\n        with_bias=with_bias,\n        data_format=""NHWC"",\n        **create_constant_initializers(1.0, 1.0, with_bias))\n    defun_conv = tf.function(conv2)\n\n    iterations = 5\n\n    for _ in range(iterations):\n      x = tf.random.uniform([1, 3, 3, 1])\n      y1 = conv1(x)\n      y2 = defun_conv(x)\n\n      self.assertAllClose(self.evaluate(y1), self.evaluate(y2), atol=1e-4)\n\n  def testUnknownBatchSizeNHWC(self):\n    x = tf.TensorSpec([None, 5, 5, 3], dtype=tf.float32)\n\n    c = conv_transpose.ConvNDTranspose(\n        num_spatial_dims=2,\n        output_channels=2,\n        kernel_shape=3,\n        data_format=""NHWC"")\n    defun_conv = tf.function(c).get_concrete_function(x)\n\n    out1 = defun_conv(tf.ones([3, 5, 5, 3]))\n    self.assertEqual(out1.shape, [3, 5, 5, 2])\n\n    out2 = defun_conv(tf.ones([5, 5, 5, 3]))\n    self.assertEqual(out2.shape, [5, 5, 5, 2])\n\n  def testUnknownBatchSizeNCHW(self):\n    if self.primary_device == ""CPU"":\n      self.skipTest(""NCHW not supported on CPU"")\n\n    x = tf.TensorSpec([None, 3, 5, 5], dtype=tf.float32)\n\n    c = conv_transpose.ConvNDTranspose(\n        num_spatial_dims=2,\n        output_channels=2,\n        kernel_shape=3,\n        data_format=""NCHW"")\n    defun_conv = tf.function(c).get_concrete_function(x)\n\n    out1 = defun_conv(tf.ones([3, 3, 5, 5]))\n    self.assertEqual(out1.shape, [3, 2, 5, 5])\n\n    out2 = defun_conv(tf.ones([5, 3, 5, 5]))\n    self.assertEqual(out2.shape, [5, 2, 5, 5])\n\n  def testUnknownShapeDims(self):\n    x = tf.TensorSpec([3, None, None, 3], dtype=tf.float32)\n\n    c = conv_transpose.ConvNDTranspose(\n        num_spatial_dims=2,\n        output_channels=2,\n        kernel_shape=3,\n        data_format=""NHWC"")\n    defun_conv = tf.function(c).get_concrete_function(x)\n\n    out1 = defun_conv(tf.ones([3, 5, 5, 3]))\n    self.assertEqual(out1.shape, [3, 5, 5, 2])\n\n    out1 = defun_conv(tf.ones([3, 3, 3, 3]))\n    self.assertEqual(out1.shape, [3, 3, 3, 2])\n\n  def testGivenOutputShape(self):\n    c = conv_transpose.ConvNDTranspose(\n        num_spatial_dims=2,\n        output_channels=2,\n        kernel_shape=3,\n        output_shape=[5, 5],\n        data_format=""NHWC"")\n\n    out1 = c(tf.ones([3, 5, 5, 3]))\n    self.assertEqual(out1.shape, [3, 5, 5, 2])\n\n  @parameterized.parameters(True, False)\n  def testUnknownChannels(self, autograph):\n    x = tf.TensorSpec([3, 3, 3, None], dtype=tf.float32)\n\n    c = conv_transpose.ConvNDTranspose(\n        num_spatial_dims=2,\n        output_channels=1,\n        kernel_shape=3,\n        data_format=""NHWC"")\n    defun_conv = tf.function(c, autograph=autograph)\n\n    with self.assertRaisesRegex(ValueError,\n                                ""The number of input channels must be known""):\n      defun_conv.get_concrete_function(x)\n\n  @parameterized.parameters(\n      (1, (3,), 128, 5, ""NWC""),\n      (2, (4, 4), 64, 3, ""NHWC""),\n      (3, (4, 4, 4), 64, 3, ""NDHWC""))\n  def testInitializerVariance(self, num_spatial_dims, kernel_shape,\n                              in_channels, output_channels, data_format):\n    inputs = tf.random.uniform([16] + ([32] * num_spatial_dims) + [in_channels])\n\n    c = conv_transpose.ConvNDTranspose(\n        num_spatial_dims=num_spatial_dims,\n        kernel_shape=kernel_shape,\n        output_channels=output_channels,\n        data_format=data_format)\n    c(inputs)\n\n    actual_std = c.w.numpy().std()\n    expected_std = 1 / (np.sqrt(np.prod(kernel_shape + (in_channels,))))\n\n    # This ratio of the error compared to the expected std might be somewhere\n    # around 0.15 normally. We check it is not > 0.5, as that would indicate\n    # something seriously wrong (ie the previous buggy initialization).\n    rel_diff = np.abs(actual_std - expected_std) / expected_std\n    self.assertLess(rel_diff, 0.5)\n\n\nclass Conv2DTransposeTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters(True, False)\n  def testComputationPaddingSame(self, with_bias):\n    expected_out = [[4, 6, 4], [6, 9, 6], [4, 6, 4]]\n    expected_out = np.asarray(expected_out, dtype=np.float32)\n    if with_bias:\n      expected_out += 1\n\n    conv_transpose1 = conv_transpose.Conv2DTranspose(\n        output_channels=1,\n        output_shape=None,\n        kernel_shape=3,\n        stride=1,\n        padding=""SAME"",\n        with_bias=with_bias,\n        **create_constant_initializers(1.0, 1.0, with_bias))\n\n    out = conv_transpose1(tf.ones([1, 3, 3, 1], dtype=tf.float32))\n    self.assertEqual(out.shape, [1, 3, 3, 1])\n    out = tf.squeeze(out, axis=(0, 3))\n\n    self.assertAllClose(self.evaluate(out), expected_out)\n\n  @parameterized.parameters(True, False)\n  def testComputationPaddingValid(self, with_bias):\n    expected_out = [[1, 2, 3, 2, 1], [2, 4, 6, 4, 2], [3, 6, 9, 6, 3],\n                    [2, 4, 6, 4, 2], [1, 2, 3, 2, 1]]\n    expected_out = np.asarray(expected_out, dtype=np.float32)\n    if with_bias:\n      expected_out += 1\n\n    conv1 = conv_transpose.Conv2DTranspose(\n        output_channels=1,\n        output_shape=None,\n        kernel_shape=3,\n        stride=1,\n        padding=""VALID"",\n        with_bias=with_bias,\n        **create_constant_initializers(1.0, 1.0, with_bias))\n\n    out = conv1(tf.ones([1, 3, 3, 1], dtype=tf.float32))\n    self.assertEqual(out.shape, [1, 5, 5, 1])\n    out = tf.squeeze(out, axis=(0, 3))\n\n    self.assertAllClose(self.evaluate(out), expected_out)\n\n  def testShapeDilated(self):\n    if ""CPU"" == self.primary_device:\n      self.skipTest(""Not supported on CPU"")\n    conv1 = conv_transpose.Conv2DTranspose(\n        output_channels=1,\n        output_shape=None,\n        kernel_shape=3,\n        rate=2,\n        padding=""VALID"")\n\n    out = conv1(tf.ones([1, 3, 3, 1]))\n    self.assertEqual(out.shape, [1, 7, 7, 1])\n\n\nclass Conv1DTransposeTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters(True, False)\n  def testComputationPaddingSame(self, with_bias):\n    expected_out = [2, 3, 2]\n    expected_out = np.asarray(expected_out, dtype=np.float32)\n    if with_bias:\n      expected_out += 1\n\n    conv1 = conv_transpose.Conv1DTranspose(\n        output_channels=1,\n        output_shape=None,\n        kernel_shape=3,\n        stride=1,\n        padding=""SAME"",\n        with_bias=with_bias,\n        **create_constant_initializers(1.0, 1.0, with_bias))\n\n    out = conv1(tf.ones([1, 3, 1], dtype=tf.float32))\n    self.assertEqual(out.shape, [1, 3, 1])\n    out = tf.squeeze(out, axis=(0, 2))\n\n    self.assertAllClose(self.evaluate(out), expected_out)\n\n  @parameterized.parameters(True, False)\n  def testComputationPaddingValid(self, with_bias):\n    expected_out = [1, 2, 3, 2, 1]\n    expected_out = np.asarray(expected_out, dtype=np.float32)\n    if with_bias:\n      expected_out += 1\n\n    conv1 = conv_transpose.Conv1DTranspose(\n        output_channels=1,\n        output_shape=None,\n        kernel_shape=3,\n        stride=1,\n        padding=""VALID"",\n        with_bias=with_bias,\n        **create_constant_initializers(1.0, 1.0, with_bias))\n\n    out = conv1(tf.ones([1, 3, 1], dtype=tf.float32))\n    self.assertEqual(out.shape, [1, 5, 1])\n    out = tf.squeeze(out, axis=(0, 2))\n\n    self.assertAllClose(self.evaluate(out), expected_out)\n\n\nclass Conv3DTransposeTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters(True, False)\n  def testComputationPaddingSame(self, with_bias):\n    expected_out = np.asarray([\n        8, 12, 8, 12, 18, 12, 8, 12, 8, 12, 18, 12, 18, 27, 18, 12, 18, 12, 8,\n        12, 8, 12, 18, 12, 8, 12, 8\n    ]).reshape((3, 3, 3))\n    if with_bias:\n      expected_out += 1\n\n    conv_transpose1 = conv_transpose.Conv3DTranspose(\n        output_channels=1,\n        output_shape=None,\n        kernel_shape=3,\n        stride=1,\n        padding=""SAME"",\n        with_bias=with_bias,\n        **create_constant_initializers(1.0, 1.0, with_bias))\n\n    out = conv_transpose1(tf.ones([1, 3, 3, 3, 1], dtype=tf.float32))\n    self.assertEqual(out.shape, [1, 3, 3, 3, 1])\n    out = tf.squeeze(out, axis=(0, 4))\n\n    self.assertAllClose(self.evaluate(out), expected_out)\n\n  @parameterized.parameters(True, False)\n  def testComputationPaddingValid(self, with_bias):\n    expected_out = np.asarray([\n        1, 2, 3, 2, 1, 2, 4, 6, 4, 2, 3, 6, 9, 6, 3, 2, 4, 6, 4, 2, 1, 2, 3, 2,\n        1, 2, 4, 6, 4, 2, 4, 8, 12, 8, 4, 6, 12, 18, 12, 6, 4, 8, 12, 8, 4, 2,\n        4, 6, 4, 2, 3, 6, 9, 6, 3, 6, 12, 18, 12, 6, 9, 18, 27, 18, 9, 6, 12,\n        18, 12, 6, 3, 6, 9, 6, 3, 2, 4, 6, 4, 2, 4, 8, 12, 8, 4, 6, 12, 18, 12,\n        6, 4, 8, 12, 8, 4, 2, 4, 6, 4, 2, 1, 2, 3, 2, 1, 2, 4, 6, 4, 2, 3, 6, 9,\n        6, 3, 2, 4, 6, 4, 2, 1, 2, 3, 2, 1.\n    ]).reshape((5, 5, 5))\n    if with_bias:\n      expected_out += 1\n\n    conv1 = conv_transpose.Conv3DTranspose(\n        output_channels=1,\n        output_shape=None,\n        kernel_shape=3,\n        stride=1,\n        padding=""VALID"",\n        with_bias=with_bias,\n        **create_constant_initializers(1.0, 1.0, with_bias))\n\n    out = conv1(tf.ones([1, 3, 3, 3, 1], dtype=tf.float32))\n    self.assertEqual(out.shape, [1, 5, 5, 5, 1])\n    out = tf.squeeze(out, axis=(0, 4))\n\n    self.assertAllClose(self.evaluate(out), expected_out)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/custom_getter.py,7,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Custom getter for module members.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport contextlib\nfrom sonnet.src import base\nimport tensorflow as tf\nimport tree\nfrom typing import Any, Callable, ContextManager, Iterable, Optional, Type\n\n_DEFAULT_CLASSES = [base.Module]\n\n\n@contextlib.contextmanager\ndef _patch_getattribute(cls, new_getattribute):\n  orig_getattribute = cls.__getattribute__  # pytype: disable=attribute-error\n  cls.__getattribute__ = new_getattribute\n  try:\n    yield\n  finally:\n    cls.__getattribute__ = orig_getattribute\n\n\ndef _custom_getter(\n    getter: Callable[[Any], Any],\n    classes: Optional[Iterable[Type[Any]]] = None,\n    instances: Optional[Iterable[Any]] = None) -> ContextManager[Any]:\n  """"""Applies the given `getter` when getting members of given `classes`.\n\n  For example:\n  >>> class X(object):\n  ...   values = [1, 2]\n\n  >>> x = X()\n  >>> x.values\n  [1, 2]\n\n  >>> with _custom_getter(lambda x: x + [3], classes=[X]):\n  ...   x.values\n  [1, 2, 3]\n\n  >>> with _custom_getter(lambda x: x + [3], instances={x}):\n  ...   x.values\n  [1, 2, 3]\n\n  >>> x.values\n  [1, 2]\n\n  Args:\n    getter: A callable to apply to each element of the class members.\n    classes: The classes in which the getter is applied. If `None`, defaults to\n      `set(o.__class__ for o in instances)`. If `classes and `instances` are\n      both `None`, defaults to `[Module]`.\n    instances: The instances in which the getter is applied. If `None`, the\n      getter will apply in all instances of `classes`.\n\n  Returns:\n    A context manager in which the custom getter is active.\n  """"""\n  # Workaround for the fact that we can\'t annotate the type as `Collection` in\n  # Python < 3.6.\n  if instances is not None:\n    instances = frozenset(instances)\n\n  if classes is None:\n    if instances is None:\n      classes = _DEFAULT_CLASSES\n    else:\n      classes = frozenset(o.__class__ for o in instances)\n\n  stack = contextlib.ExitStack()\n\n  for cls in classes:\n    orig_getattribute = cls.__getattribute__  # pytype: disable=attribute-error\n\n    def new_getattribute(obj, name, orig_getattribute=orig_getattribute):\n      attr = orig_getattribute(obj, name)\n\n      if (instances is None) or (obj in instances):\n        return getter(attr)\n      else:\n        return attr\n\n    stack.enter_context(_patch_getattribute(cls, new_getattribute))\n\n  return stack\n\n\ndef custom_variable_getter(\n    getter: Callable[[tf.Variable], Any],\n    classes: Optional[Iterable[Type[Any]]] = None,\n    instances: Optional[Iterable[Any]] = None) -> ContextManager[Any]:\n  """"""Applies the given `getter` when getting variables of given `classes`.\n\n  If a member is a nested structure containing any variable, `getter` will be\n  applied to each variable in the nest.\n\n  For example:\n  >>> class Times2(snt.Module):\n  ...   def __init__(self):\n  ...     super(Times2, self).__init__()\n  ...     self.v = tf.Variable(2.)\n  ...\n  ...   def __call__(self, x):\n  ...     return x * self.v\n\n  >>> x = 42.\n  >>> times2 = Times2()\n\n  >>> with tf.GradientTape() as tape:\n  ...   y = times2(x)\n  >>> assert tape.gradient(y, times2.v).numpy() == x\n\n  >>> with custom_variable_getter(tf.stop_gradient):\n  ...   with tf.GradientTape() as tape:\n  ...     y = times2(x)\n  >>> assert tape.gradient(y, times2.v) is None\n\n  >>> with tf.GradientTape() as tape:\n  ...   y = times2(x)\n  >>> assert tape.gradient(y, times2.v).numpy() == x\n\n  Args:\n    getter: A callable to apply to each variable of the class.\n    classes: The classes in which the getter is applied. If `None`, defaults to\n      `set(o.__class__ for o in instances)`. If `classes and `instances` are\n      both `None`, defaults to `[Module]`.\n    instances: The instances in which the getter is applied. If `None`, the\n      getter will apply in all instances of `classes`.\n\n  Returns:\n    A context manager in which the custom getter is active.\n  """"""\n\n  def wrapped_getter(x):\n    x_flat = tree.flatten(x)\n    if any(_is_variable(it) for it in x_flat):\n      return tree.unflatten_as(\n          x, [getter(it) if _is_variable(it) else it for it in x_flat])\n    else:\n      return x\n\n  return _custom_getter(wrapped_getter, classes=classes, instances=instances)\n\n\ndef _is_variable(x):\n  return isinstance(x, tf.Variable)\n'"
sonnet/src/custom_getter_test.py,2,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport doctest\n\nfrom sonnet.src import base\nfrom sonnet.src import custom_getter\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\n\nclass CustomVariableGetterTest(test_utils.TestCase):\n\n  def testDoesNotModifyNonVariables(self):\n\n    class MyModule(base.Module):\n      v = tf.Variable(21.)\n      d = {}\n\n    my_module = MyModule()\n    self.assertEqual(21., self.evaluate(my_module.v))\n\n    with custom_getter.custom_variable_getter(lambda v: v * 2):\n      self.assertEqual(42., self.evaluate(my_module.v))\n      my_module.d[""foo""] = ""bar""\n\n    self.assertEqual(21., self.evaluate(my_module.v))\n    self.assertEqual(""bar"", my_module.d[""foo""])\n\n\nclass DoctestTest(test_utils.TestCase):\n\n  def testDoctest(self):\n    num_failed, num_attempted = doctest.testmod(\n        custom_getter, extraglobs={""snt"": base})\n    self.assertGreater(num_attempted, 0, ""No doctests found."")\n    self.assertEqual(num_failed, 0, ""{} doctests failed"".format(num_failed))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/deferred.py,1,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Enables module construction to be deferred.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom sonnet.src import base\n\n\nclass Deferred(base.Module):\n  """"""Defers the construction of another module until the first call.\n\n  Deferred can be used to declare modules that depend on computed properties of\n  other modules before those modules are defined. This allows users to separate\n  the declaration and use of modules. For example at the start of your program\n  you can declare two modules which are coupled:\n\n      >>> encoder = snt.Linear(64)\n      >>> decoder = snt.Deferred(lambda: snt.Linear(encoder.input_size))\n\n  Later you can use these naturally (note: that using `decoder` first would\n  cause an error since `encoder.input_size` is only defined after `encoder` has\n  been called):\n\n      >>> x = tf.ones([8, 32])\n      >>> y = encoder(x)\n      >>> z = decoder(y)  # Constructs the Linear encoder by calling the lambda.\n\n  The result will satisfy the following conditions:\n\n      >>> assert x.shape == z.shape\n      >>> assert y.shape == [8, 64]\n      >>> assert decoder.input_size == encoder.output_size\n      >>> assert decoder.output_size == encoder.input_size\n  """"""\n\n  def __init__(self, constructor, call_methods=(""__call__"",), name=None):\n    """"""Initializes the `Deferred` module.\n\n    Args:\n      constructor: A no argument callable which constructs the module to defer\n        to. The first time one of the `call_methods` are called the constructor\n        will be run and then the constructed module will be called with the same\n        method and arguments as the deferred module.\n      call_methods: Methods which should trigger construction of the target\n        module. The default value configures this module to construct the first\n        time `__call__` is run. If you want to add methods other than call you\n        should explicitly pass them (optionally), for example\n        `call_methods=(""__call__"", ""encode"", ""decode"")`.\n      name: Name for the deferred module.\n    """"""\n    super(Deferred, self).__init__(name=name)\n    self._constructor = constructor\n    self._target = None\n\n    for call_method in call_methods:\n      if call_method == ""__call__"":\n        # Has to be handled separately because __call__ cannot be overridden at\n        # the instance level.\n        # See: https://docs.python.org/3/reference/datamodel.html#special-lookup\n        continue\n\n      setattr(self, call_method, _materialize_then_call(self, call_method))\n\n  @property\n  @base.no_name_scope\n  def target(self):\n    """"""Returns the target module.\n\n    If the constructor has not already run this will trigger construction.\n    Subsequent calls to `target` will return the same instance.\n\n    Returns:\n      A `Module` instance as created by `self.constructor()` .\n    """"""\n    if self._target is None:\n      self._target = self._constructor()\n      self._constructor = None\n    return self._target\n\n  @base.no_name_scope\n  def __call__(self, *args, **kwargs):\n    return self.target(*args, **kwargs)  # pylint: disable=not-callable\n\n  def __str__(self):\n    return ""Deferred({})"".format(str(self.target))\n\n  def __repr__(self):\n    return ""Deferred({})"".format(repr(self.target))\n\n  def __getattr__(self, name):\n    if name != ""_target"" and hasattr(self, ""_target""):\n      if self._target is not None:\n        return getattr(self._target, name)\n\n    raise AttributeError(""\'%s\' object has no attribute \'%s\'"" %\n                         (self.__class__.__name__, name))\n\n  def __setattr__(self, name, value):\n    if name != ""_target"" and hasattr(self, ""_target""):\n      if self._target is not None:\n        setattr(self._target, name, value)\n        return\n\n    super(Deferred, self).__setattr__(name, value)\n\n  def __delattr__(self, name):\n    if name != ""_target"" and hasattr(self, ""_target""):\n      if self._target is not None:\n        return delattr(self._target, name)\n\n    super(Deferred, self).__delattr__(name)\n\n\ndef _materialize_then_call(module, method_name):\n\n  def wrapped(*args, **kwargs):\n    return getattr(module.target, method_name)(*args, **kwargs)\n\n  return wrapped\n'"
sonnet/src/deferred_test.py,4,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.deferred.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom sonnet.src import base\nfrom sonnet.src import deferred\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\n\nclass DeferredTest(test_utils.TestCase):\n\n  def test_target(self):\n    target = ExampleModule()\n    mod = deferred.Deferred(lambda: target)\n    self.assertIs(mod.target, target)\n\n  def test_only_computes_target_once(self):\n    target = ExampleModule()\n    targets = [target]\n    mod = deferred.Deferred(targets.pop)\n    for _ in range(10):\n      # If target was recomputed more than once pop should fail.\n      self.assertIs(mod.target, target)\n      self.assertEmpty(targets)\n\n  def test_attr_forwarding_fails_before_construction(self):\n    mod = deferred.Deferred(ExampleModule)\n    with self.assertRaises(AttributeError):\n      getattr(mod, ""foo"")\n\n  def test_getattr(self):\n    mod = deferred.Deferred(ExampleModule)\n    mod()\n    self.assertIs(mod.w, mod.target.w)\n\n  def test_setattr(self):\n    mod = deferred.Deferred(ExampleModule)\n    mod()\n    new_w = tf.ones_like(mod.w)\n    mod.w = new_w\n    self.assertIs(mod.w, new_w)\n    self.assertIs(mod.target.w, new_w)\n\n  def test_setattr_on_target(self):\n    mod = deferred.Deferred(ExampleModule)\n    mod()\n    w = tf.ones_like(mod.w)\n    mod.w = None\n    # Assigning to the target directly should reflect in the parent.\n    mod.target.w = w\n    self.assertIs(mod.w, w)\n    self.assertIs(mod.target.w, w)\n\n  def test_delattr(self):\n    mod = deferred.Deferred(ExampleModule)\n    mod()\n    self.assertTrue(hasattr(mod.target, ""w""))\n    del mod.w\n    self.assertFalse(hasattr(mod.target, ""w""))\n\n  def test_alternative_forward(self):\n    mod = deferred.Deferred(AlternativeForwardModule, call_methods=(""forward"",))\n    self.assertEqual(mod.forward(), 42)\n\n  def test_alternative_forward_call_type_error(self):\n    mod = deferred.Deferred(AlternativeForwardModule, call_methods=(""forward"",))\n    msg = ""\'AlternativeForwardModule\' object is not callable""\n    with self.assertRaisesRegexp(TypeError, msg):\n      mod()\n\n  def test_name_scope(self):\n    mod = deferred.Deferred(ExampleModule)\n    mod()\n    self.assertEqual(mod.name_scope.name, ""deferred/"")\n    self.assertEqual(mod.target.name_scope.name, ""example_module/"")\n\n  def test_str(self):\n    m = ExampleModule()\n    d = deferred.Deferred(lambda: m)\n    self.assertEqual(""Deferred(%s)"" % m, str(d))\n\n  def test_repr(self):\n    m = ExampleModule()\n    d = deferred.Deferred(lambda: m)\n    self.assertEqual(""Deferred(%r)"" % m, repr(d))\n\n\nclass ExampleModule(base.Module):\n\n  def __init__(self):\n    super(ExampleModule, self).__init__()\n    self.w = tf.Variable(1.)\n\n  def __str__(self):\n    return ""ExampleModuleStr""\n\n  def __repr__(self):\n    return ""ExampleModuleRepr""\n\n  def __call__(self):\n    return self.w\n\n\nclass AlternativeForwardModule(base.Module):\n\n  def forward(self):\n    return 42\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/depthwise_conv.py,7,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n\n""""""Depth-wise convolutional module.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport numpy as np\nfrom sonnet.src import base\nfrom sonnet.src import initializers\nfrom sonnet.src import once\nfrom sonnet.src import utils\n\nimport tensorflow as tf\nfrom typing import Optional, Sequence, Text, Union\n\n\nclass DepthwiseConv2D(base.Module):\n  """"""Spatial depth-wise 2D convolution module, including bias.\n\n  This acts as a light wrapper around the TensorFlow ops\n  `tf.nn.depthwise_conv2d`, abstracting away variable creation and sharing.\n  """"""\n\n  def __init__(self,\n               kernel_shape: Union[int, Sequence[int]],\n               channel_multiplier: int = 1,\n               stride: Union[int, Sequence[int]] = 1,\n               rate: Union[int, Sequence[int]] = 1,\n               padding: Text = ""SAME"",\n               with_bias: bool = True,\n               w_init: Optional[initializers.Initializer] = None,\n               b_init: Optional[initializers.Initializer] = None,\n               data_format: Text = ""NHWC"",\n               name: Optional[Text] = None):\n    """"""Constructs a `DepthwiseConv2D` module.\n\n    Args:\n      kernel_shape: Sequence of kernel sizes (of length num_spatial_dims), or an\n        integer. `kernel_shape` will be expanded to define a kernel size in\n        all dimensions.\n      channel_multiplier: Number of channels to expand convolution to. Must be\n          an integer greater than 0. When `channel_multiplier` is 1, applies\n          a different filter to each input channel producing one output channel\n          per input channel. Numbers larger than 1 cause multiple different\n          filters to be applied to each input channel, with their outputs being\n          concatenated together, producing `channel_multiplier` *\n          `input_channels` output channels.\n      stride: Sequence of strides (of length num_spatial_dims), or an integer.\n        `stride` will be expanded to define stride in all dimensions.\n      rate: Sequence of dilation rates (of length num_spatial_dims), or integer\n        that is used to define dilation rate in all dimensions. 1 corresponds\n        to standard ND convolution, `rate > 1` corresponds to dilated\n        convolution.\n      padding: Padding to apply to the input. This can either ""SAME"", ""VALID"".\n      with_bias: Whether to include bias parameters. Default `True`.\n      w_init: Optional initializer for the weights. By default the weights are\n        initialized truncated random normal values with a standard deviation of\n        `1 / sqrt(input_feature_size)`, which is commonly used when the inputs\n        are zero centered (see https://arxiv.org/abs/1502.03167v3).\n      b_init: Optional initializer for the bias. By default the bias is\n        initialized to zero.\n      data_format: The data format of the input.\n      name: Name of the module.\n    """"""\n    super(DepthwiseConv2D, self).__init__(name=name)\n\n    self.channel_multiplier = channel_multiplier\n    self.kernel_shape = kernel_shape\n    self.data_format = data_format\n    self._channel_index = utils.get_channel_index(data_format)\n    stride = utils.replicate(stride, 2, ""stride"")\n    if self._channel_index == 1:\n      self.stride = (1, 1) + stride\n    else:\n      self.stride = (1,) + stride + (1,)\n    self.rate = utils.replicate(rate, 2, ""rate"")\n    self.padding = padding\n\n    self.with_bias = with_bias\n    self.w_init = w_init\n    if with_bias:\n      self.b_init = b_init if b_init is not None else initializers.Zeros()\n    elif b_init is not None:\n      raise ValueError(""When not using a bias the b_init must be None."")\n\n  def __call__(self, inputs: tf.Tensor) -> tf.Tensor:\n    self._initialize(inputs)\n\n    outputs = tf.nn.depthwise_conv2d(inputs,\n                                     self.w,\n                                     strides=self.stride,\n                                     dilations=self.rate,\n                                     padding=self.padding,\n                                     data_format=self.data_format)\n    if self.with_bias:\n      outputs = tf.nn.bias_add(outputs, self.b,\n                               data_format=self.data_format)\n\n    return outputs\n\n  @once.once\n  def _initialize(self, inputs: tf.Tensor):\n    self.input_channels = inputs.shape[self._channel_index]\n    if self.input_channels is None:\n      raise ValueError(""The number of input channels must be known."")\n    dtype = inputs.dtype\n\n    weight_shape = utils.replicate(self.kernel_shape, 2, ""kernel_shape"")\n    weight_shape = weight_shape + (self.input_channels, self.channel_multiplier)\n    if self.w_init is None:\n      # See https://arxiv.org/abs/1502.03167v3.\n      fan_in_shape = weight_shape[:2]\n      stddev = 1 / np.sqrt(np.prod(fan_in_shape))\n      self.w_init = initializers.TruncatedNormal(stddev=stddev)\n    self.w = tf.Variable(self.w_init(weight_shape, dtype), name=""w"")\n\n    output_channels = self.input_channels * self.channel_multiplier\n    if self.with_bias:\n      self.b = tf.Variable(self.b_init((output_channels,), dtype), name=""b"")\n'"
sonnet/src/depthwise_conv_test.py,28,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.depthwise_conv.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nfrom sonnet.src import depthwise_conv\nfrom sonnet.src import initializers\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\n\ndef create_constant_initializers(w, b, with_bias):\n  if with_bias:\n    return {\n        ""w_init"": initializers.Constant(w),\n        ""b_init"": initializers.Constant(b)\n    }\n  else:\n    return {""w_init"": initializers.Constant(w)}\n\n\nclass DepthwiseConvTest(test_utils.TestCase, parameterized.TestCase):\n\n  def testInitializerKeysInvalidWithoutBias(self):\n    with self.assertRaisesRegexp(ValueError, ""b_init must be None""):\n      depthwise_conv.DepthwiseConv2D(\n          channel_multiplier=1,\n          kernel_shape=3,\n          data_format=""NHWC"",\n          with_bias=False,\n          b_init=tf.zeros_initializer())\n\n  @parameterized.parameters(tf.float32, tf.float64)\n  def testDefaultInitializers(self, dtype):\n    if ""TPU"" in self.device_types and dtype == tf.float64:\n      self.skipTest(""Double precision not supported on TPU."")\n\n    conv1 = depthwise_conv.DepthwiseConv2D(\n        kernel_shape=16, stride=1, padding=""VALID"", data_format=""NHWC"")\n\n    out = conv1(tf.random.normal([8, 64, 64, 1], dtype=dtype))\n\n    self.assertAllEqual(out.shape, [8, 49, 49, 1])\n    self.assertEqual(out.dtype, dtype)\n\n    # Note that for unit variance inputs the output is below unit variance\n    # because of the use of the truncated normal initalizer\n    err = 0.2 if self.primary_device == ""TPU"" else 0.1\n    self.assertNear(out.numpy().std(), 0.87, err=err)\n\n  @parameterized.named_parameters((""SamePaddingUseBias"", True, ""SAME""),\n                                  (""SamePaddingNoBias"", False, ""SAME""),\n                                  (""ValidPaddingNoBias"", False, ""VALID""),\n                                  (""ValidPaddingUseBias"", True, ""VALID""))\n  def testFunction(self, with_bias, padding):\n    conv1 = depthwise_conv.DepthwiseConv2D(\n        channel_multiplier=1,\n        kernel_shape=3,\n        stride=1,\n        padding=padding,\n        with_bias=with_bias,\n        data_format=""NHWC"",\n        **create_constant_initializers(1.0, 1.0, with_bias))\n    conv2 = depthwise_conv.DepthwiseConv2D(\n        channel_multiplier=1,\n        kernel_shape=3,\n        stride=1,\n        padding=padding,\n        with_bias=with_bias,\n        data_format=""NHWC"",\n        **create_constant_initializers(1.0, 1.0, with_bias))\n    defun_conv = tf.function(conv2)\n\n    iterations = 5\n\n    for _ in range(iterations):\n      x = tf.random.uniform([1, 5, 5, 1])\n      y1 = conv1(x)\n      y2 = defun_conv(x)\n\n      self.assertAllClose(self.evaluate(y1), self.evaluate(y2), atol=1e-4)\n\n  def testUnknownBatchSizeNHWC(self):\n    x = tf.TensorSpec([None, 5, 5, 3], dtype=tf.float32)\n\n    c = depthwise_conv.DepthwiseConv2D(\n        channel_multiplier=1, kernel_shape=3, data_format=""NHWC"")\n    defun_conv = tf.function(c).get_concrete_function(x)\n\n    out1 = defun_conv(tf.ones([3, 5, 5, 3]))\n    self.assertEqual(out1.shape, [3, 5, 5, 3])\n\n    out2 = defun_conv(tf.ones([5, 5, 5, 3]))\n    self.assertEqual(out2.shape, [5, 5, 5, 3])\n\n  def testUnknownBatchSizeNCHW(self):\n    if self.primary_device == ""CPU"":\n      self.skipTest(""NCHW not supported on CPU"")\n\n    x = tf.TensorSpec([None, 3, 5, 5], dtype=tf.float32)\n    c = depthwise_conv.DepthwiseConv2D(\n        channel_multiplier=1, kernel_shape=3, data_format=""NCHW"")\n    defun_conv = tf.function(c).get_concrete_function(x)\n\n    out1 = defun_conv(tf.ones([3, 3, 5, 5]))\n    self.assertEqual(out1.shape, [3, 3, 5, 5])\n\n    out2 = defun_conv(tf.ones([5, 3, 5, 5]))\n    self.assertEqual(out2.shape, [5, 3, 5, 5])\n\n  def testUnknownSpatialDims(self):\n    x = tf.TensorSpec([3, None, None, 3], dtype=tf.float32)\n\n    c = depthwise_conv.DepthwiseConv2D(\n        channel_multiplier=1, kernel_shape=3, data_format=""NHWC"")\n    defun_conv = tf.function(c).get_concrete_function(x)\n\n    out = defun_conv(tf.ones([3, 5, 5, 3]))\n    expected_out = c(tf.ones([3, 5, 5, 3]))\n    self.assertEqual(out.shape, [3, 5, 5, 3])\n    self.assertAllEqual(self.evaluate(out), self.evaluate(expected_out))\n\n    out = defun_conv(tf.ones([3, 4, 4, 3]))\n    expected_out = c(tf.ones([3, 4, 4, 3]))\n    self.assertEqual(out.shape, [3, 4, 4, 3])\n    self.assertAllEqual(self.evaluate(out), self.evaluate(expected_out))\n\n  @parameterized.parameters(True, False)\n  def testUnknownChannels(self, autograph):\n    x = tf.TensorSpec([3, 3, 3, None], dtype=tf.float32)\n\n    c = depthwise_conv.DepthwiseConv2D(\n        channel_multiplier=1, kernel_shape=3, data_format=""NHWC"")\n    defun_conv = tf.function(c, autograph=autograph)\n\n    with self.assertRaisesRegex(ValueError,\n                                ""The number of input channels must be known""):\n      defun_conv.get_concrete_function(x)\n\n  @parameterized.named_parameters((""WithBias"", True), (""WithoutBias"", False))\n  def testComputationSame(self, with_bias):\n    conv1 = depthwise_conv.DepthwiseConv2D(\n        channel_multiplier=1,\n        kernel_shape=[3, 3],\n        stride=1,\n        padding=""SAME"",\n        with_bias=with_bias,\n        **create_constant_initializers(1.0, 1.0, with_bias))\n\n    out = conv1(tf.ones([1, 5, 5, 1]))\n    expected_out = np.array([[5, 7, 7, 7, 5], [7, 10, 10, 10, 7],\n                             [7, 10, 10, 10, 7], [7, 10, 10, 10, 7],\n                             [5, 7, 7, 7, 5]])\n    if not with_bias:\n      expected_out -= 1\n\n    self.assertEqual(out.shape, [1, 5, 5, 1])\n    self.assertAllClose(np.reshape(out.numpy(), [5, 5]), expected_out)\n\n  @parameterized.named_parameters((""WithBias"", True), (""WithoutBias"", False))\n  def testComputationValid(self, with_bias):\n    conv1 = depthwise_conv.DepthwiseConv2D(\n        channel_multiplier=1,\n        kernel_shape=[3, 3],\n        stride=1,\n        padding=""VALID"",\n        with_bias=with_bias,\n        **create_constant_initializers(1.0, 1.0, with_bias))\n\n    out = conv1(tf.ones([1, 5, 5, 1]))\n    expected_out = np.array([[10, 10, 10], [10, 10, 10], [10, 10, 10]])\n    if not with_bias:\n      expected_out -= 1\n\n    self.assertEqual(out.shape, [1, 3, 3, 1])\n    self.assertAllClose(np.reshape(out.numpy(), [3, 3]), expected_out)\n\n  @parameterized.named_parameters((""WithBias"", True), (""WithoutBias"", False))\n  def testComputationValidMultiChannel(self, with_bias):\n    conv1 = depthwise_conv.DepthwiseConv2D(\n        channel_multiplier=1,\n        kernel_shape=[3, 3],\n        stride=1,\n        padding=""VALID"",\n        with_bias=with_bias,\n        **create_constant_initializers(1.0, 1.0, with_bias))\n\n    out = conv1(tf.ones([1, 5, 5, 3]))\n    expected_out = np.array([[[10] * 3] * 3] * 3)\n    if not with_bias:\n      expected_out -= 1\n\n    self.assertAllClose(np.reshape(out.numpy(), [3, 3, 3]), expected_out)\n\n  @parameterized.named_parameters((""WithBias"", True), (""WithoutBias"", False))\n  def testSharing(self, with_bias):\n    """"""Sharing is working.""""""\n    conv1 = depthwise_conv.DepthwiseConv2D(\n        channel_multiplier=3,\n        kernel_shape=3,\n        stride=1,\n        padding=""SAME"",\n        with_bias=with_bias)\n\n    x = np.random.randn(1, 5, 5, 1)\n    x1 = tf.constant(x, dtype=np.float32)\n    x2 = tf.constant(x, dtype=np.float32)\n\n    self.assertAllClose(conv1(x1), conv1(x2))\n\n    # Kernel shape was set to 3, which is expandeded to [3, 3, 3].\n    # Input channels are 1, output channels := in_channels * multiplier.\n    # multiplier is kernel_shape[2] == 3. So weight layout must be:\n    # (3, 3, 1, 3).\n    w = np.random.randn(3, 3, 1, 3)  # Now change the weights.\n    conv1.w.assign(w)\n    self.assertAllClose(conv1(x1), conv1(x2))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/dropout.py,2,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n\n""""""Sonnet dropout modules.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nfrom sonnet.src import base\nfrom sonnet.src import types\nfrom sonnet.src import utils\n\nimport tensorflow as tf\nfrom typing import Optional, Text\n\n\nclass Dropout(base.Module):\n  """"""Randomly drop units in the input at a given rate.\n\n  See: http://www.cs.toronto.edu/~hinton/absps/dropout.pdf\n\n  Dropout was originally described by Hinton et al. TensorFlow deviates slightly\n  from this paper by scaling activations at training time rather than test time.\n  """"""\n\n  def __init__(self,\n               rate: types.FloatLike,\n               noise_shape: Optional[types.ShapeLike] = None,\n               seed: Optional[int] = None,\n               name: Optional[Text] = None):\n    """"""Constructs a Dropout module.\n\n    Args:\n      rate: Probability that each element of x is discarded. Must be a scalar in\n        the range `[0, 1)`.\n      noise_shape: (Optional) Shape vector controlling the shape of the random\n        noise used to apply dropout. If not set this will be the shape of the\n        input. If set it should be broadcastable to the input shape.\n      seed: (Optional) Random seed to be passed to TensorFlow ops when\n        generating dropout tensor.\n      name: (Optional) Name for this module.\n    """"""\n    super(Dropout, self).__init__(name=name)\n    self._rate = rate\n    self._noise_shape = noise_shape\n    self._seed = seed\n\n  @utils.smart_autograph\n  def __call__(self, x: tf.Tensor, is_training: types.BoolLike) -> tf.Tensor:\n    if not is_training:\n      return x\n\n    # NOTE: Even if `self._seed` is a constant value (e.g. `2`) this will\n    # produce a different random dropout each call (the per-op seed is used\n    # in conjunction with the global seed and some persistent state to produce\n    # random values).\n    # c.f. https://www.tensorflow.org/api_docs/python/tf/random/set_random_seed\n    return tf.nn.dropout(\n        x, rate=self._rate, noise_shape=self._noise_shape, seed=self._seed)\n'"
sonnet/src/dropout_test.py,17,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.dropout.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom sonnet.src import dropout\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\n\nclass DropoutTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters(np.arange(.0, .9, .1))\n  def test_sum_close(self, rate):\n    mod = dropout.Dropout(rate=rate)\n    x = tf.ones([1000])\n    rtol = 0.3 if ""TPU"" in self.device_types else 0.1\n    self.assertAllClose(\n        tf.reduce_sum(mod(x, is_training=True)),\n        tf.reduce_sum(mod(x, is_training=False)),\n        rtol=rtol)\n\n  @parameterized.parameters(np.arange(0, .9, .1))\n  def test_dropout_rate(self, rate):\n    mod = dropout.Dropout(rate=rate)\n    x = tf.ones([1000])\n    x = mod(x, is_training=True)\n\n    # We should have dropped something, test we\'re within 10% of rate.\n    # (or 30% on a TPU)\n    rtol = 0.3 if ""TPU"" in self.device_types else 0.1\n    kept = tf.math.count_nonzero(x).numpy()\n    keep_prob = 1 - rate\n    self.assertAllClose(kept, 1000 * keep_prob, rtol=rtol)\n\n  def test_dropout_is_actually_random(self):\n    mod = dropout.Dropout(rate=0.5)\n    x = tf.ones([1000])\n    tf.random.set_seed(1)\n    y1 = mod(x, is_training=True)\n    y2 = mod(x, is_training=True)\n    self.assertNotAllClose(y1, y2)\n\n  @parameterized.parameters(True, False)\n  def test_with_tf_function_with_booleans(self, autograph):\n    """"""tf.function compilation correctly handles if statement.""""""\n\n    layer = dropout.Dropout(rate=0.5)\n    layer = tf.function(layer, autograph=autograph)\n\n    inputs = tf.ones([2, 5, 3, 3, 3])\n    expected = tf.zeros_like(inputs)\n\n    for is_training in (True, False):\n      outputs = layer(inputs, is_training)\n      self.assertEqual(outputs.shape, expected.shape)\n\n  @parameterized.parameters(True, False)\n  def test_with_tf_function_with_variables(self, autograph):\n    """"""tf.function correctly handles if statement when argument is Variable.""""""\n\n    layer = dropout.Dropout(rate=0.5)\n    layer = tf.function(layer, autograph=autograph)\n\n    inputs = tf.ones([2, 5, 3, 3, 3])\n    expected = tf.zeros_like(inputs)\n    is_training_variable = tf.Variable(False, trainable=False)\n\n    for is_training in (True, False):\n      is_training_variable.assign(is_training)\n      outputs = layer(inputs, is_training_variable)\n      self.assertEqual(outputs.shape, expected.shape)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/embed.py,16,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n\n""""""Embedding module.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport math\n\nfrom sonnet.src import base\nfrom sonnet.src import initializers\nfrom sonnet.src import types\n\nimport tensorflow as tf\nfrom typing import Optional, Text\n\n\nclass Embed(base.Module):\n  """"""Module for embedding tokens in a low-dimensional space.""""""\n\n  def __init__(self,\n               vocab_size: Optional[int] = None,\n               embed_dim: Optional[int] = None,\n               existing_vocab: Optional[types.TensorLike] = None,\n               densify_gradients: bool = False,\n               initializer: Optional[initializers.Initializer] = None,\n               trainable: bool = True,\n               dtype: tf.DType = tf.float32,\n               name: Optional[Text] = None):\n    """"""Constructs an Embed module.\n\n    Args:\n      vocab_size: Number of unique tokens to embed. If not provided, an\n        existing vocabulary matrix from which vocab_size can be inferred must\n        be provided as existing_vocab.\n      embed_dim: Number of dimensions to assign to each embedding.\n        If not specified, we use ``6 * sqrt(sqrt(vocab_size))``. If an existing\n        vocabulary matrix initializes the module, this should not be provided as\n        it will be inferred.\n      existing_vocab: A ``[vocab_size, embed_dim]`` vocabulary matrix. Will be\n        converted to a tf.float32 tensor. If provided, neither or vocab_size or\n        embed_dim should be provided as they are inferred.\n      densify_gradients: If True, we convert the embedding gradient from an\n        ``tf.IndexedSlices`` to a regular tensor before sending it back to the\n        parameter server. This avoids excess computation on the parameter\n        server. Use this option for moderately sized embeddings, e.g.,\n        a vocabulary size on the order of up to thousands. For embeddings larger\n        than these, e.g. a vocabulary size on the order of tens or hundreds of\n        thousands, set this to False.\n      initializer: Initializer for the embeddings. By default,\n        embeddings are initialized via a truncated normal distribution.\n      trainable: if True, the embeddings will be updated during training. If\n        False, they are fixed to their initial values.\n      dtype: The dtype to use for the embedding. Defaults to float32.\n      name: Name for this module.\n\n    Raises:\n      ValueError: if neither one of ``vocab_size`` or ``existing_vocab`` is\n        provided, or if ``existing_vocab`` is provided along with\n        ``vocab_size``, ``embedding_dim``, ``initializer`` (as these should be\n        inferred).\n    """"""\n    super(Embed, self).__init__(name=name)\n\n    if vocab_size is None and existing_vocab is None:\n      raise ValueError(""Must provide one of vocab_size or existing_vocab."")\n\n    if existing_vocab is not None and (vocab_size or embed_dim or initializer):\n      raise ValueError(""If `existing_vocab` is provided, none of `vocab_size`, ""\n                       ""`embedding_dim`, `initializer` are needed."")\n\n    if existing_vocab is None:\n      if embed_dim is None:\n        embed_dim = embedding_dim(vocab_size)\n      if initializer is None:\n        initializer = initializers.TruncatedNormal()\n      vocab = initializer([vocab_size, embed_dim], dtype)\n    else:\n      existing_vocab = tf.convert_to_tensor(existing_vocab, dtype=dtype)\n      vocab_size, embed_dim = existing_vocab.shape\n      vocab = existing_vocab\n\n    self.vocab_size = vocab_size\n    self.embed_dim = embed_dim\n    self.densify_gradients = densify_gradients\n    self.embeddings = tf.Variable(vocab, trainable=trainable, name=""embeddings"")\n\n  def __call__(self, inputs):\n    if self.densify_gradients:\n      embeddings = dense_gradient(self.embeddings)\n    else:\n      embeddings = self.embeddings\n    return tf.nn.embedding_lookup(embeddings, inputs)\n\n\ndef embedding_dim(vocab_size: int):\n  """"""Calculate a reasonable embedding size for a vocabulary.\n\n  Rule of thumb is ``6 * sqrt(sqrt(vocab_size))``.\n\n  Args:\n    vocab_size: Size of the input vocabulary.\n\n  Returns:\n    The embedding size to use.\n\n  Raises:\n    ValueError: if ``vocab_size`` is invalid.\n  """"""\n  if not vocab_size or (vocab_size <= 0):\n    raise ValueError(""Invalid vocab_size %g."" % vocab_size)\n  return int(round(6.0 * math.sqrt(math.sqrt(vocab_size))))\n\n\n@tf.custom_gradient\ndef dense_gradient(x: tf.Tensor):\n  """"""Identity operation whose gradient is converted to a ``tf.Tensor``.\n\n  >>> embedding = tf.Variable(tf.random.normal([3, 3]))\n  >>> with tf.GradientTape() as tape:\n  ...   y = tf.nn.embedding_lookup(dense_gradient(embedding), [1])\n  >>> tape.gradient(y, embedding).numpy()\n  array([[ 0.,  0.,  0.],\n         [ 1.,  1.,  1.],\n         [ 0.,  0.,  0.]], dtype=float32)\n\n  Args:\n    x: A ``tf.Tensor``.\n\n  Returns:\n    The input ``tf.Tensor`` and a dense identity gradient function.\n  """"""\n  def grad(dy):\n    if isinstance(dy, tf.IndexedSlices):\n      return tf.convert_to_tensor(dy)\n    else:\n      return dy\n\n  return x, grad\n'"
sonnet/src/embed_test.py,9,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.embed.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom sonnet.src import embed\nfrom sonnet.src import initializers\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\n\nclass EmbedTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters([1, 10, 100])\n  def test_vocab_size(self, vocab_size):\n    e = embed.Embed(vocab_size=vocab_size)\n    self.assertEqual(e.vocab_size, vocab_size)\n    self.assertEqual(e.embeddings.shape[0], vocab_size)\n\n  @parameterized.parameters([1, 10, 100])\n  def test_embed_dim(self, embed_dim):\n    e = embed.Embed(vocab_size=100, embed_dim=embed_dim)\n    self.assertEqual(e.embed_dim, embed_dim)\n    self.assertEqual(e.embeddings.shape[1], embed_dim)\n\n  @parameterized.parameters([(1, 1), (10, 10), (100, 100)])\n  def test_existing_vocab(self, vocab_size, embed_dim):\n    existing_vocab = tf.ones([vocab_size, embed_dim])\n    e = embed.Embed(existing_vocab=existing_vocab)\n    self.assertEqual(e.vocab_size, vocab_size)\n    self.assertEqual(e.embed_dim, embed_dim)\n    self.assertAllEqual(e.embeddings.read_value(), existing_vocab)\n\n  @parameterized.parameters([True, False])\n  def test_densify_gradients(self, densify_gradients):\n    e = embed.Embed(1, densify_gradients=densify_gradients)\n    with tf.GradientTape() as tape:\n      y = e([0])\n      dy = tape.gradient(y, e.embeddings)\n    if densify_gradients:\n      self.assertIsInstance(dy, tf.Tensor)\n    else:\n      self.assertIsInstance(dy, tf.IndexedSlices)\n\n  def test_initializer(self):\n    e = embed.Embed(1, 1, initializer=initializers.Constant(28.))\n    self.assertAllEqual(e.embeddings.read_value(), [[28.]])\n\n  def test_pinned_to_cpu(self):\n    with tf.device(""CPU""):\n      e = embed.Embed(1)\n    spec = tf.DeviceSpec.from_string(e.embeddings.device)\n    self.assertEqual(spec.device_type, ""CPU"")\n\n  @parameterized.parameters([True, False])\n  def test_trainable(self, trainable):\n    e = embed.Embed(1, trainable=trainable)\n    self.assertEqual(e.embeddings.trainable, trainable)\n\n  @parameterized.parameters([tf.float32, tf.float16])\n  def test_dtype(self, dtype):\n    if dtype == tf.float16 and self.primary_device == ""TPU"":\n      self.skipTest(""float16 embeddings not supported on TPU."")\n    e = embed.Embed(1, dtype=dtype)\n    self.assertEqual(e.embeddings.dtype, dtype)\n\n  def test_name(self):\n    e = embed.Embed(1, name=""my_embedding"")\n    self.assertEqual(e.name, ""my_embedding"")\n    self.assertEqual(e.embeddings.name, ""my_embedding/embeddings:0"")\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/group_norm.py,10,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Group normalization implementation for Sonnet.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport collections\nimport six\n\nfrom sonnet.src import base\nfrom sonnet.src import initializers\nfrom sonnet.src import once\nfrom sonnet.src import types\nfrom sonnet.src import utils\n\nimport tensorflow as tf\nfrom typing import Optional, Text\n\n\nclass GroupNorm(base.Module):\n  r""""""Group normalization module.\n\n  This applies group normalization to the inputs. This involves splitting the\n  channels into groups before calculating the mean and variance. The default\n  behaviour is to compute the mean and variance over the spatial dimensions and\n  the grouped channels. The mean and variance will never be computed over the\n  created groups axis.\n\n  It transforms the input ``x`` into:\n\n  .. math::\n\n     \\d{outputs} = \\d{scale} \\dfrac{x - \\mu}{\\sigma + \\epsilon} + \\d{offset}\n\n  Where :math:`\\mu` and :math:`\\sigma` are respectively the mean and standard\n  deviation of ``x``.\n\n  There are many different variations for how users want to manage scale and\n  offset if they require them at all. These are:\n\n    - No ``scale``/``offset`` in which case ``create_*`` should be set to\n      ``False`` and ``scale``/``offset`` aren\'t passed when the module is\n      called.\n    - Trainable ``scale``/``offset`` in which case create_* should be set to\n      ``True`` and again ``scale``/``offset`` aren\'t passed when the module is\n      called. In this case this module creates and owns the scale/offset\n      variables.\n    - Externally generated ``scale``/``offset``, such as for conditional\n      normalization, in which case ``create_*`` should be set to ``False`` and\n      then the values fed in at call time.\n\n  Attributes:\n    scale: If ``create_scale=True``, a trainable :tf:`Variable` holding the\n      current scale.\n    offset: If ``create_offset=True``, a trainable :tf:`Variable` holding the\n      current offset.\n  """"""\n\n  def __init__(self,\n               groups: int,\n               axis: types.Axis = slice(1, None),\n               create_scale: bool = True,\n               create_offset: bool = True,\n               eps: types.FloatLike = 1e-5,\n               scale_init: Optional[initializers.Initializer] = None,\n               offset_init: Optional[initializers.Initializer] = None,\n               data_format: Text = ""channels_last"",\n               name: Optional[Text] = None):\n    """"""Constructs a ``GroupNorm`` module.\n\n    Args:\n      groups: number of groups to divide the channels by. The number of channels\n        must be divisible by this.\n      axis: ``int``, ``slice`` or sequence of ints representing the axes which\n        should be normalized across. By default this is all but the first\n        dimension. For time series data use `slice(2, None)` to average over the\n        none Batch and Time data.\n      create_scale: whether to create a trainable scale per channel applied\n        after the normalization.\n      create_offset: whether to create a trainable offset per channel applied\n        after normalization and scaling.\n      eps: Small epsilon to add to the variance to avoid division by zero.\n        Defaults to ``1e-5``.\n      scale_init: Optional initializer for the scale variable. Can only be set\n        if ``create_scale=True``. By default scale is initialized to ``1``.\n      offset_init: Optional initializer for the offset variable. Can only be set\n        if ``create_offset=True``. By default offset is initialized to ``0``.\n      data_format: The data format of the input. Can be either\n        ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By\n        default it is ``channels_last``.\n      name: Name of the module.\n    """"""\n    super(GroupNorm, self).__init__(name=name)\n\n    if isinstance(axis, slice):\n      self._axis = axis\n    elif isinstance(axis, six.integer_types):\n      self._axis = [axis]\n    elif (isinstance(axis, collections.Iterable) and\n          all(isinstance(ax, six.integer_types) for ax in axis)):\n      self._axis = axis\n    else:\n      raise ValueError(""`axis` should be an int, slice or iterable of ints."")\n\n    self._groups = groups\n    self._eps = eps\n\n    self._data_format = data_format\n    self._channel_index = utils.get_channel_index(data_format)\n\n    self._create_scale = create_scale\n    self._create_offset = create_offset\n\n    if self._create_scale:\n      self._scale_init = (\n          scale_init if scale_init is not None else initializers.Ones())\n    elif scale_init is not None:\n      raise ValueError(""Cannot set `scale_init` if `create_scale=False`."")\n    if self._create_offset:\n      self._offset_init = (\n          offset_init if offset_init is not None else initializers.Zeros())\n    elif offset_init is not None:\n      raise ValueError(""Cannot set `offset_init` if `create_offset=False`."")\n\n  def __call__(self,\n               inputs: tf.Tensor,\n               scale: Optional[tf.Tensor] = None,\n               offset: Optional[tf.Tensor] = None):\n    """"""Returns normalized inputs.\n\n    Args:\n      inputs: An n-D tensor of the ``data_format`` specified in the constructor\n        on which the transformation is performed.\n      scale: A tensor up to n-D. The shape of this tensor must be broadcastable\n        to the shape of ``inputs``. This is the scale applied to the normalized\n        inputs. This cannot be passed in if the module was constructed with\n        ``create_scale=True``.\n      offset: A tensor up to n-D. The shape of this tensor must be broadcastable\n        to the shape of ``inputs``. This is the offset applied to the normalized\n        ``inputs``. This cannot be passed in if the module was constructed with\n        ``create_offset=True``.\n\n    Returns:\n      An n-d tensor of the same shape as inputs that has been normalized.\n    """"""\n    self._initialize(inputs)\n    if self._create_scale:\n      if scale is not None:\n        raise ValueError(\n            ""Cannot pass `scale` at call time if `create_scale=True`."")\n      scale = self.scale\n\n    if self._create_offset:\n      if offset is not None:\n        raise ValueError(\n            ""Cannot pass `offset` at call time if `create_offset=True`."")\n      offset = self.offset\n\n    if len(inputs.shape) != self._rank:\n      raise ValueError(\n          ""The rank of the inputs cannot change between calls, the""\n          "" original call was rank={} but this call was rank={}."".format(\n              self._rank, len(inputs.shape)))\n\n    inputs = tf.reshape(inputs, self._inputs_reshape)\n    mean, var = tf.nn.moments(inputs, self._axis, keepdims=True)\n\n    normalized = tf.nn.batch_normalization(\n        inputs,\n        mean=mean,\n        variance=var,\n        scale=None,\n        offset=None,\n        variance_epsilon=self._eps)\n    outputs = tf.reshape(normalized, self._outputs_reshape)\n    outputs = outputs * scale if scale is not None else outputs\n    outputs = outputs + offset if offset is not None else outputs\n    return outputs\n\n  @once.once\n  def _initialize(self, inputs: tf.Tensor):\n    """"""Setup of rank specific values.""""""\n    self._rank = len(inputs.shape)\n\n    # Turns slice into list of axis\n    if isinstance(self._axis, slice):\n      axes = tuple(range(self._rank))\n      self._axis = axes[self._axis]\n\n    # Create scale and offset variables\n    dtype = inputs.dtype\n    if self._channel_index == -1:\n      params_shape = [inputs.shape[-1]]\n    else:  # self._channel_index == 1\n      params_shape = [inputs.shape[1]] + [1] * (self._rank - 2)\n\n    if self._create_scale:\n      self.scale = tf.Variable(\n          self._scale_init(params_shape, dtype), name=""scale"")\n    else:\n      self.scale = None\n\n    if self._create_offset:\n      self.offset = tf.Variable(\n          self._offset_init(params_shape, dtype), name=""offset"")\n    else:\n      self.offset = None\n\n    num_channels = inputs.shape[self._channel_index]\n    if num_channels % self._groups != 0:\n      raise ValueError(\n          ""The number of channels must be divisible by the number of groups, ""\n          ""was channels = {}, groups = {}"".format(num_channels, self._groups))\n    if self._channel_index == -1:\n      self._inputs_reshape = [-1] + list(\n          inputs.shape[1:-1]) + [self._groups, num_channels // self._groups]\n      self._axis = [a if a != self._rank - 1 else a + 1 for a in self._axis]\n    else:\n      self._inputs_reshape = [-1] + [\n          self._groups, num_channels // self._groups\n      ] + list(inputs.shape[2:])\n      self._axis = [a if a == 0 else a + 1 for a in self._axis]\n    self._outputs_reshape = [-1] + list(inputs.shape[1:])\n'"
sonnet/src/group_norm_test.py,57,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.group_norm.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom sonnet.src import group_norm\nfrom sonnet.src import initializers\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\n\nclass GroupNormTest(test_utils.TestCase, parameterized.TestCase):\n\n  def testSimpleCase(self):\n    layer = group_norm.GroupNorm(\n        groups=5, create_scale=False, create_offset=False)\n    inputs = tf.ones([2, 3, 3, 10])\n\n    outputs = layer(inputs).numpy()\n    for x in np.nditer(outputs):\n      self.assertEqual(x, 0.0)\n\n  def testSimpleCaseVar(self):\n    layer = group_norm.GroupNorm(\n        groups=5,\n        create_scale=True,\n        create_offset=True,\n        scale_init=initializers.Constant(0.5),\n        offset_init=initializers.Constant(2.0))\n\n    inputs = tf.ones([2, 3, 3, 10])\n\n    outputs = layer(inputs).numpy()\n    for x in np.nditer(outputs):\n      self.assertEqual(x, 2.0)\n\n  def testSimpleCaseNCHWVar(self):\n    layer = group_norm.GroupNorm(\n        groups=5,\n        create_scale=True,\n        create_offset=True,\n        scale_init=initializers.Constant(0.5),\n        offset_init=initializers.Constant(2.0),\n        data_format=""NCHW"")\n\n    inputs = tf.ones([2, 10, 3, 3])\n\n    outputs = layer(inputs).numpy()\n    for x in np.nditer(outputs):\n      self.assertEqual(x, 2.0)\n\n  def testDataFormatAgnosticVar(self):\n    c_last_layer = group_norm.GroupNorm(\n        groups=5, create_scale=True, create_offset=True)\n    c_first_layer = group_norm.GroupNorm(\n        groups=5, create_scale=True, create_offset=True, data_format=""NCHW"")\n\n    inputs = tf.random.uniform([3, 4, 4, 10], 0, 10)\n\n    c_last_output = c_last_layer(inputs)\n    inputs = tf.transpose(inputs, [0, 3, 1, 2])\n    c_first_output = c_first_layer(inputs)\n    c_first_output = tf.transpose(c_first_output, [0, 2, 3, 1])\n\n    self.assertAllClose(c_last_output.numpy(), c_first_output.numpy())\n\n  def testSimpleCaseTensor(self):\n    layer = group_norm.GroupNorm(\n        groups=5, create_scale=False, create_offset=False)\n\n    inputs = tf.ones([2, 3, 3, 10])\n    scale = tf.constant(0.5, shape=(10,))\n    offset = tf.constant(2.0, shape=(10,))\n\n    outputs = layer(inputs, scale, offset).numpy()\n    for x in np.nditer(outputs):\n      self.assertEqual(x, 2.0)\n\n  def testSimpleCaseNCHWTensor(self):\n    layer = group_norm.GroupNorm(\n        groups=5, data_format=""NCHW"", create_scale=False, create_offset=False)\n\n    inputs = tf.ones([2, 10, 3, 3])\n    scale = tf.constant(0.5, shape=(10, 1, 1))\n    offset = tf.constant(2.0, shape=(10, 1, 1))\n\n    outputs = layer(inputs, scale, offset).numpy()\n    for x in np.nditer(outputs):\n      self.assertEqual(x, 2.0)\n\n  def testDataFormatAgnosticTensor(self):\n    c_last = group_norm.GroupNorm(\n        groups=5, create_scale=False, create_offset=False)\n    c_first = group_norm.GroupNorm(\n        groups=5, data_format=""NCHW"", create_scale=False, create_offset=False)\n\n    inputs = tf.random.uniform([3, 4, 4, 10], 0, 10)\n    scale = tf.random.normal((10,), mean=1.0)\n    offset = tf.random.normal((10,))\n\n    c_last_output = c_last(inputs, scale, offset)\n    inputs = tf.transpose(inputs, [0, 3, 1, 2])\n    scale = tf.reshape(scale, (10, 1, 1))\n    offset = tf.reshape(offset, (10, 1, 1))\n    c_first_output = c_first(inputs, scale, offset)\n    c_first_output = tf.transpose(c_first_output, [0, 2, 3, 1])\n\n    self.assertAllClose(c_last_output, c_first_output, rtol=1e-5)\n\n  @parameterized.parameters(""NHW"", ""HWC"", ""channel_last"")\n  def testInvalidDataFormat(self, data_format):\n    with self.assertRaisesRegexp(\n        ValueError,\n        ""Unable to extract channel information from \'{}\'."".format(data_format)):\n      group_norm.GroupNorm(\n          groups=5,\n          data_format=data_format,\n          create_scale=False,\n          create_offset=False)\n\n  @parameterized.parameters(""NCHW"", ""NCW"", ""channels_first"")\n  def testValidDataFormatChannelsFirst(self, data_format):\n    test = group_norm.GroupNorm(\n        groups=5,\n        data_format=data_format,\n        create_scale=False,\n        create_offset=False)\n\n    self.assertEqual(test._channel_index, 1)\n\n  @parameterized.parameters(""NHWC"", ""NWC"", ""channels_last"")\n  def testValidDataFormatChannelsLast(self, data_format):\n    test = group_norm.GroupNorm(\n        groups=5,\n        data_format=data_format,\n        create_scale=False,\n        create_offset=False)\n\n    self.assertEqual(test._channel_index, -1)\n\n  @parameterized.named_parameters((""String"", ""foo""), (""ListString"", [""foo""]))\n  def testInvalidAxis(self, axis):\n    with self.assertRaisesRegexp(\n        ValueError, ""`axis` should be an int, slice or iterable of ints.""):\n      group_norm.GroupNorm(\n          groups=5, axis=axis, create_scale=False, create_offset=False)\n\n  def testNoScaleAndInitProvided(self):\n    with self.assertRaisesRegexp(\n        ValueError, ""Cannot set `scale_init` if `create_scale=False`.""):\n      group_norm.GroupNorm(\n          groups=5,\n          create_scale=False,\n          create_offset=True,\n          scale_init=initializers.Ones())\n\n  def testNoOffsetBetaInitProvided(self):\n    with self.assertRaisesRegexp(\n        ValueError, ""Cannot set `offset_init` if `create_offset=False`.""):\n      group_norm.GroupNorm(\n          groups=5,\n          create_scale=True,\n          create_offset=False,\n          offset_init=initializers.Zeros())\n\n  def testCreateScaleAndScaleProvided(self):\n    layer = group_norm.GroupNorm(\n        groups=5, create_scale=True, create_offset=False)\n\n    with self.assertRaisesRegexp(\n        ValueError, ""Cannot pass `scale` at call time if `create_scale=True`.""):\n      layer(tf.ones([2, 3, 5]), scale=tf.ones([4]))\n\n  def testCreateOffsetAndOffsetProvided(self):\n    layer = group_norm.GroupNorm(\n        groups=5, create_offset=True, create_scale=False)\n\n    with self.assertRaisesRegexp(\n        ValueError,\n        ""Cannot pass `offset` at call time if `create_offset=True`.""):\n      layer(tf.ones([2, 3, 5]), offset=tf.ones([4]))\n\n  def testSliceAxis(self):\n    slice_layer = group_norm.GroupNorm(\n        groups=5, create_scale=False, create_offset=False)\n    axis_layer = group_norm.GroupNorm(\n        groups=5, create_scale=False, create_offset=False)\n\n    inputs = tf.random.uniform([3, 4, 4, 5], 0, 10)\n    scale = tf.random.normal((5,), mean=1.0)\n    offset = tf.random.normal((5,))\n\n    slice_outputs = slice_layer(inputs, scale, offset)\n    axis_outputs = axis_layer(inputs, scale, offset)\n\n    self.assertAllEqual(slice_outputs.numpy(), axis_outputs.numpy())\n\n  def testRankChanges(self):\n    layer = group_norm.GroupNorm(\n        groups=5, create_scale=False, create_offset=False)\n\n    inputs = tf.ones([2, 3, 3, 5])\n    scale = tf.constant(0.5, shape=(5,))\n    offset = tf.constant(2.0, shape=(5,))\n\n    layer(inputs, scale, offset)\n\n    with self.assertRaisesRegexp(\n        ValueError,\n        ""The rank of the inputs cannot change between calls, the original""):\n      layer(tf.ones([2, 3, 3, 4, 5]), scale, offset)\n\n  @parameterized.named_parameters((""Small"", (2, 4, 4)), (""Bigger"", (2, 3, 8)))\n  def testIncompatibleGroupsAndTensor(self, shape):\n    layer = group_norm.GroupNorm(\n        groups=5, create_scale=False, create_offset=False)\n\n    inputs = tf.ones(shape)\n\n    with self.assertRaisesRegexp(\n        ValueError,\n        ""The number of channels must be divisible by the number of groups""):\n      layer(inputs)\n\n  def testWorksWithFunction(self):\n    layer = group_norm.GroupNorm(\n        groups=5, create_scale=False, create_offset=False)\n    function_layer = tf.function(layer)\n\n    inputs = tf.ones([2, 3, 3, 10])\n    scale = tf.constant(0.5, shape=(10,))\n    offset = tf.constant(2.0, shape=(10,))\n\n    outputs = layer(inputs, scale, offset)\n    function_outputs = function_layer(inputs, scale, offset)\n\n    self.assertAllEqual(outputs.numpy(), function_outputs.numpy())\n\n  def testBatchSizeAgnostic(self):\n    layer = group_norm.GroupNorm(\n        groups=5, create_scale=False, create_offset=False)\n    inputs_spec = tf.TensorSpec([None, 3, 3, 10], dtype=tf.float32)\n    params_spec = tf.TensorSpec([None], dtype=tf.float32)\n    function_layer = tf.function(layer).get_concrete_function(\n        inputs_spec, params_spec, params_spec)\n\n    scale = tf.constant(0.5, shape=(10,))\n    offset = tf.constant(2.0, shape=(10,))\n\n    outputs = function_layer(tf.ones([2, 3, 3, 10]), scale, offset)\n    self.assertEqual(outputs.shape, [2, 3, 3, 10])\n    for x in np.nditer(outputs):\n      self.assertEqual(x, 2.0)\n\n    scale = tf.constant(0.5, shape=(10,))\n    offset = tf.constant(2.0, shape=(10,))\n\n    outputs = function_layer(tf.ones([3, 3, 3, 10]), scale, offset)\n    self.assertEqual(outputs.shape, [3, 3, 3, 10])\n    for x in np.nditer(outputs):\n      self.assertEqual(x, 2.0)\n\n  def test5DDataFormatAgnostic(self):\n    c_last_layer = group_norm.GroupNorm(\n        groups=5, create_scale=False, create_offset=False)\n    c_first_layer = group_norm.GroupNorm(\n        groups=5, create_scale=False, create_offset=False, data_format=""NCDHW"")\n\n    inputs = tf.random.uniform([3, 4, 4, 4, 10], 0, 10)\n    scale = tf.random.normal((10,), mean=1.0)\n    offset = tf.random.normal((10,))\n\n    c_last_output = c_last_layer(inputs, scale, offset)\n    inputs = tf.transpose(inputs, [0, 4, 1, 2, 3])\n    scale = tf.reshape(scale, [-1, 1, 1, 1])\n    offset = tf.reshape(offset, [-1, 1, 1, 1])\n    c_first_output = c_first_layer(inputs, scale, offset)\n    c_first_output = tf.transpose(c_first_output, [0, 2, 3, 4, 1])\n\n    self.assertAllClose(\n        c_last_output.numpy(), c_first_output.numpy(), atol=1e-5, rtol=1e-5)\n\n  def test3DDataFormatAgnostic(self):\n    c_last_layer = group_norm.GroupNorm(\n        groups=5, create_scale=False, create_offset=False)\n    c_first_layer = group_norm.GroupNorm(\n        groups=5, create_scale=False, create_offset=False, data_format=""NCW"")\n\n    inputs = tf.random.uniform([3, 4, 10], 0, 10)\n    scale = tf.random.normal((10,), mean=1.0)\n    offset = tf.random.normal((10,))\n\n    c_last_output = c_last_layer(inputs, scale, offset)\n    inputs = tf.transpose(inputs, [0, 2, 1])\n    scale = tf.reshape(scale, [-1, 1])\n    offset = tf.reshape(offset, [-1, 1])\n    c_first_output = c_first_layer(inputs, scale, offset)\n    c_first_output = tf.transpose(c_first_output, [0, 2, 1])\n\n    self.assertAllClose(\n        c_last_output.numpy(), c_first_output.numpy(), atol=1e-5, rtol=1e-5)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/initializers.py,44,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Initializers for Sonnet.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport abc\nimport collections\nimport numpy as np\nimport six\nfrom sonnet.src import types\nimport tensorflow as tf\nfrom typing import Iterable, Mapping, Optional, Text, Union\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass Initializer(object):\n  """"""Initializer base class, all initializers must implement a call method.""""""\n\n  @abc.abstractmethod\n  def __call__(self, shape: types.ShapeLike, dtype: tf.DType) -> tf.Tensor:\n    """"""Returns a tensor of the given ``shape`` and ``dtype``.""""""\n    pass\n\n\nclass Zeros(Initializer):\n  """"""Initializer that generates tensors initialized to 0.""""""\n\n  def __call__(self, shape: types.ShapeLike, dtype: tf.DType) -> tf.Tensor:\n    dtype = _as_numerical_dtype(dtype)\n    return tf.zeros(shape, dtype)\n\n\nclass Ones(Initializer):\n  """"""Initializer that generates tensors initialized to 1.""""""\n\n  def __call__(self, shape: types.ShapeLike, dtype: tf.DType) -> tf.Tensor:\n    dtype = _as_numerical_dtype(dtype)\n    return tf.ones(shape, dtype)\n\n\nclass Constant(Initializer):\n  """"""Initializer that generates tensors initialized to the given value.""""""\n\n  def __init__(self, value: Union[float, int]):\n    if not np.isscalar(value):\n      raise TypeError(""Invalid type for value: {} (expected scalar)."".format(\n          type(value)))\n    self.value = value\n\n  def __call__(self, shape: types.ShapeLike, dtype: tf.DType) -> tf.Tensor:\n    dtype = _as_numerical_dtype(dtype)\n    value = tf.convert_to_tensor(self.value, dtype)\n    return tf.fill(value=value, dims=shape)\n\n\nclass RandomUniform(Initializer):\n  """"""Initializer that generates tensors with a uniform distribution.\n\n  The generated values follow a uniform distribution in the range\n  ``[minval, maxval)``.\n  """"""\n\n  def __init__(self,\n               minval: types.FloatLike = 0,\n               maxval: types.FloatLike = 1,\n               seed: Optional[int] = None):\n    """"""Constructs a random uniform initializer.\n\n    Args:\n      minval: A python scalar or a scalar tensor. Lower bound of the range of\n        random values to generate. Defaults to ``0``.\n      maxval: A python scalar or a scalar tensor. Upper bound of the range of\n        random values to generate. Defaults to ``1``.\n      seed: The seed used in the generation of random numbers.\n    """"""\n    self.minval = minval\n    self.maxval = maxval\n    self.seed = seed\n\n  def __call__(self, shape: types.ShapeLike, dtype: tf.DType):\n    dtype = _as_numerical_dtype(dtype)\n    return tf.random.uniform(\n        shape=shape,\n        minval=self.minval,\n        maxval=self.maxval,\n        dtype=dtype,\n        seed=self.seed)\n\n\nclass RandomNormal(Initializer):\n  """"""Initializer that generates tensors with a normal distribution.""""""\n\n  def __init__(self,\n               mean: types.FloatLike = 0.0,\n               stddev: types.FloatLike = 1.0,\n               seed: Optional[int] = None):\n    """"""Constructs a random normal initializer.\n\n    Args:\n      mean: A python scalar or a scalar tensor. Mean of the random values to\n        generate.\n      stddev: A python scalar or a scalar tensor. Standard deviation of the\n        random values to generate.\n      seed: The seed used in the generation of random numbers.\n    """"""\n    self.mean = mean\n    self.stddev = stddev\n    self.seed = seed\n\n  def __call__(self, shape: types.ShapeLike, dtype: tf.DType) -> tf.Tensor:\n    dtype = _as_floating_dtype(dtype)\n    return tf.random.normal(\n        shape=shape,\n        mean=self.mean,\n        stddev=self.stddev,\n        dtype=dtype,\n        seed=self.seed)\n\n\nclass TruncatedNormal(Initializer):\n  """"""Initializer that generates a truncated normal distribution.\n\n  These values follow a normal distribution except that values more than two\n  standard deviations from the mean are discarded and re-drawn. This is the\n  recommended initializer for neural network weights and filters.\n  """"""\n\n  def __init__(self,\n               mean: types.FloatLike = 0.0,\n               stddev: types.FloatLike = 1.0,\n               seed: Optional[int] = None):\n    """"""Constructs a truncated normal initializer.\n\n    Args:\n      mean: A python scalar or a scalar tensor. Mean of the random values to\n        generate.\n      stddev: A python scalar or a scalar tensor. Standard deviation of the\n        random values to generate.\n      seed: The seed used in the generation of random numbers.\n    """"""\n    self.mean = mean\n    self.stddev = stddev\n    self.seed = seed\n\n  def __call__(self, shape: types.ShapeLike, dtype: tf.DType):\n    dtype = _as_floating_dtype(dtype)\n    return tf.random.truncated_normal(\n        shape=shape,\n        mean=self.mean,\n        stddev=self.stddev,\n        dtype=dtype,\n        seed=self.seed)\n\n\nclass Identity(Initializer):\n  """"""Initializer that generates the identity matrix.\n\n  Constructs a 2D identity matrix or batches of these.\n  """"""\n\n  def __init__(self, gain: float = 1.0):\n    """"""Constructs an identity initializer.\n\n    Args:\n      gain: Multiplicative factor to apply to the identity matrix.\n    """"""\n    self.gain = gain\n\n  def __call__(self, shape: types.ShapeLike, dtype: tf.DType) -> tf.Tensor:\n    dtype = _as_numerical_dtype(dtype)\n    rank = shape.shape[0] if isinstance(shape, tf.Tensor) else len(shape)\n    if rank < 2:\n      raise ValueError(""The tensor to initialize must be ""\n                       ""at least two-dimensional"")\n    elif rank == 2:\n      initializer = tf.eye(num_rows=shape[0], num_columns=shape[1], dtype=dtype)\n    else:  # rank > 2\n      initializer = tf.eye(\n          num_rows=shape[-2],\n          num_columns=shape[-1],\n          batch_shape=shape[:-2],\n          dtype=dtype)\n    return self.gain * initializer\n\n\nclass Orthogonal(Initializer):\n  """"""Initializer that generates an orthogonal matrix.\n\n  NOTE: Does not support 1D tensors.\n\n  The implementation is based on :cite:`saxe2013exact`.\n\n  If the shape of the tensor to initialize is two-dimensional, it is initialized\n  with an orthogonal matrix obtained from the QR decomposition of a matrix of\n  random numbers drawn from a normal distribution.\n  If the matrix has fewer rows than columns then the output will have orthogonal\n  rows. Otherwise, the output will have orthogonal columns.\n\n  If the shape of the tensor to initialize is more than two-dimensional,\n  a matrix of shape ``(shape[0] * ... * shape[n - 2], shape[n - 1])``\n  is initialized, where ``n`` is the length of the shape vector.\n  The matrix is subsequently reshaped to give a tensor of the desired shape.\n  """"""\n\n  def __init__(self, gain: float = 1.0, seed: Optional[int] = None):\n    """"""Constructs an orthogonal initializer.\n\n    Args:\n      gain: Multiplicative factor to apply to the orthogonal matrix\n      seed: ``int``, the seed used in the generation of random numbers.\n    """"""\n    self.gain = gain\n    self.seed = seed\n\n  def __call__(self, shape: types.ShapeLike, dtype: tf.DType) -> tf.Tensor:\n    dtype = _as_floating_dtype(dtype)\n    if len(shape) < 2:\n      raise ValueError(""The tensor to initialize must be ""\n                       ""at least two-dimensional"")\n    # Flatten the input shape with the last dimension remaining\n    # its original shape so it works for conv2d\n    num_rows = 1\n    for dim in shape[:-1]:\n      num_rows *= dim\n    num_cols = shape[-1]\n    flat_shape = [\n        tf.maximum(num_cols, num_rows),\n        tf.minimum(num_cols, num_rows)\n    ]\n\n    # Generate a random matrix\n    a = tf.random.normal(flat_shape, dtype=dtype, seed=self.seed)\n    # Compute the qr factorization\n    q, r = tf.linalg.qr(a, full_matrices=False)\n    # Make Q uniform\n    d = tf.linalg.tensor_diag_part(r)\n    q *= tf.sign(d)\n    if num_rows < num_cols:\n      q = tf.linalg.matrix_transpose(q)\n    return self.gain * tf.reshape(q, shape)\n\n\nclass VarianceScaling(Initializer):\n  """"""Initializer capable of adapting its scale to the shape of weights tensors.\n\n  With ``distribution=""truncated_normal"" or ""normal""``,\n  samples are drawn from a distribution with a mean of zero and a standard\n  deviation (after truncation, if used) ``stddev = sqrt(scale / n)``\n  where ``n`` is:\n\n    - Number of input units in the weight tensor, if ``mode = fan_in``.\n    - Number of output units, if ``mode = fan_out``.\n    - Average of the numbers of input and output units, if ``mode = fan_avg``.\n\n  Note that for transposed convolution the mode selected should be reversed. For\n  number of input units use ``fan_out`` and for number of output units\n  ``fan_in``.\n\n  With ``distribution=uniform``, samples are drawn from a uniform distribution\n  within ``[-limit, limit]``, with ``limit = sqrt(3 * scale / n)``.\n\n  The variance scaling initializer can be configured to generate other standard\n  initializers using the scale, mode and distribution arguments. Here are some\n  example configurations:\n\n  ==============  ==============================================================\n  Name            Parameters\n  ==============  ==============================================================\n  glorot_uniform  scale=1.0, mode=``fan_avg``, distribution=``uniform``\n  glorot_normal   scale=1.0, mode=``fan_avg``, distribution=``truncated_normal``\n  lecun_uniform   scale=1.0, mode=``fan_in``,  distribution=``uniform``\n  lecun_normal    scale=1.0, mode=``fan_in``,  distribution=``truncated_normal``\n  he_uniform      scale=2.0, mode=``fan_in``,  distribution=``uniform``\n  he_normal       scale=2.0, mode=``fan_in``,  distribution=``truncated_normal``\n  ==============  ==============================================================\n  """"""\n\n  def __init__(self,\n               scale: float = 1.0,\n               mode: Text = ""fan_in"",\n               distribution: Text = ""truncated_normal"",\n               seed: Optional[int] = None):\n    """"""Constructs a variance scaling initalizer.\n\n    Args:\n      scale: Scaling factor (positive ``float``).\n      mode: One of ``fan_in``, ``fan_out``, ``fan_avg``.\n      distribution: Random distribution to use. One of ``truncated_normal``,\n        ``untruncated_normal`` and  ``uniform``.\n      seed: ``int``, the seed used in the generation of random numbers.\n\n    Raises:\n      ValueError: In case of an invalid value for the ``scale``, ``mode`` or\n        ``distribution`` arguments.\n    """"""\n    if scale <= 0.:\n      raise ValueError(""`scale` must be positive float."")\n    if mode not in {""fan_in"", ""fan_out"", ""fan_avg""}:\n      raise ValueError(""Invalid `mode` argument:"", mode)\n    distribution = distribution.lower()\n    if distribution not in {""uniform"", ""truncated_normal"", ""normal""}:\n      raise ValueError(""Invalid `distribution` argument:"", distribution)\n    self.scale = scale\n    self.mode = mode\n    self.distribution = distribution\n    self.seed = seed\n\n  def __call__(self, shape: types.ShapeLike, dtype: tf.DType) -> tf.Tensor:\n    dtype = _as_floating_dtype(dtype)\n    scale = self.scale\n    fan_in, fan_out = _compute_fans(shape)\n    fan_in = tf.cast(fan_in, dtype)\n    fan_out = tf.cast(fan_out, dtype)\n    if self.mode == ""fan_in"":\n      scale /= tf.maximum(1., fan_in)\n    elif self.mode == ""fan_out"":\n      scale /= tf.maximum(1., fan_out)\n    else:\n      scale /= tf.maximum(1., (fan_in + fan_out) / 2.)\n    if self.distribution == ""truncated_normal"":\n      # constant from scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)\n      distribution_stddev = .87962566103423978\n      stddev = tf.sqrt(scale) / distribution_stddev\n      return tf.random.truncated_normal(\n          shape=shape, mean=0.0, stddev=stddev, dtype=dtype, seed=self.seed)\n    elif self.distribution == ""normal"":\n      stddev = tf.sqrt(scale)\n      return tf.random.normal(\n          shape=shape, mean=0.0, stddev=stddev, dtype=dtype, seed=self.seed)\n    else:  # self.distribution == ""uniform""\n      limit = tf.sqrt(3.0 * scale)\n      return tf.random.uniform(\n          shape=shape, minval=-limit, maxval=limit, dtype=dtype, seed=self.seed)\n\n\ndef check_initializers(initializers: Mapping[Text, Initializer],\n                       expected_keys: Iterable[Text]):\n  """"""Checks a dictionary of initializers only contains the given keys.""""""\n  if initializers is None:\n    return {}\n\n  if not isinstance(initializers, collections.Mapping):\n    raise TypeError(""Initializers must be a dict-like object."")\n\n  extra_keys = frozenset(initializers) - frozenset(expected_keys)\n  if extra_keys:\n    raise KeyError(""Invalid initializer keys {}, initializers can only ""\n                   ""be provided for {}"".format(\n                       "", "".join(map(repr, extra_keys)),\n                       "", "".join(map(repr, expected_keys))))\n\n  return initializers\n\n\ndef _compute_fans(shape: types.ShapeLike):\n  """"""Computes the number of input and output units for a weight shape.\n\n  Args:\n    shape: Integer shape tuple or `tf.TensorShape`.\n\n  Returns:\n    A tuple of scalars `(fan_in, fan_out)`.\n  """"""\n  if len(shape) < 1:  # Just to avoid errors for constants.\n    fan_in = fan_out = 1\n  elif len(shape) == 1:\n    fan_in = fan_out = shape[0]\n  elif len(shape) == 2:\n    fan_in = shape[0]\n    fan_out = shape[1]\n  else:\n    # Assuming convolution kernels (2D, 3D, or more).\n    # kernel shape: (..., input_depth, depth)\n    receptive_field_size = 1.\n    for dim in shape[:-2]:\n      receptive_field_size *= dim\n    fan_in = shape[-2] * receptive_field_size\n    fan_out = shape[-1] * receptive_field_size\n  return fan_in, fan_out\n\n\ndef _as_floating_dtype(dtype: tf.DType) -> tf.DType:\n  dtype = tf.as_dtype(dtype)\n  if dtype.is_floating:\n    return dtype\n  raise ValueError(""Expected floating point type, got {}"".format(dtype))\n\n\ndef _as_numerical_dtype(dtype: tf.DType) -> tf.DType:\n  dtype = tf.as_dtype(dtype)\n  if dtype.is_floating or dtype.is_integer:\n    return dtype\n  raise ValueError(\n      ""Expected integer or floating point type, got {}"".format(dtype))\n'"
sonnet/src/initializers_test.py,82,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.initializers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom sonnet.src import initializers\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\n\nclass InitializersTest(test_utils.TestCase, parameterized.TestCase):\n\n  def assertDifferentInitializerValues(self,\n                                       init,\n                                       shape=None,\n                                       dtype=tf.float32):\n    if shape is None:\n      shape = (100,)\n    t1 = self.evaluate(init(shape, dtype))\n    t2 = self.evaluate(init(shape, dtype))\n    self.assertEqual(t1.shape, shape)\n    self.assertEqual(t2.shape, shape)\n    self.assertFalse(np.allclose(t1, t2, rtol=1e-15, atol=1e-15))\n\n  def assertRange(self,\n                  init,\n                  shape,\n                  target_mean=None,\n                  target_std=None,\n                  target_max=None,\n                  target_min=None,\n                  dtype=tf.float32):\n    output = self.evaluate(init(shape, dtype))\n    self.assertEqual(output.shape, shape)\n    lim = 4e-2\n    if target_std is not None:\n      self.assertNear(output.std(), target_std, err=lim)\n    if target_mean is not None:\n      self.assertNear(output.mean(), target_mean, err=lim)\n    if target_max is not None:\n      self.assertNear(output.max(), target_max, err=lim)\n    if target_min is not None:\n      self.assertNear(output.min(), target_min, err=lim)\n\n\nclass ConstantInitializersTest(InitializersTest):\n\n  @parameterized.parameters(tf.float32, tf.int32)\n  def testZeros(self, dtype):\n    self.assertRange(\n        initializers.Zeros(),\n        shape=(4, 5),\n        target_mean=0.,\n        target_max=0.,\n        dtype=dtype)\n\n  @parameterized.parameters(tf.float32, tf.int32)\n  def testOnes(self, dtype):\n    self.assertRange(\n        initializers.Ones(),\n        shape=(4, 5),\n        target_mean=1.,\n        target_max=1.,\n        dtype=dtype)\n\n  @parameterized.named_parameters(\n      (""Tensor"", lambda: tf.constant([1.0, 2.0, 3.0]), ""Tensor""),\n      (""Variable"", lambda: tf.Variable([3.0, 2.0, 1.0]), ""Variable""),\n      (""List"", lambda: [], ""list""), (""Tuple"", lambda: (), ""tuple""))\n  def testConstantInvalidValue(self, value, value_type):\n    with self.assertRaisesRegexp(\n        TypeError, r""Invalid type for value: .*{}.*"".format(value_type)):\n      initializers.Constant(value())\n\n  @parameterized.parameters((42, tf.float32), (42.0, tf.float32),\n                            (42, tf.int32))\n  def testConstantValidValue(self, value, dtype):\n    self.assertRange(\n        initializers.Constant(value),\n        shape=(4, 5),\n        target_mean=42.,\n        target_max=42.,\n        dtype=dtype)\n\n  @parameterized.parameters(initializers.Zeros, initializers.Ones)\n  def testInvalidDataType(self, initializer):\n    init = initializer()\n    with self.assertRaisesRegexp(\n        ValueError, r""Expected integer or floating point type, got ""):\n      init([1], dtype=tf.string)\n\n  def testInvalidDataTypeConstant(self):\n    init = initializers.Constant(0)\n    with self.assertRaisesRegexp(\n        ValueError, r""Expected integer or floating point type, got ""):\n      init([1], dtype=tf.string)\n\n  def testTFFunction(self):\n    init = initializers.Constant(2)\n    f = tf.function(lambda t: init(tf.shape(t), t.dtype))\n\n    expected = init([7, 4], tf.float32)\n    x = f(tf.zeros([7, 4]))\n    self.assertAllEqual(expected, x)\n\n  def testBatchAgnostic(self):\n    init = initializers.Constant(2)\n    spec = tf.TensorSpec(shape=[None, None])\n    f = tf.function(lambda t: init(tf.shape(t), t.dtype))\n    f = f.get_concrete_function(spec)\n\n    expected = init([7, 4], tf.float32)\n    x = f(tf.ones([7, 4]))\n    self.assertAllEqual(expected, x)\n\n\nclass RandomUniformInitializerTest(InitializersTest):\n\n  def testRangeInitializer(self):\n    shape = (16, 8, 128)\n    self.assertRange(\n        initializers.RandomUniform(minval=-1., maxval=1., seed=124.),\n        shape,\n        target_mean=0.,\n        target_max=1,\n        target_min=-1)\n\n  @parameterized.parameters(tf.float32, tf.int32)\n  def testDifferentInitializer(self, dtype):\n    init = initializers.RandomUniform(0, 10)\n    self.assertDifferentInitializerValues(init, dtype=dtype)\n\n  def testInvalidDataType(self):\n    init = initializers.RandomUniform()\n    with self.assertRaisesRegexp(\n        ValueError, r""Expected integer or floating point type, got ""):\n      init([1], dtype=tf.string)\n\n  def testTFFunction(self):\n    init = initializers.RandomUniform(seed=42)\n    f = tf.function(lambda t: init(tf.shape(t), t.dtype))\n\n    expected = init([7, 4], tf.float32)\n    x = f(tf.zeros([7, 4]))\n    self.assertEqual(x.shape, [7, 4])\n    if self.primary_device != ""TPU"":  # Seeds don\'t work as expected on TPU\n      self.assertAllEqual(expected, x)\n\n  def testBatchAgnostic(self):\n    init = initializers.RandomUniform(seed=42)\n    spec = tf.TensorSpec(shape=[None, None])\n    f = tf.function(lambda t: init(tf.shape(t), t.dtype))\n    f = f.get_concrete_function(spec)\n\n    expected = init([7, 4], tf.float32)\n    x = f(tf.ones([7, 4]))\n    self.assertEqual(x.shape, [7, 4])\n    if self.primary_device != ""TPU"":  # Seeds don\'t work as expected on TPU\n      self.assertAllEqual(expected, x)\n\n\nclass RandomNormalInitializerTest(InitializersTest):\n\n  def testRangeInitializer(self):\n    self.assertRange(\n        initializers.RandomNormal(mean=0, stddev=1, seed=153),\n        shape=(16, 8, 128),\n        target_mean=0.,\n        target_std=1)\n\n  def testDifferentInitializer(self):\n    init = initializers.RandomNormal(0.0, 1.0)\n    self.assertDifferentInitializerValues(init)\n\n  @parameterized.parameters(tf.int32, tf.string)\n  def testInvalidDataType(self, dtype):\n    init = initializers.RandomNormal(0.0, 1.0)\n    with self.assertRaisesRegexp(ValueError,\n                                 r""Expected floating point type, got ""):\n      init([1], dtype=dtype)\n\n  def testTFFunction(self):\n    init = initializers.RandomNormal(seed=42)\n    f = tf.function(lambda t: init(tf.shape(t), t.dtype))\n\n    expected = init([7, 4], tf.float32)\n    x = f(tf.zeros([7, 4]))\n    self.assertEqual(x.shape, [7, 4])\n    if self.primary_device != ""TPU"":  # Seeds don\'t work as expected on TPU\n      self.assertAllEqual(expected, x)\n\n  def testBatchAgnostic(self):\n    init = initializers.RandomNormal(seed=42)\n    spec = tf.TensorSpec(shape=[None, None])\n    f = tf.function(lambda t: init(tf.shape(t), t.dtype))\n    f = f.get_concrete_function(spec)\n\n    expected = init([7, 4], tf.float32)\n    x = f(tf.ones([7, 4]))\n    self.assertEqual(x.shape, [7, 4])\n    if self.primary_device != ""TPU"":  # Seeds don\'t work as expected on TPU\n      self.assertAllEqual(expected, x)\n\n\nclass TruncatedNormalInitializerTest(InitializersTest):\n\n  def testRangeInitializer(self):\n    self.assertRange(\n        initializers.TruncatedNormal(mean=0, stddev=1, seed=126),\n        shape=(16, 8, 128),\n        target_mean=0.,\n        target_max=2,\n        target_min=-2)\n\n  def testDifferentInitializer(self):\n    init = initializers.TruncatedNormal(0.0, 1.0)\n    self.assertDifferentInitializerValues(init)\n\n  @parameterized.parameters(tf.int32, tf.string)\n  def testInvalidDataType(self, dtype):\n    init = initializers.TruncatedNormal(0.0, 1.0)\n    with self.assertRaisesRegexp(ValueError,\n                                 r""Expected floating point type, got ""):\n      init([1], dtype=dtype)\n\n  def testTFFunction(self):\n    init = initializers.TruncatedNormal(seed=42)\n    f = tf.function(lambda t: init(tf.shape(t), t.dtype))\n\n    expected = init([7, 4], tf.float32)\n    x = f(tf.zeros([7, 4]))\n    self.assertEqual(x.shape, [7, 4])\n    if self.primary_device != ""TPU"":  # Seeds don\'t work as expected on TPU\n      self.assertAllEqual(expected, x)\n\n  def testBatchAgnostic(self):\n    init = initializers.TruncatedNormal(seed=42)\n    spec = tf.TensorSpec(shape=[None, None])\n    f = tf.function(lambda t: init(tf.shape(t), t.dtype))\n    f = f.get_concrete_function(spec)\n\n    expected = init([7, 4], tf.float32)\n    x = f(tf.ones([7, 4]))\n    self.assertEqual(x.shape, [7, 4])\n    if self.primary_device != ""TPU"":  # Seeds don\'t work as expected on TPU\n      self.assertAllEqual(expected, x)\n\n\nclass IdentityInitializerTest(InitializersTest):\n\n  @parameterized.parameters(\n      *itertools.product([(4, 5), (3, 3), (3, 4, 5),\n                          (6, 2, 3, 3)], [3, 1], [tf.float32, tf.int32]))\n  def testRange(self, shape, gain, dtype):\n    if self.primary_device == ""GPU"" and dtype == tf.int32:\n      self.skipTest(""tf.int32 not supported on GPU"")\n\n    self.assertRange(\n        initializers.Identity(gain),\n        shape=shape,\n        target_mean=gain / shape[-1],\n        target_max=gain,\n        dtype=dtype)\n\n  def testInvalidDataType(self):\n    init = initializers.Identity()\n    with self.assertRaisesRegexp(\n        ValueError, r""Expected integer or floating point type, got ""):\n      init([1, 2], dtype=tf.string)\n\n  @parameterized.parameters(tf.float32, tf.int32)\n  def testInvalidShape(self, dtype):\n    init = initializers.Identity()\n    with self.assertRaisesRegexp(\n        ValueError,\n        ""The tensor to initialize must be at least two-dimensional""):\n      init([1], dtype=dtype)\n\n  def testTFFunction(self):\n    init = initializers.Identity()\n    f = tf.function(lambda t: init(tf.shape(t), t.dtype))\n\n    expected = init([4, 4], tf.float32)\n    x = f(tf.ones([4, 4]))\n    self.assertAllEqual(expected, x)\n\n  def testTFFunction4D(self):\n    init = initializers.Identity()\n    f = tf.function(lambda t: init(tf.shape(t), t.dtype))\n\n    expected = init([4, 4, 3, 2], tf.float32)\n    x = f(tf.ones([4, 4, 3, 2]))\n    self.assertAllEqual(expected, x)\n\n  def testBatchAgnostic(self):\n    init = initializers.Identity()\n    spec = tf.TensorSpec(shape=[None, None])\n    f = tf.function(lambda t: init(tf.shape(t), t.dtype))\n    f = f.get_concrete_function(spec)\n\n    expected = init([7, 4], tf.float32)\n    x = f(tf.ones([7, 4]))\n    self.assertAllEqual(expected, x)\n\n\nclass OrthogonalInitializerTest(InitializersTest):\n\n  def testRangeInitializer(self):\n    self.assertRange(\n        initializers.Orthogonal(seed=123), shape=(20, 20), target_mean=0.)\n\n  def testDuplicatedInitializer(self):\n    init = initializers.Orthogonal()\n    self.assertDifferentInitializerValues(init, (10, 10))\n\n  @parameterized.parameters(tf.int32, tf.string)\n  def testInvalidDataType(self, dtype):\n    init = initializers.Orthogonal()\n    with self.assertRaisesRegexp(ValueError,\n                                 r""Expected floating point type, got ""):\n      init([1, 2], dtype=dtype)\n\n  def testInvalidShape(self):\n    init = initializers.Orthogonal()\n    with self.assertRaisesRegexp(\n        ValueError,\n        ""The tensor to initialize must be at least two-dimensional""):\n      init([1], tf.float32)\n\n  @parameterized.named_parameters(\n      (""Square"", (10, 10)), (""3DSquare"", (100, 5, 5)),\n      (""3DRectangle"", (10, 9, 8)), (""TallRectangle"", (50, 40)),\n      (""WideRectangle"", (40, 50)))\n  def testShapesValues(self, shape):\n    init = initializers.Orthogonal()\n    tol = 1e-5\n\n    t = self.evaluate(init(shape, tf.float32))\n    self.assertAllEqual(tuple(shape), t.shape)\n    # Check orthogonality by computing the inner product\n    t = t.reshape((np.prod(t.shape[:-1]), t.shape[-1]))\n    if t.shape[0] > t.shape[1]:\n      self.assertAllClose(\n          np.dot(t.T, t), np.eye(t.shape[1]), rtol=tol, atol=tol)\n    else:\n      self.assertAllClose(\n          np.dot(t, t.T), np.eye(t.shape[0]), rtol=tol, atol=tol)\n\n  def testTFFunctionSimple(self):\n    init = initializers.Orthogonal(seed=42)\n    f = tf.function(init)\n\n    x = f([4, 4], tf.float32)\n    self.assertAllEqual(x.shape, [4, 4])\n\n  def testTFFunction(self):\n    if self.primary_device == ""TPU"":\n      self.skipTest(""Dynamic slice not supported on TPU"")\n\n    init = initializers.Orthogonal(seed=42)\n    f = tf.function(lambda t: init(tf.shape(t), t.dtype))\n\n    expected = init([4, 4], tf.float32)\n    x = f(tf.ones([4, 4]))\n    self.assertAllEqual(expected, x)\n\n  def testBatchAgnostic(self):\n    if self.primary_device == ""TPU"":\n      self.skipTest(""Dynamic slice not supported on TPU"")\n\n    init = initializers.Orthogonal(seed=42)\n    spec = tf.TensorSpec(shape=[None, None])\n    f = tf.function(lambda t: init(tf.shape(t), t.dtype))\n    f = f.get_concrete_function(spec)\n\n    expected = init([7, 4], tf.float32)\n    x = f(tf.ones([7, 4]))\n    self.assertAllEqual(expected, x)\n\n\nclass VarianceScalingInitializerTest(InitializersTest):\n\n  def testTruncatedNormalDistribution(self):\n    shape = (100, 100)\n    init = initializers.VarianceScaling(distribution=""truncated_normal"")\n\n    self.assertRange(\n        init, shape=shape, target_mean=0., target_std=1. / np.sqrt(shape[0]))\n\n  def testNormalDistribution(self):\n    shape = (100, 100)\n    init = initializers.VarianceScaling(distribution=""normal"")\n\n    self.assertRange(\n        init, shape=shape, target_mean=0., target_std=1. / np.sqrt(shape[0]))\n\n  def testUniformDistribution(self):\n    shape = (100, 100)\n    init = initializers.VarianceScaling(distribution=""uniform"")\n\n    self.assertRange(\n        init, shape=shape, target_mean=0., target_std=1. / np.sqrt(shape[0]))\n\n  def testGlorotUniform(self):\n    shape = (5, 6, 4, 2)\n    fan_in, fan_out = initializers._compute_fans(shape)\n    std = np.sqrt(2. / (fan_in + fan_out))\n    self.assertRange(\n        initializers.VarianceScaling(\n            scale=1.0, mode=""fan_avg"", distribution=""uniform"", seed=123),\n        shape,\n        target_mean=0.,\n        target_std=std)\n\n  def test_GlorotNormal(self):\n    shape = (5, 6, 4, 2)\n    fan_in, fan_out = initializers._compute_fans(shape)\n    std = np.sqrt(2. / (fan_in + fan_out))\n    self.assertRange(\n        initializers.VarianceScaling(\n            scale=1.0,\n            mode=""fan_avg"",\n            distribution=""truncated_normal"",\n            seed=123),\n        shape,\n        target_mean=0.,\n        target_std=std)\n\n  def testLecunUniform(self):\n    shape = (5, 6, 4, 2)\n    fan_in, _ = initializers._compute_fans(shape)\n    std = np.sqrt(1. / fan_in)\n    self.assertRange(\n        initializers.VarianceScaling(\n            scale=1.0, mode=""fan_in"", distribution=""uniform"", seed=123),\n        shape,\n        target_mean=0.,\n        target_std=std)\n\n  def testLecunNormal(self):\n    shape = (5, 6, 4, 2)\n    fan_in, _ = initializers._compute_fans(shape)\n    std = np.sqrt(1. / fan_in)\n    self.assertRange(\n        initializers.VarianceScaling(\n            scale=1.0, mode=""fan_in"", distribution=""truncated_normal"",\n            seed=123),\n        shape,\n        target_mean=0.,\n        target_std=std)\n\n  def testHeUniform(self):\n    shape = (5, 6, 4, 2)\n    fan_in, _ = initializers._compute_fans(shape)\n    std = np.sqrt(2. / fan_in)\n    self.assertRange(\n        initializers.VarianceScaling(\n            scale=2.0, mode=""fan_in"", distribution=""uniform"", seed=123),\n        shape,\n        target_mean=0.,\n        target_std=std)\n\n  def testHeNormal(self):\n    shape = (5, 6, 4, 2)\n    fan_in, _ = initializers._compute_fans(shape)\n    std = np.sqrt(2. / fan_in)\n    self.assertRange(\n        initializers.VarianceScaling(\n            scale=2.0, mode=""fan_in"", distribution=""truncated_normal"",\n            seed=123),\n        shape,\n        target_mean=0.,\n        target_std=std)\n\n  @parameterized.parameters(\n      itertools.product([""fan_in"", ""fan_out"", ""fan_avg""],\n                        [""uniform"", ""truncated_normal"", ""normal""]))\n  def testMixedShape(self, mode, distribution):\n    init = initializers.VarianceScaling(mode=mode, distribution=distribution)\n    tf.random.set_seed(42)\n    x = init([tf.constant(4), 2], tf.float32)\n    tf.random.set_seed(42)\n    expected = init([4, 2], tf.float32)\n    self.assertEqual(x.shape, [4, 2])\n    if self.primary_device != ""TPU"":  # Seeds don\'t work as expected on TPU\n      self.assertAllEqual(expected, x)\n\n  @parameterized.parameters(\n      itertools.product([""fan_in"", ""fan_out"", ""fan_avg""],\n                        [""uniform"", ""truncated_normal"", ""normal""]))\n  def testWithTFFunction(self, mode, distribution):\n    init = initializers.VarianceScaling(\n        mode=mode, distribution=distribution, seed=42)\n    f = tf.function(lambda t: init(tf.shape(t), t.dtype))\n    x = f(tf.zeros([4, 2]))\n    expected = init([4, 2], tf.float32)\n    self.assertEqual(x.shape, [4, 2])\n    if self.primary_device != ""TPU"":  # Seeds don\'t work as expected on TPU\n      self.assertAllEqual(expected, x)\n\n  @parameterized.parameters(\n      itertools.product([""fan_in"", ""fan_out"", ""fan_avg""],\n                        [""uniform"", ""truncated_normal"", ""normal""]))\n  def testBatchAgnostic(self, mode, distribution):\n    init = initializers.VarianceScaling(\n        mode=mode, distribution=distribution, seed=42)\n    spec = tf.TensorSpec(shape=[None, None])\n    f = tf.function(lambda t: init(tf.shape(t), t.dtype))\n    f = f.get_concrete_function(spec)\n\n    expected = init([7, 4], tf.float32)\n    x = f(tf.ones([7, 4]))\n    self.assertEqual(x.shape, [7, 4])\n    if self.primary_device != ""TPU"":  # Seeds don\'t work as expected on TPU\n      self.assertAllClose(expected, x)\n\n  @parameterized.parameters(tf.int32, tf.string)\n  def testInvalidDataType(self, dtype):\n    init = initializers.VarianceScaling()\n    with self.assertRaisesRegexp(ValueError,\n                                 r""Expected floating point type, got ""):\n      init([1, 2], dtype=dtype)\n\n  def testCheckInitializersInvalidType(self):\n    with self.assertRaisesRegexp(TypeError,\n                                 ""Initializers must be a dict-like object.""):\n      initializers.check_initializers([1, 2, 3], (""a""))\n\n  def testCheckInitalizersEmpty(self):\n    a = initializers.check_initializers(None, (""b""))\n    self.assertEqual(a, {})\n\n  @parameterized.named_parameters((""Tuple"", (""a"", ""b"")), (""List"", [""a"", ""b""]),\n                                  (""Set"", {""a"", ""b""}))\n  def testCheckInitalizersValid(self, keys):\n    initializers.check_initializers({\n        ""a"": lambda x, y: 0,\n        ""b"": lambda x, y: 1\n    }, keys)\n\n  def testCheckInitalizersInvalid(self):\n    with self.assertRaisesRegexp(\n        KeyError,\n        r""Invalid initializer keys \'a\', initializers can only be provided for""):\n      initializers.check_initializers({\n          ""a"": lambda x, y: 0,\n          ""b"": lambda x, y: 1\n      }, (""b""))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/leaky_clip_by_value.py,10,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Clipping operation with customized gradients.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom typing import Optional, Text\n\n\n@tf.custom_gradient\ndef leaky_clip_by_value(t: tf.Tensor,\n                        clip_value_min: tf.Tensor,\n                        clip_value_max: tf.Tensor,\n                        name: Optional[Text] = None):\n  """"""Clips tensor values to a specified min and max.\n\n  The gradient is set to zero when tensor values are already out of bound and\n  gradient-descent will push them even further away from the valid range. If\n  gradient-descent pushes the values towards the valid range, the gradient will\n  pass through without change.\n  Note that this is assuming a gradient flow for minimization. For\n  maximization, flip the gradient before it back-propagates to this op.\n\n  Args:\n    t: A Tensor.\n    clip_value_min: A 0-D (scalar) Tensor, or a Tensor with the same shape as t.\n      The minimum value to clip by.\n    clip_value_max: A 0-D (scalar) Tensor, or a Tensor with the same shape as t.\n      The maximum value to clip by.\n    name: A name for the operation (optional).\n\n  Returns:\n    A clipped Tensor.\n\n  Raises:\n    ValueError: If the clip tensors would trigger array broadcasting that would\n    make the returned tensor larger than the input.\n  """"""\n  clip_t = tf.clip_by_value(t, clip_value_min, clip_value_max, name=name)\n\n  def grad(dy):\n    """"""Custom gradient.""""""\n    zeros = tf.zeros_like(dy)\n    condition = tf.logical_or(\n        tf.logical_and(t < clip_value_min, dy > 0),\n        tf.logical_and(t > clip_value_max, dy < 0),\n    )\n    dy = tf.where(condition, zeros, dy)\n    return dy, None, None\n\n  return clip_t, grad\n'"
sonnet/src/leaky_clip_by_value_test.py,4,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.leaky_clip_by_value.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom sonnet.src import leaky_clip_by_value\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\n\nclass LeakyClipByValueTest(test_utils.TestCase, parameterized.TestCase):\n\n  def test_leaky_clip_by_value_forward(self):\n    t = tf.Variable([1.0, 2.0, 3.0])\n    # Test when min/max are scalar values.\n    clip_min = [1.5]\n    clip_max = [2.5]\n    clip_t = leaky_clip_by_value.leaky_clip_by_value(t, clip_min, clip_max)\n    self.assertAllEqual(clip_t.numpy(), [1.5, 2.0, 2.5])\n    # Test when min/max are of same sizes as t.\n    clip_min_array = [0.5, 2.5, 2.5]\n    clip_max_array = [1.5, 3.0, 3.5]\n    clip_t_2 = leaky_clip_by_value.leaky_clip_by_value(t, clip_min_array,\n                                                       clip_max_array)\n    self.assertAllEqual(clip_t_2.numpy(), [1.0, 2.5, 3.0])\n\n  @parameterized.parameters([\n      (0.5, lambda x: x, [1.0]),\n      (1.5, lambda x: x, [1.0]),\n      (1.5, lambda x: -x, [0.0]),\n      (-.5, lambda x: x, [0.0]),\n      (-.5, lambda x: -x, [-1.0]),\n  ])\n  def test_leaky_clip_by_value_backward(self, init, fn, expected_grad):\n    t = tf.Variable([init])\n    max_val = 1.0\n    min_val = 0.0\n    with tf.GradientTape() as tape:\n      clip_t = leaky_clip_by_value.leaky_clip_by_value(t, min_val, max_val)\n      f = fn(clip_t)\n    grad = tape.gradient(f, t)\n    clip_t_value = clip_t.numpy()\n    self.assertAllEqual(grad.numpy(), expected_grad)\n    self.assertGreaterEqual(clip_t_value, min_val)\n    self.assertLessEqual(clip_t_value, max_val)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/linear.py,7,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Linear module.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport math\n\nfrom sonnet.src import base\nfrom sonnet.src import initializers\nfrom sonnet.src import once\nfrom sonnet.src import utils\n\nimport tensorflow as tf\nfrom typing import Optional, Text\n\n\nclass Linear(base.Module):\n  """"""Linear module, optionally including bias.""""""\n\n  def __init__(self,\n               output_size: int,\n               with_bias: bool = True,\n               w_init: Optional[initializers.Initializer] = None,\n               b_init: Optional[initializers.Initializer] = None,\n               name: Optional[Text] = None):\n    """"""Constructs a `Linear` module.\n\n    Args:\n      output_size: Output dimensionality.\n      with_bias: Whether to include bias parameters. Default `True`.\n      w_init: Optional initializer for the weights. By default the weights are\n        initialized truncated random normal values with a standard deviation of\n        `1 / sqrt(input_feature_size)`, which is commonly used when the inputs\n        are zero centered (see https://arxiv.org/abs/1502.03167v3).\n      b_init: Optional initializer for the bias. By default the bias is\n        initialized to zero.\n      name: Name of the module.\n    """"""\n    super(Linear, self).__init__(name=name)\n    self.output_size = output_size\n    self.with_bias = with_bias\n    self.w_init = w_init\n    if with_bias:\n      self.b_init = b_init if b_init is not None else initializers.Zeros()\n    elif b_init is not None:\n      raise ValueError(""When not using a bias the b_init must be None."")\n\n  @once.once\n  def _initialize(self, inputs: tf.Tensor):\n    """"""Constructs parameters used by this module.""""""\n    utils.assert_minimum_rank(inputs, 2)\n\n    input_size = inputs.shape[-1]\n    if input_size is None:  # Can happen inside an @tf.function.\n      raise ValueError(""Input size must be specified at module build time."")\n\n    self.input_size = input_size\n\n    if self.w_init is None:\n      # See https://arxiv.org/abs/1502.03167v3.\n      stddev = 1 / math.sqrt(self.input_size)\n      self.w_init = initializers.TruncatedNormal(stddev=stddev)\n\n    self.w = tf.Variable(\n        self.w_init([self.input_size, self.output_size], inputs.dtype),\n        name=""w"")\n\n    if self.with_bias:\n      self.b = tf.Variable(\n          self.b_init([self.output_size], inputs.dtype), name=""b"")\n\n  def __call__(self, inputs: tf.Tensor) -> tf.Tensor:\n    self._initialize(inputs)\n\n    outputs = tf.matmul(inputs, self.w)\n    if self.with_bias:\n      outputs = tf.add(outputs, self.b)\n    return outputs\n'"
sonnet/src/linear_test.py,34,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.linear.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom sonnet.src import linear\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\n\nclass LinearTest(test_utils.TestCase, parameterized.TestCase):\n\n  def testInitW(self):\n    my_initializer = lambda shape, dtype: None\n    mod = linear.Linear(1, w_init=my_initializer)\n    self.assertIs(mod.w_init, my_initializer)\n\n  def testInitB(self):\n    my_initializer = lambda shape, dtype: None\n    mod = linear.Linear(1, b_init=my_initializer)\n    self.assertIs(mod.b_init, my_initializer)\n\n  def testInitializerKeysInvalidWithoutBias(self):\n    with self.assertRaisesRegexp(ValueError, ""b_init must be None""):\n      linear.Linear(1, with_bias=False, b_init=tf.zeros_initializer())\n\n  def testParametersCreatedOnce(self):\n    mod = linear.Linear(1)\n    mod(tf.constant([[1.]]))\n    w, b = mod.w, mod.b\n    mod(tf.constant([[1.]]))\n    self.assertIs(mod.w, w)\n    self.assertIs(mod.b, b)\n\n  def testParameterShape(self):\n    batch_size = 1\n    input_size = 2\n    output_size = 3\n    mod = linear.Linear(output_size)\n    mod(tf.ones([batch_size, input_size]))\n    self.assertEqual(mod.w.shape.as_list(), [input_size, output_size])\n    self.assertEqual(mod.b.shape.as_list(), [output_size])\n\n  @parameterized.parameters([tf.float16, tf.float32, tf.int32])\n  def testParameterDtype(self, dtype):\n    if dtype == tf.int32 and self.primary_device in (""GPU"", ""TPU""):\n      self.skipTest(""int32 not supported on %s"" % self.primary_device)\n    elif self.primary_device == ""TPU"" and dtype == tf.float16:\n      dtype = tf.bfloat16\n\n    mod = linear.Linear(1, w_init=tf.zeros_initializer())\n    out = mod(tf.ones([1, 1], dtype=dtype))\n    self.assertEqual(out.dtype, dtype)\n    self.assertEqual(mod.w.dtype, dtype)\n    self.assertEqual(mod.b.dtype, dtype)\n\n  def testBiasZeroInitialized(self):\n    mod = linear.Linear(1)\n    mod(tf.constant([[1.]]))\n    self.assertEqual(mod.b.numpy(), [0.])\n\n  def testCall(self):\n    batch_size = 1\n    input_size = 2\n    output_size = 3\n\n    def numpy_linear():\n      w = np.ndarray([input_size, output_size], dtype=np.float32)\n      w.fill(2.)\n      b = np.ndarray([output_size], dtype=np.float32)\n      b.fill(3.)\n      i = np.ones([batch_size, input_size], dtype=np.float32)\n      return np.matmul(i, w) + b\n\n    l = linear.Linear(\n        output_size,\n        w_init=tf.constant_initializer(2.),\n        b_init=tf.constant_initializer(3.))\n    tf_output = l(tf.ones([batch_size, input_size]))\n    self.assertAllEqual(tf_output, numpy_linear())\n\n  def testCallMultiBatch(self):\n    l = linear.Linear(5)\n    input_tensor = tf.random.uniform([1, 2, 3, 4])\n    tf_output = l(input_tensor)\n\n    w_np = l.w.numpy()\n    b_np = l.b.numpy()\n    input_tensor_np = input_tensor.numpy()\n    np_output = np.matmul(input_tensor_np, w_np) + b_np\n\n    # TPU uses bfloat16 internally, so larger deviations are expected.\n    self.assertAllClose(tf_output, np_output, atol=1e-2, rtol=5e-2)\n\n  @parameterized.parameters(True, False)\n  def testFunction(self, with_bias):\n    linear_1 = linear.Linear(\n        3, with_bias=with_bias, w_init=tf.ones_initializer())\n    linear_2 = linear.Linear(\n        3, with_bias=with_bias, w_init=tf.ones_initializer())\n    defun_linear = tf.function(linear_2)\n\n    iterations = 5\n\n    for _ in range(iterations):\n      x = tf.random.uniform([1, 5])\n      y1 = linear_1(x)\n      y2 = defun_linear(x)\n\n      self.assertAllClose(self.evaluate(y1), self.evaluate(y2), atol=1e-4)\n\n  def testUnknownBatchSize(self):\n    x = tf.TensorSpec([None, 4], dtype=tf.float32)\n\n    l = linear.Linear(3)\n    defun_linear = tf.function(l)\n\n    defun_linear.get_concrete_function(x)\n\n    out = defun_linear(tf.ones([2, 4]))\n    expected_out = l(tf.ones([2, 4]))\n    self.assertEqual(out.shape, [2, 3])\n    self.assertAllEqual(self.evaluate(expected_out), self.evaluate(out))\n\n    out = defun_linear(tf.ones([4, 4]))\n    self.assertEqual(out.shape, [4, 3])\n\n  def testUnknownInputSize(self):\n    x = tf.TensorSpec([None, None], dtype=tf.float32)\n\n    l = linear.Linear(3)\n    defun_linear = tf.function(l)\n\n    with self.assertRaisesRegex(\n        ValueError, ""Input size must be specified at module build time.""):\n      defun_linear.get_concrete_function(x)\n\n  def testMultiBatchOutputDimensions(self):\n    x = tf.TensorSpec([None, None, None, 2], dtype=tf.float32)\n\n    l = linear.Linear(7)\n    defun_linear = tf.function(l)\n\n    defun_linear.get_concrete_function(x)\n\n    out = defun_linear(tf.ones([1, 5, 3, 2]))\n    expected_out = l(tf.ones([1, 5, 3, 2]))\n    self.assertEqual(out.shape, [1, 5, 3, 7])\n    self.assertAllEqual(self.evaluate(expected_out), self.evaluate(out))\n\n    out = defun_linear(tf.ones([2, 4, 5, 2]))\n    self.assertEqual(out.shape, [2, 4, 5, 7])\n\n  @parameterized.named_parameters((""1D"", [1]),)\n  def testIncorrectDims(self, shape):\n    l = linear.Linear(3)\n    with self.assertRaisesRegex(ValueError, ""Shape .* must have rank >= 2""):\n      l(tf.ones(shape))\n\n  def testInputSize(self):\n    batch_size = 1\n    input_size = 2\n    output_size = 3\n    mod = linear.Linear(output_size)\n    mod(tf.ones([batch_size, input_size]))\n    self.assertEqual(mod.input_size, input_size)\n\n  def testOutputSize(self):\n    mod = linear.Linear(1)\n    self.assertEqual(mod.output_size, 1)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/metrics.py,13,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Base class for general metrics within Sonnet.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport abc\nimport six\n\nfrom sonnet.src import base\nfrom sonnet.src import once\nimport tensorflow as tf\nfrom typing import Optional, Text\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass Metric(base.Module):\n  """"""Metric base class.""""""\n\n  @abc.abstractmethod\n  def initialize(self, value):\n    """"""Creates any input dependent variables or state.""""""\n\n  @abc.abstractmethod\n  def update(self, value):\n    """"""Accumulates values.""""""\n\n  @abc.abstractproperty\n  def value(self):\n    """"""Returns the current value of the metric.""""""\n\n  @abc.abstractmethod\n  def reset(self):\n    """"""Resets the metric.""""""\n\n  def __call__(self, value):\n    """"""Updates the metric and returns the new value.""""""\n    self.update(value)\n    return self.value\n\n\nclass Sum(Metric):\n  """"""Calculates the element-wise sum of the given values.""""""\n\n  def __init__(self, name: Optional[Text] = None):\n    super(Sum, self).__init__(name=name)\n    self.sum = None\n\n  @once.once\n  def initialize(self, value: tf.Tensor):\n    """"""See base class.""""""\n    self.sum = tf.Variable(tf.zeros_like(value), trainable=False, name=""sum"")\n\n  def update(self, value: tf.Tensor):\n    """"""See base class.""""""\n    self.initialize(value)\n    self.sum.assign_add(value)\n\n  @property\n  def value(self) -> tf.Tensor:\n    """"""See base class.""""""\n    return tf.convert_to_tensor(self.sum)\n\n  def reset(self):\n    """"""See base class.""""""\n    self.sum.assign(tf.zeros_like(self.sum))\n\n\nclass Mean(Metric):\n  """"""Calculates the element-wise mean of the given values.""""""\n\n  def __init__(self, name: Optional[Text] = None):\n    super(Mean, self).__init__(name=name)\n    self.sum = None\n    self.count = tf.Variable(0, dtype=tf.int64, trainable=False, name=""count"")\n\n  @once.once\n  def initialize(self, value: tf.Tensor):\n    """"""See base class.""""""\n    self.sum = tf.Variable(tf.zeros_like(value), trainable=False, name=""sum"")\n\n  def update(self, value: tf.Tensor):\n    """"""See base class.""""""\n    self.initialize(value)\n    self.sum.assign_add(value)\n    self.count.assign_add(1)\n\n  @property\n  def value(self) -> tf.Tensor:\n    """"""See base class.""""""\n    # TODO(cjfj): Assert summed type is floating-point?\n    return self.sum / tf.cast(self.count, dtype=self.sum.dtype)\n\n  def reset(self):\n    self.sum.assign(tf.zeros_like(self.sum))\n    self.count.assign(0)\n'"
sonnet/src/metrics_test.py,14,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.metrics.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom sonnet.src import metrics\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\n\nclass SumTest(test_utils.TestCase):\n\n  def testSimple(self):\n    acc = metrics.Sum()\n    self.assertAllEqual([2., 3.], acc(tf.constant([2., 3.])))\n    self.assertAllEqual([6., 8.], acc(tf.constant([4., 5.])))\n\n  def testInitialize(self):\n    acc = metrics.Sum()\n    acc.initialize(tf.constant([1., 2.]))\n    self.assertAllEqual([0., 0.], acc.value)\n\n  def testReset(self):\n    acc = metrics.Sum()\n    self.assertAllEqual([2., 3.], acc(tf.constant([2., 3.])))\n    self.assertAllEqual([6., 8.], acc(tf.constant([4., 5.])))\n    acc.reset()\n    self.assertAllEqual([7., 8.], acc(tf.constant([7., 8.])))\n\n\nclass MeanTest(test_utils.TestCase):\n\n  def testSimple(self):\n    mean = metrics.Mean()\n    self.assertAllEqual([2., 3.], mean(tf.constant([2., 3.])))\n    self.assertAllEqual([3., 4.], mean(tf.constant([4., 5.])))\n\n  def testInitialize(self):\n    mean = metrics.Mean()\n    mean.initialize(tf.constant([1., 2.]))\n    self.assertAllEqual([1., 2.], mean(tf.constant([1., 2.])))\n\n  def testReset(self):\n    mean = metrics.Mean()\n    self.assertAllEqual([2., 3.], mean(tf.constant([2., 3.])))\n    self.assertAllEqual([3., 4.], mean(tf.constant([4., 5.])))\n    mean.reset()\n    self.assertAllEqual([7., 8.], mean(tf.constant([7., 8.])))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/mixed_precision.py,15,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Mixed Precision Decorator for Sonnet 2.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport uuid\nimport contextlib\n\nfrom sonnet.src import custom_getter\nfrom sonnet.src import utils\nimport tensorflow as tf\nimport tree\n\n# TODO(loreno): Make this a thread local variable\n_mixed_precision_mode = None\n_MP_SEEN_PROPERTY = \'_mp_seen\'\n\n\ndef enable(dtype):\n  """"""Set the mixed precision mode.\n\n  Args:\n    dtype: type to cast to.\n  """"""\n  global _mixed_precision_mode\n  _mixed_precision_mode = dtype\n\n\ndef disable():\n  """"""Disable mixed precision training.""""""\n  enable(None)\n\n\ndef _get_mixed_precision_mode():\n  return _mixed_precision_mode\n\n\n# TODO(loreno): Consider casting non-tensor/variable inputs\ndef _maybe_cast_element(x, dtype):\n  if isinstance(x, (tf.Tensor, tf.Variable)) and x.dtype.is_floating:\n    x = tf.cast(x, dtype)\n  return x\n\n\ndef _maybe_cast_structure(x, dtype: tf.DType):\n  return tree.map_structure(lambda x: _maybe_cast_element(x, dtype), x)\n\n\ndef _cast_call(f, new_dtype, args, kwargs):\n  """"""Runs the function with all tensor/variable arguments casted.""""""\n  # TODO(loreno): Implement more granular casting, not all Tensors/Variables\n  args = _maybe_cast_structure(args, new_dtype)\n  kwargs = _maybe_cast_structure(kwargs, new_dtype)\n\n  # TODO(loreno): Remove float32 hardcode and replace with original dtype\n  with custom_getter.custom_variable_getter(\n      lambda x: _maybe_cast_structure(x, new_dtype)):\n    ret = f(*args, **kwargs)\n  return _maybe_cast_structure(ret, tf.float32)\n\n\ndef modes(valid_types):\n  """"""Decorate a function to cast inputs/outputs to different precision.\n\n  >>> support_modes = snt.mixed_precision.modes([tf.float32, tf.float16])\n  >>> snt.Linear.__call__ = support_modes(snt.Linear.__call__)\n  >>> mod = snt.Linear(10)\n  >>> snt.mixed_precision.enable(tf.float16)\n  >>> y = mod(tf.ones([1, 1]))  # First call will be done in F32.\n  >>> y = mod(tf.ones([1, 1]))  # MatMul/Add will be done in F16.\n  >>> snt.mixed_precision.disable()\n\n  Args:\n    valid_types: Collection of types that the function being decorated is legal\n    to run in.\n\n  Returns:\n    A decorator that will cast the inputs and outputs of the decorated function\n    according to the global mixed precision policy and the functions eligibility\n    for mixed precision.\n  """"""\n  mp_id = uuid.uuid4()\n\n  @utils.decorator\n  def _wrapper(f, instance, args, kwargs):\n    """"""Decorator to cast inputs and outputs for mixed precision.\n\n    Args:\n      f: function to handle mixed precision casting for.\n      instance: instance of f.\n      args: positional arguments to f.\n      kwargs: keyword arguments to f.\n\n    Returns:\n      A wrapped version of `f` that casts input Variables and Tensors to the\n      global mixed_precision_mode dtype if that dtype is legal for this function\n      as determined by `valid_types`.\n    """"""\n    new_dtype = _get_mixed_precision_mode()\n    if new_dtype is None or new_dtype not in valid_types:\n      # TODO(loreno): consider throwing an error or doing nothing if input dtype\n      # doesn\'t match any valid types\n      return f(*args, **kwargs)\n\n    if instance is None:\n      if not _wrapper.seen_none:\n        # TODO(loreno): Make this thread safe\n        res = f(*args, **kwargs)\n        _wrapper.seen_none = True\n        return res\n      return _cast_call(f, new_dtype, args, kwargs)\n\n    else:\n      seen = getattr(instance, _MP_SEEN_PROPERTY, None)\n      if seen is None:\n        seen = set()\n        # TODO(loreno): use a weakrefset to address instances that define slots\n        setattr(instance, _MP_SEEN_PROPERTY, seen)\n      if mp_id not in seen:\n        res = f(*args, **kwargs)\n        seen.add(mp_id)\n        return res\n      return _cast_call(f, new_dtype, args, kwargs)\n\n  _wrapper.seen_none = False\n  return _wrapper\n\n\n@contextlib.contextmanager\ndef scope(dtype: tf.DType):\n  """"""Temporarily set the global mixed precision type to dtype.\n\n  The global type is reset to its original value when the context is exited.::\n\n      snt.mixed_precision.enable(tf.float32)\n      support_modes = snt.mixed_precision.modes([tf.float32, tf.float16])\n      snt.Linear.__call__ = support_modes(snt.Linear.__call__)\n      mod = snt.Linear(10)\n\n      with snt.mixed_precision.scope(tf.float16):\n          y = mod(tf.ones([1, 1]))  # First call will be done in F32.\n          y = mod(tf.ones([1, 1]))  # MatMul/Add will be done in F16.\n      y = mod(tf.ones([1, 1]))  # Outside the scope will be done in F32.\n\n  Args:\n    dtype: type to set the mixed precision mode to.\n\n  Yields:\n    Nothing. This is required for contextlib.contextmanager.\n  """"""\n  # TODO(petebu) Make this a doctest once python2 is deprecated\n  old_mode = _get_mixed_precision_mode()\n  enable(dtype)\n  try:\n    yield\n  finally:\n    enable(old_mode)\n'"
sonnet/src/mixed_precision_test.py,150,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for Mixed Precision.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom sonnet.src import base\nfrom sonnet.src import mixed_precision\nfrom sonnet.src import test_utils\nimport tensorflow as tf\nimport tree\n\n\nclass DummyVar(base.Module, test_utils.TestCase):\n\n  def __init__(self, x):\n    super(DummyVar, self).__init__()\n    test_utils.TestCase.__init__(self)\n    self.x = x\n\n  def check_type(self, _, dtype):\n    # TODO(loreno): handle dictionaries with non-sortable keys and change\n    # this test to assertEqual once that works\n    self.assertTrue(self.x.dtype == dtype)  # pylint: disable=g-generic-assert\n    return self.x\n\n  def check_type_structure(self, _, dtype):\n    # pylint: disable=g-generic-assert\n    tree.map_structure(lambda y: self.assertTrue(y.dtype == dtype), self.x)\n    return self.x\n\n  def runTest(self):\n    pass\n\n\nclass DummyInput(test_utils.TestCase):\n\n  def __init__(self, _):\n    super(DummyInput, self).__init__()\n    test_utils.TestCase.__init__(self)\n\n  def check_type(self, x, dtype):\n    self.assertEqual(x.dtype, dtype)\n    return x\n\n  def check_type_structure(self, x, dtype):\n    tree.map_structure(lambda y: self.assertEqual(y.dtype, dtype), x)\n    return x\n\n  def runTest(self):\n    pass\n\n\n@parameterized.parameters(DummyVar, DummyInput)\nclass MixedPrecisionClassTest(test_utils.TestCase):\n\n  def test_float16_mode_variable_eligible_class(self, test_class):\n    mixed_precision.enable(tf.float32)\n\n    x = tf.Variable([[1., 9.], [5., 0.]])\n    d = test_class(x)\n    d.check_type = mixed_precision.modes([tf.float32, tf.float16])(d.check_type)\n\n    mixed_precision.enable(tf.float16)\n    # First call to forward fn always runs in full precision.\n    self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)\n    # Subsequent calls run in mixed precision.\n    self.assertEqual(d.check_type(x, tf.float16).dtype, tf.float32)\n\n  def test_float16_mode_disable_class(self, test_class):\n    mixed_precision.enable(tf.float32)\n\n    x = tf.Variable([[1., 9.], [5., 0.]])\n    d = test_class(x)\n    d.check_type = mixed_precision.modes([tf.float32, tf.float16])(d.check_type)\n\n    mixed_precision.enable(tf.float16)\n    self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)\n    self.assertEqual(d.check_type(x, tf.float16).dtype, tf.float32)\n    mixed_precision.disable()\n    self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)\n\n  def test_float16_mode_nested_eligible_class(self, test_class):\n    mixed_precision.enable(tf.float32)\n\n    # TODO(loreno): test nested combo of tensor and Variables once the custom\n    # variable getter can cast tensors.\n    x = tf.Variable([[1., 9.], [5., 0.]])\n    y = tf.Variable([[1., 9.], [8., 9.]])\n    z = (x, y)\n\n    d = test_class(z)\n    d.check_type_structure = mixed_precision.modes([tf.float32, tf.float16])(\n        d.check_type_structure)\n\n    self.assertTrue(tree.is_nested(z))\n    mixed_precision.enable(tf.float16)\n\n    first_run = d.check_type_structure(z, tf.float32)\n    self.assertEqual(first_run[0].dtype, tf.float32)\n    self.assertEqual(first_run[1].dtype, tf.float32)\n    second_run = d.check_type_structure(z, tf.float16)\n    self.assertEqual(second_run[0].dtype, tf.float32)\n    self.assertEqual(second_run[1].dtype, tf.float32)\n\n  def test_float16_mode_eligible_multiple_instances_class(self, test_class):\n    mixed_precision.enable(tf.float32)\n\n    x = tf.Variable([[1., 9.], [5., 0.]])\n    d = test_class(x)\n    d.check_type = mixed_precision.modes([tf.float32, tf.float16])(d.check_type)\n\n    d2 = test_class(x)\n    d2.check_type = mixed_precision.modes([tf.float32, tf.float16])(\n        d2.check_type)\n\n    mixed_precision.enable(tf.float16)\n    self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)\n    self.assertEqual(d.check_type(x, tf.float16).dtype, tf.float32)\n    self.assertEqual(d2.check_type(x, tf.float32).dtype, tf.float32)\n    self.assertEqual(d2.check_type(x, tf.float16).dtype, tf.float32)\n\n  def test_float16_mode_ineligible_multiple_instances_class(self, test_class):\n    mixed_precision.enable(tf.float32)\n\n    x = tf.Variable([[1., 9.], [5., 0.]])\n    d = test_class(x)\n    d.check_type = mixed_precision.modes([tf.float32, tf.bfloat16])(\n        d.check_type)\n\n    d2 = test_class(x)\n    d2.check_type = mixed_precision.modes([tf.float32, tf.bfloat16])(\n        d2.check_type)\n\n    mixed_precision.enable(tf.float16)\n    self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)\n    self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)\n    self.assertEqual(d2.check_type(x, tf.float32).dtype, tf.float32)\n    self.assertEqual(d2.check_type(x, tf.float32).dtype, tf.float32)\n\n  def test_float16_mode_multiple_instances_different_eligibility_class(\n      self, test_class):\n    mixed_precision.enable(tf.float32)\n\n    x = tf.Variable([[1., 9.], [5., 0.]])\n    d = test_class(x)\n    d.check_type = mixed_precision.modes([tf.float32, tf.bfloat16])(\n        d.check_type)\n\n    d2 = test_class(x)\n    d2.check_type = mixed_precision.modes([tf.float32, tf.float16])(\n        d2.check_type)\n\n    mixed_precision.enable(tf.float16)\n    self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)\n    self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)\n    self.assertEqual(d2.check_type(x, tf.float32).dtype, tf.float32)\n    self.assertEqual(d2.check_type(x, tf.float16).dtype, tf.float32)\n\n  def test_bfloat16_input_float16_mode_eligible_class(self, test_class):\n    mixed_precision.enable(tf.float32)\n\n    x = tf.Variable([[1., 9.], [5., 0.]], dtype=tf.bfloat16)\n    d = test_class(x)\n    d.check_type = mixed_precision.modes([tf.float32, tf.float16])(d.check_type)\n\n    mixed_precision.enable(tf.float16)\n    self.assertEqual(d.check_type(x, tf.bfloat16).dtype, tf.bfloat16)\n    self.assertEqual(d.check_type(x, tf.float16).dtype, tf.float32)\n\n  def test_float16_input_float32_mode_eligible_class(self, test_class):\n    if self.primary_device == \'TPU\':\n      self.skipTest(\'float16 not supported on TPU\')\n\n    mixed_precision.enable(tf.float32)\n\n    x = tf.Variable([[1., 9.], [5., 0.]], dtype=tf.float16)\n    d = test_class(x)\n    d.check_type = mixed_precision.modes([tf.float32, tf.float16])(d.check_type)\n\n    self.assertEqual(d.check_type(x, tf.float16).dtype, tf.float16)\n    self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)\n\n  def test_function_create_module_eligible(self, test_class):\n    mixed_precision.enable(tf.float16)\n\n    @mixed_precision.modes([tf.float32, tf.float16])\n    def model():\n      x = tf.Variable([[1., 9.], [8., 9.]])\n      d = test_class(x)\n      d.check_type = mixed_precision.modes([tf.float32, tf.float16])(\n          d.check_type)\n\n      self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)\n      self.assertEqual(d.check_type(x, tf.float16).dtype, tf.float32)\n\n    model()\n\n  def test_function_create_module_ineligible(self, test_class):\n    mixed_precision.enable(tf.float16)\n\n    @mixed_precision.modes([tf.float32, tf.float16])\n    def model():\n      x = tf.Variable([[1., 9.], [8., 9.]])\n      d = test_class(x)\n      d.check_type = mixed_precision.modes([tf.float32, tf.bfloat16])(\n          d.check_type)\n\n      self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)\n      self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)\n\n    model()\n\n  def test_function_create_module_not_decorated(self, test_class):\n    mixed_precision.enable(tf.float16)\n\n    @mixed_precision.modes([tf.float32, tf.float16])\n    def model():\n      x = tf.Variable([[1., 9.], [8., 9.]])\n      d = test_class(x)\n      self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)\n      self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)\n\n    model()\n\n  def test_scoping_option(self, test_class):\n    mixed_precision.enable(tf.float32)\n\n    x = tf.Variable([[1., 9.], [8., 9.]])\n    d = test_class(x)\n    d.check_type = mixed_precision.modes([tf.float32, tf.float16])(d.check_type)\n\n    with mixed_precision.scope(tf.float16):\n      self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)\n      self.assertEqual(d.check_type(x, tf.float16).dtype, tf.float32)\n\n    self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)\n\n  def test_scoping_disable(self, test_class):\n    mixed_precision.enable(tf.float32)\n\n    x = tf.Variable([[1., 9.], [8., 9.]])\n    d = test_class(x)\n    d.check_type = mixed_precision.modes([tf.float32, tf.float16])(d.check_type)\n\n    with mixed_precision.scope(tf.float16):\n      self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)\n      self.assertEqual(d.check_type(x, tf.float16).dtype, tf.float32)\n\n      mixed_precision.disable()\n      self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)\n\n  def test_nested_scoping(self, test_class):\n    mixed_precision.enable(tf.float32)\n\n    x = tf.Variable([[1., 9.], [8., 9.]])\n    d = test_class(x)\n    d.check_type = mixed_precision.modes([tf.float32, tf.float16])(d.check_type)\n\n    with mixed_precision.scope(tf.float16):\n      self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)\n      self.assertEqual(d.check_type(x, tf.float16).dtype, tf.float32)\n      with mixed_precision.scope(tf.float32):\n        self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)\n        with mixed_precision.scope(tf.float16):\n          self.assertEqual(d.check_type(x, tf.float16).dtype, tf.float32)\n\n    self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)\n\n\nclass MixedPrecisionTest(test_utils.TestCase):\n\n  def test_float16_mode_eligible_func(self):\n    mixed_precision.enable(tf.float32)\n    self.assertEqual(mixed_precision._get_mixed_precision_mode(), tf.float32)\n\n    @mixed_precision.modes([tf.float32, tf.float16])\n    def check_type(x, expected_dtype):\n      self.assertEqual(x.dtype, expected_dtype)\n      return x\n\n    mixed_precision.enable(tf.float16)\n\n    x = tf.Variable([[1., 3], [5., 7.]])\n    self.assertEqual(x.dtype, tf.float32)\n    self.assertEqual(check_type(x, tf.float32).dtype, tf.float32)\n    self.assertEqual(check_type(x, tf.float16).dtype, tf.float32)\n\n  def test_float32_mode_eligible_func(self):\n    mixed_precision.enable(tf.float32)\n    self.assertEqual(mixed_precision._get_mixed_precision_mode(), tf.float32)\n\n    @mixed_precision.modes([tf.float32, tf.float16])\n    def fwd_func(x):\n      self.assertEqual(x.dtype, tf.float32)\n      return x\n\n    x = tf.Variable([[1., 3], [5., 7.]])\n    self.assertEqual(x.dtype, tf.float32)\n    self.assertEqual(fwd_func(x).dtype, tf.float32)\n    self.assertEqual(fwd_func(x).dtype, tf.float32)\n\n  def test_float16_mode_ineligible_func(self):\n    mixed_precision.enable(tf.float32)\n\n    @mixed_precision.modes([tf.float32, tf.bfloat16])\n    def fwd_func(x):\n      self.assertEqual(x.dtype, tf.float32)\n      return x\n\n    x = tf.Variable([[1., 3], [5., 7.]])\n    self.assertEqual(x.dtype, tf.float32)\n\n    mixed_precision.enable(tf.float16)\n    self.assertEqual(fwd_func(x).dtype, tf.float32)\n    self.assertEqual(fwd_func(x).dtype, tf.float32)\n\n  def test_dont_cast_non_floats_func(self):\n    mixed_precision.enable(tf.float32)\n\n    @mixed_precision.modes([tf.float32, tf.float16])\n    def fwd_func(x):\n      self.assertTrue(x.dtype.is_integer)\n      return x\n\n    x = tf.Variable([[1, 9], [8, 9]])\n    self.assertTrue(x.dtype.is_integer)\n\n    mixed_precision.enable(tf.float16)\n    self.assertTrue(fwd_func(x).dtype.is_integer)\n    self.assertTrue(fwd_func(x).dtype.is_integer)\n\n  def test_non_tensor_variable_input_no_cast_func(self):\n    mixed_precision.enable(tf.float32)\n\n    @mixed_precision.modes([tf.float32, tf.float16])\n    def fwd_func(x):\n      self.assertEqual(type(x[0][0]), float)\n      return x\n\n    x = [[1., 3], [5., 7.]]\n    self.assertEqual(type(x[0][0]), float)\n\n    mixed_precision.enable(tf.float16)\n    self.assertEqual(type(fwd_func(x)[0][0]), float)\n    self.assertEqual(type(fwd_func(x)[0][0]), float)\n\n  def test_float16_mode_enabled_call_function(self):\n    mixed_precision.enable(tf.float32)\n\n    class DummyCall(base.Module, test_utils.TestCase):\n\n      def __init__(self):\n        super(DummyCall, self).__init__()\n        test_utils.TestCase.__init__(self)\n        self.y = tf.Variable([[1., 3], [5., 7.]])\n\n      @mixed_precision.modes([tf.float16, tf.float32])\n      def __call__(self, x, dtype):\n        # pylint: disable=g-generic-assert\n        self.assertTrue(self.y.dtype == dtype)\n        self.assertTrue(x.dtype == dtype)\n        return x\n\n      def runTest(self):\n        pass\n\n    x = tf.Variable([[1., 3], [5., 7.]])\n    self.assertEqual(x.dtype, tf.float32)\n\n    d = DummyCall()\n    mixed_precision.enable(tf.float16)\n    self.assertEqual(d(x, tf.float32).dtype, tf.float32)\n    self.assertEqual(d(x, tf.float16).dtype, tf.float32)\n\n  # TODO(loreno): Run this test against custom variable getters once they can\n  # handle and cast tensors\n  def test_float16_mode_tensor_eligible_class(self):\n    mixed_precision.enable(tf.float32)\n\n    x = tf.constant([[1., 9.], [5., 0.]])\n    d = DummyInput(x)\n    d.check_type = mixed_precision.modes([tf.float32, tf.float16])(d.check_type)\n\n    mixed_precision.enable(tf.float16)\n    self.assertEqual(d.check_type(x, tf.float32).dtype, tf.float32)\n    self.assertEqual(d.check_type(x, tf.float16).dtype, tf.float32)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
sonnet/src/moving_averages.py,15,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Exponential moving average for Sonnet.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nfrom sonnet.src import metrics\nfrom sonnet.src import once\nfrom sonnet.src import types\n\nimport tensorflow as tf\nfrom typing import Optional, Text\n\n\nclass ExponentialMovingAverage(metrics.Metric):\n  """"""Maintains an exponential moving average for a value.\n\n  Note this module uses debiasing by default. If you don\'t want this please use\n  an alternative implementation.\n\n  This module keeps track of a hidden exponential moving average that is\n  initialized as a vector of zeros which is then normalized to give the average.\n  This gives us a moving average which isn\'t biased towards either zero or the\n  initial value. Reference (https://arxiv.org/pdf/1412.6980.pdf)\n\n  Initially:\n\n      hidden_0 = 0\n\n  Then iteratively:\n\n      hidden_i = (hidden_{i-1} - value) * (1 - decay)\n      average_i = hidden_i / (1 - decay^i)\n\n  Attributes:\n    average: Variable holding average. Note that this is None until the first\n      value is passed.\n  """"""\n\n  def __init__(self, decay: types.FloatLike, name: Optional[Text] = None):\n    """"""Creates a debiased moving average module.\n\n    Args:\n      decay: The decay to use. Note values close to 1 result in a slow decay\n        whereas values close to 0 result in faster decay, tracking the input\n        values more closely.\n      name: Name of the module.\n    """"""\n    super(ExponentialMovingAverage, self).__init__(name=name)\n    self._decay = decay\n    self._counter = tf.Variable(\n        0, trainable=False, dtype=tf.int64, name=""counter"")\n\n    self._hidden = None\n    self.average = None\n\n  def update(self, value: tf.Tensor):\n    """"""Applies EMA to the value given.""""""\n    self.initialize(value)\n\n    self._counter.assign_add(1)\n    value = tf.convert_to_tensor(value)\n    counter = tf.cast(self._counter, value.dtype)\n    self._hidden.assign_sub((self._hidden - value) * (1 - self._decay))\n    self.average.assign((self._hidden / (1. - tf.pow(self._decay, counter))))\n\n  @property\n  def value(self) -> tf.Tensor:\n    """"""Returns the current EMA.""""""\n    return self.average.read_value()\n\n  def reset(self):\n    """"""Resets the EMA.""""""\n    self._counter.assign(tf.zeros_like(self._counter))\n    self._hidden.assign(tf.zeros_like(self._hidden))\n    self.average.assign(tf.zeros_like(self.average))\n\n  @once.once\n  def initialize(self, value: tf.Tensor):\n    self._hidden = tf.Variable(\n        tf.zeros_like(value), trainable=False, name=""hidden"")\n    self.average = tf.Variable(\n        tf.zeros_like(value), trainable=False, name=""average"")\n'"
sonnet/src/moving_averages_test.py,8,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.moving_averages.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom sonnet.src import moving_averages\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\n\nclass ExponentialMovingAverageTest(test_utils.TestCase, parameterized.TestCase):\n\n  def testCall(self):\n    ema = moving_averages.ExponentialMovingAverage(0.50)\n\n    self.assertAllClose(ema(3.0).numpy(), 3.0)\n    self.assertAllClose(ema(6.0).numpy(), 5.0)\n\n  def testUpdateAndValue(self):\n    ema = moving_averages.ExponentialMovingAverage(0.50)\n    ema.update(3.0)\n    self.assertAllClose(ema.value.numpy(), 3.0, atol=1e-3, rtol=1e-5)\n\n    ema.update(6.0)\n    self.assertAllClose(ema.value.numpy(), 5.0, atol=1e-3, rtol=1e-5)\n\n  def testReset(self):\n    ema = moving_averages.ExponentialMovingAverage(0.90)\n    self.assertAllClose(ema(3.0).numpy(), 3.0, atol=1e-3, rtol=1e-5)\n\n    ema.reset()\n    self.assertEqual(ema.value.shape, ())\n    self.assertEqual(ema.value.numpy(), 0.0)\n\n    self.assertAllClose(ema(3.0).numpy(), 3.0, atol=1e-3, rtol=1e-5)\n\n  def testResetVector(self):\n    ema = moving_averages.ExponentialMovingAverage(0.90)\n    random_input = tf.random.normal((1, 5))\n    ema(random_input)\n    ema.reset()\n    self.assertEqual(ema.value.shape, (1, 5))\n    self.assertAllClose(ema.value.numpy(), tf.zeros_like(random_input))\n    self.assertEqual(ema._counter.dtype, tf.int64)\n\n  def testValueEqualsLatestUpdate(self):\n    ema = moving_averages.ExponentialMovingAverage(0.50)\n\n    self.assertAllClose(ema(3.0).numpy(), 3.0, atol=1e-3, rtol=1e-5)\n    self.assertAllClose(ema.value.numpy(), 3.0, atol=1e-3, rtol=1e-5)\n\n    self.assertAllClose(ema(6.0).numpy(), 5.0, atol=1e-3, rtol=1e-5)\n    self.assertAllClose(ema.value.numpy(), 5.0, atol=1e-3, rtol=1e-5)\n\n  @parameterized.parameters(True, False)\n  def testWithTFFunction(self, autograph):\n    ema_1 = moving_averages.ExponentialMovingAverage(0.95)\n    ema_2 = moving_averages.ExponentialMovingAverage(0.95)\n    ema_func = tf.function(ema_2, autograph=autograph)\n\n    for _ in range(10):\n      x = tf.random.uniform((), 0, 10)\n      self.assertAllClose(\n          ema_1(x).numpy(), ema_func(x).numpy(), atol=1e-3, rtol=1e-5)\n\n  @parameterized.parameters(True, False)\n  def testResetWithTFFunction(self, autograph):\n    ema = moving_averages.ExponentialMovingAverage(0.90)\n    ema_func = tf.function(ema, autograph=autograph)\n    self.assertAllClose(ema_func(3.0).numpy(), 3.0, atol=1e-3, rtol=1e-5)\n\n    ema.reset()\n    self.assertEqual(ema.value.numpy(), 0.0)\n\n    self.assertAllClose(ema_func(3.0).numpy(), 3.0, atol=1e-3, rtol=1e-5)\n\n  @parameterized.named_parameters((""2D"", [2, 2]), (""3D"", [1, 1, 3]))\n  def testAlternativeShape(self, shape):\n    ema = moving_averages.ExponentialMovingAverage(0.90)\n    value = tf.random.uniform(shape)\n    result = ema(value)\n    self.assertEqual(value.shape, result.shape)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/once.py,0,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Utility to run functions and methods once.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport uuid\n\nfrom sonnet.src import utils\n\n_ONCE_PROPERTY = ""_snt_once""\n\n\ndef _check_no_output(output):\n  if output is not None:\n    raise ValueError(""@snt.once decorated functions cannot return values"")\n\n\ndef once(f):\n  """"""Decorator which ensures a wrapped method is only ever run once.\n\n      >>> @snt.once\n      ... def f():\n      ...   print(\'Hello, world!\')\n      >>> f()\n      Hello, world!\n      >>> f()\n      >>> f()\n\n  If `f` is a method then it will be evaluated once per instance:\n\n      >>> class MyObject(object):\n      ...   @snt.once\n      ...   def f(self):\n      ...     print(\'Hello, world!\')\n\n      >>> o = MyObject()\n      >>> o.f()\n      Hello, world!\n      >>> o.f()\n\n      >>> o2 = MyObject()\n      >>> o2.f()\n      Hello, world!\n      >>> o.f()\n      >>> o2.f()\n\n  If an error is raised during execution of `f` it will be raised to the user.\n  Next time the method is run, it will be treated as not having run before.\n\n  Args:\n    f: A function to wrap which should only be called once.\n\n  Returns:\n    Wrapped version of `f` which will only evaluate `f` the first time it is\n    called.\n  """"""\n\n  # TODO(tomhennigan) Perhaps some more human friendly identifier?\n  once_id = uuid.uuid4()\n\n  @utils.decorator\n  def wrapper(wrapped, instance, args, kwargs):\n    """"""Decorator which ensures a wrapped method is only ever run once.""""""\n    if instance is None:\n      # NOTE: We can\'t use the weakset since you can\'t weakref None.\n      if not wrapper.seen_none:\n        _check_no_output(wrapped(*args, **kwargs))\n        wrapper.seen_none = True\n      return\n\n    # Get or set the `seen` set for this object.\n    seen = getattr(instance, _ONCE_PROPERTY, None)\n    if seen is None:\n      seen = set()\n      setattr(instance, _ONCE_PROPERTY, seen)\n\n    if once_id not in seen:\n      _check_no_output(wrapped(*args, **kwargs))\n      seen.add(once_id)\n\n  wrapper.seen_none = False\n\n  decorated = wrapper(f)  # pylint: disable=no-value-for-parameter,assignment-from-none\n  decorated.__snt_once_wrapped__ = f\n  return decorated\n'"
sonnet/src/once_test.py,0,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.once.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport pickle\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nfrom sonnet.src import once\n\n\nclass OnceTest(parameterized.TestCase):\n\n  def test_runs_once(self):\n    r = []\n\n    @once.once\n    def f():\n      r.append(None)\n\n    for _ in range(3):\n      f()\n\n    self.assertEqual(r, [None])\n\n  def test_always_returns_none(self):\n    f = once.once(lambda: ""Hello, world!"")\n    with self.assertRaisesRegexp(ValueError, ""snt.once .* cannot return""):\n      f()\n\n  def test_does_not_cache_on_error(self):\n\n    @once.once\n    def f():\n      raise ValueError\n\n    with self.assertRaises(ValueError):\n      f()\n    with self.assertRaises(ValueError):\n      f()\n\n  def test_method(self):\n    o1 = Counter()\n    o2 = Counter()\n    for _ in range(10):\n      o1.increment()\n      o2.increment()\n\n    self.assertEqual(o1.call_count, 1)\n    self.assertEqual(o2.call_count, 1)\n\n  def test_method_does_not_cache_on_error(self):\n\n    class Dummy(object):\n\n      @once.once\n      def f(self):\n        raise ValueError\n\n    o = Dummy()\n    with self.assertRaises(ValueError):\n      o.f()\n    with self.assertRaises(ValueError):\n      o.f()\n\n  def test_pickle_method_before_evaluation(self):\n    c1 = Counter()\n    c2 = pickle.loads(pickle.dumps(c1))\n    c1.increment()\n    self.assertEqual(c1.call_count, 1)\n    self.assertEqual(c2.call_count, 0)\n    c2.increment()\n    self.assertEqual(c1.call_count, 1)\n    self.assertEqual(c2.call_count, 1)\n\n  def test_pickle_method_already_evaluated(self):\n    c1 = Counter()\n    c1.increment()\n    self.assertEqual(c1.call_count, 1)\n    c2 = pickle.loads(pickle.dumps(c1))\n    self.assertEqual(c2.call_count, 1)\n    c2.increment()\n    self.assertEqual(c2.call_count, 1)\n\n  def test_inline(self):\n    r = []\n    f = once.once(lambda: r.append(None))\n    for _ in range(10):\n      f()\n    self.assertEqual(r, [None])\n\n  @parameterized.named_parameters(\n      (""lambda"", lambda: lambda: None), (""function"", lambda: nop),\n      (""method"", lambda: NoOpCallable().nop),\n      (""special_method"", lambda: NoOpCallable().__call__),\n      (""object"", lambda: NoOpCallable()))  # pylint: disable=unnecessary-lambda\n  def test_adds_property(self, factory):\n    f = factory()\n    self.assertIs(once.once(f).__snt_once_wrapped__, f)\n\n\ndef nop():\n  pass\n\n\nclass NoOpCallable(object):\n\n  def nop(self):\n    pass\n\n  def __call__(self):\n    pass\n\n\nclass Counter(object):\n  call_count = 0\n\n  @once.once\n  def increment(self):\n    self.call_count += 1\n\n\nif __name__ == ""__main__"":\n  absltest.main()\n'"
sonnet/src/pad.py,0,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Padding module for Sonnet.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nfrom sonnet.src import utils\nfrom typing import Callable, Sequence, Union\n\nPadding = Callable[[int], Sequence[int]]\nPaddings = Union[Padding, Sequence[Padding]]\n\n\ndef valid(effective_kernel_size: int):  # pylint: disable=unused-argument\n  """"""No padding.""""""\n  return [0, 0]\n\n\ndef same(effective_kernel_size: int):\n  """"""Pads such that the output size matches input size for stride=1.""""""\n  return [(effective_kernel_size - 1) // 2, effective_kernel_size // 2]\n\n\ndef full(effective_kernel_size: int):\n  """"""Maximal padding whilst not convolving over just padded elements.""""""\n  return [effective_kernel_size - 1, effective_kernel_size - 1]\n\n\ndef causal(effective_kernel_size: int):\n  """"""Pre-padding such that output has no dependence on the future.""""""\n  return [effective_kernel_size - 1, 0]\n\n\ndef reverse_causal(effective_kernel_size: int):\n  """"""Post-padding such that output has no dependence on the past.""""""\n  return [0, effective_kernel_size - 1]\n\n\ndef create(\n    padding: Paddings,\n    kernel: Union[int, Sequence[int]],\n    rate: Union[int, Sequence[int]],\n    n: int,\n    channel_index: int,\n):\n  """"""Generates the padding required for a given padding algorithm.\n\n  Args:\n    padding: callable or list of callables of length n. The callables take an\n      integer representing the effective kernel size (kernel size when the rate\n      is 1) and return a list of two integers representing the padding before\n      and padding after for that dimension.\n    kernel: int or list of ints of length n. The size of the kernel for each\n      dimension. If it is an int it will be replicated for the non channel and\n      batch dimensions.\n    rate: int or list of ints of length n. The dilation rate for each dimension.\n      If it is an int it will be replicated for the non channel and batch\n      dimensions.\n    n: the number of spatial dimensions.\n    channel_index: the channel position of the input to which the padding will\n      be applied.\n\n  Returns:\n    A list of length n+2 containing the padding for each element. These are of\n    the form [pad_before, pad_after].\n  """"""\n  # The effective kernel size includes any holes/gaps introduced by the\n  # dilation rate. It\'s equal to kernel_size when rate == 1.\n  effective_kernel_size = map(  # pylint: disable=deprecated-lambda\n      lambda kernel, rate: (kernel - 1) * rate + 1,\n      utils.replicate(kernel, n, ""kernel""), utils.replicate(rate, n, ""rate""))\n  paddings = map(  # pylint: disable=deprecated-lambda\n      lambda x, y: x(y), utils.replicate(padding, n, ""padding""),\n      effective_kernel_size)\n  if channel_index == 1:  # N, C, ...\n    paddings = [[0, 0], [0, 0]] + list(paddings)\n  else:  # channel_index == -1 N, ..., C\n    paddings = [[0, 0]] + list(paddings) + [[0, 0]]\n\n  return paddings\n'"
sonnet/src/pad_test.py,11,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.pad.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom sonnet.src import pad\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\n\nclass PadTest(test_utils.TestCase, parameterized.TestCase):\n\n  def test_padding_2d(self):\n    a = pad.create([pad.causal, pad.full], [3], [1, 1], 2, -1)\n    self.assertEqual(a, [[0, 0], [2, 0], [2, 2], [0, 0]])\n\n  def test_padding_1d(self):\n    a = pad.create(pad.full, 3, 1, 1, 1)\n    self.assertEqual(a, [[0, 0], [0, 0], [2, 2]])\n\n  def test_padding_3d(self):\n    a = pad.create([pad.causal, pad.full, pad.full], [3, 2, 3], [1], 3, -1)\n    self.assertEqual(a, [[0, 0], [2, 0], [1, 1], [2, 2], [0, 0]])\n\n  @parameterized.parameters((2, [2, 2]), (3, [4, 4, 4, 4]), ([2, 2], 3),\n                            ([4, 4, 4, 4], 3))\n  def test_padding_incorrect_input(self, kernel_size, rate):\n    with self.assertRaisesRegexp(\n        TypeError,\n        r""must be a scalar or sequence of length 1 or sequence of length 3.""):\n      pad.create(pad.full, kernel_size, rate, 3, -1)\n\n  def test_padding_valid(self):\n    a = pad.create(pad.valid, 4, 3, 2, -1)\n    self.assertEqual(a, [[0, 0], [0, 0], [0, 0], [0, 0]])\n\n  def test_padding_same(self):\n    a = pad.create(pad.same, 4, 3, 2, -1)\n    self.assertEqual(a, [[0, 0], [4, 5], [4, 5], [0, 0]])\n\n  def test_padding_full(self):\n    a = pad.create(pad.full, 4, 3, 2, -1)\n    self.assertEqual(a, [[0, 0], [9, 9], [9, 9], [0, 0]])\n\n  def test_padding_causal(self):\n    a = pad.create(pad.causal, 4, 3, 2, -1)\n    self.assertEqual(a, [[0, 0], [9, 0], [9, 0], [0, 0]])\n\n  def test_padding_reverse_causal(self):\n    a = pad.create(pad.reverse_causal, 4, 3, 2, -1)\n    self.assertEqual(a, [[0, 0], [0, 9], [0, 9], [0, 0]])\n\n  @parameterized.parameters((1, 1, 1), (3, 1, 1), (1, 3, 1), (1, 1, 3),\n                            (3, 3, 1), (3, 1, 3), (1, 3, 3), (3, 3, 3))\n  def test_same_padding(self, kernel_size, stride, rate):\n    a = tf.random.normal([2, 4, 3])\n    k = tf.random.normal([kernel_size, 3, 4])\n    padding = pad.create(pad.same, kernel_size, rate, 1, -1)\n    a_padded = tf.pad(a, padding)\n    y1 = tf.nn.conv1d(\n        a_padded, k, stride=stride, dilations=rate, padding=""VALID"")\n    y2 = tf.nn.conv1d(a, k, stride=stride, dilations=rate, padding=""SAME"")\n    self.assertEqual(y1.shape, y2.shape)\n    self.assertAllClose(y1.numpy(), y2.numpy())\n\n  @parameterized.parameters((1, 1, 1), (3, 1, 1), (1, 3, 1), (1, 1, 3),\n                            (3, 3, 1), (3, 1, 3), (1, 3, 3), (3, 3, 3))\n  def test_valid_padding(self, kernel_size, stride, rate):\n    a = tf.random.normal([2, 8, 3])\n    k = tf.random.normal([kernel_size, 3, 4])\n    padding = pad.create(pad.valid, kernel_size, rate, 1, -1)\n    a_padded = tf.pad(a, padding)\n    y1 = tf.nn.conv1d(\n        a_padded, k, stride=stride, dilations=rate, padding=""VALID"")\n    y2 = tf.nn.conv1d(a, k, stride=stride, dilations=rate, padding=""VALID"")\n    self.assertAllEqual(y1.numpy(), y2.numpy())\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/parallel_linear.py,8,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Parallel linear module.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport math\n\nfrom sonnet.src import base\nfrom sonnet.src import initializers\nfrom sonnet.src import once\nfrom sonnet.src import utils\nimport tensorflow as tf\nfrom typing import Optional, Text\n\n\nclass ParallelLinears(base.Module):\n  """"""Parallel linear.\n\n  This is equivalent to n separate linears applied in parallel to n inputs. It\n  takes an input of shape [num_linears, batch_size, input_size] and returns an\n  output of shape [num_linears, batch_size, output_size].\n\n  It uses a single batched matmul which is more efficient than stacking separate\n  snt.Linear layers. This is implemented using `num_linear`s first to avoid the\n  need for transposes in order to make it efficient when stacking these.\n  """"""\n\n  def __init__(self,\n               output_size: int,\n               with_bias: bool = True,\n               w_init: Optional[initializers.Initializer] = None,\n               b_init: Optional[initializers.Initializer] = None,\n               name: Optional[Text] = None):\n    """"""Constructs a `ParallelLinear` module.\n\n    Args:\n      output_size: Output dimensionality.\n      with_bias: Whether to include bias parameters. Default `True`.\n      w_init: Optional initializer for the weights. By default the weights are\n        initialized truncated random normal values with a standard deviation of\n        `1 / sqrt(input_feature_size)`, which is commonly used when the inputs\n        are zero centered (see https://arxiv.org/abs/1502.03167v3).\n      b_init: Optional initializer for the bias. By default the bias is\n        initialized to zero.\n      name: Name of the module.\n    """"""\n    super(ParallelLinears, self).__init__(name=name)\n    self.output_size = output_size\n    self.with_bias = with_bias\n    self.w_init = w_init\n    if with_bias:\n      self.b_init = b_init if b_init is not None else initializers.Zeros()\n    elif b_init is not None:\n      raise ValueError(""When not using a bias the b_init must be None."")\n\n  @once.once\n  def _initialize(self, inputs: tf.Tensor):\n    """"""Constructs parameters used by this module.""""""\n    utils.assert_rank(inputs, 3)\n\n    self.input_size = inputs.shape[2]\n    if self.input_size is None:  # Can happen inside an @tf.function.\n      raise ValueError(""Input size must be specified at module build time."")\n    num_linears = inputs.shape[0]\n    if num_linears is None:  # Can happen inside an @tf.function.\n      raise ValueError(\n          ""The number of linears must be specified at module build time."")\n\n    if self.w_init is None:\n      # See https://arxiv.org/abs/1502.03167v3.\n      stddev = 1. / math.sqrt(self.input_size)\n      self.w_init = initializers.TruncatedNormal(stddev=stddev)\n\n    self.w = tf.Variable(\n        self.w_init([num_linears, self.input_size, self.output_size],\n                    inputs.dtype),\n        name=""w"")\n\n    if self.with_bias:\n      self.b = tf.Variable(\n          self.b_init([num_linears, 1, self.output_size], inputs.dtype),\n          name=""b"")\n\n  def __call__(self, inputs: tf.Tensor) -> tf.Tensor:\n    self._initialize(inputs)\n\n    outputs = tf.matmul(inputs, self.w)\n    if self.with_bias:\n      outputs = tf.add(outputs, self.b)\n    return outputs\n'"
sonnet/src/parallel_linear_test.py,6,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.parallel_linear.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom sonnet.src import linear\nfrom sonnet.src import parallel_linear\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\n\nclass ParallelLinearTest(test_utils.TestCase):\n\n  def test_output_size_correct(self):\n    layer = parallel_linear.ParallelLinears(3)\n\n    outputs = layer(tf.ones([4, 2, 6]))\n    self.assertEqual(outputs.shape, [4, 2, 3])\n\n  def test_behaves_same_as_stacked_linears(self):\n    w_init = tf.random.normal((3, 5, 7))\n    b_init = tf.random.normal((3, 1, 7))\n    inputs = tf.random.normal((3, 2, 5))\n\n    parallel = parallel_linear.ParallelLinears(\n        7, w_init=lambda s, d: w_init, b_init=lambda s, d: b_init)\n    parallel_outputs = parallel(inputs)\n\n    stacked_outputs = []\n    for i in range(3):\n      layer = linear.Linear(\n          7,\n          w_init=lambda s, d, i=i: w_init[i],\n          b_init=lambda s, d, i=i: b_init[i])\n      stacked_outputs.append(layer(inputs[i]))\n    stacked_outputs = tf.stack(stacked_outputs, axis=0)\n\n    self.assertAllClose(parallel_outputs.numpy(), stacked_outputs.numpy())\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
sonnet/src/recurrent.py,127,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Recurrent Neural Network cores.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport abc\nimport collections\nimport functools\nimport uuid\n\nimport six\nfrom sonnet.src import base\nfrom sonnet.src import conv\nfrom sonnet.src import initializers\nfrom sonnet.src import linear\nfrom sonnet.src import once\nfrom sonnet.src import types\nfrom sonnet.src import utils\n\nimport tensorflow.compat.v1 as tf1\nimport tensorflow as tf\nimport tree\n\nfrom typing import Optional, Sequence, Text, Tuple, Union\n\n# pylint: disable=g-direct-tensorflow-import\n# Required for specializing `UnrolledLSTM` per device.\nfrom tensorflow.python import context as context_lib\nfrom tensorflow.python.eager import function as function_lib\n# pylint: enable=g-direct-tensorflow-import\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass RNNCore(base.Module):\n  """"""Base class for Recurrent Neural Network cores.\n\n  This class defines the basic functionality that every core should\n  implement: :meth:`initial_state`, used to construct an example of the\n  core state; and :meth:`__call__` which applies the core parameterized\n  by a previous state to an input.\n\n  Cores are typically used with :func:`dynamic_unroll` and\n  :func:`static_unroll` to iteratively construct an output sequence from\n  the given input sequence.\n  """"""\n\n  @abc.abstractmethod\n  def __call__(self, inputs: types.TensorNest, prev_state):\n    """"""Performs one step of an RNN.\n\n    Args:\n      inputs: An arbitrarily nested structure of shape [B, ...] where B is the\n        batch size.\n      prev_state: Previous core state.\n\n    Returns:\n      A tuple with two elements:\n      * **outputs** - An arbitrarily nested structure of shape [B, ...].\n        Dimensions following the batch size could be different from that\n        of `inputs`.\n      * **next_state** - Next core state, must be of the same shape as the\n        previous one.\n    """"""\n\n  @abc.abstractmethod\n  def initial_state(self, batch_size: types.IntegerLike, **kwargs):\n    """"""Constructs an initial state for this core.\n\n    Args:\n      batch_size: An int or an integral scalar tensor representing batch size.\n      **kwargs: Optional keyword arguments.\n\n    Returns:\n      Arbitrarily nested initial state for this core.\n    """"""\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass UnrolledRNN(base.Module):\n  """"""Base class for unrolled Recurrent Neural Networks.\n\n  This class is a generalization of :class:`RNNCore` which operates on\n  an input sequence as opposed to a single time step.\n  """"""\n\n  @abc.abstractmethod\n  def __call__(self, input_sequence: types.TensorNest,\n               initial_state: types.TensorNest):\n    """"""Apply this RNN to the input sequence.\n\n    Args:\n      input_sequence: An arbitrarily nested structure of shape ``[T, B, ...]``\n        where ``T`` is the number of time steps and B is the batch size.\n      initial_state: Initial RNN state.\n\n    Returns:\n      A tuple with two elements:\n        * **output_sequence** - An arbitrarily nested structure of tensors of\n          shape ``[T, B, ...]``. Dimensions following the batch size could be\n          different from that of the ``input_sequence``.\n        * **final_state** - Final RNN state, must be of the same shape as the\n          initial one.\n    """"""\n\n  @abc.abstractmethod\n  def initial_state(self, batch_size: types.IntegerLike, **kwargs):\n    """"""Construct an initial state for this RNN.\n\n    Args:\n      batch_size: An int or an integral scalar tensor representing batch size.\n      **kwargs: Optional keyword arguments.\n\n    Returns:\n      Arbitrarily nested initial state for this RNN.\n    """"""\n\n\nclass TrainableState(base.Module):\n  """"""Trainable state for an :class:`RNNCore`.\n\n  The state can be constructed manually from a nest of initial values::\n\n      >>> state = snt.TrainableState((tf.zeros([16]), tf.zeros([16])))\n\n  or automatically for a given :class:`RNNCore`::\n\n      >>> core = snt.LSTM(hidden_size=16)\n      >>> state = snt.TrainableState.for_core(core)\n  """"""\n\n  @classmethod\n  def for_core(cls,\n               core: RNNCore,\n               mask: Optional[types.TensorNest] = None,\n               name: Optional[Text] = None):\n    """"""Constructs a trainable state for a given :class:`RNNCore`.\n\n    Args:\n      core: An :class:`RNNCore` to construct the state for.\n      mask: Optional boolean mask of the same structure as the initial state of\n        `core` specifying which components should be trainable. If not given,\n        the whole state is considered trainable.\n      name: Name of the module.\n\n    Returns:\n      A `TrainableState`.\n    """"""\n    initial_values = tree.map_structure(lambda s: tf.squeeze(s, axis=0),\n                                        core.initial_state(batch_size=1))\n    return cls(initial_values, mask, name)\n\n  def __init__(self,\n               initial_values: types.TensorNest,\n               mask: types.TensorNest = None,\n               name: Optional[Text] = None):\n    """"""Constructs a trainable state from initial values.\n\n    Args:\n      initial_values: Arbitrarily nested initial values for the state.\n      mask: Optional boolean mask of the same structure as ``initial_values``\n        specifying which components should be trainable. If not given, the whole\n        state is considered trainable.\n      name: Name of the module.\n    """"""\n    super(TrainableState, self).__init__(name)\n\n    flat_initial_values = tree.flatten_with_path(initial_values)\n    if mask is None:\n      flat_mask = [True] * len(flat_initial_values)\n    else:\n      tree.assert_same_structure(initial_values, mask)\n      flat_mask = tree.flatten(mask)\n\n    flat_template = []\n    for (path, initial_value), trainable in zip(flat_initial_values, flat_mask):\n      # `""state""` is only used if initial_values is not nested.\n      name = ""/"".join(map(str, path)) or ""state""\n      flat_template.append(\n          tf.Variable(\n              tf.expand_dims(initial_value, axis=0),\n              trainable=trainable,\n              name=name))\n\n    self._template = tree.unflatten_as(initial_values, flat_template)\n\n  def __call__(self, batch_size: int) -> types.TensorNest:\n    """"""Returns a trainable state for the given batch size.""""""\n    return tree.map_structure(\n        lambda s: tf.tile(s, [batch_size] + [1] * (s.shape.rank - 1)),\n        self._template)\n\n\ndef static_unroll(\n    core: RNNCore,\n    input_sequence: types.TensorNest,  # time-major.\n    initial_state: types.TensorNest,\n    sequence_length: Optional[types.IntegerLike] = None\n) -> Tuple[types.TensorNest, types.TensorNest]:\n  """"""Performs a static unroll of an RNN.\n\n      >>> core = snt.LSTM(hidden_size=16)\n      >>> batch_size = 3\n      >>> input_sequence = tf.random.uniform([1, batch_size, 2])\n      >>> output_sequence, final_state = snt.static_unroll(\n      ...     core,\n      ...     input_sequence,\n      ...     core.initial_state(batch_size))\n\n  An *unroll* corresponds to calling the core on each element of the\n  input sequence in a loop, carrying the state through::\n\n      state = initial_state\n      for t in range(len(input_sequence)):\n         outputs, state = core(input_sequence[t], state)\n\n  A *static* unroll replaces a loop with its body repeated multiple\n  times when executed inside :tf:`function`::\n\n      state = initial_state\n      outputs0, state = core(input_sequence[0], state)\n      outputs1, state = core(input_sequence[1], state)\n      outputs2, state = core(input_sequence[2], state)\n      ...\n\n  See :func:`dynamic_unroll` for a loop-preserving unroll function.\n\n  Args:\n    core: An :class:`RNNCore` to unroll.\n    input_sequence: An arbitrarily nested structure of tensors of shape\n      ``[T, B, ...]`` where ``T`` is the number of time steps, and ``B`` is\n      the batch size.\n    initial_state: An initial state of the given core.\n    sequence_length: An optional tensor of shape ``[B]`` specifying the lengths\n      of sequences within the (padded) batch.\n\n  Returns:\n    A tuple with two elements:\n      * **output_sequence** - An arbitrarily nested structure of tensors\n        of shape ``[T, B, ...]``. Dimensions following the batch size could\n        be different from that of the ``input_sequence``.\n      * **final_state** - Core state at time step ``T``.\n\n  Raises:\n    ValueError: If ``input_sequence`` is empty or its leading dimension is\n      not known statically.\n  """"""\n  num_steps, input_tas = _unstack_input_sequence(input_sequence)\n  if not isinstance(num_steps, six.integer_types):\n    raise ValueError(\n        ""input_sequence must have a statically known number of time steps"")\n\n  outputs = None\n  state = initial_state\n  output_accs = None\n  for t in six.moves.range(num_steps):\n    outputs, state = _rnn_step(\n        core,\n        input_tas,\n        sequence_length,\n        t,\n        prev_outputs=outputs,\n        prev_state=state)\n    if t == 0:\n      output_accs = tree.map_structure(lambda o: _ListWrapper([o]), outputs)\n    else:\n      tree.map_structure(lambda acc, o: acc.data.append(o), output_accs,\n                         outputs)\n\n  output_sequence = tree.map_structure(lambda acc: tf.stack(acc.data),\n                                       output_accs)\n  return output_sequence, state\n\n\nclass _ListWrapper(object):\n  """"""A wrapper hiding a list from `nest`.\n\n  This allows to use `tree.map_structure` without recursing into the\n  wrapped list.\n  """"""\n\n  __slots__ = [""data""]\n\n  def __init__(self, data):\n    self.data = data\n\n\n# TODO(slebedev): core can be core_fn: Callable[[I, S], Tuple[O, S]].\n# TODO(slebedev): explain sequence_length with ASCII art?\n@utils.smart_autograph\ndef dynamic_unroll(\n    core,\n    input_sequence,  # time-major.\n    initial_state,\n    sequence_length=None,\n    parallel_iterations=1,\n    swap_memory=False):\n  """"""Performs a dynamic unroll of an RNN.\n\n      >>> core = snt.LSTM(hidden_size=16)\n      >>> batch_size = 3\n      >>> input_sequence = tf.random.uniform([1, batch_size, 2])\n      >>> output_sequence, final_state = snt.dynamic_unroll(\n      ...     core,\n      ...     input_sequence,\n      ...     core.initial_state(batch_size))\n\n  An *unroll* corresponds to calling the core on each element of the\n  input sequence in a loop, carrying the state through::\n\n      state = initial_state\n      for t in range(len(input_sequence)):\n         outputs, state = core(input_sequence[t], state)\n\n  A *dynamic* unroll preserves the loop structure when executed within\n  :tf:`function`. See :func:`static_unroll` for an unroll function which\n  replaces a loop with its body repeated multiple times.\n\n  Args:\n    core: An :class:`RNNCore` to unroll.\n    input_sequence: An arbitrarily nested structure of tensors of shape\n      ``[T, B, ...]`` where ``T`` is the number of time steps, and ``B`` is the\n      batch size.\n    initial_state: initial state of the given core.\n    sequence_length: An optional tensor of shape ``[B]`` specifying the lengths\n      of sequences within the (padded) batch.\n    parallel_iterations: An optional ``int`` specifying the number of iterations\n      to run in parallel. Those operations which do not have any temporal\n      dependency and can be run in parallel, will be. This parameter trades off\n      time for space. Values >> 1 use more memory but take less time, while\n      smaller values use less memory but computations take longer. Defaults to\n      1.\n    swap_memory: Transparently swap the tensors produced in forward inference\n      but needed for back prop from GPU to CPU. This allows training RNNs which\n      would typically not fit on a single GPU, with very minimal (or no)\n      performance penalty. Defaults to False.\n\n  Returns:\n    A tuple with two elements:\n      * **output_sequence** - An arbitrarily nested structure of tensors\n        of shape ``[T, B, ...]``. Dimensions following the batch size could\n        be different from that of the ``input_sequence``.\n      * **final_state** - Core state at time step ``T``.\n\n  Raises:\n    ValueError: If ``input_sequence`` is empty.\n  """"""\n  num_steps, input_tas = _unstack_input_sequence(input_sequence)\n\n  # Unroll the first time step separately to infer outputs structure.\n  outputs, state = _rnn_step(\n      core,\n      input_tas,\n      sequence_length,\n      t=0,\n      prev_outputs=None,\n      prev_state=initial_state)\n  output_tas = tree.map_structure(\n      lambda o: tf.TensorArray(o.dtype, num_steps).write(0, o), outputs)\n\n  # AutoGraph converts a for loop over `tf.range` to `tf.while_loop`.\n  # `maximum_iterations` are needed to backprop through the loop on TPU.\n  for t in tf.range(1, num_steps):\n    tf.autograph.experimental.set_loop_options(\n        parallel_iterations=parallel_iterations,\n        swap_memory=swap_memory,\n        maximum_iterations=num_steps - 1)\n    outputs, state = _rnn_step(\n        core,\n        input_tas,\n        sequence_length,\n        t,\n        prev_outputs=outputs,\n        prev_state=state)\n    output_tas = tree.map_structure(\n        lambda ta, o, _t=t: ta.write(_t, o), output_tas, outputs)\n\n  output_sequence = tree.map_structure(tf.TensorArray.stack, output_tas)\n  return output_sequence, state\n\n\ndef _unstack_input_sequence(input_sequence):\n  r""""""Unstacks the input sequence into a nest of :tf:`TensorArray`\\ s.\n\n  This allows to traverse the input sequence using :tf:`TensorArray.read`\n  instead of a slice, avoiding O(sliced tensor) slice gradient\n  computation during the backwards pass.\n\n  Args:\n    input_sequence: See :func:`dynamic_unroll` or :func:`static_unroll`.\n\n  Returns:\n    num_steps: Number of steps in the input sequence.\n    input_tas: An arbitrarily nested structure of :tf:`TensorArray`\\ s of\n      size ``num_steps``.\n\n  Raises:\n    ValueError: If tensors in ``input_sequence`` have inconsistent number\n      of steps or the number of steps is 0.\n  """"""\n  flat_input_sequence = tree.flatten(input_sequence)\n  all_num_steps = {i.shape[0] for i in flat_input_sequence}\n  if len(all_num_steps) > 1:\n    raise ValueError(\n        ""input_sequence tensors must have consistent number of time steps"")\n  [num_steps] = all_num_steps\n  if num_steps == 0:\n    raise ValueError(""input_sequence must have at least a single time step"")\n  elif num_steps is None:\n    # Number of steps is not known statically, fall back to dynamic shape.\n    num_steps = tf.shape(flat_input_sequence[0])[0]\n    # TODO(b/141910613): uncomment when the bug is fixed.\n    # for i in flat_input_sequence[1:]:\n    #   tf.debugging.assert_equal(\n    #       tf.shape(i)[0], num_steps,\n    #       ""input_sequence tensors must have consistent number of time steps"")\n\n  input_tas = tree.map_structure(\n      lambda i: tf.TensorArray(i.dtype, num_steps).unstack(i), input_sequence)\n  return num_steps, input_tas\n\n\ndef _safe_where(condition, x, y):  # pylint: disable=g-doc-args\n  """"""`tf.where` which allows scalar inputs.""""""\n  if x.shape.rank == 0:\n    # This is to match the `tf.nn.*_rnn` behavior. In general, we might\n    # want to branch on `tf.reduce_all(condition)`.\n    return y\n  # TODO(tomhennigan) Broadcasting with SelectV2 is currently broken.\n  return tf1.where(condition, x, y)\n\n\ndef _rnn_step(core, input_tas, sequence_length, t, prev_outputs, prev_state):\n  """"""Performs a single RNN step optionally accounting for variable length.""""""\n  outputs, state = core(\n      tree.map_structure(lambda i: i.read(t), input_tas), prev_state)\n\n  if prev_outputs is None:\n    assert t == 0\n    prev_outputs = tree.map_structure(tf.zeros_like, outputs)\n\n  # TODO(slebedev): do not go into this block if t < min_len.\n  if sequence_length is not None:\n    # Selectively propagate outputs/state to the not-yet-finished\n    # sequences.\n    maybe_propagate = functools.partial(_safe_where, t >= sequence_length)\n    outputs = tree.map_structure(maybe_propagate, prev_outputs, outputs)\n    state = tree.map_structure(maybe_propagate, prev_state, state)\n\n  return outputs, state\n\n\nclass VanillaRNN(RNNCore):\n  """"""Basic fully-connected RNN core.\n\n  Given :math:`x_t` and the previous hidden state :math:`h_{t-1}` the\n  core computes\n\n  .. math::\n\n     h_t = w_i x_t + w_h h_{t-1} + b\n\n  Attributes:\n    input_to_hidden: Input-to-hidden weights :math:`w_i`, a tensor of shape\n      ``[hidden_size, hidden_size]``.\n    hidden_to_hidden: Hidden-to-hidden weights :math:`w_i`, a tensor of shape\n      ``[input_size, hidden_size]``.\n    b: bias, a tensor or shape ``[hidden_size]``.\n  """"""\n\n  def __init__(self,\n               hidden_size: int,\n               activation: types.ActivationFn = tf.tanh,\n               w_i_init: Optional[initializers.Initializer] = None,\n               w_h_init: Optional[initializers.Initializer] = None,\n               b_init: Optional[initializers.Initializer] = None,\n               dtype: tf.DType = tf.float32,\n               name: Optional[Text] = None):\n    """"""Constructs a vanilla RNN core.\n\n    Args:\n      hidden_size: Hidden layer size.\n      activation: Activation function to use. Defaults to ``tf.tanh``.\n      w_i_init: Optional initializer for the input-to-hidden weights.\n        Defaults to :class:`~initializers.TruncatedNormal` with a standard\n        deviation of ``1 / sqrt(input_size)``.\n      w_h_init: Optional initializer for the hidden-to-hidden weights.\n        Defaults to :class:`~initializers.TruncatedNormal` with a standard\n        deviation of ``1 / sqrt(hidden_size)``.\n      b_init: Optional initializer for the bias. Defaults to\n        :class:`~initializers.Zeros`.\n      dtype: Optional :tf:`DType` of the core\'s variables. Defaults to\n        ``tf.float32``.\n      name: Name of the module.\n    """"""\n    super(VanillaRNN, self).__init__(name)\n    self._hidden_size = hidden_size\n    self._activation = activation\n    self._b_init = b_init or initializers.Zeros()\n    self._dtype = dtype\n\n    self._input_to_hidden = linear.Linear(\n        hidden_size, with_bias=False, w_init=w_i_init, name=""input_to_hidden"")\n    self._hidden_to_hidden = linear.Linear(\n        hidden_size, with_bias=False, w_init=w_h_init, name=""hidden_to_hidden"")\n\n  @property\n  def input_to_hidden(self) -> tf.Variable:\n    return self._input_to_hidden.w\n\n  @property\n  def hidden_to_hidden(self) -> tf.Variable:\n    return self._hidden_to_hidden.w\n\n  def __call__(self, inputs: types.TensorNest,\n               prev_state: types.TensorNest) -> Tuple[tf.Tensor, tf.Tensor]:\n    """"""See base class.""""""\n    self._initialize(inputs)\n\n    outputs = self._activation(\n        self._input_to_hidden(inputs) + self._hidden_to_hidden(prev_state) +\n        self._b)\n\n    # For VanillaRNN, the next state of the RNN is the same as the outputs.\n    return outputs, outputs\n\n  def initial_state(self, batch_size: int) -> tf.Tensor:\n    """"""See base class.""""""\n    return tf.zeros(shape=[batch_size, self._hidden_size], dtype=self._dtype)\n\n  @once.once\n  def _initialize(self, inputs: tf.Tensor):\n    dtype = _check_inputs_dtype(inputs, self._dtype)\n    self._b = tf.Variable(self._b_init([self._hidden_size], dtype), name=""b"")\n\n\nclass _LegacyDeepRNN(RNNCore):\n  """"""Sonnet 1 compatible :class:`DeepRNN` implementation.\n\n  This class is not intended to be used directly. Refer to :class:`DeepRNN`\n  and ``deep_rnn_with_*_connections``.\n  """"""\n\n  def __init__(self,\n               layers,\n               skip_connections,\n               concat_final_output_if_skip=True,\n               name: Optional[Text] = None):\n    r""""""Constructs a ``DeepRNN``.\n\n    Args:\n      layers: A list of :class:`RNNCore`\\ s or callables.\n      skip_connections: See :func:`deep_rnn_with_skip_connections`.\n      concat_final_output_if_skip: See :func:`deep_rnn_with_skip_connections`.\n      name: Name of the module.\n    """"""\n    super(_LegacyDeepRNN, self).__init__(name)\n    self._layers = layers if layers is not None else []\n    self._skip_connections = skip_connections\n    self._concat_final_output_if_skip = concat_final_output_if_skip\n\n  def __call__(self, inputs, prev_state):\n    """"""See base class.""""""\n    current_inputs = inputs\n    outputs = []\n    next_states = []\n    recurrent_idx = 0\n    concat = lambda *args: tf.concat(args, axis=-1)\n    for idx, layer in enumerate(self._layers):\n      if self._skip_connections and idx > 0:\n        current_inputs = tree.map_structure(concat, inputs, current_inputs)\n\n      if isinstance(layer, RNNCore):\n        current_inputs, next_state = layer(current_inputs,\n                                           prev_state[recurrent_idx])\n        next_states.append(next_state)\n        recurrent_idx += 1\n      else:\n        current_inputs = layer(current_inputs)\n\n      if self._skip_connections:\n        outputs.append(current_inputs)\n\n    if self._skip_connections and self._concat_final_output_if_skip:\n      outputs = tree.map_structure(concat, *outputs)\n    else:\n      outputs = current_inputs\n\n    return outputs, tuple(next_states)\n\n  def initial_state(self, batch_size, **kwargs):\n    """"""See base class.""""""\n    return tuple(\n        layer.initial_state(batch_size, **kwargs)\n        for layer in self._layers\n        if isinstance(layer, RNNCore))\n\n\nclass DeepRNN(_LegacyDeepRNN):\n  r""""""Linear chain of :class:`RNNCore`\\ s or callables.\n\n  The core takes ``(input, prev_state)`` as input and passes the input\n  through each internal module in the order they were presented, using\n  elements from ``prev_state`` as necessary for internal RNN cores.\n\n      >>> deep_rnn = snt.DeepRNN([\n      ...     snt.LSTM(hidden_size=16),\n      ...     snt.LSTM(hidden_size=16),\n      ... ])\n\n  Note that the state of a ``DeepRNN`` is always a tuple, which will\n  contain the same number of elements as there are internal RNN cores.\n  If no internal modules are RNN cores, the state of the ``DeepRNN`` as\n  a whole is an empty tuple.\n\n  Wrapping non-recurrent modules into a ``DeepRNN`` can be useful to\n  produce something API compatible with a ""real"" recurrent module,\n  simplifying code that handles the cores.\n  """"""\n\n  # TODO(slebedev): currently called `layers` to be in-sync with `Sequential`.\n  def __init__(self, layers, name: Optional[Text] = None):\n    super(DeepRNN, self).__init__(layers, skip_connections=False, name=name)\n\n\ndef deep_rnn_with_skip_connections(\n    layers: Sequence[RNNCore],\n    concat_final_output: bool = True,\n    name: Text = ""deep_rnn_with_skip_connections"") -> RNNCore:\n  r""""""Constructs a :class:`DeepRNN` with skip connections.\n\n  Skip connections alter the dependency structure within a :class:`DeepRNN`.\n  Specifically, input to the i-th layer (i > 0) is given by a\n  concatenation of the core\'s inputs and the outputs of the (i-1)-th layer.\n  ::\n\n      outputs0, ... = layers[0](inputs, ...)\n      outputs1, ... = layers[1](tf.concat([inputs, outputs0], axis=1], ...)\n      outputs2, ... = layers[2](tf.concat([inputs, outputs1], axis=1], ...)\n      ...\n\n  This allows the layers to learn decoupled features.\n\n  Args:\n    layers: A list of :class:`RNNCore`\\ s.\n    concat_final_output: If enabled (default), the outputs of the core is a\n      concatenation of the outputs of all intermediate layers; otherwise, only\n      the outputs of the final layer, i.e. that of ``layers[-1]``, are returned.\n    name: Name of the module.\n\n  Returns:\n    A :class:`DeepRNN` with skip connections.\n\n  Raises:\n    ValueError: If any of the layers is not an :class:`RNNCore`.\n  """"""\n  if not all(isinstance(l, RNNCore) for l in layers):\n    raise ValueError(""deep_rnn_with_skip_connections requires all layers to be ""\n                     ""instances of RNNCore"")\n\n  return _LegacyDeepRNN(\n      layers,\n      skip_connections=True,\n      concat_final_output_if_skip=concat_final_output,\n      name=name)\n\n\nclass _ResidualWrapper(RNNCore):\n  """"""Residual connection wrapper for a base :class:`RNNCore`.\n\n  The output of the wrapper is the sum of the outputs of the base core\n  with its inputs.\n  """"""\n\n  def __init__(self, base_core: RNNCore):\n    super(_ResidualWrapper, self).__init__(name=base_core.name + ""_residual"")\n    self._base_core = base_core\n\n  def __call__(self, inputs: types.TensorNest, prev_state: types.TensorNest):\n    """"""See base class.""""""\n    outputs, next_state = self._base_core(inputs, prev_state)\n    residual = tree.map_structure(lambda i, o: i + o, inputs, outputs)\n    return residual, next_state\n\n  def initial_state(self, batch_size, **kwargs):\n    return self._base_core.initial_state(batch_size, **kwargs)\n\n\ndef deep_rnn_with_residual_connections(\n    layers: Sequence[RNNCore],\n    name: Text = ""deep_rnn_with_residual_connections"") -> RNNCore:\n  r""""""Constructs a :class:`DeepRNN` with residual connections.\n\n  Residual connections alter the dependency structure in a :class:`DeepRNN`.\n  Specifically, the input to the i-th intermediate layer is a sum of\n  the original core\'s inputs and the outputs of all the preceding\n  layers (<i).\n  ::\n\n      outputs0, ... = layers[0](inputs, ...)\n      outputs0 += inputs\n      outputs1, ... = layers[1](outputs0, ...)\n      outputs1 += outputs0\n      outputs2, ... = layers[2](outputs1, ...)\n      outputs2 += outputs1\n      ...\n\n  This allows the layers to learn specialized features that compose\n  incrementally.\n\n  Args:\n    layers: A list of :class:`RNNCore`\\ s.\n    name: Name of the module.\n\n  Returns:\n    A :class:`DeepRNN` with residual connections.\n\n  Raises:\n    ValueError: If any of the layers is not an :class:`RNNCore`.\n  """"""\n  if not all(isinstance(l, RNNCore) for l in layers):\n    raise ValueError(\n        ""deep_rnn_with_residual_connections requires all layers to be ""\n        ""instances of RNNCore"")\n\n  return _LegacyDeepRNN([_ResidualWrapper(l) for l in layers],\n                        skip_connections=False,\n                        name=name)\n\n\nLSTMState = collections.namedtuple(""LSTMState"", [""hidden"", ""cell""])\n\n\nclass LSTM(RNNCore):\n  r""""""Long short-term memory (LSTM) RNN core.\n\n  The implementation is based on :cite:`zaremba2014recurrent`. Given\n  :math:`x_t` and the previous state :math:`(h_{t-1}, c_{t-1})` the core\n  computes\n\n  .. math::\n\n     \\begin{array}{ll}\n     i_t = \\sigma(W_{ii} x_t + W_{hi} h_{t-1} + b_i) \\\\\n     f_t = \\sigma(W_{if} x_t + W_{hf} h_{t-1} + b_f) \\\\\n     g_t = \\tanh(W_{ig} x_t + W_{hg} h_{t-1} + b_g) \\\\\n     o_t = \\sigma(W_{io} x_t + W_{ho} h_{t-1} + b_o) \\\\\n     c_t = f_t c_{t-1} + i_t g_t \\\\\n     h_t = o_t \\tanh(c_t)\n     \\end{array}\n\n  Where :math:`i_t`, :math:`f_t`, :math:`o_t` are input, forget and\n  output gate activations, and :math:`g_t` is a vector of cell updates.\n\n  Notes:\n    Forget gate initialization:\n      Following :cite:`jozefowicz2015empirical` we add a constant\n      ``forget_bias`` (defaults to 1.0) to :math:`b_f` after initialization\n      in order to reduce the scale of forgetting in the beginning of\n      the training.\n    Recurrent projections:\n      Hidden state could be projected (via the ``project_size`` parameter)\n      to reduce the number of parameters and speed up computation. For more\n      details see :cite:`sak2014long`.\n\n  Attributes:\n    input_to_hidden: Input-to-hidden weights :math:`W_{ii}`, :math:`W_{if}`,\n      :math:`W_{ig}` and :math:`W_{io}` concatenated into a tensor of shape\n      ``[input_size, 4 * hidden_size]``.\n    hidden_to_hidden: Hidden-to-hidden weights :math:`W_{hi}`, :math:`W_{hf}`,\n      :math:`W_{hg}` and :math:`W_{ho}` concatenated into a tensor of shape\n      ``[hidden_size, 4 * hidden_size]``.\n    b: Biases :math:`b_i`, :math:`b_f`, :math:`b_g` and :math:`b_o` concatenated\n      into a tensor of shape ``[4 * hidden_size]``.\n  """"""\n\n  def __init__(self,\n               hidden_size: int,\n               projection_size: Optional[int] = None,\n               projection_init: Optional[initializers.Initializer] = None,\n               w_i_init: Optional[initializers.Initializer] = None,\n               w_h_init: Optional[initializers.Initializer] = None,\n               b_init: Optional[initializers.Initializer] = None,\n               forget_bias: types.FloatLike = 1.0,\n               dtype: tf.DType = tf.float32,\n               name: Optional[Text] = None):\n    """"""Constructs an LSTM.\n\n    Args:\n      hidden_size: Hidden layer size.\n      projection_size: Optional int; if set, then the hidden state is projected\n        to this size via a trainable projection matrix.\n      projection_init: Optional initializer for the projection matrix.\n        Defaults to :class:`~initializers.TruncatedNormal` with a standard\n        deviation of ``1 / sqrt(hidden_size)``.\n      w_i_init: Optional initializer for the input-to-hidden weights.\n        Defaults to :class:`~initializers.TruncatedNormal` with a standard\n        deviation of ``1 / sqrt(input_size)``.\n      w_h_init: Optional initializer for the hidden-to-hidden weights.\n        Defaults to :class:`~initializers.TruncatedNormal` with a standard\n        deviation of ``1 / sqrt(hidden_size)``.\n      b_init: Optional initializer for the biases. Defaults to\n        :class:`~initializers.Zeros`.\n      forget_bias: Optional float to add to the bias of the forget gate after\n        initialization.\n      dtype: Optional :tf:`DType` of the core\'s variables. Defaults to\n        ``tf.float32``.\n      name: Name of the module.\n    """"""\n    super(LSTM, self).__init__(name)\n    self._hidden_size = hidden_size\n    self._projection_size = projection_size\n    self._eff_hidden_size = self._projection_size or self._hidden_size\n    self._projection_init = projection_init\n    if projection_size is None and projection_init is not None:\n      raise ValueError(\n          ""projection_init must be None when projection is not used"")\n\n    self._w_i_init = w_i_init\n    self._w_h_init = w_h_init\n    self._b_init = b_init or initializers.Zeros()\n    self._forget_bias = forget_bias\n    self._dtype = dtype\n\n  def __call__(self, inputs, prev_state):\n    """"""See base class.""""""\n    self._initialize(inputs)\n    return _lstm_fn(inputs, prev_state, self._w_i, self._w_h, self.b,\n                    self.projection)\n\n  def initial_state(self, batch_size: int) -> LSTMState:\n    """"""See base class.""""""\n    return LSTMState(\n        hidden=tf.zeros([batch_size, self._eff_hidden_size], dtype=self._dtype),\n        cell=tf.zeros([batch_size, self._hidden_size], dtype=self._dtype))\n\n  @property\n  def input_to_hidden(self):\n    return self._w_i\n\n  @property\n  def hidden_to_hidden(self):\n    return self._w_h\n\n  @once.once\n  def _initialize(self, inputs):\n    utils.assert_rank(inputs, 2)\n    input_size = inputs.shape[1]\n    dtype = _check_inputs_dtype(inputs, self._dtype)\n\n    w_i_init = self._w_i_init or initializers.TruncatedNormal(\n        stddev=1.0 / tf.sqrt(tf.cast(input_size, dtype)))\n    w_h_init = self._w_h_init or initializers.TruncatedNormal(\n        stddev=1.0 / tf.sqrt(tf.constant(self._eff_hidden_size, dtype=dtype)))\n    self._w_i = tf.Variable(\n        w_i_init([input_size, 4 * self._hidden_size], dtype), name=""w_i"")\n    self._w_h = tf.Variable(\n        w_h_init([self._eff_hidden_size, 4 * self._hidden_size], dtype),\n        name=""w_h"")\n\n    b_i, b_f, b_g, b_o = tf.split(\n        self._b_init([4 * self._hidden_size], dtype), num_or_size_splits=4)\n    b_f += self._forget_bias\n    self.b = tf.Variable(tf.concat([b_i, b_f, b_g, b_o], axis=0), name=""b"")\n\n    if self._projection_size is None:\n      self.projection = None\n    else:\n      projection_init = self._projection_init\n      if projection_init is None:\n        projection_init = initializers.TruncatedNormal(\n            stddev=1.0 / tf.sqrt(tf.constant(self._hidden_size, dtype=dtype)))\n      self.projection = tf.Variable(\n          projection_init([self._hidden_size, self._projection_size], dtype),\n          name=""projection"")\n\n\ndef _lstm_fn(inputs, prev_state, w_i, w_h, b, projection=None):\n  """"""Compute one step of an LSTM.""""""\n  gates_x = tf.matmul(inputs, w_i)\n  gates_h = tf.matmul(prev_state.hidden, w_h)\n  gates = gates_x + gates_h + b\n\n  # i = input, f = forget, g = cell updates, o = output.\n  i, f, g, o = tf.split(gates, num_or_size_splits=4, axis=1)\n\n  next_cell = tf.sigmoid(f) * prev_state.cell\n  next_cell += tf.sigmoid(i) * tf.tanh(g)\n  next_hidden = tf.sigmoid(o) * tf.tanh(next_cell)\n\n  if projection is not None:\n    next_hidden = tf.matmul(next_hidden, projection)\n\n  return next_hidden, LSTMState(hidden=next_hidden, cell=next_cell)\n\n\nclass UnrolledLSTM(UnrolledRNN):\n  """"""Unrolled long short-term memory (LSTM).\n\n  The implementation uses efficient device-specialized ops, e.g. CuDNN-RNN\n  on a CUDA-enabled GPU, and can be an order of magnitude faster than\n  ``snt.*_unroll`` with an :class:`LSTM` core.\n  """"""\n\n  def __init__(self,\n               hidden_size,\n               w_i_init: Optional[initializers.Initializer] = None,\n               w_h_init: Optional[initializers.Initializer] = None,\n               b_init: Optional[initializers.Initializer] = None,\n               forget_bias: types.FloatLike = 1.0,\n               dtype: tf.DType = tf.float32,\n               name: Optional[Text] = None):\n    """"""Construct an unrolled LSTM.\n\n    Args:\n      hidden_size: Hidden layer size.\n      w_i_init: Optional initializer for the input-to-hidden weights.\n        Defaults to :class:`~initializers.TruncatedNormal` with a standard\n        deviation of ``1 / sqrt(input_size)``.\n      w_h_init: Optional initializer for the hidden-to-hidden weights.\n        Defaults to :class:`~initializers.TruncatedNormal` with a standard\n        deviation of ``1 / sqrt(hidden_size)``.\n      b_init: Optional initializer for the biases. Defaults to\n        :class:`~initializers.Zeros`.\n      forget_bias: Optional float to add to the bias of the forget gate after\n        initialization.\n      dtype: Optional :tf:`DType` of the core\'s variables. Defaults to\n        ``tf.float32``.\n      name: Name of the module.\n    """"""\n    super(UnrolledLSTM, self).__init__(name)\n    self._hidden_size = hidden_size\n    self._w_i_init = w_i_init\n    self._w_h_init = w_h_init\n    self._b_init = b_init or initializers.Zeros()\n    self._forget_bias = forget_bias\n    self._dtype = dtype\n\n  def __call__(self, input_sequence, initial_state):\n    """"""See base class.""""""\n    self._initialize(input_sequence)\n    return _specialized_unrolled_lstm(input_sequence, initial_state, self._w_i,\n                                      self._w_h, self.b)\n\n  def initial_state(self, batch_size):\n    """"""See base class.""""""\n    return LSTMState(\n        hidden=tf.zeros([batch_size, self._hidden_size], dtype=self._dtype),\n        cell=tf.zeros([batch_size, self._hidden_size], dtype=self._dtype))\n\n  @property\n  def input_to_hidden(self):\n    return self._w_i\n\n  @property\n  def hidden_to_hidden(self):\n    return self._w_h\n\n  @once.once\n  def _initialize(self, input_sequence):\n    utils.assert_rank(input_sequence, 3)  # [num_steps, batch_size, input_size].\n    input_size = input_sequence.shape[2]\n    dtype = _check_inputs_dtype(input_sequence, self._dtype)\n\n    w_i_init = self._w_i_init or initializers.TruncatedNormal(\n        stddev=1.0 / tf.sqrt(tf.cast(input_size, dtype)))\n    w_h_init = self._w_h_init or initializers.TruncatedNormal(\n        stddev=1.0 / tf.sqrt(tf.constant(self._hidden_size, dtype=dtype)))\n    self._w_i = tf.Variable(\n        w_i_init([input_size, 4 * self._hidden_size], dtype), name=""w_i"")\n    self._w_h = tf.Variable(\n        w_h_init([self._hidden_size, 4 * self._hidden_size], dtype), name=""w_h"")\n\n    b_i, b_f, b_g, b_o = tf.split(\n        self._b_init([4 * self._hidden_size], dtype), num_or_size_splits=4)\n    b_f += self._forget_bias\n    self.b = tf.Variable(tf.concat([b_i, b_f, b_g, b_o], axis=0), name=""b"")\n\n\n# TODO(b/133740216): consider upstreaming into TensorFlow.\ndef _specialize_per_device(api_name, specializations, default):\n  """"""Create a :tf:`function` specialized per-device.\n\n  Args:\n    api_name: Name of the function, e.g. ``""lstm""``.\n    specializations: A mapping from device type (e.g. ``""CPU""`` or ``""TPU``) to\n      a Python function with a specialized implementation for that device.\n    default: Default device type to use (typically, ``""CPU""``).\n\n  Returns:\n    A :tf:`function` which when called dispatches to the specialization\n    for the current device.\n  """"""\n  # Cached to avoid redundant ``ModuleWrapper.__getattribute__`` calls.\n  list_logical_devices = tf.config.experimental.list_logical_devices\n\n  def wrapper(*args, **kwargs):\n    """"""Specialized {}.\n\n    In eager mode the specialization is chosen based on the current\n    device context or, if no device context is active, on availability\n    of a GPU.\n\n    In graph mode (inside tf.function) the choice is delegated to the\n    implementation selector pass in Grappler.\n\n    Args:\n      *args: Positional arguments to pass to the chosen specialization.\n      **kwargs: Keyword arguments to pass to the chosen specialization.\n    """""".format(api_name)\n    ctx = context_lib.context()\n    if ctx.executing_eagerly():\n      device = ctx.device_spec.device_type\n      if device is None:\n        # Soft-placement will never implicitly place an op an a TPU, so\n        # we only need to consider CPU/GPU.\n        device = ""GPU"" if list_logical_devices(""GPU"") else ""CPU""\n\n      specialization = specializations.get(device) or specializations[default]\n      return specialization(*args, **kwargs)\n\n    # Implementation selector requires a globally unique name for each\n    # .register() call.\n    unique_api_name = ""{}_{}"".format(api_name, uuid.uuid4())\n    functions = {}\n    for device, specialization in specializations.items():\n      functions[device] = function_lib.defun_with_attributes(\n          specialization,\n          attributes={\n              ""api_implements"": unique_api_name,\n              ""api_preferred_device"": device\n          })\n      function_lib.register(functions[device], *args, **kwargs)\n    return functions[default](*args, **kwargs)\n\n  return wrapper\n\n\ndef _fallback_unrolled_lstm(input_sequence, initial_state, w_i, w_h, b):\n  """"""Fallback version of :class:`UnrolledLSTM` which works on any device.""""""\n  return dynamic_unroll(\n      functools.partial(_lstm_fn, w_i=w_i, w_h=w_h, b=b), input_sequence,\n      initial_state)\n\n\ndef _block_unrolled_lstm(input_sequence, initial_state, w_i, w_h, b):\n  """"""Efficient CPU specialization of :class:`UnrolledLSTM`.""""""\n  w_peephole = tf.zeros(\n      tf.shape(initial_state.hidden)[1:], dtype=initial_state.hidden.dtype)\n  _, all_cell, _, _, _, _, all_hidden = tf.raw_ops.BlockLSTMV2(\n      seq_len_max=tf.cast(tf.shape(input_sequence)[0], tf.int64),\n      x=input_sequence,\n      cs_prev=initial_state.cell,\n      h_prev=initial_state.hidden,\n      w=tf.concat([w_i, w_h], axis=0),\n      wci=w_peephole,\n      wcf=w_peephole,\n      wco=w_peephole,\n      b=b,\n      use_peephole=False)\n  return all_hidden, LSTMState(all_hidden[-1], all_cell[-1])\n\n\ndef _cudnn_unrolled_lstm(input_sequence, initial_state, w_i, w_h, b):\n  """"""GPU/CuDNN-RNN specialization of :class:`UnrolledLSTM`.""""""\n  # Intuitively, concat/transpose is not free but we did not see\n  # it significantly affecting performance in benchmarks.\n  output_sequence, all_hidden, all_cell, _ = tf.raw_ops.CudnnRNN(\n      input=input_sequence,\n      input_h=tf.expand_dims(initial_state.hidden, axis=0),\n      input_c=tf.expand_dims(initial_state.cell, axis=0),\n      params=tf.concat(\n          [\n              tf.reshape(tf.transpose(w_i), [-1]),\n              tf.reshape(tf.transpose(w_h), [-1]),\n              b,\n              # CuDNN has two sets of biases: b_i and b_h, zero-out b_h.\n              tf.zeros_like(b),\n          ],\n          axis=0),\n      rnn_mode=""lstm"")\n  return output_sequence, LSTMState(all_hidden[-1], all_cell[-1])\n\n\n_unrolled_lstm_impls = {\n    ""GPU"": _cudnn_unrolled_lstm,\n    ""TPU"": _fallback_unrolled_lstm,\n}\n# TODO(tomhennigan) Remove this check when TF 2.1 is released.\nif hasattr(tf.raw_ops, ""BlockLSTMV2""):\n  _unrolled_lstm_impls[""CPU""] = _block_unrolled_lstm\n\n_specialized_unrolled_lstm = _specialize_per_device(\n    ""snt_unrolled_lstm"", specializations=_unrolled_lstm_impls, default=""TPU"")\n\n\nclass _RecurrentDropoutWrapper(RNNCore):\n  """"""Recurrent dropout wrapper for a base RNN core.\n\n  The wrapper drops the previous state of the base core according to\n  dropout ``rates``. Specifically, dropout is only applied if the rate\n  corresponding to the state element is not `None`. Dropout masks\n  are sampled in `initial_state` of the wrapper.\n\n  This class is not intended to be used directly. See\n  ``lstm_with_recurrent_dropout``.\n  """"""\n\n  def __init__(self, base_core: RNNCore, rates, seed: int = None):\n    """"""Wraps a given base RNN core.\n\n    Args:\n      base_core: The ``RNNCore`` to be wrapped\n      rates: Recurrent dropout probabilities. The structure should match that of\n        ``base_core.initial_state``.\n      seed: Optional int; seed passed to :tf:`nn.dropout`.\n    """"""\n    super(_RecurrentDropoutWrapper,\n          self).__init__(name=base_core.name + ""_recurrent_dropout"")\n    self._base_core = base_core\n    self._rates = rates\n    self._seed = seed\n\n  def __call__(self, inputs, prev_state):\n    prev_core_state, dropout_masks = prev_state\n    prev_core_state = tree.map_structure(\n        lambda s, mask: s  # pylint: disable=g-long-lambda\n        if mask is None else s * mask,\n        prev_core_state,\n        dropout_masks)\n    output, next_core_state = self._base_core(inputs, prev_core_state)\n    return output, (next_core_state, dropout_masks)\n\n  def initial_state(self, batch_size, **kwargs):\n    core_initial_state = self._base_core.initial_state(batch_size, **kwargs)\n\n    def maybe_dropout(s, rate):\n      if rate is None:\n        return None\n      else:\n        return tf.nn.dropout(tf.ones_like(s), rate=rate, seed=self._seed)\n\n    dropout_masks = tree.map_structure(maybe_dropout, core_initial_state,\n                                       self._rates)\n    return core_initial_state, dropout_masks\n\n\ndef lstm_with_recurrent_dropout(hidden_size, dropout=0.5, seed=None, **kwargs):\n  r""""""Constructs an LSTM with recurrent dropout.\n\n  The implementation is based on :cite:`gal2016theoretically`. Dropout\n  is applied on the previous hidden state :math:`h_{t-1}` during the\n  computation of gate activations:\n\n  .. math::\n\n     \\begin{array}{ll}\n     i_t = \\sigma(W_{ii} x_t + W_{hi} d(h_{t-1}) + b_i) \\\\\n     f_t = \\sigma(W_{if} x_t + W_{hf} d(h_{t-1}) + b_f) \\\\\n     g_t = \\tanh(W_{ig} x_t + W_{hg} d(h_{t-1}) + b_g) \\\\\n     o_t = \\sigma(W_{io} x_t + W_{ho} d(h_{t-1}) + b_o)\n     \\end{array}\n\n  Args:\n    hidden_size: Hidden layer size.\n    dropout: Dropout probability.\n    seed: Optional int; seed passed to :tf:`nn.dropout`.\n    **kwargs: Optional keyword arguments to pass to the :class:`LSTM`\n      constructor.\n\n  Returns:\n    A tuple of two elements:\n      * **train_lstm** - An :class:`LSTM` with recurrent dropout enabled for\n        training.\n      * **test_lstm** - The same as ``train_lstm`` but without recurrent\n        dropout.\n\n  Raises:\n    ValueError: If ``dropout`` is not in ``[0, 1)``.\n  """"""\n  if dropout < 0 or dropout >= 1:\n    raise ValueError(\n        ""dropout must be in the range [0, 1), got {}"".format(dropout))\n\n  lstm = LSTM(hidden_size, **kwargs)\n  rate = LSTMState(hidden=dropout, cell=None)\n  return _RecurrentDropoutWrapper(lstm, rate, seed), lstm\n\n\nclass _ConvNDLSTM(RNNCore):\n  r""""""``num_spatial_dims``-D convolutional LSTM.\n\n  The implementation is based on :cite:`xingjian2015convolutional`.\n  Given :math:`x_t` and the previous state :math:`(h_{t-1}, c_{t-1})`\n  the core computes\n\n  .. math::\n\n     \\begin{array}{ll}\n     i_t = \\sigma(W_{ii} * x_t + W_{hi} * h_{t-1} + b_i) \\\\\n     f_t = \\sigma(W_{if} * x_t + W_{hf} * h_{t-1} + b_f) \\\\\n     g_t = \\tanh(W_{ig} * x_t + W_{hg} * h_{t-1} + b_g) \\\\\n     o_t = \\sigma(W_{io} * x_t + W_{ho} * h_{t-1} + b_o) \\\\\n     c_t = f_t c_{t-1} + i_t g_t \\\\\n     h_t = o_t \\tanh(c_t)\n     \\end{array}\n\n  where :math:`*` denotes the convolution operator; :math:`i_t`,\n  :math:`f_t`, :math:`o_t` are input, forget and output gate activations,\n  and :math:`g_t` is a vector of cell updates.\n\n  Notes:\n    Forget gate initialization:\n      Following :cite:`jozefowicz2015empirical` we add a constant\n      ``forget_bias`` (defaults to 1.0) to :math:`b_f` after initialization\n      in order to reduce the scale of forgetting in the beginning of\n      the training.\n\n  Attributes:\n    input_to_hidden: Input-to-hidden convolution weights :math:`W_{ii}`,\n      :math:`W_{if}`, :math:`W_{ig}` and :math:`W_{io}` concatenated into a\n      single tensor of shape ``[kernel_shape*, input_channels, 4 *\n      output_channels]`` where ``kernel_shape`` is repeated ``num_spatial_dims``\n      times.\n    hidden_to_hidden: Hidden-to-hidden convolution weights :math:`W_{hi}`,\n      :math:`W_{hf}`, :math:`W_{hg}` and :math:`W_{ho}` concatenated into a\n      single tensor of shape ``[kernel_shape*, input_channels, 4 *\n      output_channels]`` where ``kernel_shape`` is repeated ``num_spatial_dims``\n      times.\n    b: Biases :math:`b_i`, :math:`b_f`, :math:`b_g` and :math:`b_o` concatenated\n      into a tensor of shape ``[4 * output_channels]``.\n  """"""\n\n  def __init__(self,\n               num_spatial_dims: int,\n               input_shape: types.ShapeLike,\n               output_channels: int,\n               kernel_shape: Union[int, Sequence[int]],\n               data_format: Optional[Text] = None,\n               w_i_init: Optional[initializers.Initializer] = None,\n               w_h_init: Optional[initializers.Initializer] = None,\n               b_init: Optional[initializers.Initializer] = None,\n               forget_bias: types.FloatLike = 1.0,\n               dtype: tf.DType = tf.float32,\n               name: Optional[Text] = None):\n    """"""Constructs a convolutional LSTM.\n\n    Args:\n      num_spatial_dims: Number of spatial dimensions of the input.\n      input_shape: Shape of the inputs excluding batch size.\n      output_channels: Number of output channels.\n      kernel_shape: Sequence of kernel sizes (of length ``num_spatial_dims``),\n        or an int. ``kernel_shape`` will be expanded to define a kernel size in\n        all dimensions.\n      data_format: The data format of the input.\n      w_i_init: Optional initializer for the input-to-hidden convolution\n        weights. Defaults to :class:`~initializers.TruncatedNormal` with a\n        standard deviation of ``1 / sqrt(kernel_shape**num_spatial_dims *\n        input_channels)``.\n      w_h_init: Optional initializer for the hidden-to-hidden convolution\n        weights. Defaults to :class:`~initializers.TruncatedNormal` with a\n        standard deviation of ``1 / sqrt(kernel_shape**num_spatial_dims *\n        input_channels)``.\n      b_init: Optional initializer for the biases. Defaults to\n        :class:`~initializers.Zeros`.\n      forget_bias: Optional float to add to the bias of the forget gate after\n        initialization.\n      dtype: Optional :tf:`DType` of the core\'s variables. Defaults to\n        ``tf.float32``.\n      name: Name of the module.\n    """"""\n    super(_ConvNDLSTM, self).__init__(name)\n    self._num_spatial_dims = num_spatial_dims\n    self._input_shape = list(input_shape)\n    self._channel_index = 1 if (data_format is not None and\n                                data_format.startswith(""NC"")) else -1\n    self._output_channels = output_channels\n    self._b_init = b_init or initializers.Zeros()\n    self._forget_bias = forget_bias\n    self._dtype = dtype\n\n    self._input_to_hidden = conv.ConvND(\n        self._num_spatial_dims,\n        output_channels=4 * output_channels,\n        kernel_shape=kernel_shape,\n        padding=""SAME"",\n        with_bias=False,\n        w_init=w_i_init,\n        data_format=data_format,\n        name=""input_to_hidden"")\n    self._hidden_to_hidden = conv.ConvND(\n        self._num_spatial_dims,\n        output_channels=4 * output_channels,\n        kernel_shape=kernel_shape,\n        padding=""SAME"",\n        with_bias=False,\n        w_init=w_h_init,\n        data_format=data_format,\n        name=""hidden_to_hidden"")\n\n  def __call__(self, inputs, prev_state):\n    """"""See base class.""""""\n    self._initialize(inputs)\n\n    gates = self._input_to_hidden(inputs)\n    gates += self._hidden_to_hidden(prev_state.hidden)\n    gates += self.b\n\n    # i = input, f = forget, g = cell updates, o = output.\n    i, f, g, o = tf.split(\n        gates, num_or_size_splits=4, axis=self._num_spatial_dims + 1)\n\n    next_cell = tf.sigmoid(f) * prev_state.cell\n    next_cell += tf.sigmoid(i) * tf.tanh(g)\n    next_hidden = tf.sigmoid(o) * tf.tanh(next_cell)\n    return next_hidden, LSTMState(hidden=next_hidden, cell=next_cell)\n\n  @property\n  def input_to_hidden(self):\n    return self._input_to_hidden.w\n\n  @property\n  def hidden_to_hidden(self):\n    return self._hidden_to_hidden.w\n\n  def initial_state(self, batch_size):\n    """"""See base class.""""""\n    shape = list(self._input_shape)\n    shape[self._channel_index] = self._output_channels\n    shape = [batch_size] + shape\n    return LSTMState(\n        hidden=tf.zeros(shape, dtype=self._dtype),\n        cell=tf.zeros(shape, dtype=self._dtype))\n\n  @once.once\n  def _initialize(self, inputs):\n    dtype = _check_inputs_dtype(inputs, self._dtype)\n    b_i, b_f, b_g, b_o = tf.split(\n        self._b_init([4 * self._output_channels], dtype), num_or_size_splits=4)\n    b_f += self._forget_bias\n    self.b = tf.Variable(tf.concat([b_i, b_f, b_g, b_o], axis=0), name=""b"")\n\n\nclass Conv1DLSTM(_ConvNDLSTM):  # pylint: disable=missing-docstring\n  __doc__ = _ConvNDLSTM.__doc__.replace(""``num_spatial_dims``"", ""1"")\n\n  def __init__(self,\n               input_shape: types.ShapeLike,\n               output_channels: int,\n               kernel_shape: Union[int, Sequence[int]],\n               data_format=""NWC"",\n               w_i_init: Optional[initializers.Initializer] = None,\n               w_h_init: Optional[initializers.Initializer] = None,\n               b_init: Optional[initializers.Initializer] = None,\n               forget_bias: types.FloatLike = 1.0,\n               dtype: tf.DType = tf.float32,\n               name: Optional[Text] = None):\n    """"""Constructs a 1-D convolutional LSTM.\n\n    Args:\n      input_shape: Shape of the inputs excluding batch size.\n      output_channels: Number of output channels.\n      kernel_shape: Sequence of kernel sizes (of length 1), or an int.\n        ``kernel_shape`` will be expanded to define a kernel size in all\n        dimensions.\n      data_format: The data format of the input.\n      w_i_init: Optional initializer for the input-to-hidden convolution\n        weights. Defaults to :class:`~initializers.TruncatedNormal` with a\n        standard deviation of ``1 / sqrt(kernel_shape * input_channels)``.\n      w_h_init: Optional initializer for the hidden-to-hidden convolution\n        weights. Defaults to :class:`~initializers.TruncatedNormal` with a\n        standard deviation of ``1 / sqrt(kernel_shape * input_channels)``.\n      b_init: Optional initializer for the biases. Defaults to\n        :class:`~initializers.Zeros`.\n      forget_bias: Optional float to add to the bias of the forget gate after\n        initialization.\n      dtype: Optional :tf:`DType` of the core\'s variables. Defaults to\n        ``tf.float32``.\n      name: Name of the module.\n    """"""\n    super(Conv1DLSTM, self).__init__(\n        num_spatial_dims=1,\n        input_shape=input_shape,\n        output_channels=output_channels,\n        kernel_shape=kernel_shape,\n        data_format=data_format,\n        w_i_init=w_i_init,\n        w_h_init=w_h_init,\n        b_init=b_init,\n        forget_bias=forget_bias,\n        dtype=dtype,\n        name=name)\n\n\nclass Conv2DLSTM(_ConvNDLSTM):  # pylint: disable=missing-docstring\n  __doc__ = _ConvNDLSTM.__doc__.replace(""``num_spatial_dims``"", ""2"")\n\n  def __init__(self,\n               input_shape: types.ShapeLike,\n               output_channels: int,\n               kernel_shape: Union[int, Sequence[int]],\n               data_format: Text = ""NHWC"",\n               w_i_init: Optional[initializers.Initializer] = None,\n               w_h_init: Optional[initializers.Initializer] = None,\n               b_init: Optional[initializers.Initializer] = None,\n               forget_bias: types.FloatLike = 1.0,\n               dtype: tf.DType = tf.float32,\n               name: Optional[Text] = None):\n    """"""Constructs a 2-D convolutional LSTM.\n\n    Args:\n      input_shape: Shape of the inputs excluding batch size.\n      output_channels: Number of output channels.\n      kernel_shape: Sequence of kernel sizes (of length 2), or an int.\n        ``kernel_shape`` will be expanded to define a kernel size in all\n        dimensions.\n      data_format: The data format of the input.\n      w_i_init: Optional initializer for the input-to-hidden convolution\n        weights. Defaults to :class:`~initializers.TruncatedNormal` with a\n        standard deviation of ``1 / sqrt(kernel_shape**2 * input_channels)``.\n      w_h_init: Optional initializer for the hidden-to-hidden convolution\n        weights. Defaults to :class:`~initializers.TruncatedNormal` with a\n        standard deviation of ``1 / sqrt(kernel_shape**2 * input_channels)``.\n      b_init: Optional initializer for the biases. Defaults to\n        :class:`~initializers.Zeros`.\n      forget_bias: Optional float to add to the bias of the forget gate after\n        initialization.\n      dtype: Optional :tf:`DType` of the core\'s variables. Defaults to\n        ``tf.float32``.\n      name: Name of the module.\n    """"""\n    super(Conv2DLSTM, self).__init__(\n        num_spatial_dims=2,\n        input_shape=input_shape,\n        output_channels=output_channels,\n        kernel_shape=kernel_shape,\n        data_format=data_format,\n        w_i_init=w_i_init,\n        w_h_init=w_h_init,\n        b_init=b_init,\n        forget_bias=forget_bias,\n        dtype=dtype,\n        name=name)\n\n\nclass Conv3DLSTM(_ConvNDLSTM):  # pylint: disable=missing-docstring\n  __doc__ = _ConvNDLSTM.__doc__.replace(""``num_spatial_dims``"", ""3"")\n\n  def __init__(self,\n               input_shape: types.ShapeLike,\n               output_channels: int,\n               kernel_shape: Union[int, Sequence[int]],\n               data_format: Text = ""NDHWC"",\n               w_i_init: Optional[initializers.Initializer] = None,\n               w_h_init: Optional[initializers.Initializer] = None,\n               b_init: Optional[initializers.Initializer] = None,\n               forget_bias: types.FloatLike = 1.0,\n               dtype: tf.DType = tf.float32,\n               name: Optional[Text] = None):\n    """"""Constructs a 3-D convolutional LSTM.\n\n    Args:\n      input_shape: Shape of the inputs excluding batch size.\n      output_channels: Number of output channels.\n      kernel_shape: Sequence of kernel sizes (of length 3), or an int.\n        ``kernel_shape`` will be expanded to define a kernel size in all\n        dimensions.\n      data_format: The data format of the input.\n      w_i_init: Optional initializer for the input-to-hidden convolution\n        weights. Defaults to :class:`~initializers.TruncatedNormal` with a\n        standard deviation of ``1 / sqrt(kernel_shape**3 * input_channels)``.\n      w_h_init: Optional initializer for the hidden-to-hidden convolution\n        weights. Defaults to :class:`~initializers.TruncatedNormal` with a\n        standard deviation of ``1 / sqrt(kernel_shape**3 * input_channels)``.\n      b_init: Optional initializer for the biases. Defaults to\n        :class:`~initializers.Zeros`.\n      forget_bias: Optional float to add to the bias of the forget gate after\n        initialization.\n      dtype: Optional :tf:`DType` of the core\'s variables. Defaults to\n        ``tf.float32``.\n      name: Name of the module.\n    """"""\n    super(Conv3DLSTM, self).__init__(\n        num_spatial_dims=3,\n        input_shape=input_shape,\n        output_channels=output_channels,\n        kernel_shape=kernel_shape,\n        data_format=data_format,\n        w_i_init=w_i_init,\n        w_h_init=w_h_init,\n        b_init=b_init,\n        forget_bias=forget_bias,\n        dtype=dtype,\n        name=name)\n\n\nclass GRU(RNNCore):\n  r""""""Gated recurrent unit (GRU) RNN core.\n\n  The implementation is based on :cite:`chung2014empirical`. Given\n  :math:`x_t` and the previous state :math:`h_{t-1}` the core computes\n\n  .. math::\n\n     \\begin{array}{ll}\n     z_t &= \\sigma(W_{iz} x_t + W_{hz} h_{t-1} + b_z) \\\\\n     r_t &= \\sigma(W_{ir} x_t + W_{hr} h_{t-1} + b_r) \\\\\n     a_t &= \\tanh(W_{ia} x_t + W_{ha} (r_t h_{t-1}) + b_a) \\\\\n     h_t &= (1 - z_t) h_{t-1} + z_t a_t\n     \\end{array}\n\n  where :math:`z_t` and :math:`r_t` are reset and update gates.\n\n  Attributes:\n    input_to_hidden: Input-to-hidden weights :math:`W_{iz}`, :math:`W_{ir}`\n      and :math:`W_{ia}` concatenated into a tensor of shape\n      ``[input_size, 3 * hidden_size]``.\n    hidden_to_hidden: Hidden-to-hidden weights :math:`W_{hz}`, :math:`W_{hr}`\n      and :math:`W_{ha}` concatenated into a tensor of shape\n      ``[hidden_size, 3 * hidden_size]``.\n    b: Biases :math:`b_z`, :math:`b_r` and :math:`b_a` concatenated into a\n      tensor of shape ``[3 * hidden_size]``.\n  """"""\n\n  def __init__(self,\n               hidden_size,\n               w_i_init: Optional[initializers.Initializer] = None,\n               w_h_init: Optional[initializers.Initializer] = None,\n               b_init: Optional[initializers.Initializer] = None,\n               dtype: tf.DType = tf.float32,\n               name: Optional[Text] = None):\n    """"""Constructs a GRU.\n\n    Args:\n      hidden_size: Hidden layer size.\n      w_i_init: Optional initializer for the input-to-hidden weights. Defaults\n        to Glorot uniform initializer.\n      w_h_init: Optional initializer for the hidden-to-hidden weights. Defaults\n        to Glorot uniform initializer.\n      b_init: Optional initializer for the biases. Defaults to\n        :class:`~initializers.Zeros`.\n      dtype: Optional :tf:`DType` of the core\'s variables. Defaults to\n        ``tf.float32``.\n      name: Name of the module.\n    """"""\n    super(GRU, self).__init__(name)\n    self._hidden_size = hidden_size\n    glorot_uniform = initializers.VarianceScaling(\n        mode=""fan_avg"", distribution=""uniform"")\n    self._w_i_init = w_i_init or glorot_uniform\n    self._w_h_init = w_h_init or glorot_uniform\n    self._b_init = b_init or initializers.Zeros()\n    self._dtype = dtype\n\n  def __call__(self, inputs, prev_state):\n    """"""See base class.""""""\n    self._initialize(inputs)\n\n    gates_x = tf.matmul(inputs, self._w_i)\n    zr_idx = slice(2 * self._hidden_size)\n    zr_x = gates_x[:, zr_idx]\n    zr_h = tf.matmul(prev_state, self._w_h[:, zr_idx])\n    zr = zr_x + zr_h + self.b[zr_idx]\n    z, r = tf.split(tf.sigmoid(zr), num_or_size_splits=2, axis=1)\n\n    a_idx = slice(2 * self._hidden_size, 3 * self._hidden_size)\n    a_x = gates_x[:, a_idx]\n    a_h = tf.matmul(r * prev_state, self._w_h[:, a_idx])\n    a = tf.tanh(a_x + a_h + self.b[a_idx])\n\n    next_state = (1 - z) * prev_state + z * a\n    return next_state, next_state\n\n  def initial_state(self, batch_size):\n    """"""See base class.""""""\n    return tf.zeros([batch_size, self._hidden_size], dtype=self._dtype)\n\n  @property\n  def input_to_hidden(self):\n    return self._w_i\n\n  @property\n  def hidden_to_hidden(self):\n    return self._w_h\n\n  @once.once\n  def _initialize(self, inputs):\n    utils.assert_rank(inputs, 2)\n    input_size = inputs.shape[1]\n    dtype = _check_inputs_dtype(inputs, self._dtype)\n    self._w_i = tf.Variable(\n        self._w_i_init([input_size, 3 * self._hidden_size], dtype), name=""w_i"")\n    self._w_h = tf.Variable(\n        self._w_h_init([self._hidden_size, 3 * self._hidden_size], dtype),\n        name=""w_h"")\n    self.b = tf.Variable(self._b_init([3 * self._hidden_size], dtype), name=""b"")\n\n\n# TODO(slebedev): remove or document and export.\nclass CuDNNGRU(RNNCore):\n  """"""Gated recurrent unit (GRU) RNN core implemented using CuDNN-RNN.\n\n  The (CuDNN) implementation is based on https://arxiv.org/abs/1406.1078\n  and differs from `GRU` in the way a_t and h_t are computed:\n\n      a_t = tanh(W_{ia} x_t + r_t (W_{ha} h_{t-1}) + b_a)\n      h_t = (1 - z_t) a_t + z_t h_{t-1}\n\n  Unlike `GRU` this core operates on the whole batch of sequences at\n  once, i.e. the expected shape of `inputs` is\n  `[num_steps, batch_size, input_size]`.\n  """"""\n\n  def __init__(self,\n               hidden_size,\n               w_i_init: Optional[initializers.Initializer] = None,\n               w_h_init: Optional[initializers.Initializer] = None,\n               b_init: Optional[initializers.Initializer] = None,\n               dtype: tf.DType = tf.float32,\n               name: Optional[Text] = None):\n    """"""Constructs a `GRU`.\n\n    Args:\n      hidden_size: Hidden layer size.\n      w_i_init: Optional initializer for the input-to-hidden weights. Defaults\n        to Glorot uniform initializer.\n      w_h_init: Optional initializer for the hidden-to-hidden weights. Defaults\n        to Glorot uniform initializer.\n      b_init: Optional initializer for the biases. Defaults to\n        :class:`~initializers.Zeros`.\n      dtype: Optional :tf:`DType` of the core\'s variables. Defaults to\n        ``tf.float32``.\n      name: Name of the module.\n    """"""\n    super(CuDNNGRU, self).__init__(name)\n    self._hidden_size = hidden_size\n    glorot_uniform = initializers.VarianceScaling(\n        mode=""fan_avg"", distribution=""uniform"")\n    self._w_i_init = w_i_init or glorot_uniform\n    self._w_h_init = w_h_init or glorot_uniform\n    self._b_init = b_init or initializers.Zeros()\n    self._dtype = dtype\n\n  def __call__(self, inputs, prev_state):\n    """"""See base class.""""""\n    self._initialize(inputs)\n\n    # TODO(slebedev): consider allocating a single parameter Tensor.\n    # This will remove the need for tf.transpose and tf.concat and\n    # will likely result in a significant speedup. On the downside,\n    # checkpoints of `CuDNNGRU` incompatible with that of `GRU`.\n    # CuDNN orders the gates as r, z (instead of z, r).\n    w_iz, w_ir, w_ia = tf.split(self._w_i, num_or_size_splits=3, axis=1)\n    w_hz, w_hr, w_ha = tf.split(self._w_h, num_or_size_splits=3, axis=1)\n    b_z, b_r, b_a = tf.split(self.b, num_or_size_splits=3)\n    b_h_zero = tf.zeros([self._hidden_size])\n    outputs, next_hidden, _, _ = tf.raw_ops.CudnnRNN(\n        input=inputs,\n        input_h=tf.expand_dims(prev_state, axis=0),\n        input_c=0,\n        params=tf.concat(\n            [\n                tf.reshape(tf.transpose(w_ir), [-1]),\n                tf.reshape(tf.transpose(w_iz), [-1]),\n                tf.reshape(tf.transpose(w_ia), [-1]),\n                tf.reshape(tf.transpose(w_hr), [-1]),\n                tf.reshape(tf.transpose(w_hz), [-1]),\n                tf.reshape(tf.transpose(w_ha), [-1]),\n                # CuDNN has two sets of biases: b_i and b_h, zero-out b_h\n                # to match the definition in `GRU`.\n                b_r,\n                b_z,\n                b_a,\n                b_h_zero,\n                b_h_zero,\n                b_h_zero,\n            ],\n            axis=0),\n        rnn_mode=""gru"")\n\n    return outputs, next_hidden\n\n  @property\n  def input_to_hidden(self):\n    return self._w_i\n\n  @property\n  def hidden_to_hidden(self):\n    return self._w_h\n\n  def initial_state(self, batch_size):\n    """"""See base class.""""""\n    return tf.zeros([batch_size, self._hidden_size], dtype=self._dtype)\n\n  @once.once\n  def _initialize(self, inputs):\n    utils.assert_rank(inputs, 3)  # [num_steps, batch_size, input_size].\n    input_size = inputs.shape[2]\n    dtype = _check_inputs_dtype(inputs, self._dtype)\n    self._w_i = tf.Variable(\n        self._w_i_init([input_size, 3 * self._hidden_size], dtype), name=""w_i"")\n    self._w_h = tf.Variable(\n        self._w_h_init([self._hidden_size, 3 * self._hidden_size], dtype),\n        name=""w_h"")\n    self.b = tf.Variable(self._b_init([3 * self._hidden_size], dtype), name=""b"")\n\n\ndef _check_inputs_dtype(inputs, expected_dtype):\n  if inputs.dtype is not expected_dtype:\n    raise TypeError(""inputs must have dtype {!r}, got {!r}"".format(\n        expected_dtype, inputs.dtype))\n  return expected_dtype\n'"
sonnet/src/recurrent_test.py,144,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.recurrent.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\nimport unittest\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom sonnet.src import initializers\nfrom sonnet.src import recurrent\nfrom sonnet.src import test_utils\nimport tensorflow as tf\nimport tree\n\n\nclass VanillaRNNTest(test_utils.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(VanillaRNNTest, self).setUp()\n    self.batch_size = 3\n    self.input_size = 2\n    self.hidden_size = 16\n\n  @parameterized.parameters([False, True])\n  def testComputationAgainstNumPy(self, use_tf_function):\n    inputs = self.evaluate(\n        tf.random.uniform([self.batch_size, self.input_size]))\n    core = recurrent.VanillaRNN(\n        hidden_size=self.hidden_size, activation=tf.tanh)\n    prev_state = self.evaluate(core.initial_state(self.batch_size))\n\n    core_fn = tf.function(core) if use_tf_function else core\n    outputs, next_state = core_fn(tf.convert_to_tensor(inputs), prev_state)\n\n    expected_output = np.tanh(\n        inputs.dot(self.evaluate(core.input_to_hidden)) +\n        prev_state.dot(self.evaluate(core.hidden_to_hidden)) +\n        self.evaluate(core._b))\n\n    atol = 3e-2 if self.primary_device == ""TPU"" else 1e-6\n    self.assertAllClose(outputs, expected_output, atol=atol)\n    self.assertAllClose(next_state, expected_output, atol=atol)\n\n  def testDtypeMismatch(self):\n    core = recurrent.VanillaRNN(hidden_size=self.hidden_size, dtype=tf.bfloat16)\n    inputs = tf.random.uniform([self.batch_size, self.input_size])\n    prev_state = core.initial_state(self.batch_size)\n    self.assertIs(prev_state.dtype, tf.bfloat16)\n    with self.assertRaisesRegex(\n        TypeError, ""inputs must have dtype tf.bfloat16, got tf.float32""):\n      core(inputs, prev_state)\n\n  def testInitialization(self):\n    core = recurrent.VanillaRNN(\n        hidden_size=self.hidden_size,\n        w_i_init=initializers.Ones(),\n        w_h_init=initializers.Ones(),\n        b_init=initializers.Ones())\n    inputs = tf.random.uniform([self.batch_size, self.input_size])\n    prev_state = core.initial_state(self.batch_size)\n    core(inputs, prev_state)\n\n    for v in core.variables:\n      self.assertAllClose(self.evaluate(v), self.evaluate(tf.ones_like(v)))\n\n\nclass DeepRNNTest(test_utils.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(DeepRNNTest, self).setUp()\n    self.batch_size = 3\n    self.input_size = 2\n    self.hidden_size = 16\n\n  @parameterized.parameters([False, True])\n  def testComputationAgainstNumPy(self, use_tf_function):\n    inputs = self.evaluate(\n        tf.random.uniform([self.batch_size, self.input_size]))\n    core = recurrent.DeepRNN([\n        recurrent.VanillaRNN(hidden_size=self.hidden_size),\n        recurrent.VanillaRNN(hidden_size=2 * self.hidden_size)\n    ])\n    prev_state = self.evaluate(core.initial_state(self.batch_size))\n\n    core_fn = tf.function(core) if use_tf_function else core\n    outputs, next_state = core_fn(tf.convert_to_tensor(inputs), prev_state)\n\n    expected_outputs = inputs\n    expected_next_state = list(prev_state)\n    for idx, l in enumerate(core._layers):\n      expected_outputs, expected_next_state[idx] = l(expected_outputs,\n                                                     prev_state[idx])\n\n    self.assertAllClose(outputs, expected_outputs)\n    self.assertAllClose(next_state, tuple(expected_next_state))\n\n  @parameterized.parameters([False, True])\n  def testComputationAgainstNumPyWithCallables(self, use_tf_function):\n    inputs = self.evaluate(\n        tf.random.uniform([self.batch_size, self.input_size]))\n    core = recurrent.DeepRNN([tf.tanh, tf.sign])\n    prev_state = self.evaluate(core.initial_state(self.batch_size))\n\n    core_fn = tf.function(core) if use_tf_function else core\n    outputs, next_state = core_fn(tf.convert_to_tensor(inputs), prev_state)\n\n    self.assertAllClose(outputs, np.sign(np.tanh(inputs)))\n    self.assertEqual(next_state, prev_state)\n\n  def testInitialState(self):\n    core0 = recurrent.VanillaRNN(hidden_size=self.hidden_size)\n    core1 = recurrent.VanillaRNN(hidden_size=2 * self.hidden_size)\n    deep_rnn = recurrent.DeepRNN([core0, tf.tanh, core1, tf.sign])\n    prev_state = deep_rnn.initial_state(self.batch_size)\n    self.assertAllClose(prev_state[0], core0.initial_state(self.batch_size))\n    self.assertAllClose(prev_state[1], core1.initial_state(self.batch_size))\n\n  @parameterized.parameters([False, True])\n  def testWithSkipConnectionsOutputs(self, use_tf_function):\n    inputs = self.evaluate(\n        tf.random.uniform([self.batch_size, self.input_size]))\n    core = recurrent.deep_rnn_with_skip_connections([\n        recurrent.VanillaRNN(hidden_size=self.hidden_size),\n        recurrent.VanillaRNN(hidden_size=2 * self.hidden_size)\n    ],\n                                                    concat_final_output=False)\n    prev_state = self.evaluate(core.initial_state(self.batch_size))\n\n    core_fn = tf.function(core) if use_tf_function else core\n    outputs, _ = core_fn(tf.convert_to_tensor(inputs), prev_state)\n\n    self.assertEqual(outputs.shape,\n                     tf.TensorShape([self.batch_size, 2 * self.hidden_size]))\n\n  def testWithConnectionsValidation(self):\n    with self.assertRaisesRegexp(ValueError, ""to be instances of RNNCore""):\n      recurrent.deep_rnn_with_skip_connections([tf.tanh])\n    with self.assertRaisesRegexp(ValueError, ""to be instances of RNNCore""):\n      recurrent.deep_rnn_with_residual_connections([tf.tanh])\n\n\nclass LSTMTest(test_utils.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(LSTMTest, self).setUp()\n    self.batch_size = 3\n    self.input_size = 2\n    self.hidden_size = 16\n\n  @parameterized.parameters(\n      itertools.product([False, True], [None, 4], [0.0, 1.0]))\n  def testComputationAgainstNumPy(self, use_tf_function, projection_size,\n                                  forget_bias):\n    inputs = self.evaluate(\n        tf.random.uniform([self.batch_size, self.input_size]))\n    core = recurrent.LSTM(\n        self.hidden_size,\n        projection_size=projection_size,\n        forget_bias=forget_bias)\n    prev_state = self.evaluate(core.initial_state(self.batch_size))\n\n    core_fn = tf.function(core) if use_tf_function else core\n    outputs, next_state = core_fn(tf.convert_to_tensor(inputs), prev_state)\n\n    w_ii, w_if, w_ig, w_io = np.hsplit(self.evaluate(core.input_to_hidden), 4)\n    w_hi, w_hf, w_hg, w_ho = np.hsplit(self.evaluate(core.hidden_to_hidden), 4)\n    b_i, b_f, b_g, b_o = np.hsplit(self.evaluate(core.b), 4)\n    i = expit(inputs.dot(w_ii) + prev_state.hidden.dot(w_hi) + b_i)\n    f = expit(inputs.dot(w_if) + prev_state.hidden.dot(w_hf) + b_f)\n    g = np.tanh(inputs.dot(w_ig) + prev_state.hidden.dot(w_hg) + b_g)\n    o = expit(inputs.dot(w_io) + prev_state.hidden.dot(w_ho) + b_o)\n\n    expected_cell = f * prev_state.cell + i * g\n    expected_hidden = o * np.tanh(expected_cell)\n\n    if projection_size is not None:\n      expected_hidden = expected_hidden.dot(self.evaluate(core.projection))\n\n    atol = 1e-2 if self.primary_device == ""TPU"" else 1e-6\n    self.assertAllClose(outputs, next_state.hidden, atol=atol)\n    self.assertAllClose(expected_hidden, next_state.hidden, atol=atol)\n    self.assertAllClose(expected_cell, next_state.cell, atol=atol)\n\n  def testDtypeMismatch(self):\n    core = recurrent.LSTM(hidden_size=self.hidden_size, dtype=tf.bfloat16)\n    inputs = tf.random.uniform([self.batch_size, self.input_size])\n    prev_state = core.initial_state(self.batch_size)\n    self.assertIs(prev_state.hidden.dtype, tf.bfloat16)\n    self.assertIs(prev_state.cell.dtype, tf.bfloat16)\n    with self.assertRaisesRegex(\n        TypeError, ""inputs must have dtype tf.bfloat16, got tf.float32""):\n      core(inputs, prev_state)\n\n  def testInitialization(self):\n    projection_size = 4\n    core = recurrent.LSTM(\n        hidden_size=self.hidden_size,\n        projection_size=projection_size,\n        projection_init=initializers.Ones(),\n        w_i_init=initializers.Ones(),\n        w_h_init=initializers.Ones(),\n        b_init=initializers.Ones(),\n        forget_bias=0.0)\n    inputs = tf.random.uniform([self.batch_size, self.input_size])\n    prev_state = core.initial_state(self.batch_size)\n    core(inputs, prev_state)\n\n    for v in core.variables:\n      self.assertAllClose(self.evaluate(v), self.evaluate(tf.ones_like(v)))\n\n  @parameterized.parameters([1e-6, 0.5, 1 - 1e-6])\n  def testRecurrentDropout(self, rate):\n    num_steps = 2\n    inputs = tf.random.uniform([num_steps, self.batch_size, self.input_size])\n\n    train_core, test_core = recurrent.lstm_with_recurrent_dropout(\n        self.hidden_size, dropout=rate)\n    [_, train_output\n    ], _ = recurrent.dynamic_unroll(train_core, inputs,\n                                    train_core.initial_state(self.batch_size))\n    [_, test_output\n    ], _ = recurrent.dynamic_unroll(test_core, inputs,\n                                    test_core.initial_state(self.batch_size))\n\n    almost_zero = rate == 1e-6\n    if almost_zero:\n      # The train and test versions have the same output when rate is ~0.\n      rtol = 1e-3 if self.primary_device == ""TPU"" else 1e-6\n      self.assertAllClose(train_output, test_output, rtol=rtol)\n    else:\n      self.assertGreater(\n          self.evaluate(tf.reduce_max(tf.abs(train_output - test_output))),\n          0.001)\n\n  def testRecurrentDropoutInvalid(self):\n    with self.assertRaisesRegex(ValueError,\n                                r""dropout must be in the range \\[0, 1\\).+""):\n      recurrent.lstm_with_recurrent_dropout(self.hidden_size, -1)\n\n\nclass UnrolledLSTMTest(test_utils.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(UnrolledLSTMTest, self).setUp()\n\n    self.batch_size = 3\n    self.input_size = 2\n    self.hidden_size = 16\n\n  @parameterized.parameters(itertools.product([1, 4], [True, False]))\n  def testComputationAgainstLSTM(self, num_steps, use_tf_function):\n    unrolled_lstm = recurrent.UnrolledLSTM(self.hidden_size)\n    initial_state = unrolled_lstm.initial_state(self.batch_size)\n\n    if use_tf_function:\n      # TODO(b/134377706): remove the wrapper once the bug is fixed.\n      # Currently implementation selector requires an explicit device block\n      # inside a tf.function to work.\n      @tf.function\n      def unrolled_lstm_fn(*args, **kwargs):\n        with tf.device(""/device:{}:0"".format(self.primary_device)):\n          return unrolled_lstm(*args, **kwargs)\n    else:\n      unrolled_lstm_fn = unrolled_lstm\n\n    input_sequence = tf.random.uniform(\n        [num_steps, self.batch_size, self.input_size])\n    output_sequence, final_state = unrolled_lstm_fn(input_sequence,\n                                                    initial_state)\n\n    with tf.device(""/device:CPU:0""):  # Use CPU as the baseline.\n      lstm = recurrent.LSTM(self.hidden_size)\n      lstm._initialize(input_sequence[0])\n      lstm._w_i = unrolled_lstm._w_i\n      lstm._w_h = unrolled_lstm._w_h\n      lstm.b = unrolled_lstm.b\n      expected_output_sequence, expected_final_state = recurrent.dynamic_unroll(\n          lstm, input_sequence, lstm.initial_state(self.batch_size))\n\n    atol = 1e-2 if self.primary_device == ""TPU"" else 1e-6\n    self.assertAllClose(output_sequence, expected_output_sequence, atol=atol)\n    self.assertAllClose(\n        final_state.hidden, expected_final_state.hidden, atol=atol)\n    self.assertAllClose(final_state.cell, expected_final_state.cell, atol=atol)\n\n  @parameterized.parameters([True, False])\n  def testNumStepsPolymorphism(self, use_tf_function):\n    unrolled_lstm = recurrent.UnrolledLSTM(self.hidden_size)\n    initial_state = unrolled_lstm.initial_state(self.batch_size)\n\n    if use_tf_function:\n      # TODO(b/134377706): remove the wrapper once the bug is fixed.\n      # Currently implementation selector requires an explicit device block\n      # inside a tf.function to work.\n      @tf.function\n      def unrolled_lstm_fn(*args, **kwargs):\n        with tf.device(""/device:%s:0"" % self.primary_device):\n          return unrolled_lstm(*args, **kwargs)\n    else:\n      unrolled_lstm_fn = unrolled_lstm\n\n    # Check that the same instance can be called with different `num_steps`.\n    for num_steps in [1, 2, 4]:\n      output_sequence, _ = unrolled_lstm_fn(\n          tf.random.uniform([num_steps, self.batch_size, self.input_size]),\n          initial_state)\n      self.assertEqual(output_sequence.shape[0], num_steps)\n\n  def testDtypeMismatch(self):\n    unrolled_lstm = recurrent.UnrolledLSTM(\n        hidden_size=self.hidden_size, dtype=tf.bfloat16)\n    input_sequence = tf.random.uniform([1, self.batch_size, self.input_size])\n    initial_state = unrolled_lstm.initial_state(self.batch_size)\n    self.assertIs(initial_state.hidden.dtype, tf.bfloat16)\n    self.assertIs(initial_state.cell.dtype, tf.bfloat16)\n    with self.assertRaisesRegex(\n        TypeError, ""inputs must have dtype tf.bfloat16, got tf.float32""):\n      unrolled_lstm(input_sequence, initial_state)\n\n  def testInitialization(self):\n    unrolled_lstm = recurrent.UnrolledLSTM(\n        hidden_size=self.hidden_size,\n        forget_bias=0.0,\n        w_i_init=initializers.Ones(),\n        w_h_init=initializers.Ones(),\n        b_init=initializers.Ones())\n    input_sequence = tf.random.uniform([1, self.batch_size, self.input_size])\n    initial_state = unrolled_lstm.initial_state(self.batch_size)\n    unrolled_lstm(input_sequence, initial_state)\n\n    for v in unrolled_lstm.variables:\n      self.assertAllClose(self.evaluate(v), self.evaluate(tf.ones_like(v)))\n\n\nclass ConvNDLSTMTest(test_utils.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(ConvNDLSTMTest, self).setUp()\n    self.batch_size = 3\n    self.input_size = 2\n    self.hidden_size = 16\n    self.input_channels = 3\n    self.output_channels = 5\n\n  @parameterized.parameters(\n      itertools.product(\n          [False, True],\n          [recurrent.Conv1DLSTM, recurrent.Conv2DLSTM, recurrent.Conv3DLSTM]))\n  def testComputationAgainstNumPy(self, use_tf_function, core_cls):\n    if core_cls is recurrent.Conv1DLSTM:\n      num_spatial_dims = 1\n    elif core_cls is recurrent.Conv2DLSTM:\n      num_spatial_dims = 2\n    else:\n      assert core_cls is recurrent.Conv3DLSTM\n      num_spatial_dims = 3\n\n    input_shape = ((self.batch_size,) + (self.input_size,) * num_spatial_dims +\n                   (self.input_channels,))\n\n    inputs = self.evaluate(tf.random.uniform(input_shape))\n    core = core_cls(input_shape[1:], self.output_channels, kernel_shape=1)\n    prev_state = self.evaluate(core.initial_state(self.batch_size))\n\n    core_fn = tf.function(core) if use_tf_function else core\n    outputs, next_state = core_fn(tf.convert_to_tensor(inputs), prev_state)\n\n    def conv(x, f):\n      # NumPy does not have an out-of-the-box alternative.\n      return self.evaluate(tf.nn.convolution(x, f, strides=1, padding=""SAME""))\n\n    w_i = self.evaluate(core.input_to_hidden)\n    w_h = self.evaluate(core.hidden_to_hidden)\n    w_ii, w_if, w_ig, w_io = np.split(w_i, 4, axis=-1)\n    w_hi, w_hf, w_hg, w_ho = np.split(w_h, 4, axis=-1)\n    b_i, b_f, b_g, b_o = np.hsplit(self.evaluate(core.b), 4)\n    i = expit(conv(inputs, w_ii) + conv(prev_state.hidden, w_hi) + b_i)\n    f = expit(conv(inputs, w_if) + conv(prev_state.hidden, w_hf) + b_f)\n    g = np.tanh(conv(inputs, w_ig) + conv(prev_state.hidden, w_hg) + b_g)\n    o = expit(conv(inputs, w_io) + conv(prev_state.hidden, w_ho) + b_o)\n\n    expected_cell = f * prev_state.cell + i * g\n    expected_hidden = o * np.tanh(expected_cell)\n\n    atol = 1e-2 if self.primary_device == ""TPU"" else 1e-6\n    self.assertAllClose(outputs, next_state.hidden, atol=atol)\n    self.assertAllClose(expected_hidden, next_state.hidden, atol=atol)\n    self.assertAllClose(expected_cell, next_state.cell, atol=atol)\n\n  def testDtypeMismatch(self):\n    num_spatial_dims = 1\n    input_shape = ((self.batch_size,) + (self.input_size,) * num_spatial_dims +\n                   (self.input_channels,))\n\n    core = recurrent.Conv1DLSTM(\n        input_shape[1:],\n        self.output_channels,\n        kernel_shape=1,\n        dtype=tf.bfloat16)\n    inputs = tf.random.uniform(input_shape)\n    prev_state = core.initial_state(self.batch_size)\n    self.assertIs(prev_state.hidden.dtype, tf.bfloat16)\n    self.assertIs(prev_state.cell.dtype, tf.bfloat16)\n    with self.assertRaisesRegex(\n        TypeError, ""inputs must have dtype tf.bfloat16, got tf.float32""):\n      core(inputs, prev_state)\n\n  def testInitialization(self):\n    num_spatial_dims = 1\n    input_shape = ((self.batch_size,) + (self.input_size,) * num_spatial_dims +\n                   (self.input_channels,))\n\n    inputs = tf.random.uniform(input_shape)\n    core = recurrent.Conv1DLSTM(\n        input_shape[1:],\n        self.output_channels,\n        kernel_shape=1,\n        forget_bias=0.0,\n        w_i_init=initializers.Ones(),\n        w_h_init=initializers.Ones(),\n        b_init=initializers.Ones())\n    prev_state = core.initial_state(self.batch_size)\n    core(inputs, prev_state)\n\n    for v in core.variables:\n      self.assertAllClose(self.evaluate(v), self.evaluate(tf.ones_like(v)))\n\n\nclass GRUTest(test_utils.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(GRUTest, self).setUp()\n    self.batch_size = 3\n    self.input_size = 2\n    self.hidden_size = 16\n\n  @parameterized.parameters([False, True])\n  def testComputationAgainstNumPy(self, use_tf_function):\n    inputs = self.evaluate(\n        tf.random.uniform([self.batch_size, self.input_size]))\n    core = recurrent.GRU(self.hidden_size)\n    prev_state = self.evaluate(core.initial_state(self.batch_size))\n\n    core_fn = tf.function(core) if use_tf_function else core\n    outputs, next_state = core_fn(tf.convert_to_tensor(inputs), prev_state)\n\n    w_iz, w_ir, w_ia = np.hsplit(self.evaluate(core.input_to_hidden), 3)\n    w_hz, w_hr, w_ha = np.hsplit(self.evaluate(core.hidden_to_hidden), 3)\n    b_z, b_r, b_a = np.hsplit(self.evaluate(core.b), 3)\n\n    z = expit(inputs.dot(w_iz) + prev_state.dot(w_hz) + b_z)\n    r = expit(inputs.dot(w_ir) + prev_state.dot(w_hr) + b_r)\n    a = np.tanh(inputs.dot(w_ia) + (r * prev_state).dot(w_ha) + b_a)\n    expected_state = (1 - z) * prev_state + z * a\n\n    atol = 1e-2 if self.primary_device == ""TPU"" else 1e-6\n    self.assertAllClose(outputs, next_state, atol=atol)\n    self.assertAllClose(self.evaluate(next_state), expected_state, atol=atol)\n\n  def testDtypeMismatch(self):\n    core = recurrent.GRU(hidden_size=self.hidden_size, dtype=tf.bfloat16)\n    inputs = tf.random.uniform([self.batch_size, self.input_size])\n    prev_state = core.initial_state(self.batch_size)\n    self.assertIs(prev_state.dtype, tf.bfloat16)\n    with self.assertRaisesRegex(\n        TypeError, ""inputs must have dtype tf.bfloat16, got tf.float32""):\n      core(inputs, prev_state)\n\n  def testInitialization(self):\n    core = recurrent.GRU(\n        hidden_size=self.hidden_size,\n        w_i_init=initializers.Ones(),\n        w_h_init=initializers.Ones(),\n        b_init=initializers.Ones())\n    inputs = tf.random.uniform([self.batch_size, self.input_size])\n    prev_state = core.initial_state(self.batch_size)\n    core(inputs, prev_state)\n\n    for v in core.variables:\n      self.assertAllClose(self.evaluate(v), self.evaluate(tf.ones_like(v)))\n\n\ndef expit(x):\n  return 1.0 / (1 + np.exp(-x))\n\n\nclass CuDNNGRUTest(test_utils.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(CuDNNGRUTest, self).setUp()\n\n    if self.primary_device != ""GPU"":\n      self.skipTest(""Only available on GPU"")\n\n    self.batch_size = 1\n    self.input_size = 1\n    self.hidden_size = 1\n\n  @parameterized.parameters([1, 4])\n  def testComputationAgainstTF(self, num_steps):\n    inputs = tf.random.uniform([num_steps, self.batch_size, self.input_size])\n\n    cudnn_gru = recurrent.CuDNNGRU(self.hidden_size)\n    prev_state = cudnn_gru.initial_state(self.batch_size)\n    outputs, states = cudnn_gru(inputs, prev_state)\n\n    def cudnn_compatible_gru_fn(inputs, prev_state):\n      # Sonnet `GRU` computes a_t and h_t as\n      #\n      #   a_t = tanh(W_{ia} x_t + W_{ha} (r_t h_{t-1}) + b_a)\n      #   h_t = (1 - z_t) h_{t-1} + z_t a_t\n      #\n      # whereas CuDNN follows the original paper\n      #\n      #   a_t = tanh(W_{ia} x_t + r_t (W_{ha} h_{t-1}) + b_a)\n      #   h_t = (1 - z_t) a_t + z_t h_{t-1}\n      w_i = cudnn_gru.input_to_hidden\n      w_h = cudnn_gru.hidden_to_hidden\n      w_iz, w_ir, w_ia = tf.split(w_i, num_or_size_splits=3, axis=1)\n      w_hz, w_hr, w_ha = tf.split(w_h, num_or_size_splits=3, axis=1)\n      b_z, b_r, b_a = tf.split(cudnn_gru.b, num_or_size_splits=3)\n      z = tf.sigmoid(\n          tf.matmul(inputs, w_iz) + tf.matmul(prev_state, w_hz) + b_z)\n      r = tf.sigmoid(\n          tf.matmul(inputs, w_ir) + tf.matmul(prev_state, w_hr) + b_r)\n      a = tf.tanh(\n          tf.matmul(inputs, w_ia) + r * tf.matmul(prev_state, w_ha) + b_a)\n      next_state = (1 - z) * a + z * prev_state\n      return next_state, next_state\n\n    expected_outputs, expected_final_state = recurrent.dynamic_unroll(\n        cudnn_compatible_gru_fn, inputs, prev_state)\n\n    self.assertAllClose(outputs, expected_outputs)\n    self.assertAllClose(states[-1], expected_final_state)\n\n  def testDtypeMismatch(self):\n    core = recurrent.CuDNNGRU(hidden_size=self.hidden_size, dtype=tf.bfloat16)\n    inputs = tf.random.uniform([1, self.batch_size, self.input_size])\n    prev_state = core.initial_state(self.batch_size)\n    self.assertIs(prev_state.dtype, tf.bfloat16)\n    with self.assertRaisesRegex(\n        TypeError, ""inputs must have dtype tf.bfloat16, got tf.float32""):\n      core(inputs, prev_state)\n\n  def testInitialization(self):\n    core = recurrent.CuDNNGRU(\n        hidden_size=self.hidden_size,\n        w_i_init=initializers.Ones(),\n        w_h_init=initializers.Ones(),\n        b_init=initializers.Ones())\n    inputs = tf.random.uniform([1, self.batch_size, self.input_size])\n    prev_state = core.initial_state(self.batch_size)\n    core(inputs, prev_state)\n\n    for v in core.variables:\n      self.assertAllClose(self.evaluate(v), self.evaluate(tf.ones_like(v)))\n\n\nclass Counter(recurrent.RNNCore):\n  """"""Count the steps.\n\n  The output of the core at time step t is\n\n      inputs * (h + t)\n\n  where h is the hidden state which does not change with time.\n  """"""\n\n  def __init__(self, hidden_size, name=None):\n    super(Counter, self).__init__(name)\n    self._hidden_size = hidden_size\n    self._built = False\n\n  def __call__(self, inputs, prev_state):\n    if not self._built:\n      # Strictly speaking this variable is redundant, but all real-world\n      # cores have variables, so Counter is no different.\n      self.one = tf.Variable(1.0)\n      self._built = True\n\n    t, h = prev_state\n    return inputs * (h + t), (t + self.one, h)\n\n  def initial_state(self, batch_size):\n    return (tf.cast(0.0, tf.float32), tf.zeros([batch_size, self._hidden_size]))\n\n\nclass Replicate(recurrent.RNNCore):\n  """"""Replicate the output of the base RNN core.""""""\n\n  def __init__(self, base_core, n, name=None):\n    super(Replicate, self).__init__(name)\n    self._base_core = base_core\n    self._n = n\n\n  def __call__(self, inputs, prev_state):\n    outputs, next_state = self._base_core(inputs, prev_state)\n    return (outputs,) * self._n, next_state\n\n  def initial_state(self, batch_size, **kwargs):\n    return self._base_core.initial_state(batch_size, **kwargs)\n\n\nclass TrainableStateTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters([\n      {\n          ""initial_values_shape"": []\n      },\n      {\n          ""initial_values_shape"": tf.TensorShape([42])\n      },\n      {\n          ""initial_values_shape"": (tf.TensorShape([4]), tf.TensorShape([2]))\n      },\n  ])\n  def testUnmasked(self, initial_values_shape):\n    trainable_state = recurrent.TrainableState(\n        tree.map_structure(tf.ones, initial_values_shape))\n\n    if initial_values_shape:\n      self.assertEqual(\n          len(trainable_state.trainable_variables), len(initial_values_shape))\n\n    initial_state = trainable_state(batch_size=42)\n    for s, shape in zip(\n        tree.flatten(initial_state), tree.flatten(initial_values_shape)):\n      self.assertEqual(s.shape, tf.TensorShape([42] + shape.as_list()))\n\n  def testMasked(self):\n    mask = (True, False)\n    trainable_state = recurrent.TrainableState((tf.zeros([16]), tf.zeros([3])),\n                                               mask)\n\n    for var in trainable_state.trainable_variables:\n      var.assign_add(tf.ones_like(var))\n\n    initial_state = trainable_state(batch_size=42)\n    for s, trainable in zip(tree.flatten(initial_state), tree.flatten(mask)):\n      if trainable:\n        self.assertNotAllClose(s, tf.zeros_like(s))\n      else:\n        self.assertAllClose(s, tf.zeros_like(s))\n\n  def testForCore(self):\n    core = recurrent.LSTM(hidden_size=16)\n    trainable_state = recurrent.TrainableState.for_core(core)\n    self.assertAllClose(\n        trainable_state(batch_size=42), core.initial_state(batch_size=42))\n\n\n@parameterized.parameters([\n    {\n        ""use_tf_function"": False,\n        ""unroll_fn"": recurrent.dynamic_unroll\n    },\n    {\n        ""use_tf_function"": False,\n        ""unroll_fn"": recurrent.static_unroll\n    },\n    {\n        ""use_tf_function"": True,\n        ""unroll_fn"": recurrent.dynamic_unroll\n    },\n    {\n        ""use_tf_function"": True,\n        ""unroll_fn"": recurrent.static_unroll\n    },\n])\nclass UnrollTest(test_utils.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super(UnrollTest, self).setUp()\n\n    self.num_steps = 5\n    self.batch_size = 3\n    self.hidden_size = 2\n    self.core = Counter(self.hidden_size)\n\n  def testFlat(self, use_tf_function, unroll_fn):\n    if use_tf_function:\n      unroll_fn = tf.function(unroll_fn)\n\n    initial_state = _, h = self.core.initial_state(self.batch_size)\n    input_sequence = tf.random.uniform([self.num_steps, self.batch_size, 1])\n    output_sequence, final_state = unroll_fn(self.core, input_sequence,\n                                             initial_state)\n\n    self.assertAllClose(\n        output_sequence,\n        [inputs * (h + t) for t, inputs in enumerate(input_sequence)])\n    self.assertAllClose(final_state, (tf.cast(self.num_steps, tf.float32), h))\n\n  def testNestedInputs(self, use_tf_function, unroll_fn):\n    if use_tf_function:\n      unroll_fn = tf.function(unroll_fn)\n\n    initial_state = _, h = self.core.initial_state(self.batch_size)\n    input_sequence = tf.random.uniform([self.num_steps, self.batch_size, 1])\n    output_sequence, final_state = unroll_fn(\n        lambda inputs, prev_state: self.core(inputs[""x""][""y""], prev_state),\n        {""x"": {\n            ""y"": input_sequence\n        }}, initial_state)\n\n    self.assertAllClose(\n        output_sequence,\n        [inputs * (h + t) for t, inputs in enumerate(input_sequence)])\n    self.assertAllClose(final_state, (tf.cast(self.num_steps, tf.float32), h))\n\n  def testNestedOutputs(self, use_tf_function, unroll_fn):\n    if use_tf_function:\n      unroll_fn = tf.function(unroll_fn)\n\n    num_replicas = 2\n    core = Replicate(self.core, num_replicas)\n    initial_state = _, h = core.initial_state(self.batch_size)\n    input_sequence = tf.random.uniform([self.num_steps, self.batch_size, 1])\n    output_sequence, final_state = unroll_fn(core, input_sequence,\n                                             initial_state)\n\n    expected_outputs = [\n        inputs * (h + t) for t, inputs in enumerate(input_sequence)\n    ]\n    self.assertAllClose(output_sequence, (expected_outputs,) * num_replicas)\n    self.assertAllClose(final_state, (tf.cast(self.num_steps, tf.float32), h))\n\n  def testEmptyOutputs(self, use_tf_function, unroll_fn):\n    if use_tf_function:\n      unroll_fn = tf.function(unroll_fn)\n\n    def core_fn(inputs, prev_state):\n      return (inputs, tf.zeros(shape=(0,))), prev_state\n\n    input_sequence = tf.random.uniform([self.num_steps, self.batch_size, 1])\n    (_, empty), unused_final_state = unroll_fn(\n        core_fn, input_sequence, initial_state=tf.constant(0.0))\n\n    self.assertEqual(empty.shape, tf.TensorShape([self.num_steps, 0]))\n\n  def testZeroSteps(self, use_tf_function, unroll_fn):\n    if use_tf_function:\n      unroll_fn = tf.function(unroll_fn)\n\n    initial_state = self.core.initial_state(self.batch_size)\n    input_sequence = tf.random.uniform([0, self.batch_size])\n\n    with self.assertRaisesRegex(ValueError,\n                                ""must have at least a single time step""):\n      unroll_fn(self.core, input_sequence, initial_state)\n\n  def testInconsistentSteps(self, use_tf_function, unroll_fn):\n    if use_tf_function:\n      unroll_fn = tf.function(unroll_fn)\n\n    initial_state = self.core.initial_state(self.batch_size)\n    input_sequence = (tf.random.uniform([1, self.batch_size]),\n                      tf.random.uniform([2, self.batch_size]))\n\n    with self.assertRaisesRegex(ValueError,\n                                ""must have consistent number of time steps""):\n      unroll_fn(self.core, input_sequence, initial_state)\n\n  def testVariableLengthOneZeroLength(self, use_tf_function, unroll_fn):\n    if use_tf_function:\n      unroll_fn = tf.function(unroll_fn)\n\n    sequence_length = tf.constant([0] + [self.num_steps] *\n                                  (self.batch_size - 1))\n    initial_state = self.core.initial_state(self.batch_size)\n    input_sequence = tf.random.uniform([self.num_steps, self.batch_size, 1])\n    output_sequence, _ = unroll_fn(\n        self.core,\n        input_sequence,\n        initial_state,\n        sequence_length=sequence_length)\n\n    self.assertConsistentWithLength(output_sequence, sequence_length)\n\n  def testVariableLengthRange(self, use_tf_function, unroll_fn):\n    if use_tf_function:\n      unroll_fn = tf.function(unroll_fn)\n\n    sequence_length = tf.range(self.batch_size)\n    initial_state = self.core.initial_state(self.batch_size)\n    input_sequence = tf.random.uniform([self.num_steps, self.batch_size, 1])\n    output_sequence, _ = unroll_fn(\n        self.core,\n        input_sequence,\n        initial_state,\n        sequence_length=sequence_length)\n\n    self.assertConsistentWithLength(output_sequence, sequence_length)\n\n  def assertConsistentWithLength(self, output_sequence, sequence_length):\n    for t, _ in enumerate(output_sequence):\n      for b in range(self.batch_size):\n        if tf.equal(sequence_length[b], t):\n          if t == 0:\n            self.assertAllEqual(tf.reduce_sum(output_sequence[t, b]), 0.0)\n          else:\n            self.assertAllClose(output_sequence[t, b], output_sequence[t - 1,\n                                                                       b])\n\n  def testVariableLengthAllFull(self, use_tf_function, unroll_fn):\n    if use_tf_function:\n      unroll_fn = tf.function(unroll_fn)\n\n    initial_state = self.core.initial_state(self.batch_size)\n    input_sequence = tf.random.uniform([self.num_steps, self.batch_size, 1])\n    output_sequence, final_state = unroll_fn(\n        self.core,\n        input_sequence,\n        initial_state,\n        sequence_length=tf.constant([self.num_steps] * self.batch_size))\n    expected_output_sequence, expected_final_state = unroll_fn(\n        self.core, input_sequence, initial_state)\n    self.assertAllClose(output_sequence, expected_output_sequence)\n    self.assertAllClose(final_state, expected_final_state)\n\n  def testVariableLengthAllEmpty(self, use_tf_function, unroll_fn):\n    if use_tf_function:\n      unroll_fn = tf.function(unroll_fn)\n\n    initial_state = self.core.initial_state(self.batch_size)\n    input_sequence = tf.random.uniform([self.num_steps, self.batch_size, 1])\n    output_sequence, final_state = unroll_fn(\n        self.core,\n        input_sequence,\n        initial_state,\n        sequence_length=tf.zeros(self.batch_size, tf.int32))\n    self.assertAllClose(output_sequence, tf.zeros_like(output_sequence))\n    # Scalars always get updates (to match `tf.nn.*_rnn` behavior).\n    self.assertAllClose(final_state[0], self.num_steps)\n    self.assertAllClose(final_state[1], initial_state[1])\n\n\nclass UnknownStepsUnrollTest(test_utils.TestCase):\n\n  def setUp(self):\n    super(UnknownStepsUnrollTest, self).setUp()\n\n    self.num_steps = 5\n    self.batch_size = 3\n    self.hidden_size = 2\n    self.core = Counter(self.hidden_size)\n\n  def testStaticUnroll(self):\n\n    def do_unroll(input_sequence):\n      initial_state = self.core.initial_state(self.batch_size)\n      return recurrent.static_unroll(self.core, input_sequence, initial_state)\n\n    with self.assertRaisesRegex(\n        ValueError, ""must have a statically known number of time steps""):\n      tf.function(do_unroll).get_concrete_function(\n          tf.TensorSpec([None, None, 1]))\n\n  def testDynamicUnroll(self):\n\n    def do_unroll(input_sequence):\n      initial_state = self.core.initial_state(self.batch_size)\n      return recurrent.dynamic_unroll(self.core, input_sequence, initial_state)\n\n    cf = tf.function(do_unroll).get_concrete_function(\n        tf.TensorSpec([None, None, 1]))\n    output_sequence, unused_final_state = cf(\n        tf.random.uniform([self.num_steps, self.batch_size, 1]))\n    self.assertEqual(output_sequence.shape[0], self.num_steps)\n\n  @unittest.skip(""b/141910613"")\n  def testDynamicUnrollInconsistentSteps(self):\n\n    def do_unroll(*input_sequence):\n      return recurrent.dynamic_unroll(lambda inputs, _: inputs, input_sequence,\n                                      ())\n\n    cf = tf.function(do_unroll).get_concrete_function(\n        tf.TensorSpec([None, None, 1]), tf.TensorSpec([None, None, 1]))\n    with self.assertRaisesRegex(tf.errors.InvalidArgumentError,\n                                ""must have consistent number of time steps""):\n      cf(\n          tf.random.uniform([self.num_steps, self.batch_size, 1]),\n          tf.random.uniform([self.num_steps + 1, self.batch_size, 1]))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/regularizers.py,23,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Regularizers for Sonnet.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport abc\n\nimport six\nfrom sonnet.src import types\nimport tensorflow as tf\nfrom typing import Sequence\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass Regularizer(object):\n  """"""Base regularizer class.""""""\n\n  @abc.abstractmethod\n  def __call__(self, tensors: Sequence[tf.Tensor]) -> tf.Tensor:\n    """"""Apply a regularizer.\n\n    Args:\n      tensors: A sequence of tensors to regularize.\n\n    Returns:\n      Combined regularization loss for the given tensors.\n    """"""\n\n\nclass L1(Regularizer):\n  """"""L1 regularizer.\n\n  >>> reg = snt.regularizers.L1(0.01)\n  >>> reg([tf.constant([1.0, 2.0, 3.0])])\n  <tf.Tensor: ...>\n  """"""\n\n  def __init__(self, scale: types.FloatLike):\n    """"""Create an L1 regularizer.\n\n    Args:\n      scale: A non-negative regularization factor.\n\n    Raises:\n      ValueError: if scale is <0.\n    """"""\n    _check_scale(scale)\n    self.scale = scale\n\n  def __repr__(self):\n    # TODO(slebedev): replace with NamedTuple once we are 3.X-only.\n    return ""L1(scale={})"".format(self.scale)\n\n  __str__ = __repr__\n\n  def __call__(self, tensors: Sequence[tf.Tensor]) -> tf.Tensor:\n    """"""See base class.""""""\n    if not tensors:\n      return tf.zeros_like(self.scale)\n\n    return self.scale * tf.add_n([tf.reduce_sum(tf.abs(t)) for t in tensors])\n\n\nclass L2(Regularizer):\n  """"""L2 regularizer.\n\n  >>> reg = snt.regularizers.L2(0.01)\n  >>> reg([tf.constant([1.0, 2.0, 3.0])])\n  <tf.Tensor: ...>\n  """"""\n\n  def __init__(self, scale: types.FloatLike):\n    """"""Create an L2 regularizer.\n\n    Args:\n      scale: float or scalar tensor; regularization factor.\n\n    Raises:\n      ValueError: if scale is <0.\n    """"""\n    _check_scale(scale)\n    self.scale = scale\n\n  def __repr__(self):\n    # TODO(slebedev): replace with NamedTuple once we are 3.X-only.\n    return ""L2(scale={})"".format(self.scale)\n\n  __str__ = __repr__\n\n  def __call__(self, tensors: Sequence[tf.Tensor]) -> tf.Tensor:\n    """"""See base class.""""""\n    if not tensors:\n      return tf.zeros_like(self.scale)\n\n    return self.scale * tf.add_n([tf.reduce_sum(tf.square(t)) for t in tensors])\n\n\nclass OffDiagonalOrthogonal(Regularizer):\n  """"""Off-diagonal orthogonal regularizer.\n\n  The implementation is based on https://arxiv.org/abs/1809.11096.\n  Given a rank N >= 2 tensor, the regularizer computes\n  the sum of off-diagonal entries of (W^T W)^2 where\n\n  * W is the input tensor reshaped to a matrix by collapsing the\n    leading N - 1 axes into the first one;\n  * ^2 is the element-wise square.\n\n  NB: that is equivalent to computing the off-diagonal sum of (W^T W - I)^2,\n  as off-diagonal entries of I are 0.\n\n  For example,\n\n      >>> t = tf.reshape(tf.range(8, dtype=tf.float32), [2, 2, 2])\n      >>> reg = snt.regularizers.OffDiagonalOrthogonal(0.01)\n      >>> reg([t])\n      <tf.Tensor: ...>\n\n  corresponds to copmuting\n\n      >>> w = tf.reshape(t, [-1, 2])\n      >>> w_gram_sq = tf.square(tf.matmul(tf.transpose(w), w))\n      >>> 0.01 * (tf.reduce_sum(w_gram_sq) - tf.linalg.trace(w_gram_sq))\n      <tf.Tensor: ...>\n  """"""\n\n  def __init__(self, scale: types.FloatLike):\n    """"""Create an off-diagonal orthogonal regularizer.\n\n    Args:\n      scale: A non-negative regularization factor.\n\n    Raises:\n      ValueError: if scale is <0.\n    """"""\n    self.scale = _check_scale(scale)\n\n  def __repr__(self):\n    # TODO(slebedev): replace with NamedTuple once we are 3.X-only.\n    return ""Orthogonal(scale={})"".format(self.scale)\n\n  __str__ = __repr__\n\n  def __call__(self, tensors: Sequence[tf.Tensor]) -> tf.Tensor:\n    """"""See base class.""""""\n    if not tensors:\n      return tf.zeros_like(self.scale)\n\n    acc = []\n    for t in tensors:\n      shape = t.shape.with_rank_at_least(2)\n      w = tf.reshape(t, [-1, shape[-1]])\n      w_gram_sq = tf.square(tf.matmul(w, w, transpose_a=True))\n      # (off-diagonal sum) = (full sum) - (diagonal sum = trace).\n      acc.append(tf.reduce_sum(w_gram_sq) - tf.linalg.trace(w_gram_sq))\n    return self.scale * tf.add_n(acc)\n\n\ndef _check_scale(scale: types.FloatLike) -> types.FloatLike:\n  if scale < 0:\n    raise ValueError(""scale must be >=0"")\n  return scale\n'"
sonnet/src/regularizers_test.py,4,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.regularizers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom sonnet.src import regularizers\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\n\nclass L1Test(test_utils.TestCase):\n\n  def testAgainstNumPy(self):\n    regularizer = regularizers.L1(0.01)\n    tensors = [tf.random.uniform([42]), tf.random.uniform([24])]\n\n    def l1(scale, t):\n      return scale * np.abs(t).sum()\n\n    self.assertAllClose(\n        regularizer(tensors),\n        sum(l1(regularizer.scale, self.evaluate(t)) for t in tensors))\n\n  def testNegativeScale(self):\n    with self.assertRaises(ValueError):\n      regularizers.L1(-1.0)\n\n  def testEmpty(self):\n    self.assertAllClose(regularizers.L1(0.01)([]), 0.0)\n\n\nclass L2Test(test_utils.TestCase):\n\n  def testAgainstNumPy(self):\n    regularizer = regularizers.L2(0.01)\n    tensors = [tf.random.uniform([42]), tf.random.uniform([24])]\n\n    def l2(scale, t):\n      return scale * np.square(t).sum()\n\n    self.assertAllClose(\n        regularizer(tensors),\n        sum(l2(regularizer.scale, self.evaluate(t)) for t in tensors))\n\n  def testNegativeScale(self):\n    with self.assertRaises(ValueError):\n      regularizers.L2(-1.0)\n\n  def testEmpty(self):\n    self.assertAllClose(regularizers.L2(0.01)([]), 0.0)\n\n\nclass OffDiagonalOrthogonalTest(test_utils.TestCase):\n\n  def testAgainstNumPy(self):\n    regularizer = regularizers.OffDiagonalOrthogonal(0.01)\n    tensors = [tf.random.uniform([4, 2]), tf.random.uniform([2, 4])]\n\n    def odo(scale, t):\n      t2 = np.square(np.dot(t.T, t))\n      return scale * (t2.sum() - np.trace(t2))\n\n    atol = 1e-3 if self.primary_device == ""TPU"" else 1e-6\n    self.assertAllClose(\n        regularizer(tensors),\n        sum(odo(regularizer.scale, self.evaluate(t)) for t in tensors),\n        atol=atol)\n\n  def testNegativeScale(self):\n    with self.assertRaises(ValueError):\n      regularizers.OffDiagonalOrthogonal(-1.0)\n\n  def testEmpty(self):\n    self.assertAllClose(regularizers.OffDiagonalOrthogonal(0.01)([]), 0.0)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/reshape.py,10,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Reshaping Sonnet modules.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport numpy as np\nfrom sonnet.src import base\nfrom sonnet.src import once\nfrom sonnet.src import types\nimport tensorflow as tf\nfrom typing import Optional, Sequence, Text\n\n\ndef reshape(inputs: tf.Tensor,\n            output_shape: types.ShapeLike,\n            preserve_dims: int = 1,\n            name: Optional[Text] = None) -> tf.Tensor:\n  """"""A shortcut for applying :class:`Reshape` to the ``inputs``.""""""\n  return Reshape(output_shape, preserve_dims, name=name)(inputs)\n\n\ndef flatten(inputs: tf.Tensor, name: Text = ""flatten"") -> tf.Tensor:\n  """"""A shortcut for applying :class:`Flatten` to the ``inputs``.""""""\n  return Flatten(name=name)(inputs)\n\n\ndef _infer_shape(output_shape: types.ShapeLike, dimensions: Sequence[int]):\n  """"""Replaces the -1 wildcard in the output shape vector.\n\n  This function infers the correct output shape given the input dimensions.\n\n  Args:\n    output_shape: Output shape.\n    dimensions: List of input non-batch dimensions.\n\n  Returns:\n    Tuple of non-batch output dimensions.\n  """"""\n  # Size of input.\n  n = np.prod(dimensions)\n  # Size of output where defined.\n  v = np.array(output_shape)\n  m = abs(np.prod(v))\n  # Replace wildcard.\n  v[v == -1] = n // m\n  return tuple(v)\n\n\nclass Reshape(base.Module):\n  """"""Reshapes input Tensor, preserving the batch dimension.\n\n  For example, given an input tensor with shape ``[B, H, W, C, D]``::\n\n      >>> B, H, W, C, D = range(1, 6)\n      >>> x = tf.ones([B, H, W, C, D])\n\n  The default behavior when ``output_shape`` is ``(-1, D)`` is to flatten\n  all dimensions between ``B`` and ``D``::\n\n      >>> mod = snt.Reshape(output_shape=(-1, D))\n      >>> assert mod(x).shape == [B, H*W*C, D]\n\n  You can change the number of preserved leading dimensions via\n  ``preserve_dims``::\n\n      >>> mod = snt.Reshape(output_shape=(-1, D), preserve_dims=2)\n      >>> assert mod(x).shape == [B, H, W*C, D]\n\n      >>> mod = snt.Reshape(output_shape=(-1, D), preserve_dims=3)\n      >>> assert mod(x).shape == [B, H, W, C, D]\n\n      >>> mod = snt.Reshape(output_shape=(-1, D), preserve_dims=4)\n      >>> assert mod(x).shape == [B, H, W, C, 1, D]\n  """"""\n\n  def __init__(self,\n               output_shape: types.ShapeLike,\n               preserve_dims: int = 1,\n               name: Optional[Text] = None):\n    """"""Constructs a ``Reshape`` module.\n\n    Args:\n      output_shape: Shape to reshape the input tensor to while preserving its\n        first ``preserve_dims` dimensions. When the special value -1 appears in\n        ``output_shape`` the corresponding size is automatically inferred. Note\n        that -1 can only appear once in ``output_shape``.\n        To flatten all non-batch dimensions use :class:`Flatten`.\n      preserve_dims: Number of leading dimensions that will not be reshaped.\n      name: Name of the module.\n\n    Raises:\n      ValueError: If ``preserve_dims`` is not positive.\n    """"""\n    super(Reshape, self).__init__(name=name)\n\n    if preserve_dims <= 0:\n      raise ValueError(""Argument preserve_dims should be >= 1."")\n\n    self._output_shape = output_shape\n    self._preserve_dims = preserve_dims\n\n  @once.once\n  def _initialize(self, inputs: tf.Tensor):\n    if inputs.shape.rank < self._preserve_dims:\n      raise ValueError(""Input tensor has {} dimensions, should have at least ""\n                       ""as many as preserve_dims={}"".format(\n                           inputs.shape.rank, self._preserve_dims))\n\n    self._input_shape = inputs.shape\n\n  def __call__(self, inputs: tf.Tensor) -> tf.Tensor:\n    """"""Reshapes ``inputs``.\n\n    Args:\n      inputs: A tensor of shape ``[b_1, b_2, ..., b_preserve_dims,\n        b_preserve_dims + 1, ...]``.\n\n    Returns:\n      A tensor of shape\n        ``[b_1, b_2, ..., b_preserve_dims, b_reshape_1, b_reshape_2, ...]``,\n        with reshaping defined by the constructor ``output_shape`` parameter.\n\n    Raises:\n      ValueError: If ``output_shape`` is incompatible with shape of the\n        ``inputs``; or if ``output_shape`` contains more than one wildcard -1;\n        or if the ``inputs`` rank is less than ``preserved_dims``; or if\n        the ``inputs`` shape contains unknown, non-preserved dimensions\n        (except when the unknown dimension is the only non-preserved\n        dimension and doesn\'t actually need reshaping).\n    """"""\n    self._initialize(inputs)\n\n    # Resolve the wildcard if any.\n    output_shape = tuple(self._output_shape)\n    if -1 in output_shape:\n      reshaped_shape = inputs.shape[self._preserve_dims:]\n      if reshaped_shape.is_fully_defined():\n        output_shape = _infer_shape(output_shape, reshaped_shape)\n\n    preserved_shape = inputs.shape[:self._preserve_dims]\n    if preserved_shape.is_fully_defined():\n      output = tf.reshape(inputs, tuple(preserved_shape) + output_shape)\n    else:\n      dynamic_preserved_shape = tf.shape(inputs)[:self._preserve_dims]\n      output = tf.reshape(\n          inputs, tf.concat([dynamic_preserved_shape, output_shape], axis=0))\n    return output\n\n  @base.no_name_scope\n  def reversed(self, name: Optional[Text] = None) -> ""Reshape"":\n    """"""Returns inverse batch reshape.""""""\n    if name is None:\n      name = self.name + ""_reversed""\n\n    return Reshape(\n        output_shape=self._input_shape[self._preserve_dims:],\n        preserve_dims=self._preserve_dims,\n        name=name)\n\n\nclass Flatten(Reshape):\n  """"""Flattens the input Tensor, preserving the batch dimension(s).\n\n  ``Flatten`` reshapes input tensors to combine all trailing dimensions\n  apart from the first. Additional leading dimensions can be preserved\n  by setting the ``preserve_dims`` parameter.\n\n  See :class:`Reshape` for more details.\n  """"""\n\n  def __init__(self, preserve_dims: int = 1, name: Optional[Text] = None):\n    """"""Constructs a ``Flatten`` module.\n\n    Args:\n      preserve_dims: Number of leading dimensions that will not be reshaped.\n      name: Name of the module.\n    """"""\n    super(Flatten, self).__init__(\n        output_shape=(-1,), preserve_dims=preserve_dims, name=name)\n'"
sonnet/src/reshape_test.py,34,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.reshape.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom sonnet.src import reshape\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\nB, H, W, C, D = 2, 3, 4, 5, 6\n\n\nclass ReshapeTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters(\n      (1, [B, H * W * C, D]),\n      (2, [B, H, W * C, D]),\n      (3, [B, H, W, C, D]),\n      (4, [B, H, W, C, 1, D]),\n  )\n  def testReshape(self, preserve_dims, expected_output_shape):\n    mod = reshape.Reshape(output_shape=(-1, D), preserve_dims=preserve_dims)\n    outputs = mod(tf.ones([B, H, W, C, D]))\n    self.assertEqual(outputs.shape, expected_output_shape)\n\n  def testInvalid_multipleWildcard(self):\n    mod = reshape.Reshape(output_shape=[-1, -1])\n    with self.assertRaises(tf.errors.InvalidArgumentError):\n      mod(tf.ones([1, 2, 3]))\n\n  def testInvalid_negativeSize(self):\n    mod = reshape.Reshape(output_shape=[1, -2])\n    with self.assertRaisesRegexp(tf.errors.InvalidArgumentError,\n                                 ""[Ss]ize 2 must be non-negative, not -2""):\n      mod(tf.ones([1, 2, 3]))\n\n  def testInvalid_type(self):\n    mod = reshape.Reshape(output_shape=[7, ""string""])\n    with self.assertRaises(ValueError):\n      mod(tf.ones([1, 2, 3]))\n\n  def testIncompatibleShape(self):\n    mod = reshape.Reshape(output_shape=[2 * 3, 4])\n\n    input_size = 8 * 2 * 2 * 4\n    output_size = 8 * 2 * 3 * 4\n    msg = (""Input to reshape is a tensor with %d values, ""\n           ""but the requested shape has %d"" % (input_size, output_size))\n    with self.assertRaisesRegexp(tf.errors.InvalidArgumentError, msg):\n      mod(tf.ones([8, 2, 2, 4]))\n\n  def testInferShape(self):\n    batch_size = 10\n    out_size = [2, -1, 5]\n    mod = reshape.Reshape(output_shape=out_size)\n    output = mod(tf.ones([batch_size, 2, 3, 4, 5]))\n    self.assertEqual(output.shape, [batch_size, 2, 3 * 4, 5])\n\n  def testAddDimensions(self):\n    batch_size = 10\n\n    mod = reshape.Reshape(output_shape=[1, 1])\n    inputs = tf.ones([batch_size])\n    output = mod(inputs)\n    self.assertEqual(output.shape, [batch_size, 1, 1])\n\n    # Reverse should remove the additional dims.\n    mod_t = mod.reversed()\n    t_output = mod_t(output)\n    self.assertEqual(t_output.shape, [batch_size])\n\n  def testFlatten(self):\n    batch_size = 10\n    inputs = tf.ones([batch_size, 2, 3, 4, 5])\n    mod = reshape.Reshape(output_shape=[-1])\n    output = mod(inputs)\n    self.assertEqual(output.shape, [batch_size, 2 * 3 * 4 * 5])\n\n  def testUnknownBatchSize(self):\n    mod = reshape.Reshape(output_shape=[-1])\n    input_spec = tf.TensorSpec([None, 2, 3, 4, 5], tf.float32)\n    cf = tf.function(mod).get_concrete_function(input_spec)\n    output, = cf.outputs\n    self.assertEqual(output.shape.as_list(), [None, 2 * 3 * 4 * 5])\n\n  def testReverse(self):\n    batch_size = 10\n    input_shape = [batch_size, 2, 3, 4, 5]\n    expected_output_shape = [batch_size, 2, 3 * 4, 5]\n\n    inputs = tf.random.normal(input_shape)\n    mod = reshape.Reshape(output_shape=[2, -1, 5])\n    output = mod(inputs)\n    self.assertEqual(output.shape, expected_output_shape)\n\n    mod_r = mod.reversed()\n    output_r = mod_r(output)\n    self.assertEqual(output_r.shape, input_shape)\n\n    mod_r_r = mod_r.reversed()\n    output_r_r = mod_r_r(output)\n    self.assertEqual(output_r_r.shape, expected_output_shape)\n\n    input_np, output_r_np = self.evaluate([inputs, output_r])\n    self.assertAllClose(output_r_np, input_np)\n\n  def testReverse_name(self):\n    mod = reshape.Reshape(output_shape=[2, -1, 5])\n    mod(tf.ones([1, 2, 3, 4, 5]))\n    mod_r = mod.reversed()\n    self.assertEqual(mod_r.name, ""%s_reversed"" % mod.name)\n\n  def testInvalidPreserveDimsError(self):\n    with self.assertRaisesRegexp(ValueError, ""preserve_dims""):\n      reshape.Reshape((-1,), preserve_dims=0)\n\n  def testBuildDimError(self):\n    mod = reshape.Reshape((-1,), preserve_dims=2)\n    input_tensor = tf.ones([50])\n    with self.assertRaisesRegexp(ValueError, ""preserve_dims""):\n      mod(input_tensor)\n\n  @parameterized.named_parameters(\n      (""Preserve1"", (1,)),\n      (""Preserve24"", (2, 4)),\n      (""Preserve?"", (None,)),\n      (""Preserve?5"", (None, 5)),\n      (""Preserve5?"", (5, None)),\n      (""Preserve??"", (None, None)),\n  )\n  def testPreserve(self, preserve):\n    shape = list(preserve) + [13, 84, 3, 2]\n    output_shape = [13, 21, 3, 8]\n    preserve_dims = len(preserve)\n    input_spec = tf.TensorSpec(shape, tf.float32)\n    mod = reshape.Reshape(\n        output_shape=output_shape, preserve_dims=preserve_dims)\n    cf = tf.function(mod).get_concrete_function(input_spec)\n    output, = cf.outputs\n    self.assertEqual(output.shape.as_list(), list(preserve) + output_shape)\n\n  @parameterized.named_parameters(\n      (""Session1"", (1,), (2, 3), (-1,)),\n      (""Session2"", (1, 7), (2, 3), (-1,)),\n      (""Session3"", (None,), (2, 3), (-1,)),\n      (""Session4"", (None, 5, None), (2, 3, 4), (4, 6)),\n      (""Session5"", (None, None, None), (2, 3, 4), (-1,)),\n      (""Session6"", (5, None, None), (1, 3, 1), (-1,)),\n      (""Session7"", (1,), (4, 3), (2, 2, 1, 3)),\n      (""Session8"", (None,), (4, 3), (2, 2, 1, 3)),\n      (""Session9"", (1, None, 5, None), (4, 3), (2, 2, -1, 3)),\n  )\n  def testRun(self, preserve, trailing_in, trailing_out):\n    rng = np.random.RandomState(0)\n    input_shape = preserve + trailing_in\n    output_shape = preserve + np.zeros(trailing_in).reshape(trailing_out).shape\n    input_spec = tf.TensorSpec(input_shape, tf.float32)\n    mod = reshape.Reshape(\n        output_shape=trailing_out, preserve_dims=len(preserve))\n    cf = tf.function(mod).get_concrete_function(input_spec)\n    output, = cf.outputs\n    self.assertEqual(output.shape.as_list(), list(output_shape))\n\n    actual_input_shape = [13 if i is None else i for i in input_shape]\n    expected_output_shape = [13 if i is None else i for i in output_shape]\n    actual_input = rng.rand(*actual_input_shape).astype(np.float32)\n    expected_output = actual_input.reshape(expected_output_shape)\n    actual_output = cf(tf.convert_to_tensor(actual_input))\n    self.assertAllEqual(actual_output, expected_output)\n\n\nclass FlattenTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters([1, 10])\n  def testFlatten(self, batch_size):\n    in_shape = [2, 3, 4, 5]\n    inputs = tf.ones([batch_size] + in_shape)\n    mod = reshape.Flatten()\n    output = mod(inputs)\n    flattened_size = np.prod(in_shape, dtype=int)\n    self.assertEqual(output.shape, [batch_size, flattened_size])\n\n  def testFlatten_unknownBatchSize(self):\n    mod = reshape.Flatten()\n    f = tf.function(mod)\n    inputs = tf.TensorSpec([None, 1, 2, 3], tf.float32)\n    cf = f.get_concrete_function(inputs)\n    self.assertEqual(cf.outputs[0].shape.as_list(), [None, 1 * 2 * 3])\n    flat = cf(tf.ones([8, 1, 2, 3]))\n    self.assertEqual(flat.shape, [8, 1 * 2 * 3])\n\n  def testFlatten_unknownNonBatchSize(self):\n    mod = reshape.Flatten()\n    f = tf.function(mod)\n    inputs = tf.TensorSpec([8, None, None, 3], tf.float32)\n    cf = f.get_concrete_function(inputs)\n    self.assertEqual(cf.outputs[0].shape.as_list(), [8, None])\n    flat = cf(tf.ones([8, 1, 2, 3]))\n    self.assertEqual(flat.shape, [8, 1 * 2 * 3])\n\n  @parameterized.parameters(1, 2, 3, 4)\n  def testPreserveDimsOk(self, preserve_dims):\n    in_shape = [10, 2, 3, 4]\n    inputs = tf.ones(in_shape)\n    mod = reshape.Flatten(preserve_dims=preserve_dims)\n    output = mod(inputs)\n    flattened_shape = (\n        in_shape[:preserve_dims] +\n        [np.prod(in_shape[preserve_dims:], dtype=int)])\n    self.assertEqual(output.shape, flattened_shape)\n\n  @parameterized.parameters(5, 6, 7, 10)\n  def testPreserveDimsError(self, preserve_dims):\n    in_shape = [10, 2, 3, 4]\n    inputs = tf.ones(in_shape)\n    mod = reshape.Flatten(preserve_dims=preserve_dims)\n    with self.assertRaisesRegexp(ValueError, ""Input tensor has 4 dimensions""):\n      _ = mod(inputs)\n\n  def testFlattenWithZeroDim(self):\n    inputs = tf.ones([1, 0])\n    output = reshape.Flatten()(inputs)\n    self.assertEqual(output.shape, [1, 0])\n\n  def testInvalidFlattenFromError(self):\n    with self.assertRaisesRegexp(ValueError, ""preserve_dims""):\n      reshape.Flatten(preserve_dims=0)\n\n  def testBuildDimError(self):\n    mod = reshape.Flatten(preserve_dims=2)\n    input_tensor = tf.ones([50])\n    with self.assertRaisesRegexp(ValueError, ""should have at least as many as""):\n      mod(input_tensor)\n\n  @parameterized.parameters([1, 8])\n  def testReverse(self, batch_size):\n    mod = reshape.Flatten(preserve_dims=4)\n    inputs = tf.ones([batch_size, 5, 84, 84, 3, 2])\n    output = mod(inputs)\n    self.assertEqual(output.shape, inputs.shape.as_list()[:4] + [6])\n    mod_r = mod.reversed()\n    output_r = mod_r(output)\n    self.assertEqual(output_r.shape, inputs.shape)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/scale_gradient.py,4,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""TensorFlow op that scales gradient for backwards pass.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nfrom sonnet.src import types\nimport tensorflow as tf\nfrom typing import Tuple\n\n\n@tf.custom_gradient\ndef scale_gradient(\n    t: tf.Tensor, scale: types.FloatLike\n) -> Tuple[tf.Tensor, types.GradFn]:\n  """"""Scales gradients for the backwards pass.\n\n  Args:\n    t: A Tensor.\n    scale: The scale factor for the gradient on the backwards pass.\n\n  Returns:\n    A Tensor same as input, with scaled backward gradient.\n  """"""\n\n  def grad(dy: tf.Tensor) -> Tuple[tf.Tensor, None]:\n    """"""Scaled gradient.""""""\n    return scale * dy, None\n\n  return t, grad\n'"
sonnet/src/scale_gradient_test.py,3,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.scale_gradient.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\n\nfrom absl.testing import parameterized\nfrom sonnet.src import scale_gradient\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\n\nclass ScaleGradientTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters(\n      *itertools.product([-1.0, 0.0, 1.0], [-0.5, 0.0, 0.5, 2.0]))\n  def test_scale(self, t_, scale):\n    t = tf.Variable([t_])\n    with tf.GradientTape() as tape:\n      y = scale_gradient.scale_gradient(t, scale)\n      output = y * y\n    grad = tape.gradient(output, t)\n    self.assertAllEqual(grad.numpy(), [2 * t_ * scale])\n    self.assertAllEqual(output.numpy(), [t_**2])\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/sequential.py,4,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Sequential applies a linear sequence of layers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nfrom sonnet.src import base\nfrom typing import Any, Callable, Iterable, Optional, Text\n\n\nclass Sequential(base.Module):\n  """"""Sequential applies a linear chain of modules / callables.\n\n      >>> mlp = snt.Sequential([\n      ...     snt.Linear(1024),\n      ...     tf.nn.relu,\n      ...     snt.Linear(10),\n      ... ])\n      >>> mlp(tf.random.normal([8, 100]))\n      <tf.Tensor: ...>\n\n  Note that `Sequential` is limited in the range of possible architectures\n  it can handle. This is a deliberate design decision; `Sequential` is only\n  meant to be used for the simple case of fusing together modules/ops where\n  the input of a particular module/op is the output of the previous one.\n\n  Another restriction is that it is not possible to have extra arguments in the\n  `__call__` method that are passed to the constituents of the module - for\n  example, if there is a `BatchNorm` module in `Sequential` and the user wishes\n  to switch the `is_training` flag. If this is the desired use case, the\n  recommended solution is to subclass `snt.Module` and implement `__call__`:\n\n      >>> class CustomModule(snt.Module):\n      ...   def __init__(self, name=None):\n      ...     super(CustomModule, self).__init__(name=name)\n      ...     self.conv2d = snt.Conv2D(32, 4, 2)\n      ...     self.bn = snt.BatchNorm()\n      ...\n      ...   def __call__(self, inputs, is_training):\n      ...     outputs = self.conv2d(inputs)\n      ...     outputs = self.bn(outputs, is_training=is_training)\n      ...     outputs = tf.nn.relu(outputs)\n      ...     return outputs\n  """"""\n\n  def __init__(self,\n               layers: Iterable[Callable[..., Any]] = None,\n               name: Optional[Text] = None):\n    super(Sequential, self).__init__(name=name)\n    self._layers = list(layers) if layers is not None else []\n\n  def __call__(self, inputs, *args, **kwargs):\n    outputs = inputs\n    for i, mod in enumerate(self._layers):\n      if i == 0:\n        # Pass additional arguments to the first layer.\n        outputs = mod(outputs, *args, **kwargs)\n      else:\n        outputs = mod(outputs)\n    return outputs\n'"
sonnet/src/sequential_test.py,1,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.sequential.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom sonnet.src import sequential\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\ninput_parameters = parameterized.parameters(object(), ([[[1.]]],), ({1, 2, 3},),\n                                            None, ""str"", 1)\n\n\nclass SequentialTest(test_utils.TestCase, parameterized.TestCase):\n\n  @input_parameters\n  def test_empty(self, value):\n    net = sequential.Sequential()\n    self.assertIs(net(value), value)\n\n  @input_parameters\n  def test_empty_drops_varargs_varkwargs(self, value):\n    net = sequential.Sequential()\n    self.assertIs(net(value, object(), keyword=object()), value)\n\n  @input_parameters\n  def test_identity_chain(self, value):\n    net = sequential.Sequential([identity, identity, identity])\n    self.assertIs(net(value), value)\n\n  def test_call(self):\n    seq = sequential.Sequential([append_character(ch) for ch in ""rocks!""])\n    self.assertEqual(seq(""Sonnet ""), ""Sonnet rocks!"")\n\n  def test_varargs_varkwargs_to_call(self):\n    layer1 = lambda a, b, c: ((a + b + c), (c + b + a))\n    layer2 = lambda a: a[0] + "","" + a[1]\n    net = sequential.Sequential([layer1, layer2])\n    self.assertEqual(net(""a"", ""b"", c=""c""), ""abc,cba"")\n\n\ndef identity(v):\n  return v\n\n\ndef append_character(c):\n  return lambda v: v + c\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/test_utils.py,4,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Test utilities for Sonnet 2.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport functools\nimport inspect\nimport itertools\nimport os\nimport sys\nimport threading\nimport types\n\nfrom absl.testing import parameterized\nimport tensorflow as tf\nfrom typing import Sequence, Text, Tuple, Type, TypeVar\n\nModule = TypeVar(""Module"")\n\ntpu_initialized = None\ntpu_initialized_lock = threading.Lock()\n\n\nclass TestCase(tf.test.TestCase):\n  """"""Test case which handles TPU hard placement.""""""\n\n  ENTER_PRIMARY_DEVICE = True\n\n  def setUp(self):\n    super(TestCase, self).setUp()\n\n    # Enable autograph strict mode - any autograph errors will trigger an error\n    # rather than falling back to no conversion.\n    os.environ[""AUTOGRAPH_STRICT_CONVERSION""] = ""1""\n\n    self._device_types = frozenset(\n        d.device_type for d in tf.config.experimental.list_logical_devices())\n    self._on_tpu = ""TPU"" in self._device_types\n\n    # Initialize the TPU system once and only once.\n    global tpu_initialized\n    if tpu_initialized is None:\n      with tpu_initialized_lock:\n        if tpu_initialized is None and self._on_tpu:\n          tf.tpu.experimental.initialize_tpu_system()\n        tpu_initialized = True\n\n    if self.ENTER_PRIMARY_DEVICE:\n      self._device = tf.device(""/device:%s:0"" % self.primary_device)\n      self._device.__enter__()\n\n  def tearDown(self):\n    super(TestCase, self).tearDown()\n    if self.ENTER_PRIMARY_DEVICE:\n      self._device.__exit__(*sys.exc_info())\n      del self._device\n\n  @property\n  def primary_device(self):\n    if ""TPU"" in self._device_types:\n      return ""TPU""\n    elif ""GPU"" in self._device_types:\n      return ""GPU""\n    else:\n      return ""CPU""\n\n  @property\n  def device_types(self):\n    return self._device_types\n\n  def get_atol(self):\n    """"""Returns a good tolerance for numerical closeness tests.\n\n    Any TPU matmuls go via bfloat16, so an assertAllClose which passes under\n    some constant small tolerance on CPU will generally fail on TPU. All test\n    cases can call get_atol to get an appropriate number.\n\n    TODO(mareynolds): assess these thresholds in detail.\n\n    Returns:\n      small float, eg 1e-4 on CPU/GPU, 5se-3 on TPU.\n    """"""\n    if self._on_tpu:\n      return 5e-3\n    else:\n      return 1e-4\n\n\ndef find_all_sonnet_modules(\n    root_python_module: types.ModuleType,\n    base_class: Type[Module],\n) -> Sequence[Type[Module]]:\n  """"""Finds all subclasses of `base_class` under `root_python_module`.""""""\n  modules = []\n  for _, python_module in find_sonnet_python_modules(root_python_module):\n    for name in dir(python_module):\n      value = getattr(python_module, name)\n      if inspect.isclass(value) and issubclass(value, base_class):\n        modules.append(value)\n  return modules\n\n\ndef find_sonnet_python_modules(\n    root_module: types.ModuleType,) -> Sequence[Tuple[Text, types.ModuleType]]:\n  """"""Returns `(name, module)` for all Sonnet submodules under `root_module`.""""""\n  modules = set([(root_module.__name__, root_module)])\n  visited = set()\n  to_visit = [root_module]\n\n  while to_visit:\n    mod = to_visit.pop()\n    visited.add(mod)\n\n    for name in dir(mod):\n      obj = getattr(mod, name)\n      if inspect.ismodule(obj) and obj not in visited:\n        if obj.__name__.startswith(""sonnet""):\n          to_visit.append(obj)\n          modules.add((obj.__name__, obj))\n\n  return sorted(modules)\n\n\ndef combined_named_parameters(*parameters):\n  """"""Combines multiple ``@parameterized.named_parameters`` compatible sequences.\n\n  >>> foos = (""a_for_foo"", ""a""), (""b_for_foo"", ""b"")\n  >>> bars = (""c_for_bar"", ""c""), (""d_for_bar"", ""d"")\n\n  >>> @named_parameters(foos)\n  ... def testFoo(self, foo):\n  ...   assert foo in (""a"", ""b"")\n\n  >>> @combined_named_parameters(foos, bars):\n  ... def testFooBar(self, foo, bar):\n  ...   assert foo in (""a"", ""b"")\n  ...   assert bar in (""c"", ""d"")\n\n  Args:\n    *parameters: A sequence of parameters that will be combined and be passed\n      into ``parameterized.named_parameters``.\n\n  Returns:\n    A test generator to be handled by ``parameterized.TestGeneratorMetaclass``.\n  """"""\n  combine = lambda a, b: (""_"".join((a[0], b[0])),) + a[1:] + b[1:]\n  return parameterized.named_parameters(\n      functools.reduce(combine, r) for r in itertools.product(*parameters))\n\n\ndef named_bools(name) -> Sequence[Tuple[Text, bool]]:\n  """"""Returns a pair of booleans suitable for use with ``named_parameters``.""""""\n  return (name, True), (""not_{}"".format(name), False)\n'"
sonnet/src/types.py,4,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Type aliases for Sonnet.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom typing import Callable, Iterable, Mapping, Optional, Sequence, Text, Tuple, Union\n\n# Parameter update type, used by optimizers.\nParameterUpdate = Optional[Union[tf.Tensor, tf.IndexedSlices]]\n\n# Objects that can be treated like tensors (in TF2).\nTensorLike = Union[np.ndarray, tf.Tensor, tf.Variable]\n\n# Note that we have no way of statically verifying the tensor\'s shape.\nBoolLike = Union[bool, np.bool, TensorLike]\nIntegerLike = Union[int, np.integer, TensorLike]\nFloatLike = Union[float, np.floating, TensorLike]\n\nShapeLike = Union[int, Sequence[int], tf.TensorShape]\n\n# Note that this is effectively treated as `Any`; see b/109648354.\nTensorNest = Union[TensorLike, Iterable[\'TensorNest\'],\n                   Mapping[Text, \'TensorNest\'],]  # pytype: disable=not-supported-yet\n\nActivationFn = Callable[[TensorLike], TensorLike]\nAxis = Union[int, slice, Sequence[int]]\nGradFn = Callable[[tf.Tensor], Tuple[tf.Tensor, Optional[tf.Tensor]]]\n'"
sonnet/src/utils.py,36,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Utils for Sonnet.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport collections\nimport functools\nimport inspect\nimport re\n\nfrom absl import logging\nimport six\nfrom sonnet.src import initializers\nimport tabulate\nimport tensorflow as tf\nfrom typing import Any, Callable, Dict, Generic, Optional, Sequence, Text, Tuple, TypeVar, Union\n\nT = TypeVar(""T"")\n\n\ndef replicate(\n    element: Union[T, Sequence[T]],\n    num_times: int,\n    name: Text,\n) -> Tuple[T]:\n  """"""Replicates entry in `element` `num_times` if needed.""""""\n  if not isinstance(element, collections.Sequence):\n    return (element,) * num_times\n  elif len(element) == 1:\n    return tuple(element * num_times)\n  elif len(element) == num_times:\n    return tuple(element)\n  raise TypeError(\n      ""{} must be a scalar or sequence of length 1 or sequence of length {}.""\n      .format(name, num_times))\n\n\ndef _is_object(f: Any) -> bool:\n  return not inspect.isfunction(f) and not inspect.ismethod(f)\n\n\n# TODO(b/123870292) Remove this and use wrapt.decorator when supported by TF.\ndef decorator(\n    decorator_fn: Callable[[T, Any, Sequence[Any], Dict[Text, Any]],\n                           Any],) -> T:\n  """"""Returns a wrapt style decorator.""""""\n\n  @functools.wraps(decorator_fn)\n  def _decorator(f):\n    """"""Wraps f such that it returns the result of applying decorator_fn.""""""\n    if _is_object(f):\n\n      @functools.wraps(f.__call__)\n      def _decorate_object(*args, **kwargs):\n        return decorator_fn(f.__call__, f, args, kwargs)\n\n      return _decorate_object\n\n    if inspect.ismethod(f):\n\n      @functools.wraps(f)\n      def _decorate_bound_method(*args, **kwargs):\n        return decorator_fn(f, f.__self__, args, kwargs)\n\n      return _decorate_bound_method\n\n    argspec = getfullargspec(f)\n    if argspec.args and argspec.args[0] == ""self"":\n\n      @functools.wraps(f)\n      def _decorate_unbound_method(self, *args, **kwargs):\n        bound_method = f.__get__(self, self.__class__)  # pytype: disable=attribute-error\n        return decorator_fn(bound_method, self, args, kwargs)\n\n      return _decorate_unbound_method\n\n    @functools.wraps(f)\n    def _decorate_fn(*args, **kwargs):\n      return decorator_fn(f, None, args, kwargs)\n\n    return _decorate_fn\n\n  return _decorator\n\n\n_SPATIAL_CHANNELS_FIRST = re.compile(""^NC[^C]*$"")\n_SPATIAL_CHANNELS_LAST = re.compile(""^N[^C]*C$"")\n_SEQUENTIAL = re.compile(""^((BT)|(TB))[^D]*D$"")\n\n\ndef get_channel_index(data_format: Text) -> int:\n  """"""Returns the channel index when given a valid data format.\n\n  Args:\n    data_format: String, the data format to get the channel index from. Valid\n      data formats are spatial (e.g.`NCHW`), sequential (e.g. `BTHWD`),\n      `channels_first` and `channels_last`).\n\n  Returns:\n    The channel index as an int - either 1 or -1.\n\n  Raises:\n    ValueError: If the data format is unrecognised.\n  """"""\n  if data_format == ""channels_first"":\n    return 1\n  if data_format == ""channels_last"":\n    return -1\n  if _SPATIAL_CHANNELS_FIRST.match(data_format):\n    return 1\n  if _SPATIAL_CHANNELS_LAST.match(data_format):\n    return -1\n  if _SEQUENTIAL.match(data_format):\n    return -1\n  raise ValueError(\n      ""Unable to extract channel information from \'{}\'. Valid data formats are ""\n      ""spatial (e.g.`NCHW`), sequential (e.g. `BTHWD`), `channels_first` and ""\n      ""`channels_last`)."".format(data_format))\n\n\ndef assert_rank(inputs, rank: int):\n  """"""Asserts the rank of the input is `rank`.""""""\n  shape = tuple(inputs.shape)\n  actual_rank = len(shape)\n  if rank != actual_rank:\n    raise ValueError(""Shape %r must have rank %d"" % (shape, rank))\n\n\ndef assert_minimum_rank(inputs, rank: int):\n  """"""Asserts the rank of the input is at least `rank`.""""""\n  shape = tuple(inputs.shape)\n  actual_rank = len(shape)\n  if actual_rank < rank:\n    raise ValueError(""Shape %r must have rank >= %d"" % (shape, rank))\n\n\ndef smart_autograph(f: T) -> T:\n  """"""Wraps `f` such that in graph mode it uses autograph but not in eager.\n\n  Whilst wrapping `f` in autograph is (intended to be) semantics preserving,\n  some things (e.g. breakpoints) are not preserved. Using `smart_autograph`\n  users can write code with eager syntax, add breakpoints and debug it as you\n  might expect and still be compatible with code that uses\n  `@tf.function(autograph=False)`.\n\n      >>> @smart_autograph\n      ... def f(x):\n      ...   if x > 0:\n      ...     y = x * x\n      ...   else:\n      ...     y = -x\n      ...   return y\n\n      >>> f = tf.function(f, autograph=False)\n      >>> f(tf.constant(2))\n      <tf.Tensor: ... numpy=4>\n\n  Args:\n    f: A function to wrap conditionally in `tf.autograph`.\n\n  Returns:\n    A wrapper for `f` that dispatches to the original or autograph version of f.\n  """"""\n  f_autograph = tf.autograph.to_graph(f)\n\n  @functools.wraps(f)\n  def smart_autograph_wrapper(*args, **kwargs):\n    if tf.executing_eagerly():\n      return f(*args, **kwargs)\n    else:\n      return f_autograph(*args, **kwargs)\n\n  return smart_autograph_wrapper\n\n\ndef getfullargspec(func):\n  """"""Gets the names and default values of a function\'s parameters.""""""\n  if six.PY2:\n    # Assume that we are running with PyType patched Python 2.7 and getargspec\n    # will not barf if `func` has type annotations.\n    return inspect.getargspec(func)\n  else:\n    return inspect.getfullargspec(func)\n\n\ndef variable_like(inputs: Union[tf.Tensor, tf.Variable],\n                  initializer: initializers.Initializer = initializers.Zeros(),\n                  trainable: Optional[bool] = None,\n                  name: Optional[Text] = None) -> tf.Variable:\n  """"""Creates a new variable with the same shape/dtype/device as the input.""""""\n  if trainable is None:\n    trainable = getattr(inputs, ""trainable"", None)\n  if name is None:\n    name = getattr(inputs, ""name"", ""Variable"").split("":"")[0]\n  with tf.device(inputs.device):\n    initial_value = initializer(inputs.shape, inputs.dtype)\n    return tf.Variable(initial_value, trainable=trainable, name=name)\n\n\ndef _render_spec(shape: tf.TensorShape, dtype: tf.DType) -> Text:\n  """"""Renders the given shape/dtype as a short specification.""""""\n\n  format_map = {\n      tf.float16: ""f16"",\n      tf.float32: ""f32"",\n      tf.float64: ""f64"",\n      tf.bfloat16: ""bf16"",\n      tf.complex64: ""c64"",\n      tf.complex128: ""c128"",\n      tf.uint8: ""u8"",\n      tf.uint16: ""u16"",\n      tf.uint32: ""u32"",\n      tf.uint64: ""u64"",\n      tf.int8: ""i8"",\n      tf.int16: ""i16"",\n      tf.int32: ""i32"",\n      tf.int64: ""i64"",\n      tf.qint8: ""qi8"",\n      tf.qint16: ""qi16"",\n      tf.qint32: ""qi32"",\n      tf.quint8: ""qu8"",\n      tf.quint16: ""qu16"",\n  }\n\n  return ""{dtype}[{shape}]"".format(\n      dtype=format_map.get(dtype, dtype.name),\n      shape="","".join(str(d) for d in shape))\n\n\ndef _simple_device(var: tf.Variable) -> Text:\n  device = tf.DeviceSpec.from_string(var.device)\n  if device.job == ""localhost"" and device.replica == 0 and device.task == 0:\n    if device.device_index == 0:\n      return device.device_type\n    else:\n      return ""{} {}"".format(device.device_type, device.device_index)\n  return device\n\n\ndef _name_scope_then_rank(var: tf.Variable):\n  name_scope = ""/"".join(var.name.split(""/"")[:-1])\n  rank = len(var.shape)\n  return (name_scope, -rank, var.name)\n\n\ndef format_variables(variables: Sequence[tf.Variable],\n                     tablefmt: Text = ""orgtbl"") -> Text:\n  """"""Takes a collection of variables and formats it as a table.""""""\n  rows = []\n  for var in sorted(variables, key=_name_scope_then_rank):\n    name = var.name.split("":"")[0]  # Remove the "":0"" suffix.\n    spec = _render_spec(var.shape, var.dtype)\n    trainable = str(var.trainable)\n    device = _simple_device(var)\n    rows.append((name, spec, trainable, device))\n  return tabulate.tabulate(\n      rows,\n      headers=(""Variable"", ""Spec"", ""Trainable"", ""Device""),\n      tablefmt=tablefmt)\n\n\ndef log_variables(variables: Sequence[tf.Variable]):\n  """"""Logs variable information.\n\n  This function logs the name, shape, type, trainability, and device for a\n  given iterable of variables.\n\n  Args:\n    variables: iterable of variables (e.g., `module.variables`, if `module` is a\n      `snt.Module` instance).\n  """"""\n  for line in format_variables(variables).split(""\\n""):\n    logging.info(line)\n\n\n@functools.total_ordering\nclass CompareById(Generic[T]):\n  """"""Container providing hash/eq based on object id.""""""\n\n  def __init__(self, wrapped: T):\n    self.wrapped = wrapped\n\n  def __hash__(self):\n    # NOTE: `dict` has special casing to allow for hash values that are\n    # sequential ints (since `hash(i: int) -> i`) so the using `id` as a hash\n    # code (at least with `dict` and `set`) does not have a big performance\n    # penalty.\n    # https://github.com/python/cpython/blob/master/Objects/dictobject.c#L135\n    return id(self.wrapped)\n\n  def __eq__(self, other):\n    if other is None:\n      return False\n    return self.wrapped is getattr(other, ""wrapped"", None)\n\n  def __lt__(self, other):\n    return id(self.wrapped) < id(getattr(other, ""wrapped"", None))\n'"
sonnet/src/utils_test.py,34,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.utils.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom sonnet.src import initializers\nfrom sonnet.src import test_utils\nfrom sonnet.src import utils\nimport tensorflow as tf\n\n# We have a first ""\\"" for the new line and one at the end. The rest is a direct\n# copy-paste of the ground truth output.\n_EXPECTED_FORMATTED_VARIABLE_LIST = (""""""\\\n| Variable   | Spec     | Trainable   | Device   |\n|------------+----------+-------------+----------|\n| m1/v1      | f32[3,4] | True        | CPU      |\n| m2/v2      | i32[5]   | False       | CPU      |\\\n"""""")\n\n\nclass ReplicateTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters((""Int"", 42), (""Callable"", lambda a: a))\n  def testSingleValue(self, value):\n    result = utils.replicate(value, 3, ""value"")\n    self.assertLen(result, 3)\n    self.assertAllEqual(result, (value,) * 3)\n\n  @parameterized.named_parameters((""Int"", 42), (""String"", ""foo""),\n                                  (""Callable"", lambda a: a))\n  def testListLengthOne(self, value):\n    result = utils.replicate([value], 3, ""value"")\n    self.assertLen(result, 3)\n    self.assertAllEqual(result, (value,) * 3)\n\n  @parameterized.named_parameters((""Int"", 42), (""String"", ""foo""),\n                                  (""Callable"", lambda a: a))\n  def testTupleLengthN(self, value):\n    v = (value,) * 3\n    result = utils.replicate(v, 3, ""value"")\n    self.assertLen(result, 3)\n    self.assertAllEqual(result, (value,) * 3)\n\n  @parameterized.named_parameters((""Int"", 42), (""String"", ""foo""),\n                                  (""Callable"", lambda a: a))\n  def testListLengthN(self, value):\n    v = list((value,) * 3)\n    result = utils.replicate(v, 3, ""value"")\n    self.assertLen(result, 3)\n    self.assertAllEqual(result, (value,) * 3)\n\n  def testIncorrectLength(self):\n    v = [2, 2]\n    with self.assertRaisesRegexp(\n        TypeError,\n        r""must be a scalar or sequence of length 1 or sequence of length 3""):\n      utils.replicate(v, 3, ""value"")\n\n\nclass DecoratorTest(test_utils.TestCase):\n\n  def test_callable_object(self):\n\n    class MyObject(object):\n\n      def __call__(self, x, y):\n        return x**y\n\n    @utils.decorator\n    def double(wrapped, instance, args, kwargs):\n      self.assertIs(instance, o)\n      return 2 * wrapped(*args, **kwargs)\n\n    o = MyObject()\n    f = double(o)  # pylint: disable=no-value-for-parameter\n    self.assertEqual(f(3, y=4), 2 * (3**4))\n\n  def test_function(self):\n\n    @utils.decorator\n    def double(wrapped, instance, args, kwargs):\n      self.assertIsNone(instance)\n      return 2 * wrapped(*args, **kwargs)\n\n    f = double(lambda x, y: x**y)  # pylint: disable=no-value-for-parameter\n    self.assertEqual(f(3, 4), 2 * (3**4))\n\n  def test_unbound_method(self):\n\n    @utils.decorator\n    def double(wrapped, instance, args, kwargs):\n      self.assertIs(instance, o)\n      return 2 * wrapped(*args, **kwargs)\n\n    class MyObject(object):\n\n      @double\n      def f(self, x, y):\n        return x**y\n\n    o = MyObject()\n    self.assertEqual(o.f(3, 4), 2 * (3**4))\n\n  def test_bound_method(self):\n\n    @utils.decorator\n    def double(wrapped, instance, args, kwargs):\n      self.assertIs(instance, o)\n      return 2 * wrapped(*args, **kwargs)\n\n    class MyObject(object):\n\n      def f(self, x, y):\n        return x**y\n\n    o = MyObject()\n    self.assertEqual(double(o.f)(3, 4), 2 * (3**4))  # pylint: disable=no-value-for-parameter\n\n\nclass ChannelIndexTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters(""channels_first"", ""NCHW"", ""NC"", ""NCDHW"")\n  def test_returns_index_channels_first(self, data_format):\n    self.assertEqual(utils.get_channel_index(data_format), 1)\n\n  @parameterized.parameters(""channels_last"", ""NHWC"", ""NDHWC"", ""BTWHD"", ""TBD"")\n  def test_returns_index_channels_last(self, data_format):\n    self.assertEqual(utils.get_channel_index(data_format), -1)\n\n  @parameterized.parameters(""foo"", ""NCHC"", ""BTDTD"", ""chanels_first"", ""NHW"")\n  def test_invalid_strings(self, data_format):\n    with self.assertRaisesRegexp(\n        ValueError,\n        ""Unable to extract channel information from \'{}\'."".format(data_format)):\n      utils.get_channel_index(data_format)\n\n\nclass AssertRankTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      (""tf_tensor"", lambda rank: tf.ones([1] * rank)),\n      (""tf_variable"", lambda rank: tf.Variable(tf.ones([1] * rank))),\n      (""tf_tensorspec"", lambda rank: tf.TensorSpec([1] * rank)),\n      (""np_ndarray"", lambda rank: np.ones([1] * rank)))\n  def test_valid_rank(self, input_fn):\n    for rank in range(2, 5):\n      inputs = input_fn(rank)\n      utils.assert_rank(inputs, rank)\n      utils.assert_minimum_rank(inputs, rank - 2)\n\n  @parameterized.parameters(range(10))\n  def test_invalid_rank(self, rank):\n    x = tf.ones([1] * rank)\n    # pylint: disable=g-error-prone-assert-raises\n    with self.assertRaisesRegexp(ValueError, ""must have rank %d"" % (rank + 1)):\n      utils.assert_rank(x, rank + 1)\n\n    with self.assertRaisesRegexp(ValueError, ""must have rank %d"" % (rank - 1)):\n      utils.assert_rank(x, rank - 1)\n\n    with self.assertRaisesRegexp(ValueError,\n                                 ""must have rank >= %d"" % (rank + 1)):\n      utils.assert_minimum_rank(x, rank + 1)\n    # pylint: enable=g-error-prone-assert-raises\n\n\nclass SmartAutographTest(test_utils.TestCase):\n\n  def test_smart_ag(self):\n\n    def foo(x):\n      if x > 0:\n        y = x * x\n      else:\n        y = -x\n      return y\n\n    with self.assertRaises(Exception):\n      # Without autograph `foo` should not be traceable.\n      func_foo = tf.function(foo, autograph=False)\n      func_foo(tf.constant(2.))\n\n    smart_foo = utils.smart_autograph(foo)\n    func_smart_foo = tf.function(smart_foo, autograph=False)\n    for x in tf.range(-10, 10):\n      y = foo(x)\n      self.assertAllEqual(smart_foo(x), y)\n      self.assertAllEqual(func_smart_foo(x), y)\n\n\nclass VariableLikeTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters(\n      [lambda: tf.constant([0., 1.]), lambda: tf.Variable([0., 1.])])\n  def test_copies_shape(self, a):\n    a = a()\n    b = utils.variable_like(a)\n    self.assertEqual(a.shape, b.shape)\n\n  @parameterized.parameters([\n      lambda: tf.constant(1, dtype=tf.int64),\n      lambda: tf.Variable(1, dtype=tf.int64)\n  ])\n  def test_copies_dtype(self, a):\n    a = a()\n    b = utils.variable_like(a)\n    self.assertEqual(a.dtype, b.dtype)\n\n  @parameterized.parameters([lambda: tf.constant(1.), lambda: tf.Variable(1.)])\n  def test_copies_device(self, a):\n    with tf.device(""CPU:0""):\n      a = a()\n    b = utils.variable_like(a)\n    self.assertEqual(a.device, b.device)\n\n  def test_default_initializer_is_zero(self):\n    a = tf.Variable(1.)\n    b = utils.variable_like(a)\n    self.assertEqual(0., b.numpy())\n\n  def test_override_initializer(self):\n    a = tf.Variable(1.)\n    b = utils.variable_like(a, initializer=initializers.Ones())\n    self.assertEqual(1., b.numpy())\n\n  @parameterized.parameters([True, False])\n  def test_copies_variable_trainable(self, trainable):\n    a = tf.Variable(1., trainable=trainable)\n    b = utils.variable_like(a)\n    self.assertEqual(a.trainable, b.trainable)\n\n  def test_default_trainable_for_tensor(self):\n    a = tf.constant(1.)\n    b = utils.variable_like(a)\n    self.assertEqual(True, b.trainable)\n\n  @parameterized.parameters([True, False])\n  def test_override_trainable(self, trainable):\n    a = tf.Variable(1.)\n    b = utils.variable_like(a, trainable=trainable)\n    self.assertEqual(trainable, b.trainable)\n\n  def test_copies_variable_name(self):\n    a = tf.Variable(1., name=""a"")\n    b = utils.variable_like(a)\n    self.assertEqual(a.name, b.name)\n\n  def test_default_name_for_tensor(self):\n    a = tf.constant(1.)\n    b = utils.variable_like(a)\n    self.assertEqual(""Variable:0"", b.name)\n\n  @parameterized.parameters([lambda: tf.constant(1.), lambda: tf.Variable(1.)])\n  def test_override_name(self, a):\n    a = a()\n    b = utils.variable_like(a, name=""b"")\n    self.assertEqual(""b:0"", b.name)\n\n\nclass FormatVariablesTest(test_utils.TestCase):\n\n  def test_format_variables(self):\n    with tf.device(""/device:CPU:0""):\n      with tf.name_scope(""m1""):\n        v1 = tf.Variable(tf.zeros([3, 4]), name=""v1"")\n      with tf.name_scope(""m2""):\n        v2 = tf.Variable(\n            tf.zeros([5], dtype=tf.int32), trainable=False, name=""v2"")\n      self.assertEqual(\n          utils.format_variables([v2, v1]), _EXPECTED_FORMATTED_VARIABLE_LIST)\n\n  def test_log_variables(self):\n    with tf.device(""/device:CPU:0""):\n      with tf.name_scope(""m1""):\n        v1 = tf.Variable(tf.zeros([3, 4]), name=""v1"")\n      with tf.name_scope(""m2""):\n        v2 = tf.Variable(\n            tf.zeros([5], dtype=tf.int32), trainable=False, name=""v2"")\n      utils.log_variables([v2, v1])\n\n\nclass NotHashable(object):\n\n  def __hash__(self):\n    raise ValueError(""Not hashable"")\n\n\nclass CompareByIdTest(test_utils.TestCase):\n\n  def test_access(self):\n    original = NotHashable()\n    wrapped = utils.CompareById(original)\n    self.assertIs(wrapped.wrapped, original)\n\n  def test_hash(self):\n    original = NotHashable()\n    wrapped = utils.CompareById(original)\n    self.assertEqual(hash(wrapped), id(original))\n\n  def test_eq(self):\n    original1 = NotHashable()\n    original2 = NotHashable()\n    # Different wrappers pointing to the same object should be equal.\n    self.assertEqual(utils.CompareById(original1), utils.CompareById(original1))\n    # The original objet and the wrapped object should not be equal.\n    self.assertNotEqual(original1, utils.CompareById(original1))\n    # Similarly a different object should not be equal to a wrapped object.\n    self.assertNotEqual(original2, utils.CompareById(original1))\n    # None should also not compare.\n    self.assertNotEqual(None, utils.CompareById(original1))\n    # Different wrapped objects should not be equal.\n    self.assertNotEqual(\n        utils.CompareById(original1), utils.CompareById(original2))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/conformance/api_test.py,1,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for Sonnet\'s public API.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom six.moves import reload_module as reload\nimport sonnet as snt\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\n\nclass PublicSymbolsTest(test_utils.TestCase):\n\n  def test_src_not_exported(self):\n    self.assertFalse(hasattr(snt, ""src""))\n\n  def test_supports_reload(self):\n    mysnt = snt\n    for _ in range(2):\n      mysnt = reload(mysnt)\n      self.assertFalse(hasattr(mysnt, ""src""))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/conformance/build_test.py,5,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests modules support `snt.build`.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport sonnet as snt\nfrom sonnet.src import test_utils\nfrom sonnet.src.conformance import descriptors\nimport tensorflow as tf\nimport tree\n\nBATCH_MODULES = descriptors.BATCH_MODULES\nRECURRENT_MODULES = descriptors.RECURRENT_MODULES\n\n\ndef if_present(f):\n  return lambda o: f(o) if o is not None else None\n\n\nclass BuildTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters(*(BATCH_MODULES + RECURRENT_MODULES))\n  def test_build(self, module_fn, input_shape, dtype):\n    module = module_fn()\n    build_output_spec = snt.build(module, tf.TensorSpec(input_shape, dtype))\n    actual_output = module(tf.ones(input_shape, dtype))\n    actual_output_spec = tree.map_structure(\n        if_present(lambda t: tf.TensorSpec(t.shape, t.dtype)), actual_output)\n    tree.map_structure(self.assertCompatible, build_output_spec,\n                       actual_output_spec)\n\n  def assertCompatible(self, a: tf.TensorSpec, b: tf.TensorSpec):\n    self.assertTrue(a.shape.is_compatible_with(b.shape))\n    self.assertEqual(a.dtype, b.dtype)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/conformance/checkpoint_test.py,21,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests checkpointing with Sonnet.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl import logging\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nfrom sonnet.src import test_utils\nfrom sonnet.src.conformance import goldens\nfrom sonnet.src.distribute import replicator as snt_replicator\nfrom sonnet.src.distribute import replicator_test_utils as replicator_utils\nimport tensorflow as tf\nimport tree\n\n\nclass TestCheckpoint(object):\n  """"""Wraps a tf.train.Checkpoint to make it more convenient for testing.""""""\n\n  def __init__(self, golden=None, **kwargs):\n    if golden is None:\n      root = absltest.get_default_test_tmpdir()\n    else:\n      root = os.path.join(\n          ""sonnet/src/conformance/checkpoints/"", golden.name)\n    self._root = root\n    self._prefix = os.path.join(self._root, ""checkpoint"")\n    self._checkpoint = tf.train.Checkpoint(**kwargs)\n\n  def save(self):\n    self._checkpoint.save(file_prefix=self._prefix)\n\n  def restore_latest(self, assert_consumed):\n    status = self._checkpoint.restore(tf.train.latest_checkpoint(self._root))\n    if assert_consumed:\n      # Ensures that all values in the checkpoint have been consumed by some\n      # checkpointable Python object.\n      status.assert_consumed()\n    return status\n\n\ndef with_soft_placement(f):\n  """"""Wraps `f` such that it runs with soft device placement.""""""\n\n  def wrapper(*a, **k):\n    with tf.device(None):\n      return f(*a, **k)\n\n  return wrapper\n\n\nclass GoldenCheckpointsTest(test_utils.TestCase, parameterized.TestCase):\n  """"""Adds test methods running standard checkpointing tests.""""""\n\n  @goldens.all_goldens\n  def test_save_load(self, golden):\n    """"""Test a basic save/load cycle.""""""\n    module = golden.create_module()\n    checkpoint = TestCheckpoint(module=module)\n    all_variables = golden.create_all_variables(module)\n\n    # Save zeros into the checkpoint.\n    self.assertNotEmpty(all_variables)\n    self.assertEqual(all_variables, module.variables)\n    for variable in all_variables:\n      # TODO(tomhennigan) Perhaps limit the range/switch to random to avoid\n      # overflow/underflow in the forward pass?\n      variable.assign(goldens.range_like(variable))\n    checkpoint.save()\n    old_y = golden.forward(module)\n\n    # Overwrite zeros with ones.\n    for variable in all_variables:\n      variable.assign(tf.ones_like(variable))\n\n    # Check restored values match the saved values.\n    checkpoint.restore_latest(assert_consumed=True)\n    for variable in all_variables:\n      self.assertAllClose(\n          variable.read_value(),\n          goldens.range_like(variable),\n          msg=variable.name)\n\n    # Test the output from the module remains stable.\n    if golden.deterministic:\n      tree.map_structure(self.assertAllClose, golden.forward(module), old_y)\n\n  @goldens.all_goldens\n  def test_save_then_load_new_instance(self, golden):\n    """"""Checks that a checkpoint created for one instance can restore another.""""""\n    module_1 = golden.create_module()\n    checkpoint_1 = TestCheckpoint(module=module_1)\n    variables_1 = golden.create_all_variables(module_1)\n\n    module_2 = golden.create_module()\n    checkpoint_2 = TestCheckpoint(module=module_2)\n    variables_2 = golden.create_all_variables(module_2)\n\n    for v1, v2 in zip(variables_1, variables_2):\n      v1.assign(goldens.range_like(v1))\n      v2.assign(tf.ones_like(v2))\n\n    checkpoint_1.save()\n    checkpoint_2.restore_latest(assert_consumed=True)\n\n    # Assert the parameters in both modules are the same.\n    for variable in variables_2:\n      self.assertAllClose(\n          variable.read_value(),\n          goldens.range_like(variable),\n          msg=variable.name)\n\n    # Assert the output from both modules are the same.\n    if golden.deterministic:\n      tree.map_structure(self.assertAllClose, golden.forward(module_1),\n                         golden.forward(module_2))\n\n  @goldens.all_goldens\n  def test_restore_on_create(self, golden):\n    """"""Tests that Variable values are restored on creation.""""""\n    # Create a module, set its variables to sequential values and save.\n    module_1 = golden.create_module()\n    checkpoint_1 = TestCheckpoint(module=module_1)\n    variables_1 = golden.create_all_variables(module_1)\n    for variable in variables_1:\n      variable.assign(goldens.range_like(variable))\n    checkpoint_1.save()\n    golden.forward(module_1)\n\n    # Create a different module, restore from a checkpoint, create parameters\n    # and assert their values are sequential.\n    module_2 = golden.create_module()\n    checkpoint_2 = TestCheckpoint(module=module_2)\n    status = checkpoint_2.restore_latest(assert_consumed=False)\n    variables_2 = golden.create_all_variables(module_2)\n    status.assert_consumed()\n    for var1, var2 in zip(variables_1, variables_2):\n      self.assertAllEqual(var1.read_value(), var2.read_value(), msg=var1.name)\n\n    # Assert the output from both modules is the same.\n    if golden.deterministic:\n      tree.map_structure(self.assertAllClose, golden.forward(module_1),\n                         golden.forward(module_2))\n\n  @goldens.all_goldens\n  def test_restore_golden(self, golden):\n    """"""Test restoring from a golden checkpoint still works.""""""\n    module = golden.create_module()\n    checkpoint = TestCheckpoint(golden=golden, module=module)\n    variables = golden.create_all_variables(module)\n    for variable in variables:\n      variable.assign(tf.zeros_like(variable))\n    checkpoint.restore_latest(assert_consumed=True)\n    for variable in variables:\n      self.assertAllEqual(\n          variable.read_value(),\n          goldens.range_like(variable),\n          msg=variable.name)\n\n\nclass ReplicatorCheckpointTest(test_utils.TestCase, parameterized.TestCase):\n\n  def replicator_or_skip(self, replicator_fn, use_function):\n    replicator = replicator_fn()\n    if not use_function and isinstance(replicator,\n                                       snt_replicator.TpuReplicator):\n      self.skipTest(""TpuReplicator does not support eager mode."")\n    return replicator\n\n  @test_utils.combined_named_parameters(goldens.named_goldens(),\n                                        replicator_utils.named_replicators(),\n                                        test_utils.named_bools(""use_function""))\n  def test_save_restore(self, golden, replicator_fn, use_function):\n    replicator = self.replicator_or_skip(replicator_fn, use_function)\n\n    with replicator.scope():\n      module = golden.create_module()\n      variables = golden.create_all_variables(module)\n\n    def forward():\n      per_replica = replicator.run(\n          lambda: golden.forward(module))\n      return tree.map_structure(\n          lambda args: tf.stack(replicator.unwrap(args), axis=0), per_replica)\n\n    if use_function:\n      forward = tf.function(forward)\n      if self.primary_device == ""TPU"":\n        # TODO(b/132329316) Remove when `xla.compile` allows tf.device(TPU).\n        forward = with_soft_placement(forward)\n\n    # Assign sequential values to the weights.\n    for index, variable in enumerate(variables):\n      variable.assign(goldens.range_like(variable, start=index))\n\n    # Create a checkpoint and save the weights.\n    checkpoint = TestCheckpoint(module=module)\n    checkpoint.save()\n\n    # Compute a forward pass of the previously saved module.\n    before_save_ys = forward()\n\n    # Assign different values into the weights and do another forward pass. The\n    # result should be different.\n    for variable in variables:\n      variable.assign(-tf.ones_like(variable))\n\n    if golden.deterministic:\n      y = forward()\n      self.assertNotAllClose(y, before_save_ys)\n\n    # Restore from the checkpoint and assert the module is in the same state.\n    checkpoint.restore_latest(assert_consumed=True)\n\n    for index, variable in enumerate(variables):\n      # Parameters should be restored to their previous values.\n      self.assertAllEqual(\n          variable.read_value(),\n          goldens.range_like(variable, start=index),\n          msg=variable.name)\n\n    if golden.deterministic:\n      tree.map_structure(self.assertAllEqual, forward(), before_save_ys)\n\n  @test_utils.combined_named_parameters(goldens.named_goldens(),\n                                        replicator_utils.named_replicators())\n  def test_restore_from_golden(self, golden, replicator_fn):\n    replicator = self.replicator_or_skip(replicator_fn, use_function=False)\n\n    with replicator.scope():\n      module = golden.create_module()\n      variables = golden.create_all_variables(module)\n    checkpoint = TestCheckpoint(golden=golden, module=module)\n    checkpoint.restore_latest(assert_consumed=True)\n    for variable in variables:\n      self.assertAllEqual(\n          variable.read_value(),\n          goldens.range_like(variable),\n          msg=variable.name)\n\n  @test_utils.combined_named_parameters(goldens.named_goldens(),\n                                        replicator_utils.named_replicators(),\n                                        test_utils.named_bools(""use_function""))\n  def test_restore_from_non_distributed(self, golden, replicator_fn,\n                                        use_function):\n    replicator = self.replicator_or_skip(replicator_fn, use_function)\n\n    # Save a checkpoint from a non-distributed model.\n    module = golden.create_module()\n    normal_variables = golden.create_all_variables(module)\n    for index, variable in enumerate(normal_variables):\n      variable.assign(goldens.range_like(variable, start=(index + 1)))\n    checkpoint = TestCheckpoint(module=module)\n    checkpoint.save()\n\n    # Create the same model (new params) in the replicator scope.\n    with replicator.scope():\n      module2 = golden.create_module()\n      replicator_variables = golden.create_all_variables(module2)\n\n    # Ensure the distributed params are != the values in the checkpoint.\n    for normal, distributed in zip(normal_variables, replicator_variables):\n      distributed.assign(tf.zeros_like(distributed))\n      self.assertNotAllClose(normal.read_value(), distributed.read_value())\n\n    # Restore the checkpoint and ensure the parameters are the same.\n    checkpoint = TestCheckpoint(module=module2)\n    checkpoint.restore_latest(assert_consumed=True)\n\n    for normal, distributed in zip(normal_variables, replicator_variables):\n      self.assertAllEqual(\n          normal.read_value(), distributed.read_value(), msg=normal.name)\n\n    if golden.deterministic:\n\n      def run_forward(module):\n        forward = lambda: golden.forward(module)\n        if use_function:\n          forward = tf.function(forward)\n          if self.primary_device == ""TPU"":\n            # TODO(b/132329316) Remove when `xla.compile` allows tf.device(TPU).\n            forward = with_soft_placement(forward)\n        return forward()\n\n      y_before = run_forward(module)\n      y_after = run_forward(module2)\n      tree.map_structure(self.assertAllEqual, y_before, y_after)\n\n  @test_utils.combined_named_parameters(goldens.named_goldens(),\n                                        replicator_utils.named_replicators())\n  def test_restore_on_create(self, golden, replicator_fn):\n    replicator = self.replicator_or_skip(replicator_fn, use_function=False)\n\n    # Save a checkpoint from a non-distributed model.\n    module = golden.create_module()\n    normal_variables = golden.create_all_variables(module)\n    for index, variable in enumerate(normal_variables):\n      variable.assign(goldens.range_like(variable, start=(index + 1)))\n    checkpoint = TestCheckpoint(module=module)\n    checkpoint.save()\n    golden.forward(module)\n\n    # Create the same model (new params) in the replicator scope.\n    with replicator.scope():\n      module = golden.create_module()\n      checkpoint = TestCheckpoint(module=module)\n      status = checkpoint.restore_latest(assert_consumed=False)\n      golden.forward(module)\n      status.assert_consumed()\n      replicator_variables = module.variables\n\n    for normal, distributed in zip(normal_variables, replicator_variables):\n      self.assertAllEqual(\n          normal.read_value(), distributed.read_value(), msg=normal.name)\n\n  @test_utils.combined_named_parameters(goldens.named_goldens(),\n                                        replicator_utils.named_replicators(),\n                                        test_utils.named_bools(""use_function""))\n  def test_restore_on_create_in_replica_context(self, golden, replicator_fn,\n                                                use_function):\n    replicator = self.replicator_or_skip(replicator_fn, use_function)\n\n    # Save a checkpoint from a non-distributed model.\n    module = golden.create_module()\n    normal_variables = golden.create_all_variables(module)\n    for index, variable in enumerate(normal_variables):\n      variable.assign(goldens.range_like(variable, start=(index + 1)))\n    checkpoint = TestCheckpoint(module=module)\n    checkpoint.save()\n    golden.forward(module)\n\n    with replicator.scope():\n      module = golden.create_module()\n\n    def forward():\n      return replicator.run(lambda: golden.forward(module))\n\n    if use_function:\n      forward = tf.function(forward)\n      if self.primary_device == ""TPU"":\n        # TODO(b/132329316) Remove when `xla.compile` allows tf.device(TPU).\n        forward = with_soft_placement(forward)\n\n    checkpoint = TestCheckpoint(module=module)\n    status = checkpoint.restore_latest(assert_consumed=False)\n    result = forward()\n    status.assert_consumed()\n\n    if golden.deterministic:\n      result_iter = iter(replicator.experimental_local_results(result))\n      first_replica = next(result_iter)\n      for next_replica in result_iter:\n        self.assertAllEqual(first_replica, next_replica)\n\n    if not golden.has_side_effects:\n      replicator_variables = module.variables\n      for normal, distributed in zip(normal_variables, replicator_variables):\n        self.assertAllClose(\n            normal.read_value(), distributed.read_value(), msg=normal.name)\n\n\ndef setUpModule():\n  # If a physical GPU is available make sure TF sees at least two.\n  gpus = tf.config.experimental.list_physical_devices(device_type=""GPU"")\n  if len(gpus) == 1:\n    logging.info(""Splitting one physical GPU into two logical GPUs."")\n    tf.config.experimental.set_virtual_device_configuration(\n        gpus[0], [\n            tf.config.experimental.VirtualDeviceConfiguration(\n                memory_limit=1024),\n            tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)\n        ])\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/conformance/copy_test.py,1,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n\n""""""Tests copying Sonnet modules.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport copy\n\nfrom absl.testing import parameterized\nfrom sonnet.src import test_utils\nfrom sonnet.src.conformance import goldens\nimport tensorflow as tf\nimport tree\n\n\nclass CopyTest(test_utils.TestCase, parameterized.TestCase):\n\n  @goldens.all_goldens\n  def test_copy(self, golden):\n    m1 = golden.create_module()\n    golden.create_all_variables(m1)\n    m2 = copy.deepcopy(m1)\n    self.assertIsNot(m1, m2)\n\n    # Check that module variables are recreated with equivalent properties.\n    for v1, v2 in zip(m1.variables, m2.variables):\n      self.assertIsNot(v1, v2)\n      self.assertEqual(v1.name, v2.name)\n      self.assertEqual(v1.device, v2.device)\n      self.assertAllEqual(v1.read_value(), v2.read_value())\n\n    if golden.deterministic:\n      y1 = golden.forward(m1)\n      y2 = golden.forward(m2)\n      tree.map_structure(self.assertAllEqual, y1, y2)\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/conformance/descriptors.py,7,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Module descriptors programatically describe how to use modules.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport collections\n\nimport sonnet as snt\nimport tensorflow as tf\nfrom typing import Callable, Union\n\n\nclass Wrapped(snt.Module):\n\n  @snt.no_name_scope\n  def __init__(self, wrapped: snt.Module):\n    super(Wrapped, self).__init__()\n    self.wrapped = wrapped\n\n\nclass Training(Wrapped):\n\n  @snt.no_name_scope\n  def __call__(self, x: tf.Tensor):\n    return self.wrapped(x, is_training=True)\n\n\nclass Recurrent(Wrapped):\n  """"""Unrolls a recurrent module.""""""\n\n  def __init__(self,\n               module: Union[snt.RNNCore, snt.UnrolledRNN],\n               unroller=None):\n    super(Recurrent, self).__init__(module)\n    self.unroller = unroller\n\n  @snt.no_name_scope\n  def __call__(self, x: tf.Tensor):\n    initial_state = self.wrapped.initial_state(batch_size=tf.shape(x)[0])\n    if isinstance(self.wrapped, snt.UnrolledRNN):\n      assert self.unroller is None\n      # The module expects TB...-shaped input as opposed to BT...\n      x = tf.transpose(x, [1, 0] + list(range(2, x.shape.rank)))\n      return self.wrapped(x, initial_state)\n    else:\n      x = tf.expand_dims(x, axis=0)\n      return self.unroller(self.wrapped, x, initial_state)\n\n\ndef unwrap(module: snt.Module) -> snt.Module:\n  while isinstance(module, Wrapped):\n    module = module.wrapped\n  return module\n\n\n# TODO(tomhennigan) De-duplicate this, BATCH_MODULES and goldens.py.\nModuleDescriptor = collections.namedtuple(""ModuleDescriptor"",\n                                          [""name"", ""create"", ""shape"", ""dtype""])\nModuleDescriptor.__new__.__defaults__ = (None, None, None, tf.float32)\n\nBATCH_SIZE = 8\n\n# pylint: disable=unnecessary-lambda\nBATCH_MODULES = (\n    ModuleDescriptor(\n        name=""BatchNorm"",\n        create=lambda: Training(snt.BatchNorm(True, True)),\n        shape=(BATCH_SIZE, 2, 2, 3)),\n    ModuleDescriptor(\n        name=""Bias"", create=lambda: snt.Bias(), shape=(BATCH_SIZE, 3, 3, 3)),\n    ModuleDescriptor(\n        name=""Conv1D"",\n        create=lambda: snt.Conv1D(3, 3),\n        shape=(BATCH_SIZE, 2, 2)),\n    ModuleDescriptor(\n        name=""Conv1DTranspose"",\n        create=lambda: snt.Conv1DTranspose(3, 3),\n        shape=(BATCH_SIZE, 2, 2)),\n    ModuleDescriptor(\n        name=""Conv2D"",\n        create=lambda: snt.Conv2D(3, 3),\n        shape=(BATCH_SIZE, 2, 2, 2)),\n    ModuleDescriptor(\n        name=""Conv2DTranspose"",\n        create=lambda: snt.Conv2DTranspose(3, 3),\n        shape=(BATCH_SIZE, 2, 2, 2)),\n    ModuleDescriptor(\n        name=""Conv3D"",\n        create=lambda: snt.Conv3D(3, 3),\n        shape=(BATCH_SIZE, 2, 2, 2, 2)),\n    ModuleDescriptor(\n        name=""Conv3DTranspose"",\n        create=lambda: snt.Conv3DTranspose(3, 3),\n        shape=(BATCH_SIZE, 2, 2, 2, 2)),\n    ModuleDescriptor(\n        name=""CrossReplicaBatchNorm"",\n        create=lambda: Training(snt.distribute.CrossReplicaBatchNorm(  # pylint: disable=g-long-lambda\n            True, True,\n            snt.ExponentialMovingAverage(0.9),\n            snt.ExponentialMovingAverage(0.9))),\n        shape=(BATCH_SIZE, 2, 2, 3)),\n    ModuleDescriptor(\n        name=""DepthwiseConv2D"",\n        create=lambda: snt.DepthwiseConv2D(3),\n        shape=(BATCH_SIZE, 2, 2, 2)),\n    ModuleDescriptor(\n        name=""Dropout"",\n        create=lambda: Training(snt.Dropout(0.5)),\n        shape=(BATCH_SIZE, 3, 3)),\n    ModuleDescriptor(\n        name=""Embed"",\n        create=lambda: snt.Embed(10),\n        shape=(BATCH_SIZE,),\n        dtype=tf.int32),\n    ModuleDescriptor(\n        name=""Flatten"",\n        create=lambda: snt.Flatten(),\n        shape=(BATCH_SIZE, 3, 3, 3)),\n    ModuleDescriptor(\n        name=""GroupNorm"",\n        create=lambda: snt.GroupNorm(2, True, True),\n        shape=(BATCH_SIZE, 3, 4)),\n    ModuleDescriptor(\n        name=""InstanceNorm"",\n        create=lambda: snt.InstanceNorm(True, True),\n        shape=(BATCH_SIZE, 3, 2)),\n    ModuleDescriptor(\n        name=""LayerNorm"",\n        create=lambda: snt.LayerNorm(1, True, True),\n        shape=(BATCH_SIZE, 3, 2)),\n    ModuleDescriptor(\n        name=""Linear"", create=lambda: snt.Linear(10), shape=(BATCH_SIZE, 1)),\n    ModuleDescriptor(\n        name=""Sequential"",\n        create=lambda: snt.Sequential([lambda x: x]),\n        shape=(BATCH_SIZE, 2, 2)),\n    ModuleDescriptor(\n        name=""nets.VectorQuantizer"",\n        create=lambda: Training(snt.nets.VectorQuantizer(4, 6, 0.25)),\n        shape=(BATCH_SIZE, 3, 4)),\n    ModuleDescriptor(\n        name=""nets.VectorQuantizerEMA"",\n        create=lambda: Training(snt.nets.VectorQuantizerEMA(5, 7, 0.5, 0.9)),\n        shape=(BATCH_SIZE, 5)),\n    ModuleDescriptor(\n        name=""nets.Cifar10ConvNet"",\n        create=lambda: Training(snt.nets.Cifar10ConvNet()),\n        shape=(BATCH_SIZE, 3, 3, 2)),\n    ModuleDescriptor(\n        name=""nets.ResNet50"",\n        create=lambda: Training(snt.nets.ResNet([1, 1, 1, 1], 4)),\n        shape=(BATCH_SIZE, 3, 3, 2)),\n    ModuleDescriptor(\n        name=""nets.MLP"",\n        create=lambda: snt.nets.MLP([3, 4, 5]),\n        shape=(BATCH_SIZE, 3)),\n)\n\nRNN_CORES = (\n    ModuleDescriptor(\n        name=""Conv1DLSTM"",\n        create=lambda: snt.Conv1DLSTM((2, 2), 3, 3),\n        shape=(BATCH_SIZE, 2, 2)),\n    ModuleDescriptor(\n        name=""Conv2DLSTM"",\n        create=lambda: snt.Conv2DLSTM((2, 2, 2), 3, 3),\n        shape=(BATCH_SIZE, 2, 2, 2)),\n    ModuleDescriptor(\n        name=""Conv3DLSTM"",\n        create=lambda: snt.Conv3DLSTM((2, 2, 2, 2), 3, 3),\n        shape=(BATCH_SIZE, 2, 2, 2, 2)),\n    ModuleDescriptor(\n        name=""GRU"",\n        create=lambda: snt.GRU(1),\n        shape=(BATCH_SIZE, 128)),\n    ModuleDescriptor(\n        name=""LSTM"",\n        create=lambda: snt.LSTM(1),\n        shape=(BATCH_SIZE, 128)),\n    ModuleDescriptor(\n        name=""VanillaRNN"",\n        create=lambda: snt.VanillaRNN(8),\n        shape=(BATCH_SIZE, 128)),\n)\n\nUNROLLED_RNN_CORES = (\n    ModuleDescriptor(\n        name=""UnrolledLSTM"",\n        create=lambda: snt.UnrolledLSTM(1),\n        shape=(BATCH_SIZE, 1, 128)),\n)\n\n\ndef recurrent_factory(\n    create_core: Callable[[], snt.RNNCore],\n    unroller,\n) -> Callable[[], Recurrent]:\n  return lambda: Recurrent(create_core(), unroller)\n\n\ndef unroll_descriptors(descriptors, unroller=None):\n  """"""Returns `Recurrent` wrapped descriptors with the given unroller applied.""""""\n  out = []\n  for name, create, shape, dtype in descriptors:\n    if unroller is None:\n      name = ""Recurrent({})"".format(name)\n    else:\n      name = ""Recurrent({}, {})"".format(name, unroller.__name__)\n    out.append(\n        ModuleDescriptor(name=name,\n                         create=recurrent_factory(create, unroller),\n                         shape=shape,\n                         dtype=dtype))\n  return tuple(out)\n\n\nRECURRENT_MODULES = (\n    unroll_descriptors(RNN_CORES, snt.dynamic_unroll) +\n    unroll_descriptors(RNN_CORES, snt.static_unroll) +\n    unroll_descriptors(UNROLLED_RNN_CORES))\n\n\nOPTIMIZER_MODULES = (\n    ModuleDescriptor(\n        name=""optimizers.Adam"",\n        create=lambda: snt.optimizers.Adam(learning_rate=0.1)),\n    ModuleDescriptor(\n        name=""optimizers.Momentum"",\n        create=lambda: snt.optimizers.Momentum(learning_rate=0.1, momentum=.9)),\n    ModuleDescriptor(\n        name=""optimizers.RMSProp"",\n        create=lambda: snt.optimizers.RMSProp(learning_rate=0.1)),\n    ModuleDescriptor(\n        name=""optimizers.SGD"",\n        create=lambda: snt.optimizers.SGD(learning_rate=0.1)),\n)\n\nIGNORED_MODULES = {\n    # Stateless or abstract.\n    snt.BatchApply,\n    snt.Deferred,\n    snt.Module,\n    snt.Optimizer,\n    snt.Reshape,\n\n    # Metrics.\n    snt.ExponentialMovingAverage,\n    snt.Mean,\n    snt.Metric,\n    snt.Sum,\n\n    # Normalization.\n    snt.BaseBatchNorm,  # Tested via `snt.BatchNorm`.\n\n    # Recurrent.\n    snt.DeepRNN,\n    snt.RNNCore,\n    snt.TrainableState,\n    snt.UnrolledRNN,\n\n    # Tested via `snt.nets.ResNet`.\n    snt.nets.ResNet50,\n    snt.nets.resnet.BottleNeckBlockV1,\n    snt.nets.resnet.BottleNeckBlockV2,\n    snt.nets.resnet.BlockGroup,\n}\n'"
sonnet/src/conformance/descriptors_test.py,1,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.conformance.descriptors.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sonnet as snt\nfrom sonnet.src import test_utils\nfrom sonnet.src.conformance import descriptors\nimport tensorflow as tf\n\nBATCH_MODULES = descriptors.BATCH_MODULES\nRECURRENT_MODULES = descriptors.RECURRENT_MODULES\nOPTIMIZER_MODULES = descriptors.OPTIMIZER_MODULES\nIGNORED_MODULES = descriptors.IGNORED_MODULES\n\n\nclass DescriptorsTest(test_utils.TestCase):\n\n  def test_coverage(self):\n    all_modules = frozenset(test_utils.find_all_sonnet_modules(snt, snt.Module))\n    tested_modules = {\n        type(descriptors.unwrap(d.create()))\n        for d in BATCH_MODULES + RECURRENT_MODULES + OPTIMIZER_MODULES\n    }\n    self.assertEmpty(all_modules - (tested_modules | IGNORED_MODULES))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
sonnet/src/conformance/distribute_test.py,11,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests Sonnet and TF Distribution Strategy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport sonnet as snt\nfrom sonnet.src import test_utils\nfrom sonnet.src.conformance import descriptors\nfrom sonnet.src.conformance import goldens\nfrom sonnet.src.distribute import replicator as snt_replicator\nfrom sonnet.src.distribute import replicator_test_utils as replicator_utils\nimport tensorflow as tf\nfrom typing import Callable, Tuple\n\n\nclass TpuReplicatorTest(test_utils.TestCase, parameterized.TestCase):\n\n  @test_utils.combined_named_parameters(goldens.named_goldens(),\n                                        replicator_utils.named_replicators())\n  def test_variable_creation_in_replica_context(self, golden, replicator_fn):\n    tf.random.set_seed(None)\n    replicator = replicator_fn()\n\n    with replicator.scope():\n      mod = golden.create_module()\n\n    @tf.function\n    def forward():\n      step = lambda: golden.create_all_variables(mod)\n      return replicator.run(step)\n\n    # TODO(b/132329316) Remove when `xla.compile` allows tf.device(TPU).\n    with tf.device(None):\n      variables_per_replica = forward()\n\n    self.assertLen(variables_per_replica, golden.num_variables)\n\n    for per_replica_variable in variables_per_replica:\n      self.assertSameValuePerReplica(replicator, per_replica_variable)\n\n  def assertSameValuePerReplica(self, replicator, per_replica):\n    per_replica = replicator.experimental_local_results(per_replica)\n    first_replica = per_replica[0]\n    for nth_replica in per_replica[1:]:\n      self.assertAllEqual(first_replica, nth_replica)\n\n  @test_utils.combined_named_parameters(descriptors.RNN_CORES,\n                                        test_utils.named_bools(""dynamic""),\n                                        replicator_utils.named_replicators())\n  def test_unroll(\n      self,\n      core_fn: Callable[[], snt.RNNCore],\n      input_shape: Tuple[int],\n      dtype: tf.DType,\n      dynamic: bool,\n      replicator_fn: tf.distribute.Strategy,\n  ):\n    replicator = replicator_fn()\n    with replicator.scope():\n      core = core_fn()\n\n    def step_fn():\n      def forward():\n        unroll = snt.dynamic_unroll if dynamic else snt.static_unroll\n        sequence = tf.ones((1,) + input_shape, dtype)\n        state = core.initial_state(input_shape[0])\n        return unroll(core, sequence, state)\n\n      return replicator.run(forward)\n\n    # TpuReplicator doesn\'t support pure eager mode.\n    if isinstance(replicator, snt_replicator.TpuReplicator):\n      step_fn = tf.function(step_fn)\n\n    # TODO(b/132329316) Remove when `xla.compile` allows tf.device(TPU).\n    with tf.device(None):\n      out_sequence, final_state = step_fn()\n\n    self.assertSameValuePerReplica(replicator, out_sequence)\n    self.assertSameValuePerReplica(replicator, final_state)\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/conformance/doctest_test.py,2,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Ensures that code samples in Sonnet are accurate.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport doctest\nimport inspect\n\nfrom absl.testing import parameterized\nimport sonnet as snt\nfrom sonnet.src import test_utils\nimport tensorflow as tf\n\n\nclass DoctestTest(test_utils.TestCase, parameterized.TestCase):\n\n  # Avoid running doctests inside a `with tf.device` block.\n  ENTER_PRIMARY_DEVICE = False\n\n  def setUp(self):\n    super(DoctestTest, self).setUp()\n    if self.primary_device != ""TPU"":\n      # `TpuReplicator` cannot be constructed without a TPU, however it has\n      # exactly the same API as `Replicator` so we can run doctests using that\n      # instead.\n      snt.distribute.TpuReplicator = snt.distribute.Replicator\n\n  @parameterized.named_parameters(test_utils.find_sonnet_python_modules(snt))\n  def test_doctest(self, module):\n    # `snt` et al import all dependencies from `src`, however doctest does not\n    # test imported deps so we must manually set `__test__` such that imported\n    # symbols are tested.\n    # See: docs.python.org/3/library/doctest.html#which-docstrings-are-examined\n    if not hasattr(module, ""__test__"") or not module.__test__:\n      module.__test__ = {}\n    for name in module.__all__:\n      value = getattr(module, name)\n      if not inspect.ismodule(value):\n        if (inspect.isclass(value) or isinstance(value, str) or\n            inspect.isfunction(value) or inspect.ismethod(value)):\n          module.__test__[name] = value\n        elif hasattr(value, ""__doc__""):\n          module.__test__[name] = value.__doc__\n\n    num_failed, num_attempted = doctest.testmod(\n        module,\n        optionflags=doctest.ELLIPSIS | doctest.NORMALIZE_WHITESPACE,\n        extraglobs={\n            ""snt"": snt,\n            ""tf"": tf\n        })\n    if num_attempted == 0:\n      self.skipTest(""No doctests in %s"" % module.__name__)\n    self.assertEqual(num_failed, 0, ""{} doctests failed"".format(num_failed))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/conformance/function_test.py,26,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Ensures that all Sonnet modules support ``tf.function``.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport sonnet as snt\nfrom sonnet.src import test_utils\nfrom sonnet.src.conformance import descriptors\nimport tensorflow as tf\nfrom typing import Callable, Tuple\n\nModuleFn = Callable[[], snt.Module]\nBATCH_MODULES = descriptors.BATCH_MODULES\nRECURRENT_MODULES = descriptors.RECURRENT_MODULES\nOPTIMIZER_MODULES = descriptors.OPTIMIZER_MODULES\nIGNORED_MODULES = descriptors.IGNORED_MODULES\n\n\nclass FunctionTest(test_utils.TestCase, parameterized.TestCase):\n\n  @test_utils.combined_named_parameters(BATCH_MODULES + RECURRENT_MODULES,\n                                        test_utils.named_bools(""autograph""))\n  def test_trace(\n      self,\n      module_fn: ModuleFn,\n      input_shape: Tuple[int],\n      dtype: tf.DType,\n      autograph: bool,\n  ):\n    module = module_fn()\n    forward = tf.function(module, autograph=autograph)\n    forward(tf.ones(input_shape, dtype=dtype))\n\n  @test_utils.combined_named_parameters(BATCH_MODULES + RECURRENT_MODULES,\n                                        test_utils.named_bools(""autograph""))\n  def test_create_variables_eagerly(\n      self,\n      module_fn: ModuleFn,\n      input_shape: Tuple[int],\n      dtype: tf.DType,\n      autograph: bool,\n  ):\n    module = module_fn()\n    f = snt.distribute.create_variables_eagerly(module)\n    forward = tf.function(f, autograph=autograph)\n    forward(tf.ones(input_shape, dtype=dtype))\n\n  @test_utils.combined_named_parameters(BATCH_MODULES + RECURRENT_MODULES,\n                                        test_utils.named_bools(""autograph""))\n  def test_trace_batch_agnostic(\n      self,\n      module_fn: ModuleFn,\n      input_shape: Tuple[int],\n      dtype: tf.DType,\n      autograph: bool,\n  ):\n    module = module_fn()\n    forward = tf.function(module, autograph=autograph)\n    input_spec = tf.TensorSpec((None,) + input_shape[1:], dtype=dtype)\n    cf = forward.get_concrete_function(input_spec)\n    cf(tf.ones(input_shape, dtype=dtype))\n\n  @test_utils.combined_named_parameters(BATCH_MODULES,\n                                        test_utils.named_bools(""autograph""))\n  def test_trace_batch_apply_batch_agnostic(\n      self,\n      module_fn: ModuleFn,\n      input_shape: Tuple[int],\n      dtype: tf.DType,\n      autograph: bool,\n  ):\n    module = snt.BatchApply(module_fn())\n    forward = tf.function(module, autograph=autograph)\n    input_shape = (8,) + input_shape\n    input_spec = tf.TensorSpec((None, None) + input_shape[2:], dtype=dtype)\n    cf = forward.get_concrete_function(input_spec)\n    if isinstance(\n        descriptors.unwrap(module.module),\n        (snt.nets.VectorQuantizer, snt.nets.VectorQuantizerEMA)):\n      # TODO(tomhennigan) Make VQ and VQ-EMA batch agnostic under BatchApply.\n      return\n    cf(tf.ones(input_shape, dtype=dtype))\n\n  @test_utils.combined_named_parameters(OPTIMIZER_MODULES,\n                                        test_utils.named_bools(""autograph""))\n  def test_optimizer_dense(\n      self,\n      optimizer_fn: ModuleFn,\n      input_shape: Tuple[int],\n      dtype: tf.DType,\n      autograph: bool,\n  ):\n    del input_shape, dtype  # Unused.\n    parameters = [tf.Variable([1., 2.])]\n    updates = [tf.constant([5., 5.])]\n    optimizer = optimizer_fn()\n    optimizer_apply = tf.function(optimizer.apply, autograph=autograph)\n    optimizer_apply(updates, parameters)\n\n  # TODO(petebu) Add a test with completely dynamic shapes.\n\n  @test_utils.combined_named_parameters(OPTIMIZER_MODULES,\n                                        test_utils.named_bools(""autograph""))\n  def test_optimizer_sparse(\n      self,\n      optimizer_fn: ModuleFn,\n      input_shape: Tuple[int],\n      dtype: tf.DType,\n      autograph: bool,\n  ):\n    del input_shape, dtype  # Unused.\n    if self.primary_device == ""TPU"":\n      self.skipTest(""IndexedSlices not supported on TPU."")\n    parameters = [tf.Variable([[1.], [2.]])]\n    updates = [\n        tf.IndexedSlices(\n            tf.constant([0.1], shape=[1, 1]), tf.constant([0]),\n            tf.constant([2, 1]))\n    ]\n    optimizer = optimizer_fn()\n    optimizer_apply = tf.function(optimizer.apply, autograph=autograph)\n    optimizer_apply(updates, parameters)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/conformance/goldens.py,52,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Golden test cases.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport abc\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport six\nimport sonnet as snt\nimport tensorflow as tf\nfrom typing import Text, Tuple, Sequence\n\n_all_goldens = []\n\n\ndef named_goldens() -> Sequence[Tuple[Text, ""Golden""]]:\n  return ((name, cls()) for _, name, cls in list_goldens())\n\n\ndef all_goldens(test_method):\n  return parameterized.named_parameters(named_goldens())(test_method)\n\n\ndef _register_golden(module_cls, golden_name):\n\n  def registration_fn(golden_cls):\n    _all_goldens.append((module_cls, golden_name, golden_cls))\n    golden_cls.name = golden_name\n    return golden_cls\n\n  return registration_fn\n\n\ndef list_goldens():\n  return list(_all_goldens)\n\n\ndef range_like(t, start=0):\n  """"""Returns a tensor with sequential values of the same dtype/shape as `t`.\n\n  >>> range_like(tf.ones([2, 2]))\n  <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n  array([[ 0.,  1.],\n         [ 2.,  3.]], dtype=float32)>\n\n  >>> range_like(tf.ones([2, 2]), start=5)\n  <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n  array([[ 5.,  6.],\n         [ 7.,  8.]], dtype=float32)>\n\n  Args:\n    t: A tensor like object (with shape and dtype).\n    start: Value to start the range from.\n\n  Returns:\n    A `tf.Tensor` with sequential element values the same shape/dtype as `t`.\n  """"""\n  return tf.reshape(\n      tf.cast(\n          tf.range(start,\n                   np.prod(t.shape, dtype=int) + start), dtype=t.dtype),\n      t.shape)\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass Golden(object):\n  """"""Represents a golden checkpoint file.""""""\n\n  @abc.abstractmethod\n  def create_module(self):\n    """"""Should create a new module instance and return it.""""""\n    pass\n\n  @abc.abstractmethod\n  def create_all_variables(self, module):\n    """"""Create all variables for the given model and return them.""""""\n    pass\n\n  @abc.abstractmethod\n  def forward(self, module):\n    """"""Return the output from calling the module with a fixed input.""""""\n    pass\n\n\n@six.add_metaclass(abc.ABCMeta)\nclass AbstractGolden(Golden):\n  """"""Abstract base class for golden tests of single input modules.""""""\n\n  deterministic = True\n\n  has_side_effects = False\n\n  # Tolerance to be used for assertAllClose calls on TPU, where lower precision\n  # can mean results differ more.\n  tpu_atol = 1e-3\n\n  @abc.abstractproperty\n  def input_spec(self):\n    pass\n\n  @abc.abstractproperty\n  def num_variables(self):\n    pass\n\n  def forward(self, module):\n    x = range_like(self.input_spec, start=1)\n    return module(x)\n\n  def create_all_variables(self, module):\n    self.forward(module)\n    variables = module.variables\n    assert len(variables) == self.num_variables, (\n        ""Expected %d params, got %d %r"" %\n        (self.num_variables, len(variables), variables))\n    return variables\n\n\n# pylint: disable=missing-docstring\n@_register_golden(snt.Linear, ""linear_1x1"")\nclass Linear1x1Test(AbstractGolden):\n  create_module = lambda _: snt.Linear(1)\n  input_spec = tf.TensorSpec([128, 1])\n  num_variables = 2\n\n\n@_register_golden(snt.Linear, ""linear_nobias_1x1"")\nclass LinearNoBias1x1(AbstractGolden):\n  create_module = lambda _: snt.Linear(1, with_bias=False)\n  input_spec = tf.TensorSpec([1, 1])\n  num_variables = 1\n\n\n@_register_golden(snt.Conv1D, ""conv1d_3x3_2x2"")\nclass Conv1D(AbstractGolden):\n  create_module = lambda _: snt.Conv1D(output_channels=3, kernel_shape=3)\n  input_spec = tf.TensorSpec([1, 2, 2])\n  num_variables = 2\n\n\n@_register_golden(snt.Conv2D, ""conv2d_3x3_2x2"")\nclass Conv2D(AbstractGolden):\n  create_module = lambda _: snt.Conv2D(output_channels=3, kernel_shape=3)\n  input_spec = tf.TensorSpec([1, 2, 2, 2])\n  num_variables = 2\n\n\n@_register_golden(snt.Conv3D, ""conv3d_3x3_2x2"")\nclass Conv3D(AbstractGolden):\n  create_module = lambda _: snt.Conv3D(output_channels=3, kernel_shape=3)\n  input_spec = tf.TensorSpec([1, 2, 2, 2, 2])\n  num_variables = 2\n\n\n@_register_golden(snt.Conv1DTranspose, ""conv1d_transpose_3x3_2x2"")\nclass Conv1DTranspose(AbstractGolden):\n  create_module = (\n      lambda _: snt.Conv1DTranspose(output_channels=3, kernel_shape=3))\n  input_spec = tf.TensorSpec([1, 2, 2])\n  num_variables = 2\n\n\n@_register_golden(snt.Conv2DTranspose, ""conv2d_transpose_3x3_2x2"")\nclass Conv2DTranspose(AbstractGolden):\n  create_module = (\n      lambda _: snt.Conv2DTranspose(output_channels=3, kernel_shape=3))\n  input_spec = tf.TensorSpec([1, 2, 2, 2])\n  num_variables = 2\n\n\n@_register_golden(snt.Conv3DTranspose, ""conv3d_transpose_3x3_2x2"")\nclass Conv3DTranspose(AbstractGolden):\n  create_module = (\n      lambda _: snt.Conv3DTranspose(output_channels=3, kernel_shape=3))\n  input_spec = tf.TensorSpec([1, 2, 2, 2, 2])\n  num_variables = 2\n\n\n@_register_golden(snt.DepthwiseConv2D, ""depthwise_conv2d_3x3_2x2"")\nclass DepthwiseConv2D(AbstractGolden):\n  create_module = lambda _: snt.DepthwiseConv2D(kernel_shape=3)\n  input_spec = tf.TensorSpec([1, 2, 2, 2])\n  num_variables = 2\n\n\n@_register_golden(snt.nets.MLP, ""mlp_3x4x5_1x3"")\nclass MLP(AbstractGolden):\n  create_module = (lambda _: snt.nets.MLP([3, 4, 5]))\n  input_spec = tf.TensorSpec([1, 3])\n  num_variables = 6\n\n\n@_register_golden(snt.nets.MLP, ""mlp_nobias_3x4x5_1x3"")\nclass MLPNoBias(AbstractGolden):\n  create_module = (lambda _: snt.nets.MLP([3, 4, 5], with_bias=False))\n  input_spec = tf.TensorSpec([1, 3])\n  num_variables = 3\n\n\n@_register_golden(snt.nets.Cifar10ConvNet, ""cifar10_convnet_2x3_2x2_1x3x3x2"")\nclass Cifar10ConvNet(AbstractGolden):\n  create_module = (\n      lambda _: snt.nets.Cifar10ConvNet(output_channels=(2, 3), strides=(2, 2)))\n  input_spec = tf.TensorSpec([1, 3, 3, 2])\n  num_variables = 22\n\n  def forward(self, module):\n    x = range_like(self.input_spec, start=1)\n    return module(x, is_training=False, test_local_stats=True)[""logits""]\n\n\n@_register_golden(snt.LayerNorm, ""layer_norm_1_1x3_2"")\nclass LayerNorm(AbstractGolden):\n  create_module = (\n      lambda _: snt.LayerNorm(1, create_scale=True, create_offset=True))\n  input_spec = tf.TensorSpec([1, 3, 2])\n  num_variables = 2\n\n\n@_register_golden(snt.InstanceNorm, ""instance_norm_1_1x3_2"")\nclass Instance(AbstractGolden):\n  create_module = (\n      lambda _: snt.InstanceNorm(create_scale=True, create_offset=True))\n  input_spec = tf.TensorSpec([1, 3, 2])\n  num_variables = 2\n\n\n@_register_golden(snt.GroupNorm, ""group_norm_2_1x3x4"")\nclass GroupNorm(AbstractGolden):\n  create_module = (\n      lambda _: snt.GroupNorm(2, create_scale=True, create_offset=True))\n  input_spec = tf.TensorSpec([1, 3, 4])\n  num_variables = 2\n\n\n@_register_golden(snt.BaseBatchNorm, ""base_batch_norm_1x2x2x3"")\nclass BaseBatchNorm(AbstractGolden):\n  create_module = (\n      lambda _: snt.BaseBatchNorm(True, False, FooMetric(), FooMetric()))  # pytype: disable=wrong-arg-types\n  input_spec = tf.TensorSpec([1, 2, 2, 3])\n  num_variables = 2\n\n  def forward(self, module):\n    x = range_like(self.input_spec, start=1)\n    return module(x, is_training=False, test_local_stats=True)\n\n\n@_register_golden(snt.BaseBatchNorm, ""base_batch_norm_scale_offset_1x2x2x3"")\nclass BaseBatchNormScaleOffset(AbstractGolden):\n  create_module = (\n      lambda _: snt.BaseBatchNorm(True, False, FooMetric(), FooMetric()))  # pytype: disable=wrong-arg-types\n  input_spec = tf.TensorSpec([1, 2, 2, 3])\n  num_variables = 2\n\n  def forward(self, module):\n    x = range_like(self.input_spec, start=1)\n    return module(x, is_training=False, test_local_stats=True)\n\n\n@_register_golden(snt.BatchNorm, ""batch_norm_1x2x2x3"")\nclass BatchNorm(AbstractGolden):\n  create_module = (lambda _: snt.BatchNorm(True, True))\n  input_spec = tf.TensorSpec([1, 2, 2, 3])\n  num_variables = 8\n\n  def forward(self, module):\n    x = range_like(self.input_spec, start=1)\n    return module(x, is_training=False, test_local_stats=True)\n\n\n@_register_golden(snt.BatchNorm, ""batch_norm_scale_offset_1x2x2x3"")\nclass BatchNormScaleOffset(AbstractGolden):\n  create_module = (lambda _: snt.BatchNorm(True, True))\n  input_spec = tf.TensorSpec([1, 2, 2, 3])\n  num_variables = 8\n\n  def forward(self, module):\n    x = range_like(self.input_spec, start=1)\n    return module(x, is_training=False, test_local_stats=True)\n\n\n@_register_golden(snt.ExponentialMovingAverage, ""ema_2"")\nclass ExponentialMovingAverage(AbstractGolden):\n  create_module = (lambda _: snt.ExponentialMovingAverage(decay=0.9))\n  input_spec = tf.TensorSpec([2])\n  num_variables = 3\n  has_side_effects = True\n\n  def forward(self, module):\n    x = range_like(self.input_spec, start=1)\n    return module(x)\n\n\n@_register_golden(snt.BatchNorm, ""batch_norm_training_1x2x2x3"")\nclass BatchNormTraining(AbstractGolden):\n  create_module = (lambda _: snt.BatchNorm(True, True))\n  input_spec = tf.TensorSpec([1, 2, 2, 3])\n  num_variables = 8\n  has_side_effects = True\n\n  def forward(self, module):\n    x = range_like(self.input_spec, start=1)\n    return module(x, is_training=True)\n\n\n@_register_golden(snt.distribute.CrossReplicaBatchNorm,\n                  ""cross_replica_batch_norm_1x2x2x3"")\nclass CrossReplicaBatchNorm(AbstractGolden):\n  create_module = (\n      lambda _: snt.BaseBatchNorm(True, False, FooMetric(), FooMetric()))\n  input_spec = tf.TensorSpec([1, 2, 2, 3])\n  num_variables = 2\n\n  def forward(self, module):\n    x = range_like(self.input_spec, start=1)\n    return module(x, is_training=False, test_local_stats=True)\n\n\n@_register_golden(snt.Dropout, ""dropout"")\nclass DropoutVariableRate(AbstractGolden):\n  create_module = lambda _: snt.Dropout(rate=tf.Variable(0.5))\n  input_spec = tf.TensorSpec([3, 3, 3])\n  num_variables = 1\n  deterministic = False\n\n  def forward(self, module):\n    tf.random.set_seed(3)\n    x = range_like(self.input_spec, start=1)\n    return module(x, is_training=True)\n\n\nclass AbstractRNNGolden(AbstractGolden):\n\n  def forward(self, module):\n    # Small inputs to ensure that tf.tanh and tf.sigmoid don\'t saturate.\n    x = 1.0 / range_like(self.input_spec, start=1)\n    batch_size = self.input_spec.shape[0]\n    prev_state = module.initial_state(batch_size)\n    y, next_state = module(x, prev_state)\n    del next_state\n    return y\n\n\n@_register_golden(snt.Conv1DLSTM, ""conv1d_lstm_3x3_2x2"")\nclass Conv1DLSTM(AbstractRNNGolden):\n  input_spec = tf.TensorSpec([1, 2, 2])\n  num_variables = 3\n\n  def create_module(self):\n    return snt.Conv1DLSTM(\n        input_shape=self.input_spec.shape[1:],\n        output_channels=3,\n        kernel_shape=3)\n\n\n@_register_golden(snt.Conv2DLSTM, ""conv2d_lstm_3x3_2x2"")\nclass Conv2DLSTM(AbstractRNNGolden):\n  input_spec = tf.TensorSpec([1, 2, 2, 2])\n  num_variables = 3\n\n  def create_module(self):\n    return snt.Conv2DLSTM(\n        input_shape=self.input_spec.shape[1:],\n        output_channels=3,\n        kernel_shape=3)\n\n\n@_register_golden(snt.Conv3DLSTM, ""conv3d_lstm_3x3_2x2"")\nclass Conv3DLSTM(AbstractRNNGolden):\n  input_spec = tf.TensorSpec([1, 2, 2, 2, 2])\n  num_variables = 3\n\n  def create_module(self):\n    return snt.Conv3DLSTM(\n        input_shape=self.input_spec.shape[1:],\n        output_channels=3,\n        kernel_shape=3)\n\n\n@_register_golden(snt.GRU, ""gru_1"")\nclass GRU(AbstractRNNGolden):\n  create_module = lambda _: snt.GRU(hidden_size=1)\n  input_spec = tf.TensorSpec([1, 128])\n  num_variables = 3\n\n\n@_register_golden(snt.LSTM, ""lstm_1"")\nclass LSTM(AbstractRNNGolden):\n  create_module = lambda _: snt.LSTM(hidden_size=1)\n  input_spec = tf.TensorSpec([1, 128])\n  num_variables = 3\n\n\n@_register_golden(snt.LSTM, ""lstm_8_projected_1"")\nclass LSTMWithProjection(AbstractRNNGolden):\n  create_module = lambda _: snt.LSTM(hidden_size=8, projection_size=1)\n  input_spec = tf.TensorSpec([1, 128])\n  num_variables = 4\n\n\n@_register_golden(snt.UnrolledLSTM, ""unrolled_lstm_1"")\nclass UnrolledLSTM(AbstractRNNGolden):\n  create_module = lambda _: snt.UnrolledLSTM(hidden_size=1)\n  input_spec = tf.TensorSpec([1, 1, 128])\n  num_variables = 3\n\n\n@_register_golden(snt.VanillaRNN, ""vanilla_rnn_8"")\nclass VanillaRNN(AbstractRNNGolden):\n  create_module = lambda _: snt.VanillaRNN(hidden_size=8)\n  input_spec = tf.TensorSpec([1, 128])\n  num_variables = 3\n\n\n@_register_golden(snt.TrainableState, ""trainable_state"")\nclass TrainableState(AbstractGolden):\n  create_module = lambda _: snt.TrainableState(tf.zeros([1]))\n  input_spec = tf.TensorSpec(())\n  num_variables = 1\n\n\n@_register_golden(snt.Bias, ""bias_3x3x3"")\nclass BiasTest(AbstractGolden):\n  create_module = lambda _: snt.Bias()\n  input_spec = tf.TensorSpec([1, 3, 3, 3])\n  num_variables = 1\n\n\n@_register_golden(snt.Embed, ""embed_100_100"")\nclass EmbedTest(AbstractGolden):\n  create_module = lambda _: snt.Embed(vocab_size=100, embed_dim=100)\n  input_spec = tf.TensorSpec([10], dtype=tf.int32)\n  num_variables = 1\n\n\n@_register_golden(snt.Mean, ""mean_2x2"")\nclass MeanTest(AbstractGolden):\n  create_module = lambda _: snt.Mean()\n  input_spec = tf.TensorSpec([2, 2])\n  num_variables = 2\n  has_side_effects = True\n\n\n@_register_golden(snt.Sum, ""sum_2x2"")\nclass SumTest(AbstractGolden):\n  create_module = lambda _: snt.Sum()\n  input_spec = tf.TensorSpec([2, 2])\n  num_variables = 1\n  has_side_effects = True\n\n\n@_register_golden(snt.nets.ResNet, ""resnet50"")\nclass ResNet(AbstractGolden):\n  create_module = (lambda _: snt.nets.ResNet([1, 1, 1, 1], 9))\n  input_spec = tf.TensorSpec([1, 8, 8, 3])\n  num_variables = 155\n  has_side_effects = True\n\n  def forward(self, module):\n    x = range_like(self.input_spec, start=1)\n    return module(x, is_training=True)\n\n\n@_register_golden(snt.nets.VectorQuantizer, ""vqvae"")\nclass VectorQuantizerTest(AbstractGolden):\n\n  def create_module(self):\n    return snt.nets.VectorQuantizer(\n        embedding_dim=4, num_embeddings=6, commitment_cost=0.25)\n\n  # Input can be any shape as long as final dimension is equal to embedding_dim.\n  input_spec = tf.TensorSpec([2, 3, 4])\n\n  def forward(self, module):\n    x = range_like(self.input_spec)\n    return module(x, is_training=True)\n\n  # Numerical results can be quite different on TPU, be a bit more loose here.\n  tpu_atol = 4e-2\n\n  num_variables = 1\n\n\n@_register_golden(snt.nets.VectorQuantizerEMA, ""vqvae_ema_train"")\nclass VectorQuantizerEMATrainTest(AbstractGolden):\n\n  def create_module(self):\n    return snt.nets.VectorQuantizerEMA(\n        embedding_dim=5, num_embeddings=7, commitment_cost=0.5, decay=0.9)\n\n  # Input can be any shape as long as final dimension is equal to embedding_dim.\n  input_spec = tf.TensorSpec([2, 5])\n\n  def forward(self, module):\n    x = range_like(self.input_spec)\n    return module(x, is_training=True)\n\n  # Numerical results can be quite different on TPU, be a bit more loose here.\n  tpu_atol = 4e-2\n\n  num_variables = 7  # 1 embedding, then 2 EMAs each of which contain 3.\n  has_side_effects = True\n\n\n@_register_golden(snt.nets.VectorQuantizerEMA, ""vqvae_ema_eval"")\nclass VectorQuantizerEMAEvalTest(AbstractGolden):\n\n  def create_module(self):\n    return snt.nets.VectorQuantizerEMA(\n        embedding_dim=3, num_embeddings=4, commitment_cost=0.5, decay=0.9)\n\n  # Input can be any shape as long as final dimension is equal to embedding_dim.\n  input_spec = tf.TensorSpec([2, 3])\n\n  def forward(self, module):\n    x = range_like(self.input_spec)\n    return module(x, is_training=False)\n\n  # Numerical results can be quite different on TPU, be a bit more loose here.\n  tpu_atol = 4e-2\n\n  num_variables = 7  # 1 embedding, then 2 EMAs each of which contain 3.\n  has_side_effects = False  # only has side effects when is_training==True\n\n\n# pylint: enable=missing-docstring\n\n\nclass FooMetric(snt.Metric):\n  """"""Used for testing a class which uses Metrics.""""""\n\n  def initialize(self, x):\n    pass\n\n  def reset(self):\n    pass\n\n  def update(self, x):\n    pass\n'"
sonnet/src/conformance/goldens_test.py,1,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests goldens cover all modules.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport inspect\n\nimport sonnet as snt\nfrom sonnet.src import test_utils\nfrom sonnet.src.conformance import goldens\nimport tensorflow as tf\n\n\nclass CoverageTest(test_utils.TestCase):\n\n  def test_all_modules_covered(self):\n    no_checkpoint_whitelist = set([\n        # TODO(petebu): Remove this once optimizer goldens check works.\n        snt.optimizers.Adam,\n        snt.optimizers.Momentum,\n        snt.optimizers.RMSProp,\n        snt.optimizers.SGD,\n\n        # Stateless or abstract.\n        snt.BatchApply,\n        snt.DeepRNN,\n        snt.Deferred,\n        snt.Flatten,\n        snt.Metric,\n        snt.Module,\n        snt.Optimizer,\n        snt.Reshape,\n        snt.RNNCore,\n        snt.Sequential,\n        snt.UnrolledRNN,\n\n        # Tested via snt.nets.ResNet\n        snt.nets.ResNet50,\n        snt.nets.resnet.BottleNeckBlockV1,\n        snt.nets.resnet.BottleNeckBlockV2,\n        snt.nets.resnet.BlockGroup\n    ])\n\n    # Find all the snt.Module types reachable from `import sonnet as snt`\n    all_sonnet_types = set()\n    for _, python_module in test_utils.find_sonnet_python_modules(snt):\n      for _, cls in inspect.getmembers(python_module, inspect.isclass):\n        if issubclass(cls, snt.Module):\n          all_sonnet_types.add(cls)\n\n    # Find all the modules that have checkpoint tests.\n    tested_modules = {module_cls for module_cls, _, _ in goldens.list_goldens()}\n\n    # Make sure we don\'t leave entries in no_checkpoint_whitelist if they are\n    # actually tested.\n    self.assertEmpty(tested_modules & no_checkpoint_whitelist)\n\n    # Make sure everything is covered.\n    self.assertEqual(tested_modules | no_checkpoint_whitelist, all_sonnet_types)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/conformance/keras_test.py,22,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests Sonnet and Keras compatibility.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport sonnet as snt\nfrom sonnet.src import test_utils\nfrom sonnet.src.conformance import descriptors\nimport tensorflow as tf\nimport tree\n\nBATCH_MODULES = descriptors.BATCH_MODULES\nRECURRENT_MODULES = descriptors.RECURRENT_MODULES\n\n\n# TODO(tomhennigan) Add tests with Keras optimizers.\n# TODO(tomhennigan) Test Keras compile/fit.\nclass KerasTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.named_parameters(*(BATCH_MODULES + RECURRENT_MODULES))\n  def test_build_without_batch(self, module_fn, input_shape, dtype):\n    # For Keras test that building with unknown batch dim is supported.\n    layer = LayerAdapter(module=module_fn(), dtype=dtype)\n    layer.build((None,) + input_shape[1:])\n\n    # For Sonnet just call with the example input.\n    mod = module_fn()\n    mod(tf.ones(input_shape, dtype))\n\n    # Some modules (e.g. Sequential) are parameter-less.\n    snt.allow_empty_variables(mod)\n\n    # Test that module variables look the same.\n    by_name = lambda c: sorted(c, key=lambda v: v.name)\n    abstract = lambda v: (v.name, v.shape, v.dtype)\n    for collection in (""variables"", ""trainable_variables""):\n      for m, l in zip(\n          by_name(getattr(mod, collection)),\n          by_name(getattr(layer, collection))):\n        self.assertEqual(abstract(m), abstract(l))\n\n  @parameterized.named_parameters(*(BATCH_MODULES + RECURRENT_MODULES))\n  def test_sonnet_module_as_layer(self, module_fn, input_shape, dtype):\n    mod = module_fn()\n    layer = LayerAdapter(module=module_fn(), dtype=dtype)\n    example_input = tf.ones(input_shape, dtype=dtype)\n\n    # Check outputs are the same.\n    for m_y, l_y in zip(\n        tree.flatten(mod(example_input)), tree.flatten(layer(example_input))):\n      self.assertEqual(m_y.shape, l_y.shape)\n      self.assertEqual(m_y.dtype, l_y.dtype)\n\n    # Some modules (e.g. Sequential) are parameter-less.\n    snt.allow_empty_variables(mod)\n\n    # Check that variables are the same.\n    self.assertEqual(len(mod.variables), len(layer.variables))\n    self.assertEqual(\n        len(mod.trainable_variables), len(layer.trainable_variables))\n\n    # Check that Keras layer freezing works\n    layer.trainable = False\n    self.assertEmpty(layer.trainable_variables)\n\n  def test_build_with_updating_module(self):\n    # Calling the module creates and updates `w`.\n    mod = ModuleWithUpdateInCall()\n    mod(tf.ones([]))\n    self.assertEqual(mod.w.numpy(), 1)\n\n    # Calling build() should not trigger updating `w`, just creating it.\n    layer = LayerAdapter(ModuleWithUpdateInCall())\n    layer.build([])\n    self.assertEqual(layer.module.w.numpy(), 0)\n\n  def test_layer_with_model(self):\n    layers = [\n        LayerAdapter(snt.Linear(3)),\n        LayerAdapter(snt.Linear(2)),\n        LayerAdapter(snt.Linear(1))\n    ]\n\n    model = tf.keras.models.Sequential(layers)\n    model.build([None, 4])\n    for idx, input_size in enumerate([4, 3, 2]):\n      self.assertEqual(layers[idx].module.input_size, input_size)\n\n    output_shape = model.compute_output_shape([None, 4])\n    self.assertTrue(output_shape.is_compatible_with([None, 1]))\n\n    self.assertEqual(model(tf.ones([1, 4])).shape, [1, 1])\n\n  @parameterized.named_parameters(*(BATCH_MODULES + RECURRENT_MODULES))\n  def test_symbolic_model(self, module_fn, input_shape, dtype):\n    module = module_fn()\n\n    inputs = tf.keras.Input(input_shape[1:], dtype=dtype)\n    layer = LayerAdapter(module=module, dtype=dtype)\n    output = layer(inputs)\n    model = tf.keras.Model(inputs=inputs, outputs=output)\n\n    example_input = tf.ones(input_shape, dtype=dtype)\n    # Check outputs are the same.\n    for m_y, l_y in zip(\n        tree.flatten(module(example_input)),\n        tree.flatten(model(example_input))):\n      self.assertEqual(m_y.shape, l_y.shape)\n      self.assertEqual(m_y.dtype, l_y.dtype)\n\n  def test_layer_adapter_custom_method(self):\n    module = ModuleWithCustomForward()\n\n    inputs = tf.keras.Input([], batch_size=1)\n    layer = LayerAdapter(module=module, method=""forward"")\n    output = layer(inputs)\n    model = tf.keras.Model(inputs=inputs, outputs=output)\n\n    self.assertEqual(model(tf.ones([])).numpy(), [2.])\n    self.assertEqual(model.trainable_variables, [module.w])\n\n  def test_keras_layer_inside_sonnet_module(self):\n    mod = ModuleWithLayer()\n    mod(tf.ones([1, 1]))\n    self.assertEqual(mod.submodules, (mod.dense,))\n    self.assertLen(mod.variables, 2)\n    self.assertLen(mod.trainable_variables, 2)\n\n    # Test that layer freezing does not change tf.Module tracking.\n    mod.dense.trainable = False\n    self.assertLen(mod.variables, 2)\n    self.assertLen(mod.trainable_variables, 2)\n\n  def test_to_config(self):\n    mod = LayerAdapter(ModuleWithLayer())\n    with self.assertRaises(NotImplementedError):\n      mod.to_config()\n\n  def test_from_config(self):\n    with self.assertRaises(NotImplementedError):\n      LayerAdapter.from_config(None)\n\n\n# TODO(tomhennigan) Make this part of the public API?\nclass LayerAdapter(tf.keras.layers.Layer):\n  """"""Adapts a Sonnet module to conform to the Keras Layer API.\n\n      >>> layer = LayerAdapter(snt.Linear(1))\n      >>> assert isinstance(layer, tf.keras.layers.Layer)\n\n  We support building without ``__call__``, even with unknown dimensions:\n\n      >>> layer.build(input_shape=[None, 28 * 28])\n\n  Of course now features of Keras work as expected, for example layer freezing:\n\n      >>> [v.name for v in layer.trainable_variables]\n      [""linear/b:0"", ""linear/w:0""]\n\n      >>> layer.trainable = False\n      >>> layer.trainable_variables\n      []\n  """"""\n\n  def __init__(self, module, method=""__call__"", dtype=tf.float32):\n    super(LayerAdapter, self).__init__(dtype=dtype)\n    self.module = module\n    self._module_call_method = getattr(module, method)\n    self._output_shapes = None\n\n  @classmethod\n  def from_config(cls, config):\n    raise NotImplementedError\n\n  def to_config(self):\n    raise NotImplementedError\n\n  def _trace_and_initialize(self, input_shape):\n    if self._output_shapes is None:\n      self._output_shapes = tree.map_structure(\n          lambda spec: spec.shape if spec is not None else spec,\n          snt.build(self, tf.TensorSpec(input_shape, self.dtype)))\n\n    return self._output_shapes\n\n  def compute_output_shape(self, input_shape):\n    output_shapes = self._trace_and_initialize(input_shape)\n    return output_shapes\n\n  def build(self, input_shape):\n    super(LayerAdapter, self).build(input_shape)\n\n    # Trigger variable initialization by tracing the module.\n    self._trace_and_initialize(input_shape)\n\n    # Make sure Keras variable tracking finds our weights.\n    # Keras has a setattr override which can be used to register weights in a\n    # similar way to `Layer.add_weight`. By setting `_sonnet_weights` we trigger\n    # this mechanism and module weights are found in `Layer.trainable_variables`\n    # and `Layer.variables`.\n    snt.allow_empty_variables(self.module)\n    self._sonnet_weights = self.module.variables\n\n  def call(self, inputs):\n    return self._module_call_method(inputs)\n\n\nclass ModuleWithLayer(snt.Module):\n\n  def __init__(self):\n    super(ModuleWithLayer, self).__init__()\n    self.dense = tf.keras.layers.Dense(10)\n\n  def __call__(self, x):\n    return self.dense(x)\n\n\nclass ModuleWithUpdateInCall(snt.Module):\n\n  @snt.once\n  def _init(self, x):\n    self.w = tf.Variable(tf.zeros(x.shape), name=""w"")\n\n  def __call__(self, x):\n    self._init(x)\n    self.w.assign_add(tf.ones_like(self.w))\n    return self.w.read_value()\n\n\nclass ModuleWithCustomForward(snt.Module):\n\n  @snt.once\n  def _init(self, x):\n    self.w = tf.Variable(tf.ones(x.shape), name=""w"")\n\n  def forward(self, x):\n    self._init(x)\n    return x + self.w\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/conformance/optimizer_test.py,4,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Conformance tests for models and optimization.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom sonnet.src import test_utils\nfrom sonnet.src.conformance import descriptors\nimport tensorflow as tf\n\nBATCH_MODULES = descriptors.BATCH_MODULES\nRECURRENT_MODULES = descriptors.RECURRENT_MODULES\n\n\nclass OptimizerConformanceTest(test_utils.TestCase, parameterized.TestCase):\n\n  @test_utils.combined_named_parameters(\n      BATCH_MODULES + RECURRENT_MODULES,\n      test_utils.named_bools(""construct_module_in_function""),\n  )\n  def test_variable_order_is_constant(self, module_fn, input_shape, dtype,\n                                      construct_module_in_function):\n    """"""Test that variable access order is consistent in built in modules.""""""\n    logged_variables = []\n    mod = [None]\n    if not construct_module_in_function:\n      mod[0] = module_fn()\n\n    x = tf.zeros(input_shape, dtype=dtype)\n\n    @tf.function(autograph=False)\n    def f():\n      with tf.GradientTape() as tape:\n        if not mod[0]:\n          mod[0] = module_fn()\n        mod[0](x)  # pylint: disable=not-callable\n\n      # Leak out the variables that were used.\n      logged_variables.append(\n          [(id(v), v.name) for v in tape.watched_variables()])\n\n    # NOTE: This will run `f` twice iff `f` creates params.\n    f()\n\n    if len(logged_variables) == 1:\n      self.skipTest(""Module did not create variables in forward pass."")\n    else:\n      assert len(logged_variables) == 2\n      self.assertCountEqual(logged_variables[0], logged_variables[1])\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/conformance/pickle_test.py,1,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests pickling Sonnet modules.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport pickle\n\nfrom absl.testing import parameterized\nfrom sonnet.src import test_utils\nfrom sonnet.src.conformance import goldens\nimport tensorflow as tf\nimport tree\n\n\nclass PickleTest(test_utils.TestCase, parameterized.TestCase):\n\n  # TODO(tomhennigan) Add tests with dill and cloudpickle.\n\n  @goldens.all_goldens\n  def test_pickle(self, golden):\n    m1 = golden.create_module()\n    golden.create_all_variables(m1)\n    m2 = pickle.loads(pickle.dumps(m1))\n    self.assertIsNot(m1, m2)\n\n    # Check that module variables are recreated with equivalent properties.\n    for v1, v2 in zip(m1.variables, m2.variables):\n      self.assertIsNot(v1, v2)\n      self.assertEqual(v1.name, v2.name)\n      self.assertEqual(v1.device, v2.device)\n      self.assertAllEqual(v1.read_value(), v2.read_value())\n\n    if golden.deterministic:\n      y1 = golden.forward(m1)\n      y2 = golden.forward(m2)\n      tree.map_structure(self.assertAllEqual, y1, y2)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/conformance/saved_model_test.py,5,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests using tf.saved_model and Sonnet.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport sonnet as snt\nfrom sonnet.src import test_utils\nfrom sonnet.src.conformance import goldens\nimport tensorflow as tf\nimport tree\n\n\nclass SavedModelTest(test_utils.TestCase, parameterized.TestCase):\n\n  @goldens.all_goldens\n  def test_save_restore_cycle(self, golden):\n    module = golden.create_module()\n\n    # Create all parameters and set them to sequential (but different) values.\n    variables = golden.create_all_variables(module)\n    for index, variable in enumerate(variables):\n      variable.assign(goldens.range_like(variable, start=index))\n\n    @tf.function(input_signature=[golden.input_spec])\n    def inference(x):\n      # We\'ll let `golden.forward` run the model with a fixed input. This allows\n      # for additional positional arguments like is_training.\n      del x\n      return golden.forward(module)\n\n    # Create a saved model, add a method for inference and a dependency on our\n    # module such that it can find dependencies.\n    saved_model = snt.Module()\n    saved_model._module = module\n    saved_model.inference = inference\n    saved_model.all_variables = list(module.variables)\n\n    # Sample input, the value is not important (it is not used in the inference\n    # function).\n    x = goldens.range_like(golden.input_spec)\n\n    # Run the saved model and pull variable values.\n    saved_model.inference(x)\n    v1 = saved_model.all_variables\n\n    # Save the model to disk and restore it.\n    tmp_dir = os.path.join(absltest.get_default_test_tmpdir(), golden.name)\n    tf.saved_model.save(saved_model, tmp_dir)\n    restored_model = tf.saved_model.load(tmp_dir)\n\n    # Run the loaded model and pull variable values.\n    v2 = restored_model.all_variables\n    y2 = restored_model.inference(x)\n\n    if golden.deterministic:\n      # The output from both the saved and restored model should be close.\n      y1 = saved_model.inference(x)\n      tree.map_structure(self.assertAllEqual, y1, y2)\n\n    for a, b in zip(v1, v2):\n      self.assertEqual(a.name, b.name)\n      self.assertEqual(a.device, b.device)\n      self.assertAllEqual(a.read_value(), b.read_value())\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/conformance/tensorflow1_test.py,3,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n\n""""""Tests Sonnet 2 with TF1.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sonnet as snt\nfrom sonnet.src import test_utils\nimport tensorflow.compat.v1 as tf\n\n\nclass TensorFlow1Test(test_utils.TestCase):\n\n  def test_requires_tf2(self):\n    if tf.version.GIT_VERSION != ""unknown"":\n      self.skipTest(""This test only runs if testing against TF at head."")\n\n    with self.assertRaisesRegexp(AssertionError, ""requires TensorFlow 2""):\n      snt.Module()\n\nif __name__ == ""__main__"":\n  tf.disable_v2_behavior()\n  tf.test.main()\n'"
sonnet/src/conformance/xla_test.py,7,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests Sonnet and XLA.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport functools\n\nfrom absl.testing import parameterized\nfrom sonnet.src import test_utils\nfrom sonnet.src.conformance import goldens\nimport tensorflow as tf\nimport tree\n\n\nclass XLATest(test_utils.TestCase, parameterized.TestCase):\n\n  @goldens.all_goldens\n  def test_compile(self, golden):\n    mod = golden.create_module()\n    golden.create_all_variables(mod)\n\n    @tf.function\n    def forward():\n      f = lambda: golden.forward(mod)\n      out = tf.xla.experimental.compile(f)\n      if len(out) == 1:\n        return out[0]\n      else:\n        return out if out else None\n\n    if self.primary_device == ""TPU"":\n      # TODO(b/132329316) Remove when `xla.compile` allows tf.device(TPU).\n      with tf.device(None):\n        xla_out = forward()\n      atol = golden.tpu_atol\n    else:\n      xla_out = forward()\n      atol = 1e-3\n\n    if golden.deterministic and not golden.has_side_effects:\n      out = golden.forward(mod)\n      tree.map_structure(\n          functools.partial(self.assertAllClose, atol=atol), out, xla_out)\n\n  @goldens.all_goldens\n  def test_jit_scope(self, golden):\n    mod = golden.create_module()\n    golden.create_all_variables(mod)\n\n    @tf.function\n    def forward():\n      with tf.xla.experimental.jit_scope():\n        return golden.forward(mod)\n\n    xla_out = forward()\n    if self.primary_device == ""TPU"":\n      atol = golden.tpu_atol\n    else:\n      atol = 1e-3\n\n    if golden.deterministic and not golden.has_side_effects:\n      out = golden.forward(mod)\n      tree.map_structure(\n          functools.partial(self.assertAllClose, atol=atol), out, xla_out)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/distribute/batch_norm.py,8,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Distributed batch normalization module.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nfrom sonnet.src import batch_norm\nfrom sonnet.src import initializers\nfrom sonnet.src import metrics\nfrom sonnet.src import once\nfrom sonnet.src import types\n\nimport tensorflow as tf\nfrom typing import Optional, Text, Tuple\n\n\nclass CrossReplicaBatchNorm(batch_norm.BaseBatchNorm):\n  """"""Cross-replica Batch Normalization.\n\n  At every step the full batch is used to calculate the batch statistics even\n  within a distributed setting (note only with ``snt.(Tpu)Replicator``).\n\n  See :class:`BaseBatchNorm` for details.\n\n  Attributes:\n    scale: If ``create_scale=True``, a trainable :tf:`Variable` holding the\n      current scale after the module is connected for the first time.\n    offset: If ``create_offset``, a trainable :tf:`Variable` holding the current\n      offset after the module is connected for the first time.\n  """"""\n\n  def __init__(self,\n               create_scale: bool,\n               create_offset: bool,\n               moving_mean: metrics.Metric,\n               moving_variance: metrics.Metric,\n               eps: types.FloatLike = 1e-5,\n               scale_init: Optional[initializers.Initializer] = None,\n               offset_init: Optional[initializers.Initializer] = None,\n               data_format: Text = ""channels_last"",\n               name: Optional[Text] = None):\n    """"""Constructs a ``CrossReplicaBatchNorm`` module.\n\n    Args:\n      create_scale: whether to create a trainable scale per channel applied\n        after the normalization.\n      create_offset: whether to create a trainable offset per channel applied\n        after normalization and scaling.\n      moving_mean: An object which keeps track of the moving average of the mean\n        which can be used to normalize at test time. This object must have an\n        update method which takes a value and updates the internal state and a\n        value property which returns the current mean.\n      moving_variance: An object which keeps track of the moving average of the\n        variance which can be used to normalize at test time. This object must\n        have an update method which takes a value and updates the internal state\n        and a value property which returns the current variance.\n      eps: Small epsilon to avoid division by zero variance. Defaults to\n        ``1e-5``.\n      scale_init: Optional initializer for the scale variable. Can only be set\n        if ``create_scale=True``. By default scale is initialized to ``1``.\n      offset_init: Optional initializer for the offset variable. Can only be set\n        if ``create_offset=True``. By default offset is initialized to ``0``.\n      data_format: The data format of the input. Can be either\n        ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By\n        default it is ``channels_last``.\n      name: Name of the module.\n    """"""\n    super(CrossReplicaBatchNorm, self).__init__(\n        create_scale=create_scale,\n        create_offset=create_offset,\n        moving_mean=moving_mean,\n        moving_variance=moving_variance,\n        eps=eps,\n        scale_init=scale_init,\n        offset_init=offset_init,\n        data_format=data_format,\n        name=name)\n\n  @once.once\n  def _initialize(self, inputs: tf.Tensor):\n    super(CrossReplicaBatchNorm, self)._initialize(inputs)\n\n    # Always use the unfused op here as mean/var are calculated before the op is\n    # called so no speed-up is gained from the fused op\n    self._fused = False\n\n  def _moments(self, inputs: tf.Tensor,\n               use_batch_stats: types.BoolLike) -> Tuple[tf.Tensor, tf.Tensor]:\n    replica_context = tf.distribute.get_replica_context()\n    if replica_context is None:\n      raise TypeError(\n          ""Cross replica batch norm cannot be called in cross-replica context."")\n\n    if use_batch_stats:\n      # Note: This uses var=E(x^2) - E(x)^2 instead of the more numerically\n      # stable var=E((x-E(x))^2) as this means that with XLA the all_reduces can\n      # be combined and a fusion removed giving significant speed-up.\n      # If you see NaNs in your model please try the alternative formula and\n      # file a bug with your use-case.\n      mean = tf.reduce_mean(inputs, self._axis, keepdims=True)\n      mean = replica_context.all_reduce(""MEAN"", mean)\n      mean_of_squares = tf.reduce_mean(\n          tf.square(inputs), self._axis, keepdims=True)\n      mean_of_squares = replica_context.all_reduce(""MEAN"", mean_of_squares)\n      mean_squared = tf.square(mean)\n      var = mean_of_squares - mean_squared\n      return mean, var\n\n    else:  # use moving statistics\n      mean = self.moving_mean.value\n      variance = self.moving_variance.value\n      return mean, variance\n'"
sonnet/src/distribute/batch_norm_test.py,23,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.distribute.batch_norm.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import logging\n\nfrom absl.testing import parameterized\nfrom sonnet.src import test_utils\nfrom sonnet.src.distribute import batch_norm\nfrom sonnet.src.distribute import replicator\nimport tensorflow as tf\n\n\nclass CrossReplicaBatchNormTest(test_utils.TestCase, parameterized.TestCase):\n  # Avoid running tests inside a `with tf.device(""TPU:0""):` block.\n  ENTER_PRIMARY_DEVICE = False\n\n  def testDefaultReplicaContext(self):\n    layer = batch_norm.CrossReplicaBatchNorm(False, False, TestMetric(),\n                                             TestMetric())\n\n    inputs = tf.ones([2, 3, 3, 5])\n    scale = tf.constant(0.5, shape=(5,))\n    offset = tf.constant(2.0, shape=(5,))\n\n    outputs = layer(inputs, True, scale=scale, offset=offset).numpy()\n    self.assertAllEqual(outputs, tf.fill(inputs.shape, 2.0))\n\n  def testWithMultipleDevicesMirrored(self):\n    if self.primary_device == ""CPU"":\n      self.skipTest(""No devices to mirror across."")\n    elif self.primary_device == ""GPU"":\n      devices = tf.config.experimental.list_logical_devices(""GPU"")\n    else:\n      devices = tf.config.experimental.list_logical_devices(""TPU"")\n\n    strategy = replicator.Replicator([device.name for device in devices])\n    with strategy.scope():\n      mean_metric = TestMetric()\n      var_metric = TestMetric()\n      layer = batch_norm.CrossReplicaBatchNorm(False, False, mean_metric,\n                                               var_metric)\n\n    scale = tf.constant(0.5, shape=(5,))\n    offset = tf.constant(2.0, shape=(5,))\n\n    def foo():\n      inputs = tf.random.normal([2, 3, 3, 5])\n      outputs = layer(inputs, True, False, scale, offset)\n      return inputs, outputs\n\n    inputs, outputs = strategy.run(foo)\n    local_mean_metric = strategy.experimental_local_results(mean_metric.value)\n    local_var_metric = strategy.experimental_local_results(var_metric.value)\n    self.assertAllEqual(local_mean_metric[0].numpy(),\n                        local_mean_metric[1].numpy())\n    self.assertAllEqual(local_var_metric[0].numpy(),\n                        local_var_metric[1].numpy())\n    mean = local_mean_metric[0]\n    var = local_var_metric[0]\n\n    for inp, out in zip(\n        strategy.experimental_local_results(inputs),\n        strategy.experimental_local_results(outputs)):\n      expected_out = (inp - mean) * tf.math.rsqrt(var + 1e-5) * scale + offset\n      self.assertAllClose(out, expected_out)\n\n  def testWithTpuStrategy(self):\n    if self.primary_device != ""TPU"":\n      self.skipTest(""TPU strategy only runs on TPU\'s"")\n\n    strategy = replicator.TpuReplicator()\n    with strategy.scope():\n      mean_metric = TestMetric()\n      var_metric = TestMetric()\n      layer = batch_norm.CrossReplicaBatchNorm(False, False,\n                                               mean_metric, var_metric)\n    scale = tf.constant(0.5, shape=(5,))\n    offset = tf.constant(2.0, shape=(5,))\n\n    @tf.function\n    def run():\n      def compute():\n        inputs = tf.ones([2, 3, 3, 5])\n        outputs = layer(inputs, True, False, scale, offset)\n        return inputs, outputs\n\n      return strategy.run(compute)\n    inputs, outputs = run()\n\n    local_mean_metric = strategy.experimental_local_results(mean_metric.value)\n    local_var_metric = strategy.experimental_local_results(var_metric.value)\n    self.assertAllEqual(local_mean_metric[0].numpy(),\n                        local_mean_metric[1].numpy())\n    self.assertAllEqual(local_var_metric[0].numpy(),\n                        local_var_metric[1].numpy())\n    mean = local_mean_metric[0]\n    var = local_var_metric[0]\n\n    for inp, out in zip(\n        strategy.experimental_local_results(inputs),\n        strategy.experimental_local_results(outputs)):\n      expected_out = (inp - mean) * tf.math.rsqrt(var + 1e-5) * scale + offset\n      self.assertAllClose(out, expected_out)\n\n\nclass TestMetric(object):\n\n  def __init__(self):\n    self._foo = None\n    self._built = False\n\n  def update(self, x):\n    if self._built:\n      self._foo.assign(x)\n    else:\n      self._foo = tf.Variable(x)\n      self._built = True\n\n  @property\n  def value(self):\n    return self._foo\n\n  def initialize(self, x):\n    self._foo = tf.Variable(x)\n    self._built = True\n\n\ndef setUpModule():\n  # If a physical GPU is available make sure TF sees at least two.\n  gpus = tf.config.experimental.list_physical_devices(device_type=""GPU"")\n  if len(gpus) == 1:\n    logging.info(""Splitting one physical GPU into two logical GPUs."")\n    tf.config.experimental.set_virtual_device_configuration(\n        gpus[0], [\n            tf.config.experimental.VirtualDeviceConfiguration(\n                memory_limit=1024),\n            tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)\n        ])\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/distribute/replicator.py,22,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Replicator Distribution Strategy.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nfrom absl import logging\nimport contextlib\nfrom sonnet.src import initializers\nimport tensorflow as tf\nfrom typing import Callable, TypeVar\n\nT = TypeVar(""T"")\n\n\ndef replica_local_creator(next_creator, **kwargs) -> tf.Variable:\n  """"""Variable creator that by default creates replica local variables.""""""\n  if kwargs[""synchronization""] == tf.VariableSynchronization.AUTO:\n    kwargs[""synchronization""] = tf.VariableSynchronization.ON_READ\n    if kwargs[""aggregation""] == tf.VariableAggregation.NONE:\n      kwargs[""aggregation""] = tf.VariableAggregation.ONLY_FIRST_REPLICA\n    if kwargs[""trainable""] is None:\n      kwargs[""trainable""] = True\n  return next_creator(**kwargs)\n\n\nclass Replicator(tf.distribute.MirroredStrategy):\n  r""""""Replicates input, parameters and compute over multiple accelerators.\n\n  ``Replicator`` is a TensorFlow ""Distribution Strategy"" implementing the\n  programming model described in the TF-Replicator paper\n  :cite:`buchlovsky2019tf` and TensorFlow RFC\n  :cite:`buchlovsky2019distribution`. ``Replicator`` enables data-parallel\n  training across multiple accelerators on a single machine, it supports\n  eager execution and :tf:`function`.\n\n  To get started create a ``Replicator`` instance:\n\n      >>> replicator = snt.distribute.Replicator()\n\n  Replicator provides a scope inside which any new :tf:`Variable`\\ s will be\n  replicated across all local devices:\n\n      >>> with replicator.scope():\n      ...    mod = snt.Linear(32)\n\n  Additionally replicator provides utility functions to apply a module in\n  parallel on multiple devices. First we need to define some computation that\n  runs on each GPU. The ""replica context"" object provides us a way to\n  communicate between replicas (e.g. to perform an ``all_reduce``):\n\n      >>> def forward():\n      ...   # Compute a random output on each GPU.\n      ...   x = tf.random.normal([8, 28 * 28])\n      ...   y = mod(x)\n      ...   # Synchronize the value of `y` between all GPUs.\n      ...   ctx = tf.distribute.get_replica_context()\n      ...   y = ctx.all_reduce(""mean"", y)\n      ...   return y\n\n  Finally we use the run API to apply ``forward`` in parallel on all accelerator\n  devices:\n\n      >>> per_replica_y = replicator.run(forward)\n  """"""\n\n  @contextlib.contextmanager\n  def scope(self):\n    with contextlib.ExitStack() as stack:\n      stack.enter_context(super(Replicator, self).scope())\n      stack.enter_context(tf.variable_creator_scope(replica_local_creator))\n      yield\n\n\nclass TpuReplicator(tf.distribute.experimental.TPUStrategy):\n  r""""""Replicates input, parameters and compute over multiple TPUs.\n\n  ``TpuReplicator`` is a TensorFlow ""Distribution Strategy"" implementing the\n  programming model described in the TF-Replicator paper\n  :cite:`buchlovsky2019tf` and TensorFlow RFC\n  :cite:`buchlovsky2019distribution`. ``TpuReplicator`` enables data-parallel\n  training across multiple TPUs on one or more machines, it supports\n  :tf:`function`.\n\n  To get started create a ``TpuReplicator`` instance:\n\n      >>> replicator = snt.distribute.TpuReplicator()\n\n  This provides a scope inside which any new :tf:`Variable`\\ s will be\n  replicated across all TPU cores:\n\n      >>> with replicator.scope():\n      ...    mod = snt.Linear(32)\n\n  Additionally replicator provides utility functions to apply a module in\n  parallel on multiple devices. First we need to define some computation that\n  runs on each TPU. The ""replica context"" object provides us a way to\n  communicate between replicas:\n\n      >>> def forward():\n      ...   # Compute a random output on each GPU.\n      ...   x = tf.random.normal([8, 28 * 28])\n      ...   y = mod(x)\n      ...   # Synchronize the value of `y` between all GPUs.\n      ...   ctx = tf.distribute.get_replica_context()\n      ...   y = ctx.all_reduce(""mean"", y)\n      ...   return y\n\n  Finally we use the run API to apply ``forward`` in parallel on all TPU\n  devices. This must be run as part of a :tf:`function` since ``TpuReplicator``\n  uses XLA to compile and replicate our function to run in parallel over all\n  TPU cores:\n\n      >>> @tf.function(autograph=False)\n      ... def all_forward():\n      ...   return replicator.run(forward)\n      >>> per_replica_y = all_forward()\n  """"""\n\n  @contextlib.contextmanager\n  def scope(self):\n    with contextlib.ExitStack() as stack:\n      stack.enter_context(super(TpuReplicator, self).scope())\n      stack.enter_context(tf.variable_creator_scope(replica_local_creator))\n      yield\n\n\ndef create_variables_eagerly(f: Callable[..., T]) -> Callable[..., T]:\n  """"""Wraps a function and attempts to create variables using eager mode.\n\n  Example usage:\n\n  >>> model = snt.Sequential([snt.Linear(1) for _ in range(100)])\n\n  >>> @tf.function\n  ... @snt.distribute.create_variables_eagerly\n  ... def f(x):\n  ...   return model(x)\n\n  >>> _ = f(tf.ones([1, 1]))\n\n  On a CPU only machine ``f`` will run ~4x faster (700ms vs. 3s), the benefits\n  are more pronounced in a distributed setup since eager variable creation can\n  skip a number of checks that are required in graph mode (e.g. checking whether\n  the variable has already been created) which end up ping-ponging RPCs.\n\n  Args:\n    f: Any function.\n\n  Returns:\n    A function running `f` in a context where variables are created eagerly.\n  """"""\n  def wrapper(*args, **kwargs):\n    with contextlib.ExitStack() as stack:\n      # The two hacks below enable a large speedup when initializing large\n      # models on TPU pods.\n      # TODO(b/141243467) Remove these workarounds.\n      stack.enter_context(_eager_initial_values())\n      stack.enter_context(tf.variable_creator_scope(_eager_variable_creator))\n      return f(*args, **kwargs)\n  return wrapper\n\n\ndef _eager_variable_creator(getter, initial_value, **kwargs):\n  """"""Attempts to force variable creation to be eager.""""""\n  eager_initial_value = None\n\n  if isinstance(initial_value, tf.Tensor):\n    eager_initial_value = tf.get_static_value(initial_value)\n\n  if eager_initial_value is not None:\n    # If we have an eager initial value we can create variables in eager mode.\n    with tf.init_scope():\n      return getter(initial_value=eager_initial_value, **kwargs)\n\n  else:\n    # Fall back to creating in whatever context we\'re in with user input.\n    return getter(initial_value=initial_value, **kwargs)\n\n\n@contextlib.contextmanager\ndef _eager_initial_values():\n  """"""Attempts to force all initializers to create eager tensors.""""""\n  all_initializers = {cls: cls.__call__\n                      for cls in initializers.Initializer.__subclasses__()}\n\n  def patched_call(self, shape, dtype):\n    """"""Monkey-patched verison of `Initializer.__call__`.""""""\n    cls = type(self)\n    orig_call = all_initializers[cls]\n    try:\n      with tf.init_scope():\n        return orig_call(self, shape, dtype)\n    except:  # pylint: disable=bare-except\n      if not tf.executing_eagerly():\n        logging.exception(\n            ""Failed to create initial value eagerly for %s shape=%s dtype=%s"",\n            type(self).__name__, shape, dtype)\n      return orig_call(self, shape, dtype)\n\n  try:\n    for cls in all_initializers:\n      cls.__call__ = patched_call\n    yield\n\n  finally:\n    # Restore\n    for cls, orig_call in all_initializers.items():\n      cls.__call__ = orig_call\n'"
sonnet/src/distribute/replicator_test.py,33,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.replicator.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl import logging\nfrom absl.testing import parameterized\nfrom sonnet.src import initializers\nfrom sonnet.src import test_utils\nfrom sonnet.src.distribute import replicator as snt_replicator\nfrom sonnet.src.distribute import replicator_test_utils as replicator_utils\nimport tensorflow as tf\n\n\ndef _create_variable_in_cross_replica_context(replicator):\n  with replicator.scope():\n    v = tf.Variable(1.)\n  return v\n\n\nclass TrainableVariable(object):\n\n  def __call__(self):\n    if not hasattr(self, ""v""):\n      self.v = tf.Variable(1.)\n    return self.v\n\n\ndef _create_variable_in_replica_context(replicator):\n  o = TrainableVariable()\n\n  def create_var():\n    replicator.run(o)\n\n  # TpuReplicator doesn\'t support pure eager mode.\n  if isinstance(replicator, snt_replicator.TpuReplicator):\n    create_var = tf.function(create_var)\n\n  create_var()\n  return o.v\n\n\ndef all_variable_creators():\n  return ((""cross_replica_context"", _create_variable_in_cross_replica_context),\n          (""replica_context"", _create_variable_in_replica_context))\n\n\nclass ReplicatorTest(test_utils.TestCase, parameterized.TestCase):\n\n  # Avoid running tests inside a `with tf.device(""TPU:0""):` block.\n  ENTER_PRIMARY_DEVICE = False\n\n  @test_utils.combined_named_parameters(replicator_utils.named_replicators(),\n                                        all_variable_creators())\n  def test_variable_synchronization_default(self, replicator_fn, create_var):\n    replicator = replicator_fn()\n    if replicator is None:\n      self.skipTest(""No replicator supplied."")\n    v = create_var(replicator)\n    self.assertEqual(tf.VariableSynchronization.ON_READ,\n                     v.values[0].synchronization)\n\n  @test_utils.combined_named_parameters(replicator_utils.named_replicators(),\n                                        all_variable_creators())\n  def test_variable_aggregation_default(self, replicator_fn, create_var):\n    replicator = replicator_fn()\n    if replicator is None:\n      self.skipTest(""No replicator supplied."")\n    v = create_var(replicator)\n    self.assertEqual(tf.VariableAggregation.ONLY_FIRST_REPLICA, v.aggregation)\n\n  @test_utils.combined_named_parameters(replicator_utils.named_replicators(),\n                                        all_variable_creators())\n  def test_variable_trainable_default(self, replicator_fn, create_var):\n    replicator = replicator_fn()\n    if replicator is None:\n      self.skipTest(""No replicator supplied."")\n    v = create_var(replicator)\n    self.assertTrue(v.trainable)\n\n  @test_utils.combined_named_parameters(replicator_utils.named_replicators(),\n                                        test_utils.named_bools(""trainable""))\n  def test_variable_trainable(self, replicator_fn, trainable):\n    replicator = replicator_fn()\n    if replicator is None:\n      self.skipTest(""No replicator supplied."")\n    with replicator.scope():\n      v = tf.Variable(1., trainable=trainable)\n    self.assertEqual(trainable, v.trainable)\n\n  @test_utils.combined_named_parameters(replicator_utils.named_replicators(),\n                                        ((""assign"", ""assign"", 1.),\n                                         (""assign_add"", ""assign_add"", 1.),\n                                         (""assign_sub"", ""assign_sub"", -1.)),\n                                        test_utils.named_bools(""cross_replica""))\n  def test_assign(self, replicator_fn, method_name, value, cross_replica):\n    replicator = replicator_fn()\n    if replicator is None:\n      self.skipTest(""No replicator supplied."")\n\n    with replicator.scope():\n      v = tf.Variable(0.)\n    update_fn = lambda: getattr(v, method_name)(value)\n    if cross_replica:\n      # NOTE: Explicitly not running inside replicator.scope (fn should handle).\n      update_fn()\n    else:\n      # TpuReplicator doesn\'t support pure eager mode.\n      if isinstance(replicator, snt_replicator.TpuReplicator):\n        update_fn = tf.function(update_fn)\n      replicator.run(update_fn)\n    for component in v._values:\n      self.assertAllEqual(component.read_value(), tf.ones_like(component))\n\n  @test_utils.combined_named_parameters(replicator_utils.named_replicators(),\n                                        test_utils.named_bools(""cross_replica""))\n  def test_read_value(self, replicator_fn, cross_replica):\n    replicator = replicator_fn()\n    if replicator is None:\n      self.skipTest(""No replicator supplied."")\n\n    with replicator.scope():\n      v = tf.Variable(0.)\n    if cross_replica:\n      values = [v.read_value()]\n    else:\n      # TpuReplicator doesn\'t support pure eager mode.\n      if isinstance(replicator, snt_replicator.TpuReplicator):\n        read_value_fn = tf.function(v.read_value)\n      else:\n        read_value_fn = v.read_value\n      values = replicator.run(read_value_fn)\n      values = replicator.experimental_local_results(values)\n    for component in v._values:\n      for value in values:\n        self.assertAllEqual(component.read_value(), value)\n\n  @parameterized.parameters(True, False)\n  def test_falls_back_to_graph(self, autograph):\n    init = FailsInEagerMode()\n    value = tf.function(\n        snt_replicator.create_variables_eagerly(\n            lambda: init([], tf.float32)), autograph=autograph)()\n    self.assertEqual(value.numpy(), 1.)\n\n  @parameterized.parameters(True, False)\n  def test_requires_eager(self, autograph):\n    init = MyOnesInitializer()\n    value = tf.function(\n        snt_replicator.create_variables_eagerly(\n            lambda: init([], tf.float32)), autograph=autograph)()\n    self.assertEqual(value.numpy(), 1.)\n\n  @parameterized.parameters(True, False)\n  def test_eager_variable_creator(self, autograph):\n    variables = [None, None]\n    eager_ones = tf.ones([])\n\n    @snt_replicator.create_variables_eagerly\n    def f():\n      if variables[0] is None:\n        graph_ones = tf.ones([])\n        # NOTE: `graph_ones` will be resolved by `tf.get_static_value`.\n        v1 = tf.Variable(graph_ones)\n        v2 = tf.Variable(eager_ones)\n        # Even though we\'re in a tf.function here, eager_variable_creator\n        # should have popped us into an init_scope so we have eager variables.\n        with tf.init_scope():\n          self.assertEqual(v1.numpy(), 1.)\n          self.assertEqual(v2.numpy(), 1.)\n        variables[0] = v1\n        variables[1] = v2\n\n    tf.function(f, autograph=autograph)()\n\n\nclass MyOnesInitializer(initializers.Initializer):\n\n  def __call__(self, shape, dtype):\n    assert tf.executing_eagerly()\n    return tf.ones(shape, dtype)\n\n\nclass FailsInEagerMode(initializers.Initializer):\n\n  def __call__(self, shape, dtype):\n    if tf.executing_eagerly():\n      raise ValueError(""Eager mode"")\n    return tf.ones(shape, dtype)\n\n\ndef setUpModule():\n  # If a physical GPU is available make sure TF sees at least two.\n  gpus = tf.config.experimental.list_physical_devices(device_type=""GPU"")\n  if len(gpus) == 1:\n    logging.info(""Splitting one physical GPU into two logical GPUs."")\n    tf.config.experimental.set_virtual_device_configuration(\n        gpus[0], [\n            tf.config.experimental.VirtualDeviceConfiguration(\n                memory_limit=1024),\n            tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)\n        ])\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/distribute/replicator_test_utils.py,3,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Utilities for tests working with replicator.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport unittest\n\nfrom absl import logging\nfrom sonnet.src.distribute import replicator as snt_replicator\nimport tensorflow as tf\nfrom typing import Callable, Sequence, Text, Tuple\n\n\ndef _replicator_primary_device() -> snt_replicator.Replicator:\n  # NOTE: The explicit device list is required since currently Replicator\n  # only considers CPU and GPU devices. This means on TPU by default we only\n  # mirror on the local CPU.\n  for device_type in (""TPU"", ""GPU"", ""CPU""):\n    devices = tf.config.experimental.list_logical_devices(\n        device_type=device_type)\n    if devices:\n      devices = [d.name for d in devices]\n      logging.info(""Replicating over %s"", devices)\n      return snt_replicator.Replicator(devices=devices)\n\n  assert False, ""No TPU/GPU or CPU found""\n\n\ndef _tpu_replicator_or_skip_test() -> snt_replicator.TpuReplicator:\n  tpus = tf.config.experimental.list_logical_devices(device_type=""TPU"")\n  if not tpus:\n    raise unittest.SkipTest(""No TPU available."")\n\n  logging.info(""Using TpuReplicator over %s"", [t.name for t in tpus])\n  return snt_replicator.TpuReplicator()\n\n\nStrategy = tf.distribute.Strategy\n\n\ndef named_replicators() -> Sequence[Tuple[Text, Callable[[], Strategy]]]:\n  return ((""TpuReplicator"", _tpu_replicator_or_skip_test),\n          (""Replicator"", _replicator_primary_device))\n'"
sonnet/src/nets/cifar10_convnet.py,5,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Convnet module for Cifar10 classification.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nfrom sonnet.src import base\nfrom sonnet.src import batch_norm\nfrom sonnet.src import conv\nfrom sonnet.src import initializers\nfrom sonnet.src import linear\nfrom sonnet.src import types\n\nimport tensorflow as tf\nfrom typing import Mapping, Optional, Sequence, Text, Union\n\n\nclass Cifar10ConvNet(base.Module):\n  """"""Convolutional network designed for Cifar10.\n\n  Approximately equivalent to ""VGG, minus max pooling, plus BatchNorm"". For best\n  results the input data should be scaled to be between -1 and 1 when using the\n  standard initializers.\n  """"""\n\n  def __init__(self,\n               num_classes: int = 10,\n               w_init: Optional[initializers.Initializer] = None,\n               b_init: Optional[initializers.Initializer] = None,\n               data_format: Text = \'NHWC\',\n               output_channels: Sequence[int] = (\n                   64,\n                   64,\n                   128,\n                   128,\n                   128,\n                   256,\n                   256,\n                   256,\n                   512,\n                   512,\n                   512,\n               ),\n               strides: Sequence[int] = (1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1),\n               name: Optional[Text] = None):\n    super(Cifar10ConvNet, self).__init__(name=name)\n    self._num_classes = num_classes\n    self._data_format = data_format\n    if len(strides) != len(output_channels):\n      raise ValueError(\n          \'The length of `output_channels` and `strides` must be equal.\')\n    self._output_channels = output_channels\n    self._strides = strides\n    self._num_layers = len(self._output_channels)\n    self._kernel_shapes = [[3, 3]] * self._num_layers  # All kernels are 3x3.\n    self._w_init = w_init\n    self._b_init = b_init\n\n    self._conv_modules = list(\n        conv.Conv2D(  # pylint: disable=g-complex-comprehension\n            output_channels=self._output_channels[i],\n            kernel_shape=self._kernel_shapes[i],\n            stride=self._strides[i],\n            w_init=self._w_init,\n            b_init=self._b_init,\n            data_format=self._data_format,\n            name=\'conv_2d_{}\'.format(i)) for i in range(self._num_layers))\n    self._bn_modules = list(\n        batch_norm.BatchNorm(  # pylint: disable=g-complex-comprehension\n            create_offset=True,\n            create_scale=False,\n            decay_rate=0.999,\n            data_format=self._data_format,\n            name=\'batch_norm_{}\'.format(i)) for i in range(self._num_layers))\n    self._logits_module = linear.Linear(\n        self._num_classes,\n        w_init=self._w_init,\n        b_init=self._b_init,\n        name=\'logits\')\n\n  def __call__(\n      self,\n      inputs: tf.Tensor,\n      is_training: types.BoolLike,\n      test_local_stats: bool = True\n  ) -> Mapping[Text, Union[tf.Tensor, Sequence[tf.Tensor]]]:\n    """"""Connects the module to some inputs.\n\n    Args:\n      inputs: A Tensor of size [batch_size, input_height, input_width,\n        input_channels], representing a batch of input images.\n      is_training: Boolean to indicate to `snt.BatchNorm` if we are currently\n        training.\n      test_local_stats: Boolean to indicate to `snt.BatchNorm` if batch\n        normalization should  use local batch statistics at test time. By\n        default `True`.\n\n    Returns:\n      A dictionary containing two items:\n      - logits: The output logits of the network, this will be of size\n        [batch_size, num_classes]\n      - activations: A list of `tf.Tensor`, the feature activations of the\n        module. The order of the activations is preserved in the output list.\n        The activations in the output list are those computed after the\n        activation function is applied, if one is applied at that layer.\n    """"""\n    activations = []\n    net = inputs\n    for conv_layer, bn_layer in zip(self._conv_modules, self._bn_modules):\n      net = conv_layer(net)\n      net = bn_layer(\n          net, is_training=is_training, test_local_stats=test_local_stats)\n      net = tf.nn.relu(net)\n      activations.append(net)\n\n    flat_output = tf.reduce_mean(\n        net, axis=[1, 2], keepdims=False, name=\'avg_pool\')\n    activations.append(flat_output)\n\n    logits = self._logits_module(flat_output)\n\n    return {\'logits\': logits, \'activations\': activations}\n'"
sonnet/src/nets/cifar10_convnet_test.py,13,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.nets.cifar10_convnet.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom sonnet.src import test_utils\nfrom sonnet.src.nets import cifar10_convnet\nimport tensorflow as tf\n\n\nclass ModelTest(parameterized.TestCase, test_utils.TestCase):\n\n  def testModelCreation(self):\n    convnet = cifar10_convnet.Cifar10ConvNet()\n\n    self.assertLen(convnet.submodules, 45)\n\n  def testFailedModelCreation(self):\n    with self.assertRaisesRegexp(\n        ValueError,\n        \'The length of `output_channels` and `strides` must be equal.\'):\n      cifar10_convnet.Cifar10ConvNet(strides=(1, 2, 3), output_channels=(1,))\n\n  @parameterized.parameters({\'batch_size\': 1}, {\'batch_size\': 4},\n                            {\'batch_size\': 128})\n  def testModelForwards(self, batch_size):\n    image_batch = tf.constant(\n        np.random.randn(batch_size, 24, 24, 3), dtype=tf.float32)\n\n    convnet = cifar10_convnet.Cifar10ConvNet()\n    output = convnet(image_batch, is_training=True)\n    self.assertLen(convnet.variables, 112)\n    self.assertEqual(output[\'logits\'].shape, [batch_size, 10])\n    # One intermediate activation per conv layer, plus one after the global\n    # mean pooling, before the linear.\n    self.assertLen(output[\'activations\'], 12)\n\n  @parameterized.parameters({\'batch_size\': 1}, {\'batch_size\': 4},\n                            {\'batch_size\': 128})\n  def testModelForwardsFunction(self, batch_size):\n    image_batch = tf.constant(\n        np.random.randn(batch_size, 24, 24, 3), dtype=tf.float32)\n\n    convnet = cifar10_convnet.Cifar10ConvNet()\n    convnet_function = tf.function(convnet)\n    output = convnet_function(image_batch, is_training=True)\n    self.assertLen(convnet.variables, 112)\n    self.assertEqual(output[\'logits\'].shape, [batch_size, 10])\n    # One intermediate activation per conv layer, plus one after the global\n    # mean pooling, before the linear.\n    self.assertLen(output[\'activations\'], 12)\n\n  def testDifferentSizedImages(self):\n    # Due to global average pooling, different sized images should work fine\n    # as long they are above some minimum size.\n    convnet = cifar10_convnet.Cifar10ConvNet()\n\n    small_image = tf.constant(np.random.randn(4, 32, 32, 3), dtype=tf.float32)\n    small_output = convnet(small_image, is_training=True)\n    self.assertEqual(small_output[\'logits\'].shape, [4, 10])\n\n    # Change height, width and batch size\n    big_image = tf.constant(np.random.randn(12, 64, 64, 3), dtype=tf.float32)\n    big_output = convnet(big_image, is_training=True)\n    self.assertEqual(big_output[\'logits\'].shape, [12, 10])\n\n  def testDefunBackProp(self):\n\n    convnet = cifar10_convnet.Cifar10ConvNet()\n\n    @tf.function\n    def do_training_step(image, labels):\n      with tf.GradientTape() as tape:\n        logits = convnet(image, is_training=True)[\'logits\']\n        loss = tf.reduce_mean(\n            tf.nn.sparse_softmax_cross_entropy_with_logits(\n                logits=logits, labels=labels))\n      grads = tape.gradient(loss, convnet.trainable_variables)\n      return loss, grads\n\n    image = tf.random.normal([4, 32, 32, 3])\n    labels = np.random.randint(low=0, high=10, size=[4], dtype=np.int64)\n    loss, grads = do_training_step(image, labels)\n    self.assertEqual(loss.numpy().shape, ())\n    for grad, var in zip(grads, convnet.trainable_variables):\n      self.assertIsNotNone(grad)\n      self.assertEqual(grad.numpy().shape, var.shape)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
sonnet/src/nets/mlp.py,5,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""A minimal interface mlp module.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nfrom sonnet.src import base\nfrom sonnet.src import initializers\nfrom sonnet.src import linear\n\nimport tensorflow as tf\nfrom typing import Callable, Iterable, Optional, Text\n\n\nclass MLP(base.Module):\n  """"""A multi-layer perceptron module.""""""\n\n  def __init__(self,\n               output_sizes: Iterable[int],\n               w_init: Optional[initializers.Initializer] = None,\n               b_init: Optional[initializers.Initializer] = None,\n               with_bias: bool = True,\n               activation: Callable[[tf.Tensor], tf.Tensor] = tf.nn.relu,\n               dropout_rate=None,\n               activate_final: bool = False,\n               name: Optional[Text] = None):\n    """"""Constructs an MLP.\n\n    Args:\n      output_sizes: Sequence of layer sizes.\n      w_init: Initializer for Linear weights.\n      b_init: Initializer for Linear bias. Must be `None` if `with_bias` is\n        `False`.\n      with_bias: Whether or not to apply a bias in each layer.\n      activation: Activation function to apply between linear layers. Defaults\n        to ReLU.\n      dropout_rate: Dropout rate to apply, a rate of `None` (the default) or `0`\n        means no dropout will be applied.\n      activate_final: Whether or not to activate the final layer of the MLP.\n      name: Optional name for this module.\n\n    Raises:\n      ValueError: If with_bias is False and b_init is not None.\n    """"""\n    if not with_bias and b_init is not None:\n      raise ValueError(""When with_bias=False b_init must not be set."")\n\n    super(MLP, self).__init__(name=name)\n    self._with_bias = with_bias\n    self._w_init = w_init\n    self._b_init = b_init\n    self._activation = activation\n    self._activate_final = activate_final\n    self._dropout_rate = dropout_rate\n    self._layers = []\n    for index, output_size in enumerate(output_sizes):\n      self._layers.append(\n          linear.Linear(\n              output_size=output_size,\n              w_init=w_init,\n              b_init=b_init,\n              with_bias=with_bias,\n              name=""linear_%d"" % index))\n\n  def __call__(self, inputs: tf.Tensor, is_training=None) -> tf.Tensor:\n    """"""Connects the module to some inputs.\n\n    Args:\n      inputs: A Tensor of shape `[batch_size, input_size]`.\n      is_training: A bool indicating if we are currently training. Defaults to\n        `None`. Required if using dropout.\n\n    Returns:\n      output: The output of the model of size `[batch_size, output_size]`.\n    """"""\n    use_dropout = self._dropout_rate not in (None, 0)\n    if use_dropout and is_training is None:\n      raise ValueError(\n          ""The `is_training` argument is required when dropout is used."")\n    elif not use_dropout and is_training is not None:\n      raise ValueError(\n          ""The `is_training` argument should only be used with dropout."")\n\n    num_layers = len(self._layers)\n\n    for i, layer in enumerate(self._layers):\n      inputs = layer(inputs)\n      if i < (num_layers - 1) or self._activate_final:\n        # Only perform dropout if we are activating the output.\n        if use_dropout and is_training:\n          inputs = tf.nn.dropout(inputs, rate=self._dropout_rate)\n        inputs = self._activation(inputs)\n\n    return inputs\n\n  def reverse(self,\n              activate_final: Optional[bool] = None,\n              name: Optional[Text] = None) -> ""MLP"":\n    """"""Returns a new MLP which is the layer-wise reverse of this MLP.\n\n    NOTE: Since computing the reverse of an MLP requires knowing the input size\n    of each linear layer this method will fail if the module has not been called\n    at least once. See `snt.Deferred` as a possible solution to this problem.\n\n    The contract of reverse is that the reversed module will accept the output\n    of the parent module as input and produce an output which is the input size\n    of the parent.\n\n    >>> mlp = snt.nets.MLP([1, 2, 3])\n    >>> y = mlp(tf.ones([1, 2]))\n    >>> rev = mlp.reverse()\n    >>> rev(y)\n    <tf.Tensor: shape=(1, 2), ...>\n\n    Args:\n      activate_final: Whether the final layer of the MLP should be activated.\n      name: Optional name for the new module. The default name will be the name\n        of the current module prefixed with ``""reversed_""``.\n\n    Returns:\n      An MLP instance which is the reverse of the current instance. Note these\n      instances do not share weights and, apart from being symmetric to each\n      other, are not coupled in any way.\n    """"""\n\n    if activate_final is None:\n      activate_final = self._activate_final\n    if name is None:\n      name = self.name + ""_reversed""\n\n    return MLP(\n        output_sizes=(layer.input_size for layer in reversed(self.submodules)),\n        w_init=self._w_init,\n        b_init=self._b_init,\n        with_bias=self._with_bias,\n        activation=self._activation,\n        dropout_rate=self._dropout_rate,\n        activate_final=activate_final,\n        name=name)\n'"
sonnet/src/nets/mlp_test.py,13,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.nets.mlp.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport itertools\nfrom absl.testing import parameterized\nfrom sonnet.src import test_utils\nfrom sonnet.src.nets import mlp\nimport tensorflow as tf\n\n\nclass MLPTest(test_utils.TestCase, parameterized.TestCase):\n\n  def test_b_init_when_with_bias_false(self):\n    with self.assertRaisesRegexp(ValueError, ""b_init must not be set""):\n      mlp.MLP([1], with_bias=False, b_init=object())\n\n  @parameterized.parameters(itertools.product((1, 2, 3), (0.1, 0.0, None)))\n  def test_submodules(self, num_layers, dropout_rate):\n    mod = mlp.MLP([1] * num_layers, dropout_rate=dropout_rate)\n    self.assertLen(mod.submodules, num_layers)\n\n  @parameterized.parameters(1, 2, 3)\n  def test_applies_activation(self, num_layers):\n    activation = CountingActivation()\n    mod = mlp.MLP([1] * num_layers, activation=activation)\n    mod(tf.ones([1, 1]))\n    self.assertEqual(activation.count, num_layers - 1)\n\n  @parameterized.parameters(1, 2, 3)\n  def test_activate_final(self, num_layers):\n    activation = CountingActivation()\n    mod = mlp.MLP([1] * num_layers, activate_final=True, activation=activation)\n    mod(tf.ones([1, 1]))\n    self.assertEqual(activation.count, num_layers)\n\n  @parameterized.parameters(1, 2, 3)\n  def test_adds_index_to_layer_names(self, num_layers):\n    mod = mlp.MLP([1] * num_layers)\n    for index, linear in enumerate(mod.submodules):\n      self.assertEqual(linear.name, ""linear_%d"" % index)\n\n  @parameterized.parameters(False, True)\n  def test_passes_with_bias_to_layers(self, with_bias):\n    mod = mlp.MLP([1, 1, 1], with_bias=with_bias)\n    for linear in mod.submodules:\n      self.assertEqual(linear.with_bias, with_bias)\n\n  def test_repeat_initializer(self):\n    w_init = CountingInitializer()\n    b_init = CountingInitializer()\n    mod = mlp.MLP([1, 1, 1], w_init=w_init, b_init=b_init)\n    mod(tf.ones([1, 1]))\n    self.assertEqual(w_init.count, 3)\n    self.assertEqual(b_init.count, 3)\n\n  def test_default_name(self):\n    mod = mlp.MLP([1])\n    self.assertEqual(mod.name, ""mlp"")\n\n  def test_custom_name(self):\n    mod = mlp.MLP([1], name=""foobar"")\n    self.assertEqual(mod.name, ""foobar"")\n\n  def test_reverse_default_name(self):\n    mod = reversed_mlp()\n    self.assertEqual(mod.name, ""mlp_reversed"")\n\n  def test_reverse_custom_name(self):\n    mod = reversed_mlp(name=""foobar"")\n    self.assertEqual(mod.name, ""foobar_reversed"")\n\n  def test_reverse_override_name(self):\n    mod = mlp.MLP([2, 3, 4])\n    mod(tf.ones([1, 1]))\n    rev = mod.reverse(name=""foobar"")\n    self.assertEqual(rev.name, ""foobar"")\n\n  def test_reverse(self):\n    mod = reversed_mlp()\n    self.assertEqual([l.output_size for l in mod.submodules], [3, 2, 1])\n\n  @parameterized.parameters(True, False)\n  def test_reverse_passed_with_bias(self, with_bias):\n    mod = reversed_mlp(with_bias=with_bias)\n    for linear in mod.submodules:\n      self.assertEqual(linear.with_bias, with_bias)\n\n  def test_reverse_w_init(self):\n    w_init = CountingInitializer()\n    mod = reversed_mlp(w_init=w_init)\n    for linear in mod.submodules:\n      self.assertIs(linear.w_init, w_init)\n\n  def test_reverse_b_init(self):\n    b_init = CountingInitializer()\n    mod = reversed_mlp(b_init=b_init)\n    for linear in mod.submodules:\n      self.assertIs(linear.b_init, b_init)\n\n  def test_reverse_activation(self):\n    activation = CountingActivation()\n    mod = reversed_mlp(activation=activation)\n    activation.count = 0\n    mod(tf.ones([1, 1]))\n    self.assertEqual(activation.count, 2)\n\n  def test_dropout_requires_is_training(self):\n    mod = mlp.MLP([1, 1], dropout_rate=0.5)\n    with self.assertRaisesRegexp(ValueError, ""is_training.* is required""):\n      mod(tf.ones([1, 1]))\n\n  @parameterized.parameters(False, True)\n  def test_no_dropout_rejects_is_training(self, is_training):\n    mod = mlp.MLP([1, 1])\n    with self.assertRaisesRegexp(ValueError, ""is_training.*only.*with dropout""):\n      mod(tf.ones([1, 1]), is_training=is_training)\n\n  @parameterized.parameters(False, True)\n  def test_reverse_activate_final(self, activate_final):\n    activation = CountingActivation()\n    mod = reversed_mlp(activation=activation, activate_final=activate_final)\n    activation.count = 0\n    mod(tf.ones([1, 1]))\n    self.assertEqual(activation.count, 3 if activate_final else 2)\n\n  @parameterized.parameters(itertools.product((False, True), (False, True)))\n  def test_applies_activation_with_dropout(self, use_dropout, is_training):\n    activation = CountingActivation()\n    mod = mlp.MLP([1, 1, 1],\n                  dropout_rate=(0.5 if use_dropout else None),\n                  activation=activation)\n    mod(tf.ones([1, 1]), is_training=(is_training if use_dropout else None))\n    self.assertEqual(activation.count, 2)\n\n  def test_repr(self):\n    mod = mlp.MLP([1, 2, 3])\n    for index, linear in enumerate(mod.submodules):\n      self.assertEqual(\n          repr(linear),\n          ""Linear(output_size={}, name=\'linear_{}\')"".format(index + 1, index))\n\n\ndef reversed_mlp(**kwargs):\n  mod = mlp.MLP([2, 3, 4], **kwargs)\n  mod(tf.ones([1, 1]))\n  return mod.reverse()\n\n\nclass CountingActivation(object):\n\n  def __init__(self):\n    self.count = 0\n\n  def __call__(self, x):\n    self.count += 1\n    return x\n\n\nclass CountingInitializer(object):\n\n  def __init__(self):\n    self.count = 0\n\n  def __call__(self, shape, dtype=tf.float32):\n    self.count += 1\n    return tf.ones(shape, dtype=dtype)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/nets/resnet.py,7,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""ResNet model for Sonnet.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nfrom sonnet.src import base\nfrom sonnet.src import batch_norm\nfrom sonnet.src import conv\nfrom sonnet.src import initializers\nfrom sonnet.src import linear\nfrom sonnet.src import pad\n\nimport tensorflow as tf\nfrom typing import Mapping, Optional, Sequence, Text, Union\n\n\nclass BottleNeckBlockV1(base.Module):\n  """"""Bottleneck Block for a ResNet implementation.""""""\n\n  def __init__(self,\n               channels: int,\n               stride: Union[int, Sequence[int]],\n               use_projection: bool,\n               bn_config: Mapping[Text, float],\n               name: Optional[Text] = None):\n    super(BottleNeckBlockV1, self).__init__(name=name)\n    self._channels = channels\n    self._stride = stride\n    self._use_projection = use_projection\n    self._bn_config = bn_config\n\n    batchnorm_args = {""create_scale"": True, ""create_offset"": True}\n    batchnorm_args.update(bn_config)\n\n    if self._use_projection:\n      self._proj_conv = conv.Conv2D(\n          output_channels=channels,\n          kernel_shape=1,\n          stride=stride,\n          with_bias=False,\n          padding=pad.same,\n          name=""shortcut_conv"")\n      self._proj_batchnorm = batch_norm.BatchNorm(\n          name=""shortcut_batchnorm"", **batchnorm_args)\n\n    self._layers = []\n    conv_0 = conv.Conv2D(\n        output_channels=channels // 4,\n        kernel_shape=1,\n        stride=1,\n        with_bias=False,\n        padding=pad.same,\n        name=""conv_0"")\n    self._layers.append(\n        [conv_0,\n         batch_norm.BatchNorm(name=""batchnorm_0"", **batchnorm_args)])\n\n    conv_1 = conv.Conv2D(\n        output_channels=channels // 4,\n        kernel_shape=3,\n        stride=stride,\n        with_bias=False,\n        padding=pad.same,\n        name=""conv_1"")\n    self._layers.append(\n        [conv_1,\n         batch_norm.BatchNorm(name=""batchnorm_1"", **batchnorm_args)])\n\n    conv_2 = conv.Conv2D(\n        output_channels=channels,\n        kernel_shape=1,\n        stride=1,\n        with_bias=False,\n        padding=pad.same,\n        name=""conv_2"")\n    batchnorm_2 = batch_norm.BatchNorm(\n        name=""batchnorm_2"", scale_init=initializers.Zeros(), **batchnorm_args)\n    self._layers.append([conv_2, batchnorm_2])\n\n  def __call__(self, inputs, is_training):\n    if self._use_projection:\n      shortcut = self._proj_conv(inputs)\n      shortcut = self._proj_batchnorm(shortcut, is_training=is_training)\n    else:\n      shortcut = inputs\n\n    net = inputs\n    for i, [conv_layer, batchnorm_layer] in enumerate(self._layers):\n      net = conv_layer(net)\n      net = batchnorm_layer(net, is_training=is_training)\n      net = tf.nn.relu(net) if i < 2 else net  # Don\'t apply relu on last layer\n\n    return tf.nn.relu(net + shortcut)\n\n\nclass BottleNeckBlockV2(base.Module):\n  """"""Bottleneck Block for a Resnet implementation.""""""\n\n  def __init__(self,\n               channels: int,\n               stride: Union[int, Sequence[int]],\n               use_projection: bool,\n               bn_config: Mapping[Text, float],\n               name: Optional[Text] = None):\n    super(BottleNeckBlockV2, self).__init__(name=name)\n    self._channels = channels\n    self._stride = stride\n    self._use_projection = use_projection\n    self._bn_config = bn_config\n\n    batchnorm_args = {""create_scale"": True, ""create_offset"": True}\n    batchnorm_args.update(bn_config)\n\n    if self._use_projection:\n      self._proj_conv = conv.Conv2D(\n          output_channels=channels,\n          kernel_shape=1,\n          stride=stride,\n          with_bias=False,\n          padding=pad.same,\n          name=""shortcut_conv"")\n\n    self._conv_0 = conv.Conv2D(\n        output_channels=channels // 4,\n        kernel_shape=1,\n        stride=1,\n        with_bias=False,\n        padding=pad.same,\n        name=""conv_0"")\n\n    self._bn_0 = batch_norm.BatchNorm(name=""batchnorm_0"", **batchnorm_args)\n\n    self._conv_1 = conv.Conv2D(\n        output_channels=channels // 4,\n        kernel_shape=3,\n        stride=stride,\n        with_bias=False,\n        padding=pad.same,\n        name=""conv_1"")\n\n    self._bn_1 = batch_norm.BatchNorm(name=""batchnorm_1"", **batchnorm_args)\n\n    self._conv_2 = conv.Conv2D(\n        output_channels=channels,\n        kernel_shape=1,\n        stride=1,\n        with_bias=False,\n        padding=pad.same,\n        name=""conv_2"")\n\n    # NOTE: Some implementations of ResNet50 v2 suggest initializing gamma/scale\n    # here to zeros.\n    self._bn_2 = batch_norm.BatchNorm(name=""batchnorm_2"", **batchnorm_args)\n\n  def __call__(self, inputs, is_training):\n    net = inputs\n    shortcut = inputs\n\n    for i, (conv_i, bn_i) in enumerate(((self._conv_0, self._bn_0),\n                                        (self._conv_1, self._bn_1),\n                                        (self._conv_2, self._bn_2))):\n      net = bn_i(net, is_training=is_training)\n      net = tf.nn.relu(net)\n      if i == 0 and self._use_projection:\n        shortcut = self._proj_conv(net)\n      net = conv_i(net)\n\n    return net + shortcut\n\n\nclass BlockGroup(base.Module):\n  """"""Higher level block for ResNet implementation.""""""\n\n  def __init__(self,\n               channels: int,\n               num_blocks: int,\n               stride: Union[int, Sequence[int]],\n               bn_config: Mapping[Text, float],\n               resnet_v2: bool = False,\n               name: Optional[Text] = None):\n    super(BlockGroup, self).__init__(name=name)\n    self._channels = channels\n    self._num_blocks = num_blocks\n    self._stride = stride\n    self._bn_config = bn_config\n\n    if resnet_v2:\n      bottle_neck_block = BottleNeckBlockV2\n    else:\n      bottle_neck_block = BottleNeckBlockV1\n\n    self._blocks = []\n    for id_block in range(num_blocks):\n      self._blocks.append(\n          bottle_neck_block(\n              channels=channels,\n              stride=stride if id_block == 0 else 1,\n              use_projection=(id_block == 0),\n              bn_config=bn_config,\n              name=""block_%d"" % (id_block)))\n\n  def __call__(self, inputs, is_training):\n    net = inputs\n    for block in self._blocks:\n      net = block(net, is_training=is_training)\n    return net\n\n\nclass ResNet(base.Module):\n  """"""ResNet model.""""""\n\n  def __init__(self,\n               blocks_per_group_list: Sequence[int],\n               num_classes: int,\n               bn_config: Optional[Mapping[Text, float]] = None,\n               resnet_v2: bool = False,\n               channels_per_group_list: Sequence[int] = (256, 512, 1024, 2048),\n               name: Optional[Text] = None):\n    """"""Constructs a ResNet model.\n\n    Args:\n      blocks_per_group_list: A sequence of length 4 that indicates the number of\n        blocks created in each group.\n      num_classes: The number of classes to classify the inputs into.\n      bn_config: A dictionary of two elements, `decay_rate` and `eps` to be\n        passed on to the `BatchNorm` layers. By default the `decay_rate` is\n        `0.9` and `eps` is `1e-5`.\n      resnet_v2: Whether to use the v1 or v2 ResNet implementation. Defaults to\n        False.\n      channels_per_group_list: A sequence of length 4 that indicates the number\n        of channels used for each block in each group.\n      name: Name of the module.\n    """"""\n    super(ResNet, self).__init__(name=name)\n    if bn_config is None:\n      bn_config = {""decay_rate"": 0.9, ""eps"": 1e-5}\n    self._bn_config = bn_config\n    self._resnet_v2 = resnet_v2\n\n    # Number of blocks in each group for ResNet.\n    if len(blocks_per_group_list) != 4:\n      raise ValueError(\n          ""`blocks_per_group_list` must be of length 4 not {}"".format(\n              len(blocks_per_group_list)))\n    self._blocks_per_group_list = blocks_per_group_list\n\n    # Number of channels in each group for ResNet.\n    if len(channels_per_group_list) != 4:\n      raise ValueError(\n          ""`channels_per_group_list` must be of length 4 not {}"".format(\n              len(channels_per_group_list)))\n    self._channels_per_group_list = channels_per_group_list\n\n    self._initial_conv = conv.Conv2D(\n        output_channels=64,\n        kernel_shape=7,\n        stride=2,\n        with_bias=False,\n        padding=pad.same,\n        name=""initial_conv"")\n    if not self._resnet_v2:\n      self._initial_batchnorm = batch_norm.BatchNorm(\n          create_scale=True,\n          create_offset=True,\n          name=""initial_batchnorm"",\n          **bn_config)\n\n    self._block_groups = []\n    strides = [1, 2, 2, 2]\n    for i in range(4):\n      self._block_groups.append(\n          BlockGroup(\n              channels=self._channels_per_group_list[i],\n              num_blocks=self._blocks_per_group_list[i],\n              stride=strides[i],\n              bn_config=bn_config,\n              resnet_v2=resnet_v2,\n              name=""block_group_%d"" % (i)))\n\n    if self._resnet_v2:\n      self._final_batchnorm = batch_norm.BatchNorm(\n          create_scale=True,\n          create_offset=True,\n          name=""final_batchnorm"",\n          **bn_config)\n\n    self._logits = linear.Linear(\n        output_size=num_classes, w_init=initializers.Zeros(), name=""logits"")\n\n  def __call__(self, inputs, is_training):\n    net = inputs\n    net = self._initial_conv(net)\n    if not self._resnet_v2:\n      net = self._initial_batchnorm(net, is_training=is_training)\n      net = tf.nn.relu(net)\n\n    net = tf.nn.max_pool2d(\n        net, ksize=3, strides=2, padding=""SAME"", name=""initial_max_pool"")\n\n    for block_group in self._block_groups:\n      net = block_group(net, is_training)\n\n    if self._resnet_v2:\n      net = self._final_batchnorm(net, is_training=is_training)\n      net = tf.nn.relu(net)\n    net = tf.reduce_mean(net, axis=[1, 2], name=""final_avg_pool"")\n    return self._logits(net)\n\n\nclass ResNet50(ResNet):\n  """"""ResNet50 module.""""""\n\n  def __init__(self,\n               num_classes: int,\n               bn_config: Optional[Mapping[Text, float]] = None,\n               resnet_v2: bool = False,\n               name: Optional[Text] = None):\n    """"""Constructs a ResNet model.\n\n    Args:\n      num_classes: The number of classes to classify the inputs into.\n      bn_config: A dictionary of two elements, `decay_rate` and `eps` to be\n        passed on to the `BatchNorm` layers.\n      resnet_v2: Whether to use the v1 or v2 ResNet implementation. Defaults to\n        False.\n      name: Name of the module.\n    """"""\n    super(ResNet50, self).__init__([3, 4, 6, 3],\n                                   num_classes=num_classes,\n                                   bn_config=bn_config,\n                                   resnet_v2=resnet_v2,\n                                   name=name)\n'"
sonnet/src/nets/resnet_test.py,4,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.nets.resnet.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nfrom sonnet.src import test_utils\nfrom sonnet.src.nets import resnet\nimport tensorflow as tf\n\n\nclass ResnetTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters(True, False)\n  def test_simple(self, resnet_v2):\n    image = tf.random.normal([2, 64, 64, 3])\n    model = resnet.ResNet([1, 1, 1, 1], 10, resnet_v2=resnet_v2)\n\n    logits = model(image, is_training=True)\n    self.assertIsNotNone(logits)\n    self.assertEqual(logits.shape, [2, 10])\n\n  @parameterized.parameters(True, False)\n  def test_tf_function(self, resnet_v2):\n    image = tf.random.normal([2, 64, 64, 3])\n    model = resnet.ResNet(\n        [1, 1, 1, 1],\n        10,\n        resnet_v2=resnet_v2,\n    )\n    f = tf.function(model)\n\n    logits = f(image, is_training=True)\n    self.assertIsNotNone(logits)\n    self.assertEqual(logits.shape, [2, 10])\n    self.assertAllEqual(model(image, is_training=True).numpy(), logits.numpy())\n\n  @parameterized.parameters(3, 5)\n  def test_error_incorrect_args_block_list(self, list_length):\n    block_list = [i for i in range(list_length)]\n    with self.assertRaisesRegexp(\n        ValueError, ""blocks_per_group_list` must be of length 4 not {}"".format(\n            list_length)):\n      resnet.ResNet(block_list, 10, {""decay_rate"": 0.9, ""eps"": 1e-5})\n\n  @parameterized.parameters(3, 5)\n  def test_error_incorrect_args_channel_list(self, list_length):\n    channel_list = [i for i in range(list_length)]\n    with self.assertRaisesRegexp(\n        ValueError,\n        ""channels_per_group_list` must be of length 4 not {}"".format(\n            list_length)):\n      resnet.ResNet([1, 1, 1, 1], 10, {""decay_rate"": 0.9, ""eps"": 1e-5},\n                    channels_per_group_list=channel_list)\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/nets/vqvae.py,40,"b'# Copyright 2018 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Sonnet implementation of VQ-VAE https://arxiv.org/abs/1711.00937.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nfrom sonnet.src import base\nfrom sonnet.src import initializers\nfrom sonnet.src import moving_averages\nfrom sonnet.src import types\n\nimport tensorflow as tf\nfrom typing import Text\n\n\nclass VectorQuantizer(base.Module):\n  """"""Sonnet module representing the VQ-VAE layer.\n\n  Implements the algorithm presented in\n  \'Neural Discrete Representation Learning\' by van den Oord et al.\n  https://arxiv.org/abs/1711.00937\n\n  Input any tensor to be quantized. Last dimension will be used as space in\n  which to quantize. All other dimensions will be flattened and will be seen\n  as different examples to quantize.\n\n  The output tensor will have the same shape as the input.\n\n  For example a tensor with shape [16, 32, 32, 64] will be reshaped into\n  [16384, 64] and all 16384 vectors (each of 64 dimensions)  will be quantized\n  independently.\n\n  Attributes:\n    embedding_dim: integer representing the dimensionality of the tensors in the\n      quantized space. Inputs to the modules must be in this format as well.\n    num_embeddings: integer, the number of vectors in the quantized space.\n    commitment_cost: scalar which controls the weighting of the loss terms (see\n      equation 4 in the paper - this variable is Beta).\n  """"""\n\n  def __init__(self,\n               embedding_dim: int,\n               num_embeddings: int,\n               commitment_cost: types.FloatLike,\n               dtype: tf.DType = tf.float32,\n               name: Text = \'vector_quantizer\'):\n    """"""Initializes a VQ-VAE module.\n\n    Args:\n      embedding_dim: dimensionality of the tensors in the quantized space.\n        Inputs to the modules must be in this format as well.\n      num_embeddings: number of vectors in the quantized space.\n      commitment_cost: scalar which controls the weighting of the loss terms\n        (see equation 4 in the paper - this variable is Beta).\n      dtype: dtype for the embeddings variable, defaults to tf.float32.\n      name: name of the module.\n    """"""\n    super(VectorQuantizer, self).__init__(name=name)\n    self.embedding_dim = embedding_dim\n    self.num_embeddings = num_embeddings\n    self.commitment_cost = commitment_cost\n\n    embedding_shape = [embedding_dim, num_embeddings]\n    initializer = initializers.VarianceScaling(distribution=\'uniform\')\n    self.embeddings = tf.Variable(\n        initializer(embedding_shape, dtype), name=\'embeddings\')\n\n  def __call__(self, inputs, is_training):\n    """"""Connects the module to some inputs.\n\n    Args:\n      inputs: Tensor, final dimension must be equal to embedding_dim. All other\n        leading dimensions will be flattened and treated as a large batch.\n      is_training: boolean, whether this connection is to training data.\n\n    Returns:\n      dict containing the following keys and values:\n        quantize: Tensor containing the quantized version of the input.\n        loss: Tensor containing the loss to optimize.\n        perplexity: Tensor containing the perplexity of the encodings.\n        encodings: Tensor containing the discrete encodings, ie which element\n        of the quantized space each input element was mapped to.\n        encoding_indices: Tensor containing the discrete encoding indices, ie\n        which element of the quantized space each input element was mapped to.\n    """"""\n    flat_inputs = tf.reshape(inputs, [-1, self.embedding_dim])\n\n    distances = (\n        tf.reduce_sum(flat_inputs**2, 1, keepdims=True) -\n        2 * tf.matmul(flat_inputs, self.embeddings) +\n        tf.reduce_sum(self.embeddings**2, 0, keepdims=True))\n\n    encoding_indices = tf.argmax(-distances, 1)\n    encodings = tf.one_hot(encoding_indices,\n                           self.num_embeddings,\n                           dtype=distances.dtype)\n\n    # NB: if your code crashes with a reshape error on the line below about a\n    # Tensor containing the wrong number of values, then the most likely cause\n    # is that the input passed in does not have a final dimension equal to\n    # self.embedding_dim. Ideally we would catch this with an Assert but that\n    # creates various other problems related to device placement / TPUs.\n    encoding_indices = tf.reshape(encoding_indices, tf.shape(inputs)[:-1])\n    quantized = self.quantize(encoding_indices)\n\n    e_latent_loss = tf.reduce_mean((tf.stop_gradient(quantized) - inputs)**2)\n    q_latent_loss = tf.reduce_mean((quantized - tf.stop_gradient(inputs))**2)\n    loss = q_latent_loss + self.commitment_cost * e_latent_loss\n\n    # Straight Through Estimator\n    quantized = inputs + tf.stop_gradient(quantized - inputs)\n    avg_probs = tf.reduce_mean(encodings, 0)\n    perplexity = tf.exp(-tf.reduce_sum(avg_probs *\n                                       tf.math.log(avg_probs + 1e-10)))\n\n    return {\n        \'quantize\': quantized,\n        \'loss\': loss,\n        \'perplexity\': perplexity,\n        \'encodings\': encodings,\n        \'encoding_indices\': encoding_indices,\n        \'distances\': distances,\n    }\n\n  def quantize(self, encoding_indices):\n    """"""Returns embedding tensor for a batch of indices.""""""\n    w = tf.transpose(self.embeddings, [1, 0])\n    # TODO(mareynolds) in V1 we had a validate_indices kwarg, this is no longer\n    # supported in V2. Are we missing anything here?\n    return tf.nn.embedding_lookup(w, encoding_indices)\n\n\nclass VectorQuantizerEMA(base.Module):\n  """"""Sonnet module representing the VQ-VAE layer.\n\n  Implements a slightly modified version of the algorithm presented in\n  \'Neural Discrete Representation Learning\' by van den Oord et al.\n  https://arxiv.org/abs/1711.00937\n\n  The difference between VectorQuantizerEMA and VectorQuantizer is that\n  this module uses exponential moving averages to update the embedding vectors\n  instead of an auxiliary loss. This has the advantage that the embedding\n  updates are independent of the choice of optimizer (SGD, RMSProp, Adam, K-Fac,\n  ...) used for the encoder, decoder and other parts of the architecture. For\n  most experiments the EMA version trains faster than the non-EMA version.\n\n  Input any tensor to be quantized. Last dimension will be used as space in\n  which to quantize. All other dimensions will be flattened and will be seen\n  as different examples to quantize.\n\n  The output tensor will have the same shape as the input.\n\n  For example a tensor with shape [16, 32, 32, 64] will be reshaped into\n  [16384, 64] and all 16384 vectors (each of 64 dimensions)  will be quantized\n  independently.\n\n  Attributes:\n    embedding_dim: integer representing the dimensionality of the tensors in the\n      quantized space. Inputs to the modules must be in this format as well.\n    num_embeddings: integer, the number of vectors in the quantized space.\n    commitment_cost: scalar which controls the weighting of the loss terms (see\n      equation 4 in the paper).\n    decay: float, decay for the moving averages.\n    epsilon: small float constant to avoid numerical instability.\n  """"""\n\n  def __init__(self,\n               embedding_dim,\n               num_embeddings,\n               commitment_cost,\n               decay,\n               epsilon=1e-5,\n               dtype=tf.float32,\n               name=\'vector_quantizer_ema\'):\n    """"""Initializes a VQ-VAE EMA module.\n\n    Args:\n      embedding_dim: integer representing the dimensionality of the tensors in\n        the quantized space. Inputs to the modules must be in this format as\n        well.\n      num_embeddings: integer, the number of vectors in the quantized space.\n      commitment_cost: scalar which controls the weighting of the loss terms\n        (see equation 4 in the paper - this variable is Beta).\n      decay: float between 0 and 1, controls the speed of the Exponential Moving\n        Averages.\n      epsilon: small constant to aid numerical stability, default 1e-5.\n      dtype: dtype for the embeddings variable, defaults to tf.float32.\n      name: name of the module.\n    """"""\n    super(VectorQuantizerEMA, self).__init__(name=name)\n    self.embedding_dim = embedding_dim\n    self.num_embeddings = num_embeddings\n    if not 0 <= decay <= 1:\n      raise ValueError(\'decay must be in range [0, 1]\')\n    self.decay = decay\n    self.commitment_cost = commitment_cost\n    self.epsilon = epsilon\n\n    embedding_shape = [embedding_dim, num_embeddings]\n    initializer = initializers.VarianceScaling(distribution=\'uniform\')\n    self.embeddings = tf.Variable(\n        initializer(embedding_shape, dtype), name=\'embeddings\')\n\n    self.ema_cluster_size = moving_averages.ExponentialMovingAverage(\n        decay=self.decay, name=\'ema_cluster_size\')\n    self.ema_cluster_size.initialize(tf.zeros([num_embeddings], dtype=dtype))\n\n    self.ema_dw = moving_averages.ExponentialMovingAverage(\n        decay=self.decay, name=\'ema_dw\')\n    self.ema_dw.initialize(self.embeddings)\n\n  def __call__(self, inputs, is_training):\n    """"""Connects the module to some inputs.\n\n    Args:\n      inputs: Tensor, final dimension must be equal to embedding_dim. All other\n        leading dimensions will be flattened and treated as a large batch.\n      is_training: boolean, whether this connection is to training data. When\n        this is set to False, the internal moving average statistics will not be\n        updated.\n\n    Returns:\n      dict containing the following keys and values:\n        quantize: Tensor containing the quantized version of the input.\n        loss: Tensor containing the loss to optimize.\n        perplexity: Tensor containing the perplexity of the encodings.\n        encodings: Tensor containing the discrete encodings, ie which element\n        of the quantized space each input element was mapped to.\n        encoding_indices: Tensor containing the discrete encoding indices, ie\n        which element of the quantized space each input element was mapped to.\n    """"""\n    flat_inputs = tf.reshape(inputs, [-1, self.embedding_dim])\n\n    distances = (\n        tf.reduce_sum(flat_inputs**2, 1, keepdims=True) -\n        2 * tf.matmul(flat_inputs, self.embeddings) +\n        tf.reduce_sum(self.embeddings**2, 0, keepdims=True))\n\n    encoding_indices = tf.argmax(-distances, 1)\n    encodings = tf.one_hot(encoding_indices,\n                           self.num_embeddings,\n                           dtype=distances.dtype)\n\n    # NB: if your code crashes with a reshape error on the line below about a\n    # Tensor containing the wrong number of values, then the most likely cause\n    # is that the input passed in does not have a final dimension equal to\n    # self.embedding_dim. Ideally we would catch this with an Assert but that\n    # creates various other problems related to device placement / TPUs.\n    encoding_indices = tf.reshape(encoding_indices, tf.shape(inputs)[:-1])\n    quantized = self.quantize(encoding_indices)\n    e_latent_loss = tf.reduce_mean((tf.stop_gradient(quantized) - inputs)**2)\n\n    if is_training:\n      updated_ema_cluster_size = self.ema_cluster_size(\n          tf.reduce_sum(encodings, axis=0))\n\n      dw = tf.matmul(flat_inputs, encodings, transpose_a=True)\n      updated_ema_dw = self.ema_dw(dw)\n\n      n = tf.reduce_sum(updated_ema_cluster_size)\n      updated_ema_cluster_size = ((updated_ema_cluster_size + self.epsilon) /\n                                  (n + self.num_embeddings * self.epsilon) * n)\n\n      normalised_updated_ema_w = (\n          updated_ema_dw / tf.reshape(updated_ema_cluster_size, [1, -1]))\n\n      self.embeddings.assign(normalised_updated_ema_w)\n      loss = self.commitment_cost * e_latent_loss\n\n    else:\n      loss = self.commitment_cost * e_latent_loss\n\n    # Straight Through Estimator\n    quantized = inputs + tf.stop_gradient(quantized - inputs)\n    avg_probs = tf.reduce_mean(encodings, 0)\n    perplexity = tf.exp(-tf.reduce_sum(avg_probs *\n                                       tf.math.log(avg_probs + 1e-10)))\n\n    return {\n        \'quantize\': quantized,\n        \'loss\': loss,\n        \'perplexity\': perplexity,\n        \'encodings\': encodings,\n        \'encoding_indices\': encoding_indices,\n        \'distances\': distances,\n    }\n\n  def quantize(self, encoding_indices):\n    """"""Returns embedding tensor for a batch of indices.""""""\n    w = tf.transpose(self.embeddings, [1, 0])\n    return tf.nn.embedding_lookup(w, encoding_indices)\n'"
sonnet/src/nets/vqvae_test.py,14,"b'# Copyright 2018 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.nets.vqvae.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\n\nimport numpy as np\nfrom sonnet.src import test_utils\nfrom sonnet.src.nets import vqvae\nimport tensorflow as tf\nimport tree\n\n\nclass VqvaeTest(parameterized.TestCase, test_utils.TestCase):\n\n  @parameterized.parameters((vqvae.VectorQuantizer, {\n      \'embedding_dim\': 4,\n      \'num_embeddings\': 8,\n      \'commitment_cost\': 0.25\n  }), (vqvae.VectorQuantizerEMA, {\n      \'embedding_dim\': 6,\n      \'num_embeddings\': 13,\n      \'commitment_cost\': 0.5,\n      \'decay\': 0.1\n  }))\n  def testConstruct(self, constructor, kwargs):\n    vqvae_module = constructor(**kwargs)\n    # Batch of input vectors to quantize\n    inputs_np = np.random.randn(100, kwargs[\'embedding_dim\']).astype(np.float32)\n    inputs = tf.constant(inputs_np)\n\n    # Set is_training to False, otherwise for the EMA case just evaluating the\n    # forward pass will change the embeddings, meaning that some of our computed\n    # closest embeddings will be incorrect.\n    vq_output = vqvae_module(inputs, is_training=False)\n\n    # Output shape is correct\n    self.assertEqual(vq_output[\'quantize\'].shape, inputs.shape)\n\n    vq_output_np = tree.map_structure(lambda t: t.numpy(), vq_output)\n    embeddings_np = vqvae_module.embeddings.numpy()\n\n    self.assertEqual(embeddings_np.shape,\n                     (kwargs[\'embedding_dim\'], kwargs[\'num_embeddings\']))\n\n    # Check that each input was assigned to the embedding it is closest to.\n    distances = ((inputs_np**2).sum(axis=1, keepdims=True) -\n                 2 * np.dot(inputs_np, embeddings_np) +\n                 (embeddings_np**2).sum(axis=0, keepdims=True))\n    closest_index = np.argmax(-distances, axis=1)\n    # On TPU, distances can be different by ~1% due to precision. This can cause\n    # the distanc to the closest embedding to flip, leading to a difference\n    # in the encoding indices tensor. First we check that the continuous\n    # distances are reasonably close, and then we only allow N differences in\n    # the encodings. For batch of 100, N == 3 seems okay (passed 1000x tests).\n    self.assertAllClose(distances, vq_output_np[\'distances\'], atol=4e-2)\n    num_differences_in_encodings = (closest_index !=\n                                    vq_output_np[\'encoding_indices\']).sum()\n    num_differences_allowed = 3\n    self.assertLessEqual(num_differences_in_encodings, num_differences_allowed)\n\n  @parameterized.parameters((vqvae.VectorQuantizer, {\n      \'embedding_dim\': 4,\n      \'num_embeddings\': 8,\n      \'commitment_cost\': 0.25\n  }), (vqvae.VectorQuantizerEMA, {\n      \'embedding_dim\': 6,\n      \'num_embeddings\': 13,\n      \'commitment_cost\': 0.5,\n      \'decay\': 0.1\n  }))\n  def testShapeChecking(self, constructor, kwargs):\n    vqvae_module = constructor(**kwargs)\n    wrong_shape_input = np.random.randn(100, kwargs[\'embedding_dim\'] * 2)\n    with self.assertRaisesRegexp(tf.errors.InvalidArgumentError,\n                                 \'but the requested shape has\'):\n      vqvae_module(\n          tf.constant(wrong_shape_input.astype(np.float32)), is_training=False)\n\n  @parameterized.parameters((vqvae.VectorQuantizer, {\n      \'embedding_dim\': 4,\n      \'num_embeddings\': 8,\n      \'commitment_cost\': 0.25\n  }), (vqvae.VectorQuantizerEMA, {\n      \'embedding_dim\': 6,\n      \'num_embeddings\': 13,\n      \'commitment_cost\': 0.5,\n      \'decay\': 0.1\n  }))\n  def testNoneBatch(self, constructor, kwargs):\n    """"""Check that vqvae can be built on input with a None batch dimension.""""""\n    vqvae_module = constructor(**kwargs)\n    inputs = tf.zeros([0, 5, 5, kwargs[\'embedding_dim\']])\n    vqvae_module(inputs, is_training=False)\n\n  @parameterized.parameters({\'use_tf_function\': True, \'dtype\': tf.float32},\n                            {\'use_tf_function\': True, \'dtype\': tf.float64},\n                            {\'use_tf_function\': False, \'dtype\': tf.float32},\n                            {\'use_tf_function\': False, \'dtype\': tf.float64})\n  def testEmaUpdating(self, use_tf_function, dtype):\n    if self.primary_device == \'TPU\' and dtype == tf.float64:\n      self.skipTest(\'F64 not supported by TPU\')\n\n    embedding_dim = 6\n    np_dtype = np.float64 if dtype is tf.float64 else np.float32\n    decay = np.array(0.1, dtype=np_dtype)\n    vqvae_module = vqvae.VectorQuantizerEMA(\n        embedding_dim=embedding_dim,\n        num_embeddings=7,\n        commitment_cost=0.5,\n        decay=decay,\n        dtype=dtype)\n    if use_tf_function:\n      vqvae_module = tf.function(vqvae_module)\n\n    batch_size = 16\n\n    prev_embeddings = vqvae_module.embeddings.numpy()\n\n    # Embeddings should change with every forwards pass if is_training == True.\n    for _ in range(10):\n      inputs = tf.random.normal([batch_size, embedding_dim], dtype=dtype)\n      vqvae_module(inputs, is_training=True)\n      current_embeddings = vqvae_module.embeddings.numpy()\n      self.assertFalse((prev_embeddings == current_embeddings).all())\n      prev_embeddings = current_embeddings\n\n    # Forward passes with is_training == False don\'t change anything\n    for _ in range(10):\n      inputs = tf.random.normal([batch_size, embedding_dim], dtype=dtype)\n      vqvae_module(inputs, is_training=False)\n      current_embeddings = vqvae_module.embeddings.numpy()\n      self.assertTrue((current_embeddings == prev_embeddings).all())\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
sonnet/src/optimizers/adam.py,21,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Adaptive Moment Estimation (Adam) module.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nfrom sonnet.src import base\nfrom sonnet.src import once\nfrom sonnet.src import types\nfrom sonnet.src import utils\nfrom sonnet.src.optimizers import optimizer_utils\n\nimport tensorflow as tf\nfrom typing import Optional, Sequence, Text, Union\n\n\ndef adam_update(g, alpha, beta_1, beta_2, epsilon, t, m, v):\n  """"""Implements \'Algorithm 1\' from :cite:`kingma2014adam`.""""""\n  m = beta_1 * m + (1. - beta_1) * g      # Biased first moment estimate.\n  v = beta_2 * v + (1. - beta_2) * g * g  # Biased second raw moment estimate.\n  m_hat = m / (1. - tf.pow(beta_1, t))    # Bias corrected 1st moment estimate.\n  v_hat = v / (1. - tf.pow(beta_2, t))    # Bias corrected 2nd moment estimate.\n  update = alpha * m_hat / (tf.sqrt(v_hat) + epsilon)\n  return update, m, v\n\n\nclass Adam(base.Optimizer):\n  """"""Adaptive Moment Estimation (Adam) optimizer.\n\n  Adam is an algorithm for first-order gradient-based optimization of stochastic\n  objective functions, based on adaptive estimates of lower-order moments. See\n  :cite:`kingma2014adam` for more details.\n\n  Note: default parameter values have been taken from the paper.\n\n  Attributes:\n    learning_rate: Step size (``alpha`` in the paper).\n    beta1: Exponential decay rate for first moment estimate.\n    beta2: Exponential decay rate for second moment estimate.\n    epsilon: Small value to avoid zero denominator.\n    step: Step count.\n    m: Biased first moment estimate (a list with one value per parameter).\n    v: Biased second raw moment estimate (a list with one value per parameter).\n  """"""\n\n  def __init__(\n      self,\n      learning_rate: Union[types.FloatLike, tf.Variable] = 0.001,\n      beta1: Union[types.FloatLike, tf.Variable] = 0.9,\n      beta2: Union[types.FloatLike, tf.Variable] = 0.999,\n      epsilon: Union[types.FloatLike, tf.Variable] = 1e-8,\n      name: Optional[Text] = None):\n    """"""Constructs an `Adam` module.\n\n    Args:\n      learning_rate: Step size (``alpha`` in the paper).\n      beta1: Exponential decay rate for first moment estimate.\n      beta2: Exponential decay rate for second moment estimate.\n      epsilon: Small value to avoid zero denominator.\n      name: Name of the module.\n    """"""\n    super(Adam, self).__init__(name=name)\n    self.learning_rate = learning_rate\n    self.beta1 = beta1\n    self.beta2 = beta2\n    self.epsilon = epsilon\n    # TODO(petebu): Consider allowing the user to pass in a step.\n    self.step = tf.Variable(0, trainable=False, name=""t"", dtype=tf.int64)\n    self.m = []\n    self.v = []\n\n  @once.once\n  def _initialize(self, parameters: Sequence[tf.Variable]):\n    """"""First and second order moments are initialized to zero.""""""\n    zero_var = lambda p: utils.variable_like(p, trainable=False)\n    with tf.name_scope(""m""):\n      self.m.extend(zero_var(p) for p in parameters)\n    with tf.name_scope(""v""):\n      self.v.extend(zero_var(p) for p in parameters)\n\n  def apply(self, updates: Sequence[types.ParameterUpdate],\n            parameters: Sequence[tf.Variable]):\n    r""""""Applies updates to parameters.\n\n    Applies the Adam update rule for each update, parameter pair:\n\n    .. math::\n\n       \\begin{array}{ll}\n       m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot update \\\\\n       v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot update^2 \\\\\n       \\hat{m}_t = m_t / (1 - \\beta_1^t) \\\\\n       \\hat{v}_t = v_t / (1 - \\beta_2^t) \\\\\n       delta = \\alpha \\cdot \\hat{m}_t / (\\sqrt{\\hat{v}_t} + \\epsilon) \\\\\n       param_t = param_{t-1} - delta \\\\\n       \\end{array}\n\n    Args:\n      updates: A list of updates to apply to parameters. Updates are often\n        gradients as returned by :tf:`GradientTape.gradient`.\n      parameters: A list of parameters.\n\n    Raises:\n      ValueError: If `updates` and `parameters` are empty, have different\n        lengths, or have inconsistent types.\n    """"""\n    optimizer_utils.check_distribution_strategy()\n    optimizer_utils.check_updates_parameters(updates, parameters)\n    self._initialize(parameters)\n    self.step.assign_add(1)\n    for update, param, m_var, v_var in zip(updates, parameters, self.m, self.v):\n      if update is None:\n        continue\n\n      optimizer_utils.check_same_dtype(update, param)\n      learning_rate = tf.cast(self.learning_rate, update.dtype)\n      beta_1 = tf.cast(self.beta1, update.dtype)\n      beta_2 = tf.cast(self.beta2, update.dtype)\n      epsilon = tf.cast(self.epsilon, update.dtype)\n      step = tf.cast(self.step, update.dtype)\n\n      if isinstance(update, tf.IndexedSlices):\n        # Sparse read our state.\n        update, indices = optimizer_utils.deduplicate_indexed_slices(update)\n        m = m_var.sparse_read(indices)\n        v = v_var.sparse_read(indices)\n\n        # Compute and apply a sparse update to our parameter and state.\n        update, m, v = adam_update(\n            g=update, alpha=learning_rate, beta_1=beta_1, beta_2=beta_2,\n            epsilon=epsilon, t=step, m=m, v=v)\n        param.scatter_sub(tf.IndexedSlices(update, indices))\n        m_var.scatter_update(tf.IndexedSlices(m, indices))\n        v_var.scatter_update(tf.IndexedSlices(v, indices))\n\n      else:\n        # Compute and apply a dense update to our parameter and state.\n        update, m, v = adam_update(\n            g=update, alpha=learning_rate, beta_1=beta_1, beta_2=beta_2,\n            epsilon=epsilon, t=step, m=m_var, v=v_var)\n        param.assign_sub(update)\n        m_var.assign(m)\n        v_var.assign(v)\n'"
sonnet/src/optimizers/adam_test.py,27,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.adam.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom sonnet.src import test_utils\nfrom sonnet.src.optimizers import adam\nfrom sonnet.src.optimizers import optimizer_tests\nimport tensorflow as tf\n\nCONFIGS = optimizer_tests.named_product(learning_rate=(0.1, 0.01, 0.001),\n                                        beta_1=(0.9, 0.99, 0.999),\n                                        beta_2=(0.9, 0.99, 0.999),\n                                        epsilon=(1e-8,),\n                                        seed=(28, 2, 90))\n\n\nclass ComparisonTest(optimizer_tests.AbstractFuzzTest):\n  """"""Ensures Sonnet optimizers have equivalent behavior to TensorFlow.""""""\n\n  def _make_tf(self, learning_rate, beta_1, beta_2, epsilon):\n    optimizer = tf.optimizers.Adam(learning_rate=learning_rate,\n                                   beta_1=beta_1,\n                                   beta_2=beta_2,\n                                   epsilon=epsilon)\n    return lambda g, p: optimizer.apply_gradients(zip(g, p))\n\n  def _make_snt(self, learning_rate, beta_1, beta_2, epsilon):\n    optimizer = adam.Adam(learning_rate=learning_rate,\n                          beta1=beta_1,\n                          beta2=beta_2,\n                          epsilon=epsilon)\n    return optimizer.apply\n\n  @test_utils.combined_named_parameters(CONFIGS)\n  def testComparingSonnetAndTensorFlow(self, config):\n    seed = config.pop(""seed"")\n    self.assertParametersRemainClose(seed, config)\n\n\nclass AdamTest(optimizer_tests.OptimizerTestBase):\n\n  def make_optimizer(self, **kwargs):\n    if ""learning_rate"" not in kwargs:\n      kwargs[""learning_rate""] = 0.001\n    return adam.Adam(**kwargs)\n\n  def testDense(self):\n    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]\n    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]\n    optimizer = self.make_optimizer(learning_rate=0.001)\n    # Step 1 of Adam\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[0.999, 1.999], [2.999, 3.999]],\n                        [x.numpy() for x in parameters])\n    # Step 2 of Adam\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[0.998, 1.998], [2.998, 3.998]],\n                        [x.numpy() for x in parameters])\n    # Step 3 of Adam\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[0.997, 1.997], [2.997, 3.997]],\n                        [x.numpy() for x in parameters])\n\n  def testSparse(self):\n    if self.primary_device in (""GPU"", ""TPU""):\n      self.skipTest(""IndexedSlices not supported on {}."".format(\n          self.primary_device))\n\n    parameters = [tf.Variable([[1.], [2.]]), tf.Variable([[3.], [4.]])]\n    tf_parameters = [tf.Variable([[1.], [2.]]), tf.Variable([[3.], [4.]])]\n    updates = [\n        tf.IndexedSlices(\n            tf.constant([0.1], shape=[1, 1]), tf.constant([0]),\n            tf.constant([2, 1])),\n        tf.IndexedSlices(\n            tf.constant([0.01], shape=[1, 1]), tf.constant([1]),\n            tf.constant([2, 1]))\n    ]\n    optimizer = self.make_optimizer(learning_rate=0.001)\n\n    # Compare against TF optimizer.\n    tf_optimizer = tf.optimizers.Adam(learning_rate=0.001)\n    # Step 1 of Adam\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[0.999], [2.0]], parameters[0].numpy())\n    self.assertAllClose([[3.0], [3.999]], parameters[1].numpy())\n    tf_optimizer.apply_gradients(zip(updates, tf_parameters))\n    self.assertAllClose(tf_parameters[0].numpy(), parameters[0].numpy())\n    self.assertAllClose(tf_parameters[1].numpy(), parameters[1].numpy())\n    # Step 2 of Adam\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[0.998], [2.0]], parameters[0].numpy())\n    self.assertAllClose([[3.0], [3.998]], parameters[1].numpy())\n    tf_optimizer.apply_gradients(zip(updates, tf_parameters))\n    self.assertAllClose(tf_parameters[0].numpy(), parameters[0].numpy())\n    self.assertAllClose(tf_parameters[1].numpy(), parameters[1].numpy())\n    # Step 3 of Adam\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[0.997], [2.0]], parameters[0].numpy())\n    self.assertAllClose([[3.0], [3.997]], parameters[1].numpy())\n    tf_optimizer.apply_gradients(zip(updates, tf_parameters))\n    self.assertAllClose(tf_parameters[0].numpy(), parameters[0].numpy())\n    self.assertAllClose(tf_parameters[1].numpy(), parameters[1].numpy())\n\n  def testVariableHyperParams(self):\n    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]\n    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]\n    learning_rate = tf.Variable(0.001)\n    optimizer = self.make_optimizer(learning_rate=learning_rate)\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[0.999, 1.999], [2.999, 3.999]],\n                        [x.numpy() for x in parameters])\n    learning_rate.assign(0.1)\n    self.assertAlmostEqual(0.1, optimizer.learning_rate.numpy())\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[0.899, 1.899], [2.899, 3.899]],\n                        [x.numpy() for x in parameters],\n                        rtol=1e-4)\n\n  def testHyperParamDTypeConversion(self):\n    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]\n    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]\n    dtype = tf.float32 if self.primary_device == ""TPU"" else tf.float64\n    learning_rate = tf.Variable(0.001, dtype=dtype)\n    beta1 = tf.Variable(0.9, dtype=dtype)\n    beta2 = tf.Variable(0.999, dtype=dtype)\n    epsilon = tf.Variable(1e-8, dtype=dtype)\n    optimizer = self.make_optimizer(\n        learning_rate=learning_rate, beta1=beta1, beta2=beta2, epsilon=epsilon)\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[0.999, 1.999], [2.999, 3.999]],\n                        [x.numpy() for x in parameters],\n                        rtol=1e-4)\n\n  def testAuxVariablesColocatedWithOriginal(self):\n    optimizer = self.make_optimizer(learning_rate=0.001)\n    with tf.device(""CPU:0""):\n      var = tf.Variable(1.0)\n    optimizer.apply([tf.constant(0.1)], [var])\n    self.assertEqual(optimizer.m[0].device, var.device)\n    self.assertEqual(optimizer.v[0].device, var.device)\n\n\nclass ReferenceAdamTest(optimizer_tests.OptimizerTestBase):\n\n  def make_optimizer(self, **kwargs):\n    if ""learning_rate"" not in kwargs:\n      kwargs[""learning_rate""] = 0.001\n    return optimizer_tests.WrappedTFOptimizer(tf.optimizers.Adam(**kwargs))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/optimizers/momentum.py,11,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""SGD with Momentum module.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nfrom sonnet.src import base\nfrom sonnet.src import once\nfrom sonnet.src import types\nfrom sonnet.src import utils\nfrom sonnet.src.optimizers import optimizer_utils\n\nimport tensorflow as tf\nfrom typing import Optional, Sequence, Text, Union\n\n\ndef momentum_update(update, learning_rate, mu, momentum, use_nesterov):\n  """"""Computes a momentum update for a single parameter.""""""\n  momentum = (mu * momentum) + update\n  if use_nesterov:\n    update = learning_rate * ((mu * momentum) + update)\n  else:\n    update = learning_rate * momentum\n  return update, momentum\n\n\nclass Momentum(base.Optimizer):\n  """"""SGD with Momentum module.\n\n  Attributes:\n    learning_rate: Learning rate.\n    momentum: Momentum scalar.\n    use_nesterov: `True` if using Nesterov momentum.\n    accumulated_momentum: Accumulated momentum for each parameter.\n  """"""\n\n  def __init__(self,\n               learning_rate: Union[types.FloatLike, tf.Variable],\n               momentum: Union[types.FloatLike, tf.Variable],\n               use_nesterov: bool = False,\n               name: Optional[Text] = None):\n    """"""Constructs a `Momentum` module.\n\n    Args:\n      learning_rate: Learning rate.\n      momentum: Momentum scalar.\n      use_nesterov: Whether to use Nesterov momentum.\n      name: Name of the module.\n    """"""\n    super(Momentum, self).__init__(name)\n    self.learning_rate = learning_rate\n    self.momentum = momentum  # TODO(petebu) Reconsider name.\n    self.use_nesterov = use_nesterov\n    self.accumulated_momentum = []  # TODO(petebu) Reconsider name.\n\n  @once.once\n  def _initialize(self, parameters):\n    with tf.name_scope(""accumulated_momentum""):\n      self.accumulated_momentum.extend(\n          utils.variable_like(p, trainable=False) for p in parameters)\n\n  def apply(self, updates: Sequence[types.ParameterUpdate],\n            parameters: Sequence[tf.Variable]):\n    """"""Applies updates to parameters.\n\n    By default it applies the momentum update rule for each update, parameter\n    pair:\n\n        accum_t <- momentum * accum_{t-1} + update\n        parameter <- parameter - learning_rate * accum_t\n\n    And when using Nesterov momentum (`use_nesterov=True`) it applies:\n\n        accum_t <- momentum * accum_{t-1} + update\n        parameter <- parameter - (learning_rate * update +\n        learning_rate * momentum * accum_t)\n\n    Args:\n      updates: A list of updates to apply to parameters. Updates are often\n        gradients as returned by `tf.GradientTape.gradient`.\n      parameters: A list of parameters. A parameter is a `tf.Variable`.\n\n    Raises:\n      ValueError: If `updates` and `parameters` are empty, have different\n        lengths, or have inconsistent types.\n    """"""\n    optimizer_utils.check_distribution_strategy()\n    optimizer_utils.check_updates_parameters(updates, parameters)\n    self._initialize(parameters)\n    for update, param, momentum_var in zip(updates, parameters,\n                                           self.accumulated_momentum):\n      if update is None:\n        continue\n\n      optimizer_utils.check_same_dtype(update, param)\n      learning_rate = tf.cast(self.learning_rate, update.dtype)\n      mu = tf.cast(self.momentum, update.dtype)\n\n      if isinstance(update, tf.IndexedSlices):\n        # Sparse read our state.\n        update, indices = optimizer_utils.deduplicate_indexed_slices(update)\n        momentum = momentum_var.sparse_read(indices)\n\n        # Compute and apply a sparse update to our parameter and state.\n        update, momentum = momentum_update(update, learning_rate, mu, momentum,\n                                           self.use_nesterov)\n        momentum_var.scatter_update(tf.IndexedSlices(momentum, indices))\n        param.scatter_sub(tf.IndexedSlices(update, indices))\n\n      else:\n        # Compute and apply a dense update.\n        update, momentum = momentum_update(update, learning_rate, mu,\n                                           momentum_var, self.use_nesterov)\n        momentum_var.assign(momentum)\n        param.assign_sub(update)\n'"
sonnet/src/optimizers/momentum_test.py,33,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.momentum.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom sonnet.src import test_utils\nfrom sonnet.src.optimizers import momentum as momentum_lib\nfrom sonnet.src.optimizers import optimizer_tests\nimport tensorflow as tf\n\nCONFIGS = optimizer_tests.named_product(learning_rate=(0.1, 0.01, 0.001),\n                                        momentum=(0.9, 0.5, 0.2),\n                                        use_nesterov=(True, False),\n                                        seed=(28, 2, 90))\n\n\nclass ComparisonTest(optimizer_tests.AbstractFuzzTest):\n  """"""Ensures Sonnet optimizers have equivalent behavior to TensorFlow.""""""\n\n  def _make_tf(self, learning_rate, momentum, use_nesterov):\n    optimizer = tf.optimizers.SGD(learning_rate=learning_rate,\n                                  momentum=momentum,\n                                  nesterov=use_nesterov)\n    return lambda g, p: optimizer.apply_gradients(zip(g, p))\n\n  def _make_snt(self, learning_rate, momentum, use_nesterov):\n    optimizer = momentum_lib.Momentum(learning_rate=learning_rate,\n                                      momentum=momentum,\n                                      use_nesterov=use_nesterov)\n    return optimizer.apply\n\n  @test_utils.combined_named_parameters(CONFIGS)\n  def testComparingSonnetAndTensorFlow(self, config):\n    seed = config.pop(""seed"")\n    self.assertParametersRemainClose(seed, config)\n\n\nclass MomentumTest(optimizer_tests.OptimizerTestBase):\n\n  def make_optimizer(self, **kwargs):\n    if ""learning_rate"" not in kwargs:\n      kwargs[""learning_rate""] = 0.1\n    if ""momentum"" not in kwargs:\n      kwargs[""momentum""] = 0.9\n    return momentum_lib.Momentum(**kwargs)\n\n  def testDense(self):\n    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]\n    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]\n    optimizer = self.make_optimizer(learning_rate=0.1, momentum=0.9)\n    # Step 1 of Momentum\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[0.5, 1.5], [2.7, 3.7]],\n                        [x.numpy() for x in parameters])\n    # Step 2 of Momentum\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[-0.45, 0.55], [2.13, 3.13]],\n                        [x.numpy() for x in parameters])\n    # Step 3 of Momentum\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[-1.805, -0.805], [1.317, 2.317]],\n                        [x.numpy() for x in parameters])\n\n  def testDenseNesterov(self):\n    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]\n    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]\n    optimizer = self.make_optimizer(\n        learning_rate=0.1, momentum=0.9, use_nesterov=True)\n    # Step 1 of Momentum\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[0.05, 1.05], [2.43, 3.43]],\n                        [x.numpy() for x in parameters])\n    # Step 2 of Momentum\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[-1.305, -0.305], [1.617, 2.617]],\n                        [x.numpy() for x in parameters])\n    # Step 3 of Momentum\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[-3.0245, -2.0245], [0.5853, 1.5853]],\n                        [x.numpy() for x in parameters])\n\n  def testSparse(self):\n    if self.primary_device in (""GPU"", ""TPU""):\n      self.skipTest(""IndexedSlices not supported on {}."".format(\n          self.primary_device))\n\n    parameters = [tf.Variable([[1.], [2.]]), tf.Variable([[3.], [4.]])]\n    updates = [\n        tf.IndexedSlices(\n            tf.constant([0.1], shape=[1, 1]), tf.constant([0]),\n            tf.constant([2, 1])),\n        tf.IndexedSlices(\n            tf.constant([0.01], shape=[1, 1]), tf.constant([1]),\n            tf.constant([2, 1]))\n    ]\n    optimizer = self.make_optimizer(learning_rate=3., momentum=0.9)\n    # Step 1 of Momentum\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[1.0 - 3.0 * 0.1], [2.0]], parameters[0].numpy())\n    self.assertAllClose([[3.0], [4.0 - 3.0 * 0.01]], parameters[1].numpy())\n    # Step 2 of Momentum\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[0.7 - 3.0 * 0.19], [2.0]], parameters[0].numpy())\n    self.assertAllClose([[3.0], [3.97 - 3.0 * 0.019]], parameters[1].numpy())\n    # Step 3 of Momentum\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[0.13 - 3.0 * 0.271], [2.0]], parameters[0].numpy())\n    self.assertAllClose([[3.0], [3.913 - 3.0 * 0.0271]], parameters[1].numpy())\n\n  def testSparseNesterov(self):\n    if self.primary_device in (""GPU"", ""TPU""):\n      self.skipTest(""IndexedSlices not supported on {}."".format(\n          self.primary_device))\n\n    parameters = [tf.Variable([[1.], [2.]]), tf.Variable([[3.], [4.]])]\n    updates = [\n        tf.IndexedSlices(\n            tf.constant([0.1], shape=[1, 1]), tf.constant([0]),\n            tf.constant([2, 1])),\n        tf.IndexedSlices(\n            tf.constant([0.01], shape=[1, 1]), tf.constant([1]),\n            tf.constant([2, 1]))\n    ]\n    optimizer = self.make_optimizer(\n        learning_rate=3., momentum=0.9, use_nesterov=True)\n    # Step 1 of Momentum\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[0.43], [2.0]], parameters[0].numpy())\n    self.assertAllClose([[3.0], [3.943]], parameters[1].numpy())\n    # Step 2 of Momentum\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[-0.383], [2.0]], parameters[0].numpy())\n    self.assertAllClose([[3.0], [3.8617]], parameters[1].numpy())\n    # Step 3 of Momentum\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[-1.4147], [2.0]], parameters[0].numpy())\n    self.assertAllClose([[3.0], [3.75853]], parameters[1].numpy())\n\n  def testVariableHyperParams(self):\n    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]\n    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]\n    learning_rate = tf.Variable(0.1)\n    momentum_coeff = tf.Variable(0.9)\n    optimizer = self.make_optimizer(\n        learning_rate=learning_rate, momentum=momentum_coeff)\n    if optimizer_tests.is_tf_optimizer(optimizer):\n      self.skipTest(""TF SGD optimizer doesn\'t support variable learning rate."")\n\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[0.5, 1.5], [2.7, 3.7]],\n                        [x.numpy() for x in parameters])\n    learning_rate.assign(0.01)\n    momentum_coeff.assign(0.09)\n    self.assertAlmostEqual(0.01, optimizer.learning_rate.numpy())\n    self.assertAlmostEqual(0.09, optimizer.momentum.numpy())\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[0.4455, 1.4455], [2.6673, 3.6673]],\n                        [x.numpy() for x in parameters])\n\n  def testHyperParamDTypeConversion(self):\n    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]\n    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]\n    dtype = tf.float32 if self.primary_device == ""TPU"" else tf.float64\n    learning_rate = tf.Variable(0.1, dtype=dtype)\n    momentum_coeff = tf.Variable(0.9, dtype=dtype)\n    optimizer = self.make_optimizer(\n        learning_rate=learning_rate, momentum=momentum_coeff)\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[0.5, 1.5], [2.7, 3.7]],\n                        [x.numpy() for x in parameters])\n\n  def testAuxVariablesColocatedWithOriginal(self):\n    optimizer = self.make_optimizer(learning_rate=0.1, momentum=0.9)\n    if optimizer_tests.is_tf_optimizer(optimizer):\n      self.skipTest(""TF slot variables are in a different location."")\n\n    with tf.device(""CPU:0""):\n      var = tf.Variable(1.0)\n    optimizer.apply([tf.constant(0.1)], [var])\n    self.assertEqual(optimizer.accumulated_momentum[0].device, var.device)\n\n\nclass ReferenceMomentumTest(MomentumTest):\n\n  def make_optimizer(self, **kwargs):\n    if ""learning_rate"" not in kwargs:\n      kwargs[""learning_rate""] = 0.1\n    if ""momentum"" not in kwargs:\n      kwargs[""momentum""] = 0.9\n    if ""use_nesterov"" in kwargs:\n      kwargs[""nesterov""] = kwargs[""use_nesterov""]\n      del kwargs[""use_nesterov""]\n    return optimizer_tests.WrappedTFOptimizer(tf.optimizers.SGD(**kwargs))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/optimizers/optimizer_tests.py,13,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Common tests for Sonnet optimizers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport itertools\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom sonnet.src import base\nfrom sonnet.src import test_utils\nimport tensorflow as tf\nimport tree\n\n\nclass WrappedTFOptimizer(base.Optimizer):\n  """"""Wraps a TF optimizer in the Sonnet API.""""""\n\n  wrapped = None\n\n  def __init__(self, optimizer: tf.optimizers.Optimizer):\n    super(WrappedTFOptimizer, self).__init__()\n    self.wrapped = optimizer\n\n  def __getattr__(self, name):\n    return getattr(self.wrapped, name)\n\n  def apply(self, updates, params):\n    self.wrapped.apply_gradients(zip(updates, params))\n\n\ndef is_tf_optimizer(optimizer):\n  return isinstance(optimizer, WrappedTFOptimizer)\n\n\nclass OptimizerTestBase(test_utils.TestCase):\n  """"""Common tests for Sonnet optimizers.""""""\n\n  def make_optimizer(self, *args, **kwargs):\n    raise NotImplementedError()\n\n  def testNoneUpdate(self):\n    parameters = [tf.Variable(1.), tf.Variable(2.)]\n    updates = [None, tf.constant(3.)]\n    optimizer = self.make_optimizer()\n    optimizer.apply(updates, parameters)\n    self.assertAllClose(1., parameters[0].numpy())\n\n  def testDifferentLengthUpdatesParams(self):\n    optimizer = self.make_optimizer()\n    if is_tf_optimizer(optimizer):\n      self.skipTest(""TF optimizers don\'t check the lenghs of params/updates."")\n\n    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]\n    updates = [tf.constant([5., 5.])]\n    with self.assertRaisesRegexp(\n        ValueError, ""`updates` and `parameters` must be the same length.""):\n      optimizer.apply(updates, parameters)\n\n  def testEmptyParams(self):\n    optimizer = self.make_optimizer()\n    if is_tf_optimizer(optimizer):\n      self.skipTest(""TF optimizers don\'t error on empty params."")\n\n    with self.assertRaisesRegexp(ValueError, ""`parameters` cannot be empty.""):\n      optimizer.apply([], [])\n\n  def testAllUpdatesNone(self):\n    parameters = [tf.Variable(1.), tf.Variable(2.)]\n    updates = [None, None]\n    optimizer = self.make_optimizer()\n    if is_tf_optimizer(optimizer):\n      msg = ""No gradients provided for any variable""\n    else:\n      msg = ""No updates provided for any parameter""\n    with self.assertRaisesRegexp(ValueError, msg):\n      optimizer.apply(updates, parameters)\n\n  def testInconsistentDTypes(self):\n    optimizer = self.make_optimizer()\n    if is_tf_optimizer(optimizer):\n      self.skipTest(""TF optimizers raise a cryptic error message here."")\n\n    parameters = [tf.Variable([1., 2.], name=""param0"")]\n    updates = [tf.constant([5, 5])]\n    with self.assertRaisesRegexp(\n        ValueError, ""DType of .* is not equal to that of parameter .*param0.*""):\n      optimizer.apply(updates, parameters)\n\n  def testUnsuppportedStrategyError(self):\n    strategy = tf.distribute.MirroredStrategy()\n    with strategy.scope():\n      parameters = [tf.Variable(1.0)]\n      updates = [tf.constant(0.1)]\n      optimizer = self.make_optimizer()\n      if is_tf_optimizer(optimizer):\n        self.skipTest(""TF optimizers aren\'t restricted to Sonnet strategies."")\n    with self.assertRaisesRegexp(\n        ValueError,\n        ""Sonnet optimizers are not compatible with `MirroredStrategy`""):\n      strategy.run(lambda: optimizer.apply(updates, parameters))\n\n\n# NOTE: Avoiding ABCMeta because of metaclass conflict.\nclass AbstractFuzzTest(test_utils.TestCase, parameterized.TestCase):\n  """"""Tests TF and Sonnet run concurrently produce equivalent output.""""""\n\n  def _make_tf(self, learning_rate, momentum, use_nesterov):\n    raise NotImplementedError()\n\n  def _make_snt(self, learning_rate, momentum, use_nesterov):\n    raise NotImplementedError()\n\n  def assertParametersRemainClose(self, seed, config, num_steps=100, atol=1e-4):\n    tf_opt = self._make_tf(**config)\n    snt_opt = self._make_snt(**config)\n\n    # TODO(tomhennigan) Add sparse data.\n    data = _generate_dense_data(seed, num_steps)\n    tf_params = _apply_optimizer(data, tf_opt)\n    snt_params = _apply_optimizer(data, snt_opt)\n    assert tf_params and len(tf_params) == len(snt_params)\n\n    for step, (tf_param, snt_param) in enumerate(zip(tf_params, snt_params)):\n      msg = ""TF and Sonnet diverged at step {}"".format(step)\n      for tf_p, snt_p in zip(tf_param, snt_param):\n        self.assertEqual(tf_p.shape, snt_p.shape)\n        self.assertEqual(tf_p.dtype, snt_p.dtype)\n        self.assertAllClose(tf_p, snt_p, atol=atol, msg=msg)\n\n\ndef _generate_dense_data(seed, num_steps):\n  """"""Generates deterministic random parameters and gradients.""""""\n  # Use numpy random since it is deterministic (unlike TF).\n  np.random.seed(seed=seed)\n  params = [\n      np.random.normal(size=(10, 10, 10)).astype(np.float32),\n      np.random.normal(size=(10, 10)).astype(np.float32),\n      np.random.normal(size=(10,)).astype(np.float32),\n  ]\n  per_step_grads = []\n  for _ in range(num_steps):\n    per_step_grads.append([\n        np.random.normal(size=(10, 10, 10)).astype(np.float32),\n        np.random.normal(size=(10, 10)).astype(np.float32),\n        np.random.normal(size=(10,)).astype(np.float32),\n    ])\n  return params, per_step_grads\n\n\ndef _apply_optimizer(data, apply_fn):\n  params, per_step_grads = data\n  params = [tf.Variable(p, name=""rank{}"".format(len(p.shape))) for p in params]\n  per_step_grads = tree.map_structure(tf.convert_to_tensor, per_step_grads)\n  param_vals = []\n  assert per_step_grads\n  for grads in per_step_grads:\n    apply_fn(grads, params)\n    param_vals.append([p.numpy() for p in params])\n  return param_vals\n\n\ndef named_product(**config):\n  keys = list(config.keys())\n  values = list(config.values())\n  configs = []\n  for val in itertools.product(*values):\n    config = dict(zip(keys, val))\n    name = "","".join(""{}={}"".format(k, v) for k, v in config.items())\n    configs.append((name, config))\n  return configs\n'"
sonnet/src/optimizers/optimizer_utils.py,9,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Utils for Sonnet optimizers.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nfrom sonnet.src import types\nfrom sonnet.src.distribute import replicator\nimport tensorflow as tf\nfrom typing import Sequence\n\n# Sonnet only supports a subset of distribution strategies since it makes use of\n# a simplified update model and replica local variables.\n# TODO(cjfj,petebu,tomhennigan) Add async parameter server strategy when needed.\n# TODO(cjfj,petebu,tomhennigan) Add sync multi-worker GPU strategy when needed.\n_SUPPORTED_STRATEGIES = (\n    tf.distribute.OneDeviceStrategy,\n    replicator.Replicator,\n    replicator.TpuReplicator,\n)\n\n\ndef check_distribution_strategy():\n  if tf.distribute.has_strategy():\n    strategy = tf.distribute.get_strategy()\n    if not isinstance(strategy, _SUPPORTED_STRATEGIES):\n      raise ValueError(""Sonnet optimizers are not compatible with `{}`. ""\n                       ""Please use one of `{}` instead."".format(\n                           strategy.__class__.__name__, ""`, `"".join(\n                               s.__name__ for s in _SUPPORTED_STRATEGIES)))\n\n\ndef check_updates_parameters(updates: Sequence[types.ParameterUpdate],\n                             parameters: Sequence[tf.Variable]):\n  if len(updates) != len(parameters):\n    raise ValueError(""`updates` and `parameters` must be the same length."")\n  if not parameters:\n    raise ValueError(""`parameters` cannot be empty."")\n  if all(x is None for x in updates):\n    raise ValueError(""No updates provided for any parameter."")\n\n\ndef check_same_dtype(update: types.ParameterUpdate, parameter: tf.Variable):\n  if update.dtype != parameter.dtype:\n    raise ValueError(\n        ""DType of update {!r} is not equal to that of parameter {!r}"".format(\n            update, parameter))\n\n\ndef deduplicate_indexed_slices(indexed_slice: tf.IndexedSlices):\n  """"""Sums `values` associated with any non-unique `indices`.\n\n  Args:\n    indexed_slice: An indexed slice with potentially duplicated indices.\n\n  Returns:\n    A tuple of (`summed_values`, `unique_indices`) where `unique_indices` is a\n    de-duplicated version of `indices` and `summed_values` contains the sum of\n    `values` slices associated with each unique index.\n  """"""\n  values, indices = indexed_slice.values, indexed_slice.indices\n  unique_indices, new_index_positions = tf.unique(indices)\n  summed_values = tf.math.unsorted_segment_sum(values, new_index_positions,\n                                               tf.shape(unique_indices)[0])\n  return summed_values, unique_indices\n'"
sonnet/src/optimizers/rmsprop.py,22,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""RMSProp module.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nimport six\n\nfrom sonnet.src import base\nfrom sonnet.src import once\nfrom sonnet.src import types\nfrom sonnet.src import utils\nfrom sonnet.src.optimizers import optimizer_utils\n\nimport tensorflow as tf\nfrom typing import Optional, Sequence, Text, Union\n\n\ndef rmsprop_update(update, decay, learning_rate, epsilon, mu, mom, ms, mg):\n  """"""Computes a single RMSProp update.""""""\n  ms = tf.square(update) * (1. - decay) + ms * decay\n  if mg is not None:  # centered\n    mg = update * (1. - decay) + mg * decay\n    denominator = ms - tf.square(mg) + epsilon\n  else:\n    denominator = ms + epsilon\n  mom = (mu * mom) + (learning_rate * update * tf.math.rsqrt(denominator))\n  return mom, ms, mg\n\n\nclass RMSProp(base.Optimizer):\n  """"""RMSProp module.\n\n  See: http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\n\n  Maintain a moving (discounted) average of the square of updates. Divides each\n  update by the root of this average.\n\n      ms <- decay * ms + (1-decay) * update^2\n      mom <- momentum * mom + learning_rate * update / sqrt(ms + epsilon)\n      parameter <- parameter - mom\n\n  This implementation of `RMSprop` uses plain momentum, not Nesterov momentum.\n\n  The centered version additionally maintains a moving average of the\n  gradients, and uses that average to estimate the variance:\n\n      mg <- decay * mg + (1-decay) * update\n      ms <- decay * ms + (1-decay) * update^2\n      mom <- momentum * mom + learning_rate * update / sqrt(ms - mg^2 + epsilon)\n      parameter <- parameter - mom\n\n  Attributes:\n    learning_rate: Learning rate.\n    decay: Learning rate decay over each update.\n    momentum: Momentum scalar.\n    epsilon: Small value to avoid zero denominator.\n    centered: `True` if centered.\n    mom: Accumulated mom for each parameter.\n    ms: Accumulated ms for each parameter.\n    mg: Accumulated mg for each parameter.\n  """"""\n\n  def __init__(self,\n               learning_rate: Union[types.FloatLike, tf.Variable],\n               decay: Union[types.FloatLike, tf.Variable] = 0.9,\n               momentum: Union[types.FloatLike, tf.Variable] = 0.0,\n               epsilon: Union[types.FloatLike, tf.Variable] = 1e-10,\n               centered: bool = False,\n               name: Optional[Text] = None):\n    """"""Constructs an `RMSProp` module.\n\n    Args:\n      learning_rate: Learning rate.\n      decay: Learning rate decay over each update.\n      momentum: Momentum scalar.\n      epsilon: Small value to avoid zero denominator.\n      centered: If True, gradients are normalized by the estimated variance of\n        the gradient; if False, by the uncentered second moment. Setting this to\n        True may help with training, but is slightly more expensive in terms of\n        computation and memory. Defaults to False.\n      name: Name for this module.\n    """"""\n    super(RMSProp, self).__init__(name)\n    self.learning_rate = learning_rate\n    self.decay = decay\n    self.momentum = momentum\n    self.epsilon = epsilon\n    self.centered = centered\n    self.mom = []\n    self.ms = []\n    self.mg = []\n\n  @once.once\n  def _initialize(self, parameters: Sequence[tf.Variable]):\n    zero_var = lambda p: utils.variable_like(p, trainable=False)\n    with tf.name_scope(""momentum""):\n      self.mom.extend(zero_var(p) for p in parameters)\n    with tf.name_scope(""rms""):\n      self.ms.extend(zero_var(p) for p in parameters)\n    if self.centered:\n      with tf.name_scope(""mg""):\n        self.mg.extend(zero_var(p) for p in parameters)\n\n  def apply(self, updates: Sequence[types.ParameterUpdate],\n            parameters: Sequence[tf.Variable]):\n    """"""Applies updates to parameters.\n\n    Args:\n      updates: A list of updates to apply to parameters. Updates are often\n        gradients as returned by `tf.GradientTape.gradient`.\n      parameters: A list of parameters.\n\n    Raises:\n      ValueError: If `updates` and `parameters` are empty, have different\n        lengths, or have inconsistent types.\n    """"""\n    optimizer_utils.check_distribution_strategy()\n    optimizer_utils.check_updates_parameters(updates, parameters)\n    self._initialize(parameters)\n    for update, parameter, mom_var, ms_var, mg_var in six.moves.zip_longest(\n        updates, parameters, self.mom, self.ms, self.mg):\n      if update is None:\n        continue\n\n      optimizer_utils.check_same_dtype(update, parameter)\n      learning_rate = tf.cast(self.learning_rate, update.dtype)\n      decay = tf.cast(self.decay, update.dtype)\n      mu = tf.cast(self.momentum, update.dtype)\n      epsilon = tf.cast(self.epsilon, update.dtype)\n\n      if isinstance(update, tf.IndexedSlices):\n        # Sparse read our state.\n        update, indices = optimizer_utils.deduplicate_indexed_slices(update)\n        ms = ms_var.sparse_read(indices)\n        mg = mg_var.sparse_read(indices) if self.centered else None\n        mom = mom_var.sparse_read(indices)\n\n        # Compute and apply a sparse update to our parameter and state.\n        mom, ms, mg = rmsprop_update(update, decay, learning_rate, epsilon, mu,\n                                     mom, ms, mg)\n        parameter.scatter_sub(tf.IndexedSlices(mom, indices))\n        mom_var.scatter_update(tf.IndexedSlices(mom, indices))\n        ms_var.scatter_update(tf.IndexedSlices(ms, indices))\n        if self.centered:\n          mg_var.scatter_update(tf.IndexedSlices(mg, indices))\n\n      else:\n        # Compute and apply a dense update to our parameters and state.\n        mom, ms, mg = rmsprop_update(update, decay, learning_rate, epsilon, mu,\n                                     mom_var, ms_var, mg_var)\n        parameter.assign_sub(mom)\n        mom_var.assign(mom)\n        ms_var.assign(ms)\n        if self.centered:\n          mg_var.assign(mg)\n'"
sonnet/src/optimizers/rmsprop_test.py,34,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.rmsprop.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom sonnet.src import test_utils\nfrom sonnet.src.optimizers import optimizer_tests\nfrom sonnet.src.optimizers import rmsprop\nimport tensorflow as tf\n\nCONFIGS = optimizer_tests.named_product(learning_rate=(0.01, 0.001),\n                                        decay=(0.8, 0.9),\n                                        momentum=(0.0, 0.5),\n                                        epsilon=(1e-7, 1e-8),\n                                        centered=(False, True),\n                                        seed=(28, 2, 90))\n\n\nclass ComparisonTest(optimizer_tests.AbstractFuzzTest):\n  """"""Ensures Sonnet optimizers have equivalent behavior to TensorFlow.""""""\n\n  def _make_tf(self, learning_rate, decay, momentum, epsilon, centered):\n    optimizer = tf.optimizers.RMSprop(learning_rate=learning_rate,\n                                      rho=decay,\n                                      momentum=momentum,\n                                      epsilon=epsilon,\n                                      centered=centered)\n    return lambda g, p: optimizer.apply_gradients(zip(g, p))\n\n  def _make_snt(self, learning_rate, decay, momentum, epsilon, centered):\n    optimizer = rmsprop.RMSProp(learning_rate=learning_rate,\n                                decay=decay,\n                                momentum=momentum,\n                                epsilon=epsilon,\n                                centered=centered)\n    return optimizer.apply\n\n  @test_utils.combined_named_parameters(CONFIGS)\n  def testComparingSonnetAndTensorFlow(self, config):\n    seed = config.pop(""seed"")\n    self.assertParametersRemainClose(seed, config, atol=1e-2)\n\n\nclass RMSPropTest(optimizer_tests.OptimizerTestBase):\n\n  def make_optimizer(self, **kwargs):\n    if ""learning_rate"" not in kwargs:\n      kwargs[""learning_rate""] = 0.1\n    return rmsprop.RMSProp(**kwargs)\n\n  def testDense(self):\n    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]\n    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]\n    optimizer = self.make_optimizer(learning_rate=0.1)\n    # Step 1 of RMSProp\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[0.683772, 1.683772], [2.683772, 3.683772]],\n                        [x.numpy() for x in parameters])\n    # Step 2 of RMSProp\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[0.454357, 1.454357], [2.454357, 3.454357]],\n                        [x.numpy() for x in parameters])\n    # Step 3 of RMSProp\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[0.262262, 1.262262], [2.262262, 3.262262]],\n                        [x.numpy() for x in parameters])\n\n  def testDenseCentered(self):\n    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]\n    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]\n    optimizer = self.make_optimizer(learning_rate=0.1, centered=True)\n    # Step 1 of RMSProp\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[0.666667, 1.666667], [2.666667, 3.666667]],\n                        [x.numpy() for x in parameters])\n    # Step 2 of RMSProp\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[0.41176, 1.41176], [2.41176, 3.41176]],\n                        [x.numpy() for x in parameters])\n    # Step 3 of RMSProp\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[0.186776, 1.186776], [2.186776, 3.186776]],\n                        [x.numpy() for x in parameters])\n\n  def testSparse(self):\n    if self.primary_device in (""GPU"", ""TPU""):\n      self.skipTest(""IndexedSlices not supported on {}."".format(\n          self.primary_device))\n\n    parameters = [tf.Variable([[1.], [2.]]), tf.Variable([[3.], [4.]])]\n    updates = [\n        tf.IndexedSlices(\n            tf.constant([0.1], shape=[1, 1]), tf.constant([0]),\n            tf.constant([2, 1])),\n        tf.IndexedSlices(\n            tf.constant([0.01], shape=[1, 1]), tf.constant([1]),\n            tf.constant([2, 1]))\n    ]\n    optimizer = self.make_optimizer(learning_rate=3.)\n    # Step 1 of RMSProp\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[-8.486831], [2.0]], parameters[0].numpy(), rtol=1e-4)\n    self.assertAllClose([[3.0], [-5.486784]], parameters[1].numpy(), rtol=1e-4)\n    # Step 2 of RMSProp\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[-15.369301], [2.0]], parameters[0].numpy(), rtol=1e-4)\n    self.assertAllClose([[3.0], [-12.369237]], parameters[1].numpy(), rtol=1e-4)\n    # Step 3 of RMSProp\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[-21.132141], [2.0]], parameters[0].numpy(), rtol=1e-4)\n    self.assertAllClose([[3.0], [-18.132067]], parameters[1].numpy(), rtol=1e-4)\n\n  def testSparseCentered(self):\n    if self.primary_device in (""GPU"", ""TPU""):\n      self.skipTest(""IndexedSlices not supported on {}."".format(\n          self.primary_device))\n\n    parameters = [tf.Variable([[1.], [2.]]), tf.Variable([[3.], [4.]])]\n    updates = [\n        tf.IndexedSlices(\n            tf.constant([0.1], shape=[1, 1]), tf.constant([0]),\n            tf.constant([2, 1])),\n        tf.IndexedSlices(\n            tf.constant([0.01], shape=[1, 1]), tf.constant([1]),\n            tf.constant([2, 1]))\n    ]\n    optimizer = self.make_optimizer(learning_rate=3., centered=True)\n    # Step 1 of RMSProp\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[-8.999999], [2.0]], parameters[0].numpy(), rtol=1e-4)\n    self.assertAllClose([[3.0], [-5.999944]], parameters[1].numpy(), rtol=1e-4)\n    # Step 2 of RMSProp\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[-16.64719], [2.0]], parameters[0].numpy(), rtol=1e-4)\n    self.assertAllClose([[3.0], [-13.647109]], parameters[1].numpy(), rtol=1e-4)\n    # Step 3 of RMSProp\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[-23.396709], [2.0]], parameters[0].numpy(), rtol=1e-4)\n    self.assertAllClose([[3.0], [-20.39661]], parameters[1].numpy(), rtol=1e-4)\n\n  def testVariableHyperParams(self):\n    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]\n    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]\n    learning_rate = tf.Variable(0.1)\n    optimizer = self.make_optimizer(learning_rate=learning_rate)\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[0.683772, 1.683772], [2.683772, 3.683772]],\n                        [x.numpy() for x in parameters])\n    learning_rate.assign(0.01)\n    self.assertAlmostEqual(0.01, optimizer.learning_rate.numpy())\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[0.660831, 1.660831], [2.660831, 3.660831]],\n                        [x.numpy() for x in parameters])\n\n  def testHyperParamDTypeConversion(self):\n    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]\n    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]\n    dtype = tf.float32 if self.primary_device == ""TPU"" else tf.float64\n    learning_rate = tf.Variable(0.1, dtype=dtype)\n    decay = tf.Variable(0.9, dtype=dtype)\n    momentum = tf.Variable(0.0, dtype=dtype)\n    epsilon = tf.Variable(1e-7, dtype=dtype)\n    optimizer = self.make_optimizer(\n        learning_rate=learning_rate,\n        decay=decay,\n        momentum=momentum,\n        epsilon=epsilon)\n    if optimizer_tests.is_tf_optimizer(optimizer):\n      self.skipTest(""TF optimizers don\'t support automatic casting."")\n\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[0.683772, 1.683772], [2.683772, 3.683772]],\n                        [x.numpy() for x in parameters])\n\n  def testAuxVariablesColocatedWithOriginal(self):\n    optimizer = self.make_optimizer(learning_rate=0.1)\n    if optimizer_tests.is_tf_optimizer(optimizer):\n      self.skipTest(""Aux vars are in a different location for TF optimizers."")\n\n    with tf.device(""CPU:0""):\n      var = tf.Variable(1.0)\n    optimizer.apply([tf.constant(0.1)], [var])\n    self.assertEqual(optimizer.mom[0].device, var.device)\n    self.assertEqual(optimizer.ms[0].device, var.device)\n\n\nclass ReferenceRMSPropTest(RMSPropTest):\n\n  def make_optimizer(self, **kwargs):\n    if ""learning_rate"" not in kwargs:\n      kwargs[""learning_rate""] = 0.1\n    kwargs[""rho""] = kwargs.pop(""decay"", 0.9)\n    return optimizer_tests.WrappedTFOptimizer(tf.optimizers.RMSprop(**kwargs))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/optimizers/sgd.py,6,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Stochastic Gradient Descent module.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n# from __future__ import google_type_annotations\nfrom __future__ import print_function\n\nfrom sonnet.src import base\nfrom sonnet.src import types\nfrom sonnet.src.optimizers import optimizer_utils\n\nimport tensorflow as tf\nfrom typing import Optional, Sequence, Text, Union\n\n\nclass SGD(base.Optimizer):\n  """"""Stochastic Gradient Descent (SGD) module.\n\n  Attributes:\n    learning_rate: Learning rate.\n  """"""\n\n  def __init__(self,\n               learning_rate: Union[types.FloatLike, tf.Variable],\n               name: Optional[Text] = None):\n    """"""Constructs an `SGD` module.\n\n    Args:\n      learning_rate: Learning rate.\n      name: Name of the module.\n    """"""\n    super(SGD, self).__init__(name)\n    self.learning_rate = learning_rate\n\n  def apply(self, updates: Sequence[types.ParameterUpdate],\n            parameters: Sequence[tf.Variable]):\n    """"""Applies updates to parameters.\n\n    Args:\n      updates: A list of updates to apply to parameters.  Updates are often\n        gradients as returned by `tf.GradientTape.gradient`.\n      parameters: A list of parameters.\n\n    Raises:\n      ValueError: If `updates` and `parameters` are empty, have different\n        lengths, or have inconsistent types.\n    """"""\n    optimizer_utils.check_distribution_strategy()\n    optimizer_utils.check_updates_parameters(updates, parameters)\n    for update, parameter in zip(updates, parameters):\n      if update is not None:\n        optimizer_utils.check_same_dtype(update, parameter)\n        learning_rate = tf.cast(self.learning_rate, update.dtype)\n        if isinstance(update, tf.IndexedSlices):\n          parameter.scatter_sub(\n              tf.IndexedSlices(update.values * learning_rate, update.indices))\n        else:\n          parameter.assign_sub(update * learning_rate)\n'"
sonnet/src/optimizers/sgd_test.py,18,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.sgd.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom sonnet.src.optimizers import optimizer_tests\nfrom sonnet.src.optimizers import sgd\nimport tensorflow as tf\n\n\nclass SGDTest(optimizer_tests.OptimizerTestBase):\n\n  def make_optimizer(self, *args, **kwargs):\n    if ""learning_rate"" not in kwargs:\n      kwargs[""learning_rate""] = 3.\n    return sgd.SGD(*args, **kwargs)\n\n  def testDense(self):\n    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]\n    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]\n    optimizer = self.make_optimizer(learning_rate=3.)\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[-14., -13.], [-6., -5.]],\n                        [x.numpy() for x in parameters])\n\n  def testSparse(self):\n    if self.primary_device == ""TPU"":\n      self.skipTest(""IndexedSlices not supported on TPU."")\n\n    parameters = [tf.Variable([[1.], [2.]]), tf.Variable([[3.], [4.]])]\n    updates = [\n        tf.IndexedSlices(\n            tf.constant([0.1], shape=[1, 1]), tf.constant([0]),\n            tf.constant([2, 1])),\n        tf.IndexedSlices(\n            tf.constant([0.01], shape=[1, 1]), tf.constant([1]),\n            tf.constant([2, 1]))\n    ]\n    optimizer = self.make_optimizer(learning_rate=3.)\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[1.0 - 3.0 * 0.1], [2.0]], parameters[0].numpy())\n    self.assertAllClose([[3.0], [4.0 - 3.0 * 0.01]], parameters[1].numpy())\n\n  def testVariableLearningRate(self):\n    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]\n    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]\n    learning_rate = tf.Variable(3.)\n    optimizer = self.make_optimizer(learning_rate=learning_rate)\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[-14., -13.], [-6., -5.]],\n                        [x.numpy() for x in parameters])\n    learning_rate.assign_sub(1.)\n    self.assertEqual(2., optimizer.learning_rate.numpy())\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[-24., -23.], [-12., -11.]],\n                        [x.numpy() for x in parameters])\n\n  def testLearningRateDTypeConversion(self):\n    parameters = [tf.Variable([1., 2.]), tf.Variable([3., 4.])]\n    updates = [tf.constant([5., 5.]), tf.constant([3., 3.])]\n    dtype = tf.int32 if self.primary_device == ""TPU"" else tf.int64\n    learning_rate = tf.Variable(3, dtype=dtype)\n    optimizer = self.make_optimizer(learning_rate=learning_rate)\n    optimizer.apply(updates, parameters)\n    self.assertAllClose([[-14., -13.], [-6., -5.]],\n                        [x.numpy() for x in parameters])\n\n\nclass ReferenceSGDTest(SGDTest):\n\n  def make_optimizer(self, *args, **kwargs):\n    if ""learning_rate"" not in kwargs:\n      kwargs[""learning_rate""] = 3.\n    return optimizer_tests.WrappedTFOptimizer(tf.optimizers.SGD(**kwargs))\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
sonnet/src/conformance/checkpoints/generate.py,1,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Binary to generate golden checkpoint tests.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport re\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\nfrom sonnet.src.conformance import goldens\nimport tensorflow as tf\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(""golden_dir"",\n                    ""sonnet/src/conformance/checkpoints/"",\n                    ""Directory where golden files are to be found."")\nflags.DEFINE_string(""filter"", "".*"", ""Filter to a specific golden by name."")\nflags.DEFINE_bool(""regenerate"", False,\n                  ""Whether to regnerate existing checkpoints."")\nflags.DEFINE_bool(""dry_run"", True, ""Whether to actually apply changes."")\n\n\ndef safe_mkdir(directory):\n  if FLAGS.dry_run:\n    logging.warning(""[DRY RUN] Would create %r"", directory)\n  else:\n    logging.info(""Creating %r"", directory)\n    os.mkdir(directory)\n\n\ndef safe_unlink(path):\n  if FLAGS.dry_run:\n    logging.warning(""[DRY RUN] Would delete %r"", path)\n  else:\n    logging.info(""Deleting %r"", path)\n    os.unlink(path)\n\n\ndef main(unused_argv):\n  del unused_argv\n\n  for _, name, cls in goldens.list_goldens():\n    if not re.match(FLAGS.filter, name):\n      continue\n\n    checkpoint_dir = os.path.join(FLAGS.golden_dir, name)\n    exists = os.path.exists(checkpoint_dir)\n    if exists and not FLAGS.regenerate:\n      logging.info(""Skipping %s since it exists and --regenerate=false"", name)\n      continue\n\n    logging.info(""Processing %s"", name)\n    if not exists:\n      safe_mkdir(checkpoint_dir)\n    else:\n      # Clear out old files.\n      for file_name in os.listdir(checkpoint_dir):\n        safe_unlink(os.path.join(checkpoint_dir, file_name))\n\n    # Create the module to checkpoint.\n    golden = cls()\n    module = golden.create_module()\n    golden.create_all_variables(module)\n    for var in module.variables:\n      var.assign(goldens.range_like(var))\n\n    # Create a checkpoint and save the values to it.\n    checkpoint = tf.train.Checkpoint(module=module)\n    if FLAGS.dry_run:\n      logging.warning(""[DRY RUN] Would save %r to %r"", module, checkpoint_dir)\n    else:\n      file_prefix = os.path.join(checkpoint_dir, ""checkpoint"")\n      logging.info(""Saving to checkpoint %s."", file_prefix)\n      checkpoint.save(file_prefix=file_prefix)\n\n\nif __name__ == ""__main__"":\n  app.run(main)\n'"
sonnet/src/nets/dnc/control.py,4,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""DNC Control Modules.\n\nThese modules receive input and output parameters for the memory access module.\nWe also alias external controllers in this module that are relevant, so they can\nbe specified by string name in the core config.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\n\nfrom sonnet.src import linear\nfrom sonnet.src import recurrent\nimport tensorflow as tf\n\n\ndef get_controller_ctor(controller_name):\n  """"""Returns the constructor for a givn controller name.""""""\n  if controller_name == \'LSTM\':\n    return recurrent.LSTM\n  elif controller_name == \'GRU\':\n    return recurrent.GRU\n  else:\n    # References for other controllers can be added here\n    return getattr(sys.modules[__name__], controller_name)\n\n\nclass FeedForward(recurrent.RNNCore):\n  """"""FeedForward controller module.\n\n\n  Single feedforward linear layer, wrapped as an RNN core for convenience. There\n  is no computation performed on the state.\n\n      y <- activation(linear(x))\n      s_t+1 <- s_t\n  """"""\n\n  def __init__(self,\n               hidden_size,\n               activation=tf.nn.tanh,\n               dtype=tf.float32,\n               name=None):\n    """"""Initializes the FeedForward Module.\n\n    Args:\n      hidden_size: number of hidden units in linear layer.\n      activation: op for output activations.\n      dtype: datatype of inputs to accept, defaults to tf.float32.\n      name: module name (default \'feed_forward\').\n    """"""\n    super(FeedForward, self).__init__(name=name)\n    self.linear = linear.Linear(hidden_size)\n    self.dtype = dtype\n    self._activation = activation\n\n  def __call__(self, inputs, prev_state):\n    """"""Connects the FeedForward controller to the graph.\n\n    Args:\n      inputs: 2D Tensor [batch_size, input_size] input_size needs to be\n        specified at construction time.\n      prev_state: dummy state, 2D tensor of size [batch_size, 1]\n\n    Returns:\n      output: 2D Tensor [batch_size, hidden_size].\n      next_state: the same dummy state passed in as an argument.\n    """"""\n    output = self.linear(inputs)\n    if self._activation is not None:\n      output = self._activation(output)\n    return output, prev_state\n\n  def initial_state(self, batch_size):\n    return tf.zeros([batch_size, 1], dtype=self.dtype)\n\n\ndef deep_core(control_name,\n              control_config,\n              num_layers=1,\n              skip_connections=True,\n              name=None):\n  """"""Constructs a deep control module.\n\n  Args:\n    control_name: Name of control module (e.g. ""LSTM"").\n    control_config: Dictionary containing the configuration for the modules.\n    num_layers: Number of layers.\n    skip_connections: Boolean that indicates whether to use skip connections.\n      See documenation for sonnet.DeepRnn in\n      //learning/deepmind/tensorflow/sonnet/python/modules/basic_rnn.py for more\n      information.\n    name: module name.\n\n  Returns:\n    Deep control module.\n  """"""\n  control_class = get_controller_ctor(control_name)\n  cores = [\n      control_class(name=\'{}_{}\'.format(control_name, i), **control_config)\n      for i in range(num_layers)\n  ]\n  if skip_connections:\n    return recurrent.deep_rnn_with_skip_connections(cores, name=name)\n  else:\n    return recurrent.DeepRNN(cores, name=name)\n'"
sonnet/src/nets/dnc/control_test.py,6,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.nets.dnc.control.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom sonnet.src import recurrent\nfrom sonnet.src import test_utils\nfrom sonnet.src.nets.dnc import control\nimport tensorflow as tf\nimport tree\n\n\nclass CoreTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters({\'constructor\': recurrent.LSTM},\n                            {\'constructor\': recurrent.GRU})\n  def testShape(self, constructor):\n    batch_size = 2\n    hidden_size = 4\n    input_size = 3\n    inputs = tf.random.uniform([batch_size, input_size])\n    rnn = constructor(hidden_size)\n    prev_state = rnn.initial_state(batch_size=batch_size)\n    output, next_state = rnn(inputs, prev_state)\n\n    tree.map_structure(lambda t1, t2: self.assertEqual(t1.shape, t2.shape),\n                       prev_state, next_state)\n    self.assertShapeEqual(np.zeros([batch_size, hidden_size]), output)\n\n\nclass FeedForwardTest(test_utils.TestCase):\n\n  def testShape(self):\n    batch_size = 2\n    hidden_size = 4\n    inputs = tf.random.uniform(shape=[batch_size, hidden_size])\n    rnn = control.FeedForward(hidden_size)\n    prev_state = rnn.initial_state(batch_size=batch_size)\n    output, next_state = rnn(inputs, prev_state)\n\n    output_shape = np.ndarray((batch_size, hidden_size))\n    state_shape = np.ndarray((batch_size, 1))\n\n    self.assertShapeEqual(output_shape, output)\n    self.assertShapeEqual(state_shape, next_state)\n\n  def testValues(self):\n    batch_size = 2\n    hidden_size = 4\n    input_size = 8\n    inputs = tf.random.uniform([batch_size, input_size])\n\n    rnn = control.FeedForward(hidden_size, activation=tf.identity)\n    prev_state = rnn.initial_state(batch_size=batch_size)\n    output, next_state = rnn(inputs, prev_state)\n\n    weight, bias = rnn.linear.w, rnn.linear.b\n\n    expected_output = np.dot(inputs.numpy(), weight.numpy()) + bias.numpy()\n\n    self.assertAllClose(output.numpy(), expected_output, atol=1e-2)\n    # State should remain at dummy value.\n    self.assertAllClose(prev_state.numpy(), next_state.numpy(), atol=5e-3)\n\n\nclass DeepCore(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters({\n      \'control_name\': \'LSTM\',\n      \'num_layers\': 1\n  }, {\n      \'control_name\': \'LSTM\',\n      \'num_layers\': 2\n  }, {\n      \'control_name\': \'GRU\',\n      \'num_layers\': 1\n  }, {\n      \'control_name\': \'GRU\',\n      \'num_layers\': 2\n  })\n  def testShape(self, control_name, num_layers):\n    batch_size = 5\n    input_size = 3\n    hidden_size = 7\n    control_config = {\'hidden_size\': hidden_size}\n    inputs = tf.random.uniform([batch_size, input_size])\n    rnn = control.deep_core(\n        num_layers=num_layers,\n        control_name=control_name,\n        control_config=control_config)\n    prev_state = rnn.initial_state(batch_size=batch_size)\n    output, next_state = rnn(inputs, prev_state)\n\n    # The deep_core concatenates the outputs of the individual cores.\n    output_shape = np.ndarray((batch_size, num_layers * hidden_size))\n    self.assertShapeEqual(output_shape, output)\n\n    tree.map_structure(lambda t1, t2: self.assertEqual(t1.shape, t2.shape),\n                       prev_state, next_state)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
sonnet/src/nets/dnc/read.py,3,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Read modules.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef read(memory,\n         weights,\n         squash_op=tf.nn.tanh,\n         squash_before_access=True,\n         squash_after_access=False):\n  """"""Read from the NTM memory.\n\n  Args:\n    memory: 3D Tensor [batch_size, memory_size, word_size].\n    weights: 3D Tensor [batch_size, num_reads, memory_size].\n    squash_op: op to perform squashing of memory or read word.\n    squash_before_access: squash memory before read, default True.\n    squash_after_access: squash read word, default False.\n\n  Returns:\n    3D Tensor [batch_size, num_reads, word_size].\n  """"""\n  with tf.name_scope(""read_memory""):\n    if squash_before_access:\n      squash_op(weights)\n    read_word = tf.matmul(weights, memory)\n    if squash_after_access:\n      read_word = squash_op(read_word)\n    return read_word\n'"
sonnet/src/nets/dnc/read_test.py,7,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.nets.dnc.read.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom sonnet.src import test_utils\nfrom sonnet.src.nets.dnc import read\nimport tensorflow as tf\n\n\nclass ReadTest(test_utils.TestCase):\n\n  def testShape(self):\n    batch_size = 4\n    num_reads = 2\n    memory_size = 5\n    word_size = 3\n\n    mem = tf.random.uniform([batch_size, memory_size, word_size])\n    weights = tf.random.uniform([batch_size, num_reads, memory_size])\n    values_read = read.read(mem, weights)\n    self.assertAllEqual(values_read.shape.as_list(),\n                        [batch_size, num_reads, word_size])\n\n  def testValues(self):\n    num_reads = 2\n    memory_size = 5\n    word_size = 3\n\n    # Random memory and weights (batch_size=1)\n    mem = tf.random.uniform([1, memory_size, word_size])\n    indices = np.random.randint(0, memory_size, size=num_reads)\n    # One-hot representation\n    read_weights = tf.constant(\n        np.expand_dims(np.eye(memory_size)[indices], axis=0), dtype=tf.float32)\n\n    read_values = read.read(mem, read_weights, squash_op=tf.identity)\n    self.assertAllClose(\n        mem.numpy()[0, indices, :], read_values.numpy()[0, ...], atol=2e-3)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
sonnet/src/nets/dnc/util.py,13,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""DNC util ops and modules.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nimport tree\n\n\ndef segment_dim(inputs, dim, shapes):\n  """"""Returns tuple of Tensors output from segmenting input Tensor along dim.\n\n  The returned tuple of Tensors produced by \'segmenting\' the Tensor along a\n  certain dimension can be transformed to specified shapes.\n\n  Example:\n      input_tensor = tf.placeholder([2, 14, 3])\n      one, two = segment_dim(input_tensor, dim=1,\n                             shapes=[TensorShape([3, 3]), TensorShape([5])])\n      # one is a [2, 3, 3, 3] Tensor and two is a [2, 5, 3] Tensor.\n\n  Args:\n    inputs: `Tensor` to segment.\n    dim: dimension of the Tensor to operate on. Negative numbers count back from\n      the end of the dimensions.\n    shapes: list of TensorShapes of the output \'segments\' to produce.\n\n  Returns:\n    Tuple with resulting Tensors.\n\n  Raises:\n    ValueError: if the dim used at initialization is invalid. The valid range is\n    (-d, d], where d is the number of dimensions of the input tensor.\n  """"""\n  inputs_shape = inputs.shape\n  ndims = inputs_shape.ndims\n  dynamic_shape = tf.shape(inputs)\n  shape_as_list = [\n      dynamic_shape[i] if s is None else s\n      for i, s in enumerate(inputs_shape.as_list())\n  ]\n\n  if dim >= ndims or dim < -ndims:\n    message = \'Invalid dims ({:d})\'.format(dim)\n    raise ValueError(message)\n\n  pre_shape = shape_as_list[:dim]\n  if dim == -1:\n    post_shape = []\n  else:\n    post_shape = shape_as_list[(dim + 1):]\n\n  slice_begin = [0] * ndims\n  slice_size = [-1] * ndims\n\n  segments = []\n  for shape in shapes:\n    num_elements = shape.num_elements()\n    slice_size[dim] = num_elements\n    flat_slice = tf.slice(inputs, slice_begin, slice_size)\n\n    final_shape = pre_shape + shape.as_list() + post_shape\n    segments.append(tf.reshape(flat_slice, final_shape))\n    slice_begin[dim] += num_elements\n\n  return tuple(segments)\n\n\ndef batch_invert_permutation(permutations):\n  """"""Returns batched `tf.invert_permutation` for every row in `permutations`.""""""\n  unpacked = tf.unstack(permutations)\n  inverses = [\n      tf.math.invert_permutation(permutation) for permutation in unpacked\n  ]\n  return tf.stack(inverses)\n\n\ndef batch_gather(values, indices):\n  """"""Returns batched `tf.gather` for every row in the input.""""""\n  unpacked = zip(tf.unstack(values), tf.unstack(indices))\n  result = [tf.gather(value, index) for value, index in unpacked]\n  return tf.stack(result)\n\n\ndef one_hot(length, index):\n  """"""Return an nd array of given `length` filled with 0s and a 1 at `index`.""""""\n  result = np.zeros(length)\n  result[index] = 1\n  return result\n\n\ndef apply_linear(inputs, linear_modules, activation=tf.identity):\n  """"""Computes linear, allowing for tuple inputs (processed in parallel).\n\n  If inputs is a tuple, the linear modules must be a tuple or list of the same\n  length.\n\n  Args:\n    inputs: tensor or list / tuple of 2 tensors.\n    linear_modules: sonnet module, or list / tuple of 2 sonnet modules.\n    activation: function to call as activation, default is identity.\n\n  Returns:\n    output Tensor from one / both linear modules.\n  """"""\n  tree.assert_same_structure(inputs, linear_modules)\n  if isinstance(inputs, (tuple, list)):\n    assert len(inputs) == len(linear_modules) == 2, (\n        \'if inputs is a list, must be length 2 and match length of linears\')\n    return apply_split_linear(\n        linear_modules[0],\n        linear_modules[1],\n        inputs[0],\n        inputs[1],\n        activation=activation)\n  else:\n    return activation(linear_modules(inputs))\n\n\ndef apply_split_linear(lin_module_1,\n                       lin_module_2,\n                       input1,\n                       input2,\n                       activation=None):\n  """"""Returns a linear output of two inputs, run independently and summed.""""""\n  output_1 = lin_module_1(input1)\n  output_2 = lin_module_2(input2)\n  summed_output = output_1 + output_2\n  if activation is not None:\n    summed_output = activation(summed_output)\n  return summed_output\n'"
sonnet/src/nets/dnc/util_test.py,24,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.nets.sdnc.util.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom absl.testing import parameterized\nimport numpy as np\nfrom sonnet.src import linear\nfrom sonnet.src import test_utils\nfrom sonnet.src.nets.dnc import util\nimport tensorflow as tf\nimport tree\n\n\nclass SegmentDimTest(test_utils.TestCase, parameterized.TestCase):\n\n  @parameterized.parameters(([2], [7]), ([], [7]), ([2], []), ([2], [7, 11]),\n                            ([2, 11], [7]))\n  def testShape(self, initial_shape, final_shape):\n    first_shape = tf.TensorShape([3, 3])\n    second_shape = tf.TensorShape([5])\n    segment_shapes = [first_shape, second_shape]\n\n    inputs_shape = (\n        initial_shape +\n        [first_shape.num_elements() + second_shape.num_elements()] +\n        final_shape)\n\n    inputs = tf.random.uniform(inputs_shape)\n    first, second = util.segment_dim(\n        inputs, dim=len(initial_shape), shapes=segment_shapes)\n    self.assertAllEqual(first.shape.as_list(),\n                        initial_shape + first_shape.as_list() + final_shape)\n    self.assertAllEqual(second.shape.as_list(),\n                        initial_shape + second_shape.as_list() + final_shape)\n\n  @parameterized.parameters(([2], [7]), ([], [7]), ([2], []), ([2], [7, 11]),\n                            ([2, 11], [7]))\n  def testShapeNegative(self, initial_shape, final_shape):\n    first_shape = tf.TensorShape([3, 3])\n    second_shape = tf.TensorShape([5])\n    segment_shapes = [first_shape, second_shape]\n\n    inputs_shape = (\n        initial_shape +\n        [first_shape.num_elements() + second_shape.num_elements()] +\n        final_shape)\n\n    inputs = tf.random.uniform(inputs_shape)\n    first, second = util.segment_dim(\n        inputs, dim=-len(final_shape) - 1, shapes=segment_shapes)\n    self.assertAllEqual(first.shape.as_list(),\n                        initial_shape + first_shape.as_list() + final_shape)\n    self.assertAllEqual(second.shape.as_list(),\n                        initial_shape + second_shape.as_list() + final_shape)\n\n  def testValues(self):\n    segment_shapes = [tf.TensorShape([2]), tf.TensorShape([3])]\n    inputs = tf.constant(\n        np.hstack([np.zeros((5, 2)), np.ones((5, 3))]), dtype=tf.float32)\n    first, second = util.segment_dim(inputs, dim=1, shapes=segment_shapes)\n\n    self.assertAllEqual(first.numpy(), np.zeros_like(first))\n    self.assertAllEqual(second.numpy(), np.ones_like(second))\n\n  def testInvalidDims(self):\n    segment_shapes = [tf.TensorShape([3]), tf.TensorShape([2])]\n    inputs = tf.random.uniform([5, 5])\n    with self.assertRaisesRegexp(ValueError, \'Invalid dims\'):\n      util.segment_dim(inputs, 3, segment_shapes)\n\n\nclass BatchInvertPermutationTest(test_utils.TestCase):\n\n  def testCorrectOutput(self):\n    # Tests that the _batch_invert_permutation function correctly inverts a\n    # batch of permutations.\n    batch_size = 5\n    length = 7\n\n    permutations = np.empty([batch_size, length], dtype=int)\n    for i in range(batch_size):\n      permutations[i] = np.random.permutation(length)\n\n    inverse = util.batch_invert_permutation(tf.constant(permutations, tf.int32))\n\n    inverse_np = inverse.numpy()\n    for i in range(batch_size):\n      for j in range(length):\n        self.assertEqual(permutations[i][inverse_np[i][j]], j)\n\n\nclass BatchGatherTest(test_utils.TestCase):\n\n  def testCorrectOutput(self):\n    values = np.array([[3, 1, 4, 1], [5, 9, 2, 6], [5, 3, 5, 7]])\n    indices = np.array([[1, 2, 0, 3], [3, 0, 1, 2], [0, 2, 1, 3]])\n    target = np.array([[1, 4, 3, 1], [6, 5, 9, 2], [5, 5, 3, 7]])\n    result = util.batch_gather(tf.constant(values), tf.constant(indices))\n    self.assertAllEqual(target, result)\n\n\nclass LinearTest(test_utils.TestCase, parameterized.TestCase):\n\n  def testLinearOutputOneModule(self):\n    batch_size = 4\n    input_size = 5\n    output_size = 3\n    lin_a = linear.Linear(output_size)\n    inputs = tf.random.uniform([batch_size, input_size])\n    output = util.apply_linear(inputs, lin_a, activation=tf.nn.tanh)\n\n    expected_output = np.tanh(\n        np.matmul(inputs.numpy(), lin_a.w.numpy()) + lin_a.b.numpy())\n    self.assertAllClose(expected_output, output.numpy(), atol=self.get_atol())\n\n  def testLinearOutputTwoModules(self):\n    batch_size = 4\n    input_size_a = 5\n    input_size_b = 6\n    output_size = 3\n    lin_a = linear.Linear(output_size, name=\'lin_a\')\n    lin_b = linear.Linear(output_size, name=\'lin_b\')\n    input_a = tf.random.uniform([batch_size, input_size_a])\n    input_b = tf.random.uniform([batch_size, input_size_b])\n    output = util.apply_linear((input_a, input_b), (lin_a, lin_b),\n                               activation=tf.nn.relu)\n    expected_output = np.maximum(\n        0, (np.matmul(input_a.numpy(), lin_a.w.numpy()) + lin_a.b.numpy() +\n            np.matmul(input_b.numpy(), lin_b.w.numpy()) + lin_b.b.numpy()))\n    self.assertAllClose(expected_output, output.numpy(), atol=self.get_atol())\n\n  def testDifferentOutputSizeBreaks(self):\n    batch_size = 4\n    input_size = 5\n    output_size_a = 6\n    output_size_b = 3\n\n    lin_a = linear.Linear(output_size_a, name=\'lin_a\')\n    lin_b = linear.Linear(output_size_b, name=\'lin_b\')\n    input_a = tf.random.uniform([batch_size, input_size])\n    input_b = tf.random.uniform([batch_size, input_size])\n    with self.assertRaisesRegexp(tf.errors.InvalidArgumentError,\n                                 \'Incompatible shapes\'):\n      util.apply_linear((input_a, input_b), (lin_a, lin_b))\n\n  @parameterized.parameters(\n      {\n          \'input_sizes\': 4,\n          \'module_hidden_sizes\': (2, 3)\n      },\n      {\n          \'input_sizes\': (5, 7),\n          \'module_hidden_sizes\': 10\n      },\n  )\n  def testNonMatchingStructureBreaks(self, input_sizes, module_hidden_sizes):\n    batch_size = 16\n    inputs = tree.map_structure(\n        lambda size: tf.random.uniform([batch_size, size]), input_sizes)\n    modules = tree.map_structure(linear.Linear, module_hidden_sizes)\n\n    with self.assertRaisesRegexp(ValueError,\n                                 \'don\\\'t have the same nested structure\'):\n      util.apply_linear(inputs, modules)\n\n  @parameterized.parameters(\n      # Even when list length matches, len must be 2\n      {\n          \'input_sizes\': [10] * 3,\n          \'module_hidden_sizes\': [3] * 3\n      },\n      {\n          \'input_sizes\': [1],\n          \'module_hidden_sizes\': [4]\n      })\n  def testListMustBeLengthTwo(self, input_sizes, module_hidden_sizes):\n    batch_size = 16\n    inputs = tree.map_structure(\n        lambda size: tf.random.uniform([batch_size, size]), input_sizes)\n    modules = tree.map_structure(linear.Linear, module_hidden_sizes)\n\n    with self.assertRaisesRegexp(AssertionError, \'must be length 2\'):\n      util.apply_linear(inputs, modules)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
sonnet/src/nets/dnc/write.py,12,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Write modules.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n\ndef additive_write(memory, address, values):\n  """"""Additively writes values to memory at given address.\n\n  M_t = M_{t-1} + w_t a_t^T.\n\n  Args:\n    memory: 3D Tensor [batch_size, memory_size, word_size].\n    address: 3D Tensor [batch_size, num_writes, memory_size].\n    values: 3D Tensor [batch_size, num_writes, word_size].\n\n  Returns:\n    3D Tensor [batch_size, num_reads, word_size].\n  """"""\n  with tf.name_scope(\'write_memory\'):\n    add_matrix = tf.matmul(address, values, adjoint_a=True)\n    return memory + add_matrix\n\n\ndef erase(memory, address, reset_weights):\n  """"""Erases rows over addressing distribution by given reset word weights.\n\n  M_t(i) = M_{t-1}(i) * (1 - w_t(i) * e_t)\n\n  The erase is defined as a component-wise OR over reset strengths between\n  write heads, followed by a componentwise multiplication. The reset weights\n  contains values in [0, 1] where 1 indicates a complete reset. The reset\n  weights are granular over a word, allowing for part of the word to be erased.\n\n  Args:\n    memory: 3D Tensor [batch_size, memory_size, word_size].\n    address: 3D Tensor [batch_size, num_writes, memory_size].\n    reset_weights: 3D Tensor [batch_size, num_writes, word_size].\n\n  Returns:\n    erased memory: 3D Tensor [batch_size, num_reads, word_size].\n  """"""\n\n  with tf.name_scope(\'erase_memory\'):\n    address = tf.expand_dims(address, 3)\n    reset_weights = tf.expand_dims(reset_weights, 2)\n    weighted_resets = address * reset_weights\n    reset_gate = tf.reduce_prod(1 - weighted_resets, [1])\n    return memory * reset_gate\n\n\ndef erase_rows(memory, address, reset_row_weights):\n  """"""Erases rows over addressing distribution by given reset weight.\n\n  The reset row weight here is uniform over the values in a word.\n\n  Args:\n    memory: 3D Tensor [batch_size, memory_size, word_size].\n    address: 3D Tensor [batch_size, num_writes, memory_size].\n    reset_row_weights: 2D Tensor [batch_size, num_writes].\n\n  Returns:\n    3d Tensor of memory [batch_size, memory_size, word_size].\n  """"""\n\n  with tf.name_scope(\'erase_rows\'):\n    # Expands reset_row_weights for broadcasted cmul with address.\n    reset_row_weights = tf.expand_dims(reset_row_weights, -1)\n    weighted_resets = tf.multiply(address, reset_row_weights)\n    reset_gate = tf.reduce_prod(1 - weighted_resets, axis=[1])\n    # Expands reset_gate for broadcasted cmul with memory.\n    reset_gate = tf.expand_dims(reset_gate, -1)\n    return tf.multiply(memory, reset_gate)\n\n\ndef erase_and_write(memory, address, reset_weights, values):\n  """"""Module to erase and write in the NTM memory.\n\n  Implementation is based on equations (3) and (4) from \'Neural Turing Machines\'\n  (https://arxiv.org/pdf/1410.5401.pdf) by gravesa@, gregwayne@ and danihelka@:\n\n  Erase operation:\n    M_t\'(i) = M_{t-1}(i) * (1 - w_t(i) * e_t)\n\n  Add operation:\n    M_t(i) = M_t\'(i) + w_t(i) * a_t\n\n  where e are the reset_weights, w the write weights and a the values.\n\n  Args:\n    memory: 3D Tensor [batch_size, memory_size, word_size].\n    address: 3D Tensor [batch_size, num_writes, memory_size].\n    reset_weights: 3D Tensor [batch_size, num_writes, word_size].\n    values: 3D Tensor [batch_size, num_writes, word_size].\n\n  Returns:\n    3D Tensor [batch_size, num_reads, word_size].\n  """"""\n  memory = erase(memory, address, reset_weights)\n  memory = additive_write(memory, address, values)\n  return memory\n'"
sonnet/src/nets/dnc/write_test.py,31,"b'# Copyright 2019 The Sonnet Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ============================================================================\n""""""Tests for sonnet.v2.src.nets.dnc.write.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom sonnet.src import test_utils\nfrom sonnet.src.nets.dnc import write\nimport tensorflow as tf\n\n\nclass EraseRowsTest(test_utils.TestCase):\n\n  def testShape(self):\n    batch_size = 16\n    num_writes = 2\n    memory_size = 5\n    word_size = 3\n\n    mem = tf.random.uniform([batch_size, memory_size, word_size])\n    write_address = tf.random.uniform([batch_size, num_writes, memory_size])\n    reset_row_weights = tf.random.uniform([batch_size, num_writes])\n    eraser = write.erase_rows(mem, write_address, reset_row_weights)\n    self.assertAllEqual(eraser.shape.as_list(),\n                        [batch_size, memory_size, word_size])\n\n  def testValues(self):\n    num_writes = 2\n    memory_size = 5\n    word_size = 3\n\n    # Random memory, weights and values (batch_size=1)\n    mem = tf.random.uniform((1, memory_size, word_size))\n    mem_np = mem.numpy()\n    # Non-repeated indices in [0, memory_size)\n    perm = np.random.permutation(memory_size)\n    indices_np = perm[:num_writes]\n    excluded_indices_np = perm[num_writes:]\n\n    # One-hot representation\n    write_address = tf.constant(\n        np.expand_dims(np.eye(memory_size)[indices_np], axis=0),\n        dtype=tf.float32)\n    reset_row_weights = tf.ones((1, num_writes))\n\n    erased_mem = write.erase_rows(mem, write_address, reset_row_weights)\n\n    not_erased_mem = write.erase_rows(mem, write_address, reset_row_weights * 0)\n\n    erased_mem_np = erased_mem.numpy()\n\n    # Rows specified in indices should have been erased.\n    self.assertAllClose(\n        erased_mem_np[0, indices_np, :],\n        np.zeros((num_writes, word_size)),\n        atol=2e-3)\n\n    # Other rows should not have been erased.\n    self.assertAllClose(\n        erased_mem_np[0, excluded_indices_np, :],\n        mem_np[0, excluded_indices_np, :],\n        atol=2e-3)\n\n    # Write with reset weights zero\'d out and nothing should change.\n    self.assertAllEqual(not_erased_mem.numpy(), mem_np)\n\n\nclass EraseTest(test_utils.TestCase):\n\n  def testShape(self):\n    batch_size = 1\n    num_writes = 2\n    memory_size = 5\n    word_size = 3\n\n    mem = tf.random.uniform([batch_size, memory_size, word_size])\n    write_address = tf.random.uniform([batch_size, num_writes, memory_size])\n    reset_weights = tf.random.uniform([batch_size, num_writes, word_size])\n    writer = write.erase(mem, write_address, reset_weights)\n    self.assertTrue(writer.shape.as_list(),\n                    [batch_size, memory_size, word_size])\n\n  def testValues(self):\n    num_writes = 2\n    memory_size = 5\n    word_size = 3\n\n    # Random memory, weights and values (batch_size=1)\n    mem = tf.random.uniform([1, memory_size, word_size])\n    mem_np = mem.numpy()\n    # Non-repeated indices in [0, memory_size)\n    perm = np.random.permutation(memory_size)\n    indices = perm[:num_writes]\n    excluded_indices = perm[num_writes:]\n    # One-hot representation\n    write_address = tf.constant(\n        np.expand_dims(np.eye(memory_size)[indices], axis=0), dtype=tf.float32)\n    reset_weights = tf.ones([1, num_writes, word_size])\n    erased_mem = write.erase(mem, write_address, reset_weights)\n    not_erased_mem = write.erase(mem, write_address, reset_weights * 0.)\n\n    erased_mem_np = erased_mem.numpy()\n    not_erased_mem_np = not_erased_mem.numpy()\n\n    # Rows specified in indices should have been erased.\n    self.assertAllClose(\n        erased_mem_np[0, indices, :],\n        np.zeros((num_writes, word_size)),\n        atol=2e-3)\n\n    # Other rows should not have been erased.\n    self.assertAllClose(\n        erased_mem_np[0, excluded_indices, :],\n        mem_np[0, excluded_indices, :],\n        atol=2e-3)\n\n    # Write with reset weights zero\'d out and nothing should change.\n    self.assertAllEqual(not_erased_mem_np, mem_np)\n\n\nclass EraseAndWriteTest(test_utils.TestCase):\n\n  def testShape(self):\n    batch_size = 4\n    num_writes = 2\n    memory_size = 5\n    word_size = 3\n\n    mem = tf.random.uniform([batch_size, memory_size, word_size])\n    write_address = tf.random.uniform([batch_size, num_writes, memory_size])\n    reset_weights = tf.random.uniform([batch_size, num_writes, word_size])\n    values = tf.random.uniform([batch_size, num_writes, word_size])\n    writer = write.erase_and_write(mem, write_address, reset_weights, values)\n    self.assertTrue(writer.shape.as_list(),\n                    [batch_size, memory_size, word_size])\n\n  def testValues(self):\n    batch_size = 4\n    num_writes = 2\n    memory_size = 5\n    word_size = 3\n\n    # Random memory, weights and values (batch_size=1)\n    mem = tf.random.uniform((batch_size, memory_size, word_size))\n    # Non-repeated indices in [0, memory_size)\n    indices = np.random.permutation(memory_size)[:num_writes]\n    # One-hot representation\n    write_address = tf.constant(\n        np.tile(np.eye(memory_size)[indices], [batch_size, 1, 1]),\n        dtype=tf.float32)\n\n    reset_weights = tf.ones((batch_size, num_writes, word_size), 1)\n    write_values = tf.random.uniform([batch_size, num_writes, word_size])\n\n    written_mem = write.erase_and_write(mem, write_address, reset_weights,\n                                        write_values)\n\n    self.assertAllClose(\n        written_mem.numpy()[0, indices, :], write_values.numpy()[0], atol=2e-3)\n\n\nclass AdditiveWriteTest(test_utils.TestCase):\n\n  def testShape(self):\n    batch_size = 4\n    num_writes = 2\n    memory_size = 5\n    word_size = 3\n\n    mem = tf.random.uniform([batch_size, memory_size, word_size])\n    write_address = tf.random.uniform([batch_size, num_writes, memory_size])\n    values = tf.random.uniform([batch_size, num_writes, word_size])\n    writer = write.additive_write(mem, write_address, values)\n    self.assertAllEqual(writer.shape.as_list(),\n                        [batch_size, memory_size, word_size])\n\n  def testValues(self):\n    num_writes = 2\n    memory_size = 5\n    word_size = 3\n\n    # Random memory, address and values (batch_size=1)\n    mem = tf.random.uniform([1, memory_size, word_size])\n    mem_np = mem.numpy()\n    # Non-repeated indices in [0, memory_size)\n    indices = np.random.permutation(memory_size)[:num_writes]\n    # One-hot representation\n    write_address = tf.constant(\n        np.expand_dims(np.eye(memory_size)[indices], axis=0), dtype=tf.float32)\n    write_values = tf.random.uniform([1, num_writes, word_size])\n    write_values_np = write_values.numpy()\n\n    written_mem = write.additive_write(mem, write_address, write_values)\n    not_written_mem = write.additive_write(mem, write_address * 0, write_values)\n\n    written_mem_np = written_mem.numpy()\n    not_written_mem_np = not_written_mem.numpy()\n\n    # Check values have been correctly written\n    self.assertAllClose(\n        written_mem.numpy()[0, indices, :],\n        write_values_np[0] + mem_np[0, indices, :],\n        atol=2e-3)\n    # Check all other values in the memory are still what they started as\n    written_mem_copy = written_mem_np.copy()\n    written_mem_copy[0, indices, :] -= write_values_np[0]\n    self.assertAllClose(written_mem_copy, mem_np, atol=2e-3)\n\n    self.assertAllClose(not_written_mem_np, mem_np, atol=2e-3)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
