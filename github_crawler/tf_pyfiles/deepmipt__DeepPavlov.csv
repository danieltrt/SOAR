file_path,api_count,code
setup.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport re\n\nfrom setuptools import setup, find_packages\n\n__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\n\nmeta_path = os.path.join(__location__, \'deeppavlov\', \'_meta.py\')\nwith open(meta_path) as meta:\n    exec(meta.read())\n\n\ndef read_requirements():\n    """"""parses requirements from requirements.txt""""""\n    reqs_path = os.path.join(__location__, \'requirements.txt\')\n    with open(reqs_path, encoding=\'utf8\') as f:\n        reqs = [line.strip() for line in f if not line.strip().startswith(\'#\')]\n\n    names = []\n    links = []\n    for req in reqs:\n        if \'://\' in req:\n            links.append(req)\n        else:\n            names.append(req)\n    return {\'install_requires\': names, \'dependency_links\': links}\n\n\ndef readme():\n    with open(os.path.join(__location__, \'README.md\'), encoding=\'utf8\') as f:\n        text = f.read()\n    text = re.sub(r\']\\((?!https?://)\', r\'](https://github.com/deepmipt/DeepPavlov/blob/master/\', text)\n    text = re.sub(r\'\\ssrc=""(?!https?://)\', r\' src=""https://raw.githubusercontent.com/deepmipt/DeepPavlov/master/\', text)\n    return text\n\n\nif __name__ == \'__main__\':\n    setup(\n        name=\'deeppavlov\',\n        packages=find_packages(exclude=(\'tests\', \'docs\', \'utils\')),\n        version=__version__,\n        description=__description__,\n        long_description=readme(),\n        long_description_content_type=\'text/markdown\',\n        author=__author__,\n        author_email=__email__,\n        license=__license__,\n        url=\'https://github.com/deepmipt/DeepPavlov\',\n        download_url=f\'https://github.com/deepmipt/DeepPavlov/archive/{__version__}.tar.gz\',\n        keywords=__keywords__,\n        include_package_data=True,\n        extras_require={\n            \'tests\': [\n                \'flake8\',\n                \'pytest\',\n                \'pexpect\'],\n            \'docs\': [\n                \'sphinx>=1.7.9\',\n                \'sphinx_rtd_theme>=0.4.0\',\n                \'nbsphinx>=0.3.4\',\n                \'ipykernel>=4.8.0\'\n            ]},\n        **read_requirements()\n    )\n'"
deeppavlov/__init__.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\nfrom pathlib import Path\n\nfrom ._meta import __author__, __description__, __email__, __keywords__, __license__, __version__\nfrom .configs import configs\nfrom .core.commands.infer import build_model\nfrom .core.commands.train import train_evaluate_model_from_config\nfrom .core.common.chainer import Chainer\nfrom .core.common.log import init_logger\nfrom .download import deep_download\n\n\n# TODO: make better\ndef train_model(config: [str, Path, dict], download: bool = False, recursive: bool = False) -> Chainer:\n    train_evaluate_model_from_config(config, download=download, recursive=recursive)\n    return build_model(config, load_trained=True)\n\n\ndef evaluate_model(config: [str, Path, dict], download: bool = False, recursive: bool = False) -> dict:\n    return train_evaluate_model_from_config(config, to_train=False, download=download, recursive=recursive)\n\n\n# check version\nassert sys.hexversion >= 0x3060000, \'Does not work in python3.5 or lower\'\n\n# resolve conflicts with previous DeepPavlov installations versioned up to 0.0.9\ndot_dp_path = Path(\'~/.deeppavlov\').expanduser().resolve()\nif dot_dp_path.is_file():\n    dot_dp_path.unlink()\n\n# initiate logging\ninit_logger()\n'"
deeppavlov/__main__.py,0,"b""if __name__ == '__main__':\n    from .deep import main\n\n    main()\n"""
deeppavlov/_meta.py,0,"b""__version__ = '0.10.0'\n__author__ = 'Neural Networks and Deep Learning lab, MIPT'\n__description__ = 'An open source library for building end-to-end dialog systems and training chatbots.'\n__keywords__ = ['NLP', 'NER', 'SQUAD', 'Intents', 'Chatbot']\n__license__ = 'Apache License, Version 2.0'\n__email__ = 'info@deeppavlov.ai'\n"""
deeppavlov/deep.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nfrom logging import getLogger\n\nfrom deeppavlov.core.commands.infer import interact_model, predict_on_stream\nfrom deeppavlov.core.commands.train import train_evaluate_model_from_config\nfrom deeppavlov.core.common.cross_validation import calc_cv_score\nfrom deeppavlov.core.common.file import find_config\nfrom deeppavlov.download import deep_download\nfrom deeppavlov.utils.agent import start_rabbit_service\nfrom deeppavlov.utils.alexa import start_alexa_server\nfrom deeppavlov.utils.alice import start_alice_server\nfrom deeppavlov.utils.ms_bot_framework import start_ms_bf_server\nfrom deeppavlov.utils.pip_wrapper import install_from_config\nfrom deeppavlov.utils.server import start_model_server\nfrom deeppavlov.utils.socket import start_socket_server\nfrom deeppavlov.utils.telegram import interact_model_by_telegram\n\nlog = getLogger(__name__)\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(""mode"", help=""select a mode, train or interact"", type=str,\n                    choices={\'train\', \'evaluate\', \'interact\', \'predict\', \'telegram\', \'msbot\', \'alexa\', \'alice\',\n                             \'riseapi\', \'risesocket\', \'agent-rabbit\', \'download\', \'install\', \'crossval\'})\nparser.add_argument(""config_path"", help=""path to a pipeline json config"", type=str)\n\nparser.add_argument(""-e"", ""--start-epoch-num"", dest=""start_epoch_num"", default=None,\n                    help=""Start epoch number"", type=int)\nparser.add_argument(""--recursive"", action=""store_true"", help=""Train nested configs"")\n\nparser.add_argument(""-b"", ""--batch-size"", dest=""batch_size"", default=None, help=""inference batch size"", type=int)\nparser.add_argument(""-f"", ""--input-file"", dest=""file_path"", default=None, help=""Path to the input file"", type=str)\nparser.add_argument(""-d"", ""--download"", action=""store_true"", help=""download model components"")\n\nparser.add_argument(""--folds"", help=""number of folds"", type=int, default=5)\n\nparser.add_argument(""-t"", ""--token"", default=None, help=""telegram bot token"", type=str)\n\nparser.add_argument(""-i"", ""--ms-id"", default=None, help=""microsoft bot framework app id"", type=str)\nparser.add_argument(""-s"", ""--ms-secret"", default=None, help=""microsoft bot framework app secret"", type=str)\n\nparser.add_argument(""--https"", action=""store_true"", default=None, help=""run model in https mode"")\nparser.add_argument(""--key"", default=None, help=""ssl key"", type=str)\nparser.add_argument(""--cert"", default=None, help=""ssl certificate"", type=str)\n\nparser.add_argument(""-p"", ""--port"", default=None, help=""api port"", type=int)\n\nparser.add_argument(""--socket-type"", default=""TCP"", type=str, choices={""TCP"", ""UNIX""})\nparser.add_argument(""--socket-file"", default=""/tmp/deeppavlov_socket.s"", type=str)\n\nparser.add_argument(""-sn"", ""--service-name"", default=None, help=""service name for agent-rabbit mode"", type=str)\nparser.add_argument(""-an"", ""--agent-namespace"", default=None, help=""dp-agent namespace name"", type=str)\nparser.add_argument(""-ul"", ""--utterance-lifetime"", default=None, help=""message expiration in seconds"", type=int)\nparser.add_argument(""-rh"", ""--rabbit-host"", default=None, help=""RabbitMQ server host"", type=str)\nparser.add_argument(""-rp"", ""--rabbit-port"", default=None, help=""RabbitMQ server port"", type=int)\nparser.add_argument(""-rl"", ""--rabbit-login"", default=None, help=""RabbitMQ server login"", type=str)\nparser.add_argument(""-rpwd"", ""--rabbit-password"", default=None, help=""RabbitMQ server password"", type=str)\nparser.add_argument(""-rvh"", ""--rabbit-virtualhost"", default=None, help=""RabbitMQ server virtualhost"", type=str)\n\n\ndef main():\n    args = parser.parse_args()\n    pipeline_config_path = find_config(args.config_path)\n\n    if args.download or args.mode == \'download\':\n        deep_download(pipeline_config_path)\n\n    if args.mode == \'train\':\n        train_evaluate_model_from_config(pipeline_config_path,\n                                         recursive=args.recursive,\n                                         start_epoch_num=args.start_epoch_num)\n    elif args.mode == \'evaluate\':\n        train_evaluate_model_from_config(pipeline_config_path, to_train=False, start_epoch_num=args.start_epoch_num)\n    elif args.mode == \'interact\':\n        interact_model(pipeline_config_path)\n    elif args.mode == \'telegram\':\n        interact_model_by_telegram(model_config=pipeline_config_path, token=args.token)\n    elif args.mode == \'msbot\':\n        start_ms_bf_server(model_config=pipeline_config_path,\n                           app_id=args.ms_id,\n                           app_secret=args.ms_secret,\n                           port=args.port,\n                           https=args.https,\n                           ssl_key=args.key,\n                           ssl_cert=args.cert)\n    elif args.mode == \'alexa\':\n        start_alexa_server(model_config=pipeline_config_path,\n                           port=args.port,\n                           https=args.https,\n                           ssl_key=args.key,\n                           ssl_cert=args.cert)\n    elif args.mode == \'alice\':\n        start_alice_server(model_config=pipeline_config_path,\n                           port=args.port,\n                           https=args.https,\n                           ssl_key=args.key,\n                           ssl_cert=args.cert)\n    elif args.mode == \'riseapi\':\n        start_model_server(pipeline_config_path, args.https, args.key, args.cert, port=args.port)\n    elif args.mode == \'risesocket\':\n        start_socket_server(pipeline_config_path, args.socket_type, port=args.port, socket_file=args.socket_file)\n    elif args.mode == \'agent-rabbit\':\n        start_rabbit_service(model_config=pipeline_config_path,\n                             service_name=args.service_name,\n                             agent_namespace=args.agent_namespace,\n                             batch_size=args.batch_size,\n                             utterance_lifetime_sec=args.utterance_lifetime,\n                             rabbit_host=args.rabbit_host,\n                             rabbit_port=args.rabbit_port,\n                             rabbit_login=args.rabbit_login,\n                             rabbit_password=args.rabbit_password,\n                             rabbit_virtualhost=args.rabbit_virtualhost)\n    elif args.mode == \'predict\':\n        predict_on_stream(pipeline_config_path, args.batch_size, args.file_path)\n    elif args.mode == \'install\':\n        install_from_config(pipeline_config_path)\n    elif args.mode == \'crossval\':\n        if args.folds < 2:\n            log.error(\'Minimum number of Folds is 2\')\n        else:\n            calc_cv_score(pipeline_config_path, n_folds=args.folds, is_loo=False)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
deeppavlov/download.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport shutil\nimport sys\nfrom argparse import ArgumentParser, Namespace\nfrom collections import defaultdict\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import Union, Optional, Dict, Iterable, Set, Tuple, List\n\nimport requests\n\nimport deeppavlov\nfrom deeppavlov.core.commands.utils import expand_path, parse_config\nfrom deeppavlov.core.data.utils import download, download_decompress, get_all_elems_from_json, file_md5, \\\n    set_query_parameter, path_set_md5\n\nlog = getLogger(__name__)\n\nparser = ArgumentParser()\n\nparser.add_argument(\'--config\', \'-c\', help=""path to a pipeline json config"", type=str,\n                    default=None)\nparser.add_argument(\'-all\', action=\'store_true\',\n                    help=""Download everything. Warning! There should be at least 10 GB space""\n                         "" available on disk."")\n\n\ndef get_config_downloads(config: Union[str, Path, dict]) -> Set[Tuple[str, Path]]:\n    config = parse_config(config)\n\n    downloads = set()\n    if \'metadata\' in config and \'download\' in config[\'metadata\']:\n        for resource in config[\'metadata\'][\'download\']:\n            if isinstance(resource, str):\n                resource = {\n                    \'url\': resource\n                }\n\n            url = resource[\'url\']\n            dest = expand_path(resource.get(\'subdir\', \'\'))\n\n            downloads.add((url, dest))\n\n    config_references = [expand_path(config_ref) for config_ref in get_all_elems_from_json(config, \'config_path\')]\n\n    downloads |= {(url, dest) for config in config_references for url, dest in get_config_downloads(config)}\n\n    return downloads\n\n\ndef get_configs_downloads(config: Optional[Union[str, Path, dict]] = None) -> Dict[str, Set[Path]]:\n    all_downloads = defaultdict(set)\n    if config:\n        configs = [config]\n    else:\n        configs = list(Path(deeppavlov.__path__[0], \'configs\').glob(\'**/*.json\'))\n\n    for config in configs:\n        for url, dest in get_config_downloads(config):\n            all_downloads[url].add(dest)\n\n    return all_downloads\n\n\ndef check_md5(url: str, dest_paths: List[Path]) -> bool:\n    url_md5 = path_set_md5(url)\n    r = requests.get(url_md5)\n    if r.status_code != 200:\n        return False\n    expected = {}\n    for line in r.text.splitlines():\n        _md5, fname = line.split(\' \', maxsplit=1)\n        if fname[0] != \'*\':\n            if fname[0] == \' \':\n                log.warning(f\'Hash generated in text mode for {fname}, comparison could be incorrect\')\n            else:\n                log.error(f\'Unknown hash content format in {url + "".md5""}\')\n                return False\n        expected[fname[1:]] = _md5\n\n    done = None\n    not_done = []\n    for base_path in dest_paths:\n        if all(file_md5(base_path / p) == _md5 for p, _md5 in expected.items()):\n            done = base_path\n        else:\n            not_done.append(base_path)\n\n    if done is None:\n        return False\n\n    for base_path in not_done:\n        log.info(f\'Copying data from {done} to {base_path}\')\n        for p in expected.keys():\n            shutil.copy(done / p, base_path / p)\n    return True\n\n\ndef download_resource(url: str, dest_paths: Iterable[Union[Path, str]]) \\\n        -> None:\n    dest_paths = [Path(dest) for dest in dest_paths]\n\n    if check_md5(url, dest_paths):\n        log.info(f\'Skipped {url} download because of matching hashes\')\n    elif any(ext in url for ext in (\'.tar.gz\', \'.gz\', \'.zip\')):\n        download_path = dest_paths[0].parent\n        download_decompress(url, download_path, dest_paths)\n    else:\n        file_name = url.split(\'/\')[-1].split(\'?\')[0]\n        dest_files = [dest_path / file_name for dest_path in dest_paths]\n        download(dest_files, url)\n\n\ndef download_resources(args: Namespace) -> None:\n    if not args.all and not args.config:\n        log.error(\'You should provide either skill config path or -all flag\')\n        sys.exit(1)\n    elif args.all:\n        downloads = get_configs_downloads()\n    else:\n        config_path = Path(args.config).resolve()\n        downloads = get_configs_downloads(config=config_path)\n\n    for url, dest_paths in downloads.items():\n        download_resource(url, dest_paths)\n\n\ndef deep_download(config: Union[str, Path, dict]) -> None:\n    downloads = get_configs_downloads(config)\n\n    for url, dest_paths in downloads.items():\n        if not isinstance(config, dict):\n            url = set_query_parameter(url, \'config\', Path(config).stem)\n        download_resource(url, dest_paths)\n\n\ndef main(args: Optional[List[str]] = None) -> None:\n    args = parser.parse_args(args)\n    log.info(""Downloading..."")\n    download_resources(args)\n    log.info(""\\nDownload successful!"")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
deeppavlov/evolve.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport json\nimport os\nimport sys\nfrom collections import defaultdict\nfrom logging import getLogger\nfrom subprocess import Popen\n\nimport pandas as pd\n\nfrom deeppavlov.core.commands.utils import expand_path, parse_config, parse_value_with_config\nfrom deeppavlov.core.common.errors import ConfigError\nfrom deeppavlov.core.common.file import read_json, save_json, find_config\nfrom deeppavlov.models.evolution.evolution_param_generator import ParamsEvolution\n\nlog = getLogger(__name__)\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(""config_path"", help=""path to a pipeline json config"", type=str)\nparser.add_argument(\'--key_main_model\', help=\'key inserted in dictionary of main model in pipe\', default=""main"")\nparser.add_argument(\'--p_cross\', help=\'probability of crossover\', type=float, default=0.2)\nparser.add_argument(\'--pow_cross\', help=\'crossover power\', type=float, default=0.1)\nparser.add_argument(\'--p_mut\', help=\'probability of mutation\', type=float, default=1.)\nparser.add_argument(\'--pow_mut\', help=\'mutation power\', type=float, default=0.1)\n\nparser.add_argument(\'--p_size\', help=\'population size\', type=int, default=10)\nparser.add_argument(\'--gpus\', help=\'visible GPUs divided by comma <<,>>\', default=""-1"")\nparser.add_argument(\'--train_partition\',\n                    help=\'partition of splitted train file\', default=1)\nparser.add_argument(\'--start_from_population\',\n                    help=\'population number to start from. 0 means from scratch\', default=0)\nparser.add_argument(\'--path_to_population\',\n                    help=\'path to population to start from\', default="""")\nparser.add_argument(\'--elitism_with_weights\',\n                    help=\'whether to save elite models with weights or without\', action=\'store_true\')\nparser.add_argument(\'--iterations\', help=\'Number of iterations\', type=int, default=-1)\n\n\ndef main():\n    args = parser.parse_args()\n\n    pipeline_config_path = find_config(args.config_path)\n    key_main_model = args.key_main_model\n    population_size = args.p_size\n    gpus = [int(gpu) for gpu in args.gpus.split("","")]\n    train_partition = int(args.train_partition)\n    start_from_population = int(args.start_from_population)\n    path_to_population = args.path_to_population\n    elitism_with_weights = args.elitism_with_weights\n    iterations = int(args.iterations)\n\n    p_crossover = args.p_cross\n    pow_crossover = args.pow_cross\n    p_mutation = args.p_mut\n    pow_mutation = args.pow_mut\n\n    if os.environ.get(""CUDA_VISIBLE_DEVICES"") is None:\n        pass\n    else:\n        cvd = [int(gpu) for gpu in os.environ.get(""CUDA_VISIBLE_DEVICES"").split("","")]\n        if gpus == [-1]:\n            gpus = cvd\n        else:\n            try:\n                gpus = [cvd[gpu] for gpu in gpus]\n            except IndexError:\n                raise ConfigError(""Can not use gpus `{}` with CUDA_VISIBLE_DEVICES=\'{}\'"".format(\n                    "","".join(map(str, gpus)), "","".join(map(str, cvd))\n                ))\n\n    basic_params = read_json(pipeline_config_path)\n    log.info(""Given basic params: {}\\n"".format(json.dumps(basic_params, indent=2)))\n\n    # Initialize evolution\n    evolution = ParamsEvolution(population_size=population_size,\n                                p_crossover=p_crossover, crossover_power=pow_crossover,\n                                p_mutation=p_mutation, mutation_power=pow_mutation,\n                                key_main_model=key_main_model,\n                                seed=42,\n                                train_partition=train_partition,\n                                elitism_with_weights=elitism_with_weights,\n                                **basic_params)\n\n    considered_metrics = evolution.get_value_from_config(evolution.basic_config,\n                                                         list(evolution.find_model_path(\n                                                             evolution.basic_config, ""metrics""))[0] + [""metrics""])\n    considered_metrics = [metric[\'name\'] if isinstance(metric, dict) else metric for metric in considered_metrics]\n\n    log.info(considered_metrics)\n    evolve_metric = considered_metrics[0]\n\n    # Create table variable for gathering results\n    abs_path_to_main_models = expand_path(parse_value_with_config(evolution.models_path,\n                                                                  evolution.basic_config))\n    abs_path_to_main_models.mkdir(parents=True, exist_ok=True)\n\n    result_file = abs_path_to_main_models / ""result_table.tsv""\n    print(result_file)\n\n    result_table_columns = []\n    result_table_dict = {}\n    for el in considered_metrics:\n        result_table_dict[el + ""_valid""] = []\n        result_table_dict[el + ""_test""] = []\n        result_table_columns.extend([el + ""_valid"", el + ""_test""])\n\n    result_table_dict[""params""] = []\n    result_table_columns.append(""params"")\n\n    if start_from_population == 0:\n        # if starting evolution from scratch\n        iters = 0\n        result_table = pd.DataFrame(result_table_dict)\n        # write down result table file\n        result_table.loc[:, result_table_columns].to_csv(result_file, index=False, sep=\'\\t\')\n\n        log.info(""Iteration #{} starts"".format(iters))\n        # randomly generate the first population\n        population = evolution.first_generation()\n    else:\n        # if starting evolution from already existing population\n        iters = start_from_population\n        log.info(""Iteration #{} starts"".format(iters))\n\n        population = []\n        for i in range(population_size):\n            config = read_json(expand_path(path_to_population) / f""model_{i}"" / ""config.json"")\n\n            evolution.insert_value_or_dict_into_config(\n                config, evolution.path_to_models_save_path,\n                str(evolution.main_model_path / f""population_{start_from_population}"" / f""model_{i}""))\n\n            population.append(config)\n\n    run_population(population, evolution, gpus)\n    population_scores = results_to_table(population, evolution, considered_metrics,\n                                         result_file, result_table_columns)[evolve_metric]\n    log.info(""Population scores: {}"".format(population_scores))\n    log.info(""Iteration #{} was done"".format(iters))\n    iters += 1\n\n    while True:\n        if iterations != -1 and start_from_population + iterations == iters:\n            log.info(""End of evolution on iteration #{}"".format(iters))\n            break\n        log.info(""Iteration #{} starts"".format(iters))\n        population = evolution.next_generation(population, population_scores, iters)\n        run_population(population, evolution, gpus)\n        population_scores = results_to_table(population, evolution, considered_metrics,\n                                             result_file, result_table_columns)[evolve_metric]\n        log.info(""Population scores: {}"".format(population_scores))\n        log.info(""Iteration #{} was done"".format(iters))\n        iters += 1\n\n\ndef run_population(population, evolution, gpus):\n    """"""\n    Change save and load paths for obtained population, save config.json with model config,\n    run population via current python executor (with which evolve.py already run)\n    and on given devices (-1 means CPU, other integeres - visible for evolve.py GPUs)\n    Args:\n        population: list of dictionaries - configs of current population\n        evolution: ParamsEvolution\n        gpus: list of given devices (list of integers)\n\n    Returns:\n        None\n    """"""\n    population_size = len(population)\n    for k in range(population_size // len(gpus) + 1):\n        procs = []\n        for j in range(len(gpus)):\n            i = k * len(gpus) + j\n            if i < population_size:\n                save_path = expand_path(\n                    evolution.get_value_from_config(parse_config(population[i]),\n                                                    evolution.path_to_models_save_path))\n\n                save_path.mkdir(parents=True, exist_ok=True)\n                f_name = save_path / ""config.json""\n                save_json(population[i], f_name)\n\n                with save_path.joinpath(\'out.txt\').open(\'w\', encoding=\'utf8\') as outlog, \\\n                        save_path.joinpath(\'err.txt\').open(\'w\', encoding=\'utf8\') as errlog:\n                    env = dict(os.environ)\n                    if len(gpus) > 1 or gpus[0] != -1:\n                        env[\'CUDA_VISIBLE_DEVICES\'] = str(gpus[j])\n\n                    procs.append(Popen(""{} -m deeppavlov train {}"".format(sys.executable, str(f_name)),\n                                       shell=True, stdout=outlog, stderr=errlog, env=env))\n        for j, proc in enumerate(procs):\n            i = k * len(gpus) + j\n            log.info(f\'Waiting on {i}th proc\')\n            if proc.wait() != 0:\n                save_path = expand_path(\n                    evolution.get_value_from_config(parse_config(population[i]),\n                                                    evolution.path_to_models_save_path))\n                with save_path.joinpath(\'err.txt\').open(encoding=\'utf8\') as errlog:\n                    log.warning(f\'Population {i} returned an error code {proc.returncode} and an error log:\\n\' +\n                                errlog.read())\n    return None\n\n\ndef results_to_table(population, evolution, considered_metrics, result_file, result_table_columns):\n    population_size = len(population)\n    train_config = evolution.basic_config.get(\'train\', {})\n\n    if \'evaluation_targets\' in train_config:\n        evaluation_targets = train_config[\'evaluation_targets\']\n    else:\n        evaluation_targets = []\n        if train_config.get(\'validate_best\', True):\n            evaluation_targets.append(\'valid\')\n        elif train_config.get(\'test_best\', True):\n            evaluation_targets.append(\'test\')\n\n    if \'valid\' in evaluation_targets:\n        target = \'valid\'\n    elif \'test\' in evaluation_targets:\n        target = \'test\'\n    elif \'train\' in evaluation_targets:\n        target = \'train\'\n    else:\n        raise ConfigError(\'evaluation_targets are empty. Can not evolve\')\n\n    if target != \'valid\':\n        log.info(f""Tuning parameters on {target}"")\n\n    population_metrics = {}\n    for m in considered_metrics:\n        population_metrics[m] = []\n    for i in range(population_size):\n        log_path = expand_path(evolution.get_value_from_config(parse_config(population[i]),\n                                                               evolution.path_to_models_save_path)\n                               ) / ""out.txt""\n\n        report = {}\n        with log_path.open(encoding=\'utf8\') as f:\n            for line in f:\n                try:\n                    report.update(json.loads(line))\n                except:\n                    pass\n\n        result_table_dict = defaultdict(list)\n\n        for m in considered_metrics:\n            for data_type in evaluation_targets:\n                result_table_dict[f\'{m}_{data_type}\'].append(report[data_type][\'metrics\'][m])\n                if data_type == target:\n                    population_metrics[m].append(report[data_type][\'metrics\'][m])\n\n        result_table_dict[result_table_columns[-1]] = [json.dumps(population[i])]\n        result_table = pd.DataFrame(result_table_dict)\n        result_table.loc[:, result_table_columns].to_csv(result_file, index=False, sep=\'\\t\', mode=\'a\', header=None)\n\n    return population_metrics\n\n\nif __name__ == ""__main__"":\n    main()\n'"
deeppavlov/paramsearch.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport sys\nfrom copy import deepcopy\nfrom itertools import product\nfrom logging import getLogger\nfrom pathlib import Path\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nfrom deeppavlov.core.commands.train import train_evaluate_model_from_config, get_iterator_from_config, \\\n    read_data_by_config\nfrom deeppavlov.core.commands.utils import parse_config\nfrom deeppavlov.core.common.cross_validation import calc_cv_score\nfrom deeppavlov.core.common.file import save_json, find_config, read_json\nfrom deeppavlov.core.common.params_search import ParamsSearch\n\np = (Path(__file__) / "".."" / "".."").resolve()\nsys.path.append(str(p))\n\nlog = getLogger(__name__)\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""config_path"", help=""path to a pipeline json config"", type=str)\nparser.add_argument(""--folds"", help=""number of folds"", type=str, default=None)\nparser.add_argument(""--search_type"", help=""search type: grid or random search"", type=str, default=\'grid\')\n\n\ndef get_best_params(combinations, scores, param_names, target_metric):\n    max_id = np.argmax(scores)\n    best_params = dict(zip(param_names, combinations[max_id]))\n    best_params[target_metric] = scores[max_id]\n\n    return best_params\n\n\ndef main():\n    params_helper = ParamsSearch()\n\n    args = parser.parse_args()\n    is_loo = False\n    n_folds = None\n    if args.folds == \'loo\':\n        is_loo = True\n    elif args.folds is None:\n        n_folds = None\n    elif args.folds.isdigit():\n        n_folds = int(args.folds)\n    else:\n        raise NotImplementedError(\'Not implemented this type of CV\')\n\n    # read config\n    pipeline_config_path = find_config(args.config_path)\n    config_init = read_json(pipeline_config_path)\n    config = parse_config(config_init)\n    data = read_data_by_config(config)\n    target_metric = parse_config(config_init)[\'train\'][\'metrics\'][0]\n    if isinstance(target_metric, dict):\n        target_metric = target_metric[\'name\']\n\n    # get all params for search\n    param_paths = list(params_helper.find_model_path(config, \'search_choice\'))\n    param_values = []\n    param_names = []\n    for path in param_paths:\n        value = params_helper.get_value_from_config(config, path)\n        param_name = path[-1]\n        param_value_search = value[\'search_choice\']\n        param_names.append(param_name)\n        param_values.append(param_value_search)\n\n    # find optimal params\n    if args.search_type == \'grid\':\n        # generate params combnations for grid search\n        combinations = list(product(*param_values))\n\n        # calculate cv scores\n        scores = []\n        for comb in combinations:\n            config = deepcopy(config_init)\n            for param_path, param_value in zip(param_paths, comb):\n                params_helper.insert_value_or_dict_into_config(config, param_path, param_value)\n            config = parse_config(config)\n\n            if (n_folds is not None) | is_loo:\n                # CV for model evaluation\n                score_dict = calc_cv_score(config, data=data, n_folds=n_folds, is_loo=is_loo)\n                score = score_dict[next(iter(score_dict))]\n            else:\n                # train/valid for model evaluation\n                data_to_evaluate = data.copy()\n                if len(data_to_evaluate[\'valid\']) == 0:\n                    data_to_evaluate[\'train\'], data_to_evaluate[\'valid\'] = train_test_split(data_to_evaluate[\'train\'],\n                                                                                            test_size=0.2)\n                iterator = get_iterator_from_config(config, data_to_evaluate)\n                score = train_evaluate_model_from_config(config, iterator=iterator)[\'valid\'][target_metric]\n\n            scores.append(score)\n\n        # get model with best score\n        best_params_dict = get_best_params(combinations, scores, param_names, target_metric)\n        log.info(\'Best model params: {}\'.format(best_params_dict))\n    else:\n        raise NotImplementedError(\'Not implemented this type of search\')\n\n    # save config\n    best_config = config_init\n    for i, param_name in enumerate(best_params_dict.keys()):\n        if param_name != target_metric:\n            params_helper.insert_value_or_dict_into_config(best_config, param_paths[i], best_params_dict[param_name])\n\n    best_model_filename = pipeline_config_path.with_suffix(\'.cvbest.json\')\n    save_json(best_config, best_model_filename)\n    log.info(\'Best model saved in json-file: {}\'.format(best_model_filename))\n\n\n# try to run:\n# --config_path path_to_config.json --folds 2\nif __name__ == ""__main__"":\n    main()\n'"
deeppavlov/settings.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\n\nfrom deeppavlov.core.common.paths import get_settings_path, populate_settings_dir\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(""-d"", ""--default"", action=""store_true"", help=""return to defaults"")\n\n\ndef main():\n    """"""DeepPavlov console configuration utility.""""""\n    args = parser.parse_args()\n    path = get_settings_path()\n\n    if args.default:\n        if populate_settings_dir(force=True):\n            print(f\'Populated {path} with default settings files\')\n        else:\n            print(f\'{path} is already a default settings directory\')\n    else:\n        print(f\'Current DeepPavlov settings path: {path}\')\n\n\nif __name__ == ""__main__"":\n    main()\n'"
docs/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n\nimport sphinx_rtd_theme\n\nimport deeppavlov\n\n# -- Project information -----------------------------------------------------\n\nproject = \'DeepPavlov\'\ncopyright = \'2018, \' + deeppavlov.__author__\nauthor = deeppavlov.__author__\n\n# The short X.Y version\nversion = deeppavlov.__version__\n# The full version, including alpha/beta/rc tags\nrelease = version\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.extlinks\',\n    \'nbsphinx\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path .\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\', \'**.ipynb_checkpoints\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {\n    \'collapse_navigation\': False,\n    \'display_version\': True,\n    \'logo_only\': True,\n}\n\nhtml_logo = \'_static/deeppavlov.png\'\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\nhtml_context = {\n    \'css_files\': [\n        \'https://media.readthedocs.org/css/sphinx_rtd_theme.css\',\n        \'https://media.readthedocs.org/css/readthedocs-doc-embed.css\',\n        \'_static/deeppavlov.css\'\n    ]\n}\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\n\nhtmlhelp_basename = f\'{project}-Docs\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_engine = \'xelatex\'\n\nlatex_elements = {\n\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n\n    \'extraclassoptions\': \'openany,oneside\',\n\n    \'fncychap\': r\'\\usepackage[Sonny]{fncychap}\'\n\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, f\'{project}.tex\', f\'{project} Documentation\',\n     author, \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, project.lower(), f\'{project} Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, project, f\'{project} Documentation\',\n     author, project, deeppavlov.__description__,\n     str(deeppavlov.__keywords__)),\n]\n\n\n# -- Extension configuration -------------------------------------------------\n\nautodoc_mock_imports = [\'aiml\', \'bert_dp\', \'fastText\', \'fasttext\', \'gensim\', \'hdt\', \'kenlm\', \'librosa\', \'lxml\', \'nemo\',\n                        \'nemo_asr\', \'nemo_tts\', \'nltk\', \'rapidfuzz\', \'rasa\', \'russian_tagsets\', \'sacremoses\',\n                        \'sortedcontainers\', \'spacy\', \'tensorflow\', \'tensorflow_hub\', \'torch\', \'transformers\']\n\nextlinks = {\n    \'config\': (f\'https://github.com/deepmipt/DeepPavlov/blob/{release}/deeppavlov/configs/%s\', None)\n}\n\n# -- Options for intersphinx extension ---------------------------------------\n\n# Configuration for intersphinx\nintersphinx_mapping = {\n    \'python\': (\'https://docs.python.org/3.6\', None),\n    \'scipy\': (\'https://docs.scipy.org/doc/scipy/reference\', None)\n}\n\n# -- Options for todo extension ----------------------------------------------\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n'"
tests/__init__.py,0,b''
tests/test_aiml_skill.py,0,"b'from logging import getLogger\n\nfrom deeppavlov import configs, build_model\nfrom deeppavlov.utils.pip_wrapper.pip_wrapper import install_from_config\n\nlog = getLogger(__name__)\n\n\nclass TestAIMLSkill:\n    def setup(self):\n        config_ref = configs.skills.aiml_skill\n        install_from_config(config_ref)\n        self.aiml_skill = build_model(config_ref, download=True)\n\n    def test_simple_reaction(self):\n        user_messages_sequence = [\n            ""Hello"",\n            ""What s up?"",\n            ""Tell me a joke"",\n            ""Learn my pants are Red"",\n            ""LET DISCUSS MOVIES"",\n            ""Comedy movies are nice to watch"",\n            ""I LIKE WATCHING COMEDY!"",\n            ""Ok, goodbye""\n        ]\n\n        history_of_responses = []\n        for each_utt in user_messages_sequence:\n            log.info(f""User says: {each_utt}"")\n            responses_batch, _, _ = self.aiml_skill([each_utt], [None])\n            log.info(f"" Bot says: {responses_batch[0]}"")\n            history_of_responses.append(responses_batch)\n\n        # check the first greeting message in 0th batch\n        assert ""Well, hello!"" in history_of_responses[0][0]\n        # check fifth message in 0th batch\n        assert ""Yes movies"" in history_of_responses[4][0]\n'"
tests/test_dsl_skill.py,0,"b'from logging import getLogger\n\nfrom deeppavlov import configs, build_model\nfrom deeppavlov.core.common.file import read_json\nfrom deeppavlov.skills.dsl_skill import DSLMeta\nfrom deeppavlov.utils.pip_wrapper.pip_wrapper import install_from_config\n\nlog = getLogger(__name__)\n\n\nclass DSLSkill(metaclass=DSLMeta):\n    @DSLMeta.handler(commands=[""hello"", ""hi"", ""sup"", ""greetings""])\n    def greeting(context):\n        response = ""Hello, my friend!""\n        confidence = 1.0\n        return response, confidence\n\n\nclass StateSkill(metaclass=DSLMeta):\n    @DSLMeta.handler(commands=[""hello"", ""hi"", ""sup"", ""greetings""])\n    def greeting(context):\n        response = ""Hello, my friend!""\n        confidence = 1.0\n        context.current_state = ""state1""\n        return response, confidence\n\n    @DSLMeta.handler(commands=[""bye""],\n                     state=""state1"")\n    def bye(context):\n        response = ""bb!""\n        confidence = 1.0\n        return response, confidence\n\n\nclass ContextConditionSkill(metaclass=DSLMeta):\n    @DSLMeta.handler(commands=[""hello"", ""hi"", ""sup"", ""greetings""],\n                     context_condition=lambda context: context.user_id != 1)\n    def greeting(context):\n        response = ""Hello, my friend!""\n        confidence = 1.0\n        return response, confidence\n\n\nclass TestDSLSkill:\n    def setup(self):\n        self.skill_config = read_json(configs.skills.dsl_skill)\n        install_from_config(self.skill_config)\n\n    def test_simple_skill(self):\n        user_messages_sequence = [\n            ""Hello"",\n            ""Hi"",\n            ""Tell me a joke"",\n            ""Sup"",\n            ""Ok, goodbye""\n        ]\n\n        skill = build_model(self.skill_config, download=True)\n        history_of_responses = []\n        for user_id, each_utt in enumerate(user_messages_sequence):\n            log.info(f""User says: {each_utt}"")\n            responses_batch = skill([each_utt], [user_id])\n            log.info(f""Bot says: {responses_batch[0]}"")\n            history_of_responses.append(responses_batch)\n\n        # check the first greeting message in 0th batch\n        assert ""Hello, my friend!"" in history_of_responses[0][0]\n        # check the second greeting message in 0th batch\n        assert ""Hello, my friend!"" in history_of_responses[1][0]\n        # check `on_invalid_command`\n        assert ""Sorry, I do not understand you"" in history_of_responses[2][0]\n\n    def test_switch_state(self):\n        user_messages_sequence = [\n            ""Hello"",\n            ""bye"",\n            ""bye""\n        ]\n\n        self.skill_config[""chainer""][""pipe""][1][""class_name""] = ""StateSkill""\n        skill = build_model(self.skill_config, download=True)\n\n        history_of_responses = []\n        for user_id, each_utt in enumerate(user_messages_sequence):\n            log.info(f""User says: {each_utt}"")\n            responses_batch = skill([each_utt], [user_id % 2])\n            log.info(f""Bot says: {responses_batch[0]}"")\n            history_of_responses.append(responses_batch)\n        assert ""Hello, my friend!"" in history_of_responses[0][0]\n        assert ""Sorry, I do not understand you"" in history_of_responses[1][0]\n        assert ""bb!"" in history_of_responses[2][0]\n\n    def test_context_condition(self):\n        user_messages_sequence = [\n            ""Hello"",\n            ""Hi""\n        ]\n\n        self.skill_config[""chainer""][""pipe""][1][""class_name""] = ""ContextConditionSkill""\n        skill = build_model(self.skill_config, download=True)\n\n        history_of_responses = []\n        for user_id, each_utt in enumerate(user_messages_sequence):\n            log.info(f""User says: {each_utt}"")\n            responses_batch = skill([each_utt], [user_id])\n            log.info(f""Bot says: {responses_batch[0]}"")\n            history_of_responses.append(responses_batch)\n        assert ""Hello, my friend!"" in history_of_responses[0][0]\n        assert ""Sorry, I do not understand you"" in history_of_responses[1][0]\n'"
tests/test_quick_start.py,0,"b'import io\nimport json\nimport logging\nimport os\nimport pickle\nimport shutil\nimport signal\nimport socket\nimport sys\nfrom concurrent.futures import ProcessPoolExecutor\nfrom pathlib import Path\nfrom struct import unpack\nfrom time import sleep\nfrom typing import Optional, Union\nfrom urllib.parse import urljoin\n\nimport pexpect\nimport pexpect.popen_spawn\nimport pytest\nimport requests\n\nimport deeppavlov\nfrom deeppavlov import build_model\nfrom deeppavlov.core.commands.utils import parse_config\nfrom deeppavlov.core.data.utils import get_all_elems_from_json\nfrom deeppavlov.download import deep_download\nfrom deeppavlov.utils.server import get_server_params\nfrom deeppavlov.utils.socket import encode\n\ntests_dir = Path(__file__).parent\ntest_configs_path = tests_dir / ""deeppavlov"" / ""configs""\nsrc_dir = Path(deeppavlov.__path__[0]) / ""configs""\ntest_src_dir = tests_dir / ""test_configs""\ndownload_path = tests_dir / ""download""\n\ncache_dir: Optional[Path] = None\nif not os.getenv(\'DP_PYTEST_NO_CACHE\'):\n    cache_dir = tests_dir / \'download_cache\'\n\napi_port = os.getenv(\'DP_PYTEST_API_PORT\')\nif api_port is not None:\n    api_port = int(api_port)\n\nTEST_MODES = [\'IP\',  # test_inferring_pretrained_model\n              \'TI\',  # test_consecutive_training_and_inferring\n              \'SR\',  # test_serialization\n              ]\n\nALL_MODES = (\'IP\', \'TI\', \'SR\')\n\nONE_ARGUMENT_INFER_CHECK = (\'Dummy text\', None)\nTWO_ARGUMENTS_INFER_CHECK = (\'Dummy text\', \'Dummy text\', None)\nFOUR_ARGUMENTS_INFER_CHECK = (\'Dummy text\', \'Dummy text\', \'Dummy text\', \'Dummy_text\', None)\n\n# Mapping from model name to config-model_dir-ispretrained and corresponding queries-response list.\nPARAMS = {\n    ""ecommerce_skill"": {\n        (""ecommerce_skill/bleu_retrieve.json"", ""ecommerce_skill_bleu"", ALL_MODES): [(\'Dummy text\', [], {}, None)],\n        (""ecommerce_skill/tfidf_retrieve.json"", ""ecommerce_skill_tfidf"", ALL_MODES): [(\'Dummy text\', [], {}, None)]\n    },\n    ""faq"": {\n        (""faq/tfidf_logreg_en_faq.json"", ""faq_tfidf_logreg_en"", ALL_MODES): [ONE_ARGUMENT_INFER_CHECK],\n        (""faq/tfidf_autofaq.json"", ""faq_tfidf_cos"", ALL_MODES): [ONE_ARGUMENT_INFER_CHECK],\n        (""faq/tfidf_logreg_autofaq.json"", ""faq_tfidf_logreg"", ALL_MODES): [ONE_ARGUMENT_INFER_CHECK],\n        (""faq/fasttext_avg_autofaq.json"", ""faq_fasttext_avg"", ALL_MODES): [ONE_ARGUMENT_INFER_CHECK],\n        (""faq/fasttext_tfidf_autofaq.json"", ""faq_fasttext_tfidf"", ALL_MODES): [ONE_ARGUMENT_INFER_CHECK]\n    },\n    ""spelling_correction"": {\n        (""spelling_correction/brillmoore_wikitypos_en.json"", ""error_model"", ALL_MODES):\n            [\n                (""helllo"", ""hello""),\n                (""datha"", ""data"")\n            ],\n        (""spelling_correction/brillmoore_kartaslov_ru.json"", ""error_model"", (\'IP\',)):\n            [\n                (""\xd0\xbf\xd1\x80\xd0\xb5\xd0\xb2\xd0\xb5\xd0\xb4\xd1\x81\xd1\x82\xd0\xb2\xd1\x83\xd1\x8e"", ""\xd0\xbf\xd1\x80\xd0\xb8\xd0\xb2\xd0\xb5\xd1\x82\xd1\x81\xd1\x82\xd0\xb2\xd1\x83\xd1\x8e""),\n                (""\xd1\x8f \xd0\xb4\xd0\xb6\xd0\xb2\xd0\xb0 \xd0\xb3\xd0\xbe\xd0\xb4\xd0\xb0 \xd0\xb4\xd0\xb4\xd1\x83 \xd1\x8d\xd1\x82\xd1\x83 \xd0\xb8\xd0\xb3\xd1\x80\xd1\x83"", ""\xd1\x8f \xd0\xb4\xd0\xb2\xd0\xb0 \xd0\xb3\xd0\xbe\xd0\xb4\xd0\xb0 \xd0\xb6\xd0\xb4\xd1\x83 \xd1\x8d\xd1\x82\xd1\x83 \xd0\xb8\xd0\xb3\xd1\x80\xd1\x83"")\n            ],\n        (""spelling_correction/levenshtein_corrector_ru.json"", ""error_model"", (\'IP\',)):\n            [\n                (""\xd0\xbf\xd1\x80\xd0\xb5\xd0\xb2\xd0\xb5\xd1\x82\xd1\x81\xd1\x82\xd0\xb2\xd1\x83\xd1\x8e"", ""\xd0\xbf\xd1\x80\xd0\xb8\xd0\xb2\xd0\xb5\xd1\x82\xd1\x81\xd1\x82\xd0\xb2\xd1\x83\xd1\x8e""),\n                (""\xd0\xaf \xd0\xb4\xd0\xb6\xd0\xb2\xd0\xb0 \xd0\xb3\xd0\xbe\xd0\xb4\xd0\xb0 \xd1\x85\xd0\xbe\xd1\x87\xd1\x83 \xd1\x82\xd0\xb0\xd0\xba\xd1\x83\xd1\x8e \xd0\xb8\xd0\xb3\xd1\x80\xd1\x83"", ""\xd1\x8f \xd0\xb4\xd0\xb2\xd0\xb0 \xd0\xb3\xd0\xbe\xd0\xb4\xd0\xb0 \xd1\x85\xd0\xbe\xd1\x87\xd1\x83 \xd1\x82\xd0\xb0\xd0\xba\xd1\x83\xd1\x8e \xd0\xb8\xd0\xb3\xd1\x80\xd1\x83"")\n            ]\n    },\n    ""go_bot"": {\n        (""go_bot/gobot_dstc2.json"", ""gobot_dstc2"", ALL_MODES): [ONE_ARGUMENT_INFER_CHECK],\n        (""go_bot/gobot_dstc2_best.json"", ""gobot_dstc2_best"", ALL_MODES): [ONE_ARGUMENT_INFER_CHECK],\n        (""go_bot/gobot_dstc2_minimal.json"", ""gobot_dstc2_minimal"", (\'TI\',)): [([{""text"": ""the weather is clooudy and gloooomy""}], None)]\n    },\n    ""classifiers"": {\n        (""classifiers/paraphraser_bert.json"", ""classifiers"", (\'IP\', \'TI\')): [TWO_ARGUMENTS_INFER_CHECK],\n        (""classifiers/paraphraser_rubert.json"", ""classifiers"", (\'IP\', \'TI\')): [TWO_ARGUMENTS_INFER_CHECK],\n        (""classifiers/insults_kaggle_bert.json"", ""classifiers"", (\'IP\', \'TI\')): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/insults_kaggle_conv_bert.json"", ""classifiers"", (\'IP\', \'TI\')): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/rusentiment_bert.json"", ""classifiers"", (\'IP\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/intents_dstc2_bert.json"", ""classifiers"", (\'IP\', \'TI\')): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/intents_dstc2.json"", ""classifiers"", ALL_MODES): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/intents_dstc2_big.json"", ""classifiers"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/insults_kaggle.json"", ""classifiers"", ALL_MODES): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/sentiment_twitter.json"", ""classifiers"", ALL_MODES): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/sentiment_twitter_bert_emb.json"", ""classifiers"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/sentiment_twitter_preproc.json"", ""classifiers"", ALL_MODES): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/topic_ag_news.json"", ""classifiers"", ALL_MODES): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/rusentiment_cnn.json"", ""classifiers"", (\'IP\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/rusentiment_elmo_twitter_cnn.json"", ""classifiers"", (\'IP\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/rusentiment_bigru_superconv.json"", ""classifiers"", (\'IP\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/yahoo_convers_vs_info.json"", ""classifiers"", (\'IP\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/ru_obscenity_classifier.json"", ""classifiers"", (\'IP\',)):\n            [\n                (""\xd0\x9d\xd1\x83 \xd0\xb8 \xd1\x81\xd1\x83\xd0\xba\xd0\xb0 \xd0\xb6\xd0\xb5 \xd0\xbe\xd0\xbd\xd0\xb0"", True),\n                (""\xd1\x8f \xd0\xb4\xd0\xb2\xd0\xb0 \xd0\xb3\xd0\xbe\xd0\xb4\xd0\xb0 \xd0\xb6\xd0\xb4\xd1\x83 \xd1\x8d\xd1\x82\xd1\x83 \xd0\xb8\xd0\xb3\xd1\x80\xd1\x83"", False)\n            ],\n        (""classifiers/sentiment_sst_conv_bert.json"", ""classifiers"", (\'IP\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/sentiment_sst_multi_bert.json"", ""classifiers"", (\'IP\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/sentiment_yelp_conv_bert.json"", ""classifiers"", (\'IP\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/sentiment_yelp_multi_bert.json"", ""classifiers"", (\'IP\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/sentiment_imdb_bert.json"", ""classifiers"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/sentiment_imdb_conv_bert.json"", ""classifiers"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK]\n    },\n    ""snips"": {\n        (""classifiers/intents_snips.json"", ""classifiers"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/intents_snips_big.json"", ""classifiers"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/intents_snips_bigru.json"", ""classifiers"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/intents_snips_bilstm.json"", ""classifiers"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/intents_snips_bilstm_bilstm.json"", ""classifiers"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/intents_snips_bilstm_cnn.json"", ""classifiers"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/intents_snips_bilstm_proj_layer.json"", ""classifiers"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/intents_snips_bilstm_self_add_attention.json"", ""classifiers"", (\'TI\',)):\n            [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/intents_snips_bilstm_self_mult_attention.json"", ""classifiers"", (\'TI\',)):\n            [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/intents_snips_cnn_bilstm.json"", ""classifiers"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/intents_snips_sklearn.json"", ""classifiers"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/intents_snips_tfidf_weighted.json"", ""classifiers"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK]\n    },\n    ""sample"": {\n        (""classifiers/intents_sample_csv.json"", ""classifiers"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""classifiers/intents_sample_json.json"", ""classifiers"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK]\n    },\n    ""ner"": {\n        (""ner/conll2003_m1.json"", ""conll2003_m1"", (\'IP\', \'TI\')): [\n            ([""Peter"", ""Blackburn""], [""NNP"", ""NNP""], None)],\n        (""ner/vlsp2016_full.json"", ""vlsp2016_full"", (\'IP\', \'TI\')): [\n            ([""H\xc6\xb0\xc6\xa1ng"", ""t\xe1\xbb\xb1_tin""], [""NNP"", ""V""], [""B-NP"", ""B-VP""], None)],\n        (""ner/ner_conll2003_bert.json"", ""ner_conll2003_bert"", (\'IP\', \'TI\')): [ONE_ARGUMENT_INFER_CHECK],\n        (""ner/ner_ontonotes_bert.json"", ""ner_ontonotes_bert"", (\'IP\', \'TI\')): [ONE_ARGUMENT_INFER_CHECK],\n        (""ner/ner_ontonotes_bert_mult.json"", ""ner_ontonotes_bert_mult"", (\'IP\', \'TI\')): [ONE_ARGUMENT_INFER_CHECK],\n        (""ner/ner_rus_bert.json"", ""ner_rus_bert"", (\'IP\', \'TI\')): [ONE_ARGUMENT_INFER_CHECK],\n        (""ner/ner_conll2003.json"", ""ner_conll2003"", ALL_MODES): [ONE_ARGUMENT_INFER_CHECK],\n        (""ner/ner_dstc2.json"", ""slotfill_dstc2"", ALL_MODES): [ONE_ARGUMENT_INFER_CHECK],\n        (""ner/ner_ontonotes.json"", ""ner_ontonotes"", ALL_MODES): [ONE_ARGUMENT_INFER_CHECK],\n        (""ner/ner_ontonotes_bert_emb.json"", ""ner_ontonotes_bert_emb"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ner/ner_few_shot_ru_simulate.json"", ""ner_fs"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ner/ner_rus.json"", ""ner_rus"", (\'IP\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ner/slotfill_dstc2.json"", ""slotfill_dstc2"", (\'IP\',)):\n            [\n                (""chinese food"", {\'food\': \'chinese\'}),\n                (""in the west part"", {\'area\': \'west\'}),\n                (""moderate price range"", {\'pricerange\': \'moderate\'})\n            ]\n    },\n    ""sentence_segmentation"": {\n        (""sentence_segmentation/sentseg_dailydialog.json"", ""sentseg_dailydialog"", (\'IP\', \'TI\')): [\n            ([""hey"", ""alexa"", ""how"", ""are"", ""you""], None)]\n    },\n    ""kbqa"": {\n        (""kbqa/kbqa_rus.json"", ""kbqa"", (\'IP\',)):\n            [\n                (""\xd0\x9a\xd0\xb0\xd0\xba\xd0\xb0\xd1\x8f \xd1\x81\xd1\x82\xd0\xbe\xd0\xbb\xd0\xb8\xd1\x86\xd0\xb0 \xd0\xa0\xd0\xbe\xd1\x81\xd1\x81\xd0\xb8\xd0\xb8?"", ""\xd0\x9c\xd0\xbe\xd1\x81\xd0\xba\xd0\xb2\xd0\xb0""),\n                (""\xd0\xb0\xd0\xb1\xd0\xb2"", ""Not Found"")\n            ],\n        (""kbqa/kbqa_tree.json"", ""kbqa"", (\'IP\',)):\n            [\n                (""\xd0\x9a\xd1\x82\xd0\xbe \xd1\x82\xd0\xb0\xd0\xba\xd0\xbe\xd0\xb9 \xd0\x9e\xd0\xba\xd1\x81\xd0\xb8\xd0\xbc\xd0\xb8\xd1\x80\xd0\xbe\xd0\xbd?"", ""\xd1\x80\xd0\xbe\xd1\x81\xd1\x81\xd0\xb8\xd0\xb9\xd1\x81\xd0\xba\xd0\xb8\xd0\xb9 \xd1\x80\xd1\x8d\xd0\xbf-\xd0\xb8\xd1\x81\xd0\xbf\xd0\xbe\xd0\xbb\xd0\xbd\xd0\xb8\xd1\x82\xd0\xb5\xd0\xbb\xd1\x8c""),\n                (""\xd0\xa7\xd0\xb5\xd0\xbc \xd0\xbf\xd0\xb8\xd1\x82\xd0\xb0\xd1\x8e\xd1\x82\xd1\x81\xd1\x8f \xd0\xba\xd0\xbe\xd0\xb0\xd0\xbb\xd1\x8b?"", ""\xd0\xad\xd0\xb2\xd0\xba\xd0\xb0\xd0\xbb\xd0\xb8\xd0\xbf\xd1\x82""),\n                (""\xd0\xb0\xd0\xb1\xd0\xb2"", ""Not Found"")\n            ],\n        (""kbqa/kbqa_cq.json"", ""kbqa"", (\'IP\',)):\n            [\n                (""What is the currency of Sweden?"", ""Swedish krona""),\n                (""In which US state would you find Fort Knox?"", ""Kentucky""),\n                (""Where was Napoleon Bonaparte born?"", ""Ajaccio""),\n                (""When did the Korean War end?"", ""1953-07-27""),\n                (""   "", ""Not Found"")\n            ],\n        (""kbqa/kbqa_cq_bert_ranker.json"", ""kbqa"", (\'IP\',)):\n            [\n                (""What is the currency of Sweden?"", ""Swedish krona""),\n                (""Where was Napoleon Bonaparte born?"", ""Ajaccio""),\n                (""When did the Korean War end?"", ""1953-07-27""),\n                (""   "", ""Not Found"")\n            ]\n    },\n    ""elmo_embedder"": {\n        (""embedder/elmo_ru_news.json"", ""embedder_ru_news"", (\'IP\',)): [ONE_ARGUMENT_INFER_CHECK],\n    },\n    ""elmo_model"": {\n        (""elmo/elmo_1b_benchmark_test.json"", ""elmo_1b_benchmark_test"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n    },\n    ""ranking"": {\n        (""ranking/ranking_insurance.json"", ""ranking"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ranking/ranking_insurance_interact.json"", ""ranking"", (\'IP\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ranking/ranking_ubuntu_v2.json"", ""ranking"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ranking/ranking_ubuntu_v2_interact.json"", ""ranking"", (\'IP\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ranking/ranking_ubuntu_v2_mt.json"", ""ranking"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ranking/ranking_ubuntu_v2_mt_interact.json"", ""ranking"", (\'IP\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ranking/paraphrase_ident_paraphraser.json"", ""ranking"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ranking/paraphrase_ident_paraphraser_interact.json"", ""ranking"",\n         (\'IP\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ranking/paraphrase_ident_paraphraser_pretrain.json"", ""ranking"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ranking/paraphrase_ident_paraphraser_tune.json"", ""ranking"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ranking/paraphrase_ident_tune_interact.json"", ""ranking"",\n         (\'IP\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ranking/paraphrase_ident_paraphraser_elmo.json"", ""ranking"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ranking/paraphrase_ident_elmo_interact.json"", ""ranking"",\n         (\'IP\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ranking/paraphrase_ident_qqp.json"", ""ranking"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ranking/paraphrase_ident_qqp_bilstm_interact.json"", ""ranking"",\n         (\'IP\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ranking/paraphrase_ident_qqp_bilstm.json"", ""ranking"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ranking/paraphrase_ident_qqp_interact.json"", ""ranking"", (\'IP\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ranking/ranking_ubuntu_v2_bert_uncased.json"", ""ranking"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ranking/ranking_ubuntu_v2_bert_sep.json"", ""ranking"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ranking/ranking_ubuntu_v2_bert_sep_interact.json"", ""ranking"", (\'IP\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ranking/ranking_ubuntu_v1_mt_word2vec_smn.json"", ""ranking"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ranking/ranking_ubuntu_v1_mt_word2vec_dam.json"", ""ranking"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ranking/ranking_ubuntu_v1_mt_word2vec_dam_transformer.json"", ""ranking"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ranking/ranking_ubuntu_v2_mt_word2vec_smn.json"", ""ranking"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ranking/ranking_ubuntu_v2_mt_word2vec_dam.json"", ""ranking"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ranking/ranking_ubuntu_v2_mt_word2vec_dam_transformer.json"", ""ranking"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""ranking/ranking_ubuntu_v2_mt_word2vec_dam_transformer.json"", ""ranking"", (\'IP\',)):\n            [(\' & & & & & & & & bonhoeffer  whar drives do you want to mount what &  i have an ext3 usb drive  \'\n              \'& look with fdisk -l & hello there & fdisk is all you need\',\n              None)]\n    },\n    ""doc_retrieval"": {\n        (""doc_retrieval/en_ranker_tfidf_wiki_test.json"", ""doc_retrieval"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""doc_retrieval/ru_ranker_tfidf_wiki_test.json"", ""doc_retrieval"", (\'TI\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""doc_retrieval/en_ranker_pop_wiki_test.json"", ""doc_retrieval"", (\'TI\',)): [\n            ONE_ARGUMENT_INFER_CHECK]\n    },\n    ""squad"": {\n        (""squad/squad_ru_bert.json"", ""squad_ru_bert"", (\'IP\', \'TI\')): [TWO_ARGUMENTS_INFER_CHECK],\n        (""squad/squad_ru_bert_infer.json"", ""squad_ru_bert_infer"", (\'IP\',)): [TWO_ARGUMENTS_INFER_CHECK],\n        (""squad/squad_ru_rubert.json"", ""squad_ru_rubert"", (\'IP\', \'TI\')): [TWO_ARGUMENTS_INFER_CHECK],\n        (""squad/squad_ru_rubert_infer.json"", ""squad_ru_rubert_infer"", (\'IP\',)): [TWO_ARGUMENTS_INFER_CHECK],\n        (""squad/squad_bert.json"", ""squad_bert"", (\'IP\', \'TI\')): [TWO_ARGUMENTS_INFER_CHECK],\n        (""squad/squad_bert_infer.json"", ""squad_bert_infer"", (\'IP\',)): [TWO_ARGUMENTS_INFER_CHECK],\n        (""squad/squad.json"", ""squad_model"", ALL_MODES): [TWO_ARGUMENTS_INFER_CHECK],\n        (""squad/squad_ru.json"", ""squad_model_ru"", ALL_MODES): [TWO_ARGUMENTS_INFER_CHECK],\n        (""squad/multi_squad_noans.json"", ""multi_squad_noans"", (\'IP\',)): [TWO_ARGUMENTS_INFER_CHECK],\n        (""squad/squad_zh_bert_mult.json"", ""squad_zh_bert_mult"", ALL_MODES): [TWO_ARGUMENTS_INFER_CHECK],\n        (""squad/squad_zh_bert_zh.json"", ""squad_zh_bert_zh"", ALL_MODES): [TWO_ARGUMENTS_INFER_CHECK]\n    },\n    ""seq2seq_go_bot"": {\n        (""seq2seq_go_bot/bot_kvret_train.json"", ""seq2seq_go_bot"", (\'TI\',)):\n            [\n                (""will it snow on tuesday?"",\n                 ""f78cf0f9-7d1e-47e9-aa45-33f9942c94be"",\n                 """",\n                 """",\n                 """",\n                 None)\n            ],\n        (""seq2seq_go_bot/bot_kvret.json"", ""seq2seq_go_bot"", (\'IP\',)):\n            [\n                (""will it snow on tuesday?"",\n                 ""f78cf0f9-7d1e-47e9-aa45-33f9942c94be"",\n                 None)\n            ]\n    },\n    ""odqa"": {\n        (""odqa/en_odqa_infer_wiki_test.json"", ""odqa"", (\'IP\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""odqa/ru_odqa_infer_wiki_test.json"", ""odqa"", (\'IP\',)): [ONE_ARGUMENT_INFER_CHECK],\n        (""odqa/en_odqa_pop_infer_wiki_test.json"", ""odqa"", (\'IP\',)): [ONE_ARGUMENT_INFER_CHECK]\n    },\n    ""morpho_tagger"": {\n        (""morpho_tagger/UD2.0/morpho_en.json"", ""morpho_en"", (\'IP\', \'TI\')): [ONE_ARGUMENT_INFER_CHECK],\n        (""morpho_tagger/UD2.0/morpho_ru_syntagrus_pymorphy_lemmatize.json"", ""morpho_tagger_pymorphy"", (\'IP\', \'TI\')):\n            [ONE_ARGUMENT_INFER_CHECK],\n        (""morpho_tagger/BERT/morpho_ru_syntagrus_bert.json"", ""morpho_tagger_bert"", (\'IP\', \'TI\')):\n            [ONE_ARGUMENT_INFER_CHECK]\n    },\n    ""syntax_tagger"": {\n        (""syntax/syntax_ru_syntagrus_bert.json"", ""syntax_ru_bert"", (\'IP\', \'TI\')): [ONE_ARGUMENT_INFER_CHECK],\n        (""syntax/ru_syntagrus_joint_parsing.json"", ""syntax_ru_bert"", (\'IP\',)): [ONE_ARGUMENT_INFER_CHECK]\n    },\n    ""nemo"": {\n        (""nemo/tts2asr_test.json"", ""nemo"", (\'IP\',)): [ONE_ARGUMENT_INFER_CHECK]\n    }\n}\n\nMARKS = {""gpu_only"": [""squad""], ""slow"": [""error_model"", ""go_bot"", ""squad""]}  # marks defined in pytest.ini\n\nTEST_GRID = []\nfor model in PARAMS.keys():\n    for conf_file, model_dir, mode in PARAMS[model].keys():\n        marks = []\n        for mark in MARKS.keys():\n            if model in MARKS[mark]:\n                marks.append(eval(""pytest.mark."" + mark))\n        grid_unit = pytest.param(model, conf_file, model_dir, mode, marks=marks)\n        TEST_GRID.append(grid_unit)\n\n\ndef _override_with_test_values(item: Union[dict, list]) -> None:\n    if isinstance(item, dict):\n        keys = [k for k in item.keys() if k.startswith(\'pytest_\')]\n        for k in keys:\n            item[k[len(\'pytest_\'):]] = item.pop(k)\n        item = item.values()\n\n    for child in item:\n        if isinstance(child, (dict, list)):\n            _override_with_test_values(child)\n\n\ndef download_config(config_path):\n    src_file = src_dir / config_path\n    if not src_file.is_file():\n        src_file = test_src_dir / config_path\n\n    if not src_file.is_file():\n        raise RuntimeError(\'No config file {}\'.format(config_path))\n\n    with src_file.open(encoding=\'utf8\') as fin:\n        config: dict = json.load(fin)\n\n    # Download referenced config files\n    config_references = get_all_elems_from_json(parse_config(config), \'config_path\')\n    for config_ref in config_references:\n        splitted = config_ref.split(""/"")\n        first_subdir_index = splitted.index(""configs"") + 1\n        m_name = config_ref.split(\'/\')[first_subdir_index]\n        config_ref = \'/\'.join(config_ref.split(\'/\')[first_subdir_index:])\n\n        test_configs_path.joinpath(m_name).mkdir(exist_ok=True)\n        if not test_configs_path.joinpath(config_ref).exists():\n            download_config(config_ref)\n\n    # Update config for testing\n    config.setdefault(\'train\', {}).setdefault(\'pytest_epochs\', 1)\n    config[\'train\'].setdefault(\'pytest_max_batches\', 2)\n    config[\'train\'].setdefault(\'pytest_max_test_batches\', 2)\n    _override_with_test_values(config)\n\n    config_path = test_configs_path / config_path\n    config_path.parent.mkdir(exist_ok=True, parents=True)\n    with config_path.open(""w"", encoding=\'utf8\') as fout:\n        json.dump(config, fout)\n\n\ndef install_config(config_path):\n    logfile = io.BytesIO(b\'\')\n    p = pexpect.popen_spawn.PopenSpawn(sys.executable + "" -m deeppavlov install "" + str(config_path), timeout=None,\n                                       logfile=logfile)\n    p.readlines()\n    if p.wait() != 0:\n        raise RuntimeError(\'Installing process of {} returned non-zero exit code: \\n{}\'\n                           .format(config_path, logfile.getvalue().decode()))\n\n\ndef setup_module():\n    shutil.rmtree(str(test_configs_path), ignore_errors=True)\n    shutil.rmtree(str(download_path), ignore_errors=True)\n    test_configs_path.mkdir(parents=True)\n\n    for m_name, conf_dict in PARAMS.items():\n        test_configs_path.joinpath(m_name).mkdir(exist_ok=True, parents=True)\n        for (config_path, _, _), _ in conf_dict.items():\n            download_config(config_path)\n\n    os.environ[\'DP_ROOT_PATH\'] = str(download_path)\n    os.environ[\'DP_CONFIGS_PATH\'] = str(test_configs_path)\n\n    if cache_dir:\n        cache_dir.mkdir(parents=True, exist_ok=True)\n        os.environ[\'DP_CACHE_DIR\'] = str(cache_dir.resolve())\n\n\ndef teardown_module():\n    shutil.rmtree(str(test_configs_path.parent), ignore_errors=True)\n    shutil.rmtree(str(download_path), ignore_errors=True)\n\n    if cache_dir:\n        shutil.rmtree(str(cache_dir), ignore_errors=True)\n\n\ndef _serialize(config):\n    chainer = build_model(config, download=True)\n    return chainer.serialize()\n\n\ndef _infer(config, inputs, download=False):\n    chainer = build_model(config, download=download)\n    return chainer(*inputs) if inputs else []\n\n\ndef _deserialize(config, raw_bytes, examples):\n    chainer = build_model(config, serialized=raw_bytes)\n    for *query, expected_response in examples:\n        query = [[q] for q in query]\n        actual_response = chainer(*query)\n        if expected_response is not None:\n            if actual_response is not None and len(actual_response) > 0:\n                actual_response = actual_response[0]\n            assert expected_response == str(actual_response), \\\n                f""Error in interacting with {model_dir} ({conf_file}): {query}""\n\n\n@pytest.mark.parametrize(""model,conf_file,model_dir,mode"", TEST_GRID, scope=\'class\')\nclass TestQuickStart(object):\n    @staticmethod\n    def infer(config_path, qr_list=None, check_outputs=True):\n\n        *inputs, expected_outputs = zip(*qr_list) if qr_list else ([],)\n        with ProcessPoolExecutor(max_workers=1) as executor:\n            f = executor.submit(_infer, config_path, inputs)\n        outputs = f.result()\n\n        if check_outputs:\n            errors = \';\'.join([f\'expected `{expected}` got `{output}`\'\n                               for output, expected in zip(outputs, expected_outputs)\n                               if expected is not None and expected != output])\n            if errors:\n                raise RuntimeError(f\'Unexpected results for {config_path}: {errors}\')\n\n    @staticmethod\n    def infer_api(config_path):\n        server_params = get_server_params(config_path)\n\n        url_base = \'http://{}:{}\'.format(server_params[\'host\'], api_port or server_params[\'port\'])\n        url = urljoin(url_base.replace(\'http://0.0.0.0:\', \'http://127.0.0.1:\'), server_params[\'model_endpoint\'])\n\n        post_headers = {\'Accept\': \'application/json\'}\n\n        logfile = io.BytesIO(b\'\')\n        args = [sys.executable, ""-m"", ""deeppavlov"", ""riseapi"", str(config_path)]\n        if api_port:\n            args += [\'-p\', str(api_port)]\n        p = pexpect.popen_spawn.PopenSpawn(\' \'.join(args),\n                                           timeout=None, logfile=logfile)\n        try:\n            p.expect(url_base)\n\n            get_url = urljoin(url_base.replace(\'http://0.0.0.0:\', \'http://127.0.0.1:\'), \'/api\')\n            get_response = requests.get(get_url)\n            response_code = get_response.status_code\n            assert response_code == 200, f""GET /api request returned error code {response_code} with {config_path}""\n\n            model_args_names = get_response.json()\n            post_payload = dict()\n            for arg_name in model_args_names:\n                arg_value = \' \'.join([\'qwerty\'] * 10)\n                post_payload[arg_name] = [arg_value]\n\n            post_response = requests.post(url, json=post_payload, headers=post_headers)\n            response_code = post_response.status_code\n            assert response_code == 200, f""POST request returned error code {response_code} with {config_path}""\n\n        except pexpect.exceptions.EOF:\n            raise RuntimeError(\'Got unexpected EOF: \\n{}\'.format(logfile.getvalue().decode()))\n\n        finally:\n            p.kill(signal.SIGTERM)\n            p.wait()\n            # if p.wait() != 0:\n            #     raise RuntimeError(\'Error in shutting down API server: \\n{}\'.format(logfile.getvalue().decode()))\n\n    @staticmethod\n    def infer_socket(config_path, socket_type):\n        socket_params = get_server_params(config_path)\n        model_args_names = socket_params[\'model_args_names\']\n\n        host = socket_params[\'host\']\n        host = host.replace(\'0.0.0.0\', \'127.0.0.1\')\n        port = api_port or socket_params[\'port\']\n\n        socket_payload = {}\n        for arg_name in model_args_names:\n            arg_value = \' \'.join([\'qwerty\'] * 10)\n            socket_payload[arg_name] = [arg_value]\n\n        logfile = io.BytesIO(b\'\')\n        args = [sys.executable, ""-m"", ""deeppavlov"", ""risesocket"", str(config_path), \'--socket-type\', socket_type]\n        if socket_type == \'TCP\':\n            args += [\'-p\', str(port)]\n            address_family = socket.AF_INET\n            connect_arg = (host, port)\n        else:\n            address_family = socket.AF_UNIX\n            connect_arg = socket_params[\'unix_socket_file\']\n        p = pexpect.popen_spawn.PopenSpawn(\' \'.join(args),\n                                           timeout=None, logfile=logfile)\n        try:\n            p.expect(socket_params[\'socket_launch_message\'])\n            with socket.socket(address_family, socket.SOCK_STREAM) as s:\n                try:\n                    s.connect(connect_arg)\n                except ConnectionRefusedError:\n                    sleep(1)\n                    s.connect(connect_arg)\n                s.sendall(encode(socket_payload))\n                s.settimeout(60)\n                header = s.recv(4)\n                body_len = unpack(\'<I\', header)[0]\n                data = bytearray()\n                while len(data) < body_len:\n                    chunk = s.recv(body_len - len(data))\n                    if not chunk:\n                        raise ValueError(f\'header does not match body\\nheader: {body_len}\\nbody length: {len(data)}\'\n                                         f\'data: {data}\')\n                    data.extend(chunk)\n            try:\n                resp = json.loads(data)\n            except json.decoder.JSONDecodeError:\n                raise ValueError(f""Can\'t decode model response {data}"")\n            assert resp[\'status\'] == \'OK\', f""{socket_type} socket request returned status: {resp[\'status\']}"" \\\n                                           f"" with {config_path}\\n{logfile.getvalue().decode()}""\n\n        except pexpect.exceptions.EOF:\n            raise RuntimeError(f\'Got unexpected EOF: \\n{logfile.getvalue().decode()}\')\n\n        except json.JSONDecodeError:\n            raise ValueError(f\'Got JSON not serializable response from model: ""{data}""\\n{logfile.getvalue().decode()}\')\n\n        finally:\n            p.kill(signal.SIGTERM)\n            p.wait()\n\n    def test_inferring_pretrained_model(self, model, conf_file, model_dir, mode):\n        if \'IP\' in mode:\n            config_file_path = str(test_configs_path.joinpath(conf_file))\n            install_config(config_file_path)\n            deep_download(config_file_path)\n\n            self.infer(test_configs_path / conf_file, PARAMS[model][(conf_file, model_dir, mode)])\n        else:\n            pytest.skip(""Unsupported mode: {}"".format(mode))\n\n    def test_inferring_pretrained_model_api(self, model, conf_file, model_dir, mode):\n        if \'IP\' in mode:\n            self.infer_api(test_configs_path / conf_file)\n        else:\n            pytest.skip(""Unsupported mode: {}"".format(mode))\n\n    def test_inferring_pretrained_model_socket(self, model, conf_file, model_dir, mode):\n        if \'IP\' in mode:\n            self.infer_socket(test_configs_path / conf_file, \'TCP\')\n\n            if \'TI\' not in mode:\n                shutil.rmtree(str(download_path), ignore_errors=True)\n        else:\n            pytest.skip(f""Unsupported mode: {mode}"")\n\n    def test_serialization(self, model, conf_file, model_dir, mode):\n        if \'SR\' not in mode:\n            return pytest.skip(""Unsupported mode: {}"".format(mode))\n\n        config_file_path = test_configs_path / conf_file\n\n        with ProcessPoolExecutor(max_workers=1) as executor:\n            f = executor.submit(_serialize, config_file_path)\n        raw_bytes = f.result()\n\n        serialized: list = pickle.loads(raw_bytes)\n        if not any(serialized):\n            pytest.skip(""Serialization not supported: {}"".format(conf_file))\n            return\n        serialized.clear()\n\n        with ProcessPoolExecutor(max_workers=1) as executor:\n            f = executor.submit(_deserialize, config_file_path, raw_bytes, PARAMS[model][(conf_file, model_dir, mode)])\n\n        exc = f.exception()\n        if exc is not None:\n            raise exc\n\n    def test_consecutive_training_and_inferring(self, model, conf_file, model_dir, mode):\n        if \'TI\' in mode:\n            c = test_configs_path / conf_file\n            model_path = download_path / model_dir\n\n            if \'IP\' not in mode:\n                config_path = str(test_configs_path.joinpath(conf_file))\n                install_config(config_path)\n                deep_download(config_path)\n            shutil.rmtree(str(model_path), ignore_errors=True)\n\n            logfile = io.BytesIO(b\'\')\n            p = pexpect.popen_spawn.PopenSpawn(sys.executable + "" -m deeppavlov train "" + str(c), timeout=None,\n                                               logfile=logfile)\n            p.readlines()\n            if p.wait() != 0:\n                raise RuntimeError(\'Training process of {} returned non-zero exit code: \\n{}\'\n                                   .format(model_dir, logfile.getvalue().decode()))\n            self.infer(c, PARAMS[model][(conf_file, model_dir, mode)], check_outputs=False)\n\n            shutil.rmtree(str(download_path), ignore_errors=True)\n        else:\n            pytest.skip(""Unsupported mode: {}"".format(mode))\n\n\ndef test_crossvalidation():\n    model_dir = \'faq\'\n    conf_file = \'cv/cv_tfidf_autofaq.json\'\n\n    download_config(conf_file)\n\n    c = test_configs_path / conf_file\n    model_path = download_path / model_dir\n\n    install_config(c)\n    deep_download(c)\n    shutil.rmtree(str(model_path), ignore_errors=True)\n\n    logfile = io.BytesIO(b\'\')\n    p = pexpect.popen_spawn.PopenSpawn(sys.executable + f"" -m deeppavlov crossval {c} --folds 2"",\n                                       timeout=None, logfile=logfile)\n    p.readlines()\n    if p.wait() != 0:\n        raise RuntimeError(\'Training process of {} returned non-zero exit code: \\n{}\'\n                           .format(model_dir, logfile.getvalue().decode()))\n\n    shutil.rmtree(str(download_path), ignore_errors=True)\n\n\ndef test_param_search():\n    model_dir = \'faq\'\n    conf_file = \'paramsearch/tfidf_logreg_autofaq_psearch.json\'\n\n    download_config(conf_file)\n\n    c = test_configs_path / conf_file\n    model_path = download_path / model_dir\n\n    install_config(c)\n    deep_download(c)\n\n    shutil.rmtree(str(model_path), ignore_errors=True)\n\n    logfile = io.BytesIO(b\'\')\n    p = pexpect.popen_spawn.PopenSpawn(sys.executable + f"" -m deeppavlov.paramsearch {c} --folds 2"",\n                                       timeout=None, logfile=logfile)\n    p.readlines()\n    if p.wait() != 0:\n        raise RuntimeError(\'Training process of {} returned non-zero exit code: \\n{}\'\n                           .format(model_dir, logfile.getvalue().decode()))\n\n    shutil.rmtree(str(download_path), ignore_errors=True)\n\n\ndef test_evolving():\n    model_dir = \'evolution\'\n    conf_file = \'evolution/evolve_intents_snips.json\'\n    download_config(conf_file)\n\n    c = test_configs_path / conf_file\n    model_path = download_path / model_dir\n\n    install_config(c)\n    deep_download(c)\n\n    shutil.rmtree(str(model_path), ignore_errors=True)\n\n    logfile = io.BytesIO(b\'\')\n    p = pexpect.popen_spawn.PopenSpawn(sys.executable + f"" -m deeppavlov.evolve {c} --iterations 1 --p_size 1"",\n                                       timeout=None, logfile=logfile)\n    p.readlines()\n    if p.wait() != 0:\n        raise RuntimeError(\'Training process of {} returned non-zero exit code: \\n{}\'\n                           .format(model_dir, logfile.getvalue().decode()))\n\n    shutil.rmtree(str(download_path), ignore_errors=True)\n\n\ndef test_hashes_existence():\n    all_configs = list(src_dir.glob(\'**/*.json\')) + list(test_src_dir.glob(\'**/*.json\'))\n    url_root = \'http://files.deeppavlov.ai/\'\n    downloads_urls = set()\n    for config in all_configs:\n        config = json.loads(config.read_text(encoding=\'utf-8\'))\n        downloads_urls |= {d if isinstance(d, str) else d[\'url\'] for d in\n                           config.get(\'metadata\', {}).get(\'download\', [])}\n    downloads_urls = [url + \'.md5\' for url in downloads_urls if url.startswith(url_root)]\n    messages = []\n\n    logging.getLogger(""urllib3"").setLevel(logging.WARNING)\n\n    for url in downloads_urls:\n        status = requests.get(url).status_code\n        if status != 200:\n            messages.append(f\'got status_code {status} for {url}\')\n    if messages:\n        raise RuntimeError(\'\\n\'.join(messages))\n'"
tests/test_rasa_skill.py,0,"b'from logging import getLogger\n\nfrom deeppavlov import configs, build_model\nfrom deeppavlov.utils.pip_wrapper.pip_wrapper import install_from_config\n\nlog = getLogger(__name__)\n\n\nclass TestRASASkill:\n    def setup(self):\n        config_ref = configs.skills.rasa_skill\n        install_from_config(config_ref)\n        self.rasa_skill = build_model(config_ref, download=True)\n\n    def test_simple_reaction(self):\n        user_messages_sequence = [\n            ""Hello"",\n            ""What can you do?"",\n            ""Tell me a joke"",\n            ""Learn my pants are Red"",\n            ""LET DISCUSS MOVIES"",\n            ""Comedy movies are nice to watch"",\n            ""I LIKE WATCHING COMEDY!"",\n            ""Ok, goodbye""\n        ]\n\n        history_of_responses = []\n        for each_utt in user_messages_sequence:\n            log.info(f""User says: {each_utt}"")\n            responses_batch, _ = self.rasa_skill([each_utt])\n            log.info(f"" Bot says: {responses_batch[0]}"")\n            history_of_responses.append(responses_batch)\n\n        print(""history_of_responses:"")\n        print(history_of_responses)\n        # # check the first greeting message in 0th batch\n        # assert ""Hey! How are you?"" in history_of_responses[0][0]\n        # # check second response message in 0th batch\n        # assert ""I can chat with you. You can greet me"" in history_of_responses[1][0]\n'"
tests/test_tf_layers.py,36,"b'import shutil\nfrom functools import reduce\nfrom pathlib import Path\n\nimport numpy as np\nimport pytest\nimport tensorflow as tf\n\nfrom deeppavlov.core.layers.tf_layers import cudnn_lstm, cudnn_compatible_lstm, cudnn_gru, cudnn_compatible_gru\n\ntests_dir = Path(__file__).parent\ntf_layers_data_path = tests_dir / ""tf_layers_data""\n\n\ndef setup_module():\n    shutil.rmtree(str(tf_layers_data_path), ignore_errors=True)\n    tf_layers_data_path.mkdir(parents=True)\n\n\ndef teardown_module():\n    shutil.rmtree(str(tf_layers_data_path), ignore_errors=True)\n\n\nclass DPCudnnLSTMModel:\n    def __init__(self, num_layers, num_units):\n        sess_config = tf.ConfigProto(allow_soft_placement=True)\n        sess_config.gpu_options.allow_growth = True\n        self.sess = tf.Session(config=sess_config)\n\n        self.x = tf.placeholder(shape=(None, None, 50), dtype=tf.float32)\n        with tf.variable_scope(\'cudnn_model\'):\n            h, (h_last, c_last) = cudnn_lstm(self.x, num_units, num_layers, trainable_initial_states=True)\n\n            self.h = h\n            self.h_last = h_last\n\n        self.sess.run(tf.global_variables_initializer())\n\n    def __call__(self, x):\n        feed_dict = {\n            self.x: x,\n        }\n        return self.sess.run([self.h, self.h_last], feed_dict=feed_dict)\n\n    def save(self, path=\'model\'):\n        print(\'[saving model to {}]\'.format(path))\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    def load(self, path):\n        saver = tf.train.Saver()\n        saver.restore(self.sess, path)\n\n\nclass DPLSTMModel:\n    def __init__(self, num_layers, num_units):\n        sess_config = tf.ConfigProto(allow_soft_placement=True)\n        sess_config.gpu_options.allow_growth = True\n        self.sess = tf.Session(config=sess_config)\n\n        self.x = tf.placeholder(shape=(None, None, 50), dtype=tf.float32)\n        with tf.variable_scope(\'cudnn_model\'):\n            h, (h_last, c_last) = cudnn_compatible_lstm(self.x, num_units, num_layers, trainable_initial_states=True)\n\n            self.h = h\n            self.h_last = h_last\n\n        self.sess.run(tf.global_variables_initializer())\n\n    def __call__(self, x):\n        feed_dict = {\n            self.x: x,\n        }\n        return self.sess.run([self.h, self.h_last], feed_dict=feed_dict)\n\n    def save(self, path=\'model\'):\n        print(\'[saving model to {}]\'.format(path))\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    def load(self, path):\n        saver = tf.train.Saver()\n        saver.restore(self.sess, path)\n\n\nclass DPCudnnGRUModel:\n    def __init__(self, num_layers, num_units):\n        sess_config = tf.ConfigProto(allow_soft_placement=True)\n        sess_config.gpu_options.allow_growth = True\n        self.sess = tf.Session(config=sess_config)\n\n        self.x = tf.placeholder(shape=(None, None, 50), dtype=tf.float32)\n        with tf.variable_scope(\'cudnn_model\'):\n            h, h_last = cudnn_gru(self.x, num_units, num_layers, trainable_initial_states=True)\n\n            self.h = h\n            self.h_last = h_last\n\n        self.sess.run(tf.global_variables_initializer())\n\n    def __call__(self, x):\n        feed_dict = {\n            self.x: x,\n        }\n        return self.sess.run([self.h, self.h_last], feed_dict=feed_dict)\n\n    def save(self, path=\'model\'):\n        print(\'[saving model to {}]\'.format(path))\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    def load(self, path):\n        saver = tf.train.Saver()\n        saver.restore(self.sess, path)\n\n\nclass DPGRUModel:\n    def __init__(self, num_layers, num_units):\n        sess_config = tf.ConfigProto(allow_soft_placement=True)\n        sess_config.gpu_options.allow_growth = True\n        self.sess = tf.Session(config=sess_config)\n\n        self.x = tf.placeholder(shape=(None, None, 50), dtype=tf.float32)\n        with tf.variable_scope(\'cudnn_model\'):\n            h, h_last = cudnn_compatible_gru(self.x, num_units, num_layers, trainable_initial_states=True)\n\n            self.h = h\n            self.h_last = h_last\n\n        self.sess.run(tf.global_variables_initializer())\n\n    def __call__(self, x):\n        feed_dict = {\n            self.x: x,\n        }\n        return self.sess.run([self.h, self.h_last], feed_dict=feed_dict)\n\n    def save(self, path=\'model\'):\n        print(\'[saving model to {}]\'.format(path))\n        saver = tf.train.Saver()\n        saver.save(self.sess, path)\n\n    def load(self, path):\n        saver = tf.train.Saver()\n        saver.restore(self.sess, path)\n\n\nclass TestTFLayers:\n    allowed_error_lvl = 0.01 * 2 ** 0.5\n\n    @staticmethod\n    def equal_values(a, b, round=5):\n        a, b = np.round(a, round), np.round(b, round)\n        return np.sum(a == b) / reduce(lambda x, y: x * y, a.shape)\n\n    @pytest.mark.parametrize(""num_layers"", [1, 3])\n    def test_cudnn_lstm_save_load(self, num_layers):\n        x = np.random.normal(size=(10, 10, 50))\n        tf.reset_default_graph()\n        cdnnlstmmodel = DPCudnnLSTMModel(num_layers=num_layers, num_units=100)\n        before_load_hidden, before_load_state = cdnnlstmmodel(x)[0], cdnnlstmmodel(x)[1]\n        cdnnlstmmodel.save(str(tf_layers_data_path / \'dpcudnnlstmmodel\' / \'model\'))\n\n        tf.reset_default_graph()\n        cdnnlstmmodel = DPCudnnLSTMModel(num_layers=num_layers, num_units=100)\n        cdnnlstmmodel.load(str(tf_layers_data_path / \'dpcudnnlstmmodel\' / \'model\'))\n        after_load_hidden, after_load_state = cdnnlstmmodel(x)[0], cdnnlstmmodel(x)[1]\n\n        equal_hidden = self.equal_values(after_load_hidden, before_load_hidden)\n        equal_state = self.equal_values(after_load_state, before_load_state)\n\n        assert equal_hidden > 1 - self.allowed_error_lvl\n        assert equal_state > 1 - self.allowed_error_lvl\n\n    @pytest.mark.parametrize(""num_layers"", [1, 3])\n    def test_cudnn_lstm_save_and_cudnn_compatible_load(self, num_layers):\n        x = np.random.normal(size=(10, 10, 50))\n        tf.reset_default_graph()\n        cdnnlstmmodel = DPCudnnLSTMModel(num_layers=num_layers, num_units=100)\n        before_load_hidden, before_load_state = cdnnlstmmodel(x)[0], cdnnlstmmodel(x)[1]\n        cdnnlstmmodel.save(str(tf_layers_data_path / \'dpcudnnlstmmodel\' / \'model\'))\n\n        tf.reset_default_graph()\n        cdnnlstmmodel = DPLSTMModel(num_layers=num_layers, num_units=100)\n        cdnnlstmmodel.load(str(tf_layers_data_path / \'dpcudnnlstmmodel\' / \'model\'))\n        after_load_hidden, after_load_state = cdnnlstmmodel(x)[0], cdnnlstmmodel(x)[1]\n\n        equal_hidden = self.equal_values(after_load_hidden, before_load_hidden)\n        equal_state = self.equal_values(after_load_state, before_load_state)\n\n        assert equal_hidden > 1 - self.allowed_error_lvl\n        assert equal_state > 1 - self.allowed_error_lvl\n\n    @pytest.mark.parametrize(""num_layers"", [1, 3])\n    def test_cudnn_gru_save_load(self, num_layers):\n        x = np.random.normal(size=(10, 10, 50))\n        tf.reset_default_graph()\n        cdnngrumodel = DPCudnnGRUModel(num_layers=num_layers, num_units=100)\n        before_load_hidden, before_load_state = cdnngrumodel(x)[0], cdnngrumodel(x)[1]\n        cdnngrumodel.save(str(tf_layers_data_path / \'cdnngrumodel\' / \'model\'))\n\n        tf.reset_default_graph()\n        cdnngrumodel = DPCudnnGRUModel(num_layers=num_layers, num_units=100)\n        cdnngrumodel.load(str(tf_layers_data_path / \'cdnngrumodel\' / \'model\'))\n        after_load_hidden, after_load_state = cdnngrumodel(x)[0], cdnngrumodel(x)[1]\n\n        equal_hidden = self.equal_values(after_load_hidden, before_load_hidden)\n        equal_state = self.equal_values(after_load_state, before_load_state)\n\n        assert equal_hidden > 1 - self.allowed_error_lvl\n        assert equal_state > 1 - self.allowed_error_lvl\n\n    @pytest.mark.parametrize(""num_layers"", [1, 3])\n    def test_cudnn_gru_save_and_cudnn_compatible_load(self, num_layers):\n        x = np.random.normal(size=(10, 10, 50))\n        tf.reset_default_graph()\n        cdnngrumodel = DPCudnnGRUModel(num_layers=num_layers, num_units=100)\n        before_load_hidden, before_load_state = cdnngrumodel(x)[0], cdnngrumodel(x)[1]\n        cdnngrumodel.save(str(tf_layers_data_path / \'cdnngrumodel\' / \'model\'))\n\n        tf.reset_default_graph()\n        cdnngrumodel = DPGRUModel(num_layers=num_layers, num_units=100)\n        cdnngrumodel.load(str(tf_layers_data_path / \'cdnngrumodel\' / \'model\'))\n        after_load_hidden, after_load_state = cdnngrumodel(x)[0], cdnngrumodel(x)[1]\n\n        equal_hidden = self.equal_values(after_load_hidden, before_load_hidden)\n        equal_state = self.equal_values(after_load_state, before_load_state)\n\n        assert equal_hidden > 1 - self.allowed_error_lvl\n        assert equal_state > 1 - self.allowed_error_lvl\n'"
deeppavlov/configs/__init__.py,0,"b'from pathlib import Path\nfrom typing import Iterator, Dict, Union, Iterable\n\n\nclass Struct:\n    def __iter__(self) -> Iterator[str]:\n        return iter(self._keys)\n\n    def __len__(self) -> int:\n        return len(self._keys)\n\n    def __init__(self, tree: Dict[str, Union[dict, Path]]) -> None:\n        self._keys = set()\n        for key, value in tree.items():\n            key = key.replace(\'.\', \'_\')\n            self._keys.add(key)\n            setattr(self, key,\n                    Struct(value) if isinstance(value, dict) else value)\n        self._keys = frozenset(self._keys)\n\n        self.keys = lambda: self._keys\n\n    def _asdict(self, *, to_string: bool=False) -> dict:\n        res = []\n        for key in self._keys:\n            value = getattr(self, key)\n            if isinstance(value, Struct):\n                value = value._asdict(to_string=to_string)\n            elif to_string:\n                value = str(value)\n            res.append((key, value))\n\n        return dict(res)\n\n    def __getitem__(self, key: str) -> Union[dict, Path]:\n        if key not in self._keys:\n            raise KeyError(key)\n\n        item = getattr(self, key)\n        if isinstance(item, Struct):\n            item = item._asdict()\n        return item\n\n    def __dir__(self) -> Iterable:\n        return self._keys\n\n    def _ipython_key_completions_(self) -> Iterable:\n        return self._keys\n\n    def __str__(self) -> str:\n        return str(self._asdict(to_string=True))\n\n    def __repr__(self) -> str:\n        return f\'Struct({repr(self._asdict())})\'\n\n    def _repr_pretty_(self, p, cycle):\n        """"""method that defines ``Struct``\'s pretty printing rules for iPython\n\n        Args:\n            p (IPython.lib.pretty.RepresentationPrinter): pretty printer object\n            cycle (bool): is ``True`` if pretty detected a cycle\n        """"""\n        if cycle:\n            p.text(\'Struct(...)\')\n        else:\n            with p.group(7, \'Struct(\', \')\'):\n                p.pretty(self._asdict())\n\n\ndef _build_configs_tree() -> Struct:\n    root = Path(__file__).resolve().parent\n\n    tree = {}\n\n    for config in root.glob(\'**/*.json\'):\n        leaf = tree\n        for part in config.relative_to(root).parent.parts:\n            if part not in leaf:\n                leaf[part] = {}\n            leaf = leaf[part]\n        leaf[config.stem] = config\n\n    return Struct(tree)\n\n\nconfigs = _build_configs_tree()\n'"
deeppavlov/core/__init__.py,0,b''
deeppavlov/dataset_iterators/__init__.py,0,b''
deeppavlov/dataset_iterators/basic_classification_iterator.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom logging import getLogger\nfrom typing import List\n\nfrom sklearn.model_selection import train_test_split\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.data_learning_iterator import DataLearningIterator\n\nlog = getLogger(__name__)\n\n\n@register(\'basic_classification_iterator\')\nclass BasicClassificationDatasetIterator(DataLearningIterator):\n    """"""\n    Class gets data dictionary from DatasetReader instance, merge fields if necessary, split a field if necessary\n\n    Args:\n        data: dictionary of data with fields ""train"", ""valid"" and ""test"" (or some of them)\n        fields_to_merge: list of fields (out of ``""train"", ""valid"", ""test""``) to merge\n        merged_field: name of field (out of ``""train"", ""valid"", ""test""``) to which save merged fields\n        field_to_split: name of field (out of ``""train"", ""valid"", ""test""``) to split\n        split_fields: list of fields (out of ``""train"", ""valid"", ""test""``) to which save splitted field\n        split_proportions: list of corresponding proportions for splitting\n        seed: random seed for iterating\n        shuffle: whether to shuffle examples in batches\n        split_seed: random seed for splitting dataset, if ``split_seed`` is None, division is based on `seed`.\n        stratify: whether to use stratified split\n        *args: arguments\n        **kwargs: arguments\n\n    Attributes:\n        data: dictionary of data with fields ""train"", ""valid"" and ""test"" (or some of them)\n    """"""\n\n    def __init__(self, data: dict,\n                 fields_to_merge: List[str] = None, merged_field: str = None,\n                 field_to_split: str = None, split_fields: List[str] = None, split_proportions: List[float] = None,\n                 seed: int = None, shuffle: bool = True, split_seed: int = None,\n                 stratify: bool = None,\n                 *args, **kwargs):\n        """"""\n        Initialize dataset using data from DatasetReader,\n        merges and splits fields according to the given parameters.\n        """"""\n        super().__init__(data, seed=seed, shuffle=shuffle)\n\n        if fields_to_merge is not None:\n            if merged_field is not None:\n                log.info(""Merging fields <<{}>> to new field <<{}>>"".format(fields_to_merge,\n                                                                            merged_field))\n                self._merge_data(fields_to_merge=fields_to_merge,\n                                 merged_field=merged_field)\n            else:\n                raise IOError(""Given fields to merge BUT not given name of merged field"")\n\n        if field_to_split is not None:\n            if split_fields is not None:\n                log.info(""Splitting field <<{}>> to new fields <<{}>>"".format(field_to_split,\n                                                                              split_fields))\n                self._split_data(field_to_split=field_to_split,\n                                 split_fields=split_fields,\n                                 split_proportions=[float(s) for s in\n                                                    split_proportions],\n                                 split_seed=split_seed,\n                                 stratify=stratify)\n            else:\n                raise IOError(""Given field to split BUT not given names of split fields"")\n\n    def _split_data(self, field_to_split: str = None, split_fields: List[str] = None,\n                    split_proportions: List[float] = None, split_seed: int = None, stratify: bool = None) -> bool:\n        """"""\n        Split given field of dataset to the given list of fields with corresponding proportions\n\n        Args:\n            field_to_split: field name (out of ``""train"", ""valid"", ""test""``) which to split\n            split_fields: list of names (out of ``""train"", ""valid"", ""test""``) of fields to which split\n            split_proportions: corresponding proportions\n            split_seed: random seed for splitting dataset\n            stratify: whether to use stratified split\n\n        Returns:\n            None\n        """"""\n        if split_seed is None:\n            split_seed = self.random.randint(0, 10000)\n        data_to_div = self.data[field_to_split].copy()\n        data_size = len(self.data[field_to_split])\n\n        for i in range(len(split_fields) - 1):\n            if stratify:\n                stratify = [sample[1] for sample in data_to_div]\n            self.data[split_fields[i]], data_to_div = train_test_split(\n                data_to_div,\n                test_size=len(data_to_div) - int(data_size * split_proportions[i]),\n                random_state=split_seed,\n                stratify=stratify)\n            self.data[split_fields[-1]] = data_to_div\n        return True\n\n    def _merge_data(self, fields_to_merge: List[str] = None, merged_field: str = None) -> bool:\n        """"""\n        Merge given fields of dataset\n\n        Args:\n            fields_to_merge: list of fields (out of ``""train"", ""valid"", ""test""``) to merge\n            merged_field: name of field (out of ``""train"", ""valid"", ""test""``) to which save merged fields\n\n        Returns:\n            None\n        """"""\n        data = self.data.copy()\n        data[merged_field] = []\n        for name in fields_to_merge:\n            data[merged_field] += self.data[name]\n        self.data = data\n        return True\n'"
deeppavlov/dataset_iterators/dialog_iterator.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom overrides import overrides\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.data_learning_iterator import DataLearningIterator\n\n\n@register(\'dialog_indexing_iterator\')\nclass DialogDatasetIndexingIterator(DataLearningIterator):\n    """"""\n    Iterates over dialog data,\n    generates batches where one sample is one dialog.\n    Assigns unique index value to each turn item of each dialog.\n\n    A subclass of :class:`~deeppavlov.core.data.data_learning_iterator.DataLearningIterator`.\n\n    Attributes:\n        train: list of training dialogs (tuples ``(context, response)``)\n        valid: list of validation dialogs (tuples ``(context, response)``)\n        test: list of dialogs used for testing (tuples ``(context, response)``)\n    """"""\n\n    Xs_LABEL = \'x\'\n    Ys_LABEL = \'y\'\n\n    @overrides\n    def preprocess(self, data, *args, **kwargs):\n        dialogs = []\n        prev_resp_act = None\n        for x, y in data:\n            if x.get(\'episode_done\'):\n                del x[\'episode_done\']\n                prev_resp_act = None\n                dialogs.append(([], []))\n            x[\'prev_resp_act\'] = prev_resp_act\n            prev_resp_act = y[\'act\']\n\n            dialogue_label = str(len(dialogs))\n            dialogue_x_item_label = str(len(dialogs[-1][0]))\n            dialogue_y_item_label = str(len(dialogs[-1][1]))\n\n            x_item_full_label = f""{self.Xs_LABEL}_{dialogue_label}_{dialogue_x_item_label}""\n            y_item_full_label = f""{self.Ys_LABEL}_{dialogue_label}_{dialogue_y_item_label}""\n\n            x[\'indexed_value\'] = x_item_full_label\n            y[\'indexed_value\'] = y_item_full_label\n\n            x[\'dialogue_label\'] = dialogue_label\n            y[\'dialogue_label\'] = dialogue_label\n\n            dialogs[-1][0].append(x)\n            dialogs[-1][1].append(y)\n        return dialogs\n\n\n@register(\'dialog_iterator\')\nclass DialogDatasetIterator(DataLearningIterator):\n    """"""\n    Iterates over dialog data,\n    generates batches where one sample is one dialog.\n\n    A subclass of :class:`~deeppavlov.core.data.data_learning_iterator.DataLearningIterator`.\n\n    Attributes:\n        train: list of training dialogs (tuples ``(context, response)``)\n        valid: list of validation dialogs (tuples ``(context, response)``)\n        test: list of dialogs used for testing (tuples ``(context, response)``)\n    """"""\n\n    @overrides\n    def preprocess(self, data, *args, **kwargs):\n        dialogs = []\n        prev_resp_act = None\n        for x, y in data:\n            if x.get(\'episode_done\'):\n                del x[\'episode_done\']\n                prev_resp_act = None\n                dialogs.append(([], []))\n            x[\'prev_resp_act\'] = prev_resp_act\n            prev_resp_act = y[\'act\']\n            dialogs[-1][0].append(x)\n            dialogs[-1][1].append(y)\n        return dialogs\n\n\n@register(\'dialog_db_result_iterator\')\nclass DialogDBResultDatasetIterator(DataLearningIterator):\n    """"""\n    Iterates over dialog data,\n    outputs list of all ``\'db_result\'`` fields (if present).\n\n    The class helps to build a list of all ``\'db_result\'`` values present in a dataset.\n\n    Inherits key methods and attributes from :class:`~deeppavlov.core.data.data_learning_iterator.DataLearningIterator`.\n\n    Attributes:\n        train: list of tuples ``(db_result dictionary, \'\')`` from ""train"" data\n        valid: list of tuples ``(db_result dictionary, \'\')`` from ""valid"" data\n        test: list of tuples ``(db_result dictionary, \'\')`` from ""test"" data\n    """"""\n\n    @staticmethod\n    def _db_result(data):\n        x, y = data\n        if \'db_result\' in x:\n            return x[\'db_result\']\n\n    @overrides\n    def preprocess(self, data, *args, **kwargs):\n        return [(r, """") for r in filter(None, map(self._db_result, data))]\n'"
deeppavlov/dataset_iterators/dstc2_intents_iterator.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom logging import getLogger\nfrom typing import List\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.dataset_iterators.basic_classification_iterator import BasicClassificationDatasetIterator\n\nlog = getLogger(__name__)\n\n\n@register(\'dstc2_intents_iterator\')\nclass Dstc2IntentsDatasetIterator(BasicClassificationDatasetIterator):\n    """"""\n    Class gets data dictionary from DSTC2DatasetReader instance, construct intents from act and slots, \\\n        merge fields if necessary, split a field if necessary\n\n    Args:\n        data: dictionary of data with fields ""train"", ""valid"" and ""test"" (or some of them)\n        fields_to_merge: list of fields (out of ``""train"", ""valid"", ""test""``) to merge\n        merged_field: name of field (out of ``""train"", ""valid"", ""test""``) to which save merged fields\n        field_to_split: name of field (out of ``""train"", ""valid"", ""test""``) to split\n        split_fields: list of fields (out of ``""train"", ""valid"", ""test""``) to which save splitted field\n        split_proportions: list of corresponding proportions for splitting\n        seed: random seed\n        shuffle: whether to shuffle examples in batches\n        *args: arguments\n        **kwargs: arguments\n\n    Attributes:\n        data: dictionary of data with fields ""train"", ""valid"" and ""test"" (or some of them)\n    """"""\n\n    def __init__(self, data: dict,\n                 fields_to_merge: List[str] = None, merged_field: str = None,\n                 field_to_split: str = None, split_fields: List[str] = None, split_proportions: List[float] = None,\n                 seed: int = None, shuffle: bool = True,\n                 *args, **kwargs):\n        """"""\n        Initialize dataset using data from DatasetReader,\n        merges and splits fields according to the given parameters\n        """"""\n        super().__init__(data, fields_to_merge, merged_field,\n                         field_to_split, split_fields, split_proportions,\n                         seed=seed, shuffle=shuffle)\n\n        new_data = dict()\n        new_data[\'train\'] = []\n        new_data[\'valid\'] = []\n        new_data[\'test\'] = []\n\n        for field in [\'train\', \'valid\', \'test\']:\n            for turn in self.data[field]:\n                reply = turn[0]\n                curr_intents = []\n                if reply[\'intents\']:\n                    for intent in reply[\'intents\']:\n                        for slot in intent[\'slots\']:\n                            if slot[0] == \'slot\':\n                                curr_intents.append(intent[\'act\'] + \'_\' + slot[1])\n                            else:\n                                curr_intents.append(intent[\'act\'] + \'_\' + slot[0])\n                        if len(intent[\'slots\']) == 0:\n                            curr_intents.append(intent[\'act\'])\n                else:\n                    if reply[\'text\']:\n                        curr_intents.append(\'unknown\')\n                    else:\n                        continue\n                new_data[field].append((reply[\'text\'], curr_intents))\n\n        self.data = new_data\n'"
deeppavlov/dataset_iterators/dstc2_ner_iterator.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport logging\nfrom typing import List, Tuple, Dict, Any\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.data_learning_iterator import DataLearningIterator\n\nlogger = logging.getLogger(__name__)\n\n\n@register(\'dstc2_ner_iterator\')\nclass Dstc2NerDatasetIterator(DataLearningIterator):\n    """"""\n    Iterates over data for DSTC2 NER task. Dataset takes a dict with fields \'train\', \'test\', \'valid\'. A list of samples\n    (pairs x, y) is stored in each field.\n\n    Args:\n        data: list of (x, y) pairs, samples from the dataset: x as well as y can be a tuple of different input features.\n        dataset_path: path to dataset\n        seed: value for random seed\n        shuffle: whether to shuffle the data\n    """"""\n\n    def __init__(self,\n                 data: Dict[str, List[Tuple]],\n                 slot_values_path: str,\n                 seed: int = None,\n                 shuffle: bool = False):\n        # TODO: include slot vals to dstc2.tar.gz\n        with expand_path(slot_values_path).open(encoding=\'utf8\') as f:\n            self._slot_vals = json.load(f)\n        super().__init__(data, seed, shuffle)\n\n    def preprocess(self,\n                   data: List[Tuple[Any, Any]],\n                   *args, **kwargs) -> List[Tuple[Any, Any]]:\n        processed_data = list()\n        processed_texts = dict()\n        for x, y in data:\n            text = x[\'text\']\n            if not text.strip():\n                continue\n            intents = []\n            if \'intents\' in x:\n                intents = x[\'intents\']\n            elif \'slots\' in x:\n                intents = [x]\n            # aggregate slots from different intents\n            slots = list()\n            for intent in intents:\n                current_slots = intent.get(\'slots\', [])\n                for slot_type, slot_val in current_slots:\n                    if not self._slot_vals or (slot_type in self._slot_vals):\n                        slots.append((slot_type, slot_val,))\n            # remove duplicate pairs (text, slots)\n            if (text in processed_texts) and (slots in processed_texts[text]):\n                continue\n            processed_texts[text] = processed_texts.get(text, []) + [slots]\n\n            processed_data.append(self._add_bio_markup(text, slots))\n        return processed_data\n\n    def _add_bio_markup(self,\n                        utterance: str,\n                        slots: List[Tuple[str, str]]) -> Tuple[List, List]:\n        tokens = utterance.split()\n        n_toks = len(tokens)\n        tags = [\'O\' for _ in range(n_toks)]\n        for n in range(n_toks):\n            for slot_type, slot_val in slots:\n                for entity in self._slot_vals[slot_type].get(slot_val,\n                                                             [slot_val]):\n                    slot_tokens = entity.split()\n                    slot_len = len(slot_tokens)\n                    if n + slot_len <= n_toks and \\\n                            self._is_equal_sequences(tokens[n: n + slot_len],\n                                                     slot_tokens):\n                        tags[n] = \'B-\' + slot_type\n                        for k in range(1, slot_len):\n                            tags[n + k] = \'I-\' + slot_type\n                        break\n        return tokens, tags\n\n    @staticmethod\n    def _is_equal_sequences(seq1, seq2):\n        equality_list = [tok1 == tok2 for tok1, tok2 in zip(seq1, seq2)]\n        return all(equality_list)\n'"
deeppavlov/dataset_iterators/elmo_file_paths_iterator.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import Tuple, Iterator, Optional, Dict, List, Union\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.simple_vocab import SimpleVocabulary\nfrom deeppavlov.core.data.utils import chunk_generator\nfrom deeppavlov.dataset_iterators.file_paths_iterator import FilePathsIterator\nfrom deeppavlov.models.preprocessors.str_utf8_encoder import StrUTF8Encoder\n\nlog = getLogger(__name__)\n\n\n@register(\'elmo_file_paths_iterator\')\nclass ELMoFilePathsIterator(FilePathsIterator):\n    """"""Dataset iterator for tokenized datasets like 1 Billion Word Benchmark\n    It gets lists of file paths from the data dictionary and returns batches of lines from each file.\n\n    Args:\n        data: dict with keys ``\'train\'``, ``\'valid\'`` and ``\'test\'`` and values\n        load_path: path to the vocabulary to be load from\n        seed: random seed for data shuffling\n        shuffle: whether to shuffle data during batching\n        unroll_steps: number of unrolling steps\n        n_gpus: number of gpu to use\n        max_word_length: max length of word\n        bos: tag of begin of sentence\n        eos: tag of end of sentence\n\n    """"""\n\n    def __init__(self,\n                 data: Dict[str, List[Union[str, Path]]],\n                 load_path: Union[str, Path],\n                 seed: Optional[int] = None,\n                 shuffle: bool = True,\n                 unroll_steps: Optional[int] = None,\n                 n_gpus: Optional[int] = None,\n                 max_word_length: Optional[int] = None,\n                 bos: str = ""<S>"",\n                 eos: str = ""</S>"",\n                 *args, **kwargs) -> None:\n        self.unroll_steps = unroll_steps\n        self.n_gpus = n_gpus\n        self.bos = bos\n        self.eos = eos\n        self.str_utf8_encoder = StrUTF8Encoder(\n            max_word_length=max_word_length,\n            pad_special_char_use=True,\n            word_boundary_special_char_use=True,\n            sentence_boundary_special_char_use=False,\n            reversed_sentense_tokens=False,\n            bos=self.bos,\n            eos=self.eos,\n            save_path=load_path,\n            load_path=load_path,\n        )\n        self.simple_vocab = SimpleVocabulary(\n            min_freq=2,\n            special_tokens=[self.eos, self.bos, ""<UNK>""],\n            unk_token=""<UNK>"",\n            freq_drop_load=True,\n            save_path=load_path,\n            load_path=load_path,\n        )\n        super().__init__(data, seed, shuffle, *args, **kwargs)\n\n    def _line2ids(self, line):\n        line = [self.bos] + line.split() + [self.eos]\n\n        char_ids = self.str_utf8_encoder(line)\n        reversed_char_ids = list(reversed(char_ids))\n        char_ids = char_ids[:-1]\n        reversed_char_ids = reversed_char_ids[:-1]\n\n        token_ids = self.simple_vocab(line)\n        reversed_token_ids = list(reversed(token_ids))\n        token_ids = token_ids[1:]\n        reversed_token_ids = reversed_token_ids[1:]\n\n        return char_ids, reversed_char_ids, token_ids, reversed_token_ids\n\n    def _line_generator(self, shard_generator):\n        for shard in shard_generator:\n            line_generator = chunk_generator(shard, 1)\n            for line in line_generator:\n                line = line[0]\n                char_ids, reversed_char_ids, token_ids, reversed_token_ids = \\\n                    self._line2ids(line)\n                yield char_ids, reversed_char_ids, token_ids, reversed_token_ids\n\n    @staticmethod\n    def _batch_generator(line_generator, batch_size, unroll_steps):\n        batch = [[[] for i in range(4)] for i in range(batch_size)]\n        stream = [[[] for i in range(4)] for i in range(batch_size)]\n\n        try:\n            while True:\n                for batch_item, stream_item in zip(batch, stream):\n                    while len(stream_item[0]) < unroll_steps:\n                        line = next(line_generator)\n                        for sti, lni in zip(stream_item, line):\n                            sti.extend(lni)\n                    for sti, bchi in zip(stream_item, batch_item):\n                        _b = sti[:unroll_steps]\n                        _s = sti[unroll_steps:]\n                        bchi.clear()\n                        _b = _b\n                        bchi.extend(_b)\n\n                        sti.clear()\n                        sti.extend(_s)\n                char_ids, reversed_char_ids, token_ids, reversed_token_ids = \\\n                    zip(*batch)\n                yield char_ids, reversed_char_ids, token_ids, reversed_token_ids\n        except StopIteration:\n            pass\n\n    def gen_batches(self, batch_size: int, data_type: str = \'train\', shuffle: Optional[bool] = None) \\\n            -> Iterator[Tuple[str, str]]:\n        if shuffle is None:\n            shuffle = self.shuffle\n\n        tgt_data = self.data[data_type]\n        shard_generator = self._shard_generator(tgt_data, shuffle=shuffle)\n        line_generator = self._line_generator(shard_generator)\n\n        if data_type == \'train\':\n            unroll_steps = self.unroll_steps\n            n_gpus = self.n_gpus\n        else:\n            unroll_steps = 1\n            batch_size = 256\n            n_gpus = 1\n\n        batch_generator = self._batch_generator(line_generator, batch_size * n_gpus, unroll_steps)\n\n        for char_ids, reversed_char_ids, token_ids, reversed_token_ids in batch_generator:\n            batch = [(char_ids, reversed_char_ids), (token_ids, reversed_token_ids)]\n            yield batch\n'"
deeppavlov/dataset_iterators/file_paths_iterator.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import Tuple, Iterator, Optional, Dict, List, Union\n\nimport numpy as np\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.data_learning_iterator import DataLearningIterator\nfrom deeppavlov.core.data.utils import chunk_generator\n\nlog = getLogger(__name__)\n\n\n@register(\'file_paths_iterator\')\nclass FilePathsIterator(DataLearningIterator):\n    """"""Dataset iterator for datasets like 1 Billion Word Benchmark.\n    It gets lists of file paths from the data dictionary and returns lines from each file.\n\n    Args:\n        data: dict with keys ``\'train\'``, ``\'valid\'`` and ``\'test\'`` and values\n        seed: random seed for data shuffling\n        shuffle: whether to shuffle data during batching\n\n    """"""\n\n    def __init__(self,\n                 data: Dict[str, List[Union[str, Path]]],\n                 seed: Optional[int] = None,\n                 shuffle: bool = True,\n                 *args, **kwargs) -> None:\n        self.seed = seed\n        self.np_random = np.random.RandomState(seed)\n        super().__init__(data, seed, shuffle, *args, **kwargs)\n\n    def _shard_generator(self, shards: List[Union[str, Path]], shuffle: bool = False) -> List[str]:\n        shards_to_choose = list(shards)\n        if shuffle:\n            self.np_random.shuffle(shards_to_choose)\n        for shard in shards_to_choose:\n            log.info(f\'Loaded shard from {shard}\')\n            with open(shard, encoding=\'utf-8\') as f:\n                lines = f.readlines()\n            if shuffle:\n                self.np_random.shuffle(lines)\n            yield lines\n\n    def gen_batches(self, batch_size: int, data_type: str = \'train\', shuffle: Optional[bool] = None) \\\n            -> Iterator[Tuple[str, str]]:\n        if shuffle is None:\n            shuffle = self.shuffle\n\n        tgt_data = self.data[data_type]\n        shard_generator = self._shard_generator(tgt_data, shuffle=shuffle)\n\n        for shard in shard_generator:\n            if not (batch_size):\n                bs = len(shard)\n            lines_generator = chunk_generator(shard, bs)\n            for lines in lines_generator:\n                yield (lines, [None] * len(lines))\n'"
deeppavlov/dataset_iterators/kvret_dialog_iterator.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom overrides import overrides\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.data_learning_iterator import DataLearningIterator\n\n\n@register(\'kvret_dialog_iterator\')\nclass KvretDialogDatasetIterator(DataLearningIterator):\n    """"""\n    Inputs data from :class:`~deeppavlov.dataset_readers.dstc2_reader.DSTC2DatasetReader`, constructs dialog history for each turn, generates batches (one sample is a turn).\n\n    Inherits key methods and attributes from :class:`~deeppavlov.core.data.data_learning_iterator.DataLearningIterator`.\n\n    Attributes:\n        train: list of ""train"" ``(context, response)`` tuples\n        valid: list of ""valid"" ``(context, response)`` tuples \n        test: list of ""test"" ``(context, response)`` tuples \n    """"""\n\n    # TODO: write custom batch_generator: order of utterances from one dialogue is presumed\n    @staticmethod\n    def _dialogs(data):\n        dialogs = []\n        history = []\n        task = None\n        for x, y in data:\n            if x.get(\'episode_done\'):\n                # history = []\n                history = """"\n                dialogs.append((([], [], [], [], []), ([], [])))\n                task = y[\'task\']\n            # history.append((x, y))\n            history = history + \' \' + x[\'text\'] + \' \' + y[\'text\']\n            # x[\'history\'] = history[:-1]\n            x[\'history\'] = history[:-len(x[\'text\']) - len(y[\'text\']) - 2]\n            dialogs[-1][0][0].append(x[\'text\'])\n            dialogs[-1][0][1].append(x[\'dialog_id\'])\n            dialogs[-1][0][2].append(x[\'history\'])\n            dialogs[-1][0][3].append(x.get(\'kb_columns\', None))\n            dialogs[-1][0][4].append(x.get(\'kb_items\', None))\n            dialogs[-1][1][0].append(y[\'text\'])\n            dialogs[-1][1][1].append(task)\n        return dialogs\n\n    @overrides\n    def preprocess(self, data, *args, **kwargs):\n        utters = []\n        history = []\n        for x, y in data:\n            if x.get(\'episode_done\'):\n                # x_hist, y_hist = [], []\n                history = """"\n            # x_hist.append(x[\'text\'])\n            # y_hist.append(y[\'text\'])\n            history = history + \' \' + x[\'text\'] + \' \' + y[\'text\']\n            # x[\'x_hist\'] = x_hist[:-1]\n            # x[\'y_hist\'] = y_hist[:-1]\n            x[\'history\'] = history[:-len(x[\'text\']) - len(y[\'text\']) - 2]\n            x_tuple = (x[\'text\'], x[\'dialog_id\'], x[\'history\'],\n                       x[\'kb_columns\'], x[\'kb_items\'])\n            y_tuple = (y[\'text\'], y[\'task\'][\'intent\'])\n            utters.append((x_tuple, y_tuple))\n        return utters\n'"
deeppavlov/dataset_iterators/morphotagger_iterator.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport random\nfrom typing import Tuple, List, Dict, Any, Iterator\n\nimport numpy as np\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.data_learning_iterator import DataLearningIterator\nfrom deeppavlov.models.preprocessors.capitalization import process_word\n\n\ndef preprocess_data(data: List[Tuple[List[str], List[str]]], to_lower: bool = True,\n                    append_case: str = ""first"") -> List[Tuple[List[Tuple[str]], List[str]]]:\n    """"""Processes all words in data using\n    :func:`~deeppavlov.dataset_iterators.morphotagger_iterator.process_word`.\n\n    Args:\n        data: a list of pairs (words, tags), each pair corresponds to a single sentence\n        to_lower: whether to lowercase\n        append_case: whether to add case mark\n\n    Returns:\n        a list of preprocessed sentences\n    """"""\n    new_data = []\n    for words, tags in data:\n        new_words = [process_word(word, to_lower=to_lower, append_case=append_case)\n                     for word in words]\n        # tags could also be processed in future\n        new_tags = tags\n        new_data.append((new_words, new_tags))\n    return new_data\n\n\n@register(\'morphotagger_dataset\')\nclass MorphoTaggerDatasetIterator(DataLearningIterator):\n    """"""\n    Iterates over data for Morphological Tagging.\n    A subclass of :class:`~deeppavlov.core.data.data_learning_iterator.DataLearningIterator`.\n\n    Args:\n        seed: random seed for data shuffling\n        shuffle: whether to shuffle data during batching\n        validation_split: the fraction of validation data\n            (is used only if there is no `valid` subset in `data`)\n        min_train_fraction: minimal fraction of train data in train+dev dataset,\n                For fair comparison with UD Pipe it is set to 0.9 for UD experiments.\n                It is actually used only for Turkish data.\n    """"""\n\n    def __init__(self, data: Dict[str, List[Tuple[Any, Any]]], seed: int = None,\n                 shuffle: bool = True, min_train_fraction: float = 0.0,\n                 validation_split: float = 0.2) -> None:\n        self.validation_split = validation_split\n        self.min_train_fraction = min_train_fraction\n        super().__init__(data, seed, shuffle)\n\n    def split(self, *args, **kwargs) -> None:\n        """"""\n        Splits the `train` part to `train` and `valid`, if no `valid` part is specified.\n        Moves deficient data from `valid` to `train` if both parts are given,\n        but `train` subset is too small.\n        """"""\n        if len(self.valid) == 0:\n            if self.shuffle:\n                self.random.shuffle(self.train)\n            L = int(len(self.train) * (1.0 - self.validation_split))\n            self.train, self.valid = self.train[:L], self.train[L:]\n        elif self.min_train_fraction > 0.0:\n            train_length = len(self.train)\n            valid_length = len(self.valid)\n            gap = int(self.min_train_fraction * (train_length + valid_length)) - train_length\n            if gap > 0:\n                self.train.extend(self.valid[:gap])\n                self.valid = self.valid[gap:]\n        return\n\n    def gen_batches(self, batch_size: int, data_type: str = \'train\',\n                    shuffle: bool = None, return_indexes: bool = False) -> Iterator[tuple]:\n        """"""Generate batches of inputs and expected output to train neural networks\n        Args:\n            batch_size: number of samples in batch\n            data_type: can be either \'train\', \'test\', or \'valid\'\n            shuffle: whether to shuffle dataset before batching\n            return_indexes: whether to return indexes of batch elements in initial dataset\n        Yields:\n            a tuple of a batch of inputs and a batch of expected outputs.\n            If `return_indexes` is True, also yields indexes of batch elements.\n        """"""\n        if shuffle is None:\n            shuffle = self.shuffle\n        data = self.data[data_type]\n        lengths = [len(x[0]) for x in data]\n        indexes = np.argsort(lengths)\n        L = len(data)\n        if batch_size < 0:\n            batch_size = L\n        starts = list(range(0, L, batch_size))\n        if shuffle:\n            self.random.shuffle(starts)\n        for start in starts:\n            indexes_to_yield = indexes[start:start + batch_size]\n            data_to_yield = tuple(list(x) for x in zip(*([data[i] for i in indexes_to_yield])))\n            if return_indexes:\n                yield indexes_to_yield, data_to_yield\n            else:\n                yield data_to_yield\n'"
deeppavlov/dataset_iterators/ner_few_shot_iterator.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\nfrom typing import List, Dict, Tuple, Any, Iterator, Optional\n\nimport numpy as np\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.data_learning_iterator import DataLearningIterator\n\n\n@register(\'ner_few_shot_iterator\')\nclass NERFewShotIterator(DataLearningIterator):\n    """"""Dataset iterator for simulating few-shot Named Entity Recognition setting.\n\n    Args:\n        data: list of (x, y) pairs for every data type in ``\'train\'``, ``\'valid\'`` and ``\'test\'``\n        seed: random seed for data shuffling\n        shuffle: whether to shuffle data during batching\n        target_tag: the tag of interest. For this tag the few-shot setting will be simulated\n        filter_bi: whether to filter BIO markup or not\n        n_train_samples: number of training samples in the few shot setting. The validation and the test sets will be\n            the same\n        remove_not_targets: whether to replace all non target tags with `O` tag or not.\n    """"""\n\n    def __init__(self,\n                 data: Dict[str, List[Tuple[Any, Any]]],\n                 seed: int = None,\n                 shuffle: bool = True,\n                 target_tag: str = None,\n                 filter_bi: bool = True,\n                 n_train_samples: int = 20,\n                 remove_not_targets: bool = True,\n                 *args, **kwargs) -> None:\n        super(NERFewShotIterator, self).__init__(data=data, seed=seed, shuffle=shuffle)\n        self.target_tag = target_tag\n        self.filter_bi = filter_bi\n        self.n_train_samples = n_train_samples\n        self.remove_not_targets = remove_not_targets\n        if self.target_tag is None:\n            raise RuntimeError(\'You must provide a target tag to NERFewShotIterator!\')\n\n        self.n_samples = len(self.train)\n\n        if self.remove_not_targets:\n            self._remove_not_target_tags()\n\n        if self.filter_bi:\n            for key in self.data:\n                for n, (x, y) in enumerate(self.data[key]):\n                    self.data[key][n] = [x, [re.sub(\'(B-|I-)\', \'\', tag) for tag in y]]\n\n        self.tag_map = np.zeros(self.n_samples, dtype=bool)\n        for n, (toks, tags) in enumerate(self.data[\'train\']):\n            if self.filter_bi:\n                self.tag_map[n] = any(self.target_tag == tag for tag in tags if len(tag) > 2)\n            else:\n                self.tag_map[n] = any(self.target_tag == tag[2:] for tag in tags if len(tag) > 2)\n\n        self.marked_nums = None\n        self.unmarked_nums = None\n        self._sample_marked()\n\n    def _sample_marked(self):\n        np.zeros(len(self.data[\'train\']), dtype=bool)\n        n_marked = 0\n        self.marked_mask = np.zeros(self.n_samples, dtype=bool)\n        while n_marked < self.n_train_samples:\n            is_picked = True\n            while is_picked:\n                n = np.random.randint(self.n_samples)\n                if not self.marked_mask[n]:\n                    is_picked = False\n                    self.marked_mask[n] = True\n                    if self.tag_map[n]:\n                        n_marked += 1\n\n        self.marked_nums = np.arange(self.n_samples)[self.marked_mask]\n        self.unmarked_nums = np.arange(self.n_samples)[~self.marked_mask]\n\n    def _remove_not_target_tags(self):\n        if self.remove_not_targets:\n            for key in self.data:\n                for n, (x, y) in enumerate(self.data[key]):\n                    tags = []\n                    for tag in y:\n                        if tag.endswith(\'-\' + self.target_tag):\n                            tags.append(tag)\n                        else:\n                            tags.append(\'O\')\n                    self.data[key][n] = [x, tags]\n\n    def get_instances(self, data_type: str = \'train\') -> Tuple[List[List[str]], List[List[str]]]:\n        """"""Get all data for a selected data type\n\n        Args:\n            data_type (str): can be either ``\'train\'``, ``\'test\'``, ``\'valid\'`` or ``\'all\'``\n\n        Returns:\n             a tuple of all inputs for a data type and all expected outputs for a data type\n        """"""\n\n        if data_type == \'train\':\n            samples = [self.data[data_type][i] for i in self.marked_nums]\n        else:\n            samples = self.data[data_type][:]\n\n        x, y = list(zip(*samples))\n\n        return x, y\n\n    def gen_batches(self, batch_size: int,\n                    data_type: str = \'train\',\n                    shuffle: Optional[bool] = None) -> Iterator[Tuple[List[List[str]], List[List[str]]]]:\n        x, y = self.get_instances(data_type)\n        data_len = len(x)\n\n        if data_len == 0:\n            return\n\n        order = list(range(data_len))\n        if shuffle is None and self.shuffle:\n            self.random.shuffle(order)\n        elif shuffle:\n            self.random.shuffle(order)\n\n        if batch_size < 0:\n            batch_size = data_len\n\n        for i in range((data_len - 1) // batch_size + 1):\n            yield tuple(zip(*[(x[o], y[o]) for o in order[i * batch_size:(i + 1) * batch_size]]))\n'"
deeppavlov/dataset_iterators/siamese_iterator.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import Dict, List, Tuple\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.data_learning_iterator import DataLearningIterator\n\nlog = getLogger(__name__)\n\n\n@register(\'siamese_iterator\')\nclass SiameseIterator(DataLearningIterator):\n    """"""The class contains methods for iterating over a dataset for ranking in training, validation and test mode.""""""\n\n    def split(self, *args, len_valid=1000, len_test=1000, **kwargs) -> None:\n        if len(self.valid) == 0 and len_valid != 0:\n            self.random.shuffle(self.train)\n            self.valid = self.train[-len_valid:]\n            self.train = self.train[:-len_valid]\n        if len(self.test) == 0 and len_test != 0:\n            self.random.shuffle(self.train)\n            self.test = self.train[-len_test:]\n            self.train = self.train[:-len_test]\n'"
deeppavlov/dataset_iterators/snips_intents_iterator.py,0,"b'# Copyright 2019 Alexey Romanov\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom overrides import overrides\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.data_learning_iterator import DataLearningIterator\n\n\n@register(\'snips_intents_iterator\')\nclass SnipsIntentIterator(DataLearningIterator):\n    @overrides\n    def preprocess(self, data, *args, **kwargs):\n        result = []\n        for query in data:\n            text = \'\'.join(part[\'text\'] for part in query[\'data\'])\n            intent = query[\'intent\']\n            result.append((text, intent))\n        return result\n'"
deeppavlov/dataset_iterators/snips_ner_iterator.py,0,"b'# Copyright 2019 Alexey Romanov\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport nltk\nfrom overrides import overrides\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.data_learning_iterator import DataLearningIterator\n\n\n@register(\'snips_ner_iterator\')\nclass SnipsNerIterator(DataLearningIterator):\n    @overrides\n    def preprocess(self, data, *args, **kwargs):\n        result = []\n        for query in data:\n            query = query[\'data\']\n            words = []\n            slots = []\n            for part in query:\n                part_words = nltk.tokenize.wordpunct_tokenize(part[\'text\'])\n                entity = part.get(\'entity\', None)\n                if entity:\n                    slots.append(\'B-\' + entity)\n                    slots += [\'I-\' + entity] * (len(part_words) - 1)\n                else:\n                    slots += [\'O\'] * len(part_words)\n                words += part_words\n\n            result.append((words, slots))\n        return result\n'"
deeppavlov/dataset_iterators/sqlite_iterator.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sqlite3\nfrom logging import getLogger\nfrom pathlib import Path\nfrom random import Random\nfrom typing import List, Any, Dict, Optional, Union, Generator, Tuple\n\nfrom overrides import overrides\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.data_fitting_iterator import DataFittingIterator\n\nlogger = getLogger(__name__)\n\n\n@register(\'sqlite_iterator\')\nclass SQLiteDataIterator(DataFittingIterator):\n    """"""Iterate over SQLite database.\n    Gen batches from SQLite data.\n    Get document ids and document.\n\n    Args:\n        load_path: a path to local DB file\n        batch_size: a number of samples in a single batch\n        shuffle: whether to shuffle data during batching\n        seed: random seed for data shuffling\n\n    Attributes:\n        connect: a DB connection\n        db_name: a DB name\n        doc_ids: DB document ids\n        doc2index: a dictionary of document indices and their titles\n        batch_size: a number of samples in a single batch\n        shuffle: whether to shuffle data during batching\n        random: an instance of :class:`Random` class.\n\n    """"""\n\n    def __init__(self, load_path: Union[str, Path], batch_size: Optional[int] = None,\n                 shuffle: Optional[bool] = None, seed: Optional[int] = None, **kwargs) -> None:\n\n        load_path = str(expand_path(load_path))\n        logger.info(""Connecting to database, path: {}"".format(load_path))\n        try:\n            self.connect = sqlite3.connect(load_path, check_same_thread=False)\n        except sqlite3.OperationalError as e:\n            e.args = e.args + (""Check that DB path exists and is a valid DB file"",)\n            raise e\n        try:\n            self.db_name = self.get_db_name()\n        except TypeError as e:\n            e.args = e.args + (\n                \'Check that DB path was created correctly and is not empty. \'\n                \'Check that a correct dataset_format is passed to the ODQAReader config\',)\n            raise e\n        self.doc_ids = self.get_doc_ids()\n        self.doc2index = self.map_doc2idx()\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.random = Random(seed)\n\n    @overrides\n    def get_doc_ids(self) -> List[Any]:\n        """"""Get document ids.\n\n        Returns:\n            document ids\n        """"""\n        cursor = self.connect.cursor()\n        cursor.execute(\'SELECT id FROM {}\'.format(self.db_name))\n        ids = [ids[0] for ids in cursor.fetchall()]\n        cursor.close()\n        return ids\n\n    def get_db_name(self) -> str:\n        """"""Get DB name.\n\n        Returns:\n            DB name\n\n        """"""\n        cursor = self.connect.cursor()\n        cursor.execute(""SELECT name FROM sqlite_master WHERE type=\'table\';"")\n        assert cursor.arraysize == 1\n        name = cursor.fetchone()[0]\n        cursor.close()\n        return name\n\n    def map_doc2idx(self) -> Dict[int, Any]:\n        """"""Map DB ids to integer ids.\n\n        Returns:\n            a dictionary of document titles and correspondent integer indices\n\n        """"""\n        doc2idx = {doc_id: i for i, doc_id in enumerate(self.doc_ids)}\n        logger.info(\n            ""SQLite iterator: The size of the database is {} documents"".format(len(doc2idx)))\n        return doc2idx\n\n    @overrides\n    def get_doc_content(self, doc_id: Any) -> Optional[str]:\n        """"""Get document content by id.\n\n        Args:\n            doc_id: a document id\n\n        Returns:\n            document content if success, else raise Exception\n\n        """"""\n        cursor = self.connect.cursor()\n        cursor.execute(\n            ""SELECT text FROM {} WHERE id = ?"".format(self.db_name),\n            (doc_id,)\n        )\n        result = cursor.fetchone()\n        cursor.close()\n        return result if result is None else result[0]\n\n    @overrides\n    def gen_batches(self, batch_size: int, shuffle: bool = None) \\\n            -> Generator[Tuple[List[str], List[int]], Any, None]:\n        """"""Gen batches of documents.\n\n        Args:\n            batch_size: a number of samples in a single batch\n            shuffle: whether to shuffle data during batching\n\n        Yields:\n            generated tuple of documents and their ids\n\n        """"""\n        if shuffle is None:\n            shuffle = self.shuffle\n\n        if shuffle:\n            _doc_ids = self.random.sample(self.doc_ids, len(self.doc_ids))\n        else:\n            _doc_ids = self.doc_ids\n\n        if batch_size > 0:\n            batches = [_doc_ids[i:i + batch_size] for i in\n                       range(0, len(_doc_ids), batch_size)]\n        else:\n            batches = [_doc_ids]\n\n        for i, doc_ids in enumerate(batches):\n            docs = [self.get_doc_content(doc_id) for doc_id in doc_ids]\n            doc_nums = [self.doc2index[doc_id] for doc_id in doc_ids]\n            yield docs, zip(doc_ids, doc_nums)\n\n    def get_instances(self):\n        """"""Get all data""""""\n        doc_ids = list(self.doc_ids)\n        docs = [self.get_doc_content(doc_id) for doc_id in doc_ids]\n        doc_nums = [self.doc2index[doc_id] for doc_id in doc_ids]\n        return docs, zip(doc_ids, doc_nums)\n'"
deeppavlov/dataset_iterators/squad_iterator.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport json\nfrom typing import Dict, Any, List, Tuple, Generator, Optional\n\nimport numpy as np\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.data_learning_iterator import DataLearningIterator\n\n\n@register(\'squad_iterator\')\nclass SquadIterator(DataLearningIterator):\n    """"""SquadIterator allows to iterate over examples in SQuAD-like datasets.\n    SquadIterator is used to train :class:`~deeppavlov.models.squad.squad.SquadModel`.\n\n    It extracts ``context``, ``question``, ``answer_text`` and ``answer_start`` position from dataset.\n    Example from a dataset is a tuple of ``(context, question)`` and ``(answer_text, answer_start)``\n\n    Attributes:\n        train: train examples\n        valid: validation examples\n        test: test examples\n\n    """"""\n\n    def preprocess(self, data: Dict[str, Any], *args, **kwargs) -> \\\n            List[Tuple[Tuple[str, str], Tuple[List[str], List[int]]]]:\n        """"""Extracts context, question, answer, answer_start from SQuAD data\n\n        Args:\n            data: data in squad format\n\n        Returns:\n            list of (context, question), (answer_text, answer_start)\n            answer text and answer_start are lists\n\n        """"""\n        cqas = []\n        if data:\n            for article in data[\'data\']:\n                for par in article[\'paragraphs\']:\n                    context = par[\'context\']\n                    for qa in par[\'qas\']:\n                        q = qa[\'question\']\n                        ans_text = []\n                        ans_start = []\n                        for answer in qa[\'answers\']:\n                            ans_text.append(answer[\'text\'])\n                            ans_start.append(answer[\'answer_start\'])\n                        cqas.append(((context, q), (ans_text, ans_start)))\n        return cqas\n\n\n@register(\'multi_squad_iterator\')\nclass MultiSquadIterator(DataLearningIterator):\n    """"""Dataset iterator for multiparagraph-SQuAD dataset.\n\n    With ``with_answer_rate`` rate samples context with answer and with ``1 - with_answer_rate`` samples context\n    from the same article, but without an answer. Contexts without an answer are sampled according to\n    their tfidf scores (tfidf score between question and context).\n\n    It extracts ``context``, ``question``, ``answer_text`` and ``answer_start`` position from dataset.\n    Example from a dataset is a tuple of ``(context, question)`` and ``(answer_text, answer_start)``. If there is\n    no answer in context, then ``answer_text`` is empty string and `answer_start` is equal to -1.\n\n    Args:\n        data: dict with keys ``\'train\'``, ``\'valid\'`` and ``\'test\'`` and values\n        seed: random seed for data shuffling\n        shuffle: whether to shuffle data during batching\n        with_answer_rate: sampling rate of contexts with answer\n\n    Attributes:\n        shuffle: whether to shuffle data during batching\n        random: instance of ``Random`` initialized with a seed\n    """"""\n\n    def __init__(self, data, seed: Optional[int] = None, shuffle: bool = True, with_answer_rate: float = 0.666,\n                 *args, **kwargs) -> None:\n        self.with_answer_rate = with_answer_rate\n        self.seed = seed\n        self.np_random = np.random.RandomState(seed)\n        super().__init__(data, seed, shuffle, *args, **kwargs)\n\n    def gen_batches(self, batch_size: int, data_type: str = \'train\', shuffle: bool = None) \\\n            -> Generator[Tuple[Tuple[Tuple[str, str]], Tuple[List[str], List[int]]], None, None]:\n\n        if shuffle is None:\n            shuffle = self.shuffle\n\n        if data_type == \'train\':\n            random = self.np_random\n        else:\n            random = np.random.RandomState(self.seed)\n\n        if shuffle:\n            random.shuffle(self.data[data_type])\n\n        data = self.data[data_type]\n        data_len = len(data)\n\n        for i in range((data_len - 1) // batch_size + 1):\n            batch = []\n            for j in range(i * batch_size, min((i + 1) * batch_size, data_len)):\n                q = data[j][\'question\']\n                contexts = data[j][\'contexts\']\n                ans_contexts = [c for c in contexts if len(c[\'answer\']) > 0]\n                noans_contexts = [c for c in contexts if len(c[\'answer\']) == 0]\n                # sample context with answer or without answer\n                if random.rand() < self.with_answer_rate or len(noans_contexts) == 0:\n                    # select random context with answer\n                    context = random.choice(ans_contexts)\n                else:\n                    # select random context without answer\n                    # prob ~ context tfidf score\n                    noans_scores = np.array([x[\'score\'] for x in noans_contexts])\n                    noans_scores = noans_scores / np.sum(noans_scores)\n                    context = noans_contexts[np.argmax(random.multinomial(1, noans_scores))]\n\n                answer_text = [ans[\'text\'] for ans in context[\'answer\']] if len(context[\'answer\']) > 0 else [\'\']\n                answer_start = [ans[\'answer_start\']\n                                for ans in context[\'answer\']] if len(context[\'answer\']) > 0 else [-1]\n                batch.append(((context[\'context\'], q), (answer_text, answer_start)))\n            yield tuple(zip(*batch))\n\n    def get_instances(self, data_type: str = \'train\') -> Tuple[Tuple[Tuple[str, str]], Tuple[List[str], List[int]]]:\n        data_examples = []\n        for qcas in self.data[data_type]:  # question, contexts, answers\n            question = qcas[\'question\']\n            for context in qcas[\'contexts\']:\n                answer_text = [x[\'text\'] for x in context[\'answer\']]\n                answer_start = [x[\'answer_start\'] for x in context[\'answer\']]\n                data_examples.append(((context[\'context\'], question), (answer_text, answer_start)))\n        return tuple(zip(*data_examples))\n\n\n@register(\'multi_squad_retr_iterator\')\nclass MultiSquadRetrIterator(DataLearningIterator):\n    """"""Dataset iterator for multiparagraph-SQuAD dataset.\n\n    reads data from jsonl files\n\n    With ``with_answer_rate`` rate samples context with answer and with ``1 - with_answer_rate`` samples context\n    from the same article, but without an answer. Contexts without an answer are sampled from uniform distribution.\n    If ``with_answer_rate`` is None than we compute actual ratio for each data example.\n\n    It extracts ``context``, ``question``, ``answer_text`` and ``answer_start`` position from dataset.\n    Example from a dataset is a tuple of ``(context, question)`` and ``(answer_text, answer_start)``. If there is\n    no answer in context, then ``answer_text`` is empty string and `answer_start` is equal to -1.\n\n    Args:\n        data: dict with keys ``\'train\'``, ``\'valid\'`` and ``\'test\'`` and values\n        seed: random seed for data shuffling\n        shuffle: whether to shuffle data during batching\n        with_answer_rate: sampling rate of contexts with answer\n        squad_rate: sampling rate of context from squad dataset (actual rate would be with_answer_rate * squad_rate)\n\n    Attributes:\n        shuffle: whether to shuffle data during batching\n        random: instance of ``Random`` initialized with a seed\n    """"""\n\n    def __init__(self, data, seed: Optional[int] = None, shuffle: bool = False,\n                 with_answer_rate: Optional[float] = None,\n                 squad_rate: Optional[float] = None, *args, **kwargs) -> None:\n        self.with_answer_rate = with_answer_rate\n        self.squad_rate = squad_rate\n        self.seed = seed\n        self.np_random = np.random.RandomState(seed)\n        self.shuffle = shuffle\n\n        self.train = data.get(\'train\', [])\n        self.valid = data.get(\'valid\', [])\n        self.test = data.get(\'test\', [])\n\n        self.data = {\n            \'train\': self.train,\n            \'valid\': self.valid,\n            \'test\': self.test,\n        }\n\n        if self.shuffle:\n            raise RuntimeError(\'MultiSquadIterator doesn\\\'t support shuffling.\')\n\n    def gen_batches(self, batch_size: int, data_type: str = \'train\', shuffle: bool = None) \\\n            -> Generator[Tuple[Tuple[Tuple[str, str]], Tuple[List[str], List[int]]], None, None]:\n\n        if shuffle is None:\n            shuffle = self.shuffle\n\n        if data_type == \'train\':\n            random = self.np_random\n        else:\n            random = np.random.RandomState(self.seed)\n\n        if shuffle:\n            raise RuntimeError(\'MultiSquadIterator doesn\\\'t support shuffling.\')\n\n        datafile = self.data[data_type]\n        with datafile.open(\'r\', encoding=\'utf8\') as fin:\n            end_of_file = False\n            while not end_of_file:\n                batch = []\n                for i in range(batch_size):\n                    line = fin.readline()\n                    if len(line) == 0:\n                        end_of_file = True\n                        break\n\n                    qcas = json.loads(line)\n                    q = qcas[\'question\']\n                    contexts = qcas[\'contexts\']\n                    ans_contexts = [c for c in contexts if len(c[\'answer\']) > 0]\n                    noans_contexts = [c for c in contexts if len(c[\'answer\']) == 0]\n                    ans_clen = len(ans_contexts)\n                    noans_clen = len(noans_contexts)\n                    # sample context with answer or without answer\n                    with_answer_rate = self.with_answer_rate\n                    if with_answer_rate is None:\n                        with_answer_rate = 1.0 if noans_clen == 0 else ans_clen / (ans_clen + noans_clen)\n\n                    if random.rand() < with_answer_rate or noans_clen == 0:\n                        # select random context with answer\n                        if self.squad_rate is not None:\n                            if random.rand() < self.squad_rate or len(ans_contexts) == 1:\n                                # first context is always from squad dataset\n                                context = ans_contexts[0]\n                            else:\n                                context = random.choice(ans_contexts[1:])\n                        else:\n                            context = random.choice(ans_contexts)\n                    else:\n                        # select random context without answer\n                        # prob ~ context tfidf score\n                        # noans_scores = np.array([x[\'score\'] for x in noans_contexts])\n                        # noans_scores = noans_scores / np.sum(noans_scores)\n                        # context = noans_contexts[np.argmax(random.multinomial(1, noans_scores))]\n                        context = random.choice(noans_contexts)\n\n                    answer_text = [ans[\'text\'] for ans in context[\'answer\']] if len(context[\'answer\']) > 0 else [\'\']\n                    answer_start = [ans[\'answer_start\']\n                                    for ans in context[\'answer\']] if len(context[\'answer\']) > 0 else [-1]\n                    batch.append(((context[\'context\'], q), (answer_text, answer_start)))\n                if batch:\n                    yield tuple(zip(*batch))\n\n    def get_instances(self, data_type: str = \'train\') -> Tuple[Tuple[Tuple[str, str]], Tuple[List[str], List[int]]]:\n        data_examples = []\n        for f in self.data[data_type]:  # question, contexts, answers\n            for line in f.open(\'r\', encoding=\'utf8\'):\n                qcas = json.loads(line)\n                question = qcas[\'question\']\n                for context in qcas[\'contexts\']:\n                    answer_text = [x[\'text\'] for x in context[\'answer\']]\n                    answer_start = [x[\'answer_start\'] for x in context[\'answer\']]\n                    data_examples.append(((context[\'context\'], question), (answer_text, answer_start)))\n        return tuple(zip(*data_examples))\n'"
deeppavlov/dataset_iterators/typos_iterator.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.data_learning_iterator import DataLearningIterator\n\n\n@register(\'typos_iterator\')\nclass TyposDatasetIterator(DataLearningIterator):\n    """"""Implementation of :class:`~deeppavlov.core.data.data_learning_iterator.DataLearningIterator` used for training\n     :class:`~deeppavlov.models.spelling_correction.brillmoore.ErrorModel`\n\n    """"""\n\n    def split(self, test_ratio: float = 0., *args, **kwargs):\n        """"""Split all data into train and test\n\n        Args:\n            test_ratio: ratio of test data to train, from 0. to 1.\n        """"""\n        self.train += self.valid + self.test\n\n        split = int(len(self.train) * test_ratio)\n\n        self.random.shuffle(self.train)\n\n        self.test = self.train[:split]\n        self.train = self.train[split:]\n        self.valid = []\n'"
deeppavlov/dataset_readers/__init__.py,0,b''
deeppavlov/dataset_readers/amazon_ecommerce_reader.py,0,"b'# Copyright 2018 Neural Networks and Deep Learning lab, MIPT\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import List, Any, Dict, Tuple\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.dataset_reader import DatasetReader\nfrom deeppavlov.core.data.utils import download_decompress, mark_done, is_done\n\nlogger = getLogger(__name__)\n\n\n@register(\'amazon_ecommerce_reader\')\nclass AmazonEcommerceReader(DatasetReader):\n    """"""Class to download and load ecommerce data catalog""""""\n\n    def read(self, data_path: str, catalog: list, **kwargs) -> Dict[str, List[Tuple[Any, Any]]]:\n        """"""Load data from specific catalog\n\n        Parameters:\n            data_path: where the dataset is located\n            catalog: names of the specific subcategories\n\n        Returns:\n            dataset: loaded dataset\n        """"""\n\n        logger.info(f""Ecommerce loader is loaded with catalog {catalog}"")\n\n        if not isinstance(catalog, list):\n            catalog = [catalog]\n\n        ec_data_global: List[Any] = []\n        data_path = Path(expand_path(data_path))\n\n        if not is_done(data_path):\n            self._download_data(data_path)\n\n        if data_path.is_dir():\n            for fname in data_path.rglob(""*.txt""):\n                if any(cat in fname.name for cat in catalog):\n                    logger.info(f""File {fname.name} is loaded"")\n                    ec_data_global += self._load_amazon_ecommerce_file(fname)\n\n        dataset = {\n            \'train\': [((item[\'Title\'], [], {}), item) for item in ec_data_global],\n            \'valid\': [],\n            \'test\': []\n        }\n\n        logger.info(f""In total {len(ec_data_global)} items are loaded"")\n        return dataset\n\n    def _download_data(self, data_path: str) -> None:\n        """"""Download dataset""""""\n        url = ""https://github.com/SamTube405/Amazon-E-commerce-Data-set/archive/master.zip""\n        download_decompress(url, data_path)\n        mark_done(data_path)\n\n    def _load_amazon_ecommerce_file(self, fname: str) -> List[Dict[Any, Any]]:\n        """"""Parse dataset\n\n        Parameters:\n            fname: catalog file\n            \n        Returns:\n            ec_data: parsed catalog data\n        """"""\n\n        ec_data = []\n        item: Dict = {}\n        new_item_re = re.compile(""ITEM *\\d+"")\n\n        with open(fname, \'r\', encoding=\'utf-8\', errors=\'ignore\') as file:\n            for line in file:\n                if new_item_re.match(line):\n                    if len(item.keys()) > 0:\n                        if \'Title\' in item and \'Feature\' in item:\n                            ec_data.append(item)\n                    item = {\'Item\': int(line[5:]), \'Category\': fname.name.split(""_"")[1]}\n                else:\n                    row = line.strip().split(""="")\n                    if len(row) == 2:\n                        if row[0] in item:\n                            item[row[0]] += ""."" + row[1]\n                        else:\n                            item[row[0]] = row[1]\n        return ec_data\n'"
deeppavlov/dataset_readers/basic_classification_reader.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom logging import getLogger\nfrom pathlib import Path\n\nimport pandas as pd\nfrom overrides import overrides\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.dataset_reader import DatasetReader\nfrom deeppavlov.core.data.utils import download\n\nlog = getLogger(__name__)\n\n\n@register(\'basic_classification_reader\')\nclass BasicClassificationDatasetReader(DatasetReader):\n    """"""\n    Class provides reading dataset in .csv format\n    """"""\n\n    @overrides\n    def read(self, data_path: str, url: str = None,\n             format: str = ""csv"", class_sep: str = None,\n             *args, **kwargs) -> dict:\n        """"""\n        Read dataset from data_path directory.\n        Reading files are all data_types + extension\n        (i.e for data_types=[""train"", ""valid""] files ""train.csv"" and ""valid.csv"" form\n        data_path will be read)\n\n        Args:\n            data_path: directory with files\n            url: download data files if data_path not exists or empty\n            format: extension of files. Set of Values: ``""csv"", ""json""``\n            class_sep: string separator of labels in column with labels\n            sep (str): delimeter for ``""csv""`` files. Default: None -> only one class per sample\n            header (int): row number to use as the column names\n            names (array): list of column names to use\n            orient (str): indication of expected JSON string format\n            lines (boolean): read the file as a json object per line. Default: ``False``\n\n        Returns:\n            dictionary with types from data_types.\n            Each field of dictionary is a list of tuples (x_i, y_i)\n        """"""\n        data_types = [""train"", ""valid"", ""test""]\n\n        train_file = kwargs.get(\'train\', \'train.csv\')\n\n        if not Path(data_path, train_file).exists():\n            if url is None:\n                raise Exception(\n                    ""data path {} does not exist or is empty, and download url parameter not specified!"".format(\n                        data_path))\n            log.info(""Loading train data from {} to {}"".format(url, data_path))\n            download(source_url=url, dest_file_path=Path(data_path, train_file))\n\n        data = {""train"": [],\n                ""valid"": [],\n                ""test"": []}\n        for data_type in data_types:\n            file_name = kwargs.get(data_type, \'{}.{}\'.format(data_type, format))\n            if file_name is None:\n                continue\n            \n            file = Path(data_path).joinpath(file_name)\n            if file.exists():\n                if format == \'csv\':\n                    keys = (\'sep\', \'header\', \'names\')\n                    options = {k: kwargs[k] for k in keys if k in kwargs}\n                    df = pd.read_csv(file, **options)\n                elif format == \'json\':\n                    keys = (\'orient\', \'lines\')\n                    options = {k: kwargs[k] for k in keys if k in kwargs}\n                    df = pd.read_json(file, **options)\n                else:\n                    raise Exception(\'Unsupported file format: {}\'.format(format))\n\n                x = kwargs.get(""x"", ""text"")\n                y = kwargs.get(\'y\', \'labels\')\n                if isinstance(x, list):\n                    if class_sep is None:\n                        # each sample is a tuple (""text"", ""label"")\n                        data[data_type] = [([row[x_] for x_ in x], str(row[y]))\n                                           for _, row in df.iterrows()]\n                    else:\n                        # each sample is a tuple (""text"", [""label"", ""label"", ...])\n                        data[data_type] = [([row[x_] for x_ in x], str(row[y]).split(class_sep))\n                                           for _, row in df.iterrows()]\n                else:\n                    if class_sep is None:\n                        # each sample is a tuple (""text"", ""label"")\n                        data[data_type] = [(row[x], str(row[y])) for _, row in df.iterrows()]\n                    else:\n                        # each sample is a tuple (""text"", [""label"", ""label"", ...])\n                        data[data_type] = [(row[x], str(row[y]).split(class_sep)) for _, row in df.iterrows()]\n            else:\n                log.warning(""Cannot find {} file"".format(file))\n\n        return data\n'"
deeppavlov/dataset_readers/conll2003_reader.py,0,"b'from logging import getLogger\nfrom pathlib import Path\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.dataset_reader import DatasetReader\nfrom deeppavlov.core.data.utils import download_decompress\n\nlog = getLogger(__name__)\n\n\n@register(\'conll2003_reader\')\nclass Conll2003DatasetReader(DatasetReader):\n    """"""Class to read training datasets in CoNLL-2003 format""""""\n\n    def read(self,\n             data_path: str,\n             dataset_name: str = None,\n             provide_pos: bool = False,\n             provide_chunk: bool = False,\n             provide_doc_ids: bool = False,\n             iob: bool = False,\n             iobes: bool = False,\n             docstart_token: str = None):\n        self.provide_pos = provide_pos\n        self.provide_chunk = provide_chunk\n        self.provide_doc_ids = provide_doc_ids\n        self.iob = iob\n        self.iobes = iobes\n        self.docstart_token = docstart_token\n        self.num_docs = 0\n        self.x_is_tuple = self.provide_pos or self.provide_doc_ids\n        data_path = Path(data_path)\n        files = list(data_path.glob(\'*.txt\'))\n        if \'train.txt\' not in {file_path.name for file_path in files}:\n            if dataset_name == \'conll2003\':\n                url = \'http://files.deeppavlov.ai/deeppavlov_data/conll2003_v2.tar.gz\'\n            elif dataset_name == \'collection_rus\':\n                url = \'http://files.deeppavlov.ai/deeppavlov_data/collection3_v2.tar.gz\'\n            elif dataset_name == \'ontonotes\':\n                url = \'http://files.deeppavlov.ai/deeppavlov_data/ontonotes_ner.tar.gz\'\n            elif dataset_name == \'vlsp2016\':\n                url = \'http://files.deeppavlov.ai/deeppavlov_data/vlsp2016.tar.gz\'\n            elif dataset_name == \'dailydialog\':\n                url = \'http://files.deeppavlov.ai/deeppavlov_data/dailydialog.tar.gz\'\n            else:\n                raise RuntimeError(\'train.txt not found in ""{}""\'.format(data_path))\n            data_path.mkdir(exist_ok=True, parents=True)\n            download_decompress(url, data_path)\n            files = list(data_path.glob(\'*.txt\'))\n        dataset = {}\n\n        for file_name in files:\n            name = file_name.with_suffix(\'\').name\n            dataset[name] = self.parse_ner_file(file_name)\n        return dataset\n\n    def parse_ner_file(self, file_name: Path):\n        samples = []\n        with file_name.open(encoding=\'utf8\') as f:\n            tokens = []\n            pos_tags = []\n            chunk_tags = []\n            tags = []\n            expected_items = 2 + int(self.provide_pos) + int(self.provide_chunk)\n            for line in f:\n                # Check end of the document\n                if \'DOCSTART\' in line:\n                    if len(tokens) > 1:\n                        x = tokens if not self.x_is_tuple else (tokens,)\n                        if self.provide_pos:\n                            x = x + (pos_tags,)\n                        if self.provide_chunk:\n                            x = x + (chunk_tags,)\n                        if self.provide_doc_ids:\n                            x = x + (self.num_docs,)\n                        samples.append((x, tags))\n                        tokens = []\n                        pos_tags = []\n                        chunk_tags = []\n                        tags = []\n                    self.num_docs += 1\n                    if self.docstart_token is not None:\n                        tokens = [self.docstart_token]\n                        pos_tags = [\'O\']\n                        chunk_tags = [\'O\']\n                        tags = [\'O\']\n                elif len(line) < 2:\n                    if (len(tokens) > 0) and (tokens != [self.docstart_token]):\n                        x = tokens if not self.x_is_tuple else (tokens,)\n                        if self.provide_pos:\n                            x = x + (pos_tags,)\n                        if self.provide_chunk:\n                            x = x + (chunk_tags,)\n                        if self.provide_doc_ids:\n                            x = x + (self.num_docs,)\n                        samples.append((x, tags))\n                        tokens = []\n                        pos_tags = []\n                        chunk_tags = []\n                        tags = []\n                else:\n                    items = line.split()\n                    if len(items) < expected_items:\n                        raise Exception(f""Input is not valid {line}"")\n                    tokens.append(items[0])\n                    tags.append(items[-1])\n                    if self.provide_pos:\n                        pos_tags.append(items[1])\n                    if self.provide_chunk:\n                        chunk_tags.append(items[2])\n            if tokens:\n                x = tokens if not self.x_is_tuple else (tokens,)\n                if self.provide_pos:\n                    x = x + (pos_tags,)\n                if self.provide_chunk:\n                    x = x + (chunk_tags,)\n                if self.provide_doc_ids:\n                    x = x + (self.num_docs,)\n                samples.append((x, tags))\n                self.num_docs += 1\n\n            if self.iob:\n                return [(x, self._iob2_to_iob(tags)) for x, tags in samples]\n            if self.iobes:\n                return [(x, self._iob2_to_iobes(tags)) for x, tags in samples]\n\n        return samples\n\n    @staticmethod\n    def _iob2_to_iob(tags):\n        iob_tags = []\n\n        for n, tag in enumerate(tags):\n            if tag.startswith(\'B-\') and (not n or (tags[n - 1][2:] != tag[2:])):\n                tag = tag.replace(""B-"", ""I-"")\n            iob_tags.append(tag)\n\n        return iob_tags\n\n    @staticmethod\n    def _iob2_to_iobes(tags):\n        tag_map = {""BB"": ""S"", ""BO"": ""S"", ""IB"": ""E"", ""IO"": ""E""}\n        tags = tags + [""O""]\n        iobes_tags = []\n        for i in range(len(tags) - 1):\n            tagtag = tags[i][0] + tags[i + 1][0]\n            if tagtag in tag_map:\n                iobes_tags.append(tag_map[tagtag] + tags[i][1:])\n            else:\n                iobes_tags.append(tags[i])\n        return iobes_tags\n'"
deeppavlov/dataset_readers/dstc2_reader.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, softwaredata\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport copy\nimport json\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import Dict, List\n\nfrom overrides import overrides\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.dataset_reader import DatasetReader\nfrom deeppavlov.core.data.utils import download_decompress, mark_done\n\nlog = getLogger(__name__)\n\n\n@register(\'dstc2_reader\')\nclass DSTC2DatasetReader(DatasetReader):\n    """"""\n    Contains labelled dialogs from Dialog State Tracking Challenge 2\n    (http://camdial.org/~mh521/dstc/).\n\n    There\'ve been made the following modifications to the original dataset:\n\n       1. added api calls to restaurant database\n\n          - example: ``{""text"": ""api_call area=\\""south\\"" food=\\""dontcare\\""\n            pricerange=\\""cheap\\"""", ""dialog_acts"": [""api_call""]}``.\n\n       2. new actions\n\n          - bot dialog actions were concatenated into one action\n            (example: ``{""dialog_acts"": [""ask"", ""request""]}`` ->\n            ``{""dialog_acts"": [""ask_request""]}``)\n\n          - if a slot key was associated with the dialog action, the new act\n            was a concatenation of an act and a slot key (example:\n            ``{""dialog_acts"": [""ask""], ""slot_vals"": [""area""]}`` ->\n            ``{""dialog_acts"": [""ask_area""]}``)\n\n       3. new train/dev/test split\n\n          - original dstc2 consisted of three different MDP policies, the original\n            train and dev datasets (consisting of two policies) were merged and\n            randomly split into train/dev/test\n\n       4. minor fixes\n\n          - fixed several dialogs, where actions were wrongly annotated\n          - uppercased first letter of bot responses\n          - unified punctuation for bot responses\n    """"""\n\n    url = \'http://files.deeppavlov.ai/datasets/dstc2_v2.tar.gz\'\n\n    @staticmethod\n    def _data_fname(datatype):\n        assert datatype in (\'trn\', \'val\', \'tst\'), ""wrong datatype name""\n        return f""dstc2-{datatype}.jsonlist""\n\n    @classmethod\n    @overrides\n    def read(self, data_path: str, dialogs: bool = False) -> Dict[str, List]:\n        """"""\n        Downloads ``\'dstc2_v2.tar.gz\'`` archive from ipavlov internal server,\n        decompresses and saves files to ``data_path``.\n\n        Parameters:\n            data_path: path to save DSTC2 dataset\n            dialogs: flag which indicates whether to output list of turns or\n             list of dialogs\n\n        Returns:\n            dictionary that contains ``\'train\'`` field with dialogs from\n            ``\'dstc2-trn.jsonlist\'``, ``\'valid\'`` field with dialogs from\n            ``\'dstc2-val.jsonlist\'`` and ``\'test\'`` field with dialogs from\n            ``\'dstc2-tst.jsonlist\'``. Each field is a list of tuples ``(x_i, y_i)``.\n        """"""\n        required_files = (self._data_fname(dt) for dt in (\'trn\', \'val\', \'tst\'))\n        if not all(Path(data_path, f).exists() for f in required_files):\n            log.info(f""[downloading data from {self.url} to {data_path}]"")\n            download_decompress(self.url, data_path)\n            mark_done(data_path)\n\n        data = {\n            \'train\': self._read_from_file(\n                Path(data_path, self._data_fname(\'trn\')), dialogs),\n            \'valid\': self._read_from_file(\n                Path(data_path, self._data_fname(\'val\')), dialogs),\n            \'test\': self._read_from_file(\n                Path(data_path, self._data_fname(\'tst\')), dialogs)\n        }\n        return data\n\n    @classmethod\n    def _read_from_file(cls, file_path, dialogs=False):\n        """"""Returns data from single file""""""\n        log.info(f""[loading dialogs from {file_path}]"")\n\n        utterances, responses, dialog_indices = \\\n            cls._get_turns(cls._iter_file(file_path), with_indices=True)\n\n        data = list(map(cls._format_turn, zip(utterances, responses)))\n\n        if dialogs:\n            return [data[idx[\'start\']:idx[\'end\']] for idx in dialog_indices]\n        return data\n\n    @staticmethod\n    def _format_turn(turn):\n        turn_x, turn_y = turn\n        x = {\'text\': turn_x[\'text\'],\n             \'intents\': turn_x[\'dialog_acts\']}\n        if turn_x.get(\'db_result\') is not None:\n            x[\'db_result\'] = turn_x[\'db_result\']\n        if turn_x.get(\'episode_done\'):\n            x[\'episode_done\'] = True\n        y = {\'text\': turn_y[\'text\'],\n             \'act\': turn_y[\'dialog_acts\'][0][\'act\']}\n        return (x, y)\n\n    @staticmethod\n    def _iter_file(file_path):\n        for ln in open(file_path, \'rt\', encoding=\'utf8\'):\n            if ln.strip():\n                yield json.loads(ln)\n            else:\n                yield {}\n\n    @staticmethod\n    def _get_turns(data, with_indices=False):\n        utterances = []\n        responses = []\n        dialog_indices = []\n        n = 0\n        num_dialog_utter, num_dialog_resp = 0, 0\n        episode_done = True\n        for turn in data:\n            if not turn:\n                if num_dialog_utter != num_dialog_resp:\n                    raise RuntimeError(""Datafile in the wrong format."")\n                episode_done = True\n                n += num_dialog_utter\n                dialog_indices.append({\n                    \'start\': n - num_dialog_utter,\n                    \'end\': n,\n                })\n                num_dialog_utter, num_dialog_resp = 0, 0\n            else:\n                speaker = turn.pop(\'speaker\')\n                if speaker == 1:\n                    if episode_done:\n                        turn[\'episode_done\'] = True\n                    utterances.append(turn)\n                    num_dialog_utter += 1\n                elif speaker == 2:\n                    if num_dialog_utter - 1 == num_dialog_resp:\n                        responses.append(turn)\n                    elif num_dialog_utter - 1 < num_dialog_resp:\n                        if episode_done:\n                            responses.append(turn)\n                            utterances.append({\n                                ""text"": """",\n                                ""dialog_acts"": [],\n                                ""episode_done"": True}\n                            )\n                        else:\n                            new_turn = copy.deepcopy(utterances[-1])\n                            if \'db_result\' not in responses[-1]:\n                                raise RuntimeError(f""Every api_call action""\n                                                   f"" should have db_result,""\n                                                   f"" turn = {responses[-1]}"")\n                            new_turn[\'db_result\'] = responses[-1].pop(\'db_result\')\n                            utterances.append(new_turn)\n                            responses.append(turn)\n                        num_dialog_utter += 1\n                    else:\n                        raise RuntimeError(""there cannot be two successive turns of""\n                                           "" speaker 1"")\n                    num_dialog_resp += 1\n                else:\n                    raise RuntimeError(""Only speakers 1 and 2 are supported"")\n                episode_done = False\n\n        if with_indices:\n            return utterances, responses, dialog_indices\n        return utterances, responses\n\n\n@register(\'simple_dstc2_reader\')\nclass SimpleDSTC2DatasetReader(DatasetReader):\n    """"""\n    Contains labelled dialogs from Dialog State Tracking Challenge 2\n    (http://camdial.org/~mh521/dstc/).\n\n    There\'ve been made the following modifications to the original dataset:\n\n       1. added api calls to restaurant database\n\n          - example: ``{""text"": ""api_call area=\\""south\\"" food=\\""dontcare\\""\n            pricerange=\\""cheap\\"""", ""dialog_acts"": [""api_call""]}``.\n\n       2. new actions\n\n          - bot dialog actions were concatenated into one action\n            (example: ``{""dialog_acts"": [""ask"", ""request""]}`` ->\n            ``{""dialog_acts"": [""ask_request""]}``)\n\n          - if a slot key was associated with the dialog action, the new act\n            was a concatenation of an act and a slot key (example:\n            ``{""dialog_acts"": [""ask""], ""slot_vals"": [""area""]}`` ->\n            ``{""dialog_acts"": [""ask_area""]}``)\n\n       3. new train/dev/test split\n\n          - original dstc2 consisted of three different MDP policies, the original\n            train and dev datasets (consisting of two policies) were merged and\n            randomly split into train/dev/test\n\n       4. minor fixes\n\n          - fixed several dialogs, where actions were wrongly annotated\n          - uppercased first letter of bot responses\n          - unified punctuation for bot responses\n    """"""\n\n    url = \'http://files.deeppavlov.ai/datasets/simple_dstc2.tar.gz\'\n\n    @staticmethod\n    def _data_fname(datatype):\n        assert datatype in (\'trn\', \'val\', \'tst\'), ""wrong datatype name""\n        return f""simple-dstc2-{datatype}.json""\n\n    @classmethod\n    @overrides\n    def read(self, data_path: str, dialogs: bool = False, encoding = \'utf-8\') -> Dict[str, List]:\n        """"""\n        Downloads ``\'simple_dstc2.tar.gz\'`` archive from internet,\n        decompresses and saves files to ``data_path``.\n\n        Parameters:\n            data_path: path to save DSTC2 dataset\n            dialogs: flag which indicates whether to output list of turns or\n             list of dialogs\n\n        Returns:\n            dictionary that contains ``\'train\'`` field with dialogs from\n            ``\'simple-dstc2-trn.json\'``, ``\'valid\'`` field with dialogs\n            from ``\'simple-dstc2-val.json\'`` and ``\'test\'`` field with\n            dialogs from ``\'simple-dstc2-tst.json\'``.\n            Each field is a list of tuples ``(user turn, system turn)``.\n        """"""\n        required_files = (self._data_fname(dt) for dt in (\'trn\', \'val\', \'tst\'))\n        if not all(Path(data_path, f).exists() for f in required_files):\n            log.info(f""{[Path(data_path, f) for f in required_files]}]"")\n            log.info(f""[downloading data from {self.url} to {data_path}]"")\n            download_decompress(self.url, data_path)\n            mark_done(data_path)\n\n        data = {\n            \'train\': self._read_from_file(\n                Path(data_path, self._data_fname(\'trn\')), dialogs, encoding),\n            \'valid\': self._read_from_file(\n                Path(data_path, self._data_fname(\'val\')), dialogs, encoding),\n            \'test\': self._read_from_file(\n                Path(data_path, self._data_fname(\'tst\')), dialogs, encoding)\n        }\n        log.info(f""There are {len(data[\'train\'])} samples in train split."")\n        log.info(f""There are {len(data[\'valid\'])} samples in valid split."")\n        log.info(f""There are {len(data[\'test\'])} samples in test split."")\n        return data\n\n    @classmethod\n    def _read_from_file(cls, file_path: str, dialogs: bool = False, encoding = \'utf-8\'):\n        """"""Returns data from single file""""""\n        log.info(f""[loading dialogs from {file_path}]"")\n\n        utterances, responses, dialog_indices = \\\n            cls._get_turns(json.load(open(file_path, mode = \'rt\', encoding = encoding)), with_indices=True)\n\n        data = list(map(cls._format_turn, zip(utterances, responses)))\n\n        if dialogs:\n            return [data[idx[\'start\']:idx[\'end\']] for idx in dialog_indices]\n        return data\n\n    @staticmethod\n    def _format_turn(turn):\n        turn_x, turn_y = turn\n        x = {\'text\': turn_x[\'text\']}\n        y = {\'text\': turn_y[\'text\'],\n             \'act\': turn_y[\'act\']}\n        if \'act\' in turn_x:\n            x[\'intents\'] = turn_x[\'act\']\n        if \'episode_done\' in turn_x:\n            x[\'episode_done\'] = turn_x[\'episode_done\']\n        if turn_x.get(\'db_result\') is not None:\n            x[\'db_result\'] = turn_x[\'db_result\']\n        if turn_x.get(\'slots\'):\n            x[\'slots\'] = turn_x[\'slots\']\n        if turn_y.get(\'slots\'):\n            y[\'slots\'] = turn_y[\'slots\']\n        return (x, y)\n\n    @staticmethod\n    def _get_turns(data, with_indices=False):\n        n = 0\n        utterances, responses, dialog_indices = [], [], []\n        for dialog in data:\n            cur_n_utter, cur_n_resp = 0, 0\n            for i, turn in enumerate(dialog):\n                speaker = turn.pop(\'speaker\')\n                if speaker == 1:\n                    if i == 0:\n                        turn[\'episode_done\'] = True\n                    utterances.append(turn)\n                    cur_n_utter += 1\n                elif speaker == 2:\n                    responses.append(turn)\n                    cur_n_resp += 1\n                    if cur_n_utter not in range(cur_n_resp - 2, cur_n_resp + 1):\n                        raise RuntimeError(""Datafile has wrong format."")\n                    if cur_n_utter != cur_n_resp:\n                        if i == 0:\n                            new_utter = {\n                                ""text"": """",\n                                ""episode_done"": True\n                            }\n                        else:\n                            new_utter = copy.deepcopy(utterances[-1])\n                            if \'db_result\' not in responses[-2]:\n                                raise RuntimeError(""Every api_call action""\n                                                   "" should have db_result"")\n                            db_result = responses[-2].pop(\'db_result\')\n                            new_utter[\'db_result\'] = db_result\n                        utterances.append(new_utter)\n                        cur_n_utter += 1\n            if cur_n_utter != cur_n_resp:\n                raise RuntimeError(""Datafile has wrong format."")\n            n += cur_n_utter\n            dialog_indices.append({\n                \'start\': n - cur_n_utter,\n                \'end\': n,\n            })\n\n        if with_indices:\n            return utterances, responses, dialog_indices\n        return utterances, responses\n'"
deeppavlov/dataset_readers/faq_reader.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, softwaredata\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Dict\n\nfrom pandas import read_csv\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.dataset_reader import DatasetReader\n\n\n@register(\'faq_reader\')\nclass FaqDatasetReader(DatasetReader):\n    """"""Reader for FAQ dataset""""""\n\n    def read(self, data_path: str = None, data_url: str = None, x_col_name: str = \'x\', y_col_name: str = \'y\') -> Dict:\n        """"""\n        Read FAQ dataset from specified csv file or remote url\n\n        Parameters:\n            data_path: path to csv file of FAQ\n            data_url: url to csv file of FAQ\n            x_col_name: name of Question column in csv file\n            y_col_name: name of Answer column in csv file\n\n        Returns:\n            A dictionary containing training, validation and test parts of the dataset obtainable via\n            ``train``, ``valid`` and ``test`` keys.\n        """"""\n\n        if data_url is not None:\n            data = read_csv(data_url)\n        elif data_path is not None:\n            data = read_csv(data_path)\n        else:\n            raise ValueError(""Please specify data_path or data_url parameter"")\n\n        x = data[x_col_name]\n        y = data[y_col_name]\n\n        train_xy_tuples = [(x[i].strip(), y[i].strip()) for i in range(len(x))]\n\n        dataset = dict()\n        dataset[""train""] = train_xy_tuples\n        dataset[""valid""] = []\n        dataset[""test""] = []\n\n        return dataset\n'"
deeppavlov/dataset_readers/file_paths_reader.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, softwaredata\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import Dict, Optional, Union\n\nfrom overrides import overrides\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.dataset_reader import DatasetReader\n\nlog = getLogger(__name__)\n\n\n@register(\'file_paths_reader\')\nclass FilePathsReader(DatasetReader):\n    """"""Find all file paths by a data path glob""""""\n\n    @overrides\n    def read(self, data_path: Union[str, Path], train: Optional[str] = None,\n             valid: Optional[str] = None, test: Optional[str] = None,\n             *args, **kwargs) -> Dict:\n        """"""\n        Find all file paths by a data path glob\n\n        Args:\n            data_path: directory with data\n            train: data path glob relative to data_path\n            valid: data path glob relative to data_path\n            test: data path glob relative to data_path\n\n        Returns:\n            A dictionary containing training, validation and test parts of the dataset obtainable via ``train``,\n            ``valid`` and ``test`` keys.\n        """"""\n\n        dataset = dict()\n        dataset[""train""] = self._get_files(data_path, train)\n        dataset[""valid""] = self._get_files(data_path, valid)\n        dataset[""test""] = self._get_files(data_path, test)\n        return dataset\n\n    def _get_files(self, data_path, tgt):\n        if tgt is not None:\n            paths = Path(data_path).resolve().glob(tgt)\n            files = [file for file in paths if Path(file).is_file()]\n            paths_info = Path(data_path, tgt).absolute().as_posix()\n            if not files:\n                raise Exception(f""Not find files. Data path \'{paths_info}\' does not exist or does not hold files!"")\n            else:\n                log.info(f""Found {len(files)} files located \'{paths_info}\'."")\n        else:\n            files = []\n        return files\n'"
deeppavlov/dataset_readers/imdb_reader.py,0,"b'# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom logging import getLogger\nfrom typing import List, Dict, Any, Optional, Tuple\nfrom pathlib import Path\n\nfrom overrides import overrides\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.dataset_reader import DatasetReader\nfrom deeppavlov.core.data.utils import download_decompress, mark_done, is_done\n\nlog = getLogger(__name__)\n\n\n@register(\'imdb_reader\')\nclass ImdbReader(DatasetReader):\n    """"""This class downloads and reads the IMDb sentiment classification dataset.\n\n    https://ai.stanford.edu/~amaas/data/sentiment/\n\n    Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts.\n    (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association\n    for Computational Linguistics (ACL 2011).\n    """"""\n\n    @overrides\n    def read(self, data_path: str, url: Optional[str] = None,\n             *args, **kwargs) -> Dict[str, List[Tuple[Any, Any]]]:\n        """"""\n        Args:\n            data_path: A path to a folder with dataset files.\n            url: A url to the archive with the dataset to download if the data folder is empty.\n        """"""\n        data_path = Path(data_path)\n\n        if url is None:\n            url = ""http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz""\n\n        if not is_done(data_path):\n            log.info(\'[downloading data from {} to {}]\'.format(url, data_path))\n            download_decompress(url, data_path)\n            mark_done(data_path)\n\n        alternative_data_path = data_path / ""aclImdb""\n        if alternative_data_path.exists():\n            data_path = alternative_data_path\n\n        data = {""train"": [],\n                ""test"": []}\n        for data_type in data.keys():\n            for label in [""neg"", ""pos""]:\n                labelpath = data_path / data_type / label\n                if not labelpath.exists():\n                    raise RuntimeError(f""Cannot load data: {labelpath} does not exist"")\n                for filename in labelpath.glob(""*.txt""):\n                    with filename.open(encoding=\'utf-8\') as f:\n                        text = f.read()\n                    data[data_type].append((text, [label]))\n\n            if not data[data_type]:\n                raise RuntimeError(f""Could not load the \'{data_type}\' dataset, ""\n                                   ""probably data dirs are empty"")\n\n        return data\n'"
deeppavlov/dataset_readers/insurance_reader.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.dataset_reader import DatasetReader\n\n\n@register(\'insurance_reader\')\nclass InsuranceReader(DatasetReader):\n    """"""The class to read the InsuranceQA V1 dataset from files.\n\n    Please, see https://github.com/shuzi/insuranceQA.\n    """"""\n\n    def read(self, data_path: str, **kwargs) -> Dict[str, List[Tuple[List[str], int]]]:\n        """"""Read the InsuranceQA V1 dataset from files.\n\n        Args:\n            data_path: A path to a folder with dataset files.\n        """"""\n\n        data_path = expand_path(data_path)\n        dataset = {\'train\': None, \'valid\': None, \'test\': None}\n        train_fname = data_path / \'insuranceQA-master/V1/question.train.token_idx.label\'\n        valid_fname = data_path / \'insuranceQA-master/V1/question.dev.label.token_idx.pool\'\n        test_fname = data_path / \'insuranceQA-master/V1/question.test1.label.token_idx.pool\'\n        int2tok_fname = data_path / \'insuranceQA-master/V1/vocabulary\'\n        response2ints_fname = data_path / \'insuranceQA-master/V1/answers.label.token_idx\'\n        self.int2tok_vocab = self._build_int2tok_vocab(int2tok_fname)\n        self.idxs2cont_vocab = self._build_context2toks_vocab(train_fname, valid_fname, test_fname)\n        self.response2str_vocab = self._build_response2str_vocab(response2ints_fname)\n        dataset[""valid""] = self._preprocess_data_valid_test(valid_fname)\n        dataset[""train""] = self._preprocess_data_train(train_fname)\n        dataset[""test""] = self._preprocess_data_valid_test(test_fname)\n\n        return dataset\n\n    def _build_context2toks_vocab(self, train_f: Path, val_f: Path, test_f: Path) -> Dict[int, str]:\n        contexts = []\n        with open(train_f, \'r\') as f:\n            data = f.readlines()\n        for eli in data:\n            eli = eli[:-1]\n            c, _ = eli.split(\'\\t\')\n            contexts.append(c)\n        with open(val_f, \'r\') as f:\n            data = f.readlines()\n        for eli in data:\n            eli = eli[:-1]\n            _, c, _ = eli.split(\'\\t\')\n            contexts.append(c)\n        with open(test_f, \'r\') as f:\n            data = f.readlines()\n        for eli in data:\n            eli = eli[:-1]\n            _, c, _ = eli.split(\'\\t\')\n            contexts.append(c)\n        idxs2cont_vocab = {el[1]: el[0] for el in enumerate(contexts)}\n        return idxs2cont_vocab\n\n    def _build_int2tok_vocab(self, fname: Path) -> Dict[int, str]:\n        with open(fname, \'r\') as f:\n            data = f.readlines()\n        int2tok_vocab = {int(el.split(\'\\t\')[0].split(\'_\')[1]): el.split(\'\\t\')[1][:-1] for el in data}\n        return int2tok_vocab\n\n    def _build_response2str_vocab(self, fname: Path) -> Dict[int, str]:\n        with open(fname, \'r\') as f:\n            data = f.readlines()\n            response2idxs_vocab = {int(el.split(\'\\t\')[0]) - 1:\n                                       (el.split(\'\\t\')[1][:-1]).split(\' \') for el in data}\n        response2str_vocab = {el[0]: \' \'.join([self.int2tok_vocab[int(x.split(\'_\')[1])]\n                                               for x in el[1]]) for el in response2idxs_vocab.items()}\n        return response2str_vocab\n\n    def _preprocess_data_train(self, fname: Path) -> List[Tuple[List[str], int]]:\n        positive_responses_pool = []\n        contexts = []\n        responses = []\n        labels = []\n        with open(fname, \'r\') as f:\n            data = f.readlines()\n        for k, eli in enumerate(data):\n            eli = eli[:-1]\n            q, pa = eli.split(\'\\t\')\n            q_tok = \' \'.join([self.int2tok_vocab[int(el.split(\'_\')[1])] for el in q.split()])\n            pa_list = [int(el) - 1 for el in pa.split(\' \')]\n            pa_list_tok = [self.response2str_vocab[el] for el in pa_list]\n            for elj in pa_list_tok:\n                contexts.append(q_tok)\n                responses.append(elj)\n                positive_responses_pool.append(pa_list_tok)\n                labels.append(k)\n        train_data = list(zip(contexts, responses))\n        train_data = list(zip(train_data, labels))\n        return train_data\n\n    def _preprocess_data_valid_test(self, fname: Path) -> List[Tuple[List[str], int]]:\n        pos_responses_pool = []\n        neg_responses_pool = []\n        contexts = []\n        pos_responses = []\n        with open(fname, \'r\') as f:\n            data = f.readlines()\n        for eli in data:\n            eli = eli[:-1]\n            pa, q, na = eli.split(\'\\t\')\n            q_tok = \' \'.join([self.int2tok_vocab[int(el.split(\'_\')[1])] for el in q.split()])\n            pa_list = [int(el) - 1 for el in pa.split(\' \')]\n            pa_list_tok = [self.response2str_vocab[el] for el in pa_list]\n            nas = [int(el) - 1 for el in na.split(\' \')]\n            nas_tok = [self.response2str_vocab[el] for el in nas]\n            for elj in pa_list_tok:\n                contexts.append(q_tok)\n                pos_responses.append(elj)\n                pos_responses_pool.append(pa_list_tok)\n                neg_responses_pool.append(nas_tok)\n        data = [[el[0]] + el[1] for el in zip(contexts, neg_responses_pool)]\n        data = [(el[0], len(el[1])) for el in zip(data, pos_responses_pool)]\n        return data\n'"
deeppavlov/dataset_readers/kbqa_reader.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom pathlib import Path\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.dataset_reader import DatasetReader\nfrom deeppavlov.core.data.utils import download_decompress\n\n\n@register(\'kbqa_reader\')\nclass KBQAReader(DatasetReader):\n    """"""Class to read test set of questions and answers for knowledge base question answering""""""\n\n    def read(self, data_path: str):\n        data_path = Path(data_path)\n        files = list(data_path.glob(\'*.txt\'))\n        test_set_filename = ""test_set_with_answers.txt""\n        if test_set_filename not in {file_path.name for file_path in files}:\n            url = \'http://files.deeppavlov.ai/kbqa/test_set_with_answers.zip\'\n            data_path.mkdir(exist_ok=True, parents=True)\n            download_decompress(url, data_path)\n        dataset = {}\n\n        dataset[""test""] = self.parse_ner_file(data_path / test_set_filename)\n        dataset[""train""] = []\n        dataset[""valid""] = []\n        return dataset\n\n    def parse_ner_file(self, file_name: Path):\n        samples = []\n        with file_name.open(encoding=\'utf8\') as f:\n            for line in f:\n                line_split = line.strip(\'\\n\').split(\'\\t\')\n                samples.append((line_split[0], tuple(line_split[1:])))\n\n        return samples\n'"
deeppavlov/dataset_readers/kvret_reader.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, softwaredata\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import Dict, List\n\nfrom overrides import overrides\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.dataset_reader import DatasetReader\nfrom deeppavlov.core.data.utils import download_decompress, mark_done\n\nlog = getLogger(__name__)\n\n\n@register(\'kvret_reader\')\nclass KvretDatasetReader(DatasetReader):\n    """"""\n    A New Multi-Turn, Multi-Domain, Task-Oriented Dialogue Dataset.\n\n    Stanford NLP released a corpus of 3,031 multi-turn dialogues in three distinct domains appropriate for an in-car assistant: calendar scheduling, weather information retrieval, and point-of-interest navigation. The dialogues are grounded through knowledge bases ensuring that they are versatile in their natural language without being completely free form.\n\n    For details see https://nlp.stanford.edu/blog/a-new-multi-turn-multi-domain-task-oriented-dialogue-dataset/.\n    """"""\n\n    url = \'http://files.deeppavlov.ai/datasets/kvret_public.tar.gz\'\n\n    @staticmethod\n    def _data_fname(datatype):\n        assert datatype in (\'train\', \'dev\', \'test\'), ""wrong datatype name""\n        return \'kvret_{}_public.json\'.format(datatype)\n\n    @classmethod\n    @overrides\n    def read(self, data_path: str, dialogs: bool = False) -> Dict[str, List]:\n        """"""\n        Downloads ``\'kvrest_public.tar.gz\'``, decompresses, saves files to ``data_path``.\n\n        Parameters:\n            data_path: path to save data\n            dialogs: flag indices whether to output list of turns or list of dialogs\n\n        Returns:\n            dictionary with ``\'train\'`` containing dialogs from ``\'kvret_train_public.json\'``, ``\'valid\'`` containing dialogs from ``\'kvret_valid_public.json\'``, ``\'test\'`` containing dialogs from ``\'kvret_test_public.json\'``. Each fields is a list of tuples ``(x_i, y_i)``.\n        """"""\n\n        required_files = (self._data_fname(dt) for dt in (\'train\', \'dev\', \'test\'))\n        if not all(Path(data_path, f).exists() for f in required_files):\n            log.info(\'[downloading dstc2 from {} to {}]\'.format(self.url, data_path))\n            download_decompress(self.url, data_path)\n            mark_done(data_path)\n\n        data = {\n            \'train\': self._read_from_file(\n                Path(data_path, self._data_fname(\'train\')), dialogs),\n            \'valid\': self._read_from_file(\n                Path(data_path, self._data_fname(\'dev\')), dialogs),\n            \'test\': self._read_from_file(\n                Path(data_path, self._data_fname(\'test\')), dialogs)\n        }\n        return data\n\n    @classmethod\n    def _read_from_file(cls, file_path, dialogs=False):\n        """"""Returns data from single file""""""\n        log.info(""[loading dialogs from {}]"".format(file_path))\n\n        utterances, responses, dialog_indices = \\\n            cls._get_turns(cls._iter_file(file_path), with_indices=True)\n\n        data = list(map(cls._format_turn, zip(utterances, responses)))\n\n        if dialogs:\n            return [data[idx[\'start\']:idx[\'end\']] for idx in dialog_indices]\n        return data\n\n    @staticmethod\n    def _format_turn(turn):\n        x = {\'text\': turn[0][\'utterance\'],\n             \'dialog_id\': turn[0][\'dialog_id\'],\n             \'kb_columns\': turn[0][\'kb_columns\'],\n             \'kb_items\': turn[0][\'kb_items\'],\n             \'requested\': turn[0].get(\'requested\', {}),\n             \'slots\': turn[0].get(\'slots\', {})}\n        if turn[0].get(\'episode_done\') is not None:\n            x[\'episode_done\'] = turn[0][\'episode_done\']\n        y = {\'text\': turn[1][\'utterance\'],\n             \'task\': turn[0][\'task\'],\n             \'requested\': turn[1].get(\'requested\', {}),\n             \'slots\': turn[1].get(\'slots\', {})}\n        return (x, y)\n\n    @staticmethod\n    def _check_dialog(dialog):\n        # TODO: manually fix bad dialogs\n        driver = True\n        for turn in dialog:\n            if turn[\'turn\'] not in (\'driver\', \'assistant\'):\n                raise RuntimeError(""Dataset wrong format: `turn` key value is""\n                                   "" either `driver` or `assistant`."")\n            if driver and turn[\'turn\'] != \'driver\':\n                log.debug(""Turn is expected to by driver\'s, but it\'s {}\'s"" \\\n                          .format(turn[\'turn\']))\n                return False\n            if not driver and turn[\'turn\'] != \'assistant\':\n                log.debug(""Turn is expected to be assistant\'s but it\'s {}\'s"" \\\n                          .format(turn[\'turn\']))\n                return False\n            driver = not driver\n        # if not driver:\n        #    log.debug(""Last turn is expected to be by assistant"")\n        #    return False\n        return True\n\n    @staticmethod\n    def _filter_duplicates(dialog):\n        last_turn, last_utter = None, None\n        for turn in dialog:\n            curr_turn, curr_utter = turn[\'turn\'], turn[\'data\'][\'utterance\']\n            if (curr_turn != last_turn) or (curr_utter != last_utter):\n                yield turn\n            last_turn, last_utter = curr_turn, curr_utter\n\n    @classmethod\n    def _iter_file(cls, file_path):\n        with open(file_path, \'rt\', encoding=\'utf8\') as f:\n            data = json.load(f)\n        for i, sample in enumerate(data):\n            dialog = list(cls._filter_duplicates(sample[\'dialogue\']))\n            if cls._check_dialog(dialog):\n                yield dialog, sample[\'scenario\']\n            else:\n                log.warning(""Skipping {}th dialogue with uuid={}: wrong format."" \\\n                            .format(i, sample[\'scenario\'][\'uuid\']))\n\n    @staticmethod\n    def _get_turns(data, with_indices=False):\n        utterances, responses, dialog_indices = [], [], []\n        for dialog, scenario in data:\n            for i, turn in enumerate(dialog):\n                replica = turn[\'data\']\n                if i == 0:\n                    replica[\'episode_done\'] = True\n                if turn[\'turn\'] == \'driver\':\n                    replica[\'task\'] = scenario[\'task\']\n                    replica[\'dialog_id\'] = scenario[\'uuid\']\n                    replica[\'kb_columns\'] = scenario[\'kb\'][\'column_names\']\n                    replica[\'kb_items\'] = scenario[\'kb\'][\'items\']\n                    utterances.append(replica)\n                else:\n                    responses.append(replica)\n\n            # if last replica was by driver\n            if len(responses) != len(utterances):\n                utterances[-1][\'end_dialogue\'] = False\n                responses.append({\'utterance\': \'\', \'end_dialogue\': True})\n\n            last_utter = responses[-1][\'utterance\']\n            if last_utter and not last_utter[-1].isspace():\n                last_utter += \' \'\n            responses[-1][\'utterance\'] = last_utter + \'END_OF_DIALOGUE\'\n\n            dialog_indices.append({\n                \'start\': len(utterances),\n                \'end\': len(utterances) + len(dialog),\n            })\n\n        if with_indices:\n            return utterances, responses, dialog_indices\n        return utterances, responses\n'"
deeppavlov/dataset_readers/line_reader.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, softwaredata\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Dict\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.dataset_reader import DatasetReader\n\n\n@register(\'line_reader\')\nclass LineReader(DatasetReader):\n    """"""Read txt file by lines""""""\n\n    def read(self, data_path: str = None, *args, **kwargs) -> Dict:\n        """"""Read lines from txt file\n\n        Args:\n            data_path: path to txt file\n\n        Returns:\n            A dictionary containing training, validation and test parts of the dataset obtainable via ``train``, ``valid`` and ``test`` keys.\n        """"""\n\n        with open(data_path) as f:\n            content = f.readlines()\n\n        dataset = dict()\n        dataset[""train""] = [(line,) for line in content]\n        dataset[""valid""] = []\n        dataset[""test""] = []\n\n        return dataset\n'"
deeppavlov/dataset_readers/morphotagging_dataset_reader.py,0,"b'# Copyright 2018 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import Dict, List, Union, Tuple, Optional\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.dataset_reader import DatasetReader\nfrom deeppavlov.core.data.utils import download_decompress, mark_done\n\nWORD_COLUMN, POS_COLUMN, TAG_COLUMN = 1, 3, 5\nHEAD_COLUMN, DEP_COLUMN = 6, 7\n\nlog = getLogger(__name__)\n\n\ndef get_language(filepath: str) -> str:\n    """"""Extracts language from typical UD filename\n    """"""\n    return filepath.split(""-"")[0]\n\n\ndef read_infile(infile: Union[Path, str], *, from_words=False,\n                word_column: int = WORD_COLUMN, pos_column: int = POS_COLUMN,\n                tag_column: int = TAG_COLUMN, head_column: int = HEAD_COLUMN,\n                dep_column: int = DEP_COLUMN, max_sents: int = -1,\n                read_only_words: bool = False, read_syntax: bool = False) -> List[Tuple[List, Union[List, None]]]:\n    """"""Reads input file in CONLL-U format\n\n    Args:\n        infile: a path to a file\n        word_column: column containing words (default=1)\n        pos_column: column containing part-of-speech labels (default=3)\n        tag_column: column containing fine-grained tags (default=5)\n        head_column: column containing syntactic head position (default=6)\n        dep_column: column containing syntactic dependency label (default=7)\n        max_sents: maximal number of sentences to read\n        read_only_words: whether to read only words\n        read_syntax: whether to return ``heads`` and ``deps`` alongside ``tags``. Ignored if read_only_words is ``True``\n\n    Returns:\n        a list of sentences. Each item contains a word sequence and an output sequence.\n        The output sentence is ``None``, if ``read_only_words`` is ``True``,\n        a single list of word tags if ``read_syntax`` is False,\n        and a list of the form [``tags``, ``heads``, ``deps``] in case ``read_syntax`` is ``True``.\n\n    """"""\n    answer, curr_word_sent, curr_tag_sent = [], [], []\n    curr_head_sent, curr_dep_sent = [], []\n    # read_syntax = read_syntax and read_only_words\n    if from_words:\n        word_column, read_only_words = 0, True\n    if infile is not sys.stdin:\n        fin = open(infile, ""r"", encoding=""utf8"")\n    else:\n        fin = sys.stdin\n    for line in fin:\n        line = line.strip()\n        if line.startswith(""#""):\n            continue\n        if line == """":\n            if len(curr_word_sent) > 0:\n                if read_only_words:\n                    curr_tag_sent = None\n                elif read_syntax:\n                    curr_tag_sent = [curr_tag_sent, curr_head_sent, curr_dep_sent]\n                answer.append((curr_word_sent, curr_tag_sent))\n            curr_tag_sent, curr_word_sent = [], []\n            curr_head_sent, curr_dep_sent = [], []\n            if len(answer) == max_sents:\n                break\n            continue\n        splitted = line.split(""\\t"")\n        index = splitted[0]\n        if not from_words and not index.isdigit():\n            continue\n        curr_word_sent.append(splitted[word_column])\n        if not read_only_words:\n            pos, tag = splitted[pos_column], splitted[tag_column]\n            tag = pos if tag == ""_"" else ""{},{}"".format(pos, tag)\n            curr_tag_sent.append(tag)\n            if read_syntax:\n                curr_head_sent.append(int(splitted[head_column]))\n                curr_dep_sent.append(splitted[dep_column])\n    if len(curr_word_sent) > 0:\n        if read_only_words:\n            curr_tag_sent = None\n        elif read_syntax:\n            curr_tag_sent = [curr_tag_sent, curr_head_sent, curr_dep_sent]\n        answer.append((curr_word_sent, curr_tag_sent))\n    if infile is not sys.stdin:\n        fin.close()\n    return answer\n\n\n@register(\'morphotagger_dataset_reader\')\nclass MorphotaggerDatasetReader(DatasetReader):\n    """"""Class to read training datasets in UD format""""""\n\n    URL = \'http://files.deeppavlov.ai/datasets/UD2.0_source/\'\n\n    def read(self, data_path: Union[List, str],\n             language: Optional[str] = None,\n             data_types: Optional[List[str]] = None,\n             **kwargs) -> Dict[str, List]:\n        """"""Reads UD dataset from data_path.\n\n        Args:\n            data_path: can be either\n                1. a directory containing files. The file for data_type \'mode\'\n                is then data_path / {language}-ud-{mode}.conllu\n                2. a list of files, containing the same number of items as data_types\n            language: a language to detect filename when it is not given\n            data_types: which dataset parts among \'train\', \'dev\', \'test\' are returned\n\n        Returns:\n            a dictionary containing dataset fragments (see ``read_infile``) for given data types\n        """"""\n        if data_types is None:\n            data_types = [""train"", ""dev""]\n        elif isinstance(data_types, str):\n            data_types = list(data_types)\n        for data_type in data_types:\n            if data_type not in [""train"", ""dev"", ""test""]:\n                raise ValueError(""Unknown data_type: {}, only train, dev and test ""\n                                 ""datatypes are allowed"".format(data_type))\n        if isinstance(data_path, str):\n            data_path = Path(data_path)\n        if isinstance(data_path, Path):\n            if data_path.exists():\n                is_file = data_path.is_file()\n            else:\n                is_file = (len(data_types) == 1)\n            if is_file:\n                # path to a single file\n                data_path, reserve_data_path = [data_path], None\n            else:\n                # path to data directory\n                if language is None:\n                    raise ValueError(""You must implicitly provide language ""\n                                     ""when providing data directory as source"")\n                reserve_data_path = data_path\n                data_path = [data_path / ""{}-ud-{}.conllu"".format(language, mode)\n                             for mode in data_types]\n                reserve_data_path = [\n                    reserve_data_path / language / ""{}-ud-{}.conllu"".format(language, mode)\n                    for mode in data_types]\n        else:\n            data_path = [Path(data_path) for data_path in data_path]\n            reserve_data_path = None\n        if len(data_path) != len(data_types):\n            raise ValueError(""The number of input files in data_path and data types ""\n                             ""in data_types must be equal"")\n        has_missing_files = any(not filepath.exists() for filepath in data_path)\n        if has_missing_files and reserve_data_path is not None:\n            has_missing_files = any(not filepath.exists() for filepath in reserve_data_path)\n            if not has_missing_files:\n                data_path = reserve_data_path\n        if has_missing_files:\n            # Files are downloaded from the Web repository\n            dir_path = data_path[0].parent\n            language = language or get_language(data_path[0].parts[-1])\n            url = self.URL + ""{}.tar.gz"".format(language)\n            log.info(\'[downloading data from {} to {}]\'.format(url, dir_path))\n            dir_path.mkdir(exist_ok=True, parents=True)\n            download_decompress(url, dir_path)\n            mark_done(dir_path)\n        data = {}\n        for mode, filepath in zip(data_types, data_path):\n            if mode == ""dev"":\n                mode = ""valid""\n#             if mode == ""test"":\n#                 kwargs[""read_only_words""] = True\n            data[mode] = read_infile(filepath, **kwargs)\n        return data\n'"
deeppavlov/dataset_readers/odqa_reader.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport logging\nimport sqlite3\nimport unicodedata\nfrom multiprocessing import Pool\nfrom pathlib import Path\nfrom typing import Union, List, Tuple, Generator, Any, Optional\n\nfrom tqdm import tqdm\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.errors import ConfigError\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.dataset_reader import DatasetReader\nfrom deeppavlov.core.data.utils import download\n\nlogger = logging.getLogger(__name__)\n\n\n@register(\'odqa_reader\')\nclass ODQADataReader(DatasetReader):\n    """"""Build a SQLite database from folder with txt files, json files or\n    `Wiki Extractor <https://github.com/attardi/wikiextractor>`_ files.\n\n    """"""\n\n    def read(self, data_path: Union[Path, str], db_url: Optional[str] = None, *args,\n             **kwargs) -> None:\n        """"""Build a SQLite database from provided files, download SQLite database from a provided URL,\n         or do nothing.\n\n        Args:\n            data_path: a directory/file with texts to create a database from\n            db_url: path to a database url\n            kwargs:\n                save_path: a path where a database should be saved to, or path to a ready database\n                dataset_format: initial data format; should be selected from [\'txt\', \'wiki\', \'json\']\n\n        Returns:\n            None\n\n        """"""\n        logger.info(\'Reading files...\')\n        try:\n            save_path = expand_path(kwargs[\'save_path\'])\n        except KeyError:\n            raise ConfigError(\n                f\'\\""save_path\\"" attribute should be set for {self.__class__.__name__}\\\n                 in the JSON config.\')\n        if save_path.exists() and save_path.with_suffix(f\'{save_path.suffix}.done\').exists():\n            return\n        try:\n            dataset_format = kwargs[\'dataset_format\']\n        except KeyError:\n            raise ConfigError(\n                f\'\\""dataset_format\\"" attribute should be set for {self.__class__.__name__}\\\n                 in the JSON config.\')\n\n        save_path.parent.mkdir(parents=True, exist_ok=True)\n\n        if db_url:\n            download_dir = save_path.parent\n            logger.info(f\'Downloading database from {db_url} to {download_dir}\')\n            download(download_dir, db_url, force_download=False)\n            return\n\n        self._build_db(save_path, dataset_format, expand_path(data_path))\n\n    def iter_files(self, path: Union[Path, str]) -> Generator[Path, Any, Any]:\n        """"""Iterate over folder with files or a single file and generate file paths.\n\n        Args:\n            path: path to a folder or a file\n\n        Raises:\n            RuntimeError if the provided `path` doesn\'t exist\n\n        Yields:\n            file paths one by one\n\n        Returns:\n            None\n\n        """"""\n        path = Path(path)\n        if path.is_file():\n            yield path\n        elif path.is_dir():\n            for item in path.iterdir():\n                yield from self.iter_files(item)\n        else:\n            raise RuntimeError(""Path doesn\'t exist: {}"".format(path))\n\n    def _build_db(self, save_path: Union[Path, str], dataset_format: str,\n                  data_path: Union[Path, str],\n                  num_workers: int = 8) -> None:\n        """"""Build a SQLite database in parallel and save it to a pointed path.\n\n        Args:\n            save_path: a path where the ready database should be saved\n            dataset_format: a data format, should be selected from [\'txt\', \'json\', \'wiki\']\n            data_path: path to a folder/file from which to build a database\n            num_workers: a number of workers for parallel database building\n\n        Raises:\n            sqlite3.OperationalError if `save_path` doesn\'t exist.\n            RuntimeError if dataset_format is not in [\'txt\', \'json\', \'wiki\']\n\n        Returns:\n            None\n\n        """"""\n        done_path = save_path.with_suffix(f\'{save_path.suffix}.done\')\n\n        if Path(save_path).exists():\n            Path(save_path).unlink()\n        if done_path.exists():\n            done_path.unlink()\n\n        logger.info(\'Building the database...\')\n\n        try:\n            conn = sqlite3.connect(str(save_path))\n        except sqlite3.OperationalError as e:\n            e.args = e.args + (""Check that DB path exists."",)\n            raise e\n        c = conn.cursor()\n        sql_table = ""CREATE TABLE documents (id PRIMARY KEY, text);""\n        c.execute(sql_table)\n\n        files = [f for f in self.iter_files(data_path)]\n        workers = Pool(num_workers)\n\n        if dataset_format == \'txt\':\n            fn = self._get_file_contents\n        elif dataset_format == \'json\':\n            fn = self._get_json_contents\n        elif dataset_format == \'wiki\':\n            fn = self._get_wiki_contents\n        else:\n            raise RuntimeError(\'Unknown dataset format.\')\n\n        with tqdm(total=len(files)) as pbar:\n            for data in tqdm(workers.imap_unordered(fn, files)):\n                try:\n                    c.executemany(""INSERT INTO documents VALUES (?,?)"", data)\n                    pbar.update()\n                except sqlite3.IntegrityError as e:\n                    logger.warning(e)\n\n        conn.commit()\n        conn.close()\n        done_path.touch()\n\n    @staticmethod\n    def _get_file_contents(fpath: Union[Path, str]) -> List[Tuple[str, str]]:\n        """"""Extract file contents from \'.txt\' file.\n\n        Args:\n            fpath: path to a \'.txt\' file.\n\n        Returns:\n             a list with tuple of normalized file name and file contents\n\n        """"""\n        with open(fpath, encoding=\'utf-8\') as fin:\n            text = fin.read()\n            normalized_text = unicodedata.normalize(\'NFD\', text)\n            return [(fpath.name, normalized_text)]\n\n    @staticmethod\n    def _get_json_contents(fpath: Union[Path, str]) -> List[Tuple[str, str]]:\n        """"""Extract file contents from \'.json\' file. JSON files should be formatted as list with dicts\n        which contain \'title\' and \'doc\' keywords.\n\n        Args:\n            fpath: path to a \'.json\' file.\n\n        Returns:\n            a list with tuples of normalized file name and file contents\n\n        """"""\n        docs = []\n        with open(fpath, encoding=\'utf-8\') as fin:\n            for line in fin:\n                data = json.loads(line)\n                for doc in data:\n                    if not doc:\n                        continue\n                    text = doc[\'text\']\n                    normalized_text = unicodedata.normalize(\'NFD\', text)\n                    docs.append((doc[\'title\'], normalized_text))\n        return docs\n\n    @staticmethod\n    def _get_wiki_contents(fpath: Union[Path, str]) -> List[Tuple[str, str]]:\n        """"""Extract file contents from wiki extractor formatted files.\n\n        Args:\n            fpath: path to a \'.txt\' file in wiki extractor format\n\n        Returns:\n            a list with tuples of normalized file name and file contents\n\n        """"""\n        docs = []\n        with open(fpath, encoding=\'utf-8\') as fin:\n            for line in fin:\n                doc = json.loads(line)\n                if not doc:\n                    continue\n                text = doc[\'text\']\n                normalized_text = unicodedata.normalize(\'NFD\', text)\n                docs.append((doc[\'title\'], normalized_text))\n        return docs\n'"
deeppavlov/dataset_readers/ontonotes_reader.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pickle\nfrom logging import getLogger\nfrom pathlib import Path\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.dataset_reader import DatasetReader\nfrom deeppavlov.core.data.utils import download\n\nlog = getLogger(__name__)\n\n\n@register(\'ontonotes_reader\')\nclass OntonotesReader(DatasetReader):\n    """"""Class to read training datasets in OntoNotes format""""""\n    URL = \'http://files.deeppavlov.ai/datasets/ontonotes_senna.pckl\'\n\n    def __init__(self):\n        log.warning(\'ontonotes_reader is deprecated and will be removed in future versions.\'\n                    \' Please, use conll2003_reader with `""dataset_name"": ""ontonotes""` instead\')\n\n    def read(self, data_path, file_name: str = \'ontonotes_senna.pckl\', provide_senna_pos=False,\n             provide_senna_ner=False):\n        path = Path(data_path).resolve() / file_name\n        if not path.exists():\n            download(str(path), self.URL)\n        with open(path, \'rb\') as f:\n            dataset = pickle.load(f)\n\n        dataset_filtered = {}\n        for key, data in dataset.items():\n            dataset_filtered[key] = []\n            for (toks, pos, ner), tags in data:\n                if not provide_senna_pos and not provide_senna_ner:\n                    dataset_filtered[key].append((toks, tags))\n                else:\n                    x = [toks]\n                    if provide_senna_pos:\n                        x.append(pos)\n                    if provide_senna_ner:\n                        x.append(ner)\n                    dataset_filtered[key].append((x, tags))\n\n        return dataset_filtered\n'"
deeppavlov/dataset_readers/paraphraser_pretrain_reader.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nfrom typing import Dict, List, Tuple\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.dataset_reader import DatasetReader\n\n\n@register(""paraphraser_pretrain_reader"")\nclass ParaphraserPretrainReader(DatasetReader):\n    """"""The class to read the pretraining dataset for the paraphrase identification task from files.""""""\n\n    def read(self,\n             data_path: str,\n             seed: int = None, *args, **kwargs) -> Dict[str, List[Tuple[List[str], int]]]:\n        """"""Read the pretraining dataset for the paraphrase identification task from files.\n\n        Args:\n            data_path: A path to a folder with dataset files.\n            seed: Random seed.\n        """"""\n\n        data_path = expand_path(data_path)\n        train_fname = data_path / \'paraphraser_pretrain_train.json\'\n        test_fname = data_path / \'paraphraser_pretrain_val.json\'\n        train_data = self.build_data(train_fname)\n        test_data = self.build_data(test_fname)\n        dataset = {""train"": train_data, ""valid"": test_data, ""test"": test_data}\n        return dataset\n\n    def int_class(self, str_y):\n        if str_y == \'-1\':\n            return 0\n        else:\n            return 1\n\n    def build_data(self, name):\n        with open(name) as f:\n            data = json.load(f)\n        return [([doc[\'text_1\'], doc[\'text_2\']], self.int_class(doc[\'class\'])) for doc in data]\n'"
deeppavlov/dataset_readers/paraphraser_reader.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport xml.etree.ElementTree as ET\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.dataset_reader import DatasetReader\n\n\n@register(\'paraphraser_reader\')\nclass ParaphraserReader(DatasetReader):\n    """"""The class to read the paraphraser.ru dataset from files.\n\n    Please, see https://paraphraser.ru.\n    """"""\n\n    def read(self,\n             data_path: str,\n             do_lower_case: bool = True,\n             *args, **kwargs) -> Dict[str, List[Tuple[Tuple[str, str], int]]]:\n        """"""Read the paraphraser.ru dataset from files.\n\n        Args:\n            data_path: A path to a folder with dataset files.\n            do_lower_case: Do you want to lowercase all texts\n        """"""\n\n        data_path = expand_path(data_path)\n        train_fname = data_path / \'paraphrases.xml\'\n        test_fname = data_path / \'paraphrases_gold.xml\'\n\n        train_data = self._build_data(train_fname, do_lower_case)\n        test_data = self._build_data(test_fname, do_lower_case)\n        return {""train"": train_data, ""valid"": [], ""test"": test_data}\n\n    @staticmethod\n    def _build_data(data_path: Path, do_lower_case: bool) -> List[Tuple[Tuple[str, str], int]]:\n        root = ET.fromstring(data_path.read_text(encoding=\'utf8\'))\n        data = {}\n        for paraphrase in root.findall(\'corpus/paraphrase\'):\n            key = (paraphrase.find(\'value[@name=""text_1""]\').text,\n                   paraphrase.find(\'value[@name=""text_2""]\').text)\n            if do_lower_case:\n                key = tuple([t.lower() for t in key])\n\n            data[key] = 1 if int(paraphrase.find(\'value[@name=""class""]\').text) >= 0 else 0\n        return list(data.items())\n'"
deeppavlov/dataset_readers/quora_question_pairs_reader.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport csv\nfrom typing import Dict, List, Tuple\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.dataset_reader import DatasetReader\n\n\n@register(\'qqp_reader\')\nclass QuoraQuestionPairsReader(DatasetReader):\n    """"""The class to read the Quora Question Pairs dataset from files.\n\n    Please, see https://www.kaggle.com/c/quora-question-pairs/data.\n\n    Args:\n        data_path: A path to a folder with dataset files.\n        seed: Random seed.\n    """"""\n\n    def read(self, data_path: str,\n             seed: int = None, *args, **kwargs) -> Dict[str, List[Tuple[Tuple[str, str], int]]]:\n        data_path = expand_path(data_path)\n        fname = data_path / \'train.csv\'\n        contexts = []\n        responses = []\n        labels = []\n        with open(fname, \'r\') as f:\n            reader = csv.reader(f)\n            next(reader)\n            for el in reader:\n                contexts.append(el[-3].replace(\'\\n\', \'\').lower())\n                responses.append(el[-2].replace(\'\\n\', \'\').lower())\n                labels.append(int(el[-1]))\n        data = list(zip(contexts, responses))\n        data = list(zip(data, labels))\n        data = {""train"": data,\n                ""valid"": [],\n                ""test"": []}\n        return data\n'"
deeppavlov/dataset_readers/siamese_reader.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport csv\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.dataset_reader import DatasetReader\n\n\n@register(\'siamese_reader\')\nclass SiameseReader(DatasetReader):\n    """"""The class to read dataset for ranking or paraphrase identification with Siamese networks.""""""\n\n    def read(self, data_path: str, **kwargs) -> Dict[str, List[Tuple[List[str], int]]]:\n        """"""Read the dataset for ranking or paraphrase identification with Siamese networks.\n\n        Args:\n            data_path: A path to a folder with dataset files.\n        """"""\n\n        dataset = {\'train\': None, \'valid\': None, \'test\': None}\n        data_path = expand_path(data_path)\n        train_fname = data_path / \'train.csv\'\n        valid_fname = data_path / \'valid.csv\'\n        test_fname = data_path / \'test.csv\'\n        dataset[""train""] = self._preprocess_data_train(train_fname)\n        dataset[""valid""] = self._preprocess_data_valid_test(valid_fname)\n        dataset[""test""] = self._preprocess_data_valid_test(test_fname)\n        return dataset\n\n    def _preprocess_data_train(self, fname: Path) -> List[Tuple[List[str], int]]:\n        data = []\n        with open(fname, \'r\') as f:\n            reader = csv.reader(f, delimiter=\'\\t\')\n            for el in reader:\n                data.append((el[:2], int(el[2])))\n        return data\n\n    def _preprocess_data_valid_test(self, fname: Path) -> List[Tuple[List[str], int]]:\n        data = []\n        with open(fname, \'r\') as f:\n            reader = csv.reader(f, delimiter=\'\\t\')\n            for el in reader:\n                data.append((el, 1))\n        return data\n'"
deeppavlov/dataset_readers/snips_reader.py,0,"b'# Copyright 2019 Alexey Romanov\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import List, Dict, Any, Optional\n\nfrom overrides import overrides\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.dataset_reader import DatasetReader\nfrom deeppavlov.core.data.utils import download_decompress, mark_done, is_done\n\nlog = getLogger(__name__)\n\n\n@register(\'snips_reader\')\nclass SnipsReader(DatasetReader):\n    """"""The class to download and read Snips NLU Benchmark dataset (custom intents section).\n\n    See https://github.com/snipsco/nlu-benchmark.\n    """"""\n\n    # noinspection PyAttributeOutsideInit\n    @overrides\n    def read(self, data_path: str, queries_per_intent: Optional[int] = None, test_validate_split: float = 0.5,\n             *args, **kwargs) -> \\\n            Dict[str, List[Dict[str, Any]]]:\n        """"""\n        Each query in the output has the following form:\n            { \'intent\': intent_name,\n              \'data\': [ { \'text\': text, (\'entity\': slot_name)? } ]\n            }\n\n        Args:\n            data_path: A path to a folder with dataset files.\n            queries_per_intent: Number of queries to load for each intent. None to load all.\n                If the requested number is greater than available in file, all queries are returned.\n            test_validate_split: Proportion of `_validate` files to be used as test dataset (since Snips\n                is split into training and validation sets without a separate test set).\n        """"""\n        data_path = Path(data_path)\n        intents = [\'AddToPlaylist\', \'BookRestaurant\', \'GetWeather\', \'PlayMusic\',\n                   \'RateBook\', \'SearchCreativeWork\', \'SearchScreeningEvent\']\n\n        if not is_done(data_path):\n            url = \'http://files.deeppavlov.ai/datasets/snips.tar.gz\'\n            log.info(\'[downloading data from {} to {}]\'.format(url, data_path))\n            download_decompress(url, data_path)\n            mark_done(data_path)\n\n        use_full_file = queries_per_intent is None or queries_per_intent > 70\n        training_data = []\n        validation_data = []\n        test_data = []\n\n        for intent in intents:\n            intent_path = data_path / intent\n            train_file_name = f""train_{intent}{\'_full\' if use_full_file else \'\'}.json""\n            validate_file_name = f""validate_{intent}.json""\n\n            train_queries = self._load_file(intent_path / train_file_name, intent, queries_per_intent)\n            validate_queries = self._load_file(intent_path / validate_file_name, intent, queries_per_intent)\n            num_test_queries = round(len(validate_queries) * test_validate_split)\n\n            training_data.extend(train_queries)\n            validation_data.extend(validate_queries[num_test_queries:])\n            test_data.extend(validate_queries[:num_test_queries])\n\n        return {\'train\': training_data, \'valid\': validation_data, \'test\': test_data}\n\n    @staticmethod\n    def _load_file(path: Path, intent: str, num_queries: Optional[int]):\n        with path.open(encoding=\'latin_1\') as f:\n            data = json.load(f)\n\n        # restrict number of queries\n        queries = data[intent][:num_queries]\n        for query in queries:\n            query[\'intent\'] = intent\n        return queries\n'"
deeppavlov/dataset_readers/sq_reader.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pickle\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.dataset_reader import DatasetReader\n\n\n@register(\'sq_reader\')\nclass OntonotesReader(DatasetReader):\n    """"""Class to read training datasets in OntoNotes format""""""\n\n    def read(self, data_path: str):\n        with open(data_path, \'rb\') as f:\n            dataset = pickle.load(f)\n\n        return dataset\n'"
deeppavlov/dataset_readers/squad_dataset_reader.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport json\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.dataset_reader import DatasetReader\nfrom deeppavlov.core.data.utils import download_decompress\n\n\n@register(\'squad_dataset_reader\')\nclass SquadDatasetReader(DatasetReader):\n    """"""\n    Downloads dataset files and prepares train/valid split.\n\n    SQuAD:\n    Stanford Question Answering Dataset\n    https://rajpurkar.github.io/SQuAD-explorer/\n\n    SberSQuAD:\n    Dataset from SDSJ Task B\n    https://www.sdsj.ru/ru/contest.html\n\n    MultiSQuAD:\n    SQuAD dataset with additional contexts retrieved (by tfidf) from original Wikipedia article.\n\n    MultiSQuADRetr:\n    SQuAD dataset with additional contexts retrieved by tfidf document ranker from full Wikipedia.\n\n    """"""\n\n    url_squad = \'http://files.deeppavlov.ai/datasets/squad-v1.1.tar.gz\'\n    url_sber_squad = \'http://files.deeppavlov.ai/datasets/sber_squad-v1.1.tar.gz\'\n    url_multi_squad = \'http://files.deeppavlov.ai/datasets/multiparagraph_squad.tar.gz\'\n\n    def read(self, dir_path: str, dataset: Optional[str] = \'SQuAD\', url: Optional[str] = None, *args, **kwargs) \\\n            -> Dict[str, Dict[str, Any]]:\n        """"""\n\n        Args:\n            dir_path: path to save data\n            dataset: default dataset names: ``\'SQuAD\'``, ``\'SberSQuAD\'`` or ``\'MultiSQuAD\'``\n            url: link to archive with dataset, use url argument if non-default dataset is used\n\n        Returns:\n            dataset split on train/valid\n\n        Raises:\n            RuntimeError: if `dataset` is not one of these: ``\'SQuAD\'``, ``\'SberSQuAD\'``, ``\'MultiSQuAD\'``.\n        """"""\n        if url is not None:\n            self.url = url\n        elif dataset == \'SQuAD\':\n            self.url = self.url_squad\n        elif dataset == \'SberSQuAD\':\n            self.url = self.url_sber_squad\n        elif dataset == \'MultiSQuAD\':\n            self.url = self.url_multi_squad\n        else:\n            raise RuntimeError(\'Dataset {} is unknown\'.format(dataset))\n\n        dir_path = Path(dir_path)\n        required_files = [\'{}-v1.1.json\'.format(dt) for dt in [\'train\', \'dev\']]\n        if not dir_path.exists():\n            dir_path.mkdir()\n\n        if not all((dir_path / f).exists() for f in required_files):\n            download_decompress(self.url, dir_path)\n\n        dataset = {}\n        for f in required_files:\n            with dir_path.joinpath(f).open(\'r\', encoding=\'utf8\') as fp:\n                data = json.load(fp)\n            if f == \'dev-v1.1.json\':\n                dataset[\'valid\'] = data\n            else:\n                dataset[\'train\'] = data\n\n        return dataset\n\n\n@register(\'multi_squad_dataset_reader\')\nclass MultiSquadDatasetReader(DatasetReader):\n    """"""\n    Downloads dataset files and prepares train/valid split.\n\n    MultiSQuADRetr:\n    Multiparagraph SQuAD dataset with additional contexts retrieved by tfidf document ranker from full En Wikipedia.\n\n    MultiSQuADRuRetr:\n    Multiparagraph SberSQuAD dataset with additional contexts retrieved by tfidf document ranker from  Ru Wikipedia.\n\n    """"""\n\n    url_multi_squad_retr = \'http://files.deeppavlov.ai/datasets/multi_squad_retr_enwiki20161221.tar.gz\'\n    url_multi_squad_ru_retr = \'http://files.deeppavlov.ai/datasets/multi_squad_ru_retr.tar.gz\'\n\n    def read(self, dir_path: str, dataset: Optional[str] = \'MultiSQuADRetr\', url: Optional[str] = None, *args,\n             **kwargs) -> Dict[str, Dict[str, Any]]:\n        """"""\n\n        Args:\n            dir_path: path to save data\n            dataset: default dataset names: ``\'MultiSQuADRetr\'``, ``\'MultiSQuADRuRetr\'``\n            url: link to archive with dataset, use url argument if non-default dataset is used\n\n        Returns:\n            dataset split on train/valid\n\n        Raises:\n            RuntimeError: if `dataset` is not one of these: ``\'MultiSQuADRetr\'``, ``\'MultiSQuADRuRetr\'``.\n        """"""\n        if url is not None:\n            self.url = url\n        elif dataset == \'MultiSQuADRetr\':\n            self.url = self.url_multi_squad_retr\n        elif dataset == \'MultiSQuADRuRetr\':\n            self.url = self.url_multi_squad_ru_retr\n        else:\n            raise RuntimeError(\'Dataset {} is unknown\'.format(dataset))\n\n        dir_path = Path(dir_path)\n        required_files = [\'{}.jsonl\'.format(dt) for dt in [\'train\', \'dev\']]\n        if not dir_path.exists():\n            dir_path.mkdir(parents=True)\n\n        if not all((dir_path / f).exists() for f in required_files):\n            download_decompress(self.url, dir_path)\n\n        dataset = {}\n        for f in required_files:\n            if \'dev\' in f:\n                dataset[\'valid\'] = dir_path.joinpath(f)\n            else:\n                dataset[\'train\'] = dir_path.joinpath(f)\n\n        return dataset\n'"
deeppavlov/dataset_readers/typos_reader.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport csv\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport requests\nfrom lxml import html\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.dataset_reader import DatasetReader\nfrom deeppavlov.core.data.utils import is_done, download, mark_done\n\nlog = getLogger(__name__)\n\n\n@register(\'typos_custom_reader\')\nclass TyposCustom(DatasetReader):\n    """"""Base class for reading spelling corrections dataset files\n\n    """"""\n\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def build(data_path: str) -> Path:\n        """"""Base method that interprets ``data_path`` argument.\n\n        Args:\n            data_path: path to the tsv-file containing erroneous and corrected words\n\n        Returns:\n            the same path as a :class:`~pathlib.Path` object\n        """"""\n        return Path(data_path)\n\n    @classmethod\n    def read(cls, data_path: str, *args, **kwargs) -> Dict[str, List[Tuple[str, str]]]:\n        """"""Read train data for spelling corrections algorithms\n\n        Args:\n            data_path: path that needs to be interpreted with :meth:`~deeppavlov.dataset_readers.typos_reader.TyposCustom.build`\n\n        Returns:\n            train data to pass to a :class:`~deeppavlov.dataset_iterators.typos_iterator.TyposDatasetIterator`\n        """"""\n        fname = cls.build(data_path)\n        with fname.open(newline=\'\', encoding=\'utf8\') as tsvfile:\n            reader = csv.reader(tsvfile, delimiter=\'\\t\')\n            next(reader)\n            res = [(mistake, correct) for mistake, correct in reader]\n        return {\'train\': res}\n\n\n@register(\'typos_wikipedia_reader\')\nclass TyposWikipedia(TyposCustom):\n    """"""Implementation of :class:`~deeppavlov.dataset_readers.typos_reader.TyposCustom` that works with\n     English Wikipedia\'s list of common misspellings\n\n    """"""\n\n    @staticmethod\n    def build(data_path: str) -> Path:\n        """"""Download and parse common misspellings list from `Wikipedia <https://en.wikipedia.org/wiki/Wikipedia:Lists_of_common_misspellings/For_machines>`_\n\n        Args:\n            data_path: target directory to download the data to\n\n        Returns:\n            path to the resulting tsv-file\n        """"""\n        data_path = Path(data_path) / \'typos_wiki\'\n\n        fname = data_path / \'misspelings.tsv\'\n\n        if not is_done(data_path):\n            url = \'https://en.wikipedia.org/wiki/Wikipedia:Lists_of_common_misspellings/For_machines\'\n\n            page = requests.get(url)\n            tree = html.fromstring(page.content)\n            raw = tree.xpath(\'//pre/text()\')[0].splitlines()\n            data = []\n            for pair in raw:\n                typo, corrects = pair.strip().split(\'->\')\n                for correct in corrects.split(\',\'):\n                    data.append([typo.strip(), correct.strip()])\n\n            fname.parent.mkdir(parents=True, exist_ok=True)\n            with fname.open(\'w\', newline=\'\', encoding=\'utf8\') as tsvfile:\n                writer = csv.writer(tsvfile, delimiter=\'\\t\')\n                for line in data:\n                    writer.writerow(line)\n\n            mark_done(data_path)\n\n            log.info(\'Built\')\n        return fname\n\n\n@register(\'typos_kartaslov_reader\')\nclass TyposKartaslov(DatasetReader):\n    """"""Implementation of :class:`~deeppavlov.dataset_readers.typos_reader.TyposCustom` that works with\n     a Russian misspellings dataset from `kartaslov <https://github.com/dkulagin/kartaslov>`_\n\n    """"""\n\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def build(data_path: str) -> Path:\n        """"""Download misspellings list from `github <https://raw.githubusercontent.com/dkulagin/kartaslov/master/dataset/orfo_and_typos/orfo_and_typos.L1_5.csv>`_\n\n        Args:\n            data_path: target directory to download the data to\n\n        Returns:\n            path to the resulting csv-file\n        """"""\n        data_path = Path(data_path) / \'kartaslov\'\n\n        fname = data_path / \'orfo_and_typos.L1_5.csv\'\n\n        if not is_done(data_path):\n            url = \'https://raw.githubusercontent.com/dkulagin/kartaslov/master/dataset/orfo_and_typos/orfo_and_typos.L1_5.csv\'\n\n            download(fname, url)\n\n            mark_done(data_path)\n\n            log.info(\'Built\')\n        return fname\n\n    @staticmethod\n    def read(data_path: str, *args, **kwargs) -> Dict[str, List[Tuple[str, str]]]:\n        """"""Read train data for spelling corrections algorithms\n\n        Args:\n            data_path: path that needs to be interpreted with :meth:`~deeppavlov.dataset_readers.typos_reader.TyposKartaslov.build`\n\n        Returns:\n            train data to pass to a :class:`~deeppavlov.dataset_iterators.typos_iterator.TyposDatasetIterator`\n        """"""\n        fname = TyposKartaslov.build(data_path)\n        with open(str(fname), newline=\'\', encoding=\'utf8\') as csvfile:\n            reader = csv.reader(csvfile, delimiter=\';\')\n            next(reader)\n            res = [(mistake, correct) for correct, mistake, weight in reader]\n        return {\'train\': res}\n'"
deeppavlov/dataset_readers/ubuntu_dstc7_mt_reader.py,0,"b'# Copyright 2018 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nfrom pathlib import Path\nfrom typing import List, Tuple, Dict\n\nimport numpy as np\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.dataset_reader import DatasetReader\n\n\n@register(\'ubuntu_dstc7_mt_reader\')\nclass UbuntuDSTC7MTReader(DatasetReader):\n    """"""\n    DatasetReader for Ubuntu Dialogue Corpus Dataset (version 3), prepared for DSTC 7 competition Track 1 Subtrack 1.\n\n    https://github.com/IBM/dstc7-noesis\n\n    Args:\n        data_path (str): A path to a folder with dataset json files.\n        num_context_turns (int): A maximum number of dialogue ``context`` turns.\n        num_responses (int): A number of responses for each context; default is equal to all 100 responses,\n            it can be reduced to 10 (1 true response + 9 random wrong responses) to adapt with succeeding pipeline\n        padding (str): ""post"" or ""pre"" context sentences padding\n    """"""\n\n    def read(self,\n             data_path: str,\n             num_context_turns: int = 10,\n             num_responses: int = 100,\n             padding: str = ""post"",\n             seed: int = 42,\n             *args, **kwargs) -> Dict[str, List[Tuple[List[str], int]]]:\n\n        self.num_turns = num_context_turns\n        self.padding = padding\n        self.num_responses = num_responses\n        self.np_random = np.random.RandomState(seed)\n\n        dataset = {}\n        dataset[""train""] = self._create_dialog_iter(Path(data_path) / \'ubuntu_train_subtask_1.json\', ""train"")\n        dataset[""valid""] = self._create_dialog_iter(Path(data_path) / \'ubuntu_dev_subtask_1.json\', ""valid"")\n        dataset[""test""] = self._create_dialog_iter(Path(data_path) / \'ubuntu_test_subtask_1.json\', ""test"")\n        return dataset\n\n    def _create_dialog_iter(self, filename, mode=""train""):\n        """"""\n        Read input json file with test data and transform it to the following format:\n        [\n            ( [context_utt_1, ..., context_utt_10, response_utt_1, ..., response_utt_N], label ),\n            ( [context_utt_1, ..., context_utt_10, response_utt_1, ..., response_utt_N], label ),\n            ...\n        ]\n\n        where\n        * [context_utt_1, ..., context_utt_10, response_utt_1, ..., response_utt_N] - list that consists of\n        ``num_context_turn`` utterances, followed by ``num_responses`` responses.\n        Where\n        * label - label of the sample\n\n        Args:\n            filename (Path): filename to read\n            mode (str): which dataset to return. Can be ""train"", ""valid"" or ""test""\n\n        Returns:\n             list of contexts and responses with their labels. More details about the format are provided above\n        """"""\n        data = []\n        with open(filename, encoding=\'utf-8\') as f:\n            json_data = json.load(f)\n            for entry in json_data:\n\n                dialog = entry\n                utterances = []  # all the context sentences\n                for msg in dialog[\'messages-so-far\']:\n                    utterances.append(msg[\'utterance\'])\n\n                true_response = """"  # true response sentence\n                if mode != ""test"":\n                    true_response = dialog[\'options-for-correct-answers\'][0][\'utterance\']\n\n                fake_responses = []  # rest (wrong) responses\n                target_id = """"\n                if mode != ""test"":\n                    correct_answer = dialog[\'options-for-correct-answers\'][0]\n                    target_id = correct_answer[\'candidate-id\']\n                for i, utterance in enumerate(dialog[\'options-for-next\']):\n                    if utterance[\'candidate-id\'] != target_id:\n                        fake_responses.append(utterance[\'utterance\'])\n\n                # aligned list of context utterances\n                expanded_context = self._expand_context(utterances, padding=self.padding)\n\n                if mode == \'train\':\n                    data.append((expanded_context + [true_response], 1))\n                    data.append(\n                        (expanded_context + list(self.np_random.choice(fake_responses, size=1)), 0))  # random 1 from 99\n\n                elif mode == \'valid\':\n                    # NOTE: labels are useless here...\n                    data.append((expanded_context + [true_response] + list(\n                        self.np_random.choice(fake_responses, self.num_responses - 1)), 0))\n\n                elif mode == \'test\':\n                    data.append((expanded_context + fake_responses, 0))\n\n        return data\n\n    def _expand_context(self, context: List[str], padding: str) -> List[str]:\n        """"""\n        Align context length by using pre/post padding of empty sentences up to ``self.num_turns`` sentences\n        or by reducing the number of context sentences to ``self.num_turns`` sentences.\n\n        Args:\n            context (List[str]): list of raw context sentences\n            padding (str): ""post"" or ""pre"" context sentences padding\n\n        Returns:\n            List[str]: list of ``self.num_turns`` context sentences\n        """"""\n        if padding == ""post"":\n            sent_list = context\n            res = sent_list + (self.num_turns - len(sent_list)) * \\\n                  [\'\'] if len(sent_list) < self.num_turns else sent_list[:self.num_turns]\n            return res\n        elif padding == ""pre"":\n            sent_list = context[-(self.num_turns + 1):-1]\n            if len(sent_list) <= self.num_turns:\n                tmp = sent_list[:]\n                sent_list = [\'\'] * (self.num_turns - len(sent_list))\n                sent_list.extend(tmp)\n            return sent_list\n'"
deeppavlov/dataset_readers/ubuntu_v1_mt_reader.py,0,"b'# Copyright 2018 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom pathlib import Path\nfrom typing import List, Tuple, Union, Dict\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.dataset_reader import DatasetReader\n\n\n@register(\'ubuntu_v1_mt_reader\')\nclass UbuntuV1MTReader(DatasetReader):\n    """"""The class to read the Ubuntu V1 dataset from csv files taking into account multi-turn dialogue ``context``.\n\n    Please, see https://github.com/rkadlec/ubuntu-ranking-dataset-creator.\n\n    Args:\n        data_path: A path to a folder with dataset csv files.\n        num_context_turns: A maximum number of dialogue ``context`` turns.\n        padding: ""post"" or ""pre"" context sentences padding\n    """"""\n\n    def read(self, data_path: str,\n             num_context_turns: int = 1,\n             padding: str = ""post"",\n             *args, **kwargs) -> Dict[str, List[Tuple[List[str], int]]]:\n        self.num_turns = num_context_turns\n        self.padding = padding\n        dataset = {\'train\': None, \'valid\': None, \'test\': None}\n        train_fname = Path(data_path) / \'train.txt\'\n        valid_fname = Path(data_path) / \'valid.txt\'\n        test_fname = Path(data_path) / \'test.txt\'\n        self.sen2int_vocab = {}\n        self.classes_vocab_train = {}\n        self.classes_vocab_valid = {}\n        self.classes_vocab_test = {}\n        dataset[""train""] = self.preprocess_data_train(train_fname)\n        dataset[""valid""] = self.preprocess_data_validation(valid_fname)\n        dataset[""test""] = self.preprocess_data_validation(test_fname)\n        return dataset\n\n    def preprocess_data_train(self, train_fname: Union[Path, str]) -> List[Tuple[List[str], int]]:\n        contexts = []\n        responses = []\n        labels = []\n        with open(train_fname, encoding=\'utf-8\') as f:\n            for line in f:\n                line = line.replace(\'_\', \'\')\n                parts = line.strip().split(\'\\t\')\n\n                label = int(parts[0])\n                context = parts[1:-1]\n                response = parts[-1]\n\n                contexts.append(self._expand_context(context, padding=self.padding))\n                responses.append(response)\n                labels.append(label)\n        data = [el[0] + [el[1]] for el in zip(contexts, responses)]\n        data = list(zip(data, labels))\n        return data\n\n    def preprocess_data_validation(self, fname: Union[Path, str]) -> List[Tuple[List[str], int]]:\n        contexts = []\n        responses = []\n        with open(fname, encoding=\'utf-8\') as f:\n            responses_buf = []\n            for line in f:\n                line = line.replace(\'_\', \'\')\n                parts = line.strip().split(\'\\t\')\n\n                label = int(parts[0])  # labels are not used\n                context = parts[1:-1]\n                responses_buf.append(parts[-1])  # add the next response\n\n                if len(responses_buf) % 10 == 0:  # add context and 10 response candidates\n                    contexts.append(self._expand_context(context, padding=self.padding))\n                    responses.append(responses_buf)\n                    responses_buf = []\n\n        data = [el[0] + el[1] for el in zip(contexts, responses)]\n        data = [(el, 1) for el in data]  # NOTE: labels are useless here actually...\n        return data\n\n    def _expand_context(self, context: List[str], padding: str) -> List[str]:\n        """"""\n        Align context length by using pre/post padding of empty sentences up to ``self.num_turns`` sentences\n        or by reducing the number of context sentences to ``self.num_turns`` sentences.\n\n        Args:\n            context (List[str]): list of raw context sentences\n            padding (str): ""post"" or ""pre"" context sentences padding\n\n        Returns:\n            List[str]: list of ``self.num_turns`` context sentences\n        """"""\n        if padding == ""post"":\n            sent_list = context\n            res = sent_list + (self.num_turns - len(sent_list)) * \\\n                  [\'\'] if len(sent_list) < self.num_turns else sent_list[:self.num_turns]\n            return res\n        elif padding == ""pre"":\n            # context[-self.num_turns:]  because there is no empty strings in `context`\n            sent_list = context[-self.num_turns:]\n            if len(sent_list) <= self.num_turns:\n                tmp = sent_list[:]\n                sent_list = [\'\'] * (self.num_turns - len(sent_list))\n                sent_list.extend(tmp)\n            return sent_list\n'"
deeppavlov/dataset_readers/ubuntu_v2_mt_reader.py,0,"b'# Copyright 2018 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport csv\nfrom pathlib import Path\nfrom typing import List, Tuple, Union, Dict\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.dataset_reader import DatasetReader\n\n\n@register(\'ubuntu_v2_mt_reader\')\nclass UbuntuV2MTReader(DatasetReader):\n    """"""The class to read the Ubuntu V2 dataset from csv files taking into account multi-turn dialogue ``context``.\n\n    Please, see https://github.com/rkadlec/ubuntu-ranking-dataset-creator.\n\n    Args:\n        data_path: A path to a folder with dataset csv files.\n        num_context_turns: A maximum number of dialogue ``context`` turns.\n        padding: ""post"" or ""pre"" context sentences padding\n    """"""\n\n    def read(self, data_path: str,\n             num_context_turns: int = 1,\n             padding: str = ""post"",\n             *args, **kwargs) -> Dict[str, List[Tuple[List[str], int]]]:\n        """"""Read the Ubuntu V2 dataset from csv files taking into account multi-turn dialogue ``context``.\n\n        Args:\n            data_path: A path to a folder with dataset csv files.\n            num_context_turns: A maximum number of dialogue ``context`` turns.\n            padding: ""post"" or ""pre"" context sentences padding\n\n        Returns:\n            Dictionary with keys ""train"", ""valid"", ""test"" and parts of the dataset as their values\n        """"""\n\n        self.num_turns = num_context_turns\n        self.padding = padding\n        dataset = {\'train\': None, \'valid\': None, \'test\': None}\n        train_fname = Path(data_path) / \'train.csv\'\n        valid_fname = Path(data_path) / \'valid.csv\'\n        test_fname = Path(data_path) / \'test.csv\'\n        self.sen2int_vocab = {}\n        self.classes_vocab_train = {}\n        self.classes_vocab_valid = {}\n        self.classes_vocab_test = {}\n        dataset[""train""] = self.preprocess_data_train(train_fname)\n        dataset[""valid""] = self.preprocess_data_validation(valid_fname)\n        dataset[""test""] = self.preprocess_data_validation(test_fname)\n        return dataset\n\n    def preprocess_data_train(self, train_fname: Union[Path, str]) -> List[Tuple[List[str], int]]:\n        contexts = []\n        responses = []\n        labels = []\n        with open(train_fname, \'r\') as f:\n            reader = csv.reader(f)\n            next(reader)\n            for el in reader:\n                contexts.append(self._expand_context(el[0].split(\'__eot__\'), padding=self.padding))\n                responses.append(el[1])\n                labels.append(int(el[2]))\n        data = [el[0] + [el[1]] for el in zip(contexts, responses)]\n        data = list(zip(data, labels))\n        return data\n\n    def preprocess_data_validation(self, fname: Union[Path, str]) -> List[Tuple[List[str], int]]:\n        contexts = []\n        responses = []\n        with open(fname, \'r\') as f:\n            reader = csv.reader(f)\n            next(reader)\n            for el in reader:\n                contexts.append(self._expand_context(el[0].split(\'__eot__\'), padding=self.padding))\n                responses.append(el[1:])\n        data = [el[0] + el[1] for el in zip(contexts, responses)]\n        data = [(el, 1) for el in data]  # NOTE: labels are useless here actually...\n        return data\n\n    def _expand_context(self, context: List[str], padding: str) -> List[str]:\n        """"""\n        Align context length by using pre/post padding of empty sentences up to ``self.num_turns`` sentences\n        or by reducing the number of context sentences to ``self.num_turns`` sentences.\n\n        Args:\n            context (List[str]): list of raw context sentences\n            padding (str): ""post"" or ""pre"" context sentences padding\n\n        Returns:\n            List[str]: list of ``self.num_turns`` context sentences\n        """"""\n        if padding == ""post"":\n            sent_list = context\n            res = sent_list + (self.num_turns - len(sent_list)) * \\\n                  [\'\'] if len(sent_list) < self.num_turns else sent_list[:self.num_turns]\n            return res\n        elif padding == ""pre"":\n            # context[-(self.num_turns + 1):-1]  because the last item of `context` is always \'\' (empty string)\n            sent_list = context[-(self.num_turns + 1):-1]\n            if len(sent_list) <= self.num_turns:\n                tmp = sent_list[:]\n                sent_list = [\'\'] * (self.num_turns - len(sent_list))\n                sent_list.extend(tmp)\n            return sent_list\n'"
deeppavlov/dataset_readers/ubuntu_v2_reader.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport csv\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple, Union\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.dataset_reader import DatasetReader\n\n\n@register(\'ubuntu_v2_reader\')\nclass UbuntuV2Reader(DatasetReader):\n    """"""The class to read the Ubuntu V2 dataset from csv files.\n\n    Please, see https://github.com/rkadlec/ubuntu-ranking-dataset-creator.\n    """"""\n\n    def read(self, data_path: str,\n             positive_samples=False,\n             *args, **kwargs) -> Dict[str, List[Tuple[List[str], int]]]:\n        """"""Read the Ubuntu V2 dataset from csv files.\n\n        Args:\n            data_path: A path to a folder with dataset csv files.\n            positive_samples: if `True`, only positive context-response pairs will be taken for train\n        """"""\n\n        data_path = expand_path(data_path)\n        dataset = {\'train\': None, \'valid\': None, \'test\': None}\n        train_fname = Path(data_path) / \'train.csv\'\n        valid_fname = Path(data_path) / \'valid.csv\'\n        test_fname = Path(data_path) / \'test.csv\'\n        self.positive_samples = positive_samples\n        self.sen2int_vocab = {}\n        self.classes_vocab_train = {}\n        self.classes_vocab_valid = {}\n        self.classes_vocab_test = {}\n        dataset[""train""] = self.preprocess_data_train(train_fname)\n        dataset[""valid""] = self.preprocess_data_validation(valid_fname)\n        dataset[""test""] = self.preprocess_data_validation(test_fname)\n        return dataset\n\n    def preprocess_data_train(self, train_fname: Union[Path, str]) -> List[Tuple[List[str], int]]:\n        contexts = []\n        responses = []\n        labels = []\n        with open(train_fname, \'r\') as f:\n            reader = csv.reader(f)\n            next(reader)\n            for el in reader:\n                contexts.append(el[0])\n                responses.append(el[1])\n                labels.append(int(el[2]))\n            data = list(zip(contexts, responses))\n            data = list(zip(data, labels))\n            if self.positive_samples:\n                data = [el[0] for el in data if el[1] == 1]\n                data = list(zip(data, range(len(data))))\n        return data\n\n    def preprocess_data_validation(self, fname: Union[Path, str]) -> List[Tuple[List[str], int]]:\n        contexts = []\n        responses = []\n        with open(fname, \'r\') as f:\n            reader = csv.reader(f)\n            next(reader)\n            for el in reader:\n                contexts.append(el[0])\n                responses.append(el[1:])\n        data = [[el[0]] + el[1] for el in zip(contexts, responses)]\n        data = [(el, 1) for el in data]\n        return data\n'"
deeppavlov/deprecated/__init__.py,0,b''
deeppavlov/metrics/__init__.py,0,b''
deeppavlov/metrics/accuracy.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport itertools\nfrom typing import List, Iterable\n\nimport numpy as np\n\nfrom deeppavlov.core.common.metrics_registry import register_metric\nfrom deeppavlov.models.go_bot.nlg.dto.json_nlg_response import JSONNLGResponse\n\n\n@register_metric(\'accuracy\')\ndef accuracy(y_true: [list, np.ndarray], y_predicted: [list, np.ndarray]) -> float:\n    """"""\n    Calculate accuracy in terms of absolute coincidence\n\n    Args:\n        y_true: array of true values\n        y_predicted: array of predicted values\n\n    Returns:\n        fraction of absolutely coincidental samples\n    """"""\n    examples_len = len(y_true)\n    # if y1 and y2 are both arrays, == can be erroneously interpreted as element-wise equality\n\n    def _are_equal(y1, y2):\n        answer = (y1 == y2)\n        if isinstance(answer, np.ndarray):\n            answer = answer.all()\n        return answer\n\n    equalities = [_are_equal(y1, y2) for y1, y2 in zip(y_true, y_predicted)]\n    correct = sum(equalities)\n    return correct / examples_len if examples_len else 0\n\n\n@register_metric(\'multitask_accuracy\')\ndef multitask_accuracy(*args) -> float:\n    """"""\n    Accuracy for multiple simultaneous tasks.\n\n    Args:\n        *args: a list of `2n` inputs. The first `n` inputs are the correct answers for `n` tasks,\n            and the last `n` are the predicted ones.\n\n    Returns:\n        The percentage of inputs where the answers for all `n` tasks are correct.\n    """"""\n    n = len(args)\n    y_true_by_tasks, y_predicted_by_tasks = args[:n // 2], args[n // 2:]\n    y_true, y_predicted = list(zip(*y_true_by_tasks)), list(zip(*y_predicted_by_tasks))\n    return accuracy(y_true, y_predicted)\n\n\n@register_metric(\'multitask_sequence_accuracy\')\ndef multitask_sequence_accuracy(*args) -> float:\n    """"""\n    Accuracy for multiple simultaneous sequence labeling (tagging) tasks.\n    For each sequence the model checks whether all its elements\n    are labeled correctly for all the individual taggers.\n\n    Args:\n        *args: a list of `2n` inputs. The first `n` inputs are the correct answers for `n` tasks,\n            and the last `n` are the predicted ones. For each task an\n\n    Returns:\n        The percentage of sequences where all the items has correct answers for all `n` tasks.\n\n    """"""\n    n = len(args)\n    y_true_by_tasks, y_predicted_by_tasks = args[:n // 2], args[n // 2:]\n    y_true_by_sents = list(zip(*y_true_by_tasks))\n    y_predicted_by_sents = list(zip(*y_predicted_by_tasks))\n    y_true = list(list(zip(*elem)) for elem in y_true_by_sents)\n    y_predicted = list(list(zip(*elem)) for elem in y_predicted_by_sents)\n    return accuracy(y_true, y_predicted)\n\n\n@register_metric(\'multitask_token_accuracy\')\ndef multitask_token_accuracy(*args) -> float:\n    """"""\n        Per-item accuracy for multiple simultaneous sequence labeling (tagging) tasks.\n\n        Args:\n            *args: a list of `2n` inputs. The first `n` inputs are the correct answers for `n` tasks\n                and the last `n` are the predicted ones. For each task an\n\n        Returns:\n            The percentage of sequence elements for which the answers for all `n` tasks are correct.\n\n        """"""\n    n = len(args)\n    y_true_by_tasks, y_predicted_by_tasks = args[:n // 2], args[n // 2:]\n    y_true_by_sents = list(zip(*y_true_by_tasks))\n    y_predicted_by_sents = list(zip(*y_predicted_by_tasks))\n    y_true = list(list(zip(*elem)) for elem in y_true_by_sents)\n    y_predicted = list(list(zip(*elem)) for elem in y_predicted_by_sents)\n    return per_token_accuracy(y_true, y_predicted)\n\n\n@register_metric(\'sets_accuracy\')\ndef sets_accuracy(y_true: [list, np.ndarray], y_predicted: [list, np.ndarray]) -> float:\n    """"""\n    Calculate accuracy in terms of sets coincidence\n\n    Args:\n        y_true: true values\n        y_predicted: predicted values\n\n    Returns:\n        portion of samples with absolutely coincidental sets of predicted values\n    """"""\n    examples_len = len(y_true)\n    correct = sum([set(y1) == set(y2) for y1, y2 in zip(y_true, y_predicted)])\n    return correct / examples_len if examples_len else 0\n\n\n@register_metric(\'slots_accuracy\')\ndef slots_accuracy(y_true, y_predicted):\n    y_true = [{tag.split(\'-\')[-1] for tag in s if tag != \'O\'} for s in y_true]\n    y_predicted = [set(s.keys()) for s in y_predicted]\n    return accuracy(y_true, y_predicted)\n\n\n@register_metric(\'per_token_accuracy\')\ndef per_token_accuracy(y_true, y_predicted):\n    y_true = list(itertools.chain(*y_true))\n    y_predicted = itertools.chain(*y_predicted)\n    examples_len = len(y_true)\n    correct = sum([y1 == y2 for y1, y2 in zip(y_true, y_predicted)])\n    return correct / examples_len if examples_len else 0\n\n\n# region go-bot metrics\n\n@register_metric(\'per_item_dialog_accuracy\')\ndef per_item_dialog_accuracy(y_true, y_predicted: List[List[str]]):\n    # todo metric classes???\n    y_true = [y[\'text\'] for dialog in y_true for y in dialog]\n    y_predicted = itertools.chain(*y_predicted)\n    examples_len = len(y_true)\n    correct = sum([y1.strip().lower() == y2.strip().lower() for y1, y2 in zip(y_true, y_predicted)])\n    return correct / examples_len if examples_len else 0\n\n\n@register_metric(""per_item_action_accuracy"")\ndef per_item_action_accuracy(dialogs_true, dialog_jsons_predicted: List[List[JSONNLGResponse]]):\n    # todo metric classes???\n    # todo oop instead of serialization/deserialization\n    utterances_actions_true = [utterance[\'act\']\n                               for dialog in dialogs_true\n                               for utterance in dialog]\n\n    utterances_actions_predicted: Iterable[JSONNLGResponse] = itertools.chain(*dialog_jsons_predicted)\n    examples_len = len(utterances_actions_true)\n    correct = sum([y1.strip().lower() == \'+\'.join(y2.actions_tuple).lower()\n                   for y1, y2 in zip(utterances_actions_true, utterances_actions_predicted)])  # todo ugly\n    return correct / examples_len if examples_len else 0\n\n# endregion go-bot metrics\n\n\n@register_metric(\'acc\')\ndef round_accuracy(y_true, y_predicted):\n    """"""\n    Rounds predictions and calculates accuracy in terms of absolute coincidence.\n\n    Args:\n        y_true: list of true values\n        y_predicted: list of predicted values\n\n    Returns:\n        portion of absolutely coincidental samples\n    """"""\n    predictions = [round(x) for x in y_predicted]\n    examples_len = len(y_true)\n    correct = sum([y1 == y2 for y1, y2 in zip(y_true, predictions)])\n    return correct / examples_len if examples_len else 0\n\n\n@register_metric(\'kbqa_accuracy\')\ndef kbqa_accuracy(y_true, y_predicted):\n    total_correct = 0\n    for answer_true, answer_predicted in zip(y_true, y_predicted):\n        if answer_predicted in answer_true:\n            total_correct += 1\n\n    return total_correct / len(y_true) if len(y_true) else 0\n'"
deeppavlov/metrics/bleu.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport itertools\nfrom typing import List, Tuple, Any\n\nfrom nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction, brevity_penalty, closest_ref_length\n\nfrom deeppavlov.core.common.metrics_registry import register_metric\nfrom deeppavlov.metrics.google_bleu import compute_bleu\n\nSMOOTH = SmoothingFunction()\n\n\n@register_metric(\'bleu_advanced\')\ndef bleu_advanced(y_true: List[Any], y_predicted: List[Any],\n                  weights: Tuple = (1,), smoothing_function=SMOOTH.method1,\n                  auto_reweigh=False, penalty=True) -> float:\n    """"""Calculate BLEU score\n\n    Parameters:\n        y_true: list of reference tokens\n        y_predicted: list of query tokens\n        weights: n-gram weights\n        smoothing_function: SmoothingFunction\n        auto_reweigh: Option to re-normalize the weights uniformly\n        penalty: either enable brevity penalty or not\n\n    Return:\n        BLEU score\n    """"""\n\n    bleu_measure = sentence_bleu([y_true], y_predicted, weights, smoothing_function, auto_reweigh)\n\n    hyp_len = len(y_predicted)\n    hyp_lengths = hyp_len\n    ref_lengths = closest_ref_length([y_true], hyp_len)\n\n    bpenalty = brevity_penalty(ref_lengths, hyp_lengths)\n\n    if penalty is True or bpenalty == 0:\n        return bleu_measure\n\n    return bleu_measure / bpenalty\n\n\n@register_metric(\'bleu\')\ndef bleu(y_true, y_predicted):\n    return corpus_bleu([[y_t.lower().split()] for y_t in y_true],\n                       [y_p.lower().split() for y_p in y_predicted])\n\n\n@register_metric(\'google_bleu\')\ndef google_bleu(y_true, y_predicted):\n    return compute_bleu(([y_t.lower().split()] for y_t in y_true),\n                        (y_p.lower().split() for y_p in y_predicted))[0]\n\n\n@register_metric(\'per_item_bleu\')\ndef per_item_bleu(y_true, y_predicted):\n    y_predicted = itertools.chain(*y_predicted)\n    return corpus_bleu([[y_t.lower().split()] for y_t in y_true],\n                       [y_p.lower().split() for y_p in y_predicted])\n\n\n@register_metric(\'per_item_dialog_bleu\')\ndef per_item_dialog_bleu(y_true, y_predicted):\n    y_true = (y[\'text\'] for dialog in y_true for y in dialog)\n    return corpus_bleu([[y_t.lower().split()] for y_t in y_true],\n                       [y.lower().split() for y_p in y_predicted for y in y_p])\n'"
deeppavlov/metrics/elmo_metrics.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import List\n\nimport numpy as np\n\nfrom deeppavlov.core.common.metrics_registry import register_metric\n\n\n@register_metric(\'elmo_loss2ppl\')\ndef elmo_loss2ppl(losses: List[np.ndarray]) -> float:\n    """""" Calculates perplexity by loss\n\n    Args:\n        losses: list of numpy arrays of model losses\n\n    Returns:\n        perplexity : float\n    """"""\n    avg_loss = np.mean(losses)\n    return float(np.exp(avg_loss))\n'"
deeppavlov/metrics/fmeasure.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport itertools\nfrom collections import OrderedDict\nfrom itertools import chain\nfrom logging import getLogger\n\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\nfrom deeppavlov.core.common.metrics_registry import register_metric\n\nlog = getLogger(__name__)\n\n\n@register_metric(\'ner_f1\')\ndef ner_f1(y_true, y_predicted):\n    y_true = list(chain(*y_true))\n    y_predicted = list(chain(*y_predicted))\n    results = precision_recall_f1(y_true,\n                                  y_predicted,\n                                  print_results=True)\n    f1 = results[\'__total__\'][\'f1\']\n    return f1\n\n\n@register_metric(\'ner_token_f1\')\ndef ner_token_f1(y_true, y_pred, print_results=False):\n    y_true = list(chain(*y_true))\n    y_pred = list(chain(*y_pred))\n\n    # Drop BIO or BIOES markup\n    assert all(len(tag.split(\'-\')) <= 2 for tag in y_true)\n\n    y_true = [tag.split(\'-\')[-1] for tag in y_true]\n    y_pred = [tag.split(\'-\')[-1] for tag in y_pred]\n    tags = set(y_true) | set(y_pred)\n    tags_dict = {tag: n for n, tag in enumerate(tags)}\n\n    y_true_inds = np.array([tags_dict[tag] for tag in y_true])\n    y_pred_inds = np.array([tags_dict[tag] for tag in y_pred])\n\n    results = {}\n    for tag, tag_ind in tags_dict.items():\n        if tag == \'O\':\n            continue\n        tp = np.sum((y_true_inds == tag_ind) & (y_pred_inds == tag_ind))\n        fn = np.sum((y_true_inds == tag_ind) & (y_pred_inds != tag_ind))\n        fp = np.sum((y_true_inds != tag_ind) & (y_pred_inds == tag_ind))\n        n_pred = np.sum(y_pred_inds == tag_ind)\n        n_true = np.sum(y_true_inds == tag_ind)\n        if tp + fp > 0:\n            precision = tp / (tp + fp) * 100\n        else:\n            precision = 0\n        if tp + fn > 0:\n            recall = tp / (tp + fn) * 100\n        else:\n            recall = 0\n        if precision + recall > 0:\n            f1 = 2 * precision * recall / (precision + recall)\n        else:\n            f1 = 0\n        results[tag] = {\'precision\': precision, \'recall\': recall,\n                        \'f1\': f1, \'n_true\': n_true, \'n_pred\': n_pred,\n                        \'tp\': tp, \'fp\': fp, \'fn\': fn}\n\n    results[\'__total__\'], accuracy, total_true_entities, total_predicted_entities, total_correct = _global_stats_f1(\n        results)\n    n_tokens = len(y_true)\n    if print_results:\n        log.debug(\'TOKEN LEVEL F1\')\n        _print_conll_report(results, accuracy, total_true_entities, total_predicted_entities, n_tokens, total_correct)\n    return results[\'__total__\'][\'f1\']\n\n\ndef _print_conll_report(results, accuracy, total_true_entities, total_predicted_entities, n_tokens, total_correct,\n                        short_report=False, entity_of_interest=None):\n    tags = list(results.keys())\n\n    s = \'processed {len} tokens \' \\\n        \'with {tot_true} phrases; \' \\\n        \'found: {tot_pred} phrases;\' \\\n        \' correct: {tot_cor}.\\n\\n\'.format(len=n_tokens,\n                                          tot_true=total_true_entities,\n                                          tot_pred=total_predicted_entities,\n                                          tot_cor=total_correct)\n\n    s += \'precision:  {tot_prec:.2f}%; \' \\\n         \'recall:  {tot_recall:.2f}%; \' \\\n         \'FB1:  {tot_f1:.2f}\\n\\n\'.format(acc=accuracy,\n                                         tot_prec=results[\'__total__\'][\'precision\'],\n                                         tot_recall=results[\'__total__\'][\'recall\'],\n                                         tot_f1=results[\'__total__\'][\'f1\'])\n\n    if not short_report:\n        for tag in tags:\n            if entity_of_interest is not None:\n                if entity_of_interest in tag:\n                    s += \'\\t\' + tag + \': precision:  {tot_prec:.2f}%; \' \\\n                                      \'recall:  {tot_recall:.2f}%; \' \\\n                                      \'F1:  {tot_f1:.2f} \' \\\n                                      \'{tot_predicted}\\n\\n\'.format(tot_prec=results[tag][\'precision\'],\n                                                                   tot_recall=results[tag][\'recall\'],\n                                                                   tot_f1=results[tag][\'f1\'],\n                                                                   tot_predicted=results[tag][\'n_pred\'])\n            elif tag != \'__total__\':\n                s += \'\\t\' + tag + \': precision:  {tot_prec:.2f}%; \' \\\n                                  \'recall:  {tot_recall:.2f}%; \' \\\n                                  \'F1:  {tot_f1:.2f} \' \\\n                                  \'{tot_predicted}\\n\\n\'.format(tot_prec=results[tag][\'precision\'],\n                                                               tot_recall=results[tag][\'recall\'],\n                                                               tot_f1=results[tag][\'f1\'],\n                                                               tot_predicted=results[tag][\'n_pred\'])\n    elif entity_of_interest is not None:\n        s += \'\\t\' + entity_of_interest + \': precision:  {tot_prec:.2f}%; \' \\\n                                         \'recall:  {tot_recall:.2f}%; \' \\\n                                         \'F1:  {tot_f1:.2f} \' \\\n                                         \'{tot_predicted}\\n\\n\'.format(tot_prec=results[entity_of_interest][\'precision\'],\n                                                                      tot_recall=results[entity_of_interest][\'recall\'],\n                                                                      tot_f1=results[entity_of_interest][\'f1\'],\n                                                                      tot_predicted=results[entity_of_interest][\n                                                                          \'n_pred\'])\n    log.debug(s)\n\n\ndef _global_stats_f1(results):\n    total_true_entities = 0\n    total_predicted_entities = 0\n    total_precision = 0\n    total_recall = 0\n    total_f1 = 0\n    total_correct = 0\n    for tag in results:\n        if tag == \'__total__\':\n            continue\n\n        n_pred = results[tag][\'n_pred\']\n        n_true = results[tag][\'n_true\']\n        total_correct += results[tag][\'tp\']\n        total_true_entities += n_true\n        total_predicted_entities += n_pred\n        total_precision += results[tag][\'precision\'] * n_pred\n        total_recall += results[tag][\'recall\'] * n_true\n        total_f1 += results[tag][\'f1\'] * n_true\n    if total_true_entities > 0:\n        accuracy = total_correct / total_true_entities * 100\n        total_recall = total_recall / total_true_entities\n    else:\n        accuracy = 0\n        total_recall = 0\n    if total_predicted_entities > 0:\n        total_precision = total_precision / total_predicted_entities\n    else:\n        total_precision = 0\n\n    if total_precision + total_recall > 0:\n        total_f1 = 2 * total_precision * total_recall / (total_precision + total_recall)\n    else:\n        total_f1 = 0\n\n    total_res = {\'n_predicted_entities\': total_predicted_entities,\n                 \'n_true_entities\': total_true_entities,\n                 \'precision\': total_precision,\n                 \'recall\': total_recall,\n                 \'f1\': total_f1}\n    return total_res, accuracy, total_true_entities, total_predicted_entities, total_correct\n\n\n@register_metric(\'f1\')\ndef round_f1(y_true, y_predicted):\n    """"""\n    Calculates F1 (binary) measure.\n\n    Args:\n        y_true: list of true values\n        y_predicted: list of predicted values\n\n    Returns:\n        F1 score\n    """"""\n    try:\n        predictions = [np.round(x) for x in y_predicted]\n    except TypeError:\n        predictions = y_predicted\n\n    return f1_score(y_true, predictions)\n\n\n@register_metric(\'f1_macro\')\ndef round_f1_macro(y_true, y_predicted):\n    """"""\n    Calculates F1 macro measure.\n\n    Args:\n        y_true: list of true values\n        y_predicted: list of predicted values\n\n    Returns:\n        F1 score\n    """"""\n    try:\n        predictions = [np.round(x) for x in y_predicted]\n    except TypeError:\n        predictions = y_predicted\n\n    return f1_score(np.array(y_true), np.array(predictions), average=""macro"")\n\n\n@register_metric(\'f1_weighted\')\ndef round_f1_weighted(y_true, y_predicted):\n    """"""\n    Calculates F1 weighted measure.\n\n    Args:\n        y_true: list of true values\n        y_predicted: list of predicted values\n\n    Returns:\n        F1 score\n    """"""\n    try:\n        predictions = [np.round(x) for x in y_predicted]\n    except TypeError:\n        predictions = y_predicted\n\n    return f1_score(np.array(y_true), np.array(predictions), average=""weighted"")\n\n\ndef chunk_finder(current_token, previous_token, tag):\n    current_tag = current_token.split(\'-\', 1)[-1]\n    previous_tag = previous_token.split(\'-\', 1)[-1]\n    if previous_tag != tag:\n        previous_tag = \'O\'\n    if current_tag != tag:\n        current_tag = \'O\'\n\n    if current_tag != \'O\' and (\n            previous_tag == \'O\' or\n            previous_token in [\'E-\' + tag, \'L-\' + tag, \'S-\' + tag, \'U-\' + tag] or\n            current_token in [\'B-\' + tag, \'S-\' + tag, \'U-\' + tag]\n    ):\n        create_chunk = True\n    else:\n        create_chunk = False\n\n    if previous_tag != \'O\' and (\n            current_tag == \'O\' or\n            previous_token in [\'E-\' + tag, \'L-\' + tag, \'S-\' + tag, \'U-\' + tag] or\n            current_token in [\'B-\' + tag, \'S-\' + tag, \'U-\' + tag]\n    ):\n        pop_out = True\n    else:\n        pop_out = False\n    return create_chunk, pop_out\n\n\ndef precision_recall_f1(y_true, y_pred, print_results=True, short_report=False, entity_of_interest=None):\n    # Find all tags\n    tags = set()\n    for tag in itertools.chain(y_true, y_pred):\n        if tag != \'O\':\n            current_tag = tag[2:]\n            tags.add(current_tag)\n    tags = sorted(list(tags))\n\n    results = OrderedDict()\n    for tag in tags:\n        results[tag] = OrderedDict()\n    results[\'__total__\'] = OrderedDict()\n    n_tokens = len(y_true)\n    total_correct = 0\n    # Firstly we find all chunks in the ground truth and prediction\n    # For each chunk we write starting and ending indices\n\n    for tag in tags:\n        count = 0\n        true_chunk = []\n        pred_chunk = []\n        y_true = [str(y) for y in y_true]\n        y_pred = [str(y) for y in y_pred]\n        prev_tag_true = \'O\'\n        prev_tag_pred = \'O\'\n        while count < n_tokens:\n            yt = y_true[count]\n            yp = y_pred[count]\n\n            create_chunk_true, pop_out_true = chunk_finder(yt, prev_tag_true, tag)\n            if pop_out_true:\n                true_chunk[-1] = (true_chunk[-1], count - 1)\n            if create_chunk_true:\n                true_chunk.append(count)\n\n            create_chunk_pred, pop_out_pred = chunk_finder(yp, prev_tag_pred, tag)\n            if pop_out_pred:\n                pred_chunk[-1] = (pred_chunk[-1], count - 1)\n            if create_chunk_pred:\n                pred_chunk.append(count)\n            prev_tag_true = yt\n            prev_tag_pred = yp\n            count += 1\n\n        if len(true_chunk) > 0 and not isinstance(true_chunk[-1], tuple):\n            true_chunk[-1] = (true_chunk[-1], count - 1)\n        if len(pred_chunk) > 0 and not isinstance(pred_chunk[-1], tuple):\n            pred_chunk[-1] = (pred_chunk[-1], count - 1)\n\n        # Then we find all correctly classified intervals\n        # True positive results\n        tp = len(set(pred_chunk).intersection(set(true_chunk)))\n        # And then just calculate errors of the first and second kind\n        # False negative\n        fn = len(true_chunk) - tp\n        # False positive\n        fp = len(pred_chunk) - tp\n        if tp + fp > 0:\n            precision = tp / (tp + fp) * 100\n        else:\n            precision = 0\n        if tp + fn > 0:\n            recall = tp / (tp + fn) * 100\n        else:\n            recall = 0\n        if precision + recall > 0:\n            f1 = 2 * precision * recall / (precision + recall)\n        else:\n            f1 = 0\n        results[tag][\'precision\'] = precision\n        results[tag][\'recall\'] = recall\n        results[tag][\'f1\'] = f1\n        results[tag][\'n_pred\'] = len(pred_chunk)\n        results[tag][\'n_true\'] = len(true_chunk)\n        results[tag][\'tp\'] = tp\n        results[tag][\'fn\'] = fn\n        results[tag][\'fp\'] = fp\n\n    results[\'__total__\'], accuracy, total_true_entities, total_predicted_entities, accuracy = _global_stats_f1(results)\n    results[\'__total__\'][\'n_pred\'] = total_predicted_entities\n    results[\'__total__\'][\'n_true\'] = total_true_entities\n\n    if print_results:\n        s = \'processed {len} tokens \' \\\n            \'with {tot_true} phrases; \' \\\n            \'found: {tot_pred} phrases;\' \\\n            \' correct: {tot_cor}.\\n\\n\'.format(len=n_tokens,\n                                              tot_true=total_true_entities,\n                                              tot_pred=total_predicted_entities,\n                                              tot_cor=total_correct)\n\n        s += \'precision:  {tot_prec:.2f}%; \' \\\n             \'recall:  {tot_recall:.2f}%; \' \\\n             \'FB1:  {tot_f1:.2f}\\n\\n\'.format(acc=accuracy,\n                                             tot_prec=results[\'__total__\'][\'precision\'],\n                                             tot_recall=results[\'__total__\'][\'recall\'],\n                                             tot_f1=results[\'__total__\'][\'f1\'])\n\n        if not short_report:\n            for tag in tags:\n                if entity_of_interest is not None:\n                    if entity_of_interest in tag:\n                        s += \'\\t\' + tag + \': precision:  {tot_prec:.2f}%; \' \\\n                                          \'recall:  {tot_recall:.2f}%; \' \\\n                                          \'F1:  {tot_f1:.2f} \' \\\n                                          \'{tot_predicted}\\n\\n\'.format(tot_prec=results[tag][\'precision\'],\n                                                                       tot_recall=results[tag][\'recall\'],\n                                                                       tot_f1=results[tag][\'f1\'],\n                                                                       tot_predicted=results[tag][\'n_pred\'])\n                elif tag != \'__total__\':\n                    s += \'\\t\' + tag + \': precision:  {tot_prec:.2f}%; \' \\\n                                      \'recall:  {tot_recall:.2f}%; \' \\\n                                      \'F1:  {tot_f1:.2f} \' \\\n                                      \'{tot_predicted}\\n\\n\'.format(tot_prec=results[tag][\'precision\'],\n                                                                   tot_recall=results[tag][\'recall\'],\n                                                                   tot_f1=results[tag][\'f1\'],\n                                                                   tot_predicted=results[tag][\'n_pred\'])\n        elif entity_of_interest is not None:\n            s += \'\\t\' + entity_of_interest + \': precision:  {tot_prec:.2f}%; \' \\\n                                             \'recall:  {tot_recall:.2f}%; \' \\\n                                             \'F1:  {tot_f1:.2f} \' \\\n                                             \'{tot_predicted}\\n\\n\'.format(\n                tot_prec=results[entity_of_interest][\'precision\'],\n                tot_recall=results[entity_of_interest][\'recall\'],\n                tot_f1=results[entity_of_interest][\'f1\'],\n                tot_predicted=results[entity_of_interest][\'n_pred\'])\n        log.debug(s)\n    return results\n'"
deeppavlov/metrics/google_bleu.py,0,"b'# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Python implementation of BLEU and smooth-BLEU.\n\nThis module provides a Python implementation of BLEU and smooth-BLEU.\nSmooth BLEU is computed following the method outlined in the paper:\nChin-Yew Lin, Franz Josef Och. ORANGE: a method for evaluating automatic\nevaluation metrics for machine translation. COLING 2004.\n""""""\n\nimport collections\nimport math\n\n\ndef _get_ngrams(segment, max_order):\n    """"""Extracts all n-grams upto a given maximum order from an input segment.\n\n    Args:\n      segment: text segment from which n-grams will be extracted.\n      max_order: maximum length in tokens of the n-grams returned by this\n          methods.\n\n    Returns:\n      The Counter containing all n-grams upto max_order in segment\n      with a count of how many times each n-gram occurred.\n    """"""\n    ngram_counts = collections.Counter()\n    for order in range(1, max_order + 1):\n        for i in range(0, len(segment) - order + 1):\n            ngram = tuple(segment[i:i + order])\n            ngram_counts[ngram] += 1\n    return ngram_counts\n\n\ndef compute_bleu(reference_corpus, translation_corpus, max_order=4,\n                 smooth=False):\n    """"""Computes BLEU score of translated segments against one or more references.\n\n    Args:\n      reference_corpus: list of lists of references for each translation. Each\n          reference should be tokenized into a list of tokens.\n      translation_corpus: list of translations to score. Each translation\n          should be tokenized into a list of tokens.\n      max_order: Maximum n-gram order to use when computing BLEU score.\n      smooth: Whether or not to apply Lin et al. 2004 smoothing.\n\n    Returns:\n      3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n      precisions and brevity penalty.\n    """"""\n    matches_by_order = [0] * max_order\n    possible_matches_by_order = [0] * max_order\n    reference_length = 0\n    translation_length = 0\n    for (references, translation) in zip(reference_corpus,\n                                         translation_corpus):\n        reference_length += min(len(r) for r in references)\n        translation_length += len(translation)\n\n        merged_ref_ngram_counts = collections.Counter()\n        for reference in references:\n            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n        translation_ngram_counts = _get_ngrams(translation, max_order)\n        overlap = translation_ngram_counts & merged_ref_ngram_counts\n        for ngram in overlap:\n            matches_by_order[len(ngram) - 1] += overlap[ngram]\n        for order in range(1, max_order + 1):\n            possible_matches = len(translation) - order + 1\n            if possible_matches > 0:\n                possible_matches_by_order[order - 1] += possible_matches\n\n    precisions = [0] * max_order\n    for i in range(0, max_order):\n        if smooth:\n            precisions[i] = ((matches_by_order[i] + 1.) /\n                             (possible_matches_by_order[i] + 1.))\n        else:\n            if possible_matches_by_order[i] > 0:\n                precisions[i] = (float(matches_by_order[i]) /\n                                 possible_matches_by_order[i])\n            else:\n                precisions[i] = 0.0\n\n    if min(precisions) > 0:\n        p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n        geo_mean = math.exp(p_log_sum)\n    else:\n        geo_mean = 0\n\n    ratio = float(translation_length) / reference_length\n\n    if ratio > 1.0:\n        bp = 1.\n    else:\n        bp = math.exp(1 - 1. / ratio)\n\n    bleu = geo_mean * bp\n\n    return (bleu, precisions, bp, ratio, translation_length, reference_length)\n'"
deeppavlov/metrics/log_loss.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom typing import List, Union\n\nimport numpy as np\nfrom sklearn.metrics import log_loss\n\nfrom deeppavlov.core.common.metrics_registry import register_metric\n\n\n@register_metric(\'log_loss\')\ndef sk_log_loss(y_true: Union[List[List[float]], List[List[int]], np.ndarray],\n                y_predicted: Union[List[List[float]], List[List[int]], np.ndarray]) -> float:\n    """"""\n    Calculates log loss.\n\n    Args:\n        y_true: list or array of true values\n        y_predicted: list or array of predicted values\n\n    Returns:\n        Log loss\n    """"""\n    return log_loss(y_true, y_predicted)\n'"
deeppavlov/metrics/recall_at_k.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom typing import List\n\nimport numpy as np\n\nfrom deeppavlov.core.common.metrics_registry import register_metric\n\n\ndef recall_at_k(y_true: List[int], y_pred: List[List[np.ndarray]], k: int):\n    """"""\n    Calculates recall at k ranking metric.\n\n    Args:\n        y_true: Labels. Not used in the calculation of the metric.\n        y_predicted: Predictions.\n            Each prediction contains ranking score of all ranking candidates for the particular data sample.\n            It is supposed that the ranking score for the true candidate goes first in the prediction.\n\n    Returns:\n        Recall at k\n    """"""\n    num_examples = float(len(y_pred))\n    predictions = np.array(y_pred)\n    predictions = np.flip(np.argsort(predictions, -1), -1)[:, :k]\n    num_correct = 0\n    for el in predictions:\n        if 0 in el:\n            num_correct += 1\n    return float(num_correct) / num_examples\n\n\n@register_metric(\'r@1\')\ndef r_at_1(y_true, y_pred):\n    return recall_at_k(y_true, y_pred, k=1)\n\n\n@register_metric(\'r@2\')\ndef r_at_2(y_true, y_pred):\n    return recall_at_k(y_true, y_pred, k=2)\n\n\n@register_metric(\'r@5\')\ndef r_at_5(labels, predictions):\n    return recall_at_k(labels, predictions, k=5)\n\n\n@register_metric(\'r@10\')\ndef r_at_10(labels, predictions):\n    return recall_at_k(labels, predictions, k=10)\n'"
deeppavlov/metrics/roc_auc_score.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom typing import List, Union\n\nimport numpy as np\nimport sklearn.metrics\n\nfrom deeppavlov.core.common.metrics_registry import register_metric\n\n\n@register_metric(\'roc_auc\')\ndef roc_auc_score(y_true: Union[List[List[float]], List[List[int]], np.ndarray],\n                  y_pred: Union[List[List[float]], List[List[int]], np.ndarray]) -> float:\n    """"""\n    Compute Area Under the Curve (AUC) from prediction scores.\n\n    Args:\n        y_true: true binary labels\n        y_pred: target scores, can either be probability estimates of the positive class\n\n    Returns:\n        Area Under the Curve (AUC) from prediction scores\n    """"""\n    try:\n        return sklearn.metrics.roc_auc_score(np.squeeze(np.array(y_true)),\n                                             np.squeeze(np.array(y_pred)), average=""macro"")\n    except ValueError:\n        return 0.\n'"
deeppavlov/metrics/squad_metrics.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\nimport string\nfrom collections import Counter\nfrom typing import List\n\nfrom deeppavlov.core.common.metrics_registry import register_metric\n\n\n@register_metric(\'squad_v2_em\')\ndef squad_v2_exact_match(y_true: List[List[str]], y_predicted: List[str]) -> float:\n    """""" Calculates Exact Match score between y_true and y_predicted\n        EM score uses the best matching y_true answer:\n            if y_pred equal at least to one answer in y_true then EM = 1, else EM = 0\n\n    The same as in SQuAD-v2.0\n\n    Args:\n        y_true: list of correct answers (correct answers are represented by list of strings)\n        y_predicted: list of predicted answers\n\n    Returns:\n        exact match score : float\n    """"""\n    EM_total = sum(normalize_answer(prediction) in map(normalize_answer, ground_truth)\n                   for ground_truth, prediction in zip(y_true, y_predicted))\n    return 100 * EM_total / len(y_true) if len(y_true) > 0 else 0\n\n\n@register_metric(\'squad_v1_em\')\ndef squad_v1_exact_match(y_true: List[List[str]], y_predicted: List[str]) -> float:\n    """""" Calculates Exact Match score between y_true and y_predicted\n        EM score uses the best matching y_true answer:\n            if y_pred equal at least to one answer in y_true then EM = 1, else EM = 0\n        Skips examples without an answer.\n    Args:\n        y_true: list of correct answers (correct answers are represented by list of strings)\n        y_predicted: list of predicted answers\n    Returns:\n        exact match score : float\n    """"""\n    EM_total = 0\n    count = 0\n    for ground_truth, prediction in zip(y_true, y_predicted):\n        if len(ground_truth[0]) == 0:\n            # skip empty answers\n            continue\n        count += 1\n        EMs = [int(normalize_answer(gt) == normalize_answer(prediction)) for gt in ground_truth]\n        EM_total += max(EMs)\n    return 100 * EM_total / count if count > 0 else 0\n\n\n@register_metric(\'squad_v2_f1\')\ndef squad_v2_f1(y_true: List[List[str]], y_predicted: List[str]) -> float:\n    """""" Calculates F-1 score between y_true and y_predicted\n        F-1 score uses the best matching y_true answer\n\n    The same as in SQuAD-v2.0\n\n    Args:\n        y_true: list of correct answers (correct answers are represented by list of strings)\n        y_predicted: list of predicted answers\n\n    Returns:\n        F-1 score : float\n    """"""\n    f1_total = 0.0\n    for ground_truth, prediction in zip(y_true, y_predicted):\n        prediction_tokens = normalize_answer(prediction).split()\n        f1s = []\n        for gt in ground_truth:\n            gt_tokens = normalize_answer(gt).split()\n            if len(gt_tokens) == 0 or len(prediction_tokens) == 0:\n                f1s.append(float(gt_tokens == prediction_tokens))\n                continue\n            common = Counter(prediction_tokens) & Counter(gt_tokens)\n            num_same = sum(common.values())\n            if num_same == 0:\n                f1s.append(0.0)\n                continue\n            precision = 1.0 * num_same / len(prediction_tokens)\n            recall = 1.0 * num_same / len(gt_tokens)\n            f1 = (2 * precision * recall) / (precision + recall)\n            f1s.append(f1)\n        f1_total += max(f1s)\n    return 100 * f1_total / len(y_true) if len(y_true) > 0 else 0\n\n\n@register_metric(\'squad_v1_f1\')\ndef squad_v1_f1(y_true: List[List[str]], y_predicted: List[str]) -> float:\n    """""" Calculates F-1 score between y_true and y_predicted\n        F-1 score uses the best matching y_true answer\n\n        Skips examples without an answer.\n    Args:\n        y_true: list of correct answers (correct answers are represented by list of strings)\n        y_predicted: list of predicted answers\n    Returns:\n        F-1 score : float\n    """"""\n    f1_total = 0.0\n    count = 0\n    for ground_truth, prediction in zip(y_true, y_predicted):\n        if len(ground_truth[0]) == 0:\n            # skip empty answers\n            continue\n        count += 1\n        prediction_tokens = normalize_answer(prediction).split()\n        f1s = []\n        for gt in ground_truth:\n            gt_tokens = normalize_answer(gt).split()\n            common = Counter(prediction_tokens) & Counter(gt_tokens)\n            num_same = sum(common.values())\n            if num_same == 0:\n                f1s.append(0.0)\n                continue\n            precision = 1.0 * num_same / len(prediction_tokens)\n            recall = 1.0 * num_same / len(gt_tokens)\n            f1 = (2 * precision * recall) / (precision + recall)\n            f1s.append(f1)\n        f1_total += max(f1s)\n    return 100 * f1_total / count if count > 0 else 0\n\n\ndef normalize_answer(s: str) -> str:\n    def remove_articles(text):\n        return re.sub(r\'\\b(a|an|the)\\b\', \' \', text)\n\n    def white_space_fix(text):\n        return \' \'.join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \'\'.join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n'"
deeppavlov/models/__init__.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\n\nimport nltk\n\nfrom deeppavlov.core.common.prints import RedirectedPrints\n\nif not os.environ.get(\'DP_SKIP_NLTK_DOWNLOAD\'):\n    with RedirectedPrints():\n        nltk.download(\'punkt\')\n        nltk.download(\'stopwords\')\n        nltk.download(\'perluniprops\')\n        nltk.download(\'nonbreaking_prefixes\')\n'"
deeppavlov/skills/__init__.py,0,b''
deeppavlov/utils/__init__.py,0,b''
deeppavlov/vocabs/__init__.py,0,b''
deeppavlov/vocabs/typos.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport shutil\nfrom collections import defaultdict\nfrom logging import getLogger\nfrom pathlib import Path\n\nimport requests\nfrom lxml import html\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.file import load_pickle, save_pickle\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.utils import is_done, mark_done\n\nlog = getLogger(__name__)\n\n\n@register(\'static_dictionary\')\nclass StaticDictionary:\n    """"""Trie vocabulary used in spelling correction algorithms\n\n    Args:\n        data_dir: path to the directory where the built trie will be stored. Relative paths are interpreted as\n            relative to pipeline\'s data directory\n        dictionary_name: logical name of the dictionary\n        raw_dictionary_path: path to the source file with the list of words\n\n    Attributes:\n        dict_name: logical name of the dictionary\n        alphabet: set of all the characters used in this dictionary\n        words_set: set of all the words\n        words_trie: trie structure of all the words\n    """"""\n\n    def __init__(self, data_dir: [Path, str] = \'\', *args, dictionary_name: str = \'dictionary\', **kwargs):\n        data_dir = expand_path(data_dir) / dictionary_name\n\n        alphabet_path = data_dir / \'alphabet.pkl\'\n        words_path = data_dir / \'words.pkl\'\n        words_trie_path = data_dir / \'words_trie.pkl\'\n\n        if not is_done(data_dir):\n            log.info(\'Trying to build a dictionary in {}\'.format(data_dir))\n            if data_dir.is_dir():\n                shutil.rmtree(str(data_dir))\n            data_dir.mkdir(parents=True)\n\n            words = self._get_source(data_dir, *args, **kwargs)\n            words = {self._normalize(word) for word in words}\n\n            alphabet = {c for w in words for c in w}\n            alphabet.remove(\'\xe2\x9f\xac\')\n            alphabet.remove(\'\xe2\x9f\xad\')\n\n            save_pickle(alphabet, alphabet_path)\n            save_pickle(words, words_path)\n\n            words_trie = defaultdict(set)\n            for word in words:\n                for i in range(len(word)):\n                    words_trie[word[:i]].add(word[:i + 1])\n                words_trie[word] = set()\n            words_trie = {k: sorted(v) for k, v in words_trie.items()}\n\n            save_pickle(words_trie, words_trie_path)\n\n            mark_done(data_dir)\n            log.info(\'built\')\n        else:\n            log.info(\'Loading a dictionary from {}\'.format(data_dir))\n\n        self.alphabet = load_pickle(alphabet_path)\n        self.words_set = load_pickle(words_path)\n        self.words_trie = load_pickle(words_trie_path)\n\n    @staticmethod\n    def _get_source(data_dir, raw_dictionary_path, *args, **kwargs):\n        raw_path = expand_path(raw_dictionary_path)\n        with raw_path.open(newline=\'\', encoding=\'utf8\') as f:\n            data = [line.strip().split(\'\\t\')[0] for line in f]\n        return data\n\n    @staticmethod\n    def _normalize(word):\n        return \'\xe2\x9f\xac{}\xe2\x9f\xad\'.format(word.strip().lower().replace(\'\xd1\x91\', \'\xd0\xb5\'))\n\n\n@register(\'russian_words_vocab\')\nclass RussianWordsVocab(StaticDictionary):\n    """"""Implementation of :class:`~deeppavlov.vocabs.typos.StaticDictionary` that builds data from https://github.com/danakt/russian-words/\n\n    Args:\n        data_dir: path to the directory where the built trie will be stored. Relative paths are interpreted as\n            relative to pipeline\'s data directory\n\n    Attributes:\n        dict_name: logical name of the dictionary\n        alphabet: set of all the characters used in this dictionary\n        words_set: set of all the words\n        words_trie: trie structure of all the words\n    """"""\n\n    def __init__(self, data_dir: [Path, str] = \'\', *args, **kwargs):\n        kwargs[\'dictionary_name\'] = \'russian_words_vocab\'\n        super().__init__(data_dir, *args, **kwargs)\n\n    @staticmethod\n    def _get_source(*args, **kwargs):\n        log.info(\'Downloading russian vocab from https://github.com/danakt/russian-words/\')\n        url = \'https://github.com/danakt/russian-words/raw/master/russian.txt\'\n        page = requests.get(url)\n        return [word.strip() for word in page.content.decode(\'cp1251\').strip().split(\'\\n\')]\n\n\n@register(\'wikitionary_100K_vocab\')\nclass Wiki100KDictionary(StaticDictionary):\n    """"""Implementation of :class:`~deeppavlov.vocabs.typos.StaticDictionary` that builds data\n    from `Wikitionary <https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists#Project_Gutenberg>`__\n\n    Args:\n        data_dir: path to the directory where the built trie will be stored. Relative paths are interpreted as\n            relative to pipeline\'s data directory\n\n    Attributes:\n        dict_name: logical name of the dictionary\n        alphabet: set of all the characters used in this dictionary\n        words_set: set of all the words\n        words_trie: trie structure of all the words\n    """"""\n\n    def __init__(self, data_dir: [Path, str] = \'\', *args, **kwargs):\n        kwargs[\'dictionary_name\'] = \'wikipedia_100K_vocab\'\n        super().__init__(data_dir, *args, **kwargs)\n\n    @staticmethod\n    def _get_source(*args, **kwargs):\n        words = []\n        log.info(\'Downloading english vocab from Wiktionary\')\n        for i in range(1, 100000, 10000):\n            k = 10000 + i - 1\n            url = \'https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/PG/2005/08/{}-{}\'.format(i, k)\n            page = requests.get(url)\n            tree = html.fromstring(page.content)\n            words += tree.xpath(\'//div[@class=""mw-parser-output""]/p/a/text()\')\n        return words\n'"
deeppavlov/vocabs/wiki_sqlite.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import List, Any, Optional, Union\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.dataset_iterators.sqlite_iterator import SQLiteDataIterator\n\nlogger = getLogger(__name__)\n\n\n@register(\'wiki_sqlite_vocab\')\nclass WikiSQLiteVocab(SQLiteDataIterator, Component):\n    """"""Get content from SQLite database by document ids.\n\n    Args:\n        load_path: a path to local DB file\n        join_docs: whether to join extracted docs with \' \' or not\n        shuffle: whether to shuffle data or not\n\n    Attributes:\n        join_docs: whether to join extracted docs with \' \' or not\n\n    """"""\n\n    def __init__(self, load_path: str, join_docs: bool = True, shuffle: bool = False, **kwargs) -> None:\n        SQLiteDataIterator.__init__(self, load_path=load_path, shuffle=shuffle)\n        self.join_docs = join_docs\n\n    def __call__(self, doc_ids: Optional[List[List[Any]]] = None, *args, **kwargs) -> List[Union[str, List[str]]]:\n        """"""Get the contents of files, stacked by space or as they are.\n\n        Args:\n            doc_ids: a batch of lists of ids to get contents for\n\n        Returns:\n            a list of contents / list of lists of contents\n        """"""\n        all_contents = []\n        if not doc_ids:\n            logger.warning(\'No doc_ids are provided in WikiSqliteVocab, return all docs\')\n            doc_ids = [self.get_doc_ids()]\n\n        for ids in doc_ids:\n            contents = [self.get_doc_content(doc_id) for doc_id in ids]\n            if self.join_docs:\n                contents = \' \'.join(contents)\n            all_contents.append(contents)\n\n        return all_contents\n'"
utils/prepare/__init__.py,0,b''
utils/prepare/hashes.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport gzip\nimport sys\nimport tarfile\nfrom hashlib import md5\nfrom pathlib import Path\nfrom typing import List, Dict, Union\nfrom zipfile import ZipFile\n\nfrom deeppavlov.core.data.utils import file_md5\n\n\ndef tar_md5(fpath: Union[str, Path], chunk_size: int = 2 ** 16) -> Dict[str, str]:\n    tar = tarfile.open(fpath)\n    res = {}\n    while True:\n        item: tarfile.TarInfo = tar.next()\n        if item is None:\n            break\n        if not item.isfile():\n            continue\n        file_hash = md5()\n        with tar.extractfile(item) as f:\n            for chunk in iter(lambda: f.read(chunk_size), b""""):\n                file_hash.update(chunk)\n        res[item.name] = file_hash.hexdigest()\n    return res\n\n\ndef gzip_md5(fpath: Union[str, Path], chunk_size: int = 2 ** 16) -> str:\n    file_hash = md5()\n    with gzip.open(fpath, \'rb\') as f:\n        for chunk in iter(lambda: f.read(chunk_size), b""""):\n            file_hash.update(chunk)\n    return file_hash.hexdigest()\n\n\ndef zip_md5(fpath: Union[str, Path], chunk_size: int = 2 ** 16) -> Dict[str, str]:\n    res = {}\n    with ZipFile(fpath) as zip_f:\n        for item in zip_f.infolist():\n            if item.is_dir():\n                continue\n            file_hash = md5()\n            with zip_f.open(item) as f:\n                for chunk in iter(lambda: f.read(chunk_size), b""""):\n                    file_hash.update(chunk)\n            res[item.filename] = file_hash.hexdigest()\n    return res\n\n\ndef compute_hashes(fpath: Union[str, Path]) -> Dict[str, str]:\n    p = Path(fpath).expanduser()\n    if not p.is_file():\n        raise RuntimeError(f\'{p} is not a file\')\n\n    if \'.tar\' in {s.lower() for s in p.suffixes}:\n        hashes = tar_md5(p)\n    elif p.suffix.lower() == \'.gz\':\n        hashes = {p.with_suffix(\'\').name: gzip_md5(p)}\n    elif p.suffix.lower() == \'.zip\':\n        hashes = zip_md5(p)\n    else:\n        hashes = {p.name: file_md5(p)}\n    return hashes\n\n\ndef main(args: List[str] = None) -> None:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""fname"", help=""path to a file to compute hash for"", type=str)\n    parser.add_argument(\'-o\', \'--outfile\', help=\'where to write the hashes\', default=None, type=str)\n\n    args = parser.parse_args(args)\n\n    p = Path(args.fname).expanduser()\n    hashes = compute_hashes(p)\n\n    outfile = args.outfile\n    if outfile is None:\n        outfile = p.with_suffix(p.suffix + \'.md5\').open(\'w\', encoding=\'utf-8\')\n    elif outfile == \'-\':\n        outfile = sys.stdout\n    else:\n        outfile = Path(outfile).expanduser().open(\'w\', encoding=\'utf-8\')\n\n    for fname, fhash in hashes.items():\n        print(f\'{fhash} *{fname}\', file=outfile, flush=True)\n\n    if outfile is not sys.stdout:\n        outfile.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
utils/prepare/registry.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport pkgutil\nfrom importlib import import_module, reload\n\nimport deeppavlov\nfrom deeppavlov.core.common.metrics_registry import _registry_path as m_registry_path, _REGISTRY as M_REGISTRY\nfrom deeppavlov.core.common.registry import _registry_path as c_registry_path, _REGISTRY as C_REGISTRY\n\nif __name__ == \'__main__\':\n    C_REGISTRY.clear()\n    M_REGISTRY.clear()\n\n    for _, pkg_name, _ in pkgutil.walk_packages(deeppavlov.__path__, deeppavlov.__name__ + \'.\'):\n        if pkg_name not in (\'deeppavlov.core.common.registry\', \'deeppavlov.core.common.metrics_registry\'):\n            reload(import_module(pkg_name))\n\n    with c_registry_path.open(\'w\', encoding=\'utf-8\') as f:\n        json.dump(dict(sorted(C_REGISTRY.items())), f, indent=2)\n\n    with m_registry_path.open(\'w\', encoding=\'utf-8\') as f:\n        json.dump(dict(sorted(M_REGISTRY.items())), f, indent=2)\n'"
deeppavlov/core/commands/__init__.py,0,b''
deeppavlov/core/commands/infer.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport json\nimport pickle\nimport sys\nfrom itertools import islice\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import Optional, Union\n\nfrom deeppavlov.core.commands.utils import import_packages, parse_config\nfrom deeppavlov.core.common.chainer import Chainer\nfrom deeppavlov.core.common.params import from_params\nfrom deeppavlov.core.data.utils import jsonify_data\nfrom deeppavlov.download import deep_download\n\nlog = getLogger(__name__)\n\n\ndef build_model(config: Union[str, Path, dict], mode: str = \'infer\',\n                load_trained: bool = False, download: bool = False,\n                serialized: Optional[bytes] = None) -> Chainer:\n    """"""Build and return the model described in corresponding configuration file.""""""\n    config = parse_config(config)\n\n    if serialized:\n        serialized: list = pickle.loads(serialized)\n\n    if download:\n        deep_download(config)\n\n    import_packages(config.get(\'metadata\', {}).get(\'imports\', []))\n\n    model_config = config[\'chainer\']\n\n    model = Chainer(model_config[\'in\'], model_config[\'out\'], model_config.get(\'in_y\'))\n\n    for component_config in model_config[\'pipe\']:\n        if load_trained and (\'fit_on\' in component_config or \'in_y\' in component_config):\n            try:\n                component_config[\'load_path\'] = component_config[\'save_path\']\n            except KeyError:\n                log.warning(\'No ""save_path"" parameter for the {} component, so ""load_path"" will not be renewed\'\n                            .format(component_config.get(\'class_name\', component_config.get(\'ref\', \'UNKNOWN\'))))\n\n        if serialized and \'in\' in component_config:\n            component_serialized = serialized.pop(0)\n        else:\n            component_serialized = None\n\n        component = from_params(component_config, mode=mode, serialized=component_serialized)\n\n        if \'id\' in component_config:\n            model._components_dict[component_config[\'id\']] = component\n\n        if \'in\' in component_config:\n            c_in = component_config[\'in\']\n            c_out = component_config[\'out\']\n            in_y = component_config.get(\'in_y\', None)\n            main = component_config.get(\'main\', False)\n            model.append(component, c_in, c_out, in_y, main)\n\n    return model\n\n\ndef interact_model(config: Union[str, Path, dict]) -> None:\n    """"""Start interaction with the model described in corresponding configuration file.""""""\n    model = build_model(config)\n\n    while True:\n        args = []\n        for in_x in model.in_x:\n            args.append((input(\'{}::\'.format(in_x)),))\n            # check for exit command\n            if args[-1][0] in {\'exit\', \'stop\', \'quit\', \'q\'}:\n                return\n\n        pred = model(*args)\n        if len(model.out_params) > 1:\n            pred = zip(*pred)\n\n        print(\'>>\', *pred)\n\n\ndef predict_on_stream(config: Union[str, Path, dict],\n                      batch_size: Optional[int] = None,\n                      file_path: Optional[str] = None) -> None:\n    """"""Make a prediction with the component described in corresponding configuration file.""""""\n\n    batch_size = batch_size or 1\n    if file_path is None or file_path == \'-\':\n        if sys.stdin.isatty():\n            raise RuntimeError(\'To process data from terminal please use interact mode\')\n        f = sys.stdin\n    else:\n        f = open(file_path, encoding=\'utf8\')\n\n    model: Chainer = build_model(config)\n\n    args_count = len(model.in_x)\n    while True:\n        batch = list((l.strip() for l in islice(f, batch_size * args_count)))\n\n        if not batch:\n            break\n\n        args = []\n        for i in range(args_count):\n            args.append(batch[i::args_count])\n\n        res = model(*args)\n        if len(model.out_params) == 1:\n            res = [res]\n        for res in zip(*res):\n            res = json.dumps(jsonify_data(res), ensure_ascii=False)\n            print(res, flush=True)\n\n    if f is not sys.stdin:\n        f.close()\n'"
deeppavlov/core/commands/train.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import Dict, Union, Optional, Iterable\n\nfrom deeppavlov.core.commands.utils import expand_path, import_packages, parse_config\nfrom deeppavlov.core.common.errors import ConfigError\nfrom deeppavlov.core.common.params import from_params\nfrom deeppavlov.core.common.registry import get_model\nfrom deeppavlov.core.data.data_fitting_iterator import DataFittingIterator\nfrom deeppavlov.core.data.data_learning_iterator import DataLearningIterator\nfrom deeppavlov.core.data.utils import get_all_elems_from_json\nfrom deeppavlov.download import deep_download\n\nlog = getLogger(__name__)\n\n\ndef read_data_by_config(config: dict):\n    """"""Read data by dataset_reader from specified config.""""""\n    dataset_config = config.get(\'dataset\', None)\n\n    if dataset_config:\n        config.pop(\'dataset\')\n        ds_type = dataset_config[\'type\']\n        if ds_type == \'classification\':\n            reader = {\'class_name\': \'basic_classification_reader\'}\n            iterator = {\'class_name\': \'basic_classification_iterator\'}\n            config[\'dataset_reader\'] = {**dataset_config, **reader}\n            config[\'dataset_iterator\'] = {**dataset_config, **iterator}\n        else:\n            raise Exception(""Unsupported dataset type: {}"".format(ds_type))\n\n    try:\n        reader_config = dict(config[\'dataset_reader\'])\n    except KeyError:\n        raise ConfigError(""No dataset reader is provided in the JSON config."")\n\n    reader = get_model(reader_config.pop(\'class_name\'))()\n    data_path = reader_config.pop(\'data_path\', \'\')\n    if isinstance(data_path, list):\n        data_path = [expand_path(x) for x in data_path]\n    else:\n        data_path = expand_path(data_path)\n\n    return reader.read(data_path, **reader_config)\n\n\ndef get_iterator_from_config(config: dict, data: dict):\n    """"""Create iterator (from config) for specified data.""""""\n    iterator_config = config[\'dataset_iterator\']\n    iterator: Union[DataLearningIterator, DataFittingIterator] = from_params(iterator_config,\n                                                                             data=data)\n    return iterator\n\n\ndef train_evaluate_model_from_config(config: Union[str, Path, dict],\n                                     iterator: Union[DataLearningIterator, DataFittingIterator] = None, *,\n                                     to_train: bool = True,\n                                     evaluation_targets: Optional[Iterable[str]] = None,\n                                     to_validate: Optional[bool] = None,\n                                     download: bool = False,\n                                     start_epoch_num: Optional[int] = None,\n                                     recursive: bool = False) -> Dict[str, Dict[str, float]]:\n    """"""Make training and evaluation of the model described in corresponding configuration file.""""""\n    config = parse_config(config)\n\n    if download:\n        deep_download(config)\n\n    if to_train and recursive:\n        for subconfig in get_all_elems_from_json(config[\'chainer\'], \'config_path\'):\n            log.info(f\'Training ""{subconfig}""\')\n            train_evaluate_model_from_config(subconfig, download=False, recursive=True)\n\n    import_packages(config.get(\'metadata\', {}).get(\'imports\', []))\n\n    if iterator is None:\n        try:\n            data = read_data_by_config(config)\n        except ConfigError as e:\n            to_train = False\n            log.warning(f\'Skipping training. {e.message}\')\n        else:\n            iterator = get_iterator_from_config(config, data)\n\n    if \'train\' not in config:\n        log.warning(\'Train config is missing. Populating with default values\')\n    train_config = config.get(\'train\')\n\n    if start_epoch_num is not None:\n        train_config[\'start_epoch_num\'] = start_epoch_num\n\n    if \'evaluation_targets\' not in train_config and (\'validate_best\' in train_config\n                                                     or \'test_best\' in train_config):\n        log.warning(\'""validate_best"" and ""test_best"" parameters are deprecated.\'\n                    \' Please, use ""evaluation_targets"" list instead\')\n\n        train_config[\'evaluation_targets\'] = []\n        if train_config.pop(\'validate_best\', True):\n            train_config[\'evaluation_targets\'].append(\'valid\')\n        if train_config.pop(\'test_best\', True):\n            train_config[\'evaluation_targets\'].append(\'test\')\n\n    trainer_class = get_model(train_config.pop(\'class_name\', \'nn_trainer\'))\n    trainer = trainer_class(config[\'chainer\'], **train_config)\n\n    if to_train:\n        trainer.train(iterator)\n\n    res = {}\n\n    if iterator is not None:\n        if to_validate is not None:\n            if evaluation_targets is None:\n                log.warning(\'""to_validate"" parameter is deprecated and will be removed in future versions.\'\n                            \' Please, use ""evaluation_targets"" list instead\')\n                evaluation_targets = [\'test\']\n                if to_validate:\n                    evaluation_targets.append(\'valid\')\n            else:\n                log.warning(\'Both ""evaluation_targets"" and ""to_validate"" parameters are specified.\'\n                            \' ""to_validate"" is deprecated and will be ignored\')\n\n        res = trainer.evaluate(iterator, evaluation_targets, print_reports=True)\n        trainer.get_chainer().destroy()\n\n    res = {k: v[\'metrics\'] for k, v in res.items()}\n\n    return res\n'"
deeppavlov/core/commands/utils.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nfrom pathlib import Path\nfrom typing import Union, Dict, TypeVar\n\nfrom deeppavlov.core.common.file import read_json, find_config\n\n# noinspection PyShadowingBuiltins\n_T = TypeVar(\'_T\', str, float, bool, list, dict)\n\n\ndef _parse_config_property(item: _T, variables: Dict[str, Union[str, Path, float, bool, int, None]],\n                           variables_exact: Dict[str, Union[str, Path, float, bool, int, None]]) -> _T:\n    """"""Recursively apply config\'s variables values to its property""""""\n    if isinstance(item, str):\n        if item in variables_exact:\n            return variables_exact[item]\n        return item.format(**variables)\n    elif isinstance(item, list):\n        return [_parse_config_property(item, variables, variables_exact) for item in item]\n    elif isinstance(item, dict):\n        return {k: _parse_config_property(v, variables, variables_exact) for k, v in item.items()}\n    else:\n        return item\n\n\ndef _get_variables_from_config(config: Union[str, Path, dict]):\n    """"""Read config\'s variables""""""\n    if isinstance(config, (str, Path)):\n        config = read_json(find_config(config))\n\n    variables = {\n        \'DEEPPAVLOV_PATH\': os.getenv(f\'DP_DEEPPAVLOV_PATH\', Path(__file__).parent.parent.parent)\n    }\n    variables_exact = {f\'{{{k}}}\': v for k, v in variables.items()}\n    for name, value in config.get(\'metadata\', {}).get(\'variables\', {}).items():\n        env_name = f\'DP_{name}\'\n        if env_name in os.environ:\n            value = os.getenv(env_name)\n        if value in variables_exact:\n            value = variables_exact[value]\n        elif isinstance(value, str):\n            value = value.format(**variables)\n        variables[name] = value\n        variables_exact[f\'{{{name}}}\'] = value\n\n    return variables, variables_exact\n\n\ndef parse_config(config: Union[str, Path, dict]) -> dict:\n    """"""Apply variables\' values to all its properties""""""\n    if isinstance(config, (str, Path)):\n        config = read_json(find_config(config))\n\n    variables, variables_exact = _get_variables_from_config(config)\n\n    return _parse_config_property(config, variables, variables_exact)\n\n\ndef expand_path(path: Union[str, Path]) -> Path:\n    """"""Convert relative paths to absolute with resolving user directory.""""""\n    return Path(path).expanduser().resolve()\n\n\ndef import_packages(packages: list) -> None:\n    """"""Import packages from list to execute their code.""""""\n    for package in packages:\n        __import__(package)\n\n\ndef parse_value_with_config(value: Union[str, Path], config: Union[str, Path, dict]) -> Path:\n    """"""Fill the variables in `value` with variables values from `config`.\n    `value` should be a string. If `value` is a string of only variable, `value` will be replaced with\n    variable\'s value from config (the variable\'s value could be anything then).""""""\n    variables, variables_exact = _get_variables_from_config(config)\n\n    return _parse_config_property(str(value), variables, variables_exact)\n'"
deeppavlov/core/common/__init__.py,0,b''
deeppavlov/core/common/chainer.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pickle\nfrom itertools import islice\nfrom logging import getLogger\nfrom types import FunctionType\nfrom typing import Union, Tuple, List, Optional, Hashable, Reversible\n\nfrom deeppavlov.core.common.errors import ConfigError\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.core.models.nn_model import NNModel\nfrom deeppavlov.core.models.serializable import Serializable\n\nlog = getLogger(__name__)\n\n\nclass Chainer(Component):\n    """"""\n    Builds a component pipeline from heterogeneous components (Rule-based/ML/DL). It allows to train\n    and infer models in a pipeline as a whole.\n\n    Attributes:\n        pipe: list of components and their input and output variable names for inference\n        train_pipe: list of components and their input and output variable names for training and evaluation\n        in_x: names of inputs for pipeline inference mode\n        out_params: names of pipeline inference outputs\n        in_y: names of additional inputs for pipeline training and evaluation modes\n        forward_map: list of all variables in chainer\'s memory after  running every component in ``self.pipe``\n        train_map: list of all variables in chainer\'s memory after  running every component in ``train_pipe.pipe``\n        main: reference to the main component\n\n    Args:\n        in_x: names of inputs for pipeline inference mode\n        out_params: names of pipeline inference outputs\n        in_y: names of additional inputs for pipeline training and evaluation modes\n    """"""\n\n    def __init__(self, in_x: Union[str, list] = None, out_params: Union[str, list] = None,\n                 in_y: Union[str, list] = None, *args, **kwargs) -> None:\n        self.pipe: List[Tuple[Tuple[List[str], List[str]], List[str], Component]] = []\n        self.train_pipe = []\n        if isinstance(in_x, str):\n            in_x = [in_x]\n        if isinstance(in_y, str):\n            in_y = [in_y]\n        if isinstance(out_params, str):\n            out_params = [out_params]\n        self.in_x = in_x or [\'x\']\n        self.in_y = in_y or [\'y\']\n        self.out_params = out_params or self.in_x\n\n        self.forward_map = set(self.in_x)\n        self.train_map = self.forward_map.union(self.in_y)\n\n        self._components_dict = {}\n\n        self.main = None\n\n    def __getitem__(self, item):\n        if isinstance(item, int):\n            in_params, out_params, component = self.train_pipe[item]\n            return component\n        return self._components_dict[item]\n\n    def _ipython_key_completions_(self):\n        return self._components_dict.keys()\n\n    def __repr__(self):\n        reversed_components_dict = {v: f\'{repr(k)}: \' for k, v in self._components_dict.items()\n                                    if isinstance(v, Hashable)}\n\n        components_list = []\n        for in_params, out_params, component in self.train_pipe:\n            component_repr = repr(component)\n            if isinstance(component, Hashable) and component in reversed_components_dict:\n                component_repr = reversed_components_dict[component] + component_repr\n            else:\n                for k, v in self._components_dict.items():\n                    if v is component:\n                        component_repr = f\'{k}: {component_repr}\'\n                        break\n            components_list.append(component_repr)\n\n        return f\'Chainer[{"", "".join(components_list)}]\'\n\n    def _repr_pretty_(self, p, cycle):\n        """"""method that defines ``Struct``\'s pretty printing rules for iPython\n\n        Args:\n            p (IPython.lib.pretty.RepresentationPrinter): pretty printer object\n            cycle (bool): is ``True`` if pretty detected a cycle\n        """"""\n        if cycle:\n            p.text(\'Chainer(...)\')\n        else:\n            with p.group(8, \'Chainer[\', \']\'):\n                reversed_components_dict = {v: k for k, v in self._components_dict.items()\n                                            if isinstance(v, Hashable)}\n                # p.pretty(self.__prepare_repr())\n                for i, (in_params, out_params, component) in enumerate(self.train_pipe):\n                    if i > 0:\n                        p.text(\',\')\n                        p.breakable()\n                    if isinstance(component, Hashable) and component in reversed_components_dict:\n                        p.pretty(reversed_components_dict[component])\n                        p.text(\': \')\n                    else:\n                        for k, v in self._components_dict.items():\n                            if v is component:\n                                p.pretty(k)\n                                p.text(\': \')\n                                break\n                    p.pretty(component)\n\n    def append(self, component: Union[Component, FunctionType], in_x: [str, list, dict] = None,\n               out_params: [str, list] = None, in_y: [str, list, dict] = None, main: bool = False):\n        if isinstance(in_x, str):\n            in_x = [in_x]\n        if isinstance(in_y, str):\n            in_y = [in_y]\n        if isinstance(out_params, str):\n            out_params = [out_params]\n        in_x = in_x or self.in_x\n\n        if isinstance(in_x, dict):\n            x_keys, in_x = zip(*in_x.items())\n        else:\n            x_keys = []\n        out_params = out_params or in_x\n        if in_y is not None:\n            if isinstance(in_y, dict):\n                y_keys, in_y = zip(*in_y.items())\n            else:\n                y_keys = []\n            keys = x_keys + y_keys\n\n            if bool(x_keys) != bool(y_keys):\n                raise ConfigError(\'`in` and `in_y` for a component have to both be lists or dicts\')\n\n            component: NNModel\n            main = True\n            assert self.train_map.issuperset(in_x + in_y), (\'Arguments {} are expected but only {} are set\'\n                                                            .format(in_x + in_y, self.train_map))\n            preprocessor = Chainer(self.in_x, in_x + in_y, self.in_y)\n            for (t_in_x_keys, t_in_x), t_out, t_component in self.train_pipe:\n                if t_in_x_keys:\n                    t_in_x = dict(zip(t_in_x_keys, t_in_x))\n                preprocessor.append(t_component, t_in_x, t_out)\n\n            def train_on_batch(*args, **kwargs):\n                preprocessed = preprocessor.compute(*args, **kwargs)\n                if len(in_x + in_y) == 1:\n                    preprocessed = [preprocessed]\n                if keys:\n                    return component.train_on_batch(**dict(zip(keys, preprocessed)))\n                else:\n                    return component.train_on_batch(*preprocessed)\n\n            self.train_on_batch = train_on_batch\n            self.process_event = component.process_event\n        if main:\n            self.main = component\n        if self.forward_map.issuperset(in_x):\n            self.pipe.append(((x_keys, in_x), out_params, component))\n            self.forward_map = self.forward_map.union(out_params)\n\n        if self.train_map.issuperset(in_x):\n            self.train_pipe.append(((x_keys, in_x), out_params, component))\n            self.train_map = self.train_map.union(out_params)\n        else:\n            raise ConfigError(\'Arguments {} are expected but only {} are set\'.format(in_x, self.train_map))\n\n    def compute(self, x, y=None, targets=None):\n        if targets is None:\n            targets = self.out_params\n        in_params = list(self.in_x)\n        if len(in_params) == 1:\n            args = [x]\n        else:\n            args = list(zip(*x))\n\n        if y is None:\n            pipe = self.pipe\n        else:\n            pipe = self.train_pipe\n            if len(self.in_y) == 1:\n                args.append(y)\n            else:\n                args += list(zip(*y))\n            in_params += self.in_y\n\n        return self._compute(*args, pipe=pipe, param_names=in_params, targets=targets)\n\n    def __call__(self, *args):\n        return self._compute(*args, param_names=self.in_x, pipe=self.pipe, targets=self.out_params)\n\n    @staticmethod\n    def _compute(*args, param_names, pipe, targets):\n        expected = set(targets)\n        final_pipe = []\n        for (in_keys, in_params), out_params, component in reversed(pipe):\n            if expected.intersection(out_params):\n                expected = expected - set(out_params) | set(in_params)\n                final_pipe.append(((in_keys, in_params), out_params, component))\n        final_pipe.reverse()\n        if not expected.issubset(param_names):\n            raise RuntimeError(f\'{expected} are required to compute {targets} but were not found in memory or inputs\')\n        pipe = final_pipe\n\n        mem = dict(zip(param_names, args))\n        del args\n\n        for (in_keys, in_params), out_params, component in pipe:\n            x = [mem[k] for k in in_params]\n            if in_keys:\n                res = component.__call__(**dict(zip(in_keys, x)))\n            else:\n                res = component.__call__(*x)\n            if len(out_params) == 1:\n                mem[out_params[0]] = res\n            else:\n                mem.update(zip(out_params, res))\n\n        res = [mem[k] for k in targets]\n        if len(res) == 1:\n            res = res[0]\n        return res\n\n    def batched_call(self, *args: Reversible, batch_size: int = 16) -> Union[list, Tuple[list, ...]]:\n        """"""\n        Partitions data into mini-batches and applies :meth:`__call__` to each batch.\n\n        Args:\n            args: input data, each element of the data corresponds to a single model inputs sequence.\n            batch_size: the size of a batch.\n\n        Returns:\n            the model output as if the data was passed to the :meth:`__call__` method.\n        """"""\n        args = [iter(arg) for arg in args]\n        answer = [[] for _ in self.out_params]\n\n        while True:\n            batch = [list(islice(arg, batch_size)) for arg in args]\n            if not any(batch):  # empty batch, reached the end\n                break\n\n            curr_answer = self.__call__(*batch)\n            if len(self.out_params) == 1:\n                curr_answer = [curr_answer]\n\n            for y, curr_y in zip(answer, curr_answer):\n                y.extend(curr_y)\n\n        if len(self.out_params) == 1:\n            answer = answer[0]\n        return answer\n\n    def get_main_component(self) -> Optional[Serializable]:\n        try:\n            return self.main or self.pipe[-1][-1]\n        except IndexError:\n            log.warning(\'Cannot get a main component for an empty chainer\')\n            return None\n\n    def save(self) -> None:\n        main_component = self.get_main_component()\n        if isinstance(main_component, Serializable):\n            main_component.save()\n\n    def load(self) -> None:\n        for in_params, out_params, component in self.train_pipe:\n            if callable(getattr(component, \'load\', None)):\n                component.load()\n\n    def reset(self) -> None:\n        for in_params, out_params, component in self.train_pipe:\n            if callable(getattr(component, \'reset\', None)):\n                component.reset()\n\n    def destroy(self):\n        if hasattr(self, \'train_pipe\'):\n            for in_params, out_params, component in self.train_pipe:\n                if callable(getattr(component, \'destroy\', None)):\n                    component.destroy()\n            self.train_pipe.clear()\n        if hasattr(self, \'pipe\'):\n            self.pipe.clear()\n        super().destroy()\n\n    def serialize(self) -> bytes:\n        data = []\n        for in_params, out_params, component in self.train_pipe:\n            serialized = component.serialize() if isinstance(component, Component) else None\n            data.append(serialized)\n        return pickle.dumps(data, protocol=4)\n\n    def deserialize(self, data: bytes) -> None:\n        data = pickle.loads(data)\n        for (in_params, out_params, component), component_data in zip(self.train_pipe, data):\n            if isinstance(component, Component):\n                component.deserialize(component_data)\n'"
deeppavlov/core/common/check_gpu.py,2,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\n\nimport tensorflow as tf\nfrom tensorflow.python.client import device_lib\n\nlog = getLogger(__name__)\n\n_gpu_available = None\n\n\ndef check_gpu_existence():\n    r""""""Return True if at least one GPU is available""""""\n    global _gpu_available\n    if _gpu_available is None:\n        sess_config = tf.ConfigProto()\n        sess_config.gpu_options.allow_growth = True\n        try:\n            with tf.Session(config=sess_config):\n                device_list = device_lib.list_local_devices()\n                _gpu_available = any(device.device_type == \'GPU\' for device in device_list)\n        except AttributeError as e:\n            log.warning(f\'Got an AttributeError `{e}`, assuming documentation building\')\n            _gpu_available = False\n    return _gpu_available\n'"
deeppavlov/core/common/cross_validation.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport shutil\nfrom collections import OrderedDict\nfrom logging import getLogger\nfrom pathlib import Path\n\nimport numpy as np\nfrom sklearn.model_selection import KFold\n\nfrom deeppavlov.core.commands.train import train_evaluate_model_from_config, get_iterator_from_config, \\\n    read_data_by_config\nfrom deeppavlov.core.commands.utils import expand_path, parse_config\nfrom deeppavlov.core.common.params_search import ParamsSearch\n\nSAVE_PATH_ELEMENT_NAME = \'save_path\'\nTEMP_DIR_FOR_CV = \'cv_tmp\'\nlog = getLogger(__name__)\n\n\ndef change_savepath_for_model(config):\n    params_helper = ParamsSearch()\n\n    dirs_for_saved_models = set()\n    for p in params_helper.find_model_path(config, SAVE_PATH_ELEMENT_NAME):\n        p.append(SAVE_PATH_ELEMENT_NAME)\n        save_path = Path(params_helper.get_value_from_config(config, p))\n        new_save_path = save_path.parent / TEMP_DIR_FOR_CV / save_path.name\n\n        dirs_for_saved_models.add(expand_path(new_save_path.parent))\n\n        params_helper.insert_value_or_dict_into_config(config, p, str(new_save_path))\n\n    return config, dirs_for_saved_models\n\n\ndef delete_dir_for_saved_models(dirs_for_saved_models):\n    for new_save_dir in dirs_for_saved_models:\n        shutil.rmtree(str(new_save_dir))\n\n\ndef create_dirs_to_save_models(dirs_for_saved_models):\n    for new_save_dir in dirs_for_saved_models:\n        new_save_dir.mkdir(exist_ok=True, parents=True)\n\n\ndef generate_train_valid(data, n_folds=5, is_loo=False):\n    all_data = data[\'train\'] + data[\'valid\']\n\n    if is_loo:\n        # for Leave One Out\n        for i in range(len(all_data)):\n            data_i = {\n                \'train\': all_data.copy(),\n                \'test\': data[\'test\']\n            }\n            data_i[\'valid\'] = [data_i[\'train\'].pop(i)]\n\n            yield data_i\n    else:\n        # for Cross Validation\n        kf = KFold(n_splits=n_folds, shuffle=True)\n        for train_index, valid_index in kf.split(all_data):\n            data_i = {\n                \'train\': [all_data[i] for i in train_index],\n                \'valid\': [all_data[i] for i in valid_index],\n                \'test\': data[\'test\']\n            }\n\n            yield data_i\n\n\ndef calc_cv_score(config, data=None, n_folds=5, is_loo=False):\n    config = parse_config(config)\n\n    if data is None:\n        data = read_data_by_config(config)\n\n    config, dirs_for_saved_models = change_savepath_for_model(config)\n\n    cv_score = OrderedDict()\n    for data_i in generate_train_valid(data, n_folds=n_folds, is_loo=is_loo):\n        iterator = get_iterator_from_config(config, data_i)\n        create_dirs_to_save_models(dirs_for_saved_models)\n        score = train_evaluate_model_from_config(config, iterator=iterator)\n        delete_dir_for_saved_models(dirs_for_saved_models)\n        for key, value in score[\'valid\'].items():\n            if key not in cv_score:\n                cv_score[key] = []\n            cv_score[key].append(value)\n\n    for key, value in cv_score.items():\n        cv_score[key] = np.mean(value)\n        log.info(\'Cross-Validation \\""{}\\"" is: {}\'.format(key, cv_score[key]))\n\n    return cv_score\n'"
deeppavlov/core/common/errors.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass ConfigError(Exception):\n    """"""Any configuration error.""""""\n\n    def __init__(self, message):\n        super(ConfigError, self).__init__()\n        self.message = message\n\n    def __str__(self):\n        return repr(self.message)\n'"
deeppavlov/core/common/file.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport pickle\nfrom collections import OrderedDict\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import Union, Any\n\nfrom ruamel.yaml import YAML\n\nlog = getLogger(__name__)\n\n\ndef find_config(pipeline_config_path: Union[str, Path]) -> Path:\n    if not Path(pipeline_config_path).is_file():\n        configs = [c for c in Path(__file__).parent.parent.parent.glob(f\'configs/**/{pipeline_config_path}.json\')\n                   if str(c.with_suffix(\'\')).endswith(pipeline_config_path)]  # a simple way to not allow * and ?\n        if configs:\n            log.info(f""Interpreting \'{pipeline_config_path}\' as \'{configs[0]}\'"")\n            pipeline_config_path = configs[0]\n    return Path(pipeline_config_path)\n\n\ndef read_json(fpath: Union[str, Path]) -> dict:\n    with open(fpath, encoding=\'utf8\') as fin:\n        return json.load(fin, object_pairs_hook=OrderedDict)\n\n\ndef save_json(data: dict, fpath: Union[str, Path]) -> None:\n    with open(fpath, \'w\', encoding=\'utf8\') as fout:\n        json.dump(data, fout, ensure_ascii=False, indent=2)\n\n\ndef save_pickle(data: dict, fpath: Union[str, Path]) -> None:\n    with open(fpath, \'wb\') as fout:\n        pickle.dump(data, fout, protocol=4)\n\n\ndef load_pickle(fpath: Union[str, Path]) -> Any:\n    with open(fpath, \'rb\') as fin:\n        return pickle.load(fin)\n\n\ndef read_yaml(fpath: Union[str, Path]) -> dict:\n    yaml = YAML(typ=""safe"")\n    with open(fpath, encoding=\'utf8\') as fin:\n        return yaml.load(fin)\n'"
deeppavlov/core/common/log.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport logging\nimport logging.config\nfrom pathlib import Path\n\nfrom .paths import get_settings_path\n\nLOG_CONFIG_FILENAME = \'log_config.json\'\nTRACEBACK_LOGGER_ERRORS = True\n\nroot_path = Path(__file__).resolve().parents[3]\n\nlog_config_path = get_settings_path() / LOG_CONFIG_FILENAME\n\nwith log_config_path.open(encoding=\'utf8\') as log_config_json:\n    log_config = json.load(log_config_json)\n\n\nclass ProbeFilter(logging.Filter):\n    """"""ProbeFilter class is used to filter POST requests to /probe endpoint from logs.""""""\n\n    def filter(self, record: logging.LogRecord) -> bool:\n        """"""To log the record method should return True.""""""\n        return \'POST /probe HTTP\' not in record.getMessage()\n\n\ndef init_logger():\n    configured_loggers = [log_config.get(\'root\', {})] + [logger for logger in\n                                                         log_config.get(\'loggers\', {}).values()]\n\n    used_handlers = {handler for log in configured_loggers for handler in log.get(\'handlers\', [])}\n\n    for handler_id, handler in list(log_config[\'handlers\'].items()):\n        if handler_id not in used_handlers:\n            del log_config[\'handlers\'][handler_id]\n        elif \'filename\' in handler.keys():\n            filename = handler[\'filename\']\n            logfile_path = Path(filename).expanduser().resolve()\n            handler[\'filename\'] = str(logfile_path)\n\n    logging.config.dictConfig(log_config)\n'"
deeppavlov/core/common/metrics_registry.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport importlib\nimport json\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import Callable, Any\n\nfrom deeppavlov.core.common.errors import ConfigError\n\nlog = getLogger(__name__)\n\n_registry_path = Path(__file__).parent / \'metrics_registry.json\'\nif _registry_path.exists():\n    with _registry_path.open(encoding=\'utf-8\') as f:\n        _REGISTRY = json.load(f)\nelse:\n    _REGISTRY = {}\n\n\ndef fn_from_str(name: str) -> Callable[..., Any]:\n    """"""Returns a function object with the name given in string.""""""\n    try:\n        module_name, fn_name = name.split(\':\')\n    except ValueError:\n        raise ConfigError(\'Expected function description in a `module.submodules:function_name` form, but got `{}`\'\n                          .format(name))\n\n    return getattr(importlib.import_module(module_name), fn_name)\n\n\ndef register_metric(metric_name: str) -> Callable[..., Any]:\n    """"""Decorator for metric registration.""""""\n\n    def decorate(fn):\n        fn_name = fn.__module__ + \':\' + fn.__name__\n        if metric_name in _REGISTRY and _REGISTRY[metric_name] != fn_name:\n            log.warning(\'""{}"" is already registered as a metric name, the old function will be ignored\'\n                        .format(metric_name))\n        _REGISTRY[metric_name] = fn_name\n        return fn\n\n    return decorate\n\n\ndef get_metric_by_name(name: str) -> Callable[..., Any]:\n    """"""Returns a metric callable with a corresponding name.""""""\n    if name not in _REGISTRY:\n        raise ConfigError(f\'""{name}"" is not registered as a metric\')\n    return fn_from_str(_REGISTRY[name])\n'"
deeppavlov/core/common/params.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport inspect\nfrom logging import getLogger\nfrom types import FunctionType\nfrom typing import Any, Dict, Union\n\nfrom deeppavlov.core.commands.utils import expand_path, parse_config\nfrom deeppavlov.core.common.errors import ConfigError\nfrom deeppavlov.core.common.registry import get_model\nfrom deeppavlov.core.models.component import Component\n\nlog = getLogger(__name__)\n\n_refs = {}\n\n\ndef _resolve(val):\n    if isinstance(val, str) and val.startswith(\'#\'):\n        component_id, *attributes = val[1:].split(\'.\')\n        try:\n            val = _refs[component_id]\n        except KeyError:\n            e = ConfigError(\'Component with id ""{id}"" was referenced but not initialized\'\n                            .format(id=component_id))\n            log.exception(e)\n            raise e\n        attributes = [\'val\'] + attributes\n        val = eval(\'.\'.join(attributes))\n    return val\n\n\ndef _init_param(param, mode):\n    if isinstance(param, str):\n        param = _resolve(param)\n    elif isinstance(param, (list, tuple)):\n        param = [_init_param(p, mode) for p in param]\n    elif isinstance(param, dict):\n        if {\'ref\', \'class_name\', \'config_path\'}.intersection(param.keys()):\n            param = from_params(param, mode=mode)\n        else:\n            param = {k: _init_param(v, mode) for k, v in param.items()}\n    return param\n\n\ndef from_params(params: Dict, mode: str = \'infer\', serialized: Any = None, **kwargs) -> Union[Component, FunctionType]:\n    """"""Builds and returns the Component from corresponding dictionary of parameters.""""""\n    # what is passed in json:\n    config_params = {k: _resolve(v) for k, v in params.items()}\n\n    # get component by reference (if any)\n    if \'ref\' in config_params:\n        try:\n            component = _refs[config_params[\'ref\']]\n            if serialized is not None:\n                component.deserialize(serialized)\n            return component\n        except KeyError:\n            e = ConfigError(\'Component with id ""{id}"" was referenced but not initialized\'\n                            .format(id=config_params[\'ref\']))\n            log.exception(e)\n            raise e\n\n    elif \'config_path\' in config_params:\n        from deeppavlov.core.commands.infer import build_model\n        refs = _refs.copy()\n        _refs.clear()\n        config = parse_config(expand_path(config_params[\'config_path\']))\n        model = build_model(config, serialized=serialized)\n        _refs.clear()\n        _refs.update(refs)\n        try:\n            _refs[config_params[\'id\']] = model\n        except KeyError:\n            pass\n        return model\n\n    cls_name = config_params.pop(\'class_name\', None)\n    if not cls_name:\n        e = ConfigError(\'Component config has no `class_name` nor `ref` fields\')\n        log.exception(e)\n        raise e\n    obj = get_model(cls_name)\n\n    if inspect.isclass(obj):\n        # find the submodels params recursively\n        config_params = {k: _init_param(v, mode) for k, v in config_params.items()}\n\n        try:\n            spec = inspect.getfullargspec(obj)\n            if \'mode\' in spec.args + spec.kwonlyargs or spec.varkw is not None:\n                kwargs[\'mode\'] = mode\n\n            component = obj(**dict(config_params, **kwargs))\n            try:\n                _refs[config_params[\'id\']] = component\n            except KeyError:\n                pass\n        except Exception:\n            log.exception(""Exception in {}"".format(obj))\n            raise\n\n        if serialized is not None:\n            component.deserialize(serialized)\n    else:\n        component = obj\n\n    return component\n'"
deeppavlov/core/common/params_search.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport random\nfrom copy import deepcopy\nfrom logging import getLogger\nfrom typing import List, Generator, Any, Tuple\n\nimport numpy as np\n\nfrom deeppavlov.core.common.registry import register\n\nlog = getLogger(__name__)\n\n\n@register(\'params_search\')\nclass ParamsSearch:\n    """"""\n    Class determine the main operations for parameters search\n    like finding all changing parameters.\n\n    Args:\n        prefix: prefix to determine special keys like ""`prefix`_range"", ""`prefix`_bool"", ""`prefix`_choice""\n        seed: random seed for initialization\n        **kwargs: basic config with parameters\n\n    Attributes:\n        basic_config: dictionary with initial config with possible values of searched parameters\n        prefix: prefix to determine special keys like ""`prefix`_range"", ""`prefix`_bool"", ""`prefix`_choice""\n        paths_to_params: list of lists of keys and/or integers (for list)\n                with relative paths to searched parameters\n        n_params: number of searched parameters\n        eps: threshold value\n    """"""\n\n    def __init__(self,\n                 prefix=""search"",\n                 seed: int = None,\n                 **kwargs):\n        """"""\n        Initialize evolution with random population\n        """"""\n\n        self.basic_config = deepcopy(kwargs)\n        self.prefix = prefix\n\n        self.paths_to_params = []\n        for search_type in [prefix + ""_range"", prefix + ""_choice"", prefix + ""_bool""]:\n            for path_ in self.find_model_path(self.basic_config, search_type):\n                self.paths_to_params.append(path_)\n\n        self.n_params = len(self.paths_to_params)\n\n        self.eps = 1e-6\n\n        if seed is None:\n            pass\n        else:\n            np.random.seed(seed)\n            random.seed(seed)\n\n    def find_model_path(self, config: dict, key_model: str, path: list = []) -> Generator:\n        """"""\n        Find paths to all dictionaries in config that contain key \'key_model\'\n\n        Args:\n            config: dictionary\n            key_model: key of sub-dictionary to be found\n            path: list of keys and/or integers (for list) with relative path (needed for recursion)\n\n        Returns:\n            path in config -- list of keys (strings and integers)\n        """"""\n        config_pointer = config\n        if isinstance(config_pointer, dict) and key_model in config_pointer.keys():\n            yield path\n        else:\n            if isinstance(config_pointer, dict):\n                for key in list(config_pointer.keys()):\n                    for path_ in self.find_model_path(config_pointer[key], key_model, path + [key]):\n                        yield path_\n            elif isinstance(config_pointer, list):\n                for i in range(len(config_pointer)):\n                    for path_ in self.find_model_path(config_pointer[i], key_model, path + [i]):\n                        yield path_\n\n    @staticmethod\n    def insert_value_or_dict_into_config(config: dict, path: list,\n                                         value: [int, float, str, bool, list, dict, np.ndarray]) -> None:\n        """"""\n        Insert value to dictionary determined by path[:-1] in field with key path[-1]\n\n        Args:\n            config: dictionary\n            path: list of keys and/or integers (for list)\n            value: value to be inserted\n\n        Returns:\n            config with inserted value\n        """"""\n        config_pointer = config\n        for el in path[:-1]:\n            if isinstance(config_pointer, dict):\n                config_pointer = config_pointer.setdefault(el, {})\n            elif isinstance(config_pointer, list):\n                config_pointer = config_pointer[el]\n            else:\n                pass\n        config_pointer[path[-1]] = value\n\n    @staticmethod\n    def get_value_from_config(config: dict, path: list) -> Any:\n        """"""\n        Return value of config element determined by path\n\n        Args:\n            config: dictionary\n            path: list of keys and/or integers (for list)\n\n        Returns:\n            value\n        """"""\n        config_copy = deepcopy(config)\n        config_pointer = config_copy\n        for el in path[:-1]:\n            if isinstance(config_pointer, dict):\n                config_pointer = config_pointer.setdefault(el, {})\n            elif isinstance(config_pointer, list):\n                config_pointer = config_pointer[el]\n            else:\n                pass\n        return config_pointer[path[-1]]\n\n    @staticmethod\n    def remove_key_from_config(config: dict, path: list) -> Tuple[dict, Any]:\n        """"""\n        Remove config element determined by path\n\n        Args:\n            config: dictionary\n            path: list of keys and/or integers (for list)\n\n        Returns:\n            dictionary without value from path, value from path\n        """"""\n        config_copy = deepcopy(config)\n        config_pointer = config_copy\n        for el in path[:-1]:\n            if isinstance(config_pointer, dict):\n                config_pointer = config_pointer.setdefault(el, {})\n            elif isinstance(config_pointer, list):\n                config_pointer = config_pointer[el]\n            else:\n                pass\n        value = config_pointer.pop(path[-1])\n        return config_copy, value\n\n    def initialize_params_in_config(self, basic_config: dict, paths: List[list]) -> dict:\n        """"""\n        Randomly initialize all the changable parameters in config\n\n        Args:\n            basic_config: config where changable parameters are dictionaries with keys\n                ``prefix`_range`, ``prefix`_bool`, ``prefix`_choice`\n            paths: list of paths to changable parameters\n\n        Returns:\n            config\n        """"""\n        config = deepcopy(basic_config)\n        for path_ in paths:\n            param_name = path_[-1]\n            value = self.get_value_from_config(basic_config, path_)\n            if isinstance(value, dict):\n                if (value.get(self.prefix + ""_choice"") or\n                        value.get(self.prefix + ""_range"") or\n                        value.get(self.prefix + ""_bool"")):\n                    self.insert_value_or_dict_into_config(\n                        config, path_,\n                        self.sample_params(**{param_name: deepcopy(value)})[param_name])\n\n        return config\n\n    def sample_params(self, **params) -> dict:\n        """"""\n        Sample parameters according to the given possible values\n\n        Args:\n            **params: dictionary like {""param_0"": {""`prefix`_range"": [0, 10]},\n                                       ""param_1"": {""`prefix`_range"": [0, 10], ""discrete"": true},\n                                       ""param_2"": {""`prefix`_range"": [0, 1], ""scale"": ""log""},\n                                       ""param_3"": {""`prefix`_bool"": true},\n                                       ""param_4"": {""`prefix`_choice"": [0, 1, 2, 3]}}\n\n        Returns:\n            dictionary with randomly sampled parameters\n        """"""\n        if not params:\n            return {}\n        else:\n            params_copy = deepcopy(params)\n        params_sample = dict()\n        for param, param_val in params_copy.items():\n            if isinstance(param_val, dict):\n                if self.prefix + \'_bool\' in param_val and param_val[self.prefix + \'_bool\']:\n                    sample = bool(random.choice([True, False]))\n                elif self.prefix + \'_range\' in param_val:\n                    sample = self._sample_from_ranges(param_val)\n                elif self.prefix + \'_choice\' in param_val:\n                    sample = random.choice(param_val[self.prefix + \'_choice\'])\n                else:\n                    sample = param_val\n                params_sample[param] = sample\n            else:\n                params_sample[param] = params_copy[param]\n        return params_sample\n\n    def _sample_from_ranges(self, opts: dict) -> [int, float]:\n        """"""\n        Sample parameters from ranges\n\n        Args:\n            opts: dictionary  {""`prefix`_range"": [0, 10]} or \\\n                              {""`prefix`_range"": [0, 10], ""discrete"": true} or \\\n                              {""`prefix`_range"": [0, 1], ""scale"": ""log""}\n\n        Returns:\n            random parameter value from range\n        """"""\n        from_ = opts[self.prefix + \'_range\'][0]\n        to_ = opts[self.prefix + \'_range\'][1]\n        if opts.get(\'scale\', None) == \'log\':\n            sample = self._sample_log(from_, to_)\n        else:\n            sample = np.random.uniform(from_, to_)\n        if opts.get(\'discrete\', False):\n            sample = int(np.round(sample))\n        return sample\n\n    @staticmethod\n    def _sample_log(from_: float = 0., to_: float = 1.) -> float:\n        """"""\n        Sample parameters from ranges with log scale\n\n        Args:\n            from_: lower boundary of values\n            to_:  upper boundary of values\n\n        Returns:\n            random parameters value from range with log scale\n        """"""\n        sample = np.exp(np.random.uniform(np.log(from_), np.log(to_)))\n        return float(sample)\n'"
deeppavlov/core/common/paths.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport shutil\n\nfrom pathlib import Path\n\n_root_path = Path(__file__).resolve().parents[3]\n_default_settings_path: Path = _root_path / \'deeppavlov\' / \'utils\' / \'settings\'\n_settings_path = Path(os.getenv(\'DP_SETTINGS_PATH\', _default_settings_path)).expanduser().resolve()\nif _settings_path.is_file():\n    raise FileExistsError(f\'DP_SETTINGS_PATH={_settings_path} is a file and not a directory\')\n\nif _default_settings_path in _settings_path.parents:\n    raise RecursionError(f\'DP_SETTINGS_PATH={_settings_path} is relative\'\n                         f\' to the default settings path {_default_settings_path}\')\n\n\ndef get_settings_path() -> Path:\n    """"""Return an absolute path to the DeepPavlov settings directory""""""\n    populate_settings_dir()\n    return _settings_path\n\n\ndef populate_settings_dir(force: bool = False) -> bool:\n    """"""\n    Populate settings directory with default settings files\n\n    Args:\n        force: if ``True``, replace existing settings files with default ones\n\n    Returns:\n        ``True`` if any files were copied and ``False`` otherwise\n    """"""\n    res = False\n    if _default_settings_path == _settings_path:\n        return res\n\n    for src in list(_default_settings_path.glob(\'**/*.json\')):\n        dest = _settings_path / src.relative_to(_default_settings_path)\n        if not force and dest.exists():\n            continue\n        res = True\n        dest.parent.mkdir(parents=True, exist_ok=True)\n        shutil.copy(src, dest)\n    return res\n'"
deeppavlov/core/common/prints.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\nfrom contextlib import redirect_stdout\n\n\nclass RedirectedPrints(redirect_stdout):\n    """"""Context manager for temporarily redirecting stdout to another stream """"""\n\n    def __init__(self, new_target=sys.stderr):\n        super().__init__(new_target=new_target)\n'"
deeppavlov/core/common/registry.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport importlib\nimport json\nfrom logging import getLogger\nfrom pathlib import Path\n\nfrom deeppavlov.core.common.errors import ConfigError\n\nlogger = getLogger(__name__)\n\n_registry_path = Path(__file__).parent / \'registry.json\'\nif _registry_path.exists():\n    with _registry_path.open(encoding=\'utf-8\') as f:\n        _REGISTRY = json.load(f)\nelse:\n    _REGISTRY = {}\n\n\ndef cls_from_str(name: str) -> type:\n    """"""Returns a class object with the name given as a string.""""""\n    try:\n        module_name, cls_name = name.split(\':\')\n    except ValueError:\n        raise ConfigError(\'Expected class description in a `module.submodules:ClassName` form, but got `{}`\'\n                          .format(name))\n\n    return getattr(importlib.import_module(module_name), cls_name)\n\n\ndef register(name: str = None) -> type:\n    """"""\n    Register classes that could be initialized from JSON configuration file.\n    If name is not passed, the class name is converted to snake-case.\n    """"""\n\n    def decorate(model_cls: type, reg_name: str = None) -> type:\n        model_name = reg_name or short_name(model_cls)\n        global _REGISTRY\n        cls_name = model_cls.__module__ + \':\' + model_cls.__name__\n        if model_name in _REGISTRY and _REGISTRY[model_name] != cls_name:\n            logger.warning(\'Registry name ""{}"" has been already registered and will be overwritten.\'.format(model_name))\n        _REGISTRY[model_name] = cls_name\n        return model_cls\n\n    return lambda model_cls_name: decorate(model_cls_name, name)\n\n\ndef short_name(cls: type) -> str:\n    """"""Returns just a class name (without package and module specification).""""""\n    return cls.__name__.split(\'.\')[-1]\n\n\ndef get_model(name: str) -> type:\n    """"""Returns a registered class object with the name given in the string.""""""\n    if name not in _REGISTRY:\n        if \':\' not in name:\n            raise ConfigError(""Model {} is not registered."".format(name))\n        return cls_from_str(name)\n    return cls_from_str(_REGISTRY[name])\n\n\ndef list_models() -> list:\n    """"""Returns a list of names of registered classes.""""""\n    return list(_REGISTRY)\n'"
deeppavlov/core/data/__init__.py,0,b''
deeppavlov/core/data/data_fitting_iterator.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom random import Random\nfrom typing import List, Generator, Tuple, Any, Optional\n\nfrom deeppavlov.core.common.registry import register\n\nlogger = getLogger(__name__)\n\n\n@register(\'data_fitting_iterator\')\nclass DataFittingIterator:\n    """"""Dataset iterator for fitting estimator models, like vocabs, kNN, vectorizers.\n    Data is passed as a list of strings(documents).\n    Generate batches (for large datasets).\n\n    Args:\n        data: list of documents\n        doc_ids: provided document ids\n        seed: random seed for data shuffling\n        shuffle: whether to shuffle data during batching\n\n    Attributes:\n        shuffle: whether to shuffle data during batching\n        random: instance of :class:`Random` initialized with a seed\n        data: list of documents\n        doc_ids: provided by a user ids or generated automatically ids\n\n    """"""\n\n    def __init__(self, data: List[str], doc_ids: List[Any] = None,\n                 seed: int = None, shuffle: bool = True,\n                 *args, **kwargs) -> None:\n\n        self.shuffle = shuffle\n        self.random = Random(seed)\n        self.data = data\n        self.doc_ids = doc_ids or self.get_doc_ids()\n\n    def get_doc_ids(self):\n        """"""Generate doc ids.\n\n        Returns: doc ids\n\n        """"""\n        return list(range(len(self.data)))\n\n    def get_doc_content(self, doc_id: Any) -> Optional[str]:\n        """"""Get doc content by id.\n\n        Args:\n            doc_id: an id for a doc which content should be extracted\n\n        Returns:\n            doc content as a string if id exists or raise an error\n\n        """"""\n        return self.data[doc_id]\n\n    def gen_batches(self, batch_size: int, shuffle: bool = None) \\\n            -> Generator[Tuple[List[str], List[int]], Any, None]:\n        """"""Gen batches of documents.\n\n        Args:\n            batch_size: a number of samples in a single batch\n            shuffle: whether to shuffle data during batching\n\n        Yields:\n            generated tuple of documents and their ids\n\n        """"""\n        if shuffle is None:\n            shuffle = self.shuffle\n\n        if shuffle:\n            _doc_ids = self.random.sample(self.doc_ids, len(self.doc_ids))\n        else:\n            _doc_ids = self.doc_ids\n\n        if batch_size > 0:\n            batches = [_doc_ids[i:i + batch_size] for i in\n                       range(0, len(_doc_ids), batch_size)]\n        else:\n            batches = [_doc_ids]\n\n        # DEBUG\n        # len_batches = len(batches)\n\n        for i, doc_ids in enumerate(batches):\n            # DEBUG\n            # logger.info(\n            #     ""Processing batch # {} of {} ({} documents)"".format(i, len_batches, len(doc_index)))\n            docs = [self.get_doc_content(doc_id) for doc_id in doc_ids]\n            yield docs, doc_ids\n\n    def get_instances(self):\n        """"""Get all data""""""\n        doc_ids = list(self.doc_ids)\n        docs = [self.get_doc_content(doc_id) for doc_id in doc_ids]\n        return docs, doc_ids\n'"
deeppavlov/core/data/data_learning_iterator.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom random import Random\nfrom typing import List, Dict, Tuple, Any, Iterator\n\nfrom deeppavlov.core.common.registry import register\n\n\n@register(\'data_learning_iterator\')\nclass DataLearningIterator:\n    """"""Dataset iterator for learning models, e. g. neural networks.\n\n    Args:\n        data: list of (x, y) pairs for every data type in ``\'train\'``, ``\'valid\'`` and ``\'test\'``\n        seed: random seed for data shuffling\n        shuffle: whether to shuffle data during batching\n\n    Attributes:\n        shuffle: whether to shuffle data during batching\n        random: instance of ``Random`` initialized with a seed\n    """"""\n\n    def split(self, *args, **kwargs):\n        """""" Manipulate self.train, self.valid, and self.test into their final form. """"""\n        pass\n\n    def preprocess(self, data: List[Tuple[Any, Any]], *args, **kwargs) -> List[Tuple[Any, Any]]:\n        """""" Transform the data for a specific data type (e.g. ``\'train\'``). """"""\n        return data\n\n    def __init__(self, data: Dict[str, List[Tuple[Any, Any]]], seed: int = None, shuffle: bool = True,\n                 *args, **kwargs) -> None:\n        self.shuffle = shuffle\n\n        self.random = Random(seed)\n\n        self.train = self.preprocess(data.get(\'train\', []), *args, **kwargs)\n        self.valid = self.preprocess(data.get(\'valid\', []), *args, **kwargs)\n        self.test = self.preprocess(data.get(\'test\', []), *args, **kwargs)\n        self.split(*args, **kwargs)\n        self.data = {\n            \'train\': self.train,\n            \'valid\': self.valid,\n            \'test\': self.test,\n            \'all\': self.train + self.test + self.valid\n        }\n\n    def gen_batches(self, batch_size: int, data_type: str = \'train\',\n                    shuffle: bool = None) -> Iterator[Tuple[tuple, tuple]]:\n        """"""Generate batches of inputs and expected output to train neural networks\n\n        Args:\n            batch_size: number of samples in batch\n            data_type: can be either \'train\', \'test\', or \'valid\'\n            shuffle: whether to shuffle dataset before batching\n\n        Yields:\n             a tuple of a batch of inputs and a batch of expected outputs\n        """"""\n        if shuffle is None:\n            shuffle = self.shuffle\n\n        data = self.data[data_type]\n        data_len = len(data)\n\n        if data_len == 0:\n            return\n\n        order = list(range(data_len))\n        if shuffle:\n            self.random.shuffle(order)\n\n        if batch_size < 0:\n            batch_size = data_len\n\n        for i in range((data_len - 1) // batch_size + 1):\n            yield tuple(zip(*[data[o] for o in order[i * batch_size:(i + 1) * batch_size]]))\n\n    def get_instances(self, data_type: str = \'train\') -> Tuple[tuple, tuple]:\n        """"""Get all data for a selected data type\n\n        Args:\n            data_type (str): can be either ``\'train\'``, ``\'test\'``, ``\'valid\'`` or ``\'all\'``\n\n        Returns:\n             a tuple of all inputs for a data type and all expected outputs for a data type\n        """"""\n        data = self.data[data_type]\n        return tuple(zip(*data))\n'"
deeppavlov/core/data/dataset_reader.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import List, Dict, Tuple, Any\n\n\nclass DatasetReader:\n    """"""An abstract class for reading data from some location and construction of a dataset.""""""\n\n    def read(self, data_path: str, *args, **kwargs) -> Dict[str, List[Tuple[Any, Any]]]:\n        """"""Reads a file from a path and returns data as a list of tuples of inputs and correct outputs\n         for every data type in ``train``, ``valid`` and ``test``.\n        """"""\n        raise NotImplementedError\n'"
deeppavlov/core/data/simple_vocab.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom collections import Counter, defaultdict, Iterable\nfrom itertools import chain\nfrom logging import getLogger\nfrom typing import Optional, Tuple, List\n\nimport numpy as np\n\nfrom deeppavlov.core.common.errors import ConfigError\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.utils import zero_pad, is_str_batch, flatten_str_batch\nfrom deeppavlov.core.models.estimator import Estimator\n\nlog = getLogger(__name__)\n\n\n@register(\'simple_vocab\')\nclass SimpleVocabulary(Estimator):\n    """"""Implements simple vocabulary.\n\n    Parameters:\n        special_tokens: tuple of tokens that shouldn\'t be counted.\n        max_tokens: upper bound for number of tokens in the vocabulary.\n        min_freq: minimal count of a token (except special tokens).\n        pad_with_zeros: if True, then batch of elements will be padded with zeros up to length of\n            the longest element in batch.\n        unk_token: label assigned to unknown tokens.\n        freq_drop_load: if True, then frequencies of tokens are set to min_freq on the model load.\n        """"""\n\n    def __init__(self,\n                 special_tokens: Tuple[str, ...] = tuple(),\n                 max_tokens: int = 2 ** 30,\n                 min_freq: int = 0,\n                 pad_with_zeros: bool = False,\n                 unk_token: Optional[str] = None,\n                 freq_drop_load: Optional[bool] = None,\n                 *args,\n                 **kwargs):\n        super().__init__(**kwargs)\n        self.special_tokens = special_tokens\n        self._max_tokens = max_tokens\n        self._min_freq = min_freq\n        self._pad_with_zeros = pad_with_zeros\n        self.unk_token = unk_token\n        self.freq_drop_load = freq_drop_load\n        self.reset()\n        if self.load_path:\n            self.load()\n\n    def fit(self, *args):\n        self.reset()\n        tokens = chain(*args)\n        # filter(None, <>) -- to filter empty tokens\n        self.freqs = Counter(filter(None, flatten_str_batch(tokens)))\n        for special_token in self.special_tokens:\n            self._t2i[special_token] = self.count\n            self._i2t.append(special_token)\n            self.count += 1\n        for token, freq in self.freqs.most_common()[:self._max_tokens]:\n            if token in self.special_tokens:\n                continue\n            if freq >= self._min_freq:\n                self._t2i[token] = self.count\n                self._i2t.append(token)\n                self.count += 1\n\n    def _add_tokens_with_freqs(self, tokens, freqs):\n        self.freqs = Counter()\n        self.freqs.update(dict(zip(tokens, freqs)))\n        for token, freq in zip(tokens, freqs):\n            if freq >= self._min_freq or token in self.special_tokens:\n                self._t2i[token] = self.count\n                self._i2t.append(token)\n                self.count += 1\n\n    def __call__(self, batch, is_top=True, **kwargs):\n        if isinstance(batch, Iterable) and not isinstance(batch, str):\n            looked_up_batch = [self(sample, is_top=False) for sample in batch]\n        else:\n            return self[batch]\n        if self._pad_with_zeros and is_top and not is_str_batch(looked_up_batch):\n            looked_up_batch = zero_pad(looked_up_batch)\n\n        return looked_up_batch\n\n    def save(self):\n        log.info(""[saving vocabulary to {}]"".format(self.save_path))\n        with self.save_path.open(\'wt\', encoding=\'utf8\') as f:\n            for n in range(len(self)):\n                token = self._i2t[n]\n                cnt = self.freqs[token]\n                f.write(\'{}\\t{:d}\\n\'.format(token, cnt))\n\n    def serialize(self) -> List[Tuple[str, int]]:\n        return [(token, self.freqs[token]) for token in self._i2t]\n\n    def load(self):\n        self.reset()\n        if self.load_path:\n            if self.load_path.is_file():\n                log.info(""[loading vocabulary from {}]"".format(self.load_path))\n                tokens, counts = [], []\n                for ln in self.load_path.open(\'r\', encoding=\'utf8\'):\n                    token, cnt = self.load_line(ln)\n                    tokens.append(token)\n                    counts.append(int(cnt))\n                self._add_tokens_with_freqs(tokens, counts)\n            elif not self.load_path.parent.is_dir():\n                raise ConfigError(""Provided `load_path` for {} doesn\'t exist!"".format(\n                    self.__class__.__name__))\n        else:\n            raise ConfigError(""`load_path` for {} is not provided!"".format(self))\n\n    def deserialize(self, data: List[Tuple[str, int]]) -> None:\n        self.reset()\n        if data:\n            tokens, counts = zip(*data)\n            self._add_tokens_with_freqs(tokens, counts)\n\n    def load_line(self, ln):\n        if self.freq_drop_load:\n            token = ln.strip().split()[0]\n            cnt = self._min_freq\n        else:\n            token, cnt = ln.rsplit(\'\\t\', 1)\n        return token, cnt\n\n    @property\n    def len(self):\n        return len(self)\n\n    def keys(self):\n        return (self[n] for n in range(self.len))\n\n    def values(self):\n        return list(range(self.len))\n\n    def items(self):\n        return zip(self.keys(), self.values())\n\n    def __getitem__(self, key):\n        if isinstance(key, (int, np.integer)):\n            return self._i2t[key]\n        elif isinstance(key, str):\n            return self._t2i[key]\n        else:\n            raise NotImplementedError(""not implemented for type `{}`"".format(type(key)))\n\n    def __contains__(self, item):\n        return item in self._t2i\n\n    def __len__(self):\n        return len(self._i2t)\n\n    def reset(self):\n        self.freqs = None\n        unk_index = 0\n        if self.unk_token in self.special_tokens:\n            unk_index = self.special_tokens.index(self.unk_token)\n        self._t2i = defaultdict(lambda: unk_index)\n        self._i2t = []\n        self.count = 0\n\n    def idxs2toks(self, idxs):\n        return [self[idx] for idx in idxs]\n'"
deeppavlov/core/data/sqlite_database.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sqlite3\nfrom logging import getLogger\nfrom typing import List, Dict\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.estimator import Estimator\n\nlog = getLogger(__name__)\n\n\n@register(\'sqlite_database\')\nclass Sqlite3Database(Estimator):\n    """"""\n    Loads and trains sqlite table of any items (with name ``table_name``\n    and path ``save_path``).\n\n    Primary (unique) keys must be specified, all other keys are infered from data.\n    Batch here is a list of dictionaries, where each dictionary corresponds to an item.\n    If an item doesn\'t contain values for all keys, then missing values will be stored\n    with ``unknown_value``.\n\n    Parameters:\n        save_path: sqlite database path.\n        primary_keys: list of table primary keys\' names.\n        keys: all table keys\' names.\n        table_name: name of the sqlite table.\n        unknown_value: value assigned to missing item values.\n        **kwargs: parameters passed to parent\n            :class:`~deeppavlov.core.models.estimator.Estimator` class.\n    """"""\n\n    def __init__(self,\n                 save_path: str,\n                 primary_keys: List[str],\n                 keys: List[str] = None,\n                 table_name: str = ""mytable"",\n                 unknown_value: str = \'UNK\',\n                 *args, **kwargs) -> None:\n        super().__init__(save_path=save_path, *args, **kwargs)\n\n        self.primary_keys = primary_keys\n        if not self.primary_keys:\n            raise ValueError(""Primary keys list can\'t be empty"")\n        self.tname = table_name\n        self.keys = keys\n        self.unknown_value = unknown_value\n\n        self.conn = sqlite3.connect(str(self.save_path),\n                                    check_same_thread=False)\n        self.cursor = self.conn.cursor()\n        if self._check_if_table_exists():\n            log.info(f""Loading database from {self.save_path}."")\n            if not self.keys:\n                self.keys = self._get_keys()\n        else:\n            log.info(f""Initializing empty database on {self.save_path}."")\n\n    def __call__(self, batch: List[Dict],\n                 order_by: str = None,\n                 ascending: bool = False) -> List[List[Dict]]:\n        order = \'ASC\' if ascending else \'DESC\'\n        if not self._check_if_table_exists():\n            log.warning(""Database is empty, call fit() before using."")\n            return [[] for i in range(len(batch))]\n        return [self._search(b, order_by=order_by, order=order) for b in batch]\n\n    def _check_if_table_exists(self):\n        self.cursor.execute(f""SELECT name FROM sqlite_master""\n                            f"" WHERE type=\'table\'""\n                            f"" AND name=\'{self.tname}\';"")\n        return bool(self.cursor.fetchall())\n\n    def _search(self, kv=None, order_by=None, order=\'\'):\n        order_expr = f"" ORDER BY {order_by} {order}"" if order_by else \'\'\n        if kv:\n            keys, values = zip(*kv.items())\n            where_expr = "" AND "".join(f""{k}=?"" for k in keys)\n            self.cursor.execute(f""SELECT * FROM {self.tname} WHERE {where_expr}"" + order_expr, values)\n        else:\n            self.cursor.execute(f""SELECT * FROM {self.tname}"" + order_expr)\n        return [self._wrap_selection(s) for s in self.cursor.fetchall()]\n\n    def _wrap_selection(self, selection):\n        if not self.keys:\n            self.keys = self._get_keys()\n        return {f: v for f, v in zip(self.keys, selection)}\n\n    def _get_keys(self):\n        self.cursor.execute(f""PRAGMA table_info({self.tname});"")\n        return [info[1] for info in self.cursor]\n\n    def _get_types(self):\n        self.cursor.execute(f""PRAGMA table_info({self.tname});"")\n        return {info[1]: info[2] for info in self.cursor}\n\n    def fit(self, data: List[Dict]) -> None:\n        if not self._check_if_table_exists():\n            self.keys = self.keys or [key for key in data[0]]\n            # because in the next line we assume that in the first dict there are all (!) necessary keys:\n            types = (\'integer\' if isinstance(data[0][k], int) else \'text\' for k in self.keys)\n            self._create_table(self.keys, types)\n        elif not self.keys:\n            self.keys = self._get_keys()\n\n        self._insert_many(data)\n\n    def _create_table(self, keys, types):\n        if any(pk not in keys for pk in self.primary_keys):\n            raise ValueError(f""Primary keys must be from {keys}."")\n        new_types = (f""{k} {t} primary key""\n                     if k in self.primary_keys else f""{k} {t}""\n                     for k, t in zip(keys, types))\n        new_types_joined = \', \'.join(new_types)\n        self.cursor.execute(f""CREATE TABLE IF NOT EXISTS {self.tname}""\n                            f"" ({new_types_joined})"")\n        log.info(f""Created table with keys {self._get_types()}."")\n\n    def _insert_many(self, data):\n        to_insert = {}\n        to_update = {}\n        for kv in filter(None, data):\n            primary_values = tuple(kv[pk] for pk in self.primary_keys)\n            record = tuple(kv.get(k, self.unknown_value) for k in self.keys)\n            curr_record = self._get_record(primary_values)\n            if curr_record:\n                if primary_values in to_update:\n                    curr_record = to_update[primary_values]\n                if curr_record != record:\n                    to_update[primary_values] = record\n            else:\n                to_insert[primary_values] = record\n\n        if to_insert:\n            fformat = \',\'.join([\'?\'] * len(self.keys))\n            self.cursor.executemany(f""INSERT into {self.tname}"" +\n                                    f"" VALUES ({fformat})"",\n                                    to_insert.values())\n        if to_update:\n            for record in to_update.values():\n                self._update_one(record)\n\n        self.conn.commit()\n\n    def _get_record(self, primary_values):\n        ffields = "", "".join(self.keys) or ""*""\n        where_expr = "" AND "".join(f""{pk}=?"" for pk in self.primary_keys)\n        fetched = self.cursor.execute(f""SELECT {ffields} FROM {self.tname}"" +\n                                      f"" WHERE {where_expr}"", primary_values).fetchone()\n        if not fetched:\n            return None\n        return fetched\n\n    def _update_one(self, record):\n        set_values, where_values = [], []\n        set_fields, where_fields = [], []\n        for k, v in zip(self.keys, record):\n            if k in self.primary_keys:\n                where_fields.append(f""{k}=?"")\n                where_values.append(v)\n            else:\n                set_fields.append(f""{k}=?"")\n                set_values.append(v)\n        set_expr = "", "".join(set_fields)\n        where_expr = "" AND "".join(where_fields)\n        self.cursor.execute(f""UPDATE {self.tname}"" +\n                            f"" SET {set_expr}"" +\n                            f"" WHERE {where_expr}"", set_values+where_values)\n\n    def save(self):\n        pass\n\n    def load(self):\n        pass\n'"
deeppavlov/core/data/utils.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport collections\nimport gzip\nimport os\nimport secrets\nimport shutil\nimport tarfile\nimport zipfile\nfrom hashlib import md5\nfrom itertools import chain\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import Any, Generator, Iterable, List, Mapping, Optional, Sequence, Sized, Union, Collection\nfrom urllib.parse import urlencode, parse_qs, urlsplit, urlunsplit, urlparse\n\nimport numpy as np\nimport requests\nfrom tqdm import tqdm\n\nlog = getLogger(__name__)\n\n_MARK_DONE = \'.done\'\n\ntqdm.monitor_interval = 0\n\n\ndef _get_download_token() -> str:\n    """"""Return a download token from ~/.deeppavlov/token file.\n\n    If token file does not exists, creates the file and writes to it a random URL-safe text string\n    containing 32 random bytes.\n\n    Returns:\n        32 byte URL-safe text string from ~/.deeppavlov/token.\n\n    """"""\n    token_file = Path.home() / \'.deeppavlov\' / \'token\'\n    if not token_file.exists():\n        if token_file.parent.is_file():\n            token_file.parent.unlink()\n        token_file.parent.mkdir(parents=True, exist_ok=True)\n        token_file.write_text(secrets.token_urlsafe(32), encoding=\'utf8\')\n\n    return token_file.read_text(encoding=\'utf8\').strip()\n\n\ndef simple_download(url: str, destination: Union[Path, str]) -> None:\n    """"""Download a file from URL to target location.\n\n    Displays progress bar to the terminal during the download process.\n\n    Args:\n        url: The source URL.\n        destination: Path to the file destination (including file name).\n\n    """"""\n    chunk_size = 32 * 1024\n\n    destination = Path(destination)\n    destination.parent.mkdir(parents=True, exist_ok=True)\n    temporary = destination.with_suffix(destination.suffix + \'.part\')\n\n    headers = {\'dp-token\': _get_download_token()}\n    r = requests.get(url, stream=True, headers=headers)\n    total_length = int(r.headers.get(\'content-length\', 0))\n\n    log.info(\'Downloading from {} to {}\'.format(url, destination))\n\n    if temporary.exists() and temporary.stat().st_size > total_length:\n        temporary.write_bytes(b\'\')  # clearing temporary file when total_length is inconsistent\n\n    with temporary.open(\'ab\') as f:\n        done = False\n        downloaded = f.tell()\n        if downloaded != 0:\n            log.warning(f\'Found a partial download {temporary}\')\n        with tqdm(initial=downloaded, total=total_length, unit=\'B\', unit_scale=True) as pbar:\n            while not done:\n                if downloaded != 0:\n                    log.warning(f\'Download stopped abruptly, trying to resume from {downloaded} \'\n                                f\'to reach {total_length}\')\n                    headers[\'Range\'] = f\'bytes={downloaded}-\'\n                    r = requests.get(url, headers=headers, stream=True)\n                    if \'content-length\' not in r.headers or \\\n                            total_length - downloaded != int(r.headers[\'content-length\']):\n                        raise RuntimeError(f\'It looks like the server does not support resuming \'\n                                           f\'downloads.\')\n                for chunk in r.iter_content(chunk_size=chunk_size):\n                    if chunk:  # filter out keep-alive new chunks\n                        downloaded += len(chunk)\n                        pbar.update(len(chunk))\n                        f.write(chunk)\n                if downloaded >= total_length:\n                    # Note that total_length is 0 if the server didn\'t return the content length,\n                    # in this case we perform just one iteration and assume that we are done.\n                    done = True\n\n    temporary.rename(destination)\n\n\ndef download(dest_file_path: [List[Union[str, Path]]], source_url: str, force_download: bool = True) -> None:\n    """"""Download a file from URL to one or several target locations.\n\n    Args:\n        dest_file_path: Path or list of paths to the file destination (including file name).\n        source_url: The source URL.\n        force_download: Download file if it already exists, or not.\n\n    """"""\n\n    if isinstance(dest_file_path, list):\n        dest_file_paths = [Path(path) for path in dest_file_path]\n    else:\n        dest_file_paths = [Path(dest_file_path).absolute()]\n\n    if not force_download:\n        to_check = list(dest_file_paths)\n        dest_file_paths = []\n        for p in to_check:\n            if p.exists():\n                log.info(f\'File already exists in {p}\')\n            else:\n                dest_file_paths.append(p)\n\n    if dest_file_paths:\n        cache_dir = os.getenv(\'DP_CACHE_DIR\')\n        cached_exists = False\n        if cache_dir:\n            first_dest_path = Path(cache_dir) / md5(source_url.encode(\'utf8\')).hexdigest()[:15]\n            cached_exists = first_dest_path.exists()\n        else:\n            first_dest_path = dest_file_paths.pop()\n\n        if not cached_exists:\n            first_dest_path.parent.mkdir(parents=True, exist_ok=True)\n\n            simple_download(source_url, first_dest_path)\n        else:\n            log.info(f\'Found cached {source_url} in {first_dest_path}\')\n\n        for dest_path in dest_file_paths:\n            dest_path.parent.mkdir(parents=True, exist_ok=True)\n            shutil.copy(str(first_dest_path), str(dest_path))\n\n\ndef untar(file_path: Union[Path, str], extract_folder: Optional[Union[Path, str]] = None) -> None:\n    """"""Simple tar archive extractor.\n\n    Args:\n        file_path: Path to the tar file to be extracted.\n        extract_folder: Folder to which the files will be extracted.\n\n    """"""\n    file_path = Path(file_path)\n    if extract_folder is None:\n        extract_folder = file_path.parent\n    extract_folder = Path(extract_folder)\n    tar = tarfile.open(file_path)\n    tar.extractall(extract_folder)\n    tar.close()\n\n\ndef ungzip(file_path: Union[Path, str], extract_path: Optional[Union[Path, str]] = None) -> None:\n    """"""Simple .gz archive extractor.\n\n    Args:\n        file_path: Path to the gzip file to be extracted.\n        extract_path: Path where the file will be extracted.\n\n    """"""\n    chunk_size = 16 * 1024\n    file_path = Path(file_path)\n    if extract_path is None:\n        extract_path = file_path.with_suffix(\'\')\n    extract_path = Path(extract_path)\n\n    with gzip.open(file_path, \'rb\') as fin, extract_path.open(\'wb\') as fout:\n        while True:\n            block = fin.read(chunk_size)\n            if not block:\n                break\n            fout.write(block)\n\n\ndef download_decompress(url: str,\n                        download_path: Union[Path, str],\n                        extract_paths: Optional[Union[List[Union[Path, str]], Path, str]] = None) -> None:\n    """"""Download and extract .tar.gz or .gz file to one or several target locations.\n\n    The archive is deleted if extraction was successful.\n\n    Args:\n        url: URL for file downloading.\n        download_path: Path to the directory where downloaded file will be stored until the end of extraction.\n        extract_paths: Path or list of paths where contents of archive will be extracted.\n\n    """"""\n    file_name = Path(urlparse(url).path).name\n    download_path = Path(download_path)\n\n    if extract_paths is None:\n        extract_paths = [download_path]\n    elif isinstance(extract_paths, list):\n        extract_paths = [Path(path) for path in extract_paths]\n    else:\n        extract_paths = [Path(extract_paths)]\n\n    cache_dir = os.getenv(\'DP_CACHE_DIR\')\n    extracted = False\n    if cache_dir:\n        cache_dir = Path(cache_dir)\n        url_hash = md5(url.encode(\'utf8\')).hexdigest()[:15]\n        arch_file_path = cache_dir / url_hash\n        extracted_path = cache_dir / (url_hash + \'_extracted\')\n        extracted = extracted_path.exists()\n        if not extracted and not arch_file_path.exists():\n            simple_download(url, arch_file_path)\n        else:\n            if extracted:\n                log.info(f\'Found cached and extracted {url} in {extracted_path}\')\n            else:\n                log.info(f\'Found cached {url} in {arch_file_path}\')\n    else:\n        arch_file_path = download_path / file_name\n        simple_download(url, arch_file_path)\n        extracted_path = extract_paths.pop()\n\n    if not extracted:\n        log.info(\'Extracting {} archive into {}\'.format(arch_file_path, extracted_path))\n        extracted_path.mkdir(parents=True, exist_ok=True)\n\n        if file_name.endswith(\'.tar.gz\'):\n            untar(arch_file_path, extracted_path)\n        elif file_name.endswith(\'.gz\'):\n            ungzip(arch_file_path, extracted_path / Path(file_name).with_suffix(\'\').name)\n        elif file_name.endswith(\'.zip\'):\n            with zipfile.ZipFile(arch_file_path, \'r\') as zip_ref:\n                zip_ref.extractall(extracted_path)\n        else:\n            raise RuntimeError(f\'Trying to extract an unknown type of archive {file_name}\')\n\n        if not cache_dir:\n            arch_file_path.unlink()\n\n    for extract_path in extract_paths:\n        for src in extracted_path.iterdir():\n            dest = extract_path / src.name\n            if src.is_dir():\n                _copytree(src, dest)\n            else:\n                extract_path.mkdir(parents=True, exist_ok=True)\n                shutil.copy(str(src), str(dest))\n\n\ndef _copytree(src: Path, dest: Path) -> None:\n    """"""Recursively copies directory.\n\n    Destination directory could exist (unlike if we used shutil.copytree).\n\n    Args:\n        src: Path to copied directory.\n        dest: Path to destination directory.\n\n    """"""\n    dest.mkdir(parents=True, exist_ok=True)\n    for f in src.iterdir():\n        f_dest = dest / f.name\n        if f.is_dir():\n            _copytree(f, f_dest)\n        else:\n            shutil.copy(str(f), str(f_dest))\n\n\ndef file_md5(fpath: Union[str, Path], chunk_size: int = 2 ** 16) -> Optional[str]:\n    """"""Return md5 hash value for file contents.\n\n    Args:\n        fpath: Path to file.\n        chunk_size: md5 object updated by ``chunk_size`` bytes from file.\n\n    Returns:\n        None if ``fpath`` does not point to a file, else returns md5 hash value as string.\n\n    """"""\n    fpath = Path(fpath)\n    if not fpath.is_file():\n        return None\n    file_hash = md5()\n    with fpath.open(\'rb\') as f:\n        for chunk in iter(lambda: f.read(chunk_size), b""""):\n            file_hash.update(chunk)\n    return file_hash.hexdigest()\n\n\ndef mark_done(path: Union[Path, str]) -> None:\n    """"""Create ``.done`` empty file in the directory.\n\n    Args:\n        path: Path to directory.\n\n    Raises:\n        NotADirectoryError: If ``path`` does not point to a directory.\n\n    """"""\n    path = Path(path)\n    if not path.is_dir():\n        raise NotADirectoryError(f""Not a directory: \'{path}\'"")\n    mark = path / _MARK_DONE\n    mark.touch(exist_ok=True)\n\n\ndef is_done(path: Union[Path, str]) -> bool:\n    """"""Check if ``.done`` file exists in directory.\n\n    Args:\n        path: Path to directory.\n\n    Returns:\n        True if directory contains ``.done`` file, False otherwise.\n\n    """"""\n    mark = Path(path) / _MARK_DONE\n    return mark.is_file()\n\n\ndef _get_all_dimensions(batch: Sequence, level: int = 0, res: Optional[List[List[int]]] = None) -> List[List[int]]:\n    """"""Return all presented element sizes of each dimension.\n\n    Args:\n        batch: Data array.\n        level: Recursion level.\n        res: List containing element sizes of each dimension.\n\n    Return:\n        List, i-th element of which is list containing all presented sized of batch\'s i-th dimension.\n\n    Examples:\n        >>> x = [[[1], [2, 3]], [[4], [5, 6, 7], [8, 9]]]\n        >>> _get_all_dimensions(x)\n        [[2], [2, 3], [1, 2, 1, 3, 2]]\n\n    """"""\n    if not level:\n        res = [[len(batch)]]\n    if len(batch) and isinstance(batch[0], Sized) and not isinstance(batch[0], str):\n        level += 1\n        if len(res) <= level:\n            res.append([])\n        for item in batch:\n            res[level].append(len(item))\n            _get_all_dimensions(item, level, res)\n    return res\n\n\ndef get_dimensions(batch: Sequence) -> List[int]:\n    """"""Return maximal size of each batch dimension.""""""\n    return list(map(max, _get_all_dimensions(batch)))\n\n\ndef zero_pad(batch: Sequence,\n             zp_batch: Optional[np.ndarray] = None,\n             dtype: type = np.float32,\n             padding: Union[int, float] = 0) -> np.ndarray:\n    """"""Fills the end of each array item to make its length maximal along each dimension.\n\n    Args:\n        batch: Initial array.\n        zp_batch: Padded array.\n        dtype = Type of padded array.\n        padding = Number to will initial array with.\n\n    Returns:\n        Padded array.\n\n    Examples:\n        >>> x = np.array([[1, 2, 3], [4], [5, 6]])\n        >>> zero_pad(x)\n        array([[1., 2., 3.],\n               [4., 0., 0.],\n               [5., 6., 0.]], dtype=float32)\n\n    """"""\n    if zp_batch is None:\n        dims = get_dimensions(batch)\n        zp_batch = np.ones(dims, dtype=dtype) * padding\n    if zp_batch.ndim == 1:\n        zp_batch[:len(batch)] = batch\n    else:\n        for b, zp in zip(batch, zp_batch):\n            zero_pad(b, zp)\n    return zp_batch\n\n\ndef is_str_batch(batch: Iterable) -> bool:\n    """"""Checks if iterable argument contains string at any nesting level.""""""\n    while True:\n        if isinstance(batch, Iterable):\n            if isinstance(batch, str):\n                return True\n            elif isinstance(batch, np.ndarray):\n                return batch.dtype.kind == \'U\'\n            else:\n                if len(batch) > 0:\n                    batch = batch[0]\n                else:\n                    return True\n        else:\n            return False\n\n\ndef flatten_str_batch(batch: Union[str, Iterable]) -> Union[list, chain]:\n    """"""Joins all strings from nested lists to one ``itertools.chain``.\n\n    Args:\n        batch: List with nested lists to flatten.\n\n    Returns:\n        Generator of flat List[str]. For str ``batch`` returns [``batch``].\n\n    Examples:\n        >>> [string for string in flatten_str_batch([\'a\', [\'b\'], [[\'c\', \'d\']]])]\n        [\'a\', \'b\', \'c\', \'d\']\n\n    """"""\n    if isinstance(batch, str):\n        return [batch]\n    else:\n        return chain(*[flatten_str_batch(sample) for sample in batch])\n\n\ndef zero_pad_truncate(batch: Sequence[Sequence[Union[int, float, np.integer, np.floating,\n                                                     Sequence[Union[int, float, np.integer, np.floating]]]]],\n                      max_len: int, pad: str = \'post\', trunc: str = \'post\',\n                      dtype: Optional[Union[type, str]] = None) -> np.ndarray:\n    """"""\n\n    Args:\n        batch: assumes a batch of lists of word indexes or their vector representations\n        max_len: resulting length of every batch item\n        pad: how to pad shorter batch items: can be ``\'post\'`` or ``\'pre\'``\n        trunc: how to truncate a batch item: can be ``\'post\'`` or ``\'pre\'``\n        dtype: overrides dtype for the resulting ``ndarray`` if specified,\n         otherwise ``np.int32`` is used for 2-d arrays and ``np.float32`` \xe2\x80\x94 for 3-d arrays\n\n    Returns:\n        a 2-d array of size ``(len(batch), max_len)`` or a 3-d array of size ``(len(batch), max_len, len(batch[0][0]))``\n    """"""\n    if isinstance(batch[0][0], Collection):  # ndarray behaves like a Sequence without actually being one\n        size = (len(batch), max_len, len(batch[0][0]))\n        dtype = dtype or np.float32\n    else:\n        size = (len(batch), max_len)\n        dtype = dtype or np.int32\n\n    padded_batch = np.zeros(size, dtype=dtype)\n    for i, batch_item in enumerate(batch):\n        if len(batch_item) > max_len:  # trunc\n            padded_batch[i] = batch_item[slice(max_len) if trunc == \'post\' else slice(-max_len, None)]\n        else:  # pad\n            padded_batch[i, slice(len(batch_item)) if pad == \'post\' else slice(-len(batch_item), None)] = batch_item\n\n    return np.asarray(padded_batch)\n\n\ndef get_all_elems_from_json(search_json: dict, search_key: str) -> list:\n    """"""Returns values by key in all nested dicts.\n\n    Args:\n        search_json: Dictionary in which one needs to find all values by specific key.\n        search_key: Key for search.\n\n    Returns:\n        List of values stored in nested structures by ``search_key``.\n\n    Examples:\n        >>> get_all_elems_from_json({\'a\':{\'b\': [1,2,3]}, \'b\':42}, \'b\')\n        [[1, 2, 3], 42]\n\n    """"""\n    result = []\n    if isinstance(search_json, dict):\n        for key in search_json:\n            if key == search_key:\n                result.append(search_json[key])\n            else:\n                result.extend(get_all_elems_from_json(search_json[key], search_key))\n    elif isinstance(search_json, list):\n        for item in search_json:\n            result.extend(get_all_elems_from_json(item, search_key))\n\n    return result\n\n\ndef check_nested_dict_keys(check_dict: dict, keys: list) -> bool:\n    """"""Checks if dictionary contains nested keys from keys list.\n\n    Args:\n        check_dict: Dictionary to check.\n        keys: Keys list. i-th nested dict of ``check_dict`` should contain dict containing (i+1)-th key\n        from the ``keys`` list by i-th key.\n\n    Returns:\n        True if dictionary contains nested keys from keys list, False otherwise.\n\n    Examples:\n        >>> check_nested_dict_keys({\'x\': {\'y\': {\'z\': 42}}}, [\'x\', \'y\', \'z\'])\n        True\n        >>> check_nested_dict_keys({\'x\': {\'y\': {\'z\': 42}}}, [\'x\', \'z\', \'y\'])\n        False\n        >>> check_nested_dict_keys({\'x\': {\'y\': 1, \'z\': 42}}, [\'x\', \'y\', \'z\'])\n        False\n\n    """"""\n    if isinstance(keys, list) and len(keys) > 0:\n        element = check_dict\n        for key in keys:\n            if isinstance(element, dict) and key in element.keys():\n                element = element[key]\n            else:\n                return False\n        return True\n    else:\n        return False\n\n\ndef jsonify_data(data: Any) -> Any:\n    """"""Replaces JSON-non-serializable objects with JSON-serializable.\n\n    Function replaces numpy arrays and numbers with python lists and numbers, tuples is replaces with lists. All other\n    object types remain the same.\n\n    Args:\n        data: Object to make JSON-serializable.\n\n    Returns:\n        Modified input data.\n\n    """"""\n    if isinstance(data, (list, tuple)):\n        result = [jsonify_data(item) for item in data]\n    elif isinstance(data, dict):\n        result = {}\n        for key in data.keys():\n            result[key] = jsonify_data(data[key])\n    elif isinstance(data, np.ndarray):\n        result = data.tolist()\n    elif isinstance(data, np.integer):\n        result = int(data)\n    elif isinstance(data, np.floating):\n        result = float(data)\n    elif callable(getattr(data, ""to_serializable_dict"", None)):\n        result = data.to_serializable_dict()\n    else:\n        result = data\n    return result\n\n\ndef chunk_generator(items_list: list, chunk_size: int) -> Generator[list, None, None]:\n    """"""Yields consecutive slices of list.\n\n    Args:\n        items_list: List to slice.\n        chunk_size: Length of slice.\n\n    Yields:\n        list: ``items_list`` consecutive slices.\n\n    """"""\n    for i in range(0, len(items_list), chunk_size):\n        yield items_list[i:i + chunk_size]\n\n\ndef update_dict_recursive(editable_dict: dict, editing_dict: Mapping) -> None:\n    """"""Updates dict recursively.\n\n    You need to use this function to update dictionary if depth of editing_dict is more then 1.\n\n    Args:\n        editable_dict: Dictionary to edit.\n        editing_dict: Dictionary containing edits.\n\n    """"""\n    for k, v in editing_dict.items():\n        if isinstance(v, collections.Mapping):\n            update_dict_recursive(editable_dict.get(k, {}), v)\n        else:\n            editable_dict[k] = v\n\n\ndef path_set_md5(url: str) -> str:\n    """"""Given a file URL, return a md5 query of the file.\n\n    Args:\n        url: A given URL.\n\n    Returns:\n        URL of the md5 file.\n\n    """"""\n    scheme, netloc, path, query_string, fragment = urlsplit(url)\n    path += \'.md5\'\n\n    return urlunsplit((scheme, netloc, path, query_string, fragment))\n\n\ndef set_query_parameter(url: str, param_name: str, param_value: str) -> str:\n    """"""Given a URL, set or replace a query parameter and return the modified URL.\n\n    Args:\n        url: A given  URL.\n        param_name: The parameter name to add.\n        param_value: The parameter value.\n\n    Returns:\n        URL with the added parameter.\n\n    """"""\n    scheme, netloc, path, query_string, fragment = urlsplit(url)\n    query_params = parse_qs(query_string)\n\n    query_params[param_name] = [param_value]\n    new_query_string = urlencode(query_params, doseq=True)\n\n    return urlunsplit((scheme, netloc, path, new_query_string, fragment))\n'"
deeppavlov/core/layers/__init__.py,0,b''
deeppavlov/core/layers/keras_layers.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.activations import softmax\nfrom tensorflow.keras.layers import Dense, Reshape, Concatenate, Lambda, Layer, Multiply\n\n\ndef expand_tile(units, axis):\n    """"""\n    Expand and tile tensor along given axis\n\n    Args:\n        units: tf tensor with dimensions [batch_size, time_steps, n_input_features]\n        axis: axis along which expand and tile. Must be 1 or 2\n\n    """"""\n    assert axis in (1, 2)\n    n_time_steps = K.int_shape(units)[1]\n    repetitions = [1, 1, 1, 1]\n    repetitions[axis] = n_time_steps\n    if axis == 1:\n        expanded = Reshape(target_shape=((1,) + K.int_shape(units)[1:]))(units)\n    else:\n        expanded = Reshape(target_shape=(K.int_shape(units)[1:2] + (1,) + K.int_shape(units)[2:]))(units)\n    return K.tile(expanded, repetitions)\n\n\ndef additive_self_attention(units, n_hidden=None, n_output_features=None, activation=None):\n    """"""\n    Compute additive self attention for time series of vectors (with batch dimension)\n            the formula: score(h_i, h_j) = <v, tanh(W_1 h_i + W_2 h_j)>\n            v is a learnable vector of n_hidden dimensionality,\n            W_1 and W_2 are learnable [n_hidden, n_input_features] matrices\n\n    Args:\n        units: tf tensor with dimensionality [batch_size, time_steps, n_input_features]\n        n_hidden: number of2784131 units in hidden representation of similarity measure\n        n_output_features: number of features in output dense layer\n        activation: activation at the output\n\n    Returns:\n        output: self attended tensor with dimensionality [batch_size, time_steps, n_output_features]\n        """"""\n    n_input_features = K.int_shape(units)[2]\n    if n_hidden is None:\n        n_hidden = n_input_features\n    if n_output_features is None:\n        n_output_features = n_input_features\n    exp1 = Lambda(lambda x: expand_tile(x, axis=1))(units)\n    exp2 = Lambda(lambda x: expand_tile(x, axis=2))(units)\n    units_pairs = Concatenate(axis=3)([exp1, exp2])\n    query = Dense(n_hidden, activation=""tanh"")(units_pairs)\n    attention = Dense(1, activation=lambda x: softmax(x, axis=2))(query)\n    attended_units = Lambda(lambda x: K.sum(attention * x, axis=2))(exp1)\n    output = Dense(n_output_features, activation=activation)(attended_units)\n    return output\n\n\ndef multiplicative_self_attention(units, n_hidden=None, n_output_features=None, activation=None):\n    """"""\n    Compute multiplicative self attention for time series of vectors (with batch dimension)\n    the formula: score(h_i, h_j) = <W_1 h_i,  W_2 h_j>,  W_1 and W_2 are learnable matrices\n    with dimensionality [n_hidden, n_input_features]\n\n    Args:\n        units: tf tensor with dimensionality [batch_size, time_steps, n_input_features]\n        n_hidden: number of units in hidden representation of similarity measure\n        n_output_features: number of features in output dense layer\n        activation: activation at the output\n\n    Returns:\n        output: self attended tensor with dimensionality [batch_size, time_steps, n_output_features]\n    """"""\n    n_input_features = K.int_shape(units)[2]\n    if n_hidden is None:\n        n_hidden = n_input_features\n    if n_output_features is None:\n        n_output_features = n_input_features\n    exp1 = Lambda(lambda x: expand_tile(x, axis=1))(units)\n    exp2 = Lambda(lambda x: expand_tile(x, axis=2))(units)\n    queries = Dense(n_hidden)(exp1)\n    keys = Dense(n_hidden)(exp2)\n    scores = Lambda(lambda x: K.sum(queries * x, axis=3, keepdims=True))(keys)\n    attention = Lambda(lambda x: softmax(x, axis=2))(scores)\n    mult = Multiply()([attention, exp1])\n    attended_units = Lambda(lambda x: K.sum(x, axis=2))(mult)\n    output = Dense(n_output_features, activation=activation)(attended_units)\n    return output\n\n\nclass MatchingLayer(Layer):\n    def __init__(self, output_dim, **kwargs):\n        self.output_dim = output_dim\n        self.W = []\n        super().__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert isinstance(input_shape, list)\n        self.W = []\n        for i in range(self.output_dim):\n            self.W.append(self.add_weight(name=\'kernel\',\n                                          shape=(1, input_shape[0][-1]),\n                                          initializer=\'uniform\',\n                                          trainable=True))\n        super().build(input_shape)  # Be sure to call this at the end\n\n    def compute_output_shape(self, input_shape):\n        assert isinstance(input_shape, list)\n        shape_a, shape_b = input_shape\n        return [(shape_a[0], shape_a[1], self.output_dim), (shape_a[0], shape_a[1], self.output_dim)]\n\n\nclass FullMatchingLayer(MatchingLayer):\n\n    def call(self, x, **kwargs):\n        assert isinstance(x, list)\n        inp_a, inp_b = x\n        last_state = K.expand_dims(inp_b[:, -1, :], 1)\n        m = []\n        for i in range(self.output_dim):\n            outp_a = inp_a * self.W[i]\n            outp_last = last_state * self.W[i]\n            outp_a = K.l2_normalize(outp_a, -1)\n            outp_last = K.l2_normalize(outp_last, -1)\n            outp = K.batch_dot(outp_a, outp_last, axes=[2, 2])\n            m.append(outp)\n        if self.output_dim > 1:\n            persp = K.concatenate(m, 2)\n        else:\n            persp = m[0]\n        return [persp, persp]\n\n\nclass MaxpoolingMatchingLayer(MatchingLayer):\n\n    def call(self, x, **kwargs):\n        assert isinstance(x, list)\n        inp_a, inp_b = x\n        m = []\n        for i in range(self.output_dim):\n            outp_a = inp_a * self.W[i]\n            outp_b = inp_b * self.W[i]\n            outp_a = K.l2_normalize(outp_a, -1)\n            outp_b = K.l2_normalize(outp_b, -1)\n            outp = K.batch_dot(outp_a, outp_b, axes=[2, 2])\n            outp = K.max(outp, -1, keepdims=True)\n            m.append(outp)\n        if self.output_dim > 1:\n            persp = K.concatenate(m, 2)\n        else:\n            persp = m[0]\n        return [persp, persp]\n\n\nclass AttentiveMatchingLayer(MatchingLayer):\n\n    def call(self, x, **kwargs):\n        assert isinstance(x, list)\n        inp_a, inp_b = x\n\n        outp_a = K.l2_normalize(inp_a, -1)\n        outp_b = K.l2_normalize(inp_b, -1)\n        alpha = K.batch_dot(outp_b, outp_a, axes=[1, 1])\n        alpha = K.l2_normalize(alpha, 1)\n        hmean = K.batch_dot(outp_b, alpha, axes=[2, 1])\n        kcon = K.eye(K.int_shape(inp_a)[1], dtype=\'float32\')\n\n        m = []\n        for i in range(self.output_dim):\n            outp_a = inp_a * self.W[i]\n            outp_hmean = hmean * self.W[i]\n            outp_a = K.l2_normalize(outp_a, -1)\n            outp_hmean = K.l2_normalize(outp_hmean, -1)\n            outp = K.batch_dot(outp_hmean, outp_a, axes=[2, 2])\n            outp = K.sum(outp * kcon, -1, keepdims=True)\n            m.append(outp)\n        if self.output_dim > 1:\n            persp = K.concatenate(m, 2)\n        else:\n            persp = m[0]\n        return [persp, persp]\n\n\nclass MaxattentiveMatchingLayer(MatchingLayer):\n\n    def call(self, x, **kwargs):\n        assert isinstance(x, list)\n        inp_a, inp_b = x\n\n        outp_a = K.l2_normalize(inp_a, -1)\n        outp_b = K.l2_normalize(inp_b, -1)\n        alpha = K.batch_dot(outp_b, outp_a, axes=[2, 2])\n        alpha = K.l2_normalize(alpha, 1)\n        alpha = K.one_hot(K.argmax(alpha, 1), K.int_shape(inp_a)[1])\n        hmax = K.batch_dot(alpha, outp_b, axes=[1, 1])\n        kcon = K.eye(K.int_shape(inp_a)[1], dtype=\'float32\')\n\n        m = []\n        for i in range(self.output_dim):\n            outp_a = inp_a * self.W[i]\n            outp_hmax = hmax * self.W[i]\n            outp_a = K.l2_normalize(outp_a, -1)\n            outp_hmax = K.l2_normalize(outp_hmax, -1)\n            outp = K.batch_dot(outp_hmax, outp_a, axes=[2, 2])\n            outp = K.sum(outp * kcon, -1, keepdims=True)\n            m.append(outp)\n        if self.output_dim > 1:\n            persp = K.concatenate(m, 2)\n        else:\n            persp = m[0]\n        return [persp, persp]\n'"
deeppavlov/core/layers/tf_attention_mechanisms.py,78,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\n\nimport tensorflow as tf\nfrom tensorflow.contrib.layers import xavier_initializer as xav\n\nfrom deeppavlov.core.layers import tf_csoftmax_attention as csoftmax_attention\n\nlog = getLogger(__name__)\n\n\ndef general_attention(key, context, hidden_size, projected_align=False):\n    """""" It is a implementation of the Luong et al. attention mechanism with general score. Based on the paper:\n        https://arxiv.org/abs/1508.04025 ""Effective Approaches to Attention-based Neural Machine Translation""\n    Args:\n        key: A tensorflow tensor with dimensionality [None, None, key_size]\n        context: A tensorflow tensor with dimensionality [None, None, max_num_tokens, token_size]\n        hidden_size: Number of units in hidden representation\n        projected_align: Using bidirectional lstm for hidden representation of context.\n        If true, beetween input and attention mechanism insert layer of bidirectional lstm with dimensionality [hidden_size].\n        If false, bidirectional lstm is not used.\n    Returns:\n        output: Tensor at the output with dimensionality [None, None, hidden_size]\n    """"""\n\n    if hidden_size % 2 != 0:\n        raise ValueError(""hidden size must be dividable by two"")\n    batch_size = tf.shape(context)[0]\n    max_num_tokens, token_size = context.get_shape().as_list()[-2:]\n    r_context = tf.reshape(context, shape=[-1, max_num_tokens, token_size])\n\n    # projected_key: [None, None, hidden_size]\n    projected_key = \\\n        tf.layers.dense(key, hidden_size, kernel_initializer=xav())\n    r_projected_key = tf.reshape(projected_key, shape=[-1, hidden_size, 1])\n\n    lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size // 2)\n    lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size // 2)\n    (output_fw, output_bw), states = \\\n        tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fw_cell,\n                                        cell_bw=lstm_bw_cell,\n                                        inputs=r_context,\n                                        dtype=tf.float32)\n    # bilstm_output: [-1, max_num_tokens, hidden_size]\n    bilstm_output = tf.concat([output_fw, output_bw], -1)\n\n    attn = tf.nn.softmax(tf.matmul(bilstm_output, r_projected_key), dim=1)\n\n    if projected_align:\n        log.info(""Using projected attention alignment"")\n        t_context = tf.transpose(bilstm_output, [0, 2, 1])\n        output = tf.reshape(tf.matmul(t_context, attn),\n                            shape=[batch_size, -1, hidden_size])\n    else:\n        log.info(""Using without projected attention alignment"")\n        t_context = tf.transpose(r_context, [0, 2, 1])\n        output = tf.reshape(tf.matmul(t_context, attn),\n                            shape=[batch_size, -1, token_size])\n    return output\n\n\ndef light_general_attention(key, context, hidden_size, projected_align=False):\n    """""" It is a implementation of the Luong et al. attention mechanism with general score. Based on the paper:\n        https://arxiv.org/abs/1508.04025 ""Effective Approaches to Attention-based Neural Machine Translation""\n    Args:\n        key: A tensorflow tensor with dimensionality [None, None, key_size]\n        context: A tensorflow tensor with dimensionality [None, None, max_num_tokens, token_size]\n        hidden_size: Number of units in hidden representation\n        projected_align: Using dense layer for hidden representation of context.\n        If true, between input and attention mechanism insert a dense layer with dimensionality [hidden_size].\n        If false, a dense layer is not used.\n    Returns:\n        output: Tensor at the output with dimensionality [None, None, hidden_size]\n    """"""\n    batch_size = tf.shape(context)[0]\n    max_num_tokens, token_size = context.get_shape().as_list()[-2:]\n    r_context = tf.reshape(context, shape=[-1, max_num_tokens, token_size])\n\n    # projected_key: [None, None, hidden_size]\n    projected_key = tf.layers.dense(key, hidden_size, kernel_initializer=xav())\n    r_projected_key = tf.reshape(projected_key, shape=[-1, hidden_size, 1])\n\n    # projected context: [None, None, hidden_size]\n    projected_context = \\\n        tf.layers.dense(r_context, hidden_size, kernel_initializer=xav())\n\n    attn = tf.nn.softmax(tf.matmul(projected_context, r_projected_key), dim=1)\n\n    if projected_align:\n        log.info(""Using projected attention alignment"")\n        t_context = tf.transpose(projected_context, [0, 2, 1])\n        output = tf.reshape(tf.matmul(t_context, attn),\n                            shape=[batch_size, -1, hidden_size])\n    else:\n        log.info(""Using without projected attention alignment"")\n        t_context = tf.transpose(r_context, [0, 2, 1])\n        output = tf.reshape(tf.matmul(t_context, attn),\n                            shape=[batch_size, -1, token_size])\n    return output\n\n\ndef cs_general_attention(key, context, hidden_size, depth, projected_align=False):\n    """""" It is a implementation of the Luong et al. attention mechanism with general score and the constrained softmax (csoftmax).\n        Based on the papers:\n        https://arxiv.org/abs/1508.04025 ""Effective Approaches to Attention-based Neural Machine Translation""\n        https://andre-martins.github.io/docs/emnlp2017_final.pdf ""Learning What\'s Easy: Fully Differentiable Neural Easy-First Taggers""\n    Args:\n        key: A tensorflow tensor with dimensionality [None, None, key_size]\n        context: A tensorflow tensor with dimensionality [None, None, max_num_tokens, token_size]\n        hidden_size: Number of units in hidden representation\n        depth: Number of csoftmax usages\n        projected_align: Using bidirectional lstm for hidden representation of context.\n        If true, beetween input and attention mechanism insert layer of bidirectional lstm with dimensionality [hidden_size].\n        If false, bidirectional lstm is not used.\n    Returns:\n        output: Tensor at the output with dimensionality [None, None, depth * hidden_size]\n    """"""\n    if hidden_size % 2 != 0:\n        raise ValueError(""hidden size must be dividable by two"")\n    key_size = tf.shape(key)[-1]\n    batch_size = tf.shape(context)[0]\n    max_num_tokens, token_size = context.get_shape().as_list()[-2:]\n    r_context = tf.reshape(context, shape=[-1, max_num_tokens, token_size])\n    # projected_context: [None, max_num_tokens, token_size]\n    projected_context = tf.layers.dense(r_context, token_size,\n                                        kernel_initializer=xav(),\n                                        name=\'projected_context\')\n\n    lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size // 2)\n    lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size // 2)\n    (output_fw, output_bw), states = \\\n        tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fw_cell,\n                                        cell_bw=lstm_bw_cell,\n                                        inputs=projected_context,\n                                        dtype=tf.float32)\n    # bilstm_output: [-1, max_num_tokens, hidden_size]\n    bilstm_output = tf.concat([output_fw, output_bw], -1)\n    h_state_for_sketch = bilstm_output\n\n    if projected_align:\n        log.info(""Using projected attention alignment"")\n        h_state_for_attn_alignment = bilstm_output\n        aligned_h_state = csoftmax_attention.attention_gen_block(\n            h_state_for_sketch, h_state_for_attn_alignment, key, depth)\n        output = \\\n            tf.reshape(aligned_h_state, shape=[batch_size, -1, depth * hidden_size])\n    else:\n        log.info(""Using without projected attention alignment"")\n        h_state_for_attn_alignment = projected_context\n        aligned_h_state = csoftmax_attention.attention_gen_block(\n            h_state_for_sketch, h_state_for_attn_alignment, key, depth)\n        output = \\\n            tf.reshape(aligned_h_state, shape=[batch_size, -1, depth * token_size])\n    return output\n\n\ndef bahdanau_attention(key, context, hidden_size, projected_align=False):\n    """""" It is a implementation of the Bahdanau et al. attention mechanism. Based on the paper:\n        https://arxiv.org/abs/1409.0473 ""Neural Machine Translation by Jointly Learning to Align and Translate""\n    Args:\n        key: A tensorflow tensor with dimensionality [None, None, key_size]\n        context: A tensorflow tensor with dimensionality [None, None, max_num_tokens, token_size]\n        hidden_size: Number of units in hidden representation\n        projected_align: Using bidirectional lstm for hidden representation of context.\n        If true, beetween input and attention mechanism insert layer of bidirectional lstm with dimensionality [hidden_size].\n        If false, bidirectional lstm is not used.\n    Returns:\n        output: Tensor at the output with dimensionality [None, None, hidden_size]\n    """"""\n    if hidden_size % 2 != 0:\n        raise ValueError(""hidden size must be dividable by two"")\n    batch_size = tf.shape(context)[0]\n    max_num_tokens, token_size = context.get_shape().as_list()[-2:]\n    r_context = tf.reshape(context, shape=[-1, max_num_tokens, token_size])\n\n    # projected_key: [None, None, hidden_size]\n    projected_key = tf.layers.dense(key, hidden_size, kernel_initializer=xav())\n    r_projected_key = \\\n        tf.tile(tf.reshape(projected_key, shape=[-1, 1, hidden_size]),\n                [1, max_num_tokens, 1])\n\n    lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size // 2)\n    lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size // 2)\n    (output_fw, output_bw), states = \\\n        tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fw_cell,\n                                        cell_bw=lstm_bw_cell,\n                                        inputs=r_context,\n                                        dtype=tf.float32)\n\n    # bilstm_output: [-1,self.max_num_tokens,_n_hidden]\n    bilstm_output = tf.concat([output_fw, output_bw], -1)\n    concat_h_state = tf.concat([r_projected_key, output_fw, output_bw], -1)\n    projected_state = \\\n        tf.layers.dense(concat_h_state, hidden_size, use_bias=False,\n                        kernel_initializer=xav())\n    score = \\\n        tf.layers.dense(tf.tanh(projected_state), units=1, use_bias=False,\n                        kernel_initializer=xav())\n\n    attn = tf.nn.softmax(score, dim=1)\n\n    if projected_align:\n        log.info(""Using projected attention alignment"")\n        t_context = tf.transpose(bilstm_output, [0, 2, 1])\n        output = tf.reshape(tf.matmul(t_context, attn),\n                            shape=[batch_size, -1, hidden_size])\n    else:\n        log.info(""Using without projected attention alignment"")\n        t_context = tf.transpose(r_context, [0, 2, 1])\n        output = tf.reshape(tf.matmul(t_context, attn),\n                            shape=[batch_size, -1, token_size])\n    return output\n\n\ndef light_bahdanau_attention(key, context, hidden_size, projected_align=False):\n    """""" It is a implementation of the Bahdanau et al. attention mechanism. Based on the paper:\n        https://arxiv.org/abs/1409.0473 ""Neural Machine Translation by Jointly Learning to Align and Translate""\n    Args:\n        key: A tensorflow tensor with dimensionality [None, None, key_size]\n        context: A tensorflow tensor with dimensionality [None, None, max_num_tokens, token_size]\n        hidden_size: Number of units in hidden representation\n        projected_align: Using dense layer for hidden representation of context.\n        If true, between input and attention mechanism insert a dense layer with dimensionality [hidden_size].\n        If false, a dense layer is not used.\n    Returns:\n        output: Tensor at the output with dimensionality [None, None, hidden_size]\n    """"""\n    batch_size = tf.shape(context)[0]\n    max_num_tokens, token_size = context.get_shape().as_list()[-2:]\n    r_context = tf.reshape(context, shape=[-1, max_num_tokens, token_size])\n\n    # projected_key: [None, None, hidden_size]\n    projected_key = tf.layers.dense(key, hidden_size, kernel_initializer=xav())\n    r_projected_key = \\\n        tf.tile(tf.reshape(projected_key, shape=[-1, 1, hidden_size]),\n                [1, max_num_tokens, 1])\n\n    # projected_context: [None, max_num_tokens, hidden_size]\n    projected_context = \\\n        tf.layers.dense(r_context, hidden_size, kernel_initializer=xav())\n    concat_h_state = tf.concat([projected_context, r_projected_key], -1)\n\n    projected_state = \\\n        tf.layers.dense(concat_h_state, hidden_size, use_bias=False,\n                        kernel_initializer=xav())\n    score = \\\n        tf.layers.dense(tf.tanh(projected_state), units=1, use_bias=False,\n                        kernel_initializer=xav())\n\n    attn = tf.nn.softmax(score, dim=1)\n\n    if projected_align:\n        log.info(""Using projected attention alignment"")\n        t_context = tf.transpose(projected_context, [0, 2, 1])\n        output = tf.reshape(tf.matmul(t_context, attn),\n                            shape=[batch_size, -1, hidden_size])\n    else:\n        log.info(""Using without projected attention alignment"")\n        t_context = tf.transpose(r_context, [0, 2, 1])\n        output = tf.reshape(tf.matmul(t_context, attn),\n                            shape=[batch_size, -1, token_size])\n    return output\n\n\ndef cs_bahdanau_attention(key, context, hidden_size, depth, projected_align=False):\n    """""" It is a implementation of the Bahdanau et al. attention mechanism. Based on the papers:\n        https://arxiv.org/abs/1409.0473 ""Neural Machine Translation by Jointly Learning to Align and Translate""\n        https://andre-martins.github.io/docs/emnlp2017_final.pdf ""Learning What\'s Easy: Fully Differentiable Neural Easy-First Taggers""\n    Args:\n        key: A tensorflow tensor with dimensionality [None, None, key_size]\n        context: A tensorflow tensor with dimensionality [None, None, max_num_tokens, token_size]\n        hidden_size: Number of units in hidden representation\n        depth: Number of csoftmax usages\n        projected_align: Using bidirectional lstm for hidden representation of context.\n        If true, beetween input and attention mechanism insert layer of bidirectional lstm with dimensionality [hidden_size].\n        If false, bidirectional lstm is not used.\n    Returns:\n        output: Tensor at the output with dimensionality [None, None, depth * hidden_size]\n    """"""\n    if hidden_size % 2 != 0:\n        raise ValueError(""hidden size must be dividable by two"")\n    batch_size = tf.shape(context)[0]\n    max_num_tokens, token_size = context.get_shape().as_list()[-2:]\n\n    r_context = tf.reshape(context, shape=[-1, max_num_tokens, token_size])\n    # projected context: [None, max_num_tokens, token_size]\n    projected_context = tf.layers.dense(r_context, token_size,\n                                        kernel_initializer=xav(),\n                                        name=\'projected_context\')\n\n    # projected_key: [None, None, hidden_size]\n    projected_key = tf.layers.dense(key, hidden_size, kernel_initializer=xav(),\n                                    name=\'projected_key\')\n    r_projected_key = \\\n        tf.tile(tf.reshape(projected_key, shape=[-1, 1, hidden_size]),\n                [1, max_num_tokens, 1])\n\n    lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size // 2)\n    lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(hidden_size // 2)\n    (output_fw, output_bw), states = \\\n        tf.nn.bidirectional_dynamic_rnn(cell_fw=lstm_fw_cell,\n                                        cell_bw=lstm_bw_cell,\n                                        inputs=projected_context,\n                                        dtype=tf.float32)\n\n    # bilstm_output: [-1, max_num_tokens, hidden_size]\n    bilstm_output = tf.concat([output_fw, output_bw], -1)\n    concat_h_state = tf.concat([r_projected_key, output_fw, output_bw], -1)\n\n    if projected_align:\n        log.info(""Using projected attention alignment"")\n        h_state_for_attn_alignment = bilstm_output\n        aligned_h_state = csoftmax_attention.attention_bah_block(\n            concat_h_state, h_state_for_attn_alignment, depth)\n        output = \\\n            tf.reshape(aligned_h_state, shape=[batch_size, -1, depth * hidden_size])\n    else:\n        log.info(""Using without projected attention alignment"")\n        h_state_for_attn_alignment = projected_context\n        aligned_h_state = csoftmax_attention.attention_bah_block(\n            concat_h_state, h_state_for_attn_alignment, depth)\n        output = \\\n            tf.reshape(aligned_h_state, shape=[batch_size, -1, depth * token_size])\n    return output\n'"
deeppavlov/core/layers/tf_csoftmax_attention.py,58,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport tensorflow as tf\n\n\ndef csoftmax_for_slice(input):\n    """""" It is a implementation of the constrained softmax (csoftmax) for slice.\n        Based on the paper:\n        https://andre-martins.github.io/docs/emnlp2017_final.pdf ""Learning What\'s Easy: Fully Differentiable Neural Easy-First Taggers"" (page 4)\n    Args:\n        input: A list of [input tensor, cumulative attention].\n    Returns:\n        output: A list of [csoftmax results, masks]\n    """"""\n\n    [ten, u] = input\n\n    shape_t = ten.shape\n    shape_u = u.shape\n\n    ten -= tf.reduce_mean(ten)\n    q = tf.exp(ten)\n    active = tf.ones_like(u, dtype=tf.int32)\n    mass = tf.constant(0, dtype=tf.float32)\n    found = tf.constant(True, dtype=tf.bool)\n\n    def loop(q_, mask, mass_, found_):\n        q_list = tf.dynamic_partition(q_, mask, 2)\n        condition_indices = tf.dynamic_partition(tf.range(tf.shape(q_)[0]), mask, 2)  # 0 element it False,\n        #  1 element if true\n\n        p = q_list[1] * (1.0 - mass_) / tf.reduce_sum(q_list[1])\n        p_new = tf.dynamic_stitch(condition_indices, [q_list[0], p])\n\n        # condition verification and mask modification\n        less_mask = tf.cast(tf.less(u, p_new), tf.int32)  # 0 when u is bigger than p, 1 when u is less than p\n        condition_indices = tf.dynamic_partition(tf.range(tf.shape(p_new)[0]), less_mask,\n                                                 2)  # 0 when u is bigger than p, 1 when u is less than p\n\n        split_p_new = tf.dynamic_partition(p_new, less_mask, 2)\n        split_u = tf.dynamic_partition(u, less_mask, 2)\n\n        alpha = tf.dynamic_stitch(condition_indices, [split_p_new[0], split_u[1]])\n        mass_ += tf.reduce_sum(split_u[1])\n\n        mask = mask * (tf.ones_like(less_mask) - less_mask)\n\n        found_ = tf.cond(tf.equal(tf.reduce_sum(less_mask), 0),\n                         lambda: False,\n                         lambda: True)\n\n        alpha = tf.reshape(alpha, q_.shape)\n\n        return alpha, mask, mass_, found_\n\n    (csoft, mask_, _, _) = tf.while_loop(cond=lambda _0, _1, _2, f: f,\n                                         body=loop,\n                                         loop_vars=(q, active, mass, found))\n\n    return [csoft, mask_]\n\n\ndef csoftmax(tensor, inv_cumulative_att):\n    """""" It is a implementation of the constrained softmax (csoftmax).\n        Based on the paper:\n        https://andre-martins.github.io/docs/emnlp2017_final.pdf ""Learning What\'s Easy: Fully Differentiable Neural Easy-First Taggers""\n    Args:\n        tensor: A tensorflow tensor is score. This tensor have dimensionality [None, n_tokens]\n        inv_cumulative_att: A inverse cumulative attention tensor with dimensionality [None, n_tokens]\n    Returns:\n        cs: Tensor at the output with dimensionality [None, n_tokens]\n    """"""\n    shape_ten = tensor.shape\n    shape_cum = inv_cumulative_att.shape\n\n    merge_tensor = [tensor, inv_cumulative_att]\n    cs, _ = tf.map_fn(csoftmax_for_slice, merge_tensor, dtype=[tf.float32, tf.float32])  # [bs, L]\n    return cs\n\n\ndef attention_gen_step(hidden_for_sketch, hidden_for_attn_alignment, sketch, key, cum_att):\n    """""" It is a implementation one step of block of the Luong et al. attention mechanism with general score and the constrained softmax (csoftmax).\n        Based on the papers:\n        https://arxiv.org/abs/1508.04025 ""Effective Approaches to Attention-based Neural Machine Translation""\n        https://andre-martins.github.io/docs/emnlp2017_final.pdf ""Learning What\'s Easy: Fully Differentiable Neural Easy-First Taggers""\n    Args:\n        hidden_for_sketch: A tensorflow tensor for a sketch computing. This tensor have dimensionality [None, max_num_tokens, sketch_hidden_size]\n        hidden_for_attn_alignment: A tensorflow tensor is aligned for output during a performing. This tensor have dimensionality [None, max_num_tokens, hidden_size_for_attn_alignment]\n        sketch: A previous step sketch tensor for a sketch computing. This tensor have dimensionality [None, sketch_hidden_size]\n        key: A tensorflow tensor with dimensionality [None, None, key_size]\n        cum_att: A cumulative attention tensor with dimensionality [None, max_num_tokens]\n    Returns:\n        next_sketch: Tensor of the current step sketch with dimensionality [None, sketch_hidden_size]\n        att: Tensor of the current step attention with dimensionality [None, max_num_tokens]\n        aligned_hidden_sketch: Tensor of aligned hidden state of current step with dimensionality [None, hidden_size_for_attn_alignment]\n    """"""\n    with tf.name_scope(\'attention_step\'):\n        sketch_dims = hidden_for_sketch.get_shape().as_list()\n        batch_size = sketch_dims[0]\n        num_tokens = sketch_dims[1]\n        hidden_size = sketch_dims[2]\n        attn_alignment_dims = hidden_for_attn_alignment.get_shape().as_list()\n        attn_alignment_hidden_size = attn_alignment_dims[2]\n\n        repeated_sketch = tf.tile(tf.reshape(sketch, [-1, 1, hidden_size]), (1, num_tokens, 1))\n        concat_mem = tf.concat([hidden_for_sketch, repeated_sketch], -1)\n\n        concat_mem = tf.reshape(concat_mem, [-1, num_tokens, 2 * hidden_size])  # dirty trick\n        reduce_mem = tf.layers.dense(concat_mem, hidden_size)\n\n        projected_key = tf.layers.dense(key, hidden_size)\n        t_key = tf.reshape(projected_key, [-1, hidden_size, 1])\n\n        score = tf.reshape(tf.matmul(reduce_mem, t_key), [-1, num_tokens])\n\n        inv_cum_att = tf.reshape(tf.ones_like(cum_att) - cum_att, [-1, num_tokens])\n        att = csoftmax(score, inv_cum_att)\n\n        t_reduce_mem = tf.transpose(reduce_mem, [0, 2, 1])\n        t_hidden_for_attn_alignment = tf.transpose(hidden_for_attn_alignment, [0, 2, 1])\n\n        r_att = tf.reshape(att, [-1, num_tokens, 1])\n\n        next_sketch = tf.squeeze(tf.matmul(t_reduce_mem, r_att), -1)\n        aligned_hidden_sketch = tf.squeeze(tf.matmul(t_hidden_for_attn_alignment, r_att), -1)\n    return next_sketch, att, aligned_hidden_sketch\n\n\ndef attention_gen_block(hidden_for_sketch, hidden_for_attn_alignment, key, attention_depth):\n    """""" It is a implementation of the Luong et al. attention mechanism with general score and the constrained softmax (csoftmax).\n        Based on the papers:\n        https://arxiv.org/abs/1508.04025 ""Effective Approaches to Attention-based Neural Machine Translation""\n        https://andre-martins.github.io/docs/emnlp2017_final.pdf ""Learning What\'s Easy: Fully Differentiable Neural Easy-First Taggers""\n    Args:\n        hidden_for_sketch: A tensorflow tensor for a sketch computing. This tensor have dimensionality [None, max_num_tokens, sketch_hidden_size]\n        hidden_for_attn_alignment: A tensorflow tensor is aligned for output during a performing. This tensor have dimensionality [None, max_num_tokens, hidden_size_for_attn_alignment]\n        key: A tensorflow tensor with dimensionality [None, None, key_size]\n        attention_depth: Number of usage csoftmax\n    Returns:\n        final_aligned_hiddens: Tensor at the output with dimensionality [1, attention_depth, hidden_size_for_attn_alignment]\n    """"""\n    with tf.name_scope(\'attention_block\'):\n        sketch_dims = tf.shape(hidden_for_sketch)\n        batch_size = sketch_dims[0]\n        num_tokens = sketch_dims[1]\n        hidden_size = sketch_dims[2]\n\n        attn_alignment_dims = tf.shape(hidden_for_attn_alignment)\n        attn_alignment_hidden_size = attn_alignment_dims[2]\n\n        sketches = [tf.zeros(shape=[batch_size, hidden_size], dtype=tf.float32)]\n        aligned_hiddens = []\n        cum_att = tf.zeros(shape=[batch_size, num_tokens])  # cumulative attention\n        for i in range(attention_depth):\n            sketch, cum_att_, aligned_hidden = attention_gen_step(hidden_for_sketch, hidden_for_attn_alignment,\n                                                                  sketches[-1], key, cum_att)\n            sketches.append(sketch)  # sketch\n            aligned_hiddens.append(aligned_hidden)  # sketch\n            cum_att += cum_att_\n        final_aligned_hiddens = tf.reshape(tf.transpose(tf.stack(aligned_hiddens), [1, 0, 2]),\n                                           [1, attention_depth, attn_alignment_hidden_size])\n    return final_aligned_hiddens\n\n\ndef attention_bah_step(hidden_for_sketch, hidden_for_attn_alignment, sketch, cum_att):\n    """""" It is a implementation one step of block of the Bahdanau et al. attention mechanism with concat score and the constrained softmax (csoftmax).\n        Based on the papers:\n            https://arxiv.org/abs/1409.0473 ""Neural Machine Translation by Jointly Learning to Align and Translate""\n            https://andre-martins.github.io/docs/emnlp2017_final.pdf ""Learning What\'s Easy: Fully Differentiable Neural Easy-First Taggers""\n    Args:\n        hidden_for_sketch: A tensorflow tensor for a sketch computing. This tensor have dimensionality [None, max_num_tokens, sketch_hidden_size]\n        hidden_for_attn_alignment: A tensorflow tensor is aligned for output during a performing. This tensor have dimensionality [None, max_num_tokens, hidden_size_for_attn_alignment]\n        sketch: A previous step sketch tensor for a sketch computing. This tensor have dimensionality [None, sketch_hidden_size]\n        key: A tensorflow tensor with dimensionality [None, None, key_size]\n        cum_att: A cumulative attention tensor with dimensionality [None, max_num_tokens]\n    Returns:\n        next_sketch: Tensor of the current step sketch with dimensionality [None, sketch_hidden_size]\n        att: Tensor of the current step attention with dimensionality [None, max_num_tokens]\n        aligned_hidden_sketch: Tensor of aligned hidden state of current step with dimensionality [None, hidden_size_for_attn_alignment]\n    """"""\n    with tf.name_scope(\'attention_step\'):\n        sketch_dims = hidden_for_sketch.get_shape().as_list()\n        batch_size = sketch_dims[0]\n        num_tokens = sketch_dims[1]\n        hidden_size = sketch_dims[2]\n        attn_alignment_dims = hidden_for_attn_alignment.get_shape().as_list()\n        attn_alignment_hidden_size = attn_alignment_dims[2]\n\n        repeated_sketch = tf.tile(tf.reshape(sketch, [-1, 1, hidden_size]), (1, num_tokens, 1))\n        concat_mem = tf.concat([hidden_for_sketch, repeated_sketch], -1)\n\n        concat_mem = tf.reshape(concat_mem, [-1, num_tokens, 2 * hidden_size])  # dirty trick\n        reduce_mem = tf.layers.dense(concat_mem, hidden_size)\n\n        score = tf.squeeze(tf.layers.dense(reduce_mem, units=1,\n                                           use_bias=False), -1)\n        inv_cum_att = tf.reshape(tf.ones_like(cum_att) - cum_att, [-1, num_tokens])\n        att = csoftmax(score, inv_cum_att)\n\n        t_reduce_mem = tf.transpose(reduce_mem, [0, 2, 1])\n        t_hidden_for_attn_alignment = tf.transpose(hidden_for_attn_alignment, [0, 2, 1])\n\n        r_att = tf.reshape(att, [-1, num_tokens, 1])\n\n        next_sketch = tf.squeeze(tf.matmul(t_reduce_mem, r_att), -1)\n        aligned_hidden_sketch = tf.squeeze(tf.matmul(t_hidden_for_attn_alignment, r_att), -1)\n    return next_sketch, att, aligned_hidden_sketch\n\n\ndef attention_bah_block(hidden_for_sketch, hidden_for_attn_alignment, attention_depth):\n    """""" It is a implementation of the Bahdanau et al. attention mechanism with concat score and the constrained softmax (csoftmax).\n        Based on the papers:\n            https://arxiv.org/abs/1409.0473 ""Neural Machine Translation by Jointly Learning to Align and Translate""\n            https://andre-martins.github.io/docs/emnlp2017_final.pdf ""Learning What\'s Easy: Fully Differentiable Neural Easy-First Taggers""\n    Args:\n        hidden_for_sketch: A tensorflow tensor for a sketch computing. This tensor have dimensionality [None, max_num_tokens, sketch_hidden_size]\n        hidden_for_attn_alignment: A tensorflow tensor is aligned for output during a performing. This tensor have dimensionality [None, max_num_tokens, hidden_size_for_attn_alignment]\n        key: A tensorflow tensor with dimensionality [None, None, key_size]\n        attention_depth: Number of usage csoftmax\n    Returns:\n        final_aligned_hiddens: Tensor at the output with dimensionality [1, attention_depth, hidden_size_for_attn_alignment]\n    """"""\n    with tf.name_scope(\'attention_block\'):\n        sketch_dims = tf.shape(hidden_for_sketch)\n        batch_size = sketch_dims[0]\n        num_tokens = sketch_dims[1]\n        hidden_size = sketch_dims[2]\n\n        attn_alignment_dims = tf.shape(hidden_for_attn_alignment)\n        attn_alignment_hidden_size = attn_alignment_dims[2]\n\n        sketches = [tf.zeros(shape=[batch_size, hidden_size], dtype=tf.float32)]\n        aligned_hiddens = []\n        cum_att = tf.zeros(shape=[batch_size, num_tokens])  # cumulative attention\n        for i in range(attention_depth):\n            sketch, cum_att_, aligned_hidden = attention_bah_step(hidden_for_sketch, hidden_for_attn_alignment,\n                                                                  sketches[-1], cum_att)\n            sketches.append(sketch)  # sketch\n            aligned_hiddens.append(aligned_hidden)  # sketch\n            cum_att += cum_att_\n        final_aligned_hiddens = tf.reshape(tf.transpose(tf.stack(aligned_hiddens), [1, 0, 2]),\n                                           [1, attention_depth, attn_alignment_hidden_size])\n    return final_aligned_hiddens\n'"
deeppavlov/core/layers/tf_layers.py,179,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import List\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom deeppavlov.core.common.check_gpu import check_gpu_existence\n\nlog = getLogger(__name__)\n\nINITIALIZER = tf.orthogonal_initializer\n\n\n# INITIALIZER = xavier_initializer\n\n\ndef stacked_cnn(units: tf.Tensor,\n                n_hidden_list: List,\n                filter_width=3,\n                use_batch_norm=False,\n                use_dilation=False,\n                training_ph=None,\n                add_l2_losses=False):\n    """""" Number of convolutional layers stacked on top of each other\n\n    Args:\n        units: a tensorflow tensor with dimensionality [None, n_tokens, n_features]\n        n_hidden_list: list with number of hidden units at the ouput of each layer\n        filter_width: width of the kernel in tokens\n        use_batch_norm: whether to use batch normalization between layers\n        use_dilation: use power of 2 dilation scheme [1, 2, 4, 8 .. ] for layers 1, 2, 3, 4 ...\n        training_ph: boolean placeholder determining whether is training phase now or not.\n            It is used only for batch normalization to determine whether to use\n            current batch average (std) or memory stored average (std)\n        add_l2_losses: whether to add l2 losses on network kernels to\n                tf.GraphKeys.REGULARIZATION_LOSSES or not\n\n    Returns:\n        units: tensor at the output of the last convolutional layer\n    """"""\n    l2_reg = tf.nn.l2_loss if add_l2_losses else None\n    for n_layer, n_hidden in enumerate(n_hidden_list):\n        if use_dilation:\n            dilation_rate = 2 ** n_layer\n        else:\n            dilation_rate = 1\n        units = tf.layers.conv1d(units,\n                                 n_hidden,\n                                 filter_width,\n                                 padding=\'same\',\n                                 dilation_rate=dilation_rate,\n                                 kernel_initializer=INITIALIZER(),\n                                 kernel_regularizer=l2_reg)\n        if use_batch_norm:\n            assert training_ph is not None\n            units = tf.layers.batch_normalization(units, training=training_ph)\n        units = tf.nn.relu(units)\n    return units\n\n\ndef dense_convolutional_network(units: tf.Tensor,\n                                n_hidden_list: List,\n                                filter_width=3,\n                                use_dilation=False,\n                                use_batch_norm=False,\n                                training_ph=None):\n    """""" Densely connected convolutional layers. Based on the paper:\n        [Gao 17] https://arxiv.org/abs/1608.06993\n\n        Args:\n            units: a tensorflow tensor with dimensionality [None, n_tokens, n_features]\n            n_hidden_list: list with number of hidden units at the ouput of each layer\n            filter_width: width of the kernel in tokens\n            use_batch_norm: whether to use batch normalization between layers\n            use_dilation: use power of 2 dilation scheme [1, 2, 4, 8 .. ] for layers 1, 2, 3, 4 ...\n            training_ph: boolean placeholder determining whether is training phase now or not.\n                It is used only for batch normalization to determine whether to use\n                current batch average (std) or memory stored average (std)\n        Returns:\n            units: tensor at the output of the last convolutional layer\n                with dimensionality [None, n_tokens, n_hidden_list[-1]]\n        """"""\n    units_list = [units]\n    for n_layer, n_filters in enumerate(n_hidden_list):\n        total_units = tf.concat(units_list, axis=-1)\n        if use_dilation:\n            dilation_rate = 2 ** n_layer\n        else:\n            dilation_rate = 1\n        units = tf.layers.conv1d(total_units,\n                                 n_filters,\n                                 filter_width,\n                                 dilation_rate=dilation_rate,\n                                 padding=\'same\',\n                                 kernel_initializer=INITIALIZER())\n        if use_batch_norm:\n            units = tf.layers.batch_normalization(units, training=training_ph)\n        units = tf.nn.relu(units)\n        units_list.append(units)\n    return units\n\n\ndef bi_rnn(units: tf.Tensor,\n           n_hidden: List,\n           cell_type=\'gru\',\n           seq_lengths=None,\n           trainable_initial_states=False,\n           use_peepholes=False,\n           name=\'Bi-\'):\n    """""" Bi directional recurrent neural network. GRU or LSTM\n\n        Args:\n            units: a tensorflow tensor with dimensionality [None, n_tokens, n_features]\n            n_hidden: list with number of hidden units in the output of each layer\n            seq_lengths: length of sequences for different length sequences in batch\n                can be None for maximum length as a length for every sample in the batch\n            cell_type: \'lstm\' or \'gru\'\n            trainable_initial_states: whether to create a special trainable variable\n                to initialize the hidden states of the network or use just zeros\n            use_peepholes: whether to use peephole connections (only \'lstm\' case affected)\n            name: what variable_scope to use for the network parameters\n\n        Returns:\n            units: tensor at the output of the last recurrent layer\n                with dimensionality [None, n_tokens, n_hidden_list[-1]]\n            last_units: tensor of last hidden states for GRU and tuple\n                of last hidden stated and last cell states for LSTM\n                dimensionality of cell states and hidden states are\n                similar and equal to [B x 2 * H], where B - batch\n                size and H is number of hidden units\n    """"""\n\n    with tf.variable_scope(name + \'_\' + cell_type.upper()):\n        if cell_type == \'gru\':\n            forward_cell = tf.nn.rnn_cell.GRUCell(n_hidden, kernel_initializer=INITIALIZER())\n            backward_cell = tf.nn.rnn_cell.GRUCell(n_hidden, kernel_initializer=INITIALIZER())\n            if trainable_initial_states:\n                initial_state_fw = tf.tile(tf.get_variable(\'init_fw_h\', [1, n_hidden]), (tf.shape(units)[0], 1))\n                initial_state_bw = tf.tile(tf.get_variable(\'init_bw_h\', [1, n_hidden]), (tf.shape(units)[0], 1))\n            else:\n                initial_state_fw = initial_state_bw = None\n        elif cell_type == \'lstm\':\n            forward_cell = tf.nn.rnn_cell.LSTMCell(n_hidden, use_peepholes=use_peepholes, initializer=INITIALIZER())\n            backward_cell = tf.nn.rnn_cell.LSTMCell(n_hidden, use_peepholes=use_peepholes, initializer=INITIALIZER())\n            if trainable_initial_states:\n                initial_state_fw = tf.nn.rnn_cell.LSTMStateTuple(\n                    tf.tile(tf.get_variable(\'init_fw_c\', [1, n_hidden]), (tf.shape(units)[0], 1)),\n                    tf.tile(tf.get_variable(\'init_fw_h\', [1, n_hidden]), (tf.shape(units)[0], 1)))\n                initial_state_bw = tf.nn.rnn_cell.LSTMStateTuple(\n                    tf.tile(tf.get_variable(\'init_bw_c\', [1, n_hidden]), (tf.shape(units)[0], 1)),\n                    tf.tile(tf.get_variable(\'init_bw_h\', [1, n_hidden]), (tf.shape(units)[0], 1)))\n            else:\n                initial_state_fw = initial_state_bw = None\n        else:\n            raise RuntimeError(\'cell_type must be either ""gru"" or ""lstm""s\')\n        (rnn_output_fw, rnn_output_bw), (fw, bw) = \\\n            tf.nn.bidirectional_dynamic_rnn(forward_cell,\n                                            backward_cell,\n                                            units,\n                                            dtype=tf.float32,\n                                            sequence_length=seq_lengths,\n                                            initial_state_fw=initial_state_fw,\n                                            initial_state_bw=initial_state_bw)\n    kernels = [var for var in forward_cell.trainable_variables +\n               backward_cell.trainable_variables if \'kernel\' in var.name]\n    for kernel in kernels:\n        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, tf.nn.l2_loss(kernel))\n    return (rnn_output_fw, rnn_output_bw), (fw, bw)\n\n\ndef stacked_bi_rnn(units: tf.Tensor,\n                   n_hidden_list: List,\n                   cell_type=\'gru\',\n                   seq_lengths=None,\n                   use_peepholes=False,\n                   name=\'RNN_layer\'):\n    """""" Stackted recurrent neural networks GRU or LSTM\n\n        Args:\n            units: a tensorflow tensor with dimensionality [None, n_tokens, n_features]\n            n_hidden_list: list with number of hidden units at the ouput of each layer\n            seq_lengths: length of sequences for different length sequences in batch\n                can be None for maximum length as a length for every sample in the batch\n            cell_type: \'lstm\' or \'gru\'\n            use_peepholes: whether to use peephole connections (only \'lstm\' case affected)\n            name: what variable_scope to use for the network parameters\n        Returns:\n            units: tensor at the output of the last recurrent layer\n                with dimensionality [None, n_tokens, n_hidden_list[-1]]\n            last_units: tensor of last hidden states for GRU and tuple\n                of last hidden stated and last cell states for LSTM\n                dimensionality of cell states and hidden states are\n                similar and equal to [B x 2 * H], where B - batch\n                size and H is number of hidden units\n    """"""\n    for n, n_hidden in enumerate(n_hidden_list):\n        with tf.variable_scope(name + \'_\' + str(n)):\n            if cell_type == \'gru\':\n                forward_cell = tf.nn.rnn_cell.GRUCell(n_hidden)\n                backward_cell = tf.nn.rnn_cell.GRUCell(n_hidden)\n            elif cell_type == \'lstm\':\n                forward_cell = tf.nn.rnn_cell.LSTMCell(n_hidden, use_peepholes=use_peepholes)\n                backward_cell = tf.nn.rnn_cell.LSTMCell(n_hidden, use_peepholes=use_peepholes)\n            else:\n                raise RuntimeError(\'cell_type must be either gru or lstm\')\n\n            (rnn_output_fw, rnn_output_bw), (fw, bw) = \\\n                tf.nn.bidirectional_dynamic_rnn(forward_cell,\n                                                backward_cell,\n                                                units,\n                                                dtype=tf.float32,\n                                                sequence_length=seq_lengths)\n            units = tf.concat([rnn_output_fw, rnn_output_bw], axis=2)\n            if cell_type == \'gru\':\n                last_units = tf.concat([fw, bw], axis=1)\n            else:\n                (c_fw, h_fw), (c_bw, h_bw) = fw, bw\n                c = tf.concat([c_fw, c_bw], axis=1)\n                h = tf.concat([h_fw, h_bw], axis=1)\n                last_units = (h, c)\n    return units, last_units\n\n\ndef u_shape(units: tf.Tensor,\n            n_hidden_list: List,\n            filter_width=7,\n            use_batch_norm=False,\n            training_ph=None):\n    """""" Network architecture inspired by One Hundred layer Tiramisu.\n        https://arxiv.org/abs/1611.09326. U-Net like.\n\n        Args:\n            units: a tensorflow tensor with dimensionality [None, n_tokens, n_features]\n            n_hidden_list: list with number of hidden units at the ouput of each layer\n            filter_width: width of the kernel in tokens\n            use_batch_norm: whether to use batch normalization between layers\n            training_ph: boolean placeholder determining whether is training phase now or not.\n                It is used only for batch normalization to determine whether to use\n                current batch average (std) or memory stored average (std)\n        Returns:\n            units: tensor at the output of the last convolutional layer\n                    with dimensionality [None, n_tokens, n_hidden_list[-1]]\n    """"""\n\n    # Bread Crumbs\n    units_for_skip_conn = []\n    conv_net_params = {\'filter_width\': filter_width,\n                       \'use_batch_norm\': use_batch_norm,\n                       \'training_ph\': training_ph}\n\n    # Go down the rabbit hole\n    for n_hidden in n_hidden_list:\n        units = stacked_cnn(units, [n_hidden], **conv_net_params)\n        units_for_skip_conn.append(units)\n        units = tf.layers.max_pooling1d(units, pool_size=2, strides=2, padding=\'same\')\n\n    units = stacked_cnn(units, [n_hidden], **conv_net_params)\n\n    # Up to the sun light\n    for down_step, n_hidden in enumerate(n_hidden_list[::-1]):\n        units = tf.expand_dims(units, axis=2)\n        units = tf.layers.conv2d_transpose(units, n_hidden, filter_width, strides=(2, 1), padding=\'same\')\n        units = tf.squeeze(units, axis=2)\n\n        # Skip connection\n        skip_units = units_for_skip_conn[-(down_step + 1)]\n        if skip_units.get_shape().as_list()[-1] != n_hidden:\n            skip_units = tf.layers.dense(skip_units, n_hidden)\n        units = skip_units + units\n\n        units = stacked_cnn(units, [n_hidden], **conv_net_params)\n    return units\n\n\ndef stacked_highway_cnn(units: tf.Tensor,\n                        n_hidden_list: List,\n                        filter_width=3,\n                        use_batch_norm=False,\n                        use_dilation=False,\n                        training_ph=None):\n    """""" Highway convolutional network. Skip connection with gating\n        mechanism.\n\n    Args:\n        units: a tensorflow tensor with dimensionality [None, n_tokens, n_features]\n        n_hidden_list: list with number of hidden units at the output of each layer\n        filter_width: width of the kernel in tokens\n        use_batch_norm: whether to use batch normalization between layers\n        use_dilation: use power of 2 dilation scheme [1, 2, 4, 8 .. ] for layers 1, 2, 3, 4 ...\n        training_ph: boolean placeholder determining whether is training phase now or not.\n            It is used only for batch normalization to determine whether to use\n            current batch average (std) or memory stored average (std)\n    Returns:\n        units: tensor at the output of the last convolutional layer\n                with dimensionality [None, n_tokens, n_hidden_list[-1]]\n    """"""\n\n    for n_layer, n_hidden in enumerate(n_hidden_list):\n        input_units = units\n        # Projection if needed\n        if input_units.get_shape().as_list()[-1] != n_hidden:\n            input_units = tf.layers.dense(input_units, n_hidden)\n        if use_dilation:\n            dilation_rate = 2 ** n_layer\n        else:\n            dilation_rate = 1\n        units = tf.layers.conv1d(units,\n                                 n_hidden,\n                                 filter_width,\n                                 padding=\'same\',\n                                 dilation_rate=dilation_rate,\n                                 kernel_initializer=INITIALIZER())\n        if use_batch_norm:\n            units = tf.layers.batch_normalization(units, training=training_ph)\n        sigmoid_gate = tf.layers.dense(input_units, 1, activation=tf.sigmoid, kernel_initializer=INITIALIZER())\n        input_units = sigmoid_gate * input_units + (1 - sigmoid_gate) * units\n        input_units = tf.nn.relu(input_units)\n    units = input_units\n    return units\n\n\ndef embedding_layer(token_indices=None,\n                    token_embedding_matrix=None,\n                    n_tokens=None,\n                    token_embedding_dim=None,\n                    name: str = None,\n                    trainable=True):\n    """""" Token embedding layer. Create matrix of for token embeddings.\n        Can be initialized with given matrix (for example pre-trained\n        with word2ve algorithm\n\n    Args:\n        token_indices: token indices tensor of type tf.int32\n        token_embedding_matrix: matrix of embeddings with dimensionality\n            [n_tokens, embeddings_dimension]\n        n_tokens: total number of unique tokens\n        token_embedding_dim: dimensionality of embeddings, typical 100..300\n        name: embedding matrix name (variable name)\n        trainable: whether to set the matrix trainable or not\n\n    Returns:\n        embedded_tokens: tf tensor of size [B, T, E], where B - batch size\n            T - number of tokens, E - token_embedding_dim\n    """"""\n    if token_embedding_matrix is not None:\n        tok_mat = token_embedding_matrix\n        if trainable:\n            Warning(\'Matrix of embeddings is passed to the embedding_layer, \'\n                    \'possibly there is a pre-trained embedding matrix. \'\n                    \'Embeddings paramenters are set to Trainable!\')\n    else:\n        tok_mat = np.random.randn(n_tokens, token_embedding_dim).astype(np.float32) / np.sqrt(token_embedding_dim)\n    tok_emb_mat = tf.Variable(tok_mat, name=name, trainable=trainable)\n    embedded_tokens = tf.nn.embedding_lookup(tok_emb_mat, token_indices)\n    return embedded_tokens\n\n\ndef character_embedding_network(char_placeholder: tf.Tensor,\n                                n_characters: int = None,\n                                emb_mat: np.array = None,\n                                char_embedding_dim: int = None,\n                                filter_widths=(3, 4, 5, 7),\n                                highway_on_top=False):\n    """""" Characters to vector. Every sequence of characters (token)\n        is embedded to vector space with dimensionality char_embedding_dim\n        Convolution plus max_pooling is used to obtain vector representations\n        of words.\n\n    Args:\n        char_placeholder: placeholder of int32 type with dimensionality [B, T, C]\n            B - batch size (can be None)\n            T - Number of tokens (can be None)\n            C - number of characters (can be None)\n        n_characters: total number of unique characters\n        emb_mat: if n_characters is not provided the emb_mat should be provided\n            it is a numpy array with dimensions [V, E], where V - vocabulary size\n            and E - embeddings dimension\n        char_embedding_dim: dimensionality of characters embeddings\n        filter_widths: array of width of kernel in convolutional embedding network\n            used in parallel\n\n    Returns:\n        embeddings: tf.Tensor with dimensionality [B, T, F],\n            where F is dimensionality of embeddings\n    """"""\n    if emb_mat is None:\n        emb_mat = np.random.randn(n_characters, char_embedding_dim).astype(np.float32) / np.sqrt(char_embedding_dim)\n    else:\n        char_embedding_dim = emb_mat.shape[1]\n    char_emb_var = tf.Variable(emb_mat, trainable=True)\n    with tf.variable_scope(\'Char_Emb_Network\'):\n        # Character embedding layer\n        c_emb = tf.nn.embedding_lookup(char_emb_var, char_placeholder)\n\n        # Character embedding network\n        conv_results_list = []\n        for filter_width in filter_widths:\n            conv_results_list.append(tf.layers.conv2d(c_emb,\n                                                      char_embedding_dim,\n                                                      (1, filter_width),\n                                                      padding=\'same\',\n                                                      kernel_initializer=INITIALIZER))\n        units = tf.concat(conv_results_list, axis=3)\n        units = tf.reduce_max(units, axis=2)\n        if highway_on_top:\n            sigmoid_gate = tf.layers.dense(units,\n                                           1,\n                                           activation=tf.sigmoid,\n                                           kernel_initializer=INITIALIZER,\n                                           kernel_regularizer=tf.nn.l2_loss)\n            deeper_units = tf.layers.dense(units,\n                                           tf.shape(units)[-1],\n                                           kernel_initializer=INITIALIZER,\n                                           kernel_regularizer=tf.nn.l2_loss)\n            units = sigmoid_gate * units + (1 - sigmoid_gate) * deeper_units\n            units = tf.nn.relu(units)\n    return units\n\n\ndef expand_tile(units, axis):\n    """"""Expand and tile tensor along given axis\n    Args:\n        units: tf tensor with dimensions [batch_size, time_steps, n_input_features]\n        axis: axis along which expand and tile. Must be 1 or 2\n\n    """"""\n    assert axis in (1, 2)\n    n_time_steps = tf.shape(units)[1]\n    repetitions = [1, 1, 1, 1]\n    repetitions[axis] = n_time_steps\n    return tf.tile(tf.expand_dims(units, axis), repetitions)\n\n\ndef additive_self_attention(units, n_hidden=None, n_output_features=None, activation=None):\n    """""" Computes additive self attention for time series of vectors (with batch dimension)\n        the formula: score(h_i, h_j) = <v, tanh(W_1 h_i + W_2 h_j)>\n        v is a learnable vector of n_hidden dimensionality,\n        W_1 and W_2 are learnable [n_hidden, n_input_features] matrices\n\n    Args:\n        units: tf tensor with dimensionality [batch_size, time_steps, n_input_features]\n        n_hidden: number of units in hidden representation of similarity measure\n        n_output_features: number of features in output dense layer\n        activation: activation at the output\n\n    Returns:\n        output: self attended tensor with dimensionality [batch_size, time_steps, n_output_features]\n    """"""\n    n_input_features = units.get_shape().as_list()[2]\n    if n_hidden is None:\n        n_hidden = n_input_features\n    if n_output_features is None:\n        n_output_features = n_input_features\n    units_pairs = tf.concat([expand_tile(units, 1), expand_tile(units, 2)], 3)\n    query = tf.layers.dense(units_pairs, n_hidden, activation=tf.tanh, kernel_initializer=INITIALIZER())\n    attention = tf.nn.softmax(tf.layers.dense(query, 1), dim=2)\n    attended_units = tf.reduce_sum(attention * expand_tile(units, 1), axis=2)\n    output = tf.layers.dense(attended_units, n_output_features, activation, kernel_initializer=INITIALIZER())\n    return output\n\n\ndef multiplicative_self_attention(units, n_hidden=None, n_output_features=None, activation=None):\n    """""" Computes multiplicative self attention for time series of vectors (with batch dimension)\n        the formula: score(h_i, h_j) = <W_1 h_i,  W_2 h_j>,  W_1 and W_2 are learnable matrices\n        with dimensionality [n_hidden, n_input_features], where <a, b> stands for a and b\n        dot product\n\n    Args:\n        units: tf tensor with dimensionality [batch_size, time_steps, n_input_features]\n        n_hidden: number of units in hidden representation of similarity measure\n        n_output_features: number of features in output dense layer\n        activation: activation at the output\n\n    Returns:\n        output: self attended tensor with dimensionality [batch_size, time_steps, n_output_features]\n    """"""\n    n_input_features = units.get_shape().as_list()[2]\n    if n_hidden is None:\n        n_hidden = n_input_features\n    if n_output_features is None:\n        n_output_features = n_input_features\n    queries = tf.layers.dense(expand_tile(units, 1), n_hidden, kernel_initializer=INITIALIZER())\n    keys = tf.layers.dense(expand_tile(units, 2), n_hidden, kernel_initializer=INITIALIZER())\n    scores = tf.reduce_sum(queries * keys, axis=3, keep_dims=True)\n    attention = tf.nn.softmax(scores, dim=2)\n    attended_units = tf.reduce_sum(attention * expand_tile(units, 1), axis=2)\n    output = tf.layers.dense(attended_units, n_output_features, activation, kernel_initializer=INITIALIZER())\n    return output\n\n\ndef cudnn_gru(units, n_hidden, n_layers=1, trainable_initial_states=False,\n              seq_lengths=None, input_initial_h=None, name=\'cudnn_gru\', reuse=False):\n    """""" Fast CuDNN GRU implementation\n\n    Args:\n        units: tf.Tensor with dimensions [B x T x F], where\n            B - batch size\n            T - number of tokens\n            F - features\n\n        n_hidden: dimensionality of hidden state\n        trainable_initial_states: whether to create a special trainable variable\n            to initialize the hidden states of the network or use just zeros\n        seq_lengths: tensor of sequence lengths with dimension [B]\n        n_layers: number of layers\n        input_initial_h: initial hidden state, tensor\n        name: name of the variable scope to use\n        reuse:whether to reuse already initialized variable\n\n    Returns:\n        h - all hidden states along T dimension,\n            tf.Tensor with dimensionality [B x T x F]\n        h_last - last hidden state, tf.Tensor with dimensionality [B x H]\n    """"""\n    with tf.variable_scope(name, reuse=reuse):\n        gru = tf.contrib.cudnn_rnn.CudnnGRU(num_layers=n_layers,\n                                            num_units=n_hidden)\n\n        if trainable_initial_states:\n            init_h = tf.get_variable(\'init_h\', [n_layers, 1, n_hidden])\n            init_h = tf.tile(init_h, (1, tf.shape(units)[0], 1))\n        else:\n            init_h = tf.zeros([n_layers, tf.shape(units)[0], n_hidden])\n\n        initial_h = input_initial_h or init_h\n\n        h, h_last = gru(tf.transpose(units, (1, 0, 2)), (initial_h,))\n        h = tf.transpose(h, (1, 0, 2))\n        h_last = tf.squeeze(h_last, axis=0)[-1]  # extract last layer state\n\n        # Extract last states if they are provided\n        if seq_lengths is not None:\n            indices = tf.stack([tf.range(tf.shape(h)[0]), seq_lengths - 1], axis=1)\n            h_last = tf.gather_nd(h, indices)\n\n        return h, h_last\n\n\ndef cudnn_compatible_gru(units, n_hidden, n_layers=1, trainable_initial_states=False,\n                         seq_lengths=None, input_initial_h=None, name=\'cudnn_gru\', reuse=False):\n    """""" CuDNN Compatible GRU implementation.\n        It should be used to load models saved with CudnnGRUCell to run on CPU.\n\n        Args:\n            units: tf.Tensor with dimensions [B x T x F], where\n                B - batch size\n                T - number of tokens\n                F - features\n\n            n_hidden: dimensionality of hidden state\n            trainable_initial_states: whether to create a special trainable variable\n                to initialize the hidden states of the network or use just zeros\n            seq_lengths: tensor of sequence lengths with dimension [B]\n            n_layers: number of layers\n            input_initial_h: initial hidden state, tensor\n            name: name of the variable scope to use\n            reuse:whether to reuse already initialized variable\n\n        Returns:\n            h - all hidden states along T dimension,\n                tf.Tensor with dimensionality [B x T x F]\n            h_last - last hidden state, tf.Tensor with dimensionality [B x H]\n        """"""\n    with tf.variable_scope(name, reuse=reuse):\n\n        if trainable_initial_states:\n            init_h = tf.get_variable(\'init_h\', [n_layers, 1, n_hidden])\n            init_h = tf.tile(init_h, (1, tf.shape(units)[0], 1))\n        else:\n            init_h = tf.zeros([n_layers, tf.shape(units)[0], n_hidden])\n\n        initial_h = input_initial_h or init_h\n\n        with tf.variable_scope(\'cudnn_gru\', reuse=reuse):\n            def single_cell(): return tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell(n_hidden)\n\n            cell = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(n_layers)])\n\n            units = tf.transpose(units, (1, 0, 2))\n\n            h, h_last = tf.nn.dynamic_rnn(cell=cell, inputs=units, time_major=True,\n                                          initial_state=tuple(tf.unstack(initial_h, axis=0)))\n            h = tf.transpose(h, (1, 0, 2))\n\n            h_last = h_last[-1]  # h_last is tuple: n_layers x batch_size x n_hidden\n\n            # Extract last states if they are provided\n            if seq_lengths is not None:\n                indices = tf.stack([tf.range(tf.shape(h)[0]), seq_lengths - 1], axis=1)\n                h_last = tf.gather_nd(h, indices)\n\n            return h, h_last\n\n\ndef cudnn_gru_wrapper(units, n_hidden, n_layers=1, trainable_initial_states=False,\n                      seq_lengths=None, input_initial_h=None, name=\'cudnn_gru\', reuse=False):\n    if check_gpu_existence():\n        return cudnn_gru(units, n_hidden, n_layers, trainable_initial_states,\n                         seq_lengths, input_initial_h, name, reuse)\n\n    log.info(\'\\nWarning! tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell is used. \'\n             \'It is okay for inference mode, but \'\n             \'if you train your model with this cell it could NOT be used with \'\n             \'tf.contrib.cudnn_rnn.CudnnGRUCell later. \'\n             )\n\n    return cudnn_compatible_gru(units, n_hidden, n_layers, trainable_initial_states,\n                                seq_lengths, input_initial_h, name, reuse)\n\n\ndef cudnn_lstm(units, n_hidden, n_layers=1, trainable_initial_states=None, seq_lengths=None, initial_h=None,\n               initial_c=None, name=\'cudnn_lstm\', reuse=False):\n    """""" Fast CuDNN LSTM implementation\n\n        Args:\n            units: tf.Tensor with dimensions [B x T x F], where\n                B - batch size\n                T - number of tokens\n                F - features\n            n_hidden: dimensionality of hidden state\n            n_layers: number of layers\n            trainable_initial_states: whether to create a special trainable variable\n                to initialize the hidden states of the network or use just zeros\n            seq_lengths: tensor of sequence lengths with dimension [B]\n            initial_h: optional initial hidden state, masks trainable_initial_states\n                if provided\n            initial_c: optional initial cell state, masks trainable_initial_states\n                if provided\n            name: name of the variable scope to use\n            reuse:whether to reuse already initialized variable\n\n\n        Returns:\n            h - all hidden states along T dimension,\n                tf.Tensor with dimensionality [B x T x F]\n            h_last - last hidden state, tf.Tensor with dimensionality [B x H]\n                where H - number of hidden units\n            c_last - last cell state, tf.Tensor with dimensionality [B x H]\n                where H - number of hidden units\n        """"""\n    with tf.variable_scope(name, reuse=reuse):\n        lstm = tf.contrib.cudnn_rnn.CudnnLSTM(num_layers=n_layers,\n                                              num_units=n_hidden)\n        if trainable_initial_states:\n            init_h = tf.get_variable(\'init_h\', [n_layers, 1, n_hidden])\n            init_h = tf.tile(init_h, (1, tf.shape(units)[0], 1))\n            init_c = tf.get_variable(\'init_c\', [n_layers, 1, n_hidden])\n            init_c = tf.tile(init_c, (1, tf.shape(units)[0], 1))\n        else:\n            init_h = init_c = tf.zeros([n_layers, tf.shape(units)[0], n_hidden])\n\n        initial_h = initial_h or init_h\n        initial_c = initial_c or init_c\n\n        h, (h_last, c_last) = lstm(tf.transpose(units, (1, 0, 2)), (initial_h, initial_c))\n        h = tf.transpose(h, (1, 0, 2))\n        h_last = h_last[-1]\n        c_last = c_last[-1]\n\n        # Extract last states if they are provided\n        if seq_lengths is not None:\n            indices = tf.stack([tf.range(tf.shape(h)[0]), seq_lengths - 1], axis=1)\n            h_last = tf.gather_nd(h, indices)\n\n        return h, (h_last, c_last)\n\n\ndef cudnn_compatible_lstm(units, n_hidden, n_layers=1, trainable_initial_states=None, seq_lengths=None, initial_h=None,\n                          initial_c=None, name=\'cudnn_lstm\', reuse=False):\n    """""" CuDNN Compatible LSTM implementation.\n        It should be used to load models saved with CudnnLSTMCell to run on CPU.\n\n        Args:\n            units: tf.Tensor with dimensions [B x T x F], where\n                B - batch size\n                T - number of tokens\n                F - features\n            n_hidden: dimensionality of hidden state\n            n_layers: number of layers\n            trainable_initial_states: whether to create a special trainable variable\n                to initialize the hidden states of the network or use just zeros\n            seq_lengths: tensor of sequence lengths with dimension [B]\n            initial_h: optional initial hidden state, masks trainable_initial_states\n                if provided\n            initial_c: optional initial cell state, masks trainable_initial_states\n                if provided\n            name: name of the variable scope to use\n            reuse:whether to reuse already initialized variable\n\n\n        Returns:\n            h - all hidden states along T dimension,\n                tf.Tensor with dimensionality [B x T x F]\n            h_last - last hidden state, tf.Tensor with dimensionality [B x H]\n                where H - number of hidden units\n            c_last - last cell state, tf.Tensor with dimensionality [B x H]\n                where H - number of hidden units\n        """"""\n\n    with tf.variable_scope(name, reuse=reuse):\n        if trainable_initial_states:\n            init_h = tf.get_variable(\'init_h\', [n_layers, 1, n_hidden])\n            init_h = tf.tile(init_h, (1, tf.shape(units)[0], 1))\n            init_c = tf.get_variable(\'init_c\', [n_layers, 1, n_hidden])\n            init_c = tf.tile(init_c, (1, tf.shape(units)[0], 1))\n        else:\n            init_h = init_c = tf.zeros([n_layers, tf.shape(units)[0], n_hidden])\n\n        initial_h = initial_h or init_h\n        initial_c = initial_c or init_c\n\n        with tf.variable_scope(\'cudnn_lstm\', reuse=reuse):\n            def single_cell(): return tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell(n_hidden)\n\n            cell = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(n_layers)])\n\n            units = tf.transpose(units, (1, 0, 2))\n\n            init = tuple([tf.nn.rnn_cell.LSTMStateTuple(ic, ih) for ih, ic in\n                          zip(tf.unstack(initial_h, axis=0), tf.unstack(initial_c, axis=0))])\n\n            h, state = tf.nn.dynamic_rnn(cell=cell, inputs=units, time_major=True, initial_state=init)\n\n            h = tf.transpose(h, (1, 0, 2))\n            h_last = state[-1].h\n            c_last = state[-1].c\n\n            # Extract last states if they are provided\n            if seq_lengths is not None:\n                indices = tf.stack([tf.range(tf.shape(h)[0]), seq_lengths - 1], axis=1)\n                h_last = tf.gather_nd(h, indices)\n\n            return h, (h_last, c_last)\n\n\ndef cudnn_lstm_wrapper(units, n_hidden, n_layers=1, trainable_initial_states=None, seq_lengths=None, initial_h=None,\n                       initial_c=None, name=\'cudnn_lstm\', reuse=False):\n    if check_gpu_existence():\n        return cudnn_lstm(units, n_hidden, n_layers, trainable_initial_states,\n                          seq_lengths, initial_h, initial_c, name, reuse)\n\n    log.info(\'\\nWarning! tf.contrib.cudnn_rnn.CudnnCompatibleLSTMCell is used. \'\n             \'It is okay for inference mode, but \'\n             \'if you train your model with this cell it could NOT be used with \'\n             \'tf.contrib.cudnn_rnn.CudnnLSTMCell later. \'\n             )\n\n    return cudnn_compatible_lstm(units, n_hidden, n_layers, trainable_initial_states,\n                                 seq_lengths, initial_h, initial_c, name, reuse)\n\n\ndef cudnn_bi_gru(units,\n                 n_hidden,\n                 seq_lengths=None,\n                 n_layers=1,\n                 trainable_initial_states=False,\n                 name=\'cudnn_bi_gru\',\n                 reuse=False):\n    """""" Fast CuDNN Bi-GRU implementation\n\n    Args:\n        units: tf.Tensor with dimensions [B x T x F], where\n            B - batch size\n            T - number of tokens\n            F - features\n        n_hidden: dimensionality of hidden state\n        seq_lengths: number of tokens in each sample in the batch\n        n_layers: number of layers\n        trainable_initial_states: whether to create a special trainable variable\n                to initialize the hidden states of the network or use just zeros\n        name: name of the variable scope to use\n        reuse:whether to reuse already initialized variable\n\n\n    Returns:\n        h - all hidden states along T dimension,\n            tf.Tensor with dimensionality [B x T x F]\n        h_last - last hidden state, tf.Tensor with dimensionality [B x H * 2]\n            where H - number of hidden units\n    """"""\n\n    with tf.variable_scope(name, reuse=reuse):\n        if seq_lengths is None:\n            seq_lengths = tf.ones([tf.shape(units)[0]], dtype=tf.int32) * tf.shape(units)[1]\n        with tf.variable_scope(\'Forward\'):\n            h_fw, h_last_fw = cudnn_gru_wrapper(units,\n                                                n_hidden,\n                                                n_layers=n_layers,\n                                                trainable_initial_states=trainable_initial_states,\n                                                seq_lengths=seq_lengths,\n                                                reuse=reuse)\n\n        with tf.variable_scope(\'Backward\'):\n            reversed_units = tf.reverse_sequence(units, seq_lengths=seq_lengths, seq_dim=1, batch_dim=0)\n            h_bw, h_last_bw = cudnn_gru_wrapper(reversed_units,\n                                                n_hidden,\n                                                n_layers=n_layers,\n                                                trainable_initial_states=trainable_initial_states,\n                                                seq_lengths=seq_lengths,\n                                                reuse=reuse)\n            h_bw = tf.reverse_sequence(h_bw, seq_lengths=seq_lengths, seq_dim=1, batch_dim=0)\n\n    return (h_fw, h_bw), (h_last_fw, h_last_bw)\n\n\ndef cudnn_bi_lstm(units,\n                  n_hidden,\n                  seq_lengths=None,\n                  n_layers=1,\n                  trainable_initial_states=False,\n                  name=\'cudnn_bi_gru\',\n                  reuse=False):\n    """""" Fast CuDNN Bi-LSTM implementation\n\n    Args:\n        units: tf.Tensor with dimensions [B x T x F], where\n            B - batch size\n            T - number of tokens\n            F - features\n        n_hidden: dimensionality of hidden state\n        seq_lengths: number of tokens in each sample in the batch\n        n_layers: number of layers\n        trainable_initial_states: whether to create a special trainable variable\n            to initialize the hidden states of the network or use just zeros\n        name: name of the variable scope to use\n        reuse:whether to reuse already initialized variable\n\n    Returns:\n        h - all hidden states along T dimension,\n            tf.Tensor with dimensionality [B x T x F]\n        h_last - last hidden state, tf.Tensor with dimensionality [B x H * 2]\n            where H - number of hidden units\n        c_last - last cell state, tf.Tensor with dimensionality [B x H * 2]\n            where H - number of hidden units\n        """"""\n    with tf.variable_scope(name, reuse=reuse):\n        if seq_lengths is None:\n            seq_lengths = tf.ones([tf.shape(units)[0]], dtype=tf.int32) * tf.shape(units)[1]\n        with tf.variable_scope(\'Forward\'):\n            h_fw, (h_fw_last, c_fw_last) = cudnn_lstm_wrapper(units,\n                                                              n_hidden,\n                                                              n_layers=n_layers,\n                                                              trainable_initial_states=trainable_initial_states,\n                                                              seq_lengths=seq_lengths)\n\n        with tf.variable_scope(\'Backward\'):\n            reversed_units = tf.reverse_sequence(units, seq_lengths=seq_lengths, seq_dim=1, batch_dim=0)\n            h_bw, (h_bw_last, c_bw_last) = cudnn_lstm_wrapper(reversed_units,\n                                                              n_hidden,\n                                                              n_layers=n_layers,\n                                                              trainable_initial_states=trainable_initial_states,\n                                                              seq_lengths=seq_lengths)\n\n            h_bw = tf.reverse_sequence(h_bw, seq_lengths=seq_lengths, seq_dim=1, batch_dim=0)\n        return (h_fw, h_bw), ((h_fw_last, c_fw_last), (h_bw_last, c_bw_last))\n\n\ndef cudnn_stacked_bi_gru(units,\n                         n_hidden,\n                         seq_lengths=None,\n                         n_stacks=2,\n                         keep_prob=1.0,\n                         concat_stacked_outputs=False,\n                         trainable_initial_states=False,\n                         name=\'cudnn_stacked_bi_gru\',\n                         reuse=False):\n    """""" Fast CuDNN Stacked Bi-GRU implementation\n\n    Args:\n        units: tf.Tensor with dimensions [B x T x F], where\n            B - batch size\n            T - number of tokens\n            F - features\n        n_hidden: dimensionality of hidden state\n        seq_lengths: number of tokens in each sample in the batch\n        n_stacks: number of stacked Bi-GRU\n        keep_prob: dropout keep_prob between Bi-GRUs (intra-layer dropout)\n        concat_stacked_outputs: return last Bi-GRU output or concat outputs from every Bi-GRU,\n        trainable_initial_states: whether to create a special trainable variable\n                to initialize the hidden states of the network or use just zeros\n        name: name of the variable scope to use\n        reuse: whether to reuse already initialized variable\n\n\n    Returns:\n        h - all hidden states along T dimension,\n            tf.Tensor with dimensionality [B x T x ((n_hidden * 2) * n_stacks)]\n    """"""\n    if seq_lengths is None:\n        seq_lengths = tf.ones([tf.shape(units)[0]], dtype=tf.int32) * tf.shape(units)[1]\n\n    outputs = [units]\n\n    with tf.variable_scope(name, reuse=reuse):\n        for n in range(n_stacks):\n\n            if n == 0:\n                inputs = outputs[-1]\n            else:\n                inputs = variational_dropout(outputs[-1], keep_prob=keep_prob)\n\n            (h_fw, h_bw), _ = cudnn_bi_gru(inputs, n_hidden, seq_lengths,\n                                           n_layers=1,\n                                           trainable_initial_states=trainable_initial_states,\n                                           name=\'{}_cudnn_bi_gru\'.format(n),\n                                           reuse=reuse)\n\n            outputs.append(tf.concat([h_fw, h_bw], axis=2))\n\n    if concat_stacked_outputs:\n        return tf.concat(outputs[1:], axis=2)\n\n    return outputs[-1]\n\n\ndef variational_dropout(units, keep_prob, fixed_mask_dims=(1,)):\n    """""" Dropout with the same drop mask for all fixed_mask_dims\n\n    Args:\n        units: a tensor, usually with shapes [B x T x F], where\n            B - batch size\n            T - tokens dimension\n            F - feature dimension\n        keep_prob: keep probability\n        fixed_mask_dims: in these dimensions the mask will be the same\n\n    Returns:\n        dropped units tensor\n    """"""\n    units_shape = tf.shape(units)\n    noise_shape = [units_shape[n] for n in range(len(units.shape))]\n    for dim in fixed_mask_dims:\n        noise_shape[dim] = 1\n    return tf.nn.dropout(units, rate=1 - keep_prob, noise_shape=noise_shape)\n'"
deeppavlov/core/models/__init__.py,0,b''
deeppavlov/core/models/component.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import ABCMeta, abstractmethod\n\nfrom logging import getLogger\n\nlog = getLogger(__name__)\n\n\nclass Component(metaclass=ABCMeta):\n    """"""Abstract class for all callables that could be used in Chainer\'s pipe.""""""\n\n    @abstractmethod\n    def __call__(self, *args, **kwargs):\n        pass\n\n    def reset(self):\n        pass\n\n    def destroy(self):\n        attr_list = list(self.__dict__.keys())\n        for attr_name in attr_list:\n            attr = getattr(self, attr_name)\n            if hasattr(attr, \'destroy\'):\n                attr.destroy()\n            delattr(self, attr_name)\n\n    def serialize(self):\n        from deeppavlov.core.models.serializable import Serializable\n        if isinstance(self, Serializable):\n            log.warning(f\'Method for {self.__class__.__name__} serialization is not implemented!\'\n                        f\' Will not be able to load without using load_path\')\n        return None\n\n    def deserialize(self, data):\n        from deeppavlov.core.models.serializable import Serializable\n        if isinstance(self, Serializable):\n            log.warning(f\'Method for {self.__class__.__name__} deserialization is not implemented!\'\n                        f\' Please, use traditional load_path for this component\')\n        pass\n'"
deeppavlov/core/models/estimator.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import abstractmethod\n\nfrom .component import Component\nfrom .serializable import Serializable\n\n\nclass Estimator(Component, Serializable):\n    """"""Abstract class for components that could be fitted on the data as a whole.""""""\n\n    @abstractmethod\n    def fit(self, *args, **kwargs):\n        pass\n'"
deeppavlov/core/models/keras_model.py,2,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import abstractmethod\nfrom logging import getLogger\n\nimport tensorflow.compat.v1 as tf\nfrom tensorflow.keras import backend as K\nfrom overrides import overrides\n\nfrom deeppavlov.core.models.lr_scheduled_model import LRScheduledModel\nfrom deeppavlov.core.models.nn_model import NNModel\nfrom deeppavlov.core.models.tf_backend import TfModelMeta\n\nlog = getLogger(__name__)\n\n\nclass KerasModel(NNModel, metaclass=TfModelMeta):\n    """"""\n    Builds Keras model with TensorFlow backend.\n\n    Attributes:\n        epochs_done: number of epochs that were done\n        batches_seen: number of epochs that were seen\n        train_examples_seen: number of training samples that were seen\n        sess: tf session\n    """"""\n\n    def __init__(self, **kwargs) -> None:\n        """"""\n        Initialize model using keyword parameters\n\n        Args:\n            kwargs: Dictionary with model parameters\n        """"""\n        self.epochs_done = 0\n        self.batches_seen = 0\n        self.train_examples_seen = 0\n\n        super().__init__(save_path=kwargs.get(""save_path""),\n                         load_path=kwargs.get(""load_path""),\n                         mode=kwargs.get(""mode""))\n\n    @staticmethod\n    def _config_session():\n        """"""\n        Configure session for particular device\n\n        Returns:\n            tensorflow.Session\n        """"""\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        config.gpu_options.visible_device_list = \'0\'\n        return tf.Session(config=config)\n\n    @abstractmethod\n    def load(self, *args, **kwargs) -> None:\n        pass\n\n    @abstractmethod\n    def save(self, *args, **kwargs) -> None:\n        pass\n\n    def process_event(self, event_name: str, data: dict) -> None:\n        """"""\n        Process event after epoch\n        Args:\n            event_name: whether event is send after epoch or batch.\n                    Set of values: ``""after_epoch"", ""after_batch""``\n            data: event data (dictionary)\n\n        Returns:\n            None\n        """"""\n        if event_name == ""after_epoch"":\n            self.epochs_done = data[""epochs_done""]\n            self.batches_seen = data[""batches_seen""]\n            self.train_examples_seen = data[""train_examples_seen""]\n        return\n\n\nclass LRScheduledKerasModel(LRScheduledModel, KerasModel):\n    """"""\n    KerasModel enhanced with optimizer, learning rate and momentum\n    management and search.\n    """"""\n\n    def __init__(self, **kwargs):\n        """"""\n        Initialize model with given parameters\n\n        Args:\n            **kwargs: dictionary of parameters\n        """"""\n        self.opt = kwargs\n        KerasModel.__init__(self, **kwargs)\n        if not(isinstance(kwargs.get(""learning_rate""), float) and isinstance(kwargs.get(""learning_rate_decay""), float)):\n            LRScheduledModel.__init__(self, **kwargs)\n\n    @abstractmethod\n    def get_optimizer(self):\n        """"""\n        Return an instance of keras optimizer\n        """"""\n        pass\n\n    @overrides\n    def _init_learning_rate_variable(self):\n        """"""\n        Initialize learning rate\n\n        Returns:\n            None\n        """"""\n        return None\n\n    @overrides\n    def _init_momentum_variable(self):\n        """"""\n        Initialize momentum\n\n        Returns:\n            None\n        """"""\n        return None\n\n    @overrides\n    def get_learning_rate_variable(self):\n        """"""\n        Extract value of learning rate from optimizer\n\n        Returns:\n            learning rate value\n        """"""\n        return self.get_optimizer().lr\n\n    @overrides\n    def get_momentum_variable(self):\n        """"""\n        Extract values of momentum variables from optimizer\n\n        Returns:\n            optimizer\'s `rho` or `beta_1`\n        """"""\n        optimizer = self.get_optimizer()\n        if hasattr(optimizer, \'rho\'):\n            return optimizer.rho\n        elif hasattr(optimizer, \'beta_1\'):\n            return optimizer.beta_1\n        return None\n\n    @overrides\n    def _update_graph_variables(self, learning_rate: float = None, momentum: float = None):\n        """"""\n        Update graph variables setting giving `learning_rate` and `momentum`\n\n        Args:\n            learning_rate: learning rate value to be set in graph (set if not None)\n            momentum: momentum value to be set in graph (set if not None)\n\n        Returns:\n            None\n        """"""\n        if learning_rate is not None:\n            K.set_value(self.get_learning_rate_variable(), learning_rate)\n            # log.info(f""Learning rate = {learning_rate}"")\n        if momentum is not None:\n            K.set_value(self.get_momentum_variable(), momentum)\n            # log.info(f""Momentum      = {momentum}"")\n\n    def process_event(self, event_name: str, data: dict):\n        """"""\n        Process event after epoch\n        Args:\n            event_name: whether event is send after epoch or batch.\n                    Set of values: ``""after_epoch"", ""after_batch""``\n            data: event data (dictionary)\n\n        Returns:\n            None\n        """"""\n        if (isinstance(self.opt.get(""learning_rate"", None), float) and\n                isinstance(self.opt.get(""learning_rate_decay"", None), float)):\n            pass\n        else:\n            if event_name == \'after_train_log\':\n                if (self.get_learning_rate_variable() is not None) and (\'learning_rate\' not in data):\n                    data[\'learning_rate\'] = float(K.get_value(self.get_learning_rate_variable()))\n                    # data[\'learning_rate\'] = self._lr\n                if (self.get_momentum_variable() is not None) and (\'momentum\' not in data):\n                    data[\'momentum\'] = float(K.get_value(self.get_momentum_variable()))\n                    # data[\'momentum\'] = self._mom\n            else:\n                super().process_event(event_name, data)\n'"
deeppavlov/core/models/lr_scheduled_model.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport math\nfrom abc import abstractmethod\nfrom enum import IntEnum\nfrom logging import getLogger\nfrom typing import Union, Tuple, List, Optional\n\nimport numpy as np\n\nfrom deeppavlov.core.common.errors import ConfigError\n\nlog = getLogger(__name__)\n\n\nclass DecayType(IntEnum):\n    """""" Data class, each decay type is assigned a number. """"""\n    NO = 1\n    LINEAR = 2\n    COSINE = 3\n    EXPONENTIAL = 4\n    POLYNOMIAL = 5\n    ONECYCLE = 6\n    TRAPEZOID = 7\n\n    @classmethod\n    def from_str(cls, label: str) -> int:\n        """"""\n        Convert given string label of decay type to special index\n\n        Args:\n            label: name of decay type.\n                Set of values: `""linear""`, `""cosine""`, `""exponential""`,\n                `""onecycle""`, `""trapezoid""`, `[""polynomial"", K]`, where K is a polynomial power\n\n        Returns:\n            index of decay type\n        """"""\n        label_norm = label.replace(\'1\', \'one\').upper()\n        if label_norm in cls.__members__:\n            return DecayType[label_norm]\n        else:\n            raise NotImplementedError\n\n\nclass DecayScheduler:\n    """"""\n    Given initial and endvalue, this class generates the next value\n    depending on decay type and number of iterations. (by calling next_val().)\n    """"""\n\n    def __init__(self, dec_type: Union[str, DecayType], start_val: float,\n                 num_it: int = 0, end_val: float = None, extra: float = None) -> None:\n        if isinstance(dec_type, DecayType):\n            self.dec_type = dec_type\n        else:\n            self.dec_type = DecayType.from_str(dec_type)\n        self.nb, self.extra = num_it, extra\n        self.start_val, self.end_val = start_val, end_val\n        self.iters = 0\n        if self.end_val is None and not (self.dec_type in [1, 4]):\n            self.end_val = 0\n        if self.dec_type == DecayType.ONECYCLE:\n            self.cycle_nb = math.ceil(self.nb / 2)\n            self.div = 1.0 if not self.start_val else self.end_val / self.start_val\n        if self.dec_type == DecayType.TRAPEZOID:\n            self.div = 1.0 if not self.start_val else self.end_val / self.start_val\n\n    def __str__(self):\n        return f""DecayScheduler(start_val={self.start_val}, end_val={self.end_val}"" \\\n               f"", dec_type={self.dec_type.name}, num_it={self.nb}, extra={self.extra})""\n\n    def next_val(self) -> float:\n        self.iters = min(self.iters + 1, self.nb)\n        # print(f""iters = {self.iters}/{self.nb}"")\n        if self.dec_type == DecayType.NO:\n            return self.start_val\n        elif self.dec_type == DecayType.LINEAR:\n            pct = self.iters / self.nb\n            return self.start_val + pct * (self.end_val - self.start_val)\n        elif self.dec_type == DecayType.COSINE:\n            cos_out = math.cos(math.pi * self.iters / self.nb) + 1\n            return self.end_val + (self.start_val - self.end_val) / 2 * cos_out\n        elif self.dec_type == DecayType.EXPONENTIAL:\n            ratio = self.end_val / self.start_val\n            return self.start_val * (ratio ** (self.iters / self.nb))\n        elif self.dec_type == DecayType.POLYNOMIAL:\n            delta_val = self.start_val - self.end_val\n            return self.end_val + delta_val * (1 - self.iters / self.nb) ** self.extra\n        elif self.dec_type == DecayType.ONECYCLE:\n            if self.iters > self.cycle_nb:\n                # decaying from end_val to start_val for cycle_nb steps\n                pct = 1 - (self.iters - self.cycle_nb) / self.cycle_nb\n                return self.start_val * (1 + pct * (self.div - 1))\n            else:\n                # raising from start_val to end_val for cycle_nb steps\n                pct = self.iters / self.cycle_nb\n                return self.start_val * (1 + pct * (self.div - 1))\n        elif self.dec_type == DecayType.TRAPEZOID:\n            if self.iters > 0.6 * self.nb:\n                # decaying from end_val to start_val for 4/10 * nb steps\n                pct = 2.5 * (self.nb - self.iters) / self.nb\n                return self.start_val * (1 + pct * (self.div - 1))\n            elif self.iters > 0.1 * self.nb:\n                # constant end_val\n                return self.end_val\n            else:\n                # raising from start_val to end_val for 1/10 * nb steps\n                pct = 10.0 * self.iters / self.nb\n                return self.start_val * (1 + pct * (self.div - 1))\n\n\nDType = Union[str, DecayType]\n\n\nclass LRScheduledModel:\n    """"""\n    Abstract model enhanced with optimizer, learning rate and momentum\n    management and search.\n\n    Args:\n        learning_rate: learning rate value or ranges\n        learning_rate_decay: learning rate decay type.\n                Set of values: `""linear""`, `""onecycle""`, `""trapezoid""`,\n                `""exponential""`, `""cosine""`, `[""polynomial"", K]`, where K is a polynomial power\n        learning_rate_decay_epochs: number of epochs for learning rate decay process\n        learning_rate_decay_batches: number of batches for learning rate decay process\n        learning_rate_drop_div: division coefficient for learning rate in case of\n                exceeding patience `learning_rate_drop_patience`\n        learning_rate_drop_patience: patience limit of loss increase\n        momentum: range of momentum values\n        momentum_decay: momentum decay type.\n                Set of values: `""linear""`, `""onecycle""`, `""trapezoid""`,\n                `""exponential""`, `""cosine""`, `[""polynomial"", K]`, where K is a polynomial power\n        momentum_decay_epochs: number of epochs for momentum decay process\n        momentum_decay_batches: number of batches for momentum decay process\n        fit_batch_size: batch size when fitting learning rate\n        fit_learning_rate: range of learning rate values to explore\n        fit_learning_rate_div: division coefficient for best learning rate obtained from fitting,\n                divided learning rate value will be used when training model\n        fit_beta: smoothing coefficient for loss calculation when fitting learning rate\n        fit_min_batches: number of batches to train model on before fitting learning rate\n        fit_max_batches: number of batches to train model on when fitting learning rate\n        load_before_drop: set True to load saved model from disk when learning\n                rate is dropped, set False to continue training current model\n        *args: other parameters\n        **kwargs: other parameters\n    """"""\n\n    @abstractmethod\n    def _init_learning_rate_variable(self):\n        pass\n\n    @abstractmethod\n    def _init_momentum_variable(self):\n        pass\n\n    @abstractmethod\n    def _update_graph_variables(self, learning_rate=None, momentum=None):\n        """"""\n        Update learning rate in graph if `learning_rate` is not None,\n        Update momentum in graph if `momentum` is not None\n        """"""\n        if learning_rate is not None:\n            # update learning rate\n            pass\n        if momentum is not None:\n            # update momentum\n            pass\n\n    def __init__(self,\n                 learning_rate: Union[None, float, Tuple[float, float]] = None,\n                 learning_rate_decay: Union[DType, Tuple[DType, float]] = DecayType.NO,\n                 learning_rate_decay_epochs: int = 0,\n                 learning_rate_decay_batches: int = 0,\n                 learning_rate_drop_div: float = 2.0,\n                 learning_rate_drop_patience: Optional[int] = None,\n                 momentum: Union[None, float, Tuple[float, float]] = None,\n                 momentum_decay: Union[DType, Tuple[DType, float]] = DecayType.NO,\n                 momentum_decay_epochs: int = 0,\n                 momentum_decay_batches: int = 0,\n                 fit_batch_size: Union[None, int, str] = None,\n                 fit_learning_rate: Tuple[float, float] = (1e-7, 100),\n                 fit_learning_rate_div: float = 10.,\n                 fit_beta: float = 0.98,\n                 fit_min_batches: int = 10,\n                 fit_max_batches: Optional[int] = None,\n                 load_before_drop: bool = False,\n                 *args, **kwargs) -> None:\n        """"""\n        Initialize learning rate scheduler\n        """"""\n        if learning_rate_decay_epochs and learning_rate_decay_batches:\n            raise ConfigError(""isn\'t able to update learning rate every batch""\n                              "" and every epoch simultaneously"")\n        if momentum_decay_epochs and momentum_decay_batches:\n            raise ConfigError(""isn\'t able to update momentum every batch""\n                              "" and every epoch simultaneously"")\n\n        start_val, end_val = learning_rate, None\n        if isinstance(learning_rate, (tuple, list)):\n            start_val, end_val = learning_rate\n        dec_type, extra = learning_rate_decay, None\n        if isinstance(learning_rate_decay, (tuple, list)):\n            dec_type, extra = learning_rate_decay\n\n        self._lr = start_val\n        num_it, self._lr_update_on_batch = learning_rate_decay_epochs, False\n        if learning_rate_decay_batches > 0:\n            num_it, self._lr_update_on_batch = learning_rate_decay_batches, True\n\n        self._lr_schedule = DecayScheduler(start_val=start_val, end_val=end_val,\n                                           num_it=num_it, dec_type=dec_type, extra=extra)\n        self._lr_var = self._init_learning_rate_variable()\n\n        start_val, end_val = momentum, None\n        if isinstance(momentum, (tuple, list)):\n            start_val, end_val = momentum\n        dec_type, extra = momentum_decay, None\n        if isinstance(momentum_decay, (tuple, list)):\n            dec_type, extra = momentum_decay\n\n        self._mom = start_val\n        num_it, self._mom_update_on_batch = momentum_decay_epochs, False\n        if momentum_decay_batches > 0:\n            num_it, self._mom_update_on_batch = momentum_decay_batches, False\n\n        self._mom_schedule = DecayScheduler(start_val=start_val, end_val=end_val,\n                                            num_it=num_it, dec_type=dec_type,\n                                            extra=extra)\n        self._mom_var = self._init_momentum_variable()\n\n        self._learning_rate_drop_patience = learning_rate_drop_patience\n        self._learning_rate_drop_div = learning_rate_drop_div\n        self._learning_rate_cur_impatience = 0.\n        self._learning_rate_last_impatience = 0.\n        self._learning_rate_cur_div = 1.\n        self._load_before_drop = load_before_drop\n        self._fit_batch_size = fit_batch_size\n        self._fit_learning_rate = fit_learning_rate\n        self._fit_learning_rate_div = fit_learning_rate_div\n        self._fit_beta = fit_beta\n        self._fit_min_batches = fit_min_batches\n        self._fit_max_batches = fit_max_batches\n\n    def get_learning_rate(self):\n        """"""\n        Return current learning rate value\n\n        Returns:\n            learning rate\n        """"""\n        if self._lr is None:\n            raise ConfigError(""Please specify `learning_rate` parameter""\n                              "" before training"")\n        return self._lr\n\n    def get_learning_rate_variable(self):\n        """"""\n        Return current learning rate variable\n\n        Returns:\n            learning rate variable\n        """"""\n        return self._lr_var\n\n    def get_momentum(self):\n        """"""\n        Return current momentum value\n\n        Returns:\n            momentum\n        """"""\n        return self._mom\n\n    def get_momentum_variable(self):\n        """"""\n        Return current momentum variable\n\n        Returns:\n            momentum variable\n        """"""\n        return self._mom_var\n\n    def fit(self, *args):\n        """"""\n        Find the best learning rate schedule, and set obtained values of learning rate\n        and momentum for further model training. Best learning rate will be divided\n        by `fit_learning_rate_div` for further training model.\n\n        Args:\n            *args: arguments\n\n        Returns:\n\n        """"""\n        data = list(zip(*args))\n        self.save()\n        if self._fit_batch_size is None:\n            raise ConfigError(""in order to use fit() method""\n                              "" set `fit_batch_size` parameter"")\n        bs = int(self._fit_batch_size)\n        data_len = len(data)\n        num_batches = self._fit_max_batches or ((data_len - 1) // bs + 1)\n\n        avg_loss = 0.\n        best_loss = float(\'inf\')\n        lrs, losses = [], []\n        _lr_find_schedule = DecayScheduler(start_val=self._fit_learning_rate[0],\n                                           end_val=self._fit_learning_rate[1],\n                                           dec_type=""exponential"",\n                                           num_it=num_batches)\n        self._lr = _lr_find_schedule.start_val\n        self._mom = 0.\n        self._update_graph_variables(learning_rate=self._lr, momentum=self._mom)\n        best_lr = _lr_find_schedule.start_val\n        for i in range(num_batches):\n            batch_start = (i * bs) % data_len\n            batch_end = batch_start + bs\n            report = self.train_on_batch(*zip(*data[batch_start:batch_end]))\n            if not isinstance(report, dict):\n                report = {\'loss\': report}\n            # Calculating smoothed loss\n            avg_loss = self._fit_beta * avg_loss + (1 - self._fit_beta) * report[\'loss\']\n            smoothed_loss = avg_loss / (1 - self._fit_beta ** (i + 1))\n            lrs.append(self._lr)\n            losses.append(smoothed_loss)\n            log.info(f""Batch {i}/{num_batches}: smooth_loss = {smoothed_loss}""\n                     f"", lr = {self._lr}, best_lr = {best_lr}"")\n            if math.isnan(smoothed_loss) or (smoothed_loss > 4 * best_loss):\n                break\n            if (smoothed_loss < best_loss) and (i >= self._fit_min_batches):\n                best_loss = smoothed_loss\n                best_lr = self._lr\n            self._lr = _lr_find_schedule.next_val()\n            self._update_graph_variables(learning_rate=self._lr)\n\n            if i >= num_batches:\n                break\n        # best_lr /= 10\n        end_val = self._get_best(lrs, losses)\n\n        start_val = end_val\n        if self._lr_schedule.dec_type in (DecayType.ONECYCLE, DecayType.TRAPEZOID):\n            start_val = end_val / self._fit_learning_rate_div\n        elif self._lr_schedule.dec_type in (DecayType.POLYNOMIAL, DecayType.EXPONENTIAL,\n                                            DecayType.LINEAR, DecayType.COSINE):\n            start_val = end_val\n            end_val = end_val / self._fit_learning_rate_div\n        self._lr_schedule = DecayScheduler(start_val=start_val,\n                                           end_val=end_val,\n                                           num_it=self._lr_schedule.nb,\n                                           dec_type=self._lr_schedule.dec_type,\n                                           extra=self._lr_schedule.extra)\n        log.info(f""Found best learning rate value = {best_lr}""\n                 f"", setting new learning rate schedule with {self._lr_schedule}."")\n\n        self.load()\n        self._lr = self._lr_schedule.start_val\n        self._mom = self._mom_schedule.start_val\n        self._update_graph_variables(learning_rate=self._lr, momentum=self._mom)\n        return {\'smoothed_loss\': losses, \'learning_rate\': lrs}\n\n    @staticmethod\n    def _get_best(values: List[float], losses: List[float],\n                  max_loss_div: float = 0.9, min_val_div: float = 10.0) -> float:\n        """"""\n        Find the best value according to given losses\n\n        Args:\n            values: list of considered values\n            losses: list of obtained loss values corresponding to `values`\n            max_loss_div: maximal divergence of loss to be considered significant\n            min_val_div: minimum divergence of loss to be considered significant\n\n        Returns:\n            best value divided by `min_val_div`\n        """"""\n        assert len(values) == len(losses), ""lengths of values and losses should be equal""\n        min_ind = np.argmin(losses)\n        for i in range(min_ind - 1, 0, -1):\n            if (losses[i] * max_loss_div > losses[min_ind]) or \\\n                    (values[i] * min_val_div < values[min_ind]):\n                return values[i + 1]\n        return values[min_ind] / min_val_div\n\n    def process_event(self, event_name: str, data: dict) -> None:\n        """"""\n        Update learning rate and momentum variables after event (given by `event_name`)\n\n        Args:\n            event_name: name of event after which the method was called.\n                    Set of values: `""after_validation""`, `""after_batch""`, `""after_epoch""`, `""after_train_log""`\n            data: dictionary with parameters values\n\n        Returns:\n            None\n        """"""\n        if event_name == ""after_validation"":\n            if data[\'impatience\'] > self._learning_rate_last_impatience:\n                self._learning_rate_cur_impatience += 1\n            else:\n                self._learning_rate_cur_impatience = 0\n\n            self._learning_rate_last_impatience = data[\'impatience\']\n\n            if (self._learning_rate_drop_patience is not None) and \\\n                    (self._learning_rate_cur_impatience >=\n                     self._learning_rate_drop_patience):\n                self._learning_rate_cur_impatience = 0\n                self._learning_rate_cur_div *= self._learning_rate_drop_div\n                self._lr /= self._learning_rate_drop_div\n                if self._load_before_drop:\n                    self.load(path=self.save_path)\n                    self._update_graph_variables(momentum=self._mom)\n                self._update_graph_variables(learning_rate=self._lr)\n                log.info(f""New learning rate dividor = {self._learning_rate_cur_div}"")\n        if event_name == \'after_batch\':\n            if (self._lr is not None) and self._lr_update_on_batch:\n                self._lr = self._lr_schedule.next_val() / self._learning_rate_cur_div\n                self._update_graph_variables(learning_rate=self._lr)\n            if (self._mom is not None) and self._mom_update_on_batch:\n                self._mom = min(1., max(0., self._mom_schedule.next_val()))\n                self._update_graph_variables(momentum=self._mom)\n        if event_name == \'after_epoch\':\n            if (self._lr is not None) and not self._lr_update_on_batch:\n                self._lr = self._lr_schedule.next_val() / self._learning_rate_cur_div\n                self._update_graph_variables(learning_rate=self._lr)\n            if (self._mom is not None) and not self._mom_update_on_batch:\n                self._mom = min(1., max(0., self._mom_schedule.next_val()))\n                self._update_graph_variables(momentum=self._mom)\n        if event_name == \'after_train_log\':\n            if (self._lr is not None) and (\'learning_rate\' not in data):\n                data[\'learning_rate\'] = self._lr\n            if (self._mom is not None) and (\'momentum\' not in data):\n                data[\'momentum\'] = self._mom\n'"
deeppavlov/core/models/nn_model.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import abstractmethod\n\nfrom .component import Component\nfrom .serializable import Serializable\n\n\nclass NNModel(Component, Serializable):\n    """"""Abstract class for deep learning components.""""""\n\n    @abstractmethod\n    def train_on_batch(self, x: list, y: list):\n        pass\n\n    def process_event(self, event_name, data):\n        pass\n'"
deeppavlov/core/models/serializable.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import ABCMeta, abstractmethod\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import Union, Optional\n\nfrom deeppavlov.core.commands.utils import expand_path\n\nlog = getLogger(__name__)\n\n\nclass Serializable(metaclass=ABCMeta):\n    """"""\n    :class:`deeppavlov.models.model.serializable.Serializable` is an abstract base class that expresses the interface\n    for all models that can serialize data to a path.\n    """"""\n\n    def __init__(self, save_path: Optional[Union[str, Path]], load_path: Optional[Union[str, Path]] = None,\n                 mode: str = \'infer\',\n                 *args, **kwargs) -> None:\n\n        if save_path:\n            self.save_path = expand_path(save_path)\n            self.save_path.parent.mkdir(parents=True, exist_ok=True)\n        else:\n            self.save_path = None\n\n        if load_path:\n            self.load_path = expand_path(load_path)\n            if mode != \'train\' and self.save_path and self.load_path != self.save_path:\n                log.warning(""Load path \'{}\' differs from save path \'{}\' in \'{}\' mode for {}.""\n                            .format(self.load_path, self.save_path, mode, self.__class__.__name__))\n        elif mode != \'train\' and self.save_path:\n            self.load_path = self.save_path\n            log.warning(""No load path is set for {} in \'{}\' mode. Using save path instead""\n                        .format(self.__class__.__name__, mode))\n        else:\n            self.load_path = None\n            log.warning(""No load path is set for {}!"".format(self.__class__.__name__))\n\n    @abstractmethod\n    def save(self, *args, **kwargs):\n        pass\n\n    @abstractmethod\n    def load(self, *args, **kwargs):\n        pass\n'"
deeppavlov/core/models/tf_backend.py,4,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import ABCMeta\nfrom functools import wraps\n\nimport tensorflow.compat.v1 as tf\nfrom six import with_metaclass\n\n\ndef _graph_wrap(func, graph):\n    """"""Constructs function encapsulated in the graph.""""""\n\n    @wraps(func)\n    def _wrapped(*args, **kwargs):\n        with graph.as_default():\n            return func(*args, **kwargs)\n\n    return _wrapped\n\n\ndef _keras_wrap(func, session):\n    """"""Constructs function encapsulated in the graph and the session.""""""\n    @wraps(func)\n    def _wrapped(*args, **kwargs):\n        with session.graph.as_default():\n            tf.keras.backend.set_session(session)\n            return func(*args, **kwargs)\n\n    return _wrapped\n\n\nclass TfModelMeta(with_metaclass(type, ABCMeta)):\n    """"""Metaclass that helps all child classes to have their own graph and session.""""""\n\n    def __call__(cls, *args, **kwargs):\n        obj = cls.__new__(cls, *args, **kwargs)\n        from .keras_model import KerasModel\n        if issubclass(cls, KerasModel):\n            from tensorflow.keras import backend as K\n            if K.backend() != \'tensorflow\':\n                obj.__init__(*args, **kwargs)\n                return obj\n\n            K.clear_session()\n            obj.graph = tf.Graph()\n            with obj.graph.as_default():\n                if hasattr(cls, \'_config_session\'):\n                    obj.sess = cls._config_session()\n                else:\n                    obj.sess = tf.Session()\n        else:\n            obj.graph = tf.Graph()\n\n        for meth in dir(obj):\n            if meth == \'__class__\':\n                continue\n            attr = getattr(obj, meth)\n            if callable(attr):\n                if issubclass(cls, KerasModel):\n                    wrapped_attr = _keras_wrap(attr, obj.sess)\n                else:\n                    wrapped_attr = _graph_wrap(attr, obj.graph)\n                setattr(obj, meth, wrapped_attr)\n        obj.__init__(*args, **kwargs)\n        return obj\n'"
deeppavlov/core/models/tf_model.py,30,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom collections import defaultdict\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import Iterable, Union, Tuple, Optional\n\nimport numpy as np\nimport tensorflow as tf\nfrom overrides import overrides\nfrom tensorflow.python.ops import variables\n\nfrom deeppavlov.core.common.errors import ConfigError\nfrom deeppavlov.core.common.registry import cls_from_str\nfrom deeppavlov.core.models.lr_scheduled_model import LRScheduledModel\nfrom deeppavlov.core.models.nn_model import NNModel\nfrom deeppavlov.core.models.tf_backend import TfModelMeta\n\nlog = getLogger(__name__)\n\n\nclass TFModel(NNModel, metaclass=TfModelMeta):\n    """"""Parent class for all components using TensorFlow.""""""\n\n    sess: tf.Session\n\n    def __init__(self, *args, **kwargs) -> None:\n        super().__init__(*args, **kwargs)\n\n    def load(self, exclude_scopes: tuple = (\'Optimizer\',), path: Union[Path, str] = None) -> None:\n        """"""Load model parameters from self.load_path""""""\n        if not hasattr(self, \'sess\'):\n            raise RuntimeError(\'Your TensorFlow model {} must\'\n                               \' have sess attribute!\'.format(self.__class__.__name__))\n        path = path or self.load_path\n        path = str(Path(path).resolve())\n        # Check presence of the model files\n        if tf.train.checkpoint_exists(path):\n            log.info(\'[loading model from {}]\'.format(path))\n            # Exclude optimizer variables from saved variables\n            var_list = self._get_saveable_variables(exclude_scopes)\n            saver = tf.train.Saver(var_list)\n            saver.restore(self.sess, path)\n\n    def deserialize(self, weights: Iterable[Tuple[str, np.ndarray]]) -> None:\n        assign_ops = []\n        feed_dict = {}\n        for var_name, value in weights:\n            var = self.sess.graph.get_tensor_by_name(var_name)\n            value = np.asarray(value)\n            assign_placeholder = tf.placeholder(var.dtype, shape=value.shape)\n            assign_op = tf.assign(var, assign_placeholder)\n            assign_ops.append(assign_op)\n            feed_dict[assign_placeholder] = value\n        self.sess.run(assign_ops, feed_dict=feed_dict)\n\n    def save(self, exclude_scopes: tuple = (\'Optimizer\',)) -> None:\n        """"""Save model parameters to self.save_path""""""\n        if not hasattr(self, \'sess\'):\n            raise RuntimeError(\'Your TensorFlow model {} must\'\n                               \' have sess attribute!\'.format(self.__class__.__name__))\n        path = str(self.save_path.resolve())\n        log.info(\'[saving model to {}]\'.format(path))\n        var_list = self._get_saveable_variables(exclude_scopes)\n        saver = tf.train.Saver(var_list)\n        saver.save(self.sess, path)\n\n    def serialize(self) -> Tuple[Tuple[str, np.ndarray], ...]:\n        tf_vars = tf.global_variables()\n        values = self.sess.run(tf_vars)\n        return tuple(zip([var.name for var in tf_vars], values))\n\n    @staticmethod\n    def _get_saveable_variables(exclude_scopes=tuple()):\n        # noinspection PyProtectedMember\n        all_vars = variables._all_saveable_objects()\n        vars_to_train = [var for var in all_vars if all(sc not in var.name for sc in exclude_scopes)]\n        return vars_to_train\n\n    @staticmethod\n    def _get_trainable_variables(exclude_scopes=tuple()):\n        all_vars = tf.global_variables()\n        vars_to_train = [var for var in all_vars if all(sc not in var.name for sc in exclude_scopes)]\n        return vars_to_train\n\n    def get_train_op(self,\n                     loss,\n                     learning_rate,\n                     optimizer=None,\n                     clip_norm=None,\n                     learnable_scopes=None,\n                     optimizer_scope_name=None,\n                     **kwargs):\n        """"""\n        Get train operation for given loss\n\n        Args:\n            loss: loss, tf tensor or scalar\n            learning_rate: scalar or placeholder.\n            clip_norm: clip gradients norm by clip_norm.\n            learnable_scopes: which scopes are trainable (None for all).\n            optimizer: instance of tf.train.Optimizer, default Adam.\n            **kwargs: parameters passed to tf.train.Optimizer object\n               (scalars or placeholders).\n\n        Returns:\n            train_op\n        """"""\n        if optimizer_scope_name is None:\n            opt_scope = tf.variable_scope(\'Optimizer\')\n        else:\n            opt_scope = tf.variable_scope(optimizer_scope_name)\n        with opt_scope:\n            if learnable_scopes is None:\n                variables_to_train = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n            else:\n                variables_to_train = []\n                for scope_name in learnable_scopes:\n                    variables_to_train.extend(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope_name))\n\n            if optimizer is None:\n                optimizer = tf.train.AdamOptimizer\n\n            # For batch norm it is necessary to update running averages\n            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n            with tf.control_dependencies(extra_update_ops):\n\n                def clip_if_not_none(grad):\n                    if grad is not None:\n                        return tf.clip_by_norm(grad, clip_norm)\n\n                opt = optimizer(learning_rate, **kwargs)\n                grads_and_vars = opt.compute_gradients(loss, var_list=variables_to_train)\n                if clip_norm is not None:\n                    grads_and_vars = [(clip_if_not_none(grad), var)\n                                      for grad, var in grads_and_vars]\n                train_op = opt.apply_gradients(grads_and_vars)\n        return train_op\n\n    @staticmethod\n    def print_number_of_parameters():\n        """"""\n        Print number of *trainable* parameters in the network\n        """"""\n        log.info(\'Number of parameters: \')\n        variables = tf.trainable_variables()\n        blocks = defaultdict(int)\n        for var in variables:\n            # Get the top level scope name of variable\n            block_name = var.name.split(\'/\')[0]\n            number_of_parameters = np.prod(var.get_shape().as_list())\n            blocks[block_name] += number_of_parameters\n        for block_name, cnt in blocks.items():\n            log.info(""{} - {}."".format(block_name, cnt))\n        total_num_parameters = np.sum(list(blocks.values()))\n        log.info(\'Total number of parameters equal {}\'.format(total_num_parameters))\n\n    def destroy(self):\n        if hasattr(self, \'sess\'):\n            for k in list(self.sess.graph.get_all_collection_keys()):\n                self.sess.graph.clear_collection(k)\n        super().destroy()\n\n\nclass LRScheduledTFModel(TFModel, LRScheduledModel):\n    """"""\n    TFModel enhanced with optimizer, learning rate and momentum\n    management and search.\n    """"""\n\n    def __init__(self,\n                 optimizer: str = \'AdamOptimizer\',\n                 clip_norm: float = None,\n                 momentum: float = None,\n                 **kwargs) -> None:\n        TFModel.__init__(self, **kwargs)\n\n        try:\n            self._optimizer = cls_from_str(optimizer)\n        except Exception:\n            self._optimizer = getattr(tf.train, optimizer.split(\':\')[-1])\n        if not issubclass(self._optimizer, tf.train.Optimizer):\n            raise ConfigError(""`optimizer` should be tensorflow.train.Optimizer subclass"")\n        self._clip_norm = clip_norm\n\n        LRScheduledModel.__init__(self, momentum=momentum, **kwargs)\n\n    @overrides\n    def _init_learning_rate_variable(self):\n        return tf.Variable(self._lr or 0., dtype=tf.float32, name=\'learning_rate\')\n\n    @overrides\n    def _init_momentum_variable(self):\n        return tf.Variable(self._mom or 0., dtype=tf.float32, name=\'momentum\')\n\n    @overrides\n    def _update_graph_variables(self, learning_rate=None, momentum=None):\n        if learning_rate is not None:\n            self.sess.run(tf.assign(self._lr_var, learning_rate))\n            # log.info(f""Learning rate = {learning_rate}"")\n        if momentum is not None:\n            self.sess.run(tf.assign(self._mom_var, momentum))\n            # log.info(f""Momentum      = {momentum}"")\n\n    def get_train_op(self,\n                     loss,\n                     learning_rate: Union[float, tf.placeholder] = None,\n                     optimizer: tf.train.Optimizer = None,\n                     momentum: Union[float, tf.placeholder] = None,\n                     clip_norm: float = None,\n                     **kwargs):\n        if learning_rate is not None:\n            kwargs[\'learning_rate\'] = learning_rate\n        else:\n            kwargs[\'learning_rate\'] = self._lr_var\n        kwargs[\'optimizer\'] = optimizer or self.get_optimizer()\n        kwargs[\'clip_norm\'] = clip_norm or self._clip_norm\n\n        momentum_param = \'momentum\'\n        if kwargs[\'optimizer\'] == tf.train.AdamOptimizer:\n            momentum_param = \'beta1\'\n        elif kwargs[\'optimizer\'] == tf.train.AdadeltaOptimizer:\n            momentum_param = \'rho\'\n\n        if momentum is not None:\n            kwargs[momentum_param] = momentum\n        elif self.get_momentum() is not None:\n            kwargs[momentum_param] = self._mom_var\n        return TFModel.get_train_op(self, loss, **kwargs)\n\n    def get_optimizer(self):\n        return self._optimizer\n\n    def load(self,\n             exclude_scopes: Optional[Iterable] = (\'Optimizer\',\n                                                   \'learning_rate\',\n                                                   \'momentum\'),\n             **kwargs):\n        return super().load(exclude_scopes=exclude_scopes, **kwargs)\n\n    def process_event(self, *args, **kwargs):\n        LRScheduledModel.process_event(self, *args, **kwargs)\n'"
deeppavlov/core/trainers/__init__.py,0,"b'# Copyright 2019 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .fit_trainer import FitTrainer\nfrom .nn_trainer import NNTrainer\n'"
deeppavlov/core/trainers/fit_trainer.py,4,"b'# Copyright 2019 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport datetime\nimport json\nimport time\nfrom itertools import islice\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import Tuple, Dict, Union, Optional, Iterable, Any, Collection\n\nfrom deeppavlov.core.commands.infer import build_model\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.chainer import Chainer\nfrom deeppavlov.core.common.params import from_params\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.data_fitting_iterator import DataFittingIterator\nfrom deeppavlov.core.data.data_learning_iterator import DataLearningIterator\nfrom deeppavlov.core.models.estimator import Estimator\nfrom deeppavlov.core.trainers.utils import Metric, parse_metrics, prettify_metrics\n\nlog = getLogger(__name__)\n\n\n@register(\'fit_trainer\')\nclass FitTrainer:\n    """"""\n    Trainer class for fitting and evaluating :class:`Estimators <deeppavlov.core.models.estimator.Estimator>`\n\n    Args:\n        chainer_config: ``""chainer""`` block of a configuration file\n        batch_size: batch_size to use for partial fitting (if available) and evaluation,\n            the whole dataset is used if ``batch_size`` is negative or zero (default is ``-1``)\n        metrics: iterable of metrics where each metric can be a registered metric name or a dict of ``name`` and\n            ``inputs`` where ``name`` is a registered metric name and ``inputs`` is a collection of parameter names\n            from chainer\xe2\x80\x99s inner memory that will be passed to the metric function;\n            default value for ``inputs`` parameter is a concatenation of chainer\xe2\x80\x99s ``in_y`` and ``out`` fields\n            (default is ``(\'accuracy\',)``)\n        evaluation_targets: data types on which to evaluate trained pipeline (default is ``(\'valid\', \'test\')``)\n        show_examples: a flag used to print inputs, expected outputs and predicted outputs for the last batch\n            in evaluation logs (default is ``False``)\n        tensorboard_log_dir: path to a directory where tensorboard logs can be stored, ignored if None\n            (default is ``None``)\n        max_test_batches: maximum batches count for pipeline testing and evaluation, ignored if negative\n            (default is ``-1``)\n        **kwargs: additional parameters whose names will be logged but otherwise ignored\n    """"""\n\n    def __init__(self, chainer_config: dict, *, batch_size: int = -1,\n                 metrics: Iterable[Union[str, dict]] = (\'accuracy\',),\n                 evaluation_targets: Iterable[str] = (\'valid\', \'test\'),\n                 show_examples: bool = False,\n                 tensorboard_log_dir: Optional[Union[str, Path]] = None,\n                 max_test_batches: int = -1,\n                 **kwargs) -> None:\n        if kwargs:\n            log.info(f\'{self.__class__.__name__} got additional init parameters {list(kwargs)} that will be ignored:\')\n        self.chainer_config = chainer_config\n        self._chainer = Chainer(chainer_config[\'in\'], chainer_config[\'out\'], chainer_config.get(\'in_y\'))\n        self.batch_size = batch_size\n        self.metrics = parse_metrics(metrics, self._chainer.in_y, self._chainer.out_params)\n        self.evaluation_targets = tuple(evaluation_targets)\n        self.show_examples = show_examples\n\n        self.max_test_batches = None if max_test_batches < 0 else max_test_batches\n\n        self.tensorboard_log_dir: Optional[Path] = tensorboard_log_dir\n        if tensorboard_log_dir is not None:\n            try:\n                # noinspection PyPackageRequirements\n                # noinspection PyUnresolvedReferences\n                import tensorflow\n            except ImportError:\n                log.warning(\'TensorFlow could not be imported, so tensorboard log directory\'\n                            f\'`{self.tensorboard_log_dir}` will be ignored\')\n                self.tensorboard_log_dir = None\n            else:\n                self.tensorboard_log_dir = expand_path(tensorboard_log_dir)\n                self._tf = tensorflow\n\n        self._built = False\n        self._saved = False\n        self._loaded = False\n\n    def fit_chainer(self, iterator: Union[DataFittingIterator, DataLearningIterator]) -> None:\n        """"""\n        Build the pipeline :class:`~deeppavlov.core.common.chainer.Chainer` and successively fit\n        :class:`Estimator <deeppavlov.core.models.estimator.Estimator>` components using a provided data iterator\n        """"""\n        if self._built:\n            raise RuntimeError(\'Cannot fit already built chainer\')\n        for component_index, component_config in enumerate(self.chainer_config[\'pipe\'], 1):\n            component = from_params(component_config, mode=\'train\')\n            if \'fit_on\' in component_config:\n                component: Estimator\n\n                targets = component_config[\'fit_on\']\n                if isinstance(targets, str):\n                    targets = [targets]\n\n                if self.batch_size > 0 and callable(getattr(component, \'partial_fit\', None)):\n                    writer = None\n\n                    for i, (x, y) in enumerate(iterator.gen_batches(self.batch_size, shuffle=False)):\n                        preprocessed = self._chainer.compute(x, y, targets=targets)\n                        # noinspection PyUnresolvedReferences\n                        result = component.partial_fit(*preprocessed)\n\n                        if result is not None and self.tensorboard_log_dir is not None:\n                            if writer is None:\n                                writer = self._tf.summary.FileWriter(str(self.tensorboard_log_dir /\n                                                                         f\'partial_fit_{component_index}_log\'))\n                            for name, score in result.items():\n                                summary = self._tf.Summary()\n                                summary.value.add(tag=\'partial_fit/\' + name, simple_value=score)\n                                writer.add_summary(summary, i)\n                            writer.flush()\n                else:\n                    preprocessed = self._chainer.compute(*iterator.get_instances(), targets=targets)\n                    if len(targets) == 1:\n                        preprocessed = [preprocessed]\n                    result: Optional[Dict[str, Iterable[float]]] = component.fit(*preprocessed)\n\n                    if result is not None and self.tensorboard_log_dir is not None:\n                        writer = self._tf.summary.FileWriter(str(self.tensorboard_log_dir /\n                                                                 f\'fit_log_{component_index}\'))\n                        for name, scores in result.items():\n                            for i, score in enumerate(scores):\n                                summary = self._tf.Summary()\n                                summary.value.add(tag=\'fit/\' + name, simple_value=score)\n                                writer.add_summary(summary, i)\n                        writer.flush()\n\n                component.save()\n\n            if \'in\' in component_config:\n                c_in = component_config[\'in\']\n                c_out = component_config[\'out\']\n                in_y = component_config.get(\'in_y\', None)\n                main = component_config.get(\'main\', False)\n                self._chainer.append(component, c_in, c_out, in_y, main)\n        self._built = True\n\n    def _load(self) -> None:\n        if not self._loaded:\n            self._chainer.destroy()\n            self._chainer = build_model({\'chainer\': self.chainer_config}, load_trained=self._saved)\n            self._loaded = True\n\n    def get_chainer(self) -> Chainer:\n        """"""Returns a :class:`~deeppavlov.core.common.chainer.Chainer` built from ``self.chainer_config`` for inference""""""\n        self._load()\n        return self._chainer\n\n    def train(self, iterator: Union[DataFittingIterator, DataLearningIterator]) -> None:\n        """"""Calls :meth:`~fit_chainer` with provided data iterator as an argument""""""\n        self.fit_chainer(iterator)\n        self._saved = True\n\n    def test(self, data: Iterable[Tuple[Collection[Any], Collection[Any]]],\n             metrics: Optional[Collection[Metric]] = None, *,\n             start_time: Optional[float] = None, show_examples: Optional[bool] = None) -> dict:\n        """"""\n        Calculate metrics and return reports on provided data for currently stored\n        :class:`~deeppavlov.core.common.chainer.Chainer`\n\n        Args:\n            data: iterable of batches of inputs and expected outputs\n            metrics: collection of metrics namedtuples containing names for report, metric functions\n                and their inputs names (if omitted, ``self.metrics`` is used)\n            start_time: start time for test report\n            show_examples: a flag used to return inputs, expected outputs and predicted outputs for the last batch\n                in a result report (if omitted, ``self.show_examples`` is used)\n\n        Returns:\n            a report dict containing calculated metrics, spent time value, examples count in tested data\n            and maybe examples\n        """"""\n\n        if start_time is None:\n            start_time = time.time()\n        if show_examples is None:\n            show_examples = self.show_examples\n        if metrics is None:\n            metrics = self.metrics\n\n        expected_outputs = list(set().union(self._chainer.out_params, *[m.inputs for m in metrics]))\n\n        outputs = {out: [] for out in expected_outputs}\n        examples = 0\n\n        data = islice(data, self.max_test_batches)\n\n        for x, y_true in data:\n            examples += len(x)\n            y_predicted = list(self._chainer.compute(list(x), list(y_true), targets=expected_outputs))\n            if len(expected_outputs) == 1:\n                y_predicted = [y_predicted]\n            for out, val in zip(outputs.values(), y_predicted):\n                out += list(val)\n\n        if examples == 0:\n            log.warning(\'Got empty data iterable for scoring\')\n            return {\'eval_examples_count\': 0, \'metrics\': None, \'time_spent\': str(datetime.timedelta(seconds=0))}\n\n        # metrics_values = [(m.name, m.fn(*[outputs[i] for i in m.inputs])) for m in metrics]\n        metrics_values = []\n        for metric in metrics:\n            value = metric.fn(*[outputs[i] for i in metric.inputs])\n            metrics_values.append((metric.alias, value))\n\n        report = {\n            \'eval_examples_count\': examples,\n            \'metrics\': prettify_metrics(metrics_values),\n            \'time_spent\': str(datetime.timedelta(seconds=round(time.time() - start_time + 0.5)))\n        }\n\n        if show_examples:\n            y_predicted = zip(*[y_predicted_group\n                                for out_name, y_predicted_group in zip(expected_outputs, y_predicted)\n                                if out_name in self._chainer.out_params])\n            if len(self._chainer.out_params) == 1:\n                y_predicted = [y_predicted_item[0] for y_predicted_item in y_predicted]\n            report[\'examples\'] = [{\n                \'x\': x_item,\n                \'y_predicted\': y_predicted_item,\n                \'y_true\': y_true_item\n            } for x_item, y_predicted_item, y_true_item in zip(x, y_predicted, y_true)]\n\n        return report\n\n    def evaluate(self, iterator: DataLearningIterator, evaluation_targets: Optional[Iterable[str]] = None, *,\n                 print_reports: bool = True) -> Dict[str, dict]:\n        """"""\n        Run :meth:`test` on multiple data types using provided data iterator\n        \n        Args:\n            iterator: :class:`~deeppavlov.core.data.data_learning_iterator.DataLearningIterator` used for evaluation\n            evaluation_targets: iterable of data types to evaluate on\n            print_reports: a flag used to print evaluation reports as json lines\n\n        Returns:\n            a dictionary with data types as keys and evaluation reports as values\n        """"""\n        self._load()\n        if evaluation_targets is None:\n            evaluation_targets = self.evaluation_targets\n\n        res = {}\n\n        for data_type in evaluation_targets:\n            data_gen = iterator.gen_batches(self.batch_size, data_type=data_type, shuffle=False)\n            report = self.test(data_gen)\n            res[data_type] = report\n            if print_reports:\n                print(json.dumps({data_type: report}, ensure_ascii=False))\n\n        return res\n'"
deeppavlov/core/trainers/nn_trainer.py,4,"b'# Copyright 2019 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport datetime\nimport json\nimport time\nfrom itertools import islice\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import List, Tuple, Union, Optional, Iterable\n\nfrom deeppavlov.core.common.errors import ConfigError\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.data_learning_iterator import DataLearningIterator\nfrom deeppavlov.core.trainers.fit_trainer import FitTrainer\nfrom deeppavlov.core.trainers.utils import parse_metrics\n\nlog = getLogger(__name__)\n\n\n@register(\'nn_trainer\')\nclass NNTrainer(FitTrainer):\n    """"""\n    | Bases :class:`~deeppavlov.core.trainers.FitTrainer`\n    | Trainer class for training and evaluating pipelines containing\n      :class:`Estimators <deeppavlov.core.models.estimator.Estimator>`\n      and an :class:`~deeppavlov.core.models.nn_model.NNModel`\n\n    Args:\n        chainer_config: ``""chainer""`` block of a configuration file\n        batch_size: batch_size to use for partial fitting (if available) and evaluation,\n            the whole dataset is used if ``batch_size`` is negative or zero (default is ``1``)\n        epochs: maximum epochs number to train the pipeline, ignored if negative or zero (default is ``-1``)\n        start_epoch_num: starting epoch number for reports (default is ``0``)\n        max_batches: maximum batches number to train the pipeline, ignored if negative or zero (default is ``-1``)\n        metrics: iterable of metrics where each metric can be a registered metric name or a dict of ``name`` and\n            ``inputs`` where ``name`` is a registered metric name and ``inputs`` is a collection of parameter names\n            from chainer\xe2\x80\x99s inner memory that will be passed to the metric function;\n            default value for ``inputs`` parameter is a concatenation of chainer\xe2\x80\x99s ``in_y`` and ``out`` fields;\n            the first metric is used for early stopping (default is ``(\'accuracy\',)``)\n        train_metrics: metrics calculated for train logs (if omitted, ``metrics`` argument is used)\n        metric_optimization: one of ``\'maximize\'`` or ``\'minimize\'`` \xe2\x80\x94 strategy for metric optimization used in early\n            stopping (default is ``\'maximize\'``)\n        evaluation_targets: data types on which to evaluate a trained pipeline (default is ``(\'valid\', \'test\')``)\n        show_examples: a flag used to print inputs, expected outputs and predicted outputs for the last batch\n            in evaluation logs (default is ``False``)\n        tensorboard_log_dir: path to a directory where tensorboard logs can be stored, ignored if None\n            (default is ``None``)\n        validate_first: flag used to calculate metrics on the ``\'valid\'`` data type before starting training\n            (default is ``True``)\n        validation_patience: how many times in a row the validation metric has to not improve for early stopping,\n            ignored if negative or zero (default is ``5``)\n        val_every_n_epochs: how often (in epochs) to validate the pipeline, ignored if negative or zero\n            (default is ``-1``)\n        val_every_n_batches: how often (in batches) to validate the pipeline, ignored if negative or zero\n            (default is ``-1``)\n        log_every_n_epochs: how often (in epochs) to calculate metrics on train data, ignored if negative or zero\n            (default is ``-1``)\n        log_every_n_batches: how often (in batches) to calculate metrics on train data, ignored if negative or zero\n            (default is ``-1``)\n        log_on_k_batches: count of random train batches to calculate metrics in log (default is ``1``)\n        max_test_batches: maximum batches count for pipeline testing and evaluation, overrides ``log_on_k_batches``,\n            ignored if negative (default is ``-1``)\n        **kwargs: additional parameters whose names will be logged but otherwise ignored\n\n\n    Trainer saves the model if it sees progress in scores. The full rules look like following:\n\n    - For the validation savepoint:\n        * 0-th validation (optional). Don\'t save model, establish a baseline.\n        * 1-th validation.\n             + If we have a baseline, save the model if we see an improvement, don\'t save otherwise.\n             + If we don\'t have a baseline, save the model.\n        * 2nd and later validations. Save the model if we see an improvement\n    - For the at-train-exit savepoint:\n        * Save the model if it happened before 1st validation (to capture early training results), don\'t save otherwise.\n\n    """"""\n\n    def __init__(self, chainer_config: dict, *, batch_size: int = 1,\n                 epochs: int = -1,\n                 start_epoch_num: int = 0,\n                 max_batches: int = -1,\n                 metrics: Iterable[Union[str, dict]] = (\'accuracy\',),\n                 train_metrics: Optional[Iterable[Union[str, dict]]] = None,\n                 metric_optimization: str = \'maximize\',\n                 evaluation_targets: Iterable[str] = (\'valid\', \'test\'),\n                 show_examples: bool = False,\n                 tensorboard_log_dir: Optional[Union[str, Path]] = None,\n                 max_test_batches: int = -1,\n                 validate_first: bool = True,\n                 validation_patience: int = 5, val_every_n_epochs: int = -1, val_every_n_batches: int = -1,\n                 log_every_n_batches: int = -1, log_every_n_epochs: int = -1, log_on_k_batches: int = 1,\n                 **kwargs) -> None:\n        super().__init__(chainer_config, batch_size=batch_size, metrics=metrics, evaluation_targets=evaluation_targets,\n                         show_examples=show_examples, tensorboard_log_dir=tensorboard_log_dir,\n                         max_test_batches=max_test_batches, **kwargs)\n        if train_metrics is None:\n            self.train_metrics = self.metrics\n        else:\n            self.train_metrics = parse_metrics(train_metrics, self._chainer.in_y, self._chainer.out_params)\n\n        metric_optimization = metric_optimization.strip().lower()\n        self.score_best = None\n\n        def _improved(op):\n            return lambda score, baseline: False if baseline is None or score is None \\\n                else op(score, baseline)\n\n        if metric_optimization == \'maximize\':\n            self.improved = _improved(lambda a, b: a > b)\n        elif metric_optimization == \'minimize\':\n            self.improved = _improved(lambda a, b: a < b)\n        else:\n            raise ConfigError(\'metric_optimization has to be one of {}\'.format([\'maximize\', \'minimize\']))\n\n        self.validate_first = validate_first\n        self.validation_number = 0 if validate_first else 1\n        self.validation_patience = validation_patience\n        self.val_every_n_epochs = val_every_n_epochs\n        self.val_every_n_batches = val_every_n_batches\n        self.log_every_n_epochs = log_every_n_epochs\n        self.log_every_n_batches = log_every_n_batches\n        self.log_on_k_batches = log_on_k_batches if log_on_k_batches >= 0 else None\n\n        self.max_epochs = epochs\n        self.epoch = start_epoch_num\n        self.max_batches = max_batches\n\n        self.train_batches_seen = 0\n        self.examples = 0\n        self.patience = 0\n        self.last_result = {}\n        self.losses = []\n        self.start_time: Optional[float] = None\n\n        if self.tensorboard_log_dir is not None:\n            self.tb_train_writer = self._tf.summary.FileWriter(str(self.tensorboard_log_dir / \'train_log\'))\n            self.tb_valid_writer = self._tf.summary.FileWriter(str(self.tensorboard_log_dir / \'valid_log\'))\n\n    def save(self) -> None:\n        if self._loaded:\n            raise RuntimeError(\'Cannot save already finalized chainer\')\n\n        self._chainer.save()\n\n    def _is_initial_validation(self):\n        return self.validation_number == 0\n\n    def _is_first_validation(self):\n        return self.validation_number == 1\n\n    def _validate(self, iterator: DataLearningIterator,\n                  tensorboard_tag: Optional[str] = None, tensorboard_index: Optional[int] = None) -> None:\n        self._send_event(event_name=\'before_validation\')\n        report = self.test(iterator.gen_batches(self.batch_size, data_type=\'valid\', shuffle=False),\n                           start_time=self.start_time)\n\n        report[\'epochs_done\'] = self.epoch\n        report[\'batches_seen\'] = self.train_batches_seen\n        report[\'train_examples_seen\'] = self.examples\n\n        metrics = list(report[\'metrics\'].items())\n\n        if tensorboard_tag is not None and self.tensorboard_log_dir is not None:\n            summary = self._tf.Summary()\n            for name, score in metrics:\n                summary.value.add(tag=f\'{tensorboard_tag}/{name}\', simple_value=score)\n            if tensorboard_index is None:\n                tensorboard_index = self.train_batches_seen\n            self.tb_valid_writer.add_summary(summary, tensorboard_index)\n            self.tb_valid_writer.flush()\n\n        m_name, score = metrics[0]\n\n        # Update the patience\n        if self.score_best is None:\n            self.patience = 0\n        else:\n            if self.improved(score, self.score_best):\n                self.patience = 0\n            else:\n                self.patience += 1\n\n        # Run the validation model-saving logic\n        if self._is_initial_validation():\n            log.info(\'Initial best {} of {}\'.format(m_name, score))\n            self.score_best = score\n        elif self._is_first_validation() and self.score_best is None:\n            log.info(\'First best {} of {}\'.format(m_name, score))\n            self.score_best = score\n            log.info(\'Saving model\')\n            self.save()\n        elif self.improved(score, self.score_best):\n            log.info(\'Improved best {} of {}\'.format(m_name, score))\n            self.score_best = score\n            log.info(\'Saving model\')\n            self.save()\n        else:\n            log.info(\'Did not improve on the {} of {}\'.format(m_name, self.score_best))\n\n        report[\'impatience\'] = self.patience\n        if self.validation_patience > 0:\n            report[\'patience_limit\'] = self.validation_patience\n\n        self._send_event(event_name=\'after_validation\', data=report)\n        report = {\'valid\': report}\n        print(json.dumps(report, ensure_ascii=False))\n        self.validation_number += 1\n\n    def _log(self, iterator: DataLearningIterator,\n             tensorboard_tag: Optional[str] = None, tensorboard_index: Optional[int] = None) -> None:\n        self._send_event(event_name=\'before_log\')\n        if self.log_on_k_batches == 0:\n            report = {\n                \'time_spent\': str(datetime.timedelta(seconds=round(time.time() - self.start_time + 0.5)))\n            }\n        else:\n            data = islice(iterator.gen_batches(self.batch_size, data_type=\'train\', shuffle=True),\n                          self.log_on_k_batches)\n            report = self.test(data, self.train_metrics, start_time=self.start_time)\n\n        report.update({\n            \'epochs_done\': self.epoch,\n            \'batches_seen\': self.train_batches_seen,\n            \'train_examples_seen\': self.examples\n        })\n\n        metrics: List[Tuple[str, float]] = list(report.get(\'metrics\', {}).items()) + list(self.last_result.items())\n\n        report.update(self.last_result)\n        if self.losses:\n            report[\'loss\'] = sum(self.losses) / len(self.losses)\n            self.losses.clear()\n            metrics.append((\'loss\', report[\'loss\']))\n\n        if metrics and self.tensorboard_log_dir is not None:\n            summary = self._tf.Summary()\n\n            for name, score in metrics:\n                summary.value.add(tag=f\'{tensorboard_tag}/{name}\', simple_value=score)\n            self.tb_train_writer.add_summary(summary, tensorboard_index)\n            self.tb_train_writer.flush()\n\n        self._send_event(event_name=\'after_train_log\', data=report)\n\n        report = {\'train\': report}\n        print(json.dumps(report, ensure_ascii=False))\n\n    def _send_event(self, event_name: str, data: Optional[dict] = None) -> None:\n        report = {\n            \'time_spent\': str(datetime.timedelta(seconds=round(time.time() - self.start_time + 0.5))),\n            \'epochs_done\': self.epoch,\n            \'batches_seen\': self.train_batches_seen,\n            \'train_examples_seen\': self.examples\n        }\n        if data is not None:\n            report.update(data)\n        self._chainer.process_event(event_name=event_name, data=report)\n\n    def train_on_batches(self, iterator: DataLearningIterator) -> None:\n        """"""Train pipeline on batches using provided data iterator and initialization parameters""""""\n        self.start_time = time.time()\n        if self.validate_first:\n            self._validate(iterator)\n\n        while True:\n            impatient = False\n            self._send_event(event_name=\'before_train\')\n            for x, y_true in iterator.gen_batches(self.batch_size, data_type=\'train\'):\n                self.last_result = self._chainer.train_on_batch(x, y_true)\n                if self.last_result is None:\n                    self.last_result = {}\n                elif not isinstance(self.last_result, dict):\n                    self.last_result = {\'loss\': self.last_result}\n                if \'loss\' in self.last_result:\n                    self.losses.append(self.last_result.pop(\'loss\'))\n\n                self.train_batches_seen += 1\n                self.examples += len(x)\n\n                if self.log_every_n_batches > 0 and self.train_batches_seen % self.log_every_n_batches == 0:\n                    self._log(iterator, tensorboard_tag=\'every_n_batches\', tensorboard_index=self.train_batches_seen)\n\n                if self.val_every_n_batches > 0 and self.train_batches_seen % self.val_every_n_batches == 0:\n                    self._validate(iterator,\n                                   tensorboard_tag=\'every_n_batches\', tensorboard_index=self.train_batches_seen)\n\n                self._send_event(event_name=\'after_batch\')\n\n                if 0 < self.max_batches <= self.train_batches_seen:\n                    impatient = True\n                    break\n\n                if 0 < self.validation_patience <= self.patience:\n                    log.info(\'Ran out of patience\')\n                    impatient = True\n                    break\n\n            if impatient:\n                break\n\n            self.epoch += 1\n\n            if self.log_every_n_epochs > 0 and self.epoch % self.log_every_n_epochs == 0:\n                self._log(iterator, tensorboard_tag=\'every_n_epochs\', tensorboard_index=self.epoch)\n\n            if self.val_every_n_epochs > 0 and self.epoch % self.val_every_n_epochs == 0:\n                self._validate(iterator, tensorboard_tag=\'every_n_epochs\', tensorboard_index=self.epoch)\n\n            self._send_event(event_name=\'after_epoch\')\n\n            if 0 < self.max_epochs <= self.epoch:\n                break\n\n            if 0 < self.validation_patience <= self.patience:\n                log.info(\'Ran out of patience\')\n                break\n\n    def train(self, iterator: DataLearningIterator) -> None:\n        """"""Call :meth:`~fit_chainer` and then :meth:`~train_on_batches` with provided data iterator as an argument""""""\n        self.fit_chainer(iterator)\n        if callable(getattr(self._chainer, \'train_on_batch\', None)):\n            try:\n                self.train_on_batches(iterator)\n            except KeyboardInterrupt:\n                log.info(\'Stopped training\')\n        else:\n            log.warning(f\'Using {self.__class__.__name__} for a pipeline without batched training\')\n\n        # Run the at-train-exit model-saving logic\n        if self.validation_number < 1:\n            log.info(\'Save model to capture early training results\')\n            self.save()\n'"
deeppavlov/core/trainers/utils.py,0,"b'# Copyright 2019 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom collections import OrderedDict, namedtuple\nfrom typing import List, Tuple, Union, Iterable, Any\n\nfrom deeppavlov.core.common.metrics_registry import get_metric_by_name\n\nMetric = namedtuple(\'Metric\', [\'name\', \'fn\', \'inputs\', \'alias\'])\n\n\ndef parse_metrics(metrics: Iterable[Union[str, dict]], in_y: List[str], out_vars: List[str]) -> List[Metric]:\n    metrics_functions = []\n    for metric in metrics:\n        if isinstance(metric, str):\n            metric = {\'name\': metric, \'alias\': metric}\n\n        metric_name = metric[\'name\']\n        alias = metric.get(\'alias\', metric_name)\n\n        f = get_metric_by_name(metric_name)\n\n        inputs = metric.get(\'inputs\', in_y + out_vars)\n        if isinstance(inputs, str):\n            inputs = [inputs]\n        \n\n        metrics_functions.append(Metric(metric_name, f, inputs, alias))\n    return metrics_functions\n\n\ndef prettify_metrics(metrics: List[Tuple[str, float]], precision: int = 4) -> OrderedDict:\n    """"""Prettifies the dictionary of metrics.""""""\n    prettified_metrics = OrderedDict()\n    for key, value in metrics:\n        if key in prettified_metrics:\n            Warning(""Multiple metrics with the same name {}."".format(key))\n        if isinstance(value, float):\n            value = round(value, precision)\n        prettified_metrics[key] = value\n    return prettified_metrics\n'"
deeppavlov/deprecated/agent/__init__.py,0,"b'from .agent import Agent, SkillWrapper\nfrom .filter import Filter\nfrom .processor import Processor\nfrom .rich_content import RichControl, RichMessage\n'"
deeppavlov/deprecated/agent/agent.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import ABCMeta, abstractmethod\nfrom collections import defaultdict\nfrom typing import List, Dict, Tuple, Optional\n\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.utils.connector.dialog_logger import DialogLogger\n\n\nclass Agent(Component, metaclass=ABCMeta):\n    """"""Abstract class for agents.\n\n    Agent is an entity which receives inputs from the outer word, processes\n    them and returns response to each input. Usually agent implements real-life\n    task, business or user case. Agent encapsulates skills instances, management\n    of skills inference and skills inference results processing. Also agent\n    provides management both for history and state for each utterance and uses\n    only incoming utterances IDs to distinguish them.\n\n    Args:\n        skills: List of initiated agent skills instances.\n\n    Attributes:\n        skills: List of initiated Skill or Component instances.\n            Components API should should implement API of Skill abstract class.\n        history: Histories for each each dialog with agent indexed\n            by dialog ID. Each history is represented by list of incoming\n            and outcoming replicas of the dialog casted to str and updated automatically.\n        states: States for each skill with agent indexed by dialog ID. Each\n            state updated automatically after each wrapped skill inference.\n            So we highly recommend use this attribute only for reading and\n            not to use it for your custom skills management.\n        wrapped_skills: Skills wrapped to SkillWrapper objects. SkillWrapper\n            object gives to Skill __call__ signature of Agent __call__ and\n            handles automatic state management for skill. All skills are\n            wrapped to SkillsWrapper automatically during agent initialisation.\n            We highly recommend to use wrapped skills for skills inference.\n        dialog_logger: DeepPavlov dialog logging facility.\n    """"""\n\n    def __init__(self, skills: List[Component]) -> None:\n        self.skills = skills\n        self.history: Dict = defaultdict(list)\n        self.states: Dict = defaultdict(lambda: [None] * len(self.skills))\n        self.wrapped_skills: List[SkillWrapper] = \\\n            [SkillWrapper(skill, skill_id, self) for skill_id, skill in enumerate(self.skills)]\n        self.dialog_logger: DialogLogger = DialogLogger()\n\n    def __call__(self, utterances_batch: list, utterances_ids: Optional[list] = None) -> list:\n        """"""Wraps _call method and updates utterances history.\n\n        Args:\n            utterances_batch: Batch of incoming utterances.\n            utterances_ids: Batch of dialog IDs corresponding to incoming utterances.\n\n        Returns:\n            responses: A batch of responses corresponding to the\n                utterance batch received by agent.\n        """"""\n        responses_batch = self._call(utterances_batch, utterances_ids)\n\n        batch_size = len(utterances_batch)\n        ids = utterances_ids or list(range(batch_size))\n\n        for utt_batch_idx, utt_id in enumerate(ids):\n            self.history[utt_id].append(str(utterances_batch[utt_batch_idx]))\n            self.dialog_logger.log_in(utterances_batch[utt_batch_idx], utt_id)\n\n            self.history[utt_id].append(str(responses_batch[utt_batch_idx]))\n            self.dialog_logger.log_out(responses_batch[utt_batch_idx], utt_id)\n\n        return responses_batch\n\n    @abstractmethod\n    def _call(self, utterances_batch: list, utterances_ids: Optional[list] = None) -> list:\n        """"""Processes batch of utterances and returns corresponding responses batch.\n\n        Each call of Agent processes incoming utterances and returns response\n        for each utterance Batch of dialog IDs can be provided, in other case\n        utterances indexes in incoming batch are used as dialog IDs.\n\n        Args:\n            utterances_batch: Batch of incoming utterances.\n            utterances_ids: Batch of dialog IDs corresponding to incoming utterances.\n\n        Returns:\n            responses: A batch of responses corresponding to the\n                utterance batch received by agent.\n        """"""\n        pass\n\n\nclass SkillWrapper:\n    """"""Skill instances wrapper for internal use in Agent.\n\n    SkillWrapper gives to skill interface of Agent and handles automatic state\n    management for skill.\n\n    Args:\n        skill: Wrapped skill.\n        skill_id: Skill index in Agent.skills list.\n        agent: Agent instance.\n\n    Attributes:\n        skill: Wrapped skill.\n        skill_id: Skill index in Agent.skills list.\n        agent: Agent instance.\n    """"""\n\n    def __init__(self, skill: Component, skill_id: int, agent: Agent) -> None:\n        self.skill = skill\n        self.skill_id = skill_id\n        self.agent = agent\n\n    def __call__(self, utterances_batch: list, utterances_ids: Optional[list] = None) -> Tuple[list, list]:\n        """"""Wraps __call__ method of Skill instance.\n\n            Provides skill __call__ with signature of Agent __call__ and handles\n            automatic state management for skill.\n\n        Args:\n            utterances_batch: Batch of incoming utterances.\n            utterances_ids: Batch of dialog IDs corresponding to incoming utterances.\n\n        Returns:\n            response: A batch of arbitrary typed skill inference results.\n            confidence: A batch of float typed confidence levels for each of\n                skill inference result.\n            states: Optional. A batch of arbitrary typed states for each\n                response.\n        """"""\n        history_batch = [self.agent.history[utt_id] for utt_id in utterances_ids]\n        states_batch = [self.agent.states[utt_id][self.skill_id] for utt_id in utterances_ids]\n\n        predicted, confidence, *states = self.skill(utterances_batch, history_batch, states_batch)\n\n        states = states[0] if states else [None] * len(predicted)\n        for utt_id, state in zip(utterances_ids, states):\n            self.agent.states[utt_id][self.skill_id] = state\n\n        return predicted, confidence\n'"
deeppavlov/deprecated/agent/filter.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import ABCMeta, abstractmethod\n\nfrom deeppavlov.core.models.component import Component\n\n\nclass Filter(Component, metaclass=ABCMeta):\n    """"""Abstract class for filters. Filter is a DeepPavlov component,\n    which is used in Agent to select utterances from incoming batch\n    to be processed for each Agent skill.\n    """"""\n\n    @abstractmethod\n    def __call__(self, utterances_batch: list, history_batch: list) -> list:\n        """"""Returns skills-utterances application matrix.\n\n        Returns skills-utterances application matrix which contains\n        information about Agent skills to be applied to each utterance\n        from incoming batch.\n\n        Args:\n            utterances_batch: A batch of utterances of any type.\n            history_batch: A batch of list typed histories\n                for each utterance.\n\n        Returns:\n            response: Skills-utterances application matrix,\n            for example:\n            [[True, False, True, True],\n             [False, True, True, True]]\n            Where each inner dict corresponds to one of the Agent\n            skills and each value in the inner dict contains information\n            about whether the skill will be applied to the utterance\n            with the same position in the utterances_batch.\n\n        """"""\n        pass\n'"
deeppavlov/deprecated/agent/processor.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import ABCMeta, abstractmethod\n\nfrom deeppavlov.core.models.component import Component\n\n\nclass Processor(Component, metaclass=ABCMeta):\n    """"""Abstract class for processors. Processor is a DeepPavlov component,\n    which is used in Agent to process skills responses and give one final\n    response for each utterance.\n    """"""\n\n    # TODO: change *responses to [[], [], ...] argument\n    @abstractmethod\n    def __call__(self, utterances_batch: list, history_batch: list, *responses: list) -> list:\n        """"""Returns final response for each incoming utterance.\n\n        Processes Agent skills and generates one final response for each\n        utterance in incoming batch.\n\n        Args:\n            utterances_batch: A batch of utterances of any type\n            history_batch: A batch of list typed histories\n                for each utterance\n            responses: Each response positional argument corresponds to\n                response of one of Agent skills and is represented by\n                batch (list) of (response, confidence) tuple structures.\n\n        Returns:\n            responses: A batch of responses corresponding to the\n                utterance batch received by agent.\n        """"""\n        pass\n'"
deeppavlov/deprecated/agent/rich_content.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import ABCMeta, abstractmethod\nfrom typing import Union\n\n\nclass RichItem(metaclass=ABCMeta):\n    """"""Base class for rich content elements.\n\n    Every rich content element\n    is presumed to return its state (including state of nested controls)\n    at least in json format (mandatory) as well as in the formats compatible\n    with other channels.\n    """"""\n\n    @abstractmethod\n    def json(self) -> Union[list, dict]:\n        """"""Returns json compatible state of the control instance including\n        its nested controls.\n\n        Returns:\n            control: Json representation of control state.\n        """"""\n        pass\n\n    def ms_bot_framework(self):\n        """"""Returns MS Bot Framework compatible state of the control instance\n        including its nested controls.\n\n        Returns:\n            control: MS Bot Framework representation of control state.\n        """"""\n        return None\n\n    def telegram(self):\n        """"""Returns Telegram compatible state of the control instance\n        including its nested controls.\n\n        Returns:\n            control: Telegram representation of control state.\n        """"""\n        return None\n\n    def alexa(self):\n        """"""Returns Amazon Alexa compatible state of the control instance\n        including its nested controls.\n\n        Returns:\n            control: Amazon Alexa representation of control state.\n        """"""\n        return None\n\n\nclass RichControl(RichItem, metaclass=ABCMeta):\n    """"""Base class for rich controls.\n\n    Rich control can be a button, buttons box, plain text, image, etc.\n    All rich control classes should be derived from RichControl.\n\n    Args:\n        control_type: Name of the rich control type.\n\n    Attributes:\n        control_type: Name of the rich control type.\n        content: Arbitrary used control content holder.\n        control_json: Control json representation template, which\n            contains control type and content fields.\n    """"""\n\n    def __init__(self, control_type: str) -> None:\n        self.control_type: str = control_type\n        self.content = None\n        self.control_json: dict = {\'type\': control_type, \'content\': None}\n\n    def __str__(self) -> str:\n        return \'\'\n\n\nclass RichMessage(RichItem):\n    """"""Container for rich controls.\n\n    All rich content elements returned by agent as a result of single\n    inference should be embedded into RichMessage instance in the order\n    these elements should be displayed.\n\n    Attributes:\n        controls: Container for RichControl instances.\n    """"""\n\n    def __init__(self) -> None:\n        self.controls: list = []\n\n    def __str__(self) -> str:\n        result = \'\\n\'.join(filter(bool, map(str, self.controls)))\n        return result\n\n    def add_control(self, control: RichControl):\n        """"""Adds RichControl instance to RichMessage.\n\n        Args:\n            control: RichControl instance.\n        """"""\n        self.controls.append(control)\n\n    def json(self) -> list:\n        """"""Returns list of json compatible states of the RichMessage instance\n        nested controls.\n\n        Returns:\n            json_controls: Json representation of RichMessage instance\n                nested controls.\n        """"""\n        json_controls = [control.json() for control in self.controls]\n        return json_controls\n\n    def ms_bot_framework(self) -> list:\n        """"""Returns list of MS Bot Framework compatible states of the\n        RichMessage instance nested controls.\n\n        Returns:\n            ms_bf_controls: MS Bot Framework representation of RichMessage instance\n                nested controls.\n        """"""\n        ms_bf_controls = [control.ms_bot_framework() for control in self.controls]\n        return ms_bf_controls\n\n    def telegram(self) -> list:\n        """"""Returns list of Telegram compatible states of the RichMessage\n        instance nested controls.\n\n        Returns:\n            telegram_controls: Telegram representation of RichMessage instance nested\n                controls.\n        """"""\n        telegram_controls = [control.telegram() for control in self.controls]\n        return telegram_controls\n\n    def alexa(self) -> list:\n        """"""Returns list of Amazon Alexa compatible states of the RichMessage\n        instance nested controls.\n\n        Returns:\n            alexa_controls: Amazon Alexa representation of RichMessage instance nested\n                controls.\n        """"""\n        alexa_controls = [control.alexa() for control in self.controls]\n        return alexa_controls\n'"
deeppavlov/deprecated/agents/__init__.py,0,b''
deeppavlov/deprecated/skill/__init__.py,0,b'from .skill import Skill\n'
deeppavlov/deprecated/skill/skill.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import ABCMeta, abstractmethod\nfrom typing import Tuple, Optional, Union\n\nfrom deeppavlov.core.models.component import Component\n\n\nclass Skill(Component, metaclass=ABCMeta):\n    """"""Abstract class for skills.\n\n    Skill is a DeepPavlov component, which provides handling dialog state,\n    dialog history and rich content.\n    """"""\n\n    @abstractmethod\n    def __call__(self, utterances_batch: list, history_batch: list,\n                 states_batch: Optional[list] = None) -> Union[Tuple[list, list], Tuple[list, list, Optional[list]]]:\n        """"""Returns skill inference result.\n\n        Returns batches of skill inference results, estimated confidence\n        levels and up to date states corresponding to incoming utterance\n        batch.\n\n        Args:\n            utterances_batch: A batch of utterances of any type.\n            history_batch: A batch of list typed histories for each utterance.\n            states_batch: Optional. A batch of arbitrary typed states for\n                each utterance.\n\n        Returns:\n            response: A batch of arbitrary typed skill inference results.\n            confidence: A batch of float typed confidence levels for each of\n                skill inference result.\n            states: Optional. A batch of arbitrary typed states for each\n                response.\n        """"""\n'"
deeppavlov/deprecated/skills/__init__.py,0,b''
deeppavlov/models/api_requester/__init__.py,0,b'from .api_requester import *\n'
deeppavlov/models/api_requester/api_requester.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport asyncio\nfrom typing import Any, List, Dict, AsyncIterable\n\nimport requests\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\n\n\n@register(\'api_requester\')\nclass ApiRequester(Component):\n    """"""Component for forwarding parameters to APIs\n\n    Args:\n        url: url of the API.\n        out: count of expected returned values or their names in a chainer.\n        param_names: list of parameter names for API requests.\n        debatchify: if ``True``, single instances will be sent to the API endpoint instead of batches.\n\n    Attributes:\n        url: url of the API.\n        out: count of expected returned values.\n        param_names: list of parameter names for API requests.\n        debatchify: if True, single instances will be sent to the API endpoint instead of batches.\n    """"""\n\n    def __init__(self, url: str, out: [int, list], param_names: [list, tuple] = (), debatchify: bool = False,\n                 *args, **kwargs):\n        self.url = url\n        self.param_names = param_names\n        self.out_count = out if isinstance(out, int) else len(out)\n        self.debatchify = debatchify\n\n    def __call__(self, *args: List[Any], **kwargs: Dict[str, Any]):\n        """"""\n\n        Args:\n            *args: list of parameters sent to the API endpoint. Parameter names are taken from self.param_names.\n            **kwargs: named parameters to send to the API endpoint. If not empty, args are ignored\n\n        Returns:\n            result of the API request(s)\n        """"""\n        data = kwargs or dict(zip(self.param_names, args))\n\n        if self.debatchify:\n            batch_size = 0\n            for v in data.values():\n                batch_size = len(v)\n                break\n\n            assert batch_size > 0\n\n            async def collect():\n                return [j async for j in self.get_async_response(data, batch_size)]\n\n            loop = asyncio.get_event_loop()\n            response = loop.run_until_complete(collect())\n\n        else:\n            response = requests.post(self.url, json=data).json()\n\n        if self.out_count > 1:\n            response = list(zip(*response))\n\n        return response\n\n    async def get_async_response(self, data: dict, batch_size: int) -> AsyncIterable:\n        """"""Helper function for sending requests asynchronously if the API endpoint does not support batching\n\n        Args:\n            data: data to be passed to the API endpoint\n            batch_size: requests count\n\n        Yields:\n            requests results parsed as json\n        """"""\n        loop = asyncio.get_event_loop()\n        futures = [\n            loop.run_in_executor(\n                None,\n                requests.post,\n                self.url,\n                None,\n                {k: v[i] for k, v in data.items()}\n            )\n            for i in range(batch_size)\n        ]\n        for r in await asyncio.gather(*futures):\n            yield r.json()\n'"
deeppavlov/models/api_requester/api_router.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport concurrent\nfrom concurrent.futures import ProcessPoolExecutor\nfrom logging import getLogger\nfrom typing import List\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.models.api_requester import ApiRequester\n\nlogger = getLogger(__name__)\n\n\n@register(""api_router"")\nclass ApiRouter(Component):\n    """"""A helper class for running multiple API requesters on the same data in parallel\n\n    Args:\n        api_requesters: list of ApiRequester objects\n        n_workers: The maximum number of subprocesses to run\n\n    Attributes:\n        api_requesters: list of ApiRequester objects\n        n_workers: The maximum number of subprocesses to run\n    """"""\n\n    def __init__(self, api_requesters: List[ApiRequester], n_workers: int = 1, *args, **kwargs):\n        self.api_requesters = api_requesters\n        self.n_workers = n_workers\n\n    def __call__(self, *args):\n        """"""\n\n        Args:\n            *args: list of arguments to forward to the API requesters\n\n        Returns:\n            results of the requests\n        """"""\n        with ProcessPoolExecutor(self.n_workers) as executor:\n            futures = [executor.submit(api_requester, *args) for api_requester\n                       in\n                       self.api_requesters]\n\n            concurrent.futures.wait(futures)\n            results = []\n            for future, api_requester in zip(futures, self.api_requesters):\n                result = future.result()\n                if api_requester.out_count > 1:\n                    results += result\n                else:\n                    results.append(result)\n\n        return results\n'"
deeppavlov/models/bert/__init__.py,0,b''
deeppavlov/models/bert/bert_as_summarizer.py,17,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\nfrom logging import getLogger\nfrom typing import List, Optional\n\nimport numpy as np\nimport tensorflow.compat.v1 as tf\n\nfrom bert_dp.modeling import BertConfig, BertModel, create_initializer, get_assignment_map_from_checkpoint\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.tf_model import TFModel\nfrom deeppavlov.models.preprocessors.bert_preprocessor import BertPreprocessor\n\nlogger = getLogger(__name__)\n\n\n@register(\'bert_as_summarizer\')\nclass BertAsSummarizer(TFModel):\n    """"""Naive Extractive Summarization model based on BERT.\n    BERT model was trained on Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) tasks.\n    NSP head was trained to detect in ``[CLS] text_a [SEP] text_b [SEP]`` if text_b follows text_a in original document.\n\n    This NSP head can be used to stack sentences from a long document, based on a initial sentence:\n\n    summary_0 = init_sentence\n\n    summary_1 = summary_0 + argmax(nsp_score(candidates))\n\n    summary_2 = summary_1 + argmax(nsp_score(candidates))\n\n    ...\n\n    , where candidates are all sentences from a document.\n\n    Args:\n        bert_config_file: path to Bert configuration file\n        pretrained_bert: path to pretrained Bert checkpoint\n        vocab_file: path to Bert vocabulary\n        max_summary_length: limit on summary length, number of sentences is used if ``max_summary_length_in_tokens``\n            is set to False, else number of tokens is used.\n        max_summary_length_in_tokens: Use number of tokens as length of summary.\n            Defaults to ``False``.\n        max_seq_length: max sequence length in subtokens, including ``[SEP]`` and ``[CLS]`` tokens.\n            `max_seq_length` is used in Bert to compute NSP scores. Defaults to ``128``.\n        do_lower_case: set ``True`` if lowercasing is needed. Defaults to ``False``.\n        lang: use ru_sent_tokenizer for \'ru\' and ntlk.sent_tokener for other languages.\n            Defaults to ``\'ru\'``.\n    """"""\n\n    def __init__(self, bert_config_file: str,\n                 pretrained_bert: str,\n                 vocab_file: str,\n                 max_summary_length: int,\n                 max_summary_length_in_tokens: Optional[bool] = False,\n                 max_seq_length: Optional[int] = 128,\n                 do_lower_case: Optional[bool] = False,\n                 lang: Optional[str] = \'ru\',\n                 **kwargs) -> None:\n\n        self.max_summary_length = max_summary_length\n        self.max_summary_length_in_tokens = max_summary_length_in_tokens\n        self.bert_config = BertConfig.from_json_file(str(expand_path(bert_config_file)))\n\n        self.bert_preprocessor = BertPreprocessor(vocab_file=vocab_file, do_lower_case=do_lower_case,\n                                                  max_seq_length=max_seq_length)\n\n        self.tokenize_reg = re.compile(r""[\\w\']+|[^\\w ]"")\n\n        if lang == \'ru\':\n            from ru_sent_tokenize import ru_sent_tokenize\n            self.sent_tokenizer = ru_sent_tokenize\n        else:\n            from nltk import sent_tokenize\n            self.sent_tokenizer = sent_tokenize\n\n        self.sess_config = tf.ConfigProto(allow_soft_placement=True)\n        self.sess_config.gpu_options.allow_growth = True\n        self.sess = tf.Session(config=self.sess_config)\n\n        self._init_graph()\n\n        self.sess.run(tf.global_variables_initializer())\n\n        if pretrained_bert is not None:\n            pretrained_bert = str(expand_path(pretrained_bert))\n\n            if tf.train.checkpoint_exists(pretrained_bert):\n                logger.info(\'[initializing model with Bert from {}]\'.format(pretrained_bert))\n                tvars = tf.trainable_variables()\n                assignment_map, _ = get_assignment_map_from_checkpoint(tvars, pretrained_bert)\n                tf.train.init_from_checkpoint(pretrained_bert, assignment_map)\n\n    def _init_graph(self):\n        self._init_placeholders()\n\n        self.bert = BertModel(config=self.bert_config,\n                              is_training=self.is_train_ph,\n                              input_ids=self.input_ids_ph,\n                              input_mask=self.input_masks_ph,\n                              token_type_ids=self.token_types_ph,\n                              use_one_hot_embeddings=False,\n                              )\n        # next sentence prediction head\n        with tf.variable_scope(""cls/seq_relationship""):\n            output_weights = tf.get_variable(\n                ""output_weights"",\n                shape=[2, self.bert_config.hidden_size],\n                initializer=create_initializer(self.bert_config.initializer_range))\n            output_bias = tf.get_variable(\n                ""output_bias"", shape=[2], initializer=tf.zeros_initializer())\n\n        nsp_logits = tf.matmul(self.bert.get_pooled_output(), output_weights, transpose_b=True)\n        nsp_logits = tf.nn.bias_add(nsp_logits, output_bias)\n        self.nsp_probs = tf.nn.softmax(nsp_logits, axis=-1)\n\n    def _init_placeholders(self):\n        self.input_ids_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name=\'ids_ph\')\n        self.input_masks_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name=\'masks_ph\')\n        self.token_types_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name=\'token_types_ph\')\n\n        self.is_train_ph = tf.placeholder_with_default(False, shape=[], name=\'is_train_ph\')\n\n    def _build_feed_dict(self, input_ids, input_masks, token_types):\n        feed_dict = {\n            self.input_ids_ph: input_ids,\n            self.input_masks_ph: input_masks,\n            self.token_types_ph: token_types,\n        }\n        return feed_dict\n\n    def _get_nsp_predictions(self, sentences: List[str], candidates: List[str]):\n        """"""Compute NextSentence probability for every (sentence_i, candidate_i) pair.\n\n        [CLS] sentence_i [SEP] candidate_i [SEP]\n\n        Args:\n            sentences: list of sentences\n            candidates: list of candidates to be the next sentence\n\n        Returns:\n            probabilities that candidate is a next sentence\n        """"""\n        features = self.bert_preprocessor(texts_a=sentences, texts_b=candidates)\n        input_ids = [f.input_ids for f in features]\n        input_masks = [f.input_mask for f in features]\n        input_type_ids = [f.input_type_ids for f in features]\n        feed_dict = self._build_feed_dict(input_ids, input_masks, input_type_ids)\n        nsp_probs = self.sess.run(self.nsp_probs, feed_dict=feed_dict)\n        return nsp_probs[:, 0]\n\n    def __call__(self, texts: List[str], init_sentences: Optional[List[str]] = None) -> List[List[str]]:\n        """"""Builds summary for text from `texts`\n\n        Args:\n            texts: texts to build summaries for\n            init_sentences: ``init_sentence`` is used as the first sentence in summary.\n                Defaults to None.\n\n        Returns:\n            List[List[str]]: summaries tokenized on sentences\n        """"""\n        summaries = []\n        # build summaries for each text, init_sentence pair\n        if init_sentences is None:\n            init_sentences = [None] * len(texts)\n\n        for text, init_sentence in zip(texts, init_sentences):\n            text_sentences = self.sent_tokenizer(text)\n\n            if init_sentence is None:\n                init_sentence = text_sentences[0]\n                text_sentences = text_sentences[1:]\n\n            # remove duplicates\n            text_sentences = list(set(text_sentences))\n            # remove init_sentence from text sentences\n            text_sentences = [sent for sent in text_sentences if sent != init_sentence]\n\n            summary = [init_sentence]\n            if self.max_summary_length_in_tokens:\n                # get length in tokens\n                def get_length(x):\n                    return len(self.tokenize_reg.findall(\' \'.join(x)))\n            else:\n                # get length as number of sentences\n                get_length = len\n\n            candidates = text_sentences[:]\n            while len(candidates) > 0:\n                # todo: use batches\n                candidates_scores = [self._get_nsp_predictions([\' \'.join(summary)], [cand]) for cand in candidates]\n                best_candidate_idx = np.argmax(candidates_scores)\n                best_candidate = candidates[best_candidate_idx]\n                del candidates[best_candidate_idx]\n                if get_length(summary + [best_candidate]) > self.max_summary_length:\n                    break\n                summary = summary + [best_candidate]\n            summaries += [summary]\n        return summaries\n\n    def train_on_batch(self, **kwargs):\n        raise NotImplementedError\n'"
deeppavlov/models/bert/bert_classifier.py,36,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import List, Dict, Union\n\nimport tensorflow as tf\nfrom bert_dp.modeling import BertConfig, BertModel\nfrom bert_dp.optimization import AdamWeightDecayOptimizer\nfrom bert_dp.preprocessing import InputFeatures\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.tf_model import LRScheduledTFModel\n\nlogger = getLogger(__name__)\n\n\n@register(\'bert_classifier\')\nclass BertClassifierModel(LRScheduledTFModel):\n    """"""Bert-based model for text classification.\n\n    It uses output from [CLS] token and predicts labels using linear transformation.\n\n    Args:\n        bert_config_file: path to Bert configuration file\n        n_classes: number of classes\n        keep_prob: dropout keep_prob for non-Bert layers\n        one_hot_labels: set True if one-hot encoding for labels is used\n        multilabel: set True if it is multi-label classification\n        return_probas: set True if return class probabilites instead of most probable label needed\n        attention_probs_keep_prob: keep_prob for Bert self-attention layers\n        hidden_keep_prob: keep_prob for Bert hidden layers\n        optimizer: name of tf.train.* optimizer or None for `AdamWeightDecayOptimizer`\n        num_warmup_steps:\n        weight_decay_rate: L2 weight decay for `AdamWeightDecayOptimizer`\n        pretrained_bert: pretrained Bert checkpoint\n        min_learning_rate: min value of learning rate if learning rate decay is used\n    """"""\n\n    # TODO: add warmup\n    # TODO: add head-only pre-training\n    def __init__(self, bert_config_file, n_classes, keep_prob,\n                 one_hot_labels=False, multilabel=False, return_probas=False,\n                 attention_probs_keep_prob=None, hidden_keep_prob=None,\n                 optimizer=None, num_warmup_steps=None, weight_decay_rate=0.01,\n                 pretrained_bert=None, min_learning_rate=1e-06, **kwargs) -> None:\n        super().__init__(**kwargs)\n\n        self.return_probas = return_probas\n        self.n_classes = n_classes\n        self.min_learning_rate = min_learning_rate\n        self.keep_prob = keep_prob\n        self.one_hot_labels = one_hot_labels\n        self.multilabel = multilabel\n        self.optimizer = optimizer\n        self.num_warmup_steps = num_warmup_steps\n        self.weight_decay_rate = weight_decay_rate\n\n        if self.multilabel and not self.one_hot_labels:\n            raise RuntimeError(\'Use one-hot encoded labels for multilabel classification!\')\n\n        if self.multilabel and not self.return_probas:\n            raise RuntimeError(\'Set return_probas to True for multilabel classification!\')\n\n        self.bert_config = BertConfig.from_json_file(str(expand_path(bert_config_file)))\n\n        if attention_probs_keep_prob is not None:\n            self.bert_config.attention_probs_dropout_prob = 1.0 - attention_probs_keep_prob\n        if hidden_keep_prob is not None:\n            self.bert_config.hidden_dropout_prob = 1.0 - hidden_keep_prob\n\n        self.sess_config = tf.ConfigProto(allow_soft_placement=True)\n        self.sess_config.gpu_options.allow_growth = True\n        self.sess = tf.Session(config=self.sess_config)\n\n        self._init_graph()\n\n        self._init_optimizer()\n\n        self.sess.run(tf.global_variables_initializer())\n\n        if pretrained_bert is not None:\n            pretrained_bert = str(expand_path(pretrained_bert))\n\n            if tf.train.checkpoint_exists(pretrained_bert) \\\n                    and not (self.load_path and tf.train.checkpoint_exists(str(self.load_path.resolve()))):\n                logger.info(\'[initializing model with Bert from {}]\'.format(pretrained_bert))\n                # Exclude optimizer and classification variables from saved variables\n                var_list = self._get_saveable_variables(\n                    exclude_scopes=(\'Optimizer\', \'learning_rate\', \'momentum\', \'output_weights\', \'output_bias\'))\n                saver = tf.train.Saver(var_list)\n                saver.restore(self.sess, pretrained_bert)\n\n        if self.load_path is not None:\n            self.load()\n\n    def _init_graph(self):\n        self._init_placeholders()\n\n        self.bert = BertModel(config=self.bert_config,\n                              is_training=self.is_train_ph,\n                              input_ids=self.input_ids_ph,\n                              input_mask=self.input_masks_ph,\n                              token_type_ids=self.token_types_ph,\n                              use_one_hot_embeddings=False,\n                              )\n\n        output_layer = self.bert.get_pooled_output()\n        hidden_size = output_layer.shape[-1].value\n\n        output_weights = tf.get_variable(\n            ""output_weights"", [self.n_classes, hidden_size],\n            initializer=tf.truncated_normal_initializer(stddev=0.02))\n\n        output_bias = tf.get_variable(\n            ""output_bias"", [self.n_classes], initializer=tf.zeros_initializer())\n\n        with tf.variable_scope(""loss""):\n            output_layer = tf.nn.dropout(output_layer, keep_prob=self.keep_prob_ph)\n            logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n            logits = tf.nn.bias_add(logits, output_bias)\n\n            if self.one_hot_labels:\n                one_hot_labels = self.y_ph\n            else:\n                one_hot_labels = tf.one_hot(self.y_ph, depth=self.n_classes, dtype=tf.float32)\n\n            self.y_predictions = tf.argmax(logits, axis=-1)\n            if not self.multilabel:\n                log_probs = tf.nn.log_softmax(logits, axis=-1)\n                self.y_probas = tf.nn.softmax(logits, axis=-1)\n                per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n                self.loss = tf.reduce_mean(per_example_loss)\n            else:\n                self.y_probas = tf.nn.sigmoid(logits)\n                self.loss = tf.reduce_mean(\n                    tf.nn.sigmoid_cross_entropy_with_logits(labels=one_hot_labels, logits=logits))\n\n    def _init_placeholders(self):\n        self.input_ids_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name=\'ids_ph\')\n        self.input_masks_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name=\'masks_ph\')\n        self.token_types_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name=\'token_types_ph\')\n\n        if not self.one_hot_labels:\n            self.y_ph = tf.placeholder(shape=(None,), dtype=tf.int32, name=\'y_ph\')\n        else:\n            self.y_ph = tf.placeholder(shape=(None, self.n_classes), dtype=tf.float32, name=\'y_ph\')\n\n        self.learning_rate_ph = tf.placeholder_with_default(0.0, shape=[], name=\'learning_rate_ph\')\n        self.keep_prob_ph = tf.placeholder_with_default(1.0, shape=[], name=\'keep_prob_ph\')\n        self.is_train_ph = tf.placeholder_with_default(False, shape=[], name=\'is_train_ph\')\n\n    def _init_optimizer(self):\n        with tf.variable_scope(\'Optimizer\'):\n            self.global_step = tf.get_variable(\'global_step\', shape=[], dtype=tf.int32,\n                                               initializer=tf.constant_initializer(0), trainable=False)\n            # default optimizer for Bert is Adam with fixed L2 regularization\n            if self.optimizer is None:\n\n                self.train_op = self.get_train_op(self.loss, learning_rate=self.learning_rate_ph,\n                                                  optimizer=AdamWeightDecayOptimizer,\n                                                  weight_decay_rate=self.weight_decay_rate,\n                                                  beta_1=0.9,\n                                                  beta_2=0.999,\n                                                  epsilon=1e-6,\n                                                  exclude_from_weight_decay=[""LayerNorm"", ""layer_norm"", ""bias""]\n                                                  )\n            else:\n                self.train_op = self.get_train_op(self.loss, learning_rate=self.learning_rate_ph)\n\n            if self.optimizer is None:\n                new_global_step = self.global_step + 1\n                self.train_op = tf.group(self.train_op, [self.global_step.assign(new_global_step)])\n\n    def _build_feed_dict(self, input_ids, input_masks, token_types, y=None):\n        feed_dict = {\n            self.input_ids_ph: input_ids,\n            self.input_masks_ph: input_masks,\n            self.token_types_ph: token_types,\n        }\n        if y is not None:\n            feed_dict.update({\n                self.y_ph: y,\n                self.learning_rate_ph: max(self.get_learning_rate(), self.min_learning_rate),\n                self.keep_prob_ph: self.keep_prob,\n                self.is_train_ph: True,\n            })\n\n        return feed_dict\n\n    def train_on_batch(self, features: List[InputFeatures], y: Union[List[int], List[List[int]]]) -> Dict:\n        """"""Train model on given batch.\n        This method calls train_op using features and y (labels).\n\n        Args:\n            features: batch of InputFeatures\n            y: batch of labels (class id or one-hot encoding)\n\n        Returns:\n            dict with loss and learning_rate values\n\n        """"""\n        input_ids = [f.input_ids for f in features]\n        input_masks = [f.input_mask for f in features]\n        input_type_ids = [f.input_type_ids for f in features]\n\n        feed_dict = self._build_feed_dict(input_ids, input_masks, input_type_ids, y)\n\n        _, loss = self.sess.run([self.train_op, self.loss], feed_dict=feed_dict)\n        return {\'loss\': loss, \'learning_rate\': feed_dict[self.learning_rate_ph]}\n\n    def __call__(self, features: List[InputFeatures]) -> Union[List[int], List[List[float]]]:\n        """"""Make prediction for given features (texts).\n\n        Args:\n            features: batch of InputFeatures\n\n        Returns:\n            predicted classes or probabilities of each class\n\n        """"""\n        input_ids = [f.input_ids for f in features]\n        input_masks = [f.input_mask for f in features]\n        input_type_ids = [f.input_type_ids for f in features]\n\n        feed_dict = self._build_feed_dict(input_ids, input_masks, input_type_ids)\n        if not self.return_probas:\n            pred = self.sess.run(self.y_predictions, feed_dict=feed_dict)\n        else:\n            pred = self.sess.run(self.y_probas, feed_dict=feed_dict)\n        return pred\n'"
deeppavlov/models/bert/bert_ranker.py,34,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\nfrom collections import OrderedDict\nfrom logging import getLogger\nfrom operator import itemgetter\nfrom typing import List, Dict, Union\n\nimport numpy as np\nimport tensorflow as tf\nfrom bert_dp.modeling import BertConfig, BertModel\nfrom bert_dp.optimization import AdamWeightDecayOptimizer\nfrom bert_dp.preprocessing import InputFeatures\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.tf_model import LRScheduledTFModel\nfrom deeppavlov.models.bert.bert_classifier import BertClassifierModel\n\nlogger = getLogger(__name__)\n\n\n@register(\'bert_ranker\')\nclass BertRankerModel(BertClassifierModel):\n    """"""BERT-based model for interaction-based text ranking.\n\n    Linear transformation is trained over the BERT pooled output from [CLS] token.\n    Predicted probabilities of classes are used as a similarity measure for ranking.\n\n    Args:\n        bert_config_file: path to Bert configuration file\n        n_classes: number of classes\n        keep_prob: dropout keep_prob for non-Bert layers\n        return_probas: set True if class probabilities are returned instead of the most probable label\n    """"""\n\n    def __init__(self, bert_config_file, n_classes=2, keep_prob=0.9, return_probas=True, **kwargs) -> None:\n        super().__init__(bert_config_file=bert_config_file, n_classes=n_classes,\n                         keep_prob=keep_prob, return_probas=return_probas, **kwargs)\n\n    def train_on_batch(self, features_li: List[List[InputFeatures]], y: Union[List[int], List[List[int]]]) -> Dict:\n        """"""Train the model on the given batch.\n\n        Args:\n            features_li: list with the single element containing the batch of InputFeatures\n            y: batch of labels (class id or one-hot encoding)\n\n        Returns:\n            dict with loss and learning rate values\n        """"""\n\n        features = features_li[0]\n        input_ids = [f.input_ids for f in features]\n        input_masks = [f.input_mask for f in features]\n        input_type_ids = [f.input_type_ids for f in features]\n\n        feed_dict = self._build_feed_dict(input_ids, input_masks, input_type_ids, y)\n\n        _, loss = self.sess.run([self.train_op, self.loss], feed_dict=feed_dict)\n        return {\'loss\': loss, \'learning_rate\': feed_dict[self.learning_rate_ph]}\n\n    def __call__(self, features_li: List[List[InputFeatures]]) -> Union[List[int], List[List[float]]]:\n        """"""Calculate scores for the given context over candidate responses.\n\n        Args:\n            features_li: list of elements where each element contains the batch of features\n             for contexts with particular response candidates\n\n        Returns:\n            predicted scores for contexts over response candidates\n        """"""\n\n        if len(features_li) == 1 and len(features_li[0]) == 1:\n            msg = ""It is not intended to use the {} in the interact mode."".format(self.__class__)\n            logger.error(msg)\n            return [msg]\n\n        predictions = []\n        for features in features_li:\n            input_ids = [f.input_ids for f in features]\n            input_masks = [f.input_mask for f in features]\n            input_type_ids = [f.input_type_ids for f in features]\n\n            feed_dict = self._build_feed_dict(input_ids, input_masks, input_type_ids)\n            if not self.return_probas:\n                pred = self.sess.run(self.y_predictions, feed_dict=feed_dict)\n            else:\n                pred = self.sess.run(self.y_probas, feed_dict=feed_dict)\n            predictions.append(pred[:, 1])\n        if len(features_li) == 1:\n            predictions = predictions[0]\n        else:\n            predictions = np.hstack([np.expand_dims(el, 1) for el in predictions])\n        return predictions\n\n\n@register(\'bert_sep_ranker\')\nclass BertSepRankerModel(LRScheduledTFModel):\n    """"""BERT-based model for representation-based text ranking.\n\n     BERT pooled output from [CLS] token is used to get a separate representation of a context and a response.\n     Similarity measure is calculated as cosine similarity between these representations.\n\n    Args:\n        bert_config_file: path to Bert configuration file\n        keep_prob: dropout keep_prob for non-Bert layers\n        attention_probs_keep_prob: keep_prob for Bert self-attention layers\n        hidden_keep_prob: keep_prob for Bert hidden layers\n        optimizer: name of tf.train.* optimizer or None for ``AdamWeightDecayOptimizer``\n        weight_decay_rate: L2 weight decay for ``AdamWeightDecayOptimizer``\n        pretrained_bert: pretrained Bert checkpoint\n        min_learning_rate: min value of learning rate if learning rate decay is used\n    """"""\n\n    def __init__(self, bert_config_file, keep_prob=0.9,\n                 attention_probs_keep_prob=None, hidden_keep_prob=None,\n                 optimizer=None, weight_decay_rate=0.01,\n                 pretrained_bert=None, min_learning_rate=1e-06, **kwargs) -> None:\n        super().__init__(**kwargs)\n\n        self.min_learning_rate = min_learning_rate\n        self.keep_prob = keep_prob\n        self.optimizer = optimizer\n        self.weight_decay_rate = weight_decay_rate\n\n        self.bert_config = BertConfig.from_json_file(str(expand_path(bert_config_file)))\n\n        if attention_probs_keep_prob is not None:\n            self.bert_config.attention_probs_dropout_prob = 1.0 - attention_probs_keep_prob\n        if hidden_keep_prob is not None:\n            self.bert_config.hidden_dropout_prob = 1.0 - hidden_keep_prob\n\n        self.sess_config = tf.ConfigProto(allow_soft_placement=True)\n        self.sess_config.gpu_options.allow_growth = True\n        self.sess = tf.Session(config=self.sess_config)\n\n        self._init_graph()\n\n        self._init_optimizer()\n\n        if pretrained_bert is not None:\n            pretrained_bert = str(expand_path(pretrained_bert))\n\n            if tf.train.checkpoint_exists(pretrained_bert) \\\n                    and not (self.load_path and tf.train.checkpoint_exists(str(self.load_path.resolve()))):\n                logger.info(\'[initializing model with Bert from {}]\'.format(pretrained_bert))\n                # Exclude optimizer and classification variables from saved variables\n                var_list = self._get_saveable_variables(\n                    exclude_scopes=(\'Optimizer\', \'learning_rate\', \'momentum\', \'output_weights\', \'output_bias\'))\n                assignment_map = self.get_variables_to_restore(var_list, pretrained_bert)\n                tf.train.init_from_checkpoint(pretrained_bert, assignment_map)\n\n        self.sess.run(tf.global_variables_initializer())\n\n        if self.load_path is not None:\n            self.load()\n\n    @classmethod\n    def get_variables_to_restore(cls, tvars, init_checkpoint):\n        """"""Determine correspondence of checkpoint variables to current variables.""""""\n\n        assignment_map = OrderedDict()\n        graph_names = []\n        for var in tvars:\n            name = var.name\n            m = re.match(""^(.*):\\\\d+$"", name)\n            if m is not None:\n                name = m.group(1)\n                graph_names.append(name)\n        ckpt_names = [el[0] for el in tf.train.list_variables(init_checkpoint)]\n        for u in ckpt_names:\n            for v in graph_names:\n                if u in v:\n                    assignment_map[u] = v\n        return assignment_map\n\n    def _init_graph(self):\n        self._init_placeholders()\n\n        with tf.variable_scope(""model""):\n            model_a = BertModel(\n                config=self.bert_config,\n                is_training=self.is_train_ph,\n                input_ids=self.input_ids_a_ph,\n                input_mask=self.input_masks_a_ph,\n                token_type_ids=self.token_types_a_ph,\n                use_one_hot_embeddings=False)\n\n        with tf.variable_scope(""model"", reuse=True):\n            model_b = BertModel(\n                config=self.bert_config,\n                is_training=self.is_train_ph,\n                input_ids=self.input_ids_b_ph,\n                input_mask=self.input_masks_b_ph,\n                token_type_ids=self.token_types_b_ph,\n                use_one_hot_embeddings=False)\n\n        output_layer_a = model_a.get_pooled_output()\n        output_layer_b = model_b.get_pooled_output()\n\n        with tf.variable_scope(""loss""):\n            output_layer_a = tf.nn.dropout(output_layer_a, keep_prob=self.keep_prob_ph)\n            output_layer_b = tf.nn.dropout(output_layer_b, keep_prob=self.keep_prob_ph)\n            output_layer_a = tf.nn.l2_normalize(output_layer_a, axis=1)\n            output_layer_b = tf.nn.l2_normalize(output_layer_b, axis=1)\n            embeddings = tf.concat([output_layer_a, output_layer_b], axis=0)\n            labels = tf.concat([self.y_ph, self.y_ph], axis=0)\n            self.loss = tf.contrib.losses.metric_learning.triplet_semihard_loss(labels, embeddings)\n            logits = tf.multiply(output_layer_a, output_layer_b)\n            self.y_probas = tf.reduce_sum(logits, 1)\n            self.pooled_out = output_layer_a\n\n    def _init_placeholders(self):\n        self.input_ids_a_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name=\'ids_a_ph\')\n        self.input_masks_a_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name=\'masks_a_ph\')\n        self.token_types_a_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name=\'token_a_types_ph\')\n        self.input_ids_b_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name=\'ids_b_ph\')\n        self.input_masks_b_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name=\'masks_b_ph\')\n        self.token_types_b_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name=\'token_types_b_ph\')\n        self.y_ph = tf.placeholder(shape=(None,), dtype=tf.int32, name=\'y_ph\')\n        self.learning_rate_ph = tf.placeholder_with_default(0.0, shape=[], name=\'learning_rate_ph\')\n        self.keep_prob_ph = tf.placeholder_with_default(1.0, shape=[], name=\'keep_prob_ph\')\n        self.is_train_ph = tf.placeholder_with_default(False, shape=[], name=\'is_train_ph\')\n\n    def _init_optimizer(self):\n        with tf.variable_scope(\'Optimizer\'):\n            self.global_step = tf.get_variable(\'global_step\', shape=[], dtype=tf.int32,\n                                               initializer=tf.constant_initializer(0), trainable=False)\n            # default optimizer for Bert is Adam with fixed L2 regularization\n            if self.optimizer is None:\n\n                self.train_op = self.get_train_op(self.loss, learning_rate=self.learning_rate_ph,\n                                                  optimizer=AdamWeightDecayOptimizer,\n                                                  weight_decay_rate=self.weight_decay_rate,\n                                                  beta_1=0.9,\n                                                  beta_2=0.999,\n                                                  epsilon=1e-6,\n                                                  exclude_from_weight_decay=[""LayerNorm"", ""layer_norm"", ""bias""]\n                                                  )\n            else:\n                self.train_op = self.get_train_op(self.loss, learning_rate=self.learning_rate_ph)\n\n            if self.optimizer is None:\n                new_global_step = self.global_step + 1\n                self.train_op = tf.group(self.train_op, [self.global_step.assign(new_global_step)])\n\n    def _build_feed_dict(self, input_ids_a, input_masks_a, token_types_a,\n                         input_ids_b, input_masks_b, token_types_b, y=None):\n        feed_dict = {\n            self.input_ids_a_ph: input_ids_a,\n            self.input_masks_a_ph: input_masks_a,\n            self.token_types_a_ph: token_types_a,\n            self.input_ids_b_ph: input_ids_b,\n            self.input_masks_b_ph: input_masks_b,\n            self.token_types_b_ph: token_types_b,\n        }\n        if y is not None:\n            feed_dict.update({\n                self.y_ph: y,\n                self.learning_rate_ph: max(self.get_learning_rate(), self.min_learning_rate),\n                self.keep_prob_ph: self.keep_prob,\n                self.is_train_ph: True,\n            })\n\n        return feed_dict\n\n    def train_on_batch(self, features_li: List[List[InputFeatures]], y: Union[List[int], List[List[int]]]) -> Dict:\n        """"""Train the model on the given batch.\n\n        Args:\n            features_li: list with two elements, one containing the batch of context features\n             and the other containing the batch of response features\n            y: batch of labels (class id or one-hot encoding)\n\n        Returns:\n            dict with loss and learning rate values\n        """"""\n\n        input_ids_a = [f.input_ids for f in features_li[0]]\n        input_masks_a = [f.input_mask for f in features_li[0]]\n        input_type_ids_a = [f.input_type_ids for f in features_li[0]]\n        input_ids_b = [f.input_ids for f in features_li[1]]\n        input_masks_b = [f.input_mask for f in features_li[1]]\n        input_type_ids_b = [f.input_type_ids for f in features_li[1]]\n\n        feed_dict = self._build_feed_dict(input_ids_a, input_masks_a, input_type_ids_a,\n                                          input_ids_b, input_masks_b, input_type_ids_b, y)\n\n        _, loss = self.sess.run([self.train_op, self.loss], feed_dict=feed_dict)\n        return {\'loss\': loss, \'learning_rate\': feed_dict[self.learning_rate_ph]}\n\n    def __call__(self, features_li: List[List[InputFeatures]]) -> Union[List[int], List[List[float]]]:\n        """"""Calculate scores for the given context over candidate responses.\n\n        Args:\n            features_li: list of elements where the first element represents the context batch of features\n                and the rest of elements represent response candidates batches of features\n\n        Returns:\n            predicted scores for contexts over response candidates\n        """"""\n\n        if len(features_li) == 1 and len(features_li[0]) == 1:\n            msg = ""It is not intended to use the {} in the interact mode."".format(self.__class__)\n            logger.error(msg)\n            return [msg]\n\n        predictions = []\n        input_ids_a = [f.input_ids for f in features_li[0]]\n        input_masks_a = [f.input_mask for f in features_li[0]]\n        input_type_ids_a = [f.input_type_ids for f in features_li[0]]\n        for features in features_li[1:]:\n            input_ids_b = [f.input_ids for f in features]\n            input_masks_b = [f.input_mask for f in features]\n            input_type_ids_b = [f.input_type_ids for f in features]\n\n            feed_dict = self._build_feed_dict(input_ids_a, input_masks_a, input_type_ids_a,\n                                              input_ids_b, input_masks_b, input_type_ids_b)\n            pred = self.sess.run(self.y_probas, feed_dict=feed_dict)\n            predictions.append(pred)\n        if len(features_li) == 1:\n            predictions = predictions[0]\n        else:\n            predictions = np.hstack([np.expand_dims(el, 1) for el in predictions])\n        return predictions\n\n\n@register(\'bert_sep_ranker_predictor\')\nclass BertSepRankerPredictor(BertSepRankerModel):\n    """"""Bert-based model for ranking and receiving a text response.\n\n    BERT pooled output from [CLS] token is used to get a separate representation of a context and a response.\n    A similarity score is calculated as cosine similarity between these representations.\n    Based on this similarity score the text response is retrieved provided some base\n    with possible responses (and corresponding contexts).\n    Contexts of responses are used additionaly to get the best possible result of retrieval from the base.\n\n    Args:\n        bert_config_file: path to Bert configuration file\n        interact_mode: mode setting a policy to retrieve the response from the base\n        batch_size: batch size for building response (and context) vectors over the base\n        keep_prob: dropout keep_prob for non-Bert layers\n        resps: list of strings containing the base of text responses\n        resp_vecs: BERT vector respresentations of ``resps``, if is ``None`` it will be build\n        resp_features: features of ``resps`` to build their BERT vector representations\n        conts: list of strings containing the base of text contexts\n        cont_vecs: BERT vector respresentations of ``conts``, if is ``None`` it will be build\n        cont_features: features of ``conts`` to build their BERT vector representations\n    """"""\n\n    def __init__(self, bert_config_file, interact_mode=0, batch_size=32,\n                 resps=None, resp_features=None, resp_vecs=None,\n                 conts=None, cont_features=None, cont_vecs=None, **kwargs) -> None:\n        super().__init__(bert_config_file=bert_config_file,\n                         **kwargs)\n\n        self.interact_mode = interact_mode\n        self.batch_size = batch_size\n        self.resps = resps\n        self.resp_vecs = resp_vecs\n        self.resp_features = resp_features\n        self.conts = conts\n        self.cont_vecs = cont_vecs\n        self.cont_features = cont_features\n\n        if self.resps is not None and self.resp_vecs is None:\n            logger.info(""Building BERT vector representations for the response base..."")\n            self.resp_features = [resp_features[0][i * self.batch_size: (i + 1) * self.batch_size]\n                                  for i in range(len(resp_features[0]) // batch_size + 1)]\n            self.resp_vecs = self._get_predictions(self.resp_features)\n            self.resp_vecs /= np.linalg.norm(self.resp_vecs, axis=1, keepdims=True)\n            np.save(self.save_path / ""resp_vecs"", self.resp_vecs)\n\n        if self.conts is not None and self.cont_vecs is None:\n            logger.info(""Building BERT vector representations for the context base..."")\n            self.cont_features = [cont_features[0][i * self.batch_size: (i + 1) * self.batch_size]\n                                  for i in range(len(cont_features[0]) // batch_size + 1)]\n            self.cont_vecs = self._get_predictions(self.cont_features)\n            self.cont_vecs /= np.linalg.norm(self.cont_vecs, axis=1, keepdims=True)\n            np.save(self.save_path / ""cont_vecs"", self.resp_vecs)\n\n    def train_on_batch(self, features, y):\n        pass\n\n    def __call__(self, features_li):\n        """"""Get the context vector representation and retrieve the text response from the database.\n\n        Uses cosine similarity scores over vectors of responses (and corresponding contexts) from the base.\n        Based on these scores retrieves the text response from the base.\n\n        Args:\n            features_li: list of elements where elements represent context batches of features\n\n        Returns:\n            text response with the highest similarity score and its similarity score from the response base\n        """"""\n\n        pred = self._get_predictions(features_li)\n        return self._retrieve_db_response(pred)\n\n    def _get_predictions(self, features_li):\n        """"""Get BERT vector representations for a list of feature batches.""""""\n\n        pred = []\n        for features in features_li:\n            input_ids = [f.input_ids for f in features]\n            input_masks = [f.input_mask for f in features]\n            input_type_ids = [f.input_type_ids for f in features]\n            feed_dict = self._build_feed_dict(input_ids, input_masks, input_type_ids,\n                                              input_ids, input_masks, input_type_ids)\n            p = self.sess.run(self.pooled_out, feed_dict=feed_dict)\n            if len(p.shape) == 1:\n                p = np.expand_dims(p, 0)\n            p /= np.linalg.norm(p, axis=1, keepdims=True)\n            pred.append(p)\n        return np.vstack(pred)\n\n    def _retrieve_db_response(self, ctx_vec):\n        """"""Retrieve a text response from the base based on the policy determined by ``interact_mode``.\n\n        Uses cosine similarity scores over vectors of responses (and corresponding contexts) from the base.\n        """"""\n\n        bs = ctx_vec.shape[0]\n        if self.interact_mode == 0:\n            s = ctx_vec @ self.resp_vecs.T\n            ids = np.argmax(s, 1)\n            rsp = [[self.resps[ids[i]] for i in range(bs)], [s[i][ids[i]] for i in range(bs)]]\n        if self.interact_mode == 1:\n            sr = (ctx_vec @ self.resp_vecs.T + 1) / 2\n            sc = (ctx_vec @ self.cont_vecs.T + 1) / 2\n            ids = np.argsort(sr, 1)[:, -10:]\n            sc = [sc[i, ids[i]] for i in range(bs)]\n            ids = [sorted(zip(ids[i], sc[i]), key=itemgetter(1), reverse=True) for i in range(bs)]\n            sc = [list(map(lambda x: x[1], ids[i])) for i in range(bs)]\n            ids = [list(map(lambda x: x[0], ids[i])) for i in range(bs)]\n            rsp = [[self.resps[ids[i][0]] for i in range(bs)], [float(sc[i][0]) for i in range(bs)]]\n        if self.interact_mode == 2:\n            sr = (ctx_vec @ self.resp_vecs.T + 1) / 2\n            sc = (ctx_vec @ self.cont_vecs.T + 1) / 2\n            ids = np.argsort(sc, 1)[:, -10:]\n            sr = [sr[i, ids[i]] for i in range(bs)]\n            ids = [sorted(zip(ids[i], sr[i]), key=itemgetter(1), reverse=True) for i in range(bs)]\n            sr = [list(map(lambda x: x[1], ids[i])) for i in range(bs)]\n            ids = [list(map(lambda x: x[0], ids[i])) for i in range(bs)]\n            rsp = [[self.resps[ids[i][0]] for i in range(bs)], [float(sr[i][0]) for i in range(bs)]]\n        if self.interact_mode == 3:\n            sr = (ctx_vec @ self.resp_vecs.T + 1) / 2\n            sc = (ctx_vec @ self.cont_vecs.T + 1) / 2\n            s = (sr + sc) / 2\n            ids = np.argmax(s, 1)\n            rsp = [[self.resps[ids[i]] for i in range(bs)], [float(s[i][ids[i]]) for i in range(bs)]]\n        # remove special tokens if they are presented\n        rsp = [[el.replace(\'__eou__\', \'\').replace(\'__eot__\', \'\').strip() for el in rsp[0]], rsp[1]]\n        return rsp\n'"
deeppavlov/models/bert/bert_sequence_tagger.py,102,"b'# Copyright 2019 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import List, Union, Dict, Optional\n\nimport numpy as np\nimport tensorflow as tf\nfrom bert_dp.modeling import BertConfig, BertModel\nfrom bert_dp.optimization import AdamWeightDecayOptimizer\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.layers.tf_layers import bi_rnn\nfrom deeppavlov.core.models.tf_model import LRScheduledTFModel\n\nlog = getLogger(__name__)\n\n\ndef token_from_subtoken(units: tf.Tensor, mask: tf.Tensor) -> tf.Tensor:\n    """""" Assemble token level units from subtoken level units\n\n    Args:\n        units: tf.Tensor of shape [batch_size, SUBTOKEN_seq_length, n_features]\n        mask: mask of token beginnings. For example: for tokens\n\n                [[``[CLS]`` ``My``, ``capybara``, ``[SEP]``],\n                [``[CLS]`` ``Your``, ``aar``, ``##dvark``, ``is``, ``awesome``, ``[SEP]``]]\n\n            the mask will be\n\n                [[0, 1, 1, 0, 0, 0, 0],\n                [0, 1, 1, 0, 1, 1, 0]]\n\n    Returns:\n        word_level_units: Units assembled from ones in the mask. For the\n            example above this units will correspond to the following\n\n                [[``My``, ``capybara``],\n                [``Your`, ``aar``, ``is``, ``awesome``,]]\n\n            the shape of this tensor will be [batch_size, TOKEN_seq_length, n_features]\n    """"""\n    shape = tf.cast(tf.shape(units), tf.int64)\n    batch_size = shape[0]\n    nf = shape[2]\n    nf_int = units.get_shape().as_list()[-1]\n\n    # number of TOKENS in each sentence\n    token_seq_lengths = tf.cast(tf.reduce_sum(mask, 1), tf.int64)\n    # for a matrix m =\n    # [[1, 1, 1],\n    #  [0, 1, 1],\n    #  [1, 0, 0]]\n    # it will be\n    # [3, 2, 1]\n\n    n_words = tf.reduce_sum(token_seq_lengths)\n    # n_words -> 6\n\n    max_token_seq_len = tf.cast(tf.reduce_max(token_seq_lengths), tf.int64)\n    # max_token_seq_len -> 3\n\n    idxs = tf.where(mask)\n    # for the matrix mentioned above\n    # tf.where(mask) ->\n    # [[0, 0],\n    #  [0, 1]\n    #  [0, 2],\n    #  [1, 1],\n    #  [1, 2]\n    #  [2, 0]]\n\n    sample_ids_in_batch = tf.pad(idxs[:, 0], [[1, 0]])\n    # for indices\n    # [[0, 0],\n    #  [0, 1]\n    #  [0, 2],\n    #  [1, 1],\n    #  [1, 2],\n    #  [2, 0]]\n    # it is\n    # [0, 0, 0, 0, 1, 1, 2]\n    # padding is for computing change from one sample to another in the batch\n\n    a = tf.cast(tf.not_equal(sample_ids_in_batch[1:], sample_ids_in_batch[:-1]), tf.int64)\n    # for the example above the result of this statement equals\n    # [0, 0, 0, 1, 0, 1]\n    # so data samples begin in 3rd and 5th positions (the indexes of ones)\n\n    # transforming sample start masks to the sample starts themselves\n    q = a * tf.cast(tf.range(n_words), tf.int64)\n    # [0, 0, 0, 3, 0, 5]\n    count_to_substract = tf.pad(tf.boolean_mask(q, q), [(1, 0)])\n    # [0, 3, 5]\n\n    new_word_indices = tf.cast(tf.range(n_words), tf.int64) - tf.gather(count_to_substract, tf.cumsum(a))\n    # tf.range(n_words) -> [0, 1, 2, 3, 4, 5]\n    # tf.cumsum(a) -> [0, 0, 0, 1, 1, 2]\n    # tf.gather(count_to_substract, tf.cumsum(a)) -> [0, 0, 0, 3, 3, 5]\n    # new_word_indices -> [0, 1, 2, 3, 4, 5] - [0, 0, 0, 3, 3, 5] = [0, 1, 2, 0, 1, 0]\n    # new_word_indices is the concatenation of range(word_len(sentence))\n    # for all sentences in units\n\n    n_total_word_elements = tf.cast(batch_size * max_token_seq_len, tf.int32)\n    word_indices_flat = tf.cast(idxs[:, 0] * max_token_seq_len + new_word_indices, tf.int32)\n    x_mask = tf.reduce_sum(tf.one_hot(word_indices_flat, n_total_word_elements), 0)\n    x_mask = tf.cast(x_mask, tf.bool)\n    # to get absolute indices we add max_token_seq_len:\n    # idxs[:, 0] * max_token_seq_len -> [0, 0, 0, 1, 1, 2] * 2 = [0, 0, 0, 3, 3, 6]\n    # word_indices_flat -> [0, 0, 0, 3, 3, 6] + [0, 1, 2, 0, 1, 0] = [0, 1, 2, 3, 4, 6]\n    # total number of words in the batch (including paddings)\n    # batch_size * max_token_seq_len -> 3 * 3 = 9\n    # tf.one_hot(...) ->\n    # [[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n    #  [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n    #  [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n    #  [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n    #  [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n    #  [0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n    #  x_mask -> [1, 1, 1, 1, 1, 0, 1, 0, 0]\n\n    full_range = tf.cast(tf.range(batch_size * max_token_seq_len), tf.int32)\n    # full_range -> [0, 1, 2, 3, 4, 5, 6, 7, 8]\n    nonword_indices_flat = tf.boolean_mask(full_range, tf.math.logical_not(x_mask))\n    # # y_idxs -> [5, 7, 8]\n\n    # get a sequence of units corresponding to the start subtokens of the words\n    # size: [n_words, n_features]\n    elements = tf.gather_nd(units, idxs)\n\n    # prepare zeros for paddings\n    # size: [batch_size * TOKEN_seq_length - n_words, n_features]\n    paddings = tf.zeros(tf.stack([tf.reduce_sum(max_token_seq_len - token_seq_lengths),\n                                  nf], 0), tf.float32)\n\n    tensor_flat = tf.dynamic_stitch([word_indices_flat, nonword_indices_flat],\n                                    [elements, paddings])\n    # tensor_flat -> [x, x, x, x, x, 0, x, 0, 0]\n\n    tensor = tf.reshape(tensor_flat, tf.stack([batch_size, max_token_seq_len, nf_int], 0))\n    # tensor -> [[x, x, x],\n    #            [x, x, 0],\n    #            [x, 0, 0]]\n\n    return tensor\n\n\n@register(\'bert_sequence_network\')\nclass BertSequenceNetwork(LRScheduledTFModel):\n    """"""\n    Basic class for BERT-based sequential architectures.\n\n    Args:\n        keep_prob: dropout keep_prob for non-Bert layers\n        bert_config_file: path to Bert configuration file\n        pretrained_bert: pretrained Bert checkpoint\n        attention_probs_keep_prob: keep_prob for Bert self-attention layers\n        hidden_keep_prob: keep_prob for Bert hidden layers\n        encoder_layer_ids: list of averaged layers from Bert encoder (layer ids)\n            optimizer: name of tf.train.* optimizer or None for `AdamWeightDecayOptimizer`\n            weight_decay_rate: L2 weight decay for `AdamWeightDecayOptimizer`\n        encoder_dropout: dropout probability of encoder output layer\n        ema_decay: what exponential moving averaging to use for network parameters, value from 0.0 to 1.0.\n            Values closer to 1.0 put weight on the parameters history and values closer to 0.0 corresponds put weight\n            on the current parameters.\n        ema_variables_on_cpu: whether to put EMA variables to CPU. It may save a lot of GPU memory\n        freeze_embeddings: set True to not train input embeddings set True to\n            not train input embeddings set True to not train input embeddings\n        learning_rate: learning rate of BERT head\n        bert_learning_rate: learning rate of BERT body\n        min_learning_rate: min value of learning rate if learning rate decay is used\n        learning_rate_drop_patience: how many validations with no improvements to wait\n        learning_rate_drop_div: the divider of the learning rate after `learning_rate_drop_patience` unsuccessful\n            validations\n        load_before_drop: whether to load best model before dropping learning rate or not\n        clip_norm: clip gradients by norm\n    """"""\n\n    def __init__(self,\n                 keep_prob: float,\n                 bert_config_file: str,\n                 pretrained_bert: str = None,\n                 attention_probs_keep_prob: float = None,\n                 hidden_keep_prob: float = None,\n                 encoder_layer_ids: List[int] = (-1,),\n                 encoder_dropout: float = 0.0,\n                 optimizer: str = None,\n                 weight_decay_rate: float = 1e-6,\n                 ema_decay: float = None,\n                 ema_variables_on_cpu: bool = True,\n                 freeze_embeddings: bool = False,\n                 learning_rate: float = 1e-3,\n                 bert_learning_rate: float = 2e-5,\n                 min_learning_rate: float = 1e-07,\n                 learning_rate_drop_patience: int = 20,\n                 learning_rate_drop_div: float = 2.0,\n                 load_before_drop: bool = True,\n                 clip_norm: float = 1.0,\n                 **kwargs) -> None:\n        super().__init__(learning_rate=learning_rate,\n                         learning_rate_drop_div=learning_rate_drop_div,\n                         learning_rate_drop_patience=learning_rate_drop_patience,\n                         load_before_drop=load_before_drop,\n                         clip_norm=clip_norm,\n                         **kwargs)\n        self.keep_prob = keep_prob\n        self.encoder_layer_ids = encoder_layer_ids\n        self.encoder_dropout = encoder_dropout\n        self.optimizer = optimizer\n        self.weight_decay_rate = weight_decay_rate\n        self.ema_decay = ema_decay\n        self.ema_variables_on_cpu = ema_variables_on_cpu\n        self.freeze_embeddings = freeze_embeddings\n        self.bert_learning_rate_multiplier = bert_learning_rate / learning_rate\n        self.min_learning_rate = min_learning_rate\n\n        self.bert_config = BertConfig.from_json_file(str(expand_path(bert_config_file)))\n\n        if attention_probs_keep_prob is not None:\n            self.bert_config.attention_probs_dropout_prob = 1.0 - attention_probs_keep_prob\n        if hidden_keep_prob is not None:\n            self.bert_config.hidden_dropout_prob = 1.0 - hidden_keep_prob\n\n        self.sess_config = tf.ConfigProto(allow_soft_placement=True)\n        self.sess_config.gpu_options.allow_growth = True\n        self.sess = tf.Session(config=self.sess_config)\n\n        self._init_graph()\n\n        self._init_optimizer()\n\n        self.sess.run(tf.global_variables_initializer())\n\n        if pretrained_bert is not None:\n            pretrained_bert = str(expand_path(pretrained_bert))\n\n            if tf.train.checkpoint_exists(pretrained_bert) \\\n                    and not (self.load_path and tf.train.checkpoint_exists(str(self.load_path.resolve()))):\n                log.info(\'[initializing model with Bert from {}]\'.format(pretrained_bert))\n                # Exclude optimizer and classification variables from saved variables\n                var_list = self._get_saveable_variables(\n                    exclude_scopes=(\'Optimizer\', \'learning_rate\', \'momentum\', \'ner\', \'EMA\'))\n                saver = tf.train.Saver(var_list)\n                saver.restore(self.sess, pretrained_bert)\n\n        if self.load_path is not None:\n            self.load()\n\n        if self.ema:\n            self.sess.run(self.ema.init_op)\n\n    def _init_graph(self) -> None:\n        self.seq_lengths = tf.reduce_sum(self.y_masks_ph, axis=1)\n\n        self.bert = BertModel(config=self.bert_config,\n                              is_training=self.is_train_ph,\n                              input_ids=self.input_ids_ph,\n                              input_mask=self.input_masks_ph,\n                              token_type_ids=self.token_types_ph,\n                              use_one_hot_embeddings=False)\n\n        with tf.variable_scope(\'ner\'):\n            layer_weights = tf.get_variable(\'layer_weights_\',\n                                            shape=len(self.encoder_layer_ids),\n                                            initializer=tf.ones_initializer(),\n                                            trainable=True)\n            layer_mask = tf.ones_like(layer_weights)\n            layer_mask = tf.nn.dropout(layer_mask, self.encoder_keep_prob_ph)\n            layer_weights *= layer_mask\n            # to prevent zero division\n            mask_sum = tf.maximum(tf.reduce_sum(layer_mask), 1.0)\n            layer_weights = tf.unstack(layer_weights / mask_sum)\n            # TODO: may be stack and reduce_sum is faster\n            units = sum(w * l for w, l in zip(layer_weights, self.encoder_layers()))\n            units = tf.nn.dropout(units, keep_prob=self.keep_prob_ph)\n        return units\n\n    def _get_tag_mask(self) -> tf.Tensor:\n        """"""\n        Returns: tag_mask,\n            a mask that selects positions corresponding to word tokens (not padding and `CLS`)\n        """"""\n        max_length = tf.reduce_max(self.seq_lengths)\n        one_hot_max_len = tf.one_hot(self.seq_lengths - 1, max_length)\n        tag_mask = tf.cumsum(one_hot_max_len[:, ::-1], axis=1)[:, ::-1]\n        return tag_mask\n\n    def encoder_layers(self):\n        """"""\n        Returns: the output of BERT layers specfied in ``self.encoder_layers_ids``\n        """"""\n        return [self.bert.all_encoder_layers[i] for i in self.encoder_layer_ids]\n\n    def _init_placeholders(self) -> None:\n        self.input_ids_ph = tf.placeholder(shape=(None, None),\n                                           dtype=tf.int32,\n                                           name=\'token_indices_ph\')\n        self.input_masks_ph = tf.placeholder(shape=(None, None),\n                                             dtype=tf.int32,\n                                             name=\'token_mask_ph\')\n        self.token_types_ph = \\\n            tf.placeholder_with_default(tf.zeros_like(self.input_ids_ph, dtype=tf.int32),\n                                        shape=self.input_ids_ph.shape,\n                                        name=\'token_types_ph\')\n        self.learning_rate_ph = tf.placeholder_with_default(0.0, shape=[], name=\'learning_rate_ph\')\n        self.keep_prob_ph = tf.placeholder_with_default(1.0, shape=[], name=\'keep_prob_ph\')\n        self.encoder_keep_prob_ph = tf.placeholder_with_default(1.0, shape=[], name=\'encoder_keep_prob_ph\')\n        self.is_train_ph = tf.placeholder_with_default(False, shape=[], name=\'is_train_ph\')\n\n    def _init_optimizer(self) -> None:\n        with tf.variable_scope(\'Optimizer\'):\n            self.global_step = tf.get_variable(\'global_step\',\n                                               shape=[],\n                                               dtype=tf.int32,\n                                               initializer=tf.constant_initializer(0),\n                                               trainable=False)\n            # default optimizer for Bert is Adam with fixed L2 regularization\n\n        if self.optimizer is None:\n            self.train_op = \\\n                self.get_train_op(self.loss,\n                                  learning_rate=self.learning_rate_ph,\n                                  optimizer=AdamWeightDecayOptimizer,\n                                  weight_decay_rate=self.weight_decay_rate,\n                                  beta_1=0.9,\n                                  beta_2=0.999,\n                                  epsilon=1e-6,\n                                  optimizer_scope_name=\'Optimizer\',\n                                  exclude_from_weight_decay=[""LayerNorm"",\n                                                             ""layer_norm"",\n                                                             ""bias"",\n                                                             ""EMA""])\n        else:\n            self.train_op = self.get_train_op(self.loss,\n                                              learning_rate=self.learning_rate_ph,\n                                              optimizer_scope_name=\'Optimizer\')\n\n        if self.optimizer is None:\n            with tf.variable_scope(\'Optimizer\'):\n                new_global_step = self.global_step + 1\n                self.train_op = tf.group(self.train_op, [self.global_step.assign(new_global_step)])\n\n        if self.ema_decay is not None:\n            _vars = self._get_trainable_variables(exclude_scopes=[""Optimizer"",\n                                                                  ""LayerNorm"",\n                                                                  ""layer_norm"",\n                                                                  ""bias"",\n                                                                  ""learning_rate"",\n                                                                  ""momentum""])\n\n            self.ema = ExponentialMovingAverage(self.ema_decay,\n                                                variables_on_cpu=self.ema_variables_on_cpu)\n            self.train_op = self.ema.build(self.train_op, _vars, name=""EMA"")\n        else:\n            self.ema = None\n\n    def get_train_op(self, loss: tf.Tensor, learning_rate: Union[tf.Tensor, float], **kwargs) -> tf.Operation:\n        assert ""learnable_scopes"" not in kwargs, ""learnable scopes unsupported""\n        # train_op for bert variables\n        kwargs[\'learnable_scopes\'] = (\'bert/encoder\', \'bert/embeddings\')\n        if self.freeze_embeddings:\n            kwargs[\'learnable_scopes\'] = (\'bert/encoder\',)\n        bert_learning_rate = learning_rate * self.bert_learning_rate_multiplier\n        bert_train_op = super().get_train_op(loss,\n                                             bert_learning_rate,\n                                             **kwargs)\n        # train_op for ner head variables\n        kwargs[\'learnable_scopes\'] = (\'ner\',)\n        head_train_op = super().get_train_op(loss,\n                                             learning_rate,\n                                             **kwargs)\n        return tf.group(bert_train_op, head_train_op)\n\n    def _build_basic_feed_dict(self, input_ids: tf.Tensor, input_masks: tf.Tensor,\n                               token_types: Optional[tf.Tensor]=None, train: bool=False) -> dict:\n        """"""Fills the feed_dict with the tensors defined in the basic class.\n        You need to update this dict by the values of output placeholders\n        and class-specific network inputs in your derived class.\n        """"""\n        feed_dict = {\n            self.input_ids_ph: input_ids,\n            self.input_masks_ph: input_masks,\n        }\n        if token_types is not None:\n            feed_dict[self.token_types_ph] = token_types\n        if train:\n            feed_dict.update({\n                self.learning_rate_ph: max(self.get_learning_rate(), self.min_learning_rate),\n                self.keep_prob_ph: self.keep_prob,\n                self.encoder_keep_prob_ph: 1.0 - self.encoder_dropout,\n                self.is_train_ph: True,\n            })\n\n        return feed_dict\n\n    def _build_feed_dict(self, input_ids, input_masks, token_types=None, *args,  **kwargs):\n        raise NotImplementedError(""You must implement _build_feed_dict in your derived class."")\n\n    def train_on_batch(self,\n                       input_ids: Union[List[List[int]], np.ndarray],\n                       input_masks: Union[List[List[int]], np.ndarray],\n                       *args, **kwargs) -> Dict[str, float]:\n        """"""\n\n        Args:\n            input_ids: batch of indices of subwords\n            input_masks: batch of masks which determine what should be attended\n            args: arguments passed  to _build_feed_dict\n                and corresponding to additional input\n                and output tensors of the derived class.\n            kwargs: keyword arguments passed to _build_feed_dict\n                and corresponding to additional input\n                and output tensors of the derived class.\n\n        Returns:\n            dict with fields \'loss\', \'head_learning_rate\', and \'bert_learning_rate\'\n        """"""\n        feed_dict = self._build_feed_dict(input_ids, input_masks, *args, **kwargs)\n\n        if self.ema:\n            self.sess.run(self.ema.switch_to_train_op)\n        _, loss, lr = self.sess.run([self.train_op, self.loss, self.learning_rate_ph],\n                                     feed_dict=feed_dict)\n        return {\'loss\': loss,\n                \'head_learning_rate\': float(lr),\n                \'bert_learning_rate\': float(lr) * self.bert_learning_rate_multiplier}\n\n    def __call__(self,\n                 input_ids: Union[List[List[int]], np.ndarray],\n                 input_masks: Union[List[List[int]], np.ndarray],\n                 **kwargs) -> Union[List[List[int]], List[np.ndarray]]:\n        raise NotImplementedError(""You must implement method __call__ in your derived class."")\n\n    def save(self, exclude_scopes=(\'Optimizer\', \'EMA/BackupVariables\')) -> None:\n        if self.ema:\n            self.sess.run(self.ema.switch_to_train_op)\n        return super().save(exclude_scopes=exclude_scopes)\n\n    def load(self,\n             exclude_scopes=(\'Optimizer\',\n                             \'learning_rate\',\n                             \'momentum\',\n                             \'EMA/BackupVariables\'),\n             **kwargs) -> None:\n        return super().load(exclude_scopes=exclude_scopes, **kwargs)\n\n\n@register(\'bert_sequence_tagger\')\nclass BertSequenceTagger(BertSequenceNetwork):\n    """"""BERT-based model for text tagging. It predicts a label for every token (not subtoken) in the text.\n    You can use it for sequence labeling tasks, such as morphological tagging or named entity recognition.\n    See :class:`deeppavlov.models.bert.bert_sequence_tagger.BertSequenceNetwork`\n    for the description of inherited parameters.\n\n    Args:\n        n_tags: number of distinct tags\n        use_crf: whether to use CRF on top or not\n        use_birnn: whether to use bidirection rnn after BERT layers.\n            For NER and morphological tagging we usually set it to `False` as otherwise the model overfits\n        birnn_cell_type: the type of Bidirectional RNN. Either `lstm` or `gru`\n        birnn_hidden_size: number of hidden units in the BiRNN layer in each direction\n        return_probas: set this to `True` if you need the probabilities instead of raw answers\n    """"""\n\n    def __init__(self,\n                 n_tags: List[str],\n                 keep_prob: float,\n                 bert_config_file: str,\n                 pretrained_bert: str = None,\n                 attention_probs_keep_prob: float = None,\n                 hidden_keep_prob: float = None,\n                 use_crf=False,\n                 encoder_layer_ids: List[int] = (-1,),\n                 encoder_dropout: float = 0.0,\n                 optimizer: str = None,\n                 weight_decay_rate: float = 1e-6,\n                 use_birnn: bool = False,\n                 birnn_cell_type: str = \'lstm\',\n                 birnn_hidden_size: int = 128,\n                 ema_decay: float = None,\n                 ema_variables_on_cpu: bool = True,\n                 return_probas: bool = False,\n                 freeze_embeddings: bool = False,\n                 learning_rate: float = 1e-3,\n                 bert_learning_rate: float = 2e-5,\n                 min_learning_rate: float = 1e-07,\n                 learning_rate_drop_patience: int = 20,\n                 learning_rate_drop_div: float = 2.0,\n                 load_before_drop: bool = True,\n                 clip_norm: float = 1.0,\n                 **kwargs) -> None:\n        self.n_tags = n_tags\n        self.use_crf = use_crf\n        self.use_birnn = use_birnn\n        self.birnn_cell_type = birnn_cell_type\n        self.birnn_hidden_size = birnn_hidden_size\n        self.return_probas = return_probas\n        super().__init__(keep_prob=keep_prob,\n                         bert_config_file=bert_config_file,\n                         pretrained_bert=pretrained_bert,\n                         attention_probs_keep_prob=attention_probs_keep_prob,\n                         hidden_keep_prob=hidden_keep_prob,\n                         encoder_layer_ids=encoder_layer_ids,\n                         encoder_dropout=encoder_dropout,\n                         optimizer=optimizer,\n                         weight_decay_rate=weight_decay_rate,\n                         ema_decay=ema_decay,\n                         ema_variables_on_cpu=ema_variables_on_cpu,\n                         freeze_embeddings=freeze_embeddings,\n                         learning_rate=learning_rate,\n                         bert_learning_rate=bert_learning_rate,\n                         min_learning_rate=min_learning_rate,\n                         learning_rate_drop_div=learning_rate_drop_div,\n                         learning_rate_drop_patience=learning_rate_drop_patience,\n                         load_before_drop=load_before_drop,\n                         clip_norm=clip_norm,\n                         **kwargs)\n\n    def _init_graph(self) -> None:\n        self._init_placeholders()\n\n        units = super()._init_graph()\n\n        with tf.variable_scope(\'ner\'):\n            if self.use_birnn:\n                units, _ = bi_rnn(units,\n                                  self.birnn_hidden_size,\n                                  cell_type=self.birnn_cell_type,\n                                  seq_lengths=self.seq_lengths,\n                                  name=\'birnn\')\n                units = tf.concat(units, -1)\n            # TODO: maybe add one more layer?\n            logits = tf.layers.dense(units, units=self.n_tags, name=""output_dense"")\n\n            self.logits = token_from_subtoken(logits, self.y_masks_ph)\n\n            # CRF\n            if self.use_crf:\n                transition_params = tf.get_variable(\'Transition_Params\',\n                                                    shape=[self.n_tags, self.n_tags],\n                                                    initializer=tf.zeros_initializer())\n                log_likelihood, transition_params = \\\n                    tf.contrib.crf.crf_log_likelihood(self.logits,\n                                                      self.y_ph,\n                                                      self.seq_lengths,\n                                                      transition_params)\n                loss_tensor = -log_likelihood\n                self._transition_params = transition_params\n\n            self.y_predictions = tf.argmax(self.logits, -1)\n            self.y_probas = tf.nn.softmax(self.logits, axis=2)\n\n        with tf.variable_scope(""loss""):\n            tag_mask = self._get_tag_mask()\n            y_mask = tf.cast(tag_mask, tf.float32)\n            if self.use_crf:\n                self.loss = tf.reduce_mean(loss_tensor)\n            else:\n                self.loss = tf.losses.sparse_softmax_cross_entropy(labels=self.y_ph,\n                                                                   logits=self.logits,\n                                                                   weights=y_mask)\n\n    def _init_placeholders(self) -> None:\n        super()._init_placeholders()\n        self.y_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name=\'y_ph\')\n        self.y_masks_ph = tf.placeholder(shape=(None, None),\n                                         dtype=tf.int32,\n                                         name=\'y_mask_ph\')\n\n    def _decode_crf(self, feed_dict: Dict[tf.Tensor, np.ndarray]) -> List[np.ndarray]:\n        logits, trans_params, mask, seq_lengths = self.sess.run([self.logits,\n                                                                 self._transition_params,\n                                                                 self.y_masks_ph,\n                                                                 self.seq_lengths],\n                                                                feed_dict=feed_dict)\n        # iterate over the sentences because no batching in viterbi_decode\n        y_pred = []\n        for logit, sequence_length in zip(logits, seq_lengths):\n            logit = logit[:int(sequence_length)]  # keep only the valid steps\n            viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(logit, trans_params)\n            y_pred += [viterbi_seq]\n        return y_pred\n\n    def _build_feed_dict(self, input_ids, input_masks, y_masks, y=None):\n        feed_dict = self._build_basic_feed_dict(input_ids, input_masks, train=(y is not None))\n        feed_dict[self.y_masks_ph] = y_masks\n        if y is not None:\n            feed_dict[self.y_ph] = y\n        return feed_dict\n\n    def __call__(self,\n                 input_ids: Union[List[List[int]], np.ndarray],\n                 input_masks: Union[List[List[int]], np.ndarray],\n                 y_masks: Union[List[List[int]], np.ndarray]) -> Union[List[List[int]], List[np.ndarray]]:\n        """""" Predicts tag indices for a given subword tokens batch\n\n        Args:\n            input_ids: indices of the subwords\n            input_masks: mask that determines where to attend and where not to\n            y_masks: mask which determines the first subword units in the the word\n\n        Returns:\n            Label indices or class probabilities for each token (not subtoken)\n\n        """"""\n        feed_dict = self._build_feed_dict(input_ids, input_masks, y_masks)\n        if self.ema:\n            self.sess.run(self.ema.switch_to_test_op)\n        if not self.return_probas:\n            if self.use_crf:\n                pred = self._decode_crf(feed_dict)\n            else:\n                pred, seq_lengths = self.sess.run([self.y_predictions, self.seq_lengths], feed_dict=feed_dict)\n                pred = [p[:l] for l, p in zip(seq_lengths, pred)]\n        else:\n            pred = self.sess.run(self.y_probas, feed_dict=feed_dict)\n        return pred\n\n\nclass ExponentialMovingAverage:\n    def __init__(self,\n                 decay: float = 0.999,\n                 variables_on_cpu: bool = True) -> None:\n        self.decay = decay\n        self.ema = tf.train.ExponentialMovingAverage(decay=decay)\n        self.var_device_name = \'/cpu:0\' if variables_on_cpu else None\n        self.train_mode = None\n\n    def build(self,\n              minimize_op: tf.Tensor,\n              update_vars: List[tf.Variable] = None,\n              name: str = ""EMA"") -> tf.Tensor:\n        with tf.variable_scope(name):\n            if update_vars is None:\n                update_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n\n            with tf.control_dependencies([minimize_op]):\n                minimize_op = self.ema.apply(update_vars)\n\n            with tf.device(self.var_device_name):\n                # Make backup variables\n                with tf.variable_scope(\'BackupVariables\'):\n                    backup_vars = [tf.get_variable(var.op.name,\n                                                   dtype=var.value().dtype,\n                                                   trainable=False,\n                                                   initializer=var.initialized_value())\n                                   for var in update_vars]\n\n                def ema_to_weights():\n                    return tf.group(*(tf.assign(var, self.ema.average(var).read_value())\n                                      for var in update_vars))\n\n                def save_weight_backups():\n                    return tf.group(*(tf.assign(bck, var.read_value())\n                                      for var, bck in zip(update_vars, backup_vars)))\n\n                def restore_weight_backups():\n                    return tf.group(*(tf.assign(var, bck.read_value())\n                                      for var, bck in zip(update_vars, backup_vars)))\n\n                train_switch_op = restore_weight_backups()\n                with tf.control_dependencies([save_weight_backups()]):\n                    test_switch_op = ema_to_weights()\n\n            self.train_switch_op = train_switch_op\n            self.test_switch_op = test_switch_op\n            self.do_nothing_op = tf.no_op()\n\n        return minimize_op\n\n    @property\n    def init_op(self) -> tf.Operation:\n        self.train_mode = False\n        return self.test_switch_op\n\n    @property\n    def switch_to_train_op(self) -> tf.Operation:\n        assert self.train_mode is not None, ""ema variables aren\'t initialized""\n        if not self.train_mode:\n            # log.info(""switching to train mode"")\n            self.train_mode = True\n            return self.train_switch_op\n        return self.do_nothing_op\n\n    @property\n    def switch_to_test_op(self) -> tf.Operation:\n        assert self.train_mode is not None, ""ema variables aren\'t initialized""\n        if self.train_mode:\n            # log.info(""switching to test mode"")\n            self.train_mode = False\n            return self.test_switch_op\n        return self.do_nothing_op\n'"
deeppavlov/models/bert/bert_squad.py,50,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport math\nfrom logging import getLogger\nfrom typing import List, Tuple, Optional, Dict\n\nimport numpy as np\nimport tensorflow as tf\nfrom bert_dp.modeling import BertConfig, BertModel\nfrom bert_dp.optimization import AdamWeightDecayOptimizer\nfrom bert_dp.preprocessing import InputFeatures\nfrom bert_dp.tokenization import FullTokenizer\n\nfrom deeppavlov import build_model\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.estimator import Component\nfrom deeppavlov.core.models.tf_model import LRScheduledTFModel\nfrom deeppavlov.models.squad.utils import softmax_mask\n\nlogger = getLogger(__name__)\n\n\n@register(\'squad_bert_model\')\nclass BertSQuADModel(LRScheduledTFModel):\n    """"""Bert-based model for SQuAD-like problem setting:\n    It predicts start and end position of answer for given question and context.\n\n    [CLS] token is used as no_answer. If model selects [CLS] token as most probable\n    answer, it means that there is no answer in given context.\n\n    Start and end position of answer are predicted by linear transformation\n    of Bert outputs.\n\n    Args:\n        bert_config_file: path to Bert configuration file\n        keep_prob: dropout keep_prob for non-Bert layers\n        attention_probs_keep_prob: keep_prob for Bert self-attention layers\n        hidden_keep_prob: keep_prob for Bert hidden layers\n        optimizer: name of tf.train.* optimizer or None for `AdamWeightDecayOptimizer`\n        weight_decay_rate: L2 weight decay for `AdamWeightDecayOptimizer`\n        pretrained_bert: pretrained Bert checkpoint\n        min_learning_rate: min value of learning rate if learning rate decay is used\n    """"""\n\n    def __init__(self, bert_config_file: str,\n                 keep_prob: float,\n                 attention_probs_keep_prob: Optional[float] = None,\n                 hidden_keep_prob: Optional[float] = None,\n                 optimizer: Optional[str] = None,\n                 weight_decay_rate: Optional[float] = 0.01,\n                 pretrained_bert: Optional[str] = None,\n                 min_learning_rate: float = 1e-06, **kwargs) -> None:\n        super().__init__(**kwargs)\n\n        self.min_learning_rate = min_learning_rate\n        self.keep_prob = keep_prob\n        self.optimizer = optimizer\n        self.weight_decay_rate = weight_decay_rate\n\n        self.bert_config = BertConfig.from_json_file(str(expand_path(bert_config_file)))\n\n        if attention_probs_keep_prob is not None:\n            self.bert_config.attention_probs_dropout_prob = 1.0 - attention_probs_keep_prob\n        if hidden_keep_prob is not None:\n            self.bert_config.hidden_dropout_prob = 1.0 - hidden_keep_prob\n\n        self.sess_config = tf.ConfigProto(allow_soft_placement=True)\n        self.sess_config.gpu_options.allow_growth = True\n        self.sess = tf.Session(config=self.sess_config)\n\n        self._init_graph()\n\n        self._init_optimizer()\n\n        self.sess.run(tf.global_variables_initializer())\n\n        if pretrained_bert is not None:\n            pretrained_bert = str(expand_path(pretrained_bert))\n\n            if tf.train.checkpoint_exists(pretrained_bert) \\\n                    and not (self.load_path and tf.train.checkpoint_exists(str(self.load_path.resolve()))):\n                logger.info(\'[initializing model with Bert from {}]\'.format(pretrained_bert))\n                var_list = self._get_saveable_variables(\n                    exclude_scopes=(\'Optimizer\', \'learning_rate\', \'momentum\', \'squad\'))\n                saver = tf.train.Saver(var_list)\n                saver.restore(self.sess, pretrained_bert)\n\n        if self.load_path is not None:\n            self.load()\n\n    def _init_graph(self):\n        self._init_placeholders()\n\n        seq_len = tf.shape(self.input_ids_ph)[-1]\n        self.y_st = tf.one_hot(self.y_st_ph, depth=seq_len)\n        self.y_end = tf.one_hot(self.y_end_ph, depth=seq_len)\n\n        self.bert = BertModel(config=self.bert_config,\n                              is_training=self.is_train_ph,\n                              input_ids=self.input_ids_ph,\n                              input_mask=self.input_masks_ph,\n                              token_type_ids=self.token_types_ph,\n                              use_one_hot_embeddings=False,\n                              )\n\n        last_layer = self.bert.get_sequence_output()\n        hidden_size = last_layer.get_shape().as_list()[-1]\n        bs = tf.shape(last_layer)[0]\n\n        with tf.variable_scope(\'squad\'):\n            output_weights = tf.get_variable(\'output_weights\', [2, hidden_size],\n                                             initializer=tf.truncated_normal_initializer(stddev=0.02))\n            output_bias = tf.get_variable(\'output_bias\', [2], initializer=tf.zeros_initializer())\n\n            last_layer_rs = tf.reshape(last_layer, [-1, hidden_size])\n\n            logits = tf.matmul(last_layer_rs, output_weights, transpose_b=True)\n            logits = tf.nn.bias_add(logits, output_bias)\n            logits = tf.reshape(logits, [bs, -1, 2])\n            logits = tf.transpose(logits, [2, 0, 1])\n\n            logits_st, logits_end = tf.unstack(logits, axis=0)\n\n            logit_mask = self.token_types_ph\n            # [CLS] token is used as no answer\n            mask = tf.concat([tf.ones((bs, 1), dtype=tf.int32), tf.zeros((bs, seq_len - 1), dtype=tf.int32)], axis=-1)\n            logit_mask = logit_mask + mask\n\n            logits_st = softmax_mask(logits_st, logit_mask)\n            logits_end = softmax_mask(logits_end, logit_mask)\n            start_probs = tf.nn.softmax(logits_st)\n            end_probs = tf.nn.softmax(logits_end)\n\n            outer = tf.matmul(tf.expand_dims(start_probs, axis=2), tf.expand_dims(end_probs, axis=1))\n            outer_logits = tf.exp(tf.expand_dims(logits_st, axis=2) + tf.expand_dims(logits_end, axis=1))\n\n            context_max_len = tf.reduce_max(tf.reduce_sum(self.token_types_ph, axis=1))\n\n            max_ans_length = tf.cast(tf.minimum(20, context_max_len), tf.int64)\n            outer = tf.matrix_band_part(outer, 0, max_ans_length)\n            outer_logits = tf.matrix_band_part(outer_logits, 0, max_ans_length)\n\n            self.yp_score = 1 - tf.nn.softmax(logits_st)[:, 0] * tf.nn.softmax(logits_end)[:, 0]\n\n            self.start_probs = start_probs\n            self.end_probs = end_probs\n            self.start_pred = tf.argmax(tf.reduce_max(outer, axis=2), axis=1)\n            self.end_pred = tf.argmax(tf.reduce_max(outer, axis=1), axis=1)\n            self.yp_logits = tf.reduce_max(tf.reduce_max(outer_logits, axis=2), axis=1)\n\n        with tf.variable_scope(""loss""):\n            loss_st = tf.nn.softmax_cross_entropy_with_logits(logits=logits_st, labels=self.y_st)\n            loss_end = tf.nn.softmax_cross_entropy_with_logits(logits=logits_end, labels=self.y_end)\n            self.loss = tf.reduce_mean(loss_st + loss_end)\n\n    def _init_placeholders(self):\n        self.input_ids_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name=\'ids_ph\')\n        self.input_masks_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name=\'masks_ph\')\n        self.token_types_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name=\'token_types_ph\')\n\n        self.y_st_ph = tf.placeholder(shape=(None,), dtype=tf.int32, name=\'y_st_ph\')\n        self.y_end_ph = tf.placeholder(shape=(None,), dtype=tf.int32, name=\'y_end_ph\')\n\n        self.learning_rate_ph = tf.placeholder_with_default(0.0, shape=[], name=\'learning_rate_ph\')\n        self.keep_prob_ph = tf.placeholder_with_default(1.0, shape=[], name=\'keep_prob_ph\')\n        self.is_train_ph = tf.placeholder_with_default(False, shape=[], name=\'is_train_ph\')\n\n    def _init_optimizer(self):\n        with tf.variable_scope(\'Optimizer\'):\n            self.global_step = tf.get_variable(\'global_step\', shape=[], dtype=tf.int32,\n                                               initializer=tf.constant_initializer(0), trainable=False)\n            # default optimizer for Bert is Adam with fixed L2 regularization\n            if self.optimizer is None:\n\n                self.train_op = self.get_train_op(self.loss, learning_rate=self.learning_rate_ph,\n                                                  optimizer=AdamWeightDecayOptimizer,\n                                                  weight_decay_rate=self.weight_decay_rate,\n                                                  beta_1=0.9,\n                                                  beta_2=0.999,\n                                                  epsilon=1e-6,\n                                                  exclude_from_weight_decay=[""LayerNorm"", ""layer_norm"", ""bias""]\n                                                  )\n            else:\n                self.train_op = self.get_train_op(self.loss, learning_rate=self.learning_rate_ph)\n\n            if self.optimizer is None:\n                new_global_step = self.global_step + 1\n                self.train_op = tf.group(self.train_op, [self.global_step.assign(new_global_step)])\n\n    def _build_feed_dict(self, input_ids, input_masks, token_types, y_st=None, y_end=None):\n        feed_dict = {\n            self.input_ids_ph: input_ids,\n            self.input_masks_ph: input_masks,\n            self.token_types_ph: token_types,\n        }\n        if y_st is not None and y_end is not None:\n            feed_dict.update({\n                self.y_st_ph: y_st,\n                self.y_end_ph: y_end,\n                self.learning_rate_ph: max(self.get_learning_rate(), self.min_learning_rate),\n                self.keep_prob_ph: self.keep_prob,\n                self.is_train_ph: True,\n            })\n\n        return feed_dict\n\n    def train_on_batch(self, features: List[InputFeatures], y_st: List[List[int]], y_end: List[List[int]]) -> Dict:\n        """"""Train model on given batch.\n        This method calls train_op using features and labels from y_st and y_end\n\n        Args:\n            features: batch of InputFeatures instances\n            y_st: batch of lists of ground truth answer start positions\n            y_end: batch of lists of ground truth answer end positions\n\n        Returns:\n            dict with loss and learning_rate values\n\n        """"""\n        input_ids = [f.input_ids for f in features]\n        input_masks = [f.input_mask for f in features]\n        input_type_ids = [f.input_type_ids for f in features]\n\n        y_st = [x[0] for x in y_st]\n        y_end = [x[0] for x in y_end]\n\n        feed_dict = self._build_feed_dict(input_ids, input_masks, input_type_ids, y_st, y_end)\n\n        _, loss = self.sess.run([self.train_op, self.loss], feed_dict=feed_dict)\n        return {\'loss\': loss, \'learning_rate\': feed_dict[self.learning_rate_ph]}\n\n    def __call__(self, features: List[InputFeatures]) -> Tuple[List[int], List[int], List[float], List[float]]:\n        """"""get predictions using features as input\n\n        Args:\n            features: batch of InputFeatures instances\n\n        Returns:\n            predictions: start, end positions, logits for answer and no_answer score\n\n        """"""\n        input_ids = [f.input_ids for f in features]\n        input_masks = [f.input_mask for f in features]\n        input_type_ids = [f.input_type_ids for f in features]\n\n        feed_dict = self._build_feed_dict(input_ids, input_masks, input_type_ids)\n        st, end, logits, scores = self.sess.run([self.start_pred, self.end_pred, self.yp_logits, self.yp_score],\n                                                feed_dict=feed_dict)\n        return st, end, logits.tolist(), scores.tolist()\n\n\n@register(\'squad_bert_infer\')\nclass BertSQuADInferModel(Component):\n    """"""This model wraps BertSQuADModel to make predictions on longer than 512 tokens sequences.\n\n    It splits context on chunks with `max_seq_length - 3 - len(question)` length, preserving sentences boundaries.\n\n    It reassembles batches with chunks instead of full contexts to optimize performance, e.g.,:\n        batch_size = 5\n        number_of_contexts == 2\n        number of first context chunks == 8\n        number of second context chunks == 2\n\n        we will create two batches with 5 chunks\n\n    For each context the best answer is selected via logits or scores from BertSQuADModel.\n\n\n    Args:\n        squad_model_config: path to DeepPavlov BertSQuADModel config file\n        vocab_file: path to Bert vocab file\n        do_lower_case: set True if lowercasing is needed\n        max_seq_length: max sequence length in subtokens, including [SEP] and [CLS] tokens\n        batch_size: size of batch to use during inference\n        lang: either `en` or `ru`, it is used to select sentence tokenizer\n\n    """"""\n\n    def __init__(self, squad_model_config: str,\n                 vocab_file: str,\n                 do_lower_case: bool,\n                 max_seq_length: int = 512,\n                 batch_size: int = 10,\n                 lang=\'en\', **kwargs) -> None:\n        config = json.load(open(squad_model_config))\n        config[\'chainer\'][\'pipe\'][0][\'max_seq_length\'] = max_seq_length\n        self.model = build_model(config)\n        self.max_seq_length = max_seq_length\n        vocab_file = str(expand_path(vocab_file))\n        self.tokenizer = FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n        self.batch_size = batch_size\n\n        if lang == \'en\':\n            from nltk import sent_tokenize\n            self.sent_tokenizer = sent_tokenize\n        elif lang == \'ru\':\n            from ru_sent_tokenize import ru_sent_tokenize\n            self.sent_tokenizer = ru_sent_tokenize\n        else:\n            raise RuntimeError(\'en and ru languages are supported only\')\n\n    def __call__(self, contexts: List[str], questions: List[str], **kwargs) -> Tuple[List[str], List[int], List[float]]:\n        """"""get predictions for given contexts and questions\n\n        Args:\n            contexts: batch of contexts\n            questions: batch of questions\n\n        Returns:\n            predictions: answer, answer start position, logits or scores\n\n        """"""\n        batch_indices = []\n        contexts_to_predict = []\n        questions_to_predict = []\n        predictions = {}\n        for i, (context, question) in enumerate(zip(contexts, questions)):\n            context_subtokens = self.tokenizer.tokenize(context)\n            question_subtokens = self.tokenizer.tokenize(question)\n            max_chunk_len = self.max_seq_length - len(question_subtokens) - 3\n            if 0 < max_chunk_len < len(context_subtokens):\n                number_of_chunks = math.ceil(len(context_subtokens) / max_chunk_len)\n                sentences = self.sent_tokenizer(context)\n                for chunk in np.array_split(sentences, number_of_chunks):\n                    contexts_to_predict += [\' \'.join(chunk)]\n                    questions_to_predict += [question]\n                    batch_indices += [i]\n            else:\n                contexts_to_predict += [context]\n                questions_to_predict += [question]\n                batch_indices += [i]\n\n        for j in range(0, len(contexts_to_predict), self.batch_size):\n            c_batch = contexts_to_predict[j: j + self.batch_size]\n            q_batch = questions_to_predict[j: j + self.batch_size]\n            ind_batch = batch_indices[j: j + self.batch_size]\n            a_batch, a_st_batch, logits_batch = self.model(c_batch, q_batch)\n            for a, a_st, logits, ind in zip(a_batch, a_st_batch, logits_batch, ind_batch):\n                if ind in predictions:\n                    predictions[ind] += [(a, a_st, logits)]\n                else:\n                    predictions[ind] = [(a, a_st, logits)]\n\n        answers, answer_starts, logits = [], [], []\n        for ind in sorted(predictions.keys()):\n            prediction = predictions[ind]\n            best_answer_ind = np.argmax([p[2] for p in prediction])\n            answers += [prediction[best_answer_ind][0]]\n            answer_starts += [prediction[best_answer_ind][1]]\n            logits += [prediction[best_answer_ind][2]]\n\n        return answers, answer_starts, logits\n'"
deeppavlov/models/classifiers/__init__.py,0,b''
deeppavlov/models/classifiers/cos_sim_classifier.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, softwaredata\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom logging import getLogger\nfrom typing import List, Tuple, Union\n\nimport numpy as np\nfrom scipy.sparse import vstack, csr_matrix\nfrom scipy.sparse.linalg import norm as sparse_norm\n\nfrom deeppavlov.core.common.file import load_pickle\nfrom deeppavlov.core.common.file import save_pickle\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.estimator import Estimator\nfrom deeppavlov.core.models.serializable import Serializable\n\nlogger = getLogger(__name__)\n\n\n@register(""cos_sim_classifier"")\nclass CosineSimilarityClassifier(Estimator, Serializable):\n    """"""\n    Classifier based on cosine similarity between vectorized sentences\n\n    Parameters:\n        save_path: path to save the model\n        load_path: path to load the model\n    """"""\n\n    def __init__(self, top_n: int = 1, save_path: str = None, load_path: str = None, **kwargs) -> None:\n        super().__init__(save_path=save_path, load_path=load_path, **kwargs)\n        self.top_n = top_n\n\n        self.x_train_features = self.y_train = None\n\n        if kwargs[\'mode\'] != \'train\':\n            self.load()\n\n    def __call__(self, q_vects: Union[csr_matrix, List]) -> Tuple[List[str], List[int]]:\n        """"""Found most similar answer for input vectorized question\n\n        Parameters:\n            q_vects: vectorized questions\n\n        Returns:\n            Tuple of Answer and Score\n        """"""\n\n        if isinstance(q_vects[0], csr_matrix):\n            q_norm = sparse_norm(q_vects)\n            if q_norm == 0.0:\n                cos_similarities = np.zeros((q_vects.shape[0], self.x_train_features.shape[0]))\n            else:\n                norm = q_norm * sparse_norm(self.x_train_features, axis=1)\n                cos_similarities = np.array(q_vects.dot(self.x_train_features.T).todense())\n                cos_similarities = cos_similarities / norm\n        elif isinstance(q_vects[0], np.ndarray):\n            q_vects = np.array(q_vects)\n            self.x_train_features = np.array(self.x_train_features)\n            norm = np.linalg.norm(q_vects) * np.linalg.norm(self.x_train_features, axis=1)\n            cos_similarities = q_vects.dot(self.x_train_features.T) / norm\n        elif q_vects[0] is None:\n            cos_similarities = np.zeros(len(self.x_train_features))\n        else:\n            raise NotImplementedError(\'Not implemented this type of vectors\')\n\n        # get cosine similarity for each class\n        y_labels = np.unique(self.y_train)\n        labels_scores = np.zeros((len(cos_similarities), len(y_labels)))\n        for i, label in enumerate(y_labels):\n            labels_scores[:, i] = np.max([cos_similarities[:, i]\n                                          for i, value in enumerate(self.y_train) if value == label], axis=0)\n\n        labels_scores_sum = labels_scores.sum(axis=1, keepdims=True)\n        labels_scores = np.divide(labels_scores, labels_scores_sum,\n                                  out=np.zeros_like(labels_scores), where=(labels_scores_sum != 0))\n\n        answer_ids = np.argsort(labels_scores)[:, -self.top_n:]\n\n        # generate top_n answers and scores\n        answers = []\n        scores = []\n        for i in range(len(answer_ids)):\n            answers.extend([y_labels[id] for id in answer_ids[i, ::-1]])\n            scores.extend([np.round(labels_scores[i, id], 2) for id in answer_ids[i, ::-1]])\n\n        return answers, scores\n\n    def fit(self, x_train_vects: Tuple[Union[csr_matrix, List]], y_train: Tuple[str]) -> None:\n        """"""Train classifier\n\n        Parameters:\n            x_train_vects: vectorized question for train dataset\n            y_train: answers for train dataset\n\n        Returns:\n            None\n        """"""\n        if isinstance(x_train_vects, tuple):\n            if len(x_train_vects) != 0:\n                if isinstance(x_train_vects[0], csr_matrix):\n                    self.x_train_features = vstack(list(x_train_vects))\n                elif isinstance(x_train_vects[0], np.ndarray):\n                    self.x_train_features = np.vstack(list(x_train_vects))\n                else:\n                    raise NotImplementedError(\'Not implemented this type of vectors\')\n            else:\n                raise ValueError(""Train vectors can\'t be empty"")\n        else:\n            self.x_train_features = x_train_vects\n\n        self.y_train = list(y_train)\n\n    def save(self) -> None:\n        """"""Save classifier parameters""""""\n        logger.info(""Saving faq_model to {}"".format(self.save_path))\n        save_pickle((self.x_train_features, self.y_train), self.save_path)\n\n    def load(self) -> None:\n        """"""Load classifier parameters""""""\n        logger.info(""Loading faq_model from {}"".format(self.load_path))\n        self.x_train_features, self.y_train = load_pickle(self.load_path)\n'"
deeppavlov/models/classifiers/keras_classification_model.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom copy import deepcopy\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import List, Tuple, Optional, Generator, Union\n\nimport numpy as np\nimport tensorflow.keras\nfrom overrides import overrides\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import (Conv1D, Dropout, Dense, Input, BatchNormalization, GlobalMaxPooling1D,\n                                     MaxPooling1D, concatenate, Activation, Reshape,\n                                     GlobalAveragePooling1D, LSTM, GRU, Bidirectional)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.regularizers import l2\n\nfrom deeppavlov.core.common.errors import ConfigError\nfrom deeppavlov.core.common.file import save_json, read_json\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.layers.keras_layers import additive_self_attention, multiplicative_self_attention\nfrom deeppavlov.core.models.keras_model import LRScheduledKerasModel\n\nlog = getLogger(__name__)\n\n\n@register(\'keras_classification_model\')\nclass KerasClassificationModel(LRScheduledKerasModel):\n    """"""\n    Class implements Keras model for classification task for multi-class multi-labeled data.\n\n    Args:\n        embedding_size: embedding_size from embedder in pipeline\n        n_classes: number of considered classes\n        model_name: particular method of this class to initialize model configuration\n        optimizer: function name from keras.optimizers\n        loss: function name from keras.losses.\n        last_layer_activation: parameter that determines activation function after classification layer.\n                For multi-label classification use `sigmoid`,\n                otherwise, `softmax`.\n        restore_lr: in case of loading pre-trained model \\\n                whether to init learning rate with the final learning rate value from saved opt\n        classes: list or generator of considered classes\n        text_size: maximal length of text in tokens (words),\n                longer texts are cut,\n                shorter ones are padded with zeros (pre-padding)\n        padding: ``pre`` or ``post`` padding to use\n\n    Attributes:\n        opt: dictionary with all model parameters\n        n_classes: number of considered classes\n        model: keras model itself\n        epochs_done: number of epochs that were done\n        batches_seen: number of epochs that were seen\n        train_examples_seen: number of training samples that were seen\n        sess: tf session\n        optimizer: keras.optimizers instance\n        classes: list of considered classes\n        padding: ``pre`` or ``post`` padding to use\n    """"""\n\n    def __init__(self, embedding_size: int, n_classes: int,\n                 model_name: str, optimizer: str = ""Adam"", loss: str = ""binary_crossentropy"",\n                 learning_rate: Union[None, float, List[float]] = None,\n                 learning_rate_decay: Optional[Union[float, str]] = 0.,\n                 last_layer_activation: str = ""sigmoid"",\n                 restore_lr: bool = False,\n                 classes: Optional[Union[list, Generator]] = None,\n                 text_size: Optional[int] = None,\n                 padding: Optional[str] = ""pre"",\n                 **kwargs):\n        """"""\n        Initialize model using parameters\n        from opt dictionary (from config), if model is being initialized from saved.\n        """"""\n        if learning_rate is None and isinstance(learning_rate_decay, float):\n            learning_rate = 0.01\n        elif learning_rate is None and learning_rate_decay is None:\n            learning_rate = 0.01\n            learning_rate_decay = 0.\n        elif isinstance(learning_rate, float) and ""learning_rate_drop_patience"" in kwargs:\n            learning_rate_decay = ""no""\n\n        if classes is not None:\n            classes = list(classes)\n\n        given_opt = {""embedding_size"": embedding_size,\n                     ""n_classes"": n_classes,\n                     ""model_name"": model_name,\n                     ""optimizer"": optimizer,\n                     ""loss"": loss,\n                     ""learning_rate"": learning_rate,\n                     ""learning_rate_decay"": learning_rate_decay,\n                     ""last_layer_activation"": last_layer_activation,\n                     ""restore_lr"": restore_lr,\n                     ""classes"": classes,\n                     ""text_size"": text_size,\n                     ""padding"": padding,\n                     **kwargs}\n        self.opt = deepcopy(given_opt)\n        self.model = None\n        self.optimizer = None\n\n        super().__init__(**given_opt)\n\n        if classes is not None:\n            self.classes = self.opt.get(""classes"")\n\n        self.n_classes = self.opt.get(\'n_classes\')\n        if self.n_classes == 0:\n            raise ConfigError(""Please, provide vocabulary with considered classes."")\n\n        self.load()\n\n        summary = [\'Model was successfully initialized!\', \'Model summary:\']\n        self.model.summary(print_fn=summary.append)\n        log.info(\'\\n\'.join(summary))\n\n    @overrides\n    def get_optimizer(self):\n        return self.model.optimizer\n\n    def pad_texts(self, sentences: List[List[np.ndarray]]) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n        """"""\n        Cut and pad tokenized texts to self.opt[""text_size""] tokens\n\n        Args:\n            sentences: list of lists of tokens\n\n        Returns:\n            array of embedded texts\n        """"""\n        pad = np.zeros(self.opt[\'embedding_size\'])\n        cut_batch = [sen[:self.opt[\'text_size\']] for sen in sentences]\n        if self.opt[""padding""] == ""pre"":\n            cut_batch = [[pad] * (self.opt[\'text_size\'] - len(tokens)) + list(tokens) for tokens in cut_batch]\n        elif self.opt[""padding""] == ""post"":\n            cut_batch = [list(tokens) + [pad] * (self.opt[\'text_size\'] - len(tokens)) for tokens in cut_batch]\n        else:\n            raise ConfigError(""Padding type {} is not acceptable"".format(self.opt[\'padding\']))\n        return np.asarray(cut_batch)\n\n    def check_input(self, texts: List[List[np.ndarray]]) -> np.ndarray:\n        """"""\n        Check and convert input to array of tokenized embedded samples\n\n        Args:\n            texts: list of tokenized embedded text samples\n\n        Returns:\n            array of tokenized embedded texts samples that are cut and padded\n        """"""\n        if self.opt[""text_size""] is not None:\n            features = self.pad_texts(texts)\n        else:\n            if len(texts[0]):\n                features = np.array(texts)\n            else:\n                features = np.zeros((1, 1, self.opt[""embedding_size""]))\n\n        return features\n\n    def train_on_batch(self, texts: List[List[np.ndarray]], labels: list) -> Union[float, List[float]]:\n        """"""\n        Train the model on the given batch\n\n        Args:\n            texts: list of tokenized embedded text samples\n            labels: list of labels\n\n        Returns:\n            metrics values on the given batch\n        """"""\n        features = self.check_input(texts)\n\n        metrics_values = self.model.train_on_batch(features, np.array(labels))\n        return metrics_values\n\n    def __call__(self, data: List[List[np.ndarray]]) -> List[List[float]]:\n        """"""\n        Infer on the given data\n\n        Args:\n            data: list of tokenized text samples\n\n        Returns:\n            for each sentence:\n                vector of probabilities to belong with each class\n                or list of labels sentence belongs with\n        """"""\n        features = self.check_input(data)\n        return self.model.predict(features)\n\n    def init_model_from_scratch(self, model_name: str) -> Model:\n        """"""\n        Initialize uncompiled model from scratch with given params\n\n        Args:\n            model_name: name of model function described as a method of this class\n\n        Returns:\n            compiled model with given network and learning parameters\n        """"""\n        log.info(f\'[initializing `{self.__class__.__name__}` from scratch as {model_name}]\')\n        model_func = getattr(self, model_name, None)\n        if callable(model_func):\n            model = model_func(**self.opt)\n        else:\n            raise AttributeError(""Model {} is not defined"".format(model_name))\n\n        return model\n\n    def _load(self, model_name: str) -> None:\n        """"""\n        Initialize uncompiled model from saved params and weights\n\n        Args:\n            model_name: name of model function described as a method of this class\n\n        Returns:\n            model with loaded weights and network parameters from files\n            but compiled with given learning parameters\n        """"""\n        if self.load_path:\n            if isinstance(self.load_path, Path) and not self.load_path.parent.is_dir():\n                raise ConfigError(""Provided load path is incorrect!"")\n\n            opt_path = Path(""{}_opt.json"".format(str(self.load_path.resolve())))\n            weights_path = Path(""{}.h5"".format(str(self.load_path.resolve())))\n\n            if opt_path.exists() and weights_path.exists():\n\n                log.info(""[initializing `{}` from saved]"".format(self.__class__.__name__))\n\n                self.opt[""final_learning_rate""] = read_json(opt_path).get(""final_learning_rate"")\n\n                model_func = getattr(self, model_name, None)\n                if callable(model_func):\n                    model = model_func(**self.opt)\n                else:\n                    raise AttributeError(""Model {} is not defined"".format(model_name))\n\n                log.info(""[loading weights from {}]"".format(weights_path.name))\n                try:\n                    model.load_weights(str(weights_path))\n                except ValueError:\n                    raise ConfigError(""Some non-changeable parameters of neural network differ""\n                                      "" from given pre-trained model"")\n\n                self.model = model\n\n                return None\n            else:\n                self.model = self.init_model_from_scratch(model_name)\n                return None\n        else:\n            log.warning(""No `load_path` is provided for {}"".format(self.__class__.__name__))\n            self.model = self.init_model_from_scratch(model_name)\n            return None\n\n    def compile(self, model: Model, optimizer_name: str, loss_name: str,\n                learning_rate: Optional[Union[float, List[float]]],\n                learning_rate_decay: Optional[Union[float, str]]) -> Model:\n        """"""\n        Compile model with given optimizer and loss\n\n        Args:\n            model: keras uncompiled model\n            optimizer_name: name of optimizer from keras.optimizers\n            loss_name: loss function name (from keras.losses)\n            learning_rate: learning rate.\n            learning_rate_decay: learning rate decay.\n\n        Returns:\n\n        """"""\n        optimizer_func = getattr(tensorflow.keras.optimizers, optimizer_name, None)\n        if callable(optimizer_func):\n            if isinstance(learning_rate, float) and isinstance(learning_rate_decay, float):\n                # in this case decay will be either given in config or, by default, learning_rate_decay=0.\n                self.optimizer = optimizer_func(lr=learning_rate, decay=learning_rate_decay)\n            else:\n                self.optimizer = optimizer_func()\n        else:\n            raise AttributeError(""Optimizer {} is not defined in `tensorflow.keras.optimizers`"".format(optimizer_name))\n\n        loss_func = getattr(tensorflow.keras.losses, loss_name, None)\n        if callable(loss_func):\n            loss = loss_func\n        else:\n            raise AttributeError(""Loss {} is not defined"".format(loss_name))\n\n        model.compile(optimizer=self.optimizer,\n                      loss=loss)\n        return model\n\n    @overrides\n    def load(self, model_name: Optional[str] = None) -> None:\n\n        model_name = model_name or self.opt.get(\'model_name\')\n        self._load(model_name=model_name)\n        # in case of pre-trained after loading in self.opt we have stored parameters\n        # now we can restore lear rate if needed\n        if self.opt.get(""restore_lr"", False) and (""final_learning_rate"" in self.opt):\n            self.opt[""learning_rate""] = self.opt[""final_learning_rate""]\n\n        self.model = self.compile(self.model,\n                                  optimizer_name=self.opt[""optimizer""],\n                                  loss_name=self.opt[""loss""],\n                                  learning_rate=self.opt[""learning_rate""],\n                                  learning_rate_decay=self.opt[""learning_rate_decay""])\n\n    @overrides\n    def save(self, fname: str = None) -> None:\n        """"""\n        Save the model parameters into <<fname>>_opt.json (or <<ser_file>>_opt.json)\n        and model weights into <<fname>>.h5 (or <<ser_file>>.h5)\n        Args:\n            fname: file_path to save model. If not explicitly given seld.opt[""ser_file""] will be used\n\n        Returns:\n            None\n        """"""\n        if not fname:\n            fname = self.save_path\n        else:\n            fname = Path(fname).resolve()\n\n        if not fname.parent.is_dir():\n            raise ConfigError(""Provided save path is incorrect!"")\n        else:\n            opt_path = f""{fname}_opt.json""\n            weights_path = f""{fname}.h5""\n            log.info(f""[saving model to {opt_path}]"")\n            self.model.save_weights(weights_path)\n\n        # if model was loaded from one path and saved to another one\n        # then change load_path to save_path for config\n        self.opt[""epochs_done""] = self.epochs_done\n        if isinstance(self.opt.get(""learning_rate"", None), float):\n            self.opt[""final_learning_rate""] = (K.eval(self.optimizer.lr) /\n                                               (1. + K.eval(self.optimizer.decay) * self.batches_seen))\n\n        if self.opt.get(""load_path"") and self.opt.get(""save_path""):\n            if self.opt.get(""save_path"") != self.opt.get(""load_path""):\n                self.opt[""load_path""] = str(self.opt[""save_path""])\n        save_json(self.opt, opt_path)\n\n    # noinspection PyUnusedLocal\n    def cnn_model(self, kernel_sizes_cnn: List[int], filters_cnn: int, dense_size: int,\n                  coef_reg_cnn: float = 0., coef_reg_den: float = 0., dropout_rate: float = 0.,\n                  input_projection_size: Optional[int] = None, **kwargs) -> Model:\n        """"""\n        Build un-compiled model of shallow-and-wide CNN.\n\n        Args:\n            kernel_sizes_cnn: list of kernel sizes of convolutions.\n            filters_cnn: number of filters for convolutions.\n            dense_size: number of units for dense layer.\n            coef_reg_cnn: l2-regularization coefficient for convolutions.\n            coef_reg_den: l2-regularization coefficient for dense layers.\n            dropout_rate: dropout rate used after convolutions and between dense layers.\n            input_projection_size: if not None, adds Dense layer (with ``relu`` activation)\n                                   right after input layer to the size ``input_projection_size``.\n                                   Useful for input dimentionaliry recuction. Default: ``None``.\n            kwargs: other non-used parameters\n\n        Returns:\n            keras.models.Model: uncompiled instance of Keras Model\n        """"""\n        inp = Input(shape=(self.opt[\'text_size\'], self.opt[\'embedding_size\']))\n        output = inp\n\n        if input_projection_size is not None:\n            output = Dense(input_projection_size, activation=\'relu\')(output)\n\n        outputs = []\n        for i in range(len(kernel_sizes_cnn)):\n            output_i = Conv1D(filters_cnn, kernel_size=kernel_sizes_cnn[i],\n                              activation=None,\n                              kernel_regularizer=l2(coef_reg_cnn),\n                              padding=\'same\')(output)\n            output_i = BatchNormalization()(output_i)\n            output_i = Activation(\'relu\')(output_i)\n            output_i = GlobalMaxPooling1D()(output_i)\n            outputs.append(output_i)\n\n        output = concatenate(outputs, axis=1)\n\n        output = Dropout(rate=dropout_rate)(output)\n        output = Dense(dense_size, activation=None,\n                       kernel_regularizer=l2(coef_reg_den))(output)\n        output = BatchNormalization()(output)\n        output = Activation(\'relu\')(output)\n        output = Dropout(rate=dropout_rate)(output)\n        output = Dense(self.n_classes, activation=None,\n                       kernel_regularizer=l2(coef_reg_den))(output)\n        output = BatchNormalization()(output)\n        act_output = Activation(self.opt.get(""last_layer_activation"", ""sigmoid""))(output)\n        model = Model(inputs=inp, outputs=act_output)\n        return model\n\n    # noinspection PyUnusedLocal\n    def dcnn_model(self, kernel_sizes_cnn: List[int], filters_cnn: List[int], dense_size: int,\n                   coef_reg_cnn: float = 0., coef_reg_den: float = 0., dropout_rate: float = 0.,\n                   input_projection_size: Optional[int] = None, **kwargs) -> Model:\n        """"""\n        Build un-compiled model of deep CNN.\n\n        Args:\n            kernel_sizes_cnn: list of kernel sizes of convolutions.\n            filters_cnn: number of filters for convolutions.\n            dense_size: number of units for dense layer.\n            coef_reg_cnn: l2-regularization coefficient for convolutions.\n            coef_reg_den: l2-regularization coefficient for dense layers.\n            dropout_rate: dropout rate used after convolutions and between dense layers.\n            input_projection_size: if not None, adds Dense layer (with ``relu`` activation)\n                                   right after input layer to the size ``input_projection_size``.\n                                   Useful for input dimentionaliry recuction. Default: ``None``.\n            kwargs: other non-used parameters\n\n        Returns:\n            keras.models.Model: uncompiled instance of Keras Model\n        """"""\n        inp = Input(shape=(self.opt[\'text_size\'], self.opt[\'embedding_size\']))\n        output = inp\n\n        if input_projection_size is not None:\n            output = Dense(input_projection_size, activation=\'relu\')(output)\n\n        for i in range(len(kernel_sizes_cnn)):\n            output = Conv1D(filters_cnn[i], kernel_size=kernel_sizes_cnn[i],\n                            activation=None,\n                            kernel_regularizer=l2(coef_reg_cnn),\n                            padding=\'same\')(output)\n            output = BatchNormalization()(output)\n            output = Activation(\'relu\')(output)\n            output = MaxPooling1D()(output)\n\n        output = GlobalMaxPooling1D()(output)\n        output = Dropout(rate=dropout_rate)(output)\n        output = Dense(dense_size, activation=None,\n                       kernel_regularizer=l2(coef_reg_den))(output)\n        output = BatchNormalization()(output)\n        output = Activation(\'relu\')(output)\n        output = Dropout(rate=dropout_rate)(output)\n        output = Dense(self.n_classes, activation=None,\n                       kernel_regularizer=l2(coef_reg_den))(output)\n        output = BatchNormalization()(output)\n        act_output = Activation(self.opt.get(""last_layer_activation"", ""sigmoid""))(output)\n        model = Model(inputs=inp, outputs=act_output)\n        return model\n\n    # noinspection PyUnusedLocal\n    def cnn_model_max_and_aver_pool(self, kernel_sizes_cnn: List[int], filters_cnn: int, dense_size: int,\n                                    coef_reg_cnn: float = 0., coef_reg_den: float = 0., dropout_rate: float = 0.,\n                                    input_projection_size: Optional[int] = None, **kwargs) -> Model:\n        """"""\n        Build un-compiled model of shallow-and-wide CNN where average pooling after convolutions is replaced with\n        concatenation of average and max poolings.\n\n        Args:\n            kernel_sizes_cnn: list of kernel sizes of convolutions.\n            filters_cnn: number of filters for convolutions.\n            dense_size: number of units for dense layer.\n            coef_reg_cnn: l2-regularization coefficient for convolutions. Default: ``0.0``.\n            coef_reg_den: l2-regularization coefficient for dense layers. Default: ``0.0``.\n            dropout_rate: dropout rate used after convolutions and between dense layers. Default: ``0.0``.\n            input_projection_size: if not None, adds Dense layer (with ``relu`` activation)\n                                   right after input layer to the size ``input_projection_size``.\n                                   Useful for input dimentionaliry recuction. Default: ``None``.\n            kwargs: other non-used parameters\n\n        Returns:\n            keras.models.Model: uncompiled instance of Keras Model\n        """"""\n\n        inp = Input(shape=(self.opt[\'text_size\'], self.opt[\'embedding_size\']))\n        output = inp\n\n        if input_projection_size is not None:\n            output = Dense(input_projection_size, activation=\'relu\')(output)\n\n        outputs = []\n        for i in range(len(kernel_sizes_cnn)):\n            output_i = Conv1D(filters_cnn, kernel_size=kernel_sizes_cnn[i],\n                              activation=None,\n                              kernel_regularizer=l2(coef_reg_cnn),\n                              padding=\'same\')(output)\n            output_i = BatchNormalization()(output_i)\n            output_i = Activation(\'relu\')(output_i)\n            output_i_0 = GlobalMaxPooling1D()(output_i)\n            output_i_1 = GlobalAveragePooling1D()(output_i)\n            output_i = concatenate([output_i_0, output_i_1])\n            outputs.append(output_i)\n\n        output = concatenate(outputs, axis=1)\n\n        output = Dropout(rate=dropout_rate)(output)\n        output = Dense(dense_size, activation=None,\n                       kernel_regularizer=l2(coef_reg_den))(output)\n        output = BatchNormalization()(output)\n        output = Activation(\'relu\')(output)\n        output = Dropout(rate=dropout_rate)(output)\n        output = Dense(self.n_classes, activation=None,\n                       kernel_regularizer=l2(coef_reg_den))(output)\n        output = BatchNormalization()(output)\n        act_output = Activation(self.opt.get(""last_layer_activation"", ""sigmoid""))(output)\n        model = Model(inputs=inp, outputs=act_output)\n        return model\n\n    # noinspection PyUnusedLocal\n    def bilstm_model(self, units_lstm: int, dense_size: int,\n                     coef_reg_lstm: float = 0., coef_reg_den: float = 0.,\n                     dropout_rate: float = 0., rec_dropout_rate: float = 0.,\n                     input_projection_size: Optional[int] = None, **kwargs) -> Model:\n        """"""\n        Build un-compiled BiLSTM.\n\n        Args:\n            units_lstm (int): number of units for LSTM.\n            dense_size (int): number of units for dense layer.\n            coef_reg_lstm (float): l2-regularization coefficient for LSTM. Default: ``0.0``.\n            coef_reg_den (float): l2-regularization coefficient for dense layers. Default: ``0.0``.\n            dropout_rate (float): dropout rate to be used after BiLSTM and between dense layers. Default: ``0.0``.\n            rec_dropout_rate (float): dropout rate for LSTM. Default: ``0.0``.\n            input_projection_size: if not None, adds Dense layer (with ``relu`` activation)\n                                   right after input layer to the size ``input_projection_size``.\n                                   Useful for input dimentionaliry recuction. Default: ``None``.\n            kwargs: other non-used parameters\n\n        Returns:\n            keras.models.Model: uncompiled instance of Keras Model\n        """"""\n\n        inp = Input(shape=(self.opt[\'text_size\'], self.opt[\'embedding_size\']))\n        output = inp\n\n        if input_projection_size is not None:\n            output = Dense(input_projection_size, activation=\'relu\')(output)\n\n        output = Bidirectional(LSTM(units_lstm, activation=\'tanh\',\n                                    return_sequences=True,\n                                    kernel_regularizer=l2(coef_reg_lstm),\n                                    dropout=dropout_rate,\n                                    recurrent_dropout=rec_dropout_rate))(output)\n\n        output = GlobalMaxPooling1D()(output)\n        output = Dropout(rate=dropout_rate)(output)\n        output = Dense(dense_size, activation=None,\n                       kernel_regularizer=l2(coef_reg_den))(output)\n        output = Activation(\'relu\')(output)\n        output = Dropout(rate=dropout_rate)(output)\n        output = Dense(self.n_classes, activation=None,\n                       kernel_regularizer=l2(coef_reg_den))(output)\n        act_output = Activation(self.opt.get(""last_layer_activation"", ""sigmoid""))(output)\n        model = Model(inputs=inp, outputs=act_output)\n        return model\n\n    # noinspection PyUnusedLocal\n    def bilstm_bilstm_model(self, units_lstm_1: int, units_lstm_2: int, dense_size: int,\n                            coef_reg_lstm: float = 0., coef_reg_den: float = 0.,\n                            dropout_rate: float = 0., rec_dropout_rate: float = 0.,\n                            input_projection_size: Optional[int] = None, **kwargs) -> Model:\n        """"""\n        Build un-compiled two-layers BiLSTM.\n\n        Args:\n            units_lstm_1: number of units for the first LSTM layer.\n            units_lstm_2: number of units for the second LSTM layer.\n            dense_size: number of units for dense layer.\n            coef_reg_lstm: l2-regularization coefficient for LSTM. Default: ``0.0``.\n            coef_reg_den: l2-regularization coefficient for dense layers. Default: ``0.0``.\n            dropout_rate: dropout rate to be used after BiLSTM and between dense layers. Default: ``0.0``.\n            rec_dropout_rate: dropout rate for LSTM. Default: ``0.0``.\n            input_projection_size: if not None, adds Dense layer (with ``relu`` activation)\n                                   right after input layer to the size ``input_projection_size``.\n                                   Useful for input dimentionaliry recuction. Default: ``None``.\n            kwargs: other non-used parameters\n\n        Returns:\n            keras.models.Model: uncompiled instance of Keras Model\n        """"""\n\n        inp = Input(shape=(self.opt[\'text_size\'], self.opt[\'embedding_size\']))\n        output = inp\n\n        if input_projection_size is not None:\n            output = Dense(input_projection_size, activation=\'relu\')(output)\n\n        output = Bidirectional(LSTM(units_lstm_1, activation=\'tanh\',\n                                    return_sequences=True,\n                                    kernel_regularizer=l2(coef_reg_lstm),\n                                    dropout=dropout_rate,\n                                    recurrent_dropout=rec_dropout_rate))(output)\n\n        output = Dropout(rate=dropout_rate)(output)\n\n        output = Bidirectional(LSTM(units_lstm_2, activation=\'tanh\',\n                                    return_sequences=True,\n                                    kernel_regularizer=l2(coef_reg_lstm),\n                                    dropout=dropout_rate,\n                                    recurrent_dropout=rec_dropout_rate))(output)\n\n        output = GlobalMaxPooling1D()(output)\n        output = Dropout(rate=dropout_rate)(output)\n        output = Dense(dense_size, activation=None,\n                       kernel_regularizer=l2(coef_reg_den))(output)\n        output = Activation(\'relu\')(output)\n        output = Dropout(rate=dropout_rate)(output)\n        output = Dense(self.n_classes, activation=None,\n                       kernel_regularizer=l2(coef_reg_den))(output)\n        act_output = Activation(self.opt.get(""last_layer_activation"", ""sigmoid""))(output)\n        model = Model(inputs=inp, outputs=act_output)\n        return model\n\n    # noinspection PyUnusedLocal\n    def bilstm_cnn_model(self, units_lstm: int, kernel_sizes_cnn: List[int], filters_cnn: int, dense_size: int,\n                         coef_reg_lstm: float = 0., coef_reg_cnn: float = 0., coef_reg_den: float = 0.,\n                         dropout_rate: float = 0., rec_dropout_rate: float = 0.,\n                         input_projection_size: Optional[int] = None, **kwargs) -> Model:\n        """"""\n        Build un-compiled BiLSTM-CNN.\n\n        Args:\n            units_lstm: number of units for LSTM.\n            kernel_sizes_cnn: list of kernel sizes of convolutions.\n            filters_cnn: number of filters for convolutions.\n            dense_size: number of units for dense layer.\n            coef_reg_lstm: l2-regularization coefficient for LSTM. Default: ``0.0``.\n            coef_reg_cnn: l2-regularization coefficient for convolutions. Default: ``0.0``.\n            coef_reg_den: l2-regularization coefficient for dense layers. Default: ``0.0``.\n            dropout_rate: dropout rate to be used after BiLSTM and between dense layers. Default: ``0.0``.\n            rec_dropout_rate: dropout rate for LSTM. Default: ``0.0``.\n            input_projection_size: if not None, adds Dense layer (with ``relu`` activation)\n                                   right after input layer to the size ``input_projection_size``.\n                                   Useful for input dimentionaliry recuction. Default: ``None``.\n            kwargs: other non-used parameters\n\n        Returns:\n            keras.models.Model: uncompiled instance of Keras Model\n        """"""\n\n        inp = Input(shape=(self.opt[\'text_size\'], self.opt[\'embedding_size\']))\n        output = inp\n\n        if input_projection_size is not None:\n            output = Dense(input_projection_size, activation=\'relu\')(output)\n\n        output = Bidirectional(LSTM(units_lstm, activation=\'tanh\',\n                                    return_sequences=True,\n                                    kernel_regularizer=l2(coef_reg_lstm),\n                                    dropout=dropout_rate,\n                                    recurrent_dropout=rec_dropout_rate))(output)\n\n        output = Reshape(target_shape=(self.opt[\'text_size\'], 2 * units_lstm))(output)\n        outputs = []\n        for i in range(len(kernel_sizes_cnn)):\n            output_i = Conv1D(filters_cnn,\n                              kernel_size=kernel_sizes_cnn[i],\n                              activation=None,\n                              kernel_regularizer=l2(coef_reg_cnn),\n                              padding=\'same\')(output)\n            output_i = BatchNormalization()(output_i)\n            output_i = Activation(\'relu\')(output_i)\n            output_i = GlobalMaxPooling1D()(output_i)\n            outputs.append(output_i)\n\n        output = concatenate(outputs, axis=1)\n        output = Dropout(rate=dropout_rate)(output)\n        output = Dense(dense_size, activation=None,\n                       kernel_regularizer=l2(coef_reg_den))(output)\n        output = Activation(\'relu\')(output)\n        output = Dropout(rate=dropout_rate)(output)\n        output = Dense(self.n_classes, activation=None,\n                       kernel_regularizer=l2(coef_reg_den))(output)\n        act_output = Activation(self.opt.get(""last_layer_activation"", ""sigmoid""))(output)\n        model = Model(inputs=inp, outputs=act_output)\n        return model\n\n    # noinspection PyUnusedLocal\n    def cnn_bilstm_model(self, kernel_sizes_cnn: List[int], filters_cnn: int, units_lstm: int, dense_size: int,\n                         coef_reg_cnn: float = 0., coef_reg_lstm: float = 0., coef_reg_den: float = 0.,\n                         dropout_rate: float = 0., rec_dropout_rate: float = 0.,\n                         input_projection_size: Optional[int] = None, **kwargs) -> Model:\n        """"""\n        Build un-compiled BiLSTM-CNN.\n\n        Args:\n            kernel_sizes_cnn: list of kernel sizes of convolutions.\n            filters_cnn: number of filters for convolutions.\n            units_lstm: number of units for LSTM.\n            dense_size: number of units for dense layer.\n            coef_reg_cnn: l2-regularization coefficient for convolutions. Default: ``0.0``.\n            coef_reg_lstm: l2-regularization coefficient for LSTM. Default: ``0.0``.\n            coef_reg_den: l2-regularization coefficient for dense layers. Default: ``0.0``.\n            dropout_rate: dropout rate to be used after BiLSTM and between dense layers. Default: ``0.0``.\n            rec_dropout_rate: dropout rate for LSTM. Default: ``0.0``.\n            input_projection_size: if not None, adds Dense layer (with ``relu`` activation)\n                                   right after input layer to the size ``input_projection_size``.\n                                   Useful for input dimentionaliry recuction. Default: ``None``.\n            kwargs: other non-used parameters\n\n        Returns:\n            keras.models.Model: uncompiled instance of Keras Model\n        """"""\n\n        inp = Input(shape=(self.opt[\'text_size\'], self.opt[\'embedding_size\']))\n        output = inp\n\n        if input_projection_size is not None:\n            output = Dense(input_projection_size, activation=\'relu\')(output)\n\n        outputs = []\n        for i in range(len(kernel_sizes_cnn)):\n            output_i = Conv1D(filters_cnn, kernel_size=kernel_sizes_cnn[i],\n                              activation=None,\n                              kernel_regularizer=l2(coef_reg_cnn),\n                              padding=\'same\')(output)\n            output_i = BatchNormalization()(output_i)\n            output_i = Activation(\'relu\')(output_i)\n            output_i = MaxPooling1D()(output_i)\n            outputs.append(output_i)\n\n        output = concatenate(outputs, axis=-1)\n        output = Dropout(rate=dropout_rate)(output)\n\n        output = Bidirectional(LSTM(units_lstm, activation=\'tanh\',\n                                    return_sequences=True,\n                                    kernel_regularizer=l2(coef_reg_lstm),\n                                    dropout=dropout_rate,\n                                    recurrent_dropout=rec_dropout_rate))(output)\n\n        output = GlobalMaxPooling1D()(output)\n        output = Dropout(rate=dropout_rate)(output)\n        output = Dense(dense_size, activation=None,\n                       kernel_regularizer=l2(coef_reg_den))(output)\n        output = Activation(\'relu\')(output)\n        output = Dropout(rate=dropout_rate)(output)\n        output = Dense(self.n_classes, activation=None,\n                       kernel_regularizer=l2(coef_reg_den))(output)\n        act_output = Activation(self.opt.get(""last_layer_activation"", ""sigmoid""))(output)\n        model = Model(inputs=inp, outputs=act_output)\n        return model\n\n    # noinspection PyUnusedLocal\n    def bilstm_self_add_attention_model(self, units_lstm: int, dense_size: int, self_att_hid: int, self_att_out: int,\n                                        coef_reg_lstm: float = 0., coef_reg_den: float = 0.,\n                                        dropout_rate: float = 0., rec_dropout_rate: float = 0.,\n                                        input_projection_size: Optional[int] = None, **kwargs) -> Model:\n        """"""\n        Method builds uncompiled model of BiLSTM with self additive attention.\n\n        Args:\n            units_lstm: number of units for LSTM.\n            self_att_hid: number of hidden units in self-attention\n            self_att_out: number of output units in self-attention\n            dense_size: number of units for dense layer.\n            coef_reg_lstm: l2-regularization coefficient for LSTM. Default: ``0.0``.\n            coef_reg_den: l2-regularization coefficient for dense layers. Default: ``0.0``.\n            dropout_rate: dropout rate to be used after BiLSTM and between dense layers. Default: ``0.0``.\n            rec_dropout_rate: dropout rate for LSTM. Default: ``0.0``.\n            input_projection_size: if not None, adds Dense layer (with ``relu`` activation)\n                                   right after input layer to the size ``input_projection_size``.\n                                   Useful for input dimentionaliry recuction. Default: ``None``.\n            kwargs: other non-used parameters\n\n        Returns:\n            keras.models.Model: uncompiled instance of Keras Model\n        """"""\n\n        inp = Input(shape=(self.opt[\'text_size\'], self.opt[\'embedding_size\']))\n        output = inp\n\n        if input_projection_size is not None:\n            output = Dense(input_projection_size, activation=\'relu\')(output)\n\n        output = Bidirectional(LSTM(units_lstm, activation=\'tanh\',\n                                    return_sequences=True,\n                                    kernel_regularizer=l2(coef_reg_lstm),\n                                    dropout=dropout_rate,\n                                    recurrent_dropout=rec_dropout_rate))(output)\n\n        output = MaxPooling1D(pool_size=2, strides=3)(output)\n\n        output = additive_self_attention(output, n_hidden=self_att_hid,\n                                         n_output_features=self_att_out)\n        output = GlobalMaxPooling1D()(output)\n        output = Dropout(rate=dropout_rate)(output)\n        output = Dense(dense_size, activation=None,\n                       kernel_regularizer=l2(coef_reg_den))(output)\n        output = Activation(\'relu\')(output)\n        output = Dropout(rate=dropout_rate)(output)\n        output = Dense(self.n_classes, activation=None,\n                       kernel_regularizer=l2(coef_reg_den))(output)\n        act_output = Activation(self.opt.get(""last_layer_activation"", ""sigmoid""))(output)\n        model = Model(inputs=inp, outputs=act_output)\n        return model\n\n    # noinspection PyUnusedLocal\n    def bilstm_self_mult_attention_model(self, units_lstm: int, dense_size: int, self_att_hid: int, self_att_out: int,\n                                         coef_reg_lstm: float = 0., coef_reg_den: float = 0.,\n                                         dropout_rate: float = 0., rec_dropout_rate: float = 0.,\n                                         input_projection_size: Optional[int] = None, **kwargs) -> Model:\n        """"""\n        Method builds uncompiled model of BiLSTM with self multiplicative attention.\n\n        Args:\n            units_lstm: number of units for LSTM.\n            self_att_hid: number of hidden units in self-attention\n            self_att_out: number of output units in self-attention\n            dense_size: number of units for dense layer.\n            coef_reg_lstm: l2-regularization coefficient for LSTM. Default: ``0.0``.\n            coef_reg_den: l2-regularization coefficient for dense layers. Default: ``0.0``.\n            dropout_rate: dropout rate to be used after BiLSTM and between dense layers. Default: ``0.0``.\n            rec_dropout_rate: dropout rate for LSTM. Default: ``0.0``.\n            input_projection_size: if not None, adds Dense layer (with ``relu`` activation)\n                                   right after input layer to the size ``input_projection_size``.\n                                   Useful for input dimentionaliry recuction. Default: ``None``.\n            kwargs: other non-used parameters\n\n        Returns:\n            keras.models.Model: uncompiled instance of Keras Model\n        """"""\n\n        inp = Input(shape=(self.opt[\'text_size\'], self.opt[\'embedding_size\']))\n        output = inp\n\n        if input_projection_size is not None:\n            output = Dense(input_projection_size, activation=\'relu\')(output)\n\n        output = Bidirectional(LSTM(units_lstm, activation=\'tanh\',\n                                    return_sequences=True,\n                                    kernel_regularizer=l2(coef_reg_lstm),\n                                    dropout=dropout_rate,\n                                    recurrent_dropout=rec_dropout_rate))(output)\n\n        output = MaxPooling1D(pool_size=2, strides=3)(output)\n\n        output = multiplicative_self_attention(output, n_hidden=self_att_hid,\n                                               n_output_features=self_att_out)\n        output = GlobalMaxPooling1D()(output)\n        output = Dropout(rate=dropout_rate)(output)\n        output = Dense(dense_size, activation=None,\n                       kernel_regularizer=l2(coef_reg_den))(output)\n        output = Activation(\'relu\')(output)\n        output = Dropout(rate=dropout_rate)(output)\n        output = Dense(self.n_classes, activation=None,\n                       kernel_regularizer=l2(coef_reg_den))(output)\n        act_output = Activation(self.opt.get(""last_layer_activation"", ""sigmoid""))(output)\n        model = Model(inputs=inp, outputs=act_output)\n        return model\n\n    # noinspection PyUnusedLocal\n    def bigru_model(self, units_gru: int, dense_size: int,\n                    coef_reg_lstm: float = 0., coef_reg_den: float = 0.,\n                    dropout_rate: float = 0., rec_dropout_rate: float = 0.,\n                    input_projection_size: Optional[int] = None, **kwargs) -> Model:\n        """"""\n        Method builds uncompiled model BiGRU.\n\n        Args:\n            units_gru: number of units for GRU.\n            dense_size: number of units for dense layer.\n            coef_reg_lstm: l2-regularization coefficient for GRU. Default: ``0.0``.\n            coef_reg_den: l2-regularization coefficient for dense layers. Default: ``0.0``.\n            dropout_rate: dropout rate to be used after BiGRU and between dense layers. Default: ``0.0``.\n            rec_dropout_rate: dropout rate for GRU. Default: ``0.0``.\n            input_projection_size: if not None, adds Dense layer (with ``relu`` activation)\n                                   right after input layer to the size ``input_projection_size``.\n                                   Useful for input dimentionaliry recuction. Default: ``None``.\n            kwargs: other non-used parameters\n\n        Returns:\n            keras.models.Model: uncompiled instance of Keras Model\n        """"""\n\n        inp = Input(shape=(self.opt[\'text_size\'], self.opt[\'embedding_size\']))\n        output = inp\n\n        if input_projection_size is not None:\n            output = Dense(input_projection_size, activation=\'relu\')(output)\n\n        output = Bidirectional(GRU(units_gru, activation=\'tanh\',\n                                   return_sequences=True,\n                                   kernel_regularizer=l2(coef_reg_lstm),\n                                   dropout=dropout_rate,\n                                   recurrent_dropout=rec_dropout_rate))(output)\n\n        output = GlobalMaxPooling1D()(output)\n        output = Dropout(rate=dropout_rate)(output)\n        output = Dense(dense_size, activation=None,\n                       kernel_regularizer=l2(coef_reg_den))(output)\n        output = Activation(\'relu\')(output)\n        output = Dropout(rate=dropout_rate)(output)\n        output = Dense(self.n_classes, activation=None,\n                       kernel_regularizer=l2(coef_reg_den))(output)\n        act_output = Activation(self.opt.get(""last_layer_activation"", ""sigmoid""))(output)\n        model = Model(inputs=inp, outputs=act_output)\n        return model\n\n    # noinspection PyUnusedLocal\n    def bigru_with_max_aver_pool_model(self, units_gru: int, dense_size: int,\n                                       coef_reg_gru: float = 0., coef_reg_den: float = 0.,\n                                       dropout_rate: float = 0., rec_dropout_rate: float = 0.,\n                                       **kwargs) -> Model:\n        """"""\n        Method builds uncompiled model Bidirectional GRU with concatenation of max and average pooling after BiGRU.\n\n        Args:\n            units_gru: number of units for GRU.\n            dense_size: number of units for dense layer.\n            coef_reg_gru: l2-regularization coefficient for GRU. Default: ``0.0``.\n            coef_reg_den: l2-regularization coefficient for dense layers. Default: ``0.0``.\n            dropout_rate: dropout rate to be used after BiGRU and between dense layers. Default: ``0.0``.\n            rec_dropout_rate: dropout rate for GRU. Default: ``0.0``.\n            kwargs: other non-used parameters\n\n        Returns:\n            keras.models.Model: uncompiled instance of Keras Model\n        """"""\n\n        inp = Input(shape=(self.opt[\'text_size\'], self.opt[\'embedding_size\']))\n\n        output = Dropout(rate=dropout_rate)(inp)\n\n        output, state1, state2 = Bidirectional(GRU(units_gru, activation=\'tanh\',\n                                                   return_sequences=True,\n                                                   return_state=True,\n                                                   kernel_regularizer=l2(coef_reg_gru),\n                                                   dropout=dropout_rate,\n                                                   recurrent_dropout=rec_dropout_rate))(output)\n\n        output1 = GlobalMaxPooling1D()(output)\n        output2 = GlobalAveragePooling1D()(output)\n\n        output = concatenate([output1, output2, state1, state2])\n\n        output = Dropout(rate=dropout_rate)(output)\n        output = Dense(dense_size, activation=None,\n                       kernel_regularizer=l2(coef_reg_den))(output)\n        output = Activation(\'relu\')(output)\n        output = Dropout(rate=dropout_rate)(output)\n        output = Dense(self.n_classes, activation=None,\n                       kernel_regularizer=l2(coef_reg_den))(output)\n        act_output = Activation(self.opt.get(""last_layer_activation"", ""sigmoid""))(output)\n        model = Model(inputs=inp, outputs=act_output)\n        return model\n'"
deeppavlov/models/classifiers/proba2labels.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import List, Union\n\nimport numpy as np\n\nfrom deeppavlov.core.common.errors import ConfigError\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\n\nlog = getLogger(__name__)\n\n\n@register(\'proba2labels\')\nclass Proba2Labels(Component):\n    """"""\n    Class implements probability to labels processing using the following ways: \\\n     choosing one or top_n indices with maximal probability or choosing any number of indices \\\n      which probabilities to belong with are higher than given confident threshold\n\n    Args:\n        max_proba: whether to choose label with maximal probability\n        confident_threshold: boundary probability value for sample to belong with the class (best use for multi-label)\n        top_n: how many top labels with the highest probabilities to return\n\n    Attributes:\n        max_proba: whether to choose label with maximal probability\n        confident_threshold: boundary probability value for sample to belong with the class (best use for multi-label)\n        top_n: how many top labels with the highest probabilities to return\n    """"""\n\n    def __init__(self,\n                 max_proba: bool = None,\n                 confident_threshold: float = None,\n                 top_n: int = None,\n                 **kwargs) -> None:\n        """""" Initialize class with given parameters""""""\n\n        self.max_proba = max_proba\n        self.confident_threshold = confident_threshold\n        self.top_n = top_n\n\n    def __call__(self, data: Union[np.ndarray, List[List[float]], List[List[int]]],\n                 *args, **kwargs) -> Union[List[List[int]], List[int]]:\n        """"""\n        Process probabilities to labels\n\n        Args:\n            data: list of vectors with probability distribution\n\n        Returns:\n            list of labels (only label classification) or list of lists of labels (multi-label classification)\n        """"""\n        if self.confident_threshold:\n            return [list(np.where(np.array(d) > self.confident_threshold)[0])\n                    for d in data]\n        elif self.max_proba:\n            return [np.argmax(d) for d in data]\n        elif self.top_n:\n            return [np.argsort(d)[::-1][:self.top_n] for d in data]\n        else:\n            raise ConfigError(""Proba2Labels requires one of three arguments: bool `max_proba` or ""\n                              ""float `confident_threshold` for multi-label classification or""\n                              ""integer `top_n` for choosing several labels with the highest probabilities"")\n'"
deeppavlov/models/classifiers/ru_obscenity_classifier.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport re\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import List, Union\n\nimport pymorphy2\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.estimator import Component\n\nlog = getLogger(__name__)\n\n\n@register(""ru_obscenity_classifier"")\nclass RuObscenityClassifier(Component):\n    """"""Rule-Based model that decides whether the sentence is obscene or not,\n    for Russian language\n\n    Args:\n        data_path: a directory where the required files are stored.\n                   next files are required:\n                   -\'obscenity_words.json\' \xe2\x80\x94 file that stores list of obscenity words\n                   -\'obscenity_words_exception.json\' \xe2\x80\x94 file that stores list of not obscenity words,\n                     but which are detects by algorithm as obscenity(for fixing this situation)\n                   -\'obscenity_words_extended.json\' \xe2\x80\x94 file that stores list of obscenity words,\n                     in which user can add additional obscenity words\n\n    Attributes:\n        obscenity_words: list of russian obscenity words\n        obscenity_words_extended: list of russian obscenity words\n        obscenity_words_exception: list of words on that model makes mistake that they are obscene\n        regexp: reg exp that finds various obscene words\n        regexp2: reg exp that finds various obscene words\n        morph: pymorphy2.MorphAnalyzer object\n        word_pattern: reg exp that finds words in text\n    """"""\n\n    def _get_patterns(self):\n        PATTERN_1 = r\'\'.join((\n            r\'\\w{0,5}[\xd1\x85x]([\xd1\x85x\\s\\!@#\\$%\\^&*+-\\|\\/]{0,6})\',\n            r\'[\xd1\x83y]([\xd1\x83y\\s\\!@#\\$%\\^&*+-\\|\\/]{0,6})[\xd1\x91i\xd0\xbbe\xd0\xb5\xd1\x8e\xd0\xb8\xd0\xb9\xd1\x8f]\\w{0,7}|\\w{0,6}[\xd0\xbfp]\',\n            r\'([\xd0\xbfp\\s\\!@#\\$%\\^&*+-\\|\\/]{0,6})[i\xd0\xb8\xd0\xb5]([i\xd0\xb8\xd0\xb5\\s\\!@#\\$%\\^&*+-\\|\\/]{0,6})\',\n            r\'[3\xd0\xb7\xd1\x81]([3\xd0\xb7\xd1\x81\\s\\!@#\\$%\\^&*+-\\|\\/]{0,6})[\xd0\xb4d]\\w{0,10}|[\xd1\x81cs][\xd1\x83y]\',\n            r\'([\xd1\x83y\\!@#\\$%\\^&*+-\\|\\/]{0,6})[4\xd1\x87k\xd0\xba]\\w{1,3}|\\w{0,4}[b\xd0\xb1]\',\n            r\'([b\xd0\xb1\\s\\!@#\\$%\\^&*+-\\|\\/]{0,6})[l\xd0\xbb]([l\xd0\xbb\\s\\!@#\\$%\\^&*+-\\|\\/]{0,6})\',\n            r\'[y\xd1\x8f]\\w{0,10}|\\w{0,8}[\xd0\xb5\xd1\x91][b\xd0\xb1][\xd0\xbb\xd1\x81\xd0\xba\xd0\xb5@e\xd1\x8b\xd0\xb8\xd0\xb0a][\xd0\xbd\xd0\xb0\xd0\xb8@\xd0\xb9\xd0\xb2\xd0\xbb]\\w{0,8}|\\w{0,4}[\xd0\xb5e]\',\n            r\'([\xd0\xb5e\\s\\!@#\\$%\\^&*+-\\|\\/]{0,6})[\xd0\xb1b]([\xd0\xb1b\\s\\!@#\\$%\\^&*+-\\|\\/]{0,6})\',\n            r\'[u\xd1\x83]([u\xd1\x83\\s\\!@#\\$%\\^&*+-\\|\\/]{0,6})[\xd0\xbd4\xd1\x87]\\w{0,4}|\\w{0,4}[\xd0\xb5e\xd1\x91]\',\n            r\'([\xd0\xb5e\xd1\x91\\s\\!@#\\$%\\^&*+-\\|\\/]{0,6})[\xd0\xb1b]([\xd0\xb1b\\s\\!@#\\$%\\^&*+-\\|\\/]{0,6})\',\n            r\'[\xd0\xbdn]([\xd0\xbdn\\s\\!@#\\$%\\^&*+-\\|\\/]{0,6})[\xd1\x83y]\\w{0,4}|\\w{0,4}[\xd0\xb5e]\',\n            r\'([\xd0\xb5e\\s\\!@#\\$%\\^&*+-\\|\\/]{0,6})[\xd0\xb1b]([\xd0\xb1b\\s\\!@#\\$%\\^&*+-\\|\\/]{0,6})\',\n            r\'[\xd0\xbeo\xd0\xb0a@]([\xd0\xbeo\xd0\xb0a@\\s\\!@#\\$%\\^&*+-\\|\\/]{0,6})[\xd1\x82n\xd0\xbdt]\\w{0,4}|\\w{0,10}[\xd1\x91]\',\n            r\'([\xd1\x91\\!@#\\$%\\^&*+-\\|\\/]{0,6})[\xd0\xb1]\\w{0,6}|\\w{0,4}[p\xd0\xbf]\',\n            r\'([p\xd0\xbf\\s\\!@#\\$%\\^&*+-\\|\\/]{0,6})[\xd0\xb8e\xd0\xb5i]([\xd0\xb8e\xd0\xb5i\\s\\!@#\\$%\\^&*+-\\|\\/]{0,6})\',\n            r\'[\xd0\xb4d]([\xd0\xb4d\\s\\!@#\\$%\\^&*+-\\|\\/]{0,6})[o\xd0\xbe\xd0\xb0a@\xd0\xb5e\xd0\xb8i]\',\n            r\'([o\xd0\xbe\xd0\xb0a@\xd0\xb5e\xd0\xb8i\\s\\!@#\\$%\\^&*+-\\|\\/]{0,6})[\xd1\x80r]\\w{0,12}\',\n        ))\n\n        PATTERN_2 = r\'|\'.join((\n            r""(\\b[\xd1\x81s]{1}[\xd1\x81s\xd1\x86]{0,1}[u\xd1\x83y](?:[\xd1\x874]{0,1}[\xd0\xb8\xd0\xb0ak\xd0\xba][^\xd1\x86])\\w*\\b)"",\n            r""(\\b(?!\xd0\xbf\xd0\xbb\xd0\xbe|\xd1\x81\xd1\x82\xd1\x80\xd0\xb0|[\xd1\x82\xd0\xbb]\xd0\xb8)(\\w(?!(\xd1\x83|\xd0\xbf\xd0\xbb\xd0\xbe)))*[\xd1\x85x][\xd1\x83y](\xd0\xb9|\xd0\xb9\xd0\xb0|[\xd0\xb5e\xd1\x91]|\xd0\xb8|\xd1\x8f|\xd0\xbb\xd0\xb8|\xd1\x8e)(?!\xd0\xb3\xd0\xb0)\\w*\\b)"",\n            r""(\\b(\xd0\xbf[o\xd0\xbe]|[\xd0\xbd\xd0\xb7][\xd0\xb0a])*[\xd1\x85x][e\xd0\xb5][\xd1\x80p]\\w*\\b)"",\n            r""(\\b[\xd0\xbcm][\xd1\x83y][\xd0\xb4d]([\xd0\xb0a][\xd0\xbak]|[o\xd0\xbe]|\xd0\xb8)\\w*\\b)"",\n            r""(\\b\\w*\xd0\xb4[\xd1\x80p](?:[o\xd0\xbe][\xd1\x874]|[\xd0\xb0a][\xd1\x874])(?!\xd0\xbb)\\w*\\b)"",\n            r""(\\b(?!(?:\xd0\xba\xd0\xb8\xd0\xbb\xd0\xbe)?[\xd1\x82\xd0\xbc]\xd0\xb5\xd1\x82)(?!\xd1\x81\xd0\xbc\xd0\xbe)[\xd0\xb0-\xd1\x8fa-z]*(?<!\xd1\x81)\xd1\x82[\xd1\x80p][\xd0\xb0a][\xd1\x85x]\\w*\\b)"",\n            r""(\\b[\xd0\xbak][\xd0\xb0ao\xd0\xbe][\xd0\xb73z]+[e\xd0\xb5]?\xd1\x91?\xd0\xbb\\w*\\b)"",\n            r""(\\b(?!\xd1\x81\xd0\xbe)\\w*\xd0\xbf[\xd0\xb5e\xd1\x91]\xd1\x80[\xd0\xbd\xd0\xb4](\xd0\xb8|\xd0\xb8c|\xd1\x8b|\xd1\x83|\xd0\xbd|\xd0\xb5|\xd1\x8b)\\w*\\b)"",\n            r""(\\b\\w*[\xd0\xb1\xd0\xbf][\xd1\x81\xd1\x81\xd0\xb7]\xd0\xb4\\w+\\b)"",\n            r""(\\b([\xd0\xbdn\xd0\xbf][\xd0\xb0a]?[\xd0\xbeo]?[x\xd1\x85])\\b)"",\n            r""(\\b([\xd0\xb0a]?[\xd0\xbeo]?[\xd0\xbdn\xd0\xbf\xd0\xb1\xd0\xb7][\xd0\xb0a]?[\xd0\xbeo]?)?([c\xd1\x81][p\xd1\x80][\xd0\xb0a][^\xd0\xb7\xd0\xb6\xd0\xb1\xd1\x81\xd0\xb2\xd0\xbc])\\w*\\b)"",\n            r""(\\b\\w*([\xd0\xbeo]\xd1\x82|\xd0\xb2\xd1\x8b|[\xd1\x80p]\xd0\xb8|[\xd0\xbeo]|\xd0\xb8|[\xd1\x83y]){0,1}([\xd0\xbfn\xd1\x80p][i\xd0\xb8\xd0\xb5e\xd1\x91]{0,1}[3z\xd0\xb7\xd1\x81cs][\xd0\xb4d])\\w*\\b)"",\n            r""(\\b(\xd0\xb2\xd1\x8b)?\xd1\x83?[\xd0\xb5e\xd1\x91]?\xd0\xb1\xd0\xb8?\xd0\xbb\xd1\x8f[\xd0\xb4\xd1\x82]?[\xd1\x8e\xd0\xbeo]?\\w*\\b)"",\n            r""(\\b(?!\xd0\xb2\xd0\xb5\xd0\xbb\xd0\xbe|\xd1\x81\xd0\xba\xd0\xb8|\xd1\x8d\xd0\xbd)\\w*[\xd0\xbfpp][e\xd0\xb5\xd0\xb8i][\xd0\xb4d][oa\xd0\xbe\xd0\xb0\xd0\xb5e\xd0\xb8\xd1\x80p](?![\xd1\x86\xd1\x8f\xd0\xbd\xd0\xb3\xd1\x8e\xd1\x81\xd0\xbc\xd0\xb9\xd1\x87\xd0\xb2])[\xd1\x80p]?(?![\xd0\xbb\xd1\x82])\\w*\\b)"",\n            r""(\\b(?!\xd0\xb2?[\xd1\x81\xd1\x82]{1,2}\xd0\xb5\xd0\xb1)(?:(?:\xd0\xb2?[\xd1\x81c\xd0\xb73\xd0\xbe][\xd1\x82\xd1\x8fa\xd0\xb0]?[\xd1\x8c\xd1\x8a]?|\xd0\xb2\xd1\x8b|\xd0\xbf[\xd1\x80p][\xd0\xb8\xd0\xbeo]|[\xd1\x83y]|\xd1\x80[a\xd0\xb0][\xd0\xb73z][\xd1\x8c\xd1\x8a]?|\xd0\xba[\xd0\xbeo]\xd0\xbd[\xd0\xbeo])?[\xd0\xb5\xd1\x91]\xd0\xb1[\xd0\xb0-\xd1\x8fa-z]*)|(?:[\xd0\xb0-\xd1\x8fa-z]*[^\xd1\x85\xd0\xbb\xd1\x80\xd0\xb4\xd0\xb2][\xd0\xb5e\xd1\x91]\xd0\xb1)\\b)"",\n            r""(\\b[\xd0\xb73z][\xd0\xb0a\xd0\xbeo]\xd0\xbb[\xd1\x83y]\xd0\xbf[\xd0\xb0ae\xd0\xb5\xd0\xb8\xd0\xbd]\\w*\\b)"",\n        ))\n\n        return PATTERN_1, PATTERN_2\n\n    def __init__(self, data_path: Union[Path, str], *args, **kwargs) -> None:\n        log.info(f""Initializing `{self.__class__.__name__}`"")\n\n        data_path = expand_path(data_path)\n        with open(data_path / \'obscenity_words.json\', encoding=""utf-8"") as f:\n            self.obscenity_words = set(json.load(f))\n        with open(data_path / \'obscenity_words_exception.json\', encoding=""utf-8"") as f:\n            self.obscenity_words_exception = set(json.load(f))\n        if (data_path / \'obscenity_words_extended.json\').exists():\n            with open(data_path / \'obscenity_words_extended.json\', encoding=""utf-8"") as f:\n                self.obscenity_words_extended = set(json.load(f))\n            self.obscenity_words.update(self.obscenity_words_extended)\n\n        PATTERN_1, PATTERN_2 = self._get_patterns()\n        self.regexp = re.compile(PATTERN_1, re.U | re.I)\n        self.regexp2 = re.compile(PATTERN_2, re.U | re.I)\n        self.morph = pymorphy2.MorphAnalyzer()\n        self.word_pattern = re.compile(r\'[\xd0\x90-\xd1\x8f\xd0\x81\xd1\x91]+\')\n\n    def _check_obscenity(self, text: str) -> bool:\n        for word in self.word_pattern.findall(text):\n            if len(word) < 3:\n                continue\n            word = word.lower()\n            word.replace(\'\xd1\x91\', \'\xd0\xb5\')\n            normal_word = self.morph.parse(word)[0].normal_form\n            if normal_word in self.obscenity_words_exception \\\n                    or word in self.obscenity_words_exception:\n                continue\n            if normal_word in self.obscenity_words \\\n                    or word in self.obscenity_words \\\n                    or bool(self.regexp.findall(normal_word)) \\\n                    or bool(self.regexp.findall(word)) \\\n                    or bool(self.regexp2.findall(normal_word)) \\\n                    or bool(self.regexp2.findall(word)):\n                return True\n        return False\n\n    def __call__(self, texts: List[str]) -> List[bool]:\n        """"""It decides whether text is obscene or not\n\n        Args:\n            texts: list of texts, for which it needs to decide they are obscene or not\n\n        Returns:\n            list of bool:  True is for obscene text, False is for not obscene text\n        """"""\n        decisions = list(map(self._check_obscenity, texts))\n        return decisions\n'"
deeppavlov/models/classifiers/utils.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom logging import getLogger\nfrom typing import List\n\nimport numpy as np\n\nlog = getLogger(__name__)\n\n\ndef labels2onehot(labels: [List[str], List[List[str]], np.ndarray], classes: [list, np.ndarray]) -> np.ndarray:\n    """"""\n    Convert labels to one-hot vectors for multi-class multi-label classification\n\n    Args:\n        labels: list of samples where each sample is a class or a list of classes which sample belongs with\n        classes: array of classes\' names\n\n    Returns:\n        2d array with one-hot representation of given samples\n    """"""\n    n_classes = len(classes)\n    y = []\n    for sample in labels:\n        curr = np.zeros(n_classes)\n        if isinstance(sample, list):\n            for intent in sample:\n                if intent not in classes:\n                    log.warning(\'Unknown label {} detected. Assigning no class\'.format(intent))\n                else:\n                    curr[np.where(np.array(classes) == intent)[0]] = 1\n        else:\n            curr[np.where(np.array(classes) == sample)[0]] = 1\n        y.append(curr)\n    y = np.asarray(y)\n    return y\n\n\ndef proba2labels(proba: [list, np.ndarray], confident_threshold: float, classes: [list, np.ndarray]) -> List[List]:\n    """"""\n    Convert vectors of probabilities to labels using confident threshold\n    (if probability to belong with the class is bigger than confident_threshold, sample belongs with the class;\n    if no probabilities bigger than confident threshold, sample belongs with the class with the biggest probability)\n\n    Args:\n        proba: list of samples where each sample is a vector of probabilities to belong with given classes\n        confident_threshold (float): boundary of probability to belong with a class\n        classes: array of classes\' names\n\n    Returns:\n        list of lists of labels for each sample\n    """"""\n    y = []\n    for sample in proba:\n        to_add = np.where(sample > confident_threshold)[0]\n        if len(to_add) > 0:\n            y.append(np.array(classes)[to_add].tolist())\n        else:\n            y.append(np.array([np.array(classes)[np.argmax(sample)]]).tolist())\n\n    return y\n\n\ndef proba2onehot(proba: [list, np.ndarray], confident_threshold: float, classes: [list, np.ndarray]) -> np.ndarray:\n    """"""\n    Convert vectors of probabilities to one-hot representations using confident threshold\n\n    Args:\n        proba: samples where each sample is a vector of probabilities to belong with given classes\n        confident_threshold: boundary of probability to belong with a class\n        classes: array of classes\' names\n\n    Returns:\n        2d array with one-hot representation of given samples\n    """"""\n    return labels2onehot(proba2labels(proba, confident_threshold, classes), classes)\n'"
deeppavlov/models/doc_retrieval/__init__.py,0,b''
deeppavlov/models/doc_retrieval/logit_ranker.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport warnings\nfrom logging import getLogger\nfrom operator import itemgetter\nfrom typing import List, Union, Tuple\n\nfrom deeppavlov.core.common.chainer import Chainer\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.estimator import Component\n\nlogger = getLogger(__name__)\n\n\n@register(""logit_ranker"")\nclass LogitRanker(Component):\n    """"""Select best answer using squad model logits. Make several batches for a single batch, send each batch\n     to the squad model separately and get a single best answer for each batch.\n\n     Args:\n        squad_model: a loaded squad model\n        batch_size: batch size to use with squad model\n        sort_noans: whether to downgrade noans tokens in the most possible answers\n\n     Attributes:\n        squad_model: a loaded squad model\n        batch_size: batch size to use with squad model\n\n    """"""\n\n    def __init__(self, squad_model: Union[Chainer, Component], batch_size: int = 50,\n                 sort_noans: bool = False, **kwargs):\n        self.squad_model = squad_model\n        self.batch_size = batch_size\n        self.sort_noans = sort_noans\n\n    def __call__(self, contexts_batch: List[List[str]], questions_batch: List[List[str]]) -> \\\n            Tuple[List[str], List[float]]:\n        """"""\n        Sort obtained results from squad reader by logits and get the answer with a maximum logit.\n\n        Args:\n            contexts_batch: a batch of contexts which should be treated as a single batch in the outer JSON config\n            questions_batch: a batch of questions which should be treated as a single batch in the outer JSON config\n\n        Returns:\n            a batch of best answers and their scores\n\n        """"""\n        # TODO output result for top_n\n        warnings.warn(f\'{self.__class__.__name__}.__call__() API will be changed in the future release.\'\n                      \' Instead of returning Tuple(List[str], List[float] will return\'\n                      \' Tuple(List[List[str]], List[List[float]]).\', FutureWarning)\n\n        batch_best_answers = []\n        batch_best_answers_scores = []\n        for contexts, questions in zip(contexts_batch, questions_batch):\n            results = []\n            for i in range(0, len(contexts), self.batch_size):\n                c_batch = contexts[i: i + self.batch_size]\n                q_batch = questions[i: i + self.batch_size]\n                batch_predict = zip(*self.squad_model(c_batch, q_batch))\n                results += batch_predict\n            if self.sort_noans:\n                results = sorted(results, key=lambda x: (x[0] != \'\', x[2]), reverse=True)\n            else:\n                results = sorted(results, key=itemgetter(2), reverse=True)\n            batch_best_answers.append(results[0][0])\n            batch_best_answers_scores.append(results[0][2])\n        return batch_best_answers, batch_best_answers_scores\n'"
deeppavlov/models/doc_retrieval/pop_ranker.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom operator import itemgetter\nfrom typing import List, Any, Tuple\n\nimport numpy as np\nfrom sklearn.externals import joblib\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.file import read_json\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.estimator import Component\n\nlogger = getLogger(__name__)\n\n\n@register(\'pop_ranker\')\nclass PopRanker(Component):\n    """"""Rank documents according to their tfidf scores and popularities. It is not a standalone ranker,\n    it should be used for re-ranking the results of TF-IDF Ranker.\n\n    Based on a Logistic Regression trained on 3 features:\n\n    * tfidf score of the article\n    * popularity of the article obtained via Wikimedia REST API as a mean number of views for the period since 2017/11/05 to 2018/11/05\n    * multiplication of the two features above\n\n    Args:\n        pop_dict_path: a path to json file with article title to article popularity map\n        load_path: a path to saved logistic regression classifier\n        top_n: a number of doc ids to return\n        active: whether to return a number specified by :attr:`top_n` (``True``) or all ids\n         (``False``)\n\n    Attributes:\n        pop_dict: a map of article titles to their popularity\n        mean_pop: mean popularity of all popularities in :attr:`pop_dict`, use it when popularity is not found\n        clf: a loaded logistic regression classifier\n        top_n: a number of doc ids to return\n        active: whether to return a number specified by :attr:`top_n` or all ids\n\n    """"""\n\n    def __init__(self, pop_dict_path: str, load_path: str, top_n: int = 3, active: bool = True,\n                 **kwargs) -> None:\n        pop_dict_path = expand_path(pop_dict_path)\n        logger.info(f""Reading popularity dictionary from {pop_dict_path}"")\n        self.pop_dict = read_json(pop_dict_path)\n        self.mean_pop = np.mean(list(self.pop_dict.values()))\n        load_path = expand_path(load_path)\n        logger.info(f""Loading popularity ranker from {load_path}"")\n        self.clf = joblib.load(load_path)\n        self.top_n = top_n\n        self.active = active\n\n    def __call__(self, input_doc_ids: List[List[Any]], input_doc_scores: List[List[float]]) -> \\\n            Tuple[List[List], List[List]]:\n        """"""Get tfidf scores and tfidf ids, re-rank them by applying logistic regression classifier,\n        output pop ranker ids and pop ranker scores.\n\n         Args:\n            input_doc_ids: top input doc ids of tfidf ranker\n            input_doc_scores: top input doc scores of tfidf ranker corresponding to doc ids\n\n        Returns:\n            top doc ids of pop ranker and their corresponding scores\n\n        """"""\n        batch_ids = []\n        batch_scores = []\n        for instance_ids, instance_scores in zip(input_doc_ids, input_doc_scores):\n            instance_probas = []\n            for idx, score in zip(instance_ids, instance_scores):\n                pop = self.pop_dict.get(idx, self.mean_pop)\n                features = [score, pop, score * pop]\n                prob = self.clf.predict_proba([features])\n                instance_probas.append(prob[0][1])\n\n            sort = sorted(enumerate(instance_probas), key=itemgetter(1), reverse=True)\n            sorted_probas = [item[1] for item in sort]\n            sorted_ids = [instance_ids[item[0]] for item in sort]\n\n            if self.active:\n                sorted_ids = sorted_ids[:self.top_n]\n                sorted_probas = sorted_probas[:self.top_n]\n\n            batch_ids.append(sorted_ids)\n            batch_scores.append(sorted_probas)\n\n        return batch_ids, batch_scores\n'"
deeppavlov/models/doc_retrieval/tfidf_ranker.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import List, Any, Tuple\n\nimport numpy as np\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.estimator import Component\nfrom deeppavlov.models.vectorizers.hashing_tfidf_vectorizer import HashingTfIdfVectorizer\n\nlogger = getLogger(__name__)\n\n\n@register(""tfidf_ranker"")\nclass TfidfRanker(Component):\n    """"""Rank documents according to input strings.\n\n    Args:\n        vectorizer: a vectorizer class\n        top_n: a number of doc ids to return\n        active: whether to return a number specified by :attr:`top_n` (``True``) or all ids\n         (``False``)\n\n    Attributes:\n        top_n: a number of doc ids to return\n        vectorizer: an instance of vectorizer class\n        active: whether to return a number specified by :attr:`top_n` or all ids\n        index2doc: inverted :attr:`doc_index`\n        iterator: a dataset iterator used for generating batches while fitting the vectorizer\n\n    """"""\n\n    def __init__(self, vectorizer: HashingTfIdfVectorizer, top_n=5, active: bool = True, **kwargs):\n\n        self.top_n = top_n\n        self.vectorizer = vectorizer\n        self.active = active\n\n    def __call__(self, questions: List[str]) -> Tuple[List[Any], List[float]]:\n        """"""Rank documents and return top n document titles with scores.\n\n        Args:\n            questions: list of queries used in ranking\n\n        Returns:\n            a tuple of selected doc ids and their scores\n        """"""\n\n        batch_doc_ids, batch_docs_scores = [], []\n\n        q_tfidfs = self.vectorizer(questions)\n\n        for q_tfidf in q_tfidfs:\n            scores = q_tfidf * self.vectorizer.tfidf_matrix\n            scores = np.squeeze(\n                scores.toarray() + 0.0001)  # add a small value to eliminate zero scores\n\n            if self.active:\n                thresh = self.top_n\n            else:\n                thresh = len(self.vectorizer.doc_index)\n\n            if thresh >= len(scores):\n                o = np.argpartition(-scores, len(scores) - 1)[0:thresh]\n            else:\n                o = np.argpartition(-scores, thresh)[0:thresh]\n            o_sort = o[np.argsort(-scores[o])]\n\n            doc_scores = scores[o_sort]\n            doc_ids = [self.vectorizer.index2doc[i] for i in o_sort]\n            batch_doc_ids.append(doc_ids)\n            batch_docs_scores.append(doc_scores)\n\n        return batch_doc_ids, batch_docs_scores\n'"
deeppavlov/models/elmo/__init__.py,0,b''
deeppavlov/models/elmo/bilm_model.py,86,"b'# originally based on https://github.com/allenai/bilm-tf/blob/master/bilm/training.py\n\n# Modifications copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport tensorflow as tf\n\nDTYPE = \'float32\'\nDTYPE_INT = \'int64\'\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\nclass LanguageModel(object):\n    """"""\n    A class to build the tensorflow computational graph for NLMs\n\n    All hyperparameters and model configuration is specified in a dictionary\n    of \'options\'.\n\n    is_training is a boolean used to control behavior of dropout layers\n        and softmax.  Set to False for testing.\n\n    The LSTM cell is controlled by the \'lstm\' key in options\n    Here is an example:\n\n     \'lstm\': {\n      \'cell_clip\': 5,\n      \'dim\': 4096,\n      \'n_layers\': 2,\n      \'proj_clip\': 5,\n      \'projection_dim\': 512,\n      \'use_skip_connections\': True},\n\n        \'projection_dim\' is assumed token embedding size and LSTM output size.\n        \'dim\' is the hidden state size.\n        Set \'dim\' == \'projection_dim\' to skip a projection layer.\n    """"""\n\n    def __init__(self, options, is_training):\n        self.options = options\n        self.is_training = is_training\n        self.bidirectional = options.get(\'bidirectional\', False)\n\n        # use word or char inputs?\n        self.char_inputs = \'char_cnn\' in self.options\n\n        # for the loss function\n        self.share_embedding_softmax = options.get(\n            \'share_embedding_softmax\', False)\n        if self.char_inputs and self.share_embedding_softmax:\n            raise ValueError(""Sharing softmax and embedding weights requires ""\n                             ""word input"")\n\n        self.sample_softmax = options.get(\'sample_softmax\', True)\n\n        self._build()\n\n    def _build_word_embeddings(self):\n        n_tokens_vocab = self.options[\'n_tokens_vocab\']\n        batch_size = self.options[\'batch_size\']\n        unroll_steps = self.options[\'unroll_steps\']\n\n        # LSTM options\n        projection_dim = self.options[\'lstm\'][\'projection_dim\']\n\n        # the input token_ids and word embeddings\n        self.token_ids = tf.placeholder(DTYPE_INT,\n                                        shape=(batch_size, unroll_steps),\n                                        name=\'token_ids\')\n        # the word embeddings\n        with tf.device(""/cpu:0""):\n            self.embedding_weights = tf.get_variable(\n                ""embedding"", [n_tokens_vocab, projection_dim],\n                dtype=DTYPE,\n            )\n            self.embedding = tf.nn.embedding_lookup(self.embedding_weights,\n                                                    self.token_ids)\n\n        # if a bidirectional LM then make placeholders for reverse\n        # model and embeddings\n        if self.bidirectional:\n            self.token_ids_reverse = tf.placeholder(DTYPE_INT,\n                                                    shape=(batch_size, unroll_steps),\n                                                    name=\'token_ids_reverse\')\n            with tf.device(""/cpu:0""):\n                self.embedding_reverse = tf.nn.embedding_lookup(\n                    self.embedding_weights, self.token_ids_reverse)\n\n    def _build_word_char_embeddings(self):\n        """"""\n        options contains key \'char_cnn\': {\n\n        \'n_characters\': 262,\n\n        # includes the start / end characters\n        \'max_characters_per_token\': 50,\n\n        \'filters\': [\n            [1, 32],\n            [2, 32],\n            [3, 64],\n            [4, 128],\n            [5, 256],\n            [6, 512],\n            [7, 512]\n        ],\n        \'activation\': \'tanh\',\n\n        # for the character embedding\n        \'embedding\': {\'dim\': 16}\n\n        # for highway layers\n        # if omitted, then no highway layers\n        \'n_highway\': 2,\n        }\n        """"""\n        batch_size = self.options[\'batch_size\']\n        unroll_steps = self.options[\'unroll_steps\']\n        projection_dim = self.options[\'lstm\'][\'projection_dim\']\n\n        cnn_options = self.options[\'char_cnn\']\n        filters = cnn_options[\'filters\']\n        n_filters = sum(f[1] for f in filters)\n        max_chars = cnn_options[\'max_characters_per_token\']\n        char_embed_dim = cnn_options[\'embedding\'][\'dim\']\n        n_chars = cnn_options[\'n_characters\']\n        if n_chars != 261:\n            raise Exception(""Set n_characters=261 for training see a \\\n                            https://github.com/allenai/bilm-tf/blob/master/README.md"")\n        if cnn_options[\'activation\'] == \'tanh\':\n            activation = tf.nn.tanh\n        elif cnn_options[\'activation\'] == \'relu\':\n            activation = tf.nn.relu\n\n        # the input character ids\n        self.tokens_characters = tf.placeholder(DTYPE_INT,\n                                                shape=(batch_size, unroll_steps, max_chars),\n                                                name=\'tokens_characters\')\n        # the character embeddings\n        with tf.device(""/cpu:0""):\n            self.embedding_weights = tf.get_variable(""char_embed"", [n_chars, char_embed_dim],\n                                                     dtype=DTYPE,\n                                                     initializer=tf.random_uniform_initializer(-1.0, 1.0))\n            # shape (batch_size, unroll_steps, max_chars, embed_dim)\n            self.char_embedding = tf.nn.embedding_lookup(self.embedding_weights,\n                                                         self.tokens_characters)\n\n            if self.bidirectional:\n                self.tokens_characters_reverse = tf.placeholder(DTYPE_INT,\n                                                                shape=(batch_size, unroll_steps, max_chars),\n                                                                name=\'tokens_characters_reverse\')\n                self.char_embedding_reverse = tf.nn.embedding_lookup(\n                    self.embedding_weights, self.tokens_characters_reverse)\n\n        # the convolutions\n        def make_convolutions(inp, reuse):\n            with tf.variable_scope(\'CNN\', reuse=reuse):\n                convolutions = []\n                for i, (width, num) in enumerate(filters):\n                    if cnn_options[\'activation\'] == \'relu\':\n                        # He initialization for ReLU activation\n                        # with char embeddings init between -1 and 1\n                        # w_init = tf.random_normal_initializer(\n                        #    mean=0.0,\n                        #    stddev=np.sqrt(2.0 / (width * char_embed_dim))\n                        # )\n\n                        # Kim et al 2015, +/- 0.05\n                        w_init = tf.random_uniform_initializer(\n                            minval=-0.05, maxval=0.05)\n                    elif cnn_options[\'activation\'] == \'tanh\':\n                        # glorot init\n                        w_init = tf.random_normal_initializer(\n                            mean=0.0,\n                            stddev=np.sqrt(1.0 / (width * char_embed_dim))\n                        )\n                    w = tf.get_variable(\n                        ""W_cnn_%s"" % i,\n                        [1, width, char_embed_dim, num],\n                        initializer=w_init,\n                        dtype=DTYPE)\n                    b = tf.get_variable(\n                        ""b_cnn_%s"" % i, [num], dtype=DTYPE,\n                        initializer=tf.constant_initializer(0.0))\n\n                    conv = tf.nn.conv2d(inp, w,\n                                        strides=[1, 1, 1, 1],\n                                        padding=""VALID"") + b\n                    # now max pool\n                    conv = tf.nn.max_pool(conv, [1, 1, max_chars - width + 1, 1],\n                                          [1, 1, 1, 1], \'VALID\')\n\n                    # activation\n                    conv = activation(conv)\n                    conv = tf.squeeze(conv, squeeze_dims=[2])\n\n                    convolutions.append(conv)\n\n            return tf.concat(convolutions, 2)\n\n        # for first model, this is False, for others it\'s True\n        reuse = tf.get_variable_scope().reuse\n        embedding = make_convolutions(self.char_embedding, reuse)\n\n        self.token_embedding_layers = [embedding]\n\n        if self.bidirectional:\n            # re-use the CNN weights from forward pass\n            embedding_reverse = make_convolutions(\n                self.char_embedding_reverse, True)\n\n        # for highway and projection layers:\n        #   reshape from (batch_size, n_tokens, dim) to\n        n_highway = cnn_options.get(\'n_highway\')\n        use_highway = n_highway is not None and n_highway > 0\n        use_proj = n_filters != projection_dim\n\n        if use_highway or use_proj:\n            embedding = tf.reshape(embedding, [-1, n_filters])\n            if self.bidirectional:\n                embedding_reverse = tf.reshape(embedding_reverse,\n                                               [-1, n_filters])\n\n        # set up weights for projection\n        if use_proj:\n            assert n_filters > projection_dim\n            with tf.variable_scope(\'CNN_proj\'):\n                W_proj_cnn = tf.get_variable(\n                    ""W_proj"", [n_filters, projection_dim],\n                    initializer=tf.random_normal_initializer(\n                        mean=0.0, stddev=np.sqrt(1.0 / n_filters)),\n                    dtype=DTYPE)\n                b_proj_cnn = tf.get_variable(\n                    ""b_proj"", [projection_dim],\n                    initializer=tf.constant_initializer(0.0),\n                    dtype=DTYPE)\n\n        # apply highways layers\n        def high(x, ww_carry, bb_carry, ww_tr, bb_tr):\n            carry_gate = tf.nn.sigmoid(tf.matmul(x, ww_carry) + bb_carry)\n            transform_gate = tf.nn.relu(tf.matmul(x, ww_tr) + bb_tr)\n            return carry_gate * transform_gate + (1.0 - carry_gate) * x\n\n        if use_highway:\n            highway_dim = n_filters\n\n            for i in range(n_highway):\n                with tf.variable_scope(\'CNN_high_%s\' % i):\n                    W_carry = tf.get_variable(\n                        \'W_carry\', [highway_dim, highway_dim],\n                        # glorit init\n                        initializer=tf.random_normal_initializer(\n                            mean=0.0, stddev=np.sqrt(1.0 / highway_dim)),\n                        dtype=DTYPE)\n                    b_carry = tf.get_variable(\n                        \'b_carry\', [highway_dim],\n                        initializer=tf.constant_initializer(-2.0),\n                        dtype=DTYPE)\n                    W_transform = tf.get_variable(\n                        \'W_transform\', [highway_dim, highway_dim],\n                        initializer=tf.random_normal_initializer(\n                            mean=0.0, stddev=np.sqrt(1.0 / highway_dim)),\n                        dtype=DTYPE)\n                    b_transform = tf.get_variable(\n                        \'b_transform\', [highway_dim],\n                        initializer=tf.constant_initializer(0.0),\n                        dtype=DTYPE)\n\n                embedding = high(embedding, W_carry, b_carry,\n                                 W_transform, b_transform)\n                if self.bidirectional:\n                    embedding_reverse = high(embedding_reverse,\n                                             W_carry, b_carry,\n                                             W_transform, b_transform)\n                self.token_embedding_layers.append(tf.reshape(embedding,\n                                                              [batch_size, unroll_steps, highway_dim]))\n\n        # finally project down to projection dim if needed\n        if use_proj:\n            embedding = tf.matmul(embedding, W_proj_cnn) + b_proj_cnn\n            if self.bidirectional:\n                embedding_reverse = tf.matmul(embedding_reverse, W_proj_cnn) \\\n                                    + b_proj_cnn\n            self.token_embedding_layers.append(\n                tf.reshape(embedding, [batch_size, unroll_steps, projection_dim])\n            )\n\n        # reshape back to (batch_size, tokens, dim)\n        if use_highway or use_proj:\n            shp = [batch_size, unroll_steps, projection_dim]\n            embedding = tf.reshape(embedding, shp)\n            if self.bidirectional:\n                embedding_reverse = tf.reshape(embedding_reverse, shp)\n\n        # at last assign attributes for remainder of the model\n        self.embedding = embedding\n        if self.bidirectional:\n            self.embedding_reverse = embedding_reverse\n\n    def _build(self):\n        # size of input options\n        batch_size = self.options[\'batch_size\']\n\n        # LSTM options\n        lstm_dim = self.options[\'lstm\'][\'dim\']\n        projection_dim = self.options[\'lstm\'][\'projection_dim\']\n        n_lstm_layers = self.options[\'lstm\'].get(\'n_layers\', 1)\n        dropout = self.options[\'dropout\']\n        keep_prob = 1.0 - dropout\n\n        if self.char_inputs:\n            self._build_word_char_embeddings()\n        else:\n            self._build_word_embeddings()\n\n        # now the LSTMs\n        # these will collect the initial states for the forward\n        #   (and reverse LSTMs if we are doing bidirectional)\n        self.init_lstm_state = []\n        self.final_lstm_state = []\n\n        # get the LSTM inputs\n        if self.bidirectional:\n            lstm_inputs = [self.embedding, self.embedding_reverse]\n        else:\n            lstm_inputs = [self.embedding]\n\n        # now compute the LSTM outputs\n        cell_clip = self.options[\'lstm\'].get(\'cell_clip\')\n        proj_clip = self.options[\'lstm\'].get(\'proj_clip\')\n\n        use_skip_connections = self.options[\'lstm\'].get(\'use_skip_connections\')\n\n        lstm_outputs = []\n        for lstm_num, lstm_input in enumerate(lstm_inputs):\n            lstm_cells = []\n            for i in range(n_lstm_layers):\n                if projection_dim < lstm_dim:\n                    # are projecting down output\n                    lstm_cell = tf.nn.rnn_cell.LSTMCell(\n                        lstm_dim, num_proj=projection_dim,\n                        cell_clip=cell_clip, proj_clip=proj_clip)\n                else:\n                    lstm_cell = tf.nn.rnn_cell.LSTMCell(\n                        lstm_dim,\n                        cell_clip=cell_clip, proj_clip=proj_clip)\n\n                if use_skip_connections:\n                    # ResidualWrapper adds inputs to outputs\n                    if i == 0:\n                        # don\'t add skip connection from token embedding to\n                        # 1st layer output\n                        pass\n                    else:\n                        # add a skip connection\n                        lstm_cell = tf.nn.rnn_cell.ResidualWrapper(lstm_cell)\n\n                # add dropout\n                if self.is_training:\n                    lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell,\n                                                              input_keep_prob=keep_prob)\n\n                lstm_cells.append(lstm_cell)\n\n            if n_lstm_layers > 1:\n                lstm_cell = tf.nn.rnn_cell.MultiRNNCell(lstm_cells)\n            else:\n                lstm_cell = lstm_cells[0]\n\n            with tf.control_dependencies([lstm_input]):\n                self.init_lstm_state.append(\n                    lstm_cell.zero_state(batch_size, DTYPE))\n                # NOTE: this variable scope is for backward compatibility\n                # with existing models...\n                if self.bidirectional:\n                    with tf.variable_scope(\'RNN_%s\' % lstm_num):\n                        _lstm_output_unpacked, final_state = tf.nn.static_rnn(\n                            lstm_cell,\n                            tf.unstack(lstm_input, axis=1),\n                            initial_state=self.init_lstm_state[-1])\n                else:\n                    _lstm_output_unpacked, final_state = tf.nn.static_rnn(\n                        lstm_cell,\n                        tf.unstack(lstm_input, axis=1),\n                        initial_state=self.init_lstm_state[-1])\n                self.final_lstm_state.append(final_state)\n\n            # (batch_size * unroll_steps, 512)\n            lstm_output_flat = tf.reshape(\n                tf.stack(_lstm_output_unpacked, axis=1), [-1, projection_dim])\n            if self.is_training:\n                # add dropout to output\n                lstm_output_flat = tf.nn.dropout(lstm_output_flat, keep_prob)\n            tf.add_to_collection(\'lstm_output_embeddings\', _lstm_output_unpacked)\n\n            lstm_outputs.append(lstm_output_flat)\n\n        self._build_loss(lstm_outputs)\n\n    def _build_loss(self, lstm_outputs):\n        """"""\n        Create:\n            self.total_loss: total loss op for training\n            self.softmax_W, softmax_b: the softmax variables\n            self.next_token_id / _reverse: placeholders for gold input\n\n        """"""\n        batch_size = self.options[\'batch_size\']\n        unroll_steps = self.options[\'unroll_steps\']\n\n        n_tokens_vocab = self.options[\'n_tokens_vocab\']\n\n        # DEFINE next_token_id and *_reverse placeholders for the gold input\n        def _get_next_token_placeholders(suffix):\n            name = \'next_token_id\' + suffix\n            id_placeholder = tf.placeholder(DTYPE_INT,\n                                            shape=(batch_size, unroll_steps),\n                                            name=name)\n            return id_placeholder\n\n        # get the window and weight placeholders\n        self.next_token_id = _get_next_token_placeholders(\'\')\n        if self.bidirectional:\n            self.next_token_id_reverse = _get_next_token_placeholders(\n                \'_reverse\')\n\n        # DEFINE THE SOFTMAX VARIABLES\n        # get the dimension of the softmax weights\n        # softmax dimension is the size of the output projection_dim\n        softmax_dim = self.options[\'lstm\'][\'projection_dim\']\n\n        # the output softmax variables -- they are shared if bidirectional\n        if self.share_embedding_softmax:\n            # softmax_W is just the embedding layer\n            self.softmax_W = self.embedding_weights\n\n        with tf.variable_scope(\'softmax\'), tf.device(\'/cpu:0\'):\n            # Glorit init (std=(1.0 / sqrt(fan_in))\n            softmax_init = tf.random_normal_initializer(0.0, 1.0 / np.sqrt(softmax_dim))\n            if not self.share_embedding_softmax:\n                self.softmax_W = tf.get_variable(\n                    \'W\', [n_tokens_vocab, softmax_dim],\n                    dtype=DTYPE,\n                    initializer=softmax_init\n                )\n            self.softmax_b = tf.get_variable(\n                \'b\', [n_tokens_vocab],\n                dtype=DTYPE,\n                initializer=tf.constant_initializer(0.0))\n\n        # now calculate losses\n        # loss for each direction of the LSTM\n        self.individual_train_losses = []\n        self.individual_eval_losses = []\n\n        if self.bidirectional:\n            next_ids = [self.next_token_id, self.next_token_id_reverse]\n        else:\n            next_ids = [self.next_token_id]\n\n        for id_placeholder, lstm_output_flat in zip(next_ids, lstm_outputs):\n            # flatten the LSTM output and next token id gold to shape:\n            # (batch_size * unroll_steps, softmax_dim)\n            # Flatten and reshape the token_id placeholders\n            next_token_id_flat = tf.reshape(id_placeholder, [-1, 1])\n\n            with tf.control_dependencies([lstm_output_flat]):\n                sampled_losses = tf.nn.sampled_softmax_loss(self.softmax_W, self.softmax_b,\n                                                            next_token_id_flat, lstm_output_flat,\n                                                            self.options[\'n_negative_samples_batch\'],\n                                                            self.options[\'n_tokens_vocab\'],\n                                                            num_true=1)\n\n                # get the full softmax loss\n                output_scores = tf.matmul(\n                    lstm_output_flat,\n                    tf.transpose(self.softmax_W)\n                ) + self.softmax_b\n                # NOTE: tf.nn.sparse_softmax_cross_entropy_with_logits\n                #   expects unnormalized output since it performs the\n                #   softmax internally\n                losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n                    logits=output_scores,\n                    labels=tf.squeeze(next_token_id_flat, squeeze_dims=[1])\n                )\n            sampled_losses = tf.reshape(sampled_losses, [self.options[\'batch_size\'], -1])\n            losses = tf.reshape(losses, [self.options[\'batch_size\'], -1])\n            self.individual_train_losses.append(tf.reduce_mean(sampled_losses, axis=1))\n            self.individual_eval_losses.append(tf.reduce_mean(losses, axis=1))\n\n        # now make the total loss -- it\'s the train of the individual losses\n        if self.bidirectional:\n            self.total_train_loss = 0.5 * (self.individual_train_losses[0] + self.individual_train_losses[1])\n            self.total_eval_loss = 0.5 * (self.individual_eval_losses[0] + self.individual_eval_losses[1])\n        else:\n            self.total_train_loss = self.individual_train_losses[0]\n            self.total_eval_loss = self.individual_eval_losses[0]\n'"
deeppavlov/models/elmo/elmo.py,20,"b'# originally based on https://github.com/allenai/bilm-tf/blob/master/bilm/training.py\n\n# Modifications copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport copy\nimport json\nfrom logging import getLogger\nfrom typing import Optional, List\n\nimport numpy as np\nimport tensorflow as tf\nfrom overrides import overrides\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.nn_model import NNModel\nfrom deeppavlov.models.elmo.bilm_model import LanguageModel\nfrom deeppavlov.models.elmo.elmo2tfhub import export2hub\nfrom deeppavlov.models.elmo.train_utils import average_gradients, clip_grads, safely_str2int, dump_weights\n\nlog = getLogger(__name__)\n\n\n@register(\'elmo_model\')\nclass ELMo(NNModel):\n    """"""\n    The :class:`~deeppavlov.models.elmo.elmo.ELMo` is a deep contextualized word representation that models both\n    complex characteristics of word use (e.g., syntax and semantics), and how these uses vary across linguistic\n    contexts (i.e., to model polysemy).\n\n    You can use this component for LM training, fine tuning, dumping ELMo to a hdf5 file and wrapping it to\n    the tensorflow hub.\n\n\n    Parameters:\n        options_json_path: Path to the json configure.\n        char_cnn: Options of char_cnn. For example {""activation"":""relu"",""embedding"":{""dim"":16},\n            ""filters"":[[1,32],[2,32],[3,64],[4,128],[5,256],[6,512],[7,1024]],""max_characters_per_token"":50,\n            ""n_characters"":261,""n_highway"":2}\n        bidirectional: Whether to use bidirectional or not.\n        unroll_steps: Number of unrolling steps.\n        n_tokens_vocab: A size of a vocabulary.\n        lstm: Options of lstm. It is a dict of ""cell_clip"":int, ""dim"":int, ""n_layers"":int, ""proj_clip"":int, \n            ""projection_dim"":int, ""use_skip_connections"":bool\n        dropout: Probability of keeping the network state, values from 0 to 1. \n        n_negative_samples_batch: Whether to use negative samples batch or not. Number of batch samples.\n        all_clip_norm_val: Clip the gradients.\n        initial_accumulator_value: Whether to use dropout between layers or not.\n        learning_rate: Learning rate to use during the training (usually from 0.1 to 0.0001)\n        n_gpus: Number of gpu to use.\n        seed: Random seed.\n        batch_size: A size of a train batch.\n        load_epoch_num: An index of loading epoch.\n        epoch_load_path: An epoch loading path relative to save_path.\n        epoch_save_path:  An epoch saving path relative to save_path.\n            If epoch_save_path is None then epoch_save_path = epoch_load_path.\n        dumps_save_path: A dump saving path relative to save_path.\n        tf_hub_save_path: A tf_hub saving path relative to save_path.\n\n    To train ELMo representations from a paper `Deep contextualized word representations\n    <https://arxiv.org/abs/1802.05365>`__ you can use multiple GPUs by set ``n_gpus`` parameter.\n\n    You can explicitly specify the path to a json file with hyperparameters of ELMo used to train by\n    ``options_json_path`` parameter.\n    The json file must be the same as the json file from `original ELMo implementation\n    <https://github.com/allenai/bilm-tf>`__. You can define the architecture using the separate parameters.\n\n    Saving the model will take place in directories with some structure, see below example:\n\n    {MODELS_PATH}/\n        elmo_model/\n            saves/\n                epochs/\n                    1/, 2/, .... # directories of epochs\n                dumps/\n                    weights_epoch_n_1.hdf5, weights_epoch_n_2.hdf5, .... # hdf5 files of dumped ELMo weights\n                hubs/\n                    tf_hub_model_epoch_n_1/, tf_hub_model_epoch_n_2/, .... # directories of tensorflow hub wrapped\n                    ELMo\n\n    Intermediate checkpoints saved to `saves` directory.\n    To specify load/save paths use ``load_epoch_num``, ``epoch_load_path``, ``epoch_save_path``, ``dumps_save_path``,\n    ``tf_hub_save_path``.\n\n    Dumping and tf_hub wrapping of ELMo occurs after each epoch.\n\n    For learning the LM model dataset like 1 Billion Word Benchmark dataset is needed.\n    Examples of how datasets should look like you can learn from the configs of the examples below.\n\n    Vocabulary file is a text file, with one token per line, separated by newlines.\n    Each token in the vocabulary is cached as the appropriate 50 character id sequence once.\n    It is recommended to always include the special <S> and </S> tokens (case sensitive) in the vocabulary file.\n\n    For fine-tuning of LM on specific data, it is enough to save base model to path\n    ``{MODELS_PATH}/elmo_model/saves/epochs/0/`` and start training.\n\n    Also for fine-tuning of LM on specific data, you can use pre-trained model for russian language on different\n    datasets.\n\n\n    LM model pre-trained on `ru-news` dataset ( lines = 63M, tokens = 946M, size = 12GB ), model is available by\n    :config:`elmo_lm_ready4fine_tuning_ru_news </elmo/elmo_lm_ready4fine_tuning_ru_news.json>` configuration file\n    or :config:`elmo_lm_ready4fine_tuning_ru_news_simple </elmo/elmo_lm_ready4fine_tuning_ru_news_simple.json>`\n    configuration file.\n\n    LM model pre-trained on `ru-twitter` dataset ( lines = 104M, tokens = 810M, size = 8.5GB ), model is available by\n    :config:`elmo_lm_ready4fine_tuning_ru_twitter </elmo/elmo_lm_ready4fine_tuning_ru_twitter.json>` configuration file\n    or :config:`elmo_lm_ready4fine_tuning_ru_twitter_simple </elmo/elmo_lm_ready4fine_tuning_ru_twitter_simple.json>`\n    configuration file.\n\n    LM model pre-trained on `ru-wiki` dataset ( lines = 1M, tokens = 386M, size = 5GB ), model is available by\n    :config:`elmo_lm_ready4fine_tuning_ru_wiki </elmo/elmo_lm_ready4fine_tuning_ru_wiki.json>` configuration file\n    or :config:`elmo_lm_ready4fine_tuning_ru_wiki_simple </elmo/elmo_lm_ready4fine_tuning_ru_wiki_simple.json>`\n    configuration file.\n\n    `simple` configuration file is a configuration of a model without special tags of output\n    vocab used for first training.\n\n    .. note::\n\n        You need to download about **4 GB** also by default about **32 GB** of RAM and **10 GB** of GPU memory\n        required to running the :config:`elmo_lm_ready4fine_tuning_ru_* </elmo/>`\n        on one GPU.\n\n    After training you can use ``{MODELS_PATH}/elmo_model/saves/hubs/tf_hub_model_epoch_n_*/``\n    as a ``ModuleSpec`` by using `TensorFlow Hub <https://www.tensorflow.org/hub/overview>`__ or by\n    DeepPavlov :class:`~deeppavlov.models.embedders.elmo_embedder.ELMoEmbedder`.\n\n    More about the ELMo model you can get from `original ELMo implementation\n    <https://github.com/allenai/bilm-tf>`__.\n\n\n    If some required packages are missing, install all the requirements by running in command line:\n\n    .. code:: bash\n\n        python -m deeppavlov install <path_to_config>\n\n    where ``<path_to_config>`` is a path to one of the :config:`provided config files <elmo_embedder>`\n    or its name without an extension, for example :\n\n    .. code:: bash\n\n        python -m deeppavlov install elmo_1b_benchmark_test\n        \n    Examples:\n        For a quick start, you can run test training of the test model on small data by this command from bash:\n\n        .. code:: bash\n\n            python -m deeppavlov train deeppavlov/configs/elmo/elmo_1b_benchmark_test.json -d\n\n        To download the prepared `1 Billion Word Benchmark dataset <http://www.statmt.org/lm-benchmark/>`__ and\n        start a training model use this command from bash:\n\n        .. note::\n\n            You need to download about **2 GB** also by default about **10 GB** of RAM and **10 GB** of GPU memory\n            required to running :config:`elmo_1b_benchmark <elmo/elmo_1b_benchmark.json>` on one GPU.\n\n        .. code:: bash\n\n            python -m deeppavlov train deeppavlov/configs/elmo/elmo_1b_benchmark.json -d\n\n        To fine-tune ELMo as LM model on `1 Billion Word Benchmark dataset <http://www.statmt.org/lm-benchmark/>`__\n        use commands from bash :\n\n        .. code:: bash\n\n            # download the prepared 1 Billion Word Benchmark dataset\n            python -m deeppavlov download deeppavlov/configs/elmo/elmo_1b_benchmark.json\n            # copy model checkpoint, network configuration, vocabulary of pre-trained LM model\n            mkdir -p ${MODELS_PATH}/elmo-1b-benchmark/saves/epochs/0\n            cp my_ckpt.data-00000-of-00001 ${MODELS_PATH}/elmo-1b-benchmark/saves/epochs/0/model.data-00000-of-00001\n            cp my_ckpt.index ${MODELS_PATH}/elmo-1b-benchmark/saves/epochs/0/model.index\n            cp my_ckpt.meta ${MODELS_PATH}/elmo-1b-benchmark/saves/epochs/0/model.meta\n            cp checkpoint ${MODELS_PATH}/elmo-1b-benchmark/saves/epochs/0/checkpoint\n            cp my_options.json ${MODELS_PATH}/elmo-1b-benchmark/options.json\n            cp my_vocab {MODELS_PATH}/elmo-1b-benchmark/vocab-2016-09-10.txt\n            # start a fine-tuning\n            python -m deeppavlov train deeppavlov/configs/elmo/elmo_1b_benchmark.json\n\n        After training you can use the ELMo model from tf_hub wrapper by\n        `TensorFlow Hub <https://www.tensorflow.org/hub/overview>`__ or by\n        DeepPavlov :class:`~deeppavlov.models.embedders.elmo_embedder.ELMoEmbedder`:\n\n        >>> from deeppavlov.models.embedders.elmo_embedder import ELMoEmbedder\n        >>> spec = f""{MODELS_PATH}/elmo-1b-benchmark_test/saves/hubs/tf_hub_model_epoch_n_1/""\n        >>> elmo = ELMoEmbedder(spec)\n        >>> elmo([[\'\xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\', \'\xd0\xb6\xd0\xb8\xd0\xb7\xd0\xbd\xd0\xb8\', \'\xd0\x92\xd1\x81\xd0\xb5\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xbd\xd0\xbe\xd0\xb9\', \'\xd0\xb8\', \'\xd0\xb2\xd0\xbe\xd0\xbe\xd0\xb1\xd1\x89\xd0\xb5\', \'\xd0\xb2\xd1\x81\xd0\xb5\xd0\xb3\xd0\xbe\'], [\'42\']])\n        array([[ 0.00719104,  0.08544601, -0.07179783, ...,  0.10879009,\n                -0.18630421, -0.2189409 ],\n            [ 0.16325025, -0.04736076,  0.12354863, ..., -0.1889013 ,\n                0.04972512,  0.83029324]], dtype=float32)\n\n    """"""\n\n    def __init__(self,\n                 options_json_path: Optional[str] = None,  # Configure by json file\n                 char_cnn: Optional[dict] = None,  # Net architecture by direct params, use for overwrite a json arch.\n                 bidirectional: Optional[bool] = None,\n                 unroll_steps: Optional[int] = None,\n                 n_tokens_vocab: Optional[int] = None,\n                 lstm: Optional[dict] = None,\n                 dropout: Optional[float] = None,  # Regularization\n                 n_negative_samples_batch: Optional[int] = None,  # Train options\n                 all_clip_norm_val: Optional[float] = None,\n                 initial_accumulator_value: float = 1.0,\n                 learning_rate: float = 2e-1,  # For AdagradOptimizer\n                 n_gpus: int = 1,  # TODO: Add cpu supporting\n                 seed: Optional[int] = None,  # Other\n                 batch_size: int = 128,  # Data params\n                 load_epoch_num: Optional[int] = None,\n                 epoch_load_path: str = \'epochs\',\n                 epoch_save_path: Optional[str] = None,\n                 dumps_save_path: str = \'dumps\',\n                 tf_hub_save_path: str = \'hubs\',\n                 **kwargs) -> None:\n\n        # ================ Checking input args =================\n        if not (options_json_path or (char_cnn and bidirectional and unroll_steps\n                                      and n_tokens_vocab and lstm and dropout and\n                                      n_negative_samples_batch and all_clip_norm_val\n        )):\n            raise Warning(\'Use options_json_path or/and direct params to set net architecture.\')\n        self.options = self._load_options(options_json_path)\n        self._update_arch_options(char_cnn, bidirectional, unroll_steps, n_tokens_vocab, lstm)\n        self._update_other_options(dropout, n_negative_samples_batch, all_clip_norm_val)\n\n        # Special options\n        self.options[\'learning_rate\'] = learning_rate\n        self.options[\'initial_accumulator_value\'] = initial_accumulator_value\n        self.options[\'seed\'] = seed\n        self.options[\'n_gpus\'] = n_gpus\n        self.options[\'batch_size\'] = batch_size\n\n        self.permanent_options = self.options\n\n        self.train_options = {}\n        self.valid_options = {\'batch_size\': 256, \'unroll_steps\': 1, \'n_gpus\': 1}\n        self.model_mode = \'\'\n\n        tf.set_random_seed(seed)\n        np.random.seed(seed)\n\n        super().__init__(**kwargs)\n\n        self.epoch_load_path = epoch_load_path\n\n        if load_epoch_num is None:\n            load_epoch_num = self._get_epoch_from(self.epoch_load_path, None)\n\n        if epoch_save_path is None:\n            self.epoch_save_path = self.epoch_load_path\n\n        self.save_epoch_num = self._get_epoch_from(self.epoch_save_path)\n\n        self.dumps_save_path = dumps_save_path\n        self.tf_hub_save_path = tf_hub_save_path\n\n        self._build_model(train=False, epoch=load_epoch_num)\n\n        self.save()\n        # after building the model and saving to the specified save path\n        # change the way to load intermediate checkpoints\n        self.load_path = self.save_path\n\n    def _load_options(self, options_json_path):\n        if options_json_path:\n            options_json_path = expand_path(options_json_path)\n            with open(options_json_path, \'r\') as fin:\n                options = json.load(fin)\n        else:\n            options = {}\n        return options\n\n    def _update_arch_options(self, char_cnn, bidirectional, unroll_steps, n_tokens_vocab, lstm):\n        if char_cnn is not None:\n            self.options[\'char_cnn\'] = char_cnn\n        if bidirectional is not None:\n            self.options[\'bidirectional\'] = bidirectional\n        if unroll_steps is not None:\n            self.options[\'unroll_steps\'] = unroll_steps\n        if n_tokens_vocab is not None:\n            self.options[\'n_tokens_vocab\'] = n_tokens_vocab\n        if lstm is not None:\n            self.options[\'lstm\'] = lstm\n\n    def _update_other_options(self, dropout, n_negative_samples_batch, all_clip_norm_val):\n        if dropout is not None:\n            self.options[\'dropout\'] = dropout\n        if n_negative_samples_batch is not None:\n            self.options[\'n_negative_samples_batch\'] = n_negative_samples_batch\n        if all_clip_norm_val is not None:\n            self.options[\'all_clip_norm_val\'] = all_clip_norm_val\n\n    def _get_epoch_from(self, epoch_load_path, default=0):\n        path = self.load_path\n        path = path.parent / epoch_load_path\n        candidates = path.resolve().glob(\'[0-9]*\')\n        candidates = list(safely_str2int(i.parts[-1]) for i in candidates\n                          if safely_str2int(i.parts[-1]) is not None)\n        epoch_num = max(candidates, default=default)\n        return epoch_num\n\n    def _build_graph(self, graph, train=True):\n        with graph.as_default():\n            with tf.device(\'/cpu:0\'):\n                init_step = 0\n                global_step = tf.get_variable(\n                    \'global_step\', [],\n                    initializer=tf.constant_initializer(init_step), trainable=False)\n                self.global_step = global_step\n                # set up the optimizer\n                opt = tf.train.AdagradOptimizer(learning_rate=self.options[\'learning_rate\'],\n                                                initial_accumulator_value=1.0)\n\n                # calculate the gradients on each GPU\n                tower_grads = []\n                models = []\n                loss = tf.get_variable(\n                    \'train_perplexity\', [],\n                    initializer=tf.constant_initializer(0.0), trainable=False)\n                for k in range(self.options[\'n_gpus\']):\n                    with tf.device(\'/gpu:%d\' % k):\n                        with tf.variable_scope(\'lm\', reuse=k > 0):\n                            # calculate the loss for one model replica and get\n                            #   lstm states\n                            model = LanguageModel(self.options, True)\n                            total_train_loss = model.total_train_loss\n                            total_eval_loss = model.total_eval_loss\n                            models.append(model)\n                            # get gradients\n                            grads = opt.compute_gradients(\n                                tf.reduce_mean(total_train_loss) * self.options[\'unroll_steps\'],\n                                aggregation_method=tf.AggregationMethod.EXPERIMENTAL_TREE,\n                            )\n                            tower_grads.append(grads)\n                            # # keep track of loss across all GPUs\n                            if train:\n                                loss += total_train_loss\n                            else:\n                                loss += total_eval_loss\n\n                # calculate the mean of each gradient across all GPUs\n                grads = average_gradients(tower_grads, self.options[\'batch_size\'], self.options)\n                grads, _ = clip_grads(grads, self.options, True, global_step)\n                loss = loss / self.options[\'n_gpus\']\n                train_op = opt.apply_gradients(grads, global_step=global_step)\n        return models, train_op, loss, graph\n\n    def _init_session(self):\n        sess_config = tf.ConfigProto(allow_soft_placement=True)\n        sess_config.gpu_options.allow_growth = True\n\n        self.sess = tf.Session(config=sess_config)\n        self.sess.run(tf.global_variables_initializer())\n\n        batch_size = self.options[\'batch_size\']\n        unroll_steps = self.options[\'unroll_steps\']\n\n        # get the initial lstm states\n        init_state_tensors = []\n        final_state_tensors = []\n        for model in self.models:\n            init_state_tensors.extend(model.init_lstm_state)\n            final_state_tensors.extend(model.final_lstm_state)\n\n        char_inputs = \'char_cnn\' in self.options\n        if char_inputs:\n            max_chars = self.options[\'char_cnn\'][\'max_characters_per_token\']\n\n        if not char_inputs:\n            feed_dict = {\n                model.token_ids:\n                    np.zeros([batch_size, unroll_steps], dtype=np.int64)\n                for model in self.models\n            }\n        else:\n            feed_dict = {\n                model.tokens_characters:\n                    np.zeros([batch_size, unroll_steps, max_chars],\n                             dtype=np.int32)\n                for model in self.models\n            }\n\n        if self.options[\'bidirectional\']:\n            if not char_inputs:\n                feed_dict.update({\n                    model.token_ids_reverse:\n                        np.zeros([batch_size, unroll_steps], dtype=np.int64)\n                    for model in self.models\n                })\n            else:\n                feed_dict.update({\n                    model.tokens_characters_reverse:\n                        np.zeros([batch_size, unroll_steps, max_chars],\n                                 dtype=np.int32)\n                    for model in self.models\n                })\n\n        init_state_values = self.sess.run(init_state_tensors, feed_dict=feed_dict)\n        return init_state_values, init_state_tensors, final_state_tensors\n\n    def _fill_feed_dict(self,\n                        char_ids_batches,\n                        reversed_char_ids_batches,\n                        token_ids_batches=None,\n                        reversed_token_ids_batches=None):\n        # init state tensors\n        feed_dict = {t: v for t, v in zip(self.init_state_tensors, self.init_state_values)}\n\n        for k, model in enumerate(self.models):\n            start = k * self.options[\'batch_size\']\n            end = (k + 1) * self.options[\'batch_size\']\n\n            # character inputs\n            char_ids = char_ids_batches[start:end]  # get char_ids\n\n            feed_dict[model.tokens_characters] = char_ids\n\n            if self.options[\'bidirectional\']:\n                feed_dict[model.tokens_characters_reverse] = \\\n                    reversed_char_ids_batches[start:end]  # get tokens_characters_reverse\n\n            if token_ids_batches is not None:\n                feed_dict[model.next_token_id] = token_ids_batches[start:end]  # get next_token_id\n                if self.options[\'bidirectional\']:\n                    feed_dict[model.next_token_id_reverse] = \\\n                        reversed_token_ids_batches[start:end]  # get next_token_id_reverse\n\n        return feed_dict\n\n    def __call__(self, x, y, *args, **kwargs) -> List[float]:\n        if len(args) != 0:\n            return []\n        char_ids_batches, reversed_char_ids_batches = x\n        token_ids_batches, reversed_token_ids_batches = y\n\n        feed_dict = self._fill_feed_dict(char_ids_batches, reversed_char_ids_batches, token_ids_batches,\n                                         reversed_token_ids_batches)\n\n        with self.graph.as_default():\n            loss, self.init_state_values = self.sess.run([self.loss, self.final_state_tensors], feed_dict)\n        return loss\n\n    @overrides\n    def load(self, epoch: Optional[int] = None) -> None:\n        """"""Load model parameters from self.load_path""""""\n        path = self.load_path\n        if epoch is not None:\n            path = path.parent / self.epoch_save_path / str(epoch) / path.parts[-1]\n            path.resolve()\n            log.info(f\'[loading {epoch} epoch]\')\n\n        # path.parent.mkdir(parents=True, exist_ok=True)\n        path = str(path)\n\n        # Check presence of the model files\n        if tf.train.checkpoint_exists(path):\n            log.info(f\'[loading model from {path}]\')\n            with self.graph.as_default():\n                saver = tf.train.Saver()\n                saver.restore(self.sess, path)\n        else:\n            log.info(f\'[A checkpoint not found in  {path}]\')\n\n    @overrides\n    def save(self, epoch: Optional[int] = None) -> None:\n        """"""Save model parameters to self.save_path""""""\n        path = self.save_path\n        if epoch is not None:\n            path = path.parent / self.epoch_save_path / str(epoch) / path.parts[-1]\n            path.resolve()\n            log.info(f\'[saving {epoch} epoch]\')\n\n        path.parent.mkdir(parents=True, exist_ok=True)\n        path = str(path)\n\n        log.info(f\'[saving model to {path}]\')\n        with self.graph.as_default():\n            saver = tf.train.Saver()\n            saver.save(self.sess, path)\n\n    def train_on_batch(self,\n                       x_char_ids: list,\n                       y_token_ids: list) -> List[float]:\n        """"""\n        This method is called by trainer to make one training step on one batch.\n\n        Args:\n            x_char_ids:  a batch of char_ids\n            y_token_ids: a batch of token_ids\n\n        Returns:\n            value of loss function on batch\n        """"""\n\n        char_ids_batches, reversed_char_ids_batches = x_char_ids\n        token_ids_batches, reversed_token_ids_batches = y_token_ids\n\n        feed_dict = self._fill_feed_dict(char_ids_batches, reversed_char_ids_batches,\n                                         token_ids_batches, reversed_token_ids_batches)\n\n        with self.graph.as_default():\n            loss, _, self.init_state_values = self.sess.run([self.loss, self.train_op, self.final_state_tensors],\n                                                            feed_dict)\n\n        return np.mean(loss)\n\n    def _build_model(self, train: bool, epoch: Optional[int] = None, **kwargs):\n\n        if hasattr(self, \'sess\'):\n            self.sess.close()\n\n        self.options = copy.deepcopy(self.permanent_options)\n\n        if train:\n            self.options.update(self.train_options)\n            self.options.update(kwargs)\n\n            self.models, self.train_op, self.loss, self.graph = self._build_graph(tf.Graph())\n        else:\n            self.options.update(self.valid_options)\n            self.options.update(kwargs)\n\n            self.models, self.train_op, self.loss, self.graph = self._build_graph(tf.Graph(),\n                                                                                  train=False)\n\n        with self.graph.as_default():\n            self.init_state_values, self.init_state_tensors, self.final_state_tensors = \\\n                self._init_session()\n        self.load(epoch)\n\n    def process_event(self, event_name, data):\n        if event_name == \'before_train\' and self.model_mode != \'train\':\n            self._build_model(train=True)\n            self.model_mode = \'train\'\n        elif event_name == \'before_validation\' and self.model_mode != \'validation\':\n            epoch = self.save_epoch_num + int(data[\'epochs_done\'])\n            self.save(epoch)\n            self.save()\n            self.elmo_export(epoch)\n\n            self._build_model(train=False)\n            self.model_mode = \'validation\'\n\n    def elmo_export(self, epoch: Optional[int] = None) -> None:\n        """"""\n        Dump the trained weights from a model to a HDF5 file and export a TF-Hub module.\n        """"""\n        if hasattr(self, \'sess\'):\n            self.sess.close()\n        path = self.save_path\n        if epoch:\n            from_path = path.parent / self.epoch_save_path / str(epoch) / path.parts[-1]\n            weights_to_path = path.parent / self.dumps_save_path / f\'weights_epoch_n_{epoch}.hdf5\'\n            tf_hub_to_path = path.parent / self.tf_hub_save_path / f\'tf_hub_model_epoch_n_{epoch}\'\n            from_path.resolve()\n            weights_to_path.resolve()\n            tf_hub_to_path.resolve()\n            log.info(f\'[exporting {epoch} epoch]\')\n        else:\n            from_path = path\n            weights_to_path = path.parent / self.dumps_save_path / \'weights.hdf5\'\n            tf_hub_to_path = path.parent / self.tf_hub_save_path / \'tf_hub_model\'\n\n        weights_to_path.parent.mkdir(parents=True, exist_ok=True)\n        tf_hub_to_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Check presence of the model files\n        if tf.train.checkpoint_exists(str(from_path)):\n            dump_weights(from_path.parent, weights_to_path, self.permanent_options)\n\n            options = copy.deepcopy(self.permanent_options)\n            options[\'char_cnn\'][\'n_characters\'] = 262\n            export2hub(weights_to_path, tf_hub_to_path, options)\n\n    def destroy(self) -> None:\n        """"""\n        Delete model from memory\n\n        Returns:\n            None\n        """"""\n        if hasattr(self, \'sess\'):\n            for k in list(self.sess.graph.get_all_collection_keys()):\n                self.sess.graph.clear_collection(k)\n        super().destroy()\n'"
deeppavlov/models/elmo/elmo2tfhub.py,45,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport shutil\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom deeppavlov.models.elmo.elmo_model import BidirectionalLanguageModel, weight_layers\n\n\ndef make_module_spec(options, weight_file):\n    """"""Makes a module spec.\n\n    Args:\n      options: LM hyperparameters.\n      weight_file: location of the hdf5 file with LM weights.\n\n    Returns:\n      A module spec object used for constructing a TF-Hub module.\n    """"""\n\n    def module_fn():\n        """"""Spec function for a token embedding module.""""""\n        # init\n        _bos_id = 256\n        _eos_id = 257\n        _bow_id = 258\n        _eow_id = 259\n        _pad_id = 260\n\n        _max_word_length = 50\n        _parallel_iterations = 10\n        _max_batch_size = 1024\n\n        id_dtype = tf.int32\n        id_nptype = np.int32\n        max_word_length = tf.constant(_max_word_length, dtype=id_dtype, name=\'max_word_length\')\n\n        version = tf.constant(\'from_dp_1\', dtype=tf.string, name=\'version\')\n\n        # the charcter representation of the begin/end of sentence characters\n        def _make_bos_eos(c):\n            r = np.zeros([_max_word_length], dtype=id_nptype)\n            r[:] = _pad_id\n            r[0] = _bow_id\n            r[1] = c\n            r[2] = _eow_id\n            return tf.constant(r, dtype=id_dtype)\n\n        bos_ids = _make_bos_eos(_bos_id)\n        eos_ids = _make_bos_eos(_eos_id)\n\n        def token2ids(token):\n            with tf.name_scope(""token2ids_preprocessor""):\n                char_ids = tf.decode_raw(token, tf.uint8, name=\'decode_raw2get_char_ids\')\n                char_ids = tf.cast(char_ids, tf.int32, name=\'cast2int_token\')\n                char_ids = tf.strided_slice(char_ids, [0], [max_word_length - 2],\n                                            [1], name=\'slice2resized_token\')\n                ids_num = tf.shape(char_ids)[0]\n                fill_ids_num = (_max_word_length - 2) - ids_num\n                pads = tf.fill([fill_ids_num], _pad_id)\n                bow_token_eow_pads = tf.concat([[_bow_id], char_ids, [_eow_id], pads],\n                                               0, name=\'concat2bow_token_eow_pads\')\n                return bow_token_eow_pads\n\n        def sentence_tagging_and_padding(sen_dim):\n            with tf.name_scope(""sentence_tagging_and_padding_preprocessor""):\n                sen = sen_dim[0]\n                dim = sen_dim[1]\n                extra_dim = tf.shape(sen)[0] - dim\n                sen = tf.slice(sen, [0, 0], [dim, max_word_length], name=\'slice2sen\')\n\n                bos_sen_eos = tf.concat([[bos_ids], sen, [eos_ids]], 0, name=\'concat2bos_sen_eos\')\n                bos_sen_eos_plus_one = bos_sen_eos + 1\n                bos_sen_eos_pads = tf.pad(bos_sen_eos_plus_one, [[0, extra_dim], [0, 0]],\n                                          ""CONSTANT"", name=\'pad2bos_sen_eos_pads\')\n                return bos_sen_eos_pads\n\n        # Input placeholders to the biLM.\n        tokens = tf.placeholder(shape=(None, None), dtype=tf.string, name=\'ph2tokens\')\n        sequence_len = tf.placeholder(shape=(None,), dtype=tf.int32, name=\'ph2sequence_len\')\n\n        tok_shape = tf.shape(tokens)\n        line_tokens = tf.reshape(tokens, shape=[-1], name=\'reshape2line_tokens\')\n\n        with tf.device(\'/cpu:0\'):\n            tok_ids = tf.map_fn(\n                token2ids,\n                line_tokens,\n                dtype=tf.int32, back_prop=False, parallel_iterations=_parallel_iterations,\n                name=\'map_fn2get_tok_ids\')\n\n        tok_ids = tf.reshape(tok_ids, [tok_shape[0], tok_shape[1], -1], name=\'reshape2tok_ids\')\n        with tf.device(\'/cpu:0\'):\n            sen_ids = tf.map_fn(\n                sentence_tagging_and_padding,\n                (tok_ids, sequence_len),\n                dtype=tf.int32, back_prop=False, parallel_iterations=_parallel_iterations,\n                name=\'map_fn2get_sen_ids\')\n\n        # Build the biLM graph.\n        bilm = BidirectionalLanguageModel(options, str(weight_file),\n                                          max_batch_size=_max_batch_size)\n\n        embeddings_op = bilm(sen_ids)\n\n        # Get an op to compute ELMo (weighted average of the internal biLM layers)\n        elmo_output = weight_layers(\'elmo_output\', embeddings_op, l2_coef=0.0)\n\n        weighted_op = elmo_output[\'weighted_op\']\n        mean_op = elmo_output[\'mean_op\']\n        word_emb = elmo_output[\'word_emb\']\n        lstm_outputs1 = elmo_output[\'lstm_outputs1\']\n        lstm_outputs2 = elmo_output[\'lstm_outputs2\']\n\n        hub.add_signature(""tokens"", {""tokens"": tokens, ""sequence_len"": sequence_len},\n                          {""elmo"": weighted_op,\n                           ""default"": mean_op,\n                           ""word_emb"": word_emb,\n                           ""lstm_outputs1"": lstm_outputs1,\n                           ""lstm_outputs2"": lstm_outputs2,\n                           ""version"": version})\n\n        # #########################Next signature############################# #\n\n        # Input placeholders to the biLM.\n        def_strings = tf.placeholder(shape=(None), dtype=tf.string)\n        def_tokens_sparse = tf.string_split(def_strings)\n        def_tokens_dense = tf.sparse_to_dense(sparse_indices=def_tokens_sparse.indices,\n                                              output_shape=def_tokens_sparse.dense_shape,\n                                              sparse_values=def_tokens_sparse.values,\n                                              default_value=\'\'\n                                              )\n        def_mask = tf.not_equal(def_tokens_dense, \'\')\n        def_int_mask = tf.cast(def_mask, dtype=tf.int32)\n        def_sequence_len = tf.reduce_sum(def_int_mask, axis=-1)\n\n        def_tok_shape = tf.shape(def_tokens_dense)\n        def_line_tokens = tf.reshape(def_tokens_dense, shape=[-1], name=\'reshape2line_tokens\')\n\n        with tf.device(\'/cpu:0\'):\n            def_tok_ids = tf.map_fn(\n                token2ids,\n                def_line_tokens,\n                dtype=tf.int32, back_prop=False, parallel_iterations=_parallel_iterations,\n                name=\'map_fn2get_tok_ids\')\n\n        def_tok_ids = tf.reshape(def_tok_ids, [def_tok_shape[0], def_tok_shape[1], -1], name=\'reshape2tok_ids\')\n        with tf.device(\'/cpu:0\'):\n            def_sen_ids = tf.map_fn(\n                sentence_tagging_and_padding,\n                (def_tok_ids, def_sequence_len),\n                dtype=tf.int32, back_prop=False, parallel_iterations=_parallel_iterations,\n                name=\'map_fn2get_sen_ids\')\n\n        # Get ops to compute the LM embeddings.\n        def_embeddings_op = bilm(def_sen_ids)\n\n        # Get an op to compute ELMo (weighted average of the internal biLM layers)\n        def_elmo_output = weight_layers(\'elmo_output\', def_embeddings_op, l2_coef=0.0, reuse=True)\n\n        def_weighted_op = def_elmo_output[\'weighted_op\']\n        def_mean_op = def_elmo_output[\'mean_op\']\n        def_word_emb = def_elmo_output[\'word_emb\']\n        def_lstm_outputs1 = def_elmo_output[\'lstm_outputs1\']\n        def_lstm_outputs2 = def_elmo_output[\'lstm_outputs2\']\n\n        hub.add_signature(""default"", {""strings"": def_strings},\n                          {""elmo"": def_weighted_op,\n                           ""default"": def_mean_op,\n                           ""word_emb"": def_word_emb,\n                           ""lstm_outputs1"": def_lstm_outputs1,\n                           ""lstm_outputs2"": def_lstm_outputs2,\n                           ""version"": version})\n\n    return hub.create_module_spec(module_fn)\n\n\ndef export2hub(weight_file, hub_dir, options):\n    """"""Exports a TF-Hub module\n    """"""\n\n    spec = make_module_spec(options, str(weight_file))\n\n    try:\n        with tf.Graph().as_default():\n            module = hub.Module(spec)\n\n            with tf.Session() as sess:\n                sess.run(tf.global_variables_initializer())\n                if hub_dir.exists():\n                    shutil.rmtree(hub_dir)\n                module.export(str(hub_dir), sess)\n    finally:\n        pass\n'"
deeppavlov/models/elmo/elmo_model.py,104,"b'# originally based on https://github.com/allenai/bilm-tf/blob/master/bilm/model.py\n\n# Modifications copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport h5py\nimport numpy as np\nimport tensorflow as tf\n\nDTYPE = \'float32\'\n\n\nclass BidirectionalLanguageModel(object):\n    def __init__(\n            self,\n            options: dict,\n            weight_file: str,\n            use_character_inputs=True,\n            embedding_weight_file=None,\n            max_batch_size=128):\n        """"""\n        Creates the language model computational graph and loads weights\n\n        Two options for input type:\n            (1) To use character inputs (paired with Batcher)\n                pass use_character_inputs=True, and ids_placeholder\n                of shape (None, None, max_characters_per_token)\n                to __call__\n            (2) To use token ids as input (paired with TokenBatcher),\n                pass use_character_inputs=False and ids_placeholder\n                of shape (None, None) to __call__.\n                In this case, embedding_weight_file is also required input\n\n        options_file: location of the json formatted file with\n                      LM hyperparameters\n        weight_file: location of the hdf5 file with LM weights\n        use_character_inputs: if True, then use character ids as input,\n            otherwise use token ids\n        max_batch_size: the maximum allowable batch size\n        """"""\n        if not use_character_inputs:\n            if embedding_weight_file is None:\n                raise ValueError(\n                    ""embedding_weight_file is required input with ""\n                    ""not use_character_inputs""\n                )\n\n        self._options = options\n        self._weight_file = weight_file\n        self._embedding_weight_file = embedding_weight_file\n        self._use_character_inputs = use_character_inputs\n        self._max_batch_size = max_batch_size\n\n        self._ops = {}\n        self._graphs = {}\n\n    def __call__(self, ids_placeholder):\n        """"""\n        Given the input character ids (or token ids), returns a dictionary\n            with tensorflow ops:\n\n            {\'lm_embeddings\': embedding_op,\n             \'lengths\': sequence_lengths_op,\n             \'mask\': op to compute mask}\n\n        embedding_op computes the LM embeddings and is shape\n            (None, 3, None, 1024)\n        lengths_op computes the sequence lengths and is shape (None, )\n        mask computes the sequence mask and is shape (None, None)\n\n        ids_placeholder: a tf.placeholder of type int32.\n            If use_character_inputs=True, it is shape\n                (None, None, max_characters_per_token) and holds the input\n                character ids for a batch\n            If use_character_input=False, it is shape (None, None) and\n                holds the input token ids for a batch\n        """"""\n        if ids_placeholder in self._ops:\n            # have already created ops for this placeholder, just return them\n            ret = self._ops[ids_placeholder]\n\n        else:\n            # need to create the graph\n            if len(self._ops) == 0:\n                # first time creating the graph, don\'t reuse variables\n                lm_graph = BidirectionalLanguageModelGraph(\n                    self._options,\n                    self._weight_file,\n                    ids_placeholder,\n                    embedding_weight_file=self._embedding_weight_file,\n                    use_character_inputs=self._use_character_inputs,\n                    max_batch_size=self._max_batch_size)\n            else:\n                with tf.variable_scope(\'\', reuse=True):\n                    lm_graph = BidirectionalLanguageModelGraph(\n                        self._options,\n                        self._weight_file,\n                        ids_placeholder,\n                        embedding_weight_file=self._embedding_weight_file,\n                        use_character_inputs=self._use_character_inputs,\n                        max_batch_size=self._max_batch_size)\n\n            ops = self._build_ops(lm_graph)\n            self._ops[ids_placeholder] = ops\n            self._graphs[ids_placeholder] = lm_graph\n            ret = ops\n\n        return ret\n\n    def _build_ops(self, lm_graph):\n        with tf.control_dependencies([lm_graph.update_state_op]):\n            # get the LM embeddings\n            token_embeddings = lm_graph.embedding\n            layers = [\n                tf.concat([token_embeddings, token_embeddings], axis=2)\n            ]\n\n            n_lm_layers = len(lm_graph.lstm_outputs[\'forward\'])\n            for i in range(n_lm_layers):\n                layers.append(\n                    tf.concat(\n                        [lm_graph.lstm_outputs[\'forward\'][i],\n                         lm_graph.lstm_outputs[\'backward\'][i]],\n                        axis=-1\n                    )\n                )\n\n            # The layers include the BOS/EOS tokens.  Remove them\n            sequence_length_wo_bos_eos = lm_graph.sequence_lengths - 2\n            layers_without_bos_eos = []\n            for layer in layers:\n                layer_wo_bos_eos = layer[:, 1:, :]\n                layer_wo_bos_eos = tf.reverse_sequence(\n                    layer_wo_bos_eos,\n                    lm_graph.sequence_lengths - 1,\n                    seq_axis=1,\n                    batch_axis=0,\n                )\n                layer_wo_bos_eos = layer_wo_bos_eos[:, 1:, :]\n                layer_wo_bos_eos = tf.reverse_sequence(\n                    layer_wo_bos_eos,\n                    sequence_length_wo_bos_eos,\n                    seq_axis=1,\n                    batch_axis=0,\n                )\n                layers_without_bos_eos.append(layer_wo_bos_eos)\n\n            # concatenate the layers\n            lm_embeddings = tf.concat(\n                [tf.expand_dims(t, axis=1) for t in layers_without_bos_eos],\n                axis=1\n            )\n\n            # get the mask op without bos/eos.\n            # tf doesn\'t support reversing boolean tensors, so cast\n            # to int then back\n            mask_wo_bos_eos = tf.cast(lm_graph.mask[:, 1:], \'int32\')\n            mask_wo_bos_eos = tf.reverse_sequence(\n                mask_wo_bos_eos,\n                lm_graph.sequence_lengths - 1,\n                seq_axis=1,\n                batch_axis=0,\n            )\n            mask_wo_bos_eos = mask_wo_bos_eos[:, 1:]\n            mask_wo_bos_eos = tf.reverse_sequence(\n                mask_wo_bos_eos,\n                sequence_length_wo_bos_eos,\n                seq_axis=1,\n                batch_axis=0,\n            )\n            mask_wo_bos_eos = tf.cast(mask_wo_bos_eos, \'bool\')\n\n        return {\n            \'lm_embeddings\': lm_embeddings,\n            \'lengths\': sequence_length_wo_bos_eos,\n            \'token_embeddings\': lm_graph.embedding,\n            \'mask\': mask_wo_bos_eos,\n        }\n\n\ndef _pretrained_initializer(varname, weight_file, embedding_weight_file=None):\n    """"""\n    We\'ll stub out all the initializers in the pretrained LM with\n    a function that loads the weights from the file\n    """"""\n    weight_name_map = {}\n    for i in range(2):\n        for j in range(8):  # if we decide to add more layers\n            root = \'RNN_{}/RNN/MultiRNNCell/Cell{}\'.format(i, j)\n            weight_name_map[root + \'/rnn/lstm_cell/kernel\'] = \\\n                root + \'/LSTMCell/W_0\'\n            weight_name_map[root + \'/rnn/lstm_cell/bias\'] = \\\n                root + \'/LSTMCell/B\'\n            weight_name_map[root + \'/rnn/lstm_cell/projection/kernel\'] = \\\n                root + \'/LSTMCell/W_P_0\'\n\n    # convert the graph name to that in the checkpoint\n    varname_in_file = varname[5:]\n    if varname_in_file.startswith(\'RNN\'):\n        varname_in_file = weight_name_map[varname_in_file]\n\n    if varname_in_file == \'embedding\':\n        with h5py.File(embedding_weight_file, \'r\') as fin:\n            # Have added a special 0 index for padding not present\n            # in the original model.\n            embed_weights = fin[varname_in_file][...]\n            weights = np.zeros(\n                (embed_weights.shape[0] + 1, embed_weights.shape[1]),\n                dtype=DTYPE\n            )\n            weights[1:, :] = embed_weights\n    else:\n        with h5py.File(weight_file, \'r\') as fin:\n            if varname_in_file == \'char_embed\':\n                # Have added a special 0 index for padding not present\n                # in the original model.\n                char_embed_weights = fin[varname_in_file][...]\n                weights = np.zeros(\n                    (char_embed_weights.shape[0] + 1,\n                     char_embed_weights.shape[1]),\n                    dtype=DTYPE\n                )\n                weights[1:, :] = char_embed_weights\n            else:\n                weights = fin[varname_in_file][...]\n\n    # Tensorflow initializers are callables that accept a shape parameter\n    # and some optional kwargs\n    def ret(shape, **kwargs):\n        if list(shape) != list(weights.shape):\n            raise ValueError(\n                ""Invalid shape initializing {0}, got {1}, expected {2}"".format(\n                    varname_in_file, shape, weights.shape)\n            )\n        return weights\n\n    return ret\n\n\nclass BidirectionalLanguageModelGraph(object):\n    """"""\n    Creates the computational graph and holds the ops necessary for runnint\n    a bidirectional language model\n    """"""\n\n    def __init__(self, options, weight_file, ids_placeholder,\n                 use_character_inputs=True, embedding_weight_file=None,\n                 max_batch_size=128):\n\n        self.options = options\n        self._max_batch_size = max_batch_size\n        self.ids_placeholder = ids_placeholder\n        self.use_character_inputs = use_character_inputs\n\n        # this custom_getter will make all variables not trainable and\n        # override the default initializer\n        def custom_getter(getter, name, *args, **kwargs):\n            kwargs[\'trainable\'] = False\n            kwargs[\'initializer\'] = _pretrained_initializer(\n                name, weight_file, embedding_weight_file\n            )\n            return getter(name, *args, **kwargs)\n\n        if embedding_weight_file is not None:\n            # get the vocab size\n            with h5py.File(embedding_weight_file, \'r\') as fin:\n                # +1 for padding\n                self._n_tokens_vocab = fin[\'embedding\'].shape[0] + 1\n        else:\n            self._n_tokens_vocab = None\n\n        with tf.variable_scope(\'bilm\', custom_getter=custom_getter):\n            self._build()\n\n    def _build(self):\n        if self.use_character_inputs:\n            self._build_word_char_embeddings()\n        else:\n            self._build_word_embeddings()\n        self._build_lstms()\n\n    def _build_word_char_embeddings(self):\n        """"""\n        options contains key \'char_cnn\': {\n\n        \'n_characters\': 262,\n\n        # includes the start / end characters\n        \'max_characters_per_token\': 50,\n\n        \'filters\': [\n            [1, 32],\n            [2, 32],\n            [3, 64],\n            [4, 128],\n            [5, 256],\n            [6, 512],\n            [7, 512]\n        ],\n        \'activation\': \'tanh\',\n\n        # for the character embedding\n        \'embedding\': {\'dim\': 16}\n\n        # for highway layers\n        # if omitted, then no highway layers\n        \'n_highway\': 2,\n        }\n        """"""\n        projection_dim = self.options[\'lstm\'][\'projection_dim\']\n\n        cnn_options = self.options[\'char_cnn\']\n        filters = cnn_options[\'filters\']\n        n_filters = sum(f[1] for f in filters)\n        max_chars = cnn_options[\'max_characters_per_token\']\n        char_embed_dim = cnn_options[\'embedding\'][\'dim\']\n        n_chars = cnn_options[\'n_characters\']\n        if n_chars != 262:\n            raise Exception(""Set n_characters=262 after training see a \\\n                            https://github.com/allenai/bilm-tf/blob/master/README.md"")\n\n        if cnn_options[\'activation\'] == \'tanh\':\n            activation = tf.nn.tanh\n        elif cnn_options[\'activation\'] == \'relu\':\n            activation = tf.nn.relu\n\n        # the character embeddings\n        with tf.device(""/cpu:0""):\n            self.embedding_weights = tf.get_variable(""char_embed"", [n_chars, char_embed_dim],\n                                                     dtype=DTYPE,\n                                                     initializer=tf.random_uniform_initializer(-1.0, 1.0))\n            # shape (batch_size, unroll_steps, max_chars, embed_dim)\n            self.char_embedding = tf.nn.embedding_lookup(self.embedding_weights,\n                                                         self.ids_placeholder)\n\n        # the convolutions\n        def make_convolutions(inp):\n            with tf.variable_scope(\'CNN\'):\n                convolutions = []\n                for i, (width, num) in enumerate(filters):\n                    if cnn_options[\'activation\'] == \'relu\':\n                        # He initialization for ReLU activation\n                        # with char embeddings init between -1 and 1\n                        # w_init = tf.random_normal_initializer(\n                        #    mean=0.0,\n                        #    stddev=np.sqrt(2.0 / (width * char_embed_dim))\n                        # )\n\n                        # Kim et al 2015, +/- 0.05\n                        w_init = tf.random_uniform_initializer(\n                            minval=-0.05, maxval=0.05)\n                    elif cnn_options[\'activation\'] == \'tanh\':\n                        # glorot init\n                        w_init = tf.random_normal_initializer(\n                            mean=0.0,\n                            stddev=np.sqrt(1.0 / (width * char_embed_dim))\n                        )\n                    w = tf.get_variable(\n                        ""W_cnn_%s"" % i,\n                        [1, width, char_embed_dim, num],\n                        initializer=w_init,\n                        dtype=DTYPE)\n                    b = tf.get_variable(\n                        ""b_cnn_%s"" % i, [num], dtype=DTYPE,\n                        initializer=tf.constant_initializer(0.0))\n\n                    conv = tf.nn.conv2d(inp, w,\n                                        strides=[1, 1, 1, 1],\n                                        padding=""VALID"") + b\n                    # now max pool\n                    conv = tf.nn.max_pool(conv, [1, 1, max_chars - width + 1, 1],\n                                          [1, 1, 1, 1], \'VALID\')\n\n                    # activation\n                    conv = activation(conv)\n                    conv = tf.squeeze(conv, squeeze_dims=[2])\n\n                    convolutions.append(conv)\n\n            return tf.concat(convolutions, 2)\n\n        embedding = make_convolutions(self.char_embedding)\n\n        # for highway and projection layers\n        n_highway = cnn_options.get(\'n_highway\')\n        use_highway = n_highway is not None and n_highway > 0\n        use_proj = n_filters != projection_dim\n\n        if use_highway or use_proj:\n            #   reshape from (batch_size, n_tokens, dim) to (-1, dim)\n            batch_size_n_tokens = tf.shape(embedding)[0:2]\n            embedding = tf.reshape(embedding, [-1, n_filters])\n\n        # set up weights for projection\n        if use_proj:\n            assert n_filters > projection_dim\n            with tf.variable_scope(\'CNN_proj\'):\n                W_proj_cnn = tf.get_variable(\n                    ""W_proj"", [n_filters, projection_dim],\n                    initializer=tf.random_normal_initializer(\n                        mean=0.0, stddev=np.sqrt(1.0 / n_filters)),\n                    dtype=DTYPE)\n                b_proj_cnn = tf.get_variable(\n                    ""b_proj"", [projection_dim],\n                    initializer=tf.constant_initializer(0.0),\n                    dtype=DTYPE)\n\n        # apply highways layers\n        def high(x, ww_carry, bb_carry, ww_tr, bb_tr):\n            carry_gate = tf.nn.sigmoid(tf.matmul(x, ww_carry) + bb_carry)\n            transform_gate = tf.nn.relu(tf.matmul(x, ww_tr) + bb_tr)\n            return carry_gate * transform_gate + (1.0 - carry_gate) * x\n\n        if use_highway:\n            highway_dim = n_filters\n\n            for i in range(n_highway):\n                with tf.variable_scope(\'CNN_high_%s\' % i):\n                    W_carry = tf.get_variable(\n                        \'W_carry\', [highway_dim, highway_dim],\n                        # glorit init\n                        initializer=tf.random_normal_initializer(\n                            mean=0.0, stddev=np.sqrt(1.0 / highway_dim)),\n                        dtype=DTYPE)\n                    b_carry = tf.get_variable(\n                        \'b_carry\', [highway_dim],\n                        initializer=tf.constant_initializer(-2.0),\n                        dtype=DTYPE)\n                    W_transform = tf.get_variable(\n                        \'W_transform\', [highway_dim, highway_dim],\n                        initializer=tf.random_normal_initializer(\n                            mean=0.0, stddev=np.sqrt(1.0 / highway_dim)),\n                        dtype=DTYPE)\n                    b_transform = tf.get_variable(\n                        \'b_transform\', [highway_dim],\n                        initializer=tf.constant_initializer(0.0),\n                        dtype=DTYPE)\n\n                embedding = high(embedding, W_carry, b_carry,\n                                 W_transform, b_transform)\n\n        # finally project down if needed\n        if use_proj:\n            embedding = tf.matmul(embedding, W_proj_cnn) + b_proj_cnn\n\n        # reshape back to (batch_size, tokens, dim)\n        if use_highway or use_proj:\n            shp = tf.concat([batch_size_n_tokens, [projection_dim]], axis=0)\n            embedding = tf.reshape(embedding, shp)\n\n        # at last assign attributes for remainder of the model\n        self.embedding = embedding\n\n    def _build_word_embeddings(self):\n        projection_dim = self.options[\'lstm\'][\'projection_dim\']\n\n        # the word embeddings\n        with tf.device(""/cpu:0""):\n            self.embedding_weights = tf.get_variable(\n                ""embedding"", [self._n_tokens_vocab, projection_dim],\n                dtype=DTYPE,\n            )\n            self.embedding = tf.nn.embedding_lookup(self.embedding_weights,\n                                                    self.ids_placeholder)\n\n    def _build_lstms(self):\n        # now the LSTMs\n        # these will collect the initial states for the forward\n        #   (and reverse LSTMs if we are doing bidirectional)\n\n        # parse the options\n        lstm_dim = self.options[\'lstm\'][\'dim\']\n        projection_dim = self.options[\'lstm\'][\'projection_dim\']\n        n_lstm_layers = self.options[\'lstm\'].get(\'n_layers\', 1)\n        cell_clip = self.options[\'lstm\'].get(\'cell_clip\')\n        proj_clip = self.options[\'lstm\'].get(\'proj_clip\')\n        use_skip_connections = self.options[\'lstm\'][\'use_skip_connections\']\n\n        # the sequence lengths from input mask\n        if self.use_character_inputs:\n            mask = tf.reduce_any(self.ids_placeholder > 0, axis=2)\n        else:\n            mask = self.ids_placeholder > 0\n        sequence_lengths = tf.reduce_sum(tf.cast(mask, tf.int32), axis=1)\n        batch_size = tf.shape(sequence_lengths)[0]\n\n        # for each direction, we\'ll store tensors for each layer\n        self.lstm_outputs = {\'forward\': [], \'backward\': []}\n        self.lstm_state_sizes = {\'forward\': [], \'backward\': []}\n        self.lstm_init_states = {\'forward\': [], \'backward\': []}\n        self.lstm_final_states = {\'forward\': [], \'backward\': []}\n\n        update_ops = []\n        for direction in [\'forward\', \'backward\']:\n            if direction == \'forward\':\n                layer_input = self.embedding\n            else:\n                layer_input = tf.reverse_sequence(\n                    self.embedding,\n                    sequence_lengths,\n                    seq_axis=1,\n                    batch_axis=0\n                )\n\n            for i in range(n_lstm_layers):\n                if projection_dim < lstm_dim:\n                    # are projecting down output\n                    lstm_cell = tf.nn.rnn_cell.LSTMCell(\n                        lstm_dim, num_proj=projection_dim,\n                        cell_clip=cell_clip, proj_clip=proj_clip)\n                else:\n                    lstm_cell = tf.nn.rnn_cell.LSTMCell(lstm_dim,\n                                                        cell_clip=cell_clip, proj_clip=proj_clip)\n\n                if use_skip_connections:\n                    # ResidualWrapper adds inputs to outputs\n                    if i == 0:\n                        # don\'t add skip connection from token embedding to\n                        # 1st layer output\n                        pass\n                    else:\n                        # add a skip connection\n                        lstm_cell = tf.nn.rnn_cell.ResidualWrapper(lstm_cell)\n\n                # collect the input state, run the dynamic rnn, collect\n                # the output\n                # the LSTMs are stateful.  To support multiple batch sizes,\n                # we\'ll allocate size for states up to max_batch_size,\n                # then use the first batch_size entries for each batch\n                init_states = [\n                    tf.Variable(\n                        tf.zeros([self._max_batch_size, dim]),\n                        trainable=False\n                    )\n                    for dim in lstm_cell.state_size\n                ]\n                batch_init_states = [\n                    state[:batch_size, :] for state in init_states\n                ]\n\n                if direction == \'forward\':\n                    i_direction = 0\n                else:\n                    i_direction = 1\n                variable_scope_name = \'RNN_{0}/RNN/MultiRNNCell/Cell{1}\'.format(\n                    i_direction, i)\n                with tf.variable_scope(variable_scope_name):\n                    layer_output, final_state = tf.nn.dynamic_rnn(\n                        lstm_cell,\n                        layer_input,\n                        sequence_length=sequence_lengths,\n                        initial_state=tf.nn.rnn_cell.LSTMStateTuple(\n                            *batch_init_states),\n                    )\n\n                self.lstm_state_sizes[direction].append(lstm_cell.state_size)\n                self.lstm_init_states[direction].append(init_states)\n                self.lstm_final_states[direction].append(final_state)\n                if direction == \'forward\':\n                    self.lstm_outputs[direction].append(layer_output)\n                else:\n                    self.lstm_outputs[direction].append(\n                        tf.reverse_sequence(\n                            layer_output,\n                            sequence_lengths,\n                            seq_axis=1,\n                            batch_axis=0\n                        )\n                    )\n\n                with tf.control_dependencies([layer_output]):\n                    # update the initial states\n                    for i in range(2):\n                        new_state = tf.concat(\n                            [final_state[i][:batch_size, :],\n                             init_states[i][batch_size:, :]], axis=0)\n                        state_update_op = tf.assign(init_states[i], new_state)\n                        update_ops.append(state_update_op)\n\n                layer_input = layer_output\n\n        self.mask = mask\n        self.sequence_lengths = sequence_lengths\n        self.update_state_op = tf.group(*update_ops)\n\n\ndef weight_layers(name, bilm_ops, l2_coef=None,\n                  use_top_only=False, do_layer_norm=False, reuse=False):\n    """"""\n    Weight the layers of a biLM with trainable scalar weights to\n    compute ELMo representations.\n\n    For each output layer, this returns two ops.  The first computes\n        a layer specific weighted average of the biLM layers, and\n        the second the l2 regularizer loss term.\n    The regularization terms are also add to tf.GraphKeys.REGULARIZATION_LOSSES\n\n    Input:\n        name = a string prefix used for the trainable variable names\n        bilm_ops = the tensorflow ops returned to compute internal\n            representations from a biLM.  This is the return value\n            from BidirectionalLanguageModel(...)(ids_placeholder)\n        l2_coef: the l2 regularization coefficient $\\lambda$.\n            Pass None or 0.0 for no regularization.\n        use_top_only: if True, then only use the top layer.\n        do_layer_norm: if True, then apply layer normalization to each biLM\n            layer before normalizing\n        reuse: reuse an aggregation variable scope.\n\n    Output:\n        {\n            \'weighted_op\': op to compute weighted average for output,\n            \'regularization_op\': op to compute regularization term\n        }\n    """"""\n\n    def _l2_regularizer(weights):\n        if l2_coef is not None:\n            return l2_coef * tf.reduce_sum(tf.square(weights))\n        else:\n            return 0.0\n\n    # Get ops for computing LM embeddings and mask\n    lm_embeddings = bilm_ops[\'lm_embeddings\']\n    mask = bilm_ops[\'mask\']\n\n    n_lm_layers = int(lm_embeddings.get_shape()[1])\n    lm_dim = int(lm_embeddings.get_shape()[3])\n    # import pdb; pdb.set_trace()\n\n    with tf.control_dependencies([lm_embeddings, mask]):\n        # Cast the mask and broadcast for layer use.\n        mask_float = tf.cast(mask, \'float32\')\n        broadcast_mask = tf.expand_dims(mask_float, axis=-1)\n\n        def _do_ln(x):\n            # do layer normalization excluding the mask\n            x_masked = x * broadcast_mask\n            N = tf.reduce_sum(mask_float) * lm_dim\n            mean = tf.reduce_sum(x_masked) / N\n            variance = tf.reduce_sum(((x_masked - mean) * broadcast_mask) ** 2) / N\n            return tf.nn.batch_normalization(\n                x, mean, variance, None, None, 1E-12\n            )\n\n        if use_top_only:\n            layers = tf.split(lm_embeddings, n_lm_layers, axis=1)\n            # just the top layer\n            sum_pieces = tf.squeeze(layers[-1], squeeze_dims=1)\n            # no regularization\n            reg = 0.0\n        else:\n            with tf.variable_scope(""aggregation"", reuse=reuse):\n                W = tf.get_variable(\n                    \'{}_ELMo_W\'.format(name),\n                    shape=(n_lm_layers,),\n                    initializer=tf.zeros_initializer,\n                    regularizer=_l2_regularizer,\n                    trainable=True,\n                )\n\n            # normalize the weights\n            normed_weights = tf.split(\n                tf.nn.softmax(W + 1.0 / n_lm_layers), n_lm_layers\n            )\n            # split LM layers\n            layers = tf.split(lm_embeddings, n_lm_layers, axis=1)\n\n            # compute the weighted, normalized LM activations\n            pieces = []\n            for w, t in zip(normed_weights, layers):\n                if do_layer_norm:\n                    pieces.append(w * _do_ln(tf.squeeze(t, squeeze_dims=1)))\n                else:\n                    pieces.append(w * tf.squeeze(t, squeeze_dims=1))\n            sum_pieces = tf.add_n(pieces)\n\n            # get the regularizer\n            reg = [\n                r for r in tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n                if r.name.find(\'{}_ELMo_W/\'.format(name)) >= 0\n            ]\n            if len(reg) != 1:\n                raise ValueError\n\n        # scale the weighted sum by gamma\n\n        with tf.variable_scope(""aggregation"", reuse=reuse):\n            gamma = tf.get_variable(\n                \'{}_ELMo_gamma\'.format(name),\n                shape=(1,),\n                initializer=tf.ones_initializer,\n                regularizer=None,\n                trainable=True,\n            )\n\n        weighted_lm_layers = sum_pieces * gamma\n        weighted_lm_layers_masked = sum_pieces * broadcast_mask\n\n        weighted_lm_layers_sum = tf.reduce_sum(weighted_lm_layers_masked, 1)\n\n        mask_sum = tf.reduce_sum(mask_float, 1)\n        mask_sum = tf.maximum(mask_sum, [1])\n\n        weighted_lm_layers_mean = weighted_lm_layers_sum / tf.expand_dims(mask_sum, - 1)\n\n        word_emb_2n = tf.squeeze(layers[0], [1])\n        word_emb_1n = tf.slice(word_emb_2n, [0, 0, 0], [-1, -1, lm_dim // 2])  # to 512\n        lstm_outputs1 = tf.squeeze(layers[1], [1])\n        lstm_outputs2 = tf.squeeze(layers[2], [1])\n\n        ret = {\'weighted_op\': weighted_lm_layers,\n               \'mean_op\': weighted_lm_layers_mean,\n               \'regularization_op\': reg,\n               \'word_emb\': word_emb_1n,\n               \'lstm_outputs1\': lstm_outputs1,\n               \'lstm_outputs2\': lstm_outputs2, }\n\n    return ret\n'"
deeppavlov/models/elmo/train_utils.py,33,"b'# originally based on https://github.com/allenai/bilm-tf/blob/master/bilm/training.py\n\n# Modifications copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\n\nimport h5py\nimport tensorflow as tf\n\nfrom deeppavlov.models.elmo.bilm_model import LanguageModel\n\ntf.logging.set_verbosity(tf.logging.INFO)\n\n\ndef average_gradients(tower_grads, batch_size, options):\n    # calculate average gradient for each shared variable across all GPUs\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        # We need to average the gradients across each GPU.\n\n        g0, v0 = grad_and_vars[0]\n\n        if g0 is None:\n            # no gradient for this variable, skip it\n            average_grads.append((g0, v0))\n            continue\n\n        if isinstance(g0, tf.IndexedSlices):\n            # If the gradient is type IndexedSlices then this is a sparse\n            #   gradient with attributes indices and values.\n            # To average, need to concat them individually then create\n            #   a new IndexedSlices object.\n            indices = []\n            values = []\n            for g, v in grad_and_vars:\n                indices.append(g.indices)\n                values.append(g.values)\n            all_indices = tf.concat(indices, 0)\n            avg_values = tf.concat(values, 0) / len(grad_and_vars)\n            # deduplicate across indices\n            av, ai = _deduplicate_indexed_slices(avg_values, all_indices)\n            grad = tf.IndexedSlices(av, ai, dense_shape=g0.dense_shape)\n\n        else:\n            # a normal tensor can just do a simple average\n            grads = []\n            for g, v in grad_and_vars:\n                # Add 0 dimension to the gradients to represent the tower.\n                expanded_g = tf.expand_dims(g, 0)\n                # Append on a \'tower\' dimension which we will average over\n                grads.append(expanded_g)\n\n            # Average over the \'tower\' dimension.\n            grad = tf.concat(grads, 0)\n            grad = tf.reduce_mean(grad, 0)\n\n        # the Variables are redundant because they are shared\n        # across towers. So.. just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n\n        average_grads.append(grad_and_var)\n\n    assert len(average_grads) == len(list(zip(*tower_grads)))\n\n    return average_grads\n\n\ndef summary_gradient_updates(grads, opt, lr):\n    """"""get summary ops for the magnitude of gradient updates""""""\n\n    # strategy:\n    # make a dict of variable name -> [variable, grad, adagrad slot]\n    vars_grads = {}\n    for v in tf.trainable_variables():\n        vars_grads[v.name] = [v, None, None]\n    for g, v in grads:\n        vars_grads[v.name][1] = g\n        vars_grads[v.name][2] = opt.get_slot(v, \'accumulator\')\n\n    # now make summaries\n    ret = []\n    for vname, (v, g, a) in vars_grads.items():\n\n        if g is None:\n            continue\n\n        if isinstance(g, tf.IndexedSlices):\n            # a sparse gradient - only take norm of params that are updated\n            updates = lr * g.values\n            if a is not None:\n                updates /= tf.sqrt(tf.gather(a, g.indices))\n        else:\n            updates = lr * g\n            if a is not None:\n                updates /= tf.sqrt(a)\n\n        values_norm = tf.sqrt(tf.reduce_sum(v * v)) + 1.0e-7\n        updates_norm = tf.sqrt(tf.reduce_sum(updates * updates))\n        ret.append(tf.summary.scalar(\'UPDATE/\' + vname.replace("":"", ""_""), updates_norm / values_norm))\n\n    return ret\n\n\ndef _deduplicate_indexed_slices(values, indices):\n    """"""Sums `values` associated with any non-unique `indices`.\n    Args:\n      values: A `Tensor` with rank >= 1.\n      indices: A one-dimensional integer `Tensor`, indexing into the first\n      dimension of `values` (as in an IndexedSlices object).\n    Returns:\n      A tuple of (`summed_values`, `unique_indices`) where `unique_indices` is a\n      de-duplicated version of `indices` and `summed_values` contains the sum of\n      `values` slices associated with each unique index.\n    """"""\n    unique_indices, new_index_positions = tf.unique(indices)\n    summed_values = tf.unsorted_segment_sum(values,\n                                            new_index_positions,\n                                            tf.shape(unique_indices)[0])\n    return (summed_values, unique_indices)\n\n\ndef clip_by_global_norm_summary(t_list, clip_norm, norm_name, variables):\n    # wrapper around tf.clip_by_global_norm that also does summary ops of norms\n\n    # compute norms\n    # use global_norm with one element to handle IndexedSlices vs dense\n    norms = [tf.global_norm([t]) for t in t_list]\n\n    # summary ops before clipping\n    summary_ops = []\n    for ns, v in zip(norms, variables):\n        name = \'norm_pre_clip/\' + v.name.replace("":"", ""_"")\n        summary_ops.append(tf.summary.scalar(name, ns))\n\n    # clip\n    clipped_t_list, tf_norm = tf.clip_by_global_norm(t_list, clip_norm)\n\n    # summary ops after clipping\n    norms_post = [tf.global_norm([t]) for t in clipped_t_list]\n    for ns, v in zip(norms_post, variables):\n        name = \'norm_post_clip/\' + v.name.replace("":"", ""_"")\n        summary_ops.append(tf.summary.scalar(name, ns))\n\n    summary_ops.append(tf.summary.scalar(norm_name, tf_norm))\n\n    return clipped_t_list, tf_norm, summary_ops\n\n\ndef clip_grads(grads, options, do_summaries, global_step):\n    # grads = [(grad1, var1), (grad2, var2), ...]\n    def _clip_norms(grad_and_vars, val, name):\n        # grad_and_vars is a list of (g, v) pairs\n        grad_tensors = [g for g, v in grad_and_vars]\n        vv = [v for g, v in grad_and_vars]\n        scaled_val = val\n        if do_summaries:\n            clipped_tensors, g_norm, so = clip_by_global_norm_summary(\n                grad_tensors, scaled_val, name, vv)\n        else:\n            so = []\n            clipped_tensors, g_norm = tf.clip_by_global_norm(\n                grad_tensors, scaled_val)\n\n        ret = []\n        for t, (g, v) in zip(clipped_tensors, grad_and_vars):\n            ret.append((t, v))\n\n        return ret, so\n\n    all_clip_norm_val = options[\'all_clip_norm_val\']\n    ret, summary_ops = _clip_norms(grads, all_clip_norm_val, \'norm_grad\')\n\n    assert len(ret) == len(grads)\n\n    return ret, summary_ops\n\n\ndef safely_str2int(in_str: str):\n    try:\n        i = int(in_str)\n    except ValueError:\n        i = None\n    return i\n\n\ndef dump_weights(tf_save_dir, outfile, options):\n    """"""\n    Dump the trained weights from a model to a HDF5 file.\n    """"""\n\n    def _get_outname(tf_name):\n        outname = re.sub(\':0$\', \'\', tf_name)\n        outname = outname.lstrip(\'lm/\')\n        outname = re.sub(\'/rnn/\', \'/RNN/\', outname)\n        outname = re.sub(\'/multi_rnn_cell/\', \'/MultiRNNCell/\', outname)\n        outname = re.sub(\'/cell_\', \'/Cell\', outname)\n        outname = re.sub(\'/lstm_cell/\', \'/LSTMCell/\', outname)\n        if \'/RNN/\' in outname:\n            if \'projection\' in outname:\n                outname = re.sub(\'projection/kernel\', \'W_P_0\', outname)\n            else:\n                outname = re.sub(\'/kernel\', \'/W_0\', outname)\n                outname = re.sub(\'/bias\', \'/B\', outname)\n        return outname\n\n    ckpt_file = tf.train.latest_checkpoint(tf_save_dir)\n\n    config = tf.ConfigProto(allow_soft_placement=True)\n    with tf.Graph().as_default():\n        with tf.Session(config=config) as sess:\n            with tf.variable_scope(\'lm\'):\n                LanguageModel(options, False)  # Create graph\n                # we use the ""Saver"" class to load the variables\n                loader = tf.train.Saver()\n                loader.restore(sess, ckpt_file)\n\n            with h5py.File(outfile, \'w\') as fout:\n                for v in tf.trainable_variables():\n                    if v.name.find(\'softmax\') >= 0:\n                        # don\'t dump these\n                        continue\n                    outname = _get_outname(v.name)\n                    # print(""Saving variable {0} with name {1}"".format(\n                    #     v.name, outname))\n                    shape = v.get_shape().as_list()\n                    dset = fout.create_dataset(outname, shape, dtype=\'float32\')\n                    values = sess.run([v])[0]\n                    dset[...] = values\n'"
deeppavlov/models/embedders/__init__.py,0,b''
deeppavlov/models/embedders/abstract_embedder.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import ABCMeta, abstractmethod\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import List, Union, Iterator\n\nimport numpy as np\nfrom overrides import overrides\n\nfrom deeppavlov.core.data.utils import zero_pad\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.core.models.serializable import Serializable\n\nlog = getLogger(__name__)\n\n\nclass Embedder(Component, Serializable, metaclass=ABCMeta):\n    """"""\n    Class implements fastText embedding model\n\n    Args:\n        load_path: path where to load pre-trained embedding model from\n        pad_zero: whether to pad samples or not\n\n    Attributes:\n        model: model instance\n        tok2emb: dictionary with already embedded tokens\n        dim: dimension of embeddings\n        pad_zero: whether to pad sequence of tokens with zeros or not\n        mean: whether to return one mean embedding vector per sample\n        load_path: path with pre-trained fastText binary model\n    """"""\n\n    def __init__(self, load_path: Union[str, Path], pad_zero: bool = False, mean: bool = False, **kwargs) -> None:\n        """"""\n        Initialize embedder with given parameters\n        """"""\n        super().__init__(save_path=None, load_path=load_path)\n        self.tok2emb = {}\n        self.pad_zero = pad_zero\n        self.mean = mean\n        self.dim = None\n        self.model = None\n        self.load()\n\n    @overrides\n    def save(self) -> None:\n        """"""\n        Class does not save loaded model again as it is not trained during usage\n        """"""\n        raise NotImplementedError\n\n    @overrides\n    def __call__(self, batch: List[List[str]], mean: bool = None) -> List[Union[list, np.ndarray]]:\n        """"""\n        Embed sentences from batch\n\n        Args:\n            batch: list of tokenized text samples\n            mean: whether to return mean embedding of tokens per sample\n\n        Returns:\n            embedded batch\n        """"""\n        batch = [self._encode(sample, mean) for sample in batch]\n        if self.pad_zero:\n            batch = zero_pad(batch)\n        return batch\n\n    @abstractmethod\n    def __iter__(self) -> Iterator[str]:\n        """"""\n        Iterate over all words from the model vocabulary\n\n        Returns:\n            iterator\n        """"""\n\n    @abstractmethod\n    def _get_word_vector(self, w: str) -> np.ndarray:\n        """"""\n        Embed a word using ``self.model``\n\n        Args:\n            w: a word\n\n        Returns:\n            embedding vector\n        """"""\n\n    def _encode(self, tokens: List[str], mean: bool) -> Union[List[np.ndarray], np.ndarray]:\n        """"""\n        Embed one text sample\n\n        Args:\n            tokens: tokenized text sample\n            mean: whether to return mean embedding of tokens per sample\n\n        Returns:\n            list of embedded tokens or array of mean values\n        """"""\n        embedded_tokens = []\n        for t in tokens:\n            try:\n                emb = self.tok2emb[t]\n            except KeyError:\n                try:\n                    emb = self._get_word_vector(t)\n                except KeyError:\n                    emb = np.zeros(self.dim, dtype=np.float32)\n                self.tok2emb[t] = emb\n            embedded_tokens.append(emb)\n\n        if mean is None:\n            mean = self.mean\n\n        if mean:\n            filtered = [et for et in embedded_tokens if np.any(et)]\n            if filtered:\n                return np.mean(filtered, axis=0)\n            return np.zeros(self.dim, dtype=np.float32)\n\n        return embedded_tokens\n'"
deeppavlov/models/embedders/bow_embedder.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import List\n\nimport numpy as np\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\n\n\n@register(\'bow\')\nclass BoWEmbedder(Component):\n    """"""\n    Performs one-hot encoding of tokens based on a pre-built vocabulary of tokens.\n\n    Parameters:\n        depth: size of output numpy vector.\n        with_counts: flag denotes whether to use binary encoding (with zeros and ones),\n            or to use counts as token representation.\n\n    Example:\n        .. code:: python\n\n            >>> bow = BoWEmbedder(depth=3)\n\n            >>> bow([[0, 1], [1], [])\n            [array([1, 1, 0], dtype=int32),\n             array([0, 1, 0], dtype=int32),\n             array([0, 0, 0], dtype=int32)]\n    """"""\n\n    def __init__(self, depth: int, with_counts: bool = False, **kwargs) -> None:\n        self.depth = depth\n        self.with_counts = with_counts\n\n    def _encode(self, token_indices: List[int]) -> np.ndarray:\n        bow = np.zeros([self.depth], dtype=np.int32)\n        for idx in token_indices:\n            if self.with_counts:\n                bow[idx] += 1\n            else:\n                bow[idx] = 1\n        return bow\n\n    def __call__(self, batch: List[List[int]]) -> List[np.ndarray]:\n        return [self._encode(sample) for sample in batch]\n'"
deeppavlov/models/embedders/elmo_embedder.py,7,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\nfrom logging import getLogger\nfrom typing import Iterator, List, Union, Optional\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom overrides import overrides\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.utils import zero_pad, chunk_generator\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.core.models.tf_backend import TfModelMeta\n\nlog = getLogger(__name__)\n\n\n@register(\'elmo_embedder\')\nclass ELMoEmbedder(Component, metaclass=TfModelMeta):\n    """"""\n    ``ELMo`` (Embeddings from Language Models) representations are pre-trained contextual representations from\n    large-scale bidirectional language models. See a paper `Deep contextualized word representations\n    <https://arxiv.org/abs/1802.05365>`__ for more information about the algorithm and a detailed analysis.\n\n    Parameters:\n        spec: A ``ModuleSpec`` defining the Module to instantiate or a path where to load a ``ModuleSpec`` from via\n            ``tenserflow_hub.load_module_spec`` by using `TensorFlow Hub <https://www.tensorflow.org/hub/overview>`__.\n        elmo_output_names: A list of output ELMo. You can use combination of\n            ``[""word_emb"", ""lstm_outputs1"", ""lstm_outputs2"",""elmo""]`` and you can use separately ``[""default""]``.\n\n            Where,\n\n            * ``word_emb`` - CNN embedding (default dim 512)\n            * ``lstm_outputs*`` - ouputs of lstm (default dim 1024)\n            * ``elmo`` - weighted sum of cnn and lstm outputs (default dim 1024)\n            * ``default`` - mean ``elmo`` vector for sentence (default dim 1024)\n\n            See `TensorFlow Hub <https://www.tensorflow.org/hub/modules/google/elmo/2>`__ for more information about it.\n        dim: Can be used for output embeddings dimensionality reduction if elmo_output_names != [\'default\']\n        pad_zero: Whether to use pad samples or not.\n        concat_last_axis: A boolean that enables/disables last axis concatenation. It is not used for\n            ``elmo_output_names = [""default""]``.\n        max_token: The number limitation of words per a batch line.\n        mini_batch_size: It is used to reduce the memory requirements of the device.\n\n\n    If some required packages are missing, install all the requirements by running in command line:\n\n    .. code:: bash\n\n        python -m deeppavlov install <path_to_config>\n\n    where ``<path_to_config>`` is a path to one of the :config:`provided config files <elmo_embedder>`\n    or its name without an extension, for example :\n\n    .. code:: bash\n\n        python -m deeppavlov install elmo_ru-news\n        \n    Examples:\n        >>> from deeppavlov.models.embedders.elmo_embedder import ELMoEmbedder\n        >>> elmo = ELMoEmbedder(""http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz"")\n        >>> elmo([[\'\xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\', \'\xd0\xb6\xd0\xb8\xd0\xb7\xd0\xbd\xd0\xb8\', \'\xd0\x92\xd1\x81\xd0\xb5\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xbd\xd0\xbe\xd0\xb9\', \'\xd0\xb8\', \'\xd0\xb2\xd0\xbe\xd0\xbe\xd0\xb1\xd1\x89\xd0\xb5\', \'\xd0\xb2\xd1\x81\xd0\xb5\xd0\xb3\xd0\xbe\'], [\'42\']])\n        array([[ 0.00719104,  0.08544601, -0.07179783, ...,  0.10879009,\n                -0.18630421, -0.2189409 ],\n               [ 0.16325025, -0.04736076,  0.12354863, ..., -0.1889013 ,\n                 0.04972512,  0.83029324]], dtype=float32)\n\n        You can use ELMo models from DeepPavlov as usual `TensorFlow Hub Module\n        <https://www.tensorflow.org/hub/modules/google/elmo/2>`_.\n\n        >>> import tensorflow as tf\n        >>> import tensorflow_hub as hub\n        >>> elmo = hub.Module(""http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-news_wmt11-16_1.5M_steps.tar.gz"",\n        trainable=True)\n        >>> sess = tf.Session()\n        >>> sess.run(tf.global_variables_initializer())\n        >>> embeddings = elmo([""\xd1\x8d\xd1\x82\xd0\xbe \xd0\xbf\xd1\x80\xd0\xb5\xd0\xb4\xd0\xbb\xd0\xbe\xd0\xb6\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5"", ""word""], signature=""default"", as_dict=True)[""elmo""]\n        >>> sess.run(embeddings)\n        array([[[ 0.05817392,  0.22493343, -0.19202903, ..., -0.14448944,\n                 -0.12425567,  1.0148407 ],\n                [ 0.53596294,  0.2868537 ,  0.28028542, ..., -0.08028372,\n                  0.49089077,  0.75939953]],\n               [[ 0.3433637 ,  1.0031182 , -0.1597258 , ...,  1.2442509 ,\n                  0.61029315,  0.43388373],\n                [ 0.05370751,  0.02260921,  0.01074906, ...,  0.08748816,\n                 -0.0066415 , -0.01344293]]], dtype=float32)\n\n        TensorFlow Hub module also supports tokenized sentences in the following format.\n\n        >>> tokens_input = [[""\xd0\xbc\xd0\xb0\xd0\xbc\xd0\xb0"", ""\xd0\xbc\xd1\x8b\xd0\xbb\xd0\xb0"", ""\xd1\x80\xd0\xb0\xd0\xbc\xd1\x83""], [""\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb0"", """", """"]]\n        >>> tokens_length = [3, 1]\n        >>> embeddings = elmo(\n                inputs={\n                        ""tokens"": tokens_input,\n                        ""sequence_len"": tokens_length\n                        },\n                signature=""tokens"",\n                as_dict=True)[""elmo""]\n        >>> sess.run(embeddings)\n        array([[[ 0.6040001 , -0.16130011,  0.56478846, ..., -0.00376141,\n                 -0.03820051,  0.26321286],\n                [ 0.01834148,  0.17055789,  0.5311495 , ..., -0.5675535 ,\n                  0.62669843, -0.05939034],\n                [ 0.3242596 ,  0.17909613,  0.01657108, ...,  0.1866098 ,\n                  0.7392496 ,  0.08285746]],\n               [[ 1.1322289 ,  0.19077688, -0.17811403, ...,  0.42973226,\n                  0.23391506, -0.01294377],\n                [ 0.05370751,  0.02260921,  0.01074906, ...,  0.08748816,\n                 -0.0066415 , -0.01344293],\n                [ 0.05370751,  0.02260921,  0.01074906, ...,  0.08748816,\n                 -0.0066415 , -0.01344293]]], dtype=float32)\n\n        You can also get ``hub.text_embedding_column`` like described `here\n        <https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub#feature_columns>`_.\n\n\n    """"""\n\n    def __init__(self, spec: str, elmo_output_names: Optional[List] = None,\n                 dim: Optional[int] = None, pad_zero: bool = False,\n                 concat_last_axis: bool = True, max_token: Optional[int] = None,\n                 mini_batch_size: int = 32, **kwargs) -> None:\n\n        self.spec = spec if \'://\' in spec else str(expand_path(spec))\n\n        self.elmo_output_dims = {\'word_emb\': 512,\n                                 \'lstm_outputs1\': 1024,\n                                 \'lstm_outputs2\': 1024,\n                                 \'elmo\': 1024,\n                                 \'default\': 1024}\n        elmo_output_names = elmo_output_names or [\'default\']\n        self.elmo_output_names = elmo_output_names\n        elmo_output_names_set = set(self.elmo_output_names)\n        if elmo_output_names_set - set(self.elmo_output_dims.keys()):\n            log.error(f\'Incorrect elmo_output_names = {elmo_output_names} . You can use either  [""default""] or some of\'\n                      \'[""word_emb"", ""lstm_outputs1"", ""lstm_outputs2"",""elmo""]\')\n            sys.exit(1)\n\n        if elmo_output_names_set - {\'default\'} and elmo_output_names_set - {""word_emb"", ""lstm_outputs1"",\n                                                                            ""lstm_outputs2"", ""elmo""}:\n            log.error(\'Incompatible conditions: you can use either  [""default""] or list of \'\n                      \'[""word_emb"", ""lstm_outputs1"", ""lstm_outputs2"",""elmo""] \')\n            sys.exit(1)\n\n        self.pad_zero = pad_zero\n        self.concat_last_axis = concat_last_axis\n        self.max_token = max_token\n        self.mini_batch_size = mini_batch_size\n        self.elmo_outputs, self.sess, self.tokens_ph, self.tokens_length_ph = self._load()\n        self.dim = self._get_dims(self.elmo_output_names, dim, concat_last_axis)\n\n    def _get_dims(self, elmo_output_names, in_dim, concat_last_axis):\n        dims = [self.elmo_output_dims[elmo_output_name] for elmo_output_name in elmo_output_names]\n        if concat_last_axis:\n            dims = in_dim if in_dim else sum(dims)\n        else:\n            if in_dim:\n                log.warning(f""[ dim = {in_dim} is not used, because the elmo_output_names has more than one element.]"")\n        return dims\n\n    def _load(self):\n        """"""\n        Load a ELMo TensorFlow Hub Module from a self.spec.\n\n        Returns:\n            ELMo pre-trained model wrapped in TenserFlow Hub Module.\n        """"""\n        elmo_module = hub.Module(self.spec, trainable=False)\n\n        sess_config = tf.ConfigProto()\n        sess_config.gpu_options.allow_growth = True\n        sess = tf.Session(config=sess_config)\n\n        tokens_ph = tf.placeholder(shape=(None, None), dtype=tf.string, name=\'tokens\')\n        tokens_length_ph = tf.placeholder(shape=(None,), dtype=tf.int32, name=\'tokens_length\')\n\n        elmo_outputs = elmo_module(inputs={""tokens"": tokens_ph,\n                                           ""sequence_len"": tokens_length_ph},\n                                   signature=""tokens"",\n                                   as_dict=True)\n\n        sess.run(tf.global_variables_initializer())\n\n        return elmo_outputs, sess, tokens_ph, tokens_length_ph\n\n    def _fill_batch(self, batch):\n        """"""\n        Fill batch correct values.\n\n        Args:\n            batch: A list of tokenized text samples.\n\n        Returns:\n            batch: A list of tokenized text samples.\n        """"""\n\n        if not batch:\n            empty_vec = np.zeros(self.dim, dtype=np.float32)\n            return [empty_vec] if \'default\' in self.elmo_output_names else [[empty_vec]]\n\n        filled_batch = []\n        for batch_line in batch:\n            batch_line = batch_line if batch_line else [\'\']\n            filled_batch.append(batch_line)\n\n        batch = filled_batch\n\n        if self.max_token:\n            batch = [batch_line[:self.max_token] for batch_line in batch]\n        tokens_length = [len(batch_line) for batch_line in batch]\n        tokens_length_max = max(tokens_length)\n        batch = [batch_line + [\'\'] * (tokens_length_max - len(batch_line)) for batch_line in batch]\n\n        return batch, tokens_length\n\n    def _mini_batch_fit(self, batch: List[List[str]], *args, **kwargs) -> Union[List[np.ndarray], np.ndarray]:\n        """"""\n        Embed sentences from a batch.\n\n        Args:\n            batch: A list of tokenized text samples.\n\n        Returns:\n            A batch of ELMo embeddings.\n        """"""\n        batch, tokens_length = self._fill_batch(batch)\n\n        elmo_outputs = self.sess.run(self.elmo_outputs,\n                                     feed_dict={self.tokens_ph: batch,\n                                                self.tokens_length_ph: tokens_length})\n\n        if \'default\' in self.elmo_output_names:\n            elmo_output_values = elmo_outputs[\'default\']\n            dim0, dim1 = elmo_output_values.shape\n            if self.dim != dim1:\n                shape = (dim0, self.dim if isinstance(self.dim, int) else self.dim[0])\n                elmo_output_values = np.resize(elmo_output_values, shape)\n        else:\n            elmo_output_values = [elmo_outputs[elmo_output_name] for elmo_output_name in self.elmo_output_names]\n            elmo_output_values = np.concatenate(elmo_output_values, axis=-1)\n\n            dim0, dim1, dim2 = elmo_output_values.shape\n            if self.concat_last_axis and self.dim != dim2:\n                shape = (dim0, dim1, self.dim)\n                elmo_output_values = np.resize(elmo_output_values, shape)\n\n            elmo_output_values = [elmo_output_values_line[:length_line]\n                                  for length_line, elmo_output_values_line in zip(tokens_length, elmo_output_values)]\n\n            if not self.concat_last_axis:\n                slice_indexes = np.cumsum(self.dim).tolist()[:-1]\n                elmo_output_values = [[np.array_split(vec, slice_indexes) for vec in tokens]\n                                      for tokens in elmo_output_values]\n\n        return elmo_output_values\n\n    @overrides\n    def __call__(self, batch: List[List[str]],\n                 *args, **kwargs) -> Union[List[np.ndarray], np.ndarray]:\n        """"""\n        Embed sentences from a batch.\n\n        Args:\n            batch: A list of tokenized text samples.\n\n        Returns:\n            A batch of ELMo embeddings.\n        """"""\n        if len(batch) > self.mini_batch_size:\n            batch_gen = chunk_generator(batch, self.mini_batch_size)\n            elmo_output_values = []\n            for mini_batch in batch_gen:\n                mini_batch_out = self._mini_batch_fit(mini_batch, *args, **kwargs)\n                elmo_output_values.extend(mini_batch_out)\n        else:\n            elmo_output_values = self._mini_batch_fit(batch, *args, **kwargs)\n\n        if self.pad_zero:\n            elmo_output_values = zero_pad(elmo_output_values)\n\n        return elmo_output_values\n\n    def __iter__(self) -> Iterator:\n        """"""\n        Iterate over all words from a ELMo model vocabulary.\n        The ELMo model vocabulary consists of ``[\'<S>\', \'</S>\', \'<UNK>\']``.\n\n        Returns:\n            An iterator of three elements ``[\'<S>\', \'</S>\', \'<UNK>\']``.\n        """"""\n\n        yield from [\'<S>\', \'</S>\', \'<UNK>\']\n\n    def destroy(self):\n        if hasattr(self, \'sess\'):\n            for k in list(self.sess.graph.get_all_collection_keys()):\n                self.sess.graph.clear_collection(k)\n        super().destroy()\n'"
deeppavlov/models/embedders/fasttext_embedder.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import Iterator\n\nimport fasttext\n\nimport numpy as np\nfrom overrides import overrides\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.models.embedders.abstract_embedder import Embedder\n\nlog = getLogger(__name__)\n\n\n@register(\'fasttext\')\nclass FasttextEmbedder(Embedder):\n    """"""\n    Class implements fastText embedding model\n\n    Args:\n        load_path: path where to load pre-trained embedding model from\n        pad_zero: whether to pad samples or not\n\n    Attributes:\n        model: fastText model instance\n        tok2emb: dictionary with already embedded tokens\n        dim: dimension of embeddings\n        pad_zero: whether to pad sequence of tokens with zeros or not\n        load_path: path with pre-trained fastText binary model\n    """"""\n\n    def _get_word_vector(self, w: str) -> np.ndarray:\n        return self.model.get_word_vector(w)\n\n    def load(self) -> None:\n        """"""\n        Load fastText binary model from self.load_path\n        """"""\n        log.info(f""[loading fastText embeddings from `{self.load_path}`]"")\n        self.model = fasttext.load_model(str(self.load_path))\n        self.dim = self.model.get_dimension()\n\n    @overrides\n    def __iter__(self) -> Iterator[str]:\n        """"""\n        Iterate over all words from fastText model vocabulary\n\n        Returns:\n            iterator\n        """"""\n        yield from self.model.get_words()\n'"
deeppavlov/models/embedders/glove_embedder.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport pickle\nfrom logging import getLogger\nfrom typing import Iterator\n\nimport numpy as np\nfrom gensim.models import KeyedVectors\nfrom overrides import overrides\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.models.embedders.abstract_embedder import Embedder\n\nlog = getLogger(__name__)\n\n\n@register(\'glove\')\nclass GloVeEmbedder(Embedder):\n    """"""\n    Class implements GloVe embedding model\n\n    Args:\n        load_path: path where to load pre-trained embedding model from\n        pad_zero: whether to pad samples or not\n\n    Attributes:\n        model: GloVe model instance\n        tok2emb: dictionary with already embedded tokens\n        dim: dimension of embeddings\n        pad_zero: whether to pad sequence of tokens with zeros or not\n        load_path: path with pre-trained GloVe model\n    """"""\n\n    def _get_word_vector(self, w: str) -> np.ndarray:\n        return self.model[w]\n\n    def load(self) -> None:\n        """"""\n        Load dict of embeddings from given file\n        """"""\n        log.info(f""[loading GloVe embeddings from `{self.load_path}`]"")\n        if not self.load_path.exists():\n            log.warning(f\'{self.load_path} does not exist, cannot load embeddings from it!\')\n            return\n        self.model = KeyedVectors.load_word2vec_format(str(self.load_path))\n        self.dim = self.model.vector_size\n\n    @overrides\n    def __iter__(self) -> Iterator[str]:\n        """"""\n        Iterate over all words from GloVe model vocabulary\n\n        Returns:\n            iterator\n        """"""\n        yield from self.model.vocab\n\n    def serialize(self) -> bytes:\n        return pickle.dumps(self.model, protocol=4)\n\n    def deserialize(self, data: bytes) -> None:\n        self.model = pickle.loads(data)\n        self.dim = self.model.vector_size\n'"
deeppavlov/models/embedders/tfidf_weighted_embedder.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import List, Union, Optional, Tuple\n\nimport numpy as np\nfrom overrides import overrides\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.errors import ConfigError\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.utils import zero_pad\nfrom deeppavlov.core.models.component import Component\n\nlog = getLogger(__name__)\n\n\n@register(\'tfidf_weighted\')\nclass TfidfWeightedEmbedder(Component):\n    """"""\n    The class implements the functionality of embedding the sentence \\\n        as a weighted average by special coefficients of tokens embeddings. \\\n        Coefficients can be taken from the given TFIDF-vectorizer in ``vectorizer`` or \\\n        calculated as TFIDF from counter vocabulary given in ``counter_vocab_path``.\n        Also one can give ``tags_vocab_path`` to the vocabulary with weights of tags. \\\n        In this case, batch with tags should be given as a second input in ``__call__`` method.\n\n    Args:\n        embedder: embedder instance\n        tokenizer: tokenizer instance, should be able to detokenize sentence\n        pad_zero: whether to pad samples or not\n        mean: whether to return mean token embedding\n        tags_vocab_path: optional path to vocabulary with tags weights\n        vectorizer: vectorizer instance should be trained with ``analyzer=""word""``\n        counter_vocab_path: path to counter vocabulary\n        idf_base_count: minimal idf value (less time occured are not counted)\n        log_base: logarithm base for TFIDF-coefficient calculation froom counter vocabulary\n        min_idf_weight: minimal idf weight\n\n    Attributes:\n        embedder: embedder instance\n        tokenizer: tokenizer instance, should be able to detokenize sentence\n        dim: dimension of embeddings\n        pad_zero: whether to pad samples or not\n        mean: whether to return mean token embedding\n        tags_vocab: vocabulary with weigths for tags\n        vectorizer: vectorizer instance\n        counter_vocab_path: path to counter vocabulary\n        counter_vocab: counter vocabulary\n        idf_base_count: minimal idf value (less time occured are not counted)\n        log_base: logarithm base for TFIDF-coefficient calculation froom counter vocabulary\n        min_idf_weight: minimal idf weight\n\n    Examples:\n        >>> from deeppavlov.models.embedders.tfidf_weighted_embedder import TfidfWeightedEmbedder\n        >>> from deeppavlov.models.embedders.fasttext_embedder import FasttextEmbedder\n        >>> fasttext_embedder = FasttextEmbedder(\'/data/embeddings/wiki.ru.bin\')\n        >>> fastTextTfidf = TfidfWeightedEmbedder(embedder=fasttext_embedder,\n                counter_vocab_path=\'/data/vocabs/counts_wiki_lenta.txt\')\n        >>> fastTextTfidf([[\'\xd0\xb1\xd0\xbe\xd0\xbb\xd1\x8c\xd1\x88\xd0\xbe\xd0\xb9\', \'\xd0\xb8\', \'\xd1\x80\xd0\xbe\xd0\xb7\xd0\xbe\xd0\xb2\xd1\x8b\xd0\xb9\', \'\xd0\xb1\xd0\xb5\xd0\xb3\xd0\xb5\xd0\xbc\xd0\xbe\xd1\x82\']])\n        [array([ 1.99135890e-01, -7.14746421e-02,  8.01428872e-02, -5.32840924e-02,\n                 5.05212297e-02,  2.76053832e-01, -2.53270134e-01, -9.34443950e-02,\n                 ...\n                 1.18385439e-02,  1.05643446e-01, -1.21904516e-03,  7.70555378e-02])]\n    """"""\n\n    def __init__(self,\n                 embedder: Component,\n                 tokenizer: Component = None,\n                 pad_zero: bool = False,\n                 mean: bool = False,\n                 tags_vocab_path: str = None,\n                 vectorizer: Component = None,\n                 counter_vocab_path: str = None,\n                 idf_base_count: int = 100,\n                 log_base: int = 10,\n                 min_idf_weight=0.0, **kwargs) -> None:\n        self.embedder = embedder\n        self.dim = self.embedder.dim\n        self.mean = mean\n        self.pad_zero = pad_zero\n        self.tokenizer = tokenizer or self.space_detokenizer\n        self.vectorizer = vectorizer\n\n        if vectorizer and counter_vocab_path:\n            raise ConfigError(""TfidfWeightedEmbedder got vectorizer and counter_vocab_path simultaneously.""\n                              "" Remove one of them, please"")\n        elif vectorizer:\n            self.vectorizer = vectorizer\n            self.vocabulary = np.array(self.vectorizer.model.get_feature_names())\n        elif counter_vocab_path:\n            self.counter_vocab_path = expand_path(counter_vocab_path)\n            self.counter_vocab, self.min_count = self.load_counter_vocab(self.counter_vocab_path)\n            self.idf_base_count = idf_base_count\n            self.log_base = log_base\n            self.min_idf_weight = min_idf_weight\n        else:\n            raise ConfigError(""TfidfWeightedEmbedder did not get vectorizer or counter_vocab_path.""\n                              "" Set one of them, please"")\n\n        if tags_vocab_path:\n            self.tags_vocab = self.load_tags_vocab(expand_path(tags_vocab_path))\n        else:\n            self.tags_vocab = None\n\n    @staticmethod\n    def load_tags_vocab(load_path: str) -> dict:\n        """"""\n        Load tag vocabulary from the given path, each key of the vocabulary is a tag, \\\n            and the corresponding value of the item is a coefficient of words with such tags to be multiplied for.\n\n        Args:\n            load_path: path to the vocabulary to be load from\n\n        Returns:\n            vocabulary\n        """"""\n        tags_vocab = dict()\n        with open(load_path, \'r\') as f:\n            lines = f.readlines()\n            f.close()\n\n        for line in lines:\n            key, val = line[:-1].split(\' \')  # ""\\t""\n            tags_vocab[key] = val\n\n        return tags_vocab\n\n    @staticmethod\n    def load_counter_vocab(load_path: str) -> Tuple[dict, int]:\n        """"""\n        Load counter vocabulary from the given path\n\n        Args:\n            load_path: path to the vocabulary to be load from\n\n        Returns:\n            vocabulary\n        """"""\n        counter_vocab = dict()\n        with open(load_path, \'r\') as f:\n            lines = f.readlines()\n            f.close()\n\n        min_val = np.inf\n        for line in lines:\n            key, val = line[:-1].split(\'\\t\')\n            val = int(val)\n            counter_vocab[key] = val\n            if val < min_val:\n                min_val = val\n\n        return counter_vocab, min_val\n\n    @staticmethod\n    def space_detokenizer(batch: List[List[str]]) -> List[str]:\n        """"""\n        Detokenizer by default. Linking tokens by space symbol\n\n        Args:\n            batch: batch of tokenized texts\n\n        Returns:\n            batch of detokenized texts\n        """"""\n        return ["" "".join(tokens) for tokens in batch]\n\n    @overrides\n    def __call__(self, batch: List[List[str]], tags_batch: Optional[List[List[str]]] = None, mean: bool = None,\n                 *args, **kwargs) -> List[Union[list, np.ndarray]]:\n        """"""\n        Infer on the given data\n\n        Args:\n            batch: tokenized text samples\n            tags_batch: optional batch of corresponding tags\n            mean: whether to return mean token embedding (does not depend on self.mean)\n            *args: additional arguments\n            **kwargs: additional arguments\n\n        Returns:\n\n        """"""\n\n        if self.tags_vocab:\n            if tags_batch is None:\n                raise ConfigError(""TfidfWeightedEmbedder got \'tags_vocab_path\' but __call__ did not get tags_batch."")\n            batch = [self._tags_encode(sample, tags_sample, mean=mean) for sample, tags_sample in\n                     zip(batch, tags_batch)]\n        else:\n            if tags_batch:\n                raise ConfigError(""TfidfWeightedEmbedder got tags batch, but \'tags_vocab_path\' is empty."")\n            batch = [self._encode(sample, mean=mean) for sample in batch]\n\n        if self.pad_zero:\n            batch = zero_pad(batch)\n\n        return batch\n\n    def _encode(self, tokens: List[str], mean: bool) -> Union[List[np.ndarray], np.ndarray]:\n        """"""\n        Embed one text sample\n\n        Args:\n            tokens: tokenized text sample\n            mean: whether to return mean token embedding (does not depend on self.mean)\n\n        Returns:\n            list of embedded tokens or array of mean values\n        """"""\n        if self.vectorizer:\n            detokenized_sample = self.tokenizer([tokens])[0]  # str\n            vectorized_sample = self.vectorizer([detokenized_sample])  # (voc_size,)\n\n            weights = np.array([vectorized_sample[0, np.where(self.vocabulary == token)[0][0]]\n                                if len(np.where(self.vocabulary == token)[0]) else 0.\n                                for token in tokens])\n        else:\n            weights = np.array([self.get_weight(max(self.counter_vocab.get(token, 0), self.idf_base_count))\n                                for token in tokens])\n\n        if sum(weights) == 0:\n            weights = np.ones(len(tokens))\n\n        embedded_tokens = np.array(self.embedder([tokens]))[0, :, :]\n\n        if mean is None:\n            mean = self.mean\n\n        if mean:\n            embedded_tokens = np.average(embedded_tokens, weights=weights, axis=0)\n        else:\n            embedded_tokens = np.array([weights[i] * embedded_tokens[i] for i in range(len(tokens))])\n\n        return embedded_tokens\n\n    def get_weight(self, count: int) -> float:\n        """"""\n        Calculate the weight corresponding to the given count\n\n        Args:\n            count: the number of occurences of particular token\n\n        Returns:\n            weight\n        """"""\n        log_count = np.log(count) / np.log(self.log_base)\n        log_base_count = np.log(self.idf_base_count) / np.log(self.log_base)\n        weight = max(1.0 / (1.0 + log_count - log_base_count), self.min_idf_weight)\n        return weight\n\n    def _tags_encode(self, tokens: List[str], tags: List[str], mean: bool) -> Union[List[np.ndarray], np.ndarray]:\n        """"""\n        Embed one text sample\n\n        Args:\n            tokens: tokenized text sample\n            tags: tokenized tags sample\n            mean: whether to return mean token embedding (does not depend on self.mean)\n\n        Returns:\n            list of embedded tokens or array of mean values\n        """"""\n\n        embedded_tokens = np.array(self.embedder([tokens]))[0, :, :]\n\n        tags_weights = np.array([self.tags_vocab.get(tag, 1.0) for tag in tags])\n\n        detokenized_sample = self.tokenizer([tokens])[0]  # str\n        vectorized_sample = self.vectorizer([detokenized_sample])  # (voc_size,)\n\n        if self.vectorizer:\n            weights = np.array([vectorized_sample[0, np.where(self.vocabulary == token)[0][0]]\n                                if len(np.where(self.vocabulary == token)[0]) else 0.\n                                for token in tokens])\n        else:\n            weights = np.array([self.get_weight(max(self.counter_vocab.get(token, 0), self.idf_base_count))\n                                for token in tokens])\n\n        weights = np.multiply(weights, tags_weights)\n        if sum(weights) == 0:\n            weights = np.ones(len(tokens))\n\n        if mean is None:\n            mean = self.mean\n\n        if mean:\n            embedded_tokens = np.average(embedded_tokens, weights=weights, axis=0)\n        else:\n            embedded_tokens = np.array([weights[i] * embedded_tokens[i] for i in range(len(tokens))])\n\n        return embedded_tokens\n'"
deeppavlov/models/embedders/transformers_embedder.py,0,"b'# Copyright 2020 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom pathlib import Path\nfrom typing import Union, Tuple, Collection\n\nimport torch\nimport transformers\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.serializable import Serializable\n\n\n@register(\'transformers_bert_embedder\')\nclass TransformersBertEmbedder(Serializable):\n    """"""Transformers-based BERT model for embeddings tokens, subtokens and sentences\n\n    Args:\n        load_path: path to a pretrained BERT pytorch checkpoint\n        bert_config_file: path to a BERT configuration file\n        truncate: whether to remove zero-paddings from returned data\n\n    """"""\n    model: transformers.BertModel\n    dim: int\n\n    def __init__(self, load_path: Union[str, Path], bert_config_path: Union[str, Path] = None,\n                 truncate: bool = False, **kwargs):\n        super().__init__(save_path=None, load_path=load_path, **kwargs)\n        if bert_config_path is not None:\n            bert_config_path = expand_path(bert_config_path)\n        self.config = bert_config_path\n        self.truncate = truncate\n        self.device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n        self.load()\n\n    def save(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def load(self):\n        self.model = transformers.BertModel.from_pretrained(self.load_path, config=self.config).eval().to(self.device)\n        self.dim = self.model.config.hidden_size\n\n    def __call__(self, subtoken_ids_batch: Collection[Collection[int]], startofwords_batch: Collection[Collection[int]],\n                 attention_batch: Collection[Collection[int]]) -> Tuple[Collection[Collection[Collection[float]]],\n                                                                        Collection[Collection[Collection[float]]],\n                                                                        Collection[Collection[float]],\n                                                                        Collection[Collection[float]],\n                                                                        Collection[Collection[float]]]:\n        """"""Predict embeddings values for a given batch\n\n        Args:\n            subtoken_ids_batch: padded indexes for every subtoken\n            startofwords_batch: a mask matrix with ``1`` for every first subtoken init in a token and ``0``\n                for every other subtoken\n            attention_batch: a mask matrix with ``1`` for every significant subtoken and ``0`` for paddings\n        """"""\n        ids_tensor = torch.tensor(subtoken_ids_batch, device=self.device)\n        startofwords_tensor = torch.tensor(startofwords_batch, device=self.device).bool()\n        attention_tensor = torch.tensor(attention_batch, device=self.device)\n        with torch.no_grad():\n            last_hidden, pooler_output = self.model(ids_tensor, attention_tensor)\n            attention_tensor = attention_tensor.unsqueeze(-1)\n            max_emb = torch.max(last_hidden - 1e9 * (1 - attention_tensor), dim=1)[0]\n            subword_emb = last_hidden * attention_tensor\n            mean_emb = torch.sum(subword_emb, dim=1) / torch.sum(attention_tensor, dim=1)\n\n            tokens_lengths = startofwords_tensor.sum(dim=1)\n            word_emb = torch.zeros((subword_emb.shape[0], tokens_lengths.max(), subword_emb.shape[2]),\n                                   device=self.device, dtype=subword_emb.dtype)\n            target_indexes = (torch.arange(word_emb.shape[1], device=self.device).expand(word_emb.shape[:-1]) <\n                              tokens_lengths.unsqueeze(-1))\n            word_emb[target_indexes] = subword_emb[startofwords_tensor]\n\n        subword_emb = subword_emb.cpu().numpy()\n        word_emb = word_emb.cpu().numpy()\n        pooler_output = pooler_output.cpu().numpy()\n        max_emb = max_emb.cpu().numpy()\n        mean_emb = mean_emb.cpu().numpy()\n        if self.truncate:\n            subword_emb = [item[:mask.sum()] for item, mask in zip(subword_emb, attention_batch)]\n            word_emb = [item[:mask.sum()] for item, mask in zip(word_emb, startofwords_batch)]\n        return word_emb, subword_emb, max_emb, mean_emb, pooler_output\n'"
deeppavlov/models/evolution/__init__.py,0,b''
deeppavlov/models/evolution/evolution_param_generator.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom copy import deepcopy\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import List, Any\n\nimport numpy as np\n\nfrom deeppavlov.core.common.params_search import ParamsSearch\nfrom deeppavlov.core.common.registry import register\n\nlog = getLogger(__name__)\n\n\n@register(\'params_evolution\')\nclass ParamsEvolution(ParamsSearch):\n    """"""\n    Class performs full evolutionary process (task scores -> max):\n    1. initializes random population\n    2. makes replacement to get next generation:\n        a. selection according to obtained scores\n        b. crossover (recombination) with given probability p_crossover\n        c. mutation with given mutation rate p_mutation (probability to mutate)\n            according to given mutation power sigma\n            (current mutation power is randomly from -sigma to sigma)\n\n    Args:\n        population_size: number of individuums per generation\n        p_crossover: probability to cross over for current replacement\n        crossover_power: part of EVOLVING parents parameters to exchange for offsprings\n        p_mutation: probability of mutation for current replacement\n        mutation_power: allowed percentage of mutation\n        key_model_to_evolve: binary flag that should be inserted into the dictionary\n                    with main model in the basic config (to determine save and load paths that will be changed)\n        seed: random seed for initialization\n        train_partition: integer number of train data parts\n        elitism_with_weights: whether to save elite models with weigths or without\n        prefix: prefix to determine special keys like `PREFIX_range`, `PREFIX_bool`, `PREFIX_choice`\n        **kwargs: basic config with parameters\n\n    Attributes:\n        basic_config: dictionary with initial evolutionary config\n        main_model_path: list of keys and/or integers (for list) with relative path to main model (subdictionary)\n        prefix: prefix to determine special keys like `PREFIX_range`, `PREFIX_bool`, `PREFIX_choice`\n        population_size: number of individuums per generation\n        p_crossover: probability to cross over for current replacement\n        p_mutation: probability of mutation for current replacement\n        mutation_power: allowed percentage of mutation\n        crossover_power: part of EVOLVING parents parameters to exchange for offsprings\n        elitism_with_weights: whether to save elite models with weigths or without\n        n_saved_best_pretrained: number of saved models per current generation\n        train_partition: integer number of train data parts\n        paths_to_params: list of lists of keys and/or integers (for list)\n                with relative paths to evolving parameters\n        n_params: number of evolving parameters\n        evolution_model_id: identity number of model (the same for loaded pre-trained models)\n        models_path: path to models given in config variable `MODEL_PATH`. This variable \\\n            should be used as prefix to all fitted and trained model in config~\n        eps: EPS value\n        paths_to_fiton_dicts: list of lists of keys and/or integers (for list)\\\n                with relative paths to dictionaries that can be ""fitted on""\n        n_fiton_dicts: number of dictionaries that can be ""fitted on""\n        evolve_metric_optimization: whether to maximize or minimize considered metric \\\n                Set of Values: ``""maximize"", ""minimize""``\n    """"""\n\n    def __init__(self,\n                 population_size: int,\n                 p_crossover: float = 0.5, crossover_power: float = 0.5,\n                 p_mutation: float = 0.5, mutation_power: float = 0.1,\n                 key_main_model: str = ""main"",\n                 seed: int = None,\n                 train_partition: int = 1,\n                 elitism_with_weights: bool = False,\n                 prefix: str = ""evolve"",\n                 models_path_variable: str = ""MODEL_PATH"",\n                 **kwargs):\n        """"""\n        Initialize evolution with random population\n        """"""\n        super().__init__(prefix=prefix, seed=seed, **kwargs)\n\n        self.main_model_path = list(self.find_model_path(self.basic_config, key_main_model))[0]\n        log.info(""Main model path in config: {}"".format(self.main_model_path))\n\n        self.population_size = population_size\n        self.p_crossover = p_crossover\n        self.p_mutation = p_mutation\n        self.mutation_power = mutation_power\n        self.crossover_power = crossover_power\n        self.elitism_with_weights = elitism_with_weights\n\n        self.n_saved_best_pretrained = 0\n        self.train_partition = train_partition\n        self.evolution_model_id = 0\n        self.basic_config, self.models_path = self.remove_key_from_config(\n            self.basic_config, [""metadata"", ""variables"", models_path_variable])\n        self.models_path = Path(self.models_path)\n        for path_name in [""save_path"", ""load_path""]:\n            occured_mpaths = list(self.find_model_path(self.basic_config, path_name))\n            for ppath in occured_mpaths:\n                new_path = self.get_value_from_config(\n                    self.basic_config,\n                    ppath + [path_name]).replace(models_path_variable, ""MODEL_"" + path_name.upper())\n                self.insert_value_or_dict_into_config(self.basic_config, ppath + [path_name], new_path)\n\n        self.path_to_models_save_path = [""metadata"", ""variables"", ""MODEL_SAVE_PATH""]\n        self.path_to_models_load_path = [""metadata"", ""variables"", ""MODEL_LOAD_PATH""]\n        self.insert_value_or_dict_into_config(self.basic_config, self.path_to_models_save_path, str(self.models_path))\n        self.insert_value_or_dict_into_config(self.basic_config, self.path_to_models_load_path, str(self.models_path))\n\n        try:\n            self.evolve_metric_optimization = self.get_value_from_config(\n                self.basic_config, list(self.find_model_path(\n                    self.basic_config, ""metric_optimization""))[0] + [""metric_optimization""])\n        except:\n            self.evolve_metric_optimization = ""maximize""\n\n    def first_generation(self, iteration: int = 0) -> List[dict]:\n        """"""\n        Initialize first generation randomly according to the given constraints is self.params\n\n        Args:\n            iteration: number of iteration\n\n        Returns:\n            first generation that consists of self.population_size individuums\n        """"""\n        population = []\n        for i in range(self.population_size):\n            config = self.initialize_params_in_config(self.basic_config, self.paths_to_params)\n\n            self.insert_value_or_dict_into_config(config, self.path_to_models_save_path,\n                                                  str(self.models_path / f""population_{iteration}"" / f""model_{i}""))\n            self.insert_value_or_dict_into_config(config, self.path_to_models_load_path,\n                                                  str(self.models_path / f""population_{iteration}"" / f""model_{i}""))\n            # set model_id\n            config[""evolution_model_id""] = self.evolution_model_id\n            # next id available\n            self.evolution_model_id += 1\n            population.append(config)\n\n        return population\n\n    def next_generation(self, generation: List[dict], scores: List[float], iteration: int) -> List[dict]:\n        """"""\n        Provide replacement\n\n        Args:\n            generation: current generation (set of self.population_size configs\n            scores: corresponding scores that should be maximized\n            iteration: iteration number\n\n        Returns:\n            the next generation according to the given scores of current generation\n        """"""\n\n        next_population = self.selection_of_best_with_weights(generation, scores)\n        log.info(""Saved with weights: {} models"".format(self.n_saved_best_pretrained))\n        offsprings = self.crossover(generation, scores)\n\n        changable_next = self.mutation(offsprings)\n\n        next_population.extend(changable_next)\n\n        for i in range(self.n_saved_best_pretrained):\n            # if several train files:\n            if self.train_partition != 1:\n                file_ext = str(Path(next_population[i][""dataset_reader""][""train""]).suffix)\n                next_population[i][""dataset_reader""][""train""] = ""_"".join(\n                    Path(next_population[i][""dataset_reader""][""train""]).stem.split(""_"")[:-1]\n                ) + ""_"" + str(iteration % self.train_partition) + file_ext\n            # load_paths\n            if self.elitism_with_weights:\n                # if elite models are saved with weights\n                self.insert_value_or_dict_into_config(\n                    next_population[i], self.path_to_models_load_path,\n                    self.get_value_from_config(next_population[i], self.path_to_models_save_path))\n            else:\n                # if elite models are saved only as configurations and trained again\n                self.insert_value_or_dict_into_config(\n                    next_population[i], self.path_to_models_load_path,\n                    str(self.models_path / f""population_{iteration}"" / f""model_{i}""))\n\n            self.insert_value_or_dict_into_config(\n                next_population[i], self.path_to_models_save_path,\n                str(self.models_path / f""population_{iteration}"" / f""model_{i}""))\n\n        for i in range(self.n_saved_best_pretrained, self.population_size):\n            # if several train files\n            if self.train_partition != 1:\n                file_ext = str(Path(next_population[i][""dataset_reader""][""train""]).suffix)\n                next_population[i][""dataset_reader""][""train""] = ""_"".join(\n                    [str(p) for p in Path(next_population[i][""dataset_reader""][""train""]).stem.split(""_"")[:-1]]) \\\n                                                                + ""_"" + str(iteration % self.train_partition) + file_ext\n            self.insert_value_or_dict_into_config(\n                next_population[i], self.path_to_models_save_path,\n                str(self.models_path / f""population_{iteration}"" / f""model_{i}""))\n            self.insert_value_or_dict_into_config(\n                next_population[i], self.path_to_models_load_path,\n                str(self.models_path / f""population_{iteration}"" / f""model_{i}""))\n\n            next_population[i][""evolution_model_id""] = self.evolution_model_id\n            self.evolution_model_id += 1\n\n        return next_population\n\n    def selection_of_best_with_weights(self, population: List[dict], scores: List[float]) -> List[dict]:\n        """"""\n        Select individuums to save with weights for the next generation from given population.\n        Range is an order of an individuum within sorted scores (1 range = max-score, self.population_size = min-score)\n        Individuum with the best score has probability equal to 1 (100%).\n        Individuum with the worst score has probability equal to 0 (0%).\n        Probability of i-th individuum to be selected with weights is (a * range_i + b)\n        where a = 1. / (1. - self.population_size), and\n        b = self.population_size / (self.population_size - 1.)\n\n        Args:\n            population: self.population_size individuums\n            scores: list of corresponding scores\n\n        Returns:\n            selected self.n_saved_best_pretrained (changable) individuums\n        """"""\n        ranges = self.range_scores(scores)\n        a = 1. / (1. - self.population_size)\n        b = self.population_size / (self.population_size - 1.)\n        probas_to_be_selected = a * ranges + b\n\n        selected = []\n        for i in range(self.population_size):\n            if self.decision(probas_to_be_selected[i]):\n                selected.append(deepcopy(population[i]))\n\n        self.n_saved_best_pretrained = len(selected)\n        return selected\n\n    def range_scores(self, scores: List[float]) -> np.ndarray:\n        """"""\n        Ranges scores,\n        range 1 corresponds to the best score,\n        range self.population_size corresponds to the worst score.\n\n        Args:\n            scores: list of corresponding scores of population\n\n        Returns:\n            ranges\n        """"""\n        not_none_scores = np.array([x for x in scores if x is not None])\n        if len(not_none_scores) == 0:\n            not_none_scores = np.array([0])\n        min_score = np.min(not_none_scores)\n        max_score = np.max(not_none_scores)\n        for i in range(self.population_size):\n            if scores[i] is None:\n                if self.evolve_metric_optimization == ""maximize"":\n                    scores[i] = min_score - self.eps\n                else:\n                    scores[i] = max_score + self.eps\n        scores = np.array(scores, dtype=\'float\')\n\n        sorted_ids = np.argsort(scores)\n        if self.evolve_metric_optimization == ""minimize"":\n            sorted_ids = sorted_ids[::-1]\n        ranges = np.array([self.population_size - np.where(i == sorted_ids)[0][0]\n                           for i in np.arange(self.population_size)])\n        return ranges\n\n    def crossover(self, population: List[dict], scores: List[float]) -> List[dict]:\n        """"""\n        Recombine randomly population in pairs and cross over them with given probability.\n        Cross over from two parents produces two offsprings\n        each of which contains crossover_power portion of the parameter values from one parent,\n         and the other (1 - crossover_power portion) from the other parent\n\n        Args:\n            population: self.population_size individuums\n            scores: list of corresponding scores\n\n        Returns:\n            (self.population_size - self.n_saved_best_pretained) offsprings\n        """"""\n        offsprings = []\n\n        ranges = self.range_scores(scores)\n        a = 1. / (1. - self.population_size)\n        b = self.population_size / (self.population_size - 1.)\n        probas_to_be_parent = (a * ranges + b) / np.sum(a * ranges + b)\n        intervals = np.array([np.sum(probas_to_be_parent[:i]) for i in range(self.population_size)])\n\n        for i in range(self.population_size - self.n_saved_best_pretrained):\n            rs = np.random.random(2)\n            parents = population[np.where(rs[0] > intervals)[0][-1]], population[np.where(rs[1] > intervals)[0][-1]]\n\n            if self.decision(self.p_crossover):\n                params_perm = np.random.permutation(self.n_params)\n\n                curr_offsprings = [deepcopy(parents[0]),\n                                   deepcopy(parents[1])]\n\n                part = int(self.crossover_power * self.n_params)\n\n                for j in range(self.n_params - part, self.n_params):\n                    self.insert_value_or_dict_into_config(curr_offsprings[0],\n                                                          self.paths_to_params[\n                                                              params_perm[j]],\n                                                          self.get_value_from_config(\n                                                              parents[1],\n                                                              self.paths_to_params[\n                                                                  params_perm[j]]))\n\n                    self.insert_value_or_dict_into_config(curr_offsprings[1],\n                                                          self.paths_to_params[\n                                                              params_perm[j]],\n                                                          self.get_value_from_config(\n                                                              parents[0],\n                                                              self.paths_to_params[\n                                                                  params_perm[j]]))\n                offsprings.append(deepcopy(curr_offsprings[0]))\n            else:\n                offsprings.append(deepcopy(parents[0]))\n\n        return offsprings\n\n    def mutation(self, population: List[dict]) -> List[dict]:\n        """"""\n        Mutate each parameter of each individuum in population\n\n        Args:\n            population: self.population_size individuums\n\n        Returns:\n            mutated population\n        """"""\n        mutated = []\n\n        for individuum in population:\n            mutated_individuum = deepcopy(individuum)\n            for path_ in self.paths_to_params:\n                param_value = self.get_value_from_config(individuum, path_)\n                self.insert_value_or_dict_into_config(\n                    mutated_individuum, path_,\n                    self.mutation_of_param(path_, param_value))\n            mutated.append(mutated_individuum)\n\n        return mutated\n\n    def mutation_of_param(self, param_path: list,\n                          param_value: [int, float, str, list, dict, bool, np.ndarray]) -> Any:\n        """"""\n        Mutate particular parameter separately\n\n        Args:\n            param_path: path to parameter in basic config\n            param_value: current parameter valuer\n\n        Returns:\n            mutated parameter value\n        """"""\n        if self.decision(self.p_mutation):\n            param_name = param_path[-1]\n            basic_value = self.get_value_from_config(self.basic_config, param_path)\n            if isinstance(basic_value, dict):\n                if basic_value.get(\'discrete\', False):\n                    val = round(param_value +\n                                ((2 * np.random.random() - 1.) * self.mutation_power\n                                 * self.sample_params(**{param_name: basic_value})[param_name]))\n                    val = min(max(basic_value[""evolve_range""][0], val),\n                              basic_value[""evolve_range""][1])\n                    new_mutated_value = val\n                elif \'evolve_range\' in basic_value.keys():\n                    val = param_value + \\\n                          ((2 * np.random.random() - 1.) * self.mutation_power\n                           * self.sample_params(**{param_name: basic_value})[param_name])\n                    val = min(max(basic_value[""evolve_range""][0], val),\n                              basic_value[""evolve_range""][1])\n                    new_mutated_value = val\n                elif basic_value.get(""evolve_choice""):\n                    new_mutated_value = self.sample_params(**{param_name: basic_value})[param_name]\n                elif basic_value.get(""evolve_bool""):\n                    new_mutated_value = self.sample_params(**{param_name: basic_value})[param_name]\n                else:\n                    new_mutated_value = param_value\n            else:\n                new_mutated_value = param_value\n        else:\n            new_mutated_value = param_value\n\n        return new_mutated_value\n\n    @staticmethod\n    def decision(probability: float = 1.) -> bool:\n        """"""\n        Make decision whether to do action or not with given probability\n\n        Args:\n            probability: probability whether to do action or not\n\n        Returns:\n            bool decision\n        """"""\n        r = np.random.random()\n        if r < probability:\n            return True\n        else:\n            return False\n'"
deeppavlov/models/go_bot/__init__.py,0,b''
deeppavlov/models/go_bot/go_bot.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import Dict, Any, List, Optional, Union, Tuple\n\nimport numpy as np\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.core.models.nn_model import NNModel\nfrom deeppavlov.models.go_bot.nlg.dto.nlg_response_interface import NLGResponseInterface\nfrom deeppavlov.models.go_bot.nlu.dto.text_vectorization_response import TextVectorizationResponse\nfrom deeppavlov.models.go_bot.nlu.tokens_vectorizer import TokensVectorizer\nfrom deeppavlov.models.go_bot.dto.dataset_features import UtteranceDataEntry, DialogueDataEntry, \\\n    BatchDialoguesDataset, UtteranceFeatures, UtteranceTarget, BatchDialoguesFeatures\nfrom deeppavlov.models.go_bot.dto.shared_gobot_params import SharedGoBotParams\nfrom deeppavlov.models.go_bot.nlg.nlg_manager import NLGManagerInterface\nfrom deeppavlov.models.go_bot.nlu.nlu_manager import NLUManager\nfrom deeppavlov.models.go_bot.policy.policy_network import PolicyNetwork, PolicyNetworkParams\nfrom deeppavlov.models.go_bot.policy.dto.policy_prediction import PolicyPrediction\nfrom deeppavlov.models.go_bot.tracker.featurized_tracker import FeaturizedTracker\nfrom deeppavlov.models.go_bot.tracker.dialogue_state_tracker import DialogueStateTracker, MultipleUserStateTrackersPool\nfrom pathlib import Path\n\nlog = getLogger(__name__)\n\n\n# todo logging\n@register(""go_bot"")\nclass GoalOrientedBot(NNModel):\n    """"""\n    The dialogue bot is based on  https://arxiv.org/abs/1702.03274, which\n    introduces Hybrid Code Networks that combine an RNN with domain-specific\n    knowledge and system action templates.\n\n    The network handles dialogue policy management.\n    Inputs features of an utterance and predicts label of a bot action\n    (classification task).\n\n    An LSTM with a dense layer for input features and a dense layer for it\'s output.\n    Softmax is used as an output activation function.\n\n    Todo:\n        add docstring for trackers.\n\n    Parameters:\n        tokenizer: one of tokenizers from\n            :doc:`deeppavlov.models.tokenizers </apiref/models/tokenizers>` module.\n        tracker: dialogue state tracker from\n            :doc:`deeppavlov.models.go_bot.tracker </apiref/models/go_bot>`.\n        hidden_size: size of rnn hidden layer.\n        dropout_rate: probability of weights dropping out.\n        l2_reg_coef: l2 regularization weight (applied to input and output layer).\n        dense_size: rnn input size.\n        attention_mechanism: describes attention applied to embeddings of input tokens.\n\n            * **type** \xe2\x80\x93 type of attention mechanism, possible values are ``\'general\'``, ``\'bahdanau\'``,\n              ``\'light_general\'``, ``\'light_bahdanau\'``, ``\'cs_general\'`` and ``\'cs_bahdanau\'``.\n            * **hidden_size** \xe2\x80\x93 attention hidden state size.\n            * **max_num_tokens** \xe2\x80\x93 maximum number of input tokens.\n            * **depth** \xe2\x80\x93 number of averages used in constrained attentions\n              (``\'cs_bahdanau\'`` or ``\'cs_general\'``).\n            * **action_as_key** \xe2\x80\x93 whether to use action from previous time step as key\n              to attention.\n            * **intent_as_key** \xe2\x80\x93 use utterance intents as attention key or not.\n            * **projected_align** \xe2\x80\x93 whether to use output projection.\n        network_parameters: dictionary with network parameters (for compatibility with release 0.1.1,\n            deprecated in the future)\n\n        word_vocab: vocabulary of input word tokens\n            (:class:`~deeppavlov.core.data.simple_vocab.SimpleVocabulary` recommended).\n        bow_embedder: instance of one-hot word encoder\n            :class:`~deeppavlov.models.embedders.bow_embedder.BoWEmbedder`.\n        embedder: one of embedders from\n            :doc:`deeppavlov.models.embedders </apiref/models/embedders>` module.\n        slot_filler: component that outputs slot values for a given utterance\n            (:class:`~deeppavlov.models.slotfill.slotfill.DstcSlotFillingNetwork`\n            recommended).\n        intent_classifier: component that outputs intents probability\n            distribution for a given utterance (\n            :class:`~deeppavlov.models.classifiers.keras_classification_model.KerasClassificationModel`\n            recommended).\n        database: database that will be used during inference to perform\n            ``api_call_action`` actions and get ``\'db_result\'`` result (\n            :class:`~deeppavlov.core.data.sqlite_database.Sqlite3Database`\n            recommended).\n        use_action_mask: if ``True``, network output will be applied with a mask\n            over allowed actions.\n        debug: whether to display debug output.\n    """"""\n\n    DEFAULT_USER_ID = 1\n    POLICY_DIR_NAME = ""policy""\n\n    def __init__(self,\n                 tokenizer: Component,\n                 tracker: FeaturizedTracker,\n                 nlg_manager: NLGManagerInterface,\n                 save_path: str,\n                 hidden_size: int = 128,\n                 dropout_rate: float = 0.,\n                 l2_reg_coef: float = 0.,\n                 dense_size: int = None,\n                 attention_mechanism: dict = None,\n                 network_parameters: Optional[Dict[str, Any]] = None,\n                 load_path: str = None,\n                 word_vocab: Component = None,\n                 bow_embedder: Component = None,\n                 embedder: Component = None,\n                 slot_filler: Component = None,\n                 intent_classifier: Component = None,\n                 database: Component = None,\n                 use_action_mask: bool = False,\n                 debug: bool = False,\n                 **kwargs) -> None:\n        self.use_action_mask = use_action_mask  # todo not supported actually\n        super().__init__(save_path=save_path, load_path=load_path, **kwargs)\n\n        self.debug = debug\n\n        policy_network_params = PolicyNetworkParams(hidden_size, dropout_rate, l2_reg_coef,\n                                                    dense_size, attention_mechanism, network_parameters)\n\n        self.nlu_manager = NLUManager(tokenizer, slot_filler, intent_classifier)  # todo move to separate pipeline unit\n        self.nlg_manager = nlg_manager\n        self.data_handler = TokensVectorizer(debug, word_vocab, bow_embedder, embedder)\n\n        # todo make mor abstract\n        self.dialogue_state_tracker = DialogueStateTracker.from_gobot_params(tracker, self.nlg_manager,\n                                                                             policy_network_params, database)\n        # todo make mor abstract\n        self.multiple_user_state_tracker = MultipleUserStateTrackersPool(base_tracker=self.dialogue_state_tracker)\n\n        tokens_dims = self.data_handler.get_dims()\n        features_params = SharedGoBotParams.from_configured(self.nlg_manager, self.nlu_manager,\n                                                            self.dialogue_state_tracker)\n        policy_save_path = Path(save_path, self.POLICY_DIR_NAME)\n        policy_load_path = Path(load_path, self.POLICY_DIR_NAME)\n\n        self.policy = PolicyNetwork(policy_network_params, tokens_dims, features_params,\n                                    policy_load_path, policy_save_path, **kwargs)\n\n        self.dialogues_cached_features = dict()\n\n        self.reset()\n\n    def prepare_dialogues_batches_training_data(self,\n                                                batch_dialogues_utterances_contexts_info: List[List[dict]],\n                                                batch_dialogues_utterances_responses_info: List[\n                                                    List[dict]]) -> BatchDialoguesDataset:\n        """"""\n        Parse the passed dialogue information to the dialogue information object.\n\n        Args:\n            batch_dialogues_utterances_contexts_info: the dictionary containing\n                                                      the dialogue utterances training information\n            batch_dialogues_utterances_responses_info: the dictionary containing\n                                                       the dialogue utterances responses training information\n\n        Returns:\n            the dialogue data object containing the numpy-vectorized features and target extracted\n            from the utterance data\n\n        """"""\n        # todo naming, docs, comments\n        max_dialogue_length = max(len(dialogue_info_entry)\n                                  for dialogue_info_entry in batch_dialogues_utterances_contexts_info)  # for padding\n\n        batch_dialogues_dataset = BatchDialoguesDataset(max_dialogue_length)\n        for dialogue_utterances_info in zip(batch_dialogues_utterances_contexts_info,\n                                            batch_dialogues_utterances_responses_info):\n            dialogue_index_value = dialogue_utterances_info[0][0].get(""dialogue_label"")\n\n            if dialogue_index_value and dialogue_index_value in self.dialogues_cached_features.keys():\n                dialogue_training_data = self.dialogues_cached_features[dialogue_index_value]\n            else:\n                dialogue_training_data = self.prepare_dialogue_training_data(*dialogue_utterances_info)\n                if dialogue_index_value:\n                    self.dialogues_cached_features[dialogue_index_value] = dialogue_training_data\n\n            batch_dialogues_dataset.append(dialogue_training_data)\n\n        return batch_dialogues_dataset\n\n    def prepare_dialogue_training_data(self,\n                                       dialogue_utterances_contexts_info: List[dict],\n                                       dialogue_utterances_responses_info: List[dict]) -> DialogueDataEntry:\n        """"""\n        Parse the passed dialogue information to the dialogue information object.\n\n        Args:\n            dialogue_utterances_contexts_info: the dictionary containing the dialogue utterances training information\n            dialogue_utterances_responses_info: the dictionary containing\n                                                the dialogue utterances responses training information\n\n        Returns:\n            the dialogue data object containing the numpy-vectorized features and target extracted\n            from the utterance data\n\n        """"""\n        dialogue_training_data = DialogueDataEntry()\n        # we started to process new dialogue so resetting the dialogue state tracker.\n        # simplification of this logic is planned; there is a todo\n        self.dialogue_state_tracker.reset_state()\n        for context, response in zip(dialogue_utterances_contexts_info, dialogue_utterances_responses_info):\n\n            utterance_training_data = self.prepare_utterance_training_data(context, response)\n            dialogue_training_data.append(utterance_training_data)\n\n            # to correctly track the dialogue state\n            # we inform the tracker with the ground truth response info\n            # just like the tracker remembers the predicted response actions when real-time inference\n            self.dialogue_state_tracker.update_previous_action(utterance_training_data.target.action_id)\n\n            if self.debug:\n                log.debug(f""True response = \'{response[\'text\']}\'."")\n                if utterance_training_data.features.action_mask[utterance_training_data.target.action_id] != 1.:\n                    log.warning(""True action forbidden by action mask."")\n        return dialogue_training_data\n\n    def prepare_utterance_training_data(self,\n                                        utterance_context_info_dict: dict,\n                                        utterance_response_info_dict: dict) -> UtteranceDataEntry:\n        """"""\n        Parse the passed utterance information to the utterance information object.\n\n        Args:\n            utterance_context_info_dict: the dictionary containing the utterance training information\n            utterance_response_info_dict: the dictionary containing the utterance response training information\n\n        Returns:\n            the utterance data object containing the numpy-vectorized features and target extracted\n            from the utterance data\n\n        """"""\n        # todo naming, docs, comments\n        text = utterance_context_info_dict[\'text\']\n\n        # if there already were db lookups in this utterance\n        # we inform the tracker with these lookups info\n        # just like the tracker remembers the db interaction results when real-time inference\n        # todo: not obvious logic\n        self.dialogue_state_tracker.update_ground_truth_db_result_from_context(utterance_context_info_dict)\n\n        utterance_features = self.extract_features_from_utterance_text(text, self.dialogue_state_tracker)\n\n        action_id = self.nlg_manager.get_action_id(utterance_response_info_dict[\'act\'])\n        utterance_target = UtteranceTarget(action_id)\n\n        utterance_data_entry = UtteranceDataEntry.from_features_and_target(utterance_features, utterance_target)\n        return utterance_data_entry\n\n    def extract_features_from_utterance_text(self, text, tracker, keep_tracker_state=False) -> UtteranceFeatures:\n        """"""\n        Extract ML features for the input text and the respective tracker.\n        Features are aggregated from the\n        * NLU;\n        * text BOW-encoding&embedding;\n        * tracker memory.\n\n        Args:\n            text: the text to infer to\n            tracker: the tracker that tracks the dialogue from which the text is taken\n            keep_tracker_state: if True, the tracker state will not be updated during the prediction.\n                                Used to keep tracker\'s state intact when predicting the action\n                                to perform right after the api call action is predicted and performed.\n\n        Returns:\n            the utterance features object containing the numpy-vectorized features extracted from the utterance\n        """"""\n        # todo comments\n\n        nlu_response = self.nlu_manager.nlu(text)\n\n        # region text BOW-encoding and embedding | todo: to nlu\n        # todo move vectorization to NLU\n        tokens_bow_encoded = self.data_handler.bow_encode_tokens(nlu_response.tokens)\n\n        tokens_embeddings_padded = np.array([], dtype=np.float32)\n        tokens_aggregated_embedding = np.array([], dtype=np.float32)\n        if self.policy.has_attn():\n            attn_window_size = self.policy.get_attn_window_size()\n            # todo: this is ugly and caused by complicated nn configuration algorithm\n            attn_config_token_dim = self.policy.get_attn_hyperparams().token_size\n            tokens_embeddings_padded = self.data_handler.calc_tokens_embeddings(attn_window_size,\n                                                                                attn_config_token_dim,\n                                                                                nlu_response.tokens)\n        else:\n            tokens_aggregated_embedding = self.data_handler.calc_tokens_mean_embedding(nlu_response.tokens)\n        nlu_response.set_tokens_vectorized(TextVectorizationResponse(\n            tokens_bow_encoded,\n            tokens_aggregated_embedding,\n            tokens_embeddings_padded))\n        # endregion text BOW-encoding and embedding | todo: to nlu\n\n        if not keep_tracker_state:\n            tracker.update_state(nlu_response)\n\n        tracker_knowledge = tracker.get_current_knowledge()\n\n        digitized_policy_features = self.policy.digitize_features(nlu_response, tracker_knowledge)\n\n        return UtteranceFeatures(nlu_response, tracker_knowledge, digitized_policy_features)\n\n    def _infer(self, user_utterance_text: str, user_tracker: DialogueStateTracker,\n               keep_tracker_state=False) -> Tuple[BatchDialoguesFeatures, PolicyPrediction]:\n        """"""\n        Predict the action to perform in response to given text.\n\n        Args:\n            user_utterance_text: the user input text passed to the system\n            user_tracker: the tracker that tracks the dialogue with the input-provided user\n            keep_tracker_state: if True, the tracker state will not be updated during the prediction.\n                                Used to keep tracker\'s state intact when predicting the action to perform right after\n                                the api call action\n\n        Returns:\n            the features data object containing features fed to the model on inference and the model\'s prediction info\n        """"""\n        utterance_features = self.extract_features_from_utterance_text(user_utterance_text, user_tracker,\n                                                                       keep_tracker_state)\n\n        utterance_data_entry = UtteranceDataEntry.from_features(utterance_features)\n\n        # region pack an utterance to batch to further get features in batched form\n        dialogue_data_entry = DialogueDataEntry()\n        dialogue_data_entry.append(utterance_data_entry)\n        # batch is single dialogue of 1 utterance => dialogue length = 1\n        utterance_batch_data_entry = BatchDialoguesDataset(max_dialogue_length=1)\n        utterance_batch_data_entry.append(dialogue_data_entry)\n        # endregion pack an utterance to batch to further get features in batched form\n        utterance_batch_features = utterance_batch_data_entry.features\n\n        # as for RNNs: output, hidden_state < - RNN(output, hidden_state)\n        hidden_cells_state, hidden_cells_output = user_tracker.network_state[0], user_tracker.network_state[1]\n        policy_prediction = self.policy(utterance_batch_features,\n                                        hidden_cells_state,\n                                        hidden_cells_output,\n                                        prob=True)\n\n        return utterance_batch_features, policy_prediction\n\n    def __call__(self, batch: Union[List[List[dict]], List[str]],\n                 user_ids: Optional[List] = None) -> Union[List[NLGResponseInterface],\n                                                           List[List[NLGResponseInterface]]]:\n        if isinstance(batch[0], list):\n            # batch is a list of *completed* dialogues, infer on them to calculate metrics\n            # user ids are ignored here: the single tracker is used and is reset after each dialogue inference\n            # todo unify tracking: no need to distinguish tracking strategies on dialogues and realtime\n            res = []\n            for dialogue in batch:\n                dialogue: List[dict]\n                res.append(self._calc_inferences_for_dialogue(dialogue))\n        else:\n            # batch is a list of utterances possibly came from different users: real-time inference\n            res = []\n            if not user_ids:\n                user_ids = [self.DEFAULT_USER_ID] * len(batch)\n            for user_id, user_text in zip(user_ids, batch):\n                user_text: str\n                res.append(self._realtime_infer(user_id, user_text))\n\n        return res\n\n    def _realtime_infer(self, user_id, user_text) -> List[NLGResponseInterface]:\n        # realtime inference logic\n        #\n        # we have the pool of trackers, each one tracks the dialogue with its own user\n        # (1 to 1 mapping: each user has his own tracker and vice versa)\n\n        user_tracker = self.multiple_user_state_tracker.get_or_init_tracker(user_id)\n        responses = []\n\n        # todo remove duplication\n\n        # predict the action to perform (e.g. response smth or call the api)\n        utterance_batch_features, policy_prediction = self._infer(user_text, user_tracker)\n        user_tracker.update_previous_action(policy_prediction.predicted_action_ix)\n        user_tracker.network_state = policy_prediction.get_network_state()\n\n        # tracker says we need to say smth to user. we\n        # * calculate the slotfilled state:\n        #   for each slot that is relevant to dialogue we fill this slot value if possible\n        # * generate text for the predicted speech action:\n        #   using the pattern provided for the action;\n        #   the slotfilled state provides info to encapsulate to the pattern\n        tracker_slotfilled_state = user_tracker.fill_current_state_with_db_results()\n        resp = self.nlg_manager.decode_response(utterance_batch_features,\n                                                policy_prediction,\n                                                tracker_slotfilled_state)\n        responses.append(resp)\n\n        if policy_prediction.predicted_action_ix == self.nlg_manager.get_api_call_action_id():\n            # tracker says we need to make an api call.\n            # we 1) perform the api call and 2) predict what to do next\n            user_tracker.make_api_call()\n            utterance_batch_features, policy_prediction = self._infer(user_text, user_tracker,\n                                                                      keep_tracker_state=True)\n            user_tracker.update_previous_action(policy_prediction.predicted_action_ix)\n            user_tracker.network_state = policy_prediction.get_network_state()\n\n            # tracker says we need to say smth to user. we\n            # * calculate the slotfilled state:\n            #   for each slot that is relevant to dialogue we fill this slot value if possible\n            # * generate text for the predicted speech action:\n            #   using the pattern provided for the action;\n            #   the slotfilled state provides info to encapsulate to the pattern\n            tracker_slotfilled_state = user_tracker.fill_current_state_with_db_results()\n            resp = self.nlg_manager.decode_response(utterance_batch_features,\n                                                    policy_prediction,\n                                                    tracker_slotfilled_state)\n            responses.append(resp)\n\n        return responses\n\n    def _calc_inferences_for_dialogue(self, contexts: List[dict]) -> List[NLGResponseInterface]:\n        # infer on each dialogue utterance\n        # e.g. to calculate inference score via comparing the inferred predictions with the ground truth utterance\n        # todo we provide the tracker with both predicted and ground truth response actions info. is this ok?\n        # todo (response to ^) this should be used only on internal evaluations\n        # todo warning.\n        res = []\n        self.dialogue_state_tracker.reset_state()\n        for context in contexts:\n            if context.get(\'prev_resp_act\') is not None:\n                # if there already were responses to user\n                # we inform the tracker with these responses info\n                # just like the tracker remembers the predicted response actions when real-time inference\n                previous_action_id = self.nlg_manager.get_action_id(context[\'prev_resp_act\'])\n                self.dialogue_state_tracker.update_previous_action(previous_action_id)\n\n            # if there already were db lookups\n            # we inform the tracker with these lookups info\n            # just like the tracker remembers the db interaction results when real-time inference\n            self.dialogue_state_tracker.update_ground_truth_db_result_from_context(context)\n\n            utterance_batch_features, policy_prediction = self._infer(context[\'text\'], self.dialogue_state_tracker)\n            self.dialogue_state_tracker.update_previous_action(policy_prediction.predicted_action_ix)  # see above todo\n            self.dialogue_state_tracker.network_state = policy_prediction.get_network_state()\n\n            # todo fix naming: fill_current_state_with_db_results & update_ground_truth_db_result_from_context are alike\n            tracker_slotfilled_state = self.dialogue_state_tracker.fill_current_state_with_db_results()\n            resp = self.nlg_manager.decode_response(utterance_batch_features,\n                                                    policy_prediction,\n                                                    tracker_slotfilled_state)\n            res.append(resp)\n        return res\n\n    def train_on_batch(self,\n                       batch_dialogues_utterances_features: List[List[dict]],\n                       batch_dialogues_utterances_targets: List[List[dict]]) -> dict:\n        batch_dialogues_dataset = self.prepare_dialogues_batches_training_data(batch_dialogues_utterances_features,\n                                                                               batch_dialogues_utterances_targets)\n        return self.policy.train_on_batch(batch_dialogues_dataset.features,\n                                          batch_dialogues_dataset.targets)\n\n    def reset(self, user_id: Union[None, str, int] = None) -> None:\n        # WARNING: this method is confusing. todo\n        # the multiple_user_state_tracker is applicable only to the realtime inference scenario\n        # so the tracker used to calculate metrics on dialogues is never reset by this method\n        # (but that tracker usually is reset before each dialogue inference)\n        self.multiple_user_state_tracker.reset(user_id)\n        if self.debug:\n            log.debug(""Bot reset."")\n\n    def load(self, *args, **kwargs) -> None:\n        self.policy.load()\n        super().load(*args, **kwargs)\n\n    def save(self, *args, **kwargs) -> None:\n        super().save(*args, **kwargs)\n        self.policy.save()\n'"
deeppavlov/models/go_bot/wrapper.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Iterable\n\nfrom deeppavlov.core.models.component import Component\n\n\nclass DialogComponentWrapper(Component):\n\n    def __init__(self, component: Component, **kwargs):\n        self.component = component\n\n    @staticmethod\n    def _get_text(utter):\n        return utter[\'text\']\n\n    def __call__(self, batch):\n        out = []\n        if isinstance(batch[0], Iterable) and not isinstance(batch[0], str):\n            for dialog in batch:\n                res = self.component([self._get_text(utter) for utter in dialog])\n                out.append(res)\n        else:\n            out = self.component(batch)\n        return out\n\n    def fit(self, data):\n        self.component.fit([self._get_text(utter)\n                            for dialog in data for utter in dialog])\n\n    def save(self, *args, **kwargs):\n        self.component.save(*args, **kwargs)\n\n    def load(self, *args, **kwargs):\n        self.component.load(*args, **kwargs)\n'"
deeppavlov/models/kbqa/__init__.py,0,b''
deeppavlov/models/kbqa/entity_detection_parser.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import List\n\nimport numpy as np\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\n\n\n@register(\'entity_detection_parser\')\nclass EntityDetectionParser(Component):\n    """"""This class parses probabilities of tokens to be a token from the entity substring.""""""\n\n    def __init__(self, thres_proba: float = 0.8, **kwargs):\n        self.thres_proba = thres_proba\n\n    def __call__(self, question_tokens: List[List[str]],\n                 token_probas: List[List[List[float]]]) -> List[List[str]]:\n        """"""\n\n        Args:\n            question_tokens: tokenized questions\n            token_probas: list of probabilities of question tokens to belong to\n            ""B-TAG"" (beginning of entity substring), ""I-TAG"" (inner token of entity substring)\n            or ""O-TAG"" (not an entity token)\n        """"""\n        \n        entities_batch = []\n        types_batch = []\n        for tokens, probas in zip(question_tokens, token_probas):\n            tags, tag_probas = self.tags_from_probas(probas)\n            entities, types = self.entities_from_tags(tokens, tags, tag_probas)\n            entities_batch.append(entities)\n            types_batch.append(types)\n        return entities_batch, types_batch\n\n    def tags_from_probas(self, probas):\n        tag_list = [""O-TAG"", ""E-TAG"", ""T-TAG""]\n        tags = []\n        tag_probas = []\n        for proba in probas:\n            tag_num = np.argmax(proba)\n            if tag_num in [1, 2]:\n                if proba[tag_num] < self.thres_proba:\n                    tag_num = 0\n            tags.append(tag_list[tag_num])\n            tag_probas.append(proba[tag_num])\n                    \n        return tags, tag_probas\n\n    def entities_from_tags(self, tokens, tags, tag_probas):\n        entities = []\n        entity_types = []\n        entity = []\n        entity_type = []\n        types_probas = []\n        type_proba = []\n        replace_tokens = [(\' - \', \'-\'), (""\'s"", \'\'), (\' .\', \'\'), (\'{\', \'\'), (\'}\', \'\'), (\'  \', \' \'), (\'""\', ""\'""), (\'(\', \'\'), (\')\', \'\')]\n\n        for tok, tag, proba in zip(tokens, tags, tag_probas):\n            if tag == ""E-TAG"":\n                entity.append(tok)\n            elif tag == ""T-TAG"":\n                entity_type.append(tok)\n                type_proba.append(proba)\n            elif len(entity) > 0:\n                entity = \' \'.join(entity)\n                for old, new in replace_tokens:\n                    entity = entity.replace(old, new)\n                entities.append(entity)\n                entity = []\n            elif len(entity_type) > 0:\n                entity_type = \' \'.join(entity_type)\n                for old, new in replace_tokens:\n                    entity_type = entity_type.replace(old, new)\n                entity_types.append(entity_type)\n                entity_type = []\n                types_probas.append(np.mean(type_proba))\n                type_proba = []\n\n        if entity_types:\n            entity_types = sorted(zip(entity_types, types_probas), key=lambda x: x[1], reverse=True)\n            entity_types = [entity_type[0] for entity_type in entity_types]\n\n        return entities, entity_types\n'"
deeppavlov/models/kbqa/entity_linking.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import List, Dict, Tuple, Optional, Any\n\nimport nltk\nimport pymorphy2\nfrom rapidfuzz import fuzz\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.core.models.serializable import Serializable\nfrom deeppavlov.core.common.file import load_pickle\nfrom deeppavlov.models.spelling_correction.levenshtein.levenshtein_searcher import LevenshteinSearcher\nfrom deeppavlov.models.kbqa.wiki_parser import WikiParser\n\nlog = getLogger(__name__)\n\n\n@register(\'entity_linker\')\nclass EntityLinker(Serializable):\n    """"""\n        This class extracts from Wikidata candidate entities for the entity mentioned in the question and then extracts\n        triplets from Wikidata for the extracted entity. Candidate entities are searched in the dictionary where keys\n        are titles and aliases of Wikidata entities and values are lists of tuples (entity_title, entity_id,\n        number_of_relations). First candidate entities are searched in the dictionary by keys where the keys are\n        entities extracted from the question, if nothing is found entities are searched in the dictionary using\n        Levenstein distance between the entity and keys (titles) in the dictionary.\n    """"""\n\n    def __init__(self, load_path: str,\n                 inverted_index_filename: str,\n                 entities_list_filename: str,\n                 q2name_filename: str,\n                 wiki_parser: WikiParser = None,\n                 use_hdt: bool = False,\n                 lemmatize: bool = False,\n                 use_prefix_tree: bool = False,\n                 **kwargs) -> None:\n        """"""\n\n        Args:\n            load_path: path to folder with wikidata files\n            inverted_index_filename: file with dict of words (keys) and entities containing these words\n            entities_list_filename: file with the list of entities from Wikidata\n            q2name_filename: name of file which maps entity id to name\n            wiki_parser: component deeppavlov.models.kbqa.wiki_parser\n            use_hdt: whether to use hdt file with Wikidata\n            lemmatize: whether to lemmatize tokens of extracted entity\n            use_prefix_tree: whether to use prefix tree for search of entities with typos in entity labels\n            **kwargs:\n        """"""\n        super().__init__(save_path=None, load_path=load_path)\n        self.morph = pymorphy2.MorphAnalyzer()\n        self.lemmatize = lemmatize\n        self.use_prefix_tree = use_prefix_tree\n        self.use_hdt = use_hdt\n\n        self.inverted_index_filename = inverted_index_filename\n        self.entities_list_filename = entities_list_filename\n        self.q2name_filename = q2name_filename\n        self.inverted_index: Optional[Dict[str, List[Tuple[str]]]] = None\n        if self.use_hdt:\n            self.wiki_parser = wiki_parser\n        self.load()\n\n        if self.use_prefix_tree:\n            alphabet = ""!#%\\&\'()+,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyz\xc2\xbd\xc2\xbf\xc3\x81\xc3\x84"" + \\\n                       ""\xc3\x85\xc3\x86\xc3\x87\xc3\x89\xc3\x8e\xc3\x93\xc3\x96\xc3\x97\xc3\x9a\xc3\x9f\xc3\xa0\xc3\xa1\xc3\xa2\xc3\xa3\xc3\xa4\xc3\xa5\xc3\xa6\xc3\xa7\xc3\xa8\xc3\xa9\xc3\xaa\xc3\xab\xc3\xad\xc3\xae\xc3\xaf\xc3\xb0\xc3\xb1\xc3\xb2\xc3\xb3\xc3\xb4\xc3\xb6\xc3\xb8\xc3\xb9\xc3\xba\xc3\xbb\xc3\xbc\xc3\xbd\xc4\x81\xc4\x83\xc4\x85\xc4\x87\xc4\x8c\xc4\x8d\xc4\x90\xc4\x97\xc4\x99\xc4\x9b\xc4\x9e\xc4\x9f\xc4\xa9\xc4\xab\xc4\xb0\xc4\xb1\xc5\x81\xc5\x82\xc5\x84\xc5\x88\xc5\x8c\xc5\x8d\xc5\x91\xc5\x99\xc5\x9a\xc5\x9b\xc5\x9f\xc5\xa0\xc5\xa1\xc5\xa5\xc5\xa9\xc5\xab\xc5\xaf\xc5\xb5\xc5\xba\xc5\xbb\xc5\xbc\xc5\xbd\xc5\xbe\xc6\xa1\xc6\xb0\xc8\x99\xc8\x9a\xc8\x9b\xc9\x99\xca\xbb"" + \\\n                       ""\xca\xbf\xce\xa0\xce\xa1\xce\xb2\xce\xb3\xd0\x91\xd0\x9c\xd0\xb0\xd0\xb2\xd0\xb4\xd0\xb5\xd0\xb6\xd0\xb8\xd0\xba\xd0\xbc\xd0\xbd\xd0\xbe\xd1\x81\xd1\x82\xd1\x8a\xd1\x8f\xe1\xb8\xa4\xe1\xb8\xa5\xe1\xb9\x87\xe1\xb9\xac\xe1\xb9\xad\xe1\xba\xa7\xe1\xba\xbf\xe1\xbb\x9d\xe1\xbb\xa3\xe2\x80\x93\xe2\x80\x98\xe2\x80\x99\xe2\x85\xa2\xe2\x88\x92\xe2\x88\x97""\n            dictionary_words = list(self.inverted_index.keys())\n            self.searcher = LevenshteinSearcher(alphabet, dictionary_words)\n\n    def load(self) -> None:\n        self.inverted_index = load_pickle(self.load_path / self.inverted_index_filename)\n        self.entities_list = load_pickle(self.load_path / self.entities_list_filename)\n        if not self.use_hdt:\n            self.q2name = load_pickle(self.load_path / self.q2name_filename)\n\n    def save(self) -> None:\n        pass\n\n    def __call__(self, entity: str) -> Tuple[List[str], List[float]]:\n        confidences = []\n        if not entity:\n            wiki_entities = [\'None\']\n        else:\n            candidate_entities = self.candidate_entities_inverted_index(entity)\n            candidate_entities, candidate_names = self.candidate_entities_names(entity, candidate_entities)\n            wiki_entities, confidences, srtd_cand_ent = self.sort_found_entities(candidate_entities,\n                                                                                 candidate_names, entity)\n\n        return wiki_entities, confidences\n\n    def candidate_entities_inverted_index(self, entity: str) -> List[Tuple[Any, Any, Any]]:\n        word_tokens = nltk.word_tokenize(entity.lower())\n        candidate_entities = []\n\n        for tok in word_tokens:\n            if len(tok) > 1:\n                found = False\n                if tok in self.inverted_index:\n                    candidate_entities += self.inverted_index[tok]\n                    found = True\n\n                if self.lemmatize:\n                    morph_parse_tok = self.morph.parse(tok)[0]\n                    lemmatized_tok = morph_parse_tok.normal_form\n                    if lemmatized_tok in self.inverted_index:\n                        candidate_entities += self.inverted_index[lemmatized_tok]\n                        found = True\n\n                if not found and self.use_prefix_tree:\n                    words_with_levens_1 = self.searcher.search(tok, d=1)\n                    for word in words_with_levens_1:\n                        candidate_entities += self.inverted_index[word[0]]\n        candidate_entities = list(set(candidate_entities))\n        candidate_entities = [(entity[0], self.entities_list[entity[0]], entity[1]) for entity in candidate_entities]\n\n        return candidate_entities\n\n    def sort_found_entities(self, candidate_entities: List[Tuple[int, str, int]],\n                            candidate_names: List[List[str]],\n                            entity: str) -> Tuple[List[str], List[float], List[Tuple[str, str, int, int]]]:\n        entities_ratios = []\n        for candidate, entity_names in zip(candidate_entities, candidate_names):\n            entity_id = candidate[1]\n            num_rels = candidate[2]\n            entity_name = entity_names[0]\n            fuzz_ratio = max([fuzz.ratio(name, entity) for name in entity_names])\n            entities_ratios.append((entity_name, entity_id, fuzz_ratio, num_rels))\n\n        srtd_with_ratios = sorted(entities_ratios, key=lambda x: (x[2], x[3]), reverse=True)\n        wiki_entities = [ent[1] for ent in srtd_with_ratios]\n        confidences = [float(ent[2]) * 0.01 for ent in srtd_with_ratios]\n\n        return wiki_entities, confidences, srtd_with_ratios\n\n    def candidate_entities_names(self, entity: str,\n          candidate_entities: List[Tuple[int, str, int]]) -> Tuple[List[Tuple[int, str, int]], List[List[str]]]:\n        entity_length = len(entity)\n        candidate_names = []\n        candidate_entities_filter = []\n        for candidate in candidate_entities:\n            entity_num = candidate[0]\n            entity_id = candidate[1]\n            entity_names = []\n            if self.use_hdt:\n                entity_name = self.wiki_parser(""objects"", ""forw"", entity_id, find_label=True)\n                if entity_name != ""Not Found"" and len(entity_name) < 2 * entity_length:\n                    entity_names.append(entity_name)\n                    aliases = self.wiki_parser(""objects"", ""forw"", entity_id, find_alias=True)\n                    for alias in aliases:\n                        entity_names.append(alias)\n                    candidate_names.append(entity_names)\n                    candidate_entities_filter.append(candidate)\n            else:\n                entity_names_found = self.q2name[entity_num]\n                if len(entity_names_found[0]) < 6 * entity_length:\n                    entity_name = entity_names_found[0]\n                    entity_names.append(entity_name)\n                    if len(entity_names_found) > 1:\n                        for alias in entity_names_found[1:]:\n                            entity_names.append(alias)\n                    candidate_names.append(entity_names)\n                    candidate_entities_filter.append(candidate)\n\n        return candidate_entities_filter, candidate_names\n'"
deeppavlov/models/kbqa/kb_answer_parser_base.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom string import punctuation\nfrom typing import List, Tuple, Optional, Dict\n\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.core.models.serializable import Serializable\nfrom deeppavlov.core.common.file import load_pickle\nfrom deeppavlov.models.kbqa.entity_linking import EntityLinker\nfrom deeppavlov.models.kbqa.template_matcher import TemplateMatcher\n\n\nclass KBBase(Component, Serializable):\n    """"""Base class to generate an answer for a given question using Wikidata.""""""\n\n    def __init__(self, load_path: str, wiki_filename: str, linker: EntityLinker,\n                 template_matcher: TemplateMatcher, q2name_filename: str = None,\n                 relations_maping_filename: Optional[str] = None,\n                 *args, **kwargs) -> None:\n\n        """"""\n\n        Args:\n            load_path: path to folder with wikidata files\n            wiki_filename: name of file with Wikidata\n            linker: component `deeppavlov.models.kbqa.entity_linking`\n            template_matcher: component `deeppavlov.models.kbqa.template_matcher`\n            q2name_filename: file with mapping of entity ids to entity titles\n            relations_maping_filename: file with the dictionary of ids(keys) and titles(values) of relations\n            from Wikidata\n            *args:\n            **kwargs:\n        """"""\n\n        super().__init__(save_path=None, load_path=load_path)\n        self._relations_filename = relations_maping_filename\n        self.wiki_filename = wiki_filename\n        self.q2name_filename = q2name_filename\n        self.q_to_name: Optional[Dict[str, Dict[str, str]]] = None\n        self._relations_mapping: Optional[Dict[str, Dict[str, str]]] = None\n        self.linker = linker\n        self.template_matcher = template_matcher\n        self.load()\n\n    def load(self) -> None:\n        self.q_to_name = load_pickle(self.load_path / self.q2name_filename)\n        if self._relations_filename is not None:\n            self._relations_mapping = load_pickle(self.load_path / self._relations_filename)\n        self.wikidata = load_pickle(self.load_path / self.wiki_filename)\n\n    def save(self) -> None:\n        pass\n\n    def is_kbqa_question(self, question_init: str, lang: str) -> bool:\n        is_kbqa = True\n        not_kbqa_question_templates_rus = [""\xd0\xbf\xd0\xbe\xd1\x87\xd0\xb5\xd0\xbc\xd1\x83"", ""\xd0\xba\xd0\xbe\xd0\xb3\xd0\xb4\xd0\xb0 \xd0\xb1\xd1\x83\xd0\xb4\xd0\xb5\xd1\x82"", ""\xd1\x87\xd1\x82\xd0\xbe \xd0\xb1\xd1\x83\xd0\xb4\xd0\xb5\xd1\x82"", ""\xd1\x87\xd1\x82\xd0\xbe \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8"", ""\xd0\xb4\xd0\xbb\xd1\x8f \xd1\x87\xd0\xb5\xd0\xb3\xd0\xbe "", ""\xd0\xba\xd0\xb0\xd0\xba "",\n                                           ""\xd1\x87\xd1\x82\xd0\xbe \xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb0\xd1\x82\xd1\x8c"", ""\xd0\xb7\xd0\xb0\xd1\x87\xd0\xb5\xd0\xbc"", ""\xd1\x87\xd1\x82\xd0\xbe \xd0\xbc\xd0\xbe\xd0\xb6\xd0\xb5\xd1\x82""]\n        not_kbqa_question_templates_eng = [""why"", ""what if"", ""how""]\n        kbqa_question_templates_rus = [""\xd0\xba\xd0\xb0\xd0\xba \xd0\xb7\xd0\xbe\xd0\xb2\xd1\x83\xd1\x82"", ""\xd0\xba\xd0\xb0\xd0\xba \xd0\xbd\xd0\xb0\xd0\xb7\xd1\x8b\xd0\xb2\xd0\xb0\xd0\xb5\xd1\x82\xd1\x81\xd1\x8f"", ""\xd0\xba\xd0\xb0\xd0\xba \xd0\xb7\xd0\xb2\xd0\xb0\xd0\xbb\xd0\xb8"", ""\xd0\xba\xd0\xb0\xd0\xba \xd1\x82\xd1\x8b \xd0\xb4\xd1\x83\xd0\xbc\xd0\xb0\xd0\xb5\xd1\x88\xd1\x8c"", ""\xd0\xba\xd0\xb0\xd0\xba \xd1\x82\xd0\xb2\xd0\xbe\xd0\xb5 \xd0\xbc\xd0\xbd\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5"",\n                                       ""\xd0\xba\xd0\xb0\xd0\xba \xd1\x82\xd1\x8b \xd1\x81\xd1\x87\xd0\xb8\xd1\x82\xd0\xb0\xd0\xb5\xd1\x88\xd1\x8c""]\n\n        question = \'\'.join([ch for ch in question_init if ch not in punctuation]).lower()\n        if lang == ""rus"":\n            is_kbqa = (all(template not in question for template in not_kbqa_question_templates_rus) or\n                       any(template in question for template in kbqa_question_templates_rus))\n        if lang == ""eng"":\n            is_kbqa = all(template not in question for template in not_kbqa_question_templates_eng)\n        return is_kbqa\n\n    def parse_wikidata_object(self,\n                              objects_batch: List[str],\n                              confidences_batch: List[float]) -> Tuple[List[str], List[float]]:\n        parsed_objects = []\n        for n, obj in enumerate(objects_batch):\n            if len(obj) > 0:\n                if obj.startswith(\'Q\'):\n                    if obj in self.q_to_name:\n                        parsed_object = self.q_to_name[obj][""name""]\n                        parsed_objects.append(parsed_object)\n                    else:\n                        parsed_objects.append(\'Not Found\')\n                        confidences_batch[n] = 0.0\n                else:\n                    parsed_objects.append(obj)\n            else:\n                parsed_objects.append(\'Not Found\')\n                confidences_batch[n] = 0.0\n        return parsed_objects, confidences_batch\n\n    def match_triplet(self,\n                      entity_triplets: List[List[List[str]]],\n                      entity_linking_confidences: List[float],\n                      relations: List[str],\n                      relation_probs: List[float]) -> Tuple[str, float]:\n        obj = \'\'\n        confidence = 0.0\n        for predicted_relation, rel_prob in zip(relations, relation_probs):\n            for entities, linking_confidence in zip(entity_triplets, entity_linking_confidences):\n                for rel_triplets in entities:\n                    relation_from_wiki = rel_triplets[0]\n                    if predicted_relation == relation_from_wiki:\n                        obj = rel_triplets[1]\n                        confidence = linking_confidence * rel_prob\n                        return obj, confidence\n        return obj, confidence\n'"
deeppavlov/models/kbqa/kb_answer_parser_simple.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import List, Tuple, Optional, Union\n\nimport numpy as np\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.models.kbqa.kb_answer_parser_base import KBBase\n\nlog = getLogger(__name__)\n\n\n@register(\'kb_answer_parser_simple\')\nclass KBAnswerParserSimple(KBBase):\n    """"""\n        This class generates an answer for a given question using Wikidata.\n        It searches for matching triplet from the Wikidata with entity and\n        relation mentioned in the question. It uses results of the Named\n        Entity Recognition component to extract entity mention and Classification\n        component to determine relation which connects extracted entity and the\n        answer entity.\n    """"""\n\n    def __init__(self, top_k_classes: int,\n                 rule_filter_entities: bool = False,\n                 return_confidences: bool = False,\n                 language: str = ""eng"",\n                 relations_maping_filename: Optional[str] = None,\n                 templates_filename: Optional[str] = None,\n                 *args, **kwargs) -> None:\n        """"""\n\n        Args:\n            top_k_classes: number of relations with top k probabilities\n            rule_filter_entities: whether to filter entities with rules\n            language: russian or english\n            relations_maping_filename: file with the dictionary of ids(keys) and titles(values) of relations\n            from Wikidata\n            templates_filename: file with the dictionary of question templates(keys) and relations for these templates\n            (values)\n            *args\n            **kwargs\n        """"""\n        self.top_k_classes = top_k_classes\n        self.rule_filter_entities = rule_filter_entities\n        self.return_confidences = return_confidences\n        self.language = language\n        self._relations_filename = relations_maping_filename\n        self._templates_filename = templates_filename\n        super().__init__(relations_maping_filename=self._relations_filename, *args, **kwargs)\n\n    def __call__(self, questions_batch: List[str],\n                 tokens_batch: List[List[str]],\n                 tags_batch: List[List[int]],\n                 relations_probs_batch: List[List[float]],\n                 relations_labels_batch: List[List[str]],\n                 *args, **kwargs) -> Union[Tuple[List[str], List[float]], List[str]]:\n\n        objects_batch = []\n        confidences_batch = []\n\n        for question, tokens, tags, relations_probs, relations_labels in \\\n                zip(questions_batch, tokens_batch, tags_batch, relations_probs_batch, relations_labels_batch):\n            is_kbqa = self.is_kbqa_question(question, self.language)\n            entity_from_template = []\n            if is_kbqa:\n                if self._templates_filename is not None:\n                    entity_from_template, _, relations_from_template, _, query_type = self.template_matcher(question)\n                if entity_from_template:\n                    relation_from_template = relations_from_template[0][0]\n                    relation_title = self._relations_mapping[relation_from_template][""name""]\n                    log.debug(""entity {}, relation {}"".format(entity_from_template, relation_title))\n                    entity_ids, entity_linking_confidences = self.linker(entity_from_template[0])\n                    log.debug(f""entity_ids {entity_ids[:5]}"")\n                    entity_triplets = self.extract_triplets_from_wiki(entity_ids)\n                    if self.rule_filter_entities and self.language == \'rus\':\n                        entity_ids, entity_triplets, entity_linking_confidences = \\\n                            self.filter_triplets_rus(entity_triplets, entity_linking_confidences, tokens, entity_ids)\n\n                    relation_prob = 1.0\n                    obj, confidence = self.match_triplet(entity_triplets,\n                                                         entity_linking_confidences,\n                                                         [relation_from_template],\n                                                         [relation_prob])\n                else:\n                    entity_from_ner = self.extract_entities(tokens, tags)\n                    entity_ids, entity_linking_confidences = self.linker(entity_from_ner)\n                    entity_triplets = self.extract_triplets_from_wiki(entity_ids)\n                    if self.rule_filter_entities and self.language == \'rus\':\n                        entity_ids, entity_triplets, entity_linking_confidences = \\\n                            self.filter_triplets_rus(entity_triplets, entity_linking_confidences, tokens, entity_ids)\n\n                    top_k_probs = self._parse_relations_probs(relations_probs)\n                    top_k_relation_names = [self._relations_mapping[rel][""name""] for rel in relations_labels]\n                    log.debug(""entity_from_ner {}, top k relations {}"".format(str(entity_from_ner),\n                                                                              str(top_k_relation_names)))\n                    obj, confidence = self.match_triplet(entity_triplets,\n                                                         entity_linking_confidences,\n                                                         relations_labels,\n                                                         top_k_probs)\n                objects_batch.append(obj)\n                confidences_batch.append(confidence)\n            else:\n                objects_batch.append(\'\')\n                confidences_batch.append(0.0)\n\n        parsed_objects_batch, confidences_batch = self.parse_wikidata_object(objects_batch, confidences_batch)\n        \n        if self.return_confidences:\n            return parsed_objects_batch, confidences_batch\n        else:\n            return parsed_objects_batch\n\n    def _parse_relations_probs(self, probs: List[float]) -> List[float]:\n        top_k_inds = np.asarray(probs).argsort()[-self.top_k_classes:][::-1]\n        top_k_probs = [probs[k] for k in top_k_inds]\n        return top_k_probs\n\n    @staticmethod\n    def extract_entities(tokens: List[str], tags: List[int]) -> str:\n        entity = []\n        for j, tok in enumerate(tokens):\n            if tags[j] != \'O\' and tags[j] != 0:\n                entity.append(tok)\n        entity = \' \'.join(entity)\n\n        return entity\n\n    def extract_triplets_from_wiki(self, entity_ids: List[str]) -> List[List[List[str]]]:\n        entity_triplets = []\n        for entity_id in entity_ids:\n            if entity_id in self.wikidata and entity_id.startswith(\'Q\'):\n                triplets_for_entity = self.wikidata[entity_id]\n                entity_triplets.append(triplets_for_entity)\n            else:\n                entity_triplets.append([])\n\n        return entity_triplets\n\n    def filter_triplets_rus(self, entity_triplets: List[List[List[str]]], confidences: List[float],\n                            question_tokens: List[str], srtd_cand_ent: List[Tuple[str]]) -> \\\n                            Tuple[List[Tuple[str]], List[List[List[str]]], List[float]]:\n\n        question = \' \'.join(question_tokens).lower()\n        what_template = \'\xd1\x87\xd1\x82\xd0\xbe \'\n        found_what_template = question.find(what_template) > -1\n        filtered_entity_triplets = []\n        filtered_entities = []\n        filtered_confidences = []\n        for wiki_entity, confidence, triplets_for_entity in zip(srtd_cand_ent, confidences, entity_triplets):\n            entity_is_human = False\n            entity_is_asteroid = False\n            entity_is_named = False\n            entity_title = wiki_entity\n            if entity_title[0].isupper():\n                entity_is_named = True\n            property_is_instance_of = \'P31\'\n            id_for_entity_human = \'Q5\'\n            id_for_entity_asteroid = \'Q3863\'\n            for triplet in triplets_for_entity:\n                if triplet[0] == property_is_instance_of and triplet[1] == id_for_entity_human:\n                    entity_is_human = True\n                    break\n                if triplet[0] == property_is_instance_of and triplet[1] == id_for_entity_asteroid:\n                    entity_is_asteroid = True\n                    break\n            if found_what_template and (entity_is_human or entity_is_named or entity_is_asteroid):\n                continue\n            filtered_entity_triplets.append(triplets_for_entity)\n            filtered_entities.append(wiki_entity)\n            filtered_confidences.append(confidence)\n\n        return filtered_entities, filtered_entity_triplets, filtered_confidences\n'"
deeppavlov/models/kbqa/kb_tree.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import List, Tuple, Optional, Union\n\nimport nltk\nimport numpy as np\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.models.kbqa.tree_parser import TreeParser\nfrom deeppavlov.models.embedders.fasttext_embedder import FasttextEmbedder\nfrom deeppavlov.models.kbqa.kb_answer_parser_base import KBBase\n\nlog = getLogger(__name__)\n\n\n@register(\'kb_tree\')\nclass KBTree(KBBase):\n    """"""\n        This class generates an answer for a given question using Wikidata.\n        It searches for matching triplet from the Wikidata with entity and\n        relation mentioned in the question. It uses templates for entity \n        and relation extraction from the question. If the question does not\n        match with any of templates, syntactic parsing is used for entity\n        and relation extraction.\n    """"""\n\n    def __init__(self, tree_parser: TreeParser,\n                 ft_embedder: FasttextEmbedder,\n                 use_templates: bool = True,\n                 return_confidences: bool = False,\n                 relations_maping_filename: Optional[str] = None,\n                 templates_filename: Optional[str] = None,\n                 language: str = ""rus"",\n                 *args, **kwargs) -> None:\n\n        """"""\n\n        Args:\n            tree_parser: component `deeppavlov.models.kbqa.tree_parser`\n            ft_embedder: component `deeppavlov.models.embedders.fasttext`\n            use_templates: whether to use templates for entity and relation extraction\n            relations_maping_filename: file with the dictionary of ids(keys) and titles(values) of relations\n            from Wikidata\n            templates_filename: file with the dictionary of question templates(keys) and relations for these templates\n            (values)\n            language: russian or english\n            *args:\n            **kwargs:\n        """"""\n\n        self.use_templates = use_templates\n        self.return_confidences = return_confidences\n        self.tree_parser = tree_parser\n        self.ft_embedder = ft_embedder\n        self._relations_filename = relations_maping_filename\n        self._templates_filename = templates_filename\n        self.language = language\n        super().__init__(relations_maping_filename=self._relations_filename, *args, **kwargs)\n\n    def __call__(self, sentences: List[str]) -> Union[Tuple[List[str], List[float]], List[str]]:\n\n        objects_batch = []\n        confidences_batch = []\n        for sentence in sentences:\n            is_kbqa = self.is_kbqa_question(sentence, self.language)\n            if is_kbqa:\n                q_tokens = nltk.word_tokenize(sentence)\n                entity_from_template, _, relations_from_template, _, query_type = self.template_matcher(sentence)\n                if entity_from_template and self.use_templates:\n                    relation_from_template = relations_from_template[0][0]\n                    relation_title = self._relations_mapping[relation_from_template][""name""]\n                    log.debug(""using templates, entity {}, relation {}"".format(entity_from_template,\n                                                                               relation_title))\n                    entity_ids, entity_linking_confidences = self.linker(entity_from_template[0])\n                    entity_triplets = self.extract_triplets_from_wiki(entity_ids)\n                    rel_prob = 1.0\n                    obj, confidence = self.match_triplet(entity_triplets,\n                                                         entity_linking_confidences,\n                                                         [relation_from_template],\n                                                         [rel_prob])\n                    objects_batch.append(obj)\n                    confidences_batch.append(confidence)\n                else:\n                    if q_tokens:\n                        detected_entity, detected_rel = self.tree_parser([q_tokens])[0]\n                        if detected_entity:\n                            log.debug(""using syntactic tree, entity {}, relation {}"".format(detected_entity,\n                                                                                            detected_rel))\n                            entity_ids, entity_linking_confidences = self.linker(detected_entity)\n                            entity_triplets = self.extract_triplets_from_wiki(entity_ids)\n                            obj, confidence = self.match_rel(entity_triplets, entity_linking_confidences,\n                                                             detected_rel, sentence)\n                            objects_batch.append(obj)\n                            confidences_batch.append(confidence)\n                        else:\n                            objects_batch.append(\'\')\n                            confidences_batch.append(0.0)\n                    else:\n                        objects_batch.append(\'\')\n                        confidences_batch.append(0.0)\n            else:\n                objects_batch.append(\'\')\n                confidences_batch.append(0.0)\n\n        parsed_objects_batch, confidences_batch = self.parse_wikidata_object(objects_batch, confidences_batch)\n        \n        if self.return_confidences:\n            return parsed_objects_batch, confidences_batch\n        else:\n            return parsed_objects_batch\n\n    def filter_triplets(self, triplets: List[List[str]],\n                        sentence: str) -> List[List[str]]:\n        where_rels = [""P131"", ""P30"", ""P17"", ""P276"", ""P19"", ""P20"", ""P119""]\n        when_rels = [""P585"", ""P569"", ""P570"", ""P571"", ""P580"", ""P582""]\n        filtered_triplets = []\n        where_templates = [""\xd0\xb3\xd0\xb4\xd0\xb5""]\n        when_templates = [""\xd0\xba\xd0\xbe\xd0\xb3\xd0\xb4\xd0\xb0"", ""\xd0\xb4\xd0\xb0\xd1\x82\xd0\xb0"", ""\xd0\xb4\xd0\xb0\xd1\x82\xd1\x83"", ""\xd0\xb2 \xd0\xba\xd0\xb0\xd0\xba\xd0\xbe\xd0\xbc \xd0\xb3\xd0\xbe\xd0\xb4\xd1\x83"", ""\xd0\xb3\xd0\xbe\xd0\xb4""]\n        fl_where_question = False\n        fl_when_question = False\n        for template in when_templates:\n            if template in sentence.lower():\n                fl_when_question = True\n                break\n        for template in where_templates:\n            if template in sentence.lower():\n                fl_where_question = True\n                break\n\n        if fl_when_question:\n            for triplet in triplets:\n                rel_id = triplet[0]\n                if rel_id in when_rels:\n                    filtered_triplets.append(triplet)\n        elif fl_where_question:\n            for triplet in triplets:\n                rel_id = triplet[0]\n                if rel_id in where_rels:\n                    filtered_triplets.append(triplet)\n        else:\n            filtered_triplets = triplets\n\n        return filtered_triplets\n\n    def match_rel(self,\n                  entity_triplets: List[List[List[str]]],\n                  entity_linking_confidences: List[float],\n                  detected_rel: str,\n                  sentence: str) -> Tuple[str, float]:\n        """"""\n            Method which calculates cosine similarity between average fasttext embedding of\n            tokens of relation extracted from syntactic tree and all relations from wikidata\n            and we find which relation from wikidata has the biggest cosine similarity\n        """"""\n\n        av_detected_emb = self.av_emb(detected_rel)\n        max_score = 0.0\n        found_obj = """"\n        confidence = 0.0\n        entity_triplets_flat = [item for sublist in entity_triplets for item in sublist]\n        filtered_triplets = self.filter_triplets(entity_triplets_flat, sentence)\n        for triplets, linking_confidence in zip(entity_triplets, entity_linking_confidences):\n            for triplet in triplets:\n                scores = []\n                rel_id = triplet[0]\n                obj = triplet[1]\n                if rel_id in self._relations_mapping and triplet in filtered_triplets:\n                    rel_name = self._relations_mapping[rel_id][""name""]\n                    if rel_name == detected_rel:\n                        found_obj = obj\n                        rel_prob = 1.0\n                        confidence = linking_confidence * rel_prob\n                        return found_obj, confidence\n                    else:\n                        name_emb = self.av_emb(rel_name)\n                        scores.append(np.dot(av_detected_emb, name_emb))\n                        if ""aliases"" in self._relations_mapping[rel_id]:\n                            rel_aliases = self._relations_mapping[rel_id][""aliases""]\n                            for alias in rel_aliases:\n                                if alias == detected_rel:\n                                    found_obj = obj\n                                    rel_prob = 1.0\n                                    confidence = linking_confidence * rel_prob\n                                    return found_obj, confidence\n                                else:\n                                    alias_emb = self.av_emb(alias)\n                                    scores.append(np.dot(av_detected_emb, alias_emb))\n                        if np.asarray(scores).mean() > max_score:\n                            max_score = np.asarray(scores).mean()\n                            rel_prob = min(max_score / 10.0, 1.0)\n                            confidence = linking_confidence * rel_prob\n                            found_obj = obj\n\n        return found_obj, confidence\n\n    def av_emb(self, rel: str) -> List[float]:\n        rel_tokens = nltk.word_tokenize(rel)\n        emb = []\n        for tok in rel_tokens:\n            emb.append(self.ft_embedder._get_word_vector(tok))\n        av_emb = np.asarray(emb).mean(axis=0)\n        return av_emb\n\n    def extract_triplets_from_wiki(self, entity_ids: List[str]) -> List[List[List[str]]]:\n        entity_triplets = []\n        for entity_id in entity_ids:\n            if entity_id in self.wikidata and entity_id.startswith(\'Q\'):\n                triplets_for_entity = self.wikidata[entity_id]\n                entity_triplets.append(triplets_for_entity)\n            else:\n                entity_triplets.append([])\n\n        return entity_triplets\n'"
deeppavlov/models/kbqa/query_generator.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport itertools\nimport re\nfrom logging import getLogger\nfrom typing import Tuple, List, Optional, Union, Dict, Any\nfrom collections import namedtuple\n\nimport nltk\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.core.models.serializable import Serializable\nfrom deeppavlov.core.common.file import read_json\nfrom deeppavlov.models.kbqa.template_matcher import TemplateMatcher\nfrom deeppavlov.models.kbqa.entity_linking import EntityLinker\nfrom deeppavlov.models.kbqa.wiki_parser import WikiParser\nfrom deeppavlov.models.kbqa.rel_ranking_infer import RelRankerInfer\nfrom deeppavlov.models.kbqa.rel_ranking_bert_infer import RelRankerBertInfer\nfrom deeppavlov.models.kbqa.utils import \\\n    extract_year, extract_number, order_of_answers_sorting, make_combs, fill_query\n\nlog = getLogger(__name__)\n\n\n@register(\'query_generator\')\nclass QueryGenerator(Component, Serializable):\n    """"""\n        This class takes as input entity substrings, defines the template of the query and\n        fills the slots of the template with candidate entities and relations.\n    """"""\n\n    def __init__(self, template_matcher: TemplateMatcher,\n                 linker_entities: EntityLinker,\n                 linker_types: EntityLinker,\n                 wiki_parser: WikiParser,\n                 rel_ranker: Union[RelRankerInfer, RelRankerBertInfer],\n                 load_path: str,\n                 rank_rels_filename_1: str,\n                 rank_rels_filename_2: str,\n                 sparql_queries_filename: str,\n                 entities_to_leave: int = 5,\n                 rels_to_leave: int = 7,\n                 return_answers: bool = False, **kwargs) -> None:\n        """"""\n\n        Args:\n            template_matcher: component deeppavlov.models.kbqa.template_matcher\n            linker_entities: component deeppavlov.models.kbqa.entity_linking for linking of entities\n            linker_types: component deeppavlov.models.kbqa.entity_linking for linking of types\n            wiki_parser: component deeppavlov.models.kbqa.wiki_parser\n            rel_ranker: component deeppavlov.models.kbqa.rel_ranking_infer\n            load_path: path to folder with wikidata files\n            rank_rels_filename_1: file with list of rels for first rels in questions with ranking \n            rank_rels_filename_2: file with list of rels for second rels in questions with ranking\n            sparql_queries_filename: file with sparql query templates\n            entities_to_leave: how many entities to leave after entity linking\n            rels_to_leave: how many relations to leave after relation ranking\n            sparql_queries_filename: file with a dict of sparql queries\n            return_answers: whether to return answers or candidate answers\n            **kwargs:\n        """"""\n        super().__init__(save_path=None, load_path=load_path)\n        self.template_matcher = template_matcher\n        self.linker_entities = linker_entities\n        self.linker_types = linker_types\n        self.wiki_parser = wiki_parser\n        self.rel_ranker = rel_ranker\n        self.rank_rels_filename_1 = rank_rels_filename_1\n        self.rank_rels_filename_2 = rank_rels_filename_2\n        self.rank_list_0 = []\n        self.rank_list_1 = []\n        self.entities_to_leave = entities_to_leave\n        self.rels_to_leave = rels_to_leave\n        self.sparql_queries_filename = sparql_queries_filename\n        self.return_answers = return_answers\n\n        self.load()\n\n    def load(self) -> None:\n        with open(self.load_path / self.rank_rels_filename_1, \'r\') as fl1:\n            lines = fl1.readlines()\n            self.rank_list_0 = [line.split(\'\\t\')[0] for line in lines]\n\n        with open(self.load_path / self.rank_rels_filename_2, \'r\') as fl2:\n            lines = fl2.readlines()\n            self.rank_list_1 = [line.split(\'\\t\')[0] for line in lines]\n\n        self.template_queries = read_json(self.load_path / self.sparql_queries_filename)\n\n    def save(self) -> None:\n        pass\n\n    def __call__(self, question_batch: List[str],\n                 template_type_batch: List[str],\n                 entities_from_ner_batch: List[List[str]],\n                 types_from_ner_batch: List[List[str]]) -> List[Union[List[Tuple[str, Any]], List[str]]]:\n\n        candidate_outputs_batch = []\n        for question, template_type, entities_from_ner, types_from_ner in \\\n                zip(question_batch, template_type_batch, entities_from_ner_batch, types_from_ner_batch):\n\n            candidate_outputs = []\n            self.template_num = template_type\n\n            replace_tokens = [(\' - \', \'-\'), (\' .\', \'\'), (\'{\', \'\'), (\'}\', \'\'), (\'  \', \' \'), (\'""\', ""\'""), (\'(\', \'\'),\n                              (\')\', \'\'), (\'\xe2\x80\x93\', \'-\')]\n            for old, new in replace_tokens:\n                question = question.replace(old, new)\n\n            entities_from_template, types_from_template, rels_from_template, rel_dirs_from_template, \\\n            query_type_template = self.template_matcher(question)\n            self.template_num = query_type_template\n\n            log.debug(f""question: {question}\\n"")\n            log.debug(f""template_type {self.template_num}"")\n\n            if entities_from_template or types_from_template:\n                entity_ids = self.get_entity_ids(entities_from_template, ""entities"")\n                type_ids = self.get_entity_ids(types_from_template, ""types"")\n                log.debug(f""entities_from_template {entities_from_template}"")\n                log.debug(f""types_from_template {types_from_template}"")\n                log.debug(f""rels_from_template {rels_from_template}"")\n                log.debug(f""entity_ids {entity_ids}"")\n                log.debug(f""type_ids {type_ids}"")\n\n                candidate_outputs = self.find_candidate_answers(question, entity_ids, type_ids, rels_from_template,\n                                                                rel_dirs_from_template)\n\n            if not candidate_outputs and entities_from_ner:\n                log.debug(f""(__call__)entities_from_ner: {entities_from_ner}"")\n                log.debug(f""(__call__)types_from_ner: {types_from_ner}"")\n                entity_ids = self.get_entity_ids(entities_from_ner, ""entities"")\n                type_ids = self.get_entity_ids(types_from_ner, ""types"")\n                log.debug(f""(__call__)entity_ids: {entity_ids}"")\n                log.debug(f""(__call__)type_ids: {type_ids}"")\n                self.template_num = template_type[0]\n                log.debug(f""(__call__)self.template_num: {self.template_num}"")\n                candidate_outputs = self.find_candidate_answers(question, entity_ids[:2], type_ids)\n            candidate_outputs_batch.append(candidate_outputs)\n        if self.return_answers:\n            answers = self.rel_ranker(question_batch, candidate_outputs_batch)\n            log.debug(f""(__call__)answers: {answers}"")\n            return answers\n        else:\n            log.debug(f""(__call__)candidate_outputs_batch: {[output[:5] for output in candidate_outputs_batch]}"")\n            return candidate_outputs_batch\n\n    def get_entity_ids(self, entities: List[str], what_to_link: str) -> List[List[str]]:\n        entity_ids = []\n        for entity in entities:\n            entity_id = []\n            if what_to_link == ""entities"":\n                entity_id, confidences = self.linker_entities(entity)\n            if what_to_link == ""types"":\n                entity_id, confidences = self.linker_types(entity)\n            entity_ids.append(entity_id[:15])\n        return entity_ids\n\n    def find_candidate_answers(self, question: str,\n                               entity_ids: List[List[str]],\n                               type_ids: List[List[str]],\n                               rels_from_template: Optional[List[Tuple[str]]] = None,\n                               rel_dirs_from_template: Optional[List[str]] = None) -> List[Tuple[str]]:\n        candidate_outputs = []\n        log.debug(f""(find_candidate_answers)self.template_num: {self.template_num}"")\n        templates = [template for num, template in self.template_queries.items() if\n                     template[""template_num""] == self.template_num]\n        templates = [template for template in templates if (template[""exact_entity_type_match""] and\n                                                            template[""entities_and_types_num""] == [len(entity_ids),\n                                                                                                   len(type_ids)])\n                     or not template[""exact_entity_type_match""]]\n        if not templates:\n            return candidate_outputs\n        if rels_from_template is not None:\n            query_template = {}\n            for template in templates:\n                if template[""rel_dirs""] == rel_dirs_from_template:\n                    query_template = template\n            if query_template:\n                candidate_outputs = self.query_parser(question, query_template, entity_ids, type_ids,\n                                                      rels_from_template)\n        else:\n            for template in templates:\n                candidate_outputs = self.query_parser(question, template, entity_ids, type_ids, rels_from_template)\n                if candidate_outputs:\n                    return candidate_outputs\n\n            if not candidate_outputs:\n                log.debug(f""(find_candidate_answers)templates: {templates}"")\n                alternative_templates = templates[0][""alternative_templates""]\n                for template in alternative_templates:\n                    candidate_outputs = self.query_parser(question, template, entity_ids, type_ids, rels_from_template)\n                    return candidate_outputs\n\n        log.debug(""candidate_rels_and_answers:\\n"" + \'\\n\'.join([str(output) for output in candidate_outputs[:5]]))\n\n        return candidate_outputs\n\n    def query_parser(self, question: str, query_info: Dict[str, str],\n                     entity_ids: List[List[str]], type_ids: List[List[str]],\n                     rels_from_template: Optional[List[Tuple[str]]] = None) -> List[Tuple[str]]:\n        candidate_outputs = []\n        question_tokens = nltk.word_tokenize(question)\n        query = query_info[""query_template""].lower().replace(""wdt:p31"", ""wdt:P31"")\n        rels_for_search = query_info[""rank_rels""]\n        query_seq_num = query_info[""query_sequence""]\n        return_if_found = query_info[""return_if_found""]\n        log.debug(f""(query_parser)quer: {query}, {rels_for_search}, {query_seq_num}, {return_if_found}"")\n        query_triplets = re.findall(""{[ ]?(.*?)[ ]?}"", query)[0].split(\' . \')\n        log.debug(f""(query_parser)query_triplets: {query_triplets}"")\n        query_triplets = [triplet.split(\' \')[:3] for triplet in query_triplets]\n        query_sequence_dict = {num: triplet for num, triplet in zip(query_seq_num, query_triplets)}\n        query_sequence = []\n        for i in range(1, max(query_seq_num) + 1):\n            query_sequence.append(query_sequence_dict[i])\n        log.debug(f""(query_parser)query_sequence: {query_sequence}"")\n        triplet_info_list = [(""forw"" if triplet[2].startswith(\'?\') else ""backw"", search_source)\n                             for search_source, triplet in zip(rels_for_search, query_triplets) if\n                             search_source != ""do_not_rank""]\n        log.debug(f""(query_parser)rel_directions: {triplet_info_list}"")\n        entity_ids = [entity[:self.entities_to_leave] for entity in entity_ids]\n        entity_combs = make_combs(entity_ids, permut=True)\n        log.debug(f""(query_parser)entity_combs: {entity_combs[:3]}"")\n        type_combs = make_combs(type_ids, permut=False)\n        log.debug(f""(query_parser)type_combs: {type_combs[:3]}"")\n        if rels_from_template is not None:\n            rels = rels_from_template\n        else:\n            rels = [self.find_top_rels(question, entity_ids, triplet_info)\n                    for triplet_info in triplet_info_list]\n        log.debug(f""(query_parser)rels: {rels}"")\n        rels_from_query = [triplet[1] for triplet in query_triplets if triplet[1].startswith(\'?\')]\n        answer_ent = re.findall(""select [\\(]?([\\S]+) "", query)\n        order_info_nt = namedtuple(""order_info"", [""variable"", ""sorting_order""])\n        order_variable = re.findall(""order by (asc|desc)\\((.*)\\)"", query)\n        answers_sorting_order = order_of_answers_sorting(question)\n        if order_variable:\n            order_info = order_info_nt(order_variable[0][1], answers_sorting_order)\n        else:\n            order_info = order_info_nt(None, None)\n        log.debug(f""question, order_info: {question}, {order_info}"")\n        filter_from_query = re.findall(""contains\\((\\?\\w), (.+?)\\)"", query)\n        log.debug(f""(query_parser)filter_from_query: {filter_from_query}"")\n\n        year = extract_year(question_tokens, question)\n        number = extract_number(question_tokens, question)\n        log.debug(f""year {year}, number {number}"")\n        if year:\n            filter_info = [(elem[0], elem[1].replace(""n"", year)) for elem in filter_from_query]\n        elif number:\n            filter_info = [(elem[0], elem[1].replace(""n"", number)) for elem in filter_from_query]\n        else:\n            filter_info = [elem for elem in filter_from_query if elem[1] != ""n""]\n        log.debug(f""(query_parser)filter_from_query: {filter_from_query}"")\n        rel_combs = make_combs(rels, permut=False)\n        import datetime\n        start_time = datetime.datetime.now()\n        for combs in itertools.product(entity_combs, type_combs, rel_combs):\n            query_hdt_seq = [\n                fill_query(query_hdt_elem, combs[0], combs[1], combs[2]) for query_hdt_elem in query_sequence]\n            candidate_output = self.wiki_parser(\n                rels_from_query + answer_ent, query_hdt_seq, filter_info, order_info)\n            candidate_outputs += [combs[2][:-1] + output for output in candidate_output]\n            if return_if_found and candidate_output:\n                return candidate_outputs\n        log.debug(f""(query_parser)loop time: {datetime.datetime.now() - start_time}"")\n        log.debug(f""(query_parser)final outputs: {candidate_outputs[:3]}"")\n\n        return candidate_outputs\n\n    def find_top_rels(self, question: str, entity_ids: List[List[str]], triplet_info: namedtuple) -> List[str]:\n        ex_rels = []\n        direction, source = triplet_info\n        if source == ""wiki"":\n            for entity_id in entity_ids:\n                for entity in entity_id[:self.entities_to_leave]:\n                    ex_rels += self.wiki_parser.find_rels(entity, direction)\n            ex_rels = list(set(ex_rels))\n            ex_rels = [rel.split(\'/\')[-1] for rel in ex_rels]\n        elif source == ""rank_list_1"":\n            ex_rels = self.rank_list_0\n        elif source == ""rank_list_2"":\n            ex_rels = self.rank_list_1\n        scores = self.rel_ranker.rank_rels(question, ex_rels)\n        top_rels = [score[0] for score in scores]\n        return top_rels[:self.rels_to_leave]\n'"
deeppavlov/models/kbqa/rel_ranking_bert_infer.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import Tuple, List, Any\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.core.models.serializable import Serializable\nfrom deeppavlov.core.common.file import load_pickle\nfrom deeppavlov.models.ranking.rel_ranker import RelRanker\nfrom deeppavlov.models.kbqa.wiki_parser import WikiParser\n\nlog = getLogger(__name__)\n\n\n@register(\'rel_ranking_bert_infer\')\nclass RelRankerBertInfer(Component, Serializable):\n    """"""Class for ranking of paths in subgraph""""""\n\n    def __init__(self, load_path: str,\n                 rel_q2name_filename: str,\n                 wiki_parser: WikiParser,\n                 ranker: RelRanker,\n                 batch_size: int = 32,\n                 rels_to_leave: int = 40, **kwargs):\n        """"""\n\n        Args:\n            load_path: path to folder with wikidata files\n            rel_q2name_filename: name of file which maps relation id to name\n            wiki_parser: component deeppavlov.models.wiki_parser\n            ranker: component deeppavlov.models.ranking.rel_ranker\n            batch_size: infering batch size\n            rels_to_leave: how many relations to leave after relation ranking\n            **kwargs:\n        """"""\n        super().__init__(save_path=None, load_path=load_path)\n        self.rel_q2name_filename = rel_q2name_filename\n        self.ranker = ranker\n        self.wiki_parser = wiki_parser\n        self.batch_size = batch_size\n        self.rels_to_leave = rels_to_leave\n        self.load()\n\n    def load(self) -> None:\n        self.rel_q2name = load_pickle(self.load_path / self.rel_q2name_filename)\n\n    def save(self) -> None:\n        pass\n\n    def __call__(self, questions_list: List[str], candidate_answers_list: List[List[Tuple[str]]]) -> List[str]:\n        answers = []\n        for question, candidate_answers in zip(questions_list, candidate_answers_list):\n            answers_with_scores = []\n            answer = ""Not Found""\n\n            n_batches = len(candidate_answers) // self.batch_size + int(len(candidate_answers) % self.batch_size > 0)\n            for i in range(n_batches):\n                questions_batch = []\n                rels_labels_batch = []\n                answers_batch = []\n                for candidate_ans_and_rels in candidate_answers[i * self.batch_size: (i + 1) * self.batch_size]:\n                    candidate_rels = candidate_ans_and_rels[:-1]\n                    candidate_rels = [candidate_rel.split(\'/\')[-1] for candidate_rel in candidate_rels]\n                    candidate_answer = candidate_ans_and_rels[-1]\n                    candidate_rels = "" # "".join([self.rel_q2name[candidate_rel] \\\n                                                 for candidate_rel in candidate_rels if\n                                                 candidate_rel in self.rel_q2name])\n\n                    if candidate_rels:\n                        questions_batch.append(question)\n                        rels_labels_batch.append(candidate_rels)\n                        answers_batch.append(candidate_answer)\n\n                probas = self.ranker(questions_batch, rels_labels_batch)\n                probas = [proba[1] for proba in probas]\n                for j, answer in enumerate(answers_batch):\n                    answers_with_scores.append((answer, probas[j]))\n\n            answers_with_scores = sorted(answers_with_scores, key=lambda x: x[1], reverse=True)\n\n            if answers_with_scores:\n                log.debug(f""answers: {answers_with_scores[0][0]}"")\n                answer = self.wiki_parser.find_label(answers_with_scores[0][0])\n\n            answers.append(answer)\n\n        return answers\n\n    def rank_rels(self, question: str, candidate_rels: List[str]) -> List[Tuple[str, Any]]:\n        rels_with_scores = []\n        n_batches = len(candidate_rels) // self.batch_size + int(len(candidate_rels) % self.batch_size > 0)\n        for i in range(n_batches):\n            questions_batch = []\n            rels_labels_batch = []\n            rels_batch = []\n            for candidate_rel in candidate_rels[i * self.batch_size: (i + 1) * self.batch_size]:\n                if candidate_rel in self.rel_q2name:\n                    questions_batch.append(question)\n                    rels_batch.append(candidate_rel)\n                    rels_labels_batch.append(self.rel_q2name[candidate_rel])\n            probas = self.ranker(questions_batch, rels_labels_batch)\n            probas = [proba[1] for proba in probas]\n            for j, rel in enumerate(rels_batch):\n                rels_with_scores.append((rel, probas[j]))\n        rels_with_scores = sorted(rels_with_scores, key=lambda x: x[1], reverse=True)\n\n        return rels_with_scores[:self.rels_to_leave]\n'"
deeppavlov/models/kbqa/rel_ranking_infer.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Tuple, List, Any\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.core.models.serializable import Serializable\nfrom deeppavlov.core.common.file import load_pickle\nfrom deeppavlov.models.ranking.rel_ranker import RelRanker\n\n\n@register(\'rel_ranking_infer\')\nclass RelRankerInfer(Component, Serializable):\n    """"""This class performs ranking of candidate relations""""""\n\n    def __init__(self, load_path: str,\n                 rel_q2name_filename: str,\n                 ranker: RelRanker,\n                 rels_to_leave: int = 15,\n                 batch_size: int = 100, **kwargs):\n\n        """"""\n\n        Args:\n            load_path: path to folder with wikidata files\n            rel_q2name_filename: name of file which maps relation id to name\n            ranker: deeppavlov.models.ranking.rel_ranker\n            rels_to_leave: how many top scored relations leave\n            batch_size: infering batch size\n            **kwargs:\n        """"""\n        super().__init__(save_path=None, load_path=load_path)\n        self.rel_q2name_filename = rel_q2name_filename\n        self.ranker = ranker\n        self.rels_to_leave = rels_to_leave\n        self.batch_size = batch_size\n        self.load()\n\n    def load(self) -> None:\n        self.rel_q2name = load_pickle(self.load_path / self.rel_q2name_filename)\n\n    def save(self) -> None:\n        pass\n\n    def __call__(self, question_batch: List[str], candidate_rels_batch: List[List[str]]) -> List[List[Tuple[str, Any]]]:\n        rels_with_scores_batch = []\n        for question, candidate_rels in zip(question_batch, candidate_rels_batch):\n            rels_with_scores_batch.append(self.rank_rels(question, candidate_rels))\n        return rels_with_scores_batch\n\n    def rank_rels(self, question: str, candidate_rels: List[str]) -> List[Tuple[str, Any]]:\n        rels_with_scores = []\n        n_batches = len(candidate_rels) // self.batch_size + int(len(candidate_rels) % self.batch_size > 0)\n        for i in range(n_batches):\n            questions_batch = []\n            rels_labels_batch = []\n            rels_batch = []\n            for candidate_rel in candidate_rels[i * self.batch_size: (i + 1) * self.batch_size]:\n                if candidate_rel in self.rel_q2name:\n                    questions_batch.append(question)\n                    rels_batch.append(candidate_rel)\n                    rels_labels_batch.append(self.rel_q2name[candidate_rel])\n            probas = self.ranker(questions_batch, rels_labels_batch)\n            probas = [proba[1] for proba in probas]\n            for j, rel in enumerate(rels_batch):\n                rels_with_scores.append((rel, probas[j]))\n        rels_with_scores = sorted(rels_with_scores, key=lambda x: x[1], reverse=True)\n\n        return rels_with_scores[:self.rels_to_leave]\n'"
deeppavlov/models/kbqa/template_matcher.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\nimport multiprocessing as mp\nimport json\nimport functools\nfrom logging import getLogger\nfrom typing import Tuple, List\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.core.models.serializable import Serializable\n\nlog = getLogger(__name__)\n\n\nclass RegexpMatcher:\n    def __init__(self, question):\n        self.question = question\n\n    def __call__(self, template):\n        res = re.findall(template[""template_regexp""], self.question)\n        found_template = []\n        if res:\n            found_template.append((res[0], template))\n        return found_template\n\n\n@register(\'template_matcher\')\nclass TemplateMatcher(Serializable):\n    """"""\n        This class matches the question with one of the templates\n        to extract entity substrings and define which relations\n        corresponds to the question\n    """"""\n\n    def __init__(self, load_path: str, templates_filename: str,\n                 num_processors: int = None, **kwargs) -> None:\n        """"""\n\n        Args:\n            load_path: path to folder with file with templates\n            templates_filename: file with templates\n            **kwargs:\n        """"""\n        super().__init__(save_path=None, load_path=load_path)\n        self.templates_filename = templates_filename\n        self.num_processors = mp.cpu_count() if num_processors == None else num_processors\n        self.pool = mp.Pool(self.num_processors)\n        self.load()\n\n    def load(self) -> None:\n        log.debug(f""(load)self.load_path / self.templates_filename: {self.load_path / self.templates_filename}"")\n        with open(self.load_path / self.templates_filename) as fl:\n            self.templates = json.load(fl)\n\n    def save(self) -> None:\n        raise NotImplementedError\n\n    def __call__(self, question: str) -> Tuple[List[str], List[str], List[Tuple[str]], List[str], str]:\n        question = question.lower()\n        question = self.sanitize(question)\n        question_length = len(question)\n        entities, types, relations, relation_dirs = [], [], [], []\n        query_type = """"\n        results = self.pool.map(RegexpMatcher(question), self.templates)\n        results = functools.reduce(lambda x, y: x + y, results)\n        replace_tokens = [(""the uk"", ""united kingdom""), (""the us"", ""united states"")]\n        if results:\n            min_length = 100\n            for result in results:\n                found_ent, template = result\n                positions_entity_tokens = template[""positions_entity_tokens""]\n                positions_type_tokens = template[""positions_type_tokens""]\n                positions_unuseful_tokens = template[""positions_unuseful_tokens""]\n                template_len = template[""template_len""]\n                entities_cand = [found_ent[pos].replace(\'?\', \'\') for pos in positions_entity_tokens]\n                types_cand = [found_ent[pos].replace(\'?\', \'\').split(\',\')[0] for pos in positions_type_tokens]\n                unuseful_tokens = [found_ent[pos].replace(\'?\', \'\') for pos in positions_unuseful_tokens]\n                entity_lengths = [len(entity) for entity in entities_cand]\n                entity_num_tokens = all([len(entity.split(\' \')) < 6 for entity in entities_cand])\n                type_lengths = [len(entity_type) for entity_type in types_cand]\n                unuseful_tokens_len = sum([len(unuseful_tok) for unuseful_tok in unuseful_tokens])\n                log.debug(f""found template: {template}, {found_ent}"")\n\n                if 0 not in entity_lengths or 0 not in type_lengths and entity_num_tokens:\n                    cur_len = sum(entity_lengths) + sum(type_lengths)\n                    log.debug(f""lengths: entity+type {cur_len}, question {question_length}, ""\n                              f""template {template_len}, unuseful tokens {unuseful_tokens_len}"")\n                    if cur_len < min_length and unuseful_tokens_len + template_len + cur_len == question_length:\n                        entities = entities_cand\n                        for old_token, new_token in replace_tokens:\n                            entities = [entity.replace(old_token, new_token) for entity in entities]\n                        types = types_cand\n                        relations = template[""relations""]\n                        relation_dirs = template[""rel_dirs""]\n                        query_type = template[""template_type""]\n                        min_length = cur_len\n\n        return entities, types, relations, relation_dirs, query_type\n\n    def sanitize(self, question: str) -> str:\n        if question.startswith(""the ""):\n            question = question[4:]\n        if question.startswith(""a ""):\n            question = question[2:]\n\n        date_interval = re.findall(""([\\d]{4}-[\\d]{4})"", question)\n        if date_interval:\n            question = question.replace(date_interval[0], \'\')\n        question = question.replace(\'  \', \' \')\n        return question\n'"
deeppavlov/models/kbqa/tree_parser.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom io import StringIO\nfrom typing import List, Tuple\n\nfrom udapi.block.read.conllu import Conllu\nfrom udapi.core.node import Node\nfrom ufal_udpipe import Model as udModel, Pipeline\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.core.models.serializable import Serializable\n\n\ndef descendents(node, desc_list):\n    if len(node.children) > 0:\n        for child in node.children:\n            desc_list = descendents(child, desc_list)\n    desc_list.append(node.form)\n\n    return desc_list\n\n\n@register(\'tree_parser\')\nclass TreeParser(Component, Serializable):\n    """"""This class parses the question using UDPipe to detect entity and relation""""""\n\n    def __init__(self, load_path: str, udpipe_filename: str, **kwargs) -> None:\n        """"""\n        \n        Args:\n            load_path: path to file with UDPipe model\n            udpipe_filename: filename with UDPipe model\n            **kwargs\n        """"""\n        super().__init__(save_path=None, load_path=load_path)\n        self.udpipe_filename = udpipe_filename\n        self.load()\n        self.full_ud_model = Pipeline(self.ud_model, ""vertical"", Pipeline.DEFAULT, Pipeline.DEFAULT, ""conllu"")\n\n    def load(self) -> None:\n        self.ud_model = udModel.load(str(self.load_path / self.udpipe_filename))\n\n    def save(self) -> None:\n        pass\n\n    def __call__(self, q_tokens_batch: List[List[str]]) -> List[Tuple[str, str]]:\n        entity_rel_list = []\n        for q_tokens in q_tokens_batch:\n            q_str = \'\\n\'.join(q_tokens)\n            s = self.full_ud_model.process(q_str)\n            tree = Conllu(filehandle=StringIO(s)).read_tree()\n            fnd, detected_entity, detected_rel = self.find_entity(tree, q_tokens)\n            if not fnd:\n                fnd, detected_entity, detected_rel = self.find_entity_adj(tree)\n            detected_entity = detected_entity.replace(""\xd0\xbf\xd0\xb5\xd1\x80\xd0\xb2\xd1\x8b\xd0\xb9 "", \'\')\n            entity_rel_list.append((detected_entity, detected_rel))\n        return entity_rel_list\n\n    def find_entity(self, tree: Node, q_tokens: List[str]) -> Tuple[bool, str, str]:\n        detected_entity = """"\n        detected_rel = """"\n        min_tree = 10\n        leaf_node = None\n        for node in tree.descendants:\n            if len(node.children) < min_tree and node.upos in [""NOUN"", ""PROPN""]:\n                leaf_node = node\n\n        if leaf_node is not None:\n            node = leaf_node\n            desc_list = []\n            entity_tokens = []\n            while node.parent.upos in [""NOUN"", ""PROPN""] and node.parent.deprel != ""root"" \\\n                    and not node.parent.parent.form.startswith(""\xd0\x9a\xd0\xb0\xd0\xba""):\n                node = node.parent\n            detected_rel = node.parent.form\n            desc_list.append(node.form)\n            desc_list = descendents(node, desc_list)\n            num_tok = 0\n            for n, tok in enumerate(q_tokens):\n                if tok in desc_list:\n                    entity_tokens.append(tok)\n                    num_tok = n\n            if (num_tok + 1) < len(q_tokens) and q_tokens[(num_tok + 1)].isdigit():\n                    entity_tokens.append(q_tokens[(num_tok + 1)])\n            detected_entity = \' \'.join(entity_tokens)\n            return True, detected_entity, detected_rel\n\n        return False, detected_entity, detected_rel\n\n    def find_entity_adj(self, tree: Node) -> Tuple[bool, str, str]:\n        detected_rel = """"\n        detected_entity = """"\n        for node in tree.descendants:\n            if len(node.children) <= 1 and node.upos == ""ADJ"":\n                detected_rel = node.parent.form\n                detected_entity = node.form\n                return True, detected_entity, detected_rel\n\n        return False, detected_entity, detected_rel\n'"
deeppavlov/models/kbqa/utils.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\nimport itertools\nfrom typing import List\n\n\ndef extract_year(question_tokens: List[str], question: str) -> str:\n    question_patterns = [r\'.*\\d{1,2}/\\d{1,2}/(\\d{4}).*\', r\'.*\\d{1,2}-\\d{1,2}-(\\d{4}).*\', r\'.*(\\d{4})-\\d{1,2}-\\d{1,2}.*\']\n    token_patterns = [r\'(\\d{4})\', r\'^(\\d{4})-.*\', r\'.*-(\\d{4})$\']\n    year = """"\n    for pattern in question_patterns:\n        fnd = re.search(pattern, question)\n        if fnd is not None:\n            year = fnd.group(1)\n            break\n    else:\n        for token in question_tokens:\n            for pattern in token_patterns:\n                fnd = re.search(pattern, token)\n                if fnd is not None:\n                    return fnd.group(1)\n    return year\n\n\ndef extract_number(question_tokens: List[str], question: str) -> str:\n    number = """"\n    fnd = re.search(r\'.*(\\d\\.\\d+e\\+\\d+)\\D*\', question)\n    if fnd is not None:\n        number = fnd.group(1)\n    else:\n        for tok in question_tokens:\n            if tok[0].isdigit():\n                number = tok\n                break\n\n    number = number.replace(\'1st\', \'1\').replace(\'2nd\', \'2\').replace(\'3rd\', \'3\')\n    number = number.strip("".0"")\n\n    return number\n\n\ndef order_of_answers_sorting(question: str) -> str:\n    question_lower = question.lower()\n    max_words = [""maximum"", ""highest"", ""max "", ""greatest"", ""most"", ""longest"", ""biggest"", ""deepest""]\n\n    for word in max_words:\n        if word in question_lower:\n            return ""desc""\n\n    return ""asc""\n\n\ndef make_combs(entity_ids: List[List[str]], permut: bool) -> List[List[str]]:\n    entity_ids = [[(entity, n) for n, entity in enumerate(entities_list)] for entities_list in entity_ids]\n    entity_ids = list(itertools.product(*entity_ids))\n    entity_ids_permut = []\n    if permut:\n        for comb in entity_ids:\n            entity_ids_permut += itertools.permutations(comb)\n    else:\n        entity_ids_permut = entity_ids\n    entity_ids = sorted(entity_ids_permut, key=lambda x: sum([elem[1] for elem in x]))\n    ent_combs = [[elem[0] for elem in comb] + [sum([elem[1] for elem in comb])] for comb in entity_ids]\n    return ent_combs\n\n\ndef fill_query(query: List[str], entity_comb: List[str], type_comb: List[str], rel_comb: List[str]) -> List[str]:\n    \'\'\' example of query: [""wd:E1"", ""p:R1"", ""?s""]\n                   entity_comb: [""Q159""]\n                   type_comb: []\n                   rel_comb: [""P17""]\n    \'\'\'\n    query = "" "".join(query)\n    map_query_str_to_wikidata = [(""p0"", ""http://schema.org/description""),\n                                 (""wd:"", ""http://www.wikidata.org/entity/""),\n                                 (""wdt:"", ""http://www.wikidata.org/prop/direct/""),\n                                 ("" p:"", "" http://www.wikidata.org/prop/""),\n                                 (""wdt:"", ""http://www.wikidata.org/prop/direct/""),\n                                 (""ps:"", ""http://www.wikidata.org/prop/statement/""),\n                                 (""pq:"", ""http://www.wikidata.org/prop/qualifier/"")]\n\n    for query_str, wikidata_str in map_query_str_to_wikidata:\n        query = query.replace(query_str, wikidata_str)\n    for n, entity in enumerate(entity_comb[:-1]):\n        query = query.replace(f""e{n + 1}"", entity)\n    for n, entity_type in enumerate(type_comb[:-1]):  # type_entity\n        query = query.replace(f""t{n + 1}"", entity_type)\n    for n, rel in enumerate(rel_comb[:-1]):\n        query = query.replace(f""r{n + 1}"", rel)\n    query = query.split(\' \')\n    return query\n'"
deeppavlov/models/kbqa/wiki_parser.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import List, Tuple, Dict\nfrom collections import namedtuple\n\nfrom hdt import HDTDocument\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\n\nlog = getLogger(__name__)\n\n\n@register(\'wiki_parser\')\nclass WikiParser:\n    """"""This class extract relations, objects or triplets from Wikidata HDT file""""""\n\n    def __init__(self, wiki_filename: str, **kwargs) -> None:\n        """"""\n\n        Args:\n            wiki_filename: hdt file with wikidata\n            **kwargs:\n        """"""\n        log.debug(f\'__init__ wiki_filename: {wiki_filename}\')\n        wiki_path = expand_path(wiki_filename)\n        self.document = HDTDocument(str(wiki_path))\n\n    def __call__(self, what_return: List[str],\n                 query_seq: List[List[str]],\n                 filter_info: List[Tuple[str]],\n                 order_info: namedtuple) -> List[List[str]]:\n        """"""\n            Let us consider an example of the question \n                ""What is the deepest lake in Russia?""\n            with the corresponding SPARQL query            \n            ""SELECT ?ent WHERE { ?ent wdt:P31 wd:T1 . ?ent wdt:R1 ?obj . ?ent wdt:R2 wd:E1 } ORDER BY ASC(?obj) LIMIT 5""\n            arguments:\n                what_return: [""?obj""]\n                query_seq: [[""?ent"", ""http://www.wikidata.org/prop/direct/P17"", ""http://www.wikidata.org/entity/Q159""]\n                            [""?ent"", ""http://www.wikidata.org/prop/direct/P31"", ""http://www.wikidata.org/entity/Q23397""],\n                            [""?ent"", ""http://www.wikidata.org/prop/direct/P4511"", ""?obj""]]\n                filter_info: []\n                order_info: order_info(variable=\'?obj\', sorting_order=\'asc\')\n        """"""\n        extended_combs = []\n        combs = []\n        for n, query in enumerate(query_seq):\n            unknown_elem_positions = [(pos, elem) for pos, elem in enumerate(query) if elem.startswith(\'?\')]\n            """"""\n                n = 0, query = [""?ent"", ""http://www.wikidata.org/prop/direct/P17"", ""http://www.wikidata.org/entity/Q159""]\n                       unknown_elem_positions = [""?ent""]\n                n = 1, query = [""?ent"", ""http://www.wikidata.org/prop/direct/P31"", ""http://www.wikidata.org/entity/Q23397""]\n                       unknown_elem_positions = [(0, ""?ent"")]\n                n = 2, query = [""?ent"", ""http://www.wikidata.org/prop/direct/P4511"", ""?obj""]\n                       unknown_elem_positions = [(0, ""?ent""), (2, ""?obj"")]\n            """"""\n            if n == 0:\n                combs = self.search(query, unknown_elem_positions)\n                # combs = [{""?ent"": ""http://www.wikidata.org/entity/Q5513""}, ...]\n            else:\n                if combs:\n                    known_elements = []\n                    extended_combs = []\n                    for elem in query:\n                        if elem in combs[0].keys():\n                            known_elements.append(elem)\n                    for comb in combs:\n                        """"""\n                            n = 1\n                            query = [""?ent"", ""http://www.wikidata.org/prop/direct/P31"", ""http://www.wikidata.org/entity/Q23397""]\n                            comb = {""?ent"": ""http://www.wikidata.org/entity/Q5513""}\n                            known_elements = [""?ent""], known_values = [""http://www.wikidata.org/entity/Q5513""]\n                            filled_query = [""http://www.wikidata.org/entity/Q5513"", \n                                            ""http://www.wikidata.org/prop/direct/P31"", \n                                            ""http://www.wikidata.org/entity/Q23397""]\n                            new_combs = [[""http://www.wikidata.org/entity/Q5513"", \n                                          ""http://www.wikidata.org/prop/direct/P31"", \n                                          ""http://www.wikidata.org/entity/Q23397""], ...]\n                            extended_combs = [{""?ent"": ""http://www.wikidata.org/entity/Q5513""}, ...]\n                        """"""                        \n                        known_values = [comb[known_elem] for known_elem in known_elements]\n                        for known_elem, known_value in zip(known_elements, known_values):\n                            filled_query = [elem.replace(known_elem, known_value) for elem in query]\n                            new_combs = self.search(filled_query, unknown_elem_positions)\n                            for new_comb in new_combs:\n                                extended_combs.append({**comb, **new_comb})\n                combs = extended_combs\n\n        if combs:\n            if filter_info:\n                for filter_elem, filter_value in filter_info:\n                    combs = [comb for comb in combs if filter_value in comb[filter_elem]]\n\n            if order_info.variable is not None:\n                reverse = True if order_info.sorting_order == ""desc"" else False\n                sort_elem = order_info.variable\n                combs = sorted(combs, key=lambda x: float(x[sort_elem].split(\'^^\')[0].strip(\'""\')), reverse=reverse)\n                combs = [combs[0]]\n\n            if what_return[-1].startswith(""count""):\n                combs = [[combs[0][key] for key in what_return[:-1]] + [len(combs)]]\n            else:\n                combs = [[elem[key] for key in what_return] for elem in combs]\n\n        return combs\n\n    def search(self, query: List[str], unknown_elem_positions: List[Tuple[int, str]]) -> List[Dict[str, str]]:\n        query = list(map(lambda elem: """" if elem.startswith(\'?\') else elem, query))\n        subj, rel, obj = query\n        triplets, c = self.document.search_triples(subj, rel, obj)\n        combs = [{elem: triplet[pos] for pos, elem in unknown_elem_positions} for triplet in triplets]\n        return combs\n\n    def find_label(self, entity: str) -> str:\n        entity = str(entity).replace(\'""\', \'\')\n        if entity.startswith(""Q""):\n            # example: ""Q5513""\n            entity = ""http://www.wikidata.org/entity/"" + entity\n            # ""http://www.wikidata.org/entity/Q5513""\n\n        if entity.startswith(""http://www.wikidata.org/entity/""):\n            labels, cardinality = self.document.search_triples(entity, ""http://www.w3.org/2000/01/rdf-schema#label"", """")\n            # labels = [[""http://www.wikidata.org/entity/Q5513"", ""http://www.w3.org/2000/01/rdf-schema#label"", \'""Lake Baikal""@en\'], ...]\n            for label in labels:\n                if label[2].endswith(""@en""):\n                    found_label = label[2].strip(\'@en\').replace(\'""\', \'\')\n                    return found_label\n\n        elif entity.endswith(""@en""):\n            # entity: \'""Lake Baikal""@en\'\n            entity = entity.strip(\'@en\')\n            return entity\n\n        elif ""^^"" in entity:\n            """"""\n                examples:\n                    \'""1799-06-06T00:00:00Z""^^<http://www.w3.org/2001/XMLSchema#dateTime>\' (date)\n                    \'""+1642""^^<http://www.w3.org/2001/XMLSchema#decimal>\' (number)\n            """"""\n            entity = entity.split(""^^"")[0]\n            for token in [""T00:00:00Z"", ""+""]:\n                entity = entity.replace(token, \'\')\n            return entity\n\n        elif entity.isdigit():\n            return entity\n\n        return ""Not Found""\n\n    def find_alias(self, entity: str) -> List[str]:\n        aliases = []\n        if entity.startswith(""http://www.wikidata.org/entity/""):\n            labels, cardinality = self.document.search_triples(entity,\n                                                               ""http://www.w3.org/2004/02/skos/core#altLabel"", """")\n            aliases = [label[2].strip(\'@en\').strip(\'""\') for label in labels if label[2].endswith(""@en"")]\n        return aliases\n\n    def find_rels(self, entity: str, direction: str, rel_type: str = None) -> List[str]:\n        if direction == ""forw"":\n            triplets, num = self.document.search_triples(f""http://www.wikidata.org/entity/{entity}"", """", """")\n        else:\n            triplets, num = self.document.search_triples("""", """", f""http://www.wikidata.org/entity/{entity}"")\n\n        if rel_type is not None:\n            start_str = f""http://www.wikidata.org/prop/{rel_type}""\n        else:\n            start_str = ""http://www.wikidata.org/prop/P""\n        rels = [triplet[1] for triplet in triplets if triplet[1].startswith(start_str)]\n        return rels\n'"
deeppavlov/models/morpho_tagger/__init__.py,0,b''
deeppavlov/models/morpho_tagger/__main__.py,0,"b'import argparse\n\nfrom deeppavlov.core.common.file import find_config\nfrom deeppavlov.download import deep_download\nfrom deeppavlov.models.morpho_tagger.common import predict_with_model\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""config_path"", help=""path to file with prediction configuration"")\nparser.add_argument(""-d"", ""--download"", action=""store_true"", help=""download model components"")\nparser.add_argument(""-b"", ""--batch-size"", dest=""batch_size"", default=16, help=""inference batch size"", type=int)\nparser.add_argument(""-f"", ""--input-file"", dest=""file_path"", default=None, help=""path to the input file"", type=str)\nparser.add_argument(""-i"", ""--input-format"", dest=""input_format"", default=""ud"",\n                    help=""input format (\'text\' for untokenized text, \'ud\' or \'vertical\'"", type=str)\nparser.add_argument(""-o"", ""--output-format"", dest=""output_format"", default=""basic"",\n                    help=""input format (\'basic\', \'ud\' or \'conllu\' (the last two mean the same)"", type=str)\n\nif __name__ == ""__main__"":\n    args = parser.parse_args()\n    config_path = find_config(args.config_path)\n    if args.download:\n        deep_download(config_path)\n    answer = predict_with_model(config_path, infile=args.file_path, input_format=args.input_format,\n                                batch_size=args.batch_size, output_format=args.output_format)\n    for elem in answer:\n        print(elem)\n'"
deeppavlov/models/morpho_tagger/cells.py,2,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.initializers import Constant\nfrom tensorflow.keras.layers import InputSpec, Layer, Lambda, Dropout, Multiply\n\nINFTY = -100\n\n\nclass Highway(Layer):\n\n    def __init__(self, activation=None, bias_initializer=-1, **kwargs):\n        super().__init__(**kwargs)\n        self.activation = tf.keras.activations.get(activation)\n        self.bias_initializer = bias_initializer\n        if isinstance(self.bias_initializer, int):\n            self.bias_initializer = Constant(self.bias_initializer)\n        self.input_spec = [InputSpec(min_ndim=2)]\n\n    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        input_dim = input_shape[-1]\n\n        self.gate_kernel = self.add_weight(\n            shape=(input_dim, input_dim), initializer=\'uniform\', name=\'gate_kernel\')\n        self.gate_bias = self.add_weight(\n            shape=(input_dim,), initializer=self.bias_initializer, name=\'gate_bias\')\n        self.dense_kernel = self.add_weight(\n            shape=(input_dim, input_dim), initializer=\'uniform\', name=\'dense_kernel\')\n        self.dense_bias = self.add_weight(\n            shape=(input_dim,), initializer=self.bias_initializer, name=\'dense_bias\')\n        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n        self.built = True\n\n    def call(self, inputs, **kwargs):\n        gate = K.dot(inputs, self.gate_kernel)\n        gate = K.bias_add(gate, self.gate_bias, data_format=""channels_last"")\n        gate = self.activation(gate)\n        new_value = K.dot(inputs, self.dense_kernel)\n        new_value = K.bias_add(new_value, self.dense_bias, data_format=""channels_last"")\n        return gate * new_value + (1.0 - gate) * inputs\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n\ndef weighted_sum(first, second, sigma, first_threshold=-np.inf, second_threshold=np.inf):\n    logit_probs = first * sigma + second * (1.0 - sigma)\n    infty_tensor = K.ones_like(logit_probs) * INFTY\n    logit_probs = K.switch(K.greater(first, first_threshold), logit_probs, infty_tensor)\n    logit_probs = K.switch(K.greater(second, second_threshold), logit_probs, infty_tensor)\n    return logit_probs\n\n\nclass WeightedCombinationLayer(Layer):\n\n    """"""\n    A class for weighted combination of probability distributions\n    """"""\n\n    def __init__(self, first_threshold=None, second_threshold=None,\n                 use_dimension_bias=False, use_intermediate_layer=False,\n                 intermediate_dim=64, intermediate_activation=None,\n                 from_logits=False, return_logits=False,\n                 bias_initializer=1.0, **kwargs):\n        # if \'input_shape\' not in kwargs:\n        #     kwargs[\'input_shape\'] = [(None, input_dim,), (None, input_dim)]\n        super(WeightedCombinationLayer, self).__init__(**kwargs)\n        self.first_threshold = first_threshold if first_threshold is not None else INFTY\n        self.second_threshold = second_threshold if second_threshold is not None else INFTY\n        self.use_dimension_bias = use_dimension_bias\n        self.use_intermediate_layer = use_intermediate_layer\n        self.intermediate_dim = intermediate_dim\n        self.intermediate_activation = tf.keras.activations.get(intermediate_activation)\n        self.from_logits = from_logits\n        self.return_logits = return_logits\n        self.bias_initializer = bias_initializer\n        self.input_spec = [InputSpec(), InputSpec(), InputSpec()]\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n        assert input_shape[0] == input_shape[1]\n        assert input_shape[0][:-1] == input_shape[2][:-1]\n\n        input_dim, features_dim = input_shape[0][-1], input_shape[2][-1]\n        if self.use_intermediate_layer:\n            self.first_kernel = self.add_weight(\n                shape=(features_dim, self.intermediate_dim),\n                initializer=""random_uniform"", name=\'first_kernel\')\n            self.first_bias = self.add_weight(\n                shape=(self.intermediate_dim,),\n                initializer=""random_uniform"", name=\'first_bias\')\n        self.features_kernel = self.add_weight(\n            shape=(features_dim, 1), initializer=""random_uniform"", name=\'kernel\')\n        self.features_bias = self.add_weight(\n            shape=(1,), initializer=Constant(self.bias_initializer), name=\'bias\')\n        if self.use_dimension_bias:\n            self.dimensions_bias = self.add_weight(\n                shape=(input_dim,), initializer=""random_uniform"", name=\'dimension_bias\')\n        super(WeightedCombinationLayer, self).build(input_shape)\n\n    def call(self, inputs, **kwargs):\n        assert isinstance(inputs, list) and len(inputs) == 3\n        first, second, features = inputs[0], inputs[1], inputs[2]\n        if not self.from_logits:\n            first = K.clip(first, 1e-10, 1.0)\n            second = K.clip(second, 1e-10, 1.0)\n            first_, second_ = K.log(first), K.log(second)\n        else:\n            first_, second_ = first, second\n        # embedded_features.shape = (M, T, 1)\n        if self.use_intermediate_layer:\n            features = K.dot(features, self.first_kernel)\n            features = K.bias_add(features, self.first_bias, data_format=""channels_last"")\n            features = self.intermediate_activation(features)\n        embedded_features = K.dot(features, self.features_kernel)\n        embedded_features = K.bias_add(\n            embedded_features, self.features_bias, data_format=""channels_last"")\n        if self.use_dimension_bias:\n            tiling_shape = [1] * (K.ndim(first) - 1) + [K.shape(first)[-1]]\n            embedded_features = K.tile(embedded_features, tiling_shape)\n            embedded_features = K.bias_add(\n                embedded_features, self.dimensions_bias, data_format=""channels_last"")\n        sigma = K.sigmoid(embedded_features)\n\n        result = weighted_sum(first_, second_, sigma,\n                              self.first_threshold, self.second_threshold)\n        probs = K.softmax(result)\n        if self.return_logits:\n            return [probs, result]\n        return probs\n\n    def compute_output_shape(self, input_shape):\n        first_shape = input_shape[0]\n        if self.return_logits:\n            return [first_shape, first_shape]\n        return first_shape\n\n\ndef TemporalDropout(inputs, dropout=0.0):\n    """"""\n    Drops with :dropout probability temporal steps of input 3D tensor\n    """"""\n    # TO DO: adapt for >3D tensors\n    if dropout == 0.0:\n        return inputs\n    inputs_func = lambda x: K.ones_like(inputs[:, :, 0:1])\n    inputs_mask = Lambda(inputs_func)(inputs)\n    inputs_mask = Dropout(dropout)(inputs_mask)\n    tiling_shape = [1, 1, K.shape(inputs)[2]] + [1] * (K.ndim(inputs) - 3)\n    inputs_mask = Lambda(K.tile, arguments={""n"": tiling_shape},\n                         output_shape=inputs._keras_shape[1:])(inputs_mask)\n    answer = Multiply()([inputs, inputs_mask])\n    return answer\n\n\ndef positions_func(inputs, pad=0):\n    """"""\n    A layer filling i-th column of a 2D tensor with\n    1+ln(1+i) when it contains a meaningful symbol\n    and with 0 when it contains PAD\n    """"""\n    position_inputs = K.cumsum(K.ones_like(inputs, dtype=""float32""), axis=1)\n    position_inputs *= K.cast(K.not_equal(inputs, pad), ""float32"")\n    return K.log(1.0 + position_inputs)'"
deeppavlov/models/morpho_tagger/common.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\nfrom pathlib import Path\nfrom typing import List, Union, Optional\n\nfrom deeppavlov.core.commands.infer import build_model\nfrom deeppavlov.core.commands.utils import expand_path, parse_config\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.dataset_readers.morphotagging_dataset_reader import read_infile\nfrom deeppavlov.models.morpho_tagger.common_tagger import make_pos_and_tag\n\n\ndef predict_with_model(config_path: [Path, str], infile: Optional[Union[Path, str]] = None,\n                       input_format: str = ""ud"", batch_size: [int] = 16,\n                       output_format: str = ""basic"") -> List[Optional[List[str]]]:\n    """"""Returns predictions of morphotagging model given in config :config_path:.\n\n    Args:\n        config_path: a path to config\n\n    Returns:\n        a list of morphological analyses for each sentence. Each analysis is either a list of tags\n        or a list of full CONLL-U descriptions.\n\n    """"""\n    config = parse_config(config_path)\n    if infile is None:\n        if sys.stdin.isatty():\n            raise RuntimeError(\'To process data from terminal please use interact mode\')\n        infile = sys.stdin\n    else:\n        infile = expand_path(infile)\n    if input_format in [""ud"", ""conllu"", ""vertical""]:\n        from_words = (input_format == ""vertical"")\n        data: List[tuple] = read_infile(infile, from_words=from_words)\n        # keeping only sentences\n        data = [elem[0] for elem in data]\n    else:\n        if infile is not sys.stdin:\n            with open(infile, ""r"", encoding=""utf8"") as fin:\n                data = fin.readlines()\n        else:\n            data = sys.stdin.readlines()\n    model = build_model(config, load_trained=True)\n    for elem in model.pipe:\n        if isinstance(elem[-1], TagOutputPrettifier):\n            elem[-1].set_format_mode(output_format)\n    answers = model.batched_call(data, batch_size=batch_size)\n    return answers\n\n\n@register(\'tag_output_prettifier\')\nclass TagOutputPrettifier(Component):\n    """"""Class which prettifies morphological tagger output to 4-column\n    or 10-column (Universal Dependencies) format.\n\n    Args:\n        format_mode: output format,\n            in `basic` mode output data contains 4 columns (id, word, pos, features),\n            in `conllu` or `ud` mode it contains 10 columns:\n            id, word, lemma, pos, xpos, feats, head, deprel, deps, misc\n            (see http://universaldependencies.org/format.html for details)\n            Only id, word, tag and pos values are present in current version,\n            other columns are filled by `_` value.\n        return_string: whether to return a list of strings or a single string\n        begin: a string to append in the beginning\n        end: a string to append in the end\n        sep: separator between word analyses\n    """"""\n\n    def __init__(self, format_mode: str = ""basic"", return_string: bool = True,\n                 begin: str = """", end: str = """", sep: str = ""\\n"", **kwargs) -> None:\n        self.set_format_mode(format_mode)\n        self.return_string = return_string\n        self.begin = begin\n        self.end = end\n        self.sep = sep\n\n    def set_format_mode(self, format_mode: str = ""basic"") -> None:\n        """"""A function that sets format for output and recalculates `self.format_string`.\n\n        Args:\n            format_mode: output format,\n                in `basic` mode output data contains 4 columns (id, word, pos, features),\n                in `conllu` or `ud` mode it contains 10 columns:\n                id, word, lemma, pos, xpos, feats, head, deprel, deps, misc\n                (see http://universaldependencies.org/format.html for details)\n                Only id, word, tag and pos values are present in current version,\n                other columns are filled by `_` value.\n\n        Returns:\n        """"""\n        self.format_mode = format_mode\n        self._make_format_string()\n\n    def _make_format_string(self) -> None:\n        if self.format_mode == ""basic"":\n            self.format_string = ""{}\\t{}\\t{}\\t{}""\n        elif self.format_mode.lower() in [""conllu"", ""ud""]:\n            self.format_string = ""{}\\t{}\\t_\\t{}\\t_\\t{}\\t_\\t_\\t_\\t_""\n        else:\n            raise ValueError(""Wrong mode for TagOutputPrettifier: {}, ""\n                             ""it must be \'basic\', \'conllu\' or \'ud\'."".format(self.mode))\n\n    def __call__(self, X: List[List[str]], Y: List[List[str]]) -> List[Union[List[str], str]]:\n        """"""Calls the :meth:`~prettify` function for each input sentence.\n\n        Args:\n            X: a list of input sentences\n            Y: a list of list of tags for sentence words\n\n        Returns:\n            a list of prettified morphological analyses\n        """"""\n        return [self.prettify(x, y) for x, y in zip(X, Y)]\n\n    def prettify(self, tokens: List[str], tags: List[str]) -> Union[List[str], str]:\n        """"""Prettifies output of morphological tagger.\n\n        Args:\n            tokens: tokenized source sentence\n            tags: list of tags, the output of a tagger\n\n        Returns:\n            the prettified output of the tagger.\n\n        Examples:\n            >>> sent = ""John really likes pizza ."".split()\n            >>> tags = [""PROPN,Number=Sing"", ""ADV"",\n            >>>         ""VERB,Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin"",\n            >>>         ""NOUN,Number=Sing"", ""PUNCT""]\n            >>> prettifier = TagOutputPrettifier(mode=\'basic\')\n            >>> self.prettify(sent, tags)\n                1\tJohn\tPROPN\tNumber=Sing\n                2\treally\tADV\t_\n                3\tlikes\tVERB\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n                4\tpizza\tNOUN\tNumber=Sing\n                5\t.\tPUNCT\t_\n            >>> prettifier = TagOutputPrettifier(mode=\'ud\')\n            >>> self.prettify(sent, tags)\n                1\tJohn\t_\tPROPN\t_\tNumber=Sing\t_\t_\t_\t_\n                2\treally\t_\tADV\t_\t_\t_\t_\t_\t_\n                3\tlikes\t_\tVERB\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\t_\t_\t_\t_\n                4\tpizza\t_\tNOUN\t_\tNumber=Sing\t_\t_\t_\t_\n                5\t.\t_\tPUNCT\t_\t_\t_\t_\t_\t_\n        """"""\n        answer = []\n        for i, (word, tag) in enumerate(zip(tokens, tags)):\n            answer.append(self.format_string.format(i + 1, word, *make_pos_and_tag(tag)))\n        if self.return_string:\n            answer = self.begin + self.sep.join(answer) + self.end\n        return answer\n\n\n@register(\'lemmatized_output_prettifier\')\nclass LemmatizedOutputPrettifier(Component):\n    """"""Class which prettifies morphological tagger output to 4-column\n    or 10-column (Universal Dependencies) format.\n\n    Args:\n        format_mode: output format,\n            in `basic` mode output data contains 4 columns (id, word, pos, features),\n            in `conllu` or `ud` mode it contains 10 columns:\n            id, word, lemma, pos, xpos, feats, head, deprel, deps, misc\n            (see http://universaldependencies.org/format.html for details)\n            Only id, word, lemma, tag and pos columns are predicted in current version,\n            other columns are filled by `_` value.\n        return_string: whether to return a list of strings or a single string\n        begin: a string to append in the beginning\n        end: a string to append in the end\n        sep: separator between word analyses\n    """"""\n\n    def __init__(self, return_string: bool = True,\n                 begin: str = """", end: str = """", sep: str = ""\\n"", **kwargs) -> None:\n        self.return_string = return_string\n        self.begin = begin\n        self.end = end\n        self.sep = sep\n        self.format_string = ""{0}\\t{1}\\t{4}\\t{2}\\t_\\t{3}\\t_\\t_\\t_\\t_""\n\n    def __call__(self, X: List[List[str]], Y: List[List[str]], Z: List[List[str]]) -> List[Union[List[str], str]]:\n        """"""Calls the :meth:`~prettify` function for each input sentence.\n\n        Args:\n            X: a list of input sentences\n            Y: a list of list of tags for sentence words\n            Z: a list of lemmatized sentences\n\n        Returns:\n            a list of prettified morphological analyses\n        """"""\n        return [self.prettify(*elem) for elem in zip(X, Y, Z)]\n\n    def prettify(self, tokens: List[str], tags: List[str], lemmas: List[str]) -> Union[List[str], str]:\n        """"""Prettifies output of morphological tagger.\n\n        Args:\n            tokens: tokenized source sentence\n            tags: list of tags, the output of a tagger\n            lemmas: list of lemmas, the output of a lemmatizer\n\n        Returns:\n            the prettified output of the tagger.\n\n        Examples:\n            >>> sent = ""John really likes pizza ."".split()\n            >>> tags = [""PROPN,Number=Sing"", ""ADV"",\n            >>>         ""VERB,Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin"",\n            >>>         ""NOUN,Number=Sing"", ""PUNCT""]\n            >>> lemmas = ""John really like pizza ."".split()\n            >>> prettifier = LemmatizedOutputPrettifier()\n            >>> self.prettify(sent, tags, lemmas)\n                1\tJohn\tJohn\tPROPN\t_\tNumber=Sing\t_\t_\t_\t_\n                2\treally\treally\tADV\t_\t_\t_\t_\t_\t_\n                3\tlikes\tlike\tVERB\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\t_\t_\t_\t_\n                4\tpizza\tpizza\tNOUN\t_\tNumber=Sing\t_\t_\t_\t_\n                5\t.\t.\tPUNCT\t_\t_\t_\t_\t_\t_\n        """"""\n        answer = []\n        for i, (word, tag, lemma) in enumerate(zip(tokens, tags, lemmas)):\n            pos, tag = make_pos_and_tag(tag, sep="","")\n            answer.append(self.format_string.format(i + 1, word, pos, tag, lemma))\n        if self.return_string:\n            answer = self.begin + self.sep.join(answer) + self.end\n        return answer\n\n\n@register(\'dependency_output_prettifier\')\nclass DependencyOutputPrettifier(Component):\n    """"""Class which prettifies dependency parser output\n    to 10-column (Universal Dependencies) format.\n\n    Args:\n        return_string: whether to return a list of strings or a single string\n        begin: a string to append in the beginning\n        end: a string to append in the end\n        sep: separator between word analyses\n    """"""\n\n    def __init__(self, return_string: bool = True, begin: str = """",\n                 end: str = """", sep: str = ""\\n"", **kwargs) -> None:\n        self.return_string = return_string\n        self.begin = begin\n        self.end = end\n        self.sep = sep\n        self.format_string = ""{}\\t{}\\t_\\t_\\t_\\t_\\t{}\\t{}\\t_\\t_""\n\n    def __call__(self, X: List[List[str]], Y: List[List[int]], Z: List[List[str]]) -> List[Union[List[str], str]]:\n        """"""Calls the :meth:`~prettify` function for each input sentence.\n\n        Args:\n            X: a list of input sentences\n            Y: a list of lists of head positions for sentence words\n            Z: a list of lists of dependency labels for sentence words\n\n        Returns:\n            a list of prettified UD outputs\n        """"""\n        return [self.prettify(x, y, z) for x, y, z in zip(X, Y, Z)]\n\n    def prettify(self, tokens: List[str], heads: List[int], deps: List[str]) -> Union[List[str], str]:\n        """"""Prettifies output of dependency parser.\n\n        Args:\n            tokens: tokenized source sentence\n            heads: list of head positions, the output of the parser\n            deps: list of head positions, the output of the parser\n\n        Returns:\n            the prettified output of the parser\n\n        """"""\n        answer = []\n        for i, (word, head, dep) in enumerate(zip(tokens, heads, deps)):\n            answer.append(self.format_string.format(i + 1, word, head, dep))\n        if self.return_string:\n            answer = self.begin + self.sep.join(answer) + self.end\n        return answer\n'"
deeppavlov/models/morpho_tagger/common_tagger.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""File containing common operation with keras.backend objects""""""\n\nfrom typing import Union, Optional, Tuple\n\nfrom tensorflow.keras import backend as K\nimport numpy as np\n\nEPS = 1e-15\n\n\n# AUXILIARY = [\'PAD\', \'BEGIN\', \'END\', \'UNKNOWN\']\n# AUXILIARY_CODES = PAD, BEGIN, END, UNKNOWN = 0, 1, 2, 3\n\n\ndef to_one_hot(x, k):\n    """"""\n    Takes an array of integers and transforms it\n    to an array of one-hot encoded vectors\n    """"""\n    unit = np.eye(k, dtype=int)\n    return unit[x]\n\n\ndef repeat_(x, k):\n    tile_factor = [1, k] + [1] * (K.ndim(x) - 1)\n    return K.tile(x[:, None, :], tile_factor)\n\n\ndef make_pos_and_tag(tag: str, sep: str = "","",\n                     return_mode: Optional[str] = None) -> Tuple[str, Union[str, list, dict, tuple]]:\n    """"""\n    Args:\n        tag: the part-of-speech tag\n        sep: the separator between part-of-speech tag and grammatical features\n        return_mode: the type of return value, can be None, list, dict or sorted_items\n\n    Returns:\n        the part-of-speech label and grammatical features in required format\n    """"""\n    if tag.endswith("" _""):\n        tag = tag[:-2]\n    if sep in tag:\n        pos, tag = tag.split(sep, maxsplit=1)\n    else:\n        pos, tag = tag, (""_"" if return_mode is None else """")\n    if return_mode in [""dict"", ""list"", ""sorted_items""]:\n        tag = tag.split(""|"") if tag != """" else []\n        if return_mode in [""dict"", ""sorted_items""]:\n            tag = dict(tuple(elem.split(""="")) for elem in tag)\n            if return_mode == ""sorted_items"":\n                tag = tuple(sorted(tag.items()))\n    return pos, tag\n\n\ndef make_full_UD_tag(pos: str, tag: Union[str, list, dict, tuple],\n                     sep: str = "","", mode: Optional[str] = None) -> str:\n    """"""\n    Args:\n        pos: the part-of-speech label\n        tag: grammatical features in the format, specified by \'mode\'\n        sep: the separator between part of speech and features in output tag\n        mode: the input format of tag, can be None, list, dict or sorted_items\n\n    Returns:\n        the string representation of morphological tag\n    """"""\n    if tag == ""_"" or len(tag) == 0:\n        return pos\n    if mode == ""dict"":\n        tag, mode = sorted(tag.items()), ""sorted_items""\n    if mode == ""sorted_items"":\n        tag, mode = [""{}={}"".format(*elem) for elem in tag], ""list""\n    if mode == ""list"":\n        tag = ""|"".join(tag)\n    return pos + sep + tag\n\n\ndef _are_equal_pos(first, second):\n    NOUNS, VERBS, CONJ = [""NOUN"", ""PROPN""], [""AUX"", ""VERB""], [""CCONJ"", ""SCONJ""]\n    return (first == second or any((first in parts) and (second in parts)\n                                   for parts in [NOUNS, VERBS, CONJ]))\n\n\nIDLE_FEATURES = {""Voice"", ""Animacy"", ""Degree"", ""Mood"", ""VerbForm""}\n\n\ndef get_tag_distance(first, second, first_sep="","", second_sep="" ""):\n    """"""\n    Measures the distance between two (Russian) morphological tags in UD Format.\n    The first tag is usually the one predicted by our model (therefore it uses comma\n    as separator), while the second is usually the result of automatical conversion,\n    where the separator is space.\n\n    Args:\n        first: UD morphological tag\n        second: UD morphological tag (usually the output of \'russian_tagsets\' converter)\n        first_sep: separator between two parts of the first tag\n        second_sep: separator between two parts of the second tag\n\n    Returns:\n        the number of mismatched feature values\n    """"""\n    first_pos, first_feats = make_pos_and_tag(first, sep=first_sep, return_mode=""dict"")\n    second_pos, second_feats = make_pos_and_tag(second, sep=second_sep, return_mode=""dict"")\n    dist = int(not _are_equal_pos(first_pos, second_pos))\n    for key, value in first_feats.items():\n        other = second_feats.get(key)\n        if other is None:\n            dist += int(key not in IDLE_FEATURES)\n        else:\n            dist += int(value != other)\n    for key in second_feats:\n        dist += int(key not in first_feats and key not in IDLE_FEATURES)\n    return dist\n'"
deeppavlov/models/morpho_tagger/lemmatizer.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import abstractmethod\nfrom typing import List, Optional\n\nimport numpy as np\nfrom pymorphy2 import MorphAnalyzer\nfrom russian_tagsets import converters\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.serializable import Serializable\nfrom deeppavlov.models.morpho_tagger.common_tagger import get_tag_distance\n\n\nclass BasicLemmatizer(Serializable):\n    """"""\n    A basic class for lemmatizers. It must contain two methods:\n    * :meth: `_lemmatize` for single word lemmatization. It is an abstract method and should be reimplemented.\n    * :meth: `__call__` for lemmatizing a batch of sentences.\n    """"""\n\n    def __init__(self, save_path: Optional[str] = None,\n                 load_path: Optional[str] = None, **kwargs) -> None:\n        super().__init__(save_path, load_path, **kwargs)\n\n    @abstractmethod\n    def _lemmatize(self, word: str, tag: Optional[str] = None) -> str:\n        """"""\n        Lemmatizes a separate word given its tag.\n\n        Args:\n            word: the input word.\n            tag: optional morphological tag.\n\n        Returns:\n            a lemmatized word\n        """"""\n        raise NotImplementedError(""Your lemmatizer must implement the abstract method _lemmatize."")\n\n    def __call__(self, data: List[List[str]], tags: Optional[List[List[str]]] = None) -> List[List[str]]:\n        """"""\n        Lemmatizes each word in a batch of sentences.\n\n        Args:\n            data: the batch of sentences (lists of words).\n            tags: the batch of morphological tags (if available).\n\n        Returns:\n            a batch of lemmatized sentences.\n        """"""\n        if tags is None:\n            tags = [[None for _ in sent] for sent in data]\n        if len(tags) != len(data):\n            raise ValueError(""There must be the same number of tag sentences as the number of word sentences."")\n        if any((len(elem[0]) != len(elem[1])) for elem in zip(data, tags)):\n            raise ValueError(""Tag sentence must be of the same length as the word sentence."")\n        answer = [[self._lemmatize(word, tag) for word, tag in zip(*elem)] for elem in zip(data, tags)]\n        return answer\n\n\n@register(""UD_pymorphy_lemmatizer"")\nclass UDPymorphyLemmatizer(BasicLemmatizer):\n    """"""\n    A class that returns a normal form of a Russian word given its morphological tag in UD format.\n    Lemma is selected from one of PyMorphy parses,\n    the parse whose tag resembles the most a known UD tag is chosen.\n    """"""\n\n    def __init__(self, save_path: Optional[str] = None, load_path: Optional[str] = None,\n                 transform_lemmas=False, **kwargs) -> None:\n        self.transform_lemmas = transform_lemmas\n        self._reset()\n        self.analyzer = MorphAnalyzer()\n        self.converter = converters.converter(""opencorpora-int"", ""ud20"")\n        super().__init__(save_path, load_path, **kwargs)\n\n    def save(self, *args, **kwargs):\n        pass\n\n    def load(self, *args, **kwargs):\n        pass\n\n    def _reset(self):\n        self.memo = dict()\n\n    def _lemmatize(self, word: str, tag: Optional[str] = None) -> str:\n        lemma = self.memo.get((word, tag))\n        if lemma is not None:\n            return lemma\n        parses = self.analyzer.parse(word)\n        best_lemma, best_distance = word, np.inf\n        for i, parse in enumerate(parses):\n            curr_tag, curr_lemma = self.converter(str(parse.tag)), parse.normal_form\n            distance = get_tag_distance(tag, curr_tag)\n            if distance < best_distance:\n                best_lemma, best_distance = curr_lemma, distance\n                if distance == 0:\n                    break\n        self.memo[(word, tag)] = best_lemma\n        return best_lemma\n'"
deeppavlov/models/morpho_tagger/morpho_tagger.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import List, Optional, Union, Tuple\n\nimport numpy as np\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import (Input, Dense, Lambda, Concatenate, Conv2D, Dropout, LSTM, Bidirectional,\n                                     TimeDistributed)\nfrom tensorflow.keras.optimizers import Nadam\nfrom tensorflow.keras.regularizers import l2\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.simple_vocab import SimpleVocabulary\nfrom deeppavlov.core.models.keras_model import KerasModel\nfrom .cells import Highway\nfrom .common_tagger import to_one_hot\n\nlog = getLogger(__name__)\n\nMAX_WORD_LENGTH = 30\n\n\n@register(""morpho_tagger"")\nclass MorphoTagger(KerasModel):\n    """"""A class for character-based neural morphological tagger\n\n    Parameters:\n        symbols: character vocabulary\n        tags: morphological tags vocabulary\n        save_path: the path where model is saved\n        load_path: the path from where model is loaded\n        mode: usage mode\n\n        word_rnn: the type of character-level network (only `cnn` implemented)\n        char_embeddings_size: the size of character embeddings\n        char_conv_layers: the number of convolutional layers on character level\n        char_window_size: the width of convolutional filter (filters).\n            It can be a list if several parallel filters are applied, for example, [2, 3, 4, 5].\n        char_filters: the number of convolutional filters for each window width.\n            It can be a number, a list (when there are several windows of different width\n            on a single convolution layer), a list of lists, if there\n            are more than 1 convolution layers, or **None**.\n            If **None**, a layer with width **width** contains\n            min(**char_filter_multiple** * **width**, 200) filters.\n\n        char_filter_multiple: the ratio between filters number and window width\n        char_highway_layers: the number of highway layers on character level\n        conv_dropout: the ratio of dropout between convolutional layers\n        highway_dropout: the ratio of dropout between highway layers,\n        intermediate_dropout: the ratio of dropout between convolutional\n            and highway layers on character level\n        lstm_dropout: dropout ratio in word-level LSTM\n        word_vectorizers: list of parameters for additional word-level vectorizers,\n            for each vectorizer it stores a pair of vectorizer dimension and\n            the dimension of the corresponding word embedding\n        word_lstm_layers: the number of word-level LSTM layers\n        word_lstm_units: hidden dimensions of word-level LSTMs\n        word_dropout: the ratio of dropout before word level (it is applied to word embeddings)\n        regularizer: l2 regularization parameter\n        verbose: the level of verbosity\n\n    A subclass of :class:`~deeppavlov.core.models.keras_model.KerasModel`\n    """"""\n    def __init__(self,\n                 symbols: SimpleVocabulary,\n                 tags: SimpleVocabulary,\n                 save_path: Optional[Union[str, Path]] = None,\n                 load_path: Optional[Union[str, Path]] = None,\n                 mode: str = \'infer\',\n                 word_rnn: str = ""cnn"",\n                 char_embeddings_size: int = 16,\n                 char_conv_layers: int = 1,\n                 char_window_size: Union[int, List[int]] = 5,\n                 char_filters: Union[int, List[int]] = None,\n                 char_filter_multiple: int = 25,\n                 char_highway_layers: int = 1,\n                 conv_dropout: float = 0.0,\n                 highway_dropout: float = 0.0,\n                 intermediate_dropout: float = 0.0,\n                 lstm_dropout: float = 0.0,\n                 word_vectorizers: List[Tuple[int, int]] = None,\n                 word_lstm_layers: int = 1,\n                 word_lstm_units: Union[int, List[int]] = 128,\n                 word_dropout: float = 0.0,\n                 regularizer: float = None,\n                 verbose: int = 1, **kwargs):\n        # Calls parent constructor. Results in creation of save_folder if it doesn\'t exist\n        super().__init__(save_path=save_path, load_path=load_path, mode=mode, **kwargs)\n        self.symbols = symbols\n        self.tags = tags\n        self.word_rnn = word_rnn\n        self.char_embeddings_size = char_embeddings_size\n        self.char_conv_layers = char_conv_layers\n        self.char_window_size = char_window_size\n        self.char_filters = char_filters\n        self.char_filter_multiple = char_filter_multiple\n        self.char_highway_layers = char_highway_layers\n        self.conv_dropout = conv_dropout\n        self.highway_dropout = highway_dropout\n        self.intermediate_dropout = intermediate_dropout\n        self.lstm_dropout = lstm_dropout\n        self.word_dropout = word_dropout\n        self.word_vectorizers = word_vectorizers  # a list of additional vectorizer dimensions\n        self.word_lstm_layers = word_lstm_layers\n        self.word_lstm_units = word_lstm_units\n        self.regularizer = regularizer\n        self.verbose = verbose\n        self._initialize()\n        self.model_ = None\n        self.build()\n\n        # Tries to load the model from model `load_path`, if it is available\n        self.load()\n\n    def load(self) -> None:\n        """"""\n        Checks existence of the model file, loads the model if the file exists\n        Loads model weights from a file\n        """"""\n\n        # Checks presence of the model files\n        if self.load_path.exists():\n            path = str(self.load_path.resolve())\n            log.info(\'[loading model from {}]\'.format(path))\n            self.model_.load_weights(path)\n\n    def save(self) -> None:\n        """"""\n        Saves model weights to the save_path, provided in config. The directory is\n        already created by super().__init__, which is called in __init__ of this class""""""\n        path = str(self.save_path.absolute())\n        log.info(\'[saving model to {}]\'.format(path))\n        self.model_.save_weights(path)\n\n    def _initialize(self):\n        if isinstance(self.char_window_size, int):\n            self.char_window_size = [self.char_window_size]\n        if self.char_filters is None or isinstance(self.char_filters, int):\n            self.char_filters = [self.char_filters] * len(self.char_window_size)\n        if len(self.char_window_size) != len(self.char_filters):\n            raise ValueError(""There should be the same number of window sizes and filter sizes"")\n        if isinstance(self.word_lstm_units, int):\n            self.word_lstm_units = [self.word_lstm_units] * self.word_lstm_layers\n        if len(self.word_lstm_units) != self.word_lstm_layers:\n            raise ValueError(""There should be the same number of lstm layer units and lstm layers"")\n        if self.word_vectorizers is None:\n            self.word_vectorizers = []\n        if self.regularizer is not None:\n            self.regularizer = l2(self.regularizer)\n        if self.verbose > 0:\n            log.info(""{} symbols, {} tags in CharacterTagger"".format(len(self.symbols), len(self.tags)))\n\n    def build(self):\n        """"""Builds the network using Keras.\n        """"""\n        word_inputs = Input(shape=(None, MAX_WORD_LENGTH+2), dtype=""int32"")\n        inputs = [word_inputs]\n        word_outputs = self._build_word_cnn(word_inputs)\n        if len(self.word_vectorizers) > 0:\n            additional_word_inputs = [Input(shape=(None, input_dim), dtype=""float32"")\n                                      for input_dim, dense_dim in self.word_vectorizers]\n            inputs.extend(additional_word_inputs)\n            additional_word_embeddings = [Dense(dense_dim)(additional_word_inputs[i])\n                                          for i, (_, dense_dim) in enumerate(self.word_vectorizers)]\n            word_outputs = Concatenate()([word_outputs] + additional_word_embeddings)\n        outputs, lstm_outputs = self._build_basic_network(word_outputs)\n        compile_args = {""optimizer"": Nadam(lr=0.002, clipnorm=5.0),\n                        ""loss"": ""categorical_crossentropy"", ""metrics"": [""accuracy""]}\n        self.model_ = Model(inputs, outputs)\n        self.model_.compile(**compile_args)\n        if self.verbose > 0:\n            self.model_.summary(print_fn=log.info)\n        return self\n\n    def _build_word_cnn(self, inputs):\n        """"""Builds word-level network\n        """"""\n        inputs = Lambda(K.one_hot, arguments={""num_classes"": len(self.symbols)},\n                        output_shape=lambda x: tuple(x) + (len(self.symbols),))(inputs)\n        char_embeddings = Dense(self.char_embeddings_size, use_bias=False)(inputs)\n        conv_outputs = []\n        self.char_output_dim_ = 0\n        for window_size, filters_number in zip(self.char_window_size, self.char_filters):\n            curr_output = char_embeddings\n            curr_filters_number = (min(self.char_filter_multiple * window_size, 200)\n                                   if filters_number is None else filters_number)\n            for _ in range(self.char_conv_layers - 1):\n                curr_output = Conv2D(curr_filters_number, (1, window_size),\n                                     padding=""same"", activation=""relu"",\n                                     data_format=""channels_last"")(curr_output)\n                if self.conv_dropout > 0.0:\n                    curr_output = Dropout(self.conv_dropout)(curr_output)\n            curr_output = Conv2D(curr_filters_number, (1, window_size),\n                                 padding=""same"", activation=""relu"",\n                                 data_format=""channels_last"")(curr_output)\n            conv_outputs.append(curr_output)\n            self.char_output_dim_ += curr_filters_number\n        if len(conv_outputs) > 1:\n            conv_output = Concatenate(axis=-1)(conv_outputs)\n        else:\n            conv_output = conv_outputs[0]\n        highway_input = Lambda(K.max, arguments={""axis"": -2})(conv_output)\n        if self.intermediate_dropout > 0.0:\n            highway_input = Dropout(self.intermediate_dropout)(highway_input)\n        for i in range(self.char_highway_layers - 1):\n            highway_input = Highway(activation=""relu"")(highway_input)\n            if self.highway_dropout > 0.0:\n                highway_input = Dropout(self.highway_dropout)(highway_input)\n        highway_output = Highway(activation=""relu"")(highway_input)\n        return highway_output\n\n    def _build_basic_network(self, word_outputs):\n        """"""\n        Creates the basic network architecture,\n        transforming word embeddings to intermediate outputs\n        """"""\n        if self.word_dropout > 0.0:\n            lstm_outputs = Dropout(self.word_dropout)(word_outputs)\n        else:\n            lstm_outputs = word_outputs\n        for j in range(self.word_lstm_layers-1):\n            lstm_outputs = Bidirectional(\n                LSTM(self.word_lstm_units[j], return_sequences=True,\n                     dropout=self.lstm_dropout))(lstm_outputs)\n        lstm_outputs = Bidirectional(\n                LSTM(self.word_lstm_units[-1], return_sequences=True,\n                     dropout=self.lstm_dropout))(lstm_outputs)\n        pre_outputs = TimeDistributed(\n                Dense(len(self.tags), activation=""softmax"",\n                      activity_regularizer=self.regularizer),\n                name=""p"")(lstm_outputs)\n        return pre_outputs, lstm_outputs\n\n    # noinspection PyPep8Naming\n    def _transform_batch(self, data, labels=None, transform_to_one_hot=True):\n        data, additional_data = data[0], data[1:]\n        L = max(len(x) for x in data)\n        X = np.array([self._make_sent_vector(x, L) for x in data])\n        X = [X] + [np.array(x) for x in additional_data]\n        if labels is not None:\n            Y = np.array([self._make_tags_vector(y, L) for y in labels])\n            if transform_to_one_hot:\n                Y = to_one_hot(Y, len(self.tags))\n            return X, Y\n        else:\n            return X\n\n    def train_on_batch(self, *args) -> None:\n        """"""Trains the model on a single batch.\n\n        Args:\n            *args: the list of network inputs.\n            Last element of `args` is the batch of targets,\n            all previous elements are training data batches\n        """"""\n        # data: List[Iterable], labels: Iterable[list]\n        # Args:\n        #   data: a batch of word sequences\n        #   labels: a batch of correct tag sequences\n        *data, labels = args\n        # noinspection PyPep8Naming\n        X, Y = self._transform_batch(data, labels)\n        self.model_.train_on_batch(X, Y)\n\n    # noinspection PyPep8Naming\n    def predict_on_batch(self, data: Union[List[np.ndarray], Tuple[np.ndarray]],\n                         return_indexes: bool = False) -> List[List[str]]:\n        """"""\n        Makes predictions on a single batch\n\n        Args:\n            data: model inputs for a single batch, data[0] contains input character encodings\n            and is the only element of data for mist models. Subsequent elements of data\n            include the output of additional vectorizers, e.g., dictionary-based one.\n            return_indexes: whether to return tag indexes in vocabulary or the tags themselves\n\n        Returns:\n            a batch of label sequences\n        """"""\n        X = self._transform_batch(data)\n        objects_number, lengths = len(X[0]), [len(elem) for elem in data[0]]\n        Y = self.model_.predict_on_batch(X)\n        labels = np.argmax(Y, axis=-1)\n        answer: List[Optional[List[str]]] = [None] * objects_number\n        for i, (elem, length) in enumerate(zip(labels, lengths)):\n            elem = elem[:length]\n            answer[i] = elem if return_indexes else self.tags.idxs2toks(elem)\n        return answer\n\n    def __call__(self, *x_batch: np.ndarray, **kwargs) -> Union[List, np.ndarray]:\n        """"""\n        Predicts answers on batch elements.\n\n        Args:\n            x_batch: a batch to predict answers on. It can be either a single array\n                for basic model or a sequence of arrays for a complex one (\n                :config:`configuration file <morpho_tagger/UD2.0/morpho_ru_syntagrus_pymorphy.json>`\n                or its lemmatized version).\n        """"""\n        return self.predict_on_batch(x_batch, **kwargs)\n\n    def _make_sent_vector(self, sent: List, bucket_length: int = None) -> np.ndarray:\n        """"""Transforms a sentence to Numpy array, which will be the network input.\n\n        Args:\n            sent: input sentence\n            bucket_length: the width of the bucket\n\n        Returns:\n            A 3d array, answer[i][j][k] contains the index of k-th letter\n            in j-th word of i-th input sentence.\n        """"""\n        bucket_length = bucket_length or len(sent)\n        answer = np.zeros(shape=(bucket_length, MAX_WORD_LENGTH+2), dtype=np.int32)\n        for i, word in enumerate(sent):\n            answer[i, 0] = self.tags[""BEGIN""]\n            m = min(len(word), MAX_WORD_LENGTH)\n            for j, x in enumerate(word[-m:]):\n                answer[i, j+1] = self.symbols[x]\n            answer[i, m+1] = self.tags[""END""]\n            answer[i, m+2:] = self.tags[""PAD""]\n        return answer\n\n    def _make_tags_vector(self, tags, bucket_length=None) -> np.ndarray:\n        """"""Transforms a sentence of tags to Numpy array, which will be the network target.\n\n        Args:\n            tags: input sentence of tags\n            bucket_length: the width of the bucket\n\n        Returns:\n            A 2d array, answer[i][j] contains the index of j-th tag in i-th input sentence.\n        """"""\n        bucket_length = bucket_length or len(tags)\n        answer = np.zeros(shape=(bucket_length,), dtype=np.int32)\n        for i, tag in enumerate(tags):\n            answer[i] = self.tags[tag]\n        return answer\n'"
deeppavlov/models/nemo/__init__.py,0,b''
deeppavlov/models/nemo/asr.py,0,"b'# Copyright 2020 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nfrom io import BytesIO\nfrom pathlib import Path\nfrom typing import List, Optional, Tuple, Union, Dict\n\nimport torch\nfrom nemo.collections.asr import AudioToMelSpectrogramPreprocessor, JasperEncoder, JasperDecoderForCTC, GreedyCTCDecoder\nfrom nemo.collections.asr.helpers import post_process_predictions\nfrom nemo.collections.asr.parts.features import WaveformFeaturizer\nfrom nemo.core.neural_types import AudioSignal, NeuralType, LengthsType\nfrom nemo.utils.decorators import add_port_docs\nfrom torch import Tensor\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.models.nemo.common import CustomDataLayerBase, NeMoBase\n\nlog = logging.getLogger(__name__)\n\n\nclass AudioInferDataset(Dataset):\n    def __init__(self, audio_batch: List[Union[str, BytesIO]], sample_rate: int, int_values: bool, trim=False) -> None:\n        """"""Dataset reader for AudioInferDataLayer.\n\n        Args:\n            audio_batch: Batch to be read. Elements could be either paths to audio files or Binary I/O objects.\n            sample_rate: Audio files sample rate.\n            int_values: If true, load samples as 32-bit integers.\n            trim: Trim leading and trailing silence from an audio signal if True.\n\n        """"""\n        self.audio_batch = audio_batch\n        self.featurizer = WaveformFeaturizer(sample_rate=sample_rate, int_values=int_values)\n        self.trim = trim\n\n    def __getitem__(self, index: int) -> Tuple[Tensor, Tensor]:\n        """"""Processes audio batch item and extracts features.\n\n        Args:\n            index: Audio batch item index.\n\n        Returns:\n            features: Audio file\'s extracted features tensor.\n            features_length: Features length tensor.\n\n        """"""\n        sample = self.audio_batch[index]\n        features = self.featurizer.process(sample, trim=self.trim)\n        features_length = torch.tensor(features.shape[0]).long()\n\n        return features, features_length\n\n    def __len__(self) -> int:\n        return len(self.audio_batch)\n\n\nclass AudioInferDataLayer(CustomDataLayerBase):\n    """"""Data Layer for ASR pipeline inference.""""""\n\n    @property\n    @add_port_docs()\n    def output_ports(self) -> Dict[str, NeuralType]:\n        return {\n            ""audio_signal"": NeuralType((\'B\', \'T\'), AudioSignal(freq=self._sample_rate)),\n            ""a_sig_length"": NeuralType(tuple(\'B\'), LengthsType())\n        }\n\n    def __init__(self, *,\n                 audio_batch: List[Union[str, BytesIO]],\n                 batch_size: int = 32,\n                 sample_rate: int = 16000,\n                 int_values: bool = False,\n                 trim_silence: bool = False,\n                 **kwargs) -> None:\n        """"""Initializes Data Loader.\n\n        Args:\n            audio_batch: Batch to be read. Elements could be either paths to audio files or Binary I/O objects.\n            batch_size: How many samples per batch to load.\n            sample_rate: Target sampling rate for data. Audio files will be resampled to sample_rate if\n                it is not already.\n            int_values: If true, load data as 32-bit integers.\n            trim_silence: Trim leading and trailing silence from an audio signal if True.\n\n        """"""\n        self._sample_rate = sample_rate\n\n        dataset = AudioInferDataset(audio_batch=audio_batch, sample_rate=sample_rate, int_values=int_values,\n                                    trim=trim_silence)\n\n        dataloader = DataLoader(dataset=dataset, batch_size=batch_size, collate_fn=self.seq_collate_fn)\n        super(AudioInferDataLayer, self).__init__(dataset, dataloader, **kwargs)\n\n    @staticmethod\n    def seq_collate_fn(batch: Tuple[Tuple[Tensor], Tuple[Tensor]]) -> Tuple[Optional[Tensor], Optional[Tensor]]:\n        """"""Collates batch of audio signal and audio length, zero pads audio signal.\n\n        Args:\n            batch: A tuple of tuples of audio signals and signal lengths. This collate function assumes the signals\n                are 1d torch tensors (i.e. mono audio).\n\n        Returns:\n            audio_signal: Zero padded audio signal tensor.\n            audio_length: Audio signal length tensor.\n\n        """"""\n        _, audio_lengths = zip(*batch)\n        max_audio_len = 0\n        has_audio = audio_lengths[0] is not None\n        if has_audio:\n            max_audio_len = max(audio_lengths).item()\n\n        audio_signal = []\n        for sig, sig_len in batch:\n            if has_audio:\n                sig_len = sig_len.item()\n                if sig_len < max_audio_len:\n                    pad = (0, max_audio_len - sig_len)\n                    sig = torch.nn.functional.pad(sig, pad)\n                audio_signal.append(sig)\n\n        if has_audio:\n            audio_signal = torch.stack(audio_signal)\n            audio_lengths = torch.stack(audio_lengths)\n        else:\n            audio_signal, audio_lengths = None, None\n\n        return audio_signal, audio_lengths\n\n\n@register(\'nemo_asr\')\nclass NeMoASR(NeMoBase):\n    """"""ASR model on NeMo modules.""""""\n\n    def __init__(self, load_path: Union[str, Path], nemo_params_path: Union[str, Path], **kwargs) -> None:\n        """"""Initializes NeuralModules for ASR.\n\n        Args:\n            load_path: Path to a directory with pretrained checkpoints for JasperEncoder and JasperDecoderForCTC.\n            nemo_params_path: Path to a file containig labels and params for AudioToMelSpectrogramPreprocessor,\n                JasperEncoder, JasperDecoderForCTC and AudioInferDataLayer.\n\n        """"""\n        super(NeMoASR, self).__init__(load_path=load_path, nemo_params_path=nemo_params_path, **kwargs)\n\n        self.labels = self.nemo_params[\'labels\']\n\n        self.data_preprocessor = AudioToMelSpectrogramPreprocessor(\n            **self.nemo_params[\'AudioToMelSpectrogramPreprocessor\']\n        )\n        self.jasper_encoder = JasperEncoder(**self.nemo_params[\'JasperEncoder\'])\n        self.jasper_decoder = JasperDecoderForCTC(num_classes=len(self.labels), **self.nemo_params[\'JasperDecoder\'])\n        self.greedy_decoder = GreedyCTCDecoder()\n        self.modules_to_restore = [self.jasper_encoder, self.jasper_decoder]\n\n        self.load()\n\n    def __call__(self, audio_batch: List[Union[str, BytesIO]]) -> List[str]:\n        """"""Transcripts audio batch to text.\n\n        Args:\n            audio_batch: Batch to be transcribed. Elements could be either paths to audio files or Binary I/O objects.\n\n        Returns:\n            text_batch: Batch of transcripts.\n\n        """"""\n        data_layer = AudioInferDataLayer(audio_batch=audio_batch, **self.nemo_params[\'AudioToTextDataLayer\'])\n        audio_signal, audio_signal_len = data_layer()\n        processed_signal, processed_signal_len = self.data_preprocessor(input_signal=audio_signal,\n                                                                        length=audio_signal_len)\n        encoded, encoded_len = self.jasper_encoder(audio_signal=processed_signal, length=processed_signal_len)\n        log_probs = self.jasper_decoder(encoder_output=encoded)\n        predictions = self.greedy_decoder(log_probs=log_probs)\n        eval_tensors = [predictions]\n        tensors = self.neural_factory.infer(tensors=eval_tensors)\n        text_batch = post_process_predictions(tensors[0], self.labels)\n\n        return text_batch\n'"
deeppavlov/models/nemo/common.py,0,"b'# Copyright 2020 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport base64\nfrom io import BytesIO\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import Union\n\nimport nemo\nimport torch\nfrom nemo.backends.pytorch import DataLayerNM\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.file import read_yaml\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.core.models.serializable import Serializable\n\nlog = getLogger(__name__)\n\n\n@register(\'base64_decode_bytesIO\')\ndef ascii_to_bytes_io(batch: Union[str, list]) -> Union[BytesIO, list]:\n    """"""Recursively searches for strings in the input batch and converts them into the base64-encoded bytes wrapped in\n    Binary I/O objects.\n\n    Args:\n        batch: A string or an iterable container with strings at some level of nesting.\n\n    Returns:\n        The same structure where all strings are converted into the base64-encoded bytes wrapped in Binary I/O objects.\n\n    """"""\n    if isinstance(batch, str):\n        return BytesIO(base64.decodebytes(batch.encode()))\n\n    return list(map(ascii_to_bytes_io, batch))\n\n\n@register(\'bytesIO_encode_base64\')\ndef bytes_io_to_ascii(batch: Union[BytesIO, list]) -> Union[str, list]:\n    """"""Recursively searches for Binary I/O objects in the input batch and converts them into ASCII-strings.\n\n    Args:\n        batch: A BinaryIO object or an iterable container with BinaryIO objects at some level of nesting.\n\n    Returns:\n        The same structure where all BinaryIO objects are converted into strings.\n\n    """"""\n    if isinstance(batch, BytesIO):\n        return base64.encodebytes(batch.read()).decode(\'ascii\')\n\n    return list(map(bytes_io_to_ascii, batch))\n\n\nclass NeMoBase(Component, Serializable):\n    """"""Base class for NeMo Chainer\'s pipeline components.""""""\n\n    def __init__(self, load_path: Union[str, Path], nemo_params_path: Union[str, Path], **kwargs) -> None:\n        """"""Initializes NeuralModuleFactory on CPU or GPU and reads nemo modules params from yaml.\n\n        Args:\n            load_path: Path to a directory with pretrained checkpoints for NeMo modules.\n            nemo_params_path: Path to a file containig NeMo modules params.\n\n        """"""\n        super(NeMoBase, self).__init__(save_path=None, load_path=load_path, **kwargs)\n        placement = nemo.core.DeviceType.GPU if torch.cuda.is_available() else nemo.core.DeviceType.CPU\n        self.neural_factory = nemo.core.NeuralModuleFactory(placement=placement)\n        self.modules_to_restore = []\n        self.nemo_params = read_yaml(expand_path(nemo_params_path))\n\n    def __call__(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def load(self) -> None:\n        """"""Loads pretrained checkpoints for modules from self.modules_to_restore list.""""""\n        module_names = [str(module) for module in self.modules_to_restore]\n        checkpoints = nemo.utils.get_checkpoint_from_dir(module_names, self.load_path)\n        for module, checkpoint in zip(self.modules_to_restore, checkpoints):\n            log.info(f\'Restoring {module} from {checkpoint}\')\n            module.restore_from(checkpoint)\n\n    def save(self, *args, **kwargs) -> None:\n        pass\n\n\nclass CustomDataLayerBase(DataLayerNM):\n    def __init__(self, dataset: Dataset, dataloader: DataLoader, **kwargs) -> None:\n        super(CustomDataLayerBase, self).__init__()\n        self._dataset = dataset\n        self._dataloader = dataloader\n\n    def __len__(self) -> int:\n        return len(self._dataset)\n\n    @property\n    def dataset(self) -> None:\n        return None\n\n    @property\n    def data_iterator(self) -> torch.utils.data.DataLoader:\n        return self._dataloader\n'"
deeppavlov/models/nemo/tts.py,0,"b'# Copyright 2020 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom functools import partial\nfrom io import BytesIO\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import List, Optional, Tuple, Union, Dict\n\nimport torch\nfrom nemo.collections.asr.parts import collections, parsers\nfrom nemo.collections.asr.parts.dataset import TranscriptDataset\nfrom nemo.collections.tts import TextEmbedding, Tacotron2Encoder, Tacotron2DecoderInfer, Tacotron2Postnet\nfrom nemo.core.neural_types import NeuralType, LabelsType, LengthsType\nfrom nemo.utils.decorators import add_port_docs\nfrom nemo.utils.misc import pad_to\nfrom scipy.io import wavfile\nfrom torch import Tensor\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.models.nemo.common import CustomDataLayerBase, NeMoBase\nfrom deeppavlov.models.nemo.vocoder import WaveGlow, GriffinLim\n\nlog = getLogger(__name__)\n\n\nclass TextDataset(TranscriptDataset):\n    def __init__(self,\n                 text_batch: List[str],\n                 labels: List[str],\n                 bos_id: Optional[int] = None,\n                 eos_id: Optional[int] = None,\n                 lowercase: bool = True) -> None:\n        """"""Text dataset reader for TextDataLayer.\n\n        Args:\n            text_batch: Texts to be used for speech synthesis.\n            labels: List of string labels to use when to str2int translation.\n            bos_id: Label position of beginning of string symbol.\n            eos_id: Label position of end of string symbol.\n            lowercase: Whether to convert all uppercase characters in a text batch into lowercase characters.\n\n        """"""\n        parser = parsers.make_parser(labels, do_lowercase=lowercase)\n        self.texts = collections.Text(text_batch, parser)\n        self.bos_id = bos_id\n        self.eos_id = eos_id\n\n\nclass TextDataLayer(CustomDataLayerBase):\n    @property\n    @add_port_docs()\n    def output_ports(self) -> Dict[str, NeuralType]:\n        return {\n            \'texts\': NeuralType((\'B\', \'T\'), LabelsType()),\n            ""texts_length"": NeuralType(tuple(\'B\'), LengthsType())\n        }\n\n    def __init__(self, *,\n                 text_batch: List[str],\n                 labels: List[str],\n                 batch_size: int = 32,\n                 bos_id: Optional[int] = None,\n                 eos_id: Optional[int] = None,\n                 pad_id: Optional[int] = None,\n                 **kwargs) -> None:\n        """"""A simple Neural Module for loading text data.\n\n        Args:\n            text_batch: Texts to be used for speech synthesis.\n            labels: List of string labels to use when to str2int translation.\n            batch_size: How many strings per batch to load.\n            bos_id: Label position of beginning of string symbol. If None is initialized as `len(labels)`.\n            eos_id: Label position of end of string symbol. If None is initialized as `len(labels) + 1`.\n            pad_id: Label position of pad symbol. If None is initialized as `len(labels) + 2`.\n\n        """"""\n        len_labels = len(labels)\n        if bos_id is None:\n            bos_id = len_labels\n        if eos_id is None:\n            eos_id = len_labels + 1\n        if pad_id is None:\n            pad_id = len_labels + 2\n\n        dataset = TextDataset(text_batch=text_batch, labels=labels, bos_id=bos_id, eos_id=eos_id)\n\n        dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size,\n                                                 collate_fn=partial(self._collate_fn, pad_id=pad_id))\n        super(TextDataLayer, self).__init__(dataset, dataloader, **kwargs)\n\n    @staticmethod\n    def _collate_fn(batch: Tuple[Tuple[Tensor], Tuple[Tensor]], pad_id: int) -> Tuple[Tensor, Tensor]:\n        """"""Collates batch of texts.\n\n        Args:\n            batch: A tuple of tuples of audio signals and signal lengths.\n            pad_id: Label position of pad symbol.\n\n        Returns:\n            texts: Padded texts tensor.\n            texts_len: Text lengths tensor.\n\n        """"""\n        texts_list, texts_len = zip(*batch)\n        max_len = max(texts_len)\n        max_len = pad_to(max_len, 8)\n\n        texts = torch.empty(len(texts_list), max_len, dtype=torch.long)\n        texts.fill_(pad_id)\n\n        for i, text in enumerate(texts_list):\n            texts[i].narrow(0, 0, text.size(0)).copy_(text)\n\n        if len(texts.shape) != 2:\n            raise ValueError(f\'Texts in collate function have shape {texts.shape}, should have 2 dimensions.\')\n\n        return texts, torch.stack(texts_len)\n\n\n@register(\'nemo_tts\')\nclass NeMoTTS(NeMoBase):\n    """"""TTS model on NeMo modules.""""""\n    def __init__(self,\n                 load_path: Union[str, Path],\n                 nemo_params_path: Union[str, Path],\n                 vocoder: str = \'waveglow\',\n                 **kwargs) -> None:\n        """"""Initializes NeuralModules for TTS.\n\n        Args:\n            load_path: Path to a directory with pretrained checkpoints for TextEmbedding, Tacotron2Encoder,\n                Tacotron2DecoderInfer, Tacotron2Postnet and, if Waveglow vocoder is selected, WaveGlowInferNM.\n            nemo_params_path: Path to a file containig sample_rate, labels and params for TextEmbedding,\n                Tacotron2Encoder, Tacotron2Decoder, Tacotron2Postnet and TranscriptDataLayer.\n            vocoder: Vocoder used to convert from spectrograms to audio. Available options: `waveglow` (needs pretrained\n                checkpoint) and `griffin-lim`.\n\n        """"""\n        super(NeMoTTS, self).__init__(load_path=load_path, nemo_params_path=nemo_params_path, **kwargs)\n\n        self.sample_rate = self.nemo_params[\'sample_rate\']\n        self.text_embedding = TextEmbedding(\n            len(self.nemo_params[\'labels\']) + 3,  # + 3 special chars\n            **self.nemo_params[\'TextEmbedding\']\n        )\n        self.t2_enc = Tacotron2Encoder(**self.nemo_params[\'Tacotron2Encoder\'])\n        self.t2_dec = Tacotron2DecoderInfer(**self.nemo_params[\'Tacotron2Decoder\'])\n        self.t2_postnet = Tacotron2Postnet(**self.nemo_params[\'Tacotron2Postnet\'])\n        self.modules_to_restore = [self.text_embedding, self.t2_enc, self.t2_dec, self.t2_postnet]\n\n        if vocoder == \'waveglow\':\n            self.vocoder = WaveGlow(**self.nemo_params[\'WaveGlowNM\'])\n            self.modules_to_restore.append(self.vocoder)\n        elif vocoder == \'griffin-lim\':\n            self.vocoder = GriffinLim(**self.nemo_params[\'GriffinLim\'])\n        else:\n            raise ValueError(f\'{vocoder} vocoder is not supported.\')\n\n        self.load()\n\n    def __call__(self,\n                 text_batch: List[str],\n                 path_batch: Optional[List[str]] = None) -> Union[List[BytesIO], List[str]]:\n        """"""Creates wav files or file objects with speech.\n\n        Args:\n            text_batch: Text from which human audible speech should be generated.\n            path_batch: i-th element of `path_batch` is the path to save i-th generated speech file. If argument isn\'t\n                specified, the synthesized speech will be stored to Binary I/O objects.\n\n        Returns:\n            List of Binary I/O objects with generated speech if `path_batch` was not specified, list of paths to files\n                with synthesized speech otherwise.\n\n        """"""\n        if path_batch is None:\n            path_batch = [BytesIO() for _ in text_batch]\n        elif len(text_batch) != len(path_batch):\n            raise ValueError(\'Text batch length differs from path batch length.\')\n        else:\n            path_batch = [expand_path(path) for path in path_batch]\n\n        data_layer = TextDataLayer(text_batch=text_batch, **self.nemo_params[\'TranscriptDataLayer\'])\n        transcript, transcript_len = data_layer()\n        transcript_embedded = self.text_embedding(char_phone=transcript)\n        transcript_encoded = self.t2_enc(char_phone_embeddings=transcript_embedded, embedding_length=transcript_len)\n        mel_decoder, gate, alignments, mel_len = self.t2_dec(char_phone_encoded=transcript_encoded,\n                                                             encoded_length=transcript_len)\n        mel_postnet = self.t2_postnet(mel_input=mel_decoder)\n        infer_tensors = [self.vocoder(mel_postnet), mel_len]\n        evaluated_tensors = self.neural_factory.infer(tensors=infer_tensors)\n        synthesized_batch = self.vocoder.get_audio(*evaluated_tensors)\n\n        for fout, synthesized_audio in zip(path_batch, synthesized_batch):\n            wavfile.write(fout, self.sample_rate, synthesized_audio)\n\n        return path_batch\n'"
deeppavlov/models/nemo/vocoder.py,0,"b'# Copyright 2020 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import List\n\nimport librosa\nimport numpy as np\nfrom nemo.core.neural_types import NmTensor\nfrom nemo.collections.tts import WaveGlowInferNM\nfrom numpy import ndarray\n\nlog = getLogger(__name__)\n\n\nclass BaseVocoder:\n    """"""Class is used to maintain consistency in the construction of the TTS pipeline based on NeMo modules.""""""\n\n    def __call__(self, tensor: NmTensor) -> NmTensor:\n        """"""Should return the tensor after the evaluation of which speech could be synthesized with `get_audio` method""""""\n        raise NotImplementedError\n\n    def get_audio(self, evaluated_tensor: list, mel_len: list):\n        """"""Synthesizes audio from the evaluated tensor constructed by `__call__` method.""""""\n        raise NotImplementedError\n\n\nclass WaveGlow(BaseVocoder):\n    def __init__(self, *, denoiser_strength: float = 0.0, n_window_stride: int = 160, **kwargs) -> None:\n        """"""Wraps WaveGlowInferNM module.\n\n        Args:\n            denoiser_strength: Denoiser strength for waveglow.\n            n_window_stride: Stride of window for FFT in samples used in model training.\n            kwargs: Named arguments for WaveGlowInferNM constructor.\n\n        """"""\n        self.waveglow = WaveGlowInferNM(**kwargs)\n        self.denoiser_strength = denoiser_strength\n        self.n_window_stride = n_window_stride\n\n    def __call__(self, mel_postnet: NmTensor) -> NmTensor:\n        return self.waveglow(mel_spectrogram=mel_postnet)\n\n    def __str__(self):\n        return str(self.waveglow)\n\n    def restore_from(self, path: str) -> None:\n        """"""Wraps WaveGlowInferNM restore_from method.""""""\n        self.waveglow.restore_from(path)\n        if self.denoiser_strength > 0:\n            log.info(\'Setup denoiser for WaveGlow\')\n            self.waveglow.setup_denoiser()\n\n    def get_audio(self, evaluated_audio: list, mel_len: list) -> List[ndarray]:\n        """"""Unpacks audio data from evaluated tensor and denoises it if `denoiser_strength` > 0.""""""\n        audios = []\n        for i, batch in enumerate(evaluated_audio):\n            audio = batch.cpu().numpy()\n            for j, sample in enumerate(audio):\n                sample_len = mel_len[i][j] * self.n_window_stride\n                sample = sample[:sample_len]\n                if self.denoiser_strength > 0:\n                    sample, _ = self.waveglow.denoise(sample, strength=self.denoiser_strength)\n                audios.append(sample)\n        return audios\n\n\nclass GriffinLim(BaseVocoder):\n    def __init__(self, *,\n                 sample_rate: float = 16000.0,\n                 n_fft: int = 1024,\n                 mag_scale: float = 2048.0,\n                 power: float = 1.2,\n                 n_iters: int = 50,\n                 **kwargs) -> None:\n        """"""Uses Griffin Lim algorithm to generate speech from spectrograms.\n\n        Args:\n            sample_rate:  Generated audio data sample rate.\n            n_fft: The number of points to use for the FFT.\n            mag_scale: Multiplied with the linear spectrogram to avoid audio sounding muted due to mel filter\n                normalization.\n            power: The linear spectrogram is raised to this power prior to running the Griffin Lim algorithm. A power\n                of greater than 1 has been shown to improve audio quality.\n            n_iters: Number of iterations of convertion magnitude spectrograms to audio signal.\n\n        """"""\n        self.mag_scale = mag_scale\n        self.power = power\n        self.n_iters = n_iters\n        self.n_fft = n_fft\n        self.filterbank = librosa.filters.mel(sr=sample_rate, n_fft=n_fft, **kwargs)\n\n    def __call__(self, mel_postnet: NmTensor) -> NmTensor:\n        return mel_postnet\n\n    def get_audio(self, mel_spec: list, mel_len: list) -> List[ndarray]:\n        audios = []\n        for i, batch in enumerate(mel_spec):\n            log_mel = batch.cpu().numpy().transpose(0, 2, 1)\n            mel = np.exp(log_mel)\n            magnitudes = np.dot(mel, self.filterbank) * self.mag_scale\n            for j, sample in enumerate(magnitudes):\n                sample = sample[:mel_len[i][j], :]\n                audio = self.griffin_lim(sample.T ** self.power)\n                audios.append(audio)\n        return audios\n\n    def griffin_lim(self, magnitudes):\n        """"""Griffin-Lim algorithm to convert magnitude spectrograms to audio signals.""""""\n        phase = np.exp(2j * np.pi * np.random.rand(*magnitudes.shape))\n        complex_spec = magnitudes * phase\n        signal = librosa.istft(complex_spec)\n\n        for _ in range(self.n_iters):\n            _, phase = librosa.magphase(librosa.stft(signal, n_fft=self.n_fft))\n            complex_spec = magnitudes * phase\n            signal = librosa.istft(complex_spec)\n        return signal\n'"
deeppavlov/models/ner/NER_model.py,52,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport string\nfrom logging import getLogger\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom gensim.models import KeyedVectors\nfrom tensorflow.contrib.layers import xavier_initializer, xavier_initializer_conv2d\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.tf_model import LRScheduledTFModel\n\nlog = getLogger(__name__)\n\n\n@register(\'hybrid_ner_model\')\nclass HybridNerModel(LRScheduledTFModel):\n    """""" This class implements the hybrid NER model published in the paper: http://www.ijmlc.org/show-83-881-1.html\n\n    Params:\n        n_tags: Number of pre-defined tags.\n        word_emb_path: The path to the pretrained word embedding model.\n        word_emb_name: The name of pretrained word embedding model.\n            One of the two values should be set including \'glove\', \'baomoi\' corresponding to two pre-trained word\n            embedding models: GloVe (https://www.aclweb.org/anthology/D14-1162/)\n            and baomoi (https://github.com/sonvx/word2vecVN). Otherwise, the word lookup table will be trained\n            from scratch.\n        word_vocab: The word vocabulary class.\n        word_dim: The dimension of the pretrained word vector.\n        char_vocab_size: The size of character vocabulary.\n        pos_vocab_size: The size of POS vocabulary.\n        chunk_vocab_size: The size of Chunk vocabulary.\n        char_dim: The dimension of character vector.\n        elmo_dim: The dimension of ELMo-based word vector\n        pos_dim: The dimension of POS vector.\n        chunk_dim: The dimension of Chunk vector.\n        cap_dim: The dimension of capitalization vector.\n        cap_vocab_size: The size of capitalization vocabulary.\n        lstm_hidden_size: The number of units in contextualized Bi-LSTM network\n        drop_out_keep_prob: The probability of keeping hidden state\n    """"""\n\n    def __init__(self,\n                 n_tags: int,\n                 word_vocab,\n                 word_dim: int,\n                 word_emb_path: str,\n                 word_emb_name: str = None,\n                 char_vocab_size: int = None,\n                 pos_vocab_size: int = None,\n                 chunk_vocab_size: int = None,\n                 char_dim: int = None,\n                 elmo_dim: int = None,\n                 pos_dim: int = None,\n                 chunk_dim: int = None,\n                 cap_dim: int = None,\n                 cap_vocab_size: int = 5,\n                 lstm_hidden_size: int = 256,\n                 dropout_keep_prob: float = 0.5,\n                 **kwargs) -> None:\n\n        assert n_tags != 0, \'Number of classes equal 0! It seems that vocabularies is not loaded.\' \\\n                            \' Check that all vocabulary files are downloaded!\'\n\n        if \'learning_rate_drop_div\' not in kwargs:\n            kwargs[\'learning_rate_drop_div\'] = 10.0\n        if \'learning_rate_drop_patience\' not in kwargs:\n            kwargs[\'learning_rate_drop_patience\'] = 5.0\n        if \'clip_norm\' not in kwargs:\n            kwargs[\'clip_norm\'] = 5.0\n        super().__init__(**kwargs)\n\n        word2id = word_vocab.t2i\n\n        self._dropout_ph = tf.placeholder_with_default(dropout_keep_prob, shape=[], name=\'dropout\')\n        self.training_ph = tf.placeholder_with_default(False, shape=[], name=\'is_training\')\n        self._y_ph = tf.placeholder(tf.int32, [None, None], name=\'y_ph\')\n\n        self._xs_ph_list = []\n        self._input_features = []\n\n        # use for word contextualized bi-lstm, elmo\n        self.real_sent_lengths_ph = tf.placeholder(tf.int32, [None], name=""real_sent_lengths"")\n        self._xs_ph_list.append(self.real_sent_lengths_ph)\n\n        # Word emb\n        with tf.variable_scope(""word_emb""):\n            word_ids_ph = tf.placeholder(tf.int32, [None, None], name=""word_ids"")\n            self._xs_ph_list.append(word_ids_ph)\n\n            word_embeddings = self.load_pretrained_word_emb(word_emb_path, word_emb_name, word_dim, word2id)\n\n            word_lookup_table = tf.Variable(word_embeddings, dtype=tf.float32, trainable=True, name=""word_embeddings"")\n            word_emb = tf.nn.embedding_lookup(word_lookup_table, word_ids_ph, name=""embedded_word"")\n            self._input_features.append(word_emb)\n\n        # POS feature\n        if pos_dim is not None:\n            with tf.variable_scope(""pos_emb""):\n                pos_ph = tf.placeholder(tf.int32, [None, None], name=""pos_ids"")\n                self._xs_ph_list.append(pos_ph)\n\n                tf_pos_embeddings = tf.get_variable(name=""pos_embeddings"",\n                                                    dtype=tf.float32,\n                                                    shape=[pos_vocab_size, pos_dim],\n                                                    trainable=True,\n                                                    initializer=xavier_initializer())\n\n                embedded_pos = tf.nn.embedding_lookup(tf_pos_embeddings,\n                                                      pos_ph,\n                                                      name=""embedded_pos"")\n                self._input_features.append(embedded_pos)\n\n        # Chunk feature\n        if chunk_dim is not None:\n            with tf.variable_scope(""chunk_emb""):\n                chunk_ph = tf.placeholder(tf.int32, [None, None], name=""chunk_ids"")\n                self._xs_ph_list.append(chunk_ph)\n\n                tf_chunk_embeddings = tf.get_variable(name=""chunk_embeddings"",\n                                                      dtype=tf.float32,\n                                                      shape=[chunk_vocab_size, chunk_dim],\n                                                      trainable=True,\n                                                      initializer=xavier_initializer())\n\n                embedded_chunk = tf.nn.embedding_lookup(tf_chunk_embeddings,\n                                                        chunk_ph,\n                                                        name=""embedded_chunk"")\n                self._input_features.append(embedded_chunk)\n\n        # Capitalization feature\n        if cap_dim is not None:\n            with tf.variable_scope(""cap_emb""):\n                cap_ph = tf.placeholder(tf.int32, [None, None], name=""cap_ids"")\n                self._xs_ph_list.append(cap_ph)\n\n                tf_cap_embeddings = tf.get_variable(name=""cap_embeddings"",\n                                                    dtype=tf.float32,\n                                                    shape=[cap_vocab_size, cap_dim],\n                                                    trainable=True,\n                                                    initializer=xavier_initializer())\n\n                embedded_cap = tf.nn.embedding_lookup(tf_cap_embeddings,\n                                                      cap_ph,\n                                                      name=""embedded_cap"")\n                self._input_features.append(embedded_cap)\n\n        # Character feature\n        if char_dim is not None:\n            with tf.variable_scope(""char_emb""):\n                char_ids_ph = tf.placeholder(tf.int32, [None, None, None], name=""char_ids"")\n                self._xs_ph_list.append(char_ids_ph)\n\n                tf_char_embeddings = tf.get_variable(name=""char_embeddings"",\n                                                     dtype=tf.float32,\n                                                     shape=[char_vocab_size, char_dim],\n                                                     trainable=True,\n                                                     initializer=xavier_initializer())\n                embedded_cnn_chars = tf.nn.embedding_lookup(tf_char_embeddings,\n                                                            char_ids_ph,\n                                                            name=""embedded_cnn_chars"")\n                conv1 = tf.layers.conv2d(inputs=embedded_cnn_chars,\n                                         filters=128,\n                                         kernel_size=(1, 3),\n                                         strides=(1, 1),\n                                         padding=""same"",\n                                         name=""conv1"",\n                                         kernel_initializer=xavier_initializer_conv2d())\n                conv2 = tf.layers.conv2d(inputs=conv1,\n                                         filters=128,\n                                         kernel_size=(1, 3),\n                                         strides=(1, 1),\n                                         padding=""same"",\n                                         name=""conv2"",\n                                         kernel_initializer=xavier_initializer_conv2d())\n                char_cnn = tf.reduce_max(conv2, axis=2)\n\n                self._input_features.append(char_cnn)\n\n        # ELMo\n        if elmo_dim is not None:\n            with tf.variable_scope(""elmo_emb""):\n                padded_x_tokens_ph = tf.placeholder(tf.string, [None, None], name=""padded_x_tokens"")\n                self._xs_ph_list.append(padded_x_tokens_ph)\n\n                elmo = hub.Module(""https://tfhub.dev/google/elmo/2"", trainable=True)\n                emb = elmo(inputs={""tokens"": padded_x_tokens_ph, ""sequence_len"": self.real_sent_lengths_ph},\n                           signature=""tokens"", as_dict=True)[""elmo""]\n                elmo_emb = tf.layers.dense(emb, elmo_dim, activation=None)\n                self._input_features.append(elmo_emb)\n\n        features = tf.nn.dropout(tf.concat(self._input_features, axis=2), self._dropout_ph)\n\n        with tf.variable_scope(""bi_lstm_words""):\n            cell_fw = tf.contrib.rnn.LSTMCell(lstm_hidden_size)\n            cell_bw = tf.contrib.rnn.LSTMCell(lstm_hidden_size)\n            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, features,\n                                                                        sequence_length=self.real_sent_lengths_ph,\n                                                                        dtype=tf.float32)\n            self.output = tf.concat([output_fw, output_bw], axis=-1)\n\n            ntime_steps = tf.shape(self.output)[1]\n            self.output = tf.reshape(self.output, [-1, 2 * lstm_hidden_size])\n            layer1 = tf.nn.dropout(tf.layers.dense(inputs=self.output, units=lstm_hidden_size, activation=None,\n                                                   kernel_initializer=xavier_initializer()), self._dropout_ph)\n            pred = tf.layers.dense(inputs=layer1, units=n_tags, activation=None,\n                                   kernel_initializer=xavier_initializer())\n            self.logits = tf.reshape(pred, [-1, ntime_steps, n_tags])\n\n            log_likelihood, self.transition_params = tf.contrib.crf.crf_log_likelihood(self.logits,\n                                                                                       self._y_ph,\n                                                                                       self.real_sent_lengths_ph)\n        # loss and opt\n        with tf.variable_scope(""loss_and_opt""):\n            self.loss = tf.reduce_mean(-log_likelihood)\n            self.train_op = self.get_train_op(self.loss)\n\n        self.sess = tf.Session()\n        self.sess.run(tf.global_variables_initializer())\n        self.load()\n\n    def predict(self, xs):\n        feed_dict = self._fill_feed_dict(xs)\n        logits, trans_params, sent_lengths = self.sess.run([self.logits,\n                                                            self.transition_params,\n                                                            self.real_sent_lengths_ph],\n                                                           feed_dict=feed_dict)\n        # iterate over the sentences because no batching in viterbi_decode\n        y_pred = []\n        for logit, sequence_length in zip(logits, sent_lengths):\n            logit = logit[:int(sequence_length)]  # keep only the valid steps\n            viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(logit, trans_params)\n            y_pred += [viterbi_seq]\n        return y_pred\n\n    def _fill_feed_dict(self, xs, y=None, train=False):\n        assert len(xs) == len(self._xs_ph_list)\n        xs = list(xs)\n        for x in xs[1:]:\n            x = np.array(x)\n        feed_dict = {ph: x for ph, x in zip(self._xs_ph_list, xs)}\n        if y is not None:\n            feed_dict[self._y_ph] = y\n        feed_dict[self.training_ph] = train\n        if not train:\n            feed_dict[self._dropout_ph] = 1.0\n\n        return feed_dict\n\n    def __call__(self, *args, **kwargs):\n        if len(args[0]) == 0 or (args[0] == [0]):\n            return []\n        return self.predict(args)\n\n    def train_on_batch(self, *args):\n        *xs, y = args\n        feed_dict = self._fill_feed_dict(xs, y, train=True)\n        _, loss_value = self.sess.run([self.train_op, self.loss], feed_dict)\n        return {\'loss\': loss_value,\n                \'learning_rate\': self.get_learning_rate(),\n                \'momentum\': self.get_momentum()}\n\n    def load_pretrained_word_emb(self, model_path, model_name, word_dim, word2id=None, vocab_size=None):\n        loaded_words = 0\n        if word2id is not None:\n            vocab_size = len(word2id)\n        word_embeddings = np.zeros(shape=(vocab_size, word_dim))\n\n        if model_name == ""glove"":\n            model = KeyedVectors.load_word2vec_format(model_path, binary=False)\n            for word in word2id:\n                if word in model:\n                    word_embeddings[word2id[word]] = model[word]\n                    loaded_words += 1\n        elif model_name == ""baomoi"":\n            model = KeyedVectors.load_word2vec_format(model_path, binary=True, unicode_errors=\'ignore\')\n            for word in word2id:\n                if len(word) == 1:\n                    if word[0] in string.punctuation:\n                        word_embeddings[word2id[word]] = model[""<punct>""]\n                        loaded_words += 1\n                elif word.isdigit():\n                    word_embeddings[word2id[word]] = model[""<number>""]\n                    loaded_words += 1\n                elif word in model.vocab:\n                    word_embeddings[word2id[word]] = model[word]\n                    loaded_words += 1\n        elif model_name is not None:\n            raise RuntimeError(f\'got an unexpected value for model_name: `{model_name}`\')\n\n        log.info(f""{loaded_words}/{vocab_size} words were loaded from {model_path}."")\n        return word_embeddings\n'"
deeppavlov/models/ner/__init__.py,0,b''
deeppavlov/models/ner/bio.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import List\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\n\nlog = getLogger(__name__)\n\n\n@register(\'ner_bio_converter\')\nclass BIOMarkupRestorer(Component):\n    """"""Restores BIO markup for tags batch""""""\n\n    def __init__(self, *args, **kwargs) -> None:\n        pass\n\n    @staticmethod\n    def _convert_to_bio(tags: List[str]) -> List[str]:\n        tags_bio = []\n        for n, tag in enumerate(tags):\n            if tag != \'O\':\n                if n > 0 and tags[n - 1] == tag:\n                    tag = \'I-\' + tag\n                else:\n                    tag = \'B-\' + tag\n            tags_bio.append(tag)\n\n        return tags_bio\n\n    def __call__(self, tag_batch: List[List[str]], *args, **kwargs) -> List[List[str]]:\n        y = [self._convert_to_bio(sent) for sent in tag_batch]\n        return y\n'"
deeppavlov/models/ner/network.py,32,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import Tuple\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.layers.tf_layers import cudnn_bi_lstm, cudnn_bi_gru, bi_rnn, stacked_cnn, INITIALIZER\nfrom deeppavlov.core.layers.tf_layers import embedding_layer, character_embedding_network, variational_dropout\nfrom deeppavlov.core.models.tf_model import LRScheduledTFModel\n\nlog = getLogger(__name__)\n\n\n@register(\'ner\')\nclass NerNetwork(LRScheduledTFModel):\n    """"""\n    The :class:`~deeppavlov.models.ner.network.NerNetwork` is for Neural Named Entity Recognition and Slot Filling.\n\n    Parameters:\n        n_tags: Number of tags in the tag vocabulary.\n        token_emb_dim: Dimensionality of token embeddings, needed if embedding matrix is not provided.\n        char_emb_dim: Dimensionality of token embeddings.\n        capitalization_dim : Dimensionality of capitalization features, if they are provided.\n        pos_features_dim: Dimensionality of POS features, if they are provided.\n        additional_features: Some other features.\n        net_type: Type of the network, either ``\'rnn\'`` or ``\'cnn\'``.\n        cell_type: Type of the cell in RNN, either ``\'lstm\'`` or ``\'gru\'``.\n        use_cudnn_rnn: Whether to use CUDNN implementation of RNN.\n        two_dense_on_top: Additional dense layer before predictions.\n        n_hidden_list: A list of output feature dimensionality for each layer. A value (100, 200) means that there will\n            be two layers with 100 and 200 units, respectively.\n        cnn_filter_width: The width of the convolutional kernel for Convolutional Neural Networks.\n        use_crf: Whether to use Conditional Random Fields on top of the network (recommended).\n        token_emb_mat: Token embeddings matrix.\n        char_emb_mat: Character embeddings matrix.\n        use_batch_norm: Whether to use Batch Normalization or not. Affects only CNN networks.\n        dropout_keep_prob: Probability of keeping the hidden state, values from 0 to 1. 0.5 works well in most cases.\n        embeddings_dropout: Whether to use dropout on embeddings or not.\n        top_dropout: Whether to use dropout on output units of the network or not.\n        intra_layer_dropout: Whether to use dropout between layers or not.\n        l2_reg: L2 norm regularization for all kernels.\n        gpu: Number of gpu to use.\n        seed: Random seed.\n    """"""\n    GRAPH_PARAMS = [""n_tags"",  # TODO: add check\n                    ""char_emb_dim"",\n                    ""capitalization_dim"",\n                    ""additional_features"",\n                    ""use_char_embeddings"",\n                    ""additional_features"",\n                    ""net_type"",\n                    ""cell_type"",\n                    ""char_filter_width"",\n                    ""cell_type""]\n\n    def __init__(self,\n                 n_tags: int,  # Features dimensions\n                 token_emb_dim: int = None,\n                 char_emb_dim: int = None,\n                 capitalization_dim: int = None,\n                 pos_features_dim: int = None,\n                 additional_features: int = None,\n                 net_type: str = \'rnn\',  # Net architecture\n                 cell_type: str = \'lstm\',\n                 use_cudnn_rnn: bool = False,\n                 two_dense_on_top: bool = False,\n                 n_hidden_list: Tuple[int] = (128,),\n                 cnn_filter_width: int = 7,\n                 use_crf: bool = False,\n                 token_emb_mat: np.ndarray = None,\n                 char_emb_mat: np.ndarray = None,\n                 use_batch_norm: bool = False,\n                 dropout_keep_prob: float = 0.5,  # Regularization\n                 embeddings_dropout: bool = False,\n                 top_dropout: bool = False,\n                 intra_layer_dropout: bool = False,\n                 l2_reg: float = 0.0,\n                 gpu: int = None,\n                 seed: int = None,\n                 **kwargs) -> None:\n        tf.set_random_seed(seed)\n        np.random.seed(seed)\n\n        assert n_tags != 0, \'Number of classes equal 0! It seems that vocabularies is not loaded.\' \\\n                            \' Check that all vocabulary files are downloaded!\'\n\n        if \'learning_rate_drop_div\' not in kwargs:\n            kwargs[\'learning_rate_drop_div\'] = 10.0\n        if \'learning_rate_drop_patience\' not in kwargs:\n            kwargs[\'learning_rate_drop_patience\'] = 5.0\n        if \'clip_norm\' not in kwargs:\n            kwargs[\'clip_norm\'] = 5.0\n        super().__init__(**kwargs)\n        self._add_training_placeholders(dropout_keep_prob)\n        self._xs_ph_list = []\n        self._y_ph = tf.placeholder(tf.int32, [None, None], name=\'y_ph\')\n        self._input_features = []\n\n        # ================ Building input features =================\n\n        # Token embeddings\n        self._add_word_embeddings(token_emb_mat, token_emb_dim)\n\n        # Masks for different lengths utterances\n        self.mask_ph = self._add_mask()\n\n        # Char embeddings using highway CNN with max pooling\n        if char_emb_mat is not None and char_emb_dim is not None:\n            self._add_char_embeddings(char_emb_mat)\n\n        # Capitalization features\n        if capitalization_dim is not None:\n            self._add_capitalization(capitalization_dim)\n\n        # Part of speech features\n        if pos_features_dim is not None:\n            self._add_pos(pos_features_dim)\n\n        # Anything you want\n        if additional_features is not None:\n            self._add_additional_features(additional_features)\n\n        features = tf.concat(self._input_features, axis=2)\n        if embeddings_dropout:\n            features = variational_dropout(features, self._dropout_ph)\n\n        # ================== Building the network ==================\n\n        if net_type == \'rnn\':\n            if use_cudnn_rnn:\n                if l2_reg > 0:\n                    log.warning(\'cuDNN RNN are not l2 regularizable\')\n                units = self._build_cudnn_rnn(features, n_hidden_list, cell_type, intra_layer_dropout, self.mask_ph)\n            else:\n                units = self._build_rnn(features, n_hidden_list, cell_type, intra_layer_dropout, self.mask_ph)\n        elif net_type == \'cnn\':\n            units = self._build_cnn(features, n_hidden_list, cnn_filter_width, use_batch_norm)\n        self._logits = self._build_top(units, n_tags, n_hidden_list[-1], top_dropout, two_dense_on_top)\n\n        self.train_op, self.loss = self._build_train_predict(self._logits, self.mask_ph, n_tags,\n                                                             use_crf, l2_reg)\n        self.predict = self.predict_crf if use_crf else self.predict_no_crf\n\n        # ================= Initialize the session =================\n\n        sess_config = tf.ConfigProto(allow_soft_placement=True)\n        sess_config.gpu_options.allow_growth = True\n        if gpu is not None:\n            sess_config.gpu_options.visible_device_list = str(gpu)\n        self.sess = tf.Session(config=sess_config)\n        self.sess.run(tf.global_variables_initializer())\n        self.load()\n\n    def _add_training_placeholders(self, dropout_keep_prob):\n        self._dropout_ph = tf.placeholder_with_default(dropout_keep_prob, shape=[], name=\'dropout\')\n        self.training_ph = tf.placeholder_with_default(False, shape=[], name=\'is_training\')\n\n    def _add_word_embeddings(self, token_emb_mat, token_emb_dim=None):\n        if token_emb_mat is None:\n            token_ph = tf.placeholder(tf.float32, [None, None, token_emb_dim], name=\'Token_Ind_ph\')\n            emb = token_ph\n        else:\n            token_ph = tf.placeholder(tf.int32, [None, None], name=\'Token_Ind_ph\')\n            emb = embedding_layer(token_ph, token_emb_mat)\n        self._xs_ph_list.append(token_ph)\n        self._input_features.append(emb)\n\n    def _add_mask(self):\n        mask_ph = tf.placeholder(tf.float32, [None, None], name=\'Mask_ph\')\n        self._xs_ph_list.append(mask_ph)\n        return mask_ph\n\n    def _add_char_embeddings(self, char_emb_mat):\n        character_indices_ph = tf.placeholder(tf.int32, [None, None, None], name=\'Char_ph\')\n        char_embs = character_embedding_network(character_indices_ph, emb_mat=char_emb_mat)\n        self._xs_ph_list.append(character_indices_ph)\n        self._input_features.append(char_embs)\n\n    def _add_capitalization(self, capitalization_dim):\n        capitalization_ph = tf.placeholder(tf.float32, [None, None, capitalization_dim], name=\'Capitalization_ph\')\n        self._xs_ph_list.append(capitalization_ph)\n        self._input_features.append(capitalization_ph)\n\n    def _add_pos(self, pos_features_dim):\n        pos_ph = tf.placeholder(tf.float32, [None, None, pos_features_dim], name=\'POS_ph\')\n        self._xs_ph_list.append(pos_ph)\n        self._input_features.append(pos_ph)\n\n    def _add_additional_features(self, features_list):\n        for feature, dim in features_list:\n            feat_ph = tf.placeholder(tf.float32, [None, None, dim], name=feature + \'_ph\')\n            self._xs_ph_list.append(feat_ph)\n            self._input_features.append(feat_ph)\n\n    def _build_cudnn_rnn(self, units, n_hidden_list, cell_type, intra_layer_dropout, mask):\n        sequence_lengths = tf.to_int32(tf.reduce_sum(mask, axis=1))\n        for n, n_hidden in enumerate(n_hidden_list):\n            with tf.variable_scope(cell_type.upper() + \'_\' + str(n)):\n                if cell_type.lower() == \'lstm\':\n                    units, _ = cudnn_bi_lstm(units, n_hidden, sequence_lengths)\n                elif cell_type.lower() == \'gru\':\n                    units, _ = cudnn_bi_gru(units, n_hidden, sequence_lengths)\n                else:\n                    raise RuntimeError(\'Wrong cell type ""{}""! Only ""gru"" and ""lstm""!\'.format(cell_type))\n                units = tf.concat(units, -1)\n                if intra_layer_dropout and n != len(n_hidden_list) - 1:\n                    units = variational_dropout(units, self._dropout_ph)\n            return units\n\n    def _build_rnn(self, units, n_hidden_list, cell_type, intra_layer_dropout, mask):\n        sequence_lengths = tf.to_int32(tf.reduce_sum(mask, axis=1))\n        for n, n_hidden in enumerate(n_hidden_list):\n            units, _ = bi_rnn(units, n_hidden, cell_type=cell_type,\n                              seq_lengths=sequence_lengths, name=\'Layer_\' + str(n))\n            units = tf.concat(units, -1)\n            if intra_layer_dropout and n != len(n_hidden_list) - 1:\n                units = variational_dropout(units, self._dropout_ph)\n        return units\n\n    def _build_cnn(self, units, n_hidden_list, cnn_filter_width, use_batch_norm):\n        units = stacked_cnn(units, n_hidden_list, cnn_filter_width, use_batch_norm, training_ph=self.training_ph)\n        return units\n\n    def _build_top(self, units, n_tags, n_hididden, top_dropout, two_dense_on_top):\n        if top_dropout:\n            units = variational_dropout(units, self._dropout_ph)\n        if two_dense_on_top:\n            units = tf.layers.dense(units, n_hididden, activation=tf.nn.relu,\n                                    kernel_initializer=INITIALIZER(),\n                                    kernel_regularizer=tf.nn.l2_loss)\n        logits = tf.layers.dense(units, n_tags, activation=None,\n                                 kernel_initializer=INITIALIZER(),\n                                 kernel_regularizer=tf.nn.l2_loss)\n        return logits\n\n    def _build_train_predict(self, logits, mask, n_tags, use_crf, l2_reg):\n        if use_crf:\n            sequence_lengths = tf.reduce_sum(mask, axis=1)\n            log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(logits, self._y_ph, sequence_lengths)\n            loss_tensor = -log_likelihood\n            self._transition_params = transition_params\n        else:\n            ground_truth_labels = tf.one_hot(self._y_ph, n_tags)\n            loss_tensor = tf.nn.softmax_cross_entropy_with_logits(labels=ground_truth_labels, logits=logits)\n            loss_tensor = loss_tensor * mask\n            self._y_pred = tf.argmax(logits, axis=-1)\n\n        loss = tf.reduce_mean(loss_tensor)\n\n        # L2 regularization\n        if l2_reg > 0:\n            loss += l2_reg * tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n\n        train_op = self.get_train_op(loss)\n        return train_op, loss\n\n    def predict_no_crf(self, xs):\n        feed_dict = self._fill_feed_dict(xs)\n        pred_idxs, mask = self.sess.run([self._y_pred, self.mask_ph], feed_dict)\n\n        # Filter by sequece length\n        sequence_lengths = np.sum(mask, axis=1).astype(np.int32)\n        pred = []\n        for utt, l in zip(pred_idxs, sequence_lengths):\n            pred.append(utt[:l])\n        return pred\n\n    def predict_crf(self, xs):\n        feed_dict = self._fill_feed_dict(xs)\n        logits, trans_params, mask = self.sess.run([self._logits,\n                                                    self._transition_params,\n                                                    self.mask_ph],\n                                                   feed_dict=feed_dict)\n        sequence_lengths = np.maximum(np.sum(mask, axis=1).astype(np.int32), 1)\n        # iterate over the sentences because no batching in viterbi_decode\n        y_pred = []\n        for logit, sequence_length in zip(logits, sequence_lengths):\n            logit = logit[:int(sequence_length)]  # keep only the valid steps\n            viterbi_seq, viterbi_score = tf.contrib.crf.viterbi_decode(logit, trans_params)\n            y_pred += [viterbi_seq]\n        return y_pred\n\n    def _fill_feed_dict(self, xs, y=None, train=False):\n        assert len(xs) == len(self._xs_ph_list)\n        xs = list(xs)\n        xs[0] = np.array(xs[0])\n        feed_dict = {ph: x for ph, x in zip(self._xs_ph_list, xs)}\n        if y is not None:\n            feed_dict[self._y_ph] = y\n        feed_dict[self.training_ph] = train\n        if not train:\n            feed_dict[self._dropout_ph] = 1.0\n        return feed_dict\n\n    def __call__(self, *args, **kwargs):\n        if len(args[0]) == 0 or (len(args[0]) == 1 and len(args[0][0]) == 0):\n            return []\n        return self.predict(args)\n\n    def train_on_batch(self, *args):\n        *xs, y = args\n        feed_dict = self._fill_feed_dict(xs, y, train=True)\n        _, loss_value = self.sess.run([self.train_op, self.loss], feed_dict)\n        return {\'loss\': loss_value,\n                \'learning_rate\': self.get_learning_rate(),\n                \'momentum\': self.get_momentum()}\n\n    def process_event(self, event_name, data):\n        super().process_event(event_name, data)\n'"
deeppavlov/models/ner/svm.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pickle\nfrom itertools import chain\nfrom logging import getLogger\nfrom typing import List, Union\n\nimport numpy as np\nfrom sklearn.svm import SVC\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.estimator import Estimator\n\nlog = getLogger(__name__)\n\n\n@register(\'ner_svm\')\nclass SVMTagger(Estimator):\n    """"""\n    ``SVM`` (Support Vector Machines) classifier for tagging sequences\n\n    Parameters:\n        return_probabilities: whether to return probabilities or predictions\n        kernel: kernel of SVM (RBF works well in the most of the cases)\n        seed: seed for SVM initialization\n    """"""\n\n    def __init__(self, return_probabilities: bool = False, kernel: str = \'rbf\', seed=42, *args, **kwargs) -> None:\n        super().__init__(*args, **kwargs)\n        self.classifier = None\n        self.return_probabilities = return_probabilities\n        self._kernel = kernel\n        self._seed = seed\n\n        self.load()\n\n    def fit(self, tokens: List[List[str]], tags: List[List[int]], *args, **kwargs) -> None:\n        tokens = list(chain(*tokens))\n        tags = list(chain(*tags))\n        self.classifier = SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                              decision_function_shape=\'ovr\', degree=3, gamma=\'auto\',\n                              kernel=self._kernel, max_iter=-1, probability=self.return_probabilities,\n                              random_state=self._seed, shrinking=True, tol=0.001, verbose=False)\n        self.classifier.fit(tokens, tags)\n\n    def __call__(self, token_vectors_batch: List[List[str]], *args, **kwargs) -> \\\n            Union[List[List[int]], List[List[np.ndarray]]]:\n        lens = [len(utt) for utt in token_vectors_batch]\n        token_vectors_list = list(chain(*token_vectors_batch))\n        predictions = self.classifier.predict(token_vectors_list)\n        y = []\n        cl = 0\n        for l in lens:\n            y.append(predictions[cl: cl + l])\n            cl += l\n        return y\n\n    def save(self) -> None:\n        with self.save_path.open(\'wb\') as f:\n            pickle.dump(self.classifier, f, protocol=4)\n\n    def serialize(self):\n        return pickle.dumps(self.classifier, protocol=4)\n\n    def load(self) -> None:\n        if self.load_path.exists():\n            with self.load_path.open(\'rb\') as f:\n                self.classifier = pickle.load(f)\n\n    def deserialize(self, data):\n        self.classifier = pickle.loads(data)\n'"
deeppavlov/models/preprocessors/__init__.py,0,b''
deeppavlov/models/preprocessors/assemble_embeddings_matrix.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.simple_vocab import SimpleVocabulary\nfrom deeppavlov.models.embedders.abstract_embedder import Embedder\n\n\n@register(\'emb_mat_assembler\')\nclass EmbeddingsMatrixAssembler:\n    """"""For a given Vocabulary assembles matrix of embeddings obtained from some `Embedder`. This\n        class also can assemble embeddins of characters using\n\n    Args:\n        embedder: an instance of the class that convertes tokens to vectors.\n            For example :class:`~deeppavlov.models.embedders.fasttext_embedder.FasttextEmbedder` or\n            :class:`~deeppavlov.models.embedders.glove_embedder.GloVeEmbedder`\n        vocab: instance of :class:`~deeppavlov.core.data.SimpleVocab`. The matrix of embeddings\n            will be assembled relying on every token in the vocabulary. the indexing will match\n            vocabulary indexing.\n        character_level: whether to perform assembling on character level. This procedure will\n            assemble matrix with embeddings for every character using averaged embeddings of\n            words, that contain this character.\n        emb_dim: dimensionality of the resulting embeddings. If not ``None`` it should be less\n            or equal to the dimensionality of the embeddings provided by `Embedder`. The\n            reduction of dimensionality is performed by taking main components of PCA.\n        estimate_by_n: how much samples to use to estimate covariance matrix for PCA.\n            10000 seems to be enough.\n\n    Attributes:\n        dim: dimensionality of the embeddings (can be less than dimensionality of\n            embeddings produced by `Embedder`.\n    """"""\n\n    def __init__(self,\n                 embedder: Embedder,\n                 vocab: SimpleVocabulary,\n                 character_level: bool = False,\n                 emb_dim: int = None,\n                 estimate_by_n: int = 10000,\n                 *args,\n                 **kwargs) -> None:\n        if emb_dim is None:\n            emb_dim = embedder.dim\n        self.emb_mat = np.zeros([len(vocab), emb_dim], dtype=np.float32)\n        tokens_for_estimation = list(embedder)[:estimate_by_n]\n        estimation_matrix = np.array([embedder([[word]])[0][0] for word in tokens_for_estimation], dtype=np.float32)\n        emb_std = np.std(estimation_matrix)\n\n        if emb_dim < embedder.dim:\n            pca = PCA(n_components=emb_dim)\n            pca.fit(estimation_matrix)\n        elif emb_dim > embedder.dim:\n            raise RuntimeError(f\'Model dimension must be greater than requested embeddings \'\n                               f\'dimension! model_dim = {embedder.dim}, requested_dim = {emb_dim}\')\n        else:\n            pca = None\n        for n, token in enumerate(vocab):\n            if character_level:\n                char_in_word_bool = np.array([token in word for word in tokens_for_estimation], dtype=bool)\n                all_words_with_character = estimation_matrix[char_in_word_bool]\n                if len(all_words_with_character) != 0:\n                    if pca is not None:\n                        all_words_with_character = pca.transform(all_words_with_character)\n                    self.emb_mat[n] = sum(all_words_with_character) / len(all_words_with_character)\n                else:\n                    self.emb_mat[n] = np.random.randn(emb_dim) * np.std(self.emb_mat[:n])\n            else:\n                try:\n                    if pca is not None:\n                        self.emb_mat[n] = pca.transform(embedder([[token]])[0])[0]\n                    else:\n                        self.emb_mat[n] = embedder([[token]])[0][0]\n                except KeyError:\n                    self.emb_mat[n] = np.random.randn(emb_dim) * emb_std\n\n    @property\n    def dim(self):\n        return self.emb_mat.shape[1]\n'"
deeppavlov/models/preprocessors/bert_preprocessor.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport re\nimport random\nfrom logging import getLogger\nfrom typing import Tuple, List, Optional, Union\n\nfrom bert_dp.preprocessing import convert_examples_to_features, InputExample, InputFeatures\nfrom bert_dp.tokenization import FullTokenizer\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.utils import zero_pad\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.models.preprocessors.mask import Mask\n\nlog = getLogger(__name__)\n\n\n@register(\'bert_preprocessor\')\nclass BertPreprocessor(Component):\n    """"""Tokenize text on subtokens, encode subtokens with their indices, create tokens and segment masks.\n\n    Check details in :func:`bert_dp.preprocessing.convert_examples_to_features` function.\n\n    Args:\n        vocab_file: path to vocabulary\n        do_lower_case: set True if lowercasing is needed\n        max_seq_length: max sequence length in subtokens, including [SEP] and [CLS] tokens\n\n    Attributes:\n        max_seq_length: max sequence length in subtokens, including [SEP] and [CLS] tokens\n        tokenizer: instance of Bert FullTokenizer\n    """"""\n\n    def __init__(self,\n                 vocab_file: str,\n                 do_lower_case: bool = True,\n                 max_seq_length: int = 512,\n                 **kwargs) -> None:\n        self.max_seq_length = max_seq_length\n        vocab_file = str(expand_path(vocab_file))\n        self.tokenizer = FullTokenizer(vocab_file=vocab_file,\n                                       do_lower_case=do_lower_case)\n\n    def __call__(self, texts_a: List[str], texts_b: Optional[List[str]] = None) -> List[InputFeatures]:\n        """"""Call Bert :func:`bert_dp.preprocessing.convert_examples_to_features` function to tokenize and create masks.\n\n        texts_a and texts_b are separated by [SEP] token\n\n        Args:\n            texts_a: list of texts,\n            texts_b: list of texts, it could be None, e.g. single sentence classification task\n\n        Returns:\n            batch of :class:`bert_dp.preprocessing.InputFeatures` with subtokens, subtoken ids, subtoken mask, segment mask.\n\n        """"""\n\n        if texts_b is None:\n            texts_b = [None] * len(texts_a)\n        # unique_id is not used\n        examples = [InputExample(unique_id=0, text_a=text_a, text_b=text_b)\n                    for text_a, text_b in zip(texts_a, texts_b)]\n        return convert_examples_to_features(examples, self.max_seq_length, self.tokenizer)\n\n\n@register(\'bert_ner_preprocessor\')\nclass BertNerPreprocessor(Component):\n    """"""Takes tokens and splits them into bert subtokens, encodes subtokens with their indices.\n    Creates a mask of subtokens (one for the first subtoken, zero for the others).\n\n    If tags are provided, calculates tags for subtokens.\n\n    Args:\n        vocab_file: path to vocabulary\n        do_lower_case: set True if lowercasing is needed\n        max_seq_length: max sequence length in subtokens, including [SEP] and [CLS] tokens\n        max_subword_length: replace token to <unk> if it\'s length is larger than this\n            (defaults to None, which is equal to +infinity)\n        token_masking_prob: probability of masking token while training\n        provide_subword_tags: output tags for subwords or for words\n        subword_mask_mode: subword to select inside word tokens, can be ""first"" or ""last""\n            (default=""first"")\n\n    Attributes:\n        max_seq_length: max sequence length in subtokens, including [SEP] and [CLS] tokens\n        max_subword_length: rmax lenght of a bert subtoken\n        tokenizer: instance of Bert FullTokenizer\n    """"""\n\n    def __init__(self,\n                 vocab_file: str,\n                 do_lower_case: bool = False,\n                 max_seq_length: int = 512,\n                 max_subword_length: int = None,\n                 token_masking_prob: float = 0.0,\n                 provide_subword_tags: bool = False,\n                 subword_mask_mode: str = ""first"",\n                 **kwargs):\n        self._re_tokenizer = re.compile(r""[\\w\']+|[^\\w ]"")\n        self.provide_subword_tags = provide_subword_tags\n        self.mode = kwargs.get(\'mode\')\n        self.max_seq_length = max_seq_length\n        self.max_subword_length = max_subword_length\n        self.subword_mask_mode = subword_mask_mode\n        vocab_file = str(expand_path(vocab_file))\n        self.tokenizer = FullTokenizer(vocab_file=vocab_file,\n                                       do_lower_case=do_lower_case)\n        self.token_masking_prob = token_masking_prob\n\n    def __call__(self,\n                 tokens: Union[List[List[str]], List[str]],\n                 tags: List[List[str]] = None,\n                 **kwargs):\n        if isinstance(tokens[0], str):\n            tokens = [re.findall(self._re_tokenizer, s) for s in tokens]\n        subword_tokens, subword_tok_ids, startofword_markers, subword_tags = [], [], [], []\n        for i in range(len(tokens)):\n            toks = tokens[i]\n            ys = [\'O\'] * len(toks) if tags is None else tags[i]\n            assert len(toks) == len(ys), \\\n                f""toks({len(toks)}) should have the same length as ys({len(ys)})""\n            sw_toks, sw_marker, sw_ys = \\\n                self._ner_bert_tokenize(toks,\n                                        ys,\n                                        self.tokenizer,\n                                        self.max_subword_length,\n                                        mode=self.mode,\n                                        subword_mask_mode=self.subword_mask_mode,\n                                        token_masking_prob=self.token_masking_prob)\n            if self.max_seq_length is not None:\n                if len(sw_toks) > self.max_seq_length:\n                    raise RuntimeError(f""input sequence after bert tokenization""\n                                       f"" shouldn\'t exceed {self.max_seq_length} tokens."")\n            subword_tokens.append(sw_toks)\n            subword_tok_ids.append(self.tokenizer.convert_tokens_to_ids(sw_toks))\n            startofword_markers.append(sw_marker)\n            subword_tags.append(sw_ys)\n            assert len(sw_marker) == len(sw_toks) == len(subword_tok_ids[-1]) == len(sw_ys), \\\n                f""length of sow_marker({len(sw_marker)}), tokens({len(sw_toks)}),"" \\\n                f"" token ids({len(subword_tok_ids[-1])}) and ys({len(ys)})"" \\\n                f"" for tokens = `{toks}` should match""\n        subword_tok_ids = zero_pad(subword_tok_ids, dtype=int, padding=0)\n        startofword_markers = zero_pad(startofword_markers, dtype=int, padding=0)\n        attention_mask = Mask()(subword_tokens)\n\n        if tags is not None:\n            if self.provide_subword_tags:\n                return tokens, subword_tokens, subword_tok_ids, \\\n                    attention_mask, startofword_markers, subword_tags\n            else:\n                nonmasked_tags = [[t for t in ts if t != \'X\'] for ts in tags]\n                for swts, swids, swms, ts in zip(subword_tokens,\n                                                 subword_tok_ids,\n                                                 startofword_markers,\n                                                 nonmasked_tags):\n                    if (len(swids) != len(swms)) or (len(ts) != sum(swms)):\n                        log.warning(\'Not matching lengths of the tokenization!\')\n                        log.warning(f\'Tokens len: {len(swts)}\\n Tokens: {swts}\')\n                        log.warning(f\'Markers len: {len(swms)}, sum: {sum(swms)}\')\n                        log.warning(f\'Masks: {swms}\')\n                        log.warning(f\'Tags len: {len(ts)}\\n Tags: {ts}\')\n                return tokens, subword_tokens, subword_tok_ids, \\\n                    attention_mask, startofword_markers, nonmasked_tags\n        return tokens, subword_tokens, subword_tok_ids, startofword_markers, attention_mask\n\n    @staticmethod\n    def _ner_bert_tokenize(tokens: List[str],\n                           tags: List[str],\n                           tokenizer: FullTokenizer,\n                           max_subword_len: int = None,\n                           mode: str = None,\n                           subword_mask_mode: str = ""first"",\n                           token_masking_prob: float = None) -> Tuple[List[str], List[int], List[str]]:\n        do_masking = (mode == \'train\') and (token_masking_prob is not None)\n        do_cutting = (max_subword_len is not None)\n        tokens_subword = [\'[CLS]\']\n        startofword_markers = [0]\n        tags_subword = [\'X\']\n        for token, tag in zip(tokens, tags):\n            token_marker = int(tag != \'X\')\n            subwords = tokenizer.tokenize(token)\n            if not subwords or (do_cutting and (len(subwords) > max_subword_len)):\n                tokens_subword.append(\'[UNK]\')\n                startofword_markers.append(token_marker)\n                tags_subword.append(tag)\n            else:\n                if do_masking and (random.random() < token_masking_prob):\n                    tokens_subword.extend([\'[MASK]\'] * len(subwords))\n                else:\n                    tokens_subword.extend(subwords)\n                if subword_mask_mode == ""last"":\n                    startofword_markers.extend([0] * (len(subwords) - 1) + [token_marker])\n                else:\n                    startofword_markers.extend([token_marker] + [0] * (len(subwords) - 1))\n                tags_subword.extend([tag] + [\'X\'] * (len(subwords) - 1))\n\n        tokens_subword.append(\'[SEP]\')\n        startofword_markers.append(0)\n        tags_subword.append(\'X\')\n        return tokens_subword, startofword_markers, tags_subword\n\n\n@register(\'bert_ranker_preprocessor\')\nclass BertRankerPreprocessor(BertPreprocessor):\n    """"""Tokenize text to sub-tokens, encode sub-tokens with their indices, create tokens and segment masks for ranking.\n\n    Builds features for a pair of context with each of the response candidates.\n    """"""\n\n    def __call__(self, batch: List[List[str]]) -> List[List[InputFeatures]]:\n        """"""Call BERT :func:`bert_dp.preprocessing.convert_examples_to_features` function to tokenize and create masks.\n\n        Args:\n            batch: list of elemenents where the first element represents the batch with contexts\n                and the rest of elements represent response candidates batches\n\n        Returns:\n            list of feature batches with subtokens, subtoken ids, subtoken mask, segment mask.\n        """"""\n\n        if isinstance(batch[0], str):\n            batch = [batch]\n\n        cont_resp_pairs = []\n        if len(batch[0]) == 1:\n            contexts = batch[0]\n            responses_empt = [None] * len(batch)\n            cont_resp_pairs.append(zip(contexts, responses_empt))\n        else:\n            contexts = [el[0] for el in batch]\n            for i in range(1, len(batch[0])):\n                responses = []\n                for el in batch:\n                    responses.append(el[i])\n                cont_resp_pairs.append(zip(contexts, responses))\n        examples = []\n        for s in cont_resp_pairs:\n            ex = [InputExample(unique_id=0, text_a=context, text_b=response) for context, response in s]\n            examples.append(ex)\n        features = [convert_examples_to_features(el, self.max_seq_length, self.tokenizer) for el in examples]\n\n        return features\n\n\n@register(\'bert_sep_ranker_preprocessor\')\nclass BertSepRankerPreprocessor(BertPreprocessor):\n    """"""Tokenize text to sub-tokens, encode sub-tokens with their indices, create tokens and segment masks for ranking.\n\n    Builds features for a context and for each of the response candidates separately.\n    """"""\n\n    def __call__(self, batch: List[List[str]]) -> List[List[InputFeatures]]:\n        """"""Call BERT :func:`bert_dp.preprocessing.convert_examples_to_features` function to tokenize and create masks.\n\n        Args:\n            batch: list of elemenents where the first element represents the batch with contexts\n                and the rest of elements represent response candidates batches\n\n        Returns:\n            list of feature batches with subtokens, subtoken ids, subtoken mask, segment mask\n            for the context and each of response candidates separately.\n        """"""\n\n        if isinstance(batch[0], str):\n            batch = [batch]\n\n        samples = []\n        for i in range(len(batch[0])):\n            s = []\n            for el in batch:\n                s.append(el[i])\n            samples.append(s)\n        s_empt = [None] * len(samples[0])\n        # TODO: add unique id\n        examples = []\n        for s in samples:\n            ex = [InputExample(unique_id=0, text_a=text_a, text_b=text_b) for text_a, text_b in\n                  zip(s, s_empt)]\n            examples.append(ex)\n        features = [convert_examples_to_features(el, self.max_seq_length, self.tokenizer) for el in examples]\n\n        return features\n\n\n@register(\'bert_sep_ranker_predictor_preprocessor\')\nclass BertSepRankerPredictorPreprocessor(BertSepRankerPreprocessor):\n    """"""Tokenize text to sub-tokens, encode sub-tokens with their indices, create tokens and segment masks for ranking.\n\n    Builds features for a context and for each of the response candidates separately.\n    In addition, builds features for a response (and corresponding context) text base.\n\n    Args:\n        resps: list of strings containing the base of text responses\n        resp_vecs: BERT vector respresentations of ``resps``, if is ``None`` features for the response base will be build\n        conts: list of strings containing the base of text contexts\n        cont_vecs: BERT vector respresentations of ``conts``, if is ``None`` features for the response base will be build\n    """"""\n\n    def __init__(self,\n                 resps=None, resp_vecs=None, conts=None, cont_vecs=None, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.resp_features = None\n        self.cont_features = None\n        if resps is not None and resp_vecs is None:\n            log.info(""Building BERT features for the response base..."")\n            resp_batch = [[el] for el in resps]\n            self.resp_features = self(resp_batch)\n        if conts is not None and cont_vecs is None:\n            log.info(""Building BERT features for the context base..."")\n            cont_batch = [[el] for el in conts]\n            self.cont_features = self(cont_batch)\n'"
deeppavlov/models/preprocessors/capitalization.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom typing import Tuple, List, Optional\n\nimport numpy as np\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.utils import zero_pad\nfrom deeppavlov.core.models.component import Component\n\n\n@register(\'capitalization_featurizer\')\nclass CapitalizationPreprocessor(Component):\n    """"""\n    Featurizer useful for NER task. It detects following patterns in the words:\n    - no capitals\n    - single capital single character\n    - single capital multiple characters\n    - all capitals multiple characters\n\n    Args:\n        pad_zeros: whether to pad capitalization features batch with zeros up\n            to maximal length or not.\n\n    Attributes:\n        dim: dimensionality of the feature vectors, produced by the featurizer\n    """"""\n\n    def __init__(self, pad_zeros: bool = True, *args, **kwargs) -> None:\n        self.pad_zeros = pad_zeros\n        self._num_of_features = 4\n\n    @property\n    def dim(self):\n        return self._num_of_features\n\n    def __call__(self, tokens_batch, **kwargs):\n        cap_batch = []\n        max_batch_len = 0\n        for utterance in tokens_batch:\n            cap_list = []\n            max_batch_len = max(max_batch_len, len(utterance))\n            for token in utterance:\n                cap = np.zeros(4, np.float32)\n                # Check the case and produce corresponding one-hot\n                if len(token) > 0:\n                    if token[0].islower():\n                        cap[0] = 1\n                    elif len(token) == 1 and token[0].isupper():\n                        cap[1] = 1\n                    elif len(token) > 1 and token[0].isupper() and any(ch.islower() for ch in token):\n                        cap[2] = 1\n                    elif all(ch.isupper() for ch in token):\n                        cap[3] = 1\n                cap_list.append(cap)\n            cap_batch.append(cap_list)\n        if self.pad_zeros:\n            return zero_pad(cap_batch)\n        else:\n            return cap_batch\n\n\ndef process_word(word: str, to_lower: bool = False,\n                 append_case: Optional[str] = None) -> Tuple[str]:\n    """"""The method implements the following operations:\n        1. converts word to a tuple of symbols (character splitting),\n        2. optionally converts it to lowercase and\n        3. adds capitalization label.\n\n    Args:\n        word: input word\n        to_lower: whether to lowercase\n        append_case: whether to add case mark\n            (\'<FIRST_UPPER>\' for first capital and \'<ALL_UPPER>\' for all caps)\n\n    Returns:\n        a preprocessed word.\n\n    Example:\n        >>> process_word(word=""Zaman"", to_lower=True, append_case=""first"")\n        (\'<FIRST_UPPER>\', \'z\', \'a\', \'m\', \'a\', \'n\')\n        >>> process_word(word=""MSU"", to_lower=True, append_case=""last"")\n        (\'m\', \'s\', \'u\', \'<ALL_UPPER>\')\n    """"""\n    if all(x.isupper() for x in word) and len(word) > 1:\n        uppercase = ""<ALL_UPPER>""\n    elif word[0].isupper():\n        uppercase = ""<FIRST_UPPER>""\n    else:\n        uppercase = None\n    if to_lower:\n        word = word.lower()\n    if word.isdigit():\n        answer = [""<DIGIT>""]\n    elif word.startswith(""http://"") or word.startswith(""www.""):\n        answer = [""<HTTP>""]\n    else:\n        answer = list(word)\n    if to_lower and uppercase is not None:\n        if append_case == ""first"":\n            answer = [uppercase] + answer\n        elif append_case == ""last"":\n            answer = answer + [uppercase]\n    return tuple(answer)\n\n\n@register(\'char_splitting_lowercase_preprocessor\')\nclass CharSplittingLowercasePreprocessor(Component):\n    """"""A callable wrapper over :func:`process_word`.\n    Takes as input a batch of tokenized sentences\n    and returns a batch of preprocessed sentences.\n    """"""\n\n    def __init__(self, to_lower: bool = True, append_case: str = ""first"", *args, **kwargs):\n        self.to_lower = to_lower\n        self.append_case = append_case\n\n    def __call__(self, tokens_batch: List[List[str]], **kwargs) -> List[List[Tuple[str]]]:\n        answer = []\n        for elem in tokens_batch:\n            # if isinstance(elem, str):\n            #     elem = NLTKMosesTokenizer()([elem])[0]\n            #     # elem = [x for x in re.split(""(\\w+|[,.])"", elem) if x.strip() != """"]\n            answer.append([process_word(x, self.to_lower, self.append_case) for x in elem])\n        return answer\n'"
deeppavlov/models/preprocessors/char_splitter.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\n\nfrom overrides import overrides\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\n\nlog = getLogger(__name__)\n\n\n@register(\'char_splitter\')\nclass CharSplitter(Component):\n    """"""This component transforms batch of sequences of tokens into batch of sequences of character sequences.""""""\n\n    def __init__(self, **kwargs):\n        pass\n\n    @overrides\n    def __call__(self, batch, *args, **kwargs):\n        char_batch = []\n        for tokens_sequence in batch:\n            char_batch.append([list(tok) for tok in tokens_sequence])\n        return char_batch\n'"
deeppavlov/models/preprocessors/dirty_comments_preprocessor.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\nimport string\nfrom typing import List\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\n\n\n@register(\'dirty_comments_preprocessor\')\nclass DirtyCommentsPreprocessor(Component):\n    """"""\n    Class implements preprocessing of english texts with low level of literacy such as comments\n    """"""\n\n    def __init__(self, remove_punctuation: bool = True, *args, **kwargs):\n        self.remove_punctuation = remove_punctuation\n\n    def __call__(self, batch: List[str], **kwargs) -> List[str]:\n        """"""\n        Preprocess given batch\n\n        Args:\n            batch: list of text samples\n            **kwargs: additional arguments\n\n        Returns:\n            list of preprocessed text samples\n        """"""\n        f = [x.lower() for x in batch]\n        f = [re.sub(""<\\S*>"", "" "", x) for x in f]\n        f = [re.sub(\'\\s+\', \' \', x) for x in f]\n\n        f = [x.replace(""won\'t"", ""will not"") for x in f]\n        f = [x.replace(""can\'t"", ""cannot"") for x in f]\n        f = [x.replace(""i\'m"", ""i am"") for x in f]\n        f = [x.replace("" im "", "" i am "") for x in f]\n        f = [x.replace(""\'re"", "" are"") for x in f]\n        f = [x.replace(""ain\'t"", ""is not"") for x in f]\n        f = [x.replace(""\'ll"", "" will"") for x in f]\n        f = [x.replace(""n\'t"", "" not"") for x in f]\n        f = [x.replace(""\'ve"", "" have"") for x in f]\n        f = [x.replace(""\'s"", "" is"") for x in f]\n        f = [x.replace(""\'d"", "" would"") for x in f]\n\n        f = [re.sub(""ies( |$)"", ""y "", x) for x in f]\n        f = [re.sub(""s( |$)"", "" "", x) for x in f]\n        f = [re.sub(""ing( |$)"", "" "", x) for x in f]\n\n        f = [x.replace("" u "", "" you "") for x in f]\n        f = [x.replace("" em "", "" them "") for x in f]\n        f = [x.replace("" da "", "" the "") for x in f]\n        f = [x.replace("" yo "", "" you "") for x in f]\n        f = [x.replace("" ur "", "" your "") for x in f]\n        f = [x.replace("" u r "", "" you are "") for x in f]\n        f = [x.replace("" urs "", "" yours "") for x in f]\n        f = [x.replace(""y\'all"", ""you all"") for x in f]\n\n        f = [x.replace("" r u "", "" are you "") for x in f]\n        f = [x.replace("" r you"", "" are you"") for x in f]\n        f = [x.replace("" are u "", "" are you "") for x in f]\n\n        f = [x.replace(""\\\\n"", "" "") for x in f]\n        f = [x.replace(""\\\\t"", "" "") for x in f]\n        f = [x.replace(""\\\\xa0"", "" "") for x in f]\n        f = [x.replace(""\\\\xc2"", "" "") for x in f]\n        f = [re.sub(""[0-9]+"", "" 0 "", x) for x in f]\n\n        f = [re.sub(r\'([\' + string.printable + r\'])\\1{3,}\', r\'\\1\\1\', x).strip() for x in f]\n\n        if self.remove_punctuation:\n            f = [re.sub(r\'([\' + string.punctuation + \'])\', \' \', x) for x in f]\n\n        f = [re.sub(\' +\', \' \', x) for x in f]\n        return f\n'"
deeppavlov/models/preprocessors/ecommerce_preprocess.py,0,"b'# Copyright 2018 Neural Networks and Deep Learning lab, MIPT\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#     http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport math\nimport re\nfrom typing import List, Any, Dict, Iterable, Optional, Tuple\n\nimport spacy\nfrom spacy.matcher import Matcher\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.models.tokenizers.spacy_tokenizer import _try_load_spacy_model\n\n\n@register(\'ecommerce_preprocess\')\nclass EcommercePreprocess(Component):\n    """"""Class to process strings for EcommerceBot skill\n\n    Parameters:\n        spacy_model: SpaCy model name\n        disable: SpaCy pipeline to disable\n    """"""\n\n    def __init__(self, spacy_model: str = \'en_core_web_sm\', disable: Optional[Iterable[str]] = None, **kwargs):\n        if disable is None:\n            disable = [\'parser\', \'ner\']\n\n        self.model = _try_load_spacy_model(spacy_model, disable=disable)\n\n        below = lambda text: bool(re.compile(r\'below|cheap\').match(text))\n        BELOW = self.model.vocab.add_flag(below)\n\n        above = lambda text: bool(re.compile(r\'above|start\').match(text))\n        ABOVE = self.model.vocab.add_flag(above)\n\n        self.matcher = Matcher(self.model.vocab)\n\n        self.matcher.add(\'below\', None, [{BELOW: True}, {\'LOWER\': \'than\', \'OP\': \'?\'},\n                                         {\'LOWER\': \'from\', \'OP\': \'?\'}, {\'ORTH\': \'$\', \'OP\': \'?\'},\n                                         {\'ENT_TYPE\': \'MONEY\', \'LIKE_NUM\': True}])\n\n        self.matcher.add(\'above\', None, [{ABOVE: True}, {\'LOWER\': \'than\', \'OP\': \'?\'},\n                                         {\'LOWER\': \'from\', \'OP\': \'?\'}, {\'ORTH\': \'$\', \'OP\': \'?\'},\n                                         {\'ENT_TYPE\': \'MONEY\', \'LIKE_NUM\': True}])\n\n    def __call__(self, **kwargs):\n        pass\n\n    def extract_money(self, doc: spacy.tokens.Doc) -> Tuple[List, Tuple[float, float]]:\n        """"""Extract money entities and money related tokens from `doc`.\n\n        Parameters:\n            doc: a list of tokens with corresponding tags, lemmas, etc.\n\n        Returns:\n            doc_no_money: doc with no money related tokens.\n            money_range: money range from `money_range[0]` to `money_range[1]` extracted from the doc.\n        """"""\n\n        matches = self.matcher(doc)\n        money_range: Tuple = ()\n        doc_no_money = list(doc)\n        negated = False\n\n        for match_id, start, end in matches:\n            string_id = self.model.vocab.strings[match_id]\n            span = doc[start:end]\n            for child in doc[start].children:\n                if child.dep_ == \'neg\':\n                    negated = True\n\n            num_token = [token for token in span if token.like_num == True]\n            if (string_id == \'below\' and negated == False) or (string_id == \'above\' and negated == True):\n                money_range = (0, float(num_token[0].text))\n\n            if (string_id == \'above\' and negated == False) or (string_id == \'below\' and negated == True):\n                money_range = (float(num_token[0].text), float(math.inf))\n\n            del doc_no_money[start:end + 1]\n        return doc_no_money, money_range\n\n    def analyze(self, text: str) -> Iterable:\n        """"""SpaCy `text` preprocessing""""""\n        return self.model(text)\n\n    def spacy2dict(self, doc: spacy.tokens.Doc, fields: List[str] = None) -> List[Dict[Any, Any]]:\n        """"""Convert SpaCy doc into list of tokens with `fields` properties only""""""\n        if fields is None:\n            fields = [\'tag_\', \'like_num\', \'lemma_\', \'text\']\n        return [{field: getattr(token, field) for field in fields} for token in doc]\n\n    def filter_nlp(self, tokens: Iterable) -> List[Any]:\n        """"""Filter tokens according to the POS tags""""""\n        res = []\n        for word in tokens:\n            if word.tag_ not in [\'MD\', \'SP\', \'DT\', \'TO\']:\n                res.append(word)\n        return res\n\n    def filter_nlp_title(self, doc: Iterable) -> List[Any]:\n        """"""Filter item titles according to the POS tags""""""\n        return [w for w in doc if w.tag_ in [\'NNP\', \'NN\', \'PROPN\', \'JJ\'] and not w.like_num]\n\n    def lemmas(self, doc: Iterable) -> List[str]:\n        """"""Return lemma of `doc`""""""\n        return [w.get(\'lemma_\') if isinstance(w, dict) else w.lemma_ for w in doc]\n\n    def price(self, item: Dict[Any, Any]) -> float:\n        """"""Return price of item in a proper format""""""\n        if \'ListPrice\' in item:\n            return float(item[\'ListPrice\'].split(\'$\')[1].replace("","", """"))\n        return 0\n\n    def parse_input(self, inp: str) -> Dict[Any, Any]:\n        """"""Convert space-delimited string into dialog state""""""\n        state: List = []\n        for i in range(len(inp.split()) // 2, 0, -1):\n            state.append([inp.split(None, 1)[0], inp.split(None, 1)[1].split()[0]])\n\n            if i > 1:\n                inp = inp.split(None, 2)[2]\n\n        return dict(state)\n'"
deeppavlov/models/preprocessors/mask.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\n\n\n@register(\'mask\')\nclass Mask(Component):\n    """"""Takes a batch of tokens and returns the masks of corresponding length""""""\n    def __init__(self, *args, **kwargs):\n        pass\n\n    @staticmethod\n    def __call__(tokens_batch, **kwargs):\n        batch_size = len(tokens_batch)\n        max_len = max(len(utt) for utt in tokens_batch)\n        mask = np.zeros([batch_size, max_len], dtype=np.float32)\n        for n, utterance in enumerate(tokens_batch):\n            mask[n, :len(utterance)] = 1\n\n        return mask\n'"
deeppavlov/models/preprocessors/ner_preprocessor.py,0,"b'import errno\nimport os\nfrom logging import getLogger\nfrom typing import List\n\nimport numpy as np\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.estimator import Estimator\n\nlog = getLogger(__name__)\n\n\n@register(""ner_preprocessor"")\nclass NerPreprocessor():\n    """""" Preprocess the batch of list of tokens\n\n    Params:\n        get_x_padded_for_elmo: whether the padded batch used for ELMo is returned\n        get_x_cap_padded: whether the padded batch used for capitalization feature extraction is returned\n    """"""\n\n    def __init__(self, get_x_padded_for_elmo=False, get_x_cap_padded=False, **kwargs):\n        self.get_x_padded_for_elmo = get_x_padded_for_elmo\n        self.get_x_cap_padded = get_x_cap_padded\n\n        self.cap_vocab_size = 5\n\n    def encode_cap(self, s):\n        if s.upper() == s:\n            return 1\n        elif s.lower() == s:\n            return 2\n        elif (s[0].upper() == s[0]) and (s[1:].lower() == s[1:]):\n            return 3\n        else:\n            return 4\n\n    def __call__(self, batch: List[List[str]], **kwargs):\n        """""" Process the input batch\n\n        Args:\n            batch: list of list of tokens\n\n        Returns:\n            x_lower: batch in lowercase\n            sent_lengths: lengths of sents\n            x_padded_for_elmo (optional): batch padded with """", used as input for ELMo\n            x_cap_padded: batch of capitalization features\n        """"""\n\n        x_lower = [[token.lower() for token in sent] for sent in batch]\n        sent_lengths = [len(sent) for sent in batch]\n        ret = (x_lower, sent_lengths,)\n\n        max_len = max(sent_lengths)\n\n        if self.get_x_padded_for_elmo:\n            x_tokens_elmo = [sent + [""""] * (max_len - len(sent)) for sent in batch]\n            ret += (x_tokens_elmo,)\n\n        if self.get_x_cap_padded:\n            cap_seq = [[self.encode_cap(token) for token in sent] for sent in batch]\n            x_cap_padded = np.zeros((len(batch), max_len))\n            for i, caps in enumerate(cap_seq):\n                x_cap_padded[i, :len(caps)] = caps\n            ret += (x_cap_padded,)\n\n        return ret\n\n\n@register(""convert_ids2tags"")\nclass ConvertIds2Tags():\n    """""" Class used to convert the batch of indices to the batch of tags\n\n    Params:\n        id2tag: the dictionary used to convert the indices to the corresponding tags\n\n    """"""\n\n    def __init__(self, id2tag, *args, **kwargs):\n        self.id2tag = id2tag\n\n    def __call__(self, y_predicted):\n        """""" Convert the batch of indices to the corresponding batch of tags\n\n        Params:\n            y_predicted: the batch of indices\n\n        Returns:\n            the corresponding batch of tags\n        """"""\n\n        return [[self.id2tag[id] for id in seq] for seq in y_predicted]\n\n\n@register(""ner_vocab"")\nclass NerVocab(Estimator):\n    """""" Implementation of the NER vocabulary\n\n    Params:\n        word_file_path: the path to the pre-trained word embedding model\n        save_path: the folder path to save dictionary files\n        load_path: the folder path from which the dictionary files are loaded\n        char_level: the flag arg indicating the character vocabulary\n    """"""\n\n    def __init__(self,\n                 word_file_path=None,\n                 save_path=None,\n                 load_path=None,\n                 char_level=False,\n                 **kwargs):\n\n        super().__init__(save_path=save_path, load_path=load_path, **kwargs)\n\n        self.word_file_path = word_file_path\n        self.char_level = char_level\n\n        if word_file_path is not None:\n            self.load_from_file(word_file_path)\n            if self.save_path is not None:\n                self.save_to_file(self.save_path)\n        elif self.load_path is not None:\n            self.load_from_file(self.load_path)\n\n    def load_from_file(self, filename):\n        if filename is None or not os.path.exists(filename):\n            return\n\n        self._t2i, self._i2t = {}, {}\n        for i, line in enumerate(open(file=filename, mode=""r"", encoding=""utf-8"").readlines()):\n            word = line.strip()\n            self._t2i[word] = i\n            self._i2t[i] = word\n\n    def save_to_file(self, filename):\n        if filename is None:\n            return\n\n        dir_name = os.path.dirname(filename)\n        if not os.path.exists(dir_name):\n            os.makedirs(dir_name)\n        with open(file=filename, mode=""w"", encoding=""utf-8"") as fo:\n            for word in self._t2i.keys():\n                fo.write(""{}\\n"".format(word))\n\n    def fit(self, sents: [List[List[str]]], *args):\n        if self.word_file_path is not None:\n            return\n\n        if self.char_level:\n            items = set([char for sent in sents for word in sent for char in word])\n        else:\n            items = set([word for sent in sents for word in sent])\n        items = [""<UNK>"", ""<PAD>""] + list(items)\n\n        self._t2i = {k: v for v, k in enumerate(items)}\n        self._i2t = {k: v for k, v in enumerate(items)}\n\n        self.save_to_file(self.save_path)\n\n    def pad_batch(self, tokens: List[List[int]]):\n        """""" Create padded batch of words, tags, chunk pos, even batch of characters\n\n        Params:\n            tokens: list of raw words, pos, chunk, or tags.\n\n        Returns:\n            the padded batch\n        """"""\n\n        batch_size = len(tokens)\n\n        if not self.char_level:\n            max_len = max([len(seq) for seq in tokens])\n            padded_batch = np.full((batch_size, max_len), self._t2i[""<PAD>""])\n            for i, seq in enumerate(tokens):\n                padded_batch[i, :len(seq)] = seq\n        else:\n            max_len_seq = max([len(seq) for seq in tokens])\n            if max_len_seq == 0:\n                max_len_sub_seq = 0\n            else:\n                max_len_sub_seq = max([len(sub_seq) for seq in tokens for sub_seq in seq])\n            padded_batch = np.full((batch_size, max_len_seq, max_len_sub_seq), self._t2i[""<PAD>""])\n            for i, seq in enumerate(tokens):\n                for j, sub_seq in enumerate(seq):\n                    padded_batch[i, j, :len(sub_seq)] = sub_seq\n        return padded_batch\n\n    def __call__(self, sents, **kwargs):\n        if not self.char_level:\n            sents_ind = [[self._t2i[word] if word in self._t2i else 0 for word in sent] for sent in sents]\n        else:\n            sents_ind = [[[self._t2i[char] if char in self._t2i else 0 for char in word] for word in sent] for sent in\n                         sents]\n        padded_sents = self.pad_batch(sents_ind)\n\n        return padded_sents\n\n    def load(self, *args, **kwargs):\n        log.info(""[loading vocabulary from {}]"".format(self.load_path))\n        if self.load_path is not None:\n            self.load_from_file(self.load_path)\n\n    def save(self, *args, **kwargs):\n        log.info(""[saving vocabulary to {}]"".format(self.save_path))\n        if not os.path.exists(os.path.dirname(self.save_path)):\n            try:\n                os.makedirs(os.path.dirname(self.save_path))\n            except OSError as exc:\n                if exc.errno != errno.EEXIST:\n                    raise\n        self.save_to_file(self.save_path)\n\n    @property\n    def len(self):\n        return len(self._t2i)\n\n    @property\n    def t2i(self):\n        return self._t2i\n\n    @property\n    def i2t(self):\n        return self._i2t\n'"
deeppavlov/models/preprocessors/odqa_preprocessors.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom itertools import chain\nfrom logging import getLogger\nfrom typing import List, Callable, Union\n\nfrom nltk import sent_tokenize\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\n\nlogger = getLogger(__name__)\n\n\n@register(\'document_chunker\')\nclass DocumentChunker(Component):\n    """"""Make chunks from a document or a list of documents. Don\'t tear up sentences if needed.\n\n    Args:\n        sentencize_fn: a function for sentence segmentation\n        keep_sentences: whether to tear up sentences between chunks or not\n        tokens_limit: a number of tokens in a single chunk (usually this number corresponds to the squad model limit)\n        flatten_result: whether to flatten the resulting list of lists of chunks\n        paragraphs: whether to split document by paragrahs; if set to True, tokens_limit is ignored\n\n    Attributes:\n        keep_sentences: whether to tear up sentences between chunks or not\n        tokens_limit: a number of tokens in a single chunk\n        flatten_result: whether to flatten the resulting list of lists of chunks\n        paragraphs: whether to split document by paragrahs; if set to True, tokens_limit is ignored\n\n    """"""\n\n    def __init__(self, sentencize_fn: Callable = sent_tokenize, keep_sentences: bool = True,\n                 tokens_limit: int = 400, flatten_result: bool = False,\n                 paragraphs: bool = False, *args, **kwargs) -> None:\n        self._sentencize_fn = sentencize_fn\n        self.keep_sentences = keep_sentences\n        self.tokens_limit = tokens_limit\n        self.flatten_result = flatten_result\n        self.paragraphs = paragraphs\n\n    def __call__(self, batch_docs: List[Union[str, List[str]]]) -> \\\n            List[Union[List[str], List[List[str]]]]:\n        """"""Make chunks from a batch of documents. There can be several documents in each batch.\n        Args:\n            batch_docs: a batch of documents / a batch of lists of documents\n        Returns:\n            chunks of docs, flattened or not\n        """"""\n\n        result = []\n\n        for docs in batch_docs:\n            batch_chunks = []\n            if isinstance(docs, str):\n                docs = [docs]\n            for doc in docs:\n                if self.paragraphs:\n                    split_doc = doc.split(\'\\n\\n\')\n                    split_doc = [sd.strip() for sd in split_doc]\n                    split_doc = list(filter(lambda x: len(x) > 40, split_doc))\n                    batch_chunks.append(split_doc)\n                else:\n                    doc_chunks = []\n                    if self.keep_sentences:\n                        sentences = sent_tokenize(doc)\n                        n_tokens = 0\n                        keep = []\n                        for s in sentences:\n                            n_tokens += len(s.split())\n                            if n_tokens > self.tokens_limit:\n                                if keep:\n                                    doc_chunks.append(\' \'.join(keep))\n                                    n_tokens = 0\n                                    keep.clear()\n                            keep.append(s)\n                        if keep:\n                            doc_chunks.append(\' \'.join(keep))\n                        batch_chunks.append(doc_chunks)\n                    else:\n                        split_doc = doc.split()\n                        doc_chunks = [split_doc[i:i + self.tokens_limit] for i in\n                                      range(0, len(split_doc), self.tokens_limit)]\n                        batch_chunks.append(doc_chunks)\n            result.append(batch_chunks)\n\n        if self.flatten_result:\n            if isinstance(result[0][0], list):\n                for i in range(len(result)):\n                    flattened = list(chain.from_iterable(result[i]))\n                    result[i] = flattened\n\n        return result\n\n\n@register(\'string_multiplier\')\nclass StringMultiplier(Component):\n    """"""Make a list of strings from a provided string. A length of the resulting list equals a length\n    of a provided reference argument.\n\n    """"""\n\n    def __init__(self, **kwargs):\n        pass\n\n    def __call__(self, batch_s: List[str], ref: List[str]) -> List[List[str]]:\n        """""" Multiply each string in a provided batch of strings.\n\n        Args:\n            batch_s: a batch of strings to be multiplied\n            ref: a reference to obtain a length of the resulting list\n\n        Returns:\n            a multiplied s as list\n\n        """"""\n        res = []\n        for s, r in zip(batch_s, ref):\n            res.append([s] * len(r))\n\n        return res\n'"
deeppavlov/models/preprocessors/one_hotter.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import List, Union, Iterable\n\nimport numpy as np\n\nfrom deeppavlov.core.common.errors import ConfigError\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.utils import zero_pad\nfrom deeppavlov.core.models.component import Component\n\n\n@register(\'one_hotter\')\nclass OneHotter(Component):\n    """"""\n    One-hot featurizer with zero-padding.\n    If ``single_vector``, return the only vector per sample which can have several elements equal to ``1``.\n\n    Parameters:\n        depth: the depth for one-hotting\n        pad_zeros: whether to pad elements of batch with zeros\n        single_vector: whether to return one vector for the sample (sum of each one-hotted vectors)\n    """"""\n\n    def __init__(self, depth: int, pad_zeros: bool = False,\n                 single_vector=False, *args, **kwargs):\n        self._depth = depth\n        self._pad_zeros = pad_zeros\n        self.single_vector = single_vector\n        if self._pad_zeros and self.single_vector:\n            raise ConfigError(""Cannot perform ``single_vector`` with zero padding for OneHotter"")\n\n    def __call__(self, batch: List[List[int]], **kwargs) -> Union[List[List[np.ndarray]], List[np.ndarray]]:\n        """"""\n        Convert given batch of list of labels to one-hot representation of the batch.\n\n        Args:\n            batch: list of samples, where each sample is a list of integer labels.\n            **kwargs: additional arguments\n\n        Returns:\n            if ``single_vector``, list of one-hot representations of each sample,\n            otherwise, list of lists of one-hot representations of each label in a sample\n        """"""\n        one_hotted_batch = []\n\n        for utt in batch:\n            if isinstance(utt, Iterable):\n                one_hotted_utt = self._to_one_hot(utt, self._depth)\n            elif isinstance(utt, int):\n                if self._pad_zeros or self.single_vector:\n                    one_hotted_utt = self._to_one_hot([utt], self._depth)\n                else:\n                    one_hotted_utt = self._to_one_hot([utt], self._depth).reshape(-1)\n\n            if self.single_vector:\n                one_hotted_utt = np.sum(one_hotted_utt, axis=0)\n\n            one_hotted_batch.append(one_hotted_utt)\n\n        if self._pad_zeros:\n            one_hotted_batch = zero_pad(one_hotted_batch)\n        return one_hotted_batch\n\n    @staticmethod\n    def _to_one_hot(x, n):\n        b = np.zeros([len(x), n], dtype=np.float32)\n        for q, tok in enumerate(x):\n            b[q, int(tok)] = 1\n        return b\n'"
deeppavlov/models/preprocessors/random_embeddings_matrix.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\n\nfrom deeppavlov.core.common.registry import register\n\n\n@register(\'random_emb_mat\')\nclass RandomEmbeddingsMatrix:\n    """"""Assembles matrix of random embeddings.\n\n    Args:\n        vocab_len: length of the vocabulary (number of tokens in it)\n        emb_dim: dimensionality of the embeddings\n\n    Attributes:\n        dim: dimensionality of the embeddings\n    """"""\n\n    def __init__(self, vocab_len: int, emb_dim: int, *args, **kwargs) -> None:\n        self.emb_mat = np.random.randn(vocab_len, emb_dim).astype(np.float32) / np.sqrt(emb_dim)\n\n    @property\n    def dim(self):\n        return self.emb_mat.shape[1]\n'"
deeppavlov/models/preprocessors/response_base_loader.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport sys\nfrom logging import getLogger\n\nimport numpy as np\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.serializable import Serializable\n\nlogger = getLogger(__name__)\n\n\n@register(\'response_base_loader\')\nclass ResponseBaseLoader(Serializable):\n    """"""Class for loading a base with text responses (and contexts) and their vector representations.""""""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.resps = None\n        self.resp_vecs = None\n        self.conts = None\n        self.cont_vecs = None\n        self.load()\n\n    def load(self):\n        if self.load_path is not None:\n            resp_file = self.load_path / ""responses.csv""\n            if resp_file.exists():\n                with open(resp_file) as f:\n                    responses = f.readlines()\n                    self.resps = [el.strip(\'#\\n\') for el in responses]\n            else:\n                logger.error(""Please provide responses.csv file to the {} directory"".format(self.load_path))\n                sys.exit(1)\n            resp_vec_file = self.load_path / ""resp_vecs.npy""\n            if resp_vec_file.exists():\n                self.resp_vecs = np.load(resp_vec_file)\n            cont_file = self.load_path / ""contexts.csv""\n            if cont_file.exists():\n                with open(cont_file) as f:\n                    contexts = f.readlines()\n                    self.conts = [el.strip(\'#\\n\') for el in contexts]\n            else:\n                logger.error(""Please add contexts.csv file to the {} directory"".format(self.load_path))\n                sys.exit(1)\n            cont_vec_file = self.load_path / ""cont_vecs.npy""\n            if cont_vec_file.exists():\n                self.cont_vecs = np.load(cont_vec_file)\n\n    def save(self):\n        logger.error(""The method save of the {} class is not used."".format(self.__class__))\n'"
deeppavlov/models/preprocessors/russian_lemmatizer.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pymorphy2\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\n\n\n@register(\'pymorphy_russian_lemmatizer\')\nclass PymorphyRussianLemmatizer(Component):\n    """"""Class for lemmatization using PyMorphy.""""""\n\n    def __init__(self, *args, **kwargs):\n        self.lemmatizer = pymorphy2.MorphAnalyzer()\n\n    def __call__(self, tokens_batch, **kwargs):\n        """"""Takes batch of tokens and returns the lemmatized tokens.""""""\n        lemma_batch = []\n        for utterance in tokens_batch:\n            lemma_utterance = []\n            for token in utterance:\n                p = self.lemmatizer.parse(token)[0]\n                lemma_utterance.append(p.normal_form)\n            lemma_batch.append(lemma_utterance)\n        return lemma_batch\n'"
deeppavlov/models/preprocessors/sanitizer.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\nimport sys\nimport unicodedata\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\n\n\n@register(\'sanitizer\')\nclass Sanitizer(Component):\n    """"""Remove all combining characters like diacritical marks from tokens\n\n    Args:\n        diacritical: whether to remove diacritical signs or not\n            diacritical signs are something like hats and stress marks\n        nums: whether to replace all digits with 1 or not\n    """"""\n\n    def __init__(self,\n                 diacritical: bool = True,\n                 nums: bool = False,\n                 *args, **kwargs) -> None:\n        self.diacritical = diacritical\n        self.nums = nums\n        self.combining_characters = dict.fromkeys([c for c in range(sys.maxunicode)\n                                                   if unicodedata.combining(chr(c))])\n\n    def filter_diacritical(self, tokens_batch):\n        """"""Takes batch of tokens and returns the batch with sanitized tokens""""""\n        sanitized_batch = []\n        for utterance in tokens_batch:\n            sanitized_utterance = []\n            for token in utterance:\n                token = unicodedata.normalize(\'NFD\', token)\n                sanitized_utterance.append(token.translate(self.combining_characters))\n            sanitized_batch.append(sanitized_utterance)\n        return sanitized_batch\n\n    def replace_nums(self, tokens_batch):\n        sanitized_batch = []\n        for utterance in tokens_batch:\n            sanitized_batch.append([re.sub(\'[0-9]\', \'1\', token) for token in utterance])\n        return sanitized_batch\n\n    def __call__(self, tokens_batch, **kwargs):\n        if self.filter_diacritical:\n            tokens_batch = self.filter_diacritical(tokens_batch)\n        if self.nums:\n            tokens_batch = self.replace_nums(tokens_batch)\n        return tokens_batch\n'"
deeppavlov/models/preprocessors/sentseg_preprocessor.py,0,"b'from typing import List\n\nfrom deeppavlov.core.common.registry import register\n\n\n@register(""sentseg_restore_sent"")\ndef SentSegRestoreSent(batch_words: List[List[str]], batch_tags: List[List[str]]) -> List[str]:\n    ret = []\n    for words, tags in zip(batch_words, batch_tags):\n        if len(tags) == 0:\n            ret.append("""")\n            continue\n        sent = words[0]\n        punct = """" if tags[0] == ""O"" else tags[0][-1]\n        for word, tag in zip(words[1:], tags[1:]):\n            if tag != ""O"":\n                sent += punct\n                punct = tag[-1]\n            sent += "" "" + word\n        sent += punct\n        ret.append(sent)\n\n    return ret\n'"
deeppavlov/models/preprocessors/siamese_preprocessor.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import List, Union, Iterable, Optional\n\nimport numpy as np\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.utils import zero_pad_truncate\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.core.models.estimator import Estimator\n\nlog = getLogger(__name__)\n\n\n@register(\'siamese_preprocessor\')\nclass SiamesePreprocessor(Estimator):\n    """""" Preprocessing of data samples containing text strings to feed them in a siamese network.\n\n    First ``num_context_turns`` strings in each data sample corresponds to the dialogue ``context``\n    and the rest string(s) in the sample is (are) ``response(s)``.\n\n    Args:\n        save_path: The parameter is only needed to initialize the base class\n            :class:`~deeppavlov.core.models.serializable.Serializable`.\n        load_path: The parameter is only needed to initialize the base class\n            :class:`~deeppavlov.core.models.serializable.Serializable`.\n        max_sequence_length: A maximum length of text sequences in tokens.\n            Longer sequences will be truncated and shorter ones will be padded.\n        dynamic_batch:  Whether to use dynamic batching. If ``True``, the maximum length of a sequence for a batch\n            will be equal to the maximum of all sequences lengths from this batch,\n            but not higher than ``max_sequence_length``.\n        padding: Padding. Possible values are ``pre`` and ``post``.\n            If set to ``pre`` a sequence will be padded at the beginning.\n            If set to ``post`` it will padded at the end.\n        truncating: Truncating. Possible values are ``pre`` and ``post``.\n            If set to ``pre`` a sequence will be truncated at the beginning.\n            If set to ``post`` it will truncated at the end.\n        use_matrix: Whether to use a trainable matrix with token (word) embeddings.\n        num_context_turns: A number of ``context`` turns in data samples.\n        num_ranking_samples: A number of condidates for ranking including positive one.\n        add_raw_text: whether add raw text sentences to output data list or not.\n            Use with conjunction of models using sentence encoders\n        tokenizer: An instance of one of the :class:`deeppavlov.models.tokenizers`.\n        vocab: An instance of :class:`deeppavlov.core.data.simple_vocab.SimpleVocabulary`.\n        embedder: an instance of one of the :class:`deeppavlov.models.embedders`.\n        sent_vocab: An instance of of :class:`deeppavlov.core.data.simple_vocab.SimpleVocabulary`.\n            It is used to store all ``responces`` and to find the best ``response``\n            to the user ``context`` in the ``interact`` mode.\n    """"""\n\n    def __init__(self,\n                 save_path: str = \'./tok.dict\',\n                 load_path: str = \'./tok.dict\',\n                 max_sequence_length: int = None,\n                 dynamic_batch: bool = False,\n                 padding: str = \'post\',\n                 truncating: str = \'post\',\n                 use_matrix: bool = True,\n                 num_context_turns: int = 1,\n                 num_ranking_samples: int = 1,\n                 add_raw_text: bool = False,\n                 tokenizer: Component = None,\n                 vocab: Optional[Estimator] = None,\n                 embedder: Optional[Component] = None,\n                 sent_vocab: Optional[Estimator] = None,\n                 **kwargs):\n\n        self.max_sequence_length = max_sequence_length\n        self.padding = padding\n        self.truncating = truncating\n        self.dynamic_batch = dynamic_batch\n        self.use_matrix = use_matrix\n        self.num_ranking_samples = num_ranking_samples\n        self.num_context_turns = num_context_turns\n        self.add_raw_text = add_raw_text\n        self.tokenizer = tokenizer\n        self.embedder = embedder\n        self.vocab = vocab\n        self.sent_vocab = sent_vocab\n        self.save_path = expand_path(save_path).resolve()\n        self.load_path = expand_path(load_path).resolve()\n\n        super().__init__(load_path=self.load_path, save_path=self.save_path, **kwargs)\n\n    def fit(self, x: List[List[str]]) -> None:\n        if self.sent_vocab is not None:\n            self.sent_vocab.fit([el[self.num_context_turns:] for el in x])\n        x_tok = [self.tokenizer(el) for el in x]\n        self.vocab.fit([el for x in x_tok for el in x])\n\n    def __call__(self, x: Union[List[List[str]], List[str]]) -> Iterable[List[List[np.ndarray]]]:\n        if len(x) == 0 or isinstance(x[0], str):\n            if len(x) == 1:  # interact mode: len(batch) == 1\n                x_preproc = [[sent.strip() for sent in x[0].split(\'&\')]]  # List[str] -> List[List[str]]\n            elif len(x) == 0:\n                x_preproc = [[\'\']]\n            else:\n                x_preproc = [[el] for el in x]\n        else:\n            x_preproc = [el[:self.num_context_turns + self.num_ranking_samples] for el in x]\n        for el in x_preproc:\n            x_tok = self.tokenizer(el)\n            x_ctok = [y if len(y) != 0 else [\'\'] for y in x_tok]\n            if self.use_matrix:\n                x_proc = self.vocab(x_ctok)\n            else:\n                x_proc = self.embedder(x_ctok)\n            if self.dynamic_batch:\n                msl = min((max([len(y) for el in x_tok for y in el]), self.max_sequence_length))\n            else:\n                msl = self.max_sequence_length\n            x_proc = zero_pad_truncate(x_proc, msl, pad=self.padding, trunc=self.truncating)\n            x_proc = list(x_proc)\n            if self.add_raw_text:\n                x_proc += el  # add (self.num_context_turns+self.num_ranking_samples) raw sentences\n            yield x_proc\n\n    def load(self) -> None:\n        pass\n\n    def save(self) -> None:\n        if self.sent_vocab is not None:\n            self.sent_vocab.save()\n        self.vocab.save()\n'"
deeppavlov/models/preprocessors/squad_preprocessor.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport bisect\nimport pickle\nimport unicodedata\nfrom collections import Counter\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import Tuple, List, Union\n\nimport numpy as np\nfrom nltk import word_tokenize\nfrom tqdm import tqdm\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.core.models.estimator import Estimator\n\nlogger = getLogger(__name__)\n\n\n@register(\'squad_preprocessor\')\nclass SquadPreprocessor(Component):\n    """""" SquadPreprocessor is used to preprocess context and question in SQuAD-like datasets.\n\n        Preprocessing includes: sanitizing unicode symbols, quotes, word tokenizing and\n        building mapping from raw text to processed text.\n\n        Params:\n            context_limit: max context length in tokens\n            question_limit: max question length in tokens\n            char_limit: max number of characters in token\n    """"""\n\n    def __init__(self, context_limit: int = 450, question_limit: int = 150, char_limit: int = 16, *args, **kwargs):\n        self.context_limit = context_limit\n        self.question_limit = question_limit\n        self.char_limit = char_limit\n\n    def __call__(self, contexts_raw: Tuple[str, ...], questions_raw: Tuple[str, ...],\n                 **kwargs) -> Tuple[\n        List[str], List[List[str]], List[List[List[str]]],\n        List[List[int]], List[List[int]],\n        List[str], List[List[str]], List[List[List[str]]],\n        List[List[Tuple[int, int]]]\n    ]:\n        """""" Performs preprocessing of context and question\n        Args:\n            contexts_raw: batch of contexts to preprocess\n            questions_raw: batch of questions to preprocess\n\n        Returns:\n            context: batch of processed contexts\n            contexts_tokens: batch of tokenized contexts\n            contexts_chars: batch of tokenized and split on chars contexts\n            contexts_r2p: batch of mappings from raw context to processed context\n            contexts_p2r: batch of mappings from procesesd context to raw context\n            questions: batch of processed questions\n            questions_tokens: batch of tokenized questions\n            questions_chars: batch of tokenized and split on chars questions\n            spans: batch of mapping tokens to position in context\n        """"""\n        contexts = []\n        contexts_tokens = []\n        contexts_chars = []\n        contexts_r2p = []\n        contexts_p2r = []\n        questions = []\n        questions_tokens = []\n        questions_chars = []\n        spans = []\n        for c_raw, q_raw in zip(contexts_raw, questions_raw):\n            c, r2p, p2r = SquadPreprocessor.preprocess_str(c_raw, return_mapping=True)\n            c_tokens = [token.replace(""\'\'"", \'""\').replace(""``"", \'""\') for token in word_tokenize(c)][:self.context_limit]\n            c_chars = [list(token)[:self.char_limit] for token in c_tokens]\n            q = SquadPreprocessor.preprocess_str(q_raw)\n            q_tokens = [token.replace(""\'\'"", \'""\').replace(""``"", \'""\') for token in word_tokenize(q)][:self.question_limit]\n            q_chars = [list(token)[:self.char_limit] for token in q_tokens]\n            contexts.append(c)\n            contexts_tokens.append(c_tokens)\n            contexts_chars.append(c_chars)\n            contexts_r2p.append(r2p)\n            contexts_p2r.append(p2r)\n            questions.append(q)\n            questions_tokens.append(q_tokens)\n            questions_chars.append(q_chars)\n            spans.append(SquadPreprocessor.convert_idx(c, c_tokens))\n        return contexts, contexts_tokens, contexts_chars, contexts_r2p, contexts_p2r, \\\n               questions, questions_tokens, questions_chars, spans\n\n    @staticmethod\n    def preprocess_str(line: str, return_mapping: bool = False) -> Union[Tuple[str, List[int], List[int]], str]:\n        """""" Removes unicode and other characters from str\n\n        Args:\n            line: string to process\n            return_mapping: return mapping from line to preprocessed line or not\n\n        Returns:\n            preprocessed line, raw2preprocessed mapping, preprocessed2raw mapping\n\n        """"""\n        if not return_mapping:\n            return \'\'.join(c for c in line if not unicodedata.combining(c)).replace(""\'\'"", \'"" \').replace(""``"", \'"" \')\n\n        r2p = [len(line)] * (len(line) + 1)\n        p2r = [len(line)] * (len(line) + 1)\n        s = \'\'\n        for i, c in enumerate(line):\n            if unicodedata.combining(c):\n                r2p[i] = -1\n            else:\n                s += c\n                r2p[i] = len(s) - 1\n                p2r[len(s) - 1] = i\n        return s.replace(""\'\'"", \'"" \').replace(""``"", \'"" \'), r2p, p2r\n\n    @staticmethod\n    def convert_idx(text: str, tokens: List[str]) -> List[Tuple[int, int]]:\n        current = 0\n        spans = []\n        for token in tokens:\n            current = text.find(token, current)\n            if current < 0:\n                logger.error(""Token {} cannot be found"".format(token))\n                raise Exception()\n            spans.append((current, current + len(token)))\n            current += len(token)\n        return spans\n\n\n@register(\'squad_ans_preprocessor\')\nclass SquadAnsPreprocessor(Component):\n    """""" SquadAnsPreprocessor is responsible for answer preprocessing.""""""\n\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def __call__(self, answers_raw: Tuple[List[str], ...], answers_start: Tuple[List[int], ...],\n                 r2ps: List[List[int]], spans: List[List[Tuple[int, int]]],\n                 **kwargs) -> Tuple[List[List[str]], List[List[int]], List[List[int]]]:\n        """""" Processes answers for SQuAD dataset\n\n        Args:\n            answers_raw: list of str [batch_size x number_of_answers]\n            answers_start: start position of answer (in chars) [batch_size x number_of_answers]\n            r2ps: mapping from raw context to processed context\n            spans: mapping tokens to position in context\n\n        Returns:\n            processed answer text, start position in tokens, end position in tokens\n            [batch_size x number_of_answers]\n\n        """"""\n        answers = []\n        start = []\n        end = []\n        for ans_raw, ans_st, r2p, span in zip(answers_raw, answers_start, r2ps, spans):\n            start.append([])\n            end.append([])\n            answers.append([])\n            for a_raw, a_st in zip(ans_raw, ans_st):\n                ans = SquadPreprocessor.preprocess_str(a_raw)\n                ans_st = r2p[a_st]\n                ans_end = ans_st + len(ans)\n                answer_span = []\n                for idx, sp in enumerate(span):\n                    if not (ans_end <= sp[0] or ans_st >= sp[1]):\n                        answer_span.append(idx)\n                if len(answer_span) != 0:\n                    y1, y2 = answer_span[0], answer_span[-1]\n                else:\n                    # answer not found in context\n                    y1, y2 = -1, -1\n                start[-1].append(y1)\n                end[-1].append(y2)\n                answers[-1].append(ans)\n        return answers, start, end\n\n\n@register(\'squad_vocab_embedder\')\nclass SquadVocabEmbedder(Estimator):\n    """""" SquadVocabEmbedder is used to build tokens/chars vocabulary and embedding matrix.\n\n        It extracts tokens/chars form dataset and looks for pretrained embeddings.\n\n        Params:\n            emb_folder: path to download pretrained embeddings\n            emb_url: link to pretrained embeddings\n            save_path: extracted embeddings save path\n            load_path: extracted embeddigns load path\n            context_limit: max context length in tokens\n            question_limit: max question length in tokens\n            char_limit: max number of characters in token\n            level: token or char\n        """"""\n\n    def __init__(self, emb_folder: str, emb_url: str, save_path: str, load_path: str,\n                 context_limit: int = 450, question_limit: int = 150, char_limit: int = 16,\n                 level: str = \'token\', *args, **kwargs):\n        self.emb_folder = expand_path(emb_folder)\n        self.level = level\n        self.emb_url = emb_url\n        self.emb_file_name = Path(emb_url).name\n        self.save_path = expand_path(save_path)\n        self.load_path = expand_path(load_path)\n        self.context_limit = context_limit\n        self.question_limit = question_limit\n        self.char_limit = char_limit\n        self.loaded = False\n\n        self.NULL = ""<NULL>""\n        self.OOV = ""<OOV>""\n\n        self.emb_folder.mkdir(parents=True, exist_ok=True)\n\n        self.emb_dim = self.emb_mat = self.token2idx_dict = None\n\n        if self.load_path.exists():\n            self.load()\n\n    def __call__(self, contexts: List[List[str]], questions: List[List[str]]) -> Tuple[np.ndarray, np.ndarray]:\n        """""" Transforms tokens/chars to indices.\n\n        Args:\n            contexts: batch of list of tokens in context\n            questions: batch of list of tokens in question\n\n        Returns:\n            transformed contexts and questions\n        """"""\n        if self.level == \'token\':\n            c_idxs = np.zeros([len(contexts), self.context_limit], dtype=np.int32)\n            q_idxs = np.zeros([len(questions), self.question_limit], dtype=np.int32)\n            for i, context in enumerate(contexts):\n                for j, token in enumerate(context):\n                    c_idxs[i, j] = self._get_idx(token)\n\n            for i, question in enumerate(questions):\n                for j, token in enumerate(question):\n                    q_idxs[i, j] = self._get_idx(token)\n\n        elif self.level == \'char\':\n            c_idxs = np.zeros([len(contexts), self.context_limit, self.char_limit], dtype=np.int32)\n            q_idxs = np.zeros([len(questions), self.question_limit, self.char_limit], dtype=np.int32)\n            for i, context in enumerate(contexts):\n                for j, token in enumerate(context):\n                    for k, char in enumerate(token):\n                        c_idxs[i, j, k] = self._get_idx(char)\n\n            for i, question in enumerate(questions):\n                for j, token in enumerate(question):\n                    for k, char in enumerate(token):\n                        q_idxs[i, j, k] = self._get_idx(char)\n\n        return c_idxs, q_idxs\n\n    def fit(self, contexts: Tuple[List[str], ...], questions: Tuple[List[str]], *args, **kwargs):\n        self.vocab = Counter()\n        self.embedding_dict = dict()\n        if not self.loaded:\n            logger.info(\'SquadVocabEmbedder: fitting with {}s\'.format(self.level))\n            if self.level == \'token\':\n                for line in tqdm(contexts + questions):\n                    for token in line:\n                        self.vocab[token] += 1\n            elif self.level == \'char\':\n                for line in tqdm(contexts + questions):\n                    for token in line:\n                        for c in token:\n                            self.vocab[c] += 1\n            else:\n                raise RuntimeError(""SquadVocabEmbedder::fit: Unknown level: {}"".format(self.level))\n\n            with (self.emb_folder / self.emb_file_name).open(\'r\', encoding=\'utf8\') as femb:\n                emb_voc_size, self.emb_dim = map(int, femb.readline().split())\n                for line in tqdm(femb, total=emb_voc_size):\n                    line_split = line.strip().split(\' \')\n                    word = line_split[0]\n                    vec = np.array(line_split[1:], dtype=float)\n                    if len(vec) != self.emb_dim:\n                        continue\n                    if word in self.vocab:\n                        self.embedding_dict[word] = vec\n\n            self.token2idx_dict = {token: idx for idx, token in enumerate(self.embedding_dict.keys(), 2)}\n            self.token2idx_dict[self.NULL] = 0\n            self.token2idx_dict[self.OOV] = 1\n            self.embedding_dict[self.NULL] = [0.] * self.emb_dim\n            self.embedding_dict[self.OOV] = [0.] * self.emb_dim\n            idx2emb_dict = {idx: self.embedding_dict[token]\n                            for token, idx in self.token2idx_dict.items()}\n            self.emb_mat = np.array([idx2emb_dict[idx] for idx in range(len(idx2emb_dict))])\n\n    def load(self) -> None:\n        logger.info(\'SquadVocabEmbedder: loading saved {}s vocab from {}\'.format(self.level, self.load_path))\n        with self.load_path.open(\'rb\') as f:\n            self.emb_dim, self.emb_mat, self.token2idx_dict = pickle.load(f)\n        self.loaded = True\n\n    def deserialize(self, data: bytes) -> None:\n        self.emb_dim, self.emb_mat, self.token2idx_dict = pickle.loads(data)\n        self.loaded = True\n\n    def save(self) -> None:\n        logger.info(\'SquadVocabEmbedder: saving {}s vocab to {}\'.format(self.level, self.save_path))\n        self.save_path.parent.mkdir(parents=True, exist_ok=True)\n        with self.save_path.open(\'wb\') as f:\n            pickle.dump((self.emb_dim, self.emb_mat, self.token2idx_dict), f, protocol=4)\n\n    def serialize(self) -> bytes:\n        return pickle.dumps((self.emb_dim, self.emb_mat, self.token2idx_dict), protocol=4)\n\n    def _get_idx(self, el: str) -> int:\n        """""" Returns idx for el (token or char).\n\n        Args:\n            el: token or character\n\n        Returns:\n            idx in vocabulary\n        """"""\n        for e in (el, el.lower(), el.capitalize(), el.upper()):\n            if e in self.token2idx_dict:\n                return self.token2idx_dict[e]\n        return 1\n\n\n@register(\'squad_ans_postprocessor\')\nclass SquadAnsPostprocessor(Component):\n    """""" SquadAnsPostprocessor class is responsible for processing SquadModel output.\n\n        It extract answer from context using predicted by SquadModel answer positions.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def __call__(self, ans_start: Tuple[int, ...], ans_end: Tuple[int, ...], contexts: Tuple[str, ...],\n                 p2rs: List[List[int]], spans: List[List[Tuple[int, int]]],\n                 **kwargs) -> Tuple[List[str], List[int], List[int]]:\n        """""" Extracts answer from context using predicted answer positions.\n\n        Args:\n            ans_start: predicted start position in processed context: list of ints with len(ans_start) == batch_size\n            ans_end: predicted end position in processed context\n            contexts: raw contexts\n            p2rs: mapping from processed context to raw\n            spans: tokens positions in context\n\n        Returns:\n            postprocessed answer text, start position in raw context, end position in raw context\n        """"""\n        answers = []\n        start = []\n        end = []\n        for a_st, a_end, c, p2r, span in zip(ans_start, ans_end, contexts, p2rs, spans):\n            if a_st == -1 or a_end == -1:\n                start.append(-1)\n                end.append(-1)\n                answers.append(\'\')\n            else:\n                start.append(p2r[span[a_st][0]])\n                end.append(p2r[span[a_end][1]])\n                answers.append(c[start[-1]:end[-1]])\n        return answers, start, end\n\n\n@register(\'squad_bert_mapping\')\nclass SquadBertMappingPreprocessor(Component):\n    """"""Create mapping from BERT subtokens to their characters positions and vice versa.\n\n        Args:\n            do_lower_case: set True if lowercasing is needed\n\n    """"""\n\n    def __init__(self, do_lower_case: bool = True, *args, **kwargs):\n        self.do_lower_case = do_lower_case\n\n    def __call__(self, contexts, bert_features, **kwargs):\n        subtok2chars = []\n        char2subtoks = []\n        for context, features in zip(contexts, bert_features):\n            if self.do_lower_case:\n                context = context.lower()\n            subtokens = features.tokens\n            context_start = subtokens.index(\'[SEP]\') + 1\n            idx = 0\n            subtok2char = {}\n            char2subtok = {}\n            for i, subtok in list(enumerate(features.tokens))[context_start:-1]:\n                subtok = subtok[2:] if subtok.startswith(\'##\') else subtok\n                subtok_pos = context[idx:].find(subtok)\n                if subtok_pos == -1:\n                    # it could be UNK\n                    idx += 1  # len was at least one\n                else:\n                    # print(k, \'\\t\', t, p + idx)\n                    idx += subtok_pos\n                    subtok2char[i] = idx\n                    for j in range(len(subtok)):\n                        char2subtok[idx + j] = i\n                    idx += len(subtok)\n            subtok2chars.append(subtok2char)\n            char2subtoks.append(char2subtok)\n\n        return subtok2chars, char2subtoks\n\n\n@register(\'squad_bert_ans_preprocessor\')\nclass SquadBertAnsPreprocessor(Component):\n    """"""Create answer start and end positions in subtokens.\n\n        Args:\n            do_lower_case: set True if lowercasing is needed\n\n    """"""\n\n    def __init__(self, do_lower_case: bool = True, *args, **kwargs):\n        self.do_lower_case = do_lower_case\n\n    def __call__(self, answers_raw, answers_start, char2subtoks, **kwargs):\n        answers, starts, ends = [], [], []\n        for answers_raw, answers_start, c2sub in zip(answers_raw, answers_start, char2subtoks):\n            answers.append([])\n            starts.append([])\n            ends.append([])\n            for ans, ans_st in zip(answers_raw, answers_start):\n                if self.do_lower_case:\n                    ans = ans.lower()\n                try:\n                    indices = {c2sub[i] for i in range(ans_st, ans_st + len(ans)) if i in c2sub}\n                    st = min(indices)\n                    end = max(indices)\n                except ValueError:\n                    # 0 - CLS token\n                    st, end = 0, 0\n                    ans = \'\'\n                starts[-1] += [st]\n                ends[-1] += [end]\n                answers[-1] += [ans]\n        return answers, starts, ends\n\n\n@register(\'squad_bert_ans_postprocessor\')\nclass SquadBertAnsPostprocessor(Component):\n    """"""Extract answer and create answer start and end positions in characters from subtoken positions.""""""\n\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def __call__(self, answers_start, answers_end, contexts, bert_features, subtok2chars, **kwargs):\n        answers = []\n        starts = []\n        ends = []\n        for answer_st, answer_end, context, features, sub2c in \\\n                zip(answers_start, answers_end, contexts, bert_features, subtok2chars):\n            # CLS token is no_answer token\n            if answer_st == 0 or answer_end == 0:\n                answers += [\'\']\n                starts += [-1]\n                ends += [-1]\n            else:\n                st = self.get_char_position(sub2c, answer_st)\n                end = self.get_char_position(sub2c, answer_end)\n                subtok = features.tokens[answer_end]\n                subtok = subtok[2:] if subtok.startswith(\'##\') else subtok\n                answer = context[st:end + len(subtok)]\n                answers += [answer]\n                starts += [st]\n                ends += [ends]\n        return answers, starts, ends\n\n    @staticmethod\n    def get_char_position(sub2c, sub_pos):\n        keys = list(sub2c.keys())\n        found_idx = bisect.bisect(keys, sub_pos)\n        if found_idx == 0:\n            return sub2c[keys[0]]\n\n        return sub2c[keys[found_idx - 1]]\n'"
deeppavlov/models/preprocessors/str_lower.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Union\n\nfrom deeppavlov.core.common.registry import register\n\n\n@register(\'str_lower\')\ndef str_lower(batch: Union[str, list, tuple]):\n    """"""Recursively search for strings in a list and convert them to lowercase\n\n    Args:\n        batch: a string or a list containing strings at some level of nesting\n\n    Returns:\n        the same structure where all strings are converted to lowercase\n    """"""\n    if isinstance(batch, str):\n        return batch.lower()\n    else:\n        return list(map(str_lower, batch))\n'"
deeppavlov/models/preprocessors/str_token_reverser.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import List, Union\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\n\nStrTokenReverserInfo = Union[List[str], List[\'StrTokenReverserInfo\']]\n\n\n@register(\'str_token_reverser\')\nclass StrTokenReverser(Component):\n    """"""Component for converting strings to strings with reversed token positions\n\n    Args:\n        tokenized: The parameter is only needed to reverse tokenized strings.\n    """"""\n\n    def __init__(self, tokenized: bool = False, *args, **kwargs) -> None:\n        self.tokenized = tokenized\n\n    @staticmethod\n    def _reverse_str(raw_string):\n        splitted = raw_string.split()\n        splitted.reverse()\n        string = \' \'.join(splitted)\n        return string\n\n    @staticmethod\n    def _reverse_tokens(raw_tokens):\n        raw_tokens.reverse()\n        return raw_tokens\n\n    def __call__(self, batch: Union[str, list, tuple]) -> StrTokenReverserInfo:\n        """"""Recursively search for strings in a list and convert them to strings with reversed token positions\n\n        Args:\n            batch: a string or a list containing strings\n\n        Returns:\n            the same structure where all strings tokens are reversed\n        """"""\n        if isinstance(batch, (list, tuple)):\n            batch = batch.copy()\n\n        if self.tokenized:\n            if isinstance(batch, (list, tuple)):\n                if isinstance(batch[-1], str):\n                    return self._reverse_tokens(batch)\n                else:\n                    return [self(line) for line in batch]\n            raise RuntimeError(f\'The objects passed to the reverser are not list or tuple! \'\n                               f\' But they are {type(batch)}.\'\n                               f\' If you want to passed str type directly use option tokenized = False\')\n        else:\n            if isinstance(batch, (list, tuple)):\n                return [self(line) for line in batch]\n            else:\n                return self._reverse_str(batch)\n'"
deeppavlov/models/preprocessors/str_utf8_encoder.py,0,"b'# originally based on https://github.com/allenai/bilm-tf/blob/master/bilm/data.py\n\n# Modifications copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom collections import Counter, OrderedDict\nfrom itertools import chain\nfrom logging import getLogger\nfrom typing import Union, List, Tuple\n\nimport numpy as np\nfrom overrides import overrides\n\nfrom deeppavlov.core.common.errors import ConfigError\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.estimator import Estimator\n\nlog = getLogger(__name__)\n\nStrUTF8EncoderInfo = Union[List[str], List[\'StrUTF8EncoderInfo\']]\n\n\n@register(\'str_utf8_encoder\')\nclass StrUTF8Encoder(Estimator):\n    """"""Component for encoding all strings to utf8 codes\n\n    Args:\n        max_word_length: Max length of words of input and output batches.\n        pad_special_char_use: Whether to use special char for padding  or not.\n        word_boundary_special_char_use: Whether to add word boundaries by special chars or not.\n        sentence_boundary_special_char_use: Whether to add word boundaries by special chars or not.\n        reversed_sentense_tokens: Whether to use reversed sequences of tokens or not.\n        bos: Name of a special token of the begin of a sentence.\n        eos: Name of a special token of the end of a sentence.\n    """"""\n\n    def __init__(self,\n                 max_word_length: int = 50,\n                 pad_special_char_use: bool = False,\n                 word_boundary_special_char_use: bool = False,\n                 sentence_boundary_special_char_use: bool = False,\n                 reversed_sentense_tokens: bool = False,\n                 bos: str = \'<S>\',\n                 eos: str = \'</S>\',\n                 **kwargs) -> None:\n        super().__init__(**kwargs)\n\n        if word_boundary_special_char_use and max_word_length < 3:\n            raise ConfigError(f""`max_word_length` should be more than 3!"")\n        if max_word_length < 1:\n            raise ConfigError(f""`max_word_length` should be more than 1!"")\n\n        self._max_word_length = max_word_length\n        self._reverse = reversed_sentense_tokens\n\n        self._pad_special_char_use = pad_special_char_use\n        self._word_boundary_special_char_use = word_boundary_special_char_use\n        self._sentence_boundary_special_char_use = sentence_boundary_special_char_use\n\n        # char ids 0-255 come from utf-8 encoding bytes\n        # assign 256-300 to special chars\n        self.bos_char = 256  # <begin sentence>\n        self.eos_char = 257  # <end sentence>\n        self.bow_char = 258  # <begin word>\n        self.eow_char = 259  # <end word>\n        self.pad_char = 260  # <padding>\n\n        self._len = 261  # an upper bound of all indexes\n\n        # the charcter representation of the begin/end of sentence characters\n        def _make_bos_eos(indx):\n            indx = np.array([indx], dtype=np.int32)\n            if self._word_boundary_special_char_use:\n                code = np.pad(indx, (1, 1), \'constant\', constant_values=(self.bow_char, self.eow_char))\n            else:\n                code = indx\n            if self._pad_special_char_use:\n                code = np.pad(code, (0, self._max_word_length - code.shape[0]), \'constant\',\n                              constant_values=(self.pad_char))\n            else:\n                pass\n            return code\n\n        self.bos_chars = _make_bos_eos(self.bos_char)\n        self.eos_chars = _make_bos_eos(self.eos_char)\n\n        if self._sentence_boundary_special_char_use:\n            self._eos_chars = [self.eos_chars]\n            self._bos_chars = [self.bos_chars]\n        else:\n            self._eos_chars = []\n            self._bos_chars = []\n\n        if self.load_path:\n            self.load()\n        else:\n            self.tokens = []\n        self._word_char_ids = OrderedDict()\n\n        for token in self.tokens:\n            self._word_char_ids[token] = self._convert_word_to_char_ids(token)\n        self._word_char_ids[bos] = self.bos_chars\n        self._word_char_ids[eos] = self.eos_chars\n\n    def __call__(self, batch: Union[List[str], Tuple[str]]) -> StrUTF8EncoderInfo:\n        """"""Recursively search for strings in a list and utf8 encode\n\n        Args:\n            batch: a string or a list containing strings\n\n        Returns:\n            the same structure where all strings are utf8 encoded\n        """"""\n        if isinstance(batch, (list, tuple)):\n            if isinstance(batch[-1], str):\n                return self._encode_chars(batch)\n            else:\n                return [self(line) for line in batch]\n        raise RuntimeError(f\'The objects passed to the reverser are not list or tuple of str! \'\n                           f\' But they are {type(batch)}.\')\n\n    @overrides\n    def load(self) -> None:\n        if self.load_path:\n            if self.load_path.is_file():\n                log.info(f""[loading vocabulary from {self.load_path}]"")\n                self.tokens = []\n                for ln in self.load_path.open(\'r\', encoding=\'utf8\'):\n                    token = ln.strip().split()[0]\n                    self.tokens.append(token)\n            else:\n                raise ConfigError(f""Provided `load_path` for {self.__class__.__name__} doesn\'t exist!"")\n        else:\n            raise ConfigError(f""`load_path` for {self} is not provided!"")\n\n    @overrides\n    def save(self) -> None:\n        log.info(f""[saving vocabulary to {self.save_path}]"")\n        with self.save_path.open(\'wt\', encoding=\'utf8\') as f:\n            for token in self._word_char_ids.keys():\n                f.write(\'{}\\n\'.format(token))\n\n    @overrides\n    def fit(self, *args) -> None:\n        words = chain(*args)\n        # filter(None, <>) -- to filter empty words\n        freqs = Counter(filter(None, chain(*words)))\n        for token, _ in freqs.most_common():\n            if not (token in self._word_char_ids):\n                self._word_char_ids[token] = self._convert_word_to_char_ids(token)\n\n    def _convert_word_to_char_ids(self, word):\n\n        code = np.zeros([self._max_word_length], dtype=np.int32)\n        if self._pad_special_char_use:\n            code[:] = self.pad_char\n        if self._word_boundary_special_char_use:\n            word_encoded = word.encode(\'utf-8\', \'ignore\')[:self._max_word_length - 2]\n            code[0] = self.bow_char\n\n            for k, chr_id in enumerate(word_encoded, start=1):\n                code[k] = chr_id\n\n            code[len(word_encoded) + 1] = self.eow_char\n        else:\n            word_encoded = word.encode(\'utf-8\', \'ignore\')[:self._max_word_length]\n\n            for k, chr_id in enumerate(word_encoded):\n                code[k] = chr_id\n\n        if not self._pad_special_char_use:\n            if self._word_boundary_special_char_use:\n                code = code[:len(word_encoded) + 2]\n            else:\n                code = code[:len(word_encoded)]\n        return code\n\n    def _word_to_char_ids(self, word):\n        if word in self._word_char_ids:\n            return self._word_char_ids[word]\n        else:\n            return self._convert_word_to_char_ids(word)\n\n    def _encode_chars(self, sentence):\n        """"""\n        Encode the sentence as a white space delimited string of tokens.\n        """"""\n        chars_ids = [self._word_to_char_ids(cur_word)\n                     for cur_word in sentence]\n        return self._wrap_in_s_char(chars_ids)\n\n    def _wrap_in_s_char(self, chars_ids):\n        chars_ids = chars_ids if self._pad_special_char_use else list(chars_ids)\n        if self._reverse:\n            ret = self._eos_chars + chars_ids + self._bos_chars\n        else:\n            ret = self._bos_chars + chars_ids + self._eos_chars\n        return np.vstack(ret) if self._pad_special_char_use else ret\n\n    def __len__(self):\n        return self._len\n\n    @property\n    def len(self):\n        """"""\n        An upper bound of all indexes.\n        """"""\n        return len(self)\n'"
deeppavlov/models/preprocessors/transformers_preprocessor.py,0,"b'# Copyright 2020 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom logging import getLogger\nfrom typing import List, Union, Tuple\n\nimport numpy as np\nfrom transformers import BertTokenizer\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\n\nlog = getLogger(__name__)\n\n\ndef _pad(data: List[List[Union[int, float]]], value: Union[int, float] = 0):\n    max_len = max(map(len, data))\n    res = np.ones([len(data), max_len], dtype=type(value)) * value\n    for i, item in enumerate(data):\n        res[i][:len(item)] = item\n    return res\n\n\n@register(\'transformers_bert_preprocessor\')\nclass TransformersBertPreprocessor(Component):\n    def __init__(self, vocab_file: str,\n                 do_lower_case: bool = False,\n                 max_seq_length: int = 512,\n                 tokenize_chinese_chars: bool = True,\n                 **kwargs):\n        vocab_file = expand_path(vocab_file)\n        self.tokenizer = BertTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case,\n                                       tokenize_chinese_chars=tokenize_chinese_chars)\n        self.max_seq_length = max_seq_length\n\n    def __call__(self, tokens_batch: Union[List[str], List[List[str]]]) ->\\\n            Tuple[List[List[str]], List[List[str]], np.ndarray, np.ndarray, np.ndarray]:\n\n        if isinstance(tokens_batch[0], str):  # skip for already tokenized text\n            tokens_batch = [self.tokenizer.basic_tokenizer.tokenize(sentence, self.tokenizer.all_special_tokens)\n                            for sentence in tokens_batch]\n        startofword_markers_batch = []\n        subtokens_batch = []\n        for tokens in tokens_batch:\n            startofword_markers = [0]\n            subtokens = [\'[CLS]\']\n            for token in tokens:\n                for i, subtoken in enumerate(self.tokenizer.wordpiece_tokenizer.tokenize(token)):\n                    startofword_markers.append(int(i == 0))\n                    subtokens.append(subtoken)\n            startofword_markers.append(0)\n            subtokens.append(\'[SEP]\')\n            if len(subtokens) > self.max_seq_length:\n                raise RuntimeError(f""input sequence after bert tokenization""\n                                   f"" cannot exceed {self.max_seq_length} tokens."")\n\n            startofword_markers_batch.append(startofword_markers)\n            subtokens_batch.append(subtokens)\n\n        encoded = self.tokenizer.batch_encode_plus([[subtokens, None] for subtokens in subtokens_batch],\n                                                   add_special_tokens=False)\n\n        return (tokens_batch, subtokens_batch,\n                _pad(encoded[\'input_ids\'], value=self.tokenizer.pad_token_id),\n                _pad(startofword_markers_batch), _pad(encoded[\'attention_mask\']))\n'"
deeppavlov/models/ranking/__init__.py,0,b''
deeppavlov/models/ranking/bilstm_gru_siamese_network.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Input, GlobalMaxPooling1D, Lambda, Dense, GRU\nfrom tensorflow.keras.models import Model\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.models.ranking.bilstm_siamese_network import BiLSTMSiameseNetwork\n\nlog = getLogger(__name__)\n\n\n@register(\'bilstm_gru_nn\')\nclass BiLSTMGRUSiameseNetwork(BiLSTMSiameseNetwork):\n    """"""The class implementing a siamese neural network with BiLSTM, GRU and max pooling.\n\n    GRU is used to take into account multi-turn dialogue ``context``.\n\n    Args:\n        len_vocab: A size of the vocabulary to build embedding layer.\n        seed: Random seed.\n        shared_weights: Whether to use shared weights in the model to encode ``contexts`` and ``responses``.\n        embedding_dim: Dimensionality of token (word) embeddings.\n        reccurent: A type of the RNN cell. Possible values are ``lstm`` and ``bilstm``.\n        hidden_dim: Dimensionality of the hidden state of the RNN cell. If ``reccurent`` equals ``bilstm``\n            ``hidden_dim`` should be doubled to get the actual dimensionality.\n        max_pooling: Whether to use max-pooling operation to get ``context`` (``response``) vector representation.\n            If ``False``, the last hidden state of the RNN will be used.\n        triplet_loss: Whether to use a model with triplet loss.\n            If ``False``, a model with crossentropy loss will be used.\n        margin: A margin parameter for triplet loss. Only required if ``triplet_loss`` is set to ``True``.\n        hard_triplets: Whether to use hard triplets sampling to train the model\n            i.e. to choose negative samples close to positive ones.\n            If set to ``False`` random sampling will be used.\n            Only required if ``triplet_loss`` is set to ``True``.\n    """"""\n\n    def create_model(self) -> Model:\n        input = []\n        if self.use_matrix:\n            for i in range(self.num_context_turns + 1):\n                input.append(Input(shape=(self.max_sequence_length,)))\n            context = input[:self.num_context_turns]\n            response = input[-1]\n            emb_layer = self.embedding_layer()\n            emb_c = [emb_layer(el) for el in context]\n            emb_r = emb_layer(response)\n        else:\n            for i in range(self.num_context_turns + 1):\n                input.append(Input(shape=(self.max_sequence_length, self.embedding_dim,)))\n            context = input[:self.num_context_turns]\n            response = input[-1]\n            emb_c = context\n            emb_r = response\n        lstm_layer = self.lstm_layer()\n        lstm_c = [lstm_layer(el) for el in emb_c]\n        lstm_r = lstm_layer(emb_r)\n        pooling_layer = GlobalMaxPooling1D(name=""pooling"")\n        lstm_c = [pooling_layer(el) for el in lstm_c]\n        lstm_r = pooling_layer(lstm_r)\n        lstm_c = [Lambda(lambda x: K.expand_dims(x, 1))(el) for el in lstm_c]\n        lstm_c = Lambda(lambda x: K.concatenate(x, 1))(lstm_c)\n        gru_layer = GRU(2 * self.hidden_dim, name=""gru"")\n        gru_c = gru_layer(lstm_c)\n\n        if self.triplet_mode:\n            dist = Lambda(self._pairwise_distances)([gru_c, lstm_r])\n        else:\n            dist = Lambda(self._diff_mult_dist)([gru_c, lstm_r])\n            dist = Dense(1, activation=\'sigmoid\', name=""score_model"")(dist)\n        model = Model(context + [response], dist)\n        return model\n\n    def create_score_model(self) -> Model:\n        cr = self.model.inputs\n        if self.triplet_mode:\n            emb_c = self.model.get_layer(""gru"").output\n            emb_r = self.model.get_layer(""pooling"").get_output(-1)\n            dist_score = Lambda(lambda x: self.euclidian_dist(x), name=""score_model"")\n            score = dist_score([emb_c, emb_r])\n        else:\n            score = self.model.get_layer(""score_model"").output\n            score = Lambda(lambda x: 1. - K.squeeze(x, -1))(score)\n        score = Lambda(lambda x: 1. - x)(score)\n        model = Model(cr, score)\n        return model\n\n    def create_context_model(self) -> Model:\n        m = Model(self.model.inputs[:-1],\n                  self.model.get_layer(""gru"").output)\n        return m\n\n    def create_response_model(self) -> Model:\n        m = Model(self.model.inputs[-1],\n                  self.model.get_layer(""pooling"").get_output_at(-1))\n        return m\n'"
deeppavlov/models/ranking/bilstm_siamese_network.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import List\n\nimport numpy as np\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.initializers import glorot_uniform, Orthogonal\nfrom tensorflow.keras.layers import (Input, LSTM, Embedding, GlobalMaxPooling1D, Lambda, Dense, Layer, Multiply,\n                                     Bidirectional)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.python.framework.ops import Tensor\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.models.ranking.keras_siamese_model import KerasSiameseModel\n\nlog = getLogger(__name__)\n\n\n@register(\'bilstm_nn\')\nclass BiLSTMSiameseNetwork(KerasSiameseModel):\n    """"""The class implementing a siamese neural network with BiLSTM and max pooling.\n\n    There is a possibility to use a binary cross-entropy loss as well as\n    a triplet loss with random or hard negative sampling.\n\n    Args:\n        len_vocab: A size of the vocabulary to build embedding layer.\n        seed: Random seed.\n        shared_weights: Whether to use shared weights in the model to encode ``contexts`` and ``responses``.\n        embedding_dim: Dimensionality of token (word) embeddings.\n        reccurent: A type of the RNN cell. Possible values are ``lstm`` and ``bilstm``.\n        hidden_dim: Dimensionality of the hidden state of the RNN cell. If ``reccurent`` equals ``bilstm``\n            ``hidden_dim`` should be doubled to get the actual dimensionality.\n        max_pooling: Whether to use max-pooling operation to get ``context`` (``response``) vector representation.\n            If ``False``, the last hidden state of the RNN will be used.\n        triplet_loss: Whether to use a model with triplet loss.\n            If ``False``, a model with crossentropy loss will be used.\n        margin: A margin parameter for triplet loss. Only required if ``triplet_loss`` is set to ``True``.\n        hard_triplets: Whether to use hard triplets sampling to train the model\n            i.e. to choose negative samples close to positive ones.\n            If set to ``False`` random sampling will be used.\n            Only required if ``triplet_loss`` is set to ``True``.\n    """"""\n\n    def __init__(self,\n                 len_vocab: int,\n                 seed: int = None,\n                 shared_weights: bool = True,\n                 embedding_dim: int = 300,\n                 reccurent: str = ""bilstm"",\n                 hidden_dim: int = 300,\n                 max_pooling: bool = True,\n                 triplet_loss: bool = True,\n                 margin: float = 0.1,\n                 hard_triplets: bool = False,\n                 *args,\n                 **kwargs) -> None:\n\n        self.toks_num = len_vocab\n        self.seed = seed\n        self.hidden_dim = hidden_dim\n        self.shared_weights = shared_weights\n        self.pooling = max_pooling\n        self.recurrent = reccurent\n        self.margin = margin\n        self.embedding_dim = embedding_dim\n        self.hard_triplets = hard_triplets\n        self.triplet_mode = triplet_loss\n\n        super(BiLSTMSiameseNetwork, self).__init__(*args, **kwargs)\n\n    def compile(self) -> None:\n        optimizer = Adam(lr=self.learning_rate)\n        if self.triplet_mode:\n            loss = self._triplet_loss\n        else:\n            loss = losses.binary_crossentropy\n        self.model.compile(loss=loss, optimizer=optimizer)\n        self.score_model = self.create_score_model()\n\n    def load_initial_emb_matrix(self) -> None:\n        log.info(""[initializing new `{}`]"".format(self.__class__.__name__))\n        if self.use_matrix:\n            if self.shared_weights:\n                self.model.get_layer(name=""embedding"").set_weights([self.emb_matrix])\n            else:\n                self.model.get_layer(name=""embedding_a"").set_weights([self.emb_matrix])\n                self.model.get_layer(name=""embedding_b"").set_weights([self.emb_matrix])\n\n    def embedding_layer(self) -> Layer:\n        out = Embedding(self.toks_num,\n                        self.embedding_dim,\n                        input_length=self.max_sequence_length,\n                        trainable=True, name=""embedding"")\n        return out\n\n    def lstm_layer(self) -> Layer:\n        if self.pooling:\n            ret_seq = True\n        else:\n            ret_seq = False\n        ker_in = glorot_uniform(seed=self.seed)\n        rec_in = Orthogonal(seed=self.seed)\n        if self.recurrent == ""bilstm"" or self.recurrent is None:\n            out = Bidirectional(LSTM(self.hidden_dim,\n                                     input_shape=(self.max_sequence_length, self.embedding_dim,),\n                                     kernel_initializer=ker_in,\n                                     recurrent_initializer=rec_in,\n                                     return_sequences=ret_seq), merge_mode=\'concat\')\n        elif self.recurrent == ""lstm"":\n            out = LSTM(self.hidden_dim,\n                       input_shape=(self.max_sequence_length, self.embedding_dim,),\n                       kernel_initializer=ker_in,\n                       recurrent_initializer=rec_in,\n                       return_sequences=ret_seq)\n        return out\n\n    def create_model(self) -> Model:\n        if self.use_matrix:\n            context = Input(shape=(self.max_sequence_length,))\n            response = Input(shape=(self.max_sequence_length,))\n            if self.shared_weights:\n                emb_layer_a = self.embedding_layer()\n                emb_layer_b = emb_layer_a\n            else:\n                emb_layer_a = self.embedding_layer()\n                emb_layer_b = self.embedding_layer()\n            emb_c = emb_layer_a(context)\n            emb_r = emb_layer_b(response)\n        else:\n            context = Input(shape=(self.max_sequence_length, self.embedding_dim,))\n            response = Input(shape=(self.max_sequence_length, self.embedding_dim,))\n            emb_c = context\n            emb_r = response\n\n        if self.shared_weights:\n            lstm_layer_a = self.lstm_layer()\n            lstm_layer_b = lstm_layer_a\n        else:\n            lstm_layer_a = self.lstm_layer()\n            lstm_layer_b = self.lstm_layer()\n        lstm_c = lstm_layer_a(emb_c)\n        lstm_r = lstm_layer_b(emb_r)\n        if self.pooling:\n            pooling_layer = GlobalMaxPooling1D(name=""sentence_embedding"")\n            lstm_c = pooling_layer(lstm_c)\n            lstm_r = pooling_layer(lstm_r)\n\n        if self.triplet_mode:\n            dist = Lambda(self._pairwise_distances)([lstm_c, lstm_r])\n        else:\n            dist = Lambda(self._diff_mult_dist)([lstm_c, lstm_r])\n            dist = Dense(1, activation=\'sigmoid\', name=""score_model"")(dist)\n        model = Model([context, response], dist)\n        return model\n\n    def create_score_model(self) -> Model:\n        cr = self.model.inputs\n        if self.triplet_mode:\n            emb_c = self.model.get_layer(""sentence_embedding"").get_output_at(0)\n            emb_r = self.model.get_layer(""sentence_embedding"").get_output_at(1)\n            dist_score = Lambda(lambda x: self._euclidian_dist(x), name=""score_model"")\n            score = dist_score([emb_c, emb_r])\n        else:\n            score = self.model.get_layer(""score_model"").output\n            score = Lambda(lambda x: 1. - K.squeeze(x, -1))(score)\n        score = Lambda(lambda x: 1. - x)(score)\n        model = Model(cr, score)\n        return model\n\n    def _diff_mult_dist(self, inputs: List[Tensor]) -> Tensor:\n        input1, input2 = inputs\n        a = K.abs(input1 - input2)\n        b = Multiply()(inputs)\n        return K.concatenate([input1, input2, a, b])\n\n    def _euclidian_dist(self, x_pair: List[Tensor]) -> Tensor:\n        x1_norm = K.l2_normalize(x_pair[0], axis=1)\n        x2_norm = K.l2_normalize(x_pair[1], axis=1)\n        diff = x1_norm - x2_norm\n        square = K.square(diff)\n        _sum = K.sum(square, axis=1)\n        _sum = K.clip(_sum, min_value=1e-12, max_value=None)\n        dist = K.sqrt(_sum) / 2.\n        return dist\n\n    def _pairwise_distances(self, inputs: List[Tensor]) -> Tensor:\n        emb_c, emb_r = inputs\n        bs = K.shape(emb_c)[0]\n        embeddings = K.concatenate([emb_c, emb_r], 0)\n        dot_product = K.dot(embeddings, K.transpose(embeddings))\n        square_norm = K.batch_dot(embeddings, embeddings, axes=1)\n        distances = K.transpose(square_norm) - 2.0 * dot_product + square_norm\n        distances = distances[0:bs, bs:bs+bs]\n        distances = K.clip(distances, 0.0, None)\n        mask = K.cast(K.equal(distances, 0.0), K.dtype(distances))\n        distances = distances + mask * 1e-16\n        distances = K.sqrt(distances)\n        distances = distances * (1.0 - mask)\n        return distances\n\n    def _triplet_loss(self, labels: Tensor, pairwise_dist: Tensor) -> Tensor:\n        y_true = K.squeeze(labels, axis=1)\n        """"""Triplet loss function""""""\n        if self.hard_triplets:\n            triplet_loss = self._batch_hard_triplet_loss(y_true, pairwise_dist)\n        else:\n            triplet_loss = self._batch_all_triplet_loss(y_true, pairwise_dist)\n        return triplet_loss\n\n    def _batch_all_triplet_loss(self, y_true: Tensor, pairwise_dist: Tensor) -> Tensor:\n        anchor_positive_dist = K.expand_dims(pairwise_dist, 2)\n        anchor_negative_dist = K.expand_dims(pairwise_dist, 1)\n        triplet_loss = anchor_positive_dist - anchor_negative_dist + self.margin\n        mask = self._get_triplet_mask(y_true, pairwise_dist)\n        triplet_loss = mask * triplet_loss\n        triplet_loss = K.clip(triplet_loss, 0.0, None)\n        valid_triplets = K.cast(K.greater(triplet_loss, 1e-16), K.dtype(triplet_loss))\n        num_positive_triplets = K.sum(valid_triplets)\n        triplet_loss = K.sum(triplet_loss) / (num_positive_triplets + 1e-16)\n        return triplet_loss\n\n    def _batch_hard_triplet_loss(self, y_true: Tensor, pairwise_dist: Tensor) -> Tensor:\n        mask_anchor_positive = self._get_anchor_positive_triplet_mask(y_true, pairwise_dist)\n        anchor_positive_dist = mask_anchor_positive * pairwise_dist\n        hardest_positive_dist = K.max(anchor_positive_dist, axis=1, keepdims=True)\n        mask_anchor_negative = self._get_anchor_negative_triplet_mask(y_true, pairwise_dist)\n        anchor_negative_dist = mask_anchor_negative * pairwise_dist\n        mask_anchor_negative = self._get_semihard_anchor_negative_triplet_mask(anchor_negative_dist,\n                                                                               hardest_positive_dist,\n                                                                               mask_anchor_negative)\n        max_anchor_negative_dist = K.max(pairwise_dist, axis=1, keepdims=True)\n        anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n        hardest_negative_dist = K.min(anchor_negative_dist, axis=1, keepdims=True)\n        triplet_loss = K.clip(hardest_positive_dist - hardest_negative_dist + self.margin, 0.0, None)\n        triplet_loss = K.mean(triplet_loss)\n        return triplet_loss\n\n    def _get_triplet_mask(self, y_true: Tensor, pairwise_dist: Tensor) -> Tensor:\n        # mask label(a) != label(p)\n        mask1 = K.expand_dims(K.equal(K.expand_dims(y_true, 0), K.expand_dims(y_true, 1)), 2)\n        mask1 = K.cast(mask1, K.dtype(pairwise_dist))\n        # mask a == p\n        mask2 = K.expand_dims(K.not_equal(pairwise_dist, 0.0), 2)\n        mask2 = K.cast(mask2, K.dtype(pairwise_dist))\n        # mask label(n) == label(a)\n        mask3 = K.expand_dims(K.not_equal(K.expand_dims(y_true, 0), K.expand_dims(y_true, 1)), 1)\n        mask3 = K.cast(mask3, K.dtype(pairwise_dist))\n        return mask1 * mask2 * mask3\n\n    def _get_anchor_positive_triplet_mask(self, y_true: Tensor, pairwise_dist: Tensor) -> Tensor:\n        # mask label(a) != label(p)\n        mask1 = K.equal(K.expand_dims(y_true, 0), K.expand_dims(y_true, 1))\n        mask1 = K.cast(mask1, K.dtype(pairwise_dist))\n        # mask a == p\n        mask2 = K.not_equal(pairwise_dist, 0.0)\n        mask2 = K.cast(mask2, K.dtype(pairwise_dist))\n        return mask1 * mask2\n\n    def _get_anchor_negative_triplet_mask(self, y_true: Tensor, pairwise_dist: Tensor) -> Tensor:\n        # mask label(n) == label(a)\n        mask = K.not_equal(K.expand_dims(y_true, 0), K.expand_dims(y_true, 1))\n        mask = K.cast(mask, K.dtype(pairwise_dist))\n        return mask\n\n    def _get_semihard_anchor_negative_triplet_mask(self, negative_dist: Tensor,\n                                                   hardest_positive_dist: Tensor,\n                                                   mask_negative: Tensor) -> Tensor:\n        # mask max(dist(a,p)) < dist(a,n)\n        mask = K.greater(negative_dist, hardest_positive_dist)\n        mask = K.cast(mask, K.dtype(negative_dist))\n        mask_semihard = K.cast(K.expand_dims(K.greater(K.sum(mask, 1), 0.0), 1), K.dtype(negative_dist))\n        mask = mask_negative * (1 - mask_semihard) + mask * mask_semihard\n        return mask\n\n    def _predict_on_batch(self, batch: List[np.ndarray]) -> np.ndarray:\n        return self.score_model.predict_on_batch(x=batch)\n'"
deeppavlov/models/ranking/deep_attention_matching_network.py,39,"b'# Copyright 2018 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import Optional\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.models.ranking.matching_models.dam_utils import layers\nfrom deeppavlov.models.ranking.matching_models.dam_utils import operations as op\nfrom deeppavlov.models.ranking.tf_base_matching_model import TensorflowBaseMatchingModel\n\nlog = getLogger(__name__)\n\n\n@register(\'dam_nn\')\nclass DAMNetwork(TensorflowBaseMatchingModel):\n    """"""\n    Tensorflow implementation of Deep Attention Matching Network (DAM)\n\n    ```\n    @inproceedings{ ,\n      title={Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network},\n      author={Xiangyang Zhou, Lu Li, Daxiang Dong, Yi Liu, Ying Chen, Wayne Xin Zhao, Dianhai Yu and Hua Wu},\n      booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n      volume={1},\n      pages={  --  },\n      year={2018}\n    }\n    ```\n    http://aclweb.org/anthology/P18-1103\n\n    Based on authors\' Tensorflow code: https://github.com/baidu/Dialogue/tree/master/DAM\n\n    Args:\n        num_context_turns (int): A number of ``context`` turns in data samples.\n        max_sequence_length(int): A maximum length of text sequences in tokens.\n            Longer sequences will be truncated and shorter ones will be padded.\n        learning_rate (float): Initial learning rate.\n        emb_matrix (np.ndarray): An embeddings matrix to initialize an embeddings layer of a model.\n        trainable_embeddings (bool): Whether train embeddings matrix or not.\n        embedding_dim (int): Dimensionality of token (word) embeddings.\n        is_positional (bool): Adds a bunch of sinusoids of different frequencies to an embeddings.\n        filters2_conv3d (int): number of filters in the second conv3d layer (cnn aggregation). Default: 16.\n        stack_num (int): Number of stack layers, default is 5.\n        seed (int): Random seed.\n        decay_steps (int): Number of steps after which is to decay the learning rate.\n    """"""\n\n    def __init__(self,\n                 embedding_dim: int = 200,\n                 max_sequence_length: int = 50,\n                 learning_rate: float = 1e-3,\n                 emb_matrix: Optional[np.ndarray] = None,\n                 trainable_embeddings: bool = False,\n                 is_positional: bool = True,\n                 filters2_conv3d: int = 16,\n                 stack_num: int = 5,\n                 seed: int = 65,\n                 decay_steps: int = 600,\n                 *args,\n                 **kwargs):\n\n        self.seed = seed\n        tf.set_random_seed(self.seed)\n\n        self.max_sentence_len = max_sequence_length\n        self.word_embedding_size = embedding_dim\n        self.trainable = trainable_embeddings\n        self.is_positional = is_positional\n        self.stack_num = stack_num\n        self.filters2_conv3d = filters2_conv3d\n\n        self.learning_rate = learning_rate\n        self.emb_matrix = emb_matrix\n        self.decay_steps = decay_steps\n\n        super(DAMNetwork, self).__init__(*args, **kwargs)\n\n        self.sess_config = tf.ConfigProto(allow_soft_placement=True)\n        self.sess_config.gpu_options.allow_growth = True\n        self.sess = tf.Session(config=self.sess_config)\n        self._init_graph()\n        self.sess.run(tf.global_variables_initializer())\n\n        if self.load_path is not None:\n            self.load()\n\n    def _init_placeholders(self):\n        with tf.variable_scope(\'inputs\'):\n            # Utterances and their lengths\n            self.utterance_ph = tf.placeholder(tf.int32, shape=(None, self.num_context_turns, self.max_sentence_len))\n            self.all_utterance_len_ph = tf.placeholder(tf.int32, shape=(None, self.num_context_turns))\n\n            # Responses and their lengths\n            self.response_ph = tf.placeholder(tf.int32, shape=(None, self.max_sentence_len))\n            self.response_len_ph = tf.placeholder(tf.int32, shape=(None,))\n\n            # Labels\n            self.y_true = tf.placeholder(tf.int32, shape=(None,))\n\n    def _init_graph(self):\n        self._init_placeholders()\n\n        with tf.variable_scope(\'embedding_matrix_init\'):\n            word_embeddings = tf.get_variable(""word_embeddings_v"",\n                                              initializer=tf.constant(self.emb_matrix, dtype=tf.float32),\n                                              trainable=self.trainable)\n        with tf.variable_scope(\'embedding_lookup\'):\n            response_embeddings = tf.nn.embedding_lookup(word_embeddings, self.response_ph)\n\n        Hr = response_embeddings\n        if self.is_positional and self.stack_num > 0:\n            with tf.variable_scope(\'positional\'):\n                Hr = op.positional_encoding_vector(Hr, max_timescale=10)\n\n        Hr_stack = [Hr]\n\n        for index in range(self.stack_num):\n            with tf.variable_scope(\'self_stack_\' + str(index)):\n                Hr = layers.block(\n                    Hr, Hr, Hr,\n                    Q_lengths=self.response_len_ph, K_lengths=self.response_len_ph, attention_type=\'dot\')\n                Hr_stack.append(Hr)\n\n        # context part\n        # a list of length max_turn_num, every element is a tensor with shape [batch, max_turn_len]\n        list_turn_t = tf.unstack(self.utterance_ph, axis=1)\n        list_turn_length = tf.unstack(self.all_utterance_len_ph, axis=1)\n\n        sim_turns = []\n        # for every turn_t calculate matching vector\n        for turn_t, t_turn_length in zip(list_turn_t, list_turn_length):\n            Hu = tf.nn.embedding_lookup(word_embeddings, turn_t)  # [batch, max_turn_len, emb_size]\n\n            if self.is_positional and self.stack_num > 0:\n                with tf.variable_scope(\'positional\', reuse=True):\n                    Hu = op.positional_encoding_vector(Hu, max_timescale=10)\n            Hu_stack = [Hu]\n\n            for index in range(self.stack_num):\n                with tf.variable_scope(\'self_stack_\' + str(index), reuse=True):\n                    Hu = layers.block(\n                        Hu, Hu, Hu,\n                        Q_lengths=t_turn_length, K_lengths=t_turn_length, attention_type=\'dot\')\n\n                    Hu_stack.append(Hu)\n\n            r_a_t_stack = []\n            t_a_r_stack = []\n            for index in range(self.stack_num + 1):\n\n                with tf.variable_scope(\'t_attend_r_\' + str(index)):\n                    try:\n                        t_a_r = layers.block(\n                            Hu_stack[index], Hr_stack[index], Hr_stack[index],\n                            Q_lengths=t_turn_length, K_lengths=self.response_len_ph, attention_type=\'dot\')\n                    except ValueError:\n                        tf.get_variable_scope().reuse_variables()\n                        t_a_r = layers.block(\n                            Hu_stack[index], Hr_stack[index], Hr_stack[index],\n                            Q_lengths=t_turn_length, K_lengths=self.response_len_ph, attention_type=\'dot\')\n\n                with tf.variable_scope(\'r_attend_t_\' + str(index)):\n                    try:\n                        r_a_t = layers.block(\n                            Hr_stack[index], Hu_stack[index], Hu_stack[index],\n                            Q_lengths=self.response_len_ph, K_lengths=t_turn_length, attention_type=\'dot\')\n                    except ValueError:\n                        tf.get_variable_scope().reuse_variables()\n                        r_a_t = layers.block(\n                            Hr_stack[index], Hu_stack[index], Hu_stack[index],\n                            Q_lengths=self.response_len_ph, K_lengths=t_turn_length, attention_type=\'dot\')\n\n                t_a_r_stack.append(t_a_r)\n                r_a_t_stack.append(r_a_t)\n\n            t_a_r_stack.extend(Hu_stack)\n            r_a_t_stack.extend(Hr_stack)\n\n            t_a_r = tf.stack(t_a_r_stack, axis=-1)\n            r_a_t = tf.stack(r_a_t_stack, axis=-1)\n\n            # log.info(t_a_r, r_a_t)  # debug\n\n            # calculate similarity matrix\n            with tf.variable_scope(\'similarity\'):\n                # sim shape [batch, max_turn_len, max_turn_len, 2*stack_num+1]\n                # divide sqrt(200) to prevent gradient explosion\n                sim = tf.einsum(\'biks,bjks->bijs\', t_a_r, r_a_t) / tf.sqrt(float(self.word_embedding_size))\n\n            sim_turns.append(sim)\n\n        # cnn and aggregation\n        sim = tf.stack(sim_turns, axis=1)\n        log.info(\'sim shape: %s\' % sim.shape)\n        with tf.variable_scope(\'cnn_aggregation\'):\n            final_info = layers.CNN_3d(sim, 32, self.filters2_conv3d)\n            # for douban\n            # final_info = layers.CNN_3d(sim, 16, 16)\n\n        # loss and train\n        with tf.variable_scope(\'loss\'):\n            self.loss, self.logits = layers.loss(final_info, self.y_true, clip_value=10.)\n            self.y_pred = tf.nn.softmax(self.logits, name=""y_pred"")\n            tf.summary.scalar(\'loss\', self.loss)\n\n            self.global_step = tf.Variable(0, trainable=False)\n            initial_learning_rate = self.learning_rate\n            self.learning_rate = tf.train.exponential_decay(\n                initial_learning_rate,\n                global_step=self.global_step,\n                decay_steps=self.decay_steps,\n                decay_rate=0.9,\n                staircase=True)\n\n            Optimizer = tf.train.AdamOptimizer(self.learning_rate)\n            self.grads_and_vars = Optimizer.compute_gradients(self.loss)\n\n            for grad, var in self.grads_and_vars:\n                if grad is None:\n                    log.info(var)\n\n            self.capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in self.grads_and_vars]\n            self.train_op = Optimizer.apply_gradients(\n                self.capped_gvs,\n                global_step=self.global_step)\n\n        # Debug\n        self.print_number_of_parameters()\n'"
deeppavlov/models/ranking/deep_attention_matching_network_use_transformer.py,57,"b'# Copyright 2018 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import List, Dict, Tuple, Optional\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.models.ranking.matching_models.dam_utils import layers\nfrom deeppavlov.models.ranking.matching_models.dam_utils import operations as op\nfrom deeppavlov.models.ranking.tf_base_matching_model import TensorflowBaseMatchingModel\n\nlog = getLogger(__name__)\n\n\n@register(\'dam_nn_use_transformer\')\nclass DAMNetworkUSETransformer(TensorflowBaseMatchingModel):\n    """"""\n    Tensorflow implementation of Deep Attention Matching Network (DAM) [1] improved with USE [2]. We called it DAM-USE-T\n    ```\n    http://aclweb.org/anthology/P18-1103\n\n    Based on Tensorflow code: https://github.com/baidu/Dialogue/tree/master/DAM\n    We added USE-T [2] as a sentence encoder to the DAM network to achieve state-of-the-art performance on the datasets:\n    * Ubuntu Dialogue Corpus v1 (R@1: 0.7929, R@2: 0.8912, R@5: 0.9742)\n    * Ubuntu Dialogue Corpus v2 (R@1: 0.7414, R@2: 0.8656, R@5: 0.9731)\n\n    References:\n    [1]\n    ```\n    @inproceedings{ ,\n      title={Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network},\n      author={Xiangyang Zhou, Lu Li, Daxiang Dong, Yi Liu, Ying Chen, Wayne Xin Zhao, Dianhai Yu and Hua Wu},\n      booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n      volume={1},\n      pages={  --  },\n      year={2018}\n    }\n    ```\n    [2] Cer D, Yang Y, Kong S-y, Hua N, Limtiaco N, John RS, et al. 2018. Universal sentence encoder.\n    arXiv preprint arXiv:1803.11175 2018.\n\n    Args:\n        num_context_turns (int): A number of ``context`` turns in data samples.\n        max_sequence_length(int): A maximum length of text sequences in tokens.\n            Longer sequences will be truncated and shorter ones will be padded.\n        learning_rate (float): Initial learning rate.\n        emb_matrix (np.ndarray): An embeddings matrix to initialize an embeddings layer of a model.\n        trainable_embeddings (bool): Whether train embeddings matrix or not.\n        embedding_dim (int): Dimensionality of token (word) embeddings.\n        is_positional (bool): Adds a bunch of sinusoids of different frequencies to an embeddings.\n        stack_num (int): Number of stack layers, default is 5.\n        seed (int): Random seed.\n        decay_steps (int): Number of steps after which is to decay the learning rate.\n    """"""\n\n    def __init__(self,\n                 embedding_dim: int = 200,\n                 max_sequence_length: int = 50,\n                 learning_rate: float = 1e-3,\n                 emb_matrix: Optional[np.ndarray] = None,\n                 trainable_embeddings: bool = False,\n                 is_positional: bool = True,\n                 stack_num: int = 5,\n                 seed: int = 65,\n                 decay_steps: int = 600,\n                 *args,\n                 **kwargs):\n\n        self.seed = seed\n        tf.set_random_seed(self.seed)\n\n        self.max_sentence_len = max_sequence_length\n        self.word_embedding_size = embedding_dim\n        self.trainable = trainable_embeddings\n        self.is_positional = is_positional\n        self.stack_num = stack_num\n        self.learning_rate = learning_rate\n        self.emb_matrix = emb_matrix\n        self.decay_steps = decay_steps\n\n        super(DAMNetworkUSETransformer, self).__init__(*args, **kwargs)\n\n        ##############################################################################\n        self._init_graph()\n        self.sess_config = tf.ConfigProto(allow_soft_placement=True)\n        self.sess_config.gpu_options.allow_growth = True\n        self.sess = tf.Session(config=self.sess_config)\n        self.sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n        ##############################################################################\n\n        if self.load_path is not None:\n            self.load()\n\n    def _init_placeholders(self):\n        """""" Init model placeholders """"""\n        with tf.variable_scope(\'inputs\'):\n            # Utterances and their lengths\n            self.utterance_ph = tf.placeholder(tf.int32, shape=(None, self.num_context_turns, self.max_sentence_len))\n            self.all_utterance_len_ph = tf.placeholder(tf.int32, shape=(None, self.num_context_turns))\n\n            # Responses and their lengths\n            self.response_ph = tf.placeholder(tf.int32, shape=(None, self.max_sentence_len))\n            self.response_len_ph = tf.placeholder(tf.int32, shape=(None,))\n\n            # Labels\n            self.y_true = tf.placeholder(tf.int32, shape=(None,))\n\n            # Raw sentences for context and response\n            self.context_sent_ph = tf.placeholder(tf.string,\n                                                  shape=(None, self.num_context_turns),\n                                                  name=""context_sentences"")\n            self.response_sent_ph = tf.placeholder(tf.string, shape=(None,), name=""response_sentences"")\n\n    def _init_sentence_encoder(self):\n        """""" Init sentence encoder, for example USE-T """"""\n        # sentence encoder\n        self.embed = hub.Module(""https://tfhub.dev/google/universal-sentence-encoder-large/3"",\n                                trainable=False)\n\n        # embed sentences of context\n        with tf.variable_scope(\'sentence_embeddings\'):\n            x = []\n            for i in range(self.num_context_turns):\n                x.append(self.embed(tf.reshape(self.context_sent_ph[:, i], shape=(tf.shape(self.context_sent_ph)[0],))))\n            embed_context_turns = tf.stack(x, axis=1)\n            embed_response = self.embed(self.response_sent_ph)\n\n            # for context sentences: shape=(None, self.num_context_turns, 1, 512)\n            self.sent_embedder_context = tf.expand_dims(embed_context_turns, axis=2)\n            # for resp sentences: shape=(None, 1, 512)\n            self.sent_embedder_response = tf.expand_dims(embed_response, axis=1)\n\n    def _init_graph(self):\n        self._init_placeholders()\n        self._init_sentence_encoder()\n\n        with tf.variable_scope(\'sentence_emb_dim_reduction\'):\n            dense_emb = tf.layers.Dense(200,\n                                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=42),\n                                        kernel_regularizer=tf.keras.regularizers.l2(),\n                                        bias_regularizer=tf.keras.regularizers.l2(),\n                                        trainable=True)\n\n            a = []\n            for i in range(self.num_context_turns):\n                a.append(dense_emb(self.sent_embedder_context[:, i]))\n            sent_embedder_context = tf.stack(a, axis=1)\n            sent_embedder_response = dense_emb(self.sent_embedder_response)\n\n        with tf.variable_scope(\'embedding_matrix_init\'):\n            word_embeddings = tf.get_variable(""word_embeddings_v"",\n                                              initializer=tf.constant(self.emb_matrix, dtype=tf.float32),\n                                              trainable=self.trainable)\n        with tf.variable_scope(\'embedding_lookup\'):\n            response_embeddings = tf.nn.embedding_lookup(word_embeddings, self.response_ph)\n\n        Hr = response_embeddings\n        if self.is_positional and self.stack_num > 0:\n            with tf.variable_scope(\'positional\'):\n                Hr = op.positional_encoding_vector(Hr, max_timescale=10)\n\n        with tf.variable_scope(\'expand_resp_embeddings\'):\n            Hr = tf.concat([sent_embedder_response, Hr], axis=1)\n\n        Hr_stack = [Hr]\n\n        for index in range(self.stack_num):\n            with tf.variable_scope(\'self_stack_\' + str(index)):\n                Hr = layers.block(\n                    Hr, Hr, Hr,\n                    Q_lengths=self.response_len_ph, K_lengths=self.response_len_ph, attention_type=\'dot\')\n                Hr_stack.append(Hr)\n\n        # context part\n        # a list of length max_turn_num, every element is a tensor with shape [batch, max_turn_len]\n        list_turn_t = tf.unstack(self.utterance_ph, axis=1)\n        list_turn_length = tf.unstack(self.all_utterance_len_ph, axis=1)\n        list_turn_t_sent = tf.unstack(sent_embedder_context, axis=1)\n\n        sim_turns = []\n        # for every turn_t calculate matching vector\n        for turn_t, t_turn_length, turn_t_sent in zip(list_turn_t, list_turn_length, list_turn_t_sent):\n            Hu = tf.nn.embedding_lookup(word_embeddings, turn_t)  # [batch, max_turn_len, emb_size]\n\n            if self.is_positional and self.stack_num > 0:\n                with tf.variable_scope(\'positional\', reuse=True):\n                    Hu = op.positional_encoding_vector(Hu, max_timescale=10)\n\n            with tf.variable_scope(\'expand_cont_embeddings\'):\n                Hu = tf.concat([turn_t_sent, Hu], axis=1)\n\n            Hu_stack = [Hu]\n\n            for index in range(self.stack_num):\n                with tf.variable_scope(\'self_stack_\' + str(index), reuse=True):\n                    Hu = layers.block(\n                        Hu, Hu, Hu,\n                        Q_lengths=t_turn_length, K_lengths=t_turn_length, attention_type=\'dot\')\n\n                    Hu_stack.append(Hu)\n\n            r_a_t_stack = []\n            t_a_r_stack = []\n            for index in range(self.stack_num + 1):\n\n                with tf.variable_scope(\'t_attend_r_\' + str(index)):\n                    try:\n                        t_a_r = layers.block(\n                            Hu_stack[index], Hr_stack[index], Hr_stack[index],\n                            Q_lengths=t_turn_length, K_lengths=self.response_len_ph, attention_type=\'dot\')\n                    except ValueError:\n                        tf.get_variable_scope().reuse_variables()\n                        t_a_r = layers.block(\n                            Hu_stack[index], Hr_stack[index], Hr_stack[index],\n                            Q_lengths=t_turn_length, K_lengths=self.response_len_ph, attention_type=\'dot\')\n\n                with tf.variable_scope(\'r_attend_t_\' + str(index)):\n                    try:\n                        r_a_t = layers.block(\n                            Hr_stack[index], Hu_stack[index], Hu_stack[index],\n                            Q_lengths=self.response_len_ph, K_lengths=t_turn_length, attention_type=\'dot\')\n                    except ValueError:\n                        tf.get_variable_scope().reuse_variables()\n                        r_a_t = layers.block(\n                            Hr_stack[index], Hu_stack[index], Hu_stack[index],\n                            Q_lengths=self.response_len_ph, K_lengths=t_turn_length, attention_type=\'dot\')\n\n                t_a_r_stack.append(t_a_r)\n                r_a_t_stack.append(r_a_t)\n\n            t_a_r_stack.extend(Hu_stack)\n            r_a_t_stack.extend(Hr_stack)\n\n            t_a_r = tf.stack(t_a_r_stack, axis=-1)\n            r_a_t = tf.stack(r_a_t_stack, axis=-1)\n\n            # log.info(t_a_r, r_a_t)  # debug\n\n            # calculate similarity matrix\n            with tf.variable_scope(\'similarity\'):\n                # sim shape [batch, max_turn_len, max_turn_len, 2*stack_num+1]\n                # divide sqrt(200) to prevent gradient explosion\n                sim = tf.einsum(\'biks,bjks->bijs\', t_a_r, r_a_t) / tf.sqrt(float(self.word_embedding_size))\n\n            sim_turns.append(sim)\n\n        # cnn and aggregation\n        sim = tf.stack(sim_turns, axis=1)\n        log.info(\'sim shape: %s\' % sim.shape)\n        with tf.variable_scope(\'cnn_aggregation\'):\n            final_info = layers.CNN_3d(sim, 32, 32)  # We can improve performance if use 32 filters for each layer\n            # for douban\n            # final_info = layers.CNN_3d(sim, 16, 16)\n\n        # loss and train\n        with tf.variable_scope(\'loss\'):\n            self.loss, self.logits = layers.loss(final_info, self.y_true, clip_value=10.)\n            self.y_pred = tf.nn.softmax(self.logits, name=""y_pred"")\n            tf.summary.scalar(\'loss\', self.loss)\n\n            self.global_step = tf.Variable(0, trainable=False)\n            initial_learning_rate = self.learning_rate\n            self.learning_rate = tf.train.exponential_decay(\n                initial_learning_rate,\n                global_step=self.global_step,\n                decay_steps=self.decay_steps,\n                decay_rate=0.9,\n                staircase=True)\n\n            Optimizer = tf.train.AdamOptimizer(self.learning_rate)\n            self.grads_and_vars = Optimizer.compute_gradients(self.loss)\n\n            for grad, var in self.grads_and_vars:\n                if grad is None:\n                    log.info(var)\n\n            self.capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in self.grads_and_vars]\n            self.train_op = Optimizer.apply_gradients(\n                self.capped_gvs,\n                global_step=self.global_step)\n\n        # Debug\n        self.print_number_of_parameters()\n\n    def _append_sample_to_batch_buffer(self, sample: List[np.ndarray], buf: List[Tuple]):\n        """"""\n        The function for adding samples to the batch buffer\n\n        Args:\n            sample (List[nd.array]): samples generator\n            buf (List[Tuple[np.ndarray]]) : List of samples with model inputs each:\n                [( context, context_len, response, response_len ), ( ... ), ... ].\n\n        Returns:\n             None\n        """"""\n        sample_len = len(sample)\n\n        batch_buffer_context = []  # [batch_size, 10, 50]\n        batch_buffer_context_len = []  # [batch_size, 10]\n        batch_buffer_response = []  # [batch_size, 50]\n        batch_buffer_response_len = []  # [batch_size]\n\n        raw_batch_buffer_context = []  # [batch_size, 10]\n        raw_batch_buffer_response = []  # [batch_size]\n\n        context_sentences = sample[:self.num_context_turns]\n        response_sentences = sample[self.num_context_turns:sample_len // 2]\n\n        raw_context_sentences = sample[sample_len // 2:sample_len // 2 + self.num_context_turns]\n        raw_response_sentences = sample[sample_len // 2 + self.num_context_turns:]\n\n        # Format model inputs:\n        # 4 model inputs\n\n        # 1. Token indices for context\n        batch_buffer_context += [context_sentences for sent in response_sentences]  # replicate context N times\n        # 2. Token indices for response\n        batch_buffer_response += [response_sentence for response_sentence in response_sentences]\n        # 3. Lengths of all context sentences\n        lens = []\n        for context in [context_sentences for sent in response_sentences]:  # replicate context N times\n            context_sentences_lens = []\n            for sent in context:\n                sent_len = len(sent[sent != 0])\n                sent_len = sent_len + 1 if sent_len > 0 else 0  # 1 additional token is the USE token\n                context_sentences_lens.append(sent_len)\n            lens.append(context_sentences_lens)\n        batch_buffer_context_len += lens\n        # 4. Length of response\n        lens = []\n        for response in [response_sentence for response_sentence in response_sentences]:\n            sent_len = len(response[response != 0])\n            sent_len = sent_len + 1 if sent_len > 0 else 0  # 1 additional token is the USE token\n            lens.append(sent_len)\n        batch_buffer_response_len += lens\n        # 5. Raw context sentences\n        raw_batch_buffer_context += [raw_context_sentences for sent in raw_response_sentences]\n        # 6. Raw response sentences\n        raw_batch_buffer_response += [raw_sent for raw_sent in raw_response_sentences]\n\n        for i in range(len(batch_buffer_context)):\n            buf.append(tuple((\n                batch_buffer_context[i],\n                batch_buffer_context_len[i],\n                batch_buffer_response[i],\n                batch_buffer_response_len[i],\n                raw_batch_buffer_context[i],\n                raw_batch_buffer_response[i]\n            )))\n        return len(response_sentences)\n\n    def _make_batch(self, batch: List[Tuple[np.ndarray]]) -> Dict:\n        """"""\n        The function for formatting model inputs\n\n        Args:\n            batch (List[Tuple[np.ndarray]]): List of samples with model inputs each:\n                [( context, context_len, response, response_len ), ( ... ), ... ].\n            graph (str): which graph the inputs is preparing for\n\n        Returns:\n            Dict: feed_dict to feed a model\n        """"""\n        input_context = []\n        input_context_len = []\n        input_response = []\n        input_response_len = []\n        input_raw_context = []\n        input_raw_response = []\n\n        # format model inputs for MAIN graph as numpy arrays\n        for sample in batch:\n            input_context.append(sample[0])\n            input_context_len.append(sample[1])\n            input_response.append(sample[2])\n            input_response_len.append(sample[3])\n            input_raw_context.append(sample[4])  # raw context is the 4th element of each Tuple in the batch\n            input_raw_response.append(sample[5])  # raw response is the 5th element of each Tuple in the batch\n\n        return {\n            self.utterance_ph: np.array(input_context),\n            self.all_utterance_len_ph: np.array(input_context_len),\n            self.response_ph: np.array(input_response),\n            self.response_len_ph: np.array(input_response_len),\n            self.context_sent_ph: np.array(input_raw_context),\n            self.response_sent_ph: np.array(input_raw_response)\n        }\n'"
deeppavlov/models/ranking/keras_siamese_model.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import abstractmethod\nfrom logging import getLogger\nfrom typing import List\n\nimport numpy as np\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\nfrom deeppavlov.core.models.keras_model import KerasModel\nfrom deeppavlov.models.ranking.siamese_model import SiameseModel\n\nlog = getLogger(__name__)\n\n\nclass KerasSiameseModel(SiameseModel, KerasModel):\n    """"""The class implementing base functionality for siamese neural networks in keras.\n\n    Args:\n        learning_rate: Learning rate.\n        use_matrix: Whether to use a trainable matrix with token (word) embeddings.\n        emb_matrix: An embeddings matrix to initialize an embeddings layer of a model.\n            Only used if ``use_matrix`` is set to ``True``.\n        max_sequence_length: A maximum length of text sequences in tokens.\n            Longer sequences will be truncated and shorter ones will be padded.\n        dynamic_batch:  Whether to use dynamic batching. If ``True``, the maximum length of a sequence for a batch\n            will be equal to the maximum of all sequences lengths from this batch,\n            but not higher than ``max_sequence_length``.\n        attention: Whether any attention mechanism is used in the siamese network.\n        *args: Other parameters.\n        **kwargs: Other parameters.\n    """"""\n\n    def __init__(self,\n                 learning_rate: float = 1e-3,\n                 use_matrix: bool = True,\n                 emb_matrix: np.ndarray = None,\n                 max_sequence_length: int = None,\n                 dynamic_batch: bool = False,\n                 attention: bool = False,\n                 *args,\n                 **kwargs) -> None:\n\n        super(KerasSiameseModel, self).__init__(*args, **kwargs)\n\n        self.learning_rate = learning_rate\n        self.attention = attention\n        self.use_matrix = use_matrix\n        self.emb_matrix = emb_matrix\n        if dynamic_batch:\n            self.max_sequence_length = None\n        else:\n            self.max_sequence_length = max_sequence_length\n        self.model = self.create_model()\n        self.compile()\n        if self.load_path.exists():\n            self.load()\n        else:\n            self.load_initial_emb_matrix()\n\n        if not self.attention:\n            self.context_model = self.create_context_model()\n            self.response_model = self.create_response_model()\n\n    def compile(self) -> None:\n        optimizer = Adam(lr=self.learning_rate)\n        loss = losses.binary_crossentropy\n        self.model.compile(loss=loss, optimizer=optimizer)\n\n    def load(self) -> None:\n        log.info(""[initializing `{}` from saved]"".format(self.__class__.__name__))\n        self.model.load_weights(str(self.load_path))\n\n    def save(self) -> None:\n        log.info(""[saving `{}`]"".format(self.__class__.__name__))\n        self.model.save_weights(str(self.save_path))\n\n    def load_initial_emb_matrix(self) -> None:\n        log.info(""[initializing new `{}`]"".format(self.__class__.__name__))\n        if self.use_matrix:\n            self.model.get_layer(name=""embedding"").set_weights([self.emb_matrix])\n\n    @abstractmethod\n    def create_model(self) -> Model:\n        pass\n\n    def create_context_model(self) -> Model:\n        m = Model(self.model.inputs[:-1],\n                  self.model.get_layer(""sentence_embedding"").get_output_at(0))\n        return m\n\n    def create_response_model(self) -> Model:\n        m = Model(self.model.inputs[-1],\n                  self.model.get_layer(""sentence_embedding"").get_output_at(1))\n        return m\n\n    def _train_on_batch(self, batch: List[np.ndarray], y: List[int]) -> float:\n        loss = self.model.train_on_batch(batch, np.asarray(y))\n        return loss\n\n    def _predict_on_batch(self, batch: List[np.ndarray]) -> np.ndarray:\n        y_pred = self.model.predict_on_batch(batch)\n        return y_pred\n\n    def _predict_context_on_batch(self, batch: List[np.ndarray]) -> np.ndarray:\n        return self.context_model.predict_on_batch(batch)\n\n    def _predict_response_on_batch(self, batch: List[np.ndarray]) -> np.ndarray:\n        return self.response_model.predict_on_batch(batch)\n'"
deeppavlov/models/ranking/metrics.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numpy as np\n\nfrom deeppavlov.core.common.metrics_registry import register_metric\n\n\n@register_metric(\'rank_response\')\ndef rank_response(y_true, y_pred):\n    num_examples = float(len(y_pred))\n    predictions = np.array(y_pred)\n    predictions = np.flip(np.argsort(predictions, -1), -1)\n    rank_tot = 0\n    for el in predictions:\n        for i, x in enumerate(el):\n            if x == 0:\n                rank_tot += i\n                break\n    return float(rank_tot) / num_examples\n\n\n@register_metric(\'r@1_insQA\')\ndef r_at_1_insQA(y_true, y_pred):\n    return recall_at_k_insQA(y_true, y_pred, k=1)\n\n\ndef recall_at_k_insQA(y_true, y_pred, k):\n    labels = np.repeat(np.expand_dims(np.asarray(y_true), axis=1), k, axis=1)\n    predictions = np.array(y_pred)\n    predictions = np.flip(np.argsort(predictions, -1), -1)[:, :k]\n    flags = np.zeros_like(predictions)\n    for i in range(predictions.shape[0]):\n        for j in range(predictions.shape[1]):\n            if predictions[i][j] in np.arange(labels[i][j]):\n                flags[i][j] = 1.\n    return np.mean((np.sum(flags, -1) >= 1.).astype(float))\n'"
deeppavlov/models/ranking/mpm_siamese_network.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom logging import getLogger\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.initializers import glorot_uniform, Orthogonal\nfrom tensorflow.keras.layers import Input, LSTM, Lambda, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.models import Model\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.layers.keras_layers import AttentiveMatchingLayer, MaxattentiveMatchingLayer\nfrom deeppavlov.core.layers.keras_layers import FullMatchingLayer, MaxpoolingMatchingLayer\nfrom deeppavlov.models.ranking.bilstm_siamese_network import BiLSTMSiameseNetwork\n\nlog = getLogger(__name__)\n\n\n@register(\'mpm_nn\')\nclass MPMSiameseNetwork(BiLSTMSiameseNetwork):\n    """"""The class implementing a siamese neural network with bilateral multi-Perspective matching.\n\n    The network architecture is based on https://arxiv.org/abs/1702.03814.\n    \n    Args:\n        dense_dim: Dimensionality of the dense layer.\n        perspective_num: Number of perspectives in multi-perspective matching layers.\n        aggregation dim: Dimensionality of the hidden state in the second BiLSTM layer.\n        inpdrop_val: Float between 0 and 1. A dropout value for the linear transformation of the inputs.\n        recdrop_val: Float between 0 and 1. A dropout value for the linear transformation of the recurrent state.\n        ldrop_val: A dropout value of the dropout layer before the second BiLSTM layer.\n        dropout_val:  A dropout value of the dropout layer after the second BiLSTM layer.\n    """"""\n\n    def __init__(self,\n                 dense_dim: int = 50,\n                 perspective_num: int = 20,\n                 aggregation_dim: int = 200,\n                 recdrop_val: float = 0.0,\n                 inpdrop_val: float = 0.0,\n                 ldrop_val: float = 0.0,\n                 dropout_val: float = 0.0,\n                 *args,\n                 **kwargs) -> None:\n\n        self.dense_dim = dense_dim\n        self.perspective_num = perspective_num\n        self.aggregation_dim = aggregation_dim\n        self.ldrop_val = ldrop_val\n        self.recdrop_val = recdrop_val\n        self.inpdrop_val = inpdrop_val\n        self.dropout_val = dropout_val\n        self.seed = kwargs.get(""triplet_loss"")\n        self.triplet_mode = kwargs.get(""triplet_loss"")\n\n        super(MPMSiameseNetwork, self).__init__(*args, **kwargs)\n\n    def create_lstm_layer_1(self):\n        ker_in = glorot_uniform(seed=self.seed)\n        rec_in = Orthogonal(seed=self.seed)\n        bioutp = Bidirectional(LSTM(self.hidden_dim,\n                                    input_shape=(self.max_sequence_length, self.embedding_dim,),\n                                    kernel_regularizer=None,\n                                    recurrent_regularizer=None,\n                                    bias_regularizer=None,\n                                    activity_regularizer=None,\n                                    recurrent_dropout=self.recdrop_val,\n                                    dropout=self.inpdrop_val,\n                                    kernel_initializer=ker_in,\n                                    recurrent_initializer=rec_in,\n                                    return_sequences=True), merge_mode=None)\n        return bioutp\n\n    def create_lstm_layer_2(self):\n        ker_in = glorot_uniform(seed=self.seed)\n        rec_in = Orthogonal(seed=self.seed)\n        bioutp = Bidirectional(LSTM(self.aggregation_dim,\n                                    input_shape=(self.max_sequence_length, 8 * self.perspective_num,),\n                                    kernel_regularizer=None,\n                                    recurrent_regularizer=None,\n                                    bias_regularizer=None,\n                                    activity_regularizer=None,\n                                    recurrent_dropout=self.recdrop_val,\n                                    dropout=self.inpdrop_val,\n                                    kernel_initializer=ker_in,\n                                    recurrent_initializer=rec_in,\n                                    return_sequences=False),\n                               merge_mode=\'concat\',\n                               name=""sentence_embedding"")\n        return bioutp\n\n    def create_model(self) -> Model:\n        if self.use_matrix:\n            context = Input(shape=(self.max_sequence_length,))\n            response = Input(shape=(self.max_sequence_length,))\n            emb_layer = self.embedding_layer()\n            emb_c = emb_layer(context)\n            emb_r = emb_layer(response)\n        else:\n            context = Input(shape=(self.max_sequence_length, self.embedding_dim,))\n            response = Input(shape=(self.max_sequence_length, self.embedding_dim,))\n            emb_c = context\n            emb_r = response\n        lstm_layer = self.create_lstm_layer_1()\n        lstm_a = lstm_layer(emb_c)\n        lstm_b = lstm_layer(emb_r)\n\n        f_layer_f = FullMatchingLayer(self.perspective_num)\n        f_layer_b = FullMatchingLayer(self.perspective_num)\n        f_a_forw = f_layer_f([lstm_a[0], lstm_b[0]])[0]\n        f_a_back = f_layer_b([Lambda(lambda x: K.reverse(x, 1))(lstm_a[1]),\n                              Lambda(lambda x: K.reverse(x, 1))(lstm_b[1])])[0]\n        f_a_back = Lambda(lambda x: K.reverse(x, 1))(f_a_back)\n        f_b_forw = f_layer_f([lstm_b[0], lstm_a[0]])[0]\n        f_b_back = f_layer_b([Lambda(lambda x: K.reverse(x, 1))(lstm_b[1]),\n                              Lambda(lambda x: K.reverse(x, 1))(lstm_a[1])])[0]\n        f_b_back = Lambda(lambda x: K.reverse(x, 1))(f_b_back)\n\n        mp_layer_f = MaxpoolingMatchingLayer(self.perspective_num)\n        mp_layer_b = MaxpoolingMatchingLayer(self.perspective_num)\n        mp_a_forw = mp_layer_f([lstm_a[0], lstm_b[0]])[0]\n        mp_a_back = mp_layer_b([lstm_a[1], lstm_b[1]])[0]\n        mp_b_forw = mp_layer_f([lstm_b[0], lstm_a[0]])[0]\n        mp_b_back = mp_layer_b([lstm_b[1], lstm_a[1]])[0]\n\n        at_layer_f = AttentiveMatchingLayer(self.perspective_num)\n        at_layer_b = AttentiveMatchingLayer(self.perspective_num)\n        at_a_forw = at_layer_f([lstm_a[0], lstm_b[0]])[0]\n        at_a_back = at_layer_b([lstm_a[1], lstm_b[1]])[0]\n        at_b_forw = at_layer_f([lstm_b[0], lstm_a[0]])[0]\n        at_b_back = at_layer_b([lstm_b[1], lstm_a[1]])[0]\n\n        ma_layer_f = MaxattentiveMatchingLayer(self.perspective_num)\n        ma_layer_b = MaxattentiveMatchingLayer(self.perspective_num)\n        ma_a_forw = ma_layer_f([lstm_a[0], lstm_b[0]])[0]\n        ma_a_back = ma_layer_b([lstm_a[1], lstm_b[1]])[0]\n        ma_b_forw = ma_layer_f([lstm_b[0], lstm_a[0]])[0]\n        ma_b_back = ma_layer_b([lstm_b[1], lstm_a[1]])[0]\n\n        concat_a = Lambda(lambda x: K.concatenate(x, axis=-1))([f_a_forw, f_a_back,\n                                                                mp_a_forw, mp_a_back,\n                                                                at_a_forw, at_a_back,\n                                                                ma_a_forw, ma_a_back])\n        concat_b = Lambda(lambda x: K.concatenate(x, axis=-1))([f_b_forw, f_b_back,\n                                                                mp_b_forw, mp_b_back,\n                                                                at_b_forw, at_b_back,\n                                                                ma_b_forw, ma_b_back])\n\n        concat_a = Dropout(self.ldrop_val)(concat_a)\n        concat_b = Dropout(self.ldrop_val)(concat_b)\n\n        lstm_layer_agg = self.create_lstm_layer_2()\n        agg_a = lstm_layer_agg(concat_a)\n        agg_b = lstm_layer_agg(concat_b)\n\n        agg_a = Dropout(self.dropout_val)(agg_a)\n        agg_b = Dropout(self.dropout_val)(agg_b)\n\n        reduced = Lambda(lambda x: K.concatenate(x, axis=-1))([agg_a, agg_b])\n\n        if self.triplet_mode:\n            dist = Lambda(self._pairwise_distances)([agg_a, agg_b])\n        else:\n            ker_in = glorot_uniform(seed=self.seed)\n            dense = Dense(self.dense_dim, kernel_initializer=ker_in)(reduced)\n            dist = Dense(1, activation=\'sigmoid\', name=""score_model"")(dense)\n        model = Model([context, response], dist)\n        return model\n'"
deeppavlov/models/ranking/rel_ranker.py,30,"b'from typing import List, Tuple, Union, Dict, Optional\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.layers.tf_layers import variational_dropout\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.core.models.tf_model import LRScheduledTFModel\nfrom deeppavlov.models.embedders.abstract_embedder import Embedder\nfrom deeppavlov.models.squad.utils import CudnnGRU, softmax_mask\n\n\n@register(\'two_sentences_emb\')\nclass TwoSentencesEmbedder(Component):\n    """"""This class is used for embedding of two sentences.""""""\n\n    def __init__(self, embedder: Embedder, **kwargs):\n        """"""\n\n        Args:\n            embedder: what embedder to use: Glove, Fasttext or other\n            **kwargs:\n        """"""\n        self.embedder = embedder\n\n    def __call__(self, sentence_tokens_1: List[List[str]], sentence_tokens_2: List[List[str]]) -> \\\n            Tuple[List[Union[list, np.ndarray]], List[Union[list, np.ndarray]]]:\n        sentence_token_embs_1 = self.embedder(sentence_tokens_1)\n        sentence_token_embs_2 = self.embedder(sentence_tokens_2)\n        return sentence_token_embs_1, sentence_token_embs_2\n\n\n@register(\'rel_ranker\')\nclass RelRanker(LRScheduledTFModel):\n    """"""\n        This class determines whether the relation appropriate for the question or not.\n    """"""\n\n    def __init__(self, n_classes: int = 2,\n                 dropout_keep_prob: float = 0.5,\n                 return_probas: bool = False, **kwargs):\n        """"""\n\n        Args:\n            n_classes: number of classes for classification\n            dropout_keep_prob: Probability of keeping the hidden state, values from 0 to 1. 0.5 works well\n                in most cases.\n            return_probas: whether to return confidences of the relation to be appropriate or not\n            **kwargs:\n        """"""\n        kwargs.setdefault(\'learning_rate_drop_div\', 10.0)\n        kwargs.setdefault(\'learning_rate_drop_patience\', 5.0)\n        kwargs.setdefault(\'clip_norm\', 5.0)\n\n        super().__init__(**kwargs)\n\n        self.n_classes = n_classes\n        self.dropout_keep_prob = dropout_keep_prob\n        self.return_probas = return_probas\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n\n        self.question_ph = tf.placeholder(tf.float32, [None, None, 300])\n        self.rel_emb_ph = tf.placeholder(tf.float32, [None, None, 300])\n\n        r_mask_2 = tf.cast(self.rel_emb_ph, tf.bool)\n        r_len_2 = tf.reduce_sum(tf.cast(r_mask_2, tf.int32), axis=2)\n        r_mask = tf.cast(r_len_2, tf.bool)\n        r_len = tf.reduce_sum(tf.cast(r_mask, tf.int32), axis=1)\n        rel_emb = tf.math.divide_no_nan(tf.reduce_sum(self.rel_emb_ph, axis=1),\n                                        tf.cast(tf.expand_dims(r_len, axis=1), tf.float32))\n\n        self.y_ph = tf.placeholder(tf.int32, shape=(None,))\n        self.one_hot_labels = tf.one_hot(self.y_ph, depth=self.n_classes, dtype=tf.float32)\n        self.keep_prob_ph = tf.placeholder_with_default(1.0, shape=[], name=\'keep_prob_ph\')\n\n        q_mask_2 = tf.cast(self.question_ph, tf.bool)\n        q_len_2 = tf.reduce_sum(tf.cast(q_mask_2, tf.int32), axis=2)\n        q_mask = tf.cast(q_len_2, tf.bool)\n        q_len = tf.reduce_sum(tf.cast(q_mask, tf.int32), axis=1)\n\n        question_dr = variational_dropout(self.question_ph, keep_prob=self.keep_prob_ph)\n        b_size = tf.shape(self.question_ph)[0]\n\n        with tf.variable_scope(""question_encode""):\n            rnn = CudnnGRU(num_layers=2, num_units=75, batch_size=b_size, input_size=300, keep_prob=self.keep_prob_ph)\n            q = rnn(question_dr, seq_len=q_len)\n\n        with tf.variable_scope(""attention""):\n            rel_emb_exp = tf.expand_dims(rel_emb, axis=1)\n            dot_products = tf.reduce_sum(tf.multiply(q, rel_emb_exp), axis=2, keep_dims=False)\n            s_mask = softmax_mask(dot_products, q_mask)\n            att_weights = tf.expand_dims(tf.nn.softmax(s_mask), axis=2)\n            self.s_r = tf.reduce_sum(tf.multiply(att_weights, q), axis=1)\n\n            self.logits = tf.layers.dense(tf.multiply(self.s_r, rel_emb), 2, activation=None, use_bias=False)\n            self.y_pred = tf.argmax(self.logits, axis=-1)\n\n            loss_tensor = tf.nn.sigmoid_cross_entropy_with_logits(labels=self.one_hot_labels, logits=self.logits)\n\n            self.loss = tf.reduce_mean(loss_tensor)\n            self.train_op = self.get_train_op(self.loss)\n\n        self.sess = tf.Session(config=config)\n        self.sess.run(tf.global_variables_initializer())\n        self.load()\n\n    def fill_feed_dict(self, questions_embs: List[np.ndarray], rels_embs: List[np.ndarray], y=None, train=False) -> \\\n            Dict[tf.placeholder, List[np.ndarray]]:\n        questions_embs = np.array(questions_embs)\n        rels_embs = np.array(rels_embs)\n        feed_dict = {self.question_ph: questions_embs, self.rel_emb_ph: rels_embs}\n        if y is not None:\n            feed_dict[self.y_ph] = y\n        if train:\n            feed_dict[self.keep_prob_ph] = self.dropout_keep_prob\n        else:\n            feed_dict[self.keep_prob_ph] = 1.0\n\n        return feed_dict\n\n    def __call__(self, questions_embs: List[np.ndarray], rels_embs: List[np.ndarray]) -> \\\n            List[np.ndarray]:\n        feed_dict = self.fill_feed_dict(questions_embs, rels_embs)\n        if self.return_probas:\n            pred = self.sess.run(self.logits, feed_dict)\n        else:\n            pred = self.sess.run(self.y_pred, feed_dict)\n        return pred\n\n    def train_on_batch(self, questions_embs: List[np.ndarray], \n                             rels_embs: List[np.ndarray],\n                             y: List[int]) -> Dict[str, float]:\n        feed_dict = self.fill_feed_dict(questions_embs, rels_embs, y, train=True)\n        _, loss_value = self.sess.run([self.train_op, self.loss], feed_dict)\n\n        return {\'loss\': loss_value,\n                \'learning_rate\': self.get_learning_rate(),\n                \'momentum\': self.get_momentum()}\n'"
deeppavlov/models/ranking/sequential_matching_network.py,43,"b'# Copyright 2018 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import Optional\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.models.ranking.tf_base_matching_model import TensorflowBaseMatchingModel\n\nlog = getLogger(__name__)\n\n\n@register(\'smn_nn\')\nclass SMNNetwork(TensorflowBaseMatchingModel):\n    """"""\n    Tensorflow implementation of Sequential Matching Network\n\n    Wu, Yu, et al. ""Sequential Matching Network: A New Architecture for Multi-turn Response Selection in\n    Retrieval-based Chatbots."" ACL. 2017.\n    https://arxiv.org/abs/1612.01627\n\n    Based on authors\' Tensorflow code: https://github.com/MarkWuNLP/MultiTurnResponseSelection\n\n    Args:\n        num_context_turns (int): A number of ``context`` turns in data samples.\n        max_sequence_length (int): A maximum length of text sequences in tokens.\n            Longer sequences will be truncated and shorter ones will be padded.\n        learning_rate (float): Initial learning rate.\n        emb_matrix (np.ndarray): An embeddings matrix to initialize an embeddings layer of a model.\n        trainable_embeddings (bool): Whether train embeddings matrix or not.\n        embedding_dim (int): Dimensionality of token (word) embeddings.\n    """"""\n\n    def __init__(self,\n                 embedding_dim: int = 200,\n                 max_sequence_length: int = 50,\n                 learning_rate: float = 1e-3,\n                 emb_matrix: Optional[np.ndarray] = None,\n                 trainable_embeddings: bool = False,\n                 *args,\n                 **kwargs):\n\n        self.max_sentence_len = max_sequence_length\n        self.word_embedding_size = embedding_dim\n        self.trainable = trainable_embeddings\n        self.learning_rate = learning_rate\n        self.emb_matrix = emb_matrix\n\n        super(SMNNetwork, self).__init__(*args, **kwargs)\n\n        self.sess_config = tf.ConfigProto(allow_soft_placement=True)\n        self.sess_config.gpu_options.allow_growth = True\n        self.sess = tf.Session(config=self.sess_config)\n        self._init_graph()\n        self.sess.run(tf.global_variables_initializer())\n\n        if self.load_path is not None:\n            self.load()\n\n    def _init_placeholders(self):\n        with tf.variable_scope(\'inputs\'):\n            # Utterances and their lengths\n            self.utterance_ph = tf.placeholder(tf.int32, shape=(None, self.num_context_turns, self.max_sentence_len))\n            self.all_utterance_len_ph = tf.placeholder(tf.int32, shape=(None, self.num_context_turns))\n\n            # Responses and their lengths\n            self.response_ph = tf.placeholder(tf.int32, shape=(None, self.max_sentence_len))\n            self.response_len_ph = tf.placeholder(tf.int32, shape=(None,))\n\n            # Labels\n            self.y_true = tf.placeholder(tf.int32, shape=(None,))\n\n    def _init_graph(self):\n        self._init_placeholders()\n\n        word_embeddings = tf.get_variable(""word_embeddings_v"",\n                                          initializer=tf.constant(self.emb_matrix, dtype=tf.float32),\n                                          trainable=self.trainable)\n\n        all_utterance_embeddings = tf.nn.embedding_lookup(word_embeddings, self.utterance_ph)\n        response_embeddings = tf.nn.embedding_lookup(word_embeddings, self.response_ph)\n        sentence_GRU = tf.nn.rnn_cell.GRUCell(self.word_embedding_size, kernel_initializer=tf.orthogonal_initializer())\n        all_utterance_embeddings = tf.unstack(all_utterance_embeddings, num=self.num_context_turns,\n                                              axis=1)  # list of self.num_context_turns tensors with shape (?, 200)\n        all_utterance_len = tf.unstack(self.all_utterance_len_ph, num=self.num_context_turns, axis=1)\n        A_matrix = tf.get_variable(\'A_matrix_v\', shape=(self.word_embedding_size, self.word_embedding_size),\n                                   initializer=tf.contrib.layers.xavier_initializer(), dtype=tf.float32)\n        final_GRU = tf.nn.rnn_cell.GRUCell(self.word_embedding_size, kernel_initializer=tf.orthogonal_initializer())\n        reuse = None\n\n        response_GRU_embeddings, _ = tf.nn.dynamic_rnn(sentence_GRU,\n                                                       response_embeddings,\n                                                       sequence_length=self.response_len_ph,\n                                                       dtype=tf.float32,\n                                                       scope=\'sentence_GRU\')\n        response_embeddings = tf.transpose(response_embeddings, perm=[0, 2, 1])\n        response_GRU_embeddings = tf.transpose(response_GRU_embeddings, perm=[0, 2, 1])\n        matching_vectors = []\n        for utterance_embeddings, utterance_len in zip(all_utterance_embeddings, all_utterance_len):\n            matrix1 = tf.matmul(utterance_embeddings, response_embeddings)\n            utterance_GRU_embeddings, _ = tf.nn.dynamic_rnn(sentence_GRU,\n                                                            utterance_embeddings,\n                                                            sequence_length=utterance_len,\n                                                            dtype=tf.float32,\n                                                            scope=\'sentence_GRU\')\n            matrix2 = tf.einsum(\'aij,jk->aik\', utterance_GRU_embeddings, A_matrix)  # TODO:check this\n            matrix2 = tf.matmul(matrix2, response_GRU_embeddings)\n            matrix = tf.stack([matrix1, matrix2], axis=3, name=\'matrix_stack\')\n            conv_layer = tf.layers.conv2d(matrix, filters=8, kernel_size=(3, 3), padding=\'VALID\',\n                                          kernel_initializer=tf.contrib.keras.initializers.he_normal(),\n                                          activation=tf.nn.relu, reuse=reuse, name=\'conv\')  # TODO: check other params\n            pooling_layer = tf.layers.max_pooling2d(conv_layer, (3, 3), strides=(3, 3),\n                                                    padding=\'VALID\', name=\'max_pooling\')  # TODO: check other params\n            matching_vector = tf.layers.dense(tf.contrib.layers.flatten(pooling_layer), 50,\n                                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                              activation=tf.tanh, reuse=reuse,\n                                              name=\'matching_v\')  # TODO: check wthether this is correct\n            if not reuse:\n                reuse = True\n            matching_vectors.append(matching_vector)\n        _, last_hidden = tf.nn.dynamic_rnn(final_GRU,\n                                           tf.stack(matching_vectors, axis=0, name=\'matching_stack\'),\n                                           # resulting shape: (10, ?, 50)\n                                           dtype=tf.float32,\n                                           time_major=True,\n                                           scope=\'final_GRU\')  # TODO: check time_major\n        logits = tf.layers.dense(last_hidden, 2, kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                 name=\'final_v\')\n        self.y_pred = tf.nn.softmax(logits)\n        self.logits = logits\n        self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y_true, logits=logits))\n        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n        self.train_op = optimizer.minimize(self.loss)\n\n        # Debug\n        self.print_number_of_parameters()\n'"
deeppavlov/models/ranking/siamese_model.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import List, Iterable, Union, Tuple, Dict\n\nimport numpy as np\n\nfrom deeppavlov.core.models.nn_model import NNModel\n\n\nclass SiameseModel(NNModel):\n    """"""The class implementing base functionality for siamese neural networks.\n\n    Args:\n        batch_size: A size of a batch.\n        num_context_turns: A number of ``context`` turns in data samples.\n        *args: Other parameters.\n        **kwargs: Other parameters.\n    """"""\n\n    def __init__(self,\n                 batch_size: int,\n                 num_context_turns: int = 1,\n                 *args, **kwargs) -> None:\n        super().__init__(*args, **kwargs)\n\n        self.batch_size = batch_size\n        self.num_context_turns = num_context_turns\n\n    def load(self, *args, **kwargs) -> None:\n        pass\n\n    def save(self, *args, **kwargs) -> None:\n        pass\n\n    def train_on_batch(self, samples_generator: Iterable[List[np.ndarray]], y: List[int]) -> float:\n        """"""\n        This method is called by trainer to make one training step on one batch.\n        The number of samples returned by `samples_generator` is always equal to `batch_size`, so we need to:\n        1) accumulate data for all of the inputs of the model;\n        2) format inputs of a model in a proper way using `self._make_batch` function;\n        3) run a model with provided inputs and ground truth labels (`y`) using `self._train_on_batch` function;\n        4) return mean loss value on the batch\n\n        Args:\n            samples_generator (Iterable[List[np.ndarray]]): generator that returns list of numpy arrays\n                of words of all sentences represented as integers.\n                Its shape: (number_of_context_turns + 1, max_number_of_words_in_a_sentence)\n            y (List[int]): tuple of labels, with shape: (batch_size, )\n\n        Returns:\n            float: value of mean loss on the batch\n        """"""\n        buf = []\n        for sample in samples_generator:\n            self._append_sample_to_batch_buffer(sample, buf)\n        b = self._make_batch(buf)\n        loss = self._train_on_batch(b, y)\n        return loss\n\n    def __call__(self, samples_generator: Iterable[List[np.ndarray]]) -> Union[np.ndarray, List[str]]:\n        """"""\n        This method is called by trainer to make one evaluation step on one batch.\n\n        Args:\n            samples_generator (Iterable[List[np.ndarray]]):  generator that returns list of numpy arrays\n            of words of all sentences represented as integers.\n            Has shape: (number_of_context_turns + 1, max_number_of_words_in_a_sentence)\n\n        Returns:\n            np.ndarray: predictions for the batch of samples\n        """"""\n        y_pred = []\n        buf = []\n        for j, sample in enumerate(samples_generator, start=1):\n            n_responses = self._append_sample_to_batch_buffer(sample, buf)\n            if len(buf) >= self.batch_size:\n                for i in range(len(buf) // self.batch_size):\n                    b = self._make_batch(buf[i * self.batch_size:(i + 1) * self.batch_size])\n                    yp = self._predict_on_batch(b)\n                    y_pred += list(yp)\n                lenb = len(buf) % self.batch_size\n                if lenb != 0:\n                    buf = buf[-lenb:]\n                else:\n                    buf = []\n        if len(buf) != 0:\n            b = self._make_batch(buf)\n            yp = self._predict_on_batch(b)\n            y_pred += list(yp)\n        y_pred = np.asarray(y_pred)\n        # reshape to [batch_size, n_responses] if needed (n_responses > 1)\n        y_pred = np.reshape(y_pred, (j, n_responses)) if n_responses > 1 else y_pred\n        return y_pred\n\n    def reset(self) -> None:\n        pass\n\n    def _append_sample_to_batch_buffer(self, sample: List,\n                                       buf: Union[List[List[np.ndarray]], List[Tuple[np.ndarray]]]) -> int:\n        context = sample[:self.num_context_turns]\n        responses = sample[self.num_context_turns:]\n        buf += [context + [el] for el in responses]\n\n        return len(responses)\n\n    def _train_on_batch(self, batch: Union[List[np.ndarray], Dict], y: List[int]) -> float:\n        pass\n\n    def _predict_on_batch(self, batch: Union[List[np.ndarray], Dict]) -> np.ndarray:\n        pass\n\n    def _predict_context_on_batch(self, batch: List[np.ndarray]) -> np.ndarray:\n        pass\n\n    def _predict_response_on_batch(self, batch: List[np.ndarray]) -> np.ndarray:\n        pass\n\n    def _make_batch(self, x: List[List[np.ndarray]]) -> List[np.ndarray]:\n        b = []\n        for i in range(len(x[0])):\n            z = [el[i] for el in x]\n            b.append(np.asarray(z))\n        return b\n'"
deeppavlov/models/ranking/siamese_predictor.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import List, Iterable, Callable, Union\n\nimport numpy as np\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.simple_vocab import SimpleVocabulary\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.models.ranking.keras_siamese_model import SiameseModel\n\nlog = getLogger(__name__)\n\n\n@register(\'siamese_predictor\')\nclass SiamesePredictor(Component):\n    """"""The class for ranking or paraphrase identification using the trained siamese network  in the ``interact`` mode.\n\n    Args:\n        batch_size: A size of a batch.\n        num_context_turns: A number of ``context`` turns in data samples.\n        ranking: Whether to perform ranking.\n            If it is set to ``False`` paraphrase identification will be performed.\n        attention: Whether any attention mechanism is used in the siamese network.\n            If ``False`` then calculated in advance vectors of ``responses``\n            will be used to obtain similarity score for the input ``context``;\n            Otherwise the whole siamese architecture will be used\n            to obtain similarity score for the input ``context`` and each particular ``response``.\n            The parameter will be used if the ``ranking`` is set to ``True``.\n        responses: A instance of :class:`~deeppavlov.core.data.simple_vocab.SimpleVocabulary`\n            with all possible ``responses`` to perform ranking.\n            Will be used if the ``ranking`` is set to ``True``.\n        preproc_func: A ``__call__`` function of the\n            :class:`~deeppavlov.models.preprocessors.siamese_preprocessor.SiamesePreprocessor`.\n        interact_pred_num: The number of the most relevant ``responses`` which will be returned.\n            Will be used if the ``ranking`` is set to ``True``.\n        **kwargs: Other parameters.\n    """"""\n\n    def __init__(self,\n                 model: SiameseModel,\n                 batch_size: int,\n                 num_context_turns: int = 1,\n                 ranking: bool = True,\n                 attention: bool = False,\n                 responses: SimpleVocabulary = None,\n                 preproc_func: Callable = None,\n                 interact_pred_num: int = 3,\n                 *args, **kwargs) -> None:\n\n        super().__init__()\n\n        self.batch_size = batch_size\n        self.num_context_turns = num_context_turns\n        self.ranking = ranking\n        self.attention = attention\n        self.preproc_responses = []\n        self.response_embeddings = None\n        self.preproc_func = preproc_func\n        self.interact_pred_num = interact_pred_num\n        self.model = model\n        if self.ranking:\n            self.responses = {el[1]: el[0] for el in responses.items()}\n            self._build_preproc_responses()\n            if not self.attention:\n                self._build_response_embeddings()\n\n    def __call__(self, batch: Iterable[List[np.ndarray]]) -> List[Union[List[str], str]]:\n        context = next(batch)\n        try:\n            next(batch)\n            log.error(""It is not intended to use the `%s` with the batch size greater then 1."" % self.__class__)\n        except StopIteration:\n            pass\n\n        if self.ranking:\n            if len(context) == self.num_context_turns:\n                scores = []\n                if self.attention:\n                    for i in range(len(self.preproc_responses) // self.batch_size + 1):\n                        responses = self.preproc_responses[i * self.batch_size: (i + 1) * self.batch_size]\n                        b = [context + el for el in responses]\n                        b = self.model._make_batch(b)\n                        sc = self.model._predict_on_batch(b)\n                        scores += list(sc)\n                else:\n                    b = self.model._make_batch([context])\n                    context_emb = self.model._predict_context_on_batch(b)\n                    context_emb = np.squeeze(context_emb, axis=0)\n                    scores = context_emb @ self.response_embeddings.T\n                ids = np.flip(np.argsort(scores), -1)\n                return [[self.responses[el] for el in ids[:self.interact_pred_num]]]\n            else:\n                return [""Please, provide contexts separated by \'&\' in the number equal to that used while training.""]\n\n        else:\n            if len(context) == 2:\n                b = self.model._make_batch([context])\n                sc = self.model._predict_on_batch(b)[0]\n                if sc > 0.5:\n                    return [""This is a paraphrase.""]\n                else:\n                    return [""This is not a paraphrase.""]\n            else:\n                return [""Please, provide two sentences separated by \'&\'.""]\n\n    def reset(self) -> None:\n        pass\n\n    def process_event(self) -> None:\n        pass\n\n    def _build_response_embeddings(self) -> None:\n        resp_vecs = []\n        for i in range(len(self.preproc_responses) // self.batch_size + 1):\n            resp_preproc = self.preproc_responses[i * self.batch_size: (i + 1) * self.batch_size]\n            resp_preproc = self.model._make_batch(resp_preproc)\n            resp_preproc = resp_preproc\n            resp_vecs.append(self.model._predict_response_on_batch(resp_preproc))\n        self.response_embeddings = np.vstack(resp_vecs)\n\n    def _build_preproc_responses(self) -> None:\n        responses = list(self.responses.values())\n        for i in range(len(responses) // self.batch_size + 1):\n            el = self.preproc_func(responses[i * self.batch_size: (i + 1) * self.batch_size])\n            self.preproc_responses += list(el)\n\n    def rebuild_responses(self, candidates) -> None:\n        self.attention = True\n        self.interact_pred_num = 1\n        self.preproc_responses = list()\n        self.responses = {idx: sentence for idx, sentence in enumerate(candidates)}\n        self._build_preproc_responses()\n'"
deeppavlov/models/ranking/tf_base_matching_model.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import List, Dict, Tuple\n\nimport numpy as np\n\nfrom deeppavlov.core.models.tf_model import TFModel\nfrom deeppavlov.models.ranking.siamese_model import SiameseModel\n\nlog = getLogger(__name__)\n\n\nclass TensorflowBaseMatchingModel(TFModel, SiameseModel):\n    """"""\n    Base class for ranking models that uses context-response matching schemes.\n\n    Note:\n        Tensorflow session variable already presents as self.sess attribute\n        (derived from TFModel and initialized by Chainer)\n\n    Args:\n        batch_size (int): a number of samples in a batch.\n        num_context_turns (int): a number of ``context`` turns in data samples.\n        mean_oov (bool): whether to set mean embedding of all tokens. By default: True.\n        use_logits (bool): whether to use raw logits as outputs instead of softmax predictions\n\n    """"""\n\n    def __init__(self,\n                 batch_size: int,\n                 num_context_turns: int = 10,\n                 mean_oov: bool = True,\n                 use_logits: bool = False,\n                 *args,\n                 **kwargs):\n        super(TensorflowBaseMatchingModel, self).__init__(batch_size=batch_size, num_context_turns=num_context_turns,\n                                                          *args, **kwargs)\n        self.use_logits = use_logits\n        if mean_oov:\n            self.emb_matrix[1] = np.mean(self.emb_matrix[2:],\n                                         axis=0)  # set mean embedding for OOV token at the 2nd index\n\n    def _append_sample_to_batch_buffer(self, sample: List[np.ndarray], buf: List[Tuple]) -> int:\n        """"""\n\n        Args:\n            sample (List[nd.array]): samples generator\n            buf (List[Tuple]) : List of samples with model inputs each:\n                [( context, context_len, response, response_len ), ( ... ), ... ].\n        Returns:\n             a number of candidate responses\n        """"""\n        #\n        batch_buffer_context = []  # [batch_size, 10, 50]\n        batch_buffer_context_len = []  # [batch_size, 10]\n        batch_buffer_response = []  # [batch_size, 50]\n        batch_buffer_response_len = []  # [batch_size]\n\n        context_sentences = sample[:self.num_context_turns]\n        response_sentences = sample[self.num_context_turns:]\n\n        # Format model inputs:\n        # 4 model inputs\n\n        # 1. Token indices for context\n        batch_buffer_context += [context_sentences] * len(response_sentences)\n        # 2. Token indices for response\n        batch_buffer_response += list(response_sentences)\n        # 3. Lens of context sentences\n        lens = []\n        for context in [context_sentences] * len(response_sentences):\n            context_sentences_lens = []\n            for sent in context:\n                context_sentences_lens.append(len(sent[sent != 0]))\n            lens.append(context_sentences_lens)\n        batch_buffer_context_len += lens\n        # 4. Lens of response sentences\n        lens = []\n        for response_sent in response_sentences:\n            lens.append(len(response_sent[response_sent != 0]))\n        batch_buffer_response_len += lens\n\n        for i in range(len(batch_buffer_context)):\n            buf.append(tuple((\n                batch_buffer_context[i],\n                batch_buffer_context_len[i],\n                batch_buffer_response[i],\n                batch_buffer_response_len[i]\n            )))\n\n        return len(response_sentences)\n\n    def _make_batch(self, batch: List[Tuple[List[np.ndarray], List, np.ndarray, int]]) -> Dict:\n        """"""\n        The function for formatting model inputs\n\n        Args:\n            batch (List[Tuple[np.ndarray]]): List of samples with model inputs each:\n                [( context, context_len, response, response_len ), ( ... ), ... ].\n        Returns:\n            Dict: feed_dict to feed a model\n        """"""\n        input_context = []\n        input_context_len = []\n        input_response = []\n        input_response_len = []\n\n        # format model inputs as numpy arrays\n        for sample in batch:\n            input_context.append(sample[0])\n            input_context_len.append(sample[1])\n            input_response.append(sample[2])\n            input_response_len.append(sample[3])\n\n        return {\n            self.utterance_ph: np.array(input_context),\n            self.all_utterance_len_ph: np.array(input_context_len),\n            self.response_ph: np.array(input_response),\n            self.response_len_ph: np.array(input_response_len)\n        }\n\n    def _predict_on_batch(self, batch: Dict) -> np.ndarray:\n        """"""\n        Run a model with the batch of inputs.\n        The function returns a list of predictions for the batch in numpy format\n\n        Args:\n            batch (Dict): feed_dict that contains a batch with inputs for a model\n\n        Returns:\n            nd.array: predictions for the batch (raw logits or softmax outputs)\n        """"""\n        if self.use_logits:\n            return self.sess.run(self.logits, feed_dict=batch)[:, 1]\n        else:\n            return self.sess.run(self.y_pred, feed_dict=batch)[:, 1]\n\n    def _train_on_batch(self, batch: Dict, y: List[int]) -> float:\n        """"""\n        The function is for formatting of feed_dict used as an input for a model\n        Args:\n            batch (Dict): feed_dict that contains a batch with inputs for a model (except ground truth labels)\n            y (List(int)): list of ground truth labels\n\n        Returns:\n            float: value of mean loss on the batch\n        """"""\n        batch.update({self.y_true: np.array(y)})\n        return self.sess.run([self.loss, self.train_op], feed_dict=batch)[0]  # return the first item aka loss\n'"
deeppavlov/models/seq2seq_go_bot/__init__.py,0,b''
deeppavlov/models/seq2seq_go_bot/bot.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import Dict\n\nimport numpy as np\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.core.models.nn_model import NNModel\nfrom deeppavlov.models.seq2seq_go_bot.network import Seq2SeqGoalOrientedBotNetwork\n\nlog = getLogger(__name__)\n\n\n@register(""seq2seq_go_bot"")\nclass Seq2SeqGoalOrientedBot(NNModel):\n    """"""\n    A goal-oriented bot based on a sequence-to-sequence rnn. For implementation details see\n    :class:`~deeppavlov.models.seq2seq_go_bot.network.Seq2SeqGoalOrientedBotNetwork`.\n    Pretrained for :class:`~deeppavlov.dataset_readers.kvret_reader.KvretDatasetReader` dataset.\n\n    Parameters:\n        network_parameters: parameters passed to object of\n            :class:`~deeppavlov.models.seq2seq_go_bot.network.Seq2SeqGoalOrientedBotNetwork` class.\n        embedder: word embeddings model, see\n            :doc:`deeppavlov.models.embedders </apiref/models/embedders>`.\n        source_vocab: vocabulary of input tokens.\n        target_vocab: vocabulary of bot response tokens.\n        start_of_sequence_token: token that defines start of input sequence.\n        end_of_sequence_token: token that defines end of input sequence and start of\n            output sequence.\n        debug: whether to display debug output.\n        **kwargs: parameters passed to parent\n            :class:`~deeppavlov.core.models.nn_model.NNModel` class.\n    """"""\n\n    def __init__(self,\n                 network_parameters: Dict,\n                 embedder: Component,\n                 source_vocab: Component,\n                 target_vocab: Component,\n                 start_of_sequence_token: str,\n                 end_of_sequence_token: str,\n                 knowledge_base_keys,\n                 save_path: str,\n                 load_path: str = None,\n                 debug: bool = False,\n                 **kwargs) -> None:\n        super().__init__(save_path=save_path, load_path=load_path, **kwargs)\n\n        self.embedder = embedder\n        self.embedding_size = embedder.dim\n        self.src_vocab = source_vocab\n        self.tgt_vocab = target_vocab\n        self.tgt_vocab_size = len(target_vocab)\n        self.kb_keys = knowledge_base_keys\n        self.kb_size = len(self.kb_keys)\n        self.sos_token = start_of_sequence_token\n        self.eos_token = end_of_sequence_token\n        self.debug = debug\n\n        network_parameters[\'load_path\'] = load_path\n        network_parameters[\'save_path\'] = save_path\n        self.network = self._init_network(network_parameters)\n\n    def _init_network(self, params):\n        if \'target_start_of_sequence_index\' not in params:\n            params[\'target_start_of_sequence_index\'] = self.tgt_vocab[self.sos_token]\n        if \'target_end_of_sequence_index\' not in params:\n            params[\'target_end_of_sequence_index\'] = self.tgt_vocab[self.eos_token]\n        if \'source_vocab_size\' not in params:\n            params[\'source_vocab_size\'] = len(self.src_vocab)\n        if \'target_vocab_size\' not in params:\n            params[\'target_vocab_size\'] = len(self.tgt_vocab)\n        # contruct matrix of knowledge bases values embeddings\n        params[\'knowledge_base_entry_embeddings\'] = \\\n            [self._embed_kb_key(val) for val in self.kb_keys]\n        # contrcust matrix of decoder input token embeddings (zeros for sos_token)\n        dec_embs = self.embedder([[self.tgt_vocab[idx]\n                                   for idx in range(self.tgt_vocab_size)]])[0]\n        dec_embs[self.tgt_vocab[self.sos_token]][:] = 0.\n        params[\'decoder_embeddings\'] = dec_embs\n        return Seq2SeqGoalOrientedBotNetwork(**params)\n\n    def _embed_kb_key(self, key):\n        # TODO: fasttext embedder to work with tokens\n        emb = np.array(self.embedder([key.split(\'_\')], mean=True)[0])\n        if self.debug:\n            log.debug(""embedding key tokens=\'{}\', embedding shape = {}""\n                      .format(key.split(\'_\'), emb.shape))\n        return emb\n\n    def train_on_batch(self, utters, history_list, kb_entry_list, responses):\n        b_enc_ins, b_src_lens = [], []\n        b_dec_ins, b_dec_outs, b_tgt_lens = [], [], []\n        for x_tokens, history, y_tokens in zip(utters, history_list, responses):\n            x_tokens = history + x_tokens\n            enc_in = self._encode_context(x_tokens)\n            b_enc_ins.append(enc_in)\n            b_src_lens.append(len(enc_in))\n\n            dec_in, dec_out = self._encode_response(y_tokens)\n            b_dec_ins.append(dec_in)\n            b_dec_outs.append(dec_out)\n            b_tgt_lens.append(len(dec_out))\n\n        # Sequence padding\n        batch_size = len(b_enc_ins)\n        max_src_len = max(b_src_lens)\n        max_tgt_len = max(b_tgt_lens)\n        # b_enc_ins_np = self.src_vocab[self.sos_token] *\\\n        #    np.ones((batch_size, max_src_len), dtype=np.float32)\n        b_enc_ins_np = np.zeros((batch_size, max_src_len, self.embedding_size),\n                                dtype=np.float32)\n        b_dec_ins_np = self.tgt_vocab[self.eos_token] * \\\n                       np.ones((batch_size, max_tgt_len), dtype=np.float32)\n        b_dec_outs_np = self.tgt_vocab[self.eos_token] * \\\n                        np.ones((batch_size, max_tgt_len), dtype=np.float32)\n        b_tgt_weights_np = np.zeros((batch_size, max_tgt_len), dtype=np.float32)\n        b_kb_masks_np = np.zeros((batch_size, self.kb_size), np.float32)\n        for i, (src_len, tgt_len, kb_entries) in \\\n                enumerate(zip(b_src_lens, b_tgt_lens, kb_entry_list)):\n            b_enc_ins_np[i, :src_len] = b_enc_ins[i]\n            b_dec_ins_np[i, :tgt_len] = b_dec_ins[i]\n            b_dec_outs_np[i, :tgt_len] = b_dec_outs[i]\n            b_tgt_weights_np[i, :tgt_len] = 1.\n            if self.debug:\n                if len(kb_entries) != len(set([e[0] for e in kb_entries])):\n                    log.debug(""Duplicates in kb_entries = {}"".format(kb_entries))\n            for k, v in kb_entries:\n                b_kb_masks_np[i, self.kb_keys.index(k)] = 1.\n\n        """"""if self.debug:\n            log.debug(""b_enc_ins = {}"".format(b_enc_ins))\n            log.debug(""b_dec_ins = {}"".format(b_dec_ins))\n            log.debug(""b_dec_outs = {}"".format(b_dec_outs))\n            log.debug(""b_src_lens = {}"".format(b_src_lens))\n            log.debug(""b_tgt_lens = {}"".format(b_tgt_lens))\n            log.debug(""b_tgt_weights = {}"".format(b_tgt_weights))""""""\n\n        return self.network.train_on_batch(b_enc_ins_np, b_dec_ins_np, b_dec_outs_np,\n                                           b_src_lens, b_tgt_lens, b_tgt_weights_np,\n                                           b_kb_masks_np)\n\n    def _encode_context(self, tokens):\n        if self.debug:\n            log.debug(""Context tokens = \\""{}\\"""".format(tokens))\n        # token_idxs = self.src_vocab([tokens])[0]\n        # return token_idxs\n        return np.array(self.embedder([tokens])[0])\n\n    def _encode_response(self, tokens):\n        if self.debug:\n            log.debug(""Response tokens = \\""{}\\"""".format(tokens))\n        token_idxs = []\n        for token in tokens:\n            if token in self.kb_keys:\n                token_idxs.append(self.tgt_vocab_size + self.kb_keys.index(token))\n            else:\n                token_idxs.append(self.tgt_vocab[token])\n        # token_idxs = self.tgt_vocab([tokens])[0]\n        return ([self.tgt_vocab[self.sos_token]] + token_idxs,\n                token_idxs + [self.tgt_vocab[self.eos_token]])\n\n    def _decode_response(self, token_idxs):\n        def _idx2token(idxs):\n            for idx in idxs:\n                if idx < self.tgt_vocab_size:\n                    token = self.tgt_vocab([[idx]])[0][0]\n                    if token == self.eos_token:\n                        break\n                    yield token\n                else:\n                    yield self.kb_keys[idx - self.tgt_vocab_size]\n\n        return [list(_idx2token(utter_idxs)) for utter_idxs in token_idxs]\n\n    def __call__(self, *batch):\n        return self._infer_on_batch(*batch)\n\n    # def _infer_on_batch(self, utters, kb_entry_list=itertools.repeat([])):\n    def _infer_on_batch(self, utters, history_list, kb_entry_list):\n        b_enc_ins, b_src_lens = [], []\n        if (len(utters) == 1) and not utters[0]:\n            utters = [[\'hi\']]\n        for utter, history in zip(utters, history_list):\n            utter = history + utter\n            enc_in = self._encode_context(utter)\n\n            b_enc_ins.append(enc_in)\n            b_src_lens.append(len(enc_in))\n\n        # Sequence padding\n        batch_size = len(b_enc_ins)\n        max_src_len = max(b_src_lens)\n        b_enc_ins_np = np.zeros((batch_size, max_src_len, self.embedding_size),\n                                dtype=np.float32)\n        b_kb_masks_np = np.zeros((batch_size, self.kb_size), dtype=np.float32)\n        for i, (src_len, kb_entries) in enumerate(zip(b_src_lens, kb_entry_list)):\n            b_enc_ins_np[i, :src_len] = b_enc_ins[i]\n            if self.debug:\n                log.debug(""infer: kb_entries = {}"".format(kb_entries))\n            for k, v in kb_entries:\n                b_kb_masks_np[i, self.kb_keys.index(k)] = 1.\n\n        pred_idxs = self.network(b_enc_ins_np, b_src_lens, b_kb_masks_np)\n        preds = self._decode_response(pred_idxs)\n        if self.debug:\n            log.debug(""Dialog prediction = \\""{}\\"""".format(preds[-1]))\n        return preds\n\n    def save(self):\n        self.network.save()\n\n    def load(self):\n        pass\n'"
deeppavlov/models/seq2seq_go_bot/dialog_state.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\n\n\n@register(""dialog_state"")\nclass DialogState(Component):\n    def __init__(self, *args, **kwargs):\n        self.states = {}\n\n    def __call__(self, user_ids, utterances=None, *args, **kwargs):\n        if utterances is None:\n            return [self.states.get(u, []) for u in user_ids]\n\n        for user, utter in zip(user_ids, utterances):\n            self.states[user] = self.states.get(user, []) + utter\n        return\n'"
deeppavlov/models/seq2seq_go_bot/kb.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport itertools\nimport json\nimport re\nfrom collections import defaultdict\nfrom logging import getLogger\nfrom typing import Callable, List, Tuple\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.core.models.estimator import Estimator\n\nlog = getLogger(__name__)\n\n\n@register(""knowledge_base"")\nclass KnowledgeBase(Estimator):\n    """"""\n    A custom dictionary that encodes knowledge facts from\n    :class:`~deeppavlov.dataset_readers.kvret_reader.KvretDatasetReader` data.\n\n    Example:\n        .. code:: python\n\n            >>> from deeppavlov.models.seq2seq_go_bot.kb import KnowledgeBase\n            >>> kb = KnowledgeBase(save_path=""kb.json"", load_path=""kb.json"")\n            >>> kb.fit([\'person1\'], [[\'name\', \'hair\', \'eyes\']], [[{\'name\': \'Sasha\', \'hair\': \'long   dark\', \'eyes\': \'light blue \'}]])\n\n            >>> kb([\'person1\'])\n            [[(\'sasha_name\', \'Sasha\'), (\'sasha_hair\', \'long   dark\'), (\'sasha_eyes\', \'light blue \')]]\n\n            >>> kb([\'person_that_doesnt_exist\'])\n            [[]]\n\n    Parameters:\n        save_path: path to save the dictionary with knowledge.\n        load_path: path to load the json with knowledge.\n        tokenizer: tokenizer used to split entity values into tokens (inputs batch\n            of strings and outputs batch of lists of tokens).\n        **kwargs: parameters passed to parent\n            :class:`~deeppavlov.core.models.estimator.Estimator`.\n    """"""\n\n    def __init__(self,\n                 save_path: str,\n                 load_path: str = None,\n                 tokenizer: Callable = None,\n                 *args, **kwargs) -> None:\n        super().__init__(save_path=save_path,\n                         load_path=load_path,\n                         *args, **kwargs)\n        self.tokenizer = tokenizer\n        self.kb = defaultdict(lambda: [])\n        self.primary_keys = []\n        if self.load_path and self.load_path.is_file():\n            self.load()\n\n    def fit(self, *args):\n        self.reset()\n        self._update(*args)\n\n    def _update(self, keys, kb_columns_list, kb_items_list, update_primary_keys=True):\n        for key, cols, items in zip(keys, kb_columns_list, kb_items_list):\n            if (None not in (key, items, cols)) and (key not in self.kb):\n                kv_entry_list = (self._key_value_entries(item, cols,\n                                                         update=update_primary_keys)\n                                 for item in items)\n                self.kb[key] = list(itertools.chain(*kv_entry_list))\n\n    def _key_value_entries(self, kb_item, kb_columns, update=True):\n        def _format(s):\n            return re.sub(\'\\s+\', \'_\', s.lower().strip())\n\n        first_key = _format(kb_item[kb_columns[0]])\n        for col in kb_columns:\n            key = first_key + \'_\' + _format(col)\n            if update and (key not in self.primary_keys):\n                self.primary_keys.append(key)\n            if col in kb_item:\n                if self.tokenizer is not None:\n                    yield (key, self.tokenizer([kb_item[col]])[0])\n                else:\n                    yield (key, kb_item[col])\n\n    def __call__(self, keys, kb_columns_list=None, kb_items_list=None):\n        if None not in (kb_columns_list, kb_items_list):\n            self._update(keys, kb_columns_list, kb_items_list, update_primary_keys=False)\n        res = []\n        for key in keys:\n            res.append(self.kb[key])\n            for k, value in res[-1]:\n                if k not in self.primary_keys:\n                    raise ValueError(""Primary key `{}` is not present in knowledge base""\n                                     .format(k))\n        return res\n\n    def __len__(self):\n        return len(self.kb)\n\n    def keys(self):\n        return self.kb.keys()\n\n    def reset(self):\n        self.kb = defaultdict(lambda: [])\n        self.primary_keys = []\n\n    def save(self):\n        log.info(""[saving knowledge base to {}]"".format(self.save_path))\n        json.dump(self.kb, self.save_path.open(\'wt\'))\n        json.dump(self.primary_keys, self.save_path.with_suffix(\'.keys.json\').open(\'wt\'))\n\n    def load(self):\n        log.info(""[loading knowledge base from {}]"".format(self.load_path))\n        self.kb.update(json.load(self.load_path.open(\'rt\')), primary_keys=False)\n        self.primary_keys = json.load(self.load_path.with_suffix(\'.keys.json\').open(\'rt\'))\n\n\n@register(""knowledge_base_entity_normalizer"")\nclass KnowledgeBaseEntityNormalizer(Component):\n    """"""\n    Uses instance of :class:`~deeppavlov.models.seq2seq_go_bot.kb.KnowledgeBase`\n    to normalize or to undo normalization of entities in the input utterance.\n\n    To normalize is to substitute all mentions of database entities with their\n    normalized form.\n\n    To undo normalization is to substitute all mentions of database normalized entities\n    with their original form.\n\n    Example:\n        .. code:: python\n\n            >>> from deeppavlov.models.seq2seq_go_bot.kb import KnowledgeBase\n            >>> kb = KnowledgeBase(save_path=""kb.json"", load_path=""kb.json"", tokenizer=lambda strings: [s.split() for s in strings])\n            >>> kb.fit([\'person1\'], [[\'name\', \'hair\', \'eyes\']], [[{\'name\': \'Sasha\', \'hair\': \'long   dark\', \'eyes\': \'light blue \'}]])\n            >>> kb([\'person1\'])\n            [[(\'sasha_name\', [\'Sasha\']), (\'sasha_hair\', [\'long\', \'dark\']), (\'sasha_eyes\', [\'light\',\'blue\'])]]\n\n            >>> from deeppavlov.models.seq2seq_go_bot.kb import KnowledgeBaseEntityNormalizer\n            >>> normalizer = KnowledgeBaseEntityNormalizer(denormalize=False, remove=False)\n            >>> normalizer([[""some"", ""guy"", ""with"", ""long"", ""dark"", ""hair"", ""said"", ""hi""]], kb([\'person1\']))\n            [[\'some\', \'guy\', \'with\', \'sasha_hair\', \'hair\', \'said\', \'hi\']]\n\n            >>> denormalizer = KnowledgeBaseEntityNormalizer(denormalize=True)\n            >>> denormalizer([[\'some\', \'guy\', \'with\', \'sasha_hair\', \'hair\', \'said\', \'hi\']], kb([\'person1\']))\n            [[\'some\', \'guy\', \'with\', \'long\', \'dark\', \'hair\', \'said\', \'hi\']]\n\n            >>> remover = KnowledgeBaseEntityNormalizer(denormalize=False, remove=True)\n            >>> remover([[""some"", ""guy"", ""with"", ""long"", ""dark"", ""hair"", ""said"", ""hi""]], kb([\'person1\']))\n            [[\'some\', \'guy\', \'with\', \'hair\', \'said\', \'hi\']\n\n\n    Parameters:\n        denormalize: flag indicates whether to normalize or to undo normalization\n            (""denormalize"").\n        remove: flag indicates whether to remove entities or not while normalizing\n            (``denormalize=False``). Is ignored for ``denormalize=True``.\n        **kwargs: parameters passed to parent\n            :class:`~deeppavlov.core.models.component.Component` class.\n    """"""\n\n    def __init__(self,\n                 remove: bool = False,\n                 denormalize: bool = False,\n                 **kwargs):\n        self.denormalize_flag = denormalize\n        self.remove = remove\n\n    def normalize(self, tokens, entries):\n        for entity, ent_tokens in sorted(entries, key=lambda e: -len(e[1])):\n            ent_num_tokens = len(ent_tokens)\n            if \' \'.join(ent_tokens).strip():\n                for i in range(len(tokens)):\n                    if tokens[i:i + ent_num_tokens] == ent_tokens:\n                        if self.remove:\n                            tokens = tokens[:i] + tokens[i + ent_num_tokens:]\n                        else:\n                            tokens = tokens[:i] + [entity] + tokens[i + ent_num_tokens:]\n        return tokens\n\n    def denormalize(self, tokens, entries):\n        for entity, ent_tokens in entries:\n            while (entity in tokens):\n                ent_pos = tokens.index(entity)\n                tokens = tokens[:ent_pos] + ent_tokens + tokens[ent_pos + 1:]\n        return tokens\n\n    def __call__(self,\n                 tokens_list: List[List[str]],\n                 entries_list: List[Tuple[str, List[str]]]) -> List[List[str]]:\n        if self.denormalize_flag:\n            return [self.denormalize(t, e) for t, e in zip(tokens_list, entries_list)]\n        return [self.normalize(t, e) for t, e in zip(tokens_list, entries_list)]\n'"
deeppavlov/models/seq2seq_go_bot/kb_attn_layer.py,16,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.layers import base\nfrom tensorflow.python.ops import init_ops\n\n\nclass KBAttention(base.Layer):\n    # TODO: update class doc\n    """"""Densely-connected layer class.\n    Arguments:\n        units: Integer or Long, dimensionality of the output space.\n        activation: Activation function (callable). Set it to None to maintain a\n          linear activation.\n        use_bias: Boolean, whether the layer uses a bias.\n        kernel_initializer: Initializer function for the weight matrix.\n          If ``None`` (default), weights are initialized using the default\n          initializer used by `tf.get_variable`.\n        bias_initializer: Initializer function for the bias.\n        kernel_regularizer: Regularizer function for the weight matrix.\n        bias_regularizer: Regularizer function for the bias.\n        activity_regularizer: Regularizer function for the output.\n        kernel_constraint: An optional projection function to be applied to the\n            kernel after being updated by an `Optimizer` (e.g. used to implement\n            norm constraints or value constraints for layer weights). The function\n            must take as input the unprojected variable and must return the\n            projected variable (which must have the same shape). Constraints are\n            not safe to use when doing asynchronous distributed training.\n        bias_constraint: An optional projection function to be applied to the\n            bias after being updated by an `Optimizer`.\n        trainable: Boolean, if `True` also add variables to the graph collection\n          `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n        name: String, the name of the layer. Layers with the same name will\n          share weights, but to avoid mistakes we require reuse=True in such cases.\n        reuse: Boolean, whether to reuse the weights of a previous layer\n          by the same name.\n    Properties:\n        units: Python integer, dimensionality of the output space.\n        activation: Activation function (callable).\n        use_bias: Boolean, whether the layer uses a bias.\n        kernel_initializer: Initializer instance (or name) for the kernel matrix.\n        bias_initializer: Initializer instance (or name) for the bias.\n        kernel_regularizer: Regularizer instance for the kernel matrix (callable)\n        bias_regularizer: Regularizer instance for the bias (callable).\n        activity_regularizer: Regularizer instance for the output (callable)\n        kernel_constraint: Constraint function for the kernel matrix.\n        bias_constraint: Constraint function for the bias.\n        kernel: Weight matrix (TensorFlow variable or tensor).\n        bias: Bias vector, if applicable (TensorFlow variable or tensor).\n    """"""\n\n    def __init__(self, units, hidden_sizes,\n                 kb_inputs,\n                 kb_mask,\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer=None,\n                 bias_initializer=init_ops.zeros_initializer(),\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 trainable=True,\n                 name=None,\n                 reuse=None,\n                 **kwargs):\n        super(KBAttention, self).__init__(trainable=trainable, name=name,\n                                          activity_regularizer=activity_regularizer,\n                                          *kwargs)\n        self.units = units\n        self.hidden_sizes = hidden_sizes\n        self.kb_inputs = kb_inputs\n        self.kb_mask = kb_mask\n        self.kb_input_shape = kb_inputs.get_shape().as_list()\n        self.dense_name = name or ""mlp""\n        self.dense_params = {\n            ""activation"": activation,\n            ""use_bias"": use_bias,\n            ""kernel_initializer"": kernel_initializer,\n            ""bias_initializer"": bias_initializer,\n            ""kernel_regularizer"": kernel_regularizer,\n            ""bias_regularizer"": bias_regularizer,\n            ""activity_regularizer"": activity_regularizer,\n            ""kernel_constraint"": kernel_constraint,\n            ""bias_constraint"": bias_constraint,\n            ""trainable"": trainable,\n            ""dtype"": self.kb_inputs.dtype.base_dtype,\n            ""_reuse"": reuse\n        }\n        # print(""KB shape ="", self.kb_input_shape)\n\n    def build(self, input_shape):\n        # if in_shape[:-1] != self.kb_inputs.shape \n        # TODO: check input shape\n        # print(""in build"")\n        in_shape = input_shape[:1].concatenate(self.kb_input_shape)\n        in_shape = in_shape[:-1].concatenate(in_shape[-1] + input_shape[-1])\n        # print(""first in_shape ="", in_shape)\n        self.layers = []\n        for i, size in enumerate(self.hidden_sizes):\n            name = self.dense_name\n            if name is not None:\n                name = name + \'{:d}\'.format(i)\n            layer = tf.layers.Dense(size, name=name, _scope=name, **self.dense_params)\n            layer.build(in_shape)\n            in_shape = layer.compute_output_shape(in_shape)\n\n            self.layers.append(layer)\n\n        # print(""input_shape ="", input_shape)\n        # print(""last in_shape ="", in_shape)\n        # in_shape = in_shape[:-2].concatenate(in_shape[-2] + input_shape[-1])\n        # print(""last in_shape ="", in_shape)\n        self.output_layer = tf.layers.Dense(self.units, **self.dense_params)\n        self.output_layer.build(input_shape)\n        # print(""build = True"")\n        self.built = True\n\n    def call(self, inputs):\n        # print(""in call"")\n        # TODO: check input dtype\n\n        # Tile kb_inputs\n        kb_inputs = self.kb_inputs\n        for i in range(inputs.shape.ndims - 1):\n            kb_inputs = tf.expand_dims(kb_inputs, 0)\n        kb_inputs = tf.tile(kb_inputs, tf.concat((tf.shape(inputs)[:-1], [1, 1]), 0))\n\n        # Expand kb_mask\n        kb_mask = self.kb_mask\n        for i in range(inputs.shape.ndims - 2):\n            kb_mask = tf.expand_dims(kb_mask, 1)\n        kb_mask = tf.expand_dims(kb_mask, -1)\n\n        # Tile inputs\n        kb_size = tf.shape(self.kb_inputs)[0]\n        tiling = tf.concat(([1] * (inputs.shape.ndims - 1), [kb_size], [1]), 0)\n        cell_inputs = tf.tile(tf.expand_dims(inputs, -2), tiling)\n\n        outputs = tf.concat([kb_inputs, cell_inputs], -1)\n        outputs = tf.multiply(outputs, kb_mask)\n        for layer in self.layers:\n            outputs = layer.call(outputs)\n        # outputs = tf.Print(outputs, [outputs], ""KB attention pre-last layer output ="")\n        outputs = tf.squeeze(outputs, [-1])\n        # print(""inputs shape ="", inputs.shape)\n        # print(""outputs shape ="", outputs.shape)\n        outputs = tf.concat([self.output_layer(inputs), outputs], -1)\n        # print(""out of call"")\n        return outputs\n\n    def _compute_output_shape(self, input_shape):\n        input_shape = tensor_shape.TensorShape(input_shape)\n        input_shape = input_shape.with_rank_at_least(2)\n        if input_shape[-1].value is None:\n            raise ValueError(\n                \'The innermost dimension of input_shape must be defined, but saw: %s\'\n                % input_shape)\n        output_shape = input_shape[:-1].concatenate(self.units + self.kb_input_shape[0])\n        # print(""computed output shape is"", output_shape)\n        return output_shape\n\n    def compute_output_shape(self, input_shape):\n        return self._compute_output_shape(input_shape)\n'"
deeppavlov/models/seq2seq_go_bot/network.py,74,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport math\nfrom logging import getLogger\nfrom typing import List\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom deeppavlov.core.common.errors import ConfigError\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.tf_model import TFModel\nfrom deeppavlov.models.seq2seq_go_bot.kb_attn_layer import KBAttention\n\nlog = getLogger(__name__)\n\n\n@register(""seq2seq_go_bot_nn"")\nclass Seq2SeqGoalOrientedBotNetwork(TFModel):\n    """"""\n    The :class:`~deeppavlov.models.seq2seq_go_bot.bot.GoalOrientedBotNetwork`\n    is a recurrent network that encodes user utterance and generates response\n    in a sequence-to-sequence manner.\n\n    For network architecture is similar to https://arxiv.org/abs/1705.05414 .\n\n    Parameters:\n        hidden_size: RNN hidden layer size.\n        source_vocab_size: size of a vocabulary of encoder tokens.\n        target_vocab_size: size of a vocabulary of decoder tokens.\n        target_start_of_sequence_index: index of a start of sequence token during\n            decoding.\n        target_end_of_sequence_index: index of an end of sequence token during decoding.\n        knowledge_base_entry_embeddings: matrix with embeddings of knowledge base entries,\n            size is (number of entries, embedding size).\n        kb_attention_hidden_sizes: list of sizes for attention hidden units.\n        decoder_embeddings: matrix with embeddings for decoder output tokens, size is\n            (`targer_vocab_size` + number of knowledge base entries, embedding size).\n        beam_width: width of beam search decoding.\n        learning_rate: learning rate during training.\n        end_learning_rate: if set, learning rate starts from ``learning_rate`` value\n            and decays polynomially to the value of ``end_learning_rate``.\n        decay_steps: number of steps of learning rate decay.\n        decay_power: power used to calculate learning rate decay for polynomial strategy.\n        dropout_rate: probability of weights\' dropout.\n        state_dropout_rate: probability of rnn state dropout.\n        optimizer: one of tf.train.Optimizer subclasses as a string.\n        **kwargs: parameters passed to a parent\n            :class:`~deeppavlov.core.models.tf_model.TFModel` class.\n    """"""\n\n    GRAPH_PARAMS = [\'knowledge_base_size\', \'source_vocab_size\',\n                    \'target_vocab_size\', \'hidden_size\', \'embedding_size\',\n                    \'kb_embedding_control_sum\', \'kb_attention_hidden_sizes\']\n\n    def __init__(self,\n                 hidden_size: int,\n                 source_vocab_size: int,\n                 target_vocab_size: int,\n                 target_start_of_sequence_index: int,\n                 target_end_of_sequence_index: int,\n                 knowledge_base_entry_embeddings: np.ndarray,\n                 kb_attention_hidden_sizes: List[int],\n                 decoder_embeddings: np.ndarray,\n                 learning_rate: float,\n                 beam_width: int = 1,\n                 end_learning_rate: float = None,\n                 decay_steps: int = 1000,\n                 decay_power: float = 1.0,\n                 dropout_rate: float = 0.0,\n                 state_dropout_rate: float = 0.0,\n                 optimizer: str = \'AdamOptimizer\',\n                 **kwargs) -> None:\n        end_learning_rate = end_learning_rate or learning_rate\n\n        # initialize knowledge base embeddings\n        self.kb_embedding = np.array(knowledge_base_entry_embeddings)\n        log.debug(""recieved knowledge_base_entry_embeddings with shape = {}""\n                  .format(self.kb_embedding.shape))\n        # initialize decoder embeddings\n        self.decoder_embedding = np.array(decoder_embeddings)\n        if self.kb_embedding.shape[1] != self.decoder_embedding.shape[1]:\n            raise ValueError(""decoder embeddings should have the same dimension""\n                             "" as knowledge base entries\' embeddings"")\n\n        # specify model options\n        self.opt = {\n            \'hidden_size\': hidden_size,\n            \'source_vocab_size\': source_vocab_size,\n            \'target_vocab_size\': target_vocab_size,\n            \'target_start_of_sequence_index\': target_start_of_sequence_index,\n            \'target_end_of_sequence_index\': target_end_of_sequence_index,\n            \'kb_attention_hidden_sizes\': kb_attention_hidden_sizes,\n            \'kb_embedding_control_sum\': float(np.sum(self.kb_embedding)),\n            \'knowledge_base_size\': self.kb_embedding.shape[0],\n            \'embedding_size\': self.kb_embedding.shape[1],\n            \'learning_rate\': learning_rate,\n            \'beam_width\': beam_width,\n            \'end_learning_rate\': end_learning_rate,\n            \'decay_steps\': decay_steps,\n            \'decay_power\': decay_power,\n            \'dropout_rate\': dropout_rate,\n            \'state_dropout_rate\': state_dropout_rate,\n            \'optimizer\': optimizer\n        }\n\n        # initialize other parameters\n        self._init_params()\n        # build computational graph\n        self._build_graph()\n        # initialize session\n        self.sess = tf.Session()\n        # from tensorflow.python import debug as tf_debug\n        # self.sess = tf_debug.TensorBoardDebugWrapperSession(self.sess, ""vimary-pc:7019"")\n        self.global_step = 0\n\n        self.sess.run(tf.global_variables_initializer())\n\n        super().__init__(**kwargs)\n\n        if tf.train.checkpoint_exists(str(self.load_path.resolve())):\n            log.info(""[initializing `{}` from saved]"".format(self.__class__.__name__))\n            self.load()\n        else:\n            log.info(""[initializing `{}` from scratch]"".format(self.__class__.__name__))\n\n    def _init_params(self):\n        self.hidden_size = self.opt[\'hidden_size\']\n        self.src_vocab_size = self.opt[\'source_vocab_size\']\n        self.tgt_vocab_size = self.opt[\'target_vocab_size\']\n        self.tgt_sos_id = self.opt[\'target_start_of_sequence_index\']\n        self.tgt_eos_id = self.opt[\'target_end_of_sequence_index\']\n        self.learning_rate = self.opt[\'learning_rate\']\n        self.kb_attn_hidden_sizes = self.opt[\'kb_attention_hidden_sizes\']\n        self.embedding_size = self.opt[\'embedding_size\']\n        self.kb_size = self.opt[\'knowledge_base_size\']\n        self.beam_width = self.opt[\'beam_width\']\n        self.learning_rate = self.opt[\'learning_rate\']\n        self.end_learning_rate = self.opt[\'end_learning_rate\']\n        self.dropout_rate = self.opt[\'dropout_rate\']\n        self.state_dropout_rate = self.opt[\'state_dropout_rate\']\n        self.decay_steps = self.opt[\'decay_steps\']\n        self.decay_power = self.opt[\'decay_power\']\n\n        self._optimizer = None\n        if hasattr(tf.train, self.opt[\'optimizer\']):\n            self._optimizer = getattr(tf.train, self.opt[\'optimizer\'])\n        if not issubclass(self._optimizer, tf.train.Optimizer):\n            raise ConfigError(""`optimizer` parameter should be a name of""\n                              "" tf.train.Optimizer subclass"")\n\n    def _build_graph(self):\n\n        self._add_placeholders()\n\n        _logits, self._predictions = self._build_body()\n\n        _weights = tf.expand_dims(self._tgt_weights, -1)\n        _loss_tensor = \\\n            tf.losses.sparse_softmax_cross_entropy(logits=_logits,\n                                                   labels=self._decoder_outputs,\n                                                   weights=_weights,\n                                                   reduction=tf.losses.Reduction.NONE)\n        # normalize loss by batch_size\n        _loss_tensor = \\\n            tf.verify_tensor_all_finite(_loss_tensor, ""Non finite values in loss tensor."")\n        self._loss = tf.reduce_sum(_loss_tensor) / tf.cast(self._batch_size, tf.float32)\n        # self._loss = tf.reduce_mean(_loss_tensor, name=\'loss\')\n        # TODO: tune clip_norm\n        self._train_op = \\\n            self.get_train_op(self._loss,\n                              learning_rate=self._learning_rate,\n                              optimizer=self._optimizer,\n                              clip_norm=2.)\n        # log.info(""Trainable variables"")\n        # for v in tf.trainable_variables():\n        #    log.info(v)\n        # self.print_number_of_parameters()\n\n    def _add_placeholders(self):\n        self._dropout_keep_prob = tf.placeholder_with_default(\n            1.0, shape=[], name=\'dropout_keep_prob\')\n        self._state_dropout_keep_prob = tf.placeholder_with_default(\n            1.0, shape=[], name=\'state_dropout_keep_prob\')\n        self._learning_rate = tf.placeholder(tf.float32,\n                                             shape=[],\n                                             name=\'learning_rate\')\n        # _encoder_inputs: [batch_size, max_input_time]\n        # _encoder_inputs: [batch_size, max_input_time, embedding_size]\n        self._encoder_inputs = tf.placeholder(tf.float32,\n                                              [None, None, self.embedding_size],\n                                              name=\'encoder_inputs\')\n        self._batch_size = tf.shape(self._encoder_inputs)[0]\n        # _decoder_inputs: [batch_size, max_output_time]\n        self._decoder_inputs = tf.placeholder(tf.int32,\n                                              [None, None],\n                                              name=\'decoder_inputs\')\n        # _decoder_embedding: [tgt_vocab_size + kb_size, embedding_size]\n        self._decoder_embedding = \\\n            tf.get_variable(""decoder_embedding"",\n                            shape=(self.tgt_vocab_size + self.kb_size,\n                                   self.embedding_size),\n                            dtype=tf.float32,\n                            initializer=tf.constant_initializer(self.decoder_embedding),\n                            trainable=False)\n        # _decoder_outputs: [batch_size, max_output_time]\n        self._decoder_outputs = tf.placeholder(tf.int32,\n                                               [None, None],\n                                               name=\'decoder_outputs\')\n        # _kb_embedding: [kb_size, embedding_size]\n        # TODO: try training embeddings\n        kb_W = np.array(self.kb_embedding)[:, :self.embedding_size]\n        self._kb_embedding = tf.get_variable(""kb_embedding"",\n                                             shape=(kb_W.shape[0], kb_W.shape[1]),\n                                             dtype=tf.float32,\n                                             initializer=tf.constant_initializer(kb_W),\n                                             trainable=True)\n        # _kb_mask: [batch_size, kb_size]\n        self._kb_mask = tf.placeholder(tf.float32, [None, None], name=\'kb_mask\')\n\n        # TODO: compute sequence lengths on the go\n        # _src_sequence_lengths, _tgt_sequence_lengths: [batch_size]\n        self._src_sequence_lengths = tf.placeholder(tf.int32,\n                                                    [None],\n                                                    name=\'input_sequence_lengths\')\n        self._tgt_sequence_lengths = tf.placeholder(tf.int32,\n                                                    [None],\n                                                    name=\'output_sequence_lengths\')\n        # _tgt_weights: [batch_size, max_output_time]\n        self._tgt_weights = tf.placeholder(tf.int32,\n                                           [None, None],\n                                           name=\'target_weights\')\n\n    def _build_body(self):\n        self._build_encoder()\n        self._build_decoder()\n        return self._logits, self._predictions\n\n    def _build_encoder(self):\n        with tf.variable_scope(""Encoder""):\n            # Encoder embedding\n            # _encoder_embedding = tf.get_variable(\n            #   ""encoder_embedding"", [self.src_vocab_size, self.embedding_size])\n            # _encoder_emb_inp = tf.nn.embedding_lookup(_encoder_embedding,\n            #                                          self._encoder_inputs)\n            # _encoder_emb_inp = tf.one_hot(self._encoder_inputs, self.src_vocab_size)\n            _encoder_emb_inp = self._encoder_inputs\n\n            _encoder_cell = tf.nn.rnn_cell.LSTMCell(self.hidden_size,\n                                                    name=\'basic_lstm_cell\')\n            _encoder_cell = tf.contrib.rnn.DropoutWrapper(\n                _encoder_cell,\n                input_size=self.embedding_size,\n                dtype=tf.float32,\n                input_keep_prob=self._dropout_keep_prob,\n                output_keep_prob=self._dropout_keep_prob,\n                state_keep_prob=self._state_dropout_keep_prob,\n                variational_recurrent=True)\n            # Run Dynamic RNN\n            #   _encoder_outputs: [max_time, batch_size, hidden_size]\n            #   _encoder_state: [batch_size, hidden_size]\n            # input_states?\n            _encoder_outputs, _encoder_state = tf.nn.dynamic_rnn(\n                _encoder_cell, _encoder_emb_inp, dtype=tf.float32,\n                sequence_length=self._src_sequence_lengths, time_major=False)\n\n        self._encoder_outputs = _encoder_outputs\n        self._encoder_state = _encoder_state\n\n    def _build_decoder(self):\n        with tf.variable_scope(""Decoder""):\n            # Decoder embedding\n            # _decoder_embedding = tf.get_variable(\n            #    ""decoder_embedding"", [self.tgt_vocab_size + self.kb_size,\n            #                          self.embedding_size])\n            # _decoder_emb_inp = tf.one_hot(self._decoder_inputs,\n            #                              self.tgt_vocab_size + self.kb_size)\n            _decoder_emb_inp = tf.nn.embedding_lookup(self._decoder_embedding,\n                                                      self._decoder_inputs)\n\n            # Tiling outputs, states, sequence lengths\n            _tiled_encoder_outputs = tf.contrib.seq2seq.tile_batch(\n                self._encoder_outputs, multiplier=self.beam_width)\n            _tiled_encoder_state = tf.contrib.seq2seq.tile_batch(\n                self._encoder_state, multiplier=self.beam_width)\n            _tiled_src_sequence_lengths = tf.contrib.seq2seq.tile_batch(\n                self._src_sequence_lengths, multiplier=self.beam_width)\n\n            with tf.variable_scope(""AttentionOverKB""):\n                _kb_attn_layer = KBAttention(self.tgt_vocab_size,\n                                             self.kb_attn_hidden_sizes + [1],\n                                             self._kb_embedding,\n                                             self._kb_mask,\n                                             activation=tf.nn.relu,\n                                             use_bias=False)\n            # Output dense layer\n            # _projection_layer = \\\n            #  tf.layers.Dense(self.tgt_vocab_size, use_bias=False, _reuse=reuse)\n\n            # Decoder Cell\n            _decoder_cell = tf.nn.rnn_cell.LSTMCell(self.hidden_size,\n                                                    name=\'basic_lstm_cell\')\n            _decoder_cell = tf.contrib.rnn.DropoutWrapper(\n                _decoder_cell,\n                input_size=self.embedding_size + self.hidden_size,\n                dtype=tf.float32,\n                input_keep_prob=self._dropout_keep_prob,\n                output_keep_prob=self._dropout_keep_prob,\n                state_keep_prob=self._state_dropout_keep_prob,\n                variational_recurrent=True)\n\n            def build_dec_cell(enc_out, enc_seq_len, reuse=None):\n                with tf.variable_scope(""dec_cell_attn"", reuse=reuse):\n                    # Create an attention mechanism\n                    # _attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n                    _attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n                        self.hidden_size,\n                        memory=enc_out,\n                        memory_sequence_length=enc_seq_len)\n                    _cell = tf.contrib.seq2seq.AttentionWrapper(\n                        _decoder_cell,\n                        _attention_mechanism,\n                        attention_layer_size=self.hidden_size)\n                    return _cell\n\n            # TRAIN MODE\n            _decoder_cell_tr = build_dec_cell(self._encoder_outputs,\n                                              self._src_sequence_lengths)\n            self._decoder_cell_tr = _decoder_cell_tr\n            # Train Helper to feed inputs for training:\n            # read inputs from dense ground truth vectors\n            _helper_tr = tf.contrib.seq2seq.TrainingHelper(\n                _decoder_emb_inp, self._tgt_sequence_lengths, time_major=False)\n            # Copy encoder hidden state to decoder inital state\n            _decoder_init_state = \\\n                _decoder_cell_tr.zero_state(self._batch_size, dtype=tf.float32) \\\n                    .clone(cell_state=self._encoder_state)\n            _decoder_tr = \\\n                tf.contrib.seq2seq.BasicDecoder(_decoder_cell_tr, _helper_tr,\n                                                initial_state=_decoder_init_state,\n                                                output_layer=_kb_attn_layer)\n            # Wrap into variable scope to share attention parameters\n            # Required!\n            with tf.variable_scope(\'decode_with_shared_attention\'):\n                _outputs_inf, _, _ = \\\n                    tf.contrib.seq2seq.dynamic_decode(_decoder_tr,\n                                                      impute_finished=False,\n                                                      output_time_major=False)\n            # _logits = decode(_helper, ""decode"").beam_search_decoder_output.scores\n            _logits = _outputs_inf.rnn_output\n\n            # INFER MODE\n            _decoder_cell_inf = build_dec_cell(_tiled_encoder_outputs,\n                                               _tiled_src_sequence_lengths,\n                                               reuse=True)\n            self._decoder_cell_inf = _decoder_cell_inf\n            # Infer Helper\n            _max_iters = tf.round(tf.reduce_max(self._src_sequence_lengths) * 2)\n            # NOTE: helper is not needed?\n            # _helper_inf = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n            #    self._decoder_embedding,\n            #    tf.fill([self._batch_size], self.tgt_sos_id), self.tgt_eos_id)\n            #    lambda d: tf.one_hot(d, self.tgt_vocab_size + self.kb_size),\n            # Decoder Init State\n            _decoder_init_state = \\\n                _decoder_cell_inf.zero_state(tf.shape(_tiled_encoder_outputs)[0],\n                                             dtype=tf.float32) \\\n                    .clone(cell_state=_tiled_encoder_state)\n            # Define a beam-search decoder\n            _start_tokens = tf.tile(tf.constant([self.tgt_sos_id], tf.int32),\n                                    [self._batch_size])\n            # _start_tokens = tf.fill([self._batch_size], self.tgt_sos_id)\n            _decoder_inf = tf.contrib.seq2seq.BeamSearchDecoder(\n                cell=_decoder_cell_inf,\n                embedding=self._decoder_embedding,\n                start_tokens=_start_tokens,\n                end_token=self.tgt_eos_id,\n                initial_state=_decoder_init_state,\n                beam_width=self.beam_width,\n                output_layer=_kb_attn_layer,\n                length_penalty_weight=0.0)\n\n            # Wrap into variable scope to share attention parameters\n            # Required!\n            with tf.variable_scope(""decode_with_shared_attention"", reuse=True):\n                # TODO: try impute_finished = True,\n                _outputs_inf, _, _ = \\\n                    tf.contrib.seq2seq.dynamic_decode(_decoder_inf,\n                                                      impute_finished=False,\n                                                      maximum_iterations=_max_iters,\n                                                      output_time_major=False)\n            _predictions = _outputs_inf.predicted_ids[:, :, 0]\n            # TODO: rm indexing\n            # _predictions = \\\n            #    decode(_helper_infer, ""decode"", _max_iters, reuse=True).sample_id\n        self._logits = _logits\n        self._predictions = _predictions\n\n    def __call__(self, enc_inputs, src_seq_lengths, kb_masks, prob=False):\n        predictions = self.sess.run(\n            self._predictions,\n            feed_dict={\n                self._dropout_keep_prob: 1.,\n                self._state_dropout_keep_prob: 1.,\n                self._learning_rate: 1.,\n                self._encoder_inputs: enc_inputs,\n                self._src_sequence_lengths: src_seq_lengths,\n                self._kb_mask: kb_masks\n            }\n        )\n        # TODO: implement infer probabilities\n        if prob:\n            raise NotImplementedError(""Probs not available for now."")\n        return predictions\n\n    def train_on_batch(self, enc_inputs, dec_inputs, dec_outputs,\n                       src_seq_lengths, tgt_seq_lengths, tgt_weights, kb_masks):\n        _, loss_value = self.sess.run(\n            [self._train_op, self._loss],\n            feed_dict={\n                self._dropout_keep_prob: 1 - self.dropout_rate,\n                self._state_dropout_keep_prob: 1 - self.state_dropout_rate,\n                self._learning_rate: self.get_learning_rate(),\n                self._encoder_inputs: enc_inputs,\n                self._decoder_inputs: dec_inputs,\n                self._decoder_outputs: dec_outputs,\n                self._src_sequence_lengths: src_seq_lengths,\n                self._tgt_sequence_lengths: tgt_seq_lengths,\n                self._tgt_weights: tgt_weights,\n                self._kb_mask: kb_masks\n            }\n        )\n        return {\'loss\': loss_value, \'learning_rate\': self.get_learning_rate()}\n\n    def get_learning_rate(self):\n        # polynomial decay\n        global_step = min(self.global_step, self.decay_steps)\n        decayed_learning_rate = \\\n            (self.learning_rate - self.end_learning_rate) * \\\n            (1 - global_step / self.decay_steps) ** self.decay_power + \\\n            self.end_learning_rate\n        return decayed_learning_rate\n\n    def load(self, *args, **kwargs):\n        self.load_params()\n        super().load(*args, **kwargs)\n\n    def load_params(self):\n        path = str(self.load_path.with_suffix(\'.json\').resolve())\n        log.info(\'[loading parameters from {}]\'.format(path))\n        with open(path, \'r\', encoding=\'utf8\') as fp:\n            params = json.load(fp)\n        for p in self.GRAPH_PARAMS:\n            if self.opt.get(p) != params.get(p):\n                if p in (\'kb_embedding_control_sum\') and \\\n                        (math.abs(self.opt.get(p, 0.) - params.get(p, 0.)) < 1e-3):\n                    continue\n                raise ConfigError(""`{}` parameter must be equal to saved model""\n                                  "" parameter value `{}`, but is equal to `{}`""\n                                  .format(p, params.get(p), self.opt.get(p)))\n\n    def save(self, *args, **kwargs):\n        super().save(*args, **kwargs)\n        self.save_params()\n\n    def save_params(self):\n        path = str(self.save_path.with_suffix(\'.json\').resolve())\n        log.info(\'[saving parameters to {}]\'.format(path))\n        with open(path, \'w\', encoding=\'utf8\') as fp:\n            json.dump(self.opt, fp)\n\n    def process_event(self, event_name, data):\n        if event_name == \'after_epoch\':\n            log.info(""Updating global step, learning rate = {:.6f}.""\n                     .format(self.get_learning_rate()))\n            self.global_step += 1\n\n    def shutdown(self):\n        self.sess.close()\n'"
deeppavlov/models/sklearn/__init__.py,0,b'from .sklearn_component import *\n'
deeppavlov/models/sklearn/sklearn_component.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport inspect\nimport pickle\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import List, Tuple, Union, Callable\n\nimport numpy as np\nfrom scipy.sparse import issparse, csr_matrix\nfrom scipy.sparse import spmatrix\nfrom scipy.sparse import vstack, hstack\n\nfrom deeppavlov.core.common.errors import ConfigError\nfrom deeppavlov.core.common.registry import register, cls_from_str\nfrom deeppavlov.core.models.estimator import Estimator\n\nlog = getLogger(__name__)\n\n\n@register(""sklearn_component"")\nclass SklearnComponent(Estimator):\n    """"""\n    Class implements wrapper for sklearn components for feature extraction,\n    feature selection, classification, regression etc.\n\n    Args:\n        model_class: string with full name of sklearn model to use, e.g. ``sklearn.linear_model:LogisticRegression``\n        save_path: save path for model, e.g. full name ``model_path/model.pkl`` \\\n            or prefix ``model_path/model`` (still model will be saved to ``model_path/model.pkl``)\n        load_path: load path for model, e.g. full name ``model_path/model.pkl`` \\\n            or prefix ``model_path/model`` (still model will be loaded from ``model_path/model.pkl``)\n        infer_method: string name of class method to use for infering model, \\\n            e.g. ``predict``, ``predict_proba``, ``predict_log_proba``, ``transform``\n        ensure_list_output: whether to ensure that output for each sample is iterable (but not string)\n        kwargs: dictionary with parameters for the sklearn model\n\n    Attributes:\n        model: sklearn model instance\n        model_class: string with full name of sklearn model to use, e.g. ``sklearn.linear_model:LogisticRegression``\n        model_params: dictionary with parameters for the sklearn model without pipe parameters\n        pipe_params: dictionary with parameters for pipe: ``in``, ``out``, ``fit_on``, ``main``, ``name``\n        save_path: save path for model, e.g. full name ``model_path/model.pkl`` \\\n            or prefix ``model_path/model`` (still model will be saved to ``model_path/model.pkl``)\n        load_path: load path for model, e.g. full name ``model_path/model.pkl`` \\\n            or prefix ``model_path/model`` (still model will be loaded from ``model_path/model.pkl``)\n        infer_method: string name of class method to use for infering model, \\\n            e.g. ``predict``, ``predict_proba``, ``predict_log_proba``, ``transform``\n        ensure_list_output: whether to ensure that output for each sample is iterable (but not string)\n    """"""\n\n    def __init__(self, model_class: str,\n                 save_path: Union[str, Path] = None,\n                 load_path: Union[str, Path] = None,\n                 infer_method: str = ""predict"",\n                 ensure_list_output: bool = False,\n                 **kwargs) -> None:\n        """"""\n        Initialize component with given parameters\n        """"""\n\n        super().__init__(save_path=save_path, load_path=load_path, **kwargs)\n        self.model_class = model_class\n        self.model_params = kwargs\n        self.model = None\n        self.ensure_list_output = ensure_list_output\n        self.pipe_params = {}\n        for required in [""in"", ""out"", ""fit_on"", ""main"", ""name""]:\n            self.pipe_params[required] = self.model_params.pop(required, None)\n\n        self.load()\n        self.infer_method = getattr(self.model, infer_method)\n\n    def fit(self, *args) -> None:\n        """"""\n        Fit model on the given data\n\n        Args:\n            *args: list of x-inputs and, optionally, one y-input (the last one) to fit on.\n                Possible input (x0, ..., xK, y) or (x0, ..., xK) \'\n                where K is the number of input data elements (the length of list ``in`` from config). \\\n                In case of several inputs (K > 1) input features will be stacked. \\\n                For example, one has x0: (n_samples, n_features0), ..., xK: (n_samples, n_featuresK), \\\n                then model will be trained on x: (n_samples, n_features0 + ... + n_featuresK).\n\n        Returns:\n            None\n        """"""\n        n_inputs = len(self.pipe_params[""in""]) if isinstance(self.pipe_params[""in""], list) else 1\n        x_features = self.compose_input_data(args[:n_inputs])\n        if len(args) > n_inputs:\n            y_ = np.squeeze(np.array(args[-1]))\n        else:\n            y_ = None\n\n        try:\n            log.info(""Fitting model {}"".format(self.model_class))\n            self.model.fit(x_features, y_)\n        except TypeError or ValueError:\n            if issparse(x_features):\n                log.info(""Converting input for model {} to dense array"".format(self.model_class))\n                self.model.fit(x_features.todense(), y_)\n            else:\n                log.info(""Converting input for model {} to sparse array"".format(self.model_class))\n                self.model.fit(csr_matrix(x_features), y_)\n\n        return\n\n    def __call__(self, *args):\n        """"""\n        Infer on the given data according to given in the config infer method, \\\n            e.g. ``""predict"", ""predict_proba"", ""transform""``\n\n        Args:\n            *args: list of inputs\n\n        Returns:\n            predictions, e.g. list of labels, array of probability distribution, sparse array of vectorized samples\n        """"""\n        x_features = self.compose_input_data(args)\n\n        try:\n            predictions = self.infer_method(x_features)\n        except TypeError or ValueError:\n            if issparse(x_features):\n                log.info(""Converting input for model {} to dense array"".format(self.model_class))\n                predictions = self.infer_method(x_features.todense())\n            else:\n                log.info(""Converting input for model {} to sparse array"".format(self.model_class))\n                predictions = self.infer_method(csr_matrix(x_features))\n\n        if isinstance(predictions, list):\n            #  ``predict_proba`` sometimes returns list of n_outputs (each output corresponds to a label)\n            #  but we will return (n_samples, n_labels)\n            #  where each value is a probability of a sample to belong with the label\n            predictions_ = [[predictions[j][i][1] for j in range(len(predictions))] for i in range(x_features.shape[0])]\n            predictions = np.array(predictions_)\n\n        if self.ensure_list_output and len(predictions.shape) == 1:\n            predictions = predictions.reshape(-1, 1)\n\n        if issparse(predictions):\n            return predictions\n        else:\n            return predictions.tolist()\n\n    def init_from_scratch(self) -> None:\n        """"""\n        Initialize ``self.model`` as some sklearn model from scratch with given in ``self.model_params`` parameters.\n\n        Returns:\n            None\n        """"""\n        log.info(""Initializing model {} from scratch"".format(self.model_class))\n        model_function = cls_from_str(self.model_class)\n\n        if model_function is None:\n            raise ConfigError(""Model with {} model_class was not found."".format(self.model_class))\n\n        given_params = {}\n        if self.model_params:\n            available_params = self.get_function_params(model_function)\n            for param_name in self.model_params.keys():\n                if param_name in available_params:\n                    try:\n                        given_params[param_name] = cls_from_str(self.model_params[param_name])\n                    except (AttributeError, ValueError, ConfigError):\n                        given_params[param_name] = self.model_params[param_name]\n\n        self.model = model_function(**given_params)\n        return\n\n    def load(self, fname: str = None) -> None:\n        """"""\n        Initialize ``self.model`` as some sklearn model from saved re-initializing ``self.model_params`` parameters. \\\n            If in new given parameters ``warm_start`` is set to True and given model admits ``warm_start`` parameter, \\\n            model will be initilized from saved with opportunity to continue fitting.\n\n        Args:\n            fname: string name of path to model to load from\n\n        Returns:\n            None\n        """"""\n        if fname is None:\n            fname = self.load_path\n\n        fname = Path(fname).with_suffix(\'.pkl\')\n\n        if fname.exists():\n            log.info(""Loading model {} from {}"".format(self.model_class, str(fname)))\n            with open(fname, ""rb"") as f:\n                self.model = pickle.load(f)\n\n            warm_start = self.model_params.get(""warm_start"", None)\n            self.model_params = {param: getattr(self.model, param) for param in self.get_class_attributes(self.model)}\n            self.model_class = self.model.__module__ + self.model.__class__.__name__\n            log.info(""Model {} loaded  with parameters"".format(self.model_class))\n\n            if warm_start and ""warm_start"" in self.model_params.keys():\n                self.model_params[""warm_start""] = True\n                log.info(""Fitting of loaded model can be continued because `warm_start` is set to True"")\n            else:\n                log.warning(""Fitting of loaded model can not be continued. Model can be fitted from scratch.""\n                            ""If one needs to continue fitting, please, look at `warm_start` parameter"")\n        else:\n            log.warning(""Cannot load model from {}"".format(str(fname)))\n            self.init_from_scratch()\n\n        return\n\n    def save(self, fname: str = None) -> None:\n        """"""\n        Save ``self.model`` to the file from ``fname`` or, if not given, ``self.save_path``. \\\n            If ``self.save_path`` does not have ``.pkl`` extension, then it will be replaced \\\n            to ``str(Path(self.save_path).stem) + "".pkl""``\n\n        Args:\n            fname:  string name of path to model to save to\n\n        Returns:\n            None\n        """"""\n        if fname is None:\n            fname = self.save_path\n\n        fname = Path(fname).with_suffix(\'.pkl\')\n\n        log.info(""Saving model to {}"".format(str(fname)))\n        with open(fname, ""wb"") as f:\n            pickle.dump(self.model, f, protocol=4)\n        return\n\n    @staticmethod\n    def compose_input_data(x: List[Union[Tuple[Union[np.ndarray, list, spmatrix, str]],\n                                         List[Union[np.ndarray, list, spmatrix, str]],\n                                         np.ndarray, spmatrix]]) -> Union[spmatrix, np.ndarray]:\n        """"""\n        Stack given list of different types of inputs to the one matrix. If one of the inputs is a sparse matrix, \\\n            then output will be also a sparse matrix\n\n        Args:\n            x: list of data elements\n\n        Returns:\n            sparse or dense array of stacked data\n        """"""\n        x_features = []\n        for i in range(len(x)):\n            if ((isinstance(x[i], tuple) or isinstance(x[i], list) or isinstance(x[i], np.ndarray) and len(x[i]))\n                    or (issparse(x[i]) and x[i].shape[0])):\n                if issparse(x[i][0]):\n                    x_features.append(vstack(list(x[i])))\n                elif isinstance(x[i][0], np.ndarray) or isinstance(x[i][0], list):\n                    x_features.append(np.vstack(list(x[i])))\n                elif isinstance(x[i][0], str):\n                    x_features.append(np.array(x[i]))\n                else:\n                    raise ConfigError(\'Not implemented this type of vectors\')\n            else:\n                raise ConfigError(""Input vectors cannot be empty"")\n\n        sparse = False\n        for inp in x_features:\n            if issparse(inp):\n                sparse = True\n        if sparse:\n            x_features = hstack(list(x_features))\n        else:\n            x_features = np.hstack(list(x_features))\n\n        return x_features\n\n    @staticmethod\n    def get_function_params(f: Callable) -> List[str]:\n        """"""\n        Get list of names of given function\'s parameters\n\n        Args:\n            f: function\n\n        Returns:\n            list of names of given function\'s parameters\n        """"""\n        return inspect.getfullargspec(f)[0]\n\n    @staticmethod\n    def get_class_attributes(cls: type) -> List[str]:\n        """"""\n        Get list of names of given class\' attributes\n\n        Args:\n            cls: class\n\n        Returns:\n            list of names of given class\' attributes\n        """"""\n        return list(cls.__dict__.keys())\n'"
deeppavlov/models/slotfill/__init__.py,0,b''
deeppavlov/models/slotfill/slotfill.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nfrom logging import getLogger\n\nfrom rapidfuzz import process\nfrom overrides import overrides\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.utils import download\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.core.models.serializable import Serializable\n\nlog = getLogger(__name__)\n\n\n@register(\'dstc_slotfilling\')\nclass DstcSlotFillingNetwork(Component, Serializable):\n    """"""Slot filling for DSTC2 task with neural network""""""\n\n    def __init__(self, threshold: float = 0.8, **kwargs):\n        super().__init__(**kwargs)\n        self.threshold = threshold\n        self._slot_vals = None\n        # Check existance of file with slots, slot values, and corrupted (misspelled) slot values\n        self.load()\n\n    @overrides\n    def __call__(self, tokens_batch, tags_batch, *args, **kwargs):\n        slots = [{}] * len(tokens_batch)\n        m = [i for i, v in enumerate(tokens_batch) if v]\n        if m:\n            tags_batch = [tags_batch[i] for i in m]\n            tokens_batch = [tokens_batch[i] for i in m]\n            for i, tokens, tags in zip(m, tokens_batch, tags_batch):\n                slots[i] = self.predict_slots(tokens, tags)\n        return slots\n\n    def predict_slots(self, tokens, tags):\n        # For utterance extract named entities and perform normalization for slot filling\n\n        entities, slots = self._chunk_finder(tokens, tags)\n        slot_values = {}\n        for entity, slot in zip(entities, slots):\n            match, score = self.ner2slot(entity, slot)\n            if score >= self.threshold * 100:\n                slot_values[slot] = match\n        return slot_values\n\n    def ner2slot(self, input_entity, slot):\n        # Given named entity return normalized slot value\n        if isinstance(input_entity, list):\n            input_entity = \' \'.join(input_entity)\n        entities = []\n        normalized_slot_vals = []\n        for entity_name in self._slot_vals[slot]:\n            for entity in self._slot_vals[slot][entity_name]:\n                entities.append(entity)\n                normalized_slot_vals.append(entity_name)\n        best_match, score = process.extract(input_entity, entities, limit=2 ** 20)[0]\n        return normalized_slot_vals[entities.index(best_match)], score\n\n    @staticmethod\n    def _chunk_finder(tokens, tags):\n        # For BIO labeled sequence of tags extract all named entities form tokens\n        prev_tag = \'\'\n        chunk_tokens = []\n        entities = []\n        slots = []\n        for token, tag in zip(tokens, tags):\n            curent_tag = tag.split(\'-\')[-1].strip()\n            current_prefix = tag.split(\'-\')[0]\n            if tag.startswith(\'B-\'):\n                if len(chunk_tokens) > 0:\n                    entities.append(\' \'.join(chunk_tokens))\n                    slots.append(prev_tag)\n                    chunk_tokens = []\n                chunk_tokens.append(token)\n            if current_prefix == \'I\':\n                if curent_tag != prev_tag:\n                    if len(chunk_tokens) > 0:\n                        entities.append(\' \'.join(chunk_tokens))\n                        slots.append(prev_tag)\n                        chunk_tokens = []\n                else:\n                    chunk_tokens.append(token)\n            if current_prefix == \'O\':\n                if len(chunk_tokens) > 0:\n                    entities.append(\' \'.join(chunk_tokens))\n                    slots.append(prev_tag)\n                    chunk_tokens = []\n            prev_tag = curent_tag\n        if len(chunk_tokens) > 0:\n            entities.append(\' \'.join(chunk_tokens))\n            slots.append(prev_tag)\n        return entities, slots\n\n    def _download_slot_vals(self):\n        url = \'http://files.deeppavlov.ai/datasets/dstc_slot_vals.json\'\n        download(self.save_path, url)\n\n    def save(self, *args, **kwargs):\n        with open(self.save_path, \'w\', encoding=\'utf8\') as f:\n            json.dump(self._slot_vals, f)\n\n    def serialize(self):\n        return json.dumps(self._slot_vals)\n\n    def load(self, *args, **kwargs):\n        if not self.load_path.exists():\n            self._download_slot_vals()\n        with open(self.load_path, encoding=\'utf8\') as f:\n            self._slot_vals = json.load(f)\n\n    def deserialize(self, data):\n        self._slot_vals = json.loads(data)\n'"
deeppavlov/models/slotfill/slotfill_raw.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nfrom collections import defaultdict\nfrom logging import getLogger\nfrom math import exp\n\nfrom overrides import overrides\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.core.models.serializable import Serializable\n\nlog = getLogger(__name__)\n\n\n@register(\'slotfill_raw\')\nclass SlotFillingComponent(Component, Serializable):\n    """"""Slot filling using Fuzzy search""""""\n\n    def __init__(self, threshold: float = 0.7, return_all: bool = False, **kwargs):\n        super().__init__(**kwargs)\n        self.threshold = threshold\n        self.return_all = return_all\n        # self._slot_vals is the dictionary of slot values\n        self._slot_vals = None\n        self.load()\n\n    @overrides\n    def __call__(self, batch, *args, **kwargs):\n        slots = [{}] * len(batch)\n\n        m = [i for i, v in enumerate(batch) if v]\n        if m:\n            batch = [batch[i] for i in m]\n            # tags_batch = self._ner_network.predict_for_token_batch(batch)\n            # batch example: [[\'is\', \'there\', \'anything\', \'else\']]\n            for i, tokens in zip(m, batch):\n                # tokens are[\'is\', \'there\', \'anything\', \'else\']\n                slots_values_lists = self._predict_slots(tokens)\n                if self.return_all:\n                    slots[i] = dict(slots_values_lists)\n                else:\n                    slots[i] = {slot: val_list[0] for slot, val_list in slots_values_lists.items()}\n                # slots[i] example {\'food\': \'steakhouse\'}\n        # slots we want, example : [{\'pricerange\': \'moderate\', \'area\': \'south\'}]\n        return slots\n\n    def _predict_slots(self, tokens):\n        # For utterance extract named entities and perform normalization for slot filling\n        entities, slots = self._fuzzy_finder(self._slot_vals, tokens)\n        slot_values = defaultdict(list)\n        for entity, slot in zip(entities, slots):\n            slot_values[slot].append(entity)\n        return slot_values\n\n    def load(self, *args, **kwargs):\n        with open(self.load_path, encoding=\'utf8\') as f:\n            self._slot_vals = json.load(f)\n\n    def deserialize(self, data):\n        self._slot_vals = json.loads(data)\n\n    def save(self):\n        with open(self.save_path, \'w\', encoding=\'utf8\') as f:\n            json.dump(self._slot_vals, f)\n\n    def serialize(self):\n        return json.dumps(self._slot_vals)\n\n    def _fuzzy_finder(self, slot_dict, tokens):\n        global input_entity\n        if isinstance(tokens, list):\n            input_entity = \' \'.join(tokens)\n        entities = []\n        slots = []\n        for slot, tag_dict in slot_dict.items():\n            candidates = self.get_candidate(input_entity, tag_dict, self.get_ratio)\n            for candidate in candidates:\n                if candidate not in entities:\n                    entities.append(candidate)\n                    slots.append(slot)\n        return entities, slots\n\n    def get_candidate(self, input_text, tag_dict, score_function):\n        candidates = []\n        positions = []\n        for entity_name, entity_list in tag_dict.items():\n            for entity in entity_list:\n                ratio, j = score_function(entity.lower(), input_text.lower())\n                if ratio >= self.threshold:\n                    candidates.append(entity_name)\n                    positions.append(j)\n        if candidates:\n            _, candidates = list(zip(*sorted(zip(positions, candidates))))\n        return candidates\n\n    def get_ratio(self, needle, haystack):\n        d, j = self.fuzzy_substring_distance(needle, haystack)\n        m = len(needle) - d\n        return exp(-d / 5) * (m / len(needle)), j\n\n    @staticmethod\n    def fuzzy_substring_distance(needle, haystack):\n        """"""Calculates the fuzzy match of needle in haystack,\n        using a modified version of the Levenshtein distance\n        algorithm.\n        The function is modified from the Levenshtein function\n        in the bktree module by Adam Hupp\n        :type needle: string\n        :type haystack: string""""""\n        m, n = len(needle), len(haystack)\n\n        # base cases\n        if m == 1:\n            return needle not in haystack\n        if not n:\n            return m\n\n        row1 = [0] * (n + 1)\n        for j in range(0, n + 1):\n            if j == 0 or not haystack[j - 1].isalnum():\n                row1[j] = 0\n            else:\n                row1[j] = row1[j - 1] + 1\n\n        for i in range(0, m):\n            row2 = [i + 1]\n            for j in range(0, n):\n                cost = (needle[i] != haystack[j])\n                row2.append(min(row1[j + 1] + 1, row2[j] + 1, row1[j] + cost))\n            row1 = row2\n\n        d = n + m\n        j_min = 0\n        for j in range(0, n + 1):\n            if j == 0 or j == n or not haystack[j].isalnum():\n                if d > row1[j]:\n                    d = row1[j]\n                    j_min = j\n                # d = min(d, row1[j])\n        return d, j_min\n'"
deeppavlov/models/spelling_correction/__init__.py,0,b''
deeppavlov/models/squad/__init__.py,0,b''
deeppavlov/models/squad/squad.py,78,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import List, Tuple\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom deeppavlov.core.common.check_gpu import check_gpu_existence\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.layers.tf_layers import cudnn_bi_gru, variational_dropout\nfrom deeppavlov.core.models.tf_model import LRScheduledTFModel\nfrom deeppavlov.models.squad.utils import dot_attention, simple_attention, PtrNet, CudnnGRU, CudnnCompatibleGRU\n\nlogger = getLogger(__name__)\n\n\n@register(\'squad_model\')\nclass SquadModel(LRScheduledTFModel):\n    """"""\n    SquadModel predicts answer start and end position in given context by given question.\n\n    High level architecture:\n    Word embeddings -> Contextual embeddings -> Question-Context Attention -> Self-attention -> Pointer Network\n\n    If noans_token flag is True, then special noans_token is added to output of self-attention layer.\n    Pointer Network can select noans_token if there is no answer in given context.\n\n    Parameters:\n        word_emb: pretrained word embeddings\n        char_emb: pretrained char embeddings\n        context_limit: max context length in tokens\n        question_limit: max question length in tokens\n        char_limit: max number of characters in token\n        char_hidden_size: hidden size of charRNN\n        encoder_hidden_size: hidden size of encoder RNN\n        attention_hidden_size: size of projection layer in attention\n        keep_prob: dropout keep probability\n        min_learning_rate: minimal learning rate, is used in learning rate decay\n        noans_token: boolean, flags whether to use special no_ans token to make model able not to answer on question\n    """"""\n\n    def __init__(self, word_emb: np.ndarray, char_emb: np.ndarray, context_limit: int = 450, question_limit: int = 150,\n                 char_limit: int = 16, train_char_emb: bool = True, char_hidden_size: int = 100,\n                 encoder_hidden_size: int = 75, attention_hidden_size: int = 75, keep_prob: float = 0.7,\n                 min_learning_rate: float = 0.001, noans_token: bool = False, **kwargs) -> None:\n        super().__init__(**kwargs)\n\n        self.init_word_emb = word_emb\n        self.init_char_emb = char_emb\n        self.context_limit = context_limit\n        self.question_limit = question_limit\n        self.char_limit = char_limit\n        self.train_char_emb = train_char_emb\n        self.char_hidden_size = char_hidden_size\n        self.hidden_size = encoder_hidden_size\n        self.attention_hidden_size = attention_hidden_size\n        self.keep_prob = keep_prob\n        self.min_learning_rate = min_learning_rate\n        self.noans_token = noans_token\n\n        self.word_emb_dim = self.init_word_emb.shape[1]\n        self.char_emb_dim = self.init_char_emb.shape[1]\n\n        self.last_impatience = 0\n        self.lr_impatience = 0\n\n        if check_gpu_existence():\n            self.GRU = CudnnGRU\n        else:\n            self.GRU = CudnnCompatibleGRU\n\n        self.sess_config = tf.ConfigProto(allow_soft_placement=True)\n        self.sess_config.gpu_options.allow_growth = True\n        self.sess = tf.Session(config=self.sess_config)\n\n        self._init_graph()\n\n        self._init_optimizer()\n\n        self.sess.run(tf.global_variables_initializer())\n\n        # Try to load the model (if there are some model files the model will be loaded from them)\n        if self.load_path is not None:\n            self.load()\n\n    def _init_graph(self):\n        self._init_placeholders()\n\n        self.word_emb = tf.get_variable(""word_emb"", initializer=tf.constant(self.init_word_emb, dtype=tf.float32),\n                                        trainable=False)\n        self.char_emb = tf.get_variable(""char_emb"", initializer=tf.constant(self.init_char_emb, dtype=tf.float32),\n                                        trainable=self.train_char_emb)\n\n        self.c_mask = tf.cast(self.c_ph, tf.bool)\n        self.q_mask = tf.cast(self.q_ph, tf.bool)\n        self.c_len = tf.reduce_sum(tf.cast(self.c_mask, tf.int32), axis=1)\n        self.q_len = tf.reduce_sum(tf.cast(self.q_mask, tf.int32), axis=1)\n\n        bs = tf.shape(self.c_ph)[0]\n        self.c_maxlen = tf.reduce_max(self.c_len)\n        self.q_maxlen = tf.reduce_max(self.q_len)\n        self.c = tf.slice(self.c_ph, [0, 0], [bs, self.c_maxlen])\n        self.q = tf.slice(self.q_ph, [0, 0], [bs, self.q_maxlen])\n        self.c_mask = tf.slice(self.c_mask, [0, 0], [bs, self.c_maxlen])\n        self.q_mask = tf.slice(self.q_mask, [0, 0], [bs, self.q_maxlen])\n        self.cc = tf.slice(self.cc_ph, [0, 0, 0], [bs, self.c_maxlen, self.char_limit])\n        self.qc = tf.slice(self.qc_ph, [0, 0, 0], [bs, self.q_maxlen, self.char_limit])\n        self.cc_len = tf.reshape(tf.reduce_sum(tf.cast(tf.cast(self.cc, tf.bool), tf.int32), axis=2), [-1])\n        self.qc_len = tf.reshape(tf.reduce_sum(tf.cast(tf.cast(self.qc, tf.bool), tf.int32), axis=2), [-1])\n        # to remove char sequences with len equal zero (padded tokens)\n        self.cc_len = tf.maximum(tf.ones_like(self.cc_len), self.cc_len)\n        self.qc_len = tf.maximum(tf.ones_like(self.qc_len), self.qc_len)\n        self.y1 = tf.one_hot(self.y1_ph, depth=self.context_limit)\n        self.y2 = tf.one_hot(self.y2_ph, depth=self.context_limit)\n        self.y1 = tf.slice(self.y1, [0, 0], [bs, self.c_maxlen])\n        self.y2 = tf.slice(self.y2, [0, 0], [bs, self.c_maxlen])\n\n        if self.noans_token:\n            # we use additional \'no answer\' token to allow model not to answer on question\n            # later we will add \'no answer\' token as first token in context question-aware representation\n            self.y1 = tf.one_hot(self.y1_ph, depth=self.context_limit + 1)\n            self.y2 = tf.one_hot(self.y2_ph, depth=self.context_limit + 1)\n            self.y1 = tf.slice(self.y1, [0, 0], [bs, self.c_maxlen + 1])\n            self.y2 = tf.slice(self.y2, [0, 0], [bs, self.c_maxlen + 1])\n\n        with tf.variable_scope(""emb""):\n            with tf.variable_scope(""char""):\n                cc_emb = tf.reshape(tf.nn.embedding_lookup(self.char_emb, self.cc),\n                                    [bs * self.c_maxlen, self.char_limit, self.char_emb_dim])\n                qc_emb = tf.reshape(tf.nn.embedding_lookup(self.char_emb, self.qc),\n                                    [bs * self.q_maxlen, self.char_limit, self.char_emb_dim])\n\n                cc_emb = variational_dropout(cc_emb, keep_prob=self.keep_prob_ph)\n                qc_emb = variational_dropout(qc_emb, keep_prob=self.keep_prob_ph)\n\n                _, (state_fw, state_bw) = cudnn_bi_gru(cc_emb, self.char_hidden_size, seq_lengths=self.cc_len,\n                                                       trainable_initial_states=True)\n                cc_emb = tf.concat([state_fw, state_bw], axis=1)\n\n                _, (state_fw, state_bw) = cudnn_bi_gru(qc_emb, self.char_hidden_size, seq_lengths=self.qc_len,\n                                                       trainable_initial_states=True,\n                                                       reuse=True)\n                qc_emb = tf.concat([state_fw, state_bw], axis=1)\n\n                cc_emb = tf.reshape(cc_emb, [bs, self.c_maxlen, 2 * self.char_hidden_size])\n                qc_emb = tf.reshape(qc_emb, [bs, self.q_maxlen, 2 * self.char_hidden_size])\n\n            with tf.name_scope(""word""):\n                c_emb = tf.nn.embedding_lookup(self.word_emb, self.c)\n                q_emb = tf.nn.embedding_lookup(self.word_emb, self.q)\n\n            c_emb = tf.concat([c_emb, cc_emb], axis=2)\n            q_emb = tf.concat([q_emb, qc_emb], axis=2)\n\n        with tf.variable_scope(""encoding""):\n            rnn = self.GRU(num_layers=3, num_units=self.hidden_size, batch_size=bs,\n                           input_size=c_emb.get_shape().as_list()[-1],\n                           keep_prob=self.keep_prob_ph)\n            c = rnn(c_emb, seq_len=self.c_len)\n            q = rnn(q_emb, seq_len=self.q_len)\n\n        with tf.variable_scope(""attention""):\n            qc_att = dot_attention(c, q, mask=self.q_mask, att_size=self.attention_hidden_size,\n                                   keep_prob=self.keep_prob_ph)\n            rnn = self.GRU(num_layers=1, num_units=self.hidden_size, batch_size=bs,\n                           input_size=qc_att.get_shape().as_list()[-1], keep_prob=self.keep_prob_ph)\n            att = rnn(qc_att, seq_len=self.c_len)\n\n        with tf.variable_scope(""match""):\n            self_att = dot_attention(att, att, mask=self.c_mask, att_size=self.attention_hidden_size,\n                                     keep_prob=self.keep_prob_ph)\n            rnn = self.GRU(num_layers=1, num_units=self.hidden_size, batch_size=bs,\n                           input_size=self_att.get_shape().as_list()[-1], keep_prob=self.keep_prob_ph)\n            match = rnn(self_att, seq_len=self.c_len)\n\n        with tf.variable_scope(""pointer""):\n            init = simple_attention(q, self.hidden_size, mask=self.q_mask, keep_prob=self.keep_prob_ph)\n            pointer = PtrNet(cell_size=init.get_shape().as_list()[-1], keep_prob=self.keep_prob_ph)\n            if self.noans_token:\n                noans_token = tf.Variable(tf.random_uniform((match.get_shape().as_list()[-1],), -0.1, 0.1), tf.float32)\n                noans_token = tf.nn.dropout(noans_token, keep_prob=self.keep_prob_ph)\n                noans_token = tf.expand_dims(tf.tile(tf.expand_dims(noans_token, axis=0), [bs, 1]), axis=1)\n                match = tf.concat([noans_token, match], axis=1)\n                self.c_mask = tf.concat([tf.ones(shape=(bs, 1), dtype=tf.bool), self.c_mask], axis=1)\n            logits1, logits2 = pointer(init, match, self.hidden_size, self.c_mask)\n\n        with tf.variable_scope(""predict""):\n            max_ans_length = tf.cast(tf.minimum(15, self.c_maxlen), tf.int64)\n            outer_logits = tf.exp(tf.expand_dims(logits1, axis=2) + tf.expand_dims(logits2, axis=1))\n            outer_logits = tf.matrix_band_part(outer_logits, 0, max_ans_length)\n            outer = tf.matmul(tf.expand_dims(tf.nn.softmax(logits1), axis=2),\n                              tf.expand_dims(tf.nn.softmax(logits2), axis=1))\n            outer = tf.matrix_band_part(outer, 0, max_ans_length)\n            self.yp1 = tf.argmax(tf.reduce_max(outer, axis=2), axis=1)\n            self.yp2 = tf.argmax(tf.reduce_max(outer, axis=1), axis=1)\n            self.yp_logits = tf.reduce_max(tf.reduce_max(outer_logits, axis=2), axis=1)\n            if self.noans_token:\n                self.yp_score = 1 - tf.nn.softmax(logits1)[:, 0] * tf.nn.softmax(logits2)[:, 0]\n            loss_1 = tf.nn.softmax_cross_entropy_with_logits(logits=logits1, labels=self.y1)\n            loss_2 = tf.nn.softmax_cross_entropy_with_logits(logits=logits2, labels=self.y2)\n            self.loss = tf.reduce_mean(loss_1 + loss_2)\n\n    def _init_placeholders(self):\n        self.c_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name=\'c_ph\')\n        self.cc_ph = tf.placeholder(shape=(None, None, self.char_limit), dtype=tf.int32, name=\'cc_ph\')\n        self.q_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name=\'q_ph\')\n        self.qc_ph = tf.placeholder(shape=(None, None, self.char_limit), dtype=tf.int32, name=\'qc_ph\')\n        self.y1_ph = tf.placeholder(shape=(None,), dtype=tf.int32, name=\'y1_ph\')\n        self.y2_ph = tf.placeholder(shape=(None,), dtype=tf.int32, name=\'y2_ph\')\n\n        self.lear_rate_ph = tf.placeholder_with_default(0.0, shape=[], name=\'learning_rate\')\n        self.keep_prob_ph = tf.placeholder_with_default(1.0, shape=[], name=\'keep_prob_ph\')\n        self.is_train_ph = tf.placeholder_with_default(False, shape=[], name=\'is_train_ph\')\n\n    def _init_optimizer(self):\n        with tf.variable_scope(\'Optimizer\'):\n            self.global_step = tf.get_variable(\'global_step\', shape=[], dtype=tf.int32,\n                                               initializer=tf.constant_initializer(0), trainable=False)\n            self.train_op = self.get_train_op(self.loss, learning_rate=self.lear_rate_ph)\n\n    def _build_feed_dict(self, c_tokens, c_chars, q_tokens, q_chars, y1=None, y2=None):\n        feed_dict = {\n            self.c_ph: c_tokens,\n            self.cc_ph: c_chars,\n            self.q_ph: q_tokens,\n            self.qc_ph: q_chars,\n        }\n        if y1 is not None and y2 is not None:\n            feed_dict.update({\n                self.y1_ph: y1,\n                self.y2_ph: y2,\n                self.lear_rate_ph: max(self.get_learning_rate(), self.min_learning_rate),\n                self.keep_prob_ph: self.keep_prob,\n                self.is_train_ph: True,\n            })\n\n        return feed_dict\n\n    def train_on_batch(self, c_tokens: np.ndarray, c_chars: np.ndarray, q_tokens: np.ndarray, q_chars: np.ndarray,\n                       y1s: Tuple[List[int], ...], y2s: Tuple[List[int], ...]) -> float:\n        """"""\n        This method is called by trainer to make one training step on one batch.\n\n        Args:\n            c_tokens: batch of tokenized contexts\n            c_chars: batch of tokenized contexts, each token split on chars\n            q_tokens: batch of tokenized questions\n            q_chars: batch of tokenized questions, each token split on chars\n            y1s: batch of ground truth answer start positions\n            y2s: batch of ground truth answer end positions\n\n        Returns:\n            value of loss function on batch\n        """"""\n        # TODO: filter examples in batches with answer position greater self.context_limit\n        # select one answer from list of correct answers\n        y1s = np.array([x[0] for x in y1s])\n        y2s = np.array([x[0] for x in y2s])\n        if self.noans_token:\n            noans_mask = ((y1s != -1) * (y2s != -1))\n            y1s = (y1s + 1) * noans_mask\n            y2s = (y2s + 1) * noans_mask\n\n        feed_dict = self._build_feed_dict(c_tokens, c_chars, q_tokens, q_chars, y1s, y2s)\n        loss, _, lear_rate = self.sess.run([self.loss, self.train_op, self.lear_rate_ph],\n                                           feed_dict=feed_dict)\n        report = {\'loss\': loss, \'learning_rate\': float(lear_rate), \'momentum\': self.get_momentum()}\n        return report\n\n    def __call__(self, c_tokens: np.ndarray, c_chars: np.ndarray, q_tokens: np.ndarray, q_chars: np.ndarray,\n                 *args, **kwargs) -> Tuple[np.ndarray, np.ndarray, List[float]]:\n        """"""\n        Predicts answer start and end positions by given context and question.\n\n        Args:\n            c_tokens: batch of tokenized contexts\n            c_chars: batch of tokenized contexts, each token split on chars\n            q_tokens: batch of tokenized questions\n            q_chars: batch of tokenized questions, each token split on chars\n\n        Returns:\n            answer_start, answer_end positions, answer logits which represent models confidence\n        """"""\n        if any(np.sum(c_tokens, axis=-1) == 0) or any(np.sum(q_tokens, axis=-1) == 0):\n            logger.info(\'SQuAD model: Warning! Empty question or context was found.\')\n            noanswers = -np.ones(shape=(c_tokens.shape[0]), dtype=np.int32)\n            zero_probs = np.zeros(shape=(c_tokens.shape[0]), dtype=np.float32)\n            if self.noans_token:\n                return noanswers, noanswers, zero_probs, zero_probs\n            return noanswers, noanswers, zero_probs\n\n        feed_dict = self._build_feed_dict(c_tokens, c_chars, q_tokens, q_chars)\n\n        if self.noans_token:\n            yp1, yp2, logits, score = self.sess.run([self.yp1, self.yp2, self.yp_logits, self.yp_score],\n                                                    feed_dict=feed_dict)\n            noans_mask = (yp1 * yp2).astype(bool)\n            yp1 = yp1 * noans_mask - 1\n            yp2 = yp2 * noans_mask - 1\n            return yp1, yp2, logits.tolist(), score.tolist()\n\n        yp1, yp2, logits = self.sess.run([self.yp1, self.yp2, self.yp_logits], feed_dict=feed_dict)\n        return yp1, yp2, logits.tolist()\n\n    def process_event(self, event_name: str, data) -> None:\n        """"""\n        Processes events sent by trainer. Implements learning rate decay.\n\n        Args:\n            event_name: event_name sent by trainer\n            data: number of examples, epochs, metrics sent by trainer\n        """"""\n        super().process_event(event_name, data)\n'"
deeppavlov/models/squad/utils.py,78,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport tensorflow as tf\n\n\nclass CudnnGRU:\n    def __init__(self, num_layers, num_units, batch_size, input_size, keep_prob=1.0):\n        self.num_layers = num_layers\n        self.grus = []\n        self.inits = []\n        self.dropout_mask = []\n        for layer in range(num_layers):\n            input_size_ = input_size if layer == 0 else 2 * num_units\n            gru_fw = tf.contrib.cudnn_rnn.CudnnGRU(num_layers=1, num_units=num_units)\n            gru_bw = tf.contrib.cudnn_rnn.CudnnGRU(num_layers=1, num_units=num_units)\n\n            init_fw = tf.Variable(tf.zeros([num_units]))\n            init_fw = tf.expand_dims(tf.tile(tf.expand_dims(init_fw, axis=0), [batch_size, 1]), axis=0)\n            init_bw = tf.Variable(tf.zeros([num_units]))\n            init_bw = tf.expand_dims(tf.tile(tf.expand_dims(init_bw, axis=0), [batch_size, 1]), axis=0)\n\n            mask_fw = tf.nn.dropout(tf.ones([1, batch_size, input_size_], dtype=tf.float32),\n                                    keep_prob=keep_prob)\n            mask_bw = tf.nn.dropout(tf.ones([1, batch_size, input_size_], dtype=tf.float32),\n                                    keep_prob=keep_prob)\n\n            self.grus.append((gru_fw, gru_bw,))\n            self.inits.append((init_fw, init_bw,))\n            self.dropout_mask.append((mask_fw, mask_bw,))\n\n    def __call__(self, inputs, seq_len, keep_prob=1.0, is_train=None, concat_layers=True):\n        outputs = [tf.transpose(inputs, [1, 0, 2])]\n        for layer in range(self.num_layers):\n            gru_fw, gru_bw = self.grus[layer]\n            init_fw, init_bw = self.inits[layer]\n            mask_fw, mask_bw = self.dropout_mask[layer]\n            with tf.variable_scope(\'fw_{}\'.format(layer), reuse=tf.AUTO_REUSE):\n                out_fw, _ = gru_fw(outputs[-1] * mask_fw, (init_fw,))\n            with tf.variable_scope(\'bw_{}\'.format(layer), reuse=tf.AUTO_REUSE):\n                inputs_bw = tf.reverse_sequence(\n                    outputs[-1] * mask_bw, seq_lengths=seq_len, seq_dim=0, batch_dim=1)\n                out_bw, _ = gru_bw(inputs_bw, (init_bw,))\n                out_bw = tf.reverse_sequence(\n                    out_bw, seq_lengths=seq_len, seq_dim=0, batch_dim=1)\n            outputs.append(tf.concat([out_fw, out_bw], axis=2))\n        if concat_layers:\n            res = tf.concat(outputs[1:], axis=2)\n        else:\n            res = outputs[-1]\n        res = tf.transpose(res, [1, 0, 2])\n        return res\n\n\nclass CudnnCompatibleGRU:\n    def __init__(self, num_layers, num_units, batch_size, input_size, keep_prob=1.0):\n        self.num_layers = num_layers\n        self.grus = []\n        self.inits = []\n        self.dropout_mask = []\n        for layer in range(num_layers):\n            input_size_ = input_size if layer == 0 else 2 * num_units\n            gru_fw = tf.nn.rnn_cell.MultiRNNCell([\n                tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell(num_units=num_units)])\n\n            gru_bw = tf.nn.rnn_cell.MultiRNNCell([\n                tf.contrib.cudnn_rnn.CudnnCompatibleGRUCell(num_units=num_units)])\n\n            init_fw = tf.Variable(tf.zeros([num_units]))\n            init_fw = tf.expand_dims(tf.tile(tf.expand_dims(init_fw, axis=0), [batch_size, 1]), axis=0)\n            init_bw = tf.Variable(tf.zeros([num_units]))\n            init_bw = tf.expand_dims(tf.tile(tf.expand_dims(init_bw, axis=0), [batch_size, 1]), axis=0)\n\n            mask_fw = tf.nn.dropout(tf.ones([1, batch_size, input_size_], dtype=tf.float32),\n                                    keep_prob=keep_prob)\n            mask_bw = tf.nn.dropout(tf.ones([1, batch_size, input_size_], dtype=tf.float32),\n                                    keep_prob=keep_prob)\n\n            self.grus.append((gru_fw, gru_bw,))\n            self.inits.append((init_fw, init_bw,))\n            self.dropout_mask.append((mask_fw, mask_bw,))\n\n    def __call__(self, inputs, seq_len, keep_prob=1.0, is_train=None, concat_layers=True):\n        outputs = [tf.transpose(inputs, [1, 0, 2])]\n        for layer in range(self.num_layers):\n            gru_fw, gru_bw = self.grus[layer]\n            init_fw, init_bw = self.inits[layer]\n            mask_fw, mask_bw = self.dropout_mask[layer]\n            with tf.variable_scope(\'fw_{}\'.format(layer), reuse=tf.AUTO_REUSE):\n                with tf.variable_scope(\'cudnn_gru\', reuse=tf.AUTO_REUSE):\n                    out_fw, _ = tf.nn.dynamic_rnn(cell=gru_fw, inputs=outputs[-1] * mask_fw, time_major=True,\n                                                  initial_state=tuple(tf.unstack(init_fw, axis=0)))\n\n            with tf.variable_scope(\'bw_{}\'.format(layer), reuse=tf.AUTO_REUSE):\n                with tf.variable_scope(\'cudnn_gru\', reuse=tf.AUTO_REUSE):\n                    inputs_bw = tf.reverse_sequence(\n                        outputs[-1] * mask_bw, seq_lengths=seq_len, seq_dim=0, batch_dim=1)\n                    out_bw, _ = tf.nn.dynamic_rnn(cell=gru_bw, inputs=inputs_bw, time_major=True,\n                                                  initial_state=tuple(tf.unstack(init_bw, axis=0)))\n                    out_bw = tf.reverse_sequence(\n                        out_bw, seq_lengths=seq_len, seq_dim=0, batch_dim=1)\n\n            outputs.append(tf.concat([out_fw, out_bw], axis=2))\n        if concat_layers:\n            res = tf.concat(outputs[1:], axis=2)\n        else:\n            res = outputs[-1]\n        res = tf.transpose(res, [1, 0, 2])\n        return res\n\n\nclass PtrNet:\n    def __init__(self, cell_size, keep_prob=1.0, scope=""ptr_net""):\n        self.gru = tf.nn.rnn_cell.GRUCell(cell_size)\n        self.scope = scope\n        self.keep_prob = keep_prob\n\n    def __call__(self, init, match, hidden_size, mask):\n        with tf.variable_scope(self.scope):\n            BS, ML, MH = tf.unstack(tf.shape(match))\n            BS, IH = tf.unstack(tf.shape(init))\n            match_do = tf.nn.dropout(match, keep_prob=self.keep_prob, noise_shape=[BS, 1, MH])\n            dropout_mask = tf.nn.dropout(tf.ones([BS, IH], dtype=tf.float32), keep_prob=self.keep_prob)\n            inp, logits1 = attention(match_do, init * dropout_mask, hidden_size, mask)\n            inp_do = tf.nn.dropout(inp, keep_prob=self.keep_prob)\n            _, state = self.gru(inp_do, init)\n            tf.get_variable_scope().reuse_variables()\n            _, logits2 = attention(match_do, state * dropout_mask, hidden_size, mask)\n            return logits1, logits2\n\n\ndef dot_attention(inputs, memory, mask, att_size, keep_prob=1.0, scope=""dot_attention""):\n    """"""Computes attention vector for each item in inputs:\n       attention vector is a weighted sum of memory items.\n       Dot product between input and memory vector is used as similarity measure.\n\n       Gate mechanism is applied to attention vectors to produce output.\n\n    Args:\n        inputs: Tensor [batch_size x input_len x feature_size]\n        memory: Tensor [batch_size x memory_len x feature_size]\n        mask: inputs mask\n        att_size: hidden size of attention\n        keep_prob: dropout keep_prob\n        scope:\n\n    Returns:\n        attention vectors [batch_size x input_len x (feature_size + feature_size)]\n\n    """"""\n    with tf.variable_scope(scope):\n        BS, IL, IH = tf.unstack(tf.shape(inputs))\n        BS, ML, MH = tf.unstack(tf.shape(memory))\n\n        d_inputs = tf.nn.dropout(inputs, keep_prob=keep_prob, noise_shape=[BS, 1, IH])\n        d_memory = tf.nn.dropout(memory, keep_prob=keep_prob, noise_shape=[BS, 1, MH])\n\n        with tf.variable_scope(""attention""):\n            inputs_att = tf.layers.dense(d_inputs, att_size, use_bias=False, activation=tf.nn.relu)\n            memory_att = tf.layers.dense(d_memory, att_size, use_bias=False, activation=tf.nn.relu)\n            logits = tf.matmul(inputs_att, tf.transpose(memory_att, [0, 2, 1])) / (att_size ** 0.5)\n            mask = tf.tile(tf.expand_dims(mask, axis=1), [1, IL, 1])\n            att_weights = tf.nn.softmax(softmax_mask(logits, mask))\n            outputs = tf.matmul(att_weights, memory)\n            res = tf.concat([inputs, outputs], axis=2)\n\n        with tf.variable_scope(""gate""):\n            dim = res.get_shape().as_list()[-1]\n            d_res = tf.nn.dropout(res, keep_prob=keep_prob, noise_shape=[BS, 1, IH + MH])\n            gate = tf.layers.dense(d_res, dim, use_bias=False, activation=tf.nn.sigmoid)\n            return res * gate\n\n\ndef simple_attention(memory, att_size, mask, keep_prob=1.0, scope=""simple_attention""):\n    """"""Simple attention without any conditions.\n\n       Computes weighted sum of memory elements.\n    """"""\n    with tf.variable_scope(scope):\n        BS, ML, MH = tf.unstack(tf.shape(memory))\n        memory_do = tf.nn.dropout(memory, keep_prob=keep_prob, noise_shape=[BS, 1, MH])\n        logits = tf.layers.dense(tf.layers.dense(memory_do, att_size, activation=tf.nn.tanh), 1, use_bias=False)\n        logits = softmax_mask(tf.squeeze(logits, [2]), mask)\n        att_weights = tf.expand_dims(tf.nn.softmax(logits), axis=2)\n        res = tf.reduce_sum(att_weights * memory, axis=1)\n        return res\n\n\ndef attention(inputs, state, att_size, mask, scope=""attention""):\n    """"""Computes weighted sum of inputs conditioned on state""""""\n    with tf.variable_scope(scope):\n        u = tf.concat([tf.tile(tf.expand_dims(state, axis=1), [1, tf.shape(inputs)[1], 1]), inputs], axis=2)\n        logits = tf.layers.dense(tf.layers.dense(u, att_size, activation=tf.nn.tanh), 1, use_bias=False)\n        logits = softmax_mask(tf.squeeze(logits, [2]), mask)\n        att_weights = tf.expand_dims(tf.nn.softmax(logits), axis=2)\n        res = tf.reduce_sum(att_weights * inputs, axis=1)\n        return res, logits\n\n\ndef softmax_mask(val, mask):\n    INF = 1e30\n    return -INF * (1 - tf.cast(mask, tf.float32)) + val\n'"
deeppavlov/models/syntax_parser/__init__.py,0,b''
deeppavlov/models/syntax_parser/joint.py,0,"b'from typing import Union, List\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.core.common.chainer import Chainer\n\nfrom deeppavlov.models.morpho_tagger.common import TagOutputPrettifier,\\\n    LemmatizedOutputPrettifier, DependencyOutputPrettifier\n\n\nUD_COLUMN_FEAT_MAPPING = {""id"": 0, ""word"": 1, ""lemma"": 2, ""upos"": 3, ""feats"": 5, ""head"": 6, ""deprel"": 7}\n\n\n@register(""joint_tagger_parser"")\nclass JointTaggerParser(Component):\n    """"""\n    A class to perform joint morphological and syntactic parsing.\n    It is just a wrapper that calls the models for tagging and parsing\n    and comprises their results in a single output.\n\n    Args:\n        tagger: the morphological tagger model (a :class:`~deeppavlov.core.common.chainer.Chainer` instance)\n        parser_path: the syntactic parser model (a :class:`~deeppavlov.core.common.chainer.Chainer` instance)\n        output_format: the output format, it may be either `ud` (alias: `conllu`) or `json`.\n        to_output_string: whether to convert the output to a list of strings\n\n    Attributes:\n        tagger: a morphological tagger model (a :class:`~deeppavlov.core.common.chainer.Chainer` instance)\n        parser: a syntactic parser model (a :class:`~deeppavlov.core.common.chainer.Chainer` instance)\n\n    """"""\n\n    def __init__(self, tagger: Chainer, parser: Chainer,\n                 output_format: str = ""ud"", to_output_string: bool = False,\n                 *args, **kwargs):\n        if output_format not in [""ud"", ""conllu"", ""json"", ""dict""]:\n            UserWarning(""JointTaggerParser output_format can be only `ud`, `conllu` or `json`. ""\\\n                        ""Unknown format: {}, setting the output_format to `ud`."".format(output_format))\n            output_format = ""ud""\n        self.output_format = output_format\n        self.to_output_string = to_output_string\n        self.tagger = tagger\n        self.parser = parser\n        self._check_models()\n\n    def _check_models(self):\n        tagger_prettifier = self.tagger[-1]\n        if not isinstance(tagger_prettifier, (TagOutputPrettifier, LemmatizedOutputPrettifier)):\n            raise ValueError(""The tagger should output prettified data: last component of the config ""\n                             ""should be either a TagOutputPrettifier or a LemmatizedOutputPrettifier ""\n                             ""instance."")\n        if isinstance(tagger_prettifier, TagOutputPrettifier):\n            tagger_prettifier.set_format_mode(""ud"")\n        tagger_prettifier.return_string = False\n        parser_prettifier = self.parser[-1]\n        if not isinstance(parser_prettifier, DependencyOutputPrettifier):\n            raise ValueError(""The tagger should output prettified data: last component of the config ""\n                             ""should be either a DependencyOutputPrettifier instance."")\n        parser_prettifier.return_string = False\n\n    def __call__(self, data: Union[List[str], List[List[str]]])\\\n            -> Union[List[List[dict]], List[str], List[List[str]]]:\n        r""""""Parses a batch of sentences.\n\n        Args:\n            data: either a batch of tokenized sentences, or a batch of raw sentences\n\n        Returns:\n            `answer`, a batch of parsed sentences. A sentence parse is a list of single word parses.\n            Each word parse is either a CoNLL-U-formatted string or a dictionary.\n            A sentence parse is returned either as is if ``self.to_output_string`` is ``False``,\n            or as a single string, where each word parse begins with a new string.\n\n        .. code-block:: python\n\n            >>> from deeppavlov.core.commands.infer import build_model\n            >>> model = build_model(""ru_syntagrus_joint_parsing"")\n            >>> batch = [""\xd0\x94\xd0\xb5\xd0\xb2\xd1\x83\xd1\x88\xd0\xba\xd0\xb0 \xd0\xbf\xd0\xb5\xd0\xbb\xd0\xb0 \xd0\xb2 \xd1\x86\xd0\xb5\xd1\x80\xd0\xba\xd0\xbe\xd0\xb2\xd0\xbd\xd0\xbe\xd0\xbc \xd1\x85\xd0\xbe\xd1\x80\xd0\xb5."", ""\xd0\xa3 \xd1\x8d\xd1\x82\xd0\xbe\xd0\xb9 \xd0\xb7\xd0\xb0\xd0\xb4\xd0\xb0\xd1\x87\xd0\xb8 \xd0\xb5\xd1\x81\xd1\x82\xd1\x8c \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb6\xd0\xbd\xd0\xbe\xd0\xb5 \xd1\x80\xd0\xb5\xd1\x88\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5.""]\n            >>> print(*model(batch), sep=""\\\\n\\\\n"")\n                1\t\xd0\x94\xd0\xb5\xd0\xb2\xd1\x83\xd1\x88\xd0\xba\xd0\xb0\t\xd0\xb4\xd0\xb5\xd0\xb2\xd1\x83\xd1\x88\xd0\xba\xd0\xb0\tNOUN\t_\tAnimacy=Anim|Case=Nom|Gender=Fem|Number=Sing\t2\tnsubj\t_\t_\n                2\t\xd0\xbf\xd0\xb5\xd0\xbb\xd0\xb0\t\xd0\xbf\xd0\xb5\xd1\x82\xd1\x8c\tVERB\t_\tAspect=Imp|Gender=Fem|Mood=Ind|Number=Sing|Tense=Past|VerbForm=Fin|Voice=Act\t0\troot\t_\t_\n                3\t\xd0\xb2\t\xd0\xb2\tADP\t_\t_\t5\tcase\t_\t_\n                4\t\xd1\x86\xd0\xb5\xd1\x80\xd0\xba\xd0\xbe\xd0\xb2\xd0\xbd\xd0\xbe\xd0\xbc\t\xd1\x86\xd0\xb5\xd1\x80\xd0\xba\xd0\xbe\xd0\xb2\xd0\xbd\xd1\x8b\xd0\xb9\tADJ\t_\tCase=Loc|Degree=Pos|Gender=Masc|Number=Sing\t5\tamod\t_\t_\n                5\t\xd1\x85\xd0\xbe\xd1\x80\xd0\xb5\t\xd1\x85\xd0\xbe\xd1\x80\tNOUN\t_\tAnimacy=Inan|Case=Loc|Gender=Masc|Number=Sing\t2\tobl\t_\t_\n                6\t.\t.\tPUNCT\t_\t_\t2\tpunct\t_\t_\n\n                1\t\xd0\xa3\t\xd1\x83\tADP\t_\t_\t3\tcase\t_\t_\n                2\t\xd1\x8d\xd1\x82\xd0\xbe\xd0\xb9\t\xd1\x8d\xd1\x82\xd0\xbe\xd1\x82\tDET\t_\tCase=Gen|Gender=Fem|Number=Sing\t3\tdet\t_\t_\n                3\t\xd0\xb7\xd0\xb0\xd0\xb4\xd0\xb0\xd1\x87\xd0\xb8\t\xd0\xb7\xd0\xb0\xd0\xb4\xd0\xb0\xd1\x87\xd0\xb0\tNOUN\t_\tAnimacy=Inan|Case=Gen|Gender=Fem|Number=Sing\t4\tobl\t_\t_\n                4\t\xd0\xb5\xd1\x81\xd1\x82\xd1\x8c\t\xd0\xb1\xd1\x8b\xd1\x82\xd1\x8c\tVERB\t_\tAspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t0\troot\t_\t_\n                5\t\xd1\x81\xd0\xbb\xd0\xbe\xd0\xb6\xd0\xbd\xd0\xbe\xd0\xb5\t\xd1\x81\xd0\xbb\xd0\xbe\xd0\xb6\xd0\xbd\xd1\x8b\xd0\xb9\tADJ\t_\tCase=Nom|Degree=Pos|Gender=Neut|Number=Sing\t6\tamod\t_\t_\n                6\t\xd1\x80\xd0\xb5\xd1\x88\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5\t\xd1\x80\xd0\xb5\xd1\x88\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5\tNOUN\t_\tAnimacy=Inan|Case=Nom|Gender=Neut|Number=Sing\t4\tnsubj\t_\t_\n                7\t.\t.\tPUNCT\t_\t_\t4\tpunct\t_\t_\n\n            >>> # Dirty hacks to change model parameters in the code, you should do it in the configuration file.\n            >>> model[""main""].to_output_string = False\n            >>> model[""main""].output_format = ""json""\n            >>> for sent_parse in model(batch):\n            >>>     for word_parse in sent_parse:\n            >>>         print(word_parse)\n            >>>     print("""")\n                {\'id\': \'1\', \'word\': \'\xd0\x94\xd0\xb5\xd0\xb2\xd1\x83\xd1\x88\xd0\xba\xd0\xb0\', \'lemma\': \'\xd0\xb4\xd0\xb5\xd0\xb2\xd1\x83\xd1\x88\xd0\xba\xd0\xb0\', \'upos\': \'NOUN\', \'feats\': \'Animacy=Anim|Case=Nom|Gender=Fem|Number=Sing\', \'head\': \'2\', \'deprel\': \'nsubj\'}\n                {\'id\': \'2\', \'word\': \'\xd0\xbf\xd0\xb5\xd0\xbb\xd0\xb0\', \'lemma\': \'\xd0\xbf\xd0\xb5\xd1\x82\xd1\x8c\', \'upos\': \'VERB\', \'feats\': \'Aspect=Imp|Gender=Fem|Mood=Ind|Number=Sing|Tense=Past|VerbForm=Fin|Voice=Act\', \'head\':                   \'0\', \'deprel\': \'root\'}\n                {\'id\': \'3\', \'word\': \'\xd0\xb2\', \'lemma\': \'\xd0\xb2\', \'upos\': \'ADP\', \'feats\': \'_\', \'head\': \'5\', \'deprel\': \'case\'}\n                {\'id\': \'4\', \'word\': \'\xd1\x86\xd0\xb5\xd1\x80\xd0\xba\xd0\xbe\xd0\xb2\xd0\xbd\xd0\xbe\xd0\xbc\', \'lemma\': \'\xd1\x86\xd0\xb5\xd1\x80\xd0\xba\xd0\xbe\xd0\xb2\xd0\xbd\xd1\x8b\xd0\xb9\', \'upos\': \'ADJ\', \'feats\': \'Case=Loc|Degree=Pos|Gender=Masc|Number=Sing\', \'head\': \'5\', \'deprel\': \'amod\'}\n                {\'id\': \'5\', \'word\': \'\xd1\x85\xd0\xbe\xd1\x80\xd0\xb5\', \'lemma\': \'\xd1\x85\xd0\xbe\xd1\x80\', \'upos\': \'NOUN\', \'feats\': \'Animacy=Inan|Case=Loc|Gender=Masc|Number=Sing\', \'head\': \'2\', \'deprel\': \'obl\'}\n                {\'id\': \'6\', \'word\': \'.\', \'lemma\': \'.\', \'upos\': \'PUNCT\', \'feats\': \'_\', \'head\': \'2\', \'deprel\': \'punct\'}\n\n                {\'id\': \'1\', \'word\': \'\xd0\xa3\', \'lemma\': \'\xd1\x83\', \'upos\': \'ADP\', \'feats\': \'_\', \'head\': \'3\', \'deprel\': \'case\'}\n                {\'id\': \'2\', \'word\': \'\xd1\x8d\xd1\x82\xd0\xbe\xd0\xb9\', \'lemma\': \'\xd1\x8d\xd1\x82\xd0\xbe\xd1\x82\', \'upos\': \'DET\', \'feats\': \'Case=Gen|Gender=Fem|Number=Sing\', \'head\': \'3\', \'deprel\': \'det\'}\n                {\'id\': \'3\', \'word\': \'\xd0\xb7\xd0\xb0\xd0\xb4\xd0\xb0\xd1\x87\xd0\xb8\', \'lemma\': \'\xd0\xb7\xd0\xb0\xd0\xb4\xd0\xb0\xd1\x87\xd0\xb0\', \'upos\': \'NOUN\', \'feats\': \'Animacy=Inan|Case=Gen|Gender=Fem|Number=Sing\', \'head\': \'4\', \'deprel\': \'obl\'}\n                {\'id\': \'4\', \'word\': \'\xd0\xb5\xd1\x81\xd1\x82\xd1\x8c\', \'lemma\': \'\xd0\xb1\xd1\x8b\xd1\x82\xd1\x8c\', \'upos\': \'VERB\', \'feats\': \'Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\', \'head\': \'0\',                \'deprel\': \'root\'}\n                {\'id\': \'5\', \'word\': \'\xd1\x81\xd0\xbb\xd0\xbe\xd0\xb6\xd0\xbd\xd0\xbe\xd0\xb5\', \'lemma\': \'\xd1\x81\xd0\xbb\xd0\xbe\xd0\xb6\xd0\xbd\xd1\x8b\xd0\xb9\', \'upos\': \'ADJ\', \'feats\': \'Case=Nom|Degree=Pos|Gender=Neut|Number=Sing\', \'head\': \'6\', \'deprel\': \'amod\'}\n                {\'id\': \'6\', \'word\': \'\xd1\x80\xd0\xb5\xd1\x88\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5\', \'lemma\': \'\xd1\x80\xd0\xb5\xd1\x88\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5\', \'upos\': \'NOUN\', \'feats\': \'Animacy=Inan|Case=Nom|Gender=Neut|Number=Sing\', \'head\': \'4\', \'deprel\': \'nsubj\'}\n                {\'id\': \'7\', \'word\': \'.\', \'lemma\': \'.\', \'upos\': \'PUNCT\', \'feats\': \'_\', \'head\': \'4\', \'deprel\': \'punct\'}\n\n        """"""\n        tagger_output = self.tagger(data)\n        parser_output = self.parser(data)\n        answer = []\n        for i, (tagger_sent, parser_sent) in enumerate(zip(tagger_output, parser_output)):\n            curr_sent_answer = []\n            for j, curr_word_tagger_output in enumerate(tagger_sent):\n                curr_word_tagger_output = curr_word_tagger_output.split(""\\t"")\n                curr_word_parser_output = parser_sent[j].split(""\\t"")\n                curr_word_answer = curr_word_tagger_output[:]\n                # setting parser output\n                curr_word_answer[6:8] = curr_word_parser_output[6:8]\n                if self.output_format in [""json"", ""dict""]:\n                    curr_word_answer = {key: curr_word_answer[index]\n                                        for key, index in UD_COLUMN_FEAT_MAPPING.items()}\n                    if self.to_output_string:\n                        curr_word_answer = str(curr_word_answer)\n                elif self.to_output_string:\n                    curr_word_answer = ""\\t"".join(curr_word_answer)\n                curr_sent_answer.append(curr_word_answer)\n            if self.to_output_string:\n                curr_sent_answer = ""\\n"".join(str(x) for x in curr_sent_answer)\n            answer.append(curr_sent_answer)\n        return answer\n\n\n'"
deeppavlov/models/syntax_parser/network.py,61,"b'# Copyright 2019 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom logging import getLogger\nfrom typing import List, Union, Tuple\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as kb\nfrom tensorflow.contrib.layers import xavier_initializer\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.data.utils import zero_pad\nfrom deeppavlov.core.layers.tf_layers import bi_rnn\nfrom deeppavlov.models.bert.bert_sequence_tagger import BertSequenceNetwork, token_from_subtoken\n\nlog = getLogger(__name__)\n\n\ndef gather_indexes(A: tf.Tensor, B: tf.Tensor) -> tf.Tensor:\n    """"""\n    Args:\n        A: a tensor with data\n        B: an integer tensor with indexes\n\n    Returns:\n        `answer` a tensor such that ``answer[i, j] = A[i, B[i, j]]``.\n        In case `B` is one-dimensional, the output is ``answer[i] = A[i, B[i]]``\n\n    """"""\n    are_indexes_one_dim = (kb.ndim(B) == 1)\n    if are_indexes_one_dim:\n        B = tf.expand_dims(B, -1)\n    first_dim_indexes = tf.expand_dims(tf.range(tf.shape(B)[0]), -1)\n    first_dim_indexes = tf.tile(first_dim_indexes, [1, tf.shape(B)[1]])\n    indexes = tf.stack([first_dim_indexes, B], axis=-1)\n    answer = tf.gather_nd(A, indexes)\n    if are_indexes_one_dim:\n        answer = answer[:,0]\n    return answer\n\n\ndef biaffine_layer(deps: tf.Tensor, heads: tf.Tensor, deps_dim: int,\n                   heads_dim: int, output_dim: int, name: str = ""biaffine_layer"") -> tf.Tensor:\n    """"""Implements a biaffine layer from [Dozat, Manning, 2016].\n\n    Args:\n        deps: the 3D-tensor of dependency states,\n        heads: the 3D-tensor of head states,\n        deps_dim: the dimension of dependency states,\n        heads_dim: the dimension of head_states,\n        output_dim: the output dimension\n        name: the name of a layer\n\n    Returns:\n        `answer` the output 3D-tensor\n\n    """"""\n    input_shape = [kb.shape(deps)[i] for i in range(tf.keras.backend.ndim(deps))]\n    first_input = tf.reshape(deps, [-1, deps_dim])  # first_input.shape = (B*L, D1)\n    second_input = tf.reshape(heads, [-1, heads_dim])  # second_input.shape = (B*L, D2)\n    with tf.variable_scope(name):\n        kernel_shape = (deps_dim, heads_dim * output_dim)\n        kernel = tf.get_variable(\'kernel\', shape=kernel_shape, initializer=xavier_initializer())\n        first = tf.matmul(first_input, kernel)  # (B*L, D2*H)\n        first = tf.reshape(first, [-1, heads_dim, output_dim])  # (B*L, D2, H)\n        answer = kb.batch_dot(first, second_input, axes=[1, 1])  # (B*L, H)\n        first_bias = tf.get_variable(\'first_bias\', shape=(deps_dim, output_dim),\n                                     initializer=xavier_initializer())\n        answer += tf.matmul(first_input, first_bias)\n        second_bias = tf.get_variable(\'second_bias\', shape=(heads_dim, output_dim),\n                                      initializer=xavier_initializer())\n        answer += tf.matmul(second_input, second_bias)\n        label_bias = tf.get_variable(\'label_bias\', shape=(output_dim,),\n                                     initializer=xavier_initializer())\n        answer = kb.bias_add(answer, label_bias)\n        answer = tf.reshape(answer, input_shape[:-1] + [output_dim])  # (B, L, H)\n    return answer\n\n\ndef biaffine_attention(deps: tf.Tensor, heads: tf.Tensor, name=""biaffine_attention"") -> tf.Tensor:\n    """"""Implements a trainable matching layer between two families of embeddings.\n\n    Args:\n        deps: the 3D-tensor of dependency states,\n        heads: the 3D-tensor of head states,\n        name: the name of a layer\n\n    Returns:\n        `answer` a 3D-tensor of pairwise scores between deps and heads\n\n    """"""\n    deps_dim_int = deps.get_shape().as_list()[-1]\n    heads_dim_int = heads.get_shape().as_list()[-1]\n    assert deps_dim_int == heads_dim_int\n    with tf.variable_scope(name):\n        kernel_shape = (deps_dim_int, heads_dim_int)\n        kernel = tf.get_variable(\'kernel\', shape=kernel_shape, initializer=tf.initializers.identity())\n        first_bias = tf.get_variable(\'first_bias\', shape=(kernel_shape[0], 1),\n                                     initializer=xavier_initializer())\n        second_bias = tf.get_variable(\'second_bias\', shape=(kernel_shape[1], 1),\n                                      initializer=xavier_initializer())\n        # deps.shape = (B, L, D)\n        # first.shape = (B, L, D), first_rie = sum_d deps_{rid} kernel_{de}\n        first = tf.tensordot(deps, kernel, axes=[-1, -2])\n        answer = tf.matmul(first, heads, transpose_b=True)  # answer.shape = (B, L, L)\n        # add bias over x axis\n        first_bias_term = tf.tensordot(deps, first_bias, axes=[-1, -2])\n        answer += first_bias_term\n        # add bias over y axis\n        second_bias_term = tf.tensordot(heads, second_bias, axes=[-1, -2])  # (B, L, 1)\n        second_bias_term = tf.transpose(second_bias_term, [0, 2, 1])  # (B, 1, L)\n        answer += second_bias_term\n    return answer\n\n\n@register(\'bert_syntax_parser\')\nclass BertSyntaxParser(BertSequenceNetwork):\n    """"""BERT-based model for syntax parsing.\n    For each word the model predicts the index of its syntactic head\n    and the label of the dependency between this head and the current word.\n    See :class:`deeppavlov.models.bert.bert_sequence_tagger.BertSequenceNetwork`\n    for the description of inherited parameters.\n\n    Args:\n        n_deps: number of distinct syntactic dependencies\n        embeddings_dropout: dropout for embeddings in biaffine layer\n        state_size: the size of hidden state in biaffine layer\n        dep_state_size: the size of hidden state in biaffine layer\n        use_birnn: whether to use bidirection rnn after BERT layers.\n            Set it to `True` as it leads to much higher performance at least on large datasets\n        birnn_cell_type: the type of Bidirectional RNN. Either `lstm` or `gru`\n        birnn_hidden_size: number of hidden units in the BiRNN layer in each direction\n        return_probas: set this to `True` if you need the probabilities instead of raw answers\n        predict tags: whether to predict morphological tags together with syntactic information\n        n_tags: the number of morphological tags\n        tag_weight: the weight of tag model loss in multitask training\n    """"""\n\n    def __init__(self,\n                 n_deps: int,\n                 keep_prob: float,\n                 bert_config_file: str,\n                 pretrained_bert: str = None,\n                 attention_probs_keep_prob: float = None,\n                 hidden_keep_prob: float = None,\n                 embeddings_dropout: float = 0.0,\n                 encoder_layer_ids: List[int] = (-1,),\n                 encoder_dropout: float = 0.0,\n                 optimizer: str = None,\n                 weight_decay_rate: float = 1e-6,\n                 state_size: int = 256,\n                 use_birnn: bool = True,\n                 birnn_cell_type: str = \'lstm\',\n                 birnn_hidden_size: int = 256,\n                 ema_decay: float = None,\n                 ema_variables_on_cpu: bool = True,\n                 predict_tags = False,\n                 n_tags = None,\n                 tag_weight = 1.0,\n                 return_probas: bool = False,\n                 freeze_embeddings: bool = False,\n                 learning_rate: float = 1e-3,\n                 bert_learning_rate: float = 2e-5,\n                 min_learning_rate: float = 1e-07,\n                 learning_rate_drop_patience: int = 20,\n                 learning_rate_drop_div: float = 2.0,\n                 load_before_drop: bool = True,\n                 clip_norm: float = 1.0,\n                 **kwargs) -> None:\n        self.n_deps = n_deps\n        self.embeddings_dropout = embeddings_dropout\n        self.state_size = state_size\n        self.use_birnn = use_birnn\n        self.birnn_cell_type = birnn_cell_type\n        self.birnn_hidden_size = birnn_hidden_size\n        self.return_probas = return_probas\n        self.predict_tags = predict_tags\n        self.n_tags = n_tags\n        self.tag_weight = tag_weight\n        if self.predict_tags and self.n_tags is None:\n            raise ValueError(""n_tags should be given if `predict_tags`=True."")\n        super().__init__(keep_prob=keep_prob,\n                         bert_config_file=bert_config_file,\n                         pretrained_bert=pretrained_bert,\n                         attention_probs_keep_prob=attention_probs_keep_prob,\n                         hidden_keep_prob=hidden_keep_prob,\n                         encoder_layer_ids=encoder_layer_ids,\n                         encoder_dropout=encoder_dropout,\n                         optimizer=optimizer,\n                         weight_decay_rate=weight_decay_rate,\n                         ema_decay=ema_decay,\n                         ema_variables_on_cpu=ema_variables_on_cpu,\n                         freeze_embeddings=freeze_embeddings,\n                         learning_rate=learning_rate,\n                         bert_learning_rate=bert_learning_rate,\n                         min_learning_rate=min_learning_rate,\n                         learning_rate_drop_div=learning_rate_drop_div,\n                         learning_rate_drop_patience=learning_rate_drop_patience,\n                         load_before_drop=load_before_drop,\n                         clip_norm=clip_norm,\n                         **kwargs)\n\n    def _init_graph(self) -> None:\n        self._init_placeholders()\n\n        units = super()._init_graph()\n\n        with tf.variable_scope(\'ner\'):\n            units = token_from_subtoken(units, self.y_masks_ph)\n            if self.use_birnn:\n                units, _ = bi_rnn(units,\n                                  self.birnn_hidden_size,\n                                  cell_type=self.birnn_cell_type,\n                                  seq_lengths=self.seq_lengths,\n                                  name=\'birnn\')\n                units = tf.concat(units, -1)\n            # for heads\n            head_embeddings = tf.layers.dense(units, units=self.state_size, activation=""relu"")\n            head_embeddings = tf.nn.dropout(head_embeddings, self.embeddings_keep_prob_ph)\n            dep_embeddings = tf.layers.dense(units, units=self.state_size, activation=""relu"")\n            dep_embeddings = tf.nn.dropout(dep_embeddings, self.embeddings_keep_prob_ph)\n            self.dep_head_similarities = biaffine_attention(dep_embeddings, head_embeddings)\n            self.dep_heads = tf.argmax(self.dep_head_similarities, -1)\n            self.dep_head_probs = tf.nn.softmax(self.dep_head_similarities)\n            # for dependency types\n            head_embeddings = tf.layers.dense(units, units=self.state_size, activation=""relu"")\n            head_embeddings = tf.nn.dropout(head_embeddings, self.embeddings_keep_prob_ph)\n            dep_embeddings = tf.layers.dense(units, units=self.state_size, activation=""relu"")\n            dep_embeddings = tf.nn.dropout(dep_embeddings, self.embeddings_keep_prob_ph)\n            # matching each word with its head\n            head_embeddings = gather_indexes(head_embeddings, self.y_head_ph)\n            self.dep_logits = biaffine_layer(dep_embeddings, head_embeddings, \n                                             deps_dim=self.state_size, heads_dim=self.state_size, \n                                             output_dim=self.n_deps)\n            self.deps = tf.argmax(self.dep_logits, -1)\n            self.dep_probs = tf.nn.softmax(self.dep_logits)\n            if self.predict_tags:\n                tag_embeddings = tf.layers.dense(units, units=self.state_size, activation=""relu"")\n                tag_embeddings = tf.nn.dropout(tag_embeddings, self.embeddings_keep_prob_ph)\n                self.tag_logits = tf.layers.dense(tag_embeddings, units=self.n_tags)\n                self.tags = tf.argmax(self.tag_logits, -1)\n                self.tag_probs = tf.nn.softmax(self.tag_logits)\n        with tf.variable_scope(""loss""):\n            tag_mask = self._get_tag_mask()\n            y_mask = tf.cast(tag_mask, tf.float32)\n            self.loss = tf.losses.sparse_softmax_cross_entropy(labels=self.y_head_ph,\n                                                               logits=self.dep_head_similarities,\n                                                               weights=y_mask)\n            self.loss += tf.losses.sparse_softmax_cross_entropy(labels=self.y_dep_ph,\n                                                                logits=self.dep_logits,\n                                                                weights=y_mask)\n            if self.predict_tags:\n                tag_loss = tf.losses.sparse_softmax_cross_entropy(labels=self.y_tag_ph,\n                                                                  logits=self.tag_logits,\n                                                                  weights=y_mask)\n                self.loss += self.tag_weight_ph * tag_loss\n\n    def _init_placeholders(self) -> None:\n        super()._init_placeholders()\n        self.y_head_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name=\'y_head_ph\')\n        self.y_dep_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name=\'y_dep_ph\')\n        if self.predict_tags:\n            self.y_tag_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name=\'y_tag_ph\')\n        self.y_masks_ph = tf.placeholder(shape=(None, None), dtype=tf.int32, name=\'y_mask_ph\')\n        self.embeddings_keep_prob_ph = tf.placeholder_with_default(\n            1.0, shape=[], name=""embeddings_keep_prob_ph"")\n        if self.predict_tags:\n            self.tag_weight_ph = tf.placeholder_with_default(1.0, shape=[], name=""tag_weight_ph"")\n\n    def _build_feed_dict(self, input_ids, input_masks, y_masks, \n                         y_head=None, y_dep=None, y_tag=None) -> dict:\n        y_masks = np.concatenate([np.ones_like(y_masks[:,:1]), y_masks[:, 1:]], axis=1)\n        feed_dict = self._build_basic_feed_dict(input_ids, input_masks, train=(y_head is not None))\n        feed_dict[self.y_masks_ph] = y_masks\n        if y_head is not None:\n            y_head = zero_pad(y_head)\n            y_head = np.concatenate([np.zeros_like(y_head[:,:1]), y_head], axis=1)\n            y_dep = zero_pad(y_dep)\n            y_dep = np.concatenate([np.zeros_like(y_dep[:,:1]), y_dep], axis=1)\n            feed_dict.update({self.embeddings_keep_prob_ph: 1.0 - self.embeddings_dropout,\n                              self.y_head_ph: y_head,\n                              self.y_dep_ph: y_dep})\n            if self.predict_tags:\n                y_tag = np.concatenate([np.zeros_like(y_tag[:,:1]), y_tag], axis=1)\n                feed_dict.update({self.y_tag_ph: y_tag, self.tag_weight_ph: self.tag_weight})\n        return feed_dict\n\n    def __call__(self,\n                 input_ids: Union[List[List[int]], np.ndarray],\n                 input_masks: Union[List[List[int]], np.ndarray],\n                 y_masks: Union[List[List[int]], np.ndarray]) \\\n            -> Union[Tuple[List[Union[List[int], np.ndarray]], List[List[int]]],\n                     Tuple[List[Union[List[int], np.ndarray]], List[List[int]], List[List[int]]]]:\n\n        """""" Predicts the outputs for a batch of inputs.\n        By default (``return_probas`` = `False` and ``predict_tags`` = `False`) it returns two output batches.\n        The first is the batch of head indexes: `i` stands for `i`-th word in the sequence,\n        where numeration starts with 1. `0` is predicted for the syntactic root of the sentence.\n        The second is the batch of indexes for syntactic dependencies.\n        In case ``return_probas`` = `True` we return the probability distribution over possible heads\n        instead of the position of the most probable head. For a sentence of length `k` the output\n        is an array of shape `k * (k+1)`.\n        In case ``predict_tags`` = `True` the model additionally returns the index of the most probable\n        morphological tag for each word. The batch of such indexes becomes the third output of the function.\n\n        Returns:\n            `pred_heads_to_return`, either a batch of most probable head positions for each token\n            (in case ``return_probas`` = `False`)\n            or a batch of probability distribution over token head positions\n\n            `pred_deps`, the indexes of token dependency relations\n\n            `pred_tags`: the indexes of token morphological tags (only if ``predict_tags`` = `True`)\n\n        """"""\n        feed_dict = self._build_feed_dict(input_ids, input_masks, y_masks)\n        if self.ema:\n            self.sess.run(self.ema.switch_to_test_op)\n        if self.return_probas:\n            pred_head_probs, pred_heads, seq_lengths =\\\n                 self.sess.run([self.dep_head_probs, self.dep_heads, self.seq_lengths], feed_dict=feed_dict)\n            pred_heads_to_return = [np.array(p[1:l,:l]) for l, p in zip(seq_lengths, pred_head_probs)]\n        else:\n            pred_heads, seq_lengths = self.sess.run([self.dep_heads, self.seq_lengths], feed_dict=feed_dict)\n            pred_heads_to_return = [p[1:l] for l, p in zip(seq_lengths, pred_heads)]\n        feed_dict[self.y_head_ph] = pred_heads\n        pred_deps = self.sess.run(self.deps, feed_dict=feed_dict)\n        pred_deps = [p[1:l] for l, p in zip(seq_lengths, pred_deps)]\n        answer = [pred_heads_to_return, pred_deps]\n        if self.predict_tags:\n            pred_tags = self.sess.run(self.tags, feed_dict=feed_dict)\n            pred_tags = [p[1:l] for l, p in zip(seq_lengths, pred_tags)]\n            answer.append(pred_tags)\n        return tuple(answer)\n'"
deeppavlov/models/syntax_parser/parser.py,0,"b'# Copyright 2019 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import List\nimport numpy as np\n\nfrom dependency_decoding import chu_liu_edmonds\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\n\n\n@register(\'chu_liu_edmonds_transformer\')\nclass ChuLiuEdmonds(Component):\n    """"""\n    A wrapper for Chu-Liu-Edmonds algorithm for maximum spanning tree\n    """"""\n    def __init__(self, min_edge_prob=1e-6, **kwargs):\n        self.min_edge_prob = min_edge_prob\n\n    def __call__(self, probs: List[np.ndarray]) -> List[List[int]]:\n        """"""Applies Chu-Liu-Edmonds algorithm to the matrix of head probabilities.\n\n        probs: a 3D-array of probabilities of shape B*L*(L+1)\n        """"""\n        answer = []\n        for elem in probs:\n            m, n = elem.shape\n            assert n == m+1\n            elem = np.log10(np.maximum(self.min_edge_prob, elem)) - np.log10(self.min_edge_prob)\n            elem = np.concatenate([np.zeros_like(elem[:1,:]), elem], axis=0)\n            # it makes impossible to create multiple edges 0->i\n            elem[1:, 0] += np.log10(self.min_edge_prob) * len(elem)\n            chl_data = chu_liu_edmonds(elem.astype(""float64""))\n            answer.append(chl_data[0][1:])\n        return answer\n'"
deeppavlov/models/tokenizers/__init__.py,0,b''
deeppavlov/models/tokenizers/lazy_tokenizer.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\n\nfrom nltk import word_tokenize\nfrom overrides import overrides\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\n\nlog = getLogger(__name__)\n\n\n@register(\'lazy_tokenizer\')\nclass LazyTokenizer(Component):\n    """"""Tokenizes if there is something to tokenize.""""""\n\n    def __init__(self, **kwargs):\n        pass\n\n    @overrides\n    def __call__(self, batch, *args, **kwargs):\n        if len(batch) > 0 and isinstance(batch[0], str):\n            batch = [word_tokenize(utt) for utt in batch]\n        return batch\n'"
deeppavlov/models/tokenizers/nltk_moses_tokenizer.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom typing import Union, List\n\nfrom sacremoses import MosesDetokenizer, MosesTokenizer\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\n\n\n@register(""nltk_moses_tokenizer"")\nclass NLTKMosesTokenizer(Component):\n    """"""Class for splitting texts on tokens using NLTK wrapper over MosesTokenizer\n\n    Attributes:\n        escape: whether escape characters for use in html markup\n        tokenizer: tokenizer instance from nltk.tokenize.moses\n        detokenizer: detokenizer instance from nltk.tokenize.moses\n\n    Args:\n        escape: whether escape characters for use in html markup\n    """"""\n\n    def __init__(self, escape: bool = False, *args, **kwargs):\n        self.escape = escape\n        self.tokenizer = MosesTokenizer()\n        self.detokenizer = MosesDetokenizer()\n\n    def __call__(self, batch: List[Union[str, List[str]]]) -> List[Union[List[str], str]]:\n        """"""Tokenize given batch of strings or detokenize given batch of lists of tokens\n\n        Args:\n            batch: list of text samples or list of lists of tokens\n\n        Returns:\n            list of lists of tokens or list of text samples\n        """"""\n        if isinstance(batch[0], str):\n            return [self.tokenizer.tokenize(line, escape=self.escape) for line in batch]\n        else:\n            return [self.detokenizer.detokenize(line, return_str=True, unescape=self.escape)\n                    for line in batch]\n'"
deeppavlov/models/tokenizers/nltk_tokenizer.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import List\n\nimport nltk\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\n\n\n@register(""nltk_tokenizer"")\nclass NLTKTokenizer(Component):\n    """"""Class for splitting texts on tokens using NLTK\n\n    Args:\n        tokenizer: tokenization mode for `nltk.tokenize`\n        download: whether to download nltk data\n\n    Attributes:\n        tokenizer: tokenizer instance from nltk.tokenizers\n    """"""\n\n    def __init__(self, tokenizer: str = ""wordpunct_tokenize"", download: bool = False,\n                 *args, **kwargs):\n        if download:\n            nltk.download()\n        self.tokenizer = getattr(nltk.tokenize, tokenizer, None)\n        if not callable(self.tokenizer):\n            raise AttributeError(""Tokenizer {} is not defined in nltk.tokenizer"".format(tokenizer))\n\n    def __call__(self, batch: List[str]) -> List[List[str]]:\n        """"""Tokenize given batch\n\n        Args:\n            batch: list of text samples\n\n        Returns:\n            list of lists of tokens\n        """"""\n        return [self.tokenizer(sent) for sent in batch]\n'"
deeppavlov/models/tokenizers/ru_sent_tokenizer.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Set, Tuple\n\nfrom rusenttokenize import ru_sent_tokenize, SHORTENINGS, JOINING_SHORTENINGS, PAIRED_SHORTENINGS\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\n\n\n@register(""ru_sent_tokenizer"")\nclass RuSentTokenizer(Component):\n    """"""\n    Rule-base sentence tokenizer for Russian language.\n    https://github.com/deepmipt/ru_sentence_tokenizer\n\n    Args:\n        shortenings: list of known shortenings. Use default value if working on news or fiction texts\n        joining_shortenings: list of shortenings after that sentence split is not possible (i.e. ""\xd1\x83\xd0\xbb"").\n            Use default value if working on news or fiction texts\n        paired_shortenings: list of known paired shotenings (i.e. ""\xd1\x82. \xd0\xb5."").\n            Use default value if working on news or fiction texts\n\n    """"""\n\n    def __init__(self, shortenings: Set[str] = SHORTENINGS,\n                 joining_shortenings: Set[str] = JOINING_SHORTENINGS,\n                 paired_shortenings: Set[Tuple[str, str]] = PAIRED_SHORTENINGS,\n                 **kwargs):\n        self.shortenings = shortenings\n        self.joining_shortenings = joining_shortenings\n        self.paired_shortenings = paired_shortenings\n\n    def __call__(self, batch: [str]) -> [[str]]:\n        return [ru_sent_tokenize(x, self.shortenings, self.joining_shortenings, self.paired_shortenings) for x in batch]\n'"
deeppavlov/models/tokenizers/ru_tokenizer.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import List, Generator, Any, Optional, Union, Tuple\n\n# from nltk.corpus import stopwords\n# STOPWORDS = stopwords.words(\'russian\')\nimport pymorphy2\nfrom nltk.tokenize.toktok import ToktokTokenizer\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.models.tokenizers.utils import detokenize, ngramize\n\nlogger = getLogger(__name__)\n\n\n@register(\'ru_tokenizer\')\nclass RussianTokenizer(Component):\n    """"""Tokenize or lemmatize a list of documents for Russian language. Default models are\n    :class:`ToktokTokenizer` tokenizer and :mod:`pymorphy2` lemmatizer.\n    Return a list of tokens or lemmas for a whole document.\n    If is called onto ``List[str]``, performs detokenizing procedure.\n\n    Args:\n        stopwords: a list of stopwords that should be ignored during tokenizing/lemmatizing\n         and ngrams creation\n        ngram_range: size of ngrams to create; only unigrams are returned by default\n        lemmas: whether to perform lemmatizing or not\n        lowercase: whether to perform lowercasing or not; is performed by default by :meth:`_tokenize`\n         and :meth:`_lemmatize` methods\n        alphas_only: whether to filter out non-alpha tokens; is performed by default by :meth:`_filter`\n         method\n\n    Attributes:\n        stopwords: a list of stopwords that should be ignored during tokenizing/lemmatizing\n         and ngrams creation\n        tokenizer: an instance of :class:`ToktokTokenizer` tokenizer class\n        lemmatizer: an instance of :class:`pymorphy2.MorphAnalyzer` lemmatizer class\n        ngram_range: size of ngrams to create; only unigrams are returned by default\n        lemmas: whether to perform lemmatizing or not\n        lowercase: whether to perform lowercasing or not; is performed by default by :meth:`_tokenize`\n         and :meth:`_lemmatize` methods\n        alphas_only: whether to filter out non-alpha tokens; is performed by default by :meth:`_filter`\n         method\n         tok2morph: token-to-lemma cache\n\n    """"""\n\n    def __init__(self, stopwords: Optional[List[str]] = None, ngram_range: List[int] = None,\n                 lemmas: bool = False, lowercase: Optional[bool] = None,\n                 alphas_only: Optional[bool] = None, **kwargs):\n\n        if ngram_range is None:\n            ngram_range = [1, 1]\n        self.stopwords = stopwords or []\n        self.tokenizer = ToktokTokenizer()\n        self.lemmatizer = pymorphy2.MorphAnalyzer()\n        self.ngram_range = tuple(ngram_range)  # cast JSON array to tuple\n        self.lemmas = lemmas\n        self.lowercase = lowercase\n        self.alphas_only = alphas_only\n        self.tok2morph = {}\n\n    def __call__(self, batch: Union[List[str], List[List[str]]]) -> \\\n            Union[List[List[str]], List[str]]:\n        """"""Tokenize or detokenize strings, depends on the type structure of passed arguments.\n\n        Args:\n            batch: a batch of documents to perform tokenizing/lemmatizing;\n             or a batch of lists of tokens/lemmas to perform detokenizing\n\n        Returns:\n            a batch of lists of tokens/lemmas; or a batch of detokenized strings\n\n        Raises:\n            TypeError: If the first element of ``batch`` is neither ``List``, nor ``str``.\n\n        """"""\n        if isinstance(batch[0], str):\n            if self.lemmas:\n                return list(self._lemmatize(batch))\n            else:\n                return list(self._tokenize(batch))\n        if isinstance(batch[0], list):\n            return [detokenize(doc) for doc in batch]\n        raise TypeError(\n            ""StreamSpacyTokenizer.__call__() is not implemented for `{}`"".format(type(batch[0])))\n\n    def _tokenize(self, data: List[str], ngram_range: Tuple[int, int] = (1, 1), lowercase: bool = True) \\\n            -> Generator[List[str], Any, None]:\n        """"""Tokenize a list of documents.\n\n       Args:\n           data: a list of documents to tokenize\n           ngram_range: size of ngrams to create; only unigrams are returned by default\n           lowercase: whether to perform lowercasing or not; is performed by default by\n           :meth:`_tokenize` and :meth:`_lemmatize` methods\n\n       Yields:\n           list of lists of ngramized tokens or list of detokenized strings\n\n        Returns:\n            None\n\n       """"""\n        # DEBUG\n        # size = len(data)\n        _ngram_range = self.ngram_range or ngram_range\n\n        if self.lowercase is None:\n            _lowercase = lowercase\n        else:\n            _lowercase = self.lowercase\n\n        for i, doc in enumerate(data):\n            # DEBUG\n            # logger.info(""Tokenize doc {} from {}"".format(i, size))\n            tokens = self.tokenizer.tokenize(doc)\n            if _lowercase:\n                tokens = [t.lower() for t in tokens]\n            filtered = self._filter(tokens)\n            processed_doc = ngramize(filtered, ngram_range=_ngram_range)\n            yield from processed_doc\n\n    def _lemmatize(self, data: List[str], ngram_range: Tuple[int, int] = (1, 1)) -> \\\n            Generator[List[str], Any, None]:\n        """"""Lemmatize a list of documents.\n\n        Args:\n            data: a list of documents to tokenize\n            ngram_range: size of ngrams to create; only unigrams are returned by default\n\n        Yields:\n            list of lists of ngramized tokens or list of detokenized strings\n\n        Returns:\n            None\n\n        """"""\n        # DEBUG\n        # size = len(data)\n        _ngram_range = self.ngram_range or ngram_range\n\n        tokenized_data = list(self._tokenize(data))\n\n        for i, doc in enumerate(tokenized_data):\n            # DEBUG\n            # logger.info(""Lemmatize doc {} from {}"".format(i, size))\n            lemmas = []\n            for token in doc:\n                try:\n                    lemma = self.tok2morph[token]\n                except KeyError:\n                    lemma = self.lemmatizer.parse(token)[0].normal_form\n                    self.tok2morph[token] = lemma\n                lemmas.append(lemma)\n            filtered = self._filter(lemmas)\n            processed_doc = ngramize(filtered, ngram_range=_ngram_range)\n            yield from processed_doc\n\n    def _filter(self, items: List[str], alphas_only: bool = True) -> List[str]:\n        """"""Filter a list of tokens/lemmas.\n\n        Args:\n            items: a list of tokens/lemmas to filter\n            alphas_only: whether to filter out non-alpha tokens\n\n        Returns:\n            a list of filtered tokens/lemmas\n\n        """"""\n        if self.alphas_only is None:\n            _alphas_only = alphas_only\n        else:\n            _alphas_only = self.alphas_only\n\n        if _alphas_only:\n            filter_fn = lambda x: x.isalpha() and not x.isspace() and x not in self.stopwords\n        else:\n            filter_fn = lambda x: not x.isspace() and x not in self.stopwords\n\n        return list(filter(filter_fn, items))\n\n    def set_stopwords(self, stopwords: List[str]) -> None:\n        """"""Redefine a list of stopwords.\n\n       Args:\n           stopwords: a list of stopwords\n\n       Returns:\n           None\n\n       """"""\n        self.stopwords = stopwords\n'"
deeppavlov/models/tokenizers/spacy_tokenizer.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import List, Generator, Any, Optional, Union, Tuple, Iterable\n\nimport spacy\nimport spacy.language\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.models.tokenizers.utils import detokenize, ngramize\n\nlogger = getLogger(__name__)\n\n\ndef _try_load_spacy_model(model_name: str, disable: Iterable[str] = ()):\n    disable = set(disable)\n    try:\n        model = spacy.load(model_name, disable=disable)\n    except OSError as e:\n        try:\n            model = __import__(model_name).load(disable=disable)\n            if not isinstance(model, spacy.language.Language):\n                raise RuntimeError(f\'{model_name} is not a spacy model module\')\n        except Exception:\n            raise e\n    return model\n\n\n@register(\'stream_spacy_tokenizer\')\nclass StreamSpacyTokenizer(Component):\n    """"""Tokenize or lemmatize a list of documents. Default spacy model is **en_core_web_sm**.\n    Return a list of tokens or lemmas for a whole document.\n    If is called onto ``List[str]``, performs detokenizing procedure.\n\n    Args:\n        disable: spacy pipeline elements to disable, serves a purpose of performing; if nothing\n        stopwords: a list of stopwords that should be ignored during tokenizing/lemmatizing\n         and ngrams creation\n        batch_size: a batch size for spaCy buffering\n        ngram_range: size of ngrams to create; only unigrams are returned by default\n        lemmas: whether to perform lemmatizing or not\n        lowercase: whether to perform lowercasing or not; is performed by default by :meth:`_tokenize`\n         and :meth:`_lemmatize` methods\n        alphas_only: whether to filter out non-alpha tokens; is performed by default by\n         :meth:`_filter` method\n        spacy_model: a string name of spacy model to use; DeepPavlov searches for this name in\n         downloaded spacy models; default model is **en_core_web_sm**, it downloads automatically\n         during DeepPavlov installation\n\n\n    Attributes:\n        stopwords: a list of stopwords that should be ignored during tokenizing/lemmatizing\n         and ngrams creation\n        model: a loaded spacy model\n        batch_size: a batch size for spaCy buffering\n        ngram_range: size of ngrams to create; only unigrams are returned by default\n        lemmas: whether to perform lemmatizing or not\n        lowercase: whether to perform lowercasing or not; is performed by default by :meth:`_tokenize`\n         and :meth:`_lemmatize` methods\n        alphas_only: whether to filter out non-alpha tokens; is performed by default by :meth:`_filter`\n         method\n\n    """"""\n\n    def __init__(self, disable: Optional[Iterable[str]] = None, stopwords: Optional[List[str]] = None,\n                 batch_size: Optional[int] = None, ngram_range: Optional[List[int]] = None,\n                 lemmas: bool = False, lowercase: Optional[bool] = None, alphas_only: Optional[bool] = None,\n                 spacy_model: str = \'en_core_web_sm\', **kwargs):\n\n        if disable is None:\n            disable = [\'parser\', \'ner\']\n        if ngram_range is None:\n            ngram_range = [1, 1]\n        self.stopwords = stopwords or []\n        self.model = _try_load_spacy_model(spacy_model, disable=disable)\n        self.batch_size = batch_size\n        self.ngram_range = tuple(ngram_range)  # cast JSON array to tuple\n        self.lemmas = lemmas\n        self.lowercase = lowercase\n        self.alphas_only = alphas_only\n\n    def __call__(self, batch: Union[List[str], List[List[str]]]) -> Union[List[List[str]], List[str]]:\n        """"""Tokenize or detokenize strings, depends on the type structure of passed arguments.\n\n        Args:\n            batch: a batch of documents to perform tokenizing/lemmatizing;\n             or a batch of lists of tokens/lemmas to perform detokenizing\n\n        Returns:\n            a batch of lists of tokens/lemmas; or a batch of detokenized strings\n\n        Raises:\n            TypeError: If the first element of ``batch`` is neither List, nor str.\n\n        """"""\n        if isinstance(batch[0], str):\n            if self.lemmas:\n                return list(self._lemmatize(batch))\n            else:\n                return list(self._tokenize(batch))\n        if isinstance(batch[0], list):\n            return [detokenize(doc) for doc in batch]\n        raise TypeError(\n            ""StreamSpacyTokenizer.__call__() is not implemented for `{}`"".format(type(batch[0])))\n\n    def _tokenize(self, data: List[str], ngram_range: Optional[Tuple[int, int]] = None, batch_size: int = 10000,\n                  lowercase: bool = True) -> Generator[List[str], Any, None]:\n        """"""Tokenize a list of documents.\n\n        Args:\n            data: a list of documents to tokenize\n            ngram_range: size of ngrams to create; only unigrams are returned by default\n            batch_size: a batch size for spaCy buffering\n            lowercase: whether to perform lowercasing or not; is performed by default by\n                :meth:`_tokenize` and :meth:`_lemmatize` methods\n\n        Yields:\n            list of lists of ngramized tokens or list of detokenized strings\n\n        Returns:\n            None\n\n        """"""\n        _batch_size = self.batch_size or batch_size\n        _ngram_range = ngram_range or self.ngram_range\n\n        if self.lowercase is None:\n            _lowercase = lowercase\n        else:\n            _lowercase = self.lowercase\n\n        for i, doc in enumerate(\n                self.model.tokenizer.pipe(data, batch_size=_batch_size)):\n            if _lowercase:\n                tokens = [t.lower_ for t in doc]\n            else:\n                tokens = [t.text for t in doc]\n            filtered = self._filter(tokens)\n            processed_doc = ngramize(filtered, ngram_range=_ngram_range)\n            yield from processed_doc\n\n    def _lemmatize(self, data: List[str], ngram_range: Optional[Tuple[int, int]] = None, batch_size: int = 10000\n                   ) -> Generator[List[str], Any, None]:\n        """"""Lemmatize a list of documents.\n\n        Args:\n            data: a list of documents to tokenize\n            ngram_range: size of ngrams to create; only unigrams are returned by default\n            batch_size: a batch size for spaCy buffering\n\n       Yields:\n           list of lists of ngramized lemmas or list of detokenized strings\n\n        Returns:\n            None\n\n        """"""\n        _batch_size = self.batch_size or batch_size\n        _ngram_range = ngram_range or self.ngram_range\n\n        for i, doc in enumerate(\n                self.model.pipe(data, batch_size=_batch_size)):\n            lemmas = [t.lemma_ for t in doc]\n            filtered = self._filter(lemmas)\n            processed_doc = ngramize(filtered, ngram_range=_ngram_range)\n            yield from processed_doc\n\n    def _filter(self, items: List[str], alphas_only: bool = True) -> List[str]:\n        """"""Filter a list of tokens/lemmas.\n\n        Args:\n            items: a list of tokens/lemmas to filter\n            alphas_only: whether to filter out non-alpha tokens\n\n        Returns:\n            a list of filtered tokens/lemmas\n\n        """"""\n        if self.alphas_only is None:\n            _alphas_only = alphas_only\n        else:\n            _alphas_only = self.alphas_only\n\n        if _alphas_only:\n            filter_fn = lambda x: x.isalpha() and not x.isspace() and x not in self.stopwords\n        else:\n            filter_fn = lambda x: not x.isspace() and x not in self.stopwords\n\n        return list(filter(filter_fn, items))\n\n    def set_stopwords(self, stopwords: List[str]) -> None:\n        """"""Redefine a list of stopwords.\n\n        Args:\n            stopwords: a list of stopwords\n\n        Returns:\n            None\n\n        """"""\n        self.stopwords = stopwords\n'"
deeppavlov/models/tokenizers/split_tokenizer.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import List\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\n\n\n@register(""split_tokenizer"")\nclass SplitTokenizer(Component):\n    """"""\n    Generates utterance\'s tokens by mere python\'s ``str.split()``.\n\n    Doesn\'t have any parameters.\n    """"""\n\n    def __init__(self, **kwargs) -> None:\n        pass\n\n    def __call__(self, batch: List[str]) -> List[List[str]]:\n        """"""\n        Tokenize given batch\n\n        Args:\n            batch: list of texts to tokenize\n\n        Returns:\n            tokenized batch\n        """"""\n        if isinstance(batch, (list, tuple)):\n            return [sample.split() for sample in batch]\n        else:\n            raise NotImplementedError(\'not implemented for types other than\'\n                                      \' list or tuple\')\n'"
deeppavlov/models/tokenizers/utils.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\nfrom typing import List, Generator, Any\n\n\ndef detokenize(tokens):\n    """"""\n    Detokenizing a text undoes the tokenizing operation, restores\n    punctuation and spaces to the places that people expect them to be.\n    Ideally, `detokenize(tokenize(text))` should be identical to `text`,\n    except for line breaks.\n    """"""\n    text = \' \'.join(tokens)\n    step0 = text.replace(\'. . .\', \'...\')\n    step1 = step0.replace(""`` "", \'""\').replace("" \'\'"", \'""\')\n    step2 = step1.replace("" ( "", "" ("").replace("" ) "", "") "")\n    step3 = re.sub(r\' ([.,:;?!%]+)([ \\\'""`])\', r""\\1\\2"", step2)\n    step4 = re.sub(r\' ([.,:;?!%]+)$\', r""\\1"", step3)\n    step5 = step4.replace("" \'"", ""\'"").replace("" n\'t"", ""n\'t"") \\\n        .replace("" nt"", ""nt"").replace(""can not"", ""cannot"")\n    step6 = step5.replace("" ` "", "" \'"")\n    return step6.strip()\n\n\ndef ngramize(items: List[str], ngram_range=(1, 1)) -> Generator[List[str], Any, None]:\n    """"""\n    Make ngrams from a list of tokens/lemmas\n    :param items: list of tokens, lemmas or other strings to form ngrams\n    :param ngram_range: range for producing ngrams, ex. for unigrams + bigrams should be set to\n    (1, 2), for bigrams only should be set to (2, 2)\n    :return: ngrams (as strings) generator\n    """"""\n\n    ngrams = []\n    ranges = [(0, i) for i in range(ngram_range[0], ngram_range[1] + 1)]\n    for r in ranges:\n        ngrams += list(zip(*[items[j:] for j in range(*r)]))\n\n    formatted_ngrams = [\' \'.join(item) for item in ngrams]\n\n    yield formatted_ngrams\n'"
deeppavlov/models/vectorizers/__init__.py,0,b'\n'
deeppavlov/models/vectorizers/hashing_tfidf_vectorizer.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom collections import Counter\nfrom logging import getLogger\nfrom typing import List, Any, Generator, Tuple, KeysView, ValuesView, Dict, Optional\n\nimport numpy as np\nimport scipy as sp\nfrom scipy import sparse\nfrom sklearn.utils import murmurhash3_32\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.core.models.estimator import Estimator\n\nlogger = getLogger(__name__)\n\nSparse = sp.sparse.csr_matrix\n\n\ndef hash_(token: str, hash_size: int) -> int:\n    """"""Convert a token to a hash of given size.\n    Args:\n        token: a word\n        hash_size: hash size\n\n    Returns:\n        int, hashed token\n\n    """"""\n    return murmurhash3_32(token, positive=True) % hash_size\n\n\n@register(\'hashing_tfidf_vectorizer\')\nclass HashingTfIdfVectorizer(Estimator):\n    """"""Create a tfidf matrix from collection of documents of size [n_documents X n_features(hash_size)].\n\n    Args:\n        tokenizer: a tokenizer class\n        hash_size: a hash size, power of two\n        doc_index: a dictionary of document ids and their titles\n        save_path: a path to **.npz** file where tfidf matrix is saved\n        load_path: a path to **.npz** file where tfidf matrix is loaded from\n\n    Attributes:\n        hash_size: a hash size\n        tokenizer: instance of a tokenizer class\n        term_freqs: a dictionary with tfidf terms and their frequences\n        doc_index: provided by a user ids or generated automatically ids\n        rows: tfidf matrix rows corresponding to terms\n        cols: tfidf matrix cols corresponding to docs\n        data: tfidf matrix data corresponding to tfidf values\n\n    """"""\n\n    def __init__(self, tokenizer: Component, hash_size=2 ** 24, doc_index: Optional[dict] = None,\n                 save_path: Optional[str] = None, load_path: Optional[str] = None, **kwargs):\n\n        super().__init__(save_path=save_path, load_path=load_path, mode=kwargs.get(\'mode\', \'infer\'))\n\n        self.hash_size = hash_size\n        self.tokenizer = tokenizer\n        self.rows = []\n        self.cols = []\n        self.data = []\n\n        if kwargs.get(\'mode\', \'infer\') == \'infer\':\n            self.tfidf_matrix, opts = self.load()\n            self.ngram_range = opts[\'ngram_range\']\n            self.hash_size = opts[\'hash_size\']\n            self.term_freqs = opts[\'term_freqs\'].squeeze()\n            self.doc_index = opts[\'doc_index\']\n            self.index2doc = self.get_index2doc()\n        else:\n            self.term_freqs = None\n            self.doc_index = doc_index or {}\n\n    def __call__(self, questions: List[str]) -> Sparse:\n        """"""Transform input list of documents to tfidf vectors.\n\n        Args:\n            questions: a list of input strings\n\n        Returns:\n            transformed documents as a csr_matrix with shape [n_documents X :attr:`hash_size`]\n\n        """"""\n\n        sp_tfidfs = []\n\n        for question in questions:\n            ngrams = list(self.tokenizer([question]))\n            hashes = [hash_(ngram, self.hash_size) for ngram in ngrams[0]]\n\n            hashes_unique, q_hashes = np.unique(hashes, return_counts=True)\n            tfs = np.log1p(q_hashes)\n\n            if len(q_hashes) == 0:\n                sp_tfidfs.append(Sparse((1, self.hash_size)))\n                continue\n\n            size = len(self.doc_index)\n            Ns = self.term_freqs[hashes_unique]\n            idfs = np.log((size - Ns + 0.5) / (Ns + 0.5))\n            idfs[idfs < 0] = 0\n\n            tfidf = np.multiply(tfs, idfs)\n\n            indptr = np.array([0, len(hashes_unique)])\n            sp_tfidf = Sparse((tfidf, hashes_unique, indptr), shape=(1, self.hash_size)\n                              )\n            sp_tfidfs.append(sp_tfidf)\n\n        transformed = sp.sparse.vstack(sp_tfidfs)\n        return transformed\n\n    def get_index2doc(self) -> Dict[Any, int]:\n        """"""Invert doc_index.\n\n        Returns:\n            inverted doc_index dict\n\n        """"""\n        return dict(zip(self.doc_index.values(), self.doc_index.keys()))\n\n    def get_counts(self, docs: List[str], doc_ids: List[Any]) \\\n            -> Generator[Tuple[KeysView, ValuesView, List[int]], Any, None]:\n        """"""Get term counts for a list of documents.\n\n        Args:\n            docs: a list of input documents\n            doc_ids: a list of document ids corresponding to input documents\n\n        Yields:\n            a tuple of term hashes, count values and column ids\n\n        Returns:\n            None\n\n        """"""\n        logger.info(""Tokenizing batch..."")\n        batch_ngrams = list(self.tokenizer(docs))\n        logger.info(""Counting hash..."")\n        doc_id = iter(doc_ids)\n        for ngrams in batch_ngrams:\n            counts = Counter([hash_(gram, self.hash_size) for gram in ngrams])\n            hashes = counts.keys()\n            values = counts.values()\n            _id = self.doc_index[next(doc_id)]\n            if values:\n                col_id = [_id] * len(values)\n            else:\n                col_id = []\n            yield hashes, values, col_id\n\n    def get_count_matrix(self, row: List[int], col: List[int], data: List[int], size: int) \\\n            -> Sparse:\n        """"""Get count matrix.\n\n        Args:\n            row: tfidf matrix rows corresponding to terms\n            col:  tfidf matrix cols corresponding to docs\n            data: tfidf matrix data corresponding to tfidf values\n            size: :attr:`doc_index` size\n\n        Returns:\n            a count csr_matrix\n\n        """"""\n        count_matrix = Sparse((data, (row, col)), shape=(self.hash_size, size))\n        count_matrix.sum_duplicates()\n        return count_matrix\n\n    @staticmethod\n    def get_tfidf_matrix(count_matrix: Sparse) -> Tuple[Sparse, np.array]:\n        """"""Convert a count matrix into a tfidf matrix.\n\n        Args:\n            count_matrix: a count matrix\n\n        Returns:\n            a tuple of tfidf matrix and term frequences\n\n        """"""\n\n        binary = (count_matrix > 0).astype(int)\n        term_freqs = np.array(binary.sum(1)).squeeze()\n        idfs = np.log((count_matrix.shape[1] - term_freqs + 0.5) / (term_freqs + 0.5))\n        idfs[idfs < 0] = 0\n        idfs = sp.sparse.diags(idfs, 0)\n        tfs = count_matrix.log1p()\n        tfidfs = idfs.dot(tfs)\n        return tfidfs, term_freqs\n\n    def save(self) -> None:\n        """"""Save tfidf matrix into **.npz** format.\n\n        Returns:\n            None\n\n        """"""\n        logger.info(""Saving tfidf matrix to {}"".format(self.save_path))\n        count_matrix = self.get_count_matrix(self.rows, self.cols, self.data,\n                                             size=len(self.doc_index))\n        tfidf_matrix, term_freqs = self.get_tfidf_matrix(count_matrix)\n        self.term_freqs = term_freqs\n\n        opts = {\'hash_size\': self.hash_size,\n                \'ngram_range\': self.tokenizer.ngram_range,\n                \'doc_index\': self.doc_index,\n                \'term_freqs\': self.term_freqs}\n\n        data = {\n            \'data\': tfidf_matrix.data,\n            \'indices\': tfidf_matrix.indices,\n            \'indptr\': tfidf_matrix.indptr,\n            \'shape\': tfidf_matrix.shape,\n            \'opts\': opts\n        }\n        np.savez(self.save_path, **data)\n\n        # release memory\n        self.reset()\n\n    def reset(self) -> None:\n        """"""Clear :attr:`rows`, :attr:`cols` and :attr:`data`\n\n        Returns:\n            None\n\n        """"""\n        self.rows.clear()\n        self.cols.clear()\n        self.data.clear()\n\n    def load(self) -> Tuple[Sparse, Dict]:\n        """"""Load a tfidf matrix as csr_matrix.\n\n        Returns:\n            a tuple of tfidf matrix and csr data.\n\n        Raises:\n            FileNotFoundError if :attr:`load_path` doesn\'t exist.\n\n        Todo:\n            * implement loading from URL\n\n        """"""\n        if not self.load_path.exists():\n            raise FileNotFoundError(""HashingTfIdfVectorizer path doesn\'t exist!"")\n\n        logger.info(""Loading tfidf matrix from {}"".format(self.load_path))\n        loader = np.load(self.load_path, allow_pickle=True)\n        matrix = Sparse((loader[\'data\'], loader[\'indices\'],\n                         loader[\'indptr\']), shape=loader[\'shape\'])\n        return matrix, loader[\'opts\'].item(0)\n\n    def partial_fit(self, docs: List[str], doc_ids: List[Any], doc_nums: List[int]) -> None:\n        """"""Partially fit on one batch.\n\n        Args:\n            docs: a list of input documents\n            doc_ids: a list of document ids corresponding to input documents\n            doc_nums: a list of document integer ids as they appear in a database\n\n        Returns:\n            None\n\n        """"""\n        for doc_id, i in zip(doc_ids, doc_nums):\n            self.doc_index[doc_id] = i\n\n        for batch_rows, batch_data, batch_cols in self.get_counts(docs, doc_ids):\n            self.rows.extend(batch_rows)\n            self.cols.extend(batch_cols)\n            self.data.extend(batch_data)\n\n    def fit(self, docs: List[str], doc_ids: List[Any], doc_nums: List[int]) -> None:\n        """"""Fit the vectorizer.\n\n        Args:\n            docs: a list of input documents\n            doc_ids: a list of document ids corresponding to input documents\n            doc_nums: a list of document integer ids as they appear in a database\n\n        Returns:\n            None\n\n        """"""\n        self.doc_index = {}\n        self.rows = []\n        self.cols = []\n        self.data = []\n        return self.partial_fit(docs, doc_ids, doc_nums)\n'"
deeppavlov/models/vectorizers/word_vectorizer.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport pathlib\nfrom abc import abstractmethod\nfrom collections import defaultdict\nfrom typing import List, Dict, AnyStr, Union\n\nimport numpy as np\nfrom pymorphy2 import MorphAnalyzer\nfrom russian_tagsets import converters\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.core.models.serializable import Serializable\nfrom deeppavlov.models.morpho_tagger.common_tagger import make_pos_and_tag\n\n\nclass WordIndexVectorizer(Serializable, Component):\n    """"""\n    A basic class for custom word-level vectorizers\n    """"""\n\n    def __init__(self, save_path: str, load_path: Union[str, List[str]], **kwargs) -> None:\n        Serializable.__init__(self, save_path, load_path, **kwargs)\n\n    @property\n    @abstractmethod\n    def dim(self):\n        raise NotImplementedError(""You should implement dim property in your WordIndexVectorizer subclass."")\n\n    def _get_word_indexes(self, word: AnyStr) -> List:\n        """"""\n        Transforms a word to corresponding vector of indexes\n        """"""\n        raise NotImplementedError(""You should implement get_word_indexes function ""\n                                  ""in your WordIndexVectorizer subclass."")\n\n    def __call__(self, data: List) -> np.ndarray:\n        """"""\n        Transforms words to one-hot encoding according to the dictionary.\n\n        Args:\n            data: the batch of words\n\n        Returns:\n            a 3D array. answer[i][j][k] = 1 iff data[i][j] is the k-th word in the dictionary.\n        """"""\n        # if isinstance(data[0], str):\n        #     data = [[x for x in re.split(""(\\w+|[,.])"", elem) if x.strip() != """"] for elem in data]\n        max_length = max(len(x) for x in data)\n        answer = np.zeros(shape=(len(data), max_length, self.dim), dtype=int)\n        for i, sent in enumerate(data):\n            for j, word in enumerate(sent):\n                answer[i, j][self._get_word_indexes(word)] = 1\n        return answer\n\n\n@register(""dictionary_vectorizer"")\nclass DictionaryVectorizer(WordIndexVectorizer):\n    """"""\n    Transforms words into 0-1 vector of its possible tags, read from a vocabulary file.\n    The format of the vocabulary must be word<TAB>tag_1<SPACE>...<SPACE>tag_k\n\n    Args:\n        save_path: path to save the vocabulary,\n        load_path: path to the vocabulary(-ies),\n        min_freq: minimal frequency of tag to memorize this tag,\n        unk_token: unknown token to be yielded for unknown words\n    """"""\n\n    def __init__(self, save_path: str, load_path: Union[str, List[str]],\n                 min_freq: int = 1, unk_token: str = None, **kwargs) -> None:\n        super().__init__(save_path, load_path, **kwargs)\n        self.min_freq = min_freq\n        self.unk_token = unk_token\n        self.load()\n\n    @property\n    def dim(self):\n        return len(self._t2i)\n\n    def save(self) -> None:\n        """"""Saves the dictionary to self.save_path""""""\n        with self.save_path.open(""w"", encoding=""utf8"") as fout:\n            for word, curr_labels in sorted(self.word_tag_mapping.items()):\n                curr_labels = [self._i2t[index] for index in curr_labels]\n                curr_labels = [x for x in curr_labels if x != self.unk_token]\n                fout.write(""{}\\t{}"".format(word, "" "".join(curr_labels)))\n\n    def load(self) -> None:\n        """"""Loads the dictionary from self.load_path""""""\n        if not isinstance(self.load_path, list):\n            self.load_path = [self.load_path]\n        for i, path in enumerate(self.load_path):\n            if isinstance(path, str):\n                self.load_path[i] = pathlib.Path(path)\n        labels_by_words = defaultdict(set)\n        for infile in self.load_path:\n            with infile.open(""r"", encoding=""utf8"") as fin:\n                for line in fin:\n                    line = line.strip()\n                    if line.count(""\\t"") != 1:\n                        continue\n                    word, labels = line.split(""\\t"")\n                    labels_by_words[word].update(labels.split())\n        self._initialize(labels_by_words)\n\n    def _initialize(self, labels_by_words: Dict):\n        self._i2t = [self.unk_token] if self.unk_token is not None else []\n        self._t2i = defaultdict(lambda: self.unk_token)\n        freq = defaultdict(int)\n        for word, labels in labels_by_words.items():\n            for label in labels:\n                freq[label] += 1\n        self._i2t += [label for label, count in freq.items() if count >= self.min_freq]\n        for i, label in enumerate(self._i2t):\n            self._t2i[label] = i\n        if self.unk_token is not None:\n            self.word_tag_mapping = defaultdict(lambda: [self.unk_token])\n        else:\n            self.word_tag_mapping = defaultdict(list)\n        for word, labels in labels_by_words.items():\n            labels = {self._t2i[label] for label in labels}\n            self.word_tag_mapping[word] = [x for x in labels if x is not None]\n        return self\n\n    def _get_word_indexes(self, word: AnyStr):\n        return self.word_tag_mapping[word]\n\n\n@register(""pymorphy_vectorizer"")\nclass PymorphyVectorizer(WordIndexVectorizer):\n    """"""\n    Transforms russian words into 0-1 vector of its possible Universal Dependencies tags.\n    Tags are obtained using Pymorphy analyzer (pymorphy2.readthedocs.io)\n    and transformed to UD2.0 format using russian-tagsets library (https://github.com/kmike/russian-tagsets).\n    All UD2.0 tags that are compatible with produced tags are memorized.\n    The list of possible Universal Dependencies tags is read from a file,\n    which contains all the labels that occur in UD2.0 SynTagRus dataset.\n\n    Args:\n        save_path: path to save the tags list,\n        load_path: path to load the list of tags,\n        max_pymorphy_variants: maximal number of pymorphy parses to be used. If -1, all parses are used.\n    """"""\n\n    USELESS_KEYS = [""Abbr""]\n    VALUE_MAP = {""Ptan"": ""Plur"", ""Brev"": ""Short""}\n\n    def __init__(self, save_path: str, load_path: str, max_pymorphy_variants: int = -1, **kwargs) -> None:\n        super().__init__(save_path, load_path, **kwargs)\n        self.max_pymorphy_variants = max_pymorphy_variants\n        self.load()\n        self.memorized_word_indexes = dict()\n        self.memorized_tag_indexes = dict()\n        self.analyzer = MorphAnalyzer()\n        self.converter = converters.converter(\'opencorpora-int\', \'ud20\')\n\n    @property\n    def dim(self):\n        return len(self._t2i)\n\n    def save(self) -> None:\n        """"""Saves the dictionary to self.save_path""""""\n        with self.save_path.open(""w"", encoding=""utf8"") as fout:\n            fout.write(""\\n"".join(self._i2t))\n\n    def load(self) -> None:\n        """"""Loads the dictionary from self.load_path""""""\n        self._i2t = []\n        with self.load_path.open(""r"", encoding=""utf8"") as fin:\n            for line in fin:\n                line = line.strip()\n                if line == """":\n                    continue\n                self._i2t.append(line)\n        self._t2i = {tag: i for i, tag in enumerate(self._i2t)}\n        self._make_tag_trie()\n\n    def _make_tag_trie(self):\n        self._nodes = [defaultdict(dict)]\n        self._start_nodes_for_pos = dict()\n        self._data = [None]\n        for tag, code in self._t2i.items():\n            pos, tag = make_pos_and_tag(tag, sep="","", return_mode=""sorted_items"")\n            start = self._start_nodes_for_pos.get(pos)\n            if start is None:\n                start = self._start_nodes_for_pos[pos] = len(self._nodes)\n                self._nodes.append(defaultdict(dict))\n                self._data.append(None)\n            for key, value in tag:\n                values_dict = self._nodes[start][key]\n                child = values_dict.get(value)\n                if child is None:\n                    child = values_dict[value] = len(self._nodes)\n                    self._nodes.append(defaultdict(dict))\n                    self._data.append(None)\n                start = child\n            self._data[start] = code\n        return self\n\n    def find_compatible(self, tag: str) -> List[int]:\n        """"""\n        Transforms a Pymorphy tag to a list of indexes of compatible UD tags.\n\n        Args:\n            tag: input Pymorphy tag\n\n        Returns:\n            indexes of compatible UD tags\n        """"""\n        if "" "" in tag and ""_"" not in tag:\n            pos, tag = tag.split("" "", maxsplit=1)\n            tag = sorted([tuple(elem.split(""="")) for elem in tag.split(""|"")])\n        else:\n            pos, tag = tag.split()[0], []\n        if pos not in self._start_nodes_for_pos:\n            return []\n        tag = [(key, self.VALUE_MAP.get(value, value)) for key, value in tag\n               if key not in self.USELESS_KEYS]\n        if len(tag) > 0:\n            curr_nodes = [(0, self._start_nodes_for_pos[pos])]\n            final_nodes = []\n        else:\n            final_nodes = [self._start_nodes_for_pos[pos]]\n            curr_nodes = []\n        while len(curr_nodes) > 0:\n            i, node_index = curr_nodes.pop()\n            # key, value = tag[i]\n            node = self._nodes[node_index]\n            if len(node) == 0:\n                final_nodes.append(node_index)\n            for curr_key, curr_values_dict in node.items():\n                curr_i, curr_node_index = i, node_index\n                while curr_i < len(tag) and tag[curr_i][0] < curr_key:\n                    curr_i += 1\n                if curr_i == len(tag):\n                    final_nodes.extend(curr_values_dict.values())\n                    continue\n                key, value = tag[curr_i]\n                if curr_key < key:\n                    for child in curr_values_dict.values():\n                        curr_nodes.append((curr_i, child))\n                else:\n                    child = curr_values_dict.get(value)\n                    if child is not None:\n                        if curr_i < len(tag) - 1:\n                            curr_nodes.append((curr_i + 1, child))\n                        else:\n                            final_nodes.append(child)\n        answer = []\n        while len(final_nodes) > 0:\n            index = final_nodes.pop()\n            if self._data[index] is not None:\n                answer.append(self._data[index])\n            for elem in self._nodes[index].values():\n                final_nodes.extend(elem.values())\n        return answer\n\n    def _get_word_indexes(self, word):\n        answer = self.memorized_word_indexes.get(word)\n        if answer is None:\n            parse = self.analyzer.parse(word)\n            if self.max_pymorphy_variants > 0:\n                parse = parse[:self.max_pymorphy_variants]\n            tag_indexes = set()\n            for elem in parse:\n                tag_indexes.update(set(self._get_tag_indexes(elem.tag)))\n            answer = self.memorized_word_indexes[word] = list(tag_indexes)\n        return answer\n\n    def _get_tag_indexes(self, pymorphy_tag):\n        answer = self.memorized_tag_indexes.get(pymorphy_tag)\n        if answer is None:\n            tag = self.converter(str(pymorphy_tag))\n            answer = self.memorized_tag_indexes[pymorphy_tag] = self.find_compatible(tag)\n        return answer\n'"
deeppavlov/skills/aiml_skill/__init__.py,0,b'from .aiml_skill import AIMLSkill\n'
deeppavlov/skills/aiml_skill/aiml_skill.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport uuid\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import Tuple, Optional, List\n\nimport aiml\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\n\nlog = getLogger(__name__)\n\n\n@register(""aiml_skill"")\nclass AIMLSkill(Component):\n    """"""Skill wraps python-aiml library into DeepPavlov interfrace.\n    AIML uses directory with AIML scripts which are loaded at initialization and used as patterns\n    for answering at each step.\n    """"""\n\n    def __init__(self,\n                 path_to_aiml_scripts: str,\n                 positive_confidence: float = 0.66,\n                 null_response: str = ""I don\'t know what to answer you"",\n                 null_confidence: float = 0.33,\n                 **kwargs\n                 ) -> None:\n        """"""\n        Construct skill:\n            read AIML scripts,\n            load AIML kernel\n\n        Args:\n            path_to_aiml_scripts: string path to folder with AIML scripts\n            null_response: Response string to answer if no AIML Patterns matched\n            positive_confidence: The confidence of response if response was found in AIML scripts\n            null_confidence: The confidence when AIML scripts has no rule for responding and system returns null_response\n        """"""\n        # we need absolute path (expanded for user home and resolved if it relative path):\n        self.path_to_aiml_scripts = Path(path_to_aiml_scripts).expanduser().resolve()\n        log.info(f""path_to_aiml_scripts is: `{self.path_to_aiml_scripts}`"")\n\n        self.positive_confidence = positive_confidence\n        self.null_confidence = null_confidence\n        self.null_response = null_response\n        self.kernel = aiml.Kernel()\n        # to block AIML output:\n        self.kernel._verboseMode = False\n        self._load_scripts()\n\n    def _load_scripts(self) -> None:\n        """"""\n        Scripts are loaded recursively from files with extensions .xml and .aiml\n        Returns: None\n\n        """"""\n        # learn kernel to all aimls in directory tree:\n        all_files = sorted(self.path_to_aiml_scripts.rglob(\'*.*\'))\n        learned_files = []\n        for each_file_path in all_files:\n            if each_file_path.suffix in [\'.aiml\', \'.xml\']:\n                # learn the script file\n                self.kernel.learn(str(each_file_path))\n                learned_files.append(each_file_path)\n        if not learned_files:\n            log.warning(f""No .aiml or .xml files found for AIML Kernel in directory {self.path_to_aiml_scripts}"")\n\n    def process_step(self, utterance_str: str, user_id: any) -> Tuple[str, float]:\n        response = self.kernel.respond(utterance_str, sessionID=user_id)\n        # here put your estimation of confidence:\n        if response:\n            # print(f""AIML responds: {response}"")\n            confidence = self.positive_confidence\n        else:\n            # print(""AIML responses silently..."")\n            response = self.null_response\n            confidence = self.null_confidence\n        return response, confidence\n\n    def _generate_user_id(self) -> str:\n        """"""Here you put user id generative logic if you want to implement it in the skill.\n\n        Returns:\n            user_id: Random generated user ID.\n\n        """"""\n        return uuid.uuid1().hex\n\n    def __call__(self,\n                 utterances_batch: List[str],\n                 states_batch: Optional[List] = None) -> Tuple[List[str], List[float], list]:\n        """"""Returns skill inference result.\n\n        Returns batches of skill inference results, estimated confidence\n        levels and up to date states corresponding to incoming utterance\n        batch.\n\n        Args:\n            utterances_batch: A batch of utterances of str type.\n            states_batch:  A batch of arbitrary typed states for\n                each utterance.\n\n\n        Returns:\n            response: A batch of arbitrary typed skill inference results.\n            confidence: A batch of float typed confidence levels for each of\n                skill inference result.\n            output_states_batch:  A batch of arbitrary typed states for\n                each utterance.\n\n        """"""\n        # grasp user_ids from states batch.\n        # We expect that skill receives None or dict of state for each utterance.\n        # if state has user_id then skill uses it, otherwise it generates user_id and calls the\n        # user with this name in further.\n\n        # In this implementation we use current datetime for generating uniqe ids\n        output_states_batch = []\n        user_ids = []\n        if states_batch is None:\n            # generate states batch matching batch of utterances:\n            states_batch = [None] * len(utterances_batch)\n\n        for state in states_batch:\n            if not state:\n                user_id = self._generate_user_id()\n                new_state = {\'user_id\': user_id}\n\n            elif \'user_id\' not in state:\n                new_state = state\n                user_id = self._generate_user_id()\n                new_state[\'user_id\'] = self._generate_user_id()\n\n            else:\n                new_state = state\n                user_id = new_state[\'user_id\']\n\n            user_ids.append(user_id)\n            output_states_batch.append(new_state)\n\n        confident_responses = map(self.process_step, utterances_batch, user_ids)\n        responses_batch, confidences_batch = zip(*confident_responses)\n\n        return responses_batch, confidences_batch, output_states_batch\n'"
deeppavlov/skills/dsl_skill/__init__.py,0,"b'from .context import UserContext\nfrom .dsl_skill import DSLMeta\nfrom .utils import SkillResponse, UserId\n'"
deeppavlov/skills/dsl_skill/context.py,0,"b'# Copyright 2019 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nfrom typing import Optional, Union, Dict\n\nfrom deeppavlov.skills.dsl_skill.utils import UserId\n\n\nclass UserContext:\n    """"""\n    UserContext object stores information that the current skill currently knows about the user.\n\n    Args:\n        user_id: id of user\n        message: current message\n        current_state: current user state\n        payload: custom payload dictionary, or a JSON-serialized string of such dictionary\n\n    Attributes:\n        handler_payload: stores information generated by the selected handler\n\n    """"""\n\n    def __init__(\n            self,\n            user_id: Optional[UserId] = None,\n            message: Optional[str] = None,\n            current_state: Optional[str] = None,\n            payload: Optional[Union[Dict, str]] = None,\n    ):\n        self.user_id = user_id\n        self.message = message\n        self.current_state = current_state\n        self.handler_payload = {}\n\n        # some custom data added by skill creator\n        self.payload = payload\n        if payload == \'\' or payload is None:\n            self.payload = {}\n        elif isinstance(payload, str):\n            self.payload = json.loads(payload)\n'"
deeppavlov/skills/dsl_skill/dsl_skill.py,0,"b'# Copyright 2019 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom abc import ABCMeta\nfrom collections import defaultdict\nfrom functools import partial\nfrom itertools import zip_longest, starmap\nfrom typing import List, Optional, Dict, Callable, Tuple\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.skills.dsl_skill.context import UserContext\nfrom deeppavlov.skills.dsl_skill.handlers.handler import Handler\nfrom deeppavlov.skills.dsl_skill.handlers.regex_handler import RegexHandler\nfrom deeppavlov.skills.dsl_skill.utils import SkillResponse, UserId\n\n\nclass DSLMeta(ABCMeta):\n    """"""\n    This metaclass is used for creating a skill. Skill is register by its class name in registry.\n\n    Example:\n\n    .. code:: python\n\n            class ExampleSkill(metaclass=DSLMeta):\n                @DSLMeta.handler(commands=[""hello"", ""hey""])\n                def __greeting(context: UserContext):\n                    response = ""Hello, my friend!""\n                    confidence = 1.0\n                    return response, confidence\n\n    Attributes:\n        name: class name\n        state_to_handler: dict with states as keys and lists of Handler objects as values\n        user_to_context: dict with user ids as keys and UserContext objects as values\n        universal_handlers: list of handlers that can be activated from any state\n\n    """"""\n    skill_collection: Dict[str, \'DSLMeta\'] = {}\n\n    def __init__(cls, name: str,\n                 bases,\n                 namespace,\n                 **kwargs):\n        super().__init__(name, bases, namespace, **kwargs)\n        cls.name = name\n        cls.state_to_handler = defaultdict(list)\n        cls.user_to_context = defaultdict(UserContext)\n        cls.universal_handlers = []\n\n        handlers = [attribute for attribute in namespace.values() if isinstance(attribute, Handler)]\n\n        for handler in handlers:\n            if handler.state is None:\n                cls.universal_handlers.append(handler)\n            else:\n                cls.state_to_handler[handler.state].append(handler)\n\n        cls.handle = partial(DSLMeta.__handle, cls)\n        cls.__call__ = partial(DSLMeta.__handle_batch, cls)\n        cls.__init__ = partial(DSLMeta.__init__class, cls)\n        register()(cls)\n        DSLMeta.__add_to_collection(cls)\n\n    def __init__class(cls,\n                      on_invalid_command: str = ""\xd0\x9f\xd1\x80\xd0\xbe\xd1\x81\xd1\x82\xd0\xb8\xd1\x82\xd0\xb5, \xd1\x8f \xd0\xb2\xd0\xb0\xd1\x81 \xd0\xbd\xd0\xb5 \xd0\xbf\xd0\xbe\xd0\xbd\xd1\x8f\xd0\xbb"",\n                      null_confidence: float = 0,\n                      *args, **kwargs) -> None:\n        """"""\n        Initialize Skill class\n\n        Args:\n            on_invalid_command: message to be sent on message with no associated handler\n            null_confidence: the confidence when DSL has no handler that fits request\n        """"""\n        # message to be sent on message with no associated handler\n        cls.on_invalid_command = on_invalid_command\n        cls.null_confidence = null_confidence\n\n    def __handle_batch(cls: \'DSLMeta\',\n                       utterances_batch: List[str],\n                       user_ids_batch: List[UserId]) -> Tuple[List, ...]:\n        """"""Returns skill inference result.\n        Returns batches of skill inference results, estimated confidence\n        levels and up to date states corresponding to incoming utterance\n        batch.\n\n        Args:\n            utterances_batch: A batch of utterances of str type.\n            user_ids_batch: A batch of user ids.\n\n        Returns:\n            response_batch: A batch of arbitrary typed skill inference results.\n            confidence_batch: A batch of float typed confidence levels for each of\n                skill inference result.\n\n        """"""\n        return (*map(list, zip(*starmap(cls.handle, zip_longest(utterances_batch, user_ids_batch)))),)\n\n    @staticmethod\n    def __add_to_collection(cls: \'DSLMeta\') -> None:\n        """"""\n        Adds Skill class to Skill classes collection\n\n        Args:\n            cls: Skill class\n\n        """"""\n        DSLMeta.skill_collection[cls.name] = cls\n\n    @staticmethod\n    def __handle(cls: \'DSLMeta\',\n                 utterance: str,\n                 user_id: UserId) -> SkillResponse:\n        """"""\n        Handles what is going to be after a message from user arrived.\n        Simple usage:\n        skill([<message>], [<user_id>])\n\n        Args:\n            cls: instance of callee\'s class\n            utterance: a message to be handled\n            user_id: id of a user\n\n        Returns:\n            result: handler function\'s result if succeeded\n\n        """"""\n        context = cls.user_to_context[user_id]\n\n        context.user_id = user_id\n        context.message = utterance\n\n        current_handler = cls.__select_handler(context)\n        return cls.__run_handler(current_handler, context)\n\n    def __select_handler(cls,\n                         context: UserContext) -> Optional[Callable]:\n        """"""\n        Selects handler with the highest priority that could be triggered from the passed context.\n\n        Returns:\n             handler function that is selected and None if no handler fits request\n\n        """"""\n        available_handlers = cls.state_to_handler[context.current_state]\n        available_handlers.extend(cls.universal_handlers)\n        available_handlers.sort(key=lambda h: h.priority, reverse=True)\n        for handler in available_handlers:\n            if handler.check(context):\n                handler.expand_context(context)\n                return handler.func\n\n    def __run_handler(cls, handler: Optional[Callable],\n                      context: UserContext) -> SkillResponse:\n        """"""\n        Runs specified handler for current context\n\n        Args:\n            handler: handler to be run. If None, on_invalid_command is returned\n            context: user context\n\n        Returns:\n             SkillResponse\n\n        """"""\n        if handler is None:\n            return SkillResponse(cls.on_invalid_command, cls.null_confidence)\n        try:\n            return SkillResponse(*handler(context=context))\n        except Exception as exc:\n            return SkillResponse(str(exc), 1.0)\n\n    @staticmethod\n    def handler(commands: Optional[List[str]] = None,\n                state: Optional[str] = None,\n                context_condition: Optional[Callable] = None,\n                priority: int = 0) -> Callable:\n        """"""\n        Decorator to be used in skills\' classes.\n        Sample usage:\n\n        .. code:: python\n\n            class ExampleSkill(metaclass=DSLMeta):\n                @DSLMeta.handler(commands=[""hello"", ""hi"", ""sup"", ""greetings""])\n                def __greeting(context: UserContext):\n                    response = ""Hello, my friend!""\n                    confidence = 1.0\n                    return response, confidence\n\n        Args:\n            priority: integer value to indicate priority. If multiple handlers satisfy\n                          all the requirements, the handler with the greatest priority value will be used\n            context_condition: function that takes context and\n                                  returns True if this handler should be enabled\n                                  and False otherwise. If None, no condition is checked\n            commands: phrases/regexs on what the function wrapped\n                         by this decorator will trigger\n            state: state name\n\n        Returns:\n            function decorated into Handler class\n\n        """"""\n        if commands is None:\n            commands = ["".*""]\n\n        def decorator(func: Callable) -> Handler:\n            return RegexHandler(func, commands,\n                                context_condition=context_condition,\n                                priority=priority, state=state)\n\n        return decorator\n'"
deeppavlov/skills/dsl_skill/utils.py,0,"b'# Copyright 2019 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Union, NamedTuple\n\nUserId = Union[str, int]\n\n\nclass SkillResponse(NamedTuple):\n    response: str\n    confidence: float\n'"
deeppavlov/skills/rasa_skill/__init__.py,0,b'from .rasa_skill import RASASkill\n'
deeppavlov/skills/rasa_skill/rasa_skill.py,0,"b'# Copyright 2019 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport asyncio\nimport logging\nimport uuid\nfrom functools import reduce\nfrom pathlib import Path\nfrom typing import Tuple, Optional, List\n\nfrom rasa.cli.utils import get_validated_path\nfrom rasa.constants import DEFAULT_MODELS_PATH\nfrom rasa.core.agent import Agent\nfrom rasa.core.channels import CollectingOutputChannel\nfrom rasa.core.channels import UserMessage\nfrom rasa.model import get_model\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\n\nlogger = logging.getLogger(__name__)\n\n\n@register(""rasa_skill"")\nclass RASASkill(Component):\n    """"""RASASkill lets you to wrap RASA Agent as a Skill within DeepPavlov environment.\n\n    The component requires path to your RASA models (folder with timestamped tar.gz archieves)\n    as you use in command `rasa run -m models --enable-api --log-file out.log`\n\n    """"""\n\n    def __init__(self, path_to_models: str, **kwargs) -> None:\n        """"""\n        Constructs RASA Agent as a DeepPavlov skill:\n            read model folder,\n            initialize rasa.core.agent.Agent and wrap it\'s interfaces\n\n        Args:\n            path_to_models: string path to folder with RASA models\n\n        """"""\n        # we need absolute path (expanded for user home and resolved if it relative path):\n        self.path_to_models = Path(path_to_models).expanduser().resolve()\n\n        model = get_validated_path(self.path_to_models, ""model"", DEFAULT_MODELS_PATH)\n\n        model_path = get_model(model)\n        if not model_path:\n            # can not laod model path\n            raise Exception(""can not load model path: %s"" % model)\n\n        self._agent = Agent.load(model_path)\n        self.ioloop = asyncio.new_event_loop()\n        logger.info(f""path to RASA models is: `{self.path_to_models}`"")\n\n    def __call__(self,\n                 utterances_batch: List[str],\n                 states_batch: Optional[List] = None) -> Tuple[List[str], List[float], list]:\n        """"""Returns skill inference result.\n\n        Returns batches of skill inference results, estimated confidence\n        levels and up to date states corresponding to incoming utterance\n        batch.\n\n        Args:\n            utterances_batch: A batch of utterances of str type.\n            states_batch:  A batch of arbitrary typed states for\n                each utterance.\n\n\n        Returns:\n            response: A batch of arbitrary typed skill inference results.\n            confidence: A batch of float typed confidence levels for each of\n                skill inference result.\n            output_states_batch:  A batch of arbitrary typed states for\n                each utterance.\n\n        """"""\n        user_ids, output_states_batch = self._handle_user_identification(utterances_batch, states_batch)\n        #################################################################################\n        # RASA use asyncio for handling messages and handle_text is async function,\n        # so we need to instantiate event loop\n        # futures = [rasa_confident_response_decorator(self._agent, utt, sender_id=uid) for utt, uid in\n        futures = [self.rasa_confident_response_decorator(self._agent, utt, sender_id=uid) for utt, uid in\n                   zip(utterances_batch, user_ids)]\n\n        asyncio.set_event_loop(self.ioloop)\n        results = self.ioloop.run_until_complete(asyncio.gather(*futures))\n\n        responses_batch, confidences_batch = zip(*results)\n        return responses_batch, confidences_batch, output_states_batch\n\n    async def rasa_confident_response_decorator(self, rasa_agent, text_message, sender_id):\n        """"""\n        Args:\n            rasa_agent: rasa.core.agent.Agent instance\n            text_message: str with utterance from user\n            sender_id: id of the user\n\n        Returns: None or tuple with str and float, where first element is a message and second is\n            confidence\n        """"""\n\n        resp = await self.rasa_handle_text_verbosely(rasa_agent, text_message, sender_id)\n        if resp:\n            responses, confidences, actions = resp\n        else:\n            logger.warning(""Null response from RASA Skill"")\n            return None\n\n        # for adaptation to deep pavlov arch we need to merge multi-messages into single string:\n        texts = [each_resp[\'text\'] for each_resp in responses if \'text\' in each_resp]\n        merged_message = ""\\n"".join(texts)\n\n        merged_confidence = reduce(lambda a, b: a * b, confidences)\n        # TODO possibly it better to choose another function for calculation of final confidence\n        # current realisation of confidence propagation may cause confidence decay for long actions\n        # chains. If long chains is your case, try max(confidence) or confidence[0]\n        return merged_message, merged_confidence\n\n    async def rasa_handle_text_verbosely(self, rasa_agent, text_message, sender_id):\n        """"""\n        This function reimplements RASA\'s rasa.core.agent.Agent.handle_text method to allow to retrieve\n        message responses with confidence estimation altogether.\n\n        It reconstructs with merge RASA\'s methods:\n        https://github.com/RasaHQ/rasa_core/blob/master/rasa/core/agent.py#L401\n        https://github.com/RasaHQ/rasa_core/blob/master/rasa/core/agent.py#L308\n        https://github.com/RasaHQ/rasa/blob/master/rasa/core/processor.py#L327\n\n        This required to allow RASA to output confidences with actions altogether\n        (Out of the box RASA does not support such use case).\n\n        Args:\n            rasa_agent: rasa.core.agent.Agent instance\n            text_message: str with utterance from user\n            sender_id: id of the user\n\n        Returns: None or\n            tuple where first element is a list of messages dicts, the second element is a list\n                of confidence scores for all actions (it is longer than messages list, because some actions\n                does not produce messages)\n\n        """"""\n        message = UserMessage(text_message,\n                              output_channel=None,\n                              sender_id=sender_id)\n\n        processor = rasa_agent.create_processor()\n        tracker = processor._get_tracker(message.sender_id)\n\n        confidences = []\n        actions = []\n        await processor._handle_message_with_tracker(message, tracker)\n        # save tracker state to continue conversation from this state\n        processor._save_tracker(tracker)\n\n        # here we restore some of logic in RASA management.\n        # ###### Loop of IntraStep decisions  ##########################################################\n        # await processor._predict_and_execute_next_action(msg, tracker):\n        # https://github.com/RasaHQ/rasa/blob/master/rasa/core/processor.py#L327-L362\n        # keep taking actions decided by the policy until it chooses to \'listen\'\n        should_predict_another_action = True\n        num_predicted_actions = 0\n\n        def is_action_limit_reached():\n            return (num_predicted_actions == processor.max_number_of_predictions and\n                    should_predict_another_action)\n\n        # action loop. predicts actions until we hit action listen\n        while (should_predict_another_action and\n               processor._should_handle_message(tracker) and\n               num_predicted_actions < processor.max_number_of_predictions):\n            # this actually just calls the policy\'s method by the same name\n            action, policy, confidence = processor.predict_next_action(tracker)\n\n            confidences.append(confidence)\n            actions.append(action)\n\n            should_predict_another_action = await processor._run_action(\n                action,\n                tracker,\n                message.output_channel,\n                processor.nlg,\n                policy, confidence\n            )\n            num_predicted_actions += 1\n\n        if is_action_limit_reached():\n            # circuit breaker was tripped\n            logger.warning(\n                ""Circuit breaker tripped. Stopped predicting ""\n                ""more actions for sender \'{}\'"".format(tracker.sender_id))\n            if processor.on_circuit_break:\n                # call a registered callback\n                processor.on_circuit_break(tracker, message.output_channel, processor.nlg)\n\n        if isinstance(message.output_channel, CollectingOutputChannel):\n\n            return message.output_channel.messages, confidences, actions\n        else:\n            return None\n\n    def _generate_user_id(self) -> str:\n        """"""\n        Here you put user id generative logic if you want to implement it in the skill.\n\n        Although it is better to delegate user_id generation to Agent Layer\n        Returns: str\n\n        """"""\n        return uuid.uuid1().hex\n\n    def _handle_user_identification(self, utterances_batch, states_batch):\n        """"""Method preprocesses states batch to guarantee that all users are identified (or\n        identifiers are generated for all users).\n\n        Args:\n            utterances_batch: batch of utterances\n            states_batch: batch of states\n\n        Returns:\n\n        """"""\n        # grasp user_ids from states batch.\n        # We expect that skill receives None or dict of state for each utterance.\n        # if state has user_id then skill uses it, otherwise it generates user_id and calls the\n        # user with this name in further.\n\n        # In this implementation we use current datetime for generating uniqe ids\n        output_states_batch = []\n        user_ids = []\n        if states_batch is None:\n            # generate states batch matching batch of utterances:\n            states_batch = [None] * len(utterances_batch)\n\n        for state in states_batch:\n            if not state:\n                user_id = self._generate_user_id()\n                new_state = {\'user_id\': user_id}\n\n            elif \'user_id\' not in state:\n                new_state = state\n                user_id = self._generate_user_id()\n                new_state[\'user_id\'] = self._generate_user_id()\n\n            else:\n                new_state = state\n                user_id = new_state[\'user_id\']\n\n            user_ids.append(user_id)\n            output_states_batch.append(new_state)\n        return user_ids, output_states_batch\n\n    def destroy(self):\n        self.ioloop.close()\n        super().destroy()\n'"
deeppavlov/utils/agent/__init__.py,0,b'from .server import start_rabbit_service\n'
deeppavlov/utils/agent/messages.py,0,"b'# Copyright 2019 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Module contains classes defining messages received and sent by service via RabbitMQ message broker.\n\nThe classes created to document the DeepPavlov Agent API and should match the corresponding classes\nfrom https://github.com/deepmipt/dp-agent/blob/master/core/transport/messages.py\n\n""""""\n\nfrom typing import Any\n\n\nclass MessageBase:\n    agent_name: str\n    msg_type: str\n\n    def __init__(self, msg_type: str, agent_name: str) -> None:\n        self.msg_type = msg_type\n        self.agent_name = agent_name\n\n    @classmethod\n    def from_json(cls, message_json: dict):\n        return cls(**message_json)\n\n    def to_json(self) -> dict:\n        return self.__dict__\n\n\nclass ServiceTaskMessage(MessageBase):\n    payload: dict\n\n    def __init__(self, agent_name: str, payload: dict) -> None:\n        super().__init__(\'service_task\', agent_name)\n        self.payload = payload\n\n\nclass ServiceResponseMessage(MessageBase):\n    response: Any\n    task_id: str\n\n    def __init__(self, task_id: str, agent_name: str, response: Any) -> None:\n        super().__init__(\'service_response\', agent_name)\n        self.task_id = task_id\n        self.response = response\n\n\nclass ServiceErrorMessage(MessageBase):\n    formatted_exc: str\n\n    def __init__(self, task_id: str, agent_name: str, formatted_exc: str) -> None:\n        super().__init__(\'error\', agent_name)\n        self.task_id = task_id\n        self.formatted_exc = formatted_exc\n\n    @property\n    def exception(self) -> Exception:\n        return Exception(self.formatted_exc)\n\n\ndef get_service_task_message(message_json: dict) -> ServiceTaskMessage:\n    """"""Creates an instance of ServiceTaskMessage class using its json representation.\n\n    Args:\n        message_json: Dictionary with class fields.\n\n    Returns:\n        New ServiceTaskMessage instance.\n\n    Raises:\n        ValueError if dict with instance fields isn\'t from an instance of ServiceTaskMessage class.\n\n    """"""\n    message_type = message_json.pop(\'msg_type\')\n\n    if message_type != \'service_task\':\n        raise TypeError(f\'Unknown transport message type: {message_type}\')\n\n    return ServiceTaskMessage.from_json(message_json)\n'"
deeppavlov/utils/agent/rabbitmq.py,0,"b'# Copyright 2019 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport asyncio\nimport json\nimport logging\nimport time\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom traceback import format_exc\nfrom typing import Any, Dict, List, Optional, Union\n\nimport aio_pika\nfrom aio_pika import Connection, Channel, Exchange, Queue, IncomingMessage, Message\n\nfrom deeppavlov.core.commands.infer import build_model\nfrom deeppavlov.core.common.chainer import Chainer\nfrom deeppavlov.core.data.utils import jsonify_data\nfrom deeppavlov.utils.agent.messages import ServiceTaskMessage, ServiceResponseMessage, ServiceErrorMessage\nfrom deeppavlov.utils.agent.messages import get_service_task_message\nfrom deeppavlov.utils.connector import DialogLogger\nfrom deeppavlov.utils.server import get_server_params\n\ndialog_logger = DialogLogger(logger_name=\'agent_rabbit\')\nlog = logging.getLogger(__name__)\n\nAGENT_IN_EXCHANGE_NAME_TEMPLATE = \'{agent_namespace}_e_in\'\nAGENT_OUT_EXCHANGE_NAME_TEMPLATE = \'{agent_namespace}_e_out\'\nAGENT_ROUTING_KEY_TEMPLATE = \'agent.{agent_name}\'\n\nSERVICE_QUEUE_NAME_TEMPLATE = \'{agent_namespace}_q_service_{service_name}\'\nSERVICE_ROUTING_KEY_TEMPLATE = \'service.{service_name}\'\n\n\nclass RabbitMQServiceGateway:\n    """"""Class object connects to the RabbitMQ broker to process requests from the DeepPavlov Agent.""""""\n    _add_to_buffer_lock: asyncio.Lock\n    _infer_lock: asyncio.Lock\n    _model: Chainer\n    _model_args_names: List[str]\n    _incoming_messages_buffer: List[IncomingMessage]\n    _batch_size: int\n    _utterance_lifetime_sec: int\n    _in_queue: Optional[Queue]\n    _connection: Connection\n    _agent_in_exchange: Exchange\n    _agent_out_exchange: Exchange\n    _agent_in_channel: Channel\n    _agent_out_channel: Channel\n\n    def __init__(self,\n                 model_config: Union[str, Path],\n                 service_name: str,\n                 agent_namespace: str,\n                 batch_size: int,\n                 utterance_lifetime_sec: int,\n                 rabbit_host: str,\n                 rabbit_port: int,\n                 rabbit_login: str,\n                 rabbit_password: str,\n                 rabbit_virtualhost: str,\n                 loop: asyncio.AbstractEventLoop) -> None:\n        self._add_to_buffer_lock = asyncio.Lock()\n        self._infer_lock = asyncio.Lock()\n        server_params = get_server_params(model_config)\n        self._model_args_names = server_params[\'model_args_names\']\n        self._model = build_model(model_config)\n        self._in_queue = None\n        self._utterance_lifetime_sec = utterance_lifetime_sec\n        self._batch_size = batch_size\n        self._incoming_messages_buffer = []\n\n        loop.run_until_complete(self._connect(loop=loop, host=rabbit_host, port=rabbit_port, login=rabbit_login,\n                                              password=rabbit_password, virtualhost=rabbit_virtualhost,\n                                              agent_namespace=agent_namespace))\n        loop.run_until_complete(self._setup_queues(service_name, agent_namespace))\n        loop.run_until_complete(self._in_queue.consume(callback=self._on_message_callback))\n\n        log.info(f\'Service in queue started consuming\')\n\n    async def _connect(self,\n                       loop: asyncio.AbstractEventLoop,\n                       host: str,\n                       port: int,\n                       login: str,\n                       password: str,\n                       virtualhost: str,\n                       agent_namespace: str) -> None:\n        """"""Connects to RabbitMQ message broker and initiates agent in and out channels and exchanges.""""""\n        log.info(\'Starting RabbitMQ connection...\')\n\n        while True:\n            try:\n                self._connection = await aio_pika.connect_robust(loop=loop,\n                                                                 host=host,\n                                                                 port=port,\n                                                                 login=login,\n                                                                 password=password,\n                                                                 virtualhost=virtualhost)\n                log.info(\'RabbitMQ connected\')\n                break\n            except ConnectionError:\n                reconnect_timeout = 5\n                log.error(f\'RabbitMQ connection error, making another attempt in {reconnect_timeout} secs\')\n                time.sleep(reconnect_timeout)\n\n        self._agent_in_channel = await self._connection.channel()\n        agent_in_exchange_name = AGENT_IN_EXCHANGE_NAME_TEMPLATE.format(agent_namespace=agent_namespace)\n        self._agent_in_exchange = await self._agent_in_channel.declare_exchange(name=agent_in_exchange_name,\n                                                                                type=aio_pika.ExchangeType.TOPIC)\n        log.info(f\'Declared agent in exchange: {agent_in_exchange_name}\')\n\n        self._agent_out_channel = await self._connection.channel()\n        agent_out_exchange_name = AGENT_OUT_EXCHANGE_NAME_TEMPLATE.format(agent_namespace=agent_namespace)\n        self._agent_out_exchange = await self._agent_in_channel.declare_exchange(name=agent_out_exchange_name,\n                                                                                 type=aio_pika.ExchangeType.TOPIC)\n        log.info(f\'Declared agent out exchange: {agent_out_exchange_name}\')\n\n    def disconnect(self):\n        self._connection.close()\n\n    async def _setup_queues(self, service_name: str, agent_namespace: str) -> None:\n        """"""Setups input queue to get messages from DeepPavlov Agent.""""""\n        in_queue_name = SERVICE_QUEUE_NAME_TEMPLATE.format(agent_namespace=agent_namespace,\n                                                           service_name=service_name)\n\n        self._in_queue = await self._agent_out_channel.declare_queue(name=in_queue_name, durable=True)\n        log.info(f\'Declared service in queue: {in_queue_name}\')\n\n        service_routing_key = SERVICE_ROUTING_KEY_TEMPLATE.format(service_name=service_name)\n        await self._in_queue.bind(exchange=self._agent_out_exchange, routing_key=service_routing_key)\n        log.info(f\'Queue: {in_queue_name} bound to routing key: {service_routing_key}\')\n\n        await self._agent_out_channel.set_qos(prefetch_count=self._batch_size * 2)\n\n    async def _on_message_callback(self, message: IncomingMessage) -> None:\n        """"""Processes messages from the input queue.\n\n        Collects incoming messages to buffer, sends tasks batches for further processing. Depending on the success of\n        the processing result sends negative or positive acknowledgements to the input messages.\n\n        """"""\n        await self._add_to_buffer_lock.acquire()\n        self._incoming_messages_buffer.append(message)\n        log.debug(\'Incoming message received\')\n\n        if len(self._incoming_messages_buffer) < self._batch_size:\n            self._add_to_buffer_lock.release()\n\n        await self._infer_lock.acquire()\n        try:\n            messages_batch = self._incoming_messages_buffer\n            valid_messages_batch: List[IncomingMessage] = []\n            tasks_batch: List[ServiceTaskMessage] = []\n\n            if messages_batch:\n                self._incoming_messages_buffer = []\n\n                if self._add_to_buffer_lock.locked():\n                    self._add_to_buffer_lock.release()\n\n                for message in messages_batch:\n                    try:\n                        task = get_service_task_message(json.loads(message.body, encoding=\'utf-8\'))\n                        tasks_batch.append(task)\n                        valid_messages_batch.append(message)\n                    except Exception as e:\n                        log.error(f\'Failed to get ServiceTaskMessage from the incoming message: {repr(e)}\')\n                        await message.reject()\n\n            elif self._add_to_buffer_lock.locked():\n                self._add_to_buffer_lock.release()\n\n            if tasks_batch:\n                try:\n                    await self._process_tasks(tasks_batch)\n                except Exception as e:\n                    task_ids = [task.payload[""task_id""] for task in tasks_batch]\n                    log.error(f\'got exception {repr(e)} while processing tasks {"", "".join(task_ids)}\')\n                    formatted_exception = format_exc()\n                    error_replies = [self._send_results(task, formatted_exception) for task in tasks_batch]\n                    await asyncio.gather(*error_replies)\n                    for message in valid_messages_batch:\n                        await message.reject()\n                else:\n                    for message in valid_messages_batch:\n                        await message.ack()\n        finally:\n            self._infer_lock.release()\n\n    async def _process_tasks(self, tasks_batch: List[ServiceTaskMessage]) -> None:\n        """"""Gets from tasks batch payloads to infer model, processes them and creates tasks to send results.""""""\n        task_uuids_batch, payloads = \\\n            zip(*[(task.payload[\'task_id\'], task.payload[\'payload\']) for task in tasks_batch])\n\n        log.debug(f\'Prepared to infer tasks {"", "".join(task_uuids_batch)}\')\n\n        responses_batch = await asyncio.wait_for(self._interact(payloads),\n                                                 self._utterance_lifetime_sec)\n\n        results_replies = [self._send_results(task, response) for task, response in zip(tasks_batch, responses_batch)]\n        await asyncio.gather(*results_replies)\n\n        log.debug(f\'Processed tasks {"", "".join(task_uuids_batch)}\')\n\n    async def _interact(self, payloads: List[Dict]) -> List[Any]:\n        """"""Infers model with the batch.""""""\n        batch = defaultdict(list)\n\n        for payload in payloads:\n            for arg_name in self._model_args_names:\n                batch[arg_name].extend(payload.get(arg_name, [None]))\n\n        dialog_logger.log_in(batch)\n\n        prediction = self._model(*batch.values())\n        if len(self._model.out_params) == 1:\n            prediction = [prediction]\n        prediction = list(zip(*prediction))\n        result = jsonify_data(prediction)\n\n        dialog_logger.log_out(result)\n\n        return result\n\n    async def _send_results(self, task: ServiceTaskMessage, response: Union[Dict, str]) -> None:\n        """"""Sends responses batch to the DeepPavlov Agent using agent input exchange.\n\n        Args:\n            task: Task message from DeepPavlov Agent.\n            response: DeepPavlov model response (dict type) if infer was successful otherwise string representation of\n                raised error\n\n        """"""\n        if isinstance(response, dict):\n            result = ServiceResponseMessage(agent_name=task.agent_name,\n                                            task_id=task.payload[""task_id""],\n                                            response=response)\n        else:\n            result = ServiceErrorMessage(agent_name=task.agent_name,\n                                         task_id=task.payload[""task_id""],\n                                         formatted_exc=response)\n\n        message = Message(body=json.dumps(result.to_json()).encode(\'utf-8\'),\n                          delivery_mode=aio_pika.DeliveryMode.PERSISTENT,\n                          expiration=self._utterance_lifetime_sec)\n\n        routing_key = AGENT_ROUTING_KEY_TEMPLATE.format(agent_name=task.agent_name)\n        await self._agent_in_exchange.publish(message=message, routing_key=routing_key)\n        log.debug(f\'Sent response for task {str(task.payload[""task_id""])} with routing key {routing_key}\')\n'"
deeppavlov/utils/agent/server.py,0,"b'# Copyright 2019 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport asyncio\nimport logging\nfrom pathlib import Path\nfrom typing import Optional, Union\n\nfrom deeppavlov.core.common.file import read_json\nfrom deeppavlov.core.common.paths import get_settings_path\nfrom deeppavlov.utils.agent.rabbitmq import RabbitMQServiceGateway\n\nCONNECTOR_CONFIG_FILENAME = \'server_config.json\'\n\n\ndef start_rabbit_service(model_config: Union[str, Path],\n                         service_name: Optional[str] = None,\n                         agent_namespace: Optional[str] = None,\n                         batch_size: Optional[int] = None,\n                         utterance_lifetime_sec: Optional[int] = None,\n                         rabbit_host: Optional[str] = None,\n                         rabbit_port: Optional[int] = None,\n                         rabbit_login: Optional[str] = None,\n                         rabbit_password: Optional[str] = None,\n                         rabbit_virtualhost: Optional[str] = None) -> None:\n    """"""Launches DeepPavlov model receiving utterances and sending responses via RabbitMQ message broker.\n\n    Args:\n        model_config: Path to DeepPavlov model to be launched.\n        service_name: Service name set in DeepPavlov Agent config. Used to format RabbitMQ exchanges, queues and routing\n            keys names.\n        agent_namespace: Service processes messages only from agents with the same namespace value.\n        batch_size: Limits the maximum number of utterances to be processed by service at one inference.\n        utterance_lifetime_sec: RabbitMQ message expiration time in seconds.\n        rabbit_host: RabbitMQ server host name.\n        rabbit_port: RabbitMQ server port number.\n        rabbit_login: RabbitMQ server administrator username.\n        rabbit_password: RabbitMQ server administrator password.\n        rabbit_virtualhost: RabbitMQ server virtualhost name.\n\n    """"""\n    service_config_path = get_settings_path() / CONNECTOR_CONFIG_FILENAME\n    service_config: dict = read_json(service_config_path)[\'agent-rabbit\']\n\n    service_name = service_name or service_config[\'service_name\']\n    agent_namespace = agent_namespace or service_config[\'agent_namespace\']\n    batch_size = batch_size or service_config[\'batch_size\']\n    utterance_lifetime_sec = utterance_lifetime_sec or service_config[\'utterance_lifetime_sec\']\n    rabbit_host = rabbit_host or service_config[\'rabbit_host\']\n    rabbit_port = rabbit_port or service_config[\'rabbit_port\']\n    rabbit_login = rabbit_login or service_config[\'rabbit_login\']\n    rabbit_password = rabbit_password or service_config[\'rabbit_password\']\n    rabbit_virtualhost = rabbit_virtualhost or service_config[\'rabbit_virtualhost\']\n\n    loop = asyncio.get_event_loop()\n\n    gateway = RabbitMQServiceGateway(\n        model_config=model_config,\n        service_name=service_name,\n        agent_namespace=agent_namespace,\n        batch_size=batch_size,\n        utterance_lifetime_sec=utterance_lifetime_sec,\n        rabbit_host=rabbit_host,\n        rabbit_port=rabbit_port,\n        rabbit_login=rabbit_login,\n        rabbit_password=rabbit_password,\n        rabbit_virtualhost=rabbit_virtualhost,\n        loop=loop\n    )\n\n    try:\n        loop.run_forever()\n    except KeyboardInterrupt:\n        pass\n    finally:\n        gateway.disconnect()\n        loop.stop()\n        loop.close()\n        logging.shutdown()\n'"
deeppavlov/utils/alexa/__init__.py,0,b'from .server import start_alexa_server\n'
deeppavlov/utils/alexa/request_parameters.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Request parameters for the DeepPavlov model launched as a skill for Amazon Alexa.\n\nRequest parameters from this module are used to declare additional information\nand validation for request parameters to the DeepPavlov model launched as\na skill for Amazon Alexa.\n\nSee details at https://fastapi.tiangolo.com/tutorial/header-params/,\n               https://fastapi.tiangolo.com/tutorial/body-multiple-params/\n\n""""""\n\nfrom fastapi import Header, Body\n\n_signature_example = \'Z5H5wqd06ExFVPNfJiqhKvAFjkf+cTVodOUirucHGcEVAMO1LfvgqWUkZ/X1ITDZbI0w+SMwVkEQZlkeThbVS/54M22StNDUtfz4Ua20xNDpIPwcWIACAmZ38XxbbTEFJI5WwqrbilNcfzqiGrIPfdO5rl+/xUjHFUdcJdUY/QzBxXsceytVYfEiR9MzOCN2m4C0XnpThUavAu159KrLj8AkuzN0JF87iXv+zOEeZRgEuwmsAnJrRUwkJ4yWokEPnSVdjF0D6f6CscfyvRe9nsWShq7/zRTa41meweh+n006zvf58MbzRdXPB22RI4AN0ksWW7hSC8/QLAKQE+lvaw==\'\n_signature_cert_chain_url_example = \'https://s3.amazonaws.com/echo.api/echo-api-cert-6-ats.pem\'\n_body_example = {\n    ""version"": ""1.0"",\n    ""session"": {\n        ""new"": True,\n        ""sessionId"": ""amzn1.echo-api.session.ee48c20e-5ad5-461f-a735-ce058491e914"",\n        ""application"": {\n            ""applicationId"": ""amzn1.ask.skill.52b86ebd-dd7d-45c3-a763-de584f62b8d6""\n        },\n        ""user"": {\n            ""userId"": ""amzn1.ask.account.AHUAJ5RRTJDATP63AIRLNOVBC2QCJ7U5WSVSD432EA45PDVWAX5CQ6Z2OLD2H2A77VSBQGIMIWAVBMWLHK2EVZAE5VVJ2FHWS4AQM3GMIDH62GZBZ4DOUWXA3DXRBBXXXTKAITDUCZTLG5GP3XN7YORE5FQO2MERGKK7WAJUTHPMLYN4W2IUBVYDIW7544M57N4KV5HMS4DESMY""\n        }\n    },\n    ""context"": {\n        ""System"": {\n            ""application"": {\n                ""applicationId"": ""amzn1.ask.skill.52b86ebd-dd7d-45c3-a763-de584f62b8d6""\n            },\n            ""user"": {\n                ""userId"": ""amzn1.ask.account.AHUAJ5RRTJDATP63AIRLNOVBC2QCJ7U5WSVSD432EA45PDVWAX5CQ6Z2OLD2H2A77VSBQGIMIWAVBMWLHK2EVZAE5VVJ2FHWS4AQM3GMIDH62GZBZ4DOUWXA3DXRBBXXXTKAITDUCZTLG5GP3XN7YORE5FQO2MERGKK7WAJUTHPMLYN4W2IUBVYDIW7544M57N4KV5HMS4DESMY""\n            },\n            ""device"": {\n                ""deviceId"": ""amzn1.ask.device.AH777YKPTWMNQGVKUKDWPQOWWEDBDJNMIGP5GHDXOIMI3N5RYZWQ2HBQEOUXMUJEHRBKDX6HCFEA7RRWNAGKHJLSD5KWLTKR35D42TW6BVL64THCYUITTH3G6ZMWZ6GNAELTXWB4YAZJWUK4J2BIFVLUP2KHZNTQRJRBEFGNWY4V2RCEEQOZC"",\n                ""supportedInterfaces"": {}\n            },\n            ""apiEndpoint"": ""https://api.amazonalexa.com"",\n            ""apiAccessToken"": ""eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsImtpZCI6IjEifQ.eyJhdWQiOiJodHRwczovL2FwaS5hbWF6b25hbGV4YS5jb20iLCJpc3MiOiJBbGV4YVNraWxsS2l0Iiwic3ViIjoiYW16bjEuYXNrLnNraWxsLjUyYjg2ZWJkLWRkN2QtNDVjMy1hNzYzLWRlNTg0ZjYyYjhkNiIsImV4cCI6MTU2OTgzNTY5MiwiaWF0IjoxNTY5ODM1MzkyLCJuYmYiOjE1Njk4MzUzOTIsInByaXZhdGVDbGFpbXMiOnsiY29udGV4dCI6IkFBQUFBQUFBQUFCTm5aUTd4b09EcGNYL0tuMDFpZ1F6S2dFQUFBQUFBQUJSazluemRVNTlQZWVFY0t5SERSZEwzRiszdnZrVGpQWWQ3MnhFYzFQcUNSeStTTWZmaFFscUh4azJuTHNTV01JKzFnZEtYc0t1RGVSQkJqNERTck5TUWVCZjNkbmtxNERWMXRqVjhmUnB1UWRXdlY2bERZN3YycXMyZVRlZEN6V0RLY21oRXFjRHdBNWlmdUxEdzB5bmZVVVh6Rk0yLzBBeDdGUmYxaS9FWXJRaWV0T2Q1dWllYU9RUFUrUUNMUUNRMFI0Ni9Ld1d1SWdxcE5sSGw0bU0xSHNhYXJOS3VzM0hDRzNyNm9LekxkT25EVUFKTDRtajkzSGwwZUhUQ1M0WDFySEtTTHNMNUlxa2hnUTk3a0R0WVovK1dNbkVDNklGUEZ6OHdYYU9jaDJYS05EUTNERVlGWTE0WHRkTXY0MlBYeTJlQ3VjQy9udnU2ZGMxaGRjUGdkZUp2Rmw3WlBBK0RSa2RqYXovL1NNTjVQMlNBY0NqK2JBZXIrTGZOTDByYUxhbGh5OEhleGl5IiwiY29uc2VudFRva2VuIjpudWxsLCJkZXZpY2VJZCI6ImFtem4xLmFzay5kZXZpY2UuQUg3NzdZS1BUV01OUUdWS1VLRFdQUU9XV0VEQkRKTk1JR1A1R0hEWE9JTUkzTjVSWVpXUTJIQlFFT1VYTVVKRUhSQktEWDZIQ0ZFQTdSUldOQUdLSEpMU0Q1S1dMVEtSMzVENDJUVzZCVkw2NFRIQ1lVSVRUSDNHNlpNV1o2R05BRUxUWFdCNFlBWkpXVUs0SjJCSUZWTFVQMktIWk5UUVJKUkJFRkdOV1k0VjJSQ0VFUU9aQyIsInVzZXJJZCI6ImFtem4xLmFzay5hY2NvdW50LkFIVUFKNVJSVEpEQVRQNjNBSVJMTk9WQkMyUUNKN1U1V1NWU0Q0MzJFQTQ1UERWV0FYNUNRNloyT0xEMkgyQTc3VlNCUUdJTUlXQVZCTVdMSEsyRVZaQUU1VlZKMkZIV1M0QVFNM0dNSURINjJHWkJaNERPVVdYQTNEWFJCQlhYWFRLQUlURFVDWlRMRzVHUDNYTjdZT1JFNUZRTzJNRVJHS0s3V0FKVVRIUE1MWU40VzJJVUJWWURJVzc1NDRNNTdONEtWNUhNUzRERVNNWSJ9fQ.brF2UpwjKMbYhR50WdoALbz0CM9hFtfAUw4Hh9-tOMJY8imui3oadv5S6QbQlfYD4_V_mJG2WOfkLmvirdRwdY6gI289WB48a6pK29VVcJWhYv1wIEpNQUMvMQqMZpjUuCI6DR9PqSeHulqPt14ytiA1ghOVSsAsHFXGbhNNeM9SdS1Ss0JQolSvXo09qC3JFRpDBI1bzBxRthhWEwgIEkC-JuFAbCbXz-710FkI4vzlMElgvC2GIsPf-5RaTJXps4UuG1rLieerirrrZfbpmhO0x2vDbLvBCCbqUtoHPyKofexfBXebvMjjJ7PRZvKYxAg3SBVZLvpGVl0prgJ8PA""\n        },\n        ""Viewport"": {\n            ""experiences"": [\n                {\n                    ""arcMinuteWidth"": 246,\n                    ""arcMinuteHeight"": 144,\n                    ""canRotate"": False,\n                    ""canResize"": False\n                }\n            ],\n            ""shape"": ""RECTANGLE"",\n            ""pixelWidth"": 1024,\n            ""pixelHeight"": 600,\n            ""dpi"": 160,\n            ""currentPixelWidth"": 1024,\n            ""currentPixelHeight"": 600,\n            ""touch"": [\n                ""SINGLE""\n            ],\n            ""video"": {\n                ""codecs"": [\n                    ""H_264_42"",\n                    ""H_264_41""\n                ]\n            }\n        }\n    },\n    ""request"": {\n        ""type"": ""LaunchRequest"",\n        ""requestId"": ""amzn1.echo-api.request.9b112eb9-eb11-433d-b6b3-8dba7eab9637"",\n        ""timestamp"": ""2019-09-30T09:23:12Z"",\n        ""locale"": ""en-US"",\n        ""shouldLinkResultBeReturned"": False\n    }\n}\n\nsignature_header = Header(..., example=_signature_example, alias=\'Signature\')\ncert_chain_url_header = Header(..., example=_signature_cert_chain_url_example, alias=\'Signaturecertchainurl\')\ndata_body = Body(..., example=_body_example)\n'"
deeppavlov/utils/alexa/server.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport asyncio\nimport json\nfrom logging import getLogger\nfrom pathlib import Path\nfrom queue import Queue\nfrom typing import Union, Optional\n\nimport uvicorn\nfrom fastapi import FastAPI\nfrom starlette.responses import JSONResponse\n\nfrom deeppavlov.core.common.log import log_config\nfrom deeppavlov.utils.alexa.request_parameters import data_body, cert_chain_url_header, signature_header\nfrom deeppavlov.utils.connector import AlexaBot\nfrom deeppavlov.utils.server import get_ssl_params, redirect_root_to_docs, get_server_params\n\nlog = getLogger(__name__)\napp = FastAPI()\n\n\ndef start_alexa_server(model_config: Union[str, Path, dict],\n                       port: Optional[int] = None,\n                       https: Optional[bool] = None,\n                       ssl_key: Optional[str] = None,\n                       ssl_cert: Optional[str] = None) -> None:\n    """"""Initiates FastAPI web service with Alexa skill.\n\n    Allows raise Alexa web service with DeepPavlov config in backend.\n\n    Args:\n        model_config: DeepPavlov config path.\n        port: FastAPI web service port.\n        https: Flag for running Alexa skill service in https mode.\n        ssl_key: SSL key file path.\n        ssl_cert: SSL certificate file path.\n\n    """"""\n    server_params = get_server_params(model_config)\n\n    host = server_params[\'host\']\n    port = port or server_params[\'port\']\n\n    ssl_config = get_ssl_params(server_params, https, ssl_key=ssl_key, ssl_cert=ssl_cert)\n\n    input_q = Queue()\n    output_q = Queue()\n\n    bot = AlexaBot(model_config, input_q, output_q)\n    bot.start()\n\n    endpoint = \'/interact\'\n    redirect_root_to_docs(app, \'interact\', endpoint, \'post\')\n\n    @app.post(endpoint, summary=\'Amazon Alexa custom service endpoint\', response_description=\'A model response\')\n    async def interact(data: dict = data_body,\n                       signature: str = signature_header,\n                       signature_chain_url: str = cert_chain_url_header) -> JSONResponse:\n        # It is necessary for correct data validation to serialize data to a JSON formatted string with separators.\n        request_dict = {\n            \'request_body\': json.dumps(data, separators=(\',\', \':\')).encode(\'utf-8\'),\n            \'signature_chain_url\': signature_chain_url,\n            \'signature\': signature,\n            \'alexa_request\': data\n        }\n\n        bot.input_queue.put(request_dict)\n        loop = asyncio.get_event_loop()\n        response: dict = await loop.run_in_executor(None, bot.output_queue.get)\n        response_code = 400 if \'error\' in response.keys() else 200\n        return JSONResponse(response, status_code=response_code)\n\n    uvicorn.run(app, host=host, port=port, log_config=log_config, ssl_version=ssl_config.version,\n                ssl_keyfile=ssl_config.keyfile, ssl_certfile=ssl_config.certfile)\n    bot.join()\n'"
deeppavlov/utils/alice/__init__.py,0,b'from .server import start_alice_server\n'
deeppavlov/utils/alice/request_parameters.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Request parameters for the DeepPavlov model launched as a skill for Yandex.Alice.\n\nRequest parameters from this module are used to declare additional information\nand validation for request parameters to the DeepPavlov model launched as\na skill for Yandex.Alice.\n\nSee details at https://fastapi.tiangolo.com/tutorial/body-multiple-params/\n\n""""""\n\nfrom fastapi import Body\n\n_body_example = {\n    \'name\': \'data\',\n    \'in\': \'body\',\n    \'required\': \'true\',\n    \'example\': {\n        \'meta\': {\n            \'locale\': \'ru-RU\',\n            \'timezone\': \'Europe/Moscow\',\n            ""client_id"": \'ru.yandex.searchplugin/5.80 (Samsung Galaxy; Android 4.4)\'\n        },\n        \'request\': {\n            \'command\': \'\xd0\xb3\xd0\xb4\xd0\xb5 \xd0\xb1\xd0\xbb\xd0\xb8\xd0\xb6\xd0\xb0\xd0\xb9\xd1\x88\xd0\xb5\xd0\xb5 \xd0\xbe\xd1\x82\xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5\',\n            \'original_utterance\': \'\xd0\x90\xd0\xbb\xd0\xb8\xd1\x81\xd0\xb0 \xd1\x81\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd0\xb8 \xd1\x83 \xd0\xa1\xd0\xb1\xd0\xb5\xd1\x80\xd0\xb1\xd0\xb0\xd0\xbd\xd0\xba\xd0\xb0 \xd0\xb3\xd0\xb4\xd0\xb5 \xd0\xb1\xd0\xbb\xd0\xb8\xd0\xb6\xd0\xb0\xd0\xb9\xd1\x88\xd0\xb5\xd0\xb5 \xd0\xbe\xd1\x82\xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5\',\n            \'type\': \'SimpleUtterance\',\n            \'markup\': {\n                \'dangerous_context\': True\n            },\n            \'payload\': {}\n        },\n        \'session\': {\n            \'new\': True,\n            \'message_id\': 4,\n            \'session_id\': \'2eac4854-fce721f3-b845abba-20d60\',\n            \'skill_id\': \'3ad36498-f5rd-4079-a14b-788652932056\',\n            \'user_id\': \'AC9WC3DF6FCE052E45A4566A48E6B7193774B84814CE49A922E163B8B29881DC\'\n        },\n        \'version\': \'1.0\'\n    }\n}\n\ndata_body = Body(..., example=_body_example)\n'"
deeppavlov/utils/alice/server.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport asyncio\nfrom logging import getLogger\nfrom pathlib import Path\nfrom queue import Queue\nfrom typing import Optional, Union\n\nimport uvicorn\nfrom fastapi import FastAPI\n\nfrom deeppavlov.core.common.log import log_config\nfrom deeppavlov.utils.alice.request_parameters import data_body\nfrom deeppavlov.utils.connector import AliceBot\nfrom deeppavlov.utils.server import get_server_params, get_ssl_params, redirect_root_to_docs\n\nlog = getLogger(__name__)\napp = FastAPI()\n\n\ndef start_alice_server(model_config: Union[str, Path],\n                       host: Optional[str] = None,\n                       port: Optional[int] = None,\n                       endpoint: Optional[str] = None,\n                       https: Optional[bool] = None,\n                       ssl_key: Optional[str] = None,\n                       ssl_cert: Optional[str] = None) -> None:\n    server_params = get_server_params(model_config)\n\n    host = host or server_params[\'host\']\n    port = port or server_params[\'port\']\n    endpoint = endpoint or server_params[\'model_endpoint\']\n\n    ssl_config = get_ssl_params(server_params, https, ssl_key=ssl_key, ssl_cert=ssl_cert)\n\n    input_q = Queue()\n    output_q = Queue()\n\n    bot = AliceBot(model_config, input_q, output_q)\n    bot.start()\n\n    redirect_root_to_docs(app, \'answer\', endpoint, \'post\')\n\n    @app.post(endpoint, summary=\'A model endpoint\', response_description=\'A model response\')\n    async def answer(data: dict = data_body) -> dict:\n        loop = asyncio.get_event_loop()\n        bot.input_queue.put(data)\n        response: dict = await loop.run_in_executor(None, bot.output_queue.get)\n        return response\n\n    uvicorn.run(app, host=host, port=port, log_config=log_config, ssl_version=ssl_config.version,\n                ssl_keyfile=ssl_config.keyfile, ssl_certfile=ssl_config.certfile)\n    bot.join()\n'"
deeppavlov/utils/connector/__init__.py,0,"b'from .bot import AlexaBot, AliceBot, MSBot, TelegramBot\nfrom .dialog_logger import DialogLogger\n'"
deeppavlov/utils/connector/bot.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport threading\nfrom collections import namedtuple\nfrom datetime import datetime, timedelta\nfrom logging import getLogger\nfrom pathlib import Path\nfrom queue import Empty, Queue\nfrom threading import Thread, Timer\nfrom typing import Dict, Optional, Union\n\nimport requests\nimport telebot\nfrom OpenSSL.crypto import X509\nfrom requests.exceptions import HTTPError\n\nfrom deeppavlov.core.commands.infer import build_model\nfrom deeppavlov.core.common.chainer import Chainer\nfrom deeppavlov.core.common.file import read_json\nfrom deeppavlov.core.common.paths import get_settings_path\nfrom deeppavlov.utils.connector.conversation import AlexaConversation, AliceConversation, BaseConversation\nfrom deeppavlov.utils.connector.conversation import MSConversation, TelegramConversation\nfrom deeppavlov.utils.connector.ssl_tools import verify_cert, verify_signature\n\nCONNECTOR_CONFIG_FILENAME = \'server_config.json\'\nINPUT_QUEUE_TIMEOUT = 1\n\nlog = getLogger(__name__)\n\nValidatedCert = namedtuple(\'ValidatedCert\', [\'cert\', \'expiration_timestamp\'])\n\n\nclass BaseBot(Thread):\n    """"""Routes requests to conversations, sends responses to channel.\n\n    Attributes:\n        input_queue: Queue for incoming requests from the channel.\n\n    """"""\n    input_queue: Queue\n    _run_flag: bool\n    _model: Chainer\n    _conversations: Dict[str, BaseConversation]\n\n    def __init__(self,\n                 model_config: Union[str, Path, dict],\n                 input_queue: Queue) -> None:\n        """"""Builds DeepPavlov model, initiates class attributes.\n\n        Args:\n            model_config: Path to DeepPavlov model config file.\n            input_queue: Queue for incoming requests from channel.\n\n        """"""\n        super(BaseBot, self).__init__()\n        self.input_queue = input_queue\n        self._run_flag = True\n        self._model = build_model(model_config)\n        self._conversations = dict()\n        log.info(\'Bot initiated\')\n\n    def run(self) -> None:\n        """"""Thread run method implementation. Routes requests from ``input_queue`` to request handler.""""""\n        while self._run_flag:\n            try:\n                request: dict = self.input_queue.get(timeout=INPUT_QUEUE_TIMEOUT)\n            except Empty:\n                pass\n            else:\n                response = self._handle_request(request)\n                self._send_response(response)\n\n    def join(self, timeout: Optional[float] = None) -> None:\n        """"""Thread join method implementation. Stops reading requests from ``input_queue``, cancels all timers.\n\n        Args:\n            timeout: Timeout for join operation in seconds. If the timeout argument is not present or None,\n                the operation will block until the thread terminates.\n\n        """"""\n        self._run_flag = False\n        for timer in threading.enumerate():\n            if isinstance(timer, Timer):\n                timer.cancel()\n        Thread.join(self, timeout)\n\n    def _del_conversation(self, conversation_key: Union[int, str]) -> None:\n        """"""Deletes Conversation instance.\n\n        Args:\n            conversation_key: Conversation key.\n\n        """"""\n        if conversation_key in self._conversations.keys():\n            del self._conversations[conversation_key]\n            log.info(f\'Deleted conversation, key: {conversation_key}\')\n\n    def _handle_request(self, request: dict) -> Optional[dict]:\n        """"""Routes the request to the appropriate conversation.\n\n        Args:\n            request: Request from the channel.\n\n        Returns:\n            response: Corresponding response to the channel request if replies are sent via bot, None otherwise.\n\n        """"""\n        raise NotImplementedError\n\n    def _send_response(self, response: Optional[dict]) -> None:\n        """"""Sends response to the request back to the channel.\n\n        Args:\n            response: Corresponding response to the channel request if replies are sent via bot, None otherwise.\n\n        """"""\n        raise NotImplementedError\n\n    def _get_connector_params(self) -> dict:\n        """"""Reads bot and conversation default params from connector config file.\n\n         Returns:\n             connector_defaults: Dictionary containing bot defaults and conversation defaults dicts.\n\n        """"""\n        connector_config_path = get_settings_path() / CONNECTOR_CONFIG_FILENAME\n        connector_config: dict = read_json(connector_config_path)\n\n        bot_name = type(self).__name__\n        conversation_defaults = connector_config[\'telegram\']\n        bot_defaults = connector_config[\'deprecated\'].get(bot_name, conversation_defaults)\n\n        connector_defaults = {\'bot_defaults\': bot_defaults,\n                              \'conversation_defaults\': conversation_defaults}\n\n        return connector_defaults\n\n\nclass AlexaBot(BaseBot):\n    """"""Validates Alexa requests and routes them to conversations, sends responses to Alexa.\n\n    Attributes:\n        input_queue: Queue for incoming requests from Alexa.\n        output_queue: Queue for outgoing responses to Alexa.\n\n    """"""\n    output_queue: Queue\n    _conversation_config: dict\n    _amazon_cert_lifetime: timedelta\n    _request_timestamp_tolerance_secs: int\n    _refresh_valid_certs_period_secs: int\n    _valid_certificates: Dict[str, ValidatedCert]\n    _timer: Timer\n\n    def __init__(self,\n                 model_config: Union[str, Path, dict],\n                 input_queue: Queue,\n                 output_queue: Queue) -> None:\n        """"""Initiates class attributes.\n\n        Args:\n            model_config: Path to DeepPavlov model config file.\n            input_queue: Queue for incoming requests from Alexa.\n            output_queue: Queue for outgoing responses to Alexa.\n\n        """"""\n        super(AlexaBot, self).__init__(model_config, input_queue)\n        self.output_queue = output_queue\n\n        connector_config: dict = self._get_connector_params()\n        self._conversation_config: dict = connector_config[\'conversation_defaults\']\n        bot_config: dict = connector_config[\'bot_defaults\']\n\n        self._conversation_config[\'intent_name\'] = bot_config[\'intent_name\']\n        self._conversation_config[\'slot_name\'] = bot_config[\'slot_name\']\n\n        self._amazon_cert_lifetime = timedelta(seconds=bot_config[\'amazon_cert_lifetime_secs\'])\n        self._request_timestamp_tolerance_secs = bot_config[\'request_timestamp_tolerance_secs\']\n        self._refresh_valid_certs_period_secs = bot_config[\'refresh_valid_certs_period_secs\']\n        self._valid_certificates = {}\n        self._refresh_valid_certs()\n\n    def _refresh_valid_certs(self) -> None:\n        """"""Provides cleanup of periodical certificates with expired validation.""""""\n        self._timer = Timer(self._refresh_valid_certs_period_secs, self._refresh_valid_certs)\n        self._timer.start()\n\n        expired_certificates = []\n\n        for valid_cert_url, valid_cert in self._valid_certificates.items():\n            valid_cert: ValidatedCert = valid_cert\n            cert_expiration_time: datetime = valid_cert.expiration_timestamp\n            if datetime.utcnow() > cert_expiration_time:\n                expired_certificates.append(valid_cert_url)\n\n        for expired_cert_url in expired_certificates:\n            del self._valid_certificates[expired_cert_url]\n            log.info(f\'Validation period of {expired_cert_url} certificate expired\')\n\n    def _verify_request(self, signature_chain_url: str, signature: str, request_body: bytes) -> bool:\n        """"""Provides series of Alexa request verifications against Amazon Alexa requirements.\n\n        Args:\n            signature_chain_url: Signature certificate URL from SignatureCertChainUrl HTTP header.\n            signature: Base64 decoded Alexa request signature from Signature HTTP header.\n            request_body: full HTTPS request body\n\n        Returns:\n            result: True if verification was successful, False otherwise.\n\n        """"""\n        if signature_chain_url not in self._valid_certificates.keys():\n            amazon_cert: X509 = verify_cert(signature_chain_url)\n            if amazon_cert:\n                expiration_timestamp = datetime.utcnow() + self._amazon_cert_lifetime\n                validated_cert = ValidatedCert(cert=amazon_cert, expiration_timestamp=expiration_timestamp)\n                self._valid_certificates[signature_chain_url] = validated_cert\n                log.info(f\'Certificate {signature_chain_url} validated\')\n            else:\n                log.error(f\'Certificate {signature_chain_url} validation failed\')\n                return False\n        else:\n            validated_cert: ValidatedCert = self._valid_certificates[signature_chain_url]\n            amazon_cert: X509 = validated_cert.cert\n\n        if verify_signature(amazon_cert, signature, request_body):\n            result = True\n        else:\n            log.error(f\'Failed signature verification for request: {request_body.decode(""utf-8"", ""replace"")}\')\n            result = False\n\n        return result\n\n    def _handle_request(self, request: dict) -> dict:\n        """"""Processes Alexa request and returns response.\n\n        Args:\n            request: Dict with Alexa request payload and metadata.\n\n        Returns:\n            result: Alexa formatted or error response.\n\n        """"""\n        request_body: bytes = request[\'request_body\']\n        signature_chain_url: str = request[\'signature_chain_url\']\n        signature: str = request[\'signature\']\n        alexa_request: dict = request[\'alexa_request\']\n\n        if not self._verify_request(signature_chain_url, signature, request_body):\n            return {\'error\': \'failed certificate/signature check\'}\n\n        timestamp_str = alexa_request[\'request\'][\'timestamp\']\n        timestamp_datetime = datetime.strptime(timestamp_str, \'%Y-%m-%dT%H:%M:%SZ\')\n        now = datetime.utcnow()\n\n        delta = now - timestamp_datetime if now >= timestamp_datetime else timestamp_datetime - now\n\n        if abs(delta.seconds) > self._request_timestamp_tolerance_secs:\n            log.error(f\'Failed timestamp check for request: {request_body.decode(""utf-8"", ""replace"")}\')\n            return {\'error\': \'failed request timestamp check\'}\n\n        conversation_key = alexa_request[\'session\'][\'sessionId\']\n\n        if conversation_key not in self._conversations:\n            self._conversations[conversation_key] = \\\n                AlexaConversation(config=self._conversation_config,\n                                  model=self._model,\n                                  self_destruct_callback=self._del_conversation,\n                                  conversation_id=conversation_key)\n\n            log.info(f\'Created new conversation, key: {conversation_key}\')\n\n        conversation = self._conversations[conversation_key]\n        response = conversation.handle_request(alexa_request)\n\n        return response\n\n    def _send_response(self, response: dict) -> None:\n        """"""Sends response to Alexa.\n\n        Args:\n            response: Alexa formatted or error response.\n\n        """"""\n        self.output_queue.put(response)\n\n\nclass AliceBot(BaseBot):\n    """"""Processes Alice requests and routes them to conversations, returns responses to Alice.\n\n    Attributes:\n        input_queue: Queue for incoming requests from Alice.\n        output_queue: Queue for outgoing responses to Alice.\n\n    """"""\n    output_queue: Queue\n    _conversation_config: dict\n\n    def __init__(self,\n                 model_config: Union[str, Path, dict],\n                 input_queue: Queue,\n                 output_queue: Queue) -> None:\n        """"""Initiates class attributes.\n\n        Args:\n            model_config: Path to DeepPavlov model config file.\n            input_queue: Queue for incoming requests from Alice.\n            output_queue: Queue for outgoing responses to Alice.\n\n        """"""\n        super(AliceBot, self).__init__(model_config, input_queue)\n        self.output_queue = output_queue\n        connector_config: dict = self._get_connector_params()\n        self._conversation_config = connector_config[\'conversation_defaults\']\n\n    def _handle_request(self, request: dict) -> dict:\n        """"""Processes Alice request and returns response.\n\n        Args:\n            request: Dict with Alice request payload and metadata.\n\n        Returns:\n            result: Alice formatted response.\n\n        """"""\n        conversation_key = request[\'session\'][\'session_id\']\n\n        if conversation_key not in self._conversations:\n            self._conversations[conversation_key] = \\\n                AliceConversation(config=self._conversation_config,\n                                  model=self._model,\n                                  self_destruct_callback=self._del_conversation,\n                                  conversation_id=conversation_key)\n            log.info(f\'Created new conversation, key: {conversation_key}\')\n        conversation = self._conversations[conversation_key]\n        response = conversation.handle_request(request)\n\n        return response\n\n    def _send_response(self, response: dict) -> None:\n        """"""Sends response to Alice.\n\n        Args:\n            response: Alice formatted response.\n\n        """"""\n        self.output_queue.put(response)\n\n\nclass MSBot(BaseBot):\n    """"""Routes Microsoft Bot Framework requests to conversations, sends responses to Bot Framework.\n\n    Attributes:\n        input_queue: Queue for incoming requests from Microsoft Bot Framework.\n\n    """"""\n    _conversation_config: dict\n    _auth_polling_interval: int\n    _auth_url: str\n    _auth_headers: dict\n    _auth_payload: dict\n    _http_session: requests.Session\n\n    def __init__(self,\n                 model_config: Union[str, Path, dict],\n                 input_queue: Queue,\n                 client_id: Optional[str],\n                 client_secret: Optional[str]) -> None:\n        """"""Initiates class attributes.\n\n        Args:\n            model_config: Path to DeepPavlov model config file.\n            input_queue: Queue for incoming requests from Microsoft Bot Framework.\n            client_id: Microsoft App ID.\n            client_secret: Microsoft App Secret.\n\n        Raises:\n            ValueError: If ``client_id`` or ``client_secret`` were not set neither in the configuration file nor\n                in method arguments.\n\n        """"""\n        super(MSBot, self).__init__(model_config, input_queue)\n        connector_config: dict = self._get_connector_params()\n        bot_config: dict = connector_config[\'bot_defaults\']\n        bot_config[\'auth_payload\'][\'client_id\'] = client_id or bot_config[\'auth_payload\'][\'client_id\']\n        bot_config[\'auth_payload\'][\'client_secret\'] = client_secret or bot_config[\'auth_payload\'][\'client_secret\']\n\n        if not bot_config[\'auth_payload\'][\'client_id\']:\n            e = ValueError(\'Microsoft Bot Framework app id required: initiate -i param \'\n                           \'or auth_payload.client_id param in server configuration file\')\n            log.error(e)\n            raise e\n\n        if not bot_config[\'auth_payload\'][\'client_secret\']:\n            e = ValueError(\'Microsoft Bot Framework app secret required: initiate -s param \'\n                           \'or auth_payload.client_secret param in server configuration file\')\n            log.error(e)\n            raise e\n\n        self._conversation_config = connector_config[\'conversation_defaults\']\n        self._auth_polling_interval = bot_config[\'auth_polling_interval\']\n        self._auth_url = bot_config[\'auth_url\']\n        self._auth_headers = bot_config[\'auth_headers\']\n        self._auth_payload = bot_config[\'auth_payload\']\n        self._http_session = requests.Session()\n        self._update_access_info()\n\n    def _update_access_info(self) -> None:\n        """"""Updates headers for http_session used to send responses to Bot Framework.\n\n        Raises:\n            HTTPError: If authentication token request returned other than 200 status code.\n\n        """"""\n        self._timer = threading.Timer(self._auth_polling_interval, self._update_access_info)\n        self._timer.start()\n\n        result = requests.post(url=self._auth_url,\n                               headers=self._auth_headers,\n                               data=self._auth_payload)\n\n        status_code = result.status_code\n        if status_code != 200:\n            raise HTTPError(f\'Authentication token request returned wrong HTTP status code: {status_code}\')\n\n        access_info = result.json()\n        headers = {\n            \'Authorization\': f""{access_info[\'token_type\']} {access_info[\'access_token\']}"",\n            \'Content-Type\': \'application/json\'\n        }\n\n        self._http_session.headers.update(headers)\n\n        log.info(f\'Obtained authentication information from Microsoft Bot Framework: {str(access_info)}\')\n\n    def _handle_request(self, request: dict) -> None:\n        """"""Routes MS Bot Framework request to conversation.\n\n        Args:\n            request: Dict with MS Bot Framework request payload and metadata.\n\n        """"""\n        conversation_key = request[\'conversation\'][\'id\']\n\n        if conversation_key not in self._conversations:\n            self._conversations[conversation_key] = \\\n                MSConversation(config=self._conversation_config,\n                               model=self._model,\n                               self_destruct_callback=self._del_conversation,\n                               conversation_id=conversation_key,\n                               http_session=self._http_session)\n\n            log.info(f\'Created new conversation, key: {conversation_key}\')\n\n        conversation = self._conversations[conversation_key]\n        conversation.handle_request(request)\n\n    def _send_response(self, response: dict) -> None:\n        """"""Dummy method to match ``run`` method body.""""""\n        pass\n\n\nclass TelegramBot(BaseBot):\n    """"""Routes messages from Telegram to conversations, sends responses back.""""""\n    _conversation_config: dict\n    _token: str\n\n    def __init__(self, model_config: Union[str, Path, dict], token: Optional[str]) -> None:\n        """"""Initiates and validates class attributes.\n\n        Args:\n            model_config: Path to DeepPavlov model config file.\n            token: Telegram bot token.\n\n        Raises:\n            ValueError: If telegram token was not set neither in config file nor in method arguments.\n\n        """"""\n        super(TelegramBot, self).__init__(model_config, Queue())\n        connector_config: dict = self._get_connector_params()\n        bot_config: dict = connector_config[\'bot_defaults\']\n        self._conversation_config = connector_config[\'conversation_defaults\']\n        self._token = token or bot_config[\'token\']\n\n        if not self._token:\n            e = ValueError(\'Telegram token required: initiate -t param or telegram_defaults/token \'\n                           \'in server configuration file\')\n            log.error(e)\n            raise e\n\n    def start(self) -> None:\n        """"""Starts polling messages from Telegram, routes messages to handlers.""""""\n        bot = telebot.TeleBot(self._token)\n        bot.remove_webhook()\n\n        @bot.message_handler(commands=[\'start\'])\n        def send_start_message(message: telebot.types.Message) -> None:\n            chat_id = message.chat.id\n            out_message = self._conversation_config[\'start_message\']\n            bot.send_message(chat_id, out_message)\n\n        @bot.message_handler(commands=[\'help\'])\n        def send_help_message(message: telebot.types.Message) -> None:\n            chat_id = message.chat.id\n            out_message = self._conversation_config[\'help_message\']\n            bot.send_message(chat_id, out_message)\n\n        @bot.message_handler()\n        def handle_inference(message: telebot.types.Message) -> None:\n            chat_id = message.chat.id\n            context = message.text\n\n            if chat_id not in self._conversations:\n                self._conversations[chat_id] = \\\n                    TelegramConversation(config=self._conversation_config,\n                                         model=self._model,\n                                         self_destruct_callback=self._del_conversation,\n                                         conversation_id=chat_id)\n\n            conversation = self._conversations[chat_id]\n            response = conversation.handle_request(context)\n            bot.send_message(chat_id, response)\n\n        bot.polling()\n\n    def _handle_request(self, request: dict) -> None:\n        """"""Dummy method to match ``run`` method body.""""""\n        pass\n\n    def _send_response(self, response: Optional[dict]) -> None:\n        """"""Dummy method to match ``run`` method body.""""""\n        pass\n'"
deeppavlov/utils/connector/conversation.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom threading import Timer\nfrom typing import Dict, Optional, Union\nfrom urllib.parse import urljoin\n\nfrom requests import Session\n\nfrom deeppavlov.core.common.chainer import Chainer\nfrom deeppavlov.utils.connector.dialog_logger import DialogLogger\n\nlog = getLogger(__name__)\n\nDIALOG_LOGGER_NAME_MAPPING = {\n    \'AlexaConversation\': \'alexa\',\n    \'AliceConversation\': \'alice\',\n    \'MSConversation\': \'ms_bot_framework\',\n    \'TelegramConversation\': \'telegram\',\n    \'_unsupported\': \'new_conversation\'\n}\n\n\nclass BaseConversation:\n    """"""Receives requests, generates responses.""""""\n    _model: Chainer\n    _self_destruct_callback: callable\n    _conversation_id: Union[int, str]\n    _timer: Timer\n    _infer_utterances: list\n    _conversation_lifetime: int\n    _next_arg_msg: str\n    _start_message: str\n\n    def __init__(self,\n                 config: dict,\n                 model: Chainer,\n                 self_destruct_callback: callable,\n                 conversation_id: Union[int, str]) -> None:\n        """"""Initiates instance properties and starts self-destruct timer.\n\n        Args:\n            config: Dictionary containing base conversation parameters.\n            model: Model that infered with user messages.\n            self_destruct_callback: Function that removes this Conversation instance.\n            conversation_id: Conversation ID.\n\n        """"""\n        self._model = model\n        self._self_destruct_callback = self_destruct_callback\n        self._conversation_id = conversation_id\n        self._infer_utterances = list()\n        self._conversation_lifetime = config[\'conversation_lifetime\']\n        self._next_arg_msg = config[\'next_argument_message\']\n        self._start_message = config[\'start_message\']\n        self._unsupported_message = config[\'unsupported_message\']\n        logger_name: str = DIALOG_LOGGER_NAME_MAPPING.get(type(self).__name__,\n                                                          DIALOG_LOGGER_NAME_MAPPING[\'_unsupported\'])\n        self._dialog_logger = DialogLogger(logger_name=logger_name)\n        self._start_timer()\n\n    def handle_request(self, request: dict) -> Optional[dict]:\n        """"""Rearms self-destruct timer and sends the request to a processing.\n\n        Args:\n            request: Request from the channel.\n\n        Returns:\n            response: Corresponding to the channel response to the request from the channel if replies are sent via bot,\n                None otherwise.\n\n        """"""\n        self._rearm_self_destruct()\n        return self._handle_request(request)\n\n    def _start_timer(self) -> None:\n        """"""Initiates self-destruct timer.""""""\n        self._timer = Timer(self._conversation_lifetime, self._self_destruct_callback, [self._conversation_id])\n        self._timer.start()\n\n    def _rearm_self_destruct(self) -> None:\n        """"""Rearms self-destruct timer.""""""\n        self._timer.cancel()\n        self._start_timer()\n\n    def _handle_request(self, request: dict) -> Optional[dict]:\n        """"""Routes the request to the appropriate handler.\n\n        Args:\n            request: Request from the channel.\n\n        Returns:\n            response: Corresponding response to the channel request if replies are sent via bot, None otherwise.\n\n        """"""\n        raise NotImplementedError\n\n    def _handle_launch(self, request: dict) -> Optional[dict]:\n        """"""Handles launch request.\n\n        Args:\n            request: Start request from channel.\n\n        Returns:\n            response: Greeting message wrapped in the appropriate to the channel structure if replies are sent via bot,\n                None otherwise.\n\n        """"""\n        response = self._generate_response(self._start_message, request)\n\n        return response\n\n    def _handle_unsupported(self, request: dict) -> Optional[dict]:\n        """"""Handles all unsupported request types.\n\n        Args:\n            request: Request from channel for which a separate handler was not defined.\n\n        Returns:\n            response: Message that request type is not supported wrapped in the appropriate to the channel data\n                structure if replies are sent via bot, None otherwise.\n\n        """"""\n        response = self._generate_response(self._unsupported_message, request)\n        log.warning(f\'Unsupported request: {request}\')\n\n        return response\n\n    def _generate_response(self, message: str, request: dict) -> Optional[dict]:\n        """"""Wraps message in the appropriate to the channel data structure.\n\n        Args:\n            message: Raw message to be sent to the channel.\n            request: Request from the channel to which the ``message`` replies.\n\n        Returns:\n            response: Data structure to be sent to the channel if replies are sent via bot, None otherwise.\n\n        """"""\n        raise NotImplementedError\n\n    def _act(self, utterance: str) -> str:\n        """"""Infers DeepPavlov model with utterance.\n\n        If DeepPavlov model requires more than one argument, utterances are accumulated until reaching required\n        arguments amount to infer.\n\n        Args:\n            utterance: Text to be processed by DeepPavlov model.\n\n        Returns:\n            response: Model response if enough model arguments have been accumulated, message prompting for the next\n                model argument otherwise.\n\n        """"""\n        self._infer_utterances.append([utterance])\n\n        if len(self._infer_utterances) == len(self._model.in_x):\n            self._dialog_logger.log_in(self._infer_utterances, self._conversation_id)\n            prediction = self._model(*self._infer_utterances)\n            self._infer_utterances = list()\n            if len(self._model.out_params) == 1:\n                prediction = [prediction]\n            prediction = \'; \'.join([str(output[0]) for output in prediction])\n            response = prediction\n            self._dialog_logger.log_out(response, self._conversation_id)\n        else:\n            response = self._next_arg_msg.format(self._model.in_x[len(self._infer_utterances)])\n\n        return response\n\n\nclass AlexaConversation(BaseConversation):\n    """"""Receives requests from Amazon Alexa and generates responses.""""""\n    _intent_name: str\n    _slot_name: str\n    _handled_requests: Dict[str, callable]\n\n    def __init__(self, config: dict, model, self_destruct_callback: callable, conversation_id: str) -> None:\n        super(AlexaConversation, self).__init__(config, model, self_destruct_callback, conversation_id)\n        self._intent_name = config[\'intent_name\']\n        self._slot_name = config[\'slot_name\']\n\n        self._handled_requests = {\n            \'LaunchRequest\': self._handle_launch,\n            \'IntentRequest\': self._handle_intent,\n            \'SessionEndedRequest\': self._handle_end,\n            \'_unsupported\': self._handle_unsupported\n        }\n\n    def _handle_request(self, request: dict) -> dict:\n        """"""Routes Alexa requests to the appropriate handler.\n\n        Args:\n            request: Alexa request.\n\n        Returns:\n            response: Response conforming to the Alexa response specification.\n\n        """"""\n        request_type = request[\'request\'][\'type\']\n        request_id = request[\'request\'][\'requestId\']\n        log.debug(f\'Received request. Type: {request_type}, id: {request_id}\')\n\n        if request_type in self._handled_requests:\n            response = self._handled_requests[request_type](request)\n        else:\n            response = self._handled_requests[\'_unsupported\'](request)\n\n        return response\n\n    def _generate_response(self, message: str, request: dict) -> dict:\n        """"""Wraps message in the conforming to the Alexa data structure.\n\n        Args:\n            message: Raw message to be sent to Alexa.\n            request: Request from the channel to which the ``message`` replies.\n\n        Returns:\n            response: Data structure conforming to the Alexa response specification.\n\n        """"""\n        response = {\n            \'version\': \'1.0\',\n            \'sessionAttributes\': {\n                \'sessionId\': request[\'session\'][\'sessionId\']\n            },\n            \'response\': {\n                \'shouldEndSession\': False,\n                \'outputSpeech\': {\n                    \'type\': \'PlainText\',\n                    \'text\': message\n                },\n                \'card\': {\n                    \'type\': \'Simple\',\n                    \'content\': message\n                }\n            }\n        }\n\n        return response\n\n    def _handle_intent(self, request: dict) -> dict:\n        """"""Handles IntentRequest Alexa request.\n\n        Args:\n            request: Alexa request.\n\n        Returns:\n            response: Data structure conforming to the Alexa response specification.\n\n        """"""\n        request_id = request[\'request\'][\'requestId\']\n        request_intent: dict = request[\'request\'][\'intent\']\n\n        if self._intent_name != request_intent[\'name\']:\n            log.error(f""Wrong intent name received: {request_intent[\'name\']} in request {request_id}"")\n            return {\'error\': \'wrong intent name\'}\n\n        if self._slot_name not in request_intent[\'slots\'].keys():\n            log.error(f\'No slot named {self._slot_name} found in request {request_id}\')\n            return {\'error\': \'no slot found\'}\n\n        utterance = request_intent[\'slots\'][self._slot_name][\'value\']\n        model_response = self._act(utterance)\n\n        if not model_response:\n            log.error(f\'Some error during response generation for request {request_id}\')\n            return {\'error\': \'error during response generation\'}\n\n        response = self._generate_response(model_response, request)\n\n        return response\n\n    def _handle_end(self, request: dict) -> dict:\n        """"""Handles SessionEndedRequest Alexa request and deletes Conversation instance.\n\n        Args:\n            request: Alexa request.\n\n        Returns:\n            response: Dummy empty response dict.\n\n        """"""\n        response = {}\n        self._self_destruct_callback(self._conversation_id)\n        return response\n\n\nclass AliceConversation(BaseConversation):\n    """"""Receives requests from Yandex.Alice and generates responses.""""""\n    def _handle_request(self, request: dict) -> dict:\n        """"""Routes Alice requests to the appropriate handler.\n\n        Args:\n            request: Alice request.\n\n        Returns:\n            response: Response conforming to the Alice response specification.\n\n        """"""\n        message_id = request[\'session\'][\'message_id\']\n        session_id = request[\'session\'][\'session_id\']\n        log.debug(f\'Received message. Session: {session_id}, message_id: {message_id}\')\n\n        if request[\'session\'][\'new\']:\n            response = self._handle_launch(request)\n        elif request[\'request\'][\'command\'].strip():\n            text = request[\'request\'][\'command\'].strip()\n            model_response = self._act(text)\n            response = self._generate_response(model_response, request)\n        else:\n            response = self._handle_unsupported(request)\n\n        return response\n\n    def _generate_response(self, message: str, request: dict) -> dict:\n        """"""Wraps message in the conforming to the Alice data structure.\n\n        Args:\n            message: Raw message to be sent to Alice.\n            request: Request from the channel to which the ``message`` replies.\n\n        Returns:\n            response: Data structure conforming to the Alice response specification.\n\n        """"""\n        response = {\n            \'response\': {\n                \'end_session\': False,\n                \'text\': message\n            },\n            \'session\': {\n                \'session_id\': request[\'session\'][\'session_id\'],\n                \'message_id\': request[\'session\'][\'message_id\'],\n                \'user_id\': request[\'session\'][\'user_id\']\n            },\n            \'version\': \'1.0\'\n        }\n\n        return response\n\n\nclass MSConversation(BaseConversation):\n    """"""Receives requests from Microsoft Bot Framework and generates responses.""""""\n    def __init__(self,\n                 config: dict,\n                 model: Chainer,\n                 self_destruct_callback: callable,\n                 conversation_id: str,\n                 http_session: Session) -> None:\n        """"""Initiates instance properties and starts self-destruct timer.\n\n        Args:\n            config: Dictionary containing base conversation parameters.\n            model: Model that infered with user messages.\n            self_destruct_callback: Function that removes this Conversation instance.\n            conversation_id: Conversation ID.\n            http_session: Session used to send responses to Bot Framework.\n\n        """"""\n        super(MSConversation, self).__init__(config, model, self_destruct_callback, conversation_id)\n        self._http_session = http_session\n\n        self._handled_activities = {\n            \'message\': self._handle_message,\n            \'conversationUpdate\': self._handle_launch,\n            \'_unsupported\': self._handle_unsupported\n        }\n\n    def _handle_request(self, request: dict) -> None:\n        """"""Routes MS Bot requests to the appropriate handler. Returns None since handlers send responses themselves.\n\n        Args:\n            request: MS Bot request.\n\n        """"""\n        activity_type = request[\'type\']\n        activity_id = request[\'id\']\n        log.debug(f\'Received activity. Type: {activity_type}, id: {activity_id}\')\n\n        if activity_type in self._handled_activities.keys():\n            self._handled_activities[activity_type](request)\n        else:\n            self._handled_activities[\'_unsupported\'](request)\n\n        self._rearm_self_destruct()\n\n    def _handle_message(self, request: dict) -> None:\n        """"""Handles MS Bot message request.\n\n        Request redirected to ``_unsupported`` handler if ms bot message does not contain raw text.\n\n        Args:\n            request: MS Bot request.\n\n        """"""\n        if \'text\' in request:\n            in_text = request[\'text\']\n            model_response = self._act(in_text)\n            if model_response:\n                self._generate_response(model_response, request)\n        else:\n            self._handled_activities[\'_unsupported\'](request)\n\n    def _generate_response(self, message: str, request: dict) -> None:\n        """"""Wraps message in the conforming to the MS Bot data structure and sends it to MS Bot via HTTP session.\n\n        Args:\n            message: Raw message to be sent to MS Bot.\n            request: Request from the channel to which the ``message`` replies.\n\n        """"""\n        response = {\n            ""type"": ""message"",\n            ""from"": request[\'recipient\'],\n            ""recipient"": request[\'from\'],\n            \'conversation\': request[\'conversation\'],\n            \'text\': message\n        }\n\n        url = urljoin(request[\'serviceUrl\'], f""v3/conversations/{request[\'conversation\'][\'id\']}/activities"")\n\n        response = self._http_session.post(url=url, json=response)\n\n        try:\n            response_json_str = str(response.json())\n        except ValueError as e:\n            response_json_str = repr(e)\n\n        log.debug(f\'Sent activity to the MSBotFramework server. \'\n                  f\'Response code: {response.status_code}, response contents: {response_json_str}\')\n\n\nclass TelegramConversation(BaseConversation):\n    """"""Receives requests from Telegram bot and generates responses.""""""\n    def _handle_request(self, message: str) -> str:\n        """"""Handles raw text message from Telegram bot.\n\n        Args:\n            message: Message from Telegram bot.\n\n        Returns:\n            response: Response to a ``message``.\n\n        """"""\n        response = self._act(message)\n\n        return response\n\n    def _generate_response(self, message: str, request: dict) -> None:\n        """"""Does nothing.""""""\n        pass\n'"
deeppavlov/utils/connector/dialog_logger.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nfrom datetime import datetime\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import Any, Optional, Hashable\n\nfrom deeppavlov.core.common.file import read_json\nfrom deeppavlov.core.common.paths import get_settings_path\nfrom deeppavlov.core.data.utils import jsonify_data\n\nLOGGER_CONFIG_FILENAME = \'dialog_logger_config.json\'\nLOG_TIMESTAMP_FORMAT = \'%Y-%m-%d_%H-%M-%S_%f\'\n\nlog = getLogger(__name__)\n\n\nclass DialogLogger:\n    """"""DeepPavlov dialog logging facility.\n\n    DialogLogger is an entity which provides tools for dialogs logging.\n\n    Args:\n        enabled: DialogLogger on/off flag.\n        logger_name: Dialog logger name that is used for organising log files.\n\n    Attributes:\n        logger_name: Dialog logger name which is used for organising log files.\n        log_max_size: Maximum size of log file, kb.\n        self.log_file: Current log file object.\n    """"""\n    def __init__(self, enabled: bool = False, logger_name: Optional[str] = None) -> None:\n        self.config: dict = read_json(get_settings_path() / LOGGER_CONFIG_FILENAME)\n        self.enabled: bool = enabled or self.config[\'enabled\']\n\n        if self.enabled:\n            self.logger_name: str = logger_name or self.config[\'logger_name\']\n            self.log_max_size: int = self.config[\'logfile_max_size_kb\']\n            self.log_file = self._get_log_file()\n            self.log_file.writelines(\'""Dialog logger initiated""\\n\')\n\n    @staticmethod\n    def _get_timestamp_utc_str() -> str:\n        """"""Returns str converted current UTC timestamp.\n\n        Returns:\n            utc_timestamp_str: str converted current UTC timestamp.\n        """"""\n        utc_timestamp_str = datetime.strftime(datetime.utcnow(), LOG_TIMESTAMP_FORMAT)\n        return utc_timestamp_str\n\n    def _get_log_file(self):\n        """"""Returns opened file object for writing dialog logs.\n\n        Returns:\n            log_file: opened Python file object.\n        """"""\n        log_dir: Path = Path(self.config[\'log_path\']).expanduser().resolve() / self.logger_name\n        log_dir.mkdir(parents=True, exist_ok=True)\n        log_file_path = Path(log_dir, f\'{self._get_timestamp_utc_str()}_{self.logger_name}.log\')\n        log_file = open(log_file_path, \'a\', buffering=1, encoding=\'utf8\')\n        return log_file\n\n    def _log(self, utterance: Any, direction: str, dialog_id: Optional[Hashable]=None):\n        """"""Logs single dialog utterance to current dialog log file.\n\n        Args:\n            utterance: Dialog utterance.\n            direction: \'in\' or \'out\' utterance direction.\n            dialog_id: Dialog ID.\n        """"""\n        if isinstance(utterance, str):\n            pass\n        elif isinstance(utterance, (list, dict)):\n            utterance = jsonify_data(utterance)\n        else:\n            utterance = str(utterance)\n\n        dialog_id = str(dialog_id) if not isinstance(dialog_id, str) else dialog_id\n\n        if self.log_file.tell() >= self.log_max_size * 1024:\n            self.log_file.close()\n            self.log_file = self._get_log_file()\n        else:\n            try:\n                log_msg = {}\n                log_msg[\'timestamp\'] = self._get_timestamp_utc_str()\n                log_msg[\'dialog_id\'] = dialog_id\n                log_msg[\'direction\'] = direction\n                log_msg[\'message\'] = utterance\n                log_str = json.dumps(log_msg, ensure_ascii=self.config[\'ensure_ascii\'])\n                self.log_file.write(f\'{log_str}\\n\')\n            except IOError:\n                log.error(\'Failed to write dialog log.\')\n\n    def log_in(self, utterance: Any, dialog_id: Optional[Hashable] = None) -> None:\n        """"""Wraps _log method for all input utterances.\n        Args:\n            utterance: Dialog utterance.\n            dialog_id: Dialog ID.\n        """"""\n        if self.enabled:\n            self._log(utterance, \'in\', dialog_id)\n\n    def log_out(self, utterance: Any, dialog_id: Optional[Hashable] = None) -> None:\n        """"""Wraps _log method for all output utterances.\n        Args:\n            utterance: Dialog utterance.\n            dialog_id: Dialog ID.\n        """"""\n        if self.enabled:\n            self._log(utterance, \'out\', dialog_id)\n'"
deeppavlov/utils/connector/ssl_tools.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport base64\nimport re\nimport ssl\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import List, Optional\nfrom urllib.parse import urlsplit\n\nimport requests\nfrom OpenSSL import crypto\n\nlog = getLogger(__name__)\n\n\ndef verify_sc_url(url: str) -> bool:\n    """"""Verify signature certificate URL against Amazon Alexa requirements.\n\n    Batch of dialog IDs can be provided, in other case utterances indexes in\n    incoming batch are used as dialog IDs.\n\n    Args:\n        url: Signature certificate URL from SignatureCertChainUrl HTTP header.\n\n    Returns:\n        result: True if verification was successful, False if not.\n    """"""\n    parsed = urlsplit(url)\n\n    scheme: str = parsed.scheme\n    netloc: str = parsed.netloc\n    path: str = parsed.path\n\n    try:\n        port = parsed.port\n    except ValueError:\n        port = None\n\n    result = (scheme.lower() == \'https\' and\n              netloc.lower().split(\':\')[0] == \'s3.amazonaws.com\' and\n              path.startswith(\'/echo.api/\') and\n              (port == 443 or port is None))\n\n    return result\n\n\ndef extract_certs(certs_txt: str) -> List[crypto.X509]:\n    """"""Extracts pycrypto X509 objects from SSL certificates chain string.\n\n    Args:\n        certs_txt: SSL certificates chain string.\n\n    Returns:\n        result: List of pycrypto X509 objects.\n    """"""\n    pattern = r\'-----BEGIN CERTIFICATE-----.+?-----END CERTIFICATE-----\'\n    certs_txt = re.findall(pattern, certs_txt, flags=re.DOTALL)\n    certs = [crypto.load_certificate(crypto.FILETYPE_PEM, cert_txt) for cert_txt in certs_txt]\n    return certs\n\n\ndef verify_sans(amazon_cert: crypto.X509) -> bool:\n    """"""Verifies Subject Alternative Names (SANs) for Amazon certificate.\n\n    Args:\n        amazon_cert: Pycrypto X509 Amazon certificate.\n\n    Returns:\n        result: True if verification was successful, False if not.\n    """"""\n    cert_extentions = [amazon_cert.get_extension(i) for i in range(amazon_cert.get_extension_count())]\n    subject_alt_names = \'\'\n\n    for extention in cert_extentions:\n        if \'subjectAltName\' in str(extention.get_short_name()):\n            subject_alt_names = extention.__str__()\n            break\n\n    result = \'echo-api.amazon.com\' in subject_alt_names\n\n    return result\n\n\ndef verify_certs_chain(certs_chain: List[crypto.X509], amazon_cert: crypto.X509) -> bool:\n    """"""Verifies if Amazon and additional certificates creates chain of trust to a root CA.\n\n    Args:\n        certs_chain: List of pycrypto X509 intermediate certificates from signature chain URL.\n        amazon_cert: Pycrypto X509 Amazon certificate.\n\n    Returns:\n        result: True if verification was successful, False if not.\n    """"""\n    store = crypto.X509Store()\n\n    # add certificates from Amazon provided certs chain\n    for cert in certs_chain:\n        store.add_cert(cert)\n\n    # add CA certificates\n    default_verify_paths = ssl.get_default_verify_paths()\n\n    default_verify_file = default_verify_paths.cafile\n    default_verify_file = Path(default_verify_file).resolve() if default_verify_file else None\n\n    default_verify_path = default_verify_paths.capath\n    default_verify_path = Path(default_verify_path).resolve() if default_verify_path else None\n\n    ca_files = [ca_file for ca_file in default_verify_path.iterdir()] if default_verify_path else []\n    if default_verify_file:\n        ca_files.append(default_verify_file)\n\n    for ca_file in ca_files:\n        ca_file: Path\n        if ca_file.is_file():\n            with ca_file.open(\'r\', encoding=\'ascii\') as crt_f:\n                ca_certs_txt = crt_f.read()\n                ca_certs = extract_certs(ca_certs_txt)\n                for cert in ca_certs:\n                    store.add_cert(cert)\n\n    # add CA certificates (Windows)\n    ssl_context = ssl.create_default_context()\n    der_certs = ssl_context.get_ca_certs(binary_form=True)\n    pem_certs = \'\\n\'.join([ssl.DER_cert_to_PEM_cert(der_cert) for der_cert in der_certs])\n    ca_certs = extract_certs(pem_certs)\n    for ca_cert in ca_certs:\n        store.add_cert(ca_cert)\n\n    store_context = crypto.X509StoreContext(store, amazon_cert)\n\n    try:\n        store_context.verify_certificate()\n        result = True\n    except crypto.X509StoreContextError:\n        result = False\n\n    return result\n\n\ndef verify_signature(amazon_cert: crypto.X509, signature: str, request_body: bytes) -> bool:\n    """"""Verifies Alexa request signature.\n\n    Args:\n        amazon_cert: Pycrypto X509 Amazon certificate.\n        signature: Base64 decoded Alexa request signature from Signature HTTP header.\n        request_body: full HTTPS request body\n    Returns:\n        result: True if verification was successful, False if not.\n    """"""\n    signature = base64.b64decode(signature)\n\n    try:\n        crypto.verify(amazon_cert, signature, request_body, \'sha1\')\n        result = True\n    except crypto.Error:\n        result = False\n\n    return result\n\n\ndef verify_cert(signature_chain_url: str) -> Optional[crypto.X509]:\n    """"""Conducts series of Alexa SSL certificate verifications against Amazon Alexa requirements.\n\n    Args:\n        signature_chain_url: Signature certificate URL from SignatureCertChainUrl HTTP header.\n    Returns:\n        result: Amazon certificate if verification was successful, None if not.\n    """"""\n    try:\n        certs_chain_get = requests.get(signature_chain_url)\n    except requests.exceptions.ConnectionError as e:\n        log.error(f\'Amazon signature chain get error: {e}\')\n        return None\n\n    certs_chain_txt = certs_chain_get.text\n    certs_chain = extract_certs(certs_chain_txt)\n\n    amazon_cert: crypto.X509 = certs_chain.pop(0)\n\n    # verify signature chain url\n    sc_url_verification = verify_sc_url(signature_chain_url)\n    if not sc_url_verification:\n        log.error(f\'Amazon signature url {signature_chain_url} was not verified\')\n\n    # verify not expired\n    expired_verification = not amazon_cert.has_expired()\n    if not expired_verification:\n        log.error(f\'Amazon certificate ({signature_chain_url}) expired\')\n\n    # verify subject alternative names\n    sans_verification = verify_sans(amazon_cert)\n    if not sans_verification:\n        log.error(f\'Subject alternative names verification for ({signature_chain_url}) certificate failed\')\n\n    # verify certs chain\n    chain_verification = verify_certs_chain(certs_chain, amazon_cert)\n    if not chain_verification:\n        log.error(f\'Certificates chain verification for ({signature_chain_url}) certificate failed\')\n\n    result = (sc_url_verification and expired_verification and sans_verification and chain_verification)\n\n    return amazon_cert if result else None\n'"
deeppavlov/utils/ms_bot_framework/__init__.py,0,b'from .server import start_ms_bf_server\n'
deeppavlov/utils/ms_bot_framework/server.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom pathlib import Path\nfrom queue import Queue\nfrom typing import Optional\n\nimport uvicorn\nfrom fastapi import FastAPI\n\nfrom deeppavlov.core.common.log import log_config\nfrom deeppavlov.utils.connector import MSBot\nfrom deeppavlov.utils.server import get_server_params, get_ssl_params, redirect_root_to_docs\n\nlog = getLogger(__name__)\napp = FastAPI()\n\n\ndef start_ms_bf_server(model_config: Path,\n                       app_id: Optional[str],\n                       app_secret: Optional[str],\n                       port: Optional[int] = None,\n                       https: Optional[bool] = None,\n                       ssl_key: Optional[str] = None,\n                       ssl_cert: Optional[str] = None) -> None:\n\n    server_params = get_server_params(model_config)\n\n    host = server_params[\'host\']\n    port = port or server_params[\'port\']\n\n    ssl_config = get_ssl_params(server_params, https, ssl_key=ssl_key, ssl_cert=ssl_cert)\n\n    input_q = Queue()\n    bot = MSBot(model_config, input_q, app_id, app_secret)\n    bot.start()\n\n    endpoint = \'/v3/conversations\'\n    redirect_root_to_docs(app, \'answer\', endpoint, \'post\')\n\n    @app.post(endpoint)\n    async def answer(activity: dict) -> dict:\n        bot.input_queue.put(activity)\n        return {}\n\n    uvicorn.run(app, host=host, port=port, log_config=log_config, ssl_version=ssl_config.version,\n                ssl_keyfile=ssl_config.keyfile, ssl_certfile=ssl_config.certfile)\n    bot.join()\n'"
deeppavlov/utils/pip_wrapper/__init__.py,0,b'from .pip_wrapper import *\n'
deeppavlov/utils/pip_wrapper/pip_wrapper.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport re\nimport subprocess\nimport sys\nfrom logging import getLogger\nfrom pathlib import Path\n\nfrom deeppavlov.core.commands.utils import expand_path, parse_config\nfrom deeppavlov.core.data.utils import get_all_elems_from_json\n\nlog = getLogger(__name__)\n\n_tf_re = re.compile(r\'\\s*tensorflow\\s*([<=>;]|$)\')\n\n\ndef install(*packages):\n    if any(_tf_re.match(package) for package in packages) \\\n            and b\'tensorflow-gpu\' in subprocess.check_output([sys.executable, \'-m\', \'pip\', \'freeze\'],\n                                                             env=os.environ.copy()):\n        log.warning(\'found tensorflow-gpu installed, so upgrading it instead of tensorflow\')\n        packages = [_tf_re.sub(r\'tensorflow-gpu\\1\', package) for package in packages]\n    result = subprocess.check_call([sys.executable, \'-m\', \'pip\', \'install\',\n                                    *[re.sub(r\'\\s\', \'\', package) for package in packages]],\n                                   env=os.environ.copy())\n    return result\n\n\ndef get_config_requirements(config: [str, Path, dict]):\n    config = parse_config(config)\n\n    requirements = set()\n    for req in config.get(\'metadata\', {}).get(\'requirements\', []):\n        requirements.add(req)\n\n    config_references = [expand_path(config_ref) for config_ref in get_all_elems_from_json(config, \'config_path\')]\n    requirements |= {req for config in config_references for req in get_config_requirements(config)}\n\n    return requirements\n\n\ndef install_from_config(config: [str, Path, dict]):\n    requirements_files = get_config_requirements(config)\n\n    if not requirements_files:\n        log.warning(\'No requirements found in config\')\n        return\n\n    requirements = []\n    for rf in requirements_files:\n        with expand_path(rf).open(encoding=\'utf8\') as f:\n            for line in f:\n                line = re.sub(r\'\\s\', \'\', line.strip())\n                if line and not line.startswith(\'#\') and line not in requirements:\n                    requirements.append(line)\n\n    for r in requirements:\n        install(r)\n'"
deeppavlov/utils/server/__init__.py,0,"b'from .server import get_server_params, get_ssl_params, redirect_root_to_docs, start_model_server\n'"
deeppavlov/utils/server/server.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport asyncio\nfrom collections import namedtuple\nfrom logging import getLogger\nfrom pathlib import Path\nfrom ssl import PROTOCOL_TLSv1_2\nfrom typing import Dict, List, Optional, Union\n\nimport uvicorn\nfrom fastapi import Body, FastAPI, HTTPException\nfrom fastapi.utils import generate_operation_id_for_path\nfrom pydantic import BaseConfig, BaseModel\nfrom pydantic.fields import Field, ModelField\nfrom pydantic.main import ModelMetaclass\nfrom starlette.middleware.cors import CORSMiddleware\nfrom starlette.responses import RedirectResponse\n\nfrom deeppavlov.core.commands.infer import build_model\nfrom deeppavlov.core.commands.utils import parse_config\nfrom deeppavlov.core.common.chainer import Chainer\nfrom deeppavlov.core.common.file import read_json\nfrom deeppavlov.core.common.log import log_config\nfrom deeppavlov.core.common.paths import get_settings_path\nfrom deeppavlov.core.data.utils import check_nested_dict_keys, jsonify_data\nfrom deeppavlov.utils.connector import DialogLogger\n\nSERVER_CONFIG_PATH = get_settings_path() / \'server_config.json\'\nSSLConfig = namedtuple(\'SSLConfig\', [\'version\', \'keyfile\', \'certfile\'])\n\n\nlog = getLogger(__name__)\ndialog_logger = DialogLogger(logger_name=\'rest_api\')\n\napp = FastAPI(__file__)\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\'*\'],\n    allow_credentials=True,\n    allow_methods=[\'*\'],\n    allow_headers=[\'*\']\n)\n\n\ndef get_server_params(model_config: Union[str, Path]) -> Dict:\n    server_config = read_json(SERVER_CONFIG_PATH)\n    model_config = parse_config(model_config)\n\n    server_params = server_config[\'common_defaults\']\n\n    if check_nested_dict_keys(model_config, [\'metadata\', \'server_utils\']):\n        model_tag = model_config[\'metadata\'][\'server_utils\']\n        if check_nested_dict_keys(server_config, [\'model_defaults\', model_tag]):\n            model_defaults = server_config[\'model_defaults\'][model_tag]\n            for param_name in model_defaults.keys():\n                if model_defaults[param_name]:\n                    server_params[param_name] = model_defaults[param_name]\n\n    server_params[\'model_endpoint\'] = server_params.get(\'model_endpoint\', \'/model\')\n\n    arg_names = server_params[\'model_args_names\'] or model_config[\'chainer\'][\'in\']\n    if isinstance(arg_names, str):\n        arg_names = [arg_names]\n    server_params[\'model_args_names\'] = arg_names\n\n    return server_params\n\n\ndef get_ssl_params(server_params: dict,\n                   https: Optional[bool],\n                   ssl_key: Optional[str],\n                   ssl_cert: Optional[str]) -> SSLConfig:\n    https = https or server_params[\'https\']\n    if https:\n        ssh_key_path = Path(ssl_key or server_params[\'https_key_path\']).resolve()\n        if not ssh_key_path.is_file():\n            e = FileNotFoundError(\'Ssh key file not found: please provide correct path in --key param or \'\n                                  \'https_key_path param in server configuration file\')\n            log.error(e)\n            raise e\n\n        ssh_cert_path = Path(ssl_cert or server_params[\'https_cert_path\']).resolve()\n        if not ssh_cert_path.is_file():\n            e = FileNotFoundError(\'Ssh certificate file not found: please provide correct path in --cert param or \'\n                                  \'https_cert_path param in server configuration file\')\n            log.error(e)\n            raise e\n\n        ssl_config = SSLConfig(version=PROTOCOL_TLSv1_2, keyfile=str(ssh_key_path), certfile=str(ssh_cert_path))\n    else:\n        ssl_config = SSLConfig(None, None, None)\n\n    return ssl_config\n\n\ndef redirect_root_to_docs(fast_app: FastAPI, func_name: str, endpoint: str, method: str) -> None:\n    """"""Adds api route to server that redirects user from root to docs with opened `endpoint` description.""""""\n\n    @fast_app.get(\'/\', include_in_schema=False)\n    async def redirect_to_docs() -> RedirectResponse:\n        operation_id = generate_operation_id_for_path(name=func_name, path=endpoint, method=method)\n        response = RedirectResponse(url=f\'/docs#/default/{operation_id}\')\n        return response\n\n\ndef interact(model: Chainer, payload: Dict[str, Optional[List]]) -> List:\n    model_args = payload.values()\n    dialog_logger.log_in(payload)\n    error_msg = None\n    lengths = {len(model_arg) for model_arg in model_args if model_arg is not None}\n\n    if not lengths:\n        error_msg = \'got empty request\'\n    elif 0 in lengths:\n        error_msg = \'got empty array as model argument\'\n    elif len(lengths) > 1:\n        error_msg = \'got several different batch sizes\'\n\n    if error_msg is not None:\n        log.error(error_msg)\n        raise HTTPException(status_code=400, detail=error_msg)\n\n    batch_size = next(iter(lengths))\n    model_args = [arg or [None] * batch_size for arg in model_args]\n\n    prediction = model(*model_args)\n    if len(model.out_params) == 1:\n        prediction = [prediction]\n    prediction = list(zip(*prediction))\n    result = jsonify_data(prediction)\n    dialog_logger.log_out(result)\n    return result\n\n\ndef test_interact(model: Chainer, payload: Dict[str, Optional[List]]) -> List[str]:\n    model_args = [arg or [""Test string.""] for arg in payload.values()]\n    try:\n        _ = model(*model_args)\n        return [""Test passed""]\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=repr(e))\n\n\ndef start_model_server(model_config: Path,\n                       https: Optional[bool] = None,\n                       ssl_key: Optional[str] = None,\n                       ssl_cert: Optional[str] = None,\n                       port: Optional[int] = None) -> None:\n\n    server_params = get_server_params(model_config)\n\n    host = server_params[\'host\']\n    port = port or server_params[\'port\']\n    model_endpoint = server_params[\'model_endpoint\']\n    model_args_names = server_params[\'model_args_names\']\n\n    ssl_config = get_ssl_params(server_params, https, ssl_key=ssl_key, ssl_cert=ssl_cert)\n\n    model = build_model(model_config)\n\n    def batch_decorator(cls: ModelMetaclass) -> ModelMetaclass:\n        cls.__annotations__ = {arg_name: list for arg_name in model_args_names}\n        cls.__fields__ = {arg_name: ModelField(name=arg_name, type_=list, class_validators=None,\n                                               model_config=BaseConfig, required=False, field_info=Field(None))\n                          for arg_name in model_args_names}\n        return cls\n\n    @batch_decorator\n    class Batch(BaseModel):\n        pass\n\n    redirect_root_to_docs(app, \'answer\', model_endpoint, \'post\')\n\n    model_endpoint_post_example = {arg_name: [\'string\'] for arg_name in model_args_names}\n\n    @app.post(model_endpoint, summary=\'A model endpoint\')\n    async def answer(item: Batch = Body(..., example=model_endpoint_post_example)) -> List:\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(None, interact, model, item.dict())\n\n    @app.post(\'/probe\', include_in_schema=False)\n    async def probe(item: Batch) -> List[str]:\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(None, test_interact, model, item.dict())\n\n    @app.get(\'/api\', summary=\'Model argument names\')\n    async def api() -> List[str]:\n        return model_args_names\n\n    uvicorn.run(app, host=host, port=port, log_config=log_config, ssl_version=ssl_config.version,\n                ssl_keyfile=ssl_config.keyfile, ssl_certfile=ssl_config.certfile, timeout_keep_alive=20)\n'"
deeppavlov/utils/settings/__init__.py,0,b''
deeppavlov/utils/socket/__init__.py,0,"b'from .socket import encode, start_socket_server\n'"
deeppavlov/utils/socket/socket.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport asyncio\nimport json\nfrom logging import getLogger\nfrom pathlib import Path\nfrom struct import pack, unpack\nfrom typing import Any, List, Optional, Tuple, Union\n\nfrom deeppavlov.core.commands.infer import build_model\nfrom deeppavlov.core.common.chainer import Chainer\nfrom deeppavlov.core.data.utils import jsonify_data\nfrom deeppavlov.utils.connector import DialogLogger\nfrom deeppavlov.utils.server import get_server_params\n\nHEADER_FORMAT = \'<I\'\n\nlog = getLogger(__name__)\ndialog_logger = DialogLogger(logger_name=\'socket_api\')\n\n\ndef encode(data: Any) -> bytes:\n    """"""\xd0\xa1onverts data to the socket server input formatted bytes array.\n\n    Serializes ``data`` to the JSON formatted bytes array and adds 4 bytes to the beginning of the array - packed\n    to bytes length of the JSON formatted bytes array. Header format is ""<I""\n    (see https://docs.python.org/3/library/struct.html#struct-format-strings)\n\n    Args:\n        data: Object to pact to the bytes array.\n\n    Raises:\n        TypeError: If data is not JSON-serializable object.\n\n    Examples:\n        >>> from deeppavlov.utils.socket import encode\n        >>> encode({\'a\':1})\n        b\'\\x08\\x00\\x00\\x00{""a"": 1}\n        >>> encode([42])\n        b\'\\x04\\x00\\x00\\x00[42]\'\n\n    """"""\n    json_data = jsonify_data(data)\n    bytes_data = json.dumps(json_data).encode()\n    response = pack(HEADER_FORMAT, len(bytes_data)) + bytes_data\n    return response\n\n\nclass SocketServer:\n    """"""Creates socket server that sends the received data to the DeepPavlov model and returns model response.\n\n    The server receives bytes array consists of the `header` and the `body`. The `header` is the first 4 bytes\n    of the array - `body` length in bytes represented by a packed unsigned int (byte order is little-endian).\n    `body` is dictionary serialized to JSON formatted bytes array that server sends to the model. The dictionary\n    keys should match model arguments names, the values should be lists or tuples of inferenced values.\n\n    Socket server request creation example:\n        >>> from deeppavlov.utils.socket import encode\n        >>> request = encode({""context"":[""Elon Musk launched his cherry Tesla roadster to the Mars orbit""]})\n        >>> request\n        b\'I\\x00\\x00\\x00{""x"": [""Elon Musk launched his cherry Tesla roadster to the Mars orbit""]}\'\n\n    Socket server response, like the request, consists of the header and the body. Response body is dictionary\n    {\'status\': status, \'payload\': payload} serialized to a JSON formatted byte array, where:\n        status (str): \'OK\' if the model successfully processed the data, else - error message.\n        payload: (Optional[List[Tuple]]): The model result if no error has occurred, otherwise None.\n\n    """"""\n    _launch_msg: str\n    _loop: asyncio.AbstractEventLoop\n    _model: Chainer\n    _model_args_names: List\n\n    def __init__(self,\n                 model_config: Path,\n                 socket_type: str,\n                 port: Optional[int] = None,\n                 socket_file: Optional[Union[str, Path]] = None) -> None:\n        """"""Initializes socket server.\n\n        Args:\n            model_config: Path to the config file.\n            socket_type: Socket family. ""TCP"" for the AF_INET socket server, ""UNIX"" for UNIX Domain Socket server.\n            port: Port number for the AF_INET address family. If parameter is not defined, the port number from the\n                utils/settings/server_config.json is used.\n            socket_file: Path to the file to which UNIX Domain Socket server connects. If parameter is not defined,\n                the path from the utils/settings/server_config.json is used.\n\n        Raises:\n            ValueError: If ``socket_type`` parameter is neither ""TCP"" nor ""UNIX"".\n\n        """"""\n        server_params = get_server_params(model_config)\n        socket_type = socket_type or server_params[\'socket_type\']\n        self._loop = asyncio.get_event_loop()\n\n        if socket_type == \'TCP\':\n            host = server_params[\'host\']\n            port = port or server_params[\'port\']\n            self._launch_msg = f\'{server_params[""socket_launch_message""]} http://{host}:{port}\'\n            self._loop.create_task(asyncio.start_server(self._handle_client, host, port))\n        elif socket_type == \'UNIX\':\n            socket_file = socket_file or server_params[\'unix_socket_file\']\n            socket_path = Path(socket_file).resolve()\n            if socket_path.exists():\n                socket_path.unlink()\n            self._launch_msg = f\'{server_params[""socket_launch_message""]} {socket_file}\'\n            self._loop.create_task(asyncio.start_unix_server(self._handle_client, socket_file))\n        else:\n            raise ValueError(f\'socket type ""{socket_type}"" is not supported\')\n\n        self._model = build_model(model_config)\n        self._model_args_names = server_params[\'model_args_names\']\n\n    def start(self) -> None:\n        """"""Launches socket server""""""\n        log.info(self._launch_msg)\n        try:\n            self._loop.run_forever()\n        except KeyboardInterrupt:\n            pass\n        except Exception as e:\n            log.error(f\'got exception {e} while running server\')\n        finally:\n            self._loop.close()\n\n    async def _handle_client(self, reader: asyncio.StreamReader, writer: asyncio.StreamWriter) -> None:\n        """"""Handles connection from a client.\n\n        Validates requests, sends request body to DeepPavlov model, sends responses to client.\n\n        """"""\n        addr = writer.get_extra_info(\'peername\')\n        log.info(f\'handling connection from {addr}\')\n        while True:\n            header = await reader.read(4)\n            if not header:\n                log.info(f\'closing connection from {addr}\')\n                writer.close()\n                break\n            elif len(header) != 4:\n                error_msg = f\'header ""{header}"" length less than 4 bytes\'\n                log.error(error_msg)\n                response = self._response(error_msg)\n            else:\n                data_len = unpack(HEADER_FORMAT, header)[0]\n                request_body = await reader.read(data_len)\n                try:\n                    data = json.loads(request_body)\n                    response = await self._interact(data)\n                except ValueError:\n                    error_msg = f\'request ""{request_body}"" type is not json\'\n                    log.error(error_msg)\n                    response = self._response(error_msg)\n            writer.write(response)\n            await writer.drain()\n\n    async def _interact(self, data: dict) -> bytes:\n        dialog_logger.log_in(data)\n        model_args = []\n        for param_name in self._model_args_names:\n            param_value = data.get(param_name)\n            if param_value is None or (isinstance(param_value, list) and len(param_value) > 0):\n                model_args.append(param_value)\n            else:\n                error_msg = f""nonempty array expected but got \'{param_name}\'={repr(param_value)}""\n                log.error(error_msg)\n                return self._response(error_msg)\n        lengths = {len(i) for i in model_args if i is not None}\n\n        if not lengths:\n            error_msg = \'got empty request\'\n            log.error(error_msg)\n            return self._response(error_msg)\n        elif len(lengths) > 1:\n            error_msg = f\'got several different batch sizes: {lengths}\'\n            log.error(error_msg)\n            return self._response(error_msg)\n\n        batch_size = list(lengths)[0]\n        model_args = [arg or [None] * batch_size for arg in model_args]\n\n        # in case when some parameters were not described in model_args\n        model_args += [[None] * batch_size for _ in range(len(self._model.in_x) - len(model_args))]\n\n        prediction = await self._loop.run_in_executor(None, self._model, *model_args)\n        if len(self._model.out_params) == 1:\n            prediction = [prediction]\n        prediction = list(zip(*prediction))\n        dialog_logger.log_out(prediction)\n        return self._response(payload=prediction)\n\n    @staticmethod\n    def _response(status: str = \'OK\', payload: Optional[List[Tuple]] = None) -> bytes:\n        """"""Puts arguments into dict and serialize it to JSON formatted byte array with header.\n\n        Args:\n            status: Response status. \'OK\' if no error has occurred, otherwise error message.\n            payload: DeepPavlov model result if no error has occurred, otherwise None.\n\n        Returns:\n            dict({\'status\': status, \'payload\': payload}) serialized to a JSON formatted byte array starting with the\n                4-byte header - the length of serialized dict in bytes.\n\n        """"""\n        return encode({\'status\': status, \'payload\': payload})\n\n\ndef start_socket_server(model_config: Path, socket_type: str, port: Optional[int],\n                        socket_file: Optional[Union[str, Path]]) -> None:\n    server = SocketServer(model_config, socket_type, port, socket_file)\n    server.start()\n'"
deeppavlov/utils/telegram/__init__.py,0,b'from .telegram_ui import interact_model_by_telegram\n'
deeppavlov/utils/telegram/telegram_ui.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom pathlib import Path\nfrom typing import Optional, Union\n\nfrom deeppavlov.utils.connector import TelegramBot\n\n\ndef interact_model_by_telegram(model_config: Union[str, Path, dict], token: Optional[str] = None) -> None:\n    bot = TelegramBot(model_config, token)\n    bot.start()\n'"
deeppavlov/deprecated/agents/default_agent/__init__.py,0,b'from .default_agent import DefaultAgent\n'
deeppavlov/deprecated/agents/default_agent/default_agent.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import List, Optional\n\nfrom deeppavlov.deprecated.agent import Agent, Filter, Processor\nfrom deeppavlov.deprecated.agents.filters import TransparentFilter\nfrom deeppavlov.deprecated.agents.processors import HighestConfidenceSelector\nfrom deeppavlov.core.models.component import Component\n\n\nclass DefaultAgent(Agent):\n    """"""\n    DeepPavlov default implementation of Agent abstraction.\n\n    Default Agent is an implementation of agent template, with following\n    pipeline for each utterance batch received by agent:\n    1) Utterance batch is processed through agent Filter which selects utterances to be processed with each agent skill;\n    2) Utterances are processed through skills selected for them;\n    3) Utterances and skill responses are processed through agent Processor which generates agent\'s response for the outer world.\n    Defining DefaultAgent means:\n    a) To define set of skills it uses;\n    b) To implement skills Filter;\n    c) To implement Processor.\n    You can refer to :class:`deeppavlov.deprecated.skill.Skill`, :class:`deeppavlov.deprecated.agent.Filter`,\n    :class:`deeppavlov.deprecated.agent.Processor` base classes to get more info.\n\n    Args:\n        skills: List of initiated agent skills or components instances.\n        skills_processor: Initiated agent processor.\n        skills_filter: Initiated agent filter.\n\n    Attributes:\n        skills: List of initiated agent skills instances.\n        skills_processor: Initiated agent processor.\n        skills_filter: Initiated agent filter.\n    """"""\n\n    def __init__(self, skills: List[Component], skills_processor: Optional[Processor] = None,\n                 skills_filter: Optional[Filter] = None, *args, **kwargs) -> None:\n        super(DefaultAgent, self).__init__(skills=skills)\n        self.skills_filter = skills_filter or TransparentFilter(len(skills))\n        self.skills_processor = skills_processor or HighestConfidenceSelector()\n\n    def _call(self, utterances_batch: list, utterances_ids: Optional[list] = None) -> list:\n        """"""\n        Processes batch of utterances and returns corresponding responses batch.\n\n        Each call of Agent passes incoming utterances batch through skills filter,\n        agent skills, skills processor. Batch of dialog IDs can be provided, in\n        other case utterances indexes in incoming batch are used as dialog IDs.\n\n        Args:\n            utterances_batch: Batch of incoming utterances.\n            utterances_ids: Batch of dialog IDs corresponding to incoming utterances.\n\n        Returns:\n            responses: A batch of responses corresponding to the\n                utterance batch received by agent.\n        """"""\n        batch_size = len(utterances_batch)\n        ids = utterances_ids or list(range(batch_size))\n        batch_history = [self.history[utt_id] for utt_id in ids]\n        responses = []\n\n        filtered = self.skills_filter(utterances_batch, batch_history)\n\n        for skill_i, (filtered_utterances, skill) in enumerate(zip(filtered, self.wrapped_skills)):\n            skill_i_utt_indexes = [utt_index for utt_index, utt_filter in enumerate(filtered_utterances) if utt_filter]\n\n            if skill_i_utt_indexes:\n                skill_i_utt_batch = [utterances_batch[i] for i in skill_i_utt_indexes]\n                skill_i_utt_ids = [ids[i] for i in skill_i_utt_indexes]\n                res = [(None, 0.)] * batch_size\n                predicted, confidence = skill(skill_i_utt_batch, skill_i_utt_ids)\n\n                for i, predicted, confidence in zip(skill_i_utt_indexes, predicted, confidence):\n                    res[i] = (predicted, confidence)\n\n                responses.append(res)\n\n        responses = self.skills_processor(utterances_batch, batch_history, *responses)\n\n        return responses\n'"
deeppavlov/deprecated/agents/ecommerce_agent/__init__.py,0,b'from .ecommerce_agent import EcommerceAgent\n'
deeppavlov/deprecated/agents/ecommerce_agent/ecommerce_agent.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nfrom collections import defaultdict\nfrom logging import getLogger\nfrom typing import List, Dict, Any\n\nfrom deeppavlov.core.commands.infer import build_model\nfrom deeppavlov.deep import find_config\nfrom deeppavlov.deprecated.agent import Agent, RichMessage\nfrom deeppavlov.deprecated.agents.rich_content import PlainText, ButtonsFrame, Button\nfrom deeppavlov.deprecated.skill import Skill\nfrom deeppavlov.utils.ms_bot_framework import start_ms_bf_server\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""-i"", ""--ms-id"", help=""microsoft bot framework app id"", type=str)\nparser.add_argument(""-s"", ""--ms-secret"", help=""microsoft bot framework app secret"", type=str)\n\nlog = getLogger(__name__)\n\n\nclass EcommerceAgent(Agent):\n    """"""DeepPavlov Ecommerce agent.\n\n    Args:\n        skill: List of initiated agent skills instances.\n\n    Attributes:\n        skill: List of initiated agent skills instances.\n        history: Histories for each each dialog with agent indexed\n            by dialog ID. Each history is represented by list of incoming\n            and outcoming replicas of the dialog.\n        states: States for each each dialog with agent indexed by dialog ID.\n    """"""\n\n    def __init__(self, skills: List[Skill], *args, **kwargs) -> None:\n        super(EcommerceAgent, self).__init__(skills=skills)\n        self.states: dict = defaultdict(lambda: [{""start"": 0, ""stop"": 5} for _ in self.skills])\n\n    def _call(self, utterances_batch: List[str], utterances_ids: List[int] = None) -> List[RichMessage]:\n        """"""Processes batch of utterances and returns corresponding responses batch.\n\n        Args:\n            utterances_batch: Batch of incoming utterances.\n            utterances_ids: Batch of dialog IDs corresponding to incoming utterances.\n\n        Returns:\n            responses: A batch of responses corresponding to the\n                utterance batch received by agent.\n        """"""\n\n        rich_message = RichMessage()\n        for utt_id, utt in enumerate(utterances_batch):\n\n            if utterances_ids:\n                id_ = utterances_ids[utt_id]\n\n            log.debug(f\'Utterance: {utt}\')\n\n            if utt == ""/start"":\n                welcome = ""I am a new e-commerce bot. I will help you to find products that you are looking for. Please type your request in plain text.""\n                rich_message.add_control(PlainText(welcome))\n                continue\n\n            if utt[0] == ""@"":\n                command, *parts = utt.split("":"")\n                log.debug(f\'Actions: {parts}\')\n\n                if command == ""@details"":\n                    batch_index = int(parts[0])  # batch index in history list\n                    item_index = int(parts[1])  # index in batch\n                    rich_message.add_control(PlainText(show_details(\n                        self.history[id_][batch_index][item_index])))\n                    continue\n\n                if command == ""@entropy"":\n                    state = self.history[id_][int(parts[0])]\n                    state[parts[1]] = parts[2]\n                    state[""start""] = 0\n                    state[""stop""] = 5\n                    utt = state[\'query\']\n                    self.states[id_] = state\n\n                if command == ""@next"":\n                    state = self.history[id_][int(parts[0])]\n                    state[\'start\'] = state[\'stop\']\n                    state[\'stop\'] = state[\'stop\'] + 5\n                    utt = state[\'query\']\n                    self.states[id_] = state\n            else:\n                if id_ not in self.states:\n                    self.states[id_] = {}\n\n                self.states[id_][""start""] = 0\n                self.states[id_][""stop""] = 5\n\n            responses_batch, confidences_batch, state_batch = self.skills[0](\n                [utt], self.history[id_], [self.states[id_]])\n\n            # update `self.states` with retrieved results\n            self.states[id_] = state_batch[0]\n            self.states[id_][""query""] = utt\n\n            items_batch, entropy_batch = responses_batch\n\n            for batch_idx, items in enumerate(items_batch):\n\n                self.history[id_].append(items)\n                self.history[id_].append(self.states[id_])\n\n                for idx, item in enumerate(items):\n                    rich_message.add_control(_draw_item(item, idx, self.history[id_]))\n\n                if len(items) == self.states[id_][\'stop\'] - self.states[id_][\'start\']:\n                    buttons_frame = _draw_tail(entropy_batch[batch_idx], self.history[id_])\n                    rich_message.add_control(buttons_frame)\n\n        return [rich_message]\n\n\ndef _draw_tail(entropy, history):\n    buttons_frame = ButtonsFrame(text="""")\n    buttons_frame.add_button(Button(\'More\', ""@next:"" + str(len(history) - 1)))\n    caption = ""Press More ""\n\n    if entropy:\n        caption += ""specify a "" + entropy[0][1]\n        for ent_value in entropy[0][2][:4]:\n            button_a = Button(ent_value[0], f\'@entropy:{len(history) - 1}:{entropy[0][1]}:{ent_value[0]}\')\n            buttons_frame.add_button(button_a)\n\n    buttons_frame.text = caption\n    return buttons_frame\n\n\ndef _draw_item(item, idx, history):\n    title = item[\'Title\']\n    if \'ListPrice\' in item:\n        title += "" - **$"" + item[\'ListPrice\'].split(\'$\')[1] + ""**""\n\n    buttons_frame = ButtonsFrame(text=title)\n    buttons_frame.add_button(Button(\'Show details\', ""@details:"" + str(len(history) - 2) + "":"" + str(idx)))\n    return buttons_frame\n\n\ndef show_details(item_data: Dict[Any, Any]) -> str:\n    """"""Format catalog item output\n\n    Parameters:\n        item_data: item\'s attributes values\n\n    Returns:\n        [rich_message]: list of formatted rich message\n    """"""\n\n    txt = """"\n\n    for key, value in item_data.items():\n        txt += ""**"" + str(key) + ""**"" + \': \' + str(value) + ""  \\n""\n\n    return txt\n\n\ndef make_agent() -> EcommerceAgent:\n    """"""Make an agent\n\n    Returns:\n        agent: created Ecommerce agent\n    """"""\n\n    config_path = find_config(\'tfidf_retrieve\')\n    skill = build_model(config_path)\n    agent = EcommerceAgent(skills=[skill])\n    return agent\n\n\ndef main():\n    """"""Parse parameters and run ms bot framework""""""\n\n    args = parser.parse_args()\n    start_ms_bf_server(app_id=args.ms_id,\n                       app_secret=args.ms_secret)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
deeppavlov/deprecated/agents/filters/__init__.py,0,b'from .transparent_filter import TransparentFilter\n'
deeppavlov/deprecated/agents/filters/transparent_filter.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom deeppavlov.deprecated.agent import Filter\n\n\nclass TransparentFilter(Filter):\n    """"""Filter that applies each agent skill to all of batch utterances.\n\n    Args:\n        skills_count: Number of agent skills.\n\n    Attributes:\n        size: Number of agent skills.\n    """"""\n\n    def __init__(self, skills_count: int, *args, **kwargs) -> None:\n        self.size: int = skills_count\n\n    def __call__(self, utterances_batch: list, history_batch: list) -> list:\n        """"""Returns skills-utterances application matrix.\n\n        Generates skills-utterances application matrix with all True\n        elements.\n\n        Args:\n            utterances_batch: A batch of utterances of any type.\n            history_batch: Not used.\n\n        Returns:\n            response: Skills-utterances application matrix with all True\n                elements.\n        """"""\n        return [[True] * len(utterances_batch)] * self.size\n'"
deeppavlov/deprecated/agents/hello_bot_agent/__init__.py,0,b''
deeppavlov/deprecated/agents/hello_bot_agent/hello_bot_agent.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom deeppavlov.deprecated.agents.default_agent import DefaultAgent\nfrom deeppavlov.deprecated.agents.processors import HighestConfidenceSelector\nfrom deeppavlov.deprecated.skills.pattern_matching_skill import PatternMatchingSkill\n\n\ndef make_hello_bot_agent() -> DefaultAgent:\n    """"""Builds agent based on PatternMatchingSkill and HighestConfidenceSelector.\n\n    This is agent building tutorial. You can use this .py file to check how hello-bot agent works.\n\n    Returns:\n        agent: Agent capable of handling several simple greetings.\n    """"""\n    skill_hello = PatternMatchingSkill([\'Hello world\'], patterns=[\'hi\', \'hello\', \'good day\'])\n    skill_bye = PatternMatchingSkill([\'Goodbye world\', \'See you around\'], patterns=[\'bye\', \'chao\', \'see you\'])\n    skill_fallback = PatternMatchingSkill([\'I don\\\'t understand, sorry\', \'I can say ""Hello world""\'])\n\n    agent = DefaultAgent([skill_hello, skill_bye, skill_fallback], skills_processor=HighestConfidenceSelector())\n\n    return agent\n\n\nif __name__ == \'__main__\':\n    hello_bot_agent = make_hello_bot_agent()\n    response = hello_bot_agent([\'Hello\', \'Bye\', \'Or not\'])\n    print(response)\n'"
deeppavlov/deprecated/agents/processors/__init__.py,0,b'from .default_rich_content_processor import DefaultRichContentWrapper\nfrom .highest_confidence_selector import HighestConfidenceSelector\nfrom .random_selector import RandomSelector\n'
deeppavlov/deprecated/agents/processors/default_rich_content_processor.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom deeppavlov.deprecated.agent import Processor, RichMessage\nfrom deeppavlov.deprecated.agents.rich_content import PlainText\n\n\nclass DefaultRichContentWrapper(Processor):\n    """"""Returns RichControl wrapped responses with highest confidence.""""""\n\n    def __init__(self, *args, **kwargs) -> None:\n        pass\n\n    def __call__(self, utterances: list, batch_history: list, *responses: list) -> list:\n        """"""Selects for each utterance response with highest confidence and wraps them to RichControl objects.\n\n        Args:\n            utterances_batch: Not used.\n            history_batch: Not used.\n            responses: Each response positional argument corresponds to\n                response of one of Agent skills and is represented by\n                batch (list) of (response, confidence) tuple structures.\n\n        Returns:\n            result: A batch of responses corresponding to the utterance\n                batch received by agent.\n        """"""\n        responses, confidences = zip(*[zip(*r) for r in responses])\n        indexes = [c.index(max(c)) for c in zip(*confidences)]\n        result = []\n        for i, *responses in zip(indexes, *responses):\n            rich_message = RichMessage()\n            plain_text = PlainText(str(responses[i]))\n            rich_message.add_control(plain_text)\n            result.append(rich_message)\n        return result\n'"
deeppavlov/deprecated/agents/processors/highest_confidence_selector.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom deeppavlov.deprecated.agent import Processor\n\n\nclass HighestConfidenceSelector(Processor):\n    """"""Returns for each utterance response with highest confidence.""""""\n\n    def __init__(self, *args, **kwargs) -> None:\n        pass\n\n    def __call__(self, utterances: list, batch_history: list, *responses: list) -> list:\n        """"""Selects for each utterance response with highest confidence.\n\n        Args:\n            utterances_batch: Not used.\n            history_batch: Not used.\n            responses: Each response positional argument corresponds to\n                response of one of Agent skills and is represented by\n                batch (list) of (response, confidence) tuple structures.\n\n        Returns:\n            responses: A batch of responses corresponding to the\n                utterance batch received by agent.\n        """"""\n        responses, confidences = zip(*[zip(*r) for r in responses])\n        indexes = [c.index(max(c)) for c in zip(*confidences)]\n        result = [responses[i] for i, *responses in zip(indexes, *responses)]\n        return result\n'"
deeppavlov/deprecated/agents/processors/random_selector.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport random\n\nfrom deeppavlov.deprecated.agent import Processor\n\n\nclass RandomSelector(Processor):\n    """"""Returns response of a random skill for each utterance.""""""\n\n    def __init__(self, *args, **kwargs) -> None:\n        pass\n\n    def __call__(self, utterances: list, batch_history: list, *responses: list) -> list:\n        """"""Selects result of a random skill for each utterance.\n\n        Args:\n            utterances_batch: Not used.\n            history_batch: Not used.\n            responses: Each response positional argument corresponds to\n                response of one of Agent skills and is represented by\n                batch (list) of (response, confidence) tuple structures.\n\n        Returns:\n            result: A batch of responses corresponding to the utterance\n                batch received by agent.\n        """"""\n        result = [random.choice([t for t, sc in r if t]) for r in zip(*responses)]\n        return result\n'"
deeppavlov/deprecated/agents/rich_content/__init__.py,0,"b'from .default_rich_content import Button, ButtonsFrame, PlainText\n'"
deeppavlov/deprecated/agents/rich_content/default_rich_content.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Optional\n\nfrom deeppavlov.deprecated.agent import RichControl\n\n\nclass PlainText(RichControl):\n    """"""Plain text message as a rich control.\n\n    Args:\n        text: Text of the message.\n\n    Attributes:\n        content: Text of the message.\n    """"""\n\n    def __init__(self, text: str) -> None:\n        super(PlainText, self).__init__(\'plain_text\')\n        self.content: str = text\n\n    def __str__(self) -> str:\n        return self.content\n\n    def json(self) -> dict:\n        """"""Returns json compatible state of the PlainText instance.\n\n        Returns:\n            control_json: Json representation of PlainText state.\n        """"""\n        self.control_json[\'content\'] = self.content\n        return self.control_json\n\n    def ms_bot_framework(self) -> dict:\n        """"""Returns MS Bot Framework compatible state of the PlainText instance.\n\n        Creating MS Bot Framework activity blank with ""text"" field populated.\n\n        Returns:\n            out_activity: MS Bot Framework representation of PlainText state.\n        """"""\n        out_activity = {}\n        out_activity[\'type\'] = \'message\'\n        out_activity[\'text\'] = self.content\n        return out_activity\n\n    def alexa(self) -> dict:\n        """"""Returns Amazon Alexa compatible state of the PlainText instance.\n\n        Creating Amazon Alexa response blank with populated ""outputSpeech"" and\n        ""card sections.\n\n        Returns:\n            response: Amazon Alexa representation of PlainText state.\n        """"""\n        response = {\n            \'response\': {\n                \'shouldEndSession\': False,\n                \'outputSpeech\': {\n                    \'type\': \'PlainText\',\n                    \'text\': self.content},\n                \'card\': {\n                    \'type\': \'Simple\',\n                    \'content\': self.content\n                }\n            }\n        }\n\n        return response\n\n\nclass Button(RichControl):\n    """"""Button with plain text callback.\n\n    Args:\n        name: Displayed name of the button.\n        callback: Plain text returned as callback when button pressed.\n\n    Attributes:\n        name: Displayed name of the button.\n        callback: Plain text returned as callback when button pressed.\n    """"""\n\n    def __init__(self, name: str, callback: str) -> None:\n        super(Button, self).__init__(\'button\')\n        self.name: str = name\n        self.callback: str = callback\n\n    def json(self) -> dict:\n        """"""Returns json compatible state of the Button instance.\n\n        Returns:\n            control_json: Json representation of Button state.\n        """"""\n        content = {}\n        content[\'name\'] = self.name\n        content[\'callback\'] = self.callback\n        self.control_json[\'content\'] = content\n        return self.control_json\n\n    def ms_bot_framework(self) -> dict:\n        """"""Returns MS Bot Framework compatible state of the Button instance.\n\n        Creates MS Bot Framework CardAction (button) with postBack value return.\n\n        Returns:\n            control_json: MS Bot Framework representation of Button state.\n        """"""\n        card_action = {}\n        card_action[\'type\'] = \'postBack\'\n        card_action[\'title\'] = self.name\n        card_action[\'value\'] = self.callback = self.callback\n        return card_action\n\n\nclass ButtonsFrame(RichControl):\n    """"""ButtonsFrame is a container for several Buttons objects.\n\n    ButtonsFrame embeds several Buttons and allows to post them\n    in one channel message.\n\n    Args:\n        text: Text displayed with embedded buttons.\n\n    Attributes:\n        text: Text displayed with embedded buttons.\n        content: Container with Button objects.\n    """"""\n\n    def __init__(self, text: Optional[str] = None) -> None:\n        super(ButtonsFrame, self).__init__(\'buttons_frame\')\n        self.text: [str, None] = text\n        self.content: list = []\n\n    def add_button(self, button: Button):\n        """"""Adds Button instance to RichMessage.\n\n        Args:\n            button: Button instance.\n        """"""\n        self.content.append(button)\n\n    def json(self) -> dict:\n        """"""Returns json compatible state of the ButtonsFrame instance.\n\n        Returns json compatible state of the ButtonsFrame instance including\n        all nested buttons.\n\n        Returns:\n            control_json: Json representation of ButtonsFrame state.\n        """"""\n        content = {}\n\n        if self.text:\n            content[\'text\'] = self.text\n\n        content[\'controls\'] = [control.json() for control in self.content]\n\n        self.control_json[\'content\'] = content\n\n        return self.control_json\n\n    def ms_bot_framework(self) -> dict:\n        """"""Returns MS Bot Framework compatible state of the ButtonsFrame instance.\n\n        Creating MS Bot Framework activity blank with RichCard in ""attachments"". RichCard\n        is populated with CardActions corresponding buttons embedded in ButtonsFrame.\n\n        Returns:\n            control_json: MS Bot Framework representation of ButtonsFrame state.\n        """"""\n        rich_card = {}\n\n        buttons = [button.ms_bot_framework() for button in self.content]\n        rich_card[\'buttons\'] = buttons\n\n        if self.text:\n            rich_card[\'title\'] = self.text\n\n        attachments = [\n            {\n                ""contentType"": ""application/vnd.microsoft.card.thumbnail"",\n                ""content"": rich_card\n            }\n        ]\n\n        out_activity = {}\n        out_activity[\'type\'] = \'message\'\n        out_activity[\'attachments\'] = attachments\n\n        return out_activity\n'"
deeppavlov/deprecated/skills/default_skill/__init__.py,0,b'from .default_skill import DefaultStatelessSkill\n'
deeppavlov/deprecated/skills/default_skill/default_skill.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Tuple, Optional, List\n\nfrom deeppavlov.core.common.chainer import Chainer\nfrom deeppavlov.deprecated.skill import Skill\n\nproposals = {\n    \'en\': \'expecting_arg: {}\',\n    \'ru\': \'\xd0\x9f\xd0\xbe\xd0\xb6\xd0\xb0\xd0\xbb\xd1\x83\xd0\xb9\xd1\x81\xd1\x82\xd0\xb0, \xd0\xb2\xd0\xb2\xd0\xb5\xd0\xb4\xd0\xb8\xd1\x82\xd0\xb5 \xd0\xbf\xd0\xb0\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb5\xd1\x82\xd1\x80 {}\'\n}\n\n\nclass DefaultStatelessSkill(Skill):\n    """"""Default stateless skill class.\n\n    The class is intended to be used for as a default skill wrapping DeepPavlov models.\n\n    Attributes:\n        model: DeepPavlov model to be wrapped into default skill instance.\n    """"""\n\n    def __init__(self, model: Chainer, lang: str = \'en\', *args, **kwargs) -> None:\n        self.model = model\n        self.proposal: str = proposals[lang]\n\n    def __call__(self, utterances_batch: list, history_batch: list,\n                 states_batch: Optional[list] = None) -> Tuple[list, list, list]:\n        """"""Returns skill inference result.\n\n        Returns batches of skill inference results, estimated confidence\n            levels and up to date states corresponding to incoming utterance\n            batch. Also handles interaction with multiargument models using\n            skill states.\n\n        Args:\n            utterances_batch: A batch of utterances of any type.\n            history_batch: Not used. A batch of list typed histories for each\n                utterance.\n            states_batch: A batch of states for each utterance.\n\n        Returns:\n            response: A batch of arbitrary typed skill inference results.\n            confidence: A batch of float typed confidence levels for each of\n                skill inference result.\n            states: Optional. A batch of states for each response.\n        """"""\n        batch_len = len(utterances_batch)\n        confidence_batch = [1.0] * batch_len\n\n        response_batch: List[Optional[str]] = [None] * batch_len\n        infer_indexes = []\n\n        if not states_batch:\n            states_batch: List[Optional[dict]] = [None] * batch_len\n\n        for utt_i, utterance in enumerate(utterances_batch):\n            if not states_batch[utt_i]:\n                states_batch[utt_i] = {\'expected_args\': list(self.model.in_x), \'received_values\': []}\n\n            if utterance:\n                states_batch[utt_i][\'expected_args\'].pop(0)\n                states_batch[utt_i][\'received_values\'].append(utterance)\n\n            if states_batch[utt_i][\'expected_args\']:\n                response = self.proposal.format(states_batch[utt_i][\'expected_args\'][0])\n                response_batch[utt_i] = response\n            else:\n                infer_indexes.append(utt_i)\n\n        if infer_indexes:\n            infer_utterances = zip(*[tuple(states_batch[i][\'received_values\']) for i in infer_indexes])\n            infer_results = self.model(*infer_utterances)\n\n            if len(self.model.out_params) > 1:\n                infer_results = [\'; \'.join([str(out_y) for out_y in result]) for result in zip(*infer_results)]\n\n            for infer_i, infer_result in zip(infer_indexes, infer_results):\n                response_batch[infer_i] = infer_result\n                states_batch[infer_i] = None\n\n        return response_batch, confidence_batch, states_batch\n'"
deeppavlov/deprecated/skills/ecommerce_skill/__init__.py,0,b'from .bleu_retrieve import EcommerceSkillBleu\nfrom .tfidf_retrieve import EcommerceSkillTfidf\n\n'
deeppavlov/deprecated/skills/ecommerce_skill/bleu_retrieve.py,0,"b'# Copyright 2018 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport copy\nimport json\nfrom collections import Counter\nfrom logging import getLogger\nfrom operator import itemgetter\nfrom pathlib import Path\nfrom typing import List, Tuple, Dict, Any\n\nimport numpy as np\nfrom scipy.stats import entropy\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.file import save_pickle, load_pickle\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.estimator import Component\nfrom deeppavlov.deprecated.skill import Skill\nfrom deeppavlov.metrics.bleu import bleu_advanced\n\nlog = getLogger(__name__)\n\n\n@register(""ecommerce_skill_bleu"")\nclass EcommerceSkillBleu(Skill):\n    """"""Class to retrieve product items from `load_path` catalogs\n    in sorted order according to the similarity measure\n    Retrieve the specification attributes with corresponding values\n    in sorted order according to entropy.\n\n    Parameters:\n        preprocess: text preprocessing component\n        save_path: path to save a model\n        load_path: path to load a model\n        entropy_fields: the specification attributes of the catalog items\n        min_similarity: similarity threshold for ranking\n        min_entropy: min entropy threshold for specifying\n    """"""\n\n    def __init__(self,\n                 preprocess: Component,\n                 save_path: str,\n                 load_path: str,\n                 entropy_fields: list,\n                 min_similarity: float = 0.5,\n                 min_entropy: float = 0.5,\n                 **kwargs) -> None:\n\n        self.preprocess = preprocess\n        self.save_path = expand_path(save_path)\n\n        if isinstance(load_path, list):\n            self.load_path: List = [expand_path(path) for path in load_path]\n        else:\n            self.load_path: List = [expand_path(load_path)]\n\n        self.min_similarity = min_similarity\n        self.min_entropy = min_entropy\n        self.entropy_fields = entropy_fields\n        self.ec_data: List = []\n        if kwargs.get(\'mode\') != \'train\':\n            self.load()\n\n    def fit(self, data: List[Dict[Any, Any]]) -> None:\n        """"""Preprocess items `title` and `description` from the `data`\n\n        Parameters:\n            data: list of catalog items\n\n        Returns:\n            None\n        """"""\n\n        log.info(f""Items to nlp: {len(data)}"")\n        self.ec_data = [dict(item, **{\n            \'title_nlped\': self.preprocess.spacy2dict(self.preprocess.analyze(item[\'Title\'])),\n            \'feat_nlped\': self.preprocess.spacy2dict(self.preprocess.analyze(item[\'Title\'] + \'. \' + item[\'Feature\']))\n        }) for item in data]\n        log.info(\'Data are nlped\')\n\n    def save(self, **kwargs) -> None:\n        """"""Save classifier parameters""""""\n        log.info(f""Saving model to {self.save_path}"")\n        save_pickle(self.ec_data, self.save_path)\n\n    def load(self, **kwargs) -> None:\n        """"""Load classifier parameters""""""\n        log.info(f""Loading model from {self.load_path}"")\n        for path in self.load_path:\n            if Path.is_file(path):\n                self.ec_data += load_pickle(path)\n            else:\n                raise FileNotFoundError\n\n        log.info(f""Loaded items {len(self.ec_data)}"")\n\n    def __call__(self, queries: List[str], history: List[Any], states: List[Dict[Any, Any]]) -> \\\n            Tuple[Tuple[List[Any], List[Any]], List[float], List[Any]]:\n        """"""Retrieve catalog items according to the BLEU measure\n\n        Parameters:\n            queries: list of queries\n            history: list of previous queries\n            states: list of dialog state\n\n        Returns:\n            response:   items:      list of retrieved items\n                        entropies:  list of entropy attributes with corresponding values\n\n            confidence: list of similarity scores\n            state: dialog state\n        """"""\n\n        response: List = []\n        confidence: List = []\n        results_args: List = []\n        entropies: List = []\n        back_states: List = []\n        results_args_sim: List = []\n\n        log.debug(f""queries: {queries} states: {states}"")\n\n        for item_idx, query in enumerate(queries):\n\n            state = states[item_idx]\n\n            if isinstance(state, str):\n                try:\n                    state = json.loads(state)\n                except:\n                    state = self.preprocess.parse_input(state)\n\n            if not state:\n                state = {}\n\n            start = state[\'start\'] if \'start\' in state else 0\n            stop = state[\'stop\'] if \'stop\' in state else 5\n\n            state[\'start\'] = start\n            state[\'stop\'] = stop\n\n            query = self.preprocess.analyze(query)\n\n            query, money_range = self.preprocess.extract_money(query)\n            log.debug(f""money detected: {query} {money_range}"")\n\n            if len(money_range) == 2:\n                state[\'Price\'] = money_range\n\n            score_title = [bleu_advanced(self.preprocess.lemmas(item[\'title_nlped\']),\n                                         self.preprocess.lemmas(self.preprocess.filter_nlp_title(query)),\n                                         weights=(1,), penalty=False) for item in self.ec_data]\n\n            score_feat = [bleu_advanced(self.preprocess.lemmas(item[\'feat_nlped\']),\n                                        self.preprocess.lemmas(self.preprocess.filter_nlp(query)),\n                                        weights=(0.3, 0.7), penalty=False) for idx, item in enumerate(self.ec_data)]\n\n            scores = np.mean([score_feat, score_title], axis=0).tolist()\n\n            scores_title = [(score, -len(self.ec_data[idx][\'Title\'])) for idx, score in enumerate(scores)]\n\n            raw_scores_ar = np.array(scores_title, dtype=[(\'x\', \'float_\'), (\'y\', \'int_\')])\n\n            results_args = np.argsort(raw_scores_ar, order=(\'x\', \'y\'))[::-1].tolist()\n\n            results_args_sim = [idx for idx in results_args if scores[idx] >= self.min_similarity]\n\n            log.debug(\n                f""Items before similarity filtering {len(results_args)} and after {len(results_args_sim)} with th={self.min_similarity} "" +\n                f""the best one has score {scores[results_args[0]]} with title {self.ec_data[results_args[0]][\'Title\']}"")\n\n            results_args_sim = self._filter_state(state, results_args_sim)\n\n            results_args_sim_fil = [idx for idx in results_args_sim[start:stop]]\n\n            local_response = self._clean_items(results_args_sim_fil)\n\n            response.append(local_response)\n\n            confidence.append([(score_title[idx], score_feat[idx])\n                               for idx in results_args_sim[start:stop]])\n\n            entropies.append(self._entropy_subquery(results_args_sim))\n            log.debug(f""Total number of relevant answers {len(results_args_sim)}"")\n            back_states.append(state)\n\n        return (response, entropies), confidence, back_states\n\n    def _clean_items(self, results: List[int]) -> List[Any]:\n        local_response: List = []\n        for idx in results:\n            temp = copy.copy(self.ec_data[idx])\n            del temp[\'title_nlped\']\n            del temp[\'feat_nlped\']\n            local_response.append(temp)\n        return local_response\n\n    def _filter_state(self, state: Dict[Any, Any], results_args_sim: List[int]) -> List[Any]:\n        for key, value in state.items():\n            log.debug(f""Filtering for {key}:{value}"")\n\n            if key == \'Price\':\n                price = value\n                log.debug(f""Items before price filtering {len(results_args_sim)} with price {price}"")\n                results_args_sim = [idx for idx in results_args_sim\n                                    if price[0] <= self.preprocess.price(self.ec_data[idx]) <= price[1] and\n                                    self.preprocess.price(self.ec_data[idx]) != 0]\n                log.debug(f""Items after price filtering {len(results_args_sim)}"")\n\n            elif key in [\'query\', \'start\', \'stop\', \'history\']:\n                continue\n\n            else:\n                results_args_sim = [idx for idx in results_args_sim\n                                    if key in self.ec_data[idx]\n                                    if self.ec_data[idx][key].lower() == value.lower()]\n\n        return results_args_sim\n\n    def _entropy_subquery(self, results_args: List[int]) -> List[Tuple[float, str, List[Tuple[str, int]]]]:\n        """"""Calculate entropy of selected attributes for items from the catalog.\n\n        Parameters:\n            results_args: items id to consider\n\n        Returns:\n            entropies: entropy score with attribute name and corresponding values\n        """"""\n\n        ent_fields: Dict = {}\n\n        for idx in results_args:\n            for field in self.entropy_fields:\n                if field in self.ec_data[idx]:\n                    if field not in ent_fields:\n                        ent_fields[field] = []\n\n                    ent_fields[field].append(self.ec_data[idx][field].lower())\n\n        entropies = []\n        for key, value in ent_fields.items():\n            count = Counter(value)\n            entropies.append(\n                (entropy(list(count.values()), base=2), key, count.most_common()))\n\n        entropies = sorted(entropies, key=itemgetter(0), reverse=True)\n        entropies = [\n            ent_item for ent_item in entropies if ent_item[0] >= self.min_entropy]\n\n        return entropies\n'"
deeppavlov/deprecated/skills/ecommerce_skill/tfidf_retrieve.py,0,"b'# Copyright 2018 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom collections import Counter\nfrom logging import getLogger\nfrom operator import itemgetter\nfrom typing import List, Tuple, Dict, Union, Any\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix, vstack\nfrom scipy.sparse.linalg import norm as sparse_norm\nfrom scipy.stats import entropy\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.file import save_pickle, load_pickle\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.estimator import Component\n\nlog = getLogger(__name__)\n\n\n@register(""ecommerce_skill_tfidf"")\nclass EcommerceSkillTfidf(Component):\n    """"""Class to retrieve product items from `load_path` catalogs\n    in sorted order according to the similarity measure\n    Retrieve the specification attributes with corresponding values\n    in sorted order according to entropy.\n\n    Parameters:\n        save_path: path to save a model\n        load_path: path to load a model\n        entropy_fields: the specification attributes of the catalog items\n        min_similarity: similarity threshold for ranking\n        min_entropy: min entropy threshold for specifying\n    """"""\n\n    def __init__(self,\n                 save_path: str,\n                 load_path: str,\n                 entropy_fields: list,\n                 min_similarity: float = 0.5,\n                 min_entropy: float = 0.5,\n                 **kwargs) -> None:\n\n        self.save_path = expand_path(save_path)\n        self.load_path = expand_path(load_path)\n        self.min_similarity = min_similarity\n        self.min_entropy = min_entropy\n        self.entropy_fields = entropy_fields\n        self.ec_data: List = []\n        self.x_train_features = None\n        if kwargs.get(\'mode\') != \'train\':\n            self.load()\n\n    def fit(self, data, query) -> None:\n        """"""Preprocess items `title` and `description` from the `data`\n\n        Parameters:\n            data: list of catalog items\n\n        Returns:\n            None\n        """"""\n\n        self.x_train_features = vstack(list(query))\n        self.ec_data = data\n\n    def save(self) -> None:\n        """"""Save classifier parameters""""""\n        log.info(""Saving to {}"".format(self.save_path))\n        path = expand_path(self.save_path)\n        save_pickle((self.ec_data, self.x_train_features), path)\n\n    def load(self) -> None:\n        """"""Load classifier parameters""""""\n        log.info(""Loading from {}"".format(self.load_path))\n        self.ec_data, self.x_train_features = load_pickle(\n            expand_path(self.load_path))\n\n    def __call__(self, q_vects: List[csr_matrix], histories: List[Any], states: List[Dict[Any, Any]]) -> Tuple[\n        Tuple[List[Dict[Any, Any]], List[Any]], List[float], Dict[Any, Any]]:\n        """"""Retrieve catalog items according to the TFIDF measure\n\n        Parameters:\n            queries: list of queries\n            history: list of previous queries\n            states: list of dialog state\n\n        Returns:\n            response:   items:      list of retrieved items\n                        entropies:  list of entropy attributes with corresponding values\n\n            confidence: list of similarity scores\n            state: dialog state\n        """"""\n\n        log.info(f""Total catalog {len(self.ec_data)}"")\n\n        if not isinstance(q_vects, list):\n            q_vects = [q_vects]\n\n        if not isinstance(states, list):\n            states = [states]\n\n        if not isinstance(histories, list):\n            histories = [histories]\n\n        items: List = []\n        confidences: List = []\n        back_states: List = []\n        entropies: List = []\n\n        for idx, q_vect in enumerate(q_vects):\n\n            log.info(f""Search query {q_vect}"")\n\n            if len(states) >= idx + 1:\n                state = states[idx]\n            else:\n                state = {\'start\': 0, \'stop\': 5}\n\n            if not isinstance(state, dict):\n                state = {\'start\': 0, \'stop\': 5}\n\n            if \'start\' not in state:\n                state[\'start\'] = 0\n            if \'stop\' not in state:\n                state[\'stop\'] = 5\n\n            if \'history\' not in state:\n                state[\'history\'] = []\n\n            log.info(f""Current state {state}"")\n\n            if state[\'history\']:\n                his_vect = self._list_to_csr(state[\'history\'][-1])\n                if not np.array_equal(his_vect.todense(), q_vect.todense()):\n                    q_comp = q_vect.maximum(his_vect)\n                    complex_bool = self._take_complex_query(q_comp, q_vect)\n                    log.info(f""Complex query:{complex_bool}"")\n\n                    if complex_bool is True:\n                        q_vect = q_comp\n                        state[\'start\'] = 0\n                        state[\'stop\'] = 5\n                    else:\n                        # current short query wins that means that the state should be zeroed\n                        state[\'history\'] = []\n                else:\n                    log.info(""the save query came"")\n            else:\n                log.info(""history is empty"")\n\n            state[\'history\'].append(self._csr_to_list(q_vect))\n            log.info(f""Final query {q_vect}"")\n\n            scores = self._similarity(q_vect)\n            answer_ids = np.argsort(scores)[::-1]\n            answer_ids = [idx for idx in answer_ids if scores[idx] >= self.min_similarity]\n\n            answer_ids = self._state_based_filter(answer_ids, state)\n\n            items.append([self.ec_data[idx]\n                          for idx in answer_ids[state[\'start\']:state[\'stop\']]])\n            confidences.append(\n                [scores[idx] for idx in answer_ids[state[\'start\']:state[\'stop\']]])\n            back_states.append(state)\n\n            entropies.append(self._entropy_subquery(answer_ids))\n        return (items, entropies), confidences, back_states\n\n    def _csr_to_list(self, csr: csr_matrix) -> List[Any]:\n        return [csr.data.tolist(), csr.indices.tolist()]\n\n    def _list_to_csr(self, _list: List) -> csr_matrix:\n        row_ind = [0] * len(_list[0])\n        col_ind = _list[1]\n        return csr_matrix((_list[0], (row_ind, col_ind)))\n\n    def _take_complex_query(self, q_prev: csr_matrix, q_cur: csr_matrix) -> bool:\n        """"""Decides whether to use the long compound query or the current short query\n\n        Parameters:\n            q_prev: previous query\n            q_cur: current query\n\n        Returns:\n            Bool: whether to use the compound query\n        """"""\n\n        prev_sim = self._similarity(q_prev)\n        cur_sim = self._similarity(q_cur)\n\n        log.debug(f""prev_sim.max(): {prev_sim.max()}"")\n        log.debug(f""cur_sim.max(): {cur_sim.max()}"")\n\n        if prev_sim.max() > cur_sim.max():\n            return True\n\n        return False\n\n    def _similarity(self, q_vect: Union[csr_matrix, List]) -> List[float]:\n        """"""Calculates cosine similarity between the user\'s query and product items.\n\n        Parameters:\n            q_cur: user\'s query\n\n        Returns:\n            cos_similarities: lits of similarity scores\n        """"""\n\n        norm = sparse_norm(q_vect) * sparse_norm(self.x_train_features, axis=1)\n        cos_similarities = np.array(q_vect.dot(self.x_train_features.T).todense()) / norm\n\n        cos_similarities = cos_similarities[0]\n        cos_similarities = np.nan_to_num(cos_similarities)\n        return cos_similarities\n\n    def _state_based_filter(self, ids: List[int], state: Dict[Any, Any]):\n        """"""Filters the candidates based on the key-values from the state\n\n        Parameters:\n            ids: list of candidates\n            state: dialog state\n\n        Returns:\n            ids: filtered list of candidates\n        """"""\n\n        for key, value in state.items():\n            log.debug(f""Filtering for {key}:{value}"")\n\n            if key in [\'query\', \'start\', \'stop\', \'history\']:\n                continue\n\n            else:\n                ids = [idx for idx in ids\n                       if key in self.ec_data[idx]\n                       if self.ec_data[idx][key].lower() == value.lower()]\n        return ids\n\n    def _entropy_subquery(self, results_args: List[int]) -> List[Tuple[float, str, List[Tuple[str, int]]]]:\n        """"""Calculate entropy of selected attributes for items from the catalog.\n\n        Parameters:\n            results_args: items id to consider\n\n        Returns:\n            entropies: entropy score with attribute name and corresponding values\n        """"""\n\n        ent_fields: Dict = {}\n\n        for idx in results_args:\n            for field in self.entropy_fields:\n                if field in self.ec_data[idx]:\n                    if field not in ent_fields:\n                        ent_fields[field] = []\n\n                    ent_fields[field].append(self.ec_data[idx][field].lower())\n\n        entropies = []\n        for key, value in ent_fields.items():\n            count = Counter(value)\n            entropies.append((entropy(list(count.values()), base=2), key, count.most_common()))\n\n        entropies = sorted(entropies, key=itemgetter(0), reverse=True)\n        entropies = [ent_item for ent_item in entropies if ent_item[0] >= self.min_entropy]\n\n        return entropies\n'"
deeppavlov/deprecated/skills/pattern_matching_skill/__init__.py,0,b'from .pattern_matching_skill import PatternMatchingSkill\n'
deeppavlov/deprecated/skills/pattern_matching_skill/pattern_matching_skill.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport random\nimport re\nfrom typing import List, Tuple, Optional\n\nfrom deeppavlov.deprecated.skill import Skill\n\n\nclass PatternMatchingSkill(Skill):\n    """"""Skill, matches utterances to patterns, returns predefined answers.\n\n    Allows to create skills as pre-defined responses for a user\'s input\n    containing specific keywords or regular expressions. Every skill returns\n    response and confidence.\n\n    Args:\n        responses: List of str responses from which response will be randomly\n            selected.\n        patterns: List of str patterns for utterance matching. Patterns may\n            be all plain texts or all regexps.\n        regex: Turns on regular expressions matching mode.\n        ignore_case: Turns on utterances case ignoring.\n        default_confidence: The default confidence.\n\n    Attributes:\n        responses: List of str responses from which response will be randomly\n            selected.\n        patterns: List of str patterns for utterance matching. Patterns may\n            be all plain texts or all regexps.\n        regex: Turns on regular expressions matching mode.\n        ignore_case: Turns on utterances case ignoring.\n        default_confidence: The default confidence.\n    """"""\n\n    def __init__(self, responses: List[str], patterns: Optional[List[str]] = None,\n                 regex: bool = False, ignore_case: bool = True, default_confidence: float = 1) -> None:\n        if isinstance(responses, str):\n            responses = [responses]\n        self.responses = responses\n        if isinstance(patterns, str):\n            patterns = [patterns]\n        self.regex = regex\n        self.ignore_case = ignore_case\n        self.default_confidence = default_confidence\n        if regex:\n            if patterns:\n                flags = re.IGNORECASE if ignore_case else 0\n                patterns = [re.compile(pattern, flags) for pattern in patterns]\n        else:\n            if patterns and ignore_case:\n                patterns = [pattern.lower() for pattern in patterns]\n        self.patterns = patterns\n\n    def __call__(self, utterances_batch: list, history_batch: list,\n                 states_batch: Optional[list] = None) -> Tuple[list, list]:\n        """"""Returns skill inference result.\n\n        Returns batches of skill inference results, estimated confidence\n        levels and up to date states corresponding to incoming utterance\n        batch.\n\n        Args:\n            utterances_batch: A batch of utterances of any type.\n            history_batch: A batch of list typed histories for each utterance.\n            states_batch: Optional. A batch of arbitrary typed states for\n                each utterance.\n\n        Returns:\n            response: A batch of arbitrary typed skill inference results.\n            confidence: A batch of float typed confidence levels for each of\n                skill inference result.\n        """"""\n        response = [random.choice(self.responses) for _ in utterances_batch]\n        if self.patterns is None:\n            confidence = [self.default_confidence] * len(utterances_batch)\n        else:\n            if self.ignore_case:\n                utterances_batch = [utterance.lower() for utterance in utterances_batch]\n            if self.regex:\n                confidence = [\n                    self.default_confidence * float(any([pattern.search(utterance) for pattern in self.patterns]))\n                    for utterance in utterances_batch]\n            else:\n                confidence = [self.default_confidence * float(any([pattern in utterance for pattern in self.patterns]))\n                              for utterance in utterances_batch]\n\n        return response, confidence\n'"
deeppavlov/deprecated/skills/similarity_matching_skill/__init__.py,0,b'from .similarity_matching_skill import SimilarityMatchingSkill\n'
deeppavlov/deprecated/skills/similarity_matching_skill/similarity_matching_skill.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import Tuple, Optional, List\n\nfrom deeppavlov import build_model, train_model\nfrom deeppavlov.configs import configs\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.file import read_json\nfrom deeppavlov.core.data.utils import update_dict_recursive\nfrom deeppavlov.deprecated.skill import Skill\n\nlog = getLogger(__name__)\n\n\nclass SimilarityMatchingSkill(Skill):\n    """"""The skill matches utterances to predefined phrases and returns corresponding answers.\n\n    The skill is based on the FAQ-alike .csv table that contains questions and corresponding responses.\n    The skill returns responses and confidences.\n\n    Args:\n        data_path: URL or local path to \'.csv\' file that contains two columns with Utterances and Responses.\n            User\'s utterance will be compared to the Utterances column and response will be selected\n            from the Responses column.\n        config_type: The selected configuration file (\'tfidf_autofaq\' by default).\n        x_col_name: The question column name in the \'.csv\' file (\'Question\' by default).\n        y_col_name: The response column name in the \'.csv\' file (\'Answer\' by default).\n        save_load_path: Path, where the model will be saved or loaded from (\'./similarity_matching\' by default).\n        edit_dict: Dictionary of edits to the selected configuration (overwrites other parameters).\n        train: Should model be trained or not (True by default).\n\n    Attributes:\n        model: Classifies user\'s utterance\n    """"""\n\n    def __init__(self, data_path: Optional[str] = None, config_type: Optional[str] = \'tfidf_autofaq\',\n                 x_col_name: Optional[str] = \'Question\', y_col_name: Optional[str] = \'Answer\',\n                 save_load_path: Optional[str] = \'./similarity_matching\',\n                 edit_dict: Optional[dict] = None, train: Optional[bool] = True):\n\n        if config_type not in configs.faq:\n            raise ValueError(""There is no config named \'{0}\'. Possible options are: {1}""\n                             .format(config_type, "", "".join(configs.faq.keys())))\n        model_config = read_json(configs.faq[config_type])\n\n        if x_col_name is not None:\n            model_config[\'dataset_reader\'][\'x_col_name\'] = x_col_name\n        if y_col_name is not None:\n            model_config[\'dataset_reader\'][\'y_col_name\'] = y_col_name\n\n        model_config[\'metadata\'][\'variables\'][\'MODELS_PATH\'] = save_load_path\n\n        if data_path is not None:\n            if expand_path(data_path).exists():\n                if \'data_url\' in model_config[\'dataset_reader\']:\n                    del model_config[\'dataset_reader\'][\'data_url\']\n                model_config[\'dataset_reader\'][\'data_path\'] = data_path\n            else:\n                if \'data_path\' in model_config[\'dataset_reader\']:\n                    del model_config[\'dataset_reader\'][\'data_path\']\n                model_config[\'dataset_reader\'][\'data_url\'] = data_path\n\n        if edit_dict is not None:\n            update_dict_recursive(model_config, edit_dict)\n\n        if train:\n            self.model = train_model(model_config, download=True)\n            log.info(\'Your model was saved at: \\\'\' + save_load_path + \'\\\'\')\n        else:\n            self.model = build_model(model_config, download=False)\n\n    def __call__(self, utterances_batch: List[str], history_batch: List[List[str]],\n                 states_batch: Optional[list] = None) -> Tuple[List[str], List[float]]:\n        """"""It returns the skill inference result.\n\n        Output is batches of the skill inference results and estimated confidences.\n\n        Args:\n            utterances_batch: A batch of utterances.\n            history_batch: A batch of list typed histories for each utterance.\n            states_batch: Optional. A batch of arbitrary typed states for\n                each utterance.\n\n        Returns:\n            Batches of the skill inference results and estimated confidences.\n        """"""\n        responses, confidences = self.model(utterances_batch)\n\n        # in case if model returns not the highest probability, but the whole distribution\n        if isinstance(confidences[0], list):\n            confidences = [max(c) for c in confidences]\n\n        return responses, confidences\n'"
deeppavlov/models/go_bot/dto/__init__.py,0,b''
deeppavlov/models/go_bot/dto/dataset_features.py,0,"b'from typing import List\n\nimport numpy as np\n\n\n# todo remove boilerplate duplications\n# todo comments\n# todo logging\n# todo naming\nfrom deeppavlov.models.go_bot.nlu.dto.nlu_response import NLUResponse\nfrom deeppavlov.models.go_bot.policy.dto.digitized_policy_features import DigitizedPolicyFeatures\nfrom deeppavlov.models.go_bot.tracker.dto.dst_knowledge import DSTKnowledge\n\nfrom copy import deepcopy\n\n\nclass UtteranceFeatures:\n    """"""\n    the DTO-like class storing the training features of a single utterance of a dialog\n    (to feed the GO-bot policy model)\n    """"""\n\n    action_mask: np.ndarray\n    attn_key: np.ndarray\n    tokens_embeddings_padded: np.ndarray\n    features: np.ndarray\n\n    def __init__(self,\n                 nlu_response: NLUResponse,\n                 tracker_knowledge: DSTKnowledge,\n                 features: DigitizedPolicyFeatures):\n        self.action_mask = features.action_mask\n        self.attn_key = features.attn_key\n\n        tokens_vectorized = nlu_response.tokens_vectorized  # todo proper oop\n        self.tokens_embeddings_padded = tokens_vectorized.tokens_embeddings_padded\n        self.features = features.concat_feats\n\n\nclass UtteranceTarget:\n    """"""\n    the DTO-like class storing the training target of a single utterance of a dialog\n    (to feed the GO-bot policy model)\n    """"""\n    action_id: int\n\n    def __init__(self, action_id):\n        self.action_id = action_id\n\n\nclass UtteranceDataEntry:\n    """"""\n    the DTO-like class storing both the training features and target\n    of a single utterance of a dialog (to feed the GO-bot policy model)\n    """"""\n    features: UtteranceFeatures\n    target: UtteranceTarget\n\n    def __init__(self, features, target):\n        self.features = features\n        self.target = target\n\n    @staticmethod\n    def from_features_and_target(features: UtteranceFeatures, target: UtteranceTarget):\n        return UtteranceDataEntry(deepcopy(features), deepcopy(target))\n\n    @staticmethod\n    def from_features(features: UtteranceFeatures):\n        return UtteranceDataEntry(deepcopy(features), UtteranceTarget(None))\n\n\nclass DialogueFeatures:\n    """"""\n    the DTO-like class storing both the training features\n    of a dialog (to feed the GO-bot policy model)\n    """"""\n    action_masks: List[np.ndarray]\n    attn_keys: List[np.ndarray]\n    tokens_embeddings_paddeds: List[np.ndarray]\n    featuress: List[np.ndarray]\n\n    def __init__(self):\n        self.action_masks = []\n        self.attn_keys = []\n        self.tokens_embeddings_paddeds = []\n        self.featuress = []\n\n    def append(self, utterance_features: UtteranceFeatures):\n        self.action_masks.append(utterance_features.action_mask)\n        self.attn_keys.append(utterance_features.attn_key)\n        self.tokens_embeddings_paddeds.append(utterance_features.tokens_embeddings_padded)\n        self.featuress.append(utterance_features.features)\n\n    def __len__(self):\n        return len(self.featuress)\n\n\nclass DialogueTargets:\n    """"""\n    the DTO-like class storing both the training targets\n    of a dialog (to feed the GO-bot policy model)\n    """"""\n    action_ids: List[int]\n\n    def __init__(self):\n        self.action_ids = []\n\n    def append(self, utterance_target: UtteranceTarget):\n        self.action_ids.append(utterance_target.action_id)\n\n    def __len__(self):\n        return len(self.action_ids)\n\n\nclass DialogueDataEntry:\n    """"""\n    the DTO-like class storing both the training features and targets\n    of a dialog (to feed the GO-bot policy model)\n    """"""\n    features: DialogueFeatures\n    targets: DialogueTargets\n\n    def __init__(self):\n        self.features = DialogueFeatures()\n        self.targets = DialogueTargets()\n\n    def append(self, utterance_features: UtteranceDataEntry):\n        self.features.append(utterance_features.features)\n        self.targets.append(utterance_features.target)\n\n    def __len__(self):\n        return len(self.features)\n\n\nclass PaddedDialogueFeatures(DialogueFeatures):\n    """"""\n    the DTO-like class storing both the **padded to some specified length** training features\n    of a dialog (to feed the GO-bot policy model)\n    """"""\n    padded_dialogue_length_mask: List[int]\n\n    def __init__(self, dialogue_features: DialogueFeatures, sequence_length):\n        super().__init__()\n\n        padding_length = sequence_length - len(dialogue_features)\n\n        self.padded_dialogue_length_mask = [1] * len(dialogue_features) + [0] * padding_length\n\n        self.action_masks = dialogue_features.action_masks + \\\n                            [np.zeros_like(dialogue_features.action_masks[0])] * padding_length\n\n        self.attn_keys = dialogue_features.attn_keys + [np.zeros_like(dialogue_features.attn_keys[0])] * padding_length\n\n        self.tokens_embeddings_paddeds = dialogue_features.tokens_embeddings_paddeds + \\\n                                         [np.zeros_like(\n                                             dialogue_features.tokens_embeddings_paddeds[0])] * padding_length\n\n        self.featuress = dialogue_features.featuress + [np.zeros_like(dialogue_features.featuress[0])] * padding_length\n\n\nclass PaddedDialogueTargets(DialogueTargets):\n    """"""\n    the DTO-like class storing both the **padded to some specified length** training targets\n    of a dialog (to feed the GO-bot policy model)\n    """"""\n    def __init__(self, dialogue_targets: DialogueTargets, sequence_length):\n        super().__init__()\n\n        padding_length = sequence_length - len(dialogue_targets)\n        self.action_ids = dialogue_targets.action_ids + [0] * padding_length\n\n\nclass PaddedDialogueDataEntry(DialogueDataEntry):\n    """"""\n    the DTO-like class storing both the **padded to some specified length** training features and targets\n    of a dialog (to feed the GO-bot policy model)\n    """"""\n    features: PaddedDialogueFeatures\n    targets: PaddedDialogueTargets\n\n    def __init__(self, dialogue_data_entry: DialogueDataEntry, sequence_length):\n        super().__init__()\n\n        self.features = PaddedDialogueFeatures(dialogue_data_entry.features, sequence_length)\n        self.targets = PaddedDialogueTargets(dialogue_data_entry.targets, sequence_length)\n\n\nclass BatchDialoguesFeatures:\n    """"""\n    the DTO-like class storing both the training features\n    of a batch of dialogues. (to feed the GO-bot policy model)\n    """"""\n    b_action_masks: List[List[np.ndarray]]\n    b_attn_keys: List[List[np.ndarray]]\n    b_tokens_embeddings_paddeds: List[List[np.ndarray]]\n    b_featuress: List[List[np.ndarray]]\n    b_padded_dialogue_length_mask: List[List[int]]\n    max_dialogue_length: int\n\n    def __init__(self, max_dialogue_length):\n        self.b_action_masks = []\n        self.b_attn_keys = []\n        self.b_tokens_embeddings_paddeds = []\n        self.b_featuress = []\n        self.b_padded_dialogue_length_mask = []\n        self.max_dialogue_length = max_dialogue_length\n\n    def append(self, padded_dialogue_features: PaddedDialogueFeatures):\n        self.b_action_masks.append(padded_dialogue_features.action_masks)\n        self.b_attn_keys.append(padded_dialogue_features.attn_keys)\n        self.b_tokens_embeddings_paddeds.append(padded_dialogue_features.tokens_embeddings_paddeds)\n        self.b_featuress.append(padded_dialogue_features.featuress)\n        self.b_padded_dialogue_length_mask.append(padded_dialogue_features.padded_dialogue_length_mask)\n\n    def __len__(self):\n        return len(self.b_featuress)\n\n\nclass BatchDialoguesTargets:\n    """"""\n    the DTO-like class storing both the training targets\n    of a batch of dialogues. (to feed the GO-bot policy model)\n    """"""\n    b_action_ids: List[List[int]]\n    max_dialogue_length: int\n\n    def __init__(self, max_dialogue_length):\n        self.b_action_ids = []\n        self.max_dialogue_length = max_dialogue_length\n\n    def append(self, padded_dialogue_targets: PaddedDialogueTargets):\n        self.b_action_ids.append(padded_dialogue_targets.action_ids)\n\n    def __len__(self):\n        return len(self.b_action_ids)\n\n\nclass BatchDialoguesDataset:\n    """"""\n    the DTO-like class storing both the training features and target\n    of a batch of dialogues. (to feed the GO-bot policy model)\n    Handles the dialogues padding.\n    """"""\n    features: BatchDialoguesFeatures\n    targets: BatchDialoguesTargets\n\n    def __init__(self, max_dialogue_length):\n        self.features = BatchDialoguesFeatures(max_dialogue_length)\n        self.targets = BatchDialoguesTargets(max_dialogue_length)\n        self.max_dialogue_length = max_dialogue_length\n\n    def append(self, dialogue_features: DialogueDataEntry):\n        padded_dialogue_features = PaddedDialogueDataEntry(dialogue_features, self.max_dialogue_length)\n        self.features.append(padded_dialogue_features.features)\n        self.targets.append(padded_dialogue_features.targets)\n\n    def __len__(self):\n        return len(self.features)\n'"
deeppavlov/models/go_bot/dto/shared_gobot_params.py,0,"b'from deeppavlov.models.go_bot.nlu.nlu_manager import NLUManagerInterface\nfrom deeppavlov.models.go_bot.nlg.nlg_manager import NLGManagerInterface\nfrom deeppavlov.models.go_bot.tracker.featurized_tracker import FeaturizedTracker\n\n\n# todo logging\nclass SharedGoBotParams:\n    """"""the DTO-like class to share the params used in various parts of the GO-bot pipeline.""""""\n    # possibly useful: seems like the params reflect only ""real-world"" knowledge.\n    num_actions: int\n    num_intents: int\n    num_tracker_features: int\n\n    def __init__(self, num_actions: int, num_intents: int, num_tracker_features: int):\n        self.num_actions = num_actions\n        self.num_intents = num_intents\n        self.num_tracker_features = num_tracker_features\n\n    @staticmethod\n    def from_configured(nlg_manager: NLGManagerInterface, nlu_manager: NLUManagerInterface, tracker: FeaturizedTracker):\n        """"""builds the params object given some GO-bot units that are already configured""""""\n        return SharedGoBotParams(nlg_manager.num_of_known_actions(),\n                                 nlu_manager.num_of_known_intents(),\n                                 tracker.num_features)\n'"
deeppavlov/models/go_bot/nlg/__init__.py,0,b''
deeppavlov/models/go_bot/nlg/mock_json_nlg_manager.py,0,"b'import json\nfrom itertools import combinations\nfrom pathlib import Path\nfrom typing import Union, Dict\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.dataset_readers.dstc2_reader import DSTC2DatasetReader\nfrom deeppavlov.models.go_bot.dto.dataset_features import BatchDialoguesFeatures\nfrom deeppavlov.models.go_bot.nlg.dto.json_nlg_response import JSONNLGResponse\nfrom deeppavlov.models.go_bot.nlg.nlg_manager import log\nfrom deeppavlov.models.go_bot.nlg.nlg_manager_interface import NLGManagerInterface\nfrom deeppavlov.models.go_bot.policy.dto.policy_prediction import PolicyPrediction\n\n\n@register(""gobot_json_nlg_manager"")\nclass MockJSONNLGManager(NLGManagerInterface):\n\n    # todo inheritance\n    # todo force a2id, id2a mapping to be persistent for same configs\n\n    def __init__(self,\n                 actions2slots_path: Union[str, Path],\n                 api_call_action: str,\n                 data_path: Union[str, Path],\n                 debug=False):\n        self.debug = debug\n\n        if self.debug:\n            log.debug(f""BEFORE {self.__class__.__name__} init(): ""\n                      f""actions2slots_path={actions2slots_path}, ""\n                      f""api_call_action={api_call_action}, debug={debug}"")\n\n        individual_actions2slots = self._load_actions2slots_mapping(actions2slots_path)\n        possible_actions_combinations_tuples = sorted(\n            set(actions_combination_tuple\n                for actions_combination_tuple\n                in self._extract_actions_combinations(data_path)),\n            key=lambda x: \'+\'.join(x))\n\n        self.action_tuples2ids = {action_tuple: action_tuple_idx\n                                  for action_tuple_idx, action_tuple\n                                  in enumerate(possible_actions_combinations_tuples)}  # todo: typehint tuples somehow\n        self.ids2action_tuples = {v: k for k, v in self.action_tuples2ids.items()}\n\n        self.action_tuples_ids2slots = {}  # todo: typehint tuples somehow\n        for actions_combination_tuple in possible_actions_combinations_tuples:\n            actions_combination_slots = set(slot\n                                            for action in actions_combination_tuple\n                                            for slot in individual_actions2slots.get(action, []))\n            actions_combination_tuple_id = self.action_tuples2ids[actions_combination_tuple]\n            self.action_tuples_ids2slots[actions_combination_tuple_id] = actions_combination_slots\n\n        self._api_call_id = -1\n        if api_call_action is not None:\n            api_call_action_as_tuple = (api_call_action,)\n            self._api_call_id = self.action_tuples2ids[api_call_action_as_tuple]\n\n        if self.debug:\n            log.debug(f""AFTER {self.__class__.__name__} init(): ""\n                      f""actions2slots_path={actions2slots_path}, ""\n                      f""api_call_action={api_call_action}, debug={debug}"")\n\n    def get_api_call_action_id(self) -> int:\n        """"""\n        Returns:\n            an ID corresponding to the api call action\n        """"""\n        return self._api_call_id\n\n    def _extract_actions_combinations(self, dataset_path: Union[str, Path]):\n        dataset_path = expand_path(dataset_path)\n        dataset = DSTC2DatasetReader.read(data_path=dataset_path, dialogs=True)\n        actions_combinations = set()\n        for dataset_split in dataset.values():\n            for dialogue in dataset_split:\n                for user_input, system_response in dialogue:\n                    actions_tuple = tuple(system_response[""act""].split(\'+\'))\n                    actions_combinations.add(actions_tuple)\n        return actions_combinations\n\n    @staticmethod\n    def _load_actions2slots_mapping(actions2slots_json_path) -> Dict[str, str]:\n        actions2slots_json_path = expand_path(actions2slots_json_path)\n        with open(actions2slots_json_path, encoding=""utf-8"") as actions2slots_json_f:\n            actions2slots = json.load(actions2slots_json_f)\n        return actions2slots\n\n    def get_action_id(self, action_text: str) -> int:\n        """"""\n        Looks up for an ID corresponding to the passed action text.\n\n        Args:\n            action_text: the text for which an ID needs to be returned.\n        Returns:\n            an ID corresponding to the passed action text\n        """"""\n\n        actions_tuple = tuple(action_text.split(\'+\'))\n        return self.action_tuples2ids[actions_tuple]  # todo unhandled exception when not found\n\n    def decode_response(self,\n                        utterance_batch_features: BatchDialoguesFeatures,\n                        policy_prediction: PolicyPrediction,\n                        tracker_slotfilled_state: dict) -> JSONNLGResponse:\n        """"""\n        Converts the go-bot inference objects to the single output object.\n\n        Args:\n            utterance_batch_features: utterance features extracted in go-bot that\n            policy_prediction: policy model prediction (predicted action)\n            tracker_slotfilled_state: tracker knowledge before the NLG is performed\n\n        Returns:\n            The NLG output unit that stores slot values and predicted actions info.\n        """"""\n        slots_to_log = self.action_tuples_ids2slots[policy_prediction.predicted_action_ix]\n\n        slots_values = {slot_name: tracker_slotfilled_state.get(slot_name, ""unk"") for slot_name in slots_to_log}\n        actions_tuple = self.ids2action_tuples[policy_prediction.predicted_action_ix]\n\n        return JSONNLGResponse(slots_values, actions_tuple)\n\n    def num_of_known_actions(self) -> int:\n        """"""\n        Returns:\n            the number of actions known to the NLG module\n        """"""\n        return len(self.action_tuples2ids.keys())\n'"
deeppavlov/models/go_bot/nlg/nlg_manager.py,0,"b'import re\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import Union\n\nfrom deeppavlov.core.commands.utils import expand_path\nimport deeppavlov.models.go_bot.nlg.templates.templates as go_bot_templates\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.models.go_bot.dto.dataset_features import BatchDialoguesFeatures\nfrom deeppavlov.models.go_bot.nlg.nlg_manager_interface import NLGManagerInterface\nfrom deeppavlov.models.go_bot.policy.dto.policy_prediction import PolicyPrediction\n\nlog = getLogger(__name__)\n\n\n# todo add the ability to configure nlg loglevel in config (now the setting is shared across all the GO-bot)\n# todo add each method input-output logging when proper loglevel level specified\n\n\n@register(""gobot_nlg_manager"")\nclass NLGManager(NLGManagerInterface):\n    """"""\n    NLGManager is a unit of the go-bot pipeline that handles the generation of text\n    when the pattern is chosen among the known patterns and the named-entities-values-like knowledge is provided.\n    (the whole go-bot pipeline is as follows: NLU, dialogue-state-tracking&policy-NN, NLG)\n\n    Parameters:\n        template_path: file with mapping between actions and text templates\n            for response generation.\n        template_type: type of used response templates in string format.\n        api_call_action: label of the action that corresponds to database api call\n            (it must be present in your ``template_path`` file), during interaction\n            it will be used to get ``\'db_result\'`` from ``database``.\n        debug: whether to display debug output.\n    """"""\n\n    def __init__(self, template_path: Union[str, Path], template_type: str, api_call_action: str, debug=False):\n        self.debug = debug\n        if self.debug:\n            log.debug(f""BEFORE {self.__class__.__name__} init(): ""\n                      f""template_path={template_path}, template_type={template_type}, ""\n                      f""api_call_action={api_call_action}, debug={debug}"")\n\n        template_path = expand_path(template_path)\n        template_type = getattr(go_bot_templates, template_type)\n        self.templates = go_bot_templates.Templates(template_type).load(template_path)\n\n        self._api_call_id = -1\n        if api_call_action is not None:\n            self._api_call_id = self.templates.actions.index(api_call_action)\n\n        if self.debug:\n            log.debug(f""AFTER {self.__class__.__name__} init(): ""\n                      f""template_path={template_path}, template_type={template_type}, ""\n                      f""api_call_action={api_call_action}, debug={debug}"")\n\n    def get_action_id(self, action_text: str) -> int:\n        """"""\n        Looks up for an ID relevant to the passed action text in the list of known actions and their ids.\n\n        Args:\n            action_text: the text for which an ID needs to be returned.\n        Returns:\n            an ID corresponding to the passed action text\n        """"""\n        return self.templates.actions.index(action_text)  # todo unhandled exception when not found\n\n    def get_api_call_action_id(self) -> int:\n        """"""\n        Returns:\n            an ID corresponding to the api call action\n        """"""\n        return self._api_call_id\n\n    def decode_response(self,\n                        utterance_batch_features: BatchDialoguesFeatures,\n                        policy_prediction: PolicyPrediction,\n                        tracker_slotfilled_state) -> str:\n        # todo: docstring\n\n        action_text = self._generate_slotfilled_text_for_action(policy_prediction.predicted_action_ix,\n                                                                tracker_slotfilled_state)\n        # in api calls replace unknown slots to ""dontcare""\n        if policy_prediction.predicted_action_ix == self._api_call_id:\n            action_text = re.sub(""#([A-Za-z]+)"", ""dontcare"", action_text).lower()\n        return action_text\n\n    def _generate_slotfilled_text_for_action(self, action_id: int, slots: dict) -> str:\n        """"""\n        Generate text for the predicted speech action using the pattern provided for the action.\n        The slotfilled state provides info to encapsulate to the pattern.\n\n        Args:\n            action_id: the id of action to generate text for.\n            slots: the slots and their known values. usually received from dialogue state tracker.\n\n        Returns:\n            the text generated for the passed action id and slot values.\n        """"""\n        text = self.templates.templates[action_id].generate_text(slots)\n        return text\n\n    def num_of_known_actions(self) -> int:\n        """"""\n        Returns:\n            the number of actions known to the NLG module\n        """"""\n        return len(self.templates)\n'"
deeppavlov/models/go_bot/nlg/nlg_manager_interface.py,0,"b'from abc import ABCMeta, abstractmethod\n\nfrom deeppavlov.models.go_bot.dto.dataset_features import BatchDialoguesFeatures\nfrom deeppavlov.models.go_bot.nlg.dto.nlg_response_interface import NLGResponseInterface\nfrom deeppavlov.models.go_bot.policy.dto.policy_prediction import PolicyPrediction\n\n\nclass NLGManagerInterface(metaclass=ABCMeta):\n\n    @abstractmethod\n    def get_action_id(self, action_text) -> int:\n        """"""\n        Looks up for an ID relevant to the passed action text in the list of known actions and their ids.\n\n        Args:\n            action_text: the text for which an ID needs to be returned.\n        Returns:\n            an ID corresponding to the passed action text\n        """"""\n        pass\n\n    @abstractmethod\n    def get_api_call_action_id(self) -> int:\n        """"""\n        Returns:\n            an ID corresponding to the api call action\n        """"""\n        pass\n\n    @abstractmethod\n    def decode_response(self,\n                        utterance_batch_features: BatchDialoguesFeatures,\n                        policy_prediction: PolicyPrediction,\n                        tracker_slotfilled_state) -> NLGResponseInterface:\n        # todo: docstring\n        pass\n\n    @abstractmethod\n    def num_of_known_actions(self) -> int:\n        """"""\n        Returns:\n            the number of actions known to the NLG module\n        """"""\n        pass\n'"
deeppavlov/models/go_bot/nlu/__init__.py,0,b''
deeppavlov/models/go_bot/nlu/nlu_manager.py,0,"b'from logging import getLogger\nfrom typing import List\n\nfrom deeppavlov import Chainer\nfrom deeppavlov.models.go_bot.nlu.dto.nlu_response import NLUResponse\nfrom deeppavlov.models.go_bot.nlu.nlu_manager_interface import NLUManagerInterface\n\nlog = getLogger(__name__)\n\n\n# todo add the ability to configure nlu loglevel in config (now the setting is shared across all the GO-bot)\n# todo add each method input-output logging when proper loglevel level specified\n\n\nclass NLUManager(NLUManagerInterface):\n    """"""\n    NLUManager is a unit of the go-bot pipeline that handles the understanding of text.\n    Given the text it provides tokenization, intents extraction and the slots extraction.\n    (the whole go-bot pipeline is as follows: NLU, dialogue-state-tracking&policy-NN, NLG)\n    """"""\n\n    def __init__(self, tokenizer, slot_filler, intent_classifier, debug=False):\n        self.debug = debug\n        if self.debug:\n            log.debug(f""BEFORE {self.__class__.__name__} init(): ""\n                      f""tokenizer={tokenizer}, slot_filler={slot_filler}, ""\n                      f""intent_classifier={intent_classifier}, debug={debug}"")\n        # todo type hints\n        self.tokenizer = tokenizer\n        self.slot_filler = slot_filler\n        self.intent_classifier = intent_classifier\n        self.intents = []\n        if isinstance(self.intent_classifier, Chainer):\n            self.intents = self.intent_classifier.get_main_component().classes\n\n        if self.debug:\n            log.debug(f""AFTER {self.__class__.__name__} init(): ""\n                      f""tokenizer={tokenizer}, slot_filler={slot_filler}, ""\n                      f""intent_classifier={intent_classifier}, debug={debug}"")\n\n    def nlu(self, text: str) -> NLUResponse:\n        """"""\n        Extracts slot values and intents from text.\n\n        Args:\n            text: text to extract knowledge from\n\n        Returns:\n            an object storing the extracted slos and intents info\n        """"""\n        # todo meaningful type hints\n        tokens = self._tokenize_single_text_entry(text)\n\n        slots = None\n        if callable(self.slot_filler):\n            slots = self._extract_slots_from_tokenized_text_entry(tokens)\n\n        intents = []\n        if callable(self.intent_classifier):\n            intents = self._extract_intents_from_tokenized_text_entry(tokens)\n\n        return NLUResponse(slots, intents, tokens)\n\n    def _extract_intents_from_tokenized_text_entry(self, tokens: List[str]):\n        # todo meaningful type hints, relies on unannotated intent classifier\n        intent_features = self.intent_classifier([\' \'.join(tokens)])[1][0]\n        return intent_features\n\n    def _extract_slots_from_tokenized_text_entry(self, tokens: List[str]):\n        # todo meaningful type hints, relies on unannotated slot filler\n        return self.slot_filler([tokens])[0]\n\n    def _tokenize_single_text_entry(self, text: str):\n        # todo meaningful type hints, relies on unannotated tokenizer\n        return self.tokenizer([text.lower().strip()])[0]\n\n    def num_of_known_intents(self) -> int:\n        """"""\n        Returns:\n            the number of intents known to the NLU module\n        """"""\n        return len(self.intents)\n'"
deeppavlov/models/go_bot/nlu/nlu_manager_interface.py,0,"b'from abc import ABCMeta, abstractmethod\n\nfrom deeppavlov.models.go_bot.nlu.dto.nlu_response_interface import NLUResponseInterface\n\n\nclass NLUManagerInterface(metaclass=ABCMeta):\n    @abstractmethod\n    def nlu(self, text) -> NLUResponseInterface:\n        pass\n\n    @abstractmethod\n    def num_of_known_intents(self) -> int:\n        """"""\n        Returns:\n            the number of intents known to the NLU module\n        """"""\n        pass\n'"
deeppavlov/models/go_bot/nlu/tokens_vectorizer.py,0,"b'from logging import getLogger\nfrom typing import List, Optional\n\nimport numpy as np\n\nlog = getLogger(__name__)\n\n\n# todo logging\nclass TokensVectorRepresentationParams:\n    """"""the DTO-like class to transfer TokenVectorizer\'s vectorizers dimensions""""""\n\n    def __init__(self, embedding_dim: Optional[int], bow_dim: Optional[int]):\n        self.embedding_dim = embedding_dim\n        self.bow_dim = bow_dim\n\n\nclass TokensVectorizer:\n    """"""\n    the TokensVectorizer class is used in the NLU part of deeppavlov go-bot pipeline.\n    (for more info on NLU logic see the NLUManager --- the go-bot NLU main class)\n\n    TokensVectorizer is manages the BOW tokens encoding and tokens embedding.\n    Both BOW encoder and embedder are optional and have to be pre-trained:\n    this class wraps their usage but not training.\n    """"""\n\n    def __init__(self, debug, word_vocab=None, bow_embedder=None, embedder=None):\n        # todo adequate type hints\n        self.debug = debug\n        self.word_vocab = word_vocab  # TODO: isn\'t it passed with bow embedder?\n        self.bow_embedder = bow_embedder\n        self.embedder = embedder\n\n    def _use_bow_encoder(self) -> bool:\n        """"""\n        Returns:\n            is BOW encoding enabled in the TokensVectorizer\n        """"""\n        return callable(self.bow_embedder)\n\n    def _embed_tokens(self, tokens: List[str], mean_embeddings: bool) -> Optional[np.ndarray]:\n        """"""\n        Args:\n            tokens: list of tokens to embed\n            mean_embeddings: if True, will return the mean vector of calculated embeddings sequence.\n                             otherwise will return the calculated embeddings sequence.\n\n        Returns:\n            the (maybe averaged vector of) calculated embeddings sequence and None if embedder is disabled.\n        """"""\n        tokens_embedded = np.array([], dtype=np.float32)\n        if callable(self.embedder):\n            tokens_embedded = self.embedder([tokens], mean=mean_embeddings)[0]\n        return tokens_embedded\n\n    def bow_encode_tokens(self, tokens: List[str]) -> np.ndarray:\n        """"""\n        Args:\n            tokens: list of tokens to BOW encode\n\n        Returns:\n            if uses BOW encoder, returns np array with BOW encoding for tokens.\n            Otherwise returns an empty list.\n        """"""\n        bow_features = np.array([], dtype=np.float32)\n        if self._use_bow_encoder():\n            tokens_idx = self.word_vocab(tokens)\n            bow_features = self.bow_embedder([tokens_idx])[0]\n            bow_features = bow_features.astype(np.float32)\n        return bow_features\n\n    @staticmethod\n    def _standard_normal_like(source_vector: np.ndarray) -> np.ndarray:\n        """"""\n        Args:\n            source_vector: the vector of which to follow the result shape\n\n        Returns:\n            the standard normal distribution of the shape of the source vector\n        """"""\n        vector_dim = source_vector.shape[0]\n        return np.random.normal(loc=0.0, scale=1 / vector_dim, size=vector_dim)\n\n    @staticmethod\n    def _pad_sequence_to_size(out_sequence_length: int, token_dim: int, tokens_embedded: np.ndarray) -> np.ndarray:\n        """"""\n        Pad the passed vectors sequence to the specified length.\n\n        Args:\n            out_sequence_length: the length to pad sequence to\n            token_dim: the shape of output embedding\n            tokens_embedded: some sequence of vectors\n\n        Returns:\n            the padded sequence of vectors\n        """"""\n        out_sequence_length = out_sequence_length - len(tokens_embedded)\n        padding = np.zeros(shape=(out_sequence_length, token_dim), dtype=np.float32)\n        if tokens_embedded:\n            emb_context = np.concatenate((padding, np.array(tokens_embedded)))\n        else:\n            emb_context = padding\n        return emb_context\n\n    def calc_tokens_mean_embedding(self, tokens: List[str]) -> np.ndarray:\n        """"""\n        Args:\n            tokens: list of tokens to embed\n\n        Returns:\n            the average vector of embeddings sequence\n            or if avg is zeros then the standard normal distributed random vector instead.\n            None if embedder is disabled.\n        """"""\n        tokens_embedded = self._embed_tokens(tokens, True)\n        # random embedding instead of zeros\n        if tokens_embedded.size != 0 and np.all(tokens_embedded < 1e-20):\n            # TODO:  size != 0 not pythonic\n            tokens_embedded = np.fabs(self._standard_normal_like(tokens_embedded))\n        return tokens_embedded\n\n    def calc_tokens_embeddings(self, output_sequence_length: int, token_dim: int, tokens: List[str]) -> np.ndarray:\n        """"""\n        Calculate embeddings of passed tokens.\n        Args:\n            output_sequence_length: the length of sequence to output\n            token_dim: the shape of output embedding\n            tokens: list of tokens to embed\n\n        Returns:\n            the padded sequence of calculated embeddings\n        """"""\n        tokens_embedded = self._embed_tokens(tokens, False)\n        if tokens_embedded is not None:\n            emb_context = self._pad_sequence_to_size(output_sequence_length, token_dim, tokens_embedded)\n        else:\n            emb_context = np.array([], dtype=np.float32)\n        return emb_context\n\n    def get_dims(self) -> TokensVectorRepresentationParams:\n        """"""\n        Returns:\n            the TokensVectorRepresentationParams with embedder and BOW encoder output dimensions.\n            None instead of the missing dim if BOW encoder or embedder are missing.\n        """"""\n        embedder_dim = self.embedder.dim if self.embedder else None\n        bow_encoder_dim = len(self.word_vocab) if self.bow_embedder else None\n        return TokensVectorRepresentationParams(embedder_dim, bow_encoder_dim)\n'"
deeppavlov/models/go_bot/policy/__init__.py,0,b''
deeppavlov/models/go_bot/policy/policy_network.py,36,"b'import json\nfrom typing import Tuple, Optional\nfrom logging import getLogger\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom deeppavlov.core.common.errors import ConfigError\nfrom deeppavlov.core.layers import tf_attention_mechanisms as am, tf_layers\n\n# noinspection PyUnresolvedReferences\nfrom tensorflow.contrib.layers import xavier_initializer as xav\n\nfrom deeppavlov.core.models.tf_model import LRScheduledTFModel\nfrom deeppavlov.models.go_bot.nlu.dto.nlu_response import NLUResponse\n\nfrom deeppavlov.models.go_bot.nlu.tokens_vectorizer import TokensVectorRepresentationParams\nfrom deeppavlov.models.go_bot.dto.dataset_features import BatchDialoguesFeatures, BatchDialoguesTargets\n\n# todo\nfrom deeppavlov.models.go_bot.dto.shared_gobot_params import SharedGoBotParams\nfrom deeppavlov.models.go_bot.policy.dto.attn_params import GobotAttnParams\nfrom deeppavlov.models.go_bot.policy.dto.digitized_policy_features import DigitizedPolicyFeatures\nfrom deeppavlov.models.go_bot.policy.dto.policy_network_params import PolicyNetworkParams\nfrom deeppavlov.models.go_bot.policy.dto.policy_prediction import PolicyPrediction\nfrom deeppavlov.models.go_bot.tracker.dto.dst_knowledge import DSTKnowledge\n\nlog = getLogger(__name__)\n\n\nclass PolicyNetwork(LRScheduledTFModel):\n    """"""\n    the Policy Network is a ML model whose goal is to choose the right system response when in dialogue with user.\n    """"""\n\n    GRAPH_PARAMS = [""hidden_size"", ""action_size"", ""dense_size"", ""attention_params""]\n    SERIALIZABLE_FIELDS = [""hidden_size"", ""action_size"", ""dense_size"", ""dropout_rate"", ""l2_reg_coef"",\n                           ""attention_params""]\n\n    def __init__(self, network_params_passed: PolicyNetworkParams,\n                 tokens_dims: TokensVectorRepresentationParams,\n                 features_params: SharedGoBotParams,\n                 load_path,\n                 save_path,\n                 debug=False,\n                 **kwargs):\n        self.debug = debug\n        if self.debug:\n            log.debug(f""BEFORE {self.__class__.__name__} init(): ""\n                      f""network_params_passed={network_params_passed}, tokens_dims={tokens_dims}, ""\n                      f""features_params={features_params}, load_path={load_path}, save_path={save_path}, ""\n                      f""debug={debug}, kwargs={kwargs}"")\n        if network_params_passed.get_learning_rate():\n            kwargs[\'learning_rate\'] = network_params_passed.get_learning_rate()  # todo :(\n\n        super().__init__(load_path=load_path, save_path=save_path, **kwargs)\n\n        self.hidden_size = network_params_passed.get_hidden_size()\n        self.action_size = features_params.num_actions\n        self.dropout_rate = network_params_passed.get_dropout_rate()\n        self.l2_reg_coef = network_params_passed.get_l2_reg_coef()\n        self.dense_size = network_params_passed.get_dense_size()\n\n        attn_params_passed = network_params_passed.get_attn_params()\n        self.attention_params = self.configure_attn(attn_params_passed, tokens_dims, features_params)  # todo :(\n        self.input_size = self.calc_input_size(tokens_dims, features_params, self.attention_params)  # todo :(\n\n        if self.debug:\n            log.debug(f""INSIDE {self.__class__.__name__} init(). calculated NN hyperparams: ""\n                      f""attention_params={self.attention_params}, ""\n                      f""hidden_size={self.hidden_size}, action_size={self.action_size}, ""\n                      f""dropout_rate={self.dropout_rate}, l2_reg_coef={self.l2_reg_coef}, ""\n                      f""dense_size={self.dense_size}, input_size={self.input_size}"")\n\n        self._build_graph()\n        if self.debug:\n            log.debug(f""INSIDE {self.__class__.__name__} init(). build graph done."")\n        self.sess = tf.Session()\n        self.sess.run(tf.global_variables_initializer())\n        if self.debug:\n            log.debug(f""INSIDE {self.__class__.__name__} init(). ""\n                      f""Session() initialization and global_variables_initializer() done."")\n\n        if self.train_checkpoint_exists():\n            log.info(\n                f""INSIDE {self.__class__.__name__} init(). Initializing {self.__class__.__name__} from checkpoint."")\n            self.load()\n        else:\n            log.info(f""INSIDE {self.__class__.__name__} init(). Initializing {self.__class__.__name__} from scratch."")\n\n        if self.debug:\n            log.debug(f""AFTER {self.__class__.__name__} init(): ""\n                      f""network_params_passed={network_params_passed}, tokens_dims={tokens_dims}, ""\n                      f""features_params={features_params}, load_path={load_path}, save_path={save_path}, ""\n                      f""debug={debug}, kwargs={kwargs}"")\n\n    @staticmethod\n    def calc_input_size(tokens_dims: TokensVectorRepresentationParams,\n                        shared_go_bot_params: SharedGoBotParams,\n                        attention_params: Optional[GobotAttnParams]) -> int:\n        """"""\n        Args:\n            tokens_dims: the tokens vectors dimensions\n            shared_go_bot_params: GO-bot hyperparams used in various parts of the pipeline\n            attention_params: the params of attention mechanism of the network for which input size is calculated\n\n        Returns:\n            the calculated input shape of policy network\n        """"""\n        input_size = 6 + shared_go_bot_params.num_tracker_features + shared_go_bot_params.num_actions  # todo: why 6\n        if tokens_dims.bow_dim:\n            input_size += tokens_dims.bow_dim\n        if tokens_dims.embedding_dim:\n            input_size += tokens_dims.embedding_dim\n        if shared_go_bot_params.num_intents:\n            input_size += shared_go_bot_params.num_intents\n        if attention_params is not None:\n            input_size -= attention_params.token_size\n\n        return input_size\n\n    @staticmethod\n    def configure_attn(attn: dict,\n                       tokens_dims: TokensVectorRepresentationParams,\n                       features_params: SharedGoBotParams):\n        # todo store params in proper class objects not in dicts, requires serialization logic update\n\n        if not attn:\n            return None\n\n        token_size = tokens_dims.embedding_dim  # todo sync with nn params\n        action_as_key = attn.get(\'action_as_key\', False)\n        intent_as_key = attn.get(\'intent_as_key\', False)\n        key_size = PolicyNetwork.calc_attn_key_size(features_params, action_as_key, intent_as_key)\n\n        gobot_attn_params = GobotAttnParams(max_num_tokens=attn.get(""max_num_tokens""),\n                                            hidden_size=attn.get(""hidden_size""),\n                                            token_size=token_size,\n                                            key_size=key_size,\n                                            type_=attn.get(""type""),\n                                            projected_align=attn.get(""projected_align""),\n                                            depth=attn.get(""depth""),\n                                            action_as_key=action_as_key,\n                                            intent_as_key=intent_as_key)\n\n        return gobot_attn_params\n\n    @staticmethod\n    def calc_attn_key_size(shared_go_bot_params: SharedGoBotParams, action_as_key: bool, intent_as_key: bool) -> int:\n        """"""\n        Args:\n            shared_go_bot_params: GO-bot hyperparams used in various parts of the pipeline\n            action_as_key: True if actions are part of attention keys\n            intent_as_key: True if intents are part of attention keys\n\n        Returns:\n            the calculated attention key shape of policy network\n        """"""\n        # True if actions are part of attention keys -- actually *the last predicted action*\n\n        possible_key_size = 0\n        if action_as_key:\n            possible_key_size += shared_go_bot_params.num_actions\n        if intent_as_key and shared_go_bot_params.num_intents:\n            possible_key_size += shared_go_bot_params.num_intents\n        possible_key_size = possible_key_size or 1  # todo rewrite\n        return possible_key_size\n\n    def calc_attn_key(self, nlu_response: NLUResponse, tracker_knowledge: DSTKnowledge):\n        """"""\n        Args:\n            nlu_response: nlu analysis output, currently only intents data is used\n            tracker_knowledge: one-hot-encoded previous executed action\n\n        Returns:\n            vector representing an attention key\n        """"""\n        # todo dto-like class for the attn features?\n\n        attn_key = np.array([], dtype=np.float32)\n\n        if self.attention_params:\n            if self.attention_params.action_as_key:\n                attn_key = np.hstack((attn_key, tracker_knowledge.tracker_prev_action))\n            if self.attention_params.intent_as_key:\n                attn_key = np.hstack((attn_key, nlu_response.intents))\n            if len(attn_key) == 0:\n                attn_key = np.array([1], dtype=np.float32)\n        return attn_key\n\n    @staticmethod\n    def stack_features(nlu_response: NLUResponse,\n                       tracker_knowledge: DSTKnowledge):\n        return np.hstack((nlu_response.tokens_vectorized.tokens_bow_encoded,\n                          nlu_response.tokens_vectorized.tokens_aggregated_embedding,\n                          nlu_response.intents,\n                          tracker_knowledge.state_features,\n                          tracker_knowledge.context_features,\n                          tracker_knowledge.tracker_prev_action))\n\n    @staticmethod\n    def calc_action_mask(tracker_knowledge: DSTKnowledge):\n        # mask is used to prevent tracker from predicting the api call twice\n        # via logical AND of action candidates and mask\n        # todo: seems to be an efficient idea but the intuition beyond this whole hack is not obvious\n        mask = np.ones(tracker_knowledge.n_actions, dtype=np.float32)\n\n        if np.any(tracker_knowledge.tracker_prev_action):\n            prev_act_id = np.argmax(tracker_knowledge.tracker_prev_action)\n            if prev_act_id == tracker_knowledge.api_call_id:\n                mask[prev_act_id] = 0.\n\n        return mask\n\n    def digitize_features(self,\n                          nlu_response: NLUResponse,\n                          tracker_knowledge: DSTKnowledge) -> DigitizedPolicyFeatures:\n        attn_key = self.calc_attn_key(nlu_response, tracker_knowledge)\n        concat_feats = self.stack_features(nlu_response, tracker_knowledge)\n        action_mask = self.calc_action_mask(tracker_knowledge)\n\n        return DigitizedPolicyFeatures(attn_key, concat_feats, action_mask)\n\n    def _build_graph(self) -> None:\n        self._add_placeholders()\n\n        _logits, self._state = self._build_body()\n\n        # probabilities normalization : elemwise multiply with action mask\n        _logits_exp = tf.multiply(tf.exp(_logits), self._action_mask)\n        _logits_exp_sum = tf.expand_dims(tf.reduce_sum(_logits_exp, -1), -1)\n        self._probs = tf.squeeze(_logits_exp / _logits_exp_sum, name=\'probs\')\n\n        # loss, train and predict operations\n        self._prediction = tf.argmax(self._probs, axis=-1, name=\'prediction\')\n\n        # _weights = tf.expand_dims(self._utterance_mask, -1)\n        # TODO: try multiplying logits to action_mask\n        onehots = tf.one_hot(self._action, self.action_size)\n        _loss_tensor = tf.nn.softmax_cross_entropy_with_logits_v2(\n            logits=_logits, labels=onehots\n        )\n        # multiply with batch utterance mask\n        _loss_tensor = tf.multiply(_loss_tensor, self._utterance_mask)\n        self._loss = tf.reduce_mean(_loss_tensor, name=\'loss\')\n        self._loss += self.l2_reg_coef * tf.losses.get_regularization_loss()\n        self._train_op = self.get_train_op(self._loss)\n\n    def _add_placeholders(self) -> None:\n        self._dropout_keep_prob = tf.placeholder_with_default(1.0, shape=[], name=\'dropout_prob\')\n\n        self._features = tf.placeholder(tf.float32, [None, None, self.input_size], name=\'features\')\n\n        self._action = tf.placeholder(tf.int32, [None, None], name=\'ground_truth_action\')\n\n        self._action_mask = tf.placeholder(tf.float32, [None, None, self.action_size], name=\'action_mask\')\n\n        self._utterance_mask = tf.placeholder(tf.float32, shape=[None, None], name=\'utterance_mask\')\n\n        self._batch_size = tf.shape(self._features)[0]\n\n        zero_state = tf.zeros([self._batch_size, self.hidden_size], dtype=tf.float32)\n        _initial_state_c = tf.placeholder_with_default(zero_state, shape=[None, self.hidden_size])\n        _initial_state_h = tf.placeholder_with_default(zero_state, shape=[None, self.hidden_size])\n        self._initial_state = tf.nn.rnn_cell.LSTMStateTuple(_initial_state_c, _initial_state_h)\n\n        if self.attention_params:\n            _emb_context_shape = [None, None, self.attention_params.max_num_tokens,\n                                  self.attention_params.token_size]\n            self._emb_context = tf.placeholder(tf.float32, _emb_context_shape, name=\'emb_context\')\n            self._key = tf.placeholder(tf.float32, [None, None, self.attention_params.key_size], name=\'key\')\n\n    def _build_body(self) -> Tuple[tf.Tensor, tf.Tensor]:\n        # input projection\n        _units = tf.layers.dense(self._features, self.dense_size,\n                                 kernel_regularizer=tf.nn.l2_loss, kernel_initializer=xav())\n\n        if self.attention_params:\n            _attn_output = self._build_attn_body()\n            _units = tf.concat([_units, _attn_output], -1)\n\n        _units = tf_layers.variational_dropout(_units, keep_prob=self._dropout_keep_prob)\n\n        # recurrent network unit\n        _lstm_cell = tf.nn.rnn_cell.LSTMCell(self.hidden_size)\n        _utter_lengths = tf.cast(tf.reduce_sum(self._utterance_mask, axis=-1), tf.int32)\n\n        # _output: [batch_size, max_time, hidden_size]\n        # _state: tuple of two [batch_size, hidden_size]\n        _output, _state = tf.nn.dynamic_rnn(_lstm_cell, _units,\n                                            time_major=False, initial_state=self._initial_state,\n                                            sequence_length=_utter_lengths)\n\n        _output = tf.reshape(_output, (self._batch_size, -1, self.hidden_size))\n        _output = tf_layers.variational_dropout(_output, keep_prob=self._dropout_keep_prob)\n        # output projection\n        _logits = tf.layers.dense(_output, self.action_size,\n                                  kernel_regularizer=tf.nn.l2_loss, kernel_initializer=xav(), name=\'logits\')\n        return _logits, _state\n\n    def _build_attn_body(self):\n        attn_scope = f""attention_params/{self.attention_params.type_}""\n        with tf.variable_scope(attn_scope):\n            if self.attention_params.type_ == \'general\':\n                _attn_output = am.general_attention(self._key, self._emb_context,\n                                                    hidden_size=self.attention_params.hidden_size,\n                                                    projected_align=self.attention_params.projected_align)\n            elif self.attention_params.type_ == \'bahdanau\':\n                _attn_output = am.bahdanau_attention(self._key, self._emb_context,\n                                                     hidden_size=self.attention_params.hidden_size,\n                                                     projected_align=self.attention_params.projected_align)\n            elif self.attention_params.type_ == \'cs_general\':\n                _attn_output = am.cs_general_attention(self._key, self._emb_context,\n                                                       hidden_size=self.attention_params.hidden_size,\n                                                       depth=self.attention_params.depth,\n                                                       projected_align=self.attention_params.projected_align)\n            elif self.attention_params.type_ == \'cs_bahdanau\':\n                _attn_output = am.cs_bahdanau_attention(self._key, self._emb_context,\n                                                        hidden_size=self.attention_params.hidden_size,\n                                                        depth=self.attention_params.depth,\n                                                        projected_align=self.attention_params.projected_align)\n            elif self.attention_params.type_ == \'light_general\':\n                _attn_output = am.light_general_attention(self._key, self._emb_context,\n                                                          hidden_size=self.attention_params.hidden_size,\n                                                          projected_align=self.attention_params.projected_align)\n            elif self.attention_params.type_ == \'light_bahdanau\':\n                _attn_output = am.light_bahdanau_attention(self._key, self._emb_context,\n                                                           hidden_size=self.attention_params.hidden_size,\n                                                           projected_align=self.attention_params.projected_align)\n            else:\n                raise ValueError(""wrong value for attention mechanism type"")\n        return _attn_output\n\n    def train_checkpoint_exists(self):\n        return tf.train.checkpoint_exists(str(self.load_path.resolve()))\n\n    def get_attn_hyperparams(self) -> Optional[GobotAttnParams]:\n        attn_hyperparams = None\n        if self.attention_params:\n            attn_hyperparams = self.attention_params\n        return attn_hyperparams\n\n    def has_attn(self):\n        """"""\n        Returns:\n            True if the model has an attention mechanism\n        """"""\n        return self.attention_params is not None\n\n    def get_attn_window_size(self):\n        """"""\n        Returns:\n             the length of the window the model looks with attn if the attention mechanism is configured.\n             if the model has no attention mechanism returns None.\n        """"""\n        return self.attention_params.max_num_tokens if self.has_attn() else None\n\n    def __call__(self, batch_dialogues_features: BatchDialoguesFeatures,\n                 states_c: np.ndarray, states_h: np.ndarray, prob: bool = False,\n                 *args, **kwargs) -> PolicyPrediction:\n\n        states_c = [[states_c]]  # list of list aka batch of dialogues\n        states_h = [[states_h]]  # list of list aka batch of dialogues\n\n        feed_dict = {\n            self._dropout_keep_prob: 1.,\n            self._initial_state: (states_c, states_h),\n            self._utterance_mask: batch_dialogues_features.b_padded_dialogue_length_mask,\n            self._features: batch_dialogues_features.b_featuress,\n            self._action_mask: batch_dialogues_features.b_action_masks\n        }\n        if self.attention_params:\n            feed_dict[self._emb_context] = batch_dialogues_features.b_tokens_embeddings_paddeds\n            feed_dict[self._key] = batch_dialogues_features.b_attn_keys\n\n        probs, prediction, state = self.sess.run([self._probs, self._prediction, self._state], feed_dict=feed_dict)\n\n        policy_prediction = PolicyPrediction(probs, prediction, state[0], state[1])\n\n        return policy_prediction\n\n    def train_on_batch(self,\n                       batch_dialogues_features: BatchDialoguesFeatures,\n                       batch_dialogues_targets: BatchDialoguesTargets) -> dict:\n\n        feed_dict = {\n            self._dropout_keep_prob: 1.,\n            self._utterance_mask: batch_dialogues_features.b_padded_dialogue_length_mask,\n            self._features: batch_dialogues_features.b_featuress,\n            self._action: batch_dialogues_targets.b_action_ids,\n            self._action_mask: batch_dialogues_features.b_action_masks\n        }\n\n        if self.attention_params:\n            feed_dict[self._emb_context] = batch_dialogues_features.b_tokens_embeddings_paddeds\n            feed_dict[self._key] = batch_dialogues_features.b_attn_keys\n\n        _, loss_value, prediction = self.sess.run([self._train_op, self._loss, self._prediction], feed_dict=feed_dict)\n\n        return {\'loss\': loss_value,\n                \'learning_rate\': self.get_learning_rate(),\n                \'momentum\': self.get_momentum()}\n\n    def load(self, *args, **kwargs) -> None:\n        # todo move load_nn_params here?\n        self._load_nn_params()\n        super().load(*args, **kwargs)\n\n    def _load_nn_params(self) -> None:\n        if self.debug:\n            log.debug(f""BEFORE {self.__class__.__name__} _load_nn_params()"")\n\n        path = str(self.load_path.with_suffix(\'.json\').resolve())\n\n        if self.debug:\n            log.debug(f""INSIDE {self.__class__.__name__} _load_nn_params(): path={path}"")\n        # log.info(f""[loading parameters from {path}]"")\n        with open(path, \'r\', encoding=\'utf8\') as fp:\n            params = json.load(fp)\n        if self.debug:\n            log.debug(f""INSIDE {self.__class__.__name__} _load_nn_params(): ""\n                      f""params={params}, GRAPH_PARAMS={self.GRAPH_PARAMS}"")\n\n        for p in self.GRAPH_PARAMS:\n            if self.__getattribute__(p) != params.get(p) and p not in {\'attn\',\n                                                                       \'attention_mechanism\', \'attention_params\'}:\n                # todo backward-compatible attention serialization\n                raise ConfigError(f""`{p}` parameter must be equal to saved""\n                                  f"" model parameter value `{params.get(p)}`,""\n                                  f"" but is equal to `{self.__getattribute__(p)}`"")\n\n        if self.debug:\n            log.debug(f""AFTER {self.__class__.__name__} _load_nn_params()"")\n\n    def save(self, *args, **kwargs) -> None:\n        super().save(*args, **kwargs)\n        # todo move save_nn_params here?\n        self._save_nn_params()\n\n    def _save_nn_params(self) -> None:\n        if self.debug:\n            log.debug(f""BEFORE {self.__class__.__name__} _save_nn_params()"")\n\n        path = str(self.save_path.with_suffix(\'.json\').resolve())\n        if self.debug:\n            log.debug(f""INSIDE {self.__class__.__name__} _save_nn_params(): path={path}"")\n        nn_params = {opt: self.__getattribute__(opt) for opt in self.SERIALIZABLE_FIELDS}\n        if self.debug:\n            log.debug(f""INSIDE {self.__class__.__name__} _save_nn_params(): nn_params={nn_params}"")\n        # log.info(f""[saving parameters to {path}]"")\n        with open(path, \'w\', encoding=\'utf8\') as fp:\n            json.dump(nn_params, fp)\n\n        if self.debug:\n            log.debug(f""AFTER {self.__class__.__name__} _save_nn_params()"")\n'"
deeppavlov/models/go_bot/tracker/__init__.py,0,b''
deeppavlov/models/go_bot/tracker/dialogue_state_tracker.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import Dict, Any\n\nimport numpy as np\n\nfrom deeppavlov.core.models.component import Component\nfrom deeppavlov.models.go_bot.nlg.nlg_manager import NLGManagerInterface\nfrom deeppavlov.models.go_bot.policy.dto.policy_network_params import PolicyNetworkParams\nfrom deeppavlov.models.go_bot.tracker.dto.dst_knowledge import DSTKnowledge\nfrom deeppavlov.models.go_bot.tracker.featurized_tracker import FeaturizedTracker\n\nlog = getLogger(__name__)\n\n\nclass DialogueStateTracker(FeaturizedTracker):\n    def get_current_knowledge(self) -> DSTKnowledge:\n        state_features = self.get_features()\n        context_features = self.calc_context_features()\n        knowledge = DSTKnowledge(self.prev_action,\n                                 state_features, context_features,\n                                 self.api_call_id,\n                                 self.n_actions)\n        return knowledge\n\n    def __init__(self,\n                 slot_names, n_actions: int, api_call_id: int, hidden_size: int, database: Component = None) -> None:\n        super().__init__(slot_names)\n        self.hidden_size = hidden_size\n        self.database = database\n        self.n_actions = n_actions\n        self.api_call_id = api_call_id\n\n        self.reset_state()\n\n    @staticmethod\n    def from_gobot_params(parent_tracker: FeaturizedTracker,\n                          nlg_manager: NLGManagerInterface,\n                          policy_network_params: PolicyNetworkParams,\n                          database: Component):\n        return DialogueStateTracker(parent_tracker.slot_names,\n                                    nlg_manager.num_of_known_actions(), nlg_manager.get_api_call_action_id(),\n                                    policy_network_params.hidden_size,\n                                    database)\n\n    def reset_state(self):\n        super().reset_state()\n        self.db_result = None\n        self.current_db_result = None\n        self.prev_action = np.zeros(self.n_actions, dtype=np.float32)\n        self._reset_network_state()\n\n    def _reset_network_state(self):\n        self.network_state = (\n            np.zeros([1, self.hidden_size], dtype=np.float32),\n            np.zeros([1, self.hidden_size], dtype=np.float32)\n        )\n\n    def update_previous_action(self, prev_act_id: int) -> None:\n        self.prev_action *= 0.\n        self.prev_action[prev_act_id] = 1.\n\n    # todo oserikov \xd1\x8d\xd1\x82\xd0\xbe \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd1\x82 \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xbf\xd0\xb8\xd1\x81\xd0\xb0\xd1\x82\xd1\x8c\n    def update_ground_truth_db_result_from_context(self, context: Dict[str, Any]):\n        self.current_db_result = context.get(\'db_result\', None)\n        self._update_db_result()\n\n    def make_api_call(self) -> None:\n        slots = self.get_state()\n        db_results = []\n        if self.database is not None:\n\n            # filter slot keys with value equal to \'dontcare\' as\n            # there is no such value in database records\n            # and remove unknown slot keys (for example, \'this\' in dstc2 tracker)\n            db_slots = {\n                s: v for s, v in slots.items() if v != \'dontcare\' and s in self.database.keys\n            }\n\n            db_results = self.database([db_slots])[0]\n\n            # filter api results if there are more than one\n            # TODO: add sufficient criteria for database results ranking\n            if len(db_results) > 1:\n                db_results = [r for r in db_results if r != self.db_result]\n        else:\n            log.warning(""No database specified."")\n\n        log.info(f""Made api_call with {slots}, got {len(db_results)} results."")\n        self.current_db_result = {} if not db_results else db_results[0]\n        self._update_db_result()\n\n    def calc_action_mask(self) -> np.ndarray:\n        mask = np.ones(self.n_actions, dtype=np.float32)\n\n        if np.any(self.prev_action):\n            prev_act_id = np.argmax(self.prev_action)\n            if prev_act_id == self.api_call_id:\n                mask[prev_act_id] = 0.\n\n        return mask\n\n    def calc_context_features(self):\n        # todo \xd0\xbd\xd0\xb5\xd0\xba\xd1\x80\xd0\xb0\xd1\x81\xd0\xb8\xd0\xb2\xd0\xbe\n        current_db_result = self.current_db_result\n        db_result = self.db_result\n        dst_state = self.get_state()\n\n        result_matches_state = 0.\n        if current_db_result is not None:\n            matching_items = dst_state.items()\n            result_matches_state = all(v == db_result.get(s)\n                                       for s, v in matching_items\n                                       if v != \'dontcare\') * 1.\n        context_features = np.array([\n            bool(current_db_result) * 1.,\n            (current_db_result == {}) * 1.,\n            (db_result is None) * 1.,\n            bool(db_result) * 1.,\n            (db_result == {}) * 1.,\n            result_matches_state\n        ], dtype=np.float32)\n        return context_features\n\n    def _update_db_result(self):\n        if self.current_db_result is not None:\n            self.db_result = self.current_db_result\n\n    def fill_current_state_with_db_results(self) -> dict:\n        slots = self.get_state()\n        if self.db_result:\n            for k, v in self.db_result.items():\n                slots[k] = str(v)\n        return slots\n\n\nclass MultipleUserStateTrackersPool(object):\n    def __init__(self, base_tracker: DialogueStateTracker):\n        self._ids_to_trackers = {}\n        self.base_tracker = base_tracker\n\n    def check_new_user(self, user_id: int) -> bool:\n        return user_id in self._ids_to_trackers\n\n    def get_user_tracker(self, user_id: int) -> DialogueStateTracker:\n        if not self.check_new_user(user_id):\n            raise RuntimeError(f""The user with {user_id} ID is not being tracked"")\n\n        tracker = self._ids_to_trackers[user_id]\n\n        # TODO: understand why setting current_db_result to None is necessary\n        tracker.current_db_result = None\n        return tracker\n\n    def new_tracker(self):\n        tracker = DialogueStateTracker(self.base_tracker.slot_names, self.base_tracker.n_actions,\n                                       self.base_tracker.api_call_id, self.base_tracker.hidden_size,\n                                       self.base_tracker.database)\n        return tracker\n\n    def get_or_init_tracker(self, user_id: int):\n        if not self.check_new_user(user_id):\n            self.init_new_tracker(user_id, self.base_tracker)\n\n        return self.get_user_tracker(user_id)\n\n    def init_new_tracker(self, user_id: int, tracker_entity: DialogueStateTracker) -> None:\n        # TODO: implement a better way to init a tracker\n        # todo deprecated. The whole class should follow AbstractFactory or Pool pattern?\n        tracker = DialogueStateTracker(\n            tracker_entity.slot_names,\n            tracker_entity.n_actions,\n            tracker_entity.api_call_id,\n            tracker_entity.hidden_size,\n            tracker_entity.database\n        )\n\n        self._ids_to_trackers[user_id] = tracker\n\n    def reset(self, user_id: int = None) -> None:\n        if user_id is not None and not self.check_new_user(user_id):\n            raise RuntimeError(f""The user with {user_id} ID is not being tracked"")\n\n        if user_id is not None:\n            self._ids_to_trackers[user_id].reset_state()\n        else:\n            self._ids_to_trackers.clear()\n'"
deeppavlov/models/go_bot/tracker/featurized_tracker.py,0,"b'from typing import List, Iterator\n\nimport numpy as np\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.models.go_bot.nlu.dto.nlu_response import NLUResponse\nfrom deeppavlov.models.go_bot.tracker.dto.tracker_knowledge_interface import TrackerKnowledgeInterface\nfrom deeppavlov.models.go_bot.tracker.tracker_interface import TrackerInterface\n\n\n@register(\'featurized_tracker\')\nclass FeaturizedTracker(TrackerInterface):\n    """"""\n    Tracker that overwrites slots with new values.\n    Features are binary features (slot is present/absent) plus difference features\n    (slot value is (the same)/(not the same) as before last update) and count\n    features (sum of present slots and sum of changed during last update slots).\n\n    Parameters:\n        slot_names: list of slots that should be tracked.\n    """"""\n\n    def get_current_knowledge(self) -> TrackerKnowledgeInterface:\n        raise NotImplementedError(""Featurized tracker lacks get_current_knowledge() method. ""\n                                  ""To be improved in future versions."")\n\n    def __init__(self, slot_names: List[str]) -> None:\n        self.slot_names = list(slot_names)\n        self.history = []\n        self.current_features = None\n\n    @property\n    def state_size(self) -> int:\n        return len(self.slot_names)\n\n    @property\n    def num_features(self) -> int:\n        return self.state_size * 3 + 3\n\n    def update_state(self, nlu_response: NLUResponse):\n        slots = nlu_response.slots\n\n        if isinstance(slots, list):\n            self.history.extend(self._filter(slots))\n\n        elif isinstance(slots, dict):\n            for slot, value in self._filter(slots.items()):\n                self.history.append((slot, value))\n\n        prev_state = self.get_state()\n        bin_feats = self._binary_features()\n        diff_feats = self._diff_features(prev_state)\n        new_feats = self._new_features(prev_state)\n\n        self.current_features = np.hstack((\n            bin_feats,\n            diff_feats,\n            new_feats,\n            np.sum(bin_feats),\n            np.sum(diff_feats),\n            np.sum(new_feats))\n        )\n\n    def get_state(self):\n        # lasts = {}\n        # for slot, value in self.history:\n        #     lasts[slot] = value\n        # return lasts\n        return dict(self.history)\n\n    def reset_state(self):\n        self.history = []\n        self.current_features = np.zeros(self.num_features, dtype=np.float32)\n\n    def get_features(self):\n        return self.current_features\n\n    def _filter(self, slots) -> Iterator:\n        return filter(lambda s: s[0] in self.slot_names, slots)\n\n    def _binary_features(self) -> np.ndarray:\n        feats = np.zeros(self.state_size, dtype=np.float32)\n        lasts = self.get_state()\n        for i, slot in enumerate(self.slot_names):\n            if slot in lasts:\n                feats[i] = 1.\n        return feats\n\n    def _diff_features(self, state) -> np.ndarray:\n        feats = np.zeros(self.state_size, dtype=np.float32)\n        curr_state = self.get_state()\n\n        for i, slot in enumerate(self.slot_names):\n            if slot in curr_state and slot in state and curr_state[slot] != state[slot]:\n                feats[i] = 1.\n\n        return feats\n\n    def _new_features(self, state) -> np.ndarray:\n        feats = np.zeros(self.state_size, dtype=np.float32)\n        curr_state = self.get_state()\n\n        for i, slot in enumerate(self.slot_names):\n            if slot in curr_state and slot not in state:\n                feats[i] = 1.\n\n        return feats\n'"
deeppavlov/models/go_bot/tracker/tracker_interface.py,0,"b'from abc import ABCMeta, abstractmethod\nfrom typing import Any, Dict\n\nimport numpy as np\n\nfrom deeppavlov.models.go_bot.nlu.dto.nlu_response_interface import NLUResponseInterface\nfrom deeppavlov.models.go_bot.tracker.dto.tracker_knowledge_interface import TrackerKnowledgeInterface\n\n\nclass TrackerInterface(metaclass=ABCMeta):\n    """"""\n    An abstract class for trackers: a model that holds a dialogue state and\n    generates state features.\n    """"""\n\n    @abstractmethod\n    def update_state(self, nlu_response: NLUResponseInterface) -> None:\n        """"""Updates dialogue state with new ``slots``, calculates features.""""""\n        pass\n\n    @abstractmethod\n    def get_state(self) -> Dict[str, Any]:\n        """"""\n        Returns:\n            Dict[str, Any]: dictionary with current slots and their values.""""""\n        pass\n\n    @abstractmethod\n    def reset_state(self) -> None:\n        """"""Resets dialogue state""""""\n        pass\n\n    @abstractmethod\n    def get_features(self) -> np.ndarray:\n        """"""\n        Returns:\n            np.ndarray[float]: numpy array with calculates state features.""""""\n        pass\n\n    @abstractmethod\n    def get_current_knowledge(self) -> TrackerKnowledgeInterface:\n        pass\n'"
deeppavlov/models/ranking/matching_models/__init__.py,0,b''
deeppavlov/models/spelling_correction/brillmoore/__init__.py,0,b'from .error_model import ErrorModel\n'
deeppavlov/models/spelling_correction/brillmoore/error_model.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport csv\nimport itertools\nfrom collections import defaultdict, Counter\nfrom heapq import heappop, heappushpop, heappush\nfrom logging import getLogger\nfrom math import log, exp\nfrom typing import List, Iterable, Tuple\n\nfrom tqdm import tqdm\n\nfrom deeppavlov.core.common.errors import ConfigError\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.estimator import Estimator\nfrom deeppavlov.vocabs.typos import StaticDictionary\n\nlogger = getLogger(__name__)\n\n\n@register(\'spelling_error_model\')\nclass ErrorModel(Estimator):\n    """"""Component that uses statistics based error model to find best candidates in a static dictionary.\n    Based on An Improved Error Model for Noisy Channel Spelling Correction by Eric Brill and Robert C. Moore\n\n    Args:\n        dictionary: a :class:`~deeppavlov.vocabs.typos.StaticDictionary` object\n        window: maximum context window size\n        candidates_count: maximum number of replacement candidates to return for every token in the input\n\n    Attributes:\n        costs: logarithmic probabilities of character sequences replacements\n        dictionary: a :class:`~deeppavlov.vocabs.typos.StaticDictionary` object\n        window: maximum context window size\n        candidates_count: maximum number of replacement candidates to return for every token in the input\n    """"""\n\n    def __init__(self, dictionary: StaticDictionary, window: int = 1, candidates_count: int = 1, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.costs = defaultdict(itertools.repeat(float(\'-inf\')).__next__)\n        self.dictionary = dictionary\n        self.window = window\n        if self.window == 0:\n            self.find_candidates = self._find_candidates_window_0\n        else:\n            self.find_candidates = self._find_candidates_window_n\n        self.costs[(\'\', \'\')] = log(1)\n        self.costs[(\'\xe2\x9f\xac\', \'\xe2\x9f\xac\')] = log(1)\n        self.costs[(\'\xe2\x9f\xad\', \'\xe2\x9f\xad\')] = log(1)\n\n        for c in self.dictionary.alphabet:\n            self.costs[(c, c)] = log(1)\n        # if self.ser_path.is_file():\n        self.load()\n\n        self.candidates_count = candidates_count\n\n    def _find_candidates_window_0(self, word, prop_threshold=1e-6):\n        threshold = log(prop_threshold)\n        d = {}\n        prefixes_heap = [(0, {\'\'})]\n        candidates = [(float(\'-inf\'), \'\') for _ in range(self.candidates_count)]\n        word = \'\xe2\x9f\xac{}\xe2\x9f\xad\'.format(word.lower().replace(\'\xd1\x91\', \'\xd0\xb5\'))\n        word_len = len(word) + 1\n        while prefixes_heap and -prefixes_heap[0][0] > candidates[0][0]:\n            _, prefixes = heappop(prefixes_heap)\n            for prefix in prefixes:\n                res = []\n                for i in range(word_len):\n                    c = word[i - 1:i]\n                    res.append(max(\n                        (res[-1] + self.costs[(\'\', c)]) if i else float(\'-inf\'),\n                        d[prefix[:-1]][i] + self.costs[(prefix[-1], \'\')] if prefix else float(\n                            \'-inf\'),\n                        (d[prefix[:-1]][i - 1] + (self.costs[(prefix[-1], c)]))\n                        if prefix and i else float(\'-inf\')\n                    ) if i or prefix else 0)\n                d[prefix] = res\n                if prefix in self.dictionary.words_set:\n                    heappushpop(candidates, (res[-1], prefix))\n                potential = max(res)\n                if potential > threshold:\n                    heappush(prefixes_heap, (-potential, self.dictionary.words_trie[prefix]))\n        return [(w.strip(\'\xe2\x9f\xac\xe2\x9f\xad\'), score) for score, w in sorted(candidates, reverse=True) if\n                score > threshold]\n\n    def _find_candidates_window_n(self, word, prop_threshold=1e-6):\n        threshold = log(prop_threshold)\n        word = \'\xe2\x9f\xac{}\xe2\x9f\xad\'.format(word.lower().replace(\'\xd1\x91\', \'\xd0\xb5\'))\n        word_len = len(word) + 1\n        inf = float(\'-inf\')\n        d = defaultdict(list)\n        d[\'\'] = [0.] + [inf] * (word_len - 1)\n        prefixes_heap = [(0, self.dictionary.words_trie[\'\'])]\n        candidates = [(inf, \'\')] * self.candidates_count\n        while prefixes_heap and -prefixes_heap[0][0] > candidates[0][0]:\n            _, prefixes = heappop(prefixes_heap)\n            for prefix in prefixes:\n                prefix_len = len(prefix)\n                d[prefix] = res = [inf]\n                for i in range(1, word_len):\n                    c_res = [inf]\n                    for li in range(1, min(prefix_len + 1, self.window + 2)):\n                        for ri in range(1, min(i + 1, self.window + 2)):\n                            prev = d[prefix[:-li]][i - ri]\n                            if prev > threshold:\n                                edit = (prefix[-li:], word[i - ri:i])\n                                if edit in self.costs:\n                                    c_res.append(prev +\n                                                 self.costs[edit])\n                    res.append(max(c_res))\n                if prefix in self.dictionary.words_set:\n                    heappushpop(candidates, (res[-1], prefix))\n                potential = max(res)\n                # potential = max(\n                #     [e for i in range(self.window + 2) for e in d[prefix[:prefix_len - i]]])\n                if potential > threshold:\n                    heappush(prefixes_heap, (-potential, self.dictionary.words_trie[prefix]))\n        return [(w.strip(\'\xe2\x9f\xac\xe2\x9f\xad\'), score) for score, w in sorted(candidates, reverse=True) if\n                score > threshold]\n\n    def _infer_instance(self, instance: List[str]) -> List[List[Tuple[float, str]]]:\n        candidates = []\n        for incorrect in instance:\n            if any([c not in self.dictionary.alphabet for c in incorrect]):\n                candidates.append([(0, incorrect)])\n            else:\n                res = self.find_candidates(incorrect, prop_threshold=1e-6)\n                if res:\n                    candidates.append([(score, candidate) for candidate, score in res])\n                else:\n                    candidates.append([(0, incorrect)])\n        return candidates\n\n    def __call__(self, data: Iterable[Iterable[str]], *args, **kwargs) -> List[List[List[Tuple[float, str]]]]:\n        """"""Propose candidates for tokens in sentences\n\n        Args:\n            data: batch of tokenized sentences\n\n        Returns:\n            batch of lists of probabilities and candidates for every token\n        """"""\n        data = list(data)\n        if len(data) > 1:\n            data = tqdm(data, desc=\'Infering a batch with the error model\', leave=False)\n        return [self._infer_instance(instance) for instance in data]\n\n    @staticmethod\n    def _distance_edits(seq1, seq2):\n        l1, l2 = len(seq1), len(seq2)\n        d = [[(i, ()) for i in range(l2 + 1)]]\n        d += [[(i, ())] + [(0, ())] * l2 for i in range(1, l1 + 1)]\n\n        for i in range(1, l1 + 1):\n            for j in range(1, l2 + 1):\n                edits = [\n                    (d[i - 1][j][0] + 1, d[i - 1][j][1] + ((seq1[i - 1], \'\'),)),\n                    (d[i][j - 1][0] + 1, d[i][j - 1][1] + ((\'\', seq2[j - 1]),)),\n                    (d[i - 1][j - 1][0] + (seq1[i - 1] != seq2[j - 1]),\n                     d[i - 1][j - 1][1] + ((seq1[i - 1], seq2[j - 1]),))\n                ]\n                if i > 1 and j > 1 and seq1[i - 1] == seq2[j - 2] and seq1[i - 2] == seq2[j - 1]:\n                    edits.append((d[i - 2][j - 2][0] + (seq1[i - 1] != seq2[j - 1]),\n                                  d[i - 2][j - 2][1] + ((seq1[i - 2:i], seq2[j - 2:j]),)))\n                d[i][j] = min(edits, key=lambda x: x[0])\n\n        return d[-1][-1]\n\n    def fit(self, x: List[str], y: List[str]):\n        """"""Calculate character sequences replacements probabilities\n\n        Args:\n            x: words with spelling errors\n            y: words without spelling errors\n        """"""\n        changes = []\n        entries = []\n        data = list(zip(x, y))\n        window = 4\n        for error, correct in tqdm(data, desc=\'Training the error model\'):\n            correct = \'\xe2\x9f\xac{}\xe2\x9f\xad\'.format(\' \'.join(correct))\n            error = \'\xe2\x9f\xac{}\xe2\x9f\xad\'.format(\' \'.join(error))\n            d, ops = self._distance_edits(correct, error)\n            if d <= 2:\n                w_ops = set()\n                for pos in range(len(ops)):\n                    left, right = list(zip(*ops))\n                    for l in range(pos, max(0, pos - window) - 1, -1):\n                        for r in range(pos + 1, min(len(ops), l + 2 + window)):\n                            w_ops.add(((\'\'.join(left[l:r]), \'\'.join(right[l:r])), l, r))\n                ops = [x[0] for x in w_ops]\n\n                entries += [op[0] for op in ops]\n                changes += [op for op in ops]\n\n        e_count = Counter(entries)\n        c_count = Counter(changes)\n        incorrect_prior = 1\n        correct_prior = 19\n        for (w, s), c in c_count.items():\n            c = c + (incorrect_prior if w != s else correct_prior)\n            e = e_count[w] + incorrect_prior + correct_prior\n            p = c / e\n            self.costs[(w, s)] = log(p)\n\n    def save(self):\n        """"""Save replacements probabilities to a file\n\n        """"""\n        logger.info(""[saving error_model to `{}`]"".format(self.save_path))\n\n        with open(self.save_path, \'w\', newline=\'\', encoding=\'utf8\') as tsv_file:\n            writer = csv.writer(tsv_file, delimiter=\'\\t\')\n            for (w, s), log_p in self.costs.items():\n                writer.writerow([w, s, exp(log_p)])\n\n    def load(self):\n        """"""Load replacements probabilities from a file\n\n        """"""\n        if self.load_path:\n            if self.load_path.is_file():\n                logger.info(""loading error_model from `{}`"".format(self.load_path))\n                with open(self.load_path, \'r\', newline=\'\', encoding=\'utf8\') as tsv_file:\n                    reader = csv.reader(tsv_file, delimiter=\'\\t\')\n                    for w, s, p in reader:\n                        self.costs[(w, s)] = log(float(p))\n            elif not self.load_path.parent.is_dir():\n                raise ConfigError(""Provided `load_path` for {} doesn\'t exist!"".format(\n                    self.__class__.__name__))\n        else:\n            logger.info(\'No load_path provided, initializing error model from scratch\')\n'"
deeppavlov/models/spelling_correction/electors/__init__.py,0,b''
deeppavlov/models/spelling_correction/electors/kenlm_elector.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom pathlib import Path\nfrom typing import List, Tuple\n\nimport kenlm\n\nfrom deeppavlov.core.commands.utils import expand_path\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\n\nlogger = getLogger(__name__)\n\n\n@register(\'kenlm_elector\')\nclass KenlmElector(Component):\n    """"""Component that chooses a candidate with the highest product of base and language model probabilities\n\n    Args:\n         load_path: path to the kenlm model file\n         beam_size: beam size for highest probability search\n\n    Attributes:\n        lm: kenlm object\n        beam_size: beam size for highest probability search\n    """"""\n\n    def __init__(self, load_path: Path, beam_size: int = 4, *args, **kwargs):\n        self.lm = kenlm.Model(str(expand_path(load_path)))\n        self.beam_size = beam_size\n\n    def __call__(self, batch: List[List[List[Tuple[float, str]]]]) -> List[List[str]]:\n        """"""Choose the best candidate for every token\n\n        Args:\n            batch: batch of probabilities and string values of candidates for every token in a sentence\n\n        Returns:\n            batch of corrected tokenized sentences\n        """"""\n        return [self._infer_instance(candidates) for candidates in batch]\n\n    def _infer_instance(self, candidates: List[List[Tuple[float, str]]]):\n        candidates = candidates + [[(0, \'</s>\')]]\n        state = kenlm.State()\n        self.lm.BeginSentenceWrite(state)\n        beam = [(0, state, [])]\n        for sublist in candidates:\n            new_beam = []\n            for beam_score, beam_state, beam_words in beam:\n                for score, candidate in sublist:\n                    prev_state = beam_state\n                    c_score = 0\n                    cs = candidate.split()\n                    for candidate in cs:\n                        state = kenlm.State()\n                        c_score += self.lm.BaseScore(prev_state, candidate, state)\n                        prev_state = state\n                    new_beam.append((beam_score + score + c_score, state, beam_words + cs))\n            new_beam.sort(reverse=True)\n            beam = new_beam[:self.beam_size]\n        score, state, words = beam[0]\n        return words[:-1]\n'"
deeppavlov/models/spelling_correction/electors/top1_elector.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom logging import getLogger\nfrom typing import List, Tuple\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\n\nlogger = getLogger(__name__)\n\n\n@register(\'top1_elector\')\nclass TopOneElector(Component):\n    """"""Component that chooses a candidate with highest base probability for every token\n\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def __call__(self, batch: List[List[List[Tuple[float, str]]]]) -> List[List[str]]:\n        """"""Choose the best candidate for every token\n\n        Args:\n            batch: batch of probabilities and string values of candidates for every token in a sentence\n\n        Returns:\n            batch of corrected tokenized sentences\n        """"""\n        return [[max(sublist)[1] for sublist in candidates] for candidates in batch]\n'"
deeppavlov/models/spelling_correction/levenshtein/__init__.py,0,b'from .searcher_component import LevenshteinSearcherComponent\n'
deeppavlov/models/spelling_correction/levenshtein/levenshtein_searcher.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport copy\nimport itertools\n\nimport numpy as np\nfrom sortedcontainers import SortedListWithKey\n\nfrom .tabled_trie import Trie, make_trie\n\n\nclass LevenshteinSearcher:\n    """"""\n    \xd0\x9a\xd0\xbb\xd0\xb0\xd1\x81\xd1\x81 \xd0\xb4\xd0\xbb\xd1\x8f \xd0\xbf\xd0\xbe\xd0\xb8\xd1\x81\xd0\xba\xd0\xb0 \xd0\xb1\xd0\xbb\xd0\xb8\xd0\xb7\xd0\xba\xd0\xb8\xd1\x85 \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb2\n    \xd0\xb2 \xd1\x81\xd0\xbe\xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x81\xd1\x82\xd0\xb2\xd0\xb8\xd0\xb8 \xd1\x81 \xd1\x80\xd0\xb0\xd1\x81\xd1\x81\xd1\x82\xd0\xbe\xd1\x8f\xd0\xbd\xd0\xb8\xd0\xb5\xd0\xbc \xd0\x9b\xd0\xb5\xd0\xb2\xd0\xb5\xd0\xbd\xd1\x88\xd1\x82\xd0\xb5\xd0\xb9\xd0\xbd\xd0\xb0\n\n    """"""\n\n    def __init__(self, alphabet, dictionary, operation_costs=None,\n                 allow_spaces=False, euristics=\'none\'):\n        self.alphabet = alphabet\n        self.allow_spaces = allow_spaces\n        if isinstance(euristics, int):\n            if euristics < 0:\n                raise ValueError(""Euristics should be non-negative integer or None"")\n            else:\n                self.euristics = euristics if euristics != 0 else None\n        elif euristics in [""none"", ""None"", None]:\n            self.euristics = None\n        else:\n            raise ValueError(""Euristics should be non-negative integer or None"")\n        if isinstance(dictionary, Trie):\n            # \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x80\xd1\x8c \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb4\xd0\xb0\xd0\xbd \xd1\x83\xd0\xb6\xd0\xb5 \xd0\xb2 \xd0\xb2\xd0\xb8\xd0\xb4\xd0\xb5 \xd0\xb1\xd0\xbe\xd1\x80\xd0\xb0\n            self.dictionary = dictionary\n        else:\n            self.dictionary = make_trie(alphabet, dictionary, make_cashed=True,\n                                        precompute_symbols=self.euristics,\n                                        allow_spaces=self.allow_spaces)\n        self.transducer = SegmentTransducer(\n            alphabet, operation_costs=operation_costs, allow_spaces=allow_spaces)\n        self._precompute_euristics()\n        self._define_h_function()\n\n    def __contains__(self, word):\n        return word in self.dictionary\n\n    def search(self, word, d, allow_spaces=True, return_cost=True):\n        """"""\n        Finds all dictionary words in d-window from word\n        """"""\n        if not all((c in self.alphabet\n                    or (c == "" "" and self.allow_spaces)) for c in word):\n            return []\n            # raise ValueError(""{0} contains an incorrect symbol"".format(word))\n        return self._trie_search(\n            word, d, allow_spaces=allow_spaces, return_cost=return_cost)\n\n    def _trie_search(self, word, d, transducer=None,\n                     allow_spaces=True, return_cost=True):\n        """"""\n        \xd0\x9d\xd0\xb0\xd1\x85\xd0\xbe\xd0\xb4\xd0\xb8\xd1\x82 \xd0\xb2\xd1\x81\xd0\xb5 \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb2\xd0\xb0 \xd0\xb2 \xd0\xbf\xd1\x80\xd0\xb5\xd1\x84\xd0\xb8\xd0\xba\xd1\x81\xd0\xbd\xd0\xbe\xd0\xbc \xd0\xb1\xd0\xbe\xd1\x80\xd0\xb5, \xd1\x80\xd0\xb0\xd1\x81\xd1\x81\xd1\x82\xd0\xbe\xd1\x8f\xd0\xbd\xd0\xb8\xd0\xb5 \xd0\xb4\xd0\xbe \xd0\xba\xd0\xbe\xd1\x82\xd0\xbe\xd1\x80\xd1\x8b\xd1\x85\n        \xd0\xb2 \xd1\x81\xd0\xbe\xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x81\xd1\x82\xd0\xb2\xd0\xb8\xd0\xb8 \xd1\x81 \xd0\xb7\xd0\xb0\xd0\xb4\xd0\xb0\xd0\xbd\xd0\xbd\xd1\x8b\xd0\xbc \xd0\xbf\xd1\x80\xd0\xb5\xd0\xbe\xd0\xb1\xd1\x80\xd0\xb0\xd0\xb7\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x82\xd0\xb5\xd0\xbb\xd0\xb5\xd0\xbc \xd0\xbd\xd0\xb5 \xd0\xbf\xd1\x80\xd0\xb5\xd0\xb2\xd1\x8b\xd1\x88\xd0\xb0\xd0\xb5\xd1\x82 d\n        """"""\n        if transducer is None:\n            # \xd1\x80\xd0\xb0\xd0\xb7\xd0\xbe\xd0\xb1\xd1\x80\xd0\xb0\xd1\x82\xd1\x8c\xd1\x81\xd1\x8f \xd1\x81 \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb1\xd0\xb5\xd0\xbb\xd0\xb0\xd0\xbc\xd0\xb8\n            transducer = self.transducer.inverse()\n        allow_spaces &= self.allow_spaces\n        trie = self.dictionary\n        #  \xd0\xb8\xd0\xbd\xd0\xb8\xd1\x86\xd0\xb8\xd0\xb0\xd0\xbb\xd0\xb8\xd0\xb7\xd0\xb0\xd1\x86\xd0\xb8\xd1\x8f \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xbc\xd0\xb5\xd0\xbd\xd0\xbd\xd1\x8b\xd1\x85\n        used_agenda_keys = set()\n        agenda = SortedListWithKey(key=(lambda x: x[1]))\n        h = self.h_func(word, trie.root)\n        # agenda[self.agenda_key("""", 0, trie.root)] = (0.0, 0.0, h)\n        key, value = ("""", 0, trie.root), (0.0, 0.0, h)\n        agenda.add((key, value))\n        answer = dict()\n        k = 0\n        # \xd0\xbe\xd1\x87\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb4\xd1\x8c \xd1\x81 \xd0\xbf\xd1\x80\xd0\xb8\xd0\xbe\xd1\x80\xd0\xb8\xd1\x82\xd0\xb5\xd1\x82\xd0\xbe\xd0\xbc \xd1\x81 \xd0\xbf\xd1\x80\xd0\xbe\xd0\xbc\xd0\xb5\xd0\xb6\xd1\x83\xd1\x82\xd0\xbe\xd1\x87\xd0\xbd\xd1\x8b\xd0\xbc\xd0\xb8 \xd1\x80\xd0\xb5\xd0\xb7\xd1\x83\xd0\xbb\xd1\x8c\xd1\x82\xd0\xb0\xd1\x82\xd0\xb0\xd0\xbc\xd0\xb8\n        while len(agenda) > 0:\n            key, value = agenda.pop(0)\n            if key in used_agenda_keys:\n                continue\n            used_agenda_keys.add(key)\n            low, pos, index = key\n            cost, g, h = value\n            # g --- \xd1\x82\xd0\xb5\xd0\xba\xd1\x83\xd1\x89\xd0\xb0\xd1\x8f \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd1\x8c, h --- \xd0\xbd\xd0\xb8\xd0\xb6\xd0\xbd\xd1\x8f\xd1\x8f \xd0\xbe\xd1\x86\xd0\xb5\xd0\xbd\xd0\xba\xd0\xb0 \xd0\xb1\xd1\x83\xd0\xb4\xd1\x83\xd1\x89\xd0\xb5\xd0\xb9 \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd0\xb8\n            # cost = g + h --- \xd0\xbd\xd0\xb8\xd0\xb6\xd0\xbd\xd1\x8f\xd1\x8f \xd0\xbe\xd1\x86\xd0\xb5\xd0\xbd\xd0\xba\xd0\xb0 \xd1\x81\xd1\x83\xd0\xbc\xd0\xbc\xd0\xb0\xd1\x80\xd0\xbd\xd0\xbe\xd0\xb9 \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd0\xb8\n            k += 1\n            max_upperside_length = min(len(word) - pos, transducer.max_up_length)\n            for upperside_length in range(max_upperside_length + 1):\n                new_pos = pos + upperside_length\n                curr_up = word[pos: new_pos]\n                if curr_up not in transducer.operation_costs:\n                    continue\n                for curr_low, curr_cost in transducer.operation_costs[curr_up].items():\n                    new_g = g + curr_cost\n                    if new_g > d:  # \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 g > d, \xd1\x82\xd0\xbe h \xd0\xbc\xd0\xbe\xd0\xb6\xd0\xbd\xd0\xbe \xd0\xbd\xd0\xb5 \xd0\xb2\xd1\x8b\xd1\x87\xd0\xb8\xd1\x81\xd0\xbb\xd1\x8f\xd1\x82\xd1\x8c\n                        continue\n                    if curr_low == "" "":\n                        if allow_spaces and trie.is_final(index):\n                            new_index = trie.root\n                        else:\n                            new_index = Trie.NO_NODE\n                    else:\n                        new_index = trie.descend(index, curr_low)\n                    if new_index is Trie.NO_NODE:\n                        continue\n                    new_low = low + curr_low\n                    new_h = self.h_func(word[new_pos:], new_index)\n                    new_cost = new_g + new_h\n                    if new_cost > d:\n                        continue\n                    new_key = (new_low, new_pos, new_index)\n                    new_value = (new_cost, new_g, new_h)\n                    if new_pos == len(word) and trie.is_final(new_index):\n                        old_g = answer.get(new_low, None)\n                        if old_g is None or new_g < old_g:\n                            answer[new_low] = new_g\n                    agenda.add((new_key, new_value))\n        answer = sorted(answer.items(), key=(lambda x: x[1]))\n        if return_cost:\n            return answer\n        else:\n            return [elem[0] for elem in answer]\n\n    def _precompute_euristics(self):\n        """"""\n        \xd0\x9f\xd1\x80\xd0\xb5\xd0\xb4\xd0\xb2\xd1\x8b\xd1\x87\xd0\xb8\xd1\x81\xd0\xbb\xd1\x8f\xd0\xb5\xd1\x82 \xd0\xb1\xd1\x83\xd0\xb4\xd1\x83\xd1\x89\xd0\xb8\xd0\xb5 \xd1\x81\xd0\xb8\xd0\xbc\xd0\xb2\xd0\xbe\xd0\xbb\xd1\x8b \xd0\xb8 \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd0\xb8 \xd0\xbe\xd0\xbf\xd0\xb5\xd1\x80\xd0\xb0\xd1\x86\xd0\xb8\xd0\xb9 \xd1\x81 \xd0\xbd\xd0\xb8\xd0\xbc\xd0\xb8\n        \xd0\xb4\xd0\xbb\xd1\x8f h-\xd1\x8d\xd0\xb2\xd1\x80\xd0\xb8\xd1\x81\xd1\x82\xd0\xb8\xd0\xba\xd0\xb8\n        """"""\n        if self.euristics is None:\n            return\n        # \xd0\xb2\xd1\x8b\xd1\x87\xd0\xb8\xd1\x81\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 \xd0\xbc\xd0\xb8\xd0\xbd\xd0\xb8\xd0\xbc\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xbe\xd0\xb9 \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd0\xb8 \xd0\xbe\xd0\xbf\xd0\xb5\xd1\x80\xd0\xb0\xd1\x86\xd0\xb8\xd0\xb8,\n        # \xd0\xbf\xd1\x80\xd0\xb8\xd0\xb2\xd0\xbe\xd0\xb4\xd1\x8f\xd1\x89\xd0\xb5\xd0\xb9 \xd0\xba \xd0\xbf\xd0\xbe\xd1\x8f\xd0\xb2\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8e (\'+\') \xd0\xb8\xd0\xbb\xd0\xb8 \xd0\xb8\xd1\x81\xd1\x87\xd0\xb5\xd0\xb7\xd0\xbd\xd0\xbe\xd0\xb2\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8e (\'-\') \xd0\xb4\xd0\xb0\xd0\xbd\xd0\xbd\xd0\xbe\xd0\xb3\xd0\xbe \xd1\x81\xd0\xb8\xd0\xbc\xd0\xb2\xd0\xbe\xd0\xbb\xd0\xb0\n        removal_costs = {a: np.inf for a in self.alphabet}\n        insertion_costs = {a: np.inf for a in self.alphabet}\n        if self.allow_spaces:\n            removal_costs[\' \'] = np.inf\n            insertion_costs[\' \'] = np.inf\n        for up, costs in self.transducer.operation_costs.items():\n            for low, cost in costs.items():\n                if up == low:\n                    continue\n                if up != \'\':\n                    removal_cost = cost / len(up)\n                    for a in up:\n                        removal_costs[a] = min(removal_costs[a], removal_cost)\n                if low != \'\':\n                    insertion_cost = cost / len(low)\n                    for a in low:\n                        insertion_costs[a] = min(insertion_costs[a], insertion_cost)\n        # \xd0\xbf\xd1\x80\xd0\xb5\xd0\xb4\xd0\xb2\xd1\x8b\xd1\x87\xd0\xb8\xd1\x81\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 \xd0\xb2\xd0\xbe\xd0\xb7\xd0\xbc\xd0\xbe\xd0\xb6\xd0\xbd\xd1\x8b\xd1\x85 \xd0\xb1\xd1\x83\xd0\xb4\xd1\x83\xd1\x89\xd0\xb8\xd1\x85 \xd1\x81\xd0\xb8\xd0\xbc\xd0\xb2\xd0\xbe\xd0\xbb\xd0\xbe\xd0\xb2 \xd0\xb2 \xd1\x83\xd0\xb7\xd0\xbb\xd0\xb0\xd1\x85 \xd0\xb4\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb2\xd0\xb0\n        # precompute_future_symbols(self.dictionary, self.euristics, self.allow_spaces)\n        # \xd0\xbf\xd1\x80\xd0\xb5\xd0\xb4\xd0\xb2\xd1\x8b\xd1\x87\xd0\xb8\xd1\x81\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd0\xb5\xd0\xb9 \xd0\xbf\xd0\xbe\xd1\x82\xd0\xb5\xd1\x80\xd0\xb8 \xd1\x81\xd0\xb8\xd0\xbc\xd0\xb2\xd0\xbe\xd0\xbb\xd0\xb0 \xd0\xb2 \xd1\x83\xd0\xb7\xd0\xbb\xd0\xb0\xd1\x85 \xd0\xb4\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb2\xd0\xb0\n        self._absense_costs_by_node = _precompute_absense_costs(\n            self.dictionary, removal_costs, insertion_costs,\n            self.euristics, self.allow_spaces)\n        # \xd0\xbc\xd0\xb0\xd1\x81\xd1\x81\xd0\xb8\xd0\xb2 \xd0\xb4\xd0\xbb\xd1\x8f \xd1\x81\xd0\xbe\xd1\x85\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8f \xd1\x8d\xd0\xb2\xd1\x80\xd0\xb8\xd1\x81\xd1\x82\xd0\xb8\xd0\xba\n        self._temporary_euristics = [dict() for i in range(len(self.dictionary))]\n\n    def _define_h_function(self):\n        if self.euristics in [None, 0]:\n            self.h_func = (lambda *x: 0.0)\n        else:\n            self.h_func = self._euristic_h_function\n\n    def _euristic_h_function(self, suffix, index):\n        """"""\n        \xd0\x92\xd1\x8b\xd1\x87\xd0\xb8\xd1\x81\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 h-\xd1\x8d\xd0\xb2\xd1\x80\xd0\xb8\xd1\x81\xd1\x82\xd0\xb8\xd0\xba\xd0\xb8 \xd0\xb8\xd0\xb7 \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd1\x8b Hulden,2009 \xd0\xb4\xd0\xbb\xd1\x8f \xd1\x82\xd0\xb5\xd0\xba\xd1\x83\xd1\x89\xd0\xb5\xd0\xb9 \xd0\xb2\xd0\xb5\xd1\x80\xd1\x88\xd0\xb8\xd0\xbd\xd1\x8b \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x80\xd1\x8f\n\n        \xd0\x90\xd1\x80\xd0\xb3\xd1\x83\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd1\x8b:\n        ----------\n        suffix : string\n            \xd0\xbd\xd0\xb5\xd0\xbf\xd1\x80\xd0\xbe\xd1\x87\xd0\xb8\xd1\x82\xd0\xb0\xd0\xbd\xd0\xbd\xd1\x8b\xd0\xb9 \xd1\x81\xd1\x83\xd1\x84\xd1\x84\xd0\xb8\xd0\xba\xd1\x81 \xd0\xb2\xd1\x85\xd0\xbe\xd0\xb4\xd0\xbd\xd0\xbe\xd0\xb3\xd0\xbe \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb2\xd0\xb0\n        index : int\n            \xd0\xb8\xd0\xbd\xd0\xb4\xd0\xb5\xd0\xba\xd1\x81 \xd1\x82\xd0\xb5\xd0\xba\xd1\x83\xd1\x89\xd0\xb5\xd0\xb3\xd0\xbe \xd1\x83\xd0\xb7\xd0\xbb\xd0\xb0 \xd0\xb2 \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x80\xd0\xb5\n\n        \xd0\x92\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x89\xd0\xb0\xd0\xb5\xd1\x82:\n        -----------\n        cost : float\n            \xd0\xbe\xd1\x86\xd0\xb5\xd0\xbd\xd0\xba\xd0\xb0 \xd1\x81\xd0\xbd\xd0\xb8\xd0\xb7\xd1\x83 \xd0\xb4\xd0\xbb\xd1\x8f \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd0\xb8 \xd0\xb7\xd0\xb0\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x8b,\n            \xd0\xbf\xd1\x80\xd0\xb8\xd0\xb2\xd0\xbe\xd0\xb4\xd1\x8f\xd1\x89\xd0\xb5\xd0\xb9 \xd0\xba \xd0\xb2\xd1\x85\xd0\xbe\xd0\xb4\xd0\xbd\xd0\xbe\xd0\xbc\xd1\x83 \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb2\xd1\x83 \xd1\x81 \xd1\x81\xd1\x83\xd1\x84\xd1\x84\xd0\xb8\xd0\xba\xd1\x81\xd0\xbe\xd0\xbc suffix,\n            \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 \xd0\xbf\xd1\x80\xd0\xbe\xd1\x87\xd0\xb8\xd1\x82\xd0\xb0\xd0\xbd\xd0\xbd\xd1\x8b\xd0\xb9 \xd0\xbf\xd1\x80\xd0\xb5\xd1\x84\xd0\xb8\xd0\xba\xd1\x81 \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb2\xd0\xb0 \xd0\xb1\xd0\xb5\xd0\xb7 \xd0\xbe\xd0\xbf\xd0\xb5\xd1\x87\xd0\xb0\xd1\x82\xd0\xba\xd0\xb8\n            \xd0\xbf\xd1\x80\xd0\xb8\xd0\xb2\xd1\x91\xd0\xbb \xd0\xb2 \xd0\xb2\xd0\xb5\xd1\x80\xd1\x88\xd0\xb8\xd0\xbd\xd1\x83 \xd1\x81 \xd0\xbd\xd0\xbe\xd0\xbc\xd0\xb5\xd1\x80\xd0\xbe\xd0\xbc index\n        """"""\n        if self.euristics > 0:\n            suffix = suffix[:self.euristics]\n        # \xd0\xba\xd1\x8d\xd1\x88\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5 \xd1\x80\xd0\xb5\xd0\xb7\xd1\x83\xd0\xbb\xd1\x8c\xd1\x82\xd0\xb0\xd1\x82\xd0\xbe\xd0\xb2\n        index_temporary_euristics = self._temporary_euristics[index]\n        cost = index_temporary_euristics.get(suffix, None)\n        if cost is not None:\n            return cost\n        # \xd0\xb8\xd0\xb7\xd0\xb2\xd0\xbb\xd0\xb5\xd1\x87\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 \xd0\xbd\xd1\x83\xd0\xb6\xd0\xbd\xd1\x8b\xd1\x85 \xd0\xb4\xd0\xb0\xd0\xbd\xd0\xbd\xd1\x8b\xd1\x85 \xd0\xb8\xd0\xb7 \xd0\xbc\xd0\xb0\xd1\x81\xd1\x81\xd0\xb8\xd0\xb2\xd0\xbe\xd0\xb2\n        absense_costs = self._absense_costs_by_node[index]\n        data = self.dictionary.data[index]\n        costs = np.zeros(dtype=np.float64, shape=(self.euristics,))\n        # costs[j] --- \xd0\xbe\xd1\x86\xd0\xb5\xd0\xbd\xd0\xba\xd0\xb0 \xd1\x88\xd1\x82\xd1\x80\xd0\xb0\xd1\x84\xd0\xb0 \xd0\xbf\xd1\x80\xd0\xb8 \xd0\xbf\xd1\x80\xd0\xb5\xd0\xb4\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd0\xbc\xd0\xbe\xd1\x82\xd1\x80\xd0\xb5 \xd0\xb2\xd0\xbf\xd0\xb5\xd1\x80\xd1\x91\xd0\xb4 \xd0\xbd\xd0\xb0 j \xd1\x81\xd0\xb8\xd0\xbc\xd0\xb2\xd0\xbe\xd0\xbb\xd0\xbe\xd0\xb2\n        for i, a in enumerate(suffix):\n            costs[i:] += absense_costs[a][i:]\n        cost = max(costs)\n        index_temporary_euristics[suffix] = cost\n        return cost\n\n    def _minimal_replacement_cost(self, first, second):\n        first_symbols, second_symbols = set(), set()\n        removal_cost, insertion_cost = 0, 0\n        for a, b in itertools.zip_longest(first, second, fillvalue=None):\n            if a is not None:\n                first_symbols.add(a)\n            if b is not None:\n                second_symbols.add(b)\n            removal_cost = max(removal_cost, len(first_symbols - second_symbols))\n            insertion_cost = max(insertion_cost, len(second_symbols - first_symbols))\n        return min(removal_cost, insertion_cost)\n\n\ndef _precompute_absense_costs(dictionary, removal_costs, insertion_costs, n,\n                              allow_spaces=False):\n    """"""\n    \xd0\x92\xd1\x8b\xd1\x87\xd0\xb8\xd1\x81\xd0\xbb\xd1\x8f\xd0\xb5\xd1\x82 \xd0\xbc\xd0\xb8\xd0\xbd\xd0\xb8\xd0\xbc\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd1\x83\xd1\x8e \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd1\x8c \xd0\xbf\xd0\xbe\xd1\x8f\xd0\xb2\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8f \xd0\xbd\xd0\xbe\xd0\xb2\xd0\xbe\xd0\xb3\xd0\xbe \xd1\x81\xd0\xb8\xd0\xbc\xd0\xb2\xd0\xbe\xd0\xbb\xd0\xb0 \xd0\xb2 \xd1\x83\xd0\xb7\xd0\xbb\xd0\xb0\xd1\x85 \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x80\xd1\x8f\n    \xd0\xb2 \xd1\x81\xd0\xbe\xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x81\xd1\x82\xd0\xb2\xd0\xb8\xd0\xb8 \xd1\x81\xd0\xbe \xd1\x88\xd1\x82\xd1\x80\xd0\xb0\xd1\x84\xd0\xb0\xd0\xbc\xd0\xb8 \xd0\xb8\xd0\xb7 costs\n\n    \xd0\x90\xd1\x80\xd0\xb3\xd1\x83\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd1\x8b:\n    ---------------\n    dictionary : Trie\n        \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x80\xd1\x8c, \xd1\x85\xd1\x80\xd0\xb0\xd0\xbd\xd1\x8f\xd1\x89\xd0\xb8\xd0\xb9\xd1\x81\xd1\x8f \xd0\xb2 \xd0\xb2\xd0\xb8\xd0\xb4\xd0\xb5 \xd0\xb0\xd1\x86\xd0\xb8\xd0\xba\xd0\xbb\xd0\xb8\xd1\x87\xd0\xb5\xd1\x81\xd0\xba\xd0\xbe\xd0\xb3\xd0\xbe \xd0\xb0\xd0\xb2\xd1\x82\xd0\xbe\xd0\xbc\xd0\xb0\xd1\x82\xd0\xb0\n\n    removal_costs : dict\n        \xd1\x88\xd1\x82\xd1\x80\xd0\xb0\xd1\x84\xd1\x8b \xd0\xb7\xd0\xb0 \xd1\x83\xd0\xb4\xd0\xb0\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 \xd1\x81\xd0\xb8\xd0\xbc\xd0\xb2\xd0\xbe\xd0\xbb\xd0\xbe\xd0\xb2\n\n    insertion_costs : dict\n        \xd1\x88\xd1\x82\xd1\x80\xd0\xb0\xd1\x84\xd1\x8b \xd0\xb7\xd0\xb0 \xd0\xb2\xd1\x81\xd1\x82\xd0\xb0\xd0\xb2\xd0\xba\xd1\x83 \xd1\x81\xd0\xb8\xd0\xbc\xd0\xb2\xd0\xbe\xd0\xbb\xd0\xbe\xd0\xb2\n\n    n : int\n        \xd0\xb3\xd0\xbb\xd1\x83\xd0\xb1\xd0\xb8\xd0\xbd\xd0\xb0 ``\xd0\xb7\xd0\xb0\xd0\xb3\xd0\xbb\xd1\x8f\xd0\xb4\xd1\x8b\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x8f \xd0\xb2\xd0\xbf\xd0\xb5\xd1\x80\xd1\x91\xd0\xb4\'\' \xd0\xb2 \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x80\xd0\xb5\n\n    \xd0\x92\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x89\xd0\xb0\xd0\xb5\xd1\x82\n    ---------------\n    answer : list of dicts, len(answer)=len(dictionary)\n        answer[i][a][j] \xd1\x80\xd0\xb0\xd0\xb2\xd0\xbd\xd0\xbe \xd0\xbc\xd0\xb8\xd0\xbd\xd0\xb8\xd0\xbc\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xbe\xd0\xbc\xd1\x83 \xd1\x88\xd1\x82\xd1\x80\xd0\xb0\xd1\x84\xd1\x83 \xd0\xb7\xd0\xb0 \xd0\xbf\xd0\xbe\xd1\x8f\xd0\xb2\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 \xd1\x81\xd0\xb8\xd0\xbc\xd0\xb2\xd0\xbe\xd0\xbb\xd0\xb0 a\n        \xd0\xb2 j-\xd0\xbe\xd0\xb9 \xd0\xbf\xd0\xbe\xd0\xb7\xd0\xb8\xd1\x86\xd0\xb8\xd0\xb8 \xd0\xb2 \xd0\xb2\xd0\xb5\xd1\x80\xd1\x88\xd0\xb8\xd0\xbd\xd0\xb5 \xd1\x81 \xd0\xbd\xd0\xbe\xd0\xbc\xd0\xb5\xd1\x80\xd0\xbe\xd0\xbc i\n    """"""\n    answer = [dict() for node in dictionary.data]\n    if n == 0:\n        return answer\n    curr_alphabet = copy.copy(dictionary.alphabet)\n    if allow_spaces:\n        curr_alphabet += [\' \']\n    for l, (costs_in_node, node) in enumerate(zip(answer, dictionary.data)):\n        # \xd0\xbe\xd0\xbf\xd1\x80\xd0\xb5\xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 \xd0\xbc\xd0\xb8\xd0\xbd\xd0\xb8\xd0\xbc\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xbe\xd0\xb9 \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd0\xb8 \xd1\x83\xd0\xb4\xd0\xb0\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8f \xd1\x81\xd0\xb8\xd0\xbc\xd0\xb2\xd0\xbe\xd0\xbb\xd0\xbe\xd0\xb2\n        curr_node_removal_costs = np.empty(dtype=np.float64, shape=(n,))\n        if len(node[0]) > 0:\n            curr_node_removal_costs[0] = min(removal_costs[symbol] for symbol in node[0])\n            for j, symbols in enumerate(node[1:], 1):\n                if len(symbols) == 0:\n                    curr_node_removal_costs[j:] = curr_node_removal_costs[j - 1]\n                    break\n                curr_cost = min(removal_costs[symbol] for symbol in symbols)\n                curr_node_removal_costs[j] = min(curr_node_removal_costs[j - 1], curr_cost)\n        else:\n            curr_node_removal_costs[:] = np.inf\n        # \xd0\xbe\xd0\xbf\xd1\x80\xd0\xb5\xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 \xd0\xbc\xd0\xb8\xd0\xbd\xd0\xb8\xd0\xbc\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xbe\xd0\xb9 \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd0\xb8 \xd0\xb2\xd1\x81\xd1\x82\xd0\xb0\xd0\xb2\xd0\xba\xd0\xb8\n        for a in curr_alphabet:\n            curr_symbol_costs = np.empty(dtype=np.float64, shape=(n,))\n            curr_symbol_costs.fill(insertion_costs[a])\n            for j, symbols in enumerate(node):\n                if a in symbols:\n                    curr_symbol_costs[j:] = 0.0\n                    break\n                curr_symbol_costs[j] = min(curr_symbol_costs[j], curr_node_removal_costs[j])\n            costs_in_node[a] = curr_symbol_costs\n    return answer\n\n\nclass SegmentTransducer:\n    """"""\n    \xd0\x9a\xd0\xbb\xd0\xb0\xd1\x81\xd1\x81, \xd1\x80\xd0\xb5\xd0\xb0\xd0\xbb\xd0\xb8\xd0\xb7\xd1\x83\xd1\x8e\xd1\x89\xd0\xb8\xd0\xb9 \xd0\xb2\xd0\xb7\xd0\xb2\xd0\xb5\xd1\x88\xd0\xb5\xd0\xbd\xd0\xbd\xd1\x8b\xd0\xb9 \xd0\xba\xd0\xbe\xd0\xbd\xd0\xb5\xd1\x87\xd0\xbd\xd1\x8b\xd0\xb9 \xd0\xbf\xd1\x80\xd0\xb5\xd0\xbe\xd0\xb1\xd1\x80\xd0\xb0\xd0\xb7\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x82\xd0\xb5\xd0\xbb\xd1\x8c,\n    \xd0\xbe\xd1\x81\xd1\x83\xd1\x89\xd0\xb5\xd1\x81\xd1\x82\xd0\xb2\xd0\xbb\xd1\x8f\xd1\x8e\xd1\x89\xd0\xb8\xd0\xb9 \xd0\xb7\xd0\xb0\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x8b \xd0\xb8\xd0\xb7 \xd0\xb7\xd0\xb0\xd0\xb4\xd0\xb0\xd0\xbd\xd0\xbd\xd0\xbe\xd0\xb3\xd0\xbe \xd1\x81\xd0\xbf\xd0\xb8\xd1\x81\xd0\xba\xd0\xb0 \xd0\xbe\xd0\xbf\xd0\xb5\xd1\x80\xd0\xb0\xd1\x86\xd0\xb8\xd0\xb9\n\n    \xd0\x90\xd1\x80\xd0\xb3\xd1\x83\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd1\x8b:\n    ----------\n    alphabet : list\n        \xd0\xb0\xd0\xbb\xd1\x84\xd0\xb0\xd0\xb2\xd0\xb8\xd1\x82\n\n    operation_costs : dict or None(optional, default=None)\n        \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x80\xd1\x8c \xd0\xb2\xd0\xb8\xd0\xb4\xd0\xb0 {(up,low) : cost}\n\n    allow_spaces : bool(optional, default=False)\n        \xd1\x80\xd0\xb0\xd0\xb7\xd1\x80\xd0\xb5\xd1\x88\xd0\xb5\xd0\xbd\xd1\x8b \xd0\xbb\xd0\xb8 \xd1\x8d\xd0\xbb\xd0\xb5\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd1\x8b \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb8, \xd1\x81\xd0\xbe\xd0\xb4\xd0\xb5\xd1\x80\xd0\xb6\xd0\xb0\xd1\x89\xd0\xb8\xd0\xb5 \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb1\xd0\xb5\xd0\xbb\n        (\xd0\xb8\xd1\x81\xd0\xbf\xd0\xbe\xd0\xbb\xd1\x8c\xd0\xb7\xd1\x83\xd0\xb5\xd1\x82\xd1\x81\xd1\x8f \xd1\x82\xd0\xbe\xd0\xbb\xd1\x8c\xd0\xba\xd0\xbe \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 \xd1\x8f\xd0\xb2\xd0\xbd\xd0\xbe \xd0\xbd\xd0\xb5 \xd0\xb7\xd0\xb0\xd0\xb4\xd0\xb0\xd0\xbd\xd1\x8b operation costs\n        \xd0\xb8 \xd0\xbe\xd0\xbd\xd0\xb8 \xd1\x80\xd0\xb0\xd0\xb2\xd0\xbd\xd1\x8b \xd0\xb7\xd0\xbd\xd0\xb0\xd1\x87\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8e \xd0\xbf\xd0\xbe \xd1\x83\xd0\xbc\xd0\xbe\xd0\xbb\xd1\x87\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x8e)\n\n    """"""\n\n    def __init__(self, alphabet, operation_costs=None, allow_spaces=False):\n        self.alphabet = alphabet\n        if operation_costs is None:\n            self._make_default_operation_costs(allow_spaces=allow_spaces)\n        elif not isinstance(operation_costs, dict):\n            raise TypeError(""Operation costs must be a dictionary"")\n        else:\n            self.operation_costs = operation_costs\n        self._make_reversed_operation_costs()\n        self._make_maximal_key_lengths()\n        # self.maximal_value_lengths = {}\n        # for up, probs in self.operation_costs.items():\n        # \xd0\xa1\xd0\x9b\xd0\x98\xd0\xa8\xd0\x9a\xd0\x9e\xd0\x9c \xd0\x9c\xd0\x9d\xd0\x9e\xd0\x93\xd0\x9e \xd0\x92\xd0\xab\xd0\x97\xd0\x9e\xd0\x92\xd0\x9e\xd0\x92, \xd0\x9d\xd0\x90\xd0\x94\xd0\x9e \xd0\x9a\xd0\x90\xd0\x9a-\xd0\xa2\xd0\x9e \xd0\x97\xd0\x90\xd0\x9f\xd0\x9e\xd0\x9c\xd0\x9d\xd0\x98\xd0\xa2\xd0\xac\n        # \xd0\x9c\xd0\x90\xd0\x9a\xd0\xa1\xd0\x98\xd0\x9c\xd0\x90\xd0\x9b\xd0\xac\xd0\x9d\xd0\xab\xd0\x95 \xd0\x94\xd0\x9b\xd0\x98\xd0\x9d\xd0\xab \xd0\x9a\xd0\x9b\xd0\xae\xd0\xa7\xd0\x95\xd0\x99 \xd0\x9f\xd0\xa0\xd0\x98 \xd0\x9e\xd0\x91\xd0\xa0\xd0\x90\xd0\xa9\xd0\x95\xd0\x9d\xd0\x98\xd0\x98\n        # max_low_length = max(len(low) for low in probs) if (len(probs) > 0) else -1\n        # self.maximal_value_lengths[up] = self.maximal_key_length\n\n    def get_operation_cost(self, up, low):\n        """"""\n        \xd0\x92\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x89\xd0\xb0\xd0\xb5\xd1\x82 \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd1\x8c \xd1\x8d\xd0\xbb\xd0\xb5\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd0\xb0\xd1\x80\xd0\xbd\xd0\xbe\xd0\xb9 \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb8 up->low\n        \xd0\xb8\xd0\xbb\xd0\xb8 np.inf, \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 \xd1\x82\xd0\xb0\xd0\xba\xd0\xbe\xd0\xb9 \xd1\x8d\xd0\xbb\xd0\xb5\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd0\xb0\xd1\x80\xd0\xbd\xd0\xbe\xd0\xb9 \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb8 \xd0\xbd\xd0\xb5\xd1\x82\n\n        \xd0\x90\xd1\x80\xd0\xb3\xd1\x83\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd1\x8b:\n        ----------\n        up, low : string\n            \xd1\x8d\xd0\xbb\xd0\xb5\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd1\x8b \xd1\x8d\xd0\xbb\xd0\xb5\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd0\xb0\xd1\x80\xd0\xbd\xd0\xbe\xd0\xb9 \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb8\n\n        \xd0\x92\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x89\xd0\xb0\xd0\xb5\xd1\x82:\n        -----------\n        cost : float\n            \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd1\x8c \xd1\x8d\xd0\xbb\xd0\xb5\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd0\xb0\xd1\x80\xd0\xbd\xd0\xbe\xd0\xb9 \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb8 up->low\n            (np.inf, \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 \xd1\x82\xd0\xb0\xd0\xba\xd0\xb0\xd1\x8f \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd1\x8f \xd0\xbe\xd1\x82\xd1\x81\xd1\x83\xd1\x82\xd1\x81\xd1\x82\xd0\xb2\xd1\x83\xd0\xb5\xd1\x82)\n        """"""\n        up_costs = self.operation_costs.get(up, None)\n        if up_costs is None:\n            return np.inf\n        cost = up_costs.get(low, np.inf)\n        return cost\n\n    def inverse(self):\n        """"""\n        \xd0\xa1\xd1\x82\xd1\x80\xd0\xbe\xd0\xb8\xd1\x82 \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb1\xd1\x80\xd0\xb0\xd0\xb7\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x82\xd0\xb5\xd0\xbb\xd1\x8c, \xd0\xb7\xd0\xb0\xd0\xb4\xd0\xb0\xd1\x8e\xd1\x89\xd0\xb8\xd0\xb9 \xd0\xbe\xd0\xb1\xd1\x80\xd0\xb0\xd1\x82\xd0\xbd\xd0\xbe\xd0\xb5 \xd0\xba\xd0\xbe\xd0\xbd\xd0\xb5\xd1\x87\xd0\xbd\xd0\xbe\xd0\xb5 \xd0\xbf\xd1\x80\xd0\xb5\xd0\xbe\xd0\xb1\xd1\x80\xd0\xb0\xd0\xb7\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5\n        """"""\n        # \xd0\xa3\xd0\x9f\xd0\xa0\xd0\x9e\xd0\xa1\xd0\xa2\xd0\x98\xd0\xa2\xd0\xac \xd0\x9e\xd0\x91\xd0\xa0\xd0\x90\xd0\xa9\xd0\x95\xd0\x9d\xd0\x98\xd0\x95!!!\n        inversed_transducer = SegmentTransducer(self.alphabet, operation_costs=dict())\n        inversed_transducer.operation_costs = self._reversed_operation_costs\n        inversed_transducer._reversed_operation_costs = self.operation_costs\n        inversed_transducer.max_low_length = self.max_up_length\n        inversed_transducer.max_up_length = self.max_low_length\n        inversed_transducer.max_low_lengths_by_up = self.max_up_lengths_by_low\n        inversed_transducer.max_up_lengths_by_low = self.max_low_lengths_by_up\n        return inversed_transducer\n\n    def distance(self, first, second, return_transduction=False):\n        """"""\n        \xd0\x92\xd1\x8b\xd1\x87\xd0\xb8\xd1\x81\xd0\xbb\xd1\x8f\xd0\xb5\xd1\x82 \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd1\x8e \xd0\xbc\xd0\xb8\xd0\xbd\xd0\xb8\xd0\xbc\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xbe\xd0\xb9 \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd0\xb8,\n        \xd0\xbe\xd1\x82\xd0\xbe\xd0\xb1\xd1\x80\xd0\xb0\xd0\xb6\xd0\xb0\xd1\x8e\xd1\x89\xd1\x83\xd1\x8e first \xd0\xb2 second\n\n        \xd0\x90\xd1\x80\xd0\xb3\xd1\x83\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd1\x8b:\n        -----------\n        first : string\n        second : string\n            \xd0\x92\xd0\xb5\xd1\x80\xd1\x85\xd0\xbd\xd0\xb8\xd0\xb9 \xd0\xb8 \xd0\xbd\xd0\xb8\xd0\xb6\xd0\xbd\xd0\xb8\xd0\xb9 \xd1\x8d\xd0\xbb\xd0\xb5\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd1\x8b \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb8\n\n        return_transduction : bool (optional, default=False)\n            \xd1\x81\xd0\xbb\xd0\xb5\xd0\xb4\xd1\x83\xd0\xb5\xd1\x82 \xd0\xbb\xd0\xb8 \xd0\xb2\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x89\xd0\xb0\xd1\x82\xd1\x8c \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd1\x8e \xd0\xbc\xd0\xb8\xd0\xbd\xd0\xb8\xd0\xbc\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xbe\xd0\xb3\xd0\xbe \xd0\xb2\xd0\xb5\xd1\x81\xd0\xb0\n            (\xd1\x81\xd0\xbc. \xd0\xb2\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x89\xd0\xb0\xd0\xb5\xd0\xbc\xd0\xbe\xd0\xb5 \xd0\xb7\xd0\xbd\xd0\xb0\xd1\x87\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5)\n\n        \xd0\x92\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x89\xd0\xb0\xd0\xb5\xd1\x82:\n        -----------\n        (final_cost, transductions) : tuple(float, list)\n            \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 return_transduction=True, \xd1\x82\xd0\xbe \xd0\xb2\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x89\xd0\xb0\xd0\xb5\xd1\x82\n            \xd0\xbc\xd0\xb8\xd0\xbd\xd0\xb8\xd0\xbc\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd1\x83\xd1\x8e \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd1\x8c \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb8, \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb2\xd0\xbe\xd0\xb4\xd1\x8f\xd1\x89\xd0\xb5\xd0\xb9 first \xd0\xb2 second\n            \xd0\xb8 \xd1\x81\xd0\xbf\xd0\xb8\xd1\x81\xd0\xbe\xd0\xba \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb9 \xd1\x81 \xd0\xb4\xd0\xb0\xd0\xbd\xd0\xbd\xd0\xbe\xd0\xb9 \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd1\x8c\xd1\x8e\n\n        final_cost : float\n            \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 return_transduction=False, \xd1\x82\xd0\xbe \xd0\xb2\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x89\xd0\xb0\xd0\xb5\xd1\x82\n            \xd0\xbc\xd0\xb8\xd0\xbd\xd0\xb8\xd0\xbc\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd1\x83\xd1\x8e \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd1\x8c \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb8, \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb2\xd0\xbe\xd0\xb4\xd1\x8f\xd1\x89\xd0\xb5\xd0\xb9 first \xd0\xb2 second\n        """"""\n        if return_transduction:\n            add_pred = (lambda x, y: (y == np.inf or x < y))\n        else:\n            add_pred = (lambda x, y: (y == np.inf or x <= y))\n        clear_pred = (lambda x, y: x < y < np.inf)\n        update_func = lambda x, y: min(x, y)\n        costs, backtraces = self._fill_levenshtein_table(first, second,\n                                                         update_func, add_pred, clear_pred)\n        final_cost = costs[-1][-1]\n        if final_cost == np.inf:\n            transductions = [None]\n        elif return_transduction:\n            transductions = self._backtraces_to_transductions(first, second, backtraces,\n                                                              final_cost, return_cost=False)\n        if return_transduction:\n            return final_cost, transductions\n        else:\n            return final_cost\n\n    def transduce(self, first, second, threshold):\n        """"""\n        \xd0\x92\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x89\xd0\xb0\xd0\xb5\xd1\x82 \xd0\xb2\xd1\x81\xd0\xb5 \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb8, \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb2\xd0\xbe\xd0\xb4\xd1\x8f\xd1\x89\xd0\xb8\xd0\xb5 first \xd0\xb2 second,\n        \xd1\x87\xd1\x8c\xd1\x8f \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd1\x8c \xd0\xbd\xd0\xb5 \xd0\xbf\xd1\x80\xd0\xb5\xd0\xb2\xd1\x8b\xd1\x88\xd0\xb0\xd0\xb5\xd1\x82 threshold\n\n        \xd0\x92\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x89\xd0\xb0\xd0\xb5\xd1\x82:\n        ----------\n        result : list\n            \xd1\x81\xd0\xbf\xd0\xb8\xd1\x81\xd0\xbe\xd0\xba \xd0\xb2\xd0\xb8\xd0\xb4\xd0\xb0 [(\xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd1\x8f, \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd1\x8c)]\n        """"""\n        add_pred = (lambda x, y: x <= threshold)\n        clear_pred = (lambda x, y: False)\n        update_func = (lambda x, y: min(x, y))\n        costs, backtraces = self._fill_levenshtein_table(first, second,\n                                                         update_func, add_pred, clear_pred,\n                                                         threshold=threshold)\n        result = self._backtraces_to_transductions(first, second,\n                                                   backtraces, threshold, return_cost=True)\n        return result\n\n    def lower_transductions(self, word, max_cost, return_cost=True):\n        """"""\n        \xd0\x92\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x89\xd0\xb0\xd0\xb5\xd1\x82 \xd0\xb2\xd1\x81\xd0\xb5 \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb8 \xd1\x81 \xd0\xb2\xd0\xb5\xd1\x80\xd1\x85\xd0\xbd\xd0\xb8\xd0\xbc \xd1\x8d\xd0\xbb\xd0\xb5\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd0\xbe\xd0\xbc word,\n        \xd1\x87\xd1\x8c\xd1\x8f \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd1\x8c \xd0\xbd\xd0\xb5 \xd0\xbf\xd1\x80\xd0\xb5\xd0\xb2\xd1\x8b\xd1\x88\xd0\xb0\xd0\xb5\xd1\x82 max_cost\n\n    `   \xd0\x92\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x89\xd0\xb0\xd0\xb5\xd1\x82:\n        ----------\n        result : list\n            \xd1\x81\xd0\xbf\xd0\xb8\xd1\x81\xd0\xbe\xd0\xba \xd0\xb2\xd0\xb8\xd0\xb4\xd0\xb0 [(\xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd1\x8f, \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd1\x8c)], \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 return_cost=True\n            \xd1\x81\xd0\xbf\xd0\xb8\xd1\x81\xd0\xbe\xd0\xba \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb9, \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 return_cost=False\n            \xd1\x81\xd0\xbf\xd0\xb8\xd1\x81\xd0\xbe\xd0\xba \xd0\xbe\xd1\x82\xd1\x81\xd0\xbe\xd1\x80\xd1\x82\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd \xd0\xb2 \xd0\xbf\xd0\xbe\xd1\x80\xd1\x8f\xd0\xb4\xd0\xba\xd0\xb5 \xd0\xb2\xd0\xbe\xd0\xb7\xd1\x80\xd0\xb0\xd1\x81\xd1\x82\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x8f \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd0\xb8 \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb8\n        """"""\n        prefixes = [[] for i in range(len(word) + 1)]\n        prefixes[0].append(((), 0.0))\n        for pos in range(len(prefixes)):\n            # \xd0\xb2\xd1\x81\xd1\x82\xd0\xb0\xd0\xb2\xd0\xba\xd0\xb8\n            prefixes[pos] = self._perform_insertions(prefixes[pos], max_cost)\n            max_upperside_length = min(len(word) - pos, self.max_up_length)\n            for upperside_length in range(1, max_upperside_length + 1):\n                up = word[pos: pos + upperside_length]\n                for low, low_cost in self.operation_costs.get(up, dict()).items():\n                    for transduction, cost in prefixes[pos]:\n                        new_cost = cost + low_cost\n                        if new_cost <= max_cost:\n                            new_transduction = transduction + (up, low)\n                            prefixes[pos + upperside_length].append((new_transduction, new_cost))\n        answer = sorted(prefixes[-1], key=(lambda x: x[0]))\n        if return_cost:\n            return answer\n        else:\n            return [elem[0] for elem in answer]\n\n    def lower(self, word, max_cost, return_cost=True):\n        transductions = self.lower_transductions(word, max_cost, return_cost=True)\n        answer = dict()\n        for transduction, cost in transductions:\n            low = """".join(elem[1] for elem in transductions)\n            curr_cost = answer.get(low, None)\n            if curr_cost is None or cost < curr_cost:\n                answer[low] = cost\n        answer = sorted(answer.items(), key=(lambda x: x[1]))\n        if return_cost:\n            return answer\n        else:\n            return [elem[0] for elem in answer]\n\n    def upper(self, word, max_cost, return_cost=True):\n        inversed_transducer = self.inverse()\n        return inversed_transducer.lower(word, max_cost, return_cost)\n\n    def upper_transductions(self, word, max_cost, return_cost=True):\n        inversed_transducer = self.inverse()\n        return inversed_transducer.lower_transductions(word, max_cost, return_cost)\n\n    def _fill_levenshtein_table(self, first, second, update_func, add_pred, clear_pred,\n                                threshold=None):\n        """"""\n        \xd0\xa4\xd1\x83\xd0\xbd\xd0\xba\xd1\x86\xd0\xb8\xd1\x8f, \xd0\xb4\xd0\xb8\xd0\xbd\xd0\xb0\xd0\xbc\xd0\xb8\xd1\x87\xd0\xb5\xd1\x81\xd0\xba\xd0\xb8 \xd0\xb7\xd0\xb0\xd0\xbf\xd0\xbe\xd0\xbb\xd0\xbd\xd1\x8f\xd1\x8e\xd1\x89\xd0\xb0\xd1\x8f \xd1\x82\xd0\xb0\xd0\xb1\xd0\xbb\xd0\xb8\xd1\x86\xd1\x83 costs \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd0\xb8 \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb9,\n        costs[i][j] --- \xd0\xbc\xd0\xb8\xd0\xbd\xd0\xb8\xd0\xbc\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xb0\xd1\x8f \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd1\x8c \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb8,\n        \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb2\xd0\xbe\xd0\xb4\xd1\x8f\xd1\x89\xd0\xb5\xd0\xb9 first[:i] \xd0\xb2 second[:j]\n\n        \xd0\x90\xd1\x80\xd0\xb3\xd1\x83\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd1\x8b:\n        ----------\n        first, second : string\n            \xd0\x92\xd0\xb5\xd1\x80\xd1\x85\xd0\xbd\xd0\xb8\xd0\xb9 \xd0\xb8 \xd0\xbd\xd0\xb8\xd0\xb6\xd0\xbd\xd0\xb8\xd0\xb9 \xd1\x8d\xd0\xbb\xd0\xb5\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd1\x8b \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb8\n        update_func : callable, float*float -> bool\n            update_func(x, y) \xd0\xb2\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x89\xd0\xb0\xd0\xb5\xd1\x82 \xd0\xbd\xd0\xbe\xd0\xb2\xd0\xbe\xd0\xb5 \xd0\xb7\xd0\xbd\xd0\xb0\xd1\x87\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 \xd0\xb2 \xd1\x8f\xd1\x87\xd0\xb5\xd0\xb9\xd0\xba\xd0\xb5 \xd1\x82\xd0\xb0\xd0\xb1\xd0\xbb\xd0\xb8\xd1\x86\xd1\x8b costs,\n            \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 \xd1\x81\xd1\x82\xd0\xb0\xd1\x80\xd0\xbe\xd0\xb5 \xd0\xb7\xd0\xbd\xd0\xb0\xd1\x87\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 --- y, \xd0\xb0 \xd0\xbf\xd0\xbe\xd1\x82\xd0\xb5\xd0\xbd\xd1\x86\xd0\xb8\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xbe \xd0\xbd\xd0\xbe\xd0\xb2\xd0\xbe\xd0\xb5 \xd0\xb7\xd0\xbd\xd0\xb0\xd1\x87\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 --- x\n            \xd0\xb2\xd0\xb5\xd0\xb7\xd0\xb4\xd0\xb5 update_func = min\n        add_pred : callable : float*float -> bool\n            add_pred(x, y) \xd0\xb2\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x89\xd0\xb0\xd0\xb5\xd1\x82, \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb8\xd0\xb7\xd0\xb2\xd0\xbe\xd0\xb4\xd0\xb8\xd1\x82\xd1\x81\xd1\x8f \xd0\xbb\xd0\xb8 \xd0\xb4\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5\n            \xd0\xbd\xd0\xbe\xd0\xb2\xd0\xbe\xd0\xb3\xd0\xbe \xd1\x8d\xd0\xbb\xd0\xb5\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd0\xb0 p \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd0\xb8 x \xd0\xb2 \xd1\x8f\xd1\x87\xd0\xb5\xd0\xb9\xd0\xba\xd1\x83 backtraces[i][j]\n            \xd0\xb2 \xd0\xb7\xd0\xb0\xd0\xb2\xd0\xb8\xd1\x81\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd0\xb8 \xd0\xbe\xd1\x82 \xd0\xb7\xd0\xbd\xd0\xb0\xd1\x87\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8f costs[i][j]=y \xd0\xb8 \xd1\x82\xd0\xb5\xd0\xba\xd1\x83\xd1\x89\xd0\xb5\xd0\xb9 \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd0\xb8 x\n        clear_pred : callable : float*float -> bool\n            clear_pred(x, y) \xd0\xb2\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x89\xd0\xb0\xd0\xb5\xd1\x82, \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb8\xd0\xb7\xd0\xb2\xd0\xbe\xd0\xb4\xd0\xb8\xd1\x82\xd1\x81\xd1\x8f \xd0\xbb\xd0\xb8 \xd0\xbe\xd1\x87\xd0\xb8\xd1\x81\xd1\x82\xd0\xba\xd0\xb0\n            \xd1\x8f\xd1\x87\xd0\xb5\xd0\xb9\xd0\xba\xd0\xb8 backtraces[i][j] \xd0\xb2 \xd0\xb7\xd0\xb0\xd0\xb2\xd0\xb8\xd1\x81\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd0\xb8 \xd0\xbe\xd1\x82 \xd0\xb7\xd0\xbd\xd0\xb0\xd1\x87\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8f costs[i][j]=y\n            \xd0\xb8 \xd1\x82\xd0\xb5\xd0\xba\xd1\x83\xd1\x89\xd0\xb5\xd0\xb9 \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd0\xb8 x \xd1\x8d\xd0\xbb\xd0\xb5\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd0\xb0 p, \xd0\xb4\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xbb\xd1\x8f\xd0\xb5\xd0\xbc\xd0\xbe\xd0\xb3\xd0\xbe \xd0\xb2 \xd1\x8d\xd1\x82\xd1\x83 \xd1\x8f\xd1\x87\xd0\xb5\xd0\xb9\xd0\xba\xd1\x83\n\n        \xd0\x92\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x89\xd0\xb0\xd0\xb5\xd1\x82:\n        -----------\n        costs : array, dtype=float, shape=(len(first)+1, len(second)+1)\n            \xd0\xbc\xd0\xb0\xd1\x81\xd1\x81\xd0\xb8\xd0\xb2, \xd0\xb2 \xd1\x8f\xd1\x87\xd0\xb5\xd0\xb9\xd0\xba\xd0\xb5 \xd1\x81 \xd0\xb8\xd0\xbd\xd0\xb4\xd0\xb5\xd0\xba\xd1\x81\xd0\xb0\xd0\xbc\xd0\xb8 i, j \xd0\xba\xd0\xbe\xd1\x82\xd0\xbe\xd1\x80\xd0\xbe\xd0\xb3\xd0\xbe \xd1\x85\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x82\xd1\x81\xd1\x8f\n            \xd0\xbc\xd0\xb8\xd0\xbd\xd0\xb8\xd0\xbc\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xb0\xd1\x8f \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd1\x8c \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb8, \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb2\xd0\xbe\xd0\xb4\xd1\x8f\xd1\x89\xd0\xb5\xd0\xb9 first[:i] \xd0\xb2 second[:j]\n        backtraces : array, dtype=list, shape=(len(first)+1, len(second)+1)\n            \xd0\xbc\xd0\xb0\xd1\x81\xd1\x81\xd0\xb8\xd0\xb2, \xd0\xb2 \xd1\x8f\xd1\x87\xd0\xb5\xd0\xb9\xd0\xba\xd0\xb5 \xd1\x81 \xd0\xb8\xd0\xbd\xd0\xb4\xd0\xb5\xd0\xba\xd1\x81\xd0\xb0\xd0\xbc\xd0\xb8 i, j \xd0\xba\xd0\xbe\xd1\x82\xd0\xbe\xd1\x80\xd0\xbe\xd0\xb3\xd0\xbe \xd1\x85\xd1\x80\xd0\xb0\xd0\xbd\xd1\x8f\xd1\x82\xd1\x81\xd1\x8f\n            \xd0\xbe\xd0\xb1\xd1\x80\xd0\xb0\xd1\x82\xd0\xbd\xd1\x8b\xd0\xb5 \xd1\x81\xd1\x81\xd1\x8b\xd0\xbb\xd0\xba\xd0\xb8 \xd0\xbd\xd0\xb0 \xd0\xbf\xd1\x80\xd0\xb5\xd0\xb4\xd1\x8b\xd0\xb4\xd1\x83\xd1\x89\xd1\x83\xd1\x8e \xd1\x8f\xd1\x87\xd0\xb5\xd0\xb9\xd0\xba\xd1\x83 \xd0\xb2 \xd0\xbe\xd0\xbf\xd1\x82\xd0\xb8\xd0\xbc\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xbe\xd0\xb9 \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb8,\n            \xd0\xbf\xd1\x80\xd0\xb8\xd0\xb2\xd0\xbe\xd0\xb4\xd1\x8f\xd1\x89\xd0\xb5\xd0\xb9 \xd0\xb2 \xd1\x8f\xd1\x87\xd0\xb5\xd0\xb9\xd0\xba\xd1\x83 backtraces[i][j]\n        """"""\n        m, n = len(first), len(second)\n        # \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 threshold=None, \xd1\x82\xd0\xbe \xd0\xb2 \xd0\xba\xd0\xb0\xd1\x87\xd0\xb5\xd1\x81\xd1\x82\xd0\xb2\xd0\xb5 \xd0\xbf\xd0\xbe\xd1\x80\xd0\xbe\xd0\xb3\xd0\xb0 \xd0\xb1\xd0\xb5\xd1\x80\xd1\x91\xd1\x82\xd1\x81\xd1\x8f \xd1\x83\xd0\xb4\xd0\xb2\xd0\xbe\xd0\xb5\xd0\xbd\xd0\xbd\xd0\xb0\xd1\x8f \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd1\x8c\n        # \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb8, \xd0\xbe\xd1\x82\xd0\xbe\xd0\xb1\xd1\x80\xd0\xb0\xd0\xb6\xd0\xb0\xd1\x8e\xd1\x89\xd0\xb5\xd0\xb9 \xd1\x81\xd0\xb8\xd0\xbc\xd0\xb2\xd0\xbe\xd0\xbb\xd1\x8b \xd0\xbd\xd0\xb0 \xd0\xbe\xd0\xb4\xd0\xb8\xd0\xbd\xd0\xb0\xd0\xba\xd0\xbe\xd0\xb2\xd1\x8b\xd1\x85 \xd0\xbf\xd0\xbe\xd0\xb7\xd0\xb8\xd1\x86\xd0\xb8\xd1\x8f\xd1\x85 \xd0\xb4\xd1\x80\xd1\x83\xd0\xb3 \xd0\xb2 \xd0\xb4\xd1\x80\xd1\x83\xd0\xb3\xd0\xb0\n        if threshold is None:\n            threshold = 0.0\n            for a, b in zip(first, second):\n                threshold += self.get_operation_cost(a, b)\n            if m > n:\n                for a in first[n:]:\n                    threshold += self.get_operation_cost(a, \'\')\n            elif m < n:\n                for b in second[m:]:\n                    threshold += self.get_operation_cost(\'\', b)\n            threshold *= 2\n        # \xd0\xb8\xd0\xbd\xd0\xb8\xd1\x86\xd0\xb8\xd0\xb0\xd0\xbb\xd0\xb8\xd0\xb7\xd0\xb0\xd1\x86\xd0\xb8\xd1\x8f \xd0\xb2\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x89\xd0\xb0\xd0\xb5\xd0\xbc\xd1\x8b\xd1\x85 \xd0\xbc\xd0\xb0\xd1\x81\xd1\x81\xd0\xb8\xd0\xb2\xd0\xbe\xd0\xb2\n        costs = np.zeros(shape=(m + 1, n + 1), dtype=np.float64)\n        costs[:] = np.inf\n        backtraces = [None] * (m + 1)\n        for i in range(m + 1):\n            backtraces[i] = [[] for j in range(n + 1)]\n        costs[0][0] = 0.0\n        for i in range(m + 1):\n            for i_right in range(i, min(i + self.max_up_length, m) + 1):\n                up = first[i: i_right]\n                max_low_length = self.max_low_lengths_by_up.get(up, -1)\n                if max_low_length == -1:  # no up key in transduction\n                    continue\n                up_costs = self.operation_costs[up]\n                for j in range(n + 1):\n                    if costs[i][j] > threshold:\n                        continue\n                    if len(backtraces[i][j]) == 0 and i + j > 0:\n                        continue  # \xd0\xbd\xd0\xb5 \xd0\xbd\xd0\xb0\xd1\x88\xd0\xbb\xd0\xbe\xd1\x81\xd1\x8c \xd0\xbe\xd0\xb1\xd1\x80\xd0\xb0\xd1\x82\xd0\xbd\xd1\x8b\xd1\x85 \xd1\x81\xd1\x81\xd1\x8b\xd0\xbb\xd0\xbe\xd0\xba\n                    for j_right in range((j if i_right > i else j + 1),\n                                         min(j + max_low_length, n) + 1):\n                        low = second[j: j_right]\n                        curr_cost = up_costs.get(low, np.inf)\n                        old_cost = costs[i_right][j_right]\n                        new_cost = costs[i][j] + curr_cost\n                        if new_cost > threshold:\n                            continue\n                        if add_pred(new_cost, old_cost):\n                            if clear_pred(new_cost, old_cost):\n                                backtraces[i_right][j_right] = []\n                            costs[i_right][j_right] = update_func(new_cost, old_cost)\n                            backtraces[i_right][j_right].append((i, j))\n        return costs, backtraces\n\n    def _make_reversed_operation_costs(self):\n        """"""\n        \xd0\x97\xd0\xb0\xd0\xbf\xd0\xbe\xd0\xbb\xd0\xbd\xd1\x8f\xd0\xb5\xd1\x82 \xd0\xbc\xd0\xb0\xd1\x81\xd1\x81\xd0\xb8\xd0\xb2 _reversed_operation_costs\n        \xd0\xbd\xd0\xb0 \xd0\xbe\xd1\x81\xd0\xbd\xd0\xbe\xd0\xb2\xd0\xb5 \xd0\xb8\xd0\xbc\xd0\xb5\xd1\x8e\xd1\x89\xd0\xb5\xd0\xb3\xd0\xbe\xd1\x81\xd1\x8f \xd0\xbc\xd0\xb0\xd1\x81\xd1\x81\xd0\xb8\xd0\xb2\xd0\xb0 operation_costs\n        """"""\n        _reversed_operation_costs = dict()\n        for up, costs in self.operation_costs.items():\n            for low, cost in costs.items():\n                if low not in _reversed_operation_costs:\n                    _reversed_operation_costs[low] = dict()\n                _reversed_operation_costs[low][up] = cost\n        self._reversed_operation_costs = _reversed_operation_costs\n\n    def _make_maximal_key_lengths(self):\n        """"""\n        \xd0\x92\xd1\x8b\xd1\x87\xd0\xb8\xd1\x81\xd0\xbb\xd1\x8f\xd0\xb5\xd1\x82 \xd0\xbc\xd0\xb0\xd0\xba\xd1\x81\xd0\xb8\xd0\xbc\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd1\x83\xd1\x8e \xd0\xb4\xd0\xbb\xd0\xb8\xd0\xbd\xd1\x83 \xd1\x8d\xd0\xbb\xd0\xb5\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd0\xb0 low\n        \xd0\xb2 \xd1\x8d\xd0\xbb\xd0\xb5\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd0\xb0\xd1\x80\xd0\xbd\xd0\xbe\xd0\xb9 \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb8 (up, low) \xd0\xb4\xd0\xbb\xd1\x8f \xd0\xba\xd0\xb0\xd0\xb6\xd0\xb4\xd0\xbe\xd0\xb3\xd0\xbe up\n        \xd0\xb8 \xd0\xbc\xd0\xb0\xd0\xba\xd1\x81\xd0\xb8\xd0\xbc\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd1\x83\xd1\x8e \xd0\xb4\xd0\xbb\xd0\xb8\xd0\xbd\xd1\x83 \xd1\x8d\xd0\xbb\xd0\xb5\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd0\xb0 up\n        \xd0\xb2 \xd1\x8d\xd0\xbb\xd0\xb5\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd0\xb0\xd1\x80\xd0\xbd\xd0\xbe\xd0\xb9 \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb8 (up, low) \xd0\xb4\xd0\xbb\xd1\x8f \xd0\xba\xd0\xb0\xd0\xb6\xd0\xb4\xd0\xbe\xd0\xb3\xd0\xbe low\n        """"""\n        self.max_up_length = \\\n            (max(len(up) for up in self.operation_costs)\n             if len(self.operation_costs) > 0 else -1)\n        self.max_low_length = \\\n            (max(len(low) for low in self._reversed_operation_costs)\n             if len(self._reversed_operation_costs) > 0 else -1)\n        self.max_low_lengths_by_up, self.max_up_lengths_by_low = dict(), dict()\n        for up, costs in self.operation_costs.items():\n            self.max_low_lengths_by_up[up] = \\\n                max(len(low) for low in costs) if len(costs) > 0 else -1\n        for low, costs in self._reversed_operation_costs.items():\n            self.max_up_lengths_by_low[low] = \\\n                max(len(up) for up in costs) if len(costs) > 0 else -1\n\n    def _backtraces_to_transductions(self, first, second, backtraces, threshold, return_cost=False):\n        """"""\n        \xd0\x92\xd0\xbe\xd1\x81\xd1\x81\xd1\x82\xd0\xb0\xd0\xbd\xd0\xb0\xd0\xb2\xd0\xbb\xd0\xb8\xd0\xb2\xd0\xb0\xd0\xb5\xd1\x82 \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb8 \xd0\xbf\xd0\xbe \xd1\x82\xd0\xb0\xd0\xb1\xd0\xbb\xd0\xb8\xd1\x86\xd0\xb5 \xd0\xbe\xd0\xb1\xd1\x80\xd0\xb0\xd1\x82\xd0\xbd\xd1\x8b\xd1\x85 \xd1\x81\xd1\x81\xd1\x8b\xd0\xbb\xd0\xbe\xd0\xba\n\n        \xd0\x90\xd1\x80\xd0\xb3\xd1\x83\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd1\x8b:\n        ----------\n        first, second : string\n            \xd0\xb2\xd0\xb5\xd1\x80\xd1\x85\xd0\xbd\xd0\xb8\xd0\xb5 \xd0\xb8 \xd0\xbd\xd0\xb8\xd0\xb6\xd0\xbd\xd0\xb8\xd0\xb5 \xd1\x8d\xd0\xbb\xd0\xb5\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd1\x8b \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb8\n        backtraces : array-like, dtype=list, shape=(len(first)+1, len(second)+1)\n            \xd1\x82\xd0\xb0\xd0\xb1\xd0\xbb\xd0\xb8\xd1\x86\xd0\xb0 \xd0\xbe\xd0\xb1\xd1\x80\xd0\xb0\xd1\x82\xd0\xbd\xd1\x8b\xd1\x85 \xd1\x81\xd1\x81\xd1\x8b\xd0\xbb\xd0\xbe\xd0\xba\n        threshold : float\n            \xd0\xbf\xd0\xbe\xd1\x80\xd0\xbe\xd0\xb3 \xd0\xb4\xd0\xbb\xd1\x8f \xd0\xbe\xd1\x82\xd1\x81\xd0\xb5\xd0\xb2\xd0\xb0 \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb9,\n            \xd0\xb2\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x89\xd0\xb0\xd1\x8e\xd1\x82\xd1\x81\xd1\x8f \xd1\x82\xd0\xbe\xd0\xbb\xd1\x8c\xd0\xba\xd0\xbe \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb8 \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd1\x8c\xd1\x8e <= threshold\n        return_cost : bool (optional, default=False)\n            \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 True, \xd1\x82\xd0\xbe \xd0\xb2\xd0\xbc\xd0\xb5\xd1\x81\xd1\x82\xd0\xb5 \xd1\x81 \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd1\x8f\xd0\xbc\xd0\xb8 \xd0\xb2\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x89\xd0\xb0\xd0\xb5\xd1\x82\xd1\x81\xd1\x8f \xd0\xb8\xd1\x85 \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd1\x8c\n\n        \xd0\x92\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x89\xd0\xb0\xd0\xb5\xd1\x82:\n        -----------\n        result : list\n            \xd1\x81\xd0\xbf\xd0\xb8\xd1\x81\xd0\xbe\xd0\xba \xd0\xb2\xd0\xb8\xd0\xb4\xd0\xb0 [(\xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd1\x8f, \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd1\x8c)], \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 return_cost=True\n            \xd0\xb8 \xd0\xb2\xd0\xb8\xd0\xb4\xd0\xb0 [\xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd1\x8f], \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 return_cost=False,\n            \xd1\x81\xd0\xbe\xd0\xb4\xd0\xb5\xd1\x80\xd0\xb6\xd0\xb0\xd1\x89\xd0\xb8\xd0\xb9 \xd0\xb2\xd1\x81\xd0\xb5 \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb8, \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb2\xd0\xbe\xd0\xb4\xd1\x8f\xd1\x89\xd0\xb8\xd0\xb5 first \xd0\xb2 second,\n            \xd1\x87\xd1\x8c\xd1\x8f \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd1\x8c \xd0\xbd\xd0\xb5 \xd0\xbf\xd1\x80\xd0\xb5\xd0\xb2\xd1\x8b\xd1\x88\xd0\xb0\xd0\xb5\xd1\x82 threshold\n        """"""\n        m, n = len(first), len(second)\n        agenda = [None] * (m + 1)\n        for i in range(m + 1):\n            agenda[i] = [[] for j in range(n + 1)]\n        agenda[m][n] = [((), 0.0)]\n        for i_right in range(m, -1, -1):\n            for j_right in range(n, -1, -1):\n                current_agenda = agenda[i_right][j_right]\n                if len(current_agenda) == 0:\n                    continue\n                for (i, j) in backtraces[i_right][j_right]:\n                    up, low = first[i:i_right], second[j:j_right]\n                    add_cost = self.operation_costs[up][low]\n                    for elem, cost in current_agenda:\n                        new_cost = cost + add_cost\n                        if new_cost <= threshold:  # \xd1\x83\xd0\xb4\xd0\xb0\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb9 \xd0\xb1\xd0\xbe\xd0\xbb\xd1\x8c\xd1\x88\xd0\xbe\xd0\xb9 \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd0\xb8\n                            agenda[i][j].append((((up, low),) + elem, new_cost))\n        if return_cost:\n            return agenda[0][0]\n        else:\n            return [elem[0] for elem in agenda[0][0]]\n\n    def _perform_insertions(self, initial, max_cost):\n        """"""\n        \xd0\xb2\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x89\xd0\xb0\xd0\xb5\xd1\x82 \xd0\xb2\xd1\x81\xd0\xb5 \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb8 \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd0\xb8 <= max_cost,\n        \xd0\xba\xd0\xbe\xd1\x82\xd0\xbe\xd1\x80\xd1\x8b\xd0\xb5 \xd0\xbc\xd0\xbe\xd0\xb6\xd0\xbd\xd0\xbe \xd0\xbf\xd0\xbe\xd0\xbb\xd1\x83\xd1\x87\xd0\xb8\xd1\x82\xd1\x8c \xd0\xb8\xd0\xb7 \xd1\x8d\xd0\xbb\xd0\xb5\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd0\xbe\xd0\xb2 initial\n\n        \xd0\x90\xd1\x80\xd0\xb3\xd1\x83\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd1\x8b:\n        ----------\n        initial : list of tuples\n            \xd1\x81\xd0\xbf\xd0\xb8\xd1\x81\xd0\xbe\xd0\xba \xd0\xb8\xd1\x81\xd1\x85\xd0\xbe\xd0\xb4\xd0\xbd\xd1\x8b\xd1\x85 \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb9 \xd0\xb2\xd0\xb8\xd0\xb4\xd0\xb0 [(\xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd1\x8f, \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd1\x8c)]\n        max_cost : float\n            \xd0\xbc\xd0\xb0\xd0\xba\xd1\x81\xd0\xb8\xd0\xbc\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xb0\xd1\x8f \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd1\x8c \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb8\n\n        \xd0\x92\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x89\xd0\xb0\xd0\xb5\xd1\x82:\n        -----------\n        final : list of tuples\n            \xd1\x84\xd0\xb8\xd0\xbd\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd1\x8b\xd0\xb9 \xd1\x81\xd0\xbf\xd0\xb8\xd1\x81\xd0\xbe\xd0\xba \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd0\xb9 \xd0\xb2\xd0\xb8\xd0\xb4\xd0\xb0 [(\xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xb4\xd1\x83\xd0\xba\xd1\x86\xd0\xb8\xd1\x8f, \xd1\x81\xd1\x82\xd0\xbe\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd1\x8c)]\n        """"""\n        queue = list(initial)\n        final = initial\n        while len(queue) > 0:\n            transduction, cost = queue[0]\n            queue = queue[1:]\n            for string, string_cost in self.operation_costs[""""].items():\n                new_cost = cost + string_cost\n                if new_cost <= max_cost:\n                    new_transduction = transduction + ("""", string)\n                    final.append((new_transduction, new_cost))\n                    queue.append((new_transduction, new_cost))\n        return final\n\n    def _make_default_operation_costs(self, allow_spaces=False):\n        """"""\n        sets 1.0 cost for every replacement, insertion, deletion and transposition\n        """"""\n        self.operation_costs = dict()\n        self.operation_costs[""""] = {c: 1.0 for c in list(self.alphabet) + [\' \']}\n        for a in self.alphabet:\n            current_costs = {c: 1.0 for c in self.alphabet}\n            current_costs[a] = 0.0\n            current_costs[""""] = 1.0\n            if allow_spaces:\n                current_costs["" ""] = 1.0\n            self.operation_costs[a] = current_costs\n        # \xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd0\xbf\xd0\xbe\xd0\xb7\xd0\xb8\xd1\x86\xd0\xb8\xd0\xb8\n        for a, b in itertools.permutations(self.alphabet, 2):\n            self.operation_costs[a + b] = {b + a: 1.0}\n        # \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb1\xd0\xb5\xd0\xbb\xd1\x8b\n        if allow_spaces:\n            self.operation_costs["" ""] = {c: 1.0 for c in self.alphabet}\n            self.operation_costs["" ""][""""] = 1.0\n'"
deeppavlov/models/spelling_correction/levenshtein/searcher_component.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport string\nfrom logging import getLogger\nfrom math import log10\nfrom typing import Iterable, List, Tuple, Optional\n\nfrom deeppavlov.core.common.registry import register\nfrom deeppavlov.core.models.component import Component\nfrom .levenshtein_searcher import LevenshteinSearcher\n\nlogger = getLogger(__name__)\n\n\n@register(\'spelling_levenshtein\')\nclass LevenshteinSearcherComponent(Component):\n    """"""Component that finds replacement candidates for tokens at a set Damerau-Levenshtein distance\n\n    Args:\n        words: list of every correct word\n        max_distance: maximum allowed Damerau-Levenshtein distance between source words and candidates\n        error_probability: assigned probability for every edit\n        vocab_penalty: assigned probability of an out of vocabulary token being the correct one without changes\n\n    Attributes:\n        max_distance: maximum allowed Damerau-Levenshtein distance between source words and candidates\n        error_probability: assigned logarithmic probability for every edit\n        vocab_penalty: assigned logarithmic probability of an out of vocabulary token being the correct one without\n         changes\n    """"""\n\n    _punctuation = frozenset(string.punctuation)\n\n    def __init__(self, words: Iterable[str], max_distance: int = 1, error_probability: float = 1e-4,\n                 vocab_penalty: Optional[float] = None, **kwargs):\n        words = list({word.strip().lower().replace(\'\xd1\x91\', \'\xd0\xb5\') for word in words})\n        alphabet = sorted({letter for word in words for letter in word})\n        self.max_distance = max_distance\n        self.error_probability = log10(error_probability)\n        self.vocab_penalty = self.error_probability if vocab_penalty is None else log10(vocab_penalty)\n        self.searcher = LevenshteinSearcher(alphabet, words, allow_spaces=True, euristics=2)\n\n    def _infer_instance(self, tokens: Iterable[str]) -> List[List[Tuple[float, str]]]:\n        candidates = []\n        for word in tokens:\n            if word in self._punctuation:\n                candidates.append([(0, word)])\n            else:\n                c = {candidate: self.error_probability * distance\n                     for candidate, distance in self.searcher.search(word, d=self.max_distance)}\n                c[word] = c.get(word, self.vocab_penalty)\n                candidates.append([(score, candidate) for candidate, score in c.items()])\n        return candidates\n\n    def __call__(self, batch: Iterable[Iterable[str]], *args, **kwargs) -> List[List[List[Tuple[float, str]]]]:\n        """"""Propose candidates for tokens in sentences\n\n        Args:\n            batch: batch of tokenized sentences\n\n        Returns:\n            batch of lists of probabilities and candidates for every token\n        """"""\n        return [self._infer_instance(tokens) for tokens in batch]\n'"
deeppavlov/models/spelling_correction/levenshtein/tabled_trie.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport copy\nfrom collections import defaultdict\n\nimport numpy as np\n\n\nclass Trie:\n    """"""\n    \xd0\xa0\xd0\xb5\xd0\xb0\xd0\xbb\xd0\xb8\xd0\xb7\xd0\xb0\xd1\x86\xd0\xb8\xd1\x8f \xd0\xbf\xd1\x80\xd0\xb5\xd1\x84\xd0\xb8\xd0\xba\xd1\x81\xd0\xbd\xd0\xbe\xd0\xb3\xd0\xbe \xd0\xb1\xd0\xbe\xd1\x80\xd0\xb0 (\xd1\x82\xd0\xbe\xd1\x87\xd0\xbd\xd0\xb5\xd0\xb5, \xd0\xba\xd0\xbe\xd1\x80\xd0\xbd\xd0\xb5\xd0\xb2\xd0\xbe\xd0\xb3\xd0\xbe \xd0\xbd\xd0\xb0\xd0\xbf\xd1\x80\xd0\xb0\xd0\xb2\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xbd\xd0\xbe\xd0\xb3\xd0\xbe \xd0\xb0\xd1\x86\xd0\xb8\xd0\xba\xd0\xbb\xd0\xb8\xd1\x87\xd0\xb5\xd1\x81\xd0\xba\xd0\xbe\xd0\xb3\xd0\xbe \xd0\xb3\xd1\x80\xd0\xb0\xd1\x84\xd0\xb0)\n\n    \xd0\x90\xd1\x82\xd1\x80\xd0\xb8\xd0\xb1\xd1\x83\xd1\x82\xd1\x8b\n    --------\n    alphabet: list, \xd0\xb0\xd0\xbb\xd1\x84\xd0\xb0\xd0\xb2\xd0\xb8\xd1\x82\n    alphabet_codes: dict, \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x80\xd1\x8c \xd1\x81\xd0\xb8\xd0\xbc\xd0\xb2\xd0\xbe\xd0\xbb:\xd0\xba\xd0\xbe\xd0\xb4\n    compressed: bool, \xd0\xb8\xd0\xbd\xd0\xb4\xd0\xb8\xd0\xba\xd0\xb0\xd1\x82\xd0\xbe\xd1\x80 \xd1\x81\xd0\xb6\xd0\xb0\xd1\x82\xd0\xb8\xd1\x8f\n    cashed: bool, \xd0\xb8\xd0\xbd\xd0\xb4\xd0\xb8\xd0\xba\xd0\xb0\xd1\x82\xd0\xbe\xd1\x80 \xd0\xba\xd1\x8d\xd1\x88\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x8f \xd0\xb7\xd0\xb0\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd0\xbe\xd0\xb2 \xd0\xba \xd1\x84\xd1\x83\xd0\xbd\xd0\xba\xd1\x86\xd0\xb8\xd0\xb8 descend\n    root: int, \xd0\xb8\xd0\xbd\xd0\xb4\xd0\xb5\xd0\xba\xd1\x81 \xd0\xba\xd0\xbe\xd1\x80\xd0\xbd\xd1\x8f\n    graph: array, type=int, shape=(\xd1\x87\xd0\xb8\xd1\x81\xd0\xbb\xd0\xbe \xd0\xb2\xd0\xb5\xd1\x80\xd1\x88\xd0\xb8\xd0\xbd, \xd1\x80\xd0\xb0\xd0\xb7\xd0\xbc\xd0\xb5\xd1\x80 \xd0\xb0\xd0\xbb\xd1\x84\xd0\xb0\xd0\xb2\xd0\xb8\xd1\x82\xd0\xb0), \xd0\xbc\xd0\xb0\xd1\x82\xd1\x80\xd0\xb8\xd1\x86\xd0\xb0 \xd0\xbf\xd0\xbe\xd1\x82\xd0\xbe\xd0\xbc\xd0\xba\xd0\xbe\xd0\xb2\n    graph[i][j] = k <-> \xd0\xb2\xd0\xb5\xd1\x80\xd1\x88\xd0\xb8\xd0\xbd\xd0\xb0 k --- \xd0\xbf\xd0\xbe\xd1\x82\xd0\xbe\xd0\xbc\xd0\xbe\xd0\xba \xd0\xb2\xd0\xb5\xd1\x80\xd1\x88\xd0\xb8\xd0\xbd\xd1\x8b i \xd0\xbf\xd0\xbe \xd1\x80\xd0\xb5\xd0\xb1\xd1\x80\xd1\x83, \xd0\xbf\xd0\xbe\xd0\xbc\xd0\xb5\xd1\x87\xd0\xb5\xd0\xbd\xd0\xbd\xd0\xbe\xd0\xbc\xd1\x83 \xd1\x81\xd0\xb8\xd0\xbc\xd0\xb2\xd0\xbe\xd0\xbb\xd0\xbe\xd0\xbc alphabet[j]\n    data: array, type=object, shape=(\xd1\x87\xd0\xb8\xd1\x81\xd0\xbb\xd0\xbe \xd0\xb2\xd0\xb5\xd1\x80\xd1\x88\xd0\xb8\xd0\xbd), \xd0\xbc\xd0\xb0\xd1\x81\xd1\x81\xd0\xb8\xd0\xb2 \xd1\x81 \xd0\xb4\xd0\xb0\xd0\xbd\xd0\xbd\xd1\x8b\xd0\xbc\xd0\xb8, \xd1\x85\xd1\x80\xd0\xb0\xd0\xbd\xd1\x8f\xd1\x89\xd1\x8f\xd0\xbc\xd0\xb8\xd1\x81\xd1\x8f \xd0\xb2 \xd0\xb2\xd0\xb5\xd1\x80\xd1\x88\xd0\xb8\xd0\xbd\xd0\xb0\xd1\x85\n    final: array, type=bool, shape=(\xd1\x87\xd0\xb8\xd1\x81\xd0\xbb\xd0\xbe \xd0\xb2\xd0\xb5\xd1\x80\xd1\x88\xd0\xb8\xd0\xbd), \xd0\xbc\xd0\xb0\xd1\x81\xd1\x81\xd0\xb8\xd0\xb2 \xd0\xb8\xd0\xbd\xd0\xb4\xd0\xb8\xd0\xba\xd0\xb0\xd1\x82\xd0\xbe\xd1\x80\xd0\xbe\xd0\xb2\n    final[i] = True <-> i --- \xd1\x84\xd0\xb8\xd0\xbd\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xb0\xd1\x8f \xd0\xb2\xd0\xb5\xd1\x80\xd1\x88\xd0\xb8\xd0\xbd\xd0\xb0\n    """"""\n    NO_NODE = -1\n    SPACE_CODE = -1\n\n    ATTRS = [\'is_numpied\', \'precompute_symbols\', \'allow_spaces\',\n             \'is_terminated\', \'to_make_cashed\']\n\n    def __init__(self, alphabet, make_sorted=True, make_alphabet_codes=True,\n                 is_numpied=False, to_make_cashed=False,\n                 precompute_symbols=None, allow_spaces=False, dict_storage=False):\n        self.alphabet = sorted(alphabet) if make_sorted else alphabet\n        self.alphabet_codes = ({a: i for i, a in enumerate(self.alphabet)}\n                               if make_alphabet_codes else self.alphabet)\n        self.alphabet_codes["" ""] = Trie.SPACE_CODE\n        self.is_numpied = is_numpied\n        self.to_make_cashed = to_make_cashed\n        self.dict_storage = dict_storage\n        self.precompute_symbols = precompute_symbols\n        self.allow_spaces = allow_spaces\n        self.initialize()\n\n    def initialize(self):\n        self.root = 0\n        self.graph = [self._make_default_node()]\n        self.data, self.final = [None], [False]\n        self.nodes_number = 1\n        self.descend = self._descend_simple\n        self.is_terminated = False\n\n    def _make_default_node(self):\n        if self.dict_storage:\n            return defaultdict(lambda: -1)\n        elif self.is_numpied:\n            return np.full(shape=(len(self.alphabet),),\n                           fill_value=Trie.NO_NODE, dtype=int)\n        else:\n            return [Trie.NO_NODE] * len(self.alphabet)\n\n    def save(self, outfile):\n        """"""\n        \xd0\xa1\xd0\xbe\xd1\x85\xd1\x80\xd0\xb0\xd0\xbd\xd1\x8f\xd0\xb5\xd1\x82 \xd0\xb4\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb2\xd0\xbe \xd0\xb4\xd0\xbb\xd1\x8f \xd0\xb4\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xb5\xd0\xb9\xd1\x88\xd0\xb5\xd0\xb3\xd0\xbe \xd0\xb8\xd1\x81\xd0\xbf\xd0\xbe\xd0\xbb\xd1\x8c\xd0\xb7\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x8f\n        """"""\n        with open(outfile, ""w"", encoding=""utf8"") as fout:\n            attr_values = [getattr(self, attr) for attr in Trie.ATTRS]\n            attr_values.append(any(x is not None for x in self.data))\n            fout.write(""{}\\n{}\\t{}\\n"".format(\n                "" "".join(""T"" if x else ""F"" for x in attr_values),\n                self.nodes_number, self.root))\n            fout.write("" "".join(str(a) for a in self.alphabet) + ""\\n"")\n            for index, label in enumerate(self.final):\n                letters = self._get_letters(index, return_indexes=True)\n                children = self._get_children(index)\n                fout.write(""{}\\t{}\\n"".format(\n                    ""T"" if label else ""F"", "" "".join(""{}:{}"".format(*elem)\n                                                    for elem in zip(letters, children))))\n            if self.precompute_symbols is not None:\n                for elem in self.data:\n                    fout.write("":"".join("","".join(\n                        map(str, symbols)) for symbols in elem) + ""\\n"")\n        return\n\n    def make_cashed(self):\n        """"""\n        \xd0\x92\xd0\xba\xd0\xbb\xd1\x8e\xd1\x87\xd0\xb0\xd0\xb5\xd1\x82 \xd0\xba\xd1\x8d\xd1\x88\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5 \xd0\xb7\xd0\xb0\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd0\xbe\xd0\xb2 \xd0\xba descend\n        """"""\n        self._descendance_cash = [dict() for _ in self.graph]\n        self.descend = self._descend_cashed\n\n    def make_numpied(self):\n        self.graph = np.array(self.graph)\n        self.final = np.asarray(self.final, dtype=bool)\n        self.is_numpied = True\n\n    def add(self, s):\n        """"""\n        \xd0\x94\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 \xd1\x81\xd1\x82\xd1\x80\xd0\xbe\xd0\xba\xd0\xb8 s \xd0\xb2 \xd0\xbf\xd1\x80\xd0\xb5\xd1\x84\xd0\xb8\xd0\xba\xd1\x81\xd0\xbd\xd1\x8b\xd0\xb9 \xd0\xb1\xd0\xbe\xd1\x80\n        """"""\n        if self.is_terminated:\n            raise TypeError(""Impossible to add string to fitted trie"")\n        if s == """":\n            self._set_final(self.root)\n            return\n        curr = self.root\n        for i, a in enumerate(s):\n            code = self.alphabet_codes[a]\n            next = self.graph[curr][code]\n            if next == Trie.NO_NODE:\n                curr = self._add_descendant(curr, s[i:])\n                break\n            else:\n                curr = next\n        self._set_final(curr)\n        return self\n\n    def fit(self, words):\n        for s in words:\n            self.add(s)\n        self.terminate()\n\n    def terminate(self):\n        if self.is_numpied:\n            self.make_numpied()\n        self.terminated = True\n        if self.precompute_symbols is not None:\n            precompute_future_symbols(self, self.precompute_symbols,\n                                      allow_spaces=self.allow_spaces)\n        if self.to_make_cashed:\n            self.make_cashed()\n\n    def __contains__(self, s):\n        if any(a not in self.alphabet for a in s):\n            return False\n        # word = tuple(self.alphabet_codes[a] for a in s)\n        node = self.descend(self.root, s)\n        return (node != Trie.NO_NODE) and self.is_final(node)\n\n    def words(self):\n        """"""\n        \xd0\x92\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x89\xd0\xb0\xd0\xb5\xd1\x82 \xd0\xb8\xd1\x82\xd0\xb5\xd1\x80\xd0\xb0\xd1\x82\xd0\xbe\xd1\x80 \xd0\xbf\xd0\xbe \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbc, \xd1\x81\xd0\xbe\xd0\xb4\xd0\xb5\xd1\x80\xd0\xb6\xd0\xb0\xd1\x89\xd0\xb8\xd0\xbc\xd1\x81\xd1\x8f \xd0\xb2 \xd0\xb1\xd0\xbe\xd1\x80\xd0\xb5\n        """"""\n        branch, word, indexes = [self.root], [], [0]\n        letters_with_children = [self._get_children_and_letters(self.root)]\n        while len(branch) > 0:\n            if self.is_final(branch[-1]):\n                yield """".join(word)\n            while indexes[-1] == len(letters_with_children[-1]):\n                indexes.pop()\n                letters_with_children.pop()\n                branch.pop()\n                if len(indexes) == 0:\n                    raise StopIteration()\n                word.pop()\n            next_letter, next_child = letters_with_children[-1][indexes[-1]]\n            indexes[-1] += 1\n            indexes.append(0)\n            word.append(next_letter)\n            branch.append(next_child)\n            letters_with_children.append(self._get_children_and_letters(branch[-1]))\n\n    def is_final(self, index):\n        """"""\n        \xd0\x90\xd1\x80\xd0\xb3\xd1\x83\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd1\x8b\n        ---------\n        index: int, \xd0\xbd\xd0\xbe\xd0\xbc\xd0\xb5\xd1\x80 \xd0\xb2\xd0\xb5\xd1\x80\xd1\x88\xd0\xb8\xd0\xbd\xd1\x8b\n\n        \xd0\x92\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x89\xd0\xb0\xd0\xb5\xd1\x82\n        ----------\n        True: \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 index --- \xd0\xbd\xd0\xbe\xd0\xbc\xd0\xb5\xd1\x80 \xd1\x84\xd0\xb8\xd0\xbd\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xbe\xd0\xb9 \xd0\xb2\xd0\xb5\xd1\x80\xd1\x88\xd0\xb8\xd0\xbd\xd1\x8b\n        """"""\n        return self.final[index]\n\n    def find_partitions(self, s, max_count=1):\n        """"""\n        \xd0\x9d\xd0\xb0\xd1\x85\xd0\xbe\xd0\xb4\xd0\xb8\xd1\x82 \xd0\xb2\xd1\x81\xd0\xb5 \xd1\x80\xd0\xb0\xd0\xb7\xd0\xb1\xd0\xb8\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8f s = s_1 ... s_m \xd0\xbd\xd0\xb0 \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x80\xd0\xbd\xd1\x8b\xd0\xb5 \xd1\x81\xd0\xbb\xd0\xbe\xd0\xb2\xd0\xb0 s_1, ..., s_m\n        \xd0\xb4\xd0\xbb\xd1\x8f m <= max_count\n        """"""\n        curr_agenda = [(self.root, [], 0)]\n        for i, a in enumerate(s):\n            next_agenda = []\n            for curr, borders, cost in curr_agenda:\n                if cost >= max_count:\n                    continue\n                child = self.graph[curr][self.alphabet_codes[a]]\n                # child = self.graph[curr][a]\n                if child == Trie.NO_NODE:\n                    continue\n                next_agenda.append((child, borders, cost))\n                if self.is_final(child):\n                    next_agenda.append((self.root, borders + [i + 1], cost + 1))\n            curr_agenda = next_agenda\n        answer = []\n        for curr, borders, cost in curr_agenda:\n            if curr == self.root:\n                borders = [0] + borders\n                answer.append([s[left:borders[i + 1]] for i, left in enumerate(borders[:-1])])\n        return answer\n\n    def __len__(self):\n        return self.nodes_number\n\n    def __repr__(self):\n        answer = """"\n        for i, (final, data) in enumerate(zip(self.final, self.data)):\n            letters, children = self._get_letters(i), self._get_children(i)\n            answer += ""{0}"".format(i)\n            if final:\n                answer += ""F""\n            for a, index in zip(letters, children):\n                answer += "" {0}:{1}"".format(a, index)\n            answer += ""\\n""\n            if data is not None:\n                answer += ""data:{0} {1}\\n"".format(len(data), "" "".join(str(elem) for elem in data))\n        return answer\n\n    def _add_descendant(self, parent, s, final=False):\n        for a in s:\n            code = self.alphabet_codes[a]\n            parent = self._add_empty_child(parent, code, final)\n        return parent\n\n    def _add_empty_child(self, parent, code, final=False):\n        """"""\n        \xd0\x94\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 \xd1\x80\xd0\xb5\xd0\xb1\xd1\x91\xd0\xbd\xd0\xba\xd0\xb0 \xd0\xba \xd0\xb2\xd0\xb5\xd1\x80\xd1\x88\xd0\xb8\xd0\xbd\xd0\xb5 parent \xd0\xbf\xd0\xbe \xd1\x81\xd0\xb8\xd0\xbc\xd0\xb2\xd0\xbe\xd0\xbb\xd1\x83 \xd1\x81 \xd0\xba\xd0\xbe\xd0\xb4\xd0\xbe\xd0\xbc code\n        """"""\n        self.graph[parent][code] = self.nodes_number\n        self.graph.append(self._make_default_node())\n        self.data.append(None)\n        self.final.append(final)\n        self.nodes_number += 1\n        return (self.nodes_number - 1)\n\n    def _descend_simple(self, curr, s):\n        """"""\n        \xd0\xa1\xd0\xbf\xd1\x83\xd1\x81\xd0\xba \xd0\xb8\xd0\xb7 \xd0\xb2\xd0\xb5\xd1\x80\xd1\x88\xd0\xb8\xd0\xbd\xd1\x8b curr \xd0\xbf\xd0\xbe \xd1\x81\xd1\x82\xd1\x80\xd0\xbe\xd0\xba\xd0\xb5 s\n        """"""\n        for a in s:\n            curr = self.graph[curr][self.alphabet_codes[a]]\n            if curr == Trie.NO_NODE:\n                break\n        return curr\n\n    def _descend_cashed(self, curr, s):\n        """"""\n        \xd0\xa1\xd0\xbf\xd1\x83\xd1\x81\xd0\xba \xd0\xb8\xd0\xb7 \xd0\xb2\xd0\xb5\xd1\x80\xd1\x88\xd0\xb8\xd0\xbd\xd1\x8b curr \xd0\xbf\xd0\xbe \xd1\x81\xd1\x82\xd1\x80\xd0\xbe\xd0\xba\xd0\xb5 s \xd1\x81 \xd0\xba\xd1\x8d\xd1\x88\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5\xd0\xbc\n        """"""\n        if s == """":\n            return curr\n        curr_cash = self._descendance_cash[curr]\n        answer = curr_cash.get(s, None)\n        if answer is not None:\n            return answer\n        # \xd0\xb4\xd0\xbb\xd1\x8f \xd0\xbe\xd0\xbf\xd1\x82\xd0\xb8\xd0\xbc\xd0\xb8\xd0\xb7\xd0\xb0\xd1\x86\xd0\xb8\xd0\xb8 \xd0\xb4\xd1\x83\xd0\xb1\xd0\xbb\xd0\xb8\xd1\x80\xd1\x83\xd0\xb5\xd0\xbc \xd0\xba\xd0\xbe\xd0\xb4\n        res = curr\n        for a in s:\n            res = self.graph[res][self.alphabet_codes[a]]\n            # res = self.graph[res][a]\n            if res == Trie.NO_NODE:\n                break\n        curr_cash[s] = res\n        return res\n\n    def _set_final(self, curr):\n        """"""\n        \xd0\x94\xd0\xb5\xd0\xbb\xd0\xb0\xd0\xb5\xd1\x82 \xd1\x81\xd0\xbe\xd1\x81\xd1\x82\xd0\xbe\xd1\x8f\xd0\xbd\xd0\xb8\xd0\xb5 curr \xd0\xb7\xd0\xb0\xd0\xb2\xd0\xb5\xd1\x80\xd1\x88\xd0\xb0\xd1\x8e\xd1\x89\xd0\xb8\xd0\xbc\n        """"""\n        self.final[curr] = True\n\n    def _get_letters(self, index, return_indexes=False):\n        """"""\n        \xd0\x98\xd0\xb7\xd0\xb2\xd0\xbb\xd0\xb5\xd0\xba\xd0\xb0\xd0\xb5\xd1\x82 \xd0\xb2\xd1\x81\xd0\xb5 \xd0\xbc\xd0\xb5\xd1\x82\xd0\xba\xd0\xb8 \xd0\xb2\xd1\x8b\xd1\x85\xd0\xbe\xd0\xb4\xd0\xbd\xd1\x8b\xd1\x85 \xd1\x80\xd1\x91\xd0\xb1\xd0\xb5\xd1\x80 \xd0\xb2\xd0\xb5\xd1\x80\xd1\x88\xd0\xb8\xd0\xbd\xd1\x8b \xd1\x81 \xd0\xbd\xd0\xbe\xd0\xbc\xd0\xb5\xd1\x80\xd0\xbe\xd0\xbc index\n        """"""\n        if self.dict_storage:\n            answer = list(self.graph[index].keys())\n        else:\n            answer = [i for i, elem in enumerate(self.graph[index])\n                      if elem != Trie.NO_NODE]\n        if not return_indexes:\n            answer = [(self.alphabet[i] if i >= 0 else "" "") for i in answer]\n        return answer\n\n    def _get_children_and_letters(self, index, return_indexes=False):\n        if self.dict_storage:\n            answer = list(self.graph[index].items())\n        else:\n            answer = [elem for elem in enumerate(self.graph[index])\n                      if elem[1] != Trie.NO_NODE]\n        if not return_indexes:\n            for i, (letter_index, child) in enumerate(answer):\n                answer[i] = (self.alphabet[letter_index], child)\n        return answer\n\n    def _get_children(self, index):\n        """"""\n        \xd0\x98\xd0\xb7\xd0\xb2\xd0\xbb\xd0\xb5\xd0\xba\xd0\xb0\xd0\xb5\xd1\x82 \xd0\xb2\xd1\x81\xd0\xb5\xd1\x85 \xd0\xbf\xd0\xbe\xd1\x82\xd0\xbe\xd0\xbc\xd0\xba\xd0\xbe\xd0\xb2 \xd0\xb2\xd0\xb5\xd1\x80\xd1\x88\xd0\xb8\xd0\xbd\xd1\x8b \xd1\x81 \xd0\xbd\xd0\xbe\xd0\xbc\xd0\xb5\xd1\x80\xd0\xbe\xd0\xbc index\n        """"""\n        if self.dict_storage:\n            return list(self.graph[index].values())\n        else:\n            return [elem for elem in self.graph[index] if elem != Trie.NO_NODE]\n\n\nclass TrieMinimizer:\n    def __init__(self):\n        pass\n\n    def minimize(self, trie, dict_storage=False, make_cashed=False, make_numpied=False,\n                 precompute_symbols=None, allow_spaces=False, return_groups=False):\n        N = len(trie)\n        if N == 0:\n            raise ValueError(""Trie should be non-empty"")\n        node_classes = np.full(shape=(N,), fill_value=-1, dtype=int)\n        order = self.generate_postorder(trie)\n        # processing the first node\n        index = order[0]\n        node_classes[index] = 0\n        class_representatives = [index]\n        node_key = ((), (), trie.is_final(index))\n        classes, class_keys = {node_key: 0}, [node_key]\n        curr_index = 1\n        for index in order[1:]:\n            letter_indexes = tuple(trie._get_letters(index, return_indexes=True))\n            children = trie._get_children(index)\n            children_classes = tuple(node_classes[i] for i in children)\n            key = (letter_indexes, children_classes, trie.is_final(index))\n            key_class = classes.get(key, None)\n            if key_class is not None:\n                node_classes[index] = key_class\n            else:\n                # \xd0\xbf\xd0\xbe\xd1\x8f\xd0\xb2\xd0\xb8\xd0\xbb\xd1\x81\xd1\x8f \xd0\xbd\xd0\xbe\xd0\xb2\xd1\x8b\xd0\xb9 \xd0\xba\xd0\xbb\xd0\xb0\xd1\x81\xd1\x81\n                class_keys.append(key)\n                classes[key] = node_classes[index] = curr_index\n                class_representatives.append(curr_index)\n                curr_index += 1\n        # \xd0\xbf\xd0\xbe\xd1\x81\xd1\x82\xd1\x80\xd0\xbe\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 \xd0\xbd\xd0\xbe\xd0\xb2\xd0\xbe\xd0\xb3\xd0\xbe \xd0\xb4\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb2\xd0\xb0\n        compressed = Trie(trie.alphabet, is_numpied=make_numpied,\n                          dict_storage=dict_storage, allow_spaces=allow_spaces,\n                          precompute_symbols=precompute_symbols)\n        L = len(classes)\n        new_final = [elem[2] for elem in class_keys[::-1]]\n        if dict_storage:\n            new_graph = [defaultdict(int) for _ in range(L)]\n        elif make_numpied:\n            new_graph = np.full(shape=(L, len(trie.alphabet)),\n                                fill_value=Trie.NO_NODE, dtype=int)\n            new_final = np.array(new_final, dtype=bool)\n        else:\n            new_graph = [[Trie.NO_NODE for a in trie.alphabet] for i in range(L)]\n        for (indexes, children, final), class_index in \\\n                sorted(classes.items(), key=(lambda x: x[1])):\n            row = new_graph[L - class_index - 1]\n            for i, child_index in zip(indexes, children):\n                row[i] = L - child_index - 1\n        compressed.graph = new_graph\n        compressed.root = L - node_classes[trie.root] - 1\n        compressed.final = new_final\n        compressed.nodes_number = L\n        compressed.data = [None] * L\n        if make_cashed:\n            compressed.make_cashed()\n        if precompute_symbols is not None:\n            if (trie.is_terminated and trie.precompute_symbols\n                    and trie.allow_spaces == allow_spaces):\n                # \xd0\xba\xd0\xbe\xd0\xbf\xd0\xb8\xd1\x80\xd1\x83\xd0\xb5\xd0\xbc \xd0\xb1\xd1\x83\xd0\xb4\xd1\x83\xd1\x89\xd0\xb8\xd0\xb5 \xd1\x81\xd0\xb8\xd0\xbc\xd0\xb2\xd0\xbe\xd0\xbb\xd1\x8b \xd0\xb8\xd0\xb7 \xd0\xb8\xd1\x81\xd1\x85\xd0\xbe\xd0\xb4\xd0\xbd\xd0\xbe\xd0\xb3\xd0\xbe \xd0\xb4\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb2\xd0\xb0\n                # \xd0\xbd\xd1\x83\xd0\xb6\xd0\xbd\xd0\xbe, \xd1\x87\xd1\x82\xd0\xbe\xd0\xb1\xd1\x8b \xd0\xb2\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x82 \xd0\xb8\xd0\xb7 \xd1\x84\xd0\xb8\xd0\xbd\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd1\x8b\xd1\x85 \xd1\x81\xd0\xbe\xd1\x81\xd1\x82\xd0\xbe\xd1\x8f\xd0\xbd\xd0\xb8\xd0\xb9 \xd0\xb2 \xd0\xbd\xd0\xb0\xd1\x87\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xbe\xd0\xb5 \xd0\xb1\xd1\x8b\xd0\xbb \xd0\xbe\xd0\xb4\xd0\xb8\xd0\xbd\xd0\xb0\xd0\xba\xd0\xbe\xd0\xb2\xd1\x8b\xd0\xbc \xd0\xb2 \xd0\xbe\xd0\xb1\xd0\xbe\xd0\xb8\xd1\x85 \xd0\xb4\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb2\xd1\x8c\xd1\x8f\xd1\x85\n                for i, node_index in enumerate(class_representatives[::-1]):\n                    # \xd0\xb1\xd1\x83\xd0\xb4\xd1\x83\xd1\x89\xd0\xb8\xd0\xb5 \xd1\x81\xd0\xb8\xd0\xbc\xd0\xb2\xd0\xbe\xd0\xbb\xd1\x8b \xd0\xb4\xd0\xbb\xd1\x8f \xd0\xbf\xd1\x80\xd0\xb5\xd0\xb4\xd1\x81\xd1\x82\xd0\xb0\xd0\xb2\xd0\xb8\xd1\x82\xd0\xb5\xd0\xbb\xd1\x8f i-\xd0\xb3\xd0\xbe \xd0\xba\xd0\xbb\xd0\xb0\xd1\x81\xd1\x81\xd0\xb0\n                    compressed.data[i] = copy.copy(trie.data[node_index])\n            else:\n                precompute_future_symbols(compressed, precompute_symbols, allow_spaces)\n        if return_groups:\n            node_classes = [L - i - 1 for i in node_classes]\n            return compressed, node_classes\n        else:\n            return compressed\n\n    def generate_postorder(self, trie):\n        """"""\n        \xd0\x9e\xd0\xb1\xd1\x80\xd0\xb0\xd1\x82\xd0\xbd\xd0\xb0\xd1\x8f \xd1\x82\xd0\xbe\xd0\xbf\xd0\xbe\xd0\xbb\xd0\xbe\xd0\xb3\xd0\xb8\xd1\x87\xd0\xb5\xd1\x81\xd0\xba\xd0\xb0\xd1\x8f \xd1\x81\xd0\xbe\xd1\x80\xd1\x82\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xba\xd0\xb0\n        """"""\n        order, stack = [], []\n        stack.append(trie.root)\n        colors = [\'white\'] * len(trie)\n        while len(stack) > 0:\n            index = stack[-1]\n            color = colors[index]\n            if color == \'white\':  # \xd0\xb2\xd0\xb5\xd1\x80\xd1\x88\xd0\xb8\xd0\xbd\xd0\xb0 \xd0\xb5\xd1\x89\xd1\x91 \xd0\xbd\xd0\xb5 \xd0\xbe\xd0\xb1\xd1\x80\xd0\xb0\xd0\xb1\xd0\xb0\xd1\x82\xd1\x8b\xd0\xb2\xd0\xb0\xd0\xbb\xd0\xb0\xd1\x81\xd1\x8c\n                colors[index] = \'grey\'\n                for child in trie._get_children(index):\n                    # \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb5\xd1\x80\xd1\x8f\xd0\xb5\xd0\xbc, \xd0\xbf\xd0\xbe\xd1\x81\xd0\xb5\xd1\x89\xd0\xb0\xd0\xbb\xd0\xb8 \xd0\xbb\xd0\xb8 \xd0\xbc\xd1\x8b \xd1\x80\xd0\xb5\xd0\xb1\xd1\x91\xd0\xbd\xd0\xba\xd0\xb0 \xd1\x80\xd0\xb0\xd0\xbd\xd1\x8c\xd1\x88\xd0\xb5\n                    if child != Trie.NO_NODE and colors[child] == \'white\':\n                        stack.append(child)\n            else:\n                if color == \'grey\':\n                    colors[index] = \'black\'\n                    order.append(index)\n                stack = stack[:-1]\n        return order\n\n\ndef load_trie(infile):\n    with open(infile, ""r"", encoding=""utf8"") as fin:\n        line = fin.readline().strip()\n        flags = [x == \'T\' for x in line.split()]\n        if len(flags) != len(Trie.ATTRS) + 1:\n            raise ValueError(""Wrong file format"")\n        nodes_number, root = map(int, fin.readline().strip().split())\n        alphabet = fin.readline().strip().split()\n        trie = Trie(alphabet)\n        for i, attr in enumerate(Trie.ATTRS):\n            setattr(trie, attr, flags[i])\n        read_data = flags[-1]\n        final = [False] * nodes_number\n        # print(len(alphabet), nodes_number)\n        if trie.dict_storage:\n            graph = [defaultdict(lambda: -1) for _ in range(nodes_number)]\n        elif trie.is_numpied:\n            final = np.array(final)\n            graph = np.full(shape=(nodes_number, len(alphabet)),\n                            fill_value=Trie.NO_NODE, dtype=int)\n        else:\n            graph = [[Trie.NO_NODE for a in alphabet] for i in range(nodes_number)]\n        for i in range(nodes_number):\n            line = fin.readline().strip()\n            if ""\\t"" in line:\n                label, transitions = line.split(""\\t"")\n                final[i] = (label == ""T"")\n            else:\n                label = line\n                final[i] = (label == ""T"")\n                continue\n            transitions = [x.split("":"") for x in transitions.split()]\n            for code, value in transitions:\n                graph[i][int(code)] = int(value)\n        trie.graph = graph\n        trie.root = root\n        trie.final = final\n        trie.nodes_number = nodes_number\n        trie.data = [None] * nodes_number\n        if read_data:\n            for i in range(nodes_number):\n                line = fin.readline().strip(""\\n"")\n                trie.data[i] = [set(elem.split("","")) for elem in line.split("":"")]\n        if trie.to_make_cashed:\n            trie.make_cashed()\n        return trie\n\n\ndef make_trie(alphabet, words, compressed=True, is_numpied=False,\n              make_cashed=False, precompute_symbols=False,\n              allow_spaces=False, dict_storage=False):\n    trie = Trie(alphabet, is_numpied=is_numpied, to_make_cashed=make_cashed,\n                precompute_symbols=precompute_symbols, dict_storage=dict_storage)\n    trie.fit(words)\n    if compressed:\n        tm = TrieMinimizer()\n        trie = tm.minimize(trie, dict_storage=dict_storage, make_cashed=make_cashed,\n                           make_numpied=is_numpied, precompute_symbols=precompute_symbols,\n                           allow_spaces=allow_spaces)\n    return trie\n\n\ndef precompute_future_symbols(trie, n, allow_spaces=False):\n    """"""\n    Collecting possible continuations of length <= n for every node\n    """"""\n    if n == 0:\n        return\n    if trie.is_terminated and trie.precompute_symbols:\n        # \xd1\x81\xd0\xb8\xd0\xbc\xd0\xb2\xd0\xbe\xd0\xbb\xd1\x8b \xd1\x83\xd0\xb6\xd0\xb5 \xd0\xbf\xd1\x80\xd0\xb5\xd0\xb4\xd0\xbf\xd0\xbe\xd1\x81\xd1\x87\xd0\xb8\xd1\x82\xd0\xb0\xd0\xbd\xd1\x8b\n        return\n    for index, final in enumerate(trie.final):\n        trie.data[index] = [set() for i in range(n)]\n    for index, (node_data, final) in enumerate(zip(trie.data, trie.final)):\n        node_data[0] = set(trie._get_letters(index))\n        if allow_spaces and final:\n            node_data[0].add("" "")\n    for d in range(1, n):\n        for index, (node_data, final) in enumerate(zip(trie.data, trie.final)):\n            children = set(trie._get_children(index))\n            for child in children:\n                node_data[d] |= trie.data[child][d - 1]\n            # \xd0\xb2 \xd1\x81\xd0\xbb\xd1\x83\xd1\x87\xd0\xb0\xd0\xb5, \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 \xd1\x80\xd0\xb0\xd0\xb7\xd1\x80\xd0\xb5\xd1\x88\xd1\x91\xd0\xbd \xd0\xb2\xd0\xbe\xd0\xb7\xd0\xb2\xd1\x80\xd0\xb0\xd1\x82 \xd0\xbf\xd0\xbe \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb1\xd0\xb5\xd0\xbb\xd1\x83 \xd0\xb2 \xd1\x81\xd1\x82\xd0\xb0\xd1\x80\xd1\x82\xd0\xbe\xd0\xb2\xd0\xbe\xd0\xb5 \xd1\x81\xd0\xbe\xd1\x81\xd1\x82\xd0\xbe\xd1\x8f\xd0\xbd\xd0\xb8\xd0\xb5\n            if allow_spaces and final:\n                node_data[d] |= trie.data[trie.root][d - 1]\n    trie.terminated = True\n'"
deeppavlov/skills/dsl_skill/handlers/__init__.py,0,b''
deeppavlov/skills/dsl_skill/handlers/handler.py,0,"b'# Copyright 2019 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom typing import Callable, Optional\n\nfrom deeppavlov.skills.dsl_skill.context import UserContext\nfrom deeppavlov.skills.dsl_skill.utils import SkillResponse\n\n\nclass Handler:\n    """"""\n    Handler instance helps DSLMeta class distinguish functions wrapped\n    by @DSLMeta.handler to add them to handlers storage.\n    It also checks if the handler function should be triggered based on the given context.\n\n    Attributes:\n        func: handler function\n        state: state in which handler can be activated\n        priority: priority of the function. If 2 or more handlers can be activated, handler\n         with the highest priority is selected\n        context_condition: predicate that accepts user context and checks if the handler should be activated. Example:\n         `lambda context: context.user_id != 1` checks if user_id is not equal to 1.\n         That means a user with id 1 will be always ignored by the handler.\n\n    """"""\n\n    def __init__(self,\n                 func: Callable,\n                 state: Optional[str] = None,\n                 context_condition: Optional[Callable] = None,\n                 priority: int = 0):\n        self.func = func\n        self.state = state\n        self.context_condition = context_condition\n        self.priority = priority\n\n    def __call__(self, context: UserContext) -> SkillResponse:\n        return self.func(context)\n\n    def check(self, context: UserContext) -> bool:\n        """"""\n        Checks:\n         - if the handler function should be triggered based on the given context via context condition.\n\n        Args:\n            context: user context\n\n        Returns:\n            True, if handler should be activated, False otherwise\n        """"""\n        if self.context_condition is not None:\n            return self.context_condition(context)\n        return True\n\n    def expand_context(self, context: UserContext) -> UserContext:\n        context.handler_payload = {}\n        return context\n'"
deeppavlov/skills/dsl_skill/handlers/regex_handler.py,0,"b'# Copyright 2019 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\nfrom typing import List, Callable, Optional\n\nfrom deeppavlov.skills.dsl_skill.context import UserContext\nfrom deeppavlov.skills.dsl_skill.handlers.handler import Handler\n\n\nclass RegexHandler(Handler):\n    """"""\n    This handler checks whether the message that is passed to it is matched by a regex.\n\n    Adds the following key to ```context.handler_payload```:\n        - \'regex_groups\' - groups parsed from regular expression in command, by name\n\n    Attributes:\n        func: handler function\n        state: state in which handler can be activated\n        priority: priority of the function. If 2 or more handlers can be activated, function\n         with the highest priority is selected\n        context_condition: predicate that accepts user context and checks if the handler should be activated.\n         Example: `lambda context: context.user_id != 1` checks if user_id is not equal to 1.\n         That means a user with id 1 will be always ignored by the handler.\n        commands: handler is activated if regular expression from this list is matched with a user message\n\n    """"""\n\n    def __init__(self,\n                 func: Callable,\n                 commands: Optional[List[str]] = None,\n                 state: Optional[str] = None,\n                 context_condition: Optional[Callable] = None,\n                 priority: int = 0):\n        super().__init__(func, state, context_condition, priority)\n        self.commands = [re.compile(command) for command in commands]\n\n    def check(self, context: UserContext) -> bool:\n        """"""\n        Checks:\n         - if the handler function should be triggered based on the given context via context condition.\n         - if at least one of the commands is matched to the `context.message`.\n\n        Args:\n            context: user context\n\n        Returns:\n            True, if handler should be activated, False otherwise\n        """"""\n        is_previous_matches = super().check(context)\n        if not is_previous_matches:\n            return False\n\n        message = context.message\n        return any(re.search(regexp, \' \'.join(message)) for regexp in self.commands)\n\n    def expand_context(self, context: UserContext) -> UserContext:\n        context.handler_payload = {\'regex_groups\': {}}\n        message = context.message\n        for regexp in self.commands:\n            match = re.search(regexp, \' \'.join(message))\n            if match is not None:\n                for group_ind, span in enumerate(match.regs):\n                    context.handler_payload[\'regex_groups\'][group_ind] = message[span[0]: span[1]]\n                for group_name, group_ind in regexp.groupindex.items():\n                    context.handler_payload[\'regex_groups\'][group_name] = \\\n                        context.handler_payload[\'regex_groups\'][group_ind]\n                return context\n'"
deeppavlov/models/go_bot/nlg/dto/__init__.py,0,b''
deeppavlov/models/go_bot/nlg/dto/batch_nlg_response.py,0,"b'from typing import Container\nfrom deeppavlov.models.go_bot.nlg.dto.nlg_response_interface import NLGResponseInterface\n\n\nclass BatchNLGResponse:\n    def __init__(self, nlg_responses: Container[NLGResponseInterface]):\n        self.responses: Container[NLGResponseInterface] = nlg_responses\n'"
deeppavlov/models/go_bot/nlg/dto/json_nlg_response.py,0,"b'from deeppavlov.models.go_bot.nlg.dto.nlg_response_interface import NLGObjectResponseInterface\n\n\nclass JSONNLGResponse(NLGObjectResponseInterface):\n    """"""\n    The NLG output unit that stores slot values and predicted actions info.\n    """"""\n    def __init__(self, slot_values: dict, actions_tuple: tuple):\n        self.slot_values = slot_values\n        self.actions_tuple = actions_tuple\n\n    def to_serializable_dict(self) -> dict:\n        return {\'+\'.join(self.actions_tuple): self.slot_values}\n'"
deeppavlov/models/go_bot/nlg/dto/nlg_response_interface.py,0,"b'from abc import ABCMeta\nfrom typing import Tuple\n\n\nclass NLGObjectResponseInterface(metaclass=ABCMeta):\n    def to_serializable_dict(self) -> dict:\n        raise NotImplementedError(f""to_serializable_dict() not implemented in {self.__class__.__name__}"")\n\n\nNLGResponseInterface = Tuple[NLGObjectResponseInterface, str]\n'"
deeppavlov/models/go_bot/nlg/templates/__init__.py,0,b''
deeppavlov/models/go_bot/nlg/templates/templates.py,0,"b'# Copyright 2017 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport copy\nimport re\nfrom abc import ABCMeta, abstractmethod\n\n\nclass Template(metaclass=ABCMeta):\n\n    @abstractmethod\n    def from_str(cls, s):\n        return cls(s)  # TODO move deserialization logic onto separate class, smth like serialization proxy or factory\n\n\nclass DefaultTemplate(Template):\n\n    def __init__(self, text=""""):\n        self.text = text\n\n    @classmethod\n    def from_str(cls, s):\n        return cls(s)\n\n    def update(self, text=""""):\n        self.text = self.text or text\n\n    def __contains__(self, t):\n        return t.text == self.text\n\n    def __eq__(self, other):\n        if isinstance(other, self.__class__):\n            return self.text == other.text\n        return False\n\n    def __hash__(self):\n        """"""Override the default hash behavior (that returns the id)""""""\n        return hash(self.text)\n\n    def __str__(self):\n        return self.text\n\n    def generate_text(self, slots=[]):\n        t = copy.copy(self.text)\n        if isinstance(slots, dict):\n            slots = slots.items()\n        for slot, value in slots:\n            t = t.replace(\'#\' + slot, value, 1)\n        if t:\n            t = t[0].upper() + t[1:]\n        return t\n\n\nclass DualTemplate(Template):\n\n    def __init__(self, default="""", dontcare=""""):\n        self.default = default\n        self.dontcare = dontcare\n\n    @property\n    def dontcare_slots(self):\n        default_slots = self._slots(self.default)\n        dontcare_slots = self._slots(self.dontcare)\n        return default_slots - dontcare_slots\n\n    @staticmethod\n    def _slots(text):\n        return set(re.findall(\'#(\\w+)\', text))\n\n    @classmethod\n    def from_str(cls, s):\n        return cls(*s.split(\'\\t\', 1))\n\n    def update(self, default="""", dontcare=""""):\n        self.default = self.default or default\n        self.dontcare = self.dontcare or dontcare\n\n    def __contains__(self, t):\n        return t.default and (t.default == self.default) \\\n               or t.dontcare and (t.dontcare == self.dontcare)\n\n    def __eq__(self, other):\n        if isinstance(other, self.__class__):\n            return (self.default == other.default) \\\n                   and (self.dontcare == other.dontcare)\n        return False\n\n    def __hash__(self):\n        """"""Override the default hash behavior (that returns the id)""""""\n        return hash(self.default + \'\\t\' + self.dontcare)\n\n    def __str__(self):\n        return self.default + \'\\t\' + self.dontcare\n\n    def generate_text(self, slots):\n        t = copy.copy(self.default)\n        if isinstance(slots, dict):\n            slots = slots.items()\n        dontcare_slots = (s[0] for s in slots if s[1] == \'dontcare\')\n        if self.dontcare and self.dontcare_slots.issubset(dontcare_slots):\n            t = copy.copy(self.dontcare)\n        for slot, value in slots:\n            t = t.replace(\'#\' + slot, value, 1)\n        if t:\n            t = t[0].upper() + t[1:]\n        return t\n\n\nclass Templates:\n\n    def __init__(self, ttype):\n        self.ttype = ttype\n        self.act2templ = {}\n        self.templ2act = {}\n        self._actions = []\n        self._templates = []\n\n    def __contains__(self, key):\n        """"""If key is an str, returns whether the key is in the actions.\n        If key is a Template, returns if the key is templates.\n        """"""\n        if isinstance(key, str):\n            return key in self.act2templ\n        elif isinstance(key, Template):\n            return key in self.templ2act\n\n    def __getitem__(self, key):\n        """"""If key is an str, returns corresponding template.\n        If key is a Template, return corresponding action.\n        If does not exist, return None.\n        """"""\n        if isinstance(key, str):\n            return self.act2templ[key]\n        elif isinstance(key, Template):\n            return self.templ2act[key]\n\n    def __len__(self):\n        return len(self.act2templ)\n\n    def __str__(self):\n        return str(self.act2templ)\n\n    def __setitem__(self, key, value):\n        """"""If the key is not in  the dictionary, add it.""""""\n        key = str(key)\n        if key not in self.act2templ:\n            self.act2templ[key] = value\n            self.templ2act[value] = key\n            self._actions = []\n            self._templates = []\n\n    @property\n    def actions(self):\n        if not self._actions:\n            self._actions = sorted(self.act2templ.keys())\n        return self._actions\n\n    @property\n    def templates(self):\n        if not self._templates:\n            self._templates = [self.act2templ[a] for a in self.actions]\n        return self._templates\n\n    def load(self, filename):\n        with open(filename, \'r\', encoding=\'utf8\') as fp:\n            for ln in fp:\n                act, template = ln.strip(\'\\n\').split(\'\\t\', 1)\n                self.__setitem__(act, self.ttype.from_str(template))\n        return self\n\n    def save(self, filename):\n        with open(filename, \'w\', encoding=\'utf8\') as outfile:\n            for act in sorted(self.actions):\n                template = self.__getitem__(act)\n                outfile.write(\'{}\\t{}\\n\'.format(act, template))\n'"
deeppavlov/models/go_bot/nlu/dto/__init__.py,0,b''
deeppavlov/models/go_bot/nlu/dto/nlu_response.py,0,"b'from typing import Any, Dict, Tuple, List, Union, Optional\n\nfrom deeppavlov.models.go_bot.nlu.dto.nlu_response_interface import NLUResponseInterface\nfrom deeppavlov.models.go_bot.nlu.dto.text_vectorization_response import TextVectorizationResponse\n\n\nclass NLUResponse(NLUResponseInterface):\n    """"""\n    Stores the go-bot NLU knowledge: extracted slots and intents info, embedding and bow vectors.\n    """"""\n    def __init__(self, slots, intents, tokens):\n        self.slots: Union[List[Tuple[str, Any]], Dict[str, Any]] = slots\n        self.intents = intents\n        self.tokens = tokens\n        self.tokens_vectorized: Optional[TextVectorizationResponse] = None\n\n    def set_tokens_vectorized(self, tokens_vectorized):\n        self.tokens_vectorized = tokens_vectorized\n'"
deeppavlov/models/go_bot/nlu/dto/nlu_response_interface.py,0,b'from abc import ABCMeta\n\n\nclass NLUResponseInterface(metaclass=ABCMeta):\n    pass\n'
deeppavlov/models/go_bot/nlu/dto/text_vectorization_response.py,0,"b'class TextVectorizationResponse:\n    """"""\n    Stores the BOW-encodings and (padded or aggregated e.g. averaged) embeddings for text.\n    """"""\n\n    def __init__(self, tokens_bow_encoded, tokens_aggregated_embedding, tokens_embeddings_padded):\n        self.tokens_bow_encoded = tokens_bow_encoded\n        self.tokens_aggregated_embedding = tokens_aggregated_embedding\n        self.tokens_embeddings_padded = tokens_embeddings_padded\n'"
deeppavlov/models/go_bot/policy/dto/__init__.py,0,b''
deeppavlov/models/go_bot/policy/dto/attn_params.py,0,"b'from typing import NamedTuple\n\n\nclass GobotAttnParams(NamedTuple):\n    """"""\n    the DTO-like class that stores the attention mechanism configuration params.\n    """"""\n    max_num_tokens: int\n    hidden_size: int\n    token_size: int\n    key_size: int\n    type_: str\n    projected_align: bool\n    depth: int\n    action_as_key: bool\n    intent_as_key: bool\n'"
deeppavlov/models/go_bot/policy/dto/digitized_policy_features.py,0,"b'class DigitizedPolicyFeatures:\n    def __init__(self, attn_key, concat_feats, action_mask):\n        self.attn_key = attn_key\n        self.concat_feats = concat_feats\n        self.action_mask = action_mask\n'"
deeppavlov/models/go_bot/policy/dto/policy_network_params.py,0,"b'from logging import getLogger\n\nlog = getLogger(__name__)\n\n\nclass PolicyNetworkParams:\n    """"""\n    The class to deal with the overcomplicated structure of the GO-bot configs.\n    It is initialized from the config-as-is and performs all the conflicting parameters resolution internally.\n    """"""\n    # todo remove the complex config logic\n    UNSUPPORTED = [""obs_size""]\n    DEPRECATED = [""end_learning_rate"", ""decay_steps"", ""decay_power""]\n\n    def __init__(self,\n                 hidden_size,\n                 dropout_rate,\n                 l2_reg_coef,\n                 dense_size,\n                 attention_mechanism,\n                 network_parameters):\n        self.hidden_size = hidden_size\n        self.dropout_rate = dropout_rate\n        self.l2_reg_coef = l2_reg_coef\n        self.dense_size = dense_size\n        self.attention_mechanism = attention_mechanism\n        self.network_parameters = network_parameters or {}\n\n        self.log_deprecated_params(self.network_parameters.keys())\n\n    def get_hidden_size(self):\n        return self.network_parameters.get(""hidden_size"", self.hidden_size)\n\n    def get_action_size(self):\n        return self.network_parameters.get(""action_size"")\n\n    def get_dropout_rate(self):\n        return self.network_parameters.get(""dropout_rate"", self.dropout_rate)\n\n    def get_l2_reg_coef(self):\n        return self.network_parameters.get(""l2_reg_coef"", self.l2_reg_coef)\n\n    def get_dense_size(self):\n        return self.network_parameters.get(""dense_size"", self.dense_size) or self.hidden_size  # todo :(\n\n    def get_learning_rate(self):\n        return self.network_parameters.get(""learning_rate"", None)\n\n    def get_attn_params(self):\n        return self.network_parameters.get(\'attention_mechanism\', self.attention_mechanism)\n\n    def log_deprecated_params(self, network_parameters):\n        if any(p in network_parameters for p in self.DEPRECATED):\n            log.warning(f""parameters {self.DEPRECATED} are deprecated,""\n                        f"" for learning rate schedule documentation see""\n                        f"" deeppavlov.core.models.lr_scheduled_tf_model""\n                        f"" or read a github tutorial on super convergence."")\n'"
deeppavlov/models/go_bot/policy/dto/policy_prediction.py,0,"b'from typing import Tuple\n\nimport numpy as np\n\n\nclass PolicyPrediction:\n    """"""\n    Used to store policy model predictions and hidden values.\n    """"""\n    def __init__(self, probs, prediction, hidden_outs, cell_state):\n        self.probs = probs\n        self.prediction = prediction\n        self.hidden_outs = hidden_outs\n        self.cell_state = cell_state\n        self.predicted_action_ix = np.argmax(probs)\n\n    def get_network_state(self) -> Tuple:\n        return self.cell_state, self.hidden_outs\n'"
deeppavlov/models/go_bot/tracker/dto/__init__.py,0,b''
deeppavlov/models/go_bot/tracker/dto/dst_knowledge.py,0,"b'from deeppavlov.models.go_bot.tracker.dto.tracker_knowledge_interface import TrackerKnowledgeInterface\n\n\n# todo naming\nclass DSTKnowledge(TrackerKnowledgeInterface):\n    def __init__(self, tracker_prev_action, state_features, context_features, api_call_id, n_actions):\n        self.tracker_prev_action = tracker_prev_action\n        self.state_features = state_features\n        self.context_features = context_features\n        self.api_call_id = api_call_id\n        self.n_actions = n_actions\n'"
deeppavlov/models/go_bot/tracker/dto/tracker_knowledge_interface.py,0,b'from abc import ABCMeta\n\n\nclass TrackerKnowledgeInterface(metaclass=ABCMeta):\n    pass\n'
deeppavlov/models/ranking/matching_models/dam_utils/__init__.py,0,b''
deeppavlov/models/ranking/matching_models/dam_utils/layers.py,121,"b'# Copyright 2018 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# @inproceedings{ ,\n#   title={Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network},\n#   author={Xiangyang Zhou, Lu Li, Daxiang Dong, Yi Liu, Ying Chen, Wayne Xin Zhao, Dianhai Yu and Hua Wu},\n#   booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n#   volume={1},\n#   pages={  --  },\n#   year={2018}\n# }\n# ```\n# http://aclweb.org/anthology/P18-1103\n#\n# Based on authors\' Tensorflow code: https://github.com/baidu/Dialogue/tree/master/DAM\n\nfrom logging import getLogger\n\nimport tensorflow as tf\n\nimport deeppavlov.models.ranking.matching_models.dam_utils.operations as op\n\nlog = getLogger(__name__)\n\n\ndef similarity(x, y, x_lengths, y_lengths):\n    \'\'\'calculate similarity with two 3d tensor.\n\n    Args:\n        x: a tensor with shape [batch, time_x, dimension]\n        y: a tensor with shape [batch, time_y, dimension]\n\n    Returns:\n        a tensor with shape [batch, time_x, time_y]\n\n    Raises:\n        ValueError: if\n            the dimenisons of x and y are not equal.\n    \'\'\'\n    with tf.variable_scope(\'x_attend_y\'):\n        try:\n            x_a_y = block(\n                x, y, y,\n                Q_lengths=x_lengths, K_lengths=y_lengths)\n        except ValueError:\n            tf.get_variable_scope().reuse_variables()\n            x_a_y = block(\n                x, y, y,\n                Q_lengths=x_lengths, K_lengths=y_lengths)\n\n    with tf.variable_scope(\'y_attend_x\'):\n        try:\n            y_a_x = block(\n                y, x, x,\n                Q_lengths=y_lengths, K_lengths=x_lengths)\n        except ValueError:\n            tf.get_variable_scope().reuse_variables()\n            y_a_x = block(\n                y, x, x,\n                Q_lengths=y_lengths, K_lengths=x_lengths)\n\n    return tf.matmul(x + x_a_y, y + y_a_x, transpose_b=True)\n\n\ndef dynamic_L(x):\n    \'\'\'Attention machanism to combine the infomation, \n       from https://arxiv.org/pdf/1612.01627.pdf.\n\n    Args:\n        x: a tensor with shape [batch, time, dimension]\n\n    Returns:\n        a tensor with shape [batch, dimension]\n\n    Raises:\n    \'\'\'\n    key_0 = tf.get_variable(\n        name=\'key\',\n        shape=[x.shape[-1]],\n        dtype=tf.float32,\n        initializer=tf.random_uniform_initializer(\n            -tf.sqrt(6. / tf.cast(x.shape[-1], tf.float32)),\n            tf.sqrt(6. / tf.cast(x.shape[-1], tf.float32))))\n\n    key = op.dense(x, add_bias=False)  # [batch, time, dimension]\n    weight = tf.reduce_sum(tf.multiply(key, key_0), axis=-1)  # [batch, time]\n    weight = tf.expand_dims(tf.nn.softmax(weight), -1)  # [batch, time, 1]\n\n    L = tf.reduce_sum(tf.multiply(x, weight), axis=1)  # [batch, dimension]\n    return L\n\n\ndef loss(x, y, num_classes=2, is_clip=True, clip_value=10):\n    \'\'\'From info x calculate logits as return loss.\n\n    Args:\n        x: a tensor with shape [batch, dimension]\n        num_classes: a number\n\n    Returns:\n        loss: a tensor with shape [1], which is the average loss of one batch\n        logits: a tensor with shape [batch, 1]\n\n    Raises:\n        AssertionError: if\n            num_classes is not a int greater equal than 2.\n    TODO:\n        num_classes > 2 may be not adapted.\n    \'\'\'\n    assert isinstance(num_classes, int)\n    assert num_classes >= 2\n\n    # W = tf.get_variable(\n    #     name=\'weights\',\n    #     shape=[x.shape[-1], num_classes-1],\n    #     initializer=tf.orthogonal_initializer())\n    # bias = tf.get_variable(\n    #     name=\'bias\',\n    #     shape=[num_classes-1],\n    #     initializer=tf.zeros_initializer())\n    #\n    # logits = tf.reshape(tf.matmul(x, W) + bias, [-1])\n    # loss = tf.nn.sigmoid_cross_entropy_with_logits(\n    #     labels=tf.cast(y, tf.float32),\n    #     logits=logits)\n    # loss = tf.reduce_mean(tf.clip_by_value(loss, -clip_value, clip_value))\n\n    W = tf.get_variable(\n        name=\'weights\',\n        shape=[x.shape[-1], num_classes],\n        initializer=tf.orthogonal_initializer())\n    bias = tf.get_variable(\n        name=\'bias\',\n        shape=[num_classes],\n        initializer=tf.zeros_initializer())\n\n    logits = tf.matmul(x, W) + bias\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        labels=y,\n        logits=logits)\n    loss = tf.reduce_mean(tf.clip_by_value(loss, -clip_value, clip_value))\n\n    return loss, logits\n\n\ndef attention(\n        Q, K, V,\n        Q_lengths, K_lengths,\n        attention_type=\'dot\',\n        is_mask=True, mask_value=-2 ** 32 + 1,\n        drop_prob=None):\n    \'\'\'Add attention layer.\n    Args:\n        Q: a tensor with shape [batch, Q_time, Q_dimension]\n        K: a tensor with shape [batch, time, K_dimension]\n        V: a tensor with shape [batch, time, V_dimension]\n\n        Q_length: a tensor with shape [batch]\n        K_length: a tensor with shape [batch]\n\n    Returns:\n        a tensor with shape [batch, Q_time, V_dimension]\n\n    Raises:\n        AssertionError: if\n            Q_dimension not equal to K_dimension when attention type is dot.\n    \'\'\'\n    assert attention_type in (\'dot\', \'bilinear\')\n    if attention_type == \'dot\':\n        assert Q.shape[-1] == K.shape[-1]\n\n    Q_time = Q.shape[1]\n    K_time = K.shape[1]\n\n    if attention_type == \'dot\':\n        logits = op.dot_sim(Q, K)  # [batch, Q_time, time]\n    if attention_type == \'bilinear\':\n        logits = op.bilinear_sim(Q, K)\n\n    if is_mask:\n        mask = op.mask(Q_lengths, K_lengths, Q_time, K_time)  # [batch, Q_time, K_time]\n        # mask = tf.Print(mask, [logits[0], mask[0]], tf.get_variable_scope().name + "" logits, mask: "", summarize=10)\n        logits = mask * logits + (1 - mask) * mask_value\n        # logits = tf.Print(logits, [logits[0]], tf.get_variable_scope().name + "" masked logits: "", summarize=10)\n\n    attention = tf.nn.softmax(logits)\n\n    if drop_prob is not None:\n        log.info(\'use attention drop\')\n        attention = tf.nn.dropout(attention, drop_prob)\n\n    return op.weighted_sum(attention, V)\n\n\ndef FFN(x, out_dimension_0=None, out_dimension_1=None):\n    \'\'\'Add two dense connected layer, max(0, x*W0+b0)*W1+b1.\n\n    Args:\n        x: a tensor with shape [batch, time, dimension]\n        out_dimension: a number which is the output dimension\n\n    Returns:\n        a tensor with shape [batch, time, out_dimension]\n\n    Raises:\n    \'\'\'\n    with tf.variable_scope(\'FFN_1\'):\n        y = op.dense(x, out_dimension_0, initializer=tf.keras.initializers.he_normal(seed=42))\n        y = tf.nn.relu(y)\n    with tf.variable_scope(\'FFN_2\'):\n        # z = op.dense(y, out_dimension_1, initializer=tf.keras.initializers.glorot_uniform(seed=42))  # TODO: check\n        z = op.dense(y, out_dimension_1)  # , add_bias=False)  #!!!!\n    return z\n\n\ndef block(\n        Q, K, V,\n        Q_lengths, K_lengths,\n        attention_type=\'dot\',\n        is_layer_norm=True,\n        is_mask=True, mask_value=-2 ** 32 + 1,\n        drop_prob=None):\n    \'\'\'Add a block unit from https://arxiv.org/pdf/1706.03762.pdf.\n    Args:\n        Q: a tensor with shape [batch, Q_time, Q_dimension]\n        K: a tensor with shape [batch, time, K_dimension]\n        V: a tensor with shape [batch, time, V_dimension]\n\n        Q_length: a tensor with shape [batch]\n        K_length: a tensor with shape [batch]\n\n    Returns:\n        a tensor with shape [batch, time, dimension]\n\n    Raises:\n    \'\'\'\n    att = attention(Q, K, V,\n                    Q_lengths, K_lengths,\n                    attention_type=attention_type,\n                    is_mask=is_mask, mask_value=mask_value,\n                    drop_prob=drop_prob)\n    if is_layer_norm:\n        with tf.variable_scope(\'attention_layer_norm\'):\n            y = op.layer_norm_debug(Q + att)\n    else:\n        y = Q + att\n\n    z = FFN(y)\n    if is_layer_norm:\n        with tf.variable_scope(\'FFN_layer_norm\'):\n            w = op.layer_norm_debug(y + z)\n    else:\n        w = y + z\n    return w\n\n\ndef CNN(x, out_channels, filter_size, pooling_size, add_relu=True):\n    \'\'\'Add a convlution layer with relu and max pooling layer.\n\n    Args:\n        x: a tensor with shape [batch, in_height, in_width, in_channels]\n        out_channels: a number\n        filter_size: a number\n        pooling_size: a number\n\n    Returns:\n        a flattened tensor with shape [batch, num_features]\n\n    Raises:\n    \'\'\'\n    # calculate the last dimension of return\n    num_features = ((tf.shape(x)[1] - filter_size + 1) / pooling_size *\n                    (tf.shape(x)[2] - filter_size + 1) / pooling_size) * out_channels\n\n    in_channels = x.shape[-1]\n    weights = tf.get_variable(\n        name=\'filter\',\n        shape=[filter_size, filter_size, in_channels, out_channels],\n        dtype=tf.float32,\n        initializer=tf.random_uniform_initializer(-0.01, 0.01))\n    bias = tf.get_variable(\n        name=\'bias\',\n        shape=[out_channels],\n        dtype=tf.float32,\n        initializer=tf.zeros_initializer())\n\n    conv = tf.nn.conv2d(x, weights, strides=[1, 1, 1, 1], padding=""VALID"")\n    conv = conv + bias\n\n    if add_relu:\n        conv = tf.nn.relu(conv)\n\n    pooling = tf.nn.max_pool(\n        conv,\n        ksize=[1, pooling_size, pooling_size, 1],\n        strides=[1, pooling_size, pooling_size, 1],\n        padding=""VALID"")\n\n    return tf.contrib.layers.flatten(pooling)\n\n\ndef CNN_3d(x, out_channels_0, out_channels_1, add_relu=True):\n    \'\'\'Add a 3d convlution layer with relu and max pooling layer.\n\n    Args:\n        x: a tensor with shape [batch, in_depth, in_height, in_width, in_channels]\n        out_channels: a number\n        filter_size: a number\n        pooling_size: a number\n\n    Returns:\n        a flattened tensor with shape [batch, num_features]\n\n    Raises:\n    \'\'\'\n    in_channels = x.shape[-1]\n    weights_0 = tf.get_variable(\n        name=\'filter_0\',\n        shape=[3, 3, 3, in_channels, out_channels_0],\n        dtype=tf.float32,\n        initializer=tf.random_uniform_initializer(-0.001, 0.001))\n    bias_0 = tf.get_variable(\n        name=\'bias_0\',\n        shape=[out_channels_0],\n        dtype=tf.float32,\n        initializer=tf.zeros_initializer())\n\n    conv_0 = tf.nn.conv3d(x, weights_0, strides=[1, 1, 1, 1, 1], padding=""SAME"")\n    log.info(\'conv_0 shape: %s\' % conv_0.shape)\n    conv_0 = conv_0 + bias_0\n\n    if add_relu:\n        conv_0 = tf.nn.elu(conv_0)\n\n    pooling_0 = tf.nn.max_pool3d(\n        conv_0,\n        ksize=[1, 3, 3, 3, 1],\n        strides=[1, 3, 3, 3, 1],\n        padding=""SAME"")\n    log.info(\'pooling_0 shape: %s\' % pooling_0.shape)\n\n    # layer_1\n    weights_1 = tf.get_variable(\n        name=\'filter_1\',\n        shape=[3, 3, 3, out_channels_0, out_channels_1],\n        dtype=tf.float32,\n        initializer=tf.random_uniform_initializer(-0.001, 0.001))\n    bias_1 = tf.get_variable(\n        name=\'bias_1\',\n        shape=[out_channels_1],\n        dtype=tf.float32,\n        initializer=tf.zeros_initializer())\n\n    conv_1 = tf.nn.conv3d(pooling_0, weights_1, strides=[1, 1, 1, 1, 1], padding=""SAME"")\n    log.info(\'conv_1 shape: %s\' % conv_1.shape)\n    conv_1 = conv_1 + bias_1\n\n    if add_relu:\n        conv_1 = tf.nn.elu(conv_1)\n\n    pooling_1 = tf.nn.max_pool3d(\n        conv_1,\n        ksize=[1, 3, 3, 3, 1],\n        strides=[1, 3, 3, 3, 1],\n        padding=""SAME"")\n    log.info(\'pooling_1 shape: %s\' % pooling_1.shape)\n\n    return tf.contrib.layers.flatten(pooling_1)\n\n\ndef CNN_3d_2d(x, out_channels_0, out_channels_1, add_relu=True):\n    \'\'\'Add a 3d convlution layer with relu and max pooling layer.\n\n    Args:\n        x: a tensor with shape [batch, in_depth, in_height, in_width, in_channels]\n        out_channels: a number\n        filter_size: a number\n        pooling_size: a number\n\n    Returns:\n        a flattened tensor with shape [batch, num_features]\n\n    Raises:\n    \'\'\'\n    in_channels = x.shape[-1]\n    weights_0 = tf.get_variable(\n        name=\'filter_0\',\n        shape=[1, 3, 3, in_channels, out_channels_0],\n        dtype=tf.float32,\n        initializer=tf.random_uniform_initializer(-0.01, 0.01))\n    bias_0 = tf.get_variable(\n        name=\'bias_0\',\n        shape=[out_channels_0],\n        dtype=tf.float32,\n        initializer=tf.zeros_initializer())\n\n    conv_0 = tf.nn.conv3d(x, weights_0, strides=[1, 1, 1, 1, 1], padding=""SAME"")\n    log.info(\'conv_0 shape: %s\' % conv_0.shape)\n    conv_0 = conv_0 + bias_0\n\n    if add_relu:\n        conv_0 = tf.nn.elu(conv_0)\n\n    pooling_0 = tf.nn.max_pool3d(\n        conv_0,\n        ksize=[1, 1, 3, 3, 1],\n        strides=[1, 1, 3, 3, 1],\n        padding=""SAME"")\n    log.info(\'pooling_0 shape: %s\' % pooling_0.shape)\n\n    # layer_1\n    weights_1 = tf.get_variable(\n        name=\'filter_1\',\n        shape=[1, 3, 3, out_channels_0, out_channels_1],\n        dtype=tf.float32,\n        initializer=tf.random_uniform_initializer(-0.01, 0.01))\n    bias_1 = tf.get_variable(\n        name=\'bias_1\',\n        shape=[out_channels_1],\n        dtype=tf.float32,\n        initializer=tf.zeros_initializer())\n\n    conv_1 = tf.nn.conv3d(pooling_0, weights_1, strides=[1, 1, 1, 1, 1], padding=""SAME"")\n    log.info(\'conv_1 shape: %s\' % conv_1.shape)\n    conv_1 = conv_1 + bias_1\n\n    if add_relu:\n        conv_1 = tf.nn.elu(conv_1)\n\n    pooling_1 = tf.nn.max_pool3d(\n        conv_1,\n        ksize=[1, 1, 3, 3, 1],\n        strides=[1, 1, 3, 3, 1],\n        padding=""SAME"")\n    log.info(\'pooling_1 shape: %s\' % pooling_1.shape)\n\n    return tf.contrib.layers.flatten(pooling_1)\n\n\ndef CNN_3d_change(x, out_channels_0, out_channels_1, add_relu=True):\n    \'\'\'Add a 3d convlution layer with relu and max pooling layer.\n\n    Args:\n        x: a tensor with shape [batch, in_depth, in_height, in_width, in_channels]\n        out_channels: a number\n        filter_size: a number\n        pooling_size: a number\n\n    Returns:\n        a flattened tensor with shape [batch, num_features]\n\n    Raises:\n    \'\'\'\n    in_channels = x.shape[-1]\n    weights_0 = tf.get_variable(\n        name=\'filter_0\',\n        shape=[3, 3, 3, in_channels, out_channels_0],\n        dtype=tf.float32,\n        # initializer=tf.random_normal_initializer(0, 0.05))\n        initializer=tf.random_uniform_initializer(-0.01, 0.01))\n    bias_0 = tf.get_variable(\n        name=\'bias_0\',\n        shape=[out_channels_0],\n        dtype=tf.float32,\n        initializer=tf.zeros_initializer())\n    # Todo\n    g_0 = tf.get_variable(name=\'scale_0\',\n                          shape=[out_channels_0],\n                          dtype=tf.float32,\n                          initializer=tf.ones_initializer())\n    weights_0 = tf.reshape(g_0, [1, 1, 1, out_channels_0]) * tf.nn.l2_normalize(weights_0, [0, 1, 2])\n\n    conv_0 = tf.nn.conv3d(x, weights_0, strides=[1, 1, 1, 1, 1], padding=""VALID"")\n    log.info(\'conv_0 shape: %s\' % conv_0.shape)\n    conv_0 = conv_0 + bias_0\n    #######\n    \'\'\'\n    with tf.variable_scope(\'layer_0\'):\n        conv_0 = op.layer_norm(conv_0, axis=[1, 2, 3, 4])\n        log.info(\'layer_norm in cnn\')\n    \'\'\'\n    if add_relu:\n        conv_0 = tf.nn.elu(conv_0)\n\n    pooling_0 = tf.nn.max_pool3d(\n        conv_0,\n        ksize=[1, 2, 3, 3, 1],\n        strides=[1, 2, 3, 3, 1],\n        padding=""VALID"")\n    log.info(\'pooling_0 shape: %s\' % pooling_0.shape)\n\n    # layer_1\n    weights_1 = tf.get_variable(\n        name=\'filter_1\',\n        shape=[2, 2, 2, out_channels_0, out_channels_1],\n        dtype=tf.float32,\n        initializer=tf.random_uniform_initializer(-0.01, 0.01))\n\n    bias_1 = tf.get_variable(\n        name=\'bias_1\',\n        shape=[out_channels_1],\n        dtype=tf.float32,\n        initializer=tf.zeros_initializer())\n\n    g_1 = tf.get_variable(name=\'scale_1\',\n                          shape=[out_channels_1],\n                          dtype=tf.float32,\n                          initializer=tf.ones_initializer())\n    weights_1 = tf.reshape(g_1, [1, 1, 1, out_channels_1]) * tf.nn.l2_normalize(weights_1, [0, 1, 2])\n\n    conv_1 = tf.nn.conv3d(pooling_0, weights_1, strides=[1, 1, 1, 1, 1], padding=""VALID"")\n    log.info(\'conv_1 shape: %s\' % conv_1.shape)\n    conv_1 = conv_1 + bias_1\n    # with tf.variable_scope(\'layer_1\'):\n    #    conv_1 = op.layer_norm(conv_1, axis=[1, 2, 3, 4])\n\n    if add_relu:\n        conv_1 = tf.nn.elu(conv_1)\n\n    pooling_1 = tf.nn.max_pool3d(\n        conv_1,\n        ksize=[1, 3, 3, 3, 1],\n        strides=[1, 3, 3, 3, 1],\n        padding=""VALID"")\n    log.info(\'pooling_1 shape: %s\' % pooling_1.shape)\n\n    return tf.contrib.layers.flatten(pooling_1)\n\n\ndef RNN_last_state(x, lengths, hidden_size):\n    \'\'\'encode x with a gru cell and return the last state.\n    \n    Args:\n        x: a tensor with shape [batch, time, dimension]\n        length: a tensor with shape [batch]\n\n    Return:\n        a tensor with shape [batch, hidden_size]\n\n    Raises:\n    \'\'\'\n    cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n    outputs, last_states = tf.nn.dynamic_rnn(cell, x, lengths, dtype=tf.float32)\n    return outputs, last_states\n'"
deeppavlov/models/ranking/matching_models/dam_utils/operations.py,85,"b'# Copyright 2018 Neural Networks and Deep Learning lab, MIPT\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# @inproceedings{ ,\n#   title={Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network},\n#   author={Xiangyang Zhou, Lu Li, Daxiang Dong, Yi Liu, Ying Chen, Wayne Xin Zhao, Dianhai Yu and Hua Wu},\n#   booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n#   volume={1},\n#   pages={  --  },\n#   year={2018}\n# }\n# ```\n# http://aclweb.org/anthology/P18-1103\n#\n# Based on authors\' Tensorflow code: https://github.com/baidu/Dialogue/tree/master/DAM\n\nimport math\nfrom logging import getLogger\n\nimport numpy as np\nimport tensorflow as tf\nfrom scipy.stats import multivariate_normal\n\nlog = getLogger(__name__)\n\n\ndef learning_rate(step_num, d_model=512, warmup_steps=4000):\n    a = step_num ** (-0.5)\n    b = step_num * warmup_steps ** (-1.5)\n    return a, b, d_model ** (-0.5) * min(step_num ** (-0.5), step_num * (warmup_steps ** (-1.5)))\n\n\ndef selu(x):\n    alpha = 1.6732632423543772848170429916717\n    scale = 1.0507009873554804934193349852946\n    log.info(\'use selu\')\n    return scale * tf.where(x >= 0.0, x, alpha * tf.nn.elu(x))\n\n\ndef bilinear_sim_4d(x, y, is_nor=True):\n    \'\'\'calulate bilinear similarity with two 4d tensor.\n    \n    Args:\n        x: a tensor with shape [batch, time_x, dimension_x, num_stacks]\n        y: a tensor with shape [batch, time_y, dimension_y, num_stacks]\n\n    Returns:\n        a tensor with shape [batch, time_x, time_y, num_stacks]\n\n    Raises:\n        ValueError: if\n            the shapes of x and y are not match;\n            bilinear matrix reuse error.\n    \'\'\'\n    M = tf.get_variable(\n        name=""bilinear_matrix"",\n        shape=[x.shape[2], y.shape[2], x.shape[3]],\n        dtype=tf.float32,\n        initializer=tf.orthogonal_initializer())\n    sim = tf.einsum(\'biks,kls,bjls->bijs\', x, M, y)\n\n    if is_nor:\n        scale = tf.sqrt(tf.cast(x.shape[2] * y.shape[2], tf.float32))\n        scale = tf.maximum(1.0, scale)\n        return sim / scale\n    else:\n        return sim\n\n\ndef bilinear_sim(x, y, is_nor=True):\n    \'\'\'calculate bilinear similarity with two tensor.\n    Args:\n        x: a tensor with shape [batch, time_x, dimension_x]\n        y: a tensor with shape [batch, time_y, dimension_y]\n    \n    Returns:\n        a tensor with shape [batch, time_x, time_y]\n    Raises:\n        ValueError: if\n            the shapes of x and y are not match;\n            bilinear matrix reuse error.\n    \'\'\'\n    M = tf.get_variable(\n        name=""bilinear_matrix"",\n        shape=[x.shape[-1], y.shape[-1]],\n        dtype=tf.float32,\n        # initializer=tf.orthogonal_initializer())\n        initializer=tf.keras.initializers.glorot_uniform(seed=42))\n    sim = tf.einsum(\'bik,kl,bjl->bij\', x, M, y)\n\n    if is_nor:\n        scale = tf.sqrt(tf.cast(x.shape[-1] * y.shape[-1], tf.float32))\n        scale = tf.maximum(1.0, scale)\n        return sim / scale\n    else:\n        return sim\n\n\ndef dot_sim(x, y, is_nor=True):\n    \'\'\'calculate dot similarity with two tensor.\n\n    Args:\n        x: a tensor with shape [batch, time_x, dimension]\n        y: a tensor with shape [batch, time_y, dimension]\n    \n    Returns:\n        a tensor with shape [batch, time_x, time_y]\n    Raises:\n        AssertionError: if\n            the shapes of x and y are not match.\n    \'\'\'\n    assert x.shape[-1] == y.shape[-1]\n\n    sim = tf.einsum(\'bik,bjk->bij\', x, y)\n\n    if is_nor:\n        scale = tf.sqrt(tf.cast(x.shape[-1], tf.float32))\n        scale = tf.maximum(1.0, scale)\n        return sim / scale\n    else:\n        return sim\n\n\ndef layer_norm(x, axis=None, epsilon=1e-6):\n    \'\'\'Add layer normalization.\n\n    Args:\n        x: a tensor\n        axis: the dimensions to normalize\n\n    Returns:\n        a tensor the same shape as x.\n\n    Raises:\n    \'\'\'\n    log.info(\'wrong version of layer_norm\')\n    scale = tf.get_variable(\n        name=\'scale\',\n        shape=[1],\n        dtype=tf.float32,\n        initializer=tf.ones_initializer())\n    bias = tf.get_variable(\n        name=\'bias\',\n        shape=[1],\n        dtype=tf.float32,\n        initializer=tf.zeros_initializer())\n\n    if axis is None:\n        axis = [-1]\n\n    mean = tf.reduce_mean(x, axis=axis, keepdims=True)\n    variance = tf.reduce_mean(tf.square(x - mean), axis=axis, keepdims=True)\n    norm = (x - mean) * tf.rsqrt(variance + epsilon)\n    return scale * norm + bias\n\n\ndef layer_norm_debug(x, axis=None, epsilon=1e-6):\n    \'\'\'Add layer normalization.\n\n    Args:\n        x: a tensor\n        axis: the dimensions to normalize\n\n    Returns:\n        a tensor the same shape as x.\n\n    Raises:\n    \'\'\'\n    if axis is None:\n        axis = [-1]\n    shape = [x.shape[i] for i in axis]\n\n    scale = tf.get_variable(\n        name=\'scale\',\n        shape=shape,\n        dtype=tf.float32,\n        initializer=tf.ones_initializer())\n    bias = tf.get_variable(\n        name=\'bias\',\n        shape=shape,\n        dtype=tf.float32,\n        initializer=tf.zeros_initializer())\n\n    mean = tf.reduce_mean(x, axis=axis, keepdims=True)\n    variance = tf.reduce_mean(tf.square(x - mean), axis=axis, keepdims=True)\n    norm = (x - mean) * tf.rsqrt(variance + epsilon)\n    return scale * norm + bias\n\n\ndef dense(x, out_dimension=None, add_bias=True, initializer=tf.orthogonal_initializer()):\n    \'\'\'Add dense connected layer, Wx + b.\n\n    Args:\n        x: a tensor with shape [batch, time, dimension]\n        out_dimension: a number which is the output dimension\n\n    Return:\n        a tensor with shape [batch, time, out_dimension]\n\n    Raises:\n    \'\'\'\n    if out_dimension is None:\n        out_dimension = x.shape[-1]\n\n    W = tf.get_variable(\n        name=\'weights\',\n        shape=[x.shape[-1], out_dimension],\n        dtype=tf.float32,\n        initializer=initializer)\n    if add_bias:\n        bias = tf.get_variable(\n            name=\'bias\',\n            shape=[1],\n            dtype=tf.float32,\n            initializer=tf.zeros_initializer())\n        return tf.einsum(\'bik,kj->bij\', x, W) + bias\n    else:\n        return tf.einsum(\'bik,kj->bij\', x, W)\n\n\ndef matmul_2d(x, out_dimension, drop_prob=None):\n    \'\'\'Multiplies 2-d tensor by weights.\n\n    Args:\n        x: a tensor with shape [batch, dimension]\n        out_dimension: a number\n\n    Returns:\n        a tensor with shape [batch, out_dimension]\n\n    Raises:\n    \'\'\'\n    W = tf.get_variable(\n        name=\'weights\',\n        shape=[x.shape[1], out_dimension],\n        dtype=tf.float32,\n        initializer=tf.orthogonal_initializer())\n    if drop_prob is not None:\n        W = tf.nn.dropout(W, drop_prob)\n        log.info(\'W is dropout\')\n\n    return tf.matmul(x, W)\n\n\ndef gauss_positional_encoding_vector(x, role=0, value=0):\n    position = int(x.shape[1])\n    dimension = int(x.shape[2])\n    log.info(\'position: %s\' % position)\n    log.info(\'dimension: %s\' % dimension)\n\n    _lambda = tf.get_variable(\n        name=\'lambda\',\n        shape=[position],\n        dtype=tf.float32,\n        initializer=tf.constant_initializer(value))\n    _lambda = tf.expand_dims(_lambda, axis=-1)\n\n    mean = [position / 2.0, dimension / 2.0]\n\n    # cov = [[position/3.0, 0], [0, dimension/3.0]]\n    sigma_x = position / math.sqrt(4.0 * dimension)\n    sigma_y = math.sqrt(dimension / 4.0)\n    cov = [[sigma_x * sigma_x, role * sigma_x * sigma_y],\n           [role * sigma_x * sigma_y, sigma_y * sigma_y]]\n\n    pos = np.dstack(np.mgrid[0:position, 0:dimension])\n\n    rv = multivariate_normal(mean, cov)\n    signal = rv.pdf(pos)\n    signal = signal - np.max(signal) / 2.0\n\n    signal = tf.multiply(_lambda, signal)\n    signal = tf.expand_dims(signal, axis=0)\n\n    log.info(\'gauss positional encoding\')\n\n    return x + _lambda * signal\n\n\ndef positional_encoding(x, min_timescale=1.0, max_timescale=1.0e4, value=0):\n    \'\'\'Adds a bunch of sinusoids of different frequencies to a tensor.\n\n    Args:\n        x: a tensor with shape [batch, length, channels]\n        min_timescale: a float\n        max_timescale: a float\n\n    Returns:\n        a tensor the same shape as x.\n\n    Raises:\n    \'\'\'\n    length = x.shape[1]\n    channels = x.shape[2]\n    _lambda = tf.get_variable(\n        name=\'lambda\',\n        shape=[1],\n        dtype=tf.float32,\n        initializer=tf.constant_initializer(value))\n\n    position = tf.to_float(tf.range(length))\n    num_timescales = channels // 2\n    log_timescale_increment = (\n            math.log(float(max_timescale) / float(min_timescale)) /\n            (tf.to_float(num_timescales) - 1))\n    inv_timescales = min_timescale * tf.exp(\n        tf.to_float(tf.range(num_timescales)) * -log_timescale_increment)\n    scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(inv_timescales, 0)\n    signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n    signal = tf.pad(signal, [[0, 0], [0, tf.mod(channels, 2)]])\n    # signal = tf.reshape(signal, [1, length, channels])\n    signal = tf.expand_dims(signal, axis=0)\n\n    return x + _lambda * signal\n\n\ndef positional_encoding_vector(x, min_timescale=1.0, max_timescale=1.0e4, value=0):\n    \'\'\'Adds a bunch of sinusoids of different frequencies to a tensor.\n\n    Args:\n        x: a tensor with shape [batch, length, channels]\n        min_timescale: a float\n        max_timescale: a float\n\n    Returns:\n        a tensor the same shape as x.\n\n    Raises:\n    \'\'\'\n    length = x.shape[1]\n    channels = x.shape[2]\n    _lambda = tf.get_variable(\n        name=\'lambda\',\n        shape=[length],\n        dtype=tf.float32,\n        initializer=tf.constant_initializer(value))\n    _lambda = tf.expand_dims(_lambda, axis=-1)\n\n    position = tf.to_float(tf.range(length))\n    num_timescales = channels // 2\n    log_timescale_increment = (\n            math.log(float(max_timescale) / float(min_timescale)) /\n            (tf.to_float(num_timescales) - 1))\n    inv_timescales = min_timescale * tf.exp(\n        tf.to_float(tf.range(num_timescales)) * -log_timescale_increment)\n    scaled_time = tf.expand_dims(position, 1) * tf.expand_dims(inv_timescales, 0)\n    signal = tf.concat([tf.sin(scaled_time), tf.cos(scaled_time)], axis=1)\n    signal = tf.pad(signal, [[0, 0], [0, tf.mod(channels, 2)]])\n\n    signal = tf.multiply(_lambda, signal)\n    signal = tf.expand_dims(signal, axis=0)\n\n    return x + signal\n\n\ndef mask(row_lengths, col_lengths, max_row_length, max_col_length):\n    \'\'\'Return a mask tensor representing the first N positions of each row and each column.\n\n    Args:\n        row_lengths: a tensor with shape [batch]\n        col_lengths: a tensor with shape [batch]\n\n    Returns:\n        a mask tensor with shape [batch, max_row_length, max_col_length]\n\n    Raises:\n    \'\'\'\n    row_mask = tf.sequence_mask(row_lengths, max_row_length)  # bool, [batch, max_row_len]\n    col_mask = tf.sequence_mask(col_lengths, max_col_length)  # bool, [batch, max_col_len]\n\n    row_mask = tf.cast(tf.expand_dims(row_mask, -1), tf.float32)\n    col_mask = tf.cast(tf.expand_dims(col_mask, -1), tf.float32)\n\n    return tf.einsum(\'bik,bjk->bij\', row_mask, col_mask)\n\n\ndef weighted_sum(weight, values):\n    \'\'\'Calcualte the weighted sum.\n\n    Args:\n        weight: a tensor with shape [batch, time, dimension]\n        values: a tensor with shape [batch, dimension, values_dimension]\n\n    Return:\n        a tensor with shape [batch, time, values_dimension]\n\n    Raises:\n    \'\'\'\n    return tf.einsum(\'bij,bjk->bik\', weight, values)\n'"
