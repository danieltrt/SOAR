file_path,api_count,code
config.py,0,"b""import argparse\n\ndef str2floatarr(v):\n    if type(v) == str:\n        try:\n            return [float(v) for v in v.split(',')]\n        except:\n            raise argparse.ArgumentTypeError('Integers separated by commas expected.')\n    else:\n        raise argparse.ArgumentTypeError('Integers separated by commas expected.')\n\ndef str2bool(v):\n    if v.lower() in ('yes', 'true', 't', 'y', '1', True):\n        return True\n    elif v.lower() in ('no', 'false', 'f', 'n', '0', False):\n        return False\n    else:\n        raise argparse.ArgumentTypeError('Boolean value expected.')\n\ndef get_args():\n\tparser = argparse.ArgumentParser(description='')\n\n\t#Input output folders settings\n\tparser.add_argument('--decensor_input_path', default='./decensor_input/', help='input images with censored regions colored green to be decensored by decensor.py path')\n\tparser.add_argument('--decensor_input_original_path', default='./decensor_input_original/', help='input images with no modifications to be decensored by decensor.py path')\n\tparser.add_argument('--decensor_output_path', default='./decensor_output/', help='output images generated from running decensor.py path')\n\n\t#Decensor settings\n\tparser.add_argument('--mask_color_red', default=0, help='red channel of mask color in decensoring')\n\tparser.add_argument('--mask_color_green', default=255, help='green channel of mask color in decensoring')\n\tparser.add_argument('--mask_color_blue', default=0, help='blue channel of mask color in decensoring')\n\tparser.add_argument('--is_mosaic', type=str2bool, default='False', help='true if image has mosaic censoring, false otherwise')\n\tparser.add_argument('--variations', type=int, choices=[1, 2, 4], default=1, help='number of decensor variations to be generated')\n\n\t#Other settings\n\tparser.add_argument('--ui_mode', default=False, help='true if you want ui mode, false if you want command line interface')\n\n\targs = parser.parse_args()\n\treturn args\n\nif __name__ == '__main__':\n\tget_args()\n"""
decensor.py,1,"b'\ntry:\n    # default library\n    import os, logging, sys, config\nexcept ImportError as e:\n    print(""Error when importing DEFAULT library : "", e)\n    print(""\\nIf you made script named [\\""os.py\\"", \\""logging.py\\"", \\""sys.py\\"", \\""config.py\\""] rename it"")\n    print(""If not, re-install python or check your Python environment variables"")\ntry:\n    # local library\n    import file\n    from model import InpaintNN\n    from libs.utils import *\n    # external library\n    import numpy as np\n    from PIL import Image\n    import tensorflow as tf\n    from PySide2 import QtCore # for QThread\nexcept ImportError as e:\n    print(""\\n""+ \'=\'*20 + "" ImportError "" + ""="" * 20 + ""\\n"")\n    if e.__class__.__name__ == ""ModuleNotFoundError"":\n        print(e)\n        print(""Python libraries are missing. You can install all required libraries by running in the command line (terminal)"")\n        print(""cpu version : pip install -r requirements-cpu.txt"")\n        print(""gpu version : pip install -r requirements-gpu.txt"")\n    else:\n        print(""Error when importing libraries: "", e)\n    print(""\\nIf pip doesn\'t work, try update through Anaconda"")\n    print(""install Anaconda : https://www.anaconda.com/distribution/ \\n"")\n\nclass Decensor(QtCore.QThread):\n    def __init__(self, parentThread = None, text_edit = None, text_cursor = None, ui_mode = None):\n        super().__init__(parentThread)\n        args = config.get_args()\n        self.is_mosaic = args.is_mosaic\n        self.variations = args.variations\n        self.mask_color = [args.mask_color_red/255.0, args.mask_color_green/255.0, args.mask_color_blue/255.0]\n        self.decensor_input_path = args.decensor_input_path\n        self.decensor_input_original_path = args.decensor_input_original_path\n        self.decensor_output_path = args.decensor_output_path\n\n        self.signals = None # Signals class will be given by progressWindow\n\n        self.model = None\n        self.warm_up = False\n\n        # if ui_mode is not None:\n        #     self.ui_mode = ui_mode\n        # else:\n        #     self.ui_mode = args.ui_mode\n        #\n        # if self.ui_mode:\n        #     self.text_edit = text_edit\n        #     self.text_cursor = text_cursor\n        #     self.ui_mode = True\n\n        if not os.path.exists(self.decensor_output_path):\n            os.makedirs(self.decensor_output_path)\n\n    def run(self):\n        if not self.warm_up :\n            print(""if self.warm_up :"")\n            self.load_model()\n            return\n        elif self.warm_up:\n            print(""elif not self.warm_up:"")\n            self.decensor_all_images_in_folder()\n\n    def stop(self):\n        # in case of stopping decensor, terminate not to run if self while MainWindow is closed\n        self.terminate()\n\n    def find_mask(self, colored):\n        # self.signals.update_progress_LABEL.emit(""find_mask()"", ""finding mask..."")\n        mask = np.ones(colored.shape, np.uint8)\n        i, j = np.where(np.all(colored[0] == self.mask_color, axis=-1))\n        mask[0, i, j] = 0\n        return mask\n\n    def load_model(self):\n        self.signals.insertText_progressCursor.emit(""Loading model ... please wait ...\\n"")\n        if self.model is None :\n            self.model = InpaintNN(bar_model_name = ""./models/bar/Train_775000.meta"",\n                                   bar_checkpoint_name = ""./models/bar/"",\n                                   mosaic_model_name = ""./models/mosaic/Train_290000.meta"",\n                                   mosaic_checkpoint_name = ""./models/mosaic/"",\n                                   is_mosaic=self.is_mosaic)\n        self.warm_up = True\n        print(""load model finished"")\n        self.signals.insertText_progressCursor.emit(""Loading model finished!\\n"")\n        self.signals.update_decensorButton_Text.emit(""Decensor Your Images"")\n        self.signals.update_decensorButton_Enabled.emit(True)\n\n    def decensor_all_images_in_folder(self):\n        #load model once at beginning and reuse same model\n        if not self.warm_up :\n            # incase of running by source code\n            self.load_model()\n\n        input_color_dir = self.decensor_input_path\n        file_names = os.listdir(input_color_dir)\n\n        input_dir = self.decensor_input_path\n        output_dir = self.decensor_output_path\n\n        # Change False to True before release --> file.check_file(input_dir, output_dir, True)\n        # self.signals.update_progress_LABEL.emit(""file.check_file()"", ""Checking image files and directory..."")\n        self.signals.insertText_progressCursor.emit(""Checking image files and directory...\\n"")\n\n        file_names, self.files_removed = file.check_file(input_dir, output_dir, False)\n\n        # self.signals.total_ProgressBar_update_MAX_VALUE.emit(""set total progress bar MaxValue : ""+str(len(file_names)),len(file_names))\n        \'\'\'\n        print(""set total progress bar MaxValue : ""+str(len(file_names)))\n        self.signals.update_ProgressBar_MAX_VALUE.emit(len(file_names))\n        \'\'\'\n        self.signals.insertText_progressCursor.emit(""Decensoring {} image files\\n"".format(len(file_names)))\n\n        #convert all images into np arrays and put them in a list\n        for n, file_name in enumerate(file_names, start = 1):\n            # self.signals.total_ProgressBar_update_VALUE.emit(""Decensoring {} / {}"".format(n, len(file_names)), n)\n            \'\'\'\n            self.update_ProgressBar_SET_VALUE.emit(n)\n            print(""Decensoring {} / {}"".format(n, len(file_names)))\n            \'\'\'\n            self.signals.insertText_progressCursor.emit(""Decensoring image file : {}\\n"".format(file_name))\n\n            # signal progress bar value == masks decensored on image ,\n            # e.g) sample image : 17\n            # self.signals.signal_ProgressBar_update_VALUE.emit(""reset value"", 0) # set to 0 for every image at start\n            # self.signals.update_progress_LABEL.emit(""for-loop, \\""for file_name in file_names:\\"""",""Decensoring : ""+str(file_name))\n\n            color_file_path = os.path.join(input_color_dir, file_name)\n            color_basename, color_ext = os.path.splitext(file_name)\n            if os.path.isfile(color_file_path) and color_ext.casefold() == "".png"":\n                print(""--------------------------------------------------------------------------"")\n                print(""Decensoring the image {}\\n"".format(color_file_path))\n                try :\n                    colored_img = Image.open(color_file_path)\n                except:\n                    print(""Cannot identify image file ("" +str(color_file_path)+"")"")\n                    self.files_removed.append((color_file_path,3))\n                    # incase of abnormal file format change (ex : text.txt -> text.png)\n                    continue\n\n                #if we are doing a mosaic decensor\n                if self.is_mosaic:\n                    #get the original file that hasn\'t been colored\n                    ori_dir = self.decensor_input_original_path\n                    test_file_names = os.listdir(ori_dir)\n                    #since the original image might not be a png, test multiple file formats\n                    valid_formats = {"".png"", "".jpg"", "".jpeg""}\n                    for test_file_name in test_file_names:\n                        test_basename, test_ext = os.path.splitext(test_file_name)\n                        if (test_basename == color_basename) and (test_ext.casefold() in valid_formats):\n                            ori_file_path = os.path.join(ori_dir, test_file_name)\n                            ori_img = Image.open(ori_file_path)\n                            # colored_img.show()\n                            self.decensor_image_variations(ori_img, colored_img, file_name)\n                            break\n                    else: #for...else, i.e if the loop finished without encountering break\n                        print(""Corresponding original, uncolored image not found in {}"".format(color_file_path))\n                        print(""Check if it exists and is in the PNG or JPG format."")\n                        self.signals.insertText_progressCursor.emit(""Corresponding original, uncolored image not found in {}\\n"".format(color_file_path))\n                        self.signals.insertText_progressCursor.emit(""Check if it exists and is in the PNG or JPG format.\\n"")\n                #if we are doing a bar decensor\n                else:\n                    self.decensor_image_variations(colored_img, colored_img, file_name)\n            else:\n                print(""--------------------------------------------------------------------------"")\n                print(""Image can\'t be found: ""+str(color_file_path))\n                self.signals.insertText_progressCursor.emit(""Image can\'t be found: ""+str(color_file_path) + ""\\n"")\n\n        print(""--------------------------------------------------------------------------"")\n        if self.files_removed is not None:\n            file.error_messages(None, self.files_removed)\n        print(""\\nDecensoring complete!"")\n\n        #unload model to prevent memory issues\n        # self.signals.update_progress_LABEL.emit(""finished"", ""Decensoring complete! Close this window and reopen DCP to start a new session."")\n        self.signals.insertText_progressCursor.emit(""\\nDecensoring complete! remove decensored file before decensoring again not to overwrite"")\n        self.signals.update_decensorButton_Enabled.emit(True)\n        tf.reset_default_graph()\n\n    def decensor_image_variations(self, ori, colored, file_name=None):\n        for i in range(self.variations):\n            self.decensor_image_variation(ori, colored, i, file_name)\n\n    #create different decensors of the same image by flipping the input image\n    def apply_variant(self, image, variant_number):\n        if variant_number == 0:\n            return image\n        elif variant_number == 1:\n            return image.transpose(Image.FLIP_LEFT_RIGHT)\n        elif variant_number == 2:\n            return image.transpose(Image.FLIP_TOP_BOTTOM)\n        else:\n            return image.transpose(Image.FLIP_LEFT_RIGHT).transpose(Image.FLIP_TOP_BOTTOM)\n\n    #decensors one image at a time\n    #TODO: decensor all cropped parts of the same image in a batch (then i need input for colored an array of those images and make additional changes)\n    def decensor_image_variation(self, ori, colored, variant_number, file_name):\n        ori = self.apply_variant(ori, variant_number)\n        colored = self.apply_variant(colored, variant_number)\n        width, height = ori.size\n        #save the alpha channel if the image has an alpha channel\n        has_alpha = False\n        if (ori.mode == ""RGBA""):\n            has_alpha = True\n            alpha_channel = np.asarray(ori)[:,:,3]\n            alpha_channel = np.expand_dims(alpha_channel, axis =-1)\n            ori = ori.convert(\'RGB\')\n\n        ori_array = image_to_array(ori)\n        ori_array = np.expand_dims(ori_array, axis = 0)\n\n        if self.is_mosaic:\n            #if mosaic decensor, mask is empty\n            # mask = np.ones(ori_array.shape, np.uint8)\n            # print(mask.shape)\n            colored = colored.convert(\'RGB\')\n            color_array = image_to_array(colored)\n            color_array = np.expand_dims(color_array, axis = 0)\n            mask = self.find_mask(color_array)\n            mask_reshaped = mask[0,:,:,:] * 255.0\n            mask_img = Image.fromarray(mask_reshaped.astype(\'uint8\'))\n            # mask_img.show()\n\n        else:\n            mask = self.find_mask(ori_array)\n\n        #colored image is only used for finding the regions\n        regions = find_regions(colored.convert(\'RGB\'), [v*255 for v in self.mask_color])\n        print(""Found {region_count} censored regions in this image!"".format(region_count = len(regions)))\n        self.signals.insertText_progressCursor.emit(""Found {region_count} censored regions in this image!"".format(region_count = len(regions)))\n\n        if len(regions) == 0 and not self.is_mosaic:\n            print(""No green (0,255,0) regions detected! Make sure you\'re using exactly the right color."")\n            self.signals.insertText_progressCursor.emit(""No green (0,255,0) regions detected! Make sure you\'re using exactly the right color.\\n"")\n            return\n\n        # self.signals.signal_ProgressBar_update_MAX_VALUE.emit(""Found {} masked regions"".format(len(regions)), len(regions))\n        print(""Found {} masked regions"".format(len(regions)))\n\n        # self.signals.insertText_progressCursor.emit(""Found {} masked regions\\n"".format(len(regions)))\n        self.signals.update_ProgressBar_MAX_VALUE.emit(len(regions))\n        self.signals.update_ProgressBar_SET_VALUE.emit(0)\n\n        output_img_array = ori_array[0].copy()\n\n        for region_counter, region in enumerate(regions, 1):\n            # self.signals.update_progress_LABEL.emit(""\\""Decensoring regions in image\\"""",""Decensoring censor {}/{}"".format(region_counter,len(regions)))\n            self.signals.insertText_progressCursor.emit(""Decensoring regions in image, Decensoring censor {}/{}"".format(region_counter,len(regions)))\n            bounding_box = expand_bounding(ori, region, expand_factor=1.5)\n            crop_img = ori.crop(bounding_box)\n            # crop_img.show()\n            #convert mask back to image\n            mask_reshaped = mask[0,:,:,:] * 255.0\n            mask_img = Image.fromarray(mask_reshaped.astype(\'uint8\'))\n            #resize the cropped images\n            crop_img = crop_img.resize((256, 256))\n            crop_img_array = image_to_array(crop_img)\n            #resize the mask images\n            mask_img = mask_img.crop(bounding_box)\n            mask_img = mask_img.resize((256, 256))\n            # mask_img.show()\n            #convert mask_img back to array\n            mask_array = image_to_array(mask_img)\n            #the mask has been upscaled so there will be values not equal to 0 or 1\n\n            # mask_array[mask_array > 0] = 1\n            # crop_img_array[..., :-1][mask_array==0] = (0,0,0)\n\n            if not self.is_mosaic:\n                a, b = np.where(np.all(mask_array == 0, axis = -1))\n                # print(a,b)\n                # print(crop_img_array[a,b])\n                # print(crop_img_array[a,b,0])\n                # print(crop_img_array.shape)\n                # print(type(crop_img_array[0,0]))\n                crop_img_array[a,b,:] = 0.\n            # temp = Image.fromarray((crop_img_array * 255.0).astype(\'uint8\'))\n            # temp.show()\n\n            crop_img_array = np.expand_dims(crop_img_array, axis = 0)\n            mask_array = np.expand_dims(mask_array, axis = 0)\n\n            # print(np.amax(crop_img_array))\n            # print(np.amax(mask_array))\n            # print(np.amax(masked))\n\n            # print(np.amin(crop_img_array))\n            # print(np.amin(mask_array))\n            # print(np.amin(masked))\n\n            # print(mask_array)\n\n            crop_img_array = crop_img_array * 2.0 - 1\n            # mask_array = mask_array / 255.0\n\n            # Run predictions for this batch of images\n            pred_img_array = self.model.predict(crop_img_array, crop_img_array, mask_array)\n\n            pred_img_array = np.squeeze(pred_img_array, axis = 0)\n            pred_img_array = (255.0 * ((pred_img_array + 1.0) / 2.0)).astype(np.uint8)\n\n            #scale prediction image back to original size\n            bounding_width = bounding_box[2]-bounding_box[0]\n            bounding_height = bounding_box[3]-bounding_box[1]\n            #convert np array to image\n\n            # print(bounding_width,bounding_height)\n            # print(pred_img_array.shape)\n\n            pred_img = Image.fromarray(pred_img_array.astype(\'uint8\'))\n            # pred_img.show()\n            pred_img = pred_img.resize((bounding_width, bounding_height), resample = Image.BICUBIC)\n            # pred_img.show()\n\n            pred_img_array = image_to_array(pred_img)\n\n            # print(pred_img_array.shape)\n            pred_img_array = np.expand_dims(pred_img_array, axis = 0)\n\n            # copy the decensored regions into the output image\n            for i in range(len(ori_array)):\n                for col in range(bounding_width):\n                    for row in range(bounding_height):\n                        bounding_width_index = col + bounding_box[0]\n                        bounding_height_index = row + bounding_box[1]\n                        if (bounding_width_index, bounding_height_index) in region:\n                            output_img_array[bounding_height_index][bounding_width_index] = pred_img_array[i,:,:,:][row][col]\n            # self.signals.signal_ProgressBar_update_VALUE.emit(""{} out of {} regions decensored."".format(region_counter, len(regions)), region_counter)\n            self.signals.update_ProgressBar_SET_VALUE.emit(region_counter)\n            self.signals.insertText_progressCursor.emit(""{} out of {} regions decensored.\\n"".format(region_counter, len(regions)))\n            print(""{region_counter} out of {region_count} regions decensored."".format(region_counter=region_counter, region_count=len(regions)))\n\n        output_img_array = output_img_array * 255.0\n\n        #restore the alpha channel if the image had one\n        if has_alpha:\n            output_img_array = np.concatenate((output_img_array, alpha_channel), axis = 2)\n\n        output_img = Image.fromarray(output_img_array.astype(\'uint8\'))\n        output_img = self.apply_variant(output_img, variant_number)\n\n        # self.signals.update_progress_LABEL.emit(""current image finished"", ""Decensoring of current image finished. Saving image..."")\n        self.signals.insertText_progressCursor.emit(""Decensoring of current image finished. Saving image..."")\n        print(""current image finished"")\n\n        if file_name != None:\n            #save the decensored image\n            base_name, ext = os.path.splitext(file_name)\n            file_name = base_name + "" "" + str(variant_number) + ext\n            save_path = os.path.join(self.decensor_output_path, file_name)\n            output_img.save(save_path)\n            print(""Decensored image saved to {save_path}!"".format(save_path=save_path))\n            self.signals.insertText_progressCursor.emit(""Decensored image saved to {save_path}!"".format(save_path=save_path))\n            self.signals.insertText_progressCursor.emit(""=""*30)\n        else:\n            # Legacy Code piece \xe2\x86\x93, used when DCPv1 had ui with Painting\n            print(""Decensored image. Returning it."")\n            return output_img\n\n# if __name__ == \'__main__\':\n    # decensor = Decensor()\n    # decensor.decensor_all_images_in_folder()\n    # equivalent to decensor.start() (running as QtThread)\n'"
file.py,0,"b'import os\r\n\r\ndef check_file(input_dir, output_dir, release_version = True):\r\n    file_list = []\r\n    output_file_list = []\r\n    files_removed = []\r\n    input_dir = os.listdir(input_dir)\r\n    output_dir = os.listdir(output_dir)\r\n\r\n    for file_in in input_dir:\r\n        if not file_in.startswith(\'.\'):\r\n            file_list.append(file_in)\r\n\r\n    if release_version is True:\r\n        print(""\\nChecking valid files..."")\r\n        for file_out in output_dir:\r\n            if file_out.lower().endswith(\'.png\'):\r\n                output_file_list.append(file_out)\r\n\r\n        # solving https://github.com/deeppomf/DeepCreamPy/issues/25\r\n        # appending in list with reason as tuple (file name, reason)\r\n        for lhs in file_list:\r\n            lhs.lower()\r\n            if not lhs.lower().endswith(\'.png\') :\r\n                files_removed.append((lhs, 0))\r\n            for rhs in output_file_list:\r\n                if(lhs == rhs):\r\n                    files_removed.append((lhs, 1))\r\n\r\n        # seperated detecting same file names and deleting file name list\r\n        # just in case of index_error and show list of files which will not go though\r\n        # decensor process\r\n        print(""\\n\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83 These files will not be decensored for following reason  \xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\\n"")\r\n\r\n        error_messages(file_list, files_removed)\r\n        input(""\\nPress anything to continue..."")\r\n\r\n        print(""\\n\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\xef\xbc\x83\\n"")\r\n\r\n    return file_list, files_removed\r\n\r\ndef error_messages(file_list, files_removed):\r\n    if files_removed is None:\r\n        return\r\n\r\n    for remove_this, reason in files_removed:\r\n        if file_list is not None:\r\n            file_list.remove(remove_this)\r\n        if reason == 0:\r\n            print("" REMOVED : ("" + str(remove_this) +"")   is not PNG file format"")\r\n        elif reason == 1:\r\n            print("" REMOVED : ("" + str(remove_this) +"")   already exists"")\r\n        elif reason == 2:\r\n            print("" REMOVED : ("" + str(remove_this) +"")   file unreadable"")\r\n'"
main.py,0,"b'#!/usr/bin/python3\n\n#tooltips\n# Please read this tutorial on how to prepare your images for use with DeepCreamPy.\n# The greater the number of variations, the longer decensoring process will be.\n\nimport sys, time\nfrom PySide2.QtWidgets import QWidget, QHBoxLayout, QVBoxLayout, QGridLayout, QGroupBox, QDesktopWidget, QApplication\nfrom PySide2.QtWidgets import QAction, qApp, QApplication, QMessageBox, QRadioButton, QPushButton, QTextEdit, QLabel\nfrom PySide2.QtWidgets import QSizePolicy,QMainWindow, QStatusBar, QProgressBar\nfrom PySide2.QtCore import Qt, QObject\nfrom PySide2.QtGui import QFont, QTextCursor\nfrom decensor import Decensor\nfrom signals import Signals\n\n# from decensor import Decensor\n\n# from progressWindow import ProgressWindow\n\nclass MainWindow(QWidget):\n\n\tdef __init__(self):\n\t\tsuper().__init__()\n\t\tself.signals = Signals()\n\t\tself.initUI()\n\t\tself.setSignals()\n\t\tself.decensor = Decensor(self)\n\t\tself.load_model()\n\n\tdef initUI(self):\n\n\t\tgrid_layout = QGridLayout()\n\t\tgrid_layout.setSpacing(10)\n\t\tself.setLayout(grid_layout)\n\n\t\t#Tutorial\n\t\tself.tutorialLabel = QLabel()\n\t\tself.tutorialLabel.setText(""Welcome to DeepCreamPy!\\n\\nIf you\'re new to DCP, please read the README.\\nThis program does nothing without the proper setup of your images.\\n\\nReport any bugs you encounter to me on Github or Twitter @deeppomf."")\n\t\tself.tutorialLabel.setAlignment(Qt.AlignCenter)\n\t\tself.tutorialLabel.setFont(QFont(\'Sans Serif\', 13))\n\n\t\t#Censor type group\n\t\tself.censorTypeGroupBox = QGroupBox(\'Censor Type\')\n\n\t\tbarButton = QRadioButton(\'Bar censor\')\n\t\tmosaicButton = QRadioButton(\'Mosaic censor\')\n\t\tbarButton.setChecked(True)\n\n\t\tcensorLayout = QVBoxLayout()\n\t\tcensorLayout.addWidget(barButton)\n\t\tcensorLayout.addWidget(mosaicButton)\n\t\t# censorLayout.addStretch(1)\n\t\tself.censorTypeGroupBox.setLayout(censorLayout)\n\n\t\t#Variation count group\n\t\tself.variationsGroupBox = QGroupBox(\'Number of Decensor Variations\')\n\n\t\tvar1Button = QRadioButton(\'1\')\n\t\tvar2Button = QRadioButton(\'2\')\n\t\tvar3Button = QRadioButton(\'4\')\n\t\tvar1Button.setChecked(True)\n\n\t\tvarLayout = QVBoxLayout()\n\t\tvarLayout.addWidget(var1Button)\n\t\tvarLayout.addWidget(var2Button)\n\t\tvarLayout.addWidget(var3Button)\n\t\t# varLayout.addStretch(1)\n\t\tself.variationsGroupBox.setLayout(varLayout)\n\n\t\t#Decensor button\n\t\tself.decensorButton = QPushButton(\'Decensor Your Images\')\n\t\tself.decensorButton.clicked.connect(self.decensorClicked)\n\t\tself.decensorButton.setSizePolicy(\n    \t\tQSizePolicy.Preferred,\n    \t\tQSizePolicy.Preferred)\n\n\t\t#Progress message\n\t\t# self.progressGroupBox = QGroupBox(\'Progress\')\n\n\t\tself.progressMessage = QTextEdit()\n\t\tself.progressCursor = QTextCursor(self.progressMessage.document())\n\t\tself.progressMessage.setTextCursor(self.progressCursor)\n\t\tself.progressMessage.setReadOnly(True)\n\t\tself.progressCursor.insertText(""After you prepared your images, click on the decensor button once to begin decensoring.\\nPlease be patient.\\nDecensoring will take time.\\n"")\n\n\t\t# Progress Bar\n\t\tself.statusBar  = QStatusBar(self)\n\t\tself.progressBar = QProgressBar()\n\t\tself.progressBar.setMinimum(0)\n\t\tself.progressBar.setMaximum(100)\n\t\tself.progressBar.setValue(0)\n\t\tself.statusLabel = QLabel(""Showing Progress"")\n\n\t\tself.statusBar.addWidget(self.statusLabel, 1)\n\t\tself.statusBar.addWidget(self.progressBar, 2)\n\n\t\t#put all groups into grid\n\t\t# addWidget(row, column, rowSpan, columnSpan)\n\t\tgrid_layout.addWidget(self.tutorialLabel, 0, 0, 1, 2)\n\t\tgrid_layout.addWidget(self.censorTypeGroupBox, 1, 0, 1, 1)\n\t\tgrid_layout.addWidget(self.variationsGroupBox, 1, 1, 1, 1)\n\t\tgrid_layout.addWidget(self.decensorButton, 2, 0, 1, 2)\n\t\tgrid_layout.addWidget(self.progressMessage, 3, 0, 1, 2)\n\t\tgrid_layout.addWidget(self.statusBar, 4, 0, 1, 2)\n\n\t\t#window size settings\n\t\tself.resize(900, 600)\n\t\tself.center()\n\t\tself.setWindowTitle(\'DeepCreamPy v2.2.0-beta\')\n\t\tself.show()\n\n\tdef load_model(self):\n\t\t# load model to make able to decensor several times\n\t\tself.decensorButton.setEnabled(False)\n\t\tself.decensorButton.setText(""Loading Machine Learning Model (Please Wait...)"")\n\t\tself.decensor.start()\n\t\tself.decensor.signals = self.signals\n\t\tself.progressCursor.insertText(""Loading Decensor app consumes 6 GB memory at maximum"")\n\n\tdef setSignals(self):\n\t\tself.signals.update_decensorButton_Text.connect(self.decensorButton.setText)\n\t\tself.signals.update_decensorButton_Enabled.connect(self.decensorButton.setEnabled)\n\t\tself.signals.update_statusLabel_Text.connect(self.statusLabel.setText)\n\t\tself.signals.update_ProgressBar_SET_VALUE.connect(self.progressBar.setValue)\n\t\tself.signals.update_ProgressBar_MAX_VALUE.connect(self.progressBar.setMaximum)\n\t\tself.signals.update_ProgressBar_MIN_VALUE.connect(self.progressBar.setMinimum)\n\t\t# self.signals.insertText_progressCursor.connect(self.progressCursor.insertText)\n\t\tself.signals.insertText_progressCursor.connect(self.progressMessage.append)\n\t\tself.signals.clear_progressMessage.connect(self.progressMessage.clear)\n\t\tself.signals.appendText_progressMessage.connect(self.progressMessage.append)\n\n\tdef decensorClicked(self):\n\t\tself.decensorButton.setEnabled(False)\n\t\tself.progressMessage.clear()\n\t\tself.progressCursor.insertText(""Decensoring has begun!\\n"")\n\n\t\t# for now, decensor is initiated when this app is started\n\t\t# self.decensor = Decensor(text_edit = self.progressMessage, text_cursor = self.progressCursor, ui_mode = True)\n\n\t\t#https://stackoverflow.com/questions/42349470/pyqt-find-checked-radiobutton-in-a-group\n\t\t#set decensor to right settings\n\t\t#censor type\n\t\tcensorTypeElements = self.censorTypeGroupBox.children()\n\t\tcensorButtons = [elem for elem in censorTypeElements if isinstance(elem, QRadioButton)]\n\t\tfor cb in censorButtons:\n\t\t\tif cb.isChecked():\n\t\t\t\tcensorType = cb.text()\n\t\tif censorType == \'Bar censor\':\n\t\t\tself.decensor.is_mosaic = False\n\t\telse:\n\t\t\tself.decensor.is_mosaic = True\n\n\t\t#variations count\n\t\tvariationsElements = self.variationsGroupBox.children()\n\t\tvariationsButtons = [elem for elem in variationsElements if isinstance(elem, QRadioButton)]\n\t\tfor vb in variationsButtons:\n\t\t\tif vb.isChecked():\n\t\t\t\tvariations = int(vb.text())\n\t\tself.decensor.variations = variations\n\n\n\t\tself.decensorButton.setEnabled(False)\n\t\tself.decensor.start()\n\t\t# decensor.decensor_all_images_in_folder()\n\n\t# #centers the main window\n\tdef center(self):\n\t\tqr = self.frameGeometry()\n\t\tcp = QDesktopWidget().availableGeometry().center()\n\t\tqr.moveCenter(cp)\n\t\tself.move(qr.topLeft())\n\nif __name__ == \'__main__\':\n\timport os\n    # you could remove this if statement if there\'s no error without this\n\tif os.name == \'nt\':\n\t\timport PySide2\n\t\tpyqt = os.path.dirname(PySide2.__file__)\n\t\tQApplication.addLibraryPath(os.path.join(pyqt, ""plugins""))\n\tapp = QApplication(sys.argv)\n\tex = MainWindow()\n\tsys.exit(app.exec_())\n'"
model.py,29,"b'import tensorflow as tf\nimport os\nimport numpy as np\nimport module as mm\n\n#suppress tensorflow deprecation warnings\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n\nclass InpaintNN:\n\n\tdef __init__(self, input_height=256, input_width=256, batch_size = 1, bar_model_name=None, bar_checkpoint_name=None, mosaic_model_name=None, mosaic_checkpoint_name = None, is_mosaic=False):\n\t\tself.bar_model_name = bar_model_name\n\t\tself.bar_checkpoint_name = bar_checkpoint_name\n\t\tself.mosaic_model_name = mosaic_model_name\n\t\tself.mosaic_checkpoint_name = mosaic_checkpoint_name\n\t\tself.is_mosaic = is_mosaic\n\n\t\tself.input_height = input_height\n\t\tself.input_width = input_width\n\t\tself.batch_size = batch_size\n\n\t\tself.check_model_file()\n\t\tself.build_model()\n\n\tdef check_model_file(self):\n\t\tif not os.path.exists(self.bar_model_name) or not os.path.exists(self.mosaic_model_name) :\n\t\t\tprint(""\\nMissing Train Model, download train model"")\n\t\t\tprint(""Read : https://github.com/deeppomf/DeepCreamPy/blob/master/docs/INSTALLATION.md#run-code-yourself \\n"")\n\t\t\texit(-1)\n\n\tdef build_model(self):\n\t\t# ------- variables\n\n\t\tself.X = tf.placeholder(tf.float32, [self.batch_size, self.input_height, self.input_width, 3])\n\t\tself.Y = tf.placeholder(tf.float32, [self.batch_size, self.input_height, self.input_width, 3])\n\n\t\tself.MASK = tf.placeholder(tf.float32, [self.batch_size, self.input_height, self.input_width, 3])\n\t\tIT = tf.placeholder(tf.float32)\n\n\t\t# ------- structure\n\n\t\tinput = tf.concat([self.X, self.MASK], 3)\n\n\t\tvec_en = mm.encoder(input, reuse=False, name=\'G_en\')\n\n\t\tvec_con = mm.contextual_block(vec_en, vec_en, self.MASK, 3, 50.0, \'CB1\', stride=1)\n\n\t\tI_co = mm.decoder(vec_en, self.input_height, self.input_height, reuse=False, name=\'G_de\')\n\t\tI_ge = mm.decoder(vec_con, self.input_height, self.input_height, reuse=True, name=\'G_de\')\n\n\t\tself.image_result = I_ge * (1-self.MASK) + self.Y*self.MASK\n\n\t\tD_real_red = mm.discriminator_red(self.Y, reuse=False, name=\'disc_red\')\n\t\tD_fake_red = mm.discriminator_red(self.image_result, reuse=True, name=\'disc_red\')\n\n\t\t# ------- Loss\n\n\t\tLoss_D_red = tf.reduce_mean(tf.nn.relu(1+D_fake_red)) + tf.reduce_mean(tf.nn.relu(1-D_real_red))\n\n\t\tLoss_D = Loss_D_red\n\n\t\tLoss_gan_red = -tf.reduce_mean(D_fake_red)\n\n\t\tLoss_gan = Loss_gan_red\n\n\t\tLoss_s_re = tf.reduce_mean(tf.abs(I_ge - self.Y))\n\t\tLoss_hat = tf.reduce_mean(tf.abs(I_co - self.Y))\n\n\t\tA = tf.image.rgb_to_yuv((self.image_result+1)/2.0)\n\t\tA_Y = tf.to_int32(A[:, :, :, 0:1]*255.0)\n\n\t\tB = tf.image.rgb_to_yuv((self.Y+1)/2.0)\n\t\tB_Y = tf.to_int32(B[:, :, :, 0:1]*255.0)\n\n\t\tssim = tf.reduce_mean(tf.image.ssim(A_Y, B_Y, 255.0))\n\n\t\talpha = IT/1000000\n\n\t\tLoss_G = 0.1*Loss_gan + 10*Loss_s_re + 5*(1-alpha) * Loss_hat\n\n\t\t# --------------------- variable & optimizer\n\n\t\tvar_D = [v for v in tf.global_variables() if v.name.startswith(\'disc_red\')]\n\t\tvar_G = [v for v in tf.global_variables() if v.name.startswith(\'G_en\') or v.name.startswith(\'G_de\') or v.name.startswith(\'CB1\')]\n\n\t\tupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\n\t\twith tf.control_dependencies(update_ops):\n\t\t    optimize_D = tf.train.AdamOptimizer(learning_rate=0.0004, beta1=0.5, beta2=0.9).minimize(Loss_D, var_list=var_D)\n\t\t    optimize_G = tf.train.AdamOptimizer(learning_rate=0.0001, beta1=0.5, beta2=0.9).minimize(Loss_G, var_list=var_G)\n\n\t\tconfig = tf.ConfigProto()\n\t\t# config.gpu_options.per_process_gpu_memory_fraction = 0.4\n\t\t# config.gpu_options.allow_growth = False\n\n\t\tself.sess = tf.Session(config=config)\n\n\t\tinit = tf.global_variables_initializer()\n\t\tself.sess.run(init)\n\t\tsaver = tf.train.Saver()\n\n\t\tif self.is_mosaic:\n\t\t\tRestore = tf.train.import_meta_graph(self.mosaic_model_name)\n\t\t\tRestore.restore(self.sess, tf.train.latest_checkpoint(self.mosaic_checkpoint_name))\n\t\telse:\n\t\t\tRestore = tf.train.import_meta_graph(self.bar_model_name)\n\t\t\tRestore.restore(self.sess, tf.train.latest_checkpoint(self.bar_checkpoint_name))\n\n\tdef predict(self, censored, unused, mask):\n\t\timg_sample = self.sess.run(self.image_result, feed_dict={self.X: censored, self.Y: unused, self.MASK: mask})\n\n\t\treturn img_sample\n'"
module.py,112,"b'from __future__ import division\r\nfrom ops import *\r\nimport tensorflow.contrib.layers as layers\r\nimport math\r\n\r\ndef conv_nn(input, dims1, dims2, size1, size2, k_size = 3):\r\n\r\n    pp = tf.pad(input, [[0, 0], [1, 1], [1, 1], [0, 0]], ""REFLECT"")\r\n    L1 = layers.conv2d(pp, dims1, [k_size, k_size], stride=[1, 1], padding=\'VALID\', activation_fn=None)\r\n    L1 = tf.nn.elu(L1)\r\n\r\n    pp = tf.pad(L1, [[0, 0], [1, 1], [1, 1], [0, 0]], ""REFLECT"")\r\n    L2 = layers.conv2d(pp, dims2, [k_size, k_size], stride=[1, 1], padding=\'VALID\', activation_fn=None)\r\n    L2 = tf.nn.elu(L2)\r\n    L2 = tf.image.resize_nearest_neighbor(L2, (size1, size2))\r\n\r\n    return L2\r\n\r\ndef encoder(input, reuse, name):\r\n    with tf.variable_scope(name):\r\n        if reuse:\r\n            tf.get_variable_scope().reuse_variables()\r\n        else:\r\n            assert tf.get_variable_scope().reuse is False\r\n\r\n        p = tf.pad(input, [[0, 0], [2, 2], [2, 2], [0, 0]], ""REFLECT"")\r\n        CL1 = layers.conv2d(p, 32, [5, 5], stride=[1, 1], padding=\'VALID\', activation_fn=None)\r\n        CL1 = tf.nn.elu(CL1)  # 256 256 32\r\n\r\n        p = tf.pad(CL1, [[0, 0], [1, 1], [1, 1], [0, 0]], ""REFLECT"")\r\n        CL2 = layers.conv2d(p, 64, [3, 3], stride=[2, 2], padding=\'VALID\', activation_fn=None)\r\n        CL2 = tf.nn.elu(CL2)  # 128 128 64\r\n\r\n        p = tf.pad(CL2, [[0, 0], [1, 1], [1, 1], [0, 0]], ""REFLECT"")\r\n        CL3 = layers.conv2d(p, 64, [3, 3], stride=[1, 1], padding=\'VALID\', activation_fn=None)\r\n        CL3 = tf.nn.elu(CL3)  # 128 128 64\r\n\r\n        p = tf.pad(CL3, [[0, 0], [1, 1], [1, 1], [0, 0]], ""REFLECT"")\r\n        CL4 = layers.conv2d(p, 128, [3, 3], stride=[2, 2], padding=\'VALID\', activation_fn=None)\r\n        CL4 = tf.nn.elu(CL4)  # 64 64 128\r\n\r\n        p = tf.pad(CL4, [[0, 0], [1, 1], [1, 1], [0, 0]], ""REFLECT"")\r\n        CL5 = layers.conv2d(p, 128, [3, 3], stride=[1, 1], padding=\'VALID\', activation_fn=None)\r\n        CL5 = tf.nn.elu(CL5)  # 64 64 128\r\n\r\n        p = tf.pad(CL5, [[0, 0], [1, 1], [1, 1], [0, 0]], ""REFLECT"")\r\n        CL6 = layers.conv2d(p, 256, [3, 3], stride=[2, 2], padding=\'VALID\', activation_fn=None)\r\n        CL6 = tf.nn.elu(CL6)  # 32 32 128\r\n\r\n        p = tf.pad(CL6, [[0, 0], [2, 2], [2, 2], [0, 0]], ""REFLECT"")\r\n        DCL1 = layers.conv2d(p, 256, [3, 3], rate=2, stride=[1, 1], padding=\'VALID\', activation_fn=None)\r\n        DCL1 = tf.nn.elu(DCL1)\r\n        p = tf.pad(DCL1, [[0, 0], [4, 4], [4, 4], [0, 0]], ""REFLECT"")\r\n        DCL2 = layers.conv2d(p, 256, [3, 3], rate=4, stride=[1, 1], padding=\'VALID\', activation_fn=None)\r\n        DCL2 = tf.nn.elu(DCL2)\r\n        p = tf.pad(DCL2, [[0, 0], [8, 8], [8, 8], [0, 0]], ""REFLECT"")\r\n        DCL3 = layers.conv2d(p, 256, [3, 3], rate=8, stride=[1, 1], padding=\'VALID\', activation_fn=None)\r\n        DCL3 = tf.nn.elu(DCL3)\r\n        p = tf.pad(DCL3, [[0, 0], [16, 16], [16, 16], [0, 0]], ""REFLECT"")\r\n        DCL4 = layers.conv2d(p, 256, [3, 3], rate=16, stride=[1, 1], padding=\'VALID\', activation_fn=None)\r\n        DCL4 = tf.nn.elu(DCL4)  # 32 32 128\r\n\r\n        return DCL4\r\n\r\ndef decoder(input, size1, size2, reuse, name):\r\n    with tf.variable_scope(name):\r\n        if reuse:\r\n            tf.get_variable_scope().reuse_variables()\r\n        else:\r\n            assert tf.get_variable_scope().reuse is False\r\n\r\n        DL1 = conv_nn(input, 128, 128, int(size1/4), int(size2/4))  # 64 64 128\r\n\r\n        DL2 = conv_nn(DL1, 64, 64, int(size1/2), int(size2/2))  # 128 128 64\r\n\r\n        DL3 = conv_nn(DL2, 32, 32, int(size1), int(size2))\r\n\r\n        DL4 = conv_nn(DL3, 16, 16, int(size1), int(size2))\r\n\r\n        LL2 = layers.conv2d(DL4, 3, [3, 3], stride=[1, 1], padding=\'SAME\', activation_fn=None)  # 256 256 3\r\n        LL2 = tf.clip_by_value(LL2, -1.0, 1.0)\r\n\r\n        return LL2\r\n\r\ndef discriminator_G(input, reuse, name):\r\n    with tf.variable_scope(name):\r\n        # image is 256 x 256 x input_c_dim\r\n        if reuse:\r\n            tf.get_variable_scope().reuse_variables()\r\n        else:\r\n            assert tf.get_variable_scope().reuse is False\r\n\r\n        p = tf.pad(input, [[0, 0], [2, 2], [2, 2], [0, 0]], ""REFLECT"")\r\n        L1 = layers.conv2d(p, 64, [5, 5], stride=2, padding=\'VALID\', activation_fn=None)\r\n        #L1 = instance_norm(L1, \'di1\')\r\n        L1 = tf.nn.leaky_relu(L1)\r\n\r\n        p = tf.pad(L1, [[0, 0], [2, 2], [2, 2], [0, 0]], ""REFLECT"")\r\n        L2 = layers.conv2d(p, 128, [5, 5], stride=2, padding=\'VALID\', activation_fn=None)\r\n        #L2 = instance_norm(L2, \'di2\')\r\n        L2 = tf.nn.leaky_relu(L2)\r\n\r\n        p = tf.pad(L2, [[0, 0], [2, 2], [2, 2], [0, 0]], ""REFLECT"")\r\n        L3 = layers.conv2d(p, 256, [5, 5], stride=2, padding=\'VALID\', activation_fn=None)\r\n        #L3 = instance_norm(L3, \'di3\')\r\n        L3 = tf.nn.leaky_relu(L3)\r\n\r\n        p = tf.pad(L3, [[0, 0], [2, 2], [2, 2], [0, 0]], ""REFLECT"")\r\n        L4 = layers.conv2d(p, 256, [5, 5], stride=2, padding=\'VALID\', activation_fn=None)\r\n        #L4 = instance_norm(L4, \'di4\')\r\n        L4 = tf.nn.leaky_relu(L4)\r\n        L4 = layers.flatten(L4)\r\n\r\n        L5 = tf.layers.dense(L4, 1)\r\n\r\n        return L5\r\n\r\ndef discriminator_L(input, reuse, name):\r\n    with tf.variable_scope(name):\r\n        # image is 256 x 256 x input_c_dim\r\n        if reuse:\r\n            tf.get_variable_scope().reuse_variables()\r\n        else:\r\n            assert tf.get_variable_scope().reuse is False\r\n\r\n        p = tf.pad(input, [[0, 0], [2, 2], [2, 2], [0, 0]], ""REFLECT"")\r\n        L1 = layers.conv2d(p, 64, [5, 5], stride=2, padding=\'VALID\', activation_fn=None)\r\n        #L1 = instance_norm(L1, \'di1l\')\r\n        L1 = tf.nn.leaky_relu(L1) # 32 32 64\r\n\r\n        p = tf.pad(L1, [[0, 0], [2, 2], [2, 2], [0, 0]], ""REFLECT"")\r\n        L2 = layers.conv2d(p, 128, [5, 5], stride=2, padding=\'VALID\', activation_fn=None)\r\n        #L2 = instance_norm(L2, \'di2l\')\r\n        L2 = tf.nn.leaky_relu(L2) # 16 16 128\r\n\r\n        p = tf.pad(L2, [[0, 0], [2, 2], [2, 2], [0, 0]], ""REFLECT"")\r\n        L3 = layers.conv2d(p, 256, [5, 5], stride=2, padding=\'VALID\', activation_fn=None)\r\n        #L3 = instance_norm(L3, \'di3l\')\r\n        L3 = tf.nn.leaky_relu(L3) # 8 8 256\r\n\r\n        p = tf.pad(L3, [[0, 0], [2, 2], [2, 2], [0, 0]], ""REFLECT"")\r\n        L4 = layers.conv2d(p, 512, [5, 5], stride=2, padding=\'VALID\', activation_fn=None)\r\n        #L4 = instance_norm(L4, \'di4l\')\r\n        L4 = tf.nn.leaky_relu(L4) # 4 4 512\r\n        L4 = layers.flatten(L4)\r\n\r\n        L5 = tf.layers.dense(L4, 1)\r\n\r\n        return L5\r\n\r\ndef discriminator_red(input, reuse, name):\r\n    with tf.variable_scope(name):\r\n        # image is 256 x 256 x input_c_dim\r\n        if reuse:\r\n            tf.get_variable_scope().reuse_variables()\r\n        else:\r\n            assert tf.get_variable_scope().reuse is False\r\n\r\n        L1 = convolution_SN(input, 64, 5, 2, \'l1\')\r\n        # L1 = instance_norm(L1, \'di1\')\r\n        L1 = tf.nn.leaky_relu(L1)\r\n\r\n        L2 = convolution_SN(L1, 128, 5, 2, \'l2\')\r\n        # L2 = instance_norm(L2, \'di2\')\r\n        L2 = tf.nn.leaky_relu(L2)\r\n\r\n        L3 = convolution_SN(L2, 256, 5, 2, \'l3\')\r\n        # L3 = instance_norm(L3, \'di3\')\r\n        L3 = tf.nn.leaky_relu(L3)\r\n\r\n        L4 = convolution_SN(L3, 256, 5, 2, \'l4\')\r\n        # L4 = instance_norm(L4, \'di4\')\r\n        L4 = tf.nn.leaky_relu(L4)\r\n\r\n        L5 = convolution_SN(L4, 256, 5, 2, \'l5\')\r\n        # L5 = instance_norm(L5, \'di5\')\r\n        L5 = tf.nn.leaky_relu(L5)\r\n\r\n        L6 = convolution_SN(L5, 512, 5, 2, \'l6\')\r\n        # L6 = instance_norm(L6, \'di6\')\r\n        L6 = tf.nn.leaky_relu(L6)\r\n\r\n        L7 = dense_RED_SN(L6, \'l7\')\r\n\r\n        return L7\r\n\r\ndef contextual_block(bg_in, fg_in, mask, k_size, lamda, name, stride=1):\r\n    with tf.variable_scope(name):\r\n        b, h, w, dims = [i.value for i in bg_in.get_shape()]\r\n        temp = tf.image.resize_nearest_neighbor(mask, (h, w))\r\n        temp = tf.expand_dims(temp[:, :, :, 0], 3) # b 128 128 1\r\n        mask_r = tf.tile(temp, [1, 1, 1, dims]) # b 128 128 128\r\n        bg = bg_in * mask_r\r\n\r\n        kn = int((k_size - 1) / 2)\r\n        c = 0\r\n        for p in range(kn, h - kn, stride):\r\n            for q in range(kn, w - kn, stride):\r\n                c += 1\r\n\r\n        patch1 = tf.extract_image_patches(bg, [1, k_size, k_size, 1], [1, stride, stride, 1], [1, 1, 1, 1], \'VALID\')\r\n\r\n        patch1 = tf.reshape(patch1, (b, 1, c, k_size*k_size*dims))\r\n        patch1 = tf.reshape(patch1, (b, 1, 1, c, k_size * k_size * dims))\r\n        patch1 = tf.transpose(patch1, [0, 1, 2, 4, 3])\r\n\r\n        patch2 = tf.extract_image_patches(fg_in, [1,k_size,k_size,1], [1,1,1,1], [1,1,1,1], \'SAME\')\r\n        ACL = []\r\n\r\n        for ib in range(b):\r\n\r\n            k1 = patch1[ib, :, :, :, :]\r\n            k1d = tf.reduce_sum(tf.square(k1), axis=2)\r\n            k2 = tf.reshape(k1, (k_size, k_size, dims, c))\r\n            ww = patch2[ib, :, :, :]\r\n            wwd = tf.reduce_sum(tf.square(ww), axis=2, keepdims=True)\r\n            ft = tf.expand_dims(ww, 0)\r\n\r\n            CS = tf.nn.conv2d(ft, k1, strides=[1, 1, 1, 1], padding=\'SAME\')\r\n\r\n            tt = k1d + wwd\r\n\r\n            DS1 = tf.expand_dims(tt, 0) - 2 * CS\r\n\r\n            DS2 = (DS1 - tf.reduce_mean(DS1, 3, True)) / reduce_std(DS1, 3, True)\r\n            DS2 = -1 * tf.nn.tanh(DS2)\r\n\r\n            CA = softmax(lamda * DS2)\r\n\r\n            ACLt = tf.nn.conv2d_transpose(CA, k2, output_shape=[1, h, w, dims], strides=[1, 1, 1, 1], padding=\'SAME\')\r\n            ACLt = ACLt / (k_size ** 2)\r\n\r\n            if ib == 0:\r\n                ACL = ACLt\r\n            else:\r\n                ACL = tf.concat((ACL, ACLt), 0)\r\n\r\n        ACL = bg + ACL * (1.0 - mask_r)\r\n\r\n        con1 = tf.concat([bg_in, ACL], 3)\r\n        ACL2 = layers.conv2d(con1, dims, [1, 1], stride=[1, 1], padding=\'VALID\', activation_fn=None, scope=\'ML\')\r\n        ACL2 = tf.nn.elu(ACL2)\r\n\r\n        return ACL2\r\n\r\ndef contextual_block_cs(bg_in, fg_in, mask, k_size, lamda, name, stride=1):\r\n    with tf.variable_scope(name):\r\n        b, h, w, dims = [i.value for i in bg_in.get_shape()]\r\n        temp = tf.image.resize_nearest_neighbor(mask, (h, w))\r\n        temp = tf.expand_dims(temp[:, :, :, 0], 3) # b 128 128 1\r\n        mask_r = tf.tile(temp, [1, 1, 1, dims]) # b 128 128 128\r\n        bg = bg_in * mask_r\r\n\r\n        kn = int((k_size - 1) / 2)\r\n        c = 0\r\n        for p in range(kn, h - kn, stride):\r\n            for q in range(kn, w - kn, stride):\r\n                c += 1\r\n\r\n        patch1 = tf.extract_image_patches(bg, [1, k_size, k_size, 1], [1, stride, stride, 1], [1, 1, 1, 1], \'VALID\')\r\n\r\n        patch1 = tf.reshape(patch1, (b, 1, c, k_size*k_size*dims))\r\n        patch1 = tf.reshape(patch1, (b, 1, 1, c, k_size * k_size * dims))\r\n        patch1 = tf.transpose(patch1, [0, 1, 2, 4, 3])\r\n\r\n        patch2 = tf.extract_image_patches(fg_in, [1,k_size,k_size,1], [1,1,1,1], [1,1,1,1], \'SAME\')\r\n        ACL = []\r\n\r\n        fuse_weight = tf.reshape(tf.eye(3), [3, 3, 1, 1])\r\n\r\n        for ib in range(b):\r\n\r\n            k1 = patch1[ib, :, :, :, :]\r\n            k2 = k1 / tf.sqrt(tf.reduce_sum(tf.square(k1), axis=2, keepdims=True) + 1e-16)\r\n            k1 = tf.reshape(k1, (k_size, k_size, dims, c))\r\n            ww = patch2[ib, :, :, :]\r\n            ft = ww / tf.sqrt(tf.reduce_sum(tf.square(ww), axis=2, keepdims=True) + 1e-16)\r\n            ft = tf.expand_dims(ft, 0)\r\n\r\n            CA = tf.nn.conv2d(ft, k2, strides=[1, 1, 1, 1], padding=\'SAME\')\r\n\r\n            CA = tf.reshape(CA, [1, h * w, c, 1])\r\n            CA = tf.nn.conv2d(CA, fuse_weight, strides=[1, 1, 1, 1], padding=\'SAME\')\r\n            CA = tf.reshape(CA, [1, h, w, int(math.sqrt(c)), int(math.sqrt(c))])\r\n            CA = tf.transpose(CA, [0, 2, 1, 4, 3])\r\n            CA = tf.reshape(CA, [1, h * w, c, 1])\r\n            CA = tf.nn.conv2d(CA, fuse_weight, strides=[1, 1, 1, 1], padding=\'SAME\')\r\n            CA = tf.reshape(CA, [1, h, w, int(math.sqrt(c)), int(math.sqrt(c))])\r\n            CA = tf.transpose(CA, [0, 2, 1, 4, 3])\r\n            CA = tf.reshape(CA, [1, h, w, c])\r\n\r\n            CA2 = softmax(lamda * CA)\r\n\r\n            ACLt = tf.nn.conv2d_transpose(CA2, k1, output_shape=[1, h, w, dims], strides=[1, 1, 1, 1], padding=\'SAME\')\r\n            ACLt = ACLt / (k_size ** 2)\r\n\r\n            if ib == 0:\r\n                ACL = ACLt\r\n            else:\r\n                ACL = tf.concat((ACL, ACLt), 0)\r\n\r\n        ACL2 = bg + ACL * (1.0 - mask_r)\r\n\r\n        return ACL2\r\n\r\n'"
ops.py,33,"b'import tensorflow as tf\nimport tensorflow.contrib.layers as layers\nimport numpy as np\nimport random as rr\nimport math as mt\nimport cv2\nfrom scipy import misc\n\ndef instance_norm(input, name=""instance_norm""):\n    with tf.variable_scope(name):\n        depth = input.get_shape()[3]\n        scale = tf.get_variable(""scale"", [depth], initializer=tf.random_normal_initializer(1.0, 0.02, dtype=tf.float32))\n        offset = tf.get_variable(""offset"", [depth], initializer=tf.constant_initializer(0.0))\n        mean, variance = tf.nn.moments(input, axes=[1,2], keep_dims=True)\n        epsilon = 1e-5\n        inv = tf.rsqrt(variance + epsilon)\n        normalized = (input-mean)*inv\n        return scale*normalized + offset\n\ndef make_sq_mask(size, m_size, batch_size):\n\n    start_x = rr.randint(0, size - m_size-1)\n    start_y = rr.randint(0, size - m_size-1)\n\n    temp = np.ones([batch_size, size, size, 3])\n\n    temp[:, start_x:start_x + m_size, start_y:start_y + m_size, 0:3] *= 0\n\n    return temp, start_x, start_y\n\ndef softmax(input):\n\n    k = tf.exp(input - 3)\n    k = tf.reduce_sum(k, 3, True)\n    # k = k - num * tf.ones_like(k)\n\n    ouput = tf.exp(input - 3) / k\n\n    return ouput\n\ndef reduce_var(x, axis=None, keepdims=False):\n    """"""Variance of a tensor, alongside the specified axis.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: An integer, the axis to compute the variance.\n        keepdims: A boolean, whether to keep the dimensions or not.\n            If `keepdims` is `False`, the rank of the tensor is reduced\n            by 1. If `keepdims` is `True`,\n            the reduced dimension is retained with length 1.\n\n    # Returns\n        A tensor with the variance of elements of `x`.\n    """"""\n    m = tf.reduce_mean(x, axis=axis, keepdims=True)\n    devs_squared = tf.square(x - m)\n    return tf.reduce_mean(devs_squared, axis=axis, keepdims=keepdims)\n\ndef reduce_std(x, axis=None, keepdims=False):\n    """"""Standard deviation of a tensor, alongside the specified axis.\n\n    # Arguments\n        x: A tensor or variable.\n        axis: An integer, the axis to compute the standard deviation.\n        keepdims: A boolean, whether to keep the dimensions or not.\n            If `keepdims` is `False`, the rank of the tensor is reduced\n            by 1. If `keepdims` is `True`,\n            the reduced dimension is retained with length 1.\n\n    # Returns\n        A tensor with the standard deviation of elements of `x`.\n    """"""\n    return tf.sqrt(reduce_var(x, axis=axis, keepdims=keepdims))\n\ndef l2_norm(v, eps=1e-12):\n    return v / (tf.reduce_sum(v ** 2) ** 0.5 + eps)\n\ndef ff_mask(size, b_zise, maxLen, maxWid, maxAng, maxNum, maxVer, minLen = 20, minWid = 15, minVer = 5):\n\n    mask = np.ones((b_zise, size, size, 3))\n\n    num = rr.randint(3, maxNum)\n\n    for i in range(num):\n        startX = rr.randint(0, size)\n        startY = rr.randint(0, size)\n        numVer = rr.randint(minVer, maxVer)\n        width = rr.randint(minWid, maxWid)\n        for j in range(numVer):\n            angle = rr.uniform(-maxAng, maxAng)\n            length = rr.randint(minLen, maxLen)\n\n            endX = min(size-1, max(0, int(startX + length * mt.sin(angle))))\n            endY = min(size-1, max(0, int(startY + length * mt.cos(angle))))\n\n            if endX >= startX:\n                lowx = startX\n                highx = endX\n            else:\n                lowx = endX\n                highx = startX\n            if endY >= startY:\n                lowy = startY\n                highy = endY\n            else:\n                lowy = endY\n                highy = startY\n\n            if abs(startY-endY) + abs(startX - endX) != 0:\n\n                wlx = max(0, lowx-int(abs(width * mt.cos(angle))))\n                whx = min(size - 1,  highx+1 + int(abs(width * mt.cos(angle))))\n                wly = max(0, lowy - int(abs(width * mt.sin(angle))))\n                why = min(size - 1, highy+1 + int(abs(width * mt.sin(angle))))\n\n                for x in range(wlx, whx):\n                    for y in range(wly, why):\n\n                        d = abs((endY-startY)*x - (endX -startX)*y - endY*startX + startY*endX) / mt.sqrt((startY-endY)**2 + (startX -endX)**2)\n\n                        if d <= width:\n                            mask[:, x, y, :] = 0\n\n            wlx = max(0, lowx-width)\n            whx = min(size - 1, highx+width+1)\n            wly = max(0, lowy - width)\n            why = min(size - 1, highy + width + 1)\n\n            for x2 in range(wlx, whx):\n                for y2 in range(wly, why):\n\n                    d1 = (startX - x2) ** 2 + (startY - y2) ** 2\n                    d2 = (endX - x2) ** 2 + (endY - y2) ** 2\n\n                    if np.sqrt(d1) <= width:\n                        mask[:, x2, y2, :] = 0\n                    if np.sqrt(d2) <= width:\n                        mask[:, x2, y2, :] = 0\n            startX = endX\n            startY = endY\n\n    return mask\n\ndef ff_mask_batch(size, b_size, maxLen, maxWid, maxAng, maxNum, maxVer, minLen = 20, minWid = 15, minVer = 5):\n\n    mask = None\n    temp = ff_mask(size, 1, maxLen, maxWid, maxAng, maxNum, maxVer, minLen=minLen, minWid=minWid, minVer=minVer)\n    temp = temp[0]\n    for ib in range(b_size):\n        if ib == 0:\n            mask = np.expand_dims(temp, 0)\n        else:\n            mask = np.concatenate((mask, np.expand_dims(temp, 0)), 0)\n\n        temp = cv2.rotate(temp, cv2.ROTATE_90_CLOCKWISE)\n        if ib == 3:\n            temp = cv2.flip(temp, 0)\n\n    return mask\n\ndef spectral_norm(w, name, iteration=1):\n    w_shape = w.shape.as_list()\n    w = tf.reshape(w, [-1, w_shape[-1]])\n\n    u = tf.get_variable(name+""u"", [1, w_shape[-1]], initializer=tf.truncated_normal_initializer(), trainable=False)\n\n    u_hat = u\n    v_hat = None\n    for i in range(iteration):\n        """"""\n        power iteration\n        Usually iteration = 1 will be enough\n        """"""\n        v_ = tf.matmul(u_hat, tf.transpose(w))\n        v_hat = l2_norm(v_)\n\n        u_ = tf.matmul(v_hat, w)\n        u_hat = l2_norm(u_)\n\n    sigma = tf.matmul(tf.matmul(v_hat, w), tf.transpose(u_hat))\n    w_norm = w / sigma\n\n    with tf.control_dependencies([u.assign(u_hat)]):\n        w_norm = tf.reshape(w_norm, w_shape)\n\n    return w_norm\n\ndef convolution_SN(tensor, output_dim, kernel_size, stride, name):\n    _, h, w, c = [i.value for i in tensor.get_shape()]\n\n    w = tf.get_variable(name=name + \'w\', shape=[kernel_size, kernel_size, c, output_dim], initializer=layers.xavier_initializer())\n    b = tf.get_variable(name=name + \'b\', shape=[output_dim], initializer=tf.constant_initializer(0.0))\n\n    output = tf.nn.conv2d(tensor, filter=spectral_norm(w, name=name + \'w\'), strides=[1, stride, stride, 1], padding=\'SAME\') + b\n\n    return output\n\ndef dense_SN(tensor, output_dim, name):\n    _, h, w, c = [i.value for i in tensor.get_shape()]\n\n    w = tf.get_variable(name=name + \'w\', shape=[h, w, c, output_dim], initializer=layers.xavier_initializer())\n    b = tf.get_variable(name=name + \'b\', shape=[output_dim], initializer=tf.constant_initializer(0.0))\n\n    output = tf.nn.conv2d(tensor, filter=spectral_norm(w, name=name + \'w\'), strides=[1, 1, 1, 1], padding=\'VALID\') + b\n\n    return output\n\ndef dense_RED_SN(tensor, name):\n    sn_w = None\n\n    _, h, w, c = [i.value for i in tensor.get_shape()]\n    h = int(h)\n    w = int(w)\n    c = int(c)\n\n    weight = tf.get_variable(name=name + \'_w\', shape=[h*w, 1, c, 1], initializer=layers.xavier_initializer())\n    b = tf.get_variable(name=name + \'_b\', shape=[1, h, w, 1], initializer=tf.constant_initializer(0.0))\n\n    for it in range(h*w):\n        w_pixel = weight[it:it+1, :, :, :]\n        sn_w_pixel = spectral_norm(w_pixel, name=name + \'w_%d\' %it)\n\n        if it == 0:\n            sn_w = sn_w_pixel\n        else:\n            sn_w = tf.concat([sn_w, sn_w_pixel], axis=0)\n\n    w_rs = tf.reshape(sn_w, [h, w, c, 1])\n    w_rs_t = tf.transpose(w_rs, [3, 0, 1, 2])\n\n    output_RED = tf.reduce_sum(tensor*w_rs_t + b, axis=3, keepdims=True)\n\n    return output_RED'"
signals.py,0,"b'from PySide2 import QtCore\n\n# Signals used for sharing status between threads(IPC, InterProcess Connection)\nclass Signals(QtCore.QObject):\n\n    # usage example in other class(thread) :\n    # \xe2\x86\x92 self.signals.<method_name>.emit(<parameters - type strict>)\n\n    # str : String to update label\n    # direct connect to decensorButton.setText(str)\n    update_decensorButton_Text = QtCore.Signal(str)\n\n    # bool : set QPushButton Enabled (True or False)\n    # direct connect to decensorButton.setEnabled(bool)\n    update_decensorButton_Enabled = QtCore.Signal(bool)\n\n    # direct connect to progressMessage.clear(None)\n    clear_progressMessage = QtCore.Signal()\n\n    # str : text to change\n    # direct connect to statusLabel.setText\n    update_statusLabel_Text = QtCore.Signal(str)\n\n    # int : value to change\n    # direct connect to progressBar.setValue(int)\n    update_ProgressBar_SET_VALUE = QtCore.Signal(int)\n\n    # int : value to change\n    # direct connect to progressBar.setMaximum(int)\n    update_ProgressBar_MAX_VALUE = QtCore.Signal(int)\n\n    # int : value to change\n    # direct connect to self.progressBar.setMinimum(int)\n    update_ProgressBar_MIN_VALUE = QtCore.Signal(int)\n\n    # str : value to change\n    # direct connect to self.progressCursor.insertText(str)\n    insertText_progressCursor = QtCore.Signal(str)\n\n    # str : value to change\n    # direct connect to self.progressMessage.append(str)\n    appendText_progressMessage = QtCore.Signal(str)\n'"
libs/utils.py,0,"b'import numpy as np\nfrom PIL import Image, ImageDraw\n\n#convert PIL image to numpy array\ndef image_to_array(image):\n    array = np.asarray(image)\n    return np.array(array / 255.0)\n\n#find strongly connected components with the mask color\ndef find_regions(image, mask_color):\n    pixel = image.load()\n    neighbors = dict()\n    width, height = image.size\n    for x in range(width):\n        for y in range(height):\n            if is_right_color(pixel[x,y], *mask_color):\n                neighbors[x, y] = {(x,y)}\n    for x, y in neighbors:\n        candidates = (x + 1, y), (x, y + 1)\n        for candidate in candidates:\n            if candidate in neighbors:\n                neighbors[x, y].add(candidate)\n                neighbors[candidate].add((x, y))\n    closed_list = set()\n\n    def connected_component(pixel):\n        region = set()\n        open_list = {pixel}\n        while open_list:\n            pixel = open_list.pop()\n            closed_list.add(pixel)\n            open_list |= neighbors[pixel] - closed_list\n            region.add(pixel)\n        return region\n\n    regions = []\n    for pixel in neighbors:\n        if pixel not in closed_list:\n            regions.append(connected_component(pixel))\n    regions.sort(key = len, reverse = True)\n    return regions\n\n# risk of box being bigger than the image\ndef expand_bounding(img, region, expand_factor=1.5, min_size = 256):\n    #expand bounding box to capture more context\n    x, y = zip(*region)\n    min_x, min_y, max_x, max_y = min(x), min(y), max(x), max(y)\n    width, height = img.size\n    width_center = width//2\n    height_center = height//2\n    bb_width = max_x - min_x\n    bb_height = max_y - min_y\n    x_center = (min_x + max_x)//2\n    y_center = (min_y + max_y)//2\n    current_size = max(bb_width, bb_height)\n    current_size  = int(current_size * expand_factor)\n    max_size = min(width, height)\n    if current_size > max_size:\n        current_size = max_size\n    elif current_size < min_size:\n        current_size = min_size\n    x1 = x_center - current_size//2\n    x2 = x_center + current_size//2\n    y1 = y_center - current_size//2\n    y2 = y_center + current_size//2\n    x1_square = x1\n    y1_square = y1\n    x2_square = x2\n    y2_square = y2\n    #move bounding boxes that are partially outside of the image inside the image\n    if (y1_square < 0 or y2_square > (height - 1)) and (x1_square < 0 or x2_square > (width - 1)):\n        #conservative square region\n        if x1_square < 0 and y1_square < 0:\n            x1_square = 0\n            y1_square = 0\n            x2_square = current_size\n            y2_square = current_size\n        elif x2_square > (width - 1) and y1_square < 0:\n            x1_square = width - current_size - 1\n            y1_square = 0\n            x2_square = width - 1\n            y2_square = current_size\n        elif x1_square < 0 and y2_square > (height - 1):\n            x1_square = 0\n            y1_square = height - current_size - 1\n            x2_square = current_size\n            y2_square = height - 1\n        elif x2_square > (width - 1) and y2_square > (height - 1):\n            x1_square = width - current_size - 1\n            y1_square = height - current_size - 1\n            x2_square = width - 1\n            y2_square = height - 1\n        else:\n            x1_square = x1\n            y1_square = y1\n            x2_square = x2\n            y2_square = y2\n    else:\n        if x1_square < 0:\n            difference = x1_square\n            x1_square -= difference\n            x2_square -= difference\n        if x2_square > (width - 1):\n            difference = x2_square - width + 1\n            x1_square -= difference\n            x2_square -= difference\n        if y1_square < 0:\n            difference = y1_square\n            y1_square -= difference\n            y2_square -= difference\n        if y2_square > (height - 1):\n            difference = y2_square - height + 1\n            y1_square -= difference\n            y2_square -= difference\n    # if y1_square < 0 or y2_square > (height - 1):\n\n    #if bounding box goes outside of the image for some reason, set bounds to original, unexpanded values\n    #print(width, height)\n    if x2_square > width or y2_square > height:\n        print(""bounding box out of bounds!"")\n        print(x1_square, y1_square, x2_square, y2_square)\n        x1_square, y1_square, x2_square, y2_square = min_x, min_y, max_x, max_y\n    return x1_square, y1_square, x2_square, y2_square\n\ndef is_right_color(pixel, r2, g2, b2):\n    r1, g1, b1 = pixel\n    return r1 == r2 and g1 == g2 and b1 == b2\n\nif __name__ == \'__main__\':\n    image = Image.open(\'\')\n    no_alpha_image = image.convert(\'RGB\')\n    draw = ImageDraw.Draw(no_alpha_image)\n    for region in find_regions(no_alpha_image, [0,255,0]):\n        draw.rectangle(expand_bounding(no_alpha_image, region), outline=(0, 255, 0))\n    no_alpha_image.show()'"
