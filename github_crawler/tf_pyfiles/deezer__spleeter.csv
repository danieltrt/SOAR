file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\n# coding: utf8\n\n""""""  Distribution script. """"""\n\nimport sys\n\nfrom os import path\nfrom setuptools import setup\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n\n# Default project values.\nproject_name = \'spleeter\'\nproject_version = \'1.5.2\'\ntensorflow_dependency = \'tensorflow\'\ntensorflow_version = \'1.15.2\'\nhere = path.abspath(path.dirname(__file__))\nreadme_path = path.join(here, \'README.md\')\nwith open(readme_path, \'r\') as stream:\n    readme = stream.read()\n\n# Package setup entrypoint.\nsetup(\n    name=project_name,\n    version=project_version,\n    description=\'\'\'\n        The Deezer source separation library with\n        pretrained models based on tensorflow.\n    \'\'\',\n    long_description=readme,\n    long_description_content_type=\'text/markdown\',\n    author=\'Deezer Research\',\n    author_email=\'research@deezer.com\',\n    url=\'https://github.com/deezer/spleeter\',\n    license=\'MIT License\',\n    packages=[\n        \'spleeter\',\n        \'spleeter.audio\',\n        \'spleeter.commands\',\n        \'spleeter.model\',\n        \'spleeter.model.functions\',\n        \'spleeter.model.provider\',\n        \'spleeter.resources\',\n        \'spleeter.utils\',\n    ],\n    package_data={\'spleeter.resources\': [\'*.json\']},\n    python_requires=\'>=3.6, <3.8\',\n    include_package_data=True,\n    install_requires=[\n        \'ffmpeg-python\',\n        \'importlib_resources ; python_version<""3.7""\',\n        \'norbert==0.2.1\',\n        \'pandas==0.25.1\',\n        \'requests\',\n        \'setuptools>=41.0.0\',\n        \'librosa==0.7.2\',\n        \'{}=={}\'.format(tensorflow_dependency, tensorflow_version),\n    ],\n    extras_require={\n        \'evaluation\':  [\'musdb==0.3.1\', \'museval==0.3.0\']\n    },\n    entry_points={\n        \'console_scripts\': [\'spleeter=spleeter.__main__:entrypoint\']\n    },\n    classifiers=[\n        \'Environment :: Console\',\n        \'Environment :: MacOS X\',\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Information Technology\',\n        \'Intended Audience :: Science/Research\',\n        \'License :: OSI Approved :: MIT License\',\n        \'Natural Language :: English\',\n        \'Operating System :: MacOS\',\n        \'Operating System :: Microsoft :: Windows\',\n        \'Operating System :: POSIX :: Linux\',\n        \'Operating System :: Unix\',\n        \'Programming Language :: Python\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'Programming Language :: Python :: 3 :: Only\',\n        \'Programming Language :: Python :: Implementation :: CPython\',\n        \'Topic :: Artistic Software\',\n        \'Topic :: Multimedia\',\n        \'Topic :: Multimedia :: Sound/Audio\',\n        \'Topic :: Multimedia :: Sound/Audio :: Analysis\',\n        \'Topic :: Multimedia :: Sound/Audio :: Conversion\',\n        \'Topic :: Multimedia :: Sound/Audio :: Sound Synthesis\',\n        \'Topic :: Scientific/Engineering\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        \'Topic :: Scientific/Engineering :: Information Analysis\',\n        \'Topic :: Software Development\',\n        \'Topic :: Software Development :: Libraries\',\n        \'Topic :: Software Development :: Libraries :: Python Modules\',\n        \'Topic :: Utilities\']\n)\n'"
spleeter/__init__.py,0,"b'#!/usr/bin/env python\n# coding: utf8\n\n""""""\n    Spleeter is the Deezer source separation library with pretrained models.\n    The library is based on Tensorflow:\n\n    -   It provides already trained model for performing separation.\n    -   It makes it easy to train source separation model with tensorflow\n        (provided you have a dataset of isolated sources).\n\n    This module allows to interact easily from command line with Spleeter\n    by providing train, evaluation and source separation action.\n""""""\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n\n\nclass SpleeterError(Exception):\n    """""" Custom exception for Spleeter related error. """"""\n\n    pass\n'"
spleeter/__main__.py,0,"b'#!/usr/bin/env python\n# coding: utf8\n\n""""""\n    Python oneliner script usage.\n\n    USAGE: python -m spleeter {train,evaluate,separate} ...\n""""""\n\nimport sys\nimport warnings\n\nfrom . import SpleeterError\nfrom .commands import create_argument_parser\nfrom .utils.configuration import load_configuration\nfrom .utils.logging import (\n    enable_logging,\n    enable_tensorflow_logging,\n    get_logger)\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n\n\ndef main(argv):\n    """""" Spleeter runner. Parse provided command line arguments\n    and run entrypoint for required command (either train,\n    evaluate or separate).\n\n    :param argv: Provided command line arguments.\n    """"""\n    try:\n        parser = create_argument_parser()\n        arguments = parser.parse_args(argv[1:])\n        enable_logging()\n        if arguments.verbose:\n            enable_tensorflow_logging()\n        if arguments.command == \'separate\':\n            from .commands.separate import entrypoint\n        elif arguments.command == \'train\':\n            from .commands.train import entrypoint\n        elif arguments.command == \'evaluate\':\n            from .commands.evaluate import entrypoint\n        params = load_configuration(arguments.configuration)\n        entrypoint(arguments, params)\n    except SpleeterError as e:\n        get_logger().error(e)\n\n\ndef entrypoint():\n    """""" Command line entrypoint. """"""\n    warnings.filterwarnings(\'ignore\')\n    main(sys.argv)\n\n\nif __name__ == \'__main__\':\n    entrypoint()\n'"
spleeter/dataset.py,15,"b'#!/usr/bin/env python\n# coding: utf8\n\n""""""\n    Module for building data preprocessing pipeline using the tensorflow\n    data API. Data preprocessing such as audio loading, spectrogram\n    computation, cropping, feature caching or data augmentation is done\n    using a tensorflow dataset object that output a tuple (input_, output)\n    where:\n\n    -   input is a dictionary with a single key that contains the (batched)\n        mix spectrogram of audio samples\n    -   output is a dictionary of spectrogram of the isolated tracks\n        (ground truth)\n""""""\n\nimport time\nimport os\nfrom os.path import exists, join, sep as SEPARATOR\n\n# pylint: disable=import-error\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\n# pylint: enable=import-error\n\nfrom .audio.convertor import (\n    db_uint_spectrogram_to_gain,\n    spectrogram_to_db_uint)\nfrom .audio.spectrogram import (\n    compute_spectrogram_tf,\n    random_pitch_shift,\n    random_time_stretch)\nfrom .utils.logging import get_logger\nfrom .utils.tensor import (\n    check_tensor_shape,\n    dataset_from_csv,\n    set_tensor_shape,\n    sync_apply)\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n\n# Default audio parameters to use.\nDEFAULT_AUDIO_PARAMS = {\n    \'instrument_list\': (\'vocals\', \'accompaniment\'),\n    \'mix_name\': \'mix\',\n    \'sample_rate\': 44100,\n    \'frame_length\': 4096,\n    \'frame_step\': 1024,\n    \'T\': 512,\n    \'F\': 1024\n}\n\n\ndef get_training_dataset(audio_params, audio_adapter, audio_path):\n    """""" Builds training dataset.\n\n    :param audio_params: Audio parameters.\n    :param audio_adapter: Adapter to load audio from.\n    :param audio_path: Path of directory containing audio.\n    :returns: Built dataset.\n    """"""\n    builder = DatasetBuilder(\n        audio_params,\n        audio_adapter,\n        audio_path,\n        chunk_duration=audio_params.get(\'chunk_duration\', 20.0),\n        random_seed=audio_params.get(\'random_seed\', 0))\n    return builder.build(\n        audio_params.get(\'train_csv\'),\n        cache_directory=audio_params.get(\'training_cache\'),\n        batch_size=audio_params.get(\'batch_size\'),\n        n_chunks_per_song=audio_params.get(\'n_chunks_per_song\', 2),\n        random_data_augmentation=False,\n        convert_to_uint=True,\n        wait_for_cache=False)\n\n\ndef get_validation_dataset(audio_params, audio_adapter, audio_path):\n    """""" Builds validation dataset.\n\n    :param audio_params: Audio parameters.\n    :param audio_adapter: Adapter to load audio from.\n    :param audio_path: Path of directory containing audio.\n    :returns: Built dataset.\n    """"""\n    builder = DatasetBuilder(\n        audio_params,\n        audio_adapter,\n        audio_path,\n        chunk_duration=12.0)\n    return builder.build(\n        audio_params.get(\'validation_csv\'),\n        batch_size=audio_params.get(\'batch_size\'),\n        cache_directory=audio_params.get(\'validation_cache\'),\n        convert_to_uint=True,\n        infinite_generator=False,\n        n_chunks_per_song=1,\n        # should not perform data augmentation for eval:\n        random_data_augmentation=False,\n        random_time_crop=False,\n        shuffle=False,\n    )\n\n\nclass InstrumentDatasetBuilder(object):\n    """""" Instrument based filter and mapper provider. """"""\n\n    def __init__(self, parent, instrument):\n        """""" Default constructor.\n\n        :param parent: Parent dataset builder.\n        :param instrument: Target instrument.\n        """"""\n        self._parent = parent\n        self._instrument = instrument\n        self._spectrogram_key = f\'{instrument}_spectrogram\'\n        self._min_spectrogram_key = f\'min_{instrument}_spectrogram\'\n        self._max_spectrogram_key = f\'max_{instrument}_spectrogram\'\n\n    def load_waveform(self, sample):\n        """""" Load waveform for given sample. """"""\n        return dict(sample, **self._parent._audio_adapter.load_tf_waveform(\n            sample[f\'{self._instrument}_path\'],\n            offset=sample[\'start\'],\n            duration=self._parent._chunk_duration,\n            sample_rate=self._parent._sample_rate,\n            waveform_name=\'waveform\'))\n\n    def compute_spectrogram(self, sample):\n        """""" Compute spectrogram of the given sample. """"""\n        return dict(sample, **{\n            self._spectrogram_key: compute_spectrogram_tf(\n                sample[\'waveform\'],\n                frame_length=self._parent._frame_length,\n                frame_step=self._parent._frame_step,\n                spec_exponent=1.,\n                window_exponent=1.)})\n\n    def filter_frequencies(self, sample):\n        """""" """"""\n        return dict(sample, **{\n            self._spectrogram_key:\n                sample[self._spectrogram_key][:, :self._parent._F, :]})\n\n    def convert_to_uint(self, sample):\n        """""" Convert given sample from float to unit. """"""\n        return dict(sample, **spectrogram_to_db_uint(\n            sample[self._spectrogram_key],\n            tensor_key=self._spectrogram_key,\n            min_key=self._min_spectrogram_key,\n            max_key=self._max_spectrogram_key))\n\n    def filter_infinity(self, sample):\n        """""" Filter infinity sample. """"""\n        return tf.logical_not(\n            tf.math.is_inf(\n                sample[self._min_spectrogram_key]))\n\n    def convert_to_float32(self, sample):\n        """""" Convert given sample from unit to float. """"""\n        return dict(sample, **{\n            self._spectrogram_key: db_uint_spectrogram_to_gain(\n                sample[self._spectrogram_key],\n                sample[self._min_spectrogram_key],\n                sample[self._max_spectrogram_key])})\n\n    def time_crop(self, sample):\n        """""" """"""\n        def start(sample):\n            """""" mid_segment_start """"""\n            return tf.cast(\n                tf.maximum(\n                    tf.shape(sample[self._spectrogram_key])[0]\n                    / 2 - self._parent._T / 2, 0),\n                tf.int32)\n        return dict(sample, **{\n            self._spectrogram_key: sample[self._spectrogram_key][\n                start(sample):start(sample) + self._parent._T, :, :]})\n\n    def filter_shape(self, sample):\n        """""" Filter badly shaped sample. """"""\n        return check_tensor_shape(\n            sample[self._spectrogram_key], (\n                self._parent._T, self._parent._F, 2))\n\n    def reshape_spectrogram(self, sample):\n        """""" """"""\n        return dict(sample, **{\n            self._spectrogram_key: set_tensor_shape(\n                sample[self._spectrogram_key],\n                (self._parent._T, self._parent._F, 2))})\n\n\nclass DatasetBuilder(object):\n    """"""\n    """"""\n\n    # Margin at beginning and end of songs in seconds.\n    MARGIN = 0.5\n\n    # Wait period for cache (in seconds).\n    WAIT_PERIOD = 60\n\n    def __init__(\n            self,\n            audio_params, audio_adapter, audio_path,\n            random_seed=0, chunk_duration=20.0):\n        """""" Default constructor.\n\n        NOTE: Probably need for AudioAdapter.\n\n        :param audio_params: Audio parameters to use.\n        :param audio_adapter: Audio adapter to use.\n        :param audio_path:\n        :param random_seed:\n        :param chunk_duration:\n        """"""\n        # Length of segment in frames (if fs=22050 and\n        # frame_step=512, then T=512 corresponds to 11.89s)\n        self._T = audio_params[\'T\']\n        # Number of frequency bins to be used (should\n        # be less than frame_length/2 + 1)\n        self._F = audio_params[\'F\']\n        self._sample_rate = audio_params[\'sample_rate\']\n        self._frame_length = audio_params[\'frame_length\']\n        self._frame_step = audio_params[\'frame_step\']\n        self._mix_name = audio_params[\'mix_name\']\n        self._instruments = [self._mix_name] + audio_params[\'instrument_list\']\n        self._instrument_builders = None\n        self._chunk_duration = chunk_duration\n        self._audio_adapter = audio_adapter\n        self._audio_params = audio_params\n        self._audio_path = audio_path\n        self._random_seed = random_seed\n\n    def expand_path(self, sample):\n        """""" Expands audio paths for the given sample. """"""\n        return dict(sample, **{f\'{instrument}_path\': tf.string_join(\n            (self._audio_path, sample[f\'{instrument}_path\']), SEPARATOR)\n            for instrument in self._instruments})\n\n    def filter_error(self, sample):\n        """""" Filter errored sample. """"""\n        return tf.logical_not(sample[\'waveform_error\'])\n\n    def filter_waveform(self, sample):\n        """""" Filter waveform from sample. """"""\n        return {k: v for k, v in sample.items() if not k == \'waveform\'}\n\n    def harmonize_spectrogram(self, sample):\n        """""" Ensure same size for vocals and mix spectrograms. """"""\n        def _reduce(sample):\n            return tf.reduce_min([\n                tf.shape(sample[f\'{instrument}_spectrogram\'])[0]\n                for instrument in self._instruments])\n        return dict(sample, **{\n            f\'{instrument}_spectrogram\':\n                sample[f\'{instrument}_spectrogram\'][:_reduce(sample), :, :]\n            for instrument in self._instruments})\n\n    def filter_short_segments(self, sample):\n        """""" Filter out too short segment. """"""\n        return tf.reduce_any([\n            tf.shape(sample[f\'{instrument}_spectrogram\'])[0] >= self._T\n            for instrument in self._instruments])\n\n    def random_time_crop(self, sample):\n        """""" Random time crop of 11.88s. """"""\n        return dict(sample, **sync_apply({\n            f\'{instrument}_spectrogram\': sample[f\'{instrument}_spectrogram\']\n            for instrument in self._instruments},\n            lambda x: tf.image.random_crop(\n                x, (self._T, len(self._instruments) * self._F, 2),\n                seed=self._random_seed)))\n\n    def random_time_stretch(self, sample):\n        """""" Randomly time stretch the given sample. """"""\n        return dict(sample, **sync_apply({\n            f\'{instrument}_spectrogram\':\n                sample[f\'{instrument}_spectrogram\']\n            for instrument in self._instruments},\n            lambda x: random_time_stretch(\n                x, factor_min=0.9, factor_max=1.1)))\n\n    def random_pitch_shift(self, sample):\n        """""" Randomly pitch shift the given sample. """"""\n        return dict(sample, **sync_apply({\n            f\'{instrument}_spectrogram\':\n                sample[f\'{instrument}_spectrogram\']\n            for instrument in self._instruments},\n            lambda x: random_pitch_shift(\n                x, shift_min=-1.0, shift_max=1.0), concat_axis=0))\n\n    def map_features(self, sample):\n        """""" Select features and annotation of the given sample. """"""\n        input_ = {\n            f\'{self._mix_name}_spectrogram\':\n                sample[f\'{self._mix_name}_spectrogram\']}\n        output = {\n            f\'{instrument}_spectrogram\': sample[f\'{instrument}_spectrogram\']\n            for instrument in self._audio_params[\'instrument_list\']}\n        return (input_, output)\n\n    def compute_segments(self, dataset, n_chunks_per_song):\n        """""" Computes segments for each song of the dataset.\n\n        :param dataset: Dataset to compute segments for.\n        :param n_chunks_per_song: Number of segment per song to compute.\n        :returns: Segmented dataset.\n        """"""\n        if n_chunks_per_song <= 0:\n            raise ValueError(\'n_chunks_per_song must be positif\')\n        datasets = []\n        for k in range(n_chunks_per_song):\n            if n_chunks_per_song > 1:\n                datasets.append(\n                    dataset.map(lambda sample: dict(sample, start=tf.maximum(\n                        k * (\n                            sample[\'duration\'] - self._chunk_duration - 2\n                            * self.MARGIN) / (n_chunks_per_song - 1)\n                        + self.MARGIN, 0))))\n            elif n_chunks_per_song == 1:  # Take central segment.\n                datasets.append(\n                    dataset.map(lambda sample: dict(sample, start=tf.maximum(\n                        sample[\'duration\'] / 2 - self._chunk_duration / 2,\n                        0))))\n        dataset = datasets[-1]\n        for d in datasets[:-1]:\n            dataset = dataset.concatenate(d)\n        return dataset\n\n    @property\n    def instruments(self):\n        """""" Instrument dataset builder generator.\n\n        :yield InstrumentBuilder instance.\n        """"""\n        if self._instrument_builders is None:\n            self._instrument_builders = []\n            for instrument in self._instruments:\n                self._instrument_builders.append(\n                    InstrumentDatasetBuilder(self, instrument))\n        for builder in self._instrument_builders:\n            yield builder\n\n    def cache(self, dataset, cache, wait):\n        """""" Cache the given dataset if cache is enabled. Eventually waits for\n        cache to be available (useful if another process is already computing\n        cache) if provided wait flag is True.\n\n        :param dataset: Dataset to be cached if cache is required.\n        :param cache: Path of cache directory to be used, None if no cache.\n        :param wait: If caching is enabled, True is cache should be waited.\n        :returns: Cached dataset if needed, original dataset otherwise.\n        """"""\n        if cache is not None:\n            if wait:\n                while not exists(f\'{cache}.index\'):\n                    get_logger().info(\n                        \'Cache not available, wait %s\',\n                        self.WAIT_PERIOD)\n                    time.sleep(self.WAIT_PERIOD)\n            cache_path = os.path.split(cache)[0]\n            os.makedirs(cache_path, exist_ok=True)\n            return dataset.cache(cache)\n        return dataset\n\n    def build(\n            self, csv_path,\n            batch_size=8, shuffle=True, convert_to_uint=True,\n            random_data_augmentation=False, random_time_crop=True,\n            infinite_generator=True, cache_directory=None,\n            wait_for_cache=False, num_parallel_calls=4, n_chunks_per_song=2,):\n        """"""\n        TO BE DOCUMENTED.\n        """"""\n        dataset = dataset_from_csv(csv_path)\n        dataset = self.compute_segments(dataset, n_chunks_per_song)\n        # Shuffle data\n        if shuffle:\n            dataset = dataset.shuffle(\n                buffer_size=200000,\n                seed=self._random_seed,\n                # useless since it is cached :\n                reshuffle_each_iteration=True)\n        # Expand audio path.\n        dataset = dataset.map(self.expand_path)\n        # Load waveform, compute spectrogram, and filtering error,\n        # K bins frequencies, and waveform.\n        N = num_parallel_calls\n        for instrument in self.instruments:\n            dataset = (\n                dataset\n                .map(instrument.load_waveform, num_parallel_calls=N)\n                .filter(self.filter_error)\n                .map(instrument.compute_spectrogram, num_parallel_calls=N)\n                .map(instrument.filter_frequencies))\n        dataset = dataset.map(self.filter_waveform)\n        # Convert to uint before caching in order to save space.\n        if convert_to_uint:\n            for instrument in self.instruments:\n                dataset = dataset.map(instrument.convert_to_uint)\n        dataset = self.cache(dataset, cache_directory, wait_for_cache)\n        # Check for INFINITY (should not happen)\n        for instrument in self.instruments:\n            dataset = dataset.filter(instrument.filter_infinity)\n        # Repeat indefinitly\n        if infinite_generator:\n            dataset = dataset.repeat(count=-1)\n        # Ensure same size for vocals and mix spectrograms.\n        # NOTE: could be done before caching ?\n        dataset = dataset.map(self.harmonize_spectrogram)\n        # Filter out too short segment.\n        # NOTE: could be done before caching ?\n        dataset = dataset.filter(self.filter_short_segments)\n        # Random time crop of 11.88s\n        if random_time_crop:\n            dataset = dataset.map(self.random_time_crop, num_parallel_calls=N)\n        else:\n            # frame_duration = 11.88/T\n            # take central segment (for validation)\n            for instrument in self.instruments:\n                dataset = dataset.map(instrument.time_crop)\n        # Post cache shuffling. Done where the data are the lightest:\n        # after croping but before converting back to float.\n        if shuffle:\n            dataset = dataset.shuffle(\n                buffer_size=256, seed=self._random_seed,\n                reshuffle_each_iteration=True)\n        # Convert back to float32\n        if convert_to_uint:\n            for instrument in self.instruments:\n                dataset = dataset.map(\n                    instrument.convert_to_float32, num_parallel_calls=N)\n        M = 8  # Parallel call post caching.\n        # Must be applied with the same factor on mix and vocals.\n        if random_data_augmentation:\n            dataset = (\n                dataset\n                .map(self.random_time_stretch, num_parallel_calls=M)\n                .map(self.random_pitch_shift, num_parallel_calls=M))\n        # Filter by shape (remove badly shaped tensors).\n        for instrument in self.instruments:\n            dataset = (\n                dataset\n                .filter(instrument.filter_shape)\n                .map(instrument.reshape_spectrogram))\n        # Select features and annotation.\n        dataset = dataset.map(self.map_features)\n        # Make batch (done after selection to avoid\n        # error due to unprocessed instrument spectrogram batching).\n        dataset = dataset.batch(batch_size)\n        return dataset\n'"
spleeter/separator.py,5,"b'#!/usr/bin/env python\n# coding: utf8\n\n""""""\n    Module that provides a class wrapper for source separation.\n\n    :Example:\n\n    >>> from spleeter.separator import Separator\n    >>> separator = Separator(\'spleeter:2stems\')\n    >>> separator.separate(waveform, lambda instrument, data: ...)\n    >>> separator.separate_to_file(...)\n""""""\n\nimport os\nimport logging\n\nfrom time import time\nfrom multiprocessing import Pool\nfrom os.path import basename, join, splitext, dirname\nimport numpy as np\nimport tensorflow as tf\nfrom librosa.core import stft, istft\nfrom scipy.signal.windows import hann\n\nfrom . import SpleeterError\nfrom .audio.adapter import get_default_audio_adapter\nfrom .audio.convertor import to_stereo\nfrom .utils.configuration import load_configuration\nfrom .utils.estimator import create_estimator, to_predictor, get_default_model_dir\nfrom .model import EstimatorSpecBuilder, InputProviderFactory\n\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n\n\nlogger = logging.getLogger(""spleeter"")\n\n\n\ndef get_backend(backend):\n    assert backend in [""auto"", ""tensorflow"", ""librosa""]\n    if backend == ""auto"":\n        return ""tensorflow"" if tf.test.is_gpu_available() else ""librosa""\n    return backend\n\n\nclass Separator(object):\n    """""" A wrapper class for performing separation. """"""\n\n    def __init__(self, params_descriptor, MWF=False, stft_backend=""auto"", multiprocess=True):\n        """""" Default constructor.\n\n        :param params_descriptor: Descriptor for TF params to be used.\n        :param MWF: (Optional) True if MWF should be used, False otherwise.\n        """"""\n\n        self._params = load_configuration(params_descriptor)\n        self._sample_rate = self._params[\'sample_rate\']\n        self._MWF = MWF\n        self._tf_graph = tf.Graph()\n        self._predictor = None\n        self._input_provider = None\n        self._builder = None\n        self._features = None\n        self._session = None\n        self._pool = Pool() if multiprocess else None\n        self._tasks = []\n        self._params[""stft_backend""] = get_backend(stft_backend)\n\n    def __del__(self):\n        if self._session:\n            self._session.close()\n\n    def _get_predictor(self):\n        """""" Lazy loading access method for internal predictor instance.\n\n        :returns: Predictor to use for source separation.\n        """"""\n        if self._predictor is None:\n            estimator = create_estimator(self._params, self._MWF)\n            self._predictor = to_predictor(estimator)\n        return self._predictor\n\n    def join(self, timeout=200):\n        """""" Wait for all pending tasks to be finished.\n\n        :param timeout: (Optional) task waiting timeout.\n        """"""\n        while len(self._tasks) > 0:\n            task = self._tasks.pop()\n            task.get()\n            task.wait(timeout=timeout)\n\n    def _separate_tensorflow(self, waveform, audio_descriptor):\n        """"""\n        Performs source separation over the given waveform with tensorflow backend.\n\n        :param waveform: Waveform to apply separation on.\n        :returns: Separated waveforms.\n        """"""\n        if not waveform.shape[-1] == 2:\n            waveform = to_stereo(waveform)\n        predictor = self._get_predictor()\n        prediction = predictor({\n            \'waveform\': waveform,\n            \'audio_id\': audio_descriptor})\n        prediction.pop(\'audio_id\')\n        return prediction\n\n    def _stft(self, data, inverse=False, length=None):\n        """"""\n        Single entrypoint for both stft and istft. This computes stft and istft with librosa on stereo data. The two\n        channels are processed separately and are concatenated together in the result. The expected input formats are:\n        (n_samples, 2) for stft and (T, F, 2) for istft.\n        :param data: np.array with either the waveform or the complex spectrogram depending on the parameter inverse\n        :param inverse: should a stft or an istft be computed.\n        :return: Stereo data as numpy array for the transform. The channels are stored in the last dimension\n        """"""\n        assert not (inverse and length is None)\n        data = np.asfortranarray(data)\n        N = self._params[""frame_length""]\n        H = self._params[""frame_step""]\n        win = hann(N, sym=False)\n        fstft = istft if inverse else stft\n        win_len_arg = {""win_length"": None, ""length"": length} if inverse else {""n_fft"": N}\n        n_channels = data.shape[-1]\n        out = []\n        for c in range(n_channels):\n            d = data[:, :, c].T if inverse else data[:, c]\n            s = fstft(d, hop_length=H, window=win, center=False, **win_len_arg)\n            s = np.expand_dims(s.T, 2-inverse)\n            out.append(s)\n        if len(out) == 1:\n            return out[0]\n        return np.concatenate(out, axis=2-inverse)\n\n\n    def _get_input_provider(self):\n        if self._input_provider is None:\n            self._input_provider = InputProviderFactory.get(self._params)\n        return self._input_provider\n\n    def _get_features(self):\n        if self._features is None:\n            self._features = self._get_input_provider().get_input_dict_placeholders()\n        return self._features\n\n    def _get_builder(self):\n        if self._builder is None:\n            self._builder = EstimatorSpecBuilder(self._get_features(), self._params)\n        return self._builder\n\n    def _get_session(self):\n        if self._session is None:\n            saver = tf.train.Saver()\n            latest_checkpoint = tf.train.latest_checkpoint(get_default_model_dir(self._params[\'model_dir\']))\n            self._session = tf.Session()\n            saver.restore(self._session, latest_checkpoint)\n        return self._session\n\n    def _separate_librosa(self, waveform, audio_id):\n        """"""\n        Performs separation with librosa backend for STFT.\n        """"""\n        with self._tf_graph.as_default():\n            out = {}\n            features = self._get_features()\n\n            # TODO: fix the logic, build sometimes return, sometimes set attribute\n            outputs = self._get_builder().outputs\n            stft = self._stft(waveform)\n            if stft.shape[-1] == 1:\n                stft = np.concatenate([stft, stft], axis=-1)\n            elif stft.shape[-1] > 2:\n                stft = stft[:, :2]\n\n            sess = self._get_session()\n            outputs = sess.run(outputs, feed_dict=self._get_input_provider().get_feed_dict(features, stft, audio_id))\n            for inst in self._get_builder().instruments:\n                out[inst] = self._stft(outputs[inst], inverse=True, length=waveform.shape[0])\n            return out\n\n    def separate(self, waveform, audio_descriptor=""""):\n        """""" Performs separation on a waveform.\n\n        :param waveform:            Waveform to be separated (as a numpy array)\n        :param audio_descriptor:    (Optional) string describing the waveform (e.g. filename).\n        """"""\n        if self._params[""stft_backend""] == ""tensorflow"":\n            return self._separate_tensorflow(waveform, audio_descriptor)\n        else:\n            return self._separate_librosa(waveform, audio_descriptor)\n\n    def separate_to_file(\n            self, audio_descriptor, destination,\n            audio_adapter=get_default_audio_adapter(),\n            offset=0, duration=600., codec=\'wav\', bitrate=\'128k\',\n            filename_format=\'{filename}/{instrument}.{codec}\',\n            synchronous=True):\n        """""" Performs source separation and export result to file using\n        given audio adapter.\n\n        Filename format should be a Python formattable string that could use\n        following parameters : {instrument}, {filename}, {foldername} and {codec}.\n\n        :param audio_descriptor:    Describe song to separate, used by audio\n                                    adapter to retrieve and load audio data,\n                                    in case of file based audio adapter, such\n                                    descriptor would be a file path.\n        :param destination:         Target directory to write output to.\n        :param audio_adapter:       (Optional) Audio adapter to use for I/O.\n        :param offset:              (Optional) Offset of loaded song.\n        :param duration:            (Optional) Duration of loaded song (default:\n                                    600s).\n        :param codec:               (Optional) Export codec.\n        :param bitrate:             (Optional) Export bitrate.\n        :param filename_format:     (Optional) Filename format.\n        :param synchronous:         (Optional) True is should by synchronous.\n        """"""\n        waveform, sample_rate = audio_adapter.load(\n            audio_descriptor,\n            offset=offset,\n            duration=duration,\n            sample_rate=self._sample_rate)\n        sources = self.separate(waveform, audio_descriptor)\n        self.save_to_file(  sources, audio_descriptor, destination,\n                            filename_format, codec, audio_adapter,\n                            bitrate, synchronous)\n\n    def save_to_file(\n            self, sources, audio_descriptor, destination,\n            filename_format=\'{filename}/{instrument}.{codec}\',\n            codec=\'wav\', audio_adapter=get_default_audio_adapter(),\n            bitrate=\'128k\', synchronous=True):\n        """""" export dictionary of sources to files.\n\n        :param sources:             Dictionary of sources to be exported. The\n                                    keys are the name of the instruments, and\n                                    the values are Nx2 numpy arrays containing\n                                    the corresponding intrument waveform, as\n                                    returned by the separate method\n        :param audio_descriptor:    Describe song to separate, used by audio\n                                    adapter to retrieve and load audio data,\n                                    in case of file based audio adapter, such\n                                    descriptor would be a file path.\n        :param destination:         Target directory to write output to.\n        :param filename_format:     (Optional) Filename format.\n        :param codec:               (Optional) Export codec.\n        :param audio_adapter:       (Optional) Audio adapter to use for I/O.\n        :param bitrate:             (Optional) Export bitrate.\n        :param synchronous:         (Optional) True is should by synchronous.\n\n        """"""\n\n        foldername = basename(dirname(audio_descriptor))\n        filename = splitext(basename(audio_descriptor))[0]\n        generated = []\n        for instrument, data in sources.items():\n            path = join(destination, filename_format.format(\n                filename=filename,\n                instrument=instrument,\n                foldername=foldername,\n                codec=codec,\n                ))\n            directory = os.path.dirname(path)\n            if not os.path.exists(directory):\n                os.makedirs(directory)\n            if path in generated:\n                raise SpleeterError((\n                    f\'Separated source path conflict : {path},\'\n                    \'please check your filename format\'))\n            generated.append(path)\n            if self._pool:\n                task = self._pool.apply_async(audio_adapter.save, (\n                    path,\n                    data,\n                    self._sample_rate,\n                    codec,\n                    bitrate))\n                self._tasks.append(task)\n            else:\n                audio_adapter.save(path, data, self._sample_rate, codec, bitrate)\n        if synchronous and self._pool:\n            self.join()\n'"
tests/__init__.py,0,"b'#!/usr/bin/env python\n# coding: utf8\n\n"""""" Unit testing package. """"""\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n'"
tests/test_eval.py,0,"b'#!/usr/bin/env python\n# coding: utf8\n\n"""""" Unit testing for Separator class. """"""\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n\nimport filecmp\nimport itertools\nfrom os import makedirs\nfrom os.path import splitext, basename, exists, join\nfrom tempfile import TemporaryDirectory\n\nimport pytest\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom spleeter.audio.adapter import get_default_audio_adapter\nfrom spleeter.commands import create_argument_parser\n\nfrom spleeter.commands import evaluate\n\nfrom spleeter.utils.configuration import load_configuration\n\nres_4stems = {  ""vocals"": {\n                    ""SDR"": -0.007,\n                    ""SAR"": -19.231,\n                    ""SIR"": -4.528,\n                    ""ISR"": 0.000\n                },\n                ""drums"": {\n                    ""SDR"": -0.071,\n                    ""SAR"": -14.496,\n                    ""SIR"": -4.987,\n                    ""ISR"": 0.001\n                },\n                ""bass"":{\n                    ""SDR"": -0.001,\n                    ""SAR"": -12.426,\n                    ""SIR"": -7.198,\n                    ""ISR"": -0.001\n                },\n                ""other"":{\n                    ""SDR"": -1.453,\n                    ""SAR"": -14.899,\n                    ""SIR"": -4.678,\n                    ""ISR"": -0.015\n                }\n            }\n\n\ndef generate_fake_eval_dataset(path):\n    aa = get_default_audio_adapter()\n    n_songs = 2\n    fs = 44100\n    duration = 3\n    n_channels = 2\n    rng = np.random.RandomState(seed=0)\n    for song in range(n_songs):\n        song_path = join(path, ""test"", f""song{song}"")\n        makedirs(song_path, exist_ok=True)\n        for instr in [""mixture"", ""vocals"", ""bass"", ""drums"", ""other""]:\n            filename = join(song_path, f""{instr}.wav"")\n            data = rng.rand(duration*fs, n_channels)-0.5\n            aa.save(filename, data, fs)\n\n\ndef test_evaluate(path=""FAKE_MUSDB_DIR""):\n    generate_fake_eval_dataset(path)\n    p = create_argument_parser()\n    arguments = p.parse_args([""evaluate"", ""-p"", ""spleeter:4stems"", ""--mus_dir"", path])\n    params = load_configuration(arguments.configuration)\n    metrics = evaluate.entrypoint(arguments, params)\n    for instrument, metric in metrics.items():\n        for metric, value in metric.items():\n            assert np.allclose(np.median(value), res_4stems[instrument][metric], atol=1e-3)'"
tests/test_ffmpeg_adapter.py,0,"b'#!/usr/bin/env python\n# coding: utf8\n\n"""""" Unit testing for audio adapter. """"""\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n\nfrom os.path import join\nfrom tempfile import TemporaryDirectory\n\n# pylint: disable=import-error\nfrom pytest import fixture, raises\n\nimport numpy as np\nimport ffmpeg\n# pylint: enable=import-error\n\nfrom spleeter import SpleeterError\nfrom spleeter.audio.adapter import AudioAdapter\nfrom spleeter.audio.adapter import get_default_audio_adapter\nfrom spleeter.audio.adapter import get_audio_adapter\nfrom spleeter.audio.ffmpeg import FFMPEGProcessAudioAdapter\n\nTEST_AUDIO_DESCRIPTOR = \'audio_example.mp3\'\nTEST_OFFSET = 0\nTEST_DURATION = 600.\nTEST_SAMPLE_RATE = 44100\n\n\n@fixture(scope=\'session\')\ndef adapter():\n    """""" Target test audio adapter fixture. """"""\n    return get_default_audio_adapter()\n\n\n@fixture(scope=\'session\')\ndef audio_data(adapter):\n    """""" Audio data fixture based on sample loading from adapter. """"""\n    return adapter.load(\n        TEST_AUDIO_DESCRIPTOR,\n        TEST_OFFSET,\n        TEST_DURATION,\n        TEST_SAMPLE_RATE)\n\n\ndef test_default_adapter(adapter):\n    """""" Test adapter as default adapter. """"""\n    assert isinstance(adapter, FFMPEGProcessAudioAdapter)\n    assert adapter is AudioAdapter.DEFAULT\n\n\ndef test_load(audio_data):\n    """""" Test audio loading. """"""\n    waveform, sample_rate = audio_data\n    assert sample_rate == TEST_SAMPLE_RATE\n    assert waveform is not None\n    assert waveform.dtype == np.dtype(\'float32\')\n    assert len(waveform.shape) == 2\n    assert waveform.shape[0] == 479832\n    assert waveform.shape[1] == 2\n\n\ndef test_load_error(adapter):\n    """""" Test load ffprobe exception """"""\n    with raises(SpleeterError):\n        adapter.load(\n            \'Paris City Jazz\',\n            TEST_OFFSET,\n            TEST_DURATION,\n            TEST_SAMPLE_RATE)\n\n\ndef test_save(adapter, audio_data):\n    """""" Test audio saving. """"""\n    with TemporaryDirectory() as directory:\n        path = join(directory, \'ffmpeg-save.mp3\')\n        adapter.save(\n            path,\n            audio_data[0],\n            audio_data[1])\n        probe = ffmpeg.probe(TEST_AUDIO_DESCRIPTOR)\n        assert len(probe[\'streams\']) == 1\n        stream = probe[\'streams\'][0]\n        assert stream[\'codec_type\'] == \'audio\'\n        assert stream[\'channels\'] == 2\n        assert stream[\'duration\'] == \'10.919184\'\n'"
tests/test_github_model_provider.py,0,"b'#!/usr/bin/env python\n# coding: utf8\n\n"""""" TO DOCUMENT """"""\n\nfrom pytest import raises\n\nfrom spleeter.model.provider import get_default_model_provider\n\n\ndef test_checksum():\n    """""" Test archive checksum index retrieval. """"""\n    provider = get_default_model_provider()\n    assert provider.checksum(\'2stems\') == \\\n        \'f3a90b39dd2874269e8b05a48a86745df897b848c61f3958efc80a39152bd692\'\n    assert provider.checksum(\'4stems\') == \\\n        \'3adb4a50ad4eb18c7c4d65fcf4cf2367a07d48408a5eb7d03cd20067429dfaa8\'\n    assert provider.checksum(\'5stems\') == \\\n        \'25a1e87eb5f75cc72a4d2d5467a0a50ac75f05611f877c278793742513cc7218\'\n    with raises(ValueError):\n        provider.checksum(\'laisse moi stems stems stems\')\n'"
tests/test_separator.py,5,"b'#!/usr/bin/env python\n# coding: utf8\n\n"""""" Unit testing for Separator class. """"""\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n\nimport filecmp\nimport itertools\nfrom os.path import splitext, basename, exists, join\nfrom tempfile import TemporaryDirectory\n\nimport pytest\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom spleeter import SpleeterError\nfrom spleeter.audio.adapter import get_default_audio_adapter\nfrom spleeter.separator import Separator\n\nTEST_AUDIO_DESCRIPTORS = [\'audio_example.mp3\', \'audio_example_mono.mp3\']\nBACKENDS = [""tensorflow"", ""librosa""]\nMODELS = [\'spleeter:2stems\', \'spleeter:4stems\', \'spleeter:5stems\']\n\nMODEL_TO_INST = {\n    \'spleeter:2stems\': (\'vocals\', \'accompaniment\'),\n    \'spleeter:4stems\': (\'vocals\', \'drums\', \'bass\', \'other\'),\n    \'spleeter:5stems\': (\'vocals\', \'drums\', \'bass\', \'piano\', \'other\'),\n}\n\n\nMODELS_AND_TEST_FILES = list(itertools.product(TEST_AUDIO_DESCRIPTORS, MODELS))\nTEST_CONFIGURATIONS = list(itertools.product(TEST_AUDIO_DESCRIPTORS, MODELS, BACKENDS))\n\n\nprint(""RUNNING TESTS WITH TF VERSION {}"".format(tf.__version__))\n\n\n@pytest.mark.parametrize(\'test_file, configuration, backend\', TEST_CONFIGURATIONS)\ndef test_separate(test_file, configuration, backend):\n    """""" Test separation from raw data. """"""\n    tf.reset_default_graph()\n    instruments = MODEL_TO_INST[configuration]\n    adapter = get_default_audio_adapter()\n    waveform, _ = adapter.load(test_file)\n    separator = Separator(configuration, stft_backend=backend)\n    prediction = separator.separate(waveform, test_file)\n    assert len(prediction) == len(instruments)\n    for instrument in instruments:\n        assert instrument in prediction\n    for instrument in instruments:\n        track = prediction[instrument]\n        assert waveform.shape[:-1] == track.shape[:-1]\n        assert not np.allclose(waveform, track)\n        for compared in instruments:\n            if instrument != compared:\n                assert not np.allclose(track, prediction[compared])\n\n\n@pytest.mark.parametrize(\'test_file, configuration, backend\', TEST_CONFIGURATIONS)\ndef test_separate_to_file(test_file, configuration, backend):\n    """""" Test file based separation. """"""\n    tf.reset_default_graph()\n    instruments = MODEL_TO_INST[configuration]\n    separator = Separator(configuration, stft_backend=backend)\n    name = splitext(basename(test_file))[0]\n    with TemporaryDirectory() as directory:\n        separator.separate_to_file(\n            test_file,\n            directory)\n        for instrument in instruments:\n            assert exists(join(\n                directory,\n                \'{}/{}.wav\'.format(name, instrument)))\n\n\n@pytest.mark.parametrize(\'test_file, configuration, backend\', TEST_CONFIGURATIONS)\ndef test_filename_format(test_file, configuration, backend):\n    """""" Test custom filename format. """"""\n    tf.reset_default_graph()\n    instruments = MODEL_TO_INST[configuration]\n    separator = Separator(configuration, stft_backend=backend)\n    name = splitext(basename(test_file))[0]\n    with TemporaryDirectory() as directory:\n        separator.separate_to_file(\n            test_file,\n            directory,\n            filename_format=\'export/{filename}/{instrument}.{codec}\')\n        for instrument in instruments:\n            assert exists(join(\n                directory,\n                \'export/{}/{}.wav\'.format(name, instrument)))\n\n\n@pytest.mark.parametrize(\'test_file, configuration\', MODELS_AND_TEST_FILES)\ndef test_filename_conflict(test_file, configuration):\n    """""" Test error handling with static pattern. """"""\n    tf.reset_default_graph()\n    separator = Separator(configuration)\n    with TemporaryDirectory() as directory:\n        with pytest.raises(SpleeterError):\n            separator.separate_to_file(\n                test_file,\n                directory,\n                filename_format=\'I wanna be your lover\')\n'"
spleeter/audio/__init__.py,0,"b'#!/usr/bin/env python\n# coding: utf8\n\n""""""\n    `spleeter.utils.audio` package provides various\n    tools for manipulating audio content such as :\n\n    - Audio adapter class for abstract interaction with audio file.\n    - FFMPEG implementation for audio adapter.\n    - Waveform convertion and transforming functions.\n""""""\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n'"
spleeter/audio/adapter.py,4,"b'#!/usr/bin/env python\n# coding: utf8\n\n"""""" AudioAdapter class defintion. """"""\n\nimport subprocess\n\nfrom abc import ABC, abstractmethod\nfrom importlib import import_module\nfrom os.path import exists\n\n# pylint: disable=import-error\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.contrib.signal import stft, hann_window\n# pylint: enable=import-error\n\nfrom .. import SpleeterError\nfrom ..utils.logging import get_logger\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n\n\nclass AudioAdapter(ABC):\n    """""" An abstract class for manipulating audio signal. """"""\n\n    # Default audio adapter singleton instance.\n    DEFAULT = None\n\n    @abstractmethod\n    def load(\n            self, audio_descriptor, offset, duration,\n            sample_rate, dtype=np.float32):\n        """""" Loads the audio file denoted by the given audio descriptor\n        and returns it data as a waveform. Aims to be implemented\n        by client.\n\n        :param audio_descriptor:    Describe song to load, in case of file\n                                    based audio adapter, such descriptor would\n                                    be a file path.\n        :param offset:              Start offset to load from in seconds.\n        :param duration:            Duration to load in seconds.\n        :param sample_rate:         Sample rate to load audio with.\n        :param dtype:               Numpy data type to use, default to float32.\n        :returns:                   Loaded data as (wf, sample_rate) tuple.\n        """"""\n        pass\n\n    def load_tf_waveform(\n            self, audio_descriptor,\n            offset=0.0, duration=1800., sample_rate=44100,\n            dtype=b\'float32\', waveform_name=\'waveform\'):\n        """""" Load the audio and convert it to a tensorflow waveform.\n\n        :param audio_descriptor:    Describe song to load, in case of file\n                                    based audio adapter, such descriptor would\n                                    be a file path.\n        :param offset:              Start offset to load from in seconds.\n        :param duration:            Duration to load in seconds.\n        :param sample_rate:         Sample rate to load audio with.\n        :param dtype:               Numpy data type to use, default to float32.\n        :param waveform_name:       (Optional) Name of the key in output dict.\n        :returns:                   TF output dict with waveform as\n                                    (T x chan numpy array)  and a boolean that\n                                    tells whether there were an error while\n                                    trying to load the waveform.\n        """"""\n        # Cast parameters to TF format.\n        offset = tf.cast(offset, tf.float64)\n        duration = tf.cast(duration, tf.float64)\n\n        # Defined safe loading function.\n        def safe_load(path, offset, duration, sample_rate, dtype):\n            logger = get_logger()\n            logger.info(\n                f\'Loading audio {path} from {offset} to {offset + duration}\')\n            try:\n                (data, _) = self.load(\n                    path.numpy(),\n                    offset.numpy(),\n                    duration.numpy(),\n                    sample_rate.numpy(),\n                    dtype=dtype.numpy())\n                logger.info(\'Audio data loaded successfully\')\n                return (data, False)\n            except Exception as e:\n                logger.exception(\n                    \'An error occurs while loading audio\',\n                    exc_info=e)\n            return (np.float32(-1.0), True)\n\n        # Execute function and format results.\n        results = tf.py_function(\n            safe_load,\n            [audio_descriptor, offset, duration, sample_rate, dtype],\n            (tf.float32, tf.bool)),\n        waveform, error = results[0]\n        return {\n            waveform_name: waveform,\n            f\'{waveform_name}_error\': error\n        }\n\n    @abstractmethod\n    def save(\n            self, path, data, sample_rate,\n            codec=None, bitrate=None):\n        """""" Save the given audio data to the file denoted by\n        the given path.\n\n        :param path: Path of the audio file to save data in.\n        :param data: Waveform data to write.\n        :param sample_rate: Sample rate to write file in.\n        :param codec: (Optional) Writing codec to use.\n        :param bitrate: (Optional) Bitrate of the written audio file.\n        """"""\n        pass\n\n\ndef get_default_audio_adapter():\n    """""" Builds and returns a default audio adapter instance.\n\n    :returns: An audio adapter instance.\n    """"""\n    if AudioAdapter.DEFAULT is None:\n        from .ffmpeg import FFMPEGProcessAudioAdapter\n        AudioAdapter.DEFAULT = FFMPEGProcessAudioAdapter()\n    return AudioAdapter.DEFAULT\n\n\ndef get_audio_adapter(descriptor):\n    """""" Load dynamically an AudioAdapter from given class descriptor.\n\n    :param descriptor: Adapter class descriptor (module.Class)\n    :returns: Created adapter instance.\n    """"""\n    if descriptor is None:\n        return get_default_audio_adapter()\n    module_path = descriptor.split(\'.\')\n    adapter_class_name = module_path[-1]\n    module_path = \'.\'.join(module_path[:-1])\n    adapter_module = import_module(module_path)\n    adapter_class = getattr(adapter_module, adapter_class_name)\n    if not isinstance(adapter_class, AudioAdapter):\n        raise SpleeterError(\n            f\'{adapter_class_name} is not a valid AudioAdapter class\')\n    return adapter_class()\n'"
spleeter/audio/convertor.py,7,"b'#!/usr/bin/env python\n# coding: utf8\n\n"""""" This module provides audio data convertion functions. """"""\n\n# pylint: disable=import-error\nimport numpy as np\nimport tensorflow as tf\n# pylint: enable=import-error\n\nfrom ..utils.tensor import from_float32_to_uint8, from_uint8_to_float32\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n\n\ndef to_n_channels(waveform, n_channels):\n    """""" Convert a waveform to n_channels by removing or\n    duplicating channels if needed (in tensorflow).\n\n    :param waveform: Waveform to transform.\n    :param n_channels: Number of channel to reshape waveform in.\n    :returns: Reshaped waveform.\n    """"""\n    return tf.cond(\n        tf.shape(waveform)[1] >= n_channels,\n        true_fn=lambda: waveform[:, :n_channels],\n        false_fn=lambda: tf.tile(waveform, [1, n_channels])[:, :n_channels]\n    )\n\n\ndef to_stereo(waveform):\n    """""" Convert a waveform to stereo by duplicating if mono,\n    or truncating if too many channels.\n\n    :param waveform: a (N, d) numpy array.\n    :returns: A stereo waveform as a (N, 1) numpy array.\n    """"""\n    if waveform.shape[1] == 1:\n        return np.repeat(waveform, 2, axis=-1)\n    if waveform.shape[1] > 2:\n        return waveform[:, :2]\n    return waveform\n\n\ndef gain_to_db(tensor, espilon=10e-10):\n    """""" Convert from gain to decibel in tensorflow.\n\n    :param tensor: Tensor to convert.\n    :param epsilon: Operation constant.\n    :returns: Converted tensor.\n    """"""\n    return 20. / np.log(10) * tf.math.log(tf.maximum(tensor, espilon))\n\n\ndef db_to_gain(tensor):\n    """""" Convert from decibel to gain in tensorflow.\n\n    :param tensor_db: Tensor to convert.\n    :returns: Converted tensor.\n    """"""\n    return tf.pow(10., (tensor / 20.))\n\n\ndef spectrogram_to_db_uint(spectrogram, db_range=100., **kwargs):\n    """""" Encodes given spectrogram into uint8 using decibel scale.\n\n    :param spectrogram: Spectrogram to be encoded as TF float tensor.\n    :param db_range: Range in decibel for encoding.\n    :returns: Encoded decibel spectrogram as uint8 tensor.\n    """"""\n    db_spectrogram = gain_to_db(spectrogram)\n    max_db_spectrogram = tf.reduce_max(db_spectrogram)\n    db_spectrogram = tf.maximum(db_spectrogram, max_db_spectrogram - db_range)\n    return from_float32_to_uint8(db_spectrogram, **kwargs)\n\n\ndef db_uint_spectrogram_to_gain(db_uint_spectrogram, min_db, max_db):\n    """""" Decode spectrogram from uint8 decibel scale.\n\n    :param db_uint_spectrogram: Decibel pectrogram to decode.\n    :param min_db: Lower bound limit for decoding.\n    :param max_db: Upper bound limit for decoding.\n    :returns: Decoded spectrogram as float2 tensor.\n    """"""\n    db_spectrogram = from_uint8_to_float32(db_uint_spectrogram, min_db, max_db)\n    return db_to_gain(db_spectrogram)\n'"
spleeter/audio/ffmpeg.py,0,"b'#!/usr/bin/env python\n# coding: utf8\n\n""""""\n    This module provides an AudioAdapter implementation based on FFMPEG\n    process. Such implementation is POSIXish and depends on nothing except\n    standard Python libraries. Thus this implementation is the default one\n    used within this library.\n""""""\n\nimport os\n\n# pylint: disable=import-error\nimport ffmpeg\nimport numpy as np\n# pylint: enable=import-error\n\nfrom .adapter import AudioAdapter\nfrom .. import SpleeterError\nfrom ..utils.logging import get_logger\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n\n\ndef _to_ffmpeg_time(n):\n    """""" Format number of seconds to time expected by FFMPEG.\n    :param n: Time in seconds to format.\n    :returns: Formatted time in FFMPEG format.\n    """"""\n    m, s = divmod(n, 60)\n    h, m = divmod(m, 60)\n    return \'%d:%02d:%09.6f\' % (h, m, s)\n\n\ndef _to_ffmpeg_codec(codec):\n    ffmpeg_codecs = {\n        \'m4a\': \'aac\',\n        \'ogg\': \'libvorbis\',\n        \'wma\': \'wmav2\',\n    }\n    return ffmpeg_codecs.get(codec) or codec\n\n\nclass FFMPEGProcessAudioAdapter(AudioAdapter):\n    """""" An AudioAdapter implementation that use FFMPEG binary through\n    subprocess in order to perform I/O operation for audio processing.\n\n    When created, FFMPEG binary path will be checked and expended,\n    raising exception if not found. Such path could be infered using\n    FFMPEG_PATH environment variable.\n    """"""\n\n    def load(\n            self, path, offset=None, duration=None,\n            sample_rate=None, dtype=np.float32):\n        """""" Loads the audio file denoted by the given path\n        and returns it data as a waveform.\n\n        :param path: Path of the audio file to load data from.\n        :param offset: (Optional) Start offset to load from in seconds.\n        :param duration: (Optional) Duration to load in seconds.\n        :param sample_rate: (Optional) Sample rate to load audio with.\n        :param dtype: (Optional) Numpy data type to use, default to float32.\n        :returns: Loaded data a (waveform, sample_rate) tuple.\n        :raise SpleeterError: If any error occurs while loading audio.\n        """"""\n        if not isinstance(path, str):\n            path = path.decode()\n        try:\n            probe = ffmpeg.probe(path)\n        except ffmpeg._run.Error as e:\n            raise SpleeterError(\n                \'An error occurs with ffprobe (see ffprobe output below)\\n\\n{}\'\n                .format(e.stderr.decode()))\n        if \'streams\' not in probe or len(probe[\'streams\']) == 0:\n            raise SpleeterError(\'No stream was found with ffprobe\')\n        metadata = next(\n            stream\n            for stream in probe[\'streams\']\n            if stream[\'codec_type\'] == \'audio\')\n        n_channels = metadata[\'channels\']\n        if sample_rate is None:\n            sample_rate = metadata[\'sample_rate\']\n        output_kwargs = {\'format\': \'f32le\', \'ar\': sample_rate}\n        if duration is not None:\n            output_kwargs[\'t\'] = _to_ffmpeg_time(duration)\n        if offset is not None:\n            output_kwargs[\'ss\'] = _to_ffmpeg_time(offset)\n        process = (\n            ffmpeg\n            .input(path)\n            .output(\'pipe:\', **output_kwargs)\n            .run_async(pipe_stdout=True, pipe_stderr=True))\n        buffer, _ = process.communicate()\n        waveform = np.frombuffer(buffer, dtype=\'<f4\').reshape(-1, n_channels)\n        if not waveform.dtype == np.dtype(dtype):\n            waveform = waveform.astype(dtype)\n        return (waveform, sample_rate)\n\n    def save(\n            self, path, data, sample_rate,\n            codec=None, bitrate=None):\n        """""" Write waveform data to the file denoted by the given path\n        using FFMPEG process.\n\n        :param path: Path of the audio file to save data in.\n        :param data: Waveform data to write.\n        :param sample_rate: Sample rate to write file in.\n        :param codec: (Optional) Writing codec to use.\n        :param bitrate: (Optional) Bitrate of the written audio file.\n        :raise IOError: If any error occurs while using FFMPEG to write data.\n        """"""\n        directory = os.path.dirname(path)\n        if not os.path.exists(directory):\n            raise SpleeterError(f\'output directory does not exists: {directory}\')\n        get_logger().debug(\'Writing file %s\', path)\n        input_kwargs = {\'ar\': sample_rate, \'ac\': data.shape[1]}\n        output_kwargs = {\'ar\': sample_rate, \'strict\': \'-2\'}\n        if bitrate:\n            output_kwargs[\'audio_bitrate\'] = bitrate\n        if codec is not None and codec != \'wav\':\n            output_kwargs[\'codec\'] = _to_ffmpeg_codec(codec)\n        process = (\n            ffmpeg\n            .input(\'pipe:\', format=\'f32le\', **input_kwargs)\n            .output(path, **output_kwargs)\n            .overwrite_output()\n            .run_async(pipe_stdin=True, pipe_stderr=True, quiet=True))\n        try:\n            process.stdin.write(data.astype(\'<f4\').tobytes())\n            process.stdin.close()\n            process.wait()\n        except IOError:\n            raise SpleeterError(f\'FFMPEG error: {process.stderr.read()}\')\n        get_logger().info(\'File %s written succesfully\', path)\n'"
spleeter/audio/spectrogram.py,18,"b'#!/usr/bin/env python\n# coding: utf8\n\n"""""" Spectrogram specific data augmentation """"""\n\n# pylint: disable=import-error\nimport numpy as np\nimport tensorflow as tf\n\nfrom tensorflow.contrib.signal import stft, hann_window\n# pylint: enable=import-error\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n\n\ndef compute_spectrogram_tf(\n        waveform,\n        frame_length=2048, frame_step=512,\n        spec_exponent=1., window_exponent=1.):\n    """""" Compute magnitude / power spectrogram from waveform as\n    a n_samples x n_channels tensor.\n\n    :param waveform:        Input waveform as (times x number of channels)\n                            tensor.\n    :param frame_length:    Length of a STFT frame to use.\n    :param frame_step:      HOP between successive frames.\n    :param spec_exponent:   Exponent of the spectrogram (usually 1 for\n                            magnitude spectrogram, or 2 for power spectrogram).\n    :param window_exponent: Exponent applied to the Hann windowing function\n                            (may be useful for making perfect STFT/iSTFT\n                            reconstruction).\n    :returns:   Computed magnitude / power spectrogram as a\n                (T x F x n_channels) tensor.\n    """"""\n    stft_tensor = tf.transpose(\n        stft(\n            tf.transpose(waveform),\n            frame_length,\n            frame_step,\n            window_fn=lambda f, dtype: hann_window(\n                f,\n                periodic=True,\n                dtype=waveform.dtype) ** window_exponent),\n        perm=[1, 2, 0])\n    return tf.abs(stft_tensor) ** spec_exponent\n\n\ndef time_stretch(\n        spectrogram,\n        factor=1.0,\n        method=tf.image.ResizeMethod.BILINEAR):\n    """""" Time stretch a spectrogram preserving shape in tensorflow. Note that\n    this is an approximation in the frequency domain.\n\n    :param spectrogram: Input spectrogram to be time stretched as tensor.\n    :param factor: (Optional) Time stretch factor, must be >0, default to 1.\n    :param mehtod: (Optional) Interpolation method, default to BILINEAR.\n    :returns: Time stretched spectrogram as tensor with same shape.\n    """"""\n    T = tf.shape(spectrogram)[0]\n    T_ts = tf.cast(tf.cast(T, tf.float32) * factor, tf.int32)[0]\n    F = tf.shape(spectrogram)[1]\n    ts_spec = tf.image.resize_images(\n        spectrogram,\n        [T_ts, F],\n        method=method,\n        align_corners=True)\n    return tf.image.resize_image_with_crop_or_pad(ts_spec, T, F)\n\n\ndef random_time_stretch(spectrogram, factor_min=0.9, factor_max=1.1, **kwargs):\n    """""" Time stretch a spectrogram preserving shape with random ratio in\n    tensorflow. Applies time_stretch to spectrogram with a random ratio drawn\n    uniformly in [factor_min, factor_max].\n\n    :param spectrogram: Input spectrogram to be time stretched as tensor.\n    :param factor_min: (Optional) Min time stretch factor, default to 0.9.\n    :param factor_max: (Optional) Max time stretch factor, default to 1.1.\n    :returns: Randomly time stretched spectrogram as tensor with same shape.\n    """"""\n    factor = tf.random_uniform(\n        shape=(1,),\n        seed=0) * (factor_max - factor_min) + factor_min\n    return time_stretch(spectrogram, factor=factor, **kwargs)\n\n\ndef pitch_shift(\n        spectrogram,\n        semitone_shift=0.0,\n        method=tf.image.ResizeMethod.BILINEAR):\n    """""" Pitch shift a spectrogram preserving shape in tensorflow. Note that\n    this is an approximation in the frequency domain.\n\n    :param spectrogram: Input spectrogram to be pitch shifted as tensor.\n    :param semitone_shift: (Optional) Pitch shift in semitone, default to 0.0.\n    :param mehtod: (Optional) Interpolation method, default to BILINEAR.\n    :returns: Pitch shifted spectrogram (same shape as spectrogram).\n    """"""\n    factor = 2 ** (semitone_shift / 12.)\n    T = tf.shape(spectrogram)[0]\n    F = tf.shape(spectrogram)[1]\n    F_ps = tf.cast(tf.cast(F, tf.float32) * factor, tf.int32)[0]\n    ps_spec = tf.image.resize_images(\n        spectrogram,\n        [T, F_ps],\n        method=method,\n        align_corners=True)\n    paddings = [[0, 0], [0, tf.maximum(0, F - F_ps)], [0, 0]]\n    return tf.pad(ps_spec[:, :F, :], paddings, \'CONSTANT\')\n\n\ndef random_pitch_shift(spectrogram, shift_min=-1., shift_max=1., **kwargs):\n    """""" Pitch shift a spectrogram preserving shape with random ratio in\n    tensorflow. Applies pitch_shift to spectrogram with a random shift\n    amount (expressed in semitones) drawn uniformly in [shift_min, shift_max].\n\n    :param spectrogram: Input spectrogram to be pitch shifted as tensor.\n\n    :param shift_min: (Optional) Min pitch shift in semitone, default to -1.\n    :param shift_max: (Optional) Max pitch shift in semitone, default to 1.\n    :returns: Randomly pitch shifted spectrogram (same shape as spectrogram).\n    """"""\n    semitone_shift = tf.random_uniform(\n        shape=(1,),\n        seed=0) * (shift_max - shift_min) + shift_min\n    return pitch_shift(spectrogram, semitone_shift=semitone_shift, **kwargs)\n'"
spleeter/commands/__init__.py,0,"b'#!/usr/bin/env python\n# coding: utf8\n\n"""""" This modules provides spleeter command as well as CLI parsing methods. """"""\n\nimport json\nimport logging\nfrom argparse import ArgumentParser\nfrom tempfile import gettempdir\nfrom os.path import exists, join\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n\n\n\n# -i opt specification (separate).\nOPT_INPUT = {\n    \'dest\': \'inputs\',\n    \'nargs\': \'+\',\n    \'help\': \'List of input audio filenames\',\n    \'required\': True\n}\n\n# -o opt specification (evaluate and separate).\nOPT_OUTPUT = {\n    \'dest\': \'output_path\',\n    \'default\': join(gettempdir(), \'separated_audio\'),\n    \'help\': \'Path of the output directory to write audio files in\'\n}\n\n# -f opt specification (separate).\nOPT_FORMAT = {\n    \'dest\': \'filename_format\',\n    \'default\': \'{filename}/{instrument}.{codec}\',\n    \'help\': (\n        \'Template string that will be formatted to generated\'\n        \'output filename. Such template should be Python formattable\'\n        \'string, and could use {filename}, {instrument}, and {codec}\'\n        \'variables.\'\n    )\n}\n\n# -p opt specification (train, evaluate and separate).\nOPT_PARAMS = {\n    \'dest\': \'configuration\',\n    \'default\': \'spleeter:2stems\',\n    \'type\': str,\n    \'action\': \'store\',\n    \'help\': \'JSON filename that contains params\'\n}\n\n# -s opt specification (separate).\nOPT_OFFSET = {\n    \'dest\': \'offset\',\n    \'type\': float,\n    \'default\': 0.,\n    \'help\': \'Set the starting offset to separate audio from.\'\n}\n\n# -d opt specification (separate).\nOPT_DURATION = {\n    \'dest\': \'duration\',\n    \'type\': float,\n    \'default\': 600.,\n    \'help\': (\n        \'Set a maximum duration for processing audio \'\n        \'(only separate offset + duration first seconds of \'\n        \'the input file)\')\n}\n\n# -w opt specification (separate)\nOPT_STFT_BACKEND = {\n    \'dest\': \'stft_backend\',\n    \'type\': str,\n    \'choices\' : [""tensorflow"", ""librosa"", ""auto""],\n    \'default\': ""auto"",\n    \'help\': \'Who should be in charge of computing the stfts. Librosa is faster than tensorflow on CPU and uses\'\n            \' less memory. ""auto"" will use tensorflow when GPU acceleration is available and librosa when not.\'\n}\n\n\n# -c opt specification (separate).\nOPT_CODEC = {\n    \'dest\': \'codec\',\n    \'choices\': (\'wav\', \'mp3\', \'ogg\', \'m4a\', \'wma\', \'flac\'),\n    \'default\': \'wav\',\n    \'help\': \'Audio codec to be used for the separated output\'\n}\n\n# -b opt specification (separate).\nOPT_BITRATE = {\n    \'dest\': \'bitrate\',\n    \'default\': \'128k\',\n    \'help\': \'Audio bitrate to be used for the separated output\'\n}\n\n# -m opt specification (evaluate and separate).\nOPT_MWF = {\n    \'dest\': \'MWF\',\n    \'action\': \'store_const\',\n    \'const\': True,\n    \'default\': False,\n    \'help\': \'Whether to use multichannel Wiener filtering for separation\',\n}\n\n# --mus_dir opt specification (evaluate).\nOPT_MUSDB = {\n    \'dest\': \'mus_dir\',\n    \'type\': str,\n    \'required\': True,\n    \'help\': \'Path to folder with musDB\'\n}\n\n# -d opt specification (train).\nOPT_DATA = {\n    \'dest\': \'audio_path\',\n    \'type\': str,\n    \'required\': True,\n    \'help\': \'Path of the folder containing audio data for training\'\n}\n\n# -a opt specification (train, evaluate and separate).\nOPT_ADAPTER = {\n    \'dest\': \'audio_adapter\',\n    \'type\': str,\n    \'help\': \'Name of the audio adapter to use for audio I/O\'\n}\n\n# -a opt specification (train, evaluate and separate).\nOPT_VERBOSE = {\n    \'action\': \'store_true\',\n    \'help\': \'Shows verbose logs\'\n}\n\n\ndef _add_common_options(parser):\n    """""" Add common option to the given parser.\n\n    :param parser: Parser to add common opt to.\n    """"""\n    parser.add_argument(\'-a\', \'--adapter\', **OPT_ADAPTER)\n    parser.add_argument(\'-p\', \'--params_filename\', **OPT_PARAMS)\n    parser.add_argument(\'--verbose\', **OPT_VERBOSE)\n\n\ndef _create_train_parser(parser_factory):\n    """""" Creates an argparser for training command\n\n    :param parser_factory: Factory to use to create parser instance.\n    :returns: Created and configured parser.\n    """"""\n    parser = parser_factory(\'train\', help=\'Train a source separation model\')\n    _add_common_options(parser)\n    parser.add_argument(\'-d\', \'--data\', **OPT_DATA)\n    return parser\n\n\ndef _create_evaluate_parser(parser_factory):\n    """""" Creates an argparser for evaluation command\n\n    :param parser_factory: Factory to use to create parser instance.\n    :returns: Created and configured parser.\n    """"""\n    parser = parser_factory(\n        \'evaluate\',\n        help=\'Evaluate a model on the musDB test dataset\')\n    _add_common_options(parser)\n    parser.add_argument(\'-o\', \'--output_path\', **OPT_OUTPUT)\n    parser.add_argument(\'--mus_dir\', **OPT_MUSDB)\n    parser.add_argument(\'-m\', \'--mwf\', **OPT_MWF)\n    return parser\n\n\ndef _create_separate_parser(parser_factory):\n    """""" Creates an argparser for separation command\n\n    :param parser_factory: Factory to use to create parser instance.\n    :returns: Created and configured parser.\n    """"""\n    parser = parser_factory(\'separate\', help=\'Separate audio files\')\n    _add_common_options(parser)\n    parser.add_argument(\'-i\', \'--inputs\', **OPT_INPUT)\n    parser.add_argument(\'-o\', \'--output_path\', **OPT_OUTPUT)\n    parser.add_argument(\'-f\', \'--filename_format\', **OPT_FORMAT)\n    parser.add_argument(\'-d\', \'--duration\', **OPT_DURATION)\n    parser.add_argument(\'-s\', \'--offset\', **OPT_OFFSET)\n    parser.add_argument(\'-c\', \'--codec\', **OPT_CODEC)\n    parser.add_argument(\'-b\', \'--birate\', **OPT_BITRATE)\n    parser.add_argument(\'-m\', \'--mwf\', **OPT_MWF)\n    parser.add_argument(\'-B\', \'--stft-backend\', **OPT_STFT_BACKEND)\n    return parser\n\n\ndef create_argument_parser():\n    """""" Creates overall command line parser for Spleeter.\n\n    :returns: Created argument parser.\n    """"""\n    parser = ArgumentParser(prog=\'spleeter\')\n    subparsers = parser.add_subparsers()\n    subparsers.dest = \'command\'\n    subparsers.required = True\n    _create_separate_parser(subparsers.add_parser)\n    _create_train_parser(subparsers.add_parser)\n    _create_evaluate_parser(subparsers.add_parser)\n    return parser\n'"
spleeter/commands/evaluate.py,0,"b'#!/usr/bin/env python\n# coding: utf8\n\n""""""\n    Entrypoint provider for performing model evaluation.\n\n    Evaluation is performed against musDB dataset.\n\n    USAGE: python -m spleeter evaluate \\\n        -p /path/to/params \\\n        -o /path/to/output/dir \\\n        [-m] \\\n        --mus_dir /path/to/musdb dataset\n""""""\n\nimport sys\nimport json\n\nfrom argparse import Namespace\nfrom itertools import product\nfrom glob import glob\nfrom os.path import join, exists\n\n# pylint: disable=import-error\nimport numpy as np\nimport pandas as pd\n# pylint: enable=import-error\n\nfrom .separate import entrypoint as separate_entrypoint\nfrom ..utils.logging import get_logger\n\ntry:\n    import musdb\n    import museval\nexcept ImportError:\n    logger = get_logger()\n    logger.error(\'Extra dependencies musdb and museval not found\')\n    logger.error(\'Please install musdb and museval first, abort\')\n    sys.exit(1)\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n\n_SPLIT = \'test\'\n_MIXTURE = \'mixture.wav\'\n_AUDIO_DIRECTORY = \'audio\'\n_METRICS_DIRECTORY = \'metrics\'\n_INSTRUMENTS = (\'vocals\', \'drums\', \'bass\', \'other\')\n_METRICS = (\'SDR\', \'SAR\', \'SIR\', \'ISR\')\n\n\ndef _separate_evaluation_dataset(arguments, musdb_root_directory, params):\n    """""" Performs audio separation on the musdb dataset from\n    the given directory and params.\n\n    :param arguments: Entrypoint arguments.\n    :param musdb_root_directory: Directory to retrieve dataset from.\n    :param params: Spleeter configuration to apply to separation.\n    :returns: Separation output directory path.\n    """"""\n    songs = glob(join(musdb_root_directory, _SPLIT, \'*/\'))\n    mixtures = [join(song, _MIXTURE) for song in songs]\n    audio_output_directory = join(\n        arguments.output_path,\n        _AUDIO_DIRECTORY)\n    separate_entrypoint(\n        Namespace(\n            audio_adapter=arguments.audio_adapter,\n            configuration=arguments.configuration,\n            inputs=mixtures,\n            output_path=join(audio_output_directory, _SPLIT),\n            filename_format=\'{foldername}/{instrument}.{codec}\',\n            codec=\'wav\',\n            duration=600.,\n            offset=0.,\n            bitrate=\'128k\',\n            MWF=arguments.MWF,\n            verbose=arguments.verbose,\n            stft_backend=""auto""),\n        params)\n    return audio_output_directory\n\n\ndef _compute_musdb_metrics(\n        arguments,\n        musdb_root_directory,\n        audio_output_directory):\n    """""" Generates musdb metrics fro previsouly computed audio estimation.\n\n    :param arguments: Entrypoint arguments.\n    :param audio_output_directory: Directory to get audio estimation from.\n    :returns: Path of generated metrics directory.\n    """"""\n    metrics_output_directory = join(\n        arguments.output_path,\n        _METRICS_DIRECTORY)\n    get_logger().info(\'Starting musdb evaluation (this could be long) ...\')\n    dataset = musdb.DB(\n        root=musdb_root_directory,\n        is_wav=True,\n        subsets=[_SPLIT])\n    museval.eval_mus_dir(\n        dataset=dataset,\n        estimates_dir=audio_output_directory,\n        output_dir=metrics_output_directory)\n    get_logger().info(\'musdb evaluation done\')\n    return metrics_output_directory\n\n\ndef _compile_metrics(metrics_output_directory):\n    """""" Compiles metrics from given directory and returns\n    results as dict.\n\n    :param metrics_output_directory: Directory to get metrics from.\n    :returns: Compiled metrics as dict.\n    """"""\n    songs = glob(join(metrics_output_directory, \'test/*.json\'))\n    index = pd.MultiIndex.from_tuples(\n        product(_INSTRUMENTS, _METRICS),\n        names=[\'instrument\', \'metric\'])\n    pd.DataFrame([], index=[\'config1\', \'config2\'], columns=index)\n    metrics = {\n        instrument: {k: [] for k in _METRICS}\n        for instrument in _INSTRUMENTS}\n    for song in songs:\n        with open(song, \'r\') as stream:\n            data = json.load(stream)\n        for target in data[\'targets\']:\n            instrument = target[\'name\']\n            for metric in _METRICS:\n                sdr_med = np.median([\n                    frame[\'metrics\'][metric]\n                    for frame in target[\'frames\']\n                    if not np.isnan(frame[\'metrics\'][metric])])\n                metrics[instrument][metric].append(sdr_med)\n    return metrics\n\n\ndef entrypoint(arguments, params):\n    """""" Command entrypoint.\n\n    :param arguments: Command line parsed argument as argparse.Namespace.\n    :param params: Deserialized JSON configuration file provided in CLI args.\n    """"""\n    # Parse and check musdb directory.\n    musdb_root_directory = arguments.mus_dir\n    if not exists(musdb_root_directory):\n        raise IOError(f\'musdb directory {musdb_root_directory} not found\')\n    # Separate musdb sources.\n    audio_output_directory = _separate_evaluation_dataset(\n        arguments,\n        musdb_root_directory,\n        params)\n    # Compute metrics with musdb.\n    metrics_output_directory = _compute_musdb_metrics(\n        arguments,\n        musdb_root_directory,\n        audio_output_directory)\n    # Compute and pretty print median metrics.\n    metrics = _compile_metrics(metrics_output_directory)\n    for instrument, metric in metrics.items():\n        get_logger().info(\'%s:\', instrument)\n        for metric, value in metric.items():\n            get_logger().info(\'%s: %s\', metric, f\'{np.median(value):.3f}\')\n\n    return metrics\n'"
spleeter/commands/separate.py,0,"b'#!/usr/bin/env python\n# coding: utf8\n\n""""""\n    Entrypoint provider for performing source separation.\n\n    USAGE: python -m spleeter separate \\\n        -p /path/to/params \\\n        -i inputfile1 inputfile2 ... inputfilen\n        -o /path/to/output/dir \\\n        -i /path/to/audio1.wav /path/to/audio2.mp3\n""""""\n\nfrom ..audio.adapter import get_audio_adapter\nfrom ..separator import Separator\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n\n\n\ndef entrypoint(arguments, params):\n    """""" Command entrypoint.\n\n    :param arguments: Command line parsed argument as argparse.Namespace.\n    :param params: Deserialized JSON configuration file provided in CLI args.\n    """"""\n    # TODO: check with output naming.\n    audio_adapter = get_audio_adapter(arguments.audio_adapter)\n    separator = Separator(\n        arguments.configuration,\n        MWF=arguments.MWF,\n        stft_backend=arguments.stft_backend)\n    for filename in arguments.inputs:\n        separator.separate_to_file(\n            filename,\n            arguments.output_path,\n            audio_adapter=audio_adapter,\n            offset=arguments.offset,\n            duration=arguments.duration,\n            codec=arguments.codec,\n            bitrate=arguments.bitrate,\n            filename_format=arguments.filename_format,\n            synchronous=False\n        )\n    separator.join()\n'"
spleeter/commands/train.py,6,"b'#!/usr/bin/env python\n# coding: utf8\n\n""""""\n    Entrypoint provider for performing model training.\n\n    USAGE: python -m spleeter train -p /path/to/params\n""""""\n\nfrom functools import partial\n\n# pylint: disable=import-error\nimport tensorflow as tf\n# pylint: enable=import-error\n\nfrom ..audio.adapter import get_audio_adapter\nfrom ..dataset import get_training_dataset, get_validation_dataset\nfrom ..model import model_fn\nfrom ..model.provider import ModelProvider\nfrom ..utils.logging import get_logger\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n\n\ndef _create_estimator(params):\n    """""" Creates estimator.\n\n    :param params: TF params to build estimator from.\n    :returns: Built estimator.\n    """"""\n    session_config = tf.compat.v1.ConfigProto()\n    session_config.gpu_options.per_process_gpu_memory_fraction = 0.45\n    estimator = tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir=params[\'model_dir\'],\n        params=params,\n        config=tf.estimator.RunConfig(\n            save_checkpoints_steps=params[\'save_checkpoints_steps\'],\n            tf_random_seed=params[\'random_seed\'],\n            save_summary_steps=params[\'save_summary_steps\'],\n            session_config=session_config,\n            log_step_count_steps=10,\n            keep_checkpoint_max=2))\n    return estimator\n\n\ndef _create_train_spec(params, audio_adapter, audio_path):\n    """""" Creates train spec.\n\n    :param params: TF params to build spec from.\n    :returns: Built train spec.\n    """"""\n    input_fn = partial(get_training_dataset, params, audio_adapter, audio_path)\n    train_spec = tf.estimator.TrainSpec(\n        input_fn=input_fn,\n        max_steps=params[\'train_max_steps\'])\n    return train_spec\n\n\ndef _create_evaluation_spec(params, audio_adapter, audio_path):\n    """""" Setup eval spec evaluating ever n seconds\n\n    :param params: TF params to build spec from.\n    :returns: Built evaluation spec.\n    """"""\n    input_fn = partial(\n        get_validation_dataset,\n        params,\n        audio_adapter,\n        audio_path)\n    evaluation_spec = tf.estimator.EvalSpec(\n        input_fn=input_fn,\n        steps=None,\n        throttle_secs=params[\'throttle_secs\'])\n    return evaluation_spec\n\n\ndef entrypoint(arguments, params):\n    """""" Command entrypoint.\n\n    :param arguments: Command line parsed argument as argparse.Namespace.\n    :param params: Deserialized JSON configuration file provided in CLI args.\n    """"""\n    audio_adapter = get_audio_adapter(arguments.audio_adapter)\n    audio_path = arguments.audio_path\n    estimator = _create_estimator(params)\n    train_spec = _create_train_spec(params, audio_adapter, audio_path)\n    evaluation_spec = _create_evaluation_spec(\n        params,\n        audio_adapter,\n        audio_path)\n    get_logger().info(\'Start model training\')\n    tf.estimator.train_and_evaluate(\n        estimator,\n        train_spec,\n        evaluation_spec)\n    ModelProvider.writeProbe(params[\'model_dir\'])\n    get_logger().info(\'Model training done\')\n'"
spleeter/model/__init__.py,47,"b'#!/usr/bin/env python\n# coding: utf8\n\n"""""" This package provide an estimator builder as well as model functions. """"""\n\nimport importlib\n\n# pylint: disable=import-error\nimport tensorflow as tf\n\nfrom tensorflow.contrib.signal import stft, inverse_stft, hann_window\n# pylint: enable=import-error\n\nfrom ..utils.tensor import pad_and_partition, pad_and_reshape\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n\n\nplaceholder = tf.compat.v1.placeholder\n\n\ndef get_model_function(model_type):\n    """"""\n        Get tensorflow function of the model to be applied to the input tensor.\n        For instance ""unet.softmax_unet"" will return the softmax_unet function\n        in the ""unet.py"" submodule of the current module (spleeter.model).\n\n        Params:\n        - model_type: str\n        the relative module path to the model function.\n\n        Returns:\n        A tensorflow function to be applied to the input tensor to get the\n        multitrack output.\n    """"""\n    relative_path_to_module = \'.\'.join(model_type.split(\'.\')[:-1])\n    model_name = model_type.split(\'.\')[-1]\n    main_module = \'.\'.join((__name__, \'functions\'))\n    path_to_module = f\'{main_module}.{relative_path_to_module}\'\n    module = importlib.import_module(path_to_module)\n    model_function = getattr(module, model_name)\n    return model_function\n\n\nclass InputProvider(object):\n\n    def __init__(self, params):\n        self.params = params\n\n    def get_input_dict_placeholders(self):\n        raise NotImplementedError()\n\n    @property\n    def input_names(self):\n        raise NotImplementedError()\n\n    def get_feed_dict(self, features, *args):\n        raise NotImplementedError()\n\n\nclass WaveformInputProvider(InputProvider):\n\n    @property\n    def input_names(self):\n        return [""audio_id"", ""waveform""]\n\n    def get_input_dict_placeholders(self):\n        shape = (None, self.params[\'n_channels\'])\n        features = {\n            \'waveform\': placeholder(tf.float32, shape=shape, name=""waveform""),\n            \'audio_id\': placeholder(tf.string, name=""audio_id"")}\n        return features\n\n    def get_feed_dict(self, features, waveform, audio_id):\n        return {features[""audio_id""]: audio_id, features[""waveform""]: waveform}\n\n\nclass SpectralInputProvider(InputProvider):\n\n    def __init__(self, params):\n        super().__init__(params)\n        self.stft_input_name = ""{}_stft"".format(self.params[""mix_name""])\n\n    @property\n    def input_names(self):\n        return [""audio_id"", self.stft_input_name]\n\n    def get_input_dict_placeholders(self):\n        features = {\n            self.stft_input_name: placeholder(tf.complex64,\n                                              shape=(None, self.params[""frame_length""]//2+1,\n                                                     self.params[\'n_channels\']),\n                                              name=self.stft_input_name),\n            \'audio_id\': placeholder(tf.string, name=""audio_id"")}\n        return features\n\n    def get_feed_dict(self, features, stft, audio_id):\n        return {features[""audio_id""]: audio_id, features[self.stft_input_name]: stft}\n\n\nclass InputProviderFactory(object):\n\n    @staticmethod\n    def get(params):\n        stft_backend = params[""stft_backend""]\n        assert stft_backend in (""tensorflow"", ""librosa""), ""Unexpected backend {}"".format(stft_backend)\n        if stft_backend == ""tensorflow"":\n            return WaveformInputProvider(params)\n        else:\n            return SpectralInputProvider(params)\n\n\nclass EstimatorSpecBuilder(object):\n    """""" A builder class that allows to builds a multitrack unet model\n    estimator. The built model estimator has a different behaviour when\n    used in a train/eval mode and in predict mode.\n\n    * In train/eval mode:   it takes as input and outputs magnitude spectrogram\n    * In predict mode:      it takes as input and outputs waveform. The whole\n                            separation process is then done in this function\n                            for performance reason: it makes it possible to run\n                            the whole spearation process (including STFT and\n                            inverse STFT) on GPU.\n\n    :Example:\n\n    >>> from spleeter.model import EstimatorSpecBuilder\n    >>> builder = EstimatorSpecBuilder()\n    >>> builder.build_predict_model()\n    >>> builder.build_evaluation_model()\n    >>> builder.build_train_model()\n\n    >>> from spleeter.model import model_fn\n    >>> estimator = tf.estimator.Estimator(model_fn=model_fn, ...)\n    """"""\n\n    # Supported model functions.\n    DEFAULT_MODEL = \'unet.unet\'\n\n    # Supported loss functions.\n    L1_MASK = \'L1_mask\'\n    WEIGHTED_L1_MASK = \'weighted_L1_mask\'\n\n    # Supported optimizers.\n    ADADELTA = \'Adadelta\'\n    SGD = \'SGD\'\n\n    # Math constants.\n    WINDOW_COMPENSATION_FACTOR = 2./3.\n    EPSILON = 1e-10\n\n    def __init__(self, features, params):\n        """""" Default constructor. Depending on built model\n        usage, the provided features should be different:\n\n        * In train/eval mode:   features is a dictionary with a\n                                ""mix_spectrogram"" key, associated to the\n                                mix magnitude spectrogram.\n        * In predict mode:      features is a dictionary with a ""waveform""\n                                key, associated to the waveform of the sound\n                                to be separated.\n\n        :param features: The input features for the estimator.\n        :param params: Some hyperparameters as a dictionary.\n        """"""\n\n        self._features = features\n        self._params = params\n        # Get instrument name.\n        self._mix_name = params[\'mix_name\']\n        self._instruments = params[\'instrument_list\']\n        # Get STFT/signals parameters\n        self._n_channels = params[\'n_channels\']\n        self._T = params[\'T\']\n        self._F = params[\'F\']\n        self._frame_length = params[\'frame_length\']\n        self._frame_step = params[\'frame_step\']\n\n    def include_stft_computations(self):\n        return self._params[""stft_backend""] == ""tensorflow""\n\n    def _build_model_outputs(self):\n        """""" Created a batch_sizexTxFxn_channels input tensor containing\n        mix magnitude spectrogram, then an output dict from it according\n        to the selected model in internal parameters.\n\n        :returns: Build output dict.\n        :raise ValueError: If required model_type is not supported.\n        """"""\n\n        input_tensor = self.spectrogram_feature\n        model = self._params.get(\'model\', None)\n        if model is not None:\n            model_type = model.get(\'type\', self.DEFAULT_MODEL)\n        else:\n            model_type = self.DEFAULT_MODEL\n        try:\n            apply_model = get_model_function(model_type)\n        except ModuleNotFoundError:\n            raise ValueError(f\'No model function {model_type} found\')\n        self._model_outputs = apply_model(\n            input_tensor,\n            self._instruments,\n            self._params[\'model\'][\'params\'])\n\n    def _build_loss(self, labels):\n        """""" Construct tensorflow loss and metrics\n\n        :param output_dict: dictionary of network outputs (key: instrument\n            name, value: estimated spectrogram of the instrument)\n        :param labels: dictionary of target outputs (key: instrument\n            name, value: ground truth spectrogram of the instrument)\n        :returns: tensorflow (loss, metrics) tuple.\n        """"""\n        output_dict = self.model_outputs\n        loss_type = self._params.get(\'loss_type\', self.L1_MASK)\n        if loss_type == self.L1_MASK:\n            losses = {\n                name: tf.reduce_mean(tf.abs(output - labels[name]))\n                for name, output in output_dict.items()\n            }\n        elif loss_type == self.WEIGHTED_L1_MASK:\n            losses = {\n                name: tf.reduce_mean(\n                    tf.reduce_mean(\n                        labels[name],\n                        axis=[1, 2, 3],\n                        keep_dims=True) *\n                    tf.abs(output - labels[name]))\n                for name, output in output_dict.items()\n            }\n        else:\n            raise ValueError(f""Unkwnown loss type: {loss_type}"")\n        loss = tf.reduce_sum(list(losses.values()))\n        # Add metrics for monitoring each instrument.\n        metrics = {k: tf.compat.v1.metrics.mean(v) for k, v in losses.items()}\n        metrics[\'absolute_difference\'] = tf.compat.v1.metrics.mean(loss)\n        return loss, metrics\n\n    def _build_optimizer(self):\n        """""" Builds an optimizer instance from internal parameter values.\n\n        Default to AdamOptimizer if not specified.\n\n        :returns: Optimizer instance from internal configuration.\n        """"""\n        name = self._params.get(\'optimizer\')\n        if name == self.ADADELTA:\n            return tf.compat.v1.train.AdadeltaOptimizer()\n        rate = self._params[\'learning_rate\']\n        if name == self.SGD:\n            return tf.compat.v1.train.GradientDescentOptimizer(rate)\n        return tf.compat.v1.train.AdamOptimizer(rate)\n\n    @property\n    def instruments(self):\n        return self._instruments\n\n    @property\n    def stft_name(self):\n        return f\'{self._mix_name}_stft\'\n\n    @property\n    def spectrogram_name(self):\n        return f\'{self._mix_name}_spectrogram\'\n\n    def _build_stft_feature(self):\n        """""" Compute STFT of waveform and slice the STFT in segment\n         with the right length to feed the network.\n        """"""\n\n        stft_name = self.stft_name\n        spec_name = self.spectrogram_name\n\n        if stft_name not in self._features:\n            stft_feature = tf.transpose(\n                stft(\n                    tf.transpose(self._features[\'waveform\']),\n                    self._frame_length,\n                    self._frame_step,\n                    window_fn=lambda frame_length, dtype: (\n                        hann_window(frame_length, periodic=True, dtype=dtype)),\n                    pad_end=True),\n                perm=[1, 2, 0])\n            self._features[f\'{self._mix_name}_stft\'] = stft_feature\n        if spec_name not in self._features:\n            self._features[spec_name] = tf.abs(\n                pad_and_partition(self._features[stft_name], self._T))[:, :, :self._F, :]\n\n    @property\n    def model_outputs(self):\n        if not hasattr(self, ""_model_outputs""):\n            self._build_model_outputs()\n        return self._model_outputs\n\n    @property\n    def outputs(self):\n        if not hasattr(self, ""_outputs""):\n            self._build_outputs()\n        return self._outputs\n\n    @property\n    def stft_feature(self):\n        if self.stft_name not in self._features:\n            self._build_stft_feature()\n        return self._features[self.stft_name]\n\n    @property\n    def spectrogram_feature(self):\n        if self.spectrogram_name not in self._features:\n            self._build_stft_feature()\n        return self._features[self.spectrogram_name]\n\n    @property\n    def masks(self):\n        if not hasattr(self, ""_masks""):\n            self._build_masks()\n        return self._masks\n\n    @property\n    def masked_stfts(self):\n        if not hasattr(self, ""_masked_stfts""):\n            self._build_masked_stfts()\n        return self._masked_stfts\n\n    def _inverse_stft(self, stft_t, time_crop=None):\n        """""" Inverse and reshape the given STFT\n\n        :param stft_t: input STFT\n        :returns: inverse STFT (waveform)\n        """"""\n        inversed = inverse_stft(\n            tf.transpose(stft_t, perm=[2, 0, 1]),\n            self._frame_length,\n            self._frame_step,\n            window_fn=lambda frame_length, dtype: (\n                hann_window(frame_length, periodic=True, dtype=dtype))\n        ) * self.WINDOW_COMPENSATION_FACTOR\n        reshaped = tf.transpose(inversed)\n        if time_crop is None:\n            time_crop = tf.shape(self._features[\'waveform\'])[0]\n        return reshaped[:time_crop, :]\n\n    def _build_mwf_output_waveform(self):\n        """""" Perform separation with multichannel Wiener Filtering using Norbert.\n        Note: multichannel Wiener Filtering is not coded in Tensorflow and thus\n        may be quite slow.\n\n        :returns: dictionary of separated waveforms (key: instrument name,\n            value: estimated waveform of the instrument)\n        """"""\n        import norbert  # pylint: disable=import-error\n        output_dict = self.model_outputs\n        x = self.stft_feature\n        v = tf.stack(\n            [\n                pad_and_reshape(\n                    output_dict[f\'{instrument}_spectrogram\'],\n                    self._frame_length,\n                    self._F)[:tf.shape(x)[0], ...]\n                for instrument in self._instruments\n            ],\n            axis=3)\n        input_args = [v, x]\n        stft_function = tf.py_function(\n            lambda v, x: norbert.wiener(v.numpy(), x.numpy()),\n            input_args,\n            tf.complex64),\n        return {\n            instrument: self._inverse_stft(stft_function[0][:, :, :, k])\n            for k, instrument in enumerate(self._instruments)\n        }\n\n    def _extend_mask(self, mask):\n        """""" Extend mask, from reduced number of frequency bin to the number of\n        frequency bin in the STFT.\n\n        :param mask: restricted mask\n        :returns: extended mask\n        :raise ValueError: If invalid mask_extension parameter is set.\n        """"""\n        extension = self._params[\'mask_extension\']\n        # Extend with average\n        # (dispatch according to energy in the processed band)\n        if extension == ""average"":\n            extension_row = tf.reduce_mean(mask, axis=2, keepdims=True)\n        # Extend with 0\n        # (avoid extension artifacts but not conservative separation)\n        elif extension == ""zeros"":\n            mask_shape = tf.shape(mask)\n            extension_row = tf.zeros((\n                mask_shape[0],\n                mask_shape[1],\n                1,\n                mask_shape[-1]))\n        else:\n            raise ValueError(f\'Invalid mask_extension parameter {extension}\')\n        n_extra_row = self._frame_length // 2 + 1 - self._F\n        extension = tf.tile(extension_row, [1, 1, n_extra_row, 1])\n        return tf.concat([mask, extension], axis=2)\n\n    def _build_masks(self):\n        """"""\n        Compute masks from the output spectrograms of the model.\n        :return:\n        """"""\n        output_dict = self.model_outputs\n        stft_feature = self.stft_feature\n        separation_exponent = self._params[\'separation_exponent\']\n        output_sum = tf.reduce_sum(\n            [e ** separation_exponent for e in output_dict.values()],\n            axis=0\n        ) + self.EPSILON\n        out = {}\n        for instrument in self._instruments:\n            output = output_dict[f\'{instrument}_spectrogram\']\n            # Compute mask with the model.\n            instrument_mask = (output ** separation_exponent\n                               + (self.EPSILON / len(output_dict))) / output_sum\n            # Extend mask;\n            instrument_mask = self._extend_mask(instrument_mask)\n            # Stack back mask.\n            old_shape = tf.shape(instrument_mask)\n            new_shape = tf.concat(\n                [[old_shape[0] * old_shape[1]], old_shape[2:]],\n                axis=0)\n            instrument_mask = tf.reshape(instrument_mask, new_shape)\n            # Remove padded part (for mask having the same size as STFT);\n\n            instrument_mask = instrument_mask[\n                              :tf.shape(stft_feature)[0], ...]\n            out[instrument] = instrument_mask\n        self._masks = out\n\n    def _build_masked_stfts(self):\n        input_stft = self.stft_feature\n        out = {}\n        for instrument, mask in self.masks.items():\n            out[instrument] = tf.cast(mask, dtype=tf.complex64) * input_stft\n        self._masked_stfts = out\n\n    def _build_manual_output_waveform(self, masked_stft):\n        """""" Perform ratio mask separation\n\n        :param output_dict: dictionary of estimated spectrogram (key: instrument\n            name, value: estimated spectrogram of the instrument)\n        :returns: dictionary of separated waveforms (key: instrument name,\n            value: estimated waveform of the instrument)\n        """"""\n\n        output_waveform = {}\n        for instrument, stft_data in masked_stft.items():\n            output_waveform[instrument] = self._inverse_stft(stft_data)\n        return output_waveform\n\n    def _build_output_waveform(self, masked_stft):\n        """""" Build output waveform from given output dict in order to be used in\n        prediction context. Regarding of the configuration building method will\n        be using MWF.\n\n        :returns: Built output waveform.\n        """"""\n\n        if self._params.get(\'MWF\', False):\n            output_waveform = self._build_mwf_output_waveform()\n        else:\n            output_waveform = self._build_manual_output_waveform(masked_stft)\n        return output_waveform\n\n    def _build_outputs(self):\n        if self.include_stft_computations():\n            self._outputs = self._build_output_waveform(self.masked_stfts)\n        else:\n            self._outputs = self.masked_stfts\n\n        if \'audio_id\' in self._features:\n            self._outputs[\'audio_id\'] = self._features[\'audio_id\']\n\n    def build_predict_model(self):\n        """""" Builder interface for creating model instance that aims to perform\n        prediction / inference over given track. The output of such estimator\n        will be a dictionary with a ""<instrument>"" key per separated instrument\n        , associated to the estimated separated waveform of the instrument.\n\n        :returns: An estimator for performing prediction.\n        """"""\n\n        return tf.estimator.EstimatorSpec(\n            tf.estimator.ModeKeys.PREDICT,\n            predictions=self.outputs)\n\n    def build_evaluation_model(self, labels):\n        """""" Builder interface for creating model instance that aims to perform\n        model evaluation. The output of such estimator will be a dictionary\n        with a key ""<instrument>_spectrogram"" per separated instrument,\n        associated to the estimated separated instrument magnitude spectrogram.\n\n        :param labels: Model labels.\n        :returns: An estimator for performing model evaluation.\n        """"""\n        loss, metrics = self._build_loss(labels)\n        return tf.estimator.EstimatorSpec(\n            tf.estimator.ModeKeys.EVAL,\n            loss=loss,\n            eval_metric_ops=metrics)\n\n    def build_train_model(self, labels):\n        """""" Builder interface for creating model instance that aims to perform\n        model training. The output of such estimator will be a dictionary\n        with a key ""<instrument>_spectrogram"" per separated instrument,\n        associated to the estimated separated instrument magnitude spectrogram.\n\n        :param labels: Model labels.\n        :returns: An estimator for performing model training.\n        """"""\n        loss, metrics = self._build_loss(labels)\n        optimizer = self._build_optimizer()\n        train_operation = optimizer.minimize(\n                loss=loss,\n                global_step=tf.compat.v1.train.get_global_step())\n        return tf.estimator.EstimatorSpec(\n            mode=tf.estimator.ModeKeys.TRAIN,\n            loss=loss,\n            train_op=train_operation,\n            eval_metric_ops=metrics,\n        )\n\n\ndef model_fn(features, labels, mode, params, config):\n    """"""\n\n    :param features:\n    :param labels: \n    :param mode: Estimator mode.\n    :param params: \n    :param config: TF configuration (not used).\n    :returns: Built EstimatorSpec.\n    :raise ValueError: If estimator mode is not supported.\n    """"""\n    builder = EstimatorSpecBuilder(features, params)\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return builder.build_predict_model()\n    elif mode == tf.estimator.ModeKeys.EVAL:\n        return builder.build_evaluation_model(labels)\n    elif mode == tf.estimator.ModeKeys.TRAIN:\n        return builder.build_train_model(labels)\n    raise ValueError(f\'Unknown mode {mode}\')\n'"
spleeter/resources/__init__.py,0,"b'#!/usr/bin/env python\n# coding: utf8\n\n"""""" Packages that provides static resources file for the library. """"""\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n'"
spleeter/utils/__init__.py,0,"b'#!/usr/bin/env python\n# coding: utf8\n\n"""""" This package provides utility function and classes. """"""\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n'"
spleeter/utils/configuration.py,0,"b'#!/usr/bin/env python\n# coding: utf8\n\n"""""" Module that provides configuration loading function. """"""\n\nimport json\n\ntry:\n    import importlib.resources as loader\nexcept ImportError:\n    # Try backported to PY<37 `importlib_resources`.\n    import importlib_resources as loader\n\nfrom os.path import exists\n\nfrom .. import resources, SpleeterError\n\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n\n_EMBEDDED_CONFIGURATION_PREFIX = \'spleeter:\'\n\n\ndef load_configuration(descriptor):\n    """""" Load configuration from the given descriptor. Could be\n    either a `spleeter:` prefixed embedded configuration name\n    or a file system path to read configuration from.\n\n    :param descriptor: Configuration descriptor to use for lookup.\n    :returns: Loaded description as dict.\n    :raise ValueError: If required embedded configuration does not exists.\n    :raise SpleeterError: If required configuration file does not exists.\n    """"""\n    # Embedded configuration reading.\n    if descriptor.startswith(_EMBEDDED_CONFIGURATION_PREFIX):\n        name = descriptor[len(_EMBEDDED_CONFIGURATION_PREFIX):]\n        if not loader.is_resource(resources, f\'{name}.json\'):\n            raise SpleeterError(f\'No embedded configuration {name} found\')\n        with loader.open_text(resources, f\'{name}.json\') as stream:\n            return json.load(stream)\n    # Standard file reading.\n    if not exists(descriptor):\n        raise SpleeterError(f\'Configuration file {descriptor} not found\')\n    with open(descriptor, \'r\') as stream:\n        return json.load(stream)\n'"
spleeter/utils/estimator.py,5,"b'#!/usr/bin/env python\n# coding: utf8\n\n"""""" Utility functions for creating estimator. """"""\n\nfrom pathlib import Path\nfrom os.path import join\nfrom tempfile import gettempdir\n\n# pylint: disable=import-error\nimport tensorflow as tf\n\nfrom tensorflow.contrib import predictor\n# pylint: enable=import-error\n\nfrom ..model import model_fn, InputProviderFactory\nfrom ..model.provider import get_default_model_provider\n\n# Default exporting directory for predictor.\nDEFAULT_EXPORT_DIRECTORY = join(gettempdir(), \'serving\')\n\n\n\ndef get_default_model_dir(model_dir):\n    """"""\n    Transforms a string like \'spleeter:2stems\' into an actual path.\n    :param model_dir:\n    :return:\n    """"""\n    model_provider = get_default_model_provider()\n    return model_provider.get(model_dir)\n\ndef create_estimator(params, MWF):\n    """"""\n        Initialize tensorflow estimator that will perform separation\n\n        Params:\n        - params: a dictionary of parameters for building the model\n\n        Returns:\n            a tensorflow estimator\n    """"""\n    # Load model.\n\n\n    params[\'model_dir\'] = get_default_model_dir(params[\'model_dir\'])\n    params[\'MWF\'] = MWF\n    # Setup config\n    session_config = tf.compat.v1.ConfigProto()\n    session_config.gpu_options.per_process_gpu_memory_fraction = 0.7\n    config = tf.estimator.RunConfig(session_config=session_config)\n    # Setup estimator\n    estimator = tf.estimator.Estimator(\n        model_fn=model_fn,\n        model_dir=params[\'model_dir\'],\n        params=params,\n        config=config\n    )\n    return estimator\n\n\ndef to_predictor(estimator, directory=DEFAULT_EXPORT_DIRECTORY):\n    """""" Exports given estimator as predictor into the given directory\n    and returns associated tf.predictor instance.\n\n    :param estimator: Estimator to export.\n    :param directory: (Optional) path to write exported model into.\n    """"""\n\n    input_provider = InputProviderFactory.get(estimator.params)\n    def receiver():\n        features = input_provider.get_input_dict_placeholders()\n        return tf.estimator.export.ServingInputReceiver(features, features)\n\n    estimator.export_saved_model(directory, receiver)\n    versions = [\n        model for model in Path(directory).iterdir()\n        if model.is_dir() and \'temp\' not in str(model)]\n    latest = str(sorted(versions)[-1])\n    return predictor.from_saved_model(latest)\n'"
spleeter/utils/logging.py,0,"b'#!/usr/bin/env python\n# coding: utf8\n\n"""""" Centralized logging facilities for Spleeter. """"""\n\nimport logging\n\nfrom os import environ\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n\n_FORMAT = \'%(levelname)s:%(name)s:%(message)s\'\n\n\nclass _LoggerHolder(object):\n    """""" Logger singleton instance holder. """"""\n\n    INSTANCE = None\n\n\ndef get_tensorflow_logger():\n    """"""\n    """"""\n    # pylint: disable=import-error\n    from tensorflow.compat.v1 import logging\n    # pylint: enable=import-error\n    return logging\n\n\ndef get_logger():\n    """""" Returns library scoped logger.\n\n    :returns: Library logger.\n    """"""\n    if _LoggerHolder.INSTANCE is None:\n        formatter = logging.Formatter(_FORMAT)\n        handler = logging.StreamHandler()\n        handler.setFormatter(formatter)\n        logger = logging.getLogger(\'spleeter\')\n        logger.addHandler(handler)\n        logger.setLevel(logging.INFO)\n        _LoggerHolder.INSTANCE = logger\n    return _LoggerHolder.INSTANCE\n\n\ndef enable_tensorflow_logging():\n    """""" Enable tensorflow logging. """"""\n    environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'1\'\n    tf_logger = get_tensorflow_logger()\n    tf_logger.set_verbosity(tf_logger.INFO)\n    logger = get_logger()\n    logger.setLevel(logging.DEBUG)\n\n\ndef enable_logging():\n    """""" Configure default logging. """"""\n    environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n    tf_logger = get_tensorflow_logger()\n    tf_logger.set_verbosity(tf_logger.ERROR)\n'"
spleeter/utils/tensor.py,27,"b'#!/usr/bin/env python\n# coding: utf8\n\n"""""" Utility function for tensorflow. """"""\n\n# pylint: disable=import-error\nimport tensorflow as tf\nimport pandas as pd\n# pylint: enable=import-error\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n\n\ndef sync_apply(tensor_dict, func, concat_axis=1):\n    """""" Return a function that applies synchronously the provided func on the\n    provided dictionnary of tensor. This means that func is applied to the\n    concatenation of the tensors in tensor_dict. This is useful for performing\n    random operation that needs the same drawn value on multiple tensor, such\n    as a random time-crop on both input data and label (the same crop should be\n    applied to both input data and label, so random crop cannot be applied\n    separately on each of them).\n\n    IMPORTANT NOTE: all tensor are assumed to be the same shape.\n\n    Params:\n        - tensor_dict: dictionary (key: strings, values: tf.tensor)\n        a dictionary of tensor.\n        - func: function\n        function to be applied to the concatenation of the tensors in\n        tensor_dict\n        - concat_axis: int\n        The axis on which to perform the concatenation.\n\n        Returns:\n        processed tensors dictionary with the same name (keys) as input\n        tensor_dict.\n    """"""\n    if concat_axis not in {0, 1}:\n        raise NotImplementedError(\n            \'Function only implemented for concat_axis equal to 0 or 1\')\n    tensor_list = list(tensor_dict.values())\n    concat_tensor = tf.concat(tensor_list, concat_axis)\n    processed_concat_tensor = func(concat_tensor)\n    tensor_shape = tf.shape(list(tensor_dict.values())[0])\n    D = tensor_shape[concat_axis]\n    if concat_axis == 0:\n        return {\n            name: processed_concat_tensor[index * D:(index + 1) * D, :, :]\n            for index, name in enumerate(tensor_dict)\n        }\n    return {\n        name: processed_concat_tensor[:, index * D:(index + 1) * D, :]\n        for index, name in enumerate(tensor_dict)\n    }\n\n\ndef from_float32_to_uint8(\n        tensor,\n        tensor_key=\'tensor\',\n        min_key=\'min\',\n        max_key=\'max\'):\n    """"""\n\n    :param tensor:\n    :param tensor_key:\n    :param min_key:\n    :param max_key:\n    :returns:\n    """"""\n    tensor_min = tf.reduce_min(tensor)\n    tensor_max = tf.reduce_max(tensor)\n    return {\n        tensor_key: tf.cast(\n            (tensor - tensor_min) / (tensor_max - tensor_min + 1e-16)\n            * 255.9999, dtype=tf.uint8),\n        min_key: tensor_min,\n        max_key: tensor_max\n    }\n\n\ndef from_uint8_to_float32(tensor, tensor_min, tensor_max):\n    """"""\n\n    :param tensor:\n    :param tensor_min:\n    :param tensor_max:\n    :returns:\n    """"""\n    return (\n        tf.cast(tensor, tf.float32)\n        * (tensor_max - tensor_min)\n        / 255.9999 + tensor_min)\n\n\ndef pad_and_partition(tensor, segment_len):\n    """""" Pad and partition a tensor into segment of len segment_len\n    along the first dimension. The tensor is padded with 0 in order\n    to ensure that the first dimension is a multiple of segment_len.\n\n    Tensor must be of known fixed rank\n\n    :Example:\n\n    >>> tensor = [[1, 2, 3], [4, 5, 6]]\n    >>> segment_len = 2\n    >>> pad_and_partition(tensor, segment_len)\n    [[[1, 2], [4, 5]], [[3, 0], [6, 0]]]\n\n    :param tensor:\n    :param segment_len:\n    :returns:\n    """"""\n    tensor_size = tf.math.floormod(tf.shape(tensor)[0], segment_len)\n    pad_size = tf.math.floormod(segment_len - tensor_size, segment_len)\n    padded = tf.pad(\n        tensor,\n        [[0, pad_size]] + [[0, 0]] * (len(tensor.shape)-1))\n    split = (tf.shape(padded)[0] + segment_len - 1) // segment_len\n    return tf.reshape(\n        padded,\n        tf.concat(\n            [[split, segment_len], tf.shape(padded)[1:]],\n            axis=0))\n\n\ndef pad_and_reshape(instr_spec, frame_length, F):\n    """"""\n    :param instr_spec:\n    :param frame_length:\n    :param F:\n    :returns:\n    """"""\n    spec_shape = tf.shape(instr_spec)\n    extension_row = tf.zeros((spec_shape[0], spec_shape[1], 1, spec_shape[-1]))\n    n_extra_row = (frame_length) // 2 + 1 - F\n    extension = tf.tile(extension_row, [1, 1, n_extra_row, 1])\n    extended_spec = tf.concat([instr_spec, extension], axis=2)\n    old_shape = tf.shape(extended_spec)\n    new_shape = tf.concat([\n        [old_shape[0] * old_shape[1]],\n        old_shape[2:]],\n        axis=0)\n    processed_instr_spec = tf.reshape(extended_spec, new_shape)\n    return processed_instr_spec\n\n\ndef dataset_from_csv(csv_path, **kwargs):\n    """""" Load dataset from a CSV file using Pandas. kwargs if any are\n    forwarded to the `pandas.read_csv` function.\n\n    :param csv_path: Path of the CSV file to load dataset from.\n    :returns: Loaded dataset.\n    """"""\n    df = pd.read_csv(csv_path, **kwargs)\n    dataset = (\n        tf.data.Dataset.from_tensor_slices(\n            {key: df[key].values for key in df})\n    )\n    return dataset\n\n\ndef check_tensor_shape(tensor_tf, target_shape):\n    """""" Return a Tensorflow boolean graph that indicates whether\n    sample[features_key] has the specified target shape. Only check\n    not None entries of target_shape.\n\n    :param tensor_tf: Tensor to check shape for.\n    :param target_shape: Target shape to compare tensor to.\n    :returns: True if shape is valid, False otherwise (as TF boolean).\n    """"""\n    result = tf.constant(True)\n    for i, target_length in enumerate(target_shape):\n        if target_length:\n            result = tf.logical_and(\n                result,\n                tf.equal(tf.constant(target_length), tf.shape(tensor_tf)[i]))\n    return result\n\n\ndef set_tensor_shape(tensor, tensor_shape):\n    """""" Set shape for a tensor (not in place, as opposed to tf.set_shape)\n\n    :param tensor: Tensor to reshape.\n    :param tensor_shape: Shape to apply to the tensor.\n    :returns: A reshaped tensor.\n    """"""\n    # NOTE: That SOUND LIKE IN PLACE HERE ?\n    tensor.set_shape(tensor_shape)\n    return tensor\n'"
spleeter/model/functions/__init__.py,0,"b'#!/usr/bin/env python\n# coding: utf8\n\n"""""" This package provide model functions. """"""\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n\n\ndef apply(function, input_tensor, instruments, params={}):\n    """""" Apply given function to the input tensor.\n\n    :param function: Function to be applied to tensor.\n    :param input_tensor: Tensor to apply blstm to.\n    :param instruments: Iterable that provides a collection of instruments.\n    :param params: (Optional) dict of BLSTM parameters.\n    :returns: Created output tensor dict.\n    """"""\n    output_dict = {}\n    for instrument in instruments:\n        out_name = f\'{instrument}_spectrogram\'\n        output_dict[out_name] = function(\n            input_tensor,\n            output_name=out_name,\n            params=params)\n    return output_dict\n'"
spleeter/model/functions/blstm.py,0,"b'#!/usr/bin/env python\n# coding: utf8\n\n""""""\n    This system (UHL1) uses a bi-directional LSTM network as described in :\n\n    `S. Uhlich, M. Porcu, F. Giron, M. Enenkl, T. Kemp, N. Takahashi and\n    Y. Mitsufuji.\n\n    ""Improving music source separation based on deep neural networks through\n    data augmentation and network blending"", Proc. ICASSP, 2017.`\n\n    It has three BLSTM layers, each having 500 cells.  For each instrument,\n    a network is trained which predicts the target instrument amplitude from\n    the mixture amplitude in the STFT domain (frame size: 4096, hop size:\n    1024). The raw output of each network is then combined by a multichannel\n    Wiener filter. The network is trained on musdb where we split train into\n    train_train and train_valid with 86 and 14 songs, respectively. The\n    validation set is used to perform early stopping and hyperparameter\n    selection (LSTM layer dropout rate, regularization strength).\n""""""\n\n# pylint: disable=import-error\nfrom tensorflow.compat.v1.keras.initializers import he_uniform\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM\nfrom tensorflow.keras.layers import (\n    Bidirectional,\n    Dense,\n    Flatten,\n    Reshape,\n    TimeDistributed)\n# pylint: enable=import-error\n\nfrom . import apply\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n\n\ndef apply_blstm(input_tensor, output_name=\'output\', params={}):\n    """""" Apply BLSTM to the given input_tensor.\n\n    :param input_tensor: Input of the model.\n    :param output_name: (Optional) name of the output, default to \'output\'.\n    :param params: (Optional) dict of BLSTM parameters.\n    :returns: Output tensor.\n    """"""\n    units = params.get(\'lstm_units\', 250)\n    kernel_initializer = he_uniform(seed=50)\n    flatten_input = TimeDistributed(Flatten())((input_tensor))\n\n    def create_bidirectional():\n        return Bidirectional(\n            CuDNNLSTM(\n                units,\n                kernel_initializer=kernel_initializer,\n                return_sequences=True))\n\n    l1 = create_bidirectional()((flatten_input))\n    l2 = create_bidirectional()((l1))\n    l3 = create_bidirectional()((l2))\n    dense = TimeDistributed(\n        Dense(\n            int(flatten_input.shape[2]),\n            activation=\'relu\',\n            kernel_initializer=kernel_initializer))((l3))\n    output = TimeDistributed(\n        Reshape(input_tensor.shape[2:]),\n        name=output_name)(dense)\n    return output\n\n\ndef blstm(input_tensor, output_name=\'output\', params={}):\n    """""" Model function applier. """"""\n    return apply(apply_blstm, input_tensor, output_name, params)\n'"
spleeter/model/functions/unet.py,1,"b'#!/usr/bin/env python\n# coding: utf8\n\n""""""\nThis module contains building functions for U-net source\nseparation models in a similar way as in A. Jansson et al. ""Singing\nvoice separation with deep u-net convolutional networks"", ISMIR 2017.\nEach instrument is modeled by a single U-net convolutional\n/ deconvolutional network that take a mix spectrogram as input and the\nestimated sound spectrogram as output.\n""""""\n\nfrom functools import partial\n\n# pylint: disable=import-error\nimport tensorflow as tf\n\nfrom tensorflow.keras.layers import (\n    BatchNormalization,\n    Concatenate,\n    Conv2D,\n    Conv2DTranspose,\n    Dropout,\n    ELU,\n    LeakyReLU,\n    Multiply,\n    ReLU,\n    Softmax)\nfrom tensorflow.compat.v1 import logging\nfrom tensorflow.compat.v1.keras.initializers import he_uniform\n# pylint: enable=import-error\n\nfrom . import apply\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n\n\ndef _get_conv_activation_layer(params):\n    """"""\n\n    :param params:\n    :returns: Required Activation function.\n    """"""\n    conv_activation = params.get(\'conv_activation\')\n    if conv_activation == \'ReLU\':\n        return ReLU()\n    elif conv_activation == \'ELU\':\n        return ELU()\n    return LeakyReLU(0.2)\n\n\ndef _get_deconv_activation_layer(params):\n    """"""\n\n    :param params:\n    :returns: Required Activation function.\n    """"""\n    deconv_activation = params.get(\'deconv_activation\')\n    if deconv_activation == \'LeakyReLU\':\n        return LeakyReLU(0.2)\n    elif deconv_activation == \'ELU\':\n        return ELU()\n    return ReLU()\n\n\ndef apply_unet(\n        input_tensor,\n        output_name=\'output\',\n        params={},\n        output_mask_logit=False):\n    """""" Apply a convolutionnal U-net to model a single instrument (one U-net\n    is used for each instrument).\n\n    :param input_tensor:\n    :param output_name: (Optional) , default to \'output\'\n    :param params: (Optional) , default to empty dict.\n    :param output_mask_logit: (Optional) , default to False.\n    """"""\n    logging.info(f\'Apply unet for {output_name}\')\n    conv_n_filters = params.get(\'conv_n_filters\', [16, 32, 64, 128, 256, 512])\n    conv_activation_layer = _get_conv_activation_layer(params)\n    deconv_activation_layer = _get_deconv_activation_layer(params)\n    kernel_initializer = he_uniform(seed=50)\n    conv2d_factory = partial(\n        Conv2D,\n        strides=(2, 2),\n        padding=\'same\',\n        kernel_initializer=kernel_initializer)\n    # First layer.\n    conv1 = conv2d_factory(conv_n_filters[0], (5, 5))(input_tensor)\n    batch1 = BatchNormalization(axis=-1)(conv1)\n    rel1 = conv_activation_layer(batch1)\n    # Second layer.\n    conv2 = conv2d_factory(conv_n_filters[1], (5, 5))(rel1)\n    batch2 = BatchNormalization(axis=-1)(conv2)\n    rel2 = conv_activation_layer(batch2)\n    # Third layer.\n    conv3 = conv2d_factory(conv_n_filters[2], (5, 5))(rel2)\n    batch3 = BatchNormalization(axis=-1)(conv3)\n    rel3 = conv_activation_layer(batch3)\n    # Fourth layer.\n    conv4 = conv2d_factory(conv_n_filters[3], (5, 5))(rel3)\n    batch4 = BatchNormalization(axis=-1)(conv4)\n    rel4 = conv_activation_layer(batch4)\n    # Fifth layer.\n    conv5 = conv2d_factory(conv_n_filters[4], (5, 5))(rel4)\n    batch5 = BatchNormalization(axis=-1)(conv5)\n    rel5 = conv_activation_layer(batch5)\n    # Sixth layer\n    conv6 = conv2d_factory(conv_n_filters[5], (5, 5))(rel5)\n    batch6 = BatchNormalization(axis=-1)(conv6)\n    _ = conv_activation_layer(batch6)\n    #\n    #\n    conv2d_transpose_factory = partial(\n        Conv2DTranspose,\n        strides=(2, 2),\n        padding=\'same\',\n        kernel_initializer=kernel_initializer)\n    #\n    up1 = conv2d_transpose_factory(conv_n_filters[4], (5, 5))((conv6))\n    up1 = deconv_activation_layer(up1)\n    batch7 = BatchNormalization(axis=-1)(up1)\n    drop1 = Dropout(0.5)(batch7)\n    merge1 = Concatenate(axis=-1)([conv5, drop1])\n    #\n    up2 = conv2d_transpose_factory(conv_n_filters[3], (5, 5))((merge1))\n    up2 = deconv_activation_layer(up2)\n    batch8 = BatchNormalization(axis=-1)(up2)\n    drop2 = Dropout(0.5)(batch8)\n    merge2 = Concatenate(axis=-1)([conv4, drop2])\n    #\n    up3 = conv2d_transpose_factory(conv_n_filters[2], (5, 5))((merge2))\n    up3 = deconv_activation_layer(up3)\n    batch9 = BatchNormalization(axis=-1)(up3)\n    drop3 = Dropout(0.5)(batch9)\n    merge3 = Concatenate(axis=-1)([conv3, drop3])\n    #\n    up4 = conv2d_transpose_factory(conv_n_filters[1], (5, 5))((merge3))\n    up4 = deconv_activation_layer(up4)\n    batch10 = BatchNormalization(axis=-1)(up4)\n    merge4 = Concatenate(axis=-1)([conv2, batch10])\n    #\n    up5 = conv2d_transpose_factory(conv_n_filters[0], (5, 5))((merge4))\n    up5 = deconv_activation_layer(up5)\n    batch11 = BatchNormalization(axis=-1)(up5)\n    merge5 = Concatenate(axis=-1)([conv1, batch11])\n    #\n    up6 = conv2d_transpose_factory(1, (5, 5), strides=(2, 2))((merge5))\n    up6 = deconv_activation_layer(up6)\n    batch12 = BatchNormalization(axis=-1)(up6)\n    # Last layer to ensure initial shape reconstruction.\n    if not output_mask_logit:\n        up7 = Conv2D(\n            2,\n            (4, 4),\n            dilation_rate=(2, 2),\n            activation=\'sigmoid\',\n            padding=\'same\',\n            kernel_initializer=kernel_initializer)((batch12))\n        output = Multiply(name=output_name)([up7, input_tensor])\n        return output\n    return Conv2D(\n        2,\n        (4, 4),\n        dilation_rate=(2, 2),\n        padding=\'same\',\n        kernel_initializer=kernel_initializer)((batch12))\n\n\ndef unet(input_tensor, instruments, params={}):\n    """""" Model function applier. """"""\n    return apply(apply_unet, input_tensor, instruments, params)\n\n\ndef softmax_unet(input_tensor, instruments, params={}):\n    """""" Apply softmax to multitrack unet in order to have mask suming to one.\n\n    :param input_tensor: Tensor to apply blstm to.\n    :param instruments: Iterable that provides a collection of instruments.\n    :param params: (Optional) dict of BLSTM parameters.\n    :returns: Created output tensor dict.\n    """"""\n    logit_mask_list = []\n    for instrument in instruments:\n        out_name = f\'{instrument}_spectrogram\'\n        logit_mask_list.append(\n            apply_unet(\n                input_tensor,\n                output_name=out_name,\n                params=params,\n                output_mask_logit=True))\n    masks = Softmax(axis=4)(tf.stack(logit_mask_list, axis=4))\n    output_dict = {}\n    for i, instrument in enumerate(instruments):\n        out_name = f\'{instrument}_spectrogram\'\n        output_dict[out_name] = Multiply(name=out_name)([\n            masks[..., i],\n            input_tensor])\n    return output_dict\n'"
spleeter/model/provider/__init__.py,0,"b'#!/usr/bin/env python\n# coding: utf8\n\n""""""\n    This package provides tools for downloading model from network\n    using remote storage abstraction.\n\n    :Example:\n\n    >>> provider = MyProviderImplementation()\n    >>> provider.get(\'/path/to/local/storage\', params)\n""""""\n\nfrom abc import ABC, abstractmethod\nfrom os import environ, makedirs\nfrom os.path import exists, isabs, join, sep\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n\n\nclass ModelProvider(ABC):\n    """"""\n        A ModelProvider manages model files on disk and\n        file download is not available.\n    """"""\n\n    DEFAULT_MODEL_PATH = environ.get(\'MODEL_PATH\', \'pretrained_models\')\n    MODEL_PROBE_PATH = \'.probe\'\n\n    @abstractmethod\n    def download(self, name, path):\n        """""" Download model denoted by the given name to disk.\n\n        :param name: Name of the model to download.\n        :param path: Path of the directory to save model into.\n        """"""\n        pass\n\n    @staticmethod\n    def writeProbe(directory):\n        """""" Write a model probe file into the given directory.\n\n        :param directory: Directory to write probe into.\n        """"""\n        probe = join(directory, ModelProvider.MODEL_PROBE_PATH)\n        with open(probe, \'w\') as stream:\n            stream.write(\'OK\')\n\n    def get(self, model_directory):\n        """""" Ensures required model is available at given location.\n\n        :param model_directory: Expected model_directory to be available.\n        :raise IOError: If model can not be retrieved.\n        """"""\n        # Expend model directory if needed.\n        if not isabs(model_directory):\n            model_directory = join(self.DEFAULT_MODEL_PATH, model_directory)\n        # Download it if not exists.\n        model_probe = join(model_directory, self.MODEL_PROBE_PATH)\n        if not exists(model_probe):\n            if not exists(model_directory):\n                makedirs(model_directory)\n                self.download(\n                    model_directory.split(sep)[-1],\n                    model_directory)\n                self.writeProbe(model_directory)\n        return model_directory\n\n\ndef get_default_model_provider():\n    """""" Builds and returns a default model provider.\n\n    :returns: A default model provider instance to use.\n    """"""\n    from .github import GithubModelProvider\n    host = environ.get(\'GITHUB_HOST\', \'https://github.com\')\n    repository = environ.get(\'GITHUB_REPOSITORY\', \'deezer/spleeter\')\n    release = environ.get(\'GITHUB_RELEASE\', GithubModelProvider.LATEST_RELEASE)\n    return GithubModelProvider(host, repository, release)\n'"
spleeter/model/provider/github.py,0,"b'#!/usr/bin/env python\n# coding: utf8\n\n""""""\n    A ModelProvider backed by Github Release feature.\n\n    :Example:\n\n    >>> from spleeter.model.provider import github\n    >>> provider = github.GithubModelProvider(\n            \'github.com\',\n            \'Deezer/spleeter\',\n            \'latest\')\n    >>> provider.download(\'2stems\', \'/path/to/local/storage\')\n""""""\n\nimport hashlib\nimport tarfile\nimport os\n\nfrom tempfile import NamedTemporaryFile\n\nimport requests\n\nfrom . import ModelProvider\nfrom ...utils.logging import get_logger\n\n__email__ = \'research@deezer.com\'\n__author__ = \'Deezer Research\'\n__license__ = \'MIT License\'\n\n\ndef compute_file_checksum(path):\n    """""" Computes given path file sha256.\n\n    :param path: Path of the file to compute checksum for.\n    :returns: File checksum.\n    """"""\n    sha256 = hashlib.sha256()\n    with open(path, \'rb\') as stream:\n        for chunk in iter(lambda: stream.read(4096), b\'\'):\n            sha256.update(chunk)\n    return sha256.hexdigest()\n\n\nclass GithubModelProvider(ModelProvider):\n    """""" A ModelProvider implementation backed on Github for remote storage. """"""\n\n    LATEST_RELEASE = \'v1.4.0\'\n    RELEASE_PATH = \'releases/download\'\n    CHECKSUM_INDEX = \'checksum.json\'\n\n    def __init__(self, host, repository, release):\n        """""" Default constructor.\n\n        :param host: Host to the Github instance to reach.\n        :param repository: Repository path within target Github.\n        :param release: Release name to get models from.\n        """"""\n        self._host = host\n        self._repository = repository\n        self._release = release\n\n    def checksum(self, name):\n        """""" Downloads and returns reference checksum for the given model name.\n\n        :param name: Name of the model to get checksum for.\n        :returns: Checksum of the required model.\n        :raise ValueError: If the given model name is not indexed.\n        """"""\n        url = \'{}/{}/{}/{}/{}\'.format(\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            self.CHECKSUM_INDEX)\n        response = requests.get(url)\n        response.raise_for_status()\n        index = response.json()\n        if name not in index:\n            raise ValueError(\'No checksum for model {}\'.format(name))\n        return index[name]\n\n    def download(self, name, path):\n        """""" Download model denoted by the given name to disk.\n\n        :param name: Name of the model to download.\n        :param path: Path of the directory to save model into.\n        """"""\n        url = \'{}/{}/{}/{}/{}.tar.gz\'.format(\n            self._host,\n            self._repository,\n            self.RELEASE_PATH,\n            self._release,\n            name)\n        get_logger().info(\'Downloading model archive %s\', url)\n        with requests.get(url, stream=True) as response:\n            response.raise_for_status()\n            archive = NamedTemporaryFile(delete=False)\n            try:\n                with archive as stream:\n                    # Note: check for chunk size parameters ?\n                    for chunk in response.iter_content(chunk_size=8192):\n                        if chunk:\n                            stream.write(chunk)\n                get_logger().info(\'Validating archive checksum\')\n                if compute_file_checksum(archive.name) != self.checksum(name):\n                    raise IOError(\'Downloaded file is corrupted, please retry\')\n                get_logger().info(\'Extracting downloaded %s archive\', name)\n                with tarfile.open(name=archive.name) as tar:\n                    tar.extractall(path=path)\n            finally:\n                os.unlink(archive.name)\n        get_logger().info(\'%s model file(s) extracted\', name)\n'"
