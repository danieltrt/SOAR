file_path,api_count,code
config.py,0,"b""#-*- coding: utf-8 -*-\nimport argparse\n\ndef str2bool(v):\n  return v.lower() in ('true', '1')\n\narg_lists = []\nparser = argparse.ArgumentParser()\n\ndef add_argument_group(name):\n  arg = parser.add_argument_group(name)\n  arg_lists.append(arg)\n  return arg\n\n# Network\nnet_arg = add_argument_group('Network')\nnet_arg.add_argument('--hidden_dim', type=int, default=256, help='')\nnet_arg.add_argument('--num_layers', type=int, default=1, help='')\nnet_arg.add_argument('--input_dim', type=int, default=2, help='')\nnet_arg.add_argument('--max_enc_length', type=int, default=None, help='')\nnet_arg.add_argument('--max_dec_length', type=int, default=None, help='')\nnet_arg.add_argument('--init_min_val', type=float, default=-0.08, help='for uniform random initializer')\nnet_arg.add_argument('--init_max_val', type=float, default=+0.08, help='for uniform random initializer')\nnet_arg.add_argument('--num_glimpse', type=int, default=1, help='')\nnet_arg.add_argument('--use_terminal_symbol', type=str2bool, default=True, help='Not implemented yet')\n\n# Data\ndata_arg = add_argument_group('Data')\ndata_arg.add_argument('--task', type=str, default='tsp')\ndata_arg.add_argument('--batch_size', type=int, default=128)\ndata_arg.add_argument('--min_data_length', type=int, default=5)\ndata_arg.add_argument('--max_data_length', type=int, default=10)\ndata_arg.add_argument('--train_num', type=int, default=1000000)\ndata_arg.add_argument('--valid_num', type=int, default=1000)\ndata_arg.add_argument('--test_num', type=int, default=1000)\n\n# Training / test parameters\ntrain_arg = add_argument_group('Training')\ntrain_arg.add_argument('--is_train', type=str2bool, default=True, help='')\ntrain_arg.add_argument('--optimizer', type=str, default='rmsprop', help='')\ntrain_arg.add_argument('--max_step', type=int, default=1000000, help='')\ntrain_arg.add_argument('--lr_start', type=float, default=0.001, help='')\ntrain_arg.add_argument('--lr_decay_step', type=int, default=5000, help='')\ntrain_arg.add_argument('--lr_decay_rate', type=float, default=0.96, help='')\ntrain_arg.add_argument('--max_grad_norm', type=float, default=2.0, help='')\ntrain_arg.add_argument('--checkpoint_secs', type=int, default=300, help='')\n\n# Misc\nmisc_arg = add_argument_group('Misc')\nmisc_arg.add_argument('--log_step', type=int, default=50, help='')\nmisc_arg.add_argument('--num_log_samples', type=int, default=3, help='')\nmisc_arg.add_argument('--log_level', type=str, default='INFO', choices=['INFO', 'DEBUG', 'WARN'], help='')\nmisc_arg.add_argument('--log_dir', type=str, default='logs')\nmisc_arg.add_argument('--data_dir', type=str, default='data')\nmisc_arg.add_argument('--output_dir', type=str, default='outputs')\nmisc_arg.add_argument('--load_path', type=str, default='')\nmisc_arg.add_argument('--debug', type=str2bool, default=False)\nmisc_arg.add_argument('--gpu_memory_fraction', type=float, default=1.0)\nmisc_arg.add_argument('--random_seed', type=int, default=123, help='')\n\ndef get_config():\n  config, unparsed = parser.parse_known_args()\n  return config, unparsed\n"""
data_loader.py,18,"b'# Most of the codes are from \n# https://github.com/vshallc/PtrNets/blob/master/pointer/misc/tsp.py\nimport os\nimport re\nimport zipfile\nimport itertools\nimport threading\nimport numpy as np\nfrom tqdm import trange, tqdm\nfrom collections import namedtuple\n\nimport tensorflow as tf\nfrom download import download_file_from_google_drive\n\nGOOGLE_DRIVE_IDS = {\n    \'tsp5_train.zip\': \'0B2fg8yPGn2TCSW1pNTJMXzFPYTg\',\n    \'tsp10_train.zip\': \'0B2fg8yPGn2TCbHowM0hfOTJCNkU\',\n    \'tsp5-20_train.zip\': \'0B2fg8yPGn2TCTWNxX21jTDBGeXc\',\n    \'tsp50_train.zip\': \'0B2fg8yPGn2TCaVQxSl9ab29QajA\',\n    \'tsp20_test.txt\': \'0B2fg8yPGn2TCdF9TUU5DZVNCNjQ\',\n    \'tsp40_test.txt\': \'0B2fg8yPGn2TCcjFrYk85SGFVNlU\',\n    \'tsp50_test.txt.zip\': \'0B2fg8yPGn2TCUVlCQmQtelpZTTQ\',\n}\n\nTSP = namedtuple(\'TSP\', [\'x\', \'y\', \'name\'])\n\ndef length(x, y):\n  return np.linalg.norm(np.asarray(x) - np.asarray(y))\n\n# https://gist.github.com/mlalevic/6222750\ndef solve_tsp_dynamic(points):\n  #calc all lengths\n  all_distances = [[length(x,y) for y in points] for x in points]\n  #initial value - just distance from 0 to every other point + keep the track of edges\n  A = {(frozenset([0, idx+1]), idx+1): (dist, [0,idx+1]) for idx,dist in enumerate(all_distances[0][1:])}\n  cnt = len(points)\n  for m in range(2, cnt):\n    B = {}\n    for S in [frozenset(C) | {0} for C in itertools.combinations(range(1, cnt), m)]:\n      for j in S - {0}:\n        B[(S, j)] = min( [(A[(S-{j},k)][0] + all_distances[k][j], A[(S-{j},k)][1] + [j]) for k in S if k != 0 and k!=j])  #this will use 0th index of tuple for ordering, the same as if key=itemgetter(0) used\n    A = B\n  res = min([(A[d][0] + all_distances[0][d[1]], A[d][1]) for d in iter(A)])\n  return np.asarray(res[1]) + 1 # 0 for padding\n\ndef generate_one_example(n_nodes, rng):\n  nodes = rng.rand(n_nodes, 2).astype(np.float32)\n  solutions = solve_tsp_dynamic(nodes)\n  return nodes, solutions\n\ndef read_paper_dataset(paths, max_length):\n  x, y = [], []\n  for path in paths:\n    tf.logging.info(""Read dataset {} which is used in the paper.."".format(path))\n    length = max(re.findall(\'\\d+\', path))\n    with open(path) as f:\n      for l in tqdm(f):\n        inputs, outputs = l.split(\' output \')\n        x.append(np.array(inputs.split(), dtype=np.float32).reshape([-1, 2]))\n        y.append(np.array(outputs.split(), dtype=np.int32)[:-1]) # skip the last one\n  return x, y\n\nclass TSPDataLoader(object):\n  def __init__(self, config, rng=None):\n    self.config = config\n    self.rng = rng\n\n    self.task = config.task.lower()\n    self.batch_size = config.batch_size\n    self.min_length = config.min_data_length\n    self.max_length = config.max_data_length\n\n    self.is_train = config.is_train\n    self.use_terminal_symbol = config.use_terminal_symbol\n    self.random_seed = config.random_seed\n\n    self.data_num = {}\n    self.data_num[\'train\'] = config.train_num\n    self.data_num[\'test\'] = config.test_num\n\n    self.data_dir = config.data_dir\n    self.task_name = ""{}_({},{})"".format(\n        self.task, self.min_length, self.max_length)\n\n    self.data = None\n    self.coord = None\n    self.threads = None\n    self.input_ops, self.target_ops = None, None\n    self.queue_ops, self.enqueue_ops = None, None\n    self.x, self.y, self.seq_length, self.mask = None, None, None, None\n\n    paths = self.download_google_drive_file()\n    if len(paths) != 0:\n      self._maybe_generate_and_save(except_list=paths.keys())\n      for name, path in paths.items():\n        self.read_zip_and_update_data(path, name)\n    else:\n      self._maybe_generate_and_save()\n    self._create_input_queue()\n\n  def _create_input_queue(self, queue_capacity_factor=16):\n    self.input_ops, self.target_ops = {}, {}\n    self.queue_ops, self.enqueue_ops = {}, {}\n    self.x, self.y, self.seq_length, self.mask = {}, {}, {}, {}\n\n    for name in self.data_num.keys():\n      self.input_ops[name] = tf.placeholder(tf.float32, shape=[None, None])\n      self.target_ops[name] = tf.placeholder(tf.int32, shape=[None])\n\n      min_after_dequeue = 1000\n      capacity = min_after_dequeue + 3 * self.batch_size\n\n      self.queue_ops[name] = tf.RandomShuffleQueue(\n          capacity=capacity,\n          min_after_dequeue=min_after_dequeue,\n          dtypes=[tf.float32, tf.int32],\n          shapes=[[self.max_length, 2,], [self.max_length]],\n          seed=self.random_seed,\n          name=""random_queue_{}"".format(name))\n      self.enqueue_ops[name] = \\\n          self.queue_ops[name].enqueue([self.input_ops[name], self.target_ops[name]])\n\n      inputs, labels = self.queue_ops[name].dequeue()\n\n      seq_length = tf.shape(inputs)[0]\n      if self.use_terminal_symbol:\n        mask = tf.ones([seq_length + 1], dtype=tf.float32) # terminal symbol\n      else:\n        mask = tf.ones([seq_length], dtype=tf.float32)\n\n      self.x[name], self.y[name], self.seq_length[name], self.mask[name] = \\\n          tf.train.batch(\n              [inputs, labels, seq_length, mask],\n              batch_size=self.batch_size,\n              capacity=capacity,\n              dynamic_pad=True,\n              name=""batch_and_pad"")\n\n  def run_input_queue(self, sess):\n    self.threads = []\n    self.coord = tf.train.Coordinator()\n\n    for name in self.data_num.keys():\n      def load_and_enqueue(sess, name, input_ops, target_ops, enqueue_ops, coord):\n        idx = 0\n        while not coord.should_stop():\n          feed_dict = {\n              input_ops[name]: self.data[name].x[idx],\n              target_ops[name]: self.data[name].y[idx],\n          }\n          sess.run(self.enqueue_ops[name], feed_dict=feed_dict)\n          idx = idx+1 if idx+1 <= len(self.data[name].x) - 1 else 0\n\n      args = (sess, name, self.input_ops, self.target_ops, self.enqueue_ops, self.coord)\n      t = threading.Thread(target=load_and_enqueue, args=args)\n      t.start()\n      self.threads.append(t)\n      tf.logging.info(""Thread for [{}] start"".format(name))\n\n  def stop_input_queue(self):\n    self.coord.request_stop()\n    self.coord.join(self.threads)\n    tf.logging.info(""All threads stopped"")\n\n  def _maybe_generate_and_save(self, except_list=[]):\n    self.data = {}\n\n    for name, num in self.data_num.items():\n      if name in except_list:\n        tf.logging.info(""Skip creating {} because of given except_list {}"".format(name, except_list))\n        continue\n      path = self.get_path(name)\n\n      if not os.path.exists(path):\n        tf.logging.info(""Creating {} for [{}]"".format(path, self.task))\n\n        x = np.zeros([num, self.max_length, 2], dtype=np.float32)\n        y = np.zeros([num, self.max_length], dtype=np.int32)\n\n        for idx in trange(num, desc=""Create {} data"".format(name)):\n          n_nodes = self.rng.randint(self.min_length, self.max_length+ 1)\n          nodes, res = generate_one_example(n_nodes, self.rng)\n          x[idx,:len(nodes)] = nodes\n          y[idx,:len(res)] = res\n\n        np.savez(path, x=x, y=y)\n        self.data[name] = TSP(x=x, y=y, name=name)\n      else:\n        tf.logging.info(""Skip creating {} for [{}]"".format(path, self.task))\n        tmp = np.load(path)\n        self.data[name] = TSP(x=tmp[\'x\'], y=tmp[\'y\'], name=name)\n\n  def get_path(self, name):\n    return os.path.join(\n        self.data_dir, ""{}_{}={}.npz"".format(\n            self.task_name, name, self.data_num[name]))\n\n  def download_google_drive_file(self):\n    paths = {}\n    for mode in [\'train\', \'test\']:\n      candidates = []\n      candidates.append(\n          \'{}{}_{}\'.format(self.task, self.max_length, mode))\n      candidates.append(\n          \'{}{}-{}_{}\'.format(self.task, self.min_length, self.max_length, mode))\n\n      for key in candidates:\n        for search_key in GOOGLE_DRIVE_IDS.keys():\n          if search_key.startswith(key):\n            path = os.path.join(self.data_dir, search_key)\n            tf.logging.info(""Download dataset of the paper to {}"".format(path))\n\n            if not os.path.exists(path):\n              download_file_from_google_drive(GOOGLE_DRIVE_IDS[search_key], path)\n              if path.endswith(\'zip\'):\n                with zipfile.ZipFile(path, \'r\') as z:\n                  z.extractall(self.data_dir)\n            paths[mode] = path\n\n    tf.logging.info(""Can\'t found dataset from the paper!"")\n    return paths\n\n  def read_zip_and_update_data(self, path, name):\n    if path.endswith(\'zip\'):\n      filenames = zipfile.ZipFile(path).namelist()\n      paths = [os.path.join(self.data_dir, filename) for filename in filenames]\n    else:\n      paths = [path]\n\n    x_list, y_list = read_paper_dataset(paths, self.max_length)\n\n    x = np.zeros([len(x_list), self.max_length, 2], dtype=np.float32)\n    y = np.zeros([len(y_list), self.max_length], dtype=np.int32)\n\n    for idx, (nodes, res) in enumerate(tqdm(zip(x_list, y_list))):\n      x[idx,:len(nodes)] = nodes\n      y[idx,:len(res)] = res\n\n    if self.data is None:\n      self.data = {}\n\n    tf.logging.info(""Update [{}] data with {} used in the paper"".format(name, path))\n    self.data[name] = TSP(x=x, y=y, name=name)\n'"
download.py,0,"b'# Code based on\n# http://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drive/39225039#39225039\nimport requests\nfrom tqdm import tqdm\n\ndef download_file_from_google_drive(id, destination):\n  URL = ""https://docs.google.com/uc?export=download""\n\n  session = requests.Session()\n\n  response = session.get(URL, params = { \'id\' : id }, stream = True)\n  token = get_confirm_token(response)\n\n  if token:\n    params = { \'id\' : id, \'confirm\' : token }\n    response = session.get(URL, params = params, stream = True)\n\n  save_response_content(response, destination)  \n  return True\n\ndef get_confirm_token(response):\n  for key, value in response.cookies.items():\n    if key.startswith(\'download_warning\'):\n      return value\n\n  return None\n\ndef save_response_content(response, destination):\n  CHUNK_SIZE = 32768\n\n  with open(destination, ""wb"") as f:\n    for chunk in tqdm(response.iter_content(CHUNK_SIZE)):\n      if chunk: # filter out keep-alive new chunks\n        f.write(chunk)\n'"
layers.py,38,"b'import tensorflow as tf\nfrom tensorflow.contrib import rnn\nfrom tensorflow.contrib import layers\nfrom tensorflow.contrib import seq2seq\nfrom tensorflow.python.util import nest\n\ntry:\n  from tensorflow.contrib.layers.python.layers import utils\nexcept:\n  from tensorflow.contrib.layers import utils\n\nsmart_cond = utils.smart_cond\n\ntry:\n  LSTMCell = rnn.LSTMCell\n  MultiRNNCell = rnn.MultiRNNCell\n  dynamic_rnn_decoder = seq2seq.dynamic_rnn_decoder\n  simple_decoder_fn_train = seq2seq.simple_decoder_fn_train\nexcept:\n  LSTMCell = tf.contrib.rnn.LSTMCell\n  MultiRNNCell = tf.contrib.rnn.MultiRNNCell\n  dynamic_rnn_decoder = tf.contrib.seq2seq.dynamic_rnn_decoder\n  simple_decoder_fn_train = tf.contrib.seq2seq.simple_decoder_fn_train\n\ntry:\n  from tensorflow.python.ops.gen_array_ops import _concat_v2 as concat_v2\nexcept:\n  concat_v2 = tf.concat_v2\n\ndef decoder_rnn(cell, inputs,\n                enc_outputs, enc_final_states,\n                seq_length, hidden_dim,\n                num_glimpse, batch_size, is_train,\n                end_of_sequence_id=0, initializer=None,\n                max_length=None):\n  with tf.variable_scope(""decoder_rnn"") as scope:\n    def attention(ref, query, with_softmax, scope=""attention""):\n      with tf.variable_scope(scope):\n        W_ref = tf.get_variable(\n            ""W_ref"", [1, hidden_dim, hidden_dim], initializer=initializer)\n        W_q = tf.get_variable(\n            ""W_q"", [hidden_dim, hidden_dim], initializer=initializer)\n        v = tf.get_variable(\n            ""v"", [hidden_dim], initializer=initializer)\n\n        encoded_ref = tf.nn.conv1d(ref, W_ref, 1, ""VALID"", name=""encoded_ref"")\n        encoded_query = tf.expand_dims(tf.matmul(query, W_q, name=""encoded_query""), 1)\n        tiled_encoded_Query = tf.tile(\n            encoded_query, [1, tf.shape(encoded_ref)[1], 1], name=""tiled_encoded_query"")\n        scores = tf.reduce_sum(v * tf.tanh(encoded_ref + encoded_query), [-1])\n\n        if with_softmax:\n          return tf.nn.softmax(scores)\n        else:\n          return scores\n\n    def glimpse(ref, query, scope=""glimpse""):\n      p = attention(ref, query, with_softmax=True, scope=scope)\n      alignments = tf.expand_dims(p, 2)\n      return tf.reduce_sum(alignments * ref, [1])\n\n    def output_fn(ref, query, num_glimpse):\n      if query is None:\n        return tf.zeros([max_length], tf.float32) # only used for shape inference\n      else:\n        for idx in range(num_glimpse):\n          query = glimpse(ref, query, ""glimpse_{}"".format(idx))\n        return attention(ref, query, with_softmax=False, scope=""attention"")\n\n    def input_fn(sampled_idx):\n      return tf.stop_gradient(\n          tf.gather_nd(enc_outputs, index_matrix_to_pairs(sampled_idx)))\n\n    if is_train:\n      decoder_fn = simple_decoder_fn_train(enc_final_states)\n    else:\n      maximum_length = tf.convert_to_tensor(max_length, tf.int32)\n\n      def decoder_fn(time, cell_state, cell_input, cell_output, context_state):\n        cell_output = output_fn(enc_outputs, cell_output, num_glimpse)\n        if cell_state is None:\n          cell_state = enc_final_states\n          next_input = cell_input\n          done = tf.zeros([batch_size,], dtype=tf.bool)\n        else:\n          sampled_idx = tf.cast(tf.argmax(cell_output, 1), tf.int32)\n          next_input = input_fn(sampled_idx)\n          done = tf.equal(sampled_idx, end_of_sequence_id)\n\n        done = tf.cond(tf.greater(time, maximum_length),\n          lambda: tf.ones([batch_size,], dtype=tf.bool),\n          lambda: done)\n        return (done, cell_state, next_input, cell_output, context_state)\n\n    outputs, final_state, final_context_state = \\\n        dynamic_rnn_decoder(cell, decoder_fn, inputs=inputs,\n                            sequence_length=seq_length, scope=scope)\n\n    if is_train:\n      transposed_outputs = tf.transpose(outputs, [1, 0, 2])\n      fn = lambda x: output_fn(enc_outputs, x, num_glimpse)\n      outputs = tf.transpose(tf.map_fn(fn, transposed_outputs), [1, 0, 2])\n\n    return outputs, final_state, final_context_state\n\ndef trainable_initial_state(batch_size, state_size,\n                            initializer=None, name=""initial_state""):\n  flat_state_size = nest.flatten(state_size)\n\n  if not initializer:\n    flat_initializer = tuple(tf.zeros_initializer for _ in flat_state_size)\n  else:\n    flat_initializer = tuple(tf.zeros_initializer for initializer in flat_state_size)\n\n  names = [""{}_{}"".format(name, i) for i in xrange(len(flat_state_size))]\n  tiled_states = []\n\n  for name, size, init in zip(names, flat_state_size, flat_initializer):\n    shape_with_batch_dim = [1, size]\n    initial_state_variable = tf.get_variable(\n        name, shape=shape_with_batch_dim, initializer=init())\n\n    tiled_state = tf.tile(initial_state_variable,\n                          [batch_size, 1], name=(name + ""_tiled""))\n    tiled_states.append(tiled_state)\n\n  return nest.pack_sequence_as(structure=state_size,\n                               flat_sequence=tiled_states)\n\ndef index_matrix_to_pairs(index_matrix):\n  # [[3,1,2], [2,3,1]] -> [[[0, 3], [1, 1], [2, 2]], \n  #                        [[0, 2], [1, 3], [2, 1]]]\n  replicated_first_indices = tf.range(tf.shape(index_matrix)[0])\n  rank = len(index_matrix.get_shape())\n  if rank == 2:\n    replicated_first_indices = tf.tile(\n        tf.expand_dims(replicated_first_indices, dim=1),\n        [1, tf.shape(index_matrix)[1]])\n  return tf.stack([replicated_first_indices, index_matrix], axis=rank)\n'"
main.py,3,"b'import sys\nimport numpy as np\nimport tensorflow as tf\n\nfrom trainer import Trainer\nfrom config import get_config\nfrom utils import prepare_dirs_and_logger, save_config\n\nconfig = None\n\ndef main(_):\n  prepare_dirs_and_logger(config)\n\n  if not config.task.lower().startswith(\'tsp\'):\n    raise Exception(""[!] Task should starts with TSP"")\n\n  if config.max_enc_length is None:\n    config.max_enc_length = config.max_data_length\n  if config.max_dec_length is None:\n    config.max_dec_length = config.max_data_length\n\n  rng = np.random.RandomState(config.random_seed)\n  tf.set_random_seed(config.random_seed)\n\n  trainer = Trainer(config, rng)\n  save_config(config.model_dir, config)\n\n  if config.is_train:\n    trainer.train()\n  else:\n    if not config.load_path:\n      raise Exception(""[!] You should specify `load_path` to load a pretrained model"")\n    trainer.test()\n\n  tf.logging.info(""Run finished."")\n\nif __name__ == ""__main__"":\n  config, unparsed = get_config()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n'"
model.py,42,"b'import tensorflow as tf\nfrom tensorflow.contrib.framework import arg_scope\n\nfrom layers import *\n\nclass Model(object):\n  def __init__(self, config, \n               inputs, labels, enc_seq_length, dec_seq_length, mask,\n               reuse=False, is_critic=False):\n    self.task = config.task\n    self.debug = config.debug\n    self.config = config\n\n    self.input_dim = config.input_dim\n    self.hidden_dim = config.hidden_dim\n    self.num_layers = config.num_layers\n\n    self.max_enc_length = config.max_enc_length\n    self.max_dec_length = config.max_dec_length\n    self.num_glimpse = config.num_glimpse\n\n    self.init_min_val = config.init_min_val\n    self.init_max_val = config.init_max_val\n    self.initializer = \\\n        tf.random_uniform_initializer(self.init_min_val, self.init_max_val)\n\n    self.use_terminal_symbol = config.use_terminal_symbol\n\n    self.lr_start = config.lr_start\n    self.lr_decay_step = config.lr_decay_step\n    self.lr_decay_rate = config.lr_decay_rate\n    self.max_grad_norm = config.max_grad_norm\n\n    self.layer_dict = {}\n\n    ##############\n    # inputs\n    ##############\n\n    self.is_training = tf.placeholder_with_default(\n        tf.constant(False, dtype=tf.bool),\n        shape=(), name=\'is_training\'\n    )\n\n    self.enc_inputs, self.dec_targets, self.enc_seq_length, self.dec_seq_length, self.mask = \\\n        smart_cond(\n            self.is_training,\n            lambda: (inputs[\'train\'], labels[\'train\'], enc_seq_length[\'train\'],\n                     dec_seq_length[\'train\'], mask[\'train\']),\n            lambda: (inputs[\'test\'], labels[\'test\'], enc_seq_length[\'test\'],\n                     dec_seq_length[\'test\'], mask[\'test\'])\n        )\n\n    if self.use_terminal_symbol:\n      self.dec_seq_length += 1 # terminal symbol\n\n    self._build_model()\n    self._build_steps()\n\n    if not reuse:\n      self._build_optim()\n\n    self.train_summary = tf.summary.merge([\n        tf.summary.scalar(""train/total_loss"", self.total_loss),\n        tf.summary.scalar(""train/lr"", self.lr),\n    ])\n\n    self.test_summary = tf.summary.merge([\n        tf.summary.scalar(""test/total_loss"", self.total_loss),\n    ])\n\n  def _build_steps(self):\n    def run(sess, fetch, feed_dict, summary_writer, summary):\n      fetch[\'step\'] = self.global_step\n      if summary is not None:\n        fetch[\'summary\'] = summary\n\n      result = sess.run(fetch)\n      if summary_writer is not None:\n        summary_writer.add_summary(result[\'summary\'], result[\'step\'])\n        summary_writer.flush()\n      return result\n\n    def train(sess, fetch, summary_writer):\n      return run(sess, fetch, feed_dict={},\n                 summary_writer=summary_writer, summary=self.train_summary)\n\n    def test(sess, fetch, summary_writer=None):\n      return run(sess, fetch, feed_dict={self.is_training: False},\n                 summary_writer=summary_writer, summary=self.test_summary)\n\n    self.train = train\n    self.test = test\n\n  def _build_model(self):\n    tf.logging.info(""Create a model.."")\n    self.global_step = tf.Variable(0, trainable=False)\n\n    input_embed = tf.get_variable(\n        ""input_embed"", [1, self.input_dim, self.hidden_dim],\n        initializer=self.initializer)\n\n    with tf.variable_scope(""encoder""):\n      self.embeded_enc_inputs = tf.nn.conv1d(\n          self.enc_inputs, input_embed, 1, ""VALID"")\n\n    batch_size = tf.shape(self.enc_inputs)[0]\n    with tf.variable_scope(""encoder""):\n      self.enc_cell = LSTMCell(\n          self.hidden_dim,\n          initializer=self.initializer)\n\n      if self.num_layers > 1:\n        cells = [self.enc_cell] * self.num_layers\n        self.enc_cell = MultiRNNCell(cells)\n      self.enc_init_state = trainable_initial_state(\n          batch_size, self.enc_cell.state_size)\n\n      # self.encoder_outputs : [None, max_time, output_size]\n      self.enc_outputs, self.enc_final_states = tf.nn.dynamic_rnn(\n          self.enc_cell, self.embeded_enc_inputs,\n          self.enc_seq_length, self.enc_init_state)\n\n      self.first_decoder_input = tf.expand_dims(trainable_initial_state(\n          batch_size, self.hidden_dim, name=""first_decoder_input""), 1)\n      if self.use_terminal_symbol:\n        # 0 index indicates terminal\n        self.enc_outputs = concat_v2(\n            [self.first_decoder_input, self.enc_outputs], axis=1)\n\n    with tf.variable_scope(""decoder""):\n      self.idx_pairs = index_matrix_to_pairs(self.dec_targets)\n      self.embeded_dec_inputs = tf.stop_gradient(\n          tf.gather_nd(self.enc_outputs, self.idx_pairs))\n\n      if self.use_terminal_symbol:\n        tiled_zero_idxs = tf.tile(tf.zeros(\n            [1, 1], dtype=tf.int32), [batch_size, 1], name=""tiled_zero_idxs"")\n        self.dec_targets = concat_v2([self.dec_targets, tiled_zero_idxs], axis=1)\n\n      self.embeded_dec_inputs = concat_v2(\n          [self.first_decoder_input, self.embeded_dec_inputs], axis=1)\n\n      self.dec_cell = LSTMCell(\n          self.hidden_dim,\n          initializer=self.initializer)\n\n      if self.num_layers > 1:\n        cells = [self.dec_cell] * self.num_layers\n        self.dec_cell = MultiRNNCell(cells)\n\n      self.dec_pred_logits, _, _ = decoder_rnn(\n          self.dec_cell, self.embeded_dec_inputs, \n          self.enc_outputs, self.enc_final_states,\n          self.dec_seq_length, self.hidden_dim,\n          self.num_glimpse, batch_size, is_train=True,\n          initializer=self.initializer)\n      self.dec_pred_prob = tf.nn.softmax(\n          self.dec_pred_logits, 2, name=""dec_pred_prob"")\n      self.dec_pred = tf.argmax(\n          self.dec_pred_logits, 2, name=""dec_pred"")\n\n    with tf.variable_scope(""decoder"", reuse=True):\n      self.dec_inference_logits, _, _ = decoder_rnn(\n          self.dec_cell, self.first_decoder_input,\n          self.enc_outputs, self.enc_final_states,\n          self.dec_seq_length, self.hidden_dim,\n          self.num_glimpse, batch_size, is_train=False,\n          initializer=self.initializer,\n          max_length=self.max_dec_length + int(self.use_terminal_symbol))\n      self.dec_inference_prob = tf.nn.softmax(\n          self.dec_inference_logits, 2, name=""dec_inference_logits"")\n      self.dec_inference = tf.argmax(\n          self.dec_inference_logits, 2, name=""dec_inference"")\n\n  def _build_optim(self):\n    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        labels=self.dec_targets, logits=self.dec_pred_logits)\n    inference_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        labels=self.dec_targets, logits=self.dec_inference_logits)\n\n    def apply_mask(op):\n      length = tf.cast(op[:1], tf.int32)\n      loss = op[1:]\n      return tf.multiply(loss, tf.ones(length, dtype=tf.float32))\n\n    batch_loss = tf.div(\n        tf.reduce_sum(tf.multiply(losses, self.mask)),\n        tf.reduce_sum(self.mask), name=""batch_loss"")\n\n    batch_inference_loss = tf.div(\n        tf.reduce_sum(tf.multiply(losses, self.mask)),\n        tf.reduce_sum(self.mask), name=""batch_inference_loss"")\n\n    tf.losses.add_loss(batch_loss)\n    total_loss = tf.losses.get_total_loss()\n\n    self.total_loss = total_loss\n    self.target_cross_entropy_losses = losses\n    self.total_inference_loss = batch_inference_loss\n\n    self.lr = tf.train.exponential_decay(\n        self.lr_start, self.global_step, self.lr_decay_step,\n        self.lr_decay_rate, staircase=True, name=""learning_rate"")\n\n    optimizer = tf.train.AdamOptimizer(self.lr)\n\n    if self.max_grad_norm != None:\n      grads_and_vars = optimizer.compute_gradients(self.total_loss)\n      for idx, (grad, var) in enumerate(grads_and_vars):\n        if grad is not None:\n          grads_and_vars[idx] = (tf.clip_by_norm(grad, self.max_grad_norm), var)\n      self.optim = optimizer.apply_gradients(grads_and_vars, global_step=self.global_step)\n    else:\n      self.optim = optimizer.minimize(self.total_loss, global_step=self.global_step)\n'"
trainer.py,11,"b'import os\nimport numpy as np\nfrom tqdm import trange\nimport tensorflow as tf\nfrom tensorflow.contrib.framework.python.ops import arg_scope\n\nfrom model import Model\nfrom utils import show_all_variables\nfrom data_loader import TSPDataLoader\n\nclass Trainer(object):\n  def __init__(self, config, rng):\n    self.config = config\n    self.rng = rng\n\n    self.task = config.task\n    self.model_dir = config.model_dir\n    self.gpu_memory_fraction = config.gpu_memory_fraction\n\n    self.log_step = config.log_step\n    self.max_step = config.max_step\n    self.num_log_samples = config.num_log_samples\n    self.checkpoint_secs = config.checkpoint_secs\n\n    if config.task.lower().startswith(\'tsp\'):\n      self.data_loader = TSPDataLoader(config, rng=self.rng)\n    else:\n      raise Exception(""[!] Unknown task: {}"".format(config.task))\n\n    self.model = Model(\n        config,\n        inputs=self.data_loader.x,\n        labels=self.data_loader.y,\n        enc_seq_length=self.data_loader.seq_length,\n        dec_seq_length=self.data_loader.seq_length,\n        mask=self.data_loader.mask)\n\n    self.build_session()\n    show_all_variables()\n\n  def build_session(self):\n    self.saver = tf.train.Saver()\n    self.summary_writer = tf.summary.FileWriter(self.model_dir)\n\n    sv = tf.train.Supervisor(logdir=self.model_dir,\n                             is_chief=True,\n                             saver=self.saver,\n                             summary_op=None,\n                             summary_writer=self.summary_writer,\n                             save_summaries_secs=300,\n                             save_model_secs=self.checkpoint_secs,\n                             global_step=self.model.global_step)\n\n    gpu_options = tf.GPUOptions(\n        per_process_gpu_memory_fraction=self.gpu_memory_fraction,\n        allow_growth=True) # seems to be not working\n    sess_config = tf.ConfigProto(allow_soft_placement=True,\n                                 gpu_options=gpu_options)\n\n    self.sess = sv.prepare_or_wait_for_session(config=sess_config)\n\n  def train(self):\n    tf.logging.info(""Training starts..."")\n    self.data_loader.run_input_queue(self.sess)\n\n    summary_writer = None\n    for k in trange(self.max_step, desc=""train""):\n      fetch = {\n          \'optim\': self.model.optim,\n      }\n      result = self.model.train(self.sess, fetch, summary_writer)\n\n      if result[\'step\'] % self.log_step == 0:\n        self._test(self.summary_writer)\n\n      summary_writer = self._get_summary_writer(result)\n\n    self.data_loader.stop_input_queue()\n\n  def test(self):\n    tf.logging.info(""Testing starts..."")\n    self.data_loader.run_input_queue(self.sess)\n\n    for idx in range(10):\n      self._test(None)\n\n    self.data_loader.stop_input_queue()\n\n  def _test(self, summary_writer):\n    fetch = {\n        \'loss\': self.model.total_inference_loss,\n        \'pred\': self.model.dec_inference,\n        \'true\': self.model.dec_targets,\n    }\n    result = self.model.test(self.sess, fetch, summary_writer)\n\n    tf.logging.info("""")\n    tf.logging.info(""test loss: {}"".format(result[\'loss\']))\n    for idx in range(self.num_log_samples):\n      pred, true = result[\'pred\'][idx], result[\'true\'][idx]\n      tf.logging.info(""test pred: {}"".format(pred))\n      tf.logging.info(""test true: {} ({})"".format(true, np.array_equal(pred, true)))\n\n    if summary_writer:\n      summary_writer.add_summary(result[\'summary\'], result[\'step\'])\n\n  def _get_summary_writer(self, result):\n    if result[\'step\'] % self.log_step == 0:\n      return self.summary_writer\n    else:\n      return None\n'"
utils.py,4,"b'import os\nimport json\nimport logging\nimport numpy as np\nfrom datetime import datetime\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\ndef prepare_dirs_and_logger(config):\n  formatter = logging.Formatter(\n      ""%(asctime)s:%(levelname)s::%(message)s"")\n  logger = logging.getLogger(\'tensorflow\')\n\n  for hdlr in logger.handlers:\n    logger.removeHandler(hdlr)\n\n  handler = logging.StreamHandler()\n  handler.setFormatter(formatter)\n\n  logger.addHandler(handler)\n  logger.setLevel(tf.logging.INFO)\n\n  if config.load_path:\n    if config.load_path.startswith(config.task):\n      config.model_name = config.load_path\n    else:\n      config.model_name = ""{}_{}"".format(config.task, config.load_path)\n  else:\n    config.model_name = ""{}_{}"".format(config.task, get_time())\n\n  config.model_dir = os.path.join(config.log_dir, config.model_name)\n\n  for path in [config.log_dir, config.data_dir, config.model_dir]:\n    if not os.path.exists(path):\n      os.makedirs(path)\n\ndef get_time():\n  return datetime.now().strftime(""%Y-%m-%d_%H-%M-%S"")\n\ndef show_all_variables():\n  model_vars = tf.trainable_variables()\n  slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n\ndef save_config(model_dir, config):\n  param_path = os.path.join(model_dir, ""params.json"")\n\n  tf.logging.info(""MODEL dir: %s"" % model_dir)\n  tf.logging.info(""PARAM path: %s"" % param_path)\n\n  with open(param_path, \'w\') as fp:\n    json.dump(config.__dict__, fp,  indent=4, sort_keys=True)\n'"
