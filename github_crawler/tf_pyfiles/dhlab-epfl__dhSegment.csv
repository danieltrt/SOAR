file_path,api_count,code
demo.py,1,"b'#!/usr/bin/env python\n\nimport os\nfrom glob import glob\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom imageio import imread, imsave\nfrom tqdm import tqdm\n\nfrom dh_segment.io import PAGE\nfrom dh_segment.inference import LoadedModel\nfrom dh_segment.post_processing import boxes_detection, binarization\n\n# To output results in PAGE XML format (http://www.primaresearch.org/schema/PAGE/gts/pagecontent/2013-07-15/)\nPAGE_XML_DIR = \'./page_xml\'\n\n\ndef page_make_binary_mask(probs: np.ndarray, threshold: float=-1) -> np.ndarray:\n    """"""\n    Computes the binary mask of the detected Page from the probabilities outputed by network\n    :param probs: array with values in range [0, 1]\n    :param threshold: threshold between [0 and 1], if negative Otsu\'s adaptive threshold will be used\n    :return: binary mask\n    """"""\n\n    mask = binarization.thresholding(probs, threshold)\n    mask = binarization.cleaning_binary(mask, kernel_size=5)\n    return mask\n\n\ndef format_quad_to_string(quad):\n    """"""\n    Formats the corner points into a string.\n    :param quad: coordinates of the quadrilateral\n    :return:\n    """"""\n    s = \'\'\n    for corner in quad:\n        s += \'{},{},\'.format(corner[0], corner[1])\n    return s[:-1]\n\n\nif __name__ == \'__main__\':\n\n    # If the model has been trained load the model, otherwise use the given model\n    model_dir = \'demo/page_model/export\'\n    if not os.path.exists(model_dir):\n        model_dir = \'demo/model/\'\n\n    input_files = glob(\'demo/pages/test_a1/images/*\')\n\n    output_dir = \'demo/processed_images\'\n    os.makedirs(output_dir, exist_ok=True)\n    # PAGE XML format output\n    output_pagexml_dir = os.path.join(output_dir, PAGE_XML_DIR)\n    os.makedirs(output_pagexml_dir, exist_ok=True)\n\n    # Store coordinates of page in a .txt file\n    txt_coordinates = \'\'\n\n    with tf.Session():  # Start a tensorflow session\n        # Load the model\n        m = LoadedModel(model_dir, predict_mode=\'filename\')\n\n        for filename in tqdm(input_files, desc=\'Processed files\'):\n            # For each image, predict each pixel\'s label\n            prediction_outputs = m.predict(filename)\n            probs = prediction_outputs[\'probs\'][0]\n            original_shape = prediction_outputs[\'original_shape\']\n            probs = probs[:, :, 1]  # Take only class \'1\' (class 0 is the background, class 1 is the page)\n            probs = probs / np.max(probs)  # Normalize to be in [0, 1]\n\n            # Binarize the predictions\n            page_bin = page_make_binary_mask(probs)\n\n            # Upscale to have full resolution image (cv2 uses (w,h) and not (h,w) for giving shapes)\n            bin_upscaled = cv2.resize(page_bin.astype(np.uint8, copy=False),\n                                      tuple(original_shape[::-1]), interpolation=cv2.INTER_NEAREST)\n\n            # Find quadrilateral enclosing the page\n            pred_page_coords = boxes_detection.find_boxes(bin_upscaled.astype(np.uint8, copy=False),\n                                                          mode=\'min_rectangle\', min_area=0.2, n_max_boxes=1)\n\n            # Draw page box on original image and export it. Add also box coordinates to the txt file\n            original_img = imread(filename, pilmode=\'RGB\')\n            if pred_page_coords is not None:\n                cv2.polylines(original_img, [pred_page_coords[:, None, :]], True, (0, 0, 255), thickness=5)\n                # Write corners points into a .txt file\n                txt_coordinates += \'{},{}\\n\'.format(filename, format_quad_to_string(pred_page_coords))\n\n                # Create page region and XML file\n                page_border = PAGE.Border(coords=PAGE.Point.cv2_to_point_list(pred_page_coords[:, None, :]))\n            else:\n                print(\'No box found in {}\'.format(filename))\n                page_border = PAGE.Border()\n\n            basename = os.path.basename(filename).split(\'.\')[0]\n            imsave(os.path.join(output_dir, \'{}_boxes.jpg\'.format(basename)), original_img)\n\n            page_xml = PAGE.Page(image_filename=filename, image_width=original_shape[1], image_height=original_shape[0],\n                                 page_border=page_border)\n            xml_filename = os.path.join(output_pagexml_dir, \'{}.xml\'.format(basename))\n            page_xml.write_to_file(xml_filename, creator_name=\'PageExtractor\')\n\n    # Save txt file\n    with open(os.path.join(output_dir, \'pages.txt\'), \'w\') as f:\n        f.write(txt_coordinates)\n'"
setup.py,0,"b""#!/usr/bin/env python\nfrom setuptools import setup, find_packages\n\nsetup(name='dh_segment',\n      version='0.6.0',\n      license='GPL',\n      url='https://github.com/dhlab-epfl/dhSegment',\n      description='Generic framework for historical document processing',\n      packages=find_packages(),\n      project_urls={\n          'Paper': 'https://arxiv.org/abs/1804.10371',\n          'Source Code': 'https://github.com/dhlab-epfl/dhSegment'\n      },\n      install_requires=[\n        'imageio>=2.5',\n        'pandas>=0.24.2',\n        'shapely>=1.6.4',\n        'scikit-learn>=0.20.3',\n        'scikit-image>=0.15.0',\n        'opencv-python>=4.0.1',\n        'tqdm>=4.31.1',\n        'sacred==0.7.4',  # 0.7.5 causes an error\n        'requests>=2.21.0',\n        'click>=7.0'\n      ],\n      extras_require={\n          'doc': [\n              'sphinx',\n              'sphinx-autodoc-typehints',\n              'sphinx-rtd-theme',\n              'sphinxcontrib-bibtex',\n              'sphinxcontrib-websupport'\n          ],\n      },\n      zip_safe=False)\n"""
train.py,7,"b'import os\nimport tensorflow as tf\n# Tensorflow logging level\nfrom logging import WARNING  # import  DEBUG, INFO, ERROR for more/less verbosity\n\ntf.logging.set_verbosity(WARNING)\nfrom dh_segment import estimator_fn, utils\nfrom dh_segment.io import input\nimport json\nfrom glob import glob\nimport numpy as np\n\ntry:\n    import better_exceptions\nexcept ImportError:\n    print(\'/!\\ W -- Not able to import package better_exceptions\')\n    pass\nfrom tqdm import trange\nfrom sacred import Experiment\nimport pandas as pd\n\nex = Experiment(\'dhSegment_experiment\')\n\n\n@ex.config\ndef default_config():\n    train_data = None  # Directory with training data\n    eval_data = None  # Directory with validation data\n    model_output_dir = None  # Directory to output tf model\n    restore_model = False  # Set to true to continue training\n    classes_file = None  # txt file with classes values (unused for REGRESSION)\n    gpu = \'\'  # GPU to be used for training\n    prediction_type = utils.PredictionType.CLASSIFICATION  # One of CLASSIFICATION, REGRESSION or MULTILABEL\n    pretrained_model_name = \'resnet50\'\n    model_params = utils.ModelParams(pretrained_model_name=pretrained_model_name).to_dict()  # Model parameters\n    training_params = utils.TrainingParams().to_dict()  # Training parameters\n    if prediction_type == utils.PredictionType.CLASSIFICATION:\n        assert classes_file is not None\n        model_params[\'n_classes\'] = utils.get_n_classes_from_file(classes_file)\n    elif prediction_type == utils.PredictionType.REGRESSION:\n        model_params[\'n_classes\'] = 1\n    elif prediction_type == utils.PredictionType.MULTILABEL:\n        assert classes_file is not None\n        model_params[\'n_classes\'] = utils.get_n_classes_from_file_multilabel(classes_file)\n\n\n@ex.automain\ndef run(train_data, eval_data, model_output_dir, gpu, training_params, _config):\n    # Create output directory\n    if not os.path.isdir(model_output_dir):\n        os.makedirs(model_output_dir)\n    else:\n        assert _config.get(\'restore_model\'), \\\n            \'{0} already exists, you cannot use it as output directory. \' \\\n            \'Set ""restore_model=True"" to continue training, or delete dir ""rm -r {0}""\'.format(model_output_dir)\n    # Save config\n    with open(os.path.join(model_output_dir, \'config.json\'), \'w\') as f:\n        json.dump(_config, f, indent=4, sort_keys=True)\n\n    # Create export directory for saved models\n    saved_model_dir = os.path.join(model_output_dir, \'export\')\n    if not os.path.isdir(saved_model_dir):\n        os.makedirs(saved_model_dir)\n\n    training_params = utils.TrainingParams.from_dict(training_params)\n\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.visible_device_list = str(gpu)\n    session_config.gpu_options.per_process_gpu_memory_fraction = 0.9\n    estimator_config = tf.estimator.RunConfig().replace(session_config=session_config,\n                                                        save_summary_steps=10,\n                                                        keep_checkpoint_max=1)\n    estimator = tf.estimator.Estimator(estimator_fn.model_fn, model_dir=model_output_dir,\n                                       params=_config, config=estimator_config)\n\n    def get_dirs_or_files(input_data):\n        if os.path.isdir(input_data):\n            image_input, labels_input = os.path.join(input_data, \'images\'), os.path.join(input_data, \'labels\')\n            # Check if training dir exists\n            assert os.path.isdir(image_input), ""{} is not a directory"".format(image_input)\n            assert os.path.isdir(labels_input), ""{} is not a directory"".format(labels_input)\n\n        elif os.path.isfile(input_data) and input_data.endswith(\'.csv\'):\n            image_input = input_data\n            labels_input = None\n        else:\n            raise TypeError(\'input_data {} is neither a directory nor a csv file\'.format(input_data))\n        return image_input, labels_input\n\n    train_input, train_labels_input = get_dirs_or_files(train_data)\n    if eval_data is not None:\n        eval_input, eval_labels_input = get_dirs_or_files(eval_data)\n\n    # Configure exporter\n    serving_input_fn = input.serving_input_filename(training_params.input_resized_size)\n    if eval_data is not None:\n        exporter = tf.estimator.BestExporter(serving_input_receiver_fn=serving_input_fn, exports_to_keep=2)\n    else:\n        exporter = tf.estimator.LatestExporter(name=\'SimpleExporter\', serving_input_receiver_fn=serving_input_fn,\n                                               exports_to_keep=5)\n\n    for i in trange(0, training_params.n_epochs, training_params.evaluate_every_epoch, desc=\'Evaluated epochs\'):\n        estimator.train(input.input_fn(train_input,\n                                       input_label_dir=train_labels_input,\n                                       num_epochs=training_params.evaluate_every_epoch,\n                                       batch_size=training_params.batch_size,\n                                       data_augmentation=training_params.data_augmentation,\n                                       make_patches=training_params.make_patches,\n                                       image_summaries=True,\n                                       params=_config,\n                                       num_threads=32))\n\n        if eval_data is not None:\n            eval_result = estimator.evaluate(input.input_fn(eval_input,\n                                                            input_label_dir=eval_labels_input,\n                                                            batch_size=1,\n                                                            data_augmentation=False,\n                                                            make_patches=False,\n                                                            image_summaries=False,\n                                                            params=_config,\n                                                            num_threads=32))\n        else:\n            eval_result = None\n\n        exporter.export(estimator, saved_model_dir, checkpoint_path=None, eval_result=eval_result,\n                        is_the_final_export=False)\n\n    # If export directory is empty, export a model anyway\n    if not os.listdir(saved_model_dir):\n        final_exporter = tf.estimator.FinalExporter(name=\'FinalExporter\', serving_input_receiver_fn=serving_input_fn)\n        final_exporter.export(estimator, saved_model_dir, checkpoint_path=None, eval_result=eval_result,\n                              is_the_final_export=True)\n'"
dh_segment/__init__.py,0,"b""# _MODEL = [\n#     'inference_vgg16',\n#     'inference_resnet_v1_50',\n#     'inference_u_net',\n#     'vgg_16_fn',\n#     'resnet_v1_50_fn'\n# ]\n#\n# _INPUT = [\n#     'input_fn',\n#     'serving_input_filename',\n#     'serving_input_image',\n#     'data_augmentation_fn',\n#     'rotate_crop',\n#     'resize_image',\n#     'load_and_resize_image',\n#     'extract_patches_fn',\n#     'local_entropy'\n# ]\n#\n# _ESTIMATOR = [\n#     'model_fn'\n# ]\n#\n# _LOADER = [\n#     'LoadedModel'\n# ]\n#\n# _UTILS = [\n#     'PredictionType',\n#     'VGG16ModelParams',\n#     'ResNetModelParams',\n#     'UNetModelParams',\n#     'ModelParams',\n#     'TrainingParams',\n#     'label_image_to_class',\n#     'class_to_label_image',\n#     'multilabel_image_to_class',\n#     'multiclass_to_label_image',\n#     'get_classes_color_from_file',\n#     'get_n_classes_from_file',\n#     'get_classes_color_from_file_multilabel',\n#     'get_n_classes_from_file_multilabel',\n#     '_get_image_shape_tensor',\n# ]\n#\n# __all__ = _MODEL + _INPUT + _ESTIMATOR + _LOADER + _UTILS\n#\n# from dh_segment.model.pretrained_models import *\n#\n# from dh_segment.network import *\n# from .estimator_fn import *\n# from .io import *\n# from .network import *\n# from .inference import *\n# from .utils import *"""
dh_segment/estimator_fn.py,88,"b'import tensorflow as tf\nfrom .utils import PredictionType, ModelParams, TrainingParams, \\\n    class_to_label_image, multiclass_to_label_image\nimport numpy as np\nfrom .network.model import inference_resnet_v1_50, inference_vgg16, inference_u_net\n\n\ndef model_fn(mode, features, labels, params):\n    model_params = ModelParams(**params[\'model_params\'])\n    training_params = TrainingParams.from_dict(params[\'training_params\'])\n    prediction_type = params[\'prediction_type\']\n    classes_file = params[\'classes_file\']\n\n    input_images = features[\'images\']\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        margin = training_params.training_margin\n        input_images = tf.pad(input_images, [[0, 0], [margin, margin], [margin, margin], [0, 0]],\n                              mode=\'SYMMETRIC\', name=\'mirror_padding\')\n\n    if model_params.pretrained_model_name == \'vgg16\':\n        network_output = inference_vgg16(input_images,\n                                         model_params,\n                                         model_params.n_classes,\n                                         use_batch_norm=model_params.batch_norm,\n                                         weight_decay=model_params.weight_decay,\n                                         is_training=(mode == tf.estimator.ModeKeys.TRAIN)\n                                         )\n        key_restore_model = \'vgg_16\'\n\n    elif model_params.pretrained_model_name == \'resnet50\':\n        network_output = inference_resnet_v1_50(input_images,\n                                                model_params,\n                                                model_params.n_classes,\n                                                use_batch_norm=model_params.batch_norm,\n                                                weight_decay=model_params.weight_decay,\n                                                is_training=(mode == tf.estimator.ModeKeys.TRAIN)\n                                                )\n        key_restore_model = \'resnet_v1_50\'\n    elif model_params.pretrained_model_name == \'unet\':\n        network_output = inference_u_net(input_images,\n                                         model_params,\n                                         model_params.n_classes,\n                                         use_batch_norm=model_params.batch_norm,\n                                         weight_decay=model_params.weight_decay,\n                                         is_training=(mode == tf.estimator.ModeKeys.TRAIN)\n                                         )\n        key_restore_model = None\n    else:\n        raise NotImplementedError\n\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        if key_restore_model is not None:\n            # Pretrained weights as initialization\n            pretrained_restorer = tf.train.Saver(var_list=[v for v in tf.global_variables()\n                                                           if key_restore_model in v.name])\n\n            def init_fn(scaffold, session):\n                pretrained_restorer.restore(session, model_params.pretrained_model_file)\n        else:\n            init_fn = None\n    else:\n        init_fn = None\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        margin = training_params.training_margin\n        # Crop padding\n        if margin > 0:\n            network_output = network_output[:, margin:-margin, margin:-margin, :]\n\n    # Prediction\n    # ----------\n    if prediction_type == PredictionType.CLASSIFICATION:\n        prediction_probs = tf.nn.softmax(network_output, name=\'softmax\')\n        prediction_labels = tf.argmax(network_output, axis=-1, name=\'label_preds\')\n        predictions = {\'probs\': prediction_probs, \'labels\': prediction_labels}\n    elif prediction_type == PredictionType.REGRESSION:\n        predictions = {\'output_values\': network_output}\n        prediction_labels = network_output\n    elif prediction_type == PredictionType.MULTILABEL:\n        with tf.name_scope(\'prediction_ops\'):\n            prediction_probs = tf.nn.sigmoid(network_output, name=\'sigmoid\')  # [B,H,W,C]\n            prediction_labels = tf.cast(tf.greater_equal(prediction_probs, 0.5, name=\'labels\'), tf.int32)  # [B,H,W,C]\n            predictions = {\'probs\': prediction_probs, \'labels\': prediction_labels}\n    else:\n        raise NotImplementedError\n\n    # Loss\n    # ----\n    if mode in [tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL]:\n        regularized_loss = tf.losses.get_regularization_loss()\n        if prediction_type == PredictionType.CLASSIFICATION:\n            onehot_labels = tf.one_hot(indices=labels, depth=model_params.n_classes)\n            with tf.name_scope(""loss""):\n                per_pixel_loss = tf.nn.softmax_cross_entropy_with_logits(logits=network_output,\n                                                                         labels=onehot_labels, name=\'per_pixel_loss\')\n                if training_params.focal_loss_gamma > 0.0:\n                    # Probability per pixel of getting the correct label\n                    probs_correct_label = tf.reduce_max(tf.multiply(prediction_probs, onehot_labels))\n                    modulation = tf.pow((1. - probs_correct_label), training_params.focal_loss_gamma)\n                    per_pixel_loss = tf.multiply(per_pixel_loss, modulation)\n\n                if training_params.weights_labels is not None:\n                    weight_mask = tf.reduce_sum(\n                        tf.constant(np.array(training_params.weights_labels, dtype=np.float32)[None, None, None]) *\n                        onehot_labels, axis=-1)\n                    per_pixel_loss = per_pixel_loss * weight_mask\n                if training_params.local_entropy_ratio > 0:\n                    assert \'weight_maps\' in features\n                    r = training_params.local_entropy_ratio\n                    per_pixel_loss = per_pixel_loss * ((1 - r) + r * features[\'weight_maps\'])\n\n        elif prediction_type == PredictionType.REGRESSION:\n            per_pixel_loss = tf.squared_difference(labels, network_output, name=\'per_pixel_loss\')\n        elif prediction_type == PredictionType.MULTILABEL:\n            with tf.name_scope(\'sigmoid_xentropy_loss\'):\n                labels_floats = tf.cast(labels, tf.float32)\n                per_pixel_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_floats,\n                                                                         logits=network_output, name=\'per_pixel_loss\')\n                if training_params.weights_labels is not None:\n                    weight_mask = tf.maximum(\n                        tf.reduce_max(tf.constant(\n                            np.array(training_params.weights_labels, dtype=np.float32)[None, None, None])\n                                      * labels_floats, axis=-1), 1.0)\n                    per_pixel_loss = per_pixel_loss * weight_mask[:, :, :, None]\n        else:\n            raise NotImplementedError\n\n        margin = training_params.training_margin\n        input_shapes = features[\'shapes\']\n        with tf.name_scope(\'Loss\'):\n            def _fn(_in):\n                output, shape = _in\n                return tf.reduce_mean(output[margin:shape[0] - margin, margin:shape[1] - margin])\n\n            per_img_loss = tf.map_fn(_fn, (per_pixel_loss, input_shapes), dtype=tf.float32)\n            loss = tf.reduce_mean(per_img_loss, name=\'loss\')\n\n        loss += regularized_loss\n    else:\n        loss, regularized_loss = None, None\n\n    # Train\n    # -----\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        # >> Stucks the training... Why ?\n        # ema = tf.train.ExponentialMovingAverage(0.9)\n        # tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, ema.apply([loss]))\n        # ema_loss = ema.average(loss)\n\n        if training_params.exponential_learning:\n            global_step = tf.train.get_or_create_global_step()\n            learning_rate = tf.train.exponential_decay(training_params.learning_rate, global_step, decay_steps=200,\n                                                       decay_rate=0.95, staircase=False)\n        else:\n            learning_rate = training_params.learning_rate\n        tf.summary.scalar(\'learning_rate\', learning_rate)\n        optimizer = tf.train.AdamOptimizer(learning_rate)\n        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n            train_op = optimizer.minimize(loss, global_step=tf.train.get_or_create_global_step())\n    else:\n        ema_loss, train_op = None, None\n\n    # Summaries\n    # ---------\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        with tf.name_scope(\'summaries\'):\n            tf.summary.scalar(\'losses/loss\', loss)\n            tf.summary.scalar(\'losses/loss_per_batch\', loss)\n            tf.summary.scalar(\'losses/regularized_loss\', regularized_loss)\n            if prediction_type == PredictionType.CLASSIFICATION:\n                tf.summary.image(\'output/prediction\',\n                                 tf.image.resize_images(class_to_label_image(prediction_labels, classes_file),\n                                                        tf.cast(tf.shape(network_output)[1:3] / 3, tf.int32)),\n                                 max_outputs=1)\n                if model_params.n_classes == 3:\n                    tf.summary.image(\'output/probs\',\n                                     tf.image.resize_images(prediction_probs[:, :, :, :],\n                                                            tf.cast(tf.shape(network_output)[1:3] / 3, tf.int32)),\n                                     max_outputs=1)\n                if model_params.n_classes == 2:\n                    tf.summary.image(\'output/probs\',\n                                     tf.image.resize_images(prediction_probs[:, :, :, 1:2],\n                                                            tf.cast(tf.shape(network_output)[1:3] / 3, tf.int32)),\n                                     max_outputs=1)\n            elif prediction_type == PredictionType.REGRESSION:\n                summary_img = tf.nn.relu(network_output)[:, :, :, 0:1]  # Put negative values to zero\n                tf.summary.image(\'output/prediction\', summary_img, max_outputs=1)\n            elif prediction_type == PredictionType.MULTILABEL:\n                labels_visualization = tf.cast(prediction_labels, tf.int32)\n                labels_visualization = multiclass_to_label_image(labels_visualization, classes_file)\n                tf.summary.image(\'output/prediction_image\',\n                                 tf.image.resize_images(labels_visualization,\n                                                        tf.cast(tf.shape(labels_visualization)[1:3] / 3, tf.int32)),\n                                 max_outputs=1)\n                class_dim = prediction_probs.get_shape().as_list()[-1]\n                for c in range(0, class_dim):\n                    tf.summary.image(\'output/prediction_probs_{}\'.format(c),\n                                     tf.image.resize_images(prediction_probs[:, :, :, c:c + 1],\n                                                            tf.cast(tf.shape(network_output)[1:3] / 3, tf.int32)),\n                                     max_outputs=1)\n\n                    # beta = tf.get_default_graph().get_tensor_by_name(\'upsampling/deconv_5/conv5/batch_norm/beta/read:0\')\n                    # tf.summary.histogram(\'Beta\', beta)\n\n    # Evaluation\n    # ----------\n    if mode == tf.estimator.ModeKeys.EVAL:\n        if prediction_type == PredictionType.CLASSIFICATION:\n            metrics = {\n                \'eval/accuracy\': tf.metrics.accuracy(labels, predictions=prediction_labels),\n                \'eval/mIOU\': tf.metrics.mean_iou(labels, prediction_labels, num_classes=model_params.n_classes,)\n                                                 # weights=tf.cast(training_params.weights_evaluation_miou, tf.float32))\n            }\n        elif prediction_type == PredictionType.REGRESSION:\n            metrics = {\'eval/accuracy\': tf.metrics.mean_squared_error(labels, predictions=prediction_labels)}\n        elif prediction_type == PredictionType.MULTILABEL:\n            metrics = {\'eval/MSE\': tf.metrics.mean_squared_error(tf.cast(labels, tf.float32),\n                                                                 predictions=prediction_probs),\n                       \'eval/accuracy\': tf.metrics.accuracy(tf.cast(labels, tf.bool),\n                                                            predictions=tf.cast(prediction_labels, tf.bool)),\n                       \'eval/mIOU\': tf.metrics.mean_iou(labels, prediction_labels, num_classes=model_params.n_classes)\n                                                        # weights=training_params.weights_evaluation_miou)\n                       }\n    else:\n        metrics = None\n\n    # Export\n    # ------\n    if mode == tf.estimator.ModeKeys.PREDICT:\n\n        export_outputs = dict()\n\n        if \'original_shape\' in features.keys():\n            with tf.name_scope(\'ResizeOutput\'):\n                resized_predictions = dict()\n                # Resize all the elements in predictions\n                for k, v in predictions.items():\n                    # Labels is rank-3 so we need to be careful in using tf.image.resize_images\n                    assert isinstance(v, tf.Tensor)\n                    v2 = v if len(v.get_shape()) == 4 else v[:, :, :, None]\n                    v2 = tf.image.resize_images(v2, features[\'original_shape\'],\n                                                method=tf.image.ResizeMethod.BILINEAR if v.dtype == tf.float32\n                                                else tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n                    v2 = v2 if len(v.get_shape()) == 4 else v2[:, :, :, 0]\n                    resized_predictions[k] = v2\n                export_outputs[\'resized_output\'] = tf.estimator.export.PredictOutput(resized_predictions)\n\n            predictions[\'original_shape\'] = features[\'original_shape\']\n\n        export_outputs[\'output\'] = tf.estimator.export.PredictOutput(predictions)\n\n        export_outputs[tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY] = export_outputs[\'output\']\n    else:\n        export_outputs = None\n\n    return tf.estimator.EstimatorSpec(mode,\n                                      predictions=predictions,\n                                      loss=loss,\n                                      train_op=train_op,\n                                      eval_metric_ops=metrics,\n                                      export_outputs=export_outputs,\n                                      scaffold=tf.train.Scaffold(init_fn=init_fn)\n                                      )\n'"
doc/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(\'..\'))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'dhSegment\'\ncopyright = \'2018, Digital Humanities Lab - EPFL\'\nauthor = \'Sofia ARES OLIVEIRA, Benoit SEGUIN\'\n\n# The short X.Y version\nversion = \'\'\n# The full version, including alpha/beta/rc tags\nrelease = \'\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.autosummary\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.githubpages\',\n    \'sphinxcontrib.bibtex\',  # for bibtex\n    \'sphinx_autodoc_typehints\',  # for typing\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path .\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'  # alabaster, haiku, nature, pyramid, agogo, bizstyle, sphinx_rtd_theme\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'dhsegmentdoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'dhsegment.tex\', \'dhsegment Documentation\',\n     author, \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'dhsegment\', \'dhsegment Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'dhsegment\', \'dhsegment Documentation\',\n     author, \'dhsegment\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n# -- Extension configuration -------------------------------------------------\n\nautodoc_mock_imports = [\n    # \'numpy\',\n    \'scipy\',\n    \'tensorflow\',\n    \'pandas\',\n    \'sklearn\',\n    \'skimage\',\n    \'shapely\',\n    \'typing\',\n    \'cv2\',\n    \'tqdm\',\n    \'imageio\',\n    \'PIL\'\n]\n'"
exps/__init__.py,0,b''
exps/commonutils.py,0,"b'#!/usr/bin/env python\n__author__ = ""solivr""\n__license__ = ""GPL""\n\nimport os\nfrom tqdm import tqdm\nimport urllib\nimport zipfile\nimport numpy as np\nimport cv2\nfrom imageio import imsave\n\nRANDOM_SEED = 0\nnp.random.seed(RANDOM_SEED)\n\nCBAD_TRAIN_COMPLEX_FOLDER = \'cbad-icdar2017-train-complex-documents\'\nCBAD_TEST_COMPLEX_FOLDER = \'cbad-icdar2017-test-complex-documents\'\nCBAD_TRAIN_SIMPLE_FOLDER = \'cbad-icdar2017-train-simple-documents\'\nCBAD_TEST_SIMPLE_FOLDER = \'cbad-icdar2017-test-simple-documents\'\n\n\ndef get_page_filename(image_filename: str) -> str:\n    """"""\n    Given an path to a .jpg or .png file, get the corresponding .xml file.\n\n    :param image_filename: filename of the image\n    :return: the filename of the corresponding .xml file, raises exception if .xml file does not exist\n    """"""\n    page_filename = os.path.join(os.path.dirname(image_filename),\n                                 \'page\',\n                                 \'{}.xml\'.format(os.path.basename(image_filename)[:-4]))\n\n    if os.path.exists(page_filename):\n        return page_filename\n    else:\n        raise FileNotFoundError\n\n\ndef get_image_label_basename(image_filename: str) -> str:\n    """"""\n    Creates a new filename composed of the begining of the folder/collection (ex. EPFL, ABP) and the original filename\n\n    :param image_filename: path of the image filename\n    :return:\n    """"""\n    # Get acronym followed by name of file\n    directory, basename = os.path.split(image_filename)\n    acronym = directory.split(os.path.sep)[-1].split(\'_\')[0]\n    return \'{}_{}\'.format(acronym, basename.split(\'.\')[0])\n\n\ndef save_and_resize(img: np.array,\n                    filename: str,\n                    size=None,\n                    nearest: bool=False) -> None:\n    """"""\n    Resizes the image if necessary and saves it. The resizing will keep the image ratio\n\n    :param img: the image to resize and save (numpy array)\n    :param filename: filename of the saved image\n    :param size: size of the image after resizing (in pixels). The ratio of the original image will be kept\n    :param nearest: whether to use nearest interpolation method (default to False)\n    :return:\n    """"""\n    if size is not None:\n        h, w = img.shape[:2]\n        ratio = float(np.sqrt(size/(h*w)))\n        resized = cv2.resize(img, (int(w*ratio), int(h*ratio)),\n                             interpolation=cv2.INTER_NEAREST if nearest else cv2.INTER_LINEAR)\n        imsave(filename, resized)\n    else:\n        imsave(filename, img)\n\n\n# ------------------------------\n\n\ndef _progress_hook(t):\n    last_b = [0]\n\n    def update_to(b: int=1, bsize: int=1, tsize: int=None):\n        """"""\n        Adapted from: source unknown\n        :param b: Number of blocks transferred so far [default: 1].\n        :param bsize: Size of each block (in tqdm units) [default: 1].\n        :param tsize: Total size (in tqdm units). If [default: None] remains unchanged.\n        """"""\n        if tsize is not None:\n            t.total = tsize\n        t.update((b - last_b[0]) * bsize)\n        last_b[0] = b\n\n    return update_to\n\n\ndef cbad_download(output_dir: str):\n    """"""\n    Download BAD-READ dataset.\n\n    :param output_dir: folder where to download the data\n    :return:\n    """"""\n    os.makedirs(output_dir, exist_ok=True)\n    zip_filename = os.path.join(output_dir, \'cbad-icdar17.zip\')\n\n    with tqdm(unit=\'B\', unit_scale=True, unit_divisor=1024, miniters=1, desc=""Downloading cBAD-ICDAR17 dataset"") as t:\n        urllib.request.urlretrieve(\'https://zenodo.org/record/1491441/files/READ-ICDAR2017-cBAD-dataset-v4.zip\',\n                                   zip_filename, reporthook=_progress_hook(t))\n    print(\'cBAD-ICDAR2017 dataset downloaded successfully!\')\n    print(\'Extracting files ...\')\n    with zipfile.ZipFile(zip_filename, \'r\') as zip_ref:\n        zip_ref.extractall(output_dir)\n\n    # Renaming\n    os.rename(os.path.join(output_dir, \'Test-Baseline Competition - Complex Documents\'),\n              os.path.join(output_dir, CBAD_TEST_COMPLEX_FOLDER))\n    os.rename(os.path.join(output_dir, \'Test-Baseline Competition - Simple Documents\'),\n              os.path.join(output_dir, CBAD_TEST_SIMPLE_FOLDER))\n    os.rename(os.path.join(output_dir, \'Train-Baseline Competition - Complex Documents\'),\n              os.path.join(output_dir, CBAD_TRAIN_COMPLEX_FOLDER))\n    os.rename(os.path.join(output_dir, \'Train-Baseline Competition - Simple Documents\'),\n              os.path.join(output_dir, CBAD_TRAIN_SIMPLE_FOLDER))\n\n    os.remove(zip_filename)\n    print(\'Files extracted and renamed in {}\'.format(output_dir))\n'"
pretrained_models/download_resnet_pretrained_model.py,0,"b'#!/usr/bin/env python\n\nimport urllib.request\nimport tarfile\nimport os\nfrom tqdm import tqdm\n\n\ndef progress_hook(t):\n    last_b = [0]\n\n    def update_to(b=1, bsize=1, tsize=None):\n        """"""\n        b  : int, optional\n            Number of blocks transferred so far [default: 1].\n        bsize  : int, optional\n            Size of each block (in tqdm units) [default: 1].\n        tsize  : int, optional\n            Total size (in tqdm units). If [default: None] remains unchanged.\n        """"""\n        if tsize is not None:\n            t.total = tsize\n        t.update((b - last_b[0]) * bsize)\n        last_b[0] = b\n\n    return update_to\n\n\nif __name__ == \'__main__\':\n    tar_filename = \'resnet_v1_50.tar.gz\'\n    with tqdm(unit=\'B\', unit_scale=True, unit_divisor=1024, miniters=1,\n              desc=""Downloading pre-trained weights"") as t:\n        urllib.request.urlretrieve(\'http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz\', tar_filename,\n                                   reporthook=progress_hook(t))\n    tar = tarfile.open(tar_filename)\n    tar.extractall()\n    tar.close()\n    print(\'Resnet pre-trained weights downloaded!\')\n    os.remove(tar_filename)\n'"
pretrained_models/download_vgg_pretrained_model.py,0,"b'#!/usr/bin/env python\n\nimport urllib.request\nimport tarfile\nimport os\nfrom tqdm import tqdm\n\n\ndef progress_hook(t):\n    last_b = [0]\n\n    def update_to(b=1, bsize=1, tsize=None):\n        """"""\n        b  : int, optional\n            Number of blocks transferred so far [default: 1].\n        bsize  : int, optional\n            Size of each block (in tqdm units) [default: 1].\n        tsize  : int, optional\n            Total size (in tqdm units). If [default: None] remains unchanged.\n        """"""\n        if tsize is not None:\n            t.total = tsize\n        t.update((b - last_b[0]) * bsize)\n        last_b[0] = b\n\n    return update_to\n\n\nif __name__ == \'__main__\':\n    tar_filename = \'vgg_16.tar.gz\'\n    with tqdm(unit=\'B\', unit_scale=True, unit_divisor=1024, miniters=1,\n              desc=""Downloading pre-trained weights"") as t:\n        urllib.request.urlretrieve(\'http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz\', tar_filename,\n                                   reporthook=progress_hook(t))\n    tar = tarfile.open(tar_filename)\n    tar.extractall()\n    tar.close()\n    print(\'VGG-16 pre-trained weights downloaded!\')\n    os.remove(tar_filename)\n'"
dh_segment/inference/__init__.py,0,"b'r""""""\nThe :mod:`dh_segment.inference` module implements the function related to the usage of a dhSegment model,\nfor instance to use a trained model to inference on new data.\n\nLoading a model\n---------------\n\n.. autosummary::\n    LoadedModel\n\n\n-----\n""""""\n\n__all__ = [\'LoadedModel\']\n\nfrom .loader import *'"
dh_segment/inference/loader.py,3,"b'import tensorflow as tf\nimport os\nfrom threading import Semaphore\nimport numpy as np\nimport tempfile\nfrom imageio import imsave, imread\n\n_original_shape_key = \'original_shape\'\n\n\nclass LoadedModel:\n    """"""\n    Loads an exported dhSegment model\n\n    :param model_base_dir: the model directory i.e. containing `saved_model.{pb|pbtxt}`. If not, it is assumed to \\\n    be a TF exporter directory, and the latest export directory will be automatically selected.\n    :param predict_mode: defines the input/output format of the prediction output (see `.predict()`)\n    :param num_parallel_predictions: limits the number of conccurent calls of `predict` to avoid Out-Of-Memory \\\n    issues if predicting on GPU\n    """"""\n\n    def __init__(self, model_base_dir, predict_mode=\'filename\', num_parallel_predictions=2):\n        if os.path.exists(os.path.join(model_base_dir, \'saved_model.pbtxt\')) or \\\n                os.path.exists(os.path.join(model_base_dir, \'saved_model.pb\')):\n            model_dir = model_base_dir\n        else:\n            possible_dirs = os.listdir(model_base_dir)\n            model_dir = os.path.join(model_base_dir, max(possible_dirs))  # Take latest export\n        print(""Loading {}"".format(model_dir))\n\n        if predict_mode == \'filename\':\n            input_dict_key = \'filename\'\n            signature_def_key = \'serving_default\'\n        elif predict_mode == \'filename_original_shape\':\n            input_dict_key = \'filename\'\n            signature_def_key = \'resized_output\'\n        elif predict_mode == \'image\':\n            input_dict_key = \'image\'\n            signature_def_key = \'from_image:serving_default\'\n        elif predict_mode == \'image_original_shape\':\n            input_dict_key = \'image\'\n            signature_def_key = \'from_image:resized_output\'\n        elif predict_mode == \'resized_images\':\n            input_dict_key = \'resized_images\'\n            signature_def_key = \'from_resized_images:serving_default\'\n        else:\n            raise NotImplementedError\n        self.predict_mode = predict_mode\n\n        self.sess = tf.get_default_session()\n        loaded_model = tf.saved_model.loader.load(self.sess, [\'serve\'], model_dir)\n        assert \'serving_default\' in list(loaded_model.signature_def)\n\n        input_dict, output_dict = _signature_def_to_tensors(loaded_model.signature_def[signature_def_key])\n        assert input_dict_key in input_dict.keys(), ""{} not present in input_keys, "" \\\n                                                    ""possible values: {}"".format(input_dict_key, input_dict.keys())\n        self._input_tensor = input_dict[input_dict_key]\n        self._output_dict = output_dict\n        if predict_mode == \'resized_images\':\n            # This node is not defined in this specific run-mode as there is no original image\n            del self._output_dict[\'original_shape\']\n        self.sema = Semaphore(num_parallel_predictions)\n\n    def predict(self, input_tensor, prediction_key=None):\n        """"""\n        Performs the prediction from the loaded model according to the prediction mode. \\n\n        Prediction modes:\n\n        +-----------------------------+-----------------------------------------------+--------------------------------------+---------------------------------------------------------------------------------------------------+\n        | `prediction_mode`           | `input_tensor`                                | Output prediction dictionnary        | Comment                                                                                           |\n        +=============================+===============================================+======================================+===================================================================================================+\n        | `filename`                  | Single filename string                        | `labels`, `probs`, `original_shape`  | Loads the image, resizes it, and predicts                                                         |\n        +-----------------------------+-----------------------------------------------+--------------------------------------+---------------------------------------------------------------------------------------------------+\n        | `filename_original_shape`   | Single filename string                        | `labels`, `probs`                    | Loads the image, resizes it, predicts and scale the output to the original resolution of the file |\n        +-----------------------------+-----------------------------------------------+--------------------------------------+---------------------------------------------------------------------------------------------------+\n        | `image`                     | Single input image [1,H,W,3] float32 (0..255) | `labels`, `probs`, `original_shape`  | Resizes the image, and predicts                                                                   |\n        +-----------------------------+-----------------------------------------------+--------------------------------------+---------------------------------------------------------------------------------------------------+\n        | `image_original_shape`      | Single input image [1,H,W,3] float32 (0..255) | `labels`, `probs`                    | Resizes the image, predicts, and scale the output to the original resolution of the input         |\n        +-----------------------------+-----------------------------------------------+--------------------------------------+---------------------------------------------------------------------------------------------------+\n        | `image_resized`             | Single input image [1,H,W,3] float32 (0..255) | `labels`, `probs`                    | Predicts from the image input directly                                                            |\n        +-----------------------------+-----------------------------------------------+--------------------------------------+---------------------------------------------------------------------------------------------------+\n\n        :param input_tensor: a single input whose format should match the prediction mode\n        :param prediction_key: if not `None`, will returns the value of the corresponding key of the output dictionnary \\\n        instead of the full dictionnary\n        :return: the prediction output\n        """"""\n        with self.sema:\n            if prediction_key:\n                desired_output = self._output_dict[prediction_key]\n            else:\n                desired_output = self._output_dict\n            return self.sess.run(desired_output, feed_dict={self._input_tensor: input_tensor})\n\n    def predict_with_tiles(self, filename: str, resized_size: int=None, tile_size: int=500,\n                           min_overlap: float=0.2, linear_interpolation: bool=True):\n\n        # TODO this part should only happen if self.predict_mode == \'resized_images\'\n\n        if resized_size is None or resized_size < 0:\n            image_np = imread(filename)\n            h, w = image_np.shape[:2]\n            batch_size = 1\n        else:\n            raise NotImplementedError\n        assert h > tile_size, w > tile_size\n        # Get x and y coordinates of beginning of tiles and compute prediction for each tile\n        y_step = np.ceil((h - tile_size) / (tile_size * (1 - min_overlap)))\n        x_step = np.ceil((w - tile_size) / (tile_size * (1 - min_overlap)))\n        y_pos = np.round(np.arange(y_step + 1) / y_step * (h - tile_size)).astype(np.int32)\n        x_pos = np.round(np.arange(x_step + 1) / x_step * (w - tile_size)).astype(np.int32)\n\n        all_outputs = list()\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            for i, y in enumerate(y_pos):\n                inside_list = list()\n                for j, x in enumerate(x_pos):\n                    filename_tile = os.path.join(tmpdirname, \'tile{}{}.png\'.format(i, j))\n                    imsave(filename_tile, image_np[y:y + tile_size, x:x + tile_size])\n                    inside_list.append(self.predict(filename_tile))#, prediction_key=\'probs\'))\n                all_outputs.append(inside_list)\n\n        def _merge_x(full_output, assigned_up_to, new_input, begin_position):\n            assert full_output.shape[1] == new_input.shape[1], \\\n                ""Shape full output is {}, but shape new_input is {}"".format(full_output.shape[1], new_input.shape[1])\n            overlap_size = assigned_up_to - begin_position\n            normal_part_size = new_input.shape[2] - overlap_size\n            assert normal_part_size > 0\n            full_output[:, :, assigned_up_to:assigned_up_to + normal_part_size] = new_input[:, :, overlap_size:]\n            if overlap_size > 0:\n                weights = np.arange(0, overlap_size) / overlap_size\n                full_output[:, :, begin_position:assigned_up_to] = (1 - weights)[:, None] * full_output[:, :,\n                                                                                            begin_position:assigned_up_to] + \\\n                                                                   weights[:, None] * new_input[:, :, :overlap_size]\n\n        def _merge_y(full_output, assigned_up_to, new_input, begin_position):\n            assert full_output.shape[2] == new_input.shape[2]\n            overlap_size = assigned_up_to - begin_position\n            normal_part_size = new_input.shape[1] - overlap_size\n            assert normal_part_size > 0\n            full_output[:, assigned_up_to:assigned_up_to + normal_part_size] = new_input[:, overlap_size:]\n            if overlap_size > 0:\n                weights = np.arange(0, overlap_size) / overlap_size\n                full_output[:, begin_position:assigned_up_to] = (1 - weights)[:, None, None] * full_output[:,\n                                                                                               begin_position:assigned_up_to] + \\\n                                                                weights[:, None, None] * new_input[:, :overlap_size]\n\n        result = {k: np.empty([batch_size, h, w] + list(v.shape[3:]), v.dtype) for k, v in all_outputs[0][0].items()\n                  if k != _original_shape_key}  # do not try to merge \'original_shape\' content...\n        if linear_interpolation:\n            for k in result.keys():\n                assigned_up_to_y = 0\n                for y, y_outputs in zip(y_pos, all_outputs):\n                    s = list(result[k].shape)\n                    tmp = np.zeros([batch_size, tile_size] + s[2:], result[k].dtype)\n                    assigned_up_to_x = 0\n                    for x, output in zip(x_pos, y_outputs):\n                        _merge_x(tmp, assigned_up_to_x, output[k], x)\n                        assigned_up_to_x = x + tile_size\n                    _merge_y(result[k], assigned_up_to_y, tmp, y)\n                    assigned_up_to_y = y + tile_size\n        else:\n            for k in result.keys():\n                for y, y_outputs in zip(y_pos, all_outputs):\n                    for x, output in zip(x_pos, y_outputs):\n                        result[k][:, y:y + tile_size, x:x + tile_size] = output[k]\n\n        result[_original_shape_key] = np.array([h, w], np.uint)\n        return result\n\n\ndef _signature_def_to_tensors(signature_def):\n    g = tf.get_default_graph()\n    return {k: g.get_tensor_by_name(v.name) for k, v in signature_def.inputs.items()}, \\\n           {k: g.get_tensor_by_name(v.name) for k, v in signature_def.outputs.items()}\n'"
dh_segment/io/PAGE.py,0,"b'from xml.etree import ElementTree as ET\nfrom typing import List, Optional, Union, Tuple\nimport numpy as np\nimport datetime\nimport cv2\nimport os\nimport json\nfrom uuid import uuid4\nfrom shapely.geometry import Polygon\nfrom abc import ABC\nimport re\n\n# https://docs.python.org/3.5/library/xml.etree.elementtree.html#parsing-xml-with-namespaces\n_use_https = False\n_ns = {\'p\': \'http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15\'}\n_attribs = {\'xmlns:xsi\': ""http://www.w3.org/2001/XMLSchema-instance"",\n            \'xsi:schemaLocation\': ""http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15 ""\n                                  ""http://schema.primaresearch.org/PAGE/gts/pagecontent/2013-07-15/pagecontent.xsd""}\n\n\ndef _try_to_int(d: Optional[Union[str, int]]) -> Optional[int]:\n    if isinstance(d, (str, np.int32, np.int64)):\n        return int(d)\n    else:\n        return d\n\n\ndef _get_text_equiv(e: ET.Element) -> str:\n    tmp = e.find(\'p:TextEquiv\', _ns)\n    if tmp is None:\n        return \'\'\n    tmp = tmp.find(\'p:Unicode\', _ns)\n    if tmp is None:\n        return \'\'\n    return tmp.text\n\n\ndef _encode_bool(value):\n    if value:\n        return \'true\'\n    return \'false\'\n\ndef _decode_bool(value):\n    if value.lower() == \'true\':\n        return True\n    return False\n\n\nclass Point:\n    """"""Point (x,y) class.\n\n    :ivar y: vertical coordinate\n    :ivar x: horizontal coordinate\n\n    """"""\n\n    def __init__(self,\n                 y: int,\n                 x: int):\n        self.y = y\n        self.x = x\n\n    @classmethod\n    def list_from_xml(cls, etree_elem: ET.Element) -> List[\'Point\']:\n        """"""Converts a PAGEXML-formatted set of coordinates to a list of `Point`\n\n        :param etree_elem: etree XML element containing a set of coordinates\n        :return: a list of coordinates as `Point`\n        """"""\n        if etree_elem is None:\n            # print(\'warning, trying to construct list of points from None, defaulting to []\')\n            return []\n        if etree_elem.attrib[\'points\'] == """":\n            # print(\'warning, trying to construct list of points from empty string, defaulting to []\')\n            return []\n        t = etree_elem.attrib[\'points\']\n        result = []\n        for p in t.split(\' \'):\n            values = p.split(\',\')\n            assert len(values) == 2\n            x, y = int(float(values[0])), int(float(values[1]))\n            result.append(Point(y, x))\n        return result\n\n    @classmethod\n    def list_to_cv2poly(cls, list_points: List[\'Point\']) -> np.ndarray:\n        """"""Converts a list of `Point` to opencv format set of coordinates\n\n        :param list_points: set of coordinates\n        :return: opencv-formatted set of points, shape (N,1,2)\n        """"""\n        return np.array([(p.x, p.y) for p in list_points], dtype=np.int32).reshape([-1, 1, 2])\n\n    @classmethod\n    def cv2_to_point_list(cls, cv2_array: np.ndarray) -> List[\'Point\']:\n        """"""Converts an opencv-formatted set of coordinates to a list of `Point`\n\n        :param cv2_array: opencv-formatted set of coordinates, shape (N,1,2)\n        :return: list of `Point`\n        """"""\n        return [Point(p[0, 1], p[0, 0]) for p in cv2_array]\n\n    @classmethod\n    def list_point_to_string(cls, list_points: List[\'Point\']) -> str:\n        """"""Converts a list of `Point` to a string \'x,y\'\n\n        :param list_points: list of coordinates with `Point` format\n        :return: a string with the coordinates\n        """"""\n        return \' \'.join([\'{},{}\'.format(int(p.x), int(p.y)) for p in list_points])\n\n    @classmethod\n    def array_to_list(cls, array: np.ndarray) -> list:\n        """"""Converts an `np.array` to a list of coordinates\n\n        :param array: an array of coordinates. Must be of shape (N, 2)\n        :return: list of coordinates, shape (N,2)\n        """"""\n        return [list(pt) for pt in array]\n\n    @classmethod\n    def array_to_point(cls, array: np.ndarray) -> list:\n        """"""Converts an `np.array` to a list of `Point`\n\n        :param array: an array of coordinates. Must be of shape (N, 2)\n        :return: list of `Point`\n        """"""\n        return cls.list_to_point(list(array))\n\n    @classmethod\n    def list_to_point(cls, list_coords: list) -> List[\'Point\']:\n        """"""Converts a list of coordinates to a list of `Point`\n\n        :param list_coords: list of coordinates, shape (N, 2)\n        :return: list of `Point`\n        """"""\n        return [cls(coord[1], coord[0]) for coord in list_coords if len(list_coords) > 0]\n\n    @classmethod\n    def point_to_list(cls, points: List[\'Point\']) -> list:\n        """"""Converts a list of `Point` to a list of coordinates\n\n        :param points: list of Points\n        :return: list of shape (N,2)\n        """"""\n        return [[pt.x, pt.y] for pt in points]\n\n    def to_dict(self):\n        # TODO: this does not return a dictionary...\n        return [int(self.x), int(self.y)]\n\n\nclass Text:\n    """"""Text entity produced by a transcription system.\n\n    :ivar text_equiv: the transcription of the text\n    :ivar alternatives: alternative transcriptions\n    :ivar score: the confidence of the transcription output by the transcription system\n    """"""\n\n    def __init__(self,\n                 text_equiv: str = None,\n                 alternatives: List[str] = None,\n                 score: float = None):\n        self.text_equiv = text_equiv  # if text_equiv is not None else \'\'\n        self.alternatives = alternatives  # if alternatives is not None else []\n        self.score = score  # if score is not None else \'\'\n\n    def to_dict(self) -> dict:\n        return vars(self)\n\n\nclass BaseElement(ABC):\n    """"""\n    Base page element class. (Abstract)\n    """"""\n    tag = None\n\n    @classmethod\n    def full_tag(cls) -> str:\n        return \'{{{}}}{}\'.format(_ns[\'p\'], cls.tag)\n\n    @classmethod\n    def check_tag(cls, tag):\n        assert tag == cls.full_tag(), \'Invalid tag, expected {} got {}\'.format(cls.full_tag(), tag)\n\n\nclass Region(BaseElement):\n    """"""\n    Region base class. (Abstract)\n    This is the superclass for all the extracted regions\n\n    :ivar id: identifier of the `Region`\n    :ivar coords: coordinates of the `Region`\n    :ivar custom_attribute: Any custom attribute that may be linked with the region\n        (usually this is added in PAGEXML files, not in JSON files)\n    """"""\n    tag = \'Region\'\n\n    def __init__(self,\n                 id: str = None,\n                 coords: List[Point] = None,\n                 custom_attribute: str = None):\n        self.coords = coords if coords is not None else []\n        self.id = id\n        self.custom_attribute = custom_attribute if custom_attribute is not None else \'\'\n\n    @classmethod\n    def from_xml(cls, etree_element: ET.Element) -> dict:\n        """"""Creates a dictionary from a XML structure in order to create the inherited objects\n\n        :param etree_element: a xml etree\n        :return: a dictionary with keys \'id\' and \'coords\'\n        """"""\n        return {\'id\': etree_element.attrib.get(\'id\'),\n                \'custom_attribute\': etree_element.attrib.get(\'custom\'),\n                \'coords\': Point.list_from_xml(etree_element.find(\'p:Coords\', _ns))}\n\n    def to_xml(self, name_element: str = None) -> ET.Element:\n        """"""Converts a `Region` object to a xml structure\n\n        :param name_element: name of the object (optional)\n        :return: a etree structure\n        """"""\n        et = ET.Element(name_element if name_element is not None else \'\')\n        et.set(\'id\', self.id if self.id is not None else \'\')\n        et.set(\'custom\', self.custom_attribute if self.custom_attribute is not None else \'\')\n        if not not self.coords:\n            coords = ET.SubElement(et, \'Coords\')\n            coords.set(\'points\', Point.list_point_to_string(self.coords))\n        return et\n\n    def to_dict(self, non_serializable_keys: List[str] = list()) -> dict:\n        """"""Converts a `Region` object to a dictionary.\n\n        :param non_serializable_keys: list of keys that can\'t be directly serialized and that need some\n                                      internal serialization\n        :return: a dictionary with the attributes of the object serialized\n        """"""\n        if \'coords\' in vars(self).keys() and \'coords\' not in non_serializable_keys:\n            non_serializable_keys += [\'coords\']\n        return json_serialize(vars(self), non_serializable_keys=non_serializable_keys)\n\n    @classmethod\n    def from_dict(cls, dictionary: dict) -> dict:\n        """"""From a seralized dictionary creates a dictionary of the atributes (non serialized)\n\n        :param dictionary: serialized dictionary\n        :return: non serialized dictionary\n        """"""\n        return {\'id\': dictionary.get(\'id\'),\n                \'custom_attribute\': dictionary.get(\'custom_attribute\'),\n                \'coords\': Point.list_to_point(dictionary.get(\'coords\'))\n                }\n\n\nclass TextLine(Region):\n    """"""Region corresponding to a text line.\n\n    :ivar id: identifier of the `TextLine`\n    :ivar coords: coordinates of the `Texline` line\n    :ivar baseline: coordinates of the `Texline` baseline\n    :ivar text: `Text` class containing the transcription of the `TextLine`\n    :ivar line_group_id: identifier of the line group the instance belongs to\n    :ivar column_group_id: identifier of the column group the instance belongs to\n    :ivar custom_attribute: Any custom attribute that may be linked with the region\n        (usually this is added in PAGEXML files, not in JSON files)\n    """"""\n    tag = \'TextLine\'\n\n    def __init__(self,\n                 id: str = None,\n                 coords: List[Point] = None,\n                 baseline: List[Point] = None,\n                 text: Text = None,\n                 line_group_id: str = None,\n                 column_group_id: str = None,\n                 custom_attribute: str = None):\n        super().__init__(id=id if id is not None else str(uuid4()), coords=coords, custom_attribute=custom_attribute)\n        self.baseline = baseline if baseline is not None else []\n        self.text = text if text is not None else Text()\n        self.line_group_id = line_group_id if line_group_id is not None else \'\'\n        self.column_group_id = column_group_id if column_group_id is not None else \'\'\n\n    @classmethod\n    def from_xml(cls, etree_element: ET.Element) -> \'TextLine\':\n        cls.check_tag(etree_element.tag)\n        return TextLine(\n            **super().from_xml(etree_element),\n            baseline=Point.list_from_xml(etree_element.find(\'p:Baseline\', _ns)),\n            text=Text(text_equiv=_get_text_equiv(etree_element))\n        )\n\n    @classmethod\n    def from_array(cls, cv2_coords: np.array = None, baseline_coords: np.array = None,  # cv2_coords shape [N, 1, 2]\n                   text_equiv: str = None, id: str = None):\n        return TextLine(\n            id=id,\n            coords=Point.cv2_to_point_list(cv2_coords) if cv2_coords is not None else [],\n            baseline=Point.cv2_to_point_list(baseline_coords) if baseline_coords is not None else [],\n            text=Text(text_equiv=text_equiv)\n        )\n\n    def to_xml(self, name_element=\'TextLine\') -> ET.Element:\n        line_et = super().to_xml(name_element=name_element)\n        if not not self.baseline:\n            line_baseline = ET.SubElement(line_et, \'Baseline\')\n            line_baseline.set(\'points\', Point.list_point_to_string(self.baseline))\n        line_text_equiv = ET.SubElement(line_et, \'TextEquiv\')\n        text_unicode = ET.SubElement(line_text_equiv, \'Unicode\')\n        if not not self.text.text_equiv:\n            text_unicode.text = self.text.text_equiv\n        return line_et\n\n    def scale_baseline_points(self, ratio: float):\n        """"""Scales the points of the baseline by a factor `ratio`.\n\n        :param ratio: factor to rescale the baseline coordinates\n        """"""\n        scaled_points = list()\n        for pt in self.baseline:\n            scaled_points.append(Point(int(pt.y * ratio[0]), int(pt.x * ratio[1])))\n\n        self.baseline = scaled_points\n\n    def to_dict(self, non_serializable_keys: List[str] = list()):\n        return super().to_dict(non_serializable_keys=[\'text\', \'baseline\'])\n\n    @classmethod\n    def from_dict(cls, dictionary: dict) -> \'TextLine\':\n        return cls(**super().from_dict(dictionary),\n                   baseline=Point.list_to_point(dictionary.get(\'baseline\')),\n                   text=Text(**dictionary.get(\'text\', dict())),\n                   line_group_id=dictionary.get(\'line_group_id\'),\n                   column_group_id=dictionary.get(\'column_group_id\')\n                   )\n\n\nclass GraphicRegion(Region):\n    """"""Region containing simple graphics. Company logos for example should be marked as graphic regions.\n\n    :ivar id: identifier of the `GraphicRegion`\n    :ivar coords: coordinates of the `GraphicRegion`\n    """"""\n    tag = \'GraphicRegion\'\n\n    def __init__(self,\n                 id: str = None,\n                 coords: List[Point] = None,\n                 custom_attribute: str = None):\n        super().__init__(id=id, coords=coords, custom_attribute=custom_attribute)\n\n    @classmethod\n    def from_xml(cls, e: ET.Element) -> \'GraphicRegion\':\n        cls.check_tag(e.tag)\n        return GraphicRegion(**super().from_xml(e))\n\n    def to_xml(self, name_element=\'GraphicRegion\') -> ET.Element:\n        graph_et = super().to_xml(name_element)\n\n        return graph_et\n\n    @classmethod\n    def from_dict(cls, dictionary: dict) -> \'GraphicRegion\':\n        return cls(**super().from_dict(dictionary))\n\n\nclass TextRegion(Region):\n    """"""Region containing text lines. It can represent a paragraph or a page for instance.\n\n    :ivar id: identifier of the `TextRegion`\n    :ivar coords: coordinates of the `TextRegion`\n    :ivar text_equiv: the resulting text of the `Text` contained in the `TextLines`\n    :ivar text_lines: a list of `TextLine` objects\n    :ivar region_type: the type of a TextRegion (can be any string). Example : header, paragraph, page-number...\n    :ivar custom_attribute: Any custom attribute that may be linked with the region\n        (usually this is added in PAGEXML files, not in JSON files)\n    """"""\n    tag = \'TextRegion\'\n\n    def __init__(self,\n                 id: str = None,\n                 coords: List[Point] = None,\n                 text_lines: List[TextLine] = None,\n                 text_equiv: str = \'\',\n                 region_type: str = None,\n                 custom_attribute: str = None):\n        super().__init__(id=id, coords=coords, custom_attribute=custom_attribute)\n        self.text_equiv = text_equiv if text_equiv is not None else \'\'\n        self.text_lines = text_lines if text_lines is not None else []\n        self.type = region_type if region_type is not None else \'\'\n\n    def sort_text_lines(self, top_to_bottom: bool = True) -> None:\n        """"""\n        Sorts ``TextLine`` from top to bottom according to their mean y coordinate (centroid)\n        \n        :param top_to_bottom: order lines from top to bottom of image, default=True\n        """"""\n        if top_to_bottom:\n            self.text_lines.sort(key=lambda line: np.mean([c.y for c in line.coords]))\n        else:\n            raise NotImplementedError\n\n    @classmethod\n    def from_xml(cls, e: ET.Element) -> \'TextRegion\':\n        cls.check_tag(e.tag)\n        return TextRegion(\n            **super().from_xml(e),\n            text_lines=[TextLine.from_xml(tl) for tl in e.findall(\'p:TextLine\', _ns)],\n            text_equiv=_get_text_equiv(e),\n            region_type=e.attrib.get(\'type\')\n        )\n\n    def to_xml(self, name_element=\'TextRegion\') -> ET.Element:\n        text_et = super().to_xml(name_element=name_element)\n        if self.type is not None and self.type != \'\':\n            text_et.set(\'type\', self.type)\n        for tl in self.text_lines:\n            text_et.append(tl.to_xml())\n        text_equiv = ET.SubElement(text_et, \'TextEquiv\')\n        text_unicode = ET.SubElement(text_equiv, \'Unicode\')\n        if not not self.text_equiv:\n            text_unicode.text = self.text_equiv\n        return text_et\n\n    def to_dict(self, non_serializable_keys: List[str] = list()):\n        return super().to_dict(non_serializable_keys=[\'text_lines\'])\n\n    @classmethod\n    def from_dict(cls, dictionary: dict) -> \'TextRegion\':\n        return cls(**super().from_dict(dictionary),\n                   text_lines=[TextLine.from_dict(tl) for tl in dictionary.get(\'text_lines\', list())],\n                   text_equiv=dictionary.get(\'text_equiv\'),\n                   region_type=dictionary.get(\'region_type\')\n                   )\n\n\nclass TableCell(Region):\n    """""" Table cell data\n    A table cell can contain text lines and spans one or more rows and columns.\n\n    :ivar id: identifier of the `TableCell`\n    :ivar coords: coordinates of the `TableCell`\n    :ivar text_lines: text lines that are contained\n    :ivar row_index: row number starting with row 0\n    :ivar col_index: column number starting with column 0\n    :ivar row_span: number of rows the cell spans\n    :ivar col_span: number of columns the cell spans\n    :ivar header: if the cell is a column or row header\n    :ivar embedded_text: if text is embedded in the table\n    """"""\n\n    tag = \'TableCell\'\n\n    def __init__(self,\n                 id: str = None,\n                 coords: List[Point] = None,\n                 text_lines: List[TextLine] = None,\n                 row_index: int = None,\n                 col_index: int = None,\n                 row_span: int = None,\n                 col_span: int = None,\n                 header: bool = None,\n                 embedded_text: bool = None,\n                 custom_attribute: str = None):\n        super().__init__(id=id, coords=coords, custom_attribute=custom_attribute)\n        self.text_lines = text_lines if text_lines is not None else []\n        self.row_index = row_index\n        self.col_index = col_index\n        self.row_span = row_span\n        self.col_span = col_span\n        self.header = header if header is not None else False\n        self.embedded_text = embedded_text if embedded_text is not None else False\n\n    def to_xml(self, name_element=\'TableCell\') -> ET.Element:\n        cell_et = super().to_xml(name_element=name_element)\n        if self.row_index is not None:\n            cell_et.set(\'rowIndex\', \'{}\'.format(self.row_index))\n        if self.col_index is not None:\n            cell_et.set(\'columnIndex\', \'{}\'.format(self.col_index))\n        if self.row_span is not None:\n            cell_et.set(\'rowSpan\', \'{}\'.format(self.row_span))\n        if self.col_span is not None:\n            cell_et.set(\'colSpan\', \'{}\'.format(self.col_span))\n        for tl in self.text_lines:\n            cell_et.append(tl.to_xml())\n        cell_et.set(\'header\', _encode_bool(self.header))\n        cell_et.set(\'embText\', _encode_bool(self.embedded_text))\n        return cell_et\n\n    @classmethod\n    def from_xml(cls, e: ET.Element) -> \'TableCell\':\n        cls.check_tag(e.tag)\n        return TableCell(\n            **super().from_xml(e),\n            row_index=int(e.attrib.get(\'rowIndex\')),\n            col_index=int(e.attrib.get(\'columnIndex\')),\n            row_span=int(e.attrib.get(\'rowSpan\')),\n            col_span=int(e.attrib.get(\'colSpan\')),\n            text_lines=[TextLine.from_xml(tl) for tl in e.findall(\'p:TextLine\', _ns)],\n            header=_decode_bool(e.attrib.get(\'header\')),\n            embedded_text=_decode_bool(e.attrib.get(\'embText\'))\n        )\n\n    def to_dict(self, non_serializable_keys: List[str] = list()):\n        return super().to_dict(non_serializable_keys=[\'text_lines\'])\n\n    @classmethod\n    def from_dict(cls, dictionary: dict) -> \'TableCell\':\n        return cls(**super().from_dict(dictionary),\n                   row_index=int(dictionary.get(\'rowIndex\')),\n                   col_index=int(dictionary.get(\'columnIndex\')),\n                   row_span=int(dictionary.get(\'rowSpan\')),\n                   col_span=int(dictionary.get(\'colSpan\')),\n                   text_lines=[TextLine.from_dict(tl) for tl in dictionary.get(\'text_lines\', list())],\n                   header=_decode_bool(dictionary.get(\'header\')),\n                   embedded_text=_decode_bool(dictionary.get(\'embedded_text\')))\n\n\nclass TableRegion(Region):\n    """"""\n    Tabular data in any form.\n    Tabular data is represented with a table region. Rows and columns may or may not have separator lines;\n    these lines are not separator regions.\n\n    :ivar id: identifier of the `TableRegion`\n    :ivar coords: coordinates of the `TableRegion`\n    :ivar cells: list of `TableCell`\n    :ivar rows: number of rows in the table\n    :ivar columns: number of columns in the table\n    """"""\n\n    tag = \'TableRegion\'\n\n    def __init__(self,\n                 id: str = None,\n                 coords: List[Point] = None,\n                 cells: List[TableCell] = None,\n                 rows: int = None,\n                 columns: int = None,\n                 custom_attribute: str = None):\n        super().__init__(id=id, coords=coords, custom_attribute=custom_attribute)\n        self.cells = cells if cells is not None else []\n        self.rows = rows\n        self.columns = columns\n\n    @classmethod\n    def from_xml(cls, e: ET.Element) -> \'TableRegion\':\n        cls.check_tag(e.tag)\n        return TableRegion(\n            **super().from_xml(e),\n            rows=int(e.attrib.get(\'rows\')),\n            columns=int(e.attrib.get(\'columns\')),\n            cells=[TableCell.from_xml(cell) for cell in e.findall(\'p:TableCell\', _ns)]\n        )\n\n    def to_xml(self, name_element=\'TableRegion\') -> ET.Element:\n        table_et = super().to_xml(name_element)\n        table_et.set(\'rows\', \'{}\'.format(self.rows) if self.rows is not None else \'0\')\n        table_et.set(\'columns\', \'{}\'.format(self.columns) if self.columns is not None else \'0\')\n        for cell in self.cells:\n            table_et.append(cell.to_xml())\n        return table_et\n\n    @classmethod\n    def from_dict(cls, dictionary: dict) -> \'TableRegion\':\n        return cls(**super().from_dict(dictionary),\n                   rows=int(dictionary.get(\'rows\')),\n                   columns=int(dictionary.get(\'columns\')),\n                   cells=[TableCell.from_dict(cell) for cell in dictionary.get(\'cells\', list())])\n\n    def to_dict(self, non_serializable_keys: List[str] = list()):\n        return super().to_dict(non_serializable_keys=[\'cells\'])\n\n\nclass SeparatorRegion(Region):\n    """"""\n    Lines separating columns or paragraphs.\n    Separators are lines that lie between columns and paragraphs and can be used to logically separate\n    different articles from each other.\n\n    :ivar id: identifier of the `SeparatorRegion`\n    :ivar coords: coordinates of the `SeparatorRegion`\n    """"""\n\n    tag = \'SeparatorRegion\'\n\n    def __init__(self,\n                 id: str,\n                 coords: List[Point] = None,\n                 custom_attribute: str = None):\n        super().__init__(id=id, coords=coords, custom_attribute=custom_attribute)\n\n    @classmethod\n    def from_xml(cls, e: ET.Element) -> \'SeparatorRegion\':\n        cls.check_tag(e.tag)\n        return SeparatorRegion(**super().from_xml(e))\n\n    def to_xml(self, name_element=\'SeparatorRegion\') -> ET.Element:\n        separator_et = super().to_xml(name_element)\n        return separator_et\n\n    @classmethod\n    def from_dict(cls, dictionary: dict) -> \'SeparatorRegion\':\n        return cls(**super().from_dict(dictionary))\n\n\nclass Border(BaseElement):\n    """"""\n    Region containing the page.\n    It is the border of the actual page of the document (if the scanned image contains parts not belonging to the page).\n\n    :ivar coords: coordinates of the `Border` region\n    """"""\n\n    tag = \'Border\'\n\n    def __init__(self,\n                 coords: List[Point] = None,\n                 id: str = None):\n        self.coords = coords if coords is not None else []\n\n    @classmethod\n    def from_xml(cls, e: ET.Element) -> \'Border\':\n        if e is None:\n            return None\n        cls.check_tag(e.tag)\n        return Border(\n            coords=Point.list_from_xml(e.find(\'p:Coords\', _ns))\n        )\n\n    def to_xml(self) -> ET.Element:\n        border_et = ET.Element(\'Border\')\n        if not not self.coords:\n            border_coords = ET.SubElement(border_et, \'Coords\')\n            border_coords.set(\'points\', Point.list_point_to_string(self.coords))\n        return border_et\n\n    @classmethod\n    def from_dict(cls, dictionary: dict) -> \'Border\':\n        return cls(coords=Point.list_to_point(dictionary.get(\'coords\')))\n\n    def to_dict(self, non_serializable_keys: List[str] = list()) -> dict:\n        if \'coords\' in vars(self).keys() and \'coords\' not in non_serializable_keys:\n            non_serializable_keys += [\'coords\']\n        return json_serialize(vars(self), non_serializable_keys=non_serializable_keys)\n\n\nclass Metadata(BaseElement):\n    """"""Metadata information.\n\n    :ivar creator: name of the process of person that created the exported file\n    :ivar created: time of creation of the file\n    :ivar last_change: time of last modification of the file\n    :ivar comments: comments on the process\n    """"""\n    tag = \'Metadata\'\n\n    def __init__(self,\n                 creator: str = None,\n                 created: str = None,\n                 last_change: str = None,\n                 comments: str = None):\n        self.creator = creator\n        self.created = created\n        self.last_change = last_change\n        self.comments = comments if comments is not None else \'\'\n\n    @classmethod\n    def from_xml(cls, e: ET.Element) -> \'Metadata\':\n        if e is None:\n            return None\n        cls.check_tag(e.tag)\n        creator_et = e.find(\'p:Creator\', _ns)\n        created_et = e.find(\'p:Created\', _ns)\n        last_change_et = e.find(\'p:LastChange\', _ns)\n        comments_et = e.find(\'p:Comments\', _ns)\n        return Metadata(creator=creator_et.text if creator_et is not None else None,\n                        created=created_et.text if created_et is not None else None,\n                        last_change=last_change_et.text if last_change_et is not None else None,\n                        comments=comments_et.text if comments_et is not None else None)\n\n    def to_xml(self) -> ET.Element:\n        metadata_et = ET.Element(\'Metadata\')\n        creator_et = ET.SubElement(metadata_et, \'Creator\')\n        creator_et.text = self.creator if self.creator is not None else \'\'\n        created_et = ET.SubElement(metadata_et, \'Created\')\n        created_et.text = self.created if self.created is not None else \'\'\n        last_change_et = ET.SubElement(metadata_et, \'LastChange\')\n        last_change_et.text = self.last_change if self.last_change is not None else \'\'\n        comments_et = ET.SubElement(metadata_et, \'Comments\')\n        comments_et.text = self.comments if self.comments is not None else \'\'\n\n        return metadata_et\n\n    def to_dict(self):\n        return vars(self)\n\n    @classmethod\n    def from_dict(cls, dictionary: dict) -> \'Metadata\':\n        return cls(created=dictionary.get(\'created\'),\n                   creator=dictionary.get(\'creator\'),\n                   last_change=dictionary.get(\'last_change\'),\n                   comments=dictionary.get(\'comments\')\n                   )\n\n\nclass GroupSegment(Region):\n    """"""\n    Set of regions that make a bigger region (group).\n    `GroupSegment` is a region containing several `TextLine` and that form a bigger region.\n    It is used mainly to make line / column regions.\n    Only for JSON export (no PAGE XML correspondence).\n\n    :ivar id: identifier of the `GroupSegment`\n    :ivar coords: coordinates of the `GroupSegment`\n    :ivar segment_ids: list of the regions ids belonging to the group\n\n    """"""\n\n    def __init__(self,\n                 id: str = None,\n                 coords: List[Point] = None,\n                 segment_ids: List[str] = None,\n                 custom_attribute: str = None):\n        super().__init__(id=id, coords=coords, custom_attribute=custom_attribute)\n        self.segment_ids = segment_ids if segment_ids is not None else []\n\n    @classmethod\n    def from_dict(cls, dictionary: dict) -> \'GroupSegment\':\n        return cls(**super().from_dict(dictionary),\n                   segment_ids=dictionary.get(\'segment_ids\'))\n\n\nclass Page(BaseElement):\n    """"""\n    Class following PAGE-XML object.\n    This class is used to represent the information of the processed image. It is possible to export this info as\n    PAGE-XML or JSON format.\n\n    :ivar image_filename: filename of the image\n    :ivar image_width: width of the original image\n    :ivar image_height: height of the original image\n    :ivar text_regions: list of `TextRegion`\n    :ivar graphic_regions: list of `GraphicRegion`\n    :ivar page_border: `Border` of the page\n    :ivar separator_regions: list of `SeparatorRegion`\n    :ivar table_regions: list of `TableRegion`\n    :ivar metadata: `Metadata` of the image and process\n    :ivar line_groups: list of `GroupSegment` forming lines\n    :ivar column_groups: list of `GroupSegment` forming columns\n\n    """"""\n    tag = \'Page\'\n\n    def __init__(self, **kwargs):\n        self.image_filename = kwargs.get(\'image_filename\')\n        self.image_width = _try_to_int(kwargs.get(\'image_width\'))  # Needs to be int type (not np.int32/64)\n        self.image_height = _try_to_int(kwargs.get(\'image_height\'))\n        self.text_regions = kwargs.get(\'text_regions\', [])\n        self.graphic_regions = kwargs.get(\'graphic_regions\', [])\n        self.page_border = kwargs.get(\'page_border\', Border())\n        self.separator_regions = kwargs.get(\'separator_regions\', [])\n        self.table_regions = kwargs.get(\'table_regions\', [])\n        self.metadata = kwargs.get(\'metadata\', Metadata())\n        self.line_groups = kwargs.get(\'line_groups\', [])\n        self.column_groups = kwargs.get(\'column_groups\', [])\n\n    @classmethod\n    def from_xml(cls, e: ET.Element) -> \'Page\':\n        cls.check_tag(e.tag)\n        return Page(\n            image_filename=e.attrib.get(\'imageFilename\'),\n            image_width=e.attrib.get(\'imageWidth\'),\n            image_height=e.attrib.get(\'imageHeight\'),\n            text_regions=[TextRegion.from_xml(tr) for tr in e.findall(\'p:TextRegion\', _ns)],\n            graphic_regions=[GraphicRegion.from_xml(tr) for tr in e.findall(\'p:GraphicRegion\', _ns)],\n            page_border=Border.from_xml(e.find(\'p:Border\', _ns)),\n            separator_regions=[SeparatorRegion.from_xml(sep) for sep in e.findall(\'p:SeparatorRegion\', _ns)],\n            table_regions=[TableRegion.from_xml(tr) for tr in e.findall(\'p:TableRegion\', _ns)]\n        )\n\n    @classmethod\n    def from_dict(cls, dictionary: dict) -> \'Page\':\n        return cls(image_filename=dictionary.get(\'image_filename\'),\n                   image_height=dictionary.get(\'image_height\'),\n                   image_width=dictionary.get(\'image_width\'),\n                   metadata=Metadata.from_dict(dictionary.get(\'metadata\')),\n                   text_regions=[TextRegion.from_dict(tr) for tr in dictionary.get(\'text_regions\', list())],\n                   page_border=Border.from_dict(dictionary.get(\'page_border\', dict())),\n                   separator_regions=[SeparatorRegion.from_dict(sep) for sep in\n                                      dictionary.get(\'separator_regions\', list())],\n                   graphic_regions=[GraphicRegion.from_dict(gr) for gr in dictionary.get(\'graphic_regions\', list())],\n                   table_regions=[TableRegion.from_dict(tr) for tr in dictionary.get(\'table_regions\', list())],\n                   line_groups=[GroupSegment.from_dict(lr) for lr in dictionary.get(\'line_groups\', list())],\n                   column_groups=[GroupSegment.from_dict(cr) for cr in dictionary.get(\'column_groups\', list())]\n                   )\n\n    def to_xml(self) -> ET.Element:\n        page_et = ET.Element(\'Page\')\n        if self.image_filename:\n            page_et.set(\'imageFilename\', self.image_filename)\n        if self.image_width:\n            page_et.set(\'imageWidth\', str(self.image_width))\n        if self.image_height:\n            page_et.set(\'imageHeight\', str(self.image_height))\n        for tr in self.text_regions:\n            page_et.append(tr.to_xml())\n        for gr in self.graphic_regions:\n            page_et.append(gr.to_xml())\n        if self.page_border:\n            page_et.append(self.page_border.to_xml())\n        for sep in self.separator_regions:\n            page_et.append(sep.to_xml())\n        for tr in self.table_regions:\n            page_et.append(tr.to_xml())\n        # if self.metadata:\n        #     page_et.append(self.metadata.to_xml())\n        return page_et\n\n    def to_json(self) -> dict:\n        self_dict = vars(self)\n\n        serializable_keys = [\'image_filename\', \'image_height\', \'image_width\']\n        json_dict = json_serialize(self_dict, [k for k in self_dict.keys() if k not in serializable_keys])\n\n        return json_dict\n\n    def write_to_file(self, filename: str, creator_name: str = \'dhSegment\', comments: str = \'\') -> None:\n        """"""\n        Export Page object to json or page-xml format. Will assume the format based on the extension of the filename,\n        if there is no extension will export as an xml file.\n\n        :param filename: filename of the file to be exported\n        :param creator_name: name of the creator (process or person) creating the file\n        :param comments: optionnal comment to add to the metadata of the file.\n        """"""\n\n        def _write_xml():\n            root = ET.Element(\'PcGts\')\n            root.set(\'xmlns\', _ns[\'p\'])\n\n            root.append(self.metadata.to_xml())\n            root.append(self.to_xml())\n            # If https usage is needed, change http to https\n            if _use_https:\n                global _attribs\n                _attribs[\'xmlns:xsi\'] = re.sub(\'http:\', \'https:\', _attribs[\'xmlns:xsi\'])\n                _attribs[\'xsi:schemaLocation\'] = re.sub(\'http:\', \'https:\', _attribs[\'xsi:schemaLocation\'])\n            for k, v in _attribs.items():\n                root.attrib[k] = v\n            ET.ElementTree(element=root).write(filename, encoding=\'utf-8\')\n\n        def _write_json():\n            with open(filename, \'w\', encoding=\'utf8\') as file:\n                json.dump(self.to_json(), file, indent=4, sort_keys=True, allow_nan=False)\n\n        # Updating metadata\n        self.metadata.creator = creator_name\n        self.metadata.comments += comments\n        generated_on = str(datetime.datetime.now().isoformat())\n        if self.metadata.created is None:\n            self.metadata.created = generated_on\n        else:\n            self.metadata.last_change = generated_on\n\n        # Depending on the extension write xml or json file\n        extension = os.path.splitext(filename)[1]\n\n        if extension == \'.xml\':\n            _write_xml()\n        elif extension == \'.json\':\n            _write_json()\n        else:\n            print(\'WARN : No extension for export, XML export by default\')\n            _write_xml()\n\n    def draw_baselines(self, img_canvas: np.ndarray, color: Tuple[int, int, int] = (255, 0, 0), thickness: int = 2,\n                       endpoint_radius: int = 4, autoscale: bool = True):\n        """"""\n        Given an image, draws the TextLines.baselines.\n\n        :param img_canvas: 3 channel image in which the region will be drawn. The image is modified inplace.\n        :param color: (R, G, B) value color\n        :param thickness: the thickness of the line\n        :param endpoint_radius: the radius of the endpoints of line s(first and last coordinates of line)\n        :param autoscale: whether to scale the coordinates to the size of img_canvas. If True,\n                          it will use the dimensions provided in Page.image_width and Page.image_height\n                          to compute the scaling ratio\n        """"""\n\n        text_lines = [tl for tr in self.text_regions for tl in tr.text_lines]\n        if autoscale:\n            assert self.image_height is not None\n            assert self.image_width is not None\n            ratio = (img_canvas.shape[0] / self.image_height, img_canvas.shape[1] / self.image_width)\n        else:\n            ratio = (1, 1)\n\n        tl_coords = [(Point.list_to_cv2poly(tl.baseline) * ratio).astype(np.int32) for tl in text_lines\n                     if len(tl.baseline) > 0]\n        cv2.polylines(img_canvas, tl_coords, False, color, thickness=thickness)\n        for coords in tl_coords:\n            cv2.circle(img_canvas, (coords[0, 0, 0], coords[0, 0, 1]),\n                       radius=endpoint_radius, color=color, thickness=-1)\n            cv2.circle(img_canvas, (coords[-1, 0, 0], coords[-1, 0, 1]),\n                       radius=endpoint_radius, color=color, thickness=-1)\n\n    def draw_lines(self, img_canvas: np.ndarray, color: Tuple[int, int, int] = (255, 0, 0), thickness: int = 2,\n                   fill: bool = True, autoscale: bool = True):\n        """"""\n        Given an image, draws the polygons containing text lines, i.e TextLines.coords\n\n        :param img_canvas: 3 channel image in which the region will be drawn. The image is modified inplace.\n        :param color: (R, G, B) value color\n        :param thickness: the thickness of the line\n        :param fill: if True fills the polygon\n        :param autoscale: whether to scale the coordinates to the size of img_canvas. If True,\n                          it will use the dimensions provided in Page.image_width and Page.image_height\n                          to compute the scaling ratio\n        """"""\n\n        text_lines = [tl for tr in self.text_regions for tl in tr.text_lines]\n        if autoscale:\n            assert self.image_height is not None\n            assert self.image_width is not None\n            ratio = (img_canvas.shape[0] / self.image_height, img_canvas.shape[1] / self.image_width)\n        else:\n            ratio = (1, 1)\n\n        tl_coords = [(Point.list_to_cv2poly(tl.coords) * ratio).astype(np.int32) for tl in text_lines\n                     if len(tl.coords) > 0]\n\n        if fill:\n            for tl in tl_coords:  # For loop to avoid black regions when lines overlap\n                cv2.fillPoly(img_canvas, [tl], color)\n        else:\n            for tl in tl_coords:  # For loop to avoid black regions when lines overlap\n                cv2.polylines(img_canvas, [tl], False, color, thickness=thickness)\n\n    def draw_text_regions(self, img_canvas: np.ndarray, color: Tuple[int, int, int] = (255, 0, 0), fill: bool = True,\n                          thickness: int = 3, autoscale: bool = True):\n        """"""\n        Given an image, draws the TextRegions, either fills it (fill=True) or draws the contours (fill=False)\n\n        :param img_canvas: 3 channel image in which the region will be drawn. The image is modified inplace.\n        :param color: (R, G, B) value color\n        :param fill: either to fill the region (True) of only draw the external contours (False)\n        :param thickness: in case fill=True the thickness of the line\n        :param autoscale: whether to scale the coordinates to the size of img_canvas. If True,\n                          it will use the dimensions provided in Page.image_width and Page.image_height\n                          to compute the scaling ratio\n        """"""\n\n        if autoscale:\n            assert self.image_height is not None\n            assert self.image_width is not None\n            ratio = (img_canvas.shape[0] / self.image_height, img_canvas.shape[1] / self.image_width)\n        else:\n            ratio = (1, 1)\n\n        tr_coords = [(Point.list_to_cv2poly(tr.coords) * ratio).astype(np.int32) for tr in self.text_regions\n                     if len(tr.coords) > 0]\n        if fill:\n            cv2.fillPoly(img_canvas, tr_coords, color)\n        else:\n            cv2.polylines(img_canvas, tr_coords, True, color, thickness=thickness)\n\n    def draw_page_border(self, img_canvas, color: Tuple[int, int, int] = (255, 0, 0), fill: bool = True,\n                         thickness: int = 5, autoscale: bool = True):\n        """"""\n        Given an image, draws the page border, either fills it (fill=True) or draws the contours (fill=False)\n\n        :param img_canvas: 3 channel image in which the region will be drawn. The image is modified inplace.\n        :param color: (R, G, B) value color\n        :param fill: either to fill the region (True) of only draw the external contours (False)\n        :param thickness: in case fill=True the thickness of the line\n        :param autoscale: whether to scale the coordinates to the size of img_canvas. If True,\n                          it will use the dimensions provided in Page.image_width and Page.image_height\n                          to compute the scaling ratio\n        """"""\n\n        if autoscale:\n            assert self.image_height is not None\n            assert self.image_width is not None\n            ratio = (img_canvas.shape[0] / self.image_height, img_canvas.shape[1] / self.image_width)\n        else:\n            ratio = (1, 1)\n\n        border_coords = (Point.list_to_cv2poly(self.page_border.coords) * ratio).astype(np.int32) \\\n            if len(self.page_border.coords) > 0 else []\n        if fill:\n            cv2.fillPoly(img_canvas, [border_coords], color)\n        else:\n            cv2.polylines(img_canvas, [border_coords], True, color, thickness=thickness)\n\n    def draw_separator_lines(self, img_canvas: np.ndarray, color: Tuple[int, int, int] = (0, 255, 0),\n                             thickness: int = 3, filter_by_id: str = \'\', autoscale: bool = True):\n        """"""\n        Given an image, draws the SeparatorRegion.\n\n        :param img_canvas: 3 channel image in which the region will be drawn. The image is modified inplace.\n        :param color: (R, G, B) value color\n        :param thickness: thickness of the line\n        :param filter_by_id: string to filter the lines by id. For example vertical/horizontal lines can be filtered\n                             if \'vertical\' or \'horizontal\' is mentioned in the id.\n        :param autoscale: whether to scale the coordinates to the size of img_canvas. If True,\n                          it will use the dimensions provided in Page.image_width and Page.image_height\n                          to compute the scaling ratio\n        """"""\n\n        if autoscale:\n            assert self.image_height is not None\n            assert self.image_width is not None\n            ratio = (img_canvas.shape[0] / self.image_height, img_canvas.shape[1] / self.image_width)\n        else:\n            ratio = (1, 1)\n\n        sep_coords = [(Point.list_to_cv2poly(sep.coords) * ratio).astype(np.int32) for sep in self.separator_regions\n                      if len(sep.coords) > 0 and filter_by_id in sep.id]\n        cv2.polylines(img_canvas, sep_coords, True, color, thickness=thickness)\n\n    def draw_graphic_regions(self, img_canvas: np.ndarray, color: Tuple[int, int, int] = (255, 0, 0),\n                             fill: bool = True, thickness: int = 3, autoscale: bool = True):\n        """"""\n        Given an image, draws the GraphicRegions, either fills it (fill=True) or draws the contours (fill=False)\n\n        :param img_canvas: 3 channel image in which the region will be drawn.  The image is modified inplace.\n        :param color: (R, G, B) value color\n        :param fill: either to fill the region (True) of only draw the external contours (False)\n        :param thickness: in case fill=True the thickness of the line\n        :param autoscale: whether to scale the coordinates to the size of img_canvas. If True,\n                          it will use the dimensions provided in Page.image_width and Page.image_height\n                          to compute the scaling ratio\n        """"""\n\n        if autoscale:\n            assert self.image_height is not None\n            assert self.image_width is not None\n            ratio = (img_canvas.shape[0] / self.image_height, img_canvas.shape[1] / self.image_width)\n        else:\n            ratio = (1, 1)\n\n        gr_coords = [(Point.list_to_cv2poly(gr.coords) * ratio).astype(np.int32) for gr in self.graphic_regions\n                     if len(gr.coords) > 0]\n        if fill:\n            cv2.fillPoly(img_canvas, gr_coords, color)\n        else:\n            cv2.polylines(img_canvas, gr_coords, True, color, thickness=thickness)\n\n    def draw_text(self, img_canvas: np.ndarray, color: Tuple[int, int, int] = (255, 0, 0), thickness: int = 5,\n                  font=cv2.FONT_HERSHEY_SIMPLEX, font_scale: float = 1.0, autoscale: bool = True):\n        """"""\n        Writes the text of the TextLine on the given image.\n\n        :param img_canvas: 3 channel image in which the region will be drawn.  The image is modified inplace\n        :param color: (R, G, B) value color\n        :param thickness: the thickness of the characters\n        :param font: the type of font (``cv2`` constant)\n        :param font_scale: the scale of font\n        :param autoscale: whether to scale the coordinates to the size of img_canvas. If True,\n                          it will use the dimensions provided in Page.image_width and Page.image_height\n                          to compute the scaling ratio\n        """"""\n        text_lines = [tl for tr in self.text_regions for tl in tr.text_lines]\n        if autoscale:\n            assert self.image_height is not None\n            assert self.image_width is not None\n            ratio = (img_canvas.shape[0] / self.image_height, img_canvas.shape[1] / self.image_width)\n        else:\n            ratio = (1, 1)\n\n        tl_tuple_coords_text = [((np.array(Point.point_to_list(tl.coords)) * ratio).astype(np.int32),\n                                 tl.text.text_equiv) for tl in text_lines if len(tl.coords) > 0]\n\n        for coords, text in tl_tuple_coords_text:\n            polyline = Polygon(coords)\n            xmin, ymin, xmax, ymax = polyline.bounds\n            ymin = np.maximum(0, ymin - 20)\n\n            cv2.putText(img_canvas, text, (int(xmin), int(ymin)), fontFace=font, fontScale=font_scale, color=color,\n                        thickness=thickness)\n\n    def draw_line_groups(self, img_canvas: np.ndarray, color: Tuple[int, int, int] = (0, 255, 0), fill: bool = False,\n                         thickness: int = 5, autoscale: bool = True):\n        """"""\n        It will draw line groups. This is only valid when parsing JSON files.\n\n        :param img_canvas: 3 channel image in which the region will be drawn. The image is modified inplace.\n        :param color: (R, G, B) value color\n        :param fill: either to fill the region (True) of only draw the external contours (False)\n        :param thickness: in case fill=False the thickness of the line\n        :param autoscale: whether to scale the coordinates to the size of img_canvas. If True,\n                          it will use the dimensions provided in Page.image_width and Page.image_height\n                          to compute the scaling ratio\n        """"""\n        assert self.line_groups, ""There is no Line group""\n\n        if autoscale:\n            assert self.image_height is not None\n            assert self.image_width is not None\n            ratio = (img_canvas.shape[0] / self.image_height, img_canvas.shape[1] / self.image_width)\n        else:\n            ratio = (1, 1)\n\n        lg_coords = [(Point.list_to_cv2poly(lg.coords) * ratio).astype(np.int32) for lg in self.line_groups\n                     if len(lg.coords) > 0]\n        if fill:\n            cv2.fillPoly(img_canvas, lg_coords, color)\n        else:\n            cv2.polylines(img_canvas, lg_coords, True, color, thickness=thickness)\n\n    def draw_column_groups(self, img_canvas: np.ndarray, color: Tuple[int, int, int] = (0, 255, 0), fill: bool = False,\n                           thickness: int = 5, autoscale: bool = True):\n        """"""\n        It will draw column groups (in case of a table). This is only valid when parsing JSON files.\n\n        :param img_canvas: 3 channel image in which the region will be drawn. The image is modified inplace\n        :param color: (R, G, B) value color\n        :param fill: either to fill the region (True) of only draw the external contours (False)\n        :param thickness: in case fill=False the thickness of the line\n        :param autoscale: whether to scale the coordinates to the size of img_canvas. If True,\n                          it will use the dimensions provided in Page.image_width and Page.image_height\n                          to compute the scaling ratio\n        """"""\n\n        assert self.column_groups, ""There is no Line group""\n\n        if autoscale:\n            assert self.image_height is not None\n            assert self.image_width is not None\n            ratio = (img_canvas.shape[0] / self.image_height, img_canvas.shape[1] / self.image_width)\n        else:\n            ratio = (1, 1)\n\n        cg_coords = [(Point.list_to_cv2poly(cg.coords) * ratio).astype(np.int32) for cg in self.column_groups\n                     if len(cg.coords) > 0]\n        if fill:\n            cv2.fillPoly(img_canvas, cg_coords, color)\n        else:\n            cv2.polylines(img_canvas, cg_coords, True, color, thickness=thickness)\n\n\ndef parse_file(filename: str) -> Page:\n    """"""\n    Parses the files to create the corresponding ``Page`` object. The files can be a .xml or a .json.\n\n    :param filename: file to parse (either json of page xml)\n    :return: Page object containing all the parsed elements\n    """"""\n    global _use_https, _ns\n\n    extension = os.path.splitext(filename)[1]\n\n    if extension == \'.xml\':\n        xml_page = ET.parse(filename)\n        # find if https need to be used or not\n        root = xml_page.getroot()\n        if \'https\' in root.tag and not _use_https:\n            _use_https = True\n            _ns[\'p\'] = re.sub(\'http:\', \'https:\', _ns[\'p\'])\n        page_elements = xml_page.find(\'p:Page\', _ns)\n        metadata_et = xml_page.find(\'p:Metadata\', _ns)\n        page = Page.from_xml(page_elements)\n        page.metadata = Metadata.from_xml(metadata_et)\n        return page\n    elif extension == \'.json\':\n        with open(filename, \'r\', encoding=\'utf8\') as file:\n            json_dict = json.load(file)\n        return Page.from_dict(json_dict)\n    else:\n        raise NotImplementedError\n\n\ndef json_serialize(dict_to_serialize: dict,\n                   non_serializable_keys: List[str] = list()) -> dict:\n    """"""\n    Serialize a dictionary in order to export it.\n\n    :param dict_to_serialize: dictionary to serialize\n    :param non_serializable_keys: keys that are not directly serializable such as python objects\n    :return: the serialized dictionary\n    """"""\n\n    new_dict = dict_to_serialize.copy()\n    for key in non_serializable_keys:\n        if isinstance(new_dict[key], list):\n            new_dict[key] = [elem.to_dict() for elem in new_dict[key]] if new_dict[key] else []\n        elif isinstance(new_dict[key], np.ndarray):\n            new_dict[key] = new_dict[key].tolist()\n        else:\n            new_dict[key] = new_dict[key].to_dict()\n\n    return new_dict\n\n\ndef save_baselines(filename: str,\n                   baselines,\n                   ratio: Tuple[int, int] = (1, 1),\n                   predictions_shape: Tuple[int, int] = None) -> Page:\n    """"""\n\n    :param filename: filename to save baselines to\n    :param baselines: list of baselines\n    :param ratio: ratio of prediction shape over original shape\n    :param predictions_shape: shape of the masks output by the network\n    :return:\n    """"""\n    txt_lines = [TextLine.from_array(baseline_coords=b, id=\'line_{}\'.format(i)) for i, b in enumerate(baselines)]\n    for l in txt_lines:\n        l.scale_baseline_points(ratio)\n    txt_region = TextRegion(text_lines=txt_lines, id=\'region_0\')\n    page = Page(text_regions=[txt_region],\n                image_height=int(predictions_shape[0] * ratio[0]) if predictions_shape is not None else None,\n                image_width=int(predictions_shape[1] * ratio[1]) if predictions_shape is not None else None)\n    page.write_to_file(filename)\n\n    return page\n\n\ndef get_unique_tags_from_xml_text_regions(xml_filename: str,\n                                          tag_pattern: str = \'{type:.*;}\'):\n    """"""\n    Get a list of all the values of labels/tags\n\n    :param xml_filename: filename of the xml file\n    :param tag_pattern: regular expression pattern to look for in `TextRegion.custom_attribute`\n    :return:\n    """"""\n    tagset = list()\n    page = parse_file(xml_filename)\n    for tr in page.text_regions:\n        custom_attribute = tr.custom_attribute\n        matches = re.findall(tag_pattern, custom_attribute)\n        assert len(matches) <= 1, ""Found multiple matches in {}"".format(custom_attribute)\n        if matches:\n            tagset.append(matches[0][6:-2])\n\n    return list(np.unique(tagset))\n'"
dh_segment/io/__init__.py,1,"b'r""""""\nThe :mod:`dh_segment.io` module implements input / output functions and classes.\n\nInput functions for ``tf.Estimator``\n------------------------------------\n\n**Input function**\n\n.. autosummary::\n    input_fn\n\n**Data augmentation**\n\n.. autosummary::\n    data_augmentation_fn\n    extract_patches_fn\n    rotate_crop\n\n**Resizing function**\n\n.. autosummary::\n    resize_image\n    load_and_resize_image\n\n\nTensorflow serving functions\n----------------------------\n\n.. autosummary::\n    serving_input_filename\n    serving_input_image\n\n----\n\nPAGE XML and JSON import / export\n---------------------------------\n\n**PAGE classes**\n\n.. autosummary::\n    PAGE.Point\n    PAGE.Text\n    PAGE.Border\n    PAGE.TextRegion\n    PAGE.TextLine\n    PAGE.GraphicRegion\n    PAGE.TableRegion\n    PAGE.SeparatorRegion\n    PAGE.GroupSegment\n    PAGE.Metadata\n    PAGE.Page\n\n**Abstract classes**\n\n.. autosummary::\n    PAGE.BaseElement\n    PAGE.Region\n\n**Parsing and helpers**\n\n.. autosummary::\n    PAGE.parse_file\n    PAGE.json_serialize\n\n----\n\n.. _ref_via:\n\nVGG Image Annotator helpers\n---------------------------\n\n\n**VIA objects**\n\n.. autosummary::\n    via.WorkingItem\n    via.VIAttribute\n\n\n**Creating masks with VIA annotations**\n\n.. autosummary::\n    via.load_annotation_data\n    via.export_annotation_dict\n    via.get_annotations_per_file\n    via.parse_via_attributes\n    via.get_via_attributes\n    via.collect_working_items\n    via.create_masks\n\n\n**Formatting in VIA JSON format**\n\n.. autosummary::\n    via.create_via_region_from_coordinates\n    via.create_via_annotation_single_image\n\n----\n\n""""""\n\n\n_INPUT = [\n    \'input_fn\',\n    \'serving_input_filename\',\n    \'serving_input_image\',\n    \'data_augmentation_fn\',\n    \'rotate_crop\',\n    \'resize_image\',\n    \'load_and_resize_image\',\n    \'extract_patches_fn\',\n    \'local_entropy\'\n]\n\n# _PAGE_OBJECTS = [\n#     \'Point\',\n#     \'Text\',\n#     \'Region\',\n#     \'TextLine\',\n#     \'GraphicRegion\',\n#     \'TextRegion\',\n#     \'TableRegion\',\n#     \'SeparatorRegion\',\n#     \'Border\',\n#     \'Metadata\',\n#     \'GroupSegment\',\n#     \'Page\'\n# ]\n#\n# _PAGE_FN = [\n#     \'parse_file\',\n#     \'json_serialize\'\n# ]\n\n__all__ = _INPUT  # + _PAGE_OBJECTS + _PAGE_FN\n\nfrom .input import *\nfrom .input_utils import *\nfrom . import PAGE\nfrom . import via\n\n'"
dh_segment/io/input.py,33,"b'from glob import glob\nimport os\nimport tensorflow as tf\nimport numpy as np\nfrom .. import utils\nfrom tqdm import tqdm\nfrom typing import Union, List\nfrom enum import Enum\nimport pandas as pd\nfrom .input_utils import data_augmentation_fn, extract_patches_fn, load_and_resize_image, \\\n    rotate_crop, resize_image, local_entropy\n\n\nclass InputCase(Enum):\n    INPUT_LIST = \'INPUT_LIST\'\n    INPUT_DIR = \'INPUT_DIR\'\n    INPUT_CSV = \'INPUT_CSV\'\n\n\ndef input_fn(input_data: Union[str, List[str]], params: dict, input_label_dir: str=None,\n             data_augmentation: bool=False, batch_size: int=5, make_patches: bool=False, num_epochs: int=1,\n             num_threads: int=4, image_summaries: bool=False):\n    """"""\n    Input_fn for estimator\n    \n    :param input_data: input data. It can be a directory containing the images, it can be\n        a list of image filenames, or it can be a path to a csv file.\n    :param params: params from utils.Params object\n    :param input_label_dir: directory containing the label images\n    :param data_augmentation: boolean, if True will scale, roatate, ... the images\n    :param batch_size: size of the bach\n    :param make_patches: bool, whether to make patches (crop image in smaller pieces) or not\n    :param num_epochs: number of epochs to cycle trough data (set it to None for infinite repeat)\n    :param num_threads: number of thread to use in parallele when usin tf.data.Dataset.map\n    :param image_summaries: boolean, whether to make tf.Summary to watch on tensorboard\n    :return: fn\n    """"""\n    training_params = utils.TrainingParams.from_dict(params[\'training_params\'])\n    prediction_type = params[\'prediction_type\']\n    classes_file = params[\'classes_file\']\n\n    # --- Map functions\n    def _make_patches_fn(input_image: tf.Tensor, label_image: tf.Tensor, offsets: tuple) -> (tf.Tensor, tf.Tensor):\n        with tf.name_scope(\'patching\'):\n            patches_image = extract_patches_fn(input_image, training_params.patch_shape, offsets)\n            patches_label = extract_patches_fn(label_image, training_params.patch_shape, offsets)\n\n            return patches_image, patches_label\n\n    # Load and resize images\n    def _load_image_fn(image_filename, label_filename):\n        if training_params.data_augmentation and training_params.input_resized_size > 0:\n            random_scaling = tf.random_uniform([],\n                                               np.maximum(1 - training_params.data_augmentation_max_scaling, 0),\n                                               1 + training_params.data_augmentation_max_scaling)\n            new_size = training_params.input_resized_size * random_scaling\n        else:\n            new_size = training_params.input_resized_size\n\n        if prediction_type in [utils.PredictionType.CLASSIFICATION, utils.PredictionType.MULTILABEL]:\n            label_image = load_and_resize_image(label_filename, 3, new_size, interpolation=\'NEAREST\')\n        elif prediction_type == utils.PredictionType.REGRESSION:\n            label_image = load_and_resize_image(label_filename, 1, new_size, interpolation=\'NEAREST\')\n        else:\n            raise NotImplementedError\n        input_image = load_and_resize_image(image_filename, 3, new_size)\n        return input_image, label_image\n\n    # Data augmentation, patching\n    def _scaling_and_patch_fn(input_image, label_image):\n        if data_augmentation:\n            # Rotation of the original image\n            if training_params.data_augmentation_max_rotation > 0:\n                with tf.name_scope(\'random_rotation\'):\n                    rotation_angle = tf.random_uniform([],\n                                                       -training_params.data_augmentation_max_rotation,\n                                                       training_params.data_augmentation_max_rotation)\n                    label_image = rotate_crop(label_image, rotation_angle,\n                                              minimum_shape=[(i * 3) // 2 for i in training_params.patch_shape],\n                                              interpolation=\'NEAREST\')\n                    input_image = rotate_crop(input_image, rotation_angle,\n                                              minimum_shape=[(i * 3) // 2 for i in training_params.patch_shape],\n                                              interpolation=\'BILINEAR\')\n\n        if make_patches:\n            # Offsets for patch extraction\n            offsets = (tf.random_uniform(shape=[], minval=0, maxval=1, dtype=tf.float32),\n                       tf.random_uniform(shape=[], minval=0, maxval=1, dtype=tf.float32))\n            # offsets = (0, 0)\n            batch_image, batch_label = _make_patches_fn(input_image, label_image, offsets)\n        else:\n            with tf.name_scope(\'formatting\'):\n                batch_image = tf.expand_dims(input_image, axis=0)\n                batch_label = tf.expand_dims(label_image, axis=0)\n        return tf.data.Dataset.from_tensor_slices((batch_image, batch_label))\n\n    # Data augmentation\n    def _augment_data_fn(input_image, label_image): \\\n        return data_augmentation_fn(input_image, label_image, training_params.data_augmentation_flip_lr,\n                                    training_params.data_augmentation_flip_ud, training_params.data_augmentation_color)\n\n    # Assign color to class id\n    def _assign_color_to_class_id(input_image, label_image):\n        # Convert RGB to class id\n        if prediction_type == utils.PredictionType.CLASSIFICATION:\n            label_image = utils.label_image_to_class(label_image, classes_file)\n        elif prediction_type == utils.PredictionType.MULTILABEL:\n            label_image = utils.multilabel_image_to_class(label_image, classes_file)\n        output = {\'images\': input_image, \'labels\': label_image}\n\n        if training_params.local_entropy_ratio > 0 and prediction_type == utils.PredictionType.CLASSIFICATION:\n            output[\'weight_maps\'] = local_entropy(tf.equal(label_image, 1),\n                                                  sigma=training_params.local_entropy_sigma)\n        return output\n    # ---\n\n    # Finding the list of images to be used\n    if isinstance(input_data, list):\n        input_case = InputCase.INPUT_LIST\n        input_image_filenames = input_data\n        print(\'Found {} images\'.format(len(input_image_filenames)))\n\n    elif os.path.isdir(input_data):\n        input_case = InputCase.INPUT_DIR\n        input_image_filenames = glob(os.path.join(input_data, \'**\', \'*.jpg\'),\n                                     recursive=True) + \\\n                                glob(os.path.join(input_data, \'**\', \'*.png\'),\n                                     recursive=True)\n        print(\'Found {} images\'.format(len(input_image_filenames)))\n\n    elif os.path.isfile(input_data) and \\\n            input_data.endswith(\'.csv\'):\n        input_case = InputCase.INPUT_CSV\n    else:\n        raise NotImplementedError(\'Input data should be a directory, a csv file or a list of filenames but got {}\'.format(input_data))\n\n    # Finding the list of labelled images if available\n    has_labelled_data = False\n    if input_label_dir and input_case in [InputCase.INPUT_LIST, InputCase.INPUT_DIR]:\n        label_image_filenames = []\n        for input_image_filename in input_image_filenames:\n            label_image_filename = os.path.join(input_label_dir, os.path.basename(input_image_filename))\n            if not os.path.exists(label_image_filename):\n                filename, extension = os.path.splitext(os.path.basename(input_image_filename))\n                new_extension = \'.png\' if extension == \'.jpg\' else \'.jpg\'\n                label_image_filename = os.path.join(input_label_dir, filename + new_extension)\n            label_image_filenames.append(label_image_filename)\n        has_labelled_data = True\n\n    # Read image filenames and labels in case of csv file\n    if input_case == InputCase.INPUT_CSV:\n        df = pd.read_csv(input_data, header=None, names=[\'images\', \'labels\'])\n        input_image_filenames = list(df.images.values)\n        # If the label column exists\n        if not np.alltrue(pd.isnull(df.labels.values)):\n            label_image_filenames = list(df.labels.values)\n            has_labelled_data = True\n\n    # Checks that all image files can be found\n    for img_filename in input_image_filenames:\n        if not os.path.exists(img_filename):\n            raise FileNotFoundError(img_filename)\n    if has_labelled_data:\n        for label_filename in label_image_filenames:\n            if not os.path.exists(label_filename):\n                raise FileNotFoundError(label_filename)\n\n    # Tensorflow input_fn\n    def fn():\n        if not has_labelled_data:\n            encoded_filenames = [f.encode() for f in input_image_filenames]\n            dataset = tf.data.Dataset.from_generator(lambda: tqdm(encoded_filenames, desc=\'Dataset\'),\n                                                     tf.string, tf.TensorShape([]))\n            dataset = dataset.repeat(count=num_epochs)\n            dataset = dataset.map(lambda filename: {\'images\': load_and_resize_image(filename, 3,\n                                                                                    training_params.input_resized_size)})\n        else:\n            encoded_filenames = [(i.encode(), l.encode()) for i, l in zip(input_image_filenames, label_image_filenames)]\n            dataset = tf.data.Dataset.from_generator(lambda: tqdm(utils.shuffled(encoded_filenames), desc=\'Dataset\'),\n                                                         (tf.string, tf.string), (tf.TensorShape([]), tf.TensorShape([])))\n\n            dataset = dataset.repeat(count=num_epochs)\n            dataset = dataset.map(_load_image_fn, num_threads).flat_map(_scaling_and_patch_fn)\n\n            if data_augmentation:\n                dataset = dataset.map(_augment_data_fn, num_threads)\n            dataset = dataset.map(_assign_color_to_class_id, num_threads)\n\n        # Save original size of images\n        dataset = dataset.map(lambda d: {\'shapes\': tf.shape(d[\'images\'])[:2], **d})\n        if make_patches:\n            dataset = dataset.shuffle(128)\n\n        if make_patches and input_label_dir:\n            base_shape_images = list(training_params.patch_shape)\n        elif make_patches and input_case == InputCase.INPUT_CSV:\n            base_shape_images = list(training_params.patch_shape)\n        else:\n            base_shape_images = [-1, -1]\n        # Pad things\n        padded_shapes = {\n            \'images\': base_shape_images + [3],\n            \'shapes\': [2]\n        }\n        if \'labels\' in dataset.output_shapes.keys():\n            output_shapes_label = dataset.output_shapes[\'labels\']\n            padded_shapes[\'labels\'] = base_shape_images + list(output_shapes_label[2:])\n        if \'weight_maps\' in dataset.output_shapes.keys():\n            padded_shapes[\'weight_maps\'] = base_shape_images\n\n        dataset = dataset.padded_batch(batch_size=batch_size, padded_shapes=padded_shapes).prefetch(8)\n        prepared_batch = dataset.make_one_shot_iterator().get_next()\n\n        # Summaries for checking that the loading and data augmentation goes fine\n        if image_summaries:\n            shape_summary_img = tf.cast(tf.shape(prepared_batch[\'images\'])[1:3] / 3, tf.int32)\n            tf.summary.image(\'input/image\',\n                             tf.image.resize_images(prepared_batch[\'images\'], shape_summary_img),\n                             max_outputs=1)\n            if \'labels\' in prepared_batch:\n                label_export = prepared_batch[\'labels\']\n                if prediction_type == utils.PredictionType.CLASSIFICATION:\n                    label_export = utils.class_to_label_image(label_export, classes_file)\n                if prediction_type == utils.PredictionType.MULTILABEL:\n                    label_export = tf.cast(label_export, tf.int32)\n                    label_export = utils.multiclass_to_label_image(label_export, classes_file)\n                tf.summary.image(\'input/label\',\n                                 tf.image.resize_images(label_export, shape_summary_img), max_outputs=1)\n            if \'weight_maps\' in prepared_batch:\n                tf.summary.image(\'input/weight_map\',\n                                 tf.image.resize_images(prepared_batch[\'weight_maps\'][:, :, :, None],\n                                                        shape_summary_img),\n                                 max_outputs=1)\n\n        return prepared_batch, prepared_batch.get(\'labels\')\n\n    return fn\n\n\ndef serving_input_filename(resized_size):\n    def serving_input_fn():\n        # define placeholder for filename\n        filename = tf.placeholder(dtype=tf.string)\n\n        # TODO : make it batch-compatible (with Dataset or string input producer)\n        decoded_image = tf.to_float(tf.image.decode_jpeg(tf.read_file(filename), channels=3,\n                                                         try_recover_truncated=True))\n        original_shape = tf.shape(decoded_image)[:2]\n\n        if resized_size is not None and resized_size > 0:\n            image = resize_image(decoded_image, resized_size)\n        else:\n            image = decoded_image\n\n        image_batch = image[None]\n        features = {\'images\': image_batch, \'original_shape\': original_shape}\n\n        receiver_inputs = {\'filename\': filename}\n\n        input_from_resized_images = {\'resized_images\': image_batch}\n        input_from_original_image = {\'image\': decoded_image}\n\n        return tf.estimator.export.ServingInputReceiver(features, receiver_inputs,\n                                                        receiver_tensors_alternatives={\'from_image\':\n                                                                                           input_from_original_image,\n                                                                                       \'from_resized_images\':\n                                                                                           input_from_resized_images})\n\n    return serving_input_fn\n\n\ndef serving_input_image():\n    dic_input_serving = {\'images\': tf.placeholder(tf.float32, [None, None, None, 3])}\n    return tf.estimator.export.build_raw_serving_input_receiver_fn(dic_input_serving)\n'"
dh_segment/io/input_utils.py,61,"b'import tensorflow as tf\nfrom tensorflow.contrib.image import rotate as tf_rotate\nfrom scipy import ndimage\nimport numpy as np\nfrom typing import Tuple\n\n\ndef data_augmentation_fn(input_image: tf.Tensor, label_image: tf.Tensor, flip_lr: bool=True,\n                         flip_ud: bool=True, color: bool=True) -> (tf.Tensor, tf.Tensor):\n    """"""Applies data augmentation to both images and label images.\n    Includes left-right flip, up-down flip and color change.\n\n    :param input_image: images to be augmented [B, H, W, C]\n    :param label_image: corresponding label images [B, H, W, C]\n    :param flip_lr: option to flip image in left-right direction\n    :param flip_ud: option to flip image in up-down direction\n    :param color: option to change color of images\n    :return: the tuple (augmented images, augmented label images) [B, H, W, C]\n    """"""\n    with tf.name_scope(\'DataAugmentation\'):\n        if flip_lr:\n            with tf.name_scope(\'random_flip_lr\'):\n                sample = tf.random_uniform([], 0, 1)\n                label_image = tf.cond(sample > 0.5, lambda: tf.image.flip_left_right(label_image), lambda: label_image)\n                input_image = tf.cond(sample > 0.5, lambda: tf.image.flip_left_right(input_image), lambda: input_image)\n        if flip_ud:\n            with tf.name_scope(\'random_flip_ud\'):\n                sample = tf.random_uniform([], 0, 1)\n                label_image = tf.cond(sample > 0.5, lambda: tf.image.flip_up_down(label_image), lambda: label_image)\n                input_image = tf.cond(sample > 0.5, lambda: tf.image.flip_up_down(input_image), lambda: input_image)\n\n        chanels = input_image.get_shape()[-1]\n        if color:\n            input_image = tf.image.random_contrast(input_image, lower=0.8, upper=1.0)\n            if chanels == 3:\n                input_image = tf.image.random_hue(input_image, max_delta=0.1)\n                input_image = tf.image.random_saturation(input_image, lower=0.8, upper=1.2)\n        return input_image, label_image\n\n\ndef rotate_crop(image: tf.Tensor, rotation: float, crop: bool=True, minimum_shape: Tuple[int, int]=[0, 0],\n                interpolation: str=\'NEAREST\') -> tf.Tensor:\n    """"""Rotates and crops the images.\n\n    :param image: image to be rotated and cropped [H, W, C]\n    :param rotation: angle of rotation (in radians)\n    :param crop: option to crop rotated image to avoid black borders due to rotation\n    :param minimum_shape: minimum shape of the rotated image / cropped image\n    :param interpolation: which interpolation to use ``NEAREST`` or ``BILINEAR``\n    :return:\n    """"""\n    with tf.name_scope(\'RotateCrop\'):\n        rotated_image = tf_rotate(image, rotation, interpolation)\n        if crop:\n            rotation = tf.abs(rotation)\n            original_shape = tf.shape(rotated_image)[:2]\n            h, w = original_shape[0], original_shape[1]\n            # see https://stackoverflow.com/questions/16702966/rotate-image-and-crop-out-black-borders for formulae\n            old_l, old_s = tf.cond(h > w, lambda: [h, w], lambda: [w, h])\n            old_l, old_s = tf.cast(old_l, tf.float32), tf.cast(old_s, tf.float32)\n            new_l = (old_l * tf.cos(rotation) - old_s * tf.sin(rotation)) / tf.cos(2 * rotation)\n            new_s = (old_s - tf.sin(rotation) * new_l) / tf.cos(rotation)\n            new_h, new_w = tf.cond(h > w, lambda: [new_l, new_s], lambda: [new_s, new_l])\n            new_h, new_w = tf.cast(new_h, tf.int32), tf.cast(new_w, tf.int32)\n            bb_begin = tf.cast(tf.ceil((h - new_h) / 2), tf.int32), tf.cast(tf.ceil((w - new_w) / 2), tf.int32)\n            rotated_image_crop = rotated_image[bb_begin[0]:h - bb_begin[0], bb_begin[1]:w - bb_begin[1], :]\n\n            # If crop removes the entire image, keep the original image\n            rotated_image = tf.cond(tf.less_equal(tf.reduce_min(tf.shape(rotated_image_crop)[:2]),\n                                                  tf.reduce_max(minimum_shape)),\n                                    true_fn=lambda: image,\n                                    false_fn=lambda: rotated_image_crop)\n        return rotated_image\n\n\ndef resize_image(image: tf.Tensor, size: int, interpolation: str=\'BILINEAR\') -> tf.Tensor:\n    """"""Resizes the image\n\n    :param image: image to be resized [H, W, C]\n    :param size: size of the resized image (in pixels)\n    :param interpolation: which interpolation to use, ``NEAREST`` or ``BILINEAR``\n    :return: resized image\n    """"""\n    assert interpolation in [\'BILINEAR\', \'NEAREST\']\n\n    with tf.name_scope(\'ImageRescaling\'):\n        input_shape = tf.cast(tf.shape(image)[:2], tf.float32)\n        size = tf.cast(size, tf.float32)\n        # Compute new shape\n        # We want X/Y = x/y and we have size = x*y so :\n        ratio = tf.div(input_shape[1], input_shape[0])\n        new_height = tf.sqrt(tf.div(size, ratio))\n        new_width = tf.div(size, new_height)\n        new_shape = tf.cast([new_height, new_width], tf.int32)\n        resize_method = {\n            \'NEAREST\': tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n            \'BILINEAR\': tf.image.ResizeMethod.BILINEAR\n        }\n        return tf.image.resize_images(image, new_shape, method=resize_method[interpolation])\n\n\ndef load_and_resize_image(filename: str, channels: int, size: int=None, interpolation: str=\'BILINEAR\') -> tf.Tensor:\n    """"""Loads an image from its filename and resizes it to the desired output size.\n\n    :param filename: string tensor\n    :param channels: number of channels for the decoded image\n    :param size: number of desired pixels in the resized image, tf.Tensor or int (None for no resizing)\n    :param interpolation:\n    :param return_original_shape: returns the original shape of the image before resizing if this flag is True\n    :return: decoded and resized float32 tensor [h, w, channels],\n    """"""\n    with tf.name_scope(\'load_img\'):\n        decoded_image = tf.to_float(tf.image.decode_jpeg(tf.read_file(filename), channels=channels,\n                                                         try_recover_truncated=True))\n        # TODO : if one side is smaller than size of patches (and make patches == true),\n        # TODO : force the image to have at least patch size\n        if size is not None and not(isinstance(size, int) and size <= 0):\n            result_image = resize_image(decoded_image, size, interpolation)\n        else:\n            result_image = decoded_image\n\n        return result_image\n\n\ndef extract_patches_fn(image: tf.Tensor, patch_shape: Tuple[int, int], offsets: Tuple[int, int]) -> tf.Tensor:\n    """"""Will cut a given image into patches.\n\n    :param image: tf.Tensor\n    :param patch_shape: shape of the extracted patches [h, w]\n    :param offsets: offset to add to the origin of first patch top-right coordinate, useful during data augmentation \\\n    to have slighlty different patches each time. This value will be multiplied by [h/2, w/2] (range values [0,1])\n    :return: patches [batch_patches, h, w, c]\n    """"""\n    with tf.name_scope(\'patch_extraction\'):\n        h, w = patch_shape\n        c = image.get_shape()[-1]\n\n        offset_h = tf.cast(tf.round(offsets[0] * h // 2), dtype=tf.int32)\n        offset_w = tf.cast(tf.round(offsets[1] * w // 2), dtype=tf.int32)\n        offset_img = image[offset_h:, offset_w:, :]\n        offset_img = offset_img[None, :, :, :]\n\n        patches = tf.extract_image_patches(offset_img, ksizes=[1, h, w, 1], strides=[1, h // 2, w // 2, 1],\n                                           rates=[1, 1, 1, 1], padding=\'VALID\')\n        patches_shape = tf.shape(patches)\n        return tf.reshape(patches, [tf.reduce_prod(patches_shape[:3]), h, w, int(c)])\n\n\ndef local_entropy(tf_binary_img: tf.Tensor, sigma: float=3) -> tf.Tensor:\n    """"""\n\n    :param tf_binary_img:\n    :param sigma:\n    :return:\n    """"""\n    tf_binary_img.get_shape().assert_has_rank(2)\n\n    def get_gaussian_filter_1d(sigma):\n        sigma_r = int(np.round(sigma))\n        x = np.zeros(6 * sigma_r + 1, dtype=np.float32)\n        x[3 * sigma_r] = 1\n        return ndimage.filters.gaussian_filter(x, sigma=sigma)\n\n    def _fn(img):\n        labelled, nb_components = ndimage.measurements.label(img)\n        lut = np.concatenate(\n            [np.array([0], np.int32), np.random.randint(20, size=nb_components + 1, dtype=np.int32) + 1])\n        output = lut[labelled]\n        return output\n\n    label_components = tf.py_func(_fn, [tf_binary_img], tf.int32)\n    label_components.set_shape([None, None])\n    one_hot_components = tf.one_hot(label_components, tf.reduce_max(label_components))\n    one_hot_components = tf.transpose(one_hot_components, [2, 0, 1])\n\n    local_components_avg = tf.nn.conv2d(one_hot_components[:, :, :, None],\n                                        get_gaussian_filter_1d(sigma)[None, :, None, None], (1, 1, 1, 1),\n                                        padding=\'SAME\')\n    local_components_avg = tf.nn.conv2d(local_components_avg, get_gaussian_filter_1d(sigma)[:, None, None, None],\n                                        (1, 1, 1, 1), padding=\'SAME\')\n    local_components_avg = tf.transpose(local_components_avg[:, :, :, 0], [1, 2, 0])\n    local_components_avg = tf.pow(local_components_avg, 1 / 1.4)\n    local_components_avg = local_components_avg / (tf.reduce_sum(local_components_avg, axis=2, keep_dims=True) + 1e-6)\n    return -tf.reduce_sum(local_components_avg * tf.log(local_components_avg + 1e-6), axis=2)\n'"
dh_segment/io/via.py,0,"b'#!/usr/bin/env python\n# coding: utf-8 \n\n__author__ = ""maudehrmann, solivr""\n__license__ = ""GPL""\n\nimport json\nimport os\nimport re\nfrom tqdm import tqdm\nimport numpy as np\nfrom skimage import transform\nfrom collections import namedtuple\nfrom imageio import imsave, imread\nimport requests\nfrom PIL import Image\nfrom itertools import filterfalse, chain\nfrom typing import List, Tuple, Dict\nimport cv2\nfrom . import PAGE\n\n\n# To define before using the corresponding functions\n# iiif_password = os.environ[""IIIF_PWD""]\niiif_password = \'\'\n\n\nWorkingItem = namedtuple(\n    ""WorkingItem"", [\n        \'collection\',\n        \'image_name\',\n        \'original_x\',\n        \'original_y\',\n        \'reduced_x\',\n        \'reduced_y\',\n        \'iiif\',\n        \'annotations\'\n    ]\n)\nWorkingItem.__doc__ = """"""\nA container for annotated images.\n\n:param str collection: name of the collection\n:param str image_name: name of the image\n:param int original_x: original image x size (width)\n:param int original_y: original image y size (height)\n:param int reduced_x: resized x size\n:param int reduced_y: resized y size\n:param str iiif: iiif url\n:param dict annotations: VIA \'region_attributes\'\n""""""\n\n\nVIAttribute = namedtuple(\n    ""VIAttribute"", [\n        \'name\',\n        \'type\',\n        \'options\'\n    ]\n)\nVIAttribute.__doc__ = """"""\nA container for VIA attributes.\n\n:param str name: The name of attribute\n:param str type: The type of the annotation (dropdown, markbox, ...)\n:param list options: The options / labels possible for this attribute.\n""""""\n\n\ndef parse_via_attributes(via_attributes: dict) -> List[VIAttribute]:\n    """"""\n    Parses the VIA attribute dictionary and returns a list of VIAttribute instances\n\n    :param via_attributes: attributes from VIA annotation (\'_via_attributes\' field)\n    :return: list of ``VIAttribute``\n    """"""\n\n    if {\'file\', \'region\'}.issubset(set(via_attributes.keys())):\n        via_attributes = via_attributes[\'region\']\n\n    list_attributes = list()\n    for k, v in via_attributes.items():\n        if v[\'type\'] == \'text\':\n            print(\'WARNING : Please do not use text type for attributes because it is more prone to errors/typos which \'\n                  \'can make the parsing fail. Use instead ""checkbox"", ""dropdown"" or ""radio"" with defined options.\')\n            options = None\n        else:\n            options = list(v[\'options\'].keys())\n\n        list_attributes.append(VIAttribute(k,\n                                           v[\'type\'],\n                                           options))\n\n    return list_attributes\n\n\ndef get_annotations_per_file(via_dict: dict,\n                             name_file: str) -> dict:\n    """"""\n    From VIA json content, get annotations relative to the given `name_file`.\n\n    :param via_dict: VIA annotations content (originally json)\n    :param name_file: the file to look for (it can be a iiif path or a file path)\n    :return: dict\n    """"""\n\n    # Check that the annotation_dict is a ""via_project"" file (project export),\n    # or a ""via_region"" file (annotation export)\n    if \'_via_img_metadata\' in via_dict.keys():\n        annotation_dict = via_dict[\'_via_img_metadata\']\n    else:\n        annotation_dict = via_dict\n\n    #  If it looks like a iiif path add ""-1""\n    if \'http\' in name_file:\n        key = name_file + ""-1""\n    else:\n        # find the key that contains the name_file\n        list_keys = list(filterfalse(lambda x: name_file not in x, list(annotation_dict.keys())))\n        assert len(list_keys) == 1, ""There is more than one key for the file \'{} : \\n{}\'"".format(name_file, list_keys)\n        key = list_keys[0]\n\n    if key in annotation_dict.keys():\n        myannotation = annotation_dict[key]\n        if name_file == myannotation[\'filename\']:\n            return myannotation[\'regions\']\n        else:\n            return None\n\n\ndef _compute_reduced_dimensions(x: int,\n                                y: int,\n                                target_h: int = 2000) -> Tuple[int, int]:\n    """"""\n    Compute new dimensions with height set to `target_h`.\n\n    :param x: height\n    :param y: width\n    :param target_h: target height\n    :return: tuple\n    """"""\n    ratio = y / x\n    target_w = int(target_h * ratio)\n    return target_h, target_w\n\n\ndef _collect_working_items_from_local_images(via_annotations: dict,\n                                             images_dir: str,\n                                             collection_name: str) -> List[WorkingItem]:\n    """"""\n    Given VIA annotation input, collect all info on `WorkingItem` object, when images come from local files\n\n    :param via_annotations: via_annotations: via annotations (\'regions\' field)\n    :param images_dir: directory where to find the images\n    :param collection_name: name of the collection\n    :return:\n    """"""\n\n    def _formatting(name_id: str) -> str:\n        name_id = re.sub(\'.jpg\\d*\', \'.jpg\', name_id)\n        name_id = re.sub(\'.JPG\\d*\', \'.JPG\', name_id)\n        name_id = re.sub(\'.png\\d*\', \'.png\', name_id)\n        return name_id\n\n    def _get_image_shape_without_loading(filename: str) -> Tuple[int, int]:\n        image = Image.open(filename)\n        shape = image.size\n        image.close()\n        return shape\n\n    working_items = list()\n\n    for key, v in tqdm(via_annotations.items()):\n        filename = _formatting(key)\n\n        absolute_filename = os.path.join(images_dir, filename)\n        shape_image = _get_image_shape_without_loading(absolute_filename)\n\n        regions = v[\'regions\']\n\n        if regions:\n            wk_item = WorkingItem(collection=collection_name,\n                                  image_name=filename.split(\'.\')[0],\n                                  original_x=shape_image[0],\n                                  original_y=shape_image[1],\n                                  reduced_x=None,\n                                  reduced_y=None,\n                                  iiif=None,\n                                  annotations=regions)\n\n            working_items.append(wk_item)\n\n    return working_items\n\n\ndef _collect_working_items_from_iiif(via_annotations: dict,\n                                     collection_name: str,\n                                     iiif_user = \'my-team\') -> dict:\n    """"""\n    Given VIA annotation input, collect all info on `WorkingItem` object, when the images come from IIIF urls\n\n    :param via_annotations: via_annotations: via annotations (\'regions\' field)\n    :param collection_name: name of the collection\n    :param iiif_user: user param for requests.Session().get()\n    :return:\n    """"""\n\n    working_items = list()\n    session = requests.Session()\n\n    for key, v in tqdm(via_annotations.items()):\n        iiif_url = v[\'filename\']\n\n        image_name = os.path.basename(iiif_url.split(\'/full/full/\')[0])\n\n        # get image dimensions\n        iiif_json = iiif_url.replace(""default.jpg"", ""info.json"")\n        resp_json = session.get(iiif_json, auth=(iiif_user, iiif_password))\n        if resp_json.status_code == requests.codes.ok:\n            y = resp_json.json()[\'height\']\n            x = resp_json.json()[\'width\']\n            # target_h, target_w = _compute_reduced_dimensions(x, y)\n            target_h, target_w = None, None\n        else:\n            x, y, target_w, target_h = None, None, None, None\n            resp_json.raise_for_status()\n\n        regions = v[\'regions\']\n\n        if regions:\n            wk_item = WorkingItem(collection=collection_name,\n                                  image_name=image_name.split(\'.\')[0],\n                                  original_x=x,\n                                  original_y=y,\n                                  reduced_x=target_w,\n                                  reduced_y=target_h,\n                                  iiif=iiif_url,\n                                  annotations=regions)\n\n            working_items.append(wk_item)\n\n    return working_items\n\n\ndef collect_working_items(via_annotations: dict,\n                          collection_name: str,\n                          images_dir: str = None,\n                          via_version: int = 2) -> List[WorkingItem]:\n    """"""\n    Given VIA annotation input, collect all info on `WorkingItem` object.\n    This function will take care of separating images from local files and images from IIIF urls.\n\n    :param via_annotations: via annotations (\'regions\' field)\n    :param images_dir: directory where to find the images\n    :param collection_name: name of the collection\n    :param via_version: version of the VIA tool used to produce the annotations (1 or 2)\n    :return: list of `WorkingItem`\n    """"""\n\n    via_annotations_v2 = via_annotations.copy()\n    if via_version == 1:\n        for key, value in via_annotations_v2.items():\n            list_regions = list()\n            for v_region in value[\'regions\'].values():\n                list_regions.append(v_region)\n            via_annotations_v2[key][\'regions\'] = list_regions\n\n    local_annotations = {k: v for k, v in via_annotations_v2.items() if \'http\' not in k}\n    url_annotations = {k: v for k, v in via_annotations_v2.items() if \'http\' in k}\n\n    working_items = list()\n    if local_annotations:\n        assert images_dir is not None\n        working_items += _collect_working_items_from_local_images(local_annotations, images_dir, collection_name)\n    if url_annotations:\n        working_items += _collect_working_items_from_iiif(url_annotations, collection_name)\n\n    return working_items\n\n\ndef _scale_down_original(working_item,\n                         img_out_dir: str) -> None:\n    """"""\n    Copy and reduce original image files.\n\n    :param img_out_dir: where to put the downscaled images\n    :param working_item: dict of `WorkingItems`\n    :return: None\n    """"""\n\n    def _getimage_from_iiif(url, user, pwd):\n        img = requests.get(url, auth=(user, pwd))\n        return imread(img.content)\n\n    image_set_dir = os.path.join(img_out_dir, working_item.collection, ""images"")\n    if not os.path.exists(image_set_dir):\n        try:\n            os.makedirs(image_set_dir)\n        except OSError as e:\n            if e.errno != os.errno.EEXIST:\n                raise\n            pass\n\n    outfile = os.path.join(image_set_dir, working_item.image_name + ""_ds.png"")\n    if not os.path.isfile(outfile):\n        img = _getimage_from_iiif(working_item.iiif, \'epfl-team\', iiif_password)\n        img_resized = transform.resize(\n            img,\n            [working_item.reduced_y, working_item.reduced_x],\n            anti_aliasing=False,\n            preserve_range=True\n        )\n        imsave(outfile, img_resized.astype(np.uint8))\n\n\ndef load_annotation_data(via_data_filename: str,\n                         only_img_annotations: bool = False,\n                         via_version: int = 2) -> dict:\n    """"""\n    Load the content of via annotation files.\n\n    :param via_data_filename: via annotations json file\n    :param only_img_annotations: load only the images annotations (\'_via_img_metadata\' field)\n    :param via_version:\n    :return: the content of json file containing the region annotated\n    """"""\n\n    with open(via_data_filename, \'r\', encoding=\'utf8\') as f:\n        content = json.load(f)\n    if via_version == 2:\n        assert \'_via_img_metadata\' in content.keys(), ""The file is not a valid VIA project export.""\n\n        if only_img_annotations:\n            return content[\'_via_img_metadata\']\n        else:\n            return content\n    else:\n        return content\n\n\ndef export_annotation_dict(annotation_dict: dict,\n                           filename: str) -> None:\n    """"""\n    Export the annotations to json file.\n\n    :param annotation_dict: VIA annotations\n    :param filename: filename to export the data (json file)\n    :return:\n    """"""\n    with open(filename, \'w\', encoding=\'utf8\') as f:\n        json.dump(annotation_dict, f)\n\n\ndef get_via_attributes(annotation_dict: dict,\n                       via_version: int = 2) -> List[VIAttribute]:\n    """"""\n    Gets the attributes of the annotated data and returns a list of `VIAttribute`.\n\n    :param annotation_dict: json content of the VIA exported file\n    :param via_version: either 1 or 2 (for VIA v 1.0 or VIA v 2.0)\n    :return: A list containing VIAttributes\n    """"""\n\n    if via_version == 1:\n\n        list_attributes = [list(region[\'region_attributes\'].keys())\n                           for value in annotation_dict.values()\n                           for region in value[\'regions\'].values()]\n\n        # Find options\n        unique_attributes = list(np.unique(list(chain.from_iterable(list_attributes))))\n\n        dict_labels = {rgn_att: list() for rgn_att in unique_attributes}\n        for value in annotation_dict.values():\n            regions = value[\'regions\']\n            for region in regions.values():\n                for k, v in region[\'region_attributes\'].items():\n                    dict_labels[k].append(v)\n\n    elif via_version == 2:\n\n        if \'_via_attributes\' in annotation_dict.keys():  # If project_export is given\n            return parse_via_attributes(annotation_dict[\'_via_attributes\'])\n\n        else:  # else if annotation_export is given\n\n            list_attributes = [list(region[\'region_attributes\'].keys())\n                               for value in annotation_dict.values()\n                               for region in value[\'regions\']]\n\n            # Find options\n            unique_attributes = list(np.unique(list(chain.from_iterable(list_attributes))))\n\n            dict_labels = {rgn_att: list() for rgn_att in unique_attributes}\n            for value in annotation_dict.values():\n                regions = value[\'regions\']\n                for region in regions:\n                    for k, v in region[\'region_attributes\'].items():\n                        dict_labels[k].append(v)\n\n    else:\n        raise NotImplementedError\n\n    # Instantiate VIAttribute objects\n    viattribute_list = list()\n    for attribute, options in dict_labels.items():\n\n        if all(isinstance(opt, str) for opt in options):\n            viattribute_list.append(VIAttribute(name=attribute,\n                                                type=None,\n                                                options=list(np.unique(options))))\n\n        elif all(isinstance(opt, dict) for opt in options):\n            viattribute_list.append(VIAttribute(name=attribute,\n                                                type=None,\n                                                options=list(np.unique(list(chain.from_iterable(options))))))\n\n        else:\n            raise NotImplementedError\n    return viattribute_list\n\n\ndef _draw_mask(via_region: dict,\n               mask: np.array,\n               contours_only: bool = False) -> np.array:\n    """"""\n\n    :param via_region: region to draw (in VIA format)\n    :param mask: image mask to draw on\n    :param contours_only: if `True`, draws only the contours of the region, if `False`, fills the region\n    :return: the drawn mask\n    """"""\n\n    shape_attributes_dict = via_region[\'shape_attributes\']\n\n    if shape_attributes_dict[\'name\'] == \'rect\':\n        x = shape_attributes_dict[\'x\']\n        y = shape_attributes_dict[\'y\']\n        w = shape_attributes_dict[\'width\']\n        h = shape_attributes_dict[\'height\']\n\n        contours = np.array([[x, y],\n                             [x + w, y],\n                             [x + w, y + h],\n                             [x, y + h]\n                             ]).reshape((-1, 1, 2))\n\n        mask = cv2.polylines(mask, [contours], True, 255, thickness=15) if contours_only \\\n            else cv2.fillPoly(mask, [contours], 255)\n\n    elif shape_attributes_dict[\'name\'] == \'polygon\':\n        contours = np.stack([shape_attributes_dict[\'all_points_x\'],\n                             shape_attributes_dict[\'all_points_y\']], axis=1)[:, None, :]\n\n        mask = cv2.polylines(mask, [contours], True, 255, thickness=15) if contours_only \\\n            else cv2.fillPoly(mask, [contours], 255)\n\n    elif shape_attributes_dict[\'name\'] == \'circle\':\n        center_point = (shape_attributes_dict[\'cx\'], shape_attributes_dict[\'cy\'])\n        radius = shape_attributes_dict[\'r\']\n\n        mask = cv2.circle(mask, center_point, radius, 255, thickness=15) if contours_only \\\n            else cv2.circle(mask, center_point, radius, 255, thickness=-1)\n\n    elif shape_attributes_dict[\'name\'] == \'polyline\':\n        contours = np.stack([shape_attributes_dict[\'all_points_x\'],\n                             shape_attributes_dict[\'all_points_y\']], axis=1)[:, None, :]\n\n        mask = cv2.polylines(mask, [contours], False, 255, thickness=15)\n\n    else:\n        raise NotImplementedError(\n            \'Mask annotation for shape of type ""{}"" has not been implemented yet\'\n                .format(shape_attributes_dict[\'name\']))\n\n    return mask\n\n\ndef _write_mask(mask: np.ndarray,\n                masks_dir: str,\n                collection: str,\n                image_name: str,\n                label: str) -> None:\n    """"""\n    Save a mask with filename containing \'label\'.\n\n    :param mask: mask b&w image (H, W)\n    :param masks_dir: directory to output mask\n    :param collection: name of the collection\n    :param image_name: name of the image\n    :param label: label of the mask\n    :return:\n    """"""\n\n    outdir = os.path.join(masks_dir, collection, image_name)\n    if not os.path.exists(outdir):\n        os.makedirs(outdir)\n    label = label.strip(\' \\n\').replace("" "", ""_"").lower() if label is not None else \'nolabel\'\n    outfile = os.path.join(outdir, image_name + ""-mask-"" + label + "".png"")\n    imsave(outfile, mask.astype(np.uint8))\n\n\ndef create_masks(masks_dir: str,\n                 working_items: List[WorkingItem],\n                 via_attributes: List[VIAttribute],\n                 collection: str,\n                 contours_only: bool = False) -> dict:\n    """"""\n    For each annotation, create a corresponding binary mask and resize it (h = 2000). Only valid for VIA 2.0.\n    Several annotations of the same class on the same image produce one image with several masks.\n\n    :param masks_dir: where to output the masks\n    :param working_items: infos to work with\n    :param via_attributes: VIAttributes computed by ``get_via_attributes`` function.\n    :param collection: name of the nollection\n    :param contours_only: creates the binary masks only for the contours of the object (thickness of contours : 20 px)\n    :return: annotation_summary, a dictionary containing a list of labels per image\n    """"""\n\n    def resize_and_write_mask(mask_image: np.ndarray, working_item: WorkingItem, label_item: str) -> None:\n        """"""\n        Resize only if needed (if working_item.reduced != working_item.original)\n\n        :param mask_image: mask image to write\n        :param working_item: `WorkingItem` object\n        :param label_item: label name to append to filename\n        :return:\n        """"""\n\n        if not working_item.reduced_y and not working_item.reduced_x:\n            _write_mask(mask_image, masks_dir, collection, working_item.image_name, label_item)\n\n        elif working_item.reduced_x != working_item.original_x and working_item.reduced_y != working_item.original_y:\n            mask_resized = transform.resize(mask_image,\n                                            [working_item.reduced_y, working_item.reduced_x],\n                                            anti_aliasing=False,\n                                            preserve_range=True,\n                                            order=0)\n            _write_mask(mask_resized, masks_dir, collection, working_item.image_name, label_item)\n\n        else:\n            _write_mask(mask_image, masks_dir, collection, working_item.image_name, label_item)\n    # -------------------\n\n    print(""Creating masks in {}..."".format(masks_dir))\n\n    annotation_summary = dict()\n\n    for wi in tqdm(working_items, desc=""workingItem2mask""):\n        labels = list()\n\n        # the image has no annotation, writing a black mask:\n        if not wi.annotations:\n            mask = np.zeros([wi.original_y, wi.original_x], np.uint8)\n            resize_and_write_mask(mask, wi, None)\n            labels.append(""nolabel"")\n\n        # check all possible labels for the image and create mask:\n        else:\n            for attribute in via_attributes:\n                for option in attribute.options:\n                    # get annotations that have the current attribute\n                    selected_regions = list(filter(lambda r: attribute.name in r[\'region_attributes\'].keys(),\n                                                   wi.annotations))\n                    # get annotations that have the current attribute and option\n                    if selected_regions:\n                        selected_regions = list(filter(lambda r: r[\'region_attributes\'][attribute.name] == option,\n                                                       selected_regions))\n                    else:\n                        continue\n\n                    if selected_regions:\n                        # create a 0 matrix (black background)\n                        mask = np.zeros([wi.original_y, wi.original_x], np.uint8)\n\n                        # nb: if 2 labels are on the same page, they belongs to the same mask\n                        for sr in selected_regions:\n                            mask = _draw_mask(sr, mask, contours_only)\n\n                        label = \'{}-{}\'.format(attribute.name, option).lower()\n                        resize_and_write_mask(mask, wi, label)\n                        # add to existing labels\n                        labels.append(label)\n\n        # write summary: list of existing labels per image\n        annotation_summary[wi.image_name] = labels\n        outfile = os.path.join(masks_dir, collection, collection + ""-classes.txt"")\n        with open(outfile, \'a\') as fh:\n            for a in annotation_summary:\n                fh.write(a + ""\\t"" + str(annotation_summary[a]) + ""\\n"")\n\n    print(""Done."")\n    return annotation_summary\n\n\ndef _get_coordinates_from_xywh(via_regions: List[dict]) -> List[np.array]:\n    """"""\n    From VIA region dictionaries, get the coordinates array (N,2) of the annotations\n\n    :param via_regions:\n    :return:\n    """"""\n    list_coordinates_regions = list()\n    for region in via_regions:\n        shape_attributes_dict = region[\'shape_attributes\']\n        if shape_attributes_dict[\'name\'] == \'rect\':\n            x = shape_attributes_dict[\'x\']\n            y = shape_attributes_dict[\'y\']\n            w = shape_attributes_dict[\'width\']\n            h = shape_attributes_dict[\'height\']\n\n            coordinates = np.array([[x, y],\n                                    [x + w, y],\n                                    [x + w, y + h],\n                                    [x, y + h]\n                                    ])\n            list_coordinates_regions.append(coordinates)\n        elif shape_attributes_dict[\'name\'] == \'polygon\':\n            coordinates = np.stack([shape_attributes_dict[\'all_points_x\'],\n                                    shape_attributes_dict[\'all_points_y\']], axis=1)\n            list_coordinates_regions.append(coordinates)\n        elif shape_attributes_dict[\'name\'] == \'polyline\':\n            coordinates = np.stack([shape_attributes_dict[\'all_points_x\'],\n                                    shape_attributes_dict[\'all_points_y\']], axis=1)\n            list_coordinates_regions.append(coordinates)\n        else:\n            raise NotImplementedError(\n                ""This method has not been implemenetd yet for {}"".format(shape_attributes_dict[\'name\']))\n\n    return list_coordinates_regions\n\n\n# EXPORT\n# ------\n\ndef _get_xywh_from_coordinates(coordinates: np.array) -> Tuple[int, int, int, int]:\n    """"""\n    From coordinates points get x,y, width, height\n\n    :param coordinates: (N,2) coordinates (x,y)\n    :return: x, y, w, h\n    """"""\n\n    x = np.min(coordinates[:, 0])\n    y = np.min(coordinates[:, 1])\n    w = np.max(coordinates[:, 0]) - x\n    h = np.max(coordinates[:, 1]) - y\n\n    return x, y, w, h\n\n\ndef create_via_region_from_coordinates(coordinates: np.array,\n                                       region_attributes: dict,\n                                       type_region: str) -> dict:\n    """"""\n    Formats coordinates to a VIA region (dict).\n\n    :param coordinates: (N, 2) coordinates (x, y)\n    :param region_attributes: dictionary with keys : name of labels, values : values of labels\n    :param type_region: via region annotation type (\'rect\', \'polygon\')\n    :return: a region in VIA style (dict/json)\n    """"""\n    assert type_region in [\'rect\', \'polygon\', \'circle\']\n\n    if type_region == \'rect\':\n        x, y, w, h = _get_xywh_from_coordinates(coordinates)\n        shape_atributes = {\n            \'name\': \'rect\',\n            \'height\': int(h),\n            \'width\': int(w),\n            \'x\': int(x),\n            \'y\': int(y)\n        }\n    elif type_region == \'polygon\':\n        points_x = list(coordinates[:, 0])\n        points_y = list(coordinates[:, 1])\n\n        shape_atributes = {\n            \'name\': \'polygon\',\n            \'all_points_x\': [int(p) for p in points_x],\n            \'all_points_y\': [int(p) for p in points_y],\n        }\n    elif type_region == \'circle\':\n        raise NotImplementedError(\'The type {} is not supported for the export.\'.format(type))\n\n    return {\'region_attributes\': region_attributes,\n            \'shape_attributes\': shape_atributes}\n\n\ndef create_via_annotation_single_image(img_filename: str,\n                                       via_regions: List[dict],\n                                       file_attributes: dict = None) -> Dict[str, dict]:\n    """"""\n    Returns a dictionary item {key: annotation} in VIA format to further export to .json file\n\n    :param img_filename: path to the image\n    :param via_regions: regions in VIA format (output from ``create_via_region_from_coordinates``)\n    :param file_attributes: file attributes (usually None)\n    :return: dictionary item with key and annotations in VIA format\n    """"""\n    if \'http\' in img_filename:\n        basename = img_filename\n        file_size = -1\n    else:\n        basename = os.path.basename(img_filename)\n        file_size = os.path.getsize(img_filename)\n\n    via_key = \'{}{}\'.format(basename, file_size)\n\n    via_annotation = {\n        \'file_attributes\': file_attributes if file_attributes is not None else dict(),\n        \'filename\': basename,\n        \'size\': file_size,\n        \'regions\': via_regions\n    }\n\n    return {via_key: via_annotation}\n\n\n# PAGE CONVERSION\n# ---------------\n\ndef convert_via_region_page_text_region(working_item: WorkingItem,\n                                        structure_label: str) -> PAGE.Page:\n    """"""\n\n    :param working_item:\n    :param structure_label:\n    :return:\n    """"""\n\n    # TODO : this is not yet generic because we\'re missing the automatic detection of the structure label\n\n    region_coordinates = _get_coordinates_from_xywh(working_item.annotations)\n\n    page = PAGE.Page(image_filename=working_item.image_name + \'jpg\',\n                     image_width=working_item.original_x,\n                     image_height=working_item.original_y,\n                     graphic_regions=[\n                         PAGE.TextRegion(coords=PAGE.Point.array_to_point(coords),\n                                         custom_attribute=\'structure{{type:{};}}\'.format(structure_label))\n                         for coords in region_coordinates])\n    return page\n\n\n# def convert_via_region_to_text_region(via_regions: List[dict], structure_label: str) -> PAGE.TextRegion:\n#     """"""\n#\n#     :param via_region:\n#     :param structure_label:\n#     :return:\n#     """"""\n#\n#     # TODO : this is not yet generic because we\'re missing the automatic detection of the structure label\n#\n#     region_coordinates = _get_coordinates_from_xywh(working_item.annotations)\n#\n#     page = PAGE.Page(image_filename=working_item.image_name + \'jpg\',\n#                      image_width=working_item.original_x,\n#                      image_height=working_item.original_y,\n#                      graphic_regions=[\n#                          PAGE.TextRegion(coords=PAGE.Point.array_to_point(coords),\n#                                          custom_attribute=\'structure{{type:{};}}\'.format(structure_label))\n#                          for coords in region_coordinates])\n#     return page\n\n\n""""""\nExample of usage\n\n\ncollection = \'mycollection\'\nannotation_file = \'via_sample.json\'\nmasks_dir = \'/home/project/generated_masks\'\nimages_dir = \'./my_images\'\n\n# Load all the data in the annotation file (the file may be an exported project or an export of the annotations)\nvia_data = load_annotation_data(annotation_file)\n\n# In the case of an exported project file, you can set ``only_img_annotations=True`` to get only\n# the region annotations\nvia_annotations = load_annotation_data(annotation_file, only_img_annotations=True)\n\n# Collect the annotated regions\nworking_items = collect_working_items(via_annotations, collection, images_dir)\n\n# Collect the attributes and options\nif \'_via_attributes\' in via_data.keys():\n    list_attributes = parse_via_attributes(via_data[\'_via_attributes\'])\nelse:\n    list_attributes = get_via_attributes(via_annotations)\n\n# Create one mask per option per attribute\ncreate_masks(masks_dir, wi,via_attributes, collection)\n""""""\n\n\n""""""\nContent of a via_project exported file\n\n{\'_via_attributes\': {\n    ...\n    },\n \'_via_img_metadata\': {\n    ...\n    },\n \'_via_settings\': {\n    \'core\': {\n        \'buffer_size\': 18,\n        \'default_filepath\': \'\',\n        \'filepath\': {}\n    },\n    \'project\': {\n        \'name\': \'via_project_7Feb2019_10h7m\'\n    },\n    \'ui\': {\n        \'annotation_editor_fontsize\': 0.8,\n        \'annotation_editor_height\': 25,\n        \'image\': {\n            \'region_label\': \'region_id\',\n            \'region_label_font\': \'10px Sans\'\n        },\n        \'image_grid\': {\n            \'img_height\': 80,\n            \'rshape_fill\': \'none\',\n            \'rshape_fill_opacity\': 0.3,\n            \'rshape_stroke\': \'yellow\',\n            \'rshape_stroke_width\': 2,\n            \'show_image_policy\': \'all\',\n            \'show_region_shape\': True\n        },\n        \'leftsidebar_width\': 18\n    }\n }\n}\n\n""""""\n\n""""""\n""_via_attributes"": {\n    ""region"": {\n        ""attribute1"": {\n            ""type"":""text"",\n            ""description"":"""",\n            ""default_value"":""""\n        },\n        ""attribute2"": {\n            ""type"":""dropdown"",\n            ""description"":"""",\n            ""options"": {\n                ""op1"":"""",\n                ""op2"":""""\n                },\n            ""default_options"":{}\n        },\n        ""attribute3"": {\n            ""type"":""checkbox"",\n            ""description"":"""",\n            ""options"": {\n                ""op1"":"""",\n                ""op2"":""""\n            },\n            ""default_options"":{}\n        },\n        ""attribute 4"": {\n            ""type"":""radio"",\n            ""description"":"""",\n            ""options"": {\n                ""op1"":"""",\n                ""op2"":""""\n            },\n            ""default_options"":{}\n        }\n    },\n    ""file"":{}\n}\n\n""""""\n\n""""""\n\'_via_img_metadata\': {\n    \'image_filename1.jpg2209797\': {\n        \'file_attributes\': {},\n        \'filename\': \'image_filename1.jpg\',\n        \'regions\':\n            [{\n                \'region_attributes\': {\n                    \'attribute1\': {\n                        \'op1\': True,\n                        \'op2\': True\n                    },\n                    \'attribute 2\': \'label1\',\n                    \'attribute 3\': \'op1\'\n                },\n                \'shape_attributes\': {\n                    \'height\': 2277,\n                    \'name\': \'rect\',\n                    \'width\': 1541,\n                    \'x\': 225,\n                    \'y\': 458\n                }\n            },\n            {\n                \'region_attributes\': {\n                    \'attribute 4\': \'op1\',\n                    \'attribute 1\': {},\n                    \'attribute 2\': \'label1\',\n                    \'attribute 3\': \'op2\'\n                },\n                \'shape_attributes\': {\n                    \'height\': 2255,\n                    \'name\': \'rect\',\n                    \'width\': 1554,\n                    \'x\': 1845,\n                    \'y\': 476\n                }\n            }],\n            \'size\': 2209797},\n    \'https://libimages.princeton.edu/loris/pudl0001/5138415/00000011.jp2/full/full/0/default.jpg-1\': {\n        \'file_attributes\': {},\n        \'filename\': \'https://libimages.princeton.edu/loris/pudl0001/5138415/00000011.jp2/full/full/0/default.jpg\',\n        \'regions\':\n            [{\n                \'region_attributes\': {\n                    \'attribute 4\': \'op2\',\n                    \'attribute 1\': {\n                        \'op1\': True\n                    },\n                    \'attribute 2\': \'label3\',\n                    \'attribute 3\': \'op1\'\n                },\n                \'shape_attributes\': {\n                    \'height\': 1026,\n                    \'name\': \'rect\',\n                    \'width\': 1430,\n                    \'x\': 145,\n                    \'y\': 525\n                }\n            },\n            {\n                \'region_attributes\': {\n                    \'attribute 4\': \'op2\',\n                    \'attribute 1\': {\n                        \'op1\': True},\n                    \'attribute 2\': \'label 3 \',\n                    \'attribute 3\': \'op1\',\n                },\n                \'shape_attributes\': {\n                    \'all_points_x\': [2612, 2498, 2691, 2757, 2962, 3034, 2636],\n                    \'all_points_y\': [5176, 5616, 5659, 5363, 5375, 5110, 5122],\n                    \'name\': \'polygon\'\n                }\n            },\n            {\n                \'region_attributes\': {\n                    \'attribute 4\': \'op2\',\n                    \'attribute 1\': {\n                        \'op1\': True},\n                    \'attribute 2\': \'label 3 \',\n                    \'attribute 3\': \'op1\',\n                },\n                \'shape_attributes\': {\n                    \'cx\': 2793,\n                    \'cy\': 881,\n                    \'name\': \'circle\',\n                    \'r\': 524\n                }\n            },\n            {\n                \'region_attributes\': {\n                    \'attribute 4\': \'op1\',\n                    \'attribute 1\': {\n                        \'op2\': True},\n                    \'attribute 2\': \'label1\',\n                    \'attribute 3\': \'op2\',\n                },\n                \'shape_attributes\': {\n                    \'all_points_x\': [3246, 5001],\n                    \'all_points_y\': [422, 380],\n                    \'name\': \'polyline\'\n                }\n            }],\n        \'size\': -1\n    }\n}\n""""""\n'"
dh_segment/network/__init__.py,0,"b""_MODEL = [\n    'inference_vgg16',\n    'inference_resnet_v1_50',\n    'inference_u_net',\n    'vgg_16_fn',\n    'resnet_v1_50_fn'\n]\n\n__all__ = _MODEL\n\nfrom .model import *\nfrom .pretrained_models import *\n"""
dh_segment/network/model.py,41,"b'#!/usr/bin/env python\n\nimport tensorflow as tf\nfrom ..utils import ModelParams\nfrom tensorflow.contrib import layers  # TODO migration to tf.layers ?\nfrom tensorflow.contrib.slim.nets import resnet_v1\nfrom tensorflow.contrib.slim import arg_scope\nfrom .pretrained_models import vgg_16_fn, resnet_v1_50_fn\nfrom collections import OrderedDict\n\n\ndef inference_vgg16(images: tf.Tensor, params: ModelParams, num_classes: int, use_batch_norm=False, weight_decay=0.0,\n                    is_training=False) -> tf.Tensor:\n    with tf.name_scope(\'vgg_augmented\'):\n\n        if use_batch_norm:\n            if params.batch_renorm:\n                renorm_clipping = {\'rmax\': 100, \'rmin\': 0.1, \'dmax\': 10}\n                renorm_momentum = 0.98\n            else:\n                renorm_clipping = None\n                renorm_momentum = 0.99\n            batch_norm_fn = lambda x: tf.layers.batch_normalization(x, axis=-1, training=is_training, name=\'batch_norm\',\n                                                                    renorm=params.batch_renorm,\n                                                                    renorm_clipping=renorm_clipping,\n                                                                    renorm_momentum=renorm_momentum)\n        else:\n            batch_norm_fn = None\n\n        def upsample_conv(pooled_layer, previous_layer, layer_params, number):\n            with tf.name_scope(\'deconv{}\'.format(number)):\n                if previous_layer.get_shape()[1].value and previous_layer.get_shape()[2].value:\n                    target_shape = previous_layer.get_shape()[1:3]\n                else:\n                    target_shape = tf.shape(previous_layer)[1:3]\n                upsampled_layer = tf.image.resize_images(pooled_layer, target_shape,\n                                                         method=tf.image.ResizeMethod.BILINEAR)\n                input_tensor = tf.concat([upsampled_layer, previous_layer], 3)\n\n                for i, (nb_filters, filter_size) in enumerate(layer_params):\n                    input_tensor = layers.conv2d(\n                        inputs=input_tensor,\n                        num_outputs=nb_filters,\n                        kernel_size=[filter_size, filter_size],\n                        normalizer_fn=batch_norm_fn,\n                        scope=""conv{}_{}"".format(number, i + 1)\n                    )\n            return input_tensor\n\n        # Original VGG :\n        vgg_net, intermediate_levels = vgg_16_fn(images, blocks=5, weight_decay=weight_decay)\n        out_tensor = vgg_net\n\n        # Intermediate convolution\n        if params.intermediate_conv is not None:\n            with tf.name_scope(\'intermediate_convs\'):\n                for layer_params in params.intermediate_conv:\n                    for k, (nb_filters, filter_size) in enumerate(layer_params):\n                        out_tensor = layers.conv2d(inputs=out_tensor,\n                                                   num_outputs=nb_filters,\n                                                   kernel_size=[filter_size, filter_size],\n                                                   normalizer_fn=batch_norm_fn,\n                                                   scope=\'conv_{}\'.format(k + 1))\n\n        # Upsampling :\n        with tf.name_scope(\'upsampling\'):\n            selected_upscale_params = [l for i, l in enumerate(params.upscale_params)\n                                       if params.selected_levels_upscaling[i]]\n\n            assert len(params.selected_levels_upscaling) == len(intermediate_levels), \\\n                \'Upscaling : {} is different from {}\'.format(len(params.selected_levels_upscaling),\n                                                             len(intermediate_levels))\n\n            selected_intermediate_levels = [l for i, l in enumerate(intermediate_levels)\n                                            if params.selected_levels_upscaling[i]]\n\n            # Upsampling loop\n            n_layer = 1\n            for i in reversed(range(len(selected_intermediate_levels))):\n                out_tensor = upsample_conv(out_tensor, selected_intermediate_levels[i],\n                                           selected_upscale_params[i], n_layer)\n                n_layer += 1\n\n            logits = layers.conv2d(inputs=out_tensor,\n                                   num_outputs=num_classes,\n                                   activation_fn=None,\n                                   kernel_size=[1, 1],\n                                   scope=""conv{}-logits"".format(n_layer))\n\n        return logits  # [B,h,w,Classes]\n\n\ndef inference_resnet_v1_50(images, params, num_classes, use_batch_norm=False, weight_decay=0.0,\n                           is_training=False) -> tf.Tensor:\n    if use_batch_norm:\n        if params.batch_renorm:\n            renorm_clipping = {\'rmax\': 100, \'rmin\': 0.1, \'dmax\': 1}\n            renorm_momentum = 0.98\n        else:\n            renorm_clipping = None\n            renorm_momentum = 0.99\n        batch_norm_fn = lambda x: tf.layers.batch_normalization(x, axis=-1, training=is_training, name=\'batch_norm\',\n                                                                renorm=params.batch_renorm,\n                                                                renorm_clipping=renorm_clipping,\n                                                                renorm_momentum=renorm_momentum)\n    else:\n        batch_norm_fn = None\n\n    def upsample_conv(input_tensor, previous_intermediate_layer, layer_params, number) -> tf.Tensor:\n        """"""\n        Deconvolution (upscaling) layers\n\n        :param input_tensor:\n        :param previous_intermediate_layer:\n        :param layer_params:\n        :param number:\n        :return:\n        """"""\n        with tf.variable_scope(\'deconv_{}\'.format(number)):\n            if previous_intermediate_layer.get_shape()[1].value and \\\n                    previous_intermediate_layer.get_shape()[2].value:\n                target_shape = previous_intermediate_layer.get_shape()[1:3]\n            else:\n                target_shape = tf.shape(previous_intermediate_layer)[1:3]\n            upsampled_layer = tf.image.resize_images(input_tensor, target_shape,\n                                                     method=tf.image.ResizeMethod.BILINEAR)\n            net = tf.concat([upsampled_layer, previous_intermediate_layer], 3)\n\n            filter_size, nb_bottlenecks = layer_params\n            if nb_bottlenecks > 0:\n                for i in range(nb_bottlenecks):\n                    net = resnet_v1.bottleneck(\n                        inputs=net,\n                        depth=filter_size,\n                        depth_bottleneck=filter_size // 4,\n                        stride=1\n                    )\n            else:\n                net = layers.conv2d(\n                    inputs=net,\n                    num_outputs=filter_size,\n                    kernel_size=[3, 3],\n                    scope=""conv{}"".format(number)\n                )\n\n        return net\n\n    # Original ResNet\n    blocks_needed = max([i for i, is_needed in enumerate(params.selected_levels_upscaling) if is_needed])\n    resnet_net, intermediate_layers = resnet_v1_50_fn(images, is_training=False, blocks=blocks_needed,\n                                                      weight_decay=weight_decay, renorm=False,\n                                                      corrected_version=params.correct_resnet_version)\n\n    # Upsampling\n    with tf.variable_scope(\'upsampling\'):\n        with arg_scope([layers.conv2d],\n                       normalizer_fn=batch_norm_fn,\n                       weights_regularizer=layers.l2_regularizer(weight_decay)):\n            selected_upscale_params = [l for i, l in enumerate(params.upscale_params)\n                                       if params.selected_levels_upscaling[i]]\n\n            assert len(selected_upscale_params) == len(intermediate_layers), \\\n                \'Upscaling : {} is different from {}\'.format(len(selected_upscale_params),\n                                                             len(intermediate_layers))\n\n            selected_intermediate_levels = [l for i, l in enumerate(intermediate_layers)\n                                            if params.selected_levels_upscaling[i]]\n\n            # Rescaled image values to [0,1]\n            selected_intermediate_levels.insert(0, images/255.0)\n\n            # Force layers to not be too big to reduce memory usage\n            for i, l in enumerate(selected_intermediate_levels):\n                if l.get_shape()[-1] > params.max_depth:\n                    selected_intermediate_levels[i] = layers.conv2d(\n                        inputs=l,\n                        num_outputs=params.max_depth,\n                        kernel_size=[1, 1],\n                        scope=""dimreduc_{}"".format(i),\n                        # normalizer_fn=batch_norm_fn,\n                        activation_fn=None\n                    )\n\n            # Deconvolving loop\n            out_tensor = selected_intermediate_levels[-1]\n            n_layer = 1\n            for i in reversed(range(len(selected_intermediate_levels) - 1)):\n                out_tensor = upsample_conv(out_tensor, selected_intermediate_levels[i],\n                                           selected_upscale_params[i], n_layer)\n\n                n_layer += 1\n\n            if images.get_shape()[1].value and images.get_shape()[2].value:\n                target_shape = images.get_shape()[1:3]\n            else:\n                target_shape = tf.shape(images)[1:3]\n            out_tensor = tf.image.resize_images(out_tensor, target_shape,\n                                                method=tf.image.ResizeMethod.BILINEAR)\n\n        logits = layers.conv2d(inputs=out_tensor,\n                               num_outputs=num_classes,\n                               activation_fn=None,\n                               kernel_size=[1, 1],\n                               scope=""conv{}-logits"".format(n_layer))\n\n    return logits\n\n\ndef conv_bn_layer(input_tensor, kernel_size, output_channels, stride=1, bn=False,\n                  is_training=True, relu=True):\n    # with tf.variable_scope(name) as scope:\n    conv_layer = layers.conv2d(inputs=input_tensor,\n                               num_outputs=output_channels,\n                               kernel_size=kernel_size,\n                               stride=stride,\n                               activation_fn=tf.identity,\n                               padding=\'SAME\')\n    if bn and relu:\n        # How to use Batch Norm: https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/README_BATCHNORM.md\n\n        # Why scale is false when using ReLU as the next activation\n        # https://datascience.stackexchange.com/questions/22073/why-is-scale-parameter-on-batch-normalization-not-needed-on-relu/22127\n\n        # Using fuse operation: https://www.tensorflow.org/performance/performance_guide#common_fused_ops\n        conv_layer = layers.batch_norm(inputs=conv_layer, center=True, scale=False, is_training=is_training, fused=True)\n        conv_layer = tf.nn.relu(conv_layer)\n\n    if bn and not relu:\n        conv_layer = layers.batch_norm(inputs=conv_layer, center=True, scale=True, is_training=is_training)\n\n    # print(\'Conv layer {0} -> {1}\'.format(input_tensor.get_shape().as_list(),conv_layer.get_shape().as_list()))\n    return conv_layer\n\n\ndef _get_image_shape_tensor(tensor: tf.Tensor):\n    if tensor.get_shape()[1].value and \\\n                    tensor.get_shape()[2].value:\n        target_shape = tensor.get_shape()[1:3]\n    else:\n        target_shape = tf.shape(tensor)[1:3]\n    return target_shape\n\n\ndef inference_u_net(images: tf.Tensor, params: ModelParams, num_classes: int, use_batch_norm=False, weight_decay=0.0,\n                    is_training=False) -> tf.Tensor:\n    enc_layers = OrderedDict()\n    dec_layers = OrderedDict()\n\n    with tf.variable_scope(\'U-Net\'):\n\n        with tf.variable_scope(\'Encoder\'):\n\n            conv_layer = layers.conv2d(images, num_outputs=64, kernel_size=(3, 3), padding=\'SAME\',\n                                       activation_fn=tf.identity)\n\n            enc_layers[\'conv_layer_enc_64\'] = conv_bn_layer(conv_layer, kernel_size=(3, 3),\n                                                            output_channels=64,\n                                                            bn=True, is_training=is_training, relu=True)\n\n            conv_layer = layers.max_pool2d(inputs=enc_layers[\'conv_layer_enc_64\'], kernel_size=(2, 2), stride=2)\n\n            for n_feat in [128, 256, 512]:\n                enc_layers[\'conv_layer_enc_\' + str(n_feat)] = conv_bn_layer(conv_layer, kernel_size=(3, 3),\n                                                                            output_channels=n_feat,\n                                                                            bn=True,\n                                                                            is_training=is_training, relu=True)\n\n                enc_layers[\'conv_layer_enc_\' + str(n_feat)] = conv_bn_layer(\n                    enc_layers[\'conv_layer_enc_\' + str(n_feat)], kernel_size=(3, 3),\n                    output_channels=n_feat,\n                    bn=True, is_training=is_training, relu=True)\n\n                conv_layer = layers.max_pool2d(inputs=enc_layers[\'conv_layer_enc_\' + str(n_feat)], kernel_size=(2, 2), stride=2)\n\n            conv_layer_enc_1024 = conv_bn_layer(conv_layer, kernel_size=(3, 3),\n                                                output_channels=1024,\n                                                bn=True, is_training=is_training, relu=True)\n\n        with tf.variable_scope(\'Decoder\'):\n            dec_layers[\'conv_layer_dec_512\'] = conv_bn_layer(conv_layer_enc_1024, kernel_size=(3, 3),\n                                                             output_channels=512,\n                                                             bn=True, is_training=is_training, relu=True)\n\n            reduced_patchsize = _get_image_shape_tensor(enc_layers[\'conv_layer_enc_512\'])\n            dec_layers[\'conv_layer_dec_512\'] = tf.image.resize_images(dec_layers[\'conv_layer_dec_512\'], size=reduced_patchsize,\n                                                                      method=tf.image.ResizeMethod.BILINEAR)\n\n            for n_feat in [512, 256, 128, 64]:\n\n                dec_layers[\'conv_layer_dec_\' + str(n_feat * 2)] = tf.concat([dec_layers[\'conv_layer_dec_\' + str(n_feat)],\n                                                                             enc_layers[\'conv_layer_enc_\' + str(n_feat)]],\n                                                                            axis=3)\n                dec_layers[\'conv_layer_dec_\' + str(n_feat)] = conv_bn_layer(\n                    dec_layers[\'conv_layer_dec_\' + str(n_feat * 2)], kernel_size=(3, 3),\n                    output_channels=n_feat,\n                    bn=True, is_training=is_training, relu=True)\n                if n_feat > 64:\n                    dec_layers[\'conv_layer_dec_\' + str(int(n_feat / 2))] = conv_bn_layer(\n                        dec_layers[\'conv_layer_dec_\' + str(n_feat)], kernel_size=(3, 3),\n                        output_channels=n_feat / 2,\n                        bn=True, is_training=is_training, relu=True)\n\n                    reduced_patchsize = _get_image_shape_tensor(enc_layers[\'conv_layer_enc_\' + str(int(n_feat / 2))])\n                    dec_layers[\'conv_layer_dec_\' + str(int(n_feat / 2))] = tf.image.resize_images(\n                        dec_layers[\'conv_layer_dec_\' + str(int(n_feat / 2))],\n                        size=reduced_patchsize,\n                        method=tf.image.ResizeMethod.BILINEAR)\n\n            return layers.conv2d(dec_layers[\'conv_layer_dec_64\'], num_outputs=num_classes, kernel_size=(3, 3),\n                                 padding=\'SAME\', activation_fn=tf.identity)\n'"
dh_segment/network/pretrained_models.py,6,"b'from tensorflow.contrib import slim, layers\nimport tensorflow as tf\nfrom tensorflow.contrib.slim import nets\nimport numpy as np\n\n_VGG_MEANS = [123.68, 116.78, 103.94]\n\n\ndef mean_substraction(input_tensor, means=_VGG_MEANS):\n    return tf.subtract(input_tensor, np.array(means)[None, None, None, :], name=\'MeanSubstraction\')\n\n\ndef vgg_16_fn(input_tensor: tf.Tensor, scope=\'vgg_16\', blocks=5, weight_decay=0.0005) \\\n        -> (tf.Tensor, list):  # list of tf.Tensors (layers)\n    intermediate_levels = []\n    # intermediate_levels.append(input_tensor)\n    with slim.arg_scope(nets.vgg.vgg_arg_scope(weight_decay=weight_decay)):\n        with tf.variable_scope(scope, \'vgg_16\', [input_tensor]) as sc:\n            input_tensor = mean_substraction(input_tensor)\n            intermediate_levels.append(input_tensor)\n            end_points_collection = sc.original_name_scope + \'_end_points\'\n            # Collect outputs for conv2d, fully_connected and max_pool2d.\n            with slim.arg_scope(\n                    [layers.conv2d, layers.fully_connected, layers.max_pool2d],\n                    outputs_collections=end_points_collection):\n                net = layers.repeat(\n                    input_tensor, 2, layers.conv2d, 64, [3, 3], scope=\'conv1\')\n                intermediate_levels.append(net)\n                net = layers.max_pool2d(net, [2, 2], scope=\'pool1\')\n                if blocks >= 2:\n                    net = layers.repeat(net, 2, layers.conv2d, 128, [3, 3], scope=\'conv2\')\n                    intermediate_levels.append(net)\n                    net = layers.max_pool2d(net, [2, 2], scope=\'pool2\')\n                if blocks >= 3:\n                    net = layers.repeat(net, 3, layers.conv2d, 256, [3, 3], scope=\'conv3\')\n                    intermediate_levels.append(net)\n                    net = layers.max_pool2d(net, [2, 2], scope=\'pool3\')\n                if blocks >= 4:\n                    net = layers.repeat(net, 3, layers.conv2d, 512, [3, 3], scope=\'conv4\')\n                    intermediate_levels.append(net)\n                    net = layers.max_pool2d(net, [2, 2], scope=\'pool4\')\n                if blocks >= 5:\n                    net = layers.repeat(net, 3, layers.conv2d, 512, [3, 3], scope=\'conv5\')\n                    intermediate_levels.append(net)\n                    net = layers.max_pool2d(net, [2, 2], scope=\'pool5\')\n\n                return net, intermediate_levels\n\n\ndef resnet_v1_50_fn(input_tensor: tf.Tensor, is_training=False, blocks=4, weight_decay=0.0001,\n                    renorm=True, corrected_version=False) -> tf.Tensor:\n    with slim.arg_scope(nets.resnet_v1.resnet_arg_scope(weight_decay=weight_decay, batch_norm_decay=0.999)), \\\n         slim.arg_scope([layers.batch_norm], renorm_decay=0.95, renorm=renorm):\n        input_tensor = mean_substraction(input_tensor)\n        assert 0 < blocks <= 4\n\n        if corrected_version:\n            def corrected_resnet_v1_block(scope, base_depth, num_units, stride):\n                  """"""Helper function for creating a resnet_v1 bottleneck block.\n\n                  Args:\n                    scope: The scope of the block.\n                    base_depth: The depth of the bottleneck layer for each unit.\n                    num_units: The number of units in the block.\n                    stride: The stride of the block, implemented as a stride in the last unit.\n                      All other units have stride=1.\n\n                  Returns:\n                    A resnet_v1 bottleneck block.\n                  """"""\n                  return nets.resnet_utils.Block(scope, nets.resnet_v1.bottleneck,[{\n                      \'depth\': base_depth * 4,\n                      \'depth_bottleneck\': base_depth,\n                      \'stride\': stride\n                  }] + [{\n                      \'depth\': base_depth * 4,\n                      \'depth_bottleneck\': base_depth,\n                      \'stride\': 1\n                  }] * (num_units - 1))\n\n            blocks_list = [\n                corrected_resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=1),\n                corrected_resnet_v1_block(\'block2\', base_depth=128, num_units=4, stride=2),\n                corrected_resnet_v1_block(\'block3\', base_depth=256, num_units=6, stride=2),\n                corrected_resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=2),\n            ]\n            desired_endpoints = [\n                \'resnet_v1_50/conv1\',\n                \'resnet_v1_50/block1/unit_3/bottleneck_v1\',\n                \'resnet_v1_50/block2/unit_4/bottleneck_v1\',\n                \'resnet_v1_50/block3/unit_6/bottleneck_v1\',\n                \'resnet_v1_50/block4/unit_3/bottleneck_v1\'\n            ]\n        else:\n            blocks_list = [\n                nets.resnet_v1.resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n                nets.resnet_v1.resnet_v1_block(\'block2\', base_depth=128, num_units=4, stride=2),\n                nets.resnet_v1.resnet_v1_block(\'block3\', base_depth=256, num_units=6, stride=2),\n                nets.resnet_v1.resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1),\n            ]\n            desired_endpoints = [\n                \'resnet_v1_50/conv1\',\n                \'resnet_v1_50/block1/unit_2/bottleneck_v1\',\n                \'resnet_v1_50/block2/unit_3/bottleneck_v1\',\n                \'resnet_v1_50/block3/unit_5/bottleneck_v1\',\n                \'resnet_v1_50/block4/unit_3/bottleneck_v1\'\n            ]\n\n        net, endpoints = nets.resnet_v1.resnet_v1(input_tensor,\n                                                  blocks=blocks_list[:blocks],\n                                                  num_classes=None,\n                                                  is_training=is_training,\n                                                  global_pool=False,\n                                                  output_stride=None,\n                                                  include_root_block=True,\n                                                  reuse=None,\n                                                  scope=\'resnet_v1_50\')\n\n        intermediate_layers = list()\n        for d in desired_endpoints[:blocks + 1]:\n            intermediate_layers.append(endpoints[d])\n\n        return net, intermediate_layers\n'"
dh_segment/post_processing/__init__.py,0,"b'r""""""\nThe :mod:`dh_segment.post_processing` module contains functions to post-process probability maps.\n\n**Binarization**\n\n.. autosummary::\n    thresholding\n    cleaning_binary\n\n**Detection**\n\n.. autosummary::\n    find_boxes\n    find_polygonal_regions\n\n**Vectorization**\n\n.. autosummary::\n    find_lines\n\n------\n\n""""""\n\n_BINARIZATION = [\n    \'thresholding\',\n    \'cleaning_binary\',\n\n]\n\n_DETECTION = [\n    \'find_boxes\',\n    \'find_polygonal_regions\'\n]\n\n_VECTORIZATION = [\n    \'find_lines\'\n]\n\n__all__ = _BINARIZATION + _DETECTION + _VECTORIZATION\n\nfrom .binarization import *\nfrom .boxes_detection import *\nfrom .line_vectorization import *\nfrom .polygon_detection import *\n\n'"
dh_segment/post_processing/binarization.py,0,"b'import numpy as np\nimport cv2\nfrom scipy.ndimage import label\n\n\ndef thresholding(probs: np.ndarray, threshold: float=-1) -> np.ndarray:\n    """"""\n    Computes the binary mask of the detected Page from the probabilities output by network.\n\n    :param probs: array in range [0, 1] of shape HxWx2\n    :param threshold: threshold between [0 and 1], if negative Otsu\'s adaptive threshold will be used\n    :return: binary mask\n    """"""\n\n    if threshold < 0:  # Otsu\'s thresholding\n        probs = np.uint8(probs * 255)\n        #TODO Correct that weird gaussianBlur\n        probs = cv2.GaussianBlur(probs, (5, 5), 0)\n\n        thresh_val, bin_img = cv2.threshold(probs, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n        mask = np.uint8(bin_img / 255)\n    else:\n        mask = np.uint8(probs > threshold)\n\n    return mask\n\n\ndef cleaning_binary(mask: np.ndarray, kernel_size: int=5) -> np.ndarray:\n    """"""\n    Uses mathematical morphology to clean and remove small elements from binary images.\n\n    :param mask: the binary image to clean\n    :param kernel_size: size of the kernel\n    :return: the cleaned mask\n    """"""\n\n    ksize_open = (kernel_size, kernel_size)\n    ksize_close = (kernel_size, kernel_size)\n    mask = cv2.morphologyEx((mask.astype(np.uint8, copy=False) * 255), cv2.MORPH_OPEN, kernel=np.ones(ksize_open))\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel=np.ones(ksize_close))\n    return np.uint8(mask / 255)\n\n\ndef hysteresis_thresholding(probs: np.array, low_threshold: float, high_threshold: float,\n                            candidates_mask: np.ndarray=None) -> np.ndarray:\n    low_mask = probs > low_threshold\n    if candidates_mask is not None:\n        low_mask = candidates_mask & low_mask\n    # Connected components extraction\n    label_components, count = label(low_mask, np.ones((3, 3)))\n    # Keep components with high threshold elements\n    good_labels = np.unique(label_components[low_mask & (probs > high_threshold)])\n    label_masks = np.zeros((count + 1,), bool)\n    label_masks[good_labels] = 1\n    return label_masks[label_components]\n\n\ndef cleaning_probs(probs: np.ndarray, sigma: float) -> np.ndarray:\n    # Smooth\n    if sigma > 0.:\n        return cv2.GaussianBlur(probs, (int(3*sigma)*2+1, int(3*sigma)*2+1), sigma)\n    elif sigma == 0.:\n        return cv2.fastNlMeansDenoising((probs*255).astype(np.uint8), h=20)/255\n    else:  # Negative sigma, do not do anything\n        return probs\n'"
dh_segment/post_processing/boxes_detection.py,0,"b'import cv2\nimport numpy as np\nimport math\nfrom shapely import geometry\nfrom scipy.spatial import KDTree\n\n\ndef find_boxes(boxes_mask: np.ndarray,\n               mode: str= \'min_rectangle\',\n               min_area: float=0.,\n               p_arc_length: float=0.01,\n               n_max_boxes=math.inf) -> list:\n    """"""\n    Finds the coordinates of the box in the binary image `boxes_mask`.\n\n    :param boxes_mask: Binary image: the mask of the box to find. uint8, 2D array\n    :param mode: \'min_rectangle\' : minimum enclosing rectangle, can be rotated\n                 \'rectangle\' : minimum enclosing rectangle, not rotated\n                 \'quadrilateral\' : minimum polygon approximated by a quadrilateral\n    :param min_area: minimum area of the box to be found. A value in percentage of the total area of the image.\n    :param p_arc_length: used to compute the epsilon value to approximate the polygon with a quadrilateral.\n                         Only used when \'quadrilateral\' mode is chosen.\n    :param n_max_boxes: maximum number of boxes that can be found (default inf).\n                        This will select n_max_boxes with largest area.\n    :return: list of length n_max_boxes containing boxes with 4 corners [[x1,y1], ..., [x4,y4]]\n    """"""\n\n    assert len(boxes_mask.shape) == 2, \\\n        \'Input mask must be a 2D array ! Mask is now of shape {}\'.format(boxes_mask.shape)\n\n    contours, _ = cv2.findContours(boxes_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    if contours is None:\n        print(\'No contour found\')\n        return None\n    found_boxes = list()\n\n    h_img, w_img = boxes_mask.shape[:2]\n\n    def validate_box(box: np.array) -> (np.array, float):\n        """"""\n\n        :param box: array of 4 coordinates with format [[x1,y1], ..., [x4,y4]]\n        :return: (box, area)\n        """"""\n        polygon = geometry.Polygon([point for point in box])\n        if polygon.area > min_area * boxes_mask.size:\n\n            # Correct out of range corners\n            box = np.maximum(box, 0)\n            box = np.stack((np.minimum(box[:, 0], boxes_mask.shape[1]),\n                            np.minimum(box[:, 1], boxes_mask.shape[0])), axis=1)\n\n            # return box\n            return box, polygon.area\n\n    if mode not in [\'quadrilateral\', \'min_rectangle\', \'rectangle\']:\n        raise NotImplementedError\n    if mode == \'quadrilateral\':\n        for c in contours:\n            epsilon = p_arc_length * cv2.arcLength(c, True)\n            cnt = cv2.approxPolyDP(c, epsilon, True)\n            # box = np.vstack(simplify_douglas_peucker(cnt[:, 0, :], 4))\n\n            # Find extreme points in Convex Hull\n            hull_points = cv2.convexHull(cnt, returnPoints=True)\n            # points = cnt\n            points = hull_points\n            if len(points) > 4:\n                # Find closes points to corner using nearest neighbors\n                tree = KDTree(points[:, 0, :])\n                _, ul = tree.query((0, 0))\n                _, ur = tree.query((w_img, 0))\n                _, dl = tree.query((0, h_img))\n                _, dr = tree.query((w_img, h_img))\n                box = np.vstack([points[ul, 0, :], points[ur, 0, :],\n                                 points[dr, 0, :], points[dl, 0, :]])\n            elif len(hull_points) == 4:\n                box = hull_points[:, 0, :]\n            else:\n                    continue\n            # Todo : test if it looks like a rectangle (2 sides must be more or less parallel)\n            # todo : (otherwise we may end with strange quadrilaterals)\n            if len(box) != 4:\n                mode = \'min_rectangle\'\n                print(\'Quadrilateral has {} points. Switching to minimal rectangle mode\'.format(len(box)))\n            else:\n                # found_box = validate_box(box)\n                found_boxes.append(validate_box(box))\n    if mode == \'min_rectangle\':\n        for c in contours:\n            rect = cv2.minAreaRect(c)\n            box = np.int0(cv2.boxPoints(rect))\n            found_boxes.append(validate_box(box))\n    elif mode == \'rectangle\':\n        for c in contours:\n            x, y, w, h = cv2.boundingRect(c)\n            box = np.array([[x, y], [x + w, y], [x + w, y + h], [x, y + h]], dtype=int)\n            found_boxes.append(validate_box(box))\n    # sort by area\n    found_boxes = [fb for fb in found_boxes if fb is not None]\n    found_boxes = sorted(found_boxes, key=lambda x: x[1], reverse=True)\n    if n_max_boxes == 1:\n        if found_boxes:\n            return found_boxes[0][0]\n        else:\n            return None\n    else:\n        return [fb[0] for i, fb in enumerate(found_boxes) if i <= n_max_boxes]\n'"
dh_segment/post_processing/line_vectorization.py,0,"b'from skimage.graph import MCP_Connect\nfrom skimage.morphology import skeletonize\nfrom skimage.measure import label as skimage_label\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom scipy.signal import convolve2d\nfrom collections import defaultdict\nimport numpy as np\n\n\ndef find_lines(lines_mask: np.ndarray) -> list:\n    """"""\n    Finds the longest central line for each connected component in the given binary mask.\n\n    :param lines_mask: Binary mask of the detected line-areas\n    :return: a list of Opencv-style polygonal lines (each contour encoded as [N,1,2] elements where each tuple is (x,y) )\n    """"""\n    # Make sure one-pixel wide 8-connected mask\n    lines_mask = skeletonize(lines_mask)\n\n    class MakeLineMCP(MCP_Connect):\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            self.connections = dict()\n            self.scores = defaultdict(lambda: np.inf)\n\n        def create_connection(self, id1, id2, pos1, pos2, cost1, cost2):\n            k = (min(id1, id2), max(id1, id2))\n            s = cost1 + cost2\n            if self.scores[k] > s:\n                self.connections[k] = (pos1, pos2, s)\n                self.scores[k] = s\n\n        def get_connections(self, subsample=5):\n            results = dict()\n            for k, (pos1, pos2, s) in self.connections.items():\n                path = np.concatenate([self.traceback(pos1), self.traceback(pos2)[::-1]])\n                results[k] = path[::subsample]\n            return results\n\n        def goal_reached(self, int_index, float_cumcost):\n            if float_cumcost > 0:\n                return 2\n            else:\n                return 0\n\n    if np.sum(lines_mask) == 0:\n        return []\n    # Find extremities points\n    end_points_candidates = np.stack(np.where((convolve2d(lines_mask, np.ones((3, 3)), mode=\'same\') == 2) & lines_mask)).T\n    connected_components = skimage_label(lines_mask, connectivity=2)\n    # Group endpoint by connected components and keep only the two points furthest away\n    d = defaultdict(list)\n    for pt in end_points_candidates:\n        d[connected_components[pt[0], pt[1]]].append(pt)\n    end_points = []\n    for pts in d.values():\n        d = euclidean_distances(np.stack(pts), np.stack(pts))\n        i, j = np.unravel_index(d.argmax(), d.shape)\n        end_points.append(pts[i])\n        end_points.append(pts[j])\n    end_points = np.stack(end_points)\n\n    mcp = MakeLineMCP(~lines_mask)\n    mcp.find_costs(end_points)\n    connections = mcp.get_connections()\n    if not np.all(np.array(sorted([i for k in connections.keys() for i in k])) == np.arange(len(end_points))):\n        print(\'Warning : find_lines seems weird\')\n    return [c[:, None, ::-1] for c in connections.values()]\n'"
dh_segment/post_processing/polygon_detection.py,0,"b'#!/usr/bin/env python\n\nimport cv2\nimport numpy as np\nimport math\nfrom shapely import geometry\n\n\ndef find_polygonal_regions(image_mask: np.ndarray,\n                           min_area: float=0.,\n                           n_max_polygons: int=math.inf) -> list:\n    """"""\n    Finds the shapes in a binary mask and returns their coordinates as polygons.\n\n    :param image_mask: Uint8 binary 2D array\n    :param min_area: minimum area the polygon should have in order to be considered as valid\n                (value within [0,1] representing a percent of the total size of the image)\n    :param n_max_polygons: maximum number of boxes that can be found (default inf).\n                        This will select n_max_boxes with largest area.\n    :return: list of length n_max_polygons containing polygon\'s n coordinates [[x1, y1], ... [xn, yn]]\n    """"""\n\n    contours, _ = cv2.findContours(image_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    if contours is None:\n        print(\'No contour found\')\n        return None\n    found_polygons = list()\n\n    for c in contours:\n        if len(c) < 3:  # A polygon cannot have less than 3 points\n            continue\n        polygon = geometry.Polygon([point[0] for point in c])\n        # Check that polygon has area greater than minimal area\n        if polygon.area >= min_area*np.prod(image_mask.shape[:2]):\n            found_polygons.append(\n                (np.array([point for point in polygon.exterior.coords], dtype=np.uint), polygon.area)\n            )\n\n    # sort by area\n    found_polygons = [fp for fp in found_polygons if fp is not None]\n    found_polygons = sorted(found_polygons, key=lambda x: x[1], reverse=True)\n\n    if found_polygons:\n        return [fp[0] for i, fp in enumerate(found_polygons) if i <= n_max_polygons]\n    else:\n        return None\n'"
dh_segment/utils/__init__.py,0,"b'r""""""\nThe :mod:`dh_segment.utils` module contains the parameters for config with `sacred`_ package,\nimage label vizualization functions and miscelleanous helpers.\n\nParameters\n----------\n\n.. autosummary::\n    ModelParams\n    TrainingParams\n\nLabel image helpers\n-------------------\n\n.. autosummary::\n    label_image_to_class\n    class_to_label_image\n    multilabel_image_to_class\n    multiclass_to_label_image\n    get_classes_color_from_file\n    get_n_classes_from_file\n    get_classes_color_from_file_multilabel\n    get_n_classes_from_file_multilabel\n\nEvaluation utils\n----------------\n\n.. autosummary::\n    Metrics\n    intersection_over_union\n\nMiscellaneous helpers\n---------------------\n\n.. autosummary::\n    parse_json\n    dump_json\n    load_pickle\n    dump_pickle\n    hash_dict\n\n.. _sacred : https://sacred.readthedocs.io/en/latest/index.html\n\n------\n""""""\n\n_PARAMSCONFIG = [\n    \'PredictionType\',\n    \'VGG16ModelParams\',\n    \'ResNetModelParams\',\n    \'UNetModelParams\',\n    \'ModelParams\',\n    \'TrainingParams\'\n]\n\n\n_LABELS = [\n    \'label_image_to_class\',\n    \'class_to_label_image\',\n    \'multilabel_image_to_class\',\n    \'multiclass_to_label_image\',\n    \'get_classes_color_from_file\',\n    \'get_n_classes_from_file\',\n    \'get_classes_color_from_file_multilabel\',\n    \'get_n_classes_from_file_multilabel\'\n]\n\n_MISC = [\n    \'parse_json\',\n    \'dump_json\',\n    \'load_pickle\',\n    \'dump_pickle\',\n    \'hash_dict\'\n]\n\n_EVALUATION = [\n    \'Metrics\',\n    \'intersection_over_union\'\n]\n\n__all__ = _PARAMSCONFIG + _LABELS + _MISC + _EVALUATION\n\nfrom .params_config import *\nfrom .labels import *\nfrom .misc import *\nfrom .evaluation import *'"
dh_segment/utils/evaluation.py,0,"b'#!/usr/bin/env python\n__author__ = ""solivr""\n__license__ = ""GPL""\n\nimport numpy as np\nimport json\nimport cv2\n\n\nclass Metrics:\n    def __init__(self):\n        self.total_elements = 0\n        self.true_positives = 0\n        self.true_negatives = 0\n        self.false_positives = 0\n        self.false_negatives = 0\n        self.SE_list = list()\n        self.IOU_list = list()\n\n        self.MSE = 0\n        self.psnr = 0\n        self.mIOU = 0\n        self.IU = 0\n        self.accuracy = 0\n        self.recall = 0\n        self.precision = 0\n        self.f_measure = 0\n\n    def __add__(self, other):\n        if isinstance(other, self.__class__):\n            summable_attr = [\'total_elements\', \'false_negatives\', \'false_positives\', \'true_positives\', \'true_negatives\']\n            addlist_attr = [\'SE_list\', \'IOU_list\']\n            m = Metrics()\n            for k, v in self.__dict__.items():\n                if k in summable_attr:\n                    setattr(m, k, self.__dict__[k] + other.__dict__[k])\n                elif k in addlist_attr:\n                    mse1 = [self.__dict__[k]] if not isinstance(self.__dict__[k], list) else self.__dict__[k]\n                    mse2 = [other.__dict__[k]] if not isinstance(other.__dict__[k], list) else other.__dict__[k]\n\n                    setattr(m, k, mse1 + mse2)\n            return m\n        else:\n            raise NotImplementedError\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\n    def compute_mse(self):\n        self.MSE = np.sum(self.SE_list) / self.total_elements if self.total_elements > 0 else np.inf\n        return self.MSE\n\n    def compute_psnr(self):\n        if self.MSE != 0:\n            self.psnr = 10 * np.log10((1 ** 2) / self.MSE)\n            return self.psnr\n        else:\n            print(\'Cannot compute PSNR, MSE is 0.\')\n\n    def compute_prf(self, beta=1):\n        self.recall = self.true_positives / (self.true_positives + self.false_negatives) \\\n            if (self.true_positives + self.false_negatives) > 0 else 0\n        self.precision = self.true_positives / (self.true_positives + self.false_positives) \\\n            if (self.true_positives + self.false_negatives) > 0 else 0\n        self.f_measure = ((1 + beta ** 2) * self.recall * self.precision) / (self.recall + (beta ** 2) * self.precision) \\\n            if (self.recall + self.precision) > 0 else 0\n\n        return self.recall, self.precision, self.f_measure\n\n    def compute_miou(self):\n        self.mIOU = np.mean(self.IOU_list)\n        return self.mIOU\n\n    # See http://cdn.iiit.ac.in/cdn/cvit.iiit.ac.in/images/ConferencePapers/2017/DocUsingDeepFeatures.pdf\n    def compute_iu(self):\n        self.IU = self.true_positives / (self.true_positives + self.false_positives + self.false_negatives) \\\n            if (self.true_positives + self.false_positives + self.false_negatives) > 0 else 0\n        return self.IU\n\n    def compute_accuracy(self):\n        self.accuracy = (self.true_positives + self.true_negatives)/self.total_elements if self.total_elements > 0 else 0\n\n    def save_to_json(self, json_filename: str) -> None:\n        export_dic = self.__dict__.copy()\n        del export_dic[\'MSE_list\']\n\n        with open(json_filename, \'w\') as outfile:\n            json.dump(export_dic, outfile)\n\n\ndef intersection_over_union(cnt1, cnt2, shape_mask):\n    mask1 = np.zeros(shape_mask, np.uint8)\n    mask1 = cv2.fillConvexPoly(mask1, cnt1.astype(np.int32), 1).astype(np.int8)\n    mask2 = np.zeros(shape_mask, np.uint8)\n    mask2 = cv2.fillConvexPoly(mask2, cnt2.astype(np.int32), 1).astype(np.int8)\n    return np.sum(mask1 & mask2) / np.sum(mask1 | mask2)\n'"
dh_segment/utils/labels.py,18,"b'#!/usr/bin/env python\n__license__ = ""GPL""\n\nimport tensorflow as tf\nimport numpy as np\nimport os\nfrom typing import Tuple\n\n\ndef label_image_to_class(label_image: tf.Tensor, classes_file: str) -> tf.Tensor:\n    classes_color_values = get_classes_color_from_file(classes_file)\n    # Convert label_image [H,W,3] to the classes [H,W],int32 according to the classes [C,3]\n    with tf.name_scope(\'LabelAssign\'):\n        if len(label_image.get_shape()) == 3:\n            diff = tf.cast(label_image[:, :, None, :], tf.float32) - tf.constant(classes_color_values[None, None, :, :])  # [H,W,C,3]\n        elif len(label_image.get_shape()) == 4:\n            diff = tf.cast(label_image[:, :, :, None, :], tf.float32) - tf.constant(\n                classes_color_values[None, None, None, :, :])  # [B,H,W,C,3]\n        else:\n            raise NotImplementedError(\'Length is : {}\'.format(len(label_image.get_shape())))\n\n        pixel_class_diff = tf.reduce_sum(tf.square(diff), axis=-1)  # [H,W,C] or [B,H,W,C]\n        class_label = tf.argmin(pixel_class_diff, axis=-1)  # [H,W] or [B,H,W]\n        return class_label\n\n\ndef class_to_label_image(class_label: tf.Tensor, classes_file: str) -> tf.Tensor:\n    classes_color_values = get_classes_color_from_file(classes_file)\n    return tf.gather(classes_color_values, tf.cast(class_label, dtype=tf.int32))\n\n\ndef multilabel_image_to_class(label_image: tf.Tensor, classes_file: str) -> tf.Tensor:\n    """"""\n    Combines image annotations with classes info of the txt file to create the input label for the training.\n\n    :param label_image: annotated image [H,W,Ch] or [B,H,W,Ch] (Ch = color channels)\n    :param classes_file: the filename of the txt file containing the class info\n    :return: [H,W,Cl] or [B,H,W,Cl] (Cl = number of classes)\n    """"""\n    classes_color_values, colors_labels = get_classes_color_from_file_multilabel(classes_file)\n    # Convert label_image [H,W,3] to the classes [H,W,C],int32 according to the classes [C,3]\n    with tf.name_scope(\'LabelAssign\'):\n        if len(label_image.get_shape()) == 3:\n            diff = tf.cast(label_image[:, :, None, :], tf.float32) - tf.constant(classes_color_values[None, None, :, :])  # [H,W,C,3]\n        elif len(label_image.get_shape()) == 4:\n            diff = tf.cast(label_image[:, :, :, None, :], tf.float32) - tf.constant(\n                classes_color_values[None, None, None, :, :])  # [B,H,W,C,3]\n        else:\n            raise NotImplementedError(\'Length is : {}\'.format(len(label_image.get_shape())))\n\n        pixel_class_diff = tf.reduce_sum(tf.square(diff), axis=-1)  # [H,W,C] or [B,H,W,C]\n        class_label = tf.argmin(pixel_class_diff, axis=-1)  # [H,W] or [B,H,W]\n\n        return tf.gather(colors_labels, class_label) > 0\n\n\ndef multiclass_to_label_image(class_label_tensor: tf.Tensor, classes_file: str) -> tf.Tensor:\n\n    classes_color_values, colors_labels = get_classes_color_from_file_multilabel(classes_file)\n\n    n_classes = colors_labels.shape[1]\n    c = np.zeros((2,)*n_classes+(3,), np.int32)\n    for c_value, inds in zip(classes_color_values, colors_labels):\n        c[tuple(inds)] = c_value\n\n    with tf.name_scope(\'Label2Img\'):\n        return tf.gather_nd(c, tf.cast(class_label_tensor, tf.int32))\n\n\ndef get_classes_color_from_file(classes_file: str) -> np.ndarray:\n    if not os.path.exists(classes_file):\n        raise FileNotFoundError(classes_file)\n    result = np.loadtxt(classes_file).astype(np.float32)\n    assert result.shape[1] == 3, ""Color file should represent RGB values""\n    return result\n\n\ndef get_n_classes_from_file(classes_file: str) -> int:\n    return get_classes_color_from_file(classes_file).shape[0]\n\n\ndef get_classes_color_from_file_multilabel(classes_file: str) -> Tuple[np.ndarray, np.array]:\n    """"""\n    Get classes and code labels from txt file.\n    This function deals with the case of elements with multiple labels.\n\n    :param classes_file: file containing the classes (usually named *classes.txt*)\n    :return: for each class the RGB color (array size [N, 3]); and the label\'s code  (array size [N, C]),\n        with N the number of combinations and C the number of classes\n    """"""\n    if not os.path.exists(classes_file):\n        raise FileNotFoundError(classes_file)\n    result = np.loadtxt(classes_file).astype(np.float32)\n    assert result.shape[1] > 3, ""The number of columns should be greater in multilabel framework""\n    colors = result[:, :3]\n    labels = result[:, 3:]\n    return colors, labels.astype(np.int32)\n\n\ndef get_n_classes_from_file_multilabel(classes_file: str) -> int:\n    return get_classes_color_from_file_multilabel(classes_file)[1].shape[1]\n'"
dh_segment/utils/misc.py,0,"b'#!/usr/bin/env python\n__license__ = ""GPL""\n\nimport tensorflow as tf\nimport json\nimport pickle\nfrom hashlib import sha1\nfrom random import shuffle\n\n\ndef parse_json(filename):\n    with open(filename, \'r\') as f:\n        return json.load(f)\n\n\ndef dump_json(filename, dict):\n    with open(filename, \'w\') as f:\n        json.dump(dict, f, indent=4, sort_keys=True)\n\n\ndef load_pickle(filename):\n    with open(filename, \'rb\') as f:\n        return pickle.load(f)\n\n\ndef dump_pickle(filename, obj):\n    with open(filename, \'wb\') as f:\n        return pickle.dump(obj, f)\n\n\ndef hash_dict(params):\n    return sha1(json.dumps(params, sort_keys=True).encode()).hexdigest()\n\ndef shuffled(l: list) -> list:\n    ll = l.copy()\n    shuffle(ll)\n    return ll\n'"
dh_segment/utils/params_config.py,0,"b'#!/usr/bin/env python\n__author__ = ""solivr""\n__license__ = ""GPL""\n\nimport os\nimport warnings\nfrom random import shuffle\n\n\nclass PredictionType:\n    """"""\n\n    :cvar CLASSIFICATION:\n    :cvar REGRESSION:\n    :cvar MULTILABEL:\n    """"""\n    CLASSIFICATION = \'CLASSIFICATION\'\n    REGRESSION = \'REGRESSION\'\n    MULTILABEL = \'MULTILABEL\'\n\n    @classmethod\n    def parse(cls, prediction_type):\n        if prediction_type == \'CLASSIFICATION\':\n            return PredictionType.CLASSIFICATION\n        elif prediction_type == \'REGRESSION\':\n            return PredictionType.REGRESSION\n        elif prediction_type == \'MULTILABEL\':\n            return PredictionType.MULTILABEL\n        else:\n            raise NotImplementedError(\'Unknown prediction type : {}\'.format(prediction_type))\n\n\nclass BaseParams:\n    def to_dict(self):\n        return self.__dict__\n\n    @classmethod\n    def from_dict(cls, d):\n        result = cls()\n        keys = result.to_dict().keys()\n        for k, v in d.items():\n            assert k in keys, k\n            setattr(result, k, v)\n        result.check_params()\n        return result\n\n    def check_params(self):\n        pass\n\n\nclass VGG16ModelParams:\n    PRETRAINED_MODEL_FILE = \'pretrained_models/vgg_16.ckpt\'\n    INTERMEDIATE_CONV = [\n        [(256, 3)]\n    ]\n    UPSCALE_PARAMS = [\n        [(32, 3)],\n        [(64, 3)],\n        [(128, 3)],\n        [(256, 3)],\n        [(512, 3)],\n        [(512, 3)]\n    ]\n    SELECTED_LAYERS_UPSCALING = [\n        True,\n        True,  # Must have same length as vgg_upscale_params\n        True,\n        True,\n        False,\n        False\n    ]\n    CORRECTED_VERSION = None\n\n\nclass ResNetModelParams:\n    PRETRAINED_MODEL_FILE = \'pretrained_models/resnet_v1_50.ckpt\'\n    INTERMEDIATE_CONV = None\n    UPSCALE_PARAMS = [\n        # (Filter size (depth bottleneck\'s output), number of bottleneck)\n        (32, 0),\n        (64, 0),\n        (128, 0),\n        (256, 0),\n        (512, 0)\n    ]\n    SELECTED_LAYERS_UPSCALING = [\n        # Must have the same length as resnet_upscale_params\n        True,\n        True,\n        True,\n        True,\n        True\n    ]\n    CORRECT_VERSION = False\n\n\nclass UNetModelParams:\n    PRETRAINED_MODEL_FILE = None\n    INTERMEDIATE_CONV = None\n    UPSCALE_PARAMS = None\n    SELECTED_LAYERS_UPSCALING = None\n    CORRECT_VERSION = False\n\n\nclass ModelParams(BaseParams):\n    """"""Parameters related to the model\n\n    """"""\n    def __init__(self, **kwargs):\n        self.batch_norm = kwargs.get(\'batch_norm\', True)  # type: bool\n        self.batch_renorm = kwargs.get(\'batch_renorm\', True)  # type: bool\n        self.weight_decay = kwargs.get(\'weight_decay\', 1e-6)  # type: float\n        self.n_classes = kwargs.get(\'n_classes\', None)  # type: int\n        self.pretrained_model_name = kwargs.get(\'pretrained_model_name\', None)  # type: str\n        self.max_depth = kwargs.get(\'max_depth\', 512)  # type: int\n\n        if self.pretrained_model_name == \'vgg16\':\n            model_class = VGG16ModelParams\n        elif self.pretrained_model_name == \'resnet50\':\n            model_class = ResNetModelParams\n        elif self.pretrained_model_name == \'unet\':\n            model_class = UNetModelParams\n        else:\n            raise NotImplementedError\n\n        self.pretrained_model_file = kwargs.get(\'pretrained_model_file\', model_class.PRETRAINED_MODEL_FILE)\n        self.intermediate_conv = kwargs.get(\'intermediate_conv\', model_class.INTERMEDIATE_CONV)\n        self.upscale_params = kwargs.get(\'upscale_params\', model_class.UPSCALE_PARAMS)\n        self.selected_levels_upscaling = kwargs.get(\'selected_levels_upscaling\', model_class.SELECTED_LAYERS_UPSCALING)\n        self.correct_resnet_version = kwargs.get(\'correct_resnet_version\', model_class.CORRECT_VERSION)\n        self.check_params()\n\n    def check_params(self):\n        # Pretrained model name check\n        # assert self.upscale_params is not None and self.selected_levels_upscaling is not None, \\\n        #     \'Model parameters cannot be None\'\n        if self.upscale_params is not None and self.selected_levels_upscaling is not None:\n\n            assert len(self.upscale_params) == len(self.selected_levels_upscaling), \\\n                \'Upscaling levels and selection levels must have the same lengths (in model_params definition), \' \\\n                \'{} != {}\'.format(len(self.upscale_params),\n                                  len(self.selected_levels_upscaling))\n\n            # assert os.path.isfile(self.pretrained_model_file), \\\n            #     \'Pretrained weights file {} not found\'.format(self.pretrained_model_file)\n            if not os.path.isfile(self.pretrained_model_file):\n                warnings.warn(\'WARNING - Default pretrained weights file in {} was not found. \'\n                              \'Have you changed the default pretrained file ?\'.format(self.pretrained_model_file))\n\n\nclass TrainingParams(BaseParams):\n    """"""Parameters to configure training process\n\n    :ivar n_epochs: number of epoch for training\n    :vartype n_epochs: int\n    :ivar evaluate_every_epoch: the model will be evaluated every `n` epochs\n    :vartype evaluate_every_epoch: int\n    :ivar learning_rate: the starting learning rate value\n    :vartype learning_rate: float\n    :ivar exponential_learning: option to use exponential learning rate\n    :vartype exponential_learning: bool\n    :ivar batch_size: size of batch\n    :vartype batch_size: int\n    :ivar data_augmentation: option to use data augmentation (by default is set to False)\n    :vartype data_augmentation: bool\n    :ivar data_augmentation_flip_lr: option to use image flipping in right-left direction\n    :vartype data_augmentation_flip_lr: bool\n    :ivar data_augmentation_flip_ud: option to use image flipping in up down direction\n    :vartype data_augmentation_flip_ud: bool\n    :ivar data_augmentation_color: option to use data augmentation with color\n    :vartype data_augmentation_color: bool\n    :ivar data_augmentation_max_rotation: maximum angle of rotation (in radians) for data augmentation\n    :vartype data_augmentation_max_rotation: float\n    :ivar data_augmentation_max_scaling: maximum scale of zooming during data augmentation (range: [0,1])\n    :vartype data_augmentation_max_scaling: float\n    :ivar make_patches: option to crop image into patches. This will cut the entire image in several patches\n    :vartype make_patches: bool\n    :ivar patch_shape: shape of the patches\n    :vartype patch_shape: tuple\n    :ivar input_resized_size: size (in pixel) of the image after resizing. The original ratio is kept. If no resizing \\\n    is wanted, set it to -1\n    :vartype input_resized_size: int\n    :ivar weights_labels: weight given to each label. Should be a list of length = number of classes\n    :vartype weights_labels: list\n    :ivar training_margin: size of the margin to add to the images. This is particularly useful when training with \\\n    patches\n    :vartype training_margin: int\n    :ivar local_entropy_ratio:\n    :vartype local_entropy_ratio: float\n    :ivar local_entropy_sigma:\n    :vartype local_entropy_sigma: float\n    :ivar focal_loss_gamma: value of gamma for the focal loss. See paper : https://arxiv.org/abs/1708.02002\n    :vartype focal_loss_gamma: float\n    """"""\n    def __init__(self, **kwargs):\n        self.n_epochs = kwargs.get(\'n_epochs\', 20)\n        self.evaluate_every_epoch = kwargs.get(\'evaluate_every_epoch\', 10)\n        self.learning_rate = kwargs.get(\'learning_rate\', 1e-5)\n        self.exponential_learning = kwargs.get(\'exponential_learning\', True)\n        self.batch_size = kwargs.get(\'batch_size\', 5)\n        self.data_augmentation = kwargs.get(\'data_augmentation\', False)\n        self.data_augmentation_flip_lr = kwargs.get(\'data_augmentation_flip_lr\', False)\n        self.data_augmentation_flip_ud = kwargs.get(\'data_augmentation_flip_ud\', False)\n        self.data_augmentation_color = kwargs.get(\'data_augmentation_color\', False)\n        self.data_augmentation_max_rotation = kwargs.get(\'data_augmentation_max_rotation\', 0.2)\n        self.data_augmentation_max_scaling = kwargs.get(\'data_augmentation_max_scaling\', 0.05)\n        self.make_patches = kwargs.get(\'make_patches\', True)\n        self.patch_shape = kwargs.get(\'patch_shape\', (300, 300))\n        self.input_resized_size = int(kwargs.get(\'input_resized_size\', 72e4))  # (600*1200)\n        self.weights_labels = kwargs.get(\'weights_labels\')\n        self.weights_evaluation_miou = kwargs.get(\'weights_evaluation_miou\', None)\n        self.training_margin = kwargs.get(\'training_margin\', 16)\n        self.local_entropy_ratio = kwargs.get(\'local_entropy_ratio\', 0.)\n        self.local_entropy_sigma = kwargs.get(\'local_entropy_sigma\', 3)\n        self.focal_loss_gamma = kwargs.get(\'focal_loss_gamma\', 0.)\n\n    def check_params(self) -> None:\n        """"""Checks if there is no parameter inconsistency\n        """"""\n        assert self.training_margin*2 < min(self.patch_shape)\n'"
exps/cbad/__init__.py,0,"b""#!/usr/bin/env python\n__author__ = 'solivr'"""
exps/cbad/demo_processing.py,3,"b'#!/usr/bin/env python\n__author__ = ""solivr""\n__license__ = ""GPL""\n\nimport tensorflow as tf\nimport os\nfrom typing import List\nfrom dh_segment.inference import LoadedModel\nfrom dh_segment.io import PAGE\nfrom process import line_extraction_v1\nfrom imageio import imread, imsave\nfrom tqdm import tqdm\nimport click\n\n\n@click.command()\n@click.argument(\'filenames_to_process\', nargs=-1)\n@click.option(\'--model_dir\', help=""The directory of te model to use"")\n@click.option(\'--output_dir\', help=""Directory to output the PAGEXML files"")\n@click.option(\'--draw_extractions\', help=""If true, the extracted lines will be drawn and exported to the output_dir"")\ndef baseline_extraction(model_dir: str,\n                        filenames_to_process: List[str],\n                        output_dir: str,\n                        draw_extractions: bool=False,\n                        config: tf.ConfigProto=None) -> None:\n    """"""\n    Given a model directory this function will load the model and apply it to the given files.\n\n    :param model_dir: Directory containing the saved model\n    :param filenames_to_process: filenames of the images to process\n    :param output_dir: output directory to save the predictions (probability images)\n    :param draw_extractions:\n    :param config: ``ConfigProto`` object for ``tf.Session``.\n    :return:\n    """"""\n\n    os.makedirs(output_dir, exist_ok=True)\n    if draw_extractions:\n        drawing_dir = os.path.join(output_dir, \'drawings\')\n        os.makedirs(drawing_dir)\n\n    with tf.Session(config=config):\n        # Load the model\n        m = LoadedModel(model_dir, predict_mode=\'filename_original_shape\')\n        for filename in tqdm(filenames_to_process, desc=\'Prediction\'):\n            # Inference\n            prediction = m.predict(filename)\n            # Take the first element of the \'probs\' dictionary (batch size = 1)\n            probs = prediction[\'probs\'][0]\n            original_shape = probs.shape\n\n            # The baselines probs are on the second channel\n            baseline_probs = probs[:, :, 1]\n            contours, _ = line_extraction_v1(baseline_probs, low_threshold=0.2, high_threshold=0.4, sigma=1.5)\n\n            basename = os.path.basename(filename).split(\'.\')[0]\n\n            # Compute the ratio to save the coordinates in the original image coordinates reference.\n            ratio = (original_shape[0] / probs.shape[0], original_shape[1] / probs.shape[1])\n            xml_filename = os.path.join(output_dir, basename + \'.xml\')\n            page_object = PAGE.save_baselines(xml_filename, contours, ratio, predictions_shape=probs.shape[:2])\n\n            # If specified, saves the images with the annotated baslines\n            if draw_extractions:\n                image = imread(filename)\n                page_object.draw_baselines(image, color=(255, 0, 0), thickness=5)\n\n                basename = os.path.basename(filename)\n                imsave(os.path.join(drawing_dir, basename), image)\n\nif __name__ == \'__main__\':\n    baseline_extraction()\n'"
exps/cbad/evaluation.py,0,"b'import io\nimport os\nimport subprocess\nfrom glob import glob\nimport pandas as pd\nfrom tqdm import tqdm\nfrom dh_segment.io import PAGE\nfrom .process import extract_lines\n\nCBAD_JAR = \'./cBAD/TranskribusBaseLineEvaluationScheme_v0.1.3/\' \\\n           \'TranskribusBaseLineEvaluationScheme-0.1.3-jar-with-dependencies.jar\'\nPP_PARAMS = {\'sigma\': 1.5, \'low_threshold\': 0.2, \'high_threshold\': 0.4}\n\n\ndef eval_fn(input_dir: str,\n            groudtruth_dir: str,\n            output_dir: str=None,\n            post_process_params: dict=PP_PARAMS,\n            channel_baselines: int=1,\n            jar_tool_path: str=CBAD_JAR,\n            masks_dir: str=None) -> dict:\n    """"""\n    Evaluates a model against the selected set (\'groundtruth_dir\' contains XML files)\n\n    :param input_dir: Input directory containing probability maps (.npy)\n    :param groudtruth_dir: directory containg XML groundtruths\n    :param output_dir: output directory for results\n    :param post_process_params: parameters form post processing of probability maps\n    :param channel_baselines: the baseline class chanel\n    :param jar_tool_path: path to cBAD evaluation tool (.jar file)\n    :param masks_dir: optional, directory where binary masks of the page are stored (.png)\n    :return:\n    """"""\n\n    if output_dir is None:\n        output_dir = input_dir\n\n    # Apply post processing and find lines\n    for file in tqdm(glob(os.path.join(input_dir, \'*.npy\'))):\n        basename = os.path.basename(file).split(\'.\')[0]\n        gt_xml_filename = os.path.join(groudtruth_dir, basename + \'.xml\')\n        gt_page_xml = PAGE.parse_file(gt_xml_filename)\n\n        original_shape = [gt_page_xml.image_height, gt_page_xml.image_width]\n\n        _, _ = extract_lines(file, output_dir, original_shape, post_process_params,\n                             channel_baselines=channel_baselines, mask_dir=masks_dir)\n\n    # Create pairs predicted XML - groundtruth XML to be evaluated\n    xml_pred_filenames_list = glob(os.path.join(output_dir, \'*.xml\'))\n    xml_filenames_tuples = list()\n    for xml_filename in xml_pred_filenames_list:\n        basename = os.path.basename(xml_filename)\n        gt_xml_filename = os.path.join(groudtruth_dir, basename)\n\n        xml_filenames_tuples.append((gt_xml_filename, xml_filename))\n\n    gt_pages_list_filename = os.path.join(output_dir, \'gt_pages_simple.lst\')\n    generated_pages_list_filename = os.path.join(output_dir, \'generated_pages_simple.lst\')\n    with open(gt_pages_list_filename, \'w\') as f:\n        f.writelines(\'\\n\'.join([s[0] for s in xml_filenames_tuples]))\n    with open(generated_pages_list_filename, \'w\') as f:\n        f.writelines(\'\\n\'.join([s[1] for s in xml_filenames_tuples]))\n\n    # Evaluation using JAVA Tool\n    cmd = \'java -jar {} {} {}\'.format(jar_tool_path, gt_pages_list_filename, generated_pages_list_filename)\n    result = subprocess.check_output(cmd, shell=True).decode()\n    with open(os.path.join(output_dir, \'scores.txt\'), \'w\') as f:\n        f.write(result)\n    parse_score_txt(result, os.path.join(output_dir, \'scores.csv\'))\n\n    # Parse results from output of tool\n    lines = result.splitlines()\n    avg_precision = float(next(filter(lambda l: \'Avg (over pages) P value:\' in l, lines)).split()[-1])\n    avg_recall = float(next(filter(lambda l: \'Avg (over pages) R value:\' in l, lines)).split()[-1])\n    f_measure = float(next(filter(lambda l: \'Resulting F_1 value:\' in l, lines)).split()[-1])\n\n    print(\'P {}, R {}, F {}\'.format(avg_precision, avg_recall, f_measure))\n\n    return {\n        \'avg_precision\': avg_precision,\n        \'avg_recall\': avg_recall,\n        \'f_measure\': f_measure\n    }\n\n\ndef parse_score_txt(score_txt: str, output_csv: str):\n    lines = score_txt.splitlines()\n    header_ind = next((i for i, l in enumerate(lines)\n                       if l == \'#P value, #R value, #F_1 value, #TruthFileName, #HypoFileName\'))\n    final_line = next((i for i, l in enumerate(lines) if i > header_ind and l == \'\'))\n    csv_data = \'\\n\'.join(lines[header_ind:final_line])\n    df = pd.read_csv(io.StringIO(csv_data))\n    df = df.rename(columns={k: k.strip() for k in df.columns})\n    df[\'#HypoFileName\'] = [os.path.basename(f).split(\'.\')[0] for f in df[\'#HypoFileName\']]\n    del df[\'#TruthFileName\']\n    df = df.rename(columns={\'#P value\': \'P\', \'#R value\': \'R\', \'#F_1 value\': \'F_1\', \'#HypoFileName\': \'basename\'})\n    df = df.reindex(columns=[\'basename\', \'F_1\', \'P\', \'R\'])\n    df = df.sort_values(\'F_1\', ascending=True)\n    df.to_csv(output_csv, index=False)\n'"
exps/cbad/make_cbad.py,0,"b'#!/usr/bin/env python\n__author__ = ""solivr""\n__license__ = ""GPL""\n\nimport os\nimport click\n# from utils import cbad_download, cbad_set_generator, split_set_for_eval\nfrom utils import cbad_set_generator, split_set_for_eval\nfrom exps.commonutils import cbad_download, CBAD_TRAIN_COMPLEX_FOLDER, CBAD_TEST_COMPLEX_FOLDER, CBAD_TRAIN_SIMPLE_FOLDER, CBAD_TEST_SIMPLE_FOLDER\n\n\n@click.command()\n@click.option(\'--downloading_dir\', help=\'Directory to download the cBAD-ICDAR17 dataset\')\n@click.option(\'--masks_dir\', help=""Directory where to output the generated masks"")\ndef generate_cbad_dataset(downloading_dir: str, masks_dir: str):\n    # Check if dataset has already been downloaded\n    if os.path.exists(downloading_dir):\n        print(\'Dataset has already been downloaded. Skipping process.\')\n    else:\n        # Download dataset\n        cbad_download(downloading_dir)\n\n    # Create masks\n    dirs_tuple = [(os.path.join(downloading_dir, CBAD_TRAIN_COMPLEX_FOLDER), os.path.join(masks_dir, \'complex\', \'train\')),\n                  (os.path.join(downloading_dir, CBAD_TEST_COMPLEX_FOLDER), os.path.join(masks_dir, \'complex\', \'test\')),\n                  (os.path.join(downloading_dir, CBAD_TRAIN_SIMPLE_FOLDER), os.path.join(masks_dir, \'simple\', \'train\')),\n                  (os.path.join(downloading_dir, CBAD_TEST_SIMPLE_FOLDER), os.path.join(masks_dir, \'simple\', \'test\'))]\n\n    print(\'Creating sets\')\n    for dir_tuple in dirs_tuple:\n        input_dir, output_dir = dir_tuple\n        os.makedirs(output_dir, exist_ok=True)\n        # For each set create the folder with the annotated data\n        cbad_set_generator(input_dir=input_dir,\n                           output_dir=output_dir,\n                           img_size=2e6,\n                           draw_baselines=True,\n                           draw_endpoints=False)\n\n        # Split the \'official\' train set into training and validation set\n        if \'train\' in output_dir:\n            print(\'Make eval set from the given training data (0.15/0.85 eval/train)\')\n            csv_filename = os.path.join(output_dir, \'set_data.csv\')\n            split_set_for_eval(csv_filename)\n            print(\'Done!\')\n\n\nif __name__ == \'__main__\':\n    generate_cbad_dataset()\n'"
exps/cbad/process.py,2,"b'from typing import Tuple, List\nimport numpy as np\nfrom scipy.ndimage import label\nimport cv2\nimport os\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom glob import glob\nfrom imageio import imsave, imread\nimport PIL\nfrom dh_segment.utils import dump_pickle\nfrom dh_segment.post_processing.binarization import hysteresis_thresholding, cleaning_probs\nfrom dh_segment.post_processing.line_vectorization import find_lines\nfrom dh_segment.io import PAGE\nfrom dh_segment.inference import LoadedModel\n\n\ndef prediction_fn(model_dir: str,\n                  input_dir: str,\n                  output_dir: str=None,\n                  config: tf.ConfigProto=None) -> None:\n    """"""\n    Given a model directory this function will load the model and apply it to the files (.jpg, .png) found in input_dir.\n    The predictions will be saved in output_dir as .npy files (values ranging [0,255])\n\n    :param model_dir: Directory containing the saved model\n    :param input_dir: input directory where the images to predict are\n    :param output_dir: output directory to save the predictions (probability images)\n    :param config: ConfigProto object to pass to the session in order to define which GPU to use\n    :return:\n    """"""\n    if not output_dir:\n        # For model_dir of style model_name/export/timestamp/ this will create a folder model_name/predictions\'\n        output_dir = \'{}\'.format(os.path.sep).join(model_dir.split(os.path.sep)[:-3] + [\'predictions\'])\n\n    os.makedirs(output_dir, exist_ok=True)\n    filenames_to_predict = glob(os.path.join(input_dir, \'*.jpg\')) + glob(os.path.join(input_dir, \'*.png\'))\n\n    with tf.Session(config=config):\n        m = LoadedModel(model_dir, predict_mode=\'filename_original_shape\')\n        for filename in tqdm(filenames_to_predict, desc=\'Prediction\'):\n            pred = m.predict(filename)[\'probs\'][0]\n            np.save(os.path.join(output_dir, os.path.basename(filename).split(\'.\')[0]), np.uint8(255 * pred))\n\n\ndef cbad_post_processing_fn(probs: np.array,\n                            baseline_chanel: int=1,\n                            sigma: float=2.5,\n                            low_threshold: float=0.8,\n                            high_threshold: float=0.9,\n                            filter_width: float=0,\n                            vertical_maxima: bool=False,\n                            output_basename=None) -> Tuple[List[np.ndarray], np.ndarray]:\n    """"""\n    Given a probability map, returns the contour of lines and the corresponding mask.\n    Saves the results in .pkl file if requested.\n\n    :param probs: output of the model (probabilities) in range [0, 255]\n    :param baseline_chanel: channel where the baseline class is detected\n    :param sigma: sigma value for gaussian filtering\n    :param low_threshold: hysteresis low threshold\n    :param high_threshold: hysteresis high threshold\n    :param filter_width: percentage of the image width to filter out lines that are close to borders (default 0.0)\n    :param output_basename: name of file to save the intermediaty result as .pkl file.\n    :param vertical_maxima: set to True to use vertical local maxima as candidates for the hysteresis thresholding\n    :return: contours, mask\n     WARNING : contours IN OPENCV format List[np.ndarray(n_points, 1, (x,y))]\n    """"""\n\n    contours, lines_mask = line_extraction_v1(probs[:, :, baseline_chanel], sigma, low_threshold, high_threshold,\n                                              filter_width, vertical_maxima)\n    if output_basename is not None:\n        dump_pickle(output_basename+\'.pkl\', (contours, lines_mask.shape))\n    return contours, lines_mask\n\n\ndef line_extraction_v1(probs: np.ndarray,\n                       low_threshold: float,\n                       high_threshold: float,\n                       sigma: float=0.0,\n                       filter_width: float=0.00,\n                       vertical_maxima: bool=False) -> Tuple[List[np.ndarray], np.ndarray]:\n    """"""\n    Given a probability map, returns the contour of lines and the corresponding mask\n\n    :param probs: probability map (numpy array)\n    :param low_threshold: hysteresis low threshold\n    :param high_threshold: hysteresis high threshold\n    :param sigma: sigma value for gaussian filtering\n    :param filter_width: percentage of the image width to filter out lines that are close to borders (default 0.0)\n    :param vertical_maxima: set to True to use vertical local maxima as candidates for the hysteresis thresholding\n    :return:\n    """"""\n    # Smooth\n    probs2 = cleaning_probs(probs, sigma=sigma)\n\n    lines_mask = hysteresis_thresholding(probs2, low_threshold, high_threshold,\n                                         candidates_mask=vertical_local_maxima(probs2) if vertical_maxima else None)\n    # Remove lines touching border\n    # lines_mask = remove_borders(lines_mask)\n\n    # Extract polygons from line mask\n    contours = find_lines(lines_mask)\n\n    filtered_contours = []\n    page_width = probs.shape[1]\n    for cnt in contours:\n        centroid_x, centroid_y = np.mean(cnt, axis=0)[0]\n        if centroid_x < filter_width*page_width or centroid_x > (1-filter_width)*page_width:\n            continue\n        # if cv2.arcLength(cnt, False) < filter_width*page_width:\n        #    continue\n        filtered_contours.append(cnt)\n\n    return filtered_contours, lines_mask\n\n\ndef vertical_local_maxima(probs: np.ndarray) -> np.ndarray:\n    local_maxima = np.zeros_like(probs, dtype=bool)\n    local_maxima[1:-1] = (probs[1:-1] >= probs[:-2]) & (probs[2:] <= probs[1:-1])\n    local_maxima = cv2.morphologyEx(local_maxima.astype(np.uint8), cv2.MORPH_CLOSE, np.ones((5, 5), dtype=np.uint8))\n    return local_maxima > 0\n\n\ndef remove_borders(mask: np.ndarray, margin: int=5) -> np.ndarray:\n    tmp = mask.copy()\n    tmp[:margin] = 1\n    tmp[-margin:] = 1\n    tmp[:, :margin] = 1\n    tmp[:, -margin:] = 1\n    label_components, count = label(tmp, np.ones((3, 3)))\n    result = mask.copy()\n    border_component = label_components[0, 0]\n    result[label_components == border_component] = 0\n    return result\n\n\ndef extract_lines(npy_filename: str,\n                  output_dir: str,\n                  original_shape: list,\n                  post_process_params: dict,\n                  channel_baselines: int=1,\n                  mask_dir: str=None,\n                  debug: bool=False):\n    """"""\n    From the prediction files (probs) (.npy) finds and extracts the lines into PAGE-XML format.\n\n    :param npy_filename: filename of saved predictions (probs) in range (0,255)\n    :param output_dir: output direcoty to save the xml files\n    :param original_shape: shpae of the original input image (to rescale the extracted lines if necessary)\n    :param post_process_params: pramas for lines detection (sigma, thresholds, ...)\n    :param channel_baselines: channel where the baseline class is detected\n    :param mask_dir: directory containing masks of the page in order to improve the line extraction\n    :param debug: if True will output the binary image of the extracted lines\n    :return: contours of lines (open cv format), binary image of lines (lines mask)\n    """"""\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    basename = os.path.basename(npy_filename).split(\'.\')[0]\n\n    pred = np.load(npy_filename)/255  # type: np.ndarray\n    lines_prob = pred[:, :, channel_baselines]\n\n    if mask_dir is not None:\n        mask = imread(os.path.join(mask_dir, basename + \'.png\'), mode=\'L\')\n        mask = np.array(PIL.Image.fromarray(mask, mode=\'L\').resize(lines_prob.shape, resample=PIL.Image.BILINEAR))\n        lines_prob[mask == 0] = 0.\n\n    contours, lines_mask = line_extraction_v1(lines_prob, **post_process_params)\n\n    if debug:\n        imsave(os.path.join(output_dir, \'{}_bin.jpg\'.format(basename)), lines_mask)\n\n    ratio = (original_shape[0] / pred.shape[0], original_shape[1] / pred.shape[1])\n    xml_filename = os.path.join(output_dir, basename + \'.xml\')\n    PAGE.save_baselines(xml_filename, contours, ratio, predictions_shape=pred.shape[:2])\n\n    return contours, lines_mask\n'"
exps/cbad/utils.py,0,"b'import os\nimport shutil\nfrom glob import glob\nimport cv2\nimport numpy as np\nimport csv\nimport pandas as pd\nfrom typing import Tuple\nfrom imageio import imread, imsave\nfrom tqdm import tqdm\nfrom dh_segment.io import PAGE\n\n# Constant definitions\nTARGET_HEIGHT = 1100\nDRAWING_COLOR_BASELINES = (255, 0, 0)\nDRAWING_COLOR_LINES = (0, 255, 0)\nDRAWING_COLOR_POINTS = (0, 0, 255)\n\nRANDOM_SEED = 0\nnp.random.seed(RANDOM_SEED)\n\n\ndef get_page_filename(image_filename: str) -> str:\n    """"""\n    Given an path to a .jpg or .png file, get the corresponding .xml file.\n\n    :param image_filename: filename of the image\n    :return: the filename of the corresponding .xml file, raises exception if .xml file does not exist\n    """"""\n    page_filename = os.path.join(os.path.dirname(image_filename),\n                                 \'page\',\n                                 \'{}.xml\'.format(os.path.basename(image_filename)[:-4]))\n\n    if os.path.exists(page_filename):\n        return page_filename\n    else:\n        raise FileNotFoundError\n\n\ndef get_image_label_basename(image_filename: str) -> str:\n    """"""\n    Creates a new filename composed of the begining of the folder/collection (ex. EPFL, ABP) and the original filename\n\n    :param image_filename: path of the image filename\n    :return:\n    """"""\n    # Get acronym followed by name of file\n    directory, basename = os.path.split(image_filename)\n    acronym = directory.split(os.path.sep)[-1].split(\'_\')[0]\n    return \'{}_{}\'.format(acronym, basename.split(\'.\')[0])\n\n\ndef save_and_resize(img: np.array,\n                    filename: str,\n                    size=None,\n                    nearest: bool=False) -> None:\n    """"""\n    Resizes the image if necessary and saves it. The resizing will keep the image ratio\n\n    :param img: the image to resize and save (numpy array)\n    :param filename: filename of the saved image\n    :param size: size of the image after resizing (in pixels). The ratio of the original image will be kept\n    :param nearest: whether to use nearest interpolation method (default to False)\n    :return:\n    """"""\n    if size is not None:\n        h, w = img.shape[:2]\n        ratio = float(np.sqrt(size/(h*w)))\n        resized = cv2.resize(img, (int(w*ratio), int(h*ratio)),\n                             interpolation=cv2.INTER_NEAREST if nearest else cv2.INTER_LINEAR)\n        imsave(filename, resized)\n    else:\n        imsave(filename, img)\n\n\ndef annotate_one_page(image_filename: str,\n                      output_dir: str,\n                      size: int=None,\n                      draw_baselines: bool=True,\n                      draw_lines: bool=False,\n                      draw_endpoints: bool=False,\n                      baseline_thickness: float=0.2,\n                      diameter_endpoint: int=20) -> Tuple[str, str]:\n    """"""\n    Creates an annotated mask and corresponding original image and saves it in \'labels\' and \'images\' folders.\n    Also copies the corresponding .xml file into \'gt\' folder.\n\n    :param image_filename: filename of the image to process\n    :param output_dir: directory to output the annotated label image\n    :param size: Size of the resized image (# pixels)\n    :param draw_baselines: Draws the baselines (boolean)\n    :param draw_lines: Draws the polygon\'s lines (boolean)\n    :param draw_endpoints: Predict beginning and end of baselines (True, False)\n    :param baseline_thickness: Thickness of annotated baseline (percentage of the line\'s height)\n    :param diameter_endpoint: Diameter of annotated start/end points\n    :return: (output_image_path, output_label_path)\n    """"""\n\n    page_filename = get_page_filename(image_filename)\n    # Parse xml file and get TextLines\n    page = PAGE.parse_file(page_filename)\n    text_lines = [tl for tr in page.text_regions for tl in tr.text_lines]\n    img = imread(image_filename, pilmode=\'RGB\')\n    # Create empty mask\n    gt = np.zeros_like(img)\n\n    if text_lines:\n        if draw_baselines:\n            # Thickness : should be a percentage of the line height, for example 0.2\n            # First, get the mean line height.\n            mean_line_height, _, _ = _compute_statistics_line_height(page)\n            absolute_baseline_thickness = int(max(gt.shape[0]*0.002, baseline_thickness*mean_line_height))\n\n            # Draw the baselines\n            gt_baselines = np.zeros_like(img[:, :, 0])\n            gt_baselines = cv2.polylines(gt_baselines,\n                                         [PAGE.Point.list_to_cv2poly(tl.baseline) for tl in\n                                          text_lines],\n                                         isClosed=False, color=255,\n                                         thickness=absolute_baseline_thickness)\n            gt[:, :, np.argmax(DRAWING_COLOR_BASELINES)] = gt_baselines\n\n        if draw_lines:\n            # Draw the lines\n            gt_lines = np.zeros_like(img[:, :, 0])\n            for tl in text_lines:\n                gt_lines = cv2.fillPoly(gt_lines,\n                                        [PAGE.Point.list_to_cv2poly(tl.coords)],\n                                        color=255)\n            gt[:, :, np.argmax(DRAWING_COLOR_LINES)] = gt_lines\n\n        if draw_endpoints:\n            # Draw endpoints of baselines\n            gt_points = np.zeros_like(img[:, :, 0])\n            for tl in text_lines:\n                try:\n                    gt_points = cv2.circle(gt_points, (tl.baseline[0].x, tl.baseline[0].y),\n                                           radius=int((diameter_endpoint / 2 * (gt_points.shape[0] / TARGET_HEIGHT))),\n                                           color=255, thickness=-1)\n                    gt_points = cv2.circle(gt_points, (tl.baseline[-1].x, tl.baseline[-1].y),\n                                           radius=int((diameter_endpoint / 2 * (gt_points.shape[0] / TARGET_HEIGHT))),\n                                           color=255, thickness=-1)\n                except IndexError:\n                    print(\'Length of baseline is {}\'.format(len(tl.baseline)))\n            gt[:, :, np.argmax(DRAWING_COLOR_POINTS)] = gt_points\n\n    # Make output filenames\n    image_label_basename = get_image_label_basename(image_filename)\n    output_image_path = os.path.join(output_dir, \'images\', \'{}.jpg\'.format(image_label_basename))\n    output_label_path = os.path.join(output_dir, \'labels\', \'{}.png\'.format(image_label_basename))\n    # Resize (if necessary) and save image and label\n    save_and_resize(img, output_image_path, size=size)\n    save_and_resize(gt, output_label_path, size=size, nearest=True)\n    # Copy XML file to \'gt\' folder\n    shutil.copy(page_filename, os.path.join(output_dir, \'gt\', \'{}.xml\'.format(image_label_basename)))\n\n    return os.path.abspath(output_image_path), os.path.abspath(output_label_path)\n\n\ndef cbad_set_generator(input_dir: str,\n                       output_dir: str,\n                       img_size: int,\n                       multilabel: bool=False,\n                       draw_baselines: bool=True,\n                       draw_lines: bool=False,\n                       line_thickness: float=0.2,\n                       draw_endpoints: bool=False,\n                       circle_thickness: int =20) -> None:\n    """"""\n    Creates a set with \'images\', \'labels\', \'gt\' folders, classes.txt file and .csv data\n\n    :param input_dir: Input directory containing images and PAGE files\n    :param output_dir: Output directory to save images and labels\n    :param img_size: Size of the resized image (# pixels)\n    :param multilabel: whether the training will have the MULTILABEL prediction type\n    :param draw_baselines: Draws the baselines (boolean)\n    :param draw_lines: Draws the polygon\'s lines (boolean)\n    :param line_thickness: Thickness of annotated baseline (percentage of the line\'s height)\n    :param draw_endpoints: Predict beginning and end of baselines (True, False)\n    :param circle_thickness: Diameter of annotated start/end points\n    :return:\n    """"""\n\n    # Get image filenames to process\n    image_filenames_list = glob(\'{}/**/*.jpg\'.format(input_dir))\n\n    # set\n    os.makedirs(os.path.join(\'{}\'.format(output_dir), \'images\'))\n    os.makedirs(os.path.join(\'{}\'.format(output_dir), \'labels\'))\n    os.makedirs(os.path.join(\'{}\'.format(output_dir), \'gt\'))\n\n    tuples_images_labels = list()\n    for image_filename in tqdm(image_filenames_list):\n        output_image_path, output_label_path = annotate_one_page(image_filename,\n                                                                 output_dir, img_size, draw_baselines=draw_baselines,\n                                                                 draw_lines=draw_lines,\n                                                                 baseline_thickness=line_thickness,\n                                                                 draw_endpoints=draw_endpoints,\n                                                                 diameter_endpoint=circle_thickness)\n\n        tuples_images_labels.append((output_image_path, output_label_path))\n\n    # Create classes.txt file\n    classes = [(0, 0, 0)]\n    if draw_baselines:\n        classes.append(DRAWING_COLOR_BASELINES)\n    if draw_lines:\n        classes.append(DRAWING_COLOR_LINES)\n    if draw_endpoints:\n        classes.append(DRAWING_COLOR_POINTS)\n    if draw_baselines and draw_lines:\n        classes.append(tuple(np.array(DRAWING_COLOR_BASELINES) + np.array(DRAWING_COLOR_LINES)))\n    if draw_baselines and draw_endpoints:\n        classes.append(tuple(np.array(DRAWING_COLOR_BASELINES) + np.array(DRAWING_COLOR_POINTS)))\n    if draw_lines and draw_endpoints:\n        classes.append(tuple(np.array(DRAWING_COLOR_LINES) + np.array(DRAWING_COLOR_POINTS)))\n    if draw_baselines and draw_lines and draw_endpoints:\n        classes.append(tuple(np.array(DRAWING_COLOR_BASELINES) + np.array(DRAWING_COLOR_LINES) + np.array(DRAWING_COLOR_POINTS)))\n\n    # Deal with multiclassification\n    if multilabel:\n        multiclass_codes = np.greater(classes, len(classes) * [[0, 0, 0]]).astype(int)\n        final_classes = np.hstack((classes, multiclass_codes))\n    else:\n        final_classes = classes\n\n    np.savetxt(os.path.join(output_dir, \'classes.txt\'), final_classes, fmt=\'%d\')\n\n    with open(os.path.join(output_dir, \'set_data.csv\'), \'w\') as f:\n        writer = csv.writer(f)\n        for row in tuples_images_labels:\n            writer.writerow(row)\n\n\ndef split_set_for_eval(csv_filename: str) -> None:\n    """"""\n    Splits set into two sets (0.15 and 0.85).\n\n    :param csv_filename: path to csv file containing in each row image_filename,label_filename\n    :return:\n    """"""\n\n    df_data = pd.read_csv(csv_filename, header=None)\n\n    # take 15% for eval\n    df_eval = df_data.sample(frac=0.15, random_state=42)\n    indexes = df_data.index.difference(df_eval.index)\n    df_train = df_data.loc[indexes]\n\n    # save CSVs\n    saving_dir = os.path.dirname(csv_filename)\n    df_eval.to_csv(os.path.join(saving_dir, \'eval_data.csv\'), header=False, index=False, encoding=\'utf8\')\n    df_train.to_csv(os.path.join(saving_dir, \'train_data.csv\'), header=False, index=False, encoding=\'utf8\')\n\n\n# def draw_lines_fn(xml_filename: str, output_dir: str):\n#     """"""\n#     Given an XML PAGE file, draws the corresponding lines in the original image.\n#\n#     :param xml_filename:\n#     :param output_dir:\n#     :return:\n#     """"""\n#     basename = os.path.basename(xml_filename).split(\'.\')[0]\n#     generated_page = PAGE.parse_file(xml_filename)\n#     drawing_img = generated_page.image_filename\n#     generated_page.draw_baselines(drawing_img, color=(0, 0, 255))\n#     imsave(os.path.join(output_dir, \'{}.jpg\'.format(basename)), drawing_img)\n\n\ndef _compute_statistics_line_height(page_class: PAGE.Page, verbose: bool=False) -> Tuple[float, float, float]:\n    """"""\n    Function to compute mean and std of line height in a page.\n\n    :param page_class: PAGE.Page object\n    :param verbose: either to print computational info or not\n    :return: tuple (mean, standard deviation, median)\n    """"""\n    y_lines_coords = [[c.y for c in tl.coords] for tr in page_class.text_regions for tl in tr.text_lines if tl.coords]\n    line_heights = np.array([np.max(y_line_coord) - np.min(y_line_coord) for y_line_coord in y_lines_coords])\n\n    # Remove outliers\n    if len(line_heights) > 3:\n        outliers = _is_outlier(np.array(line_heights))\n        line_heights_filtered = line_heights[~outliers]\n    else:\n        line_heights_filtered = line_heights\n    if verbose:\n        print(\'Considering {}/{} lines to compute line height statistics\'.format(len(line_heights_filtered),\n                                                                                 len(line_heights)))\n\n    # Compute mean, std, median\n    mean = np.mean(line_heights_filtered)\n    median = np.median(line_heights_filtered)\n    standard_deviation = np.std(line_heights_filtered)\n\n    return mean, standard_deviation, median\n\n\ndef _is_outlier(points, thresh=3.5):\n    """"""\n    Returns a boolean array with True if points are outliers and False\n    otherwise. Used to find outliers in 1D data.\n    https://stackoverflow.com/questions/22354094/pythonic-way-of-detecting-outliers-in-one-dimensional-observation-data\n\n    References:\n        Boris Iglewicz and David Hoaglin (1993), ""Volume 16: How to Detect and\n        Handle Outliers"", The ASQC Basic References in Quality Control:\n        Statistical Techniques, Edward F. Mykytka, Ph.D., Editor.\n\n    :param points : An numobservations by numdimensions array of observations\n    :param thresh : The modified z-score to use as a threshold. Observations with\n            a modified z-score (based on the median absolute deviation) greater\n            than this value will be classified as outliers.\n\n    :return: mask : A num_observations-length boolean array.\n    """"""\n    if len(points.shape) == 1:\n        points = points[:, None]\n    median = np.median(points, axis=0)\n    diff = np.sum((points - median)**2, axis=-1)\n    diff = np.sqrt(diff)\n    med_abs_deviation = np.median(diff)\n    # Replace zero values by epsilon\n    if not isinstance(med_abs_deviation, float):\n        med_abs_deviation = np.maximum(med_abs_deviation, len(med_abs_deviation)*[1e-10])\n    else:\n        med_abs_deviation = np.maximum(med_abs_deviation, 1e-10)\n\n    modified_z_score = 0.6745 * diff / med_abs_deviation\n\n    return modified_z_score > thresh\n'"
exps/page/__init__.py,0,"b'#!/usr/bin/env python\n__author__ = ""solivr""\n__license__ = ""GPL""\n'"
exps/page/demo_processing.py,2,"b'#!/usr/bin/env python\n__author__ = ""solivr""\n__license__ = ""GPL""\n\nimport tensorflow as tf\nfrom typing import List\nimport os\nimport cv2\nfrom imageio import imread, imsave\nimport numpy as np\nimport click\nfrom tqdm import tqdm\nfrom dh_segment.inference import LoadedModel\nfrom process import page_post_processing_fn\nfrom dh_segment.post_processing.boxes_detection import find_boxes\nfrom dh_segment.io import PAGE\n\n@click.command()\n@click.argument(\'filenames_to_process\', nargs=-1)\n@click.option(\'--model_dir\', help=""The directory of te model to use"")\n@click.option(\'--output_dir\', help=""Directory to output the PAGEXML files"")\n@click.option(\'--draw_extractions\', help=""If true, the extracted lines will be drawn and exported to the output_dir"")\ndef page_extraction(model_dir: str,\n                    filenames_to_process: List[str],\n                    output_dir: str,\n                    draw_extractions: bool=False,\n                    config: tf.ConfigProto=None):\n\n    os.makedirs(output_dir, exist_ok=True)\n    if draw_extractions:\n        drawing_dir = os.path.join(output_dir, \'drawings\')\n        os.makedirs(drawing_dir)\n\n    with tf.Session(config=config):\n        # Load the model\n        m = LoadedModel(model_dir, predict_mode=\'filename\')\n        for filename in tqdm(filenames_to_process, desc=\'Prediction\'):\n            # Inference\n            prediction = m.predict(filename)\n            probs = prediction[\'probs\'][0]\n            original_shape = prediction[\'original_shape\']\n\n            probs = probs / np.max(probs)  # Normalize to be in [0, 1]\n            # Binarize the predictions\n            page_bin = page_post_processing_fn(probs, threshold=-1)\n\n            # Upscale to have full resolution image (cv2 uses (w,h) and not (h,w) for giving shapes)\n            bin_upscaled = cv2.resize(page_bin.astype(np.uint8, copy=False),\n                                      tuple(original_shape[::-1]), interpolation=cv2.INTER_NEAREST)\n\n            # Find quadrilateral enclosing the page\n            pred_page_coords = find_boxes(bin_upscaled.astype(np.uint8, copy=False),\n                                          mode=\'min_rectangle\', min_area=0.2, n_max_boxes=1)\n\n            if pred_page_coords is not None:\n                # Write corners points into a .txt file\n\n                # Create page region and XML file\n                page_border = PAGE.Border(coords=PAGE.Point.cv2_to_point_list(pred_page_coords[:, None, :]))\n\n                if draw_extractions:\n                    # Draw page box on original image and export it. Add also box coordinates to the txt file\n                    original_img = imread(filename, pilmode=\'RGB\')\n                    cv2.polylines(original_img, [pred_page_coords[:, None, :]], True, (0, 0, 255), thickness=5)\n\n                    basename = os.path.basename(filename).split(\'.\')[0]\n                    imsave(os.path.join(drawing_dir, \'{}_boxes.jpg\'.format(basename)), original_img)\n\n            else:\n                print(\'No box found in {}\'.format(filename))\n                page_border = PAGE.Border()\n\n            page_xml = PAGE.Page(image_filename=filename, image_width=original_shape[1], image_height=original_shape[0],\n                                 page_border=page_border)\n            xml_filename = os.path.join(output_dir, \'{}.xml\'.format(basename))\n            page_xml.write_to_file(xml_filename, creator_name=\'PageExtractor\')\n\n\nif __name__ == \'__main__\':\n    page_extraction()\n'"
exps/page/evaluation.py,0,"b'#!/usr/bin/env python\n__author__ = ""solivr""\n__license__ = ""GPL""\n\nfrom tqdm import tqdm\nfrom glob import glob\nimport os\nfrom imageio import imread\nimport numpy as np\nfrom .process import extract_page\nfrom dh_segment.utils.evaluation import intersection_over_union, Metrics\n\n\nPP_PARAMS = {\'threshold\': -1, \'kernel_size\': 5}\n\n\ndef eval_fn(input_dir: str, groundtruth_dir: str, post_process_params: dict=PP_PARAMS) -> Metrics:\n    """"""\n\n    :param input_dir: directory containing the predictions .npy files (range [0, 255])\n    :param groundtruth_dir: directory containing the ground truth images (.png) (must have the same name as predictions\n                            files in input_dir)\n    :param post_process_params: params for post processing fn\n    :return: Metrics object containing all the necessary metrics\n    """"""\n    global_metrics = Metrics()\n    for file in tqdm(glob(os.path.join(input_dir, \'*.npy\'))):\n        basename = os.path.basename(file).split(\'.\')[0]\n\n        prediction = np.load(file)\n        label_image = imread(os.path.join(groundtruth_dir, \'{}.png\'.format(basename)), pilmode=\'L\')\n\n        pred_box = extract_page(prediction / np.max(prediction), post_process_params=post_process_params)\n        label_box = extract_page(label_image / np.max(label_image), min_area=0.0)\n\n        if pred_box is not None and label_box is not None:\n            iou = intersection_over_union(label_box[:, None, :], pred_box[:, None, :], label_image.shape)\n            global_metrics.IOU_list.append(iou)\n        else:\n            global_metrics.IOU_list.append(0)\n\n    global_metrics.compute_miou()\n    print(\'EVAL --- mIOU : {}\\n\'.format(global_metrics.mIOU))\n\n    return global_metrics\n'"
exps/page/make_page.py,0,"b'#!/usr/bin/env python\n__author__ = ""solivr""\n__license__ = ""GPL""\n\nimport os\nimport click\nfrom exps.commonutils import cbad_download\nfrom utils import page_files_download, page_set_annotator, \\\n    format_txt_file, TRAIN_TXT_FILENAME, TEST_TXT_FILENAME, EVAL_TXT_FILENAME\n\n\n@click.command()\n@click.option(\'--downloading_dir\', help=\'Directory to download the cBAD-ICDAR17 dataset\')\n@click.option(\'--masks_dir\', help=""Directory where to output the generated masks"")\ndef generate_page_dataset(downloading_dir: str, masks_dir: str):\n    # Check if dataset has already been downloaded\n    if os.path.exists(downloading_dir):\n        print(\'Dataset has already been downloaded at {}. Skipping process.\'.format(downloading_dir))\n    else:\n        # Download dataset\n        cbad_download(downloading_dir)\n\n    page_txt_folder = os.path.join(downloading_dir, \'page-txt-files\')\n    if os.path.exists(page_txt_folder):\n        print(\'Page txt files have already been downloaded at {}. Skipping process.\'.format(page_txt_folder))\n    else:\n        # Download files\n        page_files_download(page_txt_folder)\n\n    tuple_train = (os.path.join(page_txt_folder, TRAIN_TXT_FILENAME),\n                   \'_formatted.\'.join(TRAIN_TXT_FILENAME.split(\'.\')),\n                   os.path.join(masks_dir, \'train\'))\n    tuple_test = (os.path.join(page_txt_folder, TEST_TXT_FILENAME),\n                  \'_formatted.\'.join(TEST_TXT_FILENAME.split(\'.\')),\n                  os.path.join(masks_dir, \'test\'))\n    tuple_eval = (os.path.join(page_txt_folder, EVAL_TXT_FILENAME),\n                  \'_formatted.\'.join(EVAL_TXT_FILENAME.split(\'.\')),\n                  os.path.join(masks_dir, \'eval\'))\n\n    print(\'Creating sets\')\n    for tup in [tuple_train, tuple_test, tuple_eval]:\n        input_txt_filename, output_txt_filename, set_masks_dir = tup\n\n        # Format txt files\n        format_txt_file(input_txt_filename, output_txt_filename, downloading_dir)\n\n        # Create masks\n        os.makedirs(set_masks_dir, exist_ok=True)\n        page_set_annotator(output_txt_filename, set_masks_dir)\n\n    print(\'Done!\')\n\nif __name__ == \'__main__\':\n    generate_page_dataset()\n'"
exps/page/process.py,2,"b'#!/usr/bin/env python\n__author__ = ""solivr""\n__license__ = ""GPL""\n\nimport tensorflow as tf\nimport os\nimport numpy as np\nfrom tqdm import tqdm\nfrom glob import glob\nfrom dh_segment.inference import LoadedModel\nfrom imageio import imsave\nfrom dh_segment.post_processing import binarization\nfrom dh_segment.post_processing.boxes_detection import find_boxes\n\n\ndef prediction_fn(model_dir: str, input_dir: str, output_dir: str=None, tf_config: tf.ConfigProto=None) -> None:\n    """"""\n    Given a model directory this function will load the model and apply it to the files (.jpg, .png) found in input_dir.\n    The predictions will be saved in output_dir as .npy files (values ranging [0,255])\n\n    :param model_dir: Directory containing the saved model\n    :param input_dir: input directory where the images to predict are\n    :param output_dir: output directory to save the predictions (probability images)\n    :return:\n    """"""\n    if not output_dir:\n        # For model_dir of style model_name/export/timestamp/ this will create a folder model_name/predictions\'\n        output_dir = \'{}\'.format(os.path.sep).join(model_dir.split(os.path.sep)[:-3] + [\'predictions\'])\n\n    os.makedirs(output_dir, exist_ok=True)\n    filenames_to_predict = glob(os.path.join(input_dir, \'*.jpg\')) + glob(os.path.join(input_dir, \'*.png\'))\n    # Load model\n    with tf.Session(config=tf_config):\n        m = LoadedModel(model_dir, predict_mode=\'filename_original_shape\')\n        for filename in tqdm(filenames_to_predict, desc=\'Prediction\'):\n            pred = m.predict(filename)[\'probs\'][0]\n            np.save(os.path.join(output_dir, os.path.basename(filename).split(\'.\')[0]), np.uint8(255 * pred))\n\n\ndef page_post_processing_fn(probs: np.ndarray, threshold: float=0.5, output_basename: str=None,\n                            kernel_size: int = 5) -> np.ndarray:\n    """"""\n    Computes the binary mask of the detected Page from the probabilities outputed by network\n\n    :param probs: array in range [0, 1] of shape HxWx2\n    :param threshold: threshold between [0 and 1], if negative Otsu\'s adaptive threshold will be used\n    :param output_basename:\n    :param kernel_size: size of kernel for morphological cleaning\n    """"""\n\n    mask = binarization.thresholding(probs[:, :, 1], threshold=threshold)\n    result = binarization.cleaning_binary(mask, kernel_size=kernel_size)\n\n    if output_basename is not None:\n        imsave(\'{}.png\'.format(output_basename), result*255)\n    return result\n\n\ndef format_quad_to_string(quad):\n    s = \'\'\n    for corner in quad:\n        s += \'{},{},\'.format(corner[0], corner[1])\n    return s[:-1]\n\n\ndef extract_page(prediction: np.ndarray, min_area: float=0.2, post_process_params: dict=None) -> list():\n    """"""\n    Given an image with probabilities, post-processes it and extracts one box\n\n    :param prediction: probability mask [0, 1]\n    :param min_area: minimum area to be considered as a valid extraction\n    :param post_process_params: params for page prost processing function\n    :return: list of coordinates of boxe\n    """"""\n    if post_process_params:\n        post_pred = page_post_processing_fn(prediction, **post_process_params)\n    else:\n        post_pred = prediction\n    pred_box = find_boxes(np.uint8(post_pred), mode=\'quadrilateral\', min_area=min_area, n_max_boxes=1)\n\n    return pred_box\n'"
exps/page/utils.py,0,"b'#!/usr/bin/env python\n__author__ = ""solivr""\n__license__ = ""GPL""\n\nfrom imageio import imread, imsave\nimport numpy as np\nimport cv2\nimport os\nimport re\nfrom tqdm import tqdm\nimport urllib.request\nfrom exps.commonutils import _progress_hook, CBAD_TEST_SIMPLE_FOLDER, CBAD_TEST_COMPLEX_FOLDER, \\\n    CBAD_TRAIN_SIMPLE_FOLDER, CBAD_TRAIN_COMPLEX_FOLDER\n\nTRAIN_FILE_URL = \'https://raw.githubusercontent.com/ctensmeyer/pagenet/master/annotations/cbad_train_annotator_1.txt\'\nTEST_FILE_URL = \'https://raw.githubusercontent.com/ctensmeyer/pagenet/master/annotations/cbad_test_annotator_1.txt\'\nEVAL_FILE_URL = \'https://raw.githubusercontent.com/ctensmeyer/pagenet/master/annotations/cbad_val_annotator_1.txt\'\n\nTRAIN_TXT_FILENAME = \'page_train.txt\'\nTEST_TXT_FILENAME = \'page_test.txt\'\nEVAL_TXT_FILENAME = \'page_eval.txt\'\n\n\ndef get_coords_form_txt_line(line: str)-> tuple:\n    """"""\n    gets the coordinates of the page from the txt file (line-wise)\n\n    :param line: line of the .txt file\n    :return: coordinates, filename\n    """"""\n    splits = line.split(\',\')\n    full_filename = splits[0]\n    splits = splits[1:]\n    if splits[-1] in [\'SINGLE\', \'ABNORMAL\']:\n        coords_simple = np.reshape(np.array(splits[:-1], dtype=int), (4, 2))\n        # coords_double = None\n        coords = coords_simple\n    else:\n        coords_simple = np.reshape(np.array(splits[:8], dtype=int), (4, 2))\n        # coords_double = np.reshape(np.array(splits[-4:], dtype=int), (2, 2))\n        # coords = (coords_simple, coords_double)\n        coords = coords_simple\n\n    return coords, full_filename\n\n\ndef make_binary_mask(txt_file):\n    """"""\n    From export txt file with filnenames and coordinates of qudrilaterals, generate binary mask of page\n\n    :param txt_file: txt file filename\n    :return:\n    """"""\n    for line in open(txt_file, \'r\'):\n        dirname, _ = os.path.split(txt_file)\n        c, full_name = get_coords_form_txt_line(line)\n        img = imread(full_name)\n        label_img = np.zeros((img.shape[0], img.shape[1]), np.uint8)\n        label_img = cv2.fillPoly(label_img, [c[:, None, :]], 255)\n        basename = os.path.basename(full_name)\n        imsave(os.path.join(dirname, \'{}_bin.png\'.format(basename.split(\'.\')[0])), label_img)\n\n\ndef page_set_annotator(txt_filename: str, output_dir: str):\n    """"""\n    Given a txt file (filename, coords corners), generates a dataset of images + labels\n\n    :param txt_filename: File (txt) containing list of images\n    :param input_dir: Root directory to original images\n    :param output_dir: Output directory for generated dataset\n    :return:\n    """"""\n\n    output_img_dir = os.path.join(output_dir, \'images\')\n    output_label_dir = os.path.join(output_dir, \'labels\')\n    os.makedirs(output_img_dir, exist_ok=True)\n    os.makedirs(output_label_dir, exist_ok=True)\n\n    for line in tqdm(open(txt_filename, \'r\')):\n        coords, image_filename = get_coords_form_txt_line(line)\n\n        try:\n            img = imread(image_filename)\n        except FileNotFoundError:\n            print(\'File {} not found\'.format(image_filename))\n            continue\n        label_img = np.zeros((img.shape[0], img.shape[1], 3))\n\n        label_img = cv2.fillPoly(label_img, [coords], (255, 0, 0))\n        # if coords_double is not None:\n        #     label_img = cv2.polylines(label_img, [coords_double], False, color=(0, 0, 0), thickness=50)\n\n        collection, filename = image_filename.split(os.path.sep)[-2:]\n\n        imsave(os.path.join(output_img_dir, \'{}_{}.jpg\'.format(collection.split(\'_\')[0], filename.split(\'.\')[0])), img.astype(np.uint8))\n        imsave(os.path.join(output_label_dir, \'{}_{}.png\'.format(collection.split(\'_\')[0], filename.split(\'.\')[0])), label_img.astype(np.uint8))\n\n    # Class file\n    classes = np.stack([(0, 0, 0), (255, 0, 0)])\n    np.savetxt(os.path.join(output_dir, \'classes.txt\'), classes, fmt=\'%d\')\n\n# -----------------------------\n\n\ndef page_files_download(output_dir: str) -> None:\n    """"""\n    Download Page txt files from github repository.\n\n    :param output_dir: folder where to download the data\n    :return:\n    """"""\n    os.makedirs(output_dir, exist_ok=True)\n    train_filename = os.path.join(output_dir, TRAIN_TXT_FILENAME)\n    test_filename = os.path.join(output_dir, TEST_TXT_FILENAME)\n    eval_filename = os.path.join(output_dir, EVAL_TXT_FILENAME)\n\n    with tqdm(unit=\'B\', unit_scale=True, unit_divisor=1024, miniters=1, desc=""Downloading train file"") as t:\n        urllib.request.urlretrieve(TRAIN_FILE_URL, train_filename, reporthook=_progress_hook(t))\n    with tqdm(unit=\'B\', unit_scale=True, unit_divisor=1024, miniters=1, desc=""Downloading test file"") as t:\n        urllib.request.urlretrieve(TEST_FILE_URL, test_filename, reporthook=_progress_hook(t))\n    with tqdm(unit=\'B\', unit_scale=True, unit_divisor=1024, miniters=1, desc=""Downloading eval file"") as t:\n        urllib.request.urlretrieve(EVAL_FILE_URL, eval_filename, reporthook=_progress_hook(t))\n\n    print(\'Page files downloaded successfully!\')\n\n\ndef format_txt_file(input_txt_filename: str,\n                    output_txt_filename: str,\n                    cbad_data_folder: str) -> None:\n    """"""\n    Transforms the relative path of the images into absolute path.\n\n    :param input_txt_filename: original downloaded .txt filename\n    :param output_txt_filename: filename of the formatted content\n    :param cbad_data_folder: path to the folder containing the READ-BAD data\n    :return:\n    """"""\n    final_tokens = list()\n    for line in open(input_txt_filename, \'r\'):\n        tokens = line.split(\',\')\n        filename = tokens[0]\n        full_filename = os.path.join(os.path.abspath(cbad_data_folder), filename)\n\n        if \'complex\' in filename:\n            pattern = \'complex\'\n            candidate_folders = [CBAD_TRAIN_COMPLEX_FOLDER, CBAD_TEST_COMPLEX_FOLDER]\n        elif \'simple\' in filename:\n            pattern = \'simple\'\n            candidate_folders = [CBAD_TRAIN_SIMPLE_FOLDER, CBAD_TEST_SIMPLE_FOLDER]\n        else:\n            raise Exception\n\n        option1 = re.sub(pattern, candidate_folders[0], full_filename)\n        option2 = re.sub(pattern, candidate_folders[1], full_filename)\n        # .JPG files\n        option3 = re.sub(pattern, candidate_folders[0], full_filename.split(\'.\')[0] + \'.JPG\')\n        option4 = re.sub(pattern, candidate_folders[1], full_filename.split(\'.\')[0] + \'.JPG\')\n\n        if os.path.exists(option1):\n            tokens[0] = option1\n        elif os.path.exists(option2):\n            tokens[0] = option2\n        elif os.path.exists(option3):\n            tokens[0] = option3\n        elif os.path.exists(option4):\n            tokens[0] = option4\n        else:\n            raise FileNotFoundError(\'for {}\'.format(filename))\n\n        final_tokens.append(\',\'.join(tokens))\n\n    with open(output_txt_filename, \'w\') as f:\n        for line in final_tokens:\n            f.write(line)\n'"
