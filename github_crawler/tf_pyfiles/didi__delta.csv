file_path,api_count,code
setup.py,2,"b'from setuptools import setup\nfrom setuptools import find_packages\nfrom setuptools import Extension\nfrom datetime import date\nimport os\nimport sys\nfrom glob import glob\nimport tensorflow as tf\nfrom absl import logging\n\nlogging.set_verbosity(logging.INFO)\n\n\nTF_INCLUDE, TF_CFLAG = tf.sysconfig.get_compile_flags()\nTF_INCLUDE = TF_INCLUDE.split(\'-I\')[1]\n\nTF_LIB_INC, TF_SO_LIB = tf.sysconfig.get_link_flags()\nTF_SO_LIB = TF_SO_LIB.replace(\'-l:libtensorflow_framework.2.dylib\',\n                              \'-ltensorflow_framework.2\')\nTF_LIB_INC = TF_LIB_INC.split(\'-L\')[1]\nTF_SO_LIB = TF_SO_LIB.split(\'-l\')[1]\n\nlogging.info(""TF_INCLUDE: {}"".format(TF_INCLUDE))\nlogging.info(""TF_CFLAG: {}"".format(TF_CFLAG))\nlogging.info(""TF_LIB_INC: {}"".format(TF_LIB_INC))\nlogging.info(""TF_SO_LIB: {}"".format(TF_SO_LIB))\n\nNAME = ""delta-nlp""\nGITHUB_USER_NAME = ""didi""\nAUTHOR = ""Speech@DiDi""\nAUTHOR_EMAIL = ""speech@didiglobal.com""\nMAINTAINER = ""applenob""\nMAINTAINER_EMAIL = ""chenjunwen@didiglobal.com""\nREPO_NAME = os.path.basename(os.getcwd())\nURL = ""https://github.com/{0}/{1}"".format(GITHUB_USER_NAME, REPO_NAME)\nGITHUB_RELEASE_TAG = str(date.today())\nDOWNLOAD_URL = ""https://github.com/{0}/{1}/tarball/{2}"".format(\n    GITHUB_USER_NAME, REPO_NAME, GITHUB_RELEASE_TAG)\nSHORT_DESCRIPTION = ""DELTA is a deep learning based natural language and speech processing platform.""\nLONG_DESCRIPTION = """"""\n# DELTA - A DEep learning Language Technology plAtform\n\nDELTA is a deep learning based end-to-end natural language and speech processing platform. DELTA aims to provide easy and fast experiences for using, deploying, and developing natural language processing and speech models for both academia and industry use cases. DELTA is mainly implemented using TensorFlow and Python 3.\n\nRefer to github for more information: https://github.com/didi/delta\n""""""\nPLATFORMS = [""MacOS"", ""Unix""]\nCLASSIFIERS = [\n    ""Development Status :: 4 - Beta"",\n    ""Intended Audience :: Science/Research"",\n    ""License :: OSI Approved :: Apache Software License"",\n    ""Natural Language :: English"",\n    ""Operating System :: MacOS"",\n    ""Operating System :: POSIX :: Linux"",\n    ""Programming Language :: Python :: 3.6""\n]\nthis_directory = os.path.abspath(os.path.dirname(__file__))\n\n\ndef get_requires():\n  require_file = ""tools/requirements.txt""\n  try:\n    f = open(require_file)\n    requires = [i.strip() for i in f.read().split(""\\n"")]\n  except:\n    logging.info(""{} not found!"".format(require_file))\n    requires = list()\n  return requires\n\n\ncomplie_args = [TF_CFLAG, ""-fPIC"", ""-shared"", ""-O2"", ""-std=c++11""]\nif sys.platform == \'darwin\':  # Mac os X before Mavericks (10.9)\n  complie_args.append(""-stdlib=libc++"")\ncppjieba_includes = [""tools/cppjieba/deps"",\n                     ""tools/cppjieba/include""]\ninclude_dirs = [\'delta\', \'delta/layers/ops/\', TF_INCLUDE] + cppjieba_includes\n\nmodule = Extension(\'delta.layers.ops.x_ops\',\n                   sources=glob(\'delta/layers/ops/kernels/*.cc\'),\n                   extra_compile_args=complie_args,\n                   include_dirs=include_dirs,\n                   library_dirs=[TF_LIB_INC],\n                   libraries=[TF_SO_LIB],\n                   language=\'c++\')\nlicense_ = ""Apache Software License""\npackages = find_packages()\nlogging.info(""LONG_DESCRIPTION: {}"".format(LONG_DESCRIPTION))\nlogging.info(""license: {}"".format(license_))\nlogging.info(""packages: {}"".format(packages))\n\ncustom_op_files = glob(""delta/layers/ops/x_ops*.so"")\nif len(custom_op_files) > 0:\n  for custom_op_file in custom_op_files:\n    if os.path.exists(custom_op_file):\n      logging.info(""Remove file {}."".format(custom_op_file))\n      os.remove(custom_op_file)\n\nsetup(\n    name=NAME,\n    description=SHORT_DESCRIPTION,\n    long_description=LONG_DESCRIPTION,\n    long_description_content_type=""text/markdown"",\n    version=""0.2.1"",\n    author=AUTHOR,\n    author_email=AUTHOR_EMAIL,\n    maintainer=MAINTAINER,\n    maintainer_email=MAINTAINER_EMAIL,\n    packages=packages,\n    package_data={""delta"": [""resources/cppjieba_dict/*.utf8""]},\n    entry_points={\n        \'console_scripts\': [\n            \'delta = delta.main:nlp_entry\']},\n    url=URL,\n    download_url=DOWNLOAD_URL,\n    classifiers=CLASSIFIERS,\n    platforms=PLATFORMS,\n    license=license_,\n    install_requires=get_requires(),\n    ext_modules=[module]\n)\n'"
core/__init__.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\n'"
delta/__init__.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport os\n\nPACKAGE_ROOT_DIR = os.path.dirname(os.path.abspath(__file__))\n'"
delta/compat.py,0,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""The compatible tensorflow library.""""""\n\nfrom tensorflow.compat.v1 import *  # pylint:disable=wildcard-import\n\n# Import absl.flags and absl.logging to overwrite the Tensorflow ones.\n# This is the intended behavior in TF 2.0.\n# pylint:disable=g-bad-import-order, unused-import, g-import-not-at-top\nfrom absl import flags\nfrom absl import logging\n# pylint: disable=g-direct-tensorflow-import\nfrom tensorflow.python.compat import v2_compat\n\nfrom tensorflow.python.framework import function\n# pylint: enable=g-direct-tensorflow-import\n\nv2_compat.disable_v2_behavior()\nDefun = function.Defun\n'"
delta/compat_test.py,6,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for compat.py.""""""\n\nimport delta.compat as tf\nfrom tensorflow.python.framework import function  # pylint:disable=g-direct-tensorflow-import\n\n\nclass CompatTest(tf.test.TestCase):\n\n  def testSomeTFSymbols(self):\n    self.assertFalse(tf.executing_eagerly())\n    self.assertIsNotNone(tf.logging)\n    self.assertIsNotNone(tf.flags)\n    self.assertIs(tf.Defun, function.Defun)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
delta/main.py,1,"b'#!/usr/bin/env python\n# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Main entrance of the program.""""""\n\nimport random\nimport numpy as np\nimport delta.compat as tf\nfrom absl import flags\nfrom absl import app\nfrom absl import logging\n\nfrom delta import utils\nfrom delta.utils.register import registers\nfrom delta.utils.register import import_all_modules_for_register\nfrom delta.data.datasets.build import build_dataset\n\n\ndef define_flags():\n  """"""Define flags for the program.""""""\n  flags.DEFINE_string(\'config\', \'\', \'config path\')\n  flags.DEFINE_string(\n      \'cmd\', \'\',\n      \'train, eval, infer, train_and_eval, export_model, gen_feat, gen_cmvn, build\')\n  flags.DEFINE_bool(\'test\', \'False\', \'run all unit test\')\n  flags.DEFINE_bool(\'dry_run\', \'False\', \'dry run, to no save file\')\n  flags.DEFINE_bool(\'log_debug\', \'False\', \'logging debug switch\')\n  flags.DEFINE_string(\'name\', \'\', \'Data set name\')\n  flags.DEFINE_string(\'dir\', \'\', \'Data set directory\')\nFLAGS = flags.FLAGS\n\n\ndef set_seed(config):\n  """"""Set the random seed.""""""\n  random_seed = config[\'solver\'][\'run_config\'][\'tf_random_seed\']\n  random.seed(random_seed)\n  np.random.seed(random_seed)\n  tf.set_random_seed(random_seed)\n\n\ndef main(argv):\n  """"""\n    main function\n  """"""\n  # pylint: disable=unused-argument\n\n  if FLAGS.config != \'\':\n    config = utils.load_config(FLAGS.config)\n    utils.set_logging(FLAGS.log_debug, config)\n\n    utils.copy_config(FLAGS.config, config)\n    set_seed(config)\n  else:\n    config = None\n\n  logging.info(""Loading all modules ..."")\n  import_all_modules_for_register(config, only_nlp=FLAGS.only_nlp)\n\n  logging.info(""CMD: {}"".format(FLAGS.cmd))\n  if FLAGS.cmd == \'train\' or FLAGS.cmd == \'train_and_eval\' or \\\n    FLAGS.cmd == \'eval\' or FLAGS.cmd == \'infer\' or \\\n    FLAGS.cmd == \'export_model\' or FLAGS.cmd == \'gen_feat\' or \\\n    FLAGS.cmd == \'gen_cmvn\':\n    solver_name = config[\'solver\'][\'name\']\n    solver = registers.solver[solver_name](config)\n    # config after process\n    config = solver.config\n    task_name = config[\'data\'][\'task\'][\'name\']\n    task_class = registers.task[task_name]\n    if FLAGS.cmd == \'train\':\n      solver.train()\n    elif FLAGS.cmd == \'train_and_eval\':\n      solver.train_and_eval()\n    elif FLAGS.cmd == \'eval\':\n      solver.eval()\n    elif FLAGS.cmd == \'infer\':\n      solver.infer(yield_single_examples=False)\n    elif FLAGS.cmd == \'export_model\':\n      solver.export_model()\n    elif FLAGS.cmd == \'gen_feat\':\n      assert config[\'data\'][\'task\'][\n          \'suffix\'] == \'.npy\', \'wav does not need to extractor feature\'\n      paths = []\n      for mode in [utils.TRAIN, utils.EVAL, utils.INFER]:\n        paths += config[\'data\'][mode][\'paths\']\n      task = task_class(config, utils.INFER)\n      task.generate_feat(paths, dry_run=FLAGS.dry_run)\n    elif FLAGS.cmd == \'gen_cmvn\':\n      logging.info(\n          \'\'\'using infer pipeline to compute cmvn of train_paths, and stride must be 1\'\'\'\n      )\n      paths = config[\'data\'][utils.TRAIN][\'paths\']\n      segments = config[\'data\'][utils.TRAIN][\'segments\']\n      config[\'data\'][utils.INFER][\'paths\'] = paths\n      config[\'data\'][utils.INFER][\'segments\'] = segments\n      task = task_class(config, utils.INFER)\n      task.generate_cmvn(dry_run=FLAGS.dry_run)\n  elif FLAGS.cmd == \'build\':\n    build_dataset(FLAGS.name, FLAGS.dir)\n  else:\n    raise ValueError(""Not support command: {}."".format(FLAGS.cmd))\n\n\ndef entry():\n  define_flags()\n  flags.DEFINE_bool(\'only_nlp\', \'False\', \'only use nlp modules\')\n  logging.info(""Deep Language Technology Platform start..."")\n  app.run(main)\n  logging.info(""OK. Done!"")\n\n\ndef nlp_entry():\n  define_flags()\n  flags.DEFINE_bool(\'only_nlp\', \'True\', \'only use nlp modules\')\n  logging.info(""Deep Language Technology Platform start..."")\n  app.run(main)\n  logging.info(""OK. Done!"")\n\n\nif __name__ == \'__main__\':\n  entry()\n'"
docs/conf.py,0,"b'# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\n\n# https://github.com/rtfd/recommonmark\nimport recommonmark.parser\n# https://github.com/rtfd/sphinx_rtd_theme\nimport sphinx_rtd_theme\nfrom sphinx.application import Sphinx\n\n# -- Project information -----------------------------------------------------\n\nproject = \'DELTA\'\nauthor = \'Hui Zhang\'\ncopyright = \'2019, Hui Zhang\'\n\n# The full version, including alpha/beta/rc tags\nrelease = \'1.0\'\n\n\n# -- General configuration ---------------------------------------------------\nsource_parsers = {\n    \'.md\': recommonmark.parser.CommonMarkParser,\n}\nsource_suffix = [\'.rst\', \'.md\']\n\nmaster_doc = \'index\'\npygments_style = \'sphinx\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n\'sphinx.ext.autodoc\',\n\'sphinx.ext.viewcode\',\n\'sphinx.ext.todo\',\n\'sphinx.ext.mathjax\'\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\n\'_build\', \n]\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\nsmartquotes = False\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n'"
tools/release_notes.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n"""""" Create release notes with the issues from a milestone.\n    python release_notes.py -c didi delta v.xxxxx\n""""""\n\nimport argparse\nimport urllib.request\nimport json\nimport collections\n\ngithub_url = \'https://api.github.com/repos\'\n\nif __name__ == \'__main__\':\n\n  # Parse arguments\n\n  parser = argparse.ArgumentParser(\n    description=\'Create a draft release with the issues from a milestone.\'\n  )\n\n  parser.add_argument(\n    \'user\',\n    metavar=\'user\',\n    type=str,\n    help=\'github user\'\n  )\n\n  parser.add_argument(\n    \'repository\',\n    metavar=\'repository\',\n    type=str,\n    help=\'github repository\'\n  )\n\n  parser.add_argument(\n    \'milestone\',\n    metavar=\'milestone\',\n    type=str,\n    help=\'name of used milestone\'\n  )\n\n  parser.add_argument(\n    \'-c\', \'--closed\',\n    help=\'Fetch closed milestones/issues\',\n    action=\'store_true\'\n  )\n\n  args = parser.parse_args()\n\n  # Fetch milestone id\n\n  url = ""%s/%s/%s/milestones"" % (\n    github_url,\n    args.user,\n    args.repository\n  )\n\n  headers = {\n    \'Origin\': \'https://github.com\',\n    \'User-Agent\': \'Mozilla/5.0 (X11; Linux x86_64) \'\n                  \'AppleWebKit/537.11 (KHTML, like Gecko) \'\n                  \'Chrome/23.0.1271.64 Safari/537.11\',\n    \'Accept\': \'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\',\n    \'Accept-Charset\': \'ISO-8859-1,utf-8;q=0.7,*;q=0.3\',\n    \'Accept-Encoding\': \'none\',\n    \'Accept-Language\': \'en-US,en;q=0.8\',\n    \'Connection\': \'keep-alive\'}\n\n  if args.closed:\n    url += ""?state=closed""\n  req = urllib.request.Request(url, headers=headers)\n  github_request = urllib.request.urlopen(req)\n\n  if not github_request:\n    parser.error(\'Cannot read milestone list.\')\n\n  decoder = json.JSONDecoder()\n\n  milestones = decoder.decode(github_request.read().decode(\'utf-8\'))\n\n  print(\'parse milestones\')\n\n  github_request.close()\n\n  milestone_id = None\n  for milestone in milestones:\n    if milestone[\'title\'] == args.milestone:\n      milestone_id = milestone[\'number\']\n  if not milestone_id:\n    parser.error(\'Cannot find milestone\')\n\n  url = \'%s/%s/%s/issues?milestone=%d\' % (\n    github_url,\n    args.user,\n    args.repository,\n    milestone_id\n  )\n\n  if args.closed:\n    url += ""&state=closed""\n  req = urllib.request.Request(url, headers=headers)\n  github_request = urllib.request.urlopen(req)\n  if not github_request:\n    parser.error(\'Cannot read issue list.\')\n\n  issues = decoder.decode(github_request.read().decode(\'utf-8\'))\n  print(\'parse issues\')\n  github_request.close()\n\n  final_data = []\n  labels = []\n  thanks_to = []\n  for issue in issues:\n\n    for label in issue[\'labels\']:\n      labels.append(label[\'name\'])\n\n    thanks_to.append(\'@%s\' % (issue[\'user\'][\'login\']))\n    final_data.append(\' * **[%s]** - %s #%d by **@%s**\\n\' % (\n      label[\'name\'],\n      issue[\'title\'],\n      issue[\'number\'],\n      issue[\'user\'][\'login\']\n    ))\n\n  dic = collections.defaultdict(set)\n  for l_release in list(set(labels)):\n\n    for f_data in final_data:\n      if \'[%s]\' % l_release in f_data:\n        dic[l_release].add(f_data)\n\n  for key, value in dic.items():\n    print(\'# %s\\n%s\' % (key, \'\'.join(value)))\n\n  print(\'# %s\\n%s\' % (\'Acknowledgements\', \'Special thanks to %s \' % (\'  \'.join(list(set(thanks_to))))))\n'"
utils/avg_checkpoints.py,14,"b'#!/usr/bin/env python3\n# Copyright 2019 The Tensor2Tensor Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Script to average values of variables in a list of checkpoint files.""""""\nimport os\nimport six\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\nfrom six.moves import zip  # pylint: disable=redefined-builtin\nimport numpy as np\nimport delta.compat as tf\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string(""checkpoints"", """",\n                    ""Comma-separated list of checkpoints to average."")\nflags.DEFINE_integer(\n    ""num_last_checkpoints"", 0, ""Averages the last N saved checkpoints.""\n    "" If the checkpoints flag is set, this is ignored."")\nflags.DEFINE_string(""prefix"", """",\n                    ""Prefix (e.g., directory) to append to each checkpoint."")\nflags.DEFINE_string(""output_path"", ""/tmp/averaged.ckpt"",\n                    ""Path to output the averaged checkpoint to."")\n\n\ndef checkpoint_exists(path):\n  return (tf.io.gfile.exists(path) or tf.io.gfile.exists(path + "".meta"") or\n          tf.io.gfile.exists(path + "".index""))\n\n\ndef main(_):\n  if FLAGS.checkpoints:\n    # Get the checkpoints list from flags and run some basic checks.\n    checkpoints = [c.strip() for c in FLAGS.checkpoints.split("","")]\n    checkpoints = [c for c in checkpoints if c]\n    if not checkpoints:\n      raise ValueError(""No checkpoints provided for averaging."")\n    if FLAGS.prefix:\n      checkpoints = [FLAGS.prefix + c for c in checkpoints]\n  else:\n    assert FLAGS.num_last_checkpoints >= 1, ""Must average at least one model""\n    assert FLAGS.prefix, (""Prefix must be provided when averaging last""\n                          "" N checkpoints"")\n    checkpoint_state = tf.train.get_checkpoint_state(\n        os.path.dirname(FLAGS.prefix))\n    # Checkpoints are ordered from oldest to newest.\n    checkpoints = checkpoint_state.all_model_checkpoint_paths[\n        -FLAGS.num_last_checkpoints:]\n\n  checkpoints = [c for c in checkpoints if checkpoint_exists(c)]\n  if not checkpoints:\n    if FLAGS.checkpoints:\n      raise ValueError(""None of the provided checkpoints exist. %s"" %\n                       FLAGS.checkpoints)\n    else:\n      raise ValueError(""Could not find checkpoints at %s"" %\n                       os.path.dirname(FLAGS.prefix))\n\n  # Read variables from all checkpoints and average them.\n  logging.info(""Reading variables and averaging checkpoints:"")\n  for c in checkpoints:\n    logging.info(""%s "", c)\n  var_list = tf.train.list_variables(checkpoints[0])\n  var_values, var_dtypes = {}, {}\n  for (name, shape) in var_list:\n    if not name.startswith(""global_step""):\n      var_values[name] = np.zeros(shape)\n  for checkpoint in checkpoints:\n    reader = tf.train.load_checkpoint(checkpoint)\n    for name in var_values:\n      tensor = reader.get_tensor(name)\n      var_dtypes[name] = tensor.dtype\n      var_values[name] += tensor\n    logging.info(""Read from checkpoint %s"", checkpoint)\n  for name in var_values:  # Average.\n    var_values[name] /= len(checkpoints)\n\n  with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n    tf_vars = [\n        tf.get_variable(v, shape=var_values[v].shape, dtype=var_dtypes[v])\n        for v in var_values\n    ]\n  placeholders = [tf.placeholder(v.dtype, shape=v.shape) for v in tf_vars]\n  assign_ops = [tf.assign(v, p) for (v, p) in zip(tf_vars, placeholders)]\n  global_step = tf.Variable(\n      0, name=""global_step"", trainable=False, dtype=tf.int64)\n  saver = tf.train.Saver(tf.all_variables())\n\n  # Build a model consisting only of variables, set them to the average values.\n  with tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for p, assign_op, (name, value) in zip(placeholders, assign_ops,\n                                           six.iteritems(var_values)):\n      sess.run(assign_op, {p: value})\n    # Use the built saver to save the averaged checkpoint.\n    saver.save(sess, FLAGS.output_path, global_step=global_step)\n\n  logging.info(""Averaged checkpoints saved in %s"", FLAGS.output_path)\n\n\nif __name__ == ""__main__"":\n  app.run(main)\n'"
utils/pb_pbtxt.py,6,"b'#!/usr/bin/env python3\n# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n# This file is useful for reading the contents of the ops generated by ruby.\n# You can read any graph defination in pb/pbtxt format generated by ruby\n# or by python and then convert it back and forth from human readable to binary format.\n\nfrom absl import flags\nfrom absl import app\nfrom absl import logging\nfrom pathlib import Path\n\nimport delta.compat as tf\nfrom google.protobuf import text_format\nfrom tensorflow.python.platform import gfile\n\ndump_dir = \'pbtxt/\'\n\n\ndef pbtxt_to_pb(filename):\n  assert filename.suffix == \'.pbtxt\'\n  with filename.open(\'r\') as f:\n    graph_def = tf.GraphDef()\n\n    file_content = f.read()\n    text_format.Merge(file_content, graph_def)\n\n    tf.import_graph_def(graph_def, name=\'\')\n    tf.train.write_graph(graph_def, dump_dir, \'graph.pb\', as_text=False)\n\n\ndef pb_to_pbtxt(filename):\n  assert filename.suffix == \'.pb\'\n  with filename.open(\'rb\') as f:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(f.read())\n\n    tf.import_graph_def(graph_def, name=\'\')\n    tf.train.write_graph(graph_def, dump_dir, \'graph.pbtxt\', as_text=True)\n  return\n\n\ndef main(_):\n  FLAGS = flags.FLAGS\n  assert FLAGS.graph\n  graph_file = Path(FLAGS.graph)\n\n  if FLAGS.binary_in:\n    pb_to_pbtxt(graph_file)\n  else:\n    pbtxt_to_pb(graph_file)\n  logging.info(f""dump graph to {dump_dir}"")\n\n\nif __name__ == \'__main__\':\n  # flags usage: https://abseil.io/docs/python/guides/flags\n  logging.set_verbosity(logging.INFO)\n  flags.DEFINE_string(\n      \'graph\', default=None, help=\'graph.pb file name\', short_name=\'g\')\n  flags.DEFINE_bool(\n      \'binary_in\',\n      default=True,\n      help=\'input graph is binary or not\',\n      short_name=\'b\')\n  flags.mark_flag_as_required(\'graph\')\n\n  app.run(main)\n'"
utils/replace_custom_op_attr_pbtxt.py,0,"b'#!/usr/bin/env python3\n# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport argparse\nimport os\nfrom absl import logging\nimport delta.compat as tf\nfrom tensorflow.python.platform import gfile\n\n\ndef edit_pb_txt(old_args, export_dir):\n  """"""\n  Edit file path argument in pbtxt file.\n  :param old_args: Old file paths need to be copied and edited.\n  :param export_dir: Directory of the saved model.\n  """"""\n  assets_extra_dir = os.path.join(export_dir, ""./assets.extra"")\n  if not os.path.exists(assets_extra_dir):\n    os.makedirs(assets_extra_dir)\n\n  new_args = []\n  for one_old in old_args:\n    if not os.path.exists(one_old):\n      raise ValueError(""{} do not exists!"".format(one_old))\n    one_new = os.path.join(assets_extra_dir, os.path.basename(one_old))\n    new_args.append(one_new)\n    logging.info(""Copy file: {} to: {}"".format(one_old, one_new))\n    gfile.Copy(one_old, one_new, overwrite=True)\n\n  pbtxt_file = os.path.join(export_dir, ""saved_model.pbtxt"")\n  tmp_file = pbtxt_file + "".tmp""\n  logging.info(""Editing pbtxt file: {}"".format(pbtxt_file))\n  with open(pbtxt_file, ""rt"") as fin, open(tmp_file, ""wt"") as fout:\n    for line in fin:\n      for one_old, one_new in zip(old_args, new_args):\n        line = line.replace(one_old, one_new)\n      fout.write(line)\n  gfile.Copy(tmp_file, pbtxt_file, overwrite=True)\n  gfile.Remove(tmp_file)\n\n\nif ""__main__"" in __name__:\n  ap = argparse.ArgumentParser(\n      description=""Edit file path argument in pbtxt file."")\n  ap.add_argument(\'--old_args\', type=str, help=""Old arguments, split by comma."")\n  ap.add_argument(\n      \'--export_dir\', type=str, help=""Directory of the exported saved model."")\n  args = ap.parse_args()\n  logging.set_verbosity(logging.INFO)\n  old_args = args.old_args.split("","")\n  export_dir = args.export_dir\n  edit_pb_txt(old_args, export_dir)\n'"
utils/replace_custom_op_attr_pbtxt_test.py,5,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\nimport delta.compat as tf\nfrom absl import logging\nfrom pathlib import Path\n\nfrom delta import utils\nfrom delta.layers.ops import py_x_ops\nfrom delta.utils.register import registers\nfrom utils.edit_pbtxt import edit_pb_txt\n\n\nclass EditPbtxtTest(tf.test.TestCase):\n\n  def setUp(self):\n    super().setUp()\n    main_root = os.environ[\'MAIN_ROOT\']\n    self.main_root = Path(main_root)\n    config_file = self.main_root.joinpath(\n        \'delta/config/han-cls-keras/han-cls.yml\')\n    self.config = utils.load_config(config_file)\n    solver_name = self.config[\'solver\'][\'name\']\n    self.solver = registers.solver[solver_name](self.config)\n\n  def test_export_model(self):\n\n    export_path_base = self.config[""solver""][""service""][""model_path""]\n    model_version = self.config[""solver""][""service""][""model_version""]\n    export_path = os.path.join(\n        tf.compat.as_bytes(export_path_base), tf.compat.as_bytes(model_version))\n    export_path = os.path.abspath(export_path)\n\n    if not os.path.exists(export_path):\n      self.solver.export_model()\n\n    old_paths = [\n        ""tools/cppjieba/dict/jieba.dict.utf8"",\n        ""tools/cppjieba/dict/hmm_model.utf8"", ""tools/cppjieba/dict/idf.utf8"",\n        ""tools/cppjieba/dict/stop_words.utf8"",\n        ""tools/cppjieba/dict/user.dict.utf8""\n    ]\n    old_args = [str(self.main_root.joinpath(p)) for p in old_paths]\n\n    edit_pb_txt(old_args, export_path.decode(""utf-8""))\n\n    graph = tf.Graph()\n    with self.session(graph) as sess:\n      tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING],\n                                 export_path)\n\n      input_sentence_tensor = graph.get_operation_by_name(\n          ""input_sentence"").outputs[0]\n\n      score_tensor = graph.get_operation_by_name(""score"").outputs[0]\n\n      score = sess.run(\n          score_tensor, feed_dict={input_sentence_tensor: [""\xe4\xbd\xa0\xe5\xa5\xbd\xe5\x91\x80\xe5\x8c\x97\xe4\xba\xac""]})\n      logging.info(""score: {}"".format(score))\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  tf.test.main()\n'"
utils/run_saved_model.py,0,"b'#!/usr/bin/env python3\n# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Saved & Frozen & Checkpoint model Evaluater\'\'\'\nimport os\nfrom absl import logging\nfrom absl import flags\nfrom absl import app\n\nfrom delta import utils\nfrom delta.utils.register import registers\nfrom delta.utils.register import import_all_modules_for_register\n\n\ndef main(_):\n  \'\'\' main func \'\'\'\n  FLAGS = app.flags.FLAGS  #pylint: disable=invalid-name\n  logging.info(""config: {}"".format(FLAGS.config))\n  logging.info(""mode: {}"".format(FLAGS.mode))\n  logging.info(""gpu_visible: {}"".format(FLAGS.gpu))\n  assert FLAGS.config, \'pls give a config.yaml\'\n  assert FLAGS.mode, \'pls give mode [eval|infer|eval_and_infer]\'\n  os.environ[""CUDA_VISIBLE_DEVICES""] = FLAGS.gpu  #selects a specific device\n\n  #create dataset\n  mode = utils.INFER if FLAGS.mode == \'infer\' else utils.EVAL\n\n  # load config\n  config = utils.load_config(FLAGS.config)\n\n  # process config\n  import_all_modules_for_register()\n  solver_name = config[\'solver\'][\'name\']\n  logging.info(f""sovler: {solver_name}"")\n  solver = registers.solver[solver_name](config)\n  config = solver.config\n\n  # Evaluate\n  evaluate_name = config[\'serving\'][\'name\']\n  logging.info(f""evaluate: {evaluate_name}"")\n  evaluate = registers.serving[evaluate_name](\n      config, gpu_str=FLAGS.gpu, mode=mode)\n\n  if FLAGS.debug:\n    evaluate.debug()\n  evaluate.predict()\n\n\ndef define_flags():\n  \'\'\' define flags for evaluator\'\'\'\n  # The GPU devices which are visible for current process\n  flags.DEFINE_string(\'gpu\', \'\', \'same to CUDA_VISIBLE_DEVICES\')\n  flags.DEFINE_string(\'config\', None, help=\'path to yaml config file\')\n  flags.DEFINE_enum(\'mode\', \'eval\', [\'eval\', \'infer\', \'eval_and_infer\'],\n                    \'eval or infer\')\n  flags.DEFINE_bool(\'debug\', False, \'debug mode\')\n  # https://github.com/abseil/abseil-py/blob/master/absl/flags/_validators.py#L330\n  flags.mark_flags_as_required([\'config\', \'mode\'])\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  define_flags()\n  app.run(main)\n  logging.info(""OK. Done!"")\n'"
utils/subset_data_dir_tr_cv.py,0,"b'#!/usr/bin/env python3\n# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n"""""" Split Kaldi data directory into traininng and validation sets. """"""\nimport argparse\nfrom absl import logging\n\nfrom delta.utils.kaldi import kaldi_dir\nfrom delta.utils.kaldi import kaldi_dir_utils\n\n\ndef main():\n  \'\'\' The main function. \'\'\'\n  logging.set_verbosity(logging.INFO)\n\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\'--num-spk-cv\', type=float, default=0)\n  parser.add_argument(\'--num-utt-cv\', type=float, default=0)\n  parser.add_argument(\'--cv-spk-percent\', type=float, default=0.0)\n  parser.add_argument(\'--cv-utt-percent\', type=float, default=0.0)\n  parser.add_argument(\'--fair-choice\', type=bool, default=True)\n  parser.add_argument(\'data_dir\')\n  parser.add_argument(\'data_dir_tr\')\n  parser.add_argument(\'data_dir_cv\')\n\n  args = parser.parse_args()\n\n  num_spk_cv = args.num_spk_cv\n  num_utt_cv = args.num_utt_cv\n  if args.cv_spk_percent > 0:\n    if args.cv_spk_percent >= 100:\n      raise ValueError(\'cv_spk_percent cannot >= 100\')\n    num_spk_cv = args.cv_spk_percent / 100\n  if args.cv_utt_percent > 0:\n    if args.cv_utt_percent >= 100:\n      raise ValueError(\'cv_utt_percent cannot >= 100\')\n    num_utt_cv = args.cv_utt_percent / 100\n  if num_spk_cv == 0 and num_utt_cv == 0:\n    num_spk_cv = 0.1\n\n  meta = kaldi_dir.KaldiMetaData()\n  meta.load(args.data_dir)\n  meta_tr, meta_cv = kaldi_dir_utils.subset_data_dir_tr_cv(\n      meta,\n      num_spk_cv=num_spk_cv,\n      num_utt_cv=num_utt_cv,\n      fair_choice=args.fair_choice)\n  logging.info(\'#spks tr: %d, cv: %d; #utts tr: %d, cv: %d\' % (len(\n      meta_tr.spks), len(meta_cv.spks), len(meta_tr.utts), len(meta_cv.utts)))\n\n  meta_tr.dump(args.data_dir_tr, overwrite=True)\n  meta_cv.dump(args.data_dir_cv, overwrite=True)\n\n\nif __name__ == \'__main__\':\n  main()\n'"
core/ops/__init__.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport os\n\nPACKAGE_OPS_DIR = os.path.dirname(os.path.abspath(__file__))\n'"
core/ops/gen_build.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Generate BUILD file for bazel build.""""""\n\nimport os\n\ncppjieba = [\n    \'cppjieba/include/cppjieba/Jieba.hpp\',\n    \'cppjieba/include/cppjieba/QuerySegment.hpp\',\n    \'cppjieba/deps/limonp/Logging.hpp\',\n    \'cppjieba/include/cppjieba/DictTrie.hpp\',\n    \'cppjieba/deps/limonp/StringUtil.hpp\',\n    \'cppjieba/deps/limonp/StdExtension.hpp\',\n    \'cppjieba/include/cppjieba/Unicode.hpp\',\n    \'cppjieba/deps/limonp/LocalVector.hpp\',\n    \'cppjieba/include/cppjieba/Trie.hpp\',\n    \'cppjieba/include/cppjieba/SegmentBase.hpp\',\n    \'cppjieba/include/cppjieba/PreFilter.hpp\',\n    \'cppjieba/include/cppjieba/FullSegment.hpp\',\n    \'cppjieba/include/cppjieba/MixSegment.hpp\',\n    \'cppjieba/include/cppjieba/MPSegment.hpp\',\n    \'cppjieba/include/cppjieba/SegmentTagged.hpp\',\n    \'cppjieba/include/cppjieba/PosTagger.hpp\',\n    \'cppjieba/include/cppjieba/HMMSegment.hpp\',\n    \'cppjieba/include/cppjieba/HMMModel.hpp\',\n    \'cppjieba/include/cppjieba/KeywordExtractor.hpp\',\n]\ncopts = [\n    ""-Itensorflow/core/user_ops/ops"",\n    ""-Itensorflow/core/user_ops/ops/cppjieba/include"",\n    ""-Itensorflow/core/user_ops/ops/cppjieba/deps""\n]\n\nsrc = [\n    os.path.join(""kernels"", one_path)\n    for one_path in os.listdir(""kernels"")\n    if one_path.endswith("".cc"")\n]\nsrc += [\n    os.path.join(""kernels"", one_path)\n    for one_path in os.listdir(""kernels"")\n    if one_path.endswith("".h"")\n]\n\nsrc += [\n    os.path.join(""kernels/add_rir_noise_aecres"", one_path)\n    for one_path in os.listdir(""kernels/add_rir_noise_aecres"")\n    if one_path.endswith("".cpp"")\n]\n\nsrc += [\n    os.path.join(""kernels/add_rir_noise_aecres"", one_path)\n    for one_path in os.listdir(""kernels/add_rir_noise_aecres"")\n    if one_path.endswith("".h"")\n]\n\nsrc += cppjieba\n\nfirst_line = \'load(""//tensorflow:tensorflow.bzl"",  ""tf_custom_op_library"")\'\nsecond_line = \'tf_custom_op_library(name = ""x_ops.so"", \\nsrcs = [""{}""], \\ncopts = [""{}""])\'.format(\n    \'"",\\n""\'.join(src), \'"",\\n""\'.join(copts))\n\nprint(first_line)\nprint(second_line)\n\nwith open(""BUILD"", ""w"") as f:\n  f.write(first_line + ""\\n"")\n  f.write(second_line + ""\\n"")\n\nprint(""BUILD generated successfully!"")\n'"
core/ops/py_x_ops.py,4,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' python custom ops \'\'\'\n\nimport os\nimport delta.compat as tf\nfrom absl import logging\nfrom delta import PACKAGE_ROOT_DIR\nfrom delta.data.utils import read_lines_from_text_file\n\n#pylint: disable=invalid-name\nfile_dir = tf.resource_loader.get_data_files_path()\ntry:\n  so_lib_file = tf.io.gfile.glob(file_dir + \'/x_ops*.so\')[0].split(\'/\')[-1]\nexcept IndexError as e:\n  raise FileNotFoundError(f""No x_ops*.so match under dir: {file_dir}"")\npath = tf.resource_loader.get_path_to_datafile(so_lib_file)\n\nlogging.info(\'x_ops.so path:{}\'.format(path))\n\ngen_x_ops = tf.load_op_library(path)\n\npitch = gen_x_ops.pitch\nframe_pow = gen_x_ops.frame_pow\nzcr = gen_x_ops.zcr\nspectrum = gen_x_ops.spectrum\ncepstrum = gen_x_ops.cepstrum\nplp = gen_x_ops.plp\nanalyfiltbank = gen_x_ops.analyfiltbank\nsynthfiltbank = gen_x_ops.synthfiltbank\nfbank = gen_x_ops.fbank\nngram = gen_x_ops.ngram\nvocab_token_to_id = gen_x_ops.vocab_token_to_id\nvocab_id_to_token = gen_x_ops.vocab_id_to_token\ntoken_in_vocab = gen_x_ops.token_in_vocab\nstr_lower = gen_x_ops.str_lower\nsentence_to_ids = gen_x_ops.sentence_to_ids\ndelta_delta = gen_x_ops.delta_delta\nmfcc = gen_x_ops.mfcc_dct\nadd_rir_noise_aecres = gen_x_ops.add_rir_noise_aecres\nspeed = gen_x_ops.speed\n\n\ndef jieba_cut(input_sentence, use_file=True, hmm=True):\n\n  dict_path = os.path.join(PACKAGE_ROOT_DIR,\n                           ""./resources/cppjieba_dict/jieba.dict.utf8"")\n  hmm_path = os.path.join(PACKAGE_ROOT_DIR,\n                          ""./resources/cppjieba_dict/hmm_model.utf8"")\n  user_dict_path = os.path.join(PACKAGE_ROOT_DIR,\n                                ""./resources/cppjieba_dict/user.dict.utf8"")\n  idf_path = os.path.join(PACKAGE_ROOT_DIR,\n                          ""./resources/cppjieba_dict/idf.utf8"")\n  stop_word_path = os.path.join(PACKAGE_ROOT_DIR,\n                                ""./resources/cppjieba_dict/stop_words.utf8"")\n\n  if use_file:\n    output_sentence = gen_x_ops.jieba_cut(\n        input_sentence,\n        use_file=use_file,\n        hmm=hmm,\n        dict_path=dict_path,\n        hmm_path=hmm_path,\n        user_dict_path=user_dict_path,\n        idf_path=idf_path,\n        stop_word_path=stop_word_path)\n  else:\n    dict_lines = read_lines_from_text_file(dict_path)\n    model_lines = read_lines_from_text_file(hmm_path)\n    user_dict_lines = read_lines_from_text_file(user_dict_path)\n    idf_lines = read_lines_from_text_file(idf_path)\n    stop_word_lines = read_lines_from_text_file(stop_word_path)\n\n    output_sentence = gen_x_ops.jieba_cut(\n        input_sentence,\n        use_file=use_file,\n        hmm=hmm,\n        dict_lines=dict_lines,\n        model_lines=model_lines,\n        user_dict_lines=user_dict_lines,\n        idf_lines=idf_lines,\n        stop_word_lines=stop_word_lines)\n\n  return output_sentence\n'"
delta/data/__init__.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' init of data package\'\'\'\n'"
delta/layers/__init__.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Custom layers.""""""\n\nfrom delta.layers.attention import HanAttention\nfrom delta.layers.recurrent import RnnAttentionEncoder\nfrom delta.layers.attention import MatchAttention\nfrom delta.layers.recurrent import RnnEncoder\nfrom delta.layers.recurrent import RnnDecoder\nfrom delta.layers.sub_tf import MultiHeadAttention\nfrom delta.layers.sub_tf import PositionEmbedding\nfrom delta.layers.sub_tf import PositionwiseFeedForward\nfrom delta.layers.transformer import TransformerEncoder\nfrom delta.layers.transformer import TransformerDecoder\n\nfrom delta.layers.common_layers import *\n'"
delta/layers/attention.py,35,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Attention layers.""""""\nfrom absl import logging\nimport delta.compat as tf\n\nfrom delta.layers.base_layer import Layer\n\n#pylint: disable=invalid-name, too-many-instance-attributes, too-many-arguments\n\n\ndef masked_softmax(logits, mask, axis):\n  """"""Compute softmax with input mask.""""""\n  e_logits = tf.exp(logits)\n  masked_e = tf.multiply(e_logits, mask)\n  sum_masked_e = tf.reduce_sum(masked_e, axis, keep_dims=True)\n  ones = tf.ones_like(sum_masked_e)\n  # pay attention to a situation that if len of mask is zero,\n  # denominator should be set to 1\n  sum_masked_e_safe = tf.where(tf.equal(sum_masked_e, 0), ones, sum_masked_e)\n  return masked_e / sum_masked_e_safe\n\n\nclass HanAttention(Layer):\n  """"""\n  Refer to [Hierarchical Attention Networks for Document Classification]\n    (https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf)\n    wrap `with tf.variable_scope(name, reuse=tf.AUTO_REUSE):`\n  Input shape: (Batch size, steps, features)\n  Output shape: (Batch size, features)\n  """"""\n\n  def __init__(self,\n               W_regularizer=None,\n               u_regularizer=None,\n               b_regularizer=None,\n               W_constraint=None,\n               u_constraint=None,\n               b_constraint=None,\n               use_bias=True,\n               **kwargs):\n\n    super().__init__(**kwargs)\n    self.supports_masking = True\n    self.init = tf.keras.initializers.get(\'glorot_uniform\')\n\n    self.W_regularizer = tf.keras.regularizers.get(W_regularizer)\n    self.u_regularizer = tf.keras.regularizers.get(u_regularizer)\n    self.b_regularizer = tf.keras.regularizers.get(b_regularizer)\n\n    self.W_constraint = tf.keras.constraints.get(W_constraint)\n    self.u_constraint = tf.keras.constraints.get(u_constraint)\n    self.b_constraint = tf.keras.constraints.get(b_constraint)\n\n    self.use_bias = use_bias\n\n  def build(self, input_shape):\n    # pylint: disable=attribute-defined-outside-init\n    assert len(input_shape) == 3\n\n    self.W = self.add_weight(\n        name=\'{}_W\'.format(self.name),\n        shape=(\n            int(input_shape[-1]),\n            int(input_shape[-1]),\n        ),\n        initializer=self.init,\n        regularizer=self.W_regularizer,\n        constraint=self.W_constraint)\n\n    if self.use_bias:\n      self.b = self.add_weight(\n          name=\'{}_b\'.format(self.name),\n          shape=(int(input_shape[-1]),),\n          initializer=\'zero\',\n          regularizer=self.b_regularizer,\n          constraint=self.b_constraint)\n\n    self.attention_context_vector = self.add_weight(\n        name=\'{}_att_context_v\'.format(self.name),\n        shape=(int(input_shape[-1]),),\n        initializer=self.init,\n        regularizer=self.u_regularizer,\n        constraint=self.u_constraint)\n    self.built = True\n\n  # pylint: disable=missing-docstring, no-self-use\n  def compute_mask(self, inputs, mask=None):  # pylint: disable=unused-argument\n    # do not pass the mask to the next layers\n    return None\n\n  def call(self, inputs, training=None, mask=None):\n    batch_size = tf.shape(inputs)[0]\n    W_3d = tf.tile(tf.expand_dims(self.W, axis=0), tf.stack([batch_size, 1, 1]))\n    # [batch_size, steps, features]\n    input_projection = tf.matmul(inputs, W_3d)\n\n    if self.use_bias:\n      input_projection += self.b\n\n    input_projection = tf.tanh(input_projection)\n\n    # [batch_size, steps, 1]\n    similaritys = tf.reduce_sum(\n        tf.multiply(input_projection, self.attention_context_vector),\n        axis=2,\n        keep_dims=True)\n\n    # [batch_size, steps, 1]\n    if mask is not None:\n      attention_weights = masked_softmax(similaritys, mask, axis=1)\n    else:\n      attention_weights = tf.nn.softmax(similaritys, axis=1)\n\n    # [batch_size, features]\n    attention_output = tf.reduce_sum(\n        tf.multiply(inputs, attention_weights), axis=1)\n    return attention_output\n\n  # pylint: disable=no-self-use\n  def compute_output_shape(self, input_shape):\n    """"""compute output shape""""""\n    return input_shape[0], input_shape[-1]\n\n\nclass MatchAttention(Layer):\n  """"""\n  Refer to [Learning Natural Language Inference with LSTM]\n    (https://www.aclweb.org/anthology/N16-1170)\n    wrap `with tf.variable_scope(name, reuse=tf.AUTO_REUSE):`\n  Input shape: (Batch size, steps, features)\n  Output shape: (Batch size, steps, features)\n  """"""\n\n  def __init__(self, config, **kwargs):\n    super().__init__(**kwargs)\n    logging.info(""Initialize MatchAttention {}..."".format(self.name))\n    self.fc_num_units = config[\'model\'][\'net\'][\'structure\'][\'fc_num_units\']\n    self.middle_layer = tf.keras.layers.Dense(\n        self.fc_num_units, activation=\'tanh\')\n    self.attn = tf.keras.layers.Dense(1)\n\n  # pylint: disable=arguments-differ\n  def call(self, tensors):\n    """"""Attention layer.""""""\n    left, right = tensors\n\n    len_left = left.shape[1]\n    len_right = right.shape[1]\n    tensor_left = tf.expand_dims(left, axis=2)\n    tensor_right = tf.expand_dims(right, axis=1)\n    tensor_left = tf.tile(tensor_left, [1, 1, len_right, 1])\n    tensor_right = tf.tile(tensor_right, [1, len_left, 1, 1])\n    tensor_merged = tf.concat([tensor_left, tensor_right], axis=-1)\n    middle_output = self.middle_layer(tensor_merged)\n    attn_scores = self.attn(middle_output)\n    attn_scores = tf.squeeze(attn_scores, axis=3)\n    exp_attn_scores = tf.exp(attn_scores -\n                             tf.reduce_max(attn_scores, axis=-1, keepdims=True))\n    exp_sum = tf.reduce_sum(exp_attn_scores, axis=-1, keepdims=True)\n    attention_weights = exp_attn_scores / exp_sum\n    return tf.matmul(attention_weights, right)\n'"
delta/layers/base_layer.py,1,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Base class for layer.""""""\n\nimport delta.compat as tf\n\n\nclass Layer(tf.keras.layers.Layer):\n  """"""Base class for layer.""""""\n\n  def __init__(self, **kwargs):\n    super().__init__(**kwargs)\n\n  def build(self, input_shape):\n    """"""Creates the variables of the layer.""""""\n    #pylint: disable=useless-super-delegation\n    super().build(input_shape)\n\n  def call(self, inputs, training=None, mask=None):\n    """"""This is where the layer\'s logic lives.""""""\n    # pylint: disable=arguments-differ\n    raise NotImplementedError()\n'"
delta/layers/common_layers.py,73,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Common layers.""""""\n\nimport delta.compat as tf\nfrom absl import logging\n\nfrom delta.data.feat import speech_ops\n\n#pylint: disable=invalid-name\n\n\ndef splice_layer(x, name, context):\n  \'\'\'\n  Splice a tensor along the last dimension with context.\n  e.g.:\n  t = [[[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]]]\n  splice_tensor(t, [0, 1]) =\n      [[[1, 2, 3, 4, 5, 6],\n        [4, 5, 6, 7, 8, 9],\n        [7, 8, 9, 7, 8, 9]]]\n\n  Args:\n    tensor: a tf.Tensor with shape (B, T, D) a.k.a. (N, H, W)\n    context: a list of context offsets\n\n  Returns:\n    spliced tensor with shape (..., D * len(context))\n  \'\'\'\n  with tf.variable_scope(name):\n    input_shape = tf.shape(x)\n    B, T = input_shape[0], input_shape[1]\n    context_len = len(context)\n    array = tf.TensorArray(x.dtype, size=context_len)\n    for idx, offset in enumerate(context):\n      begin = offset\n      end = T + offset\n      if begin < 0:\n        begin = 0\n        sliced = x[:, begin:end, :]\n        tiled = tf.tile(x[:, 0:1, :], [1, abs(offset), 1])\n        final = tf.concat((tiled, sliced), axis=1)\n      else:\n        end = T\n        sliced = x[:, begin:end, :]\n        tiled = tf.tile(x[:, -1:, :], [1, abs(offset), 1])\n        final = tf.concat((sliced, tiled), axis=1)\n      array = array.write(idx, final)\n    spliced = array.stack()\n    spliced = tf.transpose(spliced, (1, 2, 0, 3))\n    spliced = tf.reshape(spliced, (B, T, -1))\n  return spliced\n\n\n#pylint: disable=too-many-arguments\ndef tdnn(x,\n         name,\n         in_dim,\n         context,\n         out_dim,\n         has_bias=True,\n         method=\'splice_layer\'):\n  \'\'\'\n  TDNN implementation.\n\n  Args:\n    context:\n      a int of left and right context, or\n      a list of context indexes, e.g. (-2, 0, 2).\n    method:\n      splice_layer: use column-first patch-based copy.\n      splice_op: use row-first while_loop copy.\n      conv1d: use conv1d as TDNN equivalence.\n  \'\'\'\n  if hasattr(context, \'__iter__\'):\n    context_size = len(context)\n    if method in (\'splice_op\', \'conv1d\'):\n      msg = \'Method splice_op and conv1d does not support context list.\'\n      raise ValueError(msg)\n    context_list = context\n  else:\n    context_size = context * 2 + 1\n    context_list = range(-context, context + 1)\n  with tf.variable_scope(name):\n    if method == \'splice_layer\':\n      x = splice_layer(x, \'splice\', context_list)\n      x = linear(\n          x, \'linear\', [in_dim * context_size, out_dim], has_bias=has_bias)\n    elif method == \'splice_op\':\n      x = speech_ops.splice(x, context, context)\n      x = linear(\n          x, \'linear\', [in_dim * context_size, out_dim], has_bias=has_bias)\n    elif method == \'conv1d\':\n      kernel = tf.get_variable(\n          name=\'DW\',\n          shape=[context, in_dim, out_dim],\n          dtype=tf.float32,\n          initializer=tf.glorot_uniform_initializer())\n      x = tf.nn.conv1d(x, kernel, stride=1, padding=\'SAME\')\n      if has_bias:\n        b = tf.get_variable(\n            name=\'bias\',\n            shape=[out_dim],\n            dtype=tf.float32,\n            initializer=tf.constant_initializer(0.0))\n        x = tf.nn.bias_add(x, b)\n    else:\n      raise ValueError(\'Unsupported method: %s.\' % (method))\n    return x\n\n\ndef conv2d(x, name, filter_size, in_channels, out_channels, strides, bias=True):\n  """"""2D convolution.""""""\n  with tf.variable_scope(name):\n    kernel = tf.get_variable(\n        name=\'DW\',\n        shape=[filter_size[0], filter_size[1], in_channels, out_channels],\n        dtype=tf.float32,\n        initializer=tf.initializers.glorot_uniform())\n    if bias:\n      b = tf.get_variable(\n          name=\'bias\',\n          shape=[out_channels],\n          dtype=tf.float32,\n          initializer=tf.constant_initializer(0.0))\n    out = tf.nn.conv2d(\n        x, kernel, [1, strides[0], strides[1], 1], padding=\'SAME\')\n    if bias:\n      out = tf.nn.bias_add(out, b)\n    return out\n\n\ndef max_pool(x, ksize, strides):\n  """"""Max Pooling.""""""\n  return tf.nn.max_pool(\n      x,\n      ksize=[1, ksize[0], ksize[1], 1],\n      strides=[1, strides[0], strides[1], 1],\n      padding=\'VALID\',\n      name=\'max_pool\')\n\n\ndef linear(x, names, shapes, has_bias=True):\n  """"""Linear Layer.""""""\n  assert len(shapes) == 2\n  with tf.variable_scope(names):\n    weights = tf.get_variable(\n        name=\'weights\',\n        shape=shapes,\n        initializer=tf.initializers.glorot_uniform())\n    if has_bias:\n      bias = tf.get_variable(\n          name=\'bias\',\n          shape=shapes[1],\n          initializer=tf.initializers.glorot_uniform())\n      return tf.matmul(x, weights) + bias\n    else:\n      return tf.matmul(x, weights)\n\n\ndef attention(inputs, attention_size, time_major=False, return_alphas=False):\n  """"""Attention layer.""""""\n  if isinstance(inputs, tuple):\n    # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n    inputs = tf.concat(inputs, 2)\n\n  if time_major:\n    # (T,B,D) => (B,T,D)\n    inputs = tf.transpose(inputs, [1, 0, 2])\n\n  time_size = inputs.shape[1].value  # T value - time size of the RNN layer\n  hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n\n  # Trainable parameters\n  W_omega = tf.get_variable(\n      name=\'W_omega\',\n      initializer=tf.random_normal([hidden_size, attention_size], stddev=0.1))\n  b_omega = tf.get_variable(\n      name=\'b_omega\',\n      initializer=tf.random_normal([attention_size], stddev=0.1))\n  u_omega = tf.get_variable(\n      name=\'u_omega\',\n      initializer=tf.random_normal([attention_size, 1], stddev=0.1))\n\n  # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n  #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n  #v = tf.tanh(tf.tensordot(inputs, W_omega, axes=1) + b_omega)\n  #v = tf.sigmoid(tf.tensordot(inputs, W_omega, axes=1) + b_omega)\n  # (B, T, D) dot (D, Atten)\n\n  logging.info(\'attention inputs: {}\'.format(inputs.shape))\n  inputs_reshaped = tf.reshape(inputs, [-1, hidden_size])\n  dot = tf.matmul(inputs_reshaped, W_omega)\n  dot = tf.reshape(dot, [-1, time_size, attention_size])\n  v = tf.sigmoid(dot + b_omega)\n  logging.info(f\'attention vector: {v.shape}\')\n  # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n  # (B, T, Atten) dot (Atten)\n  #vu = tf.tensordot(v, u_omega, axes=1)   # (B,T) shape\n  v = tf.reshape(v, [-1, attention_size])\n  vu = tf.matmul(v, u_omega)  # (B,T) shape\n  vu = tf.squeeze(vu, axis=-1)\n  vu = tf.reshape(vu, [-1, time_size])\n  logging.info(f\'attention energe: {vu.shape}\')\n  alphas = tf.nn.softmax(vu)  # (B,T) shape also\n\n  # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n  # [batch, time] -> [batch, time, 1]\n  alphas = tf.expand_dims(alphas, -1)\n  # [batch, time, dim] -> [batch, dim]\n  output = tf.reduce_sum(inputs * alphas, 1)\n\n  if not return_alphas:\n    return output\n\n  return output, alphas\n\n\ndef embedding_look_up(text_inputs, vocab_size, embedding_size):\n  """"""Embedding layer.""""""\n  with tf.variable_scope(""embedding""):\n    W = tf.get_variable(\n        name=\'W\',\n        initializer=tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))\n    embedding_chars = tf.nn.embedding_lookup(W, text_inputs)\n    embedding_chars_expanded = tf.expand_dims(embedding_chars, -1)\n  return embedding_chars_expanded\n\n\n#pylint: disable=too-many-locals\ndef conv_pool(embedded_chars_expanded, filter_sizes, embedding_size,\n              num_filters, sequence_length):\n  """"""\n    text conv and max pooling to get one-dimension vector to representation of text\n    :param filter_sizes:\n    :return:\n    """"""\n  pooled_outputs = []\n  for _, filter_size in enumerate(filter_sizes):\n    with tf.variable_scope(""conv-maxpool-%s"" % filter_size):\n      # Convolution Layer\n      filter_shape = [filter_size, embedding_size, 1, num_filters]\n      W = tf.get_variable(\n          name=\'W\', initializer=tf.truncated_normal(filter_shape, stddev=0.1))\n      b = tf.get_variable(\n          name=\'b\', initializer=tf.constant(0.1, shape=[num_filters]))\n      conv = tf.nn.conv2d(\n          embedded_chars_expanded,\n          W,\n          strides=[1, 1, 1, 1],\n          padding=""VALID"",\n          name=""conv"")\n      # Apply nonlinearity\n      h = tf.nn.relu(tf.nn.bias_add(conv, b), name=""relu"")\n      # Maxpooling over the outputs\n      pooled = tf.nn.max_pool(\n          h,\n          ksize=[1, sequence_length - filter_size + 1, 1, 1],\n          strides=[1, 1, 1, 1],\n          padding=\'VALID\',\n          name=""pool"")\n      pooled_outputs.append(pooled)\n  # Combine all the pooled features\n  num_filters_total = num_filters * len(filter_sizes)\n\n  h_pool = tf.concat(pooled_outputs, 3)\n\n  h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n  return h_pool_flat\n'"
delta/layers/common_layers_test.py,34,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Common layers test.""""""\n\nimport delta.compat as tf\nfrom absl import logging\n\nimport common_layers as cl\n\n\nclass LossUtilTest(tf.test.TestCase):\n  \'\'\' common layer unittest \'\'\'\n\n  def test_splice_layer(self):\n    \'\'\'test splice layer\'\'\'\n    inputs = tf.reshape(tf.range(15), shape=[1, 5, 3])\n    context = [0, 1]\n    output = cl.splice_layer(inputs, \'splice\', context)\n    output_true = tf.constant([[[0, 1, 2, 3, 4, 5], [3, 4, 5, 6, 7, 8],\n                                [6, 7, 8, 9, 10, 11], [9, 10, 11, 12, 13, 14],\n                                [12, 13, 14, 12, 13, 14]]])\n    self.assertAllEqual(output, output_true)\n\n    context = [-1, 0, 1]\n    output = cl.splice_layer(inputs, \'splice\', context)\n    output_true = tf.constant([[[0, 1, 2, 0, 1, 2, 3, 4, 5],\n                                [0, 1, 2, 3, 4, 5, 6, 7, 8],\n                                [3, 4, 5, 6, 7, 8, 9, 10, 11],\n                                [6, 7, 8, 9, 10, 11, 12, 13, 14],\n                                [9, 10, 11, 12, 13, 14, 12, 13, 14]]])\n    self.assertAllEqual(output, output_true)\n\n    context = [0, 1, 3]\n    output = cl.splice_layer(inputs, \'splice\', context)\n    output_true = tf.constant([[[0, 1, 2, 3, 4, 5, 9, 10, 11],\n                                [3, 4, 5, 6, 7, 8, 12, 13, 14],\n                                [6, 7, 8, 9, 10, 11, 12, 13, 14],\n                                [9, 10, 11, 12, 13, 14, 12, 13, 14],\n                                [12, 13, 14, 12, 13, 14, 12, 13, 14]]])\n    self.assertAllEqual(output, output_true)\n\n    context = [1, 3]\n    output = cl.splice_layer(inputs, \'splice\', context)\n    output_true = tf.constant([[[3, 4, 5, 9, 10, 11], [6, 7, 8, 12, 13, 14],\n                                [9, 10, 11, 12, 13, 14],\n                                [12, 13, 14, 12, 13, 14],\n                                [12, 13, 14, 12, 13, 14]]])\n    self.assertAllEqual(output, output_true)\n\n    context = [1, 2, 3]\n    output = cl.splice_layer(inputs, \'splice\', context)\n    output_true = tf.constant([[[3, 4, 5, 6, 7, 8, 9, 10, 11],\n                                [6, 7, 8, 9, 10, 11, 12, 13, 14],\n                                [9, 10, 11, 12, 13, 14, 12, 13, 14],\n                                [12, 13, 14, 12, 13, 14, 12, 13, 14],\n                                [12, 13, 14, 12, 13, 14, 12, 13, 14]]])\n    self.assertAllEqual(output, output_true)\n\n  def test_tdnn(self):\n    \'\'\'test tdnn\'\'\'\n    #A 3D Tensor [batch, in_width, in_channels]\n    inputs = tf.random_uniform(shape=[2, 5, 3], dtype=tf.float32, maxval=1.0)\n    in_dim = inputs.get_shape().as_list()[2]\n    out_dim = 4\n    context = [-2, -1, 0, 1, 2]\n    output = cl.tdnn(\n        inputs, \'test_tdnn0\', in_dim, context, out_dim, method=\'splice_layer\')\n    out_shape = [2, 5, 4]\n    self.assertAllEqual(tf.shape(output), out_shape)\n\n    context = 2\n    #output = cl.tdnn(inputs, \'test_tdnn1\', in_dim, context, out_dim, method=\'splice_op\')\n    #self.assertAllEqual(tf.shape(output), out_shape)\n\n    output = cl.tdnn(\n        inputs, \'test_tdnn2\', in_dim, context, out_dim, method=\'conv1d\')\n    self.assertAllEqual(tf.shape(output), out_shape)\n\n  def test_conv2d(self):\n    \'\'\'test conv2d\'\'\'\n    inputs = tf.random_uniform(\n        shape=[2, 5, 5, 3], dtype=tf.float32, maxval=1.0)  #A 4D Tensor\n    filter_size = [3, 3]\n    in_channels = inputs.get_shape().as_list()[3]\n    out_channels = 4\n    strides = [1, 1]\n    output = cl.conv2d(inputs, \'test_conv2d\', filter_size, in_channels,\n                       out_channels, strides)\n    output_shape = [2, 5, 5, 4]\n    self.assertAllEqual(tf.shape(output), output_shape)\n\n  def test_maxpool(self):\n    \'\'\'test maxpool\'\'\'\n    inputs = tf.reshape(tf.range(25), shape=[1, 5, 5, 1])  #A 4D tensor\n    ksize = [3, 3]\n    strides = [1, 1]\n    output = cl.max_pool(inputs, ksize, strides)\n    output_shape = [1, 3, 3, 1]\n    self.assertAllEqual(tf.shape(output), output_shape)\n\n    output_true = tf.constant([[[[12], [13], [14]], [[17], [18], [19]],\n                                [[22], [23], [24]]]])\n    self.assertAllEqual(output, output_true)\n\n  def test_linear(self):\n    \'\'\'test linear\'\'\'\n    inputs = tf.random_uniform(\n        shape=[4, 5], dtype=tf.float32, maxval=1.0)  # A 2D tensor\n    shape = [5, 4]\n    output = cl.linear(inputs, \'test_linear0\', shape)\n    output_shape = [4, 4]\n    self.assertAllEqual(tf.shape(output), output_shape)\n\n    inputs = tf.random_uniform(\n        shape=[2, 4, 5], dtype=tf.float32, maxval=1.0)  # A 3D tensor\n    shape = [5, 4]\n    output = cl.linear(inputs, \'test_linear1\', shape)\n    output_shape = [2, 4, 4]\n    self.assertAllEqual(tf.shape(output), output_shape)\n\n    # A 4D tensor [B, C, H, W]\n    inputs = tf.random_uniform(shape=[2, 3, 4, 5], dtype=tf.float32, maxval=1.0)\n    shape = [5, 4]\n    output = cl.linear(inputs, \'test_linear2\', shape)\n    output_shape = [2, 3, 4, 4]\n    self.assertAllEqual(tf.shape(output), output_shape)\n\n  def test_attention(self):\n    \'\'\'test attention\'\'\'\n    # A 3D tensor [B, T, D]\n    inputs = tf.random_uniform(\n        shape=[2, 100, 512], dtype=tf.float32, maxval=1.0)\n    attention_size = 256\n    output, alpha = cl.attention(inputs, attention_size, return_alphas=True)\n    output_shape = [2, 512]\n    alpha_shape = [2, 100, 1]\n    self.assertAllEqual(tf.shape(output), output_shape)\n    self.assertAllEqual(tf.shape(alpha), alpha_shape)\n\n  def test_embedding_look_up(self):\n    \'\'\'test embedding look up\'\'\'\n    text_inputs = [0, 1, 2]\n    vocab_size = 3\n    embedding_size = 512\n    output = cl.embedding_look_up(text_inputs, vocab_size, embedding_size)\n    output_shape = [3, 512, 1]\n    self.assertAllEqual(tf.shape(output), output_shape)\n\n  def test_conv_pool(self):\n    \'\'\'test  conv pool\'\'\'\n    # A 4D tensor [B, H, W, C]\n    embedded_chars_expanded = tf.random_uniform(\n        shape=[2, 7, 7, 1], dtype=tf.float32, maxval=1.0)\n    filter_sizes = [3, 5]\n    embedding_size = 3\n    num_filters = 3\n    sequence_length = 5\n    output = cl.conv_pool(embedded_chars_expanded, filter_sizes, embedding_size,\n                          num_filters, sequence_length)\n    output_shape = [30, 6]\n    self.assertAllEqual(tf.shape(output), output_shape)\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  tf.test.main()\n'"
delta/layers/dynamic_pooling.py,9,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""An implementation of Dynamic Pooling Layer.""""""\nimport typing\n\nimport delta.compat as tf\nfrom delta.layers.base_layer import Layer\n\n\nclass DynamicPoolingLayer(Layer):\n    """"""\n    Layer that computes dynamic pooling of one tensor.\n    :param psize1: pooling size of dimension 1\n    :param psize2: pooling size of dimension 2\n    :param kwargs: Standard layer keyword arguments.\n    Examples:\n        >>> import delta\n        >>> layer = delta.layers.DynamicPoolingLayer(3, 2)\n        >>> num_batch, left_len, right_len, num_dim = 5, 3, 2, 10\n        >>> layer.build([[num_batch, left_len, right_len, num_dim],\n        ...              [num_batch, left_len, right_len, 3]])\n    """"""\n\n    def __init__(self,\n                 psize1: int,\n                 psize2: int,\n                 **kwargs):\n        """""":class:`DynamicPoolingLayer` constructor.""""""\n        super().__init__(**kwargs)\n        self._psize1 = psize1\n        self._psize2 = psize2\n\n    def build(self, input_shape: typing.List[int]):\n        """"""\n        Build the layer.\n        :param input_shape: the shapes of the input tensors,\n            for DynamicPoolingLayer we need tow input tensors.\n        """"""\n        super().build(input_shape)\n        input_shape_one = input_shape[0]\n        self._msize1 = input_shape_one[1]\n        self._msize2 = input_shape_one[2]\n\n    def call(self, inputs: list, **kwargs) -> typing.Any:\n        """"""\n        The computation logic of DynamicPoolingLayer.\n        :param inputs: two input tensors.\n        """"""\n        self._validate_dpool_size()\n        x, dpool_index = inputs\n        dpool_shape = tf.shape(dpool_index)\n        batch_index_one = tf.expand_dims(\n            tf.expand_dims(\n                tf.range(dpool_shape[0]), axis=-1),\n            axis=-1)\n        batch_index = tf.expand_dims(\n            tf.tile(batch_index_one, [1, self._msize1, self._msize2]),\n            axis=-1)\n        dpool_index_ex = tf.concat([batch_index, dpool_index], axis=3)\n        x_expand = tf.gather_nd(x, dpool_index_ex)\n        stride1 = self._msize1 // self._psize1\n        stride2 = self._msize2 // self._psize2\n\n        x_pool = tf.nn.max_pool(x_expand,\n                                [1, stride1, stride2, 1],\n                                [1, stride1, stride2, 1],\n                                ""VALID"")\n        return x_pool\n\n    def compute_output_shape(self, input_shape: list) -> tuple:\n        """"""\n        Calculate the layer output shape.\n        :param input_shape: the shapes of the input tensors,\n            for DynamicPoolingLayer we need tow input tensors.\n        """"""\n        input_shape_one = input_shape[0]\n        return (None, self._psize1, self._psize2, input_shape_one[3])\n\n    def get_config(self) -> dict:\n        """"""Get the config dict of DynamicPoolingLayer.""""""\n        config = {\n            \'psize1\': self._psize1,\n            \'psize2\': self._psize2\n        }\n        base_config = super(DynamicPoolingLayer, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def _validate_dpool_size(self):\n        suggestion = self.get_size_suggestion(\n            self._msize1, self._msize2, self._psize1, self._psize2\n        )\n        if suggestion != (self._psize1, self._psize2):\n            raise ValueError(\n                ""DynamicPooling Layer can not ""\n                f""generate ({self._psize1} x {self._psize2}) output ""\n                f""feature map, please use ({suggestion[0]} x {suggestion[1]})""\n                f"" instead. `model.params[\'dpool_size\'] = {suggestion}` ""\n            )\n\n    @classmethod\n    def get_size_suggestion(\n        cls,\n        msize1: int,\n        msize2: int,\n        psize1: int,\n        psize2: int\n    ) -> typing.Tuple[int, int]:\n        """"""\n        Get `dpool_size` suggestion for a given shape.\n        Returns the nearest legal `dpool_size` for the given combination of\n        `(psize1, psize2)`.\n        :param msize1: size of the left text.\n        :param msize2: size of the right text.\n        :param psize1: base size of the pool.\n        :param psize2: base size of the pool.\n        :return:\n        """"""\n        stride1 = msize1 // psize1\n        stride2 = msize2 // psize2\n        suggestion1 = msize1 // stride1\n        suggestion2 = msize2 // stride2\n        return (suggestion1, suggestion2)\n\n'"
delta/layers/match_pyramid.py,6,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""An implementation of Matching Layer.""""""\nimport typing\n\nimport delta.compat as tf\nfrom delta.layers.base_layer import Layer\n\n\nclass MatchingLayer(Layer):\n    """"""\n    Layer that computes a matching matrix between samples in two tensors.\n    :param normalize: Whether to L2-normalize samples along the\n        dot product axis before taking the dot product.\n        If set to True, then the output of the dot product\n        is the cosine proximity between the two samples.\n    :param matching_type: the similarity function for matching\n    :param kwargs: Standard layer keyword arguments.\n    Examples:\n        >>> import delta\n        >>> layer = delta.layers.MatchingLayer(matching_type=\'dot\',\n        ...                                 normalize=True)\n        >>> num_batch, left_len, right_len, num_dim = 5, 3, 2, 10\n        >>> layer.build([[num_batch, left_len, num_dim],\n        ...              [num_batch, right_len, num_dim]])\n    """"""\n\n    def __init__(self, normalize: bool = False,\n                 matching_type: str = \'dot\', **kwargs):\n        """""":class:`MatchingLayer` constructor.""""""\n        super().__init__(**kwargs)\n        self._normalize = normalize\n        self._validate_matching_type(matching_type)\n        self._matching_type = matching_type\n        self._shape1 = None\n        self._shape2 = None\n\n    @classmethod\n    def _validate_matching_type(cls, matching_type: str = \'dot\'):\n        valid_matching_type = [\'dot\', \'mul\', \'plus\', \'minus\', \'concat\']\n        if matching_type not in valid_matching_type:\n            raise ValueError(f""{matching_type} is not a valid matching type, ""\n                             f""{valid_matching_type} expected."")\n\n    def build(self, input_shape: list):\n        """"""\n        Build the layer.\n        :param input_shape: the shapes of the input tensors,\n            for MatchingLayer we need tow input tensors.\n        """"""\n        # Used purely for shape validation.\n        if not isinstance(input_shape, list) or len(input_shape) != 2:\n            raise ValueError(\'A `MatchingLayer` layer should be called \'\n                             \'on a list of 2 inputs.\')\n        self._shape1 = input_shape[0]\n        self._shape2 = input_shape[1]\n        for idx in 0, 2:\n            if self._shape1[idx] != self._shape2[idx]:\n                raise ValueError(\n                    \'Incompatible dimensions: \'\n                    f\'{self._shape1[idx]} != {self._shape2[idx]}.\'\n                    f\'Layer shapes: {self._shape1}, {self._shape2}.\'\n                )\n\n    def call(self, inputs: list, **kwargs) -> typing.Any:\n        """"""\n        The computation logic of MatchingLayer.\n        :param inputs: two input tensors.\n        """"""\n        x1 = inputs[0]\n        x2 = inputs[1]\n        if self._matching_type == \'dot\':\n            if self._normalize:\n                x1 = tf.math.l2_normalize(x1, axis=2)\n                x2 = tf.math.l2_normalize(x2, axis=2)\n            return tf.expand_dims(tf.einsum(\'abd,acd->abc\', x1, x2), 3)\n        else:\n            if self._matching_type == \'mul\':\n                def func(x, y):\n                    return x * y\n            elif self._matching_type == \'plus\':\n                def func(x, y):\n                    return x + y\n            elif self._matching_type == \'minus\':\n                def func(x, y):\n                    return x - y\n            elif self._matching_type == \'concat\':\n                def func(x, y):\n                    return tf.concat([x, y], axis=3)\n            else:\n                raise ValueError(f""Invalid matching type.""\n                                 f""{self._matching_type} received.""\n                                 f""Mut be in `dot`, `mul`, `plus`, ""\n                                 f""`minus` and `concat`."")\n            x1_exp = tf.stack([x1] * self._shape2[1], 2)\n            x2_exp = tf.stack([x2] * self._shape1[1], 1)\n            return func(x1_exp, x2_exp)\n\n    def compute_output_shape(self, input_shape: list) -> tuple:\n        """"""\n        Calculate the layer output shape.\n        :param input_shape: the shapes of the input tensors,\n            for MatchingLayer we need tow input tensors.\n        """"""\n        if not isinstance(input_shape, list) or len(input_shape) != 2:\n            raise ValueError(\'A `MatchingLayer` layer should be called \'\n                             \'on a list of 2 inputs.\')\n        shape1 = list(input_shape[0])\n        shape2 = list(input_shape[1])\n        if len(shape1) != 3 or len(shape2) != 3:\n            raise ValueError(\'A `MatchingLayer` layer should be called \'\n                             \'on 2 inputs with 3 dimensions.\')\n        if shape1[0] != shape2[0] or shape1[2] != shape2[2]:\n            raise ValueError(\'A `MatchingLayer` layer should be called \'\n                             \'on 2 inputs with same 0,2 dimensions.\')\n\n        if self._matching_type in [\'mul\', \'plus\', \'minus\']:\n            return shape1[0], shape1[1], shape2[1], shape1[2]\n        elif self._matching_type == \'dot\':\n            return shape1[0], shape1[1], shape2[1], 1\n        elif self._matching_type == \'concat\':\n            return shape1[0], shape1[1], shape2[1], shape1[2] + shape2[2]\n        else:\n            raise ValueError(f""Invalid `matching_type`.""\n                             f""{self._matching_type} received.""\n                             f""Must be in `mul`, `plus`, `minus` ""\n                             f""`dot` and `concat`."")\n\n    def get_config(self) -> dict:\n        """"""Get the config dict of MatchingLayer.""""""\n        config = {\n            \'normalize\': self._normalize,\n            \'matching_type\': self._matching_type,\n        }\n        base_config = super(MatchingLayer, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n'"
delta/layers/recurrent.py,37,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Recurrent neural network layers.""""""\n\nfrom absl import logging\nimport delta.compat as tf\n\nimport delta\nfrom delta.layers.base_layer import Layer\nfrom tensorflow_addons import seq2seq\nSOS_ID = 4\nEOS_ID = 5\n\n\nclass BiRnn(Layer):\n  """"""\n  Bidirectional RNN\n  Input Shape: [batch_size, steps, features]\n  Output Shape: [batch_size, units]\n  """"""\n\n  def __init__(self, config, **kwargs):\n    super().__init__(**kwargs)\n    logging.info(""Initialize Rnn {}..."".format(self.name))\n\n    model_config = config[\'model\'][\'net\'][\'structure\']\n    self.cell_dim = model_config[\'cell_dim\']\n    self.cell_type = model_config[\'cell_type\']\n    if self.cell_type.lower() == \'gru\':\n      rnn_class = tf.keras.layers.GRU\n    elif self.cell_type.lower() == \'lstm\':\n      rnn_class = tf.keras.layers.LSTM\n    elif self.cell_type.lower() == \'cudnngru\':\n      rnn_class = tf.keras.layers.CuDNNGRU\n    elif self.cell_type.lower() == \'cudnnlstm\':\n      rnn_class = tf.keras.layers.CuDNNLSTM\n    else:\n      error_info = ""Cell type: {} not supported now! Please check!"".format(\n          self.cell_type)\n      logging.error(error_info)\n      raise ValueError(error_info)\n\n    self.bi_rnn = tf.keras.layers.Bidirectional(\n        rnn_class(self.cell_dim, return_sequences=True))\n    logging.info(""Initialize Rnn {} Done."".format(self.name))\n\n  def build(self, input_shape):\n    self.built = True\n\n  def compute_output_shape(self, input_shape):\n    return tf.TensorShape([input_shape[0], self.cell_dim * 2])\n\n  def compute_mask(self, inputs, mask=None):\n    return None\n\n  def call(self, inputs, training=None, mask=None):\n    out = self.bi_rnn(inputs)\n    return out\n\n\nclass RnnAttentionEncoder(Layer):  # pylint: disable=too-many-instance-attributes\n  """"""\n  RNN + Attention\n  Input Shape: [batch_size, steps, features]\n  Output Shape: [batch_size, units]\n  """"""\n\n  def __init__(self, config, **kwargs):\n    super().__init__(**kwargs)\n    logging.info(""Initialize RnnAttentionEncoder {}..."".format(self.name))\n\n    model_config = config[\'model\'][\'net\'][\'structure\']\n    self.dropout_rate = model_config[\'dropout_rate\']\n    self.cell_dim = model_config[\'cell_dim\']\n\n    self.sen_encoder = BiRnn(config)\n    # self.sen_all_dense = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(200))\n    self.sen_att = delta.layers.HanAttention(name=""{}_att"".format(self.name))\n    self.sen_att_d = tf.keras.layers.Dropout(self.dropout_rate)\n    # self.sen_att_d_bn = tf.keras.layers.BatchNormalization()\n    logging.info(""Initialize RnnAttentionEncoder {} Done."".format(self.name))\n\n  def build(self, input_shape):\n    self.built = True\n\n  def compute_output_shape(self, input_shape):\n    return tf.TensorShape([input_shape[0], self.cell_dim * 2])\n\n  def compute_mask(self, inputs, mask=None):\n    return None\n\n  def call(self, inputs, training=None, mask=None):\n    out = self.sen_encoder(inputs)\n    # out = self.sen_all_dense(out)\n    out = self.sen_att(out, mask=mask)\n    out = self.sen_att_d(out, training=training)\n    # out = self.sen_att_d_bn(out, training=training)\n    return out\n\n\nclass RnnEncoder(Layer):  # pylint: disable=too-many-instance-attributes\n  """"""\n  RNN + Attention\n  Input Shape: [batch_size, steps, features]\n  Output Shape: [batch_size, units]\n  """"""\n\n  def __init__(self, config, **kwargs):\n    super().__init__(**kwargs)\n    logging.info(""Initialize RnnEncoder {}..."".format(self.name))\n\n    model_config = config[\'model\'][\'net\'][\'structure\']\n    self.dropout_rate = model_config[\'dropout_rate\']\n    self.cell_dim = model_config[\'cell_dim\']\n    self.cell_type = model_config[\'cell_type\']\n    if self.cell_type.lower() == \'gru\':\n      rnn_class = tf.keras.layers.GRU\n    elif self.cell_type.lower() == \'lstm\':\n      rnn_class = tf.keras.layers.LSTM\n    elif self.cell_type.lower() == \'cudnngru\':\n      rnn_class = tf.keras.layers.CuDNNGRU\n    elif self.cell_type.lower() == \'cudnnlstm\':\n      rnn_class = tf.keras.layers.CuDNNLSTM\n    else:\n      error_info = ""Cell type: {} not supported now! Please check!"".format(\n          self.cell_type)\n      logging.error(error_info)\n      raise ValueError(error_info)\n\n    self.sen_encoder = tf.keras.layers.Bidirectional(\n        rnn_class(self.cell_dim, return_sequences=True, return_state=True))\n    logging.info(""Initialize RnnEncoder {} Done."".format(self.name))\n\n  def build(self, input_shape):\n    self.built = True\n\n  def compute_output_shape(self, input_shape):\n    return tf.TensorShape([input_shape[0], self.cell_dim * 2])\n\n  def call(self, inputs, training=None, mask=None):\n    if \'lstm\' in self.cell_type.lower():\n      out, forward_h, forward_c, backward_h, backward_c = self.sen_encoder(\n          inputs)\n      state_h = tf.keras.layers.concatenate([forward_h, backward_h])\n      state_c = tf.keras.layers.concatenate([forward_c, backward_c])\n      states = tf.nn.rnn_cell.LSTMStateTuple(state_h, state_c)\n    else:\n      out, forward_h, backward_h = self.sen_encoder(inputs)\n      states = tf.keras.layers.concatenate([forward_h, backward_h])\n    return out, states\n\n\nclass RnnDecoder(Layer):  # pylint: disable=too-many-instance-attributes\n  """"""\n  RNN + Attention\n  Input Shape: [batch_size, steps, features]\n  Output Shape: [batch_size, units]\n  """"""\n\n  def __init__(self, config, emb_layer, vocab_size, **kwargs):\n    super().__init__(**kwargs)\n    logging.info(""Initialize RnnDecoder {}..."".format(self.name))\n    self.is_infer = config[\'model\'][\'is_infer\']\n    model_config = config[\'model\'][\'net\'][\'structure\']\n    self.dropout_rate = model_config[\'dropout_rate\']\n    self.cell_dim = model_config[\'cell_dim\']\n    self.decode_cell_type = model_config[\'decode_cell_type\']\n    self.max_dec_len = model_config[\'max_dec_len\']\n    self.dec_end_id = 5\n    self.dec_start_id = 4\n    self.beam_size = model_config[\'beam_size\']\n    self.length_penalty = model_config[\'length_penalty\']\n    self.swap_memory = model_config[\'swap_memory\']\n    self.time_major = model_config[\'time_major\']\n    self.initial_decode_state = model_config[\'initial_decode_state\']\n    self.attn_Type = model_config[\'attn_Type\']\n    if self.decode_cell_type.lower() == \'gru\':\n      rnn_class = tf.nn.rnn_cell.GRUCell\n    elif self.decode_cell_type.lower() == \'lstm\':\n      rnn_class = tf.nn.rnn_cell.LSTMCell\n    else:\n      error_info = ""Cell type: {} not supported now! Please check!"".format(\n          self.decode_cell_type)\n      logging.error(error_info)\n      raise ValueError(error_info)\n\n    self.cell = rnn_class(2 * self.cell_dim)\n    self.embed = emb_layer\n    self.vocab_size = vocab_size\n    self.embed_d = tf.keras.layers.Dropout(self.dropout_rate)\n\n  def build(self, input_shape):\n    self.built = True\n\n  def call(self, inputs, training=None, mask=None):\n    dec_emb_fn = lambda ids: self.embed(ids)\n    if self.is_infer:\n      enc_outputs, enc_state, enc_seq_len = inputs\n      batch_size = tf.shape(enc_outputs)[0]\n      helper = seq2seq.GreedyEmbeddingHelper(\n          embedding=dec_emb_fn,\n          start_tokens=tf.fill([batch_size], self.dec_start_id),\n          end_token=self.dec_end_id)\n    else:\n      dec_inputs, dec_seq_len, enc_outputs, enc_state, \\\n      enc_seq_len = inputs\n      batch_size = tf.shape(enc_outputs)[0]\n      dec_inputs = self.embed(dec_inputs)\n      helper = seq2seq.TrainingHelper(\n          inputs=dec_inputs, sequence_length=dec_seq_len)\n\n    if self.is_infer and self.beam_size > 1:\n      tiled_enc_outputs = seq2seq.tile_batch(\n          enc_outputs, multiplier=self.beam_size)\n      tiled_seq_len = seq2seq.tile_batch(enc_seq_len, multiplier=self.beam_size)\n      attn_mech = self._build_attention(\n          enc_outputs=tiled_enc_outputs, enc_seq_len=tiled_seq_len)\n      dec_cell = seq2seq.AttentionWrapper(self.cell, attn_mech)\n      tiled_enc_last_state = seq2seq.tile_batch(\n          enc_state, multiplier=self.beam_size)\n      tiled_dec_init_state = dec_cell.zero_state(\n          batch_size=batch_size * self.beam_size, dtype=tf.float32)\n      if self.initial_decode_state:\n        tiled_dec_init_state = tiled_dec_init_state.clone(\n            cell_state=tiled_enc_last_state)\n\n      dec = seq2seq.BeamSearchDecoder(\n          cell=dec_cell,\n          embedding=dec_emb_fn,\n          start_tokens=tf.tile([self.dec_start_id], [batch_size]),\n          end_token=self.dec_end_id,\n          initial_state=tiled_dec_init_state,\n          beam_width=self.beam_size,\n          output_layer=tf.layers.Dense(self.vocab_size),\n          length_penalty_weight=self.length_penalty)\n    else:\n      attn_mech = self._build_attention(\n          enc_outputs=enc_outputs, enc_seq_len=enc_seq_len)\n      dec_cell = seq2seq.AttentionWrapper(\n          cell=self.cell, attention_mechanism=attn_mech)\n      dec_init_state = dec_cell.zero_state(\n          batch_size=batch_size, dtype=tf.float32)\n      if self.initial_decode_state:\n        dec_init_state = dec_init_state.clone(cell_state=enc_state)\n      dec = seq2seq.BasicDecoder(\n          cell=dec_cell,\n          helper=helper,\n          initial_state=dec_init_state,\n          output_layer=tf.layers.Dense(self.vocab_size))\n    if self.is_infer:\n      dec_outputs, _, _ = \\\n        seq2seq.dynamic_decode(decoder=dec,\n                               maximum_iterations=self.max_dec_len,\n                               swap_memory=self.swap_memory,\n                               output_time_major=self.time_major)\n      return dec_outputs.predicted_ids[:, :, 0]\n    else:\n      dec_outputs, _, _ = \\\n        seq2seq.dynamic_decode(decoder=dec,\n                               maximum_iterations=tf.reduce_max(dec_seq_len),\n                               swap_memory=self.swap_memory,\n                               output_time_major=self.time_major)\n    return dec_outputs.rnn_output\n\n  def _build_attention(self, enc_outputs, enc_seq_len):\n    with tf.variable_scope(""AttentionMechanism""):\n      if self.attn_Type == \'bahdanau\':\n        attention_mechanism = seq2seq.BahdanauAttention(\n            num_units=2 * self.cell_dim,\n            memory=enc_outputs,\n            memory_sequence_length=enc_seq_len,\n            probability_fn=tf.nn.softmax,\n            normalize=True,\n            dtype=tf.get_variable_scope().dtype)\n      elif self.params[\'attention_type\'] == \'luong\':\n        attention_mechanism = seq2seq.LuongAttention(\n            num_units=2 * self.cell_dim,\n            memory=enc_outputs,\n            memory_sequence_length=enc_seq_len,\n            probability_fn=tf.nn.softmax,\n            dtype=tf.get_variable_scope().dtype)\n      else:\n        raise ValueError(\'Unknown Attention Type\')\n      return attention_mechanism\n'"
delta/layers/resnet.py,1,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' resnet layers\'\'\'\n#pylint: disable=no-name-in-module\nimport delta.compat as tf\nfrom tensorflow.python.keras import backend as K\n\nfrom delta.layers.base_layer import Layer\n\n#pylint: disable=invalid-name\n#pylint: disable=attribute-defined-outside-init\n#pylint: disable=missing-docstring\n#pylint: disable=too-many-instance-attributes\n#pylint: disable=attribute-defined-outside-init\n#pylint: disable=too-many-ancestors\n\nlayers = tf.keras.layers\n\n\nclass IdentityBlock(Layer):\n\n  def __init__(self, kernel_size, filters, stage, block):\n    """"""The identity block is the block that has no conv layer at shortcut.\n      # Arguments\n          kernel_size: default 3, the kernel size of\n              middle conv layer at main path\n          filters: list of integers, the filters of 3 conv layer at main path\n          stage: integer, current stage label, used for generating layer names\n          block: \'a\',\'b\'..., current block label, used for generating layer names\n      # Returns\n          Output tensor for the block.\n    """"""\n    super().__init__(name=\'identity\' + str(stage) + block)\n    filters1, filters2, filters3 = filters\n    if K.image_data_format() == \'channels_last\':\n      bn_axis = 3\n    else:\n      bn_axis = 1\n\n    conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n    bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n\n    self.conv1 = layers.Conv2D(\n        filters1, (1, 1),\n        kernel_initializer=\'he_normal\',\n        name=conv_name_base + \'2a\')\n    self.bn1 = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + \'2a\')\n    self.act1 = layers.Activation(\'relu\')\n\n    self.conv2 = layers.Conv2D(\n        filters2,\n        kernel_size,\n        padding=\'same\',\n        kernel_initializer=\'he_normal\',\n        name=conv_name_base + \'2b\')\n    self.bn2 = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + \'2b\')\n    self.act2 = layers.Activation(\'relu\')\n\n    self.conv3 = layers.Conv2D(\n        filters3, (1, 1),\n        kernel_initializer=\'he_normal\',\n        name=conv_name_base + \'2c\')\n    self.bn3 = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + \'2c\')\n\n    self.add = layers.Add()\n    self.act = layers.Activation(\'relu\')\n\n  #pylint: disable=arguments-differ\n  def call(self, input_tensor, training=None, mask=None):\n    x = self.conv1(input_tensor)\n    x = self.bn1(x)\n    x = self.act1(x)\n\n    x = self.conv2(x)\n    x = self.bn2(x)\n    x = self.act2(x)\n\n    x = self.conv3(x)\n    x = self.bn3(x)\n\n    x = self.add([x, input_tensor])\n    x = self.act(x)\n    return x\n\n\nclass ConvBlock(Layer):\n\n  #pylint: disable=too-many-arguments\n  def __init__(self, kernel_size, filters, stage, block, strides=(2, 2)):\n    """"""A block that has a conv layer at shortcut.\n    # Arguments\n        kernel_size: default 3, the kernel size of\n            middle conv layer at main path\n        filters: list of integers, the filters of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: \'a\',\'b\'..., current block label, used for generating layer names\n        strides: Strides for the first conv layer in the block.\n    # Returns\n        Output tensor for the block.\n    Note that from stage 3,\n    the first conv layer at main path is with strides=(2, 2)\n    And the shortcut should have strides=(2, 2) as well\n    """"""\n    super().__init__(name=\'conv_block\' + str(stage) + block)\n    filters1, filters2, filters3 = filters\n    if K.image_data_format() == \'channels_last\':\n      bn_axis = 3\n    else:\n      bn_axis = 1\n    conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n    bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n\n    self.conv1 = layers.Conv2D(\n        filters1, (1, 1),\n        strides=strides,\n        kernel_initializer=\'he_normal\',\n        name=conv_name_base + \'2a\')\n    self.bn1 = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + \'2a\')\n    self.act1 = layers.Activation(\'relu\')\n\n    self.conv2 = layers.Conv2D(\n        filters2,\n        kernel_size,\n        padding=\'same\',\n        kernel_initializer=\'he_normal\',\n        name=conv_name_base + \'2b\')\n    self.bn2 = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + \'2b\')\n    self.act2 = layers.Activation(\'relu\')\n\n    self.conv3 = layers.Conv2D(\n        filters3, (1, 1),\n        kernel_initializer=\'he_normal\',\n        name=conv_name_base + \'2c\')\n    self.bn3 = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + \'2c\')\n\n    self.shortcut_conv = layers.Conv2D(\n        filters3, (1, 1),\n        strides=strides,\n        kernel_initializer=\'he_normal\',\n        name=conv_name_base + \'1\')\n    self.shortcut_bn = layers.BatchNormalization(\n        axis=bn_axis, name=bn_name_base + \'1\')\n\n    self.add = layers.Add()\n    self.act = layers.Activation(\'relu\')\n\n  #pylint: disable=arguments-differ\n  def call(self, input_tensor, training=None, mask=None):\n    x = self.conv1(input_tensor)\n    x = self.bn1(x)\n    x = self.act1(x)\n\n    x = self.conv2(x)\n    x = self.bn2(x)\n    x = self.act2(x)\n\n    x = self.conv3(x)\n    x = self.bn3(x)\n\n    shortcut = self.shortcut_conv(input_tensor)\n    shortcut = self.shortcut_bn(shortcut)\n\n    x = self.add([x, shortcut])\n    x = self.act(x)\n    return x\n'"
delta/layers/sub_tf.py,20,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Transformer sub layers.""""""\nfrom absl import logging\nimport delta.compat as tf\nimport numpy as np\nfrom delta.layers.base_layer import Layer\nfrom delta.layers.utils_tf import shape_list\n\n#pylint: disable=invalid-name, too-many-instance-attributes, too-many-arguments\n\n\nclass PositionEmbedding(Layer):\n  """"""\n    PositionEmbedding represents the positional information of tokens\n    consisting of two optional types: constant(untrainable) and trainable.\n  """"""\n  def __init__(self, max_len, embed_dim, use_const, name, **kwargs):\n    super().__init__(**kwargs)\n    self.max_len = max_len\n    self.embed_dim = embed_dim\n    self.use_const = use_const\n    self.pos_name = name\n    self.pos_embed = self.get_pos_embedding_matrix(self.max_len,\n                                                   self.embed_dim,\n                                                   self.use_const,\n                                                   self.pos_name)\n  @staticmethod\n  def get_pos_embedding_matrix(max_len, embed_dim, use_const, name):\n    """"""\n    generate position embedding matrix, two optional types:\n    constant(untrainable) and trainable.\n    Args:\n      max_len, embed_dim, use_const\n\n    Return:\n      pos_embed: [max_len, embed_dim]\n    """"""\n    # First part of the PE function: sin and cos argument\n    if use_const:\n      pos_embed = np.array([[\n        pos / np.power(10000, (i - i % 2) / embed_dim)\n        for i in range(embed_dim)\n      ] for pos in range(max_len)])\n\n      # Second part, apply the cosine to even columns and sin to odds.\n      pos_embed[:, 0::2] = np.sin(pos_embed[:, 0::2])  # dim 2i\n      pos_embed[:, 1::2] = np.cos(pos_embed[:, 1::2])  # dim 2i+1\n      pos_embed = pos_embed[np.newaxis, ...]\n      pos_embed = tf.cast(pos_embed, dtype=tf.float32)\n    else:\n      pos_embed = tf.get_variable(\n          name=name,\n          shape=[max_len, embed_dim],\n          initializer=tf.random_uniform_initializer(-0.1, 0.1))\n      pos_embed = tf.expand_dims(pos_embed, 0)\n\n    return pos_embed\n\n  def call(self, inputs, training=None, mask=None):\n    """"""\n    Args:\n       inputs: [batch_size, seq_x_len, embed_dim]\n    Return:\n      pos_embed: [batch_size, seq_x_len, embed_dim]\n    """"""\n    seq_len = shape_list(inputs)[1]\n    pos_embed = self.pos_embed[:, :seq_len, :]\n    return pos_embed\n\n\nclass PositionwiseFeedForward(Layer):\n  """"""\n  A two-layer Feed-Forward-Network.\n  """"""\n  def __init__(self, d_model, dff, act_func, **kwargs):\n    super().__init__(**kwargs)\n    self.dense1 = tf.keras.layers.Dense(dff, activation=act_func)\n    self.dense2 = tf.keras.layers.Dense(d_model)\n\n  def call(self, inputs, training=None, mask=None):\n    """"""\n    The implementation of PositionwiseFeedForward.\n    Args:\n      inputs: [batch_size, seq_x_len, d_model]\n    Return:\n      ffn: [batch_size, seq_x_len, d_model]\n    """"""\n    ffn = self.dense2(self.dense1(inputs))\n    return ffn\n\n\nclass MultiHeadAttention(Layer):\n  """"""\n   Multi-headed attention is based on ""Attention\n  is all you Need"" (https://arxiv.org/pdf/1706.03762.pdf).\n  """"""\n  def __init__(self, hidden_size, num_heads, **kwargs):\n    super().__init__(**kwargs)\n    self.hidden_size, self.num_heads = hidden_size, num_heads\n    assert self.hidden_size % self.num_heads == 0\n\n    self.depth = self.hidden_size // self.num_heads\n\n    self.wq = tf.keras.layers.Dense(self.hidden_size)\n    self.wk = tf.keras.layers.Dense(self.hidden_size)\n    self.wv = tf.keras.layers.Dense(self.hidden_size)\n\n    self.dense = tf.keras.layers.Dense(self.hidden_size)\n\n  def split_heads(self, x, batch_size):\n    """"""\n    Split hidden_size into depth(hidden_size // num_heads) for\n    multi-head attention.\n    Args:\n      x: (batch_size, seq_len_x, hidden_size)\n      batch_size\n\n    Returns:\n      split_x: (batch_size, num_heads, seq_len_x, depth)\n    """"""\n    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n    split_x = tf.transpose(x, perm=[0, 2, 1, 3])\n    return split_x\n\n  def call(self, inputs, training=None, mask=None):\n    """"""\n    The implementation of Multi-headed attention.\n    Args:\n      inputs = (v, k, q)\n      q: (batch_size, seq_len_q, hidden_size)\n      k: (batch_size, seq_len_k, hidden_size)\n      v: (batch_size, seq_len_v, hidden_size)\n      mask: (batch_size, seq_len_q, seq_len_k)\n\n    Returns:\n      output: (batch_size, seq_len_q, hidden_size)\n      attention_weights: (batch_size, num_heads, seq_len_q, seq_len_k)\n    """"""\n    q, k, v = inputs\n    batch_size = tf.shape(q)[0]\n\n    q = self.wq(q)  # (batch_size, seq_len_q, hidden_size)\n    k = self.wk(k)  # (batch_size, seq_len_k, hidden_size)\n    v = self.wv(v)  # (batch_size, seq_len_v, hidden_size)\n\n    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n    scaled_attention, attention_weights = self.scaled_dot_product_attention(\n      q, k, v, mask)\n\n    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n    concat_attention = tf.reshape(scaled_attention,\n                                  (batch_size, -1, self.hidden_size))  # (batch_size, seq_len_q, hidden_size)\n\n    output = self.dense(concat_attention)  # (batch_size, seq_len_q, hidden_size)\n\n    return output, attention_weights\n\n  @staticmethod\n  def scaled_dot_product_attention(q, k, v, mask):\n    """"""\n    The implementation of scaled attention.\n    Args:\n      v: (batch_size, seq_len_v, hidden_size)\n      k: (batch_size, seq_len_k, hidden_size)\n      q: (batch_size, seq_len_q, hidden_size)\n      mask: (batch_size, seq_len_q, seq_len_k)\n\n    Returns:\n      output: (batch_size, seq_len_q, hidden_size)\n      attention_weights: (batch_size, num_heads, seq_len_q, seq_len_k)\n    """"""\n\n    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (batch_size, seq_len_q, seq_len_k)\n\n    # Scaled\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n    # Masked\n    if mask is not None:\n      scaled_attention_logits += (mask * -1e9)\n\n    # Normalized\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (batch_size, seq_len_q, seq_len_k)\n\n    # Weighted sum\n    output = tf.matmul(attention_weights, v)  # (batch_size, seq_len_q, depth_v)\n\n    return output, attention_weights\n'"
delta/layers/transformer.py,61,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Transformer layers.""""""\nimport math\n\nfrom absl import logging\nimport numpy as np\nimport delta.compat as tf\nfrom tensorflow.python.util import nest\n\nfrom delta.layers.base_layer import Layer\nfrom delta.layers import utils_tf as utils\nimport delta.layers\n\n# pylint: disable=invalid-name, too-many-instance-attributes, too-many-arguments, too-many-locals\n\nclass TransformerEncoderLayer(Layer):\n  """"""\n  TransformerEncoderLayer is based on ""Attention\n  is all you Need"" (https://arxiv.org/pdf/1706.03762.pdf).\n  """"""\n  def __init__(self, config, **kwargs):\n    super().__init__(**kwargs)\n    model_config = config[\'model\'][\'net\'][\'structure\']\n    self.head_num = model_config.get(\'head_num\')\n    self.hidden_dim = model_config.get(\'hidden_dim\')\n    self.inner_size = model_config.get(\'inner_size\')\n    self.feed_forward_act = config.get(\'feed_forward_act\', \'relu\')\n    self.dropout_rate = config.get(\'dropout_rate\', 0.)\n\n    self.self_attn_layer = delta.layers.MultiHeadAttention(\n      self.hidden_dim, self.head_num)\n    self.feed_forward_layer = delta.layers.PositionwiseFeedForward(\n      self.hidden_dim, self.inner_size, self.feed_forward_act)\n    self.embed_dense = tf.keras.layers.Dense(self.hidden_dim)\n    \n    self.self_attn_dropout = tf.keras.layers.Dropout(self.dropout_rate)\n    self.feed_forward_dropout = tf.keras.layers.Dropout(self.dropout_rate)\n    \n    self.self_attn_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.feed_forward_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n  def call(self, inps, training=None, mask=None):\n    \'\'\'\n    Input shape: [batch_size, seq_enc_len, hidden_dim]\n    Mask shape: [batch_size, seq_enc_len]\n    \'\'\'\n    # Multi Head Attention\n    inps = self.embed_dense(inps)\n    self_attn_outs, _ = self.self_attn_layer((inps, inps, inps),\n                                          training=training,\n                                             mask=mask)\n    self_attn_outs = self.self_attn_dropout(\n        self_attn_outs, training=training)\n    self_attn_outs += inps\n    self_attn_outs = self.self_attn_norm(self_attn_outs)\n    # Position Wise Feed Forward\n    feed_forward_outs = self.feed_forward_layer(self_attn_outs)\n    feed_forward_outs = self.feed_forward_dropout(\n        feed_forward_outs, training=training)\n    feed_forward_outs += self_attn_outs\n    outs = self.feed_forward_norm(feed_forward_outs)\n    return outs\n\n\nclass TransformerDecoderLayer(Layer):\n  """"""\n  TransformerEncoderLayer is based on ""Attention\n  is all you Need"" (https://arxiv.org/pdf/1706.03762.pdf).\n  """"""\n  def __init__(self, config, **kwargs):\n    super().__init__(**kwargs)\n    model_config = config[\'model\'][\'net\'][\'structure\']\n    self.head_num = model_config.get(\'head_num\')\n    self.hidden_dim = model_config.get(\'hidden_dim\')\n    self.inner_size = model_config.get(\'inner_size\')\n    self.feed_forward_act = config.get(\'feed_forward_act\', \'relu\')\n    self.dropout_rate = config.get(\'dropout_rate\', 0.)\n\n    self.self_attn_layer = delta.layers.MultiHeadAttention(\n      self.hidden_dim, self.head_num)\n    self.context_attn_layer = delta.layers.MultiHeadAttention(\n      self.hidden_dim, self.head_num)\n    self.feed_forward_layer = delta.layers.PositionwiseFeedForward(\n      self.hidden_dim, self.inner_size, self.feed_forward_act)\n    self.enc_embed_dense = tf.keras.layers.Dense(self.hidden_dim)\n    self.dec_embed_dense = tf.keras.layers.Dense(self.hidden_dim)\n\n    self.self_attn_dropout = tf.keras.layers.Dropout(self.dropout_rate)\n    self.context_attn_dropout = tf.keras.layers.Dropout(self.dropout_rate)\n    self.feed_forward_dropout = tf.keras.layers.Dropout(self.dropout_rate)\n\n    self.self_attn_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.context_attn_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.feed_forward_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n  def call(self, inps, training=None, mask=None):\n    \'\'\'\n    Input: [dec_inps, enc_outs]\n    Mask: [dec_mask, enc_mask]\n\n    Input shape: [[batch_size, seq_dec_len, hidden_dim],\n                    (batch_size, seq_enc_len, hidden_dim)]\n    Mask shape: [(batch_size, seq_dec_len), (batch_size, seq_enc_len)]\n    \'\'\'\n    dec_inps, enc_outs = inps\n    dec_inps = self.dec_embed_dense(dec_inps)\n    enc_outs = self.enc_embed_dense(enc_outs)\n    look_ahead_mask, enc_mask = mask\n    # Self Attention\n    self_attn_outs, _ = self.self_attn_layer(\n      (dec_inps, dec_inps, dec_inps),\n      training=training,\n      mask=look_ahead_mask)\n    self_attn_outs = self.self_attn_dropout(\n        self_attn_outs, training=training)\n    self_attn_outs += dec_inps\n    self_attn_outs = self.self_attn_norm(self_attn_outs)\n\n    # Context Attention\n    context_attn_outs, _ = self.context_attn_layer(\n      (self_attn_outs, enc_outs, enc_outs),\n      training=training,\n      mask=enc_mask)\n    context_attn_outs = self.context_attn_dropout(\n        context_attn_outs, training=training)\n    context_attn_outs += self_attn_outs\n    context_attn_outs = self.context_attn_norm(context_attn_outs)\n\n    # Position Wise Feed Forward\n    feed_forward_outs = self.feed_forward_layer(context_attn_outs)\n    feed_forward_outs = self.feed_forward_dropout(\n        feed_forward_outs, training=training)\n    feed_forward_outs = context_attn_outs + feed_forward_outs\n    outs = self.feed_forward_norm(feed_forward_outs)\n\n    return outs\n\n\nclass TransformerEncoder(Layer):\n  """"""\n  Transformer Encoder is stacked with\n  several TransformerencLayers.\n  """"""\n\n  def __init__(self, config, **kwargs):\n    super().__init__(**kwargs)\n    model_config = config[\'model\'][\'net\'][\'structure\']\n    self.num_layers = model_config.get(\'num_layers\')\n    self.transformer_encs = [\n        TransformerEncoderLayer(config) for _ in range(self.num_layers)\n    ]\n\n  def call(self, inps, training=None, mask=None):\n    """"""\n    Input shape: [Batch size, enc_seq_len]\n    Output shape: [Batch size, enc_seq_len, hidden_dim]\n    """"""\n    enc_inps = inps\n    for enc_layer in self.transformer_encs:\n      enc_inps = enc_layer(enc_inps, training=training, mask=mask)\n    enc_outs = enc_inps\n    return enc_outs\n\n\nclass TransformerDecoder(Layer):\n  """"""\n  Transformer Encoder is stacked with several\n  TransformerdecLayers, consist of beamsearch for infrence.\n  """"""\n\n  def __init__(self, config, embed_layer, vocab_size, **kwargs):\n    super().__init__(**kwargs)\n    model_config = config[\'model\'][\'net\'][\'structure\']\n    self.is_infer = config[\'model\'][\'is_infer\']\n    if self.is_infer:\n      self.length_penalty = model_config.get(\'length_penalty\')\n    self.dropout_rate = model_config.get(\'dropout_rate\')\n    self.num_layers = model_config.get(\'num_layers\')\n    self.embedding_size = model_config.get(\'embedding_size\')\n    self.max_dec_len = model_config.get(\'max_dec_len\')\n    self.padding_id = model_config.get(\'padding_id\', 0)\n    self.sos_id = model_config.get(\'sos_id\', 4)\n    self.eos_id = model_config.get(\'eos_id\', 5)\n    self.beam_size = model_config.get(\'beam_size\')\n    self.use_const = model_config.get(\'use_const\', True)\n    self.share_embedding = model_config.get(\'share_embedding\', True)\n\n    self.vocab_size = vocab_size\n    self.embed_dropout = tf.keras.layers.Dropout(self.dropout_rate)\n\n    embed_layer, pos_embed_layer = embed_layer\n    if self.share_embedding:\n      self.pos_embed_layer = pos_embed_layer\n    else:\n      self.pos_embed_layer = delta.layers.PositionEmbedding(\n        self.max_dec_len, self.embedding_size, self.use_const, ""dec_pos"")\n    self.embed_layer = embed_layer\n    self.transformer_decs = [\n        TransformerDecoderLayer(config) for _ in range(self.num_layers)\n    ]\n    self.final_dense = tf.keras.layers.Dense(vocab_size)\n\n  def decode(self, dec_inps, enc_out, training=None, mask=None):\n    """"""\n    Decoder func\n    """"""\n    look_ahead_mask = utils.create_look_ahead_mask(dec_inps)\n    mask = (look_ahead_mask, mask)\n    dec_emb = self.embed_layer(dec_inps)\n    dec_pos_emb = self.pos_embed_layer(dec_inps)\n    dec_emb += dec_pos_emb\n\n    dec_inp = dec_emb\n    for dec_layer in self.transformer_decs:\n      dec_inp = dec_layer([dec_inp, enc_out],\n                          training=training,\n                          mask=mask)\n    dec_out = dec_inp\n    return dec_out\n\n  def call(self, inps, training=None, mask=None):\n    if not self.is_infer:\n      dec_inp, enc_out = inps\n      with tf.name_scope(\'while\'):\n        dec_out = self.decode(dec_inp, enc_out, training, mask)\n        scores = self.final_dense(dec_out)\n        return scores\n    else:\n      enc_out = inps\n      init_ids = tf.cast(tf.ones([utils.shape_list(enc_out)[0]]) * self.sos_id, tf.int32)\n      # Beam Search\n      enc_shape = utils.shape_list(enc_out)\n      enc_out = tf.tile(\n          tf.expand_dims(enc_out, axis=1), [1, self.beam_size, 1, 1])\n      enc_out = tf.reshape(\n          enc_out, [enc_shape[0] * self.beam_size, enc_shape[1], enc_shape[2]])\n      enc_mask = tf.tile(tf.expand_dims(mask, axis=1), [1, self.beam_size, 1, 1, 1])\n      enc_mask = tf.reshape(enc_mask,\n                            [enc_shape[0] * self.beam_size, 1, 1, -1])\n      def symbols_to_logits_fn(dec_inps):\n        dec_out = self.decode(dec_inps, enc_out, training, enc_mask)\n        scores = self.final_dense(dec_out)\n        return scores[:, -1, :]\n\n      decoded_ids, scores, _ = self.beam_search(symbols_to_logits_fn, init_ids,\n                                                self.beam_size,\n                                                self.max_dec_len,\n                                                self.vocab_size,\n                                                self.length_penalty,\n                                                self.eos_id)\n      decoded_ids = decoded_ids[:, 0, 1:]\n\n      return decoded_ids\n\n  @staticmethod\n  def beam_search(symbols_to_logits_fn,\n                  initial_ids,\n                  beam_size,\n                  decode_length,\n                  vocab_size,\n                  alpha,\n                  eos_id,\n                  states=None,\n                  stop_early=True,\n                  INF=1. * 1e20):\n    """"""Beam search with length penalties.""""""\n    batch_size = utils.shape_list(initial_ids)[0]\n\n    initial_log_probs = tf.constant([[0.] + [-INF] * (beam_size - 1)])\n    # (batch_size, beam_size)\n    alive_log_probs = tf.tile(initial_log_probs, [batch_size, 1])\n\n    alive_seq = utils.expand_to_beam_size(initial_ids, beam_size)\n    # (batch_size, beam_size, 1)\n    alive_seq = tf.expand_dims(alive_seq, axis=2)\n    if states:\n      states = nest.map_structure(\n          lambda state: utils.expand_to_beam_size(state, beam_size), states)\n    else:\n      states = {}\n\n    # (batch_size, beam_size, 1)\n    finished_seq = tf.zeros(utils.shape_list(alive_seq), tf.int32)\n    # (batch_size, beam_size)\n    finished_scores = tf.ones([batch_size, beam_size]) * -INF\n    # (batch_size, beam_size)\n    finished_flags = tf.zeros([batch_size, beam_size], tf.bool)\n\n    def grow_finished(finished_seq, finished_scores, finished_flags, curr_seq,\n                      curr_scores, curr_finished):\n      """"""\n        Given sequences and scores from finished sequence and current finished sequence\n        , will gather the top k=beam size sequences to update finished seq.\n      """"""\n      # padding zero for finished seq\n      finished_seq = tf.concat(\n          [finished_seq,\n           tf.zeros([batch_size, beam_size, 1], tf.int32)],\n          axis=2)\n\n      # mask unfinished curr seq\n      curr_scores += (1. - tf.to_float(curr_finished)) * -INF\n\n      # concatenating the sequences and scores along beam axis\n      # (batch_size, 2xbeam_size, seq_len)\n      curr_finished_seq = tf.concat([finished_seq, curr_seq], axis=1)\n      curr_finished_scores = tf.concat([finished_scores, curr_scores], axis=1)\n      curr_finished_flags = tf.concat([finished_flags, curr_finished], axis=1)\n      return utils.compute_topk_scores_and_seq(curr_finished_seq,\n                                               curr_finished_scores,\n                                               curr_finished_scores,\n                                               curr_finished_flags, beam_size,\n                                               batch_size, ""grow_finished"")\n\n    def grow_alive(curr_seq, curr_scores, curr_log_probs, curr_finished,\n                   states):\n      """"""Given sequences and scores, will gather the top k=beam size sequences.""""""\n      curr_scores += tf.to_float(curr_finished) * -INF\n      return utils.compute_topk_scores_and_seq(curr_seq, curr_scores, curr_log_probs,\n                                               curr_finished, beam_size, batch_size,\n                                               ""grow_alive"", states)\n\n    def grow_topk(i, alive_seq, alive_log_probs, states):\n      """"""Inner beam search loop.""""""\n      flat_ids = tf.reshape(alive_seq, [batch_size * beam_size, -1])\n\n      # (batch_size * beam_size, decoded_length)\n      if states:\n        flat_states = nest.map_structure(utils.merge_beam_dim, states)\n        flat_logits, flat_states = symbols_to_logits_fn(flat_ids, i,\n                                                        flat_states)\n        states = nest.map_structure(\n            lambda t: utils.unmerge_beam_dim(t, batch_size, beam_size), flat_states)\n      else:\n        flat_logits = symbols_to_logits_fn(flat_ids)\n\n      logits = tf.reshape(flat_logits, [batch_size, beam_size, -1])\n      candidate_log_probs = utils.log_prob_from_logits(logits)\n      log_probs = candidate_log_probs + tf.expand_dims(alive_log_probs, axis=2)\n\n      length_penalty = tf.pow(((5. + tf.to_float(i + 1)) / 6.), alpha)\n\n      curr_scores = log_probs / length_penalty\n      flat_curr_scores = tf.reshape(curr_scores, [-1, beam_size * vocab_size])\n\n      topk_scores, topk_ids = tf.nn.top_k(flat_curr_scores, k=beam_size * 2)\n      topk_log_probs = topk_scores * length_penalty\n\n      topk_beam_index = topk_ids // vocab_size\n      topk_ids %= vocab_size  # Unflatten the ids\n      batch_pos = utils.compute_batch_indices(batch_size, beam_size * 2)\n      topk_coordinates = tf.stack([batch_pos, topk_beam_index], axis=2)\n\n      topk_seq = tf.gather_nd(alive_seq, topk_coordinates)\n      if states:\n        states = nest.map_structure(\n            lambda state: tf.gather_nd(state, topk_coordinates), states)\n      topk_seq = tf.concat([topk_seq, tf.expand_dims(topk_ids, axis=2)], axis=2)\n\n      topk_finished = tf.equal(topk_ids, eos_id)\n\n      return topk_seq, topk_log_probs, topk_scores, topk_finished, states\n\n    def inner_loop(i, alive_seq, alive_log_probs, finished_seq, finished_scores,\n                   finished_flags, states):\n      """"""Inner beam search loop.""""""\n      topk_seq, topk_log_probs, topk_scores, topk_finished, states = grow_topk(\n          i, alive_seq, alive_log_probs, states)\n      alive_seq, alive_log_probs, _, states = grow_alive(\n          topk_seq, topk_scores, topk_log_probs, topk_finished, states)\n      finished_seq, finished_scores, finished_flags, _ = grow_finished(\n          finished_seq, finished_scores, finished_flags, topk_seq, topk_scores,\n          topk_finished)\n\n      return (i + 1, alive_seq, alive_log_probs, finished_seq, finished_scores,\n              finished_flags, states)\n\n    def _is_finished(i, unused_alive_seq, alive_log_probs, unused_finished_seq,\n                     finished_scores, unused_finished_in_finished,\n                     unused_states):\n      """"""Checking termination condition.\n      """"""\n      max_length_penalty = tf.pow(((5. + tf.to_float(decode_length)) / 6.),\n                                  alpha)\n      lower_bound_alive_scores = alive_log_probs[:, 0] / max_length_penalty\n\n      if not stop_early:\n        lowest_score_of_finished_in_finished = tf.reduce_min(finished_scores)\n      else:\n        lowest_score_of_finished_in_finished = tf.reduce_max(\n            finished_scores, axis=1)\n\n      bound_is_met = tf.reduce_all(\n          tf.greater(lowest_score_of_finished_in_finished,\n                     lower_bound_alive_scores))\n\n      return tf.logical_and(\n          tf.less(i, decode_length), tf.logical_not(bound_is_met))\n\n    inner_shape = tf.TensorShape([None, None, None])\n\n    state_struc = nest.map_structure(utils.get_state_shape_invariants, states)\n    (_, alive_seq, alive_log_probs, finished_seq, finished_scores,\n     finished_flags, states) = tf.while_loop(\n         _is_finished,\n         inner_loop, [\n             tf.constant(0), alive_seq, alive_log_probs, finished_seq,\n             finished_scores, finished_flags, states\n         ],\n         shape_invariants=[\n             tf.TensorShape([]), inner_shape,\n             alive_log_probs.get_shape(), inner_shape,\n             finished_scores.get_shape(),\n             finished_flags.get_shape(), state_struc\n         ],\n         parallel_iterations=1,\n         back_prop=False)\n\n    alive_seq.set_shape((None, beam_size, None))\n    finished_seq.set_shape((None, beam_size, None))\n    finished_seq = tf.where(\n        tf.reduce_any(finished_flags, 1), finished_seq, alive_seq)\n    finished_scores = tf.where(\n        tf.reduce_any(finished_flags, 1), finished_scores, alive_log_probs)\n    return finished_seq, finished_scores, states\n'"
delta/layers/utils.py,28,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Utilities for building model layers.""""""\n\nimport math\nimport delta.compat as tf\n# pylint: disable=no-name-in-module\nfrom tensorflow.python.keras import backend as K\n\nimport delta.utils as utils\n\n\n# pylint: disable=invalid-name\ndef gelu(x):\n  """"""An approximation of gelu.\n     See: https://arxiv.org/pdf/1606.08415.pdf\n  """"""\n  return 0.5 * x * (\n      1.0 + K.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * K.pow(x, 3))))\n\n\ndef cut_or_padding(origin_t, new_length, padding_token=0):\n  """"""\n  If too long, cut the tensor; else pad the tensor.\n  origin_t: [batch_size, time_steps_1] or [time_steps_1]\n  new_t: [batch_size, time_steps_2] or [time_steps_2]\n  """"""\n\n  if len(origin_t.get_shape()) == 1:\n    dim = 1\n    cur_length = tf.shape(origin_t)[0]\n  elif len(origin_t.get_shape()) == 2:\n    dim = 2\n    cur_length = tf.shape(origin_t)[1]\n  else:\n    raise ValueError(""origin_t should be a tensor with rank 1 or 2."")\n\n  def cut_tensor():\n    if dim == 1:\n      new_t = origin_t[:new_length]\n    else:\n      new_t = origin_t[:, :new_length]\n    return new_t\n\n  def pad_tail_tensor():\n    if dim == 1:\n      shape = tf.constant([1, 2])\n      indices = tf.constant([[0, 1]])\n    else:\n      shape = tf.constant([2, 2])\n      indices = tf.constant([[1, 1]])\n    updates = [new_length - cur_length]\n    paddings = tf.scatter_nd(indices, updates, shape)\n    new_t = tf.pad(\n        origin_t, paddings, ""CONSTANT"", constant_values=padding_token)\n    return new_t\n\n  new_t = tf.cond(\n      cur_length < new_length, true_fn=pad_tail_tensor, false_fn=cut_tensor)\n\n  if dim == 1:\n    new_t.set_shape([new_length])\n  else:\n    new_t.set_shape([origin_t.get_shape()[0], new_length])\n\n  return new_t\n\n\ndef compute_sen_lens(inputs, padding_token=0):\n  """"""\n  Count how many words in a sentence.\n  inputs: [..., time_steps]\n  sen_lens: [...]\n  """"""\n  x_binary = tf.cast(tf.not_equal(inputs, padding_token), tf.int32)\n  sen_lens = tf.reduce_sum(x_binary, axis=-1)\n  ones = tf.ones_like(sen_lens)\n  sen_lens = tf.where(tf.equal(sen_lens, utils.PAD_IDX), x=ones, y=sen_lens)\n  return sen_lens\n\n\ndef compute_doc_lens(sen_lens):\n  """"""\n  Count how many sentences in a document.\n  inputs: [..., time_steps]\n  doc_lens: [...]\n  """"""\n  x_binary = tf.cast(tf.cast(sen_lens, tf.bool), tf.int32)\n  doc_lens = tf.reduce_sum(x_binary, axis=-1)\n  return doc_lens\n\n\ndef split_one_doc_to_true_len_sens(doc_t, split_token, padding_token,\n                                   max_doc_len, max_sen_len):\n  """"""\n  Split a document to sentences with true sentence lengths.\n  doc_t: [doc_word_len]\n  out_t: [max_doc_len, max_sen_len]\n  """"""\n  if len(doc_t.get_shape()) == 1:\n    split_token_index = tf.squeeze(\n        tf.where(tf.equal(doc_t, split_token)), axis=1)\n    split_token_index.set_shape([None])\n    split_len_part_1 = split_token_index[:1] + 1\n    split_len_part_2 = split_token_index[1:] - split_token_index[:-1]\n    split_lens = tf.concat([split_len_part_1, split_len_part_2], axis=0)\n    split_lens = cut_or_padding(\n        split_lens, max_doc_len, padding_token=padding_token)\n    new_doc_len = tf.reduce_sum(split_lens)\n    splited_sentences = tf.split(doc_t[:new_doc_len], split_lens)\n    splited_sentences = [\n        cut_or_padding(s, max_sen_len) for s in splited_sentences\n    ]\n    out_t = tf.stack(splited_sentences)\n    padding_tokens = tf.multiply(\n        tf.ones_like(out_t, dtype=tf.int32), padding_token)\n    out_t = tf.where(tf.equal(out_t, split_token), padding_tokens, out_t)\n    return out_t\n\n  raise ValueError(""doc_t should be a tensor with rank 1."")\n\n\ndef get_pad_mask_from_token_idx(inputs, pad_idx):\n  """"""\n  get padding mask from the input token idx\n  inputs: [batch_size, time_steps]\n  mask: [batch_size, time_steps]\n  """"""\n  pad_mask = tf.cast(tf.math.greater(inputs, pad_idx), tf.float32)\n  return pad_mask\n\ndef get_expand_pad_mask(inputs, pad_idx):\n  """"""\n  get padding mask from the input token idx\n  inputs: [batch_size, time_steps]\n  mask: [batch_size, time_steps, 1]\n  """"""\n  pad_mask = tf.cast(tf.math.greater(inputs, pad_idx), tf.float32)\n  pad_mask = tf.expand_dims(pad_mask, -1)\n  return pad_mask\n\ndef get_seg_mask_from_token_idx(inputs, seg_idx):\n  """"""\n  get padding mask from the input token idx\n  inputs: [batch_size, time_steps]\n  mask: [batch_size, time_steps]\n  """"""\n  seg_mask = tf.cast(tf.math.equal(inputs, seg_idx), tf.int32)\n  return seg_mask\n'"
delta/layers/utils_test.py,8,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Test for layer utilities.""""""\n\nfrom absl import logging\nimport delta.compat as tf\n\nfrom delta.layers import utils\nfrom delta.layers.utils import cut_or_padding\nfrom delta.layers.utils import compute_sen_lens\nfrom delta.layers.utils import compute_doc_lens\nfrom delta.layers.utils import split_one_doc_to_true_len_sens\n\n# pylint: disable=missing-docstring\n\n\nclass LayerUtilsTest(tf.test.TestCase):\n\n  def setUp(self):\n    super().setUp()\n\n  def tearDown(self):\n    \'\'\' tear down\'\'\'\n\n  def test_gelu(self):\n    \'\'\' test gelue activation \'\'\'\n\n    # pylint: disable=invalid-name\n    y = utils.gelu(tf.constant([0.5, 0.2], dtype=tf.float32))\n    y_true = [0.345714, 0.11585142]\n\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      y_pred = sess.run(y)\n      self.assertAllClose(y_pred, y_true)\n\n  def test_cut_or_padding(self):\n    # test for 1d\n    origin_1_t = tf.placeholder(dtype=tf.int32, shape=[None])\n    after_1_t = cut_or_padding(origin_1_t, 3)\n\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      # test for padding\n      res = sess.run(after_1_t, feed_dict={origin_1_t: [1, 2]})\n      self.assertAllEqual(res, [1, 2, 0])\n\n      # test for cut\n      res = sess.run(after_1_t, feed_dict={origin_1_t: [1, 2, 3, 4, 5]})\n      self.assertAllEqual(res, [1, 2, 3])\n\n    # test for 2d\n    origin_2_t = tf.placeholder(dtype=tf.int32, shape=[None, None])\n    after_2_t = cut_or_padding(origin_2_t, 3)\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      # test for padding\n      res = sess.run(after_2_t, feed_dict={origin_2_t: [[1, 2], [1, 2]]})\n      self.assertAllEqual(res, [[1, 2, 0], [1, 2, 0]])\n\n      # test for cut\n      res = sess.run(\n          after_2_t, feed_dict={origin_2_t: [[1, 2, 3, 4], [1, 2, 3, 4]]})\n      self.assertAllEqual(res, [[1, 2, 3], [1, 2, 3]])\n\n  def test_compute_sen_lens(self):\n    sentences = tf.placeholder(dtype=tf.int32)\n    lens = compute_sen_lens(sentences)\n\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      # test for 1d\n      res = sess.run(lens, feed_dict={sentences: [1, 2, 0, 0]})\n      self.assertEqual(res, 2)\n\n      # test for 2d\n      res = sess.run(lens, feed_dict={sentences: [[1, 2, 0, 0], [1, 2, 3, 4]]})\n      self.assertAllEqual(res, [2, 4])\n\n      # test for 3d\n      res = sess.run(\n          lens,\n          feed_dict={\n              sentences: [[[1, 2, 0, 0]], [[1, 2, 3, 4]], [[1, 0, 0, 0]]]\n          })\n      self.assertAllEqual(res, [[2], [4], [1]])\n\n  def test_compute_doc_lens(self):\n    \'\'\' compute document length\'\'\'\n    docs = tf.placeholder(dtype=tf.int32)\n    lens = compute_doc_lens(docs)\n\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      # test for 1d\n      res = sess.run(lens, feed_dict={docs: [1, 2, 0, 0]})\n      self.assertEqual(res, 2)\n\n      # test for 2d\n      res = sess.run(lens, feed_dict={docs: [[1, 2, 0, 0], [1, 2, 3, 4]]})\n      self.assertAllEqual(res, [2, 4])\n\n  def test_split_one_doc_to_true_len_sens(self):\n    doc = tf.placeholder(dtype=tf.int32, shape=[None])\n    split_token = 1\n    padding_token = 0\n    max_doc_len = 4\n    max_sen_len = 5\n    lens = split_one_doc_to_true_len_sens(doc, split_token, padding_token,\n                                          max_doc_len, max_sen_len)\n\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      res = sess.run(lens, feed_dict={doc: [2, 3, 1, 2, 1, 2, 3, 4, 5, 6, 1]})\n      self.assertAllEqual(\n          res,\n          [[2, 3, 0, 0, 0], [2, 0, 0, 0, 0], [2, 3, 4, 5, 6], [0, 0, 0, 0, 0]])\n\n      all_empty = [[0 for _ in range(max_sen_len)] for _ in range(max_doc_len)]\n      res = sess.run(lens, feed_dict={doc: []})\n      self.assertAllEqual(res, all_empty)\n\n      res = sess.run(lens, feed_dict={doc: [1, 1, 1, 1, 1]})\n      self.assertAllEqual(res, all_empty)\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  tf.test.main()\n'"
delta/layers/utils_tf.py,56,"b'# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Utilities for building transformer layers.""""""\nimport math\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.util import nest\n\ndef log_prob_from_logits(logits, reduce_axis=-1):\n  """"""return log prob use log sum func""""""\n  return logits - tf.reduce_logsumexp(logits, axis=reduce_axis, keepdims=True)\n\n#\ndef shape_list(tensor):\n  """"""Return list of dims, statically where possible.""""""\n  tensor = tf.convert_to_tensor(tensor)\n\n  if tensor.get_shape().dims is None:\n    return tf.shape(tensor)\n\n  static = tensor.get_shape().as_list()\n  shape = tf.shape(tensor)\n\n  ret = []\n  for i, _ in enumerate(static):\n    dim = static[i]\n    if dim is None:\n      dim = shape[i]\n    ret.append(dim)\n  return ret\n\n\ndef merge_beam_dim(tensor):\n  """"""Reshapes first two dimensions in to single dimension.""""""\n  shape = shape_list(tensor)\n  shape[0] *= shape[1]  # batch -> batch * beam_size\n  shape.pop(1)  # Remove beam dim\n  return tf.reshape(tensor, shape)\n\n\ndef unmerge_beam_dim(tensor, batch_size, beam_size):\n  """"""Reshapes first dimension back to [batch_size, beam_size].""""""\n  shape = shape_list(tensor)\n  new_shape = [batch_size] + [beam_size] + shape[1:]\n  return tf.reshape(tensor, new_shape)\n\n\ndef expand_to_beam_size(tensor, beam_size):\n  """"""Tiles a given tensor by beam_size.""""""\n  tensor = tf.expand_dims(tensor, axis=1)\n  tile_dims = [1] * tensor.shape.ndims\n  tile_dims[1] = beam_size\n\n  return tf.tile(tensor, tile_dims)\n\n\ndef get_state_shape_invariants(tensor):\n  """"""Returns the shape of the tensor but sets middle dims to None.""""""\n  shape = tensor.shape.as_list()\n  for i in range(1, len(shape) - 1):\n    shape[i] = None\n  return tf.TensorShape(shape)\n\n\ndef compute_batch_indices(batch_size, beam_size):\n  """"""Computes the i\'th coordinate that contains the batch index for gathers.""""""\n  batch_pos = tf.range(batch_size * beam_size) // beam_size\n  batch_pos = tf.reshape(batch_pos, [batch_size, beam_size])\n  return batch_pos\n\n\ndef create_make_unique(inputs):\n  """"""Replaces the lower bits of each element with iota.""""""\n  if inputs.shape.ndims != 2:\n    raise ValueError(""Input of top_k_with_unique must be rank-2 ""\n                     ""but got: %s"" % inputs.shape)\n\n  height = inputs.shape[0]\n  width = inputs.shape[1]\n  zeros = tf.zeros([height, width], dtype=tf.int32)\n\n  log2_ceiling = int(math.ceil(math.log(int(width), 2)))\n  next_power_of_two = 1 << log2_ceiling\n  count_mask = ~(next_power_of_two - 1)\n  count_mask_r0 = tf.constant(count_mask)\n  count_mask_r2 = tf.fill([height, width], count_mask_r0)\n\n  smallest_normal = 1 << 23\n  smallest_normal_r0 = tf.constant(smallest_normal, dtype=tf.int32)\n  smallest_normal_r2 = tf.fill([height, width], smallest_normal_r0)\n\n  low_bit_mask = ~(1 << 31)\n  low_bit_mask_r0 = tf.constant(low_bit_mask, dtype=tf.int32)\n  low_bit_mask_r2 = tf.fill([height, width], low_bit_mask_r0)\n\n  iota = tf.tile(\n      tf.expand_dims(tf.range(width, dtype=tf.int32), 0), [height, 1])\n\n  input_r2 = tf.bitcast(inputs, tf.int32)\n  abs_r2 = tf.bitwise.bitwise_and(input_r2, low_bit_mask_r2)\n  if_zero_r2 = tf.equal(abs_r2, zeros)\n  smallest_normal_preserving_sign_r2 = tf.bitwise.bitwise_or(\n      input_r2, smallest_normal_r2)\n  input_no_zeros_r2 = tf.where(if_zero_r2, smallest_normal_preserving_sign_r2,\n                               input_r2)\n\n  and_r2 = tf.bitwise.bitwise_and(input_no_zeros_r2, count_mask_r2)\n  or_r2 = tf.bitwise.bitwise_or(and_r2, iota)\n  return tf.bitcast(or_r2, tf.float32)\n\n\ndef create_topk_unique(inputs, k):\n  """"""Creates the top k values in sorted order with indices.""""""\n  height = inputs.shape[0]\n  width = inputs.shape[1]\n  neg_inf_r0 = tf.constant(-np.inf, dtype=tf.float32)\n  ones = tf.ones([height, width], dtype=tf.float32)\n  neg_inf_r2 = ones * neg_inf_r0\n  inputs = tf.where(tf.is_nan(inputs), neg_inf_r2, inputs)\n\n  tmp = inputs\n  topk_r2 = tf.zeros([height, k], dtype=tf.float32)\n  for i in range(k):\n    kth_order_statistic = tf.reduce_max(tmp, axis=1, keepdims=True)\n    k_mask = tf.tile(\n        tf.expand_dims(tf.equal(tf.range(k), tf.fill([k], i)), 0), [height, 1])\n    topk_r2 = tf.where(k_mask, tf.tile(kth_order_statistic, [1, k]), topk_r2)\n    ge_r2 = tf.greater_equal(inputs, tf.tile(kth_order_statistic, [1, width]))\n    tmp = tf.where(ge_r2, neg_inf_r2, inputs)\n\n  log2_ceiling = int(math.ceil(math.log(float(int(width)), 2)))\n  next_power_of_two = 1 << log2_ceiling\n  count_mask = next_power_of_two - 1\n  mask_r0 = tf.constant(count_mask)\n  mask_r2 = tf.fill([height, k], mask_r0)\n  topk_r2_s32 = tf.bitcast(topk_r2, tf.int32)\n  topk_indices_r2 = tf.bitwise.bitwise_and(topk_r2_s32, mask_r2)\n  return topk_r2, topk_indices_r2\n\n\ndef top_k_with_unique(inputs, k):\n  """"""Finds the values and indices of the k largests entries.""""""\n  unique_inputs = create_make_unique(tf.cast(inputs, tf.float32))\n  top_values, indices = create_topk_unique(unique_inputs, k)\n  top_values = tf.cast(top_values, inputs.dtype)\n  return top_values, indices\n\n\ndef compute_topk_scores_and_seq(sequences,\n                                scores,\n                                scores_to_gather,\n                                flags,\n                                beam_size,\n                                batch_size,\n                                prefix=""default"",\n                                states_to_gather=None):\n  """"""Given sequences and scores, will gather the top k=beam size sequences.""""""\n  _, topk_indexes = tf.nn.top_k(scores, k=beam_size)\n  batch_pos = compute_batch_indices(batch_size, beam_size)\n  top_coordinates = tf.stack([batch_pos, topk_indexes], axis=2)\n\n  def gather(tensor, name):\n    return tf.gather_nd(tensor, top_coordinates, name=(prefix + name))\n\n  topk_seq = gather(sequences, ""_topk_seq"")\n  topk_flags = gather(flags, ""_topk_flags"")\n  topk_gathered_scores = gather(scores_to_gather, ""_topk_scores"")\n  if states_to_gather:\n    topk_gathered_states = nest.map_structure(\n        lambda state: gather(state, ""_topk_states""), states_to_gather)\n  else:\n    topk_gathered_states = states_to_gather\n\n  return topk_seq, topk_gathered_scores, topk_flags, topk_gathered_states\n\n\ndef create_padding_mask(seq):\n  """"""\n  create padding mask\n  """"""\n  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n  # seq = tf.expand_dims(seq, 1)\n  # seq = tf.expand_dims(seq, 1)\n  # return seq\n\n\ndef create_masks(inp, tar):\n  """"""\n  create encode and decode mask\n  """"""\n  # encoder self-attention mask\n  enc_padding_mask = create_padding_mask(inp)\n  # decoder self-attention mask\n  dec_padding_mask = create_padding_mask(inp)\n\n  # decoder context mask\n  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n  dec_target_padding_mask = create_padding_mask(tar)\n  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n\n  return enc_padding_mask, combined_mask, dec_padding_mask\n\ndef create_look_ahead_mask(tar):\n  """"""\n  create look ahead mask for decode mask\n  """"""\n  size = tf.shape(tar)[1]\n  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n  dec_target_padding_mask = create_padding_mask(tar)\n  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n  return combined_mask  # (batch_size, 1, seq_len, seq_len)\n'"
delta/models/__init__.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Custom models.""""""\n'"
delta/models/asr_model.py,4,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' asr ctc model \'\'\'\nimport delta.compat as tf\n#pylint: disable=import-error,unused-import\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model\n\nfrom tensorflow.keras.layers import Bidirectional\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import TimeDistributed\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Lambda\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Input\n\nfrom absl import logging\n\n#delta\nfrom delta import utils\nfrom delta.utils.loss.loss_impl import CTCLoss as ctc_loss\nfrom delta.models.base_model import RawModel\nfrom delta.utils.register import registers\n\n#pylint: disable=invalid-name,missing-docstring\n\n\n@registers.model.register\nclass CTCAsrModel(RawModel):\n  \'\'\'\n  CTC ASR Model\n  reference: https://github.com/holm-aune-bachelor2018/ctc\n  \'\'\'\n\n  def __init__(self, config, name=None):\n    super().__init__(name=name)\n    self._config = config\n\n    logging.info(""--- dummy Task to get meta data ---"")\n    logging.info(""--- do not care the Task mode here ---"")\n    task = utils.task(config, mode=utils.TRAIN)\n    logging.info(""--- dummy Task to get meta data ---"")\n    logging.flush()\n\n    self._feat_shape = task.feat_shape\n    self._vocab_size = task.vocab_size\n\n    self.build()\n\n  @property\n  def feat_shape(self):\n    assert isinstance(self._feat_shape, (list))\n    return self._feat_shape\n\n  @property\n  def config(self):\n    return self._config\n\n  def get_loss_fn(self):\n    return ctc_loss(self._config)\n    #return utils.loss(self._config)\n\n  def ctc_lambda_func(self, args):\n    y_pred, input_length, labels, label_length = args\n    return self.get_loss_fn()(\n        logits=y_pred,\n        input_length=input_length,\n        labels=labels,\n        label_length=label_length,\n        name=\'ctc_loss\')\n    #return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n\n  def build(self):\n    input_tensor = Input(\n        name=\'inputs\', shape=(None, *self._feat_shape, 1), dtype=tf.float32)\n\n    x = input_tensor\n\n    x = Conv2D(\n        filters=32,\n        kernel_size=(11, 5),\n        use_bias=True,\n        activation=\'relu\',\n        padding=\'same\',\n        kernel_initializer=\'he_normal\',\n        name=""conv1"")(\n            x)\n\n    x = Conv2D(\n        filters=32,\n        kernel_size=(11, 5),\n        use_bias=True,\n        activation=\'relu\',\n        padding=\'same\',\n        kernel_initializer=\'he_normal\',\n        name=""conv2"")(\n            x)\n\n    _, _, dim, channels = x.get_shape().as_list()\n    output_dim = dim * channels\n    x = Reshape((-1, output_dim))(x)\n\n    x = TimeDistributed(Dropout(0.2))(x)\n    x = Bidirectional(\n        LSTM(\n            units=512,\n            kernel_initializer=\'glorot_uniform\',\n            bias_initializer=\'random_normal\',\n            return_sequences=True,\n            name=\'lstm\'))(\n                x)\n\n    x = TimeDistributed(Dropout(0.2))(x)\n    x = Bidirectional(\n        LSTM(\n            512,\n            kernel_initializer=\'glorot_uniform\',\n            bias_initializer=\'random_normal\',\n            return_sequences=True,\n            name=\'lstm1\'))(\n                x)\n\n    x = TimeDistributed(Dropout(0.2))(x)\n    x = Bidirectional(\n        LSTM(\n            512,\n            kernel_initializer=\'glorot_uniform\',\n            bias_initializer=\'random_normal\',\n            return_sequences=True,\n            name=\'lstm2\'))(\n                x)\n\n    x = TimeDistributed(Dropout(0.2))(x)\n    x = Bidirectional(\n        LSTM(\n            512,\n            kernel_initializer=\'glorot_uniform\',\n            bias_initializer=\'random_normal\',\n            return_sequences=True,\n            name=\'lstm3\'))(\n                x)\n\n    x = TimeDistributed(Dense(1024, activation=\'relu\'))(x)\n    x = TimeDistributed(Dropout(0.5))(x)\n\n    # Output layer with softmax\n    x = TimeDistributed(Dense(self._vocab_size), name=""outputs"")(x)\n\n    input_length = Input(name=\'input_length\', shape=[], dtype=\'int64\')\n    labels = Input(name=\'targets\', shape=[None], dtype=\'int32\')\n    label_length = Input(name=\'target_length\', shape=[], dtype=\'int64\')\n    loss_out = Lambda(\n        self.ctc_lambda_func, output_shape=(),\n        name=\'ctc\')([x, input_length, labels, label_length])\n\n    self._model = tf.keras.Model(\n        inputs=[input_tensor, labels, input_length, label_length],\n        outputs=[loss_out])\n\n  @property\n  def model(self):\n    return self._model\n\n  def call(self, inputs, **kwargs):\n    output = self.model(inputs, **kwargs)\n    return output\n\n\n@registers.model.register\nclass CTC5BlstmAsrModel(CTCAsrModel):\n  \'\'\'\n  CTC ASR Model\n  reference: https://www.cs.cmu.edu/~ymiao/pub/icassp2016_ctc.pdf\n  \'\'\'\n\n  def build(self):\n    input_tensor = Input(\n        name=\'inputs\', shape=(None, *self._feat_shape, 1), dtype=tf.float32)\n\n    x = input_tensor\n    _, _, dim, channels = x.get_shape().as_list()\n    output_dim = dim * channels\n    x = Reshape((-1, output_dim))(x)\n\n    x = Bidirectional(\n        LSTM(\n            units=320,\n            kernel_initializer=\'glorot_uniform\',\n            bias_initializer=\'random_normal\',\n            return_sequences=True,\n            name=\'lstm\'))(\n                x)\n\n    x = Bidirectional(\n        LSTM(\n            units=320,\n            kernel_initializer=\'glorot_uniform\',\n            bias_initializer=\'random_normal\',\n            return_sequences=True,\n            name=\'lstm1\'))(\n                x)\n\n    x = Bidirectional(\n        LSTM(\n            units=320,\n            kernel_initializer=\'glorot_uniform\',\n            bias_initializer=\'random_normal\',\n            return_sequences=True,\n            name=\'lstm2\'))(\n                x)\n\n    x = Bidirectional(\n        LSTM(\n            units=320,\n            kernel_initializer=\'glorot_uniform\',\n            bias_initializer=\'random_normal\',\n            return_sequences=True,\n            name=\'lstm3\'))(\n                x)\n\n    x = Bidirectional(\n        LSTM(\n            units=320,\n            kernel_initializer=\'glorot_uniform\',\n            bias_initializer=\'random_normal\',\n            return_sequences=True,\n            name=\'lstm4\'))(\n                x)\n\n    # Output layer with softmax\n    x = TimeDistributed(Dense(self._vocab_size), name=""outputs"")(x)\n\n    input_length = Input(name=\'input_length\', shape=[], dtype=\'int64\')\n    labels = Input(name=\'targets\', shape=[None], dtype=\'int32\')\n    label_length = Input(name=\'target_length\', shape=[], dtype=\'int64\')\n    loss_out = Lambda(\n        self.ctc_lambda_func, output_shape=(),\n        name=\'ctc\')([x, input_length, labels, label_length])\n\n    self._model = tf.keras.Model(\n        inputs=[input_tensor, labels, input_length, label_length],\n        outputs=[loss_out])\n'"
delta/models/base_model.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Base class for model.""""""\n\nimport re\nimport delta.compat as tf\nfrom tensorflow.python.keras import backend as K  # pylint: disable=no-name-in-module\n\n\nclass Model(tf.keras.Model):\n  """"""Base class for model.""""""\n\n  def __init__(self, **kwargs):  # pylint: disable=useless-super-delegation\n    super().__init__(**kwargs)\n\n  def __setattr__(self, key, value):\n    if key.startswith(""temp_""):\n      # this is for temporary attributes avoiding keras check\n      self.__dict__[key] = value\n    else:\n      super().__setattr__(key, value)\n\n  def call(self, inputs, training=None, mask=None):\n    raise NotImplementedError()\n\n\nclass RawModel:\n  """"""Raw model.""""""\n\n  def __init__(self, **kwargs):\n    name = kwargs.get(\'name\')\n    if not name:\n      prefix = self.__class__.__name__\n      name = self._to_snake_case(prefix) + \'_\' + str(K.get_uid(prefix))\n    self.name = name\n\n  @staticmethod\n  def _to_snake_case(name):\n    """"""Transform name to snake case.""""""\n    intermediate = re.sub(\'(.)([A-Z][a-z0-9]+)\', r\'\\1_\\2\', name)\n    insecure = re.sub(\'([a-z])([A-Z])\', r\'\\1_\\2\', intermediate).lower()\n    # If the class is private the name starts with ""_"" which is not secure\n    # for creating scopes. We prefix the name with ""private"" in this case.\n    if insecure[0] != \'_\':\n      return insecure\n    return \'private\' + insecure\n\n  def __call__(self, inputs, **kwargs):\n    with tf.variable_scope(self.name):\n      return self.call(inputs, **kwargs)\n\n  def call(self, inputs, **kwargs):\n    """"""call""""""\n    raise NotImplementedError()\n'"
delta/models/kws_model.py,5,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' tdnn kws model \'\'\'\nimport delta.compat as tf\n\nfrom delta.models.base_model import RawModel\n\nfrom delta.utils.register import registers\n\n\n@registers.model.register\nclass TdnnKwsModel(RawModel):\n  \'\'\' main model \'\'\'\n\n  def __init__(self, config, name=None):\n    super().__init__(name=name)\n    self.cfg = config\n    self.train = None\n\n  #pylint: disable=arguments-differ\n  def call(self, features, **kwargs):\n    self.train = kwargs[\'training\']\n    n_class = self.cfg[\'data\'][\'task\'][\'classes\'][\'num\']\n    return self.tdnn(features, n_class, self.train)\n\n  def tdnn(self, features, n_class, is_train):\n    \'\'\'\n        inp: (batch_size, window_len, feat_dim)\n    \'\'\'\n    inp = features[\'inputs\']\n    kernel_size = self.cfg[\'model\'][\'net\'][\'kernel_size\']\n    strides = self.cfg[\'model\'][\'net\'][\'strides\']\n    num_layers = self.cfg[\'model\'][\'net\'][\'num_layers\']\n    filters_num = inp.get_shape()[-1]\n\n    for i in range(num_layers):\n      output = tf.nn.relu(\n          tf.layers.conv1d(inp, filters_num, kernel_size, strides=strides))\n      output = tf.layers.batch_normalization(\n          output, training=is_train, name=\'bn%d\' % i)\n      inp = output\n\n    dim = output.get_shape()[1] * output.get_shape()[2]\n    output = tf.reshape(output, [-1, dim])\n\n    logits = tf.layers.dense(output, n_class)\n    return logits\n'"
delta/models/multimodal_cls_model.py,18,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' emotion keras model\'\'\'\nimport numpy as np\nfrom absl import logging\n\n#pylint: disable=no-name-in-module\nimport delta.compat as tf\nimport delta.layers\nfrom delta.models.base_model import Model\n\nfrom delta.utils.register import registers\nfrom delta.layers.utils import get_expand_pad_mask\nimport pickle\n\n@registers.model.register\nclass AlignClassModel(Model):\n  def __init__(self, config, **kwargs):\n    super().__init__(**kwargs)\n    self.config = config\n    config = self.config\n    model_config = config[\'model\'][\'net\'][\'structure\']\n    self.num_classes = config[\'data\'][\'task\'][\'classes\'][\'num\']\n    self.vocab_size = config[\'data\'][\'task\'][\'text\'][\'vocab_size\']\n    self.max_text_len = config[\'data\'][\'task\'][\'text\'][\'max_text_len\']\n    self.use_pretrained_embedding = config[\'model\'][\'use_pre_train_emb\']\n    self.embedding_size = model_config[\'embedding_size\']\n    self.hidden_dim = model_config[\'hidden_dim\']\n    self.head_num = model_config[\'head_num\']\n    self.inner_size = model_config[\'inner_size\']\n    self.dropout_rate = model_config[\'dropout_rate\']\n    self.speech_dropout_rate = model_config[\'speech_dropout_rate\']\n    self.padding_id = model_config.get(\'padding_id\', 0)\n    self.speech_dense_act = config.get(\'speech_dense_act\', \'relu\')\n\n    if self.use_pretrained_embedding:\n      self.embedding_path = config[\'model\'][\'embedding_path\']\n      logging.info(""Loading embedding file from: {}"".format(\n        self.embedding_path))\n      self._word_embedding_init = pickle.load(open(self.embedding_path, \'rb\'))\n      self.embed_initializer = tf.constant_initializer(\n        self._word_embedding_init)\n    else:\n      self.embed_initializer = tf.random_uniform_initializer(-0.1, 0.1)\n\n    self.embed = tf.keras.layers.Embedding(\n      self.vocab_size,\n      self.embedding_size,\n      embeddings_initializer=self.embed_initializer)\n    self.speech_enc_layer = delta.layers.RnnEncoder(config, name=""speech_encoder"")\n    self.text_enc_layer = delta.layers.RnnEncoder(config, name=""text_encoder"")\n\n    self.align_attn_layer = delta.layers.MultiHeadAttention(\n      self.hidden_dim, self.head_num)\n    self.align_enc_layer = delta.layers.RnnAttentionEncoder(\n      config, name=""align_encoder"")\n\n    self.embed_d = tf.keras.layers.Dropout(self.dropout_rate)\n    self.speech_d = tf.keras.layers.Dropout(self.speech_dropout_rate)\n    self.speech_enc_d = tf.keras.layers.Dropout(self.speech_dropout_rate)\n    self.text_enc_d = tf.keras.layers.Dropout(self.dropout_rate)\n    self.attn_enc_d = tf.keras.layers.Dropout(self.dropout_rate)\n    self.align_enc_d = tf.keras.layers.Dropout(self.dropout_rate)\n    self.final_dense = tf.keras.layers.Dense(\n      self.num_classes,\n      activation=tf.keras.activations.linear)\n\n    self.align_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.speech_dense = tf.keras.layers.Dense(\n      512, activation=self.speech_dense_act)\n\n  def build(self, input_shape):\n    logging.info(f""{self.__class__.__name__} input_shape : {input_shape}"")\n    _, time, feat, channels = input_shape[\'inputs\'].as_list()\n    self.speech = tf.keras.layers.InputLayer(\n      input_shape=(None, time, feat, channels), name=""speech"")\n    self.text = tf.keras.layers.InputLayer(\n      input_shape=(None, self.max_text_len), name=""text"")\n    self.reshape1 = tf.keras.layers.Reshape(\n      (time, feat * channels), input_shape=(time, feat, channels))\n    self.concatenate = tf.keras.layers.Concatenate()\n    self.reshape2 = tf.keras.layers.Reshape(\n      (self.max_text_len, self.hidden_dim),\n      input_shape=(-1, self.hidden_dim))\n    self.built = True\n\n  def call(self, inputs, training=None, mask=None):\n    #logging.info(f""xxxx input: {inputs}, training: {training}"")\n    speechs = inputs[\'inputs\']\n    speechs = self.speech(speechs)\n    texts = inputs[\'texts\']\n    texts = self.text(texts)\n    speechs = self.reshape1(speechs)\n    speechs = self.speech_dense(speechs)\n    speechs = self.speech_d(speechs)\n\n    texts_mask = get_expand_pad_mask(\n      texts, self.padding_id)\n\n    text_embed = self.embed(texts)\n    text_embed = self.embed_d(text_embed, training=training)\n\n    text_enc, _ = self.text_enc_layer(text_embed,\n                                      training=training,\n                                      mask=texts_mask)\n\n    text_enc = self.text_enc_d(text_enc, training=training)\n\n    speech_enc, _ = self.speech_enc_layer(speechs,\n                                          training=training)\n    speech_enc = self.speech_enc_d(speech_enc, training=training)\n\n    attn_outs, _ = self.align_attn_layer(\n      (text_enc, speech_enc, speech_enc),\n      training=training)\n    attn_outs = self.attn_enc_d(attn_outs, training=training)\n    attn_outs = self.reshape2(attn_outs)\n\n    attn_outs = self.concatenate([attn_outs, text_enc])\n    align_enc = self.align_enc_layer(attn_outs,\n                                     training=training,\n                                     mask=texts_mask)\n\n    align_enc = self.align_enc_d(align_enc, training=training)\n\n    scores = self.final_dense(align_enc)\n    return scores\n'"
delta/models/resnet_model.py,1,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' resnet keras model\'\'\'\nfrom absl import logging\n\n#pylint: disable=no-name-in-module\nimport delta.compat as tf\nfrom tensorflow.python.keras import backend as K\n\nfrom delta.models.base_model import Model\nfrom delta.layers.resnet import IdentityBlock\nfrom delta.layers.resnet import ConvBlock\n\nfrom delta.utils.register import registers\n\n#pylint: disable=invalid-name\n#pylint: disable=attribute-defined-outside-init\n#pylint: disable=missing-docstring\n#pylint: disable=too-many-instance-attributes\n#pylint: disable=attribute-defined-outside-init\n#pylint: disable=too-many-ancestors\n\nlayers = tf.keras.layers\n\n\n@registers.model.register\nclass ResNet50(Model):\n\n  def __init__(self, config, **kwargs):\n    """"""Instantiates the ResNet50 architecture.\n    Optionally loads weights pre-trained on ImageNet.\n    Note that the data format convention used by the model is\n    the one specified in your Keras config at `~/.keras/keras.json`.\n    # Arguments\n        include_top: whether to include the fully-connected\n            layer at the top of the network.\n        pooling: Optional pooling mode for feature extraction\n            when `include_top` is `False`.\n            - `None` means that the output of the model will be\n                the 4D tensor output of the\n                last convolutional block.\n            - `avg` means that global average pooling\n                will be applied to the output of the\n                last convolutional block, and thus\n                the output of the model will be a 2D tensor.\n            - `max` means that global max pooling will\n                be applied.\n        classes: optional number of classes to classify images\n            into, only to be specified if `include_top` is True, and\n            if no `weights` argument is specified.\n    # Returns\n        A Keras model instance.\n    # Raises\n        ValueError: in case of invalid argument for `weights`,\n            or invalid input shape.\n    # Reference\n        https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet50.py\n    """"""\n    super().__init__(**kwargs)\n    classes = config[\'data\'][\'task\'][\'classes\'][\'num\']\n    self.include_top = True\n    self.pooling = \'avg\'\n\n    if K.image_data_format() == \'channels_last\':\n      bn_axis = 3\n    else:\n      bn_axis = 1\n\n    self.zero_pad1 = layers.ZeroPadding2D(padding=(3, 3), name=\'conv1_pad\')\n    self.conv1 = layers.Conv2D(\n        64, (7, 7),\n        strides=(2, 2),\n        padding=\'valid\',\n        kernel_initializer=\'he_normal\',\n        name=\'conv1\')\n    self.bn1 = layers.BatchNormalization(axis=bn_axis, name=\'bn_conv1\')\n    self.act1 = layers.Activation(\'relu\')\n    self.zero_pad2 = layers.ZeroPadding2D(padding=(1, 1), name=\'pool1_pad\')\n    self.max_pool1 = layers.MaxPooling2D((3, 3), strides=(2, 2))\n\n    self.conv_block1a = ConvBlock(\n        3, [64, 64, 256], stage=2, block=\'a\', strides=(1, 1))\n    self.identity_block1b = IdentityBlock(3, [64, 64, 256], stage=2, block=\'b\')\n    self.identity_block1c = IdentityBlock(3, [64, 64, 256], stage=2, block=\'c\')\n\n    self.conv_block2a = ConvBlock(3, [128, 128, 512], stage=3, block=\'a\')\n    self.identity_block2b = IdentityBlock(\n        3, [128, 128, 512], stage=3, block=\'b\')\n    self.identity_block2c = IdentityBlock(\n        3, [128, 128, 512], stage=3, block=\'c\')\n    self.identity_block2d = IdentityBlock(\n        3, [128, 128, 512], stage=3, block=\'d\')\n\n    self.conv_block3a = ConvBlock(3, [256, 256, 1024], stage=4, block=\'a\')\n    self.identity_block3b = IdentityBlock(\n        3, [256, 256, 1024], stage=4, block=\'b\')\n    self.identity_block3c = IdentityBlock(\n        3, [256, 256, 1024], stage=4, block=\'c\')\n    self.identity_block3d = IdentityBlock(\n        3, [256, 256, 1024], stage=4, block=\'d\')\n    self.identity_block3e = IdentityBlock(\n        3, [256, 256, 1024], stage=4, block=\'e\')\n    self.identity_block3f = IdentityBlock(\n        3, [256, 256, 1024], stage=4, block=\'f\')\n\n    self.conv_block4a = ConvBlock(3, [512, 512, 2048], stage=5, block=\'a\')\n    self.identity_block4b = IdentityBlock(\n        3, [512, 512, 2048], stage=5, block=\'b\')\n    self.identity_block4c = IdentityBlock(\n        3, [512, 512, 2048], stage=5, block=\'c\')\n\n    self.global_avg_pool = layers.GlobalAveragePooling2D(name=\'avg_pool\')\n    self.dense = layers.Dense(classes, activation=\'softmax\', name=\'fc-class\')\n\n  #pylint: disable=arguments-differ\n  def call(self, input_tensor, training=None, mask=None):\n    logging.info(f""input: {input_tensor}"")\n    if isinstance(input_tensor, (tuple, list)):\n      x = input_tensor[0]\n    elif isinstance(input_tensor, dict):\n      x = input_tensor[\'inputs\']\n    else:\n      x = input_tensor\n\n    x = self.zero_pad1(x)\n    x = self.conv1(x)\n    x = self.bn1(x)\n    x = self.act1(x)\n    x = self.zero_pad2(x)\n    x = self.max_pool1(x)\n\n    x = self.conv_block1a(x)\n    x = self.identity_block1b(x)\n    x = self.identity_block1c(x)\n\n    x = self.conv_block2a(x)\n    x = self.identity_block2b(x)\n    x = self.identity_block2c(x)\n    x = self.identity_block2d(x)\n\n    x = self.conv_block3a(x)\n    x = self.identity_block3b(x)\n    x = self.identity_block3c(x)\n    x = self.identity_block3d(x)\n    x = self.identity_block3e(x)\n    x = self.identity_block3f(x)\n\n    x = self.conv_block4a(x)\n    x = self.identity_block4b(x)\n    x = self.identity_block4c(x)\n\n    x = self.global_avg_pool(x)\n    x = self.dense(x)\n    return x\n'"
delta/models/speaker_cls_rawmodel.py,94,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\'\nA series of models for speaker classification.\n\'\'\'\nimport math\nfrom absl import logging\nimport delta.compat as tf\nimport tensorflow.keras.layers as keras_layers\n\nfrom delta import utils\nfrom delta.layers import common_layers\nfrom delta.models.base_model import RawModel\nfrom delta.utils.register import registers\nfrom delta.utils.loss.loss_utils import arcface_loss\nfrom delta.utils.hparam import HParams\n\n#pylint: disable=invalid-name\n#pylint: disable=too-many-locals\n#pylint: disable=too-many-instance-attributes\n#pylint: disable=arguments-differ\n\n\nclass SpeakerBaseRawModel(RawModel):\n  \'\'\'\n  Base class for speaker models.\n  \'\'\'\n\n  def __init__(self, config, name=None):\n    super().__init__(name=name)\n    self.config = config\n\n    self.netconf = self.config[\'model\'][\'net\'][\'structure\']\n    self.taskconf = self.config[\'data\'][\'task\']\n    self.audioconf = self.taskconf[\'audio\']\n\n    self.attention = self.netconf[\'attention\']\n    frame_per_sec = 1 / self.taskconf[\'audio\'][\'winstep\']\n    self.input_len = self.taskconf[\'audio\'][\'clip_size\'] * frame_per_sec\n    self.input_type = \'samples\' if self.taskconf[\n        \'suffix\'] == \'.wav\' else \'features\'\n    self.input_channels = 3 if self.taskconf[\'audio\'][\'add_delta_deltas\'] else 1\n\n    # l2\n    self._extra_train_ops = []\n\n    # internal parameters\n    self.feature_params = None\n    self.mean = None\n    self.std = None\n    self.train = None\n\n  def preprocess(self, inputs):\n    \'\'\' Speech preprocessing. \'\'\'\n    with tf.variable_scope(\'feature\'):\n      if self.input_type == \'samples\':\n        # FIXME: stub\n        feats = None\n      else:\n        if \'cmvn_type\' in self.audioconf:\n          cmvn_type = self.audioconf[\'cmvn_type\']\n        else:\n          cmvn_type = \'global\'\n        logging.info(\'cmvn_type: %s\' % (cmvn_type))\n        if cmvn_type == \'global\':\n          self.mean, self.std = utils.load_cmvn(self.audioconf[\'cmvn_path\'])\n          feats = utils.apply_cmvn(inputs, self.mean, self.std)\n        elif cmvn_type == \'local\':\n          feats = utils.apply_local_cmvn(inputs)\n        elif cmvn_type == \'sliding\':\n          raise ValueError(\'cmvn_type %s not implemented yet.\' % (cmvn_type))\n        elif cmvn_type == \'none\':\n          feats = inputs\n        else:\n          raise ValueError(\'Error cmvn_type %s.\' % (cmvn_type))\n    return feats\n\n  def call(self, features, **kwargs):\n    \'\'\' Implementation of __call__(). \'\'\'\n    self.train = kwargs[\'training\']\n    feats = tf.identity(features[\'inputs\'], name=\'feats\')\n    logging.info(features)\n    if \'labels\' in features:\n      labels = features[\'labels\']\n    else:\n      # serving export mode\n      labels = None\n\n    with tf.variable_scope(\'model\', reuse=tf.AUTO_REUSE):\n      feats = self.preprocess(feats)\n      logits = self.model(feats, labels)\n    return logits\n\n  def model(self, feats, labels):\n    \'\'\' Stub function. \'\'\'\n    return NotImplementedError(\'Stub function.\')\n\n  def linear_block(self, x):\n    \'\'\'\n    linear layer for dim reduction\n    x: shape [batch, time, feat, channel]\n    output: shape [b, t, f]\n    \'\'\'\n    batch_t = tf.shape(x)[0]\n    time_t = tf.shape(x)[1]\n    feat, channel = x.shape.as_list()[2:]\n    linear_num = self.netconf[\'linear_num\']\n\n    if linear_num > 0:\n      with tf.variable_scope(\'linear\'):\n        x = tf.reshape(x, [batch_t * time_t, feat * channel])\n\n        if self.netconf[\'use_dropout\']:\n          x = tf.layers.dropout(\n              x, self.netconf[\'dropout_rate\'], training=self.train)\n\n        x = common_layers.linear(x, \'linear1\', [feat * channel, linear_num])\n\n        x = tf.nn.relu(x)\n\n        if self.netconf[\'use_bn\']:\n          bn_name = \'bn_linear\'\n          x = tf.layers.batch_normalization(\n              x, axis=-1, momentum=0.9, training=self.train, name=bn_name)\n\n        x = tf.reshape(x, [batch_t, time_t, linear_num])\n    else:\n      logging.info(\'linear_num <= 0, only apply reshape.\')\n      x = tf.reshape(x, [batch_t, time_t, feat * channel])\n\n    return x\n\n  def lstm_layer(self, x):\n    \'\'\' LSTM layers. \'\'\'\n    if self.netconf[\'use_lstm_layer\']:\n      with tf.variable_scope(\'lstm\'):\n        cell_fw = tf.nn.rnn_cell.BasicLSTMCell(\n            self.netconf[\'cell_num\'], forget_bias=1.0)\n        if self.netconf[\'use_dropout\']:\n          cell_fw = tf.nn.RNNCellDropoutWrapper(\n              cell=cell_fw,\n              output_keep_prob=1 -\n              self.netconf[\'dropout_rate\'] if self.train else 1.0)\n\n        cell_bw = tf.nn.rnn_cell.BasicLSTMCell(\n            self.netconf[\'cell_num\'], forget_bias=1.0)\n        if self.netconf[\'use_dropout\']:\n          cell_bw = tf.nn.RNNCellDropoutWrapper(\n              cell=cell_bw,\n              output_keep_prob=1 -\n              self.netconf[\'dropout_rate\'] if self.train else 1.0)\n\n        # Now we feed `linear` into the LSTM BRNN cell and obtain the LSTM BRNN output.\n        outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n            cell_fw=cell_fw,\n            cell_bw=cell_bw,\n            inputs=x,\n            dtype=tf.float32,\n            time_major=False,\n            scope=\'LSTM1\')\n    else:\n      outputs = x\n    return outputs\n\n  def pooling_layer(self, x, pooling_type=None):\n    \'\'\'\n      Add a pooling layer across the whole utterance.\n      Input: [B, T, D]\n        --> Reduce along T\n\n      Statistics pooling output: [B, D * 2]\n      Average pooling output: [B, D]\n    \'\'\'\n    assert_rank3 = tf.debugging.assert_rank(x, 3)\n    with tf.control_dependencies([assert_rank3]):\n      x = tf.identity(x)\n\n    pooling_type = pooling_type if pooling_type else self.netconf[\n        \'frame_pooling_type\']\n    if pooling_type == \'stats\':\n      with tf.name_scope(\'stats_pooling\'):\n        mean, var = tf.nn.moments(x, 1)\n        x = tf.concat([mean, tf.sqrt(var + 1e-6)], 1)\n    elif pooling_type == \'average\':\n      with tf.name_scope(\'average_pooling\'):\n        mean, _ = tf.nn.moments(x, 1)\n        x = mean\n    else:\n      raise ValueError(\'Unsupported frame_pooling_type: %s\' % (pooling_type))\n\n    assert_rank2 = tf.debugging.assert_rank(x, 2)\n    with tf.control_dependencies([assert_rank2]):\n      x = tf.identity(x)\n\n    return x\n\n  def dense_layer(self, x):\n    \'\'\' Embedding layers. \'\'\'\n    with tf.variable_scope(\'dense\'):\n      shape = x.shape[-1].value\n      hidden_dims = self.netconf[\'hidden_dims\']\n      y = x\n      use_bn = self.netconf[\'use_bn\']\n      remove_nonlin = self.netconf[\'remove_last_nonlinearity\']\n\n      for idx, hidden in enumerate(hidden_dims):\n        last_layer = idx == (len(hidden_dims) - 1)\n        layer_add_nonlin = not last_layer or not remove_nonlin\n        y = common_layers.linear(\n            y,\n            \'dense-matmul-%d\' % (idx + 1), [shape, hidden],\n            has_bias=(layer_add_nonlin or not use_bn))\n        shape = hidden\n        embedding = y\n        if layer_add_nonlin:\n          y = tf.nn.relu(y)\n        if use_bn:\n          y = tf.layers.batch_normalization(\n              y,\n              axis=-1,\n              momentum=0.99,\n              training=self.train,\n              name=\'dense-bn-%d\' % (idx + 1))\n        if self.netconf[\'use_dropout\'] and layer_add_nonlin:\n          y = tf.layers.dropout(\n              y, self.netconf[\'dropout_rate\'], training=self.train)\n      if self.netconf[\'embedding_after_linear\']:\n        logging.info(\'Output embedding right after linear layer.\')\n      else:\n        logging.info(\'Output embedding after non-lin, batch norm and dropout.\')\n        embedding = y\n    return embedding, y\n\n  def arcface_layer(self, inputs, labels, output_num, weights):\n    \'\'\' ArcFace layer. \'\'\'\n    params = self.netconf[\'arcface_params\']\n    s = params[\'scale\']\n    m = params[\'margin\']\n    limit_to_pi = params[\'limit_to_pi\']\n    return arcface_loss(\n        inputs, labels, output_num, weights, s=s, m=m, limit_to_pi=limit_to_pi)\n\n  def logits_layer(self, x, labels):\n    \'\'\' Logits layer to further produce softmax. \'\'\'\n    if labels is None:\n      # serving export mode, no need for logits\n      return x\n\n    output_num = self.taskconf[\'classes\'][\'num\']\n    logits_type = self.netconf[\'logits_type\']\n    logits_shape = [x.shape[-1].value, output_num]\n\n    with tf.variable_scope(\'logits\'):\n      init_type = self.netconf[\'logits_weight_init\'][\'type\']\n      if init_type == \'truncated_normal\':\n        stddev = self.netconf[\'logits_weight_init\'][\'stddev\']\n        init = tf.truncated_normal_initializer(stddev=stddev)\n      elif init_type == \'xavier_uniform\':\n        init = tf.initializers.glorot_uniform()\n      elif init_type == \'xavier_norm\':\n        init = tf.initializers.glorot_normal()\n      else:\n        raise ValueError(\'Unsupported weight init type: %s\' % (init_type))\n\n      weights = tf.get_variable(\n          name=\'weights\', shape=logits_shape, initializer=init)\n\n      if logits_type == \'linear\':\n        bias = tf.get_variable(\n            name=\'bias\',\n            shape=logits_shape[1],\n            initializer=tf.constant_initializer(0.0))\n        return tf.matmul(x, weights) + bias\n      elif logits_type == \'linear_no_bias\':\n        return tf.matmul(x, weights)\n      elif logits_type == \'arcface\':\n        return self.arcface_layer(x, labels, output_num, weights)\n\n\n@registers.model.register\nclass SpeakerCRNNRawModel(SpeakerBaseRawModel):\n  \'\'\' A speaker model with simple 2D conv layers. \'\'\'\n\n  def model(self, feats, labels):\n    \'\'\' Build the model. \'\'\'\n    x, _ = self.conv_block(feats, depthwise=False)\n    x = self.linear_block(x)\n    x = self.lstm_layer(x)\n    x = self.pooling_layer(x)\n    embedding, dense_output = self.dense_layer(x)\n    logits = self.logits_layer(dense_output, labels)\n    model_outputs = {\'logits\': logits, \'embeddings\': embedding}\n    return model_outputs\n\n  def conv_block(self, inputs, depthwise=False):\n    \'\'\' 2D conv layers. \'\'\'\n    filters = self.netconf[\'filters\']\n    logging.info(""filters : {}"".format(filters))\n    filters_size = self.netconf[\'filter_size\']\n    logging.info(""filters_size : {}"".format(filters_size))\n    filters_strides = self.netconf[\'filter_stride\']\n    logging.info(""filters_strides : {}"".format(filters_strides))\n    pools_size = self.netconf[\'pool_size\']\n    logging.info(""pools_size : {}"".format(pools_size))\n\n    layer_num = len(filters)\n    assert layer_num == len(filters_size)\n    assert layer_num == len(filters_strides)\n    assert layer_num == len(pools_size)\n\n    channels = [self.input_channels] + filters\n    logging.info(""channels : {}"".format(channels))\n\n    downsample_input_len = self.input_len\n    with tf.variable_scope(\'cnn\'):\n      x = tf.identity(inputs)\n      for index, filt in enumerate(filters):\n        unit_name = \'unit-\' + str(index + 1)\n        with tf.variable_scope(unit_name):\n          if depthwise:\n            x = tf.layers.separable_conv2d(\n                x,\n                filters=filt,\n                kernel_size=filters_size[index],\n                strides=filters_strides[index],\n                padding=\'same\',\n                name=unit_name)\n          else:\n            cnn_name = \'cnn-\' + str(index + 1)\n            x = common_layers.conv2d(x, cnn_name, filters_size[index],\n                                     channels[index], channels[index + 1],\n                                     filters_strides[index])\n          x = tf.nn.relu(x)\n          if self.netconf[\'use_bn\']:\n            bn_name = \'bn\' + str(index + 1)\n            x = tf.layers.batch_normalization(\n                x, axis=-1, momentum=0.9, training=self.train, name=bn_name)\n          if self.netconf[\'use_dropout\']:\n            x = tf.layers.dropout(\n                x, self.netconf[\'dropout_rate\'], training=self.train)\n          x = common_layers.max_pool(x, pools_size[index], pools_size[index])\n          downsample_input_len = downsample_input_len / pools_size[index][0]\n\n    return x, downsample_input_len\n\n\n@registers.model.register\nclass SpeakerTDNNRawModel(SpeakerBaseRawModel):\n  \'\'\' A speaker model with TDNN layers. \'\'\'\n\n  def model(self, feats, labels):\n    \'\'\' Build the model. \'\'\'\n    x, _ = self.tdnn_block(feats)\n    x = self.pooling_layer(x)\n    embedding, dense_output = self.dense_layer(x)\n    logits = self.logits_layer(dense_output, labels)\n    model_outputs = {\'logits\': logits, \'embeddings\': embedding}\n    return model_outputs\n\n  def tdnn_block(self, inputs):\n    \'\'\' TDNN layers. \'\'\'\n    if \'tdnn_method\' in self.netconf:\n      tdnn_method = self.netconf[\'tdnn_method\']\n    else:\n      # Runs faster, support discrete context, for now.\n      tdnn_method = \'splice_layer\'\n    tdnn_contexts = self.netconf[\'tdnn_contexts\']\n    logging.info(""tdnn_contexts : {}"".format(tdnn_contexts))\n    tdnn_dims = self.netconf[\'tdnn_dims\']\n    logging.info(""tdnn_dims : {}"".format(tdnn_dims))\n\n    layer_num = len(tdnn_contexts)\n    assert layer_num == len(tdnn_dims)\n\n    channels = [self.input_channels] + tdnn_dims\n    logging.info(""tdnn_channels : {}"".format(channels))\n\n    input_h_t = tf.shape(inputs)[1]\n    input_w = inputs.shape[2]\n    input_c = inputs.shape[3]\n    if tdnn_method == \'conv1d\':\n      # NHWC -> NW\'C, W\' = H * W\n      inputs = tf.reshape(inputs, [-1, input_h_t * input_w, input_c])\n      last_w = channels[0]\n    else:\n      inputs = tf.reshape(inputs, [-1, input_h_t, input_w * input_c])\n      last_w = input_w * input_c\n\n    downsample_input_len = self.input_len\n    with tf.variable_scope(\'tdnn\'):\n      x = tf.identity(inputs)\n      for index in range(layer_num):\n        unit_name = \'unit-\' + str(index + 1)\n        with tf.variable_scope(unit_name):\n          tdnn_name = \'tdnn-\' + str(index + 1)\n          x = common_layers.tdnn(\n              x,\n              tdnn_name,\n              last_w,\n              tdnn_contexts[index],\n              channels[index + 1],\n              has_bias=True,\n              method=tdnn_method)\n          last_w = channels[index + 1]\n          x = tf.nn.relu(x)\n          if self.netconf[\'use_bn\']:\n            bn_name = \'bn\' + str(index + 1)\n            x = tf.layers.batch_normalization(\n                x, axis=-1, momentum=0.9, training=self.train, name=bn_name)\n          if self.netconf[\'use_dropout\']:\n            x = tf.layers.dropout(\n                x, self.netconf[\'dropout_rate\'], training=self.train)\n          downsample_input_len = downsample_input_len\n\n    return x, downsample_input_len\n\n\n@registers.model.register\nclass SpeakerResNetRawModel(SpeakerBaseRawModel):\n  \'\'\' A speaker model with ResNet layers. \'\'\'\n\n  @classmethod\n  def params(cls, config: dict = None):\n    embedding_size = 512\n\n    #hp = HParams(cls=cls)\n    hp = HParams(cls=cls)\n    hp.add_hparam(\'embedding_size\', embedding_size)\n\n    if config is not None:\n      hp.override_from_dict(config)\n    return hp\n\n  def model(self, feats, labels):\n    \'\'\' Build the model. \'\'\'\n    x = self.resnet(feats)\n\n    with tf.variable_scope(""avg_pooling""):\n      batch_t = tf.shape(x)[0]\n      time_t = tf.shape(x)[1]\n      feat, channel = x.shape.as_list()[2:]\n      x = tf.reshape(x, [batch_t, time_t, feat * channel])\n      x = self.pooling_layer(x, pooling_type=\'average\')\n\n    with tf.variable_scope(""output_layer""):\n      shape = x.shape.as_list()\n      shape = shape[-1]\n      hidden_dims = self.params().embedding_size\n      y = x\n      y = common_layers.linear(\n          y, \'dense-matmul\', [shape, hidden_dims], has_bias=True)\n      y = tf.layers.batch_normalization(\n          y, axis=-1, momentum=0.99, training=self.train, name=\'dense-bn\')\n      embedding = y\n      dense_output = y\n\n    logits = self.logits_layer(dense_output, labels)\n    model_outputs = {\'logits\': logits, \'embeddings\': embedding}\n    return model_outputs\n\n  def bn_layer(self, x, bn_name):\n    x = tf.layers.batch_normalization(\n        x, axis=-1, momentum=0.9, training=self.train, name=bn_name)\n    return x\n\n  def prelu_layer(self, x, name, num_parameters=1, init=0.25):\n    if num_parameters == 1:\n      shape = 1\n    else:\n      shape = x.get_shape()[-1]\n    alpha = tf.get_variable(\n        name,\n        shape=shape,\n        dtype=x.dtype,\n        initializer=tf.constant_initializer(init))\n    return tf.maximum(0.0, x) + alpha * tf.minimum(0.0, x)\n\n  def se_moudle(self, x, channels, reduction, name=\'\'):\n    input_t = x\n    x = tf.reduce_mean(x, [1, 2], name=name + \'_avg\', keep_dims=True)\n    x = tf.layers.conv2d(\n        x,\n        channels // reduction, (1, 1),\n        use_bias=False,\n        name=name + \'_1x1_down\',\n        strides=(1, 1),\n        padding=\'valid\',\n        data_format=\'channels_last\',\n        activation=None,\n        kernel_initializer=tf.initializers.glorot_uniform(),\n        bias_initializer=tf.zeros_initializer())\n    x = tf.nn.relu(x, name=name + \'_1x1_down_relu\')\n\n    x = tf.layers.conv2d(\n        x,\n        channels, (1, 1),\n        use_bias=False,\n        name=name + \'_1x1_up\',\n        strides=(1, 1),\n        padding=\'valid\',\n        data_format=\'channels_last\',\n        activation=None,\n        kernel_initializer=tf.initializers.glorot_uniform(),\n        bias_initializer=tf.zeros_initializer())\n    x = tf.nn.sigmoid(x, name=name + \'_1x1_up_sigmoid\')\n    return tf.multiply(input_t, x, name=name + \'_mul\')\n\n  def resnet_layer(self, x, in_channel, out_channel, stride, dim_match,\n                   block_name):\n    conv_name_base = \'res\' + block_name + \'_branch\'\n    bn_name_base = \'bn\' + block_name + \'_branch\'\n    prelu_name_base = \'prelu\' + block_name + \'_branch\'\n\n    short_cut = x\n    if not dim_match:\n      short_cut = common_layers.conv2d(\n          short_cut,\n          conv_name_base + \'1\',\n          filter_size=(1, 1),\n          in_channels=in_channel,\n          out_channels=out_channel,\n          strides=stride,\n          bias=False)\n      short_cut = tf.layers.batch_normalization(\n          short_cut,\n          axis=-1,\n          momentum=0.9,\n          training=self.train,\n          name=bn_name_base + \'1\')\n\n    x = tf.layers.batch_normalization(\n        x, axis=-1, momentum=0.9, training=self.train, name=bn_name_base + \'2a\')\n    x = common_layers.conv2d(\n        x,\n        conv_name_base + \'2a\', (3, 3),\n        in_channel,\n        out_channel, [1, 1],\n        bias=False)\n    x = tf.layers.batch_normalization(\n        x, axis=-1, momentum=0.9, training=self.train, name=bn_name_base + \'2b\')\n    x = self.prelu_layer(x, name=prelu_name_base + \'2b\')\n    x = common_layers.conv2d(\n        x,\n        conv_name_base + \'2b\', (3, 3),\n        out_channel,\n        out_channel,\n        stride,\n        bias=False)\n    res = tf.layers.batch_normalization(\n        x, axis=-1, momentum=0.9, training=self.train, name=bn_name_base + \'2c\')\n\n    return tf.add(short_cut, res, name=\'add_\' + block_name)\n\n  def se_resnet_layer(self, x, in_channel, out_channel, stride, dim_match,\n                      block_name):\n    conv_name_base = \'res_\' + block_name + \'_branch\'\n    bn_name_base = \'bn_\' + block_name + \'_branch\'\n    prelu_name_base = \'prelu_\' + block_name + \'_branch\'\n    se_name_base = \'se_\' + block_name + \'_branch\'\n\n    short_cut = x\n    if not dim_match:\n      short_cut = common_layers.conv2d(short_cut, conv_name_base + \'1\', (1, 1),\n                                       in_channel, out_channel, stride)\n      short_cut = tf.layers.batch_normalization(\n          short_cut,\n          axis=-1,\n          momentum=0.9,\n          training=self.train,\n          name=bn_name_base + \'1\')\n    x = tf.layers.batch_normalization(\n        x, axis=-1, momentum=0.9, training=self.train, name=bn_name_base + \'2a\')\n    x = common_layers.conv2d(x, conv_name_base + \'2a\', (3, 3), in_channel,\n                             out_channel, [1, 1])\n    x = tf.layers.batch_normalization(\n        x, axis=-1, momentum=0.9, training=self.train, name=bn_name_base + \'2b\')\n    x = self.prelu_layer(x, name=prelu_name_base + \'2b\')\n    x = common_layers.conv2d(x, conv_name_base + \'2b\', (3, 3), out_channel,\n                             out_channel, stride)\n    x = tf.layers.batch_normalization(\n        x, axis=-1, momentum=0.9, training=self.train, name=bn_name_base + \'2c\')\n    res = self.se_moudle(x, out_channel, 16, name=se_name_base)\n\n    return tf.add(short_cut, res, name=\'add_\' + block_name)\n\n  def resnet_block(self, x, block_mode, layer_num, in_channel, out_channel,\n                   stride):\n    if block_mode == \'ir\':\n      block = self.resnet_layer\n    elif block_mode == \'ir_se\':\n      block = self.se_resnet_layer\n\n    x = block(\n        x, in_channel, out_channel, stride, dim_match=False, block_name=\'a\')\n    for i in range(1, layer_num):\n      x = block(\n          x,\n          out_channel,\n          out_channel, [1, 1],\n          dim_match=True,\n          block_name=chr(ord(\'a\') + i))\n\n    return x\n\n  def resnet(self, inputs):\n    \'\'\' resnet_block. \'\'\'\n    layers_list = self.netconf[\'layers_list\']\n    logging.info(""layers_list : {}"".format(layers_list))\n    filters_list = self.netconf[\'filters_list\']\n    logging.info(""filters_list : {}"".format(filters_list))\n    strides_list = self.netconf[\'strides_list\']\n    logging.info(""strides_list : {}"".format(strides_list))\n    block_mode = self.netconf[\'block_mode\']\n    logging.info(""block_mode : {}"".format(block_mode))\n\n    with tf.variable_scope(\'resnet\'):\n      x = tf.identity(inputs)\n      with tf.variable_scope(\'input_layer\'):\n        x = common_layers.conv2d(\n            x,\n            \'input_conv\', (3, 3),\n            self.input_channels,\n            filters_list[0], [1, 1],\n            bias=False)\n        x = tf.layers.batch_normalization(\n            x, axis=-1, momentum=0.9, training=self.train, name=\'input_bn\')\n        x = self.prelu_layer(x, \'input_prelu\')\n\n      for index, layer_num in enumerate(layers_list):\n        unit_name = \'resblock-\' + str(index + 1)\n        with tf.variable_scope(unit_name):\n          x = self.resnet_block(x, block_mode, layer_num, filters_list[index],\n                                filters_list[index + 1], strides_list[index])\n\n    return x\n'"
delta/models/speech_cls_model.py,41,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' emotion keras model\'\'\'\nimport numpy as np\nfrom absl import logging\n\n#pylint: disable=no-name-in-module\nimport delta.compat as tf\nfrom tensorflow.python.keras import backend as K\n\nfrom delta.models.base_model import Model\nfrom delta.layers.base_layer import Layer\n\nfrom delta import utils\nfrom delta.utils.register import registers\n\n#pylint: disable=invalid-name\n#pylint: disable=attribute-defined-outside-init\n#pylint: disable=missing-docstring\n#pylint: disable=too-many-instance-attributes\n#pylint: disable=attribute-defined-outside-init\n#pylint: disable=too-many-ancestors\n\nlayers = tf.keras.layers\n\n\n@registers.model.register\nclass EmoLstmModel(Model):\n\n  def __init__(self, config, **kwargs):\n    super().__init__(**kwargs)\n    self.config = config\n\n  def build(self, input_shape):\n    logging.info(f""{self.__class__.__name__} input_shape : {input_shape}"")\n    _, time, feat, channels = input_shape[\'inputs\'].as_list()\n\n    self.reshape = layers.Reshape((time, feat * channels),\n                                  input_shape=(time, feat, channels))\n    self.lstm1 = layers.LSTM(512, return_sequences=True)\n    self.lstm2 = layers.LSTM(256, return_sequences=False)\n    self.dense1 = layers.Dense(512, activation=\'relu\')\n    self.drop1 = layers.Dropout(rate=0.2)\n    self.dense2 = layers.Dense(4)\n\n    # https://stackoverflow.com/questions/55684949/subclass-of-tf-keras-model-can-not-get-summay-result\n    # https://stackoverflow.com/questions/52826134/keras-model-subclassing-examples\n    x = {}\n    for key, shape in input_shape.items():\n      x[key] = tf.convert_to_tensor(\n          np.random.normal(size=[1] + shape.as_list()[1:]),\n          dtype=tf.keras.backend.floatx())\n    _ = self.call(x)\n    #super().build(input_shape=[input_shape[\'inputs\'].as_list(), input_shape[\'labels\'].as_list()])\n    self.built = True\n\n  def call(self, inputs, training=None, mask=None):\n    logging.info(f""xxxx input: {inputs}, training: {training}"")\n    if isinstance(inputs, dict):\n      x = inputs[\'inputs\']\n    elif isinstance(inputs, list):\n      x = inputs[0]\n    else:\n      x = inputs\n    x = self.reshape(x)\n    x = self.lstm1(x)\n    x = self.lstm2(x)\n    x = self.dense1(x)\n    #x = self.drop1(x, training=training)\n    logits = self.dense2(x)\n    return logits\n\n\n@registers.model.register\nclass EmoBLstmModel(Model):\n\n  def __init__(self, config, **kwargs):\n    super().__init__(**kwargs)\n    self.config = config\n\n  def build(self, input_shape):\n    logging.info(f""{self.__class__.__name__} input_shape : {input_shape}"")\n    _, time, feat, channels = input_shape[\'inputs\'].as_list()\n\n    self.reshape = layers.Reshape((time, feat * channels),\n                                  input_shape=(time, feat, channels))\n    self.lstm1 = layers.Bidirectional(layers.LSTM(512, return_sequences=True))\n    self.lstm2 = layers.Bidirectional(layers.LSTM(256, return_sequences=False))\n    self.dense1 = layers.Dense(512, activation=\'relu\')\n    self.dense2 = layers.Dense(4)\n    self.drop1 = layers.Dropout(rate=0.2)\n\n    self.built = True\n\n  def call(self, inputs, training=None, mask=None):\n    logging.info(f""xxxx input: {inputs}, training: {training}"")\n    x = inputs[\'inputs\']\n    x = self.reshape(x)\n    x = self.lstm1(x)\n    x = self.lstm2(x)\n    x = self.dense1(x)\n    x = self.drop1(x, training=training)\n    logits = self.dense2(x)\n    return logits\n\n\n@registers.model.register\nclass EmoBLstmPoolModel(Model):\n\n  def __init__(self, config, **kwargs):\n    super().__init__(**kwargs)\n    self.config = config\n\n  def build(self, input_shape):\n    logging.info(f""{self.__class__.__name__} input_shape : {input_shape}"")\n    _, time, feat, channels = input_shape[\'inputs\'].as_list()\n\n    self.reshape = layers.Reshape((time, feat * channels),\n                                  input_shape=(time, feat, channels))\n    self.dense1 = layers.TimeDistributed(layers.Dense(512, activation=\'relu\'))\n    self.drop1 = layers.Dropout(0.5)\n    self.lstm1 = layers.Bidirectional(layers.LSTM(128, return_sequences=True))\n    self.drop2 = layers.Dropout(0.5)\n    self.avg_pool = layers.GlobalAveragePooling1D()\n    self.drop3 = layers.Dropout(rate=0.5)\n    self.dense2 = layers.Dense(4)\n\n    self.built = True\n\n  def call(self, inputs, training=None, mask=None):\n    logging.info(f""xxxx input: {inputs}, training: {training}"")\n    x = inputs[\'inputs\']\n    x = self.reshape(x)\n    x = self.dense1(x)\n    x = self.drop1(x, training=training)\n    x = self.lstm1(x)\n    x = self.drop2(x, training=training)\n    x = self.avg_pool(x)\n    x = self.drop3(x, training=training)\n    logits = self.dense2(x)\n    return logits\n\n\n#pylint: disable=too-many-instance-attributes\nclass CBDP(Layer):\n\n  #pylint: disable=too-many-arguments\n  def __init__(self,\n               filters=128,\n               filter_size=(5, 3),\n               filter_strides=(1, 1),\n               pool_size=(4, 4),\n               drop_rate=0.1):\n    super().__init__(name=\'cbdp\')\n    self.filters = filters\n    self.filter_size = filter_size\n    self.filter_strides = filter_strides\n    self.pool_size = pool_size\n    self.drop_rate = drop_rate\n\n    self.conv = tf.keras.layers.Conv2D(\n        filters,\n        filter_size,\n        strides=filter_strides,\n        kernel_initializer=\'glorot_normal\',\n        use_bias=True,\n        padding=\'same\')\n    self.bn = tf.keras.layers.BatchNormalization()\n    self.drop = tf.keras.layers.Dropout(self.drop_rate)\n    self.pool = tf.keras.layers.MaxPool2D(\n        pool_size=self.pool_size,\n        strides=self.pool_size,\n        padding=\'same\',\n        data_format=\'channels_last\')\n\n  #pylint: disable=arguments-differ\n  def call(self, x, training):\n    \'\'\' x shape: [batch, frame, feat, channel]\'\'\'\n    x = self.conv(x)\n    x = self.bn(x, training=training)\n    x = tf.nn.relu6(x)\n    x = self.drop(x, training=training)\n    x = self.pool(x)\n    return x\n\n\nclass CNN(Layer):\n\n  def __init__(self, drop_rate):\n    super().__init__(name=\'cnn\')\n    self.drop_rate = drop_rate\n\n    self.block1 = CBDP(\n        filters=128,\n        filter_size=[5, 3],\n        filter_strides=(1, 1),\n        pool_size=(4, 4),\n        drop_rate=self.drop_rate)\n    self.block2 = CBDP(\n        filters=256,\n        filter_size=[5, 3],\n        filter_strides=(1, 1),\n        pool_size=(1, 2),\n        drop_rate=self.drop_rate)\n    self.block3 = CBDP(\n        filters=256,\n        filter_size=[5, 3],\n        filter_strides=(1, 1),\n        pool_size=(1, 2),\n        drop_rate=self.drop_rate)\n\n  #pylint: disable=arguments-differ\n  def call(self, x, training):\n    x = self.block1(x, training)\n    x = self.block2(x, training)\n    x = self.block3(x, training)\n    return x\n\n\nclass Linear(Layer):\n  \'\'\' linear layer\'\'\'\n\n  def __init__(self, dense_dim, drop_rate):\n    super().__init__(name=\'linear\')\n    self.drop_rate = drop_rate\n    self.fc_dim = dense_dim\n\n    self.fc = tf.keras.layers.Dense(self.fc_dim)\n    self.drop = tf.keras.layers.Dropout(self.drop_rate)\n\n  def build(self, input_shape):\n    batch, time, feat, channl = input_shape\n    del batch\n    self.reshape1 = tf.keras.layers.Reshape((time, feat * channl))\n    #self.reshape2 = tf.keras.layers.Reshape((time, self.fc_dim))\n\n  #pylint: disable=arguments-differ\n  def call(self, x, training):\n    \'\'\' x shape: [batch, time, feat, channel]\n        output shape: [batch, time, dim ]\n    \'\'\'\n    x = self.reshape1(x)\n    x = self.fc(x)\n    x = tf.nn.relu6(x)\n    x = self.drop(x, training=training)\n    #x = self.reshape2(x)\n    return x\n\n\nclass TimePool(Layer):\n\n  def __init__(self):\n    super().__init__(name=\'time_pool\')\n\n  def build(self, input_shape):\n    time, dim = input_shape[1:]\n    self.time_pool = tf.keras.layers.AveragePooling1D(\n        pool_size=time.value, strides=time.value, padding=\'same\')\n    self.reshape = tf.keras.layers.Reshape((dim,))\n\n  #pylint: disable=arguments-differ\n  def call(self, x):\n    \'\'\'\n      x shape :[ batch, time, dim]\n      output shape: [batch, dim]\n    \'\'\'\n    x = self.time_pool(x)\n    x = self.reshape(x)\n    return x\n\n\nclass LBD(Layer):\n\n  def __init__(self, dense_dim, drop_rate):\n    super().__init__(name=\'lbd\')\n    self.dense_dim = dense_dim\n    self.drop_rate = drop_rate\n\n    self.fc = tf.keras.layers.Dense(self.dense_dim)\n    self.drop = tf.keras.layers.Dropout(self.drop_rate)\n    self.bn = tf.keras.layers.BatchNormalization()\n\n  #pylint: disable=arguments-differ\n  def call(self, x, training):\n    x = self.fc(x)\n    x = self.bn(x, training)\n    x = tf.nn.relu6(x)\n    x = self.drop(x, training)\n    return x\n\n\nclass Head(Layer):\n  \'\'\' output logits \'\'\'\n\n  def __init__(self, num_class):\n    super().__init__(name=\'head\')\n    self.num_class = num_class\n\n    self.fc = tf.keras.layers.Dense(self.num_class)\n\n  #pylint: disable=arguments-differ\n  def call(self, x):\n    x = self.fc(x)\n    return x\n\n\nclass Feat(Layer):\n\n  def __init__(self, cmvn_path):\n    super().__init__(name=\'cmvn\', trainable=False)\n    self.mean, self.std = utils.load_cmvn(cmvn_path)\n\n  #pylint: disable=arguments-differ\n  def call(self, x):\n    x = utils.apply_cmvn(x, self.mean, self.std)\n    return x\n\n\n#pylint: disable=too-many-instance-attributes,too-many-ancestors\n@registers.model.register\nclass EmoCRNNModel(Model):\n  \'\'\' main model \'\'\'\n\n  def __init__(self, drop_rate):\n    super().__init__()\n    self.drop_rate = drop_rate\n    self.fc1_dim = 786\n    self.fc2_dim = 64\n    self.fc3_dim = 2\n\n    self.cnn = CNN(self.drop_rate)\n    self.fc1 = Linear(self.fc1_dim, self.drop_rate)\n    self.time_pool = TimePool()\n    self.fc2 = LBD(self.fc2_dim, self.drop_rate)\n    self.head = Head(self.fc3_dim)\n\n  #pylint: disable=arguments-differ\n  def call(self, features, training):\n    x = features[\'inputs\']\n    x = self.cnn(x, training)\n    x = self.fc1(x, training)\n    x = self.time_pool(x)\n    x = self.fc2(x, training)\n    x = self.head(x)\n    return x\n\n\n#pylint: disable=too-many-instance-attributes,too-many-ancestors\n@registers.model.register\nclass EmoCFNNModel(Model):\n  \'\'\' main model \'\'\'\n\n  #pylint: disable=useless-super-delegation\n  def __init__(self):\n    super().__init__()\n\n  def build(self, input_shape):\n    batch, time, feat, channels = input_shape.as_list()\n    del batch\n    stride = 3\n    self.cnn1 = tf.keras.layers.Conv2D(\n        filters=channels * 2,\n        kernel_size=(5, 3),\n        strides=(2, 1),\n        padding=\'same\',\n        use_bias=True)\n    self.cnn2 = tf.keras.layers.Conv2D(\n        filters=channels * 2,\n        kernel_size=(5, 3),\n        strides=(stride, 1),\n        padding=\'same\',\n        use_bias=True)\n    self.cnn3 = tf.keras.layers.Conv2D(\n        filters=channels * 2,\n        kernel_size=(5, 3),\n        strides=(2, 1),\n        padding=\'same\',\n        use_bias=True)\n    time /= (2 * stride * 2)\n\n    def tanh_sig(x):\n      x1, x2 = tf.split(x, 2, axis=-1)\n      return tf.nn.tanh(x1) * tf.nn.sigmoid(x2)\n\n    #pylint: disable=unnecessary-lambda\n    self.tanh_sig = tf.keras.layers.Lambda(lambda x: tanh_sig(x))\n\n    self.reshape = tf.keras.layers.Reshape((int(time), int(feat * channels)))\n    self.cnn4 = tf.keras.layers.Conv1D(\n        filters=int(feat * channels / 2),\n        kernel_size=3,\n        strides=1,\n        padding=\'same\',\n        use_bias=True)\n    self.cnn5 = tf.keras.layers.Conv1D(\n        filters=(feat * channels),\n        kernel_size=3,\n        strides=1,\n        padding=\'same\',\n        use_bias=True)\n    self.cnn6 = tf.keras.layers.Conv1D(\n        filters=(feat * channels),\n        kernel_size=1,\n        strides=1,\n        padding=\'same\',\n        use_bias=True)\n\n    self.cnn7 = tf.keras.layers.Conv1D(\n        filters=int(feat * channels / 2),\n        kernel_size=5,\n        strides=2,\n        padding=\'same\',\n        use_bias=True)\n    time /= 2\n    self.reshape2 = tf.keras.layers.Reshape((int(time * feat * channels / 2),))\n\n    self.nn1 = tf.keras.layers.Dense(3000)\n    self.reshape3 = tf.keras.layers.Reshape(\n        (int(3000 / (feat * channels / 2)), int(feat * channels / 2)))\n\n    self.max_pool = tf.keras.layers.GlobalMaxPool1D()\n    self.nn2 = tf.keras.layers.Dense(2)\n\n  #pylint: disable=arguments-differ\n  def call(self, features):\n    x = features[\'inputs\']\n    x = self.cnn1(x)\n    x = self.tanh_sig(x)\n    x = self.cnn2(x)\n    x = self.tanh_sig(x)\n    x = self.cnn3(x)\n    x1 = self.tanh_sig(x)\n\n    x2 = self.reshape(x1)\n\n    x3 = self.cnn4(x2)\n    x3 = tf.nn.relu(x3)\n    x3 = self.cnn5(x2)\n    x3 = tf.nn.relu(x3)\n    x3 += x2\n\n    x4 = self.cnn6(x2)\n    x4 = tf.nn.relu(x4)\n    x4 += x2\n\n    x5 = x3 + x4\n    x6 = self.cnn7(x5)\n    x6 = tf.nn.relu(x6)\n    x6 = self.reshape2(x6)\n\n    x6 = self.nn1(x6)\n    x6 = tf.nn.relu(x6)\n    x7 = self.reshape3(x6)\n\n    x8 = self.max_pool(x7)\n    logits = self.nn2(x8)\n    return logits\n'"
delta/models/speech_cls_rawmodel.py,36,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' emotion crnn model support multi-modal\'\'\'\nfrom absl import logging\nimport delta.compat as tf\n\nfrom delta import utils\nfrom delta.layers import common_layers\nfrom delta.data.feat.tf_speech_feature import speech_params, extract_feature\n\nfrom delta.models.base_model import RawModel\nfrom delta.utils.register import registers\n\n#pylint: disable=invalid-name\n\n\n#pylint: disable=too-many-instance-attributes\n@registers.model.register\nclass EmoCRNNRawModel(RawModel):\n  \'\'\' main model \'\'\'\n\n  def __init__(self, config, name=None):\n    super().__init__(name=name)\n    self.config = config\n\n    self.netconf = self.config[\'model\'][\'net\'][\'structure\']\n    self.taskconf = self.config[\'data\'][\'task\']\n    self.audioconf = self.taskconf[\'audio\']\n\n    self.attention = self.netconf[\'attention\']\n    self.vocab_size = self.taskconf[\'text\'][\'vocab_size\']\n    frame_per_sec = 1 / self.taskconf[\'audio\'][\'winstep\']\n    self.input_len = self.taskconf[\'audio\'][\'clip_size\'] * frame_per_sec\n    self.input_type = \'samples\' if self.taskconf[\n        \'suffix\'] == \'.wav\' else \'features\'\n    self.input_channels = 3 if self.taskconf[\'audio\'][\'add_delta_deltas\'] else 1\n\n    # l2\n    self._extra_train_ops = []\n\n    self.std = None\n    self.hp = None\n    self.alphas = None\n    self.train = None\n    self.mean = None\n\n  def preprocess(self, inputs, input_text):\n    \'\'\' preprocess speech and text inputs\n    params:\n      inputs: speech input\n      input_text: text input\n    \'\'\'\n    with tf.variable_scope(\'feature\'):\n      if self.input_type == \'samples\':\n        # speech feature config\n        self.hp = speech_params(\n            sr=self.taskconf[\'audio\'][\'sr\'],\n            bins=self.audioconf[\'feature_size\'],\n            dither=self.train,\n            use_delta_deltas=self.audioconf[\'add_delta_deltas\'],\n            cmvn=self.audioconf[\'cmvn\'],\n            cmvn_path=self.audioconf[\'cmvn_path\'])\n\n        feats = extract_feature(inputs, params=self.hp)\n      else:\n        self.mean, self.std = utils.load_cmvn(self.audioconf[\'cmvn_path\'])\n        feats = utils.apply_cmvn(inputs, self.mean, self.std)\n    return feats, input_text\n\n  #pylint: disable=arguments-differ\n  def call(self, features, **kwargs):\n    self.train = kwargs[\'training\']\n    feats = tf.identity(features[\'inputs\'], name=\'feats\')\n    texts = features[\'texts\']\n\n    with tf.variable_scope(\'model\', reuse=tf.AUTO_REUSE):\n      feats, texts = self.preprocess(feats, texts)\n      logits = self.model(feats, texts)\n    return logits\n\n  #pylint: disable=too-many-locals\n  def conv_block(self, inputs, depthwise=False):\n    \'\'\' conv layers\'\'\'\n    filters = self.netconf[\'filters\']\n    logging.info(""filters : {}"".format(filters))\n    filters_size = self.netconf[\'filter_size\']\n    logging.info(""filters_size : {}"".format(filters_size))\n    filters_strides = self.netconf[\'filter_stride\']\n    logging.info(""filters_strides : {}"".format(filters_strides))\n    pools_size = self.netconf[\'pool_size\']\n    logging.info(""pools_size : {}"".format(pools_size))\n\n    layer_num = len(filters)\n    assert layer_num == len(filters_size)\n    assert layer_num == len(filters_strides)\n    assert layer_num == len(pools_size)\n\n    channels = [self.input_channels] + filters\n    logging.info(""channels : {}"".format(channels))\n\n    downsample_input_len = self.input_len\n    with tf.variable_scope(\'cnn\'):\n      x = tf.identity(inputs)\n      for index, _ in enumerate(filters):\n        unit_name = \'unit-\' + str(index + 1)\n        with tf.variable_scope(unit_name):\n          if depthwise:\n            x = tf.layers.separable_conv2d(\n                x,\n                filters=filters[index],\n                kernel_size=filters_size[index],\n                strides=filters_strides[index],\n                padding=\'same\',\n                name=unit_name)\n          else:\n            cnn_name = \'cnn-\' + str(index + 1)\n            x = common_layers.conv2d(x, cnn_name, filters_size[index],\n                                     channels[index], channels[index + 1],\n                                     filters_strides[index])\n          if self.netconf[\'use_bn\']:\n            bn_name = \'bn\' + str(index + 1)\n            x = tf.layers.batch_normalization(\n                x, axis=-1, momentum=0.9, training=self.train, name=bn_name)\n          x = tf.nn.relu6(x)\n          if self.netconf[\'use_dropout\']:\n            x = tf.layers.dropout(\n                x, self.netconf[\'dropout_rate\'], training=self.train)\n          x = common_layers.max_pool(x, pools_size[index], pools_size[index])\n          downsample_input_len = downsample_input_len / pools_size[index][0]\n\n    return x, downsample_input_len\n\n  def linear_block(self, x):\n    \'\'\'\n    linear layer for dim reduction\n    x: shape [batch, time, feat, channel]\n    output: shape [b, t, f]\n    \'\'\'\n    with tf.variable_scope(\'linear\'):\n      times, feat, channel = x.shape.as_list()[1:]\n      x = tf.reshape(x, [-1, feat * channel])\n      if self.netconf[\'use_dropout\']:\n        x = tf.layers.dropout(\n            x, self.netconf[\'dropout_rate\'], training=self.train)\n      x = common_layers.linear(x, \'linear1\',\n                               [feat * channel, self.netconf[\'linear_num\']])\n      #x = tf.nn.relu6(x)\n      x = tf.reshape(x, [-1, times, self.netconf[\'linear_num\']])\n    return x\n\n  def lstm_layer(self, x):\n    \'\'\' lstm layers\'\'\'\n    if self.netconf[\'use_lstm_layer\']:\n      with tf.variable_scope(\'lstm\'):\n        cell_fw = tf.nn.rnn_cell.BasicLSTMCell(\n            self.netconf[\'cell_num\'], forget_bias=1.0)\n        if self.netconf[\'use_dropout\']:\n          cell_fw = tf.nn.rnn_cell.DropoutWrapper(\n              cell=cell_fw,\n              output_keep_prob=1 -\n              self.netconf[\'dropout_rate\'] if self.train else 1.0)\n\n        cell_bw = tf.nn.rnn_cell.BasicLSTMCell(\n            self.netconf[\'cell_num\'], forget_bias=1.0)\n        if self.netconf[\'use_dropout\']:\n          cell_bw = tf.nn.rnn_cell.DropoutWrapper(\n              cell=cell_bw,\n              output_keep_prob=1 -\n              self.netconf[\'dropout_rate\'] if self.train else 1.0)\n\n        # Now we feed `linear` into the LSTM BRNN cell and obtain the LSTM BRNN output.\n        outputs, output_states = tf.nn.bidirectional_dynamic_rnn(\n            cell_fw=cell_fw,\n            cell_bw=cell_bw,\n            inputs=x,\n            dtype=tf.float32,\n            time_major=False,\n            scope=\'LSTM1\')\n        del output_states\n    else:\n      outputs = x\n    return outputs\n\n  def pooling_layer(self, x, time_len):\n    \'\'\' pooling layer\'\'\'\n    with tf.variable_scope(\'time_pooling\'):\n      if self.attention:\n        x, self.alphas = common_layers.attention(\n            x, self.netconf[\'attention_size\'], return_alphas=True)\n        #alphas shape [batch, time, 1] -> [1, batch, time, 1]-> [1, time, batch, 1]\n        tf.summary.image(\n            \'alignment\',\n            tf.transpose(tf.expand_dims(self.alphas, 0), [0, 2, 1, 3]))\n      else:\n        if self.netconf[\'use_lstm_layer\']:\n          x = tf.concat(x, 2)\n        # [batch, seq_len, dim, 1]\n        x = tf.expand_dims(x, axis=-1)\n        seq_len = time_len\n        x = common_layers.max_pool(x, ksize=[seq_len, 1], strides=[seq_len, 1])\n        if self.netconf[\'use_lstm_layer\']:\n          x = tf.reshape(x, [-1, 2 * self.netconf[\'cell_num\']])\n        else:\n          x = tf.reshape(x, [-1, self.netconf[\'linear_num\']])\n      return x\n\n  def text_layer(self, x, input_text):\n    \'\'\' text embbeding layers\'\'\'\n    with tf.variable_scope(\'text\'):\n      embedding_chars_expanded = common_layers.embedding_look_up(\n          input_text, self.vocab_size, self.netconf[\'embedding_dim\'])\n      h_pool_flat = common_layers.conv_pool(\n          embedding_chars_expanded,\n          list(map(int, self.netconf[\'filter_sizes\'])),\n          self.netconf[\'embedding_dim\'], self.netconf[\'num_filters\'],\n          input_text.shape[1])\n      outputs = tf.concat((x, h_pool_flat), axis=1)\n    return outputs\n\n  def dense_layer(self, x):\n    \'\'\' fc layers\'\'\'\n    with tf.variable_scope(\'dense\'):\n      shape = x.shape[-1].value\n      y = common_layers.linear(x, \'dense-matmul\',\n                               [shape, self.netconf[\'hidden1\']])\n      if self.netconf[\'use_bn\']:\n        y = tf.layers.batch_normalization(\n            y, axis=-1, momentum=0.99, training=self.train, name=\'dense-bn\')\n      y = tf.nn.relu6(y)\n      if self.netconf[\'use_dropout\']:\n        y = tf.layers.dropout(\n            y, self.netconf[\'dropout_rate\'], training=self.train)\n    return y\n\n  def logits_layer(self, x):\n    \'\'\' output layers\'\'\'\n    with tf.variable_scope(\'logits\'):\n      logits = common_layers.linear(\n          x, \'logits-matmul\',\n          [self.netconf[\'hidden1\'], self.taskconf[\'classes\'][\'num\']])\n    return logits\n\n  def model(self, inputs, input_text):\n    \'\'\' build model \'\'\'\n    x, downsample_time_len = self.conv_block(inputs, depthwise=False)\n    x = self.linear_block(x)\n    x = self.lstm_layer(x)\n    x = self.pooling_layer(x, downsample_time_len)\n    if self.taskconf[\'text\'][\'enable\']:\n      x = self.text_layer(x, input_text)\n    x = self.dense_layer(x)\n    logits = self.logits_layer(x)\n    return logits\n\n\n@registers.model.register\nclass EmoDCRNNRawModel(EmoCRNNRawModel):\n  \'\'\' emotion dcrnn model \'\'\'\n\n  def __init__(self):\n    super().__init__()\n    self.depthwise = True\n\n  def model(self, inputs, input_text):\n    x, downsample_time_len = self.conv_block(inputs, depthwise=self.depthwise)\n    x = self.linear_block(x)\n    x = self.lstm_layer(x)\n    x = self.pooling_layer(x, downsample_time_len)\n    if self.taskconf[\'text\'][\'enable\']:\n      x = self.text_layer(x, input_text)\n    x = self.dense_layer(x)\n    logits = self.logits_layer(x)\n    return logits\n\n\n@registers.model.register\nclass EmoNDCRNNRawModel(EmoCRNNRawModel):\n  \'\'\' emotion ndcrnn model \'\'\'\n\n  def __init__(self):\n    super().__init__()\n    self.depthwise = True\n\n  def model(self, inputs, input_text):\n    x, downsample_time_len = self.conv_block(inputs, depthwise=self.depthwise)\n    x = self.linear_block(x)\n    x = self.lstm_layer(x)\n    x = self.pooling_layer(x, downsample_time_len)\n    if self.taskconf[\'text\'][\'enable\']:\n      x = self.text_layer(x, input_text)\n    x = self.dense_layer(x)\n    logits = self.logits_layer(x)\n    return logits\n'"
delta/models/text_cls_model.py,6,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Base class for text classification.""""""\n\nimport delta.compat as tf\n\nfrom delta.models.base_model import Model\nfrom delta.layers.utils import get_pad_mask_from_token_idx\nfrom delta.layers.utils import get_seg_mask_from_token_idx\n\n\nclass TextClassModel(Model):  # pylint: disable=abstract-method\n  """"""Base class for text classification.""""""\n\n  def __init__(self, config, **kwargs):\n    super().__init__(**kwargs)\n    model_config = config[\'model\'][\'net\'][\'structure\']\n    if ""dense_input"" in model_config:\n      self.use_dense_task = True\n      self.use_dense_input = model_config[""dense_input""][""use_dense_input""]\n      self.only_dense_input = model_config[""dense_input""][""only_dense_input""]\n      self.dense_input_dim = config[\'data\'][\'task\'][\'dense_input_dim\']\n    else:\n      self.use_dense_task = False\n      self.use_dense_input = False\n\n    if self.use_dense_input:\n      self.dense_input_linear = tf.keras.layers.Dense(\n          self.dense_input_dim, activation=tf.keras.activations.linear)\n\n    self.use_pretrained_model = config[\'model\'].get(\'use_pre_train_model\',\n                                                    False)\n    if self.use_pretrained_model:\n      pretrained_model_config = config[\'model\'][\'pre_train_model\']\n      self.pretrained_model_name = pretrained_model_config[\'name\']\n      self.pretrained_model_mode = pretrained_model_config[\'mode\']\n      self.pretrained_model_dim = pretrained_model_config[\'dim\']\n      self.pretrained_model_path = pretrained_model_config[\'path\']\n      self.pretrained_model_seg = pretrained_model_config[\'seg\']\n      self.pretrained_model_cls = pretrained_model_config[\'cls\']\n      self.pretrained_model_pad = pretrained_model_config[\'pad\']\n      self.pretrained_model_layers = pretrained_model_config[\'layers\']\n      self.pretrained_model_output = pretrained_model_config[\'output\']\n\n  def get_pre_train_graph(self, inputs):\n    pretrained_model_meta = self.pretrained_model_path + \'.meta\'\n    seg_idx = self.pretrained_model_seg\n    pad_idx = self.pretrained_model_pad\n    with tf.name_scope(\'pretrain_graph\') as scope:\n      pretrained_graph = tf.get_default_graph()\n      if self.pretrained_model_name == \'elmo\':\n        pretrained_saver = tf.train.import_meta_graph(\n            pretrained_model_meta, input_map={\'input_x:0\': inputs})\n        input_x_pretrained = pretrained_graph.get_tensor_by_name(\n            scope + \'input_x_elmo:0\')\n      if self.pretrained_model_name == \'bert\':\n        pad_mask = get_pad_mask_from_token_idx(inputs, pad_idx)\n        segment_mask = get_seg_mask_from_token_idx(inputs, seg_idx)\n        pretrained_saver = tf.train.import_meta_graph(\n            pretrained_model_meta,\n            input_map={\n                \'input_ids:0\': inputs,\n                \'input_mask:0\': pad_mask,\n                \'segment_ids:0\': segment_mask\n            })\n        if self.pretrained_model_output == \'seq\':\n          input_x_pretrained = \\\n            pretrained_graph.get_tensor_by_name(scope + \'encoder_layers_{}:0\'.\n                                                format(self.pretrained_model_layers))\n        else:\n          input_x_pretrained = pretrained_graph.get_tensor_by_name(\n              scope + \'input_x_bert_cls:0\')\n      return input_x_pretrained\n'"
delta/models/text_hierarchical_model.py,17,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Hierarchical text classification models.""""""\n\nimport pickle\nfrom absl import logging\nimport delta.compat as tf\n\nimport delta.utils as utils\nimport delta.layers\nfrom delta.layers.utils import cut_or_padding\nfrom delta.layers.utils import compute_sen_lens\nfrom delta.layers.utils import compute_doc_lens\nfrom delta.layers.utils import split_one_doc_to_true_len_sens\nfrom delta.models.text_cls_model import TextClassModel\nfrom delta.utils.register import registers\n\n# pylint: disable=abstract-method, too-many-ancestors, too-many-instance-attributes\n\n\n@registers.model.register\nclass HierarchicalModel(TextClassModel):\n  """"""Hierarchical text classification model.""""""\n\n  def __init__(self, config, **kwargs):\n    super().__init__(config, **kwargs)\n\n    logging.info(""Initialize HierachicalModel..."")\n\n    self.use_pretrained_embedding = config[\'model\'][\'use_pre_train_emb\']\n    if self.use_pretrained_embedding:\n      self.embedding_path = config[\'model\'][\'embedding_path\']\n      logging.info(""Loading embedding file from: {}"".format(\n          self.embedding_path))\n      self._word_embedding_init = pickle.load(open(self.embedding_path, \'rb\'))\n      self.embed_initializer = tf.constant_initializer(\n          self._word_embedding_init)\n    else:\n      self.embed_initializer = tf.random_uniform_initializer(-0.1, 0.1)\n\n  @staticmethod\n  def pad_to_hier_input(inputs, max_doc_len, max_sen_len, padding_token=0):\n    """"""\n    Input shape: [batch_size, max_len]\n    New Input shape: [batch_size, max_doc_len, max_sen_len]\n    """"""\n    new_len = max_sen_len * max_doc_len\n    new_input = cut_or_padding(inputs, new_len, padding_token=padding_token)\n    new_input = tf.reshape(new_input, [-1, max_doc_len, max_sen_len])\n    return new_input\n\n  @staticmethod\n  def pad_to_hier_input_true_len(inputs,\n                                 max_doc_len,\n                                 max_sen_len,\n                                 split_token,\n                                 padding_token=0):\n    """"""\n    Input shape: [batch_size, max_len]\n    New Input shape: [batch_size, max_doc_len, max_sen_len]\n    """"""\n    new_input = tf.map_fn(\n        lambda x: split_one_doc_to_true_len_sens(\n            x, split_token, padding_token, max_doc_len, max_sen_len), inputs)\n    return new_input\n\n\n@registers.model.register\nclass HierarchicalAttentionModel(HierarchicalModel):\n  """"""Hierarchical text classification model with attention.""""""\n\n  def __init__(self, config, **kwargs):\n    super().__init__(config, **kwargs)\n\n    logging.info(""Initialize HierarchicalAttentionModel..."")\n\n    self.vocab_size = config[\'data\'][\'vocab_size\']\n    self.num_classes = config[\'data\'][\'task\'][\'classes\'][\'num_classes\']\n    self.use_true_length = config[\'model\'].get(\'use_true_length\', False)\n    if self.use_true_length:\n      self.split_token = config[\'data\'][\'split_token\']\n    self.padding_token = utils.PAD_IDX\n\n    model_config = config[\'model\'][\'net\'][\'structure\']\n    self.dropout_rate = model_config[\'dropout_rate\']\n    self.embedding_size = model_config[\'embedding_size\']\n    self.emb_trainable = model_config[\'emb_trainable\']\n    self.num_layers = model_config[\'num_layers\']\n    self.l2_reg_lambda = model_config[\'l2_reg_lambda\']\n    self.max_len = model_config[\'max_len\']\n    self.max_sen_len = model_config[\'max_sen_len\']\n    self.max_doc_len = model_config[\'max_doc_len\']\n\n    self.embed = tf.keras.layers.Embedding(\n        self.vocab_size,\n        self.embedding_size,\n        trainable=self.emb_trainable,\n        embeddings_initializer=self.embed_initializer)\n\n    self.embed_d = tf.keras.layers.Dropout(self.dropout_rate)\n    self.sen_encoder = delta.layers.RnnAttentionEncoder(\n        config, name=""sen_encoder"")\n    self.doc_encoder = delta.layers.RnnAttentionEncoder(\n        config, name=""doc_encoder"")\n\n    self.final_dense = tf.keras.layers.Dense(\n        self.num_classes,\n        activation=tf.keras.activations.linear,\n        name=""final_dense"")\n    logging.info(""Initialize HierarchicalAttentionModel done."")\n\n  def call(self, inputs, training=None, mask=None):  # pylint: disable=too-many-locals\n    input_x = tf.identity(inputs[""input_x""], name=\'input_x\')\n    if self.use_dense_task:\n      dense_input = inputs[""input_dense""]\n    if self.use_true_length:\n      # [batch_size, max_doc_len, max_sen_len]\n      input_hx = self.pad_to_hier_input_true_len(\n          input_x,\n          self.max_doc_len,\n          self.max_sen_len,\n          self.split_token,\n          padding_token=self.padding_token)\n    else:\n      # [batch_size, max_doc_len, max_sen_len]\n      input_hx = self.pad_to_hier_input(\n          input_x,\n          self.max_doc_len,\n          self.max_sen_len,\n          padding_token=self.padding_token)\n\n    # [batch_size, max_doc_len]\n    sen_lens = compute_sen_lens(input_hx, padding_token=self.padding_token)\n    # [batch_size]\n    doc_lens = compute_doc_lens(sen_lens)\n    # [batch_size, max_doc_len, max_sen_len, 1]\n    sen_mask = tf.expand_dims(\n        tf.sequence_mask(sen_lens, self.max_sen_len, dtype=tf.float32), axis=-1)\n\n    # [batch_size, max_doc_len, 1]\n    doc_mask = tf.expand_dims(\n        tf.sequence_mask(doc_lens, self.max_doc_len, dtype=tf.float32), axis=-1)\n\n    # [batch_size, max_doc_len, max_sen_len, embed_len]\n    out = self.embed(input_hx)\n    if self.use_pretrained_model:\n      input_px = self.get_pre_train_graph(input_x)\n      input_px = tf.reshape(\n          input_px,\n          [-1, self.max_doc_len, self.max_sen_len, self.pretrained_model_dim])\n      out = tf.concat([out, input_px], axis=-1)\n    out = self.embed_d(out, training=training)\n    all_sen_encoder = tf.keras.layers.TimeDistributed(self.sen_encoder)\n    # [batch_size, max_doc_len, features]\n    out = all_sen_encoder(out, training=training, mask=sen_mask)\n    # [batch_size, features]\n    out = self.doc_encoder(out, training=training, mask=doc_mask)\n\n    if self.use_dense_input:\n      dense_out = self.dense_input_linear(dense_input)\n      if self.only_dense_input:\n        out = dense_out\n      else:\n        out = tf.keras.layers.Concatenate()([out, dense_out])\n\n    # [batch_size, class_num]\n    scores = self.final_dense(out)\n\n    return scores\n'"
delta/models/text_match_model.py,32,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Match texts with Rnn models.""""""\n\nimport pickle\nfrom absl import logging\nimport delta.compat as tf\nfrom delta.layers.dynamic_pooling import DynamicPoolingLayer\nfrom delta.layers.match_pyramid import MatchingLayer\nfrom delta.models.base_model import Model\nfrom delta.utils.register import registers\n\n\n# pylint: disable=too-few-public-methods, abstract-method,too-many-ancestors\n@registers.model.register\nclass MatchRnn(Model):\n  """"""Match texts with Rnn models.""""""\n\n  def __init__(self, config, **kwargs):\n    super().__init__(**kwargs)\n    logging.info(""Initialize MatchRnn..."")\n\n    self.use_pretrained_embedding = config[\'model\'][\'use_pre_train_emb\']\n    if self.use_pretrained_embedding:\n      self.embedding_path = config[\'model\'][\'embedding_path\']\n      logging.info(""Loading embedding file from: {}"".format(\n        self.embedding_path))\n      self._word_embedding_init = pickle.load(open(self.embedding_path, \'rb\'))\n      self.embed_initializer = tf.constant_initializer(\n        self._word_embedding_init)\n    else:\n      self.embed_initializer = tf.random_uniform_initializer(-0.1, 0.1)\n\n\n# pylint: disable=too-many-instance-attributes,too-many-ancestors\n@registers.model.register\nclass MatchRnnTextClassModel(MatchRnn):\n  """"""Match texts model with Rnn and Attention.""""""\n\n  def __init__(self, config, **kwargs):\n    super().__init__(config, **kwargs)\n    logging.info(""Initialize MatchRnnTextClassModel..."")\n\n    self.vocab_size = config[\'data\'][\'vocab_size\']\n    self.num_classes = config[\'data\'][\'task\'][\'classes\'][\'num_classes\']\n\n    model_config = config[\'model\'][\'net\'][\'structure\']\n    self.dropout_rate = model_config[\'dropout_rate\']\n    self.embedding_size = model_config[\'embedding_size\']\n    self.emb_trainable = model_config[\'emb_trainable\']\n    self.lstm_num_units = model_config[\'lstm_num_units\']\n    self.fc_num_units = model_config[\'fc_num_units\']\n    self.l2_reg_lambda = model_config[\'l2_reg_lambda\']\n\n    self.embed = tf.keras.layers.Embedding(\n      self.vocab_size,\n      self.embedding_size,\n      trainable=self.emb_trainable,\n      name=\'embdding\',\n      embeddings_initializer=self.embed_initializer)\n\n    self.embed_d = tf.keras.layers.Dropout(self.dropout_rate)\n\n    self.lstm_left = tf.keras.layers.LSTM(\n      self.lstm_num_units, return_sequences=True, name=\'lstm_left\')\n    self.lstm_right = tf.keras.layers.LSTM(\n      self.lstm_num_units, return_sequences=True, name=\'lstm_right\')\n    self.concat = tf.keras.layers.Concatenate(axis=1)\n\n    self.dropout = tf.keras.layers.Dropout(rate=self.dropout_rate)\n    self.outlayer = tf.keras.layers.Dense(self.fc_num_units, activation=\'tanh\')\n    self.tasktype = config[\'data\'][\'task\'][\'type\']\n    # if self.tasktype == ""Classification"":\n    self.final_dense = tf.keras.layers.Dense(\n      self.num_classes,\n      activation=tf.keras.activations.linear,\n      name=""final_dense"")\n\n    logging.info(""Initialize MatchRnnTextClassModel done."")\n\n  def call(self, inputs, training=None, mask=None):  # pylint: disable=too-many-locals\n\n    input_left = inputs[""input_x_left""]\n    input_right = inputs[""input_x_right""]\n\n    embedding = self.embed\n    embed_left = embedding(input_left)\n    embed_right = embedding(input_right)\n\n    encoded_left = self.lstm_left(embed_left)\n    encoded_right = self.lstm_right(embed_right)\n\n    encoded_right = tf.transpose(encoded_right, [0, 2, 1])\n    left_right_sim = tf.matmul(encoded_left, encoded_right)\n    shape_list = left_right_sim.get_shape()\n    newdim = shape_list[1] * shape_list[2]\n    sim_matrix = tf.reshape(left_right_sim, [-1, newdim], name=""sim_matrix"")\n\n    dropout = self.dropout(sim_matrix)\n    out = self.outlayer(dropout)\n\n    scores = self.final_dense(out)\n\n    return scores\n\n\n# pylint: disable=too-many-instance-attributes,too-many-ancestors\n@registers.model.register\nclass MatchPyramidTextClassModel(MatchRnn):\n  """"""Match texts model with Match Pyramid.""""""\n\n  def __init__(self, config, **kwargs):\n    super().__init__(config, **kwargs)\n    logging.info(""Initialize MatchPyramidTextClassModel ..."")\n\n    self.vocab_size = config[\'data\'][\'vocab_size\']\n    self.num_classes = config[\'data\'][\'task\'][\'classes\'][\'num_classes\']\n    self.max_seq_len = config[\'data\'][\'task\'][\'max_seq_len\']\n    model_config = config[\'model\'][\'net\'][\'structure\']\n    self.dropout_rate = model_config[\'dropout_rate\']\n    self.embedding_size = model_config[\'embedding_size\']\n    self.emb_trainable = model_config[\'emb_trainable\']\n    self.lstm_num_units = model_config[\'lstm_num_units\']\n    self.fc_num_units = model_config[\'fc_num_units\']\n    self.l2_reg_lambda = model_config[\'l2_reg_lambda\']\n\n    # Number of convolution blocks\n    self.num_blocks = model_config[\'num_blocks\']\n    # The kernel count of the 2D convolution\n    self.kernel_count = model_config[\'kernel_count\']\n    # The kernel size of the 2D convolution of each block\n    self.kernel_size = model_config[\'kernel_size\']\n    # The max-pooling size of each block\n    self.dpool_size = model_config[\'dpool_size\']\n    # The padding mode in the convolution layer\n    self.padding = model_config[\'padding\']\n    # The activation function\n    self.activation = model_config[\'activation\']\n    self.matching_type = model_config[\'matching_type\']\n\n    self.embed = tf.keras.layers.Embedding(\n      self.vocab_size,\n      self.embedding_size,\n      trainable=self.emb_trainable,\n      name=\'embdding\',\n      embeddings_initializer=self.embed_initializer)\n\n    self.embed_d = tf.keras.layers.Dropout(self.dropout_rate)\n\n    self.matching_layer = MatchingLayer(matching_type=self.matching_type)\n\n    self.conv = []\n    for i in range(self.num_blocks):\n      conv = tf.keras.layers.Conv2D(\n        self.kernel_count,\n        self.kernel_size,\n        padding=self.padding,\n        activation=self.activation)\n      self.conv.append(conv)\n\n    self.dpool = DynamicPoolingLayer(*self.dpool_size)\n\n    self.flatten = tf.keras.layers.Flatten()\n\n    self.dropout = tf.keras.layers.Dropout(rate=self.dropout_rate)\n    self.outlayer = tf.keras.layers.Dense(self.fc_num_units, activation=\'tanh\')\n    self.tasktype = config[\'data\'][\'task\'][\'type\']\n    # if self.tasktype == ""Classification"":\n    self.final_dense = tf.keras.layers.Dense(\n      self.num_classes,\n      activation=tf.keras.activations.linear,\n      name=""final_dense"")\n\n    logging.info(""Initialize MatchPyramidTextClassModel done."")\n\n  def call(self, inputs, training=None, mask=None):  # pylint: disable=too-many-locals\n    input_left = inputs[""input_x_left""]\n    input_right = inputs[""input_x_right""]\n\n    input_x_left_len = inputs[""input_x_left_len""]\n    input_x_right_len = inputs[""input_x_right_len""]\n\n    embedding = self.embed\n    embed_left = embedding(input_left)\n    embed_right = embedding(input_right)\n\n    p_index = self._dynamic_pooling_index(input_x_left_len,\n                                          input_x_right_len,\n                                          self.max_seq_len,\n                                          self.max_seq_len,\n                                          1,\n                                          1,\n                                          )\n\n    embed_cross = self.matching_layer([embed_left, embed_right])\n    for i in range(self.num_blocks):\n      embed_cross = self.conv[i](embed_cross)\n    embed_pool = self.dpool(\n      [embed_cross, p_index])\n\n    embed_flat = self.flatten(embed_pool)\n\n    dropout = self.dropout(embed_flat)\n    out = self.outlayer(dropout)\n    scores = self.final_dense(out)\n    return scores\n\n\n\n  def _dynamic_pooling_index(self, length_left,\n                             length_right,\n                             fixed_length_left: int,\n                             fixed_length_right: int,\n                             compress_ratio_left: float,\n                             compress_ratio_right: float) -> tf.Tensor:\n    def _dpool_index(one_length_left,\n                     one_length_right,\n                     fixed_length_left,\n                     fixed_length_right):\n\n      logging.info(""fixed_length_left: {}"".format(fixed_length_left))\n      logging.info(""fixed_length_right: {}"".format(fixed_length_right))\n\n      if one_length_left == 0:\n        stride_left = fixed_length_left\n      else:\n        stride_left = 1.0 * fixed_length_left / tf.cast(one_length_left, dtype=tf.float32)\n\n      if one_length_right == 0:\n        stride_right = fixed_length_right\n      else:\n        stride_right = 1.0 * fixed_length_right / tf.cast(one_length_right, dtype=tf.float32)\n\n      one_idx_left = [tf.cast(i / stride_left, dtype=tf.int32)\n                      for i in range(fixed_length_left)]\n      one_idx_right = [tf.cast(i / stride_right, dtype=tf.int32)\n                       for i in range(fixed_length_right)]\n      mesh1, mesh2 = tf.meshgrid(one_idx_left, one_idx_right)\n      index_one = tf.transpose(\n        tf.stack([mesh1, mesh2]), (2, 1, 0))\n      return index_one\n\n    index = []\n    dpool_bias_left = dpool_bias_right = 0\n    if fixed_length_left % compress_ratio_left != 0:\n      dpool_bias_left = 1\n    if fixed_length_right % compress_ratio_right != 0:\n      dpool_bias_right = 1\n    cur_fixed_length_left = int(\n      fixed_length_left // compress_ratio_left) + dpool_bias_left\n    cur_fixed_length_right = int(\n      fixed_length_right // compress_ratio_right) + dpool_bias_right\n    logging.info(""length_left: {}"".format(length_left))\n    logging.info(""length_right: {}"".format(length_right))\n    logging.info(""cur_fixed_length_left: {}"".format(cur_fixed_length_left))\n    logging.info(""cur_fixed_length_right: {}"".format(cur_fixed_length_right))\n\n    index = tf.map_fn(lambda x: _dpool_index(x[0], x[1], cur_fixed_length_left, cur_fixed_length_right),\n                      (length_left, length_right), dtype=tf.int32)\n\n    logging.info(""index: {}"".format(index))\n\n    return index\n'"
delta/models/text_nlu_joint_model.py,11,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' nlu joint model \'\'\'\n\nimport pickle\nimport delta.compat as tf\nfrom absl import logging\n\nimport delta.utils as utils\nfrom delta.layers.recurrent import BiRnn\nfrom delta.layers.attention import HanAttention\nfrom delta.layers.utils import compute_sen_lens\nfrom delta.models.text_cls_model import TextClassModel\nfrom delta.utils.register import registers\n\n# pylint: disable=too-many-ancestors, too-many-instance-attributes, abstract-method\n\n\nclass NLUJointModel(TextClassModel):\n  """"""class for sequence labeling.""""""\n\n  def __init__(self, config, **kwargs):\n    super().__init__(config, **kwargs)\n\n    logging.info(""Initialize SeqclassModel"")\n\n    self.use_pretrained_embedding = config[\'model\'][\'use_pre_train_emb\']\n    if self.use_pretrained_embedding:\n      self.embedding_path = config[\'model\'][\'embedding_path\']\n      logging.info(""Loading embedding file from: {}"".format(\n          self.embedding_path))\n      self._word_embedding_init = pickle.load(open(self.embedding_path, \'rb\'))\n      self.embed_initializer = tf.constant_initializer(\n          self._word_embedding_init)\n    else:\n      self.embed_initializer = tf.random_uniform_initializer(-0.1, 0.1)\n\n\n@registers.model.register\nclass JointBilstmCrfModel(NLUJointModel):\n  """"""bilstmcrf class for sequence labeling.""""""\n\n  def __init__(self, config, **kwargs):\n    super().__init__(config, **kwargs)\n\n    logging.info(""Initialize JointBilstmCrfModel..."")\n\n    self.vocab_size = config[\'data\'][\'vocab_size\']\n    self.intent_num_classes = config[\'data\'][\'task\'][\'classes\'][0][\n        \'num_classes\']\n    self.seq_num_classes = config[\'data\'][\'task\'][\'classes\'][1][\'num_classes\']\n    model_config = config[\'model\'][\'net\'][\'structure\']\n    self.dropout_rate = model_config[\'dropout_rate\']\n    self.embedding_size = model_config[\'embedding_size\']\n    self.num_layers = model_config[\'num_layers\']\n    self.l2_reg_lambda = model_config[\'l2_reg_lambda\']\n    self.batch_size = model_config[\'batch_size\']\n    self.max_len = model_config[\'max_len\']\n    self.num_units = model_config[\'num_units\']\n    self.padding_token = utils.PAD_IDX\n    self.fc_dim = model_config[\'fc_dim\']  # output fully-connected layer size\n\n    self.embed = tf.keras.layers.Embedding(\n        input_dim=self.vocab_size,\n        output_dim=self.embedding_size,\n        mask_zero=True,\n        input_length=self.max_len,\n        embeddings_initializer=self.embed_initializer,\n        trainable=True)\n\n    self.embed_dropout = tf.keras.layers.Dropout(self.dropout_rate)\n    self.bi_rnn = BiRnn(config)\n    self.attention = HanAttention(name=""{}_att"".format(self.name))\n    self.dropout = tf.keras.layers.Dropout(self.dropout_rate)\n    self.intent_dense = tf.keras.layers.Dense(\n        self.intent_num_classes, name=\'intent_fc_layer\')\n    self.slots_dense = tf.keras.layers.Dense(\n        self.seq_num_classes, name=\'slots_fc_layer\')\n\n    self.transitions = self.add_weight(\n        name=\'transitions\', shape=(self.seq_num_classes, self.seq_num_classes))\n\n    logging.info(""Initialize JointBilstmCrfModel done."")\n\n  def call(self, inputs, training=None, mask=None):\n    input_x = inputs[""input_x""]\n    # [batch_size, max_len]\n    input_x_lens = compute_sen_lens(input_x, padding_token=self.padding_token)\n    # [batch_size, max_len, 1]\n    mask = tf.expand_dims(\n        tf.sequence_mask(input_x_lens, self.max_len, dtype=tf.float32), axis=-1)\n    # [batch_size, max_len, embed_len]\n    out = self.embed(input_x)\n    # [batch_size, features]\n    out = self.embed_dropout(out, training=training)\n    out = self.bi_rnn(out)\n    intent_out = self.attention(out, mask=mask)\n    intent_out = self.dropout(intent_out)\n    intent_out = self.intent_dense(intent_out)\n    intent_out = tf.identity(intent_out, name=""intent_logits"")\n    slots_out = self.dropout(out)\n    slots_out = self.slots_dense(slots_out)\n    slots_out = tf.identity(slots_out, name=""slots_logits"")\n    return intent_out, slots_out\n'"
delta/models/text_seq2seq_model.py,9,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Models for text sequence to sequence.""""""\n\nimport pickle\n\nimport delta.compat as tf\nfrom absl import logging\n\nfrom delta import layers\nfrom delta import utils\nfrom delta.models.base_model import Model\nfrom delta.utils import registers\nfrom delta.layers.utils import compute_sen_lens\nfrom delta.layers.utils_tf import create_padding_mask\n\n\nclass Seq2SeqModel(Model):\n  """"""Base class for text sequence to sequence models""""""\n\n  def __init__(self, config, **kwargs):\n    super().__init__(**kwargs)\n    logging.info(""Initialize S2SModel"")\n    data_config = config[\'data\']\n    model_config = config[\'model\'][\'net\'][\'structure\']\n    self.use_label_vocab = data_config[\'task\'][\'use_label_vocab\']\n    self.label_vocab_size = data_config[\'label_vocab_size\']\n    self.vocab_size = config[\'data\'][\'vocab_size\']\n    self.use_pretrained_embedding = config[\'model\'][\'use_pre_train_emb\']\n    self.embedding_size = model_config[\'embedding_size\']\n    if self.use_pretrained_embedding:\n      self.embedding_path = config[\'model\'][\'embedding_path\']\n      logging.info(""Loading embedding file from: {}"".format(\n          self.embedding_path))\n      self._word_embedding_init = pickle.load(open(self.embedding_path, \'rb\'))\n      self.embed_initializer = tf.constant_initializer(\n          self._word_embedding_init)\n    else:\n      self.embed_initializer = tf.random_uniform_initializer(-0.1, 0.1)\n\n    self.embed = tf.keras.layers.Embedding(\n        self.vocab_size,\n        self.embedding_size,\n        embeddings_initializer=self.embed_initializer)\n    self.share_embedding = model_config[\'share_embedding\']\n    if self.use_label_vocab:\n      self.decode_vocab_size = self.label_vocab_size\n    else:\n      self.decode_vocab_size = self.vocab_size\n    if self.share_embedding:\n      self.decoder_embed = self.embed\n    else:\n      self.decoder_embed = tf.keras.layers.Embedding(\n          self.decode_vocab_size,\n          self.embedding_size,\n          embeddings_initializer=self.embed_initializer)\n\n\n@registers.model.register\nclass TransformerSeq2SeqModel(Seq2SeqModel):\n  """"""Transformer model for text sequence to sequence""""""\n\n  def __init__(self, config, **kwargs):\n    super().__init__(config, **kwargs)\n    tf.logging.info(""Initialize TransformerModel..."")\n    model_config = config[\'model\'][\'net\'][\'structure\']\n    self.is_infer = config[\'model\'][\'is_infer\']\n    self.dropout_rate = model_config[\'dropout_rate\']\n    self.num_layers = model_config[\'num_layers\']\n    self.max_enc_len = model_config[\'max_enc_len\']\n    self.max_dec_len = model_config[\'max_dec_len\']\n    self.share_embedding = model_config.get(\'share_embedding\', True)\n    self.use_const = model_config.get(\'use_const\', True)\n\n    self.embed_d = tf.keras.layers.Dropout(self.dropout_rate)\n    self.pos_embed = layers.PositionEmbedding(\n      self.max_enc_len, self.embedding_size, self.use_const, ""enc_pos"")\n\n    self.encoder = layers.TransformerEncoder(config)\n    self.decoder = layers.TransformerDecoder(config,\n                                             (self.embed, self.pos_embed),\n                                             self.decode_vocab_size)\n    logging.info(""decode_vocab_size: {}"".format(self.decode_vocab_size))\n    logging.info(""Initialize TransformerModel done."")\n\n  def call(self, inputs, training=None, mask=None):\n    input_enc_x = inputs[""input_enc_x""]\n    enc_mask = create_padding_mask(input_enc_x)\n    enc_emb = self.embed(input_enc_x)\n    enc_pos_emb = self.pos_embed(input_enc_x)\n    enc_emb += enc_pos_emb\n    enc_emb = self.embed_d(enc_emb, training=training)\n    enc_out = self.encoder(enc_emb, training=training, mask=enc_mask)\n\n    if not self.is_infer:\n      input_dec_x = inputs[""input_dec_x""]\n      dec_inputs = [input_dec_x, enc_out]\n    else:\n      dec_inputs = enc_out\n    dec_out = self.decoder(dec_inputs, training=training, mask=enc_mask)\n    return dec_out\n\n\n@registers.model.register\nclass RnnSeq2SeqModel(Seq2SeqModel):\n  """"""RNN model for text sequence to sequence""""""\n\n  def __init__(self, config, **kwargs):\n    super().__init__(config, **kwargs)\n    logging.info(""Initialize RnnSeq2SeqModel..."")\n    model_config = config[\'model\'][\'net\'][\'structure\']\n    self.is_infer = config[\'model\'][\'is_infer\']\n    self.dropout_rate = model_config[\'dropout_rate\']\n    self.padding_token = utils.PAD_IDX\n    self.embed_d = tf.keras.layers.Dropout(self.dropout_rate)\n    self.encoder = layers.RnnEncoder(config, name=""encoder"")\n    self.decoder = layers.RnnDecoder(\n        config, self.decoder_embed, self.decode_vocab_size, name=""decoder"")\n    self.mask_layer = tf.keras.layers.Lambda(lambda inputs: tf.cast(\n        tf.not_equal(inputs, self.padding_token), tf.int32))\n\n  def call(self, inputs, training=None, mask=None):\n    enc_inputs = inputs[""input_enc_x""]\n    seq_enc_len = compute_sen_lens(enc_inputs, padding_token=self.padding_token)\n    enc_mask = self.mask_layer(enc_inputs)\n    enc_inputs = self.embed(enc_inputs)\n    enc_inputs = self.embed_d(enc_inputs)\n    enc_outputs, enc_state = self.encoder(\n        enc_inputs, training=training, mask=enc_mask)\n    if self.is_infer:\n      dec_outputs = self.decoder([enc_outputs, enc_state, seq_enc_len],\n                                 training=training)\n      return dec_outputs\n\n    else:\n      dec_inputs = inputs[""input_dec_x""]\n      seq_dec_len = compute_sen_lens(\n          dec_inputs, padding_token=self.padding_token)\n      dec_outputs = self.decoder(\n          [dec_inputs, seq_dec_len, enc_outputs, enc_state, seq_enc_len],\n          training=training)\n      return dec_outputs\n'"
delta/models/text_seq_label_model.py,9,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' bilstmcrf model \'\'\'\n\nimport pickle\nimport delta.compat as tf\nfrom absl import logging\n\nfrom delta.models.text_cls_model import TextClassModel\nfrom delta.utils.register import registers\n\n# pylint: disable=too-many-ancestors, too-many-instance-attributes, abstract-method\n\n\nclass SeqclassModel(TextClassModel):\n  """"""class for sequence labeling.""""""\n\n  def __init__(self, config, **kwargs):\n    super().__init__(config, **kwargs)\n\n    logging.info(""Initialize SeqclassModel"")\n\n    self.use_pretrained_embedding = config[\'model\'][\'use_pre_train_emb\']\n    if self.use_pretrained_embedding:\n      self.embedding_path = config[\'model\'][\'embedding_path\']\n      logging.info(""Loading embedding file from: {}"".format(\n          self.embedding_path))\n      self._word_embedding_init = pickle.load(open(self.embedding_path, \'rb\'))\n      self.embed_initializer = tf.constant_initializer(\n          self._word_embedding_init)\n    else:\n      self.embed_initializer = tf.random_uniform_initializer(-0.1, 0.1)\n\n\n@registers.model.register\nclass BilstmCrfModel(SeqclassModel):\n  """"""bilstmcrf class for sequence labeling.""""""\n\n  def __init__(self, config, **kwargs):\n    super().__init__(config, **kwargs)\n\n    logging.info(""Initialize BilstmModel..."")\n\n    self.vocab_size = config[\'data\'][\'vocab_size\']\n    self.num_classes = config[\'data\'][\'task\'][\'classes\'][\'num_classes\']\n    self.seq_num_classes = self.num_classes\n\n    model_config = config[\'model\'][\'net\'][\'structure\']\n    self.dropout_rate = model_config[\'dropout_rate\']\n    self.embedding_size = model_config[\'embedding_size\']\n    self.num_layers = model_config[\'num_layers\']\n    self.l2_reg_lambda = model_config[\'l2_reg_lambda\']\n    self.batch_size = model_config[\'batch_size\']\n    self.max_len = model_config[\'max_len\']\n    self.num_units = model_config[\'num_units\']\n    self.fc_dim = model_config[\'fc_dim\']  # output fully-connected layer size\n\n    self.embed = tf.keras.layers.Embedding(\n        input_dim=self.vocab_size,\n        output_dim=self.embedding_size,\n        mask_zero=True,\n        input_length=self.max_len,\n        embeddings_initializer=self.embed_initializer,\n        trainable=True)\n\n    self.embed_dropout = tf.keras.layers.Dropout(self.dropout_rate)\n    self.bilstm = tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(\n            units=self.num_units, return_sequences=True, recurrent_dropout=0.1),\n        merge_mode=\'concat\',\n        name=\'BiLSTM\')\n    self.dropout = tf.keras.layers.Dropout(self.dropout_rate)\n    self.dense = tf.keras.layers.Dense(self.num_classes, name=\'fc_layer\')\n\n    self.transitions = self.add_weight(\n        name=\'transitions\', shape=(self.seq_num_classes, self.seq_num_classes))\n\n    logging.info(""Initialize BilstmCrfModel done."")\n\n  def call(self, inputs, training=None, mask=None):\n    input_x = inputs[""input_x""]\n    # [batch_size, max_len, embed_len]\n    out = self.embed(input_x)\n    # [batch_size, features]\n    if self.use_pretrained_model:\n      logging.info(""use_pretrained_model: {}, {}"".format(\n          self.pretrained_model_name, self.pretrained_model_mode))\n      if self.pretrained_model_name == \'bert\' and \\\n        self.pretrained_model_mode == \'fine-tune\':\n        out = self.get_pre_train_graph(input_x)\n      else:\n        input_px = self.get_pre_train_graph(input_x)\n        out = tf.concat([out, input_px], axis=-1)\n        out = self.embed_dropout(out, training=training)\n    out = self.bilstm(out)\n    out = self.dropout(out, training=training)\n    output = self.dense(out)\n    return output\n'"
delta/models/text_seq_model.py,47,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Sequence model for text classification.""""""\n\nimport pickle\nimport delta.compat as tf\nfrom absl import logging\n\nfrom delta import layers\nfrom delta.models.text_cls_model import TextClassModel\nfrom delta.utils.register import registers\nfrom delta import utils\n\n# pylint: disable=abstract-method, too-many-ancestors, too-many-instance-attributes\n\n\nclass SeqclassModel(TextClassModel):\n  """"""Sequence model for text classification.""""""\n\n  def __init__(self, config, **kwargs):\n    super().__init__(config, **kwargs)\n\n    logging.info(""Initialize SeqclassModel"")\n\n    self.use_pretrained_embedding = config[\'model\'][\'use_pre_train_emb\']\n    if self.use_pretrained_embedding:\n      self.embedding_path = config[\'model\'][\'embedding_path\']\n      logging.info(""Loading embedding file from: {}"".format(\n          self.embedding_path))\n      self._word_embedding_init = pickle.load(open(self.embedding_path, \'rb\'))\n      self.embed_initializer = tf.constant_initializer(\n          self._word_embedding_init)\n    else:\n      self.embed_initializer = tf.random_uniform_initializer(-0.1, 0.1)\n\n\n@registers.model.register\nclass SeqclassCNNModel(SeqclassModel):\n  """"""CNN model for text classification.""""""\n\n  def __init__(self, config, **kwargs):\n    super().__init__(config, **kwargs)\n    model_config = config[\'model\'][\'net\'][\'structure\']\n    self.dropout_rate = model_config[\'dropout_rate\']\n\n    self.sequence_length = config[\'data\'][\'task\'][\'max_seq_len\']\n    self.vocab_size = config[\'data\'][\'vocab_size\']\n    self.num_classes = config[\'data\'][\'task\'][\'classes\'][\'num_classes\']\n\n    self.embedding_size = model_config[\'embedding_size\']\n    self.num_units = model_config[\'num_units\']\n    self.num_layers = model_config[\'num_layers\']\n    self.filter_sizes = model_config[\'filter_sizes\']\n    self.num_filters = model_config[\'num_filters\']\n\n    self.l2_reg_lambda = model_config[\'l2_reg_lambda\']\n\n    self.embed = tf.keras.layers.Embedding(\n        self.vocab_size,\n        self.embedding_size,\n        embeddings_initializer=self.embed_initializer)\n\n    self.conv2ds = []\n    self.pools = []\n    for i, filter_size in enumerate(self.filter_sizes):\n      conv2d = tf.keras.layers.Conv2D(\n          filters=self.num_filters,\n          kernel_size=(filter_size, self.embedding_size),\n          kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n          bias_initializer=tf.constant_initializer(value=0.0),\n          padding=\'valid\',\n          name=\'conv_{}\'.format(i))\n      pool = tf.keras.layers.MaxPool2D(\n          pool_size=(self.sequence_length - filter_size + 1, 1),\n          strides=(1, 1),\n          padding=\'valid\',\n          name=\'name_{}\'.format(i))\n      self.conv2ds.append(conv2d)\n      self.pools.append(pool)\n\n    self.flat = tf.keras.layers.Flatten()\n\n    self.dense = tf.keras.layers.Dense(64, activation=tf.keras.activations.relu)\n\n    self.dropout = tf.keras.layers.Dropout(rate=self.dropout_rate)\n\n    self.final_dense = tf.keras.layers.Dense(\n        self.num_classes, activation=tf.keras.activations.linear)\n\n  def call(self, inputs, training=None, mask=None):\n    input_x = tf.identity(inputs[""input_x""], name=""input_x"")\n    if self.use_dense_task:\n      dense_input = inputs[""input_dense""]\n    embed = self.embed(input_x)\n    embed_expand = tf.expand_dims(embed, axis=-1)\n    conv_outs = [conv2d(embed_expand) for conv2d in self.conv2ds]\n    pool_outs = [pool(co) for co, pool in zip(conv_outs, self.pools)]\n    out = tf.keras.layers.Concatenate(axis=1)(pool_outs)\n    out = self.flat(out)\n    out = self.dropout(out, training=training)\n    out = self.dense(out)\n    if self.use_dense_input:\n      dense_out = self.dense_input_linear(dense_input)\n      if self.only_dense_input:\n        out = dense_out\n      else:\n        out = tf.keras.layers.Concatenate()([out, dense_out])\n    scores = self.final_dense(out)\n    return scores\n\n\n@registers.model.register\nclass RnnAttentionModel(SeqclassModel):\n  """"""RNN model for text classification.""""""\n\n  def __init__(self, config, **kwargs):\n    super(RnnAttentionModel, self).__init__(config, **kwargs)\n\n    logging.info(""Initialize RnnAttentionModel..."")\n\n    self.vocab_size = config[\'data\'][\'vocab_size\']\n    self.num_classes = config[\'data\'][\'task\'][\'classes\'][\'num_classes\']\n\n    model_config = config[\'model\'][\'net\'][\'structure\']\n    self.dropout_rate = model_config[\'dropout_rate\']\n    self.embedding_size = model_config[\'embedding_size\']\n    self.num_layers = model_config[\'num_layers\']\n    self.l2_reg_lambda = model_config[\'l2_reg_lambda\']\n    self.batch_size = model_config[\'batch_size\']\n    self.max_len = model_config[\'max_len\']\n\n    self.embed = tf.keras.layers.Embedding(\n        self.vocab_size,\n        self.embedding_size,\n        embeddings_initializer=self.embed_initializer)\n\n    self.embed_d = tf.keras.layers.Dropout(self.dropout_rate)\n    self.encoder = layers.RnnAttentionEncoder(config, name=""encoder"")\n    self.final_dense = tf.keras.layers.Dense(\n        self.num_classes,\n        activation=tf.keras.activations.linear,\n        name=""final_dense"")\n    logging.info(""Initialize RnnAttentionModel done."")\n\n  @staticmethod\n  def compute_lens(inputs, max_len):\n    """"""count sequence length.\n    input: [batch_size, max_len]\n    lens: [batch_size]\n    """"""\n\n    x_binary = tf.cast(tf.cast(tf.reverse(inputs, axis=[1]), tf.bool), tf.int32)\n    lens = max_len - tf.argmax(x_binary, axis=1, output_type=tf.int32)\n\n    zeros = tf.zeros_like(lens, dtype=tf.int32)\n    x_sum = tf.reduce_sum(inputs, axis=1)\n    sen_lens = tf.where(tf.equal(x_sum, 0), zeros, lens)\n    return sen_lens\n\n  def call(self, inputs, training=None, mask=None):\n    input_x = inputs[""input_x""]\n    if self.use_dense_task:\n      dense_input = inputs[""input_dense""]\n\n    # [batch_size]\n    lens = self.compute_lens(input_x, self.max_len)\n\n    # [batch_size, max_len, 1]\n    mask = tf.expand_dims(\n        tf.sequence_mask(lens, self.max_len, dtype=tf.float32), axis=-1)\n\n    # [batch_size, max_len, embed_len]\n    out = self.embed(input_x)\n    out = self.embed_d(out, training=training)\n    # [batch_size, features]\n    out = self.encoder(out, training=training, mask=mask)\n    if self.use_dense_input:\n      dense_out = self.dense_input_linear(dense_input)\n      if self.only_dense_input:\n        out = dense_out\n      else:\n        out = tf.keras.layers.Concatenate()([out, dense_out])\n    # [batch_size, class_num]\n    scores = self.final_dense(out)\n    return scores\n\n\n@registers.model.register\nclass TransformerModel(SeqclassModel):\n  """"""Transformer model for text classification""""""\n\n  def __init__(self, config, **kwargs):\n    super().__init__(config, **kwargs)\n    logging.info(""Initialize TransformerModel..."")\n\n    self.vocab_size = config[\'data\'][\'vocab_size\']\n    self.num_classes = config[\'data\'][\'task\'][\'classes\'][\'num_classes\']\n\n    model_config = config[\'model\'][\'net\'][\'structure\']\n    self.dropout_rate = model_config[\'dropout_rate\']\n    self.embedding_size = model_config[\'embedding_size\']\n    self.num_layers = model_config[\'num_layers\']\n    self.l2_reg_lambda = model_config[\'l2_reg_lambda\']\n    self.max_len = model_config[\'max_len\']\n    self.transformer_dropout = model_config[\'transformer_dropout\']\n    self.residual_conn = model_config[\'residual_conn\']\n    self.head_num = model_config[\'head_num\']\n    self.hidden_dim = model_config[\'hidden_dim\']\n    self.padding_token = utils.PAD_IDX\n\n    self.mask_layer = tf.keras.layers.Lambda(lambda inputs: tf.cast(\n        tf.not_equal(inputs, self.padding_token), tf.int32))\n    self.embed = tf.keras.layers.Embedding(\n        self.vocab_size,\n        self.embedding_size,\n        embeddings_initializer=self.embed_initializer)\n\n    self.pos_embed = layers.PositionEmbedding(self.max_len, self.embedding_size)\n\n    self.embed_d = tf.keras.layers.Dropout(self.dropout_rate)\n\n    self.transformer_encoder = layers.TransformerEncoder(config)\n\n    self.pool = tf.keras.layers.GlobalMaxPooling1D()\n\n    self.final_dense = tf.keras.layers.Dense(\n        self.num_classes,\n        activation=tf.keras.activations.linear,\n        name=""final_dense"")\n    logging.info(""Initialize TransformerModel done."")\n\n  def call(self, inputs, training=None, mask=None):\n    input_x = inputs[""input_x""]\n    if self.use_dense_task:\n      dense_input = inputs[""input_dense""]\n\n    enc_mask = self.mask_layer(input_x)\n    emb = self.embed(input_x)\n    pos_emb = self.pos_embed(emb)\n    emb = tf.keras.layers.add([emb, pos_emb])\n    enc_emb = self.embed_d(emb, training=training)\n\n    enc_out = self.encoder(enc_emb, training=training, mask=enc_mask)\n\n    out = self.pool(enc_out)\n\n    if self.use_dense_input:\n      dense_out = self.dense_input_linear(dense_input)\n      if self.only_dense_input:\n        out = dense_out\n      else:\n        out = tf.keras.layers.Concatenate()([out, dense_out])\n\n    scores = self.final_dense(out)\n    return scores\n\n\n@registers.model.register\nclass FullyConnectModel(SeqclassModel):\n  """"""FullyConnect model for text classification based on\n  pretrain embedding/model""""""\n\n  def __init__(self, config, **kwargs):\n    super().__init__(config, **kwargs)\n    logging.info(""Initialize FullyConnectModel..."")\n\n    self.vocab_size = config[\'data\'][\'vocab_size\']\n    self.num_classes = config[\'data\'][\'task\'][\'classes\'][\'num_classes\']\n\n    model_config = config[\'model\'][\'net\'][\'structure\']\n    self.dropout_rate = model_config[\'dropout_rate\']\n    self.embedding_size = model_config[\'embedding_size\']\n    self.num_layers = model_config[\'num_layers\']\n    self.l2_reg_lambda = model_config[\'l2_reg_lambda\']\n    self.max_len = model_config[\'max_len\']\n\n    self.embed = tf.keras.layers.Embedding(\n        self.vocab_size,\n        self.embedding_size,\n        embeddings_initializer=self.embed_initializer)\n\n    self.embed_d = tf.keras.layers.Dropout(self.dropout_rate)\n    self.final_dense = tf.keras.layers.Dense(\n        self.num_classes,\n        activation=tf.keras.activations.linear,\n        name=""final_dense"")\n    self.dropout = tf.keras.layers.Dropout(self.dropout_rate)\n\n    logging.info(""Initialize FullyConnectModel done."")\n\n  def call(self, inputs, training=None, mask=None):\n    input_x = inputs[""input_x""]\n    if self.use_dense_task:\n      dense_input = inputs[""input_dense""]\n\n    # [batch_size, max_len, embed_len]\n    out = self.embed(input_x)\n    if self.use_pretrained_model:\n      logging.info(""use_pretrained_model: {}, {}"".format(\n          self.pretrained_model_name, self.pretrained_model_mode))\n      if self.pretrained_model_name == \'elmo\':\n        input_px = self.get_pre_train_graph(input_x)\n        input_px = tf.reshape(input_px,\n                              [-1, self.max_len, self.pretrained_model_dim])\n        out = tf.concat([out, input_px], axis=-1)\n        out = tf.reduce_max(out, axis=1)\n      if self.pretrained_model_name == \'bert\':\n        out = self.get_pre_train_graph(input_x)\n    else:\n      out = tf.reduce_max(out, axis=1)\n    out = self.embed_d(out, training=training)\n    if self.use_dense_input:\n      dense_out = self.dense_input_linear(dense_input)\n      if self.only_dense_input:\n        out = dense_out\n      else:\n        out = tf.keras.layers.Concatenate()([out, dense_out])\n    # [batch_size, class_num]\n    scores = self.final_dense(out)\n    return scores\n'"
delta/serving/__init__.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' delta.serving \'\'\'\n'"
delta/serving/base_frozen_model.py,11,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' base frozen model \'\'\'\nimport os\nimport sys\nimport abc\nfrom absl import logging\nimport delta.compat as tf\n\nfrom delta import utils\n\n\nclass ABCFrozenModel(metaclass=abc.ABCMeta):\n  \'\'\'Abstract class of FrozenModel\'\'\'\n\n  @abc.abstractmethod\n  def init_session(self, model, gpu_str):\n    \'\'\' init_session \'\'\'\n    raise NotImplementedError()\n\n  @property\n  @abc.abstractmethod\n  def graph(self):\n    \'\'\' graph \'\'\'\n    raise NotImplementedError()\n\n  @property\n  @abc.abstractmethod\n  def sess(self):\n    \'\'\' sess \'\'\'\n    raise NotImplementedError()\n\n\nclass FrozenModel(ABCFrozenModel):\n  \'\'\'FrozenModel\'\'\'\n\n  def __init__(self, model, gpu_str=None):\n    \'\'\'\n     model: saved model dir, ckpt dir or frozen_graph_pb path\n     gpu_str: list of gpu devices. e.g. \'\' for cpu, \'0,1\' for gpu 0,1\n    \'\'\'\n    self.init_session(model, gpu_str)\n\n  def init_session(self, model, gpu_str):\n    # The config for CPU usage\n    config = tf.ConfigProto()\n    if not gpu_str:\n      config.gpu_options.visible_device_list = \'\'  # pylint: disable=no-member\n    else:\n      config.gpu_options.visible_device_list = gpu_str  # pylint: disable=no-member\n      config.gpu_options.allow_growth = True  # pylint: disable=no-member\n\n    #check model dir\n    if os.path.isdir(model):\n      self._graph = tf.Graph()\n\n      if tf.saved_model.maybe_saved_model_directory(model):\n        #saved model\n        logging.info(\'saved model dir: {}\'.format(model))\n        self._sess = tf.Session(graph=self._graph, config=config)\n        tf.saved_model.loader.load(self._sess,\n                                   [tf.saved_model.tag_constants.SERVING],\n                                   model)\n      else:\n        #checkpoint\n        self._sess = tf.Session(\n            graph=self._graph,\n            config=tf.ConfigProto(\n                allow_soft_placement=True, log_device_placement=True))\n        ckpt_path = tf.train.latest_checkpoint(model)\n        # self._graph, self._sess = utils.load_graph_session_from_ckpt(ckpt_path)\n        model = ckpt_path + \'.meta\'\n        logging.info(""meta : {}"".format(model))\n        saver = tf.train.import_meta_graph(model)\n        saver.restore(self._sess, ckpt_path)\n\n    else:\n      if not os.path.exists(model):\n        logging.info(\'{}, is not exist\'.format(model))\n        logging.info(""frozen_graph : {} not exist"".format(model))\n        sys.exit(0)\n\n      #frozen graph pb\n      frozen_graph = model\n      logging.info(\'frozen graph pb : {}\'.format(frozen_graph))\n      self._graph = utils.load_frozen_graph(frozen_graph)\n      self._sess = tf.Session(graph=self._graph, config=config)\n\n  def inspect_ops(self):\n    for op in self._graph.get_operations():\n      logging.info(f""ops: {op.name}"")\n\n  def debug(self):\n    feed_dict = self.get_test_feed_dict()\n    while True:\n      tensor_name = input(""Input debug tensor name: "").strip()\n      if tensor_name == ""q"":\n        sys.exit(0)\n      try:\n        debug_tensor = self.graph.get_tensor_by_name(tensor_name)\n      except Exception as e:\n        logging.error(e)\n        continue\n      res = self.sess.run(debug_tensor, feed_dict=feed_dict)\n      logging.info(f""Result for tensor {tensor_name} is: {res}"")\n\n  @property\n  def graph(self):\n    return self._graph\n\n  @property\n  def sess(self):\n    return self._sess\n\n\nclass Evaluater(FrozenModel):\n\n  @abc.abstractmethod\n  def predict(self):\n    raise NotImplementedError()\n'"
delta/serving/eval_asr_pb.py,1,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Frozen ASR model Evaluater\'\'\'\nimport delta.compat as tf\nfrom absl import logging\n\nfrom delta import utils\nfrom delta.utils import metrics as metrics_lib\nfrom delta.utils.register import registers\nfrom delta.utils.register import import_all_modules_for_register\nfrom delta.serving.base_frozen_model import Evaluater\n\n\n@registers.serving.register\nclass ASREvaluater(Evaluater):\n  \'\'\' infer from forzen model \'\'\'\n\n  def __init__(self, config, gpu_str=None, mode=utils.INFER):\n    self._config = config\n    self._mode = mode\n    model = config[\'serving\'][\'model\']\n    super().__init__(model, gpu_str=\'0\')\n\n    self.inputs = self.graph.get_tensor_by_name(config[\'serving\'][\'inputs\'])\n    self.input_length = self.graph.get_tensor_by_name(\n        config[\'serving\'][\'input_length\'])\n    self.pred_valid = self.graph.get_tensor_by_name(\n        config[\'serving\'][\'outputs\'])\n\n  @property\n  def config(self):\n    \'\'\' config \'\'\'\n    return self._config\n\n  #pylint: disable=too-many-locals\n  def predict(self):\n    \'\'\' infer prediction results \'\'\'\n    batch = 0\n\n    solver_name = self.config[\'solver\'][\'name\']\n    solver = registers.solver[solver_name](self.config)\n\n    with self.graph.as_default():\n      dataset, _ = solver.input_data(self._mode)\n      iterator = dataset.make_one_shot_iterator()\n      next_element = iterator.get_next()\n\n    target_seq_list, predict_seq_list = [], []\n    num_input_samples, num_processed_samples = 0, 0\n    try:\n      while True:\n        batch += 1\n        logging.info(""batch : {}"".format(batch))\n\n        features, _ = self.sess.run(next_element)\n        inputs = features[""inputs""]\n        input_length = features[""input_length""]\n        y_true_valid = features[""targets""]\n        num_input_samples += input_length.shape[0]\n        logging.info(\'The size of the INFER Set increased to {}\'.format(\n            num_input_samples))\n\n        validate_feed = {self.inputs: inputs, self.input_length: input_length}\n        y_pred_valid = self.sess.run(self.pred_valid, feed_dict=validate_feed)\n        num_processed_samples += y_pred_valid.shape[0]\n        logging.info(\n            \'A total of {} samples has been successfully processed\'.format(\n                num_processed_samples))\n\n        target_seq_list.extend(y_true_valid.tolist())\n        predict_seq_list.extend(y_pred_valid.tolist())\n\n    except tf.errors.OutOfRangeError:\n      logging.info(""Infer End"")\n\n    token_errors = metrics_lib.token_error(\n        predict_seq_list=predict_seq_list,\n        target_seq_list=target_seq_list,\n        eos_id=0)\n    logging.info(\'Token ERR: {}\'.format(token_errors))\n'"
delta/serving/eval_speech_cls_pb.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Frozen model Evaluater\'\'\'\nimport numpy as np\nfrom absl import logging\n\nimport delta.compat as tf\nfrom delta import utils\nfrom delta.utils.register import registers\nfrom delta.utils.register import import_all_modules_for_register\nfrom delta.serving.base_frozen_model import Evaluater\n\n\nclass ClsMetric:\n\n  def __init__(self):\n    self.TP = 0\n    self.TN = 0\n    self.FP = 0\n    self.FN = 0\n\n  def __call__(self, y_pred, y_true):\n    for i, _ in enumerate(y_true):\n      #positive\n      if y_true[i] == 1:\n        if y_true[i] == y_pred[i]:\n          self.TP += 1\n        else:\n          self.FN += 1\n      #Negative\n      else:\n        if y_true[i] == y_pred[i]:\n          self.TN += 1\n        else:\n          self.FP += 1\n\n  def result(self, log_verbosity=False):\n    if log_verbosity:\n      logging.info(\'TP {}\'.format(self.TP))\n      logging.info(\'TN {}\'.format(self.TN))\n      logging.info(\'FP {}\'.format(self.FP))\n      logging.info(\'FN {}\'.format(self.FN))\n    acc = (self.TP + self.TN) / (self.TP + self.TN + self.FP + self.FN)\n    precision = self.TP / (self.TP + self.FP)\n    recall = self.TP / (self.TP + self.FN)\n    return acc, precision, recall\n\n\nclass SpeechEvaluater(Evaluater):\n  \'\'\' base evaluater \'\'\'\n\n  def __init__(self, config, gpu_str=None, mode=utils.INFER):\n    self._config = config\n    self._mode = mode\n    model = config[\'serving\'][\'model\']\n    super().__init__(model, gpu_str=gpu_str)\n    input_name = config[\'serving\'][\'inputs\']\n    output_name = config[\'serving\'][\'outputs\']\n\n    self.audio_ph = self.graph.get_tensor_by_name(input_name)\n    self.pred_valid = self.graph.get_tensor_by_name(output_name)\n\n    self.metric = ClsMetric()\n\n    self.inspect_ops()\n    self.build()\n\n  @property\n  def config(self):\n    \'\'\' config \'\'\'\n    return self._config\n\n  def build(self):\n    \'\'\' build graph \'\'\'\n    solver_name = self.config[\'solver\'][\'name\']\n    solver = registers.solver[solver_name](self.config)\n\n    with self.graph.as_default():\n      dataset = solver.input_fn(self._mode)()\n      iterator = dataset.make_one_shot_iterator()\n      self.next_element = iterator.get_next()\n\n  def postproc(self, pred, features=None):\n    \'\'\' prost processing \'\'\'\n    result = np.argmax(pred, axis=-1)\n    return result\n\n  def run(self):\n    \'\'\' featch predictions \'\'\'\n    features, y_true = self.sess.run(self.next_element)\n    inputs = features[""inputs""]\n    pred = self.sess.run(self.pred_valid, feed_dict={self.audio_ph: inputs})\n    y_pred = self.postproc(pred, features=features)\n    return y_pred, y_true\n\n\n@registers.serving.register\nclass EmoSpeechEvaluater(SpeechEvaluater):\n  \'\'\' infer from forzen model \'\'\'\n\n  def __init__(self, config, gpu_str, mode):\n    super().__init__(config, gpu_str, mode)\n\n  def predict(self):\n    \'\'\' infer prediction results and/or log metrics \'\'\'\n    batch = 0\n    try:\n      while True:\n        batch += 1\n        logging.info(""process {} batches"".format(batch))\n        y_pred, y_true = self.run()\n        if self._mode == utils.EVAL:\n          self.metric(y_pred, y_true)\n    except tf.errors.OutOfRangeError:\n      logging.info(""Process End"")\n\n    if self._mode == utils.EVAL:\n      acc, precision, recall = self.metric.result()\n      logging.info(\'acc {}\'.format(acc))\n      logging.info(\'precision {}\'.format(precision))\n      logging.info(\'recall {}\'.format(recall))\n\n\n@registers.serving.register\nclass SpkSpeechEvaluater(SpeechEvaluater):\n  \'\'\' infer from forzen model \'\'\'\n\n  def __init__(self, config, gpu_str, mode):\n    super().__init__(config, gpu_str, mode)\n\n    postproc_name = self.config[\'solver\'][\'postproc\'][\'name\']\n    self.post_fn = registers.postprocess[postproc_name](self.config)\n\n  def postproc(self, pred, features=None):\n    \'\'\' prost processing \'\'\'\n    self.post_fn(pred)\n    return\n\n  def run(self):\n    \'\'\' featch predictions \'\'\'\n\n    def gen():\n      features, y_true = self.sess.run(self.next_element)\n      inputs = features[""inputs""]\n      pred = self.sess.run(self.pred_valid, feed_dict={self.audio_ph: inputs})\n      features.update({\'embeddings\': pred})\n      return features\n\n    class Iter:\n\n      def __iter__(self):\n        return self\n\n      def __next__(self):\n        return gen()\n\n    self.postproc(Iter())\n    return None, None\n\n  def predict(self):\n    \'\'\' extract speaker embedding \'\'\'\n    batch = 0\n    try:\n      while True:\n        batch += 1\n        logging.info(""process {} batches"".format(batch))\n        self.run()\n    except tf.errors.OutOfRangeError:\n      logging.info(""Process End"")\n'"
delta/serving/eval_text_cls_pb.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Frozen text classification model Evaluater\'\'\'\nimport os\nimport delta.compat as tf\nfrom absl import logging\n\nfrom delta import utils\nfrom delta.utils.register import registers\nfrom delta.utils.register import import_all_modules_for_register\nfrom delta.serving.base_frozen_model import Evaluater\n\n\n@registers.serving.register\nclass TextClsEvaluater(Evaluater):\n  \'\'\' infer from forzen model \'\'\'\n\n  def __init__(self, config, gpu_str=None, mode=utils.INFER):\n    self._config = config\n    self._mode = mode\n    model = os.path.join(config[\'solver\'][\'service\'][\'model_path\'],\n                         config[\'solver\'][\'service\'][\'model_version\'])\n    super().__init__(model, gpu_str=gpu_str)\n\n    self.inspect_ops()\n\n    self.input_sentence = self.graph.get_tensor_by_name(\n        config[\'solver\'][\'service\'][\'input_sentence\'])\n    self.input_x = self.graph.get_tensor_by_name(\n        config[\'solver\'][\'service\'][\'input_x\'])\n    self.score = self.graph.get_tensor_by_name(\n        config[\'solver\'][\'service\'][\'score\'])\n    self.preds = self.graph.get_tensor_by_name(\n        config[\'solver\'][\'service\'][\'preds\'])\n\n  @property\n  def config(self):\n    \'\'\' config \'\'\'\n    return self._config\n\n  def get_test_feed_dict(self):\n    return {self.input_sentence: [""\xe4\xbd\xa0\xe5\xa5\xbd"", ""\xe5\xbe\x88\xe5\xbc\x80\xe5\xbf\x83""]}\n\n  def predict(self):\n    feed_dict = self.get_test_feed_dict()\n\n    input_x, score, preds = self.sess.run(\n        [self.input_x, self.score, self.preds], feed_dict=feed_dict)\n    logging.info(f""input_x: {input_x}"")\n    logging.info(f""preds: {preds}"")\n    logging.info(f""score: {score}"")\n'"
delta/serving/knowledge_distilling.py,1,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\'Teacher module\'\'\'\nimport numpy as np\nimport delta.compat as tf\n\nfrom delta.utils.register import registers\nfrom delta.serving.base_frozen_model import FrozenModel\n\n\n@registers.serving.register\nclass Teacher(FrozenModel):\n  \'\'\'class of Teacher\'\'\'\n\n  def __init__(self, model, gpu_str=None, temperature=1.0):\n    \'\'\'\n     model: saved model dir, ckpt dir or frozen_graph_pb path\n     gpu_str: list of gpu devices. e.g. \'\' for cpu, \'0,1\' for gpu 0,1\n    \'\'\'\n    super().__init__(model, gpu_str)\n\n    self.temperature = temperature\n    self.build()\n\n  def build(self):\n    \'\'\'build\'\'\'\n    self.audio_ph = self.graph.get_tensor_by_name(\'inputs:0\')\n    self.logits = self.graph.get_tensor_by_name(\n        \'model/logits/logits-matmul/add:0\')\n\n    with self.graph.as_default():\n      self.soft_label = tf.nn.softmax(self.logits / self.temperature)\n\n  def __call__(self, feat):\n    \'\'\' generate soft labels per example \'\'\'\n    # shape [1, T, D, C]\n    inputs = feat[np.newaxis, :, :, :]\n    validate_feed = {\n        self.audio_ph: inputs,\n    }\n    soft_label = self.sess.run(self.soft_label, feed_dict=validate_feed)\n    return soft_label[0]\n'"
delta/utils/__init__.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' utils module \'\'\'\nfrom delta.utils.cmvn import *\nfrom delta.utils.misc import *\nfrom delta.utils.plot import *\nfrom delta.utils.model import *\nfrom delta.utils.config import *\nfrom delta.utils.logger import *\n'"
delta/utils/cmvn.py,7,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' global CMVN functions \'\'\'\nimport numpy as np\nimport delta.compat as tf\n#pylint: disable=no-name-in-module\nfrom tensorflow.python.keras import backend as keras_backend\n\n\ndef create_cmvn_statis(feature_size, add_delta_deltas=True):\n  \'\'\' init sums, squares and cout of feature statistic \'\'\'\n  sums = np.zeros([1, feature_size, 3 if add_delta_deltas else 1],\n                  dtype=np.float64)\n  square = np.zeros([1, feature_size, 3 if add_delta_deltas else 1],\n                    dtype=np.float64)\n  count = 0.0\n  return sums, square, count\n\n\ndef update_cmvn_statis(feat, sums, square, count, axis=(0, 1)):\n  \'\'\' aggregate CMVN statistic \'\'\'\n  # feat shape [ batch, frames, feat, channle]\n  assert feat.ndim == 4\n  sums += np.expand_dims(np.sum(feat, axis=axis), axis=0)\n  square += np.expand_dims(np.sum(np.square(feat), axis=axis), axis=0)\n  count += np.prod(feat.shape[:len(axis)])\n  return sums, square, count\n\n\ndef compute_cmvn(sums, square, count):\n  \'\'\' compute global feature mean and variance\n     vars = E(x^2) - (E(x))^2\n  \'\'\'\n  mean = sums / count\n  var = (square / count) - np.square(mean)\n  return mean, var\n\n\ndef load_cmvn(path):\n  \'\'\' load mean and variance from cmvn.npy,\n      then convert to TF Tensor\n  \'\'\'\n  # [1, nbins, nchannels]\n  mean, variance = np.load(path)\n  # [1, 1, nbins, nchannels]\n  mean = np.expand_dims(mean, axis=0)\n  variance = np.expand_dims(variance, axis=0)\n  mean = tf.convert_to_tensor(mean, dtype=tf.float32, name=\'cmvn_mean\')\n  variance = tf.convert_to_tensor(\n      variance, dtype=tf.float32, name=\'cmvn_variance\')\n  return mean, variance\n\n\ndef apply_cmvn(feats, mean, variance, epsilon=1e-9):\n  \'\'\' TF: apply CMVN on feature\'\'\'\n  return (feats - mean) * tf.rsqrt(variance + epsilon)\n\n\ndef apply_local_cmvn(feats, epsilon=1e-9):\n  \'\'\' feats: (NHWC) \'\'\'\n  mean = tf.expand_dims(keras_backend.mean(feats, axis=1), axis=1)\n  var = tf.expand_dims(keras_backend.var(feats, axis=1), axis=1)\n  feats = (feats - mean) * tf.rsqrt(var + epsilon)\n  return feats\n'"
delta/utils/cmvn_test.py,8,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' CMVN unittest\'\'\'\nimport os\nimport numpy as np\nimport delta.compat as tf\n\nfrom delta import utils\n\n\nclass CmvnTest(tf.test.TestCase):\n  \'\'\' CMVN unittest Class\'\'\'\n\n  def setUp(self):\n    super().setUp()\n    \'\'\' setup \'\'\'\n\n  def tearDown(self):\n    \'\'\' tear down \'\'\'\n\n  def testCreateCmvnStatis(self):  #pylint: disable=invalid-name\n    \'\'\' test creat_cmvn_statics \'\'\'\n    feat_size = 40\n    delta_deltas = True\n\n    sums, square, count = utils.create_cmvn_statis(feat_size, delta_deltas)\n\n    self.assertAllEqual(sums.shape, [1, feat_size, 3])\n    self.assertAllEqual(square.shape, [1, feat_size, 3])\n    self.assertAllEqual(count, 0)\n\n    delta_deltas = False\n    sums, square, count = utils.create_cmvn_statis(feat_size, delta_deltas)\n    self.assertAllEqual(sums.shape, [1, feat_size, 1])\n    self.assertAllEqual(square.shape, [1, feat_size, 1])\n    self.assertAllEqual(count, 0)\n\n  def testUpdateCmvnStatis(self):  #pylint: disable=invalid-name\n    \'\'\' test update cmvn statics \'\'\'\n    np.random.seed(12)\n    feat_size = 40\n    delta_deltas = True\n    shape = [2, 10, feat_size, 3 if delta_deltas else 1]\n\n    sums, square, count = utils.create_cmvn_statis(feat_size, delta_deltas)\n\n    feat = np.random.randn(*shape)\n    sums_true = np.expand_dims(np.sum(feat, axis=(0, 1)), axis=0)\n    square_true = np.expand_dims(np.sum(np.square(feat), axis=(0, 1)), axis=0)\n    count_true = np.prod(shape[:2])\n\n    sums, square, count = utils.update_cmvn_statis(feat, sums, square, count)\n\n    self.assertAllEqual(sums, sums_true)\n    self.assertAllEqual(square, square_true)\n    self.assertAllEqual(count, count_true)\n\n  def testComputeCmvn(self):  #pylint: disable=invalid-name\n    \'\'\' test compute cmvn \'\'\'\n    np.random.seed(12)\n    feat_size = 40\n    delta_deltas = True\n    shape = [2, 10, feat_size, 3 if delta_deltas else 1]\n\n    sums, square, count = utils.create_cmvn_statis(feat_size, delta_deltas)\n\n    feat = np.random.randn(*shape)\n    feat = feat.astype(np.float32)\n    sums, square, count = utils.update_cmvn_statis(feat, sums, square, count)\n    mean, var = utils.compute_cmvn(sums, square, count)\n    mean_true, var_true = np.mean(feat, axis=(0, 1)), np.var(feat, axis=(0, 1))\n\n    self.assertAllEqual(mean.shape, [1] + shape[2:])\n    self.assertAllClose(np.squeeze(mean, axis=0), mean_true)\n    self.assertAllClose(np.squeeze(var, axis=0), var_true)\n\n  def testLoadCmvn(self):  #pylint: disable=invalid-name\n    \'\'\' test load cmvn \'\'\'\n    np.random.seed(12)\n    temp_dir = self.get_temp_dir()\n    temp_file = os.path.join(temp_dir, \'cmvn.npy\')\n\n    feat_size = 40\n    delta_deltas = True\n    shape = [1, feat_size, 3 if delta_deltas else 1]\n    mean = np.random.randn(*shape)\n    var = np.random.randn(*shape)\n    mean, var = mean.astype(np.float32), var.astype(np.float32)\n    with tf.gfile.Open(temp_file, \'w\') as f:  #pylint: disable=invalid-name\n      np.save(f, (mean, var))\n\n    mean_true = np.expand_dims(mean, axis=0)\n    var_true = np.expand_dims(var, axis=0)\n\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      mean, var = utils.load_cmvn(temp_file)\n      self.assertAllClose(mean.eval(), mean_true)\n      self.assertAllClose(var.eval(), var_true)\n\n  def testApplyCmvn(self):  #pylint: disable=invalid-name\n    \'\'\' test apply cmvn \'\'\'\n    np.random.seed(12)\n    tf.set_random_seed(12)\n\n    feat_size = 40\n    delta_deltas = True\n\n    feat_shape = [2, 10, feat_size, 3 if delta_deltas else 1]\n    feat = np.random.randn(*feat_shape)\n    feat = feat.astype(np.float32)\n\n    feat = tf.constant(feat)\n    mean = feat / 2\n    var = feat / 3\n\n    eps = 1e-9\n    feat_out = utils.apply_cmvn(feat, mean, var, epsilon=eps)\n    feat_true = (feat - mean) * tf.rsqrt(var + eps)\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      self.assertAllClose(feat_out.eval(), feat_true.eval())\n\n  def testApplyLocalCmvn(self):  #pylint: disable=invalid-name\n    \'\'\' test apply_local_cmvn() \'\'\'\n    np.random.seed(12)\n    tf.set_random_seed(12)\n\n    feat_size = 40\n    delta_deltas = True\n\n    feat_shape = [2, 10, feat_size, 3 if delta_deltas else 1]\n    feat = np.random.randn(*feat_shape)\n    feat = feat.astype(np.float32)\n\n    mean = np.mean(feat, axis=1, keepdims=True)\n    var = np.var(feat, axis=1, keepdims=True)\n    eps = 1e-9\n    feat_true = (feat - mean) / np.sqrt(var + eps)\n\n    feat = tf.constant(feat)\n\n    feat_out = utils.apply_local_cmvn(feat, epsilon=eps)\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      self.assertAllClose(feat_out.eval(), feat_true)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
delta/utils/config.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' configration utils\'\'\'\nfrom typing import List, Union\nimport os\nimport json\nimport time\nfrom shutil import copyfile, SameFileError\nfrom pathlib import Path\nimport yaml\nfrom absl import logging\n\n\ndef valid_config(config):\n  \'\'\' validation config\'\'\'\n  del config\n  return True\n\n\ndef config_join_project_path(project_dir: str, config: dict,\n                             key_path: List[Union[str, int]]):\n  """"""join project dir on a path""""""\n  d = config\n  try:\n    for k in key_path[:-1]:\n      d = d[k]\n    original_path = d[key_path[-1]]\n  except KeyError as e:\n    logging.warning(f""key_path: {key_path} not found!"")\n    raise KeyError(repr(e))\n  if isinstance(original_path, list):\n    d[key_path[-1]] = [os.path.join(project_dir, p) for p in original_path]\n  elif isinstance(original_path, str):\n    d[key_path[-1]] = os.path.join(project_dir, original_path)\n  else:\n    logging.warning(f""key_path: {key_path} error."")\n    raise TypeError(""path is not str or list!"")\n\ndef config_join_project_dir(config):\n  """"""operations after the config been loaded.""""""\n  if \'data\' not in config or ""project_dir"" not in config[\'data\']:\n    return\n  project_dir = config[\'data\'][""project_dir""]\n  file_key_paths = [[\'data\', \'task\', \'preparer\', \'done_sign\'],\n                    [\'data\', \'task\', \'text_vocab\'],\n                    [\'data\', \'task\', \'label_vocab\'],\n                    [\'solver\', \'service\', \'model_path\'],\n                    [\'solver\', \'saver\', \'model_path\']]\n\n  for data_type in [\'train\', \'eval\', \'infer\']:\n    if isinstance(config[\'data\'][data_type][\'paths\'], dict):\n      for k in config[\'data\'][data_type][\'paths\']:\n        file_key_paths.append([\'data\', data_type, \'paths\', k])\n    else:\n      file_key_paths.append([\'data\', data_type, \'paths\'])\n\n  if isinstance(config[\'solver\'][\'metrics\'], dict):\n    metric = config[\'solver\'][\'metrics\']\n    if ""target_file"" in metric:\n      file_key_paths.append([\'solver\', \'metrics\', \'target_file\'])\n    if ""text_vocab"" in metric:\n      file_key_paths.append([\'solver\', \'metrics\', \'text_vocab\'])\n    if ""res_file"" in metric:\n      file_key_paths.append([\'solver\', \'metrics\', \'res_file\'])\n    for j, cal in enumerate(metric[\'cals\']):\n      if cal[\'arguments\'] is not None and \'label_vocab_path\' in cal[\'arguments\']:\n        file_key_paths.append([\'solver\', \'metrics\', \'cals\', j, \'arguments\', \'label_vocab_path\'])\n  else:\n    for i, metric in enumerate(config[\'solver\'][\'metrics\']):\n      for j, cal in enumerate(metric[\'cals\']):\n        if cal[\'arguments\'] is not None and \'label_vocab_path\' in cal[\'arguments\']:\n          file_key_paths.append([\'solver\', \'metrics\', i, \'cals\', j, \'arguments\', \'label_vocab_path\'])\n\n  if isinstance(config[\'solver\'][\'postproc\'], list):\n    for i,postproc in enumerate(config[\'solver\'][\'postproc\']):\n      file_key_paths.append([\'solver\', \'postproc\', i, \'res_file\'])\n  else:\n    file_key_paths.append([\'solver\', \'postproc\', \'res_file\'])\n\n  for file_key_path in file_key_paths:\n    config_join_project_path(project_dir, config, file_key_path)\n\n\ndef load_config(config_path):\n  \'\'\' load config from file \'\'\'\n  if isinstance(config_path, Path):\n    config_path = str(config_path)\n\n  with open(config_path, \'r\') as f:  #pylint: disable=invalid-name\n    if config_path.endswith(\'yml\') or config_path.endswith(\'yaml\'):\n      config = yaml.load(f, Loader=yaml.SafeLoader)\n    elif config_path.endswith(\'json\'):\n      config = json.load(f)\n  # check config\n  # valid_config(config)\n  config_join_project_dir(config)\n  return config\n\n\ndef copy_config(config_path, config):\n  \'\'\' copy config file to ckpt dirctory \'\'\'\n  if isinstance(config_path, Path):\n    config_path = str(config_path)\n  config_name = os.path.basename(config_path)\n  save_config_path = os.path.join(config[""solver""][""saver""][""model_path""],\n                                  config_name)\n  logging.info(""Saving config file to {}"".format(save_config_path))\n  try:\n    copyfile(config_path, save_config_path)\n  except SameFileError:\n    pass\n\n  with open(config_path, \'r\') as f:\n    logging.info(""Config:"")\n    logging.info(f.read())\n  return config\n\n\ndef save_config(config, config_path):\n  \'\'\' save config to file \'\'\'\n  if isinstance(config_path, Path):\n    config_path = str(config_path)\n\n  with open(config_path, \'w\') as f:  #pylint: disable=invalid-name\n    if config_path.endswith(\'yml\') or config_path.endswith(\'yaml\'):\n      yaml.dump(config, f)\n    elif config_path.endswith(\'json\'):\n      json.dump(config, f)\n\n\ndef setdefault_config(config):\n  \'\'\' set default config \'\'\'\n  # This function only sets up those most commonly used parameters\n  # which may not change frequently.\n\n  config.setdefault(\'tfenv\', dict())\n  tfconf = config[\'tfenv\']\n  tfconf.setdefault(\'allow_soft_placement\', True)\n  tfconf.setdefault(\'log_device_placement\', True)\n  tfconf.setdefault(\'intra_op_parallelism_threads\', 10)\n  tfconf.setdefault(\'inter_op_parallelism_threads\', 10)\n  tfconf.setdefault(\'allow_growth\', False)\n  tfconf.setdefault(\'embedding_trainable\', True)\n  tfconf.setdefault(\'use_pretrained_embedding\', False)\n\n  model_conf = config[\'model\']\n  optimizer_conf = model_conf[\'optimizer\']\n  optimizer_conf.setdefault(\'dropout_keep_prob\', 0.0)\n\n  data_conf = config[\'data\']\n  saver_conf = data_conf[\'saver\']\n  saver_conf.setdefault(\'max_to_keep\', 50)\n  saver_conf.setdefault(\'checkpoint_every\', 500)\n  saver_conf.setdefault(\'evaluate_on_dev_every\', 500)\n  timestamp = str(int(time.time()))\n  out_dir = os.path.abspath(os.path.join(\'/tmp\', \'tf_runs\', timestamp))\n  saver_conf.setdefault(\'outdir\', out_dir)\n  saver_conf.setdefault(\'resume_model_path\', out_dir)\n  if not os.path.exists(out_dir):\n    os.makedirs(out_dir)\n  logging.info(""Create temp output directory: {}\\n"".format(out_dir))\n\n  data_conf.setdefault(\'outdir\', out_dir)\n  data_conf.setdefault(\'vocab\', out_dir + \'/vocab\')\n  data_conf.setdefault(\'label\', out_dir + \'/label\')\n\n  return config\n'"
delta/utils/config_test.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' configration utils unittest\'\'\'\nimport tempfile\n\nfrom absl import logging\nimport os\nimport delta.compat as tf\n\nfrom delta import utils\nfrom delta import PACKAGE_ROOT_DIR\n\n\nclass ConfigTest(tf.test.TestCase):\n  \'\'\' config unit test\'\'\'\n\n  def setUp(self):\n    super().setUp()\n    \'\'\' setup \'\'\'\n    self.conf_str = \'\'\'\n     name: Tom Smith\n     age: 37\n     spouse:\n         name: Jane Smith\n         age: 25\n     children:\n      - name: Jimmy Smith\n        age: 15\n      - name: Jenny Smith\n        age: 12\n    \'\'\'\n    self.conf_true = {\n        \'name\':\n            \'Tom Smith\',\n        \'age\':\n            37,\n        \'spouse\': {\n            \'name\': \'Jane Smith\',\n            \'age\': 25\n        },\n        \'children\': [\n            {\n                \'name\': \'Jimmy Smith\',\n                \'age\': 15\n            },\n            {\n                \'name\': \'Jenny Smith\',\n                \'age\': 12\n            },\n        ]\n    }\n\n    self.conf_file = tempfile.mktemp(suffix=\'conf.yaml\')\n    with open(self.conf_file, \'w\', encoding=\'utf-8\') as f:  #pylint: disable=invalid-name\n      f.write(self.conf_str)\n\n  def tearDown(self):\n    \'\'\' tear down \'\'\'\n\n  def test_load_config(self):\n    \'\'\' load config unittest \'\'\'\n    conf = utils.load_config(self.conf_file)\n    self.assertDictEqual(conf, self.conf_true)\n    config_dir = os.path.join(PACKAGE_ROOT_DIR, ""configs/"")\n    for config_file in os.listdir(config_dir):\n      config_file = os.path.join(config_dir, config_file)\n      logging.info(f""Loading conf {config_file}"")\n      conf = utils.load_config(config_file)\n      logging.info(conf)\n\n  def test_save_config(self):\n    \'\'\' save config unittest \'\'\'\n    utils.save_config(self.conf_true, self.conf_file)\n    conf = utils.load_config(self.conf_file)\n    self.assertDictEqual(conf, self.conf_true)\n\n  def test_valid_config(self):\n    \'\'\' valid config unittest \'\'\'\n    utils.save_config(self.conf_true, self.conf_file)\n    conf = utils.load_config(self.conf_file)\n    self.assertEqual(utils.valid_config(conf), True)\n\n  def test_setdefault_config(self):\n    \'\'\' set default config unittest \'\'\'\n    self.assertEqual(True, True)\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  tf.test.main()\n'"
delta/utils/ctc_utils.py,14,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# ==============================================================================\n\'\'\' global ASR CTC utils \'\'\'\n\nimport delta.compat as tf\n\n\ndef transform_preprocess(labels=None, blank_index=None, num_class=None):\n  \'\'\' Ensure that the value of blank_index is in a reasonable range,\n      and transform the DenseTensor labels to a SparseTensor \'\'\'\n  if blank_index is None or blank_index < 0:\n    raise ValueError(\'blank_index must be greater than or equal to zero\')\n\n  if not num_class is None and blank_index > (num_class - 1):\n    raise ValueError(\'blank_index must be less than or equal to num_class - 1\')\n\n  if labels is None:\n    return None\n\n  if not isinstance(labels, tf.SparseTensor):\n    labels = tf.cast(labels, tf.int32)\n    labels_idx = tf.where(tf.not_equal(labels, 0))\n    labels_values = tf.gather_nd(labels, labels_idx)\n    labels_shape = tf.cast(tf.shape(labels), dtype=tf.int64)\n    labels = tf.SparseTensor(\n        indices=labels_idx, values=labels_values, dense_shape=labels_shape)\n\n  return labels\n\n\ndef logits_blankid_to_last(logits, blank_index):\n  \'\'\' Moves the blank_label cloumn to the end of the logit matrix \'\'\'\n  num_class = logits.shape[2]\n  transform_preprocess(blank_index=blank_index, num_class=num_class)\n\n  if blank_index != (num_class - 1):\n    logits = tf.concat([\n        logits[:, :, :blank_index], logits[:, :, blank_index + 1:],\n        logits[:, :, blank_index:blank_index + 1]\n    ],\n                       axis=2)\n\n  return logits\n\n\ndef labels_blankid_to_last(labels, blank_index, num_class=None):\n  \'\'\' Change the value of blank_label elements from blank_index to num_class - 1\'\'\'\n  assert num_class is not None, \'The num_class should not be None!\'\n\n  labels = transform_preprocess(\n      labels=labels, blank_index=blank_index, num_class=num_class)\n\n  labels_values = labels.values\n  labels_num_class = tf.zeros_like(labels_values, dtype=tf.int32) + num_class\n  labels_values_change_blank = tf.where(\n      tf.equal(labels_values, blank_index), labels_num_class, labels_values)\n  labels_values = tf.where(labels_values_change_blank < blank_index,\n                           labels_values_change_blank,\n                           labels_values_change_blank - 1)\n\n  labels = tf.SparseTensor(\n      indices=labels.indices,\n      values=labels_values,\n      dense_shape=labels.dense_shape)\n  return labels\n\n\ndef labels_last_to_blankid(labels, blank_index, num_class=None):\n  \'\'\' Change the value of blank_label elements from num_classes - 1 to blank_index,\n      after removing blank_index by decoder. \'\'\'\n  labels = transform_preprocess(\n      labels=labels, blank_index=blank_index, num_class=num_class)\n\n  labels_values = labels.values\n  labels_change_blank_id = tf.where(labels_values >= blank_index,\n                                    labels_values + 1, labels_values)\n\n  labels = tf.SparseTensor(\n      indices=labels.indices,\n      values=labels_change_blank_id,\n      dense_shape=labels.dense_shape)\n\n  return labels\n'"
delta/utils/ctc_utils_test.py,9,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' ctc utils unittest \'\'\'\n\nimport numpy as np\nimport delta.compat as tf\n\nfrom delta.utils import ctc_utils\n\n\nclass CTCUtilTest(tf.test.TestCase):\n  \'\'\' ctc utils unittest\'\'\'\n\n  def setUp(self):\n    super().setUp()\n    \'\'\' setup \'\'\'\n    self.logits = np.asarray(\n        [[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553],\n          [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436],\n          [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688],\n          [0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533],\n          [0.158235, 0.196634, 0.123377, 0.50648837, 0.00903441, 0.00623107]],\n         [[0.30176, 0.28562, 0.0831517, 0.0862751, 0.0816851, 0.161508],\n          [0.24082, 0.397533, 0.0557226, 0.0546814, 0.0557528, 0.19549],\n          [0.230246, 0.450868, 0.0389607, 0.038309, 0.0391602, 0.202456],\n          [0.280884, 0.429522, 0.0326593, 0.0339046, 0.0326856, 0.190345],\n          [0.423286, 0.315517, 0.0338439, 0.0393744, 0.0339315, 0.154046]]],\n        dtype=np.float32)\n\n    self.labels = np.asarray([[1, 1, 1, 3], [1, 1, 1, 0]], dtype=np.int32)\n\n  def tearDown(self):\n    \'\'\' tear down \'\'\'\n\n  def test_transform_preprocess(self):\n    \'\'\' unit test case for the transform_preprocess interface \'\'\'\n    with self.cached_session():\n\n      with self.assertRaises(ValueError) as valueErr:\n        labels = ctc_utils.transform_preprocess(\n            labels=None, blank_index=None, num_class=None)\n      the_exception = valueErr.exception\n      self.assertEqual(\n          str(the_exception),\n          \'blank_index must be greater than or equal to zero\')\n\n      with self.assertRaises(ValueError) as valueErr:\n        labels = ctc_utils.transform_preprocess(\n            labels=None, blank_index=-10, num_class=None)\n      the_exception = valueErr.exception\n      self.assertEqual(\n          str(the_exception),\n          \'blank_index must be greater than or equal to zero\')\n\n      with self.assertRaises(ValueError) as valueErr:\n        labels = ctc_utils.transform_preprocess(\n            labels=None, blank_index=10, num_class=10)\n      the_exception = valueErr.exception\n      self.assertEqual(\n          str(the_exception),\n          \'blank_index must be less than or equal to num_class - 1\')\n\n      labels = ctc_utils.transform_preprocess(\n          labels=None, blank_index=0, num_class=10)\n      self.assertIsNone(labels)\n\n      labels = ctc_utils.transform_preprocess(\n          labels=tf.constant(self.labels), blank_index=0, num_class=10)\n      labels_values = np.asarray([1, 1, 1, 3, 1, 1, 1])\n      labels_index = np.asarray([[0, 0], [0, 1], [0, 2], [0, 3], [1, 0], [1, 1],\n                                 [1, 2]])\n      labels_shape = np.asarray([2, 4])\n      self.assertAllEqual(labels.eval().values, labels_values)\n      self.assertAllEqual(labels.eval().indices, labels_index)\n      self.assertAllEqual(labels.eval().dense_shape, labels_shape)\n\n  def test_logits_blankid_to_last(self):\n    \'\'\' unit test case for the logits_blankid_to_last interface \'\'\'\n    with self.cached_session():\n\n      with self.assertRaises(ValueError) as valueErr:\n        logits = ctc_utils.logits_blankid_to_last(\n            logits=tf.constant(self.logits), blank_index=10)\n      the_exception = valueErr.exception\n      self.assertEqual(\n          str(the_exception),\n          \'blank_index must be less than or equal to num_class - 1\')\n\n      logits = ctc_utils.logits_blankid_to_last(\n          logits=tf.constant(self.logits), blank_index=0)\n      logits_transform = np.asarray(\n          [[[0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553, 0.633766],\n            [0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436, 0.111121],\n            [0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688, 0.0357786],\n            [0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533, 0.0663296],\n            [0.196634, 0.123377, 0.50648837, 0.00903441, 0.00623107, 0.158235]],\n           [[0.28562, 0.0831517, 0.0862751, 0.0816851, 0.161508, 0.30176],\n            [0.397533, 0.0557226, 0.0546814, 0.0557528, 0.19549, 0.24082],\n            [0.450868, 0.0389607, 0.038309, 0.0391602, 0.202456, 0.230246],\n            [0.429522, 0.0326593, 0.0339046, 0.0326856, 0.190345, 0.280884],\n            [0.315517, 0.0338439, 0.0393744, 0.0339315, 0.154046, 0.423286]]],\n          dtype=np.float32)\n      self.assertAllClose(logits.eval(), logits)\n\n  def test_labels_blankid_to_last(self):\n    \'\'\' unit test case for the labels_blankid_to_last interface \'\'\'\n    with self.cached_session():\n\n      with self.assertRaises(AssertionError) as assert_err:\n        labels = ctc_utils.labels_blankid_to_last(\n            labels=self.labels, blank_index=0, num_class=None)\n      the_exception = assert_err.exception\n      self.assertEqual(str(the_exception), \'The num_class should not be None!\')\n\n      labels = ctc_utils.labels_blankid_to_last(\n          labels=tf.constant(self.labels), blank_index=0, num_class=6)\n      labels_values = np.asarray([0, 0, 0, 2, 0, 0, 0])\n      labels_index = np.asarray([[0, 0], [0, 1], [0, 2], [0, 3], [1, 0], [1, 1],\n                                 [1, 2]])\n      labels_shape = np.asarray([2, 4])\n      self.assertAllEqual(labels.eval().values, labels_values)\n      self.assertAllEqual(labels.eval().indices, labels_index)\n      self.assertAllEqual(labels.eval().dense_shape, labels_shape)\n\n      labels = ctc_utils.labels_blankid_to_last(\n          labels=tf.constant(self.labels), blank_index=2, num_class=6)\n      labels_values = np.asarray([1, 1, 1, 2, 1, 1, 1])\n      labels_index = np.asarray([[0, 0], [0, 1], [0, 2], [0, 3], [1, 0], [1, 1],\n                                 [1, 2]])\n      labels_shape = np.asarray([2, 4])\n      self.assertAllEqual(labels.eval().values, labels_values)\n      self.assertAllEqual(labels.eval().indices, labels_index)\n      self.assertAllEqual(labels.eval().dense_shape, labels_shape)\n\n  def test_labels_last_to_blankid(self):\n    \'\'\' unit test case for the labels_last_to_blankid interface \'\'\'\n    with self.cached_session():\n\n      labels = ctc_utils.labels_last_to_blankid(\n          labels=tf.constant(self.labels), blank_index=0, num_class=None)\n      labels_values = np.asarray([2, 2, 2, 4, 2, 2, 2])\n      labels_index = np.asarray([[0, 0], [0, 1], [0, 2], [0, 3], [1, 0], [1, 1],\n                                 [1, 2]])\n      labels_shape = np.asarray([2, 4])\n      self.assertAllEqual(labels.eval().values, labels_values)\n      self.assertAllEqual(labels.eval().indices, labels_index)\n      self.assertAllEqual(labels.eval().dense_shape, labels_shape)\n\n      labels = ctc_utils.labels_last_to_blankid(\n          labels=tf.constant(self.labels), blank_index=2, num_class=None)\n      labels_values = np.asarray([1, 1, 1, 4, 1, 1, 1])\n      labels_index = np.asarray([[0, 0], [0, 1], [0, 2], [0, 3], [1, 0], [1, 1],\n                                 [1, 2]])\n      labels_shape = np.asarray([2, 4])\n      self.assertAllEqual(labels.eval().values, labels_values)\n      self.assertAllEqual(labels.eval().indices, labels_index)\n      self.assertAllEqual(labels.eval().dense_shape, labels_shape)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
delta/utils/hparam.py,2,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Hyperparameter values.""""""\nimport json\nimport numbers\nimport re\nfrom deepdiff import DeepDiff\n\nimport six\n\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.util import compat\nfrom tensorflow.python.util import deprecation\n\n# Define the regular expression for parsing a single clause of the input\n# (delimited by commas).  A legal clause looks like:\n#   <variable name>[<index>]? = <rhs>\n# where <rhs> is either a single token or [] enclosed list of tokens.\n# For example:  ""var[1] = a"" or ""x = [1,2,3]""\nPARAM_RE = re.compile(\n    r""""""\n  (?P<name>[a-zA-Z][\\w\\.]*)      # variable name: ""var"" or ""x""\n  (\\[\\s*(?P<index>\\d+)\\s*\\])?  # (optional) index: ""1"" or None\n  \\s*=\\s*\n  ((?P<val>[^,\\[]*)            # single value: ""a"" or None\n   |\n   \\[(?P<vals>[^\\]]*)\\])       # list of values: None or ""1,2,3""\n  ($|,\\s*)"""""", re.VERBOSE)\n\n\ndef _parse_fail(name, var_type, value, values):\n  """"""Helper function for raising a value error for bad assignment.""""""\n  raise ValueError(\n      \'Could not parse hparam \\\'%s\\\' of type \\\'%s\\\' with value \\\'%s\\\' in %s\' %\n      (name, var_type.__name__, value, values))\n\n\ndef _reuse_fail(name, values):\n  """"""Helper function for raising a value error for reuse of name.""""""\n  raise ValueError(\'Multiple assignments to variable \\\'%s\\\' in %s\' %\n                   (name, values))\n\n\ndef _process_scalar_value(name, parse_fn, var_type, m_dict, values,\n                          results_dictionary):\n  """"""Update results_dictionary with a scalar value.\n\n  Used to update the results_dictionary to be returned by parse_values when\n  encountering a clause with a scalar RHS (e.g.  ""s=5"" or ""arr[0]=5"".)\n\n  Mutates results_dictionary.\n\n  Args:\n    name: Name of variable in assignment (""s"" or ""arr"").\n    parse_fn: Function for parsing the actual value.\n    var_type: Type of named variable.\n    m_dict: Dictionary constructed from regex parsing.\n      m_dict[\'val\']: RHS value (scalar)\n      m_dict[\'index\']: List index value (or None)\n    values: Full expression being parsed\n    results_dictionary: The dictionary being updated for return by the parsing\n      function.\n\n  Raises:\n    ValueError: If the name has already been used.\n  """"""\n  try:\n    parsed_value = parse_fn(m_dict[\'val\'])\n  except ValueError:\n    _parse_fail(name, var_type, m_dict[\'val\'], values)\n\n  # If no index is provided\n  if not m_dict[\'index\']:\n    if name in results_dictionary:\n      _reuse_fail(name, values)\n    results_dictionary[name] = parsed_value\n  else:\n    if name in results_dictionary:\n      # The name has already been used as a scalar, then it\n      # will be in this dictionary and map to a non-dictionary.\n      if not isinstance(results_dictionary.get(name), dict):\n        _reuse_fail(name, values)\n    else:\n      results_dictionary[name] = {}\n\n    index = int(m_dict[\'index\'])\n    # Make sure the index position hasn\'t already been assigned a value.\n    if index in results_dictionary[name]:\n      _reuse_fail(\'{}[{}]\'.format(name, index), values)\n    results_dictionary[name][index] = parsed_value\n\n\ndef _process_list_value(name, parse_fn, var_type, m_dict, values,\n                        results_dictionary):\n  """"""Update results_dictionary from a list of values.\n\n  Used to update results_dictionary to be returned by parse_values when\n  encountering a clause with a list RHS (e.g.  ""arr=[1,2,3]"".)\n\n  Mutates results_dictionary.\n\n  Args:\n    name: Name of variable in assignment (""arr"").\n    parse_fn: Function for parsing individual values.\n    var_type: Type of named variable.\n    m_dict: Dictionary constructed from regex parsing.\n      m_dict[\'val\']: RHS value (scalar)\n    values: Full expression being parsed\n    results_dictionary: The dictionary being updated for return by the parsing\n      function.\n\n  Raises:\n    ValueError: If the name has an index or the values cannot be parsed.\n  """"""\n  if m_dict[\'index\'] is not None:\n    raise ValueError(\'Assignment of a list to a list index.\')\n  elements = filter(None, re.split(\'[ ,]\', m_dict[\'vals\']))\n  # Make sure the name hasn\'t already been assigned a value\n  if name in results_dictionary:\n    raise _reuse_fail(name, values)\n  try:\n    results_dictionary[name] = [parse_fn(e) for e in elements]\n  except ValueError:\n    _parse_fail(name, var_type, m_dict[\'vals\'], values)\n\n\ndef _cast_to_type_if_compatible(name, param_type, value):\n  """"""Cast hparam to the provided type, if compatible.\n\n  Args:\n    name: Name of the hparam to be cast.\n    param_type: The type of the hparam.\n    value: The value to be cast, if compatible.\n\n  Returns:\n    The result of casting `value` to `param_type`.\n\n  Raises:\n    ValueError: If the type of `value` is not compatible with param_type.\n      * If `param_type` is a string type, but `value` is not.\n      * If `param_type` is a boolean, but `value` is not, or vice versa.\n      * If `param_type` is an integer type, but `value` is not.\n      * If `param_type` is a float type, but `value` is not a numeric type.\n  """"""\n  fail_msg = (""Could not cast hparam \'%s\' of type \'%s\' from value %r"" %\n              (name, param_type, value))\n\n  # Some callers use None, for which we can\'t do any casting/checking. :(\n  if issubclass(param_type, type(None)):\n    return value\n\n  # Avoid converting a non-string type to a string.\n  if (issubclass(param_type, (six.string_types, six.binary_type)) and\n      not isinstance(value, (six.string_types, six.binary_type))):\n    raise ValueError(fail_msg)\n\n  # Avoid converting a number or string type to a boolean or vice versa.\n  if issubclass(param_type, bool) != isinstance(value, bool):\n    raise ValueError(fail_msg)\n\n  # Avoid converting float to an integer (the reverse is fine).\n  if (issubclass(param_type, numbers.Integral) and\n      not isinstance(value, numbers.Integral)):\n    raise ValueError(fail_msg)\n\n  # Avoid converting a non-numeric type to a numeric type.\n  if (issubclass(param_type, numbers.Number) and\n      not isinstance(value, numbers.Number)):\n    raise ValueError(fail_msg)\n\n  return param_type(value)\n\n\ndef parse_values(values, type_map):\n  """"""Parses hyperparameter values from a string into a python map.\n\n  `values` is a string containing comma-separated `name=value` pairs.\n  For each pair, the value of the hyperparameter named `name` is set to\n  `value`.\n\n  If a hyperparameter name appears multiple times in `values`, a ValueError\n  is raised (e.g. \'a=1,a=2\', \'a[1]=1,a[1]=2\').\n\n  If a hyperparameter name in both an index assignment and scalar assignment,\n  a ValueError is raised.  (e.g. \'a=[1,2,3],a[0] = 1\').\n\n  The hyperparameter name may contain \'.\' symbols, which will result in an\n  attribute name that is only accessible through the getattr and setattr\n  functions.  (And must be first explicit added through add_hparam.)\n\n  WARNING: Use of \'.\' in your variable names is allowed, but is not well\n  supported and not recommended.\n\n  The `value` in `name=value` must follows the syntax according to the\n  type of the parameter:\n\n  *  Scalar integer: A Python-parsable integer point value.  E.g.: 1,\n     100, -12.\n  *  Scalar float: A Python-parsable floating point value.  E.g.: 1.0,\n     -.54e89.\n  *  Boolean: Either true or false.\n  *  Scalar string: A non-empty sequence of characters, excluding comma,\n     spaces, and square brackets.  E.g.: foo, bar_1.\n  *  List: A comma separated list of scalar values of the parameter type\n     enclosed in square brackets.  E.g.: [1,2,3], [1.0,1e-12], [high,low].\n\n  When index assignment is used, the corresponding type_map key should be the\n  list name.  E.g. for ""arr[1]=0"" the type_map must have the key ""arr"" (not\n  ""arr[1]"").\n\n  Args:\n    values: String.  Comma separated list of `name=value` pairs where\n      \'value\' must follow the syntax described above.\n    type_map: A dictionary mapping hyperparameter names to types.  Note every\n      parameter name in values must be a key in type_map.  The values must\n      conform to the types indicated, where a value V is said to conform to a\n      type T if either V has type T, or V is a list of elements of type T.\n      Hence, for a multidimensional parameter \'x\' taking float values,\n      \'x=[0.1,0.2]\' will parse successfully if type_map[\'x\'] = float.\n\n  Returns:\n    A python map mapping each name to either:\n    * A scalar value.\n    * A list of scalar values.\n    * A dictionary mapping index numbers to scalar values.\n    (e.g. ""x=5,L=[1,2],arr[1]=3"" results in {\'x\':5,\'L\':[1,2],\'arr\':{1:3}}"")\n\n  Raises:\n    ValueError: If there is a problem with input.\n    * If `values` cannot be parsed.\n    * If a list is assigned to a list index (e.g. \'a[1] = [1,2,3]\').\n    * If the same rvalue is assigned two different values (e.g. \'a=1,a=2\',\n      \'a[1]=1,a[1]=2\', or \'a=1,a=[1]\')\n  """"""\n  results_dictionary = {}\n  pos = 0\n  while pos < len(values):\n    m = PARAM_RE.match(values, pos)\n    if not m:\n      raise ValueError(\'Malformed hyperparameter value: %s\' % values[pos:])\n    # Check that there is a comma between parameters and move past it.\n    pos = m.end()\n    # Parse the values.\n    m_dict = m.groupdict()\n    name = m_dict[\'name\']\n    if name not in type_map:\n      raise ValueError(\'Unknown hyperparameter type for %s\' % name)\n    type_ = type_map[name]\n\n    # Set up correct parsing function (depending on whether type_ is a bool)\n    if type_ == bool:\n\n      def parse_bool(value):\n        if value in [\'true\', \'True\']:\n          return True\n        elif value in [\'false\', \'False\']:\n          return False\n        else:\n          try:\n            return bool(int(value))\n          except ValueError:\n            _parse_fail(name, type_, value, values)\n\n      parse = parse_bool\n    else:\n      parse = type_\n\n    # If a singe value is provided\n    if m_dict[\'val\'] is not None:\n      _process_scalar_value(name, parse, type_, m_dict, values,\n                            results_dictionary)\n\n    # If the assigned value is a list:\n    elif m_dict[\'vals\'] is not None:\n      _process_list_value(name, parse, type_, m_dict, values,\n                          results_dictionary)\n\n    else:  # Not assigned a list or value\n      _parse_fail(name, type_, \'\', values)\n\n  return results_dictionary\n\n\nclass HParams(object):\n  """"""Class to hold a set of hyperparameters as name-value pairs.\n\n  A `HParams` object holds hyperparameters used to build and train a model,\n  such as the number of hidden units in a neural net layer or the learning rate\n  to use when training.\n\n  You first create a `HParams` object by specifying the names and values of the\n  hyperparameters.\n\n  To make them easily accessible the parameter names are added as direct\n  attributes of the class.  A typical usage is as follows:\n\n  ```python\n  # Create a HParams object specifying names and values of the model\n  # hyperparameters:\n  hparams = HParams(learning_rate=0.1, num_hidden_units=100)\n\n  # The hyperparameter are available as attributes of the HParams object:\n  hparams.learning_rate ==> 0.1\n  hparams.num_hidden_units ==> 100\n  ```\n\n  Hyperparameters have type, which is inferred from the type of their value\n  passed at construction type.   The currently supported types are: integer,\n  float, boolean, string, and list of integer, float, boolean, or string.\n\n  You can override hyperparameter values by calling the\n  [`parse()`](#HParams.parse) method, passing a string of comma separated\n  `name=value` pairs.  This is intended to make it possible to override\n  any hyperparameter values from a single command-line flag to which\n  the user passes \'hyper-param=value\' pairs.  It avoids having to define\n  one flag for each hyperparameter.\n\n  The syntax expected for each value depends on the type of the parameter.\n  See `parse()` for a description of the syntax.\n\n  Example:\n\n  ```python\n  # Define a command line flag to pass name=value pairs.\n  # For example using argparse:\n  import argparse\n  parser = argparse.ArgumentParser(description=\'Train my model.\')\n  parser.add_argument(\'--hparams\', type=str,\n                      help=\'Comma separated list of ""name=value"" pairs.\')\n  args = parser.parse_args()\n  ...\n  def my_program():\n    # Create a HParams object specifying the names and values of the\n    # model hyperparameters:\n    hparams = tf.HParams(learning_rate=0.1, num_hidden_units=100,\n                         activations=[\'relu\', \'tanh\'])\n\n    # Override hyperparameters values by parsing the command line\n    hparams.parse(args.hparams)\n\n    # If the user passed `--hparams=learning_rate=0.3` on the command line\n    # then \'hparams\' has the following attributes:\n    hparams.learning_rate ==> 0.3\n    hparams.num_hidden_units ==> 100\n    hparams.activations ==> [\'relu\', \'tanh\']\n\n    # If the hyperparameters are in json format use parse_json:\n    hparams.parse_json(\'{""learning_rate"": 0.3, ""activations"": ""relu""}\')\n  ```\n  """"""\n\n  _HAS_DYNAMIC_ATTRIBUTES = True  # Required for pytype checks.\n\n  def __init__(self, **kwargs):\n    """"""Create an instance of `HParams` from keyword arguments.\n\n    The keyword arguments specify name-values pairs for the hyperparameters.\n    The parameter types are inferred from the type of the values passed.\n\n    The parameter names are added as attributes of `HParams` object, so they\n    can be accessed directly with the dot notation `hparams._name_`.\n\n    Example:\n\n    ```python\n    # Define 3 hyperparameters: \'learning_rate\' is a float parameter,\n    # \'num_hidden_units\' an integer parameter, and \'activation\' a string\n    # parameter.\n    hparams = tf.HParams(\n        learning_rate=0.1, num_hidden_units=100, activation=\'relu\')\n\n    hparams.activation ==> \'relu\'\n    ```\n\n    Note that a few names are reserved and cannot be used as hyperparameter\n    names.  If you use one of the reserved name the constructor raises a\n    `ValueError`.\n\n    Args:\n      **kwargs: Key-value pairs where the key is the hyperparameter name and\n        the value is the value for the parameter.\n\n    Raises:\n      ValueError: If one of the arguments is invalid.\n\n    """"""\n    # Register the hyperparameters and their type in _hparam_types.\n    # This simplifies the implementation of parse().\n    # _hparam_types maps the parameter name to a tuple (type, bool).\n    # The type value is the type of the parameter for scalar hyperparameters,\n    # or the type of the list elements for multidimensional hyperparameters.\n    # The bool value is True if the value is a list, False otherwise.\n    self._hparam_types = {}\n    for name, value in six.iteritems(kwargs):\n      self.add_hparam(name, value)\n\n  def add_hparam(self, name, value):\n    """"""Adds {name, value} pair to hyperparameters.\n\n    Args:\n      name: Name of the hyperparameter.\n      value: Value of the hyperparameter. Can be one of the following types:\n        int, float, string, int list, float list, or string list.\n\n    Raises:\n      ValueError: if one of the arguments is invalid.\n    """"""\n    # Keys in kwargs are unique, but \'name\' could the name of a pre-existing\n    # attribute of this object.  In that case we refuse to use it as a\n    # hyperparameter name.\n    if getattr(self, name, None) is not None:\n      raise ValueError(\'Hyperparameter name is reserved: %s\' % name)\n    if isinstance(value, (list, tuple)):\n      if not value:\n        raise ValueError(\'Multi-valued hyperparameters cannot be empty: %s\' %\n                         name)\n      self._hparam_types[name] = (type(value[0]), True)\n    else:\n      self._hparam_types[name] = (type(value), False)\n    setattr(self, name, value)\n\n  def set_hparam(self, name, value):\n    """"""Set the value of an existing hyperparameter.\n\n    This function verifies that the type of the value matches the type of the\n    existing hyperparameter.\n\n    Args:\n      name: Name of the hyperparameter.\n      value: New value of the hyperparameter.\n\n    Raises:\n      ValueError: If there is a type mismatch.\n    """"""\n    param_type, is_list = self._hparam_types[name]\n    if isinstance(value, list):\n      if not is_list:\n        raise ValueError(\n            \'Must not pass a list for single-valued parameter: %s\' % name)\n      setattr(self, name,\n              [_cast_to_type_if_compatible(name, param_type, v) for v in value])\n    else:\n      if is_list:\n        raise ValueError(\'Must pass a list for multi-valued parameter: %s.\' %\n                         name)\n      setattr(self, name, _cast_to_type_if_compatible(name, param_type, value))\n\n  def del_hparam(self, name):\n    """"""Removes the hyperparameter with key \'name\'.\n\n    Args:\n      name: Name of the hyperparameter.\n    """"""\n    if hasattr(self, name):\n      delattr(self, name)\n      del self._hparam_types[name]\n\n  def parse(self, values):\n    """"""Override hyperparameter values, parsing new values from a string.\n\n    See parse_values for more detail on the allowed format for values.\n\n    Args:\n      values: String.  Comma separated list of `name=value` pairs where\n        \'value\' must follow the syntax described above.\n\n    Returns:\n      The `HParams` instance.\n\n    Raises:\n      ValueError: If `values` cannot be parsed.\n    """"""\n    type_map = dict()\n    for name, t in self._hparam_types.items():\n      param_type, _ = t\n      type_map[name] = param_type\n\n    values_map = parse_values(values, type_map)\n    return self.override_from_dict(values_map)\n\n  def override_from_dict(self, values_dict):\n    """"""Override hyperparameter values, parsing new values from a dictionary.\n\n    Args:\n      values_dict: Dictionary of name:value pairs.\n\n    Returns:\n      The `HParams` instance.\n\n    Raises:\n      ValueError: If `values_dict` cannot be parsed.\n    """"""\n    for name, value in values_dict.items():\n      self.set_hparam(name, value)\n    return self\n\n  def parse_dict(self, values_dict):\n    """"""Add new hyperparameter values, parsing new values from a dictionary.\n\n    Args:\n      values_dict: Dictionary of name:value pairs.\n\n    Returns:\n      The `HParams` instance.\n\n    Raises:\n      ValueError: If `values_dict` cannot be parsed.\n    """"""\n    for name, value in values_dict.items():\n      self.add_hparam(name, value)\n    return self\n\n  @deprecation.deprecated(None, \'Use `override_from_dict`.\')\n  def set_from_map(self, values_map):\n    """"""DEPRECATED. Use override_from_dict.""""""\n    return self.override_from_dict(values_dict=values_map)\n\n  def to_json(self, indent=None, separators=None, sort_keys=False):\n    """"""Serializes the hyperparameters into JSON.\n\n    Args:\n      indent: If a non-negative integer, JSON array elements and object members\n        will be pretty-printed with that indent level. An indent level of 0, or\n        negative, will only insert newlines. `None` (the default) selects the\n        most compact representation.\n      separators: Optional `(item_separator, key_separator)` tuple. Default is\n        `(\', \', \': \')`.\n      sort_keys: If `True`, the output dictionaries will be sorted by key.\n\n    Returns:\n      A JSON string.\n    """"""\n    return json.dumps(\n        self.values(),\n        indent=indent,\n        separators=separators,\n        sort_keys=sort_keys)\n\n  def parse_json(self, values_json):\n    """"""Override hyperparameter values, parsing new values from a json object.\n\n    Args:\n      values_json: String containing a json object of name:value pairs.\n\n    Returns:\n      The `HParams` instance.\n\n    Raises:\n      ValueError: If `values_json` cannot be parsed.\n    """"""\n    values_map = json.loads(values_json)\n    return self.override_from_dict(values_map)\n\n  def values(self):\n    """"""Return the hyperparameter values as a Python dictionary.\n\n    Returns:\n      A dictionary with hyperparameter names as keys.  The values are the\n      hyperparameter values.\n    """"""\n    return {n: getattr(self, n) for n in self._hparam_types.keys()}\n\n  def get(self, key, default=None):\n    """"""Returns the value of `key` if it exists, else `default`.""""""\n    if key in self._hparam_types:\n      # Ensure that default is compatible with the parameter type.\n      if default is not None:\n        param_type, is_param_list = self._hparam_types[key]\n        type_str = \'list<%s>\' % param_type if is_param_list else str(param_type)\n        fail_msg = (""Hparam \'%s\' of type \'%s\' is incompatible with ""\n                    \'default=%s\' % (key, type_str, default))\n\n        is_default_list = isinstance(default, list)\n        if is_param_list != is_default_list:\n          raise ValueError(fail_msg)\n\n        try:\n          if is_default_list:\n            for value in default:\n              _cast_to_type_if_compatible(key, param_type, value)\n          else:\n            _cast_to_type_if_compatible(key, param_type, default)\n        except ValueError as e:\n          raise ValueError(\'%s. %s\' % (fail_msg, e))\n\n      return getattr(self, key)\n\n    return default\n\n  def __contains__(self, key):\n    return key in self._hparam_types\n\n  def __str__(self):\n    return str(sorted(self.values().items()))\n\n  def __repr__(self):\n    return \'%s(%s)\' % (type(self).__name__, self.__str__())\n\n  def __getitem__(self, key):\n    return self.get(key)\n\n  def __setitem__(self, key, value):\n    self.set_hparam(key, value)\n\n  def __delitem__(self, key):\n    self.del_hparam(key)\n\n  def __eq__(self, other):\n    if DeepDiff(self, other):\n      return False\n    else:\n      return True\n\n  def __ne__(self, other):\n    if DeepDiff(self, other):\n      return True\n    else:\n      return False\n\n  @staticmethod\n  def _get_kind_name(param_type, is_list):\n    """"""Returns the field name given parameter type and is_list.\n\n    Args:\n      param_type: Data type of the hparam.\n      is_list: Whether this is a list.\n\n    Returns:\n      A string representation of the field name.\n\n    Raises:\n      ValueError: If parameter type is not recognized.\n    """"""\n    if issubclass(param_type, bool):\n      # This check must happen before issubclass(param_type, six.integer_types),\n      # since Python considers bool to be a subclass of int.\n      typename = \'bool\'\n    elif issubclass(param_type, six.integer_types):\n      # Setting \'int\' and \'long\' types to be \'int64\' to ensure the type is\n      # compatible with both Python2 and Python3.\n      typename = \'int64\'\n    elif issubclass(param_type, (six.string_types, six.binary_type)):\n      # Setting \'string\' and \'bytes\' types to be \'bytes\' to ensure the type is\n      # compatible with both Python2 and Python3.\n      typename = \'bytes\'\n    elif issubclass(param_type, float):\n      typename = \'float\'\n    else:\n      raise ValueError(\'Unsupported parameter type: %s\' % str(param_type))\n\n    suffix = \'list\' if is_list else \'value\'\n    return \'_\'.join([typename, suffix])\n\n  def instantiate(self):\n    assert \'cls\' in self._hparam_types\n    assert self.cls is not None\n    return self.cls(self)\n'"
delta/utils/hparam_test.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""hparam test.""""""\nimport copy\nimport delta.compat as tf\n\nfrom delta.utils.hparam import HParams\n\n\nclass HParamsTest(tf.test.TestCase):\n  \'\'\' HParams unittest \'\'\'\n\n  def test_hparams(self):\n    hparams = HParams(cls=self.__class__, name=\'fbank\', n_mels=40)\n    hparams.del_hparam(\'cls\')\n    self.assertEqual(hparams.name, \'fbank\')\n    self.assertEqual(hparams.n_mels, 40)\n    self.assertDictEqual(hparams.values(), {\'name\': \'fbank\', \'n_mels\': 40})\n\n    hparams.add_hparam(\'sr\', 8000)\n    self.assertEqual(hparams.sr, 8000)\n\n    hparams.set_hparam(\'sr\', 16000)\n    self.assertEqual(hparams.sr, 16000)\n    self.assertEqual(hparams.get(\'sr\'), 16000)\n\n    hparams.del_hparam(\'sr\')\n    self.assertJsonEqual(hparams.to_json(), \'{""name"": ""fbank"", ""n_mels"": 40}\')\n\n    self.assertEqual(\'name\' in hparams, True)\n    self.assertEqual(hparams[\'name\'], \'fbank\')\n    self.assertEqual(hparams[\'n_mels\'], 40)\n\n    hparams[\'n_mels\'] = 80\n    self.assertEqual(hparams[\'n_mels\'], 80)\n\n    hparams2 = copy.deepcopy(hparams)\n    self.assertEqual(hparams == hparams2, True)\n    self.assertEqual(hparams != hparams2, False)\n\n    hparams2[\'name\'] = \'MFCC\'\n    self.assertEqual(hparams == hparams2, False)\n    self.assertEqual(hparams != hparams2, True)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
delta/utils/logger.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' logger utils\'\'\'\n\nimport os\nfrom absl import flags\nfrom absl import logging\nimport logging as _logging\n\nFLAGS = flags.FLAGS\n\n\ndef set_logging(is_debug, config):\n  absl_logger = logging.get_absl_logger()\n  # create formatter and add it to the handlers\n  formatter = _logging.Formatter(""[ %(asctime)-15s %(levelname)s %(filename)15s:%(lineno)-4d "" \\\n              "" %(process)-5d ]  %(message)s"")\n\n  log_dir = config[""solver""][""saver""][""model_path""]\n  if not os.path.exists(log_dir):\n    os.makedirs(log_dir)\n\n  logging.get_absl_handler().use_absl_log_file(\n      program_name=\'delta\', log_dir=log_dir)\n\n  fh = _logging.StreamHandler()\n  fh.setLevel(_logging.NOTSET)\n  fh.setFormatter(formatter)\n  absl_logger.addHandler(fh)\n\n  if is_debug:\n    logging.set_verbosity(_logging.DEBUG)\n  else:\n    logging.set_verbosity(_logging.NOTSET)\n\n  logging.info(""Also save log file to directory: {}"".format(log_dir))\n'"
delta/utils/misc.py,22,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' utils for delta \'\'\'\nimport os\nfrom absl import logging\n\nimport numpy as np\nimport delta.compat as tf\n#pylint: disable=no-name-in-module,no-member\nfrom tensorflow.python.client import device_lib\nfrom tensorflow.python.estimator.canned import metric_keys\nfrom tensorflow.python.util import nest\n\nfrom delta.utils.register import registers\n\n\n#pylint: disable=invalid-name\ndef shape_list(x):\n  """"""Return list of dims, statically where possible.""""""\n  x = tf.convert_to_tensor(x)\n\n  # If unknown rank, return dynamic shape\n  if x.get_shape().dims is None:\n    return tf.shape(x)\n\n  static = x.get_shape().as_list()\n  shape = tf.shape(x)\n\n  ret = []\n  for i, _ in enumerate(static):\n    dim = static[i]\n    if dim is None:\n      dim = shape[i]\n    ret.append(dim)\n  return ret\n\n\ndef len_to_mask(length, maxlen=None, dtype=tf.bool):\n  \'\'\' convert length to masking flag \'\'\'\n  # (B) -> (B, T)\n  mask = tf.sequence_mask(length, maxlen=maxlen, dtype=dtype)\n  return mask\n\n\ndef len_to_padding(length, maxlen=None, dtype=tf.bool):\n  \'\'\' convert length to padding flag \'\'\'\n  # (B) -> (B, T)\n  return tf.cast(1 - len_to_mask(length, maxlen=maxlen, dtype=dtype), dtype)\n\n\ndef log_vars(prefix, variables):\n  \'\'\' logging TF varables metadata \'\'\'\n  for var in variables:\n    logging.info(""{}: name: {} shape: {} device: {}"".format(\n        prefix, var.name, var.shape, var.device))\n\n\n#pylint: disable=bad-continuation\ndef losses(config):\n  \'\'\' get loss object from register \'\'\'\n  if \'distilling\' in config[\'solver\'] and config[\'solver\'][\'distilling\'][\n      \'enable\']:\n    loss_name = config[\'solver\'][\'distilling\'][\'loss\']\n  else:\n    loss_name = config[\'solver\'][\'optimizer\'][\'loss\']\n  if isinstance(loss_name, list):\n    _loss_fn = []\n    for one_loss_name in loss_name:\n      logging.info(\'loss == {}\'.format(one_loss_name))\n      _loss_fn.append(registers.loss[one_loss_name](config))\n  else:\n    logging.info(\'loss == {}\'.format(loss_name))\n    _loss_fn = registers.loss[loss_name](config)\n  return _loss_fn\n\n\ndef task(config, mode):\n  \'\'\' get task object from register \'\'\'\n  task_name = config[\'data\'][\'task\'][""name""]\n  logging.info(""task == {}, mode == {}"".format(task_name, mode))\n  _task = registers.task[task_name](config, mode)\n  return _task\n\n\ndef model(config):\n  \'\'\' get model object from register \'\'\'\n  classname = config[\'model\'][\'name\']\n  logging.info(""model == {}"".format(classname))\n  # Model initialization\n  _model = registers.model[classname](config)\n  return _model\n\n\ndef gpu_device_names():\n  \'\'\'\n  :returns, list of gpu device name, num of gpus\n  \'\'\'\n  devices = []\n  for x in device_lib.list_local_devices():  #pylint: disable=invalid-name\n    if x.device_type == \'GPU\':\n      devices.append(tf.compat.as_text(x.name))\n  return devices, len(devices)\n\n\ndef tf_version_satisfy(target_version_str):\n  \'\'\'\n  A convenient function to check TF version.\n\n  Args:\n    target_version_str: a string, e.g. \'1.14\', \'1.12.0\'\n\n  Returns:\n    True if TF version is greater or equal than target version.\n  \'\'\'\n  current_version_str = tf.__version__\n  current_version = [int(num) for num in current_version_str.split(\'.\')]\n  target_version = [int(num) for num in target_version_str.split(\'.\')]\n  satisfied = current_version >= target_version\n  return satisfied\n\n\ndef get_distribution_strategy(num_gpus, all_reduce_alg=\'nccl\'):\n  """"""Return a DistributionStrategy for running the model.\n\n  Args:\n    num_gpus: Number of GPUs to run this model.\n    all_reduce_alg: Specify which algorithm to use when performing all-reduce.\n      See tf.distribute.NcclAllReduce for available algorithms.\n      If None, Strategy will using nccl as default.\n\n  Returns:\n    tf.distribute.Strategy object.\n  """"""\n  if num_gpus == 0:  #pylint: disable=no-else-return\n    return tf.distribute.OneDeviceStrategy(""device:CPU:0"")\n  elif num_gpus == 1:\n    return tf.distribute.OneDeviceStrategy(""device:GPU:0"")\n  else:\n    return tf.distribute.MirroredStrategy(devices=None, cross_device_ops=None)\n\n\ndef per_device_batch_size(batch_size, num_gpus):\n  """"""For multi-gpu, batch-size must be a multiple of the number of GPUs.\n\n  Note that this should eventually be handled by DistributionStrategies\n  directly. Multi-GPU support is currently experimental, however,\n  so doing the work here until that feature is in place.\n\n  Args:\n    batch_size: Global batch size to be divided among devices. This should be\n      equal to num_gpus times the single-GPU batch_size for multi-gpu training.\n    num_gpus: How many GPUs are used with DistributionStrategies.\n\n  Returns:\n    Batch size per device.\n\n  Raises:\n    ValueError: if batch_size is not divisible by number of devices\n  """"""\n  if num_gpus <= 1:\n    return batch_size\n\n  remainder = batch_size % num_gpus\n  if remainder:\n    err = (""When running with multiple GPUs, batch size ""\n           ""must be a multiple of the number of available GPUs. Found {} ""\n           ""GPUs with a batch size of {}; try --batch_size={} instead."").format(\n               num_gpus, batch_size, batch_size - remainder)\n    raise ValueError(err)\n  return int(batch_size / num_gpus)\n\n\n#pylint: disable=too-many-arguments\ndef generate_synthetic_data(input_shape,\n                            input_value=0,\n                            input_dtype=None,\n                            label_shape=None,\n                            label_value=0,\n                            label_dtype=None,\n                            nepoch=None):\n  """"""Create a repeating dataset with constant values.\n\n  Args:\n    input_shape: a tf.TensorShape object or nested tf.TensorShapes. The shape of\n      the input data.\n    input_value: Value of each input element.\n    input_dtype: Input dtype. If None, will be inferred by the input value.\n    label_shape: a tf.TensorShape object or nested tf.TensorShapes. The shape of\n      the label data.\n    label_value: Value of each input element.\n    label_dtype: Input dtype. If None, will be inferred by the target value.\n    nepoch: num of epochs. If None, will repeat forever.\n\n  Returns:\n    Dataset of tensors or tuples of tensors (if label_shape is set).\n  """"""\n  # TODO(kathywu): Replace with SyntheticDataset once it is in contrib.\n  element = input_element = nest.map_structure(\n      lambda s: tf.constant(input_value, input_dtype, s), input_shape)\n\n  if label_shape:\n    label_element = nest.map_structure(\n        lambda s: tf.constant(label_value, label_dtype, s), label_shape)\n    element = (input_element, label_element)\n\n  return tf.data.Dataset.from_tensors(element).repeat(nepoch)\n\n\ndef metric_smaller(best_eval_result,\n                   current_eval_result,\n                   default_key=metric_keys.MetricKeys.AUC):\n  """"""Compares two evaluation results and returns true if the 2nd one is smaller.\n              Both evaluation results should have the values for MetricKeys.LOSS, which are\n              used for comparison.\n              Args:\n                best_eval_result: best eval metrics.\n                current_eval_result: current eval metrics.\n                default_key: metric_keys.MericKeys\n              Returns:\n                True if the loss of current_eval_result is smaller; otherwise, False.\n              Raises:\n               ValueError: If input eval result is None or no loss is available.\n        """"""\n  if not best_eval_result or default_key not in best_eval_result:\n    raise ValueError(\n        \'best_eval_result cannot be empty or no loss is found in it.\')\n\n  if not current_eval_result or default_key not in current_eval_result:\n    raise ValueError(\n        \'current_eval_result cannot be empty or no loss is found in it.\')\n  return best_eval_result[default_key] > current_eval_result[default_key]\n\n\ndef listdir(path):\n  \'\'\' generate files of path \'\'\'\n  for filename in os.listdir(path):\n    yield os.path.join(path, filename)\n\n\ndef walk(path=\'.\', depth=None):\n  """"""\n    recursively walk directory to specified depth\n    :param path: (str) the base path to start walking from\n    :param depth: (None or int) max. recursive depth, None = no limit\n    :yields: (str) filename, including path\n    """"""\n  if depth and depth == 1:\n    for filename in listdir(path):\n      yield filename\n  else:\n    top_pathlen = len(path) + len(os.path.sep)\n    for dirpath, dirnames, filenames in os.walk(path):\n      dirlevel = dirpath[top_pathlen:].count(os.path.sep)\n      if depth and dirlevel >= depth:\n        dirnames[:] = []\n      else:\n        for filename in filenames:\n          yield os.path.join(dirpath, filename)\n\n\n# \'train\', \'eval\', \'infer\'\nTRAIN = tf.estimator.ModeKeys.TRAIN\nEVAL = tf.estimator.ModeKeys.EVAL\nINFER = tf.estimator.ModeKeys.PREDICT\nPAD_IDX = 0\n'"
delta/utils/misc_test.py,9,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' misc.py unittest\'\'\'\nimport numpy as np\nimport delta.compat as tf\n\nfrom delta.utils import misc\n\n\nclass MiscTest(tf.test.TestCase):\n  \'\'\' misc unittest\'\'\'\n\n  def setUp(self):\n    super().setUp()\n    \'\'\'setup\'\'\'\n    self.length = [3, 5, 2]\n    self.mask_true = np.array([\n        [1, 1, 1, 0, 0],\n        [1, 1, 1, 1, 1],\n        [1, 1, 0, 0, 0],\n    ])\n\n  def tearDown(self):\n    \'\'\'tear down\'\'\'\n\n  def test_len_to_mask(self):\n    \'\'\' len to mask unittest\'\'\'\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      mask = misc.len_to_mask(self.length, dtype=tf.int32)\n      self.assertAllEqual(mask.eval(), self.mask_true)\n\n  def test_len_to_padding(self):\n    \'\'\' len to padding unittest\'\'\'\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      padding = misc.len_to_padding(self.length, dtype=tf.int32)\n      self.assertAllEqual(padding.eval(), 1 - self.mask_true)\n\n  def test_gpu_device_names(self):\n    \'\'\' gpu device names unittest\'\'\'\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      devices, ngpus = misc.gpu_device_names()\n      self.assertListEqual(devices, [])\n      self.assertEqual(ngpus, 0)\n\n  def test_per_device_batch_size(self):\n    \'\'\' per device batch size unittest\'\'\'\n    batch_size, ngpus = 32, 2\n    batch_per_dev = misc.per_device_batch_size(batch_size, ngpus)\n    self.assertEqual(batch_per_dev, 16)\n\n    batch_size, ngpus = 32, 1\n    batch_per_dev = misc.per_device_batch_size(batch_size, ngpus)\n    self.assertEqual(batch_per_dev, 32)\n\n    with self.assertRaises(ValueError):\n      batch_size, ngpus = 32, 3\n      batch_per_dev = misc.per_device_batch_size(batch_size, ngpus)\n\n  def test_generate_synthetic_data(self):\n    \'\'\' generate sythetic data unittest\'\'\'\n    input_shape = tf.TensorShape([2, 3])\n    input_value = 1\n    input_dtype = tf.float32\n    label_shape = tf.TensorShape([2])\n    label_value = 2\n    label_dtype = tf.int32\n    nepoch = 2\n\n    data_set = misc.generate_synthetic_data(input_shape, input_value,\n                                            input_dtype, label_shape,\n                                            label_value, label_dtype, nepoch)\n\n    iterator = data_set.make_one_shot_iterator()\n\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      data, label = iterator.get_next()\n      self.assertAllEqual(data.eval(),\n                          np.ones(shape=input_shape, dtype=np.float32))\n      self.assertAllEqual(label.eval(),\n                          2 * np.ones(shape=label_shape, dtype=np.float32))\n\n      with self.assertRaises(tf.errors.OutOfRangeError):\n        data.eval()\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
delta/utils/model.py,13,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Model related utilities.""""""\n\nimport os\nimport numpy as np\nimport hurry.filesize as hfsize\nfrom absl import logging\nimport delta.compat as tf\n# pylint: disable=no-name-in-module\nfrom tensorflow.python.framework import graph_util\n\n\ndef print_ops(graph, prefix=\'\'):\n  """"""Print tensorflow operations in a graph.""""""\n  for operator in graph.get_operations():\n    logging.info(\'{} : op name: {}\'.format(prefix, operator.name))\n\n\ndef log_vars(prefix, variables):\n  """"""Print tensorflow variables.""""""\n  for var in variables:\n    logging.info(""{}: name: {} shape: {} device: {}"".format(\n        prefix, var.name, var.shape, var.device))\n\n\ndef model_size(variables):\n  """"""Get model size.""""""\n  total_params = sum(\n      [np.prod(var.shape.as_list()) * var.dtype.size for var in variables])\n  return hfsize.size(total_params, system=hfsize.alternative)\n\n\ndef save(saver, session, ckpt_dir, ckpt_name=""best""):\n  """"""Save model checkpoint.""""""\n  if not os.path.exists(ckpt_dir):\n    os.makedirs(ckpt_dir)\n    logging.info(""make dir: {}"".format(ckpt_dir))\n  ckpt_path = os.path.join(ckpt_dir, ckpt_name)\n  logging.info(\'[*] saving checkpoints to {}...\'.format(ckpt_path))\n  saver.save(session, ckpt_path)\n\n\ndef load(saver, session, ckpt_dir, ckpt_name=""best""):\n  """"""Load model from a checkpoint.""""""\n  ckpt_path = os.path.join(ckpt_dir, ckpt_name)\n  logging.info(\'[*] Loading checkpoints from {}...\'.format(ckpt_path))\n  try:\n    saver.restore(session, ckpt_path)\n  except Exception as exception:\n    logging.info(exception)\n    logging.info(""check ckpt file path !!!"")\n    raise exception\n\n\ndef get_sess_config(gpu_str=None):\n  """"""generate a session config proto""""""\n  config = tf.ConfigProto()\n\n  # pylint: disable=no-member\n  if gpu_str is None:\n    config.gpu_options.visible_device_list = \'\'\n  else:\n    config.gpu_options.visible_device_list = gpu_str\n    config.gpu_options.allow_growth = True\n  return config\n\n\ndef get_session(sess_config):\n  """"""load a new session""""""\n  return tf.Session(config=sess_config)\n\n\ndef load_frozen_graph(frozen_graph_filename, print_op=False):\n  """"""load a graph from protocol buffer file""""""\n  # We load the protobuf file from the disk and parse it to retrieve the\n  # unserialized graph_def\n  with tf.gfile.GFile(frozen_graph_filename, ""rb"") as in_f:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(in_f.read())\n\n  # Then, we import the graph_def into a new Graph and returns it\n  with tf.Graph().as_default() as graph:  #pylint: disable=not-context-manager\n    # The name var will prefix every op/nodes in your graph\n    # Since we load everything in a new graph, this is not needed\n    tf.import_graph_def(\n        graph_def,\n        input_map=None,\n        return_elements=None,\n        name="""",\n        op_dict=None,\n        producer_op_list=None)\n    if print_op:\n      print_ops(graph, prefix=\'load_frozen_graph\')\n  return graph\n\n\ndef load_graph_session_from_ckpt(ckpt_path, sess_config, print_op=False):\n  """"""load graph and session from checkpoint file""""""\n  graph = tf.Graph()\n  with graph.as_default():  #pylint: disable=not-context-manager\n    sess = get_session(sess_config)\n    with sess.as_default():  #pylint: disable=not-context-manager\n      # Load the saved meta graph and restore variables\n      saver = tf.train.import_meta_graph(""{}.meta"".format(ckpt_path))\n      saver.restore(sess, ckpt_path)\n    if print_op:\n      print_ops(graph, prefix=\'load_graph_session_from_ckpt\')\n  return graph, sess\n\n\ndef load_graph_session_from_pb(pb_file, sess_config, print_op=False):\n  """"""load graph and session from protocol buffer file""""""\n  graph = load_frozen_graph(pb_file, print_op)\n  with graph.as_default():\n    sess = get_session(sess_config)\n  return graph, sess\n\n\ndef load_graph_session_from_saved_model(saved_model_dir,\n                                        sess_config,\n                                        print_op=False):\n  """"""Load graph session from SavedModel""""""\n  if not tf.saved_model.maybe_saved_model_directory(saved_model_dir):\n    raise ValueError(""Not a saved model dir: {}"".format(saved_model_dir))\n\n  logging.info(\'saved model dir : {}\'.format(saved_model_dir))\n  graph = tf.Graph()\n  with graph.as_default():  #pylint: disable=not-context-manager\n    sess = get_session(sess_config)\n    with sess.as_default():  #pylint: disable=not-context-manager\n      tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING],\n                                 saved_model_dir)\n      if print_op:\n        print_ops(graph, prefix=\'load_graph_session_from_saved_model\')\n  return graph, sess\n\n\ndef frozen_graph_to_pb(outputs, frozen_graph_pb_path, sess, graph=None):\n  """"""Freeze graph to a pb file.""""""\n  if not isinstance(outputs, (list)):\n    raise ValueError(""Frozen graph: outputs must be list of output node name"")\n\n  if graph is None:\n    graph = tf.get_default_graph()\n\n  input_graph_def = graph.as_graph_def()\n  logging.info(""Frozen graph: len of input graph nodes: {}"".format(\n      len(input_graph_def.node)))\n\n  # We use a built-in TF helper to export variables to constant\n  output_graph_def = graph_util.convert_variables_to_constants(\n      sess,\n      input_graph_def,\n      outputs,\n  )\n\n  logging.info(""Frozen graph: len of output graph nodes: {}"".format(\n      len(output_graph_def.node)))  # pylint: disable=no-member\n\n  with tf.gfile.GFile(frozen_graph_pb_path, ""wb"") as in_f:\n    in_f.write(output_graph_def.SerializeToString())\n'"
delta/utils/plot.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Plot graphs.""""""\n\n# pylint: disable=wrong-import-position\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\n\n\ndef plot_roc(fpr, tpr, thresholds, auc, save_path):\n  \'\'\' thresholds from hight to low \'\'\'\n  plt.figure()\n  plt.plot(\n      fpr, tpr, color=\'darkorange\', lw=2, label=\'ROC curve (area= %0.2f)\' % auc)\n  plt.plot([0, 1], [0, 1], color=\'navy\', lw=2, linestyle=\'--\')\n  plt.xlim([0.0, 1.0])\n  plt.ylim([0.0, 1.05])\n  plt.xlabel(\'False Positive Rate\')\n  plt.ylabel(\'True Positive Rate\')\n  plt.title(\'Receiver Operating Characteristic\')\n  plt.legend(loc=\'lower right\')\n\n  # create the axis of thresholds (scores)\n  ax2 = plt.gca().twinx()\n  ax2.plot(fpr, thresholds, markeredgecolor=\'r\', linestyle=\'dashed\', color=\'r\')\n  ax2.set_ylabel(\'Threshold\', color=\'r\')\n  ax2.set_ylim([thresholds[-1], thresholds[0]])\n  ax2.set_xlim([fpr[0], fpr[-1]])\n  plt.savefig(save_path)\n  plt.close()\n\n\ndef plot_pr(precision, recall, thresholds, save_path):  # pylint: disable=unused-argument\n  \'\'\' thresholds from low to hight \'\'\'\n  plt.figure()\n\n  plt.step(recall, precision, color=\'b\', alpha=0.2, where=\'post\')\n  plt.fill_between(recall, precision, step=\'post\', alpha=0.2, color=\'b\')\n\n  plt.ylim([0.0, 1.05])\n  plt.xlim([0.0, 1.0])\n  plt.xlabel(\'Recall\')\n  plt.ylabel(\'Precision\')\n  plt.title(\'2-class Precision-Recall curve\')\n  plt.legend(loc=\'lower right\')\n\n  # pylint: disable=pointless-string-statement\n  \'\'\'\n    # create the axis of thresholds (scores)\n    thresholds = np.append(thresholds, 1.0)\n    ax2 = plt.gca().twinx()\n    ax2.plot(recall, thresholds, markeredgecolor=\'r\', linestyle=\'dashed\', color=\'r\')\n    ax2.set_ylabel(\'Threshold\', color=\'r\')\n    ax2.set_ylim( [thresholds[0], thresholds[-1]] )\n    ax2.set_xlim( [recall[0], recall[-1]] )\n  \'\'\'\n\n  plt.savefig(save_path)\n  plt.close()\n\n\ndef plot_det(fps, fns, save_path):\n  """"""\n    This might be a good metric when you are doing a binary classification problem\n      with two species and you care how it is incorrectly classifying both species.\n    Given false positive and false negative rates,\n      produce a DET(detection error rate) Curve.\n    The false positive rate is assumed to be increasing while the false\n    negative rate is assumed to be decreasing.\n  """"""\n  # pylint: disable=unused-argument, unused-variable, unused-variable, invalid-name\n  axis_min = min(fps[0], fns[-1])\n  fig, ax = plt.subplots()\n  ax.plot(fps, fns)\n  ax.yscale(\'log\')\n  ax.xscale(\'log\')\n  ticks_to_use = [\n      0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50\n  ]\n  ax.get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n  ax.get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n  ax.set_xticks(ticks_to_use)\n  ax.set_yticks(ticks_to_use)\n  ax.axis([0.001, 50, 0.001, 50])\n  ax.savefig(save_path)\n'"
delta/utils/register.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Module register.""""""\n\nimport importlib\nimport os\nimport sys\nfrom absl import logging\n\n\nclass Register:\n  """"""Module register""""""\n\n  def __init__(self, registry_name):\n    self._dict = {}\n    self._name = registry_name\n\n  def __setitem__(self, key, value):\n    if not callable(value):\n      raise Exception(""Value of a Registry must be a callable."")\n    if key is None:\n      key = value.__name__\n    if key in self._dict:\n      logging.warning(""Key %s already in registry %s."" % (key, self._name))\n    self._dict[key] = value\n\n  def register(self, param):\n    """"""Decorator to register a function or class.""""""\n\n    def decorator(key, value):\n      self[key] = value\n      return value\n\n    if callable(param):\n      # @reg.register\n      return decorator(None, param)\n    # @reg.register(\'alias\')\n    return lambda x: decorator(param, x)\n\n  def __getitem__(self, key):\n    try:\n      return self._dict[key]\n    except Exception as e:\n      logging.error(f""module {key} not found: {e}"")\n      raise e\n\n  def __contains__(self, key):\n    return key in self._dict\n\n  def keys(self):\n    """"""key""""""\n    return self._dict.keys()\n\n\nclass registers():  # pylint: disable=invalid-name, too-few-public-methods\n  """"""All module registers.""""""\n\n  def __init__(self):\n    raise RuntimeError(""Registries is not intended to be instantiated"")\n\n  task = Register(\'task\')\n  model = Register(\'model\')\n  solver = Register(\'solver\')\n  loss = Register(\'loss\')\n  metric = Register(\'metric\')\n  preparer = Register(\'preparer\')\n  preprocessor = Register(\'preprocessor\')\n  postprocess = Register(\'postprocess\')\n  serving = Register(\'serving\')\n  dataset = Register(\'dataset\')\n\n\nNLP_TASK_MODULES = [\n    ""text_cls_task"", ""text_seq_label_task"", ""text_match_task"",\n    ""text_nlu_joint_task"", ""speaker_cls_task"", ""text_seq2seq_task""\n]\n\nTASK_MODULES = [\n    ""text_cls_task"", ""text_seq_label_task"", ""text_match_task"",\n    ""text_nlu_joint_task"", ""speaker_cls_task"", ""text_seq2seq_task"",\n    ""asr_seq_task"", ""kws_cls_task"", ""speech_cls_task"", ""speech_cls_task""\n]\n\nNLP_MODEL_MODULES = [\n    ""text_seq_model"", ""text_hierarchical_model"", ""text_seq_label_model"",\n    ""text_nlu_joint_model"", ""text_match_model"", ""text_seq_label_model"",\n    ""text_seq2seq_model""\n]\n\nMODEL_MODULES = [\n    ""speech_cls_rawmodel"", ""speaker_cls_rawmodel"", ""speech_cls_model"",\n    ""kws_model"", ""asr_model"", ""resnet_model"", ""text_seq_model"",\n    ""text_hierarchical_model"", ""text_seq_label_model"", ""text_nlu_joint_model"",\n    ""text_match_model"", ""text_seq_label_model"", ""text_seq2seq_model"",\n    ""multimodal_cls_model""\n]\n\nNLP_LOSS_MODULES = [""loss_impl""]\n\nLOSS_MODULES = [""loss_impl""]\n\nNLP_METRICS_MODULES = [""py_metrics""]\n\nMETRICS_MODULES = [""py_metrics""]\n\nNLP_SOLVER_MODULES = [\n    ""raw_cls_solver"", ""raw_match_solver"", ""keras_solver"",\n    ""raw_seq_label_solver"", ""raw_nlu_joint_solver"", ""raw_seq2seq_solver"",\n    ""raw_pretrain_cls_solver"", ""raw_pretrain_seq_label_solver""\n]\n\nSOLVER_MODULES = [\n    ""raw_cls_solver"", ""raw_match_solver"", ""keras_solver"", ""emotion_solver"",\n    ""kws_solver"", ""asr_solver"", ""speaker_solver"", ""raw_seq_label_solver"",\n    ""raw_nlu_joint_solver"", ""raw_seq2seq_solver"", ""raw_pretrain_cls_solver"",\n    ""raw_pretrain_seq_label_solver""\n]\n\nNLP_POSTPROCESS_MODULES = [\n    ""text_cls_proc"", ""text_seq_label_proc"", ""text_seq2seq_proc""\n]\n\nPOSTPROCESS_MODULES = [\n    ""speech_cls_proc"", ""speaker_cls_proc"", ""text_cls_proc"",\n    ""text_seq_label_proc"", ""text_seq2seq_proc""\n]\n\nNLP_SERVING_MODULES = [""eval_text_cls_pb""]\n\nSERVING_MODULES = [\n    ""knowledge_distilling"", ""eval_asr_pb"", ""eval_speech_cls_pb"",\n    ""eval_text_cls_pb""\n]\n\nNLP_PREPROCESS_MODULES = [\n    ""text_cls_preparer"", ""text_match_preparer"", ""text_seq_label_preparer"",\n    ""text_nlu_joint_preparer"", ""text_seq2seq_preparer""\n]\n\nPREPROCESS_MODULES = [\n    ""text_cls_preparer"", ""text_match_preparer"", ""text_seq_label_preparer"",\n    ""text_nlu_joint_preparer"", ""text_seq2seq_preparer""\n]\n\nNLP_DATA_SETS = [\'atis\', \'atis2\', \'mock_text_cls_data\', \'mock_text_match_data\',\n                 \'mock_text_nlu_joint_data\', \'mock_text_seq2seq_data\', \'mock_text_seq_label_data\',\n                 \'conll_2003\', \'snli\', \'trec\', \'yahoo_answer\']\n\nALL_NLP_MODULES = [(""delta.data.task"", NLP_TASK_MODULES),\n                   (""delta.models"", NLP_MODEL_MODULES),\n                   (""delta.utils.loss"", NLP_LOSS_MODULES),\n                   (""delta.utils.metrics"", NLP_METRICS_MODULES),\n                   (""delta.utils.solver"", NLP_SOLVER_MODULES),\n                   (""delta.utils.postprocess"", NLP_POSTPROCESS_MODULES),\n                   (""delta.serving"", NLP_SERVING_MODULES),\n                   (""delta.data.preprocess"", NLP_PREPROCESS_MODULES),\n                   (\'delta.data.datasets\', NLP_DATA_SETS)]\n\nALL_MODULES = [(""delta.data.task"", TASK_MODULES),\n               (""delta.models"", MODEL_MODULES),\n               (""delta.utils.loss"", LOSS_MODULES),\n               (""delta.utils.metrics"", METRICS_MODULES),\n               (""delta.utils.solver"", SOLVER_MODULES),\n               (""delta.utils.postprocess"", POSTPROCESS_MODULES),\n               (""delta.serving"", SERVING_MODULES),\n               (""delta.data.preprocess"", PREPROCESS_MODULES)]\n\n\ndef _handle_errors(errors):\n  """"""Log out and possibly reraise errors during import.""""""\n  if not errors:\n    return\n  for name, err in errors:\n    logging.warning(""Module {} import failed: {}"".format(name, err))\n  logging.fatal(""Please check these modules."")\n\n\ndef path_to_module_format(py_path):\n  """"""Transform a python file path to module format.""""""\n  return py_path.replace(""/"", ""."").rstrip("".py"")\n\n\ndef add_custom_modules(all_modules, config=None):\n  """"""Add custom modules to all_modules""""""\n  current_work_dir = os.getcwd()\n  if current_work_dir not in sys.path:\n    sys.path.append(current_work_dir)\n  if config is not None and ""custom_modules"" in config:\n    custom_modules = config[""custom_modules""]\n    if not isinstance(custom_modules, list):\n      custom_modules = [custom_modules]\n    all_modules += [\n        ("""", [path_to_module_format(module)]) for module in custom_modules\n    ]\n\n\ndef import_all_modules_for_register(config=None, only_nlp=False):\n  """"""Import all modules for register.""""""\n  if only_nlp:\n    all_modules = ALL_NLP_MODULES\n  else:\n    all_modules = ALL_MODULES\n\n  add_custom_modules(all_modules, config)\n\n  logging.debug(f""All modules: {all_modules}"")\n  errors = []\n  for base_dir, modules in all_modules:\n    for name in modules:\n      try:\n        if base_dir != """":\n          full_name = base_dir + ""."" + name\n        else:\n          full_name = name\n        importlib.import_module(full_name)\n        logging.debug(f""{full_name} loaded."")\n      except ImportError as error:\n        errors.append((name, error))\n  _handle_errors(errors)\n'"
delta/utils/summary.py,7,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Summary related utilities.""""""\n\nimport delta.compat as tf\n\n\ndef flush(writer=None, name=None):\n  """"""Flush""""""\n  tf.summary.flush(writer, name)  # pylint: disable=no-member\n\n\ndef scalar(name, value):  # pylint: redefined-outer-name\n  ""Scalar""\n  tf.summary.scalar(name, value)\n\n\ndef histogram(name, values):\n  ""Histogram""\n  tf.summary.histogram(name, values)\n\n\ndef text(name, tensor):\n  ""Text""\n  tf.summary.text(name, tensor)\n\n\ndef audio(name, tensor, sample_rate, max_outputs=3):\n  ""Audio""\n  tf.summary.audio(name, tensor, sample_rate, max_outputs)\n\n\ndef image(name, tensor, max_images=3):\n  ""Image""\n  tf.summary.image(name, tensor, max_outputs=max_images)\n\n\n# pylint: disable=too-many-arguments\ndef summary_writer(logdir,\n                   graph=None,\n                   max_queue=10,\n                   flush_secs=120,\n                   graph_def=None,\n                   filename_suffix=None,\n                   session=None,\n                   name=None):\n  """"""Summary writer.""""""\n  return tf.summary.create_file_writer(\n      logdir,\n      max_queue=max_queue,\n      flush_millis=flush_secs * 1000,\n      filename_suffix=filename_suffix,\n      name=name)\n'"
tools/install/check_install.py,3,"b'#!/usr/bin/env python\n\n# Init from espnet: https://github.com/espnet/espnet\n# modify to support tensorflow\n# Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n\nimport argparse\nimport importlib\nimport logging\nimport sys\n\n# you should add the libraries which are not included in setup.py\nMANUALLY_INSTALLED_LIBRARIES = [\n    (\'kaldiio\', None),\n    (\'matplotlib\', None),\n    (\'librosa\', None),\n    (\'sklearn\', None),\n    (\'pandas\', None),\n    (\'soundfile\', None),\n    (\'textgrid\', None),\n    (\'yapf\', None),\n    (\'jieba\', None),\n    (\'yaml\', None),\n    (\'absl\', None),\n    (\'hurry.filesize\', None),\n    (\'gensim\', None),\n]\n\nlogging.basicConfig(level=logging.INFO, format=""%(levelname)s: %(message)s"")\n\nlogging.info(""python version = "" + sys.version)\n\nlibrary_list = []\nlibrary_list.extend(MANUALLY_INSTALLED_LIBRARIES)\n\n# check library availableness\nlogging.info(""library availableness check start."")\nlogging.info(""# libraries to be checked = %d"" % len(library_list))\nis_correct_installed_list = []\nfor idx, (name, version) in enumerate(library_list):\n  try:\n    importlib.import_module(name)\n    logging.info(""--> %s is installed."" % name)\n    is_correct_installed_list.append(True)\n  except ImportError:\n    logging.warning(""--> %s is not installed."" % name)\n    is_correct_installed_list.append(False)\nlogging.info(""library availableness check done."")\nlogging.info(""%d / %d libraries are correctly installed."" %\n             (sum(is_correct_installed_list), len(library_list)))\n\nif len(library_list) != sum(is_correct_installed_list):\n  logging.info(""please try to setup again and then re-run this script."")\n  sys.exit(1)\n\n# check library version\nnum_version_specified = sum(\n    [True if v is not None else False for n, v in library_list])\nlogging.info(""library version check start."")\nlogging.info(""# libraries to be checked = %d"" % num_version_specified)\nis_correct_version_list = []\nfor idx, (name, version) in enumerate(library_list):\n  if version is not None:\n    lib = importlib.import_module(name)\n    if hasattr(lib, ""__version__""):\n      is_correct = lib.__version__ in version\n      if is_correct:\n        logging.info(""--> %s version is matched."" % name)\n        is_correct_version_list.append(True)\n      else:\n        logging.warning(""--> %s version is not matched (%s is not in %s)."" %\n                        (name, lib.__version__, str(version)))\n        is_correct_version_list.append(False)\n    else:\n      logging.info(""--> %s has no version info, but version is specified."" %\n                   name)\n      logging.info(""--> maybe it is better to reinstall the latest version."")\n      is_correct_version_list.append(False)\nlogging.info(""library version check done."")\nlogging.info(""%d / %d libraries are correct version."" %\n             (sum(is_correct_version_list), num_version_specified))\n\nif sum(is_correct_version_list) != num_version_specified:\n  logging.info(""please try to setup again and then re-run this script."")\n  sys.exit(1)\n\n# check cuda availableness\nlogging.info(""cuda availableness check start."")\nimport tensorflow as tf\ntry:\n  assert tf.test.is_gpu_available()\n  logging.info(""--> cuda is available in tensorflow."")\nexcept AssertionError:\n  logging.warning(""--> it seems that cuda is not available in tensorflow."")\n\ntry:\n  assert len(tf.test.gpu_device_name()) > 1\n  logging.info(""--> multi-gpu is available (#gpus = %d)."" %\n               len(tf.test.gpu_device_name()))\nexcept AssertionError:\n  logging.warning(""--> it seems that only single gpu is available."")\n  logging.warning(\'--> maybe your machine has only one gpu.\')\nlogging.info(""cuda availableness check done."")\n'"
utils/deploy/convert_frozen_pb_to_tftrt.py,1,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n#frozen graph tf version must be same to this tf version\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\n\nimport delta.compat as tf\nimport tensorflow.contrib.tensorrt as trt\nfrom tensorflow.python.platform import gfile\n\nworkspace_size = 1 << 30\n\n\ndef getFrozenGraph(input_graph):\n  with gfile.FastGFile(input_graph, \'rb\') as f:\n    graph_def = tf.GraphDef()\n    graph_def.ParseFromString(f.read())\n  return graph_def\n\n\ndef getFP32(input_graph, out_tensor, precision, batch_size, workspace_size):\n  graph_prefix = input_graph.split(\'.pb\')[0]\n  output_graph = graph_prefix + ""_tftrt_"" + precision + "".pb""\n  #print(""output graph is "", output_graph)\n  tftrt_graph = trt.create_inference_graph(\n      getFrozenGraph(input_graph), [out_tensor],\n      max_batch_size=batch_size,\n      max_workspace_size_bytes=workspace_size,\n      precision_mode=precision)  # Get optimized graph\n  with gfile.FastGFile(output_graph, \'wb\') as f:\n    f.write(tftrt_graph.SerializeToString())\n\n\nif ""__main__"" in __name__:\n  P = argparse.ArgumentParser(description=""tftrt grpah convert tool!!"")\n  P.add_argument(\'--input_graph\', help=""input tf frozen_gaph.pb"")\n  P.add_argument(\'--out_tensor\', help=""output tensor name "")\n  P.add_argument(\'--precision_mode\', help=""FP32, FP16, INT8"")\n  P.add_argument(\'--batch_size\', type=int, default=1024)\n  P.add_argument(\n      \'--workspace_size\',\n      type=int,\n      default=1 << 30,\n      help=""workspace size in MB"")\n  P.add_argument(\'--gpu\', default=0, help=""select gpu"")\n\n  f = P.parse_args()\n  os.environ[\'CUDA_VISIBLE_DEVICES\'] = f.gpu\n\n  input_graph = f.input_graph\n  out_tensor = f.out_tensor\n  precision = f.precision_mode\n  batch_size = f.batch_size\n  workspace_size = f.workspace_size\n\n  print(""input graph is "", input_graph)\n  print(""output tensor is "", out_tensor)\n  print(""output precision_mode is "", precision)\n  print(""batch_size is "", batch_size)\n  print(""workspace_size is "", workspace_size)\n\n  getFP32(input_graph, out_tensor, precision, batch_size, workspace_size)\n'"
utils/deploy/tflite_convert.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport delta.compat as tf\nimport sys\nimport os\n\'\'\'\npython3 tflite_convert.py saved_model_dir\n\'\'\'\n\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = \'\'\n\nconverter = tf.contrib.lite.TFLiteConverter.from_saved_model(\n    saved_model_dir=sys.argv[1], input_shapes={\'inputs\': [5, 2000, 40, 3]})\nconverter.dump_graphviz_dir = \'./lite_dump\'\n\'\'\'\nconverter.inference_type=tf.contrib.lite.constants.QUANTIZED_UINT8\nconverter.quantized_input_stats={\'inputs\': (127, 1.0/128)}\nconverter.default_ranges_stats=(0, 6)\n\'\'\'\n\ntflite_model = converter.convert()\nwith open(\'model.tflite\', \'wb\') as f:\n  f.write(tflite_model)\n'"
utils/deploy/tflite_run.py,1,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport delta.compat as tf\nimport sys\nimport os\n\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = \'\'\n\ninterpreter = tf.contrib.lite.Interpreter(model_path=\'./model.tflite\')\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nprint(\'Inputs\', input_details)\nprint(\'Outputs\', output_details)\n\n#interpreter.resize_tensor_input(input_details[0][\'index\'], [2, 2000, 40, 3])\n\ninput_details = interpreter.get_input_details()\nprint(\'Inputs\', input_details)\ninterpreter.allocate_tensors()\ninterpreter.invoke()\n'"
utils/nlp/scale.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""\nIf the scale rate is smaller than 1, it returns the subset of the origin data;\nIf the scale rate is bigger than 1, it repeats the data and return.\n""""""\n\nimport sys\nfrom absl import logging\n\n\ndef data_generator(data):\n  while True:\n    for i in range(len(data)):\n      yield data[i]\n\n\ndef scale_data(original_file, new_file, scale_rate):\n  logging.info(""Scale file from {} to {}"".format(original_file, new_file))\n\n  with open(original_file, encoding=""utf-8"") as original_f, \\\n    open(new_file, ""w"", encoding=""utf-8"") as new_f:\n\n    original_lines = original_f.readlines()\n    original_size = len(original_lines)\n    new_size = int(original_size * scale_rate)\n\n    for i, line in enumerate(data_generator(original_lines)):\n      if i >= new_size:\n        break\n      new_f.write(line)\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n\n  if len(sys.argv) != 4:\n    logging.error(""Usage python {} original_file new_file scale_rate"".format(\n        sys.argv[0]))\n    sys.exit(-1)\n\n  original_file = sys.argv[1]\n  new_file = sys.argv[2]\n  scale_rate = float(sys.argv[3])\n\n  scale_data(original_file, new_file, scale_rate)\n'"
utils/nlp/split_train_dev.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport sys\nfrom sklearn.model_selection import train_test_split\nfrom absl import logging\n\n\ndef split_train_dev(original_train, train_file, dev_file, split_rate):\n  with open(original_train, encoding=""utf-8"") as original_train_f, \\\n    open(train_file, ""w"", encoding=""utf-8"") as train_f, \\\n    open(dev_file, ""w"", encoding=""utf-8"") as dev_f:\n    lines = original_train_f.readlines()\n    lines_train, lines_dev = train_test_split(\n        lines, test_size=split_rate, random_state=2019)\n\n    logging.info(""Save train file to {}"".format(train_file))\n    for line in lines_train:\n      train_f.write(line)\n\n    logging.info(""Save train file to {}"".format(dev_file))\n    for line in lines_dev:\n      dev_f.write(line)\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n\n  if len(sys.argv) != 5:\n    logging.error(\n        ""Usage python {} original_train train_file dev_file split_rate"".format(\n            sys.argv[0]))\n    sys.exit(-1)\n\n  original_train = sys.argv[1]\n  train_file = sys.argv[2]\n  dev_file = sys.argv[3]\n  split_rate = float(sys.argv[4])\n\n  split_train_dev(original_train, train_file, dev_file, split_rate)\n'"
utils/speech/apply_cmvn.py,0,"b'#!/usr/bin/env python3\n\n# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport argparse\nimport kaldiio\nimport numpy as np\nfrom espnet.utils.cli_writers import KaldiWriter\nfrom espnet.utils.cli_readers import KaldiReader\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""\nfrom delta.data.frontend.cmvn import CMVN\n\n\ndef get_parser():\n  parser = argparse.ArgumentParser(\n      description=\'Apply mean-variance normalization to files\',\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n  parser.add_argument(\n      \'--norm_means\',\n      type=bool,\n      default=True,\n      help=\'Do mean normalization or not.\')\n  parser.add_argument(\n      \'--norm_vars\',\n      type=bool,\n      default=False,\n      help=\'Do variance normalization or not.\')\n  parser.add_argument(\n      \'--reverse\', type=bool, default=False, help=\'Do reverse mode or not\')\n  parser.add_argument(\n      \'--std_floor\',\n      type=float,\n      default=1e-20,\n      help=\'The std floor of norm_vars\')\n  parser.add_argument(\n      \'--spk2utt\',\n      type=str,\n      help=\'A text file of speaker to utterance-list map. \'\n      \'(Don\\\'t give rspecifier format, such as ""ark:spk2utt"")\')\n  parser.add_argument(\n      \'--utt2spk\',\n      type=str,\n      help=\'A text file of utterance to speaker map. \'\n      \'(Don\\\'t give rspecifier format, such as ""ark:utt2spk"")\')\n  parser.add_argument(\n      \'--write_num_frames\',\n      type=str,\n      help=\'Specify wspecifer for utt2num_frames\')\n  parser.add_argument(\n      \'--compress\',\n      type=bool,\n      default=False,\n      help=\'Save data in compressed format\')\n  parser.add_argument(\n      \'--compression_method\',\n      type=int,\n      default=2,\n      help=\'Specify the method of compression\')\n  parser.add_argument(\n      \'--verbose\', \'-V\', default=0, type=int, help=\'Verbose option\')\n  parser.add_argument(\n      \'stats_rspecifier_or_rxfilename\',\n      help=\'Input stats. e.g. ark:stats.ark or stats.ark\')\n  parser.add_argument(\n      \'rspecifier\', type=str, help=\'Read specifier id. e.g. scp:some.scp\')\n  parser.add_argument(\n      \'wspecifier\', type=str, help=\'Write specifier id. e.g. ark:some.ark\')\n\n  return parser\n\n\ndef apply_cmvn():\n  args = get_parser().parse_args()\n\n  if \':\' in args.stats_rspecifier_or_rxfilename:\n    is_rspcifier = True\n    stats_filetype = \'ark\'\n    stats_dict = dict(KaldiReader(args.stats_rspecifier_or_rxfilename))\n  else:\n    is_rspcifier = False\n    stats_filetype = \'mat\'\n    stats = kaldiio.load_mat(args.stats_rspecifier_or_rxfilename)\n    stats_dict = {None: stats}\n\n  config = {}\n  config[\'norm_means\'] = args.norm_means\n  config[\'norm_vars\'] = args.norm_vars\n  config[\'utt2spk\'] = args.utt2spk\n  config[\'spk2utt\'] = args.spk2utt\n  config[\'reverse\'] = args.reverse\n  config[\'std_floor\'] = args.std_floor\n  config[\'filetype\'] = stats_filetype\n\n  cmvn = CMVN.params(config).instantiate()\n  cmvn.call(stats_dict)\n\n  with KaldiWriter(args.wspecifier, write_num_frames=args.write_num_frames,\n                compress=args.compress, compression_method=args.compression_method) as writer, \\\n    kaldiio.ReadHelper(args.rspecifier) as reader:\n    for utt, mat in reader:\n      mat_new = cmvn.apply_cmvn(mat, utt)\n      writer[utt] = mat_new\n\n\nif __name__ == \'__main__\':\n  apply_cmvn()\n'"
utils/speech/compute_cmvn_stats.py,0,"b'#!/usr/bin/env python3\n\n# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport argparse\nimport logging\nimport kaldiio\nimport numpy as np\nfrom espnet.utils.cli_writers import KaldiWriter\nfrom espnet.utils.cli_readers import KaldiReader\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""\n\n\ndef get_parser():\n  parser = argparse.ArgumentParser(\n      description=\'Compute cepstral mean and variance normalization statistics\'\n      \'per-utterance by default, or per-speaker if spk2utt option provided,\'\n      \'if wxfilename: global\',\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument(\n      \'--spk2utt\',\n      type=str,\n      default=None,\n      help=\'A text file of speaker to utterance-list map. \'\n      \'(Don\\\'t give rspecifier format, such as ""ark:spk2utt"")\')\n  parser.add_argument(\n      \'--verbose\', \'-V\', default=0, type=int, help=\'Verbose option\')\n  parser.add_argument(\n      \'rspecifier\', type=str, help=\'Read specifier id. e.g. scp:some.scp\')\n  parser.add_argument(\n      \'wspecifier_or_wxfilename\',\n      type=str,\n      help=\'Write specifier id. e.g. ark:some.ark\')\n  return parser\n\n\ndef compute_cmvn_stats():\n  """"""\n  e.g. compute_cmvn_stats.py scp:data/train/feats.scp data/train/cmvn.ark # compute global cmvn\n  """"""\n  args = get_parser().parse_args()\n\n  is_wspecifier = \':\' in args.wspecifier_or_wxfilename\n\n  if is_wspecifier:\n    if args.spk2utt is not None:\n      utt2spk_dict = {}\n      with open(args.spk2utt) as f:\n        for line in f:\n          spk, utts = line.rstrip().split(None, 1)\n          for utt in utts.split():\n            utt2spk_dict[utt] = spk\n\n      def utt2spk(x):\n        return utt2spk_dict[x]\n    else:\n      logging.info(\'Performing as utterance CMVN mode\')\n\n      def utt2spk(x):\n        return x\n\n  else:\n    logging.info(\'Performing as gloabl CMVN model\')\n    if args.spk2utt is not None:\n      logging.warning(\'spk2utt is not used for global CMVN mode\')\n\n    def utt2spk(x):\n      return None\n\n  # Calculate stats for each speaker\n  counts = {}\n  sum_feats = {}\n  square_sum_feats = {}\n\n  idx = 0\n  for idx, (utt, matrix) in enumerate(KaldiReader(args.rspecifier), 1):\n    spk = utt2spk(utt)\n\n    if spk not in counts:\n      counts[spk] = 0\n      feat_shape = matrix.shape[1:]\n      sum_feats[spk] = np.zeros(feat_shape, dtype=np.float)\n      square_sum_feats[spk] = np.zeros(feat_shape, dtype=np.float)\n\n    counts[spk] += matrix.shape[0]\n    sum_feats[spk] += matrix.sum(axis=0)\n    square_sum_feats[spk] += (matrix**2).sum(axis=0)\n\n  assert idx > 0, idx\n\n  cmvn_stats = {}\n  for spk in counts:\n    feat_shape = sum_feats[spk].shape\n    cmvn_shape = (2, feat_shape[0] + 1) + feat_shape[1:]\n    _cmvn_stats = np.empty(cmvn_shape, dtype=np.float64)\n    _cmvn_stats[0, :-1] = sum_feats[spk]\n    _cmvn_stats[1, :-1] = square_sum_feats[spk]\n\n    _cmvn_stats[0, -1] = counts[spk]\n    _cmvn_stats[1, -1] = 0.\n\n    cmvn_stats[spk] = _cmvn_stats\n\n  if is_wspecifier:\n    with KaldiWriter(args.wspecifier_or_wxfilename) as writer:\n      for spk, mat in cmvn_stats.items():\n        writer[spk] = mat\n  else:\n    matrix = cmvn_stats[None]\n    kaldiio.save_mat(args.wspecifier_or_wxfilename, matrix)\n\n\nif __name__ == ""__main__"":\n  compute_cmvn_stats()\n'"
utils/speech/compute_fbank_feats.py,3,"b'#!/usr/bin/env python3\n\n# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Create Fbank feature files.""""""\n\nimport delta.compat as tf\nimport argparse\nfrom distutils.util import strtobool\nimport kaldiio\nimport numpy as np\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""\nfrom delta.data.frontend.fbank import Fbank\nfrom espnet.utils.cli_writers import KaldiWriter\n\n\ndef get_parser():\n  parser = argparse.ArgumentParser(\n      description=\'Compute fbank features from wav.\',\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument(\n      \'--sample_rate\', type=int, default=16000, help=\'Sampling frequency\')\n  parser.add_argument(\n      \'--upper_frequency_limit\',\n      type=float,\n      default=0,\n      help=\'Maxinum frequency\')\n  parser.add_argument(\n      \'--lower_frequency_limit\',\n      type=float,\n      default=20,\n      help=\'Minimum frequency\')\n  parser.add_argument(\n      \'--filterbank_channel_count\',\n      type=float,\n      default=23,\n      help=\'Order of fbank\')\n  parser.add_argument(\n      \'--window_length\', type=float, default=0.025, help=\'Length of a frame\')\n  parser.add_argument(\n      \'--frame_length\', type=float, default=0.010, help=\'Hop size of window\')\n  parser.add_argument(\n      \'--output_type\',\n      type=int,\n      default=1,\n      help=\'1 for power spectrum, 2 for log-power spectrum.\')\n  parser.add_argument(\n      \'--window_type\',\n      type=str,\n      default=\'povey\',\n      help=\'Type of window (""hamm""|""hann""|""povey""|""rect""|""blac""|""tria"").\')\n  parser.add_argument(\n      \'--snip_edges\',\n      type=bool,\n      default=True,\n      help=\'The last frame (shorter than window_length) will not be cutoff.\')\n  parser.add_argument(\n      \'--dither\',\n      type=float,\n      default=0.0,\n      help=\'Dithering constant (0.0 means no dither).\')\n  parser.add_argument(\n      \'--raw_energy\',\n      type=int,\n      default=1,\n      help=\'Compute frame energy before preemphasis and windowing.\')\n  parser.add_argument(\n      \'--preeph_coeff\',\n      type=float,\n      default=0.97,\n      help=\'Coefficient for use in frame-signal preemphasis.\')\n  parser.add_argument(\n      \'--remove_dc_offset\',\n      type=bool,\n      default=True,\n      help=\' Subtract mean from waveform on each frame\')\n  parser.add_argument(\n      \'--is_fbank\',\n      type=bool,\n      default=True,\n      help=\'Compute power spetrum without frame energy\')\n  parser.add_argument(\n      \'--write_num_frames\',\n      type=str,\n      help=\'Specify wspecifer for utt2num_frames\')\n  parser.add_argument(\n      \'--compress\',\n      type=strtobool,\n      default=False,\n      help=\'Save data in compressed format\')\n  parser.add_argument(\n      \'--compression_method\',\n      type=int,\n      default=2,\n      help=\'Specify the method of compression\')\n  parser.add_argument(\n      \'--verbose\', \'-V\', default=0, type=int, help=\'Verbose option\')\n  parser.add_argument(\n      \'--segments\',\n      type=str,\n      help=\'segments-file format: each line is either\'\n      \'<segment-id> <recording-id> <start-time> <end-time>\'\n      \'e.g. call-861225-A-0050-0065 call-861225-A 5.0 6.5\')\n  parser.add_argument(\'rspecifier\', type=str, help=\'WAV scp file\')\n  parser.add_argument(\'wspecifier\', type=str, help=\'Writer specifier\')\n  return parser\n\n\ndef compute_fbank():\n  parser = get_parser()\n  args = parser.parse_args()\n\n  config = {}\n  config[\'sample_rate\'] = int(args.sample_rate)\n  config[\'upper_frequency_limit\'] = float(args.upper_frequency_limit)\n  config[\'lower_frequency_limit\'] = float(args.lower_frequency_limit)\n  config[\'filterbank_channel_count\'] = float(args.filterbank_channel_count)\n  config[\'window_length\'] = args.window_length\n  config[\'frame_length\'] = args.frame_length\n  config[\'output_type\'] = args.output_type\n  config[\'window_type\'] = args.window_type\n  config[\'snip_edges\'] = args.snip_edges\n  config[\'preeph_coeff\'] = args.preeph_coeff\n  config[\'remove_dc_offset\'] = args.remove_dc_offset\n  config[\'is_fbank\'] = args.is_fbank\n  config[\'dither\'] = args.dither\n\n  fbank = Fbank.params(config).instantiate()\n\n  with kaldiio.ReadHelper(args.rspecifier,\n                          segments=args.segments) as reader, \\\n        KaldiWriter(args.wspecifier, write_num_frames=args.write_num_frames,\n                    compress=args.compress, compression_method=args.compression_method) as writer:\n    for utt_id, (sample_rate, array) in reader:\n      if sample_rate != args.sample_rate:\n        args.sample_rate = sample_rate\n      array = array.astype(np.float32)\n      audio_data = tf.constant(array, dtype=tf.float32)\n      fbank_test = tf.squeeze(fbank(audio_data, args.sample_rate))\n      sess = tf.Session()\n      fbank_feats = fbank_test.eval(session=sess)\n      writer[utt_id] = fbank_feats\n\n\nif __name__ == ""__main__"":\n  compute_fbank()\n'"
utils/speech/compute_fbank_pitch.py,2,"b'#!/usr/bin/env python3\n# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Create fbank_picth feature files.""""""\n\nimport delta.compat as tf\nimport argparse\nfrom distutils.util import strtobool\nimport kaldiio\nimport numpy as np\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""\nfrom delta.data.frontend.fbank_pitch import FbankPitch\nfrom espnet.utils.cli_writers import KaldiWriter\n\n\ndef get_parser():\n  parser = argparse.ArgumentParser(\n      description=\'Compute fbank && pitch feature from wav.\',\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument(\n      \'--sample_rate\', type=float, default=16000, help=\'Sampling frequency\')\n  parser.add_argument(\n      \'--upper_frequency_limit\',\n      type=float,\n      default=4000,\n      help=\'Maxinum frequency\')\n  parser.add_argument(\n      \'--lower_frequency_limit\',\n      type=float,\n      default=20,\n      help=\'Minimum frequency\')\n  parser.add_argument(\n      \'--filterbank_channel_count\',\n      type=float,\n      default=40,\n      help=\'Order of fbank\')\n  parser.add_argument(\n      \'--dither\',\n      type=float,\n      default=0.0,\n      help=\'Dithering constant (0.0 means no dither).\')\n  parser.add_argument(\n      \'--window_length\', type=float, default=0.025, help=\'Length of a frame\')\n  parser.add_argument(\n      \'--frame_length\', type=float, default=0.010, help=\'Hop size of window\')\n  parser.add_argument(\n      \'--output_type\',\n      type=int,\n      default=1,\n      help=\'1 for power spectrum, 2 for log-power spectrum.\')\n  parser.add_argument(\n      \'--window_type\',\n      type=str,\n      default=\'povey\',\n      help=\'Type of window (""hamm""|""hann""|""povey""|""rect""|""blac""|""tria"").\')\n  parser.add_argument(\n      \'--snip_edges\',\n      type=bool,\n      default=True,\n      help=\'The last frame (shorter than window_length) will not be cutoff.\')\n  parser.add_argument(\n      \'--raw_energy\',\n      type=int,\n      default=1,\n      help=\'Compute frame energy before preemphasis and windowing.\')\n  parser.add_argument(\n      \'--preeph_coeff\',\n      type=float,\n      default=0.97,\n      help=\'Coefficient for use in frame-signal preemphasis.\')\n  parser.add_argument(\n      \'--remove_dc_offset\',\n      type=bool,\n      default=True,\n      help=\' Subtract mean from waveform on each frame\')\n  parser.add_argument(\n      \'--is_fbank\',\n      type=bool,\n      default=True,\n      help=\'Compute power spetrum without frame energy\')\n  parser.add_argument(\n      \'--thres_autoc\', type=float, default=0.3, help=\'Threshold of autoc\')\n  parser.add_argument(\n      \'--write_num_frames\',\n      type=str,\n      help=\'Specify wspecifer for utt2num_frames\')\n  parser.add_argument(\n      \'--compress\',\n      type=strtobool,\n      default=False,\n      help=\'Save data in compressed format\')\n  parser.add_argument(\n      \'--compression_method\',\n      type=int,\n      default=2,\n      help=\'Specify the method of compression\')\n  parser.add_argument(\n      \'--verbose\', \'-V\', default=0, type=int, help=\'Verbose option\')\n  parser.add_argument(\n      \'--segments\',\n      type=str,\n      help=\'segments-file format: each line is either\'\n      \'<segment-id> <recording-id> <start-time> <end-time>\'\n      \'e.g. call-861225-A-0050-0065 call-861225-A 5.0 6.5\')\n  parser.add_argument(\'rspecifier\', type=str, help=\'WAV scp file\')\n  parser.add_argument(\'wspecifier\', type=str, help=\'Writer specifier\')\n  return parser\n\n\ndef compute_fbank_pitch():\n  parser = get_parser()\n  args = parser.parse_args()\n\n  config = {}\n  config[\'sample_rate\'] = int(args.sample_rate)\n  config[\'upper_frequency_limit\'] = float(args.upper_frequency_limit)\n  config[\'lower_frequency_limit\'] = float(args.lower_frequency_limit)\n  config[\'filterbank_channel_count\'] = float(args.filterbank_channel_count)\n  config[\'window_length\'] = args.window_length\n  config[\'frame_length\'] = args.frame_length\n  config[\'output_type\'] = int(args.output_type)\n  config[\'window_type\'] = args.window_type\n  config[\'snip_edges\'] = args.snip_edges\n  config[\'preeph_coeff\'] = args.preeph_coeff\n  config[\'remove_dc_offset\'] = args.remove_dc_offset\n  config[\'is_fbank\'] = args.is_fbank\n  config[\'thres_autoc\'] = args.thres_autoc\n  config[\'dither\'] = args.dither\n\n  fbank_pitch = FbankPitch.params(config).instantiate()\n\n  with kaldiio.ReadHelper(args.rspecifier,\n                          segments=args.segments) as reader, \\\n        KaldiWriter(args.wspecifier, write_num_frames=args.write_num_frames,\n                    compress=args.compress, compression_method=args.compression_method) as writer:\n    for utt_id, (sample_rate, array) in reader:\n      if sample_rate != args.sample_rate:\n        args.sample_rate = sample_rate\n      array = array.astype(np.float32)\n      audio_data = tf.constant(array, dtype=tf.float32)\n      fbank_pitch_test = fbank_pitch(audio_data, args.sample_rate)\n      sess = tf.Session()\n      fbank_pitch_feats = fbank_pitch_test.eval(session=sess)\n      writer[utt_id] = fbank_pitch_feats\n\n\nif __name__ == ""__main__"":\n  compute_fbank_pitch()\n'"
utils/speech/compute_mfcc_feats.py,3,"b'#!/usr/bin/env python3\n\n# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Create MFCC feature files.""""""\n\nimport delta.compat as tf\nimport argparse\nfrom distutils.util import strtobool\nimport kaldiio\nimport numpy as np\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""\nfrom delta.data.frontend.mfcc import Mfcc\nfrom espnet.utils.cli_writers import KaldiWriter\n\n\ndef get_parser():\n  parser = argparse.ArgumentParser(\n      description=\'Compute MFCC features from wav.\',\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument(\n      \'--sample_rate\', type=int, default=16000, help=\'Sampling frequency\')\n  parser.add_argument(\n      \'--upper_frequency_limit\',\n      type=float,\n      default=0,\n      help=\'Maxinum frequency\')\n  parser.add_argument(\n      \'--lower_frequency_limit\',\n      type=float,\n      default=20,\n      help=\'Minimum frequency\')\n  parser.add_argument(\n      \'--filterbank_channel_count\',\n      type=float,\n      default=23,\n      help=\'Order of fbank\')\n  parser.add_argument(\n      \'--window_length\', type=float, default=0.025, help=\'Length of a frame\')\n  parser.add_argument(\n      \'--frame_length\', type=float, default=0.010, help=\'Hop size of window\')\n  parser.add_argument(\n      \'--output_type\',\n      type=int,\n      default=1,\n      help=\'1 for power spectrum, 2 for log-power spectrum.\')\n  parser.add_argument(\n      \'--window_type\',\n      type=str,\n      default=\'povey\',\n      help=\'Type of window (""hamm""|""hann""|""povey""|""rect""|""blac""|""tria"").\')\n  parser.add_argument(\n      \'--snip_edges\',\n      type=bool,\n      default=True,\n      help=\'The last frame (shorter than window_length) will not be cutoff.\')\n  parser.add_argument(\n      \'--raw_energy\',\n      type=int,\n      default=1,\n      help=\'Compute frame energy before preemphasis and windowing.\')\n  parser.add_argument(\n      \'--preeph_coeff\',\n      type=float,\n      default=0.97,\n      help=\'Coefficient for use in frame-signal preemphasis.\')\n  parser.add_argument(\n      \'--remove_dc_offset\',\n      type=bool,\n      default=True,\n      help=\' Subtract mean from waveform on each frame.\')\n  parser.add_argument(\n      \'--is_fbank\',\n      type=bool,\n      default=True,\n      help=\'Compute power spetrum without frame energy.\')\n  parser.add_argument(\n      \'--dither\',\n      type=float,\n      default=0.0,\n      help=\'Dithering constant (0.0 means no dither).\')\n  parser.add_argument(\n      \'--cepstral_lifter\',\n      type=float,\n      default=22,\n      help=\'Constant that controls scaling of MFCCs.\')\n  parser.add_argument(\n      \'--coefficient_count\',\n      type=int,\n      default=13,\n      help=\'Number of cepstra in MFCC computation.\')\n  parser.add_argument(\n      \'--use_energy\',\n      type=bool,\n      default=True,\n      help=\'Use energy (not C0) in MFCC computation.\')\n  parser.add_argument(\n      \'--write_num_frames\',\n      type=str,\n      help=\'Specify wspecifer for utt2num_frames\')\n  parser.add_argument(\n      \'--compress\',\n      type=strtobool,\n      default=False,\n      help=\'Save data in compressed format\')\n  parser.add_argument(\n      \'--compression_method\',\n      type=int,\n      default=2,\n      help=\'Specify the method of compression\')\n  parser.add_argument(\n      \'--verbose\', \'-V\', default=0, type=int, help=\'Verbose option\')\n  parser.add_argument(\n      \'--segments\',\n      type=str,\n      help=\'segments-file format: each line is either\'\n      \'<segment-id> <recording-id> <start-time> <end-time>\'\n      \'e.g. call-861225-A-0050-0065 call-861225-A 5.0 6.5\')\n  parser.add_argument(\'rspecifier\', type=str, help=\'WAV scp file\')\n  parser.add_argument(\'wspecifier\', type=str, help=\'Writer specifier\')\n  return parser\n\n\ndef compute_mfcc():\n  parser = get_parser()\n  args = parser.parse_args()\n\n  config = {}\n  config[\'sample_rate\'] = int(args.sample_rate)\n  config[\'upper_frequency_limit\'] = float(args.upper_frequency_limit)\n  config[\'lower_frequency_limit\'] = float(args.lower_frequency_limit)\n  config[\'filterbank_channel_count\'] = float(args.filterbank_channel_count)\n  config[\'window_length\'] = args.window_length\n  config[\'frame_length\'] = args.frame_length\n  config[\'output_type\'] = args.output_type\n  config[\'window_type\'] = args.window_type\n  config[\'snip_edges\'] = args.snip_edges\n  config[\'preeph_coeff\'] = args.preeph_coeff\n  config[\'remove_dc_offset\'] = args.remove_dc_offset\n  config[\'is_fbank\'] = args.is_fbank\n  config[\'cepstral_lifter\'] = args.cepstral_lifter\n  config[\'coefficient_count\'] = args.coefficient_count\n  config[\'use_energy\'] = args.use_energy\n  config[\'dither\'] = args.dither\n\n  mfcc = Mfcc.params(config).instantiate()\n\n  with kaldiio.ReadHelper(args.rspecifier,\n                          segments=args.segments) as reader, \\\n        KaldiWriter(args.wspecifier, write_num_frames=args.write_num_frames,\n                    compress=args.compress, compression_method=args.compression_method) as writer:\n    for utt_id, (sample_rate, array) in reader:\n      if sample_rate != args.sample_rate:\n        args.sample_rate = sample_rate\n      array = array.astype(np.float32)\n      audio_data = tf.constant(array, dtype=tf.float32)\n      mfcc_test = tf.squeeze(mfcc(audio_data, args.sample_rate))\n      sess = tf.Session()\n      mfcc_feats = mfcc_test.eval(session=sess)\n      writer[utt_id] = mfcc_feats\n\n\nif __name__ == ""__main__"":\n  compute_mfcc()\n'"
utils/speech/compute_pitch_feats.py,3,"b'#!/usr/bin/env python3\n\n# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Create Pitch feature files.""""""\n\nimport delta.compat as tf\nimport argparse\nfrom distutils.util import strtobool\nimport kaldiio\nimport numpy as np\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""\nfrom delta.data.frontend.pitch import Pitch\nfrom espnet.utils.cli_writers import KaldiWriter\n\n\ndef get_parser():\n  parser = argparse.ArgumentParser(\n      description=\'Compute pitch features from wav.\',\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument(\n      \'--sample_rate\', type=int, default=16000, help=\'Sampling frequency\')\n  parser.add_argument(\n      \'--window_length\', type=float, default=0.025, help=\'Length of a frame\')\n  parser.add_argument(\n      \'--frame_length\', type=float, default=0.010, help=\'Hop size of window\')\n  parser.add_argument(\n      \'--snip_edges\',\n      type=bool,\n      default=True,\n      help=\'The last frame (shorter than window_length) will not be cutoff.\')\n  parser.add_argument(\n      \'--preemph_coeff\',\n      type=float,\n      default=0.0,\n      help=\'Coefficient for use in frame-signal preemphasis.\')\n  parser.add_argument(\n      \'--min_f0\', type=float, default=50, help=\'F0 to search for (Hz).\')\n  parser.add_argument(\n      \'--max_f0\', type=float, default=400, help=\'F0 to search for (Hz).\')\n  parser.add_argument(\n      \'--soft_min_f0\',\n      type=float,\n      default=10.0,\n      help=\'Minimum f0, applied in soft way, must not exceed min-f0.\')\n  parser.add_argument(\n      \'--penalty_factor\',\n      type=float,\n      default=0.1,\n      help=\'cost factor for FO change.\')\n  parser.add_argument(\n      \'--lowpass_cutoff\',\n      type=float,\n      default=1000,\n      help=\'cutoff frequency for LowPass filter (Hz).\')\n  parser.add_argument(\n      \'--resample_freq\',\n      type=float,\n      default=4000.0,\n      help=\'Frequency that we down-sample the signal to.  Must be more than twice lowpass-cutoff.\'\n  )\n  parser.add_argument(\n      \'--delta_pitch\',\n      type=float,\n      default=0.005,\n      help=\'Smallest relative change in pitch that our algorithm measures.\')\n  parser.add_argument(\n      \'--nccf_ballast\',\n      type=float,\n      default=7000.0,\n      help=\'Increasing this factor reduces NCCF for quiet frames.\')\n  parser.add_argument(\n      \'--lowpass_filter_width\',\n      type=int,\n      default=1,\n      help=\'Integer that determines filter width of lowpass filter, more gives sharper filter.\'\n  )\n  parser.add_argument(\n      \'--upsample_filter_width\',\n      type=int,\n      default=5,\n      help=\'Integer that determines filter width when upsampling NCCF.\')\n  parser.add_argument(\n      \'--max_frames_latency\',\n      type=int,\n      default=0,\n      help=\'Maximum number of frames of latency that we allow pitch tracking to introduce into the feature processing.\'\n  )\n  parser.add_argument(\n      \'--frames_per_chunk\',\n      type=int,\n      default=0,\n      help=\'Only relevant for offline pitch extraction.\')\n  parser.add_argument(\n      \'--recompute_frame\',\n      type=int,\n      default=500,\n      help=\'Only relevant for online pitch extraction, or for compatibility with online pitch extraction.\'\n  )\n  parser.add_argument(\n      \'--simulate_first_pass_online\',\n      type=bool,\n      default=False,\n      help=\'If true, compute-kaldi-pitch-feats will output features that correspond to what an \'\n      \'online decoder would see in the first pass of decoding.\')\n  parser.add_argument(\n      \'--nccf_ballast_online\',\n      type=bool,\n      default=False,\n      help=\'This is useful mainly for debug; it affects how the NCCF ballast is computed.\'\n  )\n  parser.add_argument(\n      \'--write_num_frames\',\n      type=str,\n      help=\'Specify wspecifer for utt2num_frames\')\n  parser.add_argument(\n      \'--compress\',\n      type=strtobool,\n      default=False,\n      help=\'Save data in compressed format\')\n  parser.add_argument(\n      \'--compression_method\',\n      type=int,\n      default=2,\n      help=\'Specify the method of compression\')\n  parser.add_argument(\n      \'--verbose\', \'-V\', default=0, type=int, help=\'Verbose option\')\n  parser.add_argument(\n      \'--segments\',\n      type=str,\n      help=\'segments-file format: each line is either\'\n      \'<segment-id> <recording-id> <start-time> <end-time>\'\n      \'e.g. call-861225-A-0050-0065 call-861225-A 5.0 6.5\')\n  parser.add_argument(\'rspecifier\', type=str, help=\'WAV scp file\')\n  parser.add_argument(\'wspecifier\', type=str, help=\'Writer specifier\')\n  return parser\n\n\ndef compute_pitch():\n  parser = get_parser()\n  args = parser.parse_args()\n\n  config = {}\n  config[\'sample_rate\'] = int(args.sample_rate)\n  config[\'window_length\'] = args.window_length\n  config[\'frame_length\'] = args.frame_length\n  config[\'snip_edges\'] = args.snip_edges\n  config[\'preemph_coeff\'] = args.preemph_coeff\n  config[\'min_f0\'] = args.min_f0\n  config[\'max_f0\'] = args.max_f0\n  config[\'soft_min_f0\'] = args.soft_min_f0\n  config[\'penalty_factor\'] = args.penalty_factor\n  config[\'lowpass_cutoff\'] = args.lowpass_cutoff\n  config[\'resample_freq\'] = args.resample_freq\n  config[\'delta_pitch\'] = args.delta_pitch\n  config[\'nccf_ballast\'] = args.nccf_ballast\n  config[\'lowpass_filter_width\'] = args.lowpass_filter_width\n  config[\'upsample_filter_width\'] = args.upsample_filter_width\n  config[\'max_frames_latency\'] = args.max_frames_latency\n  config[\'frames_per_chunk\'] = args.frames_per_chunk\n  config[\'simulate_first_pass_online\'] = args.simulate_first_pass_online\n  config[\'recompute_frame\'] = args.recompute_frame\n  config[\'nccf_ballast_online\'] = args.nccf_ballast_online\n\n  pitch = Pitch.params(config).instantiate()\n\n  with kaldiio.ReadHelper(args.rspecifier,\n                          segments=args.segments) as reader, \\\n        KaldiWriter(args.wspecifier, write_num_frames=args.write_num_frames,\n                    compress=args.compress, compression_method=args.compression_method) as writer:\n    for utt_id, (sample_rate, array) in reader:\n      if sample_rate != args.sample_rate:\n        args.sample_rate = sample_rate\n      array = array.astype(np.float32)\n      audio_data = tf.constant(array, dtype=tf.float32)\n      pitch_test = tf.squeeze(pitch(audio_data, args.sample_rate))\n      sess = tf.Session()\n      pitch_feats = pitch_test.eval(session=sess)\n      writer[utt_id] = pitch_feats\n\n\nif __name__ == ""__main__"":\n  compute_pitch()\n'"
utils/speech/compute_plp_feats.py,2,"b'#!/usr/bin/env python3\n\n# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport delta.compat as tf\nimport argparse\nfrom distutils.util import strtobool\nimport kaldiio\nimport numpy as np\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""\nfrom delta.data.frontend.plp import Plp\nfrom espnet.utils.cli_writers import KaldiWriter\n\n\ndef get_parser():\n  parser = argparse.ArgumentParser(\n      description=\'Compute plp features from wav.\',\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument(\n      \'--sample_rate\', type=int, default=16000, help=\'Sampling frequency\')\n  parser.add_argument(\'--plp_order\', type=int, default=12, help=\'Order of plp\')\n  parser.add_argument(\n      \'--window_length\', type=float, default=0.025, help=\'Length of a frame\')\n  parser.add_argument(\n      \'--frame_length\', type=float, default=0.010, help=\'Hop size of window\')\n  parser.add_argument(\n      \'--write_num_frames\',\n      type=str,\n      help=\'Specify wspecifer for utt2num_frames\')\n  parser.add_argument(\n      \'--compress\',\n      type=strtobool,\n      default=False,\n      help=\'Save data in compressed format\')\n  parser.add_argument(\n      \'--compression_method\',\n      type=int,\n      default=2,\n      help=\'Specify the method of compression\')\n  parser.add_argument(\n      \'--verbose\', \'-V\', default=0, type=int, help=\'Verbose option\')\n  parser.add_argument(\n      \'--segments\',\n      type=str,\n      help=\'segments-file format: each line is either\'\n      \'<segment-id> <recording-id> <start-time> <end-time>\'\n      \'e.g. call-861225-A-0050-0065 call-861225-A 5.0 6.5\')\n  parser.add_argument(\'rspecifier\', type=str, help=\'WAV scp file\')\n  parser.add_argument(\'wspecifier\', type=str, help=\'Writer specifier\')\n  return parser\n\n\ndef compute_plp():\n  parser = get_parser()\n  args = parser.parse_args()\n\n  config = {}\n  config[\'sample_rate\'] = int(args.sample_rate)\n  config[\'plp_order\'] = int(args.plp_order)\n  config[\'window_length\'] = args.window_length\n  config[\'frame_length\'] = args.frame_length\n\n  plp = Plp.params(config).instantiate()\n\n  with kaldiio.ReadHelper(args.rspecifier,\n                          segments=args.segments) as reader, \\\n        KaldiWriter(args.wspecifier, write_num_frames=args.write_num_frames,\n                    compress=args.compress, compression_method=args.compression_method) as writer:\n    for utt_id, (sample_rate, array) in reader:\n      if sample_rate != args.sample_rate:\n        args.sample_rate = sample_rate\n      array = array.astype(np.float32)\n      audio_data = tf.constant(array, dtype=tf.float32)\n      plp_test = plp(audio_data, args.sample_rate)\n      sess = tf.Session()\n      plp_feats = plp_test.eval(session=sess)\n      writer[utt_id] = plp_feats\n\n\nif __name__ == ""__main__"":\n  compute_plp()\n'"
utils/speech/compute_spectrum_feats.py,2,"b'#!/usr/bin/env python3\n\n# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""""Create spectrogram feature files.""""""\n\nimport delta.compat as tf\nimport argparse\nfrom distutils.util import strtobool\nimport kaldiio\nimport numpy as np\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""\nfrom delta.data.frontend.spectrum import Spectrum\nfrom espnet.utils.cli_writers import KaldiWriter\n\n\ndef get_parser():\n  parser = argparse.ArgumentParser(\n      description=\'Compute spectrum features from wav.\',\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument(\n      \'--sample_rate\', type=int, default=16000, help=\'Sampling frequency\')\n  parser.add_argument(\n      \'--window_length\', type=float, default=0.025, help=\'Length of a frame\')\n  parser.add_argument(\n      \'--frame_length\', type=float, default=0.010, help=\'Hop size of window\')\n  parser.add_argument(\n      \'--output_type\',\n      type=int,\n      default=2,\n      help=\'1 for power spectrum, 2 for log-power spectrum.\')\n  parser.add_argument(\n      \'--window_type\',\n      type=str,\n      default=\'povey\',\n      help=\'Type of window (""hamm""|""hann""|""povey""|""rect""|""blac""|""tria"").\')\n  parser.add_argument(\n      \'--snip_edges\',\n      type=bool,\n      default=True,\n      help=\'The last frame (shorter than window_length) will not be cutoff.\')\n  parser.add_argument(\n      \'--raw_energy\',\n      type=int,\n      default=1,\n      help=\'Compute frame energy before preemphasis and windowing.\')\n  parser.add_argument(\n      \'--preeph_coeff\',\n      type=float,\n      default=0.97,\n      help=\'Coefficient for use in frame-signal preemphasis.\')\n  parser.add_argument(\n      \'--remove_dc_offset\',\n      type=bool,\n      default=True,\n      help=\' Subtract mean from waveform on each frame\')\n  parser.add_argument(\n      \'--is_fbank\',\n      type=bool,\n      default=False,\n      help=\'Compute power spetrum without frame energy\')\n  parser.add_argument(\n      \'--dither\',\n      type=float,\n      default=0.0,\n      help=\'Dithering constant (0.0 means no dither).\')\n  parser.add_argument(\n      \'--write_num_frames\',\n      type=str,\n      help=\'Specify wspecifer for utt2num_frames\')\n  parser.add_argument(\n      \'--compress\',\n      type=strtobool,\n      default=False,\n      help=\'Save data in compressed format\')\n  parser.add_argument(\n      \'--compression_method\',\n      type=int,\n      default=2,\n      help=\'Specify the method of compression\')\n  parser.add_argument(\n      \'--verbose\', \'-V\', default=0, type=int, help=\'Verbose option\')\n  parser.add_argument(\n      \'--segments\',\n      type=str,\n      help=\'segments-file format: each line is either\'\n      \'<segment-id> <recording-id> <start-time> <end-time>\'\n      \'e.g. call-861225-A-0050-0065 call-861225-A 5.0 6.5\')\n  parser.add_argument(\'rspecifier\', type=str, help=\'WAV scp file\')\n  parser.add_argument(\'wspecifier\', type=str, help=\'Writer specifier\')\n  return parser\n\n\ndef compute_spectrum():\n  parser = get_parser()\n  args = parser.parse_args()\n\n  config = {}\n  config[\'sample_rate\'] = int(args.sample_rate)\n  config[\'output_type\'] = int(args.output_type)\n  config[\'window_length\'] = args.window_length\n  config[\'frame_length\'] = args.frame_length\n  config[\'output_type\'] = args.output_type\n  config[\'window_type\'] = args.window_type\n  config[\'snip_edges\'] = args.snip_edges\n  config[\'preeph_coeff\'] = args.preeph_coeff\n  config[\'remove_dc_offset\'] = args.remove_dc_offset\n  config[\'is_fbank\'] = args.is_fbank\n  config[\'dither\'] = args.dither\n\n  spectrum = Spectrum.params(config).instantiate()\n\n  with kaldiio.ReadHelper(args.rspecifier,\n                          segments=args.segments) as reader, \\\n        KaldiWriter(args.wspecifier, write_num_frames=args.write_num_frames,\n                    compress=args.compress, compression_method=args.compression_method) as writer:\n    for utt_id, (sample_rate, array) in reader:\n      if sample_rate != args.sample_rate:\n        args.sample_rate = sample_rate\n      array = array.astype(np.float32)\n      audio_data = tf.constant(array, dtype=tf.float32)\n      spectrum_test = spectrum(audio_data, args.sample_rate)\n      sess = tf.Session()\n      spectrum_feats = spectrum_test.eval(session=sess)\n      writer[utt_id] = spectrum_feats\n\n\nif __name__ == ""__main__"":\n  compute_spectrum()\n'"
utils/speech/compute_stft_feats.py,2,"b'#!/usr/bin/env python3\n\n# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport delta.compat as tf\nimport argparse\nfrom distutils.util import strtobool\nimport kaldiio\nimport numpy as np\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""\nfrom delta.data.frontend.analyfiltbank import Analyfiltbank\nfrom espnet.utils.cli_writers import KaldiWriter\n\n\ndef get_parser():\n  parser = argparse.ArgumentParser(\n      description=\'Compute power specturm or phase specturm features from wav.\',\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n  parser.add_argument(\n      \'--sample_rate\', type=int, default=16000, help=\'Sampling frequency\')\n  parser.add_argument(\n      \'--window_length\', type=float, default=0.030, help=\'Length of a frame\')\n  parser.add_argument(\n      \'--frame_length\', type=float, default=0.010, help=\'Hop size of window\')\n  parser.add_argument(\n      \'--output_type\',\n      type=int,\n      default=1,\n      help=\'1 for power spectrum, 2 for phase spectrum.\')\n  parser.add_argument(\n      \'--write_num_frames\',\n      type=str,\n      help=\'Specify wspecifer for utt2num_frames\')\n  parser.add_argument(\n      \'--compress\',\n      type=strtobool,\n      default=False,\n      help=\'Save data in compressed format\')\n  parser.add_argument(\n      \'--compression_method\',\n      type=int,\n      default=2,\n      help=\'Specify the method of compression\')\n  parser.add_argument(\n      \'--verbose\', \'-V\', default=0, type=int, help=\'Verbose option\')\n  parser.add_argument(\n      \'--segments\',\n      type=str,\n      help=\'segments-file format: each line is either\'\n      \'<segment-id> <recording-id> <start-time> <end-time>\'\n      \'e.g. call-861225-A-0050-0065 call-861225-A 5.0 6.5\')\n  parser.add_argument(\'rspecifier\', type=str, help=\'WAV scp file\')\n  parser.add_argument(\'wspecifier\', type=str, help=\'Writer specifier\')\n  return parser\n\n\ndef compute_stft():\n  parser = get_parser()\n  args = parser.parse_args()\n\n  config = {}\n  config[\'sample_rate\'] = int(args.sample_rate)\n  config[\'window_length\'] = args.window_length\n  config[\'frame_length\'] = args.frame_length\n\n  stft = Analyfiltbank.params(config).instantiate()\n\n  with kaldiio.ReadHelper(args.rspecifier,\n                          segments=args.segments) as reader, \\\n        KaldiWriter(args.wspecifier, write_num_frames=args.write_num_frames,\n                    compress=args.compress, compression_method=args.compression_method) as writer:\n    for utt_id, (sample_rate, array) in reader:\n      if sample_rate != args.sample_rate:\n        args.sample_rate = sample_rate\n      array = array.astype(np.float32)\n      audio_data = tf.constant(array, dtype=tf.float32)\n      power_spectrum, phase_spectrum = stft(audio_data, args.sample_rate)\n      sess = tf.Session()\n      if args.output_type == 1:\n        out_feats = power_spectrum.eval(session=sess)\n      else:\n        out_feats = phase_spectrum.eval(session=sess)\n      writer[utt_id] = out_feats\n\n\nif __name__ == ""__main__"":\n  compute_stft()\n'"
utils/speech/copy_feats.py,0,"b'#!/usr/bin/env python3\n\n# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport argparse\nfrom distutils.util import strtobool\nfrom espnet.utils.cli_writers import file_writer_helper\nfrom espnet.utils.cli_readers import KaldiReader\nimport kaldiio\n\n\ndef get_parser():\n  parser = argparse.ArgumentParser(\n      description=\'copy feature with preprocessing\',\n      formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n  parser.add_argument(\n      \'--verbose\', \'-V\', default=0, type=int, help=\'Verbose option\')\n  parser.add_argument(\n      \'--write_num_frames\',\n      type=str,\n      help=\'Specify wspecifer for utt2num_frames\')\n  parser.add_argument(\n      \'--compress\',\n      type=strtobool,\n      default=False,\n      help=\'Save in compressed format\')\n  parser.add_argument(\n      \'--compression_method\',\n      type=int,\n      default=2,\n      help=\'Specify the method(if mat) or gzip-level(if hdf5)\')\n  parser.add_argument(\n      \'rspecifier\',\n      type=str,\n      help=\'Read specifier for feats. e.g. ark:some.ark\')\n  parser.add_argument(\n      \'wspecifier\', type=str, help=\'Write specifier. e.g. ark:some.ark\')\n  return parser\n\n\ndef main():\n  parser = get_parser()\n  args = parser.parse_args()\n\n  d = kaldiio.load_ark(args.rspecifier)\n\n  with file_writer_helper(\n      args.wspecifier,\n      filetype=\'mat\',\n      write_num_frames=args.write_num_frames,\n      compress=args.compress,\n      compression_method=args.compression_method) as writer:\n    for utt, mat in d:\n      writer[utt] = mat\n\n\nif __name__ == ""__main__"":\n  main()\n'"
utils/speech/ctc_token_fst.py,0,"b""#!/usr/bin/env python3\n\n# Apache 2.0\n\nimport sys\n\nfread = open(sys.argv[1], 'r')\n\nprint('0 1 <eps> <eps>')\nprint('1 1 <blk> <eps>')\nprint('2 2 <blk> <eps>')\nprint('2 0 <eps> <eps>')\n\nnodeX = 3\nfor entry in fread.readlines():\n  entry = entry.replace('\\n', '').strip()\n  fields = entry.split(' ')\n  phone = fields[0]\n  if phone == '<eps>' or phone == '<blk>':\n    continue\n\n  if '#' in phone:\n    print(str(0) + ' ' + str(0) + ' ' + '<eps>' + ' ' + phone)\n  else:\n    print(str(1) + ' ' + str(nodeX) + ' ' + phone + ' ' + phone)\n    print(str(nodeX) + ' ' + str(nodeX) + ' ' + phone + ' <eps>')\n    print(str(nodeX) + ' ' + str(2) + ' ' + '<eps> <eps>')\n  nodeX += 1\nprint('0')\n\nfread.close()\n"""
utils/speech/prep_ctc_trans.py,0,"b'#!/usr/bin/env python3\n\n# Copyright 2015       Yajie Miao    (Carnegie Mellon University)\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n# MERCHANTABLITY OR NON-INFRINGEMENT.\n# See the Apache 2 License for the specific language governing permissions and\n# limitations under the License.\n\n# This python script converts the word-based transcripts into label sequences. The labels are\n# represented by their indices.\n\nimport sys\n\nif __name__ == \'__main__\':\n\n  if len(sys.argv) < 4 or len(sys.argv) > 5:\n    print(\n        ""Usage: {0} <lexicon_file> <trans_file> <unk_word> [space_word]"".format(\n            sys.argv[0]))\n    print(\n        ""e.g., utils/prep_ctc_trans.py data/lang/lexicon_numbers.txt data/train/text <UNK>""\n    )\n    print(\n        ""<lexicon_file> - the lexicon file in which entries have been represented by indices""\n    )\n    print(""<trans_file>   - the word-based transcript file"")\n    print(""<unk_word>     - the word which represents OOVs in transcripts"")\n    print(\n        ""[space_word]   - optional, the word representing spaces in the transcripts""\n    )\n    exit(1)\n\n  dict_file = sys.argv[1]\n  trans_file = sys.argv[2]\n  unk_word = sys.argv[3]\n\n  is_char = False\n  if len(sys.argv) == 5:\n    is_char = True\n    space_word = sys.argv[4]\n\n  # read the lexicon into a dictionary data structure\n  fread = open(dict_file, \'r\')\n  dict = {}\n  for line in fread.readlines():\n    line = line.replace(\'\\n\', \'\')\n    splits = line.split(\' \')  # assume there are no multiple spaces\n    word = splits[0]\n    letters = \'\'\n    for n in range(1, len(splits)):\n      letters += splits[n] + \' \'\n    dict[word] = letters.strip()\n  fread.close()\n\n  # assume that each line is formatted as ""uttid word1 word2 word3 ..."", with no multiple spaces appearing\n  fread = open(trans_file, \'r\')\n  for line in fread.readlines():\n    out_line = \'\'\n    line = line.replace(\'\\n\', \'\').strip()\n    while \'  \' in line:\n      line = line.replace(\'  \',\n                          \' \')  # remove multiple spaces in the transcripts\n\n    uttid = line.split(\' \')[0]  # the first field is always utterance id\n    trans = line.replace(uttid, \'\').strip()\n    if is_char:\n      trans = trans.replace(\' \', \' \' + space_word + \' \')\n    splits = trans.split(\' \')\n\n    out_line += uttid + \' \'\n    for n in range(0, len(splits)):\n      try:\n        out_line += dict[splits[n]] + \' \'\n      except Exception:\n        out_line += dict[unk_word] + \' \'\n    print(out_line.strip())\n'"
core/ops/kernels/__init__.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
core/ops/kernels/jieba_op_test.py,11,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' jieba op test \'\'\'\nimport os\nimport time\nimport tempfile\nimport delta.compat as tf\nfrom absl import logging\n\nfrom delta.data.utils import read_lines_from_text_file\nfrom delta.layers.ops import py_x_ops\n\n# pylint: disable=not-context-manager, invalid-name\n\n\ndef test_one(sess, ops, inputs):\n  \'\'\' elapse time of op \'\'\'\n  t1 = time.time()\n  sentence_out = sess.run(ops, inputs)\n  t2 = time.time()\n  logging.info(""inputs: {}"".format(inputs))\n  logging.info(""time cost: {}"".format(t2 - t1))\n  # logging.info(""\\n"".join([one_sen.decode(""utf-8"") for one_sen in sentence_out]))\n  return sentence_out\n\n\nclass JiebaOpsTest(tf.test.TestCase):\n  \'\'\' jieba op test\'\'\'\n\n  #pylint: disable=no-self-use\n  def build_op_use_file(self, sentence):\n    \'\'\' build graph \'\'\'\n\n    words = py_x_ops.jieba_cut(sentence, use_file=True, hmm=True)\n    return words\n\n  def build_op_no_file(self, sentence):\n    \'\'\' build graph \'\'\'\n    words = py_x_ops.jieba_cut(sentence, use_file=False, hmm=True)\n    return words\n\n  def test_jieba_cut_op_use_file(self):\n    \'\'\' test jieba \'\'\'\n    graph = tf.Graph()\n    with graph.as_default():\n      sentence_in = tf.placeholder(\n          dtype=tf.string, shape=[None], name=""sentence_in"")\n\n      sentence_out = self.build_op_use_file(sentence_in)\n\n      with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n        # self.assertShapeEqual(tf.shape(sentence_in), tf.shape(sentence_out))\n        sentence_out_res = test_one(sess, sentence_out,\n                                    {sentence_in: [""\xe6\x88\x91\xe7\x88\xb1\xe5\x8c\x97\xe4\xba\xac\xe5\xa4\xa9\xe5\xae\x89\xe9\x97\xa8""]})\n        self.assertEqual(""\xe6\x88\x91 \xe7\x88\xb1 \xe5\x8c\x97\xe4\xba\xac \xe5\xa4\xa9\xe5\xae\x89\xe9\x97\xa8"", sentence_out_res[0].decode(""utf-8""))\n        sentence_out_res = test_one(sess, sentence_out,\n                                    {sentence_in: [""\xe5\x90\x89\xe6\x9e\x97\xe7\x9c\x81\xe9\x95\xbf\xe6\x98\xa5\xe8\x8d\xaf\xe5\xba\x97""]})\n        self.assertEqual(""\xe5\x90\x89\xe6\x9e\x97\xe7\x9c\x81 \xe9\x95\xbf\xe6\x98\xa5 \xe8\x8d\xaf\xe5\xba\x97"", sentence_out_res[0].decode(""utf-8""))\n        sentence_out_res = test_one(sess, sentence_out,\n                                    {sentence_in: [""\xe5\x90\x89\xe6\x9e\x97\xe7\x9c\x81\xe9\x95\xbf\xe6\x98\xa5\xe8\x8d\xaf\xe5\xba\x97"", ""\xe5\x8d\x97\xe4\xba\xac\xe5\xb8\x82\xe9\x95\xbf\xe6\xb1\x9f\xe5\xa4\xa7\xe6\xa1\xa5""]})\n        self.assertEqual(\n            ""\xe5\x90\x89\xe6\x9e\x97\xe7\x9c\x81 \xe9\x95\xbf\xe6\x98\xa5 \xe8\x8d\xaf\xe5\xba\x97\\n\xe5\x8d\x97\xe4\xba\xac\xe5\xb8\x82 \xe9\x95\xbf\xe6\xb1\x9f\xe5\xa4\xa7\xe6\xa1\xa5"",\n            ""\\n"".join([one_sen.decode(""utf-8"") for one_sen in sentence_out_res\n                      ]))\n\n  def test_jieba_cut_op_no_file(self):\n    \'\'\' test jieba \'\'\'\n    graph = tf.Graph()\n    with graph.as_default():\n      sentence_in = tf.placeholder(\n          dtype=tf.string, shape=[None], name=""sentence_in"")\n\n      sentence_out = self.build_op_no_file(sentence_in)\n      shape_op = tf.shape(sentence_out)\n\n      with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n        # self.assertShapeEqual(tf.shape(sentence_in), tf.shape(sentence_out))\n        sentence_out_res = test_one(sess, sentence_out,\n                                    {sentence_in: [""\xe6\x88\x91\xe7\x88\xb1\xe5\x8c\x97\xe4\xba\xac\xe5\xa4\xa9\xe5\xae\x89\xe9\x97\xa8""]})\n        self.assertEqual(""\xe6\x88\x91 \xe7\x88\xb1 \xe5\x8c\x97\xe4\xba\xac \xe5\xa4\xa9\xe5\xae\x89\xe9\x97\xa8"", sentence_out_res[0].decode(""utf-8""))\n        sentence_out_res = test_one(sess, sentence_out,\n                                    {sentence_in: [""\xe5\x90\x89\xe6\x9e\x97\xe7\x9c\x81\xe9\x95\xbf\xe6\x98\xa5\xe8\x8d\xaf\xe5\xba\x97""]})\n        self.assertEqual(""\xe5\x90\x89\xe6\x9e\x97\xe7\x9c\x81 \xe9\x95\xbf\xe6\x98\xa5 \xe8\x8d\xaf\xe5\xba\x97"", sentence_out_res[0].decode(""utf-8""))\n        sentence_out_res, shape_res = test_one(\n            sess, [sentence_out, shape_op],\n            {sentence_in: [""\xe5\x90\x89\xe6\x9e\x97\xe7\x9c\x81\xe9\x95\xbf\xe6\x98\xa5\xe8\x8d\xaf\xe5\xba\x97"", ""\xe5\x8d\x97\xe4\xba\xac\xe5\xb8\x82\xe9\x95\xbf\xe6\xb1\x9f\xe5\xa4\xa7\xe6\xa1\xa5""]})\n        self.assertEqual(\n            ""\xe5\x90\x89\xe6\x9e\x97\xe7\x9c\x81 \xe9\x95\xbf\xe6\x98\xa5 \xe8\x8d\xaf\xe5\xba\x97\\n\xe5\x8d\x97\xe4\xba\xac\xe5\xb8\x82 \xe9\x95\xbf\xe6\xb1\x9f\xe5\xa4\xa7\xe6\xa1\xa5"",\n            ""\\n"".join([one_sen.decode(""utf-8"") for one_sen in sentence_out_res\n                      ]))\n        logging.info(f""shape: {shape_res}"")\n        self.assertAllEqual(shape_res, [2])\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  tf.test.main()\n'"
core/ops/kernels/ngram_op_test.py,11,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' ngram op unittest\'\'\'\nimport delta.compat as tf\nfrom absl import logging\n\nfrom delta.layers.ops import py_x_ops\n\n\nclass NGramOpsTest(tf.test.TestCase):\n  \'\'\' ngram op test\'\'\'\n\n  def setUp(self):\n    super().setUp()\n    self.testcase = [[0, 0, 0, 0], [223, 0, 0, 0], [0, 8, 0, 0], [4, 8, 0, 0],\n                     [0, 0, 10, 0], [2, 5, 3, 0], [7, 2, 1, 24]]\n\n  def test_ngram_op_2_order(self):\n    \'\'\' test ngram 2-order op\'\'\'\n    ground_truth_2 = [0, 0, 0, 0, 0, 0, 0]\n\n    word_ngram = 2\n    t_input = tf.placeholder(shape=(4,), dtype=tf.int32)\n    t_ngram = py_x_ops.ngram(\n        t_input, word_ngrams=word_ngram, vocab_size=5000, bucket_size=100000)\n    logging.info(""t_ngram: {}"".format(t_ngram))\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n      ngram_result = sess.run(t_ngram, feed_dict={t_input: self.testcase[0]})\n      self.assertAllEqual(ngram_result, ground_truth_2)\n\n  def test_batch_ngram_op_2_order(self):\n    \'\'\' tset batch 2-order ngram \'\'\'\n    ground_truth_2 = [[0, 0, 0, 0, 0, 0, 0], [223, 0, 0, 0, 0, 0, 0],\n                      [0, 8, 5008, 0, 0, 0, 0], [4, 8, 102492, 0, 0, 0, 0],\n                      [0, 0, 10, 5000, 5010, 0, 0],\n                      [2, 5, 3, 103747, 51858, 0, 0],\n                      [7, 2, 1, 24, 50599, 103743, 54395]]\n\n    word_ngram = 2\n    t_input = tf.placeholder(shape=(7, 4), dtype=tf.int32)\n    t_ngram = py_x_ops.ngram(\n        t_input, word_ngrams=word_ngram, vocab_size=5000, bucket_size=100000)\n    logging.info(""batch t_ngram: {}"".format(t_ngram))\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n      ngram_result = sess.run(t_ngram, feed_dict={t_input: self.testcase})\n      ngram_result = [list(res) for res in ngram_result]\n      self.assertAllEqual(ngram_result, ground_truth_2)\n\n  def test_batch_ngram_op_3_order(self):\n    \'\'\' test batch 3-order ngram \'\'\'\n\n    ground_truth_3 = [[0, 0, 0, 0, 0, 0, 0, 0, 0],\n                      [223, 0, 0, 0, 0, 0, 0, 0, 0],\n                      [0, 8, 5008, 0, 0, 0, 0, 0, 0],\n                      [4, 8, 102492, 0, 0, 0, 0, 0, 0],\n                      [0, 0, 10, 5000, 5010, 5010, 0, 0, 0],\n                      [2, 5, 3, 103747, 43140, 51858, 0, 0, 0],\n                      [7, 2, 1, 24, 50599, 73230, 103743, 45677, 54395]]\n\n    word_ngram = 3\n    t_input = tf.placeholder(shape=(7, 4), dtype=tf.int32)\n    t_ngram = py_x_ops.ngram(\n        t_input, word_ngrams=word_ngram, vocab_size=5000, bucket_size=100000)\n    logging.info(""batch t_ngram: {}"".format(t_ngram))\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n      ngram_result = sess.run(t_ngram, feed_dict={t_input: self.testcase})\n      ngram_result = [list(res) for res in ngram_result]\n      self.assertAllEqual(ngram_result, ground_truth_3)\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  tf.test.main()\n'"
core/ops/kernels/simple_vocab_op_test.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for simple_vocab.""""""\nimport delta.compat as tf\nfrom delta.layers.ops import py_x_ops\n\n\nclass VocabOpsTest(tf.test.TestCase):\n  \'\'\' vocab op test \'\'\'\n\n  def setUp(self):\n    super().setUp()\n\n  def tearDown(self):\n    \'\'\' tear down \'\'\'\n\n  def test_vocab_token_to_id(self):\n    \'\'\' tset vocab token to id\'\'\'\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      vocab = [\n          \'<s>\',\n          \'</s>\',\n          \'<unk>\',\n          \'<epsilon>\',\n          \'a\',\n          \'b c d e\',\n          \'\xc3\xb8ut\',\n          \'\xc3\xbcber\',\n          \'\xe2\x99\xa3\',\n          \'\xe6\x84\xa4\xe9\x9d\x92\',\n          \'\xe2\x86\x90\',\n      ]\n      self.assertEqual(0, py_x_ops.vocab_token_to_id(\'<s>\', vocab=vocab).eval())\n      self.assertEqual(4, py_x_ops.vocab_token_to_id(\'a\', vocab=vocab).eval())\n      self.assertAllEqual([5, 8],\n                          py_x_ops.vocab_token_to_id([\'b c d e\', \'\xe2\x99\xa3\'],\n                                                     vocab=vocab).eval())\n      self.assertEqual(\n          2,\n          py_x_ops.vocab_token_to_id(\'unknown\', vocab=vocab).eval())\n\n  def test_vocab_token_to_load_id(self):\n    \'\'\' test vocab token to id which is loaded from vocab file\'\'\'\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      vocab = [\n          \'<s>\t3\',\n          \'</s>\t5\',\n          \'<unk>\t7\',\n          \'<epsilon>\t9\',\n          \'a\t2\',\n          \'b c d e\t4\',\n          \'\xc3\xb8ut\t8\',\n          \'\xc3\xbcber\t10\',\n          \'\xe2\x99\xa3\t-1\',\n          \'\xe6\x84\xa4\xe9\x9d\x92\t-3\',\n          \'\xe2\x86\x90\t-5\',\n      ]\n      self.assertEqual(\n          3,\n          py_x_ops.vocab_token_to_id(\n              \'<s>\', vocab=vocab, load_token_ids_from_vocab=True).eval())\n      self.assertEqual(\n          2,\n          py_x_ops.vocab_token_to_id(\n              \'a\', vocab=vocab, load_token_ids_from_vocab=True).eval())\n      self.assertAllEqual([4, -1],\n                          py_x_ops.vocab_token_to_id(\n                              [\'b c d e\', \'\xe2\x99\xa3\'],\n                              vocab=vocab,\n                              load_token_ids_from_vocab=True).eval())\n      self.assertEqual(\n          7,\n          py_x_ops.vocab_token_to_id(\n              \'unknown\', vocab=vocab, load_token_ids_from_vocab=True).eval())\n\n  def test_vocab_id_to_token(self):\n    \'\'\' test vocab id to token \'\'\'\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      vocab = [\n          \'<s>\',\n          \'</s>\',\n          \'<unk>\',\n          \'<epsilon>\',\n          \'a\',\n          \'b c d e\',\n          \'\xc3\xb8ut\',\n          \'\xc3\xbcber\',\n          \'\xe2\x99\xa3\',\n          \'\xe6\x84\xa4\xe9\x9d\x92\',\n          \'\xe2\x86\x90\',\n      ]\n      self.assertEqual(\n          \'<s>\',\n          py_x_ops.vocab_id_to_token(0, vocab=vocab).eval().decode(\'utf-8\'))\n      self.assertEqual(\n          \'a\',\n          py_x_ops.vocab_id_to_token(4, vocab=vocab).eval().decode(\'utf-8\'))\n\n      res = py_x_ops.vocab_id_to_token([5, 8], vocab=vocab).eval()\n      res = [r.decode(\'utf-8\') for r in res]\n      self.assertAllEqual([\'b c d e\', \'\xe2\x99\xa3\'], res)\n      self.assertEqual(\n          \'<unk>\',\n          py_x_ops.vocab_id_to_token(2, vocab=vocab).eval().decode(\'utf-8\'))\n      self.assertEqual(\n          \'<unk>\',\n          py_x_ops.vocab_id_to_token(-1, vocab=vocab).eval().decode(\'utf-8\'))\n      self.assertEqual(\n          \'<unk>\',\n          py_x_ops.vocab_id_to_token(11, vocab=vocab).eval().decode(\'utf-8\'))\n\n  def test_vocab_id_to_token_load_id(self):\n    \'\'\' test vocab id to token which is loaded from vocabfile\'\'\'\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      vocab = [\n          \'<s>\t3\',\n          \'</s>\t5\',\n          \'<unk>\t7\',\n          \'<epsilon>\t9\',\n          \'a\t2\',\n          \'b c d e\t4\',\n          \'\xc3\xb8ut\t8\',\n          \'\xc3\xbcber\t10\',\n          \'\xe2\x99\xa3\t-1\',\n          \'\xe6\x84\xa4\xe9\x9d\x92\t-3\',\n          \'\xe2\x86\x90\t-5\',\n      ]\n      self.assertEqual(\n          \'<s>\',\n          py_x_ops.vocab_id_to_token(\n              3, vocab=vocab,\n              load_token_ids_from_vocab=True).eval().decode(\'utf-8\'))\n      self.assertEqual(\n          \'a\',\n          py_x_ops.vocab_id_to_token(\n              2, vocab=vocab,\n              load_token_ids_from_vocab=True).eval().decode(\'utf-8\'))\n      res = py_x_ops.vocab_id_to_token([4, -1],\n                                       vocab=vocab,\n                                       load_token_ids_from_vocab=True).eval()\n      res = [r.decode(\'utf-8\') for r in res]\n\n      self.assertAllEqual([\'b c d e\', \'\xe2\x99\xa3\'], res)\n      self.assertEqual(\n          \'<unk>\',\n          py_x_ops.vocab_id_to_token(\n              7, vocab=vocab,\n              load_token_ids_from_vocab=True).eval().decode(\'utf-8\'))\n      self.assertEqual(\n          \'<unk>\',\n          py_x_ops.vocab_id_to_token(\n              0, vocab=vocab,\n              load_token_ids_from_vocab=True).eval().decode(\'utf-8\'))\n\n  def test_token_in_vocab(self):\n    \'\'\'test token whether in vocab \'\'\'\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      vocab = [\n          \'<s>\',\n          \'</s>\',\n          \'<unk>\',\n          \'<epsilon>\',\n          \'a\',\n          \'b c d e\',\n          \'\xc3\xb8ut\',\n          \'\xc3\xbcber\',\n          \'\xe2\x99\xa3\',\n          \'\xe6\x84\xa4\xe9\x9d\x92\',\n          \'\xe2\x86\x90\',\n      ]\n      self.assertTrue(py_x_ops.token_in_vocab(\'a\', vocab=vocab).eval())\n      self.assertTrue(py_x_ops.token_in_vocab(\'<unk>\', vocab=vocab).eval())\n      self.assertTrue(\n          py_x_ops.token_in_vocab([\'b c d e\', \'\xe2\x99\xa3\'], vocab=vocab).eval().all())\n      self.assertFalse(py_x_ops.token_in_vocab(\'unknown\', vocab=vocab).eval())\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
core/ops/kernels/string_utils_test.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' string utils op unittest\'\'\'\nimport delta.compat as tf\n\nfrom delta.layers.ops import py_x_ops\n\n\nclass StringUtilsOpTest(tf.test.TestCase):\n  \'\'\' string utils test\'\'\'\n\n  def setUp(self):\n    super().setUp()\n\n  def tearDown(self):\n    \'\'\' tear down \'\'\'\n\n  def test_lower(self):\n    \'\'\' test lower string\'\'\'\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      output = py_x_ops.str_lower(""Hello WORLD"").eval()\n      self.assertEqual(b\'hello world\', output)\n      output = py_x_ops.str_lower([""Hello WORLD"", ""ABC XYZ""]).eval()\n      self.assertAllEqual([b\'hello world\', b\'abc xyz\'], output)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
core/ops/kernels/tokenizer_ops_test.py,4,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for tokenizer_ops.""""""\nimport time\nimport tempfile\nimport delta.compat as tf\nfrom absl import logging\n\nfrom delta.layers.ops import py_x_ops\n\n\nclass TokenizerOpsTest(tf.test.TestCase):\n  \'\'\' tokenizer op test\'\'\'\n\n  def setUp(self):\n    super().setUp()\n    self.vocab = [\n        \'</s>\',\n        \'<unk>\',\n        \'hello\',\n        \'\xe4\xbd\xa0\xe5\xa5\xbd\',\n        \'world\',\n    ]\n    self.vocab_filepath = tempfile.mktemp(suffix=\'vocab.txt\')\n    with open(self.vocab_filepath, mode=\'w\', encoding=\'utf-8\') as fobj:\n      for token in self.vocab:\n        fobj.write(token)\n        fobj.write(\'\\n\')\n\n  def test_text_to_tokenid_with_vocab_file(self):\n    \'\'\' test label to token id\'\'\'\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      # test batch\n      start = time.time()\n      batch_op = py_x_ops.sentence_to_ids(\n          [\'hello world\', \'\xe4\xbd\xa0\xe5\xa5\xbd hello unknown  world\'],\n          maxlen=10,\n          use_vocab_file=True,\n          vocab_filepath=self.vocab_filepath,\n          load_token_ids_from_vocab=False,\n          pad_id=-1)\n      token_ids, paddings = sess.run(batch_op)\n      elapsed = time.time() - start\n      logging.info(""Time cost: {:.4f}s"".format(elapsed))\n      logging.info(token_ids)\n      logging.info(paddings)\n      logging.info(""batch_op: {}"".format(batch_op))\n      self.assertAllEqual(token_ids, [[2, 4, -1, -1, -1, -1, -1, -1, -1, -1],\n                                      [3, 2, 1, 4, -1, -1, -1, -1, -1, -1]])\n      self.assertAllEqual(\n          paddings,\n          [[0, 0, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]])\n\n      # test single\n      single_op = py_x_ops.sentence_to_ids(\n          \'\xe4\xbd\xa0\xe5\xa5\xbd hello unknown  world\',\n          maxlen=10,\n          vocab_filepath=self.vocab_filepath,\n          use_vocab_file=True,\n          load_token_ids_from_vocab=False,\n          pad_id=-1)\n      token_ids, paddings = sess.run(single_op)\n      logging.info(""single_op: {}"".format(single_op))\n      self.assertAllEqual(token_ids, [3, 2, 1, 4, -1, -1, -1, -1, -1, -1])\n\n      # test short single\n      short_single_op = py_x_ops.sentence_to_ids(\n          \'\xe4\xbd\xa0\xe5\xa5\xbd hello unknown  world\',\n          maxlen=2,\n          use_vocab_file=True,\n          vocab_filepath=self.vocab_filepath,\n          load_token_ids_from_vocab=False,\n          pad_id=0)\n      token_ids, paddings = sess.run(short_single_op)\n      logging.info(""short_op: {}"".format(short_single_op))\n      self.assertAllEqual(token_ids, [3, 2])\n\n      # test short batch\n      short_batch_op = py_x_ops.sentence_to_ids(\n          [\'hello world\', \'\xe4\xbd\xa0\xe5\xa5\xbd hello unknown  world\'],\n          maxlen=2,\n          use_vocab_file=True,\n          vocab_filepath=self.vocab_filepath,\n          load_token_ids_from_vocab=False,\n          pad_id=0)\n      token_ids, paddings = sess.run(short_batch_op)\n      logging.info(""short_op: {}"".format(short_batch_op))\n      self.assertAllEqual(token_ids, [[2, 4], [3, 2]])\n\n  def test_text_to_tokenid(self):\n    \'\'\' test label to token id\'\'\'\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      # test batch\n      start = time.time()\n      batch_op, batch_padding_op = py_x_ops.sentence_to_ids(\n          [\'hello world\', \'\xe4\xbd\xa0\xe5\xa5\xbd hello unknown  world\'],\n          maxlen=10,\n          use_vocab_file=False,\n          vocab=self.vocab,\n          load_token_ids_from_vocab=False,\n          pad_id=-1)\n      batch_shape_op = tf.shape(batch_op)\n      shape_res, token_ids, paddings = sess.run(\n          [batch_shape_op, batch_op, batch_padding_op])\n      elapsed = time.time() - start\n      logging.info(""Time cost: {:.4f}s"".format(elapsed))\n      logging.info(token_ids)\n      logging.info(paddings)\n      logging.info(""batch_op: {}"".format(batch_op))\n      logging.info(f""batch_shape: {shape_res}"")\n      self.assertAllEqual(shape_res, [2, 10])\n      self.assertAllEqual(token_ids, [[2, 4, -1, -1, -1, -1, -1, -1, -1, -1],\n                                      [3, 2, 1, 4, -1, -1, -1, -1, -1, -1]])\n      self.assertAllEqual(\n          paddings,\n          [[0, 0, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]])\n\n      # test single\n      single_op, single_padding_op = py_x_ops.sentence_to_ids(\n          \'\xe4\xbd\xa0\xe5\xa5\xbd hello unknown  world\',\n          maxlen=10,\n          vocab=self.vocab,\n          use_vocab_file=False,\n          load_token_ids_from_vocab=False,\n          pad_id=-1)\n      single_shape_op = tf.shape(single_op)\n      single_shape_res, token_ids, paddings = sess.run(\n          [single_shape_op, single_op, single_padding_op])\n      logging.info(""single_op: {}"".format(single_op))\n      logging.info(f""single_shape: {single_shape_res}"")\n      self.assertAllEqual(single_shape_res, [10])\n      self.assertAllEqual(token_ids, [3, 2, 1, 4, -1, -1, -1, -1, -1, -1])\n\n      # test short single\n      short_single_op = py_x_ops.sentence_to_ids(\n          \'\xe4\xbd\xa0\xe5\xa5\xbd hello unknown  world\',\n          maxlen=2,\n          use_vocab_file=False,\n          vocab=self.vocab,\n          load_token_ids_from_vocab=False,\n          pad_id=0)\n      token_ids, paddings = sess.run(short_single_op)\n      logging.info(""short_op: {}"".format(short_single_op))\n      self.assertAllEqual(token_ids, [3, 2])\n\n      # test short batch\n      short_batch_op = py_x_ops.sentence_to_ids(\n          [\'hello world\', \'\xe4\xbd\xa0\xe5\xa5\xbd hello unknown  world\'],\n          maxlen=2,\n          use_vocab_file=False,\n          vocab=self.vocab,\n          load_token_ids_from_vocab=False,\n          pad_id=0)\n      token_ids, paddings = sess.run(short_batch_op)\n      logging.info(""short_op: {}"".format(short_batch_op))\n      self.assertAllEqual(token_ids, [[2, 4], [3, 2]])\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  tf.test.main()\n'"
delta/data/datasets/__init__.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
delta/data/datasets/atis.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\n## References\n\nCharles T. Hemphill, John J. Godfrey, and George R. Doddington. 1990.\nThe ATIS spoken language systems pilot corpus.\nIn Proceedings of the DARPA Speech and Natural Language Workshop.\nhttp://www.aclweb.org/anthology/ H90-1021.\n\n## Download Links\n\nhttps://github.com/howl-anderson/ATIS_dataset/raw/master/data/raw_data/ms-cntk-atis\n\n## Description\n\nthe Air Travel Information System (ATIS) pilot corpus,\na corpus designed to measure progress in Spoken Language Systems\nthat include both a speech and natural language component.\nThis pilot marks the first full-scale attempt to collect such a corpus\nand provides guidelines for future efforts.\n\n\n## Data scale introduction\n\n- Training size\xef\xbc\x9a4,978\n- Development size\xef\xbc\x9a-\n- Test size\xef\xbc\x9a893\n- Intents\xef\xbc\x9a26\n- Slots\xef\xbc\x9a129\n\n""""""\n\nimport os\nimport traceback\nimport wget\nfrom absl import logging\nfrom delta.data.datasets.base_dataset import BaseDataSet\nfrom delta.data.datasets.utils import summary_joint_nlu_data\nfrom delta.utils.register import registers\n\n\n@registers.dataset.register(\'atis\')\nclass ATIS(BaseDataSet):\n  """"""atis data class for nlu joint task.""""""\n\n  def __init__(self, project_dir):\n    super().__init__(project_dir)\n    self.train_file = ""train.txt""\n    self.test_file = ""test.txt""\n    self.data_files = [self.train_file, self.test_file]\n    self.config_files = [\'atis_nlu_joint_lstm_crf.yml\']\n    self.download_files = [""atis.train.pkl"", ""atis.test.pkl""]\n\n  def download(self) -> bool:\n    train_url = ""https://github.com/howl-anderson/ATIS_dataset/raw/master/"" \\\n                ""data/raw_data/ms-cntk-atis/atis.train.pkl""\n    test_url = ""https://github.com/howl-anderson/ATIS_dataset/raw/master/"" \\\n               ""data/raw_data/ms-cntk-atis/atis.test.pkl""\n    try:\n      wget.download(train_url, self.download_dir)\n      wget.download(test_url, self.download_dir)\n    except Exception as e:\n      logging.warning(repr(e))\n      return False\n    return True\n\n  def after_download(self) -> bool:\n    try:\n      summary_joint_nlu_data(os.path.join(self.download_dir, ""atis.train.pkl""),\n                             os.path.join(self.data_dir, self.train_file))\n      summary_joint_nlu_data(os.path.join(self.download_dir, ""atis.test.pkl""),\n                             os.path.join(self.data_dir, self.test_file))\n    except Exception as e:\n\n      logging.warning(traceback.format_exc())\n      return False\n    return True\n'"
delta/data/datasets/atis2.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\nfrom absl import logging\nimport shutil\nimport traceback\nfrom delta.data.datasets.base_dataset import BaseDataSet\nfrom delta.utils.register import registers\n\n\n@registers.dataset.register(\'atis2\')\nclass ATIS2(BaseDataSet):\n  """"""atis2 data class for nlu joint task.""""""\n\n  def __init__(self, project_dir):\n    super().__init__(project_dir)\n    self.train_file = ""train.txt""\n    self.dev_file = ""dev.txt""\n    self.test_file = ""test.txt""\n    self.data_files = [self.train_file, self.dev_file, self.test_file]\n    self.train_download = ""origin_data/atis-2.train.w-intent.iob""\n    self.dev_download = ""origin_data/atis-2.dev.w-intent.iob""\n    self.test_download = ""origin_data/atis.test.w-intent.iob""\n    self.download_files = [self.train_download, self.dev_download, self.test_download]\n    self.config_files = [\'atis2_nlu_joint_lstm_crf.yml\']\n\n  @staticmethod\n  def to_standard_format(input_file, output_file):\n    """"""change data format for data input""""""\n    logging.info(""Save file to {}"".format(output_file))\n\n    with open(input_file, encoding=""utf-8"") as in_file, \\\n      open(output_file, ""w"", encoding=""utf-8"") as out_file:\n      for row in in_file:\n        parts = row.strip().split(""\\t"")\n        if len(parts) < 2:\n          continue\n        text = parts[0]\n        sub_parts = parts[1].split("" "")\n        intent_label = sub_parts[-1]\n        slots_label = "" "".join(sub_parts[:-1])\n\n        text = text.rstrip(""EOS"")\n        text = text.strip()\n\n        out_file.write(intent_label + ""\\t""\n                       + slots_label + ""\\t""\n                       + text + ""\\n"")\n\n  def download(self) -> bool:\n      github_url = ""https://github.com/yvchen/JointSLU.git""\n      res = os.system(f\'cd {self.download_dir}; git clone {github_url}\')\n      if res != 0:\n        return False\n      return True\n\n  def after_download(self) -> bool:\n    try:\n      shutil.move(os.path.join(self.download_dir, ""JointSLU/data""),\n                  os.path.join(self.download_dir, ""origin_data""))\n      shutil.rmtree(os.path.join(self.download_dir, ""JointSLU""))\n      self.to_standard_format(os.path.join(self.download_dir, self.train_download),\n                              os.path.join(self.data_dir, self.train_file))\n      self.to_standard_format(os.path.join(self.download_dir, self.dev_download),\n                              os.path.join(self.data_dir, self.dev_file))\n      self.to_standard_format(os.path.join(self.download_dir, self.test_download),\n                              os.path.join(self.data_dir, self.test_file))\n    except Exception as e:\n      logging.warning(traceback.format_exc())\n      return False\n    return True\n'"
delta/data/datasets/atis2_test.py,2,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for compat.py.""""""\n\nfrom absl import logging\n\nimport delta.compat as tf\nfrom delta.data.datasets.atis2 import ATIS2\n\n\nclass ATIS2Test(tf.test.TestCase):\n  """"""data class test for nlu-joint task.""""""\n\n  def test_build(self):\n    atis2 = ATIS2(\'/atis2\')\n    atis2.build()\n    self.assertTrue(atis2.is_ready())\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.DEBUG)\n  tf.test.main()\n'"
delta/data/datasets/atis_test.py,2,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for compat.py.""""""\n\nfrom absl import logging\n\nimport delta.compat as tf\nfrom delta.data.datasets.atis import ATIS\n\n\nclass ATISTest(tf.test.TestCase):\n  """"""atis data class for nlu joint task.""""""\n\n  def test_build(self):\n    atis = ATIS(\'atis\')\n    atis.build()\n    self.assertTrue(atis.is_ready())\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.DEBUG)\n  tf.test.main()\n'"
delta/data/datasets/base_dataset.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Data set operation class""""""\n\nimport os\nfrom typing import List\nfrom absl import logging\n\nfrom delta import PACKAGE_ROOT_DIR\nfrom delta.utils.config import load_config\nfrom delta.utils.config import save_config\n\n\nclass BaseDataSet:\n  """"""Base Data set class.""""""\n\n  def __init__(self, project_dir: str):\n\n    self.project_dir: str = project_dir\n    # download files, must be under download_dir\n    self.download_files: List[str] = list()\n    # final generate files, must be under data_dir\n    self.data_files: List[str] = list()\n    # config files, must be under config_dir\n    self.config_files: List[str] = list()\n    self.origin_config_dir = os.path.join(PACKAGE_ROOT_DIR, ""configs"")\n\n  def download(self) -> bool:\n    """"""Download dataset from Internet.""""""\n    raise NotImplementedError\n\n  def after_download(self) -> bool:\n    """"""Dataset operations after download.""""""\n    raise NotImplementedError\n\n  def copy_config_files(self) -> None:\n    """"""Copy config files""""""\n    for config_file in self.config_files:\n      full_config_file = os.path.join(self.origin_config_dir, config_file)\n      new_config_file = os.path.join(self.config_dir, config_file)\n      config = load_config(full_config_file)\n      config[\'data\'][\'project_dir\'] = self.project_dir\n      logging.info(f""Save config from {full_config_file} to {new_config_file}"")\n      save_config(config, new_config_file)\n\n  @property\n  def data_dir(self) -> str:\n    """"""data directory""""""\n    return os.path.join(self.project_dir, ""data"")\n\n  @property\n  def download_dir(self) -> str:\n    """"""Download directory""""""\n    return os.path.join(self.project_dir, ""download"")\n\n  @property\n  def config_dir(self) -> str:\n    """"""Config directory""""""\n    return os.path.join(self.project_dir, ""config"")\n\n  def _download_ready(self) -> bool:\n    """"""If download is ready.""""""\n    for data_file in self.download_files:\n      full_data_file = os.path.join(self.data_dir, data_file)\n      if not os.path.exists(full_data_file):\n        logging.warning(f""Data: {full_data_file} do not exists!"")\n        return False\n    return True\n\n  def is_ready(self) -> bool:\n    """"""If the dataset is ready for using.""""""\n    if not os.path.exists(self.project_dir):\n      logging.warning(f""Directory: {self.project_dir} do not exists!"")\n      return False\n    for data_file in self.data_files:\n      full_data_file = os.path.join(self.data_dir, data_file)\n      if not os.path.exists(full_data_file):\n        logging.warning(f""Data file: {full_data_file} do not exists!"")\n        return False\n    for config_file in self.config_files:\n      full_config_file = os.path.join(self.config_dir, config_file)\n      if not os.path.exists(full_config_file):\n        logging.warning(f""Config file: {full_config_file} do not exists!"")\n        return False\n    return True\n\n  def build(self) -> bool:\n    """"""Build the dataset.""""""\n    if self.is_ready():\n      logging.info(""Dataset is ready."")\n      return True\n    logging.info(\'Dataset is not ready!\')\n    if not os.path.exists(self.project_dir):\n      os.mkdir(self.project_dir)\n    if not os.path.exists(self.data_dir):\n      os.mkdir(self.data_dir)\n    if not os.path.exists(self.config_dir):\n      os.mkdir(self.config_dir)\n    if not os.path.exists(self.download_dir):\n      os.mkdir(self.download_dir)\n\n    self.copy_config_files()\n    if not self._download_ready():\n      logging.info(""Download not ready!"")\n      logging.info(""Start downloading ..."")\n      download_res = self.download()\n      if not download_res:\n        logging.warning(""Download failed."")\n        return False\n    logging.info(""Start doing after download processing."")\n    after_res = self.after_download()\n    if not after_res:\n      logging.warning(""After download process failed."")\n      return False\n    logging.info(""Dataset is ready."")\n    return True\n'"
delta/data/datasets/build.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nfrom absl import logging\nfrom delta.utils.register import registers\n\n\ndef build_dataset(dataset_name, dataset_dir):\n  if dataset_name not in registers.dataset:\n    logging.warning(f""Dataset: {dataset_name} not supported!"")\n  ds_cls = registers.dataset[dataset_name]\n  ds_obj = ds_cls(dataset_dir)\n  res = ds_obj.build()\n  if not res:\n    logging.info(f""Dataset: {dataset_name} built failed!"")\n    return\n  logging.info(f""Dataset: {dataset_name} built successfully."")\n'"
delta/data/datasets/conll_2003.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\n## References\n\nhttps://www.aclweb.org/anthology/W03-0419\n\n## links for download data\n\nhttps://www.clips.uantwerpen.be/conll2003/ner/\n\n## Description\n\nThe CoNLL-2003 named entity data consists of eight files covering two languages:\nEnglish and German1.\nFor each of the languages there is a training file, a development file,\na test file and a large file with unannotated data.\nThe learning methods were trained with the training data.\nThe development data could be used for tuning the parameters of the learning methods\n\n## Data scale introduction\n\n| English DataSet |  Articles | Sentences | Tokens |\n|---|---|---|---|\n| Training set |  946 | 14,987 | 203,621\n| Development set | 216 | 3,466 | 51,362 |\n| Test set | 231 | 3,684 | 46,435 |\n\n\n| English DataSet |  LOC | MISC | ORG |PER|\n|---|---|---|---|---|\n| Training set |  7140 | 3438 | 6321 | 6600|\n| Development set |1837 | 922 | 1341 |1842|\n| Test set | 1668 |702| 1661|1617|\n\n The more details about Germanl Dataset is shown in paper.\n\n\n""""""\n\nimport os\nimport traceback\nimport wget\nfrom absl import logging\nfrom delta.data.datasets.base_dataset import BaseDataSet\nfrom delta.utils.register import registers\n\n\n@registers.dataset.register(\'conll_2003\')\nclass Conll2003(BaseDataSet):\n  """"""conll2003 data class for seqlabel task.""""""\n\n  def __init__(self, project_dir):\n    super().__init__(project_dir)\n    self.train_file = ""train.txt""\n    self.dev_file = ""dev.txt""\n    self.test_file = ""test.txt""\n    self.data_files = [self.train_file, self.test_file, self.dev_file]\n    self.config_files = [""conll_2003_seq_label_bert.yml"",\n                         ""conll_2003_seq_label_elmo.yml"",\n                         ""conll_2003_seq_label_lstm_crf.yml""]\n    self.download_files = [self.train_file, self.test_file, self.dev_file]\n\n  def download(self) -> bool:\n    train_url = ""https://raw.githubusercontent.com/kyzhouhzau/BERT-NER/master/data/train.txt""\n    dev_url = ""https://raw.githubusercontent.com/kyzhouhzau/BERT-NER/master/data/dev.txt""\n    test_url = ""https://raw.githubusercontent.com/kyzhouhzau/BERT-NER/master/data/test.txt""\n    try:\n      wget.download(train_url, self.download_dir)\n      wget.download(dev_url, self.download_dir)\n      wget.download(test_url, self.download_dir)\n    except Exception as e:\n      logging.warning(repr(e))\n      return False\n    return True\n\n  @staticmethod\n  def to_standard_format(input_file, output_file):\n\n      logging.info(""Change data format: {}"".format(input_file))\n      words, labels = [], []\n      with open(output_file, ""w"", encoding=""utf-8"") as output_file:\n        with open(input_file, ""r"", encoding=""utf-8"") as file_input:\n          for line in file_input.readlines():\n            word = line.strip().split(\' \')[0]\n            label = line.strip().split(\' \')[-1]\n            # here we dont do ""DOCSTART"" check\n            if len(line.strip()) == 0:\n              l = [label for label in labels if not label]\n              w = [word for word in words if not word]\n              assert len(l) == len(w)\n              l, w = \' \'.join(l), \' \'.join(w)\n              output_file.write(l + ""\\t"" + w + ""\\n"")\n              words, labels = [], []\n            words.append(word)\n            labels.append(label)\n      logging.info(""Change data done: {}"".format(output_file))\n\n  def after_download(self) -> bool:\n    try:\n      download_file = os.path.join(self.download_dir, ""yahoo_answers_csv.tgz"")\n      os.system(f""tar zxvf {download_file}  -C {self.download_dir}"")\n      self.to_standard_format(os.path.join(self.download_dir, self.train_file),\n                              os.path.join(self.data_dir, self.train_file))\n      self.to_standard_format(os.path.join(self.download_dir, self.dev_file),\n                              os.path.join(self.data_dir, self.dev_file))\n      self.to_standard_format(os.path.join(self.download_dir, self.test_file),\n                              os.path.join(self.data_dir, self.test_file))\n    except Exception as e:\n      logging.warning(traceback.format_exc())\n      return False\n    return True\n'"
delta/data/datasets/conll_2003_test.py,2,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for compat.py.""""""\n\nfrom absl import logging\n\nimport delta.compat as tf\nfrom delta.data.datasets.conll_2003 import Conll2003\n\n\nclass Conll2003Test(tf.test.TestCase):\n  """"""conll2003 data class for seqlabel task.""""""\n\n  def test_build(self):\n    conll_2003 = Conll2003(\'conll_2003\')\n    conll_2003.build()\n    self.assertTrue(conll_2003.is_ready())\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.DEBUG)\n  tf.test.main()\n'"
delta/data/datasets/mock_text_cls_data.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\n## Data scale introduction\n\n\n""""""\n\nimport os\nimport traceback\nfrom absl import logging\nfrom delta.data.datasets.base_dataset import BaseDataSet\nfrom delta.data.datasets.utils import mock_data\nfrom delta.utils.register import registers\n\n\n@registers.dataset.register(\'mock_text_cls_data\')\nclass MockTextCLSData(BaseDataSet):\n  """"""mock data class for cls task.""""""\n\n  def __init__(self, project_dir):\n    super().__init__(project_dir)\n\n    samples_english = [""1\\tAll is well"", ""0\\tI am very angry""]\n    samples_split_line_mark = [""1\\t\xe9\x83\xbd\xe6\x8c\xba\xe5\xa5\xbd\xe3\x80\x82|\xe9\x83\xbd\xe6\x98\xaf\xe7\x9a\x84\xe5\x91\x80"", ""0\\t\xe6\x88\x91\xe5\xbe\x88\xe6\x84\xa4\xe6\x80\x92|\xe8\xb6\x85\xe7\xba\xa7\xe7\x94\x9f\xe6\xb0\x94\xef\xbc\x81""]\n    samples_split_by_space = [""1\\t\xe9\x83\xbd \xe6\x8c\xba\xe5\xa5\xbd"", ""0\\t\xe6\x88\x91 \xe5\xbe\x88 \xe6\x84\xa4\xe6\x80\x92""]\n    samples_split_by_char = [""1\\t\xe9\x83\xbd\xe6\x8c\xba\xe5\xa5\xbd"", ""0\\t\xe6\x88\x91\xe5\xbe\x88\xe6\x84\xa4\xe6\x80\x92""]\n    samples_chinese_word = [""1\\t\xe9\x83\xbd\xe6\x8c\xba\xe5\xa5\xbd"", ""0\\t\xe6\x88\x91\xe5\xbe\x88\xe6\x84\xa4\xe6\x80\x92""]\n    self.samples_dict = {""english"": samples_english,\n                    ""split_by_line_mark"": samples_split_line_mark,\n                    ""split_by_space"": samples_split_by_space,\n                    ""split_by_char"": samples_split_by_char,\n                    ""chinese_word"": samples_chinese_word}\n\n    self.train_file = ""train.txt""\n    self.dev_file = ""dev.txt""\n    self.test_file = ""test.txt""\n    self.text_vocab = ""text_vocab.txt""\n    files = [self.train_file, self.dev_file, self.test_file]\n    self.data_files = [x.replace(""txt"", """")+data_type +"".txt""\n                       for x in files for data_type in self.samples_dict]\n    self.config_files = [\'cnn_cls_mock.yml\']\n    self.download_files = []\n\n    text_vocab_english = [""<unk>\\t0"", ""</s>\\t1"", ""all\\t3"", ""is\\t4"",\n                          ""well\\t5"", ""i\\t6"", ""am\\t7"", ""very\\t8""]\n    text_vocab_split_line_mark = [""<unk>\\t0"", ""</s>\\t1"", ""\xe9\x83\xbd\\t2"", ""\xe6\x8c\xba\xe5\xa5\xbd\\t3"",\n                                  ""\xe6\x88\x91\\t4"", ""\xe5\xbe\x88\\t5"", ""|\\t6"", ""\xe6\x98\xaf\xe7\x9a\x84\\t7"",\n                                  ""\xe5\x91\x80\\t8"", ""\xe8\xb6\x85\xe7\xba\xa7\\t9"", ""\xe7\x94\x9f\xe6\xb0\x94\\t10""]\n    text_vocab_split_by_space = [""<unk>\\t0"", ""</s>\\t1"", ""\xe9\x83\xbd\\t2"", ""\xe6\x8c\xba\xe5\xa5\xbd\\t3"",\n                                 ""\xe6\x88\x91\\t4"", ""\xe5\xbe\x88\\t5""]\n    text_vocab_split_by_char = [""<unk>\\t0"", ""</s>\\t1"", ""\xe9\x83\xbd\\t2"", ""\xe6\x8c\xba\\t3"",\n                                ""\xe5\xa5\xbd\\t4"", ""\xe6\x88\x91\\t5"", ""\xe5\xbe\x88\\t6"", ""\xe6\x84\xa4\\t7"", ""\xe6\x80\x92\\t8""]\n    text_vocab_chinese_word = [""<unk>\\t0"", ""</s>\\t1"", ""\xe9\x83\xbd\\t2"", ""\xe6\x8c\xba\xe5\xa5\xbd\\t3"",\n                               ""\xe6\x88\x91\\t4"", ""\xe5\xbe\x88\\t5""]\n    self.text_vocab_dict = {""english"": text_vocab_english,\n                       ""split_by_line_mark"": text_vocab_split_line_mark,\n                       ""split_by_space"": text_vocab_split_by_space,\n                       ""split_by_char"": text_vocab_split_by_char,\n                       ""chinese_word"": text_vocab_chinese_word}\n\n\n  def download(self) -> bool:\n    return True\n\n\n  def after_download(self) -> bool:\n    try:\n      for data_type in self.samples_dict:\n\n        samples = self.samples_dict[data_type]\n        text_vocab_list = self.text_vocab_dict[data_type]\n\n        train_file_path = os.path.join(self.data_dir,\n                                       self.train_file.replace(""txt"", """") + data_type + "".txt"")\n        dev_file_path = os.path.join(self.data_dir,\n                                     self.dev_file.replace(""txt"", """") + data_type + "".txt"")\n        test_file_path = os.path.join(self.data_dir,\n                                      self.test_file.replace(""txt"", """") + data_type + "".txt"")\n        text_vocab_file = os.path.join(self.data_dir,\n                                       self.text_vocab.replace(""txt"", """") + data_type + "".txt"")\n\n        mock_data(samples, train_file_path, dev_file_path, test_file_path, text_vocab_file, text_vocab_list)\n\n    except Exception as e:\n      logging.warning(traceback.format_exc())\n      return False\n    return True\n'"
delta/data/datasets/mock_text_cls_test.py,2,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for compat.py.""""""\n\nfrom absl import logging\n\nimport delta.compat as tf\nfrom delta.data.datasets.mock_text_cls_data import MockTextCLSData\n\n\n\nclass MockTextClsDataTest(tf.test.TestCase):\n  """"""mock cls data class for cls task.""""""\n\n  def test_build(self):\n    mock_text_cls_data = MockTextCLSData(\'mock_cls_data\')\n    mock_text_cls_data.build()\n    self.assertTrue(mock_text_cls_data.is_ready())\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.DEBUG)\n  tf.test.main()\n'"
delta/data/datasets/mock_text_match_data.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\n## Data scale introduction\n\n\n""""""\n\nimport os\nimport traceback\nfrom absl import logging\nfrom delta.data.datasets.base_dataset import BaseDataSet\nfrom delta.data.datasets.utils import mock_data\nfrom delta.utils.register import registers\n\n@registers.dataset.register(\'mock_text_match_data\')\nclass MockTextMatchData(BaseDataSet):\n  """"""mock match data class for match task.""""""\n\n  def __init__(self, project_dir):\n    super().__init__(project_dir)\n    self.train_file = ""train.txt""\n    self.dev_file = ""dev.txt""\n    self.test_file = ""test.txt""\n    self.data_files = [self.train_file, self.dev_file, self.test_file]\n    self.config_files = [\'rnn_match_mock.yml\']\n    self.download_files = []\n    self.text_vocab = ""text_vocab.txt""\n\n    # samples with label\n    self.samples = [""0\\tHow should I approach forgiveness?\\tI got chickenpox as a child."",\n               ""1\\tI love china\xe3\x80\x82\\tI love china very much\xe3\x80\x82""]\n    self.text_vocab_list = [""<unk>\\t0"", ""</s>\\t1"", ""how\\t2"", ""should\\t3"",\n                       ""i\\t4"", ""approach\\t5"", ""forgiveness\\t6"", ""got\\t7"",\n                       ""chickenpox\\t8"", ""as\\t9"", ""a\\t10"",\n                       ""child\\t11"", ""love\\t12"", ""china\\t13"",\n                       ""very\\t14"", ""much\\t15""]\n\n  def download(self) -> bool:\n    return True\n\n\n  def after_download(self) -> bool:\n    try:\n      train_file_path = os.path.join(self.data_dir, self.train_file)\n      dev_file_path = os.path.join(self.data_dir, self.dev_file)\n      test_file_path = os.path.join(self.data_dir, self.test_file)\n      text_vocab_file = os.path.join(self.data_dir, self.text_vocab)\n\n      mock_data(self.samples, train_file_path, dev_file_path, test_file_path,\n                text_vocab_file, self.text_vocab_list)\n\n    except Exception as e:\n      logging.warning(traceback.format_exc())\n      return False\n    return True\n'"
delta/data/datasets/mock_text_match_test.py,2,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for compat.py.""""""\n\nfrom absl import logging\n\nimport delta.compat as tf\nfrom delta.data.datasets.mock_text_match_data import MockTextMatchData\n\n\n\nclass MockTextMatchDataTest(tf.test.TestCase):\n  """"""mock data class test for match task.""""""\n\n  def test_build(self):\n    mock_text_match_data = MockTextMatchData(\'mock_match_data\')\n    mock_text_match_data.build()\n    self.assertTrue(mock_text_match_data.is_ready())\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.DEBUG)\n  tf.test.main()\n'"
delta/data/datasets/mock_text_nlu_joint_data.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\n## Data scale introduction\n\n\n""""""\n\nimport os\nimport traceback\nfrom absl import logging\nfrom delta.data.datasets.base_dataset import BaseDataSet\nfrom delta.data.datasets.utils import mock_data\nfrom delta.utils.register import registers\n\n\n@registers.dataset.register(\'mock_text_nlu_joint_data\')\nclass MockTextNLUJointData(BaseDataSet):\n  """"""mock nlu-joint data class for nlu-joint task.""""""\n\n  def __init__(self, project_dir):\n    super().__init__(project_dir)\n    self.train_file = ""train.txt""\n    self.dev_file = ""dev.txt""\n    self.test_file = ""test.txt""\n    self.data_files = [self.train_file, self.dev_file, self.test_file]\n    self.config_files = [\'nlu_joint_mock.yml\']\n    self.download_files = []\n    self.text_vocab = ""text_vocab.txt""\n\n    # samples with label\n    self.samples = [""0\\tO O O O\\tmy feeling is low"",\n               ""1\\tO O O O B-ORG\\ti am happy in the kfc""]\n\n    self.text_vocab_list = [""<unk>\\t0"", ""</s>\\t1"", ""i\\t2"", ""am\\t3"", ""kfc\\t4"", ""my\\t5"",\n                       ""feeling\\t6"", ""happy\\t7"", ""is\\t8"", ""low\\t9"", ""in\\t10"", ""the\\t11""]\n\n  def download(self) -> bool:\n    return True\n\n\n  def after_download(self) -> bool:\n    try:\n      train_file_path = os.path.join(self.data_dir, self.train_file)\n      dev_file_path = os.path.join(self.data_dir, self.dev_file)\n      test_file_path = os.path.join(self.data_dir, self.test_file)\n      text_vocab_file = os.path.join(self.data_dir, self.text_vocab)\n\n      mock_data(self.samples, train_file_path, dev_file_path,\n                test_file_path, text_vocab_file, self.text_vocab_list)\n\n    except Exception as e:\n      logging.warning(traceback.format_exc())\n      return False\n    return True\n'"
delta/data/datasets/mock_text_nlu_joint_test.py,2,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for compat.py.""""""\n\nfrom absl import logging\n\nimport delta.compat as tf\nfrom delta.data.datasets.mock_text_nlu_joint_data import MockTextNLUJointData\n\n\n\nclass MockTextNLUJointDataTest(tf.test.TestCase):\n  """"""mock data class test for nlu-joint task.""""""\n\n  def test_build(self):\n    mock_text_nlu_joint_data = MockTextNLUJointData(\'mock_nlu_joint_data\')\n    mock_text_nlu_joint_data.build()\n    self.assertTrue(mock_text_nlu_joint_data.is_ready())\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.DEBUG)\n  tf.test.main()\n'"
delta/data/datasets/mock_text_seq2seq_data.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\n## Data scale introduction\n\n\n""""""\n\nimport os\nimport traceback\nfrom absl import logging\nfrom delta.data.datasets.base_dataset import BaseDataSet\nfrom delta.data.datasets.utils import mock_data\nfrom delta.utils.register import registers\n\n\n@registers.dataset.register(\'mock_text_seq2seq_data\')\nclass MockTextSeq2SeqData(BaseDataSet):\n  """"""mock seq2seq data class seq2seq task.""""""\n\n  def __init__(self, project_dir):\n    super().__init__(project_dir)\n    self.train_file = ""train.txt""\n    self.dev_file = ""dev.txt""\n    self.test_file = ""test.txt""\n\n    files = [self.train_file, self.dev_file, self.test_file]\n    suffix = ["".tgt"", "".src""]\n    self.data_files = [file+suf for suf in suffix for file in files]\n\n    self.config_files = [\'transformer_s2s_mock.yml\']\n    self.download_files = []\n\n\n    # samples with label\n    self.samples = ["" a shooting at a bar popular with expatriates in mali on saturday killed ""\n               ""five people \\t killed five people"",\n               ""a pennsylvania community is pulling together to search for an eighth-grade ""\n               ""student who has been missing since wednesday\\tsearch for missing student""]\n\n  def download(self) -> bool:\n    return True\n\n\n  def after_download(self) -> bool:\n    try:\n      train_file_path = os.path.join(self.data_dir, self.train_file)\n      dev_file_path = os.path.join(self.data_dir, self.dev_file)\n      test_file_path = os.path.join(self.data_dir, self.test_file)\n\n      mock_data(self.samples, train_file_path, dev_file_path, test_file_path)\n\n    except Exception as e:\n      logging.warning(traceback.format_exc())\n      return False\n    return True\n'"
delta/data/datasets/mock_text_seq2seq_test.py,2,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for compat.py.""""""\n\nfrom absl import logging\n\nimport delta.compat as tf\nfrom delta.data.datasets.mock_text_seq2seq_data import MockTextSeq2SeqData\n\n\n\nclass MockTextSeq2SeqDataTest(tf.test.TestCase):\n  """"""data class test for seq2seq task.""""""\n\n  def test_build(self):\n    mock_text_seq2seq_data = MockTextSeq2SeqData(\'mock_seq2seq_data\')\n    mock_text_seq2seq_data.build()\n    self.assertTrue(mock_text_seq2seq_data.is_ready())\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.DEBUG)\n  tf.test.main()\n'"
delta/data/datasets/mock_text_seq_label_data.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\n## Data scale introduction\n\n\n""""""\n\nimport os\nimport traceback\nfrom absl import logging\nfrom delta.data.datasets.base_dataset import BaseDataSet\nfrom delta.data.datasets.utils import mock_data\nfrom delta.utils.register import registers\n\n\n@registers.dataset.register(\'mock_text_seq_label_data\')\nclass MockTextSeqLabelData(BaseDataSet):\n  """"""data class for mock seqlabel task.""""""\n\n  def __init__(self, project_dir):\n    super().__init__(project_dir)\n    self.train_file = ""train.txt""\n    self.dev_file = ""dev.txt""\n    self.test_file = ""test.txt""\n    self.data_files = [self.train_file, self.dev_file, self.test_file]\n    self.config_files = [\'seq_label_mock.yml\']\n    self.download_files = []\n    self.text_vocab = ""text_vocab.txt""\n    self.label_vocab = ""label_vocab.txt""\n\n    # samples with label\n    self.samples = [""O O O O\\ti feel good ."",\n               ""O O B-ORG O O O O O O\\tby stumps kent had reached 108 for three .""]\n    self.text_vocab_list = [""<unk>\\t0"", ""</s>\\t1"", ""i\\t2"", ""feel\\t3"", ""good\\t4"",\n                            "".\\t5"", ""by\\t6"", ""stumps\\t7"", ""kent\\t8"", ""had\\t9"",\n                            ""reached\\t10"", ""108\\t11"", ""for\\t12"", ""three\\t13""]\n    self.label_vocab_list = [""O\\t0"", ""B-PER\\t1"", ""I-PER\\t2"", ""B-LOC\\t3"", ""I-LOC\\t4"",\n                        ""B-ORG\\t5"", ""I-ORG\\t6"", ""B-MISC\\t7"", ""I-MISC\\t8""]\n\n\n  def download(self) -> bool:\n    return True\n\n\n  def after_download(self) -> bool:\n    try:\n      train_file_path = os.path.join(self.data_dir, self.train_file)\n      dev_file_path = os.path.join(self.data_dir, self.dev_file)\n      test_file_path = os.path.join(self.data_dir, self.test_file)\n\n      text_vocab_file = os.path.join(self.data_dir, self.text_vocab)\n      label_vocab_file = os.path.join(self.data_dir, self.label_vocab)\n\n      mock_data(self.samples, train_file_path, dev_file_path, test_file_path,\n                text_vocab_file, self.text_vocab_list, label_vocab_file, self.label_vocab_list)\n\n    except Exception as e:\n      logging.warning(traceback.format_exc())\n      return False\n    return True\n\n'"
delta/data/datasets/mock_text_seq_label_test.py,2,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for compat.py.""""""\n\nfrom absl import logging\n\nimport delta.compat as tf\nfrom delta.data.datasets.mock_text_seq_label_data import MockTextSeqLabelData\n\n\n\nclass MockTextSeqLabelDataTest(tf.test.TestCase):\n  """"""mock data class test for seqlabel task.""""""\n\n  def test_build(self):\n    mock_text_seq_label_data = MockTextSeqLabelData(\'mock_seq_label_data\')\n    mock_text_seq_label_data.build()\n    self.assertTrue(mock_text_seq_label_data.is_ready())\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.DEBUG)\n  tf.test.main()\n'"
delta/data/datasets/snli.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\n## References\n\nStanford Natural Language Inference (SNLI) corpus is released in A large annotated\ncorpus for learning natural language inference\n\nAvailable: https://sigann.github.io/LAW-XI-2017/papers/LAW01.pdf\n\n## Download Links\n\nhttps://nlp.stanford.edu/projects/snli/snli_1.0.zip\n\n## Description\n\nStanford Natural Language Inference corpus is a new, freely available collection of\nlabeled sentence pairs, written by humans doing a novel grounded task based on image captioning.\nAt 570K pairs, it is two orders of magnitude larger than all other resources of its type.\nThis in- crease in scale allows lexicalized classi- fiers to outperform some sophisticated\nexisting entailment models, and it allows a neural network-based model to perform competitively\non natural language infer- ence benchmarks for the first time.\n\n## Data scale introduction\n\n- Training pairs\xef\xbc\x9a550,152\n- Development pairs\xef\xbc\x9a10,000\n- Test pairs\xef\xbc\x9a10,000\n""""""\n\nimport wget\nimport os\nimport traceback\nimport json\nfrom absl import logging\nfrom delta.data.datasets.base_dataset import BaseDataSet\nfrom delta.utils.register import registers\n\n\n@registers.dataset.register(\'snli\')\nclass SNLI(BaseDataSet):\n  """"""snli data class test for match task.""""""\n\n  def __init__(self, project_dir):\n    super().__init__(project_dir)\n    self.train_file = ""train.txt""\n    self.dev_file = ""dev.txt""\n    self.test_file = ""test.txt""\n    self.data_files = [self.train_file, self.test_file, self.dev_file]\n    self.config_files = [""snli_match_rnn.yml""]\n    self.download_files = [""snli_1.0.zip""]\n\n  def download(self) -> bool:\n    url = ""https://nlp.stanford.edu/projects/snli/snli_1.0.zip""\n    try:\n      wget.download(url, self.download_dir)\n    except Exception as e:\n      logging.warning(repr(e))\n      return False\n    return True\n\n  @staticmethod\n  def to_standard_format(input_file, output_file):\n\n    label_dic = {""neutral"": ""2"", ""contradiction"": ""0"", ""entailment"": ""1""}\n    with open(input_file, encoding=""utf-8"") as json_file, \\\n      open(output_file, ""w"", encoding=""utf-8"") as out_file:\n      text_reader = json_file.readlines()\n      for line in text_reader:\n        line_dic = json.loads(line)\n        if ""gold_label"" not in line_dic or ""sentence1"" not in line_dic or ""sentence2"" not in line_dic:\n          continue\n        if line_dic[""gold_label""] == ""-"":\n          continue\n        label = label_dic[line_dic[""gold_label""]]\n        sentence1 = line_dic[""sentence1""]\n        sentence2 = line_dic[""sentence2""]\n        out_file.write(label + ""\\t"" + sentence1 + ""\\t"" + sentence2 + ""\\n"")\n\n  def after_download(self) -> bool:\n    try:\n      download_file = os.path.join(self.download_dir, ""snli_1.0.zip"")\n      os.system(f""unzip {download_file}  -d {self.download_dir}"")\n      self.to_standard_format(os.path.join(self.download_dir, ""snli_1.0/snli_1.0_train.jsonl""),\n                              os.path.join(self.data_dir, self.train_file))\n      self.to_standard_format(os.path.join(self.download_dir, ""snli_1.0/snli_1.0_dev.jsonl""),\n                              os.path.join(self.data_dir, self.dev_file))\n      self.to_standard_format(os.path.join(self.download_dir, ""snli_1.0/snli_1.0_test.jsonl""),\n                              os.path.join(self.data_dir, self.test_file))\n    except Exception as e:\n      logging.warning(traceback.format_exc())\n      return False\n    return True\n'"
delta/data/datasets/snli_test.py,2,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for compat.py.""""""\n\nfrom absl import logging\n\nimport delta.compat as tf\nfrom delta.data.datasets.snli import SNLI\n\n\nclass SNLITest(tf.test.TestCase):\n  """"""snli data class for match task.""""""\n\n  def test_build(self):\n    snli = SNLI(\'snli\')\n    snli.build()\n    self.assertTrue(snli.is_ready())\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.DEBUG)\n  tf.test.main()\n'"
delta/data/datasets/trec.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\n## References\n\nXin Li, Dan Roth, Learning Question Classifiers. COLING\'02, Aug., 2002.\n\n## Description\n\nThis data collection contains all the data used in our learning question classification\nexperiments(Xin Li, Dan Roth, Learning Question Classifiers. COLING\'02, Aug., 2002.),\nwhich has question class definitions, the training and testing question sets, examples\nof preprocessing the questions, feature definition scripts and examples of semantically\nrelated word features.\nThis work has been done by Xin Li and Dan Roth and supported by Research supported by\n(NSF grants IIS-9801638 and ITR IIS-0085836 and an ONR MURI Award.) .\n\n## Download Links\n\nhttps://github.com/thtrieu/qclass_dl/tree/master/data\n\n## Data scale introduction\n\n- Training size\xef\xbc\x9a5452\n- Development size\xef\xbc\x9a-\n- Test size\xef\xbc\x9a500\n\n""""""\n\nimport os\nimport traceback\nimport wget\nfrom absl import logging\nfrom delta.data.datasets.base_dataset import BaseDataSet\nfrom delta.utils.register import registers\n\n\n@registers.dataset.register(\'trec\')\nclass TREC(BaseDataSet):\n  """"""trec data class test for cls task.""""""\n\n  def __init__(self, project_dir):\n    super().__init__(project_dir)\n    self.train_file = ""train.txt""\n    self.test_file = ""test.txt""\n    self.data_files = [self.train_file, self.test_file]\n    self.config_files = [\'trec_text_cls_cnn.yml\']\n    self.download_files = [""train"", ""test""]\n\n  def download(self) -> bool:\n    train_url = ""https://raw.githubusercontent.com/thtrieu/qclass_dl/master/data/train""\n    test_url = ""https://raw.githubusercontent.com/thtrieu/qclass_dl/master/data/test""\n    try:\n      wget.download(train_url, self.download_dir)\n      wget.download(test_url, self.download_dir)\n    except Exception as e:\n      logging.warning(repr(e))\n      return False\n    return True\n\n  @staticmethod\n  def to_standard_format(input_file, output_file):\n    logging.info(""Save file to {}"".format(output_file))\n\n    max_seq = 0\n    with open(input_file, encoding=""ISO-8859-1"") as in_file, \\\n      open(output_file, ""w"", encoding=""utf-8"") as out_file:\n      for row in in_file.readlines():\n        parts = row.strip().split("" "")\n        label = parts[0].split("":"")[0]\n        text_len = len(parts[1:])\n        if text_len > max_seq:\n          max_seq = text_len\n        text = "" "".join(parts[1:])\n        out_file.write(label + ""\\t"" + text + ""\\n"")\n    logging.info(""max seq len is {}"".format(max_seq))\n\n  def after_download(self) -> bool:\n    try:\n      self.to_standard_format(os.path.join(self.download_dir, ""train""),\n                              os.path.join(self.data_dir, self.train_file))\n      self.to_standard_format(os.path.join(self.download_dir, ""test""),\n                              os.path.join(self.data_dir, self.test_file))\n    except Exception as e:\n      logging.warning(traceback.format_exc())\n      return False\n    return True\n'"
delta/data/datasets/trec_test.py,2,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for compat.py.""""""\n\nfrom absl import logging\n\nimport delta.compat as tf\nfrom delta.data.datasets.trec import TREC\n\n\nclass TRECTest(tf.test.TestCase):\n  """"""trec data class for cls task.""""""\n\n  def test_build(self):\n    trec = TREC(\'trec\')\n    trec.build()\n    self.assertTrue(trec.is_ready())\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.DEBUG)\n  tf.test.main()\n'"
delta/data/datasets/utils.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\nimport pickle\nimport numpy as np\nfrom pathlib import Path\nimport tempfile\nfrom absl import logging\nfrom sklearn.model_selection import train_test_split\n\n\ndef data_generator(data):\n  """"""Simple data generator""""""\n  while True:\n    for i, ele in enumerate(data):\n      yield data[i]\n\n\ndef scale_data(original_file, new_file, scale_rate):\n  """"""Scale data file.""""""\n  logging.info(""Scale file from {} to {}"".format(original_file, new_file))\n\n  with open(original_file, encoding=""utf-8"") as original_f, \\\n    open(new_file, ""w"", encoding=""utf-8"") as new_f:\n\n    original_lines = original_f.readlines()\n    original_size = len(original_lines)\n    new_size = int(original_size * scale_rate)\n\n    for i, line in enumerate(data_generator(original_lines)):\n      if i >= new_size:\n        break\n      new_f.write(line)\n\n\ndef split_train_dev(original_train, train_file, dev_file, split_rate):\n  """"""Split train and dev data set.""""""\n  with open(original_train, encoding=""utf-8"") as original_train_f, \\\n    open(train_file, ""w"", encoding=""utf-8"") as train_f, \\\n    open(dev_file, ""w"", encoding=""utf-8"") as dev_f:\n    lines = original_train_f.readlines()\n    lines_train, lines_dev = train_test_split(\n        lines, test_size=split_rate, random_state=2019)\n\n    logging.info(""Save train file to {}"".format(train_file))\n    for line in lines_train:\n      train_f.write(line)\n\n    logging.info(""Save train file to {}"".format(dev_file))\n    for line in lines_dev:\n      dev_f.write(line)\n\n\ndef summary_joint_nlu_data(fname, output_file_path):\n  with open(fname, \'rb\') as stream:\n    ds, dicts = pickle.load(stream)\n  logging.info(\'      samples: {:4d}\'.format(len(ds[\'query\'])))\n  logging.info(\'   vocab_size: {:4d}\'.format(len(dicts[\'token_ids\'])))\n  logging.info(\'   slot count: {:4d}\'.format(len(dicts[\'slot_ids\'])))\n  logging.info(\' intent count: {:4d}\'.format(len(dicts[\'intent_ids\'])))\n\n  t2i, s2i, in2i = map(dicts.get, [\'token_ids\', \'slot_ids\', \'intent_ids\'])\n  i2t, i2s, i2in = map(lambda d: {d[k]: k for k in d.keys()}, [t2i, s2i, in2i])\n  query, slots, intent = map(ds.get,\n                             [\'query\', \'slot_labels\', \'intent_labels\'])\n\n  with open(output_file_path, ""w"", encoding=""utf-8"") as out_file:\n    for i in range(len(query)):\n      out_file.write(i2in[intent[i][0]] + ""\\t"")\n      out_file.write(\' \'.join(map(i2s.get, slots[i])) + ""\\t"")\n      out_file.write(\' \'.join(map(i2t.get, query[i])) + ""\\n"")\n\n\ndef random_upsampling(data, sample_num):\n  """"""Up sample""""""\n  np.random.seed(2019)\n  new_indices = np.random.permutation(range(sample_num))\n  original_size = len(data)\n  new_data = [data[i % original_size] for i in new_indices]\n  return new_data\n\n\ndef mock_a_text_file(sample_lines, line_num, file_name):\n  """"""Generate a mock text file for test.""""""\n  with open(file_name, ""w"", encoding=""utf-8"") as f:\n    lines = random_upsampling(sample_lines, line_num)\n    for line in lines:\n      f.write(line + ""\\n"")\n\n\ndef generate_vocab_file():\n  """"""Generate Vocab file for test. no usage now """"""\n  tmpdir = Path(tempfile.mkdtemp())\n  vocab_file = str(tmpdir.joinpath(\'vocab.txt\'))\n  dummy_vocabs = [""</s>"", ""<unk>"", ""\xe4\xbd\xa0\xe5\xa5\xbd"", ""\xe5\x8c\x97\xe4\xba\xac""]\n  save_a_vocab_file(vocab_file, dummy_vocabs)\n  return vocab_file\n\n\ndef save_a_vocab_file(vocab_file, vocab_list):\n  """"""Save a Vocab file for test.""""""\n  with open(vocab_file, ""w"", encoding=\'utf-8\') as out_f:\n    for vocab in vocab_list:\n      out_f.write(vocab)\n      out_f.write(\'\\n\')\n  return vocab_file\n\n\ndef split_file(ori_file):\n  src_file = ori_file + \'.src\'\n  tgt_file = ori_file + \'.tgt\'\n  with open(ori_file, \'r\', encoding=\'utf8\') as in_f:\n    lines = in_f.readlines()\n  src, tgt = zip(*[sent.split(\'\\t\') for sent in lines])\n  with open(src_file, \'w\', encoding=\'utf8\') as out_f:\n    for src_sent in src:\n      out_f.write(src_sent+\'\\n\')\n  with open(tgt_file, \'w\', encoding=\'utf8\') as out_f:\n    for tgt_sent in tgt:\n      out_f.write(tgt_sent)\n  os.remove(ori_file)\n\n\ndef mock_data(samples, train_file, dev_file, test_file, text_vocab_file=None,\n              text_vocab_list=None, label_vocab_file=None, label_vocab_list=None):\n  logging.info(""Generate mock data: {}"".format(train_file))\n  mock_a_text_file(samples, 300, train_file)\n  logging.info(""Generate mock data: {}"".format(dev_file))\n  mock_a_text_file(samples, 100, dev_file)\n  logging.info(""Generate mock data: {}"".format(test_file))\n  mock_a_text_file(samples, 100, test_file)\n\n  if text_vocab_file and text_vocab_list:\n    logging.info(""Generate text vocab file: {}"".format(text_vocab_file))\n    save_a_vocab_file(text_vocab_file, text_vocab_list)\n  if label_vocab_file and label_vocab_file:\n    logging.info(""Generate label vocab file: {}"".format(label_vocab_file))\n    save_a_vocab_file(label_vocab_file, label_vocab_list)\n\n  # for seq2seq\n  if not text_vocab_file and not text_vocab_list and not \\\n    label_vocab_file and not label_vocab_list:\n    logging.info(""Generate mock data: {} and split to src and tgt."".format(train_file))\n    split_file(train_file)\n    logging.info(""Generate mock data: {} and split to src and tgt."".format(dev_file))\n    split_file(dev_file)\n    logging.info(""Generate mock data: {} and split to src and tgt."".format(test_file))\n    split_file(test_file)\n'"
delta/data/datasets/yahoo_answer.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\n## Description\nYahoo answers are obtained from (Zhang et al., 2015). This is a topic classification task with 10 classes: Society & Culture,\nScience & Mathematics, Health, Education & Reference, Computers & Internet, Sports, Business & Finance, Entertainment & Music,\nFamily & Relationships and Politics & Government. The document we use includes question titles, question contexts and best answers.\n\n## Download links\n\nhttps://s3.amazonaws.com/fast-ai-nlp/yahoo_answers_csv.tg\n\n## Data scale introduction\n\nwe split the raw data into training set, development dataset and test dataset\n- Training dataset\xef\xbc\x9a1260,000\n- Development pairs\xef\xbc\x9a140,000\n- Test pairs\xef\xbc\x9a60,000\n""""""\n\nimport wget\nimport os\nimport traceback\nimport csv\nfrom absl import logging\nfrom delta.data.datasets.base_dataset import BaseDataSet\nfrom delta.utils.register import registers\nfrom delta.data.datasets.utils import split_train_dev\n\n\n@registers.dataset.register(\'yahoo_answer\')\nclass YahooAnswer(BaseDataSet):\n  """"""yahoo answer data class for cls task.""""""\n\n  def __init__(self, project_dir):\n    super().__init__(project_dir)\n    self.data_files = [""train.txt"", ""test.txt"", ""dev.txt""]\n    self.config_files = [\'yahoo_answer_text_cls_han.yml\']\n    self.download_files = [""yahoo_answers_csv.tgz""]\n\n  def download(self) -> bool:\n    url = ""https://s3.amazonaws.com/fast-ai-nlp/yahoo_answers_csv.tgz""\n    try:\n      wget.download(url, self.download_dir)\n    except Exception as e:\n      logging.warning(repr(e))\n      return False\n    return True\n\n  @staticmethod\n  def to_standard_format(input_file, output_file):\n    logging.info(""Save file to {}"".format(output_file))\n\n    with open(input_file, encoding=""utf-8"") as csv_file, \\\n      open(output_file, ""w"", encoding=""utf-8"") as out_file:\n      csv_reader = csv.reader(csv_file)\n      for row in csv_reader:\n        if len(row) < 4:\n          continue\n        label = row[0]\n        text = "" "".join(row[1:])\n        out_file.write(label + ""\\t"" + text + ""\\n"")\n\n  def after_download(self) -> bool:\n    try:\n      download_file = os.path.join(self.download_dir, ""yahoo_answers_csv.tgz"")\n      os.system(f""tar zxvf {download_file}  -C {self.download_dir}"")\n      self.to_standard_format(os.path.join(self.download_dir, ""yahoo_answers_csv/train.csv""),\n                              os.path.join(self.data_dir, ""train_all.txt""))\n      self.to_standard_format(os.path.join(self.download_dir, ""yahoo_answers_csv/test.csv""),\n                              os.path.join(self.data_dir, ""test.txt""))\n      split_train_dev(os.path.join(self.data_dir, ""train_all.txt""),\n                      os.path.join(self.data_dir, ""train.txt""),\n                      os.path.join(self.data_dir, ""dev.txt""), 0.1)\n    except Exception as e:\n      logging.warning(traceback.format_exc())\n      return False\n    return True\n'"
delta/data/datasets/yahoo_answer_test.py,2,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for compat.py.""""""\n\nfrom absl import logging\n\nimport delta.compat as tf\nfrom delta.data.datasets.yahoo_answer import YahooAnswer\n\n\nclass YahooAnswerTest(tf.test.TestCase):\n  """"""yahoo answer data class test for cls task.""""""\n\n  def test_build(self):\n    yahoo_answer = YahooAnswer(\'yahoo_answer\')\n    yahoo_answer.build()\n    self.assertTrue(yahoo_answer.is_ready())\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.DEBUG)\n  tf.test.main()\n'"
delta/data/feat/__init__.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' speech feature \'\'\'\nfrom delta.data.feat import speech_ops\n\nfrom .speech_feature import load_wav\nfrom .speech_feature import extract_feature\nfrom .speech_feature import add_delta_delta\n\n# numpy\nfrom .speech_feature import extract_fbank\nfrom .speech_feature import delta_delta\nfrom .speech_feature import fbank_feat\nfrom .speech_feature import powspec_feat\nfrom .speech_feature import extract_feat\n'"
delta/data/feat/speech_feature.py,20,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' speech feat entrypoint unittest\'\'\'\nimport os\nimport numpy as np\nimport delta.compat as tf\nfrom absl import logging\nfrom delta.data.feat import speech_ops\nfrom core.ops import py_x_ops\nfrom delta.data.feat import python_speech_features as psf\n\n_global_sess = {}\n\n\ndef _get_session(feat_name, graph=None):\n  global _global_sess\n  sess = None\n  if feat_name not in _global_sess:\n    assert graph is not None\n    sess = tf.Session(graph=graph)\n    _global_sess[feat_name] = sess\n  else:\n    sess = _global_sess[feat_name]\n  return sess\n\n\ndef _get_out_tensor_name(tensor_name, output_index):\n  return tensor_name + "":"" + str(output_index)\n\n\ndef _freq_feat_graph(feat_name, **kwargs):\n  winlen = kwargs.get(\'winlen\')\n  winstep = kwargs.get(\'winstep\')\n  feature_size = kwargs.get(\'feature_size\')\n  sr = kwargs.get(\'sr\')  #pylint: disable=invalid-name\n  nfft = kwargs.get(\'nfft\')\n  del nfft\n\n  assert feat_name in (\'fbank\', \'spec\')\n\n  params = speech_ops.speech_params(\n      sr=sr,\n      bins=feature_size,\n      add_delta_deltas=False,\n      audio_frame_length=winlen,\n      audio_frame_step=winstep)\n\n  graph = None\n  if feat_name == \'fbank\':\n    # get session\n    if feat_name not in _global_sess:\n      graph = tf.Graph()\n      #pylint: disable=not-context-manager\n      with graph.as_default():\n        # fbank\n        filepath = tf.placeholder(dtype=tf.string, shape=[], name=\'wavpath\')\n        waveforms, sample_rate = speech_ops.read_wav(filepath, params)\n        del sample_rate\n        fbank = speech_ops.extract_feature(waveforms, params)\n        # shape must be [T, D, C]\n        feat = tf.identity(fbank, name=feat_name)\n  elif feat_name == \'spec\':\n    # magnitude spec\n    if feat_name not in _global_sess:\n      graph = tf.Graph()\n      #pylint: disable=not-context-manager\n      with graph.as_default():\n        filepath = tf.placeholder(dtype=tf.string, shape=[], name=\'wavpath\')\n        waveforms, sample_rate = speech_ops.read_wav(filepath, params)\n\n        spec = py_x_ops.spectrum(\n            waveforms[:, 0],\n            tf.cast(sample_rate, tf.dtypes.float32),\n            window_length=0.025,\n            frame_length=0.010,\n            output_type=1,\n            snip_edges=True,\n            raw_energy=1,\n            preEph_coeff=0.97,\n            window_type=\'povey\',\n            remove_dc_offset=True,\n            is_fbank=False)  #output_type: 1, power spec; 2 log power spec\n        spec = tf.sqrt(spec)\n        # shape must be [T, D, C]\n        spec = tf.expand_dims(spec, -1)\n        feat = tf.identity(spec, name=feat_name)\n  else:\n    raise ValueError(f""Not support freq feat: {feat_name}."")\n\n  return graph, (_get_out_tensor_name(\'wavpath\',\n                                      0), _get_out_tensor_name(feat_name, 0))\n\n\n#pylint: disable=too-many-locals\ndef extract_feature(*wavefiles, **kwargs):\n  \'\'\' tensorflow fbank feat \'\'\'\n  dry_run = kwargs.get(\'dry_run\')\n  feat_name = \'fbank\'\n  feat_name = kwargs.get(\'feature_name\')\n  assert feat_name\n\n  graph, (input_tensor, output_tensor) = _freq_feat_graph(feat_name, **kwargs)\n  sess = _get_session(_get_out_tensor_name(feat_name, 0), graph)\n\n  for wavpath in wavefiles:\n    savepath = os.path.splitext(wavpath)[0] + \'.npy\'\n    logging.debug(\'extract_feat: input: {}, output: {}\'.format(\n        wavpath, savepath))\n\n    feat = sess.run(output_tensor, feed_dict={input_tensor: wavpath})\n\n    # save feat\n    if dry_run:\n      logging.info(\'save feat: path {} shape:{} dtype:{}\'.format(\n          savepath, feat.shape, feat.dtype))\n    else:\n      np.save(savepath, feat)\n\n\ndef add_delta_delta(feat, feat_size, order=2):\n  \'\'\' add delta detla \'\'\'\n  feat_name = \'delta_delta\'\n  graph = None\n  # get session\n  if feat_name not in _global_sess:\n    graph = tf.Graph()\n    #pylint: disable=not-context-manager\n    with graph.as_default():\n      fbank = tf.placeholder(\n          dtype=tf.float32, shape=[None, feat_size, 1], name=\'fbank\')\n      feat_with_delta_delta = speech_ops.delta_delta(fbank, order=order)\n      feat_with_delta_delta = tf.identity(feat_with_delta_delta, name=feat_name)\n\n  sess = _get_session(feat_name, graph)\n  feat = sess.run(\n      _get_out_tensor_name(feat_name, 0), feed_dict={\'fbank:0\': feat})\n  return feat\n\n\n#pylint: disable=invalid-name\ndef load_wav(wavpath, sr=8000):\n  \'\'\'\n  audio:\n    np.float32, shape [None], sample in [-1, 1], using librosa.load\n    np.int16, shape [None], sample in [-32768, 32767], using scipy.io.wavfile\n    np.float32, shape[None, audio_channel], sample int [-1, 1], using tf.DecodeWav\n\n  return\n    sr: sample rate\n    audio: [-1, 1], same to tf.DecodeWav\n  \'\'\'\n  #from scipy.io import wavfile\n  #sample_rate, audio = wavfile.read(wavpath)\n\n  #samples, sample_rate = librosa.load(wavpath, sr=sr)\n\n  feat_name = \'load_wav\'\n  graph = None\n  # get session\n  if feat_name not in _global_sess:\n    graph = tf.Graph()\n    with graph.as_default():\n      params = speech_ops.speech_params(sr=sr, audio_desired_samples=-1)\n      t_wavpath = tf.placeholder(dtype=tf.string, name=""wavpath"")\n      t_audio, t_sample_rate = speech_ops.read_wav(t_wavpath, params)\n      t_audio = tf.identity(t_audio, name=""audio"")\n      t_sample_rate = tf.identity(t_sample_rate, name=""sample_rate"")\n\n  sess = _get_session(feat_name, graph)\n  audio, sample_rate = sess.run([\n      _get_out_tensor_name(\'audio\', 0),\n      _get_out_tensor_name(\'sample_rate\', 0)\n  ],\n                                feed_dict={""wavpath:0"": wavpath})\n  audio = audio[:, 0]\n\n  assert sample_rate == sr, \'sampling rate must be {}Hz, get {}Hz\'.format(\n      sr, sample_rate)\n  return sample_rate, audio\n\n\n#pylint: disable=invalid-name\ndef extract_fbank(samples, sr=8000, winlen=0.025, winstep=0.01,\n                  feature_size=40):\n  \'\'\' extract logfbank with delta and delta-delta\n  Return:\n      ndarray of shape [nfrmae, feature_size * 3]\n  \'\'\'\n  feat = psf.logfbank(\n      samples,\n      nfilt=feature_size,\n      samplerate=sr,\n      winlen=winlen,\n      winstep=winstep,\n      lowfreq=0,\n      highfreq=None,\n      preemph=0.97)\n  delta = psf.delta(feat, N=2)\n  _delta_delta = psf.delta(delta, N=2)\n  return np.stack([feat, delta, _delta_delta], axis=-1)\n\n\n#pylint: disable=invalid-name\ndef delta_delta(fbank, sr=8000, feature_size=40):\n  \'\'\'\n  params:\n    fbank: [nframe, nfbank]\n  return : [nframe, nfbank, 3]\n  \'\'\'\n  del sr, feature_size\n  assert fbank.ndim == 2\n  delta = psf.delta(fbank, N=2)\n  _delta_delta = psf.delta(delta, N=2)\n  return np.stack([fbank, delta, _delta_delta], axis=-1)\n\n\n#pylint: disable=too-many-arguments,invalid-name\ndef fbank_feat(powspec,\n               sr=8000,\n               feature_size=40,\n               nfft=512,\n               lowfreq=0,\n               highfreq=None):\n  \'\'\' return : [nframe, nfbank] \'\'\'\n  feat = psf.logfbank_from_powspec(\n      powspec,\n      samplerate=sr,\n      nfilt=feature_size,\n      nfft=nfft,\n      lowfreq=lowfreq,\n      highfreq=highfreq,\n  )\n  return feat\n\n\n#pylint: disable=too-many-arguments,invalid-name\ndef powspec_feat(samples,\n                 sr=8000,\n                 nfft=512,\n                 winlen=0.025,\n                 winstep=0.01,\n                 lowfreq=0,\n                 highfreq=None,\n                 preemph=0.97):\n  \'\'\' return : [nframe, nfft / 2 + 1] \'\'\'\n  feat = psf.powerspec(\n      samples,\n      nfft=nfft,\n      samplerate=sr,\n      winlen=winlen,\n      winstep=winstep,\n      lowfreq=lowfreq,\n      highfreq=highfreq,\n      preemph=preemph)\n  return feat\n\n\n#pylint: disable=invalid-name\ndef freq_resolution(sr, nfft):\n  \'\'\'\n  :param: sr, sample rate\n  :param: nfft, fft points\n  :return: freq resolution of one point\n  \'\'\'\n  return (sr / 2) / (nfft / 2)\n\n\ndef points(freq, resolution):\n  \'\'\'\n  :params: freq, freq in Hz\n  :params: resolution, Hz of one point\n  :return: number of points equal to `freq`\n  \'\'\'\n  return freq / resolution\n\n\n#pylint: disable=too-many-locals\ndef extract_feat(*args, **kwargs):\n  \'\'\' pyfeat, extract feat from utt and dump it \'\'\'\n  logging.debug(""extract_feat : {}"".format(kwargs))\n\n  winlen = kwargs.get(\'winlen\')\n  winstep = kwargs.get(\'winstep\')\n  feature_size = kwargs.get(\'feature_size\')\n  sr = kwargs.get(\'sr\')  #pylint: disable=invalid-name\n  nfft = kwargs.get(\'nfft\')\n  lowfreq = kwargs.get(\'lowfreq\')\n  highfreq = kwargs.get(\'highfreq\')\n  preemph = kwargs.get(\'preemph\')\n  save_feat_path = kwargs.get(\'save_feat_path\')\n  dry_run = kwargs.get(\'dry_run\')\n  feat_type = kwargs.get(\'feat_type\')\n\n  del lowfreq, preemph\n\n  if save_feat_path and not os.path.exists(save_feat_path):\n    os.makedirs(save_feat_path)\n\n  for wavpath in args:\n    if save_feat_path:\n      filename = os.path.splitext(os.path.split(wavpath)[-1])[0] + \'.npy\'\n      savepath = os.path.join(save_feat_path, filename)\n    else:\n      savepath = os.path.splitext(wavpath)[0] + \'.npy\'\n    logging.debug(\'input: {}, output: {}\'.format(wavpath, savepath))\n\n    sr_out, samples = load_wav(wavpath, sr=sr)\n    del sr_out\n    feat = powspec_feat(\n        samples, sr=sr, nfft=nfft, winlen=winlen, winstep=winstep)\n    logging.debug(\'apply power spectorgram\')\n\n    if feat_type == \'spectrogram\':\n      # shape: [T, F]\n      feat = psf.logpowerspec(feat)\n      if highfreq:\n        resolution = freq_resolution(sr, nfft)\n        ps = int(points(highfreq, resolution))  #pylint: disable=invalid-name\n        logging.debug(""feat slice: {} {}"".format(ps, type(ps)))\n        feat = feat[:, :ps]\n      logging.debug(\'apply log power spectorgram\')\n    elif feat_type == \'logfbank\':\n      feat = fbank_feat(feat, sr=sr, nfft=nfft, feature_size=feature_size)\n      logging.debug(\'apply fbank spectorgram\')\n    else:\n      raise ValueError(""not support feat method"")\n\n    feat = feat.astype(np.float32)\n    if dry_run:\n      logging.info(\'save feat: path {} shape:{} dtype:{}\'.format(\n          savepath, feat.shape, feat.dtype))\n    else:\n      np.save(savepath, feat)\n'"
delta/data/feat/speech_feature_test.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' speech feature entrypoint unittest\'\'\'\nimport os\nfrom pathlib import Path\nimport librosa\nimport numpy as np\nimport delta.compat as tf\nfrom absl import logging\nfrom delta.data.feat import speech_ops\nfrom delta.data.feat import speech_feature\nfrom delta import PACKAGE_ROOT_DIR\n\n\n#pylint: disable=too-many-instance-attributes\nclass SpeechFeatureTest(tf.test.TestCase):\n  \'\'\' speech feat entrypoint unittest\'\'\'\n\n  def setUp(self):\n    super().setUp()\n    self.winlen = 0.025\n    self.winstep = 0.010\n    self.feature_size = 40\n    self.sr = 8000  #pylint: disable=invalid-name\n    self.nfft = 512\n    self.feat_type = \'logfbank\'\n\n    package_root = Path(PACKAGE_ROOT_DIR)\n    self.wavfile = str(\n        package_root.joinpath(\'data/feat/python_speech_features/english.wav\'))\n    self.featfile = str(\n        package_root.joinpath(\'data/feat/python_speech_features/english.npy\'))\n\n  def tearDown(self):\n    \'\'\' tear down \'\'\'\n    if os.path.exists(self.featfile):\n      os.unlink(self.featfile)\n\n  def test_load_wav(self):\n    \'\'\' test load wav \'\'\'\n    sample_rate, audio = speech_feature.load_wav(self.wavfile, sr=self.sr)\n    audio_true, sample_rate_true = librosa.load(self.wavfile, sr=self.sr)\n\n    self.assertEqual(sample_rate, sample_rate_true)\n    self.assertEqual(sample_rate, self.sr)\n    self.assertAllClose(audio, audio_true)\n\n  def test_tf_fbank(self):\n    \'\'\' test tensorflow fbank feature interface \'\'\'\n    speech_feature.extract_feature((self.wavfile),\n                                   winlen=self.winlen,\n                                   winstep=self.winstep,\n                                   sr=self.sr,\n                                   feature_size=self.feature_size,\n                                   feature_name=\'fbank\')\n\n    feat = np.load(self.featfile)\n    logging.info(f""feat : {feat}"")\n    self.assertEqual(feat.shape, (425, 40, 1))\n\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      feat = speech_ops.delta_delta(feat, 2)\n      self.assertEqual(feat.eval().shape, (425, 40, 3))\n\n  def test_tf_spec(self):\n    \'\'\' test tensorflow spec feature interface \'\'\'\n    speech_feature.extract_feature((self.wavfile),\n                                   winlen=self.winlen,\n                                   winstep=self.winstep,\n                                   sr=self.sr,\n                                   feature_size=self.feature_size,\n                                   feature_name=\'spec\')\n    feat = np.load(self.featfile)\n    self.assertEqual(feat.shape, (425, 129, 1))\n\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      feat = speech_ops.delta_delta(feat, 2)\n      self.assertEqual(feat.eval().shape, (425, 129, 3))\n\n  def test_tf_delta_detla(self):\n    \'\'\' test tensorflow delta delta \'\'\'\n    speech_feature.extract_feature((self.wavfile),\n                                   winlen=self.winlen,\n                                   winstep=self.winstep,\n                                   sr=self.sr,\n                                   feature_size=self.feature_size,\n                                   feature_name=\'fbank\')\n\n    feat = np.load(self.featfile)\n    self.assertEqual(feat.shape, (425, 40, 1))\n    feat = speech_feature.add_delta_delta(feat, 40, order=2)\n    self.assertEqual(feat.shape, (425, 40, 3))\n\n  def test_py_extract_feat(self):\n    \'\'\' test python fbank with delta-delta interface \'\'\'\n    speech_feature.extract_feat((self.wavfile),\n                                winlen=self.winlen,\n                                winstep=self.winstep,\n                                sr=self.sr,\n                                feature_size=self.feature_size,\n                                nfft=self.nfft,\n                                feat_type=self.feat_type)\n    feat = np.load(self.featfile)\n    self.assertEqual(feat.shape, (426, 40))\n\n    feat = speech_feature.delta_delta(feat, sr=self.sr)\n    self.assertEqual(feat.shape, (426, 40, 3))\n\n  def test_py_feat_interface(self):\n    \'\'\' test python feat interface \'\'\'\n    sr_out, samples = speech_feature.load_wav(self.wavfile, sr=self.sr)\n    self.assertEqual(sr_out, self.sr)\n\n    pspec = speech_feature.powspec_feat(\n        samples,\n        sr=self.sr,\n        nfft=self.nfft,\n        winlen=self.winlen,\n        winstep=self.winstep)\n    self.assertEqual(pspec.shape, (426, 257))\n\n    fbank = speech_feature.fbank_feat(\n        pspec, feature_size=self.feature_size, sr=self.sr, nfft=self.nfft)\n    self.assertEqual(fbank.shape, (426, 40))\n\n    fbank = speech_feature.delta_delta(fbank, sr=self.sr)\n    self.assertEqual(fbank.shape, (426, 40, 3))\n\n    feat_ = speech_feature.extract_fbank(\n        samples,\n        sr=self.sr,\n        winlen=self.winlen,\n        winstep=self.winstep,\n        feature_size=self.feature_size)\n    self.assertEqual(feat_.shape, (426, 40, 3))\n\n    samples = np.pad(samples, [0, 100], mode=\'constant\')\n    pspec = speech_feature.powspec_feat(\n        samples,\n        sr=self.sr,\n        nfft=self.nfft,\n        winlen=self.winlen,\n        winstep=self.winstep)\n    self.assertEqual(pspec.shape, (427, 257))\n    fbank = speech_feature.fbank_feat(\n        pspec, feature_size=self.feature_size, sr=self.sr, nfft=self.nfft)\n    fbank = speech_feature.delta_delta(fbank, sr=self.sr)\n    self.assertEqual(fbank.shape, (427, 40, 3))\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  tf.test.main()\n'"
delta/data/feat/speech_ops.py,43,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' speech feat ops interface \'\'\'\nimport numpy as np\nfrom absl import logging\n\n#pylint: disable=no-name-in-module\nimport delta.compat as tf\nfrom tensorflow.python.ops import gen_audio_ops as audio_ops\n\nfrom delta import utils\nfrom delta.utils.hparam import HParams\nfrom core.ops import py_x_ops\n\n\n#pylint: disable=invalid-name,too-many-arguments\ndef speech_params(sr=16000,\n                  bins=40,\n                  dither=True,\n                  add_delta_deltas=False,\n                  audio_desired_samples=-1,\n                  audio_frame_length=0.025,\n                  audio_frame_step=0.010,\n                  cmvn=False,\n                  cmvn_path=\'\'):\n  \'\'\' speech feat params \'\'\'\n  p = HParams()\n  p.add_hparam(""audio_sample_rate"", sr)\n  if dither:\n    p.add_hparam(""audio_dither"", 1.0 / np.iinfo(np.int16).max)\n  else:\n    p.add_hparam(""audio_dither"", 0.0)\n\n  p.add_hparam(""audio_preemphasis"", 0.97)\n  p.add_hparam(""audio_desired_channels"", 1)\n  p.add_hparam(""audio_desired_samples"", audio_desired_samples)\n  p.add_hparam(""audio_frame_length"", audio_frame_length)\n  p.add_hparam(""audio_frame_step"", audio_frame_step)\n  p.add_hparam(""audio_lower_edge_hertz"", 20.0)\n  p.add_hparam(""audio_upper_edge_hertz"", sr / 2.0)\n  p.add_hparam(""audio_num_mel_bins"", bins)\n  p.add_hparam(""audio_add_delta_deltas"", add_delta_deltas)\n  p.add_hparam(""num_zeropad_frames"", 0)\n  p.add_hparam(""audio_global_cmvn"", cmvn)\n  p.add_hparam(""audio_cmvn_path"", cmvn_path)\n  return p\n\n\ndef read_wav(wavfile, params):\n  \'\'\'\n     params:\n       wavfile: file name\n     returns:\n       audio: samples of shape [nsample, channels]\n       sample_rate: sample rate\n  \'\'\'\n  contents = tf.read_file(wavfile)\n  #pylint: disable=no-member\n  waveforms = tf.audio.decode_wav(\n      contents,\n      desired_channels=params.audio_desired_channels,\n      desired_samples=params.audio_desired_samples,\n  )\n  return waveforms.audio, waveforms.sample_rate\n\n\n#pylint: disable=too-many-arguments\ndef powspec_feat(samples,\n                 sr=8000,\n                 nfft=512,\n                 winlen=0.025,\n                 winstep=0.010,\n                 lowfreq=0,\n                 highfreq=None,\n                 preemph=0.97):\n  \'\'\'\n  params:\n    samples: [nsample, channels]\n  returns:\n    powspec: power spectrogram, shape [channels, nframe, nfft / 2 + 1] \'\'\'\n  del nfft\n  del lowfreq\n  del highfreq\n  del preemph\n\n  #pylint: disable=no-member\n  feat = audio_ops.audio_spectrogram(\n      samples,\n      window_size=winlen * sr,\n      stride=winstep * sr,\n      magnitude_squared=True)\n  return feat\n\n\ndef fbank_feat(powspec,\n               sr=8000,\n               feature_size=40,\n               nfft=512,\n               lowfreq=0,\n               highfreq=None):\n  \'\'\' powspec: [audio_channels, spectrogram_length, spectrogram_feat_dim]\n      return : [auido_chnnels, nframe, nfbank]\n  \'\'\'\n  del nfft\n\n  true_fn = lambda: tf.expand_dims(powspec, 0)\n  false_fn = lambda: powspec\n  powspec = tf.cond(tf.equal(tf.rank(powspec), 2), true_fn, false_fn)\n\n  feat = py_x_ops.fbank(\n      powspec,\n      sr,\n      filterbank_channel_count=feature_size,\n      lower_frequency_limit=lowfreq,\n      upper_frequency_limit=highfreq,\n  )\n  return feat\n\n\ndef delta_delta(feat, order=2):\n  \'\'\'\n  params:\n    feat: a tensor of shape [nframe, nfbank] or [nframe, nfbank, 1]\n  return: [nframe, nfbank, 3]\n  \'\'\'\n  feat = tf.cond(\n      tf.equal(tf.rank(feat), 3),\n      true_fn=lambda: feat[:, :, 0],\n      false_fn=lambda: feat)\n\n  shape = tf.shape(feat)\n  # [nframe nfbank*3]\n  nframe = shape[0]\n  nfbank = shape[1]\n  delta = py_x_ops.delta_delta(feat, order=order)\n  feat_with_delta_delta = tf.reshape(delta, (nframe, nfbank, (order + 1)))\n  return feat_with_delta_delta\n\n\n#pylint: disable=too-many-arguments\ndef compute_mel_filterbank_features(waveforms,\n                                    sample_rate=16000,\n                                    preemphasis=0.97,\n                                    frame_length=0.025,\n                                    frame_step=0.010,\n                                    fft_length=None,\n                                    lower_edge_hertz=80.0,\n                                    upper_edge_hertz=7600.0,\n                                    num_mel_bins=80,\n                                    log_noise_floor=1e-3,\n                                    apply_mask=True):\n  """"""Implement mel-filterbank extraction using tf ops.\n  Args:\n    waveforms: float32 tensor with shape [max_len, nchannels]\n    sample_rate: sampling rate of the waveform\n    preemphasis: waveform high-pass filtering constant\n    frame_length: frame length in ms\n    frame_step: frame_Step in ms\n    fft_length: number of fft bins\n    lower_edge_hertz: lowest frequency of the filterbank\n    upper_edge_hertz: highest frequency of the filterbank\n    num_mel_bins: filterbank size\n    log_noise_floor: clip small values to prevent numeric overflow in log\n    apply_mask: When working on a batch of samples, set padding frames to zero\n  Returns:\n    filterbanks: a float32 tensor with shape [nchannles, max_len, num_bins]\n  """"""\n  del log_noise_floor, apply_mask\n  spectrogram = powspec_feat(\n      waveforms,\n      sr=sample_rate,\n      nfft=512 if not fft_length else fft_length,\n      winlen=frame_length,\n      winstep=frame_step,\n      lowfreq=lower_edge_hertz,\n      highfreq=upper_edge_hertz,\n      preemph=preemphasis)\n\n  # [channels, time, feat_dim]\n  fbank = fbank_feat(\n      spectrogram,\n      sr=sample_rate,\n      feature_size=num_mel_bins,\n      nfft=512 if not fft_length else fft_length,\n      lowfreq=lower_edge_hertz,\n      highfreq=upper_edge_hertz)\n\n  # [time, feat_dim]\n  fbank = tf.cond(\n      tf.equal(tf.rank(fbank), 3),\n      true_fn=lambda: fbank[0, :, :],\n      false_fn=lambda: fbank)\n  return fbank\n\n\ndef extract_logfbank_with_delta(waveforms, params):\n  \'\'\'\n   params:\n     waveforms: float32 tensor with shape [max_len]\n  \'\'\'\n  p = params\n  mel_fbanks = compute_mel_filterbank_features(\n      waveforms,\n      sample_rate=p.audio_sample_rate,\n      preemphasis=p.audio_preemphasis,\n      frame_length=p.audio_frame_length,\n      frame_step=p.audio_frame_step,\n      lower_edge_hertz=p.audio_lower_edge_hertz,\n      upper_edge_hertz=p.audio_upper_edge_hertz,\n      num_mel_bins=p.audio_num_mel_bins,\n      apply_mask=False)\n\n  if p.audio_add_delta_deltas:\n    mel_fbanks = delta_delta(mel_fbanks)\n  else:\n    mel_fbanks = tf.expand_dims(mel_fbanks, axis=-1)\n  # shape: [nframes, nbins, nchannels]\n  return mel_fbanks\n\n\ndef extract_feature(waveforms, params):\n  \'\'\'waveforms: [samples, audio_channels]\n     return: features, [nframes, feat_size, channels]\n  \'\'\'\n  p = params\n  with tf.variable_scope(\'feature_extractor\'):\n    mel_fbanks = extract_logfbank_with_delta(waveforms, params)\n    # shape: [nframes, nbins, nchannels]\n    fbank_size = utils.shape_list(mel_fbanks)\n    #assert fbank_size[0] == 1\n    logging.debug(""fbank size : {}"".format(fbank_size))\n\n    # This replaces CMVN estimation on data\n    if not p.audio_global_cmvn:\n      mean = tf.reduce_mean(mel_fbanks, keepdims=True, axis=1)\n      variance = tf.reduce_mean(\n          tf.square(mel_fbanks - mean), keepdims=True, axis=1)\n    else:\n      assert p.audio_cmvn_path\n      mean, variance = utils.load_cmvn(p.audio_cmvn_path)\n\n    var_epsilon = 1e-09\n    mel_fbanks = utils.apply_cmvn(mel_fbanks, mean, variance, var_epsilon)\n\n    # Later models like to flatten the two spatial dims. Instead, we add a\n    # unit spatial dim and flatten the frequencies and channels.\n    feats = tf.concat([\n        tf.reshape(mel_fbanks, [fbank_size[0], fbank_size[1], fbank_size[2]]),\n        tf.zeros((p.num_zeropad_frames, fbank_size[1], fbank_size[2]))\n    ], 0)\n  return feats  # shape [nframes, featue_size, chnanels]\n\n\ndef _new_tensor_array(name, size, dtype=None):\n  \'\'\' create empty TensorArray which can store size elements.\'\'\'\n  return tf.TensorArray(dtype, size, name=name)\n\n\ndef batch_extract_feature(waveforms, params):\n  \'\'\' waveforms: [batch, samples, audio_channels]\n  return: features [batch, nframes, feat_size, channles]\n  \'\'\'\n\n  def _to_tensor_array(name, v, clear_after_read=None):\n    \'\'\' create TensorArray from v, of size batch.\'\'\'\n    ta = tf.TensorArray(\n        v.dtype, batch, name=name, clear_after_read=clear_after_read)\n    ta = ta.unstack(v)\n    return ta\n\n  def _loop_continue(time, inputs, unused_output_tas):\n    del unused_output_tas\n    batch = tf.shape(inputs)[0]\n    return time < batch\n\n  def _loop_body(time, inputs, output_tas):\n    feat = extract_feature(inputs[time, ...], params)\n    new_output_tas = output_tas.write(time, feat)\n    return (time + 1, inputs, new_output_tas)\n\n  batch = tf.shape(waveforms)[0]\n  output_tas = _new_tensor_array(\'batch_feat\', batch, dtype=tf.float32)\n  time = tf.constant(0, tf.int32)\n  loop_vars = (time, waveforms, output_tas)\n\n  parallel_iterations = 10\n  shape_invariants = tf.nest.map_structure(lambda t: tf.TensorShape(None),\n                                           loop_vars)\n\n  (time, inputs, output_tas) = tf.while_loop(\n      _loop_continue,\n      _loop_body,\n      loop_vars=loop_vars,\n      shape_invariants=shape_invariants,\n      parallel_iterations=parallel_iterations,\n      swap_memory=False)\n  del inputs\n\n  batch_feats = output_tas.stack()\n  return batch_feats\n\n\ndef splice(feat, left_context, right_context):\n  \'\'\'\n  splice frame with context\n    param: feat, tf.float32, [batch, time, feat]\n    return: feat, tf.float32, [batch, time, feat*(left_context + 1 + right_context)]\n    reference:\n      https://github.com/kaldi-asr/kaldi/src/feat/feature-functions.cc#L205:6\n  \'\'\'\n\n  def _loop_continue(time, end_time, context, unused_left_context,\n                     right_context, unused_output_tas):\n    del unused_output_tas\n    del unused_left_context\n    return time < end_time\n\n  def _loop_body(time, end_time, context, left_context, right_context,\n                 output_tas):\n    shape = tf.shape(context)\n    B, _, D = shape[0], shape[1], shape[2]\n    N = (1 + left_context + right_context) * D\n\n    new_feat = context[:, time:time + left_context + 1 + right_context, :]\n    new_feat = tf.reshape(new_feat, [B, N])\n    new_output_tas = output_tas.write(time, new_feat)\n    return (time + 1, end_time, context, left_context, right_context,\n            new_output_tas)\n\n  with tf.control_dependencies([\n      tf.assert_greater_equal(left_context, 0),\n      tf.assert_greater_equal(right_context, 0)\n  ]):\n    T = tf.shape(feat)[1]\n    output_tas = _new_tensor_array(\'splice_feat_ta\', T, dtype=tf.float32)\n    time = tf.constant(0, tf.int32)\n    first = tf.tile(feat[:, 0:1, :], [1, left_context, 1])\n    last = tf.tile(feat[:, -1:, :], [1, right_context, 1])\n    context = tf.concat([first, feat], axis=1)\n    context = tf.concat([context, last], axis=1)\n\n    loop_vars = (time, T, context, left_context, right_context, output_tas)\n\n    parallel_iterations = 10\n    shape_invariants = tf.nest.map_structure(lambda t: tf.TensorShape(None),\n                                             loop_vars)\n\n    (time, end_time, context, left_context, right_context,\n     output_tas) = tf.while_loop(\n         _loop_continue,\n         _loop_body,\n         loop_vars=loop_vars,\n         shape_invariants=shape_invariants,\n         parallel_iterations=parallel_iterations,\n         swap_memory=False)\n    del context\n    del left_context\n    del right_context\n\n    batch_spliced_feats = output_tas.stack()\n    batch_spliced_feats = tf.transpose(batch_spliced_feats, [1, 0, 2])\n  return batch_spliced_feats\n'"
delta/data/feat/speech_ops_test.py,12,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' speech feat ops unittest\'\'\'\nimport os\nfrom pathlib import Path\nimport numpy as np\nimport delta.compat as tf\nfrom absl import logging\n\nfrom delta.data.feat.speech_feature import load_wav\nfrom delta.data.feat import speech_ops as tffeat\nfrom delta import PACKAGE_ROOT_DIR\n\n\nclass SpeechOpsFeatTest(tf.test.TestCase):\n  \'\'\' test speech feat ops\'\'\'\n\n  def setUp(self):\n    super().setUp()\n    self.sr_true = 8000\n    #pylint: disable=invalid-name\n    self.hp = tffeat.speech_params(\n        sr=self.sr_true,\n        bins=40,\n        cmvn=False,\n        audio_desired_samples=1000,\n        add_delta_deltas=False)\n    self.wavpath = str(\n        Path(PACKAGE_ROOT_DIR).joinpath(\n            \'data/feat/python_speech_features/english.wav\'))\n    _, self.audio_true = load_wav(self.wavpath, sr=self.sr_true)\n\n  def test_read_wav(self):\n    \'\'\' test read wav op \'\'\'\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      wavfile = tf.constant(self.wavpath)\n      # read wav\n      audio, sample_rate = tffeat.read_wav(wavfile, self.hp)\n      self.assertEqual(sample_rate.eval(), self.sr_true)\n\n      self.assertEqual(audio.eval().shape, (1000, 1))\n      self.assertAllEqual(audio.eval()[:, 0], self.audio_true[:1000])\n\n  def test_powspec_feat(self):\n    \'\'\' test spectrogram op \'\'\'\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      wavfile = tf.constant(self.wavpath)\n      # read wav\n      audio, sample_rate = tffeat.read_wav(wavfile, self.hp)\n      del sample_rate\n      # spectorgram\n      spectrogram = tffeat.powspec_feat(\n          audio,\n          sr=self.sr_true,\n          nfft=None,\n          winlen=self.hp.audio_frame_length,\n          winstep=self.hp.audio_frame_step,\n          lowfreq=self.hp.audio_lower_edge_hertz,\n          highfreq=self.hp.audio_upper_edge_hertz,\n          preemph=self.hp.audio_preemphasis)\n\n      nfft = int(np.log2(self.hp.audio_frame_length * self.sr_true)) + 1\n      nfft = 1 << nfft\n      self.assertEqual(spectrogram.eval().shape, (1, 11, int(nfft / 2 + 1)))\n\n  def test_extract_logfbank_with_delta(self):\n    \'\'\' test logfbank with delta op\'\'\'\n    #pylint: disable=invalid-name\n    hp = tffeat.speech_params(\n        sr=self.sr_true,\n        bins=40,\n        cmvn=False,\n        audio_desired_samples=1000,\n        add_delta_deltas=False)\n\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      wavfile = tf.constant(self.wavpath)\n      # read wav\n      audio, sample_rate = tffeat.read_wav(wavfile, hp)\n      del sample_rate\n      # fbank with delta delta\n      fbank = tffeat.extract_logfbank_with_delta(audio, hp)\n      self.assertEqual(fbank.eval().shape, (11, 40, 1))\n\n  def test_extract_feature(self):\n    \'\'\' test logfbank with delta, and cmvn \'\'\'\n    #pylint: disable=invalid-name\n    hp = tffeat.speech_params(\n        sr=self.sr_true,\n        bins=40,\n        cmvn=False,\n        audio_desired_samples=1000,\n        add_delta_deltas=True)\n\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      wavfile = tf.constant(self.wavpath)\n      # read wav\n      audio, sample_rate = tffeat.read_wav(wavfile, hp)\n      del sample_rate\n\n      # fbank with delta delta and cmvn\n      feature = tffeat.extract_feature(audio, hp)\n\n      self.assertEqual(feature.eval().shape, (11, 40, 3))\n\n  def test_batch_extract_feature(self):\n    \'\'\' test batched feature extraction \'\'\'\n    #pylint: disable=invalid-name\n    hp = tffeat.speech_params(\n        sr=self.sr_true,\n        bins=40,\n        cmvn=False,\n        audio_desired_samples=1000,\n        add_delta_deltas=True)\n\n    batch_size = 2\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      wavfile = tf.constant(self.wavpath)\n      # read wav\n      audio, sample_rate = tffeat.read_wav(wavfile, hp)\n      del sample_rate\n\n      audio = tf.stack([audio] * batch_size)\n\n      # fbank with delta delta and cmvn\n      feature = tffeat.batch_extract_feature(audio, hp)\n\n      self.assertEqual(feature.eval().shape, (batch_size, 11, 40, 3))\n\n  def test_compute_mel_filterbank_features(self):\n    \'\'\' test logfbank ops\'\'\'\n    #pylint: disable=invalid-name\n    p = tffeat.speech_params(\n        sr=self.sr_true,\n        bins=40,\n        cmvn=False,\n        audio_desired_samples=1000,\n        add_delta_deltas=False)\n\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      wavfile = tf.constant(self.wavpath)\n      audio, sample_rate = tffeat.read_wav(wavfile, self.hp)\n      del sample_rate\n\n      feature = tffeat.compute_mel_filterbank_features(\n          audio,\n          sample_rate=p.audio_sample_rate,\n          preemphasis=p.audio_preemphasis,\n          frame_length=p.audio_frame_length,\n          frame_step=p.audio_frame_step,\n          lower_edge_hertz=p.audio_lower_edge_hertz,\n          upper_edge_hertz=p.audio_upper_edge_hertz,\n          num_mel_bins=p.audio_num_mel_bins,\n          apply_mask=False)\n\n      self.assertEqual(feature.eval().shape, (11, 40))\n\n  def test_delta_delta(self):\n    \'\'\' test add delta detlas \'\'\'\n    #pylint: disable=invalid-name\n    p = tffeat.speech_params(\n        sr=self.sr_true,\n        bins=40,\n        cmvn=False,\n        audio_desired_samples=1000,\n        add_delta_deltas=False)\n\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      wavfile = tf.constant(self.wavpath)\n      audio, sample_rate = tffeat.read_wav(wavfile, self.hp)\n      del sample_rate\n\n      feature = tffeat.compute_mel_filterbank_features(\n          audio,\n          sample_rate=p.audio_sample_rate,\n          preemphasis=p.audio_preemphasis,\n          frame_length=p.audio_frame_length,\n          frame_step=p.audio_frame_step,\n          lower_edge_hertz=p.audio_lower_edge_hertz,\n          upper_edge_hertz=p.audio_upper_edge_hertz,\n          num_mel_bins=p.audio_num_mel_bins,\n          apply_mask=False)\n\n      feature = tffeat.delta_delta(feature, order=2)\n      self.assertEqual(feature.eval().shape, (11, 40, 3))\n\n  def test_splice(self):\n    \'\'\' test batch splice frame \'\'\'\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      feat = tf.ones([1, 3, 2], dtype=tf.float32)\n\n      for l_ctx in range(0, 4):\n        for r_ctx in range(0, 4):\n          ctx = l_ctx + 1 + r_ctx\n          out = tffeat.splice(feat, left_context=l_ctx, right_context=r_ctx)\n          self.assertTupleEqual(out.eval().shape, (1, 3, 2 * ctx))\n          self.assertAllEqual(out, tf.ones([1, 3, 2 * ctx]))\n\n      with self.assertRaises(ValueError):\n        out = tffeat.splice(feat, left_context=-2, right_context=-2).eval()\n\n      with self.assertRaises(ValueError):\n        out = tffeat.splice(feat, left_context=2, right_context=-2).eval()\n\n      with self.assertRaises(ValueError):\n        out = tffeat.splice(feat, left_context=-2, right_context=2).eval()\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  tf.test.main()\n'"
delta/data/feat/tf_speech_feature.py,28,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' speech feature in tensorflow\'\'\'\nimport functools\n\nimport numpy as np\nimport delta.compat as tf\nimport scipy.signal\n#pylint: disable=ungrouped-imports,no-name-in-module\n\nfrom delta import utils\nfrom delta.utils.hparam import HParams\n\n\ndef add_delta_deltas(filterbanks, name=None):\n  """"""Compute time first and second-order derivative channels.\n  Args:\n    filterbanks: float32 tensor with shape [batch_size, len, num_bins, 1]\n    name: scope name\n  Returns:\n    float32 tensor with shape [batch_size, len, num_bins, 3]\n  """"""\n  delta_filter = np.array([2, 1, 0, -1, -2])\n  delta_delta_filter = scipy.signal.convolve(delta_filter, delta_filter, ""full"")\n  delta_filter_stack = np.array(\n      [[0] * 4 + [1] + [0] * 4, [0] * 2 + list(delta_filter) + [0] * 2,\n       list(delta_delta_filter)],\n      dtype=np.float32).T[:, None, None, :]\n\n  delta_filter_stack /= np.sqrt(\n      np.sum(delta_filter_stack**2, axis=0, keepdims=True))\n\n  filterbanks = tf.nn.conv2d(\n      filterbanks,\n      delta_filter_stack, [1, 1, 1, 1],\n      ""SAME"",\n      data_format=""NHWC"",\n      name=name)\n  return filterbanks\n\n\n#pylint: disable=too-many-arguments,too-many-locals\ndef compute_mel_filterbank_features(waveforms,\n                                    sample_rate=16000,\n                                    dither=1.0 / np.iinfo(np.int16).max,\n                                    preemphasis=0.97,\n                                    frame_length=25,\n                                    frame_step=10,\n                                    fft_length=None,\n                                    window_fn=functools.partial(\n                                        tf.signal.hann_window, periodic=True),\n                                    lower_edge_hertz=80.0,\n                                    upper_edge_hertz=7600.0,\n                                    num_mel_bins=80,\n                                    log_noise_floor=1e-3,\n                                    apply_mask=True):\n  """"""Implement mel-filterbank extraction using tf ops.\n  Args:\n    waveforms: float32 tensor with shape [batch_size, max_len]\n    sample_rate: sampling rate of the waveform\n    dither: stddev of Gaussian noise added to waveform to prevent quantization\n      artefacts\n    preemphasis: waveform high-pass filtering constant\n    frame_length: frame length in ms\n    frame_step: frame_Step in ms\n    fft_length: number of fft bins\n    window_fn: windowing function\n    lower_edge_hertz: lowest frequency of the filterbank\n    upper_edge_hertz: highest frequency of the filterbank\n    num_mel_bins: filterbank size\n    log_noise_floor: clip small values to prevent numeric overflow in log\n    apply_mask: When working on a batch of samples, set padding frames to zero\n  Returns:\n    filterbanks: a float32 tensor with shape [batch_size, len, num_bins, 1]\n  """"""\n  #  is a complex64 Tensor representing the short-time Fourier\n  # Transform of each signal in . Its shape is\n  # [batch_size, ?, fft_unique_bins]\n  # where fft_unique_bins = fft_length // 2 + 1\n\n  # Find the wave length: the largest index for which the value is !=0\n  # note that waveforms samples that are exactly 0.0 are quite common, so\n  # simply doing sum(waveforms != 0, axis=-1) will not work correctly.\n  wav_lens = tf.reduce_max(\n      tf.expand_dims(tf.range(tf.shape(waveforms)[1]), 0) *\n      tf.to_int32(tf.not_equal(waveforms, 0.0)),\n      axis=-1) + 1\n  if dither > 0:\n    waveforms += tf.random_normal(tf.shape(waveforms), stddev=dither)\n  if preemphasis > 0:\n    waveforms = waveforms[:, 1:] - preemphasis * waveforms[:, :-1]\n    wav_lens -= 1\n  frame_length = int(frame_length * sample_rate / 1e3)\n  frame_step = int(frame_step * sample_rate / 1e3)\n  if fft_length is None:\n    fft_length = int(2**(np.ceil(np.log2(frame_length))))\n\n  stfts = tf.signal.stft(\n      waveforms,\n      frame_length=frame_length,\n      frame_step=frame_step,\n      fft_length=fft_length,\n      window_fn=window_fn,\n      pad_end=True)\n\n  stft_lens = (wav_lens + (frame_step - 1)) // frame_step\n  masks = tf.to_float(\n      tf.less_equal(\n          tf.expand_dims(tf.range(tf.shape(stfts)[1]), 0),\n          tf.expand_dims(stft_lens, 1)))\n\n  # An energy spectrogram is the magnitude of the complex-valued STFT.\n  # A float32 Tensor of shape [batch_size, ?, 257].\n  magnitude_spectrograms = tf.abs(stfts)\n\n  # Warp the linear-scale, magnitude spectrograms into the mel-scale.\n  num_spectrogram_bins = magnitude_spectrograms.shape[-1].value\n  linear_to_mel_weight_matrix = (\n      tf.signal.linear_to_mel_weight_matrix(num_mel_bins, num_spectrogram_bins,\n                                            sample_rate, lower_edge_hertz,\n                                            upper_edge_hertz))\n  mel_spectrograms = tf.tensordot(magnitude_spectrograms,\n                                  linear_to_mel_weight_matrix, 1)\n  # Note: Shape inference for tensordot does not currently handle this case.\n  mel_spectrograms.set_shape(magnitude_spectrograms.shape[:-1].concatenate(\n      linear_to_mel_weight_matrix.shape[-1:]))\n\n  log_mel_sgram = tf.log(tf.maximum(log_noise_floor, mel_spectrograms))\n\n  if apply_mask:\n    log_mel_sgram *= tf.expand_dims(tf.to_float(masks), -1)\n\n  return tf.expand_dims(log_mel_sgram, -1, name=""mel_sgrams"")\n\n\ndef read_wav(wavfile, params):\n  \'\'\' samples of shape [nsample] \'\'\'\n  contents = tf.read_file(wavfile)\n  #pylint: disable=no-member\n  waveforms = tf.audio.decode_wav(\n      contents,\n      desired_channels=params.audio_channels,\n      #desired_samples=params.audio_sample_rate,\n  )\n  return tf.squeeze(waveforms.audio, axis=-1)\n\n\n#pylint: disable=invalid-name,too-many-arguments\ndef speech_params(sr=16000,\n                  bins=40,\n                  dither=True,\n                  use_delta_deltas=True,\n                  cmvn=False,\n                  cmvn_path=\'\'):\n  \'\'\' feat params \'\'\'\n  p = HParams()\n  p.add_hparam(""audio_sample_rate"", sr)\n  p.add_hparam(""audio_channels"", 1)\n  p.add_hparam(""audio_preemphasis"", 0.97)\n  if dither:\n    p.add_hparam(""audio_dither"", 1.0 / np.iinfo(np.int16).max)\n  else:\n    p.add_hparam(""audio_dither"", 0.0)\n  p.add_hparam(""audio_frame_length"", 25.0)\n  p.add_hparam(""audio_frame_step"", 10.0)\n  p.add_hparam(""audio_lower_edge_hertz"", 20.0)\n  p.add_hparam(""audio_upper_edge_hertz"", sr / 2.0)\n  p.add_hparam(""audio_num_mel_bins"", bins)\n  p.add_hparam(""audio_add_delta_deltas"", use_delta_deltas)\n  p.add_hparam(""num_zeropad_frames"", 0)\n  p.add_hparam(""audio_global_cmvn"", cmvn)\n  p.add_hparam(""audio_cmvn_path"", cmvn_path)\n  return p\n\n\n#pylint: disable=invalid-name\ndef extract_logfbank_with_delta(waveforms, params):\n  \'\'\' extract logfbank with delta detla \'\'\'\n  p = params\n  #waveforms = tf.expand_dims(waveforms, 0) # add batch_size dim\n  mel_fbanks = compute_mel_filterbank_features(\n      waveforms,\n      sample_rate=p.audio_sample_rate,\n      dither=p.audio_dither,\n      preemphasis=p.audio_preemphasis,\n      frame_length=p.audio_frame_length,\n      frame_step=p.audio_frame_step,\n      lower_edge_hertz=p.audio_lower_edge_hertz,\n      upper_edge_hertz=p.audio_upper_edge_hertz,\n      num_mel_bins=p.audio_num_mel_bins,\n      apply_mask=False)\n  if p.audio_add_delta_deltas:\n    mel_fbanks = add_delta_deltas(mel_fbanks)\n  # shape: [batch, nframes, nbins, nchannels]\n  return mel_fbanks\n\n\n#pylint: disable=invalid-name\ndef extract_feature(waveforms, params):\n  \'\'\'extract fbank with delta-delta and do cmvn\n     waveforms: [batch, samples]\n  \'\'\'\n  p = params\n  with tf.variable_scope(\'feature_extractor\'):\n    mel_fbanks = extract_logfbank_with_delta(waveforms, params)\n    # shape: [1, nframes, nbins, nchannels]\n    fbank_size = utils.shape_list(mel_fbanks)\n    #assert fbank_size[0] == 1\n\n    # This replaces CMVN estimation on data\n    if not p.audio_global_cmvn:\n      mean = tf.reduce_mean(mel_fbanks, keepdims=True, axis=1)\n      variance = tf.reduce_mean(\n          tf.square(mel_fbanks - mean), keepdims=True, axis=1)\n    else:\n      assert p.audio_cmvn_path, p.audio_cmvn_path\n      mean, variance = utils.load_cmvn(p.audio_cmvn_path)\n\n    var_epsilon = 1e-09\n    mel_fbanks = utils.apply_cmvn(mel_fbanks, mean, variance, var_epsilon)\n\n    # Later models like to flatten the two spatial dims. Instead, we add a\n    # unit spatial dim and flatten the frequencies and channels.\n    batch_size = fbank_size[0]\n    feats = tf.concat([\n        tf.reshape(mel_fbanks,\n                   [batch_size, fbank_size[1], fbank_size[2], fbank_size[3]]),\n        tf.zeros(\n            (batch_size, p.num_zeropad_frames, fbank_size[2], fbank_size[3]))\n    ], 1)\n  return feats  # shape [batch_size, nframes, featue_size, chnanels]\n'"
delta/data/feat/tf_speech_feature_test.py,5,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Tf speech feature unittest\'\'\'\nimport os\nfrom pathlib import Path\n\nimport delta.compat as tf\nfrom absl import logging\n\nfrom delta.data.feat.speech_feature import load_wav\nfrom delta.data.feat import tf_speech_feature as tffeat\nfrom delta import PACKAGE_ROOT_DIR\n\n\nclass SpeechFeatTest(tf.test.TestCase):\n  \'\'\' tf.signal feat unittest\'\'\'\n\n  def setUp(self):\n    super().setUp()\n    package_root = Path(PACKAGE_ROOT_DIR)\n    self.params = tffeat.speech_params(sr=8000, bins=40, cmvn=False)\n    self.wavpath = str(\n        package_root.joinpath(\'data/feat/python_speech_features/english.wav\'))\n    self.sr_true, self.audio_true = load_wav(str(self.wavpath), sr=8000)\n\n  def test_extract_feature(self):\n    \'\'\' test extract feature \'\'\'\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      wavfile = tf.constant(self.wavpath)\n\n      audio = tffeat.read_wav(wavfile, self.params)\n\n      # slice and tile to batch\n      audio = tf.stack([audio[:1000]] * 32)\n\n      feature = tffeat.extract_feature(audio, self.params)\n\n      self.assertEqual(audio.eval().shape, (32, 1000))\n      self.assertAllEqual(audio.eval()[0], self.audio_true[:1000])\n      self.assertEqual(feature.eval().shape, (32, 13, 40, 3))\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  tf.test.main()\n'"
delta/data/frontend/__init__.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' init of frontend package\'\'\'\n'"
delta/data/frontend/add_noise_end_to_end.py,1,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""This model adds noise/rir to signal and writes it to file.""""""\n\nimport delta.compat as tf\nfrom delta.utils.hparam import HParams\nfrom delta.data.frontend.read_wav import ReadWav\nfrom delta.data.frontend.add_rir_noise_aecres import Add_rir_noise_aecres\nfrom delta.data.frontend.write_wav import WriteWav\nfrom delta.data.frontend.base_frontend import BaseFrontend\n\n\nclass AddNoiseEndToEnd(BaseFrontend):\n  """"""\n  Add a random signal-to-noise ratio noise or impulse response to clean speech, and\n  write it to wavfile.\n  """"""\n\n  def __init__(self, config: dict):\n    super().__init__(config)\n    self.add_noise = Add_rir_noise_aecres(config)\n    self.read_wav = ReadWav(config)\n    self.write_wav = WriteWav(config)\n\n  @classmethod\n  def params(cls, config=None):\n    """"""\n        Set params.\n        :param config: contains ten optional parameters:\n            --sample_rate\t\t\t\t  : Sample frequency of waveform data. (int, default = 16000)\n            --if_add_rir          : If true, add rir to audio data. (bool, default = False)\n            --rir_filelist        : FileList path of rir.(string, default = \'rirlist.scp\')\n            --if_add_noise        : If true, add random noise to audio data. (bool, default = False)\n            --snr_min             : Minimum SNR adds to signal. (float, default = 0)\n            --snr_max             : Maximum SNR adds to signal. (float, default = 30)\n            --noise_filelist      : FileList path of noise.(string, default = \'noiselist.scp\')\n            --if_add_aecres       : If true, add aecres to audio data. (bool, default = False)\n            --aecres_filelist     : FileList path of aecres.(string, default = \'aecreslist.scp\')\n            --speed               : Speed of sample channels wanted. (float, default=1.0)\n        :return: An object of class HParams, which is a set of hyperparameters as name-value pairs.\n        """"""\n\n    sample_rate = 16000\n    if_add_rir = False\n    rir_filelist = \'rirlist.scp\'\n    if_add_noise = False\n    noise_filelist = \'noiselist.scp\'\n    snr_min = 0\n    snr_max = 30\n    if_add_aecres = False\n    aecres_filelist = \'aecreslist.scp\'\n    audio_channels = 1\n    speed = 1.0\n\n    hparams = HParams(cls=cls)\n    hparams.add_hparam(\'sample_rate\', sample_rate)\n    hparams.add_hparam(\'speed\', speed)\n    hparams.add_hparam(\'if_add_rir\', if_add_rir)\n    hparams.add_hparam(\'if_add_noise\', if_add_noise)\n    hparams.add_hparam(\'rir_filelist\', rir_filelist)\n    hparams.add_hparam(\'noise_filelist\', noise_filelist)\n    hparams.add_hparam(\'snr_min\', snr_min)\n    hparams.add_hparam(\'snr_max\', snr_max)\n    hparams.add_hparam(\'if_add_aecres\', if_add_aecres)\n    hparams.add_hparam(\'aecres_filelist\', aecres_filelist)\n    hparams.add_hparam(\'audio_channels\', audio_channels)\n\n    if config is not None:\n      hparams.override_from_dict(config)\n\n    return hparams\n\n  def call(self, in_wavfile, out_wavfile):\n    """"""\n        Read a clean wav return a noisy wav.\n        :param in_wavfile: clean wavfile path.\n        :param out_wavfile: noisy wavfile path.\n        :return: write wav opration.\n        """"""\n\n    with tf.name_scope(\'add_noise_end_to_end\'):\n      input_data, sample_rate = self.read_wav(in_wavfile)\n      noisy_data = self.add_noise(input_data, sample_rate) / 32768\n      write_op = self.write_wav(out_wavfile, noisy_data, sample_rate)\n\n    return write_op\n'"
delta/data/frontend/add_noise_end_to_end_test.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""The model tests OP of Add_noise_rir_end_to_end """"""\n\nimport os\nfrom pathlib import Path\nimport delta.compat as tf\nfrom delta.data.frontend.add_noise_end_to_end import AddNoiseEndToEnd\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = \'-1\'\nfrom core.ops import PACKAGE_OPS_DIR\n\n\ndef change_file_path(scp_path, filetype, newfilePath):\n  with open(scp_path + filetype, \'r\') as f:\n    s = f.readlines()\n  f.close()\n  with open(scp_path + newfilePath, \'w\') as f:\n    for line in s:\n      f.write(scp_path + line)\n  f.close()\n\n\nclass AddNoiseEndToEndTest(tf.test.TestCase):\n  """"""\n  AddNoiseEndToEnd OP test.\n  """"""\n\n  def test_add_noise_end_to_end(self):\n\n    wav_path = str(Path(PACKAGE_OPS_DIR).joinpath(\'data/sm1_cln.wav\'))\n\n    # reset path of noise && rir\n    data_path = str(Path(PACKAGE_OPS_DIR).joinpath(\'data\')) + \'/\'\n    noise_file = data_path + \'noiselist_new.scp\'\n    change_file_path(data_path, \'noiselist.scp\', \'noiselist_new.scp\')\n    rir_file = data_path + \'rirlist_new.scp\'\n    change_file_path(data_path, \'rirlist.scp\', \'rirlist_new.scp\')\n\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      config = {\n          \'if_add_noise\': True,\n          \'noise_filelist\': noise_file,\n          \'if_add_rir\': True,\n          \'rir_filelist\': rir_file\n      }\n      noisy_path = wav_path[:-4] + \'_noisy.wav\'\n      add_noise_end_to_end = AddNoiseEndToEnd.params(config).instantiate()\n      writewav_op = add_noise_end_to_end(wav_path, noisy_path)\n      sess.run(writewav_op)\n\n\nif __name__ == \'__main__\':\n\n  tf.test.main()\n'"
delta/data/frontend/add_rir_noise_aecres.py,7,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""This model adds noise/rir to signal.""""""\n\nimport delta.compat as tf\nfrom delta.utils.hparam import HParams\nfrom core.ops import py_x_ops\nfrom delta.data.frontend.base_frontend import BaseFrontend\n\n\nclass Add_rir_noise_aecres(BaseFrontend):\n  """"""\n  Add a random signal-to-noise ratio noise or impulse response to clean speech.\n  """"""\n\n  def __init__(self, config: dict):\n    super().__init__(config)\n\n  @classmethod\n  def params(cls, config=None):\n    """"""\n        Set params.\n        :param config: contains nine optional parameters:\n            --sample_rate\t\t\t\t  : Sample frequency of waveform data. (int, default = 16000)\n            --if_add_rir          : If true, add rir to audio data. (bool, default = False)\n            --rir_filelist        : FileList path of rir.(string, default = \'rirlist.scp\')\n            --if_add_noise        : If true, add random noise to audio data. (bool, default = False)\n            --snr_min             : Minimum SNR adds to signal. (float, default = 0)\n            --snr_max             : Maximum SNR adds to signal. (float, default = 30)\n            --noise_filelist      : FileList path of noise.(string, default = \'noiselist.scp\')\n            --if_add_aecres       : If true, add aecres to audio data. (bool, default = False)\n            --aecres_filelist     : FileList path of aecres.(string, default = \'aecreslist.scp\')\n        :return: An object of class HParams, which is a set of hyperparameters as name-value pairs.\n        """"""\n\n    sample_rate = 16000\n    if_add_rir = False\n    rir_filelist = \'rirlist.scp\'\n    if_add_noise = False\n    noise_filelist = \'noiselist.scp\'\n    snr_min = 0\n    snr_max = 30\n    if_add_aecres = False\n    aecres_filelist = \'aecreslist.scp\'\n\n    hparams = HParams(cls=cls)\n    hparams.add_hparam(\'sample_rate\', sample_rate)\n    hparams.add_hparam(\'if_add_rir\', if_add_rir)\n    hparams.add_hparam(\'if_add_noise\', if_add_noise)\n    hparams.add_hparam(\'rir_filelist\', rir_filelist)\n    hparams.add_hparam(\'noise_filelist\', noise_filelist)\n    hparams.add_hparam(\'snr_min\', snr_min)\n    hparams.add_hparam(\'snr_max\', snr_max)\n    hparams.add_hparam(\'if_add_aecres\', if_add_aecres)\n    hparams.add_hparam(\'aecres_filelist\', aecres_filelist)\n\n    if config is not None:\n      hparams.override_from_dict(config)\n\n    return hparams\n\n  def call(self, audio_data, sample_rate=None):\n    """"""\n        Caculate power spectrum or log power spectrum of audio data.\n        :param audio_data: the audio signal from which to compute spectrum.\n                          Should be an (1, N) tensor.\n        :param sample_rate: [option]the samplerate of the signal we working with,\n                           default is 16kHz.\n        :return: A float tensor of size N containing add-noise audio.\n        """"""\n\n    p = self.config\n    with tf.name_scope(\'add_rir_noise_aecres\'):\n      if sample_rate == None:\n        sample_rate = tf.constant(p.sample_rate, dtype=tf.int32)\n\n      assert_op = tf.assert_equal(\n          tf.constant(p.sample_rate), tf.cast(sample_rate, dtype=tf.int32))\n      with tf.control_dependencies([assert_op]):\n        sample_rate = tf.cast(sample_rate, dtype=float)\n        add_rir_noise_aecres_out = py_x_ops.add_rir_noise_aecres(\n            audio_data,\n            sample_rate,\n            if_add_rir=p.if_add_rir,\n            rir_filelist=p.rir_filelist,\n            if_add_noise=p.if_add_noise,\n            snr_min=p.snr_min,\n            snr_max=p.snr_max,\n            noise_filelist=p.noise_filelist,\n            if_add_aecres=p.if_add_aecres,\n            aecres_filelist=p.aecres_filelist)\n\n        return tf.squeeze(add_rir_noise_aecres_out)\n'"
delta/data/frontend/add_rir_noise_aecres_test.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""The model tests OP of Add_noise_rir """"""\n\nimport os\nfrom pathlib import Path\nimport delta.compat as tf\nfrom delta.data.frontend.read_wav import ReadWav\nfrom delta.data.frontend.write_wav import WriteWav\nfrom delta.data.frontend.add_rir_noise_aecres import Add_rir_noise_aecres\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = \'-1\'\nfrom core.ops import PACKAGE_OPS_DIR\n\n\ndef change_file_path(scp_path, filetype, newfilePath):\n  with open(scp_path + filetype, \'r\') as f:\n    s = f.readlines()\n  f.close()\n  with open(scp_path + newfilePath, \'w\') as f:\n    for line in s:\n      f.write(scp_path + line)\n  f.close()\n\n\nclass AddRirNoiseAecresTest(tf.test.TestCase):\n  """"""\n  AddNoiseRIR OP test.\n  """"""\n\n  def test_add_rir_noise_aecres(self):\n    wav_path = str(Path(PACKAGE_OPS_DIR).joinpath(\'data/sm1_cln.wav\'))\n\n    # reset path of noise && rir\n    data_path = str(Path(PACKAGE_OPS_DIR).joinpath(\'data\')) + \'/\'\n    noise_file = data_path + \'noiselist_new.scp\'\n    change_file_path(data_path, \'noiselist.scp\', \'noiselist_new.scp\')\n    rir_file = data_path + \'rirlist_new.scp\'\n    change_file_path(data_path, \'rirlist.scp\', \'rirlist_new.scp\')\n\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      read_wav = ReadWav.params().instantiate()\n      input_data, sample_rate = read_wav(wav_path)\n      config = {\n          \'if_add_noise\': True,\n          \'noise_filelist\': noise_file,\n          \'if_add_rir\': True,\n          \'rir_filelist\': rir_file\n      }\n      add_rir_noise_aecres = Add_rir_noise_aecres.params(config).instantiate()\n      add_rir_noise_aecres_test = add_rir_noise_aecres(input_data, sample_rate)\n      print(\'Clean Data:\', input_data.eval())\n      print(\'Noisy Data:\', add_rir_noise_aecres_test.eval())\n\n      new_noise_file = data_path + \'sm1_cln_noisy.wav\'\n      write_wav = WriteWav.params().instantiate()\n      writewav_op = write_wav(new_noise_file, add_rir_noise_aecres_test / 32768,\n                              sample_rate)\n      sess.run(writewav_op)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
delta/data/frontend/analyfiltbank.py,6,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""This model extracts power-spectrum && phase-spectrum features per frame.""""""\n\nimport delta.compat as tf\nfrom core.ops import py_x_ops\nfrom delta.utils.hparam import HParams\nfrom delta.data.frontend.base_frontend import BaseFrontend\n\n\nclass Analyfiltbank(BaseFrontend):\n  """"""\n  Compute power-spectrum && phase-spectrum features of every frame in speech,\n  return two float tensors with size (num_frames, num_frequencies).\n  """"""\n\n  def __init__(self, config: dict):\n    super().__init__(config)\n\n  @classmethod\n  def params(cls, config=None):\n    """"""\n    Set params.\n    :param config: contains three optional parameters:\n        --sample_rate       : Waveform data sample frequency (must match the waveform\n                             file, if specified there). (float, default = 16000)\n        --window_length\t\t : Window length in seconds. (float, default = 0.030)\n        --frame_length\t\t : Hop length in seconds. (float, default = 0.010)\n    :return: An object of class HParams, which is a set of hyperparameters as\n             name-value pairs.\n    """"""\n\n    window_length = 0.030\n    frame_length = 0.010\n    sample_rate = 16000\n\n    hparams = HParams(cls=cls)\n    hparams.add_hparam(\'window_length\', window_length)\n    hparams.add_hparam(\'frame_length\', frame_length)\n    hparams.add_hparam(\'sample_rate\', sample_rate)\n\n    if config is not None:\n      hparams.override_from_dict(config)\n\n    return hparams\n\n  def call(self, audio_data, sample_rate=None):\n    """"""\n    Caculate power spectrum and phase spectrum of audio data.\n    :param audio_data: the audio signal from which to compute spectrum.\n                      Should be an (1, N) tensor.\n    :param sample_rate: [option]the samplerate of the signal we working with,\n                        default is 16kHz.\n    :return: Two returns:\n        power spectrum \xe2\x80\x94\xe2\x80\x94 A float tensor of size (num_frames, num_frequencies)\n                          containing power spectrum and of every frame in speech.\n        phase spectrum \xe2\x80\x94\xe2\x80\x94 A float tensor of size (num_frames, num_frequencies)\n                          containing phase spectrum and of every frame in speech.\n    """"""\n\n    p = self.config\n    with tf.name_scope(\'analyfiltbank\'):\n\n      if sample_rate == None:\n        sample_rate = tf.constant(p.sample_rate, dtype=tf.int32)\n\n      assert_op = tf.assert_equal(\n          tf.constant(p.sample_rate), tf.cast(sample_rate, dtype=tf.int32))\n      with tf.control_dependencies([assert_op]):\n\n        sample_rate = tf.cast(sample_rate, dtype=float)\n        power_spectrum, phase_spectrum = py_x_ops.analyfiltbank(\n            audio_data,\n            sample_rate,\n            window_length=p.window_length,\n            frame_length=p.frame_length)\n\n        return power_spectrum, phase_spectrum\n'"
delta/data/frontend/analyfiltbank_test.py,4,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""The model tests Analyfiltbank FE.""""""\n\nfrom pathlib import Path\nimport numpy as np\n\nimport delta.compat as tf\nfrom core.ops import PACKAGE_OPS_DIR\nfrom delta.data.frontend.read_wav import ReadWav\nfrom delta.data.frontend.analyfiltbank import Analyfiltbank\n\n\nclass Test(tf.test.TestCase):\n  """"""\n  Analyfiltbank extraction test.\n  """"""\n\n  def test_analyfiltbank(self):\n    wav_path = str(Path(PACKAGE_OPS_DIR).joinpath(\'data/sm1_cln.wav\'))\n\n    with self.cached_session(use_gpu=False, force_gpu=False):\n\n      read_wav = ReadWav.params().instantiate()\n      audio_data, sample_rate = read_wav(wav_path)\n      audio_data = audio_data / 32768\n\n      analyfiltbank = Analyfiltbank.params().instantiate()\n      power_spc, phase_spc = analyfiltbank(audio_data.eval(),\n                                           sample_rate.eval())\n\n      power_spc_true = np.array(\n          [[\n              4.2182300e-04, 3.6964193e-04, 3.9906241e-05, 2.8196722e-05,\n              3.3976138e-04, 3.7671626e-04, 2.2727624e-04, 7.2495081e-05,\n              4.3451786e-05, 3.4654513e-06\n          ],\n           [\n               1.4681223e-05, 2.8831255e-05, 3.5616580e-05, 3.9359711e-05,\n               1.2714787e-04, 1.2794189e-04, 3.6509471e-05, 1.7578101e-05,\n               5.9672035e-05, 2.9785692e-06\n           ],\n           [\n               8.8715387e-05, 6.0998322e-05, 2.7695101e-05, 1.6866413e-04,\n               4.6845453e-05, 3.3532990e-05, 5.7005627e-06, 5.1852752e-05,\n               1.8390550e-05, 8.3459439e-05\n           ],\n           [\n               1.1405386e-05, 1.8942148e-06, 1.6338145e-06, 1.8362705e-05,\n               8.4106450e-06, 4.4174294e-06, 3.6533682e-05, 5.0541588e-05,\n               1.6701326e-06, 1.8736981e-05\n           ],\n           [\n               2.9108920e-05, 1.6862698e-05, 3.3437627e-05, 6.9332527e-05,\n               5.0028186e-05, 5.9426224e-05, 2.1895030e-06, 2.3780794e-06,\n               4.7786685e-05, 7.3811811e-05\n           ],\n           [\n               1.6433882e-05, 9.5777386e-07, 2.0980822e-06, 4.8990279e-07,\n               1.4232077e-05, 1.5986938e-05, 2.9042780e-05, 1.1719906e-05,\n               2.4548817e-06, 5.3594176e-06\n           ],\n           [\n               9.1289467e-06, 9.4249899e-06, 7.4781286e-07, 1.8923520e-05,\n               6.5740237e-06, 4.3209452e-06, 3.9396346e-06, 1.2287317e-05,\n               4.6807354e-06, 5.8512210e-06\n           ],\n           [\n               1.6150383e-05, 2.6649790e-05, 1.8610657e-05, 2.2872716e-06,\n               1.4209920e-05, 2.3279742e-06, 6.6038615e-06, 2.6169775e-05,\n               2.8335158e-05, 1.7595910e-06\n           ],\n           [\n               6.8095047e-05, 9.1859045e-05, 2.6713702e-05, 3.0580850e-05,\n               1.4539381e-05, 4.2510033e-05, 2.2579852e-05, 1.4843822e-05,\n               2.0883192e-05, 6.0624756e-05\n           ],\n           [\n               1.6092306e-05, 1.4245335e-05, 2.4250150e-05, 6.0177539e-05,\n               6.7926321e-06, 3.4922948e-07, 2.1843030e-06, 8.5554876e-07,\n               2.6831965e-06, 2.0012436e-05\n           ]])\n\n      phase_spc_true = np.array(\n          [[\n              3.1415927, 3.1415927, 3.1415927, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n              3.1415927\n          ],\n           [\n               0.01752237, 1.6688037, 1.4971976, 1.4470094, 2.0516894,\n               -2.3112175, -0.7115377, 2.9614341, -1.2494497, -0.7055688\n           ],\n           [\n               2.614648, 0.63351387, -2.0660093, 1.7626916, -1.1257634,\n               3.017448, -2.892095, -1.2209401, 1.7407895, -1.0281658\n           ],\n           [\n               1.02424, -1.8967879, -0.6139833, 2.587602, 3.0070715, 1.5781559,\n               -1.899145, -1.1459525, -0.24284656, -0.8106653\n           ],\n           [\n               -0.08220324, 0.5497215, 1.7031444, -2.8960562, -1.3680246,\n               0.4349923, 2.0676146, 1.2389332, 2.6312854, -1.7511902\n           ],\n           [\n               0.17763095, 2.7475302, -0.20671827, 1.0719725, -2.388657,\n               1.189566, -1.0643665, 2.5955305, -0.69036585, -0.5287417\n           ],\n           [\n               -0.9477449, -2.7059674, 0.53469753, 1.9289348, 0.24833842,\n               0.03517391, -1.4778724, -0.16577117, -1.7509687, -0.46875867\n           ],\n           [\n               1.5570146, -2.9596932, -0.7975963, 3.0060582, -1.038453,\n               0.14911443, -1.5873562, 0.7229206, 2.679422, -1.1890441\n           ],\n           [\n               -2.2543156, 0.47845784, -2.8412538, -0.5494534, 1.6583048,\n               -1.4567885, 1.0724461, -2.70243, -0.2690962, 1.8831034\n           ],\n           [\n               -0.32710192, 0.01503609, 0.29720783, -0.7409194, -2.183623,\n               2.3637679, 0.6405145, 1.4975713, 0.18241015, 2.2659144\n           ]])\n\n      self.assertEqual(tf.rank(power_spc).eval(), 2)\n      self.assertEqual(tf.rank(phase_spc).eval(), 2)\n      self.assertAllClose(power_spc.eval().transpose()[:10, :10],\n                          power_spc_true)\n      self.assertAllClose(phase_spc.eval().transpose()[:10, :10],\n                          phase_spc_true)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
delta/data/frontend/base_frontend.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' base interface of Frontend \'\'\'\n\nimport abc\nimport delta.compat as tf\n\nfrom delta.utils.hparam import HParams\n\n\nclass ABCFrontend(metaclass=abc.ABCMeta):\n  \'\'\' abstract of Frontend \'\'\'\n\n  def __init__(self, config):\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def call(self, *args, **kwargs):\n    \'\'\' implementation func \'\'\'\n    raise NotImplementedError()\n\n\nclass BaseFrontend(ABCFrontend):\n  \'\'\' wrapper of abstrcat Frontend\'\'\'\n\n  def __init__(self, config: dict):\n    self._config = config\n\n  @property\n  def config(self):\n    \'\'\' config property \'\'\'\n    return self._config\n\n  @classmethod\n  def params(cls, config=None):\n    \'\'\' set params \'\'\'\n    raise NotImplementedError()\n\n  def __call__(self, *args, **kwargs):\n    \'\'\' call \'\'\'\n    return self.call(*args, **kwargs)\n'"
delta/data/frontend/cepstrum.py,6,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""This model extracts Cepstrum features per frame.""""""\n\nimport delta.compat as tf\n\nfrom core.ops import py_x_ops\nfrom delta.utils.hparam import HParams\nfrom delta.data.frontend.base_frontend import BaseFrontend\n\n\nclass Cepstrum(BaseFrontend):\n  """"""\n  Compute Cepstrum features of every frame in speech, return a float tensor\n  with size (num_frames, ceps_subband_num).\n  """"""\n\n  def __init__(self, config: dict):\n    super().__init__(config)\n\n  @classmethod\n  def params(cls, config=None):\n    """"""\n    Set params.\n    :param config: contains five optional parameters:\n        --sample_rate       : Waveform data sample frequency (must match the waveform\n                            file, if specified there). (float, default = 16000)\n        --window_length\t\t : Window length in seconds. (float, default = 0.025)\n        --frame_length\t\t : Hop length in seconds. (float, default = 0.010)\n        --ceps_subband_num : Number of Ceps_subband. (int, default=13).\n        --tag_ceps_mean_norm : Flag of tag_ceps_mean_norm. (bool, default=True).\n    :return:An object of class HParams, which is a set of hyperparameters as\n            name-value pairs.\n    """"""\n\n    window_length = 0.025\n    frame_length = 0.010\n    ceps_subband_num = 13\n    tag_ceps_mean_norm = True\n    sample_rate = 16000\n\n    hparams = HParams(cls=cls)\n    hparams.add_hparam(\'window_length\', window_length)\n    hparams.add_hparam(\'frame_length\', frame_length)\n    hparams.add_hparam(\'ceps_subband_num\', ceps_subband_num)\n    hparams.add_hparam(\'tag_ceps_mean_norm\', tag_ceps_mean_norm)\n    hparams.add_hparam(\'sample_rate\', sample_rate)\n\n    if config is not None:\n      hparams.override_from_dict(config)\n\n    return hparams\n\n  def call(self, audio_data, sample_rate=None):\n    """"""\n    Caculate cepstrum of audio data.\n    :param audio_data: the audio signal from which to compute spectrum.\n                        Should be an (1, N) tensor.\n    :param sample_rate: [option]the samplerate of the signal we working with,\n                        default is 16kHz.\n    :return:A float tensor of size (num_frames, ceps_subband_num) containing\n            normalized cepstrum (tag_ceps_mean_norm = True) or cepstrum\n            (tag_ceps_mean_norm = False) of every frame in speech.\n    """"""\n\n    p = self.config\n\n    with tf.name_scope(\'cepstrum\'):\n\n      if sample_rate == None:\n        sample_rate = tf.constant(p.sample_rate, dtype=tf.int32)\n\n      assert_op = tf.assert_equal(\n          tf.constant(p.sample_rate), tf.cast(sample_rate, dtype=tf.int32))\n      with tf.control_dependencies([assert_op]):\n\n        sample_rate = tf.cast(sample_rate, dtype=float)\n        cepstrum = py_x_ops.cepstrum(\n            audio_data,\n            sample_rate,\n            window_length=p.window_length,\n            frame_length=p.frame_length,\n            ceps_subband_num=p.ceps_subband_num,\n            tag_ceps_mean_norm=p.tag_ceps_mean_norm)\n\n        return cepstrum\n'"
delta/data/frontend/cepstrum_test.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""The model tests Cepstrum FE.""""""\n\nimport numpy as np\nfrom pathlib import Path\n\nimport delta.compat as tf\nfrom core.ops import PACKAGE_OPS_DIR\nfrom delta.data.frontend.read_wav import ReadWav\nfrom delta.data.frontend.cepstrum import Cepstrum\n\n\nclass CepstrumTest(tf.test.TestCase):\n  """"""\n  Cepstrum extraction test.\n  """"""\n\n  def test_cepstrum(self):\n\n    wav_path = str(Path(PACKAGE_OPS_DIR).joinpath(\'data/sm1_cln.wav\'))\n\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      read_wav = ReadWav.params().instantiate()\n      input_data, sample_rate = read_wav.call(wav_path)\n      input_data = input_data / 32768\n      cepstrum = Cepstrum.params({\'window_length\': 0.025}).instantiate()\n      cepstrum_test = cepstrum(input_data, sample_rate)\n\n      output_true = np.array(\n          [[0.525808, 0.579537, 0.159656, 0.014726, -0.1866810],\n           [0.225988, 1.557304, 3.381828, 0.132935, 0.7128600],\n           [-1.832759, -1.045178, 0.753158, 0.116107, -0.9307780],\n           [-0.696277, 1.333355, 1.590942, 2.041829, -0.0805630],\n           [-0.377375, 2.984320, 0.036302, 3.676640, 1.1709290]])\n\n      # self.assertAllClose(cepstrum_test.eval()[15:20, 7:12], output_true)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
delta/data/frontend/cmvn.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""The model computes CMVN of features.""""""\n\nimport io\nimport kaldiio\nimport numpy as np\nfrom delta.utils.hparam import HParams\nfrom delta.data.frontend.base_frontend import BaseFrontend\n\n\nclass CMVN(BaseFrontend):\n  """"""\n  Compute and apply CMVN to features.\n  """"""\n\n  def __init__(self, config: dict):\n    super().__init__(config)\n\n  @classmethod\n  def params(cls, config=None):\n    """"""\n    Set params.\n    :param config: contains seven optional parameters:\n            --norm_means   : Flag of norm_means. (bool, default=True)\n            --norm_vars    : Flag of norm_vars. (bool, default=False)\n            --utt2spk      : Use for speaker CMVN. (string, default=None)\n            --spk2utt      : Rspecifier for speaker to utterance-list map.\n                            (string, default=None)\n            --reverse      : Flag of reverse. (bool, default=False)\n            --std_floor    : Floor to std. (float, default=1.0e-20)\n            --filetype     : Type of input file. (string, default=\'mat\')\n    :return:\n    """"""\n    norm_means = True\n    norm_vars = False\n    utt2spk = None\n    spk2utt = None\n    reverse = False\n    std_floor = 1.0e-20\n    filetype = \'mat\'\n\n    hparams = HParams(cls=cls)\n    hparams.add_hparam(\'norm_means\', norm_means)\n    hparams.add_hparam(\'norm_vars\', norm_vars)\n    hparams.add_hparam(\'utt2spk\', utt2spk)\n    hparams.add_hparam(\'spk2utt\', spk2utt)\n    hparams.add_hparam(\'reverse\', reverse)\n    hparams.add_hparam(\'std_floor\', std_floor)\n    hparams.add_hparam(\'filetype\', filetype)\n\n    if config is not None:\n      hparams.override_from_dict(config)\n\n    return hparams\n\n  def call(self, stats):\n    """"""\n    Do CMVN.\n    :param stats: Statistics of features.\n    :return: Mean and std of features.\n    """"""\n    p = self.config\n\n    if isinstance(stats, dict):\n      stats_dict = dict(stats)\n    else:\n      if p.filetype == \'mat\':\n        stats_dict = {None: kaldiio.load_mat(stats)}\n      elif p.filetype == \'ark\':\n        stats_dict = dict(kaldiio.load_ark(stats))\n      else:\n        raise ValueError(\'Not supporting filetype={}\'.format(p.filetype))\n\n    if p.utt2spk is not None:\n      self.utt2spk = {}\n      with io.open(p.utt2spk, \'r\', encoding=\'utf-8\') as f:\n        for line in f:\n          utt, spk = line.rstrip().split(None, 1)\n          self.utt2spk[utt] = spk\n\n    elif p.spk2utt is not None:\n      self.utt2spk = {}\n      with io.open(p.spk2utt, \'r\', encoding=\'utf-8\') as f:\n        for line in f:\n          spk, utts = line.rstrip().split(None, 1)\n          for utt in utts.split():\n            self.utt2spk[utt] = spk\n    else:\n      self.utt2spk = None\n\n    self.bias = {}\n    self.scale = {}\n    for spk, stats in stats_dict.items():\n      assert len(stats) == 2, stats.shape\n\n      count = stats[0, -1]\n\n      if not (np.isscalar(count) or isinstance(count, (int, float))):\n        count = count.flatten()[0]\n\n      mean = stats[0, :-1] / count\n      var = stats[1, :-1] / count - mean * mean\n      std = np.maximum(np.sqrt(var), p.std_floor)\n      self.bias[spk] = -mean\n      self.scale[spk] = 1 / std\n\n  def apply_cmvn(self, x, uttid):\n\n    p = self.config\n\n    if self.utt2spk is not None:\n      spk = self.utt2spk[uttid]\n    else:\n      # using global cmvn\n      spk = None\n\n    if not p.reverse:\n      if p.norm_means:\n        x = np.add(x, self.bias[spk])\n      if p.norm_vars:\n        x = np.multiply(x, self.scale[spk])\n    else:\n      if p.norm_means:\n        x = np.subtract(x, self.bias[spk])\n      if p.norm_vars:\n        x = np.divide(x, self.scale[spk])\n\n    return x\n'"
delta/data/frontend/delta_delta.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""This model does delta_delta to features.""""""\n\nimport delta.compat as tf\n\nfrom core.ops import py_x_ops\nfrom delta.utils.hparam import HParams\nfrom delta.data.frontend.base_frontend import BaseFrontend\n\n\nclass DeltaDelta(BaseFrontend):\n  """"""\n  Do Delta_delta to features.\n  """"""\n\n  def __init__(self, config: dict):\n    super().__init__(config)\n\n  @classmethod\n  def params(cls, config=None):\n\n    hparams = HParams(cls=cls)\n\n    return hparams\n\n  def call(self, feat, order, window):\n    """"""\n    Caculate delta of feats.\n    :param feat: a float tensor of size (num_frames, dim_feat).\n    :param order: an int.\n    :param window: an int.\n    :return: A tensor with shape (num_frames, dim_feats, order + 1),\n        containing delta of features of every frame in speech.\n    """"""\n\n    p = self.config\n    with tf.name_scope(\'delta_delta\'):\n      delta_delta = py_x_ops.delta_delta(feat, order, window)\n\n    n_frame, n_feats = feat.get_shape().as_list()\n    delta_delta = tf.reshape(delta_delta, (n_frame, n_feats, order + 1))\n\n    return delta_delta\n'"
delta/data/frontend/delta_delta_test.py,3,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""The model tests Delta_delta FE.""""""\n\nimport delta.compat as tf\nfrom delta.data.frontend.delta_delta import DeltaDelta\nimport numpy as np\nimport tempfile\nfrom kaldiio import WriteHelper\n\n\nclass Delta_delta_Test(tf.test.TestCase):\n  """"""\n  Delta_delta extraction test.\n  """"""\n\n  def test_delta_delta(self):\n\n    self.feat_dim = 80\n    self.data = np.arange(self.feat_dim, dtype=np.float32).reshape((8, 10))\n\n    # compute from kaldi `add-detlas` tools\n    self.output_true = np.array([\n        0.0000000e+00,\n        1.0000000e+00,\n        2.0000000e+00,\n        3.0000000e+00,\n        4.0000000e+00,\n        5.0000000e+00,\n        6.0000000e+00,\n        7.0000000e+00,\n        8.0000000e+00,\n        9.0000000e+00,\n        1.0000000e+01,\n        1.1000000e+01,\n        1.2000000e+01,\n        1.3000000e+01,\n        1.4000000e+01,\n        1.5000000e+01,\n        1.6000000e+01,\n        1.7000000e+01,\n        1.8000000e+01,\n        1.9000000e+01,\n        2.0000000e+01,\n        2.1000000e+01,\n        2.2000000e+01,\n        2.3000000e+01,\n        2.4000000e+01,\n        2.5000000e+01,\n        2.6000000e+01,\n        2.7000000e+01,\n        2.8000000e+01,\n        2.9000000e+01,\n        3.0000000e+01,\n        3.1000000e+01,\n        3.2000000e+01,\n        3.3000000e+01,\n        3.4000000e+01,\n        3.5000000e+01,\n        3.6000000e+01,\n        3.7000000e+01,\n        3.8000000e+01,\n        3.9000000e+01,\n        4.0000000e+01,\n        4.1000000e+01,\n        4.2000000e+01,\n        4.3000000e+01,\n        4.4000000e+01,\n        4.5000000e+01,\n        4.6000000e+01,\n        4.7000000e+01,\n        4.8000000e+01,\n        4.9000000e+01,\n        5.0000000e+01,\n        5.1000000e+01,\n        5.2000000e+01,\n        5.3000000e+01,\n        5.4000000e+01,\n        5.5000000e+01,\n        5.6000000e+01,\n        5.7000000e+01,\n        5.8000000e+01,\n        5.9000000e+01,\n        6.0000000e+01,\n        6.1000000e+01,\n        6.2000000e+01,\n        6.3000000e+01,\n        6.4000000e+01,\n        6.5000000e+01,\n        6.6000000e+01,\n        6.7000000e+01,\n        6.8000000e+01,\n        6.9000000e+01,\n        7.0000000e+01,\n        7.1000000e+01,\n        7.2000000e+01,\n        7.3000000e+01,\n        7.4000000e+01,\n        7.5000000e+01,\n        7.6000000e+01,\n        7.7000000e+01,\n        7.8000000e+01,\n        7.9000000e+01,\n        0.0000000e+00,\n        -1.4901161e-08,\n        -2.9802322e-08,\n        0.0000000e+00,\n        -5.9604645e-08,\n        0.0000000e+00,\n        0.0000000e+00,\n        1.1920929e-07,\n        -1.1920929e-07,\n        1.1920929e-07,\n        0.0000000e+00,\n        -2.3841858e-07,\n        0.0000000e+00,\n        2.3841858e-07,\n        2.3841858e-07,\n        0.0000000e+00,\n        -2.3841858e-07,\n        -2.3841858e-07,\n        2.3841858e-07,\n        2.3841858e-07,\n        0.0000000e+00,\n        4.7683716e-07,\n        -4.7683716e-07,\n        4.7683716e-07,\n        0.0000000e+00,\n        0.0000000e+00,\n        4.7683716e-07,\n        -4.7683716e-07,\n        4.7683716e-07,\n        -4.7683716e-07,\n        0.0000000e+00,\n        4.7683716e-07,\n        -4.7683716e-07,\n        4.7683716e-07,\n        -4.7683716e-07,\n        0.0000000e+00,\n        4.7683716e-07,\n        -4.7683716e-07,\n        4.7683716e-07,\n        -4.7683716e-07,\n        0.0000000e+00,\n        9.5367432e-07,\n        9.5367432e-07,\n        0.0000000e+00,\n        -9.5367432e-07,\n        0.0000000e+00,\n        9.5367432e-07,\n        9.5367432e-07,\n        0.0000000e+00,\n        -9.5367432e-07,\n        0.0000000e+00,\n        9.5367432e-07,\n        9.5367432e-07,\n        0.0000000e+00,\n        -9.5367432e-07,\n        0.0000000e+00,\n        9.5367432e-07,\n        9.5367432e-07,\n        -9.5367432e-07,\n        -9.5367432e-07,\n        0.0000000e+00,\n        9.5367432e-07,\n        9.5367432e-07,\n        -9.5367432e-07,\n        -9.5367432e-07,\n        0.0000000e+00,\n        9.5367432e-07,\n        9.5367432e-07,\n        -9.5367432e-07,\n        -9.5367432e-07,\n        0.0000000e+00,\n        9.5367432e-07,\n        9.5367432e-07,\n        -9.5367432e-07,\n        -9.5367432e-07,\n        0.0000000e+00,\n        9.5367432e-07,\n        9.5367432e-07,\n        -9.5367432e-07,\n        -9.5367432e-07,\n        0.0000000e+00,\n        0.0000000e+00,\n        0.0000000e+00,\n        0.0000000e+00,\n        0.0000000e+00,\n        5.9604645e-08,\n        0.0000000e+00,\n        5.9604645e-08,\n        0.0000000e+00,\n        0.0000000e+00,\n        1.1920929e-07,\n        5.9604645e-08,\n        0.0000000e+00,\n        0.0000000e+00,\n        1.1920929e-07,\n        0.0000000e+00,\n        0.0000000e+00,\n        2.3841858e-07,\n        0.0000000e+00,\n        2.3841858e-07,\n        2.3841858e-07,\n        0.0000000e+00,\n        1.1920929e-07,\n        2.3841858e-07,\n        0.0000000e+00,\n        2.3841858e-07,\n        0.0000000e+00,\n        0.0000000e+00,\n        2.3841858e-07,\n        0.0000000e+00,\n        0.0000000e+00,\n        0.0000000e+00,\n        0.0000000e+00,\n        0.0000000e+00,\n        4.7683716e-07,\n        0.0000000e+00,\n        0.0000000e+00,\n        4.7683716e-07,\n        4.7683716e-07,\n        2.3841858e-07,\n        4.7683716e-07,\n        4.7683716e-07,\n        0.0000000e+00,\n        0.0000000e+00,\n        2.3841858e-07,\n        0.0000000e+00,\n        4.7683716e-07,\n        2.3841858e-07,\n        0.0000000e+00,\n        4.7683716e-07,\n        4.7683716e-07,\n        9.5367432e-07,\n        0.0000000e+00,\n        4.7683716e-07,\n        0.0000000e+00,\n        4.7683716e-07,\n        4.7683716e-07,\n        4.7683716e-07,\n        0.0000000e+00,\n        4.7683716e-07,\n        0.0000000e+00,\n        4.7683716e-07,\n        0.0000000e+00,\n        4.7683716e-07,\n        0.0000000e+00,\n        4.7683716e-07,\n        0.0000000e+00,\n        4.7683716e-07,\n        9.5367432e-07,\n        4.7683716e-07,\n        0.0000000e+00,\n        4.7683716e-07,\n        0.0000000e+00,\n        4.7683716e-07,\n        9.5367432e-07,\n        4.7683716e-07,\n        9.5367432e-07,\n        0.0000000e+00,\n        4.7683716e-07,\n        4.7683716e-07,\n    ],\n                                dtype=np.float32)\n\n    with self.cached_session(use_gpu=False, force_gpu=False):\n\n      self.order = 2\n      self.window = 2\n      feat = tf.constant(self.data, dtype=tf.float32)\n      delta_delta = DeltaDelta.params().instantiate()\n      delta_delta_test = delta_delta(feat, self.order, self.window)\n\n      self.assertEqual(delta_delta_test.shape, (8, 10, self.order + 1))\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
delta/data/frontend/fbank.py,9,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""This model extracts Fbank features per frame.""""""\n\nimport tensorflow as tf\nfrom core.ops import py_x_ops\nfrom delta.utils.hparam import HParams\nfrom delta.data.frontend.base_frontend import BaseFrontend\nfrom delta.data.frontend.spectrum import Spectrum\n\n\nclass Fbank(BaseFrontend):\n  """"""\n  Computing filter banks is applying triangular filters on a Mel-scale to the power\n   spectrum to extract frequency bands. Return a float tensor with shape\n   (num_channels, num_frames, num_frequencies).\n  """"""\n\n  def __init__(self, config: dict):\n    super().__init__(config)\n    self.spect = Spectrum(config)\n\n  @classmethod\n  def params(cls, config=None):\n    """"""\n    Set params.\n    :param config: contains thirteen optional parameters:\n           --window_length\t\t\t\t: Window length in seconds. (float, default = 0.025)\n           --frame_length\t\t\t\t: Hop length in seconds. (float, default = 0.010)\n           --snip_edges\t\t\t\t: If true, the last frame (shorter than window_length) will be\n                                         cutoff. If ,false 1 // 2 frame_length data will be padded\n                                         to data. (bool, default = true)\n           ---raw_energy\t\t\t\t: If 1, compute frame energy before preemphasis and\n                                         windowing. If 2,  compute frame energy after\n                                         preemphasis and windowing. (int, default = 1)\n           --preeph_coeff\t\t\t\t: Coefficient for use in frame-signal preemphasis.\n                                        (float, default = 0.97)\n           --window_type\t\t\t\t: Type of window (""hamm""|""hann""|""povey""|""rect""|""blac""|""tria"").\n                                        (string, default = ""povey"")\n           --remove_dc_offset\t\t\t: Subtract mean from waveform on each frame.\n                                         (bool, default = true)\n           --is_fbank\t\t\t\t\t: If true, compute power spetrum without frame energy.\n                                         If false, using the frame energy instead of the\n                                         square of the constant component of the signal.\n                                         (bool, default = true)\n           --output_type\t\t\t\t: If 1, return power spectrum. If 2, return log-power\n                                         spectrum. (int, default = 1)\n           --upper_frequency_limit\t\t: High cutoff frequency for mel bins (if <= 0, offset\n                                        from Nyquist) (float, default = 0)\n           --lower_frequency_limit\t\t: Low cutoff frequency for mel bins (float, default = 20)\n           --filterbank_channel_count\t: Number of triangular mel-frequency bins.\n                                        (float, default = 23)\n           --dither\t\t\t    \t: Dithering constant (0.0 means no dither).\n                                        (float, default = 1) [add robust to training]\n    :return: An object of class HParams, which is a set of hyperparameters as name-value pairs.\n    """"""\n\n    upper_frequency_limit = 0.0\n    lower_frequency_limit = 20.0\n    filterbank_channel_count = 23.0\n    window_length = 0.025\n    frame_length = 0.010\n    output_type = 1\n    sample_rate = 16000\n    snip_edges = True\n    raw_energy = 1\n    preeph_coeff = 0.97\n    window_type = \'povey\'\n    remove_dc_offset = True\n    is_fbank = True\n    dither = 0.0\n\n    hparams = HParams(cls=cls)\n    hparams.add_hparam(\'upper_frequency_limit\', upper_frequency_limit)\n    hparams.add_hparam(\'lower_frequency_limit\', lower_frequency_limit)\n    hparams.add_hparam(\'filterbank_channel_count\', filterbank_channel_count)\n    hparams.add_hparam(\'window_length\', window_length)\n    hparams.add_hparam(\'frame_length\', frame_length)\n    hparams.add_hparam(\'output_type\', output_type)\n    hparams.add_hparam(\'sample_rate\', sample_rate)\n    hparams.add_hparam(\'snip_edges\', snip_edges)\n    hparams.add_hparam(\'raw_energy\', raw_energy)\n    hparams.add_hparam(\'preeph_coeff\', preeph_coeff)\n    hparams.add_hparam(\'window_type\', window_type)\n    hparams.add_hparam(\'remove_dc_offset\', remove_dc_offset)\n    hparams.add_hparam(\'is_fbank\', is_fbank)\n    hparams.add_hparam(\'dither\', dither)\n\n    if config is not None:\n      hparams.override_from_dict(config)\n\n    return hparams\n\n  def call(self, audio_data, sample_rate=None):\n    """"""\n       Caculate fbank features of audio data.\n       :param audio_data: the audio signal from which to compute spectrum.\n                          Should be an (1, N) tensor.\n       :param sample_rate: [option]the samplerate of the signal we working with,\n                            default is 16kHz.\n       :return: A float tensor of size (num_frames, num_frequencies, num_channels) containing\n               fbank features of every frame in speech.\n    """"""\n    p = self.config\n    with tf.name_scope(\'fbank\'):\n\n      if sample_rate == None:\n        sample_rate = tf.constant(p.sample_rate, dtype=tf.int32)\n\n      if p.upper_frequency_limit <= 0:\n        p.upper_frequency_limit = p.sample_rate / 2.0 + p.upper_frequency_limit\n      elif (p.upper_frequency_limit <= p.lower_frequency_limit) or (\n          p.upper_frequency_limit > p.sample_rate / 2.0):\n        p.upper_frequency_limit = p.sample_rate / 2.0\n\n      assert_op = tf.assert_equal(\n          tf.constant(p.sample_rate), tf.cast(sample_rate, dtype=tf.int32))\n      with tf.control_dependencies([assert_op]):\n\n        spectrum = self.spect(audio_data, sample_rate)\n        spectrum = tf.expand_dims(spectrum, 0)\n\n        fbank = py_x_ops.fbank(\n            spectrum,\n            sample_rate,\n            upper_frequency_limit=p.upper_frequency_limit,\n            lower_frequency_limit=p.lower_frequency_limit,\n            filterbank_channel_count=p.filterbank_channel_count)\n\n        fbank = tf.squeeze(fbank, axis=0)\n        shape = tf.shape(fbank)\n        nframe = shape[0]\n        nfbank = shape[1]\n        fbank = tf.reshape(fbank, (nframe, nfbank, 1))\n\n        return fbank\n'"
delta/data/frontend/fbank_pitch.py,8,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""This model extracts Fbank && Pitch features per frame.""""""\n\nimport delta.compat as tf\nfrom delta.utils.hparam import HParams\nfrom delta.data.frontend.base_frontend import BaseFrontend\nfrom delta.data.frontend.pitch import Pitch\nfrom delta.data.frontend.fbank import Fbank\n\n\nclass FbankPitch(BaseFrontend):\n  """"""\n  Compute Fbank && Pitch features respectively\xef\xbc\x8cand concate them. Return\n  a tensor with shape (num_frames, dim_features).\n  """"""\n\n  def __init__(self, config: dict):\n    super().__init__(config)\n    self.fbank = Fbank(config)\n    self.pitch = Pitch(config)\n\n  @classmethod\n  def params(cls, config=None):\n    """"""\n    Set params.\n    :param config: contains twenty-nine optional parameters:\n          --sample_rate         : Samplerate of the signal we working with.\n                                  (int, default = 16000)\n          --window_length\t\t    : Window length in seconds. (float, default = 0.025)\n          --frame_length\t\t\t  : Hop length in seconds. (float, default = 0.010)\n          --snip_edges\t\t\t\t  : If true, the last frame (shorter than window_length) will\n                                        be cutoff. If false, 1 // 2 frame_length data will be padded\n                                         to data. (bool, default = true)\n          ---raw_energy\t\t\t\t  : If 1, compute frame energy before preemphasis and\n                                        windowing. If 2,  compute frame energy after preemphasis\n                                         and windowing. (int, default = 1)\n          --preEph_coeff\t\t\t  : Coefficient for use in frame-signal preemphasis.\n                                        (float, default = 0.97)\n          --window_type\t\t\t\t  : Type of window (""hamm""|""hann""|""povey""|""rect""|""blac""|""tria"").\n                                        (string, default = ""povey"")\n          --remove_dc_offset\t      : Subtract mean from waveform on each frame.\n                                        (bool, default = true)\n          --is_fbank\t\t\t\t  : If true, compute power spetrum without frame\n                                        energy. If false, using the frame energy instead\n                                         of the square of the constant component of the\n                                         signal. (bool, default = true)\n          --output_type\t\t\t\t  : If 1, return power spectrum. If 2, return\n                                        log-power spectrum. (int, default = 1)\n          --upper_frequency_limit\t  : High cutoff frequency for mel bins.\n                                        (if <= 0, offset from Nyquist) (float, default = 0)\n          --lower_frequency_limit\t  : Low cutoff frequency for mel bins.\n                                        (float, default = 20)\n          --filterbank_channel_count  : Number of triangular mel-frequency bins.\n                                        (float, default = 23)\n          --dither\t\t\t    \t  : Dithering constant (0.0 means no dither).\n                                        (float, default = 1)\n            [add robust to training]\n          --delta-pitch               : Smallest relative change in pitch that our\n                                        algorithm measures. (float, default = 0.005)\n          --frames-per-chunk          : Only relevant for offline pitch extraction.\n                                        (e.g. compute-kaldi-pitch-feats), you can set it to a\n                                        small nonzero value, such as 10, for better feature\n                                        compatibility with online decoding (affects energy\n                                        normalization in the algorithm) (int, default = 0)\n          --lowpass-cutoff            : cutoff frequency for LowPass filter (Hz).\n                                        (float, default = 1000)\n          --lowpass-filter-width      : Integer that determines filter width of lowpass filter,\n                                        more gives sharper filter (int, default = 1)\n          --max-f0                    : max. F0 to search for (Hz) (float, default = 400)\n          --max-frames-latency        : Maximum number of frames of latency that we allow pitch\n                                        tracking to introduce into the feature processing\n                                        (affects output only if --frames-per-chunk > 0 and\n                                        --simulate-first-pass-online=true (int, default = 0)\n          --min-f0                    : min. F0 to search for (Hz) (float, default = 50)\n          --nccf-ballast              : Increasing this factor reduces NCCF for quiet frames.\n                                        (float, default = 7000)\n          --nccf-ballast-online       : This is useful mainly for debug; it affects how the\n                                        NCCF ballast is computed. (bool, default = false)\n          --penalty-factor            : cost factor for FO change. (float, default = 0.1)\n          --preemphasis-coefficient   : Coefficient for use in signal preemphasis (deprecated)\n                                        (float, default = 0)\n          --recompute-frame           : Only relevant for online pitch extraction, or for\n                                        compatibility with online pitch extraction.  A\n                                        non-critical parameter; the frame at which we recompute\n                                        some of the forward pointers, after revising our\n                                        estimate of the signal energy. Relevant\n                                        if--frames-per-chunk > 0. (int, default = 500)\n          --resample-frequency        : Frequency that we down-sample the signal to. Must be\n                                        more than twice lowpass-cutoff (float, default = 4000)\n          --simulate-first-pass-online : If true, compute-kaldi-pitch-feats will output features\n                                         that correspond to what an online decoder would see in\n                                         the first pass of decoding-- not the final version of\n                                         the features, which is the default.  Relevant if\n                                         --frames-per-chunk > 0 (bool, default = false)\n          --soft-min-f0               : Minimum f0, applied in soft way, must not exceed\n                                        min-f0 (float, default = 10)\n          --upsample-filter-width     : Integer that determines filter width when upsampling\n                                        NCCF (int, default = 5)\n    :return: An object of class HParams, which is a set of hyperparameters as name-value pairs.\n    """"""\n    hparams = HParams(cls=cls)\n\n    upper_frequency_limit = 0\n    lower_frequency_limit = 20.0\n    filterbank_channel_count = 23.0\n    window_length = 0.025\n    frame_length = 0.010\n    raw_energy = 1\n    preeph_coeff = 0.97\n    window_type = \'povey\'\n    remove_dc_offset = True\n    is_fbank = True\n    output_type = 1\n    dither = 0.0\n    sample_rate = 16000\n    snip_edges = True\n    preemph_coeff = 0.0\n    min_f0 = 50.0\n    max_f0 = 400.0\n    soft_min_f0 = 10.0\n    penalty_factor = 0.1\n    lowpass_cutoff = 1000.0\n    resample_freq = 4000.0\n    delta_pitch = 0.005\n    nccf_ballast = 7000.0\n    lowpass_filter_width = 1\n    upsample_filter_width = 5\n    max_frames_latency = 0\n    frames_per_chunk = 0\n    simulate_first_pass_online = False\n    recompute_frame = 500\n    nccf_ballast_online = False\n\n    hparams.add_hparam(\'sample_rate\', sample_rate)\n    hparams.add_hparam(\'snip_edges\', snip_edges)\n    hparams.add_hparam(\'preemph_coeff\', preemph_coeff)\n    hparams.add_hparam(\'dither\', dither)\n    hparams.add_hparam(\'min_f0\', min_f0)\n    hparams.add_hparam(\'max_f0\', max_f0)\n    hparams.add_hparam(\'soft_min_f0\', soft_min_f0)\n    hparams.add_hparam(\'penalty_factor\', penalty_factor)\n    hparams.add_hparam(\'lowpass_cutoff\', lowpass_cutoff)\n    hparams.add_hparam(\'resample_freq\', resample_freq)\n    hparams.add_hparam(\'delta_pitch\', delta_pitch)\n    hparams.add_hparam(\'nccf_ballast\', nccf_ballast)\n    hparams.add_hparam(\'lowpass_filter_width\', lowpass_filter_width)\n    hparams.add_hparam(\'upsample_filter_width\', upsample_filter_width)\n    hparams.add_hparam(\'max_frames_latency\', max_frames_latency)\n    hparams.add_hparam(\'frames_per_chunk\', frames_per_chunk)\n    hparams.add_hparam(\'simulate_first_pass_online\', simulate_first_pass_online)\n    hparams.add_hparam(\'recompute_frame\', recompute_frame)\n    hparams.add_hparam(\'nccf_ballast_online\', nccf_ballast_online)\n    hparams.add_hparam(\'upper_frequency_limit\', upper_frequency_limit)\n    hparams.add_hparam(\'lower_frequency_limit\', lower_frequency_limit)\n    hparams.add_hparam(\'filterbank_channel_count\', filterbank_channel_count)\n    hparams.add_hparam(\'window_length\', window_length)\n    hparams.add_hparam(\'frame_length\', frame_length)\n    hparams.add_hparam(\'output_type\', output_type)\n    hparams.add_hparam(\'raw_energy\', raw_energy)\n    hparams.add_hparam(\'preeph_coeff\', preeph_coeff)\n    hparams.add_hparam(\'window_type\', window_type)\n    hparams.add_hparam(\'remove_dc_offset\', remove_dc_offset)\n    hparams.add_hparam(\'is_fbank\', is_fbank)\n\n    if config is not None:\n      hparams.override_from_dict(config)\n\n    return hparams\n\n  def call(self, audio_data, sample_rate=None):\n    """"""\n    Caculate fbank && pitch(concat) features of wav.\n    :param audio_data: the audio signal from which to compute spectrum.\n                       Should be an (1, N) tensor.\n    :param sample_rate: the samplerate of the signal we working with.\n    :return: A tensor with shape (num_frames, dim_features), containing\n            fbank && pitch feature of every frame in speech.\n    """"""\n\n    p = self.config\n    with tf.name_scope(\'fbank_pitch\'):\n\n      if sample_rate == None:\n        sample_rate = tf.constant(p.sample_rate, dtype=tf.int32)\n\n      assert_op = tf.assert_equal(\n          tf.constant(p.sample_rate), tf.cast(sample_rate, dtype=tf.int32))\n      with tf.control_dependencies([assert_op]):\n\n        fbank_feats = tf.squeeze(self.fbank(audio_data, sample_rate))\n        pitch_feats = tf.squeeze(self.pitch(audio_data, sample_rate))\n        fbank_pitch_feats = tf.concat([fbank_feats, pitch_feats], 1)\n\n        return fbank_pitch_feats\n'"
delta/data/frontend/fbank_pitch_test.py,3,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""The model tests Fbank&&Pitch FE.""""""\n\nimport delta.compat as tf\nimport os\nfrom pathlib import Path\nfrom delta.data.frontend.read_wav import ReadWav\nfrom delta.data.frontend.fbank_pitch import FbankPitch\nfrom core.ops import PACKAGE_OPS_DIR\n\n\nclass FbankPitchTest(tf.test.TestCase):\n  """"""\n  Compare Fbank&&Pitch FE with kaldi.\n  """"""\n\n  def test_FbankPitch(self):\n    wav_path = str(Path(PACKAGE_OPS_DIR).joinpath(\'data/sm1_cln.wav\'))\n\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      read_wav = ReadWav.params().instantiate()\n      input_data, sample_rate = read_wav(wav_path)\n      config = {\'window_length\': 0.025, \'output_type\': 1, \'frame_length\': 0.010}\n      fbank_pitch = FbankPitch.params(config).instantiate()\n      fbank_pitch_test = fbank_pitch(input_data, sample_rate)\n\n      self.assertEqual(tf.rank(fbank_pitch_test).eval(), 2)\n      print(fbank_pitch_test.eval()[0:2])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
delta/data/frontend/fbank_test.py,3,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""The model tests Fbank FE.""""""\n\nimport os\nimport numpy as np\nfrom pathlib import Path\n\nimport delta.compat as tf\nfrom core.ops import PACKAGE_OPS_DIR\nfrom delta.data.frontend.read_wav import ReadWav\nfrom delta.data.frontend.fbank import Fbank\n\n\nclass FbankTest(tf.test.TestCase):\n  """"""\n  Test Fbank FE using 8k/16k wav files.\n  """"""\n\n  def test_fbank(self):\n    wav_path = str(Path(PACKAGE_OPS_DIR).joinpath(\'data/sm1_cln.wav\'))\n\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      read_wav = ReadWav.params().instantiate()\n      input_data, sample_rate = read_wav(wav_path)\n      config = {\n          \'window_length\': 0.025,\n          \'output_type\': 1,\n          \'frame_length\': 0.010,\n          \'snip_edges\': True\n      }\n      fbank = Fbank.params(config).instantiate()\n      fbank_test = fbank(input_data, sample_rate)\n\n      self.assertEqual(tf.rank(fbank_test).eval(), 3)\n\n      real_fank_feats = np.array(\n          [[3.768338, 4.946218, 6.289874, 6.330853, 6.761764, 6.884573],\n           [3.803553, 5.450971, 6.547878, 5.796172, 6.397846, 7.242926]])\n\n      self.assertAllClose(\n          np.squeeze(fbank_test.eval()[0:2, 0:6, 0]),\n          real_fank_feats,\n          rtol=1e-05,\n          atol=1e-05)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
delta/data/frontend/framepow.py,7,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""""This model extracts framepow features per frame.""""""\n\nimport delta.compat as tf\n\nfrom core.ops import py_x_ops\nfrom delta.utils.hparam import HParams\nfrom delta.data.frontend.base_frontend import BaseFrontend\n\n\nclass Framepow(BaseFrontend):\n  """"""\n  Compute power of every frame in speech. Return a float tensor with\n  shape (1 * num_frames).\n  """"""\n\n  def __init__(self, config: dict):\n    super().__init__(config)\n\n  @classmethod\n  def params(cls, config=None):\n    """"""\n    Set params.\n    :param config: contains five optional parameters:\n        --sample_rate       : Waveform data sample frequency (must match the waveform\n                             file, if specified there). (float, default = 16000)\n        --window_length\t\t : Window length in seconds. (float, default = 0.025)\n        --frame_length\t\t : Hop length in seconds. (float, default = 0.010)\n        --snip_edges\t\t\t : If True, the last frame (shorter than window_length)\n                              will be cutoff. If False, 1 // 2 frame_length data will\n                              be padded to data. (int, default = True)\n        --remove_dc_offset : Subtract mean from waveform on each frame (bool, default = true)\n    :return:An object of class HParams, which is a set of hyperparameters as name-value pairs.\n    """"""\n\n    window_length = 0.025\n    frame_length = 0.010\n    snip_edges = True\n    remove_dc_offset = True\n    sample_rate = 16000\n\n    hparams = HParams(cls=cls)\n    hparams.add_hparam(\'window_length\', window_length)\n    hparams.add_hparam(\'frame_length\', frame_length)\n    hparams.add_hparam(\'snip_edges\', snip_edges)\n    hparams.add_hparam(\'remove_dc_offset\', remove_dc_offset)\n    hparams.add_hparam(\'sample_rate\', sample_rate)\n\n    if config is not None:\n      hparams.override_from_dict(config)\n\n    return hparams\n\n  def call(self, audio_data, sample_rate=None):\n    """"""\n    Caculate power of every frame in speech.\n    :param audio_data: the audio signal from which to compute spectrum.\n                       Should be an (1, N) tensor.\n    :param sample_rate: [option]the samplerate of the signal we working with,\n                        default is 16kHz.\n    :return:A float tensor of size (1 * num_frames) containing power of every\n            frame in speech.\n    """"""\n\n    p = self.config\n    with tf.name_scope(\'framepow\'):\n\n      if sample_rate == None:\n        sample_rate = tf.constant(p.sample_rate, dtype=tf.int32)\n\n      assert_op = tf.assert_equal(\n          tf.constant(p.sample_rate), tf.cast(sample_rate, dtype=tf.int32))\n      with tf.control_dependencies([assert_op]):\n\n        sample_rate = tf.cast(sample_rate, dtype=float)\n        framepow = py_x_ops.frame_pow(\n            audio_data,\n            sample_rate,\n            snip_edges=p.snip_edges,\n            remove_dc_offset=p.remove_dc_offset,\n            window_length=p.window_length,\n            frame_length=p.frame_length)\n\n        return tf.squeeze(framepow)\n'"
delta/data/frontend/framepow_test.py,3,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""The model tests framepow FE.""""""\n\nimport os\nimport numpy as np\nfrom pathlib import Path\n\nimport delta.compat as tf\nfrom core.ops import PACKAGE_OPS_DIR\nfrom delta.data.frontend.read_wav import ReadWav\nfrom delta.data.frontend.framepow import Framepow\n\n\nclass FramepowTest(tf.test.TestCase):\n  """"""\n  Framepow extraction test.\n  """"""\n\n  def test_framepow(self):\n    wav_path = str(Path(PACKAGE_OPS_DIR).joinpath(\'data/sm1_cln.wav\'))\n\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      read_wav = ReadWav.params().instantiate()\n      input_data, sample_rate = read_wav(wav_path)\n\n      framepow = Framepow.params({\n          \'window_length\': 0.025,\n          \'frame_length\': 0.010\n      }).instantiate()\n      framepow_test = framepow(input_data, sample_rate)\n\n      real_framepow_feats = np.array(\n          [9.819611, 9.328745, 9.247337, 9.26451, 9.266059])\n\n      self.assertEqual(tf.rank(framepow_test).eval(), 1)\n      self.assertAllClose(framepow_test.eval()[0:5], real_framepow_feats)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
delta/data/frontend/mfcc.py,8,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""This model extracts MFCC features per frame.""""""\n\nimport delta.compat as tf\nfrom core.ops import py_x_ops\nfrom delta.utils.hparam import HParams\nfrom delta.data.frontend.base_frontend import BaseFrontend\nfrom delta.data.frontend.fbank import Fbank\nfrom delta.data.frontend.framepow import Framepow\nimport copy\n\n\nclass Mfcc(BaseFrontend):\n  """"""\n  Compute mfcc features of every frame in speech, return a float tensor\n  with size (num_channels, num_frames, num_frequencies).\n  """"""\n\n  def __init__(self, config: dict):\n    super().__init__(config)\n    self.framepow = Framepow(config)\n    self.fbank = Fbank(config)\n\n  @classmethod\n  def params(cls, config=None):\n    """"""\n    Set params.\n    :param config: contains fourteen optional parameters.\n        --window_length\t\t\t\t: Window length in seconds. (float, default = 0.025)\n        --frame_length\t\t\t\t: Hop length in seconds. (float, default = 0.010)\n        --snip_edges\t\t\t\t: If True, the last frame (shorter than window_length) will\n                              be cutoff. If False, 1 // 2 frame_length data will be padded\n                              to data. (bool, default = True)\n        ---raw_energy\t\t\t\t: If 1, compute frame energy before preemphasis and\n                                      windowing. If 2, compute frame energy after\n                                      preemphasis and windowing. (int, default = 1)\n        --preEph_coeff\t\t\t    : Coefficient for use in frame-signal preemphasis.\n                                      (float, default = 0.97)\n        --window_type\t\t\t\t: Type of window (""hamm""|""hann""|""povey""|""rect""|""blac""|""tria"").\n                                      (string, default = ""povey"")\n        --remove_dc_offset\t\t    : Subtract mean from waveform on each frame\n                                      (bool, default = true)\n        --is_fbank\t\t\t\t\t: If true, compute power spetrum without frame energy. If\n                                      false, using the frame energy instead of the square of the\n                                      constant component of the signal. (bool, default = true)\n        --output_type\t\t\t\t: If 1, return power spectrum. If 2, return log-power\n                                      spectrum. (int, default = 1)\n        --upper_frequency_limit\t\t: High cutoff frequency for mel bins (if < 0, offset from\n                                      Nyquist) (float, default = 0)\n        --lower_frequency_limit\t\t: Low cutoff frequency for mel bins (float, default = 20)\n        --filterbank_channel_count\t: Number of triangular mel-frequency bins.\n                                     (float, default = 23)\n        --coefficient_count         : Number of cepstra in MFCC computation.\n                                     (int, default = 13)\n        --cepstral_lifter           : Constant that controls scaling of MFCCs.\n                                     (float, default = 22)\n        --use_energy                :Use energy (not C0) in MFCC computation.\n                                     (bool, default = True)\n    :return: An object of class HParams, which is a set of hyperparameters as name-value pairs.\n    """"""\n\n    upper_frequency_limit = 0.0\n    lower_frequency_limit = 20.0\n    filterbank_channel_count = 23.0\n    window_length = 0.025\n    frame_length = 0.010\n    output_type = 1\n    sample_rate = 16000\n    snip_edges = True\n    raw_energy = 1\n    preeph_coeff = 0.97\n    window_type = \'povey\'\n    remove_dc_offset = True\n    is_fbank = True\n    cepstral_lifter = 22.0\n    coefficient_count = 13\n    use_energy = True\n    dither = 0.0\n\n    hparams = HParams(cls=cls)\n    hparams.add_hparam(\'upper_frequency_limit\', upper_frequency_limit)\n    hparams.add_hparam(\'lower_frequency_limit\', lower_frequency_limit)\n    hparams.add_hparam(\'filterbank_channel_count\', filterbank_channel_count)\n    hparams.add_hparam(\'window_length\', window_length)\n    hparams.add_hparam(\'frame_length\', frame_length)\n    hparams.add_hparam(\'output_type\', output_type)\n    hparams.add_hparam(\'sample_rate\', sample_rate)\n    hparams.add_hparam(\'snip_edges\', snip_edges)\n    hparams.add_hparam(\'raw_energy\', raw_energy)\n    hparams.add_hparam(\'preeph_coeff\', preeph_coeff)\n    hparams.add_hparam(\'window_type\', window_type)\n    hparams.add_hparam(\'remove_dc_offset\', remove_dc_offset)\n    hparams.add_hparam(\'is_fbank\', is_fbank)\n    hparams.add_hparam(\'cepstral_lifter\', cepstral_lifter)\n    hparams.add_hparam(\'coefficient_count\', coefficient_count)\n    hparams.add_hparam(\'use_energy\', use_energy)\n    hparams.add_hparam(\'dither\', dither)\n\n    if config is not None:\n      hparams.override_from_dict(config)\n\n    return hparams\n\n  def call(self, audio_data, sample_rate=None):\n    """"""\n    Caculate mfcc features of audio data.\n    :param audio_data: the audio signal from which to compute spectrum.\n                       Should be an (1, N) tensor.\n    :param sample_rate: the samplerate of the signal we working with.\n    :return: A float tensor of size (num_channels, num_frames, num_frequencies)\n            containing mfcc features of every frame in speech.\n    """"""\n    p = self.config\n    with tf.name_scope(\'mfcc\'):\n\n      if sample_rate == None:\n        sample_rate = tf.constant(p.sample_rate, dtype=tf.int32)\n\n      assert_op = tf.assert_equal(\n          tf.constant(p.sample_rate), tf.cast(sample_rate, dtype=tf.int32))\n      with tf.control_dependencies([assert_op]):\n\n        fbank_feats = self.fbank(audio_data, sample_rate)\n        sample_rate = tf.cast(sample_rate, dtype=tf.int32)\n        shape = tf.shape(fbank_feats)\n        nframe = shape[0]\n        nfbank = shape[1]\n        fbank_feats = tf.reshape(fbank_feats, (1, nframe, nfbank))\n        framepow_feats = self.framepow(audio_data, sample_rate)\n        mfcc = py_x_ops.mfcc(\n            fbank_feats,\n            framepow_feats,\n            sample_rate,\n            use_energy=p.use_energy,\n            cepstral_lifter=p.cepstral_lifter,\n            coefficient_count=p.coefficient_count)\n        return mfcc\n'"
delta/data/frontend/mfcc_test.py,3,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""The model tests MFCC FE.""""""\n\nimport delta.compat as tf\nimport os\nfrom pathlib import Path\nfrom delta.data.frontend.read_wav import ReadWav\nfrom delta.data.frontend.mfcc import Mfcc\nimport numpy as np\nfrom core.ops import PACKAGE_OPS_DIR\n\n\nclass MfccTest(tf.test.TestCase):\n  """"""\n  MFCC extraction test.\n  """"""\n\n  def test_mfcc(self):\n    wav_path = str(Path(PACKAGE_OPS_DIR).joinpath(\'data/sm1_cln.wav\'))\n\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      read_wav = ReadWav.params().instantiate()\n      input_data, sample_rate = read_wav(wav_path)\n      config = {\'use_energy\': True}\n      mfcc = Mfcc.params(config).instantiate()\n      mfcc_test = mfcc(input_data, sample_rate)\n\n      self.assertEqual(tf.rank(mfcc_test).eval(), 3)\n\n      real_mfcc_feats = np.array(\n          [[9.819611, -30.58736, -7.088838, -10.67966, -1.646479, -4.36086],\n           [9.328745, -30.73371, -6.128432, -7.930599, 3.208357, -1.086456]])\n\n      self.assertAllClose(\n          np.squeeze(mfcc_test.eval()[0, 0:2, 0:6]),\n          real_mfcc_feats,\n          rtol=1e-05,\n          atol=1e-05)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
delta/data/frontend/pitch.py,4,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""This model extracts pitch features per frame.""""""\n\nimport delta.compat as tf\nfrom core.ops import py_x_ops\nfrom delta.utils.hparam import HParams\nfrom delta.data.frontend.base_frontend import BaseFrontend\n\n\nclass Pitch(BaseFrontend):\n  """"""\n  Compute pitch features of every frame in speech, return a float tensor\n  with size (num_frames, 2).\n  """"""\n\n  def __init__(self, config: dict):\n    super().__init__(config)\n\n  @classmethod\n  def params(cls, config=None):\n    """"""\n    Set params.\n    :param config: contains nineteen optional parameters:\n          --sample_rate               : Waveform data sample frequency (must match the waveform\n                                        file, if specified there). (float, default = 16000)\n          --delta-pitch               : Smallest relative change in pitch that our algorithm\n                                        measures (float, default = 0.005)\n          --window_length             : Frame length in seconds (float, default = 0.025)\n          --frame_length              : Frame shift in seconds (float, default = 0.010)\n          --frames-per-chunk          : Only relevant for offline pitch extraction (e.g.\n                                        compute-kaldi-pitch-feats), you can set it to a small\n                                        nonzero value, such as 10, for better feature\n                                        compatibility with online decoding (affects energy\n                                        normalization in the algorithm) (int, default = 0)\n          --lowpass-cutoff            : cutoff frequency for LowPass filter (Hz).\n                                        (float, default = 1000)\n          --lowpass-filter-width      : Integer that determines filter width of lowpass filter,\n                                        more gives sharper filter (int, default = 1)\n          --max-f0                    : max. F0 to search for (Hz) (float, default = 400)\n          --max-frames-latency        : Maximum number of frames of latency that we allow pitch\n                                        tracking to introduce into the feature processing\n                                        (affects output only if --frames-per-chunk > 0 and\n                                        --simulate-first-pass-online=true (int, default = 0)\n          --min-f0                    : min. F0 to search for (Hz) (float, default = 50)\n          --nccf-ballast              : Increasing this factor reduces NCCF for quiet frames.\n                                        (float, default = 7000)\n          --nccf-ballast-online       : This is useful mainly for debug; it affects how the NCCF\n                                        ballast is computed. (bool, default = false)\n          --penalty-factor            : cost factor for FO change. (float, default = 0.1)\n          --preemphasis-coefficient   : Coefficient for use in signal preemphasis (deprecated).\n                                        (float, default = 0)\n          --recompute-frame           : Only relevant for online pitch extraction, or for\n                                        compatibility with online pitch extraction.  A\n                                        non-critical parameter; the frame at which we recompute\n                                        some of the forward pointers, after revising our\n                                        estimate of the signal energy.  Relevant\n                                        if--frames-per-chunk > 0. (int, default = 500)\n          --resample-frequency        : Frequency that we down-sample the signal to.  Must be\n                                        more than twice lowpass-cutoff (float, default = 4000)\n          --simulate-first-pass-online : If true, compute-kaldi-pitch-feats will output features\n                                        that correspond to what an online decoder would see in\n                                        the first pass of decoding-- not the final version of\n                                        the features, which is the default.  Relevant if\n                                        --frames-per-chunk > 0 (bool, default = false)\n          --snip-edges                : If this is set to false, the incomplete frames near the\n                                        ending edge won\'t be snipped, so that the number of\n                                        frames is the file size divided by the frame-shift.\n                                        This makes different types of features give the same\n                                        number of frames. (bool, default = true)\n          --soft-min-f0               : Minimum f0, applied in soft way, must not exceed min-f0.\n                                        (float, default = 10)\n          --upsample-filter-width     : Integer that determines filter width when upsampling\n                                        NCCF. (int, default = 5)\n    :return: An object of class HParams, which is a set of hyperparameters as name-value pairs.\n    """"""\n\n    hparams = HParams(cls=cls)\n    window_length = 0.025\n    frame_length = 0.010\n    sample_rate = 16000\n    snip_edges = True\n    preemph_coeff = 0.0\n    min_f0 = 50.0\n    max_f0 = 400.0\n    soft_min_f0 = 10.0\n    penalty_factor = 0.1\n    lowpass_cutoff = 1000.0\n    resample_freq = 4000.0\n    delta_pitch = 0.005\n    nccf_ballast = 7000.0\n    lowpass_filter_width = 1\n    upsample_filter_width = 5\n    max_frames_latency = 0\n    frames_per_chunk = 0\n    simulate_first_pass_online = False\n    recompute_frame = 500\n    nccf_ballast_online = False\n\n    hparams.add_hparam(\'window_length\', window_length)\n    hparams.add_hparam(\'frame_length\', frame_length)\n    hparams.add_hparam(\'sample_rate\', sample_rate)\n    hparams.add_hparam(\'snip_edges\', snip_edges)\n    hparams.add_hparam(\'preemph_coeff\', preemph_coeff)\n    hparams.add_hparam(\'min_f0\', min_f0)\n    hparams.add_hparam(\'max_f0\', max_f0)\n    hparams.add_hparam(\'soft_min_f0\', soft_min_f0)\n    hparams.add_hparam(\'penalty_factor\', penalty_factor)\n    hparams.add_hparam(\'lowpass_cutoff\', lowpass_cutoff)\n    hparams.add_hparam(\'resample_freq\', resample_freq)\n    hparams.add_hparam(\'delta_pitch\', delta_pitch)\n    hparams.add_hparam(\'nccf_ballast\', nccf_ballast)\n    hparams.add_hparam(\'lowpass_filter_width\', lowpass_filter_width)\n    hparams.add_hparam(\'upsample_filter_width\', upsample_filter_width)\n    hparams.add_hparam(\'max_frames_latency\', max_frames_latency)\n    hparams.add_hparam(\'frames_per_chunk\', frames_per_chunk)\n    hparams.add_hparam(\'simulate_first_pass_online\', simulate_first_pass_online)\n    hparams.add_hparam(\'recompute_frame\', recompute_frame)\n    hparams.add_hparam(\'nccf_ballast_online\', nccf_ballast_online)\n\n    if config is not None:\n      hparams.override_from_dict(config)\n\n    return hparams\n\n  def call(self, audio_data, sample_rate=None):\n    """"""\n    Caculate picth features of audio data.\n    :param audio_data: the audio signal from which to compute spectrum.\n                      Should be an (1, N) tensor.\n    :param sample_rate: the samplerate of the signal we working with.\n    :return: A float tensor of size (num_frames, 2) containing\n           pitch && POV features of every frame in speech.\n    """"""\n    p = self.config\n\n    with tf.name_scope(\'pitch\'):\n\n      if sample_rate is None:\n        sample_rate = tf.constant(p.sample_rate, dtype=tf.int32)\n      else:\n        if not tf.is_tensor(sample_rate):\n          sample_rate = tf.convert_to_tensor(sample_rate)\n\n      pitch = py_x_ops.pitch(\n          audio_data,\n          sample_rate,\n          window_length=p.window_length,\n          frame_length=p.frame_length,\n          snip_edges=p.snip_edges,\n          preemph_coeff=p.preemph_coeff,\n          min_f0=p.min_f0,\n          max_f0=p.max_f0,\n          soft_min_f0=p.soft_min_f0,\n          penalty_factor=p.penalty_factor,\n          lowpass_cutoff=p.lowpass_cutoff,\n          resample_freq=p.resample_freq,\n          delta_pitch=p.delta_pitch,\n          nccf_ballast=p.nccf_ballast,\n          lowpass_filter_width=p.lowpass_filter_width,\n          upsample_filter_width=p.upsample_filter_width,\n          max_frames_latency=p.max_frames_latency,\n          frames_per_chunk=p.frames_per_chunk,\n          simulate_first_pass_online=p.simulate_first_pass_online,\n          recompute_frame=p.recompute_frame,\n          nccf_ballast_online=p.nccf_ballast_online)\n\n      return pitch\n'"
delta/data/frontend/pitch_test.py,3,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""The model tests pitch FE.""""""\n\nimport delta.compat as tf\nimport os\nfrom pathlib import Path\nfrom delta.data.frontend.read_wav import ReadWav\nfrom delta.data.frontend.pitch import Pitch\nimport numpy as np\nfrom core.ops import PACKAGE_OPS_DIR\n\n\nclass SpectrumTest(tf.test.TestCase):\n  """"""\n  Pitch extraction test.\n  """"""\n\n  def test_spectrum(self):\n    wav_path = str(Path(PACKAGE_OPS_DIR).joinpath(\'data/sm1_cln.wav\'))\n\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      read_wav = ReadWav.params().instantiate()\n      input_data, sample_rate = read_wav(wav_path)\n\n      pitch = Pitch.params({\n          \'window_length\': 0.025,\n          \'soft_min_f0\': 10.0\n      }).instantiate()\n      pitch_test = pitch(input_data, sample_rate)\n\n      self.assertEqual(tf.rank(pitch_test).eval(), 2)\n\n      output_true = [[-0.1366025, 143.8855], [-0.0226383, 143.8855],\n                     [-0.08464742, 143.8855], [-0.08458386, 143.8855],\n                     [-0.1208689, 143.8855]]\n\n      self.assertAllClose(\n          pitch_test.eval()[0:5, :], output_true, rtol=1e-05, atol=1e-05)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
delta/data/frontend/plp.py,6,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""This model extracts PLP features per frame.""""""\n\nimport delta.compat as tf\n\nfrom core.ops import py_x_ops\nfrom delta.utils.hparam import HParams\nfrom delta.data.frontend.base_frontend import BaseFrontend\n\n\nclass Plp(BaseFrontend):\n  """"""\n  Compute PLP features of every frame in speech, return a float tensor\n  with size (num_frames, plp_order + 1).\n  """"""\n\n  def __init__(self, config: dict):\n    super().__init__(config)\n\n  @classmethod\n  def params(cls, config=None):\n    """"""\n    Set params.\n    :param config: contains four optional parameters:\n        --sample_rate       : Waveform data sample frequency (must match the waveform\n                             file, if specified there). (float, default = 16000)\n        --window_length\t\t : Window length in seconds. (float, default = 0.025)\n        --frame_length\t\t : Hop length in seconds. (float, default = 0.010)\n        --plp_order        : Plp order. (int, default=12).\n    :return:An object of class HParams, which is a set of hyperparameters as name-value pairs.\n    """"""\n\n    window_length = 0.025\n    frame_length = 0.010\n    plp_order = 12\n    sample_rate = 16000\n\n    hparams = HParams(cls=cls)\n    hparams.add_hparam(\'window_length\', window_length)\n    hparams.add_hparam(\'frame_length\', frame_length)\n    hparams.add_hparam(\'plp_order\', plp_order)\n    hparams.add_hparam(\'sample_rate\', sample_rate)\n\n    if config is not None:\n      hparams.override_from_dict(config)\n\n    return hparams\n\n  def call(self, audio_data, sample_rate=None):\n    """"""\n    Caculate plp features of audio data.\n    :param audio_data: the audio signal from which to compute spectrum.\n                       Should be an (1, N) tensor.\n    :param sample_rate: [option]the samplerate of the signal we working\n                        with, default is 16kHz.\n    :return:A float tensor of size (num_frames, (plp_order + 1)) containing plp\n            features of every frame in speech.\n    """"""\n\n    p = self.config\n    with tf.name_scope(\'plp\'):\n\n      if sample_rate == None:\n        sample_rate = tf.constant(p.sample_rate, dtype=tf.int32)\n\n      assert_op = tf.assert_equal(\n          tf.constant(p.sample_rate), tf.cast(sample_rate, dtype=tf.int32))\n      with tf.control_dependencies([assert_op]):\n\n        sample_rate = tf.cast(sample_rate, dtype=float)\n        plp = py_x_ops.plp(\n            audio_data,\n            sample_rate,\n            window_length=p.window_length,\n            frame_length=p.frame_length,\n            plp_order=p.plp_order)\n        return plp\n'"
delta/data/frontend/plp_test.py,3,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""The model tests PLP FE.""""""\n\nimport delta.compat as tf\nimport os\nfrom pathlib import Path\nimport numpy as np\nfrom delta.data.frontend.read_wav import ReadWav\nfrom delta.data.frontend.plp import Plp\nfrom core.ops import PACKAGE_OPS_DIR\n\n\nclass PlpTest(tf.test.TestCase):\n  """"""\n  Plp extraction test.\n  """"""\n\n  def test_plp(self):\n    wav_path = str(Path(PACKAGE_OPS_DIR).joinpath(\'data/sm1_cln.wav\'))\n\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      read_wav = ReadWav.params().instantiate()\n      input_data, sample_rate = read_wav(wav_path)\n      input_data = input_data / 32768\n\n      plp = Plp.params({\n          \'window_length\': 0.025,\n          \'frame_length\': 0.010,\n          \'plp_order\': 12\n      }).instantiate()\n      plp_test = plp(input_data, sample_rate)\n\n      output_true = np.array(\n          [[-0.209490, -0.326126, 0.010536, -0.027167, -0.117118],\n           [-0.020293, -0.454695, -0.104243, 0.001560, -0.234854],\n           [-0.015118, -0.444044, -0.156695, -0.086221, -0.319310],\n           [-0.031856, -0.130708, 0.047435, -0.089916, -0.160247],\n           [0.052763, -0.271487, 0.011329, 0.025320, 0.012851]])\n\n      self.assertEqual(tf.rank(plp_test).eval(), 2)\n      # Because the povey window is used instead of the hamming window in spectrum.\n      self.assertAllClose(\n          plp_test.eval()[50:55, 5:10], output_true, rtol=1e-02, atol=1e-02)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
delta/data/frontend/read_wav.py,15,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""The model reads audio sample from wav file.""""""\n\nimport delta.compat as tf\nfrom delta.utils.hparam import HParams\nfrom delta.data.frontend.base_frontend import BaseFrontend\nfrom core.ops import py_x_ops\n\n\nclass ReadWav(BaseFrontend):\n  """"""\n      Read audio sample from wav file, return sample data and sample rate.\n      """"""\n\n  def __init__(self, config: dict):\n    super().__init__(config)\n\n  @classmethod\n  def params(cls, config=None):\n    """"""\n    Set params.\n    :param config: contains three optional parameters:\n          --sample_rate       : Waveform data sample frequency (must match the waveform\n                                file, if specified there). (float, default = 16000)\n          --speed             : Speed of sample channels wanted. (float, default=1.0)\n          --audio_channels    :(int, default=1).\n    :return: An object of class HParams, which is a set of hyperparameters as\n            name-value pairs.\n    """"""\n    audio_channels = 1\n    sample_rate = 16000\n    speed = 1.0\n\n    hparams = HParams(cls=cls)\n    hparams.add_hparam(\'audio_channels\', audio_channels)\n    hparams.add_hparam(\'sample_rate\', sample_rate)\n    hparams.add_hparam(\'speed\', speed)\n\n    if config is not None:\n      hparams.override_from_dict(config)\n\n    return hparams\n\n  def call(self, wavfile):\n    """"""\n    Get audio data and sample rate from a wavfile.\n    :param wavfile: filepath of wav.\n    :return: 2 values. The first is a Tensor of audio data.\n        The second return value isthe sample rate of the input wav\n        file, which is a tensor with float dtype.\n    """"""\n    p = self.config\n    contents = tf.io.read_file(wavfile)\n    audio_data, sample_rate = tf.audio.decode_wav(\n        contents, desired_channels=p.audio_channels)\n    assert_op = tf.assert_equal(\n        tf.constant(p.sample_rate), tf.cast(sample_rate, dtype=tf.int32))\n\n    with tf.control_dependencies([assert_op]):\n\n      if p.speed == 1.0:\n        return tf.squeeze(\n            audio_data * 32768, axis=-1), tf.cast(\n                sample_rate, dtype=tf.int32)\n      else:\n        resample_rate = tf.cast(\n            sample_rate, dtype=tf.float32) * tf.cast(\n                1.0 / p.speed, dtype=tf.float32)\n        speed_data = py_x_ops.speed(\n            tf.squeeze(audio_data * 32768, axis=-1),\n            tf.cast(sample_rate, dtype=tf.int32),\n            tf.cast(resample_rate, dtype=tf.int32),\n            lowpass_filter_width=5)\n        return tf.squeeze(speed_data), tf.cast(sample_rate, dtype=tf.int32)\n'"
delta/data/frontend/read_wav_test.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""The model tests OP of read_wav """"""\n\nimport delta.compat as tf\nfrom pathlib import Path\nimport librosa\nfrom delta.data.frontend.read_wav import ReadWav\nfrom core.ops import PACKAGE_OPS_DIR\n\n\nclass ReadWavTest(tf.test.TestCase):\n  """"""\n  ReadWav OP test.\n  """"""\n\n  def test_read_wav(self):\n    wav_path = str(Path(PACKAGE_OPS_DIR).joinpath(\'data/sm1_cln.wav\'))\n\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      config = {\'speed\': 1.0}\n      read_wav = ReadWav.params(config).instantiate()\n      audio_data, sample_rate = read_wav(wav_path)\n      audio_data_true, sample_rate_true = librosa.load(wav_path, sr=16000)\n      if (config[\'speed\'] == 1.0):\n        self.assertAllClose(audio_data.eval() / 32768, audio_data_true)\n        self.assertAllClose(sample_rate.eval(), sample_rate_true)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
delta/data/frontend/spectrum.py,6,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""This model extracts spetrum features per frame.""""""\n\nimport tensorflow as tf\nfrom core.ops import py_x_ops\nfrom delta.utils.hparam import HParams\nfrom delta.data.frontend.base_frontend import BaseFrontend\n\n\nclass Spectrum(BaseFrontend):\n  """"""\n  Compute spectrum features of every frame in speech, return a float tensor\n  with size (num_frames, num_frequencies).\n  """"""\n\n  def __init__(self, config: dict):\n    super().__init__(config)\n\n  @classmethod\n  def params(cls, config=None):\n    """"""\n    Set params.\n    :param config: contains nine optional parameters\xef\xbc\x9a\n          --sample_rate     : Waveform data sample frequency (must match the waveform\n                              file, if specified there). (float, default = 16000)\n          --window_length\t\t: Window length in seconds. (float, default = 0.025)\n          --frame_length\t\t: Hop length in seconds. (float, default = 0.010)\n          --snip_edges\t\t\t: If True, the last frame (shorter than window_length)\n                                  will be cutoff. If False, 1 // 2 frame_length data will\n                                  be padded to data. (bool, default = True)\n          ---raw_energy\t\t\t: If 1, compute frame energy before preemphasis and windowing.\n                                  If 2,  compute frame energy after preemphasis and windowing.\n                                  (int, default = 1)\n          --preeph_coeff\t\t: Coefficient for use in frame-signal preemphasis.\n                                 (float, default = 0.97)\n          --window_type\t\t\t: Type of window (""hamm""|""hann""|""povey""|""rect""|""blac""|""tria"").\n                                  (string, default = ""povey"")\n          --remove_dc_offset\t: Subtract mean from waveform on each frame.\n                                 (bool, default = true)\n          --is_fbank\t\t\t: If true, compute power spetrum without frame energy.\n                                  If false, using the frame energy instead of the square of the\n                                  constant component of the signal. (bool, default = false)\n          --output_type\t\t\t: If 1, return power spectrum. If 2, return log-power spectrum.\n                                  (int, default = 2)\n          --dither\t\t        : Dithering constant (0.0 means no dither).\n                                 (float, default = 1) [add robust to training]\n    :return: An object of class HParams, which is a set of hyperparameters as name-value pairs.\n    """"""\n\n    window_length = 0.025\n    frame_length = 0.010\n    output_type = 2\n    sample_rate = 16000\n    snip_edges = True\n    raw_energy = 1\n    preeph_coeff = 0.97\n    window_type = \'povey\'\n    remove_dc_offset = True\n    is_fbank = False\n    dither = 0.0\n\n    hparams = HParams(cls=cls)\n    hparams.add_hparam(\'window_length\', window_length)\n    hparams.add_hparam(\'frame_length\', frame_length)\n    hparams.add_hparam(\'output_type\', output_type)\n    hparams.add_hparam(\'sample_rate\', sample_rate)\n    hparams.add_hparam(\'snip_edges\', snip_edges)\n    hparams.add_hparam(\'raw_energy\', raw_energy)\n    hparams.add_hparam(\'preeph_coeff\', preeph_coeff)\n    hparams.add_hparam(\'window_type\', window_type)\n    hparams.add_hparam(\'remove_dc_offset\', remove_dc_offset)\n    hparams.add_hparam(\'is_fbank\', is_fbank)\n    hparams.add_hparam(\'dither\', dither)\n\n    if config is not None:\n      hparams.override_from_dict(config)\n\n    return hparams\n\n  def call(self, audio_data, sample_rate=None):\n    """"""\n    Caculate power spectrum or log power spectrum of audio data.\n    :param audio_data: the audio signal from which to compute spectrum.\n                       Should be an (1, N) tensor.\n    :param sample_rate: [option]the samplerate of the signal we working with, default is 16kHz.\n    :return: A float tensor of size (num_frames, num_frequencies) containing power\n            spectrum (output_type=1) or log power spectrum (output_type=2)\n            of every frame in speech.\n    """"""\n\n    p = self.config\n    with tf.name_scope(\'spectrum\'):\n\n      if sample_rate == None:\n        sample_rate = tf.constant(p.sample_rate, dtype=tf.int32)\n\n      assert_op = tf.assert_equal(\n          tf.constant(p.sample_rate), tf.cast(sample_rate, dtype=tf.int32))\n      with tf.control_dependencies([assert_op]):\n\n        sample_rate = tf.cast(sample_rate, dtype=float)\n        spectrum = py_x_ops.spectrum(\n            audio_data,\n            sample_rate,\n            window_length=p.window_length,\n            frame_length=p.frame_length,\n            output_type=p.output_type,\n            snip_edges=p.snip_edges,\n            raw_energy=p.raw_energy,\n            preEph_coeff=p.preeph_coeff,\n            window_type=p.window_type,\n            remove_dc_offset=p.remove_dc_offset,\n            is_fbank=p.is_fbank,\n            dither=p.dither)\n\n        return spectrum\n'"
delta/data/frontend/spectrum_test.py,3,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""The model tests spectrum FE.""""""\n\nimport os\nimport numpy as np\nfrom pathlib import Path\nimport delta.compat as tf\nfrom core.ops import PACKAGE_OPS_DIR\nfrom delta.data.frontend.read_wav import ReadWav\nfrom delta.data.frontend.spectrum import Spectrum\n\n\nclass SpectrumTest(tf.test.TestCase):\n  \'\'\'\n  Spectum extraction test.\n  \'\'\'\n\n  def test_spectrum(self):\n    wav_path = str(Path(PACKAGE_OPS_DIR).joinpath(\'data/sm1_cln.wav\'))\n\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      read_wav = ReadWav.params().instantiate()\n      input_data, sample_rate = read_wav(wav_path)\n\n      spectrum = Spectrum.params({\n          \'window_length\': 0.025,\n          \'snip_edges\': True,\n          \'dither\': 0.0\n      }).instantiate()\n      spectrum_test = spectrum(input_data, sample_rate)\n\n      output_true = np.array(\n          [[9.819611, 2.84503, 3.660894, 2.7779, 1.212233],\n           [9.328745, 2.553949, 3.276319, 3.000918, 2.499342]])\n\n      self.assertEqual(tf.rank(spectrum_test).eval(), 2)\n      self.assertAllClose(\n          spectrum_test.eval()[0:2, 0:5], output_true, rtol=1e-05, atol=1e-05)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
delta/data/frontend/synthfiltbank.py,5,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport delta.compat as tf\nfrom core.ops import py_x_ops\nfrom delta.utils.hparam import HParams\nfrom delta.data.frontend.base_frontend import BaseFrontend\n\n\nclass Synthfiltbank(BaseFrontend):\n\n  def __init__(self, config: dict):\n    super().__init__(config)\n\n  @classmethod\n  def params(cls, config=None):\n    """"""\n    Set params.\n    :param config: contains three optional parameters:window_length(float, default=0.030),\n          frame_length(float, default=0.010), sample_rate(float, default=16000).\n    :return:An object of class HParams, which is a set of hyperparameters as name-value pairs.\n    """"""\n    window_length = 0.030\n    frame_length = 0.010\n    sample_rate = 16000\n\n    hparams = HParams(cls=cls)\n    hparams.add_hparam(\'window_length\', window_length)\n    hparams.add_hparam(\'frame_length\', frame_length)\n    hparams.add_hparam(\'sample_rate\', sample_rate)\n\n    if config is not None:\n      hparams.override_from_dict(config)\n\n    return hparams\n\n  def call(self, power_spectrum, phase_spectrum, sample_rate=None):\n    """"""\n    Implement frequency domain to time domain conversion.\n    :param power_spectrum: a float tensor of size (num_frames, num_frequencies).\n    :param phase_spectrum: a float tensor of size (num_frames, num_frequencies).\n    :param sample_rate: a scalar tensor.\n    :return: audio data\n    """"""\n\n    p = self.config\n    with tf.name_scope(\'synthfiltbank\'):\n\n      if sample_rate == None:\n        sample_rate = tf.constant(p.sample_rate, dtype=tf.int32)\n\n      assert_op = tf.assert_equal(\n          tf.constant(p.sample_rate), tf.cast(sample_rate, dtype=tf.int32))\n      with tf.control_dependencies([assert_op]):\n\n        audio_data = py_x_ops.synthfiltbank(\n            power_spectrum,\n            phase_spectrum,\n            sample_rate,\n            window_length=p.window_length,\n            frame_length=p.frame_length)\n\n        return audio_data\n'"
delta/data/frontend/synthfiltbank_test.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""The model tests Synthfiltbank FE.""""""\n\nimport os\nfrom pathlib import Path\nimport delta.compat as tf\n\nfrom core.ops import PACKAGE_OPS_DIR\nfrom delta.data.frontend.read_wav import ReadWav\nfrom delta.data.frontend.analyfiltbank import Analyfiltbank\nfrom delta.data.frontend.synthfiltbank import Synthfiltbank\n\n\nclass Test(tf.test.TestCase):\n  """"""\n  Synthfiltbank extraction test.\n  """"""\n\n  def test_synthfiltbank(self):\n    wav_path = str(Path(PACKAGE_OPS_DIR).joinpath(\'data/sm1_cln.wav\'))\n\n    with self.cached_session(use_gpu=False, force_gpu=False):\n\n      read_wav = ReadWav.params().instantiate()\n      input_data, sample_rate = read_wav(wav_path)\n      input_data = input_data / 32768\n\n      analyfiltbank = Analyfiltbank.params().instantiate()\n      power_spc, phase_spc = analyfiltbank(input_data.eval(),\n                                           sample_rate.eval())\n\n      synthfiltbank = Synthfiltbank.params().instantiate()\n      audio_data = synthfiltbank(power_spc, phase_spc, sample_rate.eval())\n\n      self.assertAllClose(\n          audio_data.eval().flatten()[500:550],\n          input_data.eval().flatten()[500:550],\n          rtol=1e-4,\n          atol=1e-4)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
delta/data/frontend/write_wav.py,9,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport delta.compat as tf\n\nfrom delta.utils.hparam import HParams\nfrom delta.data.frontend.base_frontend import BaseFrontend\n\n\nclass WriteWav(BaseFrontend):\n\n  def __init__(self, config: dict):\n    super().__init__(config)\n\n  @classmethod\n  def params(cls, config=None):\n    """"""\n      Set params.\n       :param config: contains one optional parameters:sample_rate(int, default=16000).\n       :return: An object of class HParams, which is a set of hyperparameters as name-value pairs.\n       """"""\n\n    sample_rate = 16000\n\n    hparams = HParams(cls=cls)\n    hparams.add_hparam(\'sample_rate\', sample_rate)\n\n    if config is not None:\n      hparams.override_from_dict(config)\n\n    return hparams\n\n  def call(self, filename, audio_data, sample_rate=None):\n    """"""\n    Write wav using audio_data[tensor].\n    :param filename: filepath of wav.\n    :param audio_data: a tensor containing data of a wav.\n    :param sample_rate: [option]the samplerate of the signal we working with, default is 16kHz.\n    :return: write wav opration.\n    """"""\n    p = self.config\n    filename = tf.constant(filename)\n\n    if sample_rate == None:\n      sample_rate = tf.constant(p.sample_rate, dtype=tf.int32)\n\n    assert_op = tf.assert_equal(\n        tf.constant(p.sample_rate), tf.cast(sample_rate, dtype=tf.int32))\n    with tf.control_dependencies([assert_op]):\n      audio_data = tf.cast(audio_data / 32768, dtype=tf.float32)\n      contents = tf.audio.encode_wav(\n          tf.expand_dims(audio_data, 1), tf.cast(sample_rate, dtype=tf.int32))\n      w = tf.io.write_file(filename, contents)\n\n    return w\n'"
delta/data/frontend/write_wav_test.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport delta.compat as tf\nimport os\nfrom pathlib import Path\nfrom delta.data.frontend.read_wav import ReadWav\nfrom delta.data.frontend.write_wav import WriteWav\nfrom core.ops import PACKAGE_OPS_DIR\n\n\nclass WriteWavTest(tf.test.TestCase):\n\n  def test_write_wav(self):\n    wav_path = str(Path(PACKAGE_OPS_DIR).joinpath(\'data/sm1_cln.wav\'))\n\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      read_wav = ReadWav.params({\'speed\': 1.0}).instantiate()\n      input_data, sample_rate = read_wav(wav_path)\n      input_data = input_data\n      write_wav = WriteWav.params().instantiate()\n      new_path = str(Path(PACKAGE_OPS_DIR).joinpath(\'data/sm1_cln_speed.wav\'))\n      writewav_op = write_wav(new_path, input_data, sample_rate)\n      sess.run(writewav_op)\n      test_data, test_sample_rate = read_wav(new_path)\n      self.assertAllEqual(input_data.eval(), test_data.eval())\n      self.assertAllEqual(sample_rate.eval(), test_sample_rate.eval())\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
delta/data/frontend/zcr.py,6,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""This model extracts zcr features per frame.""""""\n\nimport delta.compat as tf\n\nfrom core.ops import py_x_ops\nfrom delta.utils.hparam import HParams\nfrom delta.data.frontend.base_frontend import BaseFrontend\n\n\nclass Zcr(BaseFrontend):\n  """"""\n  Compute ZCR features respectively\xef\xbc\x8cand concate them. Return\n  a tensor with shape (1, num_frames).\n  """"""\n\n  def __init__(self, config: dict):\n    super().__init__(config)\n\n  @classmethod\n  def params(cls, config=None):\n    """"""\n    Set params.\n    :param config:contains three optional parameters:\n        --sample_rate       : Waveform data sample frequency (must match the waveform\n                             file, if specified there). (float, default = 16000)\n        --window_length\t\t : Window length in seconds. (float, default = 0.025)\n        --frame_length\t\t : Hop length in seconds. (float, default = 0.010)\n    :return: An object of class HParams, which is a set of hyperparameters as\n            name-value pairs.\n    """"""\n\n    window_length = 0.025\n    frame_length = 0.010\n    sample_rate = 16000\n\n    hparams = HParams(cls=cls)\n    hparams.add_hparam(\'window_length\', window_length)\n    hparams.add_hparam(\'frame_length\', frame_length)\n    hparams.add_hparam(\'sample_rate\', sample_rate)\n\n    if config is not None:\n      hparams.override_from_dict(config)\n\n    return hparams\n\n  def call(self, audio_data, sample_rate=None):\n    """"""\n    Calculate the zero-crossing rate of speech.\n    :param audio_data: the audio signal from which to compute spectrum.\n                      Should be an (1, N) tensor.\n    :param sample_rate: [option]the samplerate of the signal we working with,\n                        default is 16kHz.\n    :return: A tensor with shape (1, num_frames), containing zero-crossing rate of\n            every frame in speech.\n    """"""\n\n    p = self.config\n    with tf.name_scope(\'zcr\'):\n\n      if sample_rate == None:\n        sample_rate = tf.constant(p.sample_rate, dtype=tf.int32)\n\n      assert_op = tf.assert_equal(\n          tf.constant(p.sample_rate), tf.cast(sample_rate, dtype=tf.int32))\n      with tf.control_dependencies([assert_op]):\n\n        sample_rate = tf.cast(sample_rate, dtype=float)\n        zcr = py_x_ops.zcr(\n            audio_data,\n            sample_rate,\n            window_length=p.window_length,\n            frame_length=p.frame_length)\n\n        return zcr\n'"
delta/data/frontend/zcr_test.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""The model tests ZCR FE.""""""\n\nimport os\nfrom pathlib import Path\nimport numpy as np\nimport delta.compat as tf\n\nfrom core.ops import PACKAGE_OPS_DIR\nfrom delta.data.frontend.read_wav import ReadWav\nfrom delta.data.frontend.zcr import Zcr\n\n\nclass ZcrTest(tf.test.TestCase):\n  """"""\n  Test Fbank FE using 8k/16k wav files.\n  """"""\n\n  def test_zcr(self):\n\n    wav_path = str(Path(PACKAGE_OPS_DIR).joinpath(\'data/sm1_cln.wav\'))\n\n    with self.cached_session(use_gpu=False, force_gpu=False):\n      read_wav = ReadWav.params().instantiate()\n      input_data, sample_rate = read_wav.call(wav_path)\n      input_data = input_data / 32768\n\n      zcr = Zcr.params({\n          \'window_length\': 0.025,\n          \'frame_length\': 0.010\n      }).instantiate()\n      zcr_test = zcr(input_data, sample_rate)\n\n      output_true = np.array([\n          0.406250, 0.418750, 0.425000, 0.407500, 0.393750, 0.392500, 0.388750,\n          0.417500, 0.427500, 0.456250, 0.447500, 0.386250, 0.357500, 0.282500,\n          0.232500, 0.262500, 0.282500, 0.295000, 0.220000, 0.157500, 0.125000,\n          0.107500, 0.100000, 0.092500, 0.092500, 0.095000, 0.097500, 0.105000,\n          0.100000, 0.112500, 0.120000, 0.132500, 0.130000, 0.135000, 0.112500,\n          0.120000, 0.090000, 0.080000, 0.070000, 0.080000, 0.087500, 0.092500,\n          0.097500, 0.097500, 0.112500, 0.090000, 0.065000, 0.087500, 0.175000,\n          0.240000\n      ])\n\n      self.assertAllClose(zcr_test.eval().flatten()[:50], output_true)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
delta/data/preprocess/__init__.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Init of preprocess""""""\n'"
delta/data/preprocess/base_preparer.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\'Base class for Preparer\'\'\'\n\nimport os\nimport math\nfrom pathlib import Path\nfrom absl import logging\nimport delta.compat as tf\nimport numpy as np\n\nfrom delta import utils\nfrom delta.data.preprocess.utils import prepare_embedding\nfrom delta.utils.solver.utils.solver_utils import get_session_conf\nfrom delta.data.preprocess.utils import prepare_vocab\nfrom delta.data.preprocess.utils import prepare_vocab_from_config\nfrom delta.data.preprocess.utils import get_pre_process_text_ds_iter\nfrom delta.data.utils.common_utils import get_file_len\n\n\nclass Preparer:\n  \'\'\'Base class for Preparer\'\'\'\n\n  def __init__(self, config):\n    self.config = config\n    self.reuse = self.config[""data""][""task""][""preparer""].get(""reuse"", True)\n    self.done_sign = self.config[""data""][""task""][""preparer""].get(\n        ""done_sign"", """")\n\n  def skip_prepare(self):\n    """"""Check if task need to skip the prepare process.""""""\n    return self.done_sign != """" and os.path.exists(self.done_sign) \\\n           and self.reuse\n\n  def done_prepare(self):\n    """"""Touch a sign file after the prepare process is done.""""""\n    if self.done_sign != """" and not os.path.exists(self.done_sign):\n      if not os.path.exists(os.path.dirname(self.done_sign)):\n        os.makedirs(os.path.dirname(self.done_sign))\n      Path(self.done_sign).touch()\n\n  def do_prepare(self, pre_process_pipeline):\n    """"""Do the prepare processing.""""""\n    raise NotImplementedError\n\n\nclass TextPreparer(Preparer):\n  """"""Base Preparer class for nlp""""""\n\n  def __init__(self, config):\n    super().__init__(config)\n    self.all_modes = (utils.INFER, utils.EVAL, utils.TRAIN)\n    self.infer_no_label = self.config[""data""][utils.INFER].get(\n        \'infer_no_label\', False)\n    self.model_config = self.config[""model""]\n    self.task_config = self.config[""data""][""task""]\n    self.batch_size = self.task_config[\'batch_size\']\n    self.num_parallel_calls = self.task_config[\'num_parallel_calls\']\n    self.vocab_min_frequency = self.task_config[\'vocab_min_frequency\']\n    self.use_custom_vocab = self.task_config.get(\'use_custom_vocab\', False)\n    self.text_vocab_file_path = self.task_config[\'text_vocab\']\n    self.label_vocab_file_paths = self.task_config[\'label_vocab\']\n    if not isinstance(self.label_vocab_file_paths, list):\n      self.label_vocab_file_paths = [self.label_vocab_file_paths]\n    self.output_num = len(self.label_vocab_file_paths)\n    self.multi_output = bool(self.output_num > 1)\n    self.multi_text = False\n    self.session_conf = get_session_conf(self.config)\n    self.init_feed_dict = {}\n\n  def prepare_raw_data(self, pre_process_pipeline):\n    """"""\n    Preparing raw data.\n    For all kinds of text input, all_texts: [sentence1, ...]\n    For single output, all_labels: [[label1, label2, ...]]\n    For multiple outputs, all_labels: [[label1_1, ...], [label1_2, ...]]\n    """"""\n    if self.output_num <= 1:\n      all_labels = []\n    else:\n      all_labels = [[] for _ in range(self.output_num)]\n    all_texts = []\n    for mode in self.all_modes:\n      paths = self.config[""data""][mode][\'paths\']\n      paths_after_pre_process = [one_path + "".after"" for one_path in paths]\n      logging.debug(\n          ""paths_after_pre_process: {}"".format(paths_after_pre_process))\n\n      infer_without_label = bool(mode == utils.INFER and self.infer_no_label)\n\n      for one_path, one_path_after in zip(paths, paths_after_pre_process):\n        if not os.path.exists(one_path):\n          raise FileNotFoundError(""{} does not exist!"".format(one_path))\n        data_size = get_file_len([one_path])\n        self.prepare_one_raw_data([one_path], one_path_after, mode,\n                                  infer_without_label, pre_process_pipeline,\n                                  all_texts, all_labels, data_size)\n    if self.output_num <= 1:\n      all_labels = [all_labels]\n    return all_texts, all_labels\n\n  def prepare_one_raw_data(self, one_path, one_path_after, mode,\n                           infer_without_label, pre_process_pipeline, all_texts,\n                           all_labels, data_size):\n    """"""Prepare one raw data.""""""\n    text, label = self.load_a_raw_file(one_path, infer_without_label)\n\n    batch_num = int(math.ceil(data_size / float(self.batch_size)))\n    if self.multi_text:\n      one_text_after = []\n      for i, one_text in enumerate(text):  #to be confirmed\n        one_text_iterator = get_pre_process_text_ds_iter(\n            one_text, pre_process_pipeline, self.num_parallel_calls,\n            self.batch_size)\n        text_after_arr = self.run_dataset(one_text_iterator, batch_num)\n        text_after = [one_line.decode(""utf-8"") for one_line in text_after_arr]\n        all_texts += text_after\n        one_text_after.append(text_after)\n    else:\n      text = text[0]\n      text_iterator = get_pre_process_text_ds_iter(text, pre_process_pipeline,\n                                                   self.num_parallel_calls,\n                                                   self.batch_size)\n      text_after_arr = self.run_dataset(text_iterator, batch_num)\n      text_after = [one_line.decode(""utf-8"") for one_line in text_after_arr]\n      all_texts += text_after\n      one_text_after = text_after\n\n    one_label_after = []\n    if not infer_without_label:\n      if self.multi_output:\n        for i in range(self.output_num):\n          label_ds = label[i].batch(self.batch_size)\n          label_iterator = label_ds.make_initializable_iterator()\n          label_after_arr = self.run_dataset(label_iterator, batch_num)\n          label_after_one = [\n              one_line.decode(""utf-8"") for one_line in label_after_arr\n          ]\n          one_label_after.append(label_after_one)\n          all_labels[i] += label_after_one\n      else:\n        label = label[0]\n        label_ds = label.batch(self.batch_size)\n        label_iterator = label_ds.make_initializable_iterator()\n        label_after_arr = self.run_dataset(label_iterator, batch_num)\n        one_label_after = [\n            one_line.decode(""utf-8"") for one_line in label_after_arr\n        ]\n        all_labels += one_label_after\n\n    self.save_a_raw_file(one_label_after, one_text_after, one_path_after,\n                         infer_without_label)\n\n  def run_dataset(self, data_iterator, batch_num):\n    """"""Run the text pre-process pipeline, fetch data in numpy array format.""""""\n    data_after = []\n    data_t = data_iterator.get_next()\n    with tf.Session(config=self.session_conf) as sess:\n      sess.run(data_iterator.initializer, feed_dict=self.init_feed_dict)\n      for _ in range(batch_num):\n        try:\n          data_after.append(sess.run(data_t))\n        except tf.errors.OutOfRangeError:\n          break\n    data_after_arr = np.concatenate(data_after, axis=0)\n    return data_after_arr\n\n  def load_a_raw_file(self, one_path, infer_without_label):\n    """"""\n    Load a raw file. Return text and label.\n    For single text input, text: [sentence1, ...]\n    For multiple text inputs, text: [[sentence1_1, ...], [sentence1_2, ...]]\n    For single output, label: [label1, label2, ...]\n    For multiple outputs, label: [[label1_1, ...], [label1_2, ...]]\n    """"""\n    raise NotImplementedError\n\n  def save_a_raw_file(self, label, text_after, one_path_after,\n                      infer_without_label):\n    """"""Save a raw file.""""""\n    raise NotImplementedError\n\n  def prepare_embed(self):\n    """"""Preparing embedding.""""""\n    logging.info(""Preparing embedding ..."")\n    if self.model_config[""use_pre_train_emb""]:\n      prepare_embedding(self.model_config[""pre_train_emb_path""],\n                        self.task_config[""text_vocab""],\n                        self.model_config[""embedding_path""])\n\n  def prepare_text_vocab(self, all_texts):\n    """"""Preparing text vocab""""""\n    if os.path.exists(self.text_vocab_file_path) and \\\n      self.use_custom_vocab:\n      logging.info(""Reuse text vocab file: {}"".format(\n          self.text_vocab_file_path))\n    else:\n      prepare_vocab(\n          self.text_vocab_file_path,\n          all_texts,\n          min_frequency=self.vocab_min_frequency)\n      logging.info(""Generate text vocab file: {}"".format(\n          self.text_vocab_file_path))\n\n  def prepare_label_vocab(self, all_labels):\n    """"""Prepare label vocab""""""\n    for i in range(self.output_num):\n      if os.path.exists(self.label_vocab_file_paths[i]) and \\\n        self.use_custom_vocab:\n        logging.info(""Reuse label vocab file: {}"".format(\n            self.label_vocab_file_paths[i]))\n      else:\n        if ""vocab"" in self.config[""data""][""task""][""classes""]:\n          output_index = i if self.multi_output else None\n          prepare_vocab_from_config(\n              self.label_vocab_file_paths[i],\n              self.config,\n              output_index=output_index)\n        else:\n          prepare_vocab(\n              self.label_vocab_file_paths[i],\n              all_labels[i],\n              min_frequency=1,\n              use_default_dict=False)\n        logging.info(""Generate label vocab file: {}"".format(\n            self.label_vocab_file_paths[i]))\n\n  def prepare_vocabs(self, all_texts, all_labels):\n    """"""Preparing vocab for x.""""""\n    logging.info(""Preparing vocab for x ..."")\n    self.prepare_text_vocab(all_texts)\n    logging.info(""Preparing vocab for y ..."")\n    self.prepare_label_vocab(all_labels)\n\n  def do_prepare(self, pre_process_pipeline):\n    """"""Do the prepare processing.""""""\n    all_texts, all_labels = self.prepare_raw_data(pre_process_pipeline)\n    self.prepare_vocabs(all_texts, all_labels)\n    self.prepare_embed()\n'"
delta/data/preprocess/text_cls_preparer.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Preparer for text classification.\'\'\'\n\nfrom delta.data.preprocess.base_preparer import TextPreparer\nfrom delta.data.preprocess.text_ops import load_textline_dataset\nfrom delta.data import utils as data_utils\nfrom delta.utils.register import registers\n\n# pylint: disable=too-many-instance-attributes\n\n\n@registers.preparer.register\nclass TextClsPreparer(TextPreparer):\n  """"""Preparer for text classification.""""""\n\n  def __init__(self, config):\n    super().__init__(config)\n\n  def load_a_raw_file(self, one_path, infer_without_label):\n    """"""\n    Load a raw file. Return text and label.\n    For single text input, text: [sentence1, ...]\n    For multiple text inputs, text: [[sentence1_1, ...], [sentence1_2, ...]]\n    For single output, label: [label1, label2, ...]\n    For multiple outputs, label: [[label1_1, ...], [label1_2, ...]]\n    """"""\n    if infer_without_label:\n      column_num = 1\n    else:\n      column_num = 2\n    ds_list = load_textline_dataset(one_path, column_num)\n    if infer_without_label:\n      text = ds_list\n      label = []  #to modifiy\n    else:\n      text = ds_list[1:]\n      label = ds_list[:1]\n    return (text, label)\n\n  def save_a_raw_file(self, label, text_after, one_path_after,\n                      infer_without_label):\n    """"""Save a raw file.""""""\n    data_utils.save_a_text_cls_file(label, text_after, one_path_after,\n                                    infer_without_label)\n'"
delta/data/preprocess/text_match_preparer.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Preparer for text match.\'\'\'\n\nfrom delta.data.preprocess.base_preparer import TextPreparer\nfrom delta.data.preprocess.text_ops import load_textline_dataset\nfrom delta.data import utils as data_utils\nfrom delta.utils.register import registers\n# pylint: disable=too-many-instance-attributes\n\n\n@registers.preparer.register\nclass TextMatchPreparer(TextPreparer):\n  """"""Preparer for text match.""""""\n\n  def __init__(self, config):\n    super().__init__(config)\n    self.multi_text = True\n\n  def load_a_raw_file(self, one_path, infer_without_label):\n    """"""\n    Load a raw file. Return text and label.\n    For single text input, text: [sentence1, ...]\n    For multiple text inputs, text: [[sentence1_1, ...], [sentence1_2, ...]]\n    For single output, label: [label1, label2, ...]\n    For multiple outputs, label: [[label1_1, ...], [label1_2, ...]]\n    """"""\n\n    if infer_without_label:\n      column_num = 2\n    else:\n      column_num = 3\n\n    ds_list = load_textline_dataset([one_path], column_num)\n    if infer_without_label:\n      text = ds_list\n      label = []\n    else:\n      text = ds_list[1:]\n      label = ds_list[:1]\n\n    return (text, label)\n\n  def save_a_raw_file(self, label, text_after, one_path_after,\n                      infer_without_label):\n    """"""Save a raw file.""""""\n    data_utils.save_a_text_match_file(label, text_after, one_path_after,\n                                      infer_without_label)\n'"
delta/data/preprocess/text_nlu_joint_preparer.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Preparer for NLU joint learning \'\'\'\n\nfrom delta.data.preprocess.base_preparer import TextPreparer\nfrom delta.data.preprocess.text_ops import load_textline_dataset\nfrom delta.data import utils as data_utils\nfrom delta.utils.register import registers\n\n# pylint: disable=too-many-instance-attributes, too-many-locals\n\n\n@registers.preparer.register\nclass TextNLUJointPreparer(TextPreparer):\n  """"""Preparer for NLU joint learning.""""""\n\n  def load_a_raw_file(self, one_path, infer_without_label):\n    """"""\n    Load a raw file. Return text and label.\n    For single text input, text: [sentence1, ...]\n    For multiple text inputs, text: [[sentence1_1, ...], [sentence1_2, ...]]\n    For single output, label: [label1, label2, ...]\n    For multiple outputs, label: [[label1_1, ...], [label1_2, ...]]\n    """"""\n    if infer_without_label:\n      column_num = 1\n    else:\n      column_num = 3\n    ds_list = load_textline_dataset(one_path, column_num)\n    if infer_without_label:\n      text = ds_list\n      label = []\n    else:\n      text = ds_list[2:]\n      label = ds_list[:2]\n    return (text, label)\n\n  def save_a_raw_file(self, label, text_after, one_path_after,\n                      infer_without_label):\n    """"""Save a raw file.""""""\n    data_utils.save_a_text_nlu_joint_file(label, text_after, one_path_after,\n                                          infer_without_label)\n'"
delta/data/preprocess/text_ops.py,27,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Text related pre-process in ops.\'\'\'\n\nimport delta.compat as tf\nfrom absl import logging\n\nfrom delta import utils\nfrom core.ops import py_x_ops\nfrom delta.data.utils import read_lines_from_text_file\n\n\ndef tokenize_label(label, maxlen, label_vocab_file_path, pad_id):\n  """"""Tokenize labels""""""\n  vocabs = read_lines_from_text_file(label_vocab_file_path)\n  label_id, _ = py_x_ops.sentence_to_ids(\n      label,\n      maxlen=maxlen,\n      use_vocab_file=False,\n      vocab=vocabs,\n      load_token_ids_from_vocab=True,\n      pad_id=pad_id,\n      check_tokens=False)\n  return label_id\n\n\ndef tokenize_sentence(texts, max_seq_len, vocab_path):\n  """"""Tokenize sentence""""""\n  vocabs = read_lines_from_text_file(vocab_path)\n  token_ids, _ = py_x_ops.sentence_to_ids(\n      texts,\n      maxlen=max_seq_len,\n      use_vocab_file=False,\n      vocab=vocabs,\n      load_token_ids_from_vocab=True,\n      pad_id=utils.PAD_IDX,\n      check_tokens=False)\n  return token_ids\n\n\ndef chinese_word_cut_tf(input_str, use_file=False):\n  """"""""""""\n\n  output_str = py_x_ops.jieba_cut(input_str, use_file=use_file, hmm=True)\n  return output_str\n\n\ndef clean_english_str_tf(input_str):\n  """"""Clean English string with tensorflow oprations.""""""\n  # pylint: disable=anomalous-backslash-in-string\n  string = tf.regex_replace(input_str, r""[^A-Za-z0-9(),!?\\\'\\`<>/]"", "" "")\n  string = tf.regex_replace(string, ""\\\'s"", "" \\\'s"")\n  string = tf.regex_replace(string, ""\\\'ve"", "" \\\'ve"")\n  string = tf.regex_replace(string, ""n\\\'t"", "" n\\\'t"")\n  string = tf.regex_replace(string, ""\\\'re"", "" \\\'re"")\n  string = tf.regex_replace(string, ""\\\'d"", "" \\\'d"")\n  string = tf.regex_replace(string, ""\\\'ll"", "" \\\'ll"")\n  string = tf.regex_replace(string, "","", "" , "")\n  string = tf.regex_replace(string, ""!"", "" ! "")\n  string = tf.regex_replace(string, ""\\("", "" ( "")\n  string = tf.regex_replace(string, ""\\)"", "" ) "")\n  string = tf.regex_replace(string, ""\\?"", "" ? "")\n  string = tf.regex_replace(string, ""\\s{2,}"", "" "")\n  string = tf.string_strip(string)\n  string = py_x_ops.str_lower(string)\n  return string\n\n\ndef char_cut_tf(input_str):\n  """"""Cut sentence char by char with tensoflow operations.""""""\n  input_str = tf.convert_to_tensor(input_str)\n  rank = len(input_str.get_shape())\n  if rank == 1:\n    output_str = tf.strings.unicode_split(input_str,\n                                          ""UTF-8"").to_tensor(default_value="""")\n    output_str = tf.strings.reduce_join(output_str, axis=1, separator="" "")\n  elif rank == 0:\n    output_str = tf.strings.unicode_split(input_str, ""UTF-8"")\n    output_str = tf.strings.reduce_join(output_str, axis=0, separator="" "")\n  else:\n    logging.error(""Please check the shape of input_str!"")\n    raise Exception(""Error input shape for input_str."")\n  output_str = tf.strings.strip(output_str)\n  return output_str\n\n\ndef load_textline_dataset(paths, column_num):\n  """"""Load raw data for text task.""""""\n  ds = tf.data.TextLineDataset(paths)\n  ds = ds.map(\n      lambda x: tf.strings.split(x, sep=""\\t"", result_type=""RaggedTensor""))\n  ds = ds.filter(lambda line: tf.equal(tf.size(line), column_num))\n  ds_list = []\n  for i in range(column_num):\n    ds_list.append(ds.map(lambda x: x[i]))\n\n  return tuple(ds_list)\n\n\ndef process_one_label_dataset(label_ds, config, output_index=None):\n  """"""process one-label data set.""""""\n\n  logging.info(""Loading one label dataset..."")\n  num_parallel_calls = config[""data""][""task""][""num_parallel_calls""]\n  classes = config[""data""][""task""][""classes""]\n  if isinstance(classes, list):\n    if output_index is None or output_index not in range(len(classes)):\n      raise IndexError(""output_index:{} not in the range of classes length: ""\n                       ""{}!"".format(output_index, len(classes)))\n    num_classes = classes[output_index][""num_classes""]\n    label_vocab_file_path = config[""data""][""task""][""label_vocab""][output_index]\n  else:\n    num_classes = classes[""num_classes""]\n    label_vocab_file_path = config[""data""][""task""][""label_vocab""]\n\n  label_ds = label_ds.map(\n      lambda x: tokenize_label(\n          x, maxlen=1, label_vocab_file_path=label_vocab_file_path, pad_id=0),\n      num_parallel_calls=num_parallel_calls)\n\n  label_ds = label_ds.map(\n      lambda l: tf.one_hot(l, num_classes, dtype=tf.int32),\n      num_parallel_calls=num_parallel_calls)\n\n  label_ds = label_ds.map(tf.squeeze, num_parallel_calls=num_parallel_calls)\n\n  return label_ds\n\n\ndef process_multi_label_dataset(label_ds, config, output_index=None):\n  """"""process multi-label data set.""""""\n  logging.info(""Loading multi label dataset..."")\n  label_vocab_file_path = config[""data""][""task""][""label_vocab""]\n  num_parallel_calls = config[""data""][""task""][""num_parallel_calls""]\n  max_seq_len = config[""data""][""task""][""max_seq_len""]\n\n  label_vocab_file_path = config[""data""][""task""][""label_vocab""]\n  if isinstance(label_vocab_file_path, list):\n    if output_index is None or output_index not in range(\n        len(label_vocab_file_path)):\n      raise IndexError(""output_index:{} not in the range of classes length: ""\n                       ""{}!"".format(output_index, len(label_vocab_file_path)))\n    label_vocab_file_path = label_vocab_file_path[output_index]\n\n  else:\n    label_vocab_file_path = label_vocab_file_path\n\n  label_ds = label_ds.map(\n      lambda x: tokenize_label(\n          x,\n          maxlen=max_seq_len,\n          label_vocab_file_path=label_vocab_file_path,\n          pad_id=0),\n      num_parallel_calls=num_parallel_calls)\n  label_ds = label_ds.map(tf.squeeze, num_parallel_calls=num_parallel_calls)\n\n  return label_ds\n\n\ndef load_dense_dataset(dense_feature):\n  """"""Load dense data set""""""\n  dataset = tf.data.Dataset.from_tensor_slices(dense_feature)\n  return dataset\n'"
delta/data/preprocess/text_ops_test.py,9,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' text ops utils unittest\'\'\'\n\n# pylint: disable=missing-docstring\n\nimport os\nfrom pathlib import Path\nimport delta.compat as tf\nfrom absl import logging\nimport tempfile\nimport numpy as np\n\nfrom delta import PACKAGE_ROOT_DIR\nfrom delta import utils\nfrom delta.data.preprocess.text_ops import clean_english_str_tf\nfrom delta.data.preprocess.text_ops import char_cut_tf\nfrom delta.data.preprocess.text_ops import tokenize_label\nfrom delta.data.preprocess.text_ops import tokenize_sentence\nfrom delta.data.preprocess.text_ops import process_one_label_dataset\nfrom delta.data.preprocess.text_ops import process_multi_label_dataset\n\n\nclass TextOpsTest(tf.test.TestCase):\n\n  def setUp(self):\n    super().setUp()\n    package_root = Path(PACKAGE_ROOT_DIR)\n    self.config_file = package_root.joinpath(\n        \'../egs/mock_text_seq_label_data/seq-label/v1/config/seq-label-mock.yml\'\n    )\n    self.config = utils.load_config(self.config_file)\n\n    self.vocab_text = [\'<unk>\\t1\', \'</s>\\t2\', \'O\\t3\']\n    self.vocab_label = [\n        \'B\\t0\', ""B-PER\\t1"", ""I-PER\\t2"", ""B-LOC\\t3"", ""I-LOC\\t4"", ""B-ORG5\\t5"",\n        ""I-ORG\\t6"", ""B-MISC\\t7"", ""I-MISC\\t8""\n    ]\n    self.vocab_text_filepath = tempfile.mktemp(suffix=\'text_vocab.txt\')\n    self.vocab_label_filepath = tempfile.mktemp(suffix=\'label_vocab.txt\')\n    with open(self.vocab_text_filepath, mode=\'w\', encoding=\'utf-8\') as fobj:\n      for token in self.vocab_text:\n        fobj.write(token)\n        fobj.write(\'\\n\')\n    with open(self.vocab_label_filepath, mode=\'w\', encoding=\'utf-8\') as fobj:\n      for token in self.vocab_label:\n        fobj.write(token)\n        fobj.write(\'\\n\')\n\n  def tearDown(self):\n    \'\'\' tear down \'\'\'\n\n  def test_label_and_text(self):\n    text = [""O O""]\n    maxlen = 2\n    text_tokenize_t = tokenize_sentence(text, maxlen, self.vocab_text_filepath)\n    label = [""B B""]\n    maxlen = 2\n    label_tokenize_t = tokenize_label(label, maxlen, self.vocab_label_filepath,\n                                      -1)\n\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      res = sess.run([text_tokenize_t, label_tokenize_t])\n      logging.debug(res)\n      self.assertAllEqual(res[0], [[3, 3]])\n      self.assertAllEqual(res[1], [[0, 0]])\n\n  def test_clean_english_str_tf(self):\n    t_sentence_in = tf.placeholder(dtype=tf.string)\n    t_sentence_out = clean_english_str_tf(t_sentence_in)\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      sentence_out = sess.run(t_sentence_out,\n                              {t_sentence_in: ""I\'d like to have an APPLE! ""})\n      logging.info(sentence_out)\n      self.assertEqual(""i \'d like to have an apple !"",\n                       sentence_out.decode(""utf-8""))\n      sentence_out = sess.run(t_sentence_out,\n                              {t_sentence_in: [""I\'d like to have an APPLE! ""]})\n      logging.info(sentence_out)\n      self.assertEqual(""i \'d like to have an apple !"",\n                       sentence_out[0].decode(""utf-8""))\n\n  def test_char_cut_tf_str(self):\n    t_sen_in = tf.placeholder(dtype=tf.string, shape=())\n    t_sen_out = char_cut_tf(t_sen_in)\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      sen_out = sess.run(t_sen_out, {t_sen_in: ""\xe6\x88\x91\xe7\x88\xb1\xe5\x8c\x97\xe4\xba\xac\xe5\xa4\xa9\xe5\xae\x89\xe9\x97\xa8""})\n      logging.info(sen_out.decode(""utf-8""))\n      self.assertEqual(""\xe6\x88\x91 \xe7\x88\xb1 \xe5\x8c\x97 \xe4\xba\xac \xe5\xa4\xa9 \xe5\xae\x89 \xe9\x97\xa8"", sen_out.decode(""utf-8""))\n\n  def test_char_cut_tf_list(self):\n    t_sen_in = tf.placeholder(dtype=tf.string, shape=(None,))\n    t_sen_out = char_cut_tf(t_sen_in)\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      sen_out = sess.run(t_sen_out, {t_sen_in: [""\xe6\x88\x91\xe7\x88\xb1\xe5\x8c\x97\xe4\xba\xac\xe5\xa4\xa9\xe5\xae\x89\xe9\x97\xa8"", ""\xe5\xa4\xa9\xe5\xae\x89\xe9\x97\xa8\xe5\x89\x8d\xe5\xa4\xaa\xe9\x98\xb3\xe5\x8d\x87\xe5\x95\x8a""]})\n      logging.info([one.decode(""utf-8"") for one in sen_out])\n      self.assertAllEqual([""\xe6\x88\x91 \xe7\x88\xb1 \xe5\x8c\x97 \xe4\xba\xac \xe5\xa4\xa9 \xe5\xae\x89 \xe9\x97\xa8"", ""\xe5\xa4\xa9 \xe5\xae\x89 \xe9\x97\xa8 \xe5\x89\x8d \xe5\xa4\xaa \xe9\x98\xb3 \xe5\x8d\x87 \xe5\x95\x8a""],\n                          [one.decode(""utf-8"") for one in sen_out])\n\n  def test_process_one_label_dataset(self):\n    label = [""O"", ""O"", ""O"", ""I-MISC""]\n    label_filepath = tempfile.mktemp(suffix=\'label_file_for_unitest.txt\')\n    with open(label_filepath, mode=\'w\', encoding=\'utf-8\') as fobj:\n      for token in label:\n        fobj.write(token)\n        fobj.write(\'\\n\')\n    label_ds = tf.data.TextLineDataset(label_filepath)\n    true_res = [0, 0, 0, 8]\n    label_ds = process_one_label_dataset(label_ds, self.config)\n\n    iterator = label_ds.make_initializable_iterator()\n    label_res = iterator.get_next()\n\n    with tf.Session() as sess:\n      sess.run(iterator.initializer)\n      for i in range(len(label)):\n        self.assertEqual(np.argmax(sess.run(label_res)), true_res[i])\n\n  def test_process_multi_label_dataset(self):\n    label = [""O I-MISC I-MISC"", ""O B-MISC I-MISC""]\n    label_filepath = tempfile.mktemp(suffix=\'label_file_for_unitest.txt\')\n    with open(label_filepath, mode=\'w\', encoding=\'utf-8\') as fobj:\n      for token in label:\n        fobj.write(token)\n        fobj.write(\'\\n\')\n    label_ds = tf.data.TextLineDataset(label_filepath)\n    true_res = [[0, 8, 8], [0, 7, 8]]\n    label_ds = process_multi_label_dataset(label_ds, self.config)\n    iterator = label_ds.make_initializable_iterator()\n    label_res = iterator.get_next()\n\n    with tf.Session() as sess:\n      sess.run(iterator.initializer)\n      for i in range(len(label)):\n        self.assertEqual(list(sess.run(label_res)[:3]), true_res[i])\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.DEBUG)\n  tf.test.main()\n'"
delta/data/preprocess/text_seq2seq_preparer.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Preparer for text sequence to sequence.\'\'\'\n\nimport os\nfrom delta import utils\nfrom absl import logging\nfrom delta.data.preprocess.base_preparer import TextPreparer\nfrom delta.data import utils as data_utils\nfrom delta.utils.register import registers\nfrom delta.data.preprocess.utils import prepare_vocab\nfrom delta.data.preprocess.utils import prepare_vocab_from_config\nfrom delta.data.preprocess.text_ops import load_textline_dataset\nfrom delta.data.utils.common_utils import get_file_len\n# pylint: disable=too-many-instance-attributes, too-many-locals\n\n\n@registers.preparer.register\nclass TextS2SPreparer(TextPreparer):\n  """"""Preparer for sequence labeling.""""""\n\n  def __init__(self, config):\n    super().__init__(config)\n    self.multi_text = True\n\n  def prepare_raw_data(self, pre_process_pipeline):\n    """"""\n    Preparing raw data.\n    For all kinds of text input, all_texts: [sentence1, ...]\n    For single output, all_labels: [label1, label2, ...]\n    For multiple outputs, all_labels: [[label1_1, ...], [label1_2, ...]]\n    """"""\n    if self.output_num <= 1:\n      all_labels = []\n    else:\n      all_labels = [[] for _ in range(self.output_num)]\n    all_texts = []\n    for mode in self.all_modes:\n      paths = self.config[""data""][mode][\'paths\']\n      paths = [paths[\'source\'], paths[\'target\']]\n      paths_after_pre_process = [\n          [one_path + "".after"" for one_path in path] for path in paths\n      ]\n      logging.debug(\n          ""paths_after_pre_process: {}"".format(paths_after_pre_process))\n\n      infer_without_label = bool(mode == utils.INFER and self.infer_no_label)\n\n      for one_path_text, one_path_target, \\\n          one_path_text_after, one_path_target_after in zip(*paths, *paths_after_pre_process):\n        data_size = get_file_len([one_path_text])\n        self.prepare_one_raw_data((one_path_text, one_path_target),\n                                  (one_path_text_after, one_path_target_after),\n                                  mode, infer_without_label,\n                                  pre_process_pipeline, all_texts, all_labels,\n                                  data_size)\n    return all_texts, all_labels\n\n  def load_a_raw_file(self, one_path, infer_without_label):\n    """"""\n    Load a raw file. Return text and label.\n    For single text input, text: [sentence1, ...]\n    For multiple text inputs, text: [[sentence1_1, ...], [sentence1_2, ...]]\n    For single output, label: [label1, label2, ...]\n    For multiple outputs, label: [[label1_1, ...], [label1_2, ...]]\n    """"""\n    column_num = 1\n    text_path, target_path = one_path\n    texts = load_textline_dataset([text_path], column_num)\n    # texts = data_utils.load_seq2seq_raw_data([text_path])\n    if not infer_without_label:\n      target = load_textline_dataset([target_path], column_num)\n      return texts + target, target\n    return texts, []\n\n  def save_a_raw_file(self, label, text_after, one_path_after,\n                      infer_without_label):\n    text_path, target_path = one_path_after\n    if infer_without_label:\n      text = text_after[0]\n    else:\n      text, target = text_after\n      data_utils.save_a_text_seq2seq_file(target, target_path)\n    data_utils.save_a_text_seq2seq_file(text, text_path)\n\n  def prepare_label_vocab(self, all_labels):\n    """"""Prepare label vocab""""""\n    for i in range(self.output_num):\n      if os.path.exists(self.label_vocab_file_paths[i]) and \\\n        self.use_custom_vocab:\n        logging.info(""Reuse label vocab file: {}"".format(\n            self.label_vocab_file_paths[i]))\n      else:\n        prepare_vocab(\n            self.label_vocab_file_paths[i],\n            all_labels[i],\n            min_frequency=self.vocab_min_frequency,\n            use_default_dict=True)\n        logging.info(""Generate label vocab file: {}"".format(\n            self.label_vocab_file_paths[i]))\n'"
delta/data/preprocess/text_seq_label_preparer.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Preparer for sequence labeling \'\'\'\n\nfrom delta.data.preprocess.base_preparer import TextPreparer\nfrom delta.data.preprocess.text_ops import load_textline_dataset\nfrom delta.data import utils as data_utils\nfrom delta.utils.register import registers\n\n# pylint: disable=too-many-instance-attributes, too-many-locals\n\n\n@registers.preparer.register\nclass TextSeqLabelPreparer(TextPreparer):\n  """"""Preparer for sequence labeling.""""""\n\n  def load_a_raw_file(self, one_path, infer_without_label):\n    """"""\n    Load a raw file. Return text and label.\n    For single text input, text: [sentence1, ...]\n    For multiple text inputs, text: [[sentence1_1, ...], [sentence1_2, ...]]\n    For single output, label: [label1, label2, ...]\n    For multiple outputs, label: [[label1_1, ...], [label1_2, ...]]\n    """"""\n    if infer_without_label:\n      column_num = 1\n    else:\n      column_num = 2\n    ds_list = load_textline_dataset(one_path, column_num)\n    if infer_without_label:\n      text = ds_list\n      label = []\n    else:\n      text = ds_list[1:]\n      label = ds_list[:1]\n    return (text, label)\n\n  def save_a_raw_file(self, label, text_after, one_path_after,\n                      infer_without_label):\n    """"""Save a raw file.""""""\n    data_utils.save_a_text_seq_label_file(label, text_after, one_path_after,\n                                          infer_without_label)\n'"
delta/data/preprocess/utils.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\'Utilities for data preprocessing\'\'\'\n\nimport os\nimport pickle\nimport collections\nimport numpy as np\nfrom absl import logging\nimport delta.compat as tf\n\nfrom delta.data.utils.vocabulary import Vocabulary\n\n\ndef get_pre_process_text_ds_iter(\n    text_ds,\n    pipeline_func,\n    num_parallel_calls,\n    batch_size,\n):\n  """"""Get pre-process oprators.""""""\n\n  text_ds = text_ds.map(pipeline_func, num_parallel_calls=num_parallel_calls)\n\n  text_ds = text_ds.batch(batch_size)\n\n  iterator = text_ds.make_initializable_iterator()\n\n  return iterator\n\n\ndef process_vocab(vocab_file_path, data, vocab, min_frequency=0):\n  """"""Process vocab""""""\n  for line in data:\n    for word in line.split():\n      vocab.add(word)\n  if min_frequency > 0:\n    vocab.trim(min_frequency)\n  save_vocabs(vocab.mapping, vocab_file_path)\n\n\ndef save_vocabs(vocabs, vocab_file_path):\n  """"""Save vocabs, vocab: {""word"": 1, ...}""""""\n  logging.info(""Saving vocab to {}"".format(vocab_file_path))\n  id_to_vocab = {v: k for k, v in vocabs.items()}\n  ordered_vocabs = collections.OrderedDict()\n  for _id in sorted(vocabs.values()):\n    ordered_vocabs[id_to_vocab[_id]] = _id\n\n  if os.path.isfile(vocab_file_path):\n    os.remove(vocab_file_path)\n  if not os.path.exists(os.path.dirname(vocab_file_path)):\n    os.makedirs(os.path.dirname(vocab_file_path))\n\n  with open(vocab_file_path, ""w"", encoding=\'utf-8\') as out_f:\n    for word, _id in ordered_vocabs.items():\n      out_f.write(""{}\\t{}\\n"".format(word, _id))\n\n\ndef load_vocab_dict(vocab_file_path):\n  """"""Load vocabs, vocab: {""word"": 1, ...}""""""\n  logging.info(""Loading vocab from {}"".format(vocab_file_path))\n  with open(vocab_file_path, encoding=\'utf-8\') as in_f:\n    vocabs = {}\n    for line in in_f:\n      parts = line.rstrip().split(""\\t"")\n      if len(parts) < 2:\n        continue\n      vocabs[parts[0]] = parts[1]\n  logging.info(""Loded {} vocabs from {}"".format(len(vocabs), vocab_file_path))\n  return vocabs\n\n\ndef get_vocab_size(vocab_file_path):\n  """"""Get vocab size.""""""\n  vocab_dict = load_vocab_dict(vocab_file_path)\n  return len(vocab_dict)\n\n\ndef prepare_vocab(vocab_file_path, text, min_frequency=1,\n                  use_default_dict=True):\n  """"""Prepare vocab""""""\n  text_vocab = Vocabulary(use_default_dict=use_default_dict)\n  process_vocab(vocab_file_path, text, text_vocab, min_frequency=min_frequency)\n\n\ndef prepare_vocab_from_config(vocab_file_path, config, output_index=None):\n  """"""Prepare vocab from config.""""""\n  if output_index is None:\n    label_config = config[""data""][""task""][""classes""][""vocab""]\n  else:\n    label_config = config[""data""][""task""][""classes""][output_index][""vocab""]\n\n  save_vocabs(label_config, vocab_file_path)\n\n\ndef prepare_embedding(pre_train_emb_path, text_vocab_path, embedding_path):\n  """"""Prepare embedding""""""\n  # pylint: disable=too-many-locals\n\n  if os.path.isfile(embedding_path):\n    os.remove(embedding_path)\n\n  # load pre_train model\n  logging.info(""Loading embedding from {}"".format(pre_train_emb_path))\n  emb_data = open(pre_train_emb_path, encoding=\'utf-8\', mode=\'r\')\n  emb_dict = {}\n  emb_size = 0\n  line_num = 0\n  for line in emb_data.readlines():\n    line_num += 1\n    if line_num == 1:\n      continue\n    line = line.strip().split(\' \')\n    word = line[0]\n    vector = [float(i) for i in line[1:]]\n    vector = vector / np.linalg.norm(vector)\n    emb_dict[word] = vector\n    emb_size = len(vector)\n  logging.info(""Load {} vectors"".format(line_num))\n  # load text vocab\n  vocabs = load_vocab_dict(text_vocab_path)\n\n  # get embedding vector for words in vocab\n  vocab_size = len(vocabs)\n  emb_list = [[]] * vocab_size\n  bound = np.sqrt(1.0) / np.sqrt(vocab_size)\n  count_exist = 0\n  count_not_exist = 0\n  word_id = 0\n\n  for word in vocabs:\n    try:\n      word_vector = emb_dict[word]\n    except Exception:  # pylint: disable=broad-except\n      word_vector = None\n\n    if word_vector is not None:\n      emb_list[word_id] = word_vector\n      count_exist += 1\n    else:\n      count_not_exist += 1\n      emb_list[word_id] = np.random.uniform(-bound, bound, emb_size)\n    word_id += 1\n\n  emb_list = np.array(emb_list)\n  logging.info(""embedding exist : {}, embedding not exist : {}"".format(\n      count_exist, count_not_exist))\n  logging.info(""embedding exist dump to: {}"".format(embedding_path))\n  with open(embedding_path, mode=\'wb\') as out_f:\n    pickle.dump(emb_list, out_f)\n'"
delta/data/task/__init__.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
delta/data/task/asr_seq_task.py,19,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' A sequential ASR task. \'\'\'\nimport numpy as np\nimport delta.compat as tf\nfrom absl import logging\n\nfrom delta import utils\nfrom delta.data.utils import espnet_utils\n\nfrom delta.utils.register import registers\nfrom delta.data.task.base_speech_task import SpeechTask\n\n# pylint: disable=consider-using-enumerate\n# pylint: disable=invalid-name\n\n\ndef _make_example(uttids, feats, ilens, targets, olens):\n  features = {\n      \'uttids\':\n          uttids,\n      \'inputs\':\n          tf.expand_dims(feats, axis=-1) if not isinstance(feats, np.ndarray)\n          else np.expand_dims(feats, axis=-1),\n      \'input_length\':\n          ilens,\n      \'targets\':\n          targets,\n      \'target_length\':\n          olens\n  }\n  labels = {\n      \'ctc\':\n          tf.ones(tf.shape(feats)[0])\n          if not isinstance(feats, np.ndarray) else np.ones(feats.shape[0])\n  }  # dummy data for dummy loss function\n  return features, labels\n\n\n@registers.task.register  #pylint: disable=too-many-instance-attributes\nclass AsrSeqTask(SpeechTask, tf.keras.utils.Sequence):\n  \'\'\' ASR Task \'\'\'\n\n  def __init__(self, config, mode):\n    super().__init__(config, mode)\n    self.dummy = config[\'data\'][\'task\'][\'dummy\']\n    self.batch_mode = config[\'data\'][\'task\'][\'batch_mode\']\n    self.batch_size = config[\'solver\'][\'optimizer\'][\'batch_size\']\n    self._shuffle_buffer_size = config[\'data\'][\'task\'][\'shuffle_buffer_size\']\n    self._need_shuffle = config[\'data\'][\'task\'][\n        \'need_shuffle\'] and mode == utils.TRAIN\n    # get batches form data path\n    if self.dummy:\n      self._feat_shape = [40]\n      logging.info(""Dummy data: feat {}"".format(self.feat_shape))\n      self._vocab_size = 100\n    else:\n      data_metas = espnet_utils.get_batches(self.config, mode)\n      self.batches = data_metas[\'data\']\n      self.n_utts = data_metas[\'n_utts\']\n      logging.info(""utts: {}"".format(self.n_utts))\n      # [nframe, feat_shape, ...]\n      self._feat_shape = self.batches[0][0][1][\'input\'][0][\'shape\'][1:]\n      # [tgt_len, vocab_size]\n      self._vocab_size = self.batches[0][0][1][\'output\'][0][\'shape\'][1]\n    logging.info(\'#input feat shape: \' + str(self.feat_shape))\n    logging.info(\'#output dims: \' + str(self.vocab_size))\n\n    self._converter = espnet_utils.ASRConverter(self.config)\n    self.on_epoch_end()\n\n  @property\n  def converter(self):\n    \'\'\' return self._converter which is espnet_utils.ASRConverter \'\'\'\n    return self._converter\n\n  @property\n  def feat_shape(self):\n    \'\'\' Feature shape. \'\'\'\n    assert isinstance(self._feat_shape, (list))\n    return self._feat_shape\n\n  @property\n  def vocab_size(self):\n    \'\'\' Vocabulary size. \'\'\'\n    return self._vocab_size\n\n  @property\n  def steps_per_epoch(self):\n    \'\'\' Steps per epoch. \'\'\'\n    if self.dummy:\n      step = 1\n      logging.info(""Dummy data: step {}"".format(step))\n      return step\n\n    steps = None\n    if self.batch_mode:\n      steps = len(self.batches)\n    else:\n      batch_size = self._config[\'data\'][\'task\'][\'batch\'][\'batch_size\']\n      steps = int(self.n_utts / batch_size)\n    return steps\n\n  def generate_feat(self, paths):\n    pass\n\n  def generate_cmvn(self, paths):\n    pass\n\n  def __len__(self):\n    \'\'\' Denotes the number of batches per epoch\'\'\'\n    return self.steps_per_epoch\n\n  def on_epoch_end(self):\n    \'\'\'shuffle data after each epoch\'\'\'\n    self.batch_num = self.steps_per_epoch\n    self.indexes = np.arange(self.batch_num)\n    if self._need_shuffle:\n      np.random.shuffle(self.indexes)\n\n  def __getitem__(self, batch_index):\n    \'\'\' Generates a batch of correctly shaped X and Y data\n    :param batch_index: index of the batch to generate\n    :return: batch of (x, y)\n    \'\'\'\n\n    assert self.batch_mode\n    batch_index_after_shuffle = self.indexes[batch_index]\n    batch = self.batches[batch_index_after_shuffle]\n\n    uttids, feats, ilens, targets, olens = self._process_batch(batch)\n    return _make_example(uttids, feats, ilens, targets, olens)\n\n  #pylint: disable=too-many-locals\n  def _process_batch(self, batch):\n    srcs, ilens, tgts, olens, uttid_list = self.converter(batch)\n\n    imax = max(ilens)\n    omax = max(olens)\n    batch_feat = []\n    batch_target = []\n    for i in range(len(srcs)):\n      #pad feat\n      ipad_len = imax - srcs[i].shape[0]\n      feat = np.pad(\n          srcs[i],\n          pad_width=((0, ipad_len), (0, 0)),\n          mode=\'constant\',\n          constant_values=0)\n\n      #pad target\n      opad_len = omax - tgts[i].shape[0]\n      target = np.pad(\n          tgts[i], pad_width=(0, opad_len), mode=\'constant\', constant_values=0)\n\n      batch_feat.append(feat)\n      batch_target.append(target)\n\n    batch_uttid = np.array(uttid_list)\n    batch_feat = np.stack(batch_feat).astype(np.float32)\n    batch_target = np.stack(batch_target).astype(np.int64)\n    ilens = np.array(ilens).astype(np.int64)\n    olens = np.array(olens).astype(np.int64)\n\n    return batch_uttid, batch_feat, ilens, batch_target, olens\n\n  def generate_data(self):  #pylint: disable=too-many-locals\n    \'\'\'\n        :return: feat, feat_len, target, terget_len\n        \'\'\'\n    if self.batch_mode:\n      for batch in self.batches:\n        batch_uttid, batch_feat, ilens, batch_target, olens = self._process_batch(\n            batch)\n        yield batch_uttid, batch_feat, ilens, batch_target, olens\n    else:\n      for batch in self.batches:\n        srcs, ilens, tgts, olens, uttid_list = self.converter(batch)\n        for i in range(len(srcs)):\n          yield uttid_list[i], srcs[i], ilens[i], tgts[i], olens[i]\n\n  def feature_spec(self, batch_size_):  # pylint: disable=arguments-differ\n    \'\'\'\n        uttid: []\n        feat: [feat_shape]\n        src_len: []\n        label: [None]\n        tgt_len: []\n        \'\'\'\n    values = None\n    batch_size = None\n    time = None\n    if self.dummy:\n      batch_size = batch_size_\n      time = 10\n      logging.info(""Dummy data: batch size {} time {}"".format(batch_size, time))\n\n    types = (tf.string, tf.float32, tf.int32, tf.int32, tf.int32)\n    if self.batch_mode or self.dummy:\n      # batch of examples\n      shapes = (\n          #uttid\n          tf.TensorShape([batch_size]),\n          # input\n          tf.TensorShape([batch_size, time, *self.feat_shape]),\n          # input len\n          tf.TensorShape([batch_size]),\n          # output\n          tf.TensorShape([batch_size, time]),\n          # output len\n          tf.TensorShape([batch_size]),\n      )\n    else:\n      # one example\n      shapes = (\n          #uttid\n          tf.TensorShape([]),\n          # input\n          tf.TensorShape([time, *self.feat_shape]),\n          # input len\n          tf.TensorShape([]),\n          # output\n          tf.TensorShape([time]),\n          # output len\n          tf.TensorShape([]),\n      )\n    if self.dummy:\n      values = (""uttid_1"", 1, 2, 3, 4)\n      logging.info(""Dummy data: shapes {}"".format(shapes))\n      logging.info(""Dummy data: types {}"".format(types))\n      logging.info(""Dummy data: values {}"".format(values))\n    return types, shapes, values\n\n  def preprocess_batch(self, batch):\n    return batch\n\n  def dataset(self, mode, batch_size, epoch):  # pylint: disable=arguments-differ\n    if batch_size != self.batch_size:\n      logging.warning(""dataset: batch_size not equal to config: {} {}"".format(\n          batch_size, self.batch_size))\n\n    types, shapes, values = self.feature_spec(batch_size)\n    logging.debug(\'dtypes: {} shapes: {} values: {}\'.format(\n        types, shapes, values))\n\n    if self.dummy:\n      logging.info(""Dummy data: dataset"")\n      dss = []\n      for i in range(len(shapes)):\n        dss.append(\n            utils.generate_synthetic_data(\n                input_shape=shapes[i],\n                input_value=values[i],\n                input_dtype=types[i],\n                nepoch=epoch))\n      ds = tf.data.Dataset.zip(tuple(dss))\n    else:\n      del values\n      ds = tf.data.Dataset.from_generator(\n          generator=lambda: self.generate_data(),  # pylint: disable=unnecessary-lambda\n          output_types=types,\n          output_shapes=shapes)\n\n      if mode == utils.TRAIN:\n        if self._need_shuffle:\n          ds = ds.shuffle(self._shuffle_buffer_size, seed=None)\n        ds = ds.repeat(count=epoch)\n\n      if not self.batch_mode:\n        ds = ds.padded_batch(\n            batch_size,\n            padded_shapes=shapes,\n            padding_values=None,\n            drop_remainder=True if mode == utils.TRAIN else False)  #pylint: disable=simplifiable-if-expression\n\n    ds = ds.map(_make_example)\n    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\n  def batch_input_shape(self):\n    \'\'\' batch input TensorShape \'\'\'\n    feature, labels = self.__getitem__(0)\n\n    feature_shape, label_shape = {}, {}\n    for feature_key, feature_val in feature.items():\n      feature_shape[feature_key] = tf.TensorShape((None,) +\n                                                  feature_val.shape[1:])\n\n    for label_key, label_val in labels.items():\n      label_shape[label_key] = tf.TensorShape((None,) + label_val.shape[1:])\n\n    return feature_shape, label_shape\n'"
delta/data/task/asr_seq_task_test.py,6,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Test for asr_seq_task.py \'\'\'\n\nfrom pathlib import Path\nfrom absl import logging\nimport numpy as np\nimport delta.compat as tf\n\nfrom delta import utils\nfrom delta.data.utils.test_utils import generate_json_data\nfrom delta.utils.register import registers\nfrom delta.utils.register import import_all_modules_for_register\n\n# pylint: disable=missing-docstring\n\n\nclass AsrSeqTaskTest(tf.test.TestCase):\n  \'\'\' Unit test for AsrSeqTask. \'\'\'\n\n  def setUp(self):\n    super().setUp()\n    self.conf_str = \'\'\'\n    data:\n      train:\n        paths: null\n        segments: null\n      eval:\n        paths: null\n        segments: null\n      infer:\n        paths: null\n        segments: null\n      task:\n        dummy: true # dummy inputs \n        name: AsrSeqTask\n        type: asr # asr, tts\n        audio:\n          dry_run: false # not save feat\n        src:\n          max_len: 3000 # max length for frames\n          subsampling_factor: 1\n          preprocess_conf: null\n        tgt:\n          max_len: 100 # max length for target tokens\n        vocab:\n          type: char # char, bpe, wpm, word\n          size: 3653 # vocab size in vocab_file\n          path: \'/nfs/cold_project/dataset/opensource/librispeech/espnet/egs/hkust/asr1/data/lang_1char/train_nodup_sp_units.txt\' # path to vocab(default: \'vocab\n        batch:\n          batch_size: 32 # number of elements in a training batch\n          batch_bins: 0 # maximum number of bins (frames x dim) in a trainin batch\n          batch_frames_in: 0 # maximum number of input frames in a training batch\n          batch_frames_out: 0 # maximum number of output frames in a training batch\n          batch_frames_inout: 0 # maximum number of input+output frames in a training batch\n          batch_strategy: auto # strategy to count maximum size of batch(support 4 values: ""auto"", ""seq"", ""frame"", ""bin"")\n        batch_mode: false # ture, user control batch; false, `generate` will yeild one example \n        num_parallel_calls: 12\n        num_prefetch_batch: 2\n        shuffle_buffer_size: 200000\n        need_shuffle: true\n        sortagrad: true\n        batch_sort_key: \'input\' # shuffle, input, output for asr and tts, and sortagrad for asr\n        num_batches: 0 # for debugging\n\n    model:\n      name: CTCAsrModel\n      type: keras # raw, keras or eager model\n      net:\n        structure:\n          encoder:\n            name:\n            filters: # equal number of cnn layers\n            - 128\n            - 512\n            - 512\n            filter_size: # equal number of cnn layers\n            - [5, 3]\n            - [5, 3]\n            - [5, 3]\n            filter_stride: # equal number of cnn layers\n            - [1, 1]\n            - [1, 1]\n            - [1, 1]\n            pool_size: # equal number of cnn layers\n            - [4, 4]\n            - [1, 2]\n            - [1, 2]\n            num_filters: 128\n            linear_num: 786 # hidden number of linear layer\n            cell_num: 128 # cell units of the lstm\n            hidden1: 64 # number of hidden units of fully connected layer\n            attention: false # whether to use attention, false mean use max-pooling\n            attention_size: 128 # attention_size\n            use_lstm_layer: false # whether to use lstm layer, false mean no lstm layer\n            use_dropout: true # whether to use bn, dropout layer\n            dropout_rate: 0.2\n            use_bn: true # whether to use bn, dropout layer\n          decoder:\n            name: \n          attention:\n            name:\n    solver:\n      name: AsrSolver\n      adversarial:\n        enable: false # whether to using adversiral training\n        adv_alpha: 0.5 # adviseral alpha of loss\n        adv_epslion: 0.1 # adviseral example epslion\n      model_average:\n        enable: false # use average model\n        var_avg_decay: 0.99 # the decay rate of varaibles\n      distilling:\n        enable: false \n        name : Teacher\n        loss : DistillationLoss\n        temperature: 5\n        alpha: 0.5\n        teacher_model: null # fronzen_graph.pb \n      optimizer:\n        name: adam\n        epochs: 5 # maximum epochs\n        loss: CTCLoss \n        label_smoothing: 0.0 # label smoothing rate\n        learning_rate:\n          rate: 0.0001 # learning rate of Adam optimizer\n          type:  exp_decay # learning rate type\n          decay_rate: 0.99  # the lr decay rate\n          decay_steps: 100  # the lr decay_step for optimizer\n        clip_global_norm: 3.0 # clip global norm\n        early_stopping: # keras early stopping\n          enable: true\n          monitor: val_loss\n          min_delta: 0\n          patience: 5\n      metrics:\n        pos_label: 1 # int, same to sklearn\n        cals:\n        - name: AccuracyCal\n          arguments: null \n        - name: ConfusionMatrixCal\n          arguments: null\n        - name: PrecisionCal\n          arguments:\n            average: \'binary\'\n        - name: RecallCal\n          arguments:\n            average: \'binary\'\n        - name: F1ScoreCal\n          arguments:\n            average: \'binary\'\n      postproc:\n          enbale: false\n          name: EmoPostProc\n          log_verbose: false \n          eval: true # compute metrics\n          infer: true  # get predict results\n          pred_path: null # None for `model_path`/infer, dumps infer output to this dir\n          thresholds:\n              - 0.5\n          smoothing:\n              enable: true\n              count: 2\n      saver:\n        model_path: ""ckpt/asr-seq/test""\n        max_to_keep: 10\n        save_checkpoints_steps: 100\n        keep_checkpoint_every_n_hours: 10000\n        checkpoint_every: 100 # the step to save checkpoint\n        summary: false\n        save_summary_steps: 100\n        eval_on_dev_every_secs: 1\n        print_every: 10\n        resume_model_path: """"\n      run_config:\n        debug: false # use tfdbug\n        tf_random_seed: null # 0-2**32; null is None, try to read data from /dev/urandom if available or seed from the clock otherwise\n        allow_soft_placement: true\n        log_device_placement: false\n        intra_op_parallelism_threads: 10\n        inter_op_parallelism_threads: 10\n        allow_growth: true\n        log_step_count_steps: 100 #The frequency, in number of global steps, that the global step/sec and the loss will be logged during training.\n      run_options:\n        trace_level: 3 # 0: no trace, 1: sotware trace, 2: hardware_trace, 3: full trace\n        inter_op_thread_pool: -1\n        report_tensor_allocations_upon_oom: true\n    \n    serving:\n      enable: false \n      name : Evaluate\n      model: null # saved model dir, ckpt dir, or frozen_model.pb\n      inputs: \'inputs:0\'\n      outpus: \'softmax_output:0\'   \n    \'\'\'\n    import_all_modules_for_register()\n    tempdir = self.get_temp_dir()\n\n    config_path = str(Path(tempdir).joinpath(""asr_seq.yaml""))\n    logging.info(""config path: {}"".format(config_path))\n    with open(config_path, \'w\', encoding=\'utf-8\') as f:  #pylint: disable=invalid-name\n      f.write(self.conf_str)\n\n    self.config = utils.load_config(config_path)\n    self.mode = utils.TRAIN\n    self.batch_size = 4\n    self.config[\'solver\'][\'optimizer\'][\'batch_size\'] = self.batch_size\n\n    #generate dummpy data\n    nexamples = 10\n    generate_json_data(self.config, self.mode, nexamples)\n\n  def test_generate_data(self):\n    for batch_mode in [True, False]:\n      task_name = self.config[\'data\'][\'task\'][\'name\']\n      self.config[\'data\'][\'task\'][\'batch_mode\'] = batch_mode\n      self.config[\'data\'][\'task\'][\'dummy\'] = False\n      task = registers.task[task_name](self.config, self.mode)\n\n      with self.cached_session(use_gpu=False, force_gpu=False):\n        for uttid, feats, src_lens, targets, tgt_lens in task.generate_data():\n          logging.debug(\'uttid : {}\'.format(uttid))\n          logging.debug(""feats : {}, shape : {}"".format(feats, feats.shape))\n          logging.debug(""targets : {}, shape : {}"".format(\n              targets, targets.shape))\n          logging.debug(\'src_len : {}\'.format(src_lens))\n          logging.debug(\'tgt_len : {}\'.format(tgt_lens))\n          self.assertDTypeEqual(feats, np.float32)\n          self.assertDTypeEqual(src_lens, np.int64)\n          self.assertDTypeEqual(targets, np.int64)\n          self.assertDTypeEqual(tgt_lens, np.int64)\n\n          if batch_mode:\n            self.assertEqual(len(uttid.shape), 1)\n            self.assertEqual(len(feats.shape), 3)\n            self.assertEqual(len(targets.shape), 2)\n            self.assertEqual(len(src_lens.shape), 1)\n            self.assertEqual(len(tgt_lens.shape), 1)\n          else:\n            self.assertEqual(tf.rank(uttid).numpy(), 0)\n            self.assertEqual(len(feats.shape), 2)\n            self.assertEqual(len(targets.shape), 1)\n            self.assertEqual(tf.rank(src_lens).numpy(), 0)\n            self.assertEqual(tf.rank(tgt_lens).numpy(), 0)\n\n  def test_dataset(self):\n    for batch_mode in [True, False]:\n      task_name = self.config[\'data\'][\'task\'][\'name\']\n      self.config[\'data\'][\'task\'][\'batch_mode\'] = batch_mode\n      self.config[\'data\'][\'task\'][\'dummy\'] = False\n      task = registers.task[task_name](self.config, self.mode)\n\n      with self.cached_session(use_gpu=False, force_gpu=False):\n        for features, labels in task.dataset(\n            self.mode, self.batch_size, epoch=1):  # pylint: disable=bad-continuation\n          logging.debug(""feats : {} : {}"".format(features[\'inputs\'],\n                                                 features[\'inputs\'].shape))\n          logging.debug(""ilens : {} : {}"".format(\n              features[\'input_length\'], features[\'input_length\'].shape))\n          logging.debug(""targets : {} : {}"".format(features[\'targets\'],\n                                                   features[\'targets\'].shape))\n          logging.debug(""olens : {} : {}"".format(\n              features[\'target_length\'], features[\'target_length\'].shape))\n          logging.debug(""ctc : {}, shape : {}"".format(labels[\'ctc\'],\n                                                      labels[\'ctc\'].shape))\n          self.assertDTypeEqual(features[\'inputs\'], np.float32)\n          self.assertDTypeEqual(features[\'targets\'], np.int32)\n          self.assertDTypeEqual(features[\'input_length\'], np.int32)\n          self.assertDTypeEqual(features[\'target_length\'], np.int32)\n\n          self.assertEqual(len(features[\'inputs\'].shape), 4)\n          self.assertEqual(len(features[\'input_length\'].shape), 1)\n          self.assertEqual(len(features[\'targets\'].shape), 2)\n          self.assertEqual(len(features[\'target_length\'].shape), 1)\n\n  def test_dummy_dataset(self):\n    for batch_mode in [True, False]:\n      task_name = self.config[\'data\'][\'task\'][\'name\']\n      self.config[\'data\'][\'task\'][\'batch_mode\'] = batch_mode\n      self.config[\'data\'][\'task\'][\'dummy\'] = True\n      task = registers.task[task_name](self.config, self.mode)\n\n      with self.cached_session(use_gpu=False, force_gpu=False):\n        for _ in task.dataset(self.mode, self.batch_size, epoch=1):\n          break\n        for features, labels in task.dataset(\n            self.mode, self.batch_size, epoch=1):  # pylint: disable=bad-continuation\n          logging.debug(""feats : {} : {}"".format(features[\'inputs\'],\n                                                 features[\'inputs\'].shape))\n          logging.debug(""ilens : {} : {}"".format(\n              features[\'input_length\'], features[\'input_length\'].shape))\n          logging.debug(""targets : {} : {}"".format(features[\'targets\'],\n                                                   features[\'targets\'].shape))\n          logging.debug(""olens : {} : {}"".format(\n              features[\'target_length\'], features[\'target_length\'].shape))\n          logging.debug(""ctc : {}, shape : {}"".format(labels[\'ctc\'],\n                                                      labels[\'ctc\'].shape))\n          self.assertDTypeEqual(features[\'inputs\'], np.float32)\n          self.assertDTypeEqual(features[\'targets\'], np.int32)\n          self.assertDTypeEqual(features[\'input_length\'], np.int32)\n          self.assertDTypeEqual(features[\'target_length\'], np.int32)\n\n          self.assertEqual(len(features[\'inputs\'].shape), 4)\n          self.assertEqual(len(features[\'input_length\'].shape), 1)\n          self.assertEqual(len(features[\'targets\'].shape), 2)\n          self.assertEqual(len(features[\'target_length\'].shape), 1)\n\n\nif __name__ == ""__main__"":\n  logging.set_verbosity(logging.INFO)\n  tf.enable_eager_execution()\n  tf.test.main()\n'"
delta/data/task/base_speech_task.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Base Speech Task\'\'\'\nfrom delta import utils\nfrom delta.data import utils as data_utils\nfrom delta.data.task.base_task import WavSpeechTask\n\n#pylint: disable=abstract-method\n\n\nclass SpeechTask(WavSpeechTask):\n  \'\'\' base class for speech task\'\'\'\n\n  def __init__(self, config, mode):\n    super().__init__(config)\n    assert mode in (utils.TRAIN, utils.EVAL, utils.INFER)\n    self._mode = mode\n\n  @property\n  def mode(self):\n    return self._mode\n\n  #pylint: disable=arguments-differ\n  def input_fn(self, mode, batch_size, num_epoch=None):\n    \'\'\' estimator input_fn\'\'\'\n    return data_utils.input_fn(self.dataset, mode, batch_size, num_epoch)\n'"
delta/data/task/base_task.py,1,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Task abstract class for data process\'\'\'\nimport abc\n\n\nclass Task(metaclass=abc.ABCMeta):\n  \'\'\' abstract class\'\'\'\n\n  def __init__(self, config):\n    self._config = config\n\n  @property\n  def config(self):\n    \'\'\' config property\'\'\'\n    return self._config\n\n  @abc.abstractmethod\n  def generate_data(self):\n    \'\'\' generate one example\'\'\'\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def feature_spec(self):\n    \'\'\' dataset meta data\'\'\'\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def preprocess_batch(self, batch):\n    \'\'\' pre-proecss of data\'\'\'\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def dataset(self):\n    \'\'\' generate batch examples with epoch\n    return tf.data.Dataset\n    \'\'\'\n    return NotImplementedError()\n\n  @abc.abstractmethod\n  def input_fn(self):\n    \'\'\' return `def _input_fn()` function\'\'\'\n    return NotImplementedError()\n\n\nclass WavSpeechTask(Task):\n  \'\'\' Speech task which need generate feat and cmvn\'\'\'\n\n  @abc.abstractmethod\n  def generate_feat(self, paths):\n    \'\'\'\n        generate features,\n           paths: list or tuple\n        \'\'\'\n    return NotImplementedError()\n\n  @abc.abstractmethod\n  def generate_cmvn(self, paths):\n    \'\'\'\n        generate mean and vars of features,\n            paths: list or tuple\n        \'\'\'\n    return NotImplementedError()\n'"
delta/data/task/base_text_task.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Base task for NLP.""""""\n\nimport os\nfrom absl import logging\n\nfrom delta.data.task.base_task import Task\nfrom delta import utils\nfrom delta.utils.register import registers\nfrom delta.data.preprocess.text_ops import clean_english_str_tf\nfrom delta.data.preprocess.text_ops import char_cut_tf\nfrom delta.data.preprocess.text_ops import tokenize_sentence\nfrom delta.data.preprocess.text_ops import chinese_word_cut_tf\n\n# pylint: disable=abstract-method\n\n\nclass TextTask(Task):\n  """"""Base task for NLP.""""""\n\n  def __init__(self, config, mode):\n    super().__init__(config)\n    self.all_modes = [utils.TRAIN, utils.EVAL, utils.INFER]\n    assert mode in self.all_modes\n    self.preparer = None\n    self.use_preparer = True\n    self.mode = mode\n    self.model_config = config[""model""]\n    self.data_config = config[\'data\']\n    self.task_config = self.data_config[\'task\']\n\n    self.infer_no_label = self.data_config[utils.INFER].get(\n        \'infer_no_label\', False)\n    if self.mode == utils.INFER and self.infer_no_label:\n      self.infer_without_label = True\n    else:\n      self.infer_without_label = False\n\n    self.batch_size = self.task_config[\'batch_size\']\n    self.epochs = self.task_config[\'epochs\']\n    self.num_parallel_calls = self.task_config[\'num_parallel_calls\']\n    self.num_prefetch_batch = self.task_config[\'num_prefetch_batch\']\n    self.shuffle_buffer_size = self.task_config[\'shuffle_buffer_size\']\n    self.need_shuffle = self.task_config[\'need_shuffle\']\n\n  def input_fn(self):\n\n    def _input_fn():\n      return self.dataset()\n\n    return _input_fn\n\n  def preprocess_batch(self, batch):\n    """"""\n    Pre-process batch.\n    This function is not used in all nlp tasks.\n    """"""\n    return batch\n\n  def pre_process_pipeline(self, input_sentences):\n    """"""Data pipeline function for pre-processing.""""""\n    language = self.task_config[""language""]\n    clean_english = self.task_config.get(""clean_english"", False)\n    split_by_space = self.task_config.get(""split_by_space"", False)\n    use_word = self.task_config.get(""use_word"", False)\n\n    if language == ""english"":\n      if clean_english:\n        batch = clean_english_str_tf(input_sentences)\n      else:\n        batch = input_sentences\n    else:\n      if split_by_space:\n        batch = input_sentences\n      else:\n        if use_word:\n          batch = chinese_word_cut_tf(input_sentences)\n        else:\n          batch = char_cut_tf(input_sentences)\n    return batch\n\n  def common_process_pipeline(self, batch):\n    """"""\n    Data pipeline function for common process.\n    This function is used both by online training and offline inference.\n    """"""\n    text_vocab_file_path = self.task_config[\'text_vocab\']\n    max_seq_len = self.task_config[\'max_seq_len\']\n    vocab_path = os.path.abspath(text_vocab_file_path)\n    token_ids = tokenize_sentence(batch, max_seq_len, vocab_path)\n    return token_ids\n\n  def get_input_pipeline(self, for_export):\n    """"""Get the input pipeline function.""""""\n\n    def input_pipeline_func(input_sentences):\n      """"""This input pipeline function will be used in online inference.""""""\n      if for_export:\n        input_sentences = self.pre_process_pipeline(input_sentences)\n      batch = self.common_process_pipeline(input_sentences)\n      return batch\n\n    return input_pipeline_func\n\n  def set_preparer(self):\n    """"""Set the preparer""""""\n    if \'preparer\' not in self.config[\'data\'][\'task\']:\n      self.use_preparer = False\n    else:\n      self.use_preparer = self.config[\'data\'][\'task\'][\'preparer\'][""enable""]\n    if self.use_preparer:\n      preparer_name = self.config[\'data\'][\'task\'][\'preparer\'][""name""]\n      self.preparer = registers.preparer[preparer_name](self.config)\n\n  def prepare(self):\n    """"""\n    Do all steps for pre-processing,\n    including putting data to the pre-process pipeline,\n    preparing the vocabulary file and preparing the embedding file.\n    """"""\n    self.set_preparer()\n    if not self.use_preparer:\n      logging.info(""This task do not has a Preparer."")\n      return\n    if self.preparer.skip_prepare():\n      logging.info(""Skip the Preparing process."")\n      return\n\n    self.preparer.do_prepare(self.pre_process_pipeline)\n    self.preparer.done_prepare()\n'"
delta/data/task/kws_cls_task.py,7,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' kws task \'\'\'\n# data format see: docs/data/kws.md\nimport struct\nimport numpy as np\nimport delta.compat as tf\n\nfrom delta import utils\nfrom delta.utils.register import registers\nfrom delta.data.task.base_speech_task import SpeechTask\nfrom delta.data.utils.htk_reader_lib import HtkReaderIO\n\n\n@registers.task.register\nclass KwsClsTask(SpeechTask):\n  \'\'\' kws task \'\'\'\n\n  #pylint: disable=too-many-instance-attributes\n  def __init__(self, config, mode):\n    super().__init__(config, mode)\n    self.epoch = 0\n    self.num = 0\n    self.reader = HtkReaderIO()\n\n    self.window_len = config[\'data\'][\'task\'][\'audio\'][\'window_len\']\n    self.window_shift = config[\'data\'][\'task\'][\'audio\'][\'window_shift\']\n    self.cmvn_path = config[\'data\'][\'task\'][\'audio\'][\'cmvn_path\']\n\n    self.left_context = config[\'data\'][\'task\'][\'audio\'][\'left_context\']\n    self.right_context = config[\'data\'][\'task\'][\'audio\'][\'right_context\']\n    self.delta_order = config[\'data\'][\'task\'][\'audio\'][\'delta_order\']\n    self.delta_wind = config[\'data\'][\'task\'][\'audio\'][\'delta_wind\']\n    self.splice_frame = config[\'data\'][\'task\'][\'audio\'][\'splice_frame\']\n\n    feat_dim = config[\'data\'][\'task\'][\'audio\'][\'feat_dim\']\n    if self.splice_frame:\n      feat_dim = config[\'data\'][\'task\'][\'audio\'][\'feat_dim\'] * (\n          self.left_context + 1 + self.right_context)\n    self.final_feat_dim = feat_dim * (self.delta_order + 1)\n\n    if mode == utils.TRAIN:\n      self.lines = open(config[\'data\'][\'train\'][\'paths\']).readlines()\n    else:\n      self.lines = open(config[\'data\'][\'eval\'][\'paths\']).readlines()\n\n  def generate_feat(self, paths):\n    \'\'\' generate feature\'\'\'\n\n  def generate_cmvn(self, paths):\n    \'\'\' generate cmvn \'\'\'\n\n  #pylint: disable=too-many-locals\n  def generate_data(self):\n    \'\'\'\n    train.list file:\n      /path/to/10w.42.feat\n      /path/to/10w.42.label\n      ./10w.42.desc\n      /path/to/train.7.feat\n      /path/to/train.7.label\n      ./train.7.desc\n    \'\'\'\n\n    for i in range(0, len(self.lines), 3):\n      fp_feat = open(self.lines[i].strip(), \'rb\')\n      buff = open(self.lines[i + 1].strip(), \'rb\').read()\n      # label is 0 ~ 8,\n      # one label per frame\n      label_arr = struct.unpack(\'%di\' % (len(buff) / 4), buff)  # 570485\n      #desc_lines = open(self.lines[i + 2].strip()).readlines()[1:]\n\n      # read file header, frame_bytes is 160 Bytes, 40 dimensions\n      num_frames, _, frame_bytes, _ = struct.unpack(\'!%di%dh\' % (2, 2),\n                                                    fp_feat.read(12))\n      del num_frames\n      buff = fp_feat.read()  # file body\n      fp_feat.close()\n\n      # ! means converting Big-Endian to Little-Endian\n      feat_all = struct.unpack(\'!%df\' % (len(buff) / 4), buff)\n      feat_matrix = np.array(feat_all).reshape(\n          (-1, int(frame_bytes / 4)))  # (570485, 40) (frame_num, feat_dim)\n\n      #num, bad = 0, 0\n      length = feat_matrix.shape[0] - self.window_len  #  281508\n      for j in range(0, length, self.window_shift):\n        label_t = np.unique(label_arr[j:j + self.window_len])\n        if -1 in label_t:\n          # reduce the ratio of negative samples\n          continue\n        if len(label_t) > 2 and len(label_t) < 8:\n          continue\n\n        feat = feat_matrix[j:j + self.window_len]\n        _, feat = self.reader.add_delta(feat, self.delta_order, self.delta_wind)\n        # cmvn is 120 lines, each line has mean and variance\n        _, feat = self.reader.normalization_feat_by_mean_variance(\n            feat, self.cmvn_path)\n        if self.splice_frame:\n          _, feat = self.reader.splice_frames(feat, self.left_context,\n                                              self.left_context)\n        if set(label_t).issuperset(range(0, 8)):\n          # including keyword\n          label = 1\n        else:\n          label = 0\n\n        yield feat, label\n\n  def feature_spec(self):\n    \'\'\' data meta\'\'\'\n    output_shapes = (tf.TensorShape([self.window_len,\n                                     self.final_feat_dim]), tf.TensorShape([]))\n    output_types = (tf.float32, tf.int32)\n    return output_shapes, output_types\n\n  def preprocess_batch(self, batch):\n    \'\'\' preprocess of data\'\'\'\n    return batch\n\n  #pylint: disable=arguments-differ\n  def dataset(self, mode, batch_size, epoch):\n    \'\'\' make tf dataset\'\'\'\n    shapes, types = self.feature_spec()\n    ds = tf.data.Dataset.from_generator(  #pylint: disable=invalid-name\n        generator=lambda: self.generate_data(),  #pylint: disable=unnecessary-lambda\n        output_types=types,\n        output_shapes=shapes,\n    )\n\n    if mode == utils.TRAIN:\n      ds = ds.apply(  #pylint: disable=invalid-name\n          tf.data.experimental.shuffle_and_repeat(\n              buffer_size=batch_size, count=epoch, seed=None))\n\n    def make_sample(feat, label):\n      return {""inputs"": feat, ""labels"": label}, label\n\n    return ds.apply(\n        tf.data.experimental.map_and_batch(\n            make_sample, batch_size,\n            drop_remainder=False)).prefetch(tf.data.experimental.AUTOTUNE)\n'"
delta/data/task/kws_cls_task_test.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' kws task unittest\'\'\'\nimport os\nfrom pathlib import Path\nimport delta.compat as tf\nfrom absl import logging\n\nfrom delta import utils\nfrom delta.utils.register import registers\nfrom delta.utils.register import import_all_modules_for_register\n\n\nclass KwsClsTaskTest(tf.test.TestCase):\n  \'\'\' kws task test\'\'\'\n\n  def setUp(self):\n    super().setUp()\n    import_all_modules_for_register()\n    \'\'\'\n    package_root = Path(PACKAGE_ROOT_DIR)\n    config_file = main_root.joinpath(\'delta/config/kws-cls/kws_speech_cls.yml\')\n    config = utils.load_config(config_file)\n\n    solver_name = config[\'solver\'][\'name\']\n    self.solver = registers.solver[solver_name](config)\n\n    # config after process\n    self.config = self.solver.config\n\n    self.mode = utils.EVAL\n\n    task_name = self.config[\'data\'][\'task\'][\'name\']\n    self.task = registers.task[task_name](self.config, self.mode)\n\n    self.dataset = self.task.dataset(self.mode, 25, 0)\n    self.iterator = self.dataset.make_one_shot_iterator()\n    self.one_element = self.iterator.get_next()\n    \'\'\'\n\n  def tearDown(self):\n    \'\'\' tear down \'\'\'\n\n  def test_dataset(self):\n    \'\'\' dataset unittest\'\'\'\n    pass\n    \'\'\'\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      for _ in range(2):\n        output = sess.run(self.one_element)\n        logging.info(output)\n        logging.info(""output: {} {}"".format(output.shape, output.dtype))\n    \'\'\'\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.DEBUG)\n  tf.test.main()\n'"
delta/data/task/speaker_cls_task.py,26,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Speaker Classification. \'\'\'\nimport random\nimport multiprocessing as mp\nfrom absl import logging\nimport numpy as np\nimport delta.compat as tf\nimport kaldiio\nfrom collections import defaultdict\nfrom pathlib import Path\n\nfrom delta import utils\nfrom delta.utils.register import registers\nfrom delta.utils.kaldi.kaldi_dir import KaldiMetaData\nfrom delta.data.task.base_speech_task import SpeechTask\n\n#pylint: disable=too-many-instance-attributes\n#pylint: disable=too-many-arguments\n#pylint: disable=too-many-locals\n#pylint: disable=too-few-public-methods\n#pylint: disable=arguments-differ\n\n\nclass ChunkSampler():\n  \'\'\' Produce samples from utterance. \'\'\'\n\n  def __init__(self, meta, chunk_size):\n    \'\'\'\n      Args:\n      meta: an object of `KaldiMetaData` or None\n      chunk_size: fixed chunk size in frames.\n    \'\'\'\n    self.chunk_size = chunk_size\n    self.pad_chunks = True\n    self.add_random_offset = False\n    self.drop_short_chunks = 0.0\n    self.single_chunk = False\n\n    self.meta = meta\n    self.spk_keys = None\n    self.spk_keys_pointer = 0\n    self.utt_keys = None\n    self.meta_keys_pointer = 0\n    self.select_by_spk = True\n    self.num_repeats = 1\n\n  def copy(self):\n    \'\'\' Return a copy of self, except meta data. \'\'\'\n    new = ChunkSampler(None, self.chunk_size)\n    new.pad_chunks = self.pad_chunks\n    new.add_random_offset = self.add_random_offset\n    new.drop_short_chunks = self.drop_short_chunks\n    new.single_chunk = self.single_chunk\n    new.select_by_spk = self.select_by_spk\n    return new\n\n  def get_bare_sampler(self):\n    \'\'\' Return a new sampler with no meta data. \'\'\'\n    new = self.copy()\n    return new.utt_to_samples\n\n  def utt_to_samples(self, args):\n    \'\'\'\n    Utt to samples of (feat_chunk, spk_id, sample_key).\n    Will be run in a process pool so restrictions apply.\n    \'\'\'\n    result_queue, utt_info = args\n    logging.debug(utt_info)\n\n    # TODO: wrap into a function or something\n    utt_key, utt_meta = utt_info\n\n    # Load features and select voiced frames.\n    feat_scp = utt_meta[\'feat\']\n    feat_mat = kaldiio.load_mat(feat_scp)\n    num_frames_feat, feat_dim = feat_mat.shape\n    vad_scp = utt_meta[\'vad\']\n\n    if vad_scp:\n      vad_mat = kaldiio.load_mat(vad_scp)\n      num_frames_vad = vad_mat.shape[0]\n      logging.debug(\'feat_mat: %s, vad_mat: %s\' %\n                    (str(feat_mat.shape), str(vad_mat.shape)))\n      if num_frames_feat != num_frames_vad:\n        logging.debug(\'num_frames_feat != num_frames_vad: %d vs %d\' %\n                      (num_frames_feat, num_frames_vad))\n        return None\n      voiced_frames_index = np.where(vad_mat == 1)[0]\n      logging.debug(\'voiced_frames_index: %s\' %\n                    (str(voiced_frames_index.shape)))\n      feat_mat_voiced = feat_mat[voiced_frames_index, :]\n    else:\n      # If no VAD info was found, the entire utt will be used.\n      feat_mat_voiced = feat_mat\n    num_frames_voiced = feat_mat_voiced.shape[0]\n    logging.debug(\'feat_mat_voiced: %s\' % (str(feat_mat_voiced.shape)))\n\n    spk_id = utt_meta[\'spkid\']\n\n    logging.debug(\'Chunk size: %d\' % (self.chunk_size))\n\n    results = []\n    chunk_idx = 0\n    if self.add_random_offset:\n      random_offset = np.random.randint(0, self.chunk_size)\n    else:\n      random_offset = 0\n    for offset in range(random_offset, num_frames_voiced, self.chunk_size):\n      if self.single_chunk:\n        available = num_frames_voiced - self.chunk_size\n        if available < 0:\n          # No padding.\n          logging.warn(\'Single chunk mode: available < 0.\')\n          break\n        offset = random.randint(0, available)\n      logging.debug(\'offset = %d\' % (offset))\n      feat_chunk = feat_mat_voiced[offset:offset + self.chunk_size, :]\n      unpadded_frames = feat_chunk.shape[0]\n      if self.pad_chunks and unpadded_frames < self.chunk_size:\n        rel_chunk_len = float(unpadded_frames) / self.chunk_size\n        if rel_chunk_len < self.drop_short_chunks:\n          continue\n        logging.debug(\'Padding chunk of frames %d ...\' % (unpadded_frames))\n        padded = np.zeros((self.chunk_size, feat_dim), dtype=feat_chunk.dtype)\n        padded[:unpadded_frames, :] = feat_chunk\n        feat_chunk = padded\n      feat_chunk = np.expand_dims(feat_chunk, axis=2)  # TODO: not here\n      sample_key = \'%s_chunk%02d\' % (utt_key, chunk_idx)\n      sample = (feat_chunk, spk_id, sample_key)\n      chunk_idx += 1\n      results.append(sample)\n      if self.single_chunk:\n        break\n    if result_queue:\n      # queue mode\n      result_queue.put(results)\n      return None\n    # imap mode\n    return results\n\n  def reset(self):\n    \'\'\' Reset progress. \'\'\'\n    if self.select_by_spk:\n      logging.info(\'Shuffling spk keys ...\')\n      self.spk_keys = []\n      for index in range(self.num_repeats):\n        temp_keys = list(self.meta.spks.keys())\n        random.shuffle(temp_keys)\n        self.spk_keys.extend(temp_keys)\n    else:\n      logging.info(\'Shuffling utt keys ...\')\n      self.utt_keys = []\n      for index in range(self.num_repeats):\n        temp_keys = list(self.meta.utts.keys())\n        random.shuffle(temp_keys)\n        self.utt_keys.extend(temp_keys)\n\n  def next_part_utts(self, part_size):\n    \'\'\' Return next part of utts. \'\'\'\n    if self.select_by_spk:\n      # Select utterances by speaker.\n      for index in range(part_size):\n        try:\n          spk = self.spk_keys.pop()\n        except IndexError:\n          break\n        spk_utts = self.meta.spks[spk].utts\n        if not spk_utts:\n          # None (e.g. with external id mapping) or len == 0\n          continue\n        random_idx = random.randint(0, len(spk_utts) - 1)\n        if random_idx < 0:\n          # Not likely.\n          continue\n        utt_key = spk_utts[random_idx]\n        utt_meta = self.meta.utts[utt_key]\n        yield (utt_key, utt_meta)\n    else:\n      # Randomly selects utterances.\n      for index in range(part_size):\n        try:\n          utt_key = self.utt_keys.pop()\n        except IndexError:\n          break\n        utt_meta = self.meta.utts[utt_key]\n        yield (utt_key, utt_meta)\n\n\nclass DataQueueAsync():\n  \'\'\' Sample from raw data. \'\'\'\n\n  def __init__(self, meta, sampler, num_processes=None, max_qsize=20):\n    \'\'\'\n    Params:\n      num_processes: number of IO processes. Default = None (# CPU cores).\n      max_qsize: multiprocessing.Queue(maxsize=max_qsize).\n      method: queue(use manager.Queue), async_result(AsyncResult.get())\n    \'\'\'\n\n  def start(self):\n    \'\'\' Start sampling async. \'\'\'\n\n  def get_items(self):\n    \'\'\' Get a generator of results. \'\'\'\n\n\nclass ManagerQueueDataQueue(DataQueueAsync):\n  \'\'\'\n  A DataQueueAsync implementation using multiprocessing.Manager to\n  establish a Queue between Pool workers and host process.\n  Its efficiency is quite low compared to the other implementation.\n  And speed is even lower than a local single thread implementation.\n  THe bottleneck may be the Queue constantly locking and blocking.\n  \'\'\'\n\n  def __init__(self, meta, sampler, num_processes=None, max_qsize=20):\n    super().__init__(meta, sampler, num_processes, max_qsize)\n    self.meta = meta\n    self.sampler = sampler\n    self.pool = None\n    self.pool_res = None\n    self.num_processes = num_processes\n    self.mp_manager = mp.Manager()\n    self.mp_queue = self.mp_manager.Queue(max_qsize)\n\n  def start(self):\n    \'\'\' Start sampling async. \'\'\'\n    self.pool = mp.Pool(self.num_processes)\n    pool_args = [(self.mp_queue, x) for x in self.meta.utts.items()]\n    self.pool_res = self.pool.map_async(\n        self.sampler.get_bare_sampler(), pool_args, chunksize=100)\n    self.pool.close()\n\n  def done(self):\n    \'\'\' Whether all data have been consumed. \'\'\'\n    return self.pool_res.ready() and self.mp_queue.qsize() == 0\n\n  def get_items(self):\n    \'\'\' Get a generator of results. \'\'\'\n    while not self.done():\n      yield self.mp_queue.get()\n\n\nclass ImapUnorderedDataQueue(DataQueueAsync):\n  \'\'\'\n  Use async imap() calls to fetch partial results.\n  Much faster than the Manager-based one.\n  \'\'\'\n\n  def __init__(self, meta, sampler, num_processes=None, max_qsize=20):\n    super().__init__(meta, sampler, num_processes, max_qsize)\n    self.meta = meta  # TODO: remove\n    self.sampler = sampler\n    self.pool = None\n    self.pool_chunk_size = 1000\n    self.pool_res = None\n    self.num_processes = num_processes\n\n  def start(self):\n    \'\'\' Start sampling async. \'\'\'\n    self.pool = mp.Pool(self.num_processes)\n\n  def append_requests(self):\n    \'\'\'\n    Append some requests to pool.\n\n    Returns:\n      a bool, True if still has unconsumed data, False otherwise.\n    \'\'\'\n    pool_args = [\n        (None, x) for x in self.sampler.next_part_utts(self.pool_chunk_size)\n    ]\n    self.pool_res = self.pool.imap_unordered(\n        self.sampler.get_bare_sampler(), pool_args, chunksize=100)\n    if pool_args:\n      return True\n    else:\n      return False\n\n  def get_items(self):\n    \'\'\' Get a generator of results. \'\'\'\n    while True:\n      has_remaining = self.append_requests()\n      for item in self.pool_res:\n        yield item\n      if not has_remaining:\n        break\n    self.pool.close()\n\n\n@registers.task.register\nclass SpeakerClsTask(SpeechTask):\n  \'\'\' Speaker Classification Task \'\'\'\n\n  def __init__(self, config, mode):\n    super().__init__(config, mode)\n    self.dataconf = self.config[\'data\']\n    self.taskconf = self.dataconf[\'task\']\n    self.solverconf = self.config[\'solver\']\n\n    self.data_type = self.taskconf[\'data_type\']\n\n    if \'whole_utt_inference\' in self.taskconf[\'audio\']:\n      self.whole_utt_inference = self.taskconf[\'audio\'][\'whole_utt_inference\']\n    else:\n      self.whole_utt_inference = False\n\n    if \'add_random_offset\' in self.taskconf[\'audio\']:\n      self.add_random_offset = self.taskconf[\'audio\'][\'add_random_offset\']\n    else:\n      self.add_random_offset = False\n\n    if \'drop_short_chunks\' in self.taskconf[\'audio\']:\n      self.drop_short_chunks = self.taskconf[\'audio\'][\'drop_short_chunks\']\n    else:\n      self.drop_short_chunks = 0.0\n\n    if \'single_chunk\' in self.taskconf[\'audio\']:\n      self.single_chunk = self.taskconf[\'audio\'][\'single_chunk\']\n    else:\n      self.single_chunk = 0.0\n\n    if \'select_by_spk_train\' in self.taskconf[\'audio\']:\n      self.select_by_spk_train = self.taskconf[\'audio\'][\'select_by_spk_train\']\n    else:\n      self.select_by_spk_train = False\n\n    if \'select_by_spk_eval\' in self.taskconf[\'audio\']:\n      self.select_by_spk_eval = self.taskconf[\'audio\'][\'select_by_spk_eval\']\n    else:\n      self.select_by_spk_eval = False\n\n    if \'num_repeats\' in self.taskconf[\'audio\']:\n      self.num_repeats = self.taskconf[\'audio\'][\'num_repeats\']\n    else:\n      self.num_repeats = 1\n\n    self.chunk_size_seconds = self.taskconf[\'audio\'][\'clip_size\']\n    # TODO: configurable frame rate\n    self.chunk_size_frames = self.chunk_size_seconds * 100\n    self.feature_dims = self.taskconf[\'audio\'][\'feature_size\']\n    # TODO: delta features\n    if self.mode == utils.INFER and self.whole_utt_inference:\n      self.feature_shape = (None, self.feature_dims, 1)\n    else:\n      self.feature_shape = (self.chunk_size_frames, self.feature_dims, 1)\n    self.feature_shape = (None, self.feature_dims, 1)\n\n    # TODO: not implemented\n    self.uniform_resample = False\n\n    # 10k sample/utts is somehow enough.\n    self.cmvn_max_samples = 10000\n\n    self._cmvn_path = self.taskconf[\'audio\'][\'cmvn_path\']\n\n    # meta data\n    self.meta = KaldiMetaData()\n    self._classes = {}\n\n    logging.info(\'Loading meta data ...\')\n    self.load_meta_data()\n\n    if self.mode == utils.INFER and self.whole_utt_inference:\n      # Do whole utterance inference.\n      logging.info(\'Set chunk_size = 10M and padding = False.\')\n      self.sampler = ChunkSampler(self.meta, 10000000)\n      self.sampler.pad_chunks = False\n    else:\n      self.sampler = ChunkSampler(self.meta, self.chunk_size_frames)\n\n    if self.mode != utils.INFER and self.add_random_offset:\n      self.sampler.add_random_offset = True\n\n    if self.mode != utils.INFER and self.drop_short_chunks > 0.0:\n      logging.info(\'Dropping chunks < %f .\' % (self.drop_short_chunks))\n      self.sampler.drop_short_chunks = self.drop_short_chunks\n\n    if self.mode != utils.INFER:\n      logging.info(\'Single chunk sampling enabled.\')\n      self.sampler.single_chunk = self.single_chunk\n\n    if self.mode == utils.TRAIN:\n      logging.info(\'Utt selection by spk enabled for training.\')\n      self.sampler.select_by_spk = self.select_by_spk_train\n    elif self.mode == utils.EVAL:\n      logging.info(\'Utt selection by spk enabled for evaluation.\')\n      self.sampler.select_by_spk = self.select_by_spk_eval\n\n    if self.mode == utils.TRAIN:\n      logging.info(\'Num repeats = %d.\' % (self.num_repeats))\n      self.sampler.num_repeats = self.num_repeats\n\n  def load_meta_data(self):\n    \'\'\' Load meta data. \'\'\'\n    data_paths = self.dataconf[self.mode][\'paths\']\n    logging.info(\'Loading mode %s dirs: %s ...\' % (self.mode, data_paths))\n    if len(data_paths) != 1:\n      raise ValueError(\'More than 1 data dirs is not supported by now.\')\n    for data_path in data_paths:\n      logging.info(\'Loading dir %s ...\' % (data_path))\n      if self.data_type == \'KaldiDataDirectory\':\n        self.meta.load(data_path)\n      else:\n        raise ValueError(\'Unsupported data type: %s\' % (self.data_type))\n    self._classes = self.meta.spk2id\n\n  @property\n  def num_class(self):\n    \'\'\' Return number of classes. \'\'\'\n    return len(self._classes)\n\n  @property\n  def classes(self):\n    \'\'\' Return a map from class names to label ids. \'\'\'\n    return self._classes\n\n  def class_id(self, class_name):\n    \'\'\' Return the numeric label of a given class name. \'\'\'\n    return self._classes[class_name]\n\n  def generate_feat(self, filelist, dry_run=False):\n    \'\'\' Stub. Not implemented because we use Kaldi features. \'\'\'\n\n  def generate_cmvn(self, filelist=None, dry_run=False):  # pylint: disable=unused-argument\n    \'\'\' Generate mean and vars of features. \'\'\'\n    sums, square, count = utils.create_cmvn_statis(\n        self.taskconf[\'audio\'][\'feature_size\'],\n        self.taskconf[\'audio\'][\'add_delta_deltas\'])\n\n    self.sampler.chunk_size = 100000\n    self.sampler.pad_chunks = False\n\n    num_done = 0\n    for inputs, _, _, _, _ in \\\n        self.generate_data():\n      # update stats\n      if inputs.ndim == 3:\n        inputs = np.expand_dims(inputs, axis=0)\n      sums, square, count = utils.update_cmvn_statis(\n          inputs, sums, square, count, axis=(0, 1))\n      num_done += 1\n      if num_done % 100 == 0:\n        logging.info(\'Done %d samples.\' % (num_done))\n      if num_done > self.cmvn_max_samples:\n        break\n    # compute cmvn\n    mean, var = utils.compute_cmvn(sums, square, count)\n    if dry_run:\n      logging.info(\'save cmvn:{}\'.format(self._cmvn_path))\n    else:\n      np.save(self._cmvn_path, (mean, var))\n    logging.info(\'generate cmvn done\')\n    logging.info(mean)\n    logging.info(var)\n\n  #pylint: disable=stop-iteration-return\n  def generate_data(self):\n    \'\'\'\n    Yields samples.\n\n    Args:\n      multiprocess: use multiprocessing. Default = True.\n\n    Yields:\n      (inputs, label, filename, clip_id, soft_label)\n    \'\'\'\n    class_num = self.taskconf[\'classes\'][\'num\']\n\n    def process_sample(sample, clip_id):\n      \'\'\'\n      Pack various info into a tuple, to be furtherly processed by Dataset.\n\n      Args:\n        sample: a clip (or chunk) of an utterance generated previously.\n        clip_id: the index of clip in the entire utterance.\n\n      Returns:\n        a tuple of feature, label and everything else for training.\n      \'\'\'\n      inputs, label, utt_key = sample\n      filename = utt_key\n      clip_id = clip_id\n      soft_label = np.zeros((1,))  # disabled for speaker model\n      return inputs, label, filename, clip_id, soft_label\n\n    if self.mode == utils.INFER:\n      # Estimator.predict might cause multiprocessing to fail.\n      multiprocess = False\n    else:\n      multiprocess = True\n\n    self.sampler.reset()\n\n    if multiprocess:\n      q = ImapUnorderedDataQueue\n      data_queue = q(self.meta, self.sampler, num_processes=4)\n      data_queue.start()\n      for samples in data_queue.get_items():\n        for idx, sample in enumerate(samples):\n          yield process_sample(sample, idx)\n    else:\n      for item in self.meta.utts.items():\n        samples = self.sampler.utt_to_samples((None, item))\n        for idx, sample in enumerate(samples):\n          yield process_sample(sample, idx)\n    raise StopIteration\n\n  def feature_spec(self):\n    output_shapes = (\n        tf.TensorShape(self.feature_shape),  # audio_feat e.g. (3000, 40, 3)\n        tf.TensorShape([]),  # label\n        tf.TensorShape([]),  # filename\n        tf.TensorShape([]),  # clip_id\n        tf.TensorShape([1]),  # soft_label, disabled for speaker model\n    )\n    output_types = (\n        tf.float32,\n        tf.int32,\n        tf.string,\n        tf.int32,\n        tf.float32,\n    )\n    assert len(output_shapes) == len(output_types)\n    return output_shapes, output_types\n\n  def preprocess_batch(self, batch):\n    return batch\n\n  def dataset(self, mode, batch_size, num_epoch):  # pylint: disable=unused-argument\n    shapes, types = self.feature_spec()\n    data = tf.data.Dataset.from_generator(\n        generator=lambda: self.generate_data(),  # pylint: disable=unnecessary-lambda\n        output_types=types,\n        output_shapes=shapes,\n    )\n\n    buffer_size = self.taskconf[\'shuffle_buffer_size\']\n    logging.info(\'Using buffer size of %d samples in shuffle_and_repeat().\' %\n                 (buffer_size))\n    if mode == utils.TRAIN:\n      data = data.shuffle(buffer_size=buffer_size)\n      if self.uniform_resample:\n\n        def class_func(inputs, labels, filenames, clip_ids, soft_labels):\n          \'\'\' Return the label of a sample tuple. \'\'\'\n          return labels\n\n        target_dist = tf.ones((self.num_class,), dtype=tf.float32) / \\\n                      self.num_class\n        data = data.apply(\n            tf.data.experimental.rejection_resample(class_func, target_dist))\n\n    def make_example(inputs, labels, filenames, clip_ids, soft_labels):\n      features = {\n          \'inputs\': inputs,\n          \'labels\': labels,\n          \'filepath\': filenames,\n          \'clipid\': clip_ids,\n          \'soft_labels\': soft_labels,\n      }\n      return features, labels\n\n    if self.mode == utils.INFER and self.whole_utt_inference:\n      # To avoid length difference since padding = False.\n      logging.info(\'Inference mode, set batch_size to 1.\')\n      batch_size = 1\n    return data.map(make_example, num_parallel_calls=10).\\\n                batch(batch_size, drop_remainder=False).\\\n                prefetch(tf.data.experimental.AUTOTUNE)\n\n\nclass KaldiDir:\n\n  def __init__(self, kaldi_dir):\n    self._dir = Path(kaldi_dir)\n\n    feats_scp_file = self._dir.joinpath(\'feats.scp\')\n    spk2utt_file = self._dir.joinpath(\'spk2utt\')\n    utt2spk_file = self._dir.joinpath(\'utt2spk\')\n    spk2id_file = self._dir.joinpath(\'spk2id\')\n\n    self._feats_scp = defaultdict(str)\n    with feats_scp_file.open(mode=\'r\', encoding=\'utf-8\') as scp_reader:\n      for line in scp_reader:\n        line = line.strip().replace(\'\\n\', \'\').replace(\'\\r\',\n                                                      \'\').replace(\'\\t\', \' \')\n        splits = line.split()\n        if len(splits) < 2:\n          continue\n        utt_id = splits[0]\n        utt_path = splits[1]\n        self._feats_scp[utt_id] = utt_path\n    self._num_utt = len(self._feats_scp)\n    logging.info(f""kaldi dir: {self._num_utt} utts"")\n\n    self._spk2utt = defaultdict(list)\n    with spk2utt_file.open(mode=\'r\', encoding=\'utf-8\') as reader:\n      for line in reader:\n        spk, utt_str = line.strip().split(maxsplit=1)\n        utts = utt_str.split()\n        self._spk2utt[spk] = utts\n\n    self._spk2id = defaultdict(lambda: -1)\n    with spk2id_file.open(mode=\'r\', encoding=\'utf-8\') as reader:\n      for line in reader:\n        spk, spk_id = line.strip().split()\n        self._spk2id[spk] = spk_id\n    self._num_spk = len(self._spk2id)\n    logging.info(f""kaldi dir: {self._num_spk} spks"")\n\n    self._utt2spk = defaultdict(str)\n    self._utt2spk_ids = defaultdict(int)\n    with utt2spk_file.open(mode=\'r\', encoding=\'utf-8\') as reader:\n      for line in reader:\n        utt, spk = line.strip().split()\n        self._utt2spk[utt] = spk\n        self._utt2spk_ids[utt] = self._spk2id[spk]\n\n  @property\n  def num_utts(self):\n    return self._num_utt\n\n  @property\n  def num_spks(self):\n    return self._num_spk\n\n  @property\n  def utt2spk(self):\n    return self._utt2spk\n\n  @property\n  def utt2spkid(self):\n    return self._utt2spk_ids\n\n  @property\n  def feats_scp(self):\n    return self._feats_scp\n\n  @property\n  def spk2utt(self):\n    return self._spk2utt\n\n  @property\n  def spk2id(self):\n    return self._spk2id\n\n\ndef rand_segment(feat, segment_length):\n  if feat is None:\n    return None\n  num_frame = feat.shape[0]\n  if segment_length < num_frame:\n    index = random.randint(0, (num_frame - segment_length) - 1)\n    segment_feat = feat[index:index + segment_length, :]\n  else:\n    segment_feat = feat\n    while segment_feat.shape[0] < segment_length:\n      segment_feat = np.concatenate([segment_feat, feat], 0)\n\n    seg_len = segment_feat.shape[0]\n    elen = seg_len - segment_length - 1\n    if elen > 1:\n      s = np.round(random.randint(0, elen - 1))\n    else:\n      s = 0\n    e = s + segment_length\n    segment_feat = segment_feat[s:e, :]\n\n  return [segment_feat]\n\n\ndef segments(inputs, segment_length, segment_shift_rate=0.5, rand=False):\n  \'\'\'\n  param:\n    inputs: [time, feat size, 1]\n  return:\n    list of feats, shape [segment_len, feat size, 1] or None\n  \'\'\'\n  if rand:\n    return rand_segment(inputs, segment_length)\n\n  segment_shift = int(segment_length * segment_shift_rate)\n  length = inputs.shape[0]\n  segs = []\n  for x in range(0, length, segment_shift):\n    end = x + segment_length\n    if end < length:\n      feature_mat = inputs[x:end, :, :]\n    else:\n      input_data = inputs\n      while input_data.shape[0] < segment_length:\n        input_data = np.concatenate([input_data, inputs], 0)\n\n      seg_len = input_data.shape[0]\n      elen = seg_len - segment_length - 1\n      if elen > 1:\n        s = np.round(random.randint(0, elen - 1))\n      else:\n        s = 0\n      e = s + segment_length\n      feature_mat = input_data[s:e, :, :]\n    segs.append(feature_mat)\n  return segs\n\n\n@registers.task.register\nclass SpeakerUttTask(SpeechTask, tf.keras.utils.Sequence):\n  \'\'\' Speaker Task for uttrance with segments \'\'\'\n\n  def __init__(self, config, mode):\n    super().__init__(config, mode)\n    self.shuffle = mode == utils.TRAIN\n    self.dataconf = self.config[\'data\']\n    self.taskconf = self.dataconf[\'task\']\n    self.solverconf = self.config[\'solver\']\n    self.batch_size = self.solverconf[\'optimizer\'][\'batch_size\']\n\n    self.data_type = self.taskconf[\'data_type\']\n    self.min_segment_length = 240\n    self.max_segment_length = 280\n    self.segment_shift_rate = 0.5\n\n    self.collect_meta()\n    self.on_epoch_end()\n\n  def collect_meta(self):\n    data_paths = self.dataconf[self.mode][\'paths\']\n    logging.info(\'Loading mode: [%s] dirs: %s ...\' % (self.mode, data_paths))\n    if len(data_paths) != 1:\n      raise ValueError(\'More than 1 data dirs is not supported by now.\')\n\n    for data_path in data_paths:\n      logging.info(\'Loading dir %s ...\' % (data_path))\n      if self.data_type == \'KaldiDataDirectory\':\n        self.kaldi_meta = KaldiDir(data_path)\n\n        self.feats_scp_items = list(self.kaldi_meta.feats_scp.items())\n        self.num_utts = self.kaldi_meta.num_utts\n        self.class_nums = self.kaldi_meta.num_spks\n        assert self.class_nums == self.taskconf[\'classes\'][\'num\']\n        logging.info(\'The dataset have {} utts and {} speakers\'.format(\n            self.num_utts, self.class_nums))\n\n        for _, (utt, feat_path) in enumerate(self.kaldi_meta.feats_scp.items()):\n          self.feat_size = kaldiio.load_mat(feat_path).shape[1]\n          break\n        logging.info(\'feat_size {}\'.format(self.feat_size))\n        if self.feat_size < 0:\n          raise Exception(\'Wrong feat_size {}\'.format(self.feat_size))\n      else:\n        raise ValueError(\'Unsupported data type: %s\' % (self.data_type))\n\n    self._classes = self.kaldi_meta.spk2id\n\n  def __len__(self):\n    \'\'\' the number of exmaples\'\'\'\n    # using this to make shure the last exmaples less than batch size used.\n    if self.mode == utils.TRAIN:\n      steps_per_epoch = (self.num_utts - self.batch_size) / self.batch_size + 1\n    else:\n      steps_per_epoch = (self.num_utts / self.batch_size) + 1\n    return int(steps_per_epoch)\n\n  def on_epoch_end(self):\n    \'\'\' update indexes after each epoch\'\'\'\n    self.indexs = np.arange(self.num_utts)\n    if self.shuffle:\n      logging.info(""shuffle data"")\n      np.random.shuffle(self.indexs)\n    logging.info(f""{self.indexs}"")\n\n    self.segment_win = int(\n        (self.min_segment_length + self.max_segment_length) / 2)\n    logging.info(f""default segment length: {self.segment_win}"")\n\n    if self.mode == utils.TRAIN:\n      if self.min_segment_length <= self.max_segment_length and self.max_segment_length > 0:\n        self.segment_win = np.random.randint(self.min_segment_length,\n                                             self.max_segment_length)\n      logging.info(f""For training, using segment length: {self.segment_win}"")\n\n  def collate_fn(self, batch):\n\n    minibatch_size = len(batch)\n\n    inputs = []\n    utt_ids = []\n    spk_ids = []\n    seg_ids = []\n    for x in range(minibatch_size):\n      sample = batch[x]\n      utt_id = sample[0]\n      spk_id = int(sample[1])\n      spect = sample[2]\n\n      # for train\n      for i, spect_slice in enumerate(\n          segments(\n              spect,\n              self.segment_win,\n              segment_shift_rate=self.segment_shift_rate,\n              rand=self.mode == utils.TRAIN)):\n        inputs.append(spect_slice)\n        seg_ids.append(i)\n        utt_ids.append(utt_id)\n        spk_ids.append(spk_id)\n\n    targets = spk_ids\n    return utt_ids, seg_ids, inputs, targets\n\n  def __getitem__(self, batch_index, return_format=True):\n    \'\'\' get batch_index\'s batch data \'\'\'\n    indexs = self.indexs[batch_index * self.batch_size:(batch_index + 1) *\n                         self.batch_size]\n    # key, feat_path\n    batch_meta = [self.feats_scp_items[i] for i in indexs]\n\n    batches = []\n    for _, (utt_id, utt_path) in enumerate(batch_meta):\n      spk_id = self.kaldi_meta.utt2spkid[utt_id]\n      in_feat = kaldiio.load_mat(utt_path)\n      in_feat = np.expand_dims(in_feat, axis=-1)\n      batches.append((utt_id, spk_id, in_feat))\n\n    uttids, segids, feats, spkids = self.collate_fn(batches)\n\n    if not return_format:\n      return uttids, segids, feats, spkids\n\n    labels = np.array(spkids, dtype=np.int32)\n    features = {\n        \'inputs\': np.array(feats, dtype=np.float64),\n        \'labels\': labels,\n        \'filepath\': np.array(uttids),\n        \'clipid\': np.array(segids, dtype=np.int32),\n    }\n    one_hot_labels = tf.keras.utils.to_categorical(\n        labels, num_classes=self.class_nums)\n    return features, one_hot_labels\n\n  def generate_cmvn(self, filelist=None, dry_run=False):  # pylint: disable=unused-argument\n    pass\n\n  def generate_feat(self, filelist, dry_run=False):\n    pass\n\n  def preprocess_batch(self, batch):\n    return batch\n\n  def generate_data(self):\n    \'\'\'\n    Yields samples.\n\n    Args:\n      multiprocess: use multiprocessing. Default = True.\n\n    Yields:\n      (inputs, label, filename, clip_id, soft_label)\n    \'\'\'\n    for i in range(len(self)):\n      uttids, segids, feats, spkids = self.__getitem__(i, return_format=False)\n      for b, _ in enumerate(uttids):\n        yield uttids[b], segids[b], feats[b], spkids[b]\n\n    self.on_epoch_end()\n    raise StopIteration\n\n  def feature_spec(self):\n    output_shapes = (\n        tf.TensorShape([]),  # utt\n        tf.TensorShape([]),  # segid\n        tf.TensorShape([None, self.feat_size,\n                        1]),  # audio_feat, without batch dim e.g. (3000, 40, 3)\n        tf.TensorShape([]),  # spkid\n    )\n    output_types = (\n        tf.string,\n        tf.int32,\n        tf.float32,\n        tf.int32,\n    )\n    assert len(output_shapes) == len(output_types)\n    return output_shapes, output_types\n\n  def dataset(self, mode, batch_size, num_epoch):  # pylint: disable=unused-argument\n    shapes, types = self.feature_spec()\n    data = tf.data.Dataset.from_generator(\n        generator=lambda: self.generate_data(),  # pylint: disable=unnecessary-lambda\n        output_types=types,\n        output_shapes=shapes,\n    )\n\n    buffer_size = self.taskconf[\'shuffle_buffer_size\']\n    logging.info(\'Using buffer size of %d samples in shuffle_and_repeat().\' %\n                 (buffer_size))\n\n    if mode == utils.TRAIN:\n      data = data.shuffle(buffer_size=buffer_size)\n\n    def make_example(utts, segids, inputs, labels):\n      features = {\n          \'inputs\': inputs,\n          \'labels\': labels,\n          \'filepath\': utts,\n          \'clipid\': segids,\n      }\n      return features, labels\n\n    return data.map(make_example, num_parallel_calls=30).\\\n                batch(batch_size, drop_remainder=False).\\\n                prefetch(tf.data.experimental.AUTOTUNE)\n'"
delta/data/task/speaker_cls_task_test.py,3,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' speaker task unittest\'\'\'\nimport os\nimport tempfile\nfrom pathlib import Path\n\nimport numpy as np\nimport delta.compat as tf\nfrom absl import logging\n\nfrom delta import utils\nfrom delta.utils.register import registers\nfrom delta.utils.register import import_all_modules_for_register\nfrom delta.utils.kaldi import kaldi_dir_utils\n\n\nclass SpeakerClsTaskTest(tf.test.TestCase):\n  \'\'\' speaker task test\'\'\'\n\n  def setUp(self):\n    super().setUp()\n    import_all_modules_for_register()\n    self.conf_str = \'\'\'\n    data:\n      train:\n        paths:\n        - \'\'\n      eval:\n        paths:\n        - \'\'\n      infer:\n        paths:\n        - \'\'\n      task:\n        name: SpeakerClsTask\n        data_type: KaldiDataDirectory\n        suffix: .npy # file suffix\n        audio:\n          dry_run: false # not save feat\n          # params\n          clip_size: 3 # clip len in seconds\n          stride: 0.5 # stride in ratio of clip_size\n          sr: 8000 # sample rate\n          winlen: 0.025 # window len\n          winstep: 0.01 # window stride\n          nfft: 512 # fft bins, default: 512\n          lowfreq: 0\n          highfreq: null # default: null, 200 points for 800 nfft, 400 points for 1600 nfft\n          preemph: 0.97 # default: 0.97\n          # extractor\n          feature_extractor: tffeat # `tffeat` to use TF feature_extraction .so library, \'pyfeat\' to python_speech_feature\n          save_feat_path: null  # null for dump feat with same dir of wavs\n          # fbank\n          save_fbank: true # save fbank or power spec\n          feature_size: 23 # extract feature size\n          add_delta_deltas: false # delta deltas\n          # log pwoer\n          log_powspec: false # true, save log power spec; otherwise save power spec\n          # cmvn\n          cmvn: true # apply cmvn or generate cmvn\n          cmvn_path: ./cmvn_speaker.npy # cmvn file\n        classes:\n          num: 2 \n          vocab: null\n        num_parallel_calls: 12\n        num_prefetch_batch: 2\n        shuffle_buffer_size: 200000\n        need_shuffle: true\n\n    model:\n      name: SpeakerCRNNRawModel\n      type: raw # raw, keras or eager model\n      net:\n        structure:\n          embedding_size: 2\n          filters: # equal number of cnn layers\n          - 2\n          filter_size: # equal number of cnn layers\n          - [1, 1]\n          filter_stride: # equal number of cnn layers\n          - [1, 1]\n          pool_size: # equal number of cnn layers\n          - [8, 8]\n          tdnn_contexts:\n          - 3\n          - 3\n          tdnn_dims:\n          - 128\n          - 128\n          num_filters: 2\n          linear_num: 2 # hidden number of linear layer\n          cell_num: 2 # cell units of the lstm\n          hidden1: 2 # number of hidden units of fully connected layer\n          attention: false # whether to use attention, false mean use max-pooling\n          attention_size: 64 # attention_size\n          use_lstm_layer: false # whether to use lstm layer, false mean no lstm layer\n          use_dropout: true # whether to use bn, dropout layer\n          dropout_rate: 0.2\n          use_bn: true # whether to use bn, dropout layer\n\n          score_threshold: 0.5 # threshold to predict POS example\n          threshold: 3 # threshold to predict POS example\n\n    solver:\n      name: SpeakerSolver\n      adversarial:\n        enable: false # whether to using adversiral training\n        adv_alpha: 0.5 # adviseral alpha of loss\n        adv_epslion: 0.1 # adviseral example epslion\n      model_average:\n        enable: false # use average model\n        var_avg_decay: 0.99 # the decay rate of varaibles\n      optimizer:\n        name: adam\n        epochs: 5 # maximum epochs\n        batch_size: 4 # number of elements in a training batch\n        loss: CrossEntropyLoss\n        label_smoothing: 0.0 # label smoothing rate\n        learning_rate:\n          rate: 0.0001 # learning rate of Adam optimizer\n          type:  exp_decay # learning rate type\n          decay_rate: 0.99  # the lr decay rate\n          decay_steps: 100  # the lr decay_step for optimizer\n        clip_global_norm: 3.0 # clip global norm\n      metrics:\n        pos_label: 1 # int, same to sklearn\n        cals:\n        - name: AccuracyCal\n          arguments: null\n        - name: ConfusionMatrixCal\n          arguments: null\n        - name: PrecisionCal\n          arguments:\n            average: \'binary\'\n        - name: RecallCal\n          arguments:\n            average: \'binary\'\n        - name: F1ScoreCal\n          arguments:\n            average: \'binary\'\n      postproc:\n          name: SpeakerPostProc\n          log_verbose: false\n          eval: true # compute metrics\n          infer: true  # get predict results\n          pred_path: null # None for `model_path`/infer, dumps infer output to this dir\n          thresholds:\n              - 0.5\n          smoothing:\n              enable: true\n              count: 2\n      saver:\n        model_path: ""ckpt/emotion-speech-cls/test""\n        max_to_keep: 10\n        save_checkpoints_steps: 10\n        keep_checkpoint_every_n_hours: 10000\n        checkpoint_every: 10 # the step to save checkpoint\n        summary: false\n        save_summary_steps: 5\n        eval_on_dev_every_secs: 1\n        print_every: 10\n        resume_model_path: """"\n      run_config:\n        debug: false # use tfdbug\n        tf_random_seed: null # 0-2**32; null is None, try to read data from /dev/urandom if available or seed from the clock otherwise\n        allow_soft_placement: true\n        log_device_placement: false\n        intra_op_parallelism_threads: 1\n        inter_op_parallelism_threads: 1\n        allow_growth: true\n        log_step_count_steps: 1 #The frequency, in number of global steps, that the global step/sec and the loss will be logged during training.\n      distilling:\n        enable: false\n        name : Teacher\n        loss : DistillationLoss\n        temperature: 5\n        alpha: 0.5\n        teacher_model: \'\'\n\n    serving:\n      enable: true\n      name : Evaluate\n      model: \'\' # saved model dir, ckpt dir, or frozen_model.pb\n      inputs: \'inputs:0\'\n      outpus: \'softmax_output:0\'\n    \'\'\'\n\n    # write config to file\n    tempdir = self.get_temp_dir()\n    #tempdir = \'bar\'\n    os.makedirs(tempdir, exist_ok=True)\n\n    config_path = str(Path(tempdir).joinpath(\'speaker_task.yaml\'))\n    logging.info(""config path: {}"".format(config_path))\n    with open(config_path, \'w\', encoding=\'utf-8\') as f:  #pylint: disable=invalid-name\n      f.write(self.conf_str)\n\n    # load config\n    config = utils.load_config(config_path)\n    logging.info(""config: {}"".format(config))\n\n    # edit path in config\n    dataset_path = Path(tempdir).joinpath(\'data\')\n    if not dataset_path.exists():\n      dataset_path.mkdir()\n    dataset_path_str = str(dataset_path)\n    config[\'data\'][\'train\'][\'paths\'] = [dataset_path_str]\n    config[\'data\'][\'eval\'][\'paths\'] = [dataset_path_str]\n    config[\'data\'][\'infer\'][\'paths\'] = [dataset_path_str]\n\n    # generate dummy data\n    feat_dim = config[\'data\'][\'task\'][\'audio\'][\'feature_size\']\n    kaldi_dir_utils.gen_dummy_data_dir(\n        dataset_path_str, 2, 2, feat_dim=feat_dim)\n\n    solver_name = config[\'solver\'][\'name\']\n    self.solver = registers.solver[solver_name](config)\n\n    # config after process\n    self.config = self.solver.config\n\n  def tearDown(self):\n    \'\'\' tear down\'\'\'\n\n  def test_generate_feat(self):\n    \'\'\' test generate feature\'\'\'\n    task_name = self.config[\'data\'][\'task\'][\'name\']\n    task_class = registers.task[task_name]\n\n    paths = []\n    for mode in [utils.TRAIN, utils.EVAL, utils.INFER]:\n      paths += self.config[\'data\'][mode][\'paths\']\n\n    task = task_class(self.config, utils.INFER)\n    task.generate_feat(paths, dry_run=False)\n\n  def test_generate_cmvn(self):\n    \'\'\' test generate cmvn\'\'\'\n    tmp = tempfile.mktemp(suffix=\'cmvn.npy\')\n    self.config[\'data\'][\'task\'][\'audio\'][\'cmvn_path\'] = tmp\n\n    self.config[\'data\'][\'task\'][\'suffix\'] = \'.wav\'\n    self.config[\'data\'][\'task\'][\'stride\'] = 1.0\n    paths = self.config[\'data\'][utils.TRAIN][\'paths\']\n    self.config[\'data\'][utils.INFER][\'paths\'] = paths\n\n    task_name = self.config[\'data\'][\'task\'][\'name\']\n    task_class = registers.task[task_name]\n\n    task = task_class(self.config, utils.INFER)\n    task.generate_cmvn(dry_run=False)\n\n    self.assertTrue(\n        os.path.exists(self.config[\'data\'][\'task\'][\'audio\'][\'cmvn_path\']))\n    cmvn = np.load(self.config[\'data\'][\'task\'][\'audio\'][\'cmvn_path\'])\n    self.assertEqual(cmvn.ndim, 4)\n\n  def test_generate_data(self):\n    \'\'\' test generate data\'\'\'\n    self.config[\'data\'][\'task\'][\'suffix\'] = \'.npy\'\n\n    task_name = self.config[\'data\'][\'task\'][\'name\']\n    task_class = registers.task[task_name]\n\n    task = task_class(self.config, utils.TRAIN)\n\n    for inputs, label, filename, clip_id, soft_labels in task.generate_data():\n      logging.info(\n          ""feat shape:{} \\n labels:{} \\nfilename:{} \\nclip_id:{}\\nsoft_labels:{}""\n          .format(inputs.shape, label, filename, clip_id, soft_labels))\n      break\n\n  #pylint: disable=too-many-locals\n  def test_dataset(self):\n    \'\'\' dataset unittest\'\'\'\n    batch_size = 4\n    self.config[\'solver\'][\'optimizer\'][\'batch_size\'] = batch_size\n\n    task_name = self.config[\'data\'][\'task\'][\'name\']\n    task_class = registers.task[task_name]\n    task = task_class(self.config, utils.TRAIN)\n\n    dataset = task.input_fn(utils.TRAIN, batch_size, 1)()\n\n    features, labels = dataset.make_one_shot_iterator().get_next()\n    samples = features[\'inputs\']\n    filenames = features[\'filepath\']\n    clip_ids = features[\'clipid\']\n    soft_labels = features[\'soft_labels\']\n\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      while True:\n        batch_inputs, batch_labels, batch_files, batch_clipids, labels_onehot, batch_soft_labels = \\\n           sess.run([samples, labels, filenames, clip_ids, tf.one_hot(labels, 2), soft_labels])\n\n        del labels_onehot\n        logging.info(""feat shape: {}"".format(batch_inputs.shape))\n        logging.info(""labels: {}"".format(batch_labels))\n        logging.info(""filename: {}"".format(batch_files))\n        logging.info(""clip id: {}"".format(batch_clipids))\n        logging.info(""soft_labels: {}"".format(batch_soft_labels))\n        break\n\n  def test_speaker_utt_task_dataset(self):\n    task_name = \'SpeakerUttTask\'\n    self.config[\'data\'][\'task\'][\'name\'] = task_name\n    batch_size = 4\n    self.config[\'solver\'][\'optimizer\'][\'batch_size\'] = batch_size\n\n    task_name = self.config[\'data\'][\'task\'][\'name\']\n    task_class = registers.task[task_name]\n\n    for mode in (utils.TRAIN, utils.EVAL, utils.INFER):\n      task = task_class(self.config, mode)\n      dataset = task.input_fn(mode, batch_size, 1)()\n      features, one_hot_labels = dataset.make_one_shot_iterator().get_next()\n      samples = features[\'inputs\']\n      filenames = features[\'filepath\']\n      clip_ids = features[\'clipid\']\n      labels = features[\'labels\']\n\n      with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n        while True:\n          batch_inputs, batch_labels, batch_files, batch_clipids, labels_onehot = \\\n             sess.run([samples, labels, filenames, clip_ids, one_hot_labels])\n\n          del labels_onehot\n          logging.info(""feat shape: {}"".format(batch_inputs.shape))\n          logging.info(""filename: {}"".format(batch_files))\n          logging.info(""clip id: {}"".format(batch_clipids))\n          logging.info(""labels: {}"".format(batch_labels))\n          break\n\n  def test_speaker_utt_task_getitem(self):\n    task_name = \'SpeakerUttTask\'\n    self.config[\'data\'][\'task\'][\'name\'] = task_name\n    task_class = registers.task[task_name]\n\n    for mode in (utils.TRAIN, utils.EVAL, utils.INFER):\n      task = task_class(self.config, mode)\n      for i, (feats, labels) in enumerate(task):\n        logging.info(f""SpkUttTask: __getitem__: {feats.keys()} {labels}"")\n\n  def test_speaker_utt_task_generate_data(self):\n    task_name = \'SpeakerUttTask\'\n    self.config[\'data\'][\'task\'][\'name\'] = task_name\n    task_class = registers.task[task_name]\n    for mode in (utils.TRAIN, utils.EVAL, utils.INFER):\n      task = task_class(self.config, mode)\n      logging.info(f""mode: {mode}"")\n      for i in range(10):\n        for utt, segid, feat, spkid in task.generate_data():\n          logging.info(\n              f""SpkUttTask: generate_data: {utt} {segid} {feat.shape} {spkid}"")\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  tf.test.main()\n'"
delta/data/task/speech_cls_task.py,29,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' emotion speech task \'\'\'\nimport re\nimport ast\nimport os\nimport copy\nimport threading\nimport itertools\nimport functools\nimport random\nfrom collections import defaultdict\n\nimport librosa\nimport numpy as np\nimport pandas as pd\nimport delta.compat as tf\nfrom absl import logging\nfrom sklearn.model_selection import train_test_split\n\nfrom delta import utils\nfrom delta.data import feat as feat_lib\nfrom delta.utils.register import registers\nfrom delta.data.task.base_speech_task import SpeechTask\n\n\ndef _load_text(text_path):\n  with open(text_path + \'.txt\', \'r\', encoding=\'utf-8\') as fin:\n    text = \' \'.join([line.strip() for line in fin.readlines()])\n    return text\n\n\ndef _process_text(text):\n  text = re.findall(r""[\\w\']+|[.,!?;]"", text.lower())\n  return text\n\n#pylint: disable=too-many-public-methods,too-many-instance-attributes\n@registers.task.register\nclass SpeechClsTask(SpeechTask):\n  \'\'\' Emotion Task \'\'\'\n\n  #pylint: disable=too-many-statements\n  def __init__(self, config, mode):\n    super().__init__(config, mode)\n    # for dev, test dataset split\n    self._dev_size = 0.0\n    self._test_size = 0.0\n\n    #self.dataconf = self.processdataconf(config)\n    self.dataconf = self.config[\'data\']\n    self.taskconf = self.dataconf[\'task\']\n    self.solverconf = self.config[\'solver\']\n\n    self._data_path = self.dataconf[mode][\'paths\']\n    logging.info(""data_path : {}"".format(self._data_path))\n    if \'segments\' in self.dataconf[\n        mode] and self.dataconf[mode][\'segments\'] is not None:\n      self._textgrid_path = self.dataconf[mode][\'segments\']\n    else:\n      self._textgrid_path = []\n\n    self._file_suffix = self.taskconf[\'suffix\']\n    assert self._file_suffix in [\'.wav\', \'.npy\'], ""Only support wav and npy""\n    assert isinstance(self._data_path, (tuple, list))\n    assert isinstance(self._textgrid_path, (tuple, list))\n    assert self._data_path\n\n    self.input_type = \'samples\' if self._file_suffix == \'.wav\' else \'features\'\n    self._clip_size = self.taskconf[\'audio\'][\'clip_size\']  # seconds\n    if mode == utils.TRAIN:\n      self._stride = self.taskconf[\'audio\'][\'stride\']  # percent\n    else:\n      self._stride = 1.0  # percent\n    logging.info(""Mode: {}, stride {}"".format(mode, self._stride))\n    self._feature_type = self.taskconf[\'audio\'][\'feature_extractor\']\n    self._feature_name = self.taskconf[\'audio\'][\'feature_name\']\n    logging.info(\n        f""feature type: {self._feature_type}, feature name: {self._feature_name}""\n    )\n    self._sample_rate = self.taskconf[\'audio\'][\'sr\']\n    self._winstep = self.taskconf[\'audio\'][\'winstep\']\n    self._feature_size = self.taskconf[\'audio\'][\'feature_size\']\n    self._input_channels = 3 if self.taskconf[\'audio\'][\'add_delta_deltas\'] else 1\n    self._cmvn_path = self.taskconf[\'audio\'][\'cmvn_path\']\n    self._save_feat_path = self.taskconf[\'audio\'][\'save_feat_path\']\n\n    # {class: [filename, duration, class],...}\n    self._class_file = defaultdict(list)\n    self._num_class = 0\n    self._classes = self.taskconf[\'classes\'][\'vocab\']\n    assert self._classes\n\n    self._max_text_len = self.taskconf[\'text\'][\'max_text_len\']\n    self._vocab_path = self.taskconf[\'text\'][\'vocab_path\']\n    self._epoch = 0\n\n    self._class_file_train = None\n    self._train_meta = None\n    self._train_by_filename = None\n    self._class_file_dev = None\n    self._class_train_dev_test = None\n    self._postive_segs_in_sec = None\n    self._train_label_example = None\n    self._class_file_test = None\n    self._postive_segs = None\n    self.data_items = None\n\n    # generate segment index\n    self.generate_meta(mode)\n\n    # load text vocab table\n    self.use_text = self.taskconf[\'text\'][\'enable\']\n    if self.use_text:\n      self.load_text_vocab_table()\n\n    # use distilling\n    self.use_distilling = False\n    if \'distilling\' in self.solverconf:\n      self.use_distilling = self.solverconf[\'distilling\'][\'enable\']\n\n    if self.use_distilling:\n      temperature = self.solverconf[\'distilling\'][\'temperature\']\n      model = self.solverconf[\'distilling\'][\'teacher_model\']\n      name = self.solverconf[\'distilling\'][\'name\']\n      self.teacher = registers.serving[name](model, None, temperature)\n\n  @property\n  def max_text_len(self):\n    \'\'\' max length of text\'\'\'\n    return self._max_text_len\n\n  @property\n  def num_class(self):\n    \'\'\' num class\'\'\'\n    return len(self._classes)\n\n  @property\n  def classes(self):\n    \'\'\' classes for labels\'\'\'\n    return self._classes\n\n  def class_id(self, cls):\n    \'\'\' class id \'\'\'\n    return self._classes[cls]\n\n  @property\n  def pos_id(self):\n    \'\'\' postive label id\'\'\'\n    return self.class_id(self.taskconf[\'classes\'][\'positive\'])\n\n  @property\n  def example_len(self):\n    \'\'\' samples per clip\'\'\'\n    samples_per_clip = self._sample_rate * self._clip_size\n    return samples_per_clip\n\n  def sample_to_frame(self, samples):\n    \'\'\' sample num to frame num\'\'\'\n    nframe = librosa.core.samples_to_frames(\n        samples,\n        hop_length=self.taskconf[\'audio\'][\'winstep\'] * self._sample_rate)\n    return int(nframe)\n\n  @property\n  def nframe(self):\n    \'\'\' num frames\'\'\'\n    return self.sample_to_frame(self.example_len)\n\n  @staticmethod\n  def feat_output_shape(config):\n    \'\'\' without batch_size\'\'\'\n    if \'feature_shape\' in config[\'task\'][\'audio\'] and config[\'task\'][\'audio\'][\n        \'feature_shape\']:\n      return config[\'task\'][\'audio\'][\'feature_shape\']\n\n    if config[\'task\'][\'suffix\'] == \'.npy\':\n      input_channels = 3 if config[\'task\'][\'audio\'][\'add_delta_deltas\'] else 1\n      nframe = librosa.time_to_frames(\n          config[\'task\'][\'audio\'][\'clip_size\'],\n          sr=config[\'task\'][\'audio\'][\'sr\'],\n          hop_length=config[\'task\'][\'audio\'][\'winstep\'] *\n          config[\'task\'][\'audio\'][\'sr\'])\n      feature_shape = [\n          nframe, config[\'task\'][\'audio\'][\'feature_size\'], input_channels\n      ]\n    else:\n      feature_shape = [\n          config[\'task\'][\'audio\'][\'sr\'] * config[\'task\'][\'audio\'][\'clip_size\']\n      ]\n    config[\'task\'][\'audio\'][\'feature_shape\'] = feature_shape\n    return feature_shape\n\n  @property\n  def feature_shape(self):\n    \'\'\' feature shape w/o batch size \'\'\'\n    return SpeechClsTask.feat_output_shape(self.dataconf)\n\n  @staticmethod\n  def _down_sample(feat, skip_num=4, context=(-7, 7)):\n    left_ctx, right_ctx = context\n\n    left = np.tile(feat[0], [abs(left_ctx), 1, 1])\n    right = np.tile(\n        feat[-1],\n        [min(abs(right_ctx),\n             abs(right_ctx) - feat.shape[0] % skip_num), 1, 1])\n    context = np.concatenate((left, feat, right), axis=0)\n\n    index = np.array([i for i in range(feat.shape[0]) if i % skip_num == 0\n                     ]) + abs(left_ctx)\n    res = [context[i + left_ctx:i + right_ctx + 1] for i in index]\n    return res\n\n  #pylint: disable=arguments-differ\n  def generate_feat(self, filelist, dry_run=False):\n    \'\'\'\n    collect *.wav under `paths` and generate *.npy,\n       withoud consider segments file\n    filelist: paths: list, path of wav data\n    \'\'\'\n    featconf = self.taskconf[\'audio\'].update({\'dry_run\': dry_run}) \\\n      if dry_run else self.taskconf[\'audio\']\n    logging.debug(""feat config: {}"".format(featconf))\n\n    files = []\n    threads = []\n    for data_path in filelist:\n      for root, dirname, filenames in os.walk(data_path):\n        del dirname\n        for filename in filenames:\n          if filename.endswith(\'.wav\'):\n            filename = os.path.join(root, filename)\n            files.append(filename)\n\n      if self._feature_type == \'tffeat\':\n        func = feat_lib.extract_feature\n        featconf.update({\'feature_name\': self._feature_name})\n      elif self._feature_type == \'pyfeat\':\n        func = feat_lib.extract_feat\n      else:\n        raise ValueError(""Not support feat: {}"".format(self._feature_type))\n\n      #pylint: disable=invalid-name\n      t = threading.Thread(\n          target=func, args=tuple(files), kwargs=featconf, daemon=True)\n      threads.append(t)\n      t.start()\n      files = []\n\n    for t in threads:  #pylint: disable=invalid-name\n      t.join()\n    logging.info(\'generate feature done\')\n\n  #pylint: disable=arguments-differ,too-many-locals\n  def generate_cmvn(self, filelist=None, dry_run=False):\n    del filelist\n    assert self._stride == 1.0\n    batch_size = self.config[\'solver\'][\'optimizer\'][\'batch_size\']\n    features, labels = self.input_fn(\n        utils.INFER, batch_size,\n        num_epoch=1)().make_one_shot_iterator().get_next()\n    del labels\n\n    suffix = self.taskconf[\'suffix\']\n    if suffix == \'.npy\':\n      logging.info(\'generate cmvn from numpy\')\n      feature = features[\'inputs\']\n    else:\n      logging.info(\'genearte cmvn from wav\')\n      # tf extractor graph\n      params = feat_lib.speech_ops.speech_params(\n          sr=self.taskconf[\'audio\'][\'sr\'],\n          bins=self.taskconf[\'audio\'][\'feature_size\'],\n          add_delta_deltas=self.taskconf[\'audio\'][\'add_delta_deltas\'],\n          audio_frame_length=self.taskconf[\'audio\'][\'winlen\'],\n          audio_frame_step=self.taskconf[\'audio\'][\'winstep\'])\n\n      #[batch, Time] -> [batch, time, audio_channel]\n      waveforms = tf.expand_dims(features[\'inputs\'], axis=-1)\n      #[batch, Time, feat_size, channles]\n      feature = feat_lib.speech_ops.batch_extract_feature(waveforms, params)\n\n    # create stats vars\n    sums, square, count = utils.create_cmvn_statis(\n        self.taskconf[\'audio\'][\'feature_size\'],\n        self.taskconf[\'audio\'][\'add_delta_deltas\'])\n    try:\n      with tf.Session() as sess:\n        while True:\n          feat_np = sess.run(feature)\n          # update stats\n          sums, square, count = utils.update_cmvn_statis(\n              feat_np, sums, square, count, axis=(0, 1))\n    except tf.errors.OutOfRangeError:\n      pass\n\n    # compute cmvn\n    mean, var = utils.compute_cmvn(sums, square, count)\n    logging.info(\'mean:{}\'.format(mean))\n    logging.info(\'var:{}\'.format(var))\n    if not dry_run:\n      np.save(self._cmvn_path, (mean, var))\n    logging.info(\'save cmvn:{}\'.format(self._cmvn_path))\n    logging.info(\'generate cmvn done\')\n\n  def get_duration(self, filename, sr):  #pylint: disable=invalid-name\n    \'\'\' time in second \'\'\'\n    if filename.endswith(\'.npy\'):\n      nframe = np.load(filename).shape[0]\n      return librosa.frames_to_time(\n          nframe, hop_length=self._winstep * sr, sr=sr)\n\n    if filename.endswith(\'.wav\'):\n      return librosa.get_duration(filename=filename)\n\n    raise ValueError(""filename suffix not .npy or .wav: {}"".format(\n        os.path.splitext(filename)[-1]))\n\n  def get_class_files_duration(self):\n    \'\'\' dirnames under dataset is class name\n     all data_path have same dirnames \'\'\'\n    classes = None\n    for root, dirnames, filenames in os.walk(self._data_path[0]):\n      classes = dirnames\n      break\n\n    assert classes, \'can not acsess {}\'.format(self._data_path[0])\n    assert set(classes) == set(self._classes.keys()), \'{} {}\'.format(\n        classes, self._classes.keys())\n\n    def _get_class(path):\n      ret = None\n      for cls in self._classes:\n        if cls in path:\n          ret = cls\n      return ret\n\n    # to exclude some data under some dir\n    excludes = []\n    #pylint: disable=too-many-nested-blocks\n    for data_path in self._data_path:\n      logging.debug(""data path: {}"".format(data_path))\n      for root, dirname, filenames in os.walk(data_path):\n        del dirname\n        for filename in filenames:\n          if filename.endswith(self._file_suffix):\n            class_name = _get_class(root)  # \'conflict\' or \'normal\' str\n            assert class_name is not None\n            filename = os.path.join(root, filename)\n\n            if excludes:\n              for exclude in excludes:\n                if exclude in filename:\n                  pass\n\n            duration = self.get_duration(\n                filename=filename, sr=self._sample_rate)\n            self._class_file[class_name].append(\n                (filename, duration, class_name))\n          else:\n            pass\n\n    if not self._class_file:\n      logging.debug(""class file: {}"".format(self._class_file))\n      logging.warn(""maybe the suffix {} file not exits"".format(\n          self._file_suffix))\n\n  def save_csv_files_duration(self, save_file=\'./filelist.csv\', force=False):\n    \'\'\' save meta data to csv \'\'\'\n    #pylint: disable=invalid-name\n    if not os.path.exists(save_file) or force:\n      df = pd.DataFrame.from_dict(self._class_file, orient=\'index\')\n      df = df.transpose()\n      #logging.info(df)\n      df.to_csv(save_file, sep=\' \')\n\n  def split_train_dev_test(self, dev_size=0.1, test_size=0.1):\n    \'\'\' split data to train, dev, test set\'\'\'\n    self._class_file_train = []\n    self._class_file_dev = []\n    self._class_file_test = []\n    self._class_train_dev_test = defaultdict(tuple)\n\n    def _split_train_dev_test(filelist, dev_size, test_size, dummpy=True):\n      if dummpy:\n        return filelist, [], []\n\n      train_file, test_file = train_test_split(filelist, test_size=test_size)\n      train_file, dev_file = train_test_split(train_file, test_size=dev_size)\n      return train_file, dev_file, test_file\n\n    # class file: dict: key `class`, value: [filename, duration, class]\n    nclass = len(self._class_file)\n    try:\n      sub_dev_size = dev_size / nclass\n      sub_test_size = test_size / nclass\n    except ZeroDivisionError:\n      logging.info(""{}"".format(self._class_file.keys()))\n\n    for cls in self._class_file.keys():  # class\n      train_file, dev_file, test_file = \\\n        _split_train_dev_test(self._class_file[cls], sub_dev_size, sub_test_size)\n\n      self._class_train_dev_test[cls] = (train_file, dev_file, test_file)\n\n      self._class_file_train.extend(train_file)\n      self._class_file_dev.extend(dev_file)\n      self._class_file_test.extend(test_file)\n\n    #logging.info(self._class_train_dev_test)\n    logging.debug(\'train: {}\'.format(self._class_file_train))\n    logging.debug(\'dev: {}\'.format(self._class_file_dev))\n    logging.debug(\'test: {}\'.format(self._class_file_test))\n\n  def class_features(self, segments_path):\n    \'\'\' map filename w/o suffix to segment samples\n       {hxx/xxx/abc:[ [200, 300], ...]}\n    \'\'\'\n    self._postive_segs = defaultdict(list)\n    self._postive_segs_in_sec = defaultdict(list)\n\n    #pylint: disable=invalid-name\n    def seconds_to_samples(segs_list, pos_example=True, sr=8000):\n      \'\'\'\n            Args:\n               segs_list: list of tuple (min, max), unit seconds\n               pos_example:\n                  if True, POS segments in Positive sentence\n                  else, NEG segments in Positive sentence\n            Return:\n            segs in samples\n            \'\'\'\n\n      def _convert(second_range):\n        \'\'\' second_range, tuple(min, max), unit sencods \'\'\'\n        low, hight = list(\n            map(\n                functools.partial(librosa.time_to_samples, sr=sr),\n                second_range))\n        return (low, hight)\n\n      segs_list = list(map(_convert, segs_list))\n\n      assert pos_example, \'Now only use POS segment in POSITIVE sentece\'\n      if pos_example:\n        # positive segment in positive sentence\n        return segs_list\n\n      # negtive segment in positive sentence\n      res = []\n      flat_list = [0] + list(itertools.chain.from_iterable(segs_list)) + [None]\n      for _ in range(0, len(flat_list), 2):\n        #if flat_list[0] != flat_list[1]:\n        #pylint: disable=invalid-name\n        s, e = flat_list[0], flat_list[1]\n        if s != e:\n          res.append((s, e))\n      return res\n\n    def load_segments(segments_file):\n      \'\'\' load TextGrid file \'\'\'\n      if self._postive_segs:\n        logging.debug(\'TextGrid has been loaded: {}\'.format(segments_file))\n        return\n\n      if segments_file:\n        logging.debug(\'TextGrid loading: {}\'.format(segments_file))\n        for segment_file in segments_file:\n          with open(segment_file) as f:  #pylint: disable=invalid-name\n            for line in f:\n              filename, segs = line.split(maxsplit=1)\n              # using abspath of filename, without file suffix\n              filename = os.path.splitext(filename)[0]\n              segs = list(map(ast.literal_eval, segs.split()))\n              self._postive_segs_in_sec[filename].extend(segs)\n              # second -> sample\n              segs = seconds_to_samples(segs)\n              self._postive_segs[filename].extend(segs)\n        #logging.info(self._postive_segs)\n\n    def extract_segments(filename, segments_file):\n      \'\'\'\n            Brief:\n              extract segments from filename with TextGrid file\n              if `filename` in TextGrid file, then extract segments\n              else load hole file samples\n            Args:\n              filename: wavfile name, abspath\n              segments_file: TextGrid path\n            Return :\n                list of segments index (min, max), unit sample\n            \'\'\'\n      # load TextGrid file\n      load_segments(segments_file)\n\n      seg_flag = False  # whther using segment or whole file\n      segs = []\n      # using abspath without file suffix\n      filename = os.path.splitext(filename)[0]\n      if filename in self._postive_segs:\n        # segement wave file\n        # now only positive example has TextGrid file\n        segs_index = self._postive_segs[filename]\n        segs.extend(segs_index)\n        seg_flag = True\n      else:\n        # return whole file\n        segs.append((0, None))\n        seg_flag = False\n      logging.debug(""filename: {} segs_index:{} seg_or_whole:{}"".format(\n          filename, segs, seg_flag))\n      return segs, seg_flag\n\n    #pylint: disable=too-many-locals,invalid-name\n    def segment_indexes(filename,\n                        segments_file,\n                        clip_size=3,\n                        stride=0.5,\n                        sr=8000):\n      \'\'\' generate clip index in samples \'\'\'\n      duration = self.get_duration(filename=filename, sr=self._sample_rate)\n      clip_samples, stride_samples, total_samples = librosa.time_to_samples(\n          [clip_size, clip_size * stride, duration], sr=sr)\n\n      segs_index, _ = extract_segments(filename, segments_file)\n      seg_index = []\n\n      #pylint: disable=invalid-name\n      for st, ed in segs_index:\n        if ed is None:\n          ed = librosa.time_to_samples(duration, sr=sr)\n          logging.debug(\'file end: {} {}\'.format(duration, ed))\n        seg_len = ed - st\n\n        sub_segs = []\n        if seg_len > clip_samples:\n          nstride = int((seg_len - clip_samples) / stride_samples + 1)\n          # (strart, end, npad), only support right hand pad\n          sub_segs = [(i * stride_samples, i * stride_samples + clip_samples, 0)\n                      for i in range(nstride)]\n          seg_index.extend(sub_segs)\n        else:\n          gap = clip_samples - seg_len\n          if st - gap >= 0:\n            assert ed - (st - gap) == clip_samples\n            sub_segs.append((st - gap, ed, 0))\n          if ed + gap <= total_samples:\n            assert ed + gap - st == clip_samples\n            sub_segs.append((st, ed + gap, 0))\n          else:\n            npad = ed + gap - total_samples\n            assert ed + gap - st == clip_samples\n            sub_segs.append((st, ed + gap, npad))\n          seg_index.extend(sub_segs)\n        logging.debug(\'Input segs: {}, Output segs: {}\'.format((st, ed),\n                                                               sub_segs))\n      return seg_index\n\n    def _generate_online():\n      \'\'\' generate clips of file \'\'\'\n      class_file = self._class_file_train\n      # file, duration, class\n      for filename, duration, label in class_file:\n        del duration\n        # [ (start, end, npad)...]\n        seg_idxs = segment_indexes(\n            filename,\n            segments_path,\n            clip_size=self._clip_size,\n            stride=self._stride,\n            sr=self._sample_rate)\n\n        for clip_id, seg in enumerate(seg_idxs):\n          example = (filename, label, seg, clip_id)\n          self._train_meta.append(example)\n          self._train_label_example[label].append(example)\n\n    self._train_meta = []\n    self._train_label_example = defaultdict(list)\n\n    _generate_online()\n\n  def over_sampling(self, datatype=\'train\', ratio=1.0):\n    \'\'\' over sampling\n    prams: ratio: float, adjust of positive example by ratio when do two classes task\n    \'\'\'\n    assert datatype == \'train\'\n\n    def _balance(raw_labels):\n      labels = copy.deepcopy(raw_labels)\n      labels_len = {k: len(v) for k, v in raw_labels.items()}\n      logging.info(\'label_len: {}\'.format(labels_len))\n      logging.info(\'ratio: {}\'.format(ratio))\n\n      maxlen = max(labels_len.values())\n      gaps = {k: maxlen - v for k, v in labels_len.items()}\n      logging.info(\'gaps need: {}\'.format(gaps))\n\n      if self.taskconf[\'classes\'][\'num\'] == 2:\n        #for two classes, adjust postive example weight by ratio\n        pos_len = int(maxlen * abs(ratio - 1))\n        maxlen = maxlen + pos_len\n        for k in gaps:\n          if k == self.taskconf[\'classes\'][\'positive\']:\n            gaps[k] += pos_len\n        logging.info(\'gaps by ratio: {}\'.format(gaps))\n\n      for k in raw_labels:\n        raw_len = int(len(raw_labels[k]))\n        if gaps[k] > raw_len:\n          for _ in range(int(gaps[k] / raw_len)):\n            indexs = np.random.choice(raw_len, size=raw_len, replace=False)\n            examples = [raw_labels[k][i] for i in indexs]\n            labels[k].extend(examples)\n          gaps[k] %= raw_len\n        indexs = np.random.choice(raw_len, size=gaps[k], replace=False)\n        examples = [raw_labels[k][i] for i in indexs]\n        labels[k].extend(examples)\n\n      return maxlen, labels\n\n    maxlen, labels = _balance(self._train_label_example)\n    assert all([len(v) <= maxlen for _, v in labels.items()])\n    self._train_meta = list(itertools.chain(*labels.values()))\n\n  def collect_by_filename(self):\n    \'\'\' collect metadata: filepath -> (label, seg, clip_id)\'\'\'\n    self._train_by_filename = defaultdict(list)\n    for _, (filename, label, seg, clip_id) in enumerate(self._train_meta):\n      self._train_by_filename[filename].append((label, seg, clip_id))\n\n  def load_text_vocab_table(self):\n    \'\'\' load vocab \'\'\'\n    if not self._vocab_path and not os.path.exists(self._vocab_path):\n      logging.info(\'vocab_path is not exist.\')\n      return\n    self.word2id = {}\n    with open(self._vocab_path, \'r\', encoding=\'utf-8\') as vocab_f:\n      for line in vocab_f:\n        word, idx = line.strip().split()\n        self.word2id[word] = idx\n\n  def generate_meta(self, mode):\n    \'\'\' collection examples metadata\'\'\'\n    assert mode in [utils.TRAIN, utils.EVAL, utils.INFER]\n    logging.info(""{} data"".format(mode))\n    logging.info(""Get classes and coresspoding wavfile"")\n    self.get_class_files_duration()\n\n    logging.info(""Split train dev test dataset "")\n    self.split_train_dev_test(\n        dev_size=self._dev_size, test_size=self._test_size)\n\n    logging.info(""Cut clips and extract features "")\n    self.class_features(segments_path=self._textgrid_path)\n    if utils.TRAIN in mode:\n      logging.info(""Blance classes exmales of train dataset "")\n      self.over_sampling()\n\n    logging.info(""Collection by filepath"")\n    self.collect_by_filename()\n    self.data_items = list(self._train_by_filename.items())\n\n  def shuffle(self):\n    \'\'\' oversampling and get examples list meta\'\'\'\n    logging.info(""Blance classes exmales of train dataset "")\n    self.over_sampling()\n\n    logging.info(""Collection by filepath"")\n    self.collect_by_filename()\n\n    self.data_items = list(self._train_by_filename.items())\n\n  def _word_table_lookup(self, text):\n    \'\'\' convert text to id\'\'\'\n    max_text_len = self._max_text_len\n    text2id = np.zeros(shape=[max_text_len])\n    text = _process_text(text)\n    pad_len = min(max_text_len, len(text))\n    for char_num in range(pad_len):\n      ## handle unk\n      if text[char_num] in self.word2id:\n        text2id[char_num] = self.word2id[text[char_num]]\n      else:\n        text2id[char_num] = self.word2id[\'<unk>\']\n    return text2id\n\n  #pylint: disable=too-many-statements,too-many-locals,too-many-branches\n  def generate_data(self):\n    \'\'\' generate one example\'\'\'\n\n    # total files\n    total = len(self._train_by_filename.values())\n    self._epoch += 1  # epcoh from 1\n\n    batch = []\n    np.random.shuffle(self.data_items)\n    for i, (filename, examples) in enumerate(self.data_items):\n      #logging.info(""example info"", filename, examples)\n\n      # convert txt to ids\n      if self.use_text:\n        text = _load_text(\'.\'.join(filename.split(\'.\')[:-1]))\n        text2id = self._word_table_lookup(text)\n      else:\n        text2id = np.array([0] * self._max_text_len)\n\n      # gen audio or load feat\n      if self._file_suffix == \'.wav\':\n        sr, raw_samples = feat_lib.load_wav(filename)  #pylint: disable=invalid-name\n        for label, seg, clip_id in examples:\n          # examples of one file\n          samples = raw_samples\n          if seg[2]:\n            samples = np.pad(samples, [0, seg[2]], mode=\'constant\')\n          samples = samples[seg[0]:seg[1]]\n          assert len(samples) == self.example_len, ""{} {}"".format(filename, seg)\n\n          labelid = self.class_id(label)\n\n          if self.use_distilling:\n            raise ValueError(""Not Support distilation for *.wav input"")\n          else:\n            class_num = self.taskconf[\'classes\'][\'num\']\n            soft_label = [0] * class_num\n\n          if self.use_text:\n            if clip_id == 0:\n              # only add into batch when meet the first clip\n              batch.append(\n                  (samples, text2id, labelid, filename, clip_id, soft_label))\n          else:\n            batch.append(\n                (samples, text2id, labelid, filename, clip_id, soft_label))\n\n      else:\n        feat = np.load(filename)\n\n        # shape : [nframe, feat_size, 3]\n        if self._feature_type:\n          fbank = feat_lib.add_delta_delta(feat, self._feature_size, order=2)\n          if self._input_channels == 1:\n            fbank = fbank[:, :, 0:1]\n        else:\n          fbank = feat_lib.delta_delta(feat)\n\n        for label, seg, clip_id in examples:\n          feat = fbank\n          #logging.info(""feat shape: {}"".format(feat.shape))\n\n          seg = list(map(self.sample_to_frame, seg))\n          if seg[2]:\n            # need padding\n            feat = np.pad(feat, [(0, seg[2]), (0, 0), (0, 0)], mode=\'constant\')\n\n          feat = feat[seg[0]:seg[1], :, :]\n          expect_nframes = self.sample_to_frame(self.example_len)\n          if len(feat) != expect_nframes:\n            logging.warn(""{} {} {} {} {} {}"".format(\n                filename, seg, len(feat), self.example_len,\n                self.sample_to_frame(self.example_len), seg[2]))\n            feat = np.pad(\n                feat, [(0, expect_nframes - len(feat)), (0, 0), (0, 0)],\n                mode=\'constant\')\n\n          if self.use_distilling:\n            soft_label = self.teacher(feat)\n          else:\n            class_num = self.taskconf[\'classes\'][\'num\']\n            soft_label = [0] * class_num\n\n          # convert string label to int label\n          labelid = self.class_id(label)\n\n          if self.use_text:\n            if clip_id == 0:\n              # only add into batch when meet the first clip\n              batch.append(\n                  (feat, text2id, labelid, filename, clip_id, soft_label))\n          else:\n            batch.append(\n                (feat, text2id, labelid, filename, clip_id, soft_label))\n\n      #if i % 100000:\n      #  logging.info(\'epoch:{} iter exmaple:{} total:{} : {:.2f}%\'.format(\n      #     self._epoch, i, total, i * 100 / total))\n\n      for inputs, texts, label, filepath, clip_id, soft_label in batch:\n        yield inputs, texts, label, filepath, clip_id, soft_label\n\n      batch.clear()\n\n    logging.info(""Out of range"")\n    raise StopIteration  #pylint: disable=stop-iteration-return\n\n  def feature_spec(self):\n    class_num = self.taskconf[\'classes\'][\'num\']\n    if self.input_type == \'samples\':\n      # wavforms\n      output_shapes = (\n          tf.TensorShape([self.example_len]),\n          tf.TensorShape([self.max_text_len]),\n          tf.TensorShape([]),\n          tf.TensorShape([]),\n          tf.TensorShape([]),\n          tf.TensorShape([class_num]),  # soft_label\n      )\n    else:\n      # features\n      output_shapes = (\n          tf.TensorShape(self.feature_shape),  # audio_feat (3000, 40, 3)\n          tf.TensorShape([self.max_text_len]),  # text\n          tf.TensorShape([]),  # label\n          tf.TensorShape([]),  # filename\n          tf.TensorShape([]),  # clip_id\n          tf.TensorShape([class_num]),  # soft_label\n      )\n    output_types = (\n        tf.float32,\n        tf.int32,\n        tf.int32,\n        tf.string,\n        tf.int32,\n        tf.float32,\n    )\n    return output_shapes, output_types\n\n  def preprocess_batch(self, batch):\n    return batch\n\n  #pylint: disable=arguments-differ\n  def dataset(self, mode, batch_size, num_epoch):\n    shapes, types = self.feature_spec()\n    ds = tf.data.Dataset.from_generator(  #pylint: disable=invalid-name\n        generator=lambda: self.generate_data(),  #pylint: disable=unnecessary-lambda\n        output_types=types,\n        output_shapes=shapes,\n    )\n\n    if mode == utils.TRAIN:\n      ds = ds.apply(  #pylint: disable=invalid-name\n          tf.data.experimental.shuffle_and_repeat(\n              buffer_size=100 * batch_size, count=num_epoch, seed=None))\n\n    #pylint: disable=too-many-arguments\n    def make_example(inputs, texts, labels, filenames, clip_ids, soft_labels):\n      features = {\n          \'inputs\': inputs,\n          \'texts\': texts,\n          \'labels\': labels,\n          \'filepath\': filenames,\n          \'clipid\': clip_ids,\n          \'soft_labels\': soft_labels,\n      }\n      return features, labels\n\n    return ds.apply(\n        tf.data.experimental.map_and_batch(\n            make_example, batch_size,\n            drop_remainder=False)).prefetch(tf.data.experimental.AUTOTUNE)\n\n\n@registers.task.register\nclass IEmoCapTask(SpeechClsTask, tf.keras.utils.Sequence):\n  \'\'\' iemocap dataset \'\'\'\n\n  def __init__(self, config, mode):\n    super().__init__(config, mode)\n    self.shuffle = mode == utils.TRAIN\n    self.batch_size = self.config[\'solver\'][\'optimizer\'][\'batch_size\']\n    subset = self.config[\'data\'][\'task\'][\'subset\']\n    subset = subset if subset else \'all\'\n    self.subset = subset\n    assert self.subset in (\'impro\', \'script\', \'all\')\n    logging.info(f""using subset data: {self.subset}, shuffle: {self.shuffle}"")\n\n\n    self.examples_meta = []\n    for _, (filename, examples) in enumerate(self.data_items):\n      for label, seg, clip_id in examples:\n        if clip_id == 0:\n          if self.subset in filename or self.subset == \'all\':\n            # impro script\n            self.examples_meta.append((filename, label, seg))\n\n    self.num_examples = len(self.examples_meta)\n    logging.info(f""num examples: {self.num_examples}"")\n\n    self.on_epoch_end()\n\n  def __len__(self):\n    \'\'\' the number of examples \'\'\'\n    if self.mode == utils.TRAIN:\n      steps_per_epoch = (len(self.examples_meta) -\n                         self.batch_size) / self.batch_size + 1\n    else:\n      steps_per_epoch = len(self.examples_meta) / self.batch_size + 1\n    return int(steps_per_epoch)\n\n  def on_epoch_end(self):\n    \'\'\' update indexes after each epoch\'\'\'\n    self.indexes = np.arange(len(self.examples_meta))\n    if self.shuffle:\n      np.random.shuffle(self.indexes)\n\n  #pylint: disable=too-many-locals\n  def __getitem__(self, batch_index):\n    \'\'\' get batch_index\'s batch data \'\'\'\n    assert self._file_suffix == \'.npy\'\n\n    logging.debug(f\'\'\'\n        batch_index: {batch_index},\n        num_batches: {len(self)},\n        num exapmples: {self.num_examples},\n        label dict: {self.classes}\'\'\')\n    indexes = self.indexes[batch_index * self.batch_size:(batch_index + 1) *\n                           self.batch_size]\n    #logging.info(f""examples meta: {self.examples_meta}"")\n    batch_meta = [self.examples_meta[i] for i in indexes]\n\n    if self.shuffle:\n      random.shuffle(batch_meta)\n\n    logging.debug(f""batch metah: {batch_meta}"")\n    feats = []\n    labels = []\n    filenames = []\n    texts = []\n    for i, (filename, label, seg) in enumerate(batch_meta):\n      feat = np.load(filename)\n\n      # shape : [nframe, feat_size, 3]\n      feat = feat_lib.add_delta_delta(feat, self._feature_size, order=2)\n      if self.feature_shape[-1] == 1:\n        feat = feat[:, :, 0:1]\n\n      seg = list(map(self.sample_to_frame, seg))\n      if seg[2]:\n        # need padding\n        feat = np.pad(feat, [(0, seg[2]), (0, 0), (0, 0)], mode=\'constant\')\n\n      feat = feat[seg[0]:seg[1], :, :]\n      assert len(feat) == self.sample_to_frame(\n          self.example_len), ""{} {} {} {} {} {}"".format(\n              filename, seg, len(feat), self.example_len,\n              self.sample_to_frame(self.example_len), seg[2])\n\n      # convert string label to int label\n      labelid = self.class_id(label)\n      if self.use_text:\n        text = _load_text(\'.\'.join(filename.split(\'.\')[:-1]))\n        text2id = self._word_table_lookup(text)\n        texts.append(text2id)\n      feats.append(feat)\n      filenames.append(filename)\n      labels.append(labelid)\n\n    if self.use_text:\n      features = {\n          \'inputs\': np.array(feats, dtype=np.float32),\n          \'labels\': np.array(labels, dtype=np.int32),\n          \'texts\': np.array(texts, dtype=np.int32),\n      }\n    else:\n      features = {\n          \'inputs\': np.array(feats, dtype=np.float32),\n          \'labels\': np.array(labels, dtype=np.int32),\n      }\n\n    one_hot_label = np.array(labels, dtype=np.int32)\n    one_hot_label = tf.keras.utils.to_categorical(\n        one_hot_label, num_classes=len(self.classes))\n    return features, one_hot_label\n\n  def batch_input_shape(self):\n    \'\'\' batch input TensorShape\'\'\'\n    feat, label = self.__getitem__(0)\n    feat_shape = {}\n    for key, val in feat.items():\n      feat_shape[key] = tf.TensorShape((None,) + val.shape[1:])\n\n    label_shape = tf.TensorShape((None,) + label.shape[1:])\n    return feat_shape, label_shape\n'"
delta/data/task/speech_cls_task_test.py,3,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' emotion speech task unittest\'\'\'\nimport os\nimport shutil\nimport tempfile\nfrom pathlib import Path\n\nimport numpy as np\nimport delta.compat as tf\nfrom absl import logging\n\nfrom delta import utils\nfrom delta.utils.register import registers\nfrom delta.utils.register import import_all_modules_for_register\nfrom delta import PACKAGE_ROOT_DIR\n\n\nclass SpeechClsTaskTest(tf.test.TestCase):\n  \'\'\' emotion task test\'\'\'\n\n  def setUp(self):\n    super().setUp()\n    self.conf_str = \'\'\'\n    data:\n      train:\n        paths:\n        - null \n        segments: null\n      eval:\n        paths:\n        - null\n        segments: null\n      infer:\n        paths:\n        - null \n        segments: null\n      task:\n        name: SpeechClsTask\n        suffix: .npy # file suffix\n        audio:\n          dry_run: false # not save feat\n          # params\n          clip_size: 30 # clip len in seconds\n          stride: 0.5 # stride in ratio of clip_size\n          sr: 8000 # sample rate\n          winlen: 0.025 # window len\n          winstep: 0.01 # window stride\n          nfft: 512 # fft bins, default: 512\n          lowfreq: 0\n          highfreq: null # default: null, 200 points for 800 nfft, 400 points for 1600 nfft\n          preemph: 0.97 # default: 0.97\n          # extractor\n          feature_extractor: tffeat # `tffeat` to use TF feature_extraction .so library, \'pyfeat\' to python_speech_feature\n          feature_name: fbank # fbank or spec\n          save_feat_path: null  # null for dump feat with same dir of wavs\n          feature_size: 40 # extract feature size\n          add_delta_deltas: true # delta deltas\n          # log pwoer\n          log_powspec: false # true, save log power spec; otherwise save power spec\n          # cmvn\n          cmvn: true # apply cmvn or generate cmvn\n          cmvn_path: ./cmvn_conflict.npy # cmvn file\n        text:\n          enable: False\n          vocab_path: /vocab/chars5004_attention.txt\n          vocab_size: 5004 # vocab size\n          max_text_len: 100 # max length for text\n        classes:\n          num: 2\n          vocab:\n            normal: 0\n            conflict: 1\n        num_parallel_calls: 12\n        num_prefetch_batch: 2\n        shuffle_buffer_size: 200000\n        need_shuffle: true\n    solver:\n      name: EmotionSolver\n      optimizer:\n        name: adam\n        epochs: 5 # maximum epochs\n        batch_size: 32 # number of elements in a training batch\n        loss: CrossEntropyLoss\n        label_smoothing: 0.0 # label smoothing rate\n        learning_rate:\n          rate: 0.0001 # learning rate of Adam optimizer\n          type:  exp_decay # learning rate type\n          decay_rate: 0.99  # the lr decay rate\n          decay_steps: 100  # the lr decay_step for optimizer\n        clip_global_norm: 3.0 # clip global norm\n      metrics:\n        pos_label: 1 # int, same to sklearn\n        cals:\n        - name: AccuracyCal\n          arguments: null \n        - name: ConfusionMatrixCal\n          arguments: null\n        - name: PrecisionCal\n          arguments:\n            average: \'binary\'\n        - name: RecallCal\n          arguments:\n            average: \'binary\'\n        - name: F1ScoreCal\n          arguments:\n            average: \'binary\'\n      saver:\n        model_path: ""ckpt/emotion-speech-cls/test""\n        max_to_keep: 10\n        save_checkpoints_steps: 100\n        keep_checkpoint_every_n_hours: 10000\n        checkpoint_every: 100 # the step to save checkpoint\n        summary: false\n        save_summary_steps: 100\n        eval_on_dev_every_secs: 1\n        print_every: 10\n        resume_model_path: """"\n    \'\'\'\n    import_all_modules_for_register()\n    #tempdir = tempfile.mkdtemp()\n    tempdir = self.get_temp_dir()\n\n    config_path = str(Path(tempdir).joinpath(""speech_task.yaml""))\n    logging.info(""config path: {}"".format(config_path))\n    with open(config_path, \'w\', encoding=\'utf-8\') as f:  #pylint: disable=invalid-name\n      f.write(self.conf_str)\n\n    dataset_path = Path(tempdir).joinpath(""data"")\n    if not dataset_path.exists():\n      dataset_path.mkdir()\n    postive_path = dataset_path.joinpath(""conflict"")\n    if not postive_path.exists():\n      postive_path.mkdir()\n    negtive_path = dataset_path.joinpath(""normal"")\n    if not negtive_path.exists():\n      negtive_path.mkdir()\n\n    wav_path = Path(PACKAGE_ROOT_DIR).joinpath(\n        \'data/feat/python_speech_features/english.wav\')\n    for i in range(10):\n      pos_file = postive_path.joinpath(""{}.wav"".format(i))\n      neg_file = negtive_path.joinpath(""{}.wav"".format(i))\n      shutil.copyfile(str(wav_path), str(pos_file))\n      shutil.copyfile(str(wav_path), str(neg_file))\n\n    config = utils.load_config(config_path)\n    config[\'data\'][\'train\'][\'paths\'] = [str(dataset_path)]\n    config[\'data\'][\'eval\'][\'paths\'] = [str(dataset_path)]\n    config[\'data\'][\'infer\'][\'paths\'] = [str(dataset_path)]\n    logging.info(""config: {}"".format(config))\n\n    solver_name = config[\'solver\'][\'name\']\n    self.solver = registers.solver[solver_name](config)\n\n    # config after process\n    self.config = self.solver.config\n\n    task_name = self.config[\'data\'][\'task\'][\'name\']\n    self.task_class = registers.task[task_name]\n\n  def tearDown(self):\n    \'\'\' tear down\'\'\'\n\n  def test_generate_feat(self):\n    \'\'\' test generate feature\'\'\'\n    self.config[\'data\'][\'task\'][\'suffix\'] = \'.wav\'\n    paths = []\n    for mode in [utils.TRAIN, utils.EVAL, utils.INFER]:\n      paths += self.config[\'data\'][mode][\'paths\']\n\n    task = self.task_class(self.config, utils.INFER)\n    task.generate_feat(paths, dry_run=False)\n\n  def test_generate_cmvn(self):\n    \'\'\' test generate cmvn\'\'\'\n    tmp = tempfile.mktemp(suffix=\'cmvn.npy\')\n    self.config[\'data\'][\'task\'][\'audio\'][\'cmvn_path\'] = tmp\n\n    self.config[\'data\'][\'task\'][\'suffix\'] = \'.wav\'\n    self.config[\'data\'][\'task\'][\'stride\'] = 1.0\n    paths = self.config[\'data\'][utils.TRAIN][\'paths\']\n    segments = self.config[\'data\'][utils.TRAIN][\'segments\']\n    self.config[\'data\'][utils.INFER][\'paths\'] = paths\n    self.config[\'data\'][utils.INFER][\'segments\'] = segments\n\n    task = self.task_class(self.config, utils.INFER)\n    task.generate_cmvn(dry_run=False)\n\n    self.assertTrue(\n        os.path.exists(self.config[\'data\'][\'task\'][\'audio\'][\'cmvn_path\']))\n    cmvn = np.load(self.config[\'data\'][\'task\'][\'audio\'][\'cmvn_path\'])\n    self.assertEqual(cmvn.ndim, 4)\n\n  def test_generate_data(self):\n    \'\'\' test generate data\'\'\'\n    self.config[\'data\'][\'task\'][\'suffix\'] = \'.wav\'\n    task = self.task_class(self.config, utils.TRAIN)\n\n    for inputs, texts, label, filename, clip_id, soft_labels in task.generate_data(\n    ):\n      if self.config[\'data\'][\'task\'][\'classes\'][\'positive\'] in filename:\n        logging.info(\n            ""feat shape:{} \\ntext: {} \\nlabels:{} \\nfilename:{} \\nclip_id:{}\\nsoft_labels:{}""\n            .format(inputs.shape, texts, label, filename, clip_id, soft_labels))\n        break\n\n  #pylint: disable=too-many-locals\n  def test_dataset(self):\n    \'\'\' dataset unittest\'\'\'\n    batch_size = 4\n    self.config[\'data\'][\'task\'][\'suffix\'] = \'.wav\'\n    self.config[\'solver\'][\'optimizer\'][\'batch_size\'] = batch_size\n\n    task = self.task_class(self.config, utils.TRAIN)\n\n    dataset = task.input_fn(utils.TRAIN, batch_size, 1)()\n\n    features, labels = dataset.make_one_shot_iterator().get_next()\n    samples = features[\'inputs\']\n    filenames = features[\'filepath\']\n    clip_ids = features[\'clipid\']\n    soft_labels = features[\'soft_labels\']\n\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      while True:\n        batch_inputs, batch_labels, batch_files, batch_clipids, labels_onehot, batch_soft_labels = \\\n           sess.run([samples, labels, filenames, clip_ids, tf.one_hot(labels, 2), soft_labels])\n\n        del labels_onehot\n        logging.info(""feat shape: {}"".format(batch_inputs.shape))\n        logging.info(""labels: {}"".format(batch_labels))\n        logging.info(""filename: {}"".format(batch_files))\n        logging.info(""clip id: {}"".format(batch_clipids))\n        logging.info(""soft_labels: {}"".format(batch_soft_labels))\n        break\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.DEBUG)\n  tf.test.main()\n'"
delta/data/task/text_cls_task.py,13,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Task class for text classification.""""""\n\nimport collections\nimport delta.compat as tf\nfrom absl import logging\n\nfrom delta.data.task.base_text_task import TextTask\n\nfrom delta.data.utils.common_utils import load_npy\nfrom delta.data.utils.common_utils import get_file_len\nfrom delta.data.preprocess.text_ops import process_one_label_dataset\nfrom delta.data.preprocess.text_ops import load_dense_dataset\nfrom delta.data.preprocess.utils import load_vocab_dict\nfrom delta.data.preprocess.text_ops import load_textline_dataset\nfrom delta import utils\nfrom delta.utils.register import registers\nfrom delta.layers.utils import compute_sen_lens\n\n# pylint: disable=too-many-instance-attributes, too-many-locals\n\n\n@registers.task.register\nclass TextClsTask(TextTask):\n  """"""Task class for text classification.""""""\n\n  def __init__(self, config, mode):\n    super().__init__(config, mode)\n    self.infer_no_label = self.config[""data""][utils.INFER].get(\n        \'infer_no_label\', False)\n    self.vocab_min_frequency = self.task_config[\'vocab_min_frequency\']\n    self.text_vocab_file_path = self.task_config[\'text_vocab\']\n    self.label_vocab_file_path = self.task_config[\'label_vocab\']\n    self.max_seq_len = self.task_config[\'max_seq_len\']\n    self.num_classes = self.task_config[\'classes\'][\'num_classes\']\n    self.use_true_length = self.model_config.get(""use_true_length"", False)\n    self.split_token = self.model_config.get(""split_token"", """")\n    self.use_dense = self.task_config[""use_dense""]\n    if self.use_dense:\n      self.dense_input_dim = self.task_config[""dense_input_dim""]\n      self.dense_npy = config[""data""][self.mode][""dense_npy""]\n    self.paths = self.data_config[mode][\'paths\']\n    self.paths_after_pre_process = [\n        one_path + "".after"" for one_path in self.paths\n    ]\n    self.infer_without_label = bool(mode == utils.INFER and self.infer_no_label)\n\n    self.prepare()\n\n  def generate_data(self):\n    """"""Generate data for offline training.""""""\n    if self.infer_without_label:\n      column_num = 1\n      text_ds = load_textline_dataset(self.paths_after_pre_process, column_num)\n    else:\n      column_num = 2\n      label_ds, text_ds = load_textline_dataset(self.paths_after_pre_process,\n                                                column_num)\n\n    input_pipeline_func = self.get_input_pipeline(for_export=False)\n\n    text_ds = text_ds.map(\n        input_pipeline_func, num_parallel_calls=self.num_parallel_calls)\n\n    text_size_ds = text_ds.map(\n        lambda x: compute_sen_lens(x, padding_token=utils.PAD_IDX),\n        num_parallel_calls=self.num_parallel_calls)\n\n    text_ds = tf.data.Dataset.zip((text_ds, text_size_ds))\n\n    if self.use_dense:\n      dense = load_npy(self.dense_npy)\n      dense_ds = load_dense_dataset(dense)\n\n    if self.infer_without_label:\n      if self.use_dense:\n        data_set = tf.data.Dataset.zip((text_ds, dense_ds))\n      else:\n        data_set = text_ds\n    else:\n      label_ds = process_one_label_dataset(label_ds, self.config)\n      if self.use_dense:\n        data_set = tf.data.Dataset.zip((text_ds, dense_ds, label_ds))\n      else:\n        data_set = tf.data.Dataset.zip((text_ds, label_ds))\n\n    vocab_dict = load_vocab_dict(self.text_vocab_file_path)\n    vocab_size = len(vocab_dict)\n    if self.use_true_length and self.split_token != """":\n      if self.split_token not in vocab_dict:\n        raise ValueError(\n            ""The Model uses split token: {}, not in corpus."".format(\n                self.split_token))\n      self.config[\'data\'][\'split_token\'] = int(vocab_dict[self.split_token])\n    self.config[\'data\'][\'vocab_size\'] = vocab_size\n    self.config[\'data\'][\'{}_data_size\'.format(self.mode)] = get_file_len(\n        self.paths_after_pre_process)\n\n    return data_set\n\n  def feature_spec(self):\n    """"""Get shapes for feature.""""""\n    feature_shapes = [(tf.TensorShape([self.max_seq_len]), tf.TensorShape([]))]\n    if self.use_dense:\n      feature_shapes.append(tf.TensorShape(self.dense_input_dim))\n    if not self.infer_without_label:\n      feature_shapes.append(tf.TensorShape([self.num_classes]))\n    if len(feature_shapes) == 1:\n      return feature_shapes[0]\n    return tuple(feature_shapes)\n\n  def export_inputs(self):\n    """"""Inputs for exported model.""""""\n    vocab_dict = load_vocab_dict(self.text_vocab_file_path)\n    vocab_size = len(vocab_dict)\n    if self.use_true_length and self.split_token != """":\n      if self.split_token not in vocab_dict:\n        raise ValueError(\n            ""The Model uses split token: {}, not in corpus."".format(\n                self.split_token))\n      self.config[\'data\'][\'split_token\'] = int(vocab_dict[self.split_token])\n    self.config[\'data\'][\'vocab_size\'] = vocab_size\n\n    input_sentence = tf.placeholder(\n        shape=(None,), dtype=tf.string, name=""input_sentence"")\n\n    input_pipeline_func = self.get_input_pipeline(for_export=True)\n\n    token_ids = input_pipeline_func(input_sentence)\n    token_ids_len = tf.map_fn(lambda x: compute_sen_lens(x, padding_token=0),\n                              token_ids)\n\n    export_data = {\n        ""export_inputs"": {\n            ""input_sentence"": input_sentence\n        },\n        ""model_inputs"": {\n            ""input_x"": token_ids,\n            ""input_x_len"": token_ids_len\n        }\n    }\n\n    if self.use_dense:\n      input_dense = tf.placeholder(\n          shape=(None,), dtype=tf.float32, name=""input_dense"")\n      export_data[""export_inputs""][""input_dense""] = input_dense\n\n    return export_data\n\n  def dataset(self):\n    """"""Data set function""""""\n\n    data_set = self.generate_data()\n    logging.debug(""data_set: {}"".format(data_set))\n    if self.mode == \'train\':\n      if self.need_shuffle:\n        # shuffle batch size and repeat\n        logging.debug(""shuffle and repeat dataset ..."")\n        data_set = data_set.apply(\n            tf.data.experimental.shuffle_and_repeat(\n                buffer_size=self.shuffle_buffer_size, count=None))\n      else:\n        logging.debug(""repeat dataset ..."")\n        data_set = data_set.repeat(count=None)\n\n    feature_shape = self.feature_spec()\n    logging.debug(""feature_shape: {}"".format(feature_shape))\n    data_set = data_set.padded_batch(\n        batch_size=self.batch_size, padded_shapes=feature_shape)\n\n    data_set = data_set.prefetch(self.num_prefetch_batch)\n\n    iterator = data_set.make_initializable_iterator()\n\n    # pylint: disable=unused-variable\n    if self.infer_without_label:\n      if self.use_dense:\n        (input_x, input_x_len), input_dense = iterator.get_next()\n      else:\n        input_x, input_x_len = iterator.get_next()\n    else:\n      if self.use_dense:\n        (input_x, input_x_len), input_dense, input_y = iterator.get_next()\n      else:\n        (input_x, input_x_len), input_y = iterator.get_next()\n\n    input_x_dict = collections.OrderedDict([(""input_x"", input_x)])\n    return_dict = {\n        ""input_x_dict"": input_x_dict,\n        ""input_x_len"": input_x_len,\n        ""iterator"": iterator,\n    }\n\n    if self.use_dense:\n      input_x_dict[""input_dense""] = input_dense\n\n    if not self.infer_without_label:\n      return_dict[""input_y_dict""] = collections.OrderedDict([(""input_y"",\n                                                              input_y)])\n\n    return return_dict\n'"
delta/data/task/text_cls_task_test.py,3,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n# pylint: disable=missing-docstring\n\nimport os\nfrom pathlib import Path\nfrom absl import logging\nimport numpy as np\nimport delta.compat as tf\n\n# delta\nfrom delta import utils\nfrom delta.data.task.text_cls_task import TextClsTask\nfrom delta.utils.register import import_all_modules_for_register\nfrom delta import PACKAGE_ROOT_DIR\n\n\nclass TextClsTaskTest(tf.test.TestCase):\n\n  def setUp(self):\n    super().setUp()\n    import_all_modules_for_register()\n    package_root = Path(PACKAGE_ROOT_DIR)\n    self.config_file = package_root.joinpath(\n        \'../egs/mock_text_cls_data/text_cls/v1/config/han-cls.yml\')\n\n  def tearDown(self):\n    \'\'\' tear down \'\'\'\n\n  def test_english(self):\n    config = utils.load_config(self.config_file)\n    class_num = config[""data""][""task""][""classes""][""num_classes""]\n    task_config = config[""data""][""task""]\n    task_config[""language""] = ""english""\n    task_config[""split_by_space""] = True\n    task_config[""clean_english""] = True\n    data_config = config[""data""]\n    data_config[""train""][""paths""] = [\n        ""egs/mock_text_cls_data/text_cls/v1/data/train.english.txt""\n    ]\n    data_config[""eval""][""paths""] = [\n        ""egs/mock_text_cls_data/text_cls/v1/data/eval.english.txt""\n    ]\n    data_config[""infer""][""paths""] = [\n        ""egs/mock_text_cls_data/text_cls/v1/data/test.english.txt""\n    ]\n    task_config[\n        ""text_vocab""] = ""egs/mock_text_cls_data/text_cls/v1/data/text_vocab.english.txt""\n    task_config[""need_shuffle""] = False\n    config[""model""][""split_token""] = """"\n    task_config[""preparer""][""reuse""] = False\n\n    task = TextClsTask(config, utils.TRAIN)\n\n    # test offline data\n    data = task.dataset()\n    self.assertTrue(""input_x_dict"" in data and\n                    ""input_x"" in data[""input_x_dict""])\n    self.assertTrue(""input_y_dict"" in data and\n                    ""input_y"" in data[""input_y_dict""])\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      sess.run(data[""iterator""].initializer)\n      res = sess.run(\n          [data[""input_x_dict""][""input_x""], data[""input_y_dict""][""input_y""]])\n      logging.debug(res[0][0][:5])\n      logging.debug(res[1][0][:5])\n      self.assertAllEqual(res[0][0][:5], [3, 4, 5, 0, 0])\n      self.assertEqual(np.shape(res[1]), (32, class_num))\n\n    # test online data\n    export_inputs = task.export_inputs()\n    self.assertTrue(""export_inputs"" in export_inputs and\n                    ""input_sentence"" in export_inputs[""export_inputs""])\n    input_sentence = export_inputs[""export_inputs""][""input_sentence""]\n    input_x = export_inputs[""model_inputs""][""input_x""]\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      res = sess.run(input_x, feed_dict={input_sentence: [""All is well.""]})\n      logging.debug(res[0][:5])\n      self.assertAllEqual(res[0][:5], [3, 4, 5, 0, 0])\n\n  # # comment it for no dense data now\n  # def test_english_dense(self):\n  #   config = utils.load_config(self.config_file)\n  #   max_len = config[""model""][""net""][""structure""][""max_len""]\n  #   class_num = config[""data""][""task""][""classes""][""num_classes""]\n  #   data_config = config[""data""]\n  #   task_config = data_config[""task""]\n  #   task_config[""language""] = ""chinese""\n  #   task_config[""split_by_space""] = True\n  #   task_config[""use_dense""] = True\n  #   task_config[""dense_input_dim""] = 31\n  #   data_config[""train""][\n  #       ""dense_npy""] = ""./delta/config/data/text_cls/english/dense_data/ds_train_scale.npy""\n  #   data_config[""eval""][\n  #       ""dense_npy""] = ""./delta/config/data/text_cls/english/dense_data/ds_eval_scale.npy""\n  #   data_config[""infer""][\n  #       ""dense_npy""] = ""./delta/config/data/text_cls/english/dense_data/ds_test_scale.npy""\n  #\n  #   task = TextClsTask(config, utils.TRAIN)\n  #\n  #   # test offline data\n  #   # task.do_pre_process()\n  #   data = task.dataset()\n  #   self.assertTrue(""input_x_dict"" in data and\n  #                   ""input_x"" in data[""input_x_dict""])\n  #   self.assertTrue(""input_x_dict"" in data and\n  #                   ""input_dense"" in data[""input_x_dict""])\n  #   self.assertTrue(""input_y_dict"" in data and\n  #                   ""input_y"" in data[""input_y_dict""])\n  #   with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n  #     sess.run(data[""iterator""].initializer, feed_dict=data[""init_feed_dict""])\n  #     res = sess.run([\n  #         data[""input_x_dict""][""input_x""], data[""input_x_dict""][""input_dense""],\n  #         data[""input_y_dict""][""input_y""]\n  #     ])\n  #     logging.debug(res[0][0])\n  #     logging.debug(res[1][0])\n  #     logging.debug(res[2][0])\n  #     self.assertEqual(np.shape(res[0]), (32, max_len))\n  #     self.assertEqual(np.shape(res[1]), (32, task_config[""dense_input_dim""]))\n  #     self.assertEqual(np.shape(res[2]), (32, class_num))\n  #\n  #   # test online data\n  #   export_inputs = task.export_inputs()\n  #   self.assertTrue(""export_inputs"" in export_inputs and\n  #                   ""input_sentence"" in export_inputs[""export_inputs""])\n  #   input_sentence = export_inputs[""export_inputs""][""input_sentence""]\n  #   input_x = export_inputs[""model_inputs""][""input_x""]\n  #   with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n  #     res = sess.run(input_x, feed_dict={input_sentence: [""All is well.""]})\n  #     logging.debug(res[0])\n  #     self.assertEqual(np.shape(res[0]), (max_len,))\n\n  def test_chinese_split_by_space(self):\n    config = utils.load_config(self.config_file)\n    class_num = config[""data""][""task""][""classes""][""num_classes""]\n    data_config = config[""data""]\n    task_config = data_config[""task""]\n    task_config[""language""] = ""chinese""\n    task_config[""split_by_space""] = True\n    task_config[""use_word""] = False\n    data_config = config[""data""]\n    data_config[""train""][""paths""] = [\n        ""egs/mock_text_cls_data/text_cls/v1/data/train.split_by_space.txt""\n    ]\n    data_config[""eval""][""paths""] = [\n        ""egs/mock_text_cls_data/text_cls/v1/data/eval.split_by_space.txt""\n    ]\n    data_config[""infer""][""paths""] = [\n        ""egs/mock_text_cls_data/text_cls/v1/data/test.split_by_space.txt""\n    ]\n    task_config[\n        ""text_vocab""] = ""egs/mock_text_cls_data/text_cls/v1/data/text_vocab.split_by_space.txt""\n    task_config[""need_shuffle""] = False\n    config[""model""][""split_token""] = """"\n    task_config[""preparer""][""reuse""] = False\n\n    task = TextClsTask(config, utils.TRAIN)\n\n    # test offline data\n    data = task.dataset()\n    self.assertTrue(""input_x_dict"" in data and\n                    ""input_x"" in data[""input_x_dict""])\n    self.assertTrue(""input_y_dict"" in data and\n                    ""input_y"" in data[""input_y_dict""])\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      sess.run(data[""iterator""].initializer)\n      res = sess.run(\n          [data[""input_x_dict""][""input_x""], data[""input_y_dict""][""input_y""]])\n      logging.debug(res[0][0])\n      logging.debug(res[1][0])\n      self.assertAllEqual(res[0][0][:5], [2, 3, 0, 0, 0])\n      self.assertEqual(np.shape(res[1]), (32, class_num))\n\n    # test online data\n    export_inputs = task.export_inputs()\n    self.assertTrue(""export_inputs"" in export_inputs and\n                    ""input_sentence"" in export_inputs[""export_inputs""])\n    input_sentence = export_inputs[""export_inputs""][""input_sentence""]\n    input_x = export_inputs[""model_inputs""][""input_x""]\n\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      res = sess.run(input_x, feed_dict={input_sentence: [""\xe9\x83\xbd \xe6\x8c\xba\xe5\xa5\xbd""]})\n      logging.debug(res[0][:5])\n      logging.debug(np.shape(res[0]))\n      self.assertAllEqual(res[0][:5], [2, 3, 0, 0, 0])\n\n  def test_chinese_word(self):\n    config = utils.load_config(self.config_file)\n    class_num = config[""data""][""task""][""classes""][""num_classes""]\n    data_config = config[""data""]\n    task_config = data_config[""task""]\n    task_config[""language""] = ""chinese""\n    task_config[""split_by_space""] = False\n    task_config[""use_word""] = True\n    data_config = config[""data""]\n    data_config[""train""][""paths""] = \\\n      [""egs/mock_text_cls_data/text_cls/v1/data/train.chinese_word.txt""]\n    data_config[""eval""][""paths""] = \\\n      [""egs/mock_text_cls_data/text_cls/v1/data/eval.chinese_word.txt""]\n    data_config[""infer""][""paths""] = \\\n      [""egs/mock_text_cls_data/text_cls/v1/data/test.chinese_word.txt""]\n    task_config[\n        ""text_vocab""] = ""egs/mock_text_cls_data/text_cls/v1/data/text_vocab.chinese_word.txt""\n    task_config[""need_shuffle""] = False\n    config[""model""][""split_token""] = """"\n    task_config[""preparer""][""reuse""] = False\n\n    task = TextClsTask(config, utils.TRAIN)\n\n    # test offline data\n    data = task.dataset()\n    self.assertTrue(""input_x_dict"" in data and\n                    ""input_x"" in data[""input_x_dict""])\n    self.assertTrue(""input_y_dict"" in data and\n                    ""input_y"" in data[""input_y_dict""])\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      sess.run(data[""iterator""].initializer)\n      res = sess.run(\n          [data[""input_x_dict""][""input_x""], data[""input_y_dict""][""input_y""]])\n      logging.debug(res[0][0])\n      logging.debug(res[1][0])\n      self.assertAllEqual(res[0][0][:5], [2, 0, 0, 0, 0])\n      self.assertEqual(np.shape(res[1]), (32, class_num))\n\n    # test online data\n    export_inputs = task.export_inputs()\n    self.assertTrue(""export_inputs"" in export_inputs and\n                    ""input_sentence"" in export_inputs[""export_inputs""])\n    input_sentence = export_inputs[""export_inputs""][""input_sentence""]\n    input_x = export_inputs[""model_inputs""][""input_x""]\n    shape_op = tf.shape(input_x)\n\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      res, shape_res = sess.run([input_x, shape_op],\n                                feed_dict={input_sentence: [""\xe6\x88\x91\xe5\xbe\x88\xe6\x84\xa4\xe6\x80\x92""]})\n      logging.debug(res[0])\n      logging.debug(np.shape(res[0]))\n      logging.debug(f""shape: {shape_res}"")\n      self.assertAllEqual(shape_res, [1, 1024])\n      self.assertAllEqual(res[0][:5], [4, 5, 0, 0, 0])\n\n  def test_chinese_char(self):\n    config = utils.load_config(self.config_file)\n    max_len = config[""model""][""net""][""structure""][""max_len""]\n    class_num = config[""data""][""task""][""classes""][""num_classes""]\n    data_config = config[""data""]\n    task_config = data_config[""task""]\n    task_config[""language""] = ""chinese""\n    task_config[""split_by_space""] = False\n    task_config[""use_word""] = False\n    data_config = config[""data""]\n    data_config[""train""][""paths""] = [\n        ""egs/mock_text_cls_data/text_cls/v1/data/train.split_by_char.txt""\n    ]\n    data_config[""eval""][""paths""] = [\n        ""egs/mock_text_cls_data/text_cls/v1/data/eval.split_by_char.txt""\n    ]\n    data_config[""infer""][""paths""] = [\n        ""egs/mock_text_cls_data/text_cls/v1/data/test.split_by_char.txt""\n    ]\n    task_config[\n        ""text_vocab""] = ""egs/mock_text_cls_data/text_cls/v1/data/text_vocab.split_by_char.txt""\n    task_config[""need_shuffle""] = False\n    config[""model""][""split_token""] = """"\n    task_config[""preparer""][""reuse""] = False\n\n    task = TextClsTask(config, utils.TRAIN)\n\n    # test offline data\n    data = task.dataset()\n    self.assertTrue(""input_x_dict"" in data and\n                    ""input_x"" in data[""input_x_dict""])\n    self.assertTrue(""input_y_dict"" in data and\n                    ""input_y"" in data[""input_y_dict""])\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      sess.run(data[""iterator""].initializer)\n      res = sess.run([\n          data[""input_x_dict""][""input_x""], data[""input_y_dict""][""input_y""],\n          data[""input_x_len""]\n      ])\n      logging.debug(res[0][0])\n      logging.debug(res[1][0])\n      self.assertAllEqual(res[0][0][:5], [2, 3, 4, 0, 0])\n      self.assertEqual(np.shape(res[0]), (32, max_len))\n      self.assertEqual(np.shape(res[1]), (32, class_num))\n      self.assertEqual(np.shape(res[2]), (32,))\n\n    # test online data\n    export_inputs = task.export_inputs()\n    self.assertTrue(""export_inputs"" in export_inputs and\n                    ""input_sentence"" in export_inputs[""export_inputs""])\n    input_sentence = export_inputs[""export_inputs""][""input_sentence""]\n    input_x = export_inputs[""model_inputs""][""input_x""]\n\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      res = sess.run(input_x, feed_dict={input_sentence: [""\xe9\x83\xbd\xe6\x8c\xba\xe5\xa5\xbd""]})\n      logging.debug(res[0][:5])\n      logging.debug(np.shape(res[0]))\n      self.assertEqual(np.shape(res[0]), (max_len,))\n      self.assertAllEqual(res[0][:5], [2, 3, 4, 0, 0])\n\n  def test_chinese_with_split_token(self):\n    config = utils.load_config(self.config_file)\n    max_len = config[""model""][""net""][""structure""][""max_len""]\n    class_num = config[""data""][""task""][""classes""][""num_classes""]\n    data_config = config[""data""]\n    task_config = data_config[""task""]\n    task_config[""language""] = ""chinese""\n    task_config[""split_by_space""] = False\n    task_config[""use_word""] = True\n    data_config = config[""data""]\n    data_config[""train""][""paths""] = \\\n      [""egs/mock_text_cls_data/text_cls/v1/data/train.split_by_line_mark.txt""]\n    data_config[""eval""][""paths""] = \\\n      [""egs/mock_text_cls_data/text_cls/v1/data/eval.split_by_line_mark.txt""]\n    data_config[""infer""][""paths""] = \\\n      [""egs/mock_text_cls_data/text_cls/v1/data/test.split_by_line_mark.txt""]\n    task_config[\n        ""text_vocab""] = ""egs/mock_text_cls_data/text_cls/v1/data/text_vocab.split_by_line_mark.txt""\n    task_config[""need_shuffle""] = False\n    task_config[""preparer""][""reuse""] = False\n\n    task = TextClsTask(config, utils.TRAIN)\n\n    # test offline data\n    data = task.dataset()\n    self.assertTrue(""input_x_dict"" in data and\n                    ""input_x"" in data[""input_x_dict""])\n    self.assertTrue(""input_y_dict"" in data and\n                    ""input_y"" in data[""input_y_dict""])\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      sess.run(data[""iterator""].initializer)\n      res = sess.run([\n          data[""input_x_dict""][""input_x""], data[""input_y_dict""][""input_y""],\n          data[""input_x_len""]\n      ])\n      logging.debug(res[0][0][:10])\n      logging.debug(res[1][0])\n      self.assertAllEqual(\n          res[0][0][:10],\n          [2, 0, 0, 0, 6, 2, 0, 0, 8, 0])  #[2,3,0,0,6,2,0,0,8,0]\n      self.assertEqual(np.shape(res[0]), (32, max_len))\n      self.assertEqual(np.shape(res[1]), (32, class_num))\n      self.assertEqual(np.shape(res[2]), (32,))\n\n    # test online data\n    export_inputs = task.export_inputs()\n    self.assertTrue(""export_inputs"" in export_inputs and\n                    ""input_sentence"" in export_inputs[""export_inputs""])\n    input_sentence = export_inputs[""export_inputs""][""input_sentence""]\n    input_x = export_inputs[""model_inputs""][""input_x""]\n\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      res = sess.run(input_x, feed_dict={input_sentence: [""\xe6\x88\x91\xe5\xbe\x88\xe6\x84\xa4\xe6\x80\x92\xe3\x80\x82|\xe8\xb6\x85\xe7\xba\xa7\xe7\x94\x9f\xe6\xb0\x94\xef\xbc\x81""]})\n      logging.debug(res[0][:10])\n      logging.debug(np.shape(res[0]))\n      self.assertEqual(np.shape(res[0]), (max_len,))\n      self.assertAllEqual(res[0][:10], [4, 5, 0, 0, 6, 9, 10, 0, 0, 0])\n\n\nif __name__ == ""__main__"":\n  logging.set_verbosity(logging.DEBUG)\n  tf.test.main()\n'"
delta/data/task/text_match_task.py,15,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Task class for text match.""""""\n\nimport collections\nfrom absl import logging\nimport delta.compat as tf\n\nfrom delta.data.task.base_text_task import TextTask\nfrom delta.data.utils.common_utils import get_file_len\nfrom delta.data.preprocess.text_ops import load_textline_dataset\nfrom delta.data.preprocess.utils import load_vocab_dict\nfrom delta.data.preprocess.text_ops import process_one_label_dataset\nfrom delta.utils.register import registers\nfrom delta.layers.utils import compute_sen_lens\nfrom delta import utils\n\n\n# pylint: disable=too-many-instance-attributes\n\n\n@registers.task.register\nclass TextMatchTask(TextTask):\n  """"""Task class for text Match.""""""\n\n  def __init__(self, config, mode):\n    super().__init__(config, mode)\n\n    self.vocab_min_frequency = self.task_config[\'vocab_min_frequency\']\n    self.text_vocab_file_path = self.task_config[\'text_vocab\']\n    self.label_vocab_file_path = self.task_config[\'label_vocab\']\n    self.max_seq_len = self.task_config[\'max_seq_len\']\n    self.num_classes = self.task_config[\'classes\'][\'num_classes\']\n\n    self.paths = self.data_config[mode][\'paths\']\n    self.paths_after_pre_process = [\n      one_path + "".after"" for one_path in self.paths\n    ]\n    self.infer_no_label = self.config[""data""][utils.INFER].get(\n      \'infer_no_label\', False)\n    self.infer_without_label = bool(mode == utils.INFER and self.infer_no_label)\n\n    self.prepare()\n\n  # pylint: disable=too-many-locals\n  def generate_data(self):\n    """"""Generate data for offline training.""""""\n    if self.infer_without_label:\n      column_num = 2\n      text_ds_left, text_ds_right = load_textline_dataset(\n        self.paths_after_pre_process, column_num)\n    else:\n      column_num = 3\n      label, text_ds_left, text_ds_right = load_textline_dataset(\n        self.paths_after_pre_process, column_num)\n\n    input_pipeline_func = self.get_input_pipeline(for_export=False)\n    text_ds_left = text_ds_left.map(\n      input_pipeline_func, num_parallel_calls=self.num_parallel_calls)\n    text_ds_right = text_ds_right.map(\n      input_pipeline_func, num_parallel_calls=self.num_parallel_calls)\n    text_size_ds_left = text_ds_left.map(\n      lambda x: compute_sen_lens(x, padding_token=0),\n      num_parallel_calls=self.num_parallel_calls)\n    text_size_ds_right = text_ds_right.map(\n      lambda x: compute_sen_lens(x, padding_token=0),\n      num_parallel_calls=self.num_parallel_calls)\n    text_ds_left_right = tf.data.Dataset.zip((text_ds_left, text_ds_right))\n    text_len_left_right = tf.data.Dataset.zip(\n      (text_size_ds_left, text_size_ds_right))\n    if self.infer_without_label:\n      data_set_left_right = text_ds_left_right\n    else:\n      label_ds = process_one_label_dataset(label, self.config)\n      data_set_left_right = tf.data.Dataset.zip((text_ds_left_right, label_ds))\n    vocab_dict = load_vocab_dict(self.text_vocab_file_path)\n    vocab_size = len(vocab_dict)\n\n    self.config[\'data\'][\'vocab_size\'] = vocab_size\n    self.config[\'data\'][\'{}_data_size\'.format(self.mode)] = get_file_len(\n      self.paths_after_pre_process)\n\n    return data_set_left_right, text_len_left_right\n\n  def feature_spec(self):\n    """"""Get shapes for feature.""""""\n    feature_shapes = [(tf.TensorShape([self.max_seq_len]),\n                       tf.TensorShape([self.max_seq_len]))]\n    if not self.infer_without_label:\n      feature_shapes.append(tf.TensorShape([self.num_classes]))\n\n    feature_shapes = [tuple(feature_shapes), (tf.TensorShape([]), tf.TensorShape([]))]\n\n    if len(feature_shapes) == 1:\n      return feature_shapes[0]\n\n    return tuple(feature_shapes)\n\n  def export_inputs(self):\n    """"""Inputs for exported model.""""""\n    vocab_dict = load_vocab_dict(self.text_vocab_file_path)\n    vocab_size = len(vocab_dict)\n    self.config[\'data\'][\'vocab_size\'] = vocab_size\n\n    input_sent_left = tf.placeholder(\n      shape=(None,), dtype=tf.string, name=""input_sent_left"")\n    input_sent_right = tf.placeholder(\n      shape=(None,), dtype=tf.string, name=""input_sent_right"")\n    input_pipeline_func = self.get_input_pipeline(for_export=True)\n\n    token_ids_left = input_pipeline_func(input_sent_left)\n    token_ids_right = input_pipeline_func(input_sent_right)\n    token_ids_len_left = tf.map_fn(\n      lambda x: compute_sen_lens(x, padding_token=0), token_ids_left)\n    token_ids_len_right = tf.map_fn(\n      lambda x: compute_sen_lens(x, padding_token=0), token_ids_right)\n\n    export_data = {\n      ""export_inputs"": {\n        ""input_sent_left"": input_sent_left,\n        ""input_sent_right"": input_sent_right,\n      },\n      ""model_inputs"": {\n        ""input_x_left"": token_ids_left,\n        ""input_x_right"": token_ids_right,\n        ""input_x_left_len"": token_ids_len_left,\n        ""input_x_right_len"": token_ids_len_right,\n        ""input_x_len"": [token_ids_len_left, token_ids_len_right]\n      }\n    }\n    return export_data\n\n  def dataset(self):\n    """"""Data set function""""""\n    ds_left_right, ds_left_right_len = self.generate_data()\n    text_ds_left_right = tf.data.Dataset.zip((ds_left_right, ds_left_right_len))\n\n    if self.mode == \'train\':\n      if self.need_shuffle:\n        # shuffle batch size and repeat\n        logging.debug(""shuffle and repeat dataset ..."")\n        text_ds_left_right = text_ds_left_right.apply(\n          tf.data.experimental.shuffle_and_repeat(\n            buffer_size=self.shuffle_buffer_size, count=None))\n      else:\n        logging.debug(""repeat dataset ..."")\n        text_ds_left_right = text_ds_left_right.repeat(count=None)\n\n    feature_shape = self.feature_spec()\n    logging.debug(""feature_shape: {}"".format(feature_shape))\n\n    # logging.debug(""data_set_left_right\xef\xbc\x9a{}"".format(data_set_left_right))\n\n    text_ds_left_right = text_ds_left_right.padded_batch(\n      batch_size=self.batch_size, padded_shapes=feature_shape)\n\n    text_ds_left_right = text_ds_left_right.prefetch(self.num_prefetch_batch)\n\n    iterator = text_ds_left_right.make_initializable_iterator()\n    # pylint: disable=unused-variable\n    if self.infer_without_label:\n      (input_x_left, input_x_right), (input_x_left_len, input_x_right_len) = iterator.get_next()\n    else:\n      ((input_x_left, input_x_right), input_y), (input_x_left_len, input_x_right_len) = iterator.get_next()\n\n    input_x_dict = collections.OrderedDict([(""input_x_left"", input_x_left),\n                                            (""input_x_right"", input_x_right),\n                                            (""input_x_left_len"", input_x_left_len),\n                                            (""input_x_right_len"", input_x_right_len),\n                                            ])\n    input_x_len = collections.OrderedDict([\n      (""input_x_left_len"", input_x_left_len),\n      (""input_x_right_len"", input_x_right_len)\n    ])\n\n    return_dict = {\n      ""input_x_dict"": input_x_dict,\n      ""input_x_len"": input_x_len,\n      ""iterator"": iterator,\n    }\n\n    if not self.infer_without_label:\n      return_dict[""input_y_dict""] = collections.OrderedDict([(""input_y"",\n                                                              input_y)])\n    return return_dict\n\n'"
delta/data/task/text_match_task_test.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n"""""" text match task unit test """"""\n\nimport os\nfrom pathlib import Path\nfrom absl import logging\nimport numpy as np\nimport delta.compat as tf\n\nfrom delta import utils\nfrom delta.data.task.text_match_task import TextMatchTask\nfrom delta.utils.register import import_all_modules_for_register\nfrom delta import PACKAGE_ROOT_DIR\n\n\nclass TextMatchTaskTest(tf.test.TestCase):\n  """"""text match task test""""""\n\n  # pylint: disable=invalid-name\n  def setUp(self):\n    super().setUp()\n    package_root = Path(PACKAGE_ROOT_DIR)\n    self.config_file = package_root.joinpath(\n        \'../egs/mock_text_match_data/text_match/v1/config/rnn-match-mock.yml\')\n    import_all_modules_for_register()\n\n  def tearDown(self):\n    \'\'\' tear down \'\'\'\n\n  # pylint: disable=too-many-locals\n\n  def test_english(self):\n    """""" test text match task of english data """"""\n    config = utils.load_config(self.config_file)\n    max_seq_len = config[""data""][""task""][""max_seq_len""]\n    class_num = config[""data""][""task""][""classes""][""num_classes""]\n    batch_size = config[""data""][""task""][""batch_size""]\n    data_config = config[""data""]\n    task_config = data_config[""task""]\n    task_config[""language""] = ""english""\n    task_config[""split_by_space""] = False\n    task_config[""use_word""] = True\n    task_config[\n        ""text_vocab""] = ""egs/mock_text_match_data/text_match/v1/data/text_vocab.txt""\n    task_config[""need_shuffle""] = False\n\n    # generate_mock_files(config)\n\n    task = TextMatchTask(config, utils.TRAIN)\n\n    # test offline data\n    data = task.dataset()\n    self.assertTrue(""input_x_dict"" in data and\n                    ""input_x_left"" in data[""input_x_dict""] and\n                    ""input_x_right"" in data[""input_x_dict""])\n    self.assertTrue(""input_y_dict"" in data and\n                    ""input_y"" in data[""input_y_dict""])\n    # with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n    #  sess.run(data[""iterator""].initializer)\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      sess.run([data[""iterator""].initializer])\n      res = sess.run([\n          data[""input_x_dict""][""input_x_left""],\n          data[""input_x_dict""][""input_x_right""],\n          data[""input_y_dict""][""input_y""],\n          data[""input_x_len""][""input_x_left_len""],\n          data[""input_x_len""][""input_x_right_len""],\n      ])\n      logging.debug(res[0][0][:10])\n      logging.debug(res[1][0])\n      logging.debug(res[2][0])\n      logging.debug(res[3])\n      logging.debug(res[4])\n\n      self.assertAllEqual(res[0][0][:10], [2, 3, 4, 5, 6, 0, 0, 0, 0, 0])\n      self.assertEqual(np.shape(res[0]), (batch_size, max_seq_len))\n      self.assertEqual(np.shape(res[1]), (batch_size, max_seq_len))\n      self.assertEqual(np.shape(res[2]), (batch_size, class_num))\n      self.assertEqual(np.shape(res[3]), (batch_size,))\n      self.assertEqual(np.shape(res[4]), (batch_size,))\n    # test online data\n    export_inputs = task.export_inputs()\n    self.assertTrue(""export_inputs"" in export_inputs and\n                    ""input_sent_left"" in export_inputs[""export_inputs""] and\n                    ""input_sent_right"" in export_inputs[""export_inputs""])\n\n    input_sent_left = export_inputs[""export_inputs""][""input_sent_left""]\n    input_sent_right = export_inputs[""export_inputs""][""input_sent_right""]\n    input_x_left = export_inputs[""model_inputs""][""input_x_left""]\n    input_x_right = export_inputs[""model_inputs""][""input_x_right""]\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      # sess.run(data[""iterator""].initializer)\n      sess.run(data[""iterator""].initializer)\n      res1, res2 = sess.run(\n          [input_x_left, input_x_right],\n          feed_dict={\n              input_sent_left: [""How should I approach forgiveness?""],\n              input_sent_right: [""I got chickenpox as a child.""]\n          })\n      logging.debug(res1[0][:10])\n      logging.debug(res2[0][:10])\n      self.assertAllEqual(res1[0][:10], [2, 3, 4, 5, 6, 0, 0, 0, 0, 0])\n      self.assertAllEqual(res2[0][:10], [4, 7, 8, 9, 10, 11, 0, 0, 0, 0])\n      self.assertEqual(np.shape(res1[0]), (max_seq_len,))\n      self.assertEqual(np.shape(res2[0]), (max_seq_len,))\n\n\nif __name__ == ""__main__"":\n  logging.set_verbosity(logging.DEBUG)\n  tf.test.main()\n'"
delta/data/task/text_nlu_joint_task.py,10,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' NLU joint learning task \'\'\'\n\nimport collections\nimport delta.compat as tf\nfrom absl import logging\n\nfrom delta.data.task.base_text_task import TextTask\nfrom delta.data.preprocess.text_ops import process_one_label_dataset\nfrom delta.data.preprocess.text_ops import process_multi_label_dataset\nfrom delta.data.utils.common_utils import get_file_len\nfrom delta.data.preprocess.text_ops import load_textline_dataset\nfrom delta.data.preprocess.utils import get_vocab_size\nfrom delta.utils.register import registers\nfrom delta.layers.utils import compute_sen_lens\nfrom delta import utils\n\n# pylint: disable=too-many-instance-attributes\n\n\n@registers.task.register\nclass TextNLUJointTask(TextTask):\n  """"""Task class for NLU joint learning.""""""\n\n  def __init__(self, config, mode):\n    super().__init__(config, mode)\n\n    self.vocab_min_frequency = self.task_config[\'vocab_min_frequency\']\n    self.text_vocab_file_path = self.task_config[\'text_vocab\']\n    self.label_vocab_file_path = self.task_config[\'label_vocab\']\n    self.max_seq_len = self.task_config[\'max_seq_len\']\n    self.intent_num_classes = self.task_config[\'classes\'][0][\'num_classes\']\n\n    self.paths = self.data_config[mode][\'paths\']\n    self.paths_after_pre_process = [\n        one_path + "".after"" for one_path in self.paths\n    ]\n    self.infer_no_label = self.config[""data""][utils.INFER].get(\n        \'infer_no_label\', False)\n    self.infer_without_label = bool(mode == utils.INFER and self.infer_no_label)\n\n    self.prepare()\n\n  def load_text_dataset(self, text_ds):\n    """"""Load text data set.""""""\n    logging.info(""Loading text dataset..."")\n    input_pipeline_func = self.get_input_pipeline(for_export=False)\n    text_ds = text_ds.map(\n        input_pipeline_func, num_parallel_calls=self.num_parallel_calls)\n    text_size_ds = text_ds.map(\n        lambda x: compute_sen_lens(x, padding_token=0),\n        num_parallel_calls=self.num_parallel_calls)\n    text_ds = tf.data.Dataset.zip((text_ds, text_size_ds))\n\n    return text_ds\n\n  def generate_data(self):\n    """"""Generate data for offline training.""""""\n    if self.infer_without_label:\n      column_num = 1\n      text_ds = load_textline_dataset(self.paths_after_pre_process, column_num)\n    else:\n      column_num = 3\n      intent_label_ds, slots_label_ds, text_ds = load_textline_dataset(\n          self.paths_after_pre_process, column_num)\n\n    logging.info(""Loading text dataset..."")\n    input_pipeline_func = self.get_input_pipeline(for_export=False)\n    text_ds = text_ds.map(\n        input_pipeline_func, num_parallel_calls=self.num_parallel_calls)\n    text_size_ds = text_ds.map(\n        lambda x: compute_sen_lens(x, padding_token=0),\n        num_parallel_calls=self.num_parallel_calls)\n    text_ds = tf.data.Dataset.zip((text_ds, text_size_ds))\n\n    if self.infer_without_label:\n      data_set = text_ds\n    else:\n      intent_label_ds = process_one_label_dataset(\n          intent_label_ds, self.config, output_index=0)\n      slots_label_ds = process_multi_label_dataset(\n          slots_label_ds, self.config, output_index=1)\n      data_set = tf.data.Dataset.zip((text_ds, intent_label_ds, slots_label_ds))\n\n    self.config[\'data\'][\'vocab_size\'] = get_vocab_size(\n        self.text_vocab_file_path)\n    self.config[\'data\'][\'{}_data_size\'.format(self.mode)] = get_file_len(\n        self.paths_after_pre_process)\n\n    return data_set\n\n  def feature_spec(self):\n    """"""Get shapes for feature.""""""\n    feature_shapes = [(tf.TensorShape([self.max_seq_len]), tf.TensorShape([]))]\n    if not self.infer_without_label:\n      feature_shapes.append(tf.TensorShape([self.intent_num_classes]))\n      feature_shapes.append(tf.TensorShape([self.max_seq_len]))\n    if len(feature_shapes) == 1:\n      return feature_shapes[0]\n    return tuple(feature_shapes)\n\n  def export_inputs(self):\n    """"""Inputs for exported model.""""""\n    self.config[\'data\'][\'vocab_size\'] = get_vocab_size(\n        self.text_vocab_file_path)\n    input_sentence = tf.placeholder(\n        shape=(None,), dtype=tf.string, name=""input_sentence"")\n\n    input_pipeline_func = self.get_input_pipeline(for_export=True)\n    token_ids = input_pipeline_func(input_sentence)\n    token_ids_len = tf.map_fn(lambda x: compute_sen_lens(x, padding_token=0),\n                              token_ids)\n\n    export_data = {\n        ""export_inputs"": {\n            ""input_sentence"": input_sentence\n        },\n        ""model_inputs"": {\n            ""input_x"": token_ids,\n            ""input_x_len"": token_ids_len\n        }\n    }\n\n    return export_data\n\n  def dataset(self):\n    """"""Dataset function""""""\n    data_set = self.generate_data()\n\n    if self.mode == \'train\':\n      if self.need_shuffle:\n        # shuffle batch size and repeat\n        logging.debug(""shuffle and repeat dataset ..."")\n        data_set = data_set.apply(\n            tf.data.experimental.shuffle_and_repeat(\n                buffer_size=self.shuffle_buffer_size, count=None))\n      else:\n        logging.debug(""repeat dataset ..."")\n        data_set = data_set.repeat(count=None)\n\n    feature_shape = self.feature_spec()\n    logging.debug(""feature_shape: {}"".format(feature_shape))\n\n    data_set = data_set.padded_batch(\n        batch_size=self.batch_size, padded_shapes=feature_shape)\n\n    data_set = data_set.prefetch(self.num_prefetch_batch)\n\n    iterator = data_set.make_initializable_iterator()\n\n    if self.infer_without_label:\n      input_x, input_x_len = iterator.get_next()\n    else:\n      (input_x,\n       input_x_len), input_intent_y, input_slots_y = iterator.get_next()\n\n    input_x_dict = collections.OrderedDict([(""input_x"", input_x)])\n    return_dict = {\n        ""input_x_dict"": input_x_dict,\n        ""input_x_len"": input_x_len,\n        ""iterator"": iterator\n    }\n\n    if not self.infer_without_label:\n      return_dict[""input_y_dict""] = {""input_y"": (input_intent_y, input_slots_y)}\n\n    return return_dict\n'"
delta/data/task/text_nlu_joint_task_test.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' NLU joint learning task unittest \'\'\'\n\nimport os\nfrom pathlib import Path\nimport numpy as np\nimport delta.compat as tf\nfrom absl import logging\nfrom delta import utils\nfrom delta.data.task.text_nlu_joint_task import TextNLUJointTask\nfrom delta.utils.register import import_all_modules_for_register\nfrom delta import PACKAGE_ROOT_DIR\n\n\nclass NLUJointTaskTest(tf.test.TestCase):\n  \'\'\' NLU joint task test\'\'\'\n\n  def setUp(self):\n    super().setUp()\n    import_all_modules_for_register()\n    package_root = Path(PACKAGE_ROOT_DIR)\n    self.config_file = package_root.joinpath(\n        \'../egs/mock_text_nlu_joint_data/nlu-joint/v1/config/nlu_joint.yml\')\n\n  def tearDown(self):\n    \'\'\' tear down \'\'\'\n\n  def test_english(self):\n    """""" test NLU joint task of chiniese data, split sentences by space""""""\n\n    config = utils.load_config(self.config_file)\n    max_len = config[""model""][""net""][""structure""][""max_len""]\n    batch_size = config[""data""][""task""][""batch_size""]\n    data_config = config[""data""]\n    task_config = data_config[""task""]\n    task_config[""language""] = ""english""\n    task_config[""split_by_space""] = False\n    task_config[""use_word""] = True\n    task_config[\n        ""text_vocab""] = ""egs/mock_text_nlu_joint_data/nlu-joint/v1/data/text_vocab.txt""\n    task_config[""need_shuffle""] = False\n\n    # generate_mock_files(config)\n    task = TextNLUJointTask(config, utils.TRAIN)\n\n    # test offline data\n    data = task.dataset()\n    self.assertTrue(""input_x_dict"" in data and\n                    ""input_x"" in data[""input_x_dict""])\n    self.assertTrue(""input_y_dict"" in data and\n                    ""input_y"" in data[""input_y_dict""])\n    input_intent_y, input_slots_y = data[""input_y_dict""][""input_y""]\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      sess.run(data[""iterator""].initializer)\n      res = sess.run([\n          data[""input_x_dict""][""input_x""], data[""input_x_len""], input_intent_y,\n          input_slots_y\n      ])\n\n      logging.debug(res[0][0][:5])\n      logging.debug(res[1][0])\n      logging.debug(res[2])\n      logging.debug(res[3])\n\n      self.assertAllEqual(res[0][0][:5], [5, 6, 8, 9, 0])\n      self.assertEqual(np.shape(res[0]), (batch_size, max_len))\n      self.assertEqual(np.shape(res[1]), (batch_size,))\n      self.assertEqual(np.shape(res[2]), (batch_size, 2))\n      self.assertEqual(np.shape(res[3]), (batch_size, max_len))\n\n    # test online data\n    export_inputs = task.export_inputs()\n    self.assertTrue(""export_inputs"" in export_inputs and\n                    ""input_sentence"" in export_inputs[""export_inputs""])\n    input_sentence = export_inputs[""export_inputs""][""input_sentence""]\n    input_x = export_inputs[""model_inputs""][""input_x""]\n\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      sess.run(data[""iterator""].initializer)\n      res = sess.run(input_x, feed_dict={input_sentence: [""i am happy""]})\n      logging.debug(res[0][:5])\n      logging.debug(np.shape(res[0]))\n      self.assertAllEqual(res[0][:5], [2, 3, 7, 0, 0])\n      self.assertEqual(np.shape(res[0]), (max_len,))\n\n\nif __name__ == ""__main__"":\n  logging.set_verbosity(logging.DEBUG)\n  tf.test.main()\n'"
delta/data/task/text_seq2seq_task.py,14,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Task class for text sequence to sequence.""""""\n\nimport collections\nimport os\n\nimport delta.compat as tf\nfrom absl import logging\n\nfrom delta import utils\nfrom delta.data.preprocess.text_ops import tokenize_sentence\nfrom delta.data.preprocess.utils import load_vocab_dict\nfrom delta.data.task.base_text_task import TextTask\nfrom delta.data.preprocess.text_ops import load_textline_dataset\nfrom delta.data.utils.common_utils import get_file_len\nfrom delta.layers.utils import compute_sen_lens\nfrom delta.utils.register import registers\n\n# pylint: disable=too-many-instance-attributes, too-many-locals\n\n\n@registers.task.register\nclass TextS2STask(TextTask):\n  """"""Task class for text sequence to sequence.""""""\n  START_TOKEN = \'<sos>\'\n  END_TOKEN = \'<eos>\'\n\n  def __init__(self, config, mode):\n    super().__init__(config, mode)\n\n    self.src_paths = self.data_config[mode][\'paths\'][\'source\']\n    self.tgt_paths = self.data_config[mode][\'paths\'][\'target\']\n\n    self.vocab_min_frequency = self.task_config[\'vocab_min_frequency\']\n    self.text_vocab_file_path = self.task_config[\'text_vocab\']\n    self.label_vocab_file_paths = self.task_config[\'label_vocab\']\n    if not isinstance(self.label_vocab_file_paths, list):\n      self.label_vocab_file_paths = [self.label_vocab_file_paths]\n    self.use_label_vocab = self.task_config[\'use_label_vocab\']\n\n    self.max_enc_len = self.task_config[\'max_enc_len\']\n    self.max_dec_len = self.task_config[\'max_dec_len\']\n\n    self.src_paths_after_pre_process = [\n        one_path + "".after"" for one_path in self.src_paths\n    ]\n    self.tgt_paths_after_pre_process = [\n        one_path + "".after"" for one_path in self.tgt_paths\n    ]\n    self.infer_no_label = self.config[""data""][utils.INFER].get(\n        \'infer_no_label\', False)\n    self.infer_without_label = bool(mode == utils.INFER and self.infer_no_label)\n\n    self.prepare()\n\n  def common_process_pipeline(self, batch):\n    """"""\n    Data pipeline function for common process.\n    This function is used both by online training and offline inference.\n    """"""\n    vocab_path = os.path.abspath(self.text_vocab_file_path)\n    token_ids = self.text_pipeline_func(batch, self.max_enc_len, vocab_path)\n    return token_ids\n\n  def text_pipeline_func(self, batch, seq_len, vocab_path):\n    """"""\n    Data pipeline function for core text process.\n    """"""\n    vocab_path = os.path.abspath(vocab_path)\n    token_ids = tokenize_sentence(batch, seq_len, vocab_path)\n    return token_ids\n\n  def exclude_padding(self, batch):\n    x_binary = tf.cast(tf.not_equal(batch, utils.PAD_IDX), tf.int32)\n    sen_lens = tf.reduce_sum(x_binary, axis=-1)\n    return batch[:sen_lens]\n\n  def generate_data(self):\n    """"""Generate data for offline training.""""""\n\n    column_num = 1\n    src_path = self.src_paths_after_pre_process\n    target_path = self.tgt_paths_after_pre_process\n\n    src_ds = load_textline_dataset([src_path], column_num)\n\n    src_ds = src_ds[0]\n\n    input_pipeline_func = self.get_input_pipeline(for_export=False)\n\n    src_ds = src_ds.map(\n        input_pipeline_func, num_parallel_calls=self.num_parallel_calls)\n\n    src_size_ds = src_ds.map(\n        lambda x: compute_sen_lens(x, padding_token=utils.PAD_IDX),\n        num_parallel_calls=self.num_parallel_calls)\n\n    # src_ds = src_ds.map(\n    #     self.exclude_padding, num_parallel_calls=self.num_parallel_calls)\n\n    if self.infer_without_label:\n      data_set = tf.data.Dataset.zip((src_ds, src_size_ds))\n\n    else:\n      tgt = load_textline_dataset([target_path], column_num)\n      tgt = tgt[0]\n      tgt_out_ds = tgt.map(lambda x: x + \' \' + self.END_TOKEN)\n      tgt_in_ds = tgt.map(lambda x: self.START_TOKEN + \' \' + x)\n\n      tgt_in_ds = tgt_in_ds.map(\n          lambda batch: self.text_pipeline_func(batch, self.max_dec_len, self.\n                                                text_vocab_file_path),\n          num_parallel_calls=self.num_parallel_calls)\n\n      tgt_in_size_ds = tgt_in_ds.map(\n          lambda x: compute_sen_lens(x, padding_token=utils.PAD_IDX),\n          num_parallel_calls=self.num_parallel_calls)\n\n      # tgt_in_ds = tgt_in_ds.map(\n      #     self.exclude_padding, num_parallel_calls=self.num_parallel_calls)\n\n      inp_ds = tf.data.Dataset.zip(\n          (src_ds, src_size_ds, tgt_in_ds, tgt_in_size_ds))\n\n      if self.use_label_vocab:\n        target_vocab_file_path = self.label_vocab_file_paths[0]\n      else:\n        target_vocab_file_path = self.text_vocab_file_path\n      tgt_out_ds = tgt_out_ds.map(\n          lambda batch: self.text_pipeline_func(batch, self.max_dec_len,\n                                                target_vocab_file_path),\n          num_parallel_calls=self.num_parallel_calls)\n\n      # tgt_out_ds = tgt_out_ds.map(\n      #     self.exclude_padding, num_parallel_calls=self.num_parallel_calls)\n      data_set = tf.data.Dataset.zip((inp_ds, tgt_out_ds))\n\n    vocab_dict = load_vocab_dict(self.text_vocab_file_path)\n    vocab_size = len(vocab_dict)\n    label_vocab_dict = load_vocab_dict(self.label_vocab_file_paths[0])\n    label_vocab_size = len(label_vocab_dict)\n    data_size = get_file_len(self.src_paths_after_pre_process)\n    self.config[\'data\'][\'vocab_size\'] = vocab_size\n    self.config[\'data\'][\'label_vocab_size\'] = label_vocab_size\n    self.config[\'data\'][\'{}_data_size\'.format(self.mode)] = data_size\n\n    return data_set\n\n  def feature_spec(self):\n    """"""Get shapes for feature.""""""\n    if not self.infer_without_label:\n      feature_shapes = [\n          (tf.TensorShape([tf.Dimension(None)]), tf.TensorShape([]),\n           tf.TensorShape([tf.Dimension(None)]), tf.TensorShape([]))\n      ]\n      feature_shapes.append(tf.TensorShape([tf.Dimension(None)]))\n    else:\n      feature_shapes = [(tf.TensorShape([tf.Dimension(None)]),\n                         tf.TensorShape([]))]\n    if len(feature_shapes) == 1:\n      return feature_shapes[0]\n    return tuple(feature_shapes)\n\n  def export_inputs(self):\n    """"""Inputs for exported model.""""""\n    vocab_dict = load_vocab_dict(self.text_vocab_file_path)\n    vocab_size = len(vocab_dict)\n    label_vocab_dict = load_vocab_dict(self.label_vocab_file_paths[0])\n    label_vocab_size = len(label_vocab_dict)\n    self.config[\'data\'][\'vocab_size\'] = vocab_size\n    self.config[\'data\'][\'label_vocab_size\'] = label_vocab_size\n\n    input_sentence = tf.placeholder(\n        shape=(None,), dtype=tf.string, name=""input_sentence"")\n\n    input_pipeline_func = self.get_input_pipeline(for_export=True)\n\n    token_ids = input_pipeline_func(input_sentence)\n    token_ids_len = tf.map_fn(lambda x: compute_sen_lens(x, padding_token=0),\n                              token_ids)\n\n    export_data = {\n        ""export_inputs"": {\n            ""input_sentence"": input_sentence\n        },\n        ""model_inputs"": {\n            ""input_enc_x"": token_ids,\n            ""input_x_len"": token_ids_len\n        }\n    }\n\n    return export_data\n\n  def dataset(self):\n    """"""Data set function""""""\n\n    data_set = self.generate_data()\n    logging.debug(""data_set: {}"".format(data_set))\n    if self.need_shuffle and self.mode == \'train\':\n      # shuffle batch size and repeat\n      logging.debug(""shuffle dataset ..."")\n      data_set = data_set.apply(\n          tf.data.experimental.shuffle_and_repeat(\n              buffer_size=self.shuffle_buffer_size, count=None))\n\n    feature_shape = self.feature_spec()\n    logging.debug(""feature_shape: {}"".format(feature_shape))\n    data_set = data_set.padded_batch(\n        batch_size=self.batch_size, padded_shapes=feature_shape)\n\n    data_set = data_set.prefetch(self.num_prefetch_batch)\n\n    iterator = data_set.make_initializable_iterator()\n\n    # pylint: disable=unused-variable\n    if self.infer_without_label:\n      input_enc_x, input_enc_x_len = iterator.get_next()\n      input_x_dict = collections.OrderedDict([(""input_enc_x"", input_enc_x)])\n\n    else:\n      (input_enc_x, input_enc_x_len, input_dec_x,\n       input_dec_x_len), input_y = iterator.get_next()\n\n      input_x_dict = collections.OrderedDict([(""input_enc_x"", input_enc_x),\n                                              (""input_dec_x"", input_dec_x)])\n\n    return_dict = {\n        ""input_x_dict"": input_x_dict,\n        ""input_x_len"": input_enc_x_len,\n        ""iterator"": iterator,\n    }\n\n    if not self.infer_without_label:\n      return_dict[""input_y_dict""] = collections.OrderedDict([(""input_y"",\n                                                              input_y)])\n      return_dict[""input_y_len""] = input_dec_x_len\n\n    return return_dict\n'"
delta/data/task/text_seq2seq_task_test.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' text sequence to sequence task unittest \'\'\'\n\nimport os\nfrom pathlib import Path\nimport numpy as np\nimport delta.compat as tf\nfrom absl import logging\nfrom delta import utils\nfrom delta import PACKAGE_ROOT_DIR\nfrom delta.data.task.text_seq2seq_task import TextS2STask\nfrom delta.utils.register import import_all_modules_for_register\n\n\nclass TextS2STaskTest(tf.test.TestCase):\n  \'\'\' sequence to sequence task test\'\'\'\n\n  def setUp(self):\n    super().setUp()\n    import_all_modules_for_register()\n    package_root = Path(PACKAGE_ROOT_DIR)\n    self.config_file = package_root.joinpath(\n        \'../egs/mock_text_seq2seq_data/seq2seq/v1/config/transformer-s2s.yml\')\n\n  def tearDown(self):\n    \'\'\' tear down \'\'\'\n\n  def test_english(self):\n    """""" test seq to seq task of chiniese data, split sentences by space""""""\n\n    config = utils.load_config(self.config_file)\n    max_len = config[""model""][""net""][""structure""][""max_enc_len""]\n    data_config = config[""data""]\n    task_config = data_config[""task""]\n    task_config[""language""] = ""english""\n    task_config[""split_by_space""] = False\n    task_config[""use_word""] = True\n\n    # test offline data for \'train\'\n\n    task = TextS2STask(config, utils.TRAIN)\n    data = task.dataset()\n    self.assertTrue(""input_x_dict"" in data and\n                    ""input_enc_x"" in data[""input_x_dict""] and\n                    ""input_dec_x"" in data[""input_x_dict""])\n    self.assertTrue(""input_y_dict"" in data and\n                    ""input_y"" in data[""input_y_dict""])\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      sess.run(data[""iterator""].initializer)\n      res = sess.run([\n          data[""input_x_dict""][""input_enc_x""],\n          data[""input_x_dict""][""input_dec_x""], data[""input_y_dict""][""input_y""],\n          data[""input_x_len""]\n      ])\n\n      logging.debug(res[0][0])\n      logging.debug(res[1][0])\n      logging.debug(res[2][0])\n      logging.debug(res[3])\n\n      self.assertEqual(np.shape(res[0])[0], 16)\n      self.assertEqual(np.shape(res[1])[0], 16)\n      self.assertEqual(np.shape(res[2])[0], 16)\n      self.assertEqual(np.shape(res[3])[0], 16)\n\n    # test offline data for \'infer\'\n    task = TextS2STask(config, utils.INFER)\n    task.infer_without_label = True\n    data = task.dataset()\n    self.assertTrue(""input_x_dict"" in data and\n                    ""input_enc_x"" in data[""input_x_dict""])\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      sess.run(data[""iterator""].initializer)\n      res = sess.run([data[""input_x_dict""][""input_enc_x""], data[""input_x_len""]])\n\n      logging.debug(res[0][0])\n      logging.debug(res[1][0])\n\n      self.assertEqual(np.shape(res[0])[0], 16)\n      self.assertEqual(np.shape(res[1])[0], 16)\n\n    # test online data\n    export_inputs = task.export_inputs()\n    self.assertTrue(""export_inputs"" in export_inputs and\n                    ""input_sentence"" in export_inputs[""export_inputs""])\n    input_sentence = export_inputs[""export_inputs""][""input_sentence""]\n    input_x = export_inputs[""model_inputs""][""input_enc_x""]\n\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      sess.run(data[""iterator""].initializer)\n      res = sess.run(\n          input_x,\n          feed_dict={\n              input_sentence: [\n                  "" vice president walter ""\n                  ""mondale was released""\n              ]\n          })\n      logging.debug(res[0][:5])\n      logging.debug(np.shape(res[0]))\n      self.assertEqual(np.shape(res[0]), (max_len,))\n\n\nif __name__ == ""__main__"":\n  logging.set_verbosity(logging.DEBUG)\n  tf.test.main()\n'"
delta/data/task/text_seq_label_task.py,8,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' sequence labeling task \'\'\'\n\nimport collections\nimport delta.compat as tf\nfrom absl import logging\n\nfrom delta.data.task.base_text_task import TextTask\nfrom delta.data.preprocess.text_ops import process_multi_label_dataset\nfrom delta.data.preprocess.utils import get_vocab_size\nfrom delta.utils.register import registers\nfrom delta.layers.utils import compute_sen_lens\nfrom delta.data.preprocess.text_ops import load_textline_dataset\nfrom delta.data.utils.common_utils import get_file_len\n\n# pylint: disable=too-many-instance-attributes\n\n\n@registers.task.register\nclass TextSeqLabelTask(TextTask):\n  """"""Task class for sequence labeling.""""""\n\n  def __init__(self, config, mode):\n    super().__init__(config, mode)\n\n    self.vocab_min_frequency = self.task_config[\'vocab_min_frequency\']\n    self.text_vocab_file_path = self.task_config[\'text_vocab\']\n    self.label_vocab_file_path = self.task_config[\'label_vocab\']\n    self.max_seq_len = self.task_config[\'max_seq_len\']\n    self.num_classes = self.task_config[""classes""][\'num_classes\']\n\n    self.paths = self.data_config[mode][\'paths\']\n    self.paths_after_pre_process = [\n        one_path + "".after"" for one_path in self.paths\n    ]\n\n    self.prepare()\n\n  def generate_data(self):\n    """"""Generate data for offline training.""""""\n    paths = self.paths_after_pre_process\n    if self.infer_without_label:\n      self.column_num = 1\n      text_ds = load_textline_dataset(paths, self.column_num)\n    else:\n      self.column_num = 2\n      label_ds, text_ds = load_textline_dataset(paths, self.column_num)\n\n    logging.info(""process text ds..."")\n    input_pipeline_func = self.get_input_pipeline(for_export=False)\n    text_ds = text_ds.map(\n        input_pipeline_func, num_parallel_calls=self.num_parallel_calls)\n    text_size_ds = text_ds.map(\n        lambda x: compute_sen_lens(x, padding_token=0),\n        num_parallel_calls=self.num_parallel_calls)\n    text_ds = tf.data.Dataset.zip((text_ds, text_size_ds))\n\n    logging.info(""process label ds..."")\n    if self.infer_without_label:\n      data_set = text_ds\n    else:\n      label_ds = process_multi_label_dataset(label_ds, self.config)\n      data_set = tf.data.Dataset.zip((text_ds, label_ds))\n\n    self.config[\'data\'][\'vocab_size\'] = get_vocab_size(\n        self.text_vocab_file_path)\n    self.config[\'data\'][\'{}_data_size\'.format(self.mode)] = get_file_len(\n        self.paths)\n\n    return data_set\n\n  def feature_spec(self):\n    """"""Get shapes for feature.""""""\n    feature_shapes = [(tf.TensorShape([self.max_seq_len]), tf.TensorShape([]))]\n    if not self.infer_without_label:\n      feature_shapes.append(tf.TensorShape([self.max_seq_len]))\n    if len(feature_shapes) == 1:\n      return feature_shapes[0]\n    return tuple(feature_shapes)\n\n  def export_inputs(self):\n    """"""Inputs for exported model.""""""\n    self.config[\'data\'][\'vocab_size\'] = get_vocab_size(\n        self.text_vocab_file_path)\n    input_sentence = tf.placeholder(\n        shape=(None,), dtype=tf.string, name=""input_sentence"")\n\n    input_pipeline_func = self.get_input_pipeline(for_export=True)\n    token_ids = input_pipeline_func(input_sentence)\n    token_ids_len = tf.map_fn(lambda x: compute_sen_lens(x, padding_token=0),\n                              token_ids)\n\n    export_data = {\n        ""export_inputs"": {\n            ""input_sentence"": input_sentence\n        },\n        ""model_inputs"": {\n            ""input_x"": token_ids,\n            ""input_x_len"": token_ids_len\n        }\n    }\n\n    return export_data\n\n  def dataset(self):\n    """"""Dataset function""""""\n    data_set = self.generate_data()\n\n    if self.mode == \'train\':\n      if self.need_shuffle:\n        # shuffle batch size and repeat\n        logging.debug(""shuffle and repeat dataset ..."")\n        data_set = data_set.apply(\n            tf.data.experimental.shuffle_and_repeat(\n                buffer_size=self.shuffle_buffer_size, count=None))\n      else:\n        logging.debug(""repeat dataset ..."")\n        data_set = data_set.repeat(count=None)\n\n    feature_shape = self.feature_spec()\n    logging.debug(""feature_shape: {}"".format(feature_shape))\n\n    data_set = data_set.padded_batch(\n        batch_size=self.batch_size, padded_shapes=feature_shape)\n\n    data_set = data_set.prefetch(self.num_prefetch_batch)\n\n    iterator = data_set.make_initializable_iterator()\n\n    if self.infer_without_label:\n      input_x, input_x_len = iterator.get_next()\n    else:\n      (input_x, input_x_len), input_y = iterator.get_next()\n\n    input_x_dict = collections.OrderedDict([(""input_x"", input_x)])\n    return_dict = {\n        ""input_x_dict"": input_x_dict,\n        ""input_x_len"": input_x_len,\n        ""iterator"": iterator\n    }\n\n    if not self.infer_without_label:\n      return_dict[""input_y_dict""] = collections.OrderedDict([(""input_y"",\n                                                              input_y)])\n\n    return return_dict\n'"
delta/data/task/text_seq_label_task_test.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' text sequence labeling task unittest \'\'\'\n\nimport os\nfrom pathlib import Path\nimport numpy as np\nimport delta.compat as tf\nfrom absl import logging\nfrom delta import utils\nfrom delta.data.task.text_seq_label_task import TextSeqLabelTask\nfrom delta.utils.register import import_all_modules_for_register\nfrom delta import PACKAGE_ROOT_DIR\n\n\nclass TextSeqLabelTaskTest(tf.test.TestCase):\n  \'\'\' sequence labeling task test\'\'\'\n\n  def setUp(self):\n    super().setUp()\n    import_all_modules_for_register()\n    package_root = Path(PACKAGE_ROOT_DIR)\n    self.config_file = package_root.joinpath(\n        \'../egs/mock_text_seq_label_data/seq-label/v1/config/seq-label-mock.yml\'\n    )\n\n  def tearDown(self):\n    \'\'\' tear down \'\'\'\n\n  def test_english(self):\n    """""" test seq label task of english data """"""\n    config = utils.load_config(self.config_file)\n    max_len = config[""model""][""net""][""structure""][""max_len""]\n    config[""data""][""task""][""language""] = ""english""\n    task_config = config[""data""][""task""]\n    task_config[\n        ""text_vocab""] = ""egs/mock_text_seq_label_data/seq-label/v1/data/text_vocab.txt""\n    task_config[""need_shuffle""] = False\n\n    task = TextSeqLabelTask(config, utils.TRAIN)\n\n    # test offline data\n    data = task.dataset()\n    self.assertTrue(""input_x_dict"" in data and\n                    ""input_x"" in data[""input_x_dict""])\n    self.assertTrue(""input_y_dict"" in data and\n                    ""input_y"" in data[""input_y_dict""])\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      sess.run(data[""iterator""].initializer)\n      res = sess.run(\n          [data[""input_x_dict""][""input_x""], data[""input_y_dict""][""input_y""]])\n      logging.debug(res[0][0][:5])\n      logging.debug(res[1][0])\n      self.assertAllEqual(res[0][0][:5], [2, 3, 4, 5, 0])\n      self.assertEqual(np.shape(res[0]), (10, max_len))\n      self.assertEqual(np.shape(res[1]), (10, max_len))\n\n    # test online data\n    export_inputs = task.export_inputs()\n    self.assertTrue(""export_inputs"" in export_inputs and\n                    ""input_sentence"" in export_inputs[""export_inputs""])\n    input_sentence = export_inputs[""export_inputs""][""input_sentence""]\n    input_x = export_inputs[""model_inputs""][""input_x""]\n    with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n      sess.run(data[""iterator""].initializer)\n      res = sess.run(input_x, feed_dict={input_sentence: [""I feel good .""]})\n      logging.debug(res[0][:5])\n      self.assertAllEqual(res[0][:5], [0, 3, 4, 5, 0])\n      self.assertEqual(np.shape(res[0]), (max_len,))\n\n\nif __name__ == ""__main__"":\n  logging.set_verbosity(logging.DEBUG)\n  tf.test.main()\n'"
delta/data/utils/__init__.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Utilities for data related operations.""""""\nfrom delta.data.utils.common_utils import *\n'"
delta/data/utils/common_utils.py,1,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Common utilities for data related operations.""""""\n# pylint: disable=invalid-name\n\nimport re\nimport json\nimport numpy as np\nfrom absl import logging\nimport delta.compat as tf\nimport subprocess\n\nfrom delta import utils\n\n\ndef input_fn(dataset, mode, batch_size, num_epoch=None):\n  \'\'\'\n  params: dataset, tf.data.Dataset\n  params: mode, learning phase\n  params: batch size\n  params: num of epoch\n  \'\'\'\n  if mode == utils.TRAIN:\n    _, num_gpus = utils.gpu_device_names()\n    per_device_batch_size = utils.per_device_batch_size(batch_size, num_gpus)\n  else:\n    # using one device to eval or infer,\n    # otherwise will drop reminder samples, e.g. 32 batch with 3 gpus\n    per_device_batch_size = batch_size\n    num_epoch = 1\n\n  logging.info(\n      ""Learning Phase: {}, Total Batch size:{}, Per device batch size: {}""\n      .format(mode, batch_size, per_device_batch_size))\n\n  def _input_fn():\n    return dataset(mode, per_device_batch_size, num_epoch)\n\n  return _input_fn\n\n\nclass JsonNumpyEncoder(json.JSONEncoder):\n  """"""JSONEncoder warpper for numpy data.""""""\n\n  # pylint: disable=arguments-differ, method-hidden\n  def default(self, obj):\n    if isinstance(obj, np.integer):\n      return int(obj)\n    if isinstance(obj, np.floating):\n      return float(obj)\n    if isinstance(obj, np.ndarray):\n      return obj.tolist()\n    return super().default(obj)\n\n\ndef clean_english_str(string):\n  """"""\n  Tokenization/string cleaning for all datasets except for SST.\n  Original taken from\n  https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n  """"""\n  # pylint: disable=anomalous-backslash-in-string\n  string = re.sub(r""[^A-Za-z0-9(),!?\\\'\\`]"", "" "", string)\n  string = re.sub(r""\\\'s"", "" \\\'s"", string)\n  string = re.sub(r""\\\'ve"", "" \\\'ve"", string)\n  string = re.sub(r""n\\\'t"", "" n\\\'t"", string)\n  string = re.sub(r""\\\'re"", "" \\\'re"", string)\n  string = re.sub(r""\\\'d"", "" \\\'d"", string)\n  string = re.sub(r""\\\'ll"", "" \\\'ll"", string)\n  string = re.sub(r"","", "" , "", string)\n  string = re.sub(r""!"", "" ! "", string)\n  string = re.sub(r""\\("", "" \\( "", string)\n  string = re.sub(r""\\)"", "" \\) "", string)\n  string = re.sub(r""\\?"", "" \\? "", string)\n  string = re.sub(r""\\s{2,}"", "" "", string)\n  return string.strip().lower()\n\n\ndef save_a_text_cls_file(label, texts_after, new_path, no_label):\n  """"""Save a text classification data to a file.""""""\n  logging.info(""Saving processed file to: {}"".format(new_path))\n  with open(new_path, ""w"", encoding=""utf-8"") as out_f:\n    for i, one_line in enumerate(texts_after):\n      if no_label:\n        out_f.write(one_line + ""\\n"")\n      else:\n        out_f.write(label[i] + ""\\t"" + one_line + ""\\n"")\n\n\ndef save_a_text_match_file(label, texts_after, new_path, no_label):\n  """"""Save a text match data to a file.""""""\n  logging.info(""Saving processed file to: {}"".format(new_path))\n  with open(new_path, ""w"", encoding=""utf-8"") as out_f:\n    for i, (one_line_l, one_line_r) in enumerate(zip(*texts_after)):\n      if no_label:\n        out_f.write(one_line_l + \'\\t\' + one_line_r + ""\\n"")\n      else:\n        out_f.write(label[i] + ""\\t"" + one_line_l + \'\\t\' + one_line_r + ""\\n"")\n\n\ndef save_a_text_seq_label_file(label, texts_after, new_path, no_label):\n  """"""Save a text seqlabel data to a file.""""""\n  logging.info(""Saving processed file to: {}"".format(new_path))\n  with open(new_path, ""w"", encoding=""utf-8"") as out_f:\n    for i, one_line in enumerate(texts_after):\n      if no_label:\n        out_f.write(one_line + ""\\n"")\n      else:\n        out_f.write(label[i] + ""\\t"" + one_line + ""\\n"")\n\n\ndef save_a_text_seq2seq_file(texts_after, new_path):\n  """"""Save a text sequence data to a file""""""\n  logging.info(""Saving processed file to: {}"".format(new_path))\n  with open(new_path, ""w"", encoding=""utf-8"") as out_f:\n    for _, one_line in enumerate(texts_after):\n      out_f.write(one_line + ""\\n"")\n\n\ndef save_a_text_nlu_joint_file(label, texts_after, new_path, no_label):\n  """"""Save a text nlu joint data to a file.""""""\n  intent_label, slots_label = label\n  logging.info(""Saving processed file to: {}"".format(new_path))\n  with open(new_path, ""w"", encoding=""utf-8"") as out_f:\n    for i, one_line in enumerate(texts_after):\n      if no_label:\n        out_f.write(one_line + ""\\n"")\n      else:\n        out_f.write(intent_label[i] + ""\\t"" + slots_label[i] + ""\\t"" + one_line +\n                    ""\\n"")\n\n\ndef load_npy(npy_path, dtype=np.float32):\n  """"""Load a data in npy format.""""""\n  dense_feature = np.load(npy_path).astype(dtype)\n  return dense_feature\n\n\ndef get_file_len(fname_paths):\n  len_res = []\n  for fname in fname_paths:\n    p = subprocess.Popen([\'wc\', \'-l\', fname],\n                         stdout=subprocess.PIPE,\n                         stderr=subprocess.PIPE)\n    result, err = p.communicate()\n    if p.returncode != 0:\n      raise IOError(err)\n    len_res.append(int(result.strip().split()[0]))\n\n  return sum(len_res)\n\n\ndef read_lines_from_text_file(file_path):\n  """"""Read lines from a text file.""""""\n  with open(file_path) as f:\n    lines = [line.strip() for line in f.readlines()]\n    return lines\n'"
delta/data/utils/common_utils_test.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' common utils unittest\'\'\'\n\nimport os\nfrom pathlib import Path\nimport delta.compat as tf\nfrom delta import utils\nfrom delta import PACKAGE_ROOT_DIR\nfrom delta.data.utils.common_utils import get_file_len\n\n# pylint: disable=invalid-name,too-many-locals,missing-docstring\n\n\nclass CommonUtilsTest(tf.test.TestCase):\n\n  def setUp(self):\n    super().setUp()\n    package_root = Path(PACKAGE_ROOT_DIR)\n    self.config_file = package_root.joinpath(\n        \'../egs/mock_text_seq_label_data/seq-label/v1/config/seq-label-mock.yml\'\n    )\n    self.config = utils.load_config(self.config_file)\n\n  def tearDown(self):\n    \'\'\' tear down \'\'\'\n\n  def test_get_file_name(self):\n    paths = self.config[""data""][""train""][""paths""]\n    self.assertEqual(get_file_len(paths), 300)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
delta/data/utils/espnet_utils.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' espnet utils\'\'\'\nimport json\nfrom collections import OrderedDict\nimport numpy as np\nfrom absl import logging\n\nfrom espnet.utils.cli_readers import file_reader_helper\nfrom espnet.utils.cli_writers import file_writer_helper\nfrom espnet.utils.training.batchfy import make_batchset as make_batchset_espnet\nfrom espnet.utils.io_utils import LoadInputsAndTargets\n\nfrom delta import utils\n\nTASK_SET = {\'asr\': \'asr\', \'tts\': \'tts\'}\n\n\n#pylint: disable=too-many-locals\n#pylint: disable=too-many-arguments\ndef make_batchset(task,\n                  data,\n                  batch_size,\n                  max_length_in,\n                  max_length_out,\n                  num_batches=0,\n                  batch_sort_key=\'shuffle\',\n                  min_batch_size=1,\n                  shortest_first=False,\n                  batch_bins=0,\n                  batch_frames_in=0,\n                  batch_frames_out=0,\n                  batch_frames_inout=0,\n                  batch_strategy=\'auto\'):\n  """"""Make batch set from json dictionary\n\n  if utts have ""category"" value,\n\n     >>> data = {\'utt1\': {\'category\': \'A\', \'input\': ...},\n     ...         \'utt2\': {\'category\': \'B\', \'input\': ...},\n     ...         \'utt3\': {\'category\': \'B\', \'input\': ...},\n     ...         \'utt4\': {\'category\': \'A\', \'input\': ...}}\n     >>> make_batchset(data, batchsize=2, ...)\n     [[(\'utt1\', ...), (\'utt4\', ...)], [(\'utt2\', ...), (\'utt3\': ...)]]\n\n  Note that if any utts doesn\'t have ""category"",\n  perform as same as batchfy_by_{batch_strategy}\n\n  :param string task: task type, which in [asr, tts]\n  :param Dict[str, Dict[str, Any]] data: dictionary loaded from data.json\n  :param int batch_size: maximum number of sequences in a minibatch.\n  :param int batch_bins: maximum number of bins (frames x dim) in a minibatch.\n  :param int batch_frames_in:  maximum number of input frames in a minibatch.\n  :param int batch_frames_out: maximum number of output frames in a minibatch.\n  :param int batch_frames_out: maximum number of input+output frames in a minibatch.\n  :param str batch_strategy: strategy to count maximum size of batch [auto, seq, bin, frame].\n      ""auto"" : automatically detect make_batch strategy by finding enabled args\n      ""seq""  : create the minibatch that has the maximum number of seqs under batch_size.\n      ""bin""  : create the minibatch that has the maximum number of bins under batch_bins,\n               where the ""bin"" means the number of frames x dim.\n      ""frame"": create the minibatch that has the maximum number of input, output and input+output\n               frames under batch_frames_in, batch_frames_out and batch_frames_inout, respectively\n\n  :param int max_length_in: maximum length of input to decide adaptive batch size\n  :param int max_length_out: maximum length of output to decide adaptive batch size\n  :param int num_batches: # number of batches to use (for debug)\n  :param int min_batch_size: minimum batch size (for multi-gpu)\n  :param bool shortest_first: Sort from batch with shortest samples to longest if true,\n                              otherwise reverse\n  :param str batch_sort_key: how to sort data before creating minibatches [input, output, shuffle]\n      :return: List[List[Tuple[str, dict]]] list of batches\n\n  Reference: https://github.com/espnet/espnet/pull/759/files\n             https://github.com/espnet/espnet/commit/dc0a0d3cfc271af945804f391e81cd5824b08725\n             https://github.com/espnet/espnet/commit/73018318a65d18cf2e644a45aa725323c9e4a0e6\n  """"""\n\n  assert task in list(TASK_SET.keys())\n\n  #swap_io: if True, use ""input"" as output and ""output"" as input in `data` dict\n  swap_io = False\n  if task == TASK_SET[\'tts\']:\n    swap_io = True\n\n  minibatches = make_batchset_espnet(\n      data,\n      batch_size=batch_size,\n      max_length_in=max_length_in,\n      max_length_out=max_length_out,\n      num_batches=num_batches,\n      min_batch_size=min_batch_size,\n      shortest_first=shortest_first,\n      batch_sort_key=batch_sort_key,\n      swap_io=swap_io,\n      count=batch_strategy,\n      batch_bins=batch_bins,\n      batch_frames_in=batch_frames_in,\n      batch_frames_out=batch_frames_out,\n      batch_frames_inout=batch_frames_inout)\n  return minibatches\n\n\n#pylint: disable=too-many-locals\ndef get_batches(config, mode):\n  \'\'\' make batches of metas and get dataset size\'\'\'\n  assert mode in (utils.TRAIN, utils.EVAL, utils.INFER)\n\n  # read meta of json\n  json_path = config[\'data\'][mode][\'paths\']\n  assert len(json_path) == 1\n  logging.info(f""=== load json data {json_path} ==="")\n  logging.info(f"" # learning phase: {mode}"")\n\n  #pylint: disable=invalid-name\n  with open(json_path[0], \'r\', encoding=\'utf-8\') as f:\n    metas_raw = json.load(f)[\'utts\']\n\n  # sort by utts id\n  metas = OrderedDict(sorted(metas_raw.items(), key=lambda t: t[0]))\n\n  # dataset size\n  utts = len(metas.keys())\n  logging.info(\' # utts: \' + str(utts))\n\n  # make batchset\n  use_sortagrad = config[\'data\'][\'task\'][\'sortagrad\']\n  logging.info(f\' # sortagrad: {use_sortagrad}\')\n\n  task = config[\'data\'][\'task\'][\'type\']\n  assert task in list(TASK_SET.keys())\n  # using same json for asr and tts task\n  if task == TASK_SET[\'asr\']:\n    src = \'src\'\n    tgt = \'tgt\'\n  elif task == TASK_SET[\'tts\']:\n    src = \'tgt\'\n    tgt = \'src\'\n  else:\n    raise ValueError(""task type must int : {} get : {}"".format(\n        list(TASK_SET.keys()), task))\n  logging.info(f"" # task: {task}"")\n\n  # delta config\n  maxlen_src = config[\'data\'][\'task\'][src][\'max_len\']\n  maxlen_tgt = config[\'data\'][\'task\'][tgt][\'max_len\']\n  batch_sort_key = config[\'data\'][\'task\'][\'batch_sort_key\']\n  num_batches = config[\'data\'][\'task\'][\'num_batches\']  # for debug\n  global_batch_size = config[\'solver\'][\'optimizer\'][\'batch_size\']\n  batch_bins = config[\'data\'][\'task\'][\'batch\'][\'batch_bins\']\n  batch_frames_in = config[\'data\'][\'task\'][\'batch\'][\'batch_frames_in\']\n  batch_frames_out = config[\'data\'][\'task\'][\'batch\'][\'batch_frames_out\']\n  batch_frames_inout = config[\'data\'][\'task\'][\'batch\'][\'batch_frames_inout\']\n  batch_strategy = config[\'data\'][\'task\'][\'batch\'][\'batch_strategy\']\n\n  _, ngpu = utils.gpu_device_names()\n  min_batch_size = ngpu if ngpu else 1\n  logging.info(f"" # ngpu: {ngpu}"")\n  logging.info(f"" # min_batch_size: {min_batch_size}"")\n\n  minibatches = make_batchset(\n      task=task,\n      data=metas,\n      batch_size=global_batch_size,\n      max_length_in=maxlen_src,\n      max_length_out=maxlen_tgt,\n      num_batches=num_batches,\n      batch_sort_key=batch_sort_key,\n      min_batch_size=min_batch_size,\n      shortest_first=use_sortagrad,\n      batch_bins=batch_bins,\n      batch_frames_in=batch_frames_in,\n      batch_frames_out=batch_frames_out,\n      batch_frames_inout=batch_frames_inout,\n      batch_strategy=batch_strategy)\n\n  return {\'data\': minibatches, \'n_utts\': utts}\n\n\nclass Converter:\n  \'\'\'custom batch converter for kaldi\n  :param int subsampling_factor: The subsampling factor\n  :param object preprocess_conf: The preprocessing config\n  \'\'\'\n\n  def __init__(self, config):\n    self._config = config\n    self.subsampling_factor = None\n    self.preprocess_conf = None\n    self.load_inputs_and_targets = lambda x: x\n\n  @property\n  def config(self):\n    \'\'\' candy _config\'\'\'\n    return self._config\n\n  def transform(self, item):\n    \'\'\' load inputs and outputs \'\'\'\n    return self.load_inputs_and_targets(item)\n\n  def __call__(self, batch):\n    \'\'\' Transforms a batch\n    param: batch, list of (uttid, {\'input\': [{...}], \'output\': [{...}]})\n    \'\'\'\n    # batch should be located in list\n    # list of examples\n    (xs, ys), uttid_list = self.transform(batch)  #pylint: disable=invalid-name\n\n    # perform subsampling\n    if self.subsampling_factor > 1:\n      xs = [x[::self.subsampling_factor, :] for x in xs]  #pylint: disable=invalid-name\n\n    # get batch of lengths of input and output sequences\n    ilens = [x.shape[0] for x in xs]\n    olens = [y.shape[0] for y in ys]\n\n    return xs, ilens, ys, olens, uttid_list\n\n\nclass ASRConverter(Converter):\n  \'\'\' ASR preprocess \'\'\'\n\n  def __init__(self, config):\n    super().__init__(config)\n    taskconf = self.config[\'data\'][\'task\']\n    assert taskconf[\'type\'] == TASK_SET[\'asr\']\n    self.subsampling_factor = taskconf[\'src\'][\'subsampling_factor\']\n    self.preprocess_conf = taskconf[\'src\'][\'preprocess_conf\']\n    # mode: asr or tts\n    self.load_inputs_and_targets = LoadInputsAndTargets(\n        mode=taskconf[\'type\'],\n        load_output=True,\n        preprocess_conf=self.preprocess_conf)\n\n  #pylint: disable=arguments-differ\n  #pylint: disable=too-many-branches\n  def transform(self, batch):\n    """"""Function to load inputs, targets and uttid from list of dicts\n\n    :param List[Tuple[str, dict]] batch: list of dict which is subset of\n        loaded data.json\n    :return: list of input token id sequences [(L_1), (L_2), ..., (L_B)]\n    :return: list of input feature sequences\n        [(T_1, D), (T_2, D), ..., (T_B, D)]\n    :rtype: list of float ndarray\n    :return: list of target token id sequences [(L_1), (L_2), ..., (L_B)]\n    :rtype: list of int ndarray\n    Reference: Espnet source code, /espnet/utils/io_utils.py\n               https://github.com/espnet/espnet/blob/master/espnet/utils/io_utils.py\n    """"""\n    x_feats_dict = OrderedDict()  # OrderedDict[str, List[np.ndarray]]\n    y_feats_dict = OrderedDict()  # OrderedDict[str, List[np.ndarray]]\n    uttid_list = []  # List[str]\n\n    mode = self.load_inputs_and_targets.mode\n    for uttid, info in batch:\n      uttid_list.append(uttid)\n\n      if self.load_inputs_and_targets.load_input:\n        # Note(kamo): This for-loop is for multiple inputs\n        for idx, inp in enumerate(info[\'input\']):\n          # {""input"":\n          #  [{""feat"": ""some/path.h5:F01_050C0101_PED_REAL"",\n          #    ""filetype"": ""hdf5"",\n          #    ""name"": ""input1"", ...}], ...}\n\n          #pylint: disable=protected-access\n          x_data = self.load_inputs_and_targets._get_from_loader(\n              filepath=inp[\'feat\'], filetype=inp.get(\'filetype\', \'mat\'))\n          x_feats_dict.setdefault(inp[\'name\'], []).append(x_data)\n\n      elif mode == \'tts\' and self.load_inputs_and_targets.use_speaker_embedding:\n        for idx, inp in enumerate(info[\'input\']):\n          if idx != 1 and len(info[\'input\']) > 1:\n            x_data = None\n          else:\n            x_data = self.load_inputs_and_targets._get_from_loader(  #pylint: disable=protected-access\n                filepath=inp[\'feat\'],\n                filetype=inp.get(\'filetype\', \'mat\'))\n          x_feats_dict.setdefault(inp[\'name\'], []).append(x_data)\n\n      if self.load_inputs_and_targets.load_output:\n        for idx, inp in enumerate(info[\'output\']):\n          if \'tokenid\' in inp:\n            # ======= Legacy format for output =======\n            # {""output"": [{""tokenid"": ""1 2 3 4""}])\n            x_data = np.fromiter(\n                map(int, inp[\'tokenid\'].split()), dtype=np.int64)\n          else:\n            # ======= New format =======\n            # {""input"":\n            #  [{""feat"": ""some/path.h5:F01_050C0101_PED_REAL"",\n            #    ""filetype"": ""hdf5"",\n            #    ""name"": ""target1"", ...}], ...}\n            x_data = self.load_inputs_and_targets._get_from_loader(  #pylint: disable=protected-access\n                filepath=inp[\'feat\'],\n                filetype=inp.get(\'filetype\', \'mat\'))\n\n          y_feats_dict.setdefault(inp[\'name\'], []).append(x_data)\n    if self.load_inputs_and_targets.mode == \'asr\':\n      #pylint: disable=protected-access\n      return_batch, uttid_list = self.load_inputs_and_targets._create_batch_asr(\n          x_feats_dict, y_feats_dict, uttid_list)\n\n    elif self.load_inputs_and_targets.mode == \'tts\':\n      _, info = batch[0]\n      eos = int(info[\'output\'][0][\'shape\'][1]) - 1\n      #pylint: disable=protected-access\n      return_batch, uttid_list = self.load_inputs_and_targets._create_batch_tts(\n          x_feats_dict, y_feats_dict, uttid_list, eos)\n    else:\n      raise NotImplementedError\n\n    if self.load_inputs_and_targets.preprocessing is not None:\n      # Apply pre-processing only to input1 feature, now\n      if \'input1\' in return_batch:\n        return_batch[\'input1\'] = \\\n            self.load_inputs_and_targets.preprocessing(return_batch[\'input1\'], uttid_list,\n                               **self.load_inputs_and_targets.preprocess_args)\n\n    # Doesn\'t return the names now.\n    return tuple(return_batch.values()), uttid_list\n'"
delta/data/utils/espnet_utils_test.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' espnet utils unittest\'\'\'\nimport tempfile\nfrom pathlib import Path\n\nimport numpy as np\nimport delta.compat as tf\nimport h5py\nimport kaldiio\nfrom absl import logging\n\n# espnet\nfrom espnet.utils.io_utils import LoadInputsAndTargets\nfrom espnet.utils.io_utils import SoundHDF5File\n# test is package of espnet\nfrom test.utils_test import make_dummy_json  #pylint: disable=wrong-import-order\n\n# delta\nfrom delta import utils\nfrom delta.data.utils import espnet_utils\nfrom delta.data.utils.test_utils import generate_json_data\n\n#pylint: disable=invalid-name,too-many-locals,missing-docstring\n\n\nclass KaldiJsonReaderTest(tf.test.TestCase):\n\n  def setUp(self):\n    super().setUp()\n    self.conf_str = \'\'\'\n    data:\n      train:\n        paths: null\n        segments: null\n      eval:\n        paths: null\n        segments: null\n      infer:\n        paths: null\n        segments: null\n      task:\n        dummy: true # dummy inputs \n        name: AsrSeqTask\n        type: asr # asr, tts\n        audio:\n          dry_run: false # not save feat\n        src:\n          max_len: 3000 # max length for frames\n          subsampling_factor: 1\n          preprocess_conf: null\n        tgt:\n          max_len: 100 # max length for target tokens\n        vocab:\n          type: char # char, bpe, wpm, word\n          size: 3653 # vocab size in vocab_file\n          path: \'/nfs/cold_project/dataset/opensource/librispeech/espnet/egs/hkust/asr1/data/lang_1char/train_nodup_sp_units.txt\' # path to vocab(default: \'vocab\n        batch:\n          batch_size: 32 # number of elements in a training batch\n          batch_bins: 0 # maximum number of bins (frames x dim) in a trainin batch\n          batch_frames_in: 0 # maximum number of input frames in a training batch\n          batch_frames_out: 0 # maximum number of output frames in a training batch\n          batch_frames_inout: 0 # maximum number of input+output frames in a training batch\n          batch_strategy: auto # strategy to count maximum size of batch(support 4 values: ""auto"", ""seq"", ""frame"", ""bin"")\n        batch_mode: false # ture, user control batch; false, `generate` will yeild one example \n        num_parallel_calls: 12\n        num_prefetch_batch: 2\n        shuffle_buffer_size: 200000\n        need_shuffle: true\n        sortagrad: true\n        batch_sort_key: \'input\' # shuffle, input, output for asr and tts, and sortagrad for asr\n        num_batches: 0 # for debugging\n\n    model:\n      name: CTCAsrModel\n      type: keras # raw, keras or eager model\n      net:\n        structure:\n          encoder:\n            name:\n            filters: # equal number of cnn layers\n            - 128\n            - 512\n            - 512\n            filter_size: # equal number of cnn layers\n            - [5, 3]\n            - [5, 3]\n            - [5, 3]\n            filter_stride: # equal number of cnn layers\n            - [1, 1]\n            - [1, 1]\n            - [1, 1]\n            pool_size: # equal number of cnn layers\n            - [4, 4]\n            - [1, 2]\n            - [1, 2]\n            num_filters: 128\n            linear_num: 786 # hidden number of linear layer\n            cell_num: 128 # cell units of the lstm\n            hidden1: 64 # number of hidden units of fully connected layer\n            attention: false # whether to use attention, false mean use max-pooling\n            attention_size: 128 # attention_size\n            use_lstm_layer: false # whether to use lstm layer, false mean no lstm layer\n            use_dropout: true # whether to use bn, dropout layer\n            dropout_rate: 0.2\n            use_bn: true # whether to use bn, dropout layer\n          decoder:\n            name: \n          attention:\n            name:\n    solver:\n      name: AsrSolver\n      adversarial:\n        enable: false # whether to using adversiral training\n        adv_alpha: 0.5 # adviseral alpha of loss\n        adv_epslion: 0.1 # adviseral example epslion\n      model_average:\n        enable: false # use average model\n        var_avg_decay: 0.99 # the decay rate of varaibles\n      distilling:\n        enable: false \n        name : Teacher\n        loss : DistillationLoss\n        temperature: 5\n        alpha: 0.5\n        teacher_model: null # fronzen_graph.pb \n      optimizer:\n        name: adam\n        epochs: 5 # maximum epochs\n        loss: CTCLoss \n        label_smoothing: 0.0 # label smoothing rate\n        learning_rate:\n          rate: 0.0001 # learning rate of Adam optimizer\n          type:  exp_decay # learning rate type\n          decay_rate: 0.99  # the lr decay rate\n          decay_steps: 100  # the lr decay_step for optimizer\n        clip_global_norm: 3.0 # clip global norm\n        early_stopping: # keras early stopping\n          enable: true\n          monitor: val_loss\n          min_delta: 0\n          patience: 5\n      metrics:\n        pos_label: 1 # int, same to sklearn\n        cals:\n        - name: AccuracyCal\n          arguments: null \n        - name: ConfusionMatrixCal\n          arguments: null\n        - name: PrecisionCal\n          arguments:\n            average: \'binary\'\n        - name: RecallCal\n          arguments:\n            average: \'binary\'\n        - name: F1ScoreCal\n          arguments:\n            average: \'binary\'\n      postproc:\n          enbale: false\n          name: EmoPostProc\n          log_verbose: false \n          eval: true # compute metrics\n          infer: true  # get predict results\n          pred_path: null # None for `model_path`/infer, dumps infer output to this dir\n          thresholds:\n              - 0.5\n          smoothing:\n              enable: true\n              count: 2\n      saver:\n        model_path: ""ckpt/asr-seq/test""\n        max_to_keep: 10\n        save_checkpoints_steps: 100\n        keep_checkpoint_every_n_hours: 10000\n        checkpoint_every: 100 # the step to save checkpoint\n        summary: false\n        save_summary_steps: 100\n        eval_on_dev_every_secs: 1\n        print_every: 10\n        resume_model_path: """"\n      run_config:\n        debug: false # use tfdbug\n        tf_random_seed: null # 0-2**32; null is None, try to read data from /dev/urandom if available or seed from the clock otherwise\n        allow_soft_placement: true\n        log_device_placement: false\n        intra_op_parallelism_threads: 10\n        inter_op_parallelism_threads: 10\n        allow_growth: true\n        log_step_count_steps: 100 #The frequency, in number of global steps, that the global step/sec and the loss will be logged during training.\n      run_options:\n        trace_level: 3 # 0: no trace, 1: sotware trace, 2: hardware_trace, 3: full trace\n        inter_op_thread_pool: -1\n        report_tensor_allocations_upon_oom: true\n    \n    serving:\n      enable: false \n      name : Evaluate\n      model: null # saved model dir, ckpt dir, or frozen_model.pb\n      inputs: \'inputs:0\'\n      outpus: \'softmax_output:0\'   \n    \'\'\'\n\n    tempdir = self.get_temp_dir()\n\n    config_path = str(Path(tempdir).joinpath(""asr_seq.yaml""))\n    logging.info(""config path: {}"".format(config_path))\n    with open(config_path, \'w\', encoding=\'utf-8\') as f:  #pylint: disable=invalid-name\n      f.write(self.conf_str)\n\n    self.config = utils.load_config(config_path)\n\n  def tearDown(self):\n    \'\'\' tear down \'\'\'\n\n  def test_make_batchset(self):\n    dummy_json = make_dummy_json(128, [128, 512], [16, 128])\n    for task in espnet_utils.TASK_SET:\n      # check w/o adaptive batch size\n      batchset = espnet_utils.make_batchset(\n          task,\n          data=dummy_json,\n          batch_size=24,\n          max_length_in=2**10,\n          max_length_out=2**10,\n          min_batch_size=1)\n      self.assertEqual(\n          sum([len(batch) >= 1 for batch in batchset]), len(batchset))\n      logging.info(\'batch: {}\'.format(([len(batch) for batch in batchset])))\n\n      batchset = espnet_utils.make_batchset(\n          task, dummy_json, 24, 2**10, 2**10, min_batch_size=10)\n      self.assertEqual(\n          sum([len(batch) >= 10 for batch in batchset]), len(batchset))\n      logging.info(\'batch: {}\'.format(([len(batch) for batch in batchset])))\n\n      # check w/ adaptive batch size\n      batchset = espnet_utils.make_batchset(\n          task, dummy_json, 24, 256, 64, min_batch_size=10)\n      self.assertEqual(\n          sum([len(batch) >= 10 for batch in batchset]), len(batchset))\n      logging.info(\'batch: {}\'.format(([len(batch) for batch in batchset])))\n\n      batchset = espnet_utils.make_batchset(\n          task, dummy_json, 24, 256, 64, min_batch_size=10)\n      self.assertEqual(\n          sum([len(batch) >= 10 for batch in batchset]), len(batchset))\n\n  def test_sortagrad(self):\n    dummy_json = make_dummy_json(128, [1, 700], [1, 700])\n\n    for task in espnet_utils.TASK_SET:\n      if task == \'tts\':\n        batchset = espnet_utils.make_batchset(\n            task,\n            dummy_json,\n            16,\n            2**10,\n            2**10,\n            batch_sort_key=""input"",\n            shortest_first=True)\n        key = \'output\'\n      elif task == \'asr\':\n        batchset = espnet_utils.make_batchset(\n            task,\n            dummy_json,\n            16,\n            2**10,\n            2**10,\n            batch_sort_key=\'input\',\n            shortest_first=True)\n        key = \'input\'\n\n      prev_start_ilen = batchset[0][0][1][key][0][\'shape\'][0]\n      for batch in batchset:\n        # short to long\n        cur_start_ilen = batch[0][1][key][0][\'shape\'][0]\n        self.assertGreaterEqual(cur_start_ilen, prev_start_ilen)\n\n        prev_ilen = cur_start_ilen\n        for sample in batch:\n          cur_ilen = sample[1][key][0][\'shape\'][0]\n          # long to short in minibatch\n          self.assertLessEqual(cur_ilen, prev_ilen)\n          prev_ilen = cur_ilen\n        prev_start_ilen = cur_start_ilen\n\n  def test_load_inputs_and_targets_legacy_format(self):\n    # batch = [(""F01_050C0101_PED_REAL"",\n    #          {""input"": [{""feat"": ""some/path.ark:123""}],\n    #           ""output"": [{""tokenid"": ""1 2 3 4""}],\n\n    tmpdir = Path(tempfile.mkdtemp())\n    ark = str(tmpdir.joinpath(\'test.ark\'))\n    scp = str(tmpdir.joinpath(\'test.scp\'))\n\n    desire_xs = []\n    desire_ys = []\n    with kaldiio.WriteHelper(\'ark,scp:{},{}\'.format(ark, scp)) as f:\n      for i in range(10):\n        x = np.random.random((100, 100)).astype(np.float32)\n        uttid = \'uttid{}\'.format(i)\n        f[uttid] = x\n        desire_xs.append(x)\n        desire_ys.append(np.array([1, 2, 3, 4]))\n\n    batch = []\n    with open(scp, \'r\') as f:\n      for line in f:\n        uttid, path = line.strip().split()\n        batch.append((uttid, {\n            \'input\': [{\n                \'feat\': path,\n                \'name\': \'input1\'\n            }],\n            \'output\': [{\n                \'tokenid\': \'1 2 3 4\',\n                \'name\': \'target1\'\n            }]\n        }))\n\n    logging.info(""kaldi meta: {}"".format(batch[0]))\n    load_inputs_and_targets = LoadInputsAndTargets()\n    xs, ys = load_inputs_and_targets(batch)\n    for x, xd in zip(xs, desire_xs):\n      self.assertAllEqual(x, xd)\n    for y, yd in zip(ys, desire_ys):\n      self.assertAllEqual(y, yd)\n\n  def test_load_inputs_and_targets_new_format(self):\n    # batch = [(""F01_050C0101_PED_REAL"",\n    #           {""input"": [{""feat"": ""some/path.h5"",\n    #                       ""filetype"": ""hdf5""}],\n    #           ""output"": [{""tokenid"": ""1 2 3 4""}],\n\n    tmpdir = tempfile.mkdtemp()\n    p = Path(tmpdir).joinpath(\'test.h5\')\n\n    desire_xs = []\n    desire_ys = []\n    batch = []\n    with h5py.File(str(p), \'w\') as f:\n      # batch: List[Tuple[str, Dict[str, List[Dict[str, Any]]]]]\n      for i in range(10):\n        x = np.random.random((100, 100)).astype(np.float32)\n        uttid = \'uttid{}\'.format(i)\n        f[uttid] = x\n        batch.append((uttid, {\n            \'input\': [{\n                \'feat\': str(p) + \':\' + uttid,\n                \'filetype\': \'hdf5\',\n                \'name\': \'input1\'\n            }],\n            \'output\': [{\n                \'tokenid\': \'1 2 3 4\',\n                \'name\': \'target1\'\n            }]\n        }))\n        desire_xs.append(x)\n        desire_ys.append(np.array([1, 2, 3, 4]))\n\n    logging.info(""h5py meta: {}"".format(batch[0]))\n    load_inputs_and_targets = LoadInputsAndTargets()\n    xs, ys = load_inputs_and_targets(batch)\n    for x, xd in zip(xs, desire_xs):\n      self.assertAllEqual(x, xd)\n    for y, yd in zip(ys, desire_ys):\n      self.assertAllEqual(y, yd)\n\n  def test_sound_hdf5_file(self):\n    tmpdir = Path(tempfile.mkdtemp())\n    for fmt in [\'flac\', \'wav\']:\n      valid = {\n          \'a\': np.random.randint(-100, 100, 25, dtype=np.int16),\n          \'b\': np.random.randint(-1000, 1000, 100, dtype=np.int16)\n      }\n\n      # Note: Specify the file format by extension\n      p = tmpdir.joinpath(\'test.{}.h5\'.format(fmt))\n      p.touch(exist_ok=True)\n      p = str(p.resolve())\n      f = SoundHDF5File(p, \'a\')\n\n      for k, v in valid.items():\n        f[k] = (v, 8000)\n\n      for k, v in valid.items():\n        t, r = f[k]\n        self.assertEqual(r, 8000)\n        self.assertAllEqual(t, v)\n\n  def test_converter(self):\n    np.random.seed(100)\n\n    nexamples_list = (10, 12)\n    batch_size = 4\n    self.config[\'solver\'][\'optimizer\'][\'batch_size\'] = batch_size\n    converter = espnet_utils.ASRConverter(self.config)\n\n    for mode in (utils.TRAIN, utils.EVAL, utils.INFER):\n      for nexamples in nexamples_list:\n        desire_xs, desire_ilens, desire_ys, desire_olens = generate_json_data(\n            self.config, mode, nexamples)\n        del desire_xs, desire_ilens, desire_ys, desire_olens\n        data_metas = espnet_utils.get_batches(self.config, mode)\n\n        batches = data_metas[\'data\']\n        n_utts = data_metas[\'n_utts\']\n        self.assertEqual(n_utts, nexamples)\n\n        o_uttids = []\n        o_xs = []\n        o_ilens = []\n        o_ys = []\n        o_olens = []\n        for _, batch in enumerate(batches):\n          batch_data = converter(batch)\n          self.assertEqual(len(batch_data), 5)\n\n          xs, ilens, ys, olens, uttids = batch_data\n          for x, ilen, y, olen, uttid in zip(xs, ilens, ys, olens, uttids):\n            self.assertDTypeEqual(x, np.float32)\n            self.assertDTypeEqual(y, np.int64)\n            o_uttids.append(uttid)\n            o_xs.append(x)\n            o_ilens.append(ilen)\n            o_ys.append(y)\n            o_olens.append(olen)\n\n        self.assertEqual(len(o_xs), nexamples)\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
delta/data/utils/htk_reader_lib.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' HTK Reader\'\'\'\nimport os\nimport math\nimport numpy as np\n\n\nclass HtkReaderIO:\n  """"""\n    read the kaldi ark format file\n    """"""\n\n  def __init__(self):\n    self._fp = None\n    self._open_file_name = """"\n\n    self._delta_normalizer = 0\n    self._delta_window = 1\n\n    self._calutlate_mean_variance_start = True\n    self._meam_variance_list = []\n    self._mean_variance_statu = False\n    self._mean_array = None\n    self._variance_array = None\n\n  def _compute_delta_normalizer(self, delta_window=2):\n    """"""\n        compute the delta normalizer\n        args:\n            delta_window: the detla window\n        return:\n            -1: failed\n            0: success\n        """"""\n    self._delta_normalizer = 0\n    self._delta_window = delta_window\n\n    self._delta_window_array = np.array(\n        range(-1 * self._delta_window, self._delta_window + 1))\n    self._delta_normalizer = np.sum(np.square(self._delta_window_array))\n    self._delta_normalizer = 1.0 / self._delta_normalizer\n    self._delta_window_array = self._delta_window_array * self._delta_normalizer\n\n    # base method\n    #for tmp_window in range(-1*delta_window, delta_window+1):\n    #    self._delta_normalizer += tmp_window * tmp_window\n\n    #self._delta_normalizer = 1.0 / self._delta_normalizer\n    return 0\n\n  def add_delta(self, feat_array, delta_order=2, delta_window=2):\n    """"""\n        add delta to feat\n        args:\n            feat_array: input feat which is from one sentence\n            delta_order: the order of delta\n            delta_window: the window of delta\n        return:\n            -1, array: failed\n            0, feat_array: success\n        """"""\n    if self._delta_normalizer == 0 or self._delta_window != delta_window:\n      self._compute_delta_normalizer(delta_window)\n\n    frame_num, feat_dim = feat_array.shape\n    tmp_output_delta_feat = [feat_array]\n    for _ in range(1, delta_order + 1):\n      tmp_delta_feat = np.zeros([frame_num, feat_dim])\n      for tmp_frame_num in range(frame_num):\n        tmp_action_frame_begin = tmp_frame_num - delta_window\n        tmp_action_frame_end = tmp_frame_num + delta_window\n        if tmp_action_frame_begin < 0 or tmp_action_frame_end >= frame_num:\n          for tmp_detal_window in range(-1 * delta_window, delta_window + 1):\n            tmp_action_frame = tmp_frame_num + tmp_detal_window\n            if tmp_action_frame < 0:\n              tmp_action_frame = 0\n              fill_status = True\n            if tmp_action_frame >= frame_num:\n              tmp_action_frame = frame_num - 1\n              fill_status = True\n\n            tmp_delta_feat[tmp_frame_num] += (\n                tmp_detal_window * tmp_output_delta_feat[-1][tmp_action_frame])\n\n          tmp_delta_feat[tmp_frame_num] = (\n              tmp_delta_feat[tmp_frame_num] * self._delta_normalizer)\n        else:\n          tmp_delta_feat[tmp_frame_num] = (\n              np.sum(\n                  (tmp_output_delta_feat[-1]\n                   [tmp_action_frame_begin:tmp_action_frame_end + 1].T *\n                   self._delta_window_array).T,\n                  axis=0))\n      tmp_output_delta_feat.append(tmp_delta_feat)\n    output_delta_feat = np.hstack(tmp_output_delta_feat)\n    return 0, output_delta_feat\n\n  def calculate_mean_variance(self, feat_list, is_end=False):\n    """"""\n        calculate the mean and variance of the feat\n        args:\n            feat_list: feat list such as [[key, array]...]\n            is_end: if is the last feat_list, set it to True,\n                    else set it to False\n        return:\n            -1, []: failed\n            0, []: when is_end is True, return the mean and variance list\n                   else return empty list\n        """"""\n    if self._calutlate_mean_variance_start == True:\n      self._meam_variance_list = [\n          0,\n          np.zeros([feat_list[0][1].shape[1]]),\n          np.zeros([feat_list[0][1].shape[1]])\n      ]\n      self._calutlate_mean_variance_start = False\n\n    for tmp_feat_list in feat_list:\n      self._meam_variance_list[0] += tmp_feat_list[1].shape[0]\n      self._meam_variance_list[1] += np.sum(tmp_feat_list[1], axis=0)\n      self._meam_variance_list[2] += (\n          np.sum(np.multiply(tmp_feat_list[1], tmp_feat_list[1]), axis=0))\n\n    if is_end:\n      self._calutlate_mean_variance_start = True\n      final_mean = self._meam_variance_list[1] / self._meam_variance_list[0]\n      final_variance = (\n          self._meam_variance_list[2] / self._meam_variance_list[0] -\n          np.multiply(final_mean, final_mean))\n      output_list = [[""mean"", final_mean], [""variance"", final_variance]]\n      return 0, output_list\n    return 0, []\n\n  def _read_mean_variance(self, mean_variance_file, dim_number):\n    """"""\n        read mean variance file\n        args:\n            mean_variance_file: the input mean variance file\n            dim_number: gen dim_number array\n        return:\n            -1: failed\n            0: success\n        """"""\n    if not os.path.exists(mean_variance_file):\n      return -1\n\n    with open(mean_variance_file) as fp:\n      input_list = fp.readlines()\n      input_dim_length = len(input_list)\n      if dim_number > input_dim_length:\n        return -1\n\n      self._mean_array = np.zeros([dim_number])\n      self._variance_array = np.zeros([dim_number])\n      for input_line_number in range(0, dim_number):\n        tmp_mean, tmp_variance = input_list[input_line_number].strip().split()\n        self._mean_array[input_line_number] = float(tmp_mean)\n        self._variance_array[input_line_number] = math.sqrt(float(tmp_variance))\n    self._mean_variance_statu = True\n    return 0\n\n  def normalization_feat_by_mean_variance(self, feat_array, mean_variance_file):\n    """"""\n        normalization feat using mean variance\n        args:\n            feat_array: input feat array\n            mean_variance_file: input mean variance file\n        return:\n            -1, array: failed\n            0, array: success and return the new feat array\n        """"""\n    if not self._mean_variance_statu:\n      if self._read_mean_variance(mean_variance_file, feat_array.shape[1]) < 0:\n        print(""[ERROR] _read_mean_variance failed with shape %s %s"" %\n              (feat_array.shape[0], feat_array.shape[1]))\n        return -1, np.array([])\n\n    mean_array = np.array([self._mean_array] * feat_array.shape[0])\n    variance_array = np.array([self._variance_array] * feat_array.shape[0])\n\n    new_feat_array = (feat_array - mean_array) / variance_array\n    return 0, new_feat_array\n\n  def splice_frames(self, feat_array, left_context, right_context):\n    """"""\n        splice the feature\n        args:\n            feat_array: the input feature\n            left_context: int type and must >= 0\n            right_context: int type and must >= 0\n        return:\n            -1, array: failed\n            0, array: success\n        """"""\n    if left_context < 0 or right_context < 0:\n      return -1, np.array([])\n\n    feat_row, feat_dim = feat_array.shape\n    length_for_per_feat = 1 + left_context + right_context\n    #aim_feat_dim = feat_dim * length_for_per_feat\n    output_list = []\n    for row_number in range(feat_row):\n      read_row_number_begin = row_number - left_context\n      read_row_number_end = row_number + right_context\n      if read_row_number_begin < 0 or read_row_number_end >= feat_row:\n        tmp_output_list = []\n        for tmp_row_number in range(length_for_per_feat):\n          read_row_number = row_number + tmp_row_number - left_context\n          if read_row_number < 0:\n            read_row_number = 0\n\n          if read_row_number >= feat_row:\n            read_row_number = feat_row - 1\n\n          tmp_output_list.append(feat_array[read_row_number])\n        tmp_output_array = np.hstack(tmp_output_list)\n        output_list.append(tmp_output_array)\n      else:\n        output_list.append(\n            feat_array[read_row_number_begin:read_row_number_end + 1].flatten())\n    output_array = np.vstack(output_list)\n    return 0, output_array\n'"
delta/data/utils/test_utils.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Tests for data utilities.""""""\nimport json\nimport tempfile\nfrom pathlib import Path\nimport kaldiio\nimport numpy as np\nfrom delta.data.utils.common_utils import JsonNumpyEncoder\n\n\ndef generate_json_data(config, mode, nexamples):\n  """"""Generate Json data for test.""""""\n\n  # pylint: disable=too-many-locals\n  tmpdir = Path(tempfile.mkdtemp())\n  ark = str(tmpdir.joinpath(\'test.ark\'))\n  scp = str(tmpdir.joinpath(\'test.scp\'))\n  ilens = 100\n  nfeat = 40\n  nexamples = nexamples\n  desire_xs = []\n  desire_ilens = []\n  desire_ys = []\n  desire_olens = []\n  with kaldiio.WriteHelper(\'ark,scp:{},{}\'.format(ark, scp)) as out_f:\n    for i in range(nexamples):\n      # pylint: disable=invalid-name\n      x = np.random.random((ilens, nfeat)).astype(np.float32)\n      uttid = \'uttid{}\'.format(i)\n      out_f[uttid] = x\n      desire_xs.append(x)\n      desire_ilens.append(ilens)\n      desire_ys.append(np.array([1, 2, 3, 10]))\n      desire_olens.append(4)\n\n  dummy_json = {}\n  dummy_json[\'utts\'] = {}\n  with open(scp, \'r\') as out_f:\n    for line in out_f:\n      uttid, path = line.strip().split()\n      dummy_json[\'utts\'][uttid] = {\n          \'input\': [{\n              \'feat\': path,\n              \'name\': \'input1\',\n              \'shape\': [ilens, nfeat]\n          }],\n          \'output\': [{\n              \'tokenid\': \'1 2 3 10\',\n              \'name\': \'output1\',\n              \'shape\': [4, 10]\n          }]\n      }\n\n  path = tmpdir.joinpath(\'{}.json\'.format(mode))\n  path.touch(exist_ok=True)\n  path = str(path.resolve())\n  with open(path, \'w\') as out_f:\n    json.dump(dummy_json, out_f, cls=JsonNumpyEncoder)\n    config[\'data\'][mode][\'paths\'] = [path]\n\n  return desire_xs, desire_ilens, desire_ys, desire_olens\n\n\ndef generate_vocab_file():\n  """"""Generate Vocab file for test.""""""\n  tmpdir = Path(tempfile.mkdtemp())\n  vocab_file = str(tmpdir.joinpath(\'vocab.txt\'))\n  dummy_vocabs = [""</s>"", ""<unk>"", ""\xe4\xbd\xa0\xe5\xa5\xbd"", ""\xe5\x8c\x97\xe4\xba\xac""]\n  save_a_vocab_file(vocab_file, dummy_vocabs)\n  return vocab_file\n\n\ndef save_a_vocab_file(vocab_file, vocab_list):\n  """"""Save a Vocab file for test.""""""\n  with open(vocab_file, ""w"", encoding=\'utf-8\') as out_f:\n    for vocab in vocab_list:\n      out_f.write(vocab)\n      out_f.write(\'\\n\')\n  return vocab_file\n\n\ndef random_upsampling(data, sample_num):\n  """"""Up sample""""""\n  np.random.seed(2019)\n  new_indices = np.random.permutation(range(sample_num))\n  original_size = len(data)\n  new_data = [data[i % original_size] for i in new_indices]\n  return new_data\n\n\ndef mock_a_text_file(sample_lines, line_num, file_name):\n  """"""Generate a mock text file for test.""""""\n  with open(file_name, ""w"", encoding=""utf-8"") as f:\n    lines = random_upsampling(sample_lines, line_num)\n    for line in lines:\n      f.write(line + ""\\n"")\n\n\ndef mock_a_npy_file(data, npy_name):\n  """"""Generate a mock npy file for test.""""""\n  data_arr = np.array(data)\n  np.save(npy_name, data_arr)\n'"
delta/data/utils/vocabulary.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Going to be deprecated""""""\nimport copy\nimport collections\n\n# pylint: disable=too-many-instance-attributes\n\n\nclass Vocabulary:\n  \'\'\' vocabulary \'\'\'\n\n  def __init__(self, use_default_dict):\n    self._padding_token = ""<pad>""\n    self._unknown_token = ""<unk>""\n    self._start_of_sentence = ""<sos>""\n    self._end_of_sentence = ""<eos>""\n    self._s_token = ""<s>""\n    self._slash_s_token = ""</s>""\n    self._default_dict = {\n        self._padding_token: 0,\n        self._s_token: 1,\n        self._slash_s_token: 2,\n        self._unknown_token: 3,\n        self._start_of_sentence: 4,\n        self._end_of_sentence: 5\n    }\n    self.use_default_dict = use_default_dict\n    if self.use_default_dict:\n      self._mapping = copy.deepcopy(self._default_dict)\n    else:\n      self._mapping = {}\n    self._freq = collections.defaultdict(int)\n\n  def __getitem__(self, key):\n    return self._mapping[key]\n\n  def add(self, word):\n    \'\'\' update vocab statis\'\'\'\n    if word not in self._mapping:\n      self._mapping[word] = len(self._mapping)\n    self._freq[word] += 1\n\n  def trim(self, min_frequency):\n    \'\'\' trim word freq less than min_frequency\'\'\'\n    # sort by frequency\n    self._freq = sorted(self._freq.items(), key=lambda x: x[1], reverse=True)\n\n    if self.use_default_dict:\n      self._mapping = copy.deepcopy(self._default_dict)\n      idx = len(self._default_dict)\n    else:\n      self._mapping = {}\n      idx = 0\n\n    for word, count in self._freq:\n      if count < min_frequency:\n        break\n      if word in self._mapping:\n        continue\n      self._mapping[word] = idx\n      idx += 1\n    self._freq = dict(self._freq[:idx - 1])\n\n  @property\n  def freq(self):\n    \'\'\'candy _freq\'\'\'\n    return self._freq\n\n  @property\n  def mapping(self):\n    \'\'\' candy _mapping\'\'\'\n    return self._mapping\n'"
delta/utils/decode/py_ctc.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' ctc python decoder \'\'\'\n\nfrom itertools import groupby\nimport numpy as np\n\n\ndef ctc_greedy_decode(predict_seqs, blank_id, unique=True):\n  """"""\n        decod using simple greedy search\n        :param predict_seqs: ([B, T, C], type=float) the output of CTC-based asr model.\n        :param blank_id: (type=int) the index of blank symbol\n        :param unique: (type=boolean) if merge consecutive repeated classes in output\n        :return: ([B, T]) the result of greedy decode\n        """"""\n  decode_seq = []\n\n  for predict_seq in predict_seqs:\n    cur_token_list = [np.argmax(prob_list) for prob_list in predict_seq]\n\n    cur_decode_list = []\n    if unique:\n      temp = groupby(cur_token_list)\n      cur_decode_list = [key for key, group in temp if key != blank_id]\n    else:\n      cur_decode_list = [\n          token_id for token_id in cur_token_list if token_id != blank_id\n      ]\n    decode_seq.append(cur_decode_list)\n\n  return decode_seq\n'"
delta/utils/decode/py_ctc_test.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' ctc python decoder test \'\'\'\n\nimport delta.compat as tf\nfrom delta.utils.decode import py_ctc\n\n\nclass PyCtcTest(tf.test.TestCase):\n  \'\'\' ctc python decode unittest\'\'\'\n\n  def setUp(self):\n    super().setUp()\n    \'\'\' setup \'\'\'\n    self.model_output = [[[0.1, 0.3, 0.5, 0.1], [0.1, 0.3, 0.5, 0.1],\n                          [0.5, 0.1, 0.3, 0.1]],\n                         [[0.1, 0.2, 0.3, 0.4], [0.3, 0.4, 0.2, 0.1]]]\n\n  def tearDown(self):\n    \'\'\' tear down \'\'\'\n\n  def test_ctc_greedy_decode(self):\n    \'\'\' ctc greedy decode unittest\'\'\'\n    decode_result_list = py_ctc.ctc_greedy_decode(\n        self.model_output, 0, unique=False)\n    self.assertEqual(decode_result_list, [[2, 2], [3, 1]])\n\n    decode_result_list = py_ctc.ctc_greedy_decode(\n        self.model_output, 0, unique=True)\n    self.assertEqual(decode_result_list, [[2], [3, 1]])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
delta/utils/decode/tf_ctc.py,14,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' ctc tensorflow decoder \'\'\'\n\nimport delta.compat as tf\nfrom delta.utils import ctc_utils\n\n\ndef ctc_decode_blankid_to_last(logits, sequence_length, blank_id=None):\n  \'\'\'\n    Moves the blank_label cloumn to the end of the logit matrix,\n    and adjust the rank of sequence_length to 1\n    param: logits, (B, T, C), output of ctc asr model\n    param: sequence_length, (B, 1), sequence lengths\n    param: blank_id, None, default blank_id is 0, same to espnet.\n    return: logits_return, (T, B, C)\n            sequence_length_return, (B)\n    \'\'\'\n\n  logits = tf.transpose(logits, [1, 0, 2])\n  #blank_id=0 is used as default in Espnet,\n  #while blank_id is set as C-1 in tf.nn.ctc_decoder\n  if blank_id is None:\n    blank_id = 0\n  logits = ctc_utils.logits_blankid_to_last(logits=logits, blank_index=blank_id)\n\n  sequence_length_return = tf.cond(\n      pred=tf.equal(tf.rank(sequence_length), 1),\n      true_fn=lambda: sequence_length,\n      false_fn=lambda: tf.squeeze(sequence_length),\n  )\n\n  return logits, sequence_length_return, blank_id\n\n\ndef ctc_greedy_decode(logits,\n                      sequence_length,\n                      merge_repeated=True,\n                      blank_id=None):\n  \'\'\'\n    ctc greedy decode function\n    param: logits, (B, T, C), output of ctc asr model\n    param: sequence_length, (B, 1), sequence lengths\n    param: merge_repeated, boolean, if merge consecutive repeated classes in output\n    returns:\n        decode_result, (B, T), decode result\n        probs, (B), A float matrix containing, for the sequence found,\n            the negative of the sum of the greatest logit at each timeframe.\n    \'\'\'\n\n  logits, sequence_len, blank_id = ctc_decode_blankid_to_last(\n      logits, sequence_length, blank_id)\n  deps = [\n      tf.assert_rank(logits, 3),\n      tf.assert_rank(sequence_len, 1),\n  ]\n\n  with tf.control_dependencies(deps):\n    decode_result, probs = tf.nn.ctc_greedy_decoder(\n        logits, sequence_len, merge_repeated=merge_repeated)\n    decode_result = [\n        ctc_utils.labels_last_to_blankid(single_decode_result, blank_id)\n        for single_decode_result in decode_result\n    ]\n    decode_result = tf.sparse_tensor_to_dense(\n        decode_result[0], default_value=blank_id, name=""outputs"")\n  return decode_result, probs\n\n\ndef ctc_beam_search_decode(logits,\n                           sequence_length,\n                           beam_width=1,\n                           top_paths=1,\n                           blank_id=None):\n  \'\'\'\n    ctc beam search decode function\n    param: logits, (B, T, C), output of ctc asr model\n    param: sequence_length, (B, 1), sequence lengths\n    param: beam_width, int, beam search beam width\n    param: top_paths, int, controls output size\n    return:\n       decode_result, (B, T), decode result\n       probs: A float matrix [batch_size, top_paths] containing sequence log-probabilities.\n    \'\'\'\n\n  logits, sequence_len, blank_id = ctc_decode_blankid_to_last(\n      logits, sequence_length, blank_id)\n\n  deps = [tf.assert_rank(logits, 3), tf.assert_rank(sequence_len, 1)]\n\n  with tf.control_dependencies(deps):\n    decode_result, probs = tf.nn.ctc_beam_search_decoder_v2(\n        logits, sequence_len, beam_width=beam_width, top_paths=top_paths)\n    decode_result_recovery_blank_id = [\n        ctc_utils.labels_last_to_blankid(single_decode_result, blank_id)\n        for single_decode_result in decode_result\n    ]\n    decode_result_dense = [\n        tf.sparse_tensor_to_dense(result, default_value=blank_id)\n        for result in decode_result_recovery_blank_id\n    ]\n  return decode_result_dense, probs\n'"
delta/utils/decode/tf_ctc_test.py,10,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' ctc tensorflow decode unittest \'\'\'\n\nimport numpy as np\nimport delta.compat as tf\n\nfrom delta.utils.decode import tf_ctc\n\n\nclass DecodeUtilTest(tf.test.TestCase):\n  \'\'\' ctc tensorflow decode util unittest\'\'\'\n\n  def setUp(self):\n    super().setUp()\n    \'\'\' setup \'\'\'\n    self.logits = np.asarray(\n        [[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553],\n          [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436],\n          [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688],\n          [0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533],\n          [0.158235, 0.196634, 0.123377, 0.50648837, 0.00903441, 0.00623107]],\n         [[0.30176, 0.28562, 0.0831517, 0.0862751, 0.0816851, 0.161508],\n          [0.24082, 0.397533, 0.0557226, 0.0546814, 0.0557528, 0.19549],\n          [0.230246, 0.450868, 0.0389607, 0.038309, 0.0391602, 0.202456],\n          [0.280884, 0.429522, 0.0326593, 0.0339046, 0.0326856, 0.190345],\n          [0.423286, 0.315517, 0.0338439, 0.0393744, 0.0339315, 0.154046]]],\n        dtype=np.float32)\n\n    self.sequence_lens = np.expand_dims(np.asarray([5, 5], dtype=np.int32), 1)\n    self.decode_result = np.asarray([[1, 1, 1, 3], [1, 1, 1, 0]],\n                                    dtype=np.int32)\n\n  def tearDown(self):\n    \'\'\' tear down \'\'\'\n\n  def test_ctc_decode_blankid_to_last(self):\n    \'\'\' unit test case for the ctc_decode_blankid_to_last interface \'\'\'\n    with self.cached_session():\n      logits, sequence_lens, blank_id = tf_ctc.ctc_decode_blankid_to_last(\n          tf.constant(self.logits), tf.constant(self.sequence_lens))\n      logits_after_transform = np.asarray(\n          [[[0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553, 0.633766],\n            [0.28562, 0.0831517, 0.0862751, 0.0816851, 0.161508, 0.30176]],\n           [[0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436, 0.111121],\n            [0.397533, 0.0557226, 0.0546814, 0.0557528, 0.19549, 0.24082]],\n           [[0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688, 0.0357786],\n            [0.450868, 0.0389607, 0.038309, 0.0391602, 0.202456, 0.230246]],\n           [[0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533, 0.0663296],\n            [0.429522, 0.0326593, 0.0339046, 0.0326856, 0.190345, 0.280884]],\n           [[0.196634, 0.123377, 0.5064884, 0.00903441, 0.00623107, 0.158235],\n            [0.315517, 0.0338439, 0.0393744, 0.0339315, 0.154046, 0.423286]]],\n          dtype=np.float32)\n      sequence_lens_after_transform = np.asarray([5, 5], dtype=np.int32)\n      blank_id_after_transform = 0\n      self.assertAllClose(logits.eval(), logits_after_transform)\n      self.assertAllEqual(sequence_lens.eval(), sequence_lens_after_transform)\n      self.assertAllEqual(blank_id, blank_id_after_transform)\n\n      logits, sequence_lens, blank_id = tf_ctc.ctc_decode_blankid_to_last(\n          tf.constant(self.logits), tf.constant(self.sequence_lens), blank_id=2)\n      logits_after_transform = np.asarray(\n          [[[0.633766, 0.221185, 0.0129757, 0.0142857, 0.0260553, 0.0917319],\n            [0.30176, 0.28562, 0.0862751, 0.0816851, 0.161508, 0.0831517]],\n           [[0.111121, 0.588392, 0.0055756, 0.00569609, 0.010436, 0.278779],\n            [0.24082, 0.397533, 0.0546814, 0.0557528, 0.19549, 0.0557226]],\n           [[0.0357786, 0.633813, 0.00249248, 0.00272882, 0.0037688, 0.321418],\n            [0.230246, 0.450868, 0.038309, 0.0391602, 0.202456, 0.0389607]],\n           [[0.0663296, 0.643849, 0.00283995, 0.0035545, 0.00331533, 0.280111],\n            [0.280884, 0.429522, 0.0339046, 0.0326856, 0.190345, 0.0326593]],\n           [[0.158235, 0.196634, 0.5064884, 0.00903441, 0.00623107, 0.123377],\n            [0.423286, 0.315517, 0.0393744, 0.0339315, 0.154046, 0.0338439]]],\n          dtype=np.float32)\n      sequence_lens_after_transform = np.asarray([5, 5], dtype=np.int32)\n      blank_id_after_transform = 2\n      self.assertAllClose(logits.eval(), logits_after_transform)\n      self.assertAllEqual(sequence_lens.eval(), sequence_lens_after_transform)\n      self.assertAllEqual(blank_id, blank_id_after_transform)\n\n  def test_ctc_greedy_decode(self):\n    \'\'\' ctc tensorflow greedy decode unittest \'\'\'\n\n    with self.cached_session():\n      decode_result, _ = tf_ctc.ctc_greedy_decode(\n          tf.constant(self.logits),\n          tf.constant(self.sequence_lens),\n          merge_repeated=True)\n      self.assertAllEqual(decode_result.eval(), [[1, 3], [1, 0]])\n\n      decode_result, _ = tf_ctc.ctc_greedy_decode(\n          tf.constant(self.logits),\n          tf.constant(self.sequence_lens),\n          merge_repeated=False)\n      self.assertAllEqual(decode_result.eval(), [[1, 1, 1, 3], [1, 1, 1, 0]])\n\n  def test_ctc_beam_search_decode(self):\n    \'\'\' ctc tensorflow beam search unittest\'\'\'\n\n    with self.cached_session():\n      decode_result, _ = tf_ctc.ctc_beam_search_decode(\n          tf.constant(self.logits),\n          tf.constant(self.sequence_lens),\n          beam_width=1,\n          top_paths=1)\n      self.assertAllEqual(decode_result[0].eval(), [[1], [1]])\n\n\nif __name__ == \'__main__\':\n  tf.test.main()\n'"
delta/utils/kaldi/__init__.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n"""""" Utils for Kaldi data IO. """"""\n'"
delta/utils/kaldi/kaldi_dir.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n"""""" Meta info for Kaldi data directory. """"""\nimport os\nfrom collections import defaultdict\n\nfrom absl import logging\n\n#pylint: disable=too-many-instance-attributes\n#pylint: disable=attribute-defined-outside-init\n\n\ndef _declare_dict_property(cls, name):\n  \'\'\' Declare getter and setter property for a given name. \'\'\'\n\n  def getter(self):\n    return self[name]\n\n  def setter(self, value):\n    self[name] = value\n\n  prop = property(fget=getter, fset=setter)\n  setattr(cls, name, prop)\n\n\ndef _return_none():\n  return None\n\n\nclass Utt(defaultdict):\n  \'\'\' An utterance. \'\'\'\n\n  def __init__(self, default_factory=None):  # pylint: disable=unused-argument\n    super().__init__(_return_none)\n\n\nLIST_OF_UTT_PROPS = (\'wavlen\', \'featlen\', \'wav\', \'feat\', \'vad\', \'spk\')\nfor one_prop_name in LIST_OF_UTT_PROPS:\n  _declare_dict_property(Utt, one_prop_name)\n\n\nclass Spk(defaultdict):\n  \'\'\' A speaker. \'\'\'\n\n  def __init__(self, default_factory=None):  # pylint: disable=unused-argument\n    super().__init__(_return_none)\n\n\nLIST_OF_UTT_PROPS = (\'id\', \'utts\', \'numutt\')\nfor one_prop_name in LIST_OF_UTT_PROPS:\n  _declare_dict_property(Spk, one_prop_name)\n\n\nclass KaldiMetaData():\n  \'\'\' Meta data for a kaldi corpus directory. \'\'\'\n\n  def __init__(self, extra_files=None):\n    self.data_path = None\n    self.utts = defaultdict(Utt)\n    self.spks = defaultdict(Spk)\n    self.files_to_load = [\n        (\'feats.scp\', \'feat\'),\n        (\'feats.len\', \'featlen\'),\n        (\'vad.scp\', \'vad\'),\n        (\'utt2spk\', \'spk\'),\n    ]\n    if extra_files:\n      logging.info(\'Adding extra files to load: %s\' % (extra_files))\n      self.files_to_load.extend(extra_files)\n\n  @property\n  def spk2id(self):\n    \'\'\' spk to id list \'\'\'\n    return {s[0]: s[1].id for s in self.spks.items()}\n\n  def validate(self):\n    \'\'\' Sanity check. Make sure everything is (probably) OK. \'\'\'\n    # TODO: more efficient and robust. Also check speakers.\n    for utt_key in self.utts.keys():\n      first_utt_key = utt_key\n      break\n    num_props = len(self.utts[first_utt_key])\n    for utt_key, utt in self.utts.items():\n      if len(utt) != num_props:\n        logging.warning(\'Utt %s has unequal number of props with %s.\' % \\\n                     (utt_key, first_utt_key))\n        return False\n      if \'spkid\' not in utt:\n        utt[\'spkid\'] = self.spks[utt.spk].id\n    logging.warning(\n        \'All utts have same number of props, data dir appears to be OK.\')\n    return True\n\n  def collect_spks_from_utts(self):\n    \'\'\' Convert utt2spk to spk2utt. \'\'\'\n    for utt_key, utt in self.utts.items():\n      if self.spks[utt.spk].utts is None:\n        self.spks[utt.spk].utts = []\n      self.spks[utt.spk].utts.append(utt_key)\n    for spk_idx, spk in enumerate(sorted(self.spks.keys())):\n      self.spks[spk].id = spk_idx\n      self.spks[spk].numutt = len(self.spks[spk].utts)\n\n  def select_spks(self, spk_list):\n    \'\'\' Select speakers and perform a shallow copy. \'\'\'\n    new_meta = KaldiMetaData()\n    for spk in spk_list:\n      if spk in self.spks:\n        for utt_key in self.spks[spk].utts:\n          new_meta.utts[utt_key] = self.utts[utt_key]\n    new_meta.collect_spks_from_utts()\n    return new_meta\n\n  def select_utts(self, utt_list):\n    \'\'\' Select utts and perform a shallow copy. \'\'\'\n    new_meta = KaldiMetaData()\n    for utt_key in utt_list:\n      new_meta.utts[utt_key] = self.utts[utt_key]\n    new_meta.collect_spks_from_utts()\n    return new_meta\n\n  def load(self, data_path):\n    \'\'\' Load meta data from Kaldi data directory. \'\'\'\n    # TODO: support multiple data directories\n    logging.info(\'Loading Kaldi dir: %s ...\' % (data_path))\n    self.data_path = data_path\n\n    def scp_to_dict(file_path, target_dict, prop):\n      \'\'\'\n      Parse a scp file and target_dict[key][prop] = value.\n\n      Args:\n        file_path: str, path to scp file.\n        target_dict: the dict to write to.\n        prop: prop name used as dict key.\n\n      Return:\n        number of lines processed. None if file not found.\n      \'\'\'\n      num_lines = 0\n      try:\n        with open(file_path, \'r\') as fp_in:\n          for line in fp_in:\n            num_lines += 1\n            tokens = line.strip().split(None, 1)\n            target_dict[tokens[0]][prop] = tokens[1]\n        return num_lines\n      except FileNotFoundError:\n        return None\n\n    # Load various scp/ark meta files.\n    for file_name, prop_name in self.files_to_load:\n      file_path = os.path.join(data_path, file_name)\n      num_lines = scp_to_dict(file_path, self.utts, prop_name)\n      if num_lines:\n        logging.info(\'Loaded file %s (key %s) %d lines.\' \\\n                     % (file_name, prop_name, num_lines))\n      else:\n        logging.info(\'File %s (key %s) failed to load.\' \\\n                     % (file_name, prop_name))\n\n    # Reduce by speaker.\n    self.collect_spks_from_utts()\n    logging.info(\'Found %d spks.\' % (len(self.spks)))\n\n    # Load optional speaker -> id file.\n    spk2id_path = os.path.join(data_path, \'spk2id\')\n    num_lines = scp_to_dict(spk2id_path, self.spks, \'id\')\n    if num_lines:\n      logging.info(\'Loaded %d spks from spk2id.\' % (num_lines))\n\n    # Sanity check.\n    self.validate()\n\n  def dump(self, data_path, overwrite=False):\n    \'\'\' Write contents to dir in Kaldi style. \'\'\'\n\n    # Don\'t overwrite existing data repo.\n    logging.info(\'Dumping to data dir %s ...\' % (data_path))\n    if os.path.isdir(data_path) and not overwrite:\n      logging.warning(\'Dir %s exists and overwrite = False, skip.\' %\n                      (data_path))\n      return\n    os.makedirs(data_path, mode=0o755, exist_ok=True)\n\n    # Dump data in sorted order.\n    sorted_utt_keys = sorted(self.utts.keys())\n    for file_name, prop_name in self.files_to_load:\n      empty = True\n      for utt_key in sorted_utt_keys:\n        if self.utts[utt_key][prop_name]:\n          empty = False\n          break\n      if not empty:\n        file_path = os.path.join(data_path, file_name)\n        logging.info(\'Dumping prop %s to file %s ...\' % (prop_name, file_name))\n        with open(file_path, \'w\') as fp_out:\n          for utt_key in sorted_utt_keys:\n            if self.utts[utt_key][prop_name]:\n              fp_out.write(\'%s %s\\n\' % (utt_key, self.utts[utt_key][prop_name]))\n\n    # Generate speaker -> various data.\n    spk_files_to_dump = [\n        (\'spk2id\', {s[0]: s[1].id for s in self.spks.items()}),\n        (\'spk2utt\', {s[0]: s[1].utts for s in self.spks.items()}),\n        (\'spk2numutt\', {s[0]: s[1].numutt for s in self.spks.items()})\n    ]\n    sorted_spks = sorted(self.spks.keys())\n    for file_name, dict_inst in spk_files_to_dump:\n      # Maybe we can wrap this into a function?\n      logging.info(\'Dumping file %s ...\' % (file_name))\n      with open(os.path.join(data_path, file_name), \'w\') as fp_out:\n        for spk in sorted_spks:\n          value = dict_inst[spk]\n          if hasattr(value, \'__iter__\'):\n            fp_out.write(\'%s %s\\n\' % (spk, \' \'.join(sorted(value))))\n          else:\n            fp_out.write(\'%s %s\\n\' % (spk, value))\n'"
delta/utils/kaldi/kaldi_dir_test.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' speaker task unittest\'\'\'\nimport os\n\nfrom absl import logging\nimport delta.compat as tf\n\nfrom delta.utils.kaldi import kaldi_dir\nfrom delta.utils.kaldi.kaldi_dir_utils import gen_dummy_meta\n\n\nclass KaldiDirTest(tf.test.TestCase):\n  \'\'\' Kaldi dir meta data IO test\'\'\'\n\n  def setUp(self):\n    super().setUp()\n\n  def tearDown(self):\n    \'\'\' tear down\'\'\'\n\n  def test_property(self):\n    \'\'\' test custom properties  \'\'\'\n    utt = kaldi_dir.Utt()\n    self.assertIsNone(utt.wavlen)\n    utt.wavlen = 1\n    self.assertEqual(utt.wavlen, 1)\n\n  def test_gen_dummy_data(self):\n    \'\'\' test dump and load data \'\'\'\n    num_spk = 5\n    num_utt_per_spk = 3\n    meta = gen_dummy_meta(num_spk, num_utt_per_spk)\n    self.assertEqual(len(meta.spks), num_spk)\n\n  def test_dump_and_load(self):\n    \'\'\' test dump and load data \'\'\'\n    temp_dir = self.get_temp_dir()\n    num_spk = 5\n    num_utt_per_spk = 3\n    meta = gen_dummy_meta(num_spk, num_utt_per_spk)\n    meta.dump(temp_dir, True)\n    with open(os.path.join(temp_dir, \'feats.scp\'), \'r\') as fp_in:\n      logging.info(\'feats.scp:\\n%s\' % (fp_in.read()))\n    loaded_meta = kaldi_dir.KaldiMetaData()\n    loaded_meta.load(temp_dir)\n    self.assertEqual(len(meta.utts), len(loaded_meta.utts))\n    for utt_key in meta.utts.keys():\n      self.assertIn(utt_key, loaded_meta.utts)\n    self.assertEqual(len(meta.spks), len(loaded_meta.spks))\n    for spk_key in meta.spks.keys():\n      self.assertIn(spk_key, loaded_meta.spks)\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.DEBUG)\n  tf.test.main()\n'"
delta/utils/kaldi/kaldi_dir_utils.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n"""""" Meta info for Kaldi data directory. """"""\nimport os\nfrom random import Random\nfrom absl import logging\nimport numpy as np\n\nimport kaldiio\n\nfrom delta.utils.kaldi import kaldi_dir\n\n\ndef gen_dummy_meta(num_spk, num_utt_per_spk):\n  \'\'\' Generate a dummy data. \'\'\'\n  meta = kaldi_dir.KaldiMetaData()\n  for spk_idx in range(num_spk):\n    for utt_idx in range(num_utt_per_spk):\n      spk = str(spk_idx)\n      utt = \'%s_%d\' % (spk, utt_idx)\n      utt_meta = kaldi_dir.Utt()\n      utt_meta.feat = \'foo/bar/feat/%s\' % (utt)\n      utt_meta.vad = \'foo/bar/vad/%s\' % (utt)\n      utt_meta.spk = spk\n      meta.utts[utt] = utt_meta\n  meta.collect_spks_from_utts()\n  return meta\n\n\n# pylint: disable=too-many-locals\ndef gen_dummy_data_dir(data_dir,\n                       num_spk,\n                       num_utt_per_spk,\n                       feat_len=100,\n                       feat_dim=40):\n  \'\'\' Generate a dummy data directory and return its meta. \'\'\'\n  os.makedirs(data_dir, exist_ok=True)\n\n  meta = kaldi_dir.KaldiMetaData()\n  feats = {}\n  vads = {}\n  for spk_idx in range(num_spk):\n    for utt_idx in range(num_utt_per_spk):\n      spk = str(spk_idx)\n      utt = \'%s_%d\' % (spk, utt_idx)\n      utt_meta = kaldi_dir.Utt()\n      feat_mat = np.ones((feat_len, feat_dim), dtype=\'float32\')\n      feats[utt] = feat_mat\n      utt_meta.featlen = feat_len\n      vad_mat = np.ones((feat_len,), dtype=\'float32\')\n      vads[utt] = vad_mat\n      utt_meta.spk = spk\n      meta.utts[utt] = utt_meta\n  meta.collect_spks_from_utts()\n  meta.dump(data_dir, True)\n\n  feats_ark_path = os.path.join(data_dir, \'feats.ark\')\n  feats_scp_path = os.path.join(data_dir, \'feats.scp\')\n  kaldiio.save_ark(feats_ark_path, feats, scp=feats_scp_path, text=True)\n  vad_ark_path = os.path.join(data_dir, \'vad.ark\')\n  vad_scp_path = os.path.join(data_dir, \'vad.scp\')\n  kaldiio.save_ark(vad_ark_path, vads, scp=vad_scp_path, text=True)\n\n  loaded_meta = kaldi_dir.KaldiMetaData()\n  loaded_meta.load(data_dir)\n  return loaded_meta\n\n\ndef subset_data_dir_tr_cv(meta,\n                          num_spk_cv=0,\n                          num_utt_cv=0,\n                          fair_choice=True,\n                          keep_spk_id=True,\n                          seed=123):\n  \'\'\'\n  Randomly split a data directory into training and validation set.\n  This function acts like Kaldi\'s subset_data_dir_tr_cv.sh .\n\n  Args:\n    meta: a KaldiMetaData instance.\n    num_spk_cv: number of speakers in the validation set. <1 means fraction.\n                e.g. 0.1 = 10% of total speakers.\n    num_utt_cv: number of utts in the validation set. <1 means fraction.\n    fair_choice: utt mode only, choose utts from each spk with uniform chance.\n    keep_spk_id: keep spk2id mapping from source data dir?\n    seed: random seed.\n\n  Returns:\n    A tuple of (tr, cv) containing KaldiMetaData instances.\n\n  Note:\n    This function is definitive given the same seed.\n  \'\'\'\n  if num_spk_cv < 0 or num_utt_cv < 0:\n    raise ValueError\n  if num_spk_cv > 0 and num_utt_cv > 0:\n    raise ValueError\n  num_spks = len(meta.spks)\n  num_utts = len(meta.utts)\n  meta_tr = None\n  meta_cv = None\n  rand = Random(seed)\n\n  if num_spk_cv > 0:\n    if num_spk_cv < 1:\n      num_spk_cv = int(num_spks * num_spk_cv)\n    else:\n      if num_spk_cv != int(num_spk_cv):\n        raise ValueError\n      num_spk_cv = int(num_spk_cv)\n    logging.info(\'Intended #spks cv: %d\' % (num_spk_cv))\n    spks_shuffled = list(meta.spks.keys())\n    rand.shuffle(spks_shuffled)\n    spks_tr = spks_shuffled[num_spk_cv:]\n    spks_cv = spks_shuffled[0:num_spk_cv]\n    logging.info(\'Actual #spks cv: %d, tr: %d\' % (len(spks_cv), len(spks_tr)))\n    meta_tr = meta.select_spks(spks_tr)\n    meta_cv = meta.select_spks(spks_cv)\n    assert len(meta_tr.spks) + len(meta_cv.spks) == num_spks\n  else:\n    if num_utt_cv < 1:\n      num_utt_cv = int(num_utts * num_utt_cv)\n    else:\n      if num_utt_cv != int(num_utt_cv):\n        raise ValueError\n      num_utt_cv = int(num_utt_cv)\n    logging.info(\'Intended #utt cv: %d\' % (num_utt_cv))\n    if num_utt_cv > num_utts:\n      num_utt_cv = num_utts\n\n    logging.info(\'fair_choice = %s\' % fair_choice)\n    if fair_choice:\n      logging.info(\'Applying fair choice across spks.\')\n      spk_utts = [list(spk.utts) for spk in meta.spks.values()]\n      rand.shuffle(spk_utts)\n      utts_tr = []\n      utts_cv = []\n      while len(utts_cv) < num_utt_cv:\n        for utts in spk_utts:\n          utt_idx = rand.randint(0, len(utts) - 1)\n          utts_cv.append(utts.pop(utt_idx))\n          if not utts:\n            utts = []\n            spk_utts.remove(utts)\n          if len(utts_cv) >= num_utt_cv:\n            break\n      for utts in spk_utts:\n        utts_tr.extend(utts)\n    else:\n      utts_shuffled = list(meta.utts.keys())\n      rand.shuffle(utts_shuffled)\n      utts_tr = utts_shuffled[num_utt_cv:]\n      utts_cv = utts_shuffled[0:num_utt_cv]\n\n    logging.info(\'Actual #utts cv: %d, tr: %d\' % (len(utts_cv), len(utts_tr)))\n    meta_tr = meta.select_utts(utts_tr)\n    meta_cv = meta.select_utts(utts_cv)\n\n  if keep_spk_id:\n    for spk in meta.spks:\n      meta_tr.spks[spk].id = meta.spks[spk].id\n      meta_cv.spks[spk].id = meta.spks[spk].id\n\n  assert len(meta_tr.utts) + len(meta_cv.utts) == num_utts\n  return meta_tr, meta_cv\n'"
delta/utils/loss/__init__.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' loss module \'\'\'\n'"
delta/utils/loss/base_loss.py,1,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' base interface of loss \'\'\'\nimport abc\nimport delta.compat as tf\n\nfrom delta.utils import summary\n\n\nclass ABCLoss(metaclass=abc.ABCMeta):  #pylint: disable=too-few-public-methods\n  \'\'\' abstract of Loss \'\'\'\n  #pylint: disable=too-many-arguments\n  @abc.abstractmethod\n  def call(self,\n           logits=None,\n           input_length=None,\n           labels=None,\n           label_length=None,\n           **kwargs):\n    \'\'\'\n    param: logits, (B, T, D)\n    param: labels, (B, T)\n    param: label_length, (B), converts labels form dense to sparse\n    param: input_length, (B), input length of encoder\n    returns: loss, scalar\n    \'\'\'\n    raise NotImplementedError()\n\n\nclass Loss(ABCLoss):  #pylint: disable=abstract-method\n  \'\'\' wappwer of abstrcat Loss \'\'\'\n\n  def __init__(self, config):\n    self._config = config\n\n  @property\n  def config(self):\n    \'\'\' config property \'\'\'\n    return self._config\n\n  def __call__(self, **kwargs):\n    name = kwargs.get(\'name\')\n    kwargs.pop(\'name\')\n    with tf.variable_scope(name):\n      loss = self.call(**kwargs)\n    summary.scalar(name, loss)\n    return loss\n'"
delta/utils/loss/loss_impl.py,5,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' loss implementation\'\'\'\nimport delta.compat as tf\n\nfrom delta.utils.loss.base_loss import Loss\nfrom delta.utils.loss.loss_utils import cross_entropy\nfrom delta.utils.loss.loss_utils import mask_sequence_loss\nfrom delta.utils.loss.loss_utils import ctc_lambda_loss\nfrom delta.utils.loss.loss_utils import crf_log_likelihood\nfrom delta.utils.loss.loss_utils import focal_loss\n\nfrom delta.utils.register import registers\n\n\n@registers.loss.register\nclass CrossEntropyLoss(Loss):\n  \'\'\' cross entropy loss for classfication and sequence classfication \'\'\'\n\n  def __init__(self, config):\n    super().__init__(config)\n    self.smoothing = self.config[\'solver\'][\'optimizer\'][\'label_smoothing\']\n\n  #pylint: disable=too-many-arguments\n  def call(self,\n           logits=None,\n           input_length=None,\n           labels=None,\n           label_length=None,\n           **kwargs):\n\n    loss = cross_entropy(\n        logits=logits,\n        input_length=input_length,\n        labels=labels,\n        label_length=label_length,\n        smoothing=self.smoothing)\n    return loss\n\n\n@registers.loss.register\nclass DistillationLoss(Loss):\n  \'\'\' Distilling the Knowledge in a Neural Network, arXiv:1503.02531 \'\'\'\n\n  def __init__(self, config):\n    super().__init__(config)\n    self.smoothing = self.config[\'solver\'][\'optimizer\'][\'label_smoothing\']\n    self.temperature = self.config[\'solver\'][\'distilling\'][\'temperature\']\n    self.alpha = self.config[\'solver\'][\'distilling\'][\'alpha\']\n\n    assert self.alpha >= 0.0, ""alpha : {}"".format(self.alpha)\n    assert self.alpha <= 1.0, ""alpha : {}"".format(self.alpha)\n    assert self.temperature >= 1, ""temperature : {}"".format(self.temperature)\n    self.T = tf.convert_to_tensor(self.temperature, dtype=tf.float32)  #pylint: disable=invalid-name\n\n  #pylint: disable=too-many-arguments\n  def call(self,\n           logits=None,\n           input_length=None,\n           labels=None,\n           label_length=None,\n           **kwargs):\n\n    assert ""soft_lables"" in kwargs\n    soft_labels = kwargs[""soft_labels""]\n\n    loss_standard = cross_entropy(\n        logits=logits,\n        input_length=input_length,\n        labels=labels,\n        label_length=label_length,\n        smoothing=self.smoothing)\n    loss_soft = cross_entropy(\n        logits=logits / self.T,\n        input_length=input_length,\n        labels=soft_labels,\n        label_length=label_length,\n        smoothing=self.smoothing)\n    # Since the magnitudes of the gradients produced by the soft targets\n    # scale as 1/T2 , it is important to multiply them by T2 when using\n    # both hard and soft targets\n    total_loss = self.alpha * tf.square(\n        self.T) * loss_soft + (1 - self.alpha) * loss_standard\n\n    return total_loss\n\n\n@registers.loss.register\nclass CTCLoss(Loss):\n  \'\'\' ctc loss \'\'\'\n\n  def __init__(self, config):  #pylint: disable=useless-super-delegation\n    super().__init__(config)\n\n  #pylint: disable=too-many-arguments\n  def call(self,\n           logits=None,\n           input_length=None,\n           labels=None,\n           label_length=None,\n           **kwargs):\n\n    blank_index = kwargs.get(\'blank_index\', 0)\n    return ctc_lambda_loss(\n        logits=logits,\n        input_length=input_length,\n        labels=labels,\n        label_length=label_length,\n        blank_index=blank_index)\n\n\n@registers.loss.register\nclass CrfLoss(Loss):\n  \'\'\'crf loss for sequence labeling\'\'\'\n\n  def __init__(self, config):\n    super().__init__(config)\n\n  # pylint: disable=too-many-arguments\n  def call(self,\n           logits=None,\n           input_length=None,\n           labels=None,\n           label_length=None,\n           **kwargs):\n    assert ""model"" in kwargs\n    model = kwargs[""model""]\n    tags_scores = tf.reshape(\n        logits, [-1, model.max_len, model.seq_num_classes], name=""scores"")\n    loss, _ = crf_log_likelihood(tags_scores, labels, input_length,\n                                 model.transitions)\n\n    return loss\n\n\n@registers.loss.register\nclass SequenceCrossEntropyLoss(Loss):\n  \'\'\' cross entropy loss for sequence to sequence \'\'\'\n\n  def __init__(self, config):\n    super().__init__(config)\n\n  #pylint: disable=too-many-arguments\n  def call(self,\n           logits=None,\n           input_length=None,\n           labels=None,\n           label_length=None,\n           **kwargs):\n\n    loss = mask_sequence_loss(logits, labels, input_length, label_length)\n    return loss\n\n\n@registers.loss.register\nclass FocalLoss(Loss):\n\n  def __init__(self, config):\n    super().__init__(config)\n\n    class_num = self._config[\'data\'][\'task\'][\'classes\'][\'num\']\n    if \'alpha\' in self._config[\'data\'][\'task\'][\'classes\']:\n      self.alpha = self._config[\'data\'][\'task\'][\'classes\'][\'alpha\']\n      assert len(self.alpha) == class_num, \'alpha len is not equal to class_num\'\n      self.alpha = tf.constant(self.alpha)\n    else:\n      self.alpha = tf.ones([class_num])\n\n    self.gamma = 2\n    if \'gamma\' in self._config[\'solver\'][\'optimizer\']:\n      self.gamma = self._config[\'solver\'][\'optimizer\'][\'gamma\']\n    assert self.gamma >= 0, \'gamma must greater than or equal to zero\'\n\n  def call(self,\n           logits=None,\n           input_length=None,\n           labels=None,\n           label_length=None,\n           **kwargs):\n\n    del input_length\n    del label_length\n\n    return focal_loss(\n        logits=logits, labels=labels, gamma=self.gamma, name=\'focal_loss\')\n'"
delta/utils/loss/loss_utils.py,53,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' loss implementation function \'\'\'\nimport math\nimport delta.compat as tf\nimport tensorflow_addons as tfa\n\nfrom delta import utils\nfrom delta.utils import ctc_utils\n\n\n#pylint: disable=too-many-arguments\ndef cross_entropy(logits,\n                  labels,\n                  input_length=None,\n                  label_length=None,\n                  smoothing=0.0,\n                  reduction=tf.losses.Reduction.SUM_BY_NONZERO_WEIGHTS):\n  \'\'\'\n  cross entropy function for classfication and seq classfication\n  :param, label_length, for seq task, this for target seq length, e.g. a b c </s>, 4\n  \'\'\'\n  del input_length\n\n  onehot_labels = tf.cond(\n      pred=tf.equal(tf.rank(logits) - tf.rank(labels), 1),\n      true_fn=lambda: tf.one_hot(labels, tf.shape(logits)[-1], dtype=tf.int32),\n      false_fn=lambda: labels)\n\n  if label_length is not None:\n    max_len = tf.shape(logits)[1]\n    weights = utils.len_to_mask(label_length, max_len)\n  else:\n    weights = 1.0\n\n  loss = tf.losses.softmax_cross_entropy(\n      onehot_labels=onehot_labels,\n      logits=logits,\n      weights=weights,\n      label_smoothing=smoothing,\n      reduction=reduction)\n\n  return loss\n\n\ndef ctc_lambda_loss(logits, labels, input_length, label_length, blank_index=0):\n  \'\'\'\n  ctc loss function\n  psram: logits, (B, T, D)\n  psram: input_length,  (B, 1), input length of encoder\n  psram: labels, (B, T)\n  psram: label_length,  (B, 1), label length for convert dense label to sparse\n  returns: loss, scalar\n  \'\'\'\n  ilen = tf.cond(\n      pred=tf.equal(tf.rank(input_length), 1),\n      true_fn=lambda: input_length,\n      false_fn=lambda: tf.squeeze(input_length),\n  )\n  ilen = tf.cast(ilen, tf.int32)\n\n  olen = tf.cond(\n      pred=tf.equal(tf.rank(label_length), 1),\n      true_fn=lambda: label_length,\n      false_fn=lambda: tf.squeeze(label_length))\n  olen = tf.cast(olen, tf.int32)\n\n  deps = [\n      tf.assert_rank(labels, 2, name=\'label_rank_check\'),\n      tf.assert_rank(logits, 3, name=\'logits_rank_check\'),\n      tf.assert_rank(ilen, 1, name=\'src_len_rank_check\'),  # input_length\n      tf.assert_rank(olen, 1, name=\'tgt_len_rank_check\'),  # output_length\n  ]\n\n  labels, logits = ctc_data_transform(labels, logits, blank_index)\n\n  with tf.control_dependencies(deps):\n    # (B, 1)\n    # blank index is consistent with Espnet, zero\n    batch_loss = tf.nn.ctc_loss(\n        labels=labels,\n        inputs=logits,\n        sequence_length=ilen,\n        time_major=False,\n        preprocess_collapse_repeated=False,\n        ctc_merge_repeated=True,\n        ignore_longer_outputs_than_inputs=False)\n  return batch_loss\n\n\ndef ctc_data_transform(labels, logits, blank_index):\n  \'\'\'\n  data transform according blank_index\n  \'\'\'\n  logits = ctc_utils.logits_blankid_to_last(\n      logits=logits, blank_index=blank_index)\n\n  num_class = logits.shape[2]\n  labels = ctc_utils.labels_blankid_to_last(\n      labels=labels, blank_index=blank_index, num_class=num_class)\n  return labels, logits\n\n\ndef crf_log_likelihood(tags_scores, labels, input_length, transitions):\n  \'\'\'\n  :param tags_scores:  [batch_size, max_seq_len, num_tags]\n  :param labels:  [batch_size, max_seq_len]\n  :param input_length:  [batch_size,]\n  :param transitions: [num_tags, num_tags]\n  :return: loss, transition_params\n  \'\'\'\n  log_likelihood, transition_params = tfa.text.crf_log_likelihood(\n      inputs=tags_scores,\n      tag_indices=labels,\n      sequence_lengths=input_length,\n      transition_params=transitions)\n\n  loss = tf.reduce_mean(-log_likelihood)\n\n  return loss, transition_params\n\n\ndef mask_sequence_loss(logits,\n                       labels,\n                       input_length,\n                       label_length,\n                       smoothing=0.0):\n  \'\'\'\n  softmax cross entropy loss for sequence to sequence\n  :param logits: [batch_size, max_seq_len, vocab_size]\n  :param labels: [batch_size, max_seq_len]\n  :param input_length: [batch_size]\n  :param label_length: [batch_size]\n  :return: loss, scalar\n  \'\'\'\n  del smoothing\n  del input_length\n\n  if label_length is not None:\n    weights = tf.cast(utils.len_to_mask(label_length), tf.float32)\n  else:\n    weights = tf.ones_like(labels)\n  loss = tfa.seq2seq.loss.sequence_loss(logits, labels, weights)\n  return loss\n\n\n#pylint: disable=too-many-locals\ndef arcface_loss(embedding,\n                 labels,\n                 out_num,\n                 weights=None,\n                 s=64.,\n                 m=0.5,\n                 limit_to_pi=True):\n  \'\'\'\n  https://github.com/auroua/InsightFace_TF/blob/master/losses/face_losses.py\n  :param embedding: the input embedding vectors\n  :param labels:  the input labels, the shape should be eg: (batch_size, 1)\n  :param s: scalar value default is 64\n  :param out_num: output class num\n  :param weights: a tf.variable with shape (embedding.shape[-1], out_num)\n                  or None to make a new one internally. default = None\n  :param m: the margin value, default is 0.5\n  :return: the final cacualted output, this output is send into the tf.nn.softmax directly\n  \'\'\'\n  cos_m = math.cos(m)\n  sin_m = math.sin(m)\n  mm = sin_m * m  # issue 1\n  threshold = math.cos(math.pi - m)\n  with tf.variable_scope(\'arcface_loss\'):\n    # inputs and weights norm\n    embedding_norm = tf.norm(embedding, axis=1, keep_dims=True)\n    embedding = tf.div(embedding, embedding_norm, name=\'norm_embedding\')\n    if weights is None:\n      weights = tf.get_variable(\n          name=\'weights\',\n          shape=[embedding.shape[-1].value, out_num],\n          initializer=tf.initializer.glorot_unifrom())\n    weights_norm = tf.norm(weights, axis=0, keep_dims=True)\n    weights = tf.div(weights, weights_norm, name=\'norm_weights\')\n    # cos(theta+m)\n    cos_t = tf.matmul(embedding, weights, name=\'cos_t\')\n    cos_t2 = tf.square(cos_t, name=\'cos_2\')\n    sin_t2 = tf.subtract(1., cos_t2, name=\'sin_2\')\n    sin_t = tf.sqrt(sin_t2, name=\'sin_t\')\n    cos_mt = s * tf.subtract(\n        tf.multiply(cos_t, cos_m), tf.multiply(sin_t, sin_m), name=\'cos_mt\')\n\n    if limit_to_pi:\n      # this condition controls the theta+m should in range [0, pi]\n      #      0<=theta+m<=pi\n      #     -m<=theta<=pi-m\n      cond_v = cos_t - threshold\n      cond = tf.cast(tf.nn.relu(cond_v, name=\'if_else\'), dtype=tf.bool)\n\n      keep_val = s * (cos_t - mm)\n      cos_mt_temp = tf.where(cond, cos_mt, keep_val)\n    else:\n      cos_mt_temp = cos_mt\n\n    mask = tf.one_hot(labels, depth=out_num, name=\'one_hot_mask\')\n    # mask = tf.squeeze(mask, 1)\n    inv_mask = tf.subtract(1., mask, name=\'inverse_mask\')\n\n    s_cos_t = tf.multiply(s, cos_t, name=\'scalar_cos_t\')\n\n    output = tf.add(\n        tf.multiply(s_cos_t, inv_mask),\n        tf.multiply(cos_mt_temp, mask),\n        name=\'arcface_loss_output\')\n  return output\n\n\ndef focal_loss(logits, labels, alpha, gamma=2, name=\'focal_loss\'):\n  """"""\n    Focal loss for multi classification\n    :param logits: A float32 tensor of shape [batch_size num_class].\n    :param labels: A int32 tensor of shape [batch_size, num_class] or [batch_size].\n    :param alpha: A 1D float32 tensor for focal loss alpha hyper-parameter\n    :param gamma: A scalar for focal loss gamma hyper-parameter.\n    Returns: A tensor of the same shape as `lables`\n    """"""\n  if len(labels.shape) == 1:\n    labels = tf.one_hot(labels, logits.shape[-1])\n  else:\n    labels = labels\n  labels = tf.to_float(labels)\n\n  y_pred = tf.nn.softmax(logits, dim=-1)\n  L = -labels * tf.log(y_pred)\n  L *= alpha * ((1 - y_pred)**gamma)\n  loss = tf.reduce_sum(L)\n\n  tf.summary.scalar(name, loss)\n  return loss\n'"
delta/utils/loss/loss_utils_test.py,41,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' loss implementation function unittest \'\'\'\nimport numpy as np\nimport delta.compat as tf\nfrom absl import logging\n\nfrom delta.utils.loss import loss_utils\n\n\nclass LossUtilTest(tf.test.TestCase):\n  \'\'\' loss util unittest\'\'\'\n\n  def setUp(self):\n    super().setUp()\n    \'\'\' setup \'\'\'\n    # classfication: shape [2, 6]\n    self.logits = np.array([[10, 23, 43, 23, 12, 23], [32, 10, 23, 45, 23, 0]],\n                           dtype=np.float32)\n    self.labels = np.array([2, 3], dtype=np.int32)\n    # seq2seq: shape [2, 3, 6]\n    self.seq_logits = np.array(\n        [[[10, 2, 11, 23, 12, 42], [12, 32, 11, 2, 0, 0], [12, 32, 11, 2, 0, 0]\n         ], [[3, 11, 2, 32, 4, 8], [12, 1, 32, 0, 0, 0], [0, 0, 0, 0, 0, 0]]],\n        dtype=np.float32)\n    self.seq_labels = np.array([[5, 1, 1], [3, 2, 0]], dtype=np.int32)\n    self.input_length = np.array([[3, 2]], dtype=np.int32)\n    self.label_length = np.array([3, 2], dtype=np.int32)\n    # test misclassified examples\n    self.logits_2 = np.array([[10, 2, 3, 4, 5, 6], [2, 3, 10, 4, 5, 1]],\n                             dtype=np.float32)\n\n  def tearDown(self):\n    \'\'\' tear down \'\'\'\n\n  def test_cross_entropy(self):\n    \'\'\' test cross entropy\'\'\'\n    with self.cached_session():\n      loss = loss_utils.cross_entropy(\n          logits=tf.constant(self.logits),\n          input_length=None,\n          labels=tf.constant(self.labels),\n          label_length=None)\n      self.assertAllClose(loss.eval(), 0.0, rtol=1e-06, atol=1.5e-6)\n\n      loss_2 = loss_utils.cross_entropy(\n          logits=tf.constant(self.logits_2), labels=tf.constant(self.labels))\n      self.assertAllClose(loss_2.eval(), 6.5194526, rtol=1e-06, atol=1.5e-6)\n\n      loss = loss_utils.cross_entropy(\n          logits=tf.constant(self.seq_logits),\n          input_length=tf.constant(self.input_length),\n          labels=tf.constant(self.seq_labels),\n          label_length=tf.constant(self.label_length),\n          reduction=tf.losses.Reduction.NONE)\n      self.assertEqual(loss.eval().shape, (2, 3))\n      self.assertAllClose(\n          loss.eval(),\n          np.zeros((2, 3), dtype=np.float32),\n          rtol=1e-06,\n          atol=1.5e-6)\n\n      loss = loss_utils.cross_entropy(\n          logits=tf.constant(self.seq_logits),\n          input_length=tf.constant(self.input_length),\n          labels=tf.constant(self.seq_labels),\n          label_length=tf.constant(self.label_length),\n          reduction=tf.losses.Reduction.SUM_BY_NONZERO_WEIGHTS)\n      self.assertEqual(loss.eval().shape, ())\n      self.assertAllClose(loss.eval(), 0.0, rtol=1e-06, atol=1.5e-6)\n\n  def test_ctc_lambda_loss(self):\n    \'\'\' test ctc loss \'\'\'\n    with self.cached_session():\n      label_lens = np.expand_dims(np.asarray([5, 3]), 1)\n      input_lens = np.expand_dims(np.asarray([5, 3]), 1)  # number of timesteps\n      loss_log_probs = [9.409339, 5.37700698]\n\n      # dimensions are batch x time x categories\n      labels = np.asarray([[1, 2, 5, 4, 5], [3, 1, 2, 0, 0]])\n      inputs = np.asarray(\n          [[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553],\n            [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436],\n            [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688],\n            [0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533],\n            [0.458235, 0.396634, 0.123377, 0.00648837, 0.00903441, 0.00623107]],\n           [[0.30176, 0.28562, 0.0831517, 0.0862751, 0.0816851, 0.161508],\n            [0.24082, 0.397533, 0.0557226, 0.0546814, 0.0557528, 0.19549],\n            [0.230246, 0.450868, 0.0389607, 0.038309, 0.0391602, 0.202456],\n            [0.280884, 0.429522, 0.0326593, 0.0339046, 0.0326856, 0.190345],\n            [0.423286, 0.315517, 0.0338439, 0.0393744, 0.0339315, 0.154046]]],\n          dtype=np.float32)\n\n      loss = loss_utils.ctc_lambda_loss(\n          labels=tf.constant(labels),\n          logits=tf.constant(inputs),\n          input_length=tf.constant(input_lens),\n          label_length=tf.constant(label_lens),\n          blank_index=0)\n      self.assertEqual(loss.eval().shape[0], inputs.shape[0])\n      self.assertAllClose(loss.eval(), loss_log_probs, atol=1e-05)\n      self.assertAllClose(\n          np.mean(loss.eval()), np.mean(loss_log_probs), atol=1e-05)\n\n      # test when batch_size = 1, that is, one sample only\n      ref = [9.409339]\n      input_lens = np.asarray([5])\n      label_lens = np.asarray([5])\n\n      labels = np.asarray([[1, 2, 5, 4, 5]])\n      inputs = np.asarray(\n          [[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553],\n            [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436],\n            [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688],\n            [0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533],\n            [0.458235, 0.396634, 0.123377, 0.00648837, 0.00903441, 0.00623107]]\n          ],\n          dtype=np.float32)\n      loss = loss_utils.ctc_lambda_loss(\n          labels=tf.constant(labels),\n          logits=tf.constant(inputs),\n          input_length=tf.constant(input_lens),\n          label_length=tf.constant(label_lens),\n          blank_index=0)\n      self.assertAllClose(loss.eval(), ref, atol=1e-05)\n      self.assertAllClose(np.mean(loss.eval()), np.mean(ref), atol=1e-05)\n\n  def test_ctc_data_transform(self):\n    \'\'\' test ctc_data_transform \'\'\'\n    with self.cached_session():\n      \'\'\'\n      in this test case, the shape of inputs: (B,T,D) = (1, 3, 6)\n                         the shape of labels: (B,T) = (1,3)\n      \'\'\'\n      inputs = np.asarray(\n          [[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553],\n            [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436],\n            [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688]]\n          ],\n          dtype=np.float32)\n      labels = np.asarray([[1, 2, 3]], dtype=np.int64)\n\n      blank_index = 0\n      labels_after_transform, inputs_after_transform = loss_utils.ctc_data_transform(\n          labels, inputs, blank_index)\n      labels_after_transform = tf.sparse_tensor_to_dense(labels_after_transform)\n      new_labels = [[0, 1, 2]]\n      new_inputs = [\n          [[0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553, 0.633766],\n           [0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436, 0.111121],\n           [0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688, 0.0357786]]\n      ]\n      self.assertAllEqual(labels_after_transform, new_labels)\n      self.assertAllClose(inputs_after_transform, new_inputs)\n\n      blank_index = 2\n      labels_after_transform, inputs_after_transform = loss_utils.ctc_data_transform(\n          labels, inputs, blank_index)\n      labels_after_transform = tf.sparse_tensor_to_dense(labels_after_transform)\n      new_labels = [[1, 5, 2]]\n      new_inputs = [\n          [[0.633766, 0.221185, 0.0129757, 0.0142857, 0.0260553, 0.0917319],\n           [0.111121, 0.588392, 0.0055756, 0.00569609, 0.010436, 0.278779],\n           [0.0357786, 0.633813, 0.00249248, 0.00272882, 0.0037688, 0.321418]]\n      ]\n      self.assertAllEqual(labels_after_transform, new_labels)\n      self.assertAllClose(inputs_after_transform, new_inputs)\n\n      blank_index = 5\n      labels_after_transform, inputs_after_transform = loss_utils.ctc_data_transform(\n          labels, inputs, blank_index)\n      labels_after_transform = tf.sparse_tensor_to_dense(labels_after_transform)\n      new_labels = [[1, 2, 3]]\n      new_inputs = [\n          [[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553],\n           [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436],\n           [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688]]\n      ]\n      self.assertAllEqual(labels_after_transform, new_labels)\n      self.assertAllClose(inputs_after_transform, new_inputs)\n\n      with self.assertRaises(ValueError) as valueErr:\n        blank_index = -1\n        labels_after_transform, inputs_after_transform = loss_utils.ctc_data_transform(\n            labels, inputs, blank_index)\n      the_exception = valueErr.exception\n      self.assertEqual(\n          str(the_exception),\n          \'blank_index must be greater than or equal to zero\')\n\n      with self.assertRaises(ValueError) as valueErr:\n        blank_index = 10\n        labels_after_transform, inputs_after_transform = loss_utils.ctc_data_transform(\n            labels, inputs, blank_index)\n      the_exception = valueErr.exception\n      self.assertEqual(\n          str(the_exception),\n          \'blank_index must be less than or equal to num_class - 1\')\n\n  def test_crf_loss(self):\n    \'\'\' test crf loss \'\'\'\n    with self.cached_session():\n      loss_true = np.float32(5.5096426)\n      logits = np.asarray([[[0.3, 0.4, 0.3], [0.1, 0.9, 0.0], [0.2, 0.7, 0.1],\n                            [0.3, 0.2, 0.5], [0.6, 0.2, 0.2]]],\n                          dtype=np.float32)  # [1,5,3]\n      trans_params = tf.fill([3, 3], 0.5, name=\'trans_params\')\n      labels = np.asarray([[0, 1, 2, 0, 1]], dtype=np.int32)  # shape=[1,5]\n      sequence_lengths = np.asarray([5], dtype=np.int32)  # shape=[1,]\n      loss, _ = loss_utils.crf_log_likelihood(\n          tf.constant(logits), tf.constant(labels),\n          tf.constant(sequence_lengths), trans_params)\n\n      self.assertEqual(loss.eval(), loss_true)\n\n  def test_arcface_loss(self):\n    \'\'\' test arcface loss \'\'\'\n\n    def gen_fake_data(batch_size, embedding_size, num_spks):\n      \'\'\' generate fake embeddings and labels \'\'\'\n      assert batch_size == embedding_size\n      assert num_spks == embedding_size\n      embeddings = np.eye(batch_size, dtype=\'float32\')\n      labels = np.zeros((batch_size,), dtype=\'int32\')\n      for spk in range(batch_size):\n        labels[spk] = spk\n      return embeddings, labels\n\n    with self.cached_session():\n      batch_size = 4\n      embedding_size = 4\n      num_spks = 4\n      embeddings, labels = gen_fake_data(batch_size, embedding_size, num_spks)\n      weights = embeddings  # use whatever data is (somehow) trivial\n\n      weights_tensor = tf.constant(weights)\n      embeddings_tensor = tf.constant(embeddings)\n      labels_tensor = tf.constant(labels)\n\n      output_true = np.asarray(\n          [[56.165283, 0., 0., 0.], [0., 56.165283, 0., 0.],\n           [0., 0., 56.165283, 0.], [0., 0., 0., 56.165283]],\n          dtype=\'float32\')\n      output = loss_utils.arcface_loss(\n          embeddings_tensor,\n          labels_tensor,\n          num_spks,\n          weights_tensor,\n          s=64.0,\n          m=0.5,\n          limit_to_pi=True)\n      self.assertAllClose(output.eval(), output_true)\n\n  def test_focal_loss(self):\n    with self.cached_session():\n      logits = np.array([[22, 23, 24]], dtype=np.float32)\n      labels = np.array([2], dtype=np.int32)\n      alpha = tf.ones([3])\n\n      ce_loss = loss_utils.cross_entropy(\n          logits=tf.constant(logits),\n          input_length=None,\n          labels=tf.constant(labels),\n          label_length=None)\n\n      fl_loss0 = loss_utils.focal_loss(\n          logits=tf.constant(logits),\n          labels=tf.constant(labels),\n          alpha=alpha,\n          gamma=0)\n\n      self.assertAllClose(fl_loss0.eval(), 0.407606, rtol=1e-06, atol=1e-6)\n      self.assertAllClose(\n          fl_loss0.eval(), ce_loss.eval(), rtol=1e-07, atol=1e-7)\n\n      fl_loss2 = loss_utils.focal_loss(\n          logits=tf.constant(logits),\n          labels=tf.constant(labels),\n          alpha=alpha,\n          gamma=2)\n\n      fl_loss5 = loss_utils.focal_loss(\n          logits=tf.constant(logits),\n          labels=tf.constant(labels),\n          alpha=alpha,\n          gamma=5)\n\n      self.assertAllClose(fl_loss2.eval(), 0.045677, rtol=1e-06, atol=1e-6)\n      self.assertAllClose(fl_loss5.eval(), 0.001713, rtol=1e-06, atol=1e-6)\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  tf.test.main()\n'"
delta/utils/metrics/__init__.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' metrics utils \'\'\'\nfrom delta.utils.metrics.py_metrics import *\nfrom delta.utils.metrics.tf_metrics import *\nfrom delta.utils.metrics.metric_utils import *\n'"
delta/utils/metrics/metric_utils.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' metrics utils of numpy \'\'\'\nimport numpy as np\n\n\n#pylint: disable=invalid-name\ndef f1_score(tn, fp, fn, tp):\n  \'\'\'  f1 score from confusion matrix \'\'\'\n  del tn\n  return 2 * tp / (2 * tp + fp + fn)\n\n\n#pylint: disable=invalid-name\ndef stats_confusion(confusion):\n  \'\'\' confusion matrix to TP, FP, TN, FN \'\'\'\n  FP = confusion.sum(axis=0) - np.diag(confusion)\n  FN = confusion.sum(axis=1) - np.diag(confusion)\n  TP = np.diag(confusion)\n  TN = confusion.sum() - (TP + FN + TP)\n  return TN, FP, FN, TP\n\n\ndef token_error(predict_seq_list=None, target_seq_list=None, eos_id=None):\n  \'\'\' computing token error\n    :param predict_seq_list: (shape=[B,T], type=int), the list of predict token sequence\n    :param target_seq_list: ([B,T], type=int), the list of target token sequence\n    :param eos_id: (type=int), the end symbol of the target token sequence\n    return: (type=float), the token error between predict and target token sequences\n    \'\'\'\n  if len(predict_seq_list) != len(target_seq_list):\n    raise ValueError(\n        \'the number of prdict sequence and target sequence is not equal!\')\n  if eos_id is None:\n    raise ValueError(\'the end symbol of target sequence is None!\')\n\n  levenshtein_distance, target_length = 0.0, 0.0\n\n  for index, cur_predict_seq in enumerate(predict_seq_list):\n    cur_target_seq = target_seq_list[index]\n    if eos_id in cur_target_seq:\n      target_seq_end = cur_target_seq.index(eos_id)\n      cur_target_seq = cur_target_seq[:target_seq_end]\n    if eos_id in cur_predict_seq:\n      predict_seq_end = cur_predict_seq.index(eos_id)\n      cur_predict_seq = cur_predict_seq[:predict_seq_end]\n\n    levenshtein_distance += levenshtein(cur_predict_seq, cur_target_seq)\n    target_length += len(cur_target_seq)\n\n  errs = levenshtein_distance / target_length\n  return errs\n\n\ndef levenshtein(short_seq, long_seq):\n  \'\'\' levenshtein distance\n    :param short_seq: (shape=[T], type=int), the shorter sequence\n    :param long_seq: (shape=[T], tpye=int), the longer sequence\n    return: (type=float), the levenshtein distance between short_seq and long_seq\n    \'\'\'\n  min_len, max_len = len(short_seq), len(long_seq)\n  if min_len > max_len:\n    short_seq, long_seq = long_seq, short_seq\n    min_len, max_len = max_len, min_len\n\n  current = list(range(min_len + 1))\n  for index in range(1, max_len + 1):\n    previous, current = current, [index] + [0] * min_len\n    for index_1 in range(1, min_len + 1):\n      add, delete = previous[index_1] + 1, current[index_1 - 1] + 1\n      change = previous[index_1 - 1]\n      # string index from zero, but editditance from one\n      if short_seq[index_1 - 1] != long_seq[index - 1]:\n        change += 1\n      current[index_1] = min(change, add, delete)\n  return current[min_len]\n'"
delta/utils/metrics/metric_utils_test.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' metrics utils unittest \'\'\'\nimport numpy as np\nimport delta.compat as tf\n\nfrom delta.utils.metrics import metric_utils\n\n\nclass MetricUtilsTest(tf.test.TestCase):\n  \'\'\' metrics utils unittest\'\'\'\n\n  def setUp(self):\n    super().setUp()\n\n  def tearDown(self):\n    \'\'\' tear down \'\'\'\n\n  #pylint: disable=invalid-name\n  def test_stats_confusion(self):\n    \'\'\' test stats of confusion matrix\'\'\'\n    confusion = np.array([[5, 1, 1, 0], [0, 10, 0, 1], [0, 0, 6, 1],\n                          [0, 0, 1, 6]])\n\n    tn, fp, fn, tp = metric_utils.stats_confusion(confusion)\n    self.assertEqual(len(tp), 4)\n    self.assertAllEqual(tn, [20, 11, 19, 19])\n    self.assertAllEqual(fp, [0, 1, 2, 2])\n    self.assertAllEqual(fn, [2, 1, 1, 1])\n    self.assertAllEqual(tp, [5, 10, 6, 6])\n\n  #pylint: disable=invalid-name\n  def test_f1_score(self):\n    \'\'\' test f1 score \'\'\'\n    confusion = np.array([[5, 1, 1, 0], [0, 10, 0, 1], [0, 0, 6, 1],\n                          [0, 0, 1, 6]])\n    tn, fp, fn, tp = metric_utils.stats_confusion(confusion)\n\n    f1 = metric_utils.f1_score(tn, fp, fn, tp)\n    self.assertAllClose(f1, [0.83333333, 0.90909091, 0.8, 0.8])\n\n  def test_token_error(self):\n    \'\'\' test token_error \'\'\'\n    seq_list_one = [[5, 1, 1, 1, 1], [5, 2, 6, 10, 2]]\n    seq_list_two = [[5, 2, 3, 1, 2], [5, 2, 6, 10, 2]]\n\n    token_errors = metric_utils.token_error(\n        seq_list_one, seq_list_two, eos_id=0)\n    self.assertAllClose(token_errors, 0.3)\n\n    seq_list_three = [[5, 1, 1, 1, 1, 0, 0], [5, 2, 6, 10, 2, 0]]\n    token_errors = metric_utils.token_error(\n        seq_list_three, seq_list_two, eos_id=0)\n    self.assertAllClose(token_errors, 0.3)\n\n    token_errors = metric_utils.token_error(\n        seq_list_two, seq_list_three, eos_id=0)\n    self.assertAllClose(token_errors, 0.3)\n\n  def test_levenshtein(self):\n    \'\'\' test levenshtein distance \'\'\'\n    seq_one = [5, 1, 1, 1, 0]\n    seq_two = [5, 2, 3, 0, 2]\n\n    levenshtein_distance = metric_utils.levenshtein(seq_one, seq_two)\n    self.assertAllClose(levenshtein_distance, 4)\n\n    #another case\n    seq_one = [5, 1, 1, 1, 0]\n    seq_two = [5, 1, 1, 1, 0]\n\n    levenshtein_distance = metric_utils.levenshtein(seq_one, seq_two)\n    self.assertAllClose(levenshtein_distance, 0)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
delta/utils/metrics/py_metrics.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' sklearn metrics \'\'\'\nimport os\nimport abc\nfrom absl import logging\nimport numpy as np\nfrom sklearn import metrics\nfrom seqeval.metrics import classification_report as seq_classification_report\nfrom delta.utils.register import registers\nfrom delta.utils.postprocess.postprocess_utils import ids_to_sentences\nfrom rouge import FilesRouge\nfrom sacrebleu import corpus_bleu\nfrom delta.utils.metrics import metric_utils\n\n\n#pylint: disable=too-few-public-methods\nclass ABCMetric(metaclass=abc.ABCMeta):\n  \'\'\' abstract class of metric \'\'\'\n\n  @abc.abstractmethod\n  def call(self, y_true=None, y_pred=None, arguments=None):\n    \'\'\' compute entrypoint \'\'\'\n    raise NotImplementedError(""Calling an abstract method."")\n\n\n# pylint: disable=abstract-method\nclass Metric(ABCMetric):\n  \'\'\' wapper of metric \'\'\'\n\n  def __init__(self, config):\n    self.config = config\n    self.pos_label = config[\'pos_label\']\n\n  def __call__(self, *args, **kwargs):\n    return self.call(*args, **kwargs)\n\n\n@registers.metric.register\nclass AccuracyCal(Metric):\n  \'\'\' accuracy metric \'\'\'\n\n  # pylint: disable=useless-super-delegation\n  def __init__(self, config):\n    super().__init__(config)\n\n  def call(self, y_true=None, y_pred=None, arguments=None):\n    \'\'\' compute metric \'\'\'\n    return metrics.accuracy_score(y_true, y_pred, normalize=True)\n\n\n@registers.metric.register\nclass F1ScoreCal(Metric):\n  \'\'\' F1 score \'\'\'\n\n  # pylint: disable=useless-super-delegation\n  def __init__(self, config):\n    super().__init__(config)\n\n  def call(self, y_true=None, y_pred=None, arguments=None):\n    \'\'\' compute metric \'\'\'\n    average = arguments[\'average\'].lower()\n    return metrics.f1_score(y_true, y_pred, average=average)\n\n\n@registers.metric.register\nclass PrecisionCal(Metric):\n  \'\'\' precision metric \'\'\'\n\n  # pylint: disable=useless-super-delegation\n  def __init__(self, config):\n    super().__init__(config)\n\n  def call(self, y_true=None, y_pred=None, arguments=None):\n    \'\'\' compute metric \'\'\'\n    average = arguments[\'average\'].lower()\n    return metrics.precision_score(y_true, y_pred, average=average)\n\n\n@registers.metric.register\nclass RecallCal(Metric):\n  \'\'\' recall metric \'\'\'\n\n  # pylint: disable=useless-super-delegation\n  def __init__(self, config):\n    super().__init__(config)\n\n  # pylint: disable=too-many-locals\n  def call(self, y_true=None, y_pred=None, arguments=None):\n    \'\'\' compute metric \'\'\'\n    average = arguments[\'average\'].lower()\n    return metrics.recall_score(y_true, y_pred, average=average)\n\n\n@registers.metric.register\nclass ConfusionMatrixCal(Metric):\n  \'\'\' confusion matrix \'\'\'\n\n  # pylint: disable=useless-super-delegation\n  def __init__(self, config):\n    super().__init__(config)\n\n  def call(self, y_true=None, y_pred=None, arguments=None):\n    \'\'\' compute metric \'\'\'\n    del arguments\n    return metrics.confusion_matrix(y_true, y_pred)\n\n\n@registers.metric.register\nclass ClassReportCal(Metric):\n  \'\'\' accuracy metric \'\'\'\n\n  # pylint: disable=useless-super-delegation\n  def __init__(self, config):\n    super().__init__(config)\n\n  def call(self, y_true=None, y_pred=None, arguments=None):\n    \'\'\' compute metric \'\'\'\n    return metrics.classification_report(y_true, y_pred)\n\n\n@registers.metric.register\nclass TokenErrCal(Metric):\n  \'\'\' token error \'\'\'\n\n  #pylint: disable=useless-super-delegation\n  def __init__(self, config):\n    super().__init__(config)\n\n  def call(self, y_true=None, y_pred=None, arguments=None):\n    \'\'\' compute metric \'\'\'\n    eos_id = arguments[\'eos_id\']\n    return metric_utils.token_error(\n        predict_seq_list=y_pred, target_seq_list=y_true, eos_id=eos_id)\n\n\n@registers.metric.register\nclass CrfCal(Metric):\n  \'\'\' crf(ner) metric \'\'\'\n\n  # pylint: disable=useless-super-delegation\n  def __init__(self, config):\n    super().__init__(config)\n\n  def call(self, y_true=None, y_pred=None, arguments=None):\n    \'\'\' compute metric \'\'\'\n\n    label_path_file = arguments[""label_vocab_path""]\n    return ""\\n"" + seq_classification_report(\n        ids_to_sentences(y_true, label_path_file),\n        ids_to_sentences(y_pred, label_path_file),\n        digits=4)\n\n\ndef run_metrics_for_one_output(metric_config, y_true=None, y_pred=None):\n  metrics_list_config = metric_config[\'cals\']\n  score = dict()\n  for one_metric in metrics_list_config:\n    metric_name = one_metric[\'name\']\n    calculator = registers.metric[metric_name](metric_config)\n    arguments = one_metric[\'arguments\']\n    metric_score = calculator(y_true=y_true, y_pred=y_pred, arguments=arguments)\n    score[metric_name] = metric_score\n  return score\n\n\n@registers.metric.register\nclass BleuCal(Metric):\n  \'\'\' rouge metric\'\'\'\n\n  def __init__(self, config):\n    super().__init__(config)\n    print(config.keys())\n    self.hyp_path = self.config[""res_file""]\n    self.tgt_paths = self.config[\'target_file\']\n\n  def call(self, y_true=None, y_pred=None, arguments=None):\n    with open(self.tgt_paths[0]) as ref, open(self.hyp_path) as hyp:\n      bleu = corpus_bleu(hyp, [ref])\n      return ""\\n bleu:"" + str(bleu.score)\n\n\n@registers.metric.register\nclass RougeCal(Metric):\n  \'\'\' rouge metric\'\'\'\n\n  def __init__(self, config):\n    super().__init__(config)\n    print(config.keys())\n    self.hyp_path = self.config[""res_file""]\n    self.ref_path = self.hyp_path + \'.gt\'\n    self.tgt_paths = self.config[\'target_file\']\n    self.label_path_file = self.config[""text_vocab""]\n    self.tgt_paths_after_pre_process = [\n        one_path + "".after"" for one_path in self.tgt_paths\n    ]\n\n  def call(self, y_true=None, y_pred=None, arguments=None):\n    ref_sents = []\n    for tgt_path in self.tgt_paths_after_pre_process:\n      with open(tgt_path, ""r"", encoding=\'utf8\') as tgt_f:\n        ref_sents.extend(tgt_f.readlines())\n    ref_sents = [sent.strip() for sent in ref_sents]\n\n    with open(self.ref_path, ""w"", encoding=""utf-8"") as in_f:\n      for ref_sent in ref_sents:\n        in_f.write(ref_sent)\n        in_f.write(""\\n"")\n\n    files_rouge = FilesRouge(self.hyp_path, self.ref_path)\n    scores = files_rouge.get_scores(avg=True)\n    return self.get_scores_output(scores)\n\n  @staticmethod\n  def get_scores_output(score_dict):\n    res = \'\\n\'\n    for rouge_mode in [\'rouge-1\', \'rouge-2\', \'rouge-l\']:\n      res += \'-\' * 30 + \'\\n\'\n      for metric in [\'f\', \'p\', \'r\']:\n        res += \'{}\\tAverage_{}:\\t{:.5f}\\n\'.format(\n            rouge_mode.upper(), metric.upper(), score_dict[rouge_mode][metric])\n    return res\n\n\ndef get_metrics(config, y_true=None, y_pred=None):\n  \'\'\' candies function of metrics\n      calc metrics through `y_true` and `y_pred` or\n      `confusion` will be deprecated\n  \'\'\'\n  metrics = config[\'solver\'][\'metrics\']\n  if isinstance(metrics, list):  # metrics for multi-outputs\n    if len(y_true) != len(y_pred) or len(metrics) != len(y_true):\n      raise ValueError(""Length of y_true, y_pred and metrics must be equal!"")\n    score = []\n    for i, metrics_one_output in enumerate(metrics):\n      score.append(\n          run_metrics_for_one_output(metrics_one_output, y_true[i], y_pred[i]))\n  else:\n    score = run_metrics_for_one_output(metrics, y_true, y_pred)\n  return score\n'"
delta/utils/metrics/py_metrics_test.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' python metric unittest \'\'\'\nimport os\nfrom pathlib import Path\nimport tempfile\nimport numpy as np\nimport delta.compat as tf\n\nfrom delta import utils\nfrom delta.utils import metrics\nfrom delta import PACKAGE_ROOT_DIR\n\n\n#pylint: disable=too-many-instance-attributes\nclass MetricTest(tf.test.TestCase):\n  \'\'\' python metrix unittest \'\'\'\n\n  def setUp(self):\n    super().setUp()\n    \'\'\' setup \'\'\'\n    package_root = Path(PACKAGE_ROOT_DIR)\n    self.config_file_crf = \\\n      package_root.joinpath(\'../egs/mock_text_seq_label_data/seq-label/v1/config/seq-label-mock.yml\')\n\n    self.conf_str = \'\'\'\n      solver:\n        metrics:\n          pos_label: 1 # int, same to sklearn\n          cals:\n          - name: AccuracyCal\n            arguments: null \n          - name: ConfusionMatrixCal\n            arguments: null\n          - name: PrecisionCal\n            arguments:\n              average: \'micro\'\n          - name: RecallCal\n            arguments:\n              average: \'micro\'\n          - name: F1ScoreCal\n            arguments:\n              average: \'micro\'\n    \'\'\'\n\n    self.conf_file = tempfile.mktemp(suffix=\'metric.yaml\')\n    with open(self.conf_file, \'w\', encoding=\'utf-8\') as f:  #pylint: disable=invalid-name\n      f.write(self.conf_str)\n\n    self.true_label = np.array([1, 1, 2, 3, 4, 6, 5])\n    self.pred1 = np.array([1, 1, 2, 3, 4, 6, 5])\n    self.pred2 = np.array([2, 2, 1, 1, 1, 1, 1])\n\n    # config for test token error metircs\n    self.token_conf_str = \'\'\'\n      solver:\n        metrics:\n          pos_label: 1 # int, same to sklearn\n          cals:\n          - name: TokenErrCal\n            arguments:\n              eos_id: 0\n    \'\'\'\n\n    self.token_conf_file = tempfile.mktemp(suffix=\'token.yaml\')\n    with open(self.token_conf_file, \'w\', encoding=\'utf-8\') as f:  #pylint: disable=invalid-name\n      f.write(self.token_conf_str)\n\n    self.token_true_label = [[1, 1, 1, 1], [1, 3, 4, 5]]\n    self.token_pred1 = [[1, 1, 1, 1], [1, 3, 4, 5]]\n    self.token_pred2 = [[1, 2, 2, 2], [1, 0, 0, 0]]\n\n  def tearDown(self):\n    \'\'\' tear down \'\'\'\n    if os.path.exists(self.conf_file):\n      os.unlink(self.conf_file)\n\n  def test_metric(self):\n    \'\'\' test get_metrics function \'\'\'\n    config = utils.load_config(self.conf_file)\n\n    metrics1 = metrics.get_metrics(\n        config, y_true=self.true_label, y_pred=self.pred1)\n    self.assertEqual(1.0, metrics1[\'AccuracyCal\'])\n    self.assertEqual(1.0, metrics1[\'PrecisionCal\'])\n    self.assertEqual(1.0, metrics1[\'RecallCal\'])\n    self.assertEqual(1.0, metrics1[\'F1ScoreCal\'])\n\n    metrics2 = metrics.get_metrics(\n        config, y_true=self.true_label, y_pred=self.pred2)\n    self.assertEqual(0.0, metrics2[\'AccuracyCal\'])\n    self.assertEqual(0.0, metrics2[\'PrecisionCal\'])\n    self.assertEqual(0.0, metrics2[\'RecallCal\'])\n    self.assertEqual(0.0, metrics2[\'F1ScoreCal\'])\n\n  def test_token_err(self):\n    \'\'\' test tooken error rate \'\'\'\n    config = utils.load_config(self.token_conf_file)\n\n    metrics1 = metrics.get_metrics(\n        config, y_true=self.token_true_label, y_pred=self.token_pred1)\n    self.assertEqual(0.0, metrics1[\'TokenErrCal\'])\n\n    metrics2 = metrics.get_metrics(\n        config, y_true=self.token_true_label, y_pred=self.token_pred2)\n    self.assertEqual(0.75, metrics2[\'TokenErrCal\'])\n\n  def test_crf_metrics(self):\n    \'\'\' test crf metrics \'\'\'\n    config = utils.load_config(self.config_file_crf)\n    metrics3 = metrics.get_metrics(\n        config, y_true=[self.true_label], y_pred=[self.pred1])\n    # metrics3: one string. Text summary of the precision, recall, F1 score for each class.\n    # res3 = metrics3[\'CrfCal\']\n    # print(res3)\n    # for i, s in enumerate(res3):\n    #   print(i, s)\n    self.assertEqual(\'1.0000\', metrics3[\'CrfCal\'][67:73])\n\n    metrics4 = metrics.get_metrics(\n        config, y_true=[self.true_label], y_pred=[self.pred2])\n    self.assertEqual(\'0.0000\', metrics4[\'CrfCal\'][67:73])\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
delta/utils/metrics/tf_metrics.py,11,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' tensorflow metcis utils\'\'\'\nimport delta.compat as tf\n\n\ndef accuracy(logits, labels):\n  \'\'\' accuracy candies\n  params:\n    logits: [B, ..., D]\n    labels: [B, ...]\n  return:\n    accuracy tensor\n  \'\'\'\n  with tf.name_scope(\'accuracy\'):\n    assert_rank = tf.assert_equal(tf.rank(logits), tf.rank(labels) + 1)\n    assert_shape = tf.assert_equal(tf.shape(logits)[:-1], tf.shape(labels))\n    with tf.control_dependencies([assert_rank, assert_shape]):\n      predictions = tf.argmax(logits, axis=-1, output_type=tf.int64)\n      labels = tf.cast(labels, tf.int64)\n      return tf.reduce_mean(\n          tf.cast(tf.equal(predictions, labels), dtype=tf.float32))\n\n\ndef confusion_matrix(logits, labels, num_class):\n  \'\'\' confusion matrix candies \'\'\'\n  return tf.confusion_matrix(\n      labels=tf.reshape(labels, [-1]),\n      predictions=tf.reshape(tf.argmax(logits, -1), [-1]),\n      num_classes=num_class)\n'"
delta/utils/metrics/tf_metrics_test.py,7,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' tf metrics utils unittest \'\'\'\nimport delta.compat as tf\n\nfrom delta.utils.metrics import tf_metrics\n\n\nclass TFMetricUtilsTest(tf.test.TestCase):\n  \'\'\' tf metrics utils unittest\'\'\'\n\n  def test_accuracy(self):\n    \'\'\' test accuracy\'\'\'\n    logits = tf.constant([[0.1, 0.2, 0.7], [0.5, 0.2, 0.3], [0.2, 0.2, 0.6]])\n    labels = tf.constant([2, 0, 1])\n    output = tf_metrics.accuracy(logits, labels)\n    self.assertAllClose(output, 0.6666667)\n\n  def test_confusion_matrix(self):\n    \'\'\'test confusion matrix\'\'\'\n    logits = tf.constant([[0.1, 0.2, 0.7], [0.5, 0.2, 0.3], [0.6, 0.1, 0.3],\n                          [0.2, 0.3, 0.5], [0.2, 0.5, 0.3], [0.2, 0.2, 0.6]])\n    labels = tf.constant([2, 0, 0, 2, 1, 1])\n    num_class = 3\n    output = tf_metrics.confusion_matrix(logits, labels, num_class)\n    output_true = tf.constant([[2, 0, 0], [0, 1, 1], [0, 0, 2]])\n    self.assertAllEqual(output, output_true)\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
delta/utils/optimizer/__init__.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' init of optimizer\'\'\'\nfrom .yellowfin import YFOptimizer\n'"
delta/utils/optimizer/yellowfin.py,114,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""\nYellowFin optimizer.\n\nYellowFin and the Art of Momentum Tuning\nhttps://arxiv.org/abs/1706.03471\n\nrepo: https://github.com/JianGoForIt/YellowFin\nlicense: Apache-2.0\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nimport delta.compat as tf\nfrom tensorflow.python.framework import ops\n\n# EPS for numerical stability\nEPS = 1e-6\nLARGE_FLOAT_VAL = 1e15\n\n\nclass YFOptimizer(object):\n  """"""\n  Optimizer that implements the YellowFin algorithm.\n\n  Implemented as a wrapper around tf.train.MomentumOptimizer\n  """"""\n  # Available gate_gradients values\n  GATE_NONE = tf.train.Optimizer.GATE_NONE\n  GATE_OP = tf.train.Optimizer.GATE_OP\n  GATE_GRAPH = tf.train.Optimizer.GATE_GRAPH\n\n  def __init__(self,\n               learning_rate=0.0001,\n               momentum=0.0,\n               clip_thresh=None,\n               beta=0.999,\n               curv_win_width=20,\n               zero_debias=True,\n               delta_mu=0.0,\n               sparsity_debias=False,\n               use_locking=False,\n               name=""YellowFin"",\n               use_nesterov=False,\n               use_unsmoothed_lr_mu=True,\n               h_max_log_smooth=True,\n               h_min_log_smooth=True,\n               use_adapt_grad_clip=True,\n               stat_protect_fac=100.0):\n    """"""\n    Construct a new YellowFin optimizer.\n\n    Args:\n      learning rate: Python scalar. The initial value of learning rate,\n        we use 1.0 in our paper.\n      momentum: Python scalar. The initial value of momentum, we use\n        0.0 in our paper.\n      clip_thresh: Python scalar. The cliping threshold for\n        `tf.clip_by_global_norm`. If None, no clipping will be used.\n      beta: Python scalar. The smoothing parameter for estimations.\n      curv_win_width: TODO\n      zero_debias: TODO\n      delta_mu: for extensions. Not necessary in the basic use.\n      sparsity_debias: Python boolean. Gradient norm and curvature are\n        biased to larger values when calculated with sparse gradient.\n        This is useful when the model is very sparse, e.g. LSTM with\n        word embedding. For non-sparse CNN, turning it off could\n        slightly accelerate the speed.\n      use_locking: If True, use locks for update operations.\n      name: Optional name prefix for the operations created when\n        applying gradients. Defaults to ""YellowFin"".\n      use_nesterov: If True, the underlying MomentumOptimizer uses Nesterov\n        Momentum. Set to False in the default YellowFin algorithm.\n\n    Notes:\n      `clip_thresh` is the threshold value on ||lr * gradient||\n      `delta_mu` can be a placeholder/variable/python scalar. Used for\n      additional momentum in situations such as asynchronous-parallel\n      training. The default is 0.0 for basic usage of the optimizer.\n\n    Other features:\n      If you want to manually control the learning rates,\n      `self.lr_factor` is an interface to the outside. It is a\n      multiplier for the internal learning rate in YellowFin. It is\n      helpful when you want to do additional hand tuning or some\n      decaying scheme for the internal learning rate. Example on using\n      `lr_factor` can be found here:\n      https://github.com/JianGoForIt/YellowFin/blob/master/char-rnn-tensorflow/train_YF.py#L140\n    """"""\n    self._lr = learning_rate\n    self._mu = momentum\n\n    self._lr_var = tf.Variable(\n        learning_rate, dtype=tf.float32, name=""YF_lr"", trainable=False)\n    self._mu_var = tf.Variable(\n        momentum, dtype=tf.float32, name=""YF_mu"", trainable=False)\n    # for step scheme or decaying scheme for the learning rates\n    self.lr_factor = tf.Variable(\n        1.0, dtype=tf.float32, name=""YF_lr_factor"", trainable=False)\n    if clip_thresh is not None:\n      self._clip_thresh_var = tf.Variable(\n          clip_thresh, dtype=tf.float32, name=""YF_clip_thresh"", trainable=False)\n    else:\n      self._clip_thresh_var = None\n\n    # the underlying momentum optimizer\n    self._optimizer = tf.train.MomentumOptimizer(self._lr_var * self.lr_factor,\n                                                 self._mu_var + delta_mu,\n                                                 use_locking, name,\n                                                 use_nesterov)\n\n    # moving average for statistics\n    self._beta = beta\n    self._moving_averager = None\n\n    # for global step counting\n    self._global_step = tf.Variable(0, trainable=False)\n\n    self._do_tune = tf.greater(self._global_step, tf.constant(0))\n\n    self._zero_debias = zero_debias\n    self._sparsity_debias = sparsity_debias\n\n    self._tvars = None\n\n    # for curvature range\n    self._curv_win_width = curv_win_width\n    self._curv_win = None\n\n    # option for using smoothed or unsmoothed lr and mu\n    self._use_unsmoothed_lr_mu = use_unsmoothed_lr_mu\n\n    # options for curvature envelop smoothing\n    self._h_max_log_smooth = h_max_log_smooth\n    self._h_min_log_smooth = h_min_log_smooth\n\n    # for adaptive gradient clipping\n    self._use_adapt_grad_clip = use_adapt_grad_clip\n    self._adapt_grad_clip_thresh = \\\n      tf.Variable(LARGE_FLOAT_VAL, dtype=tf.float32, trainable=False)\n    self._adapt_grad_clip_target_val = \\\n      tf.Variable(LARGE_FLOAT_VAL, dtype=tf.float32, trainable=False)\n\n    # prevent exploding gradient from ruining the statistics\n    self._stat_protect_fac = stat_protect_fac\n\n  def curvature_range(self):\n    # set up the curvature window\n    self._curv_win = tf.Variable(\n        np.zeros([\n            self._curv_win_width,\n        ]),\n        dtype=tf.float32,\n        name=""curv_win"",\n        trainable=False)\n    # we can use log smoothing for curvature range to follow trend faster\n    # self._curv_win = tf.scatter_update(\n    #   self._curv_win, self._global_step % self._curv_win_width,\n    #   tf.log(self._grad_norm_squared + EPS))\n    self._curv_win = tf.scatter_update(self._curv_win,\n                                       self._global_step % self._curv_win_width,\n                                       self._grad_norm_squared + EPS)\n    # note here the iterations start from iteration 0\n    valid_window = tf.slice(\n        self._curv_win, tf.constant([\n            0,\n        ]),\n        tf.expand_dims(\n            tf.minimum(\n                tf.constant(self._curv_win_width), self._global_step + 1),\n            dim=0))\n\n    if self._h_min_log_smooth:\n      self._h_min_t = tf.log(tf.reduce_min(valid_window) + EPS)\n    else:\n      self._h_min_t = tf.reduce_min(valid_window)\n    if self._h_max_log_smooth:\n      self._h_max_t = tf.log(tf.reduce_max(valid_window) + EPS)\n    else:\n      self._h_max_t = tf.reduce_max(valid_window)\n\n    curv_range_ops = []\n    with tf.control_dependencies([self._h_min_t, self._h_max_t]):\n      avg_op = self._moving_averager.apply([self._h_min_t, self._h_max_t])\n      with tf.control_dependencies([avg_op]):\n        if self._h_min_log_smooth:\n          self._h_min = tf.exp(\n              tf.identity(self._moving_averager.average(self._h_min_t)))\n        else:\n          self._h_min = \\\n            tf.identity(self._moving_averager.average(self._h_min_t))\n        if self._h_max_log_smooth:\n          self._h_max = tf.exp(\n              tf.identity(self._moving_averager.average(self._h_max_t)))\n        else:\n          self._h_max = \\\n            tf.identity(self._moving_averager.average(self._h_max_t))\n      if self._sparsity_debias:\n        self._h_min = self._h_min * self._sparsity_avg\n        self._h_max = self._h_max * self._sparsity_avg\n    curv_range_ops.append(avg_op)\n    return curv_range_ops\n\n  def grad_variance(self):\n    grad_var_ops = []\n    tensor_to_avg = []\n    for t, g in zip(self._tvars, self._grads):\n      if isinstance(g, ops.IndexedSlices):\n        tensor_to_avg.append(\n            tf.reshape(\n                tf.unsorted_segment_sum(g.values, g.indices, g.dense_shape[0]),\n                shape=t.get_shape()))\n      else:\n        tensor_to_avg.append(g)\n    avg_op = self._moving_averager.apply(tensor_to_avg)\n    grad_var_ops.append(avg_op)\n    with tf.control_dependencies([avg_op]):\n      self._grad_avg = [\n          self._moving_averager.average(val) for val in tensor_to_avg\n      ]\n      self._grad_avg_squared = [tf.square(val) for val in self._grad_avg]\n    self._grad_var = tf.maximum(\n        tf.constant(EPS, dtype=self._grad_norm_squared_avg.dtype),\n        self._grad_norm_squared_avg -\n        tf.add_n([tf.reduce_sum(val) for val in self._grad_avg_squared]))\n    if self._sparsity_debias:\n      self._grad_var *= self._sparsity_avg\n    return grad_var_ops\n\n  def dist_to_opt(self):\n    dist_to_opt_ops = []\n    # running average of the norm of gradeint\n    self._grad_norm = tf.sqrt(self._grad_norm_squared)\n    avg_op = self._moving_averager.apply([\n        self._grad_norm,\n    ])\n    dist_to_opt_ops.append(avg_op)\n    with tf.control_dependencies([avg_op]):\n      self._grad_norm_avg = self._moving_averager.average(self._grad_norm)\n      # single iteration distance estimation\n      # note that self._grad_norm_avg is per variable\n      self._dist_to_opt = (\n          self._grad_norm_avg / (self._grad_norm_squared_avg + EPS))\n    # running average of distance\n    avg_op = self._moving_averager.apply([self._dist_to_opt])\n    dist_to_opt_ops.append(avg_op)\n    with tf.control_dependencies([avg_op]):\n      self._dist_to_opt_avg = tf.identity(\n          self._moving_averager.average(self._dist_to_opt))\n      if self._sparsity_debias:\n        self._dist_to_opt_avg /= (tf.sqrt(self._sparsity_avg) + EPS)\n    return dist_to_opt_ops\n\n  def grad_sparsity(self):\n    # If the sparse minibatch gradient has 10 percent of its entries\n    # non-zero, its sparsity is 0.1.\n    # The norm of dense gradient averaged from full dataset\n    # are roughly estimated norm of minibatch\n    # sparse gradient norm * sqrt(sparsity)\n    # An extension maybe only correct the sparse blob.\n    non_zero_cnt = tf.add_n([tf.count_nonzero(g) for g in self._grads])\n    all_entry_cnt = tf.add_n([tf.size(g) for g in self._grads])\n    self._sparsity = tf.cast(non_zero_cnt, self._grads[0].dtype) \\\n      / tf.cast(all_entry_cnt, self._grads[0].dtype)\n    avg_op = self._moving_averager.apply([\n        self._sparsity,\n    ])\n    with tf.control_dependencies([avg_op]):\n      self._sparsity_avg = self._moving_averager.average(self._sparsity)\n    return avg_op\n\n  def before_apply(self):\n    self._moving_averager = tf.train.ExponentialMovingAverage(\n        decay=self._beta, zero_debias=self._zero_debias)\n    assert self._grads is not None and len(self._grads) > 0\n    before_apply_ops = []\n\n    # get per var g**2 and norm**2\n    self._grad_squared = []\n    self._grad_norm_squared = []\n    for v, g in zip(self._tvars, self._grads):\n      if g is None:\n        continue\n      with ops.colocate_with(v):\n        self._grad_squared.append(tf.square(g))\n    self._grad_norm_squared = [\n        tf.reduce_sum(grad_squared) for grad_squared in self._grad_squared\n    ]\n\n    if self._sparsity_debias:\n      avg_op_sparsity = self.grad_sparsity()\n      before_apply_ops.append(avg_op_sparsity)\n\n    # the following running average on squared norm of gradient is shared\n    # by `grad_variance` and `dist_to_opt`\n    avg_op = self._moving_averager.apply(self._grad_norm_squared)\n    with tf.control_dependencies([avg_op]):\n      self._grad_norm_squared_avg = [\n          self._moving_averager.average(val) for val in self._grad_norm_squared\n      ]\n      self._grad_norm_squared = tf.add_n(self._grad_norm_squared)\n      self._grad_norm_squared_avg = tf.add_n(self._grad_norm_squared_avg)\n    before_apply_ops.append(avg_op)\n\n    with tf.control_dependencies([avg_op]):\n      curv_range_ops = self.curvature_range()\n      before_apply_ops += curv_range_ops\n      grad_var_ops = self.grad_variance()\n      before_apply_ops += grad_var_ops\n      dist_to_opt_ops = self.dist_to_opt()\n      before_apply_ops += dist_to_opt_ops\n    return tf.group(*before_apply_ops)\n\n  def get_lr_tensor(self):\n    lr = (1.0 - tf.sqrt(self._mu))**2 / (self._h_min + EPS)\n    lr = tf.minimum(\n        lr,\n        lr * (tf.to_float(self._global_step) + 1.0) / 10.0 /\n        tf.to_float(tf.constant(self._curv_win_width)))\n    return lr\n\n  def get_cubic_root(self):\n    # We have the equation x^2 D^2 + (1-x)^4 * C / h_min^2\n    # where x = sqrt(mu).\n    # We substitute x, which is sqrt(mu), with x = y + 1.\n    # It gives y^3 + py = q\n    # where p = (D^2 h_min^2)/(2*C) and q = -p.\n    # We use the Vieta\'s substution to compute the root.\n    # There is only one real solution y (which is in [0, 1] ).\n    # http://mathworld.wolfram.com/VietasSubstitution.html\n    # assert_array = \\\n    #   [tf.Assert(tf.logical_not(tf.is_nan(self._dist_to_opt_avg) ), [self._dist_to_opt_avg,]),\n    #   tf.Assert(tf.logical_not(tf.is_nan(self._h_min) ), [self._h_min,]),\n    #   tf.Assert(tf.logical_not(tf.is_nan(self._grad_var) ), [self._grad_var,]),\n    #   tf.Assert(tf.logical_not(tf.is_inf(self._dist_to_opt_avg) ), [self._dist_to_opt_avg,]),\n    #   tf.Assert(tf.logical_not(tf.is_inf(self._h_min) ), [self._h_min,]),\n    #   tf.Assert(tf.logical_not(tf.is_inf(self._grad_var) ), [self._grad_var,])]\n    # with tf.control_dependencies(assert_array):\n    # EPS in the numerator to prevent momentum being exactly one in case of 0 gradient\n    p = (self._dist_to_opt_avg + EPS)**2 * (self._h_min + EPS)**2 / 2 / (\n        self._grad_var + EPS)\n    w3 = (-tf.sqrt(p**2 + 4.0 / 27.0 * p**3) - p) / 2.0\n    w = tf.sign(w3) * tf.pow(tf.abs(w3), 1.0 / 3.0)\n    y = w - p / 3.0 / (w + EPS)\n    x = y + 1\n    return x\n\n  def get_mu_tensor(self):\n    root = self.get_cubic_root()\n    dr = tf.maximum((self._h_max + EPS) / (self._h_min + EPS), 1.0 + EPS)\n    mu = tf.maximum(root**2, ((tf.sqrt(dr) - 1) / (tf.sqrt(dr) + 1))**2)\n    return mu\n\n  def update_hyper_param(self):\n    assign_hyper_ops = []\n    self._mu = tf.identity(\n        tf.cond(self._do_tune, lambda: self.get_mu_tensor(),\n                lambda: self._mu_var))\n    with tf.control_dependencies([self._mu]):\n      self._lr = tf.identity(\n          tf.cond(self._do_tune, lambda: self.get_lr_tensor(),\n                  lambda: self._lr_var))\n\n    with tf.control_dependencies([self._mu, self._lr]):\n      if self._use_unsmoothed_lr_mu:\n        assign_hyper_ops.append(tf.assign(self._mu_var, self._mu))\n        assign_hyper_ops.append(tf.assign(self._lr_var, self._lr))\n      else:\n        self._mu = self._beta * self._mu_var + (1 - self._beta) * self._mu\n        self._lr = self._beta * self._lr_var + (1 - self._beta) * self._lr\n        with tf.control_dependencies([self._mu, self._lr]):\n          assign_hyper_ops.append(tf.assign(self._mu_var, self._mu))\n          assign_hyper_ops.append(tf.assign(self._lr_var, self._lr))\n    assign_hyper_op = tf.group(*assign_hyper_ops)\n    return assign_hyper_op\n\n  def get_name(self):\n    return self._optimizer.get_name()\n\n  def apply_gradients(self, grads_tvars, global_step=None, name=None):\n    self._grads, self._tvars = zip(\n        *[(g, t) for g, t in grads_tvars if g is not None])\n\n    # for manual gradient clipping\n    if self._clip_thresh_var is not None:\n      self._grads, self._grads_norm = tf.clip_by_global_norm(\n          self._grads, self._clip_thresh_var)\n\n    # loosely adaptive clipping of gradient in case exploding gradient ruins statistics\n    if self._use_adapt_grad_clip:\n      thresh = tf.cond(\n          self._do_tune, lambda: tf.sqrt(self._stat_protect_fac * self.\n                                         _adapt_grad_clip_thresh**2),\n          lambda: tf.to_float(tf.constant(LARGE_FLOAT_VAL)))\n      self._grads, self._grads_norm = tf.clip_by_global_norm(\n          self._grads, thresh)\n\n    with tf.variable_scope(""before_apply""):\n      before_apply_op = self.before_apply()\n\n    with tf.variable_scope(""update_hyper""):\n      with tf.control_dependencies([before_apply_op]):\n        update_hyper_op = self.update_hyper_param()\n\n    with tf.variable_scope(""apply_updates""):\n      with tf.control_dependencies([update_hyper_op]):\n\n        # clip exploding gradient according to h_max\n        if self._use_adapt_grad_clip:\n          thresh = tf.cond(\n              tf.greater(\n                  tf.global_norm(self._grads), self._adapt_grad_clip_thresh),\n              lambda: self._adapt_grad_clip_target_val,\n              lambda: tf.to_float(tf.constant(LARGE_FLOAT_VAL)))\n          self._grads, self._grads_norm = tf.clip_by_global_norm(\n              self._grads, thresh)\n\n        apply_grad_op = self._optimizer.apply_gradients(\n            zip(self._grads, self._tvars), global_step, name)\n\n    with tf.control_dependencies([apply_grad_op]):\n      self._increment_global_step_op = tf.assign(self._global_step,\n                                                 self._global_step + 1)\n\n      self._adapt_grad_clip_thresh_op = \\\n        tf.assign(self._adapt_grad_clip_thresh, tf.sqrt(self._h_max) )\n      self._adapt_grad_clip_target_val_op = \\\n        tf.assign(self._adapt_grad_clip_target_val, tf.sqrt(self._h_max) )\n      # self._adapt_grad_clip_target_val_op = \\\n      #   tf.assign(self._adapt_grad_clip_target_val, tf.sqrt(tf.sqrt(self._h_max * self._h_min)))\n\n    return tf.group(before_apply_op, update_hyper_op, apply_grad_op,\n                    self._adapt_grad_clip_thresh_op,\n                    self._adapt_grad_clip_target_val_op,\n                    self._increment_global_step_op)\n\n  def compute_gradients(self,\n                        loss,\n                        var_list=None,\n                        gate_gradients=GATE_OP,\n                        aggregation_method=None,\n                        colocate_gradients_with_ops=False,\n                        grad_loss=None):\n    return self._optimizer.compute_gradients(\n        loss,\n        var_list=var_list,\n        gate_gradients=gate_gradients,\n        aggregation_method=aggregation_method,\n        colocate_gradients_with_ops=colocate_gradients_with_ops,\n        grad_loss=grad_loss)\n\n  def minimize(self,\n               loss,\n               global_step=None,\n               var_list=None,\n               gate_gradients=GATE_OP,\n               aggregation_method=None,\n               colocate_gradients_with_ops=False,\n               name=None,\n               grad_loss=None):\n    """"""Add operations to minimize `loss` by updating `var_list`.\n\n    This method simply combines calls `compute_gradients()` and\n    `apply_gradients()`. If you want to process the gradient before\n    applying them, call `tf.gradients()` and `self.apply_gradients()`\n    explicitly instead of using this function.\n\n    Adapted from Tensorflow Optimizer base class member function.\n    """"""\n    grads_and_vars = self._optimizer.compute_gradients(\n        loss,\n        var_list=var_list,\n        gate_gradients=gate_gradients,\n        aggregation_method=aggregation_method,\n        colocate_gradients_with_ops=colocate_gradients_with_ops,\n        grad_loss=grad_loss)\n\n    vars_with_grad = [v for g, v in grads_and_vars if g is not None]\n    if not vars_with_grad:\n      raise ValueError(\n          ""No gradients provided for any variable, check your graph for ""\n          ""ops that do not support gradients, between variables ""\n          ""%s and loss %s."" % ([str(v) for _, v in grads_and_vars], loss))\n\n    return self.apply_gradients(grads_and_vars, global_step, name)\n\n  def get_slot(self, var, name):\n    """"""\n    Return a slot named `name` created for `var` by\n    the underlying MomentumOptimizer.\n\n    Args:\n      var: A variable passed to `minimize()` or `apply_gradients()`.\n      name: A string.\n\n    Returns:\n      The `Variable` for the slot if it was created, `None` otherwise.\n    """"""\n    return self._optimizer.get_slot(var, name)\n\n  def get_slot_names(self):\n    """"""\n    Return a list of the names of the slots created by the\n    underlying MomentumOptimizer.\n\n    Returns:\n      A list of strings.\n    """"""\n    return self._optimizer.get_slot_names()\n'"
delta/utils/postprocess/__init__.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' init of postprocess\'\'\'\n'"
delta/utils/postprocess/base_postproc.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' postprocess abstract class \'\'\'\nimport abc\n\n\n#pylint: disable=too-few-public-methods\nclass PostProcABC(metaclass=abc.ABCMeta):\n  \'\'\' postprocess abstract class\'\'\'\n\n  def __init__(self, config):\n    pass\n\n  @abc.abstractmethod\n  def call(self):\n    \'\'\' implementation func \'\'\'\n    raise NotImplementedError()\n\n\n#pylint: disable=abstract-method\nclass PostProc(PostProcABC):\n  \'\'\' base class of postprocess class\'\'\'\n\n  def __init__(self, config):\n    super().__init__(config)\n    self.config = config\n\n  def __call__(self, *args, **kwargs):\n    return self.call(*args, **kwargs)\n'"
delta/utils/postprocess/postprocess_utils.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' postprocess utils \'\'\'\nfrom absl import logging\nfrom delta.data.preprocess.utils import load_vocab_dict\n\n\ndef ids_to_sentences(ids, vocab_file_path):\n  """"""\n  transform array of numbers to array of tags/words\n  ids:  [[1,2],[3,4]...]\n  """"""\n\n  vocab_dict = load_vocab_dict(vocab_file_path)\n  id_to_vocab = {int(v): k for k, v in vocab_dict.items()}\n\n  sentences = []\n  for sent in ids:\n    sent_char = []\n    for s_char in sent:\n      if s_char not in id_to_vocab:\n        logging.error(""label not in vocabs"")\n      else:\n        sent_char.append(id_to_vocab[s_char])\n    sentences.append(sent_char)\n  assert len(sentences) == len(ids)\n  return sentences\n'"
delta/utils/postprocess/postprocess_utils_test.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' metrics utils unittest \'\'\'\nimport os\nfrom pathlib import Path\nimport delta.compat as tf\nfrom delta import utils\nfrom delta.utils.postprocess.postprocess_utils import ids_to_sentences\nfrom delta import PACKAGE_ROOT_DIR\n\n\nclass PostprocessUtilsTest(tf.test.TestCase):\n  \'\'\' metrics utils unittest\'\'\'\n\n  def setUp(self):\n    super().setUp()\n    package_root = Path(PACKAGE_ROOT_DIR)\n    self.config_file = package_root.joinpath(\n        \'../egs/mock_text_seq_label_data/seq-label/v1/config/seq-label-mock.yml\'\n    )\n\n  def tearDown(self):\n    \'\'\' tear down \'\'\'\n\n  def test_ids_to_sentences(self):\n    \'\'\' test ids_to_sentences function \'\'\'\n    config = utils.load_config(self.config_file)\n    ids = [[2, 3, 1]]\n    vocab_file_path = config[""data""][""task""][""label_vocab""]\n    sents = ids_to_sentences(ids, vocab_file_path)\n    self.assertAllEqual(sents, [[""I-PER"", ""B-LOC"", ""B-PER""]])\n\n\nif __name__ == ""__main__"":\n  tf.test.main()\n'"
delta/utils/postprocess/speaker_cls_proc.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Stub post processing for speaker tasks. \'\'\'\nimport os\nimport collections\nimport numpy as np\nfrom absl import logging\nfrom kaldiio import WriteHelper\n\nfrom delta.utils.postprocess.base_postproc import PostProc\nfrom delta.utils.register import registers\n\n#pylint: disable=too-many-instance-attributes\n#pylint: disable=too-many-locals\n#pylint: disable=too-many-nested-blocks\n#pylint: disable=too-many-branches\n#pylint: disable=too-few-public-methods\n\n\ndef format_kaldi_vector(vector):\n  \'\'\' Print a vector in Kaldi format. \'\'\'\n  return \'[ \' + \' \'.join([str(val) for val in vector]) + \' ]\'\n\n\n@registers.postprocess.register\nclass SpeakerPostProc(PostProc):\n  \'\'\' Apply speaker embedding extraction on hidden layer outputs. \'\'\'\n\n  def __init__(self, config):\n    super().__init__(config)\n\n    postconf = self.config[\'solver\'][\'postproc\']\n\n    output_dir = postconf[\'pred_path\']\n    self.output_dir = output_dir if output_dir else os.path.join(\n        self.config[\'solver\'][\'saver\'][\'model_path\'], \'infer\')\n    if not os.path.exists(self.output_dir):\n      os.makedirs(self.output_dir)\n\n    self.log_verbose = postconf[\'log_verbose\']\n\n    self.eval = postconf[\'eval\']\n    self.infer = postconf[\'infer\']\n\n    self.stats = None\n    self.confusion = None\n\n    # e.g.[""embeddings"", ""softmax""]\n    self.outputs = postconf[\'output_nodes\']\n    assert \'embeddings\' in self.outputs\n\n    self.output_files = collections.defaultdict(dict)\n\n    # e.g. [""utt"", ""chunk""]\n    self.output_levels = postconf[\'output_levels\']\n    assert \'utt\' in self.output_levels\n\n    for output_level in self.output_levels:\n      for output_key in self.outputs:\n        output_file_name = \'%s_%s\' % (output_level, output_key)\n        self.output_files[output_level][output_key] = \\\n            os.path.join(self.output_dir, output_file_name)\n\n    logging.info(f""save to: {self.output_files}"")\n    self.pred_metrics_path = os.path.join(self.output_dir, \'metrics.txt\')\n\n  # pylint: disable=arguments-differ\n  def call(self, predictions, log_verbose=False):\n    \'\'\' Implementation of postprocessing. \'\'\'\n\n    num_clips_processed = 0\n    last_utt_key = None\n    last_utt_chunk_outputs = {}\n    for output_key in self.outputs:\n      last_utt_chunk_outputs[output_key] = []\n\n    if self.infer:\n      file_pointers = collections.defaultdict(dict)\n      for output_level in self.output_levels:\n        for output_key in self.outputs:\n          file_pointers[output_level][output_key] = \\\n              open(self.output_files[output_level][output_key] + \'.txt\', \'w\')\n\n    for batch_index, batch in enumerate(predictions):\n      # batch = {\'inputs\': [clip_0, clip_1, ...],\n      #          \'labels\': [clip_0, clip_1, ...],\n      #          \'embeddings\': [clip_0, clip_1, ...],\n      #          ...}\n      # Now we extract each clip from the minibatch.\n      clips = collections.defaultdict(dict)\n      for key, batch_values in batch.items():\n        for clip_index, clip_data in enumerate(batch_values):\n          clips[clip_index][key] = clip_data\n\n      for clip_index, clip in sorted(clips.items()):\n        if log_verbose or self.log_verbose:\n          logging.debug(clip)\n        chunk_key = clip[\'filepath\'].decode()\n        utt_key, utt_chunk_index_str = chunk_key.rsplit(\'_\', 1)\n        utt_chunk_index = int(utt_chunk_index_str[-2:])\n        utt_chunk_index_from_clip_id = clip[\'clipid\']\n        assert utt_chunk_index == utt_chunk_index_from_clip_id\n\n        for output_key in self.outputs:\n          chunk_output = clip[output_key]\n          if self.infer:\n            formatted_output = format_kaldi_vector(chunk_output)\n            if \'chunk\' in self.output_levels:\n              file_pointers[\'chunk\'][output_key].write(\n                  \'%s %s\\n\' % (chunk_key, formatted_output))\n\n          embeddings = last_utt_chunk_outputs[output_key]\n          # Check if an utterance is over.\n          if utt_key != last_utt_key:\n            if last_utt_key is not None:\n              # Average over all chunks.\n              logging.debug(\'Utt %s: averaging ""%s"" over %d chunks\' %\n                            (last_utt_key, output_key, len(embeddings)))\n              utt_embedding = np.average(embeddings, axis=0)\n              if self.infer:\n                formatted_output = format_kaldi_vector(utt_embedding)\n                if \'utt\' in self.output_levels:\n                  file_pointers[\'utt\'][output_key].write(\n                      \'%s %s\\n\' % (last_utt_key, formatted_output))\n\n            # Start a new utterance.\n            embeddings.clear()\n          embeddings.append(chunk_output)\n        last_utt_key = utt_key\n\n      num_clips_processed += len(clips)\n      if (batch_index + 1) % 10 == 0:\n        logging.info(\'Processed %d batches, %d clips.\' %\n                     (batch_index + 1, num_clips_processed))\n\n    # Average over all chunks for the last utterance.\n    # TODO: reusability\n    for output_key in self.outputs:\n      embeddings = last_utt_chunk_outputs[output_key]\n      # Average over all chunks.\n      logging.debug(\'Utt %s: averaging ""%s"" over %d chunks\' %\n                    (last_utt_key, output_key, len(embeddings)))\n      utt_embedding = np.average(embeddings, axis=0)\n      if self.infer:\n        formatted_output = format_kaldi_vector(utt_embedding)\n        if \'utt\' in self.output_levels:\n          file_pointers[\'utt\'][output_key].write(\n              \'%s %s\\n\' % (last_utt_key, formatted_output))\n\n    if self.infer:\n      for output_level in self.output_levels:\n        for output_key in self.outputs:\n          file_pointers[output_level][output_key].close()\n\n    logging.info(\'Postprocessing completed.\')\n\n\n@registers.postprocess.register\nclass SpkUttPostProc(SpeakerPostProc):\n  \'\'\' Apply speaker embedding extraction on hidden layer outputs. \'\'\'\n\n  def __init__(self, config):\n    super().__init__(config)\n\n  # pylint: disable=arguments-differ\n  def call(self, predictions, log_verbose=False):\n    \'\'\' Implementation of postprocessing. \'\'\'\n    if self.infer:\n      file_pointers = collections.defaultdict(dict)\n      for output_level in self.output_levels:\n        for output_key in self.outputs:\n          file_path = self.output_files[output_level][output_key]\n          file_pointers[output_level][output_key] = \\\n              WriteHelper(f""ark,t,scp:{file_path}.ark,{file_path}.scp"")\n\n    utt2clips = collections.defaultdict(list)\n    last_utt = None\n    num_clips_processed = 0\n\n    def _process_utt(utt):\n      num_clips = 0\n      chunks_out = collections.defaultdict(list)\n      for i, item in enumerate(utt2clips[utt]):\n        logging.debug(f""{utt} {item[0]} {utt}"")\n        num_clips += 1\n\n        for j, output_key in enumerate(self.outputs):\n          chunk_output = item[j + 1]  # offset 1 for first filed is clipid\n          chunks_out[output_key].append(chunk_output)\n          if self.infer:\n            chunk_key = utt.decode() + \'_\' + str(item[0])\n            if \'chunk\' in self.output_levels:\n              file_pointers[\'chunk\'][output_key](chunk_key, chunk_output)\n\n      utts_out = collections.defaultdict(lambda: np.zeros(\n          (None), dtype=np.float32))\n      for i, output_key in enumerate(self.outputs):\n        utt_output = np.mean(chunks_out[output_key], axis=0)\n        utts_out[output_key] = utt_output\n        utt_key = utt.decode()\n        if self.infer:\n          if \'utt\' in self.output_levels:\n            file_pointers[\'utt\'][output_key](utt_key, utt_output)\n      return num_clips\n\n    for batch_index, batch in enumerate(predictions):\n      # batch = {\'inputs\': [clip_0, clip_1, ...],\n      #          \'labels\': [clip_0, clip_1, ...],\n      #          \'embeddings\': [clip_0, clip_1, ...],\n      #          \'clipid\': [clip_0, clip_1, ...],\n      #          \'filepath\': [clip_0, clip_1, ...],\n      #          ...}\n      # Now we extract each clip from the minibatch.\n      logging.debug(\n          f""{batch_index} {batch.keys()} {batch[\'labels\']} {batch[\'clipid\']}"")\n      for i, utt in enumerate(batch[\'filepath\']):\n        if last_utt is None:\n          last_utt = utt\n\n        value = (batch[\'clipid\'][i],)\n        for key in self.outputs:\n          value += (batch[key][i],)  # utt -> (clipid, skpid, embeddings, ...)\n        utt2clips[utt].append(value)\n        logging.debug(f""utt2clips: {utt} {value[0]} {len(utt2clips[utt])}"")\n\n        if last_utt != utt:\n          num_clips_processed += _process_utt(last_utt)\n          last_utt = utt\n\n      if (batch_index + 1) % 10 == 0:\n        logging.info(\'Processed %d batches, %d clips.\' %\n                     (batch_index + 1, num_clips_processed))\n\n    # save last\n    num_clips_processed += _process_utt(last_utt)\n    logging.info(\'Processed %d clips.\' % (num_clips_processed))\n\n    # close files\n    if self.infer:\n      for output_level in self.output_levels:\n        for output_key in self.outputs:\n          file_pointers[output_level][output_key].close()\n\n    logging.info(\'Postprocessing completed.\')\n'"
delta/utils/postprocess/speech_cls_proc.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Speech Postprocess \'\'\'\nimport os\nimport pickle\nimport collections\nimport numpy as np\nfrom absl import logging\nfrom sklearn.metrics import confusion_matrix\n\nfrom delta import utils\nfrom delta.utils.postprocess.base_postproc import PostProc\nfrom delta.utils.register import registers\n\n\n#pylint: disable=too-many-instance-attributes\n@registers.postprocess.register\nclass EmoPostProc(PostProc):\n  \'\'\' emotion postprocess\'\'\'\n\n  def __init__(self, config):\n    super().__init__(config)\n\n    taskconf = self.config[\'data\'][\'task\']\n    self.positive_id = self.config[\'solver\'][\'metrics\'][\'pos_label\']\n    self.class_vocab = taskconf[\'classes\'][\'vocab\']\n    self.reverse_vocab = taskconf[\'classes\'][\'reverse_vocab\']\n    self.num_class = taskconf[\'classes\'][\'num\']\n\n    postconf = self.config[\'solver\'][\'postproc\']\n    output_dir = postconf[\'pred_path\']\n    self.output_dir = output_dir if output_dir else os.path.join(\n        self.config[\'solver\'][\'saver\'][\'model_path\'], \'infer\')\n    if not os.path.exists(self.output_dir):\n      os.makedirs(self.output_dir)\n\n    self.log_verbose = postconf[\'log_verbose\']\n\n    self.eval = postconf[\'eval\']\n    self.infer = postconf[\'infer\']\n\n    self.thresholds = postconf[\'thresholds\']\n    self.smoothing = postconf[\'smoothing\'][\'enable\']\n    self.smoothing_cnt = postconf[\'smoothing\'][\'count\']\n\n    if self.infer:\n      self.pred_score_path = os.path.join(self.output_dir, \'predict_scores.txt\')\n      self.pred_result_path = os.path.join(self.output_dir,\n                                           \'predict_results.pkl\')\n    if self.eval:\n      self.pred_metrics_path = os.path.join(self.output_dir, \'metrics.txt\')\n\n  #pylint: disable=no-self-use\n  def update_stats(self, prediction, stats):\n    \'\'\' update true label and pred label\'\'\'\n    y_true = prediction[\'labels\']\n    y_pred = np.argmax(prediction[\'softmax\'], -1)\n    stats[0] = np.append(stats[0], y_true)\n    stats[1] = np.append(stats[1], y_pred)\n    return stats\n\n  def update_metrics(self, prediction, confusion):\n    \'\'\' update confusion\'\'\'\n    conf = confusion_matrix(\n        y_true=prediction[\'labels\'],\n        y_pred=np.argmax(prediction[\'softmax\'], -1),\n        labels=list(range(self.num_class)))\n    if conf.shape != (self.num_class, self.num_class):\n      raise ValueError(\'Warning: confusion matrix shape error, shape {}\'.format(\n          conf.shape))\n    confusion += conf\n    return confusion\n\n  def log_metrics(self, stats):\n    \'\'\' compute metrics\'\'\'\n    socres = utils.metrics.get_metrics(\n        self.config, y_true=stats[0], y_pred=stats[1])\n    with open(self.pred_metrics_path, \'w\') as f:  #pylint: disable=invalid-name\n      for key, val in socres.items():\n        logging.info(""{}: {}"".format(key, val))\n        f.write(""{}: {}\\n"".format(key, val))\n\n  def collect_results(self, prediction, result, scorefile):\n    \'\'\' collect scores and meta of results\'\'\'\n    for filepath, clipid, label, score in zip(prediction[\'filepath\'],\n                                              prediction[\'clipid\'],\n                                              prediction[\'labels\'],\n                                              prediction[\'softmax\']):\n\n      # record predict score for ROC curve\n      scorefile.write(""{}, {}, {}, {}\\n"".format(filepath, clipid, label,\n                                                score[self.positive_id]))\n      result[filepath].append({\n          ""clipid"": clipid,\n          \'label\': label,\n          \'softmax\': score\n      })\n\n  #pylint: disable=too-many-locals,too-many-locals\n  def post_proc_results(self, results, thresholds=None):\n    \'\'\' smoothing score and predict label \'\'\'\n    thresholds = thresholds or np.linspace(0, 1, num=10, endpoint=False)\n    for threshold in thresholds:\n      output_path = os.path.join(self.output_dir,\n                                 \'predict_ths_%3f.txt\' % (threshold))\n      with open(output_path, \'w\') as predfile:\n        # history len `maxlen`\n        smoothing = self.smoothing\n        maxlen = self.smoothing_cnt\n        history = collections.deque(maxlen=maxlen)\n        vote_all = collections.deque(maxlen=maxlen)\n\n        for path in results:  # filepath\n          # sort file clips by `clipid`\n          clips = results[path]\n          clips.sort(key=lambda entry: entry[\'clipid\'])\n          pos_cnt = 0\n\n          for elem in clips:  # clipid\n            clipid = elem[\'clipid\']\n            label = elem[\'label\']\n            score = elem[\'softmax\']\n\n            # append history deque\n            y_pos = score[self.positive_id]\n            history.append(y_pos)\n\n            hist = list(history)\n            if smoothing:\n              # smoothing\n              y_pos = np.mean(hist)\n            else:\n              pass\n\n            # predict result for file\n            if y_pos > threshold:\n              y_pred = self.positive_id\n            else:\n              y_pred = 1 - self.positive_id\n\n            if y_pred == self.positive_id:\n              pos_cnt += 1\n              vote_all.append(True)\n            logging.info(\'file {}, clipid {}, label {}, pred {}\'.format(\n                path, clipid, label, y_pred))\n\n          #sentence_pred = pos_cnt >= FLAGS.threshold\n          sentence_pred = all(list(vote_all))\n          sentence_label = self.positive_id if \'conflict\' in str(path) else (\n              1 - self.positive_id)\n\n          logging.info(""file ths_{}: {} {} {}"".format(threshold, path, pos_cnt,\n                                                      sentence_pred))\n\n          predfile.write(""{} {} {}\\n"".format(path, sentence_label, pos_cnt))\n\n  #pylint: disable=arguments-differ\n  def call(self, predictions, log_verbose=False):\n    \'\'\' main func entrypoint\'\'\'\n    # [true_label, pred_label]\n    stats = [np.array([]), np.array([])]\n    confusion = np.zeros((self.num_class, self.num_class), dtype=np.int32)\n\n    pred_results = collections.defaultdict(list)\n\n    self.pred_score_path = os.path.join(self.output_dir, \'predict_scores.txt\')\n    self.pred_result_path = os.path.join(self.output_dir, \'predict_results.pkl\')\n\n    with open(self.pred_score_path, \'w\') as score_file, \\\n      open(self.pred_result_path, \'wb\') as result_file:\n      for i, pred in enumerate(predictions):\n        if log_verbose or self.log_verbose:\n          logging.info(\'index: {} pred: {}\'.format(i, pred))\n          for key, val in pred.items():\n            logging.info(""{}: val: {} type: {}"".format(key, val, type(val)))\n\n        if self.eval:\n          confusion = self.update_metrics(pred, confusion)\n          stats = self.update_stats(pred, stats)\n\n        if self.infer:\n          self.collect_results(pred, pred_results, score_file)\n\n      if self.infer:\n        pickle.dump(pred_results, result_file)\n\n    if self.eval:\n      self.log_metrics(stats)\n\n    if self.infer:\n      self.post_proc_results(pred_results, self.thresholds)\n'"
delta/utils/postprocess/text_cls_proc.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Speech Postprocess \'\'\'\nimport os\nfrom absl import logging\n\nfrom delta.utils.postprocess.base_postproc import PostProc\nfrom delta.utils.register import registers\n\n\n#pylint: disable=too-many-instance-attributes, too-few-public-methods\n@registers.postprocess.register\nclass SavePredPostProc(PostProc):\n  \'\'\'Save the result of inference.\'\'\'\n\n  #pylint: disable=arguments-differ, unused-argument\n  def call(self, predictions, log_verbose=False):\n    \'\'\' main func entrypoint\'\'\'\n    logits = predictions[""logits""]\n    preds = predictions[""preds""]\n    output_index = predictions[""output_index""]\n    if output_index is None:\n      res_file = self.config[""solver""][""postproc""].get(""res_file"", """")\n    else:\n      res_file = self.config[""solver""][""postproc""][output_index].get(\n          ""res_file"", """")\n    if res_file == """":\n      logging.info(\n          ""Infer res not saved. You can check \'res_file\' in your config."")\n      return\n    res_dir = os.path.dirname(res_file)\n    if not os.path.exists(res_dir):\n      os.makedirs(res_dir)\n    logging.info(""Save inference result to: {}"".format(res_file))\n    with open(res_file, ""w"") as in_f:\n      for logit, pred in zip(logits, preds):\n        in_f.write("" "".join([""{:.3f}"".format(num) for num in logit]) +\n                   ""\\t{}\\n"".format(pred))\n'"
delta/utils/postprocess/text_seq2seq_proc.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Text Sequence to Sequence Postprocess \'\'\'\nimport os\nimport numpy as np\nfrom absl import logging\nfrom delta.utils.postprocess.base_postproc import PostProc\nfrom delta.utils.register import registers\nfrom delta.utils.postprocess.postprocess_utils import ids_to_sentences\n\n#pylint: disable=too-many-instance-attributes, too-few-public-methods, too-many-locals\n\n\n@registers.postprocess.register\nclass SavePredSeqPostProc(PostProc):\n  \'\'\'Save the result of inference.\'\'\'\n\n  #pylint: disable=arguments-differ, unused-argument\n  def call(self, predictions, log_verbose=False):\n    \'\'\' main func entrypoint\'\'\'\n    preds = predictions[""preds""]\n\n    res_file = self.config[""solver""][""postproc""].get(""res_file"", """")\n    if res_file == """":\n      logging.info(\n          ""Infer res not saved. You can check \'res_file\' in your config."")\n      return\n    res_dir = os.path.dirname(res_file)\n    if not os.path.exists(res_dir):\n      os.makedirs(res_dir)\n    logging.info(""Save inference result to: {}"".format(res_file))\n    self.task_config = self.config[\'data\'][\'task\']\n    self.label_vocab_file_paths = self.task_config[\'label_vocab\']\n    if not isinstance(self.label_vocab_file_paths, list):\n      self.label_vocab_file_paths = [self.label_vocab_file_paths]\n    self.use_label_vocab = self.task_config[\'use_label_vocab\']\n    if self.use_label_vocab:\n      label_path_file = self.label_vocab_file_paths[0]\n    else:\n      label_path_file = self.task_config[""text_vocab""]\n    preds = ids_to_sentences(preds, label_path_file)\n    with open(res_file, ""w"", encoding=""utf-8"") as in_f:\n      for i, pre in enumerate(preds):\n        while len(pre) > 1 and pre[-1] in [\'<unk>\', \'<pad>\', \'<eos>\']:\n          pre.pop()\n        pred_abs = \' \'.join(pre)\n        in_f.write(pred_abs)\n        in_f.write(""\\n"")\n'"
delta/utils/postprocess/text_seq_label_proc.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Speech Postprocess \'\'\'\nimport os\nfrom absl import logging\nfrom seqeval.metrics.sequence_labeling import get_entities\nfrom delta.utils.postprocess.base_postproc import PostProc\nfrom delta.utils.register import registers\nfrom delta.utils.postprocess.postprocess_utils import ids_to_sentences\n\n\n#pylint: disable=too-many-instance-attributes, too-few-public-methods, too-many-locals\n@registers.postprocess.register\nclass SavePredEntityPostProc(PostProc):\n  \'\'\'Save the result of inference.\'\'\'\n\n  #pylint: disable=arguments-differ, unused-argument\n  def call(self, predictions, log_verbose=False):\n    \'\'\' main func entrypoint\'\'\'\n    preds = predictions[""preds""]\n    output_index = predictions[""output_index""]\n    if output_index is None:\n      res_file = self.config[""solver""][""postproc""].get(""res_file"", """")\n      label_path_file = self.config[""data""][""task""][""label_vocab""]\n    else:\n      res_file = self.config[""solver""][""postproc""][output_index].get(\n          ""res_file"", """")\n      label_path_file = self.config[""data""][""task""][""label_vocab""][output_index]\n\n    if res_file == """":\n      logging.info(\n          ""Infer res not saved. You can check \'res_file\' in your config."")\n      return\n    res_dir = os.path.dirname(res_file)\n    if not os.path.exists(res_dir):\n      os.makedirs(res_dir)\n    logging.info(""Save inference result to: {}"".format(res_file))\n\n    preds = ids_to_sentences(preds, label_path_file)\n\n    with open(res_file, ""w"", encoding=""utf-8"") as in_f:\n      for i, pre in enumerate(preds):\n        entities = get_entities(pre)  # [(\'PER\', 0, 1), (\'LOC\', 3, 3)]\n        if not entities:\n          in_f.write(""Null"")\n        else:\n          new_line = ""\\t"".join(\n              ["" "".join(map(str, entity)) for entity in entities])\n          in_f.write(new_line)\n        in_f.write(""\\n"")\n'"
delta/utils/solver/__init__.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Solvers.""""""\n'"
delta/utils/solver/asr_solver.py,10,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' asr sovler based on Solver\'\'\'\n\nfrom pathlib import Path\nfrom datetime import datetime\n\nfrom absl import logging\nimport delta.compat as tf\n\n#pylint: disable=import-error\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Lambda\nfrom tensorflow.keras.experimental import export_saved_model\n\nfrom delta import utils\nfrom delta.utils.decode import py_ctc\nfrom delta.utils import metrics as metrics_lib\nfrom delta.utils.solver.keras_base_solver import KerasBaseSolver\nfrom delta.utils.register import registers\nfrom delta.utils.solver.utils.callbacks import TokenErrMetricCallBack\nfrom delta.utils.decode.tf_ctc import ctc_greedy_decode\n\n\n#pylint: disable=too-many-instance-attributes,too-many-public-methods\n@registers.solver.register\nclass AsrSolver(KerasBaseSolver):\n  \'\'\' asr keras solver\'\'\'\n\n  def __init__(self, config):\n    super().__init__(config)\n\n  def input_fn(self, mode):\n    \'\'\' input function for tf.data.Dataset\'\'\'\n    super().input_fn(mode)\n    assert self.task\n    self.batch_input_shape = self.task.batch_input_shape()\n    batch_size = self.config[\'solver\'][\'optimizer\'][\'batch_size\']\n    num_epoch = self.config[\'solver\'][\'optimizer\'][\'epochs\']\n    return self.task.input_fn(mode, batch_size, num_epoch), self.task\n\n  def input_data(self, mode):\n    \'\'\' input data \'\'\'\n    input_fn, _task = self.input_fn(mode)\n    ds_ = input_fn()\n    #iterator = ds_.make_one_shot_iterator()\n    #return iterator, task\n    return ds_, _task\n\n  #pylint: disable=no-self-use\n  def get_loss(self):\n    \'\'\' dummy ctc loss, since ctc is implemented as a kearas layer \'\'\'\n    loss = {\'ctc\': lambda y_true, y_pred: tf.reduce_mean(y_pred)}\n    return loss\n\n  def get_metric_callbacks(self, eval_gen, eval_task, monitor_used,\n                           decoder_type):\n    \'\'\' metric_specific callbacks\'\'\'\n    callbacks = []\n\n    if monitor_used == \'val_token_err\':\n      metric_func = self.get_metric_func()\n      metric_cal = TokenErrMetricCallBack(metric_func, eval_gen, eval_task,\n                                          decoder_type)\n      callbacks.append(metric_cal)\n\n    logging.info(f""CallBack: Val Metric on {monitor_used}"")\n    return callbacks\n\n  def get_callbacks(self,\n                    eval_ds,\n                    eval_task,\n                    monitor_used=\'val_acc\',\n                    decoder_type=\'argmax\'):\n    \'\'\' callbacks for traning, metrics callbacks must be first, then misc callbacks\'\'\'\n    callbacks = self.get_metric_callbacks(eval_ds, eval_task, monitor_used,\n                                          decoder_type)\n    misc_cbs = super().get_callbacks(monitor_used)\n    callbacks.extend(misc_cbs)\n    return callbacks\n\n  def save_model(self):\n    \'\'\' save keras model \'\'\'\n    if self._model_path:\n      save_model = self._model_path + str(\'/final_model.h5\')\n      self.model.save(save_model)\n      logging.info(""Model saved: {}"".format(save_model))\n\n  def train(self):\n    \'\'\' only train \'\'\'\n    _, train_task = self.input_data(mode=utils.TRAIN)\n    self.model_fn(mode=utils.TRAIN)\n\n    callbacks = self.get_misc_callbacks(monitor_used=\'loss\')\n\n    self.active_model.fit_generator(\n        train_task,\n        steps_per_epoch=len(train_task),\n        epochs=self._num_epochs,\n        verbose=1,\n        callbacks=callbacks,\n        max_queue_size=20,\n        workers=1,\n        use_multiprocessing=False,\n        shuffle=True,\n        initial_epoch=self._init_epoch)\n\n  def get_metric_func(self):\n    \'\'\' build metric function \'\'\'\n    _input_data = self.model.get_layer(\'inputs\').input\n    y_pred = self.model.get_layer(\'ctc\').input[0]\n    metric_func = K.function([_input_data], [y_pred])\n    return metric_func\n\n  #pylint: disable=too-many-locals\n  def eval(self):\n    \'\'\' only eval\'\'\'\n    #get eval dataset\n    # data must be init before model build\n    logging.info(""make Task"")\n    eval_ds, eval_task = self.input_data(mode=utils.EVAL)\n    eval_gen = tf.data.make_one_shot_iterator(eval_ds)\n\n    logging.info(""build Model"")\n    #get eval model\n    self.model_fn(mode=utils.EVAL)\n    assert self._built\n\n    #load model\n    eval_func = self.get_metric_func()\n\n    target_seq_list, predict_seq_list = [], []\n    for _ in range(len(eval_task)):\n      batch_data = tf.keras.backend.get_session().run(eval_gen.get_next()[0])\n\n      batch_input = batch_data[\'inputs\']\n      batch_target = batch_data[\'targets\'].tolist()\n\n      batch_predict = eval_func(batch_input)[0]\n\n      batch_decode = py_ctc.ctc_greedy_decode(batch_predict, 0, unique=True)\n\n      target_seq_list += batch_target\n      predict_seq_list += batch_decode\n\n    token_errors = metrics_lib.token_error(\n        predict_seq_list=predict_seq_list,\n        target_seq_list=target_seq_list,\n        eos_id=0)\n    logging.info(""eval finish!"")\n    logging.info(""Token Error: {}"".format(token_errors))\n\n  def train_and_eval(self):\n    \'\'\' train and eval \'\'\'\n    # data must be init before model builg\n    #backend_sess = K.get_session()\n    train_ds, train_task = self.input_data(mode=utils.TRAIN)\n    #train_gen = self.input_generator(tf.data.make_one_shot_iterator(train_ds), train_task, backend_sess, mode=utils.TRAIN)\n    eval_ds, eval_task = self.input_data(mode=utils.EVAL)\n    #eval_gen = self.input_generator(tf.data.make_one_shot_iterator(eval_ds), eval_task, backend_sess, mode=utils.EVAL)\n\n    self.model_fn(mode=utils.TRAIN)\n    assert self._built\n\n    callbacks = self.get_callbacks(\n        eval_ds, eval_task, monitor_used=self._monitor_used)\n\n    try:\n      # Run training\n      self.active_model.fit_generator(\n          train_task,\n          steps_per_epoch=len(train_task),\n          epochs=self._num_epochs,\n          verbose=1,\n          callbacks=callbacks,\n          validation_data=eval_task,\n          validation_steps=len(eval_task),\n          validation_freq=1,\n          class_weight=None,\n          max_queue_size=100,\n          workers=4,\n          use_multiprocessing=False,\n          shuffle=True,\n          initial_epoch=self._init_epoch)\n      #save model\n      # not work for subclassed model, using tf.keras.experimental.export_saved_model\n      #self.save_model()\n\n    except (Exception, ArithmeticError) as err:  #pylint: disable=broad-except\n      template = ""An exception of type {0} occurred. Arguments:\\n{1!r}""\n      message = template.format(type(err).__name__, err.args)\n      logging.error(message)\n      raise err\n\n    finally:\n      # Clear memory\n      K.clear_session()\n      logging.info(""Ending time: {}"".format(\n          datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\')))\n\n  #pylint: disable=unused-argument,too-many-locals\n  def infer(self, yield_single_examples=False):\n    \'\'\' only for infer \'\'\'\n    #load data\n    mode = utils.INFER\n    # data must be init before model build\n    infer_ds, infer_task = self.input_data(mode=mode)\n    infer_gen = tf.data.make_one_shot_iterator(infer_ds)\n\n    self.model_fn(mode=mode)\n    assert self._built\n\n    #load model\n    infer_func = self.get_metric_func()\n\n    for _ in range(len(infer_task)):\n      batch_data = tf.keras.backend.get_session().run(infer_gen.get_next()[0])\n      batch_input = batch_data[\'inputs\']\n      batch_uttid = batch_data[\'uttids\'].tolist()\n      batch_predict = infer_func(batch_input)[0]\n      batch_decode = py_ctc.ctc_greedy_decode(batch_predict, 0, unique=True)\n      for utt_index, uttid in enumerate(batch_uttid):\n        logging.info(""utt ID: {}"".format(uttid))\n        logging.info(""infer result: {}"".format(batch_decode[utt_index]))\n\n  def export_model(self):\n    \'\'\'export saved_model\'\'\'\n    mode = utils.INFER\n    self.model_fn(mode=mode)\n    assert self._built\n\n    input_feat = self.model.get_layer(\'inputs\').input\n    input_length = self.model.get_layer(\'input_length\').input\n\n    def ctc_greedy_decode_lambda_func(args):\n      y_pred, input_length = args\n      input_length = tf.cast(input_length, dtype=tf.int32)\n      decode_result, _ = ctc_greedy_decode(\n          logits=y_pred,\n          sequence_length=input_length,\n          merge_repeated=True,\n          blank_id=None)\n      return decode_result\n\n    model_outputs = self.model.get_layer(\'outputs\').output\n    greedy_decode = Lambda(\n        ctc_greedy_decode_lambda_func, output_shape=(),\n        name=\'decode\')([model_outputs, input_length])\n\n    model_to_export = Model(\n        inputs=[input_feat, input_length], outputs=greedy_decode)\n\n    model_export_path = Path(self._model_path).joinpath(""export"")\n    export_saved_model(\n        model=model_to_export,\n        saved_model_path=str(model_export_path),\n        custom_objects=None,\n        as_text=False,\n        input_signature=None,\n        serving_only=False)\n'"
delta/utils/solver/base_solver.py,40,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Base classes of solver.""""""\n\nimport abc\nimport math\nimport delta.compat as tf\nimport tensorflow_addons as tfa\nfrom absl import logging\n\nfrom delta import utils\nfrom delta.utils import optimizer\nfrom delta.utils.register import registers\n\n# pylint: disable=abstract-method\n\n\nclass ABCSolver(metaclass=abc.ABCMeta):\n  """"""Abstract class of solver.""""""\n\n  @abc.abstractmethod\n  def process_config(self, config):\n    """"""Process the configs.""""""\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def input_fn(self, mode):\n    """"""Get the input function.""""""\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def model_fn(self):\n    """"""Get the model function.""""""\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def get_loss_fn(self):\n    """"""Get the loss function.""""""\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def get_learning_rate(self):\n    """"""Get the learning rate.""""""\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def get_optimizer(self):\n    """"""Get the optimizer.""""""\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def get_apply_gradients_op(self):\n    """"""Get the apply gradients operator.""""""\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def get_train_op(self):\n    """"""Get the training operator.""""""\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def get_saver(self):\n    """"""Get the saver.""""""\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def get_scaffold(self):\n    """"""Get the scaffold.""""""\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def train(self):\n    """"""Train the model.""""""\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def eval(self):\n    """"""Evaluate the model.""""""\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def infer(self):\n    """"""Make a inference.""""""\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def train_and_eval(self):\n    """"""Train and evaluate.""""""\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def export_model(self):\n    """"""Export model to tensorflow SavedModel.""""""\n    raise NotImplementedError()\n\n\nclass Solver(ABCSolver):\n  """"""Base class of solver.""""""\n\n  def __init__(self, config):\n    super().__init__()\n    self._config = self.process_config(config)\n    self._task = None\n\n  @property\n  def config(self):\n    """"""Get the config.""""""\n    return self._config\n\n  def input_fn(self, mode):\n    """"""Get the input function.\n    return a Task class\n    """"""\n    task_name = self.config[\'data\'][\'task\'][""name""]\n    self._task = registers.task[task_name](self.config, mode)\n    return self._task\n\n  @property\n  def task(self):\n    """"""Get the task.""""""\n    return self._task\n\n  def model_fn(self):\n    \'\'\' return Model class \'\'\'\n    classname = self.config[\'model\'][\'name\']\n    logging.info(""__name__=%s\\tclassname==%s"", __name__, classname)\n\n    # Model initialization\n    model = registers.model[classname](self.config)\n    return model\n\n  def get_loss_fn(self):\n    """"""Get the loss function.""""""\n    return utils.misc.losses(self.config)\n\n  def get_learning_rate(self):\n    """"""Get the learning rate.""""""\n    lrconf = self.config[\'solver\'][\'optimizer\'][\'learning_rate\']\n    learning_rate = lrconf[\'rate\']\n    learning_type = lrconf[\'type\']\n\n    #pylint: disable=invalid-name\n    if learning_type == \'exp_decay\':\n      lr = tf.train.exponential_decay(\n          learning_rate,\n          tf.train.get_or_create_global_step(),\n          lrconf[\'decay_steps\'],\n          lrconf[\'decay_rate\'],\n          staircase=True)\n    elif learning_type == \'piecewise\':\n      #boundaries = [15000, 30000]\n      #values = [1e-3, 1e-4, 1e-5]\n      boundaries = lrconf[\'boundaries\']\n      values = lrconf[\'values\']\n      assert len(values) == len(\n          boundaries) + 1, \'values len must equal boundaries len plus one\'\n      lr = tf.train.piecewise_constant(\n          tf.train.get_or_create_global_step(),\n          boundaries=boundaries,\n          values=values)\n    elif learning_type == \'warmup\':\n      learning_rate = tf.constant(\n          value=learning_rate, shape=[], dtype=tf.float32)\n      global_step = tf.train.get_or_create_global_step()\n      data_size = self.config[\'data\'][\'train_data_size\']\n      num_epochs = self.config[""data""][""task""][\'epochs\']\n      batch_size = self.config[""data""][""task""][\'batch_size\']\n      num_batch = int(math.ceil(data_size * num_epochs / batch_size))\n      learning_rate = tf.train.polynomial_decay(\n          learning_rate,\n          global_step,\n          num_batch,\n          end_learning_rate=0.0,\n          power=1.0,\n          cycle=False)\n      global_steps_int = tf.cast(global_step, tf.int32)\n      warmup_steps_int = tf.constant(lrconf[\'num_warmup_steps\'], dtype=tf.int32)\n\n      global_steps_float = tf.cast(global_steps_int, tf.float32)\n      warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n\n      warmup_percent_done = global_steps_float / warmup_steps_float\n      warmup_learning_rate = learning_rate * warmup_percent_done\n\n      is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n      lr = ((1.0 - is_warmup) * learning_rate +\n            is_warmup * warmup_learning_rate)\n    elif learning_type == \'const\':\n      lr = learning_rate\n    else:\n      raise ValueError(\n          ""Not support learning rate type: {}"".format(learning_type))\n    tf.summary.scalar(\'lr\', lr)\n    return lr\n\n  #pylint: disable=arguments-differ\n  def get_optimizer(self):\n    """"""Get the optimizer.""""""\n    optconf = self.config[\'solver\'][\'optimizer\']\n    method = optconf[\'name\']\n    learning_rate = self.get_learning_rate()\n    if method == \'adadelta\':\n      opt = tf.train.AdadeltaOptimizer(learning_rate)\n    elif method == \'adam\':\n      opt = tf.train.AdamOptimizer(learning_rate)\n    elif method == \'adagrad\':\n      opt = tf.train.AdagradOptimizer(learning_rate)\n    elif method == \'momentum\':\n      opt = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n    elif method == \'rmsprop\':\n      opt = tf.train.RMSPropOptimizer(learning_rate)\n    elif method == \'gradientdecent\':\n      opt = tf.train.GradientDescentOptimizer(learning_rate)\n    elif method == \'lazyadam\':\n      opt = tfa.optimizers.LazyAdam(learning_rate)\n    elif method == \'weightedadam\':\n      weight_decay = self.config[\'solver\'][\'optimizer\'][\'weight_decay\']\n      opt = tfa.optimizers.AdamW(\n          weight_decay=weight_decay, learning_rate=learning_rate)\n    elif method == \'yellowfin\':\n      opt = optimizer.YFOptimizer(learning_rate)\n    else:\n      raise ValueError(""Not support optimizer: {}"".format(method))\n\n    return opt\n\n  #pylint: disable=no-self-use\n  def clip_gradients(self, grads_and_vars, clip_ratio):\n    """"""Clip the gradients.""""""\n    is_zip_obj = False\n    if isinstance(grads_and_vars, zip):\n      grads_and_vars = list(grads_and_vars)\n      is_zip_obj = True\n\n    with tf.variable_scope(\'grad\'):\n      for grad, var in grads_and_vars:\n        if grad is not None:\n          tf.summary.histogram(var.name[:-2], grad)\n        else:\n          logging.debug(\'%s gradient is None\' % (var.name))\n\n    # not clip\n    if not clip_ratio:\n      if is_zip_obj:\n        grads, variables = zip(*grads_and_vars)\n        grads_and_vars = zip(grads, variables)\n      return grads_and_vars\n\n    gradients, variables = zip(*grads_and_vars)\n    clipped, global_norm = tf.clip_by_global_norm(gradients, clip_ratio)\n    grad_and_var_clipped = zip(clipped, variables)\n\n    tf.summary.scalar(\'gradient/global_norm\', global_norm)\n    return grad_and_var_clipped\n\n  def get_apply_gradients_op(self, loss, global_step=None):\n    """"""Get Apply gradients operator.""""""\n    opt = self.get_optimizer()\n    grads_and_vars = opt.compute_gradients(loss)\n\n    # clip gradient\n    optconf = self.config[\'solver\'][\'optimizer\']\n    global_norm = optconf[\'clip_global_norm\']\n    grads_and_vars = self.clip_gradients(grads_and_vars, global_norm)\n\n    apply_gradient_op = opt.apply_gradients(\n        grads_and_vars,\n        global_step=global_step or tf.train.get_or_create_global_step())\n    return apply_gradient_op\n\n  def get_var_avg_ema(self, decay, global_step=None):\n    \'\'\' make var average ema \'\'\'\n    return tf.train.ExponentialMovingAverage(\n        decay, global_step or tf.train.get_or_create_global_step())\n\n  def make_restore_average_vars_dict(self, global_step=None):\n    \'\'\' using vars_average to restotre vars\'\'\'\n    model_avg_conf = self.config[\'solver\'][\'model_average\']\n    var_avg_decay = model_avg_conf[\'var_avg_decay\']\n\n    var_restore_dict = {}\n    variable_averages = self.get_var_avg_ema(var_avg_decay, global_step)\n    for var in tf.global_variables():\n      if var in tf.trainable_variables():\n        name = variable_averages.average_name(var)\n      else:\n        name = var.op.name\n      var_restore_dict[name] = var\n    return var_restore_dict\n\n  def var_avg(self, global_step=None):\n    \'\'\' average model variables, add average_op to UPDATES_OPS\'\'\'\n    model_avg_conf = self.config[\'solver\'][\'model_average\']\n    var_avg_model = model_avg_conf[\'enable\']\n    if var_avg_model:\n      var_avg_decay = model_avg_conf[\'var_avg_decay\']\n      variable_averages = self.get_var_avg_ema(var_avg_decay, global_step)\n      apply_op = variable_averages.apply(tf.trainable_variables())\n      tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, apply_op)\n      utils.log_vars(\'Avg Trainable Vars\', tf.trainable_variables())\n\n  def get_train_op(self, loss, global_step=None):\n    """"""Get the training operator.""""""\n    apply_gradient_op = self.get_apply_gradients_op(loss, global_step)\n\n    # model average\n    self.var_avg(global_step)\n\n    # model average after apply gradients\n    with tf.control_dependencies([apply_gradient_op]):\n      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n      train_op = tf.group(*update_ops)\n\n    utils.log_vars(\'moving vars\', tf.moving_average_variables())\n    return train_op\n\n  def get_saver(self, global_step=None):\n    """"""Get the saver.""""""\n    solverconf = self.config[\'solver\']\n    max_to_keep = solverconf[\'saver\'][\'max_to_keep\']\n    model_avg_conf = self.config[\'solver\'][\'model_average\']\n    model_average = model_avg_conf[\'enable\']\n    if model_average:\n      var_avg_decay = model_avg_conf[\'var_avg_decay\']\n      variable_averages = self.get_var_avg_ema(var_avg_decay, global_step)\n      variable_to_restore = variable_averages.variables_to_restore()\n      logging.info(\'Restore: name to var : {}\'.format(variable_to_restore))\n      saver = tf.train.Saver(variable_to_restore, max_to_keep=max_to_keep)\n      logging.info(\'Restore vars from moving variables\')\n    else:\n      saver = tf.train.Saver(max_to_keep=max_to_keep)\n    return saver\n\n  def get_scaffold(self, mode, global_step=None):\n    """"""Get the scaffold.""""""\n    if mode != utils.TRAIN:\n      # for model average\n      saver = self.get_saver(global_step)\n      scaffold = tf.train.Scaffold(saver=saver)\n    else:\n      scaffold = None  # default\n    return scaffold\n\n\nclass ABCEstimatorSolver(Solver):\n  """"""Abstract solver using tensorflow Esitimator.""""""\n\n  @abc.abstractmethod\n  def create_estimator(self):\n    \'\'\' create tf.estimator.Estimator obj\'\'\'\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def get_train_hooks(self, labels, logits, alpha=None):\n    \'\'\' return train_hooks \'\'\'\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def get_eval_hooks(self, labels, logits):\n    \'\'\' return eval_hooks, eval_metric_ops \'\'\'\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def get_infer_predictions(self):\n    \'\'\' get infer predictions output\'\'\'\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def create_serving_input_receiver_fn(self):\n    \'\'\' input pipeline when export model \'\'\'\n    raise NotImplementedError()\n\n  @abc.abstractmethod\n  def postproc_fn(self):\n    \'\'\' postprocess of predictions\'\'\'\n    raise NotImplementedError()\n'"
delta/utils/solver/emotion_solver.py,6,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Speech Emotion Solver based on EstimatorSolver\'\'\'\nimport librosa\nfrom absl import logging\n\n#pylint: disable=no-name-in-module\nimport delta.compat as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.python.keras.utils import losses_utils\n\nfrom delta import utils\nfrom delta.utils.solver.estimator_solver import EstimatorSolver\nfrom delta.utils.solver.utils.callbacks import ClassReportCallBack\n#from delta.utils.solver.keras_base_solver import KerasBaseSolver\nfrom delta.utils.solver.asr_solver import AsrSolver\nfrom delta.utils.register import registers\n\n\n@registers.solver.register\nclass EmotionSolver(EstimatorSolver):\n  \'\'\' Speech Emotion Solver base on Estimator\'\'\'\n\n  #pylint: disable=useless-super-delegation\n  def __init__(self, config):\n    super().__init__(config)\n\n  def process_config(self, config):\n    \'\'\' preprocess config \'\'\'\n    data_conf = config[\'data\']\n    class_vocab = data_conf[\'task\'][\'classes\'][\'vocab\']\n    assert len(class_vocab) == data_conf[\'task\'][\'classes\'][\'num\']\n\n    # add revere_vocab, positive_id\n    reverse_vocab = {val: key for key, val in class_vocab.items()}\n    data_conf[\'task\'][\'classes\'][\'reverse_vocab\'] = reverse_vocab\n\n    # binary class\n    pos_id = config[\'solver\'][\'metrics\'][\'pos_label\']\n    data_conf[\'task\'][\'classes\'][\'positive_id\'] = pos_id\n    data_conf[\'task\'][\'classes\'][\'positive\'] = reverse_vocab[pos_id]\n\n    # add feature shape, withoud batch_size\n    if data_conf[\'task\'][\'suffix\'] == \'.npy\':\n      input_channels = 3 if data_conf[\'task\'][\'audio\'][\'add_delta_deltas\'] else 1\n      nframe = librosa.time_to_frames(\n          data_conf[\'task\'][\'audio\'][\'clip_size\'],\n          sr=data_conf[\'task\'][\'audio\'][\'sr\'],\n          hop_length=data_conf[\'task\'][\'audio\'][\'winstep\'] *\n          data_conf[\'task\'][\'audio\'][\'sr\'])\n      feature_shape = [\n          nframe, data_conf[\'task\'][\'audio\'][\'feature_size\'], input_channels\n      ]\n    else:\n      feature_shape = [\n          data_conf[\'task\'][\'audio\'][\'sr\'] *\n          data_conf[\'task\'][\'audio\'][\'clip_size\']\n      ]\n    data_conf[\'task\'][\'audio\'][\'feature_shape\'] = feature_shape\n    return config\n\n  def create_serving_input_receiver_fn(self):\n    \'\'\' infer input pipeline \'\'\'\n    # with batch_size\n    taskconf = self.config[\'data\'][\'task\']\n    shape = [None] + taskconf[\'audio\'][\'feature_shape\']\n    logging.debug(\'serving input shape:{}\'.format(shape))\n\n    #pylint: disable=no-member\n    return tf.estimator.export.build_raw_serving_input_receiver_fn(\n        features={\n            \'inputs\':\n                tf.placeholder(name=""inputs"", shape=shape, dtype=tf.float32),\n            \'texts\':\n                tf.placeholder(\n                    name=""texts"",\n                    shape=(None, taskconf[\'text\'][\'max_text_len\']),\n                    dtype=tf.int32)\n        },\n        default_batch_size=None,\n    )\n\n\n@registers.solver.register\nclass EmoKerasSolver(AsrSolver):\n  \'\'\' emotion keras solver \'\'\'\n\n  def __init__(self, config):\n    super().__init__(config)\n    self.batch_input_shape = None\n    self._label_smoothing = config[\'solver\'][\'optimizer\'][\'label_smoothing\']\n\n  @property\n  def model(self):\n    \'\'\' keras Model \'\'\'\n    return self.raw_model\n\n  def input_fn(self, mode):\n    \'\'\' input function for tf.data.Dataset\'\'\'\n    super().input_fn(mode)\n    assert self.task\n    self.batch_input_shape = self.task.batch_input_shape()\n    return None, self.task\n\n  def input_data(self, mode):\n    \'\'\' get input data \'\'\'\n    _, task = self.input_fn(mode)\n    assert self.task\n    return None, task\n\n  def get_loss(self):\n    \'\'\' keras losses  \'\'\'\n    return tf.keras.losses.CategoricalCrossentropy(\n        from_logits=True,\n        label_smoothing=self._label_smoothing,\n        reduction=losses_utils.ReductionV2.AUTO)\n\n  def eval(self):\n    \'\'\' evaluation \'\'\'\n    # must first construct input data, then build model\n    eval_ds, eval_task = self.input_data(mode=utils.EVAL)\n    self.model_fn(mode=utils.EVAL)\n    assert self._built\n\n    callbacks = []\n\n    self.active_model.evaluate_generator(\n        eval_task,\n        steps=len(eval_task),\n        verbose=1,\n        callbacks=callbacks,\n        max_queue_size=20,\n        workers=1,\n        use_multiprocessing=False)\n\n    logging.info(""Eval End."")\n\n  def infer(self, yield_single_examples=False):\n    \'\'\' inference \'\'\'\n    logging.fatal(""Not Implemented"")\n\n  def export_model(self):\n    logging.fatal(""Not Implemented"")\n\n  def get_metric_callbacks(self, eval_gen, eval_task, monitor_used,\n                           decoder_type=None):\n    \'\'\' metric_specific callbacks\'\'\'\n    callbacks = []\n\n    if monitor_used == \'ClassReport\':\n      metric_func = self.model\n      metric_cal = ClassReportCallBack(metric_func, eval_gen, eval_task)\n      callbacks.append(metric_cal)\n\n    logging.info(f""CallBack: Val Metric on {monitor_used}"")\n    return callbacks\n\n'"
delta/utils/solver/estimator_solver.py,44,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Estimator base class for classfication \'\'\'\nimport os\nimport functools\nfrom absl import logging\nimport delta.compat as tf\nfrom tensorflow.python import debug as tf_debug  #pylint: disable=no-name-in-module\nfrom tensorflow.python.estimator.canned import metric_keys\n# See: tensorboard/tensorboard/plugins/pr_curve/README.md\nfrom tensorboard.plugins.pr_curve import summary as pr_summary\n\nfrom delta import utils\nfrom delta.utils.hparam import HParams\nfrom delta.utils import metrics as metrics_lib\nfrom delta.utils import summary as summary_lib\nfrom delta.utils.register import registers\nfrom delta.utils.solver.base_solver import ABCEstimatorSolver\n\n\n#pylint: disable=abstract-method\nclass EstimatorSolver(ABCEstimatorSolver):\n  \'\'\' base class for classfication \'\'\'\n\n  def __init__(self, config):\n    super().__init__(config)\n\n    saver_conf = config[\'solver\'][\'saver\']\n    self.eval_path = os.path.join(saver_conf[\'model_path\'], \'eval\')\n    if not os.path.exists(self.eval_path):\n      os.makedirs(self.eval_path)\n    self.eval_metrics_path = os.path.join(self.eval_path, \'metrics.txt\')\n\n  def input_fn(self, mode):\n    \'\'\' return input_fn \'\'\'\n    super().input_fn(mode)\n    batch_size = self.config[\'solver\'][\'optimizer\'][\'batch_size\']\n    num_epoch = self.config[\'solver\'][\'optimizer\'][\'epochs\']\n    return self.task.input_fn(mode, batch_size, num_epoch)\n\n  def get_scaffold(self, mode, global_step=None):\n    if mode != utils.TRAIN:\n      # for model average\n      saver = self.get_saver(global_step)\n      scaffold = tf.train.Scaffold(saver=saver)\n    else:\n      scaffold = None  # default\n    return scaffold\n\n  def l2_loss(self, tvars=None):\n    _l2_loss = 0.0\n    weight_decay = self.config[\'solver\'][\'optimizer\'].get(\'weight_decay\', None)\n    if weight_decay:\n      logging.info(f""add L2 Loss with decay: {weight_decay}"")\n      with tf.name_scope(\'l2_loss\'):\n        tvars = tvars if tvars else tf.trainable_variables()\n        tvars = [v for v in tvars if \'bias\' not in v.name]\n        _l2_loss = weight_decay * tf.add_n([tf.nn.l2_loss(v) for v in tvars])\n        summary_lib.scalar(\'l2_loss\', _l2_loss)\n    return _l2_loss\n\n  def model_fn(self):\n    \'\'\' return model_fn \'\'\'\n    model_class = super().model_fn()\n    model_name = self.config[\'model\'][\'name\']\n    model_type = self.config[\'model\'][\'type\']\n    logging.info(""Model: {}, {}"".format(model_type, model_name))\n\n    #pylint: disable=too-many-locals\n    def _model_fn(features, labels, mode, params):\n      del params\n      is_train = mode == utils.TRAIN\n\n      # Supports both dict output and legacy single logits output.\n      model_outputs = model_class(features, training=is_train)\n      if isinstance(model_outputs, dict):\n        logits = model_outputs[\'logits\']\n        extra_outputs = dict(model_outputs)\n        extra_outputs.pop(\'logits\')\n      else:\n        logits = model_outputs\n        extra_outputs = None\n\n      alignment = model_class.alphas if hasattr(model_class, \'alphas\') else None\n\n      if mode == utils.INFER:\n        softmax = tf.nn.softmax(logits, name=\'softmax_output\')\n        predictions = self.get_infer_predictions(\n            features, softmax, alpha=alignment, extra_outputs=extra_outputs)\n        return tf.estimator.EstimatorSpec( #pylint: disable=no-member\n            mode=mode,\n            predictions=predictions,\n            scaffold=self.get_scaffold(mode),\n            export_outputs={\n                \'predictions\': tf.estimator.export.PredictOutput(predictions) #pylint: disable=no-member\n            })\n\n      if \'soft_labels\' in features.keys():\n        soft_labels = features[\'soft_labels\']\n      else:\n        soft_labels = None\n\n      loss = self.get_loss_fn()(\n          labels=labels,\n          logits=logits,\n          soft_labels=soft_labels,\n          name=\'x_loss\',\n      )\n\n      if mode == utils.TRAIN:  #pylint: disable=no-else-return\n        if self.config[\'solver\'][\'adversarial\'][\'enable\']:\n          x = features[\'inputs\']  #pylint: disable=invalid-name\n          grad, = tf.gradients(loss, x)\n          x_adv = x + self.config[\'solver\'][\'adversarial\'][\n              \'adv_epslion\'] * tf.sign(grad)\n          x_adv = tf.stop_gradient(x_adv)\n          features_adv = {\'inputs\': x_adv, \'texts\': features[\'text\']}\n          logits_adv = model_class(features_adv)\n\n          loss_adv = self.get_loss_fn()(\n              labels=labels,\n              logits=logits_adv,\n              soft_labels=soft_labels,\n              name=\'x_adv_loss\',\n          )\n          adv_alpha = self.config[\'solver\'][\'adversarial\'][\'adv_alpha\']\n          loss_all = (1 - adv_alpha) * loss + adv_alpha * loss_adv\n        else:\n          loss_all = loss\n\n        # L2 loss\n        loss_all += self.l2_loss()\n\n        train_op = self.get_train_op(loss_all)\n        train_hooks = self.get_train_hooks(labels, logits, alpha=alignment)\n\n        utils.log_vars(\'Global Vars\', tf.global_variables())\n        return tf.estimator.EstimatorSpec(  #pylint: disable=no-member\n            mode=mode,\n            loss=loss_all,\n            train_op=train_op,\n            training_chief_hooks=train_hooks,\n            training_hooks=None,\n            scaffold=None,\n        )\n      else:  # eval\n        loss_all = loss\n        eval_hooks, eval_metrics_ops = self.get_eval_hooks(labels, logits)\n        return tf.estimator.EstimatorSpec(  #pylint: disable=no-member\n            mode=mode,\n            loss=loss_all,\n            eval_metric_ops=eval_metrics_ops,\n            evaluation_hooks=eval_hooks,\n            scaffold=self.get_scaffold(mode),\n        )\n\n    return _model_fn\n\n  def create_estimator(self):\n    # Set model params\n    model_params = HParams()\n\n    # create model func\n    model_fn = self.model_fn()\n\n    # multi-gpus\n    devices, num_gpu = utils.gpu_device_names()\n    distribution = utils.get_distribution_strategy(num_gpu)\n    logging.info(\'Device: {}/{}\'.format(num_gpu, devices))\n\n    # run config\n    tfconf = self.config[\'solver\'][\'run_config\']\n    saverconf = self.config[\'solver\'][\'saver\']\n    session_config = tf.ConfigProto(\n        allow_soft_placement=tfconf[\'allow_soft_placement\'],\n        log_device_placement=tfconf[\'log_device_placement\'],\n        intra_op_parallelism_threads=tfconf[\'intra_op_parallelism_threads\'],\n        inter_op_parallelism_threads=tfconf[\'inter_op_parallelism_threads\'],\n        gpu_options=tf.GPUOptions(allow_growth=tfconf[\'allow_growth\']))\n\n    run_config = tf.estimator.RunConfig(  #pylint: disable=no-member\n        tf_random_seed=tfconf[\'tf_random_seed\'],\n        session_config=session_config,\n        save_summary_steps=saverconf[\'save_summary_steps\'],\n        keep_checkpoint_max=saverconf[\'max_to_keep\'],\n        log_step_count_steps=tfconf[\'log_step_count_steps\'],\n        train_distribute=distribution,\n        device_fn=None,\n        protocol=None,\n        eval_distribute=None,\n        experimental_distribute=None,\n    )\n\n    # Instantiate Estimator\n    nn = tf.estimator.Estimator(  #pylint: disable=no-member,invalid-name\n        model_fn=model_fn,\n        model_dir=saverconf[\'model_path\'],\n        config=run_config,\n        params=model_params,\n        warm_start_from=None,\n    )\n    return nn\n\n  def get_train_hooks(self, labels, logits, alpha=None):\n    nclass = self.config[\'data\'][\'task\'][\'classes\'][\'num\']\n    metric_tensor = {\n        ""batch_accuracy"": metrics_lib.accuracy(logits, labels),\n        \'global_step\': tf.train.get_or_create_global_step(),\n    }\n    if nclass > 100:\n      logging.info(\'Too many classes, disable confusion matrix in train: %d\' %\n                   (nclass))\n    else:\n      metric_tensor[\'batch_confusion\'] = \\\n          metrics_lib.confusion_matrix(logits, labels, nclass)\n    summary_lib.scalar(\'batch_accuracy\', metric_tensor[\'batch_accuracy\'])\n    if alpha:\n      metric_tensor.update({""alignment"": alpha})\n\n    # plot PR curve\n    true_label_bool = tf.cast(labels, tf.bool)\n    softmax = tf.nn.softmax(logits)\n    pr_summary.op(\n        name=\'pr_curve_train_batch\',\n        labels=true_label_bool,\n        predictions=softmax[:, -1],\n        num_thresholds=16,\n        weights=None)\n\n    train_hooks = [\n        tf.train.StepCounterHook(\n            every_n_steps=100,\n            every_n_secs=None,\n            output_dir=None,\n            summary_writer=None),\n        tf.train.FinalOpsHook(\n            final_ops=[tf.train.get_or_create_global_step()],\n            final_ops_feed_dict=None),\n        tf.train.LoggingTensorHook(\n            tensors=metric_tensor,\n            every_n_iter=100,\n            every_n_secs=None,\n            at_end=False,\n            formatter=None),\n    ]\n    return train_hooks\n\n  def get_eval_hooks(self, labels, logits):\n    \'\'\' lables: [batch]\n        logits: [batch, num_classes]\n    \'\'\'\n    nclass = self.config[\'data\'][\'task\'][\'classes\'][\'num\']\n\n    eval_hooks = []\n    metric_tensor = {}\n    with tf.variable_scope(\'metrics\'):\n      true_label = labels\n      true_label_bool = tf.cast(labels, tf.bool)\n      softmax = tf.nn.softmax(logits)\n      pred_label = tf.argmax(softmax, -1)\n      eval_metrics_ops = {\n          \'accuracy\':\n              tf.metrics.accuracy(\n                  labels=true_label, predictions=pred_label, weights=None),\n      }\n      if nclass == 2:\n        eval_metrics_ops.update({\n            \'auc\':\n                tf.metrics.auc(\n                    labels=true_label,\n                    predictions=softmax[:, -1],\n                    num_thresholds=20,\n                    curve=\'ROC\',\n                    summation_method=\'trapezoidal\'),\n            \'precision\':\n                tf.metrics.precision(\n                    labels=true_label, predictions=pred_label, weights=None),\n            \'recall\':\n                tf.metrics.recall(\n                    labels=true_label, predictions=pred_label, weights=None),\n            \'tp\':\n                tf.metrics.true_positives(\n                    labels=true_label, predictions=pred_label, weights=None),\n            \'fn\':\n                tf.metrics.false_negatives(\n                    labels=true_label, predictions=pred_label, weights=None),\n            \'fp\':\n                tf.metrics.false_positives(\n                    labels=true_label, predictions=pred_label, weights=None),\n            \'tn\':\n                tf.metrics.true_negatives(\n                    labels=true_label, predictions=pred_label, weights=None),\n            \'pr_curve_eval\':\n                pr_summary.streaming_op(\n                    name=\'pr_curve_eval\',\n                    labels=true_label_bool,\n                    predictions=softmax[:, -1],\n                    num_thresholds=50,\n                    weights=None)\n        })\n\n    metric_tensor.update({key: val[0] for key, val in eval_metrics_ops.items()})\n    metric_hook = tf.train.LoggingTensorHook(\n        tensors=metric_tensor,\n        every_n_iter=100,\n        every_n_secs=None,\n        at_end=False,\n        formatter=None)\n    eval_hooks.append(metric_hook)\n    return eval_hooks, eval_metrics_ops\n\n  #pylint: disable=arguments-differ\n  def get_infer_predictions(self,\n                            features,\n                            softmax,\n                            alpha=None,\n                            extra_outputs=None):\n    \'\'\'\n    Get prediction results for postprocessing at inference time.\n    Args:\n      extra_outputs: optional extra outputs from model() other than logits.\n    \'\'\'\n    predictions = {""softmax"": softmax}\n    if alpha:\n      predictions.update({\n          ""alignment"": alpha,\n      })\n    predictions.update(features)\n    predictions.pop(\'audio\', None)\n    if extra_outputs is not None:\n      predictions.update(extra_outputs)\n\n    logging.info(\'predictions: {}\'.format(predictions))\n    return predictions\n\n  def train_one_epoch(self, nn, steps=None):  #pylint: disable=invalid-name\n    \'\'\' train for one epoch \'\'\'\n    mode = utils.TRAIN\n    tfconf = self.config[\'solver\'][\'run_config\']\n    nn.train(\n        input_fn=self.input_fn(mode),\n        steps=steps,\n        hooks=[tf_debug.LocalCLIDebugHook()] if tfconf[\'debug\'] else None)\n\n  def train(self):\n    \'\'\' only train \'\'\'\n    nn = self.create_estimator()  #pylint: disable=invalid-name\n\n    num_epochs = self.config[\'solver\'][\'optimizer\'][\'epochs\']\n    for epoch in range(num_epochs):\n      logging.info(""epoch: {}"".format(epoch + 1))\n      try:\n        self.train_one_epoch(nn)\n      except tf.errors.OutOfRangeError as err:\n        logging.info(epoch, err)\n        raise err\n\n  def log_eval_metrics(self, metrics, save=False):\n    \'\'\' eval metrics \'\'\'\n    logging.info(\'Eval Result:\')\n    if save:\n      fobj = open(self.eval_metrics_path, \'w\')\n    for key, value in metrics.items():\n      logging.info(""{}: {}"".format(key, value))\n      if save:\n        fobj.write(""{}: {}\\n"".format(key, value))\n    if \'tp\' in metrics and \'fp\' in metrics and \'fn\' in metrics and \'tn\' in metrics:\n      f1_score = metrics_lib.f1_score(metrics[\'tp\'], metrics[\'fp\'],\n                                      metrics[\'fn\'], metrics[\'tn\'])\n      logging.info(""F1: {}"".format(f1_score))\n      if save:\n        fobj.write(""F1: {}\\n"".format(f1_score))\n\n  #pylint: disable=arguments-differ\n  def eval(self, steps=None):\n    \'\'\' only eval \'\'\'\n    mode = utils.EVAL\n    nn = self.create_estimator()  #pylint: disable=invalid-name\n    ev = nn.evaluate(  #pylint: disable=invalid-name\n        input_fn=self.input_fn(mode),\n        steps=steps,\n        hooks=None,\n        checkpoint_path=None,\n        name=None)\n    self.log_eval_metrics(ev, save=True)\n\n  #pylint: disable=invalid-name\n  def train_and_eval_one_epoch(self, nn, train_spec, eval_spec):\n    \'\'\' train and eval for one epoch \'\'\'\n    eval_result, export_result = tf.estimator.train_and_evaluate(  #pylint: disable=no-member\n        nn, train_spec, eval_spec)\n    logging.info(""Export result:{}"".format(export_result))\n    self.log_eval_metrics(eval_result)\n\n  def train_and_eval(self):\n    \'\'\' train and eval \'\'\'\n    nn = self.create_estimator()  #pylint: disable=invalid-name\n    #logging.info(""Vars: {}"".format(nn.get_variable_names()))\n\n    #pylint: disable=no-member\n    train_spec = tf.estimator.TrainSpec(\n        input_fn=self.input_fn(utils.TRAIN), max_steps=None, hooks=None)\n\n    #pylint: disable=no-member\n    nclass = self.config[\'data\'][\'task\'][\'classes\'][\'num\']\n    # https://github.com/tensorflow/estimator/blob/master/tensorflow_estimator/python/estimator/canned/metric_keys.py\n    if nclass == 2:\n      default_key = metric_keys.MetricKeys.AUC\n    else:\n      default_key = metric_keys.MetricKeys.ACCURACY\n    compare_fn = functools.partial(\n        utils.metric_smaller, default_key=default_key)\n    logging.info(f""Using {default_key} metric for best exporter"")\n\n    eval_spec = tf.estimator.EvalSpec(\n        input_fn=self.input_fn(utils.EVAL),\n        steps=None,\n        name=\'dev\',\n        hooks=None,\n        exporters=[\n            tf.estimator.FinalExporter(\n                name=\'final_export\',\n                serving_input_receiver_fn=self.create_serving_input_receiver_fn(\n                ),\n                assets_extra=None,\n                as_text=False),\n            tf.estimator.BestExporter(\n                name=\'best_exporter\',\n                serving_input_receiver_fn=self.create_serving_input_receiver_fn(\n                ),\n                event_file_pattern=\'eval/*.tfevents.*\',\n                compare_fn=compare_fn,\n                assets_extra=None,\n                as_text=False,\n                exports_to_keep=1,\n            ),\n        ],\n        start_delay_secs=60,\n        throttle_secs=600)\n\n    num_epochs = self.config[\'solver\'][\'optimizer\'][\'epochs\']\n    for epoch in range(num_epochs):\n      logging.info(""epoch: {}"".format(epoch + 1))\n      try:\n        self.train_and_eval_one_epoch(nn, train_spec, eval_spec)\n      except tf.errors.OutOfRangeError as err:\n        logging.info(epoch, err)\n        raise err\n\n  #pylint: disable=arguments-differ\n  def infer(self, yield_single_examples=True):\n    nn = self.create_estimator()  #pylint: disable=invalid-name\n    predictions = nn.predict(\n        input_fn=self.input_fn(utils.INFER),\n        predict_keys=None,\n        hooks=None,\n        checkpoint_path=None,\n        yield_single_examples=yield_single_examples,\n    )\n    return self.postproc_fn()(predictions, log_verbose=False)\n\n  def export_model(self):\n    saver_conf = self.config[\'solver\'][\'saver\']\n    nn = self.create_estimator()  #pylint: disable=invalid-name\n    nn.export_saved_model(\n        export_dir_base=os.path.join(saver_conf[\'model_path\'], \'export\'),\n        serving_input_receiver_fn=self.create_serving_input_receiver_fn(),\n        assets_extra=None,\n        as_text=False,\n        checkpoint_path=None,\n    )\n\n  def postproc_fn(self):\n    postproc_name = self.config[\'solver\'][\'postproc\'][""name""]\n    postproc = registers.postprocess[postproc_name](self.config)\n\n    return postproc\n'"
delta/utils/solver/keras_base_solver.py,9,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' the base solver for asr and emotion \'\'\'\n\nfrom pathlib import Path\nfrom absl import logging\nimport delta.compat as tf\n\n#pylint: disable=import-error\nfrom tensorflow.keras.utils import multi_gpu_model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras.callbacks import CSVLogger\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nfrom delta import utils\nfrom delta.utils.solver.utils import solver_utils\nfrom delta.utils.solver.base_solver import Solver\nfrom delta.utils.solver.utils.callbacks import ParallelModelCheckpoint\nfrom delta.utils.register import registers\n\n\n#pylint: disable=too-many-instance-attributes,too-many-public-methods\nclass KerasBaseSolver(Solver):\n  \'\'\' asr keras base solver\'\'\'\n\n  def __init__(self, config):\n    super().__init__(config)\n    self.batch_input_shape = None\n\n    self._solver = config[\'solver\']\n    self._num_epochs = self._solver[\'optimizer\'][\'epochs\']\n\n    self._lr = self._solver[\'optimizer\'][\'learning_rate\'][\'rate\']\n    self._decay_rate = self._solver[\'optimizer\'][\'learning_rate\'][\'decay_rate\']\n    self._val_metric = self._solver[\'optimizer\'][\'learning_rate\'][\n        \'type\'] == \'val_metric\'\n    if self._val_metric:\n      self._min_lr = self._solver[\'optimizer\'][\'learning_rate\'][\'min_rate\']\n      self._patience = self._solver[\'optimizer\'][\'learning_rate\'][\'patience\']\n\n    self._clipnorm = self._solver[\'optimizer\'][\'clip_global_norm\']\n    self._early_stopping = self._solver[\'optimizer\'][\'early_stopping\'][\'enable\']\n\n    self._monitor_used = self._solver[\'metrics\'][\'monitor_used\'] or \'val_loss\'\n    self._metrics_used = [] if self._solver[\'metrics\'][\n        \'metrics_used\'] is None else self._solver[\'metrics\'][\'metrics_used\']\n\n    self._model_path = self._solver[\'saver\'][\'model_path\']\n    self._model_load_type = self._solver[\'loader\'][\'model_load_type\']\n    self._init_epoch = self._solver[\'loader\'][\'init_epoch\']\n    self._specified_model_file = self._solver[\'loader\'][\'file_name\']\n\n    self._checkpoint_file_pattern = \'model.{epoch:02d}-{monitor_used:.2f}.ckpt\'\n\n    logging.info(\'num_epochs : {}\'.format(self._num_epochs))\n    logging.info(\'lr : {}\'.format(self._lr))\n    logging.info(\'saver path : {}\'.format(self._model_path))\n\n    devices, self._ngpu = utils.gpu_device_names()\n    logging.info(f""ngpu: {self._ngpu}, device list: {devices}"")\n\n    #model\n    self._model = None\n    self._parallel_model = None\n    self._built = False\n\n  @property\n  def ngpu(self):\n    \'\'\' number of gpus \'\'\'\n    return self._ngpu\n\n  @property\n  def raw_model(self):\n    \'\'\' Delta RawModel \'\'\'\n    assert self._model is not None\n    return self._model\n\n  @property\n  def model(self):\n    \'\'\' keras Model before doing `multi_gpu_model` \'\'\'\n    return self.raw_model.model\n\n  @property\n  def parallel_model(self):\n    \'\'\' `multi_gpu_model` of keras Model \'\'\'\n    assert self._parallel_model is not None\n    return self._parallel_model\n\n  @property\n  def active_model(self):\n    \'\'\' real keras model for run\'\'\'\n    return self.parallel_model if self.ngpu > 1 else self.model\n\n  def process_config(self, config):\n    \'\'\' preprocess of config\'\'\'\n    return config\n\n  #pylint: disable=arguments-differ\n  def model_fn(self, mode):\n    \'\'\' build model like tf.estimator.Estimator\'\'\'\n    with tf.device(\'/cpu:0\'):\n      self._model = super().model_fn()\n\n    if not self.model.built:\n      assert self.batch_input_shape\n      # data must be (features, labels), only using features as input\n      self.model.build(input_shape=self.batch_input_shape[0])\n\n    assert self._init_epoch in range(0, self._num_epochs)\n    model_load_type, model_file_name = solver_utils.get_model_file(\n        dir_name=self._model_path,\n        file_name_pattern=self._checkpoint_file_pattern,\n        mode=mode,\n        model_load_type=self._model_load_type,\n        specified_model_file_name=self._specified_model_file)\n\n    logging.info(""{}-{}: load model from {}"".format(mode, model_load_type,\n                                                    model_file_name))\n    if model_file_name is not None:\n      if self.model.built:\n        self.model.load_weights(str(model_file_name), by_name=False)\n      else:\n        self._model = tf.keras.models.load_model(str(model_file_name))\n\n    # parallel and compile model\n    self.build(multi_gpu=(mode == utils.TRAIN))\n\n  def build(self, multi_gpu=False):\n    \'\'\' main entrypoint to build model \'\'\'\n    assert self.model\n\n    loss = self.get_loss()\n    optimizer = self.get_optimizer()\n\n    run_opts, run_metas = self.get_run_opts_metas()\n\n    # compile model\n    if self.ngpu > 1 and multi_gpu:\n      self._parallel_model = multi_gpu_model(\n          self.model, gpus=self.ngpu, cpu_relocation=False, cpu_merge=False)\n      self.parallel_model.compile(\n          loss=loss,\n          optimizer=optimizer,\n          metrics=self._metrics_used,\n          options=run_opts,\n          run_metadata=run_metas)\n    else:\n      self.model.compile(\n          loss=loss,\n          optimizer=optimizer,\n          metrics=self._metrics_used,\n          options=run_opts,\n          run_metadata=run_metas)\n\n    # Print model summary\n    if self.model.built and self.model._is_graph_network:\n      self.model.summary()\n    self._built = True\n\n  def get_run_opts_metas(self):\n    \'\'\' RunOptions and RunMetadata \'\'\'\n    opts_conf = self.config[\'solver\'][\'run_options\']\n    run_opts = tf.RunOptions(\n        trace_level=opts_conf[\'trace_level\'],\n        inter_op_thread_pool=opts_conf[\'inter_op_thread_pool\'],\n        report_tensor_allocations_upon_oom=opts_conf[\n            \'report_tensor_allocations_upon_oom\'])\n    run_metas = tf.RunMetadata()\n    run_metas = None\n    run_opts = None\n    return run_opts, run_metas\n\n  def get_misc_callbacks(self, monitor_used=None):\n    \'\'\'misc_specific callbacks\'\'\'\n    callbacks = []\n    #tensorboard\n    tb_cb = TensorBoard(log_dir=self._model_path)\n    callbacks.append(tb_cb)\n    logging.info(f""CallBack: Tensorboard"")\n\n    # metric history\n    metric_log = \'metrics.csv\'\n    csv_logger = CSVLogger(\n        filename=Path(self._model_path).joinpath(metric_log), separator=\'\\t\')\n    callbacks.append(csv_logger)\n    logging.info(f""CallBack: Metric log to {metric_log}"")\n\n    #save model\n    save_best = Path(self._model_path).joinpath(\'best_model.ckpt\')\n    save_best_cb = ParallelModelCheckpoint(\n        model=self.model,\n        filepath=str(save_best),\n        monitor=monitor_used,\n        verbose=1,\n        save_best_only=True,\n        save_weights_only=False,\n        period=1)\n    callbacks.append(save_best_cb)\n    logging.info(f""CallBack: Save Best Model"")\n\n    # save checkpoint\n    save_file_pattern = self._checkpoint_file_pattern.replace(\n        \'monitor_used\', monitor_used)\n    save_ckpt = Path(self._model_path).joinpath(save_file_pattern)\n    save_ckpt_cb = ParallelModelCheckpoint(\n        model=self.model,\n        filepath=str(save_ckpt),\n        monitor=monitor_used,\n        verbose=1,\n        save_best_only=False,\n        save_weights_only=False,\n        period=1)\n    callbacks.append(save_ckpt_cb)\n    logging.info(f""CallBack: Save Model Checkpoint."")\n\n    # nan check\n    callbacks.append(tf.keras.callbacks.TerminateOnNaN())\n\n    # Stops the model early if the metrics isn\'t improving\n    if self._early_stopping:\n      logging.info(f""CallBack: Early Stop on {monitor_used}"")\n      es_cb = EarlyStopping(\n          monitor=monitor_used, min_delta=0, patience=5, verbose=0, mode=\'auto\')\n      callbacks.append(es_cb)\n\n    # shcedule  learning rate\n    if self._val_metric:\n      logging.info(f""CallBack: Learning Rate Shcedule on {monitor_used}"")\n      lr_shcedule = ReduceLROnPlateau(\n          monitor=monitor_used,\n          factor=self._decay_rate,\n          patience=self._patience,\n          verbose=1,\n          mode=\'auto\',\n          min_delta=0.0001,\n          cooldown=0,\n          min_lr=self._min_lr)\n      callbacks.append(lr_shcedule)\n    return callbacks\n\n  def get_callbacks(self, monitor_used=None):\n    \'\'\' callbacks for traning\'\'\'\n    #Here only the misc callbacks will be return\n    #In furture work, the metric callbacks will also been return\n    return self.get_misc_callbacks(monitor_used)\n\n  def get_optimizer(self):\n    \'\'\' keras optimizer \'\'\'\n    optconf = self.config[\'solver\'][\'optimizer\']\n    method = optconf[\'name\']\n\n    learning_rate = optconf[\'learning_rate\'][\'rate\']\n    if method == \'adam\':\n      opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    elif method == \'adadelta\':\n      opt = tf.keras.optimizers.Adadelta(learning_rate=learning_rate)\n    else:\n      raise ValueError(f""Not support optimmizer: {method}"")\n    return opt\n\n  def input_generator(self, input_iterator, input_task, cur_sess, mode):\n    \'\'\' dataset_based generator used in keras.model.fit_generator()\n        in future, it will be replaced by tf.keras.utils.Sequence\'\'\'\n    next_batch = input_iterator.get_next()\n    generate_time = len(\n        input_task) * self._num_epochs if mode == utils.TRAIN else len(\n            input_task)\n    for _ in range(generate_time):\n      next_batch_data = cur_sess.run(next_batch)\n      yield next_batch_data\n\n  def train(self):\n    """"""Train the model.""""""\n    raise NotImplementedError()\n\n  def eval(self):\n    """"""Evaluate the model.""""""\n    raise NotImplementedError()\n\n  def infer(self):\n    """"""Make a inference.""""""\n    raise NotImplementedError()\n\n  def train_and_eval(self):\n    """"""Train and evaluate.""""""\n    raise NotImplementedError()\n\n  def export_model(self):\n    """"""Export model.""""""\n    raise NotImplementedError()\n'"
delta/utils/solver/keras_solver.py,16,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n#!/usr/bin/env python\n""""""Keras solver is not stable now""""""\n\nimport os\nimport math\nimport numpy as np\nimport delta.compat as tf\nfrom absl import logging\n\nfrom delta.utils.solver.base_solver import Solver\n\nfrom delta import utils\nfrom delta.utils.register import registers\nfrom delta.utils.solver.utils.solver_utils import get_checkpoint_dir\nfrom delta.utils.solver.utils.solver_utils import get_session_conf\nfrom delta.utils.solver.utils.solver_utils import to_saved_model\nfrom delta.utils.solver.utils.solver_utils import save_infer_res\n\n\n@registers.solver.register\nclass KerasSolver(Solver):\n  """"""Solver in Keras way.""""""\n\n  def __init__(self, config):\n    super().__init__(config)\n    self.model_compiled = False\n    self.model_path = config[\'solver\'][\'saver\'][\'model_path\']\n    self.checkpoint_dir = get_checkpoint_dir(self.config)\n    self.session_conf = get_session_conf(self.config)\n    self.session = tf.Session(config=self.session_conf)\n    tf.keras.backend.set_session(self.session)\n    self.metrics = self.get_metrics()\n\n  def process_config(self, config):\n    """"""Process configs.""""""\n    return config\n\n  def input_fn(self, mode):\n    """"""Get the input function for model.""""""\n    super().input_fn(mode)\n    return self.task.input_fn()()\n\n  def get_metrics(self):\n    """"""Get metrics.""""""\n    metrics_list = self.config[\'solver\'][\'metrics\'][\'keras\']\n    return [m[""name""] for m in metrics_list]\n\n  def build_inputs(self, mode):\n    """"""Build the inputs.""""""\n    inputs = self.input_fn(mode)\n\n    self.config[\'data\'][\'sequence_length\'] = inputs.max_seq_len\n    self.config[\'data\'][\'vocab_size\'] = inputs.vocab_size\n    self.config[\'data\'][\'{}_data_size\'.format(mode)] = inputs.data_size\n\n    return inputs\n\n  def build(self):\n    """"""Build the model.""""""\n\n    self.model = self.model_fn()  # pylint: disable=attribute-defined-outside-init\n\n    loss_fn = self.get_loss_fn()\n\n    optimizer = self.get_optimizer()\n\n    self.model.compile(optimizer=optimizer, loss=loss_fn, metrics=self.metrics)\n\n    self.model_compiled = True\n    logging.info(""Model is built."")\n\n  def train_core(self, train_inputs, eval_inputs=None):\n    """"""Core part of training.""""""\n\n    self.build()\n\n    self.session.run(tf.global_variables_initializer())\n    self.session.run(tf.tables_initializer())\n    self.session.run(train_inputs.iterator.initializer)\n    if eval_inputs is not None:\n      self.session.run(eval_inputs.iterator.initializer)\n      validation_data = (eval_inputs.input_x_dict,\n                         eval_inputs.input_y_dict[""input_y""])\n      eval_data_size = self.config[\'data\'][\'eval_data_size\']\n      batch_size = self.config[\'data\'][\'task\'][\'batch_size\']\n      validation_steps = int(eval_data_size / batch_size)\n    else:\n      validation_data = None\n      validation_steps = None\n\n    train_data_size = self.config[\'data\'][\'train_data_size\']\n    num_epochs = self.config[\'solver\'][\'optimizer\'][\'epochs\']\n    batch_size = self.config[\'data\'][\'task\'][\'batch_size\']\n    num_batch_per_epoch = int(math.ceil(train_data_size / batch_size))\n\n    callbacks = [\n        tf.keras.callbacks.TensorBoard(\n            os.path.join(self.model_path, ""logs""),\n            histogram_freq=0,\n            write_graph=True,\n            write_grads=True,\n            write_images=True),\n        tf.keras.callbacks.ModelCheckpoint(\n            os.path.join(self.checkpoint_dir, ""weights.{epoch:02d}""),\n            save_weights_only=True,\n            save_best_only=True)\n    ]\n\n    self.model.fit(\n        train_inputs.input_x_dict,\n        train_inputs.input_y_dict[""input_y""],\n        callbacks=callbacks,\n        epochs=num_epochs,\n        steps_per_epoch=num_batch_per_epoch,\n        validation_data=validation_data,\n        validation_steps=validation_steps)\n\n  def train(self):\n    """"""Train the model.""""""\n    inputs = self.build_inputs(utils.TRAIN)\n\n    self.train_core(inputs)\n\n  def train_and_eval(self):\n    """"""Train and evaluate the model.""""""\n    train_inputs = self.build_inputs(utils.TRAIN)\n    eval_inputs = self.build_inputs(utils.EVAL)\n    self.train_core(train_inputs, eval_inputs)\n\n  def eval(self):\n    """"""Evaluate the model.""""""\n    inputs = self.build_inputs(utils.EVAL)\n    self.build()\n    self.session.run(tf.global_variables_initializer())\n    self.session.run(tf.tables_initializer())\n    self.session.run(inputs.iterator.initializer)\n    eval_data_size = self.config[\'data\'][\'eval_data_size\']\n    batch_size = self.config[\'data\'][\'task\'][\'batch_size\']\n    steps = int(math.ceil(eval_data_size / batch_size))\n    weights_ckpt_dir = tf.train.latest_checkpoint(self.checkpoint_dir)\n    self.model.load_weights(weights_ckpt_dir)\n    results = self.model.evaluate(\n        inputs.input_x_dict, inputs.input_y_dict[""input_y""], steps=steps)\n    for metric, res in zip(self.model.metrics_names, results):\n      print(""{}: {}"".format(metric, res))\n\n  def infer(self, **kwargs):  # pylint: disable=arguments-differ, unused-argument\n    """"""Make a inference.""""""\n    inputs = self.build_inputs(utils.INFER)\n    self.build()\n    self.session.run(tf.global_variables_initializer())\n    self.session.run(tf.tables_initializer())\n    self.session.run(inputs.iterator.initializer)\n    infer_data_size = self.config[\'data\'][\'infer_data_size\']\n    batch_size = self.config[\'data\'][\'task\'][\'batch_size\']\n    steps = int(math.ceil(infer_data_size / batch_size))\n    weights_ckpt_dir = tf.train.latest_checkpoint(self.checkpoint_dir)\n    self.model.load_weights(weights_ckpt_dir)\n    logits = self.model.predict(inputs.input_x_dict, steps=steps)\n    preds = np.argmax(logits, axis=-1)\n    save_infer_res(self.config, logits, preds)\n\n  def export_model(self):\n    """"""Export a model to tensorflow SavedModel.""""""\n    inputs = self.build_inputs(utils.INFER)\n    self.build()\n    logits = self.model(inputs.input_x_dict)\n    score = tf.nn.softmax(logits)\n\n    self.session.run(tf.global_variables_initializer())\n    self.session.run(tf.tables_initializer())\n    self.session.run(inputs.iterator.initializer)\n    weights_ckpt_dir = tf.train.latest_checkpoint(self.checkpoint_dir)\n    self.model.load_weights(weights_ckpt_dir)\n\n    output_dict = {""score"": score}\n    to_saved_model(self.config, self.session, inputs.input_x_dict, output_dict)\n'"
delta/utils/solver/kws_solver.py,20,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' kws solver based on EstimatorSolver\'\'\'\nimport delta.compat as tf\n\nfrom delta.utils import metrics\nfrom delta.utils.register import registers\nfrom delta.utils.solver.estimator_solver import EstimatorSolver\n\n\n@registers.solver.register\nclass KwsSolver(EstimatorSolver):\n  \'\'\' kws solver\'\'\'\n\n  #pylint: disable=useless-super-delegation\n  def __init__(self, config):\n    super().__init__(config)\n\n  def process_config(self, config):\n    data_conf = config[\'data\']\n    class_vocab = data_conf[\'task\'][\'classes\'][\'vocab\']\n    assert len(class_vocab) == data_conf[\'task\'][\'classes\'][\'num\']\n\n    # add revere_vocab, positive_id\n    reverse_vocab = {val: key for key, val in class_vocab.items()}\n    data_conf[\'task\'][\'classes\'][\'reverse_vocab\'] = reverse_vocab\n\n    # add feature shape, without batch_size\n    feat_dim = data_conf[\'task\'][\'audio\'][\'feat_dim\']\n    if data_conf[\'task\'][\'audio\'][\'splice_frame\']:\n      feat_dim = feat_dim * (data_conf[\'task\'][\'audio\'][\'left_context\'] \\\n              + 1 + data_conf[\'task\'][\'audio\'][\'right_context\'])\n    final_feat_dim = feat_dim * (data_conf[\'task\'][\'audio\'][\'delta_order\'] + 1)\n    window_len = data_conf[\'task\'][\'audio\'][\'window_len\']\n    feature_shape = [window_len, final_feat_dim]\n    data_conf[\'task\'][\'audio\'][\'feature_shape\'] = feature_shape\n    return config\n\n  def get_train_hooks(self, labels, logits, alpha=None):\n    nclass = self.config[\'data\'][\'task\'][\'classes\'][\'num\']\n    metric_tensor = {\n        \'global_step\': tf.train.get_or_create_global_step(),\n        ""batch_accuracy"": metrics.accuracy(logits, labels),\n        ""batch_confusion"": metrics.confusion_matrix(logits, labels, nclass),\n    }\n    tf.summary.scalar(\'batch_accuracy\', metric_tensor[\'batch_accuracy\'])\n\n    train_hooks = [\n        tf.train.StepCounterHook(\n            every_n_steps=500,\n            every_n_secs=None,\n            output_dir=None,\n            summary_writer=None),\n        tf.train.FinalOpsHook(\n            final_ops=[tf.train.get_or_create_global_step()],\n            final_ops_feed_dict=None),\n        tf.train.LoggingTensorHook(\n            tensors=metric_tensor,\n            every_n_iter=1000,\n            every_n_secs=None,\n            at_end=False,\n            formatter=None),\n    ]\n    return train_hooks\n\n  def get_eval_hooks(self, labels, logits):\n    \'\'\' lables: [batch]\n            logits: [batch, num_classes]\n        \'\'\'\n    eval_hooks = []\n    metric_tensor = {}\n    with tf.variable_scope(\'metrics\'):\n      true_label = labels\n      softmax = tf.nn.softmax(logits)\n      pred_label = tf.argmax(softmax, -1)\n      eval_metrics_ops = {\n          \'accuracy\':\n              tf.metrics.accuracy(\n                  labels=true_label, predictions=pred_label, weights=None),\n          \'auc\':\n              tf.metrics.auc(\n                  labels=true_label,\n                  predictions=softmax[:, -1],\n                  num_thresholds=20,\n                  curve=\'ROC\',\n                  summation_method=\'trapezoidal\'),\n          \'precision\':\n              tf.metrics.precision(\n                  labels=true_label, predictions=pred_label, weights=None),\n          \'recall\':\n              tf.metrics.recall(\n                  labels=true_label, predictions=pred_label, weights=None),\n          \'tp\':\n              tf.metrics.true_positives(\n                  labels=true_label, predictions=pred_label, weights=None),\n          \'fn\':\n              tf.metrics.false_negatives(\n                  labels=true_label, predictions=pred_label, weights=None),\n          \'fp\':\n              tf.metrics.false_positives(\n                  labels=true_label, predictions=pred_label, weights=None),\n          \'tn\':\n              tf.metrics.true_negatives(\n                  labels=true_label, predictions=pred_label, weights=None),\n      }\n\n    metric_tensor.update({key: val[0] for key, val in eval_metrics_ops.items()})\n    metric_hook = tf.train.LoggingTensorHook(\n        tensors=metric_tensor,\n        every_n_iter=10000,\n        every_n_secs=None,\n        at_end=False,\n        formatter=None)\n    eval_hooks.append(metric_hook)\n    return eval_hooks, eval_metrics_ops\n\n  def create_serving_input_receiver_fn(self):\n    # shape must be with batch_size\n    taskconf = self.config[\'data\'][\'task\']\n    shape = taskconf[\'audio\'][\'feature_shape\']\n    shape.insert(0, None)\n\n    return tf.estimator.export.build_raw_serving_input_receiver_fn(  #pylint:disable=no-member\n        features={\n            \'inputs\':\n                tf.placeholder(name=""inputs"", shape=shape, dtype=tf.float32),\n        },\n        default_batch_size=None,\n    )\n'"
delta/utils/solver/raw_cls_solver.py,5,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Solver for text classification model in raw tensorflow.""""""\n\nimport delta.compat as tf\n\nfrom delta.utils.register import registers\nfrom delta.utils.solver.raw_solver import RawSolver\n\n# pylint: disable=too-many-instance-attributes, not-context-manager, bad-continuation\n\n\n@registers.solver.register\nclass RawClassSolver(RawSolver):\n  """"""Solver for raw tensorflow model.""""""\n\n  def build_output(self, model):  # pylint: disable=no-self-use\n    """"""\n    Build the output of the model.\n    `score` and `input_y` are for loss calculation.\n    `preds` and `y_ground_truth` are for metric calculation.\n    """"""\n\n    model.score = tf.nn.softmax(model.logits, name=""score"")\n    model.preds = tf.argmax(model.logits, axis=-1, name=""preds"")\n    model.y_ground_truth = tf.argmax(model.input_y, axis=-1)\n\n  def build_export_output(self, model):  # pylint: disable=no-self-use\n    """"""\n    Build the output of the model.\n    `score` and `input_y` are for loss calculation.\n    `preds` and `y_ground_truth` are for metric calculation.\n    """"""\n\n    model.score = tf.nn.softmax(model.logits, name=""score"")\n    model.preds = tf.argmax(model.logits, axis=-1, name=""preds"")\n    model.output_dict = {""score"": model.score, ""preds"": model.preds}\n'"
delta/utils/solver/raw_cls_solver_test.py,5,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Test for raw text class solver.""""""\n\nimport os\nfrom pathlib import Path\nfrom absl import logging\nimport delta.compat as tf\n\nfrom delta import utils\nfrom delta.utils.solver.raw_cls_solver import RawClassSolver\nfrom delta.utils.register import import_all_modules_for_register\nfrom delta import PACKAGE_ROOT_DIR\n\n# pylint: disable=missing-docstring\n\n\nclass RawClassSolverTest(tf.test.TestCase):\n\n  def setUp(self):\n    super().setUp()\n    package_root = Path(PACKAGE_ROOT_DIR)\n    self.config_file = package_root.joinpath(\n        \'../egs/mock_text_cls_data/text_cls/v1/config/han-cls.yml\')\n    self.config = utils.load_config(self.config_file)\n    import_all_modules_for_register()\n\n  def test_all(self):\n    # train and eval\n    solver = RawClassSolver(self.config)\n    solver.train_and_eval()\n    model_path = solver.get_generated_model_path()\n    self.assertNotEqual(model_path, None)\n\n    # infer\n    solver.first_eval = True\n    solver.infer()\n    res_file = self.config[""solver""][""postproc""].get(""res_file"", """")\n    self.assertTrue(os.path.exists(res_file))\n\n    # export model\n    solver.export_model()\n\n    export_path_base = self.config[""solver""][""service""][""model_path""]\n    model_version = self.config[""solver""][""service""][""model_version""]\n    export_path = os.path.join(\n        tf.compat.as_bytes(export_path_base), tf.compat.as_bytes(model_version))\n    export_path = os.path.abspath(export_path)\n    logging.info(""Load exported model from: {}"".format(export_path))\n\n    # load the model and run\n    graph = tf.Graph()\n    with graph.as_default():  # pylint: disable=not-context-manager\n      with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n        tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING],\n                                   export_path)\n\n        input_sentence_tensor = graph.get_operation_by_name(\n            ""input_sentence"").outputs[0]\n\n        score_tensor = graph.get_operation_by_name(""score"").outputs[0]\n\n        score = sess.run(\n            score_tensor,\n            feed_dict={input_sentence_tensor: [""I am very angry""]})\n        logging.info(""score: {}"".format(score))\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  tf.test.main()\n'"
delta/utils/solver/raw_match_solver.py,5,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Solver for text classification model in raw tensorflow.""""""\n\nimport delta.compat as tf\n\nfrom delta.utils.register import registers\nfrom delta.utils.solver.raw_solver import RawSolver\n\n# pylint: disable=too-many-instance-attributes, not-context-manager, bad-continuation\n\n\n@registers.solver.register\nclass RawMatchSolver(RawSolver):\n  """"""Solver for raw tensorflow model.""""""\n\n  def __init__(self, config):\n    super().__init__(config)\n    self.tasktype = config[\'data\'][\'task\'][\'type\']\n\n  def build_output(self, model):  # pylint: disable=no-self-use\n    """"""\n    Build the output of the model.\n    `score` and `input_y` are for loss calculation.\n    `preds` and `y_ground_truth` are for metric calculation.\n    """"""\n    if self.tasktype == ""Classification"":\n      model.score = tf.nn.softmax(model.logits, name=""score"")\n      model.preds = tf.argmax(model.logits, axis=-1)\n      model.y_ground_truth = tf.argmax(model.input_y, axis=-1)\n    else:\n      raise ValueError(""%s is not a valid task type.""\n                       ""Must be in `Ranking` and `Classification`."" %\n                       (self.tasktype))\n\n  def build_export_output(self, model):  # pylint: disable=no-self-use\n    """"""\n    Build the output of the model.\n    `score` and `input_y` are for loss calculation.\n    `preds` and `y_ground_truth` are for metric calculation.\n    """"""\n    if self.tasktype == ""Classification"":\n      model.score = tf.nn.softmax(model.logits, name=""score"")\n      model.preds = tf.argmax(model.logits, axis=-1)\n      model.output_dict = {""score"": model.score, ""preds"": model.preds}\n    else:\n      raise ValueError(""%s is not a valid task type.""\n                       ""Must be in `Ranking` and `Classification`."" %\n                       (self.tasktype))\n'"
delta/utils/solver/raw_match_solver_test.py,5,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Test for raw text_match solver.""""""\n\nimport os\nfrom pathlib import Path\nfrom absl import logging\nimport delta.compat as tf\n\nfrom delta import utils\nfrom delta.utils.solver.raw_match_solver import RawMatchSolver\nfrom delta.utils.register import import_all_modules_for_register\nfrom delta import PACKAGE_ROOT_DIR\n\n\n# pylint: disable=missing-docstring\nclass RawMatchSolverTest(tf.test.TestCase):\n  # pylint: disable=invalid-name\n  def setUp(self):\n    super().setUp()\n    package_root = Path(PACKAGE_ROOT_DIR)\n    self.config_file = package_root.joinpath(\n        \'../egs/mock_text_match_data/text_match/v1/config/rnn-match-mock.yml\')\n    self.config = utils.load_config(self.config_file)\n    import_all_modules_for_register()\n\n  def test_all(self):\n    # train and eval\n    import_all_modules_for_register()\n    solver = RawMatchSolver(self.config)\n    solver.train_and_eval()\n    model_path = solver.get_generated_model_path()\n    self.assertNotEqual(model_path, None)\n\n    # infer\n    solver.first_eval = True\n    solver.infer()\n    res_file = self.config[""solver""][""postproc""].get(""res_file"", """")\n    self.assertTrue(os.path.exists(res_file))\n\n    # export model\n    solver.export_model()\n\n    export_path_base = self.config[""solver""][""service""][""model_path""]\n    model_version = self.config[""solver""][""service""][""model_version""]\n    export_path = os.path.join(\n        tf.compat.as_bytes(export_path_base), tf.compat.as_bytes(model_version))\n    export_path = os.path.abspath(export_path)\n    logging.info(""Load exported model from: {}"".format(export_path))\n\n    # load the model and run\n    graph = tf.Graph()\n    with graph.as_default():  # pylint: disable=not-context-manager\n      with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n        tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING],\n                                   export_path)\n\n        input_sentence_tensor_left = graph.get_operation_by_name(\n            ""input_sent_left"").outputs[0]\n        input_sentence_tensor_right = graph.get_operation_by_name(\n            ""input_sent_right"").outputs[0]\n        score_tensor = graph.get_operation_by_name(""score"").outputs[0]\n\n        score = sess.run(\n            score_tensor,\n            feed_dict={\n                input_sentence_tensor_left: [""I love china""],\n                input_sentence_tensor_right: [""I am lovely""]\n            })\n        logging.info(""score: {}"".format(score))\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  tf.test.main()\n'"
delta/utils/solver/raw_nlu_joint_solver.py,10,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Solver for NLU joint model in raw tensorflow.""""""\n\n# pylint: disable=too-many-instance-attributes, not-context-manager, bad-continuation, no-name-in-module\n\nfrom absl import logging\nimport delta.compat as tf\nfrom tensorflow_addons.text import crf_decode\n\nfrom delta.utils.register import registers\nfrom delta.utils.solver.raw_solver import RawSolver\n\n\n@registers.solver.register\nclass RawNLUJointSolver(RawSolver):\n  """"""Solver for NLU joint model.""""""\n\n  def build_output(self, model):  # pylint: disable=no-self-use\n    """"""\n    Build the output of the model.\n    `score` and `input_y` are for loss calculation.\n    `preds` and `y_ground_truth` are for metric calculation.\n    """"""\n\n    transitions = model.transitions\n    intent_logits, slots_logits = model.logits\n    input_intent_y, input_slots_y = model.input_y\n\n    intent_score = tf.nn.softmax(intent_logits, name=""intent_score"")\n    intent_preds = tf.argmax(intent_logits, axis=-1, name=""intent_preds"")\n    y_intent_ground_truth = tf.argmax(\n        input_intent_y, axis=-1, name=""y_intent_ground_truth"")\n\n    slots_preds, slots_score = crf_decode(slots_logits, transitions,\n                                          model.input_x_len)\n\n    slots_preds = tf.identity(slots_preds, name=""slots_preds"")\n    slots_score = tf.identity(slots_score, name=""slots_score"")\n    y_slots_ground_truth = tf.identity(\n        input_slots_y, name=""y_slots_ground_truth"")\n\n    model.preds = intent_preds, slots_preds\n    model.score = intent_score, slots_score\n    model.y_ground_truth = y_intent_ground_truth, y_slots_ground_truth\n    logging.info(""Model built."")\n\n  def build_export_output(self, model):  # pylint: disable=no-self-use\n    """"""\n    Build the output of the model for export.\n    `score` and `input_y` are for loss calculation.\n    `preds` and `y_ground_truth` are for metric calculation.\n    """"""\n    transitions = model.transitions\n    intent_logits, slots_logits = model.logits\n\n    intent_score = tf.nn.softmax(intent_logits, name=""intent_score"")\n    intent_preds = tf.argmax(intent_logits, axis=-1, name=""intent_preds"")\n\n    slots_preds, slots_score = crf_decode(slots_logits, transitions,\n                                          model.input_x_len)\n\n    slots_preds = tf.identity(slots_preds, name=""slots_preds"")\n    slots_score = tf.identity(slots_score, name=""slots_score"")\n\n    model.preds = intent_preds, slots_preds\n    model.score = intent_score, slots_score\n    model.output_dict = {\n        ""slots_score"": slots_score,\n        ""slots_preds"": slots_preds,\n        ""intent_score"": intent_score,\n        ""intent_preds"": intent_preds\n    }\n    logging.info(""Model built."")\n'"
delta/utils/solver/raw_nlu_joint_solver_test.py,5,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Test for raw nlu joint solver.""""""\n\nimport os\nfrom pathlib import Path\nfrom absl import logging\nimport delta.compat as tf\n\nfrom delta import utils\nfrom delta.utils.solver.raw_nlu_joint_solver import RawNLUJointSolver\nfrom delta.utils.register import import_all_modules_for_register\nfrom delta import PACKAGE_ROOT_DIR\n\n# pylint: disable=missing-docstring\n\n\nclass RawNLUJointSolverTest(tf.test.TestCase):\n\n  def setUp(self):\n    super().setUp()\n    package_root = Path(PACKAGE_ROOT_DIR)\n    self.config_file = package_root.joinpath(\n        \'../egs/mock_text_nlu_joint_data/nlu-joint/v1/config/nlu_joint.yml\')\n    self.config = utils.load_config(self.config_file)\n    import_all_modules_for_register()\n\n  def test_all(self):\n    # train and eval\n    solver = RawNLUJointSolver(self.config)\n    solver.train_and_eval()\n    model_path = solver.get_generated_model_path()\n    self.assertNotEqual(model_path, None)\n\n    # infer\n    solver.first_eval = True\n    solver.infer()\n    intent_res_file = self.config[""solver""][""postproc""][0].get(""res_file"", """")\n    slots_res_file = self.config[""solver""][""postproc""][1].get(""res_file"", """")\n    self.assertTrue(os.path.exists(intent_res_file))\n    self.assertTrue(os.path.exists(slots_res_file))\n\n    # export model\n    solver.export_model()\n\n    export_path_base = self.config[""solver""][""service""][""model_path""]\n    model_version = self.config[""solver""][""service""][""model_version""]\n    export_path = os.path.join(\n        tf.compat.as_bytes(export_path_base), tf.compat.as_bytes(model_version))\n    export_path = os.path.abspath(export_path)\n    logging.info(""Load exported model from: {}"".format(export_path))\n\n    # load the model and run\n    graph = tf.Graph()\n    with graph.as_default():  # pylint: disable=not-context-manager\n      with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n        tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING],\n                                   export_path)\n\n        input_sentence_tensor = graph.get_operation_by_name(\n            ""input_sentence"").outputs[0]\n\n        intent_score_tensor = graph.get_operation_by_name(\n            ""intent_score"").outputs[0]\n        slots_score_tensor = graph.get_operation_by_name(\n            ""slots_score"").outputs[0]\n\n        score = sess.run(\n            [intent_score_tensor, slots_score_tensor],\n            feed_dict={input_sentence_tensor: [""I am happy in the KFC""]})\n        logging.info(""score: {}"".format(score))\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  tf.test.main()\n'"
delta/utils/solver/raw_pretrain_cls_solver.py,8,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Solver for pretrain text classification model in raw tensorflow.""""""\n\nimport re\nimport logging\nimport delta.compat as tf\nfrom delta.utils.register import registers\nfrom delta.utils.solver.raw_solver import RawSolver\n\n# pylint: disable=too-many-instance-attributes, not-context-manager, bad-continuation\n\n\n@registers.solver.register\nclass RawPretrainClassSolver(RawSolver):\n  """"""Solver for raw tensorflow model.""""""\n\n  def build_output(self, model):  # pylint: disable=no-self-use\n    """"""\n    Build the output of the model.\n    `score` and `input_y` are for loss calculation.\n    `preds` and `y_ground_truth` are for metric calculation.\n    """"""\n\n    model.score = tf.nn.softmax(model.logits, name=""score"")\n    model.preds = tf.argmax(model.logits, axis=-1)\n    if hasattr(model, ""input_y""):\n      model.y_ground_truth = tf.argmax(model.input_y, axis=-1)\n    model.output_dict = {""score"": model.score, ""preds"": model.preds}\n\n    if model.use_pretrained_model:\n      self.initialize_pretrained_model_variables(model.pretrained_model_path,\n                                                 model.pretrained_model_mode)\n\n  def get_assignment_map_from_checkpoint(self, all_variables, init_checkpoint):\n    """"""\n    Get the map of the current variables and init checkpoint variables.\n    """"""\n    assignment_map = {}\n    name_to_var = {}\n    init_set = set()\n    for var in all_variables:\n      name = var.name\n      m = re.match(""^(.*):\\\\d+$"", name)\n      if m is not None:\n        name = m.group(1)\n      name_to_var[name] = var\n\n    init_vars = tf.train.list_variables(init_checkpoint)\n\n    for name, var_shape in init_vars:\n      for k, v in name_to_var.items():\n        if re.findall(name + \'$\', k):\n          assignment_map[name] = name_to_var[k]\n          init_set.add(k)\n    return assignment_map, init_set\n\n  def remove_trainable_variables(self, init_set):\n    """"""\n    Make the variables of the pretrained model untrainable\n    """"""\n\n    variables_to_untrain = list()\n    trainable_collection = tf.get_collection_ref(\n        tf.GraphKeys.TRAINABLE_VARIABLES)\n    for var in trainable_collection:\n      if var.name in init_set:\n        variables_to_untrain.append(var)\n\n    for var in variables_to_untrain:\n      trainable_collection.remove(var)\n\n  def initialize_pretrained_model_variables(self, pretrained_model_path,\n                                            pretrained_model_mode):\n    """"""\n    Initialize the variables of the pretrained model\n    according to fine-tune of feature mode\n    """"""\n    all_variables = tf.get_collection_ref(tf.GraphKeys.GLOBAL_VARIABLES)\n    init_checkpoint = pretrained_model_path\n    pretrained_assignment_map, init_set = self.get_assignment_map_from_checkpoint(\n        all_variables, init_checkpoint)\n    tf.train.init_from_checkpoint(init_checkpoint, pretrained_assignment_map)\n\n    if pretrained_model_mode == ""feature"":\n      self.remove_trainable_variables(init_set)\n'"
delta/utils/solver/raw_pretrain_seq_label_solver.py,7,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Solver for sequence labeling model in raw tensorflow.""""""\n\n# pylint: disable=too-many-instance-attributes, not-context-manager, bad-continuation, no-name-in-module\n\nimport re\nimport delta.compat as tf\nfrom absl import logging\nfrom tensorflow_addons.text import crf_decode\nfrom delta.utils.register import registers\nfrom delta.utils.solver.raw_solver import RawSolver\n\n\n@registers.solver.register\nclass PretrainRawSeqLabelSolver(RawSolver):\n  """"""Solver for raw tensorflow model.""""""\n\n  def build_output(self, model):  # pylint: disable=no-self-use\n    """"""\n    Build the output of the model.\n    `score` and `input_y` are for loss calculation.\n    `preds` and `y_ground_truth` are for metric calculation.\n    """"""\n    model.preds, score = crf_decode(model.logits, model.transitions,\n                                    model.input_x_len)\n\n    model.score = tf.identity(score, name=""score"")\n    model.y_ground_truth = model.input_y\n    if model.use_pretrained_model:\n      logging.info(""initialize_pretrained_model_variables"")\n      self.initialize_pretrained_model_variables(model.pretrained_model_path,\n                                                 model.pretrained_model_mode)\n\n  def build_export_output(self, model):  # pylint: disable=no-self-use\n    """"""\n    Build the output of the model.\n    `score` and `input_y` are for loss calculation.\n    `preds` and `y_ground_truth` are for metric calculation.\n    """"""\n    model.preds, score = crf_decode(model.logits, model.transitions,\n                                    model.input_x_len)\n\n    model.score = tf.identity(score, name=""score"")\n    model.output_dict = {""score"": model.score, ""preds"": model.preds}\n\n  def get_assignment_map_from_checkpoint(self, all_variables, init_checkpoint):\n    """"""\n    Get the map of the current variables and init checkpoint variables.\n    """"""\n    assignment_map = {}\n    name_to_var = {}\n    init_set = set()\n    for var in all_variables:\n      name = var.name\n      m = re.match(""^(.*):\\\\d+$"", name)\n      if m is not None:\n        name = m.group(1)\n      name_to_var[name] = var\n\n    init_vars = tf.train.list_variables(init_checkpoint)\n\n    for name, var_shape in init_vars:\n      for k, v in name_to_var.items():\n        if re.findall(name + \'$\', k):\n          assignment_map[name] = name_to_var[k]\n          init_set.add(name_to_var[k])\n    return assignment_map, init_set\n\n  def remove_trainable_variables(self, init_set):\n    """"""\n    Make the variables of the pretrained model untrainable\n    """"""\n\n    variables_to_untrain = list()\n    trainable_collection = tf.get_collection_ref(\n        tf.GraphKeys.TRAINABLE_VARIABLES)\n    for var in trainable_collection:\n      if var in init_set:\n        variables_to_untrain.append(var)\n\n    for var in variables_to_untrain:\n      trainable_collection.remove(var)\n\n  def initialize_pretrained_model_variables(self, pretrained_model_path,\n                                            pretrained_model_mode):\n    """"""\n    Initialize the variables of the pretrained model\n    according to fine-tune of feature mode\n    """"""\n    all_variables = tf.get_collection_ref(tf.GraphKeys.GLOBAL_VARIABLES)\n    init_checkpoint = pretrained_model_path\n    pretrained_assignment_map, init_set = self.get_assignment_map_from_checkpoint(\n        all_variables, init_checkpoint)\n    tf.train.init_from_checkpoint(init_checkpoint, pretrained_assignment_map)\n    if pretrained_model_mode == ""feature"":\n      self.remove_trainable_variables(init_set)\n'"
delta/utils/solver/raw_seq2seq_solver.py,4,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Solver for text sequence to sequence model in raw tensorflow.""""""\n\nimport math\n\nimport numpy as np\nimport delta.compat as tf\nfrom absl import logging\n\nfrom delta import utils\nfrom delta.utils import metrics\nfrom delta.utils.register import registers\nfrom delta.utils.solver.raw_solver import RawSolver\n\n# pylint: disable=too-many-instance-attributes, not-context-manager, bad-continuation\n\n\n@registers.solver.register\nclass RawS2SSolver(RawSolver):\n  """"""Solver for raw tensorflow model.""""""\n\n  def build_output(self, model):  # pylint: disable=no-self-use\n    """"""\n    Build the output of the model.\n    `score` and `input_y` are for loss calculation.\n    `preds` and `y_ground_truth` are for metric calculation.\n    """"""\n    if model.mode != utils.INFER:\n      model.score = tf.nn.softmax(model.logits, name=""score"")\n      model.preds = tf.argmax(model.logits, axis=-1)\n      model.output_dict = {""score"": model.score, ""preds"": model.preds}\n    else:\n      model.preds = model.logits\n      model.output_dict = {""preds"": model.preds}\n    if hasattr(model, ""input_y""):\n      model.y_ground_truth = model.input_y\n\n  def build_export_output(self, model):  # pylint: disable=no-self-use\n    """"""\n    Build the output of the model.\n    `score` and `input_y` are for loss calculation.\n    `preds` and `y_ground_truth` are for metric calculation.\n    """"""\n    model.preds = tf.identity(model.logits, name=""preds"")\n    model.output_dict = {""preds"": model.preds}\n\n  def build(self, mode: str):\n    """"""Build the model for training, eval and infer.""""""\n    inputs = self.input_fn(mode)\n\n    self.config[""model""][""is_infer""] = mode == utils.INFER\n\n    model = self.model_fn()\n    training = mode == utils.TRAIN\n    model.logits = model(inputs[""input_x_dict""], training=training)\n    model.iterator = inputs[""iterator""]\n    model.input_x_dict = inputs[""input_x_dict""]\n    model.input_x_len = inputs[""input_x_len""]\n    model.mode = mode\n    loss_fn = self.get_loss_fn()\n    if mode != utils.INFER or not self.infer_no_label:\n      input_y = inputs[""input_y_dict""][""input_y""]\n      model.input_y = input_y\n\n    if mode != utils.INFER:\n      input_y_len = inputs[""input_y_len""]\n      model.loss = loss_fn(\n          labels=model.input_y,\n          logits=model.logits,\n          input_length=model.input_x_len,\n          label_length=input_y_len,\n          name=""loss"",\n      )\n      model.loss_op = model.loss\n      logging.info(""model.loss done"")\n\n    # output related\n    self.build_output(model)\n    return model\n\n  def build_export_model(self):\n    """"""Build the model for export.""""""\n    mode = utils.INFER\n    self.config[""model""][""is_infer""] = mode == utils.INFER\n    export_inputs = self.export_input(mode)\n\n    model = self.model_fn()\n    training = mode == utils.TRAIN\n    model.logits = model(export_inputs[""model_inputs""], training=training)\n    model.model_inputs = export_inputs[""model_inputs""]\n    model.export_inputs = export_inputs[""export_inputs""]\n    model.input_x_len = export_inputs[""model_inputs""][""input_x_len""]\n    # output related\n    self.build_export_output(model)\n    return model\n\n  def eval_or_infer_core(self, model, mode):  # pylint: disable=too-many-locals, too-many-branches\n    """"""The core part of evaluation.""""""\n    model_path = self.get_model_path(mode)\n    if model_path is None:\n      logging.warning(""model_path is None!"")\n      return\n\n    with model.sess.graph.as_default():\n      model.saver.restore(model.sess, save_path=model_path)\n      if self.first_eval:\n        model.sess.run(tf.tables_initializer())\n        self.first_eval = False\n      model.sess.run(model.iterator.initializer)\n\n      # Evaluating loop.\n      total_loss = 0.0\n      data_size = self.config[""data""][\'{}_data_size\'.format(mode)]\n      num_batch_every_epoch = int(math.ceil(data_size / self.batch_size))\n\n      y_ground_truth = []\n      y_preds = []\n\n      for i in range(num_batch_every_epoch):\n\n        if mode == utils.EVAL:\n          loss_val, \\\n          batch_preds, \\\n          batch_y_ground_truth = model.sess.run(\n              [model.loss, model.preds, model.y_ground_truth])\n        elif not self.infer_no_label:\n          batch_preds, \\\n          batch_y_ground_truth = model.sess.run(\n            [model.preds, model.y_ground_truth])\n        else:\n          batch_preds = model.sess.run([model.preds])\n          batch_preds = batch_preds[0]\n\n        if mode == utils.EVAL:\n          total_loss += loss_val\n          y_preds.append([preds for preds in batch_preds])\n        else:\n          end_id = (i + 1) * self.batch_size\n\n          if data_size < end_id:\n            act_end_id = self.batch_size - end_id + data_size\n            batch_preds = batch_preds[:act_end_id]\n            if not self.infer_no_label:\n              batch_y_ground_truth = batch_y_ground_truth[:act_end_id]\n          y_preds.extend([preds for preds in batch_preds])\n\n          if not self.infer_no_label:\n            y_ground_truth.extend(\n                [ground_truth for ground_truth in batch_y_ground_truth])\n\n        if i % 10 == 0 or i == num_batch_every_epoch - 1:\n          logging.info(""Evaluation rate of ""\n                       ""progress: [ {:.2%} ]"".format(\n                           i / (num_batch_every_epoch - 1)))\n\n      if mode == utils.EVAL:\n        logging.info(""Evaluation Average Loss: {:.6}"".format(total_loss /\n                                                             len(y_preds)))\n\n      else:\n        predictions = {""preds"": y_preds}\n        self.postproc_fn()(predictions, log_verbose=False)\n\n        if not self.infer_no_label:\n          metcs = metrics.get_metrics(\n              config=self.config, y_pred=y_preds, y_true=y_ground_truth)\n          logging.info(""Evaluation on %s:"" % mode)\n          # add sort function to make sequence of metrics identical.\n          for key in sorted(metcs.keys()):\n            logging.info(key + "":"" + str(metcs[key]))\n'"
delta/utils/solver/raw_seq2seq_solver_test.py,5,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Test for raw sequence to sequence solver.""""""\n\nimport os\nfrom pathlib import Path\nfrom absl import logging\nimport delta.compat as tf\n\nfrom delta import utils\nfrom delta.utils.solver.raw_seq2seq_solver import RawS2SSolver\nfrom delta.utils.register import import_all_modules_for_register\nfrom delta import PACKAGE_ROOT_DIR\n\n# pylint: disable=missing-docstring\n\n\nclass RawS2SSolverTest(tf.test.TestCase):\n\n  def setUp(self):\n    super().setUp()\n    package_root = Path(PACKAGE_ROOT_DIR)\n    self.config_file = package_root.joinpath(\n        \'../egs/mock_text_seq2seq_data/seq2seq/v1/config/transformer-s2s.yml\')\n    self.config = utils.load_config(self.config_file)\n    import_all_modules_for_register()\n\n  def tearDown(self):\n    \'\'\' tear down \'\'\'\n\n  def test_all(self):\n    # train and eval\n    solver = RawS2SSolver(self.config)\n    solver.train_and_eval()\n    model_path = solver.get_generated_model_path()\n    self.assertNotEqual(model_path, None)\n\n    # infer\n    solver.first_eval = True\n    solver.infer()\n    res_file = self.config[""solver""][""postproc""].get(""res_file"", """")\n    self.assertTrue(os.path.exists(res_file))\n\n    # export model\n    solver.export_model()\n\n    export_path_base = self.config[""solver""][""service""][""model_path""]\n    model_version = self.config[""solver""][""service""][""model_version""]\n    export_path = os.path.join(\n        tf.compat.as_bytes(export_path_base), tf.compat.as_bytes(model_version))\n    export_path = os.path.abspath(export_path)\n    logging.info(""Load exported model from: {}"".format(export_path))\n\n    # load the model and run\n    graph = tf.Graph()\n    with graph.as_default():  # pylint: disable=not-context-manager\n      with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n        tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING],\n                                   export_path)\n\n        input_sentence_tensor = graph.get_operation_by_name(\n            ""input_sentence"").outputs[0]\n\n        preds_tensor = graph.get_operation_by_name(""preds"").outputs[0]\n\n        preds = sess.run(\n            preds_tensor,\n            feed_dict={\n                input_sentence_tensor: [\n                    "" vice president walter ""\n                    ""mondale was released""\n                ]\n            })\n        logging.info(""preds: {}"".format(preds))\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  tf.test.main()\n'"
delta/utils/solver/raw_seq_label_solver.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Solver for sequence labeling model in raw tensorflow.""""""\n\n# pylint: disable=too-many-instance-attributes, not-context-manager, bad-continuation, no-name-in-module\n\nimport delta.compat as tf\nfrom tensorflow_addons.text import crf_decode\nfrom delta.utils.register import registers\nfrom delta.utils.solver.raw_solver import RawSolver\n\n\n@registers.solver.register\nclass RawSeqLabelSolver(RawSolver):\n  """"""Solver for raw tensorflow model.""""""\n\n  def build_output(self, model):  # pylint: disable=no-self-use\n    """"""\n    Build the output of the model.\n    `score` and `input_y` are for loss calculation.\n    `preds` and `y_ground_truth` are for metric calculation.\n    """"""\n    model.preds, score = crf_decode(model.logits, model.transitions,\n                                    model.input_x_len)\n\n    model.score = tf.identity(score, name=""score"")\n    model.y_ground_truth = model.input_y\n\n  def build_export_output(self, model):  # pylint: disable=no-self-use\n    """"""\n    Build the output of the model.\n    `score` and `input_y` are for loss calculation.\n    `preds` and `y_ground_truth` are for metric calculation.\n    """"""\n    model.preds, score = crf_decode(model.logits, model.transitions,\n                                    model.input_x_len)\n\n    model.score = tf.identity(score, name=""score"")\n    model.output_dict = {""score"": model.score, ""preds"": model.preds}\n'"
delta/utils/solver/raw_seq_label_solver_test.py,5,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Test for raw seqlabel solver.""""""\n\nimport os\nfrom pathlib import Path\nfrom absl import logging\nimport delta.compat as tf\n\nfrom delta import utils\nfrom delta.utils.solver.raw_seq_label_solver import RawSeqLabelSolver\nfrom delta.utils.register import import_all_modules_for_register\nfrom delta import PACKAGE_ROOT_DIR\n\n# pylint: disable=missing-docstring\n\n\nclass RawSeqLabelSolverTest(tf.test.TestCase):\n\n  def setUp(self):\n    super().setUp()\n    package_root = Path(PACKAGE_ROOT_DIR)\n    self.config_file = package_root.joinpath(\n        \'../egs/mock_text_seq_label_data/seq-label/v1/config/seq-label-mock.yml\'\n    )\n    self.config = utils.load_config(self.config_file)\n    import_all_modules_for_register()\n\n  def test_all(self):\n    # train and eval\n    solver = RawSeqLabelSolver(self.config)\n    solver.train_and_eval()\n    model_path = solver.get_generated_model_path()\n    self.assertNotEqual(model_path, None)\n\n    # infer\n    solver.first_eval = True\n    solver.infer()\n    res_file = self.config[""solver""][""postproc""].get(""res_file"", """")\n    self.assertTrue(os.path.exists(res_file))\n\n    # export model\n    solver.export_model()\n\n    export_path_base = self.config[""solver""][""service""][""model_path""]\n    model_version = self.config[""solver""][""service""][""model_version""]\n    export_path = os.path.join(\n        tf.compat.as_bytes(export_path_base), tf.compat.as_bytes(model_version))\n    export_path = os.path.abspath(export_path)\n    logging.info(""Load exported model from: {}"".format(export_path))\n\n    # load the model and run\n    graph = tf.Graph()\n    with graph.as_default():  # pylint: disable=not-context-manager\n      with self.cached_session(use_gpu=False, force_gpu=False) as sess:\n        tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING],\n                                   export_path)\n\n        input_sentence_tensor = graph.get_operation_by_name(\n            ""input_sentence"").outputs[0]\n\n        score_tensor = graph.get_operation_by_name(""score"").outputs[0]\n\n        score = sess.run(\n            score_tensor,\n            feed_dict={input_sentence_tensor: [""I am very angry""]})\n        logging.info(""score: {}"".format(score))\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  tf.test.main()\n'"
delta/utils/solver/raw_solver.py,23,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Solver for raw tensorflow model.""""""\n\nimport re\nimport math\nimport numpy as np\nimport delta.compat as tf\nfrom absl import logging\n\nfrom delta.utils.solver.base_solver import Solver\n\nfrom delta import utils\nfrom delta.utils.register import registers\nfrom delta.utils.solver.utils.solver_utils import get_checkpoint_dir\nfrom delta.utils.solver.utils.solver_utils import get_ckpt_state\nfrom delta.utils.solver.utils.solver_utils import get_session_conf\nfrom delta.utils.solver.utils.solver_utils import to_saved_model\nfrom delta.utils.solver.utils.solver_utils import run_metrics\n\n# pylint: disable=too-many-instance-attributes, not-context-manager, bad-continuation\n\n\n@registers.solver.register\nclass RawSolver(Solver):\n  """"""Solver for raw tensorflow model.""""""\n\n  def __init__(self, config):\n    super().__init__(config)\n    self.session_conf, self.smax_to_keep, \\\n    self.batch_size, self.num_epochs, \\\n    self.save_checkpoint_steps, \\\n    self.resume_model_path, self.print_every = self.set_experimental_environment()\n    self.first_eval = True\n    self.do_eval = False\n    self.is_multi_output = False\n    self.output_num = 1\n    self.infer_no_label = self.config[\'data\'][utils.INFER].get(\n        \'infer_no_label\', False)\n\n  def process_config(self, config):\n    """"""Process the configs.""""""\n    return config\n\n  def input_fn(self, mode):\n    """"""Get the input function for training, evaluation and inference.""""""\n    super().input_fn(mode)\n    return self.task.input_fn()()\n\n  def export_input(self, mode):\n    """"""Get the input function for model export.""""""\n    super().input_fn(mode)\n    return self.task.export_inputs()\n\n  def set_experimental_environment(self):\n    """"""Set the experimental environment.""""""\n    # Set configuration\n    session_conf = get_session_conf(self.config)\n\n    task_config = self.config[""data""][""task""]\n    batch_size = task_config[\'batch_size\']\n    num_epochs = task_config[\'epochs\']\n\n    saver_conf = self.config[\'solver\'][\'saver\']\n    smax_to_keep = saver_conf[\'max_to_keep\']\n    save_checkpoint_steps = saver_conf[\'save_checkpoint_steps\']\n    resume_model_path = saver_conf.get(\'resume_model_path\', None)\n    print_every = saver_conf[\'print_every\']\n\n    return session_conf, smax_to_keep, batch_size, num_epochs, \\\n        save_checkpoint_steps, \\\n        resume_model_path, print_every\n\n  def get_scaffold(self, mode, global_step=None, iter_initializer=None):\n    """"""Get training scaffold.""""""\n\n    init_op = tf.global_variables_initializer()\n    if iter_initializer is None:\n      local_init_op = tf.tables_initializer()\n    else:\n      local_init_op = tf.group(tf.tables_initializer(), iter_initializer)\n    saver = self.get_saver(global_step)\n    scaffold = tf.train.Scaffold(\n        saver=saver, init_op=init_op, local_init_op=local_init_op)\n    return scaffold\n\n  def get_generated_model_path(self):\n    """"""Get the path of the checkpoint which is most recently generated during training process.""""""\n    ckpt = get_ckpt_state(self.config)\n    if ckpt is None:\n      return None\n    model_path = ckpt.model_checkpoint_path  # pylint: disable=no-member\n    return model_path\n\n  def get_model_path(self, mode):\n    """"""Get the path of the checkpoint of the model.""""""\n    model_path = """"\n    if ""{}_model_path"".format(mode) in self.config[""solver""][""saver""]:\n      model_path = self.config[""solver""][""saver""][""{}_model_path"".format(mode)]\n    if model_path == """":\n      model_path = self.get_generated_model_path()\n    return model_path\n\n  def build(self, mode: str):\n    """"""Build the model for training, eval and infer.""""""\n    inputs = self.input_fn(mode)\n    logging.info(""build input data done..."")\n\n    model = self.model_fn()\n    training = mode == utils.TRAIN\n    model.logits = model(inputs[""input_x_dict""], training=training)\n    model.input_x_len = inputs[""input_x_len""]\n    model.iterator = inputs[""iterator""]\n    model.input_x_dict = inputs[""input_x_dict""]\n    model.input_x_len = inputs[""input_x_len""]\n    model.loss_fn = self.get_loss_fn()\n    if mode != utils.INFER or not self.infer_no_label:\n      input_y = inputs[""input_y_dict""][""input_y""]\n      if isinstance(model.loss_fn, list):\n        model.loss = []\n        for i, one_loss_fn in enumerate(model.loss_fn):\n          one_loss = one_loss_fn(\n              labels=input_y[i],\n              logits=model.logits[i],\n              input_length=model.input_x_len,\n              model=model,\n              name=""loss_{}"".format(i))\n          model.loss.append(one_loss)\n        model.loss_op = tf.add_n(model.loss, name=""loss_sum"")\n      else:\n        model.loss = model.loss_fn(\n            labels=input_y,\n            logits=model.logits,\n            input_length=model.input_x_len,\n            model=model,\n            name=""loss"")\n        model.loss_op = model.loss\n      logging.info(""model.loss done"")\n      model.input_y = input_y\n\n    # output related\n    self.build_output(model)\n    return model\n\n  def build_export_model(self):\n    """"""Build the model for export.""""""\n    mode = utils.INFER\n    export_inputs = self.export_input(mode)\n\n    model = self.model_fn()\n    training = mode == utils.TRAIN\n    model.logits = model(export_inputs[""model_inputs""], training=training)\n    model.model_inputs = export_inputs[""model_inputs""]\n    model.export_inputs = export_inputs[""export_inputs""]\n    model.input_x_len = export_inputs[""model_inputs""][""input_x_len""]\n\n    # output related\n    self.build_export_output(model)\n    return model\n\n  def build_output(self, model):  # pylint: disable=no-self-use\n    """"""\n    Build the output of the model.\n    `score` and `input_y` are for loss calculation.\n    `preds` and `y_ground_truth` are for metric calculation.\n    """"""\n    raise NotImplementedError\n\n  def build_export_output(self, model):  # pylint: disable=no-self-use\n    """"""\n    Build the output of the model for export.\n    `score` and `input_y` are for loss calculation.\n    `preds` and `y_ground_truth` are for metric calculation.\n    """"""\n    raise NotImplementedError\n\n  def eval(self):\n    """"""Evaluate the model.""""""\n    mode = utils.EVAL\n    graph = tf.Graph()\n    with graph.as_default():\n      self.eval_or_infer_once(mode)\n\n  def infer(self, **kwargs):  # pylint: disable=unused-argument, arguments-differ\n    """"""Make a inference.""""""\n    mode = utils.INFER\n    graph = tf.Graph()\n    with graph.as_default():\n      self.eval_or_infer_once(mode)\n\n  def postproc_fn(self):\n    """"""Post-process function, called after inference.""""""\n    postproc = self.config[\'solver\'][\'postproc\']\n    if isinstance(postproc, list):\n      postproc_fn = []\n      for one_postproc in postproc:\n        postproc_fn.append(registers.postprocess[one_postproc[""name""]](\n            self.config))\n    else:\n      postproc_fn = registers.postprocess[postproc[""name""]](self.config)\n    return postproc_fn\n\n  def eval_or_infer_once(self, mode):\n    """"""Do evaluation or inference once.""""""\n    model = self.build(mode)\n    model.sess = tf.Session(config=self.session_conf)\n    model.saver = tf.train.Saver()\n    self.eval_or_infer_core(model, mode)\n    model.sess.close()\n\n  def eval_or_infer_core(self, model, mode):  # pylint: disable=too-many-locals, too-many-branches, too-many-statements\n    """"""The core part of evaluation.""""""\n\n    self.do_eval = bool(mode == utils.EVAL or not self.infer_no_label)\n    self.is_multi_output = bool(isinstance(model.preds, (tuple, list)))\n    if self.is_multi_output:\n      self.output_num = len(model.preds)\n    model_path = self.get_model_path(mode)\n    if model_path is None:\n      logging.warning(""model_path is None!"")\n      return\n\n    with model.sess.graph.as_default():\n      model.saver.restore(model.sess, save_path=model_path)\n      if self.first_eval:\n        model.sess.run(tf.tables_initializer())\n        self.first_eval = False\n      model.sess.run(model.iterator.initializer)\n\n      # Evaluating loop.\n      data_size = self.config[""data""][\'{}_data_size\'.format(mode)]\n      num_batch_every_epoch = int(math.ceil(data_size / self.batch_size))\n\n      all_fetch_vals = []\n\n      logging.info(""Total eval data size: {},""\n                   ""batch num per epoch: {}"".format(data_size,\n                                                    num_batch_every_epoch))\n\n      for i in range(num_batch_every_epoch):\n        if self.do_eval:\n          if self.is_multi_output:\n            fetch_ops = model.loss + list(model.logits) + list(\n                model.preds) + list(model.y_ground_truth)\n          else:\n            fetch_ops = [\n                model.loss, model.logits, model.preds, model.y_ground_truth\n            ]\n        else:\n          fetch_ops = [model.logits, model.preds]\n        logging.debug(""fetch_ops: {}"".format(fetch_ops))\n        fetch_vals = model.sess.run(fetch_ops)\n\n        end_id = (i + 1) * self.batch_size\n\n        if data_size < end_id:\n          logging.debug(""data_size: {}, end_id: {}"".format(data_size, end_id))\n          act_end_id = self.batch_size - end_id + data_size\n          new_fetch_vals = []\n          for fetch_val in fetch_vals:\n            if np.isscalar(fetch_val):\n              new_fetch_vals.append(fetch_val)\n            else:\n              new_fetch_vals.append(fetch_val[:act_end_id])\n        else:\n          new_fetch_vals = fetch_vals\n\n        all_fetch_vals.append(new_fetch_vals)\n\n        if i % self.print_every == 0 or i == num_batch_every_epoch - 1:\n          logging.info(""Evaluation rate of ""\n                       ""progress: [ {:.2%} ]"".format(\n                           i / (num_batch_every_epoch - 1)))\n\n      all_fetch_nps = []\n      for one_fetch_vals in zip(*all_fetch_vals):\n        if len(np.shape(one_fetch_vals[0])) <= 0:  # pylint: disable=len-as-condition\n          one_fetch_np = one_fetch_vals\n        else:\n          one_fetch_np = np.concatenate(one_fetch_vals, axis=0)\n        all_fetch_nps.append(one_fetch_np)\n\n      # reshape for multi-output\n      if self.is_multi_output:\n        logging.debug(""all_fetch_nps before reshape: {}"".format(\n            len(all_fetch_nps)))\n        new_all_fetch_nps = []\n        sub_fetch_nps = []\n        for one_fetch_np in all_fetch_nps:\n          sub_fetch_nps.append(one_fetch_np)\n          if len(sub_fetch_nps) == self.output_num:\n            new_all_fetch_nps.append(sub_fetch_nps)\n            sub_fetch_nps = []\n\n        logging.debug(""new_all_fetch_nps after reshape: {}"".format(\n            len(new_all_fetch_nps)))\n      else:\n        new_all_fetch_nps = all_fetch_nps\n\n      if self.do_eval:\n        _, _, preds_val, y_ground_truth_val = new_all_fetch_nps\n        run_metrics(self.config, preds_val, y_ground_truth_val, mode)\n\n      if mode == utils.INFER:\n        if self.do_eval:\n          _, logits_val, preds_val, _ = new_all_fetch_nps\n        else:\n          logits_val, preds_val = new_all_fetch_nps\n\n        postproc_fn = self.postproc_fn()\n        logging.info(postproc_fn)\n        if isinstance(postproc_fn, list):\n          for i, one_postproc_fn in enumerate(postproc_fn):\n            predictions = {\n                ""logits"": logits_val[i],\n                ""preds"": preds_val[i],\n                ""output_index"": i\n            }\n            one_postproc_fn(predictions, log_verbose=False)\n        else:\n          predictions = {\n              ""logits"": logits_val,\n              ""preds"": preds_val,\n              ""output_index"": None\n          }\n          postproc_fn(predictions, log_verbose=False)\n\n  def export_model(self):\n    """"""Export a model to tensorflow SavedModel.""""""\n    mode = utils.INFER\n    graph = tf.Graph()\n    with graph.as_default():\n      infer_model = self.build_export_model()\n      infer_model.sess = tf.Session(config=self.session_conf)\n      infer_model.saver = tf.train.Saver()\n\n      model_path = self.get_model_path(mode)\n      infer_model.saver.restore(infer_model.sess, save_path=model_path)\n\n      to_saved_model(self.config, infer_model.sess, infer_model.export_inputs,\n                     infer_model.output_dict)\n\n  def train(self):  # pylint: disable=too-many-locals\n    """"""Train the model.""""""\n    mode = utils.TRAIN\n    train_model = self.build(mode)\n\n    # Supervisor\n    with tf.name_scope(""train""):\n      global_step = tf.train.get_or_create_global_step()\n      train_op = self.get_train_op(train_model.loss_op, global_step)\n\n      checkpoint_dir = get_checkpoint_dir(self.config)\n\n      # scaffold\n      scaffold = self.get_scaffold(mode, global_step,\n                                   train_model.iterator.initializer)\n\n    with tf.train.MonitoredTrainingSession(\n        checkpoint_dir=checkpoint_dir,\n        scaffold=scaffold,\n        save_checkpoint_steps=self.save_checkpoint_steps,\n        config=self.session_conf) as sess:\n      # Training loop. For each batch...\n      data_size = self.config[\'data\'][\'train_data_size\']\n      num_epochs = self.config[""data""][""task""][\'epochs\']\n      num_batch = int(math.ceil(data_size * num_epochs / self.batch_size))\n      num_batch_per_epoch = int(data_size / self.batch_size)\n      logging.info(\n          ""num_batch: {}, num_batch_per_epoch: {}, num_epochs: {}"".format(\n              num_batch, num_batch_per_epoch, num_epochs))\n      for i in range(num_batch):\n        _, _, out_loss = sess.run([train_op, global_step, train_model.loss_op])\n        if i % self.print_every == 0 or i == num_batch - 1:\n          logging.info(""Training for epoch {}: [ {:.2%} ] loss is {:g}"".format(\n              int(i / num_batch_per_epoch),\n              (i % num_batch_per_epoch) / num_batch_per_epoch, out_loss))\n\n  def train_and_eval(self):  # pylint: disable=too-many-locals\n    """"""Train and evaluate the model.""""""\n    # train related\n    g_train = tf.Graph()\n    with g_train.as_default():\n      logging.info(""Compiling train model ..."")\n      train_model = self.build(utils.TRAIN)\n    # eval related\n    g_eval = tf.Graph()\n    with g_eval.as_default():\n      logging.info(""Compiling eval model ..."")\n      eval_model = self.build(utils.EVAL)\n      eval_model.sess = tf.Session(config=self.session_conf, graph=g_eval)\n      eval_model.saver = tf.train.Saver()\n\n    # start train\n    with g_train.as_default():\n      # Supervisor\n      with tf.name_scope(""train""):\n        global_step = tf.train.get_or_create_global_step()\n\n        train_op = self.get_train_op(train_model.loss_op, global_step)\n\n        checkpoint_dir = get_checkpoint_dir(self.config)\n\n        # scaffold\n        scaffold = self.get_scaffold(utils.TRAIN, global_step,\n                                     train_model.iterator.initializer)\n\n        with tf.train.MonitoredTrainingSession(\n            checkpoint_dir=checkpoint_dir,\n            scaffold=scaffold,\n            save_checkpoint_steps=self.save_checkpoint_steps,\n            config=self.session_conf) as sess:\n          # Training loop. For each batch...\n          train_data_size = self.config[\'data\'][\'train_data_size\']\n          num_batch = math.ceil(train_data_size * self.num_epochs /\n                                self.batch_size)\n          num_batch_per_epoch = math.ceil(train_data_size / self.batch_size)\n          logging.info(""Total data size: {}, batch num: {}, ""\n                       ""batch num per epoch: {}"".format(train_data_size,\n                                                        num_batch,\n                                                        num_batch_per_epoch))\n          for i in range(0, num_batch):\n\n            if i % self.save_checkpoint_steps == 0 and i != 0:\n              self.eval_or_infer_core(eval_model, utils.EVAL)\n            _, _, out_loss = sess.run(\n                [train_op, global_step, train_model.loss_op])\n            if i % self.print_every == 0 or i == num_batch - 1 or (\n                i +\n                1) % num_batch_per_epoch == 0 or i % num_batch_per_epoch == 0:\n              logging.info(\n                  ""Training for epoch {}: [ {:.2%} ] loss is {:g}"".format(\n                      int(i / num_batch_per_epoch),\n                      (i % num_batch_per_epoch) / num_batch_per_epoch,\n                      out_loss))\n    eval_model.sess.close()\n'"
delta/utils/solver/speaker_solver.py,2,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Solver for speaker classification. \'\'\'\nfrom absl import logging\nimport librosa\nimport delta.compat as tf\n\nfrom delta.utils.solver.estimator_solver import EstimatorSolver\nfrom delta.utils.register import registers\n\n\n@registers.solver.register\nclass SpeakerSolver(EstimatorSolver):\n  \'\'\' For training speaker recognition models. \'\'\'\n\n  def process_config(self, config):\n    data_conf = config[\'data\']\n\n    feature_shape = data_conf[\'task\'][\'audio\'].get(\'feature_shape\', None)\n\n    if not feature_shape:\n      # add feature shape, withoud batch_size\n      if data_conf[\'task\'][\'suffix\'] == \'.npy\':\n        input_channels = 3 if data_conf[\'task\'][\'audio\'][\n            \'add_delta_deltas\'] else 1\n        nframe = librosa.time_to_frames(\n            data_conf[\'task\'][\'audio\'][\'clip_size\'],\n            sr=data_conf[\'task\'][\'audio\'][\'sr\'],\n            hop_length=data_conf[\'task\'][\'audio\'][\'winstep\'] *\n            data_conf[\'task\'][\'audio\'][\'sr\'])\n        feature_shape = [\n            nframe, data_conf[\'task\'][\'audio\'][\'feature_size\'], input_channels\n        ]\n      else:\n        feature_shape = [\n            data_conf[\'task\'][\'audio\'][\'sr\'] *\n            data_conf[\'task\'][\'audio\'][\'clip_size\']\n        ]\n      data_conf[\'task\'][\'audio\'][\'feature_shape\'] = feature_shape\n    logging.info(f""FEATURE SHAPE: {feature_shape}"")\n    return config\n\n  def create_serving_input_receiver_fn(self):\n    # with batch_size\n    taskconf = self.config[\'data\'][\'task\']\n    shape = [None] + taskconf[\'audio\'][\'feature_shape\']\n    logging.debug(\'serving input shape:{}\'.format(shape))\n\n    return tf.estimator.export.build_raw_serving_input_receiver_fn(\n        features={\n            \'inputs\':\n                tf.placeholder(name=""inputs"", shape=shape, dtype=tf.float32),\n        },\n        default_batch_size=None,\n    )\n'"
delta/utils/textgrid/generate_segment_from_textgrid.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n#!/usr/bin/env python3\n#pylint: disable=bad-indentation\n\nimport os\nimport sys\nimport codecs\nimport textgrid\n\nSAMPLE_FREQ = 8000\nSAMPLE_DTYPE = \'int16\'\nSAMPLE_DBYTE = 2\n\nTEXTGRID_SUFFIX = \'textgrid\'\nPCM_SUFFIX = \'wav\'\nVALID_SEGMENT_TEXTGRID_LABEL = \'#\'\nVALID_SEGMENT_OUTPUT_LABEL = \'1\'\n\nif __name__ == \'__main__\':\n  if len(sys.argv) < 3:\n    print(\'Usage: %s textgrid_file_list output_segment_list\')\n    exit(1)\n\n  textgrid_file_list = sys.argv[1]\n  output_segment_list = sys.argv[2]\n\n  with open(textgrid_file_list) as fp_in, open(output_segment_list,\n                                               \'w\') as fp_out:\n    for line in fp_in:\n      textgrid_file = line.strip()\n      if textgrid_file[-len(TEXTGRID_SUFFIX):].lower() != TEXTGRID_SUFFIX:\n        print(\'File %s does not have textgrid suffix, skipped.\' %\n              (textgrid_file))\n        continue\n\n      base_file = textgrid_file[:-len(TEXTGRID_SUFFIX)]\n      pcm_file = base_file + PCM_SUFFIX\n      \'\'\'\n            # unfinished code to check PCM length\n            print(\'Loading PCM file: %s\' % (pcm_file))\n            pcm_file_basename = os.path.split(pcm_file)[-1]\n            pcm_file_basename_no_postfix = pcm_file_basename.split(\'.\')[0]\n            print(\'PCM file name (no postfix): \', pcm_file_basename_no_postfix)\n\n            with open(pcm_file, \'rb\') as fp_in:\n                pcm_array = np.frombuffer(fp_in.read(), dtype = SAMPLE_DTYPE)\n            pcm_length_sec = pcm_array.shape[0] / SAMPLE_FREQ\n            print(\'PCM length: %d:%d\' % (pcm_length_sec / 60, pcm_length_sec % 60))\n            \'\'\'\n      print(\'Loading TextGrid file: %s\' % (textgrid_file))\n      with codecs.open(textgrid_file, \'r\', encoding=\'utf-8\') as fp_in:\n        fp_out.write(\'%s\' % (os.path.abspath(pcm_file)))\n        the_grid = textgrid.TextGrid(fp_in.read())\n        for idx, tier in enumerate(the_grid):\n          #print(idx)\n          #print(tier.size)\n          #print(tier.xmin)\n          #print(tier.xmax)\n          #print(tier.nameid)\n\n          seg_idx = 0\n          for xmin, xmax, text in tier.simple_transcript:\n            #print(xmin, xmax, end=\' \')\n            #print(codecs.encode(text, \'utf-8\'))\n\n            xmin = float(xmin)\n            xmax = float(xmax)\n\n            if text == VALID_SEGMENT_TEXTGRID_LABEL:\n              fp_out.write(\' (%f,%f)\' % (xmin, xmax))\n\n            seg_idx += 1\n\n        fp_out.write(\'\\n\')\n\nprint(\'Done.\')\n'"
delta/utils/textgrid/split_pcm_by_text_grid.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n#pylint: disable=bad-indentation\n#!/usr/bin/env python3\n\nimport os\nimport sys\nimport codecs\nimport numpy as np\nimport textgrid\n\nSAMPLE_FREQ = 16000\nSAMPLE_DTYPE = \'int16\'\nSAMPLE_DBYTE = 2\n\nif __name__ == \'__main__\':\n  if len(sys.argv) < 5:\n    print(\'Usage: %s pcm_file textgrid_file out_trans_file out_pcm_dir\')\n    exit(1)\n\n  pcm_file = sys.argv[1]\n  textgrid_file = sys.argv[2]\n  out_trans_file = sys.argv[3]\n  out_pcm_dir = sys.argv[4]\n\n  print(\'Loading PCM file: %s\' % (pcm_file))\n  pcm_file_basename = os.path.split(pcm_file)[-1]\n  pcm_file_basename_no_postfix = pcm_file_basename.split(\'.\')[0]\n  print(\'PCM file name (no postfix): \', pcm_file_basename_no_postfix)\n\n  with open(pcm_file, \'rb\') as fp_in:\n    pcm_array = np.frombuffer(fp_in.read(), dtype=SAMPLE_DTYPE)\n  pcm_length_sec = pcm_array.shape[0] / SAMPLE_FREQ\n  print(\'PCM length: %d:%d\' % (pcm_length_sec / 60, pcm_length_sec % 60))\n\n  print(\'Loading TextGrid file and output PCM/trans: %s\' % (textgrid_file))\n  with codecs.open(textgrid_file, \'r\', encoding=\'utf-16\') as fp_in:\n    with codecs.open(out_trans_file, \'w\', encoding=\'gb18030\') as fp_out_trans:\n      the_grid = textgrid.TextGrid(fp_in.read())\n      for idx, tier in enumerate(the_grid):\n        print(idx)\n        print(tier.size)\n        print(tier.xmin)\n        print(tier.xmax)\n        print(tier.nameid)\n\n        seg_idx = 0\n        for xmin, xmax, text in tier.simple_transcript:\n          print(xmin, xmax, end=\' \')\n          print(codecs.encode(text, \'utf-8\'))\n\n          xmin = float(xmin)\n          xmax = float(xmax)\n\n          seg_str_esc = \'%.2f-%.2f\' % (xmin, xmax)\n\n          # index into PCM array and output .pcm file for this segment\n          xmin_samples = int(xmin * SAMPLE_FREQ)\n          xmax_samples = int(xmax * SAMPLE_FREQ)\n          seg_array = pcm_array[xmin_samples:xmax_samples]\n\n          output_file_name = \'%s__%s__%s.pcm\' % (pcm_file_basename, seg_idx,\n                                                 seg_str_esc)\n          output_file_path = os.path.join(out_pcm_dir, output_file_name)\n          print(output_file_path)\n          fp_out_trans.write(output_file_path)\n          fp_out_trans.write(\':\')\n          fp_out_trans.write(text)\n          fp_out_trans.write(\'\\n\')\n          with open(output_file_path, \'wb\') as fp_out:\n            fp_out.write(np.ascontiguousarray(seg_array))\n\n          seg_idx += 1\n'"
delta/utils/textgrid/textgrid.py,0,"b'# Natural Language Toolkit: TextGrid analysis\n#\n# Copyright (C) 2001-2011 NLTK Project\n# Author: Margaret Mitchell <itallow@gmail.com>\n#         Steven Bird <sb@csse.unimelb.edu.au> (revisions)\n# URL: <http://www.nltk.org>\n# For license information, see LICENSE.TXT\n# https://github.com/nltk/nltk_contrib/blob/master/nltk_contrib/textgrid.py\n\n#pylint: disable=invalid-name,unused-argument,anomalous-backslash-in-string,missing-docstring\n#pylint: disable=method-hidden,singleton-comparison,useless-object-inheritance,raising-format-tuple\n#pylint: disable=no-member,non-iterator-returned,unused-import,trailing-whitespace\n""""""\nTools for reading TextGrid files, the format used by Praat.\n\nModule contents\n===============\n\nThe textgrid corpus reader provides 4 data items and 1 function\nfor each textgrid file.  For each tier in the file, the reader\nprovides 10 data items and 2 functions.\n \nFor the full textgrid file: \n\n  - size\n    The number of tiers in the file.\n\n  - xmin\n    First marked time of the file.\n\n  - xmax\n    Last marked time of the file.\n\n  - t_time\n    xmax - xmin.\n\n  - text_type\n    The style of TextGrid format:\n        - ooTextFile:  Organized by tier.\n        - ChronTextFile:  Organized by time.\n        - OldooTextFile:  Similar to ooTextFile.\n\n  - to_chron()\n    Convert given file to a ChronTextFile format.\n\n  - to_oo()\n    Convert given file to an ooTextFile format.\n\nFor each tier:\n\n  - text_type\n    The style of TextGrid format, as above.\n\n  - classid\n    The style of transcription on this tier:\n        - IntervalTier:  Transcription is marked as intervals.\n        - TextTier:  Transcription is marked as single points.\n\n  - nameid\n    The name of the tier.\n\n  - xmin\n    First marked time of the tier.\n\n  - xmax\n    Last marked time of the tier.\n\n  - size\n    Number of entries in the tier.\n\n  - transcript\n    The raw transcript for the tier.\n\n  - simple_transcript\n    The transcript formatted as a list of tuples: (time1, time2, utterance).\n\n  - tier_info\n    List of (classid, nameid, xmin, xmax, size, transcript).\n\n  - min_max()\n    A tuple of (xmin, xmax).  \n\n  - time(non_speech_marker)\n    Returns the utterance time of a given tier.\n    Excludes entries that begin with a non-speech marker.\n\n""""""\n\n# needs more cleanup, subclassing, epydoc docstrings\n\nimport sys\nimport re\n\nTEXTTIER = ""TextTier""\nINTERVALTIER = ""IntervalTier""\n\nOOTEXTFILE = re.compile(r""""""(?x)\n            xmin\\ =\\ (.*)[\\r\\n]+\n            xmax\\ =\\ (.*)[\\r\\n]+\n            [\\s\\S]+?size\\ =\\ (.*)[\\r\\n]+ \n"""""")\n\nCHRONTEXTFILE = re.compile(r""""""(?x)\n            [\\r\\n]+(\\S+)\\ \n            (\\S+)\\ +!\\ Time\\ domain.\\ *[\\r\\n]+\n            (\\S+)\\ +!\\ Number\\ of\\ tiers.\\ *[\\r\\n]+""\n"""""")\n\nOLDOOTEXTFILE = re.compile(r""""""(?x)\n            [\\r\\n]+(\\S+)\n            [\\r\\n]+(\\S+)\n            [\\r\\n]+.+[\\r\\n]+(\\S+)\n"""""")\n\n#################################################################\n# TextGrid Class\n#################################################################\n\n\nclass TextGrid(object):\n  """"""\n    Class to manipulate the TextGrid format used by Praat.\n    Separates each tier within this file into its own Tier\n    object.  Each TextGrid object has\n    a number of tiers (size), xmin, xmax, a text type to help\n    with the different styles of TextGrid format, and tiers with their\n    own attributes.\n    """"""\n\n  def __init__(self, read_file):\n    """"""\n        Takes open read file as input, initializes attributes \n        of the TextGrid file.\n        @type read_file: An open TextGrid file, mode ""r"".\n        @param size:  Number of tiers.\n        @param xmin: xmin.\n        @param xmax: xmax.\n        @param t_time:  Total time of TextGrid file.\n        @param text_type:  TextGrid format.\n        @type tiers:  A list of tier objects.\n        """"""\n\n    self.read_file = read_file\n    self.size = 0\n    self.xmin = 0\n    self.xmax = 0\n    self.t_time = 0\n    self.text_type = self._check_type()\n    self.tiers = self._find_tiers()\n\n  def __iter__(self):\n    for tier in self.tiers:\n      yield tier\n\n  def next(self):\n    if self.idx == (self.size - 1):\n      raise StopIteration\n    self.idx += 1\n    return self.tiers[self.idx]\n\n  @staticmethod\n  def load(file):\n    """"""\n        @param file: a file in TextGrid format\n        """"""\n\n    return TextGrid(open(file).read())\n\n  def _load_tiers(self, header):\n    """"""\n        Iterates over each tier and grabs tier information.\n        """"""\n\n    tiers = []\n    if self.text_type == ""ChronTextFile"":\n      m = re.compile(header)\n      tier_headers = m.findall(self.read_file)\n      tier_re = "" \\d+.?\\d* \\d+.?\\d*[\\r\\n]+\\""[^\\""]*\\""""\n      for i in range(0, self.size):\n        tier_info = [tier_headers[i]] + \\\n        re.findall(str(i + 1) + tier_re, self.read_file)\n        tier_info = ""\\n"".join(tier_info)\n        tiers.append(Tier(tier_info, self.text_type, self.t_time))\n      return tiers\n\n    tier_re = header + ""[\\s\\S]+?(?="" + header + ""|$$)""\n    m = re.compile(tier_re)\n    tier_iter = m.finditer(self.read_file)\n    for iterator in tier_iter:\n      (begin, end) = iterator.span()\n      tier_info = self.read_file[begin:end]\n      tiers.append(Tier(tier_info, self.text_type, self.t_time))\n    return tiers\n\n  def _check_type(self):\n    """"""\n        Figures out the TextGrid format.\n        """"""\n\n    m = re.match(""(.*)[\\r\\n](.*)[\\r\\n](.*)[\\r\\n](.*)"", self.read_file)\n    try:\n      type_id = m.group(1).strip()\n    except AttributeError:\n      raise TypeError(""Cannot read file -- try TextGrid.load()"")\n    xmin = m.group(4)\n    if type_id == ""File type = \\""ooTextFile\\"""":\n      if ""xmin"" not in xmin:\n        text_type = ""OldooTextFile""\n      else:\n        text_type = ""ooTextFile""\n    elif type_id == ""\\""Praat chronological TextGrid text file\\"""":\n      text_type = ""ChronTextFile""\n    else:\n      raise TypeError(""Unknown format \'(%s)\'"", (type_id))\n    return text_type\n\n  def _find_tiers(self):\n    """"""\n        Splits the textgrid file into substrings corresponding to tiers. \n        """"""\n\n    if self.text_type == ""ooTextFile"":\n      m = OOTEXTFILE\n      header = "" +item \\[""\n    elif self.text_type == ""ChronTextFile"":\n      m = CHRONTEXTFILE\n      header = ""\\""\\S+\\"" \\"".*\\"" \\d+\\.?\\d* \\d+\\.?\\d*""\n    elif self.text_type == ""OldooTextFile"":\n      m = OLDOOTEXTFILE\n      header = ""\\"".*\\""[\\r\\n]+\\"".*\\""""\n\n    file_info = m.findall(self.read_file)[0]\n    self.xmin = float(file_info[0])\n    self.xmax = float(file_info[1])\n    self.t_time = self.xmax - self.xmin\n    self.size = int(file_info[2])\n    tiers = self._load_tiers(header)\n    return tiers\n\n  def to_chron(self):\n    """""" \n        @return:  String in Chronological TextGrid file format.\n        """"""\n\n    chron_file = """"\n    chron_file += ""\\""Praat chronological TextGrid text file\\""\\n""\n    chron_file += str(self.xmin) + "" "" + str(self.xmax)\n    chron_file += ""   ! Time domain.\\n""\n    chron_file += str(self.size) + ""   ! Number of tiers.\\n""\n    for tier in self.tiers:\n      idx = (self.tiers.index(tier)) + 1\n      tier_header = ""\\"""" + tier.classid + ""\\"" \\"""" \\\n                    + tier.nameid + ""\\"" "" + str(tier.xmin) \\\n                    + "" "" + str(tier.xmax)\n      chron_file += tier_header + ""\\n""\n      transcript = tier.simple_transcript\n      for (xmin, xmax, utt) in transcript:\n        chron_file += str(idx) + "" "" + str(xmin)\n        chron_file += "" "" + str(xmax) + ""\\n""\n        chron_file += ""\\"""" + utt + ""\\""\\n""\n    return chron_file\n\n  def to_oo(self):\n    """""" \n        @return:  A string in OoTextGrid file format.\n        """"""\n\n    oo_file = """"\n    oo_file += ""File type = \\""ooTextFile\\""\\n""\n    oo_file += ""Object class = \\""TextGrid\\""\\n\\n""\n    oo_file += ""xmin = "", self.xmin, ""\\n""\n    oo_file += ""xmax = "", self.xmax, ""\\n""\n    oo_file += ""tiers? <exists>\\n""\n    oo_file += ""size = "", self.size, ""\\n""\n    oo_file += ""item []:\\n""\n    for i in range(len(self.tiers)):\n      oo_file += ""%4s%s [%s]"" % ("""", ""item"", i + 1)\n      _curr_tier = self.tiers[i]\n      for (x, y) in _curr_tier.header:\n        oo_file += ""%8s%s = \\""%s\\"""" % ("""", x, y)\n      if _curr_tier.classid != TEXTTIER:\n        for (xmin, xmax, text) in _curr_tier.simple_transcript:\n          oo_file += ""%12s%s = %s"" % ("""", ""xmin"", xmin)\n          oo_file += ""%12s%s = %s"" % ("""", ""xmax"", xmax)\n          oo_file += ""%12s%s = \\""%s\\"""" % ("""", ""text"", text)\n      else:\n        for (time, mark) in _curr_tier.simple_transcript:\n          oo_file += ""%12s%s = %s"" % ("""", ""time"", time)\n          oo_file += ""%12s%s = %s"" % ("""", ""mark"", mark)\n    return oo_file\n\n\n#################################################################\n# Tier Class\n#################################################################\n\n\nclass Tier(object):\n  """""" \n    A container for each tier.\n    """"""\n\n  def __init__(self, tier, text_type, t_time):\n    """"""\n        Initializes attributes of the tier: class, name, xmin, xmax\n        size, transcript, total time.  \n        Utilizes text_type to guide how to parse the file.\n        @type tier: a tier object; single item in the TextGrid list.\n        @param text_type:  TextGrid format\n        @param t_time:  Total time of TextGrid file.\n        @param classid:  Type of tier (point or interval).\n        @param nameid:  Name of tier.\n        @param xmin:  xmin of the tier.\n        @param xmax:  xmax of the tier.\n        @param size:  Number of entries in the tier\n        @param transcript:  The raw transcript for the tier.\n        """"""\n\n    self.tier = tier\n    self.text_type = text_type\n    self.t_time = t_time\n    self.classid = """"\n    self.nameid = """"\n    self.xmin = 0\n    self.xmax = 0\n    self.size = 0\n    self.transcript = """"\n    self.tier_info = """"\n    self._make_info()\n    self.simple_transcript = self.make_simple_transcript()\n    if self.classid != TEXTTIER:\n      self.mark_type = ""intervals""\n    else:\n      self.mark_type = ""points""\n      self.header = [(""class"", self.classid), (""name"", self.nameid), \\\n      (""xmin"", self.xmin), (""xmax"", self.xmax), (""size"", self.size)]\n\n  def __iter__(self):\n    return self\n\n  def _make_info(self):\n    """"""\n        Figures out most attributes of the tier object:\n        class, name, xmin, xmax, transcript.\n        """"""\n\n    trans = ""([\\S\\s]*)""\n    if self.text_type == ""ChronTextFile"":\n      classid = ""\\""(.*)\\"" +""\n      nameid = ""\\""(.*)\\"" +""\n      xmin = ""(\\d+\\.?\\d*) +""\n      xmax = ""(\\d+\\.?\\d*) *[\\r\\n]+""\n      # No size values are given in the Chronological Text File format.\n      self.size = None\n      size = """"\n    elif self.text_type == ""ooTextFile"":\n      classid = "" +class = \\""(.*)\\"" *[\\r\\n]+""\n      nameid = "" +name = \\""(.*)\\"" *[\\r\\n]+""\n      xmin = "" +xmin = (\\d+\\.?\\d*) *[\\r\\n]+""\n      xmax = "" +xmax = (\\d+\\.?\\d*) *[\\r\\n]+""\n      size = "" +\\S+: size = (\\d+) *[\\r\\n]+""\n    elif self.text_type == ""OldooTextFile"":\n      classid = ""\\""(.*)\\"" *[\\r\\n]+""\n      nameid = ""\\""(.*)\\"" *[\\r\\n]+""\n      xmin = ""(\\d+\\.?\\d*) *[\\r\\n]+""\n      xmax = ""(\\d+\\.?\\d*) *[\\r\\n]+""\n      size = ""(\\d+) *[\\r\\n]+""\n    m = re.compile(classid + nameid + xmin + xmax + size + trans)\n    self.tier_info = m.findall(self.tier)[0]\n    self.classid = self.tier_info[0]\n    self.nameid = self.tier_info[1]\n    self.xmin = float(self.tier_info[2])\n    self.xmax = float(self.tier_info[3])\n    if self.size != None:\n      self.size = int(self.tier_info[4])\n    self.transcript = self.tier_info[-1]\n\n  def make_simple_transcript(self):\n    """""" \n        @return:  Transcript of the tier, in form [(start_time end_time label)]\n        """"""\n\n    if self.text_type == ""ChronTextFile"":\n      trans_head = """"\n      trans_xmin = "" (\\S+)""\n      trans_xmax = "" (\\S+)[\\r\\n]+""\n      trans_text = ""\\""([\\S\\s]*?)\\""""\n    elif self.text_type == ""ooTextFile"":\n      trans_head = "" +\\S+ \\[\\d+\\]: *[\\r\\n]+""\n      trans_xmin = "" +\\S+ = (\\S+) *[\\r\\n]+""\n      trans_xmax = "" +\\S+ = (\\S+) *[\\r\\n]+""\n      trans_text = "" +\\S+ = \\""([^\\""]*?)\\""""\n    elif self.text_type == ""OldooTextFile"":\n      trans_head = """"\n      trans_xmin = ""(.*)[\\r\\n]+""\n      trans_xmax = ""(.*)[\\r\\n]+""\n      trans_text = ""\\""([\\S\\s]*?)\\""""\n    if self.classid == TEXTTIER:\n      trans_xmin = """"\n    trans_m = re.compile(trans_head + trans_xmin + trans_xmax + trans_text)\n    self.simple_transcript = trans_m.findall(self.transcript)\n    return self.simple_transcript\n\n  def transcript(self):\n    """"""\n        @return:  Transcript of the tier, as it appears in the file.\n        """"""\n\n    return self.transcript\n\n  def time(self, non_speech_char="".""):\n    """"""\n        @return: Utterance time of a given tier.\n        Screens out entries that begin with a non-speech marker.        \n        """"""\n\n    total = 0.0\n    if self.classid != TEXTTIER:\n      for (time1, time2, utt) in self.simple_transcript:\n        utt = utt.strip()\n        if utt and not utt[0] == ""."":\n          total += (float(time2) - float(time1))\n    return total\n\n  def tier_name(self):\n    """"""\n        @return:  Tier name of a given tier.\n        """"""\n\n    return self.nameid\n\n  def classid(self):\n    """"""\n        @return:  Type of transcription on tier.\n        """"""\n\n    return self.classid\n\n  def min_max(self):\n    """"""\n        @return:  (xmin, xmax) tuple for a given tier.\n        """"""\n\n    return (self.xmin, self.xmax)\n\n  def __repr__(self):\n    return ""<%s \\""%s\\"" (%.2f, %.2f) %.2f%%>"" % (self.classid, self.nameid,\n                                                self.xmin, self.xmax,\n                                                100 * self.time() / self.t_time)\n\n  def __str__(self):\n    return self.__repr__() + ""\\n  "" + ""\\n  "".join(\n        "" "".join(row) for row in self.simple_transcript)\n\n\ndef demo_TextGrid(demo_data):\n  print(""** Demo of the TextGrid class. **"")\n\n  fid = TextGrid(demo_data)\n  print(""Tiers: %s"" % (fid.size))\n\n  for i, tier in enumerate(fid):\n    print(""\\n***"")\n    print(""Tier: %s"" % (i + 1))\n    print(tier)\n\n\ndef demo():\n  # Each demo demonstrates different TextGrid formats.\n  print(""Format 1"")\n  demo_TextGrid(demo_data1)\n  print(""\\nFormat 2"")\n  demo_TextGrid(demo_data2)\n  print(""\\nFormat 3"")\n  demo_TextGrid(demo_data3)\n\n\ndemo_data1 = """"""File type = ""ooTextFile""\nObject class = ""TextGrid""\n\nxmin = 0 \nxmax = 2045.144149659864\ntiers? <exists> \nsize = 3 \nitem []: \n    item [1]:\n        class = ""IntervalTier"" \n        name = ""utterances"" \n        xmin = 0 \n        xmax = 2045.144149659864 \n        intervals: size = 5 \n        intervals [1]:\n            xmin = 0 \n            xmax = 2041.4217474125382 \n            text = """" \n        intervals [2]:\n            xmin = 2041.4217474125382 \n            xmax = 2041.968276643991 \n            text = ""this"" \n        intervals [3]:\n            xmin = 2041.968276643991 \n            xmax = 2042.5281632653062 \n            text = ""is"" \n        intervals [4]:\n            xmin = 2042.5281632653062 \n            xmax = 2044.0487352585324 \n            text = ""a"" \n        intervals [5]:\n            xmin = 2044.0487352585324 \n            xmax = 2045.144149659864 \n            text = ""demo"" \n    item [2]:\n        class = ""TextTier"" \n        name = ""notes"" \n        xmin = 0 \n        xmax = 2045.144149659864 \n        points: size = 3 \n        points [1]:\n            time = 2041.4217474125382 \n            mark = "".begin_demo""\n        points [2]:\n            time = 2043.8338291031832\n            mark = ""voice gets quiet here"" \n        points [3]:\n            time = 2045.144149659864\n            mark = "".end_demo"" \n    item [3]:\n        class = ""IntervalTier"" \n        name = ""phones"" \n        xmin = 0 \n        xmax = 2045.144149659864\n        intervals: size = 12\n        intervals [1]:\n            xmin = 0 \n            xmax = 2041.4217474125382 \n            text = """" \n        intervals [2]:\n            xmin = 2041.4217474125382 \n            xmax = 2041.5438290324326 \n            text = ""D""\n        intervals [3]:\n            xmin = 2041.5438290324326\n            xmax = 2041.7321032910372\n            text = ""I""\n        intervals [4]:\n            xmin = 2041.7321032910372            \n            xmax = 2041.968276643991 \n            text = ""s"" \n        intervals [5]:\n            xmin = 2041.968276643991 \n            xmax = 2042.232189031843\n            text = ""I""\n        intervals [6]:\n            xmin = 2042.232189031843\n            xmax = 2042.5281632653062 \n            text = ""z"" \n        intervals [7]:\n            xmin = 2042.5281632653062 \n            xmax = 2044.0487352585324 \n            text = ""eI"" \n        intervals [8]:\n            xmin = 2044.0487352585324 \n            xmax = 2044.2487352585324\n            text = ""dc""\n        intervals [9]:\n            xmin = 2044.2487352585324\n            xmax = 2044.3102321849011\n            text = ""d""\n        intervals [10]:\n            xmin = 2044.3102321849011\n            xmax = 2044.5748932104329\n            text = ""E""\n        intervals [11]:\n            xmin = 2044.5748932104329\n            xmax = 2044.8329108578437\n            text = ""m""\n        intervals [12]:\n            xmin = 2044.8329108578437\n            xmax = 2045.144149659864 \n            text = ""oU"" \n""""""\n\ndemo_data2 = """"""File type = ""ooTextFile""\nObject class = ""TextGrid""\n\n0\n2.8\n<exists>\n2\n""IntervalTier""\n""utterances""\n0\n2.8\n3\n0\n1.6229213249309031\n""""\n1.6229213249309031\n2.341428074708195\n""demo""\n2.341428074708195\n2.8\n""""\n""IntervalTier""\n""phones""\n0\n2.8\n6\n0\n1.6229213249309031\n""""\n1.6229213249309031\n1.6428291382019483\n""dc""\n1.6428291382019483\n1.65372183721983721\n""d""\n1.65372183721983721\n1.94372874328943728\n""E""\n1.94372874328943728\n2.13821938291038210\n""m""\n2.13821938291038210\n2.341428074708195\n""oU""\n2.341428074708195\n2.8\n""""\n""""""\n\ndemo_data3 = """"""""Praat chronological TextGrid text file""\n0 2.8   ! Time domain.\n2   ! Number of tiers.\n""IntervalTier"" ""utterances"" 0 2.8\n""IntervalTier"" ""utterances"" 0 2.8\n1 0 1.6229213249309031\n""""\n2 0 1.6229213249309031\n""""\n2 1.6229213249309031 1.6428291382019483\n""dc""\n2 1.6428291382019483 1.65372183721983721\n""d""\n2 1.65372183721983721 1.94372874328943728\n""E""\n2 1.94372874328943728 2.13821938291038210\n""m""\n2 2.13821938291038210 2.341428074708195\n""oU""\n1 1.6229213249309031 2.341428074708195\n""demo""\n1 2.341428074708195 2.8\n""""\n2 2.341428074708195 2.8\n""""\n""""""\n\nif __name__ == ""__main__"":\n  demo()\n'"
deltann/infer/python/setup.py,0,"b'import os\nimport sys\nimport re\nimport platform\nimport subprocess\n\nfrom setuptools import setup, Extension, find_packages\nfrom setuptools.command.build_ext import build_ext\nfrom distutils.version import LooseVersion\n\n#if ""DELTA_BUILD_DIR"" in os.environ:\n#    DELTA_BUILD_DIR = os.environ[\'DELTA_BUILD_DIR\']\n#else:\n#    raise RuntimeError(""You must set the env variable DELTA_BUILD_DIR, which is build dir of the delta infer with python."")\n\ndef load_readme(path):\n    return open(os.path.join(os.path.dirname(__file__), ""README.md"")).read()\n\nclass DeltaExportExtension(Extension):\n    def __init__(self, name, sourcedir=\'\'):\n        Extension.__init__(self, name, sources=[])\n        self.sourcedir = os.path.abspath(sourcedir)\n\nclass DeltaExtBuild(build_ext):\n    def run(self):\n        try:\n            out = subprocess.check_output([\'cmake\', \'--version\'])\n        except OSError:\n            raise RuntimeError(""CMake must be installed to build the following extensions: "" +\n                               "", "".join(e.name for e in self.extensions))\n\n        if platform.system() == ""Windows"":\n            cmake_version = LooseVersion(re.search(r\'version\\s*([\\d.]+)\', out.decode()).group(1))\n            if cmake_version < \'3.1.0\':\n                raise RuntimeError(""CMake >= 3.1.0 is required on Windows"")\n\n        for ext in self.extensions:\n            self.build_extension(ext)\n\n    def build_extension(self, ext):\n        extdir = os.path.abspath(os.path.dirname(self.get_ext_fullpath(ext.name)))\n        cmake_args = [\'-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=\' + extdir,\n                      \'-DPYTHON_EXECUTABLE=\' + sys.executable,\n                      \'-DBUILD_DELTA_INFER_PYTHON=ON\',\n                      \'-DCMAKE_INSTALL_RPATH=$ORIGIN\']\n\n        BuildWithDebug = \'ON\' if self.debug else \'OFF\'\n        BuildMode = \'Debug\' if self.debug else \'Release\'\n        build_args = [\'--config\', BuildMode]\n\n        if platform.system() == ""Windows"":\n            cmake_args += [\'-DCMAKE_LIBRARY_OUTPUT_DIRECTORY_{}={}\'.format(BuildMode.upper(), extdir)]\n            if sys.maxsize > 2**32:\n                cmake_args += [\'-A\', \'x64\']\n            build_args += [\'--\', \'/m\']\n        else:\n            #cmake_args += [\'-DBUILD_DEBUG=\' + BuildWithDebug]\n            cmake_args += [\'-DBUILD_DEBUG=\' + ""ON""]\n            build_args += [\'--\', \'-j8\']\n\n        env = os.environ.copy()\n        env[\'CXXFLAGS\'] = \'{} -DVERSION_INFO=\\\\""{}\\\\""\'.format(env.get(\'CXXFLAGS\', \'\'),\n                                                              self.distribution.get_version())\n        if not os.path.exists(self.build_temp):\n            os.makedirs(self.build_temp)\n        subprocess.check_call([\'cmake\', ext.sourcedir] + cmake_args, cwd=self.build_temp, env=env)\n        subprocess.check_call([\'cmake\', \'--build\', \'.\'] + build_args, cwd=self.build_temp)\n\nsetup(\n    name=\'delta_infer\',\n    version=\'0.0.1\',\n    author=\'Speech-HPC\',\n    author_email=\'cuichaowen@didiglobal.com\',\n    description=\'Delta inference python api setup\',\n    long_description=load_readme(""./README.txt""),\n    packages=find_packages(),\n    include_package_data=True,\n    ext_modules=[DeltaExportExtension(\'export_py\', \'..\')],\n    cmdclass=dict(build_ext=DeltaExtBuild),\n    entry_points={\n        \'console_scripts\': [\n            \'visual = delta_infer.visual_pattern:command\',\n        ]\n    },\n    install_requires=[\n        ""absl-py >= 0.8.0"",\n        ""netron >= 3.5.9"",\n    ],\n    zip_safe=False,\n)\n\n'"
delta/data/feat/python_speech_features/__init__.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Speech feature extractor. \'\'\'\nfrom .base import *\n'"
delta/data/feat/python_speech_features/base.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\'\ncalculate filterbank features. Provides e.g. fbank and mfcc features for use in ASR applications\n# Author: James Lyons 2012\n\'\'\'\n\nfrom __future__ import division\nimport numpy\nfrom scipy.fftpack import dct\n\nfrom delta.data.feat.python_speech_features import sigproc\n\n# numpy.log() is not recognized by pylint\n# pylint: disable=assignment-from-no-return\n\n# pylint: disable=unsupported-assignment-operation\n# pylint: disable=unused-argument\n# pylint: disable=unused-variable\n# pylint: disable=invalid-name\n# pylint: disable=line-too-long\n# pylint: disable=too-many-arguments\n\n\ndef mfcc(signal,\n         samplerate=16000,\n         winlen=0.025,\n         winstep=0.01,\n         numcep=13,\n         nfilt=26,\n         nfft=512,\n         lowfreq=0,\n         highfreq=None,\n         preemph=0.97,\n         ceplifter=22,\n         append_energy=True,\n         winfunc=lambda x: numpy.ones((x,))):\n  """"""Compute MFCC features from an audio signal.\n\n    :param signal: the audio signal from which to compute features. Should be an N*1 array\n    :param samplerate: the samplerate of the signal we are working with.\n    :param winlen: the length of the analysis window in seconds. Default is 0.025s (25 milliseconds)\n    :param winstep: the step between successive windows in seconds. Default is 0.01s (10 milliseconds)\n    :param numcep: the number of cepstrum to return, default 13\n    :param nfilt: the number of filters in the filterbank, default 26.\n    :param nfft: the FFT size. Default is 512.\n    :param lowfreq: lowest band edge of mel filters. In Hz, default is 0.\n    :param highfreq: highest band edge of mel filters. In Hz, default is samplerate/2\n    :param preemph: apply preemphasis filter with preemph as coefficient. 0 is no filter. Default is 0.97.\n    :param ceplifter: apply a lifter to final cepstral coefficients. 0 is no lifter. Default is 22.\n    :param appendEnergy: if this is true, the zeroth cepstral coefficient is replaced with the log of the total frame energy.\n    :param winfunc: the analysis window to apply to each frame. By default no window is applied. You can use numpy window functions here e.g. winfunc=numpy.hamming\n    :returns: A numpy array of size (NUMFRAMES by numcep) containing features. Each row holds 1 feature vector.\n    """"""\n  feat, energy = fbank(signal, samplerate, winlen, winstep, nfilt, nfft,\n                       lowfreq, highfreq, preemph, winfunc)\n  feat = numpy.log(feat)\n  feat = dct(feat, type=2, axis=1, norm=\'ortho\')[:, :numcep]\n  feat = lifter(feat, ceplifter)\n  if append_energy:\n    feat[:, 0] = numpy.log(\n        energy)  # replace first cepstral coefficient with log of frame energy\n  return feat\n\n\ndef powerspec(signal,\n              samplerate=16000,\n              winlen=0.025,\n              winstep=0.01,\n              nfft=512,\n              lowfreq=0,\n              highfreq=None,\n              preemph=0.97,\n              winfunc=lambda x: numpy.ones((x,))):\n  """"""Compute power spectorgram features from an audio signal.\n\n    :param signal: the audio signal from which to compute features. Should be an N*1 array\n    :param samplerate: the samplerate of the signal we are working with.\n    :param winlen: the length of the analysis window in seconds. Default is 0.025s (25 milliseconds)\n    :param winstep: the step between successive windows in seconds. Default is 0.01s (10 milliseconds)\n    :param nfft: the FFT size. Default is 512.\n    :param lowfreq: lowest band edge of mel filters. In Hz, default is 0.\n    :param highfreq: highest band edge of mel filters. In Hz, default is samplerate/2\n    :param preemph: apply preemphasis filter with preemph as coefficient. 0 is no filter. Default is 0.97.\n    :param winfunc: the analysis window to apply to each frame. By default no window is applied. You can use numpy window functions here e.g. winfunc=numpy.hamming\n    :returns:  first is a numpy array of size (NUMFRAMES by nfft) containing power spectrogram.\n    """"""\n  highfreq = highfreq or samplerate / 2\n  signal = sigproc.preemphasis(signal, preemph)\n  frames = sigproc.framesig(signal, winlen * samplerate, winstep * samplerate,\n                            winfunc)\n  pspec = sigproc.powspec(frames, nfft)\n  return pspec\n\n\ndef logpowerspec(ps, norm=1):\n  """"""Compute the log power spectorgram features from powerspec.\n\n    :param ps: power spectrum\n    :param norm: If norm=1, the log power spectrum is normalised so that the max value (across all frames) is 0.\n    :returns: If frames is an NxD matrix, output will be Nx(NFFT/2+1). Each row will be the log power spectrum of the corresponding frame.\n    """"""\n  ps[ps <= 1e-30] = 1e-30\n  lps = 10 * numpy.log10(ps)\n  if norm:\n    return lps - numpy.max(lps)\n  return lps\n\n\ndef fbank(signal,\n          samplerate=16000,\n          winlen=0.025,\n          winstep=0.01,\n          nfilt=26,\n          nfft=512,\n          lowfreq=0,\n          highfreq=None,\n          preemph=0.97,\n          winfunc=lambda x: numpy.ones((x,))):\n  """"""Compute Mel-filterbank energy features from an audio signal.\n\n    :param signal: the audio signal from which to compute features. Should be an N*1 array\n    :param samplerate: the samplerate of the signal we are working with.\n    :param winlen: the length of the analysis window in seconds. Default is 0.025s (25 milliseconds)\n    :param winstep: the step between successive windows in seconds. Default is 0.01s (10 milliseconds)\n    :param nfilt: the number of filters in the filterbank, default 26.\n    :param nfft: the FFT size. Default is 512.\n    :param lowfreq: lowest band edge of mel filters. In Hz, default is 0.\n    :param highfreq: highest band edge of mel filters. In Hz, default is samplerate/2\n    :param preemph: apply preemphasis filter with preemph as coefficient. 0 is no filter. Default is 0.97.\n    :param winfunc: the analysis window to apply to each frame. By default no window is applied. You can use numpy window functions here e.g. winfunc=numpy.hamming\n    :returns: 2 values. The first is a numpy array of size (NUMFRAMES by nfilt) containing features. Each row holds 1 feature vector. The\n        second return value is the energy in each frame (total energy, unwindowed)\n    """"""\n  pspec = powerspec(signal, samplerate, winlen, winstep, nfft, lowfreq,\n                    highfreq, preemph, winfunc)\n  energy = numpy.sum(pspec, 1)  # this stores the total energy in each frame\n  energy = numpy.where(energy == 0,\n                       numpy.finfo(float).eps,\n                       energy)  # if energy is zero, we get problems with log\n\n  fb = get_filterbanks(nfilt, nfft, samplerate, lowfreq, highfreq)\n  feat = numpy.dot(pspec, fb.T)  # compute the filterbank energies\n  feat = numpy.where(feat == 0,\n                     numpy.finfo(float).eps,\n                     feat)  # if feat is zero, we get problems with log\n\n  return feat, energy\n\n\ndef logfbank(signal,\n             samplerate=16000,\n             winlen=0.025,\n             winstep=0.01,\n             nfilt=26,\n             nfft=512,\n             lowfreq=0,\n             highfreq=None,\n             preemph=0.97,\n             winfunc=lambda x: numpy.ones((x,))):\n  """"""Compute log Mel-filterbank energy features from an audio signal.\n\n    :param signal: the audio signal from which to compute features. Should be an N*1 array\n    :param samplerate: the samplerate of the signal we are working with.\n    :param winlen: the length of the analysis window in seconds. Default is 0.025s (25 milliseconds)\n    :param winstep: the step between successive windows in seconds. Default is 0.01s (10 milliseconds)\n    :param nfilt: the number of filters in the filterbank, default 26.\n    :param nfft: the FFT size. Default is 512.\n    :param lowfreq: lowest band edge of mel filters. In Hz, default is 0.\n    :param highfreq: highest band edge of mel filters. In Hz, default is samplerate/2\n    :param preemph: apply preemphasis filter with preemph as coefficient. 0 is no filter. Default is 0.97.\n    :param winfunc: the analysis window to apply to each frame. By default no window is applied. You can use numpy window functions here e.g. winfunc=numpy.hamming\n    :returns: A numpy array of size (NUMFRAMES by nfilt) containing features. Each row holds 1 feature vector.\n    """"""\n  feat, energy = fbank(signal, samplerate, winlen, winstep, nfilt, nfft,\n                       lowfreq, highfreq, preemph, winfunc)\n  return numpy.log(feat)\n\n\ndef logfbank_from_powspec(pspec,\n                          samplerate=16000,\n                          nfilt=26,\n                          nfft=512,\n                          lowfreq=0,\n                          highfreq=None):\n  \'\'\' Log fbank from power spectrum. \'\'\'\n  energy = numpy.sum(pspec, 1)  # this stores the total energy in each frame\n  energy = numpy.where(energy == 0,\n                       numpy.finfo(float).eps,\n                       energy)  # if energy is zero, we get problems with log\n\n  fb = get_filterbanks(nfilt, nfft, samplerate, lowfreq, highfreq)\n  feat = numpy.dot(pspec, fb.T)  # compute the filterbank energies\n  feat = numpy.where(feat == 0,\n                     numpy.finfo(float).eps,\n                     feat)  # if feat is zero, we get problems with log\n\n  return numpy.log(feat)\n\n\ndef ssc(signal,\n        samplerate=16000,\n        winlen=0.025,\n        winstep=0.01,\n        nfilt=26,\n        nfft=512,\n        lowfreq=0,\n        highfreq=None,\n        preemph=0.97,\n        winfunc=lambda x: numpy.ones((x,))):\n  """"""Compute Spectral Subband Centroid features from an audio signal.\n\n    :param signal: the audio signal from which to compute features. Should be an N*1 array\n    :param samplerate: the samplerate of the signal we are working with.\n    :param winlen: the length of the analysis window in seconds. Default is 0.025s (25 milliseconds)\n    :param winstep: the step between successive windows in seconds. Default is 0.01s (10 milliseconds)\n    :param nfilt: the number of filters in the filterbank, default 26.\n    :param nfft: the FFT size. Default is 512.\n    :param lowfreq: lowest band edge of mel filters. In Hz, default is 0.\n    :param highfreq: highest band edge of mel filters. In Hz, default is samplerate/2\n    :param preemph: apply preemphasis filter with preemph as coefficient. 0 is no filter. Default is 0.97.\n    :param winfunc: the analysis window to apply to each frame. By default no window is applied. You can use numpy window functions here e.g. winfunc=numpy.hamming\n    :returns: A numpy array of size (NUMFRAMES by nfilt) containing features. Each row holds 1 feature vector.\n    """"""\n  highfreq = highfreq or samplerate / 2\n  signal = sigproc.preemphasis(signal, preemph)\n  frames = sigproc.framesig(signal, winlen * samplerate, winstep * samplerate,\n                            winfunc)\n  pspec = sigproc.powspec(frames, nfft)\n  pspec = numpy.where(pspec == 0,\n                      numpy.finfo(float).eps,\n                      pspec)  # if things are all zeros we get problems\n\n  fb = get_filterbanks(nfilt, nfft, samplerate, lowfreq, highfreq)\n  feat = numpy.dot(pspec, fb.T)  # compute the filterbank energies\n  R = numpy.tile(\n      numpy.linspace(1, samplerate / 2, numpy.size(pspec, 1)),\n      (numpy.size(pspec, 0), 1))\n\n  return numpy.dot(pspec * R, fb.T) / feat\n\n\ndef hz2mel(hz):\n  """"""Convert a value in Hertz to Mels\n\n    :param hz: a value in Hz. This can also be a numpy array, conversion proceeds element-wise.\n    :returns: a value in Mels. If an array was passed in, an identical sized array is returned.\n    """"""\n  return 2595 * numpy.log10(1 + hz / 700.)\n\n\ndef mel2hz(mel):\n  """"""Convert a value in Mels to Hertz\n\n    :param mel: a value in Mels. This can also be a numpy array, conversion proceeds element-wise.\n    :returns: a value in Hertz. If an array was passed in, an identical sized array is returned.\n    """"""\n  return 700 * (10**(mel / 2595.0) - 1)\n\n\ndef get_filterbanks(nfilt=20,\n                    nfft=512,\n                    samplerate=16000,\n                    lowfreq=0,\n                    highfreq=None):\n  """"""Compute a Mel-filterbank. The filters are stored in the rows, the columns correspond\n    to fft bins. The filters are returned as an array of size nfilt * (nfft/2 + 1)\n\n    :param nfilt: the number of filters in the filterbank, default 20.\n    :param nfft: the FFT size. Default is 512.\n    :param samplerate: the samplerate of the signal we are working with. Affects mel spacing.\n    :param lowfreq: lowest band edge of mel filters, default 0 Hz\n    :param highfreq: highest band edge of mel filters, default samplerate/2\n    :returns: A numpy array of size nfilt * (nfft/2 + 1) containing filterbank. Each row holds 1 filter.\n    """"""\n  highfreq = highfreq or samplerate / 2\n  assert highfreq <= samplerate / 2, ""highfreq is greater than samplerate/2""\n\n  # compute points evenly spaced in mels\n  lowmel = hz2mel(lowfreq)\n  highmel = hz2mel(highfreq)\n  melpoints = numpy.linspace(lowmel, highmel, nfilt + 2)\n  # our points are in Hz, but we use fft bins, so we have to convert\n  #  from Hz to fft bin number\n  bins = numpy.floor((nfft + 1) * mel2hz(melpoints) / samplerate)\n\n  fbanks = numpy.zeros([nfilt, nfft // 2 + 1])\n  for j in range(0, nfilt):\n    for i in range(int(bins[j]), int(bins[j + 1])):\n      fbanks[j, i] = (i - bins[j]) / (bins[j + 1] - bins[j])\n    for i in range(int(bins[j + 1]), int(bins[j + 2])):\n      fbanks[j, i] = (bins[j + 2] - i) / (bins[j + 2] - bins[j + 1])\n  return fbanks\n\n\ndef lifter(cepstra, L=22):\n  """"""Apply a cepstral lifter the the matrix of cepstra. This has the effect of increasing the\n    magnitude of the high frequency DCT coeffs.\n\n    :param cepstra: the matrix of mel-cepstra, will be numframes * numcep in size.\n    :param L: the liftering coefficient to use. Default is 22. L <= 0 disables lifter.\n    """"""\n  if L > 0:\n    nframes, ncoeff = numpy.shape(cepstra)\n    n = numpy.arange(ncoeff)\n    lift = 1 + (L / 2.) * numpy.sin(numpy.pi * n / L)\n    return lift * cepstra\n  # values of L <= 0, do nothing\n  return cepstra\n\n\ndef delta(feat, N):\n  """"""Compute delta features from a feature vector sequence.\n\n    :param feat: A numpy array of size (NUMFRAMES by number of features) containing features. Each row holds 1 feature vector.\n    :param N: For each frame, calculate delta features based on preceding and following N frames\n    :returns: A numpy array of size (NUMFRAMES by number of features) containing delta features. Each row holds 1 delta feature vector.\n    """"""\n  if N < 1:\n    raise ValueError(\'N must be an integer >= 1\')\n  NUMFRAMES = len(feat)\n  denominator = 2 * sum([i**2 for i in range(1, N + 1)])\n  delta_feat = numpy.empty_like(feat)\n  padded = numpy.pad(\n      feat, ((N, N), (0, 0)), mode=\'edge\')  # padded version of feat\n  for t in range(NUMFRAMES):\n    delta_feat[t] = numpy.dot(\n        numpy.arange(-N, N + 1),\n        padded[t:t + 2 * N +\n               1]) / denominator  # [t : t+2*N+1] == [(N+t)-N : (N+t)+N+1]\n  return delta_feat\n'"
delta/data/feat/python_speech_features/example.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n#!/usr/bin/env python3\n\'\'\' Example for sigproc.py \'\'\'\n\n# pylint: skip-file\n\nimport scipy.io.wavfile as wav\n\nfrom base import mfcc\nfrom base import delta\nfrom base import logfbank\n\nif __name__ == \'__main__\':\n  (rate, sig) = wav.read(""english.wav"")\n  mfcc_feat = mfcc(sig, rate)\n  d_mfcc_feat = delta(mfcc_feat, 2)\n  fbank_feat = logfbank(sig, rate)\n\n  print(fbank_feat[1:3, :])\n'"
delta/data/feat/python_speech_features/sigproc.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\'\nThis file includes routines for basic signal processing including\nframing and computing power spectra.\n# Author: James Lyons 2012\n\'\'\'\n\nimport decimal\nimport math\nimport logging\n\nimport numpy\n\n# pylint: disable=line-too-long\n\n\ndef round_half_up(number):\n  \'\'\' To nearest with ties going away from zero. \'\'\'\n  return int(\n      decimal.Decimal(number).quantize(\n          decimal.Decimal(\'1\'), rounding=decimal.ROUND_HALF_UP))\n\n\ndef rolling_window(sig, window, step=1):\n  \'\'\' Apply rolling window. \'\'\'\n  # http://ellisvalentiner.com/post/2017-03-21-np-strides-trick\n  shape = sig.shape[:-1] + (sig.shape[-1] - window + 1, window)\n  strides = sig.strides + (sig.strides[-1],)\n  return numpy.lib.stride_tricks.as_strided(\n      sig, shape=shape, strides=strides)[::step]\n\n\ndef framesig(sig,\n             frame_len,\n             frame_step,\n             winfunc=lambda x: numpy.ones((x,)),\n             stride_trick=True):\n  """"""Frame a signal into overlapping frames.\n\n    :param sig: the audio signal to frame.\n    :param frame_len: length of each frame measured in samples.\n    :param frame_step: number of samples after the start of the previous frame that the next frame should begin.\n    :param winfunc: the analysis window to apply to each frame. By default no window is applied.\n    :param stride_trick: use stride trick to compute the rolling window and window multiplication faster\n    :returns: an array of frames. Size is NUMFRAMES by frame_len.\n    """"""\n  slen = len(sig)\n  frame_len = int(round_half_up(frame_len))\n  frame_step = int(round_half_up(frame_step))\n  if slen <= frame_len:\n    numframes = 1\n  else:\n    numframes = 1 + int(math.ceil((1.0 * slen - frame_len) / frame_step))\n\n  padlen = int((numframes - 1) * frame_step + frame_len)\n\n  zeros = numpy.zeros((padlen - slen,))\n  padsignal = numpy.concatenate((sig, zeros))\n  if stride_trick:\n    win = winfunc(frame_len)\n    frames = rolling_window(padsignal, window=frame_len, step=frame_step)\n  else:\n    indices = numpy.tile(numpy.arange(\n        0, frame_len), (numframes, 1)) + numpy.tile(\n            numpy.arange(0, numframes * frame_step, frame_step),\n            (frame_len, 1)).T\n    indices = numpy.array(indices, dtype=numpy.int32)\n    frames = padsignal[indices]\n    win = numpy.tile(winfunc(frame_len), (numframes, 1))\n\n  return frames * win\n\n\ndef deframesig(frames,\n               siglen,\n               frame_len,\n               frame_step,\n               winfunc=lambda x: numpy.ones((x,))):\n  """"""Does overlap-add procedure to undo the action of framesig.\n\n    :param frames: the array of frames.\n    :param siglen: the length of the desired signal, use 0 if unknown. Output will be truncated to siglen samples.\n    :param frame_len: length of each frame measured in samples.\n    :param frame_step: number of samples after the start of the previous frame that the next frame should begin.\n    :param winfunc: the analysis window to apply to each frame. By default no window is applied.\n    :returns: a 1-D signal.\n    """"""\n  frame_len = round_half_up(frame_len)\n  frame_step = round_half_up(frame_step)\n  numframes = numpy.shape(frames)[0]\n  assert numpy.shape(\n      frames\n  )[1] == frame_len, \'""frames"" matrix is wrong size, 2nd dim is not equal to frame_len\'\n\n  indices = numpy.tile(numpy.arange(0, frame_len), (numframes, 1)) + numpy.tile(\n      numpy.arange(0, numframes * frame_step, frame_step), (frame_len, 1)).T\n  indices = numpy.array(indices, dtype=numpy.int32)\n  padlen = (numframes - 1) * frame_step + frame_len\n\n  if siglen <= 0:\n    siglen = padlen\n\n  rec_signal = numpy.zeros((padlen,))\n  window_correction = numpy.zeros((padlen,))\n  win = winfunc(frame_len)\n\n  for i in range(0, numframes):\n    window_correction[indices[i, :]] = window_correction[\n        indices[i, :]] + win + 1e-15  # add a little bit so it is never zero\n    rec_signal[indices[i, :]] = rec_signal[indices[i, :]] + frames[i, :]\n\n  rec_signal = rec_signal / window_correction\n  return rec_signal[0:siglen]\n\n\ndef magspec(frames, nfft):\n  """"""Compute the magnitude spectrum of each frame in frames. If frames is an NxD matrix, output will be Nx(NFFT/2+1).\n\n    :param frames: the array of frames. Each row is a frame.\n    :param NFFT: the FFT length to use. If NFFT > frame_len, the frames are zero-padded.\n    :returns: If frames is an NxD matrix, output will be Nx(NFFT/2+1). Each row will be the magnitude spectrum of the corresponding frame.\n    """"""\n  if numpy.shape(frames)[1] > nfft:\n    logging.warning(\n        \'frame length (%d) is greater than FFT size (%d), frame will be truncated. Increase NFFT to avoid.\',\n        numpy.shape(frames)[1], nfft)\n  complex_spec = numpy.fft.rfft(frames, nfft)\n  return numpy.absolute(complex_spec)\n\n\ndef powspec(frames, nfft):\n  """"""Compute the power spectrum of each frame in frames. If frames is an NxD matrix, output will be Nx(NFFT/2+1).\n\n    :param frames: the array of frames. Each row is a frame.\n    :param NFFT: the FFT length to use. If NFFT > frame_len, the frames are zero-padded.\n    :returns: If frames is an NxD matrix, output will be Nx(NFFT/2+1). Each row will be the power spectrum of the corresponding frame.\n    """"""\n  return 1.0 / nfft * numpy.square(magspec(frames, nfft))\n\n\ndef logpowspec(frames, nfft, norm=1):\n  """"""Compute the log power spectrum of each frame in frames. If frames is an NxD matrix, output will be Nx(NFFT/2+1).\n\n    :param frames: the array of frames. Each row is a frame.\n    :param NFFT: the FFT length to use. If NFFT > frame_len, the frames are zero-padded.\n    :param norm: If norm=1, the log power spectrum is normalised so that the max value (across all frames) is 0.\n    :returns: If frames is an NxD matrix, output will be Nx(NFFT/2+1). Each row will be the log power spectrum of the corresponding frame.\n    """"""\n  power = powspec(frames, nfft)\n  power[power <= 1e-30] = 1e-30\n  log_power = 10 * numpy.log10(power)\n  if norm:\n    return log_power - numpy.max(log_power)\n  return log_power\n\n\ndef preemphasis(signal, coeff=0.95):\n  """"""perform preemphasis on the input signal.\n\n    :param signal: The signal to filter.\n    :param coeff: The preemphasis coefficient. 0 is no filter, default is 0.95.\n    :returns: the filtered signal.\n    """"""\n  return numpy.append(signal[0], signal[1:] - coeff * signal[:-1])\n'"
delta/utils/solver/utils/__init__.py,0,b''
delta/utils/solver/utils/callbacks.py,4,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Callback utilities.\'\'\'\n\nfrom absl import logging\n\nimport delta.compat as tf\nimport numpy as np\n#pylint: disable=import-error\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras import backend as K\nfrom sklearn import metrics\n#pylint: disable=no-name-in-module\nfrom tensorflow.python.data.ops import iterator_ops\nfrom tensorflow.python.data.ops import dataset_ops\n\nfrom delta.utils.decode import py_ctc\nfrom delta.utils.decode import tf_ctc\nfrom delta.utils import metrics as metrics_lib\n\n\n#pylint: disable=too-few-public-methods\nclass TokenErrMetricCallBack(Callback):\n  \'\'\'Callback to compute specific metric and logs during train and eval\'\'\'\n\n  def __init__(self, func, eval_ds, eval_task, decoder_type):\n    self.func = func\n    self.eval_task = eval_task\n    self.eval_ds = eval_ds\n    self.next_batch_gen = None\n    self.decoder_type = decoder_type\n\n  #pylint: disable=dangerous-default-value\n  def on_epoch_end(self, epoch, logs={}):\n    \'\'\'computing token error\'\'\'\n\n    cur_session = tf.keras.backend.get_session()\n    target_seq_list, predict_seq_list = [], []\n\n    is_py_sequence = True\n    if isinstance(self.eval_ds, (dataset_ops.DatasetV2, dataset_ops.DatasetV1)):\n      eval_gen = self.eval_ds.make_one_shot_iterator()\n      self.next_batch_gen = eval_gen.get_next()[0]\n      is_py_sequence = False\n    elif isinstance(self.eval_ds,\n                    (iterator_ops.IteratorV2, iterator_ops.Iterator)):\n      self.next_batch_gen = self.ds.get_next()[0]\n      is_py_sequence = False\n\n    for index in range(len(self.eval_task)):\n      batch_data = None\n      if is_py_sequence:\n        batch_data = self.eval_ds[index][0]\n      else:\n        batch_data = cur_session.run(self.next_batch_gen)\n      batch_input = batch_data[\'inputs\']\n      batch_target = batch_data[\'targets\'].tolist()\n      batch_predict = self.func(batch_input)[0]\n\n      if self.decoder_type == \'argmax\':\n        predict_seq_list += py_ctc.ctc_greedy_decode(\n            batch_predict, 0, unique=True)\n      else:\n        sequence_lens = [len(pre_sequence) for pre_sequence in batch_predict]\n        batch_decoder, _ = tf_ctc.ctc_beam_search_decode(\n            tf.constant(batch_predict),\n            tf.constant(sequence_lens),\n            beam_width=3,\n            top_paths=3)\n        predict_seq_list += cur_session.run(batch_decoder)[0].tolist()\n      target_seq_list += batch_target\n\n    val_token_errors = metrics_lib.token_error(\n        predict_seq_list=predict_seq_list,\n        target_seq_list=target_seq_list,\n        eos_id=0)\n    logs[\'val_token_err\'] = val_token_errors\n\n    if \'val_loss\' in logs:\n      logging.info(""Epoch {}: on eval, val_loss is {}."".format(\n          epoch + 1, logs[\'val_loss\']))\n    logging.info(""Epoch {}: on eval, token_err is {}."".format(\n        epoch + 1, val_token_errors))\n    logging.info(""Epoch {}: loss on train is {}"".format(epoch + 1,\n                                                        logs[\'loss\']))\n\n\nclass ClassReportCallBack(Callback):\n  def __init__(self, model, eval_ds, eval_task):\n    self.model = model\n    self.eval_task = eval_task\n    self.eval_ds = eval_ds\n    self.next_batch_gen = None\n\n  def on_epoch_end(self, epoch, logs={}):\n    \'\'\'computing every class prec/rec\'\'\'\n\n    cur_session = tf.keras.backend.get_session()\n    truth, predict = [], []\n\n    is_py_sequence = True\n    if isinstance(self.eval_task, (dataset_ops.DatasetV2, dataset_ops.DatasetV1)):\n      eval_gen = self.eval_task.make_one_shot_iterator()\n      self.next_batch_gen = eval_gen.get_next()\n      is_py_sequence = False\n    elif isinstance(self.eval_task,\n                    (iterator_ops.IteratorV2, iterator_ops.Iterator)):\n      self.next_batch_gen = self.ds.get_next()\n      is_py_sequence = False\n\n    for index in range(len(self.eval_task)):\n      batch_data = None\n      if is_py_sequence:\n        batch_data, batch_truth = self.eval_task[index]\n      else:\n        batch_data = cur_session.run(self.next_batch_gen)\n      #print(""batch_data"", batch_data)\n      batch_input = batch_data\n      batch_truth = batch_truth.tolist()\n\n      text = self.model.get_layer(\'text\').input\n      speech = self.model.get_layer(\'speech\').input\n      y_pred = self.model(batch_input)\n      f = K.function([text, speech], y_pred)\n      batch_predict = f([batch_input[\'inputs\'], batch_input[\'texts\']])\n      truth.extend(batch_truth)\n      predict.extend(batch_predict)\n    y_true = np.argmax(np.asarray(truth), axis=1)\n    y_pred = np.argmax(np.asarray(predict), axis=1)\n    accuracy = metrics.accuracy_score(y_true, y_pred)\n    unw_accuracy = metrics.precision_score(y_true, y_pred, average=\'macro\')\n    logs[\'ClassReport\'] = accuracy\n    logging.info(""Epoch {}: on eval."".format(\n        epoch + 1))\n    logging.info(""Weighted accuracy: {}"".format(accuracy))\n    logging.info(""Unweighted accuracy: {}"".format(unw_accuracy))\n    logging.info(""Specific results: {}"".format(\'\\n\' + metrics.classification_report(\n      y_true, y_pred, digits=4)))\n\n\nclass ParallelModelCheckpoint(ModelCheckpoint):\n  \'\'\'Callback to save multi_gpu_model\'\'\'\n\n  #pylint: disable=too-many-arguments\n  def __init__(self,\n               model,\n               filepath,\n               monitor=\'val_loss\',\n               verbose=0,\n               save_best_only=False,\n               save_weights_only=False,\n               mode=\'auto\',\n               save_freq=\'epoch\',\n               load_weights_on_restart=False,\n               period=1):\n    self.model_to_save = model\n    super().__init__(\n        filepath=filepath,\n        monitor=monitor,\n        verbose=verbose,\n        save_best_only=save_best_only,\n        save_weights_only=save_weights_only,\n        mode=mode,\n        save_freq=save_freq,\n        load_weights_on_restart=load_weights_on_restart,\n        period=period)\n\n  #pylint: disable=unused-argument\n  def set_model(self, model):\n    \'\'\'set the model to saved\'\'\'\n    super().set_model(self.model_to_save)\n\n'"
delta/utils/solver/utils/hooks.py,10,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Hooks""""""\n\nimport delta.compat as tf\nfrom bisect import bisect_right\nfrom absl import logging\n\n\nclass DatasetInitializerHook(tf.estimator.SessionRunHook):\n  \'\'\' iterator dataset initailizer \'\'\'\n\n  def __init__(self, iterator, init_feed_dict):\n    self._iterator = iterator\n    self._init_feed_dict = init_feed_dict\n\n  def begin(self):\n    self._initializer = self._iterator.initializer\n\n  def after_create_session(self, session, coord):\n    del coord\n    session.run(self._initializer, self._init_feed_dict)\n\n\nclass EpochHook(tf.estimator.SessionRunHook):\n\n  def __init__(self, examples_per_epoch, global_batch_size):\n    self._num_examples_per_epoch = examples_per_epoch\n    self._global_batch_size = global_batch_size\n    self._epoch = 0\n\n  @property\n  def epoch(self):\n    return self._epoch\n\n  def begin(self):\n    self._global_step_tensor = tf.train.get_or_create_global_step()\n    if self._global_step_tensor is None:\n      raise RuntimeError(""Global step should be created to use StopAtStepHook."")\n    self._epoch_tensor = (self._global_step_tensor * tf.constant(\n        self._num_examples_per_epoch)) / tf.constant(self._global_batch_size)\n\n  def after_create_session(self, session, coord):\n    pass\n\n  def before_run(self, run_context):  # pylint: disable=unused-argument\n    return tf.train.SessionRunArgs(self._global_step_tensor)\n\n  def after_run(self, run_context, run_values):\n    global_step = run_values.results + 1\n\n    # Check latest global step to ensure that the targeted last step is\n    # reached. global_step read tensor is the value of global step\n    # before running the operation. We\'re not sure whether current session.run\n    # incremented the global_step or not. Here we\'re checking it.\n\n    step = run_context.session.run(self._global_step_tensor)\n    assert step == global_step\n    self._epoch = int(\n        (self._global_batch_size * step) / self._num_examples_per_epoch)\n    logging.info(f""{self.__class__.__name__}: Epoch {self.epoch}"")\n\n  def end(self, session):\n    pass\n\n\nclass MultiStepLRHook(tf.estimator.SessionRunHook):\n  \'\'\' Set the learning rate of each parameter group to the initial lr decayed \n      by gamma once the number of epoch reaches one of the milestones. \n      When last_epoch=-1, sets initial lr as lr.\n  params:\n    lr (flaot) : init learning rate\n    milestones (list) : List of epoch indices. Must be increasing.\n    gamma (float) : Multiplicative factor of learning rate decay. Default: 0.1.\n    last_epoch (int) : The index of last epoch. Default: -1.\n  \'\'\'\n\n  def __init__(self, lr, milestones, gamma=0.1, last_epoch=-1):\n    if not list(milestones) == sorted(milestones):\n      raise ValueError(\n          \'Milestones should be a list of\'\n          \' increasing integers. Got {}\', milestones)\n    self._milestones = milestones\n    self._lrn_rate = lr\n    self._gamma = gamma\n    self._last_epoch = last_epoch\n\n  def begin(self):\n    self._global_step_tensor = tf.train.get_or_create_global_step()\n    self._lrn_rate_tensor = tf.get_default_graph().get_tensor_by_name(\n        \'learning_rate:0\')\n\n  def after_create_session(self, session, coord):\n    pass\n\n  def before_run(self, run_context):\n    return tf.train.SessionRunArgs(\n        self._global_step_tensor,  # Asks for global step value.\n        feed_dict={self._lrn_rate_tensor: self._lrn_rate})  # Sets learning rate\n\n  def after_run(self, run_context, run_values):\n    train_step = run_values.results\n    self._lrn_rate = self.get_lr()\n\n  def get_lr(self):\n    return self._lrn_rate * self._gamma**bisect_right(self._milestones,\n                                                      self.last_epoch)\n\n  def end(self, session):\n    pass\n'"
delta/utils/solver/utils/solver_utils.py,8,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Solver utilities.""""""\n\nimport os\nimport re\nfrom pathlib import Path\nimport numpy as np\nimport delta.compat as tf\nfrom absl import logging\nimport shutil\n\nfrom delta import utils\nfrom delta.utils import metrics\n\n\ndef get_checkpoint_dir(config):\n  """"""Get the directory of the checkpoint.""""""\n  model_path = config[\'solver\'][\'saver\'][\'model_path\']\n  checkpoint_dir = os.path.join(model_path, ""model"")\n  return checkpoint_dir\n\n\ndef get_ckpt_state(config):\n  """"""Get the checkpoint state.""""""\n  checkpoint_dir = get_checkpoint_dir(config)\n  ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n  return ckpt\n\n\ndef get_session_conf(config):\n  """"""Get the config for the tensorflow session.""""""\n  tfconf = config[\'solver\'][\'run_config\']\n  session_conf = tf.ConfigProto(\n      allow_soft_placement=tfconf[\'allow_soft_placement\'],\n      log_device_placement=tfconf[\'log_device_placement\'],\n      intra_op_parallelism_threads=tfconf[\'intra_op_parallelism_threads\'],\n      inter_op_parallelism_threads=tfconf[\'inter_op_parallelism_threads\'],\n      gpu_options=tf.GPUOptions(allow_growth=tfconf[\'allow_growth\']))\n  return session_conf\n\n\ndef to_saved_model(config, sess, inputs: dict, outputs: dict):\n  """"""Save model to tensorflow SavedModel.""""""\n  export_path_base = config[""solver""][""service""][""model_path""]\n  model_version = config[""solver""][""service""][""model_version""]\n  export_path = os.path.join(\n      tf.compat.as_bytes(export_path_base), tf.compat.as_bytes(model_version))\n  export_path = os.path.abspath(export_path)\n  logging.info(\'Exporting model to: {}\'.format(export_path))\n  if os.path.exists(export_path):\n    files = [\n        one.decode()\n        for one in os.listdir(export_path)\n        if isinstance(one, bytes)\n    ]\n    if ""variables"" in files:\n      cmd = input(\n          f""Export directory already exists, and isn\'t empty. Overwrite? [y/n]""\n      ).strip().lower()\n      if cmd == """" or cmd == ""y"":\n        shutil.rmtree(export_path)\n  builder = tf.saved_model.builder.SavedModelBuilder(export_path)\n  # Build the signature_def_map.\n  signature_def = tf.saved_model.predict_signature_def(inputs, outputs)\n  builder.add_meta_graph_and_variables(\n      sess, [tf.saved_model.tag_constants.SERVING],\n      signature_def_map={\'infer\': signature_def},\n      strip_default_attrs=True)\n  builder.save(as_text=True)\n  logging.info(\'Done exporting!\')\n\n\ndef save_infer_res(config, logits, preds):\n  """"""Save the result of inference.""""""\n  res_file = config[""data""][""infer""][""res""]\n  res_dir = os.path.dirname(res_file)\n  if not os.path.exists(res_dir):\n    os.makedirs(res_dir)\n  logging.info(""Save inference result to: {}"".format(res_file))\n  with open(res_file, ""w"") as in_f:\n    for logit, pred in zip(logits, preds):\n      in_f.write("" "".join([""{:.3f}"".format(num) for num in logit]) +\n                 ""\\t{}\\n"".format(pred))\n\n\ndef get_model_file(dir_name, file_name_pattern, mode, model_load_type,\n                   specified_model_file_name):\n  """"""\n    Return model file according the specified model_load_type\n    :param dir_name: the folder path where a file search will start\n    :param file_name_pattern: the filename pattern that will be matched,\n                              when searching model file with model_load_type=latest\n    :param mode: which kind of command is performing [train, eval, infer]\n    :param model_load_type: restore which kind of model [best, lastest, scratch, specific]\n    :param specified_model_file_name: the model file which will be restored \n                                      with model_load_type=specific\n    """"""\n  assert model_load_type in (None, ""best"", ""latest"", ""scratch"", ""specific"")\n\n  if model_load_type is None:\n    logging.warning(""The values of model_load_type is not specified."")\n    model_load_type = ""latest"" if mode == utils.TRAIN else ""best""\n    logging.warning(""For the {} command, model_load_type:{} is adopted."".format(\n        mode, model_load_type))\n\n  #model_load_type can not be \'scratch\' when performing EVAL or INFER command\n  if model_load_type == \'scratch\' and mode != utils.TRAIN:\n    model_load_type = ""best""\n    logging.warning(\n        ""The model_load_type cannot be scratch when performing {} command, and is changed to {}""\n        .format(mode, model_load_type))\n\n  #get the path of model file according the specificed model_load_type\n  model_file_name = None\n  if model_load_type == ""specific"":\n    model_file_name = Path(dir_name).joinpath(specified_model_file_name)\n    #the value of model_load_type will be changed to latest when specified_model_file_name is None\n    if not model_file_name.exists():\n      model_load_type = ""latest""\n      logging.warning(\n          ""The specified model file {} is not exist, model_load_type:{} is adopted""\n          .format(model_file_name, model_load_type))\n\n  if model_load_type == ""latest"":\n    model_file_name = get_most_recently_modified_file_matching_pattern(\n        dir_name, file_name_pattern)\n  elif model_load_type == ""best"":\n    model_file_name = Path(dir_name).joinpath(\'best_model.ckpt\')\n\n  #verify the existence of the file\n  #model_file_name will be None when\n  #     1.model_load_type=scratch\n  #     2.no model_file is found with model_load_type=latest\n  if model_file_name is None:\n    logging.warning(\n        \'No model file is found in {} with model_load_type={}\'.format(\n            dir_name, model_load_type))\n    if mode == utils.TRAIN:\n      model_load_type = \'scratch\'\n      model_file_name = None\n      logging.warning(\'The model will be trained with model_load_type:scratch\')\n    else:\n      assert False, \'{} END, since no model file is found\'.format(mode)\n\n  return model_load_type, model_file_name\n\n\ndef get_most_recently_modified_file_matching_pattern(dir_name,\n                                                     file_name_pattern):\n  """"""Return the most recently checkpoint file matching file_name_pattern""""""\n  file_name_regex = \'^\' + re.sub(r\'{.*}\', r\'.*\', file_name_pattern) + \'$\'\n\n  tf_checkpoint_file = tf.train.latest_checkpoint(dir_name)\n  if tf_checkpoint_file is not None and re.match(file_name_regex,\n                                                 tf_checkpoint_file):\n    return tf_checkpoint_file\n\n  file_list = [\n      file_name for file_name in Path(dir_name).iterdir()\n      if re.match(file_name_regex, file_name.name)\n  ]\n  file_time_list = [single_file.stat().st_mtime for single_file in file_list]\n  file_sort_by_time = np.argsort(file_time_list)\n  latest_file = file_list[\n      file_sort_by_time[-1]] if file_sort_by_time.shape[0] > 0 else None\n  return latest_file\n\n\ndef run_metrics(config, y_preds, y_ground_truth, mode):\n  """"""Run metrics for one output""""""\n  metcs = metrics.get_metrics(\n      config=config, y_pred=y_preds, y_true=y_ground_truth)\n  logging.info(""Evaluation on %s:"" % mode)\n  if isinstance(metcs, list):\n    for one_metcs in metcs:\n      for key in sorted(one_metcs.keys()):\n        logging.info(key + "":"" + str(one_metcs[key]))\n  else:\n    for key in sorted(metcs.keys()):\n      logging.info(key + "":"" + str(metcs[key]))\n'"
delta/utils/solver/utils/solver_utils_test.py,3,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Test for solver_utils.py \'\'\'\n\nimport time\nfrom pathlib import Path\nimport delta.compat as tf\nfrom absl import logging\n\nfrom delta import utils\nfrom delta.utils.solver.utils import solver_utils\n\n# pylint: disable=missing-docstring\n\n\nclass SolverUtilsTest(tf.test.TestCase):\n  \'\'\' Unit test for solver_utils. \'\'\'\n\n  def setUp(self):\n    super().setUp()\n    self.model_path = self.get_temp_dir()\n    self.file_name_pattern = \'model.{epoch:02d}-{monitor:02f}.ckpt\'\n    self.specified_model_file_name = \'model.09-1.00.ckpt\'\n\n  # pylint: disable=too-many-locals\n  def test_get_model_file(self):\n    \'\'\' get model file according the specified model_load_type unittest\'\'\'\n    model_load_dict = {\n        \'best\':\n            Path(self.model_path).joinpath(\'best_model.ckpt\'),\n        \'specific\':\n            Path(self.model_path).joinpath(self.specified_model_file_name),\n        \'latest\':\n            Path(self.model_path).joinpath(\'model.00-1.00.ckpt\')\n    }\n\n    # There is no model file in model_path\n    mode_list = [utils.EVAL, utils.INFER]\n    for cur_mode in mode_list:\n      model_load_type_list = [""specific"", ""latest""]\n      for cur_model_load_type in model_load_type_list:\n        with self.assertRaises(AssertionError) as assert_err:\n          _, _ = solver_utils.get_model_file(\n              dir_name=self.model_path,\n              file_name_pattern=self.file_name_pattern,\n              mode=cur_mode,\n              model_load_type=cur_model_load_type,\n              specified_model_file_name=self.specified_model_file_name)\n        the_exception = assert_err.exception\n        self.assertEqual(\n            str(the_exception),\n            \'{} END, since no model file is found\'.format(cur_mode))\n\n      model_load_type_list = [None, \'scratch\', \'best\']\n      for cur_model_load_type in model_load_type_list:\n        model_load_type, model_file_name = solver_utils.get_model_file(\n            dir_name=self.model_path,\n            file_name_pattern=self.file_name_pattern,\n            mode=cur_mode,\n            model_load_type=cur_model_load_type,\n            specified_model_file_name=self.specified_model_file_name)\n        self.assertEqual(\'best\', model_load_type)\n        self.assertEqual(model_load_dict[\'best\'], model_file_name)\n\n    cur_mode = utils.TRAIN\n    model_load_type_list = [None, \'scratch\', \'specific\', ""latest""]\n    for cur_model_load_type in model_load_type_list:\n      model_load_type, model_file_name = solver_utils.get_model_file(\n          dir_name=self.model_path,\n          file_name_pattern=self.file_name_pattern,\n          mode=cur_mode,\n          model_load_type=cur_model_load_type,\n          specified_model_file_name=self.specified_model_file_name)\n      self.assertEqual(model_load_type, \'scratch\')\n      self.assertIsNone(model_file_name)\n\n    cur_model_load_type = \'best\'\n    model_load_type, model_file_name = solver_utils.get_model_file(\n        dir_name=self.model_path,\n        file_name_pattern=self.file_name_pattern,\n        mode=cur_mode,\n        model_load_type=cur_model_load_type,\n        specified_model_file_name=self.specified_model_file_name)\n    self.assertEqual(model_load_type, cur_model_load_type)\n    self.assertEqual(model_file_name, model_load_dict[cur_model_load_type])\n    # create model files in model_path\n    file_name_list = [\n        \'best_model.ckpt\', \'model.09-1.00.ckpt\', \'model.00-1.00.ckpt\'\n    ]\n    for file_name in file_name_list:\n      file_path = Path(self.model_path).joinpath(file_name)\n      with open(file_path, \'w\', encoding=\'utf-8\') as f:  #pylint: disable=invalid-name\n        f.write(\'test ckpt file\')\n        time.sleep(1)\n\n    all_mode_list = [utils.TRAIN, utils.EVAL, utils.INFER]\n    for cur_mode in all_mode_list:\n      for cur_model_load_type in model_load_dict:\n        model_load_type, model_file_name = solver_utils.get_model_file(\n            dir_name=self.model_path,\n            file_name_pattern=self.file_name_pattern,\n            mode=cur_mode,\n            model_load_type=cur_model_load_type,\n            specified_model_file_name=self.specified_model_file_name)\n        self.assertEqual(cur_model_load_type, model_load_type)\n        self.assertEqual(model_load_dict[cur_model_load_type], model_file_name)\n\n    model_load_type_list = [\'scratch\', None]\n    mode_list = [utils.EVAL, utils.INFER]\n    for cur_model_load_type in model_load_type_list:\n      for cur_mode in mode_list:\n        model_load_type, model_file_name = solver_utils.get_model_file(\n            dir_name=self.model_path,\n            file_name_pattern=self.file_name_pattern,\n            mode=cur_mode,\n            model_load_type=cur_model_load_type,\n            specified_model_file_name=self.specified_model_file_name)\n        self.assertEqual(\'best\', model_load_type)\n        self.assertEqual(\n            Path(self.model_path).joinpath(\'best_model.ckpt\'), model_file_name)\n\n    cur_mode = utils.TRAIN\n    cur_model_load_type = \'scratch\'\n    model_load_type, model_file_name = solver_utils.get_model_file(\n        dir_name=self.model_path,\n        file_name_pattern=self.file_name_pattern,\n        mode=cur_mode,\n        model_load_type=cur_model_load_type,\n        specified_model_file_name=self.specified_model_file_name)\n    self.assertEqual(cur_model_load_type, model_load_type)\n    self.assertIsNone(model_file_name)\n\n    cur_model_load_type = None\n    model_load_type, model_file_name = solver_utils.get_model_file(\n        dir_name=self.model_path,\n        file_name_pattern=self.file_name_pattern,\n        mode=cur_mode,\n        model_load_type=cur_model_load_type,\n        specified_model_file_name=self.specified_model_file_name)\n    self.assertEqual(\'latest\', model_load_type)\n    self.assertEqual(\n        Path(self.model_path).joinpath(\'model.00-1.00.ckpt\'), model_file_name)\n\n  def test_get_most_recently_modified_file_matching_pattern(self):\n    \'\'\' get the most recently modified model file matching pattern unittest\'\'\'\n    # There is no model file in model_path\n    most_rencently_modified_file = solver_utils.get_most_recently_modified_file_matching_pattern(\n        self.model_path, self.file_name_pattern)\n    self.assertIsNone(most_rencently_modified_file)\n\n    #pylint: disable=invalid-name\n    with open(\n        Path(self.model_path).joinpath(\'model.01-1.00.h5\'),\n        \'w\',\n        encoding=\'utf-8\') as f:\n      f.write(\'test ckpt file\')\n    most_rencently_modified_file = solver_utils.get_most_recently_modified_file_matching_pattern(\n        self.model_path, self.file_name_pattern)\n    self.assertIsNone(most_rencently_modified_file)\n\n    file_name_list = [\n        \'model.01-1.00.ckpt\', \'model.02-1.00.ckpt\', \'model.03-1.00.ckpt\'\n    ]\n    for file_name in file_name_list:\n      file_path = Path(self.model_path).joinpath(file_name)\n      with open(file_path, \'w\', encoding=\'utf-8\') as f:  #pylint: disable=invalid-name\n        f.write(\'test ckpt file\')\n        time.sleep(1)\n    most_rencently_modified_file = solver_utils.get_most_recently_modified_file_matching_pattern(\n        self.model_path, self.file_name_pattern)\n    self.assertEqual(most_rencently_modified_file.name, file_name_list[-1])\n\n\nif __name__ == ""__main__"":\n  logging.set_verbosity(logging.INFO)\n  tf.enable_eager_execution()\n  tf.test.main()\n'"
deltann/infer/example/python/complex_transformer.py,9,"b'import tensorflow as tf\nimport delta_infer as dti\nfrom nlp_transformer.model import *\n\n@dti.RegistPattern(name=""TransformerCellNLP"")\ndef TransformerCellTypeNLP(output=None,\n                         pos_emb=None,\n                         r_w_bias=None,\n                         r_r_bias=None,\n                         attn_mask=None,\n                         mems=None,\n                         d_model = 256,\n                         n_head = 6,\n                         d_head = 64,\n                         dropout = 0.0,\n                         dropatt = 0.0,\n                         is_training = False,\n                         initializer=None,\n                         index=0,\n                         d_inner=1024):\n    with tf.compat.v1.variable_scope(\'layer_{}\'.format(index)):\n        output = rel_multihead_attn(\n        w=output,\n        r=pos_emb,\n        r_w_bias=r_w_bias,\n        r_r_bias=r_r_bias,\n        attn_mask=attn_mask,\n        mems=mems,#[index],\n        d_model=d_model,\n        n_head=n_head,\n        d_head=d_head,\n        dropout=dropout,\n        dropatt=dropatt,\n        is_training=is_training,\n        kernel_initializer=initializer,\n        index = index)\n    output = positionwise_FF(\n        inp=output,\n        d_model=d_model,\n        d_inner=d_inner,\n        dropout=dropout,\n        kernel_initializer=initializer,\n        is_training=is_training)\n    return output\n\nif __name__ == ""__main__"":\n    # open graph optimizer stream\n    with dti.GraphStream(""./model.pb"") as gs:\n    #with dti.GraphStream(""./nlp_fasttransformer_cell.pb"") as gs:\n        qlen = None\n        mlen = 16\n        output = tf.compat.v1.placeholder(tf.float32, shape=(None, 4, 256 ))\n        pos_emb = tf.compat.v1.placeholder(tf.float32, shape=(None, 1, 256 )) # None = qlen+mlen\n        r_w_bias = tf.compat.v1.placeholder(tf.float32, shape=(6, 64))\n        r_r_bias = tf.compat.v1.placeholder(tf.float32, shape=(6, 64))\n        attn_mask = tf.compat.v1.placeholder(tf.float32, shape=(None, None)) # None = qlen+mlen\n        mems = tf.compat.v1.placeholder(tf.float32, shape=(mlen, 4, 256))\n\n        initializer = tf.initializers.random_normal(stddev=0.02, seed=None)\n\n        TransformerCellTypeNLP(output = output,\n                               pos_emb = pos_emb,\n                               r_w_bias = r_w_bias,\n                               r_r_bias = r_r_bias,\n                               attn_mask = attn_mask,\n                               mems = mems,\n                               d_model = 256,\n                               n_head = 6,\n                               d_head = 64,\n                               initializer = initializer,\n                               d_inner = 1024)\n\n        # remove in the future\n        gs.register_hint_op(""TransformerCellNLP"", ""BatchMatMulV2"")\n        gs.save(""./result.pb"")\n\n    #with tf.compat.v1.Session() as sess:\n    #    graph_def = dti.RegistPattern.get_patterns(""TransformerCellNLP"")[0]\n    #    with open(""TransformerCellNLP.pb"", ""wb"") as f:\n    #        f.write(graph_def.SerializeToString())\n    #sess.close()\n'"
deltann/infer/example/python/simple_bert_transformer.py,13,"b'import tensorflow as tf\nimport delta_infer as dti\nfrom standard_transformer.model import *\n\n@dti.RegistPattern(name=""TransformerCellBertOther"")\ndef standard_transformer(input_tensor = None,\n                         attention_mask = None,\n                         hidden_size = None,\n                         num_hidden_layers = 1,\n                         num_attention_heads = 12,\n                         intermediate_size = 12802,\n                         intermediate_act_fn = gelu_new,#gelu,\n                         hidden_dropout_prob = 0.1,\n                         initializer_range = 0.02,\n                         batch_size=None,\n                         seq_length=None,\n                         attention_head_size=None):\n    with tf.variable_scope(""layer_0""):\n        layer_input = input_tensor\n        with tf.variable_scope(""attention""):\n            attention_heads = []\n            with tf.variable_scope(""self""):\n                attention_head = attention_layer(\n                        from_tensor=layer_input,\n                        to_tensor=layer_input,\n                        attention_mask=attention_mask,\n                        num_attention_heads=num_attention_heads,\n                        size_per_head=attention_head_size,\n                        attention_probs_dropout_prob=hidden_dropout_prob,\n                        initializer_range=initializer_range,\n                        do_return_2d_tensor=True,\n                        batch_size=batch_size,\n                        from_seq_length=seq_length,\n                        to_seq_length=seq_length)\n                attention_heads.append(attention_head)\n            attention_output = None\n            if len(attention_heads) == 1:\n                attention_output = attention_heads[0]\n            else:\n                # In the case where we have other sequences, we just concatenate\n                # them to the self-attention head before the projection. \n                attention_output = tf.concat(attention_heads, axis=-1)\n\n            # Run a linear projection of `hidden_size` then add a residual\n            # with `layer_input`.\n            with tf.variable_scope(""output""):\n                attention_output = tf.layers.dense(\n                                    attention_output,\n                                    hidden_size,\n                                    kernel_initializer=create_initializer(initializer_range))\n                #attention_output = dropout(attention_output, hidden_dropout_prob)\n                attention_output = layer_norm(attention_output + layer_input)\n\n        # The activation is only applied to the ""intermediate"" hidden layer. \n        with tf.variable_scope(""intermediate""):\n            intermediate_output = tf.layers.dense(\n                    attention_output,\n                    intermediate_size,\n                    activation=intermediate_act_fn,\n                    kernel_initializer=create_initializer(initializer_range))\n\n        # Down-project back to `hidden_size` then add the residual.\n        with tf.variable_scope(""output""):\n            layer_output = tf.layers.dense(\n                    intermediate_output,\n                    hidden_size,\n                    kernel_initializer=create_initializer(initializer_range))\n            #layer_output = dropout(layer_output, hidden_dropout_prob)\n            layer_output = layer_norm(layer_output + attention_output)\n    return layer_output\n\nif __name__ == ""__main__"":\n    # open graph optimizer stream\n    with dti.GraphStream(""/path/to/frozen_model_tf14.pb"") as gs:\n        batch_size = 1\n        seq_length = 100\n        hidden_size = 768\n        num_attention_heads =12\n        attention_head_size = int(hidden_size / num_attention_heads)\n\n        # Tensor of shape [batch_size, from_seq_length, to_seq_length].\n        attention_mask = tf.placeholder(tf.float32, shape=(batch_size, seq_length, seq_length))\n\n        layer_input = tf.placeholder(tf.float32, shape=(batch_size * seq_length, hidden_size))\n\n        output_rnn = standard_transformer(input_tensor=layer_input,\n                                          attention_mask=attention_mask,\n                                          hidden_size=hidden_size,\n                                          num_attention_heads=num_attention_heads,\n                                          intermediate_size=1280,\n                                          batch_size=batch_size,\n                                          seq_length=seq_length,\n                                          attention_head_size=attention_head_size)\n\n        # remove in the future\n        gs.register_hint_op(""TransformerCellBertOther"", ""BatchMatMulV2"")\n        gs.save(""./result.pb"")\n\n    with tf.compat.v1.Session() as sess:\n        graph_def = dti.RegistPattern.get_patterns(""TransformerCellBertOther"")[0]\n        with open(""TransformerCellBertOther.pb"", ""wb"") as f:\n            f.write(graph_def.SerializeToString())\n\n'"
deltann/infer/example/python/simple_transformer.py,13,"b'import tensorflow as tf\nimport delta_infer as dti\nfrom standard_transformer.model import *\n\n@dti.RegistPattern(name=""TransformerCell"")\ndef standard_transformer(input_tensor = None,\n                         attention_mask = None,\n                         hidden_size = None,\n                         num_hidden_layers = 1,\n                         num_attention_heads = 12,\n                         intermediate_size = 12802,\n                         intermediate_act_fn = gelu,\n                         hidden_dropout_prob = 0.1,\n                         initializer_range = 0.02,\n                         batch_size=None,\n                         seq_length=None,\n                         attention_head_size=None):\n    with tf.variable_scope(""layer_0""):\n        layer_input = input_tensor\n        with tf.variable_scope(""attention""):\n            attention_heads = []\n            with tf.variable_scope(""self""):\n                attention_head = attention_layer(\n                        from_tensor=layer_input,\n                        to_tensor=layer_input,\n                        attention_mask=attention_mask,\n                        num_attention_heads=num_attention_heads,\n                        size_per_head=attention_head_size,\n                        attention_probs_dropout_prob=hidden_dropout_prob,\n                        initializer_range=initializer_range,\n                        do_return_2d_tensor=True,\n                        batch_size=batch_size,\n                        from_seq_length=seq_length,\n                        to_seq_length=seq_length)\n                attention_heads.append(attention_head)\n            attention_output = None\n            if len(attention_heads) == 1:\n                attention_output = attention_heads[0]\n            else:\n                # In the case where we have other sequences, we just concatenate\n                # them to the self-attention head before the projection. \n                attention_output = tf.concat(attention_heads, axis=-1)\n\n            # Run a linear projection of `hidden_size` then add a residual\n            # with `layer_input`.\n            with tf.variable_scope(""output""):\n                attention_output = tf.layers.dense(\n                                    attention_output,\n                                    hidden_size,\n                                    kernel_initializer=create_initializer(initializer_range))\n                #attention_output = dropout(attention_output, hidden_dropout_prob)\n                attention_output = layer_norm(attention_output + layer_input)\n\n        # The activation is only applied to the ""intermediate"" hidden layer. \n        with tf.variable_scope(""intermediate""):\n            intermediate_output = tf.layers.dense(\n                    attention_output,\n                    intermediate_size,\n                    activation=intermediate_act_fn,\n                    kernel_initializer=create_initializer(initializer_range))\n\n        # Down-project back to `hidden_size` then add the residual.\n        with tf.variable_scope(""output""):\n            layer_output = tf.layers.dense(\n                    intermediate_output,\n                    hidden_size,\n                    kernel_initializer=create_initializer(initializer_range))\n            #layer_output = dropout(layer_output, hidden_dropout_prob)\n            layer_output = layer_norm(layer_output + attention_output)\n    return layer_output\n\nif __name__ == ""__main__"":\n    # open graph optimizer stream\n    with dti.GraphStream(""/path/to/DeepLearningExamples/FasterTransformer/build/nv_fasttransformer.pb"") as gs:\n        batch_size = 1\n        seq_length = 100\n        hidden_size = 768\n        num_attention_heads =12\n        attention_head_size = int(hidden_size / num_attention_heads)\n\n        # Tensor of shape [batch_size, from_seq_length, to_seq_length].\n        attention_mask = tf.placeholder(tf.int32, shape=(batch_size, seq_length, seq_length))\n\n        layer_input = tf.placeholder(tf.float32, shape=(batch_size * seq_length, hidden_size))\n\n        output_rnn = standard_transformer(input_tensor=layer_input,\n                                          attention_mask=attention_mask,\n                                          hidden_size=hidden_size,\n                                          num_attention_heads=num_attention_heads,\n                                          intermediate_size=1280,\n                                          batch_size=batch_size,\n                                          seq_length=seq_length,\n                                          attention_head_size=attention_head_size)\n\n        # remove in the future\n        gs.register_hint_op(""TransformerCell"", ""BatchMatMulV2"")\n        gs.save(""./result.pb"")\n\n    with tf.compat.v1.Session() as sess:\n        graph_def = dti.RegistPattern.get_patterns(""TransformerCell"")[0]\n        with open(""TransformerCell.pb"", ""wb"") as f:\n            f.write(graph_def.SerializeToString())\n\n'"
deltann/infer/example/python/tts_transformer.py,5,"b'import tensorflow as tf\nimport delta_infer as dti\nfrom tts_transformer.model import *\n\n@dti.RegistPattern(name=""TransformerCell"")\ndef TransformerCellType(src,\n                        src_mask,\n                        d_model,\n                        nhead,\n                        dim_feedforward=2048):\n    output = TransformerEncoderLayer(d_model,\n                                     nhead,\n                                     dim_feedforward=2048,\n                                     dropout=0.1,\n                                     activation=""gelu"")(src, src_mask=src_mask, training=None)\n    return output\n\n\nif __name__ == ""__main__"":\n    tf.compat.v1.disable_eager_execution()\n    # open graph optimizer stream\n    with dti.GraphStream(""./model2pb.pb"") as gs:\n        batch_size = 16\n        seq_length = 1600\n        hidden_size = 512\n        nhead = 4\n\n        #src_mask = tf.placeholder(tf.float32, shape=(batch_size, 1, seq_length, seq_length))\n\n        src = tf.compat.v1.placeholder(tf.float32, shape=(batch_size, seq_length, hidden_size))\n\n        out_trans = TransformerCellType(src=src,\n                                        src_mask=None,\n                                        d_model=hidden_size,\n                                        nhead=nhead,\n                                        dim_feedforward=1080)\n        # remove in the future\n        gs.register_hint_op(""TransformerCell"", ""BatchMatMulV2"")\n        gs.save(""./tts_result.pb"")\n\n    with tf.compat.v1.Session() as sess:\n        graph_def = dti.RegistPattern.get_patterns(""TransformerCell"")[0]\n        with open(""TransformerCell_tts.pb"", ""wb"") as f:\n            f.write(tf.compat.v1.graph_util.remove_training_nodes(graph_def).SerializeToString())\n    sess.close()\n'"
deltann/infer/python/delta_infer/__init__.py,0,b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport os\n\nfrom .subgraphs import *\nfrom .optimizer import GraphStream\n'
deltann/infer/python/delta_infer/optimizer.py,0,"b'from __future__ import absolute_import, division, print_function\nimport os\nimport sys\n\nfrom .subgraphs import *\nfrom .cpp import DeltaGraph, AutoOptimizer, RegisterPattern\n\n__all__ = [""GraphStream""]\n\nclass GraphStream(object):\n    """""" GraphStream is a class for delta automatical optimization""""""\n    def __init__(self, pb_path):\n        self.__path = pb_path\n        self.__hint_map = {}\n\n    def __run(self):\n        graph_defs = []\n        for pattern_name in RegistPattern.Patterns().keys():\n            for graph_def in RegistPattern.get_patterns(pattern_name):\n                delta_graph = DeltaGraph(pb_model=graph_def)\n                RegisterPattern(pattern_name, delta_graph, self.__hint_op_type(pattern_name))\n        if pattern_name not in RegistPattern.Patterns().keys():\n            raise ValueError(""Err: there isn\'t any pattern invoked within scope of GraphStream."")\n        self.__optimizer = AutoOptimizer(self.__delta_graph_original)\n        # run optimizer automatically\n        self.__optimizer.run()\n\n    def __hint_op_type(self, pattern_name):\n        assert (pattern_name in self.__hint_map), \\\n                ""Pattern name({}) with hint op must be registered by \\\n                function register_hint_op."".format(pattern_name)\n        return self.__hint_map[pattern_name]\n\n    def register_hint_op(self, pattern_name, hint_op_type):\n        """""" register hint op for pattern\n\n        Arguments:\n            pattern_name (string): name of this pattern\n            hint_op_type (string): op type of of pattern,\n                                   this param can be any op exist in pattern graph\n        """"""\n        self.__hint_map[pattern_name] = hint_op_type\n\n    def save(self, path=None):\n        """""" set save path for final otimized graph by graph stream """"""\n        self.__save_path = path if path is not None else ""./result.pb""\n\n    def __enter__(self):\n        self.__delta_graph_original = DeltaGraph(pb_file=self.__path)\n        return self\n\n    def __exit__(self, type, value, traceback):\n        self.__run()\n        self.__optimizer.serialization(self.__save_path)\n'"
deltann/infer/python/delta_infer/visual_pattern.py,5,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport socket\nimport tempfile\nimport shutil\n\nimport netron\nimport tensorflow as tf\nif tf.__version__ <= \'1.8.0\':\n    from tensorflow import Session\n    from tensorflow import summary\n    from tensorflow import global_variables_initializer\nelse:\n    from tensorflow.compat.v1 import Session\n    from tensorflow.compat.v1 import summary\n    from tensorflow.compat.v1 import global_variables_initializer\nfrom tensorboard import program, default\n\nfrom absl import app\nfrom absl import flags\n\nfrom .subgraphs import *\n\nflags.DEFINE_enum(\'mode\', \'visual\', [\'visual\', \'simplify\', \'save_pattern\', \'print_pattern\'], \'running mode.\')\nflags.DEFINE_enum(\'type\', \'netron\', [\'netron\', \'tf\'], \'running graph type of visual mode.\')\nflags.DEFINE_string(\'name\', None, \'Pattern name.\')\nflags.DEFINE_integer(\'idx\', 0, ""pattern graph index of RegistPattern."", lower_bound=0)\n\n# used when in mode: visual and graph is from graph files.\nflags.DEFINE_string(\'graph_path\', None, \'tensorflow graph proto file.\')\n# used when in mode: visual\nflags.DEFINE_string(\'pt\', \'8080\', \'tensorboard or netron server port.\')\n# used when in save_pattern\nflags.DEFINE_string(\'dir\', None, \'save pattern to dir.\')\n# used when in mode: simplify\nflags.DEFINE_string(\'outs\', None, \'set outs of graph when in mode: simplify.\')\n\ndef get_ip_address():\n    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    s.connect((""8.8.8.8"", 80))\n    return s.getsockname()[0]\n\ndef visual_mode(mode_type=\'netron\', pattern_name=None, pattern_idx=0, graph_path=None):\n    def get_graph_def(pattern_name=None, pattern_idx=0, graph_path=None):\n        assert (pattern_name is not None) or (graph_path is not None), \\\n                ""pattern_name or graph_path should at least have one is None and the other is not.""\n        if graph_path is not None:\n            name = graph_path.split(\'/\')[-1]\n            with open(graph_path, ""rb"") as f:\n                graph_def = tf.GraphDef()\n                graph_def.ParseFromString(f.read())\n        else:\n            name = pattern_name\n            graph_def = RegistPattern.get_patterns(pattern_name)[pattern_idx]\n        return graph_def, name\n\n    graph_def, name = get_graph_def(pattern_name=pattern_name,\n                              pattern_idx=pattern_idx,\n                              graph_path=graph_path)\n    if mode_type==\'netron\':\n        tmp_dir = tempfile.mkdtemp()\n        model_path = tmp_dir + ""/"" + name\n        with open(model_path, ""wb"") as f:\n            f.write(graph_def.SerializeToString())\n        netron.start(file=model_path, host=get_ip_address(), port=flags.FLAGS.pt)\n        shutil.rmtree(tmp_dir)\n\n    else:# type == \'tf\'\n        with Session() as sess:\n            tmp_dir = tempfile.mkdtemp()\n            tf.import_graph_def(graph_def)\n            train_writer = summary.FileWriter(tmp_dir)\n            train_writer.add_graph(sess.graph)\n            train_writer.flush()\n            train_writer.close()\n            tb = program.TensorBoard(default.get_plugins())\n            tb.configure(argv=[None, \'--logdir\', tmp_dir, \'--port\', flags.FLAGS.pt, \'--host\', get_ip_address()])\n            tb.main()\n            shutil.rmtree(tmp_dir)\n\ndef main(argv):\n    if flags.FLAGS.mode == ""visual"":\n        visual_mode(mode_type=flags.FLAGS.type,\n                    pattern_name=flags.FLAGS.name,\n                    pattern_idx=flags.FLAGS.idx,\n                    graph_path=flags.FLAGS.graph_path)\n    elif flags.FLAGS.mode == ""save_pattern"":\n        if flags.FLAGS.name is None:\n            raise ValueError(""Flags \'s pattern name({}) cann\'t be None."".format(flags.FLAGS.name))\n        elif flags.FLAGS.dir == None:\n            raise ValueError(""Flags \'s pattern dir({}) cann\'t be None."".format(flags.FLAGS.dir))\n        else:\n            with Session() as sess:\n                graph_def = RegistPattern.get_patterns(flags.FLAGS.name)[flags.FLAGS.idx]\n                model_path = flags.FLAGS.dir+""/""+flags.FLAGS.name+"".pb""\n                with open(model_path, ""wb"") as f:\n                    f.write(graph_def.SerializeToString())\n            sess.close()\n    elif flags.FLAGS.mode == ""simplify"":\n        if flags.FLAGS.graph_path is None:\n            raise ValueError(""Flags \'s graph_path({}) cann\'t be None."".format(flags.FLAGS.graph_path))\n        else:\n            with Session() as sess:\n                with open(flags.FLAGS.graph_path, ""rb"") as f:\n                    graph_def = tf.GraphDef()\n                    graph_def.ParseFromString(f.read())\n                    g_in = tf.import_graph_def(graph_def)\n                    global_variables_initializer().run()\n                    #graph_summary = GraphSummary(graph_def=sess.graph_def)\n                    #graph_summary.Summary()\n                    outs = flags.FLAGS.outs.split(\',\')\n                    graph_def = graph_util.convert_variables_to_constants(sess, sess.graph_def, outs)\n                    graph_summary = GraphSummary(graph_def=graph_def)\n                    graph_summary.Summary()\n                    graph_def_name = flags.FLAGS.graph_path.split(\'/\')[-1]\n                    model_path = flags.FLAGS.dir+""/""+graph_def_name\n                    with open(model_path, ""wb"") as f:\n                        f.write(graph_def.SerializeToString())\n    else:\n        for pattern_name in RegistPattern.Patterns().keys():\n            print(""Registered Pattern name: {} with {} different GraphDef."".\\\n                    format(pattern_name, len(RegistPattern.get_patterns(pattern_name))))\n\ndef command():\n    app.run(main)\n'"
egs/sre16/v1/local/make_musan.py,0,"b'#!/usr/bin/env python3\n# Copyright 2015   David Snyder\n# Apache 2.0.\n#\n# This file is meant to be invoked by make_musan.sh.\n\nimport os, sys\n\ndef process_music_annotations(path):\n  utt2spk = {}\n  utt2vocals = {}\n  lines = open(path, \'r\').readlines()\n  for line in lines:\n    utt, genres, vocals, musician = line.rstrip().split()[:4]\n    # For this application, the musican ID isn\'t important\n    utt2spk[utt] = utt\n    utt2vocals[utt] = vocals == ""Y""\n  return utt2spk, utt2vocals\n\ndef prepare_music(root_dir, use_vocals):\n  utt2vocals = {}\n  utt2spk = {}\n  utt2wav = {}\n  num_good_files = 0\n  num_bad_files = 0\n  music_dir = os.path.join(root_dir, ""music"")\n  for root, dirs, files in os.walk(music_dir):\n    for file in files:\n      file_path = os.path.join(root, file)\n      if file.endswith("".wav""):\n        utt = str(file).replace("".wav"", """")\n        utt2wav[utt] = file_path\n      elif str(file) == ""ANNOTATIONS"":\n        utt2spk_part, utt2vocals_part = process_music_annotations(file_path)\n        utt2spk.update(utt2spk_part)\n        utt2vocals.update(utt2vocals_part)\n  utt2spk_str = """"\n  utt2wav_str = """"\n  for utt in utt2vocals:\n    if utt in utt2wav:\n      if use_vocals or not utt2vocals[utt]:\n        utt2spk_str = utt2spk_str + utt + "" "" + utt2spk[utt] + ""\\n""\n        utt2wav_str = utt2wav_str + utt + "" sox -t wav "" + utt2wav[utt] + "" -r 8k -t wav - |\\n""\n      num_good_files += 1\n    else:\n      print(""Missing file"", utt)\n      num_bad_files += 1\n  print(""In music directory, processed"", num_good_files, ""files;"", num_bad_files, ""had missing wav data"")\n  return utt2spk_str, utt2wav_str\n\ndef prepare_speech(root_dir):\n  utt2spk = {}\n  utt2wav = {}\n  num_good_files = 0\n  num_bad_files = 0\n  speech_dir = os.path.join(root_dir, ""speech"")\n  for root, dirs, files in os.walk(speech_dir):\n    for file in files:\n      file_path = os.path.join(root, file)\n      if file.endswith("".wav""):\n        utt = str(file).replace("".wav"", """")\n        utt2wav[utt] = file_path\n        utt2spk[utt] = utt\n  utt2spk_str = """"\n  utt2wav_str = """"\n  for utt in utt2spk:\n    if utt in utt2wav:\n      utt2spk_str = utt2spk_str + utt + "" "" + utt2spk[utt] + ""\\n""\n      utt2wav_str = utt2wav_str + utt + "" sox -t wav "" + utt2wav[utt] + "" -r 8k -t wav - |\\n""\n      num_good_files += 1\n    else:\n      print(""Missing file"", utt)\n      num_bad_files += 1\n  print(""In speech directory, processed"", num_good_files, ""files;"", num_bad_files, ""had missing wav data"")\n  return utt2spk_str, utt2wav_str\n\ndef prepare_noise(root_dir):\n  utt2spk = {}\n  utt2wav = {}\n  num_good_files = 0\n  num_bad_files = 0\n  noise_dir = os.path.join(root_dir, ""noise"")\n  for root, dirs, files in os.walk(noise_dir):\n    for file in files:\n      file_path = os.path.join(root, file)\n      if file.endswith("".wav""):\n        utt = str(file).replace("".wav"", """")\n        utt2wav[utt] = file_path\n        utt2spk[utt] = utt\n  utt2spk_str = """"\n  utt2wav_str = """"\n  for utt in utt2spk:\n    if utt in utt2wav:\n      utt2spk_str = utt2spk_str + utt + "" "" + utt2spk[utt] + ""\\n""\n      utt2wav_str = utt2wav_str + utt + "" sox -t wav "" + utt2wav[utt] + "" -r 8k -t wav - |\\n""\n      num_good_files += 1\n    else:\n      print(""Missing file"", utt)\n      num_bad_files += 1\n  print(""In noise directory, processed"", num_good_files, ""files;"", num_bad_files, ""had missing wav data"")\n  return utt2spk_str, utt2wav_str\n\ndef main():\n  in_dir = sys.argv[1]\n  out_dir = sys.argv[2]\n  use_vocals = sys.argv[3] == ""Y""\n  utt2spk_music, utt2wav_music = prepare_music(in_dir, use_vocals)\n  utt2spk_speech, utt2wav_speech = prepare_speech(in_dir)\n  utt2spk_noise, utt2wav_noise = prepare_noise(in_dir)\n  utt2spk = utt2spk_speech + utt2spk_music + utt2spk_noise\n  utt2wav = utt2wav_speech + utt2wav_music + utt2wav_noise\n  wav_fi = open(os.path.join(out_dir, ""wav.scp""), \'w\')\n  wav_fi.write(utt2wav)\n  utt2spk_fi = open(os.path.join(out_dir, ""utt2spk""), \'w\')\n  utt2spk_fi.write(utt2spk)\n\n\nif __name__==""__main__"":\n  main()\n'"
egs/sre16/v1/local/make_sre18_dev.py,0,"b'#!/usr/bin/env python\nimport threading\nimport warnings\n\nwarnings.filterwarnings(""ignore"", message=""numpy.dtype size changed"")\nwarnings.filterwarnings(""ignore"", message=""numpy.ufunc size changed"")\nimport argparse\nimport os\nimport sys\nimport traceback\n\n\ndef get_args():\n    """""" Get args from stdin.\n    """"""\n\n    parser = argparse.ArgumentParser(\n        description=""Create a tar file for fast DNN training.  Each of minibatch data will ""\n                    ""saved to a separate numpy file within tar file.  The output file can be ""\n                    ""accessed in sequential mode or in random access mode but the sequential ""\n                    ""more is faster."",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        conflict_handler=\'resolve\')\n\n    parser.add_argument(""--sampling-frequency"", dest=""sampling_frequency"", type=str, choices=[""8k"", ""16k""], default=""8k"",\n                        help=""Sampling frequency of the input files."")\n\n    parser.add_argument(""sre18_dev_dir"", type=str, help=""Path to SRE18 development directory."")\n\n    parser.add_argument(""output_dir"", type=str, help=""Path to output directory."")\n\n    print(\' \'.join(sys.argv))\n\n    args = parser.parse_args()\n\n    args = process_args(args)\n\n    return args\n\n\ndef process_args(args):\n    """""" Process the options got from get_args()\n    """"""\n\n    if args.sre18_dev_dir == \'\' or not os.path.exists(args.sre18_dev_dir):\n        raise Exception(""The specified sre18_dev_dir \'{0}\' not exist."".format(args.sre18_dev_dir))\n\n    return args\n\n\ndef write_to_wav_scp(wav_scp, sampling_frequency, extension, name, file_path):\n    if sampling_frequency == ""8k"":\n        if extension == \'sph\':\n            wav_scp.write(\'{utt} sph2pipe -f wav -p -c 1 {sph} |\\n\'.format(utt=name, sph=file_path))\n        else:\n            wav_scp.write(\'{utt} ffmpeg -i {flac} -f wav -ar 8000 - |\\n\'.format(utt=name, flac=file_path))\n    else:\n        if extension == \'sph\':\n            wav_scp.write(\'{utt} sph2pipe -f wav -p -c 1 {sph} | sox -t wav - -r 16k -t wav - |\\n\'.format(utt=name, sph=file_path))\n        else:\n            wav_scp.write(\'{utt} ffmpeg -i {flac} -f wav -ar 16000 - |\\n\'.format(utt=name, flac=file_path))\n\n\ndef process_files(args):\n    sre18_dev_dir = args.sre18_dev_dir\n    output_dir = os.path.join(args.output_dir, \'sre18_dev_enroll\')\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    sampling_frequency = args.sampling_frequency\n    # temp_dir = os.path.join(output_dir, \'tmp\')\n    # if not os.path.exists(temp_dir):\n    #     os.makedirs(temp_dir)\n\n    utt2spk = open(os.path.join(output_dir, \'utt2spk\'), \'wt\')\n    wav_scp = open(os.path.join(output_dir, \'wav.scp\'), \'wt\')\n    meta = open(os.path.join(sre18_dev_dir, \'docs/sre18_dev_enrollment.tsv\'), \'rt\')\n\n    meta.readline()  # header line\n    line = meta.readline()\n    utt2fixedutt = {}\n    while line:\n        tokens = line.split(""\\t"")\n        model_id, segment_id, side = tokens\n        segment_id = segment_id.split(\'.\')[0]\n        utt2spk.write(""{spk}-{utt} {spk}\\n"".format(spk=model_id, utt=segment_id))\n        utt2fixedutt[segment_id] = ""{spk}-{utt}"".format(spk=model_id, utt=segment_id)\n        line = meta.readline()\n    utt2spk.close()\n    meta.close()\n\n    audio_files = os.listdir(os.path.join(sre18_dev_dir, \'data/enrollment\'))\n    for file_name in audio_files:\n        file_path = os.path.join(sre18_dev_dir, \'data/enrollment/\' + file_name)\n        name, extension = file_name.split(\'.\')\n        write_to_wav_scp(wav_scp, sampling_frequency, extension, utt2fixedutt[name], file_path)\n    wav_scp.close()\n\n    if os.system(""utils/utt2spk_to_spk2utt.pl {out_dir}/utt2spk > {out_dir}/spk2utt"".format(out_dir=output_dir)) != 0:\n        raise Exception(""Error creating spk2utt file in directory {out_dir}"".format(out_dir=output_dir))\n\n    if os.system(""utils/fix_data_dir.sh {out_dir}"".format(out_dir=output_dir)) != 0:\n        raise Exception(""Error fixing data dir {out_dir}"".format(out_dir=output_dir))\n\n    #################### TEST PART ####################\n    output_dir = os.path.join(args.output_dir, \'sre18_dev_test\')\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    # temp_dir = os.path.join(output_dir, \'tmp\')\n    # if not os.path.exists(temp_dir):\n    #     os.makedirs(temp_dir)\n\n    utt2spk = open(os.path.join(output_dir, \'utt2spk\'), \'wt\')\n    wav_scp = open(os.path.join(output_dir, \'wav.scp\'), \'wt\')\n    trials = open(os.path.join(output_dir, \'trials\'), \'wt\')\n    trial_key = open(os.path.join(sre18_dev_dir, \'docs/sre18_dev_trial_key.tsv\'), \'rt\')\n    segment_key = open(os.path.join(sre18_dev_dir, \'docs/sre18_dev_segment_key.tsv\'), \'rt\')\n\n    segment_key.readline()  # header line\n    line = segment_key.readline()\n    utt2subject = {}\n    while line:\n        parts = line.split(\'\\t\')\n        utt = parts[0].split(\'.\')[0]\n        subject = parts[1]\n        utt2subject[utt] = subject\n        line = segment_key.readline()\n\n    audio_files = os.listdir(os.path.join(sre18_dev_dir, \'data/test\'))\n    for file_name in audio_files:\n        file_path = os.path.join(sre18_dev_dir, \'data/test/\' + file_name)\n        name, extension = file_name.split(\'.\')\n        utt2spk.write(\'{utt} {utt}\\n\'.format(utt=name))\n        write_to_wav_scp(wav_scp, sampling_frequency, extension, name, file_path)\n    wav_scp.close()\n    utt2spk.close()\n\n    trial_key.readline()  # header line\n    line = trial_key.readline()\n    while line:\n        parts = line.split(\'\\t\')\n        spk = parts[0]\n        utt = parts[1].split(\'.\')[0]\n        target_type = parts[3]\n        trials.write(\'{spk} {utt} {target_type}\\n\'.format(spk=spk, utt=utt, target_type=target_type))\n        line = trial_key.readline()\n    trials.close()\n\n    if os.system(""utils/utt2spk_to_spk2utt.pl {out_dir}/utt2spk > {out_dir}/spk2utt"".format(out_dir=output_dir)) != 0:\n        raise Exception(""Error creating spk2utt file in directory {out_dir}"".format(out_dir=output_dir))\n\n    if os.system(""utils/fix_data_dir.sh {out_dir}"".format(out_dir=output_dir)) != 0:\n        raise Exception(""Error fixing data dir {out_dir}"".format(out_dir=output_dir))\n\n    #################### UNLABELED PART ####################\n    output_dir = os.path.join(args.output_dir, \'sre18_dev_unlabeled\')\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    utt2spk = open(os.path.join(output_dir, \'utt2spk\'), \'wt\')\n    wav_scp = open(os.path.join(output_dir, \'wav.scp\'), \'wt\')\n\n    audio_files = os.listdir(os.path.join(sre18_dev_dir, \'data/unlabeled\'))\n    for file_name in audio_files:\n        file_path = os.path.join(sre18_dev_dir, \'data/unlabeled/\' + file_name)\n        name, extension = file_name.split(\'.\')\n        utt2spk.write(\'{utt} {utt}\\n\'.format(utt=name))\n        write_to_wav_scp(wav_scp, sampling_frequency, extension, name, file_path)\n    wav_scp.close()\n    utt2spk.close()\n\n    if os.system(""utils/utt2spk_to_spk2utt.pl {out_dir}/utt2spk > {out_dir}/spk2utt"".format(out_dir=output_dir)) != 0:\n        raise Exception(""Error creating spk2utt file in directory {out_dir}"".format(out_dir=output_dir))\n\n    if os.system(""utils/fix_data_dir.sh {out_dir}"".format(out_dir=output_dir)) != 0:\n        raise Exception(""Error fixing data dir {out_dir}"".format(out_dir=output_dir))\n\n\ndef wait_for_background_commands():\n    """""" This waits for all threads to exit.  You will often want to\n        run this at the end of programs that have launched background\n        threads, so that the program will wait for its child processes\n        to terminate before it dies.""""""\n    for t in threading.enumerate():\n        if not t == threading.current_thread():\n            t.join()\n\n\ndef main():\n    args = get_args()\n    try:\n        process_files(args)\n        wait_for_background_commands()\n    except BaseException as e:\n        # look for BaseException so we catch KeyboardInterrupt, which is\n        # what we get when a background thread dies.\n        if not isinstance(e, KeyboardInterrupt):\n            traceback.print_exc()\n        sys.exit(1)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
egs/sre16/v1/local/make_sre18_eval.py,0,"b'#!/usr/bin/env python\nimport threading\nimport warnings\n\nwarnings.filterwarnings(""ignore"", message=""numpy.dtype size changed"")\nwarnings.filterwarnings(""ignore"", message=""numpy.ufunc size changed"")\nimport argparse\nimport os\nimport sys\nimport traceback\n\n\ndef get_args():\n    """""" Get args from stdin.\n    """"""\n\n    parser = argparse.ArgumentParser(\n        description=""Create a tar file for fast DNN training.  Each of minibatch data will ""\n                    ""saved to a separate numpy file within tar file.  The output file can be ""\n                    ""accessed in sequential mode or in random access mode but the sequential ""\n                    ""more is faster."",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        conflict_handler=\'resolve\')\n\n    parser.add_argument(""--sampling-frequency"", dest=""sampling_frequency"", type=str, choices=[""8k"", ""16k""], default=""8k"",\n                        help=""Sampling frequency of the input files."")\n\n    parser.add_argument(""sre18_dev_dir"", type=str, help=""Path to SRE18 development directory."")\n\n    parser.add_argument(""output_dir"", type=str, help=""Path to output directory."")\n\n    print(\' \'.join(sys.argv))\n\n    args = parser.parse_args()\n\n    args = process_args(args)\n\n    return args\n\n\ndef process_args(args):\n    """""" Process the options got from get_args()\n    """"""\n\n    if args.sre18_dev_dir == \'\' or not os.path.exists(args.sre18_dev_dir):\n        raise Exception(""The specified sre18_dev_dir \'{0}\' not exist."".format(args.sre18_dev_dir))\n\n    return args\n\n\ndef write_to_wav_scp(wav_scp, sampling_frequency, extension, name, file_path):\n    if sampling_frequency == ""8k"":\n        if extension == \'sph\':\n            wav_scp.write(\'{utt} sph2pipe -f wav -p -c 1 {sph} |\\n\'.format(utt=name, sph=file_path))\n        else:\n            wav_scp.write(\'{utt} ffmpeg -i {flac} -f wav -ar 8000 - |\\n\'.format(utt=name, flac=file_path))\n    else:\n        if extension == \'sph\':\n            wav_scp.write(\'{utt} sph2pipe -f wav -p -c 1 {sph} | sox -t wav - -r 16k -t wav - |\\n\'.format(utt=name, sph=file_path))\n        else:\n            wav_scp.write(\'{utt} ffmpeg -i {flac} -f wav -ar 16000 - |\\n\'.format(utt=name, flac=file_path))\n\n\ndef process_files(args):\n    sre18_dev_dir = args.sre18_dev_dir\n    output_dir = os.path.join(args.output_dir, \'sre18_dev_enroll\')\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    sampling_frequency = args.sampling_frequency\n    # temp_dir = os.path.join(output_dir, \'tmp\')\n    # if not os.path.exists(temp_dir):\n    #     os.makedirs(temp_dir)\n\n    utt2spk = open(os.path.join(output_dir, \'utt2spk\'), \'wt\')\n    wav_scp = open(os.path.join(output_dir, \'wav.scp\'), \'wt\')\n    meta = open(os.path.join(sre18_dev_dir, \'docs/sre18_dev_enrollment.tsv\'), \'rt\')\n\n    meta.readline()  # header line\n    line = meta.readline()\n    utt2fixedutt = {}\n    while line:\n        tokens = line.split(""\\t"")\n        model_id, segment_id, side = tokens\n        segment_id = segment_id.split(\'.\')[0]\n        utt2spk.write(""{spk}-{utt} {spk}\\n"".format(spk=model_id, utt=segment_id))\n        utt2fixedutt[segment_id] = ""{spk}-{utt}"".format(spk=model_id, utt=segment_id)\n        line = meta.readline()\n    utt2spk.close()\n    meta.close()\n\n    audio_files = os.listdir(os.path.join(sre18_dev_dir, \'data/enrollment\'))\n    for file_name in audio_files:\n        file_path = os.path.join(sre18_dev_dir, \'data/enrollment/\' + file_name)\n        name, extension = file_name.split(\'.\')\n        write_to_wav_scp(wav_scp, sampling_frequency, extension, utt2fixedutt[name], file_path)\n    wav_scp.close()\n\n    if os.system(""utils/utt2spk_to_spk2utt.pl {out_dir}/utt2spk > {out_dir}/spk2utt"".format(out_dir=output_dir)) != 0:\n        raise Exception(""Error creating spk2utt file in directory {out_dir}"".format(out_dir=output_dir))\n\n    if os.system(""utils/fix_data_dir.sh {out_dir}"".format(out_dir=output_dir)) != 0:\n        raise Exception(""Error fixing data dir {out_dir}"".format(out_dir=output_dir))\n\n    #################### TEST PART ####################\n    output_dir = os.path.join(args.output_dir, \'sre18_dev_test\')\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    # temp_dir = os.path.join(output_dir, \'tmp\')\n    # if not os.path.exists(temp_dir):\n    #     os.makedirs(temp_dir)\n\n    utt2spk = open(os.path.join(output_dir, \'utt2spk\'), \'wt\')\n    wav_scp = open(os.path.join(output_dir, \'wav.scp\'), \'wt\')\n    trials = open(os.path.join(output_dir, \'trials\'), \'wt\')\n    trial_key = open(os.path.join(sre18_dev_dir, \'docs/sre18_dev_trial_key.tsv\'), \'rt\')\n    segment_key = open(os.path.join(sre18_dev_dir, \'docs/sre18_dev_segment_key.tsv\'), \'rt\')\n\n    segment_key.readline()  # header line\n    line = segment_key.readline()\n    utt2subject = {}\n    while line:\n        parts = line.split(\'\\t\')\n        utt = parts[0].split(\'.\')[0]\n        subject = parts[1]\n        utt2subject[utt] = subject\n        line = segment_key.readline()\n\n    audio_files = os.listdir(os.path.join(sre18_dev_dir, \'data/test\'))\n    for file_name in audio_files:\n        file_path = os.path.join(sre18_dev_dir, \'data/test/\' + file_name)\n        name, extension = file_name.split(\'.\')\n        utt2spk.write(\'{utt} {utt}\\n\'.format(utt=name))\n        write_to_wav_scp(wav_scp, sampling_frequency, extension, name, file_path)\n    wav_scp.close()\n    utt2spk.close()\n\n    trial_key.readline()  # header line\n    line = trial_key.readline()\n    while line:\n        parts = line.split(\'\\t\')\n        spk = parts[0]\n        utt = parts[1].split(\'.\')[0]\n        target_type = parts[3]\n        trials.write(\'{spk} {utt} {target_type}\\n\'.format(spk=spk, utt=utt, target_type=target_type))\n        line = trial_key.readline()\n    trials.close()\n\n    if os.system(""utils/utt2spk_to_spk2utt.pl {out_dir}/utt2spk > {out_dir}/spk2utt"".format(out_dir=output_dir)) != 0:\n        raise Exception(""Error creating spk2utt file in directory {out_dir}"".format(out_dir=output_dir))\n\n    if os.system(""utils/fix_data_dir.sh {out_dir}"".format(out_dir=output_dir)) != 0:\n        raise Exception(""Error fixing data dir {out_dir}"".format(out_dir=output_dir))\n\n    #################### UNLABELED PART ####################\n    output_dir = os.path.join(args.output_dir, \'sre18_dev_unlabeled\')\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    utt2spk = open(os.path.join(output_dir, \'utt2spk\'), \'wt\')\n    wav_scp = open(os.path.join(output_dir, \'wav.scp\'), \'wt\')\n\n    audio_files = os.listdir(os.path.join(sre18_dev_dir, \'data/unlabeled\'))\n    for file_name in audio_files:\n        file_path = os.path.join(sre18_dev_dir, \'data/unlabeled/\' + file_name)\n        name, extension = file_name.split(\'.\')\n        utt2spk.write(\'{utt} {utt}\\n\'.format(utt=name))\n        write_to_wav_scp(wav_scp, sampling_frequency, extension, name, file_path)\n    wav_scp.close()\n    utt2spk.close()\n\n    if os.system(""utils/utt2spk_to_spk2utt.pl {out_dir}/utt2spk > {out_dir}/spk2utt"".format(out_dir=output_dir)) != 0:\n        raise Exception(""Error creating spk2utt file in directory {out_dir}"".format(out_dir=output_dir))\n\n    if os.system(""utils/fix_data_dir.sh {out_dir}"".format(out_dir=output_dir)) != 0:\n        raise Exception(""Error fixing data dir {out_dir}"".format(out_dir=output_dir))\n\n\ndef wait_for_background_commands():\n    """""" This waits for all threads to exit.  You will often want to\n        run this at the end of programs that have launched background\n        threads, so that the program will wait for its child processes\n        to terminate before it dies.""""""\n    for t in threading.enumerate():\n        if not t == threading.current_thread():\n            t.join()\n\n\ndef main():\n    args = get_args()\n    try:\n        process_files(args)\n        wait_for_background_commands()\n    except BaseException as e:\n        # look for BaseException so we catch KeyboardInterrupt, which is\n        # what we get when a background thread dies.\n        if not isinstance(e, KeyboardInterrupt):\n            traceback.print_exc()\n        sys.exit(1)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
egs/wmt14_en_de/nlp1/local/generate_stand_vocab.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport sys\nfrom absl import logging\n\ndef generate_stand_vocab(old_vocab, new_vocab):\n  vocab_file = open(new_vocab, \'w\')\n  vocab_file.write(\'<pad>\' + \'\\t\' + \'0\' + \'\\n\')\n  vocab_file.write(\'<s>\' + \'\\t\' + \'1\' + \'\\n\')\n  vocab_file.write(\'</s>\' + \'\\t\' + \'2\' + \'\\n\')\n  vocab_file.write(\'<unk>\' + \'\\t\' + \'3\' + \'\\n\')\n  vocab_file.write(\'<sos>\' + \'\\t\' + \'4\' + \'\\n\')\n  vocab_file.write(\'<eos>\' + \'\\t\' + \'5\' + \'\\n\')\n  idx = 6\n  with open(old_vocab, \'r\') as f:\n    for i, line in enumerate(f.readlines()):\n      if i > 2:\n        vocab_file.write(line.strip() + \'\\t\' +\n                         str(idx) + \'\\n\')\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n\n  if len(sys.argv) != 3:\n    logging.error(""Usage python {} old_vocab new_vocab"".format(sys.argv[0]))\n    sys.exit(-1)\n\n  old_vocab = sys.argv[1]\n  new_vocab = sys.argv[2]\n  generate_stand_vocab(old_vocab, new_vocab)\n\n\n'"
delta/data/feat/python_speech_features/test/__init__.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n'"
delta/data/feat/python_speech_features/test/test_sigproc.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\' Test sigproc.py \'\'\'\n\nimport time\nimport unittest\n\nimport numpy as np\n\nimport sigproc\n\n# pylint: disable=assignment-from-no-return\n# pylint: disable=invalid-name\n# pylint: disable=no-self-use\n\n\nclass test_case(unittest.TestCase):\n  \'\'\' Test case. \'\'\'\n\n  def test_frame_sig(self):\n    \'\'\' Test frame signal processing. \'\'\'\n    n = 10000124\n    frame_len = 37\n    frame_step = 13\n    x = np.random.rand(n)\n    t0 = time.time()\n    y_old = sigproc.framesig(\n        x, frame_len=frame_len, frame_step=frame_step, stride_trick=False)\n    t1 = time.time()\n    y_new = sigproc.framesig(\n        x, frame_len=frame_len, frame_step=frame_step, stride_trick=True)\n    t_new = time.time() - t1\n    t_old = t1 - t0\n    self.assertTupleEqual(y_old.shape, y_new.shape)\n    np.testing.assert_array_equal(y_old, y_new)\n    self.assertLess(t_new, t_old)\n    print(\'new run time %3.2f < %3.2f sec\' % (t_new, t_old))\n\n  def test_rolling(self):\n    \'\'\' Test rolling window. \'\'\'\n    x = np.arange(10)\n    y = sigproc.rolling_window(x, window=4, step=3)\n    y_expected = np.array([[0, 1, 2, 3], [3, 4, 5, 6], [6, 7, 8, 9]])\n    y = np.testing.assert_array_equal(y, y_expected)\n'"
deltann/infer/example/python/nlp_transformer/__init__.py,0,b''
deltann/infer/example/python/nlp_transformer/model.py,170,"b'import tensorflow as tf\n\n\ndef positional_embedding(pos_seq, inv_freq, bsz=None):\n  sinusoid_inp = tf.einsum(\'i,j->ij\', pos_seq, inv_freq)\n  pos_emb = tf.concat([tf.sin(sinusoid_inp), tf.cos(sinusoid_inp)], -1)\n  if bsz is not None:\n    return tf.tile(pos_emb[:, None, :], [1, bsz, 1])\n  else:\n    return pos_emb[:, None, :]\n\n\ndef positionwise_FF(inp, d_model, d_inner, dropout, kernel_initializer,\n                    scope=\'ff\', is_training=True):\n  output = inp\n  with tf.variable_scope(scope):\n    output = tf.layers.dense(inp, d_inner, activation=tf.nn.relu,\n                             kernel_initializer=kernel_initializer,\n                             bias_initializer=kernel_initializer,\n                             name=\'layer_1\')\n    #output = tf.layers.dense(inp, 50,\n                             #kernel_initializer=kernel_initializer,\n                             #name=\'layer_1.1\')\n    #output = tf.layers.dense(output, d_inner, activation=tf.nn.relu,\n                             #kernel_initializer=kernel_initializer,\n                             #name=\'layer_1.2\')\n    output = tf.layers.dropout(output, dropout, training=is_training,\n                               name=\'drop_1\')\n    #output = tf.layers.dense(output, d_model,\n                             #kernel_initializer=kernel_initializer,\n                             #name=\'layer_2\')\n    output = tf.layers.dense(output, 50,\n                             kernel_initializer=kernel_initializer,\n                             bias_initializer=kernel_initializer,\n                             name=\'layer_2.1\')\n    output = tf.layers.dense(output, d_model,\n                             kernel_initializer=kernel_initializer,\n                             bias_initializer=kernel_initializer,\n                             name=\'layer_2\')\n    output = tf.layers.dropout(output, dropout, training=is_training,\n                               name=\'drop_2\')\n    #ff_out_before = output\n    output = tf.contrib.layers.layer_norm(output + inp, begin_norm_axis=-1)\n  return output#, ff_out_before\n\n\ndef rel_shift(x):\n  x_size = tf.shape(x)\n\n  x = tf.pad(x, [[0, 0], [1, 0], [0, 0], [0, 0]])\n  x = tf.reshape(x, [x_size[1] + 1, x_size[0], x_size[2], x_size[3]])\n  x = tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])\n  x = tf.reshape(x, x_size)\n\n  return x\n\n\ndef rel_multihead_attn(w, r, r_w_bias, r_r_bias, attn_mask, mems, d_model,\n                       n_head, d_head, dropout, dropatt, is_training,\n                       kernel_initializer, index, scope=\'rel_attn\'):\n  scale = 1 / (d_head ** 0.5)\n  with tf.variable_scope(scope):\n    qlen = tf.shape(w)[0]\n    rlen = tf.shape(r)[0]\n    bsz = tf.shape(w)[1]\n\n    cat = tf.concat([mems, w],\n                    0) if mems is not None and mems.shape.ndims > 1 else w\n    #w_heads = tf.layers.dense(cat, 3 * n_head * d_head, use_bias=False,\n                              #kernel_initializer=kernel_initializer, name=\'qkv\')\n    if index <= 3:\n        rank = 75\n    else:\n        rank = 75\n    w_heads = tf.layers.dense(cat, 3 * rank, use_bias=False,\n                              kernel_initializer=kernel_initializer, name=\'qkv_\');\n    tmp = w_heads\n    w_heads = tf.layers.dense(w_heads, 3 * n_head * d_head, use_bias=False,\n                              kernel_initializer=kernel_initializer, name=\'qkv\');\n    r_head_k = tf.layers.dense(r, n_head * d_head, use_bias=False,\n                               kernel_initializer=kernel_initializer, name=\'r\')\n\n    w_head_q, w_head_k, w_head_v = tf.split(w_heads, 3, -1)\n    w_head_q = w_head_q[-qlen:]\n\n    klen = tf.shape(w_head_k)[0]\n\n    w_head_q = tf.reshape(w_head_q, [qlen, bsz, n_head, d_head])\n    w_head_k = tf.reshape(w_head_k, [klen, bsz, n_head, d_head])\n    w_head_v = tf.reshape(w_head_v, [klen, bsz, n_head, d_head])\n\n    r_head_k = tf.reshape(r_head_k, [rlen, n_head, d_head])\n\n    rw_head_q = w_head_q + r_w_bias\n    rr_head_q = w_head_q + r_r_bias\n\n    AC = tf.einsum(\'ibnd,jbnd->ijbn\', rw_head_q, w_head_k)\n    BD = tf.einsum(\'ibnd,jnd->ijbn\', rr_head_q, r_head_k)\n    bd_before = BD\n    BD = rel_shift(BD)\n\n    attn_score = (AC + BD) * scale\n    attn_mask_t = attn_mask[:, :, None, None]\n    attn_score = attn_score * (1 - attn_mask_t) - 1e30 * attn_mask_t\n\n    attn_prob = tf.nn.softmax(attn_score, 1)\n    attn_prob = tf.layers.dropout(attn_prob, dropatt, training=is_training)\n\n    attn_vec = tf.einsum(\'ijbn,jbnd->ibnd\', attn_prob, w_head_v)\n    size_t = tf.shape(attn_vec)\n    attn_vec = tf.reshape(attn_vec, [size_t[0], size_t[1], n_head * d_head])\n\n    attn_out = tf.layers.dense(attn_vec, d_model, use_bias=False,\n                               kernel_initializer=kernel_initializer, name=\'o\')\n    attn_out = tf.layers.dropout(attn_out, dropout, training=is_training)\n    befornorm = attn_out\n\n    output = tf.contrib.layers.layer_norm(attn_out + w, begin_norm_axis=-1)\n  return output#, rw_head_q, mems, w_heads, tmp, AC, bd_before, BD, attn_prob, output, attn_vec, befornorm\n\n\ndef embedding_lookup(lookup_table, x, use_tpu=True):\n  if use_tpu:\n    n_token = tf.shape(lookup_table)[0]\n    one_hot_idx = tf.one_hot(x, n_token)\n    if one_hot_idx.shape.ndims == 2:\n      return tf.einsum(\'nd,in->id\', lookup_table, one_hot_idx)\n    else:\n      return tf.einsum(\'nd,ibn->ibd\', lookup_table, one_hot_idx)\n  else:\n    return tf.nn.embedding_lookup(lookup_table, x)\n\n\ndef mask_adaptive_embedding_lookup(x, n_token, d_embed, d_proj, cutoffs, initializer,\n                                   proj_initializer, div_val=1,\n                                   proj_same_dim=True,\n                                   scope=\'adaptive_embed\', **kwargs):\n  emb_scale = d_proj ** 0.5\n  with tf.variable_scope(scope):\n    if div_val == 1:\n      lookup_table = tf.get_variable(\'lookup_table\', [n_token, d_embed],\n                                     initializer=initializer)\n      y = embedding_lookup(lookup_table, x, use_tpu=False)\n      if d_proj != d_embed:\n        proj_W = tf.get_variable(\'proj_W\', [d_embed, d_proj],\n                                 initializer=proj_initializer)\n        y = tf.einsum(\'ibe,ed->ibd\', y, proj_W)\n      else:\n        proj_W = None\n      ret_params = [lookup_table, proj_W]\n    else:\n      tables, projs = [], []\n      cutoff_ends = [0] + cutoffs + [n_token]\n      x_size = tf.shape(x)\n      y = tf.zeros([x_size[0], x_size[1], d_proj])\n      for i in range(len(cutoff_ends) - 1):\n        with tf.variable_scope(\'cutoff_{}\'.format(i)):\n          l_idx, r_idx = cutoff_ends[i], cutoff_ends[i + 1]\n          mask = (x >= l_idx) & (x < r_idx)\n          cur_x = tf.boolean_mask(x, mask) - l_idx\n          cur_d_embed = d_embed // (div_val ** i)\n          print(""shape of lookup_table is {0}/{1}"".format(r_idx-l_idx,cur_d_embed))\n          lookup_table = tf.get_variable(\'lookup_table\',\n                                         [r_idx - l_idx, cur_d_embed],\n                                         initializer=initializer)\n          cur_y = embedding_lookup(lookup_table, cur_x, use_tpu=False)\n          if d_proj == cur_d_embed and not proj_same_dim:\n            proj_W = None\n          else:\n            proj_W = tf.get_variable(\'proj_W\', [cur_d_embed, d_proj],\n                                     initializer=proj_initializer)\n            cur_y = tf.einsum(\'id,de->ie\', cur_y, proj_W)\n          mask_idx = tf.to_int64(tf.where(mask))\n          y += tf.scatter_nd(mask_idx, cur_y, tf.to_int64(tf.shape(y)))\n          tables.append(lookup_table)\n          projs.append(proj_W)\n      ret_params = [tables, projs]\n\n  y *= emb_scale\n  return y, ret_params\n\n\ndef mul_adaptive_embedding_lookup(x, n_token, d_embed, d_proj, cutoffs, initializer,\n                                  proj_initializer, div_val=1, perms=None,\n                                  proj_same_dim=True,\n                                  scope=\'adaptive_embed\'):\n  """"""\n  perms: If None, first compute W = W1 x W2 (projection for each bin),\n      and then compute X x W (embedding lookup). If not None,\n      use bin-based embedding lookup with max_bin_size defined by\n      the shape of perms.\n  """"""\n  emb_scale = d_proj ** 0.5\n  with tf.variable_scope(scope):\n    if div_val == 1:\n      lookup_table = tf.get_variable(\'lookup_table\', [n_token, d_embed],\n                                     initializer=initializer)\n      y = embedding_lookup(lookup_table, x)\n      if d_proj != d_embed:\n        proj_W = tf.get_variable(\'proj_W\', [d_embed, d_proj],\n                                 initializer=proj_initializer)\n        y = tf.einsum(\'ibe,ed->ibd\', y, proj_W)\n      else:\n        proj_W = None\n      ret_params = [lookup_table, proj_W]\n    else:\n      tables, projs = [], []\n      cutoff_ends = [0] + cutoffs + [n_token]\n      x_size = tf.shape(x)\n      if perms is None:\n        cat_lookup = []\n      else:\n        cat_lookup = tf.zeros([x_size[0], x_size[1], d_proj])\n      for i in range(len(cutoff_ends) - 1):\n        with tf.variable_scope(\'cutoff_{}\'.format(i)):\n          l_idx, r_idx = cutoff_ends[i], cutoff_ends[i + 1]\n          cur_d_embed = d_embed // (div_val ** i)\n          lookup_table = tf.get_variable(\'lookup_table\',\n                                         [r_idx - l_idx, cur_d_embed],\n                                         initializer=initializer)\n          if cur_d_embed == d_proj and not proj_same_dim:\n            proj_W = None\n          else:\n            proj_W = tf.get_variable(\'proj_W\', [cur_d_embed, d_proj],\n                                   initializer=proj_initializer)\n          if perms is None:\n            cat_lookup.append(tf.einsum(\'ie,ed->id\', lookup_table, proj_W))\n          else:\n            # speed up the computation of the first bin\n            # also save some meory\n            if i == 0:\n              cur_y = embedding_lookup(lookup_table, tf.minimum(x, r_idx - 1))\n              if proj_W is not None:\n                cur_y = tf.einsum(\'ibe,ed->ibd\', cur_y, proj_W)\n              cur_y *= perms[i][:, :, None]\n              cat_lookup += cur_y\n            else:\n              cur_x = tf.einsum(\'ib,ibk->k\', tf.to_float(x - l_idx), perms[i])\n              cur_x = tf.to_int32(cur_x)\n              cur_y = embedding_lookup(lookup_table, cur_x)\n              if proj_W is not None:\n                cur_y = tf.einsum(\'ke,ed->kd\', cur_y, proj_W)\n              cat_lookup += tf.einsum(\'kd,ibk->ibd\', cur_y, perms[i])\n          tables.append(lookup_table)\n          projs.append(proj_W)\n      if perms is None:\n        cat_lookup = tf.concat(cat_lookup, 0)\n        y = embedding_lookup(cat_lookup, x)\n      else:\n        y = cat_lookup\n      ret_params = [tables, projs]\n\n  y *= emb_scale\n  return y, ret_params\n\n\ndef mask_adaptive_logsoftmax(hidden, target, n_token, d_embed, d_proj, cutoffs,\n                             params, tie_projs,\n                             initializer=None, proj_initializer=None,\n                             div_val=1, scope=\'adaptive_softmax\',\n                             proj_same_dim=True,\n                             return_mean=True, **kwargs):\n  def _logit(x, W, b, proj):\n    y = x\n    if proj is not None:\n      y = tf.einsum(\'ibd,ed->ibe\', y, proj)\n    return tf.einsum(\'ibd,nd->ibn\', y, W) + b\n\n  params_W, params_projs = params[0], params[1]\n\n  def _gather_logprob(logprob, target):\n    lp_size = tf.shape(logprob)\n    r = tf.range(lp_size[0])\n    idx = tf.stack([r, target], 1)\n    return tf.gather_nd(logprob, idx)\n\n  with tf.variable_scope(scope):\n    if len(cutoffs) == 0:\n      print(""no adaptive"")\n      softmax_b = tf.get_variable(\'bias\', [n_token],\n                                  initializer=tf.zeros_initializer())\n      output = _logit(hidden, params_W, softmax_b, params_projs)\n      output_ = tf.nn.softmax(output, dim = 2)\n      nll = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target,\n                                                           logits=output)\n    else:\n      cutoff_ends = [0] + cutoffs + [n_token]\n      nll = tf.zeros_like(target, dtype=tf.float32)\n      for i in range(len(cutoff_ends) - 1):\n        with tf.variable_scope(\'cutoff_{}\'.format(i)):\n          l_idx, r_idx = cutoff_ends[i], cutoff_ends[i + 1]\n          mask = (target >= l_idx) & (target < r_idx)\n          mask_idx = tf.where(mask)\n          cur_target = tf.boolean_mask(target, mask) - l_idx\n          cur_d_embed = d_embed // (div_val ** i)\n\n          if div_val == 1:\n            cur_W = params_W[l_idx: r_idx]\n          else:\n            cur_W = params_W[i]\n          cur_b = tf.get_variable(\'b\', [r_idx - l_idx],\n                                  initializer=tf.zeros_initializer())\n          if tie_projs[i]:\n            if div_val == 1:\n              cur_proj = params_projs\n            else:\n              cur_proj = params_projs[i]\n          else:\n            if (div_val == 1 or not proj_same_dim) and d_proj == cur_d_embed:\n              cur_proj = None\n            else:\n              cur_proj = tf.get_variable(\'proj\', [cur_d_embed, d_proj],\n                                         initializer=proj_initializer)\n          if i == 0:\n            cluster_W = tf.get_variable(\'cluster_W\', [len(cutoffs), d_embed],\n                                        initializer=tf.zeros_initializer())\n            cluster_b = tf.get_variable(\'cluster_b\', [len(cutoffs)],\n                                        initializer=tf.zeros_initializer())\n            cur_W = tf.concat([cur_W, cluster_W], 0)\n            cur_b = tf.concat([cur_b, cluster_b], 0)\n\n            head_logit = _logit(hidden, cur_W, cur_b, cur_proj)\n            head_logprob = tf.nn.log_softmax(head_logit)\n            cur_head_logprob = tf.boolean_mask(head_logprob, mask)\n            cur_logprob = _gather_logprob(cur_head_logprob, cur_target)\n          else:\n            cur_head_logprob = tf.boolean_mask(head_logprob, mask)\n            cur_hidden = tf.boolean_mask(hidden, mask)\n            tail_logit = tf.squeeze(_logit(\n                cur_hidden[None], cur_W, cur_b, cur_proj), 0)\n            tail_logprob = tf.nn.log_softmax(tail_logit)\n            cur_logprob = (cur_head_logprob[:, cutoff_ends[1] + i - 1] +\n                           _gather_logprob(tail_logprob, cur_target))\n          nll += tf.scatter_nd(mask_idx, -cur_logprob,\n                                 tf.to_int64(tf.shape(nll)))\n  if return_mean:\n    nll = tf.reduce_mean(nll)\n  return nll, output_\n\n\ndef mul_adaptive_logsoftmax(hidden, target, n_token, d_embed, d_proj, cutoffs,\n                            params, tie_projs,\n                            initializer=None, proj_initializer=None,\n                            div_val=1, perms=None, proj_same_dim=True,\n                            scope=\'adaptive_softmax\',\n                            **kwargs):\n  def _logit(x, W, b, proj):\n    y = x\n    if x.shape.ndims == 3:\n      if proj is not None:\n        y = tf.einsum(\'ibd,ed->ibe\', y, proj)\n      return tf.einsum(\'ibd,nd->ibn\', y, W) + b\n    else:\n      if proj is not None:\n        y = tf.einsum(\'id,ed->ie\', y, proj)\n      return tf.einsum(\'id,nd->in\', y, W) + b\n\n  params_W, params_projs = params[0], params[1]\n\n  with tf.variable_scope(scope):\n    if len(cutoffs) == 0:\n      softmax_b = tf.get_variable(\'bias\', [n_token],\n                                  initializer=tf.zeros_initializer())\n      output = _logit(hidden, params_W, softmax_b, params_projs)\n      nll = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target,\n                                                           logits=output)\n      nll = tf.reduce_mean(nll)\n    else:\n      total_loss, total_cnt = 0, 0\n      cutoff_ends = [0] + cutoffs + [n_token]\n      for i in range(len(cutoff_ends) - 1):\n        with tf.variable_scope(\'cutoff_{}\'.format(i)):\n          l_idx, r_idx = cutoff_ends[i], cutoff_ends[i + 1]\n\n          cur_d_embed = d_embed // (div_val ** i)\n\n          if div_val == 1:\n            cur_W = params_W[l_idx: r_idx]\n          else:\n            cur_W = params_W[i]\n          cur_b = tf.get_variable(\'b\', [r_idx - l_idx],\n                                  initializer=tf.zeros_initializer())\n          if tie_projs[i]:\n            if div_val == 1:\n              cur_proj = params_projs\n            else:\n              cur_proj = params_projs[i]\n          else:\n            if (div_val == 1 or not proj_same_dim) and d_proj == cur_d_embed:\n              cur_proj = None\n            else:\n              cur_proj = tf.get_variable(\'proj\', [cur_d_embed, d_proj],\n                                         initializer=proj_initializer)\n\n          if i == 0:\n            cluster_W = tf.get_variable(\'cluster_W\', [len(cutoffs), d_embed],\n                                        initializer=tf.zeros_initializer())\n            cluster_b = tf.get_variable(\'cluster_b\', [len(cutoffs)],\n                                        initializer=tf.zeros_initializer())\n            cur_W = tf.concat([cur_W, cluster_W], 0)\n            cur_b = tf.concat([cur_b, cluster_b], 0)\n\n            head_logit = _logit(hidden, cur_W, cur_b, cur_proj)\n\n            head_target = kwargs.get(""head_target"")\n            head_nll = tf.nn.sparse_softmax_cross_entropy_with_logits(\n                labels=head_target,\n                logits=head_logit)\n\n            masked_loss = head_nll * perms[i]\n            total_loss += tf.reduce_sum(masked_loss)\n            total_cnt += tf.reduce_sum(perms[i])\n\n            # head_logprob = tf.nn.log_softmax(head_logit)\n\n            # final_logprob = head_logprob * perms[i][:, :, None]\n            # final_target = tf.one_hot(target, tf.shape(head_logprob)[2])\n            # total_loss -= tf.einsum(\'ibn,ibn->\', final_logprob, final_target)\n            # total_cnt += tf.reduce_sum(perms[i])\n          else:\n            cur_head_nll = tf.einsum(\'ib,ibk->k\', head_nll, perms[i])\n\n            cur_hidden = tf.einsum(\'ibd,ibk->kd\', hidden, perms[i])\n            tail_logit = _logit(cur_hidden, cur_W, cur_b, cur_proj)\n\n            tail_target = tf.einsum(\'ib,ibk->k\', tf.to_float(target - l_idx),\n                                    perms[i])\n            tail_nll = tf.nn.sparse_softmax_cross_entropy_with_logits(\n                labels=tf.to_int32(tail_target),\n                logits=tail_logit)\n\n            sum_nll = cur_head_nll + tail_nll\n            mask = tf.reduce_sum(perms[i], [0, 1])\n\n            masked_loss = sum_nll * mask\n            total_loss += tf.reduce_sum(masked_loss)\n            total_cnt += tf.reduce_sum(mask)\n\n      nll = total_loss / total_cnt\n\n  return nll, output\n\n\ndef _create_mask(qlen, mlen, same_length=False):\n  attn_mask = tf.ones([qlen, qlen])\n  mask_u = tf.matrix_band_part(attn_mask, 0, -1)\n  mask_dia = tf.matrix_band_part(attn_mask, 0, 0)\n  attn_mask_pad = tf.zeros([qlen, mlen])\n  ret = tf.concat([attn_mask_pad, mask_u - mask_dia], 1)\n  if same_length:\n    mask_l = tf.matrix_band_part(attn_mask, -1, 0)\n    ret = tf.concat([ret[:, :qlen] + mask_l - mask_dia, ret[:, qlen:]], 1)\n  return ret\n\ndef _cache_mem(curr_out, prev_mem, mem_len=None):\n  if mem_len is None or prev_mem is None:\n    new_mem = curr_out\n  elif mem_len == 0:\n    return prev_mem\n  else:\n    new_mem = tf.concat([prev_mem, curr_out], 0)[- mem_len:]\n\n  return tf.stop_gradient(new_mem)\n\n\ndef transformer(dec_inp, target, mems, n_token, n_layer, d_model, d_embed,\n                n_head, d_head, d_inner, dropout, dropatt,\n                initializer, is_training, proj_initializer=None,\n                mem_len=None, cutoffs=[], div_val=1, tie_projs=[],\n                same_length=False, clamp_len=-1, use_tpu=True,\n                input_perms=None, target_perms=None, head_target=None,\n                untie_r=False, proj_same_dim=True,\n                scope=\'transformer\'):\n  """"""\n  cutoffs: a list of python int. Cutoffs for adaptive softmax.\n  tie_projs: a list of python bools. Whether to tie the projections.\n  use_tpu: if True, use one_hot in embedding lookup and bin-based implementation\n        of adaptive softmax.\n  perms: a list of tensors. Each tensor should of size [len, bsz, bin_size].\n        Only used in the adaptive setting.\n  """"""\n  new_mems = []\n  with tf.variable_scope(scope):\n    if untie_r:\n      r_w_bias = tf.get_variable(\'r_w_bias\', [n_layer, n_head, d_head],\n                               initializer=initializer)\n      r_r_bias = tf.get_variable(\'r_r_bias\', [n_layer, n_head, d_head],\n                                 initializer=initializer)\n    else:\n      r_w_bias = tf.get_variable(\'r_w_bias\', [n_head, d_head],\n                                 initializer=initializer)\n      r_r_bias = tf.get_variable(\'r_r_bias\', [n_head, d_head],\n                                 initializer=initializer)\n\n    qlen = tf.shape(dec_inp)[0]\n    #qlen = tf.Print(qlen, [qlen], message = \'debug qlen\')\n    mlen = tf.shape(mems[0])[0] if mems is not None else 0\n    #mlen = tf.Print(mlen, [mlen], message = \'debug mlen\')\n    klen = mlen + qlen\n\n    if proj_initializer is None:\n      proj_initializer = initializer\n    lookup_fn = (mul_adaptive_embedding_lookup if use_tpu else\n                 mask_adaptive_embedding_lookup)\n    embeddings, shared_params = lookup_fn(\n        x=dec_inp,\n        n_token=n_token,\n        d_embed=d_embed,\n        d_proj=d_model,\n        cutoffs=cutoffs,\n        initializer=initializer,\n        proj_initializer=proj_initializer,\n        div_val= div_val,\n        perms=input_perms,\n        proj_same_dim=proj_same_dim)\n\n    attn_mask = _create_mask(qlen, mlen, same_length)\n\n    pos_seq = tf.range(klen - 1, -1, -1.0)\n    if clamp_len > 0:\n      pos_seq = tf.minimum(pos_seq, clamp_len)\n    inv_freq = 1 / (10000 ** (tf.range(0, d_model, 2.0) / d_model))\n    pos_emb = positional_embedding(pos_seq, inv_freq)\n\n    output = tf.layers.dropout(embeddings, dropout, training=is_training)\n    pos_emb = tf.layers.dropout(pos_emb, dropout, training=is_training)\n\n    if mems is None:\n      mems = [None] * n_layer\n\n    for i in range(n_layer):\n      # cache new mems\n      new_mems.append(_cache_mem(output, mems[i], mem_len))\n\n      with tf.variable_scope(\'layer_{}\'.format(i)):\n        output = rel_multihead_attn(\n            w=output,\n            r=pos_emb,\n            r_w_bias=r_w_bias if not untie_r else r_w_bias[i],\n            r_r_bias=r_r_bias if not untie_r else r_r_bias[i],\n            attn_mask=attn_mask,\n            mems=mems[i],\n            d_model=d_model,\n            n_head=n_head,\n            d_head=d_head,\n            dropout=dropout,\n            dropatt=dropatt,\n            is_training=is_training,\n            kernel_initializer=initializer,\n            index = i)\n        output = positionwise_FF(\n            inp=output,\n            d_model=d_model,\n            d_inner=d_inner,\n            dropout=dropout,\n            kernel_initializer=initializer,\n            is_training=is_training)\n\n    output = tf.layers.dropout(output, dropout, training=is_training)\n\n    logsoftmax_fn = (mul_adaptive_logsoftmax if use_tpu else\n                     mask_adaptive_logsoftmax)\n    loss, output_ = logsoftmax_fn(\n        hidden=output,\n        target=target,\n        n_token=n_token,\n        d_embed=d_embed,\n        d_proj=d_model,\n        cutoffs=cutoffs,\n        params=shared_params,\n        tie_projs=tie_projs,\n        initializer=initializer,\n        proj_initializer=proj_initializer,\n        div_val=div_val,\n        perms=target_perms,\n        head_target=head_target,\n        proj_same_dim=proj_same_dim)\n    return loss, new_mems, output_\n\n'"
deltann/infer/example/python/standard_transformer/__init__.py,0,b''
deltann/infer/example/python/standard_transformer/model.py,82,"b'# coding=utf-8\n""""""The main BERT model and related functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport copy\nimport json\nimport math\nimport re\nimport numpy as np\nimport six\nimport tensorflow as tf\n\n\nclass BertConfig(object):\n  """"""Configuration for `BertModel`.""""""\n\n  def __init__(self,\n               vocab_size,\n               hidden_size=768,\n               num_hidden_layers=12,\n               num_attention_heads=12,\n               intermediate_size=3072,\n               hidden_act=""gelu"",\n               hidden_dropout_prob=0.1,\n               attention_probs_dropout_prob=0.1,\n               max_position_embeddings=512,\n               type_vocab_size=16,\n               initializer_range=0.02):\n    """"""Constructs BertConfig.\n\n    Args:\n      vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n      hidden_size: Size of the encoder layers and the pooler layer.\n      num_hidden_layers: Number of hidden layers in the Transformer encoder.\n      num_attention_heads: Number of attention heads for each attention layer in\n        the Transformer encoder.\n      intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)\n        layer in the Transformer encoder.\n      hidden_act: The non-linear activation function (function or string) in the\n        encoder and pooler.\n      hidden_dropout_prob: The dropout probability for all fully connected\n        layers in the embeddings, encoder, and pooler.\n      attention_probs_dropout_prob: The dropout ratio for the attention\n        probabilities.\n      max_position_embeddings: The maximum sequence length that this model might\n        ever be used with. Typically set this to something large just in case\n        (e.g., 512 or 1024 or 2048).\n      type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n        `BertModel`.\n      initializer_range: The stdev of the truncated_normal_initializer for\n        initializing all weight matrices.\n    """"""\n    self.vocab_size = vocab_size\n    self.hidden_size = hidden_size\n    self.num_hidden_layers = num_hidden_layers\n    self.num_attention_heads = num_attention_heads\n    self.hidden_act = hidden_act\n    self.intermediate_size = intermediate_size\n    self.hidden_dropout_prob = hidden_dropout_prob\n    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n    self.max_position_embeddings = max_position_embeddings\n    self.type_vocab_size = type_vocab_size\n    self.initializer_range = initializer_range\n\n  @classmethod\n  def from_dict(cls, json_object):\n    """"""Constructs a `BertConfig` from a Python dictionary of parameters.""""""\n    config = BertConfig(vocab_size=None)\n    for (key, value) in six.iteritems(json_object):\n      config.__dict__[key] = value\n    return config\n\n  @classmethod\n  def from_json_file(cls, json_file):\n    """"""Constructs a `BertConfig` from a json file of parameters.""""""\n    with tf.gfile.GFile(json_file, ""r"") as reader:\n      text = reader.read()\n    return cls.from_dict(json.loads(text))\n\n  def to_dict(self):\n    """"""Serializes this instance to a Python dictionary.""""""\n    output = copy.deepcopy(self.__dict__)\n    return output\n\n  def to_json_string(self):\n    """"""Serializes this instance to a JSON string.""""""\n    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n\nclass BertModel(object):\n  """"""BERT model (""Bidirectional Encoder Representations from Transformers"").\n\n  Example usage:\n\n  ```python\n  # Already been converted into WordPiece token ids\n  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n\n  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n\n  model = modeling.BertModel(config=config, is_training=True,\n    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n\n  label_embeddings = tf.get_variable(...)\n  pooled_output = model.get_pooled_output()\n  logits = tf.matmul(pooled_output, label_embeddings)\n  ...\n  ```\n  """"""\n\n  def __init__(self,\n               config,\n               is_training,\n               input_ids,\n               input_mask=None,\n               token_type_ids=None,\n               use_one_hot_embeddings=False,\n               scope=None):\n    """"""Constructor for BertModel.\n\n    Args:\n      config: `BertConfig` instance.\n      is_training: bool. true for training model, false for eval model. Controls\n        whether dropout will be applied.\n      input_ids: int32 Tensor of shape [batch_size, seq_length].\n      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n        embeddings or tf.embedding_lookup() for the word embeddings.\n      scope: (optional) variable scope. Defaults to ""bert"".\n\n    Raises:\n      ValueError: The config is invalid or one of the input tensor shapes\n        is invalid.\n    """"""\n    config = copy.deepcopy(config)\n    if not is_training:\n      config.hidden_dropout_prob = 0.0\n      config.attention_probs_dropout_prob = 0.0\n\n    input_shape = get_shape_list(input_ids, expected_rank=2)\n    batch_size = input_shape[0]\n    seq_length = input_shape[1]\n\n    if input_mask is None:\n      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    if token_type_ids is None:\n      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    with tf.variable_scope(scope, default_name=""bert""):\n      with tf.variable_scope(""embeddings""):\n        # Perform embedding lookup on the word ids.\n        (self.embedding_output, self.embedding_table) = embedding_lookup(\n            input_ids=input_ids,\n            vocab_size=config.vocab_size,\n            embedding_size=config.hidden_size,\n            initializer_range=config.initializer_range,\n            word_embedding_name=""word_embeddings"",\n            use_one_hot_embeddings=use_one_hot_embeddings)\n\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.embedding_output,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=config.type_vocab_size,\n            token_type_embedding_name=""token_type_embeddings"",\n            use_position_embeddings=True,\n            position_embedding_name=""position_embeddings"",\n            initializer_range=config.initializer_range,\n            max_position_embeddings=config.max_position_embeddings,\n            dropout_prob=config.hidden_dropout_prob)\n\n      with tf.variable_scope(""encoder""):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask = create_attention_mask_from_input_mask(\n            input_ids, input_mask)\n\n        # Run the stacked transformer.\n        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n        self.all_encoder_layers = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=config.hidden_size,\n            num_hidden_layers=config.num_hidden_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            intermediate_act_fn=get_activation(config.hidden_act),\n            hidden_dropout_prob=config.hidden_dropout_prob,\n            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n            initializer_range=config.initializer_range,\n            do_return_all_layers=True)\n\n      self.sequence_output = self.all_encoder_layers[-1]\n      # The ""pooler"" converts the encoded sequence tensor of shape\n      # [batch_size, seq_length, hidden_size] to a tensor of shape\n      # [batch_size, hidden_size]. This is necessary for segment-level\n      # (or segment-pair-level) classification tasks where we need a fixed\n      # dimensional representation of the segment.\n      with tf.variable_scope(""pooler""):\n        # We ""pool"" the model by simply taking the hidden state corresponding\n        # to the first token. We assume that this has been pre-trained\n        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n        self.pooled_output = tf.layers.dense(\n            first_token_tensor,\n            config.hidden_size,\n            activation=tf.tanh,\n            kernel_initializer=create_initializer(config.initializer_range))\n\n  def get_pooled_output(self):\n    return self.pooled_output\n\n  def get_sequence_output(self):\n    """"""Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    """"""\n    return self.sequence_output\n\n  def get_all_encoder_layers(self):\n    return self.all_encoder_layers\n\n  def get_embedding_output(self):\n    """"""Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    """"""\n    return self.embedding_output\n\n  def get_embedding_table(self):\n    return self.embedding_table\n\n\ndef gelu(x):\n  """"""Gaussian Error Linear Unit.\n\n  This is a smoother version of the RELU.\n  Original paper: https://arxiv.org/abs/1606.08415\n  Args:\n    x: float Tensor to perform activation.\n\n  Returns:\n    `x` with the GELU activation applied.\n  """"""\n  cdf = 0.5 * (1.0 + tf.tanh(\n      (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n  return x * cdf\n\ndef gelu_new(x):\n    cdf = 0.5 * (1.0 + tf.erf(tf.realdiv(x, tf.sqrt(tf.constant(2, dtype=tf.float32)))))\n    return x * cdf\n\n\ndef get_activation(activation_string):\n  """"""Maps a string to a Python function, e.g., ""relu"" => `tf.nn.relu`.\n\n  Args:\n    activation_string: String name of the activation function.\n\n  Returns:\n    A Python function corresponding to the activation function. If\n    `activation_string` is None, empty, or ""linear"", this will return None.\n    If `activation_string` is not a string, it will return `activation_string`.\n\n  Raises:\n    ValueError: The `activation_string` does not correspond to a known\n      activation.\n  """"""\n\n  # We assume that anything that""s not a string is already an activation\n  # function, so we just return it.\n  if not isinstance(activation_string, six.string_types):\n    return activation_string\n\n  if not activation_string:\n    return None\n\n  act = activation_string.lower()\n  if act == ""linear"":\n    return None\n  elif act == ""relu"":\n    return tf.nn.relu\n  elif act == ""gelu"":\n    return gelu\n  elif act == ""tanh"":\n    return tf.tanh\n  else:\n    raise ValueError(""Unsupported activation: %s"" % act)\n\n\ndef get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n  """"""Compute the union of the current variables and checkpoint variables.""""""\n  assignment_map = {}\n  initialized_variable_names = {}\n\n  name_to_variable = collections.OrderedDict()\n  for var in tvars:\n    name = var.name\n    m = re.match(""^(.*):\\\\d+$"", name)\n    if m is not None:\n      name = m.group(1)\n    name_to_variable[name] = var\n\n  init_vars = tf.train.list_variables(init_checkpoint)\n\n  assignment_map = collections.OrderedDict()\n  for x in init_vars:\n    (name, var) = (x[0], x[1])\n    if name not in name_to_variable:\n      continue\n    assignment_map[name] = name\n    initialized_variable_names[name] = 1\n    initialized_variable_names[name + "":0""] = 1\n\n  return (assignment_map, initialized_variable_names)\n\n\ndef dropout(input_tensor, dropout_prob):\n  """"""Perform dropout.\n\n  Args:\n    input_tensor: float Tensor.\n    dropout_prob: Python float. The probability of dropping out a value (NOT of\n      *keeping* a dimension as in `tf.nn.dropout`).\n\n  Returns:\n    A version of `input_tensor` with dropout applied.\n  """"""\n  if dropout_prob is None or dropout_prob == 0.0:\n    return input_tensor\n\n  output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n  return output\n\n\ndef layer_norm(input_tensor, name=None):\n  """"""Run layer normalization on the last dimension of the tensor.""""""\n  return tf.contrib.layers.layer_norm(\n      inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)\n\n\ndef layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\n  """"""Runs layer normalization followed by dropout.""""""\n  output_tensor = layer_norm(input_tensor, name)\n  output_tensor = dropout(output_tensor, dropout_prob)\n  return output_tensor\n\n\ndef create_initializer(initializer_range=0.02):\n  """"""Creates a `truncated_normal_initializer` with the given range.""""""\n  return tf.truncated_normal_initializer(stddev=initializer_range)\n\n\ndef embedding_lookup(input_ids,\n                     vocab_size,\n                     embedding_size=128,\n                     initializer_range=0.02,\n                     word_embedding_name=""word_embeddings"",\n                     use_one_hot_embeddings=False):\n  """"""Looks up words embeddings for id tensor.\n\n  Args:\n    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n      ids.\n    vocab_size: int. Size of the embedding vocabulary.\n    embedding_size: int. Width of the word embeddings.\n    initializer_range: float. Embedding initialization range.\n    word_embedding_name: string. Name of the embedding table.\n    use_one_hot_embeddings: bool. If True, use one-hot method for word\n      embeddings. If False, use `tf.gather()`.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, embedding_size].\n  """"""\n  # This function assumes that the input is of shape [batch_size, seq_length,\n  # num_inputs].\n  #\n  # If the input is a 2D tensor of shape [batch_size, seq_length], we\n  # reshape to [batch_size, seq_length, 1].\n  if input_ids.shape.ndims == 2:\n    input_ids = tf.expand_dims(input_ids, axis=[-1])\n\n  embedding_table = tf.get_variable(\n      name=word_embedding_name,\n      shape=[vocab_size, embedding_size],\n      initializer=create_initializer(initializer_range))\n\n  flat_input_ids = tf.reshape(input_ids, [-1])\n  if use_one_hot_embeddings:\n    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n    output = tf.matmul(one_hot_input_ids, embedding_table)\n  else:\n    output = tf.gather(embedding_table, flat_input_ids)\n\n  input_shape = get_shape_list(input_ids)\n\n  output = tf.reshape(output,\n                      input_shape[0:-1] + [input_shape[-1] * embedding_size])\n  return (output, embedding_table)\n\n\ndef embedding_postprocessor(input_tensor,\n                            use_token_type=False,\n                            token_type_ids=None,\n                            token_type_vocab_size=16,\n                            token_type_embedding_name=""token_type_embeddings"",\n                            use_position_embeddings=True,\n                            position_embedding_name=""position_embeddings"",\n                            initializer_range=0.02,\n                            max_position_embeddings=512,\n                            dropout_prob=0.1):\n  """"""Performs various post-processing on a word embedding tensor.\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length,\n      embedding_size].\n    use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      Must be specified if `use_token_type` is True.\n    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.\n    token_type_embedding_name: string. The name of the embedding table variable\n      for token type ids.\n    use_position_embeddings: bool. Whether to add position embeddings for the\n      position of each token in the sequence.\n    position_embedding_name: string. The name of the embedding table variable\n      for positional embeddings.\n    initializer_range: float. Range of the weight initialization.\n    max_position_embeddings: int. Maximum sequence length that might ever be\n      used with this model. This can be longer than the sequence length of\n      input_tensor, but cannot be shorter.\n    dropout_prob: float. Dropout probability applied to the final output tensor.\n\n  Returns:\n    float tensor with same shape as `input_tensor`.\n\n  Raises:\n    ValueError: One of the tensor shapes or input values is invalid.\n  """"""\n  input_shape = get_shape_list(input_tensor, expected_rank=3)\n  batch_size = input_shape[0]\n  seq_length = input_shape[1]\n  width = input_shape[2]\n\n  output = input_tensor\n\n  if use_token_type:\n    if token_type_ids is None:\n      raise ValueError(""`token_type_ids` must be specified if""\n                       ""`use_token_type` is True."")\n    token_type_table = tf.get_variable(\n        name=token_type_embedding_name,\n        shape=[token_type_vocab_size, width],\n        initializer=create_initializer(initializer_range))\n    # This vocab will be small so we always do one-hot here, since it is always\n    # faster for a small vocabulary.\n    flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)\n    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n    token_type_embeddings = tf.reshape(token_type_embeddings,\n                                       [batch_size, seq_length, width])\n    output += token_type_embeddings\n\n  if use_position_embeddings:\n    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)\n    with tf.control_dependencies([assert_op]):\n      full_position_embeddings = tf.get_variable(\n          name=position_embedding_name,\n          shape=[max_position_embeddings, width],\n          initializer=create_initializer(initializer_range))\n      # Since the position embedding table is a learned variable, we create it\n      # using a (long) sequence length `max_position_embeddings`. The actual\n      # sequence length might be shorter than this, for faster training of\n      # tasks that do not have long sequences.\n      #\n      # So `full_position_embeddings` is effectively an embedding table\n      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n      # perform a slice.\n      position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n                                     [seq_length, -1])\n      num_dims = len(output.shape.as_list())\n\n      # Only the last two dimensions are relevant (`seq_length` and `width`), so\n      # we broadcast among the first dimensions, which is typically just\n      # the batch size.\n      position_broadcast_shape = []\n      for _ in range(num_dims - 2):\n        position_broadcast_shape.append(1)\n      position_broadcast_shape.extend([seq_length, width])\n      position_embeddings = tf.reshape(position_embeddings,\n                                       position_broadcast_shape)\n      output += position_embeddings\n\n  output = layer_norm_and_dropout(output, dropout_prob)\n  return output\n\n\ndef create_attention_mask_from_input_mask(from_tensor, to_mask):\n  """"""Create 3D attention mask from a 2D tensor mask.\n\n  Args:\n    from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n    to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n  """"""\n  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n  batch_size = from_shape[0]\n  from_seq_length = from_shape[1]\n\n  to_shape = get_shape_list(to_mask, expected_rank=2)\n  to_seq_length = to_shape[1]\n\n  to_mask = tf.cast(\n      tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n  # We don\'t assume that `from_tensor` is a mask (although it could be). We\n  # don\'t actually care if we attend *from* padding tokens (only *to* padding)\n  # tokens so we create a tensor of all ones.\n  #\n  # `broadcast_ones` = [batch_size, from_seq_length, 1]\n  broadcast_ones = tf.ones(\n      shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n  # Here we broadcast along two dimensions to create the mask.\n  mask = broadcast_ones * to_mask\n\n  return mask\n\n\ndef attention_layer(from_tensor,\n                    to_tensor,\n                    attention_mask=None,\n                    num_attention_heads=1,\n                    size_per_head=512,\n                    query_act=None,\n                    key_act=None,\n                    value_act=None,\n                    attention_probs_dropout_prob=0.0,\n                    initializer_range=0.02,\n                    do_return_2d_tensor=False,\n                    batch_size=None,\n                    from_seq_length=None,\n                    to_seq_length=None):\n  """"""Performs multi-headed attention from `from_tensor` to `to_tensor`.\n\n  This is an implementation of multi-headed attention based on ""Attention\n  is all you Need"". If `from_tensor` and `to_tensor` are the same, then\n  this is self-attention. Each timestep in `from_tensor` attends to the\n  corresponding sequence in `to_tensor`, and returns a fixed-with vector.\n\n  This function first projects `from_tensor` into a ""query"" tensor and\n  `to_tensor` into ""key"" and ""value"" tensors. These are (effectively) a list\n  of tensors of length `num_attention_heads`, where each tensor is of shape\n  [batch_size, seq_length, size_per_head].\n\n  Then, the query and key tensors are dot-producted and scaled. These are\n  softmaxed to obtain attention probabilities. The value tensors are then\n  interpolated by these probabilities, then concatenated back to a single\n  tensor and returned.\n\n  In practice, the multi-headed attention are done with transposes and\n  reshapes rather than actual separate tensors.\n\n  Args:\n    from_tensor: float Tensor of shape [batch_size, from_seq_length,\n      from_width].\n    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n    attention_mask: (optional) int32 Tensor of shape [batch_size,\n      from_seq_length, to_seq_length]. The values should be 1 or 0. The\n      attention scores will effectively be set to -infinity for any positions in\n      the mask that are 0, and will be unchanged for positions that are 1.\n    num_attention_heads: int. Number of attention heads.\n    size_per_head: int. Size of each attention head.\n    query_act: (optional) Activation function for the query transform.\n    key_act: (optional) Activation function for the key transform.\n    value_act: (optional) Activation function for the value transform.\n    attention_probs_dropout_prob: (optional) float. Dropout probability of the\n      attention probabilities.\n    initializer_range: float. Range of the weight initializer.\n    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size\n      * from_seq_length, num_attention_heads * size_per_head]. If False, the\n      output will be of shape [batch_size, from_seq_length, num_attention_heads\n      * size_per_head].\n    batch_size: (Optional) int. If the input is 2D, this might be the batch size\n      of the 3D version of the `from_tensor` and `to_tensor`.\n    from_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `from_tensor`.\n    to_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `to_tensor`.\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length,\n      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is\n      true, this will be of shape [batch_size * from_seq_length,\n      num_attention_heads * size_per_head]).\n\n  Raises:\n    ValueError: Any of the arguments or tensor shapes are invalid.\n  """"""\n\n  def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width):\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n  to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])\n\n  if len(from_shape) != len(to_shape):\n    raise ValueError(\n        ""The rank of `from_tensor` must match the rank of `to_tensor`."")\n\n  if len(from_shape) == 3:\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n    to_seq_length = to_shape[1]\n  elif len(from_shape) == 2:\n    if (batch_size is None or from_seq_length is None or to_seq_length is None):\n      raise ValueError(\n          ""When passing in rank 2 tensors to attention_layer, the values ""\n          ""for `batch_size`, `from_seq_length`, and `to_seq_length` ""\n          ""must all be specified."")\n\n  # Scalar dimensions referenced here:\n  #   B = batch size (number of sequences)\n  #   F = `from_tensor` sequence length\n  #   T = `to_tensor` sequence length\n  #   N = `num_attention_heads`\n  #   H = `size_per_head`\n\n  from_tensor_2d = reshape_to_matrix(from_tensor)\n  to_tensor_2d = reshape_to_matrix(to_tensor)\n\n  # `query_layer` = [B*F, N*H]\n  query_layer = tf.layers.dense(\n      from_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=query_act,\n      name=""query"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  # `key_layer` = [B*T, N*H]\n  key_layer = tf.layers.dense(\n      to_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=key_act,\n      name=""key"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  # `value_layer` = [B*T, N*H]\n  value_layer = tf.layers.dense(\n      to_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=value_act,\n      name=""value"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  # `query_layer` = [B, N, F, H]\n  query_layer = transpose_for_scores(query_layer, batch_size,\n                                     num_attention_heads, from_seq_length,\n                                     size_per_head)\n\n  # `key_layer` = [B, N, T, H]\n  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,\n                                   to_seq_length, size_per_head)\n\n  # Take the dot product between ""query"" and ""key"" to get the raw\n  # attention scores.\n  # `attention_scores` = [B, N, F, T]\n  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n  attention_scores = tf.multiply(attention_scores,\n                                 1.0 / math.sqrt(float(size_per_head)))\n\n  if attention_mask is not None:\n    # `attention_mask` = [B, 1, F, T]\n    attention_mask = tf.expand_dims(attention_mask, axis=[1])\n\n    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n    # masked positions, this operation will create a tensor which is 0.0 for\n    # positions we want to attend and -10000.0 for masked positions.\n    adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n\n    # Since we are adding it to the raw scores before the softmax, this is\n    # effectively the same as removing these entirely.\n    attention_scores += adder\n\n  # Normalize the attention scores to probabilities.\n  # `attention_probs` = [B, N, F, T]\n  attention_probs = tf.nn.softmax(attention_scores)\n\n  # This is actually dropping out entire tokens to attend to, which might\n  # seem a bit unusual, but is taken from the original Transformer paper.\n  #attention_probs = dropout(attention_probs, attention_probs_dropout_prob)\n\n  # `value_layer` = [B, T, N, H]\n  value_layer = tf.reshape(\n      value_layer,\n      [batch_size, to_seq_length, num_attention_heads, size_per_head])\n\n  # `value_layer` = [B, N, T, H]\n  value_layer = tf.transpose(value_layer, [0, 2, 1, 3])\n\n  # `context_layer` = [B, N, F, H]\n  context_layer = tf.matmul(attention_probs, value_layer)\n\n  # `context_layer` = [B, F, N, H]\n  context_layer = tf.transpose(context_layer, [0, 2, 1, 3])\n\n  if do_return_2d_tensor:\n    # `context_layer` = [B*F, N*H]\n    context_layer = tf.reshape(\n        context_layer,\n        [batch_size * from_seq_length, num_attention_heads * size_per_head])\n  else:\n    # `context_layer` = [B, F, N*H]\n    context_layer = tf.reshape(\n        context_layer,\n        [batch_size, from_seq_length, num_attention_heads * size_per_head])\n\n  return context_layer\n\n\ndef transformer_model(input_tensor,\n                      attention_mask=None,\n                      hidden_size=768,\n                      num_hidden_layers=12,\n                      num_attention_heads=12,\n                      intermediate_size=3072,\n                      intermediate_act_fn=gelu,\n                      hidden_dropout_prob=0.1,\n                      attention_probs_dropout_prob=0.1,\n                      initializer_range=0.02,\n                      do_return_all_layers=False):\n  """"""Multi-headed, multi-layer Transformer from ""Attention is All You Need"".\n\n  This is almost an exact implementation of the original Transformer encoder.\n\n  See the original paper:\n  https://arxiv.org/abs/1706.03762\n\n  Also see:\n  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n      seq_length], with 1 for positions that can be attended to and 0 in\n      positions that should not be.\n    hidden_size: int. Hidden size of the Transformer.\n    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n    num_attention_heads: int. Number of attention heads in the Transformer.\n    intermediate_size: int. The size of the ""intermediate"" (a.k.a., feed\n      forward) layer.\n    intermediate_act_fn: function. The non-linear activation function to apply\n      to the output of the intermediate/feed-forward layer.\n    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n    attention_probs_dropout_prob: float. Dropout probability of the attention\n      probabilities.\n    initializer_range: float. Range of the initializer (stddev of truncated\n      normal).\n    do_return_all_layers: Whether to also return all layers or just the final\n      layer.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n    hidden layer of the Transformer.\n\n  Raises:\n    ValueError: A Tensor shape or parameter is invalid.\n  """"""\n  if hidden_size % num_attention_heads != 0:\n    raise ValueError(\n        ""The hidden size (%d) is not a multiple of the number of attention ""\n        ""heads (%d)"" % (hidden_size, num_attention_heads))\n\n  attention_head_size = int(hidden_size / num_attention_heads)\n  input_shape = get_shape_list(input_tensor, expected_rank=3)\n  batch_size = input_shape[0]\n  seq_length = input_shape[1]\n  input_width = input_shape[2]\n\n  # The Transformer performs sum residuals on all layers so the input needs\n  # to be the same as the hidden size.\n  if input_width != hidden_size:\n    raise ValueError(""The width of the input tensor (%d) != hidden size (%d)"" %\n                     (input_width, hidden_size))\n\n  # We keep the representation as a 2D tensor to avoid re-shaping it back and\n  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on\n  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to\n  # help the optimizer.\n  prev_output = reshape_to_matrix(input_tensor)\n\n  all_layer_outputs = []\n  for layer_idx in range(num_hidden_layers):\n    with tf.variable_scope(""layer_%d"" % layer_idx):\n      layer_input = prev_output\n\n      with tf.variable_scope(""attention""):\n        attention_heads = []\n        with tf.variable_scope(""self""):\n          attention_head = attention_layer(\n              from_tensor=layer_input,\n              to_tensor=layer_input,\n              attention_mask=attention_mask,\n              num_attention_heads=num_attention_heads,\n              size_per_head=attention_head_size,\n              attention_probs_dropout_prob=attention_probs_dropout_prob,\n              initializer_range=initializer_range,\n              do_return_2d_tensor=True,\n              batch_size=batch_size,\n              from_seq_length=seq_length,\n              to_seq_length=seq_length)\n          attention_heads.append(attention_head)\n\n        attention_output = None\n        if len(attention_heads) == 1:\n          attention_output = attention_heads[0]\n        else:\n          # In the case where we have other sequences, we just concatenate\n          # them to the self-attention head before the projection.\n          attention_output = tf.concat(attention_heads, axis=-1)\n\n        # Run a linear projection of `hidden_size` then add a residual\n        # with `layer_input`.\n        with tf.variable_scope(""output""):\n          attention_output = tf.layers.dense(\n              attention_output,\n              hidden_size,\n              kernel_initializer=create_initializer(initializer_range))\n          attention_output = dropout(attention_output, hidden_dropout_prob)\n          attention_output = layer_norm(attention_output + layer_input)\n\n      # The activation is only applied to the ""intermediate"" hidden layer.\n      with tf.variable_scope(""intermediate""):\n        intermediate_output = tf.layers.dense(\n            attention_output,\n            intermediate_size,\n            activation=intermediate_act_fn,\n            kernel_initializer=create_initializer(initializer_range))\n\n      # Down-project back to `hidden_size` then add the residual.\n      with tf.variable_scope(""output""):\n        layer_output = tf.layers.dense(\n            intermediate_output,\n            hidden_size,\n            kernel_initializer=create_initializer(initializer_range))\n        layer_output = dropout(layer_output, hidden_dropout_prob)\n        layer_output = layer_norm(layer_output + attention_output)\n        prev_output = layer_output\n        all_layer_outputs.append(layer_output)\n\n  if do_return_all_layers:\n    final_outputs = []\n    for layer_output in all_layer_outputs:\n      final_output = reshape_from_matrix(layer_output, input_shape)\n      final_outputs.append(final_output)\n    return final_outputs\n  else:\n    final_output = reshape_from_matrix(prev_output, input_shape)\n    return final_output\n\n\ndef get_shape_list(tensor, expected_rank=None, name=None):\n  """"""Returns a list of the shape of tensor, preferring static dimensions.\n\n  Args:\n    tensor: A tf.Tensor object to find the shape of.\n    expected_rank: (optional) int. The expected rank of `tensor`. If this is\n      specified and the `tensor` has a different rank, and exception will be\n      thrown.\n    name: Optional name of the tensor for the error message.\n\n  Returns:\n    A list of dimensions of the shape of tensor. All static dimensions will\n    be returned as python integers, and dynamic dimensions will be returned\n    as tf.Tensor scalars.\n  """"""\n  if name is None:\n    name = tensor.name\n\n  if expected_rank is not None:\n    assert_rank(tensor, expected_rank, name)\n\n  shape = tensor.shape.as_list()\n\n  non_static_indexes = []\n  for (index, dim) in enumerate(shape):\n    if dim is None:\n      non_static_indexes.append(index)\n\n  if not non_static_indexes:\n    return shape\n\n  dyn_shape = tf.shape(tensor)\n  for index in non_static_indexes:\n    shape[index] = dyn_shape[index]\n  return shape\n\n\ndef reshape_to_matrix(input_tensor):\n  """"""Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).""""""\n  ndims = input_tensor.shape.ndims\n  if ndims < 2:\n    raise ValueError(""Input tensor must have at least rank 2. Shape = %s"" %\n                     (input_tensor.shape))\n  if ndims == 2:\n    return input_tensor\n\n  width = input_tensor.shape[-1]\n  output_tensor = tf.reshape(input_tensor, [-1, width])\n  return output_tensor\n\n\ndef reshape_from_matrix(output_tensor, orig_shape_list):\n  """"""Reshapes a rank 2 tensor back to its original rank >= 2 tensor.""""""\n  if len(orig_shape_list) == 2:\n    return output_tensor\n\n  output_shape = get_shape_list(output_tensor)\n\n  orig_dims = orig_shape_list[0:-1]\n  width = output_shape[-1]\n\n  return tf.reshape(output_tensor, orig_dims + [width])\n\n\ndef assert_rank(tensor, expected_rank, name=None):\n  """"""Raises an exception if the tensor rank is not of the expected rank.\n\n  Args:\n    tensor: A tf.Tensor to check the rank of.\n    expected_rank: Python integer or list of integers, expected rank.\n    name: Optional name of the tensor for the error message.\n\n  Raises:\n    ValueError: If the expected shape doesn\'t match the actual shape.\n  """"""\n  if name is None:\n    name = tensor.name\n\n  expected_rank_dict = {}\n  if isinstance(expected_rank, six.integer_types):\n    expected_rank_dict[expected_rank] = True\n  else:\n    for x in expected_rank:\n      expected_rank_dict[x] = True\n\n  actual_rank = tensor.shape.ndims\n  if actual_rank not in expected_rank_dict:\n    scope_name = tf.get_variable_scope().name\n    raise ValueError(\n        ""For the tensor `%s` in scope `%s`, the actual rank ""\n        ""`%d` (shape = %s) is not equal to the expected rank `%s`"" %\n        (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))\n'"
deltann/infer/example/python/standard_transformer/train.py,5,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.framework import graph_util\nfrom model import *\nfrom summarize_graph import GraphSummary\n\ndef create_model():\n    batch_size = None\n    seq_length = None\n    hidden_size = 768\n    num_attention_heads =12\n    size_per_head = int(hidden_size / num_attention_heads)\n\n    layer_input = tf.placeholder(tf.float32, shape=(batch_size, seq_length, hidden_size))\n    # Tensor of shape [batch_size, from_seq_length, to_seq_length].\n    attention_mask = tf.placeholder(tf.float32, shape=(batch_size, seq_length, seq_length))\n\n    #output_rnn = transformer_model(input_tensor=layer_input,\n    #                               attention_mask=attention_mask,\n    #                               hidden_size=hidden_size,\n    #                               num_attention_heads=num_attention_heads,\n    #                               intermediate_size=1280,\n    #                               do_return_all_layers=False)\n\n    out = transformer_model(input_tensor=layer_input,\n                        attention_mask=attention_mask,\n                        hidden_size=hidden_size,\n                        num_hidden_layers=3,\n                        num_attention_heads=num_attention_heads,\n                        intermediate_size=12802,\n                        intermediate_act_fn=gelu,\n                        hidden_dropout_prob=0.1,\n                        attention_probs_dropout_prob=0.1,\n                        initializer_range=0.02,\n                        do_return_all_layers=False)\n    return out\n\ndef to_graph_def(graph_path):\n    with tf.Session() as sess:\n        ret = create_model()\n        #sess.run(tf.compat.v1.global_variables_initializer())\n        tf.global_variables_initializer().run()\n        graph_summary = GraphSummary(graph_def=sess.graph_def)\n        graph_summary.Summary()\n        graph_def = graph_util.convert_variables_to_constants(sess, sess.graph_def, graph_summary[""outputs""])\n        with open(graph_path, ""wb"") as f:\n            f.write(graph_def.SerializeToString())\n\nto_graph_def(""./transformer_pattern.pb"")\n'"
deltann/infer/example/python/tts_transformer/__init__.py,0,b''
deltann/infer/example/python/tts_transformer/model.py,26,"b'import numpy as np\nimport tensorflow as tf\nimport delta_infer as dti\nfrom tts_transformer.model import *\n\ndef gelu(x):\n    """"""Gaussian Error Linear Unit.\n    This is a smoother version of the RELU.\n    Original paper: https://arxiv.org/abs/1606.08415\n    Args:\n        x: float Tensor to perform activation.\n    Returns:\n        `x` with the GELU activation applied.\n    """"""\n    cdf = 0.5 * (1.0 + tf.tanh((np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n    return x * cdf\n\nclass ScaledDotProductAttention(tf.keras.layers.Layer):\n    """"""Calculate the attention weights.\n    q, k, v must have matching leading dimensions.\n    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n    The mask has different shapes depending on its type(padding or look ahead)\n    but it must be broadcastable for addition.\n\n    Args:\n        q: query shape == (..., seq_len_q, depth)\n        k: key shape == (..., seq_len_k, depth)\n        v: value shape == (..., seq_len_v, depth_v)\n        mask: Float tensor with shape broadcastable\n          to (..., seq_len_q, seq_len_k). Defaults to None.\n\n    Returns:\n        output, attention_weights\n    """"""\n\n    def call(self, q, k, v, mask):\n        """"""This is where the layer\'s logic lives.""""""\n        matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n\n        # scale matmul_qk\n        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n        # add the mask to the scaled tensor.\n        if mask is not None:\n            scaled_attention_logits += mask * -1e9\n\n        # softmax is normalized on the last axis (seq_len_k) so that the scores\n        # add up to 1.\n        # (..., seq_len_q, seq_len_k)\n        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n\n        output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n        return output, attention_weights\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    """""" Multi-head attention\n\n    Multi-head attention consists of four parts: * Linear layers and split into\n    heads. * Scaled dot-product attention. * Concatenation of heads. * Final linear layer.\n    Each multi-head attention block gets three inputs; Q (query), K (key), V (value).\n    These are put through linear (Dense) layers and split up into multiple heads.\n    The scaled_dot_product_attention defined above is applied to each head (broadcasted for\n    efficiency). An appropriate mask must be used in the attention step. The attention\n    output for each head is then concatenated (using tf.transpose, and tf.reshape) and\n    put through a final Dense layer.\n    Instead of one single attention head, Q, K, and V are split into multiple heads because\n    it allows the model to jointly attend to information at different positions from\n    different representational spaces. After the split each head has a reduced dimensionality,\n    so the total computation cost is the same as a single head attention with full\n    dimensionality.\n    """"""\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        assert d_model % self.num_heads == 0\n\n        self.depth = d_model // self.num_heads\n\n        layers = tf.compat.v1.keras.layers\n\n        self.wq = layers.Dense(\n            d_model,\n            kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02),\n            input_shape=(d_model,),\n        )\n        self.wk = layers.Dense(\n            d_model,\n            kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02),\n            input_shape=(d_model,),\n        )\n        self.wv = layers.Dense(\n            d_model,\n            kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02),\n            input_shape=(d_model,),\n        )\n\n        self.attention = ScaledDotProductAttention()\n\n        self.dense = layers.Dense(\n            d_model,\n            kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02),\n            input_shape=(d_model,),\n        )\n\n    def split_heads(self, x, batch_size):\n        """"""Split the last dimension into (num_heads, depth).\n\n        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n        """"""\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, v, k, q, mask):\n        """""" call function """"""\n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)  # (batch_size, seq_len, hiddn_dim)\n        k = self.wk(k)  # (batch_size, seq_len, hiddn_dim)\n        v = self.wv(v)  # (batch_size, seq_len, hiddn_dim)\n\n        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n        scaled_attention, attention_weights = self.attention(q, k, v, mask)\n\n        # (batch_size, seq_len_q, num_heads, depth)\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n\n        # (batch_size, seq_len_q, d_model)\n        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n\n        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n        return output, attention_weights\n\nclass TransformerEncoderLayer(tf.keras.layers.Layer):\n    """"""TransformerEncoderLayer is made up of self-attn and feedforward network.\n    This standard encoder layer is based on the paper ""Attention Is All You Need"".\n    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n    in a different way during application.\n\n    Args:\n        d_model: the number of expected features in the input (required).\n        nhead: the number of heads in the multiheadattention models (required).\n        dim_feedforward: the dimension of the feedforward network model (default=2048).\n        dropout: the dropout value (default=0.1).\n        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n\n    Examples::\n        >>> encoder_layer = TransformerEncoderLayer(d_model=512, nhead=8)\n        >>> src = tf.random(10, 32, 512)\n        >>> out = encoder_layer(src)\n    """"""\n\n    def __init__(\n        self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=""gelu""\n    ):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, nhead)\n        # Implementation of Feedforward model\n        #layers = tf.keras.layers\n        layers = tf.compat.v1.keras.layers\n        self.ffn = tf.keras.Sequential(\n            [\n                layers.Dense(\n                    dim_feedforward,\n                    activation=gelu,\n                    kernel_initializer=tf.compat.v1.truncated_normal_initializer(\n                        stddev=0.02\n                    ),\n                    input_shape=(d_model,),\n                ),\n                layers.Dropout(dropout, input_shape=(dim_feedforward,)),\n                layers.Dense(\n                    d_model,\n                    kernel_initializer=tf.compat.v1.truncated_normal_initializer(\n                        stddev=0.02\n                    ),\n                    input_shape=(dim_feedforward,),\n                ),\n                layers.Dropout(dropout, input_shape=(d_model,)),\n            ]\n        )\n\n        self.norm1 = layers.LayerNormalization(epsilon=1e-8, input_shape=(d_model,))\n        self.norm2 = layers.LayerNormalization(epsilon=1e-8, input_shape=(d_model,))\n        self.dropout = layers.Dropout(dropout, input_shape=(d_model,))\n\n    def call(self, src, src_mask=None, training=None):\n        """"""Pass the input through the endocder layer.\n\n        Args:\n            src: the sequnce to the encoder layer (required).\n            mask: the mask for the src sequence (optional).\n\n        Shape:\n            see the docs in Transformer class.\n        """"""\n        out = self.self_attn(src, src, src, mask=src_mask)[0]\n        out = self.norm1(src + self.dropout(out, training=training))\n        out = self.norm2(out + self.ffn(out, training=training))\n\n        return out\n'"
deltann/infer/python/delta_infer/cpp/__init__.py,1,"b'from __future__ import absolute_import, division, print_function\nimport export_py as ep\nimport tensorflow as tf\n\n__all__ = [""DeltaGraph"", ""AutoOptimizer"", ""RegisterPattern""]\n\nclass LocalOptimizerMgr(object):\n    """""" Note:\n        not work on global static initialization used in c++ lib.\n    """"""\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def names():\n        mgr = ep.optimizer.OptimizerManager.Instance()\n        return mgr.names()\n\n    @staticmethod\n    def method(name):\n        mgr = ep.optimizer.OptimizerManager.Instance()\n        if mgr.contain(name):\n            return mgr.Get(name)\n        else:\n            return None\n\ndef RegisterPattern(pattern_name, pattern, hint_op_type):\n    """""" """"""\n    assert isinstance(pattern, DeltaGraph), \\\n            ""type of pattern must be instance of DeltaGraph. but got {}"".format(type(pattern))\n    ep.optimizer.RegisterFusionPattern(pattern_name, pattern.pattern(), hint_op_type)\n\nclass DeltaGraph(object):\n    """""" """"""\n    def __init__(self, pb_file=None, pb_model=None):\n        """""" """"""\n        self.__pattern = ep.core.Pattern()\n        if pb_file is not None:\n            self.__pattern.LoadModel(pb_file)\n        elif pb_model is not None:\n            print(type(pb_model))\n            assert isinstance(pb_model, tf.compat.v1.GraphDef), \\\n                    ""type of pb_model must be instance of GraphDef, but got {}"".format(type(pb_model))\n            self.__pattern.LoadModelCT(pb_model.SerializeToString())\n        else:\n            raise ValueError(""Err: pb_file and pb_model can\'t be both None! "")\n\n    def pattern(self):\n        return self.__pattern\n\n    @property\n    def graph(self):\n        return self.__pattern.graph()\n\nclass AutoOptimizer(object):\n    """""" """"""\n    def __init__(self, graph):\n        """""" """"""\n        assert isinstance(graph, DeltaGraph), \\\n                "" type of Parameter graph must be instance of DeltaGraph. but got {}"".format(type(graph))\n        self.__optimizer = ep.optimizer.LocalAutoOptimizer(graph.pattern())\n\n    def run(self):\n        """""" run opitimization automatically """"""\n        self.__optimizer.run()\n\n    def serialization(self, path):\n        """""" serialization to path """"""\n        self.__optimizer.serialization(path)\n'"
deltann/infer/python/delta_infer/subgraphs/__init__.py,4,"b'from .transformer import *\nfrom .common import *\n\n#tf.compat.v1.disable_eager_execution()\n#\n#batch_size = 40\n#seq_length = 200\n#hidden_size = 768\n#num_attention_heads =12\n#size_per_head = int(hidden_size / num_attention_heads)\n#\n#layer_input = tf.compat.v1.placeholder(tf.float32, shape=(batch_size*seq_length, hidden_size))\n## Tensor of shape [batch_size, from_seq_length, to_seq_length].\n#attention_mask = tf.compat.v1.placeholder(tf.float32, shape=(batch_size, seq_length, seq_length))\n#\n#output_rnn = transformer_cell(input_tensor=layer_input,#tf.reshape(layer_input, [-1, hidden_size]),\n#                              attention_mask=attention_mask,\n#                              hidden_size=hidden_size,\n#                              num_attention_heads=num_attention_heads,\n#                              attention_head_size=size_per_head,\n#                              batch_size = batch_size,\n#                              seq_length = seq_length,\n#                              intermediate_size=1280)\n'"
egs/atis/nlu-joint/v1/local/summary_data.py,0,"b'import os\nimport sys\nimport pickle\nfrom absl import logging\n\nDATA_DIR = ""data/""\nlogging.info(os.listdir(DATA_DIR))\n\n\ndef load_ds(fname, output_file_path):\n    with open(fname, \'rb\') as stream:\n        ds, dicts = pickle.load(stream)\n    logging.info(\'      samples: {:4d}\'.format(len(ds[\'query\'])))\n    logging.info(\'   vocab_size: {:4d}\'.format(len(dicts[\'token_ids\'])))\n    logging.info(\'   slot count: {:4d}\'.format(len(dicts[\'slot_ids\'])))\n    logging.info(\' intent count: {:4d}\'.format(len(dicts[\'intent_ids\'])))\n\n    t2i, s2i, in2i = map(dicts.get, [\'token_ids\', \'slot_ids\', \'intent_ids\'])\n    i2t, i2s, i2in = map(lambda d: {d[k]: k for k in d.keys()}, [t2i, s2i, in2i])\n    query, slots, intent = map(ds.get,\n                               [\'query\', \'slot_labels\', \'intent_labels\'])\n\n    with open(output_file_path, ""w"", encoding=""utf-8"") as out_file:\n      for i in range(len(query)):\n        out_file.write(i2in[intent[i][0]]+""\\t"")\n        out_file.write(\' \'.join(map(i2s.get, slots[i])) + ""\\t"")\n        out_file.write(\' \'.join(map(i2t.get, query[i])) + ""\\n"")\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  if len(sys.argv) != 3:\n    logging.error(""Usage {} input_file output_file"".format(sys.argv[0]))\n    sys.exit(-1)\n\n  output_train_file = sys.argv[1]\n  output_test_file = sys.argv[2]\n\n  load_ds(os.path.join(DATA_DIR, \'atis.train.pkl\'), output_train_file)\n  load_ds(os.path.join(DATA_DIR, \'atis.test.pkl\'), output_test_file)\n'"
egs/atis2/nlu_joint/v1/local/generate_standard_format.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport sys\nfrom absl import logging\n\n\ndef to_standard_format(input_file, output_file):\n  logging.info(""Save file to {}"".format(output_file))\n\n  with open(input_file, encoding=""utf-8"") as in_file, \\\n    open(output_file, ""w"", encoding=""utf-8"") as out_file:\n    for row in in_file:\n      parts = row.strip().split(""\\t"")\n      if len(parts) < 2:\n        continue\n      text = parts[0]\n      sub_parts = parts[1].split("" "")\n      intent_label = sub_parts[-1]\n      slots_label = "" "".join(sub_parts[:-1])\n\n      text = text.rstrip(""EOS"")\n      text = text.strip()\n\n      out_file.write(intent_label + ""\\t""\n                     + slots_label + ""\\t""\n                     + text + ""\\n"")\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  if len(sys.argv) != 3:\n    logging.error(""Usage {} input_file output_file"".format(sys.argv[0]))\n    sys.exit(-1)\n\n  input_file = sys.argv[1]\n  output_file = sys.argv[2]\n  to_standard_format(input_file, output_file)\n'"
egs/cnn_dailymail/seq2seq/v1/local/make_datafiles.py,0,"b'import collections\nimport hashlib\nimport os\nimport subprocess\nimport sys\nfrom absl import logging\n\ndm_single_close_quote = u\'\\u2019\' \ndm_double_close_quote = u\'\\u201d\'\nEND_TOKENS = [\'.\', \'!\', \'?\', \'...\', ""\'"", ""`"", \'""\',\n              dm_single_close_quote, dm_double_close_quote,\n              "")""]\n\nSENTENCE_START = \'<s>\'\nSENTENCE_END = \'</s>\'\n\nall_train_urls = ""cnn-dailymail/url_lists/all_train.txt""\nall_val_urls = ""cnn-dailymail/url_lists/all_val.txt""\nall_test_urls = ""cnn-dailymail/url_lists/all_test.txt""\n\ncnn_tokenized_stories_dir = ""cnn_stories_tokenized""\n\nnum_expected_cnn_stories = 92578\n\nVOCAB_SIZE = 200000\n\n\ndef tokenize_stories(stories_dir, tokenized_stories_dir):\n  """"""Maps a whole directory of .story files\n   to a tokenized version using Stanford CoreNLP Tokenizer""""""\n  logging.info(""Preparing to tokenize {} to {}..."".format(stories_dir, tokenized_stories_dir))\n  stories = os.listdir(stories_dir)\n  # make IO list file\n  logging.info(""Making list of files to tokenize..."")\n  with open(""mapping.txt"", ""w"") as f:\n    for s in stories:\n      f.write(""%s \\t %s\\n"" % (os.path.join(stories_dir, s),\n                              os.path.join(tokenized_stories_dir, s)))\n  command = [\'java\', \'edu.stanford.nlp.process.PTBTokenizer\',\n             \'-ioFileList\', \'-preserveLines\', \'mapping.txt\']\n  logging.info(""Tokenizing {} files in {} and saving in {}...""\n        .format(len(stories), stories_dir, tokenized_stories_dir))\n  subprocess.call(command)\n  logging.info(""Stanford CoreNLP Tokenizer has finished."")\n  os.remove(""mapping.txt"")\n\n  num_orig = len(os.listdir(stories_dir))\n  num_tokenized = len(os.listdir(tokenized_stories_dir))\n  if num_orig != num_tokenized:\n    raise Exception(\n      ""The tokenized stories directory {} contains {} files, but it ""\n      ""should contain the same number as {} (which has {} files).""\n      "" Was there an error during tokenization?"".format(\n      tokenized_stories_dir, num_tokenized, stories_dir, num_orig))\n  logging.info(""Successfully finished tokenizing {} to {}.\\n"".format(stories_dir, tokenized_stories_dir))\n\n\ndef read_text_file(text_file):\n  lines = []\n  with open(text_file, ""r"", encoding=\'utf8\') as f:\n    for line in f:\n      lines.append(line.strip())\n  return lines\n\n\ndef hashhex(s):\n  """"""Returns a heximal formated SHA1 hash of the input string.""""""\n  h = hashlib.sha1()\n  h.update(s.encode())\n  return h.hexdigest()\n\n\ndef get_url_hashes(url_list):\n  return [hashhex(url) for url in url_list]\n\n\ndef fix_missing_period(line):\n  """"""Adds a period to a line that is missing a period""""""\n  if ""@highlight"" in line: return line\n  if line == """": return line\n  if line[-1] in END_TOKENS: return line\n  return line + "" .""\n\n\ndef get_art_abs(story_file):\n  lines = read_text_file(story_file)\n\n  lines = [line.lower() for line in lines]\n\n  lines = [fix_missing_period(line) for line in lines]\n\n  article_lines = []\n  highlights = []\n  next_is_highlight = False\n  for idx, line in enumerate(lines):\n    if line == """":\n      continue  # empty line\n    elif line.startswith(""@highlight""):\n      next_is_highlight = True\n    elif next_is_highlight:\n      highlights.append(line)\n    else:\n      article_lines.append(line)\n\n  article = \' \'.join(article_lines)\n\n  abstract = \' \'.join(highlights)\n\n  return article, abstract\n\n\ndef write_to_file(url_file, art_out_file,\n                  abs_out_file,\n                  makevocab=False,\n                  vocab_dir=None):\n  """"""Reads the tokenized .story files corresponding\n  to the urls listed in the url_file and writes them to a out_file.""""""\n  logging.info(""Making bin file for URLs listed in {}..."".format(url_file))\n  url_list = read_text_file(url_file)\n  url_hashes = get_url_hashes(url_list)\n  story_fnames = [s + "".story"" for s in url_hashes]\n  num_stories = len(story_fnames)\n\n  if makevocab:\n    vocab_counter = collections.Counter()\n\n  abs_writer = open(abs_out_file, \'w\', encoding=\'utf8\')\n  art_writer = open(art_out_file, \'w\', encoding=\'utf8\')\n\n  for idx, s in enumerate(story_fnames):\n    if idx % 1000 == 0:\n      logging.info(""Writing story {} of {}; {:.2f} percent done""\n            .format(idx, num_stories, float(idx) * 100.0 / float(num_stories)))\n\n    # Look in the tokenized story dirs\n    # to find the .story file corresponding to this url\n    if os.path.isfile(os.path.join(cnn_tokenized_stories_dir, s)):\n      story_file = os.path.join(cnn_tokenized_stories_dir, s)\n    else:\n      continue\n\n    article, abstract = get_art_abs(story_file)\n    if len(article) == 0:\n      continue\n\n    art_writer.write(article + \'\\n\')\n    abs_writer.write(abstract + \'\\n\')\n\n    # Write the vocab to file, if applicable\n    if makevocab:\n      art_tokens = article.split(\' \')\n      abs_tokens = abstract.split(\' \')\n      abs_tokens = [t for t in abs_tokens if t not in [SENTENCE_START, SENTENCE_END]]\n      tokens = art_tokens + abs_tokens\n      tokens = [t.strip() for t in tokens]\n      tokens = [t for t in tokens if t != """"]\n      vocab_counter.update(tokens)\n\n  logging.info(""Finished writing file\\n"")\n\n  # write vocab to file\n  if makevocab:\n    logging.info(""Writing vocab file..."")\n    with open(os.path.join(vocab_dir, ""vocab""), \'w\') as writer:\n      for word, count in vocab_counter.most_common(VOCAB_SIZE):\n        writer.write(word + \' \' + str(count) + \'\\n\')\n    logging.info(""Finished writing vocab file"")\n\n\ndef check_num_stories(stories_dir, num_expected):\n  num_stories = len(os.listdir(stories_dir))\n  if num_stories != num_expected:\n    raise Exception(\n      ""stories directory {} contains {} files but should contain {}"".format(stories_dir, num_stories, num_expected))\n\n\nif __name__ == \'__main__\':\n  if len(sys.argv) != 3:\n    logging.info(""USAGE: python make_datafiles.py <stories_dir> <output_dir>"")\n    sys.exit()\n  stories_dir = sys.argv[1]\n  output_dir = sys.argv[2]\n  stories_dir = os.path.join(stories_dir, \'cnn/stories\')\n  # Check the stories directories contain the correct number of .story files\n  check_num_stories(stories_dir, num_expected_cnn_stories)\n\n  # Create some new directories\n  if not os.path.exists(cnn_tokenized_stories_dir):\n    os.makedirs(cnn_tokenized_stories_dir)\n  if not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n  # tokenize_stories(stories_dir, cnn_tokenized_stories_dir)\n  cnn_tokenized_stories_dir = stories_dir\n\n  set_list = [\'test\', \'val\', \'train\']\n  for set_name in set_list:\n    urls_set = eval(\'all_{}_urls\'.format(set_name))\n    art_path = os.path.join(output_dir, ""{}.cnndm.src"".format(set_name))\n    abs_path = os.path.join(output_dir, ""{}.cnndm.tgt"".format(set_name))\n    write_to_file(urls_set, art_path, abs_path)\n\n'"
egs/conll2003/pretrain/v1/local/generate_bert_vocab.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport sys\nfrom absl import logging\n\ndef generate_vocab(train_path, raw_vocab_path, new_vocab_path, label_vocab_path):\n  f1 = open(new_vocab_path, \'w\')\n  with open(raw_vocab_path, \'r\') as f:\n    for idx, line in enumerate(f.readlines()):\n      word = line.strip()\n      if word == \'[UNK]\':\n        word = \'<unk>\'\n      f1.write(word + \'\\t\' + str(idx) + \'\\n\')\n  label_vocab_file = open(label_vocab_path, \'w\')\n  label_vocab = {}\n  with open(train_path, \'r\') as f:\n    for line in f.readlines():\n      labels = line.strip().split(\'\\t\')[0]\n      for t in labels.split(\' \'):\n        if t in label_vocab:\n          label_vocab[t] += 1\n        else:\n          label_vocab[t] = 1\n  label_vocab = sorted(label_vocab.items(), key=lambda x: x[1], reverse=True)\n  idx = 0\n  for label, count in label_vocab:\n    label_vocab_file.write(label + \'\\t\' + str(idx) + \'\\n\')\n    idx += 1\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n\n  if len(sys.argv) != 5:\n    logging.error(""Usage python {} train_path raw_vocab_path, new_vocab_path, ""\n                  ""label_vocab_path"".format(sys.argv[0]))\n    sys.exit(-1)\n\n  train_path = sys.argv[1]\n  raw_vocab_path = sys.argv[2]\n  new_vocab_path = sys.argv[3]\n  label_vocab_path = sys.argv[4]\n  generate_vocab(train_path, raw_vocab_path, new_vocab_path, label_vocab_path)\n'"
egs/conll2003/pretrain/v1/local/generate_elmo_vocab.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport sys\nfrom absl import logging\n\ndef generate_vocab(data_path, text_vocab_path,\n                   label_vocab_path, min_freq):\n  text_vocab = {}\n  label_vocab = {}\n  file_list = [""train.txt"", ""dev.txt"", ""test.txt""]\n  for f in file_list:\n    file_name = data_path + f\n    with open(file_name, \'r\') as f:\n      for line in f.readlines():\n        text = line.strip().split(\'\\t\')[1]\n        labels = line.strip().split(\'\\t\')[0]\n        for w in text.split(\' \'):\n          if w in text_vocab:\n            text_vocab[w] += 1\n          else:\n            text_vocab[w] = 1\n        for t in labels.split(\' \'):\n          if t in label_vocab:\n            label_vocab[t] += 1\n          else:\n            label_vocab[t] = 1\n  text_vocab_file = open(text_vocab_path, \'w\')\n  label_vocab_file = open(label_vocab_path, \'w\')\n  text_vocab_file.write(\'<pad>\' + \'\\t\' + \'0\' + \'\\n\')\n  text_vocab_file.write(\'<s>\' + \'\\t\' + \'1\' + \'\\n\')\n  text_vocab_file.write(\'</s>\' + \'\\t\' + \'2\' + \'\\n\')\n  text_vocab_file.write(\'<unk>\' + \'\\t\' + \'3\' + \'\\n\')\n  text_vocab_file.write(\'<sos>\' + \'\\t\' + \'4\' + \'\\n\')\n  text_vocab_file.write(\'<eos>\' + \'\\t\' + \'5\' + \'\\n\')\n  text_vocab = sorted(text_vocab.items(), key=lambda x: x[1], reverse=True)\n  label_vocab = sorted(label_vocab.items(), key=lambda x: x[1], reverse=True)\n  idx = 6\n  for word, count in text_vocab:\n    if count >= min_freq:\n      text_vocab_file.write(word + \'\\t\' + str(idx) + \'\\n\')\n      idx += 1\n  idx = 0\n  for label, count in label_vocab:\n    label_vocab_file.write(label + \'\\t\' + str(idx) + \'\\n\')\n    idx += 1\n  logging.info(""finish generate vocab!"")\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n\n  if len(sys.argv) != 5:\n    logging.error(""Usage python {} data_path, text_vocab_path, label_vocab_path, ""\n                  ""min_freq"".format(sys.argv[0]))\n    sys.exit(-1)\n\n  data_path = sys.argv[1]\n  text_vocab_path = sys.argv[2]\n  label_vocab_path = sys.argv[3]\n  min_freq = int(sys.argv[4])\n  generate_vocab(data_path, text_vocab_path,\n                 label_vocab_path, min_freq)\n\n\n\n'"
egs/conll2003/pretrain/v1/local/modeling.py,82,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""The main BERT model and related functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport copy\nimport json\nimport math\nimport re\nimport numpy as np\nimport six\nimport tensorflow as tf\n\n\nclass BertConfig(object):\n  """"""Configuration for `BertModel`.""""""\n\n  def __init__(self,\n               vocab_size,\n               hidden_size=768,\n               num_hidden_layers=12,\n               num_attention_heads=12,\n               intermediate_size=3072,\n               hidden_act=""gelu"",\n               hidden_dropout_prob=0.1,\n               attention_probs_dropout_prob=0.1,\n               max_position_embeddings=512,\n               type_vocab_size=16,\n               initializer_range=0.02):\n    """"""Constructs BertConfig.\n\n    Args:\n      vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n      hidden_size: Size of the encoder layers and the pooler layer.\n      num_hidden_layers: Number of hidden layers in the Transformer encoder.\n      num_attention_heads: Number of attention heads for each attention layer in\n        the Transformer encoder.\n      intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)\n        layer in the Transformer encoder.\n      hidden_act: The non-linear activation function (function or string) in the\n        encoder and pooler.\n      hidden_dropout_prob: The dropout probability for all fully connected\n        layers in the embeddings, encoder, and pooler.\n      attention_probs_dropout_prob: The dropout ratio for the attention\n        probabilities.\n      max_position_embeddings: The maximum sequence length that this model might\n        ever be used with. Typically set this to something large just in case\n        (e.g., 512 or 1024 or 2048).\n      type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n        `BertModel`.\n      initializer_range: The stdev of the truncated_normal_initializer for\n        initializing all weight matrices.\n    """"""\n    self.vocab_size = vocab_size\n    self.hidden_size = hidden_size\n    self.num_hidden_layers = num_hidden_layers\n    self.num_attention_heads = num_attention_heads\n    self.hidden_act = hidden_act\n    self.intermediate_size = intermediate_size\n    self.hidden_dropout_prob = hidden_dropout_prob\n    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n    self.max_position_embeddings = max_position_embeddings\n    self.type_vocab_size = type_vocab_size\n    self.initializer_range = initializer_range\n\n  @classmethod\n  def from_dict(cls, json_object):\n    """"""Constructs a `BertConfig` from a Python dictionary of parameters.""""""\n    config = BertConfig(vocab_size=None)\n    for (key, value) in six.iteritems(json_object):\n      config.__dict__[key] = value\n    return config\n\n  @classmethod\n  def from_json_file(cls, json_file):\n    """"""Constructs a `BertConfig` from a json file of parameters.""""""\n    with tf.io.gfile.GFile(json_file, ""r"") as reader:\n      text = reader.read()\n    return cls.from_dict(json.loads(text))\n\n  def to_dict(self):\n    """"""Serializes this instance to a Python dictionary.""""""\n    output = copy.deepcopy(self.__dict__)\n    return output\n\n  def to_json_string(self):\n    """"""Serializes this instance to a JSON string.""""""\n    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n\nclass BertModel(object):\n  """"""BERT model (""Bidirectional Encoder Representations from Transformers"").\n\n  Example usage:\n\n  ```python\n  # Already been converted into WordPiece token ids\n  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n\n  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n\n  model = modeling.BertModel(config=config, is_training=True,\n    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n\n  label_embeddings = tf.get_variable(...)\n  pooled_output = model.get_pooled_output()\n  logits = tf.matmul(pooled_output, label_embeddings)\n  ...\n  ```\n  """"""\n\n  def __init__(self,\n               config,\n               is_training,\n               input_ids,\n               input_mask=None,\n               token_type_ids=None,\n               use_one_hot_embeddings=False,\n               scope=None):\n    """"""Constructor for BertModel.\n\n    Args:\n      config: `BertConfig` instance.\n      is_training: bool. true for training model, false for eval model. Controls\n        whether dropout will be applied.\n      input_ids: int32 Tensor of shape [batch_size, seq_length].\n      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n        embeddings or tf.embedding_lookup() for the word embeddings.\n      scope: (optional) variable scope. Defaults to ""bert"".\n\n    Raises:\n      ValueError: The config is invalid or one of the input tensor shapes\n        is invalid.\n    """"""\n    config = copy.deepcopy(config)\n    if not is_training:\n      config.hidden_dropout_prob = 0.0\n      config.attention_probs_dropout_prob = 0.0\n\n    input_shape = get_shape_list(input_ids, expected_rank=2)\n    batch_size = input_shape[0]\n    seq_length = input_shape[1]\n\n    if input_mask is None:\n      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    if token_type_ids is None:\n      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    with tf.compat.v1.variable_scope(scope, default_name=""bert""):\n      with tf.compat.v1.variable_scope(""embeddings""):\n        # Perform embedding lookup on the word ids.\n        (self.embedding_output, self.embedding_table) = embedding_lookup(\n            input_ids=input_ids,\n            vocab_size=config.vocab_size,\n            embedding_size=config.hidden_size,\n            initializer_range=config.initializer_range,\n            word_embedding_name=""word_embeddings"",\n            use_one_hot_embeddings=use_one_hot_embeddings)\n\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.embedding_output,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=config.type_vocab_size,\n            token_type_embedding_name=""token_type_embeddings"",\n            use_position_embeddings=True,\n            position_embedding_name=""position_embeddings"",\n            initializer_range=config.initializer_range,\n            max_position_embeddings=config.max_position_embeddings,\n            dropout_prob=config.hidden_dropout_prob)\n\n      with tf.compat.v1.variable_scope(""encoder""):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask = create_attention_mask_from_input_mask(\n            input_ids, input_mask)\n\n        # Run the stacked transformer.\n        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n        self.all_encoder_layers = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=config.hidden_size,\n            num_hidden_layers=config.num_hidden_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            intermediate_act_fn=get_activation(config.hidden_act),\n            hidden_dropout_prob=config.hidden_dropout_prob,\n            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n            initializer_range=config.initializer_range,\n            do_return_all_layers=True)\n\n      self.sequence_output = self.all_encoder_layers[-1]\n      # The ""pooler"" converts the encoded sequence tensor of shape\n      # [batch_size, seq_length, hidden_size] to a tensor of shape\n      # [batch_size, hidden_size]. This is necessary for segment-level\n      # (or segment-pair-level) classification tasks where we need a fixed\n      # dimensional representation of the segment.\n      with tf.compat.v1.variable_scope(""pooler""):\n        # We ""pool"" the model by simply taking the hidden state corresponding\n        # to the first token. We assume that this has been pre-trained\n        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n        self.pooled_output = tf.compat.v1.layers.dense(\n            first_token_tensor,\n            config.hidden_size,\n            activation=tf.tanh,\n            kernel_initializer=create_initializer(config.initializer_range))\n\n  def get_pooled_output(self):\n    return self.pooled_output\n\n  def get_sequence_output(self):\n    """"""Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    """"""\n    return self.sequence_output\n\n  def get_all_encoder_layers(self):\n    return self.all_encoder_layers\n\n  def get_embedding_output(self):\n    """"""Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    """"""\n    return self.embedding_output\n\n  def get_embedding_table(self):\n    return self.embedding_table\n\n\ndef gelu(x):\n  """"""Gaussian Error Linear Unit.\n\n  This is a smoother version of the RELU.\n  Original paper: https://arxiv.org/abs/1606.08415\n  Args:\n    x: float Tensor to perform activation.\n\n  Returns:\n    `x` with the GELU activation applied.\n  """"""\n  cdf = 0.5 * (1.0 + tf.tanh(\n      (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n  return x * cdf\n\n\ndef get_activation(activation_string):\n  """"""Maps a string to a Python function, e.g., ""relu"" => `tf.nn.relu`.\n\n  Args:\n    activation_string: String name of the activation function.\n\n  Returns:\n    A Python function corresponding to the activation function. If\n    `activation_string` is None, empty, or ""linear"", this will return None.\n    If `activation_string` is not a string, it will return `activation_string`.\n\n  Raises:\n    ValueError: The `activation_string` does not correspond to a known\n      activation.\n  """"""\n\n  # We assume that anything that""s not a string is already an activation\n  # function, so we just return it.\n  if not isinstance(activation_string, six.string_types):\n    return activation_string\n\n  if not activation_string:\n    return None\n\n  act = activation_string.lower()\n  if act == ""linear"":\n    return None\n  elif act == ""relu"":\n    return tf.nn.relu\n  elif act == ""gelu"":\n    return gelu\n  elif act == ""tanh"":\n    return tf.tanh\n  else:\n    raise ValueError(""Unsupported activation: %s"" % act)\n\n\ndef get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n  """"""Compute the union of the current variables and checkpoint variables.""""""\n  assignment_map = {}\n  initialized_variable_names = {}\n\n  name_to_variable = collections.OrderedDict()\n  for var in tvars:\n    name = var.name\n    m = re.match(""^(.*):\\\\d+$"", name)\n    if m is not None:\n      name = m.group(1)\n    name_to_variable[name] = var\n\n  init_vars = tf.train.list_variables(init_checkpoint)\n\n  assignment_map = collections.OrderedDict()\n  for x in init_vars:\n    (name, var) = (x[0], x[1])\n    if name not in name_to_variable:\n      continue\n    assignment_map[name] = name\n    initialized_variable_names[name] = 1\n    initialized_variable_names[name + "":0""] = 1\n\n  return (assignment_map, initialized_variable_names)\n\n\ndef dropout(input_tensor, dropout_prob):\n  """"""Perform dropout.\n\n  Args:\n    input_tensor: float Tensor.\n    dropout_prob: Python float. The probability of dropping out a value (NOT of\n      *keeping* a dimension as in `tf.nn.dropout`).\n\n  Returns:\n    A version of `input_tensor` with dropout applied.\n  """"""\n  if dropout_prob is None or dropout_prob == 0.0:\n    return input_tensor\n\n  output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n  return output\n\n\ndef layer_norm(input_tensor, name=None):\n  """"""Run layer normalization on the last dimension of the tensor.""""""\n  # return tf.layers.layer_norm(\n  #     inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)\n  return tf.keras.layers.LayerNormalization(name=""LayerNorm"")(input_tensor)\n\n\n\n\ndef layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\n  """"""Runs layer normalization followed by dropout.""""""\n  output_tensor = layer_norm(input_tensor, name)\n  output_tensor = dropout(output_tensor, dropout_prob)\n  return output_tensor\n\n\ndef create_initializer(initializer_range=0.02):\n  """"""Creates a `truncated_normal_initializer` with the given range.""""""\n  return tf.compat.v1.truncated_normal_initializer(stddev=initializer_range)\n\n\ndef embedding_lookup(input_ids,\n                     vocab_size,\n                     embedding_size=128,\n                     initializer_range=0.02,\n                     word_embedding_name=""word_embeddings"",\n                     use_one_hot_embeddings=False):\n  """"""Looks up words embeddings for id tensor.\n\n  Args:\n    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n      ids.\n    vocab_size: int. Size of the embedding vocabulary.\n    embedding_size: int. Width of the word embeddings.\n    initializer_range: float. Embedding initialization range.\n    word_embedding_name: string. Name of the embedding table.\n    use_one_hot_embeddings: bool. If True, use one-hot method for word\n      embeddings. If False, use `tf.gather()`.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, embedding_size].\n  """"""\n  # This function assumes that the input is of shape [batch_size, seq_length,\n  # num_inputs].\n  #\n  # If the input is a 2D tensor of shape [batch_size, seq_length], we\n  # reshape to [batch_size, seq_length, 1].\n  if input_ids.shape.ndims == 2:\n    input_ids = tf.expand_dims(input_ids, axis=[-1])\n\n  embedding_table = tf.compat.v1.get_variable(\n      name=word_embedding_name,\n      shape=[vocab_size, embedding_size],\n      initializer=create_initializer(initializer_range))\n\n  flat_input_ids = tf.reshape(input_ids, [-1])\n  if use_one_hot_embeddings:\n    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n    output = tf.matmul(one_hot_input_ids, embedding_table)\n  else:\n    output = tf.gather(embedding_table, flat_input_ids)\n\n  input_shape = get_shape_list(input_ids)\n\n  output = tf.reshape(output,\n                      input_shape[0:-1] + [input_shape[-1] * embedding_size])\n  return (output, embedding_table)\n\n\ndef embedding_postprocessor(input_tensor,\n                            use_token_type=False,\n                            token_type_ids=None,\n                            token_type_vocab_size=16,\n                            token_type_embedding_name=""token_type_embeddings"",\n                            use_position_embeddings=True,\n                            position_embedding_name=""position_embeddings"",\n                            initializer_range=0.02,\n                            max_position_embeddings=512,\n                            dropout_prob=0.1):\n  """"""Performs various post-processing on a word embedding tensor.\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length,\n      embedding_size].\n    use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      Must be specified if `use_token_type` is True.\n    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.\n    token_type_embedding_name: string. The name of the embedding table variable\n      for token type ids.\n    use_position_embeddings: bool. Whether to add position embeddings for the\n      position of each token in the sequence.\n    position_embedding_name: string. The name of the embedding table variable\n      for positional embeddings.\n    initializer_range: float. Range of the weight initialization.\n    max_position_embeddings: int. Maximum sequence length that might ever be\n      used with this model. This can be longer than the sequence length of\n      input_tensor, but cannot be shorter.\n    dropout_prob: float. Dropout probability applied to the final output tensor.\n\n  Returns:\n    float tensor with same shape as `input_tensor`.\n\n  Raises:\n    ValueError: One of the tensor shapes or input values is invalid.\n  """"""\n  input_shape = get_shape_list(input_tensor, expected_rank=3)\n  batch_size = input_shape[0]\n  seq_length = input_shape[1]\n  width = input_shape[2]\n\n  output = input_tensor\n\n  if use_token_type:\n    if token_type_ids is None:\n      raise ValueError(""`token_type_ids` must be specified if""\n                       ""`use_token_type` is True."")\n    token_type_table = tf.compat.v1.get_variable(\n        name=token_type_embedding_name,\n        shape=[token_type_vocab_size, width],\n        initializer=create_initializer(initializer_range))\n    # This vocab will be small so we always do one-hot here, since it is always\n    # faster for a small vocabulary.\n    flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)\n    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n    token_type_embeddings = tf.reshape(token_type_embeddings,\n                                       [batch_size, seq_length, width])\n    output += token_type_embeddings\n\n  if use_position_embeddings:\n    assert_op = tf.compat.v1.assert_less_equal(seq_length, max_position_embeddings)\n    with tf.control_dependencies([assert_op]):\n      full_position_embeddings = tf.compat.v1.get_variable(\n          name=position_embedding_name,\n          shape=[max_position_embeddings, width],\n          initializer=create_initializer(initializer_range))\n      # Since the position embedding table is a learned variable, we create it\n      # using a (long) sequence length `max_position_embeddings`. The actual\n      # sequence length might be shorter than this, for faster training of\n      # tasks that do not have long sequences.\n      #\n      # So `full_position_embeddings` is effectively an embedding table\n      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n      # perform a slice.\n      position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n                                     [seq_length, -1])\n      num_dims = len(output.shape.as_list())\n\n      # Only the last two dimensions are relevant (`seq_length` and `width`), so\n      # we broadcast among the first dimensions, which is typically just\n      # the batch size.\n      position_broadcast_shape = []\n      for _ in range(num_dims - 2):\n        position_broadcast_shape.append(1)\n      position_broadcast_shape.extend([seq_length, width])\n      position_embeddings = tf.reshape(position_embeddings,\n                                       position_broadcast_shape)\n      output += position_embeddings\n\n  output = layer_norm_and_dropout(output, dropout_prob)\n  return output\n\n\ndef create_attention_mask_from_input_mask(from_tensor, to_mask):\n  """"""Create 3D attention mask from a 2D tensor mask.\n\n  Args:\n    from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n    to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n  """"""\n  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n  batch_size = from_shape[0]\n  from_seq_length = from_shape[1]\n\n  to_shape = get_shape_list(to_mask, expected_rank=2)\n  to_seq_length = to_shape[1]\n\n  to_mask = tf.cast(\n      tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n  # We don\'t assume that `from_tensor` is a mask (although it could be). We\n  # don\'t actually care if we attend *from* padding tokens (only *to* padding)\n  # tokens so we create a tensor of all ones.\n  #\n  # `broadcast_ones` = [batch_size, from_seq_length, 1]\n  broadcast_ones = tf.ones(\n      shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n  # Here we broadcast along two dimensions to create the mask.\n  mask = broadcast_ones * to_mask\n\n  return mask\n\n\ndef attention_layer(from_tensor,\n                    to_tensor,\n                    attention_mask=None,\n                    num_attention_heads=1,\n                    size_per_head=512,\n                    query_act=None,\n                    key_act=None,\n                    value_act=None,\n                    attention_probs_dropout_prob=0.0,\n                    initializer_range=0.02,\n                    do_return_2d_tensor=False,\n                    batch_size=None,\n                    from_seq_length=None,\n                    to_seq_length=None):\n  """"""Performs multi-headed attention from `from_tensor` to `to_tensor`.\n\n  This is an implementation of multi-headed attention based on ""Attention\n  is all you Need"". If `from_tensor` and `to_tensor` are the same, then\n  this is self-attention. Each timestep in `from_tensor` attends to the\n  corresponding sequence in `to_tensor`, and returns a fixed-with vector.\n\n  This function first projects `from_tensor` into a ""query"" tensor and\n  `to_tensor` into ""key"" and ""value"" tensors. These are (effectively) a list\n  of tensors of length `num_attention_heads`, where each tensor is of shape\n  [batch_size, seq_length, size_per_head].\n\n  Then, the query and key tensors are dot-producted and scaled. These are\n  softmaxed to obtain attention probabilities. The value tensors are then\n  interpolated by these probabilities, then concatenated back to a single\n  tensor and returned.\n\n  In practice, the multi-headed attention are done with transposes and\n  reshapes rather than actual separate tensors.\n\n  Args:\n    from_tensor: float Tensor of shape [batch_size, from_seq_length,\n      from_width].\n    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n    attention_mask: (optional) int32 Tensor of shape [batch_size,\n      from_seq_length, to_seq_length]. The values should be 1 or 0. The\n      attention scores will effectively be set to -infinity for any positions in\n      the mask that are 0, and will be unchanged for positions that are 1.\n    num_attention_heads: int. Number of attention heads.\n    size_per_head: int. Size of each attention head.\n    query_act: (optional) Activation function for the query transform.\n    key_act: (optional) Activation function for the key transform.\n    value_act: (optional) Activation function for the value transform.\n    attention_probs_dropout_prob: (optional) float. Dropout probability of the\n      attention probabilities.\n    initializer_range: float. Range of the weight initializer.\n    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size\n      * from_seq_length, num_attention_heads * size_per_head]. If False, the\n      output will be of shape [batch_size, from_seq_length, num_attention_heads\n      * size_per_head].\n    batch_size: (Optional) int. If the input is 2D, this might be the batch size\n      of the 3D version of the `from_tensor` and `to_tensor`.\n    from_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `from_tensor`.\n    to_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `to_tensor`.\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length,\n      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is\n      true, this will be of shape [batch_size * from_seq_length,\n      num_attention_heads * size_per_head]).\n\n  Raises:\n    ValueError: Any of the arguments or tensor shapes are invalid.\n  """"""\n\n  def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width):\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n  to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])\n\n  if len(from_shape) != len(to_shape):\n    raise ValueError(\n        ""The rank of `from_tensor` must match the rank of `to_tensor`."")\n\n  if len(from_shape) == 3:\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n    to_seq_length = to_shape[1]\n  elif len(from_shape) == 2:\n    if (batch_size is None or from_seq_length is None or to_seq_length is None):\n      raise ValueError(\n          ""When passing in rank 2 tensors to attention_layer, the values ""\n          ""for `batch_size`, `from_seq_length`, and `to_seq_length` ""\n          ""must all be specified."")\n\n  # Scalar dimensions referenced here:\n  #   B = batch size (number of sequences)\n  #   F = `from_tensor` sequence length\n  #   T = `to_tensor` sequence length\n  #   N = `num_attention_heads`\n  #   H = `size_per_head`\n\n  from_tensor_2d = reshape_to_matrix(from_tensor)\n  to_tensor_2d = reshape_to_matrix(to_tensor)\n\n  # `query_layer` = [B*F, N*H]\n  query_layer = tf.compat.v1.layers.dense(\n      from_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=query_act,\n      name=""query"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  # `key_layer` = [B*T, N*H]\n  key_layer = tf.compat.v1.layers.dense(\n      to_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=key_act,\n      name=""key"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  # `value_layer` = [B*T, N*H]\n  value_layer = tf.compat.v1.layers.dense(\n      to_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=value_act,\n      name=""value"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  # `query_layer` = [B, N, F, H]\n  query_layer = transpose_for_scores(query_layer, batch_size,\n                                     num_attention_heads, from_seq_length,\n                                     size_per_head)\n\n  # `key_layer` = [B, N, T, H]\n  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,\n                                   to_seq_length, size_per_head)\n\n  # Take the dot product between ""query"" and ""key"" to get the raw\n  # attention scores.\n  # `attention_scores` = [B, N, F, T]\n  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n  attention_scores = tf.multiply(attention_scores,\n                                 1.0 / math.sqrt(float(size_per_head)))\n\n  if attention_mask is not None:\n    # `attention_mask` = [B, 1, F, T]\n    attention_mask = tf.expand_dims(attention_mask, axis=[1])\n\n    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n    # masked positions, this operation will create a tensor which is 0.0 for\n    # positions we want to attend and -10000.0 for masked positions.\n    adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n\n    # Since we are adding it to the raw scores before the softmax, this is\n    # effectively the same as removing these entirely.\n    attention_scores += adder\n\n  # Normalize the attention scores to probabilities.\n  # `attention_probs` = [B, N, F, T]\n  attention_probs = tf.nn.softmax(attention_scores)\n\n  # This is actually dropping out entire tokens to attend to, which might\n  # seem a bit unusual, but is taken from the original Transformer paper.\n  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)\n\n  # `value_layer` = [B, T, N, H]\n  value_layer = tf.reshape(\n      value_layer,\n      [batch_size, to_seq_length, num_attention_heads, size_per_head])\n\n  # `value_layer` = [B, N, T, H]\n  value_layer = tf.transpose(value_layer, [0, 2, 1, 3])\n\n  # `context_layer` = [B, N, F, H]\n  context_layer = tf.matmul(attention_probs, value_layer)\n\n  # `context_layer` = [B, F, N, H]\n  context_layer = tf.transpose(context_layer, [0, 2, 1, 3])\n\n  if do_return_2d_tensor:\n    # `context_layer` = [B*F, N*H]\n    context_layer = tf.reshape(\n        context_layer,\n        [batch_size * from_seq_length, num_attention_heads * size_per_head])\n  else:\n    # `context_layer` = [B, F, N*H]\n    context_layer = tf.reshape(\n        context_layer,\n        [batch_size, from_seq_length, num_attention_heads * size_per_head])\n\n  return context_layer\n\n\ndef transformer_model(input_tensor,\n                      attention_mask=None,\n                      hidden_size=768,\n                      num_hidden_layers=12,\n                      num_attention_heads=12,\n                      intermediate_size=3072,\n                      intermediate_act_fn=gelu,\n                      hidden_dropout_prob=0.1,\n                      attention_probs_dropout_prob=0.1,\n                      initializer_range=0.02,\n                      do_return_all_layers=False):\n  """"""Multi-headed, multi-layer Transformer from ""Attention is All You Need"".\n\n  This is almost an exact implementation of the original Transformer encoder.\n\n  See the original paper:\n  https://arxiv.org/abs/1706.03762\n\n  Also see:\n  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n      seq_length], with 1 for positions that can be attended to and 0 in\n      positions that should not be.\n    hidden_size: int. Hidden size of the Transformer.\n    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n    num_attention_heads: int. Number of attention heads in the Transformer.\n    intermediate_size: int. The size of the ""intermediate"" (a.k.a., feed\n      forward) layer.\n    intermediate_act_fn: function. The non-linear activation function to apply\n      to the output of the intermediate/feed-forward layer.\n    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n    attention_probs_dropout_prob: float. Dropout probability of the attention\n      probabilities.\n    initializer_range: float. Range of the initializer (stddev of truncated\n      normal).\n    do_return_all_layers: Whether to also return all layers or just the final\n      layer.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n    hidden layer of the Transformer.\n\n  Raises:\n    ValueError: A Tensor shape or parameter is invalid.\n  """"""\n  if hidden_size % num_attention_heads != 0:\n    raise ValueError(\n        ""The hidden size (%d) is not a multiple of the number of attention ""\n        ""heads (%d)"" % (hidden_size, num_attention_heads))\n\n  attention_head_size = int(hidden_size / num_attention_heads)\n  input_shape = get_shape_list(input_tensor, expected_rank=3)\n  batch_size = input_shape[0]\n  seq_length = input_shape[1]\n  input_width = input_shape[2]\n\n  # The Transformer performs sum residuals on all layers so the input needs\n  # to be the same as the hidden size.\n  if input_width != hidden_size:\n    raise ValueError(""The width of the input tensor (%d) != hidden size (%d)"" %\n                     (input_width, hidden_size))\n\n  # We keep the representation as a 2D tensor to avoid re-shaping it back and\n  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on\n  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to\n  # help the optimizer.\n  prev_output = reshape_to_matrix(input_tensor)\n\n  all_layer_outputs = []\n  for layer_idx in range(num_hidden_layers):\n    with tf.compat.v1.variable_scope(""layer_%d"" % layer_idx):\n      layer_input = prev_output\n\n      with tf.compat.v1.variable_scope(""attention""):\n        attention_heads = []\n        with tf.compat.v1.variable_scope(""self""):\n          attention_head = attention_layer(\n              from_tensor=layer_input,\n              to_tensor=layer_input,\n              attention_mask=attention_mask,\n              num_attention_heads=num_attention_heads,\n              size_per_head=attention_head_size,\n              attention_probs_dropout_prob=attention_probs_dropout_prob,\n              initializer_range=initializer_range,\n              do_return_2d_tensor=True,\n              batch_size=batch_size,\n              from_seq_length=seq_length,\n              to_seq_length=seq_length)\n          attention_heads.append(attention_head)\n\n        attention_output = None\n        if len(attention_heads) == 1:\n          attention_output = attention_heads[0]\n        else:\n          # In the case where we have other sequences, we just concatenate\n          # them to the self-attention head before the projection.\n          attention_output = tf.concat(attention_heads, axis=-1)\n\n        # Run a linear projection of `hidden_size` then add a residual\n        # with `layer_input`.\n        with tf.compat.v1.variable_scope(""output""):\n          attention_output = tf.compat.v1.layers.dense(\n              attention_output,\n              hidden_size,\n              kernel_initializer=create_initializer(initializer_range))\n          attention_output = dropout(attention_output, hidden_dropout_prob)\n          attention_output = layer_norm(attention_output + layer_input)\n\n      # The activation is only applied to the ""intermediate"" hidden layer.\n      with tf.compat.v1.variable_scope(""intermediate""):\n        intermediate_output = tf.compat.v1.layers.dense(\n            attention_output,\n            intermediate_size,\n            activation=intermediate_act_fn,\n            kernel_initializer=create_initializer(initializer_range))\n\n      # Down-project back to `hidden_size` then add the residual.\n      with tf.compat.v1.variable_scope(""output""):\n        layer_output = tf.compat.v1.layers.dense(\n            intermediate_output,\n            hidden_size,\n            kernel_initializer=create_initializer(initializer_range))\n        layer_output = dropout(layer_output, hidden_dropout_prob)\n        layer_output = layer_norm(layer_output + attention_output)\n        prev_output = layer_output\n        all_layer_outputs.append(layer_output)\n\n  if do_return_all_layers:\n    final_outputs = []\n    for layer_output in all_layer_outputs:\n      final_output = reshape_from_matrix(layer_output, input_shape)\n      final_outputs.append(final_output)\n    return final_outputs\n  else:\n    final_output = reshape_from_matrix(prev_output, input_shape)\n    return final_output\n\n\ndef get_shape_list(tensor, expected_rank=None, name=None):\n  """"""Returns a list of the shape of tensor, preferring static dimensions.\n\n  Args:\n    tensor: A tf.Tensor object to find the shape of.\n    expected_rank: (optional) int. The expected rank of `tensor`. If this is\n      specified and the `tensor` has a different rank, and exception will be\n      thrown.\n    name: Optional name of the tensor for the error message.\n\n  Returns:\n    A list of dimensions of the shape of tensor. All static dimensions will\n    be returned as python integers, and dynamic dimensions will be returned\n    as tf.Tensor scalars.\n  """"""\n  if name is None:\n    name = tensor.name\n\n  if expected_rank is not None:\n    assert_rank(tensor, expected_rank, name)\n\n  shape = tensor.shape.as_list()\n\n  non_static_indexes = []\n  for (index, dim) in enumerate(shape):\n    if dim is None:\n      non_static_indexes.append(index)\n\n  if not non_static_indexes:\n    return shape\n\n  dyn_shape = tf.shape(tensor)\n  for index in non_static_indexes:\n    shape[index] = dyn_shape[index]\n  return shape\n\n\ndef reshape_to_matrix(input_tensor):\n  """"""Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).""""""\n  ndims = input_tensor.shape.ndims\n  if ndims < 2:\n    raise ValueError(""Input tensor must have at least rank 2. Shape = %s"" %\n                     (input_tensor.shape))\n  if ndims == 2:\n    return input_tensor\n\n  width = input_tensor.shape[-1]\n  output_tensor = tf.reshape(input_tensor, [-1, width])\n  return output_tensor\n\n\ndef reshape_from_matrix(output_tensor, orig_shape_list):\n  """"""Reshapes a rank 2 tensor back to its original rank >= 2 tensor.""""""\n  if len(orig_shape_list) == 2:\n    return output_tensor\n\n  output_shape = get_shape_list(output_tensor)\n\n  orig_dims = orig_shape_list[0:-1]\n  width = output_shape[-1]\n\n  return tf.reshape(output_tensor, orig_dims + [width])\n\n\ndef assert_rank(tensor, expected_rank, name=None):\n  """"""Raises an exception if the tensor rank is not of the expected rank.\n\n  Args:\n    tensor: A tf.Tensor to check the rank of.\n    expected_rank: Python integer or list of integers, expected rank.\n    name: Optional name of the tensor for the error message.\n\n  Raises:\n    ValueError: If the expected shape doesn\'t match the actual shape.\n  """"""\n  if name is None:\n    name = tensor.name\n\n  expected_rank_dict = {}\n  if isinstance(expected_rank, six.integer_types):\n    expected_rank_dict[expected_rank] = True\n  else:\n    for x in expected_rank:\n      expected_rank_dict[x] = True\n\n  actual_rank = tensor.shape.ndims\n  if actual_rank not in expected_rank_dict:\n    scope_name = tf.get_variable_scope().name\n    raise ValueError(\n        ""For the tensor `%s` in scope `%s`, the actual rank ""\n        ""`%d` (shape = %s) is not equal to the expected rank `%s`"" %\n        (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))\n'"
egs/conll2003/pretrain/v1/local/preprocess_bert_dataset.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport sys\nfrom absl import logging\nfrom bert import tokenization\n\ndef bert_preprocess(filename, vocab):\n  tokenizer = tokenization.FullTokenizer(\n    vocab_file=vocab, do_lower_case=False)\n  new_filename = filename + "".bert""\n  f1 = open(new_filename, \'w\')\n  per_count = 0\n  with open(filename, ""r"") as f:\n    lines = f.readlines()\n    for line in lines:\n      str1 = line.split(""\\t"")[1]\n      label1 = line.split(""\\t"")[0]\n      new_label_list = []\n      old_label_list = label1.split(\' \')\n      word_list = str1.split(\' \')\n      tokens = []\n      tokens.append(\'[CLS]\')\n      new_label_list.append(\'O\')\n      per_count = 0\n      for i, (w, t) in enumerate(zip(word_list, old_label_list)):\n        token = tokenizer.tokenize(w)\n        tokens.extend(token)\n        for i, _ in enumerate(token):\n          if i == 0:\n            new_label_list.append(t)\n          else:\n            new_label_list.append(""X"")\n      tokens.append(\'[SEG]\')\n      new_label_list.append(\'O\')\n      assert len(tokens) == len(new_label_list)\n      rm_new_label_list = [i for i in new_label_list if i != \'O\' and i != \'X\']\n      rm_old_label_list = [i for i in old_label_list if i != \'O\' and i != \'X\']\n      assert len(rm_new_label_list) == len(rm_old_label_list)\n      f1.write("" "".join(new_label_list) + \'\\t\' +\n               "" "".join(tokens) + \'\\n\')\n\ndef change_data_format(in_file, out_file):\n  with open(out_file, ""w"") as f_out:\n    with open(in_file, ""r"") as f_in:\n      for line in f_in.readlines():\n        line_parts = line.strip().split(""\\t"")\n        if len(line_parts) != 2:\n          logging.error(""line error"")\n        else:\n          label = line_parts[0].split("" "")\n          text = line_parts[1]\n          new_label = [x for x in label]\n          ind = 0\n          while ind < len(label):\n            if label[ind] == ""X"":\n              start, end = ind, ind+1\n              while end < len(label) and label[end] == ""X"":\n                end += 1\n              label_parts = label[ind-1].split(""-"")\n              if len(label_parts) == 2:\n                _, type = label_parts[0], label_parts[1]\n                for i in range(start, end):\n                  new_label[i] = ""I-"" + type\n              elif len(label_parts) == 1:\n                for i in range(start, end):\n                  new_label[i] = label_parts[0]\n              else:\n                logging.error(""label error"")\n              ind = end + 1\n            else:\n              ind += 1\n          assert len(new_label) == len(text.split("" ""))\n          new_line = "" "".join(new_label)+""\\t""+text+""\\n""\n          f_out.write(new_line)\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n\n  if len(sys.argv) != 5:\n    logging.error(""Usage python {} train_path, val_path, test_path, raw_vocab""\n                  .format(sys.argv[0]))\n    sys.exit(-1)\n\n  train_path = sys.argv[1]\n  val_path = sys.argv[2]\n  test_path = sys.argv[3]\n  raw_vocab = sys.argv[4]\n  bert_preprocess(train_path, raw_vocab)\n  change_data_format(train_path + "".bert"", train_path + "".bert.new"")\n  bert_preprocess(val_path, raw_vocab)\n  change_data_format(val_path + "".bert"", val_path + "".bert.new"")\n  bert_preprocess(test_path, raw_vocab)\n  change_data_format(test_path + "".bert"", test_path + "".bert.new"")\n'"
egs/conll2003/pretrain/v1/local/preprocess_elmo_dataset.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport sys\nfrom absl import logging\n\ndef add_start_end_token(file):\n  f1 = open(file + "".elmo"", \'w\')\n  with open(file, \'r\') as f:\n    add_lines = [\'O \' + line.strip().split(\'\\t\')[0] + \' O\'\n                 + \'\\t\' + \'<s> \' + line.strip().split(\'\\t\')[1] + \' </s>\'\n                 for line in f.readlines()]\n    f1.write(\'\\n\'.join(add_lines))\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n\n  if len(sys.argv) != 4:\n    logging.error(""Usage python {} train_path, val_path, test_path"".format(sys.argv[0]))\n    sys.exit(-1)\n\n  train_path = sys.argv[1]\n  val_path = sys.argv[2]\n  test_path = sys.argv[3]\n  add_start_end_token(train_path)\n  add_start_end_token(val_path)\n  add_start_end_token(test_path)\n'"
egs/conll2003/pretrain/v1/local/tokenization.py,2,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tokenization classes.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport re\nimport unicodedata\nimport six\nimport tensorflow as tf\n\n\ndef validate_case_matches_checkpoint(do_lower_case, init_checkpoint):\n  """"""Checks whether the casing config is consistent with the checkpoint name.""""""\n\n  # The casing has to be passed in by the user and there is no explicit check\n  # as to whether it matches the checkpoint. The casing information probably\n  # should have been stored in the bert_config.json file, but it\'s not, so\n  # we have to heuristically detect it to validate.\n\n  if not init_checkpoint:\n    return\n\n  m = re.match(""^.*?([A-Za-z0-9_-]+)/bert_model.ckpt"", init_checkpoint)\n  if m is None:\n    return\n\n  model_name = m.group(1)\n\n  lower_models = [\n      ""uncased_L-24_H-1024_A-16"", ""uncased_L-12_H-768_A-12"",\n      ""multilingual_L-12_H-768_A-12"", ""chinese_L-12_H-768_A-12""\n  ]\n\n  cased_models = [\n      ""cased_L-12_H-768_A-12"", ""cased_L-24_H-1024_A-16"",\n      ""multi_cased_L-12_H-768_A-12""\n  ]\n\n  is_bad_config = False\n  if model_name in lower_models and not do_lower_case:\n    is_bad_config = True\n    actual_flag = ""False""\n    case_name = ""lowercased""\n    opposite_flag = ""True""\n\n  if model_name in cased_models and do_lower_case:\n    is_bad_config = True\n    actual_flag = ""True""\n    case_name = ""cased""\n    opposite_flag = ""False""\n\n  if is_bad_config:\n    raise ValueError(\n        ""You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. ""\n        ""However, `%s` seems to be a %s model, so you ""\n        ""should pass in `--do_lower_case=%s` so that the fine-tuning matches ""\n        ""how the model was pre-training. If this error is wrong, please ""\n        ""just comment out this check."" % (actual_flag, init_checkpoint,\n                                          model_name, case_name, opposite_flag))\n\n\ndef convert_to_unicode(text):\n  """"""Converts `text` to Unicode (if it\'s not already), assuming utf-8 input.""""""\n  if six.PY3:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, bytes):\n      return text.decode(""utf-8"", ""ignore"")\n    else:\n      raise ValueError(""Unsupported string type: %s"" % (type(text)))\n  elif six.PY2:\n    if isinstance(text, str):\n      return text.decode(""utf-8"", ""ignore"")\n    elif isinstance(text, unicode):  # noqa: F821\n      return text\n    else:\n      raise ValueError(""Unsupported string type: %s"" % (type(text)))\n  else:\n    raise ValueError(""Not running on Python2 or Python 3?"")\n\n\ndef printable_text(text):\n  """"""Returns text encoded in a way suitable for print or `tf.logging`.""""""\n\n  # These functions want `str` for both Python2 and Python3, but in one case\n  # it\'s a Unicode string and in the other it\'s a byte string.\n  if six.PY3:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, bytes):\n      return text.decode(""utf-8"", ""ignore"")\n    else:\n      raise ValueError(""Unsupported string type: %s"" % (type(text)))\n  elif six.PY2:\n    if isinstance(text, str):\n      return text\n    elif isinstance(text, unicode):  # noqa: F821\n      return text.encode(""utf-8"")\n    else:\n      raise ValueError(""Unsupported string type: %s"" % (type(text)))\n  else:\n    raise ValueError(""Not running on Python2 or Python 3?"")\n\n\ndef load_vocab(vocab_file):\n  """"""Loads a vocabulary file into a dictionary.""""""\n  vocab = collections.OrderedDict()\n  index = 0\n  with tf.io.gfile.GFile(vocab_file, ""r"") as reader:\n    while True:\n      token = convert_to_unicode(reader.readline())\n      if not token:\n        break\n      token = token.strip()\n      vocab[token] = index\n      index += 1\n  return vocab\n\n\ndef convert_by_vocab(vocab, items):\n  """"""Converts a sequence of [tokens|ids] using the vocab.""""""\n  output = []\n  for item in items:\n    output.append(vocab[item])\n  return output\n\n\ndef convert_tokens_to_ids(vocab, tokens):\n  return convert_by_vocab(vocab, tokens)\n\n\ndef convert_ids_to_tokens(inv_vocab, ids):\n  return convert_by_vocab(inv_vocab, ids)\n\n\ndef whitespace_tokenize(text):\n  """"""Runs basic whitespace cleaning and splitting on a piece of text.""""""\n  text = text.strip()\n  if not text:\n    return []\n  tokens = text.split()\n  return tokens\n\n\nclass FullTokenizer(object):\n  """"""Runs end-to-end tokenziation.""""""\n\n  def __init__(self, vocab_file, do_lower_case=True):\n    self.vocab = load_vocab(vocab_file)\n    self.inv_vocab = {v: k for k, v in self.vocab.items()}\n    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n\n  def tokenize(self, text):\n    split_tokens = []\n    for token in self.basic_tokenizer.tokenize(text):\n      for sub_token in self.wordpiece_tokenizer.tokenize(token):\n        split_tokens.append(sub_token)\n\n    return split_tokens\n\n  def convert_tokens_to_ids(self, tokens):\n    return convert_by_vocab(self.vocab, tokens)\n\n  def convert_ids_to_tokens(self, ids):\n    return convert_by_vocab(self.inv_vocab, ids)\n\n\nclass BasicTokenizer(object):\n  """"""Runs basic tokenization (punctuation splitting, lower casing, etc.).""""""\n\n  def __init__(self, do_lower_case=True):\n    """"""Constructs a BasicTokenizer.\n\n    Args:\n      do_lower_case: Whether to lower case the input.\n    """"""\n    self.do_lower_case = do_lower_case\n\n  def tokenize(self, text):\n    """"""Tokenizes a piece of text.""""""\n    text = convert_to_unicode(text)\n    text = self._clean_text(text)\n\n    # This was added on November 1st, 2018 for the multilingual and Chinese\n    # models. This is also applied to the English models now, but it doesn\'t\n    # matter since the English models were not trained on any Chinese data\n    # and generally don\'t have any Chinese data in them (there are Chinese\n    # characters in the vocabulary because Wikipedia does have some Chinese\n    # words in the English Wikipedia.).\n    text = self._tokenize_chinese_chars(text)\n\n    orig_tokens = whitespace_tokenize(text)\n    split_tokens = []\n    for token in orig_tokens:\n      if self.do_lower_case:\n        token = token.lower()\n        token = self._run_strip_accents(token)\n      split_tokens.extend(self._run_split_on_punc(token))\n\n    output_tokens = whitespace_tokenize("" "".join(split_tokens))\n    return output_tokens\n\n  def _run_strip_accents(self, text):\n    """"""Strips accents from a piece of text.""""""\n    text = unicodedata.normalize(""NFD"", text)\n    output = []\n    for char in text:\n      cat = unicodedata.category(char)\n      if cat == ""Mn"":\n        continue\n      output.append(char)\n    return """".join(output)\n\n  def _run_split_on_punc(self, text):\n    """"""Splits punctuation on a piece of text.""""""\n    chars = list(text)\n    i = 0\n    start_new_word = True\n    output = []\n    while i < len(chars):\n      char = chars[i]\n      if _is_punctuation(char):\n        output.append([char])\n        start_new_word = True\n      else:\n        if start_new_word:\n          output.append([])\n        start_new_word = False\n        output[-1].append(char)\n      i += 1\n\n    return ["""".join(x) for x in output]\n\n  def _tokenize_chinese_chars(self, text):\n    """"""Adds whitespace around any CJK character.""""""\n    output = []\n    for char in text:\n      cp = ord(char)\n      if self._is_chinese_char(cp):\n        output.append("" "")\n        output.append(char)\n        output.append("" "")\n      else:\n        output.append(char)\n    return """".join(output)\n\n  def _is_chinese_char(self, cp):\n    """"""Checks whether CP is the codepoint of a CJK character.""""""\n    # This defines a ""chinese character"" as anything in the CJK Unicode block:\n    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n    #\n    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n    # despite its name. The modern Korean Hangul alphabet is a different block,\n    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n    # space-separated words, so they are not treated specially and handled\n    # like the all of the other languages.\n    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n        (cp >= 0x3400 and cp <= 0x4DBF) or  #\n        (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n        (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n        (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n        (cp >= 0x2B820 and cp <= 0x2CEAF) or\n        (cp >= 0xF900 and cp <= 0xFAFF) or  #\n        (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n      return True\n\n    return False\n\n  def _clean_text(self, text):\n    """"""Performs invalid character removal and whitespace cleanup on text.""""""\n    output = []\n    for char in text:\n      cp = ord(char)\n      if cp == 0 or cp == 0xfffd or _is_control(char):\n        continue\n      if _is_whitespace(char):\n        output.append("" "")\n      else:\n        output.append(char)\n    return """".join(output)\n\n\nclass WordpieceTokenizer(object):\n  """"""Runs WordPiece tokenziation.""""""\n\n  def __init__(self, vocab, unk_token=""[UNK]"", max_input_chars_per_word=200):\n    self.vocab = vocab\n    self.unk_token = unk_token\n    self.max_input_chars_per_word = max_input_chars_per_word\n\n  def tokenize(self, text):\n    """"""Tokenizes a piece of text into its word pieces.\n\n    This uses a greedy longest-match-first algorithm to perform tokenization\n    using the given vocabulary.\n\n    For example:\n      input = ""unaffable""\n      output = [""un"", ""##aff"", ""##able""]\n\n    Args:\n      text: A single token or whitespace separated tokens. This should have\n        already been passed through `BasicTokenizer.\n\n    Returns:\n      A list of wordpiece tokens.\n    """"""\n\n    text = convert_to_unicode(text)\n\n    output_tokens = []\n    for token in whitespace_tokenize(text):\n      chars = list(token)\n      if len(chars) > self.max_input_chars_per_word:\n        output_tokens.append(self.unk_token)\n        continue\n\n      is_bad = False\n      start = 0\n      sub_tokens = []\n      while start < len(chars):\n        end = len(chars)\n        cur_substr = None\n        while start < end:\n          substr = """".join(chars[start:end])\n          if start > 0:\n            substr = ""##"" + substr\n          if substr in self.vocab:\n            cur_substr = substr\n            break\n          end -= 1\n        if cur_substr is None:\n          is_bad = True\n          break\n        sub_tokens.append(cur_substr)\n        start = end\n\n      if is_bad:\n        output_tokens.append(self.unk_token)\n      else:\n        output_tokens.extend(sub_tokens)\n    return output_tokens\n\n\ndef _is_whitespace(char):\n  """"""Checks whether `chars` is a whitespace character.""""""\n  # \\t, \\n, and \\r are technically contorl characters but we treat them\n  # as whitespace since they are generally considered as such.\n  if char == "" "" or char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n    return True\n  cat = unicodedata.category(char)\n  if cat == ""Zs"":\n    return True\n  return False\n\n\ndef _is_control(char):\n  """"""Checks whether `chars` is a control character.""""""\n  # These are technically control characters but we count them as whitespace\n  # characters.\n  if char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n    return False\n  cat = unicodedata.category(char)\n  if cat in (""Cc"", ""Cf""):\n    return True\n  return False\n\n\ndef _is_punctuation(char):\n  """"""Checks whether `chars` is a punctuation character.""""""\n  cp = ord(char)\n  # We treat all non-letter/number ASCII as punctuation.\n  # Characters such as ""^"", ""$"", and ""`"" are not in the Unicode\n  # Punctuation class but we treat them as punctuation anyways, for\n  # consistency.\n  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n    return True\n  cat = unicodedata.category(char)\n  if cat.startswith(""P""):\n    return True\n  return False\n'"
egs/conll2003/pretrain/v1/local/transfer_bert_model.py,10,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport os\nimport sys\nfrom bert import modeling\nfrom absl import logging\n\nimport delta.compat as tf\n\ndef transfer_bert_model(bert_model_dir, output_bert_model):\n  graph = tf.Graph()\n  max_seq_len = 512\n  num_labels = 2\n  use_one_hot_embeddings = False\n  with graph.as_default():\n    with tf.Session() as sess:\n      input_ids = tf.placeholder(tf.int32, (None, None), \'input_ids\')\n      input_mask = tf.placeholder(tf.int32, (None, None), \'input_mask\')\n      segment_ids = tf.placeholder(tf.int32, (None, None), \'segment_ids\')\n\n      bert_config = modeling.BertConfig.from_json_file(os.path.join(bert_model_dir, \'bert_config.json\'))\n      model = modeling.BertModel(\n        config=bert_config,\n        is_training=False,\n        input_ids=input_ids,\n        input_mask=input_mask,\n        token_type_ids=segment_ids,\n        use_one_hot_embeddings=use_one_hot_embeddings)\n      all_encoder_layers = model.get_all_encoder_layers()\n      input_x_bert_cls = model.get_pooled_output()\n      for idx, layer in enumerate(all_encoder_layers):\n        layer = tf.identity(layer, ""encoder_layers_"" + str(idx))\n        print(""layer:"", layer)\n      input_x_bert_cls = tf.identity(input_x_bert_cls, ""input_x_bert_cls"")\n      print(""input_x_bert_cls"", input_x_bert_cls)\n      saver = tf.train.Saver()\n\n    with tf.Session() as sess:\n      sess.run(tf.global_variables_initializer())\n      saver.restore(sess, bert_model_dir + ""/bert_model.ckpt"")\n      saver.save(sess, output_bert_model)\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n\n  if len(sys.argv) != 3:\n    logging.error(""Usage python {} bert_model_dir output_bert_model"".format(sys.argv[0]))\n    sys.exit(-1)\n  bert_model_dir = sys.argv[1]\n  output_bert_model = sys.argv[2]\n  transfer_bert_model(bert_model_dir, output_bert_model)\n'"
egs/conll2003/pretrain/v1/local/transfer_elmo_model.py,7,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport sys\nimport tensorflow as tf\nfrom absl import logging\nfrom bilm.bilm import TokenBatcher, BidirectionalLanguageModel, weight_layers, \\\n    dump_token_embeddings\n\ndef transfer_elmo_model(vocab_file, options_file, weight_file, token_embedding_file,\n                        output_elmo_model):\n\n  dump_token_embeddings(\n      vocab_file, options_file, weight_file, token_embedding_file\n  )\n  logging.info(""finish dump_token_embeddings"")\n  tf.reset_default_graph()\n\n  with tf.Session(graph=tf.Graph()) as sess:\n    bilm = BidirectionalLanguageModel(\n      options_file,\n      weight_file,\n      use_character_inputs=False,\n      embedding_weight_file=token_embedding_file\n    )\n    input_x = tf.placeholder(tf.int32, shape=[None, None],\n                             name=\'input_x\')\n    train_embeddings_op = bilm(input_x)\n    input_x_elmo_op = weight_layers(\n      \'output\', train_embeddings_op, l2_coef=0.0\n    )[\'weighted_op\']\n    input_x_elmo = tf.identity(input_x_elmo_op, name=""input_x_elmo"")\n    logging.info(""input_x_elmo shape: {}"".format(input_x_elmo))\n    sess.run(tf.global_variables_initializer())\n    saver = tf.train.Saver()\n    saver.save(sess, output_elmo_model)\n    logging.info(""finish saving!"")\n\n    all_variables = tf.get_collection_ref(tf.GraphKeys.GLOBAL_VARIABLES)\n    for v in all_variables:\n      logging.info(""variable name: {}"".format(v.name))\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n\n  if len(sys.argv) != 6:\n    logging.error(""Usage python {} vocab_file, options_file, ""\n                  ""weight_file, token_embedding_file, output_bert_model""\n                  .format(sys.argv[0]))\n    sys.exit(-1)\n\n  vocab_file = sys.argv[1]\n  options_file = sys.argv[2]\n  weight_file = sys.argv[3]\n  token_embedding_file = sys.argv[4]\n  output_bert_model = sys.argv[5]\n  transfer_elmo_model(vocab_file, options_file,\n                      weight_file, token_embedding_file, output_bert_model)\n'"
egs/conll2003/seq_label/v1/local/change_data_format.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport sys\nfrom absl import logging\n\n\ndef change_data_format(files):\n  for data_file_in in files:\n    logging.info(""Change data format: {}"".format(data_file_in))\n    data_file_out = data_file_in.replace("".in"", "".out"")\n    words, labels = [], []\n    with open(data_file_out, ""w"", encoding=""utf-8"") as output_file:\n      with open(data_file_in, ""r"", encoding=""utf-8"") as file_input:\n        for line in file_input.readlines():\n          word = line.strip().split(\' \')[0]\n          label = line.strip().split(\' \')[-1]\n          # here we dont do ""DOCSTART"" check\n          if len(line.strip()) == 0:\n            l = [label for label in labels if len(label) > 0]\n            w = [word for word in words if len(word) > 0]\n            assert len(l) == len(w)\n            l, w = \' \'.join(l), \' \'.join(w)\n            output_file.write(l + ""\\t"" + w + ""\\n"")\n            words, labels = [], []\n          words.append(word)\n          labels.append(label)\n    logging.info(""Change data done: {}"".format(data_file_out))\n\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n\n  if len(sys.argv) != 4:\n    logging.error(""Usage python {} train_file, dev_file, test_file"".format(sys.argv[0]))\n    sys.exit(-1)\n\n  train_file = sys.argv[1]\n  dev_file = sys.argv[2]\n  test_file = sys.argv[3]\n  files = [train_file, dev_file, test_file]\n\n  change_data_format(files)\n'"
egs/hkust/asr/v1/local/hkust_segment.py,0,"b'#!/usr/bin/env python2\n#coding:utf-8\n\nfrom __future__ import print_function\nimport sys\nfrom mmseg import seg_txt\nfor line in sys.stdin:\n  blks = str.split(line)\n  out_line = blks[0]\n  for i in range(1, len(blks)):\n    if blks[i] == ""[VOCALIZED-NOISE]"" or blks[i] == ""[NOISE]"" or blks[i] == ""[LAUGHTER]"":\n      out_line += "" "" + blks[i]\n      continue\n    for j in seg_txt(blks[i]):\n      out_line += "" "" + j\n  print(out_line)\n'"
egs/mini_an4/asr/v1/local/data_prep.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2016  Allen Guo\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n# MERCHANTABLITY OR NON-INFRINGEMENT.\n# See the Apache 2 License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import print_function\nimport os\nimport re\nimport sys\n\nif len(sys.argv) != 3:\n    print (\'Usage: python data_prep.py [an4_root] [sph2pipe]\')\n    sys.exit(1)\nan4_root = sys.argv[1]\nsph2pipe = sys.argv[2]\n\nsph_dir = {\n    \'train\': \'an4_clstk\',\n    \'test\': \'an4test_clstk\'\n}\n\nfor x in [\'train\', \'test\']:\n    with open(os.path.join(an4_root, \'etc\', \'an4_\' + x + \'.transcription\')) as transcript_f, \\\n         open(os.path.join(\'data\', x, \'text\'), \'w\') as text_f, \\\n         open(os.path.join(\'data\', x, \'wav.scp\'), \'w\') as wav_scp_f, \\\n         open(os.path.join(\'data\', x, \'utt2spk\'), \'w\') as utt2spk_f:\n\n        text_f.truncate()\n        wav_scp_f.truncate()\n        utt2spk_f.truncate()\n\n        lines = sorted(transcript_f.readlines(), key=lambda s: s.split(\' \')[0])\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n            words = re.search(r\'^(.*) \\(\', line).group(1)\n            if words[:4] == \'<s> \':\n                words = words[4:]\n            if words[-5:] == \' </s>\':\n                words = words[:-5]\n            source = re.search(r\'\\((.*)\\)\', line).group(1)\n            pre, mid, last = source.split(\'-\')\n            utt_id = \'-\'.join([mid, pre, last])\n\n            text_f.write(utt_id + \' \' + words + \'\\n\')\n            wav_scp_f.write(utt_id + \' \' + sph2pipe + \' -f wav -p -c 1 \' + \\\n                os.path.join(an4_root, \'wav\', sph_dir[x], mid, source + \'.sph\') + \' |\\n\')\n            utt2spk_f.write(utt_id + \' \' + mid + \'\\n\')\n'"
egs/mock_text_cls_data/text_cls/v1/local/generate_mock_data.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport sys\nfrom absl import logging\nfrom delta.data.utils.test_utils import mock_a_text_file\nfrom delta.data.utils.test_utils import save_a_vocab_file\n\nsamples_english = [""1\\tAll is well"", ""0\\tI am very angry""]\nsamples_split_line_mark = [""1\\t\xe9\x83\xbd\xe6\x8c\xba\xe5\xa5\xbd\xe3\x80\x82|\xe9\x83\xbd\xe6\x98\xaf\xe7\x9a\x84\xe5\x91\x80"", ""0\\t\xe6\x88\x91\xe5\xbe\x88\xe6\x84\xa4\xe6\x80\x92|\xe8\xb6\x85\xe7\xba\xa7\xe7\x94\x9f\xe6\xb0\x94\xef\xbc\x81""]\nsamples_split_by_space = [""1\\t\xe9\x83\xbd \xe6\x8c\xba\xe5\xa5\xbd"", ""0\\t\xe6\x88\x91 \xe5\xbe\x88 \xe6\x84\xa4\xe6\x80\x92""]\nsamples_split_by_char = [""1\\t\xe9\x83\xbd\xe6\x8c\xba\xe5\xa5\xbd"", ""0\\t\xe6\x88\x91\xe5\xbe\x88\xe6\x84\xa4\xe6\x80\x92""]\nsamples_chinese_word = [""1\\t\xe9\x83\xbd\xe6\x8c\xba\xe5\xa5\xbd"", ""0\\t\xe6\x88\x91\xe5\xbe\x88\xe6\x84\xa4\xe6\x80\x92""]\n\nsamples_dict = {""english"": samples_english,\n                ""split_by_line_mark"": samples_split_line_mark,\n                ""split_by_space"": samples_split_by_space,\n                ""split_by_char"": samples_split_by_char,\n                ""chinese_word"": samples_chinese_word}\n\ntext_vocab_english = [""<unk>\\t0"", ""</s>\\t1"", ""all\\t3"", ""is\\t4"",\n                      ""well\\t5"", ""i\\t6"", ""am\\t7"", ""very\\t8""]\ntext_vocab_split_line_mark = [""<unk>\\t0"", ""</s>\\t1"", ""\xe9\x83\xbd\\t2"", ""\xe6\x8c\xba\xe5\xa5\xbd\\t3"",\n                              ""\xe6\x88\x91\\t4"", ""\xe5\xbe\x88\\t5"", ""|\\t6"", ""\xe6\x98\xaf\xe7\x9a\x84\\t7"",\n                              ""\xe5\x91\x80\\t8"", ""\xe8\xb6\x85\xe7\xba\xa7\\t9"", ""\xe7\x94\x9f\xe6\xb0\x94\\t10""]\ntext_vocab_split_by_space = [""<unk>\\t0"", ""</s>\\t1"", ""\xe9\x83\xbd\\t2"", ""\xe6\x8c\xba\xe5\xa5\xbd\\t3"",\n                             ""\xe6\x88\x91\\t4"", ""\xe5\xbe\x88\\t5""]\ntext_vocab_split_by_char = [""<unk>\\t0"", ""</s>\\t1"", ""\xe9\x83\xbd\\t2"", ""\xe6\x8c\xba\\t3"",\n                            ""\xe5\xa5\xbd\\t4"", ""\xe6\x88\x91\\t5"", ""\xe5\xbe\x88\\t6"",""\xe6\x84\xa4\\t7"",""\xe6\x80\x92\\t8""]\ntext_vocab_chinese_word = [""<unk>\\t0"", ""</s>\\t1"", ""\xe9\x83\xbd\\t2"", ""\xe6\x8c\xba\xe5\xa5\xbd\\t3"",\n                             ""\xe6\x88\x91\\t4"", ""\xe5\xbe\x88\\t5""]\ntext_vocab_dict = {""english"": text_vocab_english,\n                   ""split_by_line_mark"": text_vocab_split_line_mark,\n                   ""split_by_space"": text_vocab_split_by_space,\n                   ""split_by_char"": text_vocab_split_by_char,\n                   ""chinese_word"": text_vocab_chinese_word}\n\n\ndef mock_text_class_data(train_file, dev_file, test_file, text_vocab_file, data_type):\n  samples = samples_dict[data_type]\n  logging.info(""Generate mock data: {}"".format(train_file))\n  mock_a_text_file(samples, 300, train_file)\n  logging.info(""Generate mock data: {}"".format(dev_file))\n  mock_a_text_file(samples, 100, dev_file)\n  logging.info(""Generate mock data: {}"".format(test_file))\n  mock_a_text_file(samples, 100, test_file)\n  text_vocab_list = text_vocab_dict[data_type]\n  logging.info(""Generate text vocab file: {}"".format(text_vocab_file))\n  save_a_vocab_file(text_vocab_file, text_vocab_list)\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n\n  if len(sys.argv) != 5:\n    logging.error(""Usage python {} train_file dev_file test_file text_vocab_file"".format(sys.argv[0]))\n    sys.exit(-1)\n\n  for data_type in samples_dict.keys():\n    train_file = sys.argv[1].replace(""txt"", """") + data_type + "".txt""\n    dev_file = sys.argv[2].replace(""txt"", """") + data_type + "".txt""\n    test_file = sys.argv[3].replace(""txt"", """") + data_type + "".txt""\n    text_vocab_file = sys.argv[4].replace(""txt"", """") + data_type + "".txt""\n\n    mock_text_class_data(train_file, dev_file, test_file, text_vocab_file, data_type)\n'"
egs/mock_text_match_data/text_match/v1/local/generate_mock_data.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport sys\nfrom absl import logging\nfrom delta.data.utils.test_utils import mock_a_text_file\nfrom delta.data.utils.test_utils import save_a_vocab_file\n\n# samples with label\nsamples = [""0\\tHow should I approach forgiveness?\\tI got chickenpox as a child."",\n           ""1\\tI love china\xe3\x80\x82\\tI love china very much\xe3\x80\x82""]\ntext_vocab_list = [""<unk>\\t0"", ""</s>\\t1"", ""how\\t2"", ""should\\t3"",\n                   ""i\\t4"", ""approach\\t5"", ""forgiveness\\t6"", ""got\\t7"",\n                   ""chickenpox\\t8"", ""as\\t9"", ""a\\t10"",\n                   ""child\\t11"", ""love\\t12"", ""china\\t13"",\n                   ""very\\t14"", ""much\\t15""]\n\n\ndef mock_text_class_data(train_file, dev_file, test_file, text_vocab_file):\n  logging.info(""Generate mock data: {}"".format(train_file))\n  mock_a_text_file(samples, 300, train_file)\n  logging.info(""Generate mock data: {}"".format(dev_file))\n  mock_a_text_file(samples, 100, dev_file)\n  logging.info(""Generate mock data: {}"".format(test_file))\n  mock_a_text_file(samples, 100, test_file)\n  logging.info(""Generate text vocab file: {}"".format(text_vocab_file))\n  save_a_vocab_file(text_vocab_file, text_vocab_list)\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n\n  if len(sys.argv) != 5:\n    logging.error(""Usage python {} train_file dev_file test_file text_vocab_file"".format(sys.argv[0]))\n    sys.exit(-1)\n\n  train_file = sys.argv[1]\n  dev_file = sys.argv[2]\n  test_file = sys.argv[3]\n  text_vocab_file = sys.argv[4]\n\n  mock_text_class_data(train_file, dev_file, test_file, text_vocab_file)\n'"
egs/mock_text_nlu_joint_data/nlu-joint/v1/local/generate_mock_data.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport sys\nfrom absl import logging\nfrom delta.data.utils.test_utils import mock_a_text_file\nfrom delta.data.utils.test_utils import save_a_vocab_file\n\n# samples with label\nsamples = [""0\\tO O O O\\tmy feeling is low"",\n           ""1\\tO O O O B-ORG\\ti am happy in the kfc""]\n\ntext_vocab_list = [""<unk>\\t0"", ""</s>\\t1"", ""i\\t2"", ""am\\t3"", ""kfc\\t4"", ""my\\t5"",\n                   ""feeling\\t6"", ""happy\\t7"", ""is\\t8"", ""low\\t9"", ""in\\t10"", ""the\\t11""]\n\n\ndef mock_text_class_data(train_file, dev_file, test_file, text_vocab_file):\n  logging.info(""Generate mock data: {}"".format(train_file))\n  mock_a_text_file(samples, 300, train_file)\n  logging.info(""Generate mock data: {}"".format(dev_file))\n  mock_a_text_file(samples, 100, dev_file)\n  logging.info(""Generate mock data: {}"".format(test_file))\n  mock_a_text_file(samples, 100, test_file)\n  logging.info(""Generate text vocab file: {}"".format(text_vocab_file))\n  save_a_vocab_file(text_vocab_file, text_vocab_list)\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n\n  if len(sys.argv) != 5:\n    logging.error(""Usage python {} train_file dev_file test_file text_vocab_file"".format(sys.argv[0]))\n    sys.exit(-1)\n\n  train_file = sys.argv[1]\n  dev_file = sys.argv[2]\n  test_file = sys.argv[3]\n  text_vocab_file = sys.argv[4]\n\n  mock_text_class_data(train_file, dev_file, test_file, text_vocab_file)\n'"
egs/mock_text_seq2seq_data/seq2seq/v1/local/generate_mock_data.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport os\nimport sys\nfrom absl import logging\nfrom delta.data.utils.test_utils import mock_a_text_file\n\n# samples with label\nsamples = ["" a shooting at a bar popular with expatriates in mali on saturday killed ""\n           ""five people \\t killed five people"",\n           ""a pennsylvania community is pulling together to search for an eighth-grade ""\n           ""student who has been missing since wednesday\\tsearch for missing student""]\n\n\ndef mock_text_class_data(train_file, dev_file, test_file):\n  logging.info(""Generate mock data: {}"".format(train_file))\n  mock_a_text_file(samples, 300, train_file)\n  split_file(train_file)\n  logging.info(""Generate mock data: {}"".format(dev_file))\n  mock_a_text_file(samples, 100, dev_file)\n  split_file(dev_file)\n  logging.info(""Generate mock data: {}"".format(test_file))\n  mock_a_text_file(samples, 100, test_file)\n  split_file(test_file)\n\n\ndef split_file(ori_file):\n  src_file = ori_file + \'.src\'\n  tgt_file = ori_file + \'.tgt\'\n  with open(ori_file, \'r\', encoding=\'utf8\') as f:\n    lines = f.readlines()\n  src, tgt = zip(*[sent.split(\'\\t\') for sent in lines])\n  with open(src_file, \'w\', encoding=\'utf8\') as f:\n    for src_sent in src:\n      f.write(src_sent+\'\\n\')\n  with open(tgt_file, \'w\', encoding=\'utf8\') as f:\n    for tgt_sent in tgt:\n      f.write(tgt_sent)\n  os.remove(ori_file)\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n\n  if len(sys.argv) != 4:\n    logging.error(""Usage python {} train_file, dev_file, test_file"".format(sys.argv[0]))\n    sys.exit(-1)\n\n  train_file = sys.argv[1]\n  dev_file = sys.argv[2]\n  test_file = sys.argv[3]\n\n  mock_text_class_data(train_file, dev_file, test_file)\n'"
egs/mock_text_seq_label_data/seq-label/v1/local/generate_mock_data.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport sys\nfrom absl import logging\nfrom delta.data.utils.test_utils import mock_a_text_file\nfrom delta.data.utils.test_utils import save_a_vocab_file\n\n# samples with label\nsamples = [""O O O O\\ti feel good ."",\n           ""O O B-ORG O O O O O O\\tby stumps kent had reached 108 for three .""]\ntext_vocab_list = [""<unk>\\t0"", ""</s>\\t1"", ""i\\t2"", ""feel\\t3"", ""good\\t4"", "".\\t5"",\n                   ""by\\t6"", ""stumps\\t7"", ""kent\\t8"", ""had\\t9"", ""reached\\t10"", ""108\\t11"", ""for\\t12"", ""three\\t13""]\nlabel_vocab_list = [""O\\t0"", ""B-PER\\t1"", ""I-PER\\t2"", ""B-LOC\\t3"", ""I-LOC\\t4"",\n                    ""B-ORG\\t5"", ""I-ORG\\t6"", ""B-MISC\\t7"", ""I-MISC\\t8""]\n\n\ndef mock_text_class_data(train_file, dev_file, test_file, text_vocab_file, label_vocab_file):\n  logging.info(""Generate mock data: {}"".format(train_file))\n  mock_a_text_file(samples, 300, train_file)\n  logging.info(""Generate mock data: {}"".format(dev_file))\n  mock_a_text_file(samples, 100, dev_file)\n  logging.info(""Generate mock data: {}"".format(test_file))\n  mock_a_text_file(samples, 100, test_file)\n  logging.info(""Generate text vocab file: {}"".format(text_vocab_file))\n  save_a_vocab_file(text_vocab_file, text_vocab_list)\n  logging.info(""Generate label vocab file: {}"".format(label_vocab_file))\n  save_a_vocab_file(label_vocab_file, label_vocab_list)\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n\n  if len(sys.argv) != 6:\n    logging.error(""Usage python {} train_file dev_file test_file text_vocab_file"".format(sys.argv[0]))\n    sys.exit(-1)\n\n  train_file = sys.argv[1]\n  dev_file = sys.argv[2]\n  test_file = sys.argv[3]\n  text_vocab_file = sys.argv[4]\n  label_vocab_file = sys.argv[5]\n\n\n  mock_text_class_data(train_file, dev_file, test_file, text_vocab_file, label_vocab_file)\n'"
egs/msra_ner/seq_label/v1/local/change_data_format.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport sys\nfrom absl import logging\n\n\ndef change_data_format(files):\n  for data_file_in in files:\n    if data_file_in == sys.argv[3]:\n      logging.info(""Change data format: {}"".format(data_file_in))\n      data_file_out = data_file_in.replace("".in"", "".out"")\n      with open(data_file_out, ""w"", encoding=""utf-8"") as output_file:\n        with open(data_file_in, ""r"", encoding=""utf-8"") as file_input:\n          for line in file_input.readlines():\n            word = list(line.strip())\n            if len(line.strip()) != 0:\n              output_file.write(\' \'.join(word) + ""\\n"")\n      return\n\n    logging.info(""Change data format: {}"".format(data_file_in))\n    data_file_out = data_file_in.replace("".in"", "".out"")\n    words, labels = [], []\n    with open(data_file_out, ""w"", encoding=""utf-8"") as output_file:\n      with open(data_file_in, ""r"", encoding=""utf-8"") as file_input:\n        for line in file_input.readlines():\n          word = line.strip().split(\'\\t\')[0]\n          label = line.strip().split(\'\\t\')[-1]\n          # here we dont do ""DOCSTART"" check\n          if len(line.strip()) == 0:\n            l = [label for label in labels if len(label) > 0]\n            w = [word for word in words if len(word) > 0]\n            assert len(l) == len(w)\n            l, w = \' \'.join(l), \' \'.join(w)\n            output_file.write(l + ""\\t"" + w + ""\\n"")\n            words, labels = [], []\n          words.append(word)\n          labels.append(label)\n    logging.info(""Change data done: {}"".format(data_file_out))\n\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n\n  if len(sys.argv) != 4:\n    logging.error(""Usage python {} train_file, dev_file, test_file"".format(sys.argv[0]))\n    sys.exit(-1)\n\n  train_file = sys.argv[1]\n  dev_file = sys.argv[2]\n  test_file = sys.argv[3]\n  files = [train_file, dev_file, test_file]\n\n  change_data_format(files)\n'"
egs/quora_qp/match/v1/local/generate_standard_format.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport csv\nimport sys\nfrom absl import logging\n\n\ndef generate_standard_format(input_file,output_file):\n  with open(input_file, encoding=""utf-8"") as csv_file, \\\n    open(output_file, ""w"", encoding=""utf-8"") as out_file:\n    csv_reader = csv.reader(csv_file,delimiter=\'\\t\')\n    csv_reader=list(csv_reader)[1:]\n    for row in csv_reader:\n      if len(row) < 6:\n        continue\n      label = row[5]\n      id=row[0]\n      q1=row[3]\n      q2=row[4]\n      out_file.write(label + ""\\t"" + q1+""\\t""+q2+ ""\\n"")\nif __name__==""__main__"":\n  logging.set_verbosity(logging.INFO)\n\n  if len(sys.argv) != 3:\n    # [\'generate_standard_format.py\', $data/QQP/original/quora_duplicate_questions.tsv ,$data/quora_stand.txt]\n    logging.error(""Usage {} input_file output_file"".format(sys.argv[0]))\n    sys.exit()\n  input_file = sys.argv[1]\n  output_file = sys.argv[2]\n  logging.info(""Save file to {}"".format(output_file))\n  generate_standard_format(input_file,output_file)\n'"
egs/quora_qp/match/v1/local/load_data.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Quora Question Pairs data loader.""""""\n""python local/load_data.py ""\n\nimport typing\nfrom pathlib import Path\n\nimport keras\nimport pandas as pd\nimport csv\nimport sys\nfrom absl import logging\n\n\ndef load_data(url,filepath,stage=\'train\'):\n    """"""\n    Load QuoraQP data.\n\n    :param path: `None` for download from quora, specific path for\n        downloaded data.\n    :param stage: One of `train`, `dev`, and `test`.\n    :param task: Could be one of `ranking`, `classification` or a\n        :class:`matchzoo.engine.BaseTask` instance.\n    :param return_classes: Whether return classes for classification task.\n    :return: A DataPack if `ranking`, a tuple of (DataPack, classes) if\n        `classification`.\n    """"""\n\n    data_root = _download_data(url,filepath)\n    file_path = data_root.joinpath(""{}.tsv"".format(stage))\n  #  data_pack = _read_data(file_path, stage)\n\n\ndef _download_data(url,filepath):\n    ref_path = keras.utils.data_utils.get_file(\n        \'quora_qp\', url, extract=True,\n        cache_dir=filepath,\n        cache_subdir=\'quora_qp\'\n    )\n    return Path(ref_path).parent.joinpath(\'QQP\')\n\n\nif __name__==\'__main__\':\n  if sys.argv != 3:\n    logging.error(""Usage {} input_file output_file"".format(sys.argv[0]))\n    sys.exit()\n  path = Path(sys.argv[1])\n  if path.exist():\n    url=sys.argv[1]\n    load_data(url,file_path=sys.argv[2])\n  else:\n    logging.error(""Path {} is not exit"".format(sys.argv[1]))\n\n'"
egs/snli/match/v1/local/generate_standard_format.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport json\nimport sys\nfrom absl import logging\n\n\ndef generate_standard_format(input_file,output_file):\n  label_dic={""neutral"": ""2"", ""contradiction"": ""0"", ""entailment"": ""1""}\n  with open(input_file, encoding=""utf-8"") as json_file, \\\n    open(output_file, ""w"", encoding=""utf-8"") as out_file:\n    text_reader = json_file.readlines()\n    for line in text_reader:\n      line_dic=json.loads(line)\n      if ""gold_label"" not in line_dic or ""sentence1"" not in line_dic or ""sentence2"" not in line_dic:\n\n        continue\n      if line_dic[""gold_label""]==""-"":\n        continue\n      label = label_dic[line_dic[""gold_label""]]\n      sentence1 = line_dic[""sentence1""]\n      sentence2 = line_dic[""sentence2""]\n      out_file.write(label + ""\\t"" + sentence1 + ""\\t"" + sentence2 + ""\\n"")\nif __name__==""__main__"":\n  logging.set_verbosity(logging.INFO)\n  if len(sys.argv) != 3:\n    # [\'generate_standard_format.py\', $data/QQP/original/quora_duplicate_questions.tsv ,$data/quora_stand.txt]\n    logging.error(""Usage {} input_file output_file"".format(sys.argv[0]))\n    sys.exit()\n  input_file = sys.argv[1]\n  output_file = sys.argv[2]\n  logging.info(""Save file to {}"".format(output_file))\n  generate_standard_format(input_file,output_file)\n'"
egs/trec/text_cls/v1/local/change_data_format.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport sys\nfrom absl import logging\n\n\ndef to_standard_format(input_file, output_file):\n  logging.info(""Save file to {}"".format(output_file))\n\n  max_seq = 0\n  with open(input_file, encoding=""ISO-8859-1"") as in_file, \\\n    open(output_file, ""w"", encoding=""utf-8"") as out_file:\n    for row in in_file.readlines():\n      parts = row.strip().split("" "")\n      label = parts[0].split("":"")[0]\n      text_len = len(parts[1:])\n      if text_len > max_seq:\n        max_seq = text_len\n      text = "" "".join(parts[1:])\n      out_file.write(label + ""\\t"" + text + ""\\n"")\n  logging.info(""max seq len is {}"".format(max_seq))\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  if len(sys.argv) != 3:\n    logging.error(""Usage {} input_file output_file"".format(sys.argv[0]))\n    sys.exit(-1)\n\n  input_file = sys.argv[1]\n  output_file = sys.argv[2]\n  to_standard_format(input_file, output_file)\n'"
egs/yahoo_answer/text_cls/v1/local/generate_standard_format.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport csv\nimport sys\nfrom absl import logging\n\n\ndef to_standard_format(input_file, output_file):\n  logging.info(""Save file to {}"".format(output_file))\n\n  with open(input_file, encoding=""utf-8"") as csv_file, \\\n    open(output_file, ""w"", encoding=""utf-8"") as out_file:\n    csv_reader = csv.reader(csv_file)\n    for row in csv_reader:\n      if len(row) < 4:\n        continue\n      label = row[0]\n      text = "" "".join(row[1:])\n      out_file.write(label + ""\\t"" + text + ""\\n"")\n\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  if len(sys.argv) != 3:\n    logging.error(""Usage {} input_file output_file"".format(sys.argv[0]))\n    sys.exit(-1)\n\n  input_file = sys.argv[1]\n  output_file = sys.argv[2]\n  to_standard_format(input_file, output_file)\n'"
deltann/infer/python/delta_infer/subgraphs/common/__init__.py,0,b'\nfrom .generator import *\nfrom .summarize_graph import *\n'
deltann/infer/python/delta_infer/subgraphs/common/generator.py,2,"b'from __future__ import absolute_import, division, print_function\n\nimport os\nimport sys\nimport tensorflow as tf\nfrom tensorflow.python.framework import graph_util\nfrom tensorflow.compat.v1 import Session\n\nfrom .summarize_graph import GraphSummary\n\n__all__ = [""RegistPattern""]\n\nclass RegistPattern(object):\n    """""" Rgist Pattern Decorator""""""\n    # a pattern map form name to list of GraphDef\n    # note that a key name maybe map to multi graphdefs.\n    patterns = {}\n    def __init__(self, name=None):\n        self.name = name\n        if name not in RegistPattern.patterns:\n            RegistPattern.patterns[name] = []\n\n    @staticmethod\n    def get_patterns(name):\n        return RegistPattern.patterns[name]\n\n    @staticmethod\n    def Patterns():\n        return RegistPattern.patterns\n\n    def __call__(self, func):\n        def local(*args, **kwargs):\n            with Session() as sess:\n                ret = func(*args, **kwargs)\n                #sess.run(tf.compat.v1.global_variables_initializer())\n                tf.compat.v1.global_variables_initializer().run()\n                graph_summary = GraphSummary(graph_def=sess.graph_def)\n                graph_summary.Summary()\n                graph_def = graph_util.\\\n                        convert_variables_to_constants(sess, sess.graph_def, graph_summary[""outputs""])\n                RegistPattern.patterns[self.name].append(graph_def)\n            return ret\n        return local\n'"
deltann/infer/python/delta_infer/subgraphs/common/summarize_graph.py,4,"b'from __future__ import absolute_import, division, print_function\nimport tensorflow as tf\n\nimport os\nimport sys\nimport numpy as np\nfrom tensorflow.compat.v1 import GraphDef\n\n__all__ = [""GraphSummary""]\n\nclass GraphSummary(object):\n    """""" Graph Def summary """"""\n    def __init__(self, graph_pb_path=None, graph_def=None):\n        if graph_pb_path is not None:\n            with tf.compat.v1.gfile.GFile(graph_pb_path, \'rb\') as f:\n                self.graph = tf.compat.v1.GraphDef()\n                self.graph.ParseFromString(f.read())\n        else:\n            self.graph = graph_def\n        self.summray_dict = {}\n\n    @property\n    def graph(self):\n        return self.graph_def\n\n    @graph.setter\n    def graph(self, graph):\n        if graph is not None:\n            if isinstance(graph, GraphDef):\n                self.graph_def = graph\n            else:\n                raise ValueError(""graph({}) should be type of GraphDef."".format(type(graph)))\n\n\n    def PrintNodeInfo(self, node):\n        get_real_shape = lambda dims: [ dim.size for dim in dims]\n        if ""shape"" in node.attr:\n            shape = get_real_shape(node.attr[""shape""].shape.dim)\n        if ""dtype"" in node.attr:\n            dtype = tf.DType(node.attr[""dtype""].type)\n        print(""    <<=== (name={}, type={}, shape={} )"".format(node.name, dtype, shape))\n        return (node.name, dtype, shape)\n\n    def MapNodesToOutputs(self, output_map):\n        for node in self.graph.node:\n            for input in node.input:\n                output_map[input.split("":"")[0]] = node\n\n    def __getitem__(self, key):\n        return self.summray_dict[key]\n\n    def Summary(self):\n        placeholders = []\n        variables = []\n        print(""Graph Version: {}.{}"".format(self.graph.versions.producer, self.graph.versions.min_consumer))\n        for node in self.graph.node:\n            if node.op == ""Placeholder"":\n                placeholders.append(node)\n            if node.op == ""Variable"" or node.op == ""VariableV2"":\n                variables.append(node)\n        if len(placeholders) == 0:\n            print(""No inputs spotted"")\n        else:\n            print(""Found {} possible inputs: "".format(len(placeholders)))\n            self.summray_dict[""inputs""] = []\n            for node in placeholders:\n                in_info = self.PrintNodeInfo(node)\n                self.summray_dict[""inputs""].append(in_info)\n        if len(variables) == 0:\n            pass\n            print(""No variables spotted"")\n        else:\n            print(""Found {} variables"".format(len(variables)))\n            self.summray_dict[""variables""] = []\n            for node in variables:\n                var_info = self.PrintNodeInfo(node)\n                self.summray_dict[""variables""].append(var_info)\n\n        output_map = {}\n        self.MapNodesToOutputs(output_map)\n        outputs = []\n        unlikely_output_types = [""Const"", ""Assign"", ""NoOp"", ""Placeholder"", ""VarIsInitializedOp""]\n        for node in self.graph.node:\n            if (node.name not in output_map) and (node.op not in unlikely_output_types):\n                outputs.append(node)\n        if len(outputs) == 0:\n            print(""No outputs spotted"")\n        else:\n            print(""Found {} possible outputs:"".format(len(outputs)))\n            self.summray_dict[""outputs""] = []\n            for node in outputs:\n                print(""    ===>> (name={}, op={})"".format(node.name, node.op))\n                self.summray_dict[""outputs""].append(node.name)\n\n        const_parameter_count = 0\n        variable_parameter_count = 0\n        control_edge_count = 0\n        device_counts = {}\n        for node in self.graph.node:\n            for input in node.input:\n                if input[0] == ""^"":\n                    control_edge_count+=1\n            if len(node.device)!=0:\n                device_counts[node.device] = 0 if node.device not in device_counts else device_counts[node.device]+1\n            if node.op in [""Const"", ""Variable"", ""VariableV2""]:\n                if ""value"" in node.attr:\n                    tensor = tf.io.parse_tensor(node.attr[""value""].tensor.SerializeToString(), tf.DType(node.attr[""value""].tensor.dtype))\n                    num_elements = tensor.shape.num_elements()\n                    if node.op == ""Const"":\n                        const_parameter_count += num_elements if num_elements is not None else 0\n                    else:\n                        variable_parameter_count += num_elements\n        self.summray_dict[""const_parameter_count""] = const_parameter_count\n        self.summray_dict[""variable_parameter_count""] = variable_parameter_count\n        self.summray_dict[""control_edge_count""] = control_edge_count\n        print(""Found {} const parameters, {} variable parameters, and {} control_edges"".format(\n            const_parameter_count, variable_parameter_count, control_edge_count))\n        if len(device_counts.keys()) != 0:\n            str_dev_info = """"\n            for device_info in device_counts:\n                str_dev_info += ""%s nodes assigned to device %s, "" % (str(device_info.second), str(device_info.first))\n            print(str_dev_info)\n\n        #op_counts = {}\n        #for node in self.graph.node:\n        #    op_counts[node.op] = 1 if node.op not in op_counts else op_counts[node.op]+1\n        #for function in self.graph.library.function:\n        #    for node in function.node_def.node:\n        #        op_counts[node.op] = 1 if node.op not in op_counts else op_counts[node.op]+1\n        #print(""Op types used: "")\n        #self.summray_dict[""OpCount""] = {}\n        #for op in op_counts:\n        #    print(""@ {} : {}"".format(op, op_counts[op]))\n        #self.summray_dict[""OpCount""] = op_counts[op]\n'"
deltann/infer/python/delta_infer/subgraphs/transformer/__init__.py,0,b'from .transformer import *\n'
deltann/infer/python/delta_infer/subgraphs/transformer/model.py,81,"b'# coding=utf-8\n""""""The main BERT model and related functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport copy\nimport json\nimport math\nimport re\nimport numpy as np\nimport six\nimport tensorflow as tf\n\n\nclass BertConfig(object):\n  """"""Configuration for `BertModel`.""""""\n\n  def __init__(self,\n               vocab_size,\n               hidden_size=768,\n               num_hidden_layers=12,\n               num_attention_heads=12,\n               intermediate_size=3072,\n               hidden_act=""gelu"",\n               hidden_dropout_prob=0.1,\n               attention_probs_dropout_prob=0.1,\n               max_position_embeddings=512,\n               type_vocab_size=16,\n               initializer_range=0.02):\n    """"""Constructs BertConfig.\n\n    Args:\n      vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n      hidden_size: Size of the encoder layers and the pooler layer.\n      num_hidden_layers: Number of hidden layers in the Transformer encoder.\n      num_attention_heads: Number of attention heads for each attention layer in\n        the Transformer encoder.\n      intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)\n        layer in the Transformer encoder.\n      hidden_act: The non-linear activation function (function or string) in the\n        encoder and pooler.\n      hidden_dropout_prob: The dropout probability for all fully connected\n        layers in the embeddings, encoder, and pooler.\n      attention_probs_dropout_prob: The dropout ratio for the attention\n        probabilities.\n      max_position_embeddings: The maximum sequence length that this model might\n        ever be used with. Typically set this to something large just in case\n        (e.g., 512 or 1024 or 2048).\n      type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n        `BertModel`.\n      initializer_range: The stdev of the truncated_normal_initializer for\n        initializing all weight matrices.\n    """"""\n    self.vocab_size = vocab_size\n    self.hidden_size = hidden_size\n    self.num_hidden_layers = num_hidden_layers\n    self.num_attention_heads = num_attention_heads\n    self.hidden_act = hidden_act\n    self.intermediate_size = intermediate_size\n    self.hidden_dropout_prob = hidden_dropout_prob\n    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n    self.max_position_embeddings = max_position_embeddings\n    self.type_vocab_size = type_vocab_size\n    self.initializer_range = initializer_range\n\n  @classmethod\n  def from_dict(cls, json_object):\n    """"""Constructs a `BertConfig` from a Python dictionary of parameters.""""""\n    config = BertConfig(vocab_size=None)\n    for (key, value) in six.iteritems(json_object):\n      config.__dict__[key] = value\n    return config\n\n  @classmethod\n  def from_json_file(cls, json_file):\n    """"""Constructs a `BertConfig` from a json file of parameters.""""""\n    with tf.gfile.GFile(json_file, ""r"") as reader:\n      text = reader.read()\n    return cls.from_dict(json.loads(text))\n\n  def to_dict(self):\n    """"""Serializes this instance to a Python dictionary.""""""\n    output = copy.deepcopy(self.__dict__)\n    return output\n\n  def to_json_string(self):\n    """"""Serializes this instance to a JSON string.""""""\n    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n\nclass BertModel(object):\n  """"""BERT model (""Bidirectional Encoder Representations from Transformers"").\n\n  Example usage:\n\n  ```python\n  # Already been converted into WordPiece token ids\n  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n\n  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n\n  model = modeling.BertModel(config=config, is_training=True,\n    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n\n  label_embeddings = tf.get_variable(...)\n  pooled_output = model.get_pooled_output()\n  logits = tf.matmul(pooled_output, label_embeddings)\n  ...\n  ```\n  """"""\n\n  def __init__(self,\n               config,\n               is_training,\n               input_ids,\n               input_mask=None,\n               token_type_ids=None,\n               use_one_hot_embeddings=False,\n               scope=None):\n    """"""Constructor for BertModel.\n\n    Args:\n      config: `BertConfig` instance.\n      is_training: bool. true for training model, false for eval model. Controls\n        whether dropout will be applied.\n      input_ids: int32 Tensor of shape [batch_size, seq_length].\n      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n        embeddings or tf.embedding_lookup() for the word embeddings.\n      scope: (optional) variable scope. Defaults to ""bert"".\n\n    Raises:\n      ValueError: The config is invalid or one of the input tensor shapes\n        is invalid.\n    """"""\n    config = copy.deepcopy(config)\n    if not is_training:\n      config.hidden_dropout_prob = 0.0\n      config.attention_probs_dropout_prob = 0.0\n\n    input_shape = get_shape_list(input_ids, expected_rank=2)\n    batch_size = input_shape[0]\n    seq_length = input_shape[1]\n\n    if input_mask is None:\n      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    if token_type_ids is None:\n      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    with tf.variable_scope(scope, default_name=""bert""):\n      with tf.variable_scope(""embeddings""):\n        # Perform embedding lookup on the word ids.\n        (self.embedding_output, self.embedding_table) = embedding_lookup(\n            input_ids=input_ids,\n            vocab_size=config.vocab_size,\n            embedding_size=config.hidden_size,\n            initializer_range=config.initializer_range,\n            word_embedding_name=""word_embeddings"",\n            use_one_hot_embeddings=use_one_hot_embeddings)\n\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.embedding_output,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=config.type_vocab_size,\n            token_type_embedding_name=""token_type_embeddings"",\n            use_position_embeddings=True,\n            position_embedding_name=""position_embeddings"",\n            initializer_range=config.initializer_range,\n            max_position_embeddings=config.max_position_embeddings,\n            dropout_prob=config.hidden_dropout_prob)\n\n      with tf.variable_scope(""encoder""):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask = create_attention_mask_from_input_mask(\n            input_ids, input_mask)\n\n        # Run the stacked transformer.\n        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n        self.all_encoder_layers = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=config.hidden_size,\n            num_hidden_layers=config.num_hidden_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            intermediate_act_fn=get_activation(config.hidden_act),\n            hidden_dropout_prob=config.hidden_dropout_prob,\n            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n            initializer_range=config.initializer_range,\n            do_return_all_layers=True)\n\n      self.sequence_output = self.all_encoder_layers[-1]\n      # The ""pooler"" converts the encoded sequence tensor of shape\n      # [batch_size, seq_length, hidden_size] to a tensor of shape\n      # [batch_size, hidden_size]. This is necessary for segment-level\n      # (or segment-pair-level) classification tasks where we need a fixed\n      # dimensional representation of the segment.\n      with tf.variable_scope(""pooler""):\n        # We ""pool"" the model by simply taking the hidden state corresponding\n        # to the first token. We assume that this has been pre-trained\n        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n        self.pooled_output = tf.layers.dense(\n            first_token_tensor,\n            config.hidden_size,\n            activation=tf.tanh,\n            kernel_initializer=create_initializer(config.initializer_range))\n\n  def get_pooled_output(self):\n    return self.pooled_output\n\n  def get_sequence_output(self):\n    """"""Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    """"""\n    return self.sequence_output\n\n  def get_all_encoder_layers(self):\n    return self.all_encoder_layers\n\n  def get_embedding_output(self):\n    """"""Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    """"""\n    return self.embedding_output\n\n  def get_embedding_table(self):\n    return self.embedding_table\n\n\ndef gelu(x):\n  """"""Gaussian Error Linear Unit.\n\n  This is a smoother version of the RELU.\n  Original paper: https://arxiv.org/abs/1606.08415\n  Args:\n    x: float Tensor to perform activation.\n\n  Returns:\n    `x` with the GELU activation applied.\n  """"""\n  cdf = 0.5 * (1.0 + tf.tanh(\n      (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n  return x * cdf\n\n\ndef get_activation(activation_string):\n  """"""Maps a string to a Python function, e.g., ""relu"" => `tf.nn.relu`.\n\n  Args:\n    activation_string: String name of the activation function.\n\n  Returns:\n    A Python function corresponding to the activation function. If\n    `activation_string` is None, empty, or ""linear"", this will return None.\n    If `activation_string` is not a string, it will return `activation_string`.\n\n  Raises:\n    ValueError: The `activation_string` does not correspond to a known\n      activation.\n  """"""\n\n  # We assume that anything that""s not a string is already an activation\n  # function, so we just return it.\n  if not isinstance(activation_string, six.string_types):\n    return activation_string\n\n  if not activation_string:\n    return None\n\n  act = activation_string.lower()\n  if act == ""linear"":\n    return None\n  elif act == ""relu"":\n    return tf.nn.relu\n  elif act == ""gelu"":\n    return gelu\n  elif act == ""tanh"":\n    return tf.tanh\n  else:\n    raise ValueError(""Unsupported activation: %s"" % act)\n\n\ndef get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n  """"""Compute the union of the current variables and checkpoint variables.""""""\n  assignment_map = {}\n  initialized_variable_names = {}\n\n  name_to_variable = collections.OrderedDict()\n  for var in tvars:\n    name = var.name\n    m = re.match(""^(.*):\\\\d+$"", name)\n    if m is not None:\n      name = m.group(1)\n    name_to_variable[name] = var\n\n  init_vars = tf.train.list_variables(init_checkpoint)\n\n  assignment_map = collections.OrderedDict()\n  for x in init_vars:\n    (name, var) = (x[0], x[1])\n    if name not in name_to_variable:\n      continue\n    assignment_map[name] = name\n    initialized_variable_names[name] = 1\n    initialized_variable_names[name + "":0""] = 1\n\n  return (assignment_map, initialized_variable_names)\n\n\ndef dropout(input_tensor, dropout_prob):\n  """"""Perform dropout.\n\n  Args:\n    input_tensor: float Tensor.\n    dropout_prob: Python float. The probability of dropping out a value (NOT of\n      *keeping* a dimension as in `tf.nn.dropout`).\n\n  Returns:\n    A version of `input_tensor` with dropout applied.\n  """"""\n  if dropout_prob is None or dropout_prob == 0.0:\n    return input_tensor\n\n  output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n  return output\n\n\ndef layer_norm(input_tensor, name=None):\n  """"""Run layer normalization on the last dimension of the tensor.""""""\n  return tf.contrib.layers.layer_norm(\n      inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)\n\n\ndef layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\n  """"""Runs layer normalization followed by dropout.""""""\n  output_tensor = layer_norm(input_tensor, name)\n  output_tensor = dropout(output_tensor, dropout_prob)\n  return output_tensor\n\n\ndef create_initializer(initializer_range=0.02):\n  """"""Creates a `truncated_normal_initializer` with the given range.""""""\n  return tf.truncated_normal_initializer(stddev=initializer_range)\n\n\ndef embedding_lookup(input_ids,\n                     vocab_size,\n                     embedding_size=128,\n                     initializer_range=0.02,\n                     word_embedding_name=""word_embeddings"",\n                     use_one_hot_embeddings=False):\n  """"""Looks up words embeddings for id tensor.\n\n  Args:\n    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n      ids.\n    vocab_size: int. Size of the embedding vocabulary.\n    embedding_size: int. Width of the word embeddings.\n    initializer_range: float. Embedding initialization range.\n    word_embedding_name: string. Name of the embedding table.\n    use_one_hot_embeddings: bool. If True, use one-hot method for word\n      embeddings. If False, use `tf.gather()`.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, embedding_size].\n  """"""\n  # This function assumes that the input is of shape [batch_size, seq_length,\n  # num_inputs].\n  #\n  # If the input is a 2D tensor of shape [batch_size, seq_length], we\n  # reshape to [batch_size, seq_length, 1].\n  if input_ids.shape.ndims == 2:\n    input_ids = tf.expand_dims(input_ids, axis=[-1])\n\n  embedding_table = tf.get_variable(\n      name=word_embedding_name,\n      shape=[vocab_size, embedding_size],\n      initializer=create_initializer(initializer_range))\n\n  flat_input_ids = tf.reshape(input_ids, [-1])\n  if use_one_hot_embeddings:\n    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n    output = tf.matmul(one_hot_input_ids, embedding_table)\n  else:\n    output = tf.gather(embedding_table, flat_input_ids)\n\n  input_shape = get_shape_list(input_ids)\n\n  output = tf.reshape(output,\n                      input_shape[0:-1] + [input_shape[-1] * embedding_size])\n  return (output, embedding_table)\n\n\ndef embedding_postprocessor(input_tensor,\n                            use_token_type=False,\n                            token_type_ids=None,\n                            token_type_vocab_size=16,\n                            token_type_embedding_name=""token_type_embeddings"",\n                            use_position_embeddings=True,\n                            position_embedding_name=""position_embeddings"",\n                            initializer_range=0.02,\n                            max_position_embeddings=512,\n                            dropout_prob=0.1):\n  """"""Performs various post-processing on a word embedding tensor.\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length,\n      embedding_size].\n    use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      Must be specified if `use_token_type` is True.\n    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.\n    token_type_embedding_name: string. The name of the embedding table variable\n      for token type ids.\n    use_position_embeddings: bool. Whether to add position embeddings for the\n      position of each token in the sequence.\n    position_embedding_name: string. The name of the embedding table variable\n      for positional embeddings.\n    initializer_range: float. Range of the weight initialization.\n    max_position_embeddings: int. Maximum sequence length that might ever be\n      used with this model. This can be longer than the sequence length of\n      input_tensor, but cannot be shorter.\n    dropout_prob: float. Dropout probability applied to the final output tensor.\n\n  Returns:\n    float tensor with same shape as `input_tensor`.\n\n  Raises:\n    ValueError: One of the tensor shapes or input values is invalid.\n  """"""\n  input_shape = get_shape_list(input_tensor, expected_rank=3)\n  batch_size = input_shape[0]\n  seq_length = input_shape[1]\n  width = input_shape[2]\n\n  output = input_tensor\n\n  if use_token_type:\n    if token_type_ids is None:\n      raise ValueError(""`token_type_ids` must be specified if""\n                       ""`use_token_type` is True."")\n    token_type_table = tf.get_variable(\n        name=token_type_embedding_name,\n        shape=[token_type_vocab_size, width],\n        initializer=create_initializer(initializer_range))\n    # This vocab will be small so we always do one-hot here, since it is always\n    # faster for a small vocabulary.\n    flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)\n    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n    token_type_embeddings = tf.reshape(token_type_embeddings,\n                                       [batch_size, seq_length, width])\n    output += token_type_embeddings\n\n  if use_position_embeddings:\n    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)\n    with tf.control_dependencies([assert_op]):\n      full_position_embeddings = tf.get_variable(\n          name=position_embedding_name,\n          shape=[max_position_embeddings, width],\n          initializer=create_initializer(initializer_range))\n      # Since the position embedding table is a learned variable, we create it\n      # using a (long) sequence length `max_position_embeddings`. The actual\n      # sequence length might be shorter than this, for faster training of\n      # tasks that do not have long sequences.\n      #\n      # So `full_position_embeddings` is effectively an embedding table\n      # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n      # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n      # perform a slice.\n      position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n                                     [seq_length, -1])\n      num_dims = len(output.shape.as_list())\n\n      # Only the last two dimensions are relevant (`seq_length` and `width`), so\n      # we broadcast among the first dimensions, which is typically just\n      # the batch size.\n      position_broadcast_shape = []\n      for _ in range(num_dims - 2):\n        position_broadcast_shape.append(1)\n      position_broadcast_shape.extend([seq_length, width])\n      position_embeddings = tf.reshape(position_embeddings,\n                                       position_broadcast_shape)\n      output += position_embeddings\n\n  output = layer_norm_and_dropout(output, dropout_prob)\n  return output\n\n\ndef create_attention_mask_from_input_mask(from_tensor, to_mask):\n  """"""Create 3D attention mask from a 2D tensor mask.\n\n  Args:\n    from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n    to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n  """"""\n  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n  batch_size = from_shape[0]\n  from_seq_length = from_shape[1]\n\n  to_shape = get_shape_list(to_mask, expected_rank=2)\n  to_seq_length = to_shape[1]\n\n  to_mask = tf.cast(\n      tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n  # We don\'t assume that `from_tensor` is a mask (although it could be). We\n  # don\'t actually care if we attend *from* padding tokens (only *to* padding)\n  # tokens so we create a tensor of all ones.\n  #\n  # `broadcast_ones` = [batch_size, from_seq_length, 1]\n  broadcast_ones = tf.ones(\n      shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n  # Here we broadcast along two dimensions to create the mask.\n  mask = broadcast_ones * to_mask\n\n  return mask\n\n\ndef attention_layer(from_tensor,\n                    to_tensor,\n                    attention_mask=None,\n                    num_attention_heads=1,\n                    size_per_head=512,\n                    query_act=None,\n                    key_act=None,\n                    value_act=None,\n                    attention_probs_dropout_prob=0.0,\n                    initializer_range=0.02,\n                    do_return_2d_tensor=False,\n                    batch_size=None,\n                    from_seq_length=None,\n                    to_seq_length=None):\n  """"""Performs multi-headed attention from `from_tensor` to `to_tensor`.\n\n  This is an implementation of multi-headed attention based on ""Attention\n  is all you Need"". If `from_tensor` and `to_tensor` are the same, then\n  this is self-attention. Each timestep in `from_tensor` attends to the\n  corresponding sequence in `to_tensor`, and returns a fixed-with vector.\n\n  This function first projects `from_tensor` into a ""query"" tensor and\n  `to_tensor` into ""key"" and ""value"" tensors. These are (effectively) a list\n  of tensors of length `num_attention_heads`, where each tensor is of shape\n  [batch_size, seq_length, size_per_head].\n\n  Then, the query and key tensors are dot-producted and scaled. These are\n  softmaxed to obtain attention probabilities. The value tensors are then\n  interpolated by these probabilities, then concatenated back to a single\n  tensor and returned.\n\n  In practice, the multi-headed attention are done with transposes and\n  reshapes rather than actual separate tensors.\n\n  Args:\n    from_tensor: float Tensor of shape [batch_size, from_seq_length,\n      from_width].\n    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n    attention_mask: (optional) int32 Tensor of shape [batch_size,\n      from_seq_length, to_seq_length]. The values should be 1 or 0. The\n      attention scores will effectively be set to -infinity for any positions in\n      the mask that are 0, and will be unchanged for positions that are 1.\n    num_attention_heads: int. Number of attention heads.\n    size_per_head: int. Size of each attention head.\n    query_act: (optional) Activation function for the query transform.\n    key_act: (optional) Activation function for the key transform.\n    value_act: (optional) Activation function for the value transform.\n    attention_probs_dropout_prob: (optional) float. Dropout probability of the\n      attention probabilities.\n    initializer_range: float. Range of the weight initializer.\n    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size\n      * from_seq_length, num_attention_heads * size_per_head]. If False, the\n      output will be of shape [batch_size, from_seq_length, num_attention_heads\n      * size_per_head].\n    batch_size: (Optional) int. If the input is 2D, this might be the batch size\n      of the 3D version of the `from_tensor` and `to_tensor`.\n    from_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `from_tensor`.\n    to_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `to_tensor`.\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length,\n      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is\n      true, this will be of shape [batch_size * from_seq_length,\n      num_attention_heads * size_per_head]).\n\n  Raises:\n    ValueError: Any of the arguments or tensor shapes are invalid.\n  """"""\n\n  def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width):\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n  to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])\n\n  if len(from_shape) != len(to_shape):\n    raise ValueError(\n        ""The rank of `from_tensor` must match the rank of `to_tensor`."")\n\n  if len(from_shape) == 3:\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n    to_seq_length = to_shape[1]\n  elif len(from_shape) == 2:\n    if (batch_size is None or from_seq_length is None or to_seq_length is None):\n      raise ValueError(\n          ""When passing in rank 2 tensors to attention_layer, the values ""\n          ""for `batch_size`, `from_seq_length`, and `to_seq_length` ""\n          ""must all be specified."")\n\n  # Scalar dimensions referenced here:\n  #   B = batch size (number of sequences)\n  #   F = `from_tensor` sequence length\n  #   T = `to_tensor` sequence length\n  #   N = `num_attention_heads`\n  #   H = `size_per_head`\n\n  from_tensor_2d = reshape_to_matrix(from_tensor)\n  to_tensor_2d = reshape_to_matrix(to_tensor)\n\n  # `query_layer` = [B*F, N*H]\n  query_layer = tf.layers.dense(\n      from_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=query_act,\n      name=""query"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  # `key_layer` = [B*T, N*H]\n  key_layer = tf.layers.dense(\n      to_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=key_act,\n      name=""key"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  # `value_layer` = [B*T, N*H]\n  value_layer = tf.layers.dense(\n      to_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=value_act,\n      name=""value"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  # `query_layer` = [B, N, F, H]\n  query_layer = transpose_for_scores(query_layer, batch_size,\n                                     num_attention_heads, from_seq_length,\n                                     size_per_head)\n\n  # `key_layer` = [B, N, T, H]\n  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,\n                                   to_seq_length, size_per_head)\n\n  # Take the dot product between ""query"" and ""key"" to get the raw\n  # attention scores.\n  # `attention_scores` = [B, N, F, T]\n  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n  attention_scores = tf.multiply(attention_scores,\n                                 1.0 / math.sqrt(float(size_per_head)))\n\n  if attention_mask is not None:\n    # `attention_mask` = [B, 1, F, T]\n    attention_mask = tf.expand_dims(attention_mask, axis=[1])\n\n    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n    # masked positions, this operation will create a tensor which is 0.0 for\n    # positions we want to attend and -10000.0 for masked positions.\n    adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n\n    # Since we are adding it to the raw scores before the softmax, this is\n    # effectively the same as removing these entirely.\n    attention_scores += adder\n\n  # Normalize the attention scores to probabilities.\n  # `attention_probs` = [B, N, F, T]\n  attention_probs = tf.nn.softmax(attention_scores)\n\n  # This is actually dropping out entire tokens to attend to, which might\n  # seem a bit unusual, but is taken from the original Transformer paper.\n  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)\n\n  # `value_layer` = [B, T, N, H]\n  value_layer = tf.reshape(\n      value_layer,\n      [batch_size, to_seq_length, num_attention_heads, size_per_head])\n\n  # `value_layer` = [B, N, T, H]\n  value_layer = tf.transpose(value_layer, [0, 2, 1, 3])\n\n  # `context_layer` = [B, N, F, H]\n  context_layer = tf.matmul(attention_probs, value_layer)\n\n  # `context_layer` = [B, F, N, H]\n  context_layer = tf.transpose(context_layer, [0, 2, 1, 3])\n\n  if do_return_2d_tensor:\n    # `context_layer` = [B*F, N*H]\n    context_layer = tf.reshape(\n        context_layer,\n        [batch_size * from_seq_length, num_attention_heads * size_per_head])\n  else:\n    # `context_layer` = [B, F, N*H]\n    context_layer = tf.reshape(\n        context_layer,\n        [batch_size, from_seq_length, num_attention_heads * size_per_head])\n\n  return context_layer\n\n\ndef transformer_model(input_tensor,\n                      attention_mask=None,\n                      hidden_size=768,\n                      num_hidden_layers=12,\n                      num_attention_heads=12,\n                      intermediate_size=3072,\n                      intermediate_act_fn=gelu,\n                      hidden_dropout_prob=0.1,\n                      attention_probs_dropout_prob=0.1,\n                      initializer_range=0.02,\n                      do_return_all_layers=False):\n  """"""Multi-headed, multi-layer Transformer from ""Attention is All You Need"".\n\n  This is almost an exact implementation of the original Transformer encoder.\n\n  See the original paper:\n  https://arxiv.org/abs/1706.03762\n\n  Also see:\n  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n      seq_length], with 1 for positions that can be attended to and 0 in\n      positions that should not be.\n    hidden_size: int. Hidden size of the Transformer.\n    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n    num_attention_heads: int. Number of attention heads in the Transformer.\n    intermediate_size: int. The size of the ""intermediate"" (a.k.a., feed\n      forward) layer.\n    intermediate_act_fn: function. The non-linear activation function to apply\n      to the output of the intermediate/feed-forward layer.\n    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n    attention_probs_dropout_prob: float. Dropout probability of the attention\n      probabilities.\n    initializer_range: float. Range of the initializer (stddev of truncated\n      normal).\n    do_return_all_layers: Whether to also return all layers or just the final\n      layer.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n    hidden layer of the Transformer.\n\n  Raises:\n    ValueError: A Tensor shape or parameter is invalid.\n  """"""\n  if hidden_size % num_attention_heads != 0:\n    raise ValueError(\n        ""The hidden size (%d) is not a multiple of the number of attention ""\n        ""heads (%d)"" % (hidden_size, num_attention_heads))\n\n  attention_head_size = int(hidden_size / num_attention_heads)\n  input_shape = get_shape_list(input_tensor, expected_rank=3)\n  batch_size = input_shape[0]\n  seq_length = input_shape[1]\n  input_width = input_shape[2]\n\n  # The Transformer performs sum residuals on all layers so the input needs\n  # to be the same as the hidden size.\n  if input_width != hidden_size:\n    raise ValueError(""The width of the input tensor (%d) != hidden size (%d)"" %\n                     (input_width, hidden_size))\n\n  # We keep the representation as a 2D tensor to avoid re-shaping it back and\n  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on\n  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to\n  # help the optimizer.\n  prev_output = reshape_to_matrix(input_tensor)\n\n  all_layer_outputs = []\n  for layer_idx in range(num_hidden_layers):\n    with tf.variable_scope(""layer_%d"" % layer_idx):\n      layer_input = prev_output\n\n      with tf.variable_scope(""attention""):\n        attention_heads = []\n        with tf.variable_scope(""self""):\n          attention_head = attention_layer(\n              from_tensor=layer_input,\n              to_tensor=layer_input,\n              attention_mask=attention_mask,\n              num_attention_heads=num_attention_heads,\n              size_per_head=attention_head_size,\n              attention_probs_dropout_prob=attention_probs_dropout_prob,\n              initializer_range=initializer_range,\n              do_return_2d_tensor=True,\n              batch_size=batch_size,\n              from_seq_length=seq_length,\n              to_seq_length=seq_length)\n          attention_heads.append(attention_head)\n\n        attention_output = None\n        if len(attention_heads) == 1:\n          attention_output = attention_heads[0]\n        else:\n          # In the case where we have other sequences, we just concatenate\n          # them to the self-attention head before the projection.\n          attention_output = tf.concat(attention_heads, axis=-1)\n\n        # Run a linear projection of `hidden_size` then add a residual\n        # with `layer_input`.\n        with tf.variable_scope(""output""):\n          attention_output = tf.layers.dense(\n              attention_output,\n              hidden_size,\n              kernel_initializer=create_initializer(initializer_range))\n          attention_output = dropout(attention_output, hidden_dropout_prob)\n          attention_output = layer_norm(attention_output + layer_input)\n\n      # The activation is only applied to the ""intermediate"" hidden layer.\n      with tf.variable_scope(""intermediate""):\n        intermediate_output = tf.layers.dense(\n            attention_output,\n            intermediate_size,\n            activation=intermediate_act_fn,\n            kernel_initializer=create_initializer(initializer_range))\n\n      # Down-project back to `hidden_size` then add the residual.\n      with tf.variable_scope(""output""):\n        layer_output = tf.layers.dense(\n            intermediate_output,\n            hidden_size,\n            kernel_initializer=create_initializer(initializer_range))\n        layer_output = dropout(layer_output, hidden_dropout_prob)\n        layer_output = layer_norm(layer_output + attention_output)\n        prev_output = layer_output\n        all_layer_outputs.append(layer_output)\n\n  if do_return_all_layers:\n    final_outputs = []\n    for layer_output in all_layer_outputs:\n      final_output = reshape_from_matrix(layer_output, input_shape)\n      final_outputs.append(final_output)\n    return final_outputs\n  else:\n    final_output = reshape_from_matrix(prev_output, input_shape)\n    return final_output\n\n\ndef get_shape_list(tensor, expected_rank=None, name=None):\n  """"""Returns a list of the shape of tensor, preferring static dimensions.\n\n  Args:\n    tensor: A tf.Tensor object to find the shape of.\n    expected_rank: (optional) int. The expected rank of `tensor`. If this is\n      specified and the `tensor` has a different rank, and exception will be\n      thrown.\n    name: Optional name of the tensor for the error message.\n\n  Returns:\n    A list of dimensions of the shape of tensor. All static dimensions will\n    be returned as python integers, and dynamic dimensions will be returned\n    as tf.Tensor scalars.\n  """"""\n  if name is None:\n    name = tensor.name\n\n  if expected_rank is not None:\n    assert_rank(tensor, expected_rank, name)\n\n  shape = tensor.shape.as_list()\n\n  non_static_indexes = []\n  for (index, dim) in enumerate(shape):\n    if dim is None:\n      non_static_indexes.append(index)\n\n  if not non_static_indexes:\n    return shape\n\n  dyn_shape = tf.shape(tensor)\n  for index in non_static_indexes:\n    shape[index] = dyn_shape[index]\n  return shape\n\n\ndef reshape_to_matrix(input_tensor):\n  """"""Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).""""""\n  ndims = input_tensor.shape.ndims\n  if ndims < 2:\n    raise ValueError(""Input tensor must have at least rank 2. Shape = %s"" %\n                     (input_tensor.shape))\n  if ndims == 2:\n    return input_tensor\n\n  width = input_tensor.shape[-1]\n  output_tensor = tf.reshape(input_tensor, [-1, width])\n  return output_tensor\n\n\ndef reshape_from_matrix(output_tensor, orig_shape_list):\n  """"""Reshapes a rank 2 tensor back to its original rank >= 2 tensor.""""""\n  if len(orig_shape_list) == 2:\n    return output_tensor\n\n  output_shape = get_shape_list(output_tensor)\n\n  orig_dims = orig_shape_list[0:-1]\n  width = output_shape[-1]\n\n  return tf.reshape(output_tensor, orig_dims + [width])\n\n\ndef assert_rank(tensor, expected_rank, name=None):\n  """"""Raises an exception if the tensor rank is not of the expected rank.\n\n  Args:\n    tensor: A tf.Tensor to check the rank of.\n    expected_rank: Python integer or list of integers, expected rank.\n    name: Optional name of the tensor for the error message.\n\n  Raises:\n    ValueError: If the expected shape doesn\'t match the actual shape.\n  """"""\n  if name is None:\n    name = tensor.name\n\n  expected_rank_dict = {}\n  if isinstance(expected_rank, six.integer_types):\n    expected_rank_dict[expected_rank] = True\n  else:\n    for x in expected_rank:\n      expected_rank_dict[x] = True\n\n  actual_rank = tensor.shape.ndims\n  if actual_rank not in expected_rank_dict:\n    scope_name = tf.get_variable_scope().name\n    raise ValueError(\n        ""For the tensor `%s` in scope `%s`, the actual rank ""\n        ""`%d` (shape = %s) is not equal to the expected rank `%s`"" %\n        (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))\n'"
deltann/infer/python/delta_infer/subgraphs/transformer/transformer.py,9,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport math\nimport numpy as np\nimport tensorflow as tf\nfrom ..common import *\nfrom .model import *\n\n__all__ = [""transformer_cell""]\n\n@RegistPattern(name=""TransformerCell"")\ndef transformer_cell(input_tensor,\n                     attention_mask=None,\n                     hidden_size=768,\n                     num_attention_heads=12,\n                     attention_head_size=0,\n                     batch_size=0,\n                     seq_length=0,\n                     intermediate_size=3072,\n                     intermediate_act_fn=gelu,\n                     hidden_dropout_prob=0.1,\n                     attention_probs_dropout_prob=0.1,\n                     initializer_range=0.02):\n    """"""Multi-headed, multi-layer Transformer from ""Attention is All You Need"".\n       code ref: https://github.com/google-research/bert/blob/master/modeling.py\n    """"""\n    with tf.variable_scope(""attention""):\n        attention_heads = []\n        with tf.variable_scope(""self""):\n            attention_head = attention_layer(\n              from_tensor=input_tensor,\n              to_tensor=input_tensor,\n              attention_mask=attention_mask,\n              num_attention_heads=num_attention_heads,\n              size_per_head=attention_head_size,\n              attention_probs_dropout_prob=attention_probs_dropout_prob,\n              initializer_range=initializer_range,\n              do_return_2d_tensor=True,\n              batch_size=batch_size,\n              from_seq_length=seq_length,\n              to_seq_length=seq_length)\n            attention_heads.append(attention_head)\n\n        attention_output = None\n        if len(attention_heads) == 1:\n            attention_output = attention_heads[0]\n        else:\n            # In the case where we have other sequences, we just concatenate\n            # them to the self-attention head before the projection. \n            attention_output = tf.concat(attention_heads, axis=-1)\n\n        # Run a linear projection of `hidden_size` then add a residual\n        # with `input_tensor`.\n        with tf.variable_scope(""output""):\n          attention_output = tf.layers.dense(\n              attention_output,\n              hidden_size,\n              kernel_initializer=create_initializer(initializer_range))\n          attention_output = dropout(attention_output, hidden_dropout_prob)\n          attention_output = layer_norm(attention_output + input_tensor)\n\n    # The activation is only applied to the ""intermediate"" hidden layer.\n    with tf.variable_scope(""intermediate""):\n        intermediate_output = tf.layers.dense(\n          attention_output,\n          intermediate_size,\n          activation=intermediate_act_fn,\n          kernel_initializer=create_initializer(initializer_range))\n\n    # Down-project back to `hidden_size` then add the residual.\n    with tf.variable_scope(""output""):\n        layer_output = tf.layers.dense(\n          intermediate_output,\n          hidden_size,\n          kernel_initializer=create_initializer(initializer_range))\n        layer_output = dropout(layer_output, hidden_dropout_prob)\n        layer_output = layer_norm(layer_output + attention_output)\n    return layer_output\n'"
egs/iemocap/emo/v1/local/python/compute_cmvn.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\'\'\' compute cmvn \'\'\'\n\nimport numpy as np\nfrom absl import logging\nfrom absl import app\nfrom absl import flags\n\nFLAGS = flags.FLAGS\nflags.DEFINE_string(\'path_file\', None, \'filelist of path\')\nflags.DEFINE_string(\'cmvn_path\', None, \'cmvn path\')\n\ndef main(_):\n  counts = 0.0\n  sum_feats = None\n  square_sum_feats = None\n\n  idx = 0\n  with open(FLAGS.path_file, \'r\') as fin:\n    for idx, path in enumerate(fin, 1):\n      path = path.strip()\n      feat = np.load(path)\n      if sum_feats is None and square_sum_feats is None:\n         feat_shape = feat.shape[1:]\n         logging.info(f""feat_shape: {feat_shape}"")\n         # Accumulate in double precision\n         sum_feats = np.zeros(feat_shape, dtype=np.float64)\n         square_sum_feats = np.zeros(feat_shape, dtype=np.float64)\n\n      counts += feat.shape[0]\n      sum_feats += feat.sum(axis=0)\n      square_sum_feats += np.sum(np.power(feat, 2), axis=0)\n\n  logging.info(f""sums: {sum_feats}"")\n  logging.info(f""square: {square_sum_feats}"")\n  logging.info(f""counts: {counts}"")\n  mean = sum_feats / counts\n  var = square_sum_feats / counts - np.power(mean, 2)\n\n  np.save(FLAGS.cmvn_path, (mean, var))\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  app.run(main)\n'"
egs/iemocap/emo/v1/local/python/dump_all_data.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\'\'\' dump impro and scprit to `all` data\'\'\'\n\nimport os\nimport sys\n\ndef gen_all_data(all_path, path):\n  for root, dirs, files in os.walk(path):\n    for file in files:\n      filepath = os.path.join(root, file)\n      l = filepath.split(\'/\')\n      if \'impro\' in l:\n          index = l.index(\'impro\') \n      elif \'script\' in l:\n          index = l.index(\'script\')\n      else:\n        print(\'Error,\', path)    \n        exit(-1)\n\n      l = l[index + 1:]\n      link_file = os.path.join(all_path, \'/\'.join(l))\n      link_path = os.path.dirname(link_file)\n  \n      filepath = os.path.relpath(filepath, link_path)\n  \n      if not os.path.exists(link_path):\n          os.makedirs(link_path)\n      if os.path.exists(link_file):\n          os.remove(link_file)\n  \n      os.symlink(filepath, link_file)\n\n\n\nif __name__ == \'__main__\':\n  if 4 != len(sys.argv):\n    print(\'Usage : \', sys.argv[0], \' impro_path script_path all_path\')\n    exit(-1)\n  \n  impro_path = sys.argv[1]\n  script_path = sys.argv[2]\n  all_path = sys.argv[3]\n  \n  gen_all_data(all_path, impro_path)\n  gen_all_data(all_path, script_path)\n'"
egs/iemocap/emo/v1/local/python/dump_data_from_pickle.py,0,"b""#from\n#https://github.com/zh794390558/IEMOCAP-Emotion-Detection\n\nimport os\nimport sys\nimport random\nimport numpy as np\nimport pickle\nimport copy\nimport wave\nfrom scipy.io import wavfile\n\nfrom dump_all_data import gen_all_data\nfrom helper import *\nimport kaldiio\n\n''' dump wav, text, label from pkl '''\n\nemotions_used = np.array(['ang', 'neu', 'sad', 'hap'])\nroot_path = os.path.realpath(os.getcwd())\ndata_path = os.path.join(root_path, 'data')\nsessions = ['Ses01', 'Ses02', 'Ses03', 'Ses04']\nframerate = 16000\ndump_dir= os.path.join(data_path, 'dump')\nall_dir = os.path.join(dump_dir, 'all')\n\ndef save_wav(data, filename, rate=framerate):\n    assert data.dtype == np.int16, data.dtype\n    wavfile.write(filename, rate, data)\n\ndef save_text(data, filename):\n    with open(filename, 'w') as f:\n        f.write(data)\n\ndef save_label(data, filename):\n    with open(filename, 'w') as f:\n        f.write(data)\n\nwith open(os.path.join(data_path,'data_collected.pickle'), 'rb') as handle:\n    datas = pickle.load(handle)\n    for data in datas:\n        # Ses01F_impro01_F000  Excuse me.  neu\n        key = data['id']\n        samples = data['signal'] # int16 \n        text = data['transcription']\n        label = data['emotion']\n\n        if label in emotions_used:\n            if key[7:12] == 'impro':\n                dirpath = os.path.join(dump_dir, 'impro')\n            else:\n                dirpath = os.path.join(dump_dir, 'script')\n\n            if key[0:5] in sessions:\n                dirpath = os.path.join(dirpath, 'train', label, key)\n            else:\n                dirpath = os.path.join(dirpath, 'eval', label, key)\n\n            if not os.path.exists(dirpath):\n                os.makedirs(dirpath)\n\n            filepath = os.path.join(dirpath, key) + '.wav'\n            save_wav(np.array(samples, dtype=np.int16), filepath)\n\n            filepath = os.path.join(dirpath, key) + '.txt'\n            save_text(text, filepath)\n\n            filepath = os.path.join(dirpath, key) + '.label'\n            save_label(label, filepath)\n\ngen_all_data(all_dir, dump_dir)\n"""
egs/iemocap/emo/v1/local/python/generate_vocab.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\'\'\'generate vocab and embed for text\'\'\'\n\nimport sys\nimport os\nimport re\nimport numpy as np\nfrom absl import logging\nfrom collections import defaultdict\nimport pickle\n\ndef generate_vocab(data_path, text_vocab_path, min_freq):\n  text_vocab = defaultdict(int)\n  for parent, _, filenames in os.walk(data_path):\n    for name in filenames:\n        if name.endswith("".txt""):\n          with open(os.path.join(parent, name), ""r"") as f:\n            for line in f.readlines():\n              text = re.findall(r""[\\w\']+|[.,!?;]"", line.strip().lower())\n              for w in text:\n                text_vocab[w] += 1\n  text_vocab_file = open(text_vocab_path, \'w\')\n  sorted_text_vocab = sorted(text_vocab.items(), key=lambda x: x[1], reverse=True)\n  text_vocab_file.write(\'<pad>\' + \'\\t\' + str(0) + \'\\n\')\n  text_vocab_file.write(\'<unk>\' + \'\\t\' + str(1) + \'\\n\')\n  idx = 2\n  text_vocab = {\'<pad>\': 0, \'<unk>\': 1}\n  for word, count in sorted_text_vocab:\n    if count >= min_freq:\n      text_vocab_file.write(word + \'\\t\' + str(idx) + \'\\n\')\n      text_vocab[word] = idx\n      idx += 1\n  try:\n    assert idx == len(text_vocab)\n  except Exception as e:\n    print(""idx:"", idx)\n    print(""vocab:"", len(text_vocab))\n\n  logging.info(""generate vocab: {}"".format(idx))\n  return text_vocab\n\n\ndef generate_embed(embed_path, text_vocab,\n                   embed_pickle_path, embed_size=300):\n  embeddings_dict = {}\n  with open(embed_path, encoding=""utf-8"") as f:\n    for line in f:\n      try:\n        values = line.replace(""\\n"", """").split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype=\'float32\')\n        embeddings_dict[word] = coefs\n      except Exception as e:\n        print(""\\tException: "", e)\n    embedding_matrix = np.zeros((len(text_vocab), embed_size))\n\n    for word, i in text_vocab.items():\n        if word in embeddings_dict:\n            embedding_matrix[int(i)] = embeddings_dict[word]\n        else:\n            embedding_matrix[int(i)] = np.random.uniform(-1, 1, embed_size)\n            logging.info(""out_of_vocabs {}"".format(word))\n    with open(embed_pickle_path, \'ab+\') as fp:\n      pickle.dump(embedding_matrix, fp)\n    logging.info(""embedding_matrix: {}"".format(embedding_matrix.shape))\n    logging.info(""finish generate embed"")\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n\n  if len(sys.argv) != 6:\n    logging.error(""Usage python {} data_path, text_vocab_path, ""\n                  ""min_freq, embed_path, embed_pickle_path"".format(sys.argv[0]))\n    sys.exit(-1)\n\n  data_path = sys.argv[1]\n  text_vocab_path = sys.argv[2]\n  min_freq = int(sys.argv[3])\n  embed_path = sys.argv[4]\n  embed_pickle_path = sys.argv[5]\n  text_vocab = generate_vocab(data_path, text_vocab_path, min_freq)\n  generate_embed(embed_path, text_vocab, embed_pickle_path)\n\n'"
egs/iemocap/emo/v1/local/python/helper.py,0,"b'#from\n#https://github.com/zh794390558/IEMOCAP-Emotion-Detection\n\nimport os\nimport csv\nimport wave\nimport sys\nimport numpy as np\nimport pandas as pd\nimport glob\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\n\n\ndef split_wav(wav, emotions):\n    (nchannels, sampwidth, framerate, nframes, comptype, compname), samples = wav\n\n    left = samples[0::nchannels]\n    right = samples[1::nchannels]\n\n    frames = []\n    for ie, e in enumerate(emotions):\n        start = e[\'start\']\n        end = e[\'end\']\n\n        e[\'right\'] = right[int(start * framerate):int(end * framerate)]\n        e[\'left\'] = left[int(start * framerate):int(end * framerate)]\n\n        frames.append({\'left\': e[\'left\'], \'right\': e[\'right\']})\n    return frames\n\n\ndef get_field(data, key):\n    return np.array([e[key] for e in data])\n\ndef pad_sequence_into_array(Xs, maxlen=None, truncating=\'post\', padding=\'post\', value=0.):\n\n    Nsamples = len(Xs)\n    if maxlen is None:\n        lengths = [s.shape[0] for s in Xs]    # \'sequences\' must be list, \'s\' must be numpy array, len(s) return the first dimension of s\n        maxlen = np.max(lengths)\n\n    Xout = np.ones(shape=[Nsamples, maxlen] + list(Xs[0].shape[1:]), dtype=Xs[0].dtype) * np.asarray(value, dtype=Xs[0].dtype)\n    Mask = np.zeros(shape=[Nsamples, maxlen], dtype=Xout.dtype)\n    for i in range(Nsamples):\n        x = Xs[i]\n        if truncating == \'pre\':\n            trunc = x[-maxlen:]\n        elif truncating == \'post\':\n            trunc = x[:maxlen]\n        else:\n            raise ValueError(""Truncating type \'%s\' not understood"" % truncating)\n        if padding == \'post\':\n            Xout[i, :len(trunc)] = trunc\n            Mask[i, :len(trunc)] = 1\n        elif padding == \'pre\':\n            Xout[i, -len(trunc):] = trunc\n            Mask[i, -len(trunc):] = 1\n        else:\n            raise ValueError(""Padding type \'%s\' not understood"" % padding)\n    return Xout, Mask\n\n\ndef convert_gt_from_array_to_list(gt_batch, gt_batch_mask=None):\n\n    B, L = gt_batch.shape\n    gt_batch = gt_batch.astype(\'int\')\n    gts = []\n    for i in range(B):\n        if gt_batch_mask is None:\n            l = L\n        else:\n            l = int(gt_batch_mask[i, :].sum())\n        gts.append(gt_batch[i, :l].tolist())\n    return gts\n\ndef get_audio(path_to_wav, filename):\n    wav = wave.open(os.path.join(path_to_wav, filename), mode=""r"")\n    (nchannels, sampwidth, framerate, nframes, comptype, compname) = wav.getparams()\n    assert framerate == 16000\n    content = wav.readframes(nframes)\n    samples = np.fromstring(content, dtype=np.int16)\n    return (nchannels, sampwidth, framerate, nframes, comptype, compname), samples\n\n\ndef get_transcriptions(path_to_transcriptions, filename):\n    f = open(os.path.join(path_to_transcriptions, filename), \'r\').read()\n    f = np.array(f.split(\'\\n\'))\n    transcription = {}\n    for i in range(len(f) - 1):\n        g = f[i]\n        i1 = g.find(\': \')\n        i0 = g.find(\' [\')\n        ind_id = g[:i0]\n        ind_ts = g[i1+2:]\n        transcription[ind_id] = ind_ts\n    return transcription\n\ndef get_transcriptions_align(path_to_transcriptions, filename,\n                             path_to_transcriptions_to_align):\n    f = open(os.path.join(path_to_transcriptions, filename), \'r\').read()\n    f = np.array(f.split(\'\\n\'))\n    transcription = {}\n    print(""path_to_transcriptions:"", path_to_transcriptions)\n    print(""path_to_transcriptions_to_align:"", path_to_transcriptions_to_align)\n    print(""filename:"", filename)\n    for i in range(len(f) - 1):\n        g = f[i]\n        i1 = g.find(\': \')\n        i0 = g.find(\' [\')\n        ind_id = g[:i0]\n        ind_ts = g[i1+2:]\n        ind_ts = text_to_word_sequence(ind_ts,filters=\'!""#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n\',\n                                       lower=True,split="" "")\n        align_t = []\n        #print(""ind_id_text:"", g, flush=True)\n        #print(""ind_id:"", ind_id, flush=True)\n        align_file_name = path_to_transcriptions_to_align + filename[:-4] + ""/"" + str(ind_id) + "".wdseg""\n        if os.path.exists(align_file_name):\n            f_align = open(align_file_name, \'r\').read()\n            f_align = np.array(f_align.split(\'\\n\'))\n            #print(""f_align"", f_align, flush=True)\n            for i in range(2, len(f_align) - 3):\n                # print(f\'f_align{i}\', f_align[i])\n                w = f_align[i].split()[3].split(""("")[0]\n                w = text_to_word_sequence(w,filters=\'!""#$%&()*+,-./:;=>?@[\\\\]^`{|}~\\t\\n\',\n                                           lower=True,split="" "")[0]\n                if w in ind_ts:\n                    align_t.append({\'word\': w,\n                                    \'SFrm\': f_align[i].split()[0],\n                                    \'Efrm\': f_align[i].split()[1]})\n            #print(""align_t"", align_t, flush=True)\n            #print(""w_list"", ind_ts, flush=True)\n            assert len(align_t) == len(ind_ts)\n            transcription[ind_id] = {\'ind_ts\': ind_ts,\n                                   \'align_t\': align_t}\n    return transcription\n\ndef get_emotions(path_to_emotions, filename):\n    f = open(os.path.join(path_to_emotions, filename), \'r\').read()\n    f = np.array(f.split(\'\\n\'))\n    idx = f == \'\'\n    idx_n = np.arange(len(f))[idx]\n    emotion = []\n    for i in range(len(idx_n) - 2):\n        g = f[idx_n[i]+1:idx_n[i+1]]\n        head = g[0]\n        i0 = head.find(\' - \')\n        start_time = float(head[head.find(\'[\') + 1:head.find(\' - \')])\n        end_time = float(head[head.find(\' - \') + 3:head.find(\']\')])\n        actor_id = head[head.find(filename[:-4]) + len(filename[:-4]) + 1:\n                        head.find(filename[:-4]) + len(filename[:-4]) + 5]\n        emo = head[head.find(\'\\t[\') - 3:head.find(\'\\t[\')]\n        vad = head[head.find(\'\\t[\') + 1:]\n\n        v = float(vad[1:7])\n        a = float(vad[9:15])\n        d = float(vad[17:23])\n        \n        j = 1\n        emos = []\n        while g[j][0] == ""C"":\n            head = g[j]\n            start_idx = head.find(""\\t"") + 1\n            evoluator_emo = []\n            idx = head.find("";"", start_idx)\n            while idx != -1:\n                evoluator_emo.append(head[start_idx:idx].strip().lower()[:3])\n                start_idx = idx + 1\n                idx = head.find("";"", start_idx)\n            emos.append(evoluator_emo)\n            j += 1\n\n        emotion.append({\'start\': start_time,\n                        \'end\': end_time,\n                        \'id\': filename[:-4] + \'_\' + actor_id,\n                        \'v\': v,\n                        \'a\': a,\n                        \'d\': d,\n                        \'emotion\': emo,\n                        \'emo_evo\': emos})\n    return emotion\n'"
egs/iemocap/emo/v1/local/python/inspect_feature.py,0,"b'# Copyright (C) 2017 Beijing Didi Infinity Technology and Development Co.,Ltd.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\'\'\' inspect feature shape and dtype\'\'\'\n\nimport numpy as np\nfrom absl import logging\nfrom absl import app\nfrom absl import flags\n\nFLAGS = flags.FLAGS\nflags.DEFINE_string(\'path\', None, \'feature path\')\nflags.DEFINE_boolean(\'verbose\', False, \'dump data info\')\n\ndef main(_):\n  feat = np.load(FLAGS.path)\n  logging.info(f""[{FLAGS.path}]"")\n  logging.info(f""  shape: {feat.shape}"")\n  logging.info(f""  dtype: {feat.dtype}"")\n  logging.info(f""  isnan: {np.all(np.isnan(feat))}"")\n  logging.info(f""  isinf: {np.all(np.isinf(feat))}"")\n  if FLAGS.verbose:\n    logging.info(f""  data: {feat}"")\n    logging.info(f""  data: {feat[0][:]}"")\n\nif __name__ == \'__main__\':\n  logging.set_verbosity(logging.INFO)\n  app.run(main)\n'"
egs/iemocap/emo/v1/local/python/mocap_data_collect.py,0,"b'#from\n#https://github.com/zh794390558/IEMOCAP-Emotion-Detection\n\nimport os\nimport sys\nimport tqdm\nimport numpy as np\n\nimport wave\nimport copy\nimport math\nimport pickle\n\nfrom multiprocessing import Pool, Lock, cpu_count, Manager\n\nfrom sklearn.preprocessing import label_binarize\nfrom helper import *\n\n\'\'\' collection data from corpus \'\'\'\n\niemocap_path = sys.argv[1]\nroot_path = os.path.realpath(os.getcwd())\ndata_path = os.path.join(root_path, \'data\')\nsessions = [\'Session1\', \'Session2\', \'Session3\', \'Session4\', \'Session5\']\npath_map = {\'Ses01\':\'Session1\', \'Ses02\':\'Session2\',\'Ses03\':\'Session3\',\'Ses04\':\'Session4\',\'Ses05\':\'Session5\'}\nframerate = 16000\n\ndef get_mocap_rot(path_to_mocap_rot, filename, start,end):\n    f = open(os.path.join(path_to_mocap_rot, filename), \'r\').read()\n    f = np.array(f.split(\'\\n\'))\n    mocap_rot = []\n    mocap_rot_avg = []\n    f = f[2:]\n    counter = 0\n    for data in f:\n        counter+=1\n        data2 = data.split(\' \')\n        if(len(data2)<2):\n            continue\n        if(float(data2[1])>start and float(data2[1])<end):\n            mocap_rot_avg.append(np.array(data2[2:]).astype(np.float))\n            \n    mocap_rot_avg = np.array_split(np.array(mocap_rot_avg), 200)\n    for spl in mocap_rot_avg:\n        if spl.size != 0:\n            mocap_rot.append(np.mean(spl, axis=0))\n    return np.array(mocap_rot)\n\ndef get_mocap_hand(path_to_mocap_hand, filename, start,end):\n    f = open(os.path.join(path_to_mocap_hand, filename), \'r\').read()\n    f = np.array(f.split(\'\\n\'))\n    mocap_hand = []\n    mocap_hand_avg = []\n    f = f[2:]\n    counter = 0\n    for data in f:\n        counter+=1\n        data2 = data.split(\' \')\n        if(len(data2)<2):\n            continue\n        if(float(data2[1])>start and float(data2[1])<end):\n            mocap_hand_avg.append(np.array(data2[2:]).astype(np.float))\n            \n    mocap_hand_avg = np.array_split(np.array(mocap_hand_avg), 200)\n    for spl in mocap_hand_avg:\n        if spl.size != 0:\n            mocap_hand.append(np.mean(spl, axis=0))\n    return np.array(mocap_hand)\n\ndef get_mocap_head(path_to_mocap_head, filename, start,end):\n    f = open(os.path.join(path_to_mocap_head, filename), \'r\').read()\n    f = np.array(f.split(\'\\n\'))\n    mocap_head = []\n    mocap_head_avg = []\n    f = f[2:]\n    counter = 0\n    for data in f:\n        counter+=1\n        data2 = data.split(\' \')\n        if(len(data2)<2):\n            continue\n        if(float(data2[1])>start and float(data2[1])<end):\n            mocap_head_avg.append(np.array(data2[2:]).astype(np.float))\n            \n    mocap_head_avg = np.array_split(np.array(mocap_head_avg), 200)\n    for spl in mocap_head_avg:\n        if spl.size != 0:\n            mocap_head.append(np.mean(spl, axis=0))\n    return np.array(mocap_head)\n\ndef collect(f):\n    path_to_wav = os.path.join(iemocap_path, path_map[f[0:5]], \'dialog\', \'wav\')\n    path_to_emotions = os.path.join(iemocap_path, path_map[f[0:5]], \'dialog\', \'EmoEvaluation\')\n    path_to_transcriptions = os.path.join(iemocap_path, path_map[f[0:5]], \'dialog\', \'transcriptions\')\n    path_to_mocap_hand = os.path.join(iemocap_path, path_map[f[0:5]], \'dialog\', \'MOCAP_hand\')\n    path_to_mocap_rot = os.path.join(iemocap_path, path_map[f[0:5]], \'dialog\', \'MOCAP_rotated\')\n    path_to_mocap_head = os.path.join(iemocap_path, path_map[f[0:5]], \'dialog\', \'MOCAP_head\')\n\n    mocap_f = f\n    if (f== \'Ses05M_script01_1b\'):\n        mocap_f = \'Ses05M_script01_1\'\n\n    wav = get_audio(path_to_wav, f + \'.wav\')\n    transcriptions = get_transcriptions(path_to_transcriptions, f + \'.txt\')\n    emotions = get_emotions(path_to_emotions, f + \'.txt\')\n    sample = split_wav(wav, emotions)\n\n    for ie, e in enumerate(emotions):\n        \'\'\'if \'F\' in e[\'id\']:\n            e[\'signal\'] = sample[ie][\'left\']\n        else:\n            e[\'signal\'] = sample[ie][\'right\']\'\'\'\n\n        e[\'signal\'] = sample[ie][\'left\']\n        e.pop(""left"", None)\n        e.pop(""right"", None)\n        e[\'transcription\'] = transcriptions[e[\'id\']]\n        e[\'mocap_hand\'] = get_mocap_hand(path_to_mocap_hand, mocap_f + \'.txt\', e[\'start\'], e[\'end\'])\n        e[\'mocap_rot\'] = get_mocap_rot(path_to_mocap_rot, mocap_f + \'.txt\', e[\'start\'], e[\'end\'])\n        e[\'mocap_head\'] = get_mocap_head(path_to_mocap_head, mocap_f + \'.txt\', e[\'start\'], e[\'end\'])\n        lock.acquire()\n        if e[\'id\'] not in ids:\n            data.append(e)\n            ids[e[\'id\']] = 1\n        lock.release()\n\ndef read_iemocap_mocap():\n    for session in sessions:\n        path_to_wav = os.path.join(iemocap_path, session, \'dialog\', \'wav\')\n        files2 = os.listdir(path_to_wav)\n\n        files = []\n        for f in files2:\n            if f.endswith("".wav""):\n                if \'perturb\' not in f:\n                    if f[0] == \'.\':\n                        files.append(f[2:-4])\n                    else:\n                        files.append(f[:-4])\n        print(\'Collect \', session)\n        with Pool(cpu_num) as p:\n            r = list(tqdm.tqdm(p.imap(collect, files), total=len(files)))\n                        \n    sort_key = get_field(data, ""id"")\n    return np.array(data)[np.argsort(sort_key)]\n    \ncpu_num = cpu_count()\nmanager = Manager()\ndata = manager.list()\nids = manager.dict()\nlock=Lock()\n\ndata = read_iemocap_mocap()\n\nwith open(os.path.join(data_path, \'data_collected.pickle\'), \'wb\') as handle:\n    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n'"
