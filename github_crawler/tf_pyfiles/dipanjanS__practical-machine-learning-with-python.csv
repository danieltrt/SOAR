file_path,api_count,code
bonus content/model interpretation with skater/model_evaluation_utils.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Mon Jul 31 20:05:23 2017\n\n@author: DIP\n""""""\n\nfrom sklearn import metrics\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.base import clone\nfrom sklearn.preprocessing import label_binarize\nfrom scipy import interp\nfrom sklearn.metrics import roc_curve, auc \n\n\ndef get_metrics(true_labels, predicted_labels):\n    \n    print(\'Accuracy:\', np.round(\n                        metrics.accuracy_score(true_labels, \n                                               predicted_labels),\n                        4))\n    print(\'Precision:\', np.round(\n                        metrics.precision_score(true_labels, \n                                               predicted_labels,\n                                               average=\'weighted\'),\n                        4))\n    print(\'Recall:\', np.round(\n                        metrics.recall_score(true_labels, \n                                               predicted_labels,\n                                               average=\'weighted\'),\n                        4))\n    print(\'F1 Score:\', np.round(\n                        metrics.f1_score(true_labels, \n                                               predicted_labels,\n                                               average=\'weighted\'),\n                        4))\n                        \n\ndef train_predict_model(classifier, \n                        train_features, train_labels, \n                        test_features, test_labels):\n    # build model    \n    classifier.fit(train_features, train_labels)\n    # predict using model\n    predictions = classifier.predict(test_features) \n    return predictions    \n\n\ndef display_confusion_matrix(true_labels, predicted_labels, classes=[1,0]):\n    \n    total_classes = len(classes)\n    level_labels = [total_classes*[0], list(range(total_classes))]\n\n    cm = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels, \n                                  labels=classes)\n    cm_frame = pd.DataFrame(data=cm, \n                            columns=pd.MultiIndex(levels=[[\'Predicted:\'], classes], \n                                                  labels=level_labels), \n                            index=pd.MultiIndex(levels=[[\'Actual:\'], classes], \n                                                labels=level_labels)) \n    print(cm_frame) \n    \ndef display_classification_report(true_labels, predicted_labels, classes=[1,0]):\n\n    report = metrics.classification_report(y_true=true_labels, \n                                           y_pred=predicted_labels, \n                                           labels=classes) \n    print(report)\n    \n    \n    \ndef display_model_performance_metrics(true_labels, predicted_labels, classes=[1,0]):\n    print(\'Model Performance metrics:\')\n    print(\'-\'*30)\n    get_metrics(true_labels=true_labels, predicted_labels=predicted_labels)\n    print(\'\\nModel Classification report:\')\n    print(\'-\'*30)\n    display_classification_report(true_labels=true_labels, predicted_labels=predicted_labels, \n                                  classes=classes)\n    print(\'\\nPrediction Confusion Matrix:\')\n    print(\'-\'*30)\n    display_confusion_matrix(true_labels=true_labels, predicted_labels=predicted_labels, \n                             classes=classes)\n\n\ndef plot_model_decision_surface(clf, train_features, train_labels,\n                                plot_step=0.02, cmap=plt.cm.RdYlBu,\n                                markers=None, alphas=None, colors=None):\n    \n    if train_features.shape[1] != 2:\n        raise ValueError(""X_train should have exactly 2 columnns!"")\n    \n    x_min, x_max = train_features[:, 0].min() - plot_step, train_features[:, 0].max() + plot_step\n    y_min, y_max = train_features[:, 1].min() - plot_step, train_features[:, 1].max() + plot_step\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                         np.arange(y_min, y_max, plot_step))\n\n    clf_est = clone(clf)\n    clf_est.fit(train_features,train_labels)\n    if hasattr(clf_est, \'predict_proba\'):\n        Z = clf_est.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n    else:\n        Z = clf_est.predict(np.c_[xx.ravel(), yy.ravel()])    \n    Z = Z.reshape(xx.shape)\n    cs = plt.contourf(xx, yy, Z, cmap=cmap)\n    \n    le = LabelEncoder()\n    y_enc = le.fit_transform(train_labels)\n    n_classes = len(le.classes_)\n    plot_colors = \'\'.join(colors) if colors else [None] * n_classes\n    label_names = le.classes_\n    markers = markers if markers else [None] * n_classes\n    alphas = alphas if alphas else [None] * n_classes\n    for i, color in zip(range(n_classes), plot_colors):\n        idx = np.where(y_enc == i)\n        plt.scatter(train_features[idx, 0], train_features[idx, 1], c=color,\n                    label=label_names[i], cmap=cmap, edgecolors=\'black\', \n                    marker=markers[i], alpha=alphas[i])\n    plt.legend()\n    plt.show()\n\n\ndef plot_model_roc_curve(clf, features, true_labels, label_encoder=None, class_names=None):\n    \n    ## Compute ROC curve and ROC area for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    if hasattr(clf, \'classes_\'):\n        class_labels = clf.classes_\n    elif label_encoder:\n        class_labels = label_encoder.classes_\n    elif class_names:\n        class_labels = class_names\n    else:\n        raise ValueError(\'Unable to derive prediction classes, please specify class_names!\')\n    n_classes = len(class_labels)\n    y_test = label_binarize(true_labels, classes=class_labels)\n    if n_classes == 2:\n        if hasattr(clf, \'predict_proba\'):\n            prob = clf.predict_proba(features)\n            y_score = prob[:, prob.shape[1]-1] \n        elif hasattr(clf, \'decision_function\'):\n            prob = clf.decision_function(features)\n            y_score = prob[:, prob.shape[1]-1]\n        else:\n            raise AttributeError(""Estimator doesn\'t have a probability or confidence scoring system!"")\n        \n        fpr, tpr, _ = roc_curve(y_test, y_score)      \n        roc_auc = auc(fpr, tpr)\n        plt.plot(fpr, tpr, label=\'ROC curve (area = {0:0.2f})\'\n                                 \'\'.format(roc_auc),\n                 linewidth=2.5)\n        \n    elif n_classes > 2:\n        if hasattr(clf, \'predict_proba\'):\n            y_score = clf.predict_proba(features)\n        elif hasattr(clf, \'decision_function\'):\n            y_score = clf.decision_function(features)\n        else:\n            raise AttributeError(""Estimator doesn\'t have a probability or confidence scoring system!"")\n\n        for i in range(n_classes):\n            fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n            roc_auc[i] = auc(fpr[i], tpr[i])\n\n        ## Compute micro-average ROC curve and ROC area\n        fpr[""micro""], tpr[""micro""], _ = roc_curve(y_test.ravel(), y_score.ravel())\n        roc_auc[""micro""] = auc(fpr[""micro""], tpr[""micro""])\n\n        ## Compute macro-average ROC curve and ROC area\n        # First aggregate all false positive rates\n        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n        # Then interpolate all ROC curves at this points\n        mean_tpr = np.zeros_like(all_fpr)\n        for i in range(n_classes):\n            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n        # Finally average it and compute AUC\n        mean_tpr /= n_classes\n        fpr[""macro""] = all_fpr\n        tpr[""macro""] = mean_tpr\n        roc_auc[""macro""] = auc(fpr[""macro""], tpr[""macro""])\n\n        ## Plot ROC curves\n        plt.figure(figsize=(6, 4))\n        plt.plot(fpr[""micro""], tpr[""micro""],\n                 label=\'micro-average ROC curve (area = {0:0.2f})\'\n                       \'\'.format(roc_auc[""micro""]), linewidth=3)\n\n        plt.plot(fpr[""macro""], tpr[""macro""],\n                 label=\'macro-average ROC curve (area = {0:0.2f})\'\n                       \'\'.format(roc_auc[""macro""]), linewidth=3)\n\n        for i, label in enumerate(class_labels):\n            plt.plot(fpr[i], tpr[i], label=\'ROC curve of class {0} (area = {1:0.2f})\'\n                                           \'\'.format(label, roc_auc[i]), \n                     linewidth=2, linestyle=\':\')\n    else:\n        raise ValueError(\'Number of classes should be atleast 2 or more\')\n        \n    plt.plot([0, 1], [0, 1], \'k--\')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel(\'False Positive Rate\')\n    plt.ylabel(\'True Positive Rate\')\n    plt.title(\'Receiver Operating Characteristic (ROC) Curve\')\n    plt.legend(loc=""lower right"")\n    plt.show()\n\n\n'"
bonus content/nlp proven approach/contractions.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Mon Aug 01 01:11:02 2016\n\n@author: DIP\n""""""\n\nCONTRACTION_MAP = {\n""ain\'t"": ""is not"",\n""aren\'t"": ""are not"",\n""can\'t"": ""cannot"",\n""can\'t\'ve"": ""cannot have"",\n""\'cause"": ""because"",\n""could\'ve"": ""could have"",\n""couldn\'t"": ""could not"",\n""couldn\'t\'ve"": ""could not have"",\n""didn\'t"": ""did not"",\n""doesn\'t"": ""does not"",\n""don\'t"": ""do not"",\n""hadn\'t"": ""had not"",\n""hadn\'t\'ve"": ""had not have"",\n""hasn\'t"": ""has not"",\n""haven\'t"": ""have not"",\n""he\'d"": ""he would"",\n""he\'d\'ve"": ""he would have"",\n""he\'ll"": ""he will"",\n""he\'ll\'ve"": ""he he will have"",\n""he\'s"": ""he is"",\n""how\'d"": ""how did"",\n""how\'d\'y"": ""how do you"",\n""how\'ll"": ""how will"",\n""how\'s"": ""how is"",\n""I\'d"": ""I would"",\n""I\'d\'ve"": ""I would have"",\n""I\'ll"": ""I will"",\n""I\'ll\'ve"": ""I will have"",\n""I\'m"": ""I am"",\n""I\'ve"": ""I have"",\n""i\'d"": ""i would"",\n""i\'d\'ve"": ""i would have"",\n""i\'ll"": ""i will"",\n""i\'ll\'ve"": ""i will have"",\n""i\'m"": ""i am"",\n""i\'ve"": ""i have"",\n""isn\'t"": ""is not"",\n""it\'d"": ""it would"",\n""it\'d\'ve"": ""it would have"",\n""it\'ll"": ""it will"",\n""it\'ll\'ve"": ""it will have"",\n""it\'s"": ""it is"",\n""let\'s"": ""let us"",\n""ma\'am"": ""madam"",\n""mayn\'t"": ""may not"",\n""might\'ve"": ""might have"",\n""mightn\'t"": ""might not"",\n""mightn\'t\'ve"": ""might not have"",\n""must\'ve"": ""must have"",\n""mustn\'t"": ""must not"",\n""mustn\'t\'ve"": ""must not have"",\n""needn\'t"": ""need not"",\n""needn\'t\'ve"": ""need not have"",\n""o\'clock"": ""of the clock"",\n""oughtn\'t"": ""ought not"",\n""oughtn\'t\'ve"": ""ought not have"",\n""shan\'t"": ""shall not"",\n""sha\'n\'t"": ""shall not"",\n""shan\'t\'ve"": ""shall not have"",\n""she\'d"": ""she would"",\n""she\'d\'ve"": ""she would have"",\n""she\'ll"": ""she will"",\n""she\'ll\'ve"": ""she will have"",\n""she\'s"": ""she is"",\n""should\'ve"": ""should have"",\n""shouldn\'t"": ""should not"",\n""shouldn\'t\'ve"": ""should not have"",\n""so\'ve"": ""so have"",\n""so\'s"": ""so as"",\n""that\'d"": ""that would"",\n""that\'d\'ve"": ""that would have"",\n""that\'s"": ""that is"",\n""there\'d"": ""there would"",\n""there\'d\'ve"": ""there would have"",\n""there\'s"": ""there is"",\n""they\'d"": ""they would"",\n""they\'d\'ve"": ""they would have"",\n""they\'ll"": ""they will"",\n""they\'ll\'ve"": ""they will have"",\n""they\'re"": ""they are"",\n""they\'ve"": ""they have"",\n""to\'ve"": ""to have"",\n""wasn\'t"": ""was not"",\n""we\'d"": ""we would"",\n""we\'d\'ve"": ""we would have"",\n""we\'ll"": ""we will"",\n""we\'ll\'ve"": ""we will have"",\n""we\'re"": ""we are"",\n""we\'ve"": ""we have"",\n""weren\'t"": ""were not"",\n""what\'ll"": ""what will"",\n""what\'ll\'ve"": ""what will have"",\n""what\'re"": ""what are"",\n""what\'s"": ""what is"",\n""what\'ve"": ""what have"",\n""when\'s"": ""when is"",\n""when\'ve"": ""when have"",\n""where\'d"": ""where did"",\n""where\'s"": ""where is"",\n""where\'ve"": ""where have"",\n""who\'ll"": ""who will"",\n""who\'ll\'ve"": ""who will have"",\n""who\'s"": ""who is"",\n""who\'ve"": ""who have"",\n""why\'s"": ""why is"",\n""why\'ve"": ""why have"",\n""will\'ve"": ""will have"",\n""won\'t"": ""will not"",\n""won\'t\'ve"": ""will not have"",\n""would\'ve"": ""would have"",\n""wouldn\'t"": ""would not"",\n""wouldn\'t\'ve"": ""would not have"",\n""y\'all"": ""you all"",\n""y\'all\'d"": ""you all would"",\n""y\'all\'d\'ve"": ""you all would have"",\n""y\'all\'re"": ""you all are"",\n""y\'all\'ve"": ""you all have"",\n""you\'d"": ""you would"",\n""you\'d\'ve"": ""you would have"",\n""you\'ll"": ""you will"",\n""you\'ll\'ve"": ""you will have"",\n""you\'re"": ""you are"",\n""you\'ve"": ""you have""\n}'"
bonus content/nlp proven approach/model_evaluation_utils.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Mon Jul 31 20:05:23 2017\n\n@author: DIP\n""""""\n\nfrom sklearn import metrics\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.base import clone\nfrom sklearn.preprocessing import label_binarize\nfrom scipy import interp\nfrom sklearn.metrics import roc_curve, auc \n\n\ndef get_metrics(true_labels, predicted_labels):\n    \n    print(\'Accuracy:\', np.round(\n                        metrics.accuracy_score(true_labels, \n                                               predicted_labels),\n                        4))\n    print(\'Precision:\', np.round(\n                        metrics.precision_score(true_labels, \n                                               predicted_labels,\n                                               average=\'weighted\'),\n                        4))\n    print(\'Recall:\', np.round(\n                        metrics.recall_score(true_labels, \n                                               predicted_labels,\n                                               average=\'weighted\'),\n                        4))\n    print(\'F1 Score:\', np.round(\n                        metrics.f1_score(true_labels, \n                                               predicted_labels,\n                                               average=\'weighted\'),\n                        4))\n                        \n\ndef train_predict_model(classifier, \n                        train_features, train_labels, \n                        test_features, test_labels):\n    # build model    \n    classifier.fit(train_features, train_labels)\n    # predict using model\n    predictions = classifier.predict(test_features) \n    return predictions    \n\n\ndef display_confusion_matrix(true_labels, predicted_labels, classes=[1,0]):\n    \n    total_classes = len(classes)\n    level_labels = [total_classes*[0], list(range(total_classes))]\n\n    cm = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels, \n                                  labels=classes)\n    cm_frame = pd.DataFrame(data=cm, \n                            columns=pd.MultiIndex(levels=[[\'Predicted:\'], classes], \n                                                  labels=level_labels), \n                            index=pd.MultiIndex(levels=[[\'Actual:\'], classes], \n                                                labels=level_labels)) \n    print(cm_frame) \n\n\ndef display_confusion_matrix_pretty(true_labels, predicted_labels, classes=[1,0]):\n    \n    total_classes = len(classes)\n    level_labels = [total_classes*[0], list(range(total_classes))]\n\n    cm = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels, \n                                  labels=classes)\n    cm_frame = pd.DataFrame(data=cm, \n                            columns=pd.MultiIndex(levels=[[\'Predicted:\'], classes], \n                                                  labels=level_labels), \n                            index=pd.MultiIndex(levels=[[\'Actual:\'], classes], \n                                                labels=level_labels)) \n    return cm_frame    \n    \ndef display_classification_report(true_labels, predicted_labels, classes=[1,0]):\n\n    report = metrics.classification_report(y_true=true_labels, \n                                           y_pred=predicted_labels, \n                                           labels=classes) \n    print(report)\n    \n    \n    \ndef display_model_performance_metrics(true_labels, predicted_labels, classes=[1,0]):\n    print(\'Model Performance metrics:\')\n    print(\'-\'*30)\n    get_metrics(true_labels=true_labels, predicted_labels=predicted_labels)\n    print(\'\\nModel Classification report:\')\n    print(\'-\'*30)\n    display_classification_report(true_labels=true_labels, predicted_labels=predicted_labels, \n                                  classes=classes)\n    print(\'\\nPrediction Confusion Matrix:\')\n    print(\'-\'*30)\n    display_confusion_matrix(true_labels=true_labels, predicted_labels=predicted_labels, \n                             classes=classes)\n\n\ndef plot_model_decision_surface(clf, train_features, train_labels,\n                                plot_step=0.02, cmap=plt.cm.RdYlBu,\n                                markers=None, alphas=None, colors=None):\n    \n    if train_features.shape[1] != 2:\n        raise ValueError(""X_train should have exactly 2 columnns!"")\n    \n    x_min, x_max = train_features[:, 0].min() - plot_step, train_features[:, 0].max() + plot_step\n    y_min, y_max = train_features[:, 1].min() - plot_step, train_features[:, 1].max() + plot_step\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                         np.arange(y_min, y_max, plot_step))\n\n    clf_est = clone(clf)\n    clf_est.fit(train_features,train_labels)\n    if hasattr(clf_est, \'predict_proba\'):\n        Z = clf_est.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n    else:\n        Z = clf_est.predict(np.c_[xx.ravel(), yy.ravel()])    \n    Z = Z.reshape(xx.shape)\n    cs = plt.contourf(xx, yy, Z, cmap=cmap)\n    \n    le = LabelEncoder()\n    y_enc = le.fit_transform(train_labels)\n    n_classes = len(le.classes_)\n    plot_colors = \'\'.join(colors) if colors else [None] * n_classes\n    label_names = le.classes_\n    markers = markers if markers else [None] * n_classes\n    alphas = alphas if alphas else [None] * n_classes\n    for i, color in zip(range(n_classes), plot_colors):\n        idx = np.where(y_enc == i)\n        plt.scatter(train_features[idx, 0], train_features[idx, 1], c=color,\n                    label=label_names[i], cmap=cmap, edgecolors=\'black\', \n                    marker=markers[i], alpha=alphas[i])\n    plt.legend()\n    plt.show()\n\n\ndef plot_model_roc_curve(clf, features, true_labels, label_encoder=None, class_names=None):\n    \n    ## Compute ROC curve and ROC area for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    if hasattr(clf, \'classes_\'):\n        class_labels = clf.classes_\n    elif label_encoder:\n        class_labels = label_encoder.classes_\n    elif class_names:\n        class_labels = class_names\n    else:\n        raise ValueError(\'Unable to derive prediction classes, please specify class_names!\')\n    n_classes = len(class_labels)\n    y_test = label_binarize(true_labels, classes=class_labels)\n    if n_classes == 2:\n        if hasattr(clf, \'predict_proba\'):\n            prob = clf.predict_proba(features)\n            y_score = prob[:, prob.shape[1]-1] \n        elif hasattr(clf, \'decision_function\'):\n            prob = clf.decision_function(features)\n            y_score = prob[:, prob.shape[1]-1]\n        else:\n            raise AttributeError(""Estimator doesn\'t have a probability or confidence scoring system!"")\n        \n        fpr, tpr, _ = roc_curve(y_test, y_score)      \n        roc_auc = auc(fpr, tpr)\n        plt.plot(fpr, tpr, label=\'ROC curve (area = {0:0.2f})\'\n                                 \'\'.format(roc_auc),\n                 linewidth=2.5)\n        \n    elif n_classes > 2:\n        if hasattr(clf, \'predict_proba\'):\n            y_score = clf.predict_proba(features)\n        elif hasattr(clf, \'decision_function\'):\n            y_score = clf.decision_function(features)\n        else:\n            raise AttributeError(""Estimator doesn\'t have a probability or confidence scoring system!"")\n\n        for i in range(n_classes):\n            fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n            roc_auc[i] = auc(fpr[i], tpr[i])\n\n        ## Compute micro-average ROC curve and ROC area\n        fpr[""micro""], tpr[""micro""], _ = roc_curve(y_test.ravel(), y_score.ravel())\n        roc_auc[""micro""] = auc(fpr[""micro""], tpr[""micro""])\n\n        ## Compute macro-average ROC curve and ROC area\n        # First aggregate all false positive rates\n        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n        # Then interpolate all ROC curves at this points\n        mean_tpr = np.zeros_like(all_fpr)\n        for i in range(n_classes):\n            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n        # Finally average it and compute AUC\n        mean_tpr /= n_classes\n        fpr[""macro""] = all_fpr\n        tpr[""macro""] = mean_tpr\n        roc_auc[""macro""] = auc(fpr[""macro""], tpr[""macro""])\n\n        ## Plot ROC curves\n        plt.figure(figsize=(6, 4))\n        plt.plot(fpr[""micro""], tpr[""micro""],\n                 label=\'micro-average ROC curve (area = {0:0.2f})\'\n                       \'\'.format(roc_auc[""micro""]), linewidth=3)\n\n        plt.plot(fpr[""macro""], tpr[""macro""],\n                 label=\'macro-average ROC curve (area = {0:0.2f})\'\n                       \'\'.format(roc_auc[""macro""]), linewidth=3)\n\n        for i, label in enumerate(class_labels):\n            plt.plot(fpr[i], tpr[i], label=\'ROC curve of class {0} (area = {1:0.2f})\'\n                                           \'\'.format(label, roc_auc[i]), \n                     linewidth=2, linestyle=\':\')\n    else:\n        raise ValueError(\'Number of classes should be atleast 2 or more\')\n        \n    plt.plot([0, 1], [0, 1], \'k--\')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel(\'False Positive Rate\')\n    plt.ylabel(\'True Positive Rate\')\n    plt.title(\'Receiver Operating Characteristic (ROC) Curve\')\n    plt.legend(loc=""lower right"")\n    plt.show()\n\n\n'"
notebooks/Ch01_Machine_Learning_Basics/math_basics.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Wed May 10 18:00:55 2017\n\n@author: Dipanjan\n""""""\n\n\n## vectors\n\nx = [1, 2, 3, 4, 5]\nx\n\n# using numpy\nimport numpy as np\nx = np.array([1, 2, 3, 4, 5])\n\nprint(x)\nprint(type(x))\n\n\n\n## matrices\n\nm = np.array([[1, 5, 2],\n              [4, 7, 4],\n              [2, 0, 9]])\n\n# view matrix\nprint(m)\n\n# view dimensions\nprint(m.shape)\n\n\n# matrix transpose\nprint(\'Matrix Transpose:\\n\', m.transpose(), \'\\n\')\n\n# matrix determinant\nprint (\'Matrix Determinant:\', np.linalg.det(m), \'\\n\')\n\n# matrix inverse\nm_inv = np.linalg.inv(m)\nprint (\'Matrix inverse:\\n\', m_inv, \'\\n\')\n\n# identity matrix (result of matrix x matrix_inverse)\niden_m =  np.dot(m, m_inv)\niden_m = np.round(np.abs(iden_m), 0)\nprint (\'Product of matrix and its inverse:\\n\', iden_m)\n\n\n# eigendecomposition\nm = np.array([[1, 5, 2],\n              [4, 7, 4],\n              [2, 0, 9]])\n\neigen_vals, eigen_vecs = np.linalg.eig(m)\n\nprint(\'Eigen Values:\', eigen_vals, \'\\n\')\nprint(\'Eigen Vectors:\\n\', eigen_vecs)\n\n\n# SVD\nm = np.array([[1, 5, 2],\n              [4, 7, 4],\n              [2, 0, 9]])\n\nU, S, VT = np.linalg.svd(m)\n\nprint (\'Getting SVD outputs:-\\n\')\nprint(\'U:\\n\', U, \'\\n\')\nprint(\'S:\\n\', S, \'\\n\')\nprint(\'VT:\\n\', VT, \'\\n\')\n\n\n\n\n\n# descriptive statistics\nimport scipy as sp\nimport numpy as np\n\n# get data\nnums = np.random.randint(1,20, size=(1,15))[0]\nprint(\'Data: \', nums)\n\n# get descriptive stats\nprint (\'Mean:\', sp.mean(nums))\nprint (\'Median:\', sp.median(nums))\nprint (\'Mode:\', sp.stats.mode(nums))\nprint (\'Standard Deviation:\', sp.std(nums))\nprint (\'Variance:\', sp.var(nums))\nprint (\'Skew:\', sp.stats.skew(nums))\nprint (\'Kurtosis:\', sp.stats.kurtosis(nums))\n\n'"
notebooks/Ch01_Machine_Learning_Basics/nlp_example.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sat May 13 01:29:33 2017\n\n@author: DIP\n""""""\n\nfrom nltk.parse.stanford import StanfordParser\n\nsentence = \'The quick brown fox jumps over the lazy dog\'\n\n# create parser object\nscp = StanfordParser(path_to_jar=\'E:/stanford/stanford-parser-full-2015-04-20/stanford-parser.jar\',\n                   path_to_models_jar=\'E:/stanford/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar\')\n\n\n# get parse tree\nresult = list(scp.raw_parse(sentence)) \ntree = result[0]\n\n# print the constituency parse tree\nprint(tree) \n\n# visualize constituency parse tree\ntree.draw() \n'"
notebooks/Ch02_The_Python_ML_Ecosystem/python_ml_ecosystem.py,2,"b'\n# coding: utf-8\n\n# # Numpy Introduction\n# ## numpy arrays\n\n# In[91]:\n\nimport numpy as np\narr = np.array([1,3,4,5,6])\narr\n\n\n# In[8]:\n\narr.shape\n\n\n# In[9]:\n\narr.dtype\n\n\n# In[10]:\n\narr = np.array([1,\'st\',\'er\',3])\narr.dtype\n\n\n# In[5]:\n\nnp.sum(arr)\n\n\n# ### Creating arrays\n\n# In[11]:\n\narr = np.array([[1,2,3],[2,4,6],[8,8,8]])\narr.shape\n\n\n# In[12]:\n\narr\n\n\n# In[13]:\n\narr = np.zeros((2,4))\narr\n\n\n# In[14]:\n\narr = np.ones((2,4))\narr\n\n\n# In[15]:\n\narr = np.identity(3)\narr\n\n\n# In[16]:\n\narr = np.random.randn(3,4)\narr\n\n\n# In[17]:\n\nfrom io import BytesIO\nb = BytesIO(b""2,23,33\\n32,42,63.4\\n35,77,12"")\narr = np.genfromtxt(b, delimiter="","")\narr\n\n\n# ### Accessing array elements\n# #### Simple indexing\n\n# In[18]:\n\narr[1]\n\n\n# In[19]:\n\narr = np.arange(12).reshape(2,2,3)\narr\n\n\n# In[20]:\n\narr[0]\n\n\n# In[21]:\n\narr = np.arange(10)\narr[5:]\n\n\n# In[22]:\n\narr[5:8]\n\n\n# In[23]:\n\narr[:-5]\n\n\n# In[24]:\n\narr = np.arange(12).reshape(2,2,3)\narr\n\n\n# In[25]:\n\narr[1:2]\n\n\n# In[26]:\n\narr = np.arange(27).reshape(3,3,3)\narr\n\n\n# In[27]:\n\narr[:,:,2]\n\n\n# In[28]:\n\narr[...,2]\n\n\n# #### Advanced Indexing\n\n# In[29]:\n\narr = np.arange(9).reshape(3,3)\narr\n\n\n# In[30]:\n\narr[[0,1,2],[1,0,0]]\n\n\n# ##### Boolean Indexing\n\n# In[31]:\n\ncities = np.array([""delhi"",""banglaore"",""mumbai"",""chennai"",""bhopal""])\ncity_data = np.random.randn(5,3)\ncity_data\n\n\n# In[32]:\n\ncity_data[cities ==""delhi""]\n\n\n# In[33]:\n\ncity_data[city_data >0]\n\n\n# In[34]:\n\ncity_data[city_data >0] = 0\ncity_data\n\n\n# #### Operations on arrays\n\n# In[35]:\n\narr = np.arange(15).reshape(3,5)\narr\n\n\n# In[36]:\n\narr + 5\n\n\n# In[37]:\n\narr * 2\n\n\n# In[38]:\n\narr1 = np.arange(15).reshape(5,3)\narr2 = np.arange(5).reshape(5,1)\narr2 + arr1\n\n\n# In[39]:\n\narr1\n\n\n# In[40]:\n\narr2\n\n\n# In[41]:\n\narr1 = np.random.randn(5,3)\narr1\n\n\n# In[42]:\n\nnp.modf(arr1)\n\n\n# #### Linear algebra using numpy\n\n# In[43]:\n\nA = np.array([[1,2,3],[4,5,6],[7,8,9]])\nB = np.array([[9,8,7],[6,5,4],[1,2,3]])\nA.dot(B)\n\n\n# In[44]:\n\nA = np.arange(15).reshape(3,5)\nA.T\n\n\n# In[45]:\n\nnp.linalg.svd(A)\n\n\n# In[46]:\n\na = np.array([[7,5,-3], [3,-5,2],[5,3,-7]])\nb = np.array([16,-8,0])\nx = np.linalg.solve(a, b)\nx\n\n\n# In[47]:\n\nnp.allclose(np.dot(a, x), b)\n\n\n# # Pandas\n# ## Data frames\n\n# In[48]:\n\nimport pandas as pd\nd =  [{\'city\':\'Delhi\',""data"":1000},\n      {\'city\':\'Banglaore\',""data"":2000},\n      {\'city\':\'Mumbai\',""data"":1000}]\npd.DataFrame(d)\n\n\n# In[49]:\n\ndf = pd.DataFrame(d)\n\n\n# ### Reading in data\n\n# In[92]:\n\ncity_data = pd.read_csv(filepath_or_buffer=\'simplemaps-worldcities-basic.csv\')\n\n\n# In[93]:\n\ncity_data.head(n=10)\n\n\n# In[55]:\n\ncity_data.tail()\n\n\n# In[56]:\n\nseries_es = city_data.lat\n\n\n# In[57]:\n\ntype(series_es)\n\n\n# In[58]:\n\nseries_es[1:10:2]\n\n\n# In[59]:\n\nseries_es[:7]\n\n\n# In[60]:\n\nseries_es[:-7315]\n\n\n# In[61]:\n\ncity_data[:7]\n\n\n# In[62]:\n\ncity_data.iloc[:5,:4]\n\n\n# In[63]:\n\ncity_data[city_data[\'pop\'] > 10000000][city_data.columns[pd.Series(city_data.columns).str.startswith(\'l\')]]\n\n\n# In[64]:\n\ncity_greater_10mil = city_data[city_data[\'pop\'] > 10000000]\ncity_greater_10mil.rename(columns={\'pop\':\'population\'}, inplace=True)\ncity_greater_10mil.where(city_greater_10mil.population > 15000000)\n\n\n# In[65]:\n\ndf = pd.DataFrame(np.random.randn(8, 3),\ncolumns=[\'A\', \'B\', \'C\'])\n\n\n# ### Operations on dataframes\n\n# In[66]:\n\nnparray = df.values\ntype(nparray)\n\n\n# In[67]:\n\nfrom numpy import nan\ndf.iloc[4,2] = nan\n\n\n# In[68]:\n\ndf\n\n\n# In[69]:\n\ndf.fillna(0)\n\n\n# In[70]:\n\ncolumns_numeric = [\'lat\',\'lng\',\'pop\']\n\n\n# In[71]:\n\ncity_data[columns_numeric].mean()\n\n\n# In[72]:\n\ncity_data[columns_numeric].sum()\n\n\n# In[73]:\n\ncity_data[columns_numeric].count()\n\n\n# In[74]:\n\ncity_data[columns_numeric].median()\n\n\n# In[75]:\n\ncity_data[columns_numeric].quantile(0.8)\n\n\n# In[76]:\n\ncity_data[columns_numeric].sum(axis = 1).head()\n\n\n# In[77]:\n\ncity_data[columns_numeric].describe()\n\n\n# In[78]:\n\ncity_data1 = city_data.sample(3)\n\n\n# ### Concatanating data frames\n\n# In[79]:\n\ncity_data2 = city_data.sample(3)\ncity_data_combine = pd.concat([city_data1,city_data2])\ncity_data_combine\n\n\n# In[80]:\n\ndf1 = pd.DataFrame({\'col1\': [\'col10\', \'col11\', \'col12\', \'col13\'],\n                    \'col2\': [\'col20\', \'col21\', \'col22\', \'col23\'],\n                    \'col3\': [\'col30\', \'col31\', \'col32\', \'col33\'],\n                    \'col4\': [\'col40\', \'col41\', \'col42\', \'col43\']},\n                   index=[0, 1, 2, 3])\n\n\n# In[81]:\n\ndf1\n\n\n# In[82]:\n\ndf4 = pd.DataFrame({\'col2\': [\'col22\', \'col23\', \'col26\', \'col27\'],\n                    \'Col4\': [\'Col42\', \'Col43\', \'Col46\', \'Col47\'],\n                    \'col6\': [\'col62\', \'col63\', \'col66\', \'col67\']},\n                   index=[2, 3, 6, 7])\n\npd.concat([df1,df4], axis=1)\n\n\n# In[83]:\n\ncountry_data = city_data[[\'iso3\',\'country\']].drop_duplicates()\n\n\n# In[84]:\n\ncountry_data.shape\n\n\n# In[85]:\n\ncountry_data.head()\n\n\n# In[86]:\n\ndel(city_data[\'country\'])\n\n\n# In[87]:\n\ncity_data.merge(country_data, \'inner\').head()\n\n\n# # Scikit-learn\n\n# In[94]:\n\nfrom sklearn import datasets\ndiabetes = datasets.load_diabetes()\nX = diabetes.data[:10]\ny = diabetes.target\n\n\n# In[95]:\n\nX[:5]\n\n\n# In[96]:\n\ny[:10]\n\n\n# In[97]:\n\nfeature_names=[\'age\', \'sex\', \'bmi\', \'bp\',\n               \'s1\', \'s2\', \'s3\', \'s4\', \'s5\', \'s6\']\n\n\n# ## Scikit example regression\n\n# In[98]:\n\nfrom sklearn import datasets\nfrom sklearn.linear_model import Lasso\n\nfrom sklearn import linear_model, datasets\nfrom sklearn.model_selection import GridSearchCV\n\ndiabetes = datasets.load_diabetes()\nX_train = diabetes.data[:310]\ny_train = diabetes.target[:310]\n\nX_test = diabetes.data[310:]\ny_test = diabetes.target[310:]\n\nlasso = Lasso(random_state=0)\nalphas = np.logspace(-4, -0.5, 30)\n\nscores = list()\nscores_std = list()\n\nestimator = GridSearchCV(lasso,\n                         param_grid = dict(alpha=alphas))\n\nestimator.fit(X_train, y_train)\n\n\n# In[99]:\n\nestimator.best_score_\n\n\n# In[100]:\n\nestimator.best_estimator_\n\n\n# In[101]:\n\nestimator.predict(X_test)\n\n\n# ## Deep Learning Frameworks\n\n# ### Theano example \n\n# In[1]:\n\nimport numpy\nimport theano.tensor as T\nfrom theano import function\nx = T.dscalar(\'x\')\ny = T.dscalar(\'y\')\nz = x + y\n\n\n# In[2]:\n\nf = function([x, y], z)\nf(8, 2)\n\n\n# ### Tensorflow example\n\n# In[102]:\n\nimport tensorflow as tf\nhello = tf.constant(\'Hello, TensorFlow!\')\nsess = tf.Session()\nprint(sess.run(hello))\n\n\n# ### Building a neural network model with Keras\n\n# In[103]:\n\nfrom sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\n\nX_train = cancer.data[:340]\ny_train = cancer.target[:340]\n\nX_test = cancer.data[340:]\ny_test = cancer.target[340:]\n\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n\n\n# In[150]:\n\nmodel = Sequential()\nmodel.add(Dense(15, input_dim=30, activation=\'relu\'))\nmodel.add(Dense(1, activation=\'sigmoid\'))\n\n\n# In[151]:\n\nmodel.compile(loss=\'binary_crossentropy\',\n              optimizer=\'rmsprop\',\n              metrics=[\'accuracy\'])\n\n\n# In[152]:\n\nmodel.fit(X_train, y_train,\n          epochs=20,\n          batch_size=50)\n\n\n# In[153]:\n\npredictions = model.predict_classes(X_test)\n\n\n# In[154]:\n\nfrom sklearn import metrics\n\nprint(\'Accuracy:\', metrics.accuracy_score(y_true=y_test, y_pred=predictions))\nprint(metrics.classification_report(y_true=y_test, y_pred=predictions))\n\n\n# ### The power of deep learning models\n\n# In[155]:\n\nmodel = Sequential()\nmodel.add(Dense(15, input_dim=30, activation=\'relu\'))\nmodel.add(Dense(15, activation=\'relu\'))\nmodel.add(Dense(15, activation=\'relu\'))\nmodel.add(Dense(1, activation=\'sigmoid\'))\n\nmodel.compile(loss=\'binary_crossentropy\',\n              optimizer=\'rmsprop\',\n              metrics=[\'accuracy\'])\n\nmodel.fit(X_train, y_train,\n          epochs=20,\n          batch_size=50)\n\n\n# In[156]:\n\npredictions = model.predict_classes(X_test)\n\n\n# In[157]:\n\nprint(\'Accuracy:\', metrics.accuracy_score(y_true=y_test, y_pred=predictions))\nprint(metrics.classification_report(y_true=y_test, y_pred=predictions))\n\n'"
notebooks/Ch03_Processing_Wrangling_and_Visualizing_Data/crawl_basic.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Wed May 10 19:39:39 2017\n\n@author: Raghav Bali\n""""""\n\n""""""\n\nThis script crawls apress.com\'s blog post to:\n    + extract content related to blog post using regex\n\n``Execute``\n        $ python crawl_basic.py\n\n""""""\n\nimport re\nimport requests\n\ndef extract_blog_content(content):\n    """"""This function extracts blog post content using regex\n\n    Args:\n        content (request.content): String content returned from requests.get\n\n    Returns:\n        str: string content as per regex match\n\n    """"""\n    content_pattern = re.compile(r\'<div class=""cms-richtext"">(.*?)</div>\')\n    result = re.findall(content_pattern, content)\n    return result[0] if result else ""None""\n    \n\nif __name__ ==\'__main__\':\n    \n    base_url = ""http://www.apress.com/in/blog/all-blog-posts""\n    blog_suffix = ""/wannacry-how-to-prepare/12302194""\n    \n    print(""Crawling Apress.com for required blog post...\\n\\n"")    \n    \n    response = requests.get(base_url+blog_suffix)\n    \n    if response.status_code == 200:\n        content = response.text.encode(\'utf-8\', \'ignore\').decode(\'utf-8\', \'ignore\')\n        content = content.replace(""\\n"", \'\')\n        blog_post_content = extract_blog_content(content)\n        '"
notebooks/Ch03_Processing_Wrangling_and_Visualizing_Data/crawl_bs.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Wed May 10 18:01:41 2017\n\n@author: Raghav Bali\n""""""\n\n""""""\n\nThis script crawls apress.com\'s blog page to:\n    + extract list of recent blog post titles and their URLS\n    + extract content related to each blog post in plain text\n\nusing requests and BeautifulSoup packages\n\n``Execute``\n        $ python crawl_bs.py\n\n""""""\n\n\nimport requests\nfrom time import sleep\nfrom bs4 import BeautifulSoup\n\n\ndef get_post_mapping(content):\n    """"""This function extracts blog post title and url from response object\n\n    Args:\n        content (request.content): String content returned from requests.get\n\n    Returns:\n        list: a list of dictionaries with keys title and url\n\n    """"""\n    post_detail_list = []\n    post_soup = BeautifulSoup(content,""lxml"")\n    h3_content = post_soup.find_all(""h3"")\n    \n    for h3 in h3_content:\n        post_detail_list.append(\n            {\'title\':h3.a.get_text(),\'url\':h3.a.attrs.get(\'href\')}\n            )\n    \n    return post_detail_list\n\n\ndef get_post_content(content):\n    """"""This function extracts blog post content from response object\n\n    Args:\n        content (request.content): String content returned from requests.get\n\n    Returns:\n        str: blog\'s content in plain text\n\n    """"""\n    plain_text = """"\n    text_soup = BeautifulSoup(content,""lxml"")\n    para_list = text_soup.find_all(""div"",\n                                   {\'class\':\'cms-richtext\'})\n    \n    for p in para_list[0]:\n        plain_text += p.getText()\n    \n    return plain_text\n    \n    \n\nif __name__ ==\'__main__\':\n    \n    crawl_url = ""http://www.apress.com/in/blog/all-blog-posts""\n    post_url_prefix = ""http://www.apress.com""\n    \n    print(""Crawling Apress.com for recent blog posts...\\n\\n"")    \n    \n    response = requests.get(crawl_url)\n    \n    if response.status_code == 200:\n        blog_post_details = get_post_mapping(response.content)\n    \n    if blog_post_details:\n        print(""Blog posts found:{}"".format(len(blog_post_details)))\n        \n        for post in blog_post_details:\n            print(""Crawling content for post titled:"",post.get(\'title\'))\n            post_response = requests.get(post_url_prefix+post.get(\'url\'))\n            \n            if post_response.status_code == 200:\n                post[\'content\'] = get_post_content(post_response.content)\n            \n            print(""Waiting for 10 secs before crawling next post...\\n\\n"")\n            sleep(10)\n    \n        print(""Content crawled for all posts"")\n        \n        # print/write content to file\n        for post in blog_post_details:\n            print(post)\n    '"
notebooks/Ch03_Processing_Wrangling_and_Visualizing_Data/matplotlib_viz.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sun Jun 11 09:56:39 2017\n\n@author: Raghav Bali\n""""""\n\n""""""\n\nThis script visualizes data using matplotlib  \n\n``Execute``\n        $ python matplotlib_viz.py\n\n""""""\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nif __name__==\'__main__\':\n    \n    # sample plot\n    x = np.linspace(-10, 10, 50)\n    y=np.sin(x)\n    \n    plt.plot(x,y)\n    plt.title(\'Sine Curve using matplotlib\')\n    plt.xlabel(\'x-axis\')\n    plt.ylabel(\'y-axis\')\n    plt.show()\n    \n    \n    # figure\n    plt.figure(1)\n    plt.plot(x,y)\n    plt.title(\'Fig1: Sine Curve\')\n    plt.xlabel(\'x-axis\')\n    plt.ylabel(\'y-axis\')\n    plt.show()\n    \n    plt.figure(2)\n    y=np.cos(x)\n    plt.plot(x,y)\n    plt.title(\'Fig2: Cosine Curve\')\n    plt.xlabel(\'x-axis\')\n    plt.ylabel(\'y-axis\')\n    plt.show()\n    \n    ### subplot\n    \n    # fig.add_subplot\n    y = np.sin(x)\n    figure_obj = plt.figure()\n    ax1 = figure_obj.add_subplot(2,2,1)\n    ax1.plot(x,y)\n\n    ax2 = figure_obj.add_subplot(2,2,2)\n    ax3 = figure_obj.add_subplot(2,2,3)\n    \n    ax4 = figure_obj.add_subplot(2,2,4)\n    ax4.plot(x+10,y)\n    plt.show()\n   \n    \n    # plt.subplots\n    fig, ax_list = plt.subplots(2,1,sharex=True)\n    y= np.sin(x)\n    ax_list[0].plot(x,y)\n    \n    y= np.cos(x)\n    ax_list[1].plot(x,y)\n    plt.show()\n    \n    \n    # plt.subplot (creates figure and axes objects automatically)\n    plt.subplot(2,2,1)\n    y = np.sin(x)    \n    plt.plot(x,y)\n\n    plt.subplot(2,2,2)\n    y = np.cos(x)\n    plt.plot(x,y)\n\n    plt.subplot(2,1,2)\n    y = np.tan(x)\n    plt.plot(x,y)  \n    \n    plt.show()\n    \n    \n    # subplot2grid\n    y = np.abs(x)\n    z = x**2\n    \n    plt.subplot2grid((4,3), (0, 0), rowspan=4, colspan=2)\n    plt.plot(x, y,\'b\',x,z,\'r\')\n    \n    ax2 = plt.subplot2grid((4,3), (0, 2),rowspan=2)\n    plt.plot(x, y,\'b\')\n    plt.setp(ax2.get_xticklabels(), visible=False)\n\n    plt.subplot2grid((4,3), (2, 2), rowspan=2)\n    plt.plot(x, z,\'r\')\n    \n    plt.show()\n    \n      \n    ### formatting\n    \n    y = x\n    \n    # color\n    ax1 = plt.subplot(611)\n    plt.plot(x,y,color=\'green\')\n    ax1.set_title(\'Line Color\')\n    plt.setp(ax1.get_xticklabels(), visible=False)\n    \n    # linestyle\n    # linestyles -> \'-\',\'--\',\'-.\', \':\', \'steps\'\n    ax2 = plt.subplot(612,sharex=ax1)\n    plt.plot(x,y,linestyle=\'--\')\n    ax2.set_title(\'Line Style\')\n    plt.setp(ax2.get_xticklabels(), visible=False)\n    \n    # marker\n    # markers -> \'+\', \'o\', \'*\', \'s\', \',\', \'.\', etc\n    ax3 = plt.subplot(613,sharex=ax1)\n    plt.plot(x,y,marker=\'*\')\n    ax3.set_title(\'Point Marker\')\n    plt.setp(ax3.get_xticklabels(), visible=False)\n    \n    # line width\n    ax4 = plt.subplot(614,sharex=ax1)\n    line = plt.plot(x,y)\n    line[0].set_linewidth(3.0)\n    ax4.set_title(\'Line Width\')\n    plt.setp(ax4.get_xticklabels(), visible=False)\n    \n    # alpha\n    ax5 = plt.subplot(615,sharex=ax1)\n    alpha = plt.plot(x,y)\n    alpha[0].set_alpha(0.3)\n    ax5.set_title(\'Line Alpha\')\n    plt.setp(ax5.get_xticklabels(), visible=False)\n    \n    # combine linestyle\n    ax6 = plt.subplot(616,sharex=ax1)\n    plt.plot(x,y,\'b^\')\n    ax6.set_title(\'Styling Shorthand\')\n    \n    fig = plt.gcf()\n    fig.set_figheight(15)\n    plt.show()\n    \n    \n    # legends\n    y = x**2\n    z = x\n    \n    plt.plot(x,y,\'g\',label=\'y=x^2\')\n    plt.plot(x,z,\'b:\',label=\'y=x\')\n    plt.legend(loc=""best"")\n    plt.title(\'Legend Sample\')\n    plt.show()\n    \n    # legend with latex formatting\n    plt.plot(x,y,\'g\',label=\'$y = x^2$\')\n    plt.plot(x,z,\'b:\',linewidth=3,label=\'$y = x^2$\')\n    plt.legend(loc=""best"",fontsize=\'x-large\')\n    plt.title(\'Legend with LaTEX formatting\')\n    plt.show()\n    \n    \n    ## axis controls\n    # secondary y-axis\n    fig, ax1 = plt.subplots()\n    ax1.plot(x,y,\'g\')\n    ax1.set_ylabel(r""primary y-axis"", color=""green"")\n    \n    ax2 = ax1.twinx()\n    \n    ax2.plot(x,z,\'b:\',linewidth=3)\n    ax2.set_ylabel(r""secondary y-axis"", color=""blue"")\n    \n    plt.title(\'Secondary Y Axis\')\n    plt.show()    \n    \n    # ticks\n    y = np.log(x)\n    z = np.log2(x)\n    w = np.log10(x)\n    \n    plt.plot(x,y,\'r\',x,z,\'g\',x,w,\'b\')\n    plt.title(\'Default Axis Ticks\') \n    plt.show()       \n    \n    # axis-controls\n    plt.plot(x,y,\'r\',x,z,\'g\',x,w,\'b\')\n    # values: tight, scaled, equal,auto\n    plt.axis(\'tight\')\n    plt.title(\'Tight Axis\') \n    plt.show()\n\n    # manual\n    plt.plot(x,y,\'r\',x,z,\'g\',x,w,\'b\')\n    plt.axis([0,2,-1,2])\n    plt.title(\'Manual Axis Range\') \n    plt.show()       \n        \n    # Manual ticks      \n    plt.plot(x, y)\n    ax = plt.gca()\n    ax.xaxis.set_ticks(np.arange(-2, 2, 1))\n    plt.grid(True)\n    plt.title(""Manual ticks on the x-axis"")\n    plt.show()\n    \n    \n    # minor ticks\n    plt.plot(x, z)\n    plt.minorticks_on()\n    ax = plt.gca()\n    ax.yaxis.set_ticks(np.arange(0, 5))\n    ax.yaxis.set_ticklabels([""min"", 2, 4, ""max""])\n    plt.title(""Minor ticks on the y-axis"")   \n    plt.show()\n        \n    \n    # scaling\n    plt.plot(x, y)\n    ax = plt.gca()\n    # values: log, logit, symlog\n    ax.set_yscale(""log"")\n    plt.grid(True)\n    plt.title(""Log Scaled Axis"")\n    plt.show()\n    \n    \n    # annotations\n    y = x**2\n    min_x = 0\n    min_y = min_x**2\n    \n    plt.plot(x, y, ""b-"", min_x, min_y, ""ro"")\n    plt.axis([-10,10,-25,100])\n    \n    plt.text(0, 60, ""Parabola\\n$y = x^2$"", fontsize=15, ha=""center"")\n    plt.text(min_x, min_y+2, ""Minima"", ha=""center"")\n    plt.text(min_x, min_y-6, ""(%0.1f, %0.1f)""%(min_x, min_y), ha=\'center\',color=\'gray\')\n    plt.title(""Annotated Plot"")\n    plt.show()\n    \n    \n    # global formatting params\n    params = {\'legend.fontsize\': \'large\',\n              \'figure.figsize\': (10, 10),\n             \'axes.labelsize\': \'large\',\n             \'axes.titlesize\':\'large\',\n             \'xtick.labelsize\':\'large\',\n             \'ytick.labelsize\':\'large\'}\n\n    plt.rcParams.update(params)\n    \n    \n    # saving\n    #plt.savefig(""sample_plot.png"", transparent=True)'"
notebooks/Ch03_Processing_Wrangling_and_Visualizing_Data/read_csv.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Wed May 10 19:51:47 2017\n\n@author: Raghav Bali\n""""""\n\n""""""\n\nThis script showcases methods to extract data from CSVs:\n    + csv containing delimiter separated values\n    + csv containing tabular data\n\nusing csv and pandas packages\n\n``Execute``\n        $ python read_csv.py\n\n""""""\n\nimport csv\nimport pandas as pd\nfrom pprint import pprint\n\ndef print_basic_csv(file_name, delimiter=\',\'):\n    """"""This function extracts and prints csv content from given filename\n       Details: https://docs.python.org/2/library/csv.html\n    Args:\n        file_name (str): file path to be read\n        delimiter (str): delimiter used in csv. Default is comma (\',\')\n\n    Returns:\n        None\n\n    """"""\n    csv_rows = list()\n    csv_attr_dict = dict()\n    csv_reader = None\n\n    # read csv\n    csv_reader = csv.reader(open(file_name, \'r\'), delimiter=delimiter)\n        \n    # iterate and extract data    \n    for row in csv_reader:\n        print(row)\n        csv_rows.append(row)\n    \n    # prepare attribute lists\n    for col in csv_rows[0]:\n        csv_attr_dict[col]=list()\n    \n    # iterate and add data to attribute lists\n    for row in csv_rows[1:]:\n        csv_attr_dict[\'sno\'].append(row[0])\n        csv_attr_dict[\'fruit\'].append(row[1])\n        csv_attr_dict[\'color\'].append(row[2])\n        csv_attr_dict[\'price\'].append(row[3])\n    \n    # print the result\n    print(""\\n\\n"")\n    print(""CSV Attributes::"")\n    pprint(csv_attr_dict)\n            \n\n\ndef print_tabular_data(file_name,delimiter="",""):\n    """"""This function extracts and prints tabular csv content from given filename\n       Details: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\n    Args:\n        file_name (str): file path to be read\n        delimiter (str): delimiter used in csv. Default is comma (\'\\t\')\n\n    Returns:\n        None\n\n    """"""\n    df = pd.read_csv(file_name,sep=delimiter)\n    print(df)\n    \n    \n    \nif __name__==\'__main__\':\n    print(""\\n\\n"")\n    print(""*""*30)\n    print(""Contents of sample csv file:"")\n    print(""*""*30)\n    print_basic_csv(r\'tabular_csv.csv\')\n    \n    print(""\\n\\n"")\n    print(""*""*30)\n    print(""Contents of a tabular csv file:"")\n    print(""*""*30)\n    print_tabular_data(r\'tabular_csv.csv\')'"
notebooks/Ch03_Processing_Wrangling_and_Visualizing_Data/read_json.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Wed May 10 19:51:47 2017\n\n@author: Raghav Bali\n""""""\n\n""""""\n\nThis script showcases methods to read JSON type data:\n    + using python\'s inbuilt utilities\n    + using pandas\n\n\n``Execute``\n        $ python read_json.py\n\n""""""\n\nimport json\nimport pandas as pd\n\n\ndef print_nested_dicts(nested_dict,indent_level=0):\n    """"""This function prints a nested dict object\n    Args:\n        nested_dict (dict): the dictionary to be printed\n        indent_level (int): the indentation level for nesting\n    Returns:\n        None\n\n    """"""\n    \n    for key, val in nested_dict.items():\n        if isinstance(val, dict):\n          print(""{0} : "".format(key))\n          print_nested_dicts(val,indent_level=indent_level+1)\n        elif isinstance(val,list):\n            print(""{0} : "".format(key))\n            for rec in val:\n                print_nested_dicts(rec,indent_level=indent_level+1)\n        else:\n          print(""{0}{1} : {2}"".format(""\\t""*indent_level,key, val))\n\ndef extract_json(file_name,do_print=True):\n    """"""This function extracts and prints json content from a given file\n    Args:\n        file_name (str): file path to be read\n        do_print (bool): boolean flag to print file contents or not\n\n    Returns:\n        None\n\n    """"""\n    try:\n        json_filedata = open(file_name).read() \n        json_data = json.loads(json_filedata)\n        \n        if do_print:\n            print_nested_dicts(json_data)\n    except IOError:\n        raise IOError(""File path incorrect/ File not found"")\n    except ValueError:\n        ValueError(""JSON file has errors"")\n    except Exception:\n        raise\n\ndef extract_pandas_json(file_name,orientation=""records"",do_print=True):\n    """"""This function extracts and prints json content from a file using pandas\n       This is useful when json data represents tabular, series information\n    Args:\n        file_name (str): file path to be read\n        orientation (str): orientation of json file. Defaults to records\n        do_print (bool): boolean flag to print file contents or not\n\n    Returns:\n        None\n\n    """"""\n    try:\n        df = pd.read_json(file_name,orient=orientation)\n        \n        if do_print:\n            print(df)\n    except IOError:\n        raise IOError(""File path incorrect/ File not found"")\n    except ValueError:\n        ValueError(""JSON file has errors"")\n    except Exception:\n        raise\n    \n    \n    \nif __name__==\'__main__\':\n    print(""\\n\\n"")\n    print(""*""*30)\n    print(""Contents of sample json file:"")\n    print(""*""*30)\n    extract_json(r\'sample_json.json\')\n    \n    print(""\\n\\n"")\n    print(""*""*30)\n    print(""Contents of a json file using pandas:"")\n    print(""*""*30)\n    extract_pandas_json(r\'pandas_json.json\')'"
notebooks/Ch03_Processing_Wrangling_and_Visualizing_Data/read_xml.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Wed May 10 19:51:47 2017\n\n@author: Raghav Bali\n""""""\n\n""""""\n\nThis script showcases methods to read XML type data using:\n    + powerful xml library\n    + xml to dict\n\n``Execute``\n        $ python read_xml.py\n\n""""""\n\nimport xml.etree.ElementTree as ET\nimport xmltodict\n\n\n\ndef print_nested_dicts(nested_dict,indent_level=0):\n    """"""This function prints a nested dict object\n    Args:\n        nested_dict (dict): the dictionary to be printed\n        indent_level (int): the indentation level for nesting\n    Returns:\n        None\n\n    """"""\n    \n    for key, val in nested_dict.items():\n        if isinstance(val, dict):\n          print(""{0} : "".format(key))\n          print_nested_dicts(val,indent_level=indent_level+1)\n        elif isinstance(val,list):\n            print(""{0} : "".format(key))\n            for rec in val:\n                print_nested_dicts(rec,indent_level=indent_level+1)\n        else:\n          print(""{0}{1} : {2}"".format(""\\t""*indent_level,key, val))\n          \ndef print_xml_tree(xml_root,indent_level=0):\n    """"""This function prints a nested dict object\n    Args:\n        xml_root (dict): the xml tree to be printed\n        indent_level (int): the indentation level for nesting\n    Returns:\n        None\n\n    """"""\n    for child in xml_root:\n            print(""{0}tag:{1}, attribute:{2}"".format(\n                                                ""\\t""*indent_level,\n                                                child.tag,\n                                                child.attrib))\n                                                \n            print(""{0}tag data:{1}"".format(""\\t""*indent_level,\n                                            child.text))\n                                            \n            print_xml_tree(child,indent_level=indent_level+1)\n            \n\n\ndef read_xml(file_name):\n    """"""This function extracts and prints XML content from a given file\n    Args:\n        file_name (str): file path to be read\n    Returns:\n        None\n\n    """"""\n    try:\n        tree = ET.parse(file_name)\n        root = tree.getroot()\n        \n        print(""Root tag:{0}"".format(root.tag))\n        print(""Attributes of Root:: {0}"".format(root.attrib))\n        \n        print_xml_tree(root)\n            \n    except IOError:\n        raise IOError(""File path incorrect/ File not found"")\n    except Exception:\n        raise\n\n    \n\ndef read_xml2dict_xml(file_name):\n    """"""This function extracts and prints xml content from a file using xml2dict\n    Args:\n        file_name (str): file path to be read\n    Returns:\n        None\n\n    """"""\n    try:\n        xml_filedata = open(file_name).read() \n        ordered_dict = xmltodict.parse(xml_filedata)\n        \n        print_nested_dicts(ordered_dict)\n    except IOError:\n        raise IOError(""File path incorrect/ File not found"")\n    except ValueError:\n        ValueError(""XML file has errors"")\n    except Exception:\n        raise    \n    \nif __name__==\'__main__\':\n    print(""\\n\\n"")\n    print(""*""*30)\n    print(""Contents of sample xml file:"")\n    print(""*""*30)\n    read_xml(r\'sample_xml.xml\')\n    \n    print(""\\n\\n"")\n    print(""*""*30)\n    print(""Contents of a xml file using xml2dict:"")\n    print(""*""*30)\n    read_xml2dict_xml(r\'sample_xml.xml\')'"
notebooks/Ch03_Processing_Wrangling_and_Visualizing_Data/viz_data.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Wed May 10 19:51:47 2017\n\n@author: RAGHAV\n""""""\n\n""""""\n\nThis script showcases following data viz using :\n    + pandas\n        + plots and sub plots\n        + hist\n        + box\n        + scatter\n        + timeseries\n    + matplotlib\n\n``Execute``\n        $ viz_data.py\n\n""""""\n\nimport random\nimport datetime \nimport numpy as np\nimport pandas as pd\nfrom random import randrange\n\n\nimport matplotlib.pyplot as plt\n\nparams = {\'legend.fontsize\': \'large\',\n          \'figure.figsize\': (10, 10),\n         \'axes.labelsize\': \'large\',\n         \'axes.titlesize\':\'large\',\n         \'xtick.labelsize\':\'large\',\n         \'ytick.labelsize\':\'large\'}\n\nplt.rcParams.update(params)\n\n\n\ndef _random_date(start,date_count):\n    """"""This function generates a random date based on params\n    Args:\n        start (date object): the base date\n        date_count (int): number of dates to be generated\n    Returns:\n        list of random dates\n\n    """"""\n    current = start\n    while date_count > 0:\n        curr = current + datetime.timedelta(days=randrange(42))\n        yield curr\n        date_count-=1\n\n\ndef generate_sample_data(row_count=100):\n    """"""This function generates a random transaction dataset\n    Args:\n        row_count (int): number of rows for the dataframe\n    Returns:\n        a pandas dataframe\n\n    """"""\n    \n    # sentinels\n    startDate = datetime.datetime(2016, 1, 1,13)\n    serial_number_sentinel = 1000\n    user_id_sentinel = 5001\n    product_id_sentinel = 101\n    price_sentinel = 2000\n    \n    \n    # base list of attributes\n    data_dict = {\n    \'Serial No\': np.arange(row_count)+serial_number_sentinel,\n    \'Date\': np.random.permutation(pd.to_datetime([x.strftime(""%d-%m-%Y"") \n                                                    for x in _random_date(startDate,\n                                                                          row_count)]).date\n                                  ),\n    \'User ID\': np.random.permutation(np.random.randint(0,\n                                                       row_count,\n                                                       size=int(row_count/10)) + user_id_sentinel).tolist()*10,\n    \'Product ID\': np.random.permutation(np.random.randint(0,\n                                                          row_count,\n                                                          size=int(row_count/10)) + product_id_sentinel).tolist()*10 ,\n    \'Quantity Purchased\': np.random.permutation(np.random.randint(1,\n                                                                  42,\n                                                                  size=row_count)),\n    \'Price\': np.round(np.abs(np.random.randn(row_count)+1)*price_sentinel,\n                      decimals=2),\n    \'User Type\':np.random.permutation([chr(random.randrange(97, 97 + 3 + 1)) \n                                            for i in range(row_count)])\n    }\n    \n    # introduce missing values\n    for index in range(int(np.sqrt(row_count))): \n        data_dict[\'Price\'][np.argmax(data_dict[\'Price\'] == random.choice(data_dict[\'Price\']))] = np.nan\n        data_dict[\'User Type\'][np.argmax(data_dict[\'User Type\'] == random.choice(data_dict[\'User Type\']))] = np.nan\n        data_dict[\'Date\'][np.argmax(data_dict[\'Date\'] == random.choice(data_dict[\'Date\']))] = np.nan\n        data_dict[\'Product ID\'][np.argmax(data_dict[\'Product ID\'] == random.choice(data_dict[\'Product ID\']))] = 0\n        data_dict[\'Serial No\'][np.argmax(data_dict[\'Serial No\'] == random.choice(data_dict[\'Serial No\']))] = -1\n        data_dict[\'User ID\'][np.argmax(data_dict[\'User ID\'] == random.choice(data_dict[\'User ID\']))] = -101\n        \n    \n    # create data frame\n    df = pd.DataFrame(data_dict)\n    \n    return df\n    \n    \ndef cleanup_column_names(df,rename_dict={},do_inplace=True):\n    """"""This function renames columns of a pandas dataframe\n       It converts column names to snake case if rename_dict is not passed. \n    Args:\n        rename_dict (dict): keys represent old column names and values point to \n                            newer ones\n        do_inplace (bool): flag to update existing dataframe or return a new one\n    Returns:\n        pandas dataframe if do_inplace is set to False, None otherwise\n\n    """"""\n    if not rename_dict:\n        return df.rename(columns={col: col.lower().replace(\' \',\'_\') \n                    for col in df.columns.values.tolist()}, \n                  inplace=do_inplace)\n    else:\n        return df.rename(columns=rename_dict,inplace=do_inplace)\n\ndef expand_user_type(u_type):\n    """"""This function maps user types to user classes\n    Args:\n        u_type (str): user type value\n    Returns:\n        (str) user_class value\n\n    """"""\n    if u_type in [\'a\',\'b\']:\n        return \'new\'\n    elif u_type == \'c\':\n        return \'existing\'\n    elif u_type == \'d\':\n        return \'loyal_existing\'\n    else:\n        return \'error\'\n            \nif __name__==\'__main__\':\n    df = generate_sample_data(row_count=1000)\n    cleanup_column_names(df)\n\n    df[\'date\'] = pd.to_datetime(df.date)\n\n    \n    df[\'user_class\'] = df[\'user_type\'].map(expand_user_type)\n    \n    df[\'purchase_week\'] = df[[\'date\']].applymap(lambda dt:dt.week \n                                                if not pd.isnull(dt.week) \n                                                else 0)\n\n    df = df.dropna(subset=[\'date\'])\n    df[\'price\'].fillna(value=np.round(df.price.mean(),decimals=2),\n                                inplace=True)\n    \n    ##################################################\n    # PLOTTING BEGINS\n    ##################################################\n    \n    # line\n    max_user_id = df.user_id.value_counts().index[0]\n    df[df.user_id == max_user_id][[\'price\']].plot(style=\'blue\')\n    plt.title(\'Price Trends for Particular User\')\n    plt.show()\n\n    df[df.user_id == max_user_id].plot(x=\'date\',y=\'price\',style=\'blue\')   \n    plt.title(\'Price Trends for Particular User Over Time\')\n    plt.show()\n\n    # bar\n    df[[\'purchase_week\',\n        \'quantity_purchased\']].groupby(\'purchase_week\').sum().plot.barh(\n                                                                color=\'orange\')\n    plt.title(\'Quantities Purchased per Week\')\n    plt.show()\n\n    # hist\n    df.price.hist(color=\'green\')\n    plt.title(\'Price Distribution\')\n    plt.show()\n    \n    df[[\'price\',\'purchase_week\']].hist(by=\'purchase_week\',sharex=True)\n    plt.title(\'Price Distribution per Week\')\n    plt.show()\n\n    #pie\n    class_series = df.groupby(\'user_class\').size()\n    class_series.name = \'User Class Distribution\'\n    class_series.plot.pie(autopct=\'%.2f\')\n    plt.title(\'User Class Share\')\n    plt.show()\n\n    # box\n    df[[\'quantity_purchased\',\'purchase_week\']].plot.box()\n    plt.title(\'Quantity and Week value distribution\')\n    plt.show()\n\n    # bubble\n    uclass_map = {\'new\': 1, \'existing\': 2, \'loyal_existing\': 3,\'error\':0}\n    df[\'enc_uclass\'] = df.user_class.map(uclass_map) \n    bubble_df = df[[\'enc_uclass\',\n                    \'purchase_week\',\n                    \'price\',\'product_id\']].groupby([\'purchase_week\',\n                                                    \'enc_uclass\']).agg({\'price\':\'mean\',\n                                                                        \'product_id\':\'count\'}).reset_index()\n    bubble_df.rename(columns={\'product_id\':\'total_transactions\'},inplace=True)\n                                                            \n    \n    bubble_df.plot.scatter(x=\'purchase_week\',\n                           y=\'price\')\n    plt.title(\'Purchase Week Vs Price \')\n    plt.show() \n                       \n    bubble_df.plot.scatter(x=\'purchase_week\',\n                           y=\'price\',\n                           c=bubble_df[\'enc_uclass\']) \n    plt.title(\'Purchase Week Vs Price Per User Class\')                       \n    plt.show() \n                      \n    bubble_df.plot.scatter(x=\'purchase_week\',\n                           y=\'price\',\n                           c=bubble_df[\'enc_uclass\'],\n                            s=bubble_df[\'total_transactions\']*10)\n    plt.title(\'Purchase Week Vs Price Per User Class Based on Tx\')                          \n    plt.show()\n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    '"
notebooks/Ch03_Processing_Wrangling_and_Visualizing_Data/wrangle_data.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Wed May 10 19:51:47 2017\n\n@author: Raghav Bali\n""""""\n\n""""""\n\nThis script showcases data wrangling tasks \n\n``Execute``\n        $ python wrangle_data.py\n\n""""""\n\nimport random\nimport datetime \nimport numpy as np\nimport pandas as pd\nfrom random import randrange\nfrom sklearn import preprocessing\n\n\n\ndef _random_date(start,date_count):\n    """"""This function generates a random date based on params\n    Args:\n        start (date object): the base date\n        date_count (int): number of dates to be generated\n    Returns:\n        list of random dates\n\n    """"""\n    current = start\n    while date_count > 0:\n        curr = current + datetime.timedelta(days=randrange(42))\n        yield curr\n        date_count-=1\n\n  \ndef generate_sample_data(row_count=100):\n    """"""This function generates a random transaction dataset\n    Args:\n        row_count (int): number of rows for the dataframe\n    Returns:\n        a pandas dataframe\n\n    """"""\n    \n    # sentinels\n    startDate = datetime.datetime(2016, 1, 1,13)\n    serial_number_sentinel = 1000\n    user_id_sentinel = 5001\n    product_id_sentinel = 101\n    price_sentinel = 2000\n    \n    \n    # base list of attributes\n    data_dict = {\n    \'Serial No\': np.arange(row_count)+serial_number_sentinel,\n    \'Date\': np.random.permutation(pd.to_datetime([x.strftime(""%d-%m-%Y"") \n                                                    for x in _random_date(startDate,\n                                                                          row_count)]).date\n                                  ),\n    \'User ID\': np.random.permutation(np.random.randint(0,\n                                                       row_count,\n                                                       size=int(row_count/10)) + user_id_sentinel).tolist()*10,\n    \'Product ID\': np.random.permutation(np.random.randint(0,\n                                                          row_count,\n                                                          size=int(row_count/10))+ product_id_sentinel).tolist()*10 ,\n    \'Quantity Purchased\': np.random.permutation(np.random.randint(1,\n                                                                  42,\n                                                                  size=row_count)),\n    \'Price\': np.round(np.abs(np.random.randn(row_count)+1)*price_sentinel,\n                      decimals=2),\n    \'User Type\':np.random.permutation([chr(random.randrange(97, 97 + 3 + 1)) \n                                            for i in range(row_count)])\n    }\n    \n    # introduce missing values\n    for index in range(int(np.sqrt(row_count))): \n        data_dict[\'Price\'][np.argmax(data_dict[\'Price\'] == random.choice(data_dict[\'Price\']))] = np.nan\n        data_dict[\'User Type\'][np.argmax(data_dict[\'User Type\'] == random.choice(data_dict[\'User Type\']))] = np.nan\n        data_dict[\'Date\'][np.argmax(data_dict[\'Date\'] == random.choice(data_dict[\'Date\']))] = np.nan\n        data_dict[\'Product ID\'][np.argmax(data_dict[\'Product ID\'] == random.choice(data_dict[\'Product ID\']))] = 0\n        data_dict[\'Serial No\'][np.argmax(data_dict[\'Serial No\'] == random.choice(data_dict[\'Serial No\']))] = -1\n        data_dict[\'User ID\'][np.argmax(data_dict[\'User ID\'] == random.choice(data_dict[\'User ID\']))] = -101\n        \n    \n    # create data frame\n    df = pd.DataFrame(data_dict)\n    \n    return df\n    \n\ndef describe_dataframe(df=pd.DataFrame()):\n    """"""This function generates descriptive stats of a dataframe\n    Args:\n        df (dataframe): the dataframe to be analyzed\n    Returns:\n        None\n\n    """"""\n    print(""\\n\\n"")\n    print(""*""*30)\n    print(""About the Data"")\n    print(""*""*30)\n    \n    print(""Number of rows::"",df.shape[0])\n    print(""Number of columns::"",df.shape[1])\n    print(""\\n"")\n    \n    print(""Column Names::"",df.columns.values.tolist())\n    print(""\\n"")\n    \n    print(""Column Data Types::\\n"",df.dtypes)\n    print(""\\n"")\n    \n    print(""Columns with Missing Values::"",df.columns[df.isnull().any()].tolist())\n    print(""\\n"")\n    \n    print(""Number of rows with Missing Values::"",len(pd.isnull(df).any(1).nonzero()[0].tolist()))\n    print(""\\n"")\n    \n    print(""Sample Indices with missing data::"",pd.isnull(df).any(1).nonzero()[0].tolist()[0:5])\n    print(""\\n"")\n    \n    print(""General Stats::"")\n    print(df.info())\n    print(""\\n"")\n    \n    print(""Summary Stats::"")\n    print(df.describe())\n    print(""\\n"")\n    \n    print(""Dataframe Sample Rows::"")\n    print(df.head(5))\n    \ndef cleanup_column_names(df,rename_dict={},do_inplace=True):\n    """"""This function renames columns of a pandas dataframe\n       It converts column names to snake case if rename_dict is not passed. \n    Args:\n        rename_dict (dict): keys represent old column names and values point to \n                            newer ones\n        do_inplace (bool): flag to update existing dataframe or return a new one\n    Returns:\n        pandas dataframe if do_inplace is set to False, None otherwise\n\n    """"""\n    if not rename_dict:\n        return df.rename(columns={col: col.lower().replace(\' \',\'_\') \n                    for col in df.columns.values.tolist()}, \n                  inplace=do_inplace)\n    else:\n        return df.rename(columns=rename_dict,inplace=do_inplace)\n\ndef expand_user_type(u_type):\n    """"""This function maps user types to user classes\n    Args:\n        u_type (str): user type value\n    Returns:\n        (str) user_class value\n\n    """"""\n    if u_type in [\'a\',\'b\']:\n        return \'new\'\n    elif u_type == \'c\':\n        return \'existing\'\n    elif u_type == \'d\':\n        return \'loyal_existing\'\n    else:\n        return \'error\'\n            \n            \nif __name__==\'__main__\':\n    df = generate_sample_data(row_count=1000)\n    describe_dataframe(df)\n    \n    print(""\\n\\n"")\n    print(""*""*30)\n    print(""Rename Columns"")\n    print(""*""*30)\n    cleanup_column_names(df)\n    print(df.head())\n    print(""\\n\\n"")\n    \n    \n    print(""*""*30)\n    print(""Sorting Rows"")\n    print(""*""*30)\n    print(df.sort_values([\'serial_no\', \'price\'], \n                         ascending=[True, False]).head())\n    print(""\\n\\n"")\n    \n    \n    print(""*""*30)\n    print(""Rearranging Columns"")\n    print(""*""*30)\n    print(df[[\'serial_no\',\'date\',\'user_id\',\'user_type\',\n              \'product_id\',\'quantity_purchased\',\'price\']].head())\n    print(""\\n\\n"")\n    \n    \n    print(""*""*30)\n    print(""Filtering Columns"")\n    print(""*""*30)\n    print(""Using Column Index::"")\n    print(df.iloc[:,3].values)\n    # or df[[3]].values.tolist()\n    print(""\\n"")\n    \n    print(""Using Column Name::"")\n    print(df.quantity_purchased.values)\n    # or df[\'quantity_purchased\'].values.tolist()\n    print(""\\n"")\n    \n    print(""Using Column Data Type::"")\n    print(df.select_dtypes(include=[\'float64\']).values[:,0])\n    # or use exclude to remove certain datatype columns \n    print(""\\n\\n"")\n    \n    \n    print(""*""*30)\n    print(""Subsetting Rows"")\n    print(""*""*30)\n    print(""Select Specific row indices::"")\n    print(df.iloc[[10,501,20]])\n    print(""\\n"")\n    \n    print(""Excluding Specific Row indices::"")\n    print(df.drop([0,24,51], axis=0).head())\n    print(""\\n"")\n    \n    print(""Subsetting based on logical condition(s)::"")\n    print(df[df.quantity_purchased>25].head())\n    print(""\\n"")\n    \n    print(""Subsetting based on offset from top (bottom)::"")\n    print(df[100:].head()) \n    print(""\\n"")\n    \n    \n    print(""*""*30)\n    print(""TypeCasting/Data Type Conversion"")\n    print(""*""*30)\n    df[\'date\'] = pd.to_datetime(df.date)\n    print(df.dtypes)\n    \n    print(""*""*30)\n    print(""Apply / Map"")\n    print(""*""*30)\n    \n    \n    df[\'user_class\'] = df[\'user_type\'].map(expand_user_type)\n    # or applymap\n    print(""Create a new user_class attribute::"")\n    print(df.head())\n    print(""\\n"")\n    \n    print(""Get Attribute ranges::"")\n    print(df.select_dtypes(include=[np.number]).apply(lambda x: \n                                                        x.max()- x.min()))\n    \n    print(""Get Week from date::"")\n    df[\'purchase_week\'] = df[[\'date\']].applymap(lambda dt:dt.week \n                                                if not pd.isnull(dt.week) \n                                                else 0)\n    print(df.head())\n    print(""\\n"")\n    \n    \n    print(""*""*30)\n    print(""Handling Missing Values"")\n    print(""*""*30)\n    \n    print(""Drop Rows with missing dates::"")\n    df_dropped = df.dropna(subset=[\'date\'])\n    print(df_dropped.head())\n    print(""\\n"")\n    \n    print(""Fill Missing Price values with mean price::"")\n    df_dropped[\'price\'].fillna(value=np.round(df.price.mean(),decimals=2),\n                                inplace=True)\n    \n    print(""Fill Missing user_type values with value from \\\n             previous row (forward fill) ::"")\n    df_dropped[\'user_type\'].fillna(method=\'ffill\',inplace=True)\n    \n    print(""Fill Missing user_type values with value from \\\n            next row (backward fill) ::"")\n    df_dropped[\'user_type\'].fillna(method=\'bfill\',inplace=True)\n\n    print(""\\n"")\n    \n    print(""Drop Duplicate serial_no rows::\\n"")\n    print(""Duplicate sample::"")\n    print(df_dropped[df_dropped.duplicated(subset=[\'serial_no\'])].head())\n    df_dropped.drop_duplicates(subset=[\'serial_no\'],inplace=True)\n    print(""After cleanup::"")\n    print(df_dropped.head())\n    print(""\\n"")\n    \n    print(""Remove rows which have less than 3 attributes with non-missing data::"")\n    print(df.dropna(thresh=3).head())\n    print(""\\n"")\n    \n    print(""*""*30)\n    print(""Encoding Categorical Variables"")\n    print(""*""*30)\n    print(pd.get_dummies(df,columns=[\'user_type\']).head())\n    print(""\\n"")\n    \n    type_map={\'a\':0,\'b\':1,\'c\':2,\'d\':3,np.NAN:-1}\n    df[\'encoded_user_type\'] = df.user_type.map(type_map)\n    print((df.head()))\n    print(""\\n"")\n    \n    print(""*""*30)\n    print(""Random Sampling data from DataFrame"")\n    print(""*""*30)\n    print(df.sample(frac=0.2, replace=True, random_state=42).head())\n    print(""\\n"")\n    \n    print(""*""*30)\n    print(""Normalizing Numeric Data"")\n    print(""*""*30)\n    \n    print(""Min-Max Scaler::"")\n    df_normalized = df.dropna().copy()\n    min_max_scaler = preprocessing.MinMaxScaler()\n    np_scaled = min_max_scaler.fit_transform(df_normalized[\'price\'].values.reshape(-1,1))\n    df_normalized[\'price\'] = np_scaled.reshape(-1,1)\n    print(df_normalized.head())\n    print(""\\n"")\n    \n    print(""Robust Scaler::"")\n    df_normalized = df.dropna().copy()\n    robust_scaler = preprocessing.RobustScaler()\n    rs_scaled = robust_scaler.fit_transform(df_normalized[\'quantity_purchased\'].values.reshape(-1,1))\n    df_normalized[\'quantity_purchased\'] = rs_scaled.reshape(-1,1)\n    print(df_normalized.head())\n    print(""\\n"")\n    \n    print(""*""*30)\n    print(""Data Summarization"")\n    print(""*""*30)\n    \n    print(""Aggregates based on condition::"")\n    print(df[\'price\'][df[\'user_type\']==\'a\'].mean())\n    print(""\\n"")\n    \n    print(""Row Counts on condition::"")\n    print(df[\'purchase_week\'].value_counts())\n    print(""\\n"")\n    \n    print(""GroupBy attributes::"")\n    print(df.groupby([\'user_class\'])[\'quantity_purchased\'].sum())\n    print(""\\n"")\n    \n    print(""GroupBy with different aggregates::"")\n    print(df.groupby([\'user_class\'])[\'quantity_purchased\'].agg([np.sum,\n                                                                np.mean,\n                                                                np.count_nonzero]))\n    print(""\\n"")                                                            \n                                                                \n    print(""GroupBy with specific agg for each attribute::"")\n    print(df.groupby([\'user_class\',\'user_type\']).agg({\'price\':np.mean,\n                                                        \'quantity_purchased\':np.max}))\n    print(""\\n"")\n\n    print(""GroupBy with multiple agg for each attribute::"")\n    print(df.groupby([\'user_class\',\'user_type\']).agg({\'price\':{\n                                                                \'total_price\':np.sum,\n                                                                \'mean_price\':np.mean,\n                                                                \'variance_price\':np.std,\n                                                                \'count\':np.count_nonzero},\n                                                   \'quantity_purchased\':np.sum}))  \n                                                                \n    print(""\\n"")\n    \n    \n    print(""Pivot tables::"")\n    print(df.pivot_table(index=\'date\', columns=\'user_type\', \n                         values=\'price\',aggfunc=np.mean))\n    print(""\\n"")      \n\n    print(""Stacking::"")\n    print(df.stack())\n    print(""\\n"")               \n                                                             \n                                                            \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    '"
notebooks/Ch04_Feature_Engineering_and_Selection/feature_engineering_categorical.py,0,"b'\n# coding: utf-8\n""""""\nCreated on Mon May 17 00:00:00 2017\n\n@author: DIP\n""""""\n\n# # Import necessary dependencies and settings\n\n# In[1]:\n\nimport pandas as pd\nimport numpy as np\n\n\n# # Transforming Nominal Features\n\n# In[2]:\n\nvg_df = pd.read_csv(\'datasets/vgsales.csv\', encoding=\'utf-8\')\nvg_df[[\'Name\', \'Platform\', \'Year\', \'Genre\', \'Publisher\']].iloc[1:7]\n\n\n# In[3]:\n\ngenres = np.unique(vg_df[\'Genre\'])\ngenres\n\n\n# In[4]:\n\nfrom sklearn.preprocessing import LabelEncoder\n\ngle = LabelEncoder()\ngenre_labels = gle.fit_transform(vg_df[\'Genre\'])\ngenre_mappings = {index: label for index, label in enumerate(gle.classes_)}\ngenre_mappings\n\n\n# In[5]:\n\nvg_df[\'GenreLabel\'] = genre_labels\nvg_df[[\'Name\', \'Platform\', \'Year\', \'Genre\', \'GenreLabel\']].iloc[1:7]\n\n\n# # Transforming Ordinal Features\n\n# In[6]:\n\npoke_df = pd.read_csv(\'datasets/Pokemon.csv\', encoding=\'utf-8\')\npoke_df = poke_df.sample(random_state=1, frac=1).reset_index(drop=True)\n\nnp.unique(poke_df[\'Generation\'])\n\n\n# In[7]:\n\ngen_ord_map = {\'Gen 1\': 1, \'Gen 2\': 2, \'Gen 3\': 3, \n               \'Gen 4\': 4, \'Gen 5\': 5, \'Gen 6\': 6}\n\npoke_df[\'GenerationLabel\'] = poke_df[\'Generation\'].map(gen_ord_map)\npoke_df[[\'Name\', \'Generation\', \'GenerationLabel\']].iloc[4:10]\n\n\n# # Encoding Categorical Features\n\n# ## One-hot Encoding Scheme\n\n# In[8]:\n\npoke_df[[\'Name\', \'Generation\', \'Legendary\']].iloc[4:10]\n\n\n# In[9]:\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\n# transform and map pokemon generations\ngen_le = LabelEncoder()\ngen_labels = gen_le.fit_transform(poke_df[\'Generation\'])\npoke_df[\'Gen_Label\'] = gen_labels\n\n# transform and map pokemon legendary status\nleg_le = LabelEncoder()\nleg_labels = leg_le.fit_transform(poke_df[\'Legendary\'])\npoke_df[\'Lgnd_Label\'] = leg_labels\n\npoke_df_sub = poke_df[[\'Name\', \'Generation\', \'Gen_Label\', \'Legendary\', \'Lgnd_Label\']]\npoke_df_sub.iloc[4:10]\n\n\n# In[10]:\n\n# encode generation labels using one-hot encoding scheme\ngen_ohe = OneHotEncoder()\ngen_feature_arr = gen_ohe.fit_transform(poke_df[[\'Gen_Label\']]).toarray()\ngen_feature_labels = list(gen_le.classes_)\ngen_features = pd.DataFrame(gen_feature_arr, columns=gen_feature_labels)\n\n# encode legendary status labels using one-hot encoding scheme\nleg_ohe = OneHotEncoder()\nleg_feature_arr = leg_ohe.fit_transform(poke_df[[\'Lgnd_Label\']]).toarray()\nleg_feature_labels = [\'Legendary_\'+str(cls_label) for cls_label in leg_le.classes_]\nleg_features = pd.DataFrame(leg_feature_arr, columns=leg_feature_labels)\n\n\n# In[11]:\n\npoke_df_ohe = pd.concat([poke_df_sub, gen_features, leg_features], axis=1)\ncolumns = sum([[\'Name\', \'Generation\', \'Gen_Label\'],gen_feature_labels,\n              [\'Legendary\', \'Lgnd_Label\'],leg_feature_labels], [])\npoke_df_ohe[columns].iloc[4:10]\n\n\n# In[12]:\n\nnew_poke_df = pd.DataFrame([[\'PikaZoom\', \'Gen 3\', True], \n                           [\'CharMyToast\', \'Gen 4\', False]],\n                           columns=[\'Name\', \'Generation\', \'Legendary\'])\nnew_poke_df\n\n\n# In[13]:\n\nnew_gen_labels = gen_le.transform(new_poke_df[\'Generation\'])\nnew_poke_df[\'Gen_Label\'] = new_gen_labels\n\nnew_leg_labels = leg_le.transform(new_poke_df[\'Legendary\'])\nnew_poke_df[\'Lgnd_Label\'] = new_leg_labels\n\nnew_poke_df[[\'Name\', \'Generation\', \'Gen_Label\', \'Legendary\', \'Lgnd_Label\']]\n\n\n# In[14]:\n\nnew_gen_feature_arr = gen_ohe.transform(new_poke_df[[\'Gen_Label\']]).toarray()\nnew_gen_features = pd.DataFrame(new_gen_feature_arr, columns=gen_feature_labels)\n\nnew_leg_feature_arr = leg_ohe.transform(new_poke_df[[\'Lgnd_Label\']]).toarray()\nnew_leg_features = pd.DataFrame(new_leg_feature_arr, columns=leg_feature_labels)\n\nnew_poke_ohe = pd.concat([new_poke_df, new_gen_features, new_leg_features], axis=1)\ncolumns = sum([[\'Name\', \'Generation\', \'Gen_Label\'], gen_feature_labels,\n               [\'Legendary\', \'Lgnd_Label\'], leg_feature_labels], [])\nnew_poke_ohe[columns]\n\n\n# In[15]:\n\ngen_onehot_features = pd.get_dummies(poke_df[\'Generation\'])\npd.concat([poke_df[[\'Name\', \'Generation\']], gen_onehot_features], axis=1).iloc[4:10]\n\n\n# ## Dummy Coding Scheme\n\n# In[16]:\n\ngen_dummy_features = pd.get_dummies(poke_df[\'Generation\'], drop_first=True)\npd.concat([poke_df[[\'Name\', \'Generation\']], gen_dummy_features], axis=1).iloc[4:10]\n\n\n# In[17]:\n\ngen_onehot_features = pd.get_dummies(poke_df[\'Generation\'])\ngen_dummy_features = gen_onehot_features.iloc[:,:-1]\npd.concat([poke_df[[\'Name\', \'Generation\']], gen_dummy_features], axis=1).iloc[4:10]\n\n\n# ## Effect Coding Scheme\n\n# In[18]:\n\ngen_onehot_features = pd.get_dummies(poke_df[\'Generation\'])\ngen_effect_features = gen_onehot_features.iloc[:,:-1]\ngen_effect_features.loc[np.all(gen_effect_features == 0, axis=1)] = -1.\npd.concat([poke_df[[\'Name\', \'Generation\']], gen_effect_features], axis=1).iloc[4:10]\n\n\n# ## Feature Hashing scheme\n\n# In[19]:\n\nunique_genres = np.unique(vg_df[[\'Genre\']])\nprint(""Total game genres:"", len(unique_genres))\nprint(unique_genres)\n\n\n# In[20]:\n\nfrom sklearn.feature_extraction import FeatureHasher\n\nfh = FeatureHasher(n_features=6, input_type=\'string\')\nhashed_features = fh.fit_transform(vg_df[\'Genre\'])\nhashed_features = hashed_features.toarray()\npd.concat([vg_df[[\'Name\', \'Genre\']], pd.DataFrame(hashed_features)], axis=1).iloc[1:7]\n\n\n# In[21]:\n\nfh.get_params()\n\n'"
notebooks/Ch04_Feature_Engineering_and_Selection/feature_engineering_image.py,0,"b'\n# coding: utf-8\n""""""\nCreated on Mon May 17 00:00:00 2017\n\n@author: DIP\n""""""\n\n# # Import necessary dependencies and settings\n\n# In[1]:\n\nimport skimage\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skimage import io\n\nget_ipython().magic(\'matplotlib inline\')\n\n\n# # Image metadata features\n# \n#  - Image create date & time\n#  - Image dimensions\n#  - Image compression format\n#  - Device Make & Model\n#  - Image resolution & aspect ratio\n#  - Image Artist\n#  - Flash, Aperture, Focal Length & Exposure\n\n# # Raw Image and channel pixel values\n\n# In[2]:\n\ncat = io.imread(\'datasets/cat.png\')\ndog = io.imread(\'datasets/dog.png\')\ndf = pd.DataFrame([\'Cat\', \'Dog\'], columns=[\'Image\'])\n\n\nprint(cat.shape, dog.shape)\n\n\n# In[3]:\n\n#coffee = skimage.transform.resize(coffee, (300, 451), mode=\'reflect\')\nfig = plt.figure(figsize = (8,4))\nax1 = fig.add_subplot(1,2, 1)\nax1.imshow(cat)\nax2 = fig.add_subplot(1,2, 2)\nax2.imshow(dog)\n\n\n# In[4]:\n\ndog_r = dog.copy() # Red Channel\ndog_r[:,:,1] = dog_r[:,:,2] = 0 # set G,B pixels = 0\ndog_g = dog.copy() # Green Channel\ndog_g[:,:,0] = dog_r[:,:,2] = 0 # set R,B pixels = 0\ndog_b = dog.copy() # Blue Channel\ndog_b[:,:,0] = dog_b[:,:,1] = 0 # set R,G pixels = 0\n\nplot_image = np.concatenate((dog_r, dog_g, dog_b), axis=1)\nplt.figure(figsize = (10,4))\nplt.imshow(plot_image)\n\n\n# In[5]:\n\ndog_r\n\n\n# # Grayscale image pixel values\n\n# In[6]:\n\nfrom skimage.color import rgb2gray\n\ncgs = rgb2gray(cat)\ndgs = rgb2gray(dog)\n\nprint(\'Image shape:\', cgs.shape, \'\\n\')\n\n# 2D pixel map\nprint(\'2D image pixel map\')\nprint(np.round(cgs, 2), \'\\n\')\n\n# flattened pixel feature vector\nprint(\'Flattened pixel map:\', (np.round(cgs.flatten(), 2)))\n\n\n# # Binning image intensity distribution\n\n# In[7]:\n\nfig = plt.figure(figsize = (8,4))\nax1 = fig.add_subplot(2,2, 1)\nax1.imshow(cgs, cmap=""gray"")\nax2 = fig.add_subplot(2,2, 2)\nax2.imshow(dgs, cmap=\'gray\')\nax3 = fig.add_subplot(2,2, 3)\nc_freq, c_bins, c_patches = ax3.hist(cgs.flatten(), bins=30)\nax4 = fig.add_subplot(2,2, 4)\nd_freq, d_bins, d_patches = ax4.hist(dgs.flatten(), bins=30)\n\n\n# # Image aggregation statistics\n\n# ## RGB ranges\n\n# In[8]:\n\nfrom scipy.stats import describe\n\ncat_rgb = cat.reshape((168*300), 3).T\ndog_rgb = dog.reshape((168*300), 3).T\n\ncs = describe(cat_rgb, axis=1)\nds = describe(dog_rgb, axis=1)\n\ncat_rgb_range = cs.minmax[1] - cs.minmax[0]\ndog_rgb_range = ds.minmax[1] - ds.minmax[0]\nrgb_range_df = pd.DataFrame([cat_rgb_range, dog_rgb_range], \n                            columns=[\'R_range\', \'G_range\', \'B_range\'])\npd.concat([df, rgb_range_df], axis=1)\n\n\n# # Descriptive aggregations\n\n# In[9]:\n\ncat_stats= np.array([np.round(cs.mean, 2),np.round(cs.variance, 2),\n                     np.round(cs.kurtosis, 2),np.round(cs.skewness, 2),\n                     np.round(np.median(cat_rgb, axis=1), 2)]).flatten()\ndog_stats= np.array([np.round(ds.mean, 2),np.round(ds.variance, 2),\n                        np.round(ds.kurtosis, 2),np.round(ds.skewness, 2),\n                        np.round(np.median(dog_rgb, axis=1), 2)]).flatten()\n\nstats_df = pd.DataFrame([cat_stats, dog_stats],\n                        columns=[\'R_mean\', \'G_mean\', \'B_mean\', \n                                 \'R_var\', \'G_var\', \'B_var\',\n                                 \'R_kurt\', \'G_kurt\', \'B_kurt\',\n                                 \'R_skew\', \'G_skew\', \'B_skew\',\n                                 \'R_med\', \'G_med\', \'B_med\'])\npd.concat([df, stats_df], axis=1)\n\n\n# # Edge detection\n\n# In[10]:\n\nfrom skimage.feature import canny\n\ncat_edges = canny(cgs, sigma=3)\ndog_edges = canny(dgs, sigma=3)\n\nfig = plt.figure(figsize = (8,4))\nax1 = fig.add_subplot(1,2, 1)\nax1.imshow(cat_edges, cmap=\'binary\')\nax2 = fig.add_subplot(1,2, 2)\nax2.imshow(dog_edges, cmap=\'binary\')\n\n\n# # Object detection \n\n# In[11]:\n\nfrom skimage.feature import hog\nfrom skimage import exposure\n\nfd_cat, cat_hog = hog(cgs, orientations=8, pixels_per_cell=(8, 8),\n                    cells_per_block=(3, 3), visualise=True)\nfd_dog, dog_hog = hog(dgs, orientations=8, pixels_per_cell=(8, 8),\n                    cells_per_block=(3, 3), visualise=True)\n\n# rescaling intensity to get better plots\ncat_hogs = exposure.rescale_intensity(cat_hog, in_range=(0, 0.04))\ndog_hogs = exposure.rescale_intensity(dog_hog, in_range=(0, 0.04))\n\nfig = plt.figure(figsize = (10,4))\nax1 = fig.add_subplot(1,2, 1)\nax1.imshow(cat_hogs, cmap=\'binary\')\nax2 = fig.add_subplot(1,2, 2)\nax2.imshow(dog_hogs, cmap=\'binary\')\n\n\n# In[12]:\n\nprint(fd_cat, fd_cat.shape)\n\n\n# # Localized feature extraction\n# \n\n# In[13]:\n\nfrom mahotas.features import surf\nimport mahotas as mh\n\ncat_mh = mh.colors.rgb2gray(cat)\ndog_mh = mh.colors.rgb2gray(dog)\n\ncat_surf = surf.surf(cat_mh, nr_octaves=8, nr_scales=16, initial_step_size=1, threshold=0.1, max_points=50)\ndog_surf = surf.surf(dog_mh, nr_octaves=8, nr_scales=16, initial_step_size=1, threshold=0.1, max_points=54)\n\nfig = plt.figure(figsize = (10,4))\nax1 = fig.add_subplot(1,2, 1)\nax1.imshow(surf.show_surf(cat_mh, cat_surf))\nax2 = fig.add_subplot(1,2, 2)\nax2.imshow(surf.show_surf(dog_mh, dog_surf))\n\n\n# In[14]:\n\ncat_surf_fds = surf.dense(cat_mh, spacing=10)\ndog_surf_fds = surf.dense(dog_mh, spacing=10)\ncat_surf_fds.shape\n\n\n# # Visual Bag of Words model\n\n# ## Engineering features from SURF feature descriptions with clustering\n\n# In[15]:\n\nfrom sklearn.cluster import KMeans\n\nk = 20\nkm = KMeans(k, n_init=100, max_iter=100)\n\nsurf_fd_features = np.array([cat_surf_fds, dog_surf_fds])\nkm.fit(np.concatenate(surf_fd_features))\n\nvbow_features = []\nfor feature_desc in surf_fd_features:\n    labels = km.predict(feature_desc)\n    vbow = np.bincount(labels, minlength=k)\n    vbow_features.append(vbow)\n\nvbow_df = pd.DataFrame(vbow_features)\npd.concat([df, vbow_df], axis=1)\n\n\n# ## Trying out the VBOW pipeline on a new image\n\n# In[16]:\n\nnew_cat = io.imread(\'datasets/new_cat.png\')\nnewcat_mh = mh.colors.rgb2gray(new_cat)\nnewcat_surf = surf.surf(newcat_mh, nr_octaves=8, nr_scales=16, initial_step_size=1, threshold=0.1, max_points=50)\n\nfig = plt.figure(figsize = (10,4))\nax1 = fig.add_subplot(1,2, 1)\nax1.imshow(surf.show_surf(newcat_mh, newcat_surf))\n\n\n# In[17]:\n\nnew_surf_fds = surf.dense(newcat_mh, spacing=10)\n\nlabels = km.predict(new_surf_fds)\nnew_vbow = np.bincount(labels, minlength=k)\npd.DataFrame([new_vbow])\n\n\n# In[18]:\n\nfrom sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n\neucdis = euclidean_distances(new_vbow.reshape(1,-1) , vbow_features)\ncossim = cosine_similarity(new_vbow.reshape(1,-1) , vbow_features)\n\nresult_df = pd.DataFrame({\'EuclideanDistance\': eucdis[0],\n              \'CosineSimilarity\': cossim[0]})\npd.concat([df, result_df], axis=1)\n\n\n# # Automated Feature Engineering with Deep Learning\n\n# In[19]:\n\nfrom keras.models import Sequential\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras import backend as K\n\n\n# ## Build a basic 2-layer CNN\n\n# In[55]:\n\nmodel = Sequential()\nmodel.add(Conv2D(4, (4, 4), input_shape=(168, 300, 3), activation=\'relu\', \n                 kernel_initializer=\'glorot_uniform\'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(4, (4, 4), activation=\'relu\', \n                kernel_initializer=\'glorot_uniform\'))\n\n\n# ## Visualize the CNN architecture\n\n# In[21]:\n\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\nSVG(model_to_dot(model, show_shapes=True, \n                 show_layer_names=True, rankdir=\'TB\').create(prog=\'dot\', format=\'svg\'))\n\n\n# ## Build functions to extract features from intermediate layers\n\n# In[56]:\n\nfirst_conv_layer = K.function([model.layers[0].input, K.learning_phase()], \n                              [model.layers[0].output])\nsecond_conv_layer = K.function([model.layers[0].input, K.learning_phase()], \n                               [model.layers[2].output])\n\n\n# ## Extract and visualize image representation features\n\n# In[57]:\n\ncatr = cat.reshape(1, 168, 300,3)\n\n# extract feaures \nfirst_conv_features = first_conv_layer([catr])[0][0]\nsecond_conv_features = second_conv_layer([catr])[0][0]\n\n# view feature representations\nfig = plt.figure(figsize = (14,4))\nax1 = fig.add_subplot(2,4, 1)\nax1.imshow(first_conv_features[:,:,0])\nax2 = fig.add_subplot(2,4, 2)\nax2.imshow(first_conv_features[:,:,1])\nax3 = fig.add_subplot(2,4, 3)\nax3.imshow(first_conv_features[:,:,2])\nax4 = fig.add_subplot(2,4, 4)\nax4.imshow(first_conv_features[:,:,3])\n\nax5 = fig.add_subplot(2,4, 5)\nax5.imshow(second_conv_features[:,:,0])\nax6 = fig.add_subplot(2,4, 6)\nax6.imshow(second_conv_features[:,:,1])\nax7 = fig.add_subplot(2,4, 7)\nax7.imshow(second_conv_features[:,:,2])\nax8 = fig.add_subplot(2,4, 8)\nax8.imshow(second_conv_features[:,:,3])\n\n\n# In[60]:\n\nsample_features = np.round(np.array(first_conv_features[:,:,1], dtype=\'float\'), 2)\nprint(sample_features)\nprint(sample_features.shape)\n\n'"
notebooks/Ch04_Feature_Engineering_and_Selection/feature_engineering_numeric.py,0,"b'\n# coding: utf-8\n""""""\nCreated on Mon May 17 00:00:00 2017\n\n@author: DIP\n""""""\n\n# # Import necessary dependencies and settings\n\n# In[1]:\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport numpy as np\nimport scipy.stats as spstats\n\nget_ipython().magic(\'matplotlib inline\')\nmpl.style.reload_library()\nmpl.style.use(\'classic\')\nmpl.rcParams[\'figure.facecolor\'] = (1, 1, 1, 0)\nmpl.rcParams[\'figure.figsize\'] = [6.0, 4.0]\nmpl.rcParams[\'figure.dpi\'] = 100\n\n\n# # Raw Measures\n\n# ## Values\n\n# In[2]:\n\npoke_df = pd.read_csv(\'datasets/Pokemon.csv\', encoding=\'utf-8\')\npoke_df.head()\n\n\n# In[3]:\n\npoke_df[[\'HP\', \'Attack\', \'Defense\']].head()\n\n\n# In[4]:\n\npoke_df[[\'HP\', \'Attack\', \'Defense\']].describe()\n\n\n# ## Counts\n\n# In[5]:\n\npopsong_df = pd.read_csv(\'datasets/song_views.csv\', encoding=\'utf-8\')\npopsong_df.head(10)\n\n\n# # Binarization\n\n# In[6]:\n\nwatched = np.array(popsong_df[\'listen_count\']) \nwatched[watched >= 1] = 1\npopsong_df[\'watched\'] = watched\npopsong_df.head(10)\n\n\n# In[7]:\n\nfrom sklearn.preprocessing import Binarizer\n\nbn = Binarizer(threshold=0.9)\npd_watched = bn.transform([popsong_df[\'listen_count\']])[0]\npopsong_df[\'pd_watched\'] = pd_watched\npopsong_df.head(11)\n\n\n# # Rounding\n\n# In[8]:\n\nitems_popularity = pd.read_csv(\'datasets/item_popularity.csv\', encoding=\'utf-8\')\nitems_popularity\n\n\n# In[9]:\n\nitems_popularity[\'popularity_scale_10\'] = np.array(np.round((items_popularity[\'pop_percent\'] * 10)), dtype=\'int\')\nitems_popularity[\'popularity_scale_100\'] = np.array(np.round((items_popularity[\'pop_percent\'] * 100)), dtype=\'int\')\nitems_popularity\n\n\n# # Interactions\n\n# In[10]:\n\natk_def = poke_df[[\'Attack\', \'Defense\']]\natk_def.head()\n\n\n# In[11]:\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\npf = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\nres = pf.fit_transform(atk_def)\nres\n\n\n# In[12]:\n\npd.DataFrame(pf.powers_, columns=[\'Attack_degree\', \'Defense_degree\'])\n\n\n# In[13]:\n\nintr_features = pd.DataFrame(res, columns=[\'Attack\', \'Defense\', \'Attack^2\', \'Attack x Defense\', \'Defense^2\'])\nintr_features.head(5)  \n\n\n# ## Transforming new data in the future (during predictions)\n\n# In[14]:\n\nnew_df = pd.DataFrame([[95, 75],[121, 120], [77, 60]], \n                      columns=[\'Attack\', \'Defense\'])\nnew_df\n\n\n# In[15]:\n\nnew_res = pf.transform(new_df)\nnew_intr_features = pd.DataFrame(new_res, \n                                 columns=[\'Attack\', \'Defense\', \n                                          \'Attack^2\', \'Attack x Defense\', \'Defense^2\'])\nnew_intr_features\n\n\n# # Binning\n\n# In[16]:\n\nfcc_survey_df = pd.read_csv(\'datasets/fcc_2016_coder_survey_subset.csv\', encoding=\'utf-8\')\nfcc_survey_df[[\'ID.x\', \'EmploymentField\', \'Age\', \'Income\']].head()\n\n\n# ## Fixed-width binning\n\n# ### Developer age distribution\n\n# In[17]:\n\nfig, ax = plt.subplots()\nfcc_survey_df[\'Age\'].hist(color=\'#A9C5D3\')\nax.set_title(\'Developer Age Histogram\', fontsize=12)\nax.set_xlabel(\'Age\', fontsize=12)\nax.set_ylabel(\'Frequency\', fontsize=12)\n\n\n# ### Binning based on rounding\n# \n# ``` \n# Age Range: Bin\n# ---------------\n#  0 -  9  : 0\n# 10 - 19  : 1\n# 20 - 29  : 2\n# 30 - 39  : 3\n# 40 - 49  : 4\n# 50 - 59  : 5\n# 60 - 69  : 6\n#   ... and so on\n# ```\n\n# In[18]:\n\nfcc_survey_df[\'Age_bin_round\'] = np.array(np.floor(np.array(fcc_survey_df[\'Age\']) / 10.))\nfcc_survey_df[[\'ID.x\', \'Age\', \'Age_bin_round\']].iloc[1071:1076]\n\n\n# ### Binning based on custom ranges\n# \n# ``` \n# Age Range : Bin\n# ---------------\n#  0 -  15  : 1\n# 16 -  30  : 2\n# 31 -  45  : 3\n# 46 -  60  : 4\n# 61 -  75  : 5\n# 75 - 100  : 6\n# ```\n\n# In[19]:\n\nbin_ranges = [0, 15, 30, 45, 60, 75, 100]\nbin_names = [1, 2, 3, 4, 5, 6]\nfcc_survey_df[\'Age_bin_custom_range\'] = pd.cut(np.array(fcc_survey_df[\'Age\']), \n                                               bins=bin_ranges)\nfcc_survey_df[\'Age_bin_custom_label\'] = pd.cut(np.array(fcc_survey_df[\'Age\']), \n                                               bins=bin_ranges, labels=bin_names)\nfcc_survey_df[[\'ID.x\', \'Age\', \'Age_bin_round\', \n               \'Age_bin_custom_range\', \'Age_bin_custom_label\']].iloc[1071:1076]\n\n\n# ## Quantile based binning\n\n# In[20]:\n\nfcc_survey_df[[\'ID.x\', \'Age\', \'Income\']].iloc[4:9]\n\n\n# In[21]:\n\nfig, ax = plt.subplots()\nfcc_survey_df[\'Income\'].hist(bins=30, color=\'#A9C5D3\')\nax.set_title(\'Developer Income Histogram\', fontsize=12)\nax.set_xlabel(\'Developer Income\', fontsize=12)\nax.set_ylabel(\'Frequency\', fontsize=12)\n\n\n# In[22]:\n\nquantile_list = [0, .25, .5, .75, 1.]\nquantiles = fcc_survey_df[\'Income\'].quantile(quantile_list)\nquantiles\n\n\n# In[23]:\n\nfig, ax = plt.subplots()\nfcc_survey_df[\'Income\'].hist(bins=30, color=\'#A9C5D3\')\n\nfor quantile in quantiles:\n    qvl = plt.axvline(quantile, color=\'r\')\nax.legend([qvl], [\'Quantiles\'], fontsize=10)\n\nax.set_title(\'Developer Income Histogram with Quantiles\', fontsize=12)\nax.set_xlabel(\'Developer Income\', fontsize=12)\nax.set_ylabel(\'Frequency\', fontsize=12)\n\n\n# In[24]:\n\nquantile_labels = [\'0-25Q\', \'25-50Q\', \'50-75Q\', \'75-100Q\']\nfcc_survey_df[\'Income_quantile_range\'] = pd.qcut(fcc_survey_df[\'Income\'], \n                                                 q=quantile_list)\nfcc_survey_df[\'Income_quantile_label\'] = pd.qcut(fcc_survey_df[\'Income\'], \n                                                 q=quantile_list, labels=quantile_labels)\nfcc_survey_df[[\'ID.x\', \'Age\', \'Income\', \n               \'Income_quantile_range\', \'Income_quantile_label\']].iloc[4:9]\n\n\n# # Mathematical Transformations\n\n# ## Log transform\n\n# In[25]:\n\nfcc_survey_df[\'Income_log\'] = np.log((1+ fcc_survey_df[\'Income\']))\nfcc_survey_df[[\'ID.x\', \'Age\', \'Income\', \'Income_log\']].iloc[4:9]\n\n\n# In[26]:\n\nincome_log_mean = np.round(np.mean(fcc_survey_df[\'Income_log\']), 2)\n\nfig, ax = plt.subplots()\nfcc_survey_df[\'Income_log\'].hist(bins=30, color=\'#A9C5D3\')\nplt.axvline(income_log_mean, color=\'r\')\nax.set_title(\'Developer Income Histogram after Log Transform\', fontsize=12)\nax.set_xlabel(\'Developer Income (log scale)\', fontsize=12)\nax.set_ylabel(\'Frequency\', fontsize=12)\nax.text(11.5, 450, r\'$\\mu$=\'+str(income_log_mean), fontsize=10)\n\n\n# ## Box\xe2\x80\x93Cox transform\n\n# In[27]:\n\n# get optimal lambda value from non null income values\nincome = np.array(fcc_survey_df[\'Income\'])\nincome_clean = income[~np.isnan(income)]\nl, opt_lambda = spstats.boxcox(income_clean)\nprint(\'Optimal lambda value:\', opt_lambda)\n\n\n# In[28]:\n\nfcc_survey_df[\'Income_boxcox_lambda_0\'] = spstats.boxcox((1+fcc_survey_df[\'Income\']), \n                                                         lmbda=0)\nfcc_survey_df[\'Income_boxcox_lambda_opt\'] = spstats.boxcox(fcc_survey_df[\'Income\'], \n                                                           lmbda=opt_lambda)\nfcc_survey_df[[\'ID.x\', \'Age\', \'Income\', \'Income_log\', \n               \'Income_boxcox_lambda_0\', \'Income_boxcox_lambda_opt\']].iloc[4:9]\n\n\n# In[29]:\n\nincome_boxcox_mean = np.round(np.mean(fcc_survey_df[\'Income_boxcox_lambda_opt\']), 2)\n\nfig, ax = plt.subplots()\nfcc_survey_df[\'Income_boxcox_lambda_opt\'].hist(bins=30, color=\'#A9C5D3\')\nplt.axvline(income_boxcox_mean, color=\'r\')\nax.set_title(\'Developer Income Histogram after Box\xe2\x80\x93Cox Transform\', fontsize=12)\nax.set_xlabel(\'Developer Income (Box\xe2\x80\x93Cox transform)\', fontsize=12)\nax.set_ylabel(\'Frequency\', fontsize=12)\nax.text(24, 450, r\'$\\mu$=\'+str(income_boxcox_mean), fontsize=10)\n\n'"
notebooks/Ch04_Feature_Engineering_and_Selection/feature_engineering_temporal.py,0,"b'\n# coding: utf-8\n""""""\nCreated on Mon May 17 00:00:00 2017\n\n@author: DIP\n""""""\n\n# # Import necessary dependencies and settings\n\n# In[1]:\n\nimport datetime\nimport numpy as np\nimport pandas as pd\nfrom dateutil.parser import parse\nimport pytz\n\n\n# # Load and process sample temporal data\n\n# In[2]:\n\ntime_stamps = [\'2015-03-08 10:30:00.360000+00:00\', \'2017-07-13 15:45:05.755000-07:00\',\n               \'2012-01-20 22:30:00.254000+05:30\', \'2016-12-25 00:30:00.000000+10:00\']\ndf = pd.DataFrame(time_stamps, columns=[\'Time\'])\ndf\n\n\n# In[3]:\n\nts_objs = np.array([pd.Timestamp(item) for item in np.array(df.Time)])\ndf[\'TS_obj\'] = ts_objs\nts_objs\n\n\n# # Date based features\n\n# In[4]:\n\ndf[\'Year\'] = df[\'TS_obj\'].apply(lambda d: d.year)\ndf[\'Month\'] = df[\'TS_obj\'].apply(lambda d: d.month)\ndf[\'Day\'] = df[\'TS_obj\'].apply(lambda d: d.day)\ndf[\'DayOfWeek\'] = df[\'TS_obj\'].apply(lambda d: d.dayofweek)\ndf[\'DayName\'] = df[\'TS_obj\'].apply(lambda d: d.weekday_name)\ndf[\'DayOfYear\'] = df[\'TS_obj\'].apply(lambda d: d.dayofyear)\ndf[\'WeekOfYear\'] = df[\'TS_obj\'].apply(lambda d: d.weekofyear)\ndf[\'Quarter\'] = df[\'TS_obj\'].apply(lambda d: d.quarter)\n\ndf[[\'Time\', \'Year\', \'Month\', \'Day\', \'Quarter\', \n    \'DayOfWeek\', \'DayName\', \'DayOfYear\', \'WeekOfYear\']]\n\n\n# # Time based features\n\n# In[5]:\n\ndf[\'Hour\'] = df[\'TS_obj\'].apply(lambda d: d.hour)\ndf[\'Minute\'] = df[\'TS_obj\'].apply(lambda d: d.minute)\ndf[\'Second\'] = df[\'TS_obj\'].apply(lambda d: d.second)\ndf[\'MUsecond\'] = df[\'TS_obj\'].apply(lambda d: d.microsecond)\ndf[\'UTC_offset\'] = df[\'TS_obj\'].apply(lambda d: d.utcoffset())\n\ndf[[\'Time\', \'Hour\', \'Minute\', \'Second\', \'MUsecond\', \'UTC_offset\']]\n\n\n# In[6]:\n\nhour_bins = [-1, 5, 11, 16, 21, 23]\nbin_names = [\'Late Night\', \'Morning\', \'Afternoon\', \'Evening\', \'Night\']\ndf[\'TimeOfDayBin\'] = pd.cut(df[\'Hour\'], \n                            bins=hour_bins, labels=bin_names)\ndf[[\'Time\', \'Hour\', \'TimeOfDayBin\']]\n\n\n# In[7]:\n\ndf[\'TZ_info\'] = df[\'TS_obj\'].apply(lambda d: d.tzinfo)\ndf[\'TimeZones\'] = df[\'TS_obj\'].apply(lambda d: list({d.astimezone(tz).tzname() \n                                   for tz in map(pytz.timezone, \n                                                 pytz.all_timezones_set)\n                                       if d.astimezone(tz).utcoffset() == d.utcoffset()}))\n\ndf[[\'Time\', \'UTC_offset\', \'TZ_info\', \'TimeZones\']]\n\n\n# In[8]:\n\ndf[\'TimeUTC\'] = df[\'TS_obj\'].apply(lambda d: d.tz_convert(pytz.utc))\ndf[\'Epoch\'] = df[\'TimeUTC\'].apply(lambda d: d.timestamp())\ndf[\'GregOrdinal\'] = df[\'TimeUTC\'].apply(lambda d: d.toordinal())\n\ndf[[\'Time\', \'TimeUTC\', \'Epoch\', \'GregOrdinal\']]\n\n\n# In[9]:\n\ncurr_ts = datetime.datetime.now(pytz.utc)\n# compute days elapsed since today\ndf[\'DaysElapsedEpoch\'] = (curr_ts.timestamp() - df[\'Epoch\']) / (3600*24)\ndf[\'DaysElapsedOrdinal\'] = (curr_ts.toordinal() - df[\'GregOrdinal\']) \n\ndf[[\'Time\', \'TimeUTC\', \'DaysElapsedEpoch\', \'DaysElapsedOrdinal\']]\n\n'"
notebooks/Ch04_Feature_Engineering_and_Selection/feature_engineering_text.py,0,"b'\n# coding: utf-8\n""""""\nCreated on Mon May 17 00:00:00 2017\n\n@author: DIP\n""""""\n\n# # Import necessary dependencies and settings\n\n# In[1]:\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport nltk\n\n\n# # Sample corpus of text documents\n\n# In[2]:\n\ncorpus = [\'The sky is blue and beautiful.\',\n          \'Love this blue and beautiful sky!\',\n          \'The quick brown fox jumps over the lazy dog.\',\n          \'The brown fox is quick and the blue dog is lazy!\',\n          \'The sky is very blue and the sky is very beautiful today\',\n          \'The dog is lazy but the brown fox is quick!\'    \n]\nlabels = [\'weather\', \'weather\', \'animals\', \'animals\', \'weather\', \'animals\']\ncorpus = np.array(corpus)\ncorpus_df = pd.DataFrame({\'Document\': corpus, \n                          \'Category\': labels})\ncorpus_df = corpus_df[[\'Document\', \'Category\']]\ncorpus_df\n\n\n# # Simple text pre-processing\n\n# In[3]:\n\nwpt = nltk.WordPunctTokenizer()\nstop_words = nltk.corpus.stopwords.words(\'english\')\n\ndef normalize_document(doc):\n    # lower case and remove special characters\\whitespaces\n    doc = re.sub(r\'[^a-zA-Z0-9\\s]\', \'\', doc, re.I)\n    doc = doc.lower()\n    doc = doc.strip()\n    # tokenize document\n    tokens = wpt.tokenize(doc)\n    # filter stopwords out of document\n    filtered_tokens = [token for token in tokens if token not in stop_words]\n    # re-create document from filtered tokens\n    doc = \' \'.join(filtered_tokens)\n    return doc\n\nnormalize_corpus = np.vectorize(normalize_document)\n\n\n# In[4]:\n\nnorm_corpus = normalize_corpus(corpus)\nnorm_corpus\n\n\n# # Bag of Words Model\n\n# In[5]:\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(min_df=0., max_df=1.)\ncv_matrix = cv.fit_transform(norm_corpus)\ncv_matrix = cv_matrix.toarray()\ncv_matrix\n\n\n# In[6]:\n\nvocab = cv.get_feature_names()\npd.DataFrame(cv_matrix, columns=vocab)\n\n\n# # Bag of N-Grams Model\n\n# In[7]:\n\nbv = CountVectorizer(ngram_range=(2,2))\nbv_matrix = bv.fit_transform(norm_corpus)\nbv_matrix = bv_matrix.toarray()\nvocab = bv.get_feature_names()\npd.DataFrame(bv_matrix, columns=vocab)\n\n\n# # TF-IDF Model\n\n# In[8]:\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\ntv_matrix = tv.fit_transform(norm_corpus)\ntv_matrix = tv_matrix.toarray()\n\nvocab = tv.get_feature_names()\npd.DataFrame(np.round(tv_matrix, 2), columns=vocab)\n\n\n# # Document Similarity\n\n# In[9]:\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nsimilarity_matrix = cosine_similarity(tv_matrix)\nsimilarity_df = pd.DataFrame(similarity_matrix)\nsimilarity_df\n\n\n# ## Clustering documents using similarity features\n\n# In[10]:\n\nfrom sklearn.cluster import KMeans\n\nkm = KMeans(n_clusters=2)\nkm.fit_transform(similarity_df)\ncluster_labels = km.labels_\ncluster_labels = pd.DataFrame(cluster_labels, columns=[\'ClusterLabel\'])\npd.concat([corpus_df, cluster_labels], axis=1)\n\n\n# # Topic models\n\n# In[11]:\n\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nlda = LatentDirichletAllocation(n_topics=2, max_iter=100, random_state=42)\ndt_matrix = lda.fit_transform(tv_matrix)\nfeatures = pd.DataFrame(dt_matrix, columns=[\'T1\', \'T2\'])\nfeatures\n\n\n# ## Show topics and their weights\n\n# In[12]:\n\ntt_matrix = lda.components_\nfor topic_weights in tt_matrix:\n    topic = [(token, weight) for token, weight in zip(vocab, topic_weights)]\n    topic = sorted(topic, key=lambda x: -x[1])\n    topic = [item for item in topic if item[1] > 0.6]\n    print(topic)\n    print()\n\n\n# ## Clustering documents using topic model features\n\n# In[13]:\n\nkm = KMeans(n_clusters=2)\nkm.fit_transform(features)\ncluster_labels = km.labels_\ncluster_labels = pd.DataFrame(cluster_labels, columns=[\'ClusterLabel\'])\npd.concat([corpus_df, cluster_labels], axis=1)\n\n\n# # Word Embeddings\n\n# In[14]:\n\nfrom gensim.models import word2vec\n\nwpt = nltk.WordPunctTokenizer()\ntokenized_corpus = [wpt.tokenize(document) for document in norm_corpus]\n\n# Set values for various parameters\nfeature_size = 10    # Word vector dimensionality  \nwindow_context = 10          # Context window size                                                                                    \nmin_word_count = 1   # Minimum word count                        \nsample = 1e-3   # Downsample setting for frequent words\n\nw2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size, \n                          window=window_context, min_count = min_word_count,\n                          sample=sample)\n\n\n# In[15]:\n\nw2v_model.wv[\'sky\']\n\n\n# In[16]:\n\ndef average_word_vectors(words, model, vocabulary, num_features):\n    \n    feature_vector = np.zeros((num_features,),dtype=""float64"")\n    nwords = 0.\n    \n    for word in words:\n        if word in vocabulary: \n            nwords = nwords + 1.\n            feature_vector = np.add(feature_vector, model[word])\n    \n    if nwords:\n        feature_vector = np.divide(feature_vector, nwords)\n        \n    return feature_vector\n    \n   \ndef averaged_word_vectorizer(corpus, model, num_features):\n    vocabulary = set(model.wv.index2word)\n    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n                    for tokenized_sentence in corpus]\n    return np.array(features)\n\n\n# In[17]:\n\nw2v_feature_array = averaged_word_vectorizer(corpus=tokenized_corpus, model=w2v_model,\n                                             num_features=feature_size)\npd.DataFrame(w2v_feature_array)\n\n\n# In[18]:\n\nfrom sklearn.cluster import AffinityPropagation\n\nap = AffinityPropagation()\nap.fit(w2v_feature_array)\ncluster_labels = ap.labels_\ncluster_labels = pd.DataFrame(cluster_labels, columns=[\'ClusterLabel\'])\npd.concat([corpus_df, cluster_labels], axis=1)\n\n'"
notebooks/Ch04_Feature_Engineering_and_Selection/feature_scaling.py,0,"b'\n# coding: utf-8\n""""""\nCreated on Mon May 17 00:00:00 2017\n\n@author: DIP\n""""""\n\n# # Import necessary dependencies and settings\n\n# In[1]:\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nimport numpy as np\nimport pandas as pd\nnp.set_printoptions(suppress=True)\n\n\n# # Load sample data of video views\n\n# In[2]:\n\nviews = pd.DataFrame([1295., 25., 19000., 5., 1., 300.], columns=[\'views\'])\nviews\n\n\n# # Standard Scaler $\\frac{x_i - \\mu}{\\sigma}$\n\n# In[3]:\n\nss = StandardScaler()\nviews[\'zscore\'] = ss.fit_transform(views[[\'views\']])\nviews\n\n\n# In[4]:\n\nvw = np.array(views[\'views\'])\n(vw[0] - np.mean(vw)) / np.std(vw)\n\n\n# # Min-Max Scaler $\\frac{x_i - min(x)}{max(x) - min(x)}$\n\n# In[5]:\n\nmms = MinMaxScaler()\nviews[\'minmax\'] = mms.fit_transform(views[[\'views\']])\nviews\n\n\n# In[6]:\n\n(vw[0] - np.min(vw)) / (np.max(vw) - np.min(vw))\n\n\n# # Robust Scaler $\\frac{x_i - median(x)}{IQR_{(1,3)}(x)}$\n\n# In[7]:\n\nrs = RobustScaler()\nviews[\'robust\'] = rs.fit_transform(views[[\'views\']])\nviews\n\n\n# In[8]:\n\nquartiles = np.percentile(vw, (25., 75.))\niqr = quartiles[1] - quartiles[0]\n(vw[0] - np.median(vw)) / iqr\n\n'"
notebooks/Ch04_Feature_Engineering_and_Selection/feature_selection.py,0,"b'\n# coding: utf-8\n""""""\nCreated on Mon May 17 00:00:00 2017\n\n@author: DIP\n""""""\n\n# # Import necessary dependencies and settings\n\n# In[1]:\n\nimport numpy as np\nimport pandas as pd\nnp.set_printoptions(suppress=True)\npt = np.get_printoptions()[\'threshold\']\n\n\n# # Threshold based methods\n\n# ## Limiting features in bag of word based models\n\n# In[2]:\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(min_df=0.1, max_df=0.85, max_features=2000)\ncv\n\n\n# ## Variance based thresholding\n\n# In[3]:\n\ndf = pd.read_csv(\'datasets/Pokemon.csv\')\npoke_gen = pd.get_dummies(df[\'Generation\'])\npoke_gen.head()\n\n\n# In[4]:\n\nfrom sklearn.feature_selection import VarianceThreshold\n\nvt = VarianceThreshold(threshold=.15)\nvt.fit(poke_gen)\n\n\n# In[5]:\n\npd.DataFrame({\'variance\': vt.variances_,\n              \'select_feature\': vt.get_support()},\n            index=poke_gen.columns).T\n\n\n# In[6]:\n\npoke_gen_subset = poke_gen.iloc[:,vt.get_support()].head()\npoke_gen_subset\n\n\n# # Statistical Methods\n\n# In[7]:\n\nfrom sklearn.datasets import load_breast_cancer\n\nbc_data = load_breast_cancer()\nbc_features = pd.DataFrame(bc_data.data, columns=bc_data.feature_names)\nbc_classes = pd.DataFrame(bc_data.target, columns=[\'IsMalignant\'])\n\n# build featureset and response class labels \nbc_X = np.array(bc_features)\nbc_y = np.array(bc_classes).T[0]\nprint(\'Feature set shape:\', bc_X.shape)\nprint(\'Response class shape:\', bc_y.shape)\n\n\n# In[8]:\n\nnp.set_printoptions(threshold=30)\nprint(\'Feature set data [shape: \'+str(bc_X.shape)+\']\')\nprint(np.round(bc_X, 2), \'\\n\')\nprint(\'Feature names:\')\nprint(np.array(bc_features.columns), \'\\n\')\nprint(\'Predictor Class label data [shape: \'+str(bc_y.shape)+\']\')\nprint(bc_y, \'\\n\')\nprint(\'Predictor name:\', np.array(bc_classes.columns))\nnp.set_printoptions(threshold=pt)\n\n\n# In[9]:\n\nfrom sklearn.feature_selection import chi2, SelectKBest\n\nskb = SelectKBest(score_func=chi2, k=15)\nskb.fit(bc_X, bc_y)\n\n\n# In[10]:\n\nfeature_scores = [(item, score) for item, score in zip(bc_data.feature_names, skb.scores_)]\nsorted(feature_scores, key=lambda x: -x[1])[:10]\n\n\n# In[11]:\n\nselect_features_kbest = skb.get_support()\nfeature_names_kbest = bc_data.feature_names[select_features_kbest]\nfeature_subset_df = bc_features[feature_names_kbest]\nbc_SX = np.array(feature_subset_df)\nprint(bc_SX.shape)\nprint(feature_names_kbest)\n\n\n# In[12]:\n\nnp.round(feature_subset_df.iloc[20:25], 2)\n\n\n# In[13]:\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\n# build logistic regression model\nlr = LogisticRegression()\n\n# evaluating accuracy for model built on full featureset\nfull_feat_acc = np.average(cross_val_score(lr, bc_X, bc_y, scoring=\'accuracy\', cv=5))\n# evaluating accuracy for model built on selected featureset\nsel_feat_acc = np.average(cross_val_score(lr, bc_SX, bc_y, scoring=\'accuracy\', cv=5))\n\nprint(\'Model accuracy statistics with 5-fold cross validation\')\nprint(\'Model accuracy with complete feature set\', bc_X.shape, \':\', full_feat_acc)\nprint(\'Model accuracy with selected feature set\', bc_SX.shape, \':\', sel_feat_acc)\n\n\n# # Recursive Feature Elimination\n\n# In[14]:\n\nfrom sklearn.feature_selection import RFE\n\nlr = LogisticRegression()\nrfe = RFE(estimator=lr, n_features_to_select=15, step=1)\nrfe.fit(bc_X, bc_y)\n\n\n# In[15]:\n\nselect_features_rfe = rfe.get_support()\nfeature_names_rfe = bc_data.feature_names[select_features_rfe]\nprint(feature_names_rfe)\n\n\n# In[16]:\n\nset(feature_names_kbest) & set(feature_names_rfe)\n\n\n# # Model based selection\n\n# In[17]:\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier()\nrfc.fit(bc_X, bc_y)\n\n\n# In[18]:\n\nimportance_scores = rfc.feature_importances_\nfeature_importances = [(feature, score) for feature, score in zip(bc_data.feature_names, importance_scores)]\nsorted(feature_importances, key=lambda x: -x[1])[:10]\n\n\n# # Feature extraction using dimensionality reduction\n\n# In[19]:\n\n# center the feature set\nbc_XC = bc_X - bc_X.mean(axis=0)\n\n# decompose using SVD\nU, S, VT = np.linalg.svd(bc_XC)\n\n# get principal components\nPC = VT.T\n\n# get first 3 principal components\nPC3 = PC[:, 0:3]\nPC3.shape\n\n\n# In[20]:\n\n# reduce feature set dimensionality \nnp.round(bc_XC.dot(PC3), 2)\n\n\n# In[21]:\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=3)\npca.fit(bc_X)\n\n\n# In[22]:\n\npca.explained_variance_ratio_\n\n\n# In[23]:\n\nbc_pca = pca.transform(bc_X)\nnp.round(bc_pca, 2)\n\n\n# In[24]:\n\nnp.average(cross_val_score(lr, bc_pca, bc_y, scoring=\'accuracy\', cv=5))\n\n'"
notebooks/Ch05_Building_Tuning_and_Deploying_Models/model_build_tune_deploy.py,0,"b'\n# coding: utf-8\n\n# # Classification Example\n\n# In[1]:\n\n\nfrom sklearn import datasets, metrics\nimport matplotlib.pyplot as plt\n\nget_ipython().magic(\'matplotlib inline\')\n\n\n# ## Load dataset\n\n# In[2]:\n\n\ndigits = datasets.load_digits()\n\n\n# ## View sample image\n\n# In[3]:\n\n\nplt.figure(figsize=(3, 3))\nplt.imshow(digits.images[10], cmap=plt.cm.gray_r)\n\n\n# ## Actual image pixel matrix\n\n# In[4]:\n\n\ndigits.images[10]\n\n\n# ## Flattened vector\n\n# In[5]:\n\n\ndigits.data[10]\n\n\n# ## Image class label\n\n# In[6]:\n\n\ndigits.target[10]\n\n\n# ## Build train and test datasets\n\n# In[7]:\n\n\nX_digits = digits.data\ny_digits = digits.target\n\nnum_data_points = len(X_digits)\nX_train = X_digits[:int(.7 * num_data_points)]\ny_train = y_digits[:int(.7 * num_data_points)]\nX_test = X_digits[int(.7 * num_data_points):]\ny_test = y_digits[int(.7 * num_data_points):]\nprint(X_train.shape, X_test.shape)\n\n\n# ## Train Model\n\n# In[8]:\n\n\nfrom sklearn import linear_model\n\nlogistic = linear_model.LogisticRegression()\nlogistic.fit(X_train, y_train)\n\n\n# ## Predict and Evaluate Performance\n\n# In[9]:\n\n\nprint(\'Logistic Regression mean accuracy: %f\' % logistic.score(X_test, y_test))\n\n\n# # Load Wisconsin Breast Cancer Dataset\n\n# In[10]:\n\n\nimport numpy as np\nfrom sklearn.datasets import load_breast_cancer\n\n# load data\ndata = load_breast_cancer()\nX = data.data\ny = data.target\nprint(X.shape, data.feature_names)\n\n\n# # Partition based Clustering Example\n\n# In[11]:\n\n\nfrom sklearn.cluster import KMeans\n\nkm = KMeans(n_clusters=2, random_state=2)\nkm.fit(X)\n\nlabels = km.labels_\ncenters = km.cluster_centers_\nprint(labels[:10])\n\n\n# In[12]:\n\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nbc_pca = pca.fit_transform(X)\n\n\n# In[13]:\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\nfig.suptitle(\'Visualizing breast cancer clusters\')\nfig.subplots_adjust(top=0.85, wspace=0.5)\nax1.set_title(\'Actual Labels\')\nax2.set_title(\'Clustered Labels\')\n\nfor i in range(len(y)):\n    if y[i] == 0:\n        c1 = ax1.scatter(bc_pca[i,0], bc_pca[i,1],c=\'g\', marker=\'.\')\n    if y[i] == 1:\n        c2 = ax1.scatter(bc_pca[i,0], bc_pca[i,1],c=\'r\', marker=\'.\')\n        \n    if labels[i] == 0:\n        c3 = ax2.scatter(bc_pca[i,0], bc_pca[i,1],c=\'g\', marker=\'.\')\n    if labels[i] == 1:\n        c4 = ax2.scatter(bc_pca[i,0], bc_pca[i,1],c=\'r\', marker=\'.\')\n\nl1 = ax1.legend([c1, c2], [\'0\', \'1\'])\nl2 = ax2.legend([c3, c4], [\'0\', \'1\'])\n\n\n# # Hierarchical Clustering Example\n\n# In[14]:\n\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nimport numpy as np\nnp.set_printoptions(suppress=True)\n\nZ = linkage(X, \'ward\')\nprint(Z)\n\n\n# In[15]:\n\n\nplt.figure(figsize=(8, 3))\nplt.title(\'Hierarchical Clustering Dendrogram\')\nplt.xlabel(\'Data point\')\nplt.ylabel(\'Distance\')\ndendrogram(Z)\nplt.axhline(y=10000, c=\'k\', ls=\'--\', lw=0.5)\nplt.show()\n\n\n# In[16]:\n\n\nfrom scipy.cluster.hierarchy import fcluster\n\nmax_dist = 10000\nhc_labels = fcluster(Z, max_dist, criterion=\'distance\')\n\n\n# In[17]:\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\nfig.suptitle(\'Visualizing breast cancer clusters\')\nfig.subplots_adjust(top=0.85, wspace=0.5)\nax1.set_title(\'Actual Labels\')\nax2.set_title(\'Hierarchical Clustered Labels\')\n\nfor i in range(len(y)):\n    if y[i] == 0:\n        c1 = ax1.scatter(bc_pca[i,0], bc_pca[i,1],c=\'g\', marker=\'.\')\n    if y[i] == 1:\n        c2 = ax1.scatter(bc_pca[i,0], bc_pca[i,1],c=\'r\', marker=\'.\')\n        \n    if hc_labels[i] == 1:\n        c3 = ax2.scatter(bc_pca[i,0], bc_pca[i,1],c=\'g\', marker=\'.\')\n    if hc_labels[i] == 2:\n        c4 = ax2.scatter(bc_pca[i,0], bc_pca[i,1],c=\'r\', marker=\'.\')\n\nl1 = ax1.legend([c1, c2], [\'0\', \'1\'])\nl2 = ax2.legend([c3, c4], [\'1\', \'2\'])\n\n\n# # Classification Model Evaluation Metrics\n\n# In[18]:\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nprint(X_train.shape, X_test.shape)\n\n\n# In[19]:\n\n\nfrom sklearn import linear_model\n\nlogistic = linear_model.LogisticRegression()\nlogistic.fit(X_train,y_train)\n\n\n# ## Confusion Matrix\n\n# In[20]:\n\n\nimport model_evaluation_utils as meu\n\ny_pred = logistic.predict(X_test)\nmeu.display_confusion_matrix(true_labels=y_test, predicted_labels=y_pred, classes=[0, 1])\n\n\n# ## True Positive, False Positive, True Negative and False Negative\n\n# In[21]:\n\n\npositive_class = 1\nTP = 106\nFP = 4\nTN = 59\nFN = 2\n\n\n# ## Accuracy\n\n# In[22]:\n\n\nfw_acc = round(meu.metrics.accuracy_score(y_true=y_test, y_pred=y_pred), 5)\nmc_acc = round((TP + TN) / (TP + TN + FP + FN), 5)\nprint(\'Framework Accuracy:\', fw_acc)\nprint(\'Manually Computed Accuracy:\', mc_acc)\n\n\n# ## Precision\n\n# In[23]:\n\n\nfw_prec = round(meu.metrics.precision_score(y_true=y_test, y_pred=y_pred), 5)\nmc_prec = round((TP) / (TP + FP), 5)\nprint(\'Framework Precision:\', fw_prec)\nprint(\'Manually Computed Precision:\', mc_prec)\n\n\n# ## Recall\n\n# In[24]:\n\n\nfw_rec = round(meu.metrics.recall_score(y_true=y_test, y_pred=y_pred), 5)\nmc_rec = round((TP) / (TP + FN), 5)\nprint(\'Framework Recall:\', fw_rec)\nprint(\'Manually Computed Recall:\', mc_rec)\n\n\n# ## F1-Score\n\n# In[25]:\n\n\nfw_f1 = round(meu.metrics.f1_score(y_true=y_test, y_pred=y_pred), 5)\nmc_f1 = round((2*mc_prec*mc_rec) / (mc_prec+mc_rec), 5)\nprint(\'Framework F1-Score:\', fw_f1)\nprint(\'Manually Computed F1-Score:\', mc_f1)\n\n\n# ## ROC Curve and AUC\n\n# In[26]:\n\n\nmeu.plot_model_roc_curve(clf=logistic, features=X_test, true_labels=y_test)\n\n\n# # Clustering Model Evaluation Metrics\n\n# ## Build two clustering models on the breast cancer dataset\n\n# In[27]:\n\n\nkm2 = KMeans(n_clusters=2, random_state=42).fit(X)\nkm2_labels = km2.labels_\n\nkm5 = KMeans(n_clusters=5, random_state=42).fit(X)\nkm5_labels = km5.labels_\n\n\n# ## Homogeneity, Completeness and V-measure\n\n# In[28]:\n\n\nkm2_hcv = np.round(metrics.homogeneity_completeness_v_measure(y, km2_labels), 3)\nkm5_hcv = np.round(metrics.homogeneity_completeness_v_measure(y, km5_labels), 3)\n\nprint(\'Homogeneity, Completeness, V-measure metrics for num clusters=2: \', km2_hcv)\nprint(\'Homogeneity, Completeness, V-measure metrics for num clusters=5: \', km5_hcv)\n\n\n# ## Silhouette Coefficient\n\n# In[29]:\n\n\nfrom sklearn import metrics\n\nkm2_silc = metrics.silhouette_score(X, km2_labels, metric=\'euclidean\')\nkm5_silc = metrics.silhouette_score(X, km5_labels, metric=\'euclidean\')\n\nprint(\'Silhouette Coefficient for num clusters=2: \', km2_silc)\nprint(\'Silhouette Coefficient for num clusters=5: \', km5_silc)\n\n\n# ## Calinski-Harabaz Index\n\n# In[30]:\n\n\nkm2_chi = metrics.calinski_harabaz_score(X, km2_labels)\nkm5_chi = metrics.calinski_harabaz_score(X, km5_labels)\n\nprint(\'Calinski-Harabaz Index for num clusters=2: \', km2_chi)\nprint(\'Calinski-Harabaz Index for num clusters=5: \', km5_chi)\n\n\n# # Model tuning\n\n# ## Build and Evaluate Default Model\n\n# In[31]:\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n\n# prepare datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# build default SVM model\ndef_svc = SVC(random_state=42)\ndef_svc.fit(X_train, y_train)\n\n# predict and evaluate performance\ndef_y_pred = def_svc.predict(X_test)\nprint(\'Default Model Stats:\')\nmeu.display_model_performance_metrics(true_labels=y_test, predicted_labels=def_y_pred, classes=[0,1])\n\n\n# ## Tune Model with Grid Search\n\n# In[32]:\n\n\nfrom sklearn.model_selection import GridSearchCV\n\n# setting the parameter grid\ngrid_parameters = {\'kernel\': [\'linear\', \'rbf\'], \n                   \'gamma\': [1e-3, 1e-4],\n                   \'C\': [1, 10, 50, 100]}\n\n# perform hyperparameter tuning\nprint(""# Tuning hyper-parameters for accuracy\\n"")\nclf = GridSearchCV(SVC(random_state=42), grid_parameters, cv=5, scoring=\'accuracy\')\nclf.fit(X_train, y_train)\n# view accuracy scores for all the models\nprint(""Grid scores for all the models based on CV:\\n"")\nmeans = clf.cv_results_[\'mean_test_score\']\nstds = clf.cv_results_[\'std_test_score\']\nfor mean, std, params in zip(means, stds, clf.cv_results_[\'params\']):\n    print(""%0.5f (+/-%0.05f) for %r"" % (mean, std * 2, params))\n# check out best model performance\nprint(""\\nBest parameters set found on development set:"", clf.best_params_)\nprint(""Best model validation accuracy:"", clf.best_score_)\n\n\n# ## Evaluate Grid Search Tuned Model\n\n# In[33]:\n\n\ngs_best = clf.best_estimator_\ntuned_y_pred = gs_best.predict(X_test)\n\nprint(\'\\n\\nTuned Model Stats:\')\nmeu.display_model_performance_metrics(true_labels=y_test, predicted_labels=tuned_y_pred, classes=[0,1])\n\n\n# ## Tune Model with Randomized Search\n\n# In[34]:\n\n\nimport scipy\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_grid = {\'C\': scipy.stats.expon(scale=10), \n              \'gamma\': scipy.stats.expon(scale=.1),\n              \'kernel\': [\'rbf\', \'linear\']}\n\nrandom_search = RandomizedSearchCV(SVC(random_state=42), param_distributions=param_grid,\n                                   n_iter=50, cv=5)\nrandom_search.fit(X_train, y_train)\n\nprint(""Best parameters set found on development set:"")\nrandom_search.best_params_\n\n\n# ## Evaluate Randomized Search Tuned Model\n\n# In[35]:\n\n\nrs_best = random_search.best_estimator_\nrs_y_pred = rs_best.predict(X_test)\nmeu.get_metrics(true_labels=y_test, predicted_labels=rs_y_pred)\n\n\n# # Model Interpretation\n\n# In[36]:\n\n\nfrom skater.core.explanations import Interpretation\nfrom skater.model import InMemoryModel\n\ninterpreter = Interpretation(X_test, feature_names=data.feature_names)\nmodel = InMemoryModel(logistic.predict_proba, examples=X_train, target_names=logistic.classes_)\n\n\n# ## Visualize Feature Importances\n\n# In[37]:\n\n\nplots = interpreter.feature_importance.plot_feature_importance(model, ascending=False)\n\n\n# ## One-way partial dependence plot\n\n# In[38]:\n\n\np = interpreter.partial_dependence.plot_partial_dependence([\'worst area\'], model, grid_resolution=50, \n                                                           with_variance=True, figsize = (6, 4))\n\n\n# ## Explaining Predictions\n\n# In[39]:\n\n\nfrom skater.core.local_interpretation.lime.lime_tabular import LimeTabularExplainer\nexp = LimeTabularExplainer(X_train, feature_names=data.feature_names, \n                           discretize_continuous=True, class_names=[\'0\', \'1\'])\n\n\n# In[40]:\n\n\nexp.explain_instance(X_test[0], logistic.predict_proba).show_in_notebook()\n\n\n# In[41]:\n\n\nexp.explain_instance(X_test[1], logistic.predict_proba).show_in_notebook()\n\n\n# # Model Deployment\n\n# ## Persist model to disk\n\n# In[42]:\n\n\nfrom sklearn.externals import joblib\njoblib.dump(logistic, \'lr_model.pkl\') \n\n\n# ## Load model from disk\n\n# In[43]:\n\n\nlr = joblib.load(\'lr_model.pkl\') \nlr\n\n\n# ## Predict with loaded model\n\n# In[44]:\n\n\nprint(lr.predict(X_test[10:11]), y_test[10:11])\n\n'"
notebooks/Ch05_Building_Tuning_and_Deploying_Models/model_evaluation_utils.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Mon Jul 31 20:05:23 2017\n\n@author: DIP\n@copyright: Dipanjan Sarkar\n""""""\n\nfrom sklearn import metrics\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.base import clone\nfrom sklearn.preprocessing import label_binarize\nfrom scipy import interp\nfrom sklearn.metrics import roc_curve, auc \n\n\ndef get_metrics(true_labels, predicted_labels):\n    \n    print(\'Accuracy:\', np.round(\n                        metrics.accuracy_score(true_labels, \n                                               predicted_labels),\n                        4))\n    print(\'Precision:\', np.round(\n                        metrics.precision_score(true_labels, \n                                               predicted_labels,\n                                               average=\'weighted\'),\n                        4))\n    print(\'Recall:\', np.round(\n                        metrics.recall_score(true_labels, \n                                               predicted_labels,\n                                               average=\'weighted\'),\n                        4))\n    print(\'F1 Score:\', np.round(\n                        metrics.f1_score(true_labels, \n                                               predicted_labels,\n                                               average=\'weighted\'),\n                        4))\n                        \n\ndef train_predict_model(classifier, \n                        train_features, train_labels, \n                        test_features, test_labels):\n    # build model    \n    classifier.fit(train_features, train_labels)\n    # predict using model\n    predictions = classifier.predict(test_features) \n    return predictions    \n\n\ndef display_confusion_matrix(true_labels, predicted_labels, classes=[1,0]):\n    \n    total_classes = len(classes)\n    level_labels = [total_classes*[0], list(range(total_classes))]\n\n    cm = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels, \n                                  labels=classes)\n    cm_frame = pd.DataFrame(data=cm, \n                            columns=pd.MultiIndex(levels=[[\'Predicted:\'], classes], \n                                                  labels=level_labels), \n                            index=pd.MultiIndex(levels=[[\'Actual:\'], classes], \n                                                labels=level_labels)) \n    print(cm_frame) \n    \ndef display_classification_report(true_labels, predicted_labels, classes=[1,0]):\n\n    report = metrics.classification_report(y_true=true_labels, \n                                           y_pred=predicted_labels, \n                                           labels=classes) \n    print(report)\n    \n    \n    \ndef display_model_performance_metrics(true_labels, predicted_labels, classes=[1,0]):\n    print(\'Model Performance metrics:\')\n    print(\'-\'*30)\n    get_metrics(true_labels=true_labels, predicted_labels=predicted_labels)\n    print(\'\\nModel Classification report:\')\n    print(\'-\'*30)\n    display_classification_report(true_labels=true_labels, predicted_labels=predicted_labels, \n                                  classes=classes)\n    print(\'\\nPrediction Confusion Matrix:\')\n    print(\'-\'*30)\n    display_confusion_matrix(true_labels=true_labels, predicted_labels=predicted_labels, \n                             classes=classes)\n\n\ndef plot_model_decision_surface(clf, train_features, train_labels,\n                                plot_step=0.02, cmap=plt.cm.RdYlBu,\n                                markers=None, alphas=None, colors=None):\n    \n    if train_features.shape[1] != 2:\n        raise ValueError(""X_train should have exactly 2 columnns!"")\n    \n    x_min, x_max = train_features[:, 0].min() - plot_step, train_features[:, 0].max() + plot_step\n    y_min, y_max = train_features[:, 1].min() - plot_step, train_features[:, 1].max() + plot_step\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                         np.arange(y_min, y_max, plot_step))\n\n    clf_est = clone(clf)\n    clf_est.fit(train_features,train_labels)\n    if hasattr(clf_est, \'predict_proba\'):\n        Z = clf_est.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n    else:\n        Z = clf_est.predict(np.c_[xx.ravel(), yy.ravel()])    \n    Z = Z.reshape(xx.shape)\n    cs = plt.contourf(xx, yy, Z, cmap=cmap)\n    \n    le = LabelEncoder()\n    y_enc = le.fit_transform(train_labels)\n    n_classes = len(le.classes_)\n    plot_colors = \'\'.join(colors) if colors else [None] * n_classes\n    label_names = le.classes_\n    markers = markers if markers else [None] * n_classes\n    alphas = alphas if alphas else [None] * n_classes\n    for i, color in zip(range(n_classes), plot_colors):\n        idx = np.where(y_enc == i)\n        plt.scatter(train_features[idx, 0], train_features[idx, 1], c=color,\n                    label=label_names[i], cmap=cmap, edgecolors=\'black\', \n                    marker=markers[i], alpha=alphas[i])\n    plt.legend()\n    plt.show()\n\n\ndef plot_model_roc_curve(clf, features, true_labels, label_encoder=None, class_names=None):\n    \n    ## Compute ROC curve and ROC area for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    if hasattr(clf, \'classes_\'):\n        class_labels = clf.classes_\n    elif label_encoder:\n        class_labels = label_encoder.classes_\n    elif class_names:\n        class_labels = class_names\n    else:\n        raise ValueError(\'Unable to derive prediction classes, please specify class_names!\')\n    n_classes = len(class_labels)\n    y_test = label_binarize(true_labels, classes=class_labels)\n    if n_classes == 2:\n        if hasattr(clf, \'predict_proba\'):\n            prob = clf.predict_proba(features)\n            y_score = prob[:, prob.shape[1]-1] \n        elif hasattr(clf, \'decision_function\'):\n            prob = clf.decision_function(features)\n            y_score = prob[:, prob.shape[1]-1]\n        else:\n            raise AttributeError(""Estimator doesn\'t have a probability or confidence scoring system!"")\n        \n        fpr, tpr, _ = roc_curve(y_test, y_score)      \n        roc_auc = auc(fpr, tpr)\n        plt.plot(fpr, tpr, label=\'ROC curve (area = {0:0.2f})\'\n                                 \'\'.format(roc_auc),\n                 linewidth=2.5)\n        \n    elif n_classes > 2:\n        if hasattr(clf, \'predict_proba\'):\n            y_score = clf.predict_proba(features)\n        elif hasattr(clf, \'decision_function\'):\n            y_score = clf.decision_function(features)\n        else:\n            raise AttributeError(""Estimator doesn\'t have a probability or confidence scoring system!"")\n\n        for i in range(n_classes):\n            fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n            roc_auc[i] = auc(fpr[i], tpr[i])\n\n        ## Compute micro-average ROC curve and ROC area\n        fpr[""micro""], tpr[""micro""], _ = roc_curve(y_test.ravel(), y_score.ravel())\n        roc_auc[""micro""] = auc(fpr[""micro""], tpr[""micro""])\n\n        ## Compute macro-average ROC curve and ROC area\n        # First aggregate all false positive rates\n        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n        # Then interpolate all ROC curves at this points\n        mean_tpr = np.zeros_like(all_fpr)\n        for i in range(n_classes):\n            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n        # Finally average it and compute AUC\n        mean_tpr /= n_classes\n        fpr[""macro""] = all_fpr\n        tpr[""macro""] = mean_tpr\n        roc_auc[""macro""] = auc(fpr[""macro""], tpr[""macro""])\n\n        ## Plot ROC curves\n        plt.figure(figsize=(6, 4))\n        plt.plot(fpr[""micro""], tpr[""micro""],\n                 label=\'micro-average ROC curve (area = {0:0.2f})\'\n                       \'\'.format(roc_auc[""micro""]), linewidth=3)\n\n        plt.plot(fpr[""macro""], tpr[""macro""],\n                 label=\'macro-average ROC curve (area = {0:0.2f})\'\n                       \'\'.format(roc_auc[""macro""]), linewidth=3)\n\n        for i, label in enumerate(class_labels):\n            plt.plot(fpr[i], tpr[i], label=\'ROC curve of class {0} (area = {1:0.2f})\'\n                                           \'\'.format(label, roc_auc[i]), \n                     linewidth=2, linestyle=\':\')\n    else:\n        raise ValueError(\'Number of classes should be atleast 2 or more\')\n        \n    plt.plot([0, 1], [0, 1], \'k--\')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel(\'False Positive Rate\')\n    plt.ylabel(\'True Positive Rate\')\n    plt.title(\'Receiver Operating Characteristic (ROC) Curve\')\n    plt.legend(loc=""lower right"")\n    plt.show()\n\n\n'"
notebooks/Ch06_Analyzing_Bike_Sharing_Trends/bike_sharing_eda.py,0,"b'\n# coding: utf-8\n\n# # Bike Sharing Dataset Exploratory Analysis\n# \n# + Based on Bike Sharing dataset from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset)\n# + This notebook is based upon the hourly data file, i.e. hour.csv\n# \n# ---\n# Reference:\n# Fanaee-T, Hadi, and Gama, Joao, \'Event labeling combining ensemble detectors and background knowledge\', Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg,\n\n# ## Import required packages\n\n# In[1]:\n\n\n# data manipulation \nimport numpy as np\nimport pandas as pd\n\n# plotting\nimport seaborn as sn\nimport matplotlib.pyplot as plt\nget_ipython().magic(\'matplotlib inline\')\n\n# setting params\nparams = {\'legend.fontsize\': \'x-large\',\n          \'figure.figsize\': (30, 10),\n          \'axes.labelsize\': \'x-large\',\n          \'axes.titlesize\':\'x-large\',\n          \'xtick.labelsize\':\'x-large\',\n          \'ytick.labelsize\':\'x-large\'}\n\nsn.set_style(\'whitegrid\')\nsn.set_context(\'talk\')\n\nplt.rcParams.update(params)\npd.options.display.max_colwidth = 600\n\n# pandas display data frames as tables\nfrom IPython.display import display, HTML\n\n\n# ## Load Dataset\n\n# In[2]:\n\n\nhour_df = pd.read_csv(\'hour.csv\')\nprint(""Shape of dataset::{}"".format(hour_df.shape))\n\n\n# In[3]:\n\n\ndisplay(hour_df.head())\n\n\n# ## Data Types and Summary Stats\n\n# In[4]:\n\n\n# data types of attributes\nhour_df.dtypes\n\n\n# In[5]:\n\n\n# dataset summary stats\nhour_df.describe()\n\n\n# The dataset has:\n# + 17 attributes in total and 17k+ records\n# + Except dtedat, rest all are numeric(int or float)\n# + As stated on the UCI dataset page, the following attributes have been normalized (same is confirmed above):\n#     + temp, atemp\n#     + humidity\n#     + windspeed\n# + Dataset has many categorical variables like season, yr, holiday, weathersit and so on. These will need to handled with care\n\n# ## Standardize Attribute Names\n\n# In[6]:\n\n\nhour_df.rename(columns={\'instant\':\'rec_id\',\n                      \'dteday\':\'datetime\',\n                      \'holiday\':\'is_holiday\',\n                      \'workingday\':\'is_workingday\',\n                      \'weathersit\':\'weather_condition\',\n                      \'hum\':\'humidity\',\n                      \'mnth\':\'month\',\n                      \'cnt\':\'total_count\',\n                      \'hr\':\'hour\',\n                      \'yr\':\'year\'},inplace=True)\n\n\n# ## Typecast Attributes \n\n# In[7]:\n\n\n# date time conversion\nhour_df[\'datetime\'] = pd.to_datetime(hour_df.datetime)\n\n# categorical variables\nhour_df[\'season\'] = hour_df.season.astype(\'category\')\nhour_df[\'is_holiday\'] = hour_df.is_holiday.astype(\'category\')\nhour_df[\'weekday\'] = hour_df.weekday.astype(\'category\')\nhour_df[\'weather_condition\'] = hour_df.weather_condition.astype(\'category\')\nhour_df[\'is_workingday\'] = hour_df.is_workingday.astype(\'category\')\nhour_df[\'month\'] = hour_df.month.astype(\'category\')\nhour_df[\'year\'] = hour_df.year.astype(\'category\')\nhour_df[\'hour\'] = hour_df.hour.astype(\'category\')\n\n\n# ## Visualize Attributes, Trends and Relationships\n\n# ### Hourly distribution of Total Counts\n# + Seasons are encoded as 1:spring, 2:summer, 3:fall, 4:winter\n# + Exercise: Convert season names to readable strings and visualize data again\n\n# In[8]:\n\n\nfig,ax = plt.subplots()\nsn.pointplot(data=hour_df[[\'hour\',\n                           \'total_count\',\n                           \'season\']],\n             x=\'hour\',y=\'total_count\',\n             hue=\'season\',ax=ax)\nax.set(title=""Season wise hourly distribution of counts"")\n\n\n# + The above plot shows peaks around 8am and 5pm (office hours)\n# + Overall higher usage in the second half of the day\n\n# In[9]:\n\n\nfig,ax = plt.subplots()\nsn.pointplot(data=hour_df[[\'hour\',\'total_count\',\'weekday\']],x=\'hour\',y=\'total_count\',hue=\'weekday\',ax=ax)\nax.set(title=""Weekday wise hourly distribution of counts"")\n\n\n# + Weekends (0 and 6) and Weekdays (1-5) show different usage trends with weekend\'s peak usage in during afternoon hours\n# + Weekdays follow the overall trend, similar to one visualized in the previous plot\n# + Weekdays have higher usage as compared to weekends\n# + It would be interesting to see the trends for casual and registered users separately\n\n# In[10]:\n\n\nfig,ax = plt.subplots()\nsn.boxplot(data=hour_df[[\'hour\',\'total_count\']],x=""hour"",y=""total_count"",ax=ax)\nax.set(title=""Box Pot for hourly distribution of counts"")\n\n\n# + Early hours (0-4) and late nights (21-23) have low counts but significant outliers\n# + Afternoon hours also have outliers\n# + Peak hours have higher medians and overall counts with virtually no outliers\n\n# ### Monthly distribution of Total Counts\n\n# In[11]:\n\n\nfig,ax = plt.subplots()\nsn.barplot(data=hour_df[[\'month\',\n                         \'total_count\']],\n           x=""month"",y=""total_count"")\nax.set(title=""Monthly distribution of counts"")\n\n\n# + Months June-Oct have highest counts. Fall seems to be favorite time of the year to use cycles\n\n# In[12]:\n\n\ndf_col_list = [\'month\',\'weekday\',\'total_count\']\nplot_col_list= [\'month\',\'total_count\']\nspring_df = hour_df[hour_df.season==1][df_col_list]\nsummer_df = hour_df[hour_df.season==2][df_col_list]\nfall_df = hour_df[hour_df.season==3][df_col_list]\nwinter_df = hour_df[hour_df.season==4][df_col_list]\n\nfig,ax= plt.subplots(nrows=2,ncols=2)\nsn.barplot(data=spring_df[plot_col_list],x=""month"",y=""total_count"",ax=ax[0][0],)\nax[0][0].set(title=""Spring"")\n\nsn.barplot(data=summer_df[plot_col_list],x=""month"",y=""total_count"",ax=ax[0][1])\nax[0][1].set(title=""Summer"")\n\nsn.barplot(data=fall_df[plot_col_list],x=""month"",y=""total_count"",ax=ax[1][0])\nax[1][0].set(title=""Fall"")\n\nsn.barplot(data=winter_df[plot_col_list],x=""month"",y=""total_count"",ax=ax[1][1])  \nax[1][1].set(title=""Winter"")\n\n\n# ### Year Wise Count Distributions\n\n# In[13]:\n\n\nsn.violinplot(data=hour_df[[\'year\',\n                            \'total_count\']],\n              x=""year"",y=""total_count"")\n\n\n# + Both years have multimodal distributions\n# + 2011 has lower counts overall with a lower median\n# + 2012 has a higher max count though the peaks are around 100 and 300 which is then tapering off\n\n# ### Working Day Vs Holiday Distribution\n\n# In[14]:\n\n\nfig,(ax1,ax2) = plt.subplots(ncols=2)\nsn.barplot(data=hour_df,x=\'is_holiday\',y=\'total_count\',hue=\'season\',ax=ax1)\nsn.barplot(data=hour_df,x=\'is_workingday\',y=\'total_count\',hue=\'season\',ax=ax2)\n\n\n# ### Outliers\n\n# In[15]:\n\n\nfig,(ax1,ax2)= plt.subplots(ncols=2)\nsn.boxplot(data=hour_df[[\'total_count\',\n                         \'casual\',\'registered\']],ax=ax1)\nsn.boxplot(data=hour_df[[\'temp\',\'windspeed\']],ax=ax2)\n\n\n# ### Correlations\n\n# In[16]:\n\n\ncorrMatt = hour_df[[""temp"",""atemp"",\n                    ""humidity"",""windspeed"",\n                    ""casual"",""registered"",\n                    ""total_count""]].corr()\nmask = np.array(corrMatt)\nmask[np.tril_indices_from(mask)] = False\nsn.heatmap(corrMatt, mask=mask,\n           vmax=.8, square=True,annot=True)\n\n\n# + Correlation between temp and atemp is very high (as expected)\n# + Same is te case with registered-total_count and casual-total_count\n# + Windspeed to humidity has negative correlation\n# + Overall correlational statistics are not very high.\n'"
notebooks/Ch06_Analyzing_Bike_Sharing_Trends/decision_tree_regression.py,0,"b'\n# coding: utf-8\n\n# # Bike Sharing Dataset using Decision Tree Regressor\n# \n# + Based on Bike Sharing dataset from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset)\n# + This notebook is based upon the hourly data file, i.e. hour.csv\n# + This notebook showcases regression using Decision Trees\n\n# ### Problem Statement\n# Given the Bike Sharing dataset with hourly level information of bikes along with weather and other attributes, model a system which can predict the bike count.\n\n# ## Import required packages\n\n# In[1]:\n\n\nget_ipython().magic(\'matplotlib inline\')\n\n# data manuipulation\nimport numpy as np\nimport pandas as pd\n\n# modeling utilities\nimport pydotplus \nfrom sklearn import tree\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\n\n\n# plotting libraries\nimport seaborn as sn\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\n\nsn.set_style(\'whitegrid\')\nsn.set_context(\'talk\')\nparams = {\'legend.fontsize\': \'x-large\',\n          \'figure.figsize\': (30, 10),\n          \'axes.labelsize\': \'x-large\',\n          \'axes.titlesize\':\'x-large\',\n          \'xtick.labelsize\':\'x-large\',\n          \'ytick.labelsize\':\'x-large\'}\n\nplt.rcParams.update(params)\n\n\n# ## Load Dataset\n\n# In[2]:\n\n\nhour_df = pd.read_csv(\'hour.csv\')\nprint(""Shape of dataset::{}"".format(hour_df.shape))\n\n\n# ## Preprocessing\n# + Standarize column names\n# + Typecast attributes\n# + Encode Categoricals using One Hot Encoding\n\n# ### Standarize Column Names\n\n# In[3]:\n\n\nhour_df.rename(columns={\'instant\':\'rec_id\',\n                        \'dteday\':\'datetime\',\n                        \'holiday\':\'is_holiday\',\n                        \'workingday\':\'is_workingday\',\n                        \'weathersit\':\'weather_condition\',\n                        \'hum\':\'humidity\',\n                        \'mnth\':\'month\',\n                        \'cnt\':\'total_count\',\n                        \'hr\':\'hour\',\n                        \'yr\':\'year\'},inplace=True)\n\n\n# ### Typecast Attributes\n\n# In[4]:\n\n\n# date time conversion\nhour_df[\'datetime\'] = pd.to_datetime(hour_df.datetime)\n\n# categorical variables\nhour_df[\'season\'] = hour_df.season.astype(\'category\')\nhour_df[\'is_holiday\'] = hour_df.is_holiday.astype(\'category\')\nhour_df[\'weekday\'] = hour_df.weekday.astype(\'category\')\nhour_df[\'weather_condition\'] = hour_df.weather_condition.astype(\'category\')\nhour_df[\'is_workingday\'] = hour_df.is_workingday.astype(\'category\')\nhour_df[\'month\'] = hour_df.month.astype(\'category\')\nhour_df[\'year\'] = hour_df.year.astype(\'category\')\nhour_df[\'hour\'] = hour_df.hour.astype(\'category\')\n\n\n# \n# ### Encode Categoricals (One Hot Encoding)\n\n# In[5]:\n\n\ndef fit_transform_ohe(df,col_name):\n    """"""This function performs one hot encoding for the specified\n        column.\n\n    Args:\n        df(pandas.DataFrame): the data frame containing the mentioned column name\n        col_name: the column to be one hot encoded\n\n    Returns:\n        tuple: label_encoder, one_hot_encoder, transformed column as pandas Series\n\n    """"""\n    # label encode the column\n    le = preprocessing.LabelEncoder()\n    le_labels = le.fit_transform(df[col_name])\n    df[col_name+\'_label\'] = le_labels\n    \n    # one hot encoding\n    ohe = preprocessing.OneHotEncoder()\n    feature_arr = ohe.fit_transform(df[[col_name+\'_label\']]).toarray()\n    feature_labels = [col_name+\'_\'+str(cls_label) for cls_label in le.classes_]\n    features_df = pd.DataFrame(feature_arr, columns=feature_labels)\n    \n    return le,ohe,features_df\n\n# given label encoder and one hot encoder objects, \n# encode attribute to ohe\ndef transform_ohe(df,le,ohe,col_name):\n    """"""This function performs one hot encoding for the specified\n        column using the specified encoder objects.\n\n    Args:\n        df(pandas.DataFrame): the data frame containing the mentioned column name\n        le(Label Encoder): the label encoder object used to fit label encoding\n        ohe(One Hot Encoder): the onen hot encoder object used to fit one hot encoding\n        col_name: the column to be one hot encoded\n\n    Returns:\n        tuple: transformed column as pandas Series\n\n    """"""\n    # label encode\n    col_labels = le.transform(df[col_name])\n    df[col_name+\'_label\'] = col_labels\n    \n    # ohe \n    feature_arr = ohe.fit_transform(df[[col_name+\'_label\']]).toarray()\n    feature_labels = [col_name+\'_\'+str(cls_label) for cls_label in le.classes_]\n    features_df = pd.DataFrame(feature_arr, columns=feature_labels)\n    \n    return features_df\n\n\n# ## Train-Test Split\n\n# In[6]:\n\n\nX, X_test, y, y_test = train_test_split(hour_df.iloc[:,0:-3], hour_df.iloc[:,-1], \n                                                    test_size=0.33, random_state=42)\n\nX.reset_index(inplace=True)\ny = y.reset_index()\n\nX_test.reset_index(inplace=True)\ny_test = y_test.reset_index()\n\nprint(""Training set::{}{}"".format(X.shape,y.shape))\nprint(""Testing set::{}"".format(X_test.shape))\n\n\n# In[7]:\n\n\ncat_attr_list = [\'season\',\'is_holiday\',\n                 \'weather_condition\',\'is_workingday\',\n                 \'hour\',\'weekday\',\'month\',\'year\']\nnumeric_feature_cols = [\'temp\',\'humidity\',\'windspeed\',\'hour\',\'weekday\',\'month\',\'year\']\nsubset_cat_features =  [\'season\',\'is_holiday\',\'weather_condition\',\'is_workingday\']\n\n\n# In[8]:\n\n\nencoded_attr_list = []\nfor col in cat_attr_list:\n    return_obj = fit_transform_ohe(X,col)\n    encoded_attr_list.append({\'label_enc\':return_obj[0],\n                              \'ohe_enc\':return_obj[1],\n                              \'feature_df\':return_obj[2],\n                              \'col_name\':col})\n\n\n# In[9]:\n\n\nfeature_df_list = [X[numeric_feature_cols]]\nfeature_df_list.extend([enc[\'feature_df\']                         for enc in encoded_attr_list                         if enc[\'col_name\'] in subset_cat_features])\n\ntrain_df_new = pd.concat(feature_df_list, axis=1)\nprint(""Shape::{}"".format(train_df_new.shape))\n\n\n# ## Decision Tree based Regression\n\n# In[10]:\n\n\nX = train_df_new\ny= y.total_count.values.reshape(-1,1)\n\n\n# In[11]:\n\n\nX.shape,y.shape\n\n\n# ## Sample Decision Tree Regressor\n\n# In[12]:\n\n\ndtr = DecisionTreeRegressor(max_depth=4,\n                            min_samples_split=5,\n                            max_leaf_nodes=10)\ndtr.fit(X,y)\n\n\n# In[13]:\n\n\ndtr.score(X,y)\n\n\n# ### Plot the Learnt Model\n\n# In[14]:\n\n\ndot_data = tree.export_graphviz(dtr, out_file=None) \ngraph = pydotplus.graph_from_dot_data(dot_data) \ngraph.write_pdf(""bikeshare.pdf"") \n\n\n# ### Grid Search With Cross Validation\n\n# In[15]:\n\n\nparam_grid = {""criterion"": [""mse"", ""mae""],\n              ""min_samples_split"": [10, 20, 40],\n              ""max_depth"": [2, 6, 8],\n              ""min_samples_leaf"": [20, 40, 100],\n              ""max_leaf_nodes"": [5, 20, 100, 500, 800],\n              }\n\n\n# In[16]:\n\n\ngrid_cv_dtr = GridSearchCV(dtr, param_grid, cv=5)\n\n\n# In[17]:\n\n\ngrid_cv_dtr.fit(X,y)\n\n\n# ### Cross Validation: Best Model Details\n\n# In[18]:\n\n\nprint(""R-Squared::{}"".format(grid_cv_dtr.best_score_))\nprint(""Best Hyperparameters::\\n{}"".format(grid_cv_dtr.best_params_))\n\n\n# In[19]:\n\n\ndf = pd.DataFrame(data=grid_cv_dtr.cv_results_)\ndf.head()\n\n\n# In[20]:\n\n\nfig,ax = plt.subplots()\nsn.pointplot(data=df[[\'mean_test_score\',\n                           \'param_max_leaf_nodes\',\n                           \'param_max_depth\']],\n             y=\'mean_test_score\',x=\'param_max_depth\',\n             hue=\'param_max_leaf_nodes\',ax=ax)\nax.set(title=""Effect of Depth and Leaf Nodes on Model Performance"")\n\n\n# ### Residual Plot\n\n# In[21]:\n\n\npredicted = grid_cv_dtr.best_estimator_.predict(X)\nresiduals = y.flatten()-predicted\n\n\n# In[22]:\n\n\nfig, ax = plt.subplots()\nax.scatter(y.flatten(), residuals)\nax.axhline(lw=2,color=\'black\')\nax.set_xlabel(\'Observed\')\nax.set_ylabel(\'Residual\')\nplt.show()\n\n\n# In[23]:\n\n\nr2_scores = cross_val_score(grid_cv_dtr.best_estimator_, X, y, cv=10)\nmse_scores = cross_val_score(grid_cv_dtr.best_estimator_, X, y, cv=10,scoring=\'neg_mean_squared_error\')\n\n\n# In[24]:\n\n\nprint(""avg R-squared::{}"".format(np.mean(r2_scores)))\nprint(""MSE::{}"".format(np.mean(mse_scores)))\n\n\n# ### Setting the model for Testing\n\n# In[25]:\n\n\nbest_dtr_model = grid_cv_dtr.best_estimator_\n\n\n# ## Test Dataset Performance\n\n# In[26]:\n\n\ntest_encoded_attr_list = []\nfor enc in encoded_attr_list:\n    col_name = enc[\'col_name\']\n    le = enc[\'label_enc\']\n    ohe = enc[\'ohe_enc\']\n    test_encoded_attr_list.append({\'feature_df\':transform_ohe(X_test,le,ohe,col_name),\n                                   \'col_name\':col_name})\n    \n    \ntest_feature_df_list = [X_test[numeric_feature_cols]]\ntest_feature_df_list.extend([enc[\'feature_df\'] for enc in test_encoded_attr_list if enc[\'col_name\'] in subset_cat_features])\n\ntest_df_new = pd.concat(test_feature_df_list, axis=1) \nprint(""Shape::{}"".format(test_df_new.shape))\n\n\n# In[27]:\n\n\nX_test = test_df_new\ny_test = y_test.total_count.values.reshape(-1,1)\n\n\n# In[28]:\n\n\ny_pred = best_dtr_model.predict(X_test)\nresiduals = y_test.flatten() - y_pred\n\n\n# In[29]:\n\n\nr2_score = best_dtr_model.score(X_test,y_test)\nprint(""R-squared::{}"".format(r2_score))\nprint(""MSE: %.2f""\n      % metrics.mean_squared_error(y_test, y_pred))\n\n\n# In[30]:\n\n\nfig, ax = plt.subplots()\nax.scatter(y_test.flatten(), residuals)\nax.axhline(lw=2,color=\'black\')\nax.set_xlabel(\'Observed\')\nax.set_ylabel(\'Residual\')\nplt.show()\n\nr2_score = grid_cv_dtr.best_estimator_.score(X_test,y_test)\n\n'"
notebooks/Ch06_Analyzing_Bike_Sharing_Trends/linear_regression.py,0,"b'\n# coding: utf-8\n\n# # Bike Sharing Dataset Linear Modeling\n# \n# + Based on Bike Sharing dataset from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset)\n# + This notebook is based upon the hourly data file, i.e. hour.csv\n# + This notebook showcases linear modeling using linear regression\n\n# ### Problem Statement\n# Given the Bike Sharing dataset with hourly level information of bikes along with weather and other attributes, model a system which can predict the bike count.\n\n# ## Import required packages\n\n# In[1]:\n\n\nget_ipython().magic(\'matplotlib inline\')\n# data manuipulation\nimport numpy as np\nimport pandas as pd\n\n# modeling utilities\nimport scipy.stats as stats\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn import  linear_model\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_predict\n\n# plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\n\nsn.set_style(\'whitegrid\')\nsn.set_context(\'talk\')\nparams = {\'legend.fontsize\': \'x-large\',\n          \'figure.figsize\': (30, 10),\n          \'axes.labelsize\': \'x-large\',\n          \'axes.titlesize\':\'x-large\',\n          \'xtick.labelsize\':\'x-large\',\n          \'ytick.labelsize\':\'x-large\'}\n\nplt.rcParams.update(params)\n\n\n# ## Load Dataset\n\n# In[2]:\n\n\nhour_df = pd.read_csv(\'hour.csv\')\nprint(""Shape of dataset::{}"".format(hour_df.shape))\n\n\n# ## Preprocessing\n# + Standarize column names\n# + Typecast attributes\n# + Encode Categoricals using One Hot Encoding\n\n# ### Standarize Column Names\n\n# In[3]:\n\n\nhour_df.rename(columns={\'instant\':\'rec_id\',\n                        \'dteday\':\'datetime\',\n                        \'holiday\':\'is_holiday\',\n                        \'workingday\':\'is_workingday\',\n                        \'weathersit\':\'weather_condition\',\n                        \'hum\':\'humidity\',\n                        \'mnth\':\'month\',\n                        \'cnt\':\'total_count\',\n                        \'hr\':\'hour\',\n                        \'yr\':\'year\'},inplace=True)\n\n\n# ### Typecast Attributes\n\n# In[4]:\n\n\n# date time conversion\nhour_df[\'datetime\'] = pd.to_datetime(hour_df.datetime)\n\n# categorical variables\nhour_df[\'season\'] = hour_df.season.astype(\'category\')\nhour_df[\'is_holiday\'] = hour_df.is_holiday.astype(\'category\')\nhour_df[\'weekday\'] = hour_df.weekday.astype(\'category\')\nhour_df[\'weather_condition\'] = hour_df.weather_condition.astype(\'category\')\nhour_df[\'is_workingday\'] = hour_df.is_workingday.astype(\'category\')\nhour_df[\'month\'] = hour_df.month.astype(\'category\')\nhour_df[\'year\'] = hour_df.year.astype(\'category\')\nhour_df[\'hour\'] = hour_df.hour.astype(\'category\')\n\n\n# \n# ### Encode Categoricals (One Hot Encoding)\n\n# In[5]:\n\n\ndef fit_transform_ohe(df,col_name):\n    """"""This function performs one hot encoding for the specified\n        column.\n\n    Args:\n        df(pandas.DataFrame): the data frame containing the mentioned column name\n        col_name: the column to be one hot encoded\n\n    Returns:\n        tuple: label_encoder, one_hot_encoder, transformed column as pandas Series\n\n    """"""\n    # label encode the column\n    le = preprocessing.LabelEncoder()\n    le_labels = le.fit_transform(df[col_name])\n    df[col_name+\'_label\'] = le_labels\n    \n    # one hot encoding\n    ohe = preprocessing.OneHotEncoder()\n    feature_arr = ohe.fit_transform(df[[col_name+\'_label\']]).toarray()\n    feature_labels = [col_name+\'_\'+str(cls_label) for cls_label in le.classes_]\n    features_df = pd.DataFrame(feature_arr, columns=feature_labels)\n    \n    return le,ohe,features_df\n\n# given label encoder and one hot encoder objects, \n# encode attribute to ohe\ndef transform_ohe(df,le,ohe,col_name):\n    """"""This function performs one hot encoding for the specified\n        column using the specified encoder objects.\n\n    Args:\n        df(pandas.DataFrame): the data frame containing the mentioned column name\n        le(Label Encoder): the label encoder object used to fit label encoding\n        ohe(One Hot Encoder): the onen hot encoder object used to fit one hot encoding\n        col_name: the column to be one hot encoded\n\n    Returns:\n        tuple: transformed column as pandas Series\n\n    """"""\n    # label encode\n    col_labels = le.transform(df[col_name])\n    df[col_name+\'_label\'] = col_labels\n    \n    # ohe \n    feature_arr = ohe.fit_transform(df[[col_name+\'_label\']]).toarray()\n    feature_labels = [col_name+\'_\'+str(cls_label) for cls_label in le.classes_]\n    features_df = pd.DataFrame(feature_arr, columns=feature_labels)\n    \n    return features_df\n\n\n# ## Train-Test Split\n\n# In[6]:\n\n\nX, X_test, y, y_test = train_test_split(hour_df.iloc[:,0:-3], hour_df.iloc[:,-1], \n                                                    test_size=0.33, random_state=42)\n\nX.reset_index(inplace=True)\ny = y.reset_index()\n\nX_test.reset_index(inplace=True)\ny_test = y_test.reset_index()\n\nprint(""Training set::{}{}"".format(X.shape,y.shape))\nprint(""Testing set::{}"".format(X_test.shape))\n\n\n# ## Normality Test\n\n# In[7]:\n\n\nstats.probplot(y.total_count.tolist(), dist=""norm"", plot=plt)\nplt.show()\n\n\n# In[8]:\n\n\ncat_attr_list = [\'season\',\'is_holiday\',\n                 \'weather_condition\',\'is_workingday\',\n                 \'hour\',\'weekday\',\'month\',\'year\']\nnumeric_feature_cols = [\'temp\',\'humidity\',\'windspeed\',\'hour\',\'weekday\',\'month\',\'year\']\nsubset_cat_features =  [\'season\',\'is_holiday\',\'weather_condition\',\'is_workingday\']\n\n\n# In[9]:\n\n\nencoded_attr_list = []\nfor col in cat_attr_list:\n    return_obj = fit_transform_ohe(X,col)\n    encoded_attr_list.append({\'label_enc\':return_obj[0],\n                              \'ohe_enc\':return_obj[1],\n                              \'feature_df\':return_obj[2],\n                              \'col_name\':col})\n\n\n# In[10]:\n\n\nfeature_df_list = [X[numeric_feature_cols]]\nfeature_df_list.extend([enc[\'feature_df\']                         for enc in encoded_attr_list                         if enc[\'col_name\'] in subset_cat_features])\n\ntrain_df_new = pd.concat(feature_df_list, axis=1)\nprint(""Shape::{}"".format(train_df_new.shape))\n\n\n# ## Linear Regression\n\n# In[11]:\n\n\nX = train_df_new\ny= y.total_count.values.reshape(-1,1)\n\nlin_reg = linear_model.LinearRegression()\n\n\n# ### Cross Validation\n\n# In[12]:\n\n\npredicted = cross_val_predict(lin_reg, X, y, cv=10)\n\nfig, ax = plt.subplots()\nax.scatter(y, y-predicted)\nax.axhline(lw=2,color=\'black\')\nax.set_xlabel(\'Observed\')\nax.set_ylabel(\'Residual\')\nplt.show()\n\n\n# In[13]:\n\n\nr2_scores = cross_val_score(lin_reg, X, y, cv=10)\nmse_scores = cross_val_score(lin_reg, X, y, cv=10,scoring=\'neg_mean_squared_error\')\n\n\n# In[14]:\n\n\nfig, ax = plt.subplots()\nax.plot([i for i in range(len(r2_scores))],r2_scores,lw=2)\nax.set_xlabel(\'Iteration\')\nax.set_ylabel(\'R-Squared\')\nax.title.set_text(""Cross Validation Scores, Avg:{}"".format(np.average(r2_scores)))\nplt.show()\n\n\n# In[15]:\n\n\nprint(""R-squared::{}"".format(r2_scores))\nprint(""MSE::{}"".format(mse_scores))\n\n\n# In[16]:\n\n\nlin_reg.fit(X,y)\n\n\n# ## Test Dataset Performance\n\n# In[17]:\n\n\ntest_encoded_attr_list = []\nfor enc in encoded_attr_list:\n    col_name = enc[\'col_name\']\n    le = enc[\'label_enc\']\n    ohe = enc[\'ohe_enc\']\n    test_encoded_attr_list.append({\'feature_df\':transform_ohe(X_test,\n                                                              le,ohe,\n                                                              col_name),\n                                   \'col_name\':col_name})\n    \n    \ntest_feature_df_list = [X_test[numeric_feature_cols]]\ntest_feature_df_list.extend([enc[\'feature_df\']                              for enc in test_encoded_attr_list                              if enc[\'col_name\'] in subset_cat_features])\n\ntest_df_new = pd.concat(test_feature_df_list, axis=1) \nprint(""Shape::{}"".format(test_df_new.shape))\n\n\n# In[18]:\n\n\ntest_df_new.head()\n\n\n# In[19]:\n\n\nX_test = test_df_new\ny_test = y_test.total_count.values.reshape(-1,1)\n\ny_pred = lin_reg.predict(X_test)\n\nresiduals = y_test-y_pred\n\n\n# In[20]:\n\n\nr2_score = lin_reg.score(X_test,y_test)\nprint(""R-squared::{}"".format(r2_score))\nprint(""MSE: %.2f""\n      % metrics.mean_squared_error(y_test, y_pred))\n\n\n# In[21]:\n\n\nfig, ax = plt.subplots()\nax.scatter(y_test, residuals)\nax.axhline(lw=2,color=\'black\')\nax.set_xlabel(\'Observed\')\nax.set_ylabel(\'Residuals\')\nax.title.set_text(""Residual Plot with R-Squared={}"".format(np.average(r2_score)))\nplt.show()\n\n\n# ## Stats Models\n\n# In[22]:\n\n\nimport statsmodels.api as sm\n\n# Set the independent variable\nX = X.values.tolist()\n\n# This handles the intercept. \n# Statsmodel takes 0 intercept by default\nX = sm.add_constant(X)\n\nX_test = X_test.values.tolist()\nX_test = sm.add_constant(X_test)\n\n\n# Build OLS model\nmodel = sm.OLS(y, X)\nresults = model.fit()\n\n# Get the predicted values for dependent variable\npred_y = results.predict(X_test)\n\n# View Model stats\nprint(results.summary())\n\n\n# In[23]:\n\n\nplt.scatter(pred_y,y_test)\n\n'"
notebooks/Ch07_Analyzing_Movie_Reviews_Sentiment/contractions.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Mon Aug 01 01:11:02 2016\n\n@author: DIP\n""""""\n\nCONTRACTION_MAP = {\n""ain\'t"": ""is not"",\n""aren\'t"": ""are not"",\n""can\'t"": ""cannot"",\n""can\'t\'ve"": ""cannot have"",\n""\'cause"": ""because"",\n""could\'ve"": ""could have"",\n""couldn\'t"": ""could not"",\n""couldn\'t\'ve"": ""could not have"",\n""didn\'t"": ""did not"",\n""doesn\'t"": ""does not"",\n""don\'t"": ""do not"",\n""hadn\'t"": ""had not"",\n""hadn\'t\'ve"": ""had not have"",\n""hasn\'t"": ""has not"",\n""haven\'t"": ""have not"",\n""he\'d"": ""he would"",\n""he\'d\'ve"": ""he would have"",\n""he\'ll"": ""he will"",\n""he\'ll\'ve"": ""he will have"",\n""he\'s"": ""he is"",\n""how\'d"": ""how did"",\n""how\'d\'y"": ""how do you"",\n""how\'ll"": ""how will"",\n""how\'s"": ""how is"",\n""I\'d"": ""I would"",\n""I\'d\'ve"": ""I would have"",\n""I\'ll"": ""I will"",\n""I\'ll\'ve"": ""I will have"",\n""I\'m"": ""I am"",\n""I\'ve"": ""I have"",\n""i\'d"": ""i would"",\n""i\'d\'ve"": ""i would have"",\n""i\'ll"": ""i will"",\n""i\'ll\'ve"": ""i will have"",\n""i\'m"": ""i am"",\n""i\'ve"": ""i have"",\n""isn\'t"": ""is not"",\n""it\'d"": ""it would"",\n""it\'d\'ve"": ""it would have"",\n""it\'ll"": ""it will"",\n""it\'ll\'ve"": ""it will have"",\n""it\'s"": ""it is"",\n""let\'s"": ""let us"",\n""ma\'am"": ""madam"",\n""mayn\'t"": ""may not"",\n""might\'ve"": ""might have"",\n""mightn\'t"": ""might not"",\n""mightn\'t\'ve"": ""might not have"",\n""must\'ve"": ""must have"",\n""mustn\'t"": ""must not"",\n""mustn\'t\'ve"": ""must not have"",\n""needn\'t"": ""need not"",\n""needn\'t\'ve"": ""need not have"",\n""o\'clock"": ""of the clock"",\n""oughtn\'t"": ""ought not"",\n""oughtn\'t\'ve"": ""ought not have"",\n""shan\'t"": ""shall not"",\n""sha\'n\'t"": ""shall not"",\n""shan\'t\'ve"": ""shall not have"",\n""she\'d"": ""she would"",\n""she\'d\'ve"": ""she would have"",\n""she\'ll"": ""she will"",\n""she\'ll\'ve"": ""she will have"",\n""she\'s"": ""she is"",\n""should\'ve"": ""should have"",\n""shouldn\'t"": ""should not"",\n""shouldn\'t\'ve"": ""should not have"",\n""so\'ve"": ""so have"",\n""so\'s"": ""so as"",\n""that\'d"": ""that would"",\n""that\'d\'ve"": ""that would have"",\n""that\'s"": ""that is"",\n""there\'d"": ""there would"",\n""there\'d\'ve"": ""there would have"",\n""there\'s"": ""there is"",\n""they\'d"": ""they would"",\n""they\'d\'ve"": ""they would have"",\n""they\'ll"": ""they will"",\n""they\'ll\'ve"": ""they will have"",\n""they\'re"": ""they are"",\n""they\'ve"": ""they have"",\n""to\'ve"": ""to have"",\n""wasn\'t"": ""was not"",\n""we\'d"": ""we would"",\n""we\'d\'ve"": ""we would have"",\n""we\'ll"": ""we will"",\n""we\'ll\'ve"": ""we will have"",\n""we\'re"": ""we are"",\n""we\'ve"": ""we have"",\n""weren\'t"": ""were not"",\n""what\'ll"": ""what will"",\n""what\'ll\'ve"": ""what will have"",\n""what\'re"": ""what are"",\n""what\'s"": ""what is"",\n""what\'ve"": ""what have"",\n""when\'s"": ""when is"",\n""when\'ve"": ""when have"",\n""where\'d"": ""where did"",\n""where\'s"": ""where is"",\n""where\'ve"": ""where have"",\n""who\'ll"": ""who will"",\n""who\'ll\'ve"": ""who will have"",\n""who\'s"": ""who is"",\n""who\'ve"": ""who have"",\n""why\'s"": ""why is"",\n""why\'ve"": ""why have"",\n""will\'ve"": ""will have"",\n""won\'t"": ""will not"",\n""won\'t\'ve"": ""will not have"",\n""would\'ve"": ""would have"",\n""wouldn\'t"": ""would not"",\n""wouldn\'t\'ve"": ""would not have"",\n""y\'all"": ""you all"",\n""y\'all\'d"": ""you all would"",\n""y\'all\'d\'ve"": ""you all would have"",\n""y\'all\'re"": ""you all are"",\n""y\'all\'ve"": ""you all have"",\n""you\'d"": ""you would"",\n""you\'d\'ve"": ""you would have"",\n""you\'ll"": ""you will"",\n""you\'ll\'ve"": ""you will have"",\n""you\'re"": ""you are"",\n""you\'ve"": ""you have""\n}\n'"
notebooks/Ch07_Analyzing_Movie_Reviews_Sentiment/model_evaluation_utils.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Mon Jul 31 20:05:23 2017\n\n@author: DIP\n@Copyright: Dipanjan Sarkar\n""""""\n\nfrom sklearn import metrics\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.base import clone\nfrom sklearn.preprocessing import label_binarize\nfrom scipy import interp\nfrom sklearn.metrics import roc_curve, auc \n\n\ndef get_metrics(true_labels, predicted_labels):\n    \n    print(\'Accuracy:\', np.round(\n                        metrics.accuracy_score(true_labels, \n                                               predicted_labels),\n                        4))\n    print(\'Precision:\', np.round(\n                        metrics.precision_score(true_labels, \n                                               predicted_labels,\n                                               average=\'weighted\'),\n                        4))\n    print(\'Recall:\', np.round(\n                        metrics.recall_score(true_labels, \n                                               predicted_labels,\n                                               average=\'weighted\'),\n                        4))\n    print(\'F1 Score:\', np.round(\n                        metrics.f1_score(true_labels, \n                                               predicted_labels,\n                                               average=\'weighted\'),\n                        4))\n                        \n\ndef train_predict_model(classifier, \n                        train_features, train_labels, \n                        test_features, test_labels):\n    # build model    \n    classifier.fit(train_features, train_labels)\n    # predict using model\n    predictions = classifier.predict(test_features) \n    return predictions    \n\n\ndef display_confusion_matrix(true_labels, predicted_labels, classes=[1,0]):\n    \n    total_classes = len(classes)\n    level_labels = [total_classes*[0], list(range(total_classes))]\n\n    cm = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels, \n                                  labels=classes)\n    cm_frame = pd.DataFrame(data=cm, \n                            columns=pd.MultiIndex(levels=[[\'Predicted:\'], classes], \n                                                  labels=level_labels), \n                            index=pd.MultiIndex(levels=[[\'Actual:\'], classes], \n                                                labels=level_labels)) \n    print(cm_frame) \n    \ndef display_classification_report(true_labels, predicted_labels, classes=[1,0]):\n\n    report = metrics.classification_report(y_true=true_labels, \n                                           y_pred=predicted_labels, \n                                           labels=classes) \n    print(report)\n    \n    \n    \ndef display_model_performance_metrics(true_labels, predicted_labels, classes=[1,0]):\n    print(\'Model Performance metrics:\')\n    print(\'-\'*30)\n    get_metrics(true_labels=true_labels, predicted_labels=predicted_labels)\n    print(\'\\nModel Classification report:\')\n    print(\'-\'*30)\n    display_classification_report(true_labels=true_labels, predicted_labels=predicted_labels, \n                                  classes=classes)\n    print(\'\\nPrediction Confusion Matrix:\')\n    print(\'-\'*30)\n    display_confusion_matrix(true_labels=true_labels, predicted_labels=predicted_labels, \n                             classes=classes)\n\n\ndef plot_model_decision_surface(clf, train_features, train_labels,\n                                plot_step=0.02, cmap=plt.cm.RdYlBu,\n                                markers=None, alphas=None, colors=None):\n    \n    if train_features.shape[1] != 2:\n        raise ValueError(""X_train should have exactly 2 columnns!"")\n    \n    x_min, x_max = train_features[:, 0].min() - plot_step, train_features[:, 0].max() + plot_step\n    y_min, y_max = train_features[:, 1].min() - plot_step, train_features[:, 1].max() + plot_step\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                         np.arange(y_min, y_max, plot_step))\n\n    clf_est = clone(clf)\n    clf_est.fit(train_features,train_labels)\n    if hasattr(clf_est, \'predict_proba\'):\n        Z = clf_est.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n    else:\n        Z = clf_est.predict(np.c_[xx.ravel(), yy.ravel()])    \n    Z = Z.reshape(xx.shape)\n    cs = plt.contourf(xx, yy, Z, cmap=cmap)\n    \n    le = LabelEncoder()\n    y_enc = le.fit_transform(train_labels)\n    n_classes = len(le.classes_)\n    plot_colors = \'\'.join(colors) if colors else [None] * n_classes\n    label_names = le.classes_\n    markers = markers if markers else [None] * n_classes\n    alphas = alphas if alphas else [None] * n_classes\n    for i, color in zip(range(n_classes), plot_colors):\n        idx = np.where(y_enc == i)\n        plt.scatter(train_features[idx, 0], train_features[idx, 1], c=color,\n                    label=label_names[i], cmap=cmap, edgecolors=\'black\', \n                    marker=markers[i], alpha=alphas[i])\n    plt.legend()\n    plt.show()\n\n\ndef plot_model_roc_curve(clf, features, true_labels, label_encoder=None, class_names=None):\n    \n    ## Compute ROC curve and ROC area for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    if hasattr(clf, \'classes_\'):\n        class_labels = clf.classes_\n    elif label_encoder:\n        class_labels = label_encoder.classes_\n    elif class_names:\n        class_labels = class_names\n    else:\n        raise ValueError(\'Unable to derive prediction classes, please specify class_names!\')\n    n_classes = len(class_labels)\n    y_test = label_binarize(true_labels, classes=class_labels)\n    if n_classes == 2:\n        if hasattr(clf, \'predict_proba\'):\n            prob = clf.predict_proba(features)\n            y_score = prob[:, prob.shape[1]-1] \n        elif hasattr(clf, \'decision_function\'):\n            prob = clf.decision_function(features)\n            y_score = prob[:, prob.shape[1]-1]\n        else:\n            raise AttributeError(""Estimator doesn\'t have a probability or confidence scoring system!"")\n        \n        fpr, tpr, _ = roc_curve(y_test, y_score)      \n        roc_auc = auc(fpr, tpr)\n        plt.plot(fpr, tpr, label=\'ROC curve (area = {0:0.2f})\'\n                                 \'\'.format(roc_auc),\n                 linewidth=2.5)\n        \n    elif n_classes > 2:\n        if hasattr(clf, \'predict_proba\'):\n            y_score = clf.predict_proba(features)\n        elif hasattr(clf, \'decision_function\'):\n            y_score = clf.decision_function(features)\n        else:\n            raise AttributeError(""Estimator doesn\'t have a probability or confidence scoring system!"")\n\n        for i in range(n_classes):\n            fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n            roc_auc[i] = auc(fpr[i], tpr[i])\n\n        ## Compute micro-average ROC curve and ROC area\n        fpr[""micro""], tpr[""micro""], _ = roc_curve(y_test.ravel(), y_score.ravel())\n        roc_auc[""micro""] = auc(fpr[""micro""], tpr[""micro""])\n\n        ## Compute macro-average ROC curve and ROC area\n        # First aggregate all false positive rates\n        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n        # Then interpolate all ROC curves at this points\n        mean_tpr = np.zeros_like(all_fpr)\n        for i in range(n_classes):\n            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n        # Finally average it and compute AUC\n        mean_tpr /= n_classes\n        fpr[""macro""] = all_fpr\n        tpr[""macro""] = mean_tpr\n        roc_auc[""macro""] = auc(fpr[""macro""], tpr[""macro""])\n\n        ## Plot ROC curves\n        plt.figure(figsize=(6, 4))\n        plt.plot(fpr[""micro""], tpr[""micro""],\n                 label=\'micro-average ROC curve (area = {0:0.2f})\'\n                       \'\'.format(roc_auc[""micro""]), linewidth=3)\n\n        plt.plot(fpr[""macro""], tpr[""macro""],\n                 label=\'macro-average ROC curve (area = {0:0.2f})\'\n                       \'\'.format(roc_auc[""macro""]), linewidth=3)\n\n        for i, label in enumerate(class_labels):\n            plt.plot(fpr[i], tpr[i], label=\'ROC curve of class {0} (area = {1:0.2f})\'\n                                           \'\'.format(label, roc_auc[i]), \n                     linewidth=2, linestyle=\':\')\n    else:\n        raise ValueError(\'Number of classes should be atleast 2 or more\')\n        \n    plt.plot([0, 1], [0, 1], \'k--\')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel(\'False Positive Rate\')\n    plt.ylabel(\'True Positive Rate\')\n    plt.title(\'Receiver Operating Characteristic (ROC) Curve\')\n    plt.legend(loc=""lower right"")\n    plt.show()\n\n\n'"
notebooks/Ch07_Analyzing_Movie_Reviews_Sentiment/sentiment_analysis_adv_deep_learning.py,0,"b'\n# coding: utf-8\n\n# # Import necessary depencencies\n\n# In[1]:\n\nimport pandas as pd\nimport numpy as np\nimport text_normalizer as tn\nimport model_evaluation_utils as meu\n\nnp.set_printoptions(precision=2, linewidth=80)\n\n\n# # Load and normalize data\n\n# In[2]:\n\ndataset = pd.read_csv(r\'movie_reviews.csv\')\n\n# take a peek at the data\nprint(dataset.head())\nreviews = np.array(dataset[\'review\'])\nsentiments = np.array(dataset[\'sentiment\'])\n\n# build train and test datasets\ntrain_reviews = reviews[:35000]\ntrain_sentiments = sentiments[:35000]\ntest_reviews = reviews[35000:]\ntest_sentiments = sentiments[35000:]\n\n# normalize datasets\nnorm_train_reviews = tn.normalize_corpus(train_reviews)\nnorm_test_reviews = tn.normalize_corpus(test_reviews)\n\n\n# # Tokenize train & test datasets\n\n# In[4]:\n\ntokenized_train = [tn.tokenizer.tokenize(text) for text in norm_train_reviews]\ntokenized_test = [tn.tokenizer.tokenize(text) for text in norm_test_reviews]\n\n\n# # Build Vocabulary Mapping (word to index)\n\n# In[93]:\n\nfrom collections import Counter\n\n# build word to index vocabulary\ntoken_counter = Counter([token for review in tokenized_train for token in review])\nvocab_map = {item[0]: index+1 for index, item in enumerate(dict(token_counter).items())}\nmax_index = np.max(list(vocab_map.values()))\nvocab_map[\'PAD_INDEX\'] = 0\nvocab_map[\'NOT_FOUND_INDEX\'] = max_index+1\nvocab_size = len(vocab_map)\n# view vocabulary size and part of the vocabulary map\nprint(\'Vocabulary Size:\', vocab_size)\nprint(\'Sample slice of vocabulary map:\', dict(list(vocab_map.items())[10:20]))\n\n\n# # Encode and Pad datasets & Encode prediction class labels\n\n# In[94]:\n\nfrom keras.preprocessing import sequence\nfrom sklearn.preprocessing import LabelEncoder\n\n# get max length of train corpus and initialize label encoder\nle = LabelEncoder()\nnum_classes=2 # positive -> 1, negative -> 0\nmax_len = np.max([len(review) for review in tokenized_train])\n\n## Train reviews data corpus\n# Convert tokenized text reviews to numeric vectors\ntrain_X = [[vocab_map[token] for token in tokenized_review] for tokenized_review in tokenized_train]\ntrain_X = sequence.pad_sequences(train_X, maxlen=max_len) # pad \n## Train prediction class labels\n# Convert text sentiment labels (negative\\positive) to binary encodings (0/1)\ntrain_y = le.fit_transform(train_sentiments)\n\n## Test reviews data corpus\n# Convert tokenized text reviews to numeric vectors\ntest_X = [[vocab_map[token] if vocab_map.get(token) else vocab_map[\'NOT_FOUND_INDEX\'] \n           for token in tokenized_review] \n              for tokenized_review in tokenized_test]\ntest_X = sequence.pad_sequences(test_X, maxlen=max_len)\n## Test prediction class labels\n# Convert text sentiment labels (negative\\positive) to binary encodings (0/1)\ntest_y = le.transform(test_sentiments)\n\n# view vector shapes\nprint(\'Max length of train review vectors:\', max_len)\nprint(\'Train review vectors shape:\', train_X.shape, \' Test review vectors shape:\', test_X.shape)\n\n\n# # Build the LSTM Model Architecture\n\n# In[ ]:\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, Dropout, SpatialDropout1D\nfrom keras.layers import LSTM\n\nEMBEDDING_DIM = 128 # dimension for dense embeddings for each token\nLSTM_DIM = 64 # total LSTM units\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_len))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(LSTM_DIM, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation=""sigmoid""))\n\nmodel.compile(loss=""binary_crossentropy"", optimizer=""adam"",\n              metrics=[""accuracy""])\n\n\n# In[98]:\n\nprint(model.summary())\n\n\n# # Visualize model architecture\n\n# In[97]:\n\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\nSVG(model_to_dot(model, show_shapes=True, show_layer_names=False, \n                 rankdir=\'LR\').create(prog=\'dot\', format=\'svg\'))\n\n\n# # Train the model\n\n# In[74]:\n\nbatch_size = 100\nmodel.fit(train_X, train_y, epochs=5, batch_size=batch_size, \n          shuffle=True, validation_split=0.1, verbose=1)\n\n\n# # Predict and Evaluate Model Performance\n\n# In[75]:\n\npred_test = model.predict_classes(test_X)\npredictions = le.inverse_transform(pred_test.flatten())\n\n\n# In[81]:\n\nmeu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predictions, \n                                      classes=[\'positive\', \'negative\'])  \n\n'"
notebooks/Ch07_Analyzing_Movie_Reviews_Sentiment/sentiment_causal_model_interpretation.py,0,"b'\n# coding: utf-8\n\n# # Import necessary dependencies\n\n# In[1]:\n\nimport pandas as pd\nimport numpy as np\nimport text_normalizer as tn\n\n\n# # Load and normalize data\n\n# In[2]:\n\ndataset = pd.read_csv(r\'movie_reviews.csv\')\n\n# take a peek at the data\nprint(dataset.head())\nreviews = np.array(dataset[\'review\'])\nsentiments = np.array(dataset[\'sentiment\'])\n\n# build train and test datasets\ntrain_reviews = reviews[:35000]\ntrain_sentiments = sentiments[:35000]\ntest_reviews = reviews[35000:]\ntest_sentiments = sentiments[35000:]\n\n# normalize datasets\nnorm_train_reviews = tn.normalize_corpus(train_reviews)\nnorm_test_reviews = tn.normalize_corpus(test_reviews)\n\n\n# # Build Text Classification Pipeline with The Best Model\n\n# In[3]:\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nimport warnings\nwarnings.filterwarnings(""ignore"")\n\n# build BOW features on train reviews\ncv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0, ngram_range=(1,2))\ncv_train_features = cv.fit_transform(norm_train_reviews)\n# build Logistic Regression model\nlr = LogisticRegression()\nlr.fit(cv_train_features, train_sentiments)\n\n# Build Text Classification Pipeline\nlr_pipeline = make_pipeline(cv, lr)\n\n# save the list of prediction classes (positive, negative)\nclasses = list(lr_pipeline.classes_)\n\n\n# # Analyze Model Prediction Probabilities\n\n# In[4]:\n\nlr_pipeline.predict([\'the lord of the rings is an excellent movie\', \n                     \'i hated the recent movie on tv, it was so bad\'])\n\n\n# In[5]:\n\npd.DataFrame(lr_pipeline.predict_proba([\'the lord of the rings is an excellent movie\', \n                     \'i hated the recent movie on tv, it was so bad\']), columns=classes)\n\n\n# # Interpreting Model Decisions\n\n# In[6]:\n\nfrom skater.core.local_interpretation.lime.lime_text import LimeTextExplainer\n\nexplainer = LimeTextExplainer(class_names=classes)\ndef interpret_classification_model_prediction(doc_index, norm_corpus, corpus, \n                                              prediction_labels, explainer_obj):\n    # display model prediction and actual sentiments\n    print(""Test document index: {index}\\nActual sentiment: {actual}\\nPredicted sentiment: {predicted}""\n      .format(index=doc_index, actual=prediction_labels[doc_index],\n              predicted=lr_pipeline.predict([norm_corpus[doc_index]])))\n    # display actual review content\n    print(""\\nReview:"", corpus[doc_index])\n    # display prediction probabilities\n    print(""\\nModel Prediction Probabilities:"")\n    for probs in zip(classes, lr_pipeline.predict_proba([norm_corpus[doc_index]])[0]):\n        print(probs)\n    # display model prediction interpretation\n    exp = explainer.explain_instance(norm_corpus[doc_index], \n                                     lr_pipeline.predict_proba, num_features=10, \n                                     labels=[1])\n    exp.show_in_notebook()\n\n\n# In[7]:\n\ndoc_index = 100 \ninterpret_classification_model_prediction(doc_index=doc_index, norm_corpus=norm_test_reviews,\n                                         corpus=test_reviews, prediction_labels=test_sentiments,\n                                         explainer_obj=explainer)\n\n\n# In[8]:\n\ndoc_index = 2000\ninterpret_classification_model_prediction(doc_index=doc_index, norm_corpus=norm_test_reviews,\n                                         corpus=test_reviews, prediction_labels=test_sentiments,\n                                         explainer_obj=explainer)\n\n\n# In[9]:\n\ndoc_index = 347 \ninterpret_classification_model_prediction(doc_index=doc_index, norm_corpus=norm_test_reviews,\n                                         corpus=test_reviews, prediction_labels=test_sentiments,\n                                         explainer_obj=explainer)\n\n'"
notebooks/Ch07_Analyzing_Movie_Reviews_Sentiment/sentiment_causal_topic_models.py,0,"b'\n# coding: utf-8\n\n# # Import necessary dependencies\n\n# In[9]:\n\nimport pandas as pd\nimport numpy as np\nimport text_normalizer as tn\nimport warnings\n\nwarnings.filterwarnings(""ignore"")\n\n\n# # Load and normalize data\n\n# In[2]:\n\ndataset = pd.read_csv(r\'movie_reviews.csv\')\n\n# take a peek at the data\nprint(dataset.head())\nreviews = np.array(dataset[\'review\'])\nsentiments = np.array(dataset[\'sentiment\'])\n\n# build train and test datasets\ntrain_reviews = reviews[:35000]\ntrain_sentiments = sentiments[:35000]\ntest_reviews = reviews[35000:]\ntest_sentiments = sentiments[35000:]\n\n# normalize datasets\nnorm_train_reviews = tn.normalize_corpus(train_reviews)\nnorm_test_reviews = tn.normalize_corpus(test_reviews)\n\n\n# # Extract features from positive and negative reviews\n\n# In[3]:\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# consolidate all normalized reviews\nnorm_reviews = norm_train_reviews+norm_test_reviews\n# get tf-idf features for only positive reviews\npositive_reviews = [review for review, sentiment in zip(norm_reviews, sentiments) if sentiment == \'positive\']\nptvf = TfidfVectorizer(use_idf=True, min_df=0.05, max_df=0.95, ngram_range=(1,1), sublinear_tf=True)\nptvf_features = ptvf.fit_transform(positive_reviews)\n# get tf-idf features for only negative reviews\nnegative_reviews = [review for review, sentiment in zip(norm_reviews, sentiments) if sentiment == \'negative\']\nntvf = TfidfVectorizer(use_idf=True, min_df=0.05, max_df=0.95, ngram_range=(1,1), sublinear_tf=True)\nntvf_features = ntvf.fit_transform(negative_reviews)\n# view feature set dimensions\nprint(ptvf_features.shape, ntvf_features.shape)\n\n\n# # Topic Modeling on Reviews\n\n# In[4]:\n\nimport pyLDAvis\nimport pyLDAvis.sklearn\nfrom sklearn.decomposition import NMF\nimport topic_model_utils as tmu\n\npyLDAvis.enable_notebook()\ntotal_topics = 10\n\n\n# ## Display and visualize topics for positive reviews\n\n# In[5]:\n\n# build topic model on positive sentiment review features\npos_nmf = NMF(n_components=total_topics, \n          random_state=42, alpha=0.1, l1_ratio=0.2)\npos_nmf.fit(ptvf_features)      \n# extract features and component weights\npos_feature_names = ptvf.get_feature_names()\npos_weights = pos_nmf.components_\n# extract and display topics and their components\npos_topics = tmu.get_topics_terms_weights(pos_weights, pos_feature_names)\ntmu.print_topics_udf(topics=pos_topics,\n                 total_topics=total_topics,\n                 num_terms=15,\n                 display_weights=False)\n\n\n# In[10]:\n\npyLDAvis.sklearn.prepare(pos_nmf, ptvf_features, ptvf, R=15)\n\n\n# ## Display and visualize topics for negative reviews\n\n# In[7]:\n\n# build topic model on negative sentiment review features\nneg_nmf = NMF(n_components=10, \n          random_state=42, alpha=0.1, l1_ratio=0.2)\nneg_nmf.fit(ntvf_features)      \n# extract features and component weights\nneg_feature_names = ntvf.get_feature_names()\nneg_weights = neg_nmf.components_\n# extract and display topics and their components\nneg_topics = tmu.get_topics_terms_weights(neg_weights, neg_feature_names)\ntmu.print_topics_udf(topics=neg_topics,\n                 total_topics=total_topics,\n                 num_terms=15,\n                 display_weights=False) \n\n\n# In[11]:\n\npyLDAvis.sklearn.prepare(neg_nmf, ntvf_features, ntvf, R=15)\n\n'"
notebooks/Ch07_Analyzing_Movie_Reviews_Sentiment/supervised_sentiment_analysis.py,0,"b'\n# coding: utf-8\n\n# # Import necessary depencencies\n\n# In[1]:\n\nimport pandas as pd\nimport numpy as np\nimport text_normalizer as tn\nimport model_evaluation_utils as meu\n\nnp.set_printoptions(precision=2, linewidth=80)\n\n\n# # Load and normalize data\n\n# In[2]:\n\ndataset = pd.read_csv(r\'movie_reviews.csv\')\n\n# take a peek at the data\nprint(dataset.head())\nreviews = np.array(dataset[\'review\'])\nsentiments = np.array(dataset[\'sentiment\'])\n\n# build train and test datasets\ntrain_reviews = reviews[:35000]\ntrain_sentiments = sentiments[:35000]\ntest_reviews = reviews[35000:]\ntest_sentiments = sentiments[35000:]\n\n# normalize datasets\nnorm_train_reviews = tn.normalize_corpus(train_reviews)\nnorm_test_reviews = tn.normalize_corpus(test_reviews)\n\n\n# # Traditional Supervised Machine Learning Models\n\n# ## Feature Engineering\n\n# In[3]:\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n# build BOW features on train reviews\ncv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0, ngram_range=(1,2))\ncv_train_features = cv.fit_transform(norm_train_reviews)\n# build TFIDF features on train reviews\ntv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0, ngram_range=(1,2),\n                     sublinear_tf=True)\ntv_train_features = tv.fit_transform(norm_train_reviews)\n\n\n# In[4]:\n\n# transform test reviews into features\ncv_test_features = cv.transform(norm_test_reviews)\ntv_test_features = tv.transform(norm_test_reviews)\n\n\n# In[5]:\n\nprint(\'BOW model:> Train features shape:\', cv_train_features.shape, \' Test features shape:\', cv_test_features.shape)\nprint(\'TFIDF model:> Train features shape:\', tv_train_features.shape, \' Test features shape:\', tv_test_features.shape)\n\n\n# ## Model Training, Prediction and Performance Evaluation\n\n# In[6]:\n\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\n\nlr = LogisticRegression(penalty=\'l2\', max_iter=100, C=1)\nsvm = SGDClassifier(loss=\'hinge\', n_iter=100)\n\n\n# In[7]:\n\n# Logistic Regression model on BOW features\nlr_bow_predictions = meu.train_predict_model(classifier=lr, \n                                             train_features=cv_train_features, train_labels=train_sentiments,\n                                             test_features=cv_test_features, test_labels=test_sentiments)\nmeu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_bow_predictions,\n                                      classes=[\'positive\', \'negative\'])\n\n\n# In[8]:\n\n# Logistic Regression model on TF-IDF features\nlr_tfidf_predictions = meu.train_predict_model(classifier=lr, \n                                               train_features=tv_train_features, train_labels=train_sentiments,\n                                               test_features=tv_test_features, test_labels=test_sentiments)\nmeu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_tfidf_predictions,\n                                      classes=[\'positive\', \'negative\'])\n\n\n# In[9]:\n\nsvm_bow_predictions = meu.train_predict_model(classifier=svm, \n                                             train_features=cv_train_features, train_labels=train_sentiments,\n                                             test_features=cv_test_features, test_labels=test_sentiments)\nmeu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_bow_predictions,\n                                      classes=[\'positive\', \'negative\'])\n\n\n# In[10]:\n\nsvm_tfidf_predictions = meu.train_predict_model(classifier=svm, \n                                                train_features=tv_train_features, train_labels=train_sentiments,\n                                                test_features=tv_test_features, test_labels=test_sentiments)\nmeu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=svm_tfidf_predictions,\n                                      classes=[\'positive\', \'negative\'])\n\n\n# # Newer Supervised Deep Learning Models\n\n# In[11]:\n\nimport gensim\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dropout, Activation, Dense\nfrom sklearn.preprocessing import LabelEncoder\n\n\n# ## Prediction class label encoding\n\n# In[12]:\n\nle = LabelEncoder()\nnum_classes=2 \n# tokenize train reviews & encode train labels\ntokenized_train = [tn.tokenizer.tokenize(text)\n                   for text in norm_train_reviews]\ny_tr = le.fit_transform(train_sentiments)\ny_train = keras.utils.to_categorical(y_tr, num_classes)\n# tokenize test reviews & encode test labels\ntokenized_test = [tn.tokenizer.tokenize(text)\n                   for text in norm_test_reviews]\ny_ts = le.fit_transform(test_sentiments)\ny_test = keras.utils.to_categorical(y_ts, num_classes)\n\n\n# In[13]:\n\n# print class label encoding map and encoded labels\nprint(\'Sentiment class label map:\', dict(zip(le.classes_, le.transform(le.classes_))))\nprint(\'Sample test label transformation:\\n\'+\'-\'*35,\n      \'\\nActual Labels:\', test_sentiments[:3], \'\\nEncoded Labels:\', y_ts[:3], \n      \'\\nOne hot encoded Labels:\\n\', y_test[:3])\n\n\n# ## Feature Engineering with word embeddings\n\n# In[14]:\n\n# build word2vec model\nw2v_num_features = 500\nw2v_model = gensim.models.Word2Vec(tokenized_train, size=w2v_num_features, window=150,\n                                   min_count=10, sample=1e-3)    \n\n\n# In[15]:\n\ndef averaged_word2vec_vectorizer(corpus, model, num_features):\n    vocabulary = set(model.wv.index2word)\n    \n    def average_word_vectors(words, model, vocabulary, num_features):\n        feature_vector = np.zeros((num_features,), dtype=""float64"")\n        nwords = 0.\n        \n        for word in words:\n            if word in vocabulary: \n                nwords = nwords + 1.\n                feature_vector = np.add(feature_vector, model[word])\n        if nwords:\n            feature_vector = np.divide(feature_vector, nwords)\n\n        return feature_vector\n\n    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n                    for tokenized_sentence in corpus]\n    return np.array(features)\n\n\n# In[16]:\n\n# generate averaged word vector features from word2vec model\navg_wv_train_features = averaged_word2vec_vectorizer(corpus=tokenized_train, model=w2v_model,\n                                                     num_features=500)\navg_wv_test_features = averaged_word2vec_vectorizer(corpus=tokenized_test, model=w2v_model,\n                                                    num_features=500)\n\n\n# In[17]:\n\n# feature engineering with GloVe model\ntrain_nlp = [tn.nlp(item) for item in norm_train_reviews]\ntrain_glove_features = np.array([item.vector for item in train_nlp])\n\ntest_nlp = [tn.nlp(item) for item in norm_test_reviews]\ntest_glove_features = np.array([item.vector for item in test_nlp])\n\n\n# In[18]:\n\nprint(\'Word2Vec model:> Train features shape:\', avg_wv_train_features.shape, \' Test features shape:\', avg_wv_test_features.shape)\nprint(\'GloVe model:> Train features shape:\', train_glove_features.shape, \' Test features shape:\', test_glove_features.shape)\n\n\n# ## Modeling with deep neural networks \n\n# ### Building Deep neural network architecture\n\n# In[19]:\n\ndef construct_deepnn_architecture(num_input_features):\n    dnn_model = Sequential()\n    dnn_model.add(Dense(512, activation=\'relu\', input_shape=(num_input_features,)))\n    dnn_model.add(Dropout(0.2))\n    dnn_model.add(Dense(512, activation=\'relu\'))\n    dnn_model.add(Dropout(0.2))\n    dnn_model.add(Dense(512, activation=\'relu\'))\n    dnn_model.add(Dropout(0.2))\n    dnn_model.add(Dense(2))\n    dnn_model.add(Activation(\'softmax\'))\n\n    dnn_model.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\',                 \n                      metrics=[\'accuracy\'])\n    return dnn_model\n\n\n# In[68]:\n\nw2v_dnn = construct_deepnn_architecture(num_input_features=500)\n\n\n# ### Visualize sample deep architecture\n\n# In[21]:\n\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\nSVG(model_to_dot(w2v_dnn, show_shapes=True, show_layer_names=False, \n                 rankdir=\'TB\').create(prog=\'dot\', format=\'svg\'))\n\n\n# ### Model Training, Prediction and Performance Evaluation\n\n# In[69]:\n\nbatch_size = 100\nw2v_dnn.fit(avg_wv_train_features, y_train, epochs=5, batch_size=batch_size, \n            shuffle=True, validation_split=0.1, verbose=1)\n\n\n# In[70]:\n\ny_pred = w2v_dnn.predict_classes(avg_wv_test_features)\npredictions = le.inverse_transform(y_pred) \n\n\n# In[71]:\n\nmeu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predictions, \n                                      classes=[\'positive\', \'negative\'])  \n\n\n# In[25]:\n\nglove_dnn = construct_deepnn_architecture(num_input_features=300)\n\n\n# In[26]:\n\nbatch_size = 100\nglove_dnn.fit(train_glove_features, y_train, epochs=5, batch_size=batch_size, \n              shuffle=True, validation_split=0.1, verbose=1)\n\n\n# In[27]:\n\ny_pred = glove_dnn.predict_classes(test_glove_features)\npredictions = le.inverse_transform(y_pred) \n\n\n# In[28]:\n\nmeu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predictions, \n                                      classes=[\'positive\', \'negative\'])  \n\n'"
notebooks/Ch07_Analyzing_Movie_Reviews_Sentiment/text_normalizer.py,0,"b'\n# coding: utf-8\n""""""\nCreated on Sun Jul 30 12:32:59 2017\n\n@author: DIP\n@Copyright: Dipanjan Sarkar\n""""""\n\n# # Import necessary dependencies\nimport spacy\nimport nltk\nfrom nltk.tokenize.toktok import ToktokTokenizer\nimport re\nfrom bs4 import BeautifulSoup\nfrom contractions import CONTRACTION_MAP\nimport unicodedata\n\nnlp = spacy.load(\'en\', parse = False, tag=False, entity=False)\ntokenizer = ToktokTokenizer()\nstopword_list = nltk.corpus.stopwords.words(\'english\')\nstopword_list.remove(\'no\')\nstopword_list.remove(\'not\')\n\n\n# # Cleaning Text - strip HTML\ndef strip_html_tags(text):\n    soup = BeautifulSoup(text, ""html.parser"")\n    stripped_text = soup.get_text()\n    return stripped_text\n\n\n# # Removing accented characters\ndef remove_accented_chars(text):\n    text = unicodedata.normalize(\'NFKD\', text).encode(\'ascii\', \'ignore\').decode(\'utf-8\', \'ignore\')\n    return text\n\n\n# # Expanding Contractions\ndef expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n    \n    contractions_pattern = re.compile(\'({})\'.format(\'|\'.join(contraction_mapping.keys())), \n                                      flags=re.IGNORECASE|re.DOTALL)\n    def expand_match(contraction):\n        match = contraction.group(0)\n        first_char = match[0]\n        expanded_contraction = contraction_mapping.get(match) \\\n                                   if contraction_mapping.get(match) \\\n                                    else contraction_mapping.get(match.lower())                       \n        expanded_contraction = first_char+expanded_contraction[1:]\n        return expanded_contraction\n        \n    expanded_text = contractions_pattern.sub(expand_match, text)\n    expanded_text = re.sub(""\'"", """", expanded_text)\n    return expanded_text\n\n\n# # Removing Special Characters\ndef remove_special_characters(text):\n    text = re.sub(\'[^a-zA-Z0-9\\s]\', \'\', text)\n    return text\n\n\n# # Lemmatizing text\ndef lemmatize_text(text):\n    text = nlp(text)\n    text = \' \'.join([word.lemma_ if word.lemma_ != \'-PRON-\' else word.text for word in text])\n    return text\n\n\n# # Removing Stopwords\ndef remove_stopwords(text, is_lower_case=False):\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    if is_lower_case:\n        filtered_tokens = [token for token in tokens if token not in stopword_list]\n    else:\n        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = \' \'.join(filtered_tokens)    \n    return filtered_text\n\n\n# # Normalize text corpus - tying it all together\ndef normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n                     accented_char_removal=True, text_lower_case=True, \n                     text_lemmatization=True, special_char_removal=True, \n                     stopword_removal=True):\n    \n    normalized_corpus = []\n    \n    for doc in corpus:\n        \n        if html_stripping:\n            doc = strip_html_tags(doc)\n        \n        if accented_char_removal:\n            doc = remove_accented_chars(doc)\n            \n        if contraction_expansion:\n            doc = expand_contractions(doc)\n            \n        if text_lower_case:\n            doc = doc.lower()\n            \n        # remove extra newlines\n        doc = re.sub(r\'[\\r|\\n|\\r\\n]+\', \' \',doc)\n        # insert spaces between special characters to isolate them    \n        special_char_pattern = re.compile(r\'([{.(-)!}])\')\n        doc = special_char_pattern.sub("" \\\\1 "", doc)\n        \n        if text_lemmatization:\n            doc = lemmatize_text(doc)\n            \n        if special_char_removal:\n            doc = remove_special_characters(doc)  \n            \n        # remove extra whitespace\n        doc = re.sub(\' +\', \' \', doc)\n        \n        if stopword_removal:\n            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n            \n        normalized_corpus.append(doc)\n        \n    return normalized_corpus\n'"
notebooks/Ch07_Analyzing_Movie_Reviews_Sentiment/topic_model_utils.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Tue Aug  8 20:54:15 2017\n\n@author: DIP\n@Copyright: Dipanjan Sarkar\n""""""\n\nimport numpy as np\n\n\n# prints components of all the topics \n# obtained from topic modeling\ndef print_topics_udf(topics, total_topics=1,\n                     weight_threshold=0.0001,\n                     display_weights=False,\n                     num_terms=None):\n    \n    for index in range(total_topics):\n        topic = topics[index]\n        topic = [(term, float(wt))\n                 for term, wt in topic]\n        topic = [(word, round(wt,2)) \n                 for word, wt in topic \n                 if abs(wt) >= weight_threshold]\n                     \n        if display_weights:\n            print(\'Topic #\'+str(index+1)+\' with weights\')\n            print(topic[:num_terms]) if num_terms else topic\n        else:\n            print(\'Topic #\'+str(index+1)+\' without weights\')\n            tw = [term for term, wt in topic]\n            print(tw[:num_terms]) if num_terms else tw\n        print()\n        \n\n# extracts topics with their terms and weights\n# format is Topic N: [(term1, weight1), ..., (termn, weightn)]        \ndef get_topics_terms_weights(weights, feature_names):\n    feature_names = np.array(feature_names)\n    sorted_indices = np.array([list(row[::-1]) \n                           for row \n                           in np.argsort(np.abs(weights))])\n    sorted_weights = np.array([list(wt[index]) \n                               for wt, index \n                               in zip(weights,sorted_indices)])\n    sorted_terms = np.array([list(feature_names[row]) \n                             for row \n                             in sorted_indices])\n    \n    topics = [np.vstack((terms.T, \n                     term_weights.T)).T \n              for terms, term_weights \n              in zip(sorted_terms, sorted_weights)]     \n    \n    return topics         '"
notebooks/Ch07_Analyzing_Movie_Reviews_Sentiment/unsupervised_sentiment_analysis.py,0,"b""\n# coding: utf-8\n\n# # Import necessary depencencies\n\n# In[1]:\n\nimport pandas as pd\nimport numpy as np\nimport text_normalizer as tn\nimport model_evaluation_utils as meu\n\nnp.set_printoptions(precision=2, linewidth=80)\n\n\n# # Load and normalize data\n\n# In[2]:\n\ndataset = pd.read_csv(r'movie_reviews.csv')\n\nreviews = np.array(dataset['review'])\nsentiments = np.array(dataset['sentiment'])\n\n# extract data for model evaluation\ntest_reviews = reviews[35000:]\ntest_sentiments = sentiments[35000:]\nsample_review_ids = [7626, 3533, 13010]\n\n# normalize dataset\nnorm_test_reviews = tn.normalize_corpus(test_reviews)\n\n\n# # Sentiment Analysis with AFINN\n\n# In[3]:\n\nfrom afinn import Afinn\n\nafn = Afinn(emoticons=True) \n\n\n# ## Predict sentiment for sample reviews\n\n# In[4]:\n\nfor review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n    print('REVIEW:', review)\n    print('Actual Sentiment:', sentiment)\n    print('Predicted Sentiment polarity:', afn.score(review))\n    print('-'*60)\n\n\n# ## Predict sentiment for test dataset\n\n# In[5]:\n\nsentiment_polarity = [afn.score(review) for review in test_reviews]\npredicted_sentiments = ['positive' if score >= 1.0 else 'negative' for score in sentiment_polarity]\n\n\n# ## Evaluate model performance\n\n# In[6]:\n\nmeu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, \n                                  classes=['positive', 'negative'])\n\n\n# # Sentiment Analysis with SentiWordNet\n\n# In[7]:\n\nfrom nltk.corpus import sentiwordnet as swn\n\nawesome = list(swn.senti_synsets('awesome', 'a'))[0]\nprint('Positive Polarity Score:', awesome.pos_score())\nprint('Negative Polarity Score:', awesome.neg_score())\nprint('Objective Score:', awesome.obj_score())\n\n\n# ## Build model\n\n# In[8]:\n\ndef analyze_sentiment_sentiwordnet_lexicon(review,\n                                           verbose=False):\n\n    # tokenize and POS tag text tokens\n    tagged_text = [(token.text, token.tag_) for token in tn.nlp(review)]\n    pos_score = neg_score = token_count = obj_score = 0\n    # get wordnet synsets based on POS tags\n    # get sentiment scores if synsets are found\n    for word, tag in tagged_text:\n        ss_set = None\n        if 'NN' in tag and list(swn.senti_synsets(word, 'n')):\n            ss_set = list(swn.senti_synsets(word, 'n'))[0]\n        elif 'VB' in tag and list(swn.senti_synsets(word, 'v')):\n            ss_set = list(swn.senti_synsets(word, 'v'))[0]\n        elif 'JJ' in tag and list(swn.senti_synsets(word, 'a')):\n            ss_set = list(swn.senti_synsets(word, 'a'))[0]\n        elif 'RB' in tag and list(swn.senti_synsets(word, 'r')):\n            ss_set = list(swn.senti_synsets(word, 'r'))[0]\n        # if senti-synset is found        \n        if ss_set:\n            # add scores for all found synsets\n            pos_score += ss_set.pos_score()\n            neg_score += ss_set.neg_score()\n            obj_score += ss_set.obj_score()\n            token_count += 1\n    \n    # aggregate final scores\n    final_score = pos_score - neg_score\n    norm_final_score = round(float(final_score) / token_count, 2)\n    final_sentiment = 'positive' if norm_final_score >= 0 else 'negative'\n    if verbose:\n        norm_obj_score = round(float(obj_score) / token_count, 2)\n        norm_pos_score = round(float(pos_score) / token_count, 2)\n        norm_neg_score = round(float(neg_score) / token_count, 2)\n        # to display results in a nice table\n        sentiment_frame = pd.DataFrame([[final_sentiment, norm_obj_score, norm_pos_score, \n                                         norm_neg_score, norm_final_score]],\n                                       columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n                                                             ['Predicted Sentiment', 'Objectivity',\n                                                              'Positive', 'Negative', 'Overall']], \n                                                             labels=[[0,0,0,0,0],[0,1,2,3,4]]))\n        print(sentiment_frame)\n        \n    return final_sentiment\n\n\n# ## Predict sentiment for sample reviews\n\n# In[9]:\n\nfor review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n    print('REVIEW:', review)\n    print('Actual Sentiment:', sentiment)\n    pred = analyze_sentiment_sentiwordnet_lexicon(review, verbose=True)    \n    print('-'*60)\n\n\n# ## Predict sentiment for test dataset\n\n# In[10]:\n\npredicted_sentiments = [analyze_sentiment_sentiwordnet_lexicon(review, verbose=False) for review in norm_test_reviews]\n\n\n# ## Evaluate model performance\n\n# In[11]:\n\nmeu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, \n                                  classes=['positive', 'negative'])\n\n\n# # Sentiment Analysis with VADER\n\n# In[12]:\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n\n# ## Build model\n\n# In[13]:\n\ndef analyze_sentiment_vader_lexicon(review, \n                                    threshold=0.1,\n                                    verbose=False):\n    # pre-process text\n    review = tn.strip_html_tags(review)\n    review = tn.remove_accented_chars(review)\n    review = tn.expand_contractions(review)\n    \n    # analyze the sentiment for review\n    analyzer = SentimentIntensityAnalyzer()\n    scores = analyzer.polarity_scores(review)\n    # get aggregate scores and final sentiment\n    agg_score = scores['compound']\n    final_sentiment = 'positive' if agg_score >= threshold                                   else 'negative'\n    if verbose:\n        # display detailed sentiment statistics\n        positive = str(round(scores['pos'], 2)*100)+'%'\n        final = round(agg_score, 2)\n        negative = str(round(scores['neg'], 2)*100)+'%'\n        neutral = str(round(scores['neu'], 2)*100)+'%'\n        sentiment_frame = pd.DataFrame([[final_sentiment, final, positive,\n                                        negative, neutral]],\n                                        columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n                                                                      ['Predicted Sentiment', 'Polarity Score',\n                                                                       'Positive', 'Negative', 'Neutral']], \n                                                              labels=[[0,0,0,0,0],[0,1,2,3,4]]))\n        print(sentiment_frame)\n    \n    return final_sentiment\n\n\n# ## Predict sentiment for sample reviews\n\n# In[14]:\n\nfor review, sentiment in zip(test_reviews[sample_review_ids], test_sentiments[sample_review_ids]):\n    print('REVIEW:', review)\n    print('Actual Sentiment:', sentiment)\n    pred = analyze_sentiment_vader_lexicon(review, threshold=0.4, verbose=True)    \n    print('-'*60)\n\n\n# ## Predict sentiment for test dataset\n\n# In[15]:\n\npredicted_sentiments = [analyze_sentiment_vader_lexicon(review, threshold=0.4, verbose=False) for review in test_reviews]\n\n\n# ## Evaluate model performance\n\n# In[16]:\n\nmeu.display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=predicted_sentiments, \n                                  classes=['positive', 'negative'])\n\n"""
notebooks/Ch08_Customer_Segmentation_and_Effective_Cross_Selling/cross_selling.py,0,"b'\n# coding: utf-8\n\n# # Load Necessary Dependencies\n\n# In[1]:\n\n\nimport csv\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport Orange\nfrom Orange.data import Domain, DiscreteVariable, ContinuousVariable\nfrom orangecontrib.associate.fpgrowth import *\n\nget_ipython().magic(\'matplotlib inline\')\n\n\n# # Construct and Load the Groceries Dataset\n\n# In[6]:\n\n\ngrocery_items = set()\nwith open(""grocery_dataset.txt"") as f:\n    reader = csv.reader(f, delimiter="","")\n    for i, line in enumerate(reader):\n        grocery_items.update(line)\noutput_list = list()\nwith open(""grocery_dataset.txt"") as f:\n    reader = csv.reader(f, delimiter="","")\n    for i, line in enumerate(reader):\n        row_val = {item:0 for item in grocery_items}\n        row_val.update({item:1 for item in line})\n        output_list.append(row_val)\ngrocery_df = pd.DataFrame(output_list)\n\n\n# In[7]:\n\n\ngrocery_df.head()\n\n\n# # View top sold items\n\n# In[8]:\n\n\ntotal_item_count = sum(grocery_df.sum())\nprint(total_item_count)\nitem_summary_df = grocery_df.sum().sort_values(ascending = False).reset_index().head(n=20)\nitem_summary_df.rename(columns={item_summary_df.columns[0]:\'item_name\',item_summary_df.columns[1]:\'item_count\'}, inplace=True)\nitem_summary_df.head()\n\n\n# # Visualize top sold items\n\n# In[9]:\n\n\nobjects = (list(item_summary_df[\'item_name\'].head(n=20)))\ny_pos = np.arange(len(objects))\nperformance = list(item_summary_df[\'item_count\'].head(n=20))\n \nplt.bar(y_pos, performance, align=\'center\', alpha=0.5)\nplt.xticks(y_pos, objects, rotation=\'vertical\')\nplt.ylabel(\'Item count\')\nplt.title(\'Item sales distribution\')\n\n\n# # Analyze items contributing to top sales\n\n# In[11]:\n\n\nitem_summary_df[\'item_perc\'] = item_summary_df[\'item_count\']/total_item_count\nitem_summary_df[\'total_perc\'] = item_summary_df.item_perc.cumsum()\nitem_summary_df.head(10)\n\n\n# # Analyze items contributing to top 50% of sales\n\n# In[14]:\n\n\nitem_summary_df[item_summary_df.total_perc <= 0.5].shape\n\n\n# In[13]:\n\n\nitem_summary_df[item_summary_df.total_perc <= 0.5]\n\n\n# # Construct Orange Table \n\n# In[16]:\n\n\ninput_assoc_rules = grocery_df\ndomain_grocery = Domain([DiscreteVariable.make(name=item,values=[\'0\', \'1\']) for item in input_assoc_rules.columns])\ndata_gro_1 = Orange.data.Table.from_numpy(domain=domain_grocery,  X=input_assoc_rules.as_matrix(),Y= None)\n\n\n# # Prune Dataset for frequently purchased items\n\n# In[2]:\n\n\ndef prune_dataset(input_df, length_trans = 2, total_sales_perc = 0.5, start_item = None, end_item = None):\n    if \'total_items\' in input_df.columns:\n        del(input_df[\'total_items\'])\n    item_count = input_df.sum().sort_values(ascending = False).reset_index()\n    total_items = sum(input_df.sum().sort_values(ascending = False))\n    item_count.rename(columns={item_count.columns[0]:\'item_name\',item_count.columns[1]:\'item_count\'}, inplace=True)\n    if not start_item and not end_item: \n        item_count[\'item_perc\'] = item_count[\'item_count\']/total_items\n        item_count[\'total_perc\'] = item_count.item_perc.cumsum()\n        selected_items = list(item_count[item_count.total_perc < total_sales_perc].item_name)\n        input_df[\'total_items\'] = input_df[selected_items].sum(axis = 1)\n        input_df = input_df[input_df.total_items >= length_trans]\n        del(input_df[\'total_items\'])\n        return input_df[selected_items], item_count[item_count.total_perc < total_sales_perc]\n    elif end_item > start_item:\n        selected_items = list(item_count[start_item:end_item].item_name)\n        input_df[\'total_items\'] = input_df[selected_items].sum(axis = 1)\n        input_df = input_df[input_df.total_items >= length_trans]\n        del(input_df[\'total_items\'])\n        return input_df[selected_items],item_count[start_item:end_item]\n\n\n# In[35]:\n\n\noutput_df, item_counts = prune_dataset(input_df=grocery_df, length_trans=2,total_sales_perc=0.4)\nprint(output_df.shape)\nprint(list(output_df.columns))\n\n\n# # Association Rule Mining with FP Growth\n\n# In[36]:\n\n\ninput_assoc_rules = output_df\ndomain_grocery = Domain([DiscreteVariable.make(name=item,values=[\'0\', \'1\']) for item in input_assoc_rules.columns])\ndata_gro_1 = Orange.data.Table.from_numpy(domain=domain_grocery,  X=input_assoc_rules.as_matrix(),Y= None)\ndata_gro_1_en, mapping = OneHot.encode(data_gro_1, include_class=False)\n\n\n# In[37]:\n\n\nmin_support = 0.01\nprint(""num of required transactions = "", int(input_assoc_rules.shape[0]*min_support))\nnum_trans = input_assoc_rules.shape[0]*min_support\nitemsets = dict(frequent_itemsets(data_gro_1_en, min_support=min_support))\n\n\n# In[39]:\n\n\nlen(itemsets)\n\n\n# In[59]:\n\n\nconfidence = 0.3\nrules_df = pd.DataFrame()\n\nif len(itemsets) < 1000000: \n    rules = [(P, Q, supp, conf)\n    for P, Q, supp, conf in association_rules(itemsets, confidence)\n       if len(Q) == 1 ]\n\n    names = {item: \'{}={}\'.format(var.name, val)\n        for item, var, val in OneHot.decode(mapping, data_gro_1, mapping)}\n    \n    eligible_ante = [v for k,v in names.items() if v.endswith(""1"")]\n    \n    N = input_assoc_rules.shape[0]\n    \n    rule_stats = list(rules_stats(rules, itemsets, N))\n    \n    rule_list_df = []\n    for ex_rule_frm_rule_stat in rule_stats:\n        ante = ex_rule_frm_rule_stat[0]            \n        cons = ex_rule_frm_rule_stat[1]\n        named_cons = names[next(iter(cons))]\n        if named_cons in eligible_ante:\n            rule_lhs = [names[i][:-2] for i in ante if names[i] in eligible_ante]\n            ante_rule = \', \'.join(rule_lhs)\n            if ante_rule and len(rule_lhs)>1 :\n                rule_dict = {\'support\' : ex_rule_frm_rule_stat[2],\n                             \'confidence\' : ex_rule_frm_rule_stat[3],\n                             \'coverage\' : ex_rule_frm_rule_stat[4],\n                             \'strength\' : ex_rule_frm_rule_stat[5],\n                             \'lift\' : ex_rule_frm_rule_stat[6],\n                             \'leverage\' : ex_rule_frm_rule_stat[7],\n                             \'antecedent\': ante_rule,\n                             \'consequent\':named_cons[:-2] }\n                rule_list_df.append(rule_dict)\n    rules_df = pd.DataFrame(rule_list_df)\n    print(""Raw rules data frame of {} rules generated"".format(rules_df.shape[0]))\n    if not rules_df.empty:\n        pruned_rules_df = rules_df.groupby([\'antecedent\',\'consequent\']).max().reset_index()\n    else:\n        print(""Unable to generate any rule"")\n\n\n# # Sorting rules in our Grocery Dataset\n\n# In[60]:\n\n\n(pruned_rules_df[[\'antecedent\',\'consequent\',\n                  \'support\',\'confidence\',\'lift\']].groupby(\'consequent\')\n                                                 .max()\n                                                 .reset_index()\n                                                 .sort_values([\'lift\', \'support\',\'confidence\'],\n                                                              ascending=False))\n\n\n# # Association rule mining on our Online Retail dataset\n\n# ## Load and Filter Dataset\n\n# In[3]:\n\n\ncs_mba = pd.read_excel(io=r\'Online Retail.xlsx\')\ncs_mba_uk = cs_mba[cs_mba.Country == \'United Kingdom\']\n\n\n# In[4]:\n\n\ncs_mba_uk.head()\n\n\n# Remove returned item as we are only interested in the buying patterns\n\n# In[5]:\n\n\ncs_mba_uk = cs_mba_uk[~(cs_mba_uk.InvoiceNo.str.contains(""C"") == True)]\ncs_mba_uk = cs_mba_uk[~cs_mba_uk.Quantity<0]\n\n\n# In[6]:\n\n\ncs_mba_uk.shape\n\n\n# In[7]:\n\n\ncs_mba_uk.InvoiceNo.value_counts().shape\n\n\n# ## Build Transaction Dataset\n\n# In[8]:\n\n\nitems = list(cs_mba_uk.Description.unique())\ngrouped = cs_mba_uk.groupby(\'InvoiceNo\')\ntransaction_level_df_uk = grouped.aggregate(lambda x: tuple(x)).reset_index()[[\'InvoiceNo\',\'Description\']]\n\n\n# In[9]:\n\n\ntransaction_dict = {item:0 for item in items}\noutput_dict = dict()\ntemp = dict()\nfor rec in transaction_level_df_uk.to_dict(\'records\'):\n    invoice_num = rec[\'InvoiceNo\']\n    items_list = rec[\'Description\']\n    transaction_dict = {item:0 for item in items}\n    transaction_dict.update({item:1 for item in items if item in items_list})\n    temp.update({invoice_num:transaction_dict})\n\nnew = [v for k,v in temp.items()]\ntranasction_df = pd.DataFrame(new)\ndel(tranasction_df[tranasction_df.columns[0]])\n\n\n# In[10]:\n\n\ntranasction_df.shape\n\n\n# In[11]:\n\n\ntranasction_df.head()\n\n\n# In[12]:\n\n\noutput_df_uk_n, item_counts_n = prune_dataset(input_df=tranasction_df, length_trans=2, start_item=0, end_item=15)\nprint(output_df_uk_n.shape)\n\n\n# In[13]:\n\n\noutput_df_uk_n.head()\n\n\n# ## Association Rule Mining with FP Growth\n\n# In[15]:\n\n\ninput_assoc_rules = output_df_uk_n\ndomain_transac = Domain([DiscreteVariable.make(name=item,values=[\'0\', \'1\']) for item in input_assoc_rules.columns])\ndata_tran_uk = Orange.data.Table.from_numpy(domain=domain_transac,  X=input_assoc_rules.as_matrix(),Y= None)\ndata_tran_uk_en, mapping = OneHot.encode(data_tran_uk, include_class=True)\n\n\n# In[21]:\n\n\nsupport = 0.01\nprint(""num of required transactions = "", int(input_assoc_rules.shape[0]*support))\nnum_trans = input_assoc_rules.shape[0]*support\nitemsets = dict(frequent_itemsets(data_tran_uk_en, support))\n\n\n# In[22]:\n\n\nlen(itemsets)\n\n\n# In[23]:\n\n\nconfidence = 0.3\nrules_df = pd.DataFrame()\nif len(itemsets) < 1000000: \n    rules = [(P, Q, supp, conf)\n    for P, Q, supp, conf in association_rules(itemsets, confidence)\n       if len(Q) == 1 ]\n\n    names = {item: \'{}={}\'.format(var.name, val)\n        for item, var, val in OneHot.decode(mapping, data_tran_uk, mapping)}\n    \n    eligible_ante = [v for k,v in names.items() if v.endswith(""1"")]\n    \n    N = input_assoc_rules.shape[0]\n    \n    rule_stats = list(rules_stats(rules, itemsets, N))\n    \n    rule_list_df = []\n    for ex_rule_frm_rule_stat in rule_stats:\n        ante = ex_rule_frm_rule_stat[0]            \n        cons = ex_rule_frm_rule_stat[1]\n        named_cons = names[next(iter(cons))]\n        if named_cons in eligible_ante:\n            rule_lhs = [names[i][:-2] for i in ante if names[i] in eligible_ante]\n            ante_rule = \', \'.join(rule_lhs)\n            if ante_rule and len(rule_lhs)>1 :\n                rule_dict = {\'support\' : ex_rule_frm_rule_stat[2],\n                             \'confidence\' : ex_rule_frm_rule_stat[3],\n                             \'coverage\' : ex_rule_frm_rule_stat[4],\n                             \'strength\' : ex_rule_frm_rule_stat[5],\n                             \'lift\' : ex_rule_frm_rule_stat[6],\n                             \'leverage\' : ex_rule_frm_rule_stat[7],\n                             \'antecedent\': ante_rule,\n                             \'consequent\':named_cons[:-2] }\n                rule_list_df.append(rule_dict)\n    rules_df = pd.DataFrame(rule_list_df)\n    print(""Raw rules data frame of {} rules generated"".format(rules_df.shape[0]))\n    if not rules_df.empty:\n        pruned_rules_df = rules_df.groupby([\'antecedent\',\'consequent\']).max().reset_index()\n    else:\n        print(""Unable to generate any rule"")\n\n\n# ## Sort and display rules\n\n# In[25]:\n\n\ndw = pd.options.display.max_colwidth\npd.options.display.max_colwidth = 100\n(pruned_rules_df[[\'antecedent\',\'consequent\',\n                  \'support\',\'confidence\',\'lift\']].groupby(\'consequent\')\n                                                 .max()\n                                                 .reset_index()\n                                                 .sort_values([\'lift\', \'support\',\'confidence\'],\n                                                              ascending=False)).head(5)\n\n\n# In[26]:\n\n\npd.options.display.max_colwidth = dw\n\n'"
notebooks/Ch08_Customer_Segmentation_and_Effective_Cross_Selling/customer_segmentation.py,0,"b'\n# coding: utf-8\n\n# # Load Dependencies and Configuration Settings\n\n# In[1]:\n\n\nimport pandas as pd\nimport datetime\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlab\n\nget_ipython().magic(\'matplotlib inline\')\n\n\n# # Load and View the Dataset\n\n# In[2]:\n\n\ncs_df = pd.read_excel(io=r\'Online Retail.xlsx\')\n\n\n# In[3]:\n\n\ncs_df.head()\n\n\n# Transactions size\n\n# In[4]:\n\n\ncs_df.shape\n\n\n# # Top Sales by Country\n\n# In[5]:\n\n\ncs_df.Country.value_counts().reset_index().head(n=10)\n\n\n# # Top Customers contributing to 10% of total Sales\n\n# Number of customers\n\n# In[6]:\n\n\ncs_df.CustomerID.unique().shape\n\n\n# In[7]:\n\n\n(cs_df.CustomerID.value_counts()/sum(cs_df.CustomerID.value_counts())*100).head(n=13).cumsum()\n\n\n# # Analyzing Data Quality Issues\n\n# Number of unique items\n\n# In[8]:\n\n\ncs_df.StockCode.unique().shape\n\n\n# Description of items: We see that the descriptions are more then the stock code so there must be some stock code which have more than one decription\n\n# In[9]:\n\n\ncs_df.Description.unique().shape\n\n\n# In[10]:\n\n\ncs_df.dtypes\n\n\n# In[11]:\n\n\ncat_des_df = cs_df.groupby([""StockCode"",""Description""]).count().reset_index()\n\n\n# Stockcode which have more than one description\n\n# In[12]:\n\n\ncat_des_df.StockCode.value_counts()[cat_des_df.StockCode.value_counts()>1].reset_index().head()\n\n\n# Example of one such stockcode\n\n# In[14]:\n\n\ncs_df[cs_df[\'StockCode\'] == cat_des_df.StockCode.value_counts()[cat_des_df.StockCode.value_counts()>1]\n      .reset_index()[\'index\'][5]][\'Description\'].unique()\n\n\n# In[15]:\n\n\ncs_df[\'invdatetime\'] = pd.to_datetime(cs_df.InvoiceDate)\n\n\n# In[16]:\n\n\ncs_df.Quantity.describe()\n\n\n# In[17]:\n\n\ncs_df.UnitPrice.describe()\n\n\n# # Data Cleaning\n\n# In[18]:\n\n\n# Seperate data for one geography\ncs_df = cs_df[cs_df.Country == \'United Kingdom\']\n\n# Seperate attribute for total amount\ncs_df[\'amount\'] = cs_df.Quantity*cs_df.UnitPrice\n\n# Remove negative or return transactions\ncs_df = cs_df[~(cs_df.amount<0)]\ncs_df.head()\ncs_df = cs_df[~(cs_df.CustomerID.isnull())]\n\n\n# In[19]:\n\n\ncs_df.shape\n\n\n# # Build Recency Feature\n\n# In[20]:\n\n\ncs_df.InvoiceDate.max()\n\n\n# In[21]:\n\n\ncs_df.InvoiceDate.min()\n\n\n# In[22]:\n\n\nrefrence_date = cs_df.InvoiceDate.max()\nrefrence_date = refrence_date + datetime.timedelta(days = 1)\n\n\n# In[23]:\n\n\ncs_df[\'days_since_last_purchase\'] = refrence_date - cs_df.InvoiceDate\ncs_df[\'days_since_last_purchase_num\'] = cs_df[\'days_since_last_purchase\'].astype(\'timedelta64[D]\')\n\n\n# Time period of transactions\n\n# In[24]:\n\n\ncustomer_history_df = cs_df.groupby(""CustomerID"").min().reset_index()[[\'CustomerID\', \'days_since_last_purchase_num\']]\ncustomer_history_df.rename(columns={\'days_since_last_purchase_num\':\'recency\'}, inplace=True)\ncustomer_history_df.recency.describe()\n\n\n# In[25]:\n\n\ncustomer_history_df.head()\n\n\n# In[26]:\n\n\ncustomer_history_df.recency.describe()\n\n\n# In[27]:\n\n\nx = customer_history_df.recency\nmu = np.mean(customer_history_df.recency)\nsigma = math.sqrt(np.var(customer_history_df.recency))\nn, bins, patches = plt.hist(x, 1000, facecolor=\'green\', alpha=0.75)\n# add a \'best fit\' line\ny = mlab.normpdf(bins, mu, sigma)\nl = plt.plot(bins, y, \'r--\', linewidth=2)\nplt.xlabel(\'Recency in days\')\nplt.ylabel(\'Number of transactions\')\nplt.title(r\'$\\mathrm{Histogram\\ of\\ sales\\ recency}\\ $\')\nplt.grid(True)\n\n\n# # Build Frequency & Monetary value Features\n\n# In[28]:\n\n\ncustomer_monetary_val = cs_df[[\'CustomerID\', \'amount\']].groupby(""CustomerID"").sum().reset_index()\ncustomer_history_df = customer_history_df.merge(customer_monetary_val, how=\'outer\')\ncustomer_history_df.amount = customer_history_df.amount+0.001\ncustomer_freq = cs_df[[\'CustomerID\', \'amount\']].groupby(""CustomerID"").count().reset_index()\ncustomer_freq.rename(columns={\'amount\':\'frequency\'},inplace=True)\ncustomer_history_df = customer_history_df.merge(customer_freq, how=\'outer\')\n\n\n# Remove returns so that we only have purchases of a customer\n\n# In[29]:\n\n\ncustomer_history_df.head()\n\n\n# In[30]:\n\n\nfrom sklearn import preprocessing\nimport math\n\ncustomer_history_df[\'recency_log\'] = customer_history_df[\'recency\'].apply(math.log)\ncustomer_history_df[\'frequency_log\'] = customer_history_df[\'frequency\'].apply(math.log)\ncustomer_history_df[\'amount_log\'] = customer_history_df[\'amount\'].apply(math.log)\nfeature_vector = [\'amount_log\', \'recency_log\',\'frequency_log\']\nX_subset = customer_history_df[feature_vector].as_matrix()\nscaler = preprocessing.StandardScaler().fit(X_subset)\nX_scaled = scaler.transform(X_subset)\n\n\n# # Visualizing Recency vs Monetary Value (scaled)\n\n# In[31]:\n\n\nplt.scatter(customer_history_df.recency_log, customer_history_df.amount_log, alpha=0.5)\n\n\n# # Visualizing Monetary Value distribution (scaled)\n\n# In[32]:\n\n\nx = customer_history_df.amount_log\nn, bins, patches = plt.hist(x, 1000, facecolor=\'green\', alpha=0.75)\n\nplt.xlabel(\'Log of Sales Amount\')\nplt.ylabel(\'Probability\')\nplt.title(r\'$\\mathrm{Histogram\\ of\\ Log\\ transformed\\ Customer\\ Monetary\\ value}\\ $\')\nplt.grid(True)\n#plt.show()\n\n\n# In[33]:\n\n\ncustomer_history_df.head()\n\n\n# In[34]:\n\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection=\'3d\')\n\nxs =customer_history_df.recency_log\nys = customer_history_df.frequency_log\nzs = customer_history_df.amount_log\nax.scatter(xs, ys, zs, s=5)\n\nax.set_xlabel(\'Recency\')\nax.set_ylabel(\'Frequency\')\nax.set_zlabel(\'Monetary\')\n\n#plt.show()\n\n\n# # Analyze Customer Segments with Clustering\n\n# In[34]:\n\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nimport matplotlib.cm as cm\n\nX = X_scaled\n\ncluster_centers = dict()\n\nfor n_clusters in range(3,6,2):\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    #ax2 = plt.subplot(111, projection=\'3d\')\n    fig.set_size_inches(18, 7)\n    ax1.set_xlim([-0.1, 1])\n    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    cluster_centers.update({n_clusters :{\n                                        \'cluster_center\':clusterer.cluster_centers_,\n                                        \'silhouette_score\':silhouette_avg,\n                                        \'labels\':cluster_labels}\n                           })\n\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n    y_lower = 10\n    for i in range(n_clusters):\n        ith_cluster_silhouette_values =             sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.spectral(float(i) / n_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(""The silhouette plot for the various clusters."")\n    ax1.set_xlabel(""The silhouette coefficient values"")\n    ax1.set_ylabel(""Cluster label"")\n    ax1.axvline(x=silhouette_avg, color=""red"", linestyle=""--"")\n    ax1.set_yticks([])\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n    colors = cm.spectral(cluster_labels.astype(float) / n_clusters)\n    feature1 = 0\n    feature2 = 2\n    ax2.scatter(X[:, feature1], X[:, feature2], marker=\'.\', s=30, lw=0, alpha=0.7,\n                c=colors, edgecolor=\'k\')\n    \n    centers = clusterer.cluster_centers_\n    ax2.scatter(centers[:, feature1], centers[:, feature2], marker=\'o\',\n                c=""white"", alpha=1, s=200, edgecolor=\'k\')\n    for i, c in enumerate(centers):\n        ax2.scatter(c[feature1], c[feature2], marker=\'$%d$\' % i, alpha=1,\n                    s=50, edgecolor=\'k\')\n    ax2.set_title(""The visualization of the clustered data."")\n    ax2.set_xlabel(""Feature space for the 1st feature i.e. monetary value"")\n    ax2.set_ylabel(""Feature space for the 2nd feature i.e. frequency"")\n    plt.suptitle((""Silhouette analysis for KMeans clustering on sample data ""\n                  ""with n_clusters = %d"" % n_clusters),\n                 fontsize=14, fontweight=\'bold\')\n    #plt.show()\n\n\n# In[35]:\n\n\nfor i in range(3,6,2):\n    print(""for {} number of clusters"".format(i))\n    cent_transformed = scaler.inverse_transform(cluster_centers[i][\'cluster_center\'])\n    print(pd.DataFrame(np.exp(cent_transformed),columns=feature_vector))\n    print(""Silhouette score for cluster {} is {}"". format(i, cluster_centers[i][\'silhouette_score\']))\n    print()\n\n\n# # Assign Cluster Labels\n\n# In[36]:\n\n\nlabels = cluster_centers[5][\'labels\']   \ncustomer_history_df[\'num_cluster5_labels\'] = labels\nlabels = cluster_centers[3][\'labels\']\ncustomer_history_df[\'num_cluster3_labels\'] = labels\n\n\n# In[37]:\n\n\ncustomer_history_df.head()\n\n\n# # Visualize Segments\n\n# In[38]:\n\n\nimport plotly as py\nimport plotly.graph_objs as go\npy.offline.init_notebook_mode()\n\nx_data = [\'Cluster 1\',\'Cluster 2\',\'Cluster 3\',\'Cluster 4\', \'Cluster 5\']\ncutoff_quantile = 100\nfield_to_plot = \'recency\'\n\ny0 = customer_history_df[customer_history_df[\'num_cluster5_labels\']==0][field_to_plot].values\ny0 = y0[y0<np.percentile(y0, cutoff_quantile)]\ny1 = customer_history_df[customer_history_df[\'num_cluster5_labels\']==1][field_to_plot].values\ny1 = y1[y1<np.percentile(y1, cutoff_quantile)]\ny2 = customer_history_df[customer_history_df[\'num_cluster5_labels\']==2][field_to_plot].values\ny2 = y2[y2<np.percentile(y2, cutoff_quantile)]\ny3 = customer_history_df[customer_history_df[\'num_cluster5_labels\']==3][field_to_plot].values\ny3 = y3[y3<np.percentile(y3, cutoff_quantile)]\ny4 = customer_history_df[customer_history_df[\'num_cluster5_labels\']==4][field_to_plot].values\ny4 = y4[y4<np.percentile(y4, cutoff_quantile)]\ny_data = [y0,y1,y2,y3,y4]\n\ncolors = [\'rgba(93, 164, 214, 0.5)\', \'rgba(255, 144, 14, 0.5)\', \'rgba(44, 160, 101, 0.5)\', \'rgba(255, 65, 54, 0.5)\', \'rgba(207, 114, 255, 0.5)\', \'rgba(127, 96, 0, 0.5)\']\ntraces = []\n\nfor xd, yd, cls in zip(x_data, y_data, colors):\n        traces.append(go.Box(\n            y=yd,\n            name=xd,\n            boxpoints=False,\n            jitter=0.5,\n            whiskerwidth=0.2,\n            fillcolor=cls,\n            marker=dict(\n                size=2,\n            ),\n            line=dict(width=1),\n        ))\n\nlayout = go.Layout(\n    title=\'Difference in sales {} from cluster to cluster\'.format(field_to_plot),\n    yaxis=dict(\n        autorange=True,\n        showgrid=True,\n        zeroline=True,\n        dtick=50,\n        gridcolor=\'black\',\n        gridwidth=0.1,\n        zerolinecolor=\'rgb(255, 255, 255)\',\n        zerolinewidth=2,\n    ),\n    margin=dict(\n        l=40,\n        r=30,\n        b=80,\n        t=100,\n    ),\n    paper_bgcolor=\'white\',\n    plot_bgcolor=\'white\',\n    showlegend=False\n)\n\nfig = go.Figure(data=traces, layout=layout)\npy.offline.iplot(fig)\n\n\n# In[39]:\n\n\nx_data = [\'Cluster 1\',\'Cluster 2\',\'Cluster 3\',\'Cluster 4\', \'Cluster 5\']\ncutoff_quantile = 80\nfield_to_plot = \'amount\'\ny0 = customer_history_df[customer_history_df[\'num_cluster5_labels\']==0][field_to_plot].values\ny0 = y0[y0<np.percentile(y0, cutoff_quantile)]\ny1 = customer_history_df[customer_history_df[\'num_cluster5_labels\']==1][field_to_plot].values\ny1 = y1[y1<np.percentile(y1, cutoff_quantile)]\ny2 = customer_history_df[customer_history_df[\'num_cluster5_labels\']==2][field_to_plot].values\ny2 = y2[y2<np.percentile(y2, cutoff_quantile)]\ny3 = customer_history_df[customer_history_df[\'num_cluster5_labels\']==3][field_to_plot].values\ny3 = y3[y3<np.percentile(y3, cutoff_quantile)]\ny4 = customer_history_df[customer_history_df[\'num_cluster5_labels\']==4][field_to_plot].values\ny4 = y4[y4<np.percentile(y4, cutoff_quantile)]\ny_data = [y0,y1,y2,y3,y4]\n\ncolors = [\'rgba(93, 164, 214, 0.5)\', \'rgba(255, 144, 14, 0.5)\', \'rgba(44, 160, 101, 0.5)\', \'rgba(255, 65, 54, 0.5)\', \'rgba(207, 114, 255, 0.5)\', \'rgba(127, 96, 0, 0.5)\']\ntraces = []\n\nfor xd, yd, cls in zip(x_data, y_data, colors):\n        traces.append(go.Box(\n            y=yd,\n            name=xd,\n            boxpoints=False,\n            jitter=0.5,\n            whiskerwidth=0.2,\n            fillcolor=cls,\n            marker=dict(\n                size=2,\n            ),\n            line=dict(width=1),\n        ))\n\nlayout = go.Layout(\n    title=\'Difference in sales {} from cluster to cluster\'.format(field_to_plot),\n    yaxis=dict(\n        autorange=True,\n        showgrid=True,\n        zeroline=True,\n        dtick=1000,\n        gridcolor=\'black\',\n        gridwidth=0.1,\n        zerolinecolor=\'rgb(255, 255, 255)\',\n        zerolinewidth=2,\n    ),\n    margin=dict(\n        l=40,\n        r=30,\n        b=80,\n        t=100,\n    ),\n    paper_bgcolor=\'white\',\n    plot_bgcolor=\'white\',\n    showlegend=False\n)\n\nfig = go.Figure(data=traces, layout=layout)\npy.offline.iplot(fig)\n\n\n# In[40]:\n\n\nx_data = [\'Cluster 1\',\'Cluster 2\',\'Cluster 3\',\'Cluster 4\', \'Cluster 5\']\ncutoff_quantile = 80\nfield_to_plot = \'frequency\'\ny0 = customer_history_df[customer_history_df[\'num_cluster5_labels\']==0][field_to_plot].values\ny0 = y0[y0<np.percentile(y0, cutoff_quantile)]\ny1 = customer_history_df[customer_history_df[\'num_cluster5_labels\']==1][field_to_plot].values\ny1 = y1[y1<np.percentile(y1, cutoff_quantile)]\ny2 = customer_history_df[customer_history_df[\'num_cluster5_labels\']==2][field_to_plot].values\ny2 = y2[y2<np.percentile(y2, cutoff_quantile)]\ny3 = customer_history_df[customer_history_df[\'num_cluster5_labels\']==3][field_to_plot].values\ny3 = y3[y3<np.percentile(y3, cutoff_quantile)]\ny4 = customer_history_df[customer_history_df[\'num_cluster5_labels\']==4][field_to_plot].values\ny4 = y4[y4<np.percentile(y4, cutoff_quantile)]\ny_data = [y0,y1,y2,y3,y4]\n\ncolors = [\'rgba(93, 164, 214, 0.5)\', \'rgba(255, 144, 14, 0.5)\', \'rgba(44, 160, 101, 0.5)\', \'rgba(255, 65, 54, 0.5)\', \'rgba(207, 114, 255, 0.5)\', \'rgba(127, 96, 0, 0.5)\']\ntraces = []\n\nfor xd, yd, cls in zip(x_data, y_data, colors):\n        traces.append(go.Box(\n            y=yd,\n            name=xd,\n            boxpoints=False,\n            jitter=0.5,\n            whiskerwidth=0.2,\n            fillcolor=cls,\n            marker=dict(\n                size=2,\n            ),\n            line=dict(width=1),\n        ))\n\nlayout = go.Layout(\n    title=\'Difference in sales {} from cluster to cluster\'.format(field_to_plot),\n    yaxis=dict(\n        autorange=True,\n        showgrid=True,\n        zeroline=True,\n        dtick=100,\n        gridcolor=\'black\',\n        gridwidth=0.1,\n        zerolinecolor=\'rgb(255, 255, 255)\',\n        zerolinewidth=2,\n    ),\n    margin=dict(\n        l=40,\n        r=30,\n        b=80,\n        t=100,\n    ),\n    paper_bgcolor=\'white\',\n    plot_bgcolor=\'white\',\n    showlegend=False\n)\n\nfig = go.Figure(data=traces, layout=layout)\npy.offline.iplot(fig)\n\n\n# In[41]:\n\n\nx_data = [\'Cluster 1\',\'Cluster 2\',\'Cluster 3\']\ncutoff_quantile = 100\nfield_to_plot = \'recency\'\ny0 = customer_history_df[customer_history_df[\'num_cluster3_labels\']==0][field_to_plot].values\ny0 = y0[y0<np.percentile(y0, cutoff_quantile)]\ny1 = customer_history_df[customer_history_df[\'num_cluster3_labels\']==1][field_to_plot].values\ny1 = y1[y1<np.percentile(y1, cutoff_quantile)]\ny2 = customer_history_df[customer_history_df[\'num_cluster3_labels\']==2][field_to_plot].values\ny2 = y2[y2<np.percentile(y2, cutoff_quantile)]\n\ny_data = [y0,y1,y2]\n\ncolors = [\'rgba(93, 164, 214, 0.5)\', \'rgba(255, 144, 14, 0.5)\', \'rgba(44, 160, 101, 0.5)\', \'rgba(255, 65, 54, 0.5)\', \'rgba(207, 114, 255, 0.5)\', \'rgba(127, 96, 0, 0.5)\']\ntraces = []\n\nfor xd, yd, cls in zip(x_data, y_data, colors):\n        traces.append(go.Box(\n            y=yd,\n            name=xd,\n            boxpoints=False,\n            jitter=0.5,\n            whiskerwidth=0.2,\n            fillcolor=cls,\n            marker=dict(\n                size=2,\n            ),\n            line=dict(width=1),\n        ))\n\nlayout = go.Layout(\n    title=\'Difference in sales {} from cluster to cluster\'.format(field_to_plot),\n    yaxis=dict(\n        autorange=True,\n        showgrid=True,\n        zeroline=True,\n        dtick=50,\n        gridcolor=\'black\',\n        gridwidth=0.1,\n        zerolinecolor=\'rgb(255, 255, 255)\',\n        zerolinewidth=2,\n    ),\n    margin=dict(\n        l=40,\n        r=30,\n        b=80,\n        t=100,\n    ),\n    plot_bgcolor=\'white\',\n    showlegend=False\n)\n\nfig = go.Figure(data=traces, layout=layout)\npy.offline.iplot(fig)\n\n\n# In[42]:\n\n\nx_data = [\'Cluster 1\',\'Cluster 2\',\'Cluster 3\']\ncutoff_quantile = 80\nfield_to_plot = \'amount\'\ny0 = customer_history_df[customer_history_df[\'num_cluster3_labels\']==0][field_to_plot].values\ny0 = y0[y0<np.percentile(y0, cutoff_quantile)]\ny1 = customer_history_df[customer_history_df[\'num_cluster3_labels\']==1][field_to_plot].values\ny1 = y1[y1<np.percentile(y1, cutoff_quantile)]\ny2 = customer_history_df[customer_history_df[\'num_cluster3_labels\']==2][field_to_plot].values\ny2 = y2[y2<np.percentile(y2, cutoff_quantile)]\n\ny_data = [y0,y1,y2]\n\ncolors = [\'rgba(93, 164, 214, 0.5)\', \'rgba(255, 144, 14, 0.5)\', \'rgba(44, 160, 101, 0.5)\', \'rgba(255, 65, 54, 0.5)\', \'rgba(207, 114, 255, 0.5)\', \'rgba(127, 96, 0, 0.5)\']\ntraces = []\n\nfor xd, yd, cls in zip(x_data, y_data, colors):\n        traces.append(go.Box(\n            y=yd,\n            name=xd,\n            boxpoints=False,\n            jitter=0.5,\n            whiskerwidth=0.2,\n            fillcolor=cls,\n            marker=dict(\n                size=2,\n            ),\n            line=dict(width=1),\n        ))\n\nlayout = go.Layout(\n    title=\'Difference in sales {} from cluster to cluster\'.format(field_to_plot),\n    yaxis=dict(        \n        dtick=1000,\n    )\n)\n\nfig = go.Figure(data=traces, layout=layout)\npy.offline.iplot(fig)\n\n\n# In[43]:\n\n\nx_data = [\'Cluster 1\',\'Cluster 2\',\'Cluster 3\']\ncutoff_quantile = 90\nfield_to_plot = \'frequency\'\ny0 = customer_history_df[customer_history_df[\'num_cluster3_labels\']==0][field_to_plot].values\ny0 = y0[y0<np.percentile(y0, cutoff_quantile)]\ny1 = customer_history_df[customer_history_df[\'num_cluster3_labels\']==1][field_to_plot].values\ny1 = y1[y1<np.percentile(y1, cutoff_quantile)]\ny2 = customer_history_df[customer_history_df[\'num_cluster3_labels\']==2][field_to_plot].values\ny2 = y2[y2<np.percentile(y2, cutoff_quantile)]\n\ny_data = [y0,y1,y2]\n\ncolors = [\'rgba(93, 164, 214, 0.5)\', \'rgba(255, 144, 14, 0.5)\', \'rgba(44, 160, 101, 0.5)\', \'rgba(255, 65, 54, 0.5)\', \'rgba(207, 114, 255, 0.5)\', \'rgba(127, 96, 0, 0.5)\']\ntraces = []\n\nfor xd, yd, cls in zip(x_data, y_data, colors):\n        traces.append(go.Box(\n            y=yd,\n            name=xd,\n            boxpoints=False,\n            jitter=0.5,\n            whiskerwidth=0.2,\n            fillcolor=cls,\n            marker=dict(\n                size=2,\n            ),\n            line=dict(width=1),\n        ))\n\nlayout = go.Layout(\n    title=\'Difference in sales {} from cluster to cluster\'.format(field_to_plot),\n    yaxis=dict(\n        autorange=True,\n        showgrid=True,\n        zeroline=True,\n        dtick=100,\n        gridcolor=\'black\',\n        gridwidth=0.1,\n        zerolinecolor=\'rgb(255, 255, 255)\',\n        zerolinewidth=2,\n    ),\n    margin=dict(\n        l=40,\n        r=30,\n        b=80,\n        t=100,\n    ),\n    paper_bgcolor=\'white\',\n    plot_bgcolor=\'white\',\n    showlegend=False\n)\n\nfig = go.Figure(data=traces, layout=layout)\npy.offline.iplot(fig)\n\n'"
notebooks/Ch09_Analyzing_Wine_Types_and_Quality/exploratory_data_analysis.py,0,"b'\n# coding: utf-8\n""""""\nCreated on Sun Sep 17 00:00:00 2017\n\n@author: DIP\n""""""\n\n# # Import necessary dependencies\n\n# In[1]:\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport numpy as np\nimport seaborn as sns\n\nget_ipython().magic(\'matplotlib inline\')\n\n\n# # Load and merge datasets\n\n# In[2]:\n\nwhite_wine = pd.read_csv(\'winequality-white.csv\', sep=\';\')\nred_wine = pd.read_csv(\'winequality-red.csv\', sep=\';\')\n\n# store wine type as an attribute\nred_wine[\'wine_type\'] = \'red\'   \nwhite_wine[\'wine_type\'] = \'white\'\n# bucket wine quality scores into qualitative quality labels\nred_wine[\'quality_label\'] = red_wine[\'quality\'].apply(lambda value: \'low\' \n                                                          if value <= 5 else \'medium\' \n                                                              if value <= 7 else \'high\')\nred_wine[\'quality_label\'] = pd.Categorical(red_wine[\'quality_label\'], \n                                           categories=[\'low\', \'medium\', \'high\'])\nwhite_wine[\'quality_label\'] = white_wine[\'quality\'].apply(lambda value: \'low\' \n                                                              if value <= 5 else \'medium\' \n                                                                  if value <= 7 else \'high\')\nwhite_wine[\'quality_label\'] = pd.Categorical(white_wine[\'quality_label\'], \n                                             categories=[\'low\', \'medium\', \'high\'])\n\n# merge red and white wine datasets\nwines = pd.concat([red_wine, white_wine])\n# re-shuffle records just to randomize data points\nwines = wines.sample(frac=1, random_state=42).reset_index(drop=True)\n\n\n# # Understand dataset features and values\n\n# In[3]:\n\nprint(white_wine.shape, red_wine.shape)\nprint(wines.info())\n\n\n# In[4]:\n\nwines.head()\n\n\n# ### Understanding Wine and Types\n# \n# Wine is an alcoholic beverage made from grapes which is fermented without the addition of sugars, acids, enzymes, water, or other nutrients\n# \n# Red wine is made from dark red and black grapes. The color usually ranges from various shades of red, brown and violet. This is produced with whole grapes including the skin which adds to the color and flavor of red wines, giving it a rich flavor.\n# \n# White wine is made from white grapes with no skins or seeds. The color is usually straw-yellow, yellow-green, or yellow-gold. Most white wines have a light and fruity flavor as compared to richer red wines.\n# \n# ### Understanding Wine Attributes and Properties \n# \n# \n# - **fixed acidity:** Acids are one of the fundamental properties of wine and contribute greatly to the taste of the wine. Reducing acids significantly might lead to wines tasting flat. Fixed acids include tartaric, malic, citric, and succinic acids which are found in grapes (except succinic). This variable is usually expressed in $\\frac{g(tartaricacid)}{dm^3}$ in the dataset.\n# \n# \n# - **volatile acidity:** These acids are to be distilled out from the wine before completing the production process. It is primarily constituted of acetic acid though other acids like lactic, formic and butyric acids might also be present. Excess of volatile acids are undesirable and lead to unpleasant flavor. In the US, the legal limits of volatile acidity are 1.2 g/L for red table wine and 1.1 g/L for white table wine. The volatile acidity is expressed in $\\frac{g(aceticacid)}{dm^3}$ in the dataset.\n# \n# \n# - **citric acid:** This is one of the fixed acids which gives a wine its freshness. Usually most of it is consumed during the fermentation process and sometimes it is added separately to give the wine more freshness. It\'s usually expressed in $\\frac{g}{dm^3}$ in the dataset.\n# \n# \n# - **residual sugar:** This typically refers to the natural sugar from grapes which remains after the fermentation process stops, or is stopped. It\'s usually expressed in $\\frac{g}{dm^3}$ in the dataset.\n# \n# \n# - **chlorides:** This is usually a major contributor to saltiness in wine. It\'s usually expressed in $\\frac{g(sodiumchloride)}{dm^3}$ in the dataset.\n# \n# \n# - **free sulfur dioxide:** This is the part of the sulphur dioxide that when added to a wine is said to be free after the remaining part binds. Winemakers will always try to get the highest proportion of free sulphur to bind. They are also known as sulfites and too much of it is undesirable and gives a pungent odour. This variable is expressed in $\\frac{mg}{dm^3}$ in the dataset.\n# \n# \n# - **total sulfur dioxide:** This is the sum total of the bound and the free sulfur dioxide ($SO_2$). Here, it\'s expressed in $\\frac{mg}{dm^3}$. This is mainly added to kill harmful bacteria and preserve quality and freshness. There are usually legal limits for sulfur levels in wines and excess of it can even kill good yeast and give out undesirable odour.\n# \n# \n# - **density:** This can be represented as a comparison of the weight of a specific volume of wine to an equivalent volume of water. It is generally used as a measure of the conversion of sugar to alcohol. Here, it\'s expressed in $\\frac{g}{cm^3}$.\n# \n# \n# - **pH:** Also known as the potential of hydrogen, this is a numeric scale to specify the acidity or basicity the wine. Fixed acidity contributes the most towards the pH of wines. You might know, solutions with a pH less than 7 are acidic, while solutions with a pH greater than 7 are basic. With a pH of 7, pure water is neutral. Most wines have a pH between 2.9 and 3.9 and are therefore acidic.\n# \n# \n# - **sulphates:** These are mineral salts containing sulfur. Sulphates are to wine as gluten is to food. They are a regular part of the winemaking around the world and are considered essential. They are connected to the fermentation process and affects the wine aroma and flavor. Here, it\'s expressed in $\\frac{g(potassiumsulphate)}{dm^3}$ in the dataset.\n# \n# \n# - **alcohol:** Wine is an alcoholic beverage. Alcohol is formed as a result of yeast converting sugar during the fermentation process. The percentage of alcohol can vary from wine to wine. Hence it is not a surprise for this attribute to be a part of this dataset. It\'s usually measured in % vol or alcohol by volume (ABV).\n# \n# \n# - **quality:** Wine experts graded the wine quality between 0 (very bad) and 10 (very excellent). The eventual quality score is the median of at least three evaluations made by the same wine experts.\n# \n# \n# - **wine_type:** Since we originally had two datasets for red and white wine, we introduced this attribute in the final merged dataset which indicates the type of wine for each data point. A wine can either be a \'red\' or a \'white\' wine. One of the predictive models we will build in this chapter would be such that we can predict the type of wine by looking at other wine attributes.\n# \n# \n# - **quality_label:** This is a derived attribute from the `quality` attribute. We bucket or group wine quality scores into three qualitative buckets namely low, medium and high. Wines with a quality score of 3, 4 & 5 are low quality, scores of 6 & 7 are medium quality and scores of 8 & 9 are high quality wines. We will also build another model in this chapter to predict this wine quality label based on other wine attributes. \n\n# # Exploratory Data Analysis and Visualizations\n\n# ## Descriptive Statistics\n\n# In[5]:\n\nsubset_attributes = [\'residual sugar\', \'total sulfur dioxide\', \'sulphates\', \'alcohol\', \'volatile acidity\', \'quality\']\nrs = round(red_wine[subset_attributes].describe(),2)\nws = round(white_wine[subset_attributes].describe(),2)\npd.concat([rs, ws], axis=1, keys=[\'Red Wine Statistics\', \'White Wine Statistics\'])\n\n\n# In[6]:\n\nsubset_attributes = [\'alcohol\', \'volatile acidity\', \'pH\', \'quality\']\nls = round(wines[wines[\'quality_label\'] == \'low\'][subset_attributes].describe(),2)\nms = round(wines[wines[\'quality_label\'] == \'medium\'][subset_attributes].describe(),2)\nhs = round(wines[wines[\'quality_label\'] == \'high\'][subset_attributes].describe(),2)\npd.concat([ls, ms, hs], axis=1, keys=[\'Low Quality Wine\', \'Medium Quality Wine\', \'High Quality Wine\'])\n\n\n# ## Inferential Statistics\n\n# In[7]:\n\nfrom scipy import stats\n \nF, p = stats.f_oneway(wines[wines[\'quality_label\'] == \'low\'][\'alcohol\'], \n                      wines[wines[\'quality_label\'] == \'medium\'][\'alcohol\'], \n                      wines[wines[\'quality_label\'] == \'high\'][\'alcohol\'])\nprint(\'ANOVA test for mean alcohol levels across wine samples with different quality ratings\')\nprint(\'F Statistic:\', F, \'\\tp-value:\', p)\n\nF, p = stats.f_oneway(wines[wines[\'quality_label\'] == \'low\'][\'pH\'], \n                      wines[wines[\'quality_label\'] == \'medium\'][\'pH\'], \n                      wines[wines[\'quality_label\'] == \'high\'][\'pH\'])\nprint(\'\\nANOVA test for mean pH levels across wine samples with different quality ratings\')\nprint(\'F Statistic:\', F, \'\\tp-value:\', p)\n\n\n# In[8]:\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\nf.suptitle(\'Wine Quality - Alcohol Content/pH\', fontsize=14)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nsns.boxplot(x=""quality_label"", y=""alcohol"",\n            data=wines, ax=ax1)\nax1.set_xlabel(""Wine Quality Class"",size = 12,alpha=0.8)\nax1.set_ylabel(""Wine Alcohol %"",size = 12,alpha=0.8)\n\nsns.boxplot(x=""quality_label"", y=""pH"", data=wines, ax=ax2)\nax2.set_xlabel(""Wine Quality Class"",size = 12,alpha=0.8)\nax2.set_ylabel(""Wine pH"",size = 12,alpha=0.8)\n\n\n# ## Univariate Analysis\n\n# In[9]:\n\nred_wine.hist(bins=15, color=\'red\', edgecolor=\'black\', linewidth=1.0,\n              xlabelsize=8, ylabelsize=8, grid=False)    \nplt.tight_layout(rect=(0, 0, 1.2, 1.2))   \nrt = plt.suptitle(\'Red Wine Univariate Plots\', x=0.65, y=1.25, fontsize=14)  \n\nwhite_wine.hist(bins=15, color=\'white\', edgecolor=\'black\', linewidth=1.0,\n              xlabelsize=8, ylabelsize=8, grid=False)    \nplt.tight_layout(rect=(0, 0, 1.2, 1.2))   \nwt = plt.suptitle(\'White Wine Univariate Plots\', x=0.65, y=1.25, fontsize=14)    \n\n\n# In[10]:\n\nfig = plt.figure(figsize = (10,4))\ntitle = fig.suptitle(""Residual Sugar Content in Wine"", fontsize=14)\nfig.subplots_adjust(top=0.85, wspace=0.3)\n\nax1 = fig.add_subplot(1,2, 1)\nax1.set_title(""Red Wine"")\nax1.set_xlabel(""Residual Sugar"")\nax1.set_ylabel(""Frequency"") \nax1.set_ylim([0, 2500])\nax1.text(8, 1000, r\'$\\mu$=\'+str(round(red_wine[\'residual sugar\'].mean(),2)), \n         fontsize=12)\nr_freq, r_bins, r_patches = ax1.hist(red_wine[\'residual sugar\'], color=\'red\', bins=15,\n                                    edgecolor=\'black\', linewidth=1)\n\nax2 = fig.add_subplot(1,2, 2)\nax2.set_title(""White Wine"")\nax2.set_xlabel(""Residual Sugar"")\nax2.set_ylabel(""Frequency"")\nax2.set_ylim([0, 2500])\nax2.text(30, 1000, r\'$\\mu$=\'+str(round(white_wine[\'residual sugar\'].mean(),2)), \n         fontsize=12)\nw_freq, w_bins, w_patches = ax2.hist(white_wine[\'residual sugar\'], color=\'white\', bins=15,\n                                    edgecolor=\'black\', linewidth=1)\n\n\n# In[11]:\n\nfig = plt.figure(figsize = (10,4))\ntitle = fig.suptitle(""Sulphates Content in Wine"", fontsize=14)\nfig.subplots_adjust(top=0.85, wspace=0.3)\n\nax1 = fig.add_subplot(1,2, 1)\nax1.set_title(""Red Wine"")\nax1.set_xlabel(""Sulphates"")\nax1.set_ylabel(""Frequency"") \nax1.set_ylim([0, 1200])\nax1.text(1.2, 800, r\'$\\mu$=\'+str(round(red_wine[\'sulphates\'].mean(),2)), \n         fontsize=12)\nr_freq, r_bins, r_patches = ax1.hist(red_wine[\'sulphates\'], color=\'red\', bins=15,\n                                    edgecolor=\'black\', linewidth=1)\n\nax2 = fig.add_subplot(1,2, 2)\nax2.set_title(""White Wine"")\nax2.set_xlabel(""Sulphates"")\nax2.set_ylabel(""Frequency"")\nax2.set_ylim([0, 1200])\nax2.text(0.8, 800, r\'$\\mu$=\'+str(round(white_wine[\'sulphates\'].mean(),2)), \n         fontsize=12)\nw_freq, w_bins, w_patches = ax2.hist(white_wine[\'sulphates\'], color=\'white\', bins=15,\n                                    edgecolor=\'black\', linewidth=1)\n\n\n# In[12]:\n\nfig = plt.figure(figsize = (10,4))\ntitle = fig.suptitle(""Alcohol Content in Wine"", fontsize=14)\nfig.subplots_adjust(top=0.85, wspace=0.3)\n\nax1 = fig.add_subplot(1,2, 1)\nax1.set_title(""Red Wine"")\nax1.set_xlabel(""Alcohol % by Volume"")\nax1.set_ylabel(""Frequency"") \nax1.set_ylim([0, 800])\nax1.text(12, 600, r\'$\\mu$=\'+str(round(red_wine[\'alcohol\'].mean(),2)), \n         fontsize=12)\nr_freq, r_bins, r_patches = ax1.hist(red_wine[\'alcohol\'], color=\'red\', bins=15,\n                                    edgecolor=\'black\', linewidth=1)\n\nax2 = fig.add_subplot(1,2, 2)\nax2.set_title(""White Wine"")\nax2.set_xlabel(""Alcohol % by Volume"")\nax2.set_ylabel(""Frequency"")\nax2.set_ylim([0, 800])\nax2.text(12, 600, r\'$\\mu$=\'+str(round(white_wine[\'alcohol\'].mean(),2)), \n         fontsize=12)\nw_freq, w_bins, w_patches = ax2.hist(white_wine[\'alcohol\'], color=\'white\', bins=15,\n                                    edgecolor=\'black\', linewidth=1)\n\n\n# In[13]:\n\nfig = plt.figure(figsize = (18, 4))\ntitle = fig.suptitle(""Wine Type - Quality"", fontsize=14)\nfig.subplots_adjust(top=0.85, wspace=0.3)\n\nax1 = fig.add_subplot(1,4, 1)\nax1.set_title(""Red Wine"")\nax1.set_xlabel(""Quality"")\nax1.set_ylabel(""Frequency"") \nrw_q = red_wine[\'quality\'].value_counts()\nrw_q = (list(rw_q.index), list(rw_q.values))\nax1.set_ylim([0, 2500])\nax1.tick_params(axis=\'both\', which=\'major\', labelsize=8.5)\nbar1 = ax1.bar(rw_q[0], rw_q[1], color=\'red\', \n        edgecolor=\'black\', linewidth=1)\n\n\nax2 = fig.add_subplot(1,4, 2)\nax2.set_title(""White Wine"")\nax2.set_xlabel(""Quality"")\nax2.set_ylabel(""Frequency"") \nww_q = white_wine[\'quality\'].value_counts()\nww_q = (list(ww_q.index), list(ww_q.values))\nax2.set_ylim([0, 2500])\nax2.tick_params(axis=\'both\', which=\'major\', labelsize=8.5)\nbar2 = ax2.bar(ww_q[0], ww_q[1], color=\'white\', \n        edgecolor=\'black\', linewidth=1)\n\nax3 = fig.add_subplot(1,4, 3)\nax3.set_title(""Red Wine"")\nax3.set_xlabel(""Quality Class"")\nax3.set_ylabel(""Frequency"") \nrw_q = red_wine[\'quality_label\'].value_counts()\nrw_q = (list(rw_q.index), list(rw_q.values))\nax3.set_ylim([0, 3200])\nbar3 = ax3.bar(list(range(len(rw_q[0]))), rw_q[1], color=\'red\', \n        edgecolor=\'black\', linewidth=1, tick_label =rw_q[0])\n\nax4 = fig.add_subplot(1,4, 4)\nax4.set_title(""White Wine"")\nax4.set_xlabel(""Quality Class"")\nax4.set_ylabel(""Frequency"") \nww_q = white_wine[\'quality_label\'].value_counts()\nww_q = (list(ww_q.index), list(ww_q.values))\nax4.set_ylim([0, 3200])\nbar4 = ax4.bar(list(range(len(ww_q[0]))), ww_q[1], color=\'white\', \n        edgecolor=\'black\', linewidth=1, tick_label =ww_q[0])\n\n\n# ## Multivariate Analysis\n\n# In[14]:\n\nf, ax = plt.subplots(figsize=(10, 6))\ncorr = wines.corr()\nhm = sns.heatmap(round(corr,2), annot=True, ax=ax, cmap=""coolwarm"",fmt=\'.2f\',\n            linewidths=.05)\nf.subplots_adjust(top=0.93)\nt= f.suptitle(\'Wine Attributes Correlation Heatmap\', fontsize=14)\n\n\n# In[15]:\n\ncols = [\'wine_type\', \'quality\', \'sulphates\', \'volatile acidity\']\npp = sns.pairplot(wines[cols], hue=\'wine_type\', size=1.8, aspect=1.8, \n                  palette={""red"": ""#FF9999"", ""white"": ""#FFE888""},\n                  plot_kws=dict(edgecolor=""black"", linewidth=0.5))\nfig = pp.fig \nfig.subplots_adjust(top=0.93, wspace=0.3)\nt = fig.suptitle(\'Wine Attributes Pairwise Plots\', fontsize=14)\n\n\n# In[16]:\n\nrj = sns.jointplot(x=\'quality\', y=\'sulphates\', data=red_wine,\n                   kind=\'reg\', ylim=(0, 2),  \n                   color=\'red\', space=0, size=4.5, ratio=4)\nrj.ax_joint.set_xticks(list(range(3,9)))\nfig = rj.fig \nfig.subplots_adjust(top=0.9)\nt = fig.suptitle(\'Red Wine Sulphates - Quality\', fontsize=12)\n\nwj = sns.jointplot(x=\'quality\', y=\'sulphates\', data=white_wine,\n                   kind=\'reg\', ylim=(0, 2),\n                   color=\'#FFE160\', space=0, size=4.5, ratio=4)\nwj.ax_joint.set_xticks(list(range(3,10)))\nfig = wj.fig \nfig.subplots_adjust(top=0.9)\nt = fig.suptitle(\'White Wine Sulphates - Quality\', fontsize=12)\n\n\n# In[17]:\n\ng = sns.FacetGrid(wines, col=""wine_type"", hue=\'quality_label\', \n                  col_order=[\'red\', \'white\'], hue_order=[\'low\', \'medium\', \'high\'],\n                  aspect=1.2, size=3.5, palette=sns.light_palette(\'navy\', 3))\ng.map(plt.scatter, ""volatile acidity"", ""alcohol"", alpha=0.9, \n      edgecolor=\'white\', linewidth=0.5)\nfig = g.fig \nfig.subplots_adjust(top=0.8, wspace=0.3)\nfig.suptitle(\'Wine Type - Alcohol - Quality - Acidity\', fontsize=14)\nl = g.add_legend(title=\'Wine Quality Class\')\n\n\n# In[18]:\n\ng = sns.FacetGrid(wines, col=""wine_type"", hue=\'quality_label\', \n                  col_order=[\'red\', \'white\'], hue_order=[\'low\', \'medium\', \'high\'],\n                  aspect=1.2, size=3.5, palette=sns.light_palette(\'green\', 3))\ng.map(plt.scatter, ""volatile acidity"", ""total sulfur dioxide"", alpha=0.9, \n      edgecolor=\'white\', linewidth=0.5)\nfig = g.fig \nfig.subplots_adjust(top=0.8, wspace=0.3)\nfig.suptitle(\'Wine Type - Sulfur Dioxide - Acidity - Quality\', fontsize=14)\nl = g.add_legend(title=\'Wine Quality Class\')\n\n\n# In[19]:\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\nf.suptitle(\'Wine Type - Quality - Alcohol Content\', fontsize=14)\n\nsns.boxplot(x=""quality"", y=""alcohol"", hue=""wine_type"",\n               data=wines, palette={""red"": ""#FF9999"", ""white"": ""white""}, ax=ax1)\nax1.set_xlabel(""Wine Quality"",size = 12,alpha=0.8)\nax1.set_ylabel(""Wine Alcohol %"",size = 12,alpha=0.8)\n\nsns.boxplot(x=""quality_label"", y=""alcohol"", hue=""wine_type"",\n               data=wines, palette={""red"": ""#FF9999"", ""white"": ""white""}, ax=ax2)\nax2.set_xlabel(""Wine Quality Class"",size = 12,alpha=0.8)\nax2.set_ylabel(""Wine Alcohol %"",size = 12,alpha=0.8)\nl = plt.legend(loc=\'best\', title=\'Wine Type\')\n\n\n# In[20]:\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\nf.suptitle(\'Wine Type - Quality - Acidity\', fontsize=14)\n\nsns.violinplot(x=""quality"", y=""volatile acidity"", hue=""wine_type"", \n               data=wines, split=True, inner=""quart"", linewidth=1.3,\n               palette={""red"": ""#FF9999"", ""white"": ""white""}, ax=ax1)\nax1.set_xlabel(""Wine Quality"",size = 12,alpha=0.8)\nax1.set_ylabel(""Wine Fixed Acidity"",size = 12,alpha=0.8)\n\nsns.violinplot(x=""quality_label"", y=""volatile acidity"", hue=""wine_type"", \n               data=wines, split=True, inner=""quart"", linewidth=1.3,\n               palette={""red"": ""#FF9999"", ""white"": ""white""}, ax=ax2)\nax2.set_xlabel(""Wine Quality Class"",size = 12,alpha=0.8)\nax2.set_ylabel(""Wine Fixed Acidity"",size = 12,alpha=0.8)\nl = plt.legend(loc=\'upper right\', title=\'Wine Type\')\n\n'"
notebooks/Ch09_Analyzing_Wine_Types_and_Quality/model_evaluation_utils.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Mon Jul 31 20:05:23 2017\n\n@author: DIP\n@Copyright: Dipanjan Sarkar\n""""""\n\nfrom sklearn import metrics\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.base import clone\nfrom sklearn.preprocessing import label_binarize\nfrom scipy import interp\nfrom sklearn.metrics import roc_curve, auc \n\n\ndef get_metrics(true_labels, predicted_labels):\n    \n    print(\'Accuracy:\', np.round(\n                        metrics.accuracy_score(true_labels, \n                                               predicted_labels),\n                        4))\n    print(\'Precision:\', np.round(\n                        metrics.precision_score(true_labels, \n                                               predicted_labels,\n                                               average=\'weighted\'),\n                        4))\n    print(\'Recall:\', np.round(\n                        metrics.recall_score(true_labels, \n                                               predicted_labels,\n                                               average=\'weighted\'),\n                        4))\n    print(\'F1 Score:\', np.round(\n                        metrics.f1_score(true_labels, \n                                               predicted_labels,\n                                               average=\'weighted\'),\n                        4))\n                        \n\ndef train_predict_model(classifier, \n                        train_features, train_labels, \n                        test_features, test_labels):\n    # build model    \n    classifier.fit(train_features, train_labels)\n    # predict using model\n    predictions = classifier.predict(test_features) \n    return predictions    \n\n\ndef display_confusion_matrix(true_labels, predicted_labels, classes=[1,0]):\n    \n    total_classes = len(classes)\n    level_labels = [total_classes*[0], list(range(total_classes))]\n\n    cm = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels, \n                                  labels=classes)\n    cm_frame = pd.DataFrame(data=cm, \n                            columns=pd.MultiIndex(levels=[[\'Predicted:\'], classes], \n                                                  labels=level_labels), \n                            index=pd.MultiIndex(levels=[[\'Actual:\'], classes], \n                                                labels=level_labels)) \n    print(cm_frame) \n    \ndef display_classification_report(true_labels, predicted_labels, classes=[1,0]):\n\n    report = metrics.classification_report(y_true=true_labels, \n                                           y_pred=predicted_labels, \n                                           labels=classes) \n    print(report)\n    \n    \n    \ndef display_model_performance_metrics(true_labels, predicted_labels, classes=[1,0]):\n    print(\'Model Performance metrics:\')\n    print(\'-\'*30)\n    get_metrics(true_labels=true_labels, predicted_labels=predicted_labels)\n    print(\'\\nModel Classification report:\')\n    print(\'-\'*30)\n    display_classification_report(true_labels=true_labels, predicted_labels=predicted_labels, \n                                  classes=classes)\n    print(\'\\nPrediction Confusion Matrix:\')\n    print(\'-\'*30)\n    display_confusion_matrix(true_labels=true_labels, predicted_labels=predicted_labels, \n                             classes=classes)\n\n\ndef plot_model_decision_surface(clf, train_features, train_labels,\n                                plot_step=0.02, cmap=plt.cm.RdYlBu,\n                                markers=None, alphas=None, colors=None):\n    \n    if train_features.shape[1] != 2:\n        raise ValueError(""X_train should have exactly 2 columnns!"")\n    \n    x_min, x_max = train_features[:, 0].min() - plot_step, train_features[:, 0].max() + plot_step\n    y_min, y_max = train_features[:, 1].min() - plot_step, train_features[:, 1].max() + plot_step\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                         np.arange(y_min, y_max, plot_step))\n\n    clf_est = clone(clf)\n    clf_est.fit(train_features,train_labels)\n    if hasattr(clf_est, \'predict_proba\'):\n        Z = clf_est.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n    else:\n        Z = clf_est.predict(np.c_[xx.ravel(), yy.ravel()])    \n    Z = Z.reshape(xx.shape)\n    cs = plt.contourf(xx, yy, Z, cmap=cmap)\n    \n    le = LabelEncoder()\n    y_enc = le.fit_transform(train_labels)\n    n_classes = len(le.classes_)\n    plot_colors = \'\'.join(colors) if colors else [None] * n_classes\n    label_names = le.classes_\n    markers = markers if markers else [None] * n_classes\n    alphas = alphas if alphas else [None] * n_classes\n    for i, color in zip(range(n_classes), plot_colors):\n        idx = np.where(y_enc == i)\n        plt.scatter(train_features[idx, 0], train_features[idx, 1], c=color,\n                    label=label_names[i], cmap=cmap, edgecolors=\'black\', \n                    marker=markers[i], alpha=alphas[i])\n    plt.legend()\n    plt.show()\n\n\ndef plot_model_roc_curve(clf, features, true_labels, label_encoder=None, class_names=None):\n    \n    ## Compute ROC curve and ROC area for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    if hasattr(clf, \'classes_\'):\n        class_labels = clf.classes_\n    elif label_encoder:\n        class_labels = label_encoder.classes_\n    elif class_names:\n        class_labels = class_names\n    else:\n        raise ValueError(\'Unable to derive prediction classes, please specify class_names!\')\n    n_classes = len(class_labels)\n    y_test = label_binarize(true_labels, classes=class_labels)\n    if n_classes == 2:\n        if hasattr(clf, \'predict_proba\'):\n            prob = clf.predict_proba(features)\n            y_score = prob[:, prob.shape[1]-1] \n        elif hasattr(clf, \'decision_function\'):\n            prob = clf.decision_function(features)\n            y_score = prob[:, prob.shape[1]-1]\n        else:\n            raise AttributeError(""Estimator doesn\'t have a probability or confidence scoring system!"")\n        \n        fpr, tpr, _ = roc_curve(y_test, y_score)      \n        roc_auc = auc(fpr, tpr)\n        plt.plot(fpr, tpr, label=\'ROC curve (area = {0:0.2f})\'\n                                 \'\'.format(roc_auc),\n                 linewidth=2.5)\n        \n    elif n_classes > 2:\n        if hasattr(clf, \'predict_proba\'):\n            y_score = clf.predict_proba(features)\n        elif hasattr(clf, \'decision_function\'):\n            y_score = clf.decision_function(features)\n        else:\n            raise AttributeError(""Estimator doesn\'t have a probability or confidence scoring system!"")\n\n        for i in range(n_classes):\n            fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n            roc_auc[i] = auc(fpr[i], tpr[i])\n\n        ## Compute micro-average ROC curve and ROC area\n        fpr[""micro""], tpr[""micro""], _ = roc_curve(y_test.ravel(), y_score.ravel())\n        roc_auc[""micro""] = auc(fpr[""micro""], tpr[""micro""])\n\n        ## Compute macro-average ROC curve and ROC area\n        # First aggregate all false positive rates\n        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n        # Then interpolate all ROC curves at this points\n        mean_tpr = np.zeros_like(all_fpr)\n        for i in range(n_classes):\n            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n        # Finally average it and compute AUC\n        mean_tpr /= n_classes\n        fpr[""macro""] = all_fpr\n        tpr[""macro""] = mean_tpr\n        roc_auc[""macro""] = auc(fpr[""macro""], tpr[""macro""])\n\n        ## Plot ROC curves\n        plt.figure(figsize=(6, 4))\n        plt.plot(fpr[""micro""], tpr[""micro""],\n                 label=\'micro-average ROC curve (area = {0:0.2f})\'\n                       \'\'.format(roc_auc[""micro""]), linewidth=3)\n\n        plt.plot(fpr[""macro""], tpr[""macro""],\n                 label=\'macro-average ROC curve (area = {0:0.2f})\'\n                       \'\'.format(roc_auc[""macro""]), linewidth=3)\n\n        for i, label in enumerate(class_labels):\n            plt.plot(fpr[i], tpr[i], label=\'ROC curve of class {0} (area = {1:0.2f})\'\n                                           \'\'.format(label, roc_auc[i]), \n                     linewidth=2, linestyle=\':\')\n    else:\n        raise ValueError(\'Number of classes should be atleast 2 or more\')\n        \n    plt.plot([0, 1], [0, 1], \'k--\')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel(\'False Positive Rate\')\n    plt.ylabel(\'True Positive Rate\')\n    plt.title(\'Receiver Operating Characteristic (ROC) Curve\')\n    plt.legend(loc=""lower right"")\n    plt.show()\n\n\n'"
notebooks/Ch09_Analyzing_Wine_Types_and_Quality/predictive_analytics.py,0,"b'\n# coding: utf-8\n""""""\nCreated on Sun Sep 17 00:00:00 2017\n\n@author: DIP\n""""""\n\n# # Import necessary dependencies\n\n# In[1]:\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport model_evaluation_utils as meu\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nget_ipython().magic(\'matplotlib inline\')\n\n\n# # Load and Merge datasets\n\n# In[2]:\n\nwhite_wine = pd.read_csv(\'winequality-white.csv\', sep=\';\')\nred_wine = pd.read_csv(\'winequality-red.csv\', sep=\';\')\n\nred_wine[\'wine_type\'] = \'red\'   # add a column for the type\nwhite_wine[\'wine_type\'] = \'white\'\nwines = pd.concat([red_wine, white_wine])\nwines[\'quality_label\'] = wines[\'quality\'].apply(lambda value: \'low\' if value <= 5 else \'medium\' if value <= 7 else \'high\')\nwines = wines.sample(frac=1, random_state=42).reset_index(drop=True)\n\n\n# In[3]:\n\nwines.head()\n\n\n# # Predicting Wine Types\n\n# ## Prepare Training and Testing datasets\n\n# In[58]:\n\nwtp_features = wines.iloc[:,:-3]\nwtp_feature_names = wtp_features.columns\nwtp_class_labels = np.array(wines[\'wine_type\'])\n\nwtp_train_X, wtp_test_X, wtp_train_y, wtp_test_y = train_test_split(wtp_features, wtp_class_labels, \n                                                                    test_size=0.3, random_state=42)\n\nprint(Counter(wtp_train_y), Counter(wtp_test_y))\nprint(\'Features:\', list(wtp_feature_names))\n\n\n# ## Feature Scaling\n\n# In[5]:\n\n# Define the scaler \nwtp_ss = StandardScaler().fit(wtp_train_X)\n\n# Scale the train set\nwtp_train_SX = wtp_ss.transform(wtp_train_X)\n\n# Scale the test set\nwtp_test_SX = wtp_ss.transform(wtp_test_X)\n\n\n# ## Train a Model using Logistic Regression\n\n# In[6]:\n\nfrom sklearn.linear_model import LogisticRegression\n\nwtp_lr = LogisticRegression()\nwtp_lr.fit(wtp_train_SX, wtp_train_y)\n\n\n# ## Predict and Evaluate Model Performance\n\n# In[7]:\n\nwtp_lr_predictions = wtp_lr.predict(wtp_test_SX)\nmeu.display_model_performance_metrics(true_labels=wtp_test_y, predicted_labels=wtp_lr_predictions, \n                                      classes=[\'red\', \'white\'])\n\n\n# ## Train a Model using Deep Learning (MLP)\n\n# ### Encode Response class labels \n\n# In[8]:\n\nle = LabelEncoder()\nle.fit(wtp_train_y)\n# encode wine type labels\nwtp_train_ey = le.transform(wtp_train_y)\nwtp_test_ey = le.transform(wtp_test_y)\n\n\n# ### Build & Compile DNN Model Architecture\n\n# In[41]:\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nwtp_dnn_model = Sequential()\nwtp_dnn_model.add(Dense(16, activation=\'relu\', input_shape=(11,)))\nwtp_dnn_model.add(Dense(16, activation=\'relu\'))\nwtp_dnn_model.add(Dense(16, activation=\'relu\'))\nwtp_dnn_model.add(Dense(1, activation=\'sigmoid\'))\n\nwtp_dnn_model.compile(loss=\'binary_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\'])\n\n\n# ### Train the Model\n\n# In[42]:\n\nhistory = wtp_dnn_model.fit(wtp_train_SX, wtp_train_ey, epochs=10, batch_size=5, \n                            shuffle=True, validation_split=0.1, verbose=1)\n\n\n# ### Predict on Test dataset\n\n# In[43]:\n\nwtp_dnn_ypred = wtp_dnn_model.predict_classes(wtp_test_SX)\nwtp_dnn_predictions = le.inverse_transform(wtp_dnn_ypred) \n\n\n# ### Evaluate Model Performance\n\n# In[44]:\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\nt = f.suptitle(\'Deep Neural Net Performance\', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepochs = list(range(1,11))\nax1.plot(epochs, history.history[\'acc\'], label=\'Train Accuracy\')\nax1.plot(epochs, history.history[\'val_acc\'], label=\'Validation Accuracy\')\nax1.set_xticks(epochs)\nax1.set_ylabel(\'Accuracy Value\')\nax1.set_xlabel(\'Epoch\')\nax1.set_title(\'Accuracy\')\nl1 = ax1.legend(loc=""best"")\n\nax2.plot(epochs, history.history[\'loss\'], label=\'Train Loss\')\nax2.plot(epochs, history.history[\'val_loss\'], label=\'Validation Loss\')\nax2.set_xticks(epochs)\nax2.set_ylabel(\'Loss Value\')\nax2.set_xlabel(\'Epoch\')\nax2.set_title(\'Loss\')\nl2 = ax2.legend(loc=""best"")\n\n\n# In[45]:\n\nmeu.display_model_performance_metrics(true_labels=wtp_test_y, predicted_labels=wtp_dnn_predictions, \n                                      classes=[\'red\', \'white\'])\n\n\n# # Model Interpretation\n\n# ## View Feature importances\n\n# In[14]:\n\nfrom skater.core.explanations import Interpretation\nfrom skater.model import InMemoryModel\n\nwtp_interpreter = Interpretation(wtp_test_SX, feature_names=wtp_features.columns)\nwtp_im_model = InMemoryModel(wtp_lr.predict_proba, examples=wtp_train_SX, target_names=wtp_lr.classes_)\nplots = wtp_interpreter.feature_importance.plot_feature_importance(wtp_im_model, ascending=False)\n\n\n# ## View model ROC curve\n\n# In[15]:\n\nmeu.plot_model_roc_curve(wtp_lr, wtp_test_SX, wtp_test_y)\n\n\n# ## Visualize Model Decision Surface\n\n# In[59]:\n\nfeature_indices = [i for i, feature in enumerate(wtp_feature_names) \n                       if feature in [\'density\', \'total sulfur dioxide\']]\nmeu.plot_model_decision_surface(clf=wtp_lr, train_features=wtp_train_SX[:, feature_indices], \n                                train_labels=wtp_train_y, plot_step=0.02, cmap=plt.cm.Wistia_r,\n                                markers=[\',\', \'o\'], alphas=[0.9, 0.6], colors=[\'r\', \'y\'])\n\n\n# # Predicting Wine Quality\n\n# ## Prepare Training and Testing datasets\n\n# In[17]:\n\nwqp_features = wines.iloc[:,:-3]\nwqp_class_labels = np.array(wines[\'quality_label\'])\nwqp_label_names = [\'low\', \'medium\', \'high\']\nwqp_feature_names = list(wqp_features.columns)\nwqp_train_X, wqp_test_X, wqp_train_y, wqp_test_y = train_test_split(wqp_features, wqp_class_labels, \n                                                                    test_size=0.3, random_state=42)\n\nprint(Counter(wqp_train_y), Counter(wqp_test_y))\nprint(\'Features:\', wqp_feature_names)\n\n\n# ## Feature Scaling\n\n# In[18]:\n\n# Define the scaler \nwqp_ss = StandardScaler().fit(wqp_train_X)\n\n# Scale the train set\nwqp_train_SX = wqp_ss.transform(wqp_train_X)\n\n# Scale the test set\nwqp_test_SX = wqp_ss.transform(wqp_test_X)\n\n\n# ## Train, Predict & Evaluate Model using Decision Tree \n\n# In[19]:\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nwqp_dt = DecisionTreeClassifier()\nwqp_dt.fit(wqp_train_SX, wqp_train_y)\n\nwqp_dt_predictions = wqp_dt.predict(wqp_test_SX)\n\nmeu.display_model_performance_metrics(true_labels=wqp_test_y, predicted_labels=wqp_dt_predictions, \n                                      classes=wqp_label_names)\n\n\n# ## View Feature Importances from Decision Tree Model\n\n# In[20]:\n\nwqp_dt_feature_importances = wqp_dt.feature_importances_\nwqp_dt_feature_names, wqp_dt_feature_scores = zip(*sorted(zip(wqp_feature_names, wqp_dt_feature_importances), \n                                                          key=lambda x: x[1]))\ny_position = list(range(len(wqp_dt_feature_names)))\nplt.barh(y_position, wqp_dt_feature_scores, height=0.6, align=\'center\')\nplt.yticks(y_position , wqp_dt_feature_names)\nplt.xlabel(\'Relative Importance Score\')\nplt.ylabel(\'Feature\')\nt = plt.title(\'Feature Importances for Decision Tree\')\n\n\n# ## Visualize the Decision Tree\n\n# In[21]:\n\nfrom graphviz import Source\nfrom sklearn import tree\nfrom IPython.display import Image\n\ngraph = Source(tree.export_graphviz(wqp_dt, out_file=None, class_names=wqp_label_names,\n                                    filled=True, rounded=True, special_characters=False,\n                                    feature_names=wqp_feature_names, max_depth=3))\npng_data = graph.pipe(format=\'png\')\nwith open(\'dtree_structure.png\',\'wb\') as f:\n    f.write(png_data)\n\nImage(png_data)\n\n\n# ## Train, Predict & Evaluate Model using Random Forests\n\n# In[22]:\n\nfrom sklearn.ensemble import RandomForestClassifier\n# train the model\nwqp_rf = RandomForestClassifier()\nwqp_rf.fit(wqp_train_SX, wqp_train_y)\n# predict and evaluate performance\nwqp_rf_predictions = wqp_rf.predict(wqp_test_SX)\nmeu.display_model_performance_metrics(true_labels=wqp_test_y, predicted_labels=wqp_rf_predictions, \n                                      classes=wqp_label_names)\n\n\n# ## Hyperparameter tuning with Grid Search & Cross Validation\n\n# In[23]:\n\nprint(wqp_rf.get_params())\n\n\n# ### Get the best hyperparameter values\n\n# In[24]:\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n                \'n_estimators\': [100, 200, 300, 500], \n                \'max_features\': [\'auto\', None, \'log2\']    \n              }\n\nwqp_clf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5,\n                       scoring=\'accuracy\')\nwqp_clf.fit(wqp_train_SX, wqp_train_y)\nprint(wqp_clf.best_params_)\n\n\n# ### View grid search results\n\n# In[25]:\n\nresults = wqp_clf.cv_results_\nfor param, score_mean, score_sd in zip(results[\'params\'], results[\'mean_test_score\'], results[\'std_test_score\']):\n    print(param, round(score_mean, 4), round(score_sd, 4))\n\n\n# ### Train, Predict & Evaluate Random Forest Model with tuned hyperparameters\n\n# In[26]:\n\nwqp_rf = RandomForestClassifier(n_estimators=200, max_features=\'auto\', random_state=42)\nwqp_rf.fit(wqp_train_SX, wqp_train_y)\n\nwqp_rf_predictions = wqp_rf.predict(wqp_test_SX)\nmeu.display_model_performance_metrics(true_labels=wqp_test_y, predicted_labels=wqp_rf_predictions, \n                                      classes=wqp_label_names)\n\n\n# ## Train, Predict & Evaluate Model using Extreme Gradient Boosting\n\n# ### Load and set dependencies\n\n# In[27]:\n\nimport os\n\nmingw_path = r\'C:\\mingw-w64\\mingw64\\bin\'\nos.environ[\'PATH\'] = mingw_path + \';\' + os.environ[\'PATH\']\n\nimport xgboost as xgb\n\n\n# ### Train the model\n\n# In[28]:\n\nwqp_xgb_model = xgb.XGBClassifier(seed=42)\nwqp_xgb_model.fit(wqp_train_SX, wqp_train_y)\n\n\n# ### Predict and Evaluate Model \n\n# In[29]:\n\nwqp_xgb_predictions = wqp_xgb_model.predict(wqp_test_SX)\nmeu.display_model_performance_metrics(true_labels=wqp_test_y, predicted_labels=wqp_xgb_predictions, \n                                      classes=wqp_label_names)\n\n\n# ### Get the best hyperparameter values\n\n# In[30]:\n\nparam_grid = {\n                \'n_estimators\': [100, 200, 300], \n                \'max_depth\': [5, 10, 15],\n                \'learning_rate\': [0.3, 0.5]\n              }\n\nwqp_clf = GridSearchCV(xgb.XGBClassifier(tree_method=\'exact\', seed=42), param_grid, \n                       cv=5, scoring=\'accuracy\')\nwqp_clf.fit(wqp_train_SX, wqp_train_y)\nprint(wqp_clf.best_params_)\n\n\n# ### View grid search results\n\n# In[31]:\n\nresults = wqp_clf.cv_results_\nfor param, score_mean, score_sd in zip(results[\'params\'], results[\'mean_test_score\'], results[\'std_test_score\']):\n    print(param, round(score_mean, 4), round(score_sd, 4))\n\n\n# ### Train, Predict & Evaluate Extreme Gradient Boosted Model with tuned hyperparameters\n\n# In[32]:\n\nwqp_xgb_model = xgb.XGBClassifier(seed=42, max_depth=10, learning_rate=0.3, n_estimators=100)\nwqp_xgb_model.fit(wqp_train_SX, wqp_train_y)\n\nwqp_xgb_predictions = wqp_xgb_model.predict(wqp_test_SX)\nmeu.display_model_performance_metrics(true_labels=wqp_test_y, predicted_labels=wqp_xgb_predictions, \n                                      classes=wqp_label_names)\n\n\n# # Model Interpretation\n\n# ## Comparative analysis of Model Feature importances\n\n# In[33]:\n\nfrom skater.core.explanations import Interpretation\nfrom skater.model import InMemoryModel\n# leveraging skater for feature importances\ninterpreter = Interpretation(wqp_test_SX, feature_names=wqp_feature_names)\nwqp_im_model = InMemoryModel(wqp_rf.predict_proba, examples=wqp_train_SX, target_names=wqp_rf.classes_)\n# retrieving feature importances from the scikit-learn estimator\nwqp_rf_feature_importances = wqp_rf.feature_importances_\nwqp_rf_feature_names, wqp_rf_feature_scores = zip(*sorted(zip(wqp_feature_names, wqp_rf_feature_importances), \n                                                          key=lambda x: x[1]))\n# plot the feature importance plots\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\nt = f.suptitle(\'Feature Importances for Random Forest\', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.6)\ny_position = list(range(len(wqp_rf_feature_names)))\nax1.barh(y_position, wqp_rf_feature_scores, height=0.6, align=\'center\', tick_label=wqp_rf_feature_names)\nax1.set_title(""Scikit-Learn"")\nax1.set_xlabel(\'Relative Importance Score\')\nax1.set_ylabel(\'Feature\')\nplots = interpreter.feature_importance.plot_feature_importance(wqp_im_model, ascending=False, ax=ax2)\nax2.set_title(""Skater"")\nax2.set_xlabel(\'Relative Importance Score\')\nax2.set_ylabel(\'Feature\')\n\n\n# ## View Model ROC Curve\n\n# In[34]:\n\nmeu.plot_model_roc_curve(wqp_rf, wqp_test_SX, wqp_test_y)\n\n\n# ## Visualize Model decision surface\n\n# In[35]:\n\nfeature_indices = [i for i, feature in enumerate(wqp_feature_names) \n                       if feature in [\'alcohol\', \'volatile acidity\']]\nmeu.plot_model_decision_surface(clf=wqp_rf, train_features=wqp_train_SX[:, feature_indices], \n                      train_labels=wqp_train_y, plot_step=0.02, cmap=plt.cm.RdYlBu,\n                      markers=[\',\', \'d\', \'+\'], alphas=[1.0, 0.8, 0.5], colors=[\'r\', \'b\', \'y\'])\n\n\n# ## Interpreting Model Predictions\n\n# In[36]:\n\nfrom skater.core.local_interpretation.lime.lime_tabular import LimeTabularExplainer\n\nexp = LimeTabularExplainer(wqp_train_SX, feature_names=wqp_feature_names, \n                           discretize_continuous=True, \n                           class_names=wqp_rf.classes_)\n\n\n# In[80]:\n\nexp.explain_instance(wqp_test_SX[10], wqp_rf.predict_proba, top_labels=1).show_in_notebook() \n\n\n# In[81]:\n\nexp.explain_instance(wqp_test_SX[747], wqp_rf.predict_proba, top_labels=1).show_in_notebook() \n\n\n# ## Visualizing partial dependencies\n\n# In[39]:\n\naxes_list = interpreter.partial_dependence.plot_partial_dependence([\'alcohol\'], wqp_im_model, \n                                                                   grid_resolution=100, \n                                                                   with_variance=True,\n                                                                   figsize = (6, 4))\naxs = axes_list[0][3:]\n[ax.set_ylim(0, 1) for ax in axs];\n\n\n# In[40]:\n\nplots_list = interpreter.partial_dependence.plot_partial_dependence([(\'alcohol\', \'volatile acidity\')], \n                                                                    wqp_im_model, n_samples=1000, figsize=(12, 5),\n                                                                    grid_resolution=100)\naxs = plots_list[0][3:]\n[ax.set_zlim(0, 1) for ax in axs];\n\n'"
notebooks/Ch10_Analyzing_Music_Trends_and_Recommendations/Recommenders.py,0,"b'# Thanks to Siraj Raval for this module\n# Refer to https://github.com/llSourcell/recommender_live for more details\n\nimport numpy as np\nimport pandas\n\n#Class for Popularity based Recommender System model\nclass popularity_recommender_py():\n    def __init__(self):\n        self.train_data = None\n        self.user_id = None\n        self.item_id = None\n        self.popularity_recommendations = None\n        \n    #Create the popularity based recommender system model\n    def create(self, train_data, user_id, item_id):\n        self.train_data = train_data\n        self.user_id = user_id\n        self.item_id = item_id\n\n        #Get a count of user_ids for each unique song as recommendation score\n        train_data_grouped = train_data.groupby([self.item_id]).agg({self.user_id: \'count\'}).reset_index()\n        train_data_grouped.rename(columns = {user_id: \'score\'},inplace=True)\n    \n        #Sort the songs based upon recommendation score\n        train_data_sort = train_data_grouped.sort_values([\'score\', self.item_id], ascending = [0,1])\n    \n        #Generate a recommendation rank based upon score\n        train_data_sort[\'Rank\'] = train_data_sort[\'score\'].rank(ascending=0, method=\'first\')\n        \n        #Get the top 10 recommendations\n        self.popularity_recommendations = train_data_sort.head(10)\n\n    #Use the popularity based recommender system model to\n    #make recommendations\n    def recommend(self, user_id):    \n        user_recommendations = self.popularity_recommendations\n        \n        #Add user_id column for which the recommendations are being generated\n        user_recommendations[\'user_id\'] = user_id\n    \n        #Bring user_id column to the front\n        cols = user_recommendations.columns.tolist()\n        cols = cols[-1:] + cols[:-1]\n        user_recommendations = user_recommendations[cols]\n        \n        return user_recommendations\n    \n\n#Class for Item similarity based Recommender System model\nclass item_similarity_recommender_py():\n    def __init__(self):\n        self.train_data = None\n        self.user_id = None\n        self.item_id = None\n        self.cooccurence_matrix = None\n        self.songs_dict = None\n        self.rev_songs_dict = None\n        self.item_similarity_recommendations = None\n        \n    #Get unique items (songs) corresponding to a given user\n    def get_user_items(self, user):\n        user_data = self.train_data[self.train_data[self.user_id] == user]\n        user_items = list(user_data[self.item_id].unique())\n        \n        return user_items\n        \n    #Get unique users for a given item (song)\n    def get_item_users(self, item):\n        item_data = self.train_data[self.train_data[self.item_id] == item]\n        item_users = set(item_data[self.user_id].unique())\n            \n        return item_users\n        \n    #Get unique items (songs) in the training data\n    def get_all_items_train_data(self):\n        all_items = list(self.train_data[self.item_id].unique())\n            \n        return all_items\n        \n    #Construct cooccurence matrix\n    def construct_cooccurence_matrix(self, user_songs, all_songs):\n            \n        ####################################\n        #Get users for all songs in user_songs.\n        ####################################\n        user_songs_users = []        \n        for i in range(0, len(user_songs)):\n            user_songs_users.append(self.get_item_users(user_songs[i]))\n            \n        ###############################################\n        #Initialize the item cooccurence matrix of size \n        #len(user_songs) X len(songs)\n        ###############################################\n        cooccurence_matrix = np.matrix(np.zeros(shape=(len(user_songs), len(all_songs))), float)\n           \n        #############################################################\n        #Calculate similarity between user songs and all unique songs\n        #in the training data\n        #############################################################\n        for i in range(0,len(all_songs)):\n            #Calculate unique listeners (users) of song (item) i\n            songs_i_data = self.train_data[self.train_data[self.item_id] == all_songs[i]]\n            users_i = set(songs_i_data[self.user_id].unique())\n            \n            for j in range(0,len(user_songs)):       \n                    \n                #Get unique listeners (users) of song (item) j\n                users_j = user_songs_users[j]\n                    \n                #Calculate intersection of listeners of songs i and j\n                users_intersection = users_i.intersection(users_j)\n                \n                #Calculate cooccurence_matrix[i,j] as Jaccard Index\n                if len(users_intersection) != 0:\n                    #Calculate union of listeners of songs i and j\n                    users_union = users_i.union(users_j)\n                    \n                    cooccurence_matrix[j,i] = float(len(users_intersection))/float(len(users_union))\n                else:\n                    cooccurence_matrix[j,i] = 0\n                    \n        \n        return cooccurence_matrix\n\n    \n    #Use the cooccurence matrix to make top recommendations\n    def generate_top_recommendations(self, user, cooccurence_matrix, all_songs, user_songs):\n        print(""Non zero values in cooccurence_matrix :%d"" % np.count_nonzero(cooccurence_matrix))\n        \n        #Calculate a weighted average of the scores in cooccurence matrix for all user songs.\n        user_sim_scores = cooccurence_matrix.sum(axis=0)/float(cooccurence_matrix.shape[0])\n        user_sim_scores = np.array(user_sim_scores)[0].tolist()\n \n        #Sort the indices of user_sim_scores based upon their value\n        #Also maintain the corresponding score\n        sort_index = sorted(((e,i) for i,e in enumerate(list(user_sim_scores))), reverse=True)\n    \n        #Create a dataframe from the following\n        columns = [\'user_id\', \'song\', \'score\', \'rank\']\n        #index = np.arange(1) # array of numbers for the number of samples\n        df = pandas.DataFrame(columns=columns)\n         \n        #Fill the dataframe with top 10 item based recommendations\n        rank = 1 \n        for i in range(0,len(sort_index)):\n            if ~np.isnan(sort_index[i][0]) and all_songs[sort_index[i][1]] not in user_songs and rank <= 10:\n                df.loc[len(df)]=[user,all_songs[sort_index[i][1]],sort_index[i][0],rank]\n                rank = rank+1\n        \n        #Handle the case where there are no recommendations\n        if df.shape[0] == 0:\n            print(""The current user has no songs for training the item similarity based recommendation model."")\n            return -1\n        else:\n            return df\n \n    #Create the item similarity based recommender system model\n    def create(self, train_data, user_id, item_id):\n        self.train_data = train_data\n        self.user_id = user_id\n        self.item_id = item_id\n\n    #Use the item similarity based recommender system model to\n    #make recommendations\n    def recommend(self, user):\n        \n        ########################################\n        #A. Get all unique songs for this user\n        ########################################\n        user_songs = self.get_user_items(user)    \n            \n        print(""No. of unique songs for the user: %d"" % len(user_songs))\n        \n        ######################################################\n        #B. Get all unique items (songs) in the training data\n        ######################################################\n        all_songs = self.get_all_items_train_data()\n        \n        print(""no. of unique songs in the training set: %d"" % len(all_songs))\n         \n        ###############################################\n        #C. Construct item cooccurence matrix of size \n        #len(user_songs) X len(songs)\n        ###############################################\n        cooccurence_matrix = self.construct_cooccurence_matrix(user_songs, all_songs)\n        \n        #######################################################\n        #D. Use the cooccurence matrix to make recommendations\n        #######################################################\n        df_recommendations = self.generate_top_recommendations(user, cooccurence_matrix, all_songs, user_songs)\n                \n        return df_recommendations\n    \n    #Get similar items to given items\n    def get_similar_items(self, item_list):\n        \n        user_songs = item_list\n        \n        ######################################################\n        #B. Get all unique items (songs) in the training data\n        ######################################################\n        all_songs = self.get_all_items_train_data()\n        \n        print(""no. of unique songs in the training set: %d"" % len(all_songs))\n         \n        ###############################################\n        #C. Construct item cooccurence matrix of size \n        #len(user_songs) X len(songs)\n        ###############################################\n        cooccurence_matrix = self.construct_cooccurence_matrix(user_songs, all_songs)\n        \n        #######################################################\n        #D. Use the cooccurence matrix to make recommendations\n        #######################################################\n        user = """"\n        df_recommendations = self.generate_top_recommendations(user, cooccurence_matrix, all_songs, user_songs)\n         \n        return df_recommendations'"
notebooks/Ch10_Analyzing_Music_Trends_and_Recommendations/recommendation_engines.py,0,"b'\n# coding: utf-8\n\n# # Load Necessary Dependencies\n\n# In[1]:\n\n\nimport pandas as pd\nimport numpy as np\nimport time\nimport sqlite3\n\ndata_home = \'./\'\n\n\n# # Load and Process the Datasets\n\n# ### Get more information about the Millionsong project from https://labrosa.ee.columbia.edu/millionsong/\n# \n# #### Refer to Chapter 10: Section \'The Million Song Dataset Taste Profile\' for more details\n\n# ## Load Triplets data  [user, song, play_count]\n\n# #### Get the data from http://labrosa.ee.columbia.edu/millionsong/sites/default/files/challenge/train_triplets.txt.zip\n\n# In[2]:\n\n\ntriplet_dataset = pd.read_csv(filepath_or_buffer=data_home+\'train_triplets.txt\', \n                              nrows=10000,sep=\'\\t\', header=None, \n                              names=[\'user\',\'song\',\'play_count\'])\n\n\n# In[3]:\n\n\ntriplet_dataset.head(n=10)\n\n\n# ## Get User and total play counts\n\n# In[5]:\n\n\noutput_dict = {}\nwith open(data_home+\'train_triplets.txt\') as f:\n    for line_number, line in enumerate(f):\n        user = line.split(\'\\t\')[0]\n        play_count = int(line.split(\'\\t\')[2])\n        if user in output_dict:\n            play_count +=output_dict[user]\n            output_dict.update({user:play_count})\n        output_dict.update({user:play_count})\noutput_list = [{\'user\':k,\'play_count\':v} for k,v in output_dict.items()]\nplay_count_df = pd.DataFrame(output_list)\nplay_count_df = play_count_df.sort_values(by = \'play_count\', ascending = False)\n\n\n# In[ ]:\n\n\nplay_count_df.to_csv(path_or_buf=\'user_playcount_df.csv\', index = False)\n\n\n# ## Get Song and total play counts\n\n# In[7]:\n\n\noutput_dict = {}\nwith open(data_home+\'train_triplets.txt\') as f:\n    for line_number, line in enumerate(f):\n        song = line.split(\'\\t\')[1]\n        play_count = int(line.split(\'\\t\')[2])\n        if song in output_dict:\n            play_count +=output_dict[song]\n            output_dict.update({song:play_count})\n        output_dict.update({song:play_count})\noutput_list = [{\'song\':k,\'play_count\':v} for k,v in output_dict.items()]\nsong_count_df = pd.DataFrame(output_list)\nsong_count_df = song_count_df.sort_values(by = \'play_count\', ascending = False)\n\n\n# In[ ]:\n\n\nsong_count_df.to_csv(path_or_buf=\'song_playcount_df.csv\', index = False)\n\n\n# ## View top users and songs\n\n# In[14]:\n\n\nplay_count_df = pd.read_csv(filepath_or_buffer=\'user_playcount_df.csv\')\nplay_count_df.head(n =10)\n\n\n# In[15]:\n\n\nsong_count_df = pd.read_csv(filepath_or_buffer=\'song_playcount_df.csv\')\nsong_count_df.head(10)\n\n\n# ## Subsetting the data\n\n# In[15]:\n\n\ntotal_play_count = sum(song_count_df.play_count)\n(float(play_count_df.head(n=100000).play_count.sum())/total_play_count)*100\nplay_count_subset = play_count_df.head(n=100000)\n\n\n# In[17]:\n\n\n(float(song_count_df.head(n=30000).play_count.sum())/total_play_count)*100\n\n\n# In[18]:\n\n\nsong_count_subset = song_count_df.head(n=30000)\n\n\n# In[19]:\n\n\nuser_subset = list(play_count_subset.user)\nsong_subset = list(song_count_subset.song)\n\n\n# In[20]:\n\n\ntriplet_dataset = pd.read_csv(filepath_or_buffer=data_home+\'train_triplets.txt\',sep=\'\\t\', \n                              header=None, names=[\'user\',\'song\',\'play_count\'])\ntriplet_dataset_sub = triplet_dataset[triplet_dataset.user.isin(user_subset) ]\ndel(triplet_dataset)\ntriplet_dataset_sub_song = triplet_dataset_sub[triplet_dataset_sub.song.isin(song_subset)]\ndel(triplet_dataset_sub)\n\n\n# In[ ]:\n\n\ntriplet_dataset_sub_song.to_csv(path_or_buf=data_home+\'triplet_dataset_sub_song.csv\', index=False)\n\n\n# In[25]:\n\n\ntriplet_dataset_sub_song.shape\n\n\n# In[29]:\n\n\ntriplet_dataset_sub_song.head(n=10)\n\n\n# ## Adding songs metadata from million songs dataset\n\n# #### Get the data from http://labrosa.ee.columbia.edu/millionsong/sites/default/files/AdditionalFiles/track_metadata.db\n\n# In[45]:\n\n\nconn = sqlite3.connect(data_home+\'track_metadata.db\')\ncur = conn.cursor()\ncur.execute(""SELECT name FROM sqlite_master WHERE type=\'table\'"")\ncur.fetchall()\n\n\n# In[49]:\n\n\ntrack_metadata_df.head()\n\n\n# In[47]:\n\n\ntrack_metadata_df = pd.read_sql(con=conn, sql=\'select * from songs\')\ntrack_metadata_df_sub = track_metadata_df[track_metadata_df.song_id.isin(song_subset)]\n\n\n# In[50]:\n\n\ntrack_metadata_df_sub.to_csv(path_or_buf=data_home+\'track_metadata_df_sub.csv\', index=False)\n\n\n# In[51]:\n\n\ntrack_metadata_df_sub.shape\n\n\n# ## Load up the saved data subsets\n\n# In[2]:\n\n\ntriplet_dataset_sub_song = pd.read_csv(filepath_or_buffer=data_home+\'train_triplets_sub_song.csv\')\ntrack_metadata_df_sub = pd.read_csv(filepath_or_buffer=data_home+\'track_metadata_df_sub.csv\')\n\n\n# ## Clean up datasets\n\n# In[3]:\n\n\ndel(track_metadata_df_sub[\'track_id\'])\ndel(track_metadata_df_sub[\'artist_mbid\'])\ntrack_metadata_df_sub = track_metadata_df_sub.drop_duplicates([\'song_id\'])\ntriplet_dataset_sub_song_merged = pd.merge(triplet_dataset_sub_song, track_metadata_df_sub, how=\'left\', left_on=\'song\', right_on=\'song_id\')\ntriplet_dataset_sub_song_merged.rename(columns={\'play_count\':\'listen_count\'},inplace=True)\n\n\n# In[4]:\n\n\ndel(triplet_dataset_sub_song_merged[\'song_id\'])\ndel(triplet_dataset_sub_song_merged[\'artist_id\'])\ndel(triplet_dataset_sub_song_merged[\'duration\'])\ndel(triplet_dataset_sub_song_merged[\'artist_familiarity\'])\ndel(triplet_dataset_sub_song_merged[\'artist_hotttnesss\'])\ndel(triplet_dataset_sub_song_merged[\'track_7digitalid\'])\ndel(triplet_dataset_sub_song_merged[\'shs_perf\'])\ndel(triplet_dataset_sub_song_merged[\'shs_work\'])\n\n\n# In[5]:\n\n\ntriplet_dataset_sub_song_merged.head(n=10)\n\n\n# # Some visualizations\n\n# ## Most popular songs\n\n# In[185]:\n\n\npopular_songs = triplet_dataset_sub_song_merged[[\'title\',\'listen_count\']].groupby(\'title\').sum().reset_index()\npopular_songs_top_20 = popular_songs.sort_values(\'listen_count\', ascending=False).head(n=20)\nimport matplotlib.pyplot as plt; plt.rcdefaults()\nimport numpy as np\nimport matplotlib.pyplot as plt\n \nobjects = (list(popular_songs_top_20[\'title\']))\ny_pos = np.arange(len(objects))\nperformance = list(popular_songs_top_20[\'listen_count\'])\n \nplt.bar(y_pos, performance, align=\'center\', alpha=0.5)\nplt.xticks(y_pos, objects, rotation=\'vertical\')\nplt.ylabel(\'Item count\')\nplt.title(\'Most popular songs\')\n \nplt.show()\n\n\n# ## Most popular releases\n\n# In[58]:\n\n\npopular_release = triplet_dataset_sub_song_merged[[\'release\',\'listen_count\']].groupby(\'release\').sum().reset_index()\npopular_release_top_20 = popular_release.sort_values(\'listen_count\', ascending=False).head(n=20)\n\nobjects = (list(popular_release_top_20[\'release\']))\ny_pos = np.arange(len(objects))\nperformance = list(popular_release_top_20[\'listen_count\'])\n \nplt.bar(y_pos, performance, align=\'center\', alpha=0.5)\nplt.xticks(y_pos, objects, rotation=\'vertical\')\nplt.ylabel(\'Item count\')\nplt.title(\'Most popular Release\')\n \nplt.show()\n\n\n# ## Most popular artists\n\n# In[62]:\n\n\npopular_artist = triplet_dataset_sub_song_merged[[\'artist_name\',\'listen_count\']].groupby(\'artist_name\').sum().reset_index()\npopular_artist_top_20 = popular_artist.sort_values(\'listen_count\', ascending=False).head(n=20)\n\nobjects = (list(popular_artist_top_20[\'artist_name\']))\ny_pos = np.arange(len(objects))\nperformance = list(popular_artist_top_20[\'listen_count\'])\n \nplt.bar(y_pos, performance, align=\'center\', alpha=0.5)\nplt.xticks(y_pos, objects, rotation=\'vertical\')\nplt.ylabel(\'Item count\')\nplt.title(\'Most popular Artists\')\n \nplt.show()\n\n\n# ## Song count distribution\n\n# In[64]:\n\n\nuser_song_count_distribution = triplet_dataset_sub_song_merged[[\'user\',\'title\']].groupby(\'user\').count().reset_index().sort_values(\nby=\'title\',ascending = False)\nuser_song_count_distribution.title.describe()\n\n\n# In[71]:\n\n\nx = user_song_count_distribution.title\nn, bins, patches = plt.hist(x, 50, facecolor=\'green\', alpha=0.75)\nplt.xlabel(\'Play Counts\')\nplt.ylabel(\'Num of Users\')\nplt.title(r\'$\\mathrm{Histogram\\ of\\ User\\ Play\\ Count\\ Distribution}\\ $\')\nplt.grid(True)\nplt.show()\n\n\n# # Recommendation Engines\n\n# In[1]:\n\n\nimport Recommenders as Recommenders\nfrom sklearn.model_selection import train_test_split\n\n\n# ## Popularity based recommendations\n\n# In[9]:\n\n\ntriplet_dataset_sub_song_merged_set = triplet_dataset_sub_song_merged\ntrain_data, test_data = train_test_split(triplet_dataset_sub_song_merged_set, test_size = 0.40, random_state=0)\n\n\n# In[10]:\n\n\ntrain_data.head()\n\n\n# In[11]:\n\n\ndef create_popularity_recommendation(train_data, user_id, item_id):\n    #Get a count of user_ids for each unique song as recommendation score\n    train_data_grouped = train_data.groupby([item_id]).agg({user_id: \'count\'}).reset_index()\n    train_data_grouped.rename(columns = {user_id: \'score\'},inplace=True)\n    \n    #Sort the songs based upon recommendation score\n    train_data_sort = train_data_grouped.sort_values([\'score\', item_id], ascending = [0,1])\n    \n    #Generate a recommendation rank based upon score\n    train_data_sort[\'Rank\'] = train_data_sort[\'score\'].rank(ascending=0, method=\'first\')\n        \n    #Get the top 10 recommendations\n    popularity_recommendations = train_data_sort.head(20)\n    return popularity_recommendations\n\n\n# In[82]:\n\n\nrecommendations = create_popularity_recommendation(triplet_dataset_sub_song_merged,\'user\',\'title\')\n\n\n# In[83]:\n\n\nrecommendations\n\n\n# ## Item similarity  based recommendations\n\n# In[84]:\n\n\nsong_count_subset = song_count_df.head(n=5000)\nuser_subset = list(play_count_subset.user)\nsong_subset = list(song_count_subset.song)\ntriplet_dataset_sub_song_merged_sub = triplet_dataset_sub_song_merged[triplet_dataset_sub_song_merged.song.isin(song_subset)]\n\n\n# In[36]:\n\n\ntriplet_dataset_sub_song_merged_sub.head()\n\n\n# In[85]:\n\n\ntrain_data, test_data = train_test_split(triplet_dataset_sub_song_merged_sub, test_size = 0.30, random_state=0)\nis_model = Recommenders.item_similarity_recommender_py()\nis_model.create(train_data, \'user\', \'title\')\nuser_id = list(train_data.user)[7]\nuser_items = is_model.get_user_items(user_id)\n\n\n# In[35]:\n\n\n#Recommend songs for the user using personalized model\nis_model.recommend(user_id)\n\n\n# ## Matrix factorization  based recommendations\n\n# In[5]:\n\n\ntriplet_dataset_sub_song_merged_sum_df = triplet_dataset_sub_song_merged[[\'user\',\'listen_count\']].groupby(\'user\').sum().reset_index()\ntriplet_dataset_sub_song_merged_sum_df.rename(columns={\'listen_count\':\'total_listen_count\'},inplace=True)\ntriplet_dataset_sub_song_merged = pd.merge(triplet_dataset_sub_song_merged,triplet_dataset_sub_song_merged_sum_df)\ntriplet_dataset_sub_song_merged[\'fractional_play_count\'] = triplet_dataset_sub_song_merged[\'listen_count\']/triplet_dataset_sub_song_merged[\'total_listen_count\']\n\n\n# In[6]:\n\n\ntriplet_dataset_sub_song_merged[triplet_dataset_sub_song_merged.user ==\'d6589314c0a9bcbca4fee0c93b14bc402363afea\'][[\'user\',\'song\',\'listen_count\',\'fractional_play_count\']].head()\n\n\n# In[7]:\n\n\nfrom scipy.sparse import coo_matrix\n\nsmall_set = triplet_dataset_sub_song_merged\nuser_codes = small_set.user.drop_duplicates().reset_index()\nsong_codes = small_set.song.drop_duplicates().reset_index()\nuser_codes.rename(columns={\'index\':\'user_index\'}, inplace=True)\nsong_codes.rename(columns={\'index\':\'song_index\'}, inplace=True)\nsong_codes[\'so_index_value\'] = list(song_codes.index)\nuser_codes[\'us_index_value\'] = list(user_codes.index)\nsmall_set = pd.merge(small_set,song_codes,how=\'left\')\nsmall_set = pd.merge(small_set,user_codes,how=\'left\')\nmat_candidate = small_set[[\'us_index_value\',\'so_index_value\',\'fractional_play_count\']]\ndata_array = mat_candidate.fractional_play_count.values\nrow_array = mat_candidate.us_index_value.values\ncol_array = mat_candidate.so_index_value.values\n\ndata_sparse = coo_matrix((data_array, (row_array, col_array)),dtype=float)\n\n\n# In[8]:\n\n\ndata_sparse\n\n\n# In[9]:\n\n\nuser_codes[user_codes.user ==\'2a2f776cbac6df64d6cb505e7e834e01684673b6\']\n\n\n# In[10]:\n\n\nimport math as mt\nfrom scipy.sparse.linalg import * #used for matrix multiplication\nfrom scipy.sparse.linalg import svds\nfrom scipy.sparse import csc_matrix\n\n\n# In[17]:\n\n\ndef compute_svd(urm, K):\n    U, s, Vt = svds(urm, K)\n\n    dim = (len(s), len(s))\n    S = np.zeros(dim, dtype=np.float32)\n    for i in range(0, len(s)):\n        S[i,i] = mt.sqrt(s[i])\n\n    U = csc_matrix(U, dtype=np.float32)\n    S = csc_matrix(S, dtype=np.float32)\n    Vt = csc_matrix(Vt, dtype=np.float32)\n    \n    return U, S, Vt\n\ndef compute_estimated_matrix(urm, U, S, Vt, uTest, K, test):\n    rightTerm = S*Vt \n    max_recommendation = 250\n    estimatedRatings = np.zeros(shape=(MAX_UID, MAX_PID), dtype=np.float16)\n    recomendRatings = np.zeros(shape=(MAX_UID,max_recommendation ), dtype=np.float16)\n    for userTest in uTest:\n        prod = U[userTest, :]*rightTerm\n        estimatedRatings[userTest, :] = prod.todense()\n        recomendRatings[userTest, :] = (-estimatedRatings[userTest, :]).argsort()[:max_recommendation]\n    return recomendRatings\n\n\n# In[18]:\n\n\nK=50\nurm = data_sparse\nMAX_PID = urm.shape[1]\nMAX_UID = urm.shape[0]\n\nU, S, Vt = compute_svd(urm, K)\n\n\n# In[19]:\n\n\nuTest = [4,5,6,7,8,873,23]\n\nuTest_recommended_items = compute_estimated_matrix(urm, U, S, Vt, uTest, K, True)\n\n\n# In[20]:\n\n\nfor user in uTest:\n    print(""Recommendation for user with user id {}"". format(user))\n    rank_value = 1\n    for i in uTest_recommended_items[user,0:10]:\n        song_details = small_set[small_set.so_index_value == i].drop_duplicates(\'so_index_value\')[[\'title\',\'artist_name\']]\n        print(""The number {} recommended song is {} BY {}"".format(rank_value, list(song_details[\'title\'])[0],list(song_details[\'artist_name\'])[0]))\n        rank_value+=1\n\n\n# In[15]:\n\n\nuTest = [27513]\n#Get estimated rating for test user\nprint(""Predictied ratings:"")\nuTest_recommended_items = compute_estimated_matrix(urm, U, S, Vt, uTest, K, True)\n\n\n# In[16]:\n\n\nfor user in uTest:\n    print(""Recommendation for user with user id {}"". format(user))\n    rank_value = 1\n    for i in uTest_recommended_items[user,0:10]:\n        song_details = small_set[small_set.so_index_value == i].drop_duplicates(\'so_index_value\')[[\'title\',\'artist_name\']]\n        print(""The number {} recommended song is {} BY {}"".format(rank_value, list(song_details[\'title\'])[0],list(song_details[\'artist_name\'])[0]))\n        rank_value+=1\n\n'"
notebooks/Ch11_Forecasting_Stock_and_Commodity_Prices/__init__.py,0,b'{\\rtf1}'
notebooks/Ch11_Forecasting_Stock_and_Commodity_Prices/arima_utils.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sat Sep  2 19:21:24 2017\n\n@author: RAGHAV\n""""""\n\nimport itertools\nimport numpy as np\nimport pandas as pd\n\n\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nfrom sklearn.model_selection import TimeSeriesSplit\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\nsns.set_style(\'whitegrid\')\nsns.set_context(\'talk\')\n\n\n\n# Dickey Fuller test for Stationarity\ndef ad_fuller_test(ts):\n    dftest = adfuller(ts, autolag=\'AIC\')\n    dfoutput = pd.Series(dftest[0:4], index=[\'Test Statistic\',\n                                             \'p-value\',\n                                             \'#Lags Used\',\n                                             \'Number of Observations Used\'])\n    for key,value in dftest[4].items():\n        dfoutput[\'Critical Value (%s)\'%key] = value\n    print(dfoutput)\n\n# Plot rolling stats for a time series\ndef plot_rolling_stats(ts):\n    rolling_mean = ts.rolling(window=12,center=False).mean()\n    rolling_std = ts.rolling(window=12,center=False).std()\n\n    #Plot rolling statistics:\n    orig = plt.plot(ts, color=\'blue\',label=\'Original\')\n    mean = plt.plot(rolling_mean, color=\'red\', label=\'Rolling Mean\')\n    std = plt.plot(rolling_std, color=\'black\', label = \'Rolling Std\')\n    plt.legend(loc=\'best\')\n    plt.title(\'Rolling Mean & Standard Deviation\')\n    plt.show(block=False)\n    \n    \ndef plot_acf_pacf(series):\n    fig = plt.figure(figsize=(12,8))\n    ax1 = fig.add_subplot(211)\n    fig = sm.graphics.tsa.plot_acf(series.squeeze(), lags=40, ax=ax1)\n    ax2 = fig.add_subplot(212)\n    fig = sm.graphics.tsa.plot_pacf(series, lags=40, ax=ax2)    \n    \n\ndef auto_arima(param_max=1,series=pd.Series(),verbose=True):\n    # Define the p, d and q parameters to take any value \n    # between 0 and param_max\n    p = d = q = range(0, param_max+1)\n\n    # Generate all different combinations of seasonal p, d and q triplets\n    pdq = [(x[0], x[1], x[2]) for x in list(itertools.product(p, d, q))]\n    \n    model_resuls = []\n    best_model = {}\n    min_aic = 10000000\n    for param in pdq:\n        try:\n            mod = sm.tsa.ARIMA(series, order=param)\n\n            results = mod.fit()\n            \n            if verbose:\n                print(\'ARIMA{}- AIC:{}\'.format(param, results.aic))\n            model_resuls.append({\'aic\':results.aic,\n                                 \'params\':param,\n                                 \'model_obj\':results})\n            if min_aic>results.aic:\n                best_model={\'aic\':results.aic,\n                            \'params\':param,\n                            \'model_obj\':results}\n                min_aic = results.aic\n        except Exception as ex:\n            print(ex)\n    if verbose:\n        print(""Best Model params:{} AIC:{}"".format(best_model[\'params\'],\n              best_model[\'aic\']))  \n        \n    return best_model, model_resuls\n\n\ndef arima_gridsearch_cv(series, cv_splits=2,verbose=True,show_plots=True):\n    # prepare train-test split object\n    tscv = TimeSeriesSplit(n_splits=cv_splits)\n    \n    # initialize variables\n    splits = []\n    best_models = []\n    all_models = []\n    i = 1\n    \n    # loop through each CV split\n    for train_index, test_index in tscv.split(series):\n        print(""*""*20)\n        print(""Iteration {} of {}"".format(i,cv_splits))\n        i = i + 1\n        \n        # print train and test indices\n        if verbose:\n            print(""TRAIN:"", train_index, ""TEST:"", test_index)\n        splits.append({\'train\':train_index,\'test\':test_index})\n        \n        # split train and test sets\n        train_series = series.ix[train_index]\n        test_series = series.ix[test_index]\n        \n        print(""Train shape:{}, Test shape:{}"".format(train_series.shape,\n              test_series.shape))\n        \n        # perform auto arima\n        _best_model, _all_models = auto_arima(series=train_series)\n        best_models.append(_best_model)\n        all_models.append(_all_models)\n        \n        # display summary for best fitting model\n        if verbose:\n            print(_best_model[\'model_obj\'].summary())\n        results = _best_model[\'model_obj\']\n        \n        if show_plots:\n            # show residual plots\n            residuals = pd.DataFrame(results.resid)\n            residuals.plot()\n            plt.title(\'Residual Plot\')\n            plt.show()\n            residuals.plot(kind=\'kde\')\n            plt.title(\'KDE Plot\')\n            plt.show()\n            print(residuals.describe())\n        \n            # show forecast plot\n            fig, ax = plt.subplots(figsize=(18, 4))\n            fig.autofmt_xdate()\n            ax = train_series.plot(ax=ax)\n            test_series.plot(ax=ax)\n            fig = results.plot_predict(test_series.index.min(), \n                                       test_series.index.max(), \n                                       dynamic=True,ax=ax,\n                                       plot_insample=False)\n            plt.title(\'Forecast Plot \')\n            plt.legend()\n            plt.show()\n\n            # show error plot\n            insample_fit = list(results.predict(train_series.index.min()+1, \n                                                train_series.index.max(),\n                                                typ=\'levels\')) \n            plt.plot((np.exp(train_series.ix[1:].tolist())-\\\n                             np.exp(insample_fit)))\n            plt.title(\'Error Plot\')\n            plt.show()\n    return {\'cv_split_index\':splits,\n            \'all_models\':all_models,\n            \'best_models\':best_models}\n    \n# results.predict(test_series.index.min(), test_series.index.max(),typ=\'levels\')\ndef plot_on_original(train_series,test_series,forecast_series):\n    # show forecast plot on original series\n    fig, ax = plt.subplots(figsize=(18, 4))\n    fig.autofmt_xdate()\n    plt.plot(train_series,c=\'black\')\n    plt.plot(test_series,c=\'blue\')\n    plt.plot(np.exp(forecast_series),c=\'g\')\n    plt.title(\'Forecast Plot with Original Series\')\n    plt.legend()\n    plt.show()    '"
notebooks/Ch11_Forecasting_Stock_and_Commodity_Prices/getting_started_time_series.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sat Aug 26 15:33:10 2017\n\n@author: RAGHAV\n""""""\n\nimport pandas as pd\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nsns.set_style(\'whitegrid\')\nsns.set_context(\'talk\')\n\n\nif __name__==\'__main__\':\n    #load data\n    input_df = pd.read_csv(r\'website-traffic.csv\')\n    \n    input_df[\'date_of_visit\'] = pd.to_datetime(input_df.MonthDay.\\\n                                                        str.cat(\n                                                                input_df.Year.astype(str), \n                                                                sep=\' \'))\n    \n    print(input_df[[\'date_of_visit\',\'Visits\']].head(10))\n    \n    # plot time series\n    input_df.plot(x=\'date_of_visit\',\n                  y=\'Visits\', \n                  title= ""Website Visits per Day"")\n    \n    # extract visits as series from the dataframe\n    ts_visits = pd.Series(input_df.Visits.values\n                          ,index=pd.date_range(\n                                                input_df.date_of_visit.min()\n                                                , input_df.date_of_visit.max()\n                                                , freq=\'D\')\n                         )\n                          \n    \n    deompose = seasonal_decompose(ts_visits.interpolate(),\n                                    freq=24)\n    deompose.plot()\n    \n    # moving average  \n    input_df[\'moving_average\'] = input_df[\'Visits\'].rolling(window=3,\n                                                            center=False).mean()\n    \n    print(input_df[[\'Visits\',\'moving_average\']].head(10)) \n    \n    plt.plot(input_df.Visits,\'-\',color=\'black\',alpha=0.3)\n    plt.plot(input_df.moving_average,color=\'b\')\n    plt.title(\'Website Visit and Moving Average Smoothening\')\n    plt.legend()\n    plt.show()\n    \n    \n    # exponentially weighted moving average\n    input_df[\'ewma\'] = input_df[\'Visits\'].ewm(halflife=3,\n                                                ignore_na=False,\n                                                min_periods=0,\n                                                adjust=True).mean()\n    \n    plt.plot(input_df.Visits,\'-\',color=\'black\',alpha=0.3)\n    plt.plot(input_df.ewma,color=\'g\')\n    plt.title(\'Website Visit and Exponential Smoothening\')\n    plt.legend()\n    plt.show()\n                      '"
notebooks/Ch11_Forecasting_Stock_and_Commodity_Prices/gold_price_forecast.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sun Aug 27 10:59:03 2017\n\n@author: RAGHAV\n""""""\n\nimport quandl\nimport numpy as np\nimport pandas as pd\n\nfrom arima_utils import ad_fuller_test, plot_rolling_stats\nfrom arima_utils import plot_acf_pacf, arima_gridsearch_cv\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style(\'whitegrid\')\nsns.set_context(\'talk\')\n\n\n\nif __name__ == \'__main__\':\n    commodity_dataset_name = ""BUNDESBANK/BBK01_WT5511""\n    gold_df = quandl.get(commodity_dataset_name, end_date=""2017-07-31"")\n    \n    # handle missing time stamps\n    gold_df = gold_df.reindex(pd.date_range(gold_df.index.min(), \n                                      gold_df.index.max(), \n                                      freq=\'D\')).fillna(method=\'ffill\')\n    \n    gold_df.plot(figsize=(15, 6))\n    plt.show()\n    \n    # log series\n    log_series = np.log(gold_df.Value)\n    \n    ad_fuller_test(log_series)\n    plot_rolling_stats(log_series)\n    \n    # Using log series with a shift to make it stationary\n    log_series_shift = log_series - log_series.shift()\n    log_series_shift = log_series_shift[~np.isnan(log_series_shift)]\n    \n    ad_fuller_test(log_series_shift)\n    plot_rolling_stats(log_series_shift)\n    \n    # determining p and q\n    plot_acf_pacf(log_series_shift)\n    \n    gold_df[\'log_series\'] = log_series\n    gold_df[\'log_series_shift\'] = log_series_shift\n    \n    # cross validate \n    results_dict = arima_gridsearch_cv(gold_df.log_series,cv_splits=2)'"
notebooks/Ch11_Forecasting_Stock_and_Commodity_Prices/lstm_utils.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Thu Sep  7 19:15:13 2017\n\n@author: RAGHAV\n""""""\n\nimport time\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\n\nfrom keras.layers import LSTM\nfrom keras.models import Sequential\nfrom keras.layers.wrappers import TimeDistributed\nfrom keras.layers.core import Dense, Activation, Dropout\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style(\'whitegrid\')\nsns.set_context(\'talk\')\n\nparams = {\'legend.fontsize\': \'x-large\',\n          \'figure.figsize\': (15, 5),\n          \'axes.labelsize\': \'x-large\',\n          \'axes.titlesize\':\'x-large\',\n          \'xtick.labelsize\':\'x-large\',\n          \'ytick.labelsize\':\'x-large\'}\n\nplt.rcParams.update(params)\n\n# get stock price information \ndef get_raw_data(index_name,retry_attempts = 3):   \n    if index_name:\n        while retry_attempts > 0 :\n            try:\n                df = pdr.get_data_yahoo(index_name)\n                new_df = df.reindex(index=pd.date_range(df.index.min(), \n                                          df.index.max(), \n                                          freq=\'D\')).fillna(method=\'ffill\')\n                retry_attempts = 0\n                return new_df\n            except:\n                print(""Data pull failed. {} retry attempts remaining"".\\\n                      format(retry_attempts))\n                retry_attempts = retry_attempts - 1\n    else:\n        print(""Invalid usage. Parameter index_name is required"")\n    return None\n    \n\n# prepare training and testing data sets for LSTM based regression modeling\ndef get_reg_train_test(timeseries,sequence_length= 51,\n                   train_size=0.9,roll_mean_window=5,\n                   normalize=True,scale=False):\n    # smoothen out series\n    if roll_mean_window:\n        timeseries = timeseries.rolling(roll_mean_window).mean().dropna()\n    \n    # create windows\n    result = []\n    for index in range(len(timeseries) - sequence_length):\n        result.append(timeseries[index: index + sequence_length])\n           \n    \n    # normalize data as a variation of 0th index\n    if normalize:\n        normalised_data = []\n        for window in result:\n            normalised_window = [((float(p) / float(window[0])) - 1) \\\n                                   for p in window]\n            normalised_data.append(normalised_window)\n        result = normalised_data\n    \n    # identify train-test splits\n    result = np.array(result) \n    row = round(train_size * result.shape[0])\n    \n    # split train and test sets\n    train = result[:int(row), :]\n    test = result[int(row):, :]\n    \n    # scale data in 0-1 range\n    scaler = None\n    if scale:\n        scaler=MinMaxScaler(feature_range=(0, 1))\n        train = scaler.fit_transform(train)\n        test = scaler.transform(test)\n      \n    # split independent and dependent variables  \n    x_train = train[:, :-1]\n    y_train = train[:, -1]\n        \n        \n    x_test = test[:, :-1]\n    y_test = test[:, -1]\n    \n    # Transforms for LSTM input\n    x_train = np.reshape(x_train, (x_train.shape[0], \n                                   x_train.shape[1], \n                                   1))\n    x_test = np.reshape(x_test, (x_test.shape[0], \n                                 x_test.shape[1], \n                                 1)) \n    \n    return x_train,y_train,x_test,y_test,scaler   \n\n\n# prepare training and testing data sets for LSTM based sequence modeling\ndef get_seq_train_test(time_series, scaling=True,train_size=0.9):\n    scaler = None\n    if scaling:\n        scaler = MinMaxScaler(feature_range=(0, 1))\n        time_series = np.array(time_series).reshape(-1,1)\n        scaled_stock_series = scaler.fit_transform(time_series)\n    else:\n        scaled_stock_series = time_series\n        \n    train_size = int(len(scaled_stock_series) * train_size)\n\n    train = scaled_stock_series[0:train_size]\n    test = scaled_stock_series[train_size:len(scaled_stock_series)]\n    \n    return train,test,scaler \n\n\n# Get stacked LSTM model for regression modeling\ndef get_reg_model(layer_units=[100,100],dropouts=[0.2,0.2],window_size=50):\n    # build LSTM network\n    model = Sequential()\n    \n    # hidden layer 1\n    model.add(LSTM(layer_units[0], \n                   input_shape=(window_size,1), \n                   return_sequences=True))\n    model.add(Dropout(dropouts[0]))\n    \n    # hidden layer 2\n    model.add(LSTM(layer_units[1]))\n    model.add(Dropout(dropouts[1]))\n    \n    # output layer\n    model.add(Dense(1))\n    model.add(Activation(""linear""))\n    \n    start = time.time()\n    model.compile(loss=""mse"", optimizer=""rmsprop"")\n    print(""> Compilation Time : "", time.time() - start)\n    print(model.summary())\n    return model\n\n\n# Get stacked LSTM model for sequence modeling\ndef get_seq_model(hidden_units=4,input_shape=(1,1),verbose=False):\n    # create and fit the LSTM network\n    model = Sequential()\n    # samples*timesteps*featuress\n\n    model.add(LSTM(input_shape=input_shape, \n                   units = hidden_units, \n                   return_sequences=True\n    ))\n    \n    # readout layer. TimeDistributedDense uses the same weights for all\n    # time steps.\n    model.add(TimeDistributed(Dense(1)))\n    start = time.time()\n    \n    model.compile(loss=""mse"", optimizer=""rmsprop"")\n    \n    if verbose:\n        print(""> Compilation Time : "", time.time() - start)\n        print(model.summary())\n        \n    return model\n\n\n# Window wise prediction function\ndef predict_reg_multiple(model, data, window_size=6, prediction_len=3):\n    prediction_list = []\n    \n    # loop for every sequence in the dataset\n    for window in range(int(len(data)/prediction_len)):\n        _seq = data[window*prediction_len]\n        predicted = []\n        # loop till required prediction length is achieved\n        for j in range(prediction_len):\n            predicted.append(model.predict(_seq[np.newaxis,:,:])[0,0])\n            _seq = _seq[1:]\n            _seq = np.insert(_seq, [window_size-1], predicted[-1], axis=0)\n        prediction_list.append(predicted)\n    return prediction_list\n\n\n# Plot window wise \ndef plot_reg_results(predicted_data, true_data, prediction_len=3):\n    fig = plt.figure(facecolor=\'white\')\n    ax = fig.add_subplot(111)\n    \n    # plot actual data\n    ax.plot(true_data, \n            label=\'True Data\',\n            c=\'black\',alpha=0.3)\n    \n    # plot flattened data\n    plt.plot(np.array(predicted_data).flatten(), \n             label=\'Prediction_full\',\n             c=\'g\',linestyle=\'--\')\n    \n    #plot each window in the prediction list\n    for i, data in enumerate(predicted_data):\n        padding = [None for p in range(i * prediction_len)]\n        plt.plot(padding + data, label=\'Prediction\',c=\'black\')\n\n    plt.title(""Forecast Plot with Prediction Window={}"".format(prediction_len))\n    plt.show()\n    '"
notebooks/Ch11_Forecasting_Stock_and_Commodity_Prices/stock_price_forecast_fbprophet.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sat Sep  9 17:17:11 2017\n\n@author: RAGHAV\n""""""\n\nimport pandas as pd\nfrom fbprophet import Prophet\nfrom lstm_utils import get_raw_data\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style(\'whitegrid\')\nsns.set_context(\'talk\')\n\nparams = {\'legend.fontsize\': \'x-large\',\n          \'figure.figsize\': (15, 5),\n         \'axes.labelsize\': \'x-large\',\n         \'axes.titlesize\':\'x-large\',\n         \'xtick.labelsize\':\'x-large\',\n         \'ytick.labelsize\':\'x-large\'}\nplt.rcParams.update(params)\n\n\nif __name__==\'__main__\':\n    \n    STOCK_INDEX = \'^GSPC\'\n\n    sp_df = get_raw_data(STOCK_INDEX) \n\n    print(""Data Retrieved"")\n    \n    # reset index to get date_time as a column\n    prophet_df = sp_df.reset_index()\n    \n    # prepare the required dataframe\n    prophet_df.rename(columns={\'index\':\'ds\',\'Close\':\'y\'},inplace=True)\n    prophet_df = prophet_df[[\'ds\',\'y\']]\n    \n    # prepare train and test sets\n    train_size = int(prophet_df.shape[0]*0.9)\n    train_df = prophet_df.iloc[:train_size]\n    test_df = prophet_df.iloc[train_size+1:]\n    \n    # build a prophet model\n    pro_model = Prophet()\n    \n    # fit the model\n    pro_model.fit(train_df)\n    \n    # prepare a future dataframe\n    test_dates = pro_model.make_future_dataframe(periods=test_df.shape[0])\n    \n    # forecast values\n    forecast_df = pro_model.predict(test_dates)\n    \n    # plot the forecast\n    pro_model.plot(forecast_df)\n    plt.show()\n    \n    # plot against true data\n    plt.plot(forecast_df.yhat,c=\'r\',label=\'Forecast\')\n    plt.plot(forecast_df.yhat_lower.iloc[train_size+1:],\n             linestyle=\'--\',c=\'b\',alpha=0.3,\n             label=\'Confidence Interval\')\n    plt.plot(forecast_df.yhat_upper.iloc[train_size+1:],\n             linestyle=\'--\',c=\'b\',alpha=0.3,\n             label=\'Confidence Interval\')\n    plt.plot(prophet_df.y,c=\'g\',label=\'True Data\')\n    plt.legend()\n    plt.title(\'Prophet Model Forecast Against True Data\')\n    plt.show()\n    \n    \n    '"
notebooks/Ch11_Forecasting_Stock_and_Commodity_Prices/stock_price_forecast_regression_modeling.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sun Sep  3 12:22:37 2017\n\n@author: RAGHAV\n""""""\n\nimport math\nimport numpy as np\n\nimport keras\nfrom lstm_utils import get_raw_data\nfrom lstm_utils import get_reg_model\nfrom lstm_utils import plot_reg_results\nfrom lstm_utils import get_reg_train_test\nfrom lstm_utils import predict_reg_multiple\n\nfrom sklearn.metrics import mean_squared_error\n\n\nif __name__==\'__main__\':\n    \n    WINDOW = 6\n    PRED_LENGTH = int(WINDOW/2)  \n    STOCK_INDEX = \'^GSPC\'\n\n    sp_df = get_raw_data(STOCK_INDEX)\n    sp_close_series = sp_df.Close \n\n    print(""Data Retrieved"")\n    \n    x_train,y_train,x_test,y_test,scaler = get_reg_train_test(sp_close_series,\n                                                      sequence_length=WINDOW+1,\n                                                      roll_mean_window=None,\n                                                      normalize=True,\n                                                      scale=False)\n    \n    print(""Data Split Complete"")\n    \n    print(""x_train shape={}"".format(x_train.shape))\n    print(""y_train shape={}"".format(y_train.shape))\n    print(""x_test shape={}"".format(x_test.shape))\n    print(""y_test shape={}"".format(y_test.shape))\n    \n    lstm_model=None\n    try:\n        lstm_model = get_reg_model(layer_units=[50,100],\n                               window_size=WINDOW)   \n    except:\n        print(""Model Build Failed. Trying Again"")\n        lstm_model = get_reg_model(layer_units=[50,100],\n                               window_size=WINDOW) \n        \n    # use eatrly stopping to avoid overfitting     \n    callbacks = [keras.callbacks.EarlyStopping(monitor=\'val_loss\',\n                                               patience=2,\n                                               verbose=0)]\n    lstm_model.fit(x_train, y_train, \n                   epochs=20, batch_size=16,\n                   verbose=1,validation_split=0.05,\n                   callbacks=callbacks)\n    print(""Model Fit Complete"")\n    \n    train_pred_seqs = predict_reg_multiple(lstm_model,\n                                                 x_train,\n                                                 window_size=WINDOW,\n                                                 prediction_len=PRED_LENGTH)\n    \n    train_offset = y_train.shape[0] - np.array(train_pred_seqs).flatten().shape[0]\n    train_rmse = math.sqrt(mean_squared_error(y_train[train_offset:], \n                                              np.array(train_pred_seqs).\\\n                                              flatten()))\n    print(\'Train Score: %.2f RMSE\' % (train_rmse))\n    \n    \n    test_pred_seqs = predict_reg_multiple(lstm_model,\n                                          x_test,\n                                          window_size=WINDOW,\n                                          prediction_len=PRED_LENGTH)\n    \n    test_offset = y_test.shape[0] - np.array(test_pred_seqs).flatten().shape[0]\n    test_rmse = math.sqrt(mean_squared_error(y_test[test_offset:], \n                                              np.array(test_pred_seqs).\\\n                                              flatten()))\n    print(\'Test Score: %.2f RMSE\' % (test_rmse))\n    \n    #pred_seqs = predict_point_by_point(lstm_model,x_test)\n    print(""Prediction Complete"")\n    plot_reg_results(test_pred_seqs,y_test,prediction_len=PRED_LENGTH)\n'"
notebooks/Ch11_Forecasting_Stock_and_Commodity_Prices/stock_price_forecast_sequence_modeling.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sun Sep  3 12:22:37 2017\n\n@author: RAGHAV\n""""""\nimport math\nimport warnings\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style(\'whitegrid\')\nsns.set_context(\'talk\')\n\nparams = {\'legend.fontsize\': \'x-large\',\n          \'figure.figsize\': (15, 5),\n         \'axes.labelsize\': \'x-large\',\n         \'axes.titlesize\':\'x-large\',\n         \'xtick.labelsize\':\'x-large\',\n         \'ytick.labelsize\':\'x-large\'}\n\nplt.rcParams.update(params)\n\n# specify to ignore warning messages\nwarnings.filterwarnings(""ignore"") \n\nfrom lstm_utils import get_raw_data\nfrom lstm_utils import get_seq_model\nfrom lstm_utils import get_seq_train_test\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.metrics import mean_squared_error\n\n\nif __name__==\'__main__\':\n    \n    TRAIN_PERCENT = 0.7\n    STOCK_INDEX = \'^GSPC\'\n    VERBOSE=True\n    \n    # load data\n    sp_df = get_raw_data(STOCK_INDEX)\n    sp_close_series = sp_df.Close \n\n    print(""Data Retrieved"")\n    \n    # split train and test datasets\n    train,test,scaler = get_seq_train_test(sp_close_series,\n                                       scaling=True,\n                                       train_size=TRAIN_PERCENT)\n\n    train = np.reshape(train,(1,train.shape[0],1))\n    test = np.reshape(test,(1,test.shape[0],1))\n    \n    train_x = train[:,:-1,:]\n    train_y = train[:,1:,:]\n    \n    test_x = test[:,:-1,:]\n    test_y = test[:,1:,:]\n        \n    print(""Data Split Complete"")\n    \n    print(""train_x shape={}"".format(train_x.shape))\n    print(""train_y shape={}"".format(train_y.shape))\n    print(""test_x shape={}"".format(test_x.shape))\n    print(""test_y shape={}"".format(test_y.shape))\n    \n    # build RNN model\n    seq_lstm_model=None\n    try:\n        seq_lstm_model = get_seq_model(input_shape=(train_x.shape[1],1),\n                                                    verbose=VERBOSE)   \n    except:\n        print(""Model Build Failed. Trying Again"")\n        seq_lstm_model = get_seq_model(input_shape=(train_x.shape[1],1),\n                                                    verbose=VERBOSE)\n        \n    # train the model\n    seq_lstm_model.fit(train_x, train_y, \n                   epochs=150, batch_size=1, \n                   verbose=2)\n    print(""Model Fit Complete"")\n    \n    # train fit performance\n    trainPredict = seq_lstm_model.predict(train_x)\n    trainScore = math.sqrt(mean_squared_error(train_y[0], trainPredict[0]))\n    print(\'Train Score: %.2f RMSE\' % (trainScore))\n    \n    \n    # Pad input sequence\n    testPredict = pad_sequences(test_x,\n                                    maxlen=train_x.shape[1],\n                                    padding=\'post\',\n                                    dtype=\'float64\')\n    \n    # forecast values\n    testPredict = seq_lstm_model.predict(testPredict)\n    \n    # evaluate performances\n    testScore = math.sqrt(mean_squared_error(test_y[0], \n                                             testPredict[0][:test_x.shape[1]]))\n    \n    # inverse transformation\n    trainPredict = scaler.inverse_transform(trainPredict.\\\n                                            reshape(trainPredict.shape[1]))\n    testPredict = scaler.inverse_transform(testPredict.\\\n                                           reshape(testPredict.shape[1]))\n    \n    # plot the true and forecasted values\n    train_size = len(trainPredict)+1\n\n    plt.plot(sp_close_series.index,\n             sp_close_series.values,c=\'black\',\n             alpha=0.3,label=\'True Data\')\n    plt.plot(sp_close_series.index[1:train_size],\n             trainPredict,label=\'Training Fit\',c=\'g\')\n    plt.plot(sp_close_series.index[train_size+1:],\n             testPredict[:test_x.shape[1]],label=\'Testing Forecast\')\n    plt.title(\'Forecast Plot\')\n    plt.legend()\n    plt.show()\n'"
notebooks/Ch12_Deep_Learning_for_Computer_Vision/neural_style_transfer.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sun Sep 24 20:48:50 2017\n\n@author: DIP\n""""""\n\nimport numpy as np\nfrom keras.applications import vgg19\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom scipy.optimize import fmin_l_bfgs_b\nfrom scipy.misc import imsave\nimport time\nfrom keras import backend as K\n\n\ndef preprocess_image(image_path, height=None, width=None):\n    height = 400 if not height else height\n    width = width if width else int(width * height / height)\n    img = load_img(image_path, target_size=(height, width))\n    img = img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = vgg19.preprocess_input(img)\n    return img\n\ndef deprocess_image(x):\n    # Remove zero-center by mean pixel\n    x[:, :, 0] += 103.939\n    x[:, :, 1] += 116.779\n    x[:, :, 2] += 123.68\n    # \'BGR\'->\'RGB\'\n    x = x[:, :, ::-1]\n    x = np.clip(x, 0, 255).astype(\'uint8\')\n    return x\n    \n    \ndef content_loss(base, combination):\n    return K.sum(K.square(combination - base))\n    \n    \ndef style_loss(style, combination, height, width):\n    \n    def build_gram_matrix(x):\n        features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n        gram_matrix = K.dot(features, K.transpose(features))\n        return gram_matrix\n\n    S = build_gram_matrix(style)\n    C = build_gram_matrix(combination)\n    channels = 3\n    size = height * width\n    return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))\n    \n    \ndef total_variation_loss(x):\n    a = K.square(\n        x[:, :img_height - 1, :img_width - 1, :] - x[:, 1:, :img_width - 1, :])\n    b = K.square(\n        x[:, :img_height - 1, :img_width - 1, :] - x[:, :img_height - 1, 1:, :])\n    return K.sum(K.pow(a + b, 1.25))\n    \ndef set_cnn_layers(source=\'gatys\'):\n    if source == \'gatys\':\n        content_layer = \'block5_conv2\'\n        style_layers = [\'block1_conv1\', \'block2_conv1\', \'block3_conv1\', \n                        \'block4_conv1\', \'block5_conv1\']\n    elif source == \'johnson\':\n        content_layer = \'block2_conv2\'\n        style_layers = [\'block1_conv2\', \'block2_conv2\', \'block3_conv3\', \n                        \'block4_conv3\', \'block5_conv3\']\n    else:\n        content_layer = \'block5_conv2\'\n        style_layers = [\'block1_conv1\', \'block2_conv1\', \'block3_conv1\', \n                        \'block4_conv1\', \'block5_conv1\']\n    return content_layer, style_layers\n\n    \nclass Evaluator(object):\n\n    def __init__(self, height=None, width=None):\n        self.loss_value = None\n        self.grads_values = None\n        self.height = height\n        self.width = width\n\n    def loss(self, x):\n        assert self.loss_value is None\n        x = x.reshape((1, self.height, self.width, 3))\n        outs = fetch_loss_and_grads([x])\n        loss_value = outs[0]\n        grad_values = outs[1].flatten().astype(\'float64\')\n        self.loss_value = loss_value\n        self.grad_values = grad_values\n        return self.loss_value\n\n    def grads(self, x):\n        assert self.loss_value is not None\n        grad_values = np.copy(self.grad_values)\n        self.loss_value = None\n        self.grad_values = None\n        return grad_values\n    \n\n\n# This is the path to the image you want to transform.\nTARGET_IMG = \'tiger.jpeg\'\n# This is the path to the style image.\nREFERENCE_STYLE_IMG = \'style3.png\'\n\nwidth, height = load_img(TARGET_IMG).size\nimg_height = 320\nimg_width = int(width * img_height / height)\n\ntarget_image = K.constant(preprocess_image(TARGET_IMG, height=img_height, width=img_width))\nstyle_image = K.constant(preprocess_image(REFERENCE_STYLE_IMG, height=img_height, width=img_width))\n\n# Placeholder for our generated image\ngenerated_image = K.placeholder((1, img_height, img_width, 3))\n\n# Combine the 3 images into a single batch\ninput_tensor = K.concatenate([target_image,\n                              style_image,\n                              generated_image], axis=0)\n\nmodel = vgg19.VGG19(input_tensor=input_tensor,\n                    weights=\'imagenet\',\n                    include_top=False)\nlayers = dict([(layer.name, layer.output) for layer in model.layers])\n\n# weights for the weighted average loss function\ncontent_weight = 0.025\nstyle_weight = 1.0\ntotal_variation_weight = 1e-4\n\n# set the content and style layers based on VGG architecture\nsource_paper = \'johnson\'\ncontent_layer, style_layers = set_cnn_layers(source=source_paper)\n\n## build the weighted loss function\n\n# initialize total loss\nloss = K.variable(0.)\n\n# add content loss\nlayer_features = layers[content_layer]\ntarget_image_features = layer_features[0, :, :, :]\ncombination_features = layer_features[2, :, :, :]\nloss += content_weight * content_loss(target_image_features,\n                                      combination_features)\n\n# add style loss\nfor layer_name in style_layers:\n    layer_features = layers[layer_name]\n    style_reference_features = layer_features[1, :, :, :]\n    combination_features = layer_features[2, :, :, :]\n    sl = style_loss(style_reference_features, combination_features, \n                    height=img_height, width=img_width)\n    loss += (style_weight / len(style_layers)) * sl\n\n# add total variation loss\nloss += total_variation_weight * total_variation_loss(generated_image)\n\n# Get the gradients of the generated image wrt the loss\ngrads = K.gradients(loss, generated_image)[0]\n\n# Function to fetch the values of the current loss and the current gradients\nfetch_loss_and_grads = K.function([generated_image], [loss, grads])\n\nevaluator = Evaluator(height=img_height, width=img_width)\n\nresult_prefix = \'style_transfer_result_\'+TARGET_IMG.split(\'.\')[0]\nresult_prefix = result_prefix+\'_\'+source_paper\niterations = 20\n\n# Run scipy-based optimization (L-BFGS) over the pixels of the generated image\n# so as to minimize the neural style loss.\n# This is our initial state: the target image.\n# Note that `scipy.optimize.fmin_l_bfgs_b` can only process flat vectors.\nx = preprocess_image(TARGET_IMG, height=img_height, width=img_width)\nx = x.flatten()\n\nfor i in range(iterations):\n    print(\'Start of iteration\', (i+1))\n    start_time = time.time()\n    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x,\n                                     fprime=evaluator.grads, maxfun=20)\n    print(\'Current loss value:\', min_val)\n    if (i+1) % 5 == 0 or i == 0:\n        # Save current generated image only every 5 iterations\n        img = x.copy().reshape((img_height, img_width, 3))\n        img = deprocess_image(img)\n        fname = result_prefix + \'_at_iteration_%d.png\' %(i+1)\n        imsave(fname, img)\n        print(\'Image saved as\', fname)\n    end_time = time.time()\n    print(\'Iteration %d completed in %ds\' % (i+1, end_time - start_time))\n\n\n'"
