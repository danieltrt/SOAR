file_path,api_count,code
data_utils.py,0,"b'import os\nimport wget\nimport tarfile\nimport re\nfrom nltk.tokenize import word_tokenize\nimport collections\nimport pandas as pd\nimport pickle\nimport numpy as np\n\nTRAIN_PATH = ""dbpedia_csv/train.csv""\nTEST_PATH = ""dbpedia_csv/test.csv""\n\n\ndef download_dbpedia():\n    dbpedia_url = \'https://github.com/le-scientifique/torchDatasets/raw/master/dbpedia_csv.tar.gz\'\n\n    wget.download(dbpedia_url)\n    with tarfile.open(""dbpedia_csv.tar.gz"", ""r:gz"") as tar:\n        tar.extractall()\n\n\ndef clean_str(text):\n    text = re.sub(r""[^A-Za-z0-9(),!?\\\'\\`\\""]"", "" "", text)\n    text = re.sub(r""\\s{2,}"", "" "", text)\n    text = text.strip().lower()\n\n    return text\n\n\ndef build_word_dict():\n    if not os.path.exists(""word_dict.pickle""):\n        train_df = pd.read_csv(TRAIN_PATH, names=[""class"", ""title"", ""content""])\n        contents = train_df[""content""]\n\n        words = list()\n        for content in contents:\n            for word in word_tokenize(clean_str(content)):\n                words.append(word)\n\n        word_counter = collections.Counter(words).most_common()\n        word_dict = dict()\n        word_dict[""<pad>""] = 0\n        word_dict[""<unk>""] = 1\n        word_dict[""<eos>""] = 2\n        for word, _ in word_counter:\n            word_dict[word] = len(word_dict)\n\n        with open(""word_dict.pickle"", ""wb"") as f:\n            pickle.dump(word_dict, f)\n\n    else:\n        with open(""word_dict.pickle"", ""rb"") as f:\n            word_dict = pickle.load(f)\n\n    return word_dict\n\n\ndef build_word_dataset(step, word_dict, document_max_len):\n    if step == ""train"":\n        df = pd.read_csv(TRAIN_PATH, names=[""class"", ""title"", ""content""])\n    else:\n        df = pd.read_csv(TEST_PATH, names=[""class"", ""title"", ""content""])\n\n    # Shuffle dataframe\n    df = df.sample(frac=1)\n    x = list(map(lambda d: word_tokenize(clean_str(d)), df[""content""]))\n    x = list(map(lambda d: list(map(lambda w: word_dict.get(w, word_dict[""<unk>""]), d)), x))\n    x = list(map(lambda d: d + [word_dict[""<eos>""]], x))\n    x = list(map(lambda d: d[:document_max_len], x))\n    x = list(map(lambda d: d + (document_max_len - len(d)) * [word_dict[""<pad>""]], x))\n\n    y = list(map(lambda d: d - 1, list(df[""class""])))\n\n    return x, y\n\n\ndef build_char_dataset(step, model, document_max_len):\n    alphabet = ""abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:\xe2\x80\x99\'\\""/|_#$%\xcb\x86&*\xcb\x9c\xe2\x80\x98+=<>()[]{} ""\n    if step == ""train"":\n        df = pd.read_csv(TRAIN_PATH, names=[""class"", ""title"", ""content""])\n    else:\n        df = pd.read_csv(TEST_PATH, names=[""class"", ""title"", ""content""])\n\n    # Shuffle dataframe\n    df = df.sample(frac=1)\n\n    char_dict = dict()\n    char_dict[""<pad>""] = 0\n    char_dict[""<unk>""] = 1\n    for c in alphabet:\n        char_dict[c] = len(char_dict)\n\n    alphabet_size = len(alphabet) + 2\n\n    x = list(map(lambda content: list(map(lambda d: char_dict.get(d, char_dict[""<unk>""]), content.lower())), df[""content""]))\n    x = list(map(lambda d: d[:document_max_len], x))\n    x = list(map(lambda d: d + (document_max_len - len(d)) * [char_dict[""<pad>""]], x))\n\n    y = list(map(lambda d: d - 1, list(df[""class""])))\n\n    return x, y, alphabet_size\n\n\ndef batch_iter(inputs, outputs, batch_size, num_epochs):\n    inputs = np.array(inputs)\n    outputs = np.array(outputs)\n\n    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n    for epoch in range(num_epochs):\n        for batch_num in range(num_batches_per_epoch):\n            start_index = batch_num * batch_size\n            end_index = min((batch_num + 1) * batch_size, len(inputs))\n            yield inputs[start_index:end_index], outputs[start_index:end_index]\n'"
test.py,4,"b'import tensorflow as tf\nimport argparse\nfrom data_utils import *\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--model"", type=str, default=""word_cnn"",\n                    help=""word_cnn | char_cnn | vd_cnn | word_rnn | att_rnn | rcnn"")\nargs = parser.parse_args()\n\nBATCH_SIZE = 128\nWORD_MAX_LEN = 100\nCHAR_MAX_LEN = 1014\n\nif args.model == ""char_cnn"":\n    test_x, test_y, alphabet_size = build_char_dataset(""test"", ""char_cnn"", CHAR_MAX_LEN)\nelif args.model == ""vd_cnn"":\n    test_x, test_y, alphabet_size = build_char_dataset(""test"", ""vdcnn"", CHAR_MAX_LEN)\nelse:\n    word_dict = build_word_dict()\n    test_x, test_y = build_word_dataset(""test"", word_dict, WORD_MAX_LEN)\n\ncheckpoint_file = tf.train.latest_checkpoint(args.model)\ngraph = tf.Graph()\nwith graph.as_default():\n    with tf.Session() as sess:\n        saver = tf.train.import_meta_graph(""{}.meta"".format(checkpoint_file))\n        saver.restore(sess, checkpoint_file)\n\n        x = graph.get_operation_by_name(""x"").outputs[0]\n        y = graph.get_operation_by_name(""y"").outputs[0]\n        is_training = graph.get_operation_by_name(""is_training"").outputs[0]\n        accuracy = graph.get_operation_by_name(""accuracy/accuracy"").outputs[0]\n\n        batches = batch_iter(test_x, test_y, BATCH_SIZE, 1)\n        sum_accuracy, cnt = 0, 0\n        for batch_x, batch_y in batches:\n            feed_dict = {\n                x: batch_x,\n                y: batch_y,\n                is_training: False\n            }\n\n            accuracy_out = sess.run(accuracy, feed_dict=feed_dict)\n            sum_accuracy += accuracy_out\n            cnt += 1\n\n        print(""Test Accuracy : {0}"".format(sum_accuracy / cnt))\n'"
train.py,3,"b'import argparse\nimport tensorflow as tf\nfrom data_utils import *\nfrom sklearn.model_selection import train_test_split\nfrom cnn_models.word_cnn import WordCNN\nfrom cnn_models.char_cnn import CharCNN\nfrom cnn_models.vd_cnn import VDCNN\nfrom rnn_models.word_rnn import WordRNN\nfrom rnn_models.attention_rnn import AttentionRNN\nfrom rnn_models.rcnn import RCNN\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--model"", type=str, default=""word_cnn"",\n                    help=""word_cnn | char_cnn | vd_cnn | word_rnn | att_rnn | rcnn"")\nargs = parser.parse_args()\n\nif not os.path.exists(""dbpedia_csv""):\n    print(""Downloading dbpedia dataset..."")\n    download_dbpedia()\n\nNUM_CLASS = 14\nBATCH_SIZE = 64\nNUM_EPOCHS = 10\nWORD_MAX_LEN = 100\nCHAR_MAX_LEN = 1014\n\nprint(""Building dataset..."")\nif args.model == ""char_cnn"":\n    x, y, alphabet_size = build_char_dataset(""train"", ""char_cnn"", CHAR_MAX_LEN)\nelif args.model == ""vd_cnn"":\n    x, y, alphabet_size = build_char_dataset(""train"", ""vdcnn"", CHAR_MAX_LEN)\nelse:\n    word_dict = build_word_dict()\n    vocabulary_size = len(word_dict)\n    x, y = build_word_dataset(""train"", word_dict, WORD_MAX_LEN)\n\ntrain_x, valid_x, train_y, valid_y = train_test_split(x, y, test_size=0.15)\n\n\nwith tf.Session() as sess:\n    if args.model == ""word_cnn"":\n        model = WordCNN(vocabulary_size, WORD_MAX_LEN, NUM_CLASS)\n    elif args.model == ""char_cnn"":\n        model = CharCNN(alphabet_size, CHAR_MAX_LEN, NUM_CLASS)\n    elif args.model == ""vd_cnn"":\n        model = VDCNN(alphabet_size, CHAR_MAX_LEN, NUM_CLASS)\n    elif args.model == ""word_rnn"":\n        model = WordRNN(vocabulary_size, WORD_MAX_LEN, NUM_CLASS)\n    elif args.model == ""att_rnn"":\n        model = AttentionRNN(vocabulary_size, WORD_MAX_LEN, NUM_CLASS)\n    elif args.model == ""rcnn"":\n        model = RCNN(vocabulary_size, WORD_MAX_LEN, NUM_CLASS)\n    else:\n        raise NotImplementedError()\n\n    sess.run(tf.global_variables_initializer())\n    saver = tf.train.Saver(tf.global_variables())\n\n    train_batches = batch_iter(train_x, train_y, BATCH_SIZE, NUM_EPOCHS)\n    num_batches_per_epoch = (len(train_x) - 1) // BATCH_SIZE + 1\n    max_accuracy = 0\n\n    for x_batch, y_batch in train_batches:\n        train_feed_dict = {\n            model.x: x_batch,\n            model.y: y_batch,\n            model.is_training: True\n        }\n\n        _, step, loss = sess.run([model.optimizer, model.global_step, model.loss], feed_dict=train_feed_dict)\n\n        if step % 100 == 0:\n            print(""step {0}: loss = {1}"".format(step, loss))\n\n        if step % 2000 == 0:\n            # Test accuracy with validation data for each epoch.\n            valid_batches = batch_iter(valid_x, valid_y, BATCH_SIZE, 1)\n            sum_accuracy, cnt = 0, 0\n\n            for valid_x_batch, valid_y_batch in valid_batches:\n                valid_feed_dict = {\n                    model.x: valid_x_batch,\n                    model.y: valid_y_batch,\n                    model.is_training: False\n                }\n\n                accuracy = sess.run(model.accuracy, feed_dict=valid_feed_dict)\n                sum_accuracy += accuracy\n                cnt += 1\n            valid_accuracy = sum_accuracy / cnt\n\n            print(""\\nValidation Accuracy = {1}\\n"".format(step // num_batches_per_epoch, sum_accuracy / cnt))\n\n            # Save model\n            if valid_accuracy > max_accuracy:\n                max_accuracy = valid_accuracy\n                saver.save(sess, ""{0}/{1}.ckpt"".format(args.model, args.model), global_step=step)\n                print(""Model is saved.\\n"")\n'"
cnn_models/char_cnn.py,51,"b'import tensorflow as tf\n\n\nclass CharCNN(object):\n    def __init__(self, alphabet_size, document_max_len, num_class):\n        self.learning_rate = 1e-3\n        self.filter_sizes = [7, 7, 3, 3, 3, 3]\n        self.num_filters = 256\n        self.kernel_initializer = tf.truncated_normal_initializer(stddev=0.05)\n\n        self.x = tf.placeholder(tf.int32, [None, document_max_len], name=""x"")\n        self.y = tf.placeholder(tf.int32, [None], name=""y"")\n        self.is_training = tf.placeholder(tf.bool, [], name=""is_training"")\n        self.global_step = tf.Variable(0, trainable=False)\n        self.keep_prob = tf.where(self.is_training, 0.5, 1.0)\n\n        self.x_one_hot = tf.one_hot(self.x, alphabet_size)\n        self.x_expanded = tf.expand_dims(self.x_one_hot, -1)\n\n        # ============= Convolutional Layers =============\n        with tf.name_scope(""conv-maxpool-1""):\n            conv1 = tf.layers.conv2d(\n                self.x_expanded,\n                filters=self.num_filters,\n                kernel_size=[self.filter_sizes[0], alphabet_size],\n                kernel_initializer=self.kernel_initializer,\n                activation=tf.nn.relu)\n            pool1 = tf.layers.max_pooling2d(\n                conv1,\n                pool_size=(3, 1),\n                strides=(3, 1))\n            pool1 = tf.transpose(pool1, [0, 1, 3, 2])\n\n        with tf.name_scope(""conv-maxpool-2""):\n            conv2 = tf.layers.conv2d(\n                pool1,\n                filters=self.num_filters,\n                kernel_size=[self.filter_sizes[1], self.num_filters],\n                kernel_initializer=self.kernel_initializer,\n                activation=tf.nn.relu)\n            pool2 = tf.layers.max_pooling2d(\n                conv2,\n                pool_size=(3, 1),\n                strides=(3, 1))\n            pool2 = tf.transpose(pool2, [0, 1, 3, 2])\n\n        with tf.name_scope(""conv-3""):\n            conv3 = tf.layers.conv2d(\n                pool2,\n                filters=self.num_filters,\n                kernel_size=[self.filter_sizes[2], self.num_filters],\n                kernel_initializer=self.kernel_initializer,\n                activation=tf.nn.relu)\n            conv3 = tf.transpose(conv3, [0, 1, 3, 2])\n\n        with tf.name_scope(""conv-4""):\n            conv4 = tf.layers.conv2d(\n                conv3,\n                filters=self.num_filters,\n                kernel_size=[self.filter_sizes[3], self.num_filters],\n                kernel_initializer=self.kernel_initializer,\n                activation=tf.nn.relu)\n            conv4 = tf.transpose(conv4, [0, 1, 3, 2])\n\n        with tf.name_scope(""conv-5""):\n            conv5 = tf.layers.conv2d(\n                conv4,\n                filters=self.num_filters,\n                kernel_size=[self.filter_sizes[4], self.num_filters],\n                kernel_initializer=self.kernel_initializer,\n                activation=tf.nn.relu)\n            conv5 = tf.transpose(conv5, [0, 1, 3, 2])\n\n        with tf.name_scope(""conv-maxpool-6""):\n            conv6 = tf.layers.conv2d(\n                conv5,\n                filters=self.num_filters,\n                kernel_size=[self.filter_sizes[5], self.num_filters],\n                kernel_initializer=self.kernel_initializer,\n                activation=tf.nn.relu)\n            pool6 = tf.layers.max_pooling2d(\n                conv6,\n                pool_size=(3, 1),\n                strides=(3, 1))\n            pool6 = tf.transpose(pool6, [0, 2, 1, 3])\n            h_pool = tf.reshape(pool6, [-1, 34 * self.num_filters])\n\n        # ============= Fully Connected Layers =============\n        with tf.name_scope(""fc-1""):\n            fc1_out = tf.layers.dense(h_pool, 1024, activation=tf.nn.relu, kernel_initializer=self.kernel_initializer)\n\n        with tf.name_scope(""fc-2""):\n            fc2_out = tf.layers.dense(fc1_out, 1024, activation=tf.nn.relu, kernel_initializer=self.kernel_initializer)\n\n        with tf.name_scope(""fc-3""):\n            self.logits = tf.layers.dense(fc2_out, num_class, activation=None, kernel_initializer=self.kernel_initializer)\n            self.predictions = tf.argmax(self.logits, -1, output_type=tf.int32)\n\n        # ============= Loss and Accuracy =============\n        with tf.name_scope(""loss""):\n            self.y_one_hot = tf.one_hot(self.y, num_class)\n            self.loss = tf.reduce_mean(\n                tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=self.y_one_hot))\n            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n\n        with tf.name_scope(""accuracy""):\n            correct_predictions = tf.equal(self.predictions, self.y)\n            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, ""float""), name=""accuracy"")\n'"
cnn_models/vd_cnn.py,47,"b'import tensorflow as tf\n\n\nclass VDCNN(object):\n    def __init__(self, alphabet_size, document_max_len, num_class):\n        self.embedding_size = 16\n        self.filter_sizes = [3, 3, 3, 3, 3]\n        self.num_filters = [64, 64, 128, 256, 512]\n        self.num_blocks = [2, 2, 2, 2]\n        self.learning_rate = 1e-3\n        self.cnn_initializer = tf.keras.initializers.he_normal()\n        self.fc_initializer = tf.truncated_normal_initializer(stddev=0.05)\n\n        self.x = tf.placeholder(tf.int32, [None, document_max_len], name=""x"")\n        self.y = tf.placeholder(tf.int32, [None], name=""y"")\n        self.is_training = tf.placeholder(tf.bool, [], name=""is_training"")\n        self.global_step = tf.Variable(0, trainable=False)\n\n        # ============= Embedding Layer =============\n        with tf.name_scope(""embedding""):\n            init_embeddings = tf.random_uniform([alphabet_size, self.embedding_size], -1.0, 1.0)\n            self.embeddings = tf.get_variable(""embeddings"", initializer=init_embeddings)\n            x_emb = tf.nn.embedding_lookup(self.embeddings, self.x)\n            self.x_expanded = tf.expand_dims(x_emb, -1)\n\n        # ============= First Convolution Layer =============\n        with tf.variable_scope(""conv-0""):\n            conv0 = tf.layers.conv2d(\n                self.x_expanded,\n                filters=self.num_filters[0],\n                kernel_size=[self.filter_sizes[0], self.embedding_size],\n                kernel_initializer=self.cnn_initializer,\n                activation=tf.nn.relu)\n            conv0 = tf.transpose(conv0, [0, 1, 3, 2])\n\n        # ============= Convolution Blocks =============\n        with tf.name_scope(""conv-block-1""):\n            conv1 = self.conv_block(conv0, 1)\n\n        with tf.name_scope(""conv-block-2""):\n            conv2 = self.conv_block(conv1, 2)\n\n        with tf.name_scope(""conv-block-3""):\n            conv3 = self.conv_block(conv2, 3)\n\n        with tf.name_scope(""conv-block-4""):\n            conv4 = self.conv_block(conv3, 4, max_pool=False)\n\n        # ============= k-max Pooling =============\n        with tf.name_scope(""k-max-pooling""):\n            h = tf.transpose(tf.squeeze(conv4, -1), [0, 2, 1])\n            top_k = tf.nn.top_k(h, k=8, sorted=False).values\n            h_flat = tf.reshape(top_k, [-1, 512 * 8])\n\n        # ============= Fully Connected Layers =============\n        with tf.name_scope(""fc-1""):\n            fc1_out = tf.layers.dense(h_flat, 2048, activation=tf.nn.relu, kernel_initializer=self.fc_initializer)\n\n        with tf.name_scope(""fc-2""):\n            fc2_out = tf.layers.dense(fc1_out, 2048, activation=tf.nn.relu, kernel_initializer=self.fc_initializer)\n\n        with tf.name_scope(""fc-3""):\n            self.logits = tf.layers.dense(fc2_out, num_class, activation=None, kernel_initializer=self.fc_initializer)\n            self.predictions = tf.argmax(self.logits, -1, output_type=tf.int32)\n\n        # ============= Loss and Accuracy =============\n        with tf.name_scope(""loss""):\n            y_one_hot = tf.one_hot(self.y, num_class)\n            self.loss = tf.reduce_mean(\n                tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits, labels=y_one_hot))\n\n            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n            with tf.control_dependencies(update_ops):\n                self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n\n        with tf.name_scope(""accuracy""):\n            correct_predictions = tf.equal(self.predictions, self.y)\n            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, ""float""), name=""accuracy"")\n\n    def conv_block(self, input, i, max_pool=True):\n        with tf.variable_scope(""conv-block-%s"" % i):\n            # Two ""conv-batch_norm-relu"" layers.\n            for j in range(2):\n                with tf.variable_scope(""conv-%s"" % j):\n                    # convolution\n                    conv = tf.layers.conv2d(\n                        input,\n                        filters=self.num_filters[i],\n                        kernel_size=[self.filter_sizes[i], self.num_filters[i-1]],\n                        kernel_initializer=self.cnn_initializer,\n                        activation=None)\n                    # batch normalization\n                    conv = tf.layers.batch_normalization(conv, training=self.is_training)\n                    # relu\n                    conv = tf.nn.relu(conv)\n                    conv = tf.transpose(conv, [0, 1, 3, 2])\n\n            if max_pool:\n                # Max pooling\n                pool = tf.layers.max_pooling2d(\n                    conv,\n                    pool_size=(3, 1),\n                    strides=(2, 1),\n                    padding=""SAME"")\n                return pool\n            else:\n                return conv\n'"
cnn_models/word_cnn.py,27,"b'import tensorflow as tf\n\n\nclass WordCNN(object):\n    def __init__(self, vocabulary_size, document_max_len, num_class):\n        self.embedding_size = 128\n        self.learning_rate = 1e-3\n        self.filter_sizes = [3, 4, 5]\n        self.num_filters = 100\n\n        self.x = tf.placeholder(tf.int32, [None, document_max_len], name=""x"")\n        self.y = tf.placeholder(tf.int32, [None], name=""y"")\n        self.is_training = tf.placeholder(tf.bool, [], name=""is_training"")\n        self.global_step = tf.Variable(0, trainable=False)\n        self.keep_prob = tf.where(self.is_training, 0.5, 1.0)\n\n        with tf.name_scope(""embedding""):\n            init_embeddings = tf.random_uniform([vocabulary_size, self.embedding_size])\n            self.embeddings = tf.get_variable(""embeddings"", initializer=init_embeddings)\n            self.x_emb = tf.nn.embedding_lookup(self.embeddings, self.x)\n            self.x_emb = tf.expand_dims(self.x_emb, -1)\n\n        pooled_outputs = []\n        for filter_size in self.filter_sizes:\n            conv = tf.layers.conv2d(\n                self.x_emb,\n                filters=self.num_filters,\n                kernel_size=[filter_size, self.embedding_size],\n                strides=(1, 1),\n                padding=""VALID"",\n                activation=tf.nn.relu)\n            pool = tf.layers.max_pooling2d(\n                conv,\n                pool_size=[document_max_len - filter_size + 1, 1],\n                strides=(1, 1),\n                padding=""VALID"")\n            pooled_outputs.append(pool)\n\n        h_pool = tf.concat(pooled_outputs, 3)\n        h_pool_flat = tf.reshape(h_pool, [-1, self.num_filters * len(self.filter_sizes)])\n\n        with tf.name_scope(""dropout""):\n            h_drop = tf.nn.dropout(h_pool_flat, self.keep_prob)\n\n        with tf.name_scope(""output""):\n            self.logits = tf.layers.dense(h_drop, num_class, activation=None)\n            self.predictions = tf.argmax(self.logits, -1, output_type=tf.int32)\n\n        with tf.name_scope(""loss""):\n            self.loss = tf.reduce_mean(\n                tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y))\n            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n\n        with tf.name_scope(""accuracy""):\n            correct_predictions = tf.equal(self.predictions, self.y)\n            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, ""float""), name=""accuracy"")\n'"
rnn_models/attention_rnn.py,26,"b'import tensorflow as tf\nfrom tensorflow.contrib import rnn\n\n\nclass AttentionRNN(object):\n    def __init__(self, vocabulary_size, document_max_len, num_class):\n        self.embedding_size = 256\n        self.num_hidden = 256\n        self.num_layers = 2\n        self.learning_rate = 1e-3\n\n        self.x = tf.placeholder(tf.int32, [None, document_max_len], name=""x"")\n        self.x_len = tf.reduce_sum(tf.sign(self.x), 1)\n        self.y = tf.placeholder(tf.int32, [None], name=""y"")\n        self.is_training = tf.placeholder(tf.bool, [], name=""is_training"")\n        self.global_step = tf.Variable(0, trainable=False)\n        self.keep_prob = tf.where(self.is_training, 0.5, 1.0)\n\n        with tf.name_scope(""embedding""):\n            init_embeddings = tf.random_uniform([vocabulary_size, self.embedding_size])\n            self.embeddings = tf.get_variable(""embeddings"", initializer=init_embeddings)\n            self.x_emb = tf.nn.embedding_lookup(self.embeddings, self.x)\n\n        with tf.name_scope(""birnn""):\n            fw_cells = [rnn.BasicLSTMCell(self.num_hidden) for _ in range(self.num_layers)]\n            bw_cells = [rnn.BasicLSTMCell(self.num_hidden) for _ in range(self.num_layers)]\n            fw_cells = [rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob) for cell in fw_cells]\n            bw_cells = [rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob) for cell in bw_cells]\n\n            self.rnn_outputs, _, _ = rnn.stack_bidirectional_dynamic_rnn(\n                fw_cells, bw_cells, self.x_emb, sequence_length=self.x_len, dtype=tf.float32)\n\n        with tf.name_scope(""attention""):\n            self.attention_score = tf.nn.softmax(tf.layers.dense(self.rnn_outputs, 1, activation=tf.nn.tanh), axis=1)\n            self.attention_out = tf.squeeze(\n                tf.matmul(tf.transpose(self.rnn_outputs, perm=[0, 2, 1]), self.attention_score),\n                axis=-1)\n\n        with tf.name_scope(""output""):\n            self.logits = tf.layers.dense(self.attention_out, num_class, activation=None)\n            self.predictions = tf.argmax(self.logits, -1, output_type=tf.int32)\n\n        with tf.name_scope(""loss""):\n            self.loss = tf.reduce_mean(\n                tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y))\n            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n\n        with tf.name_scope(""accuracy""):\n            correct_predictions = tf.equal(self.predictions, self.y)\n            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, ""float""), name=""accuracy"")\n'"
rnn_models/rcnn.py,28,"b'import tensorflow as tf\nfrom tensorflow.contrib import rnn\n\n\nclass RCNN(object):\n    def __init__(self, vocabulary_size, document_max_len, num_class):\n        self.embedding_size = 256\n        self.rnn_num_hidden = 256\n        self.fc_num_hidden = 256\n        self.learning_rate = 1e-3\n\n        self.x = tf.placeholder(tf.int32, [None, document_max_len], name=""x"")\n        self.x_len = tf.reduce_sum(tf.sign(self.x), 1)\n        self.y = tf.placeholder(tf.int32, [None], name=""y"")\n        self.is_training = tf.placeholder(tf.bool, [], name=""is_training"")\n        self.global_step = tf.Variable(0, trainable=False)\n        self.keep_prob = tf.where(self.is_training, 0.5, 1.0)\n\n        with tf.name_scope(""embedding""):\n            init_embeddings = tf.random_uniform([vocabulary_size, self.embedding_size])\n            self.embeddings = tf.get_variable(""embeddings"", initializer=init_embeddings)\n            self.x_emb = tf.nn.embedding_lookup(self.embeddings, self.x)\n\n        with tf.name_scope(""birnn""):\n            fw_cell = rnn.BasicLSTMCell(self.rnn_num_hidden)\n            bw_cell = rnn.BasicLSTMCell(self.rnn_num_hidden)\n            fw_cell = rnn.DropoutWrapper(fw_cell, output_keep_prob=self.keep_prob)\n            bw_cell = rnn.DropoutWrapper(bw_cell, output_keep_prob=self.keep_prob)\n\n            rnn_outputs, _ = tf.nn.bidirectional_dynamic_rnn(fw_cell, bw_cell, self.x_emb,\n                                                             sequence_length=self.x_len, dtype=tf.float32)\n            self.fw_output, self.bw_output = rnn_outputs\n\n        with tf.name_scope(""word-representation""):\n            x = tf.concat([self.fw_output, self.x_emb, self.bw_output], axis=2)\n            self.y2 = tf.layers.dense(x, self.fc_num_hidden, activation=tf.nn.tanh)\n\n        with tf.name_scope(""text-representation""):\n            self.y3 = tf.reduce_max(self.y2, axis=1)\n\n        with tf.name_scope(""output""):\n            self.logits = tf.layers.dense(self.y3, num_class)\n            self.predictions = tf.argmax(self.logits, -1, output_type=tf.int32)\n\n        with tf.name_scope(""loss""):\n            self.loss = tf.reduce_mean(\n                tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y))\n            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n\n        with tf.name_scope(""accuracy""):\n            correct_predictions = tf.equal(self.predictions, self.y)\n            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, ""float""), name=""accuracy"")\n'"
rnn_models/word_rnn.py,23,"b'import tensorflow as tf\nfrom tensorflow.contrib import rnn\n\n\nclass WordRNN(object):\n    def __init__(self, vocabulary_size, document_max_len, num_class):\n        self.embedding_size = 256\n        self.num_hidden = 256\n        self.num_layers = 2\n        self.learning_rate = 1e-3\n\n        self.x = tf.placeholder(tf.int32, [None, document_max_len], name=""x"")\n        self.x_len = tf.reduce_sum(tf.sign(self.x), 1)\n        self.y = tf.placeholder(tf.int32, [None], name=""y"")\n        self.is_training = tf.placeholder(tf.bool, [], name=""is_training"")\n        self.global_step = tf.Variable(0, trainable=False)\n        self.keep_prob = tf.where(self.is_training, 0.5, 1.0)\n\n        with tf.name_scope(""embedding""):\n            init_embeddings = tf.random_uniform([vocabulary_size, self.embedding_size])\n            self.embeddings = tf.get_variable(""embeddings"", initializer=init_embeddings)\n            self.x_emb = tf.nn.embedding_lookup(self.embeddings, self.x)\n\n        with tf.name_scope(""birnn""):\n            fw_cells = [rnn.BasicLSTMCell(self.num_hidden) for _ in range(self.num_layers)]\n            bw_cells = [rnn.BasicLSTMCell(self.num_hidden) for _ in range(self.num_layers)]\n            fw_cells = [rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob) for cell in fw_cells]\n            bw_cells = [rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob) for cell in bw_cells]\n\n            rnn_outputs, _, _ = rnn.stack_bidirectional_dynamic_rnn(\n                fw_cells, bw_cells, self.x_emb, sequence_length=self.x_len, dtype=tf.float32)\n            rnn_outputs_flat = tf.reshape(rnn_outputs, [-1, document_max_len * self.num_hidden * 2])\n\n        with tf.name_scope(""output""):\n            self.logits = tf.layers.dense(rnn_outputs_flat, num_class, activation=None)\n            self.predictions = tf.argmax(self.logits, -1, output_type=tf.int32)\n\n        with tf.name_scope(""loss""):\n            self.loss = tf.reduce_mean(\n                tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.y))\n            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n\n        with tf.name_scope(""accuracy""):\n            correct_predictions = tf.equal(self.predictions, self.y)\n            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, ""float""), name=""accuracy"")\n'"
