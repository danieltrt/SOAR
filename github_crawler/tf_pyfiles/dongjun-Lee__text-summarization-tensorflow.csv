file_path,api_count,code
model.py,53,"b'import tensorflow as tf\nfrom tensorflow.contrib import rnn\nfrom utils import get_init_embedding\n\n\nclass Model(object):\n    def __init__(self, reversed_dict, article_max_len, summary_max_len, args, forward_only=False):\n        self.vocabulary_size = len(reversed_dict)\n        self.embedding_size = args.embedding_size\n        self.num_hidden = args.num_hidden\n        self.num_layers = args.num_layers\n        self.learning_rate = args.learning_rate\n        self.beam_width = args.beam_width\n        if not forward_only:\n            self.keep_prob = args.keep_prob\n        else:\n            self.keep_prob = 1.0\n        self.cell = tf.nn.rnn_cell.BasicLSTMCell\n        with tf.variable_scope(""decoder/projection""):\n            self.projection_layer = tf.layers.Dense(self.vocabulary_size, use_bias=False)\n\n        self.batch_size = tf.placeholder(tf.int32, (), name=""batch_size"")\n        self.X = tf.placeholder(tf.int32, [None, article_max_len])\n        self.X_len = tf.placeholder(tf.int32, [None])\n        self.decoder_input = tf.placeholder(tf.int32, [None, summary_max_len])\n        self.decoder_len = tf.placeholder(tf.int32, [None])\n        self.decoder_target = tf.placeholder(tf.int32, [None, summary_max_len])\n        self.global_step = tf.Variable(0, trainable=False)\n\n        with tf.name_scope(""embedding""):\n            if not forward_only and args.glove:\n                init_embeddings = tf.constant(get_init_embedding(reversed_dict, self.embedding_size), dtype=tf.float32)\n            else:\n                init_embeddings = tf.random_uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0)\n            self.embeddings = tf.get_variable(""embeddings"", initializer=init_embeddings)\n            self.encoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.X), perm=[1, 0, 2])\n            self.decoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.decoder_input), perm=[1, 0, 2])\n\n        with tf.name_scope(""encoder""):\n            fw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n            bw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n            fw_cells = [rnn.DropoutWrapper(cell) for cell in fw_cells]\n            bw_cells = [rnn.DropoutWrapper(cell) for cell in bw_cells]\n\n            encoder_outputs, encoder_state_fw, encoder_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n                fw_cells, bw_cells, self.encoder_emb_inp,\n                sequence_length=self.X_len, time_major=True, dtype=tf.float32)\n            self.encoder_output = tf.concat(encoder_outputs, 2)\n            encoder_state_c = tf.concat((encoder_state_fw[0].c, encoder_state_bw[0].c), 1)\n            encoder_state_h = tf.concat((encoder_state_fw[0].h, encoder_state_bw[0].h), 1)\n            self.encoder_state = rnn.LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n\n        with tf.name_scope(""decoder""), tf.variable_scope(""decoder"") as decoder_scope:\n            decoder_cell = self.cell(self.num_hidden * 2)\n\n            if not forward_only:\n                attention_states = tf.transpose(self.encoder_output, [1, 0, 2])\n                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n                    self.num_hidden * 2, attention_states, memory_sequence_length=self.X_len, normalize=True)\n                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n                                                                   attention_layer_size=self.num_hidden * 2)\n                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size)\n                initial_state = initial_state.clone(cell_state=self.encoder_state)\n                helper = tf.contrib.seq2seq.TrainingHelper(self.decoder_emb_inp, self.decoder_len, time_major=True)\n                decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, initial_state)\n                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=True, scope=decoder_scope)\n                self.decoder_output = outputs.rnn_output\n                self.logits = tf.transpose(\n                    self.projection_layer(self.decoder_output), perm=[1, 0, 2])\n                self.logits_reshape = tf.concat(\n                    [self.logits, tf.zeros([self.batch_size, summary_max_len - tf.shape(self.logits)[1], self.vocabulary_size])], axis=1)\n            else:\n                tiled_encoder_output = tf.contrib.seq2seq.tile_batch(\n                    tf.transpose(self.encoder_output, perm=[1, 0, 2]), multiplier=self.beam_width)\n                tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(self.encoder_state, multiplier=self.beam_width)\n                tiled_seq_len = tf.contrib.seq2seq.tile_batch(self.X_len, multiplier=self.beam_width)\n                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n                    self.num_hidden * 2, tiled_encoder_output, memory_sequence_length=tiled_seq_len, normalize=True)\n                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n                                                                   attention_layer_size=self.num_hidden * 2)\n                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size * self.beam_width)\n                initial_state = initial_state.clone(cell_state=tiled_encoder_final_state)\n                decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n                    cell=decoder_cell,\n                    embedding=self.embeddings,\n                    start_tokens=tf.fill([self.batch_size], tf.constant(2)),\n                    end_token=tf.constant(3),\n                    initial_state=initial_state,\n                    beam_width=self.beam_width,\n                    output_layer=self.projection_layer\n                )\n                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n                    decoder, output_time_major=True, maximum_iterations=summary_max_len, scope=decoder_scope)\n                self.prediction = tf.transpose(outputs.predicted_ids, perm=[1, 2, 0])\n\n        with tf.name_scope(""loss""):\n            if not forward_only:\n                crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n                    logits=self.logits_reshape, labels=self.decoder_target)\n                weights = tf.sequence_mask(self.decoder_len, summary_max_len, dtype=tf.float32)\n                self.loss = tf.reduce_sum(crossent * weights / tf.to_float(self.batch_size))\n\n                params = tf.trainable_variables()\n                gradients = tf.gradients(self.loss, params)\n                clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n                optimizer = tf.train.AdamOptimizer(self.learning_rate)\n                self.update = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)\n'"
prep_data.py,0,"b'import wget\nimport os\nimport tarfile\nimport gzip\nimport zipfile\nimport argparse\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--glove"", action=""store_true"")\nargs = parser.parse_args()\n\n# Extract data file\nwith tarfile.open(""summary.tar.gz"", ""r:gz"") as tar:\n    tar.extractall()\n\nwith gzip.open(""sumdata/train/train.article.txt.gz"", ""rb"") as gz:\n    with open(""sumdata/train/train.article.txt"", ""wb"") as out:\n        out.write(gz.read())\n\nwith gzip.open(""sumdata/train/train.title.txt.gz"", ""rb"") as gz:\n    with open(""sumdata/train/train.title.txt"", ""wb"") as out:\n        out.write(gz.read())\n\nif args.glove:\n    glove_dir = ""glove""\n    glove_url = ""https://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip""\n\n    if not os.path.exists(glove_dir):\n        os.mkdir(glove_dir)\n\n    # Download glove vector\n    wget.download(glove_url, out=glove_dir)\n\n    # Extract glove file\n    with zipfile.ZipFile(os.path.join(""glove"", ""glove.42B.300d.zip""), ""r"") as z:\n        z.extractall(glove_dir)\n'"
test.py,3,"b'import tensorflow as tf\nimport pickle\nfrom model import Model\nfrom utils import build_dict, build_dataset, batch_iter\n\n\nwith open(""args.pickle"", ""rb"") as f:\n    args = pickle.load(f)\n\nprint(""Loading dictionary..."")\nword_dict, reversed_dict, article_max_len, summary_max_len = build_dict(""valid"", args.toy)\nprint(""Loading validation dataset..."")\nvalid_x = build_dataset(""valid"", word_dict, article_max_len, summary_max_len, args.toy)\nvalid_x_len = [len([y for y in x if y != 0]) for x in valid_x]\n\nwith tf.Session() as sess:\n    print(""Loading saved model..."")\n    model = Model(reversed_dict, article_max_len, summary_max_len, args, forward_only=True)\n    saver = tf.train.Saver(tf.global_variables())\n    ckpt = tf.train.get_checkpoint_state(""./saved_model/"")\n    saver.restore(sess, ckpt.model_checkpoint_path)\n\n    batches = batch_iter(valid_x, [0] * len(valid_x), args.batch_size, 1)\n\n    print(""Writing summaries to \'result.txt\'..."")\n    for batch_x, _ in batches:\n        batch_x_len = [len([y for y in x if y != 0]) for x in batch_x]\n\n        valid_feed_dict = {\n            model.batch_size: len(batch_x),\n            model.X: batch_x,\n            model.X_len: batch_x_len,\n        }\n\n        prediction = sess.run(model.prediction, feed_dict=valid_feed_dict)\n        prediction_output = [[reversed_dict[y] for y in x] for x in prediction[:, 0, :]]\n\n        with open(""result.txt"", ""a"") as f:\n            for line in prediction_output:\n                summary = list()\n                for word in line:\n                    if word == ""</s>"":\n                        break\n                    if word not in summary:\n                        summary.append(word)\n                print("" "".join(summary), file=f)\n\n    print(\'Summaries are saved to ""result.txt""...\')\n'"
train.py,4,"b'import time\nstart = time.perf_counter()\nimport tensorflow as tf\nimport argparse\nimport pickle\nimport os\nfrom model import Model\nfrom utils import build_dict, build_dataset, batch_iter\n\n# Uncomment next 2 lines to suppress error and Tensorflow info verbosity. Or change logging levels\n# tf.logging.set_verbosity(tf.logging.FATAL)\n# os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\ndef add_arguments(parser):\n    parser.add_argument(""--num_hidden"", type=int, default=150, help=""Network size."")\n    parser.add_argument(""--num_layers"", type=int, default=2, help=""Network depth."")\n    parser.add_argument(""--beam_width"", type=int, default=10, help=""Beam width for beam search decoder."")\n    parser.add_argument(""--glove"", action=""store_true"", help=""Use glove as initial word embedding."")\n    parser.add_argument(""--embedding_size"", type=int, default=300, help=""Word embedding size."")\n\n    parser.add_argument(""--learning_rate"", type=float, default=1e-3, help=""Learning rate."")\n    parser.add_argument(""--batch_size"", type=int, default=64, help=""Batch size."")\n    parser.add_argument(""--num_epochs"", type=int, default=10, help=""Number of epochs."")\n    parser.add_argument(""--keep_prob"", type=float, default=0.8, help=""Dropout keep prob."")\n\n    parser.add_argument(""--toy"", action=""store_true"", help=""Use only 50K samples of data"")\n\n    parser.add_argument(""--with_model"", action=""store_true"", help=""Continue from previously saved model"")\n\n\n\nparser = argparse.ArgumentParser()\nadd_arguments(parser)\nargs = parser.parse_args()\nwith open(""args.pickle"", ""wb"") as f:\n    pickle.dump(args, f)\n\nif not os.path.exists(""saved_model""):\n    os.mkdir(""saved_model"")\nelse:\n    if args.with_model:\n        old_model_checkpoint_path = open(\'saved_model/checkpoint\', \'r\')\n        old_model_checkpoint_path = """".join([""saved_model/"",old_model_checkpoint_path.read().splitlines()[0].split(\'""\')[1] ])\n\n\nprint(""Building dictionary..."")\nword_dict, reversed_dict, article_max_len, summary_max_len = build_dict(""train"", args.toy)\nprint(""Loading training dataset..."")\ntrain_x, train_y = build_dataset(""train"", word_dict, article_max_len, summary_max_len, args.toy)\n\n\nwith tf.Session() as sess:\n    model = Model(reversed_dict, article_max_len, summary_max_len, args)\n    sess.run(tf.global_variables_initializer())\n    saver = tf.train.Saver(tf.global_variables())\n    if \'old_model_checkpoint_path\' in globals():\n        print(""Continuing from previous trained model:"" , old_model_checkpoint_path , ""..."")\n        saver.restore(sess, old_model_checkpoint_path )\n\n    batches = batch_iter(train_x, train_y, args.batch_size, args.num_epochs)\n    num_batches_per_epoch = (len(train_x) - 1) // args.batch_size + 1\n\n    print(""\\nIteration starts."")\n    print(""Number of batches per epoch :"", num_batches_per_epoch)\n    for batch_x, batch_y in batches:\n        batch_x_len = list(map(lambda x: len([y for y in x if y != 0]), batch_x))\n        batch_decoder_input = list(map(lambda x: [word_dict[""<s>""]] + list(x), batch_y))\n        batch_decoder_len = list(map(lambda x: len([y for y in x if y != 0]), batch_decoder_input))\n        batch_decoder_output = list(map(lambda x: list(x) + [word_dict[""</s>""]], batch_y))\n\n        batch_decoder_input = list(\n            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[""<padding>""]], batch_decoder_input))\n        batch_decoder_output = list(\n            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[""<padding>""]], batch_decoder_output))\n\n        train_feed_dict = {\n            model.batch_size: len(batch_x),\n            model.X: batch_x,\n            model.X_len: batch_x_len,\n            model.decoder_input: batch_decoder_input,\n            model.decoder_len: batch_decoder_len,\n            model.decoder_target: batch_decoder_output\n        }\n\n        _, step, loss = sess.run([model.update, model.global_step, model.loss], feed_dict=train_feed_dict)\n\n        if step % 1000 == 0:\n            print(""step {0}: loss = {1}"".format(step, loss))\n\n        if step % num_batches_per_epoch == 0:\n            hours, rem = divmod(time.perf_counter() - start, 3600)\n            minutes, seconds = divmod(rem, 60)\n            saver.save(sess, ""./saved_model/model.ckpt"", global_step=step)\n            print("" Epoch {0}: Model is saved."".format(step // num_batches_per_epoch),\n            ""Elapsed: {:0>2}:{:0>2}:{:05.2f}"".format(int(hours),int(minutes),seconds) , ""\\n"")\n'"
utils.py,0,"b'from nltk.tokenize import word_tokenize\nimport re\nimport collections\nimport pickle\nimport numpy as np\nfrom gensim.models.keyedvectors import KeyedVectors\nfrom gensim.test.utils import get_tmpfile\nfrom gensim.scripts.glove2word2vec import glove2word2vec\n\n\ntrain_article_path = ""sumdata/train/train.article.txt""\ntrain_title_path = ""sumdata/train/train.title.txt""\nvalid_article_path = ""sumdata/train/valid.article.filter.txt""\nvalid_title_path = ""sumdata/train/valid.title.filter.txt""\n\n\ndef clean_str(sentence):\n    sentence = re.sub(""[#.]+"", ""#"", sentence)\n    return sentence\n\n\ndef get_text_list(data_path, toy):\n    with open (data_path, ""r"", encoding=""utf-8"") as f:\n        if not toy:\n            return [clean_str(x.strip()) for x in f.readlines()]\n        else:\n            return [clean_str(x.strip()) for x in f.readlines()][:50000]\n\n\ndef build_dict(step, toy=False):\n    if step == ""train"":\n        train_article_list = get_text_list(train_article_path, toy)\n        train_title_list = get_text_list(train_title_path, toy)\n\n        words = list()\n        for sentence in train_article_list + train_title_list:\n            for word in word_tokenize(sentence):\n                words.append(word)\n\n        word_counter = collections.Counter(words).most_common()\n        word_dict = dict()\n        word_dict[""<padding>""] = 0\n        word_dict[""<unk>""] = 1\n        word_dict[""<s>""] = 2\n        word_dict[""</s>""] = 3\n        for word, _ in word_counter:\n            word_dict[word] = len(word_dict)\n\n        with open(""word_dict.pickle"", ""wb"") as f:\n            pickle.dump(word_dict, f)\n\n    elif step == ""valid"":\n        with open(""word_dict.pickle"", ""rb"") as f:\n            word_dict = pickle.load(f)\n\n    reversed_dict = dict(zip(word_dict.values(), word_dict.keys()))\n\n    article_max_len = 50\n    summary_max_len = 15\n\n    return word_dict, reversed_dict, article_max_len, summary_max_len\n\n\ndef build_dataset(step, word_dict, article_max_len, summary_max_len, toy=False):\n    if step == ""train"":\n        article_list = get_text_list(train_article_path, toy)\n        title_list = get_text_list(train_title_path, toy)\n    elif step == ""valid"":\n        article_list = get_text_list(valid_article_path, toy)\n    else:\n        raise NotImplementedError\n\n    x = [word_tokenize(d) for d in article_list]\n    x = [[word_dict.get(w, word_dict[""<unk>""]) for w in d] for d in x]\n    x = [d[:article_max_len] for d in x]\n    x = [d + (article_max_len - len(d)) * [word_dict[""<padding>""]] for d in x]\n    \n    if step == ""valid"":\n        return x\n    else:        \n        y = [word_tokenize(d) for d in title_list]\n        y = [[word_dict.get(w, word_dict[""<unk>""]) for w in d] for d in y]\n        y = [d[:(summary_max_len - 1)] for d in y]\n        return x, y\n\n\ndef batch_iter(inputs, outputs, batch_size, num_epochs):\n    inputs = np.array(inputs)\n    outputs = np.array(outputs)\n\n    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n    for epoch in range(num_epochs):\n        for batch_num in range(num_batches_per_epoch):\n            start_index = batch_num * batch_size\n            end_index = min((batch_num + 1) * batch_size, len(inputs))\n            yield inputs[start_index:end_index], outputs[start_index:end_index]\n\n\ndef get_init_embedding(reversed_dict, embedding_size):\n    glove_file = ""glove/glove.42B.300d.txt""\n    word2vec_file = get_tmpfile(""word2vec_format.vec"")\n    glove2word2vec(glove_file, word2vec_file)\n    print(""Loading Glove vectors..."")\n    word_vectors = KeyedVectors.load_word2vec_format(word2vec_file)\n\n    word_vec_list = list()\n    for _, word in sorted(reversed_dict.items()):\n        try:\n            word_vec = word_vectors.word_vec(word)\n        except KeyError:\n            word_vec = np.zeros([embedding_size], dtype=np.float32)\n\n        word_vec_list.append(word_vec)\n\n    # Assign random vector to <s>, </s> token\n    word_vec_list[2] = np.random.normal(0, 1, embedding_size)\n    word_vec_list[3] = np.random.normal(0, 1, embedding_size)\n\n    return np.array(word_vec_list)\n'"
