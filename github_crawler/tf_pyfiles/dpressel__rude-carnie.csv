file_path,api_count,code
data.py,40,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport os\nimport numpy as np\nimport tensorflow as tf\n\nfrom distutils.version import LooseVersion\n\nVERSION_GTE_0_12_0 = LooseVersion(tf.__version__) >= LooseVersion(\'0.12.0\')\n\n# Name change in TF v 0.12.0\nif VERSION_GTE_0_12_0:\n    standardize_image = tf.image.per_image_standardization\nelse:\n    standardize_image = tf.image.per_image_whitening\n\ndef data_files(data_dir, subset):\n    """"""Returns a python list of all (sharded) data subset files.\n    Returns:\n      python list of all (sharded) data set files.\n    Raises:\n      ValueError: if there are not data_files matching the subset.\n    """"""\n    if subset not in [\'train\', \'validation\']:\n        print(\'Invalid subset!\')\n        exit(-1)\n\n    tf_record_pattern = os.path.join(data_dir, \'%s-*\' % subset)\n    data_files = tf.gfile.Glob(tf_record_pattern)\n    print(data_files)\n    if not data_files:\n      print(\'No files found for data dir %s at %s\' % (subset, data_dir))\n\n      exit(-1)\n    return data_files\n\ndef decode_jpeg(image_buffer, scope=None):\n  """"""Decode a JPEG string into one 3-D float image Tensor.\n  Args:\n    image_buffer: scalar string Tensor.\n    scope: Optional scope for op_scope.\n  Returns:\n    3-D float Tensor with values ranging from [0, 1).\n  """"""\n  with tf.op_scope([image_buffer], scope, \'decode_jpeg\'):\n    # Decode the string as an RGB JPEG.\n    # Note that the resulting image contains an unknown height and width\n    # that is set dynamically by decode_jpeg. In other words, the height\n    # and width of image is unknown at compile-time.\n    image = tf.image.decode_jpeg(image_buffer, channels=3)\n\n    # After this point, all image pixels reside in [0,1)\n    # until the very end, when they\'re rescaled to (-1, 1).  The various\n    # adjust_* ops all require this range for dtype float.\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image\n\ndef distort_image(image, height, width):\n\n  # Image processing for training the network. Note the many random\n  # distortions applied to the image.\n\n  distorted_image = tf.random_crop(image, [height, width, 3])\n\n  #distorted_image = tf.image.resize_images(image, [height, width])\n\n  # Randomly flip the image horizontally.\n  distorted_image = tf.image.random_flip_left_right(distorted_image)\n\n  # Because these operations are not commutative, consider randomizing\n  # the order their operation.\n\n  distorted_image = tf.image.random_brightness(distorted_image,\n                                               max_delta=63)\n\n  distorted_image = tf.image.random_contrast(distorted_image,\n                                             lower=0.2, upper=1.8)\n\n  return distorted_image\n\n\ndef _is_tensor(x):\n    return isinstance(x, (tf.Tensor, tf.Variable))\n\ndef eval_image(image, height, width):\n    return tf.image.resize_images(image, [height, width])\n\ndef data_normalization(image):\n\n    image = standardize_image(image)\n\n    return image\n\ndef image_preprocessing(image_buffer, image_size, train, thread_id=0):\n    """"""Decode and preprocess one image for evaluation or training.\n    Args:\n    image_buffer: JPEG encoded string Tensor\n    train: boolean\n    thread_id: integer indicating preprocessing thread\n    Returns:\n    3-D float Tensor containing an appropriately scaled image\n    Raises:\n    ValueError: if user does not provide bounding box\n    """"""\n\n    image = decode_jpeg(image_buffer)\n    \n    if train:\n        image = distort_image(image, image_size, image_size)\n    else:\n        image = eval_image(image, image_size, image_size)\n        \n    image = data_normalization(image)\n    return image\n\n\ndef parse_example_proto(example_serialized):\n  # Dense features in Example proto.\n  feature_map = {\n      \'image/encoded\': tf.FixedLenFeature([], dtype=tf.string,\n                                          default_value=\'\'),\n      \'image/filename\': tf.FixedLenFeature([], dtype=tf.string,\n                                          default_value=\'\'),\n\n      \'image/class/label\': tf.FixedLenFeature([1], dtype=tf.int64,\n                                              default_value=-1),\n      \'image/class/text\': tf.FixedLenFeature([], dtype=tf.string,\n                                             default_value=\'\'),\n      \'image/height\': tf.FixedLenFeature([1], dtype=tf.int64,\n                                         default_value=-1),\n      \'image/width\': tf.FixedLenFeature([1], dtype=tf.int64,\n                                         default_value=-1),\n\n  }\n\n  features = tf.parse_single_example(example_serialized, feature_map)\n  label = tf.cast(features[\'image/class/label\'], dtype=tf.int32)\n  return features[\'image/encoded\'], label, features[\'image/filename\']\n\ndef batch_inputs(data_dir, batch_size, image_size, train, num_preprocess_threads=4,\n                 num_readers=1, input_queue_memory_factor=16):\n  with tf.name_scope(\'batch_processing\'):\n\n    if train:\n        files = data_files(data_dir, \'train\')\n        filename_queue = tf.train.string_input_producer(files,\n                                                        shuffle=True,\n                                                        capacity=16)\n    else:\n        files = data_files(data_dir, \'validation\')\n        filename_queue = tf.train.string_input_producer(files,\n                                                        shuffle=False,\n                                                        capacity=1)\n    if num_preprocess_threads % 4:\n              raise ValueError(\'Please make num_preprocess_threads a multiple \'\n                       \'of 4 (%d % 4 != 0).\', num_preprocess_threads)\n\n    if num_readers < 1:\n      raise ValueError(\'Please make num_readers at least 1\')\n\n    # Approximate number of examples per shard.\n    examples_per_shard = 1024\n    # Size the random shuffle queue to balance between good global\n    # mixing (more examples) and memory use (fewer examples).\n    # 1 image uses 299*299*3*4 bytes = 1MB\n    # The default input_queue_memory_factor is 16 implying a shuffling queue\n    # size: examples_per_shard * 16 * 1MB = 17.6GB\n    min_queue_examples = examples_per_shard * input_queue_memory_factor\n    if train:\n      examples_queue = tf.RandomShuffleQueue(\n          capacity=min_queue_examples + 3 * batch_size,\n          min_after_dequeue=min_queue_examples,\n          dtypes=[tf.string])\n    else:\n      examples_queue = tf.FIFOQueue(\n          capacity=examples_per_shard + 3 * batch_size,\n          dtypes=[tf.string])\n\n    # Create multiple readers to populate the queue of examples.\n    if num_readers > 1:\n      enqueue_ops = []\n      for _ in range(num_readers):\n        reader = tf.TFRecordReader()\n        _, value = reader.read(filename_queue)\n        enqueue_ops.append(examples_queue.enqueue([value]))\n\n      tf.train.queue_runner.add_queue_runner(\n          tf.train.queue_runner.QueueRunner(examples_queue, enqueue_ops))\n      example_serialized = examples_queue.dequeue()\n    else:\n      reader = tf.TFRecordReader()\n      _, example_serialized = reader.read(filename_queue)\n\n    images_labels_fnames = []\n    for thread_id in range(num_preprocess_threads):\n      # Parse a serialized Example proto to extract the image and metadata.\n      image_buffer, label_index, fname = parse_example_proto(example_serialized)\n          \n      image = image_preprocessing(image_buffer, image_size, train, thread_id)\n      images_labels_fnames.append([image, label_index, fname])\n\n    images, label_index_batch, fnames = tf.train.batch_join(\n        images_labels_fnames,\n        batch_size=batch_size,\n        capacity=2 * num_preprocess_threads * batch_size)\n\n    images = tf.cast(images, tf.float32)\n    images = tf.reshape(images, shape=[batch_size, image_size, image_size, 3])\n\n    # Display the training images in the visualizer.\n    tf.summary.image(\'images\', images, 20)\n\n    return images, tf.reshape(label_index_batch, [batch_size]), fnames\n\ndef inputs(data_dir, batch_size=128, image_size=227, train=False, num_preprocess_threads=4):\n    with tf.device(\'/cpu:0\'):\n        images, labels, filenames = batch_inputs(\n            data_dir, batch_size, image_size, train,\n            num_preprocess_threads=num_preprocess_threads,\n            num_readers=1)\n    return images, labels, filenames\n\ndef distorted_inputs(data_dir, batch_size=128, image_size=227, num_preprocess_threads=4):\n\n  # Force all input processing onto CPU in order to reserve the GPU for\n  # the forward inference and back-propagation.\n  with tf.device(\'/cpu:0\'):\n    images, labels, filenames = batch_inputs(\n        data_dir, batch_size, image_size, train=True,\n        num_preprocess_threads=num_preprocess_threads,\n        num_readers=1)\n  return images, labels, filenames\n'"
detect.py,0,"b""import numpy as np\nimport cv2\nFACE_PAD = 50\n\nclass ObjectDetector(object):\n    def __init__(self):\n        pass\n\n    def run(self, image_file):\n        pass\n\n# OpenCV's cascade object detector\nclass ObjectDetectorCascadeOpenCV(ObjectDetector):\n    def __init__(self, model_name, basename='frontal-face', tgtdir='.', min_height_dec=20, min_width_dec=20,\n                 min_height_thresh=50, min_width_thresh=50):\n        self.min_height_dec = min_height_dec\n        self.min_width_dec = min_width_dec\n        self.min_height_thresh = min_height_thresh\n        self.min_width_thresh = min_width_thresh\n        self.tgtdir = tgtdir\n        self.basename = basename\n        self.face_cascade = cv2.CascadeClassifier(model_name)\n\n    def run(self, image_file):\n        print(image_file)\n        img = cv2.imread(image_file)\n        min_h = int(max(img.shape[0] / self.min_height_dec, self.min_height_thresh))\n        min_w = int(max(img.shape[1] / self.min_width_dec, self.min_width_thresh))\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        faces = self.face_cascade.detectMultiScale(gray, 1.3, minNeighbors=5, minSize=(min_h, min_w))\n\n        images = []\n        for i, (x, y, w, h) in enumerate(faces):\n            images.append(self.sub_image('%s/%s-%d.jpg' % (self.tgtdir, self.basename, i + 1), img, x, y, w, h))\n\n        print('%d faces detected' % len(images))\n\n        for (x, y, w, h) in faces:\n            self.draw_rect(img, x, y, w, h)\n            # Fix in case nothing found in the image\n        outfile = '%s/%s.jpg' % (self.tgtdir, self.basename)\n        cv2.imwrite(outfile, img)\n        return images, outfile\n\n    def sub_image(self, name, img, x, y, w, h):\n        upper_cut = [min(img.shape[0], y + h + FACE_PAD), min(img.shape[1], x + w + FACE_PAD)]\n        lower_cut = [max(y - FACE_PAD, 0), max(x - FACE_PAD, 0)]\n        roi_color = img[lower_cut[0]:upper_cut[0], lower_cut[1]:upper_cut[1]]\n        cv2.imwrite(name, roi_color)\n        return name\n\n    def draw_rect(self, img, x, y, w, h):\n        upper_cut = [min(img.shape[0], y + h + FACE_PAD), min(img.shape[1], x + w + FACE_PAD)]\n        lower_cut = [max(y - FACE_PAD, 0), max(x - FACE_PAD, 0)]\n        cv2.rectangle(img, (lower_cut[1], lower_cut[0]), (upper_cut[1], upper_cut[0]), (255, 0, 0), 2)\n\n"""
dlibdetect.py,0,"b""from detect import ObjectDetector\n\nimport dlib\nimport cv2\nFACE_PAD = 50\n\nclass FaceDetectorDlib(ObjectDetector):\n    def __init__(self, model_name, basename='frontal-face', tgtdir='.'):\n        self.tgtdir = tgtdir\n        self.basename = basename\n        self.detector = dlib.get_frontal_face_detector()\n        self.predictor = dlib.shape_predictor(model_name)\n\n    def run(self, image_file):\n        print(image_file)\n        img = cv2.imread(image_file)\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        faces = self.detector(gray, 1)\n        images = []\n        bb = []\n        for (i, rect) in enumerate(faces):\n            x = rect.left()\n            y = rect.top()\n            w = rect.right() - x\n            h = rect.bottom() - y\n            bb.append((x,y,w,h))\n            images.append(self.sub_image('%s/%s-%d.jpg' % (self.tgtdir, self.basename, i + 1), img, x, y, w, h))\n\n        print('%d faces detected' % len(images))\n\n        for (x, y, w, h) in bb:\n            self.draw_rect(img, x, y, w, h)\n            # Fix in case nothing found in the image\n        outfile = '%s/%s.jpg' % (self.tgtdir, self.basename)\n        cv2.imwrite(outfile, img)\n        return images, outfile\n\n    def sub_image(self, name, img, x, y, w, h):\n        upper_cut = [min(img.shape[0], y + h + FACE_PAD), min(img.shape[1], x + w + FACE_PAD)]\n        lower_cut = [max(y - FACE_PAD, 0), max(x - FACE_PAD, 0)]\n        roi_color = img[lower_cut[0]:upper_cut[0], lower_cut[1]:upper_cut[1]]\n        cv2.imwrite(name, roi_color)\n        return name\n\n    def draw_rect(self, img, x, y, w, h):\n        upper_cut = [min(img.shape[0], y + h + FACE_PAD), min(img.shape[1], x + w + FACE_PAD)]\n        lower_cut = [max(y - FACE_PAD, 0), max(x - FACE_PAD, 0)]\n        cv2.rectangle(img, (lower_cut[1], lower_cut[0]), (upper_cut[1], upper_cut[0]), (255, 0, 0), 2)\n"""
eval.py,30,"b'""""""\nAt each tick, evaluate the latest checkpoint against some validation data.\nOr, you can run once by passing --run_once.  OR, you can pass a --requested_step_seq of comma separated checkpoint #s that already exist that it can run in a row.\n\nThis program expects a training base directory with the data, and md.json file\nThere will be sub-directories for each run underneath with the name run-<PID>\nwhere <PID> is the training program\'s process ID.  To run this program, you\nwill need to pass --train_dir <DIR> which is the base path name, --run_id <PID>\nand if you are using a custom name for your checkpoint, you should\npass that as well (most times you probably wont).  This will yield a model path:\n<DIR>/run-<PID>/checkpoint\n\nNote: If you are training to use the same GPU you can supposedly \nsuspend the process.  I have not found this works reliably on my Linux machine.\nInstead, I have found that, often times, the GPU will not reclaim the resources\nand in that case, your eval may run out of GPU memory.\n\nYou can alternately run trainining for a number of steps, break the program\nand run this, then restarting training from the old checkpoint.  I also\nfound this inconvenient.  In order to control this better, the program\nrequires that you explict placement of inference.  It defaults to the CPU\nso that it can easily run side by side with training.  This does make it\nmuch slower than if it was on the GPU, but for evaluation this may not be\na major problem.  To place on the gpu, just pass --device_id /gpu:<ID> where\n<ID> is the GPU ID\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport math\nimport time\nfrom data import inputs\nimport numpy as np\nimport tensorflow as tf\nfrom model import select_model, get_checkpoint\nimport os\nimport json\n\ntf.app.flags.DEFINE_string(\'train_dir\', \'/home/dpressel/dev/work/AgeGenderDeepLearning/Folds/tf/test_fold_is_0\',\n                           \'Training directory (where training data lives)\')\n\ntf.app.flags.DEFINE_integer(\'run_id\', 0,\n                           \'This is the run number (pid) for training proc\')\n\ntf.app.flags.DEFINE_string(\'device_id\', \'/cpu:0\',\n                           \'What processing unit to execute inference on\')\n\ntf.app.flags.DEFINE_string(\'eval_dir\', \'/home/dpressel/dev/work/AgeGenderDeepLearning/Folds/tf/eval_test_fold_is_0\',\n                           \'Directory to put output to\')\n\ntf.app.flags.DEFINE_string(\'eval_data\', \'valid\',\n                           \'Data type (valid|train)\')\n\ntf.app.flags.DEFINE_integer(\'num_preprocess_threads\', 4,\n                            \'Number of preprocessing threads\')\n\ntf.app.flags.DEFINE_integer(\'eval_interval_secs\', 60 * 5,\n                            """"""How often to run the eval."""""")\ntf.app.flags.DEFINE_integer(\'num_examples\', 10000,\n                            """"""Number of examples to run."""""")\ntf.app.flags.DEFINE_boolean(\'run_once\', False,\n                         """"""Whether to run eval only once."""""")\n\ntf.app.flags.DEFINE_integer(\'image_size\', 227,\n                            \'Image size\')\n\ntf.app.flags.DEFINE_integer(\'batch_size\', 128,\n                            \'Batch size\')\n\ntf.app.flags.DEFINE_string(\'checkpoint\', \'checkpoint\',\n                          \'Checkpoint basename\')\n\n\ntf.app.flags.DEFINE_string(\'model_type\', \'default\',\n                           \'Type of convnet\')\n\ntf.app.flags.DEFINE_string(\'requested_step_seq\', \'\', \'Requested step to restore\')\nFLAGS = tf.app.flags.FLAGS\n\n        \n\ndef eval_once(saver, summary_writer, summary_op, logits, labels, num_eval, requested_step=None):\n    """"""Run Eval once.\n    Args:\n    saver: Saver.\n    summary_writer: Summary writer.\n    top_k_op: Top K op.\n    summary_op: Summary op.\n    """"""\n    top1 = tf.nn.in_top_k(logits, labels, 1)\n    top2 = tf.nn.in_top_k(logits, labels, 2)\n\n    with tf.Session() as sess:\n        checkpoint_path = \'%s/run-%d\' % (FLAGS.train_dir, FLAGS.run_id)\n\n        model_checkpoint_path, global_step = get_checkpoint(checkpoint_path, requested_step, FLAGS.checkpoint)\n\n        saver.restore(sess, model_checkpoint_path)\n\n        # Start the queue runners.\n        coord = tf.train.Coordinator()\n        try:\n            threads = []\n            for qr in tf.get_collection(tf.GraphKeys.QUEUE_RUNNERS):\n                threads.extend(qr.create_threads(sess, coord=coord, daemon=True,\n                                                 start=True))\n            num_steps = int(math.ceil(num_eval / FLAGS.batch_size))\n            true_count1 = true_count2 = 0\n            total_sample_count = num_steps * FLAGS.batch_size\n            step = 0\n            print(FLAGS.batch_size, num_steps)\n\n            while step < num_steps and not coord.should_stop():\n                start_time = time.time()\n                v, predictions1, predictions2 = sess.run([logits, top1, top2])\n                duration = time.time() - start_time\n                sec_per_batch = float(duration)\n                examples_per_sec = FLAGS.batch_size / sec_per_batch\n\n                true_count1 += np.sum(predictions1)\n                true_count2 += np.sum(predictions2)\n                format_str = (\'%s (%.1f examples/sec; %.3f sec/batch)\')\n                print(format_str % (datetime.now(),\n                                    examples_per_sec, sec_per_batch))\n\n                step += 1\n\n            # Compute precision @ 1.\n\n            at1 = true_count1 / total_sample_count\n            at2 = true_count2 / total_sample_count\n            print(\'%s: precision @ 1 = %.3f (%d/%d)\' % (datetime.now(), at1, true_count1, total_sample_count))\n            print(\'%s:    recall @ 2 = %.3f (%d/%d)\' % (datetime.now(), at2, true_count2, total_sample_count))\n\n            summary = tf.Summary()\n            summary.ParseFromString(sess.run(summary_op))\n            summary.value.add(tag=\'Precision @ 1\', simple_value=at1)\n            summary.value.add(tag=\'   Recall @ 2\', simple_value=at2)\n            summary_writer.add_summary(summary, global_step)\n        except Exception as e:  # pylint: disable=broad-except\n            coord.request_stop(e)\n\n        coord.request_stop()\n        coord.join(threads, stop_grace_period_secs=10)\n\ndef evaluate(run_dir):\n    with tf.Graph().as_default() as g:\n        input_file = os.path.join(FLAGS.train_dir, \'md.json\')\n        print(input_file)\n        with open(input_file, \'r\') as f:\n            md = json.load(f)\n\n        eval_data = FLAGS.eval_data == \'valid\'\n        num_eval = md[\'%s_counts\' % FLAGS.eval_data]\n\n        model_fn = select_model(FLAGS.model_type)\n\n\n        with tf.device(FLAGS.device_id):\n            print(\'Executing on %s\' % FLAGS.device_id)\n            images, labels, _ = inputs(FLAGS.train_dir, FLAGS.batch_size, FLAGS.image_size, train=not eval_data, num_preprocess_threads=FLAGS.num_preprocess_threads)\n            logits = model_fn(md[\'nlabels\'], images, 1, False)\n            summary_op = tf.summary.merge_all()\n            \n            summary_writer = tf.summary.FileWriter(run_dir, g)\n            saver = tf.train.Saver()\n            \n            if FLAGS.requested_step_seq:\n                sequence = FLAGS.requested_step_seq.split(\',\')\n                for requested_step in sequence:\n                    print(\'Running %s\' % sequence)\n                    eval_once(saver, summary_writer, summary_op, logits, labels, num_eval, requested_step)\n            else:\n                while True:\n                    print(\'Running loop\')\n                    eval_once(saver, summary_writer, summary_op, logits, labels, num_eval)\n                    if FLAGS.run_once:\n                        break\n                    time.sleep(FLAGS.eval_interval_secs)\n\n                \ndef main(argv=None):  # pylint: disable=unused-argument\n    run_dir = \'%s/run-%d\' % (FLAGS.eval_dir, FLAGS.run_id)\n    if tf.gfile.Exists(run_dir):\n        tf.gfile.DeleteRecursively(run_dir)\n    tf.gfile.MakeDirs(run_dir)\n    evaluate(run_dir)\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
export.py,43,"b'import tensorflow as tf\nfrom model import select_model, get_checkpoint\nfrom utils import RESIZE_AOI, RESIZE_FINAL\nfrom tensorflow.python.framework import graph_util\nfrom tensorflow.contrib.learn.python.learn.utils import export\nfrom tensorflow.python.saved_model import builder as saved_model_builder\nfrom tensorflow.python.saved_model import signature_constants\nfrom tensorflow.python.saved_model import signature_def_utils\nfrom tensorflow.python.saved_model import tag_constants\nfrom tensorflow.python.saved_model import utils\n\nimport os\n\nGENDER_LIST =[\'M\',\'F\']\nAGE_LIST = [\'(0, 2)\',\'(4, 6)\',\'(8, 12)\',\'(15, 20)\',\'(25, 32)\',\'(38, 43)\',\'(48, 53)\',\'(60, 100)\']\n\ntf.app.flags.DEFINE_string(\'checkpoint\', \'checkpoint\',\n                           \'Checkpoint basename\')\n\ntf.app.flags.DEFINE_string(\'class_type\', \'age\',\n                           \'Classification type (age|gender)\')\n\ntf.app.flags.DEFINE_string(\'model_dir\', \'\',\n                           \'Model directory (where training data lives)\')\n\ntf.app.flags.DEFINE_integer(\'model_version\', 1,\n                            """"""Version number of the model."""""")\n\ntf.app.flags.DEFINE_string(\'output_dir\', \'/tmp/tf_exported_model/0\',\n                           \'Export directory\')\n\ntf.app.flags.DEFINE_string(\'model_type\', \'default\',\n                           \'Type of convnet\')\n\ntf.app.flags.DEFINE_string(\'requested_step\', \'\', \'Within the model directory, a requested step to restore e.g., 9000\')\n\nFLAGS = tf.app.flags.FLAGS\n\ndef preproc_jpeg(image_buffer):\n    image = tf.image.decode_jpeg(image_buffer, channels=3)\n    crop = tf.image.resize_images(image, (RESIZE_AOI, RESIZE_AOI))\n    # What??\n    crop = tf.image.resize_images(crop, (RESIZE_FINAL, RESIZE_FINAL))    \n    image_out = tf.image.per_image_standardization(crop)\n    return image_out\n\ndef main(argv=None):\n    with tf.Graph().as_default():\n\n        serialized_tf_example = tf.placeholder(tf.string, name=\'tf_example\')\n        feature_configs = {\n            \'image/encoded\': tf.FixedLenFeature(shape=[], dtype=tf.string),\n        }\n        tf_example = tf.parse_example(serialized_tf_example, feature_configs)\n        jpegs = tf_example[\'image/encoded\']\n\n        images = tf.map_fn(preproc_jpeg, jpegs, dtype=tf.float32)\n        label_list = AGE_LIST if FLAGS.class_type == \'age\' else GENDER_LIST\n        nlabels = len(label_list)\n\n        config = tf.ConfigProto(allow_soft_placement=True)\n        with tf.Session(config=config) as sess:\n\n            model_fn = select_model(FLAGS.model_type)\n            logits = model_fn(nlabels, images, 1, False)\n            softmax_output = tf.nn.softmax(logits)\n            values, indices = tf.nn.top_k(softmax_output, 2 if FLAGS.class_type == \'age\' else 1)\n            class_tensor = tf.constant(label_list)\n            table = tf.contrib.lookup.index_to_string_table_from_tensor(class_tensor)\n            classes = table.lookup(tf.to_int64(indices))\n            requested_step = FLAGS.requested_step if FLAGS.requested_step else None\n            checkpoint_path = \'%s\' % (FLAGS.model_dir)\n            model_checkpoint_path, global_step = get_checkpoint(checkpoint_path, requested_step, FLAGS.checkpoint)\n\n            saver = tf.train.Saver()\n            saver.restore(sess, model_checkpoint_path)\n            print(\'Restored model checkpoint %s\' % model_checkpoint_path)\n\n            output_path = os.path.join(\n                tf.compat.as_bytes(FLAGS.output_dir),\n                tf.compat.as_bytes(str(FLAGS.model_version)))\n            print(\'Exporting trained model to %s\' % output_path)\n            builder = tf.saved_model.builder.SavedModelBuilder(output_path)\n\n            # Build the signature_def_map.\n            classify_inputs_tensor_info = tf.saved_model.utils.build_tensor_info(\n                serialized_tf_example)\n            classes_output_tensor_info = tf.saved_model.utils.build_tensor_info(\n                classes)\n            scores_output_tensor_info = tf.saved_model.utils.build_tensor_info(values)\n            classification_signature = (\n                tf.saved_model.signature_def_utils.build_signature_def(\n                    inputs={\n                        tf.saved_model.signature_constants.CLASSIFY_INPUTS:\n                    classify_inputs_tensor_info\n                    },\n                    outputs={\n                    tf.saved_model.signature_constants.CLASSIFY_OUTPUT_CLASSES:\n                        classes_output_tensor_info,\n                        tf.saved_model.signature_constants.CLASSIFY_OUTPUT_SCORES:\n                        scores_output_tensor_info\n                    },\n                    method_name=tf.saved_model.signature_constants.\n                    CLASSIFY_METHOD_NAME))\n            \n            predict_inputs_tensor_info = tf.saved_model.utils.build_tensor_info(jpegs)\n            prediction_signature = (\n                tf.saved_model.signature_def_utils.build_signature_def(\n                    inputs={\'images\': predict_inputs_tensor_info},\n                    outputs={\n                        \'classes\': classes_output_tensor_info,\n                        \'scores\': scores_output_tensor_info\n                    },\n                    method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n                ))\n            \n            legacy_init_op = tf.group(tf.tables_initializer(), name=\'legacy_init_op\')\n            builder.add_meta_graph_and_variables(\n                sess, [tf.saved_model.tag_constants.SERVING],\n                signature_def_map={\n                    \'predict_images\':\n                    prediction_signature,\n                    tf.saved_model.signature_constants.\n                    DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n                    classification_signature,\n                },\n                legacy_init_op=legacy_init_op)\n            \n            builder.save()\n            print(\'Successfully exported model to %s\' % FLAGS.output_dir)\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
filter_by_face.py,6,"b""import numpy as np\nimport tensorflow as tf\nimport os\nimport cv2\nimport time\nimport sys\nfrom utils import *\nimport csv\n\n# YOLO tiny\n#python fd.py --filename /media/dpressel/xdata/insights/converted/ --face_detection_model weights/YOLO_tiny.ckpt --face_detection_type yolo_tiny --target yolo.csv\n\n# CV2\n\n#python fd.py --filename /media/dpressel/xdata/insights/converted/ --face_detection_model /usr/share/opencv/haarcascades/haarcascade_frontalface_default.xml --target cascade.csv\n\ntf.app.flags.DEFINE_string('filename', '',\n                           'File (Image) or File list (Text/No header TSV) to process')\n\ntf.app.flags.DEFINE_string('face_detection_model', '', 'Do frontal face detection with model specified')\n\ntf.app.flags.DEFINE_string('face_detection_type', 'cascade', 'Face detection model type (yolo_tiny|cascade)')\n\ntf.app.flags.DEFINE_string('target', None, 'Target file name (defaults to {face_detection_model}.csv')\nFACE_PAD = 0\nFLAGS = tf.app.flags.FLAGS\n\ndef list_images(srcfile):\n    with open(srcfile, 'r') as csvfile:\n        delim = ',' if srcfile.endswith('.csv') else '\\t'\n        reader = csv.reader(csvfile, delimiter=delim)\n        if srcfile.endswith('.csv') or srcfile.endswith('.tsv'):\n            print('skipping header')\n            _ = next(reader)\n        \n        return [row[0] for row in reader]\n\ndef main(argv=None):  # pylint: disable=unused-argument\n\n    fd = face_detection_model(FLAGS.face_detection_type, FLAGS.face_detection_model)\n    files = []\n    contains_faces = []\n\n    target = FLAGS.target = '%s.csv' % FLAGS.face_detection_type if FLAGS.target is None else FLAGS.target\n\n    print('Creating output file %s' % target)\n    output = open(target, 'w')\n    writer = csv.writer(output)\n    writer.writerow(('file_with_face',))\n\n    if FLAGS.filename is not None:\n        if os.path.isdir(FLAGS.filename):\n            for relpath in os.listdir(FLAGS.filename):\n                abspath = os.path.join(FLAGS.filename, relpath)\n                if os.path.isfile(abspath) and any([abspath.endswith('.' + ty) for ty in ('jpg', 'png', 'JPG', 'PNG', 'jpeg')]):\n                    print(abspath)\n                    files.append(abspath)\n        elif any([FLAGS.filename.endswith('.' + ty) for ty in ('csv', 'tsv', 'txt')]):\n            files = list_images(FLAGS.filename)\n        else:\n            files = [FLAGS.filename]\n\n    for f in files:\n        try:\n            images, outfile = fd.run(f)\n            if len(images):\n                print(f, 'YES')\n                writer.writerow((f,))\n                contains_faces.append(f)\n            else:\n                print(f, 'NO')\n        except Exception as e:\n            print(e)\n\nif __name__=='__main__':\n    tf.app.run()\n"""
guess.py,20,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport math\nimport time\nfrom data import inputs\nimport numpy as np\nimport tensorflow as tf\nfrom model import select_model, get_checkpoint\nfrom utils import *\nimport os\nimport json\nimport csv\n\nRESIZE_FINAL = 227\nGENDER_LIST =['M','F']\nAGE_LIST = ['(0, 2)','(4, 6)','(8, 12)','(15, 20)','(25, 32)','(38, 43)','(48, 53)','(60, 100)']\nMAX_BATCH_SZ = 128\n\ntf.app.flags.DEFINE_string('model_dir', '',\n                           'Model directory (where training data lives)')\n\ntf.app.flags.DEFINE_string('class_type', 'age',\n                           'Classification type (age|gender)')\n\n\ntf.app.flags.DEFINE_string('device_id', '/cpu:0',\n                           'What processing unit to execute inference on')\n\ntf.app.flags.DEFINE_string('filename', '',\n                           'File (Image) or File list (Text/No header TSV) to process')\n\ntf.app.flags.DEFINE_string('target', '',\n                           'CSV file containing the filename processed along with best guess and score')\n\ntf.app.flags.DEFINE_string('checkpoint', 'checkpoint',\n                          'Checkpoint basename')\n\ntf.app.flags.DEFINE_string('model_type', 'default',\n                           'Type of convnet')\n\ntf.app.flags.DEFINE_string('requested_step', '', 'Within the model directory, a requested step to restore e.g., 9000')\n\ntf.app.flags.DEFINE_boolean('single_look', False, 'single look at the image or multiple crops')\n\ntf.app.flags.DEFINE_string('face_detection_model', '', 'Do frontal face detection with model specified')\n\ntf.app.flags.DEFINE_string('face_detection_type', 'cascade', 'Face detection model type (yolo_tiny|cascade)')\n\nFLAGS = tf.app.flags.FLAGS\n\ndef one_of(fname, types):\n    return any([fname.endswith('.' + ty) for ty in types])\n\ndef resolve_file(fname):\n    if os.path.exists(fname): return fname\n    for suffix in ('.jpg', '.png', '.JPG', '.PNG', '.jpeg'):\n        cand = fname + suffix\n        if os.path.exists(cand):\n            return cand\n    return None\n\n\ndef classify_many_single_crop(sess, label_list, softmax_output, coder, images, image_files, writer):\n    try:\n        num_batches = math.ceil(len(image_files) / MAX_BATCH_SZ)\n        pg = ProgressBar(num_batches)\n        for j in range(num_batches):\n            start_offset = j * MAX_BATCH_SZ\n            end_offset = min((j + 1) * MAX_BATCH_SZ, len(image_files))\n            \n            batch_image_files = image_files[start_offset:end_offset]\n            print(start_offset, end_offset, len(batch_image_files))\n            image_batch = make_multi_image_batch(batch_image_files, coder)\n            batch_results = sess.run(softmax_output, feed_dict={images:image_batch.eval()})\n            batch_sz = batch_results.shape[0]\n            for i in range(batch_sz):\n                output_i = batch_results[i]\n                best_i = np.argmax(output_i)\n                best_choice = (label_list[best_i], output_i[best_i])\n                print('Guess @ 1 %s, prob = %.2f' % best_choice)\n                if writer is not None:\n                    f = batch_image_files[i]\n                    writer.writerow((f, best_choice[0], '%.2f' % best_choice[1]))\n            pg.update()\n        pg.done()\n    except Exception as e:\n        print(e)\n        print('Failed to run all images')\n\ndef classify_one_multi_crop(sess, label_list, softmax_output, coder, images, image_file, writer):\n    try:\n\n        print('Running file %s' % image_file)\n        image_batch = make_multi_crop_batch(image_file, coder)\n\n        batch_results = sess.run(softmax_output, feed_dict={images:image_batch.eval()})\n        output = batch_results[0]\n        batch_sz = batch_results.shape[0]\n    \n        for i in range(1, batch_sz):\n            output = output + batch_results[i]\n        \n        output /= batch_sz\n        best = np.argmax(output)\n        best_choice = (label_list[best], output[best])\n        print('Guess @ 1 %s, prob = %.2f' % best_choice)\n    \n        nlabels = len(label_list)\n        if nlabels > 2:\n            output[best] = 0\n            second_best = np.argmax(output)\n            print('Guess @ 2 %s, prob = %.2f' % (label_list[second_best], output[second_best]))\n\n        if writer is not None:\n            writer.writerow((image_file, best_choice[0], '%.2f' % best_choice[1]))\n    except Exception as e:\n        print(e)\n        print('Failed to run image %s ' % image_file)\n\ndef list_images(srcfile):\n    with open(srcfile, 'r') as csvfile:\n        delim = ',' if srcfile.endswith('.csv') else '\\t'\n        reader = csv.reader(csvfile, delimiter=delim)\n        if srcfile.endswith('.csv') or srcfile.endswith('.tsv'):\n            print('skipping header')\n            _ = next(reader)\n        \n        return [row[0] for row in reader]\n\ndef main(argv=None):  # pylint: disable=unused-argument\n\n    files = []\n    \n    if FLAGS.face_detection_model:\n        print('Using face detector (%s) %s' % (FLAGS.face_detection_type, FLAGS.face_detection_model))\n        face_detect = face_detection_model(FLAGS.face_detection_type, FLAGS.face_detection_model)\n        face_files, rectangles = face_detect.run(FLAGS.filename)\n        print(face_files)\n        files += face_files\n\n    config = tf.ConfigProto(allow_soft_placement=True)\n    with tf.Session(config=config) as sess:\n\n        label_list = AGE_LIST if FLAGS.class_type == 'age' else GENDER_LIST\n        nlabels = len(label_list)\n\n        print('Executing on %s' % FLAGS.device_id)\n        model_fn = select_model(FLAGS.model_type)\n\n        with tf.device(FLAGS.device_id):\n            \n            images = tf.placeholder(tf.float32, [None, RESIZE_FINAL, RESIZE_FINAL, 3])\n            logits = model_fn(nlabels, images, 1, False)\n            init = tf.global_variables_initializer()\n            \n            requested_step = FLAGS.requested_step if FLAGS.requested_step else None\n        \n            checkpoint_path = '%s' % (FLAGS.model_dir)\n\n            model_checkpoint_path, global_step = get_checkpoint(checkpoint_path, requested_step, FLAGS.checkpoint)\n            \n            saver = tf.train.Saver()\n            saver.restore(sess, model_checkpoint_path)\n                        \n            softmax_output = tf.nn.softmax(logits)\n\n            coder = ImageCoder()\n\n            # Support a batch mode if no face detection model\n            if len(files) == 0:\n                if (os.path.isdir(FLAGS.filename)):\n                    for relpath in os.listdir(FLAGS.filename):\n                        abspath = os.path.join(FLAGS.filename, relpath)\n                        \n                        if os.path.isfile(abspath) and any([abspath.endswith('.' + ty) for ty in ('jpg', 'png', 'JPG', 'PNG', 'jpeg')]):\n                            print(abspath)\n                            files.append(abspath)\n                else:\n                    files.append(FLAGS.filename)\n                    # If it happens to be a list file, read the list and clobber the files\n                    if any([FLAGS.filename.endswith('.' + ty) for ty in ('csv', 'tsv', 'txt')]):\n                        files = list_images(FLAGS.filename)\n                \n            writer = None\n            output = None\n            if FLAGS.target:\n                print('Creating output file %s' % FLAGS.target)\n                output = open(FLAGS.target, 'w')\n                writer = csv.writer(output)\n                writer.writerow(('file', 'label', 'score'))\n            image_files = list(filter(lambda x: x is not None, [resolve_file(f) for f in files]))\n            print(image_files)\n            if FLAGS.single_look:\n                classify_many_single_crop(sess, label_list, softmax_output, coder, images, image_files, writer)\n\n            else:\n                for image_file in image_files:\n                    classify_one_multi_crop(sess, label_list, softmax_output, coder, images, image_file, writer)\n\n            if output is not None:\n                output.close()\n        \nif __name__ == '__main__':\n    tf.app.run()\n"""
model.py,52,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datetime import datetime\nimport time\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom data import distorted_inputs\nimport re\nfrom tensorflow.contrib.layers import *\n\nfrom tensorflow.contrib.slim.python.slim.nets.inception_v3 import inception_v3_base\n\n\nTOWER_NAME = \'tower\'\n\ndef select_model(name):\n    if name.startswith(\'inception\'):\n        print(\'selected (fine-tuning) inception model\')\n        return inception_v3\n    elif name == \'bn\':\n        print(\'selected batch norm model\')\n        return levi_hassner_bn\n    print(\'selected default model\')\n    return levi_hassner\n\n\ndef get_checkpoint(checkpoint_path, requested_step=None, basename=\'checkpoint\'):\n    if requested_step is not None:\n\n        model_checkpoint_path = \'%s/%s-%s\' % (checkpoint_path, basename, requested_step)\n        if os.path.exists(model_checkpoint_path) is None:\n            print(\'No checkpoint file found at [%s]\' % checkpoint_path)\n            exit(-1)\n            print(model_checkpoint_path)\n        print(model_checkpoint_path)\n        return model_checkpoint_path, requested_step\n\n    ckpt = tf.train.get_checkpoint_state(checkpoint_path)\n    if ckpt and ckpt.model_checkpoint_path:\n        # Restore checkpoint as described in top of this program\n        print(ckpt.model_checkpoint_path)\n        global_step = ckpt.model_checkpoint_path.split(\'/\')[-1].split(\'-\')[-1]\n\n        return ckpt.model_checkpoint_path, global_step\n    else:\n        print(\'No checkpoint file found at [%s]\' % checkpoint_path)\n        exit(-1)\n\ndef _activation_summary(x):\n    tensor_name = re.sub(\'%s_[0-9]*/\' % TOWER_NAME, \'\', x.op.name)\n    tf.summary.histogram(tensor_name + \'/activations\', x)\n    tf.summary.scalar(tensor_name + \'/sparsity\', tf.nn.zero_fraction(x))\n\ndef inception_v3(nlabels, images, pkeep, is_training):\n\n    batch_norm_params = {\n        ""is_training"": is_training,\n        ""trainable"": True,\n        # Decay for the moving averages.\n        ""decay"": 0.9997,\n        # Epsilon to prevent 0s in variance.\n        ""epsilon"": 0.001,\n        # Collection containing the moving mean and moving variance.\n        ""variables_collections"": {\n            ""beta"": None,\n            ""gamma"": None,\n            ""moving_mean"": [""moving_vars""],\n            ""moving_variance"": [""moving_vars""],\n        }\n    }\n    weight_decay = 0.00004\n    stddev=0.1\n    weights_regularizer = tf.contrib.layers.l2_regularizer(weight_decay)\n    with tf.variable_scope(""InceptionV3"", ""InceptionV3"", [images]) as scope:\n\n        with tf.contrib.slim.arg_scope(\n                [tf.contrib.slim.conv2d, tf.contrib.slim.fully_connected],\n                weights_regularizer=weights_regularizer,\n                trainable=True):\n            with tf.contrib.slim.arg_scope(\n                    [tf.contrib.slim.conv2d],\n                    weights_initializer=tf.truncated_normal_initializer(stddev=stddev),\n                    activation_fn=tf.nn.relu,\n                    normalizer_fn=batch_norm,\n                    normalizer_params=batch_norm_params):\n                net, end_points = inception_v3_base(images, scope=scope)\n                with tf.variable_scope(""logits""):\n                    shape = net.get_shape()\n                    net = avg_pool2d(net, shape[1:3], padding=""VALID"", scope=""pool"")\n                    net = tf.nn.dropout(net, pkeep, name=\'droplast\')\n                    net = flatten(net, scope=""flatten"")\n    \n    with tf.variable_scope(\'output\') as scope:\n        \n        weights = tf.Variable(tf.truncated_normal([2048, nlabels], mean=0.0, stddev=0.01), name=\'weights\')\n        biases = tf.Variable(tf.constant(0.0, shape=[nlabels], dtype=tf.float32), name=\'biases\')\n        output = tf.add(tf.matmul(net, weights), biases, name=scope.name)\n        _activation_summary(output)\n    return output\n\ndef levi_hassner_bn(nlabels, images, pkeep, is_training):\n\n    batch_norm_params = {\n        ""is_training"": is_training,\n        ""trainable"": True,\n        # Decay for the moving averages.\n        ""decay"": 0.9997,\n        # Epsilon to prevent 0s in variance.\n        ""epsilon"": 0.001,\n        # Collection containing the moving mean and moving variance.\n        ""variables_collections"": {\n            ""beta"": None,\n            ""gamma"": None,\n            ""moving_mean"": [""moving_vars""],\n            ""moving_variance"": [""moving_vars""],\n        }\n    }\n    weight_decay = 0.0005\n    weights_regularizer = tf.contrib.layers.l2_regularizer(weight_decay)\n\n    with tf.variable_scope(""LeviHassnerBN"", ""LeviHassnerBN"", [images]) as scope:\n\n        with tf.contrib.slim.arg_scope(\n                [convolution2d, fully_connected],\n                weights_regularizer=weights_regularizer,\n                biases_initializer=tf.constant_initializer(1.),\n                weights_initializer=tf.random_normal_initializer(stddev=0.005),\n                trainable=True):\n            with tf.contrib.slim.arg_scope(\n                    [convolution2d],\n                    weights_initializer=tf.random_normal_initializer(stddev=0.01),\n                    normalizer_fn=batch_norm,\n                    normalizer_params=batch_norm_params):\n\n                conv1 = convolution2d(images, 96, [7,7], [4, 4], padding=\'VALID\', biases_initializer=tf.constant_initializer(0.), scope=\'conv1\')\n                pool1 = max_pool2d(conv1, 3, 2, padding=\'VALID\', scope=\'pool1\')\n                conv2 = convolution2d(pool1, 256, [5, 5], [1, 1], padding=\'SAME\', scope=\'conv2\') \n                pool2 = max_pool2d(conv2, 3, 2, padding=\'VALID\', scope=\'pool2\')\n                conv3 = convolution2d(pool2, 384, [3, 3], [1, 1], padding=\'SAME\', biases_initializer=tf.constant_initializer(0.), scope=\'conv3\')\n                pool3 = max_pool2d(conv3, 3, 2, padding=\'VALID\', scope=\'pool3\')\n                # can use tf.contrib.layer.flatten\n                flat = tf.reshape(pool3, [-1, 384*6*6], name=\'reshape\')\n                full1 = fully_connected(flat, 512, scope=\'full1\')\n                drop1 = tf.nn.dropout(full1, pkeep, name=\'drop1\')\n                full2 = fully_connected(drop1, 512, scope=\'full2\')\n                drop2 = tf.nn.dropout(full2, pkeep, name=\'drop2\')\n\n    with tf.variable_scope(\'output\') as scope:\n        \n        weights = tf.Variable(tf.random_normal([512, nlabels], mean=0.0, stddev=0.01), name=\'weights\')\n        biases = tf.Variable(tf.constant(0.0, shape=[nlabels], dtype=tf.float32), name=\'biases\')\n        output = tf.add(tf.matmul(drop2, weights), biases, name=scope.name)\n\n    return output\n\ndef levi_hassner(nlabels, images, pkeep, is_training):\n\n    weight_decay = 0.0005\n    weights_regularizer = tf.contrib.layers.l2_regularizer(weight_decay)\n    with tf.variable_scope(""LeviHassner"", ""LeviHassner"", [images]) as scope:\n\n        with tf.contrib.slim.arg_scope(\n                [convolution2d, fully_connected],\n                weights_regularizer=weights_regularizer,\n                biases_initializer=tf.constant_initializer(1.),\n                weights_initializer=tf.random_normal_initializer(stddev=0.005),\n                trainable=True):\n            with tf.contrib.slim.arg_scope(\n                    [convolution2d],\n                    weights_initializer=tf.random_normal_initializer(stddev=0.01)):\n\n                conv1 = convolution2d(images, 96, [7,7], [4, 4], padding=\'VALID\', biases_initializer=tf.constant_initializer(0.), scope=\'conv1\')\n                pool1 = max_pool2d(conv1, 3, 2, padding=\'VALID\', scope=\'pool1\')\n                norm1 = tf.nn.local_response_normalization(pool1, 5, alpha=0.0001, beta=0.75, name=\'norm1\')\n                conv2 = convolution2d(norm1, 256, [5, 5], [1, 1], padding=\'SAME\', scope=\'conv2\') \n                pool2 = max_pool2d(conv2, 3, 2, padding=\'VALID\', scope=\'pool2\')\n                norm2 = tf.nn.local_response_normalization(pool2, 5, alpha=0.0001, beta=0.75, name=\'norm2\')\n                conv3 = convolution2d(norm2, 384, [3, 3], [1, 1], biases_initializer=tf.constant_initializer(0.), padding=\'SAME\', scope=\'conv3\')\n                pool3 = max_pool2d(conv3, 3, 2, padding=\'VALID\', scope=\'pool3\')\n                flat = tf.reshape(pool3, [-1, 384*6*6], name=\'reshape\')\n                full1 = fully_connected(flat, 512, scope=\'full1\')\n                drop1 = tf.nn.dropout(full1, pkeep, name=\'drop1\')\n                full2 = fully_connected(drop1, 512, scope=\'full2\')\n                drop2 = tf.nn.dropout(full2, pkeep, name=\'drop2\')\n\n    with tf.variable_scope(\'output\') as scope:\n        \n        weights = tf.Variable(tf.random_normal([512, nlabels], mean=0.0, stddev=0.01), name=\'weights\')\n        biases = tf.Variable(tf.constant(0.0, shape=[nlabels], dtype=tf.float32), name=\'biases\')\n        output = tf.add(tf.matmul(drop2, weights), biases, name=scope.name)\n    return output\n\n'"
preproc.py,26,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom six.moves import xrange\nfrom datetime import datetime\nimport os\nimport random\nimport sys\nimport threading\nimport numpy as np\nimport tensorflow as tf\nimport json\n\nRESIZE_HEIGHT = 256\nRESIZE_WIDTH = 256\n\ntf.app.flags.DEFINE_string(\'fold_dir\', \'/home/dpressel/dev/work/AgeGenderDeepLearning/Folds/train_val_txt_files_per_fold/test_fold_is_0\',\n                           \'Fold directory\')\n\ntf.app.flags.DEFINE_string(\'data_dir\', \'/data/xdata/age-gender/aligned\',\n                           \'Data directory\')\n\n\ntf.app.flags.DEFINE_string(\'output_dir\', \'/home/dpressel/dev/work/AgeGenderDeepLearning/Folds/tf/test_fold_is_0\',\n                           \'Output directory\')\n\n\ntf.app.flags.DEFINE_string(\'train_list\', \'age_train.txt\',\n                           \'Training list\')\ntf.app.flags.DEFINE_string(\'valid_list\', \'age_val.txt\',\n                           \'Test list\')\n\ntf.app.flags.DEFINE_integer(\'train_shards\', 10,\n                            \'Number of shards in training TFRecord files.\')\ntf.app.flags.DEFINE_integer(\'valid_shards\', 2,\n                            \'Number of shards in validation TFRecord files.\')\n\ntf.app.flags.DEFINE_integer(\'num_threads\', 2,\n                            \'Number of threads to preprocess the images.\')\n\n\nFLAGS = tf.app.flags.FLAGS\n\ndef _int64_feature(value):\n    """"""Wrapper for inserting int64 features into Example proto.""""""\n    if not isinstance(value, list):\n        value = [value]\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n        \ndef _bytes_feature(value):\n    """"""Wrapper for inserting bytes features into Example proto.""""""\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _convert_to_example(filename, image_buffer, label, height, width):\n    """"""Build an Example proto for an example.\n    Args:\n    filename: string, path to an image file, e.g., \'/path/to/example.JPG\'\n    image_buffer: string, JPEG encoding of RGB image\n    label: integer, identifier for the ground truth for the network\n    height: integer, image height in pixels\n    width: integer, image width in pixels\n    Returns:\n    Example proto\n    """"""\n\n    example = tf.train.Example(features=tf.train.Features(feature={\n        \'image/class/label\': _int64_feature(label),\n        \'image/filename\': _bytes_feature(str.encode(os.path.basename(filename))),\n        \'image/encoded\': _bytes_feature(image_buffer),\n        \'image/height\': _int64_feature(height),\n        \'image/width\': _int64_feature(width)\n    }))\n    return example\n    \nclass ImageCoder(object):\n    """"""Helper class that provides TensorFlow image coding utilities.""""""\n    \n    def __init__(self):\n        # Create a single Session to run all image coding calls.\n        self._sess = tf.Session()\n        \n        # Initializes function that converts PNG to JPEG data.\n        self._png_data = tf.placeholder(dtype=tf.string)\n        image = tf.image.decode_png(self._png_data, channels=3)\n        self._png_to_jpeg = tf.image.encode_jpeg(image, format=\'rgb\', quality=100)\n        \n        # Initializes function that decodes RGB JPEG data.\n        self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n        self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n        cropped = tf.image.resize_images(self._decode_jpeg, [RESIZE_HEIGHT, RESIZE_WIDTH])\n        cropped = tf.cast(cropped, tf.uint8) \n        self._recoded = tf.image.encode_jpeg(cropped, format=\'rgb\', quality=100)\n\n    def png_to_jpeg(self, image_data):\n        return self._sess.run(self._png_to_jpeg,\n                              feed_dict={self._png_data: image_data})\n        \n    def resample_jpeg(self, image_data):\n        image = self._sess.run(self._recoded, #self._decode_jpeg,\n                               feed_dict={self._decode_jpeg_data: image_data})\n\n        return image\n        \n\ndef _is_png(filename):\n    """"""Determine if a file contains a PNG format image.\n    Args:\n    filename: string, path of the image file.\n    Returns:\n    boolean indicating if the image is a PNG.\n    """"""\n    return \'.png\' in filename\n\ndef _process_image(filename, coder):\n    """"""Process a single image file.\n    Args:\n    filename: string, path to an image file e.g., \'/path/to/example.JPG\'.\n    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n    Returns:\n    image_buffer: string, JPEG encoding of RGB image.\n    height: integer, image height in pixels.\n    width: integer, image width in pixels.\n    """"""\n    # Read the image file.\n    with tf.gfile.FastGFile(filename, \'rb\') as f:\n        image_data = f.read()\n\n    # Convert any PNG to JPEG\'s for consistency.\n    if _is_png(filename):\n        print(\'Converting PNG to JPEG for %s\' % filename)\n        image_data = coder.png_to_jpeg(image_data)\n\n    # Decode the RGB JPEG.\n    image = coder.resample_jpeg(image_data)\n    return image, RESIZE_HEIGHT, RESIZE_WIDTH\n\ndef _process_image_files_batch(coder, thread_index, ranges, name, filenames,\n                               labels, num_shards):\n    """"""Processes and saves list of images as TFRecord in 1 thread.\n    Args:\n    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n    thread_index: integer, unique batch to run index is within [0, len(ranges)).\n    ranges: list of pairs of integers specifying ranges of each batches to\n    analyze in parallel.\n    name: string, unique identifier specifying the data set\n    filenames: list of strings; each string is a path to an image file\n    labels: list of integer; each integer identifies the ground truth\n    num_shards: integer number of shards for this data set.\n    """"""\n    # Each thread produces N shards where N = int(num_shards / num_threads).\n    # For instance, if num_shards = 128, and the num_threads = 2, then the first\n    # thread would produce shards [0, 64).\n    num_threads = len(ranges)\n    assert not num_shards % num_threads\n    num_shards_per_batch = int(num_shards / num_threads)\n\n    shard_ranges = np.linspace(ranges[thread_index][0],\n                               ranges[thread_index][1],\n                               num_shards_per_batch + 1).astype(int)\n    num_files_in_thread = ranges[thread_index][1] - ranges[thread_index][0]\n    \n    counter = 0\n    for s in xrange(num_shards_per_batch):\n        # Generate a sharded version of the file name, e.g. \'train-00002-of-00010\'\n        shard = thread_index * num_shards_per_batch + s\n        output_filename = \'%s-%.5d-of-%.5d\' % (name, shard, num_shards)\n        output_file = os.path.join(FLAGS.output_dir, output_filename)\n        writer = tf.python_io.TFRecordWriter(output_file)\n        \n        shard_counter = 0\n        files_in_shard = np.arange(shard_ranges[s], shard_ranges[s + 1], dtype=int)\n        for i in files_in_shard:\n            filename = filenames[i]\n            label = int(labels[i])\n\n            image_buffer, height, width = _process_image(filename, coder)\n            \n            example = _convert_to_example(filename, image_buffer, label,\n                                          height, width)\n            writer.write(example.SerializeToString())\n            shard_counter += 1\n            counter += 1\n\n            if not counter % 1000:\n                print(\'%s [thread %d]: Processed %d of %d images in thread batch.\' %\n                      (datetime.now(), thread_index, counter, num_files_in_thread))\n                sys.stdout.flush()\n\n        writer.close()\n        print(\'%s [thread %d]: Wrote %d images to %s\' %\n              (datetime.now(), thread_index, shard_counter, output_file))\n        sys.stdout.flush()\n        shard_counter = 0\n    print(\'%s [thread %d]: Wrote %d images to %d shards.\' %\n          (datetime.now(), thread_index, counter, num_files_in_thread))\n    sys.stdout.flush()\n\ndef _process_image_files(name, filenames, labels, num_shards):\n    """"""Process and save list of images as TFRecord of Example protos.\n    Args:\n    name: string, unique identifier specifying the data set\n    filenames: list of strings; each string is a path to an image file\n    labels: list of integer; each integer identifies the ground truth\n    num_shards: integer number of shards for this data set.\n    """"""\n    assert len(filenames) == len(labels)\n\n    # Break all images into batches with a [ranges[i][0], ranges[i][1]].\n    spacing = np.linspace(0, len(filenames), FLAGS.num_threads + 1).astype(np.int)\n    ranges = []\n    threads = []\n    for i in xrange(len(spacing) - 1):\n        ranges.append([spacing[i], spacing[i+1]])\n\n    # Launch a thread for each batch.\n    print(\'Launching %d threads for spacings: %s\' % (FLAGS.num_threads, ranges))\n    sys.stdout.flush()\n\n    # Create a mechanism for monitoring when all threads are finished.\n    coord = tf.train.Coordinator()\n    \n    coder = ImageCoder()\n\n    threads = []\n    for thread_index in xrange(len(ranges)):\n        args = (coder, thread_index, ranges, name, filenames, labels, num_shards)\n        t = threading.Thread(target=_process_image_files_batch, args=args)\n        t.start()\n        threads.append(t)\n\n    # Wait for all the threads to terminate.\n    coord.join(threads)\n    print(\'%s: Finished writing all %d images in data set.\' %\n          (datetime.now(), len(filenames)))\n    sys.stdout.flush()\n\ndef _find_image_files(list_file, data_dir):\n    print(\'Determining list of input files and labels from %s.\' % list_file)\n    files_labels = [l.strip().split(\' \') for l in tf.gfile.FastGFile(\n        list_file, \'r\').readlines()]\n\n    labels = []\n    filenames = []\n\n    # Leave label index 0 empty as a background class.\n    label_index = 1\n    \n    # Construct the list of JPEG files and labels.\n    for path, label in files_labels:\n        jpeg_file_path = \'%s/%s\' % (data_dir, path)\n        if os.path.exists(jpeg_file_path):\n            filenames.append(jpeg_file_path)\n            labels.append(label)\n\n    unique_labels = set(labels)\n    # Shuffle the ordering of all image files in order to guarantee\n    # random ordering of the images with respect to label in the\n    # saved TFRecord files. Make the randomization repeatable.\n    shuffled_index = list(range(len(filenames)))\n    random.seed(12345)\n    random.shuffle(shuffled_index)\n    \n    filenames = [filenames[i] for i in shuffled_index]\n    labels = [labels[i] for i in shuffled_index]\n    \n    print(\'Found %d JPEG files across %d labels inside %s.\' %\n          (len(filenames), len(unique_labels), data_dir))\n    return filenames, labels\n\n\ndef _process_dataset(name, filename, directory, num_shards):\n    """"""Process a complete data set and save it as a TFRecord.\n    Args:\n    name: string, unique identifier specifying the data set.\n    directory: string, root path to the data set.\n    num_shards: integer number of shards for this data set.\n    labels_file: string, path to the labels file.\n    """"""\n    filenames, labels = _find_image_files(filename, directory)\n    _process_image_files(name, filenames, labels, num_shards)\n    unique_labels = set(labels)\n    return len(labels), unique_labels\n\ndef main(unused_argv):\n    assert not FLAGS.train_shards % FLAGS.num_threads, (\n        \'Please make the FLAGS.num_threads commensurate with FLAGS.train_shards\')\n    assert not FLAGS.valid_shards % FLAGS.num_threads, (\n        \'Please make the FLAGS.num_threads commensurate with \'\n        \'FLAGS.valid_shards\')\n    print(\'Saving results to %s\' % FLAGS.output_dir)\n\n    if os.path.exists(FLAGS.output_dir) is False:\n        print(\'creating %s\' % FLAGS.output_dir)\n        os.makedirs(FLAGS.output_dir)\n\n    # Run it!\n    valid, valid_outcomes = _process_dataset(\'validation\', \'%s/%s\' % (FLAGS.fold_dir, FLAGS.valid_list), FLAGS.data_dir,\n                     FLAGS.valid_shards)\n    train, train_outcomes = _process_dataset(\'train\', \'%s/%s\' % (FLAGS.fold_dir, FLAGS.train_list), FLAGS.data_dir,\n                     FLAGS.train_shards)\n    \n    if len(valid_outcomes) != len(valid_outcomes | train_outcomes):\n        print(\'Warning: unattested labels in training data [%s]\' % (\', \'.join((valid_outcomes | train_outcomes) - valid_outcomes)))\n        \n    output_file = os.path.join(FLAGS.output_dir, \'md.json\')\n\n\n    md = { \'num_valid_shards\': FLAGS.valid_shards, \n           \'num_train_shards\': FLAGS.train_shards,\n           \'valid_counts\': valid,\n           \'train_counts\': train,\n           \'timestamp\': str(datetime.now()),\n           \'nlabels\': len(train_outcomes) }\n    with open(output_file, \'w\') as f:\n        json.dump(md, f)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n\n'"
train.py,52,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom six.moves import xrange\nfrom datetime import datetime\nimport time\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom data import distorted_inputs\nfrom model import select_model\nimport json\nimport re\n\n\nLAMBDA = 0.01\nMOM = 0.9\ntf.app.flags.DEFINE_string(\'pre_checkpoint_path\', \'\',\n                           """"""If specified, restore this pretrained model """"""\n                           """"""before beginning any training."""""")\n\ntf.app.flags.DEFINE_string(\'train_dir\', \'/home/dpressel/dev/work/AgeGenderDeepLearning/Folds/tf/test_fold_is_0\',\n                           \'Training directory\')\n\ntf.app.flags.DEFINE_boolean(\'log_device_placement\', False,\n                            """"""Whether to log device placement."""""")\n\ntf.app.flags.DEFINE_integer(\'num_preprocess_threads\', 4,\n                            \'Number of preprocessing threads\')\n\ntf.app.flags.DEFINE_string(\'optim\', \'Momentum\',\n                           \'Optimizer\')\n\ntf.app.flags.DEFINE_integer(\'image_size\', 227,\n                            \'Image size\')\n\ntf.app.flags.DEFINE_float(\'eta\', 0.01,\n                          \'Learning rate\')\n\ntf.app.flags.DEFINE_float(\'pdrop\', 0.,\n                          \'Dropout probability\')\n\ntf.app.flags.DEFINE_integer(\'max_steps\', 40000,\n                          \'Number of iterations\')\n\ntf.app.flags.DEFINE_integer(\'steps_per_decay\', 10000,\n                            \'Number of steps before learning rate decay\')\ntf.app.flags.DEFINE_float(\'eta_decay_rate\', 0.1,\n                          \'Learning rate decay\')\n\ntf.app.flags.DEFINE_integer(\'epochs\', -1,\n                            \'Number of epochs\')\n\ntf.app.flags.DEFINE_integer(\'batch_size\', 128,\n                            \'Batch size\')\n\ntf.app.flags.DEFINE_string(\'checkpoint\', \'checkpoint\',\n                          \'Checkpoint name\')\n\ntf.app.flags.DEFINE_string(\'model_type\', \'default\',\n                           \'Type of convnet\')\n\ntf.app.flags.DEFINE_string(\'pre_model\',\n                            \'\',#\'./inception_v3.ckpt\',\n                           \'checkpoint file\')\nFLAGS = tf.app.flags.FLAGS\n\n# Every 5k steps cut learning rate in half\ndef exponential_staircase_decay(at_step=10000, decay_rate=0.1):\n\n    print(\'decay [%f] every [%d] steps\' % (decay_rate, at_step))\n    def _decay(lr, global_step):\n        return tf.train.exponential_decay(lr, global_step,\n                                          at_step, decay_rate, staircase=True)\n    return _decay\n\ndef optimizer(optim, eta, loss_fn, at_step, decay_rate):\n    global_step = tf.Variable(0, trainable=False)\n    optz = optim\n    if optim == \'Adadelta\':\n        optz = lambda lr: tf.train.AdadeltaOptimizer(lr, 0.95, 1e-6)\n        lr_decay_fn = None\n    elif optim == \'Momentum\':\n        optz = lambda lr: tf.train.MomentumOptimizer(lr, MOM)\n        lr_decay_fn = exponential_staircase_decay(at_step, decay_rate)\n\n    return tf.contrib.layers.optimize_loss(loss_fn, global_step, eta, optz, clip_gradients=4., learning_rate_decay_fn=lr_decay_fn)\n\ndef loss(logits, labels):\n    labels = tf.cast(labels, tf.int32)\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n        logits=logits, labels=labels, name=\'cross_entropy_per_example\')\n    cross_entropy_mean = tf.reduce_mean(cross_entropy, name=\'cross_entropy\')\n    tf.add_to_collection(\'losses\', cross_entropy_mean)\n    losses = tf.get_collection(\'losses\')\n    regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n    total_loss = cross_entropy_mean + LAMBDA * sum(regularization_losses)\n    tf.summary.scalar(\'tl (raw)\', total_loss)\n    #total_loss = tf.add_n(losses + regularization_losses, name=\'total_loss\')\n    loss_averages = tf.train.ExponentialMovingAverage(0.9, name=\'avg\')\n    loss_averages_op = loss_averages.apply(losses + [total_loss])\n    for l in losses + [total_loss]:\n        tf.summary.scalar(l.op.name + \' (raw)\', l)\n        tf.summary.scalar(l.op.name, loss_averages.average(l))\n    with tf.control_dependencies([loss_averages_op]):\n        total_loss = tf.identity(total_loss)\n    return total_loss\n\ndef main(argv=None):\n    with tf.Graph().as_default():\n\n        model_fn = select_model(FLAGS.model_type)\n        # Open the metadata file and figure out nlabels, and size of epoch\n        input_file = os.path.join(FLAGS.train_dir, \'md.json\')\n        print(input_file)\n        with open(input_file, \'r\') as f:\n            md = json.load(f)\n\n        images, labels, _ = distorted_inputs(FLAGS.train_dir, FLAGS.batch_size, FLAGS.image_size, FLAGS.num_preprocess_threads)\n        logits = model_fn(md[\'nlabels\'], images, 1-FLAGS.pdrop, True)\n        total_loss = loss(logits, labels)\n\n        train_op = optimizer(FLAGS.optim, FLAGS.eta, total_loss, FLAGS.steps_per_decay, FLAGS.eta_decay_rate)\n        saver = tf.train.Saver(tf.global_variables())\n        summary_op = tf.summary.merge_all()\n\n        sess = tf.Session(config=tf.ConfigProto(\n            log_device_placement=FLAGS.log_device_placement))\n\n        tf.global_variables_initializer().run(session=sess)\n\n        # This is total hackland, it only works to fine-tune iv3\n        if FLAGS.pre_model:\n            inception_variables = tf.get_collection(\n                tf.GraphKeys.VARIABLES, scope=""InceptionV3"")\n            restorer = tf.train.Saver(inception_variables)\n            restorer.restore(sess, FLAGS.pre_model)\n\n        if FLAGS.pre_checkpoint_path:\n            if tf.gfile.Exists(FLAGS.pre_checkpoint_path) is True:\n                print(\'Trying to restore checkpoint from %s\' % FLAGS.pre_checkpoint_path)\n                restorer = tf.train.Saver()\n                tf.train.latest_checkpoint(FLAGS.pre_checkpoint_path)\n                print(\'%s: Pre-trained model restored from %s\' %\n                      (datetime.now(), FLAGS.pre_checkpoint_path))\n\n\n        run_dir = \'%s/run-%d\' % (FLAGS.train_dir, os.getpid())\n\n        checkpoint_path = \'%s/%s\' % (run_dir, FLAGS.checkpoint)\n        if tf.gfile.Exists(run_dir) is False:\n            print(\'Creating %s\' % run_dir)\n            tf.gfile.MakeDirs(run_dir)\n\n        tf.train.write_graph(sess.graph_def, run_dir, \'model.pb\', as_text=True)\n\n        tf.train.start_queue_runners(sess=sess)\n\n\n        summary_writer = tf.summary.FileWriter(run_dir, sess.graph)\n        steps_per_train_epoch = int(md[\'train_counts\'] / FLAGS.batch_size)\n        num_steps = FLAGS.max_steps if FLAGS.epochs < 1 else FLAGS.epochs * steps_per_train_epoch\n        print(\'Requested number of steps [%d]\' % num_steps)\n\n        \n        for step in xrange(num_steps):\n            start_time = time.time()\n            _, loss_value = sess.run([train_op, total_loss])\n            duration = time.time() - start_time\n\n            assert not np.isnan(loss_value), \'Model diverged with loss = NaN\'\n\n            if step % 10 == 0:\n                num_examples_per_step = FLAGS.batch_size\n                examples_per_sec = num_examples_per_step / duration\n                sec_per_batch = float(duration)\n                \n                format_str = (\'%s: step %d, loss = %.3f (%.1f examples/sec; %.3f \' \'sec/batch)\')\n                print(format_str % (datetime.now(), step, loss_value,\n                                    examples_per_sec, sec_per_batch))\n\n            # Loss only actually evaluated every 100 steps?\n            if step % 100 == 0:\n                summary_str = sess.run(summary_op)\n                summary_writer.add_summary(summary_str, step)\n                \n            if step % 1000 == 0 or (step + 1) == num_steps:\n                saver.save(sess, checkpoint_path, global_step=step)\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
utils.py,17,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport six.moves\nfrom datetime import datetime\nimport sys\nimport math\nimport time\nfrom data import inputs, standardize_image\nimport numpy as np\nimport tensorflow as tf\nfrom detect import *\nimport re\n\nRESIZE_AOI = 256\nRESIZE_FINAL = 227\n\n# Modifed from here\n# http://stackoverflow.com/questions/3160699/python-progress-bar#3160819\nclass ProgressBar(object):\n    DEFAULT = \'Progress: %(bar)s %(percent)3d%%\'\n    FULL = \'%(bar)s %(current)d/%(total)d (%(percent)3d%%) %(remaining)d to go\'\n\n    def __init__(self, total, width=40, fmt=DEFAULT, symbol=\'=\'):\n        assert len(symbol) == 1\n\n        self.total = total\n        self.width = width\n        self.symbol = symbol\n        self.fmt = re.sub(r\'(?P<name>%\\(.+?\\))d\',\n            r\'\\g<name>%dd\' % len(str(total)), fmt)\n\n        self.current = 0\n\n    def update(self, step=1):\n        self.current += step\n        percent = self.current / float(self.total)\n        size = int(self.width * percent)\n        remaining = self.total - self.current\n        bar = \'[\' + self.symbol * size + \' \' * (self.width - size) + \']\'\n\n        args = {\n            \'total\': self.total,\n            \'bar\': bar,\n            \'current\': self.current,\n            \'percent\': percent * 100,\n            \'remaining\': remaining\n        }\n        six.print_(\'\\r\' + self.fmt % args, end=\'\')\n\n    def done(self):\n        self.current = self.total\n        self.update(step=0)\n        print(\'\')\n\n# Read image files            \nclass ImageCoder(object):\n    \n    def __init__(self):\n        # Create a single Session to run all image coding calls.\n        config = tf.ConfigProto(allow_soft_placement=True)\n        self._sess = tf.Session(config=config)\n        \n        # Initializes function that converts PNG to JPEG data.\n        self._png_data = tf.placeholder(dtype=tf.string)\n        image = tf.image.decode_png(self._png_data, channels=3)\n        self._png_to_jpeg = tf.image.encode_jpeg(image, format=\'rgb\', quality=100)\n        \n        # Initializes function that decodes RGB JPEG data.\n        self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n        self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n        self.crop = tf.image.resize_images(self._decode_jpeg, (RESIZE_AOI, RESIZE_AOI))\n\n    def png_to_jpeg(self, image_data):\n        return self._sess.run(self._png_to_jpeg,\n                              feed_dict={self._png_data: image_data})\n        \n    def decode_jpeg(self, image_data):\n        image = self._sess.run(self.crop, #self._decode_jpeg,\n                               feed_dict={self._decode_jpeg_data: image_data})\n\n        assert len(image.shape) == 3\n        assert image.shape[2] == 3\n        return image\n        \n\ndef _is_png(filename):\n    """"""Determine if a file contains a PNG format image.\n    Args:\n    filename: string, path of the image file.\n    Returns:\n    boolean indicating if the image is a PNG.\n    """"""\n    return \'.png\' in filename\n        \ndef make_multi_image_batch(filenames, coder):\n    """"""Process a multi-image batch, each with a single-look\n    Args:\n    filenames: list of paths\n    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n    Returns:\n    image_buffer: string, JPEG encoding of RGB image.\n    """"""\n\n    images = []\n\n    for filename in filenames:\n        with tf.gfile.FastGFile(filename, \'rb\') as f:\n            image_data = f.read()\n        # Convert any PNG to JPEG\'s for consistency.\n        if _is_png(filename):\n            print(\'Converting PNG to JPEG for %s\' % filename)\n            image_data = coder.png_to_jpeg(image_data)\n    \n        image = coder.decode_jpeg(image_data)\n\n        crop = tf.image.resize_images(image, (RESIZE_FINAL, RESIZE_FINAL))\n        image = standardize_image(crop)\n        images.append(image)\n    image_batch = tf.stack(images)\n    return image_batch\n\ndef make_multi_crop_batch(filename, coder):\n    """"""Process a single image file.\n    Args:\n    filename: string, path to an image file e.g., \'/path/to/example.JPG\'.\n    coder: instance of ImageCoder to provide TensorFlow image coding utils.\n    Returns:\n    image_buffer: string, JPEG encoding of RGB image.\n    """"""\n    # Read the image file.\n    with tf.gfile.FastGFile(filename, \'rb\') as f:\n        image_data = f.read()\n\n    # Convert any PNG to JPEG\'s for consistency.\n    if _is_png(filename):\n        print(\'Converting PNG to JPEG for %s\' % filename)\n        image_data = coder.png_to_jpeg(image_data)\n    \n    image = coder.decode_jpeg(image_data)\n\n    crops = []\n    print(\'Running multi-cropped image\')\n    h = image.shape[0]\n    w = image.shape[1]\n    hl = h - RESIZE_FINAL\n    wl = w - RESIZE_FINAL\n\n    crop = tf.image.resize_images(image, (RESIZE_FINAL, RESIZE_FINAL))\n    crops.append(standardize_image(crop))\n    crops.append(standardize_image(tf.image.flip_left_right(crop)))\n\n    corners = [ (0, 0), (0, wl), (hl, 0), (hl, wl), (int(hl/2), int(wl/2))]\n    for corner in corners:\n        ch, cw = corner\n        cropped = tf.image.crop_to_bounding_box(image, ch, cw, RESIZE_FINAL, RESIZE_FINAL)\n        crops.append(standardize_image(cropped))\n        flipped = standardize_image(tf.image.flip_left_right(cropped))\n        crops.append(standardize_image(flipped))\n\n    image_batch = tf.stack(crops)\n    return image_batch\n\n\n\ndef face_detection_model(model_type, model_path):\n    model_type_lc = model_type.lower()\n    if model_type_lc == \'yolo_tiny\':\n        from yolodetect import PersonDetectorYOLOTiny\n        return PersonDetectorYOLOTiny(model_path)\n    elif model_type_lc == \'yolo_face\':\n        from yolodetect import FaceDetectorYOLO\n        return FaceDetectorYOLO(model_path)\n    elif model_type == \'dlib\':\n        from dlibdetect import FaceDetectorDlib\n        return FaceDetectorDlib(model_path)\n    return ObjectDetectorCascadeOpenCV(model_path)\n'"
yolodetect.py,23,"b""from detect import ObjectDetector\n\nimport numpy as np\nimport tensorflow as tf\nimport cv2\n\nclass YOLOBase(ObjectDetector):\n    def __init__(self):\n        pass\n\n    def _conv_layer(self, idx, inputs, filters, size, stride):\n        channels = inputs.get_shape()[3]\n        weight = tf.Variable(tf.truncated_normal([size, size, int(channels), filters], stddev=0.1))\n        biases = tf.Variable(tf.constant(0.1, shape=[filters]))\n\n        pad_size = size // 2\n        pad_mat = np.array([[0, 0], [pad_size, pad_size], [pad_size, pad_size], [0, 0]])\n        inputs_pad = tf.pad(inputs, pad_mat)\n\n        conv = tf.nn.conv2d(inputs_pad, weight, strides=[1, stride, stride, 1], padding='VALID',\n                            name=str(idx) + '_conv')\n        conv_biased = tf.add(conv, biases, name=str(idx) + '_conv_biased')\n        return tf.maximum(self.alpha * conv_biased, conv_biased, name=str(idx) + '_leaky_relu')\n\n    def _pooling_layer(self, idx, inputs, size, stride):\n        return tf.nn.max_pool(inputs, ksize=[1, size, size, 1], strides=[1, stride, stride, 1], padding='SAME',\n                              name=str(idx) + '_pool')\n\n    def _fc_layer(self, idx, inputs, hiddens, flat=False, linear=False):\n        input_shape = inputs.get_shape().as_list()\n        if flat:\n            dim = input_shape[1] * input_shape[2] * input_shape[3]\n            inputs_transposed = tf.transpose(inputs, (0, 3, 1, 2))\n            inputs_processed = tf.reshape(inputs_transposed, [-1, dim])\n        else:\n            dim = input_shape[1]\n            inputs_processed = inputs\n        weight = tf.Variable(tf.truncated_normal([dim, hiddens], stddev=0.1))\n        biases = tf.Variable(tf.constant(0.1, shape=[hiddens]))\n        if linear: return tf.add(tf.matmul(inputs_processed, weight), biases, name=str(idx) + '_fc')\n        ip = tf.add(tf.matmul(inputs_processed, weight), biases)\n        return tf.maximum(self.alpha * ip, ip, name=str(idx) + '_fc')\n\n    def _init_base_model(self):\n        self.x = tf.placeholder('float32', [None, 448, 448, 3])\n        conv_1 = self._conv_layer(1, self.x, 16, 3, 1)\n        pool_2 = self._pooling_layer(2, conv_1, 2, 2)\n        conv_3 = self._conv_layer(3, pool_2, 32, 3, 1)\n        pool_4 = self._pooling_layer(4, conv_3, 2, 2)\n        conv_5 = self._conv_layer(5, pool_4, 64, 3, 1)\n        pool_6 = self._pooling_layer(6, conv_5, 2, 2)\n        conv_7 = self._conv_layer(7, pool_6, 128, 3, 1)\n        pool_8 = self._pooling_layer(8, conv_7, 2, 2)\n        conv_9 = self._conv_layer(9, pool_8, 256, 3, 1)\n        pool_10 = self._pooling_layer(10, conv_9, 2, 2)\n        conv_11 = self._conv_layer(11, pool_10, 512, 3, 1)\n        pool_12 = self._pooling_layer(12, conv_11, 2, 2)\n        conv_13 = self._conv_layer(13, pool_12, 1024, 3, 1)\n        conv_14 = self._conv_layer(14, conv_13, 1024, 3, 1)\n        conv_15 = self._conv_layer(15, conv_14, 1024, 3, 1)\n        fc_16 = self._fc_layer(16, conv_15, 256, flat=True, linear=False)\n        return self._fc_layer(17, fc_16, 4096, flat=False, linear=False)\n\n    def _iou(self, box1, box2):\n        tb = min(box1[0] + 0.5 * box1[2], box2[0] + 0.5 * box2[2]) - max(box1[0] - 0.5 * box1[2],\n                                                                         box2[0] - 0.5 * box2[2])\n        lr = min(box1[1] + 0.5 * box1[3], box2[1] + 0.5 * box2[3]) - max(box1[1] - 0.5 * box1[3],\n                                                                         box2[1] - 0.5 * box2[3])\n        if tb < 0 or lr < 0:\n            intersection = 0\n        else:\n            intersection = tb * lr\n        return intersection / (box1[2] * box1[3] + box2[2] * box2[3] - intersection)\n\n    def sub_image(self, name, img, x, y, w, h):\n        half_w = w // 2\n        half_h = h // 2\n        upper_cut = [y + half_h, x + half_w]\n        lower_cut = [y - half_h, x - half_w];\n        roi_color = img[lower_cut[0]:upper_cut[0], lower_cut[1]:upper_cut[1]]\n        cv2.imwrite(name, roi_color)\n        return name\n\n    def draw_rect(self, img, x, y, w, h):\n        half_w = w // 2\n        half_h = h // 2\n        upper_cut = [y + half_h, x + half_w]\n        lower_cut = [y - half_h, x - half_w];\n        cv2.rectangle(img, (lower_cut[1], lower_cut[0]), (upper_cut[1], upper_cut[0]), (0, 255, 0), 2)\n\n    def run(self, filename):\n        img = cv2.imread(filename)\n        self.h_img, self.w_img, _ = img.shape\n        img_resized = cv2.resize(img, (448, 448))\n        img_RGB = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n        img_resized_np = np.asarray(img_RGB)\n        inputs = np.zeros((1, 448, 448, 3), dtype='float32')\n        inputs[0] = (img_resized_np / 255.0) * 2.0 - 1.0\n        in_dict = {self.x: inputs}\n        net_output = self.sess.run(self.fc_19, feed_dict=in_dict)\n        faces = self.interpret_output(net_output[0])\n        images = []\n        for i, (x, y, w, h, p) in enumerate(faces):\n            images.append(self.sub_image('%s/%s-%d.jpg' % (self.tgtdir, self.basename, i + 1), img, x, y, w, h))\n\n        print('%d faces detected' % len(images))\n\n        for (x, y, w, h, p) in faces:\n            print('Face found [%d, %d, %d, %d] (%.2f)' % (x, y, w, h, p));\n            self.draw_rect(img, x, y, w, h)\n            # Fix in case nothing found in the image\n        outfile = '%s/%s.jpg' % (self.tgtdir, self.basename)\n        cv2.imwrite(outfile, img)\n        return images, outfile\n\n    def __init__(self, model_name, basename, tgtdir, alpha, threshold, iou_threshold):\n        self.alpha = alpha\n        self.threshold = threshold\n        self.iou_threshold = iou_threshold\n        self.basename = basename\n        self.tgtdir = tgtdir\n        self.load_model(model_name)\n\nclass PersonDetectorYOLOTiny(YOLOBase):\n    def __init__(self, model_name, basename='frontal-face', tgtdir='.', alpha=0.1, threshold=0.2, iou_threshold=0.5):\n        self.alpha = alpha\n        self.threshold = threshold\n        self.iou_threshold = iou_threshold\n        self.basename = basename\n        self.tgtdir = tgtdir\n        self.load_model(model_name)\n\n    def load_model(self, model_name):\n        g = tf.Graph()\n\n        with g.as_default():\n            fc_17 = self._init_base_model()\n            # skip dropout_18\n            self.fc_19 = self._fc_layer(19, fc_17, 1470, flat=False, linear=True)\n            self.sess = tf.Session(graph=g)\n            self.sess.run(tf.global_variables_initializer())\n            self.saver = tf.train.Saver()\n            self.saver.restore(self.sess, model_name)\n\n    def interpret_output(self, output):\n        probs = np.zeros((7, 7, 2, 20))\n        class_probs = np.reshape(output[0:980], (7, 7, 20))\n        scales = np.reshape(output[980:1078], (7, 7, 2))\n        boxes = np.reshape(output[1078:], (7, 7, 2, 4))\n        offset = np.transpose(np.reshape(np.array([np.arange(7)] * 14), (2, 7, 7)), (1, 2, 0))\n\n        boxes[:, :, :, 0] += offset\n        boxes[:, :, :, 1] += np.transpose(offset, (1, 0, 2))\n        boxes[:, :, :, 0:2] = boxes[:, :, :, 0:2] / 7.0\n        boxes[:, :, :, 2] = np.multiply(boxes[:, :, :, 2], boxes[:, :, :, 2])\n        boxes[:, :, :, 3] = np.multiply(boxes[:, :, :, 3], boxes[:, :, :, 3])\n\n        boxes[:, :, :, 0] *= self.w_img\n        boxes[:, :, :, 1] *= self.h_img\n        boxes[:, :, :, 2] *= self.w_img\n        boxes[:, :, :, 3] *= self.h_img\n\n        for i in range(2):\n            for j in range(20):\n                probs[:, :, i, j] = np.multiply(class_probs[:, :, j], scales[:, :, i])\n\n        filter_mat_probs = np.array(probs >= self.threshold, dtype='bool')\n        filter_mat_boxes = np.nonzero(filter_mat_probs)\n        boxes_filtered = boxes[filter_mat_boxes[0], filter_mat_boxes[1], filter_mat_boxes[2]]\n        probs_filtered = probs[filter_mat_probs]\n        classes_num_filtered = np.argmax(filter_mat_probs, axis=3)[\n            filter_mat_boxes[0], filter_mat_boxes[1], filter_mat_boxes[2]]\n\n        argsort = np.array(np.argsort(probs_filtered))[::-1]\n        boxes_filtered = boxes_filtered[argsort]\n        probs_filtered = probs_filtered[argsort]\n        classes_num_filtered = classes_num_filtered[argsort]\n\n        for i in range(len(boxes_filtered)):\n            if probs_filtered[i] == 0:\n                continue\n\n            for j in range(i + 1, len(boxes_filtered)):\n                if self._iou(boxes_filtered[i], boxes_filtered[j]) > self.iou_threshold:\n                    probs_filtered[j] = 0.0\n\n        filter_iou = np.array(probs_filtered > 0.0, dtype='bool')\n        boxes_filtered = boxes_filtered[filter_iou]\n        probs_filtered = probs_filtered[filter_iou]\n        classes_num_filtered = classes_num_filtered[filter_iou]\n\n        result = []\n        for i in range(len(boxes_filtered)):\n            if classes_num_filtered[i] == 14:\n                result.append([int(boxes_filtered[i][0]),\n                               int(boxes_filtered[i][1]),\n                               int(boxes_filtered[i][2]),\n                               int(boxes_filtered[i][3]),\n                               probs_filtered[i]])\n\n        return result\n\n# This model doesnt seem to work particularly well on data I have tried\nclass FaceDetectorYOLO(YOLOBase):\n    def __init__(self, model_name, basename='frontal-face', tgtdir='.', alpha=0.1, threshold=0.2, iou_threshold=0.5):\n        self.alpha = alpha\n        self.threshold = threshold\n        self.iou_threshold = iou_threshold\n        self.basename = basename\n        self.tgtdir = tgtdir\n        self.load_model(model_name)\n\n    def load_model(self, model_name):\n        g = tf.Graph()\n\n        with g.as_default():\n            fc_17 = self._init_base_model()\n            # skip dropout_18\n            self.fc_19 = self._fc_layer(19, fc_17, 1331, flat=False, linear=True)\n            self.sess = tf.Session(graph=g)\n            self.sess.run(tf.global_variables_initializer())\n            self.saver = tf.train.Saver()\n            self.saver.restore(self.sess, model_name)\n\n    def interpret_output(self, output):\n        prob_range = [0, 11 * 11 * 1]\n        scales_range = [prob_range[1], prob_range[1] + 11 * 11 * 2]\n        boxes_range = [scales_range[1], scales_range[1] + 11 * 11 * 2 * 4]\n\n        probs = np.zeros((11, 11, 2, 1))\n        class_probs = np.reshape(output[0:prob_range[1]], (11, 11, 1))\n        scales = np.reshape(output[scales_range[0]:scales_range[1]], (11, 11, 2))\n        boxes = np.reshape(output[boxes_range[0]:], (11, 11, 2, 4))\n        offset = np.transpose(np.reshape(np.array([np.arange(11)] * (2 * 11)), (2, 11, 11)), (1, 2, 0))\n\n        boxes[:, :, :, 0] += offset\n        boxes[:, :, :, 1] += np.transpose(offset, (1, 0, 2))\n        boxes[:, :, :, 0:2] = boxes[:, :, :, 0:2] / float(11)\n        boxes[:, :, :, 2] = np.multiply(boxes[:, :, :, 2], boxes[:, :, :, 2])\n        boxes[:, :, :, 3] = np.multiply(boxes[:, :, :, 3], boxes[:, :, :, 3])\n\n        boxes[:, :, :, 0] *= self.w_img\n        boxes[:, :, :, 1] *= self.h_img\n        boxes[:, :, :, 2] *= self.w_img\n        boxes[:, :, :, 3] *= self.h_img\n\n        for i in range(2):\n            probs[:, :, i, 0] = np.multiply(class_probs[:, :, 0], scales[:, :, i])\n\n        filter_mat_probs = np.array(probs >= self.threshold, dtype='bool')\n        filter_mat_boxes = np.nonzero(filter_mat_probs)\n        boxes_filtered = boxes[filter_mat_boxes[0], filter_mat_boxes[1], filter_mat_boxes[2]]\n        probs_filtered = probs[filter_mat_probs]\n        classes_num_filtered = np.argmax(filter_mat_probs, axis=3)[\n            filter_mat_boxes[0], filter_mat_boxes[1], filter_mat_boxes[2]]\n\n        argsort = np.array(np.argsort(probs_filtered))[::-1]\n        boxes_filtered = boxes_filtered[argsort]\n        probs_filtered = probs_filtered[argsort]\n        classes_num_filtered = classes_num_filtered[argsort]\n\n        for i in range(len(boxes_filtered)):\n            if probs_filtered[i] == 0: continue\n            for j in range(i + 1, len(boxes_filtered)):\n                if self._iou(boxes_filtered[i], boxes_filtered[j]) > self.iou_threshold:\n                    probs_filtered[j] = 0.0\n\n        filter_iou = np.array(probs_filtered > 0.0, dtype='bool')\n        boxes_filtered = boxes_filtered[filter_iou]\n        probs_filtered = probs_filtered[filter_iou]\n        classes_num_filtered = classes_num_filtered[filter_iou]\n\n        result = []\n        for i in range(len(boxes_filtered)):\n            result.append([int(boxes_filtered[i][0]),\n                           int(boxes_filtered[i][1]),\n                           int(boxes_filtered[i][2]),\n                           int(boxes_filtered[i][3]),\n                           probs_filtered[i]])\n\n        return result\n\n\n"""
