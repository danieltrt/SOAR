file_path,api_count,code
ch01-人工智能绪论/autograd.py,5,"b'import tensorflow as tf \n\n# \xe5\x88\x9b\xe5\xbb\xba4\xe4\xb8\xaa\xe5\xbc\xa0\xe9\x87\x8f\na = tf.constant(1.)\nb = tf.constant(2.)\nc = tf.constant(3.)\nw = tf.constant(4.)\n\n\nwith tf.GradientTape() as tape:# \xe6\x9e\x84\xe5\xbb\xba\xe6\xa2\xaf\xe5\xba\xa6\xe7\x8e\xaf\xe5\xa2\x83\n\ttape.watch([w]) # \xe5\xb0\x86w\xe5\x8a\xa0\xe5\x85\xa5\xe6\xa2\xaf\xe5\xba\xa6\xe8\xb7\x9f\xe8\xb8\xaa\xe5\x88\x97\xe8\xa1\xa8\n\t# \xe6\x9e\x84\xe5\xbb\xba\xe8\xae\xa1\xe7\xae\x97\xe8\xbf\x87\xe7\xa8\x8b\n\ty = a * w**2 + b * w + c\n# \xe6\xb1\x82\xe5\xaf\xbc\n[dy_dw] = tape.gradient(y, [w])\nprint(dy_dw)\n\n'"
ch01-人工智能绪论/gpu_accelerate.py,10,"b""import  numpy as np\nimport  matplotlib\nfrom    matplotlib import pyplot as plt\n# Default parameters for plots\nmatplotlib.rcParams['font.size'] = 20\nmatplotlib.rcParams['figure.titlesize'] = 20\nmatplotlib.rcParams['figure.figsize'] = [9, 7]\nmatplotlib.rcParams['font.family'] = ['STKaiti']\nmatplotlib.rcParams['axes.unicode_minus']=False \n\n\n\nimport tensorflow as tf\nimport timeit\n\n\n\n\ncpu_data = []\ngpu_data = []\nfor n in range(9):\n\tn = 10**n\n\t# \xe5\x88\x9b\xe5\xbb\xba\xe5\x9c\xa8CPU\xe4\xb8\x8a\xe8\xbf\x90\xe7\xae\x97\xe7\x9a\x842\xe4\xb8\xaa\xe7\x9f\xa9\xe9\x98\xb5\n\twith tf.device('/cpu:0'):\n\t\tcpu_a = tf.random.normal([1, n])\n\t\tcpu_b = tf.random.normal([n, 1])\n\t\tprint(cpu_a.device, cpu_b.device)\n\t# \xe5\x88\x9b\xe5\xbb\xba\xe4\xbd\xbf\xe7\x94\xa8GPU\xe8\xbf\x90\xe7\xae\x97\xe7\x9a\x842\xe4\xb8\xaa\xe7\x9f\xa9\xe9\x98\xb5\n\twith tf.device('/gpu:0'):\n\t\tgpu_a = tf.random.normal([1, n])\n\t\tgpu_b = tf.random.normal([n, 1])\n\t\tprint(gpu_a.device, gpu_b.device)\n\n\tdef cpu_run():\n\t\twith tf.device('/cpu:0'):\n\t\t\tc = tf.matmul(cpu_a, cpu_b)\n\t\treturn c \n\n\tdef gpu_run():\n\t\twith tf.device('/gpu:0'):\n\t\t\tc = tf.matmul(gpu_a, gpu_b)\n\t\treturn c \n\n\t# \xe7\xac\xac\xe4\xb8\x80\xe6\xac\xa1\xe8\xae\xa1\xe7\xae\x97\xe9\x9c\x80\xe8\xa6\x81\xe7\x83\xad\xe8\xba\xab\xef\xbc\x8c\xe9\x81\xbf\xe5\x85\x8d\xe5\xb0\x86\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe9\x98\xb6\xe6\xae\xb5\xe6\x97\xb6\xe9\x97\xb4\xe7\xbb\x93\xe7\xae\x97\xe5\x9c\xa8\xe5\x86\x85\n\tcpu_time = timeit.timeit(cpu_run, number=10)\n\tgpu_time = timeit.timeit(gpu_run, number=10)\n\tprint('warmup:', cpu_time, gpu_time)\n\t# \xe6\xad\xa3\xe5\xbc\x8f\xe8\xae\xa1\xe7\xae\x9710\xe6\xac\xa1\xef\xbc\x8c\xe5\x8f\x96\xe5\xb9\xb3\xe5\x9d\x87\xe6\x97\xb6\xe9\x97\xb4\n\tcpu_time = timeit.timeit(cpu_run, number=10)\n\tgpu_time = timeit.timeit(gpu_run, number=10)\n\tprint('run time:', cpu_time, gpu_time)\n\tcpu_data.append(cpu_time/10)\n\tgpu_data.append(gpu_time/10)\n\n\tdel cpu_a,cpu_b,gpu_a,gpu_b\n\nx = [10**i for i in range(9)]\ncpu_data = [1000*i for i in cpu_data]\ngpu_data = [1000*i for i in gpu_data]\nplt.plot(x, cpu_data, 'C1')\nplt.plot(x, cpu_data, color='C1', marker='s', label='CPU')\nplt.plot(x, gpu_data,'C0')\nplt.plot(x, gpu_data, color='C0', marker='^', label='GPU')\n\n\nplt.gca().set_xscale('log')\nplt.gca().set_yscale('log')\nplt.ylim([0,100])\nplt.xlabel('\xe7\x9f\xa9\xe9\x98\xb5\xe5\xa4\xa7\xe5\xb0\x8fn:(1xn)@(nx1)')\nplt.ylabel('\xe8\xbf\x90\xe7\xae\x97\xe6\x97\xb6\xe9\x97\xb4(ms)')\nplt.legend()\nplt.savefig('gpu-time.svg')"""
ch01-人工智能绪论/tf1.py,7,"b""import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior() # \xe4\xbd\xbf\xe7\x94\xa8\xe9\x9d\x99\xe6\x80\x81\xe5\x9b\xbe\xe6\xa8\xa1\xe5\xbc\x8f\xe8\xbf\x90\xe8\xa1\x8c\xe4\xbb\xa5\xe4\xb8\x8b\xe4\xbb\xa3\xe7\xa0\x81\nassert tf.__version__.startswith('2.')\n\n# 1.\xe5\x88\x9b\xe5\xbb\xba\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe9\x98\xb6\xe6\xae\xb5\n# \xe5\x88\x9b\xe5\xbb\xba2\xe4\xb8\xaa\xe8\xbe\x93\xe5\x85\xa5\xe7\xab\xaf\xe5\xad\x90\xef\xbc\x8c\xe6\x8c\x87\xe5\xae\x9a\xe7\xb1\xbb\xe5\x9e\x8b\xe5\x92\x8c\xe5\x90\x8d\xe5\xad\x97\na_ph = tf.placeholder(tf.float32, name='variable_a')\nb_ph = tf.placeholder(tf.float32, name='variable_b')\n# \xe5\x88\x9b\xe5\xbb\xba\xe8\xbe\x93\xe5\x87\xba\xe7\xab\xaf\xe5\xad\x90\xe7\x9a\x84\xe8\xbf\x90\xe7\xae\x97\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe5\xb9\xb6\xe5\x91\xbd\xe5\x90\x8d\nc_op = tf.add(a_ph, b_ph, name='variable_c')\n\n# 2.\xe8\xbf\x90\xe8\xa1\x8c\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe9\x98\xb6\xe6\xae\xb5\n# \xe5\x88\x9b\xe5\xbb\xba\xe8\xbf\x90\xe8\xa1\x8c\xe7\x8e\xaf\xe5\xa2\x83\nsess = tf.InteractiveSession()\n# \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x93\x8d\xe4\xbd\x9c\xe4\xb9\x9f\xe9\x9c\x80\xe8\xa6\x81\xe4\xbd\x9c\xe4\xb8\xba\xe6\x93\x8d\xe4\xbd\x9c\xe8\xbf\x90\xe8\xa1\x8c\ninit = tf.global_variables_initializer()\nsess.run(init) # \xe8\xbf\x90\xe8\xa1\x8c\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe5\xae\x8c\xe6\x88\x90\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n# \xe8\xbf\x90\xe8\xa1\x8c\xe8\xbe\x93\xe5\x87\xba\xe7\xab\xaf\xe5\xad\x90\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe7\xbb\x99\xe8\xbe\x93\xe5\x85\xa5\xe7\xab\xaf\xe5\xad\x90\xe8\xb5\x8b\xe5\x80\xbc\nc_numpy = sess.run(c_op, feed_dict={a_ph: 2., b_ph: 4.})\n# \xe8\xbf\x90\xe7\xae\x97\xe5\xae\x8c\xe8\xbe\x93\xe5\x87\xba\xe7\xab\xaf\xe5\xad\x90\xe6\x89\x8d\xe8\x83\xbd\xe5\xbe\x97\xe5\x88\xb0\xe6\x95\xb0\xe5\x80\xbc\xe7\xb1\xbb\xe5\x9e\x8b\xe7\x9a\x84c_numpy\nprint('a+b=',c_numpy)"""
ch01-人工智能绪论/tf2.py,3,"b""#%%\nimport tensorflow as tf\nassert tf.__version__.startswith('2.')\n\n# 1.\xe5\x88\x9b\xe5\xbb\xba\xe8\xbe\x93\xe5\x85\xa5\xe5\xbc\xa0\xe9\x87\x8f\na = tf.constant(2.)\nb = tf.constant(4.)\n# 2.\xe7\x9b\xb4\xe6\x8e\xa5\xe8\xae\xa1\xe7\xae\x97\xe5\xb9\xb6\xe6\x89\x93\xe5\x8d\xb0\nprint('a+b=',a+b)\n\n\n"""
ch02-回归问题/linear_regression.py,0,"b'import numpy as np\n\n# data = []\n# for i in range(100):\n# \tx = np.random.uniform(3., 12.)\n# \t# mean=0, std=0.1\n# \teps = np.random.normal(0., 0.1)\n# \ty = 1.477 * x + 0.089 + eps\n# \tdata.append([x, y])\n# data = np.array(data)\n# print(data.shape, data)\n\n# y = wx + b\ndef compute_error_for_line_given_points(b, w, points):\n    totalError = 0\n    for i in range(0, len(points)):\n        x = points[i, 0]\n        y = points[i, 1]\n        # computer mean-squared-error\n        totalError += (y - (w * x + b)) ** 2\n    # average loss for each point\n    return totalError / float(len(points))\n\n\n\ndef step_gradient(b_current, w_current, points, learningRate):\n    b_gradient = 0\n    w_gradient = 0\n    N = float(len(points))\n    for i in range(0, len(points)):\n        x = points[i, 0]\n        y = points[i, 1]\n        # grad_b = 2(wx+b-y)\n        b_gradient += (2/N) * ((w_current * x + b_current) - y)\n        # grad_w = 2(wx+b-y)*x\n        w_gradient += (2/N) * x * ((w_current * x + b_current) - y)\n    # update w\'\n    new_b = b_current - (learningRate * b_gradient)\n    new_w = w_current - (learningRate * w_gradient)\n    return [new_b, new_w]\n\ndef gradient_descent_runner(points, starting_b, starting_w, learning_rate, num_iterations):\n    b = starting_b\n    w = starting_w\n    # update for several times\n    for i in range(num_iterations):\n        b, w = step_gradient(b, w, np.array(points), learning_rate)\n    return [b, w]\n\n\ndef run():\n\t\n    points = np.genfromtxt(""data.csv"", delimiter="","")\n    learning_rate = 0.0001\n    initial_b = 0 # initial y-intercept guess\n    initial_w = 0 # initial slope guess\n    num_iterations = 1000\n    print(""Starting gradient descent at b = {0}, w = {1}, error = {2}""\n          .format(initial_b, initial_w,\n                  compute_error_for_line_given_points(initial_b, initial_w, points))\n          )\n    print(""Running..."")\n    [b, w] = gradient_descent_runner(points, initial_b, initial_w, learning_rate, num_iterations)\n    print(""After {0} iterations b = {1}, w = {2}, error = {3}"".\n          format(num_iterations, b, w,\n                 compute_error_for_line_given_points(b, w, points))\n          )\n\nif __name__ == \'__main__\':\n    run()'"
ch03-分类问题/forward_layer.py,7,"b""import  os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n\n\nimport  tensorflow as tf\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers, optimizers, datasets\n\n\n\n\n(x, y), (x_val, y_val) = datasets.mnist.load_data() \nx = tf.convert_to_tensor(x, dtype=tf.float32) / 255.\ny = tf.convert_to_tensor(y, dtype=tf.int32)\ny = tf.one_hot(y, depth=10)\nprint(x.shape, y.shape)\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x, y))\ntrain_dataset = train_dataset.batch(200)\n\n \n\n\nmodel = keras.Sequential([ \n    layers.Dense(512, activation='relu'),\n    layers.Dense(256, activation='relu'),\n    layers.Dense(10)])\n\noptimizer = optimizers.SGD(learning_rate=0.001)\n\n\ndef train_epoch(epoch):\n\n    # Step4.loop\n    for step, (x, y) in enumerate(train_dataset):\n\n\n        with tf.GradientTape() as tape:\n            # [b, 28, 28] => [b, 784]\n            x = tf.reshape(x, (-1, 28*28))\n            # Step1. compute output\n            # [b, 784] => [b, 10]\n            out = model(x)\n            # Step2. compute loss\n            loss = tf.reduce_sum(tf.square(out - y)) / x.shape[0]\n\n        # Step3. optimize and update w1, w2, w3, b1, b2, b3\n        grads = tape.gradient(loss, model.trainable_variables)\n        # w' = w - lr * grad\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n        if step % 100 == 0:\n            print(epoch, step, 'loss:', loss.numpy())\n\n\n\ndef train():\n\n    for epoch in range(30):\n\n        train_epoch(epoch)\n\n\n\n\n\n\nif __name__ == '__main__':\n    train()"""
ch03-分类问题/forward_tensor.py,19,"b""import  os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nimport  matplotlib\nfrom \tmatplotlib import pyplot as plt\n# Default parameters for plots\nmatplotlib.rcParams['font.size'] = 20\nmatplotlib.rcParams['figure.titlesize'] = 20\nmatplotlib.rcParams['figure.figsize'] = [9, 7]\nmatplotlib.rcParams['font.family'] = ['STKaiTi']\nmatplotlib.rcParams['axes.unicode_minus']=False \n\nimport  tensorflow as tf\nfrom    tensorflow import keras\nfrom    tensorflow.keras import datasets\n\n\n# x: [60k, 28, 28],\n# y: [60k]\n(x, y), _ = datasets.mnist.load_data()\n# x: [0~255] => [0~1.]\nx = tf.convert_to_tensor(x, dtype=tf.float32) / 255.\ny = tf.convert_to_tensor(y, dtype=tf.int32)\n\nprint(x.shape, y.shape, x.dtype, y.dtype)\nprint(tf.reduce_min(x), tf.reduce_max(x))\nprint(tf.reduce_min(y), tf.reduce_max(y))\n\n\ntrain_db = tf.data.Dataset.from_tensor_slices((x,y)).batch(128)\ntrain_iter = iter(train_db)\nsample = next(train_iter)\nprint('batch:', sample[0].shape, sample[1].shape)\n\n\n# [b, 784] => [b, 256] => [b, 128] => [b, 10]\n# [dim_in, dim_out], [dim_out]\nw1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))\nb1 = tf.Variable(tf.zeros([256]))\nw2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))\nb2 = tf.Variable(tf.zeros([128]))\nw3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1))\nb3 = tf.Variable(tf.zeros([10]))\n\nlr = 1e-3\n\nlosses = []\n\nfor epoch in range(20): # iterate db for 10\n    for step, (x, y) in enumerate(train_db): # for every batch\n        # x:[128, 28, 28]\n        # y: [128]\n\n        # [b, 28, 28] => [b, 28*28]\n        x = tf.reshape(x, [-1, 28*28])\n\n        with tf.GradientTape() as tape: # tf.Variable\n            # x: [b, 28*28]\n            # h1 = x@w1 + b1\n            # [b, 784]@[784, 256] + [256] => [b, 256] + [256] => [b, 256] + [b, 256]\n            h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256])\n            h1 = tf.nn.relu(h1)\n            # [b, 256] => [b, 128]\n            h2 = h1@w2 + b2\n            h2 = tf.nn.relu(h2)\n            # [b, 128] => [b, 10]\n            out = h2@w3 + b3\n\n            # compute loss\n            # out: [b, 10]\n            # y: [b] => [b, 10]\n            y_onehot = tf.one_hot(y, depth=10)\n\n            # mse = mean(sum(y-out)^2)\n            # [b, 10]\n            loss = tf.square(y_onehot - out)\n            # mean: scalar\n            loss = tf.reduce_mean(loss)\n\n        # compute gradients\n        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n        # print(grads)\n        # w1 = w1 - lr * w1_grad\n        w1.assign_sub(lr * grads[0])\n        b1.assign_sub(lr * grads[1])\n        w2.assign_sub(lr * grads[2])\n        b2.assign_sub(lr * grads[3])\n        w3.assign_sub(lr * grads[4])\n        b3.assign_sub(lr * grads[5])\n\n\n        if step % 100 == 0:\n            print(epoch, step, 'loss:', float(loss))\n\n    losses.append(float(loss))\n\nplt.figure()\nplt.plot(losses, color='C0', marker='s', label='\xe8\xae\xad\xe7\xbb\x83')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylabel('MSE')\nplt.savefig('forward.svg')\n# plt.show()\n"""
ch03-分类问题/main.py,10,"b""import  tensorflow as tf\nfrom    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n\n\n# \xe8\xae\xbe\xe7\xbd\xaeGPU\xe4\xbd\xbf\xe7\x94\xa8\xe6\x96\xb9\xe5\xbc\x8f\n# \xe8\x8e\xb7\xe5\x8f\x96GPU\xe5\x88\x97\xe8\xa1\xa8\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  try:\n    # \xe8\xae\xbe\xe7\xbd\xaeGPU\xe4\xb8\xba\xe5\xa2\x9e\xe9\x95\xbf\xe5\xbc\x8f\xe5\x8d\xa0\xe7\x94\xa8\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True) \n  except RuntimeError as e:\n    # \xe6\x89\x93\xe5\x8d\xb0\xe5\xbc\x82\xe5\xb8\xb8\n    print(e)\n\n(xs, ys),_ = datasets.mnist.load_data()\nprint('datasets:', xs.shape, ys.shape, xs.min(), xs.max())\n\nbatch_size = 32\n\nxs = tf.convert_to_tensor(xs, dtype=tf.float32) / 255.\ndb = tf.data.Dataset.from_tensor_slices((xs,ys))\ndb = db.batch(batch_size).repeat(30)\n\n\nmodel = Sequential([layers.Dense(256, activation='relu'), \n                     layers.Dense(128, activation='relu'),\n                     layers.Dense(10)])\nmodel.build(input_shape=(4, 28*28))\nmodel.summary()\n\noptimizer = optimizers.SGD(lr=0.01)\nacc_meter = metrics.Accuracy()\n\nfor step, (x,y) in enumerate(db):\n\n    with tf.GradientTape() as tape:\n        # \xe6\x89\x93\xe5\xb9\xb3\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c[b, 28, 28] => [b, 784]\n        x = tf.reshape(x, (-1, 28*28))\n        # Step1. \xe5\xbe\x97\xe5\x88\xb0\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbe\x93\xe5\x87\xbaoutput [b, 784] => [b, 10]\n        out = model(x)\n        # [b] => [b, 10]\n        y_onehot = tf.one_hot(y, depth=10)\n        # \xe8\xae\xa1\xe7\xae\x97\xe5\xb7\xae\xe7\x9a\x84\xe5\xb9\xb3\xe6\x96\xb9\xe5\x92\x8c\xef\xbc\x8c[b, 10]\n        loss = tf.square(out-y_onehot)\n        # \xe8\xae\xa1\xe7\xae\x97\xe6\xaf\x8f\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe8\xaf\xaf\xe5\xb7\xae\xef\xbc\x8c[b]\n        loss = tf.reduce_sum(loss) / x.shape[0]\n\n\n    acc_meter.update_state(tf.argmax(out, axis=1), y)\n\n    grads = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n\n    if step % 200==0:\n\n        print(step, 'loss:', float(loss), 'acc:', acc_meter.result().numpy())\n        acc_meter.reset_states()\n"""
ch04-TensorFlow基础/4.10-forward-prop.py,17,"b""\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras.datasets as datasets\n\nplt.rcParams['font.size'] = 16\nplt.rcParams['font.family'] = ['STKaiti']\nplt.rcParams['axes.unicode_minus'] = False\n\n\ndef load_data():\n    # \xe5\x8a\xa0\xe8\xbd\xbd MNIST \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n    (x, y), (x_val, y_val) = datasets.mnist.load_data()\n    # \xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xba\xe6\xb5\xae\xe7\x82\xb9\xe5\xbc\xa0\xe9\x87\x8f\xef\xbc\x8c \xe5\xb9\xb6\xe7\xbc\xa9\xe6\x94\xbe\xe5\x88\xb0-1~1\n    x = tf.convert_to_tensor(x, dtype=tf.float32) / 255.\n    # \xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xba\xe6\x95\xb4\xe5\xbd\xa2\xe5\xbc\xa0\xe9\x87\x8f\n    y = tf.convert_to_tensor(y, dtype=tf.int32)\n    # one-hot \xe7\xbc\x96\xe7\xa0\x81\n    y = tf.one_hot(y, depth=10)\n\n    # \xe6\x94\xb9\xe5\x8f\x98\xe8\xa7\x86\xe5\x9b\xbe\xef\xbc\x8c [b, 28, 28] => [b, 28*28]\n    x = tf.reshape(x, (-1, 28 * 28))\n\n    # \xe6\x9e\x84\xe5\xbb\xba\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\xaf\xb9\xe8\xb1\xa1\n    train_dataset = tf.data.Dataset.from_tensor_slices((x, y))\n    # \xe6\x89\xb9\xe9\x87\x8f\xe8\xae\xad\xe7\xbb\x83\n    train_dataset = train_dataset.batch(200)\n    return train_dataset\n\n\ndef init_paramaters():\n    # \xe6\xaf\x8f\xe5\xb1\x82\xe7\x9a\x84\xe5\xbc\xa0\xe9\x87\x8f\xe9\x83\xbd\xe9\x9c\x80\xe8\xa6\x81\xe8\xa2\xab\xe4\xbc\x98\xe5\x8c\x96\xef\xbc\x8c\xe6\x95\x85\xe4\xbd\xbf\xe7\x94\xa8 Variable \xe7\xb1\xbb\xe5\x9e\x8b\xef\xbc\x8c\xe5\xb9\xb6\xe4\xbd\xbf\xe7\x94\xa8\xe6\x88\xaa\xe6\x96\xad\xe7\x9a\x84\xe6\xad\xa3\xe5\xa4\xaa\xe5\x88\x86\xe5\xb8\x83\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x9d\x83\xe5\x80\xbc\xe5\xbc\xa0\xe9\x87\x8f\n    # \xe5\x81\x8f\xe7\xbd\xae\xe5\x90\x91\xe9\x87\x8f\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xb8\xba 0 \xe5\x8d\xb3\xe5\x8f\xaf\n    # \xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n    w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))\n    b1 = tf.Variable(tf.zeros([256]))\n    # \xe7\xac\xac\xe4\xba\x8c\xe5\xb1\x82\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n    w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))\n    b2 = tf.Variable(tf.zeros([128]))\n    # \xe7\xac\xac\xe4\xb8\x89\xe5\xb1\x82\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n    w3 = tf.Variable(tf.random.truncated_normal([128, 10], stddev=0.1))\n    b3 = tf.Variable(tf.zeros([10]))\n    return w1, b1, w2, b2, w3, b3\n\n\ndef train_epoch(epoch, train_dataset, w1, b1, w2, b2, w3, b3, lr=0.001):\n    for step, (x, y) in enumerate(train_dataset):\n        with tf.GradientTape() as tape:\n            # \xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\xe8\xae\xa1\xe7\xae\x97\xef\xbc\x8c [b, 784]@[784, 256] + [256] => [b, 256] + [256] => [b,256] + [b, 256]\n            h1 = x @ w1 + tf.broadcast_to(b1, (x.shape[0], 256))\n            h1 = tf.nn.relu(h1)  # \xe9\x80\x9a\xe8\xbf\x87\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\n\n            # \xe7\xac\xac\xe4\xba\x8c\xe5\xb1\x82\xe8\xae\xa1\xe7\xae\x97\xef\xbc\x8c [b, 256] => [b, 128]\n            h2 = h1 @ w2 + b2\n            h2 = tf.nn.relu(h2)\n            # \xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe8\xae\xa1\xe7\xae\x97\xef\xbc\x8c [b, 128] => [b, 10]\n            out = h2 @ w3 + b3\n\n            # \xe8\xae\xa1\xe7\xae\x97\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe4\xb8\x8e\xe6\xa0\x87\xe7\xad\xbe\xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84\xe5\x9d\x87\xe6\x96\xb9\xe5\xb7\xae\xef\xbc\x8c mse = mean(sum(y-out)^2)\n            # [b, 10]\n            loss = tf.square(y - out)\n            # \xe8\xaf\xaf\xe5\xb7\xae\xe6\xa0\x87\xe9\x87\x8f\xef\xbc\x8c mean: scalar\n            loss = tf.reduce_mean(loss)\n\n            # \xe8\x87\xaa\xe5\x8a\xa8\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe6\xb1\x82\xe6\xa2\xaf\xe5\xba\xa6\xe7\x9a\x84\xe5\xbc\xa0\xe9\x87\x8f\xe6\x9c\x89[w1, b1, w2, b2, w3, b3]\n            grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n\n        # \xe6\xa2\xaf\xe5\xba\xa6\xe6\x9b\xb4\xe6\x96\xb0\xef\xbc\x8c assign_sub \xe5\xb0\x86\xe5\xbd\x93\xe5\x89\x8d\xe5\x80\xbc\xe5\x87\x8f\xe5\x8e\xbb\xe5\x8f\x82\xe6\x95\xb0\xe5\x80\xbc\xef\xbc\x8c\xe5\x8e\x9f\xe5\x9c\xb0\xe6\x9b\xb4\xe6\x96\xb0\n        w1.assign_sub(lr * grads[0])\n        b1.assign_sub(lr * grads[1])\n        w2.assign_sub(lr * grads[2])\n        b2.assign_sub(lr * grads[3])\n        w3.assign_sub(lr * grads[4])\n        b3.assign_sub(lr * grads[5])\n\n        if step % 100 == 0:\n            print(epoch, step, 'loss:', loss.numpy())\n\n    return loss.numpy()\n\n\ndef train(epochs):\n    losses = []\n    train_dataset = load_data()\n    w1, b1, w2, b2, w3, b3 = init_paramaters()\n    for epoch in range(epochs):\n        loss = train_epoch(epoch, train_dataset, w1, b1, w2, b2, w3, b3, lr=0.001)\n        losses.append(loss)\n\n    x = [i for i in range(0, epochs)]\n    # \xe7\xbb\x98\xe5\x88\xb6\xe6\x9b\xb2\xe7\xba\xbf\n    plt.plot(x, losses, color='blue', marker='s', label='\xe8\xae\xad\xe7\xbb\x83')\n    plt.xlabel('Epoch')\n    plt.ylabel('MSE')\n    plt.legend()\n    plt.savefig('MNIST\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\x9a\x84\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe8\xae\xad\xe7\xbb\x83\xe8\xaf\xaf\xe5\xb7\xae\xe6\x9b\xb2\xe7\xba\xbf.png')\n    plt.close()\n\n\nif __name__ == '__main__':\n    train(epochs=20)\n"""
ch05-TensorFlow进阶/acc_topk.py,11,"b""import  tensorflow as tf\nimport  os\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\ntf.random.set_seed(2467)\n\ndef accuracy(output, target, topk=(1,)):\n    maxk = max(topk)\n    batch_size = target.shape[0]\n\n    pred = tf.math.top_k(output, maxk).indices\n    pred = tf.transpose(pred, perm=[1, 0])\n    target_ = tf.broadcast_to(target, pred.shape)\n    # [10, b]\n    correct = tf.equal(pred, target_)\n\n    res = []\n    for k in topk:\n        correct_k = tf.cast(tf.reshape(correct[:k], [-1]), dtype=tf.float32)\n        correct_k = tf.reduce_sum(correct_k)\n        acc = float(correct_k* (100.0 / batch_size) )\n        res.append(acc)\n\n    return res\n\n\n\noutput = tf.random.normal([10, 6])\noutput = tf.math.softmax(output, axis=1)\ntarget = tf.random.uniform([10], maxval=6, dtype=tf.int32)\nprint('prob:', output.numpy())\npred = tf.argmax(output, axis=1)\nprint('pred:', pred.numpy())\nprint('label:', target.numpy())\n\nacc = accuracy(output, target, topk=(1,2,3,4,5,6))\nprint('top-1-6 acc:', acc)"""
ch05-TensorFlow进阶/gradient_clip.py,19,"b""import  tensorflow as tf\nfrom    tensorflow import keras\nfrom    tensorflow.keras import datasets, layers, optimizers\nimport  os\n\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\nprint(tf.__version__)\n\n(x, y), _ = datasets.mnist.load_data()\nx = tf.convert_to_tensor(x, dtype=tf.float32) / 50.\ny = tf.convert_to_tensor(y)\ny = tf.one_hot(y, depth=10)\nprint('x:', x.shape, 'y:', y.shape)\ntrain_db = tf.data.Dataset.from_tensor_slices((x,y)).batch(128).repeat(30)\nx,y = next(iter(train_db))\nprint('sample:', x.shape, y.shape)\n# print(x[0], y[0])\n\n\n\ndef main():\n\n    # 784 => 512\n    w1, b1 = tf.Variable(tf.random.truncated_normal([784, 512], stddev=0.1)), tf.Variable(tf.zeros([512]))\n    # 512 => 256\n    w2, b2 = tf.Variable(tf.random.truncated_normal([512, 256], stddev=0.1)), tf.Variable(tf.zeros([256]))\n    # 256 => 10\n    w3, b3 = tf.Variable(tf.random.truncated_normal([256, 10], stddev=0.1)), tf.Variable(tf.zeros([10]))\n\n\n\n    optimizer = optimizers.SGD(lr=0.01)\n\n\n    for step, (x,y) in enumerate(train_db):\n\n        # [b, 28, 28] => [b, 784]\n        x = tf.reshape(x, (-1, 784))\n\n        with tf.GradientTape() as tape:\n\n            # layer1.\n            h1 = x @ w1 + b1\n            h1 = tf.nn.relu(h1)\n            # layer2\n            h2 = h1 @ w2 + b2\n            h2 = tf.nn.relu(h2)\n            # output\n            out = h2 @ w3 + b3\n            # out = tf.nn.relu(out)\n\n            # compute loss\n            # [b, 10] - [b, 10]\n            loss = tf.square(y-out)\n            # [b, 10] => [b]\n            loss = tf.reduce_mean(loss, axis=1)\n            # [b] => scalar\n            loss = tf.reduce_mean(loss)\n\n\n\n        # compute gradient\n        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3])\n        # print('==before==')\n        # for g in grads:\n        #     print(tf.norm(g))\n        \n        grads,  _ = tf.clip_by_global_norm(grads, 15)\n\n        # print('==after==')\n        # for g in grads:\n        #     print(tf.norm(g))\n        # update w' = w - lr*grad\n        optimizer.apply_gradients(zip(grads, [w1, b1, w2, b2, w3, b3]))\n\n\n\n        if step % 100 == 0:\n            print(step, 'loss:', float(loss))\n\n\n\n\nif __name__ == '__main__':\n    main()"""
ch05-TensorFlow进阶/mnist_tensor.py,23,"b""#%%\nimport  matplotlib\nfrom    matplotlib import pyplot as plt\n# Default parameters for plots\nmatplotlib.rcParams['font.size'] = 20\nmatplotlib.rcParams['figure.titlesize'] = 20\nmatplotlib.rcParams['figure.figsize'] = [9, 7]\nmatplotlib.rcParams['font.family'] = ['STKaiTi']\nmatplotlib.rcParams['axes.unicode_minus']=False \nimport  tensorflow as tf\nfrom    tensorflow import keras\nfrom    tensorflow.keras import datasets, layers, optimizers\nimport  os\n\n\n\n\n\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\nprint(tf.__version__)\n\n\ndef preprocess(x, y): \n    # [b, 28, 28], [b]\n    print(x.shape,y.shape)\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    x = tf.reshape(x, [-1, 28*28])\n    y = tf.cast(y, dtype=tf.int32)\n    y = tf.one_hot(y, depth=10)\n\n    return x,y\n\n#%%\n(x, y), (x_test, y_test) = datasets.mnist.load_data()\nprint('x:', x.shape, 'y:', y.shape, 'x test:', x_test.shape, 'y test:', y_test)\n#%%\nbatchsz = 512\ntrain_db = tf.data.Dataset.from_tensor_slices((x, y))\ntrain_db = train_db.shuffle(1000)\ntrain_db = train_db.batch(batchsz)\ntrain_db = train_db.map(preprocess)\ntrain_db = train_db.repeat(20)\n\n#%%\n\ntest_db = tf.data.Dataset.from_tensor_slices((x_test, y_test))\ntest_db = test_db.shuffle(1000).batch(batchsz).map(preprocess)\nx,y = next(iter(train_db))\nprint('train sample:', x.shape, y.shape)\n# print(x[0], y[0])\n\n\n\n\n#%%\ndef main():\n\n    # learning rate\n    lr = 1e-2\n    accs,losses = [], []\n\n\n    # 784 => 512\n    w1, b1 = tf.Variable(tf.random.normal([784, 256], stddev=0.1)), tf.Variable(tf.zeros([256]))\n    # 512 => 256\n    w2, b2 = tf.Variable(tf.random.normal([256, 128], stddev=0.1)), tf.Variable(tf.zeros([128]))\n    # 256 => 10\n    w3, b3 = tf.Variable(tf.random.normal([128, 10], stddev=0.1)), tf.Variable(tf.zeros([10]))\n\n\n\n \n\n    for step, (x,y) in enumerate(train_db):\n \n        # [b, 28, 28] => [b, 784]\n        x = tf.reshape(x, (-1, 784))\n\n        with tf.GradientTape() as tape:\n\n            # layer1.\n            h1 = x @ w1 + b1\n            h1 = tf.nn.relu(h1)\n            # layer2\n            h2 = h1 @ w2 + b2\n            h2 = tf.nn.relu(h2)\n            # output\n            out = h2 @ w3 + b3\n            # out = tf.nn.relu(out)\n\n            # compute loss\n            # [b, 10] - [b, 10]\n            loss = tf.square(y-out)\n            # [b, 10] => scalar\n            loss = tf.reduce_mean(loss)\n\n \n        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3]) \n        for p, g in zip([w1, b1, w2, b2, w3, b3], grads):\n            p.assign_sub(lr * g)\n\n\n        # print\n        if step % 80 == 0:\n            print(step, 'loss:', float(loss))\n            losses.append(float(loss))\n \n        if step %80 == 0:\n            # evaluate/test\n            total, total_correct = 0., 0\n\n            for x, y in test_db:\n                # layer1.\n                h1 = x @ w1 + b1\n                h1 = tf.nn.relu(h1)\n                # layer2\n                h2 = h1 @ w2 + b2\n                h2 = tf.nn.relu(h2)\n                # output\n                out = h2 @ w3 + b3\n                # [b, 10] => [b]\n                pred = tf.argmax(out, axis=1)\n                # convert one_hot y to number y\n                y = tf.argmax(y, axis=1)\n                # bool type\n                correct = tf.equal(pred, y)\n                # bool tensor => int tensor => numpy\n                total_correct += tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()\n                total += x.shape[0]\n\n            print(step, 'Evaluate Acc:', total_correct/total)\n\n            accs.append(total_correct/total)\n\n\n    plt.figure()\n    x = [i*80 for i in range(len(losses))]\n    plt.plot(x, losses, color='C0', marker='s', label='\xe8\xae\xad\xe7\xbb\x83')\n    plt.ylabel('MSE')\n    plt.xlabel('Step')\n    plt.legend()\n    plt.savefig('train.svg')\n\n    plt.figure()\n    plt.plot(x, accs, color='C1', marker='s', label='\xe6\xb5\x8b\xe8\xaf\x95')\n    plt.ylabel('\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87')\n    plt.xlabel('Step')\n    plt.legend()\n    plt.savefig('test.svg')\n\nif __name__ == '__main__':\n    main()"""
ch06-神经网络/auto_efficency_regression.py,8,"b'#%%\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport pathlib\nimport os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n \nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, losses\n\nprint(tf.__version__)\n\n\n# \xe5\x9c\xa8\xe7\xba\xbf\xe4\xb8\x8b\xe8\xbd\xbd\xe6\xb1\xbd\xe8\xbd\xa6\xe6\x95\x88\xe8\x83\xbd\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\ndataset_path = keras.utils.get_file(""auto-mpg.data"", ""http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data"")\n\n# \xe6\x95\x88\xe8\x83\xbd\xef\xbc\x88\xe5\x85\xac\xe9\x87\x8c\xe6\x95\xb0\xe6\xaf\x8f\xe5\x8a\xa0\xe4\xbb\x91\xef\xbc\x89\xef\xbc\x8c\xe6\xb0\x94\xe7\xbc\xb8\xe6\x95\xb0\xef\xbc\x8c\xe6\x8e\x92\xe9\x87\x8f\xef\xbc\x8c\xe9\xa9\xac\xe5\x8a\x9b\xef\xbc\x8c\xe9\x87\x8d\xe9\x87\x8f\n# \xe5\x8a\xa0\xe9\x80\x9f\xe5\xba\xa6\xef\xbc\x8c\xe5\x9e\x8b\xe5\x8f\xb7\xe5\xb9\xb4\xe4\xbb\xbd\xef\xbc\x8c\xe4\xba\xa7\xe5\x9c\xb0\ncolumn_names = [\'MPG\',\'Cylinders\',\'Displacement\',\'Horsepower\',\'Weight\',\n                \'Acceleration\', \'Model Year\', \'Origin\']\nraw_dataset = pd.read_csv(dataset_path, names=column_names,\n                      na_values = ""?"", comment=\'\\t\',\n                      sep="" "", skipinitialspace=True)\n\ndataset = raw_dataset.copy()\n# \xe6\x9f\xa5\xe7\x9c\x8b\xe9\x83\xa8\xe5\x88\x86\xe6\x95\xb0\xe6\x8d\xae\ndataset.tail()\ndataset.head()\ndataset\n#%%\n\n\n#%%\n\n# \xe7\xbb\x9f\xe8\xae\xa1\xe7\xa9\xba\xe7\x99\xbd\xe6\x95\xb0\xe6\x8d\xae,\xe5\xb9\xb6\xe6\xb8\x85\xe9\x99\xa4\ndataset.isna().sum()\ndataset = dataset.dropna()\ndataset.isna().sum()\ndataset\n#%%\n\n# \xe5\xa4\x84\xe7\x90\x86\xe7\xb1\xbb\xe5\x88\xab\xe5\x9e\x8b\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe5\x85\xb6\xe4\xb8\xadorigin\xe5\x88\x97\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xba\x86\xe7\xb1\xbb\xe5\x88\xab1,2,3,\xe5\x88\x86\xe5\xb8\x83\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xba\xa7\xe5\x9c\xb0\xef\xbc\x9a\xe7\xbe\x8e\xe5\x9b\xbd\xe3\x80\x81\xe6\xac\xa7\xe6\xb4\xb2\xe3\x80\x81\xe6\x97\xa5\xe6\x9c\xac\n# \xe5\x85\xb6\xe5\xbc\xb9\xe5\x87\xba\xe8\xbf\x99\xe4\xb8\x80\xe5\x88\x97\norigin = dataset.pop(\'Origin\')\n# \xe6\xa0\xb9\xe6\x8d\xaeorigin\xe5\x88\x97\xe6\x9d\xa5\xe5\x86\x99\xe5\x85\xa5\xe6\x96\xb0\xe5\x88\x97\ndataset[\'USA\'] = (origin == 1)*1.0\ndataset[\'Europe\'] = (origin == 2)*1.0\ndataset[\'Japan\'] = (origin == 3)*1.0\ndataset.tail()\n\n\n# \xe5\x88\x87\xe5\x88\x86\xe4\xb8\xba\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe5\x92\x8c\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\ntrain_dataset = dataset.sample(frac=0.8,random_state=0)\ntest_dataset = dataset.drop(train_dataset.index) \n\n\n#%% \xe7\xbb\x9f\xe8\xae\xa1\xe6\x95\xb0\xe6\x8d\xae\nsns.pairplot(train_dataset[[""Cylinders"", ""Displacement"", ""Weight"", ""MPG""]], \ndiag_kind=""kde"")\n#%%\n# \xe6\x9f\xa5\xe7\x9c\x8b\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5X\xe7\x9a\x84\xe7\xbb\x9f\xe8\xae\xa1\xe6\x95\xb0\xe6\x8d\xae\ntrain_stats = train_dataset.describe()\ntrain_stats.pop(""MPG"")\ntrain_stats = train_stats.transpose()\ntrain_stats\n\n\n# \xe7\xa7\xbb\xe5\x8a\xa8MPG\xe6\xb2\xb9\xe8\x80\x97\xe6\x95\x88\xe8\x83\xbd\xe8\xbf\x99\xe4\xb8\x80\xe5\x88\x97\xe4\xb8\xba\xe7\x9c\x9f\xe5\xae\x9e\xe6\xa0\x87\xe7\xad\xbeY\ntrain_labels = train_dataset.pop(\'MPG\')\ntest_labels = test_dataset.pop(\'MPG\')\n\n\n# \xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe6\x95\xb0\xe6\x8d\xae\ndef norm(x):\n  return (x - train_stats[\'mean\']) / train_stats[\'std\']\nnormed_train_data = norm(train_dataset)\nnormed_test_data = norm(test_dataset)\n#%%\n\nprint(normed_train_data.shape,train_labels.shape)\nprint(normed_test_data.shape, test_labels.shape)\n#%%\n\nclass Network(keras.Model):\n    # \xe5\x9b\x9e\xe5\xbd\x92\xe7\xbd\x91\xe7\xbb\x9c\n    def __init__(self):\n        super(Network, self).__init__()\n        # \xe5\x88\x9b\xe5\xbb\xba3\xe4\xb8\xaa\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\n        self.fc1 = layers.Dense(64, activation=\'relu\')\n        self.fc2 = layers.Dense(64, activation=\'relu\')\n        self.fc3 = layers.Dense(1)\n\n    def call(self, inputs, training=None, mask=None):\n        # \xe4\xbe\x9d\xe6\xac\xa1\xe9\x80\x9a\xe8\xbf\x873\xe4\xb8\xaa\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\n        x = self.fc1(inputs)\n        x = self.fc2(x)\n        x = self.fc3(x)\n\n        return x\n\nmodel = Network()\nmodel.build(input_shape=(None, 9))\nmodel.summary()\noptimizer = tf.keras.optimizers.RMSprop(0.001)\ntrain_db = tf.data.Dataset.from_tensor_slices((normed_train_data.values, train_labels.values))\ntrain_db = train_db.shuffle(100).batch(32)\n\n# # \xe6\x9c\xaa\xe8\xae\xad\xe7\xbb\x83\xe6\x97\xb6\xe6\xb5\x8b\xe8\xaf\x95\n# example_batch = normed_train_data[:10]\n# example_result = model.predict(example_batch)\n# example_result\n\n\ntrain_mae_losses = []\ntest_mae_losses = []\nfor epoch in range(200):\n    for step, (x,y) in enumerate(train_db):\n\n        with tf.GradientTape() as tape:\n            out = model(x)\n            loss = tf.reduce_mean(losses.MSE(y, out))\n            mae_loss = tf.reduce_mean(losses.MAE(y, out)) \n\n        if step % 10 == 0:\n            print(epoch, step, float(loss))\n\n        grads = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n    train_mae_losses.append(float(mae_loss))\n    out = model(tf.constant(normed_test_data.values))\n    test_mae_losses.append(tf.reduce_mean(losses.MAE(test_labels, out)))\n\n\nplt.figure()\nplt.xlabel(\'Epoch\')\nplt.ylabel(\'MAE\')\nplt.plot(train_mae_losses,  label=\'Train\')\n\nplt.plot(test_mae_losses, label=\'Test\')\nplt.legend()\n \n# plt.ylim([0,10])\nplt.legend()\nplt.savefig(\'auto.svg\')\nplt.show() \n\n\n\n\n#%%\n'"
ch06-神经网络/forward.py,33,"b""#%%\n\nimport  tensorflow as tf\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers\nfrom    tensorflow.keras import datasets\nimport  os\n\n\n#%% \nx = tf.random.normal([2,28*28])\nw1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))\nb1 = tf.Variable(tf.zeros([256]))\no1 = tf.matmul(x,w1) + b1\no1\n#%%\nx = tf.random.normal([4,28*28])\nfc1 = layers.Dense(256, activation=tf.nn.relu) \nfc2 = layers.Dense(128, activation=tf.nn.relu) \nfc3 = layers.Dense(64, activation=tf.nn.relu) \nfc4 = layers.Dense(10, activation=None) \nh1 = fc1(x)\nh2 = fc2(h1)\nh3 = fc3(h2)\nh4 = fc4(h3)\n\nmodel = layers.Sequential([\n    layers.Dense(256, activation=tf.nn.relu) ,\n    layers.Dense(128, activation=tf.nn.relu) ,\n    layers.Dense(64, activation=tf.nn.relu) ,\n    layers.Dense(10, activation=None) ,\n])\nout = model(x)\n\n#%%\n256*784+256+128*256+128+64*128+64+10*64+10\n#%%\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n# x: [60k, 28, 28],\n# y: [60k]\n(x, y), _ = datasets.mnist.load_data()\n# x: [0~255] => [0~1.]\nx = tf.convert_to_tensor(x, dtype=tf.float32) / 255.\ny = tf.convert_to_tensor(y, dtype=tf.int32)\n\nprint(x.shape, y.shape, x.dtype, y.dtype)\nprint(tf.reduce_min(x), tf.reduce_max(x))\nprint(tf.reduce_min(y), tf.reduce_max(y))\n\n\ntrain_db = tf.data.Dataset.from_tensor_slices((x,y)).batch(128)\ntrain_iter = iter(train_db)\nsample = next(train_iter)\nprint('batch:', sample[0].shape, sample[1].shape)\n\n\n# [b, 784] => [b, 256] => [b, 128] => [b, 10]\n# [dim_in, dim_out], [dim_out]\n# \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x821\xe5\xbc\xa0\xe9\x87\x8f\nw1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))\nb1 = tf.Variable(tf.zeros([256]))\n# \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x822\xe5\xbc\xa0\xe9\x87\x8f\nw2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))\nb2 = tf.Variable(tf.zeros([128]))\n# \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x823\xe5\xbc\xa0\xe9\x87\x8f\nw3 = tf.Variable(tf.random.truncated_normal([128, 64], stddev=0.1))\nb3 = tf.Variable(tf.zeros([64]))\n# \xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe5\xbc\xa0\xe9\x87\x8f\nw4 = tf.Variable(tf.random.truncated_normal([64, 10], stddev=0.1))\nb4 = tf.Variable(tf.zeros([10]))\n\nlr = 1e-3\n\nfor epoch in range(10): # iterate db for 10\n    for step, (x, y) in enumerate(train_db): # for every batch\n        # x:[128, 28, 28]\n        # y: [128]\n\n        # [b, 28, 28] => [b, 28*28]\n        x = tf.reshape(x, [-1, 28*28])\n\n        with tf.GradientTape() as tape: # tf.Variable\n            # x: [b, 28*28]\n            #  \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x821\xe5\x89\x8d\xe5\x90\x91\xe8\xae\xa1\xe7\xae\x97\xef\xbc\x8c[b, 28*28] => [b, 256]\n            h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256])\n            h1 = tf.nn.relu(h1)\n            # \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x822\xe5\x89\x8d\xe5\x90\x91\xe8\xae\xa1\xe7\xae\x97\xef\xbc\x8c[b, 256] => [b, 128]\n            h2 = h1@w2 + b2\n            h2 = tf.nn.relu(h2)\n            # \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x823\xe5\x89\x8d\xe5\x90\x91\xe8\xae\xa1\xe7\xae\x97\xef\xbc\x8c[b, 128] => [b, 64] \n            h3 = h2@w3 + b3\n            h3 = tf.nn.relu(h3)\n            # \xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe5\x89\x8d\xe5\x90\x91\xe8\xae\xa1\xe7\xae\x97\xef\xbc\x8c[b, 64] => [b, 10] \n            h4 = h3@w4 + b4\n            out = h4\n\n            # compute loss\n            # out: [b, 10]\n            # y: [b] => [b, 10]\n            y_onehot = tf.one_hot(y, depth=10)\n\n            # mse = mean(sum(y-out)^2)\n            # [b, 10]\n            loss = tf.square(y_onehot - out)\n            # mean: scalar\n            loss = tf.reduce_mean(loss)\n\n        # compute gradients\n        grads = tape.gradient(loss, [w1, b1, w2, b2, w3, b3, w4, b4])\n        # print(grads)\n        # w1 = w1 - lr * w1_grad\n        w1.assign_sub(lr * grads[0])\n        b1.assign_sub(lr * grads[1])\n        w2.assign_sub(lr * grads[2])\n        b2.assign_sub(lr * grads[3])\n        w3.assign_sub(lr * grads[4])\n        b3.assign_sub(lr * grads[5])\n        w4.assign_sub(lr * grads[6])\n        b4.assign_sub(lr * grads[7])\n\n\n        if step % 100 == 0:\n            print(epoch, step, 'loss:', float(loss))\n\n\n\n\n#%%\n"""
ch06-神经网络/nb.py,17,"b'#%%\nimport  tensorflow as tf\nfrom    tensorflow import keras\nfrom    tensorflow.keras import datasets, layers\nimport  os\n\n\n#%%\na = tf.random.normal([4,35,8]) # \xe6\xa8\xa1\xe6\x8b\x9f\xe6\x88\x90\xe7\xbb\xa9\xe5\x86\x8cA\nb = tf.random.normal([6,35,8]) # \xe6\xa8\xa1\xe6\x8b\x9f\xe6\x88\x90\xe7\xbb\xa9\xe5\x86\x8cB\ntf.concat([a,b],axis=0) # \xe5\x90\x88\xe5\xb9\xb6\xe6\x88\x90\xe7\xbb\xa9\xe5\x86\x8c\n\n\n#%%\nx = tf.random.normal([2,784])\nw1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))\nb1 = tf.Variable(tf.zeros([256]))\no1 = tf.matmul(x,w1) + b1  #\no1 = tf.nn.relu(o1)\no1\n#%%\nx = tf.random.normal([4,28*28])\n# \xe5\x88\x9b\xe5\xbb\xba\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xef\xbc\x8c\xe6\x8c\x87\xe5\xae\x9a\xe8\xbe\x93\xe5\x87\xba\xe8\x8a\x82\xe7\x82\xb9\xe6\x95\xb0\xe5\x92\x8c\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\nfc = layers.Dense(512, activation=tf.nn.relu) \nh1 = fc(x)  # \xe9\x80\x9a\xe8\xbf\x87fc\xe7\xb1\xbb\xe5\xae\x8c\xe6\x88\x90\xe4\xb8\x80\xe6\xac\xa1\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\n\n\n#%%\nvars(fc)\n\n#%%\nx = tf.random.normal([4,4])\n# \xe5\x88\x9b\xe5\xbb\xba\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xef\xbc\x8c\xe6\x8c\x87\xe5\xae\x9a\xe8\xbe\x93\xe5\x87\xba\xe8\x8a\x82\xe7\x82\xb9\xe6\x95\xb0\xe5\x92\x8c\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\nfc = layers.Dense(3, activation=tf.nn.relu) \nh1 = fc(x)  # \xe9\x80\x9a\xe8\xbf\x87fc\xe7\xb1\xbb\xe5\xae\x8c\xe6\x88\x90\xe4\xb8\x80\xe6\xac\xa1\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\n\n\n#%%\nfc.non_trainable_variables\n\n#%%\nembedding = layers.Embedding(10000, 100)\n\n#%%\nx = tf.ones([25000,80])\n\n#%%\n\nembedding(x)\n\n#%%\nz = tf.random.normal([2,10]) # \xe6\x9e\x84\xe9\x80\xa0\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\ny_onehot = tf.constant([1,3]) # \xe6\x9e\x84\xe9\x80\xa0\xe7\x9c\x9f\xe5\xae\x9e\xe5\x80\xbc\ny_onehot = tf.one_hot(y_onehot, depth=10) # one-hot\xe7\xbc\x96\xe7\xa0\x81\n# \xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe6\x9c\xaa\xe4\xbd\xbf\xe7\x94\xa8Softmax\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe6\x95\x85from_logits\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xbaTrue\nloss = keras.losses.categorical_crossentropy(y_onehot,z,from_logits=True)\nloss = tf.reduce_mean(loss) # \xe8\xae\xa1\xe7\xae\x97\xe5\xb9\xb3\xe5\x9d\x87\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe6\x8d\x9f\xe5\xa4\xb1\nloss\n\n\n#%%\ncriteon = keras.losses.CategoricalCrossentropy(from_logits=True)\nloss = criteon(y_onehot,z) # \xe8\xae\xa1\xe7\xae\x97\xe6\x8d\x9f\xe5\xa4\xb1\nloss\n\n\n#%%\n'"
ch07-反向传播算法/2nd_derivative.py,5,"b'import tensorflow as tf\n\nw = tf.Variable(1.0)\nb = tf.Variable(2.0)\nx = tf.Variable(3.0)\n\nwith tf.GradientTape() as t1:\n  with tf.GradientTape() as t2:\n    y = x * w + b\n  dy_dw, dy_db = t2.gradient(y, [w, b])\nd2y_dw2 = t1.gradient(dy_dw, w)\n\nprint(dy_dw)\nprint(dy_db)\nprint(d2y_dw2)\n\nassert dy_dw.numpy() == 3.0\nassert d2y_dw2 is None'"
ch07-反向传播算法/chain_rule.py,7,"b'import tensorflow as tf \n\n# \xe6\x9e\x84\xe5\xbb\xba\xe5\xbe\x85\xe4\xbc\x98\xe5\x8c\x96\xe5\x8f\x98\xe9\x87\x8f\nx = tf.constant(1.)\nw1 = tf.constant(2.)\nb1 = tf.constant(1.)\nw2 = tf.constant(2.)\nb2 = tf.constant(1.)\n\n\nwith tf.GradientTape(persistent=True) as tape:\n\t# \xe9\x9d\x9etf.Variable\xe7\xb1\xbb\xe5\x9e\x8b\xe7\x9a\x84\xe5\xbc\xa0\xe9\x87\x8f\xe9\x9c\x80\xe8\xa6\x81\xe4\xba\xba\xe4\xb8\xba\xe8\xae\xbe\xe7\xbd\xae\xe8\xae\xb0\xe5\xbd\x95\xe6\xa2\xaf\xe5\xba\xa6\xe4\xbf\xa1\xe6\x81\xaf\n\ttape.watch([w1, b1, w2, b2])\n\t# \xe6\x9e\x84\xe5\xbb\xba2\xe5\xb1\x82\xe7\xbd\x91\xe7\xbb\x9c\n\ty1 = x * w1 + b1\t\n\ty2 = y1 * w2 + b2\n\n# \xe7\x8b\xac\xe7\xab\x8b\xe6\xb1\x82\xe8\xa7\xa3\xe5\x87\xba\xe5\x90\x84\xe4\xb8\xaa\xe5\xaf\xbc\xe6\x95\xb0\ndy2_dy1 = tape.gradient(y2, [y1])[0]\ndy1_dw1 = tape.gradient(y1, [w1])[0]\ndy2_dw1 = tape.gradient(y2, [w1])[0]\n\n# \xe9\xaa\x8c\xe8\xaf\x81\xe9\x93\xbe\xe5\xbc\x8f\xe6\xb3\x95\xe5\x88\x99\nprint(dy2_dy1 * dy1_dw1)\nprint(dy2_dw1)'"
ch07-反向传播算法/crossentropy_loss.py,7,"b""import tensorflow as tf \n\n\ntf.random.set_seed(4323)\n\nx=tf.random.normal([1,3])\n\nw=tf.random.normal([3,2])\n\nb=tf.random.normal([2])\n\ny = tf.constant([0, 1])\n\n\nwith tf.GradientTape() as tape:\n\n\ttape.watch([w, b])\n\tlogits = (x@w+b)\n\tloss = tf.reduce_mean(tf.losses.categorical_crossentropy(y, logits, from_logits=True))\n\ngrads = tape.gradient(loss, [w, b])\nprint('w grad:', grads[0])\n\nprint('b grad:', grads[1])"""
ch07-反向传播算法/himmelblau.py,5,"b""import  numpy as np\nfrom    mpl_toolkits.mplot3d import Axes3D\nfrom    matplotlib import pyplot as plt\nimport  tensorflow as tf\n\n\n\ndef himmelblau(x):\n    # himmelblau\xe5\x87\xbd\xe6\x95\xb0\xe5\xae\x9e\xe7\x8e\xb0\n    return (x[0] ** 2 + x[1] - 11) ** 2 + (x[0] + x[1] ** 2 - 7) ** 2\n\n\nx = np.arange(-6, 6, 0.1)\ny = np.arange(-6, 6, 0.1)\nprint('x,y range:', x.shape, y.shape)\n# \xe7\x94\x9f\xe6\x88\x90x-y\xe5\xb9\xb3\xe9\x9d\xa2\xe9\x87\x87\xe6\xa0\xb7\xe7\xbd\x91\xe6\xa0\xbc\xe7\x82\xb9\xef\xbc\x8c\xe6\x96\xb9\xe4\xbe\xbf\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\nX, Y = np.meshgrid(x, y)\nprint('X,Y maps:', X.shape, Y.shape)\nZ = himmelblau([X, Y]) # \xe8\xae\xa1\xe7\xae\x97\xe7\xbd\x91\xe6\xa0\xbc\xe7\x82\xb9\xe4\xb8\x8a\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xe5\x80\xbc\n\n# \xe7\xbb\x98\xe5\x88\xb6himmelblau\xe5\x87\xbd\xe6\x95\xb0\xe6\x9b\xb2\xe9\x9d\xa2\nfig = plt.figure('himmelblau')\nax = fig.gca(projection='3d')\nax.plot_surface(X, Y, Z)\nax.view_init(60, -30)\nax.set_xlabel('x')\nax.set_ylabel('y')\nplt.show()\n\n# \xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x80\xbc\xe5\xaf\xb9\xe4\xbc\x98\xe5\x8c\x96\xe7\x9a\x84\xe5\xbd\xb1\xe5\x93\x8d\xe4\xb8\x8d\xe5\xae\xb9\xe5\xbf\xbd\xe8\xa7\x86\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x80\x9a\xe8\xbf\x87\xe5\xb0\x9d\xe8\xaf\x95\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x80\xbc\xef\xbc\x8c\n# \xe6\xa3\x80\xe9\xaa\x8c\xe5\x87\xbd\xe6\x95\xb0\xe4\xbc\x98\xe5\x8c\x96\xe7\x9a\x84\xe6\x9e\x81\xe5\xb0\x8f\xe5\x80\xbc\xe6\x83\x85\xe5\x86\xb5\n# [1., 0.], [-4, 0.], [4, 0.]\n# x = tf.constant([4., 0.])\n# x = tf.constant([1., 0.])\n# x = tf.constant([-4., 0.])\nx = tf.constant([-2., 2.])\n\nfor step in range(200):# \xe5\xbe\xaa\xe7\x8e\xaf\xe4\xbc\x98\xe5\x8c\x96\n    with tf.GradientTape() as tape: #\xe6\xa2\xaf\xe5\xba\xa6\xe8\xb7\x9f\xe8\xb8\xaa\n        tape.watch([x]) # \xe8\xae\xb0\xe5\xbd\x95\xe6\xa2\xaf\xe5\xba\xa6\n        y = himmelblau(x) # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n    # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n    grads = tape.gradient(y, [x])[0] \n    # \xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0,0.01\xe4\xb8\xba\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n    x -= 0.01*grads\n    # \xe6\x89\x93\xe5\x8d\xb0\xe4\xbc\x98\xe5\x8c\x96\xe7\x9a\x84\xe6\x9e\x81\xe5\xb0\x8f\xe5\x80\xbc\n    if step % 20 == 19:\n        print ('step {}: x = {}, f(x) = {}'\n               .format(step, x.numpy(), y.numpy()))"""
ch07-反向传播算法/mse_grad.py,7,"b""import tensorflow as tf \n\n\n\n\nx=tf.random.normal([1,3])\n\nw=tf.ones([3,2])\n\nb=tf.ones([2])\n\ny = tf.constant([0, 1])\n\n\nwith tf.GradientTape() as tape:\n\n\ttape.watch([w, b])\n\tlogits = tf.sigmoid(x@w+b) \n\tloss = tf.reduce_mean(tf.losses.MSE(y, logits))\n\ngrads = tape.gradient(loss, [w, b])\nprint('w grad:', grads[0])\n\nprint('b grad:', grads[1])\n\n\n"""
ch07-反向传播算法/multi_output_perceptron.py,7,"b""import tensorflow as tf \n\n\n\n\nx=tf.random.normal([1,3])\n\nw=tf.ones([3,2])\n\nb=tf.ones([2])\n\ny = tf.constant([0, 1])\n\n\nwith tf.GradientTape() as tape:\n\n\ttape.watch([w, b])\n\tlogits = tf.sigmoid(x@w+b) \n\tloss = tf.reduce_mean(tf.losses.MSE(y, logits))\n\ngrads = tape.gradient(loss, [w, b])\nprint('w grad:', grads[0])\n\nprint('b grad:', grads[1])\n\n\n"""
ch07-反向传播算法/numpy-backward-prop.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n""""""\n@author: HuRuiFeng\n@file: 7.9-backward-prop.py\n@time: 2020/2/24 17:32\n@desc: 7.9 \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\xae\x97\xe6\xb3\x95\xe5\xae\x9e\xe6\x88\x98\xe7\x9a\x84\xe4\xbb\xa3\xe7\xa0\x81\n""""""\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\n\nplt.rcParams[\'font.size\'] = 16\nplt.rcParams[\'font.family\'] = [\'STKaiti\']\nplt.rcParams[\'axes.unicode_minus\'] = False\n\n\ndef load_dataset():\n    # \xe9\x87\x87\xe6\xa0\xb7\xe7\x82\xb9\xe6\x95\xb0\n    N_SAMPLES = 2000\n    # \xe6\xb5\x8b\xe8\xaf\x95\xe6\x95\xb0\xe9\x87\x8f\xe6\xaf\x94\xe7\x8e\x87\n    TEST_SIZE = 0.3\n    # \xe5\x88\xa9\xe7\x94\xa8\xe5\xb7\xa5\xe5\x85\xb7\xe5\x87\xbd\xe6\x95\xb0\xe7\x9b\xb4\xe6\x8e\xa5\xe7\x94\x9f\xe6\x88\x90\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n    X, y = make_moons(n_samples=N_SAMPLES, noise=0.2, random_state=100)\n    # \xe5\xb0\x86 2000 \xe4\xb8\xaa\xe7\x82\xb9\xe6\x8c\x89\xe7\x9d\x80 7:3 \xe5\x88\x86\xe5\x89\xb2\xe4\xb8\xba\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe5\x92\x8c\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)\n    return X, y, X_train, X_test, y_train, y_test\n\n\ndef make_plot(X, y, plot_name, XX=None, YY=None, preds=None, dark=False):\n    # \xe7\xbb\x98\xe5\x88\xb6\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\x9a\x84\xe5\x88\x86\xe5\xb8\x83\xef\xbc\x8c X \xe4\xb8\xba 2D \xe5\x9d\x90\xe6\xa0\x87\xef\xbc\x8c y \xe4\xb8\xba\xe6\x95\xb0\xe6\x8d\xae\xe7\x82\xb9\xe7\x9a\x84\xe6\xa0\x87\xe7\xad\xbe\n    if (dark):\n        plt.style.use(\'dark_background\')\n    else:\n        sns.set_style(""whitegrid"")\n    plt.figure(figsize=(16, 12))\n    axes = plt.gca()\n    axes.set(xlabel=""$x_1$"", ylabel=""$x_2$"")\n    plt.title(plot_name, fontsize=30)\n    plt.subplots_adjust(left=0.20)\n    plt.subplots_adjust(right=0.80)\n    if XX is not None and YY is not None and preds is not None:\n        plt.contourf(XX, YY, preds.reshape(XX.shape), 25, alpha=1, cmap=plt.cm.Spectral)\n        plt.contour(XX, YY, preds.reshape(XX.shape), levels=[.5], cmap=""Greys"", vmin=0, vmax=.6)\n    # \xe7\xbb\x98\xe5\x88\xb6\xe6\x95\xa3\xe7\x82\xb9\xe5\x9b\xbe\xef\xbc\x8c\xe6\xa0\xb9\xe6\x8d\xae\xe6\xa0\x87\xe7\xad\xbe\xe5\x8c\xba\xe5\x88\x86\xe9\xa2\x9c\xe8\x89\xb2\n    plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), s=40, cmap=plt.cm.Spectral, edgecolors=\'none\')\n    plt.savefig(\'\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x88\x86\xe5\xb8\x83.svg\')\n    plt.close()\n\n\nclass Layer:\n    # \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe7\xbd\x91\xe7\xbb\x9c\xe5\xb1\x82\n    def __init__(self, n_input, n_neurons, activation=None, weights=None,\n                 bias=None):\n        """"""\n        :param int n_input: \xe8\xbe\x93\xe5\x85\xa5\xe8\x8a\x82\xe7\x82\xb9\xe6\x95\xb0\n        :param int n_neurons: \xe8\xbe\x93\xe5\x87\xba\xe8\x8a\x82\xe7\x82\xb9\xe6\x95\xb0\n        :param str activation: \xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe7\xb1\xbb\xe5\x9e\x8b\n        :param weights: \xe6\x9d\x83\xe5\x80\xbc\xe5\xbc\xa0\xe9\x87\x8f\xef\xbc\x8c\xe9\xbb\x98\xe8\xae\xa4\xe7\xb1\xbb\xe5\x86\x85\xe9\x83\xa8\xe7\x94\x9f\xe6\x88\x90\n        :param bias: \xe5\x81\x8f\xe7\xbd\xae\xef\xbc\x8c\xe9\xbb\x98\xe8\xae\xa4\xe7\xb1\xbb\xe5\x86\x85\xe9\x83\xa8\xe7\x94\x9f\xe6\x88\x90\n        """"""\n        # \xe9\x80\x9a\xe8\xbf\x87\xe6\xad\xa3\xe6\x80\x81\xe5\x88\x86\xe5\xb8\x83\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\xbd\x91\xe7\xbb\x9c\xe6\x9d\x83\xe5\x80\xbc\xef\xbc\x8c\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe9\x9d\x9e\xe5\xb8\xb8\xe9\x87\x8d\xe8\xa6\x81\xef\xbc\x8c\xe4\xb8\x8d\xe5\x90\x88\xe9\x80\x82\xe7\x9a\x84\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\xb0\x86\xe5\xaf\xbc\xe8\x87\xb4\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\x8d\xe6\x94\xb6\xe6\x95\x9b\n        self.weights = weights if weights is not None else np.random.randn(n_input, n_neurons) * np.sqrt(1 / n_neurons)\n        self.bias = bias if bias is not None else np.random.rand(n_neurons) * 0.1\n        self.activation = activation  # \xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe7\xb1\xbb\xe5\x9e\x8b\xef\xbc\x8c\xe5\xa6\x82\xe2\x80\x99sigmoid\xe2\x80\x99\n        self.last_activation = None  # \xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\x80\xbco\n        self.error = None  # \xe7\x94\xa8\xe4\xba\x8e\xe8\xae\xa1\xe7\xae\x97\xe5\xbd\x93\xe5\x89\x8d\xe5\xb1\x82\xe7\x9a\x84delta \xe5\x8f\x98\xe9\x87\x8f\xe7\x9a\x84\xe4\xb8\xad\xe9\x97\xb4\xe5\x8f\x98\xe9\x87\x8f\n        self.delta = None  # \xe8\xae\xb0\xe5\xbd\x95\xe5\xbd\x93\xe5\x89\x8d\xe5\xb1\x82\xe7\x9a\x84delta \xe5\x8f\x98\xe9\x87\x8f\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe8\xae\xa1\xe7\xae\x97\xe6\xa2\xaf\xe5\xba\xa6\n\n    # \xe7\xbd\x91\xe7\xbb\x9c\xe5\xb1\x82\xe7\x9a\x84\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe5\x87\xbd\xe6\x95\xb0\xe5\xae\x9e\xe7\x8e\xb0\xe5\xa6\x82\xe4\xb8\x8b\xef\xbc\x8c\xe5\x85\xb6\xe4\xb8\xadlast_activation \xe5\x8f\x98\xe9\x87\x8f\xe7\x94\xa8\xe4\xba\x8e\xe4\xbf\x9d\xe5\xad\x98\xe5\xbd\x93\xe5\x89\x8d\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\x80\xbc\xef\xbc\x9a\n    def activate(self, x):\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe5\x87\xbd\xe6\x95\xb0\n        r = np.dot(x, self.weights) + self.bias  # X@W+b\n        # \xe9\x80\x9a\xe8\xbf\x87\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xbao\n        self.last_activation = self._apply_activation(r)\n        return self.last_activation\n\n    # \xe4\xb8\x8a\xe8\xbf\xb0\xe4\xbb\xa3\xe7\xa0\x81\xe4\xb8\xad\xe7\x9a\x84self._apply_activation \xe5\x87\xbd\xe6\x95\xb0\xe5\xae\x9e\xe7\x8e\xb0\xe4\xba\x86\xe4\xb8\x8d\xe5\x90\x8c\xe7\xb1\xbb\xe5\x9e\x8b\xe7\x9a\x84\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe5\x89\x8d\xe5\x90\x91\xe8\xae\xa1\xe7\xae\x97\xe8\xbf\x87\xe7\xa8\x8b\xef\xbc\x8c\n    # \xe5\xb0\xbd\xe7\xae\xa1\xe6\xad\xa4\xe5\xa4\x84\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaa\xe4\xbd\xbf\xe7\x94\xa8Sigmoid \xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\x80\xe7\xa7\x8d\xe3\x80\x82\xe4\xbb\xa3\xe7\xa0\x81\xe5\xa6\x82\xe4\xb8\x8b\xef\xbc\x9a\n    def _apply_activation(self, r):\n        # \xe8\xae\xa1\xe7\xae\x97\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\n        if self.activation is None:\n            return r  # \xe6\x97\xa0\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe7\x9b\xb4\xe6\x8e\xa5\xe8\xbf\x94\xe5\x9b\x9e\n        # ReLU \xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\n        elif self.activation == \'relu\':\n            return np.maximum(r, 0)\n        # tanh \xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\n        elif self.activation == \'tanh\':\n            return np.tanh(r)\n        # sigmoid \xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\n        elif self.activation == \'sigmoid\':\n            return 1 / (1 + np.exp(-r))\n        return r\n\n    # \xe9\x92\x88\xe5\xaf\xb9\xe4\xba\x8e\xe4\xb8\x8d\xe5\x90\x8c\xe7\xb1\xbb\xe5\x9e\x8b\xe7\x9a\x84\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe5\xae\x83\xe4\xbb\xac\xe7\x9a\x84\xe5\xaf\xbc\xe6\x95\xb0\xe8\xae\xa1\xe7\xae\x97\xe5\xae\x9e\xe7\x8e\xb0\xe5\xa6\x82\xe4\xb8\x8b\xef\xbc\x9a\n    def apply_activation_derivative(self, r):\n        # \xe8\xae\xa1\xe7\xae\x97\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe5\xaf\xbc\xe6\x95\xb0\n        # \xe6\x97\xa0\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe5\xaf\xbc\xe6\x95\xb0\xe4\xb8\xba1\n        if self.activation is None:\n            return np.ones_like(r)\n        # ReLU \xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe5\xaf\xbc\xe6\x95\xb0\xe5\xae\x9e\xe7\x8e\xb0\n        elif self.activation == \'relu\':\n            grad = np.array(r, copy=True)\n            grad[r > 0] = 1.\n            grad[r <= 0] = 0.\n            return grad\n        # tanh \xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe5\xaf\xbc\xe6\x95\xb0\xe5\xae\x9e\xe7\x8e\xb0\n        elif self.activation == \'tanh\':\n            return 1 - r ** 2\n        # Sigmoid \xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe5\xaf\xbc\xe6\x95\xb0\xe5\xae\x9e\xe7\x8e\xb0\n        elif self.activation == \'sigmoid\':\n            return r * (1 - r)\n        return r\n\n\n# \xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9e\x8b\nclass NeuralNetwork:\n    def __init__(self):\n        self._layers = []  # \xe7\xbd\x91\xe7\xbb\x9c\xe5\xb1\x82\xe5\xaf\xb9\xe8\xb1\xa1\xe5\x88\x97\xe8\xa1\xa8\n\n    def add_layer(self, layer):\n        # \xe8\xbf\xbd\xe5\x8a\xa0\xe7\xbd\x91\xe7\xbb\x9c\xe5\xb1\x82\n        self._layers.append(layer)\n\n    # \xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe5\x8f\xaa\xe9\x9c\x80\xe8\xa6\x81\xe5\xbe\xaa\xe7\x8e\xaf\xe8\xb0\x83\xe5\x90\x84\xe4\xb8\xaa\xe7\xbd\x91\xe7\xbb\x9c\xe5\xb1\x82\xe5\xaf\xb9\xe8\xb1\xa1\xe7\x9a\x84\xe5\x89\x8d\xe5\x90\x91\xe8\xae\xa1\xe7\xae\x97\xe5\x87\xbd\xe6\x95\xb0\xe5\x8d\xb3\xe5\x8f\xaf\xef\xbc\x8c\xe4\xbb\xa3\xe7\xa0\x81\xe5\xa6\x82\xe4\xb8\x8b\xef\xbc\x9a\n    # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n    def feed_forward(self, X):\n        for layer in self._layers:\n            # \xe4\xbe\x9d\xe6\xac\xa1\xe9\x80\x9a\xe8\xbf\x87\xe5\x90\x84\xe4\xb8\xaa\xe7\xbd\x91\xe7\xbb\x9c\xe5\xb1\x82\n            X = layer.activate(X)\n        return X\n\n    def backpropagation(self, X, y, learning_rate):\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\xae\x97\xe6\xb3\x95\xe5\xae\x9e\xe7\x8e\xb0\n        # \xe5\x89\x8d\xe5\x90\x91\xe8\xae\xa1\xe7\xae\x97\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe8\xbe\x93\xe5\x87\xba\xe5\x80\xbc\n        output = self.feed_forward(X)\n        for i in reversed(range(len(self._layers))):  # \xe5\x8f\x8d\xe5\x90\x91\xe5\xbe\xaa\xe7\x8e\xaf\n            layer = self._layers[i]  # \xe5\xbe\x97\xe5\x88\xb0\xe5\xbd\x93\xe5\x89\x8d\xe5\xb1\x82\xe5\xaf\xb9\xe8\xb1\xa1\n            # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xaf\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\n            if layer == self._layers[-1]:  # \xe5\xaf\xb9\xe4\xba\x8e\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\n                layer.error = y - output  # \xe8\xae\xa1\xe7\xae\x972 \xe5\x88\x86\xe7\xb1\xbb\xe4\xbb\xbb\xe5\x8a\xa1\xe7\x9a\x84\xe5\x9d\x87\xe6\x96\xb9\xe5\xb7\xae\xe7\x9a\x84\xe5\xaf\xbc\xe6\x95\xb0\n                # \xe5\x85\xb3\xe9\x94\xae\xe6\xad\xa5\xe9\xaa\xa4\xef\xbc\x9a\xe8\xae\xa1\xe7\xae\x97\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84delta\xef\xbc\x8c\xe5\x8f\x82\xe8\x80\x83\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe5\x85\xac\xe5\xbc\x8f\n                layer.delta = layer.error * layer.apply_activation_derivative(output)\n            else:  # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xaf\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\n                next_layer = self._layers[i + 1]  # \xe5\xbe\x97\xe5\x88\xb0\xe4\xb8\x8b\xe4\xb8\x80\xe5\xb1\x82\xe5\xaf\xb9\xe8\xb1\xa1\n                layer.error = np.dot(next_layer.weights, next_layer.delta)\n                # \xe5\x85\xb3\xe9\x94\xae\xe6\xad\xa5\xe9\xaa\xa4\xef\xbc\x9a\xe8\xae\xa1\xe7\xae\x97\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\x9a\x84delta\xef\xbc\x8c\xe5\x8f\x82\xe8\x80\x83\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe5\x85\xac\xe5\xbc\x8f\n                layer.delta = layer.error * layer.apply_activation_derivative(layer.last_activation)\n\n        # \xe5\xbe\xaa\xe7\x8e\xaf\xe6\x9b\xb4\xe6\x96\xb0\xe6\x9d\x83\xe5\x80\xbc\n        for i in range(len(self._layers)):\n            layer = self._layers[i]\n            # o_i \xe4\xb8\xba\xe4\xb8\x8a\xe4\xb8\x80\xe7\xbd\x91\xe7\xbb\x9c\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\n            o_i = np.atleast_2d(X if i == 0 else self._layers[i - 1].last_activation)\n            # \xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe7\xae\x97\xe6\xb3\x95\xef\xbc\x8cdelta \xe6\x98\xaf\xe5\x85\xac\xe5\xbc\x8f\xe4\xb8\xad\xe7\x9a\x84\xe8\xb4\x9f\xe6\x95\xb0\xef\xbc\x8c\xe6\x95\x85\xe8\xbf\x99\xe9\x87\x8c\xe7\x94\xa8\xe5\x8a\xa0\xe5\x8f\xb7\n            layer.weights += layer.delta * o_i.T * learning_rate\n\n    def train(self, X_train, X_test, y_train, y_test, learning_rate, max_epochs):\n        # \xe7\xbd\x91\xe7\xbb\x9c\xe8\xae\xad\xe7\xbb\x83\xe5\x87\xbd\xe6\x95\xb0\n        # one-hot \xe7\xbc\x96\xe7\xa0\x81\n        y_onehot = np.zeros((y_train.shape[0], 2))\n        y_onehot[np.arange(y_train.shape[0]), y_train] = 1\n\n        # \xe5\xb0\x86One-hot \xe7\xbc\x96\xe7\xa0\x81\xe5\x90\x8e\xe7\x9a\x84\xe7\x9c\x9f\xe5\xae\x9e\xe6\xa0\x87\xe7\xad\xbe\xe4\xb8\x8e\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe8\xae\xa1\xe7\xae\x97\xe5\x9d\x87\xe6\x96\xb9\xe8\xaf\xaf\xe5\xb7\xae\xef\xbc\x8c\xe5\xb9\xb6\xe8\xb0\x83\xe7\x94\xa8\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe5\x87\xbd\xe6\x95\xb0\xe6\x9b\xb4\xe6\x96\xb0\xe7\xbd\x91\xe7\xbb\x9c\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe5\xbe\xaa\xe7\x8e\xaf\xe8\xbf\xad\xe4\xbb\xa3\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x861000 \xe9\x81\x8d\xe5\x8d\xb3\xe5\x8f\xaf\n        mses = []\n        accuracys = []\n        for i in range(max_epochs + 1):  # \xe8\xae\xad\xe7\xbb\x831000 \xe4\xb8\xaaepoch\n            for j in range(len(X_train)):  # \xe4\xb8\x80\xe6\xac\xa1\xe8\xae\xad\xe7\xbb\x83\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\n                self.backpropagation(X_train[j], y_onehot[j], learning_rate)\n            if i % 10 == 0:\n                # \xe6\x89\x93\xe5\x8d\xb0\xe5\x87\xbaMSE Loss\n                mse = np.mean(np.square(y_onehot - self.feed_forward(X_train)))\n                mses.append(mse)\n                accuracy = self.accuracy(self.predict(X_test), y_test.flatten())\n                accuracys.append(accuracy)\n                print(\'Epoch: #%s, MSE: %f\' % (i, float(mse)))\n                # \xe7\xbb\x9f\xe8\xae\xa1\xe5\xb9\xb6\xe6\x89\x93\xe5\x8d\xb0\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\n                print(\'Accuracy: %.2f%%\' % (accuracy * 100))\n        return mses, accuracys\n\n    def predict(self, X):\n        return self.feed_forward(X)\n\n    def accuracy(self, X, y):\n        return np.sum(np.equal(np.argmax(X, axis=1), y)) / y.shape[0]\n\n\ndef main():\n    X, y, X_train, X_test, y_train, y_test = load_dataset()\n    # \xe8\xb0\x83\xe7\x94\xa8 make_plot \xe5\x87\xbd\xe6\x95\xb0\xe7\xbb\x98\xe5\x88\xb6\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe5\x88\x86\xe5\xb8\x83\xef\xbc\x8c\xe5\x85\xb6\xe4\xb8\xad X \xe4\xb8\xba 2D \xe5\x9d\x90\xe6\xa0\x87\xef\xbc\x8c y \xe4\xb8\xba\xe6\xa0\x87\xe7\xad\xbe\n    make_plot(X, y, ""Classification Dataset Visualization "")\n    plt.show()\n    nn = NeuralNetwork()  # \xe5\xae\x9e\xe4\xbe\x8b\xe5\x8c\x96\xe7\xbd\x91\xe7\xbb\x9c\xe7\xb1\xbb\n    nn.add_layer(Layer(2, 25, \'sigmoid\'))  # \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82 1, 2=>25\n    nn.add_layer(Layer(25, 50, \'sigmoid\'))  # \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82 2, 25=>50\n    nn.add_layer(Layer(50, 25, \'sigmoid\'))  # \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82 3, 50=>25\n    nn.add_layer(Layer(25, 2, \'sigmoid\'))  # \xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82, 25=>2\n    mses, accuracys = nn.train(X_train, X_test, y_train, y_test, 0.01, 1000)\n\n    x = [i for i in range(0, 101, 10)]\n\n    # \xe7\xbb\x98\xe5\x88\xb6MES\xe6\x9b\xb2\xe7\xba\xbf\n    plt.title(""MES Loss"")\n    plt.plot(x, mses[:11], color=\'blue\')\n    plt.xlabel(\'Epoch\')\n    plt.ylabel(\'MSE\')\n    plt.savefig(\'\xe8\xae\xad\xe7\xbb\x83\xe8\xaf\xaf\xe5\xb7\xae\xe6\x9b\xb2\xe7\xba\xbf.svg\')\n    plt.close()\n\n    # \xe7\xbb\x98\xe5\x88\xb6Accuracy\xe6\x9b\xb2\xe7\xba\xbf\n    plt.title(""Accuracy"")\n    plt.plot(x, accuracys[:11], color=\'blue\')\n    plt.xlabel(\'Epoch\')\n    plt.ylabel(\'Accuracy\')\n    plt.savefig(\'\xe7\xbd\x91\xe7\xbb\x9c\xe6\xb5\x8b\xe8\xaf\x95\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87.svg\')\n    plt.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
ch07-反向传播算法/sigmoid_grad.py,3,"b""import tensorflow as tf \n\n\na = tf.linspace(-10., 10., 10)\n\nwith tf.GradientTape() as tape:\n\ttape.watch(a)\n\ty = tf.sigmoid(a)\n\n\ngrads = tape.gradient(y, [a])\nprint('x:', a.numpy())\nprint('y:', y.numpy())\nprint('grad:', grads[0].numpy())\n"""
ch07-反向传播算法/single_output_perceptron.py,7,"b""import tensorflow as tf \n\n\n\n\nx=tf.random.normal([1,3])\n\nw=tf.ones([3,1])\n\nb=tf.ones([1])\n\ny = tf.constant([1])\n\n\nwith tf.GradientTape() as tape:\n\n\ttape.watch([w, b])\n\tlogits = tf.sigmoid(x@w+b) \n\tloss = tf.reduce_mean(tf.losses.MSE(y, logits))\n\ngrads = tape.gradient(loss, [w, b])\nprint('w grad:', grads[0])\n\nprint('b grad:', grads[1])\n\n\n"""
ch08-Keras高层接口/compile_fit.py,9,"b'import  tensorflow as tf\nfrom    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n\n\ndef preprocess(x, y):\n    """"""\n    x is a simple image, not a batch\n    """"""\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    x = tf.reshape(x, [28*28])\n    y = tf.cast(y, dtype=tf.int32)\n    y = tf.one_hot(y, depth=10)\n    return x,y\n\n\nbatchsz = 128\n(x, y), (x_val, y_val) = datasets.mnist.load_data()\nprint(\'datasets:\', x.shape, y.shape, x.min(), x.max())\n\n\n\ndb = tf.data.Dataset.from_tensor_slices((x,y))\ndb = db.map(preprocess).shuffle(60000).batch(batchsz)\nds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\nds_val = ds_val.map(preprocess).batch(batchsz) \n\nsample = next(iter(db))\nprint(sample[0].shape, sample[1].shape)\n\n\nnetwork = Sequential([layers.Dense(256, activation=\'relu\'),\n                     layers.Dense(128, activation=\'relu\'),\n                     layers.Dense(64, activation=\'relu\'),\n                     layers.Dense(32, activation=\'relu\'),\n                     layers.Dense(10)])\nnetwork.build(input_shape=(None, 28*28))\nnetwork.summary()\n\n\n\n\nnetwork.compile(optimizer=optimizers.Adam(lr=0.01),\n\t\tloss=tf.losses.CategoricalCrossentropy(from_logits=True),\n\t\tmetrics=[\'accuracy\']\n\t)\n\nnetwork.fit(db, epochs=5, validation_data=ds_val, validation_freq=2)\n \nnetwork.evaluate(ds_val)\n\nsample = next(iter(ds_val))\nx = sample[0]\ny = sample[1] # one-hot\npred = network.predict(x) # [b, 10]\n# convert back to number \ny = tf.argmax(y, axis=1)\npred = tf.argmax(pred, axis=1)\n\nprint(pred)\nprint(y)\n'"
ch08-Keras高层接口/keras_train.py,15,"b'import  os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'2\'\n\nimport  tensorflow as tf\nfrom    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\nfrom \ttensorflow import keras\n\n\n\ndef preprocess(x, y):\n    # [0~255] => [-1~1]\n    x = 2 * tf.cast(x, dtype=tf.float32) / 255. - 1.\n    y = tf.cast(y, dtype=tf.int32)\n    return x,y\n\n\nbatchsz = 128\n# [50k, 32, 32, 3], [10k, 1]\n(x, y), (x_val, y_val) = datasets.cifar10.load_data()\ny = tf.squeeze(y)\ny_val = tf.squeeze(y_val)\ny = tf.one_hot(y, depth=10) # [50k, 10]\ny_val = tf.one_hot(y_val, depth=10) # [10k, 10]\nprint(\'datasets:\', x.shape, y.shape, x_val.shape, y_val.shape, x.min(), x.max())\n\n\ntrain_db = tf.data.Dataset.from_tensor_slices((x,y))\ntrain_db = train_db.map(preprocess).shuffle(10000).batch(batchsz)\ntest_db = tf.data.Dataset.from_tensor_slices((x_val, y_val))\ntest_db = test_db.map(preprocess).batch(batchsz)\n\n\nsample = next(iter(train_db))\nprint(\'batch:\', sample[0].shape, sample[1].shape)\n\n\nclass MyDense(layers.Layer):\n    # to replace standard layers.Dense()\n    def __init__(self, inp_dim, outp_dim):\n        super(MyDense, self).__init__()\n\n        self.kernel = self.add_variable(\'w\', [inp_dim, outp_dim])\n        # self.bias = self.add_variable(\'b\', [outp_dim])\n\n    def call(self, inputs, training=None):\n\n        x = inputs @ self.kernel\n        return x\n\nclass MyNetwork(keras.Model):\n\n    def __init__(self):\n        super(MyNetwork, self).__init__()\n\n        self.fc1 = MyDense(32*32*3, 256)\n        self.fc2 = MyDense(256, 128)\n        self.fc3 = MyDense(128, 64)\n        self.fc4 = MyDense(64, 32)\n        self.fc5 = MyDense(32, 10)\n\n\n\n    def call(self, inputs, training=None):\n        """"""\n\n        :param inputs: [b, 32, 32, 3]\n        :param training:\n        :return:\n        """"""\n        x = tf.reshape(inputs, [-1, 32*32*3])\n        # [b, 32*32*3] => [b, 256]\n        x = self.fc1(x)\n        x = tf.nn.relu(x)\n        # [b, 256] => [b, 128]\n        x = self.fc2(x)\n        x = tf.nn.relu(x)\n        # [b, 128] => [b, 64]\n        x = self.fc3(x)\n        x = tf.nn.relu(x)\n        # [b, 64] => [b, 32]\n        x = self.fc4(x)\n        x = tf.nn.relu(x)\n        # [b, 32] => [b, 10]\n        x = self.fc5(x)\n\n        return x\n\n\nnetwork = MyNetwork()\nnetwork.compile(optimizer=optimizers.Adam(lr=1e-3),\n                loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n                metrics=[\'accuracy\'])\nnetwork.fit(train_db, epochs=15, validation_data=test_db, validation_freq=1)\n\nnetwork.evaluate(test_db)\nnetwork.save_weights(\'ckpt/weights.ckpt\')\ndel network\nprint(\'saved to ckpt/weights.ckpt\')\n\n\nnetwork = MyNetwork()\nnetwork.compile(optimizer=optimizers.Adam(lr=1e-3),\n                loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n                metrics=[\'accuracy\'])\nnetwork.load_weights(\'ckpt/weights.ckpt\')\nprint(\'loaded weights from file.\')\nnetwork.evaluate(test_db)'"
ch08-Keras高层接口/layer_model.py,13,"b'import  tensorflow as tf\nfrom    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\nfrom \ttensorflow import keras\n\ndef preprocess(x, y):\n    """"""\n    x is a simple image, not a batch\n    """"""\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    x = tf.reshape(x, [28*28])\n    y = tf.cast(y, dtype=tf.int32)\n    y = tf.one_hot(y, depth=10)\n    return x,y\n\n\nbatchsz = 128\n(x, y), (x_val, y_val) = datasets.mnist.load_data()\nprint(\'datasets:\', x.shape, y.shape, x.min(), x.max())\n\n\n\ndb = tf.data.Dataset.from_tensor_slices((x,y))\ndb = db.map(preprocess).shuffle(60000).batch(batchsz)\nds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\nds_val = ds_val.map(preprocess).batch(batchsz) \n\nsample = next(iter(db))\nprint(sample[0].shape, sample[1].shape)\n\n\nnetwork = Sequential([layers.Dense(256, activation=\'relu\'),\n                     layers.Dense(128, activation=\'relu\'),\n                     layers.Dense(64, activation=\'relu\'),\n                     layers.Dense(32, activation=\'relu\'),\n                     layers.Dense(10)])\nnetwork.build(input_shape=(None, 28*28))\nnetwork.summary()\n\n\nclass MyDense(layers.Layer):\n\n\tdef __init__(self, inp_dim, outp_dim):\n\t\tsuper(MyDense, self).__init__()\n\n\t\tself.kernel = self.add_weight(\'w\', [inp_dim, outp_dim])\n\t\tself.bias = self.add_weight(\'b\', [outp_dim])\n\n\tdef call(self, inputs, training=None):\n\n\t\tout = inputs @ self.kernel + self.bias\n\n\t\treturn out \n\nclass MyModel(keras.Model):\n\n\tdef __init__(self):\n\t\tsuper(MyModel, self).__init__()\n\n\t\tself.fc1 = MyDense(28*28, 256)\n\t\tself.fc2 = MyDense(256, 128)\n\t\tself.fc3 = MyDense(128, 64)\n\t\tself.fc4 = MyDense(64, 32)\n\t\tself.fc5 = MyDense(32, 10)\n\n\tdef call(self, inputs, training=None):\n\n\t\tx = self.fc1(inputs)\n\t\tx = tf.nn.relu(x)\n\t\tx = self.fc2(x)\n\t\tx = tf.nn.relu(x)\n\t\tx = self.fc3(x)\n\t\tx = tf.nn.relu(x)\n\t\tx = self.fc4(x)\n\t\tx = tf.nn.relu(x)\n\t\tx = self.fc5(x) \n\n\t\treturn x\n\n\nnetwork = MyModel()\n\n\nnetwork.compile(optimizer=optimizers.Adam(lr=0.01),\n\t\tloss=tf.losses.CategoricalCrossentropy(from_logits=True),\n\t\tmetrics=[\'accuracy\']\n\t)\n\nnetwork.fit(db, epochs=5, validation_data=ds_val,\n              validation_freq=2)\n \nnetwork.evaluate(ds_val)\n\nsample = next(iter(ds_val))\nx = sample[0]\ny = sample[1] # one-hot\npred = network.predict(x) # [b, 10]\n# convert back to number \ny = tf.argmax(y, axis=1)\npred = tf.argmax(pred, axis=1)\n\nprint(pred)\nprint(y)\n'"
ch08-Keras高层接口/metrics.py,13,"b""import  tensorflow as tf\nfrom    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n\n\ndef preprocess(x, y):\n\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    y = tf.cast(y, dtype=tf.int32)\n\n    return x,y\n\n\nbatchsz = 128\n(x, y), (x_val, y_val) = datasets.mnist.load_data()\nprint('datasets:', x.shape, y.shape, x.min(), x.max())\n\n\n\ndb = tf.data.Dataset.from_tensor_slices((x,y))\ndb = db.map(preprocess).shuffle(60000).batch(batchsz).repeat(10)\n\nds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\nds_val = ds_val.map(preprocess).batch(batchsz) \n\n\n\n\nnetwork = Sequential([layers.Dense(256, activation='relu'),\n                     layers.Dense(128, activation='relu'),\n                     layers.Dense(64, activation='relu'),\n                     layers.Dense(32, activation='relu'),\n                     layers.Dense(10)])\nnetwork.build(input_shape=(None, 28*28))\nnetwork.summary()\n\noptimizer = optimizers.Adam(lr=0.01)\n\nacc_meter = metrics.Accuracy()\nloss_meter = metrics.Mean()\n\n\nfor step, (x,y) in enumerate(db):\n\n    with tf.GradientTape() as tape:\n        # [b, 28, 28] => [b, 784]\n        x = tf.reshape(x, (-1, 28*28))\n        # [b, 784] => [b, 10]\n        out = network(x)\n        # [b] => [b, 10]\n        y_onehot = tf.one_hot(y, depth=10) \n        # [b]\n        loss = tf.reduce_mean(tf.losses.categorical_crossentropy(y_onehot, out, from_logits=True))\n\n        loss_meter.update_state(loss)\n\n \n\n    grads = tape.gradient(loss, network.trainable_variables)\n    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n\n\n    if step % 100 == 0:\n\n        print(step, 'loss:', loss_meter.result().numpy()) \n        loss_meter.reset_states()\n\n\n    # evaluate\n    if step % 500 == 0:\n        total, total_correct = 0., 0\n        acc_meter.reset_states()\n\n        for step, (x, y) in enumerate(ds_val): \n            # [b, 28, 28] => [b, 784]\n            x = tf.reshape(x, (-1, 28*28))\n            # [b, 784] => [b, 10]\n            out = network(x) \n\n\n            # [b, 10] => [b] \n            pred = tf.argmax(out, axis=1) \n            pred = tf.cast(pred, dtype=tf.int32)\n            # bool type \n            correct = tf.equal(pred, y)\n            # bool tensor => int tensor => numpy\n            total_correct += tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()\n            total += x.shape[0]\n\n            acc_meter.update_state(y, pred)\n\n\n        print(step, 'Evaluate Acc:', total_correct/total, acc_meter.result().numpy())\n"""
ch08-Keras高层接口/nb.py,12,"b""#%%\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers,Sequential,losses,optimizers,datasets\n\n\n#%%\nx = tf.constant([2.,1.,0.1])\nlayer = layers.Softmax(axis=-1)\nlayer(x)\n#%%\ndef proprocess(x,y):\n    x = tf.reshape(x, [-1]) \n    return x,y\n\n# x: [60k, 28, 28],\n# y: [60k]\n(x, y), (x_test,y_test) = datasets.mnist.load_data()\n# x: [0~255] => [0~1.]\nx = tf.convert_to_tensor(x, dtype=tf.float32) / 255.\ny = tf.convert_to_tensor(y, dtype=tf.int32) \n\n# x: [0~255] => [0~1.]\nx_test = tf.convert_to_tensor(x_test, dtype=tf.float32) / 255.\ny_test = tf.convert_to_tensor(y_test, dtype=tf.int32) \n\ntrain_db = tf.data.Dataset.from_tensor_slices((x,y))\ntrain_db = train_db.shuffle(1000).map(proprocess).batch(128)\n\nval_db = tf.data.Dataset.from_tensor_slices((x_test,y_test))\nval_db = val_db.shuffle(1000).map(proprocess).batch(128)\n\nx,y = next(iter(train_db))\nprint(x.shape, y.shape)\n#%%\n\nfrom tensorflow.keras import layers, Sequential\nnetwork = Sequential([\n    layers.Dense(3, activation=None),\n    layers.ReLU(),\n    layers.Dense(2, activation=None),\n    layers.ReLU()\n])\nx = tf.random.normal([4,3])\nnetwork(x)\n\n#%%\nlayers_num = 2\nnetwork = Sequential([])\nfor _ in range(layers_num):\n    network.add(layers.Dense(3))\n    network.add(layers.ReLU())\nnetwork.build(input_shape=(None, 4))\nnetwork.summary()\n\n#%%\nfor p in network.trainable_variables:\n    print(p.name, p.shape)\n\n#%%\n# \xe5\x88\x9b\xe5\xbb\xba5\xe5\xb1\x82\xe7\x9a\x84\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe7\xbd\x91\xe7\xbb\x9c\nnetwork = Sequential([layers.Dense(256, activation='relu'),\n                     layers.Dense(128, activation='relu'),\n                     layers.Dense(64, activation='relu'),\n                     layers.Dense(32, activation='relu'),\n                     layers.Dense(10)])\nnetwork.build(input_shape=(4, 28*28))\nnetwork.summary()\n\n\n#%%\n# \xe5\xaf\xbc\xe5\x85\xa5\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xef\xbc\x8c\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe6\xa8\xa1\xe5\x9d\x97\nfrom tensorflow.keras import optimizers,losses \n# \xe9\x87\x87\xe7\x94\xa8Adam\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xef\xbc\x8c\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe4\xb8\xba0.01;\xe9\x87\x87\xe7\x94\xa8\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe5\x8c\x85\xe5\x90\xabSoftmax\nnetwork.compile(optimizer=optimizers.Adam(lr=0.01),\n        loss=losses.CategoricalCrossentropy(from_logits=True),\n        metrics=['accuracy'] # \xe8\xae\xbe\xe7\xbd\xae\xe6\xb5\x8b\xe9\x87\x8f\xe6\x8c\x87\xe6\xa0\x87\xe4\xb8\xba\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\n)\n\n\n#%%\n# \xe6\x8c\x87\xe5\xae\x9a\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe4\xb8\xbadb\xef\xbc\x8c\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\xbaval_db,\xe8\xae\xad\xe7\xbb\x835\xe4\xb8\xaaepochs\xef\xbc\x8c\xe6\xaf\x8f2\xe4\xb8\xaaepoch\xe9\xaa\x8c\xe8\xaf\x81\xe4\xb8\x80\xe6\xac\xa1\nhistory = network.fit(train_db, epochs=5, validation_data=val_db, validation_freq=2)\n\n\n#%%\nhistory.history # \xe6\x89\x93\xe5\x8d\xb0\xe8\xae\xad\xe7\xbb\x83\xe8\xae\xb0\xe5\xbd\x95\n\n#%%\n# \xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\xe5\x88\xb0\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\x8a\nnetwork.save_weights('weights.ckpt')\nprint('saved weights.')\ndel network # \xe5\x88\xa0\xe9\x99\xa4\xe7\xbd\x91\xe7\xbb\x9c\xe5\xaf\xb9\xe8\xb1\xa1\n# \xe9\x87\x8d\xe6\x96\xb0\xe5\x88\x9b\xe5\xbb\xba\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\nnetwork = Sequential([layers.Dense(256, activation='relu'),\n                     layers.Dense(128, activation='relu'),\n                     layers.Dense(64, activation='relu'),\n                     layers.Dense(32, activation='relu'),\n                     layers.Dense(10)])\nnetwork.compile(optimizer=optimizers.Adam(lr=0.01),\n        loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n        metrics=['accuracy']\n    ) \n# \xe4\xbb\x8e\xe5\x8f\x82\xe6\x95\xb0\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\xe8\xaf\xbb\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\xe5\xb9\xb6\xe5\x86\x99\xe5\x85\xa5\xe5\xbd\x93\xe5\x89\x8d\xe7\xbd\x91\xe7\xbb\x9c\nnetwork.load_weights('weights.ckpt')\nprint('loaded weights!')\n\n\n#%%\n# \xe6\x96\xb0\xe5\xbb\xba\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\nglobal_average_layer = layers.GlobalAveragePooling2D()\n# \xe5\x88\xa9\xe7\x94\xa8\xe4\xb8\x8a\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe4\xbd\x9c\xe4\xb8\xba\xe6\x9c\xac\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8c\xe6\xb5\x8b\xe8\xaf\x95\xe5\x85\xb6\xe8\xbe\x93\xe5\x87\xba\nx = tf.random.normal([4,7,7,2048])\nout = global_average_layer(x) # \xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\xe9\x99\x8d\xe7\xbb\xb4\nprint(out.shape)\n\n\n#%%\n# \xe6\x96\xb0\xe5\xbb\xba\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\nfc = layers.Dense(100)\n# \xe5\x88\xa9\xe7\x94\xa8\xe4\xb8\x8a\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe4\xbd\x9c\xe4\xb8\xba\xe6\x9c\xac\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8c\xe6\xb5\x8b\xe8\xaf\x95\xe5\x85\xb6\xe8\xbe\x93\xe5\x87\xba\nx = tf.random.normal([4,2048])\nout = fc(x)\nprint(out.shape)\n\n\n#%%\n"""
ch08-Keras高层接口/pretained.py,5,"b""#%%\nimport  tensorflow as tf\nfrom    tensorflow import keras\nfrom    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n\n#%%\n# \xe5\x8a\xa0\xe8\xbd\xbd\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\xb9\xb6\xe5\x8e\xbb\xe6\x8e\x89\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\nresnet = keras.applications.ResNet50(weights='imagenet',include_top=False)\nresnet.summary()\n# \xe6\xb5\x8b\xe8\xaf\x95\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\nx = tf.random.normal([4,224,224,3])\nout = resnet(x)\nout.shape\n#%%\n# \xe6\x96\xb0\xe5\xbb\xba\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\nglobal_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n# \xe5\x88\xa9\xe7\x94\xa8\xe4\xb8\x8a\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe4\xbd\x9c\xe4\xb8\xba\xe6\x9c\xac\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8c\xe6\xb5\x8b\xe8\xaf\x95\xe5\x85\xb6\xe8\xbe\x93\xe5\x87\xba\nx = tf.random.normal([4,7,7,2048])\nout = global_average_layer(x)\nprint(out.shape)\n#%%\n# \xe6\x96\xb0\xe5\xbb\xba\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\nfc = tf.keras.layers.Dense(100)\n# \xe5\x88\xa9\xe7\x94\xa8\xe4\xb8\x8a\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe4\xbd\x9c\xe4\xb8\xba\xe6\x9c\xac\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8c\xe6\xb5\x8b\xe8\xaf\x95\xe5\x85\xb6\xe8\xbe\x93\xe5\x87\xba\nx = tf.random.normal([4,2048])\nout = fc(x)\nprint(out.shape)\n#%%\n# \xe9\x87\x8d\xe6\x96\xb0\xe5\x8c\x85\xe8\xa3\xb9\xe6\x88\x90\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9e\x8b\nmynet = Sequential([resnet, global_average_layer, fc])\nmynet.summary()\n#%%\nresnet.trainable = False\nmynet.summary()\n\n#%%"""
ch08-Keras高层接口/save_load_model.py,14,"b'import  os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'2\'\n\nimport  tensorflow as tf\nfrom    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n\n\ndef preprocess(x, y):\n    """"""\n    x is a simple image, not a batch\n    """"""\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    x = tf.reshape(x, [28*28])\n    y = tf.cast(y, dtype=tf.int32)\n    y = tf.one_hot(y, depth=10)\n    return x,y\n\n\nbatchsz = 128\n(x, y), (x_val, y_val) = datasets.mnist.load_data()\nprint(\'datasets:\', x.shape, y.shape, x.min(), x.max())\n\n\n\ndb = tf.data.Dataset.from_tensor_slices((x,y))\ndb = db.map(preprocess).shuffle(60000).batch(batchsz)\nds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\nds_val = ds_val.map(preprocess).batch(batchsz) \n\nsample = next(iter(db))\nprint(sample[0].shape, sample[1].shape)\n\n\nnetwork = Sequential([layers.Dense(256, activation=\'relu\'),\n                     layers.Dense(128, activation=\'relu\'),\n                     layers.Dense(64, activation=\'relu\'),\n                     layers.Dense(32, activation=\'relu\'),\n                     layers.Dense(10)])\nnetwork.build(input_shape=(None, 28*28))\nnetwork.summary()\n\n\n\n\nnetwork.compile(optimizer=optimizers.Adam(lr=0.01),\n\t\tloss=tf.losses.CategoricalCrossentropy(from_logits=True),\n\t\tmetrics=[\'accuracy\']\n\t)\n\nnetwork.fit(db, epochs=3, validation_data=ds_val, validation_freq=2)\n \nnetwork.evaluate(ds_val)\n\nnetwork.save(\'model.h5\')\nprint(\'saved total model.\')\ndel network\n\nprint(\'loaded model from file.\')\nnetwork = tf.keras.models.load_model(\'model.h5\', compile=False)\nnetwork.compile(optimizer=optimizers.Adam(lr=0.01),\n        loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n        metrics=[\'accuracy\']\n    )\nx_val = tf.cast(x_val, dtype=tf.float32) / 255.\nx_val = tf.reshape(x_val, [-1, 28*28])\ny_val = tf.cast(y_val, dtype=tf.int32)\ny_val = tf.one_hot(y_val, depth=10)\nds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(128)\nnetwork.evaluate(ds_val)\n'"
ch08-Keras高层接口/save_load_weight.py,8,"b'import  os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'2\'\n\nimport  tensorflow as tf\nfrom    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n\n\ndef preprocess(x, y):\n    """"""\n    x is a simple image, not a batch\n    """"""\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    x = tf.reshape(x, [28*28])\n    y = tf.cast(y, dtype=tf.int32)\n    y = tf.one_hot(y, depth=10)\n    return x,y\n\n\nbatchsz = 128\n(x, y), (x_val, y_val) = datasets.mnist.load_data()\nprint(\'datasets:\', x.shape, y.shape, x.min(), x.max())\n\n\n\ndb = tf.data.Dataset.from_tensor_slices((x,y))\ndb = db.map(preprocess).shuffle(60000).batch(batchsz)\nds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\nds_val = ds_val.map(preprocess).batch(batchsz) \n\nsample = next(iter(db))\nprint(sample[0].shape, sample[1].shape)\n\n\nnetwork = Sequential([layers.Dense(256, activation=\'relu\'),\n                     layers.Dense(128, activation=\'relu\'),\n                     layers.Dense(64, activation=\'relu\'),\n                     layers.Dense(32, activation=\'relu\'),\n                     layers.Dense(10)])\nnetwork.build(input_shape=(None, 28*28))\nnetwork.summary()\n\n\n\n\nnetwork.compile(optimizer=optimizers.Adam(lr=0.01),\n\t\tloss=tf.losses.CategoricalCrossentropy(from_logits=True),\n\t\tmetrics=[\'accuracy\']\n\t)\n\nnetwork.fit(db, epochs=3, validation_data=ds_val, validation_freq=2)\n \nnetwork.evaluate(ds_val)\n\nnetwork.save_weights(\'weights.ckpt\')\nprint(\'saved weights.\')\ndel network\n\nnetwork = Sequential([layers.Dense(256, activation=\'relu\'),\n                     layers.Dense(128, activation=\'relu\'),\n                     layers.Dense(64, activation=\'relu\'),\n                     layers.Dense(32, activation=\'relu\'),\n                     layers.Dense(10)])\nnetwork.compile(optimizer=optimizers.Adam(lr=0.01),\n\t\tloss=tf.losses.CategoricalCrossentropy(from_logits=True),\n\t\tmetrics=[\'accuracy\']\n\t)\nnetwork.load_weights(\'weights.ckpt\')\nprint(\'loaded weights!\')\nnetwork.evaluate(ds_val)\n'"
ch09-过拟合/9.8-over-fitting-and-under-fitting.py,0,"b'\nimport matplotlib.pyplot as plt\n# \xe5\xaf\xbc\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\x94\x9f\xe6\x88\x90\xe5\xb7\xa5\xe5\x85\xb7\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import layers, Sequential, regularizers\nfrom mpl_toolkits.mplot3d import Axes3D\n\nplt.rcParams[\'font.size\'] = 16\nplt.rcParams[\'font.family\'] = [\'STKaiti\']\nplt.rcParams[\'axes.unicode_minus\'] = False\n\nOUTPUT_DIR = \'output_dir\'\nN_EPOCHS = 500\n\n\ndef load_dataset():\n    # \xe9\x87\x87\xe6\xa0\xb7\xe7\x82\xb9\xe6\x95\xb0\n    N_SAMPLES = 1000\n    # \xe6\xb5\x8b\xe8\xaf\x95\xe6\x95\xb0\xe9\x87\x8f\xe6\xaf\x94\xe7\x8e\x87\n    TEST_SIZE = None\n\n    # \xe4\xbb\x8e moon \xe5\x88\x86\xe5\xb8\x83\xe4\xb8\xad\xe9\x9a\x8f\xe6\x9c\xba\xe9\x87\x87\xe6\xa0\xb7 1000 \xe4\xb8\xaa\xe7\x82\xb9\xef\xbc\x8c\xe5\xb9\xb6\xe5\x88\x87\xe5\x88\x86\xe4\xb8\xba\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86-\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\n    X, y = make_moons(n_samples=N_SAMPLES, noise=0.25, random_state=100)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)\n    return X, y, X_train, X_test, y_train, y_test\n\n\ndef make_plot(X, y, plot_name, file_name, XX=None, YY=None, preds=None, dark=False, output_dir=OUTPUT_DIR):\n    # \xe7\xbb\x98\xe5\x88\xb6\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\x9a\x84\xe5\x88\x86\xe5\xb8\x83\xef\xbc\x8c X \xe4\xb8\xba 2D \xe5\x9d\x90\xe6\xa0\x87\xef\xbc\x8c y \xe4\xb8\xba\xe6\x95\xb0\xe6\x8d\xae\xe7\x82\xb9\xe7\x9a\x84\xe6\xa0\x87\xe7\xad\xbe\n    if dark:\n        plt.style.use(\'dark_background\')\n    else:\n        sns.set_style(""whitegrid"")\n    axes = plt.gca()\n    axes.set_xlim([-2, 3])\n    axes.set_ylim([-1.5, 2])\n    axes.set(xlabel=""$x_1$"", ylabel=""$x_2$"")\n    plt.title(plot_name, fontsize=20, fontproperties=\'SimHei\')\n    plt.subplots_adjust(left=0.20)\n    plt.subplots_adjust(right=0.80)\n    if XX is not None and YY is not None and preds is not None:\n        plt.contourf(XX, YY, preds.reshape(XX.shape), 25, alpha=0.08, cmap=plt.cm.Spectral)\n        plt.contour(XX, YY, preds.reshape(XX.shape), levels=[.5], cmap=""Greys"", vmin=0, vmax=.6)\n    # \xe7\xbb\x98\xe5\x88\xb6\xe6\x95\xa3\xe7\x82\xb9\xe5\x9b\xbe\xef\xbc\x8c\xe6\xa0\xb9\xe6\x8d\xae\xe6\xa0\x87\xe7\xad\xbe\xe5\x8c\xba\xe5\x88\x86\xe9\xa2\x9c\xe8\x89\xb2m=markers\n    markers = [\'o\' if i == 1 else \'s\' for i in y.ravel()]\n    mscatter(X[:, 0], X[:, 1], c=y.ravel(), s=20, cmap=plt.cm.Spectral, edgecolors=\'none\', m=markers, ax=axes)\n    # \xe4\xbf\x9d\xe5\xad\x98\xe7\x9f\xa2\xe9\x87\x8f\xe5\x9b\xbe\n    plt.savefig(output_dir + \'/\' + file_name)\n    plt.close()\n\n\ndef mscatter(x, y, ax=None, m=None, **kw):\n    import matplotlib.markers as mmarkers\n    if not ax: ax = plt.gca()\n    sc = ax.scatter(x, y, **kw)\n    if (m is not None) and (len(m) == len(x)):\n        paths = []\n        for marker in m:\n            if isinstance(marker, mmarkers.MarkerStyle):\n                marker_obj = marker\n            else:\n                marker_obj = mmarkers.MarkerStyle(marker)\n            path = marker_obj.get_path().transformed(\n                marker_obj.get_transform())\n            paths.append(path)\n        sc.set_paths(paths)\n    return sc\n\n\ndef network_layers_influence(X_train, y_train):\n    # \xe6\x9e\x84\xe5\xbb\xba 5 \xe7\xa7\x8d\xe4\xb8\x8d\xe5\x90\x8c\xe5\xb1\x82\xe6\x95\xb0\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\n    for n in range(5):\n        # \xe5\x88\x9b\xe5\xbb\xba\xe5\xae\xb9\xe5\x99\xa8\n        model = Sequential()\n        # \xe5\x88\x9b\xe5\xbb\xba\xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\n        model.add(layers.Dense(8, input_dim=2, activation=\'relu\'))\n        # \xe6\xb7\xbb\xe5\x8a\xa0 n \xe5\xb1\x82\xef\xbc\x8c\xe5\x85\xb1 n+2 \xe5\xb1\x82\n        for _ in range(n):\n            model.add(layers.Dense(32, activation=\'relu\'))\n        # \xe5\x88\x9b\xe5\xbb\xba\xe6\x9c\x80\xe6\x9c\xab\xe5\xb1\x82\n        model.add(layers.Dense(1, activation=\'sigmoid\'))\n        # \xe6\xa8\xa1\xe5\x9e\x8b\xe8\xa3\x85\xe9\x85\x8d\xe4\xb8\x8e\xe8\xae\xad\xe7\xbb\x83\n        model.compile(loss=\'binary_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\'])\n        model.fit(X_train, y_train, epochs=N_EPOCHS, verbose=1)\n        # \xe7\xbb\x98\xe5\x88\xb6\xe4\xb8\x8d\xe5\x90\x8c\xe5\xb1\x82\xe6\x95\xb0\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe5\x86\xb3\xe7\xad\x96\xe8\xbe\xb9\xe7\x95\x8c\xe6\x9b\xb2\xe7\xba\xbf\n        # \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe7\x9a\x84 x \xe5\x9d\x90\xe6\xa0\x87\xe8\x8c\x83\xe5\x9b\xb4\xe4\xb8\xba[-2, 3]\n        xx = np.arange(-2, 3, 0.01)\n        # \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe7\x9a\x84 y \xe5\x9d\x90\xe6\xa0\x87\xe8\x8c\x83\xe5\x9b\xb4\xe4\xb8\xba[-1.5, 2]\n        yy = np.arange(-1.5, 2, 0.01)\n        # \xe7\x94\x9f\xe6\x88\x90 x-y \xe5\xb9\xb3\xe9\x9d\xa2\xe9\x87\x87\xe6\xa0\xb7\xe7\xbd\x91\xe6\xa0\xbc\xe7\x82\xb9\xef\xbc\x8c\xe6\x96\xb9\xe4\xbe\xbf\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n        XX, YY = np.meshgrid(xx, yy)\n        preds = model.predict_classes(np.c_[XX.ravel(), YY.ravel()])\n        title = ""\xe7\xbd\x91\xe7\xbb\x9c\xe5\xb1\x82\xe6\x95\xb0\xef\xbc\x9a{0}"".format(2 + n)\n        file = ""\xe7\xbd\x91\xe7\xbb\x9c\xe5\xae\xb9\xe9\x87\x8f_%i.png"" % (2 + n)\n        make_plot(X_train, y_train, title, file, XX, YY, preds, output_dir=OUTPUT_DIR + \'/network_layers\')\n\n\ndef dropout_influence(X_train, y_train):\n    # \xe6\x9e\x84\xe5\xbb\xba 5 \xe7\xa7\x8d\xe4\xb8\x8d\xe5\x90\x8c\xe6\x95\xb0\xe9\x87\x8f Dropout \xe5\xb1\x82\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\n    for n in range(5):\n        # \xe5\x88\x9b\xe5\xbb\xba\xe5\xae\xb9\xe5\x99\xa8\n        model = Sequential()\n        # \xe5\x88\x9b\xe5\xbb\xba\xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\n        model.add(layers.Dense(8, input_dim=2, activation=\'relu\'))\n        counter = 0\n        # \xe7\xbd\x91\xe7\xbb\x9c\xe5\xb1\x82\xe6\x95\xb0\xe5\x9b\xba\xe5\xae\x9a\xe4\xb8\xba 5\n        for _ in range(5):\n            model.add(layers.Dense(64, activation=\'relu\'))\n        # \xe6\xb7\xbb\xe5\x8a\xa0 n \xe4\xb8\xaa Dropout \xe5\xb1\x82\n        if counter < n:\n            counter += 1\n            model.add(layers.Dropout(rate=0.5))\n\n        # \xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\n        model.add(layers.Dense(1, activation=\'sigmoid\'))\n        # \xe6\xa8\xa1\xe5\x9e\x8b\xe8\xa3\x85\xe9\x85\x8d\n        model.compile(loss=\'binary_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\'])\n        # \xe8\xae\xad\xe7\xbb\x83\n        model.fit(X_train, y_train, epochs=N_EPOCHS, verbose=1)\n        # \xe7\xbb\x98\xe5\x88\xb6\xe4\xb8\x8d\xe5\x90\x8c Dropout \xe5\xb1\x82\xe6\x95\xb0\xe7\x9a\x84\xe5\x86\xb3\xe7\xad\x96\xe8\xbe\xb9\xe7\x95\x8c\xe6\x9b\xb2\xe7\xba\xbf\n        # \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe7\x9a\x84 x \xe5\x9d\x90\xe6\xa0\x87\xe8\x8c\x83\xe5\x9b\xb4\xe4\xb8\xba[-2, 3]\n        xx = np.arange(-2, 3, 0.01)\n        # \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe7\x9a\x84 y \xe5\x9d\x90\xe6\xa0\x87\xe8\x8c\x83\xe5\x9b\xb4\xe4\xb8\xba[-1.5, 2]\n        yy = np.arange(-1.5, 2, 0.01)\n        # \xe7\x94\x9f\xe6\x88\x90 x-y \xe5\xb9\xb3\xe9\x9d\xa2\xe9\x87\x87\xe6\xa0\xb7\xe7\xbd\x91\xe6\xa0\xbc\xe7\x82\xb9\xef\xbc\x8c\xe6\x96\xb9\xe4\xbe\xbf\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n        XX, YY = np.meshgrid(xx, yy)\n        preds = model.predict_classes(np.c_[XX.ravel(), YY.ravel()])\n        title = ""\xe6\x97\xa0Dropout\xe5\xb1\x82"" if n == 0 else ""{0}\xe5\xb1\x82 Dropout\xe5\xb1\x82"".format(n)\n        file = ""Dropout_%i.png"" % n\n        make_plot(X_train, y_train, title, file, XX, YY, preds, output_dir=OUTPUT_DIR + \'/dropout\')\n\n\ndef build_model_with_regularization(_lambda):\n    # \xe5\x88\x9b\xe5\xbb\xba\xe5\xb8\xa6\xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\xe9\xa1\xb9\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\n    model = Sequential()\n    model.add(layers.Dense(8, input_dim=2, activation=\'relu\'))  # \xe4\xb8\x8d\xe5\xb8\xa6\xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\xe9\xa1\xb9\n    # 2-4\xe5\xb1\x82\xe5\x9d\x87\xe6\x98\xaf\xe5\xb8\xa6 L2 \xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\xe9\xa1\xb9\n    model.add(layers.Dense(256, activation=\'relu\', kernel_regularizer=regularizers.l2(_lambda)))\n    model.add(layers.Dense(256, activation=\'relu\', kernel_regularizer=regularizers.l2(_lambda)))\n    model.add(layers.Dense(256, activation=\'relu\', kernel_regularizer=regularizers.l2(_lambda)))\n    # \xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\n    model.add(layers.Dense(1, activation=\'sigmoid\'))\n    model.compile(loss=\'binary_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\'])  # \xe6\xa8\xa1\xe5\x9e\x8b\xe8\xa3\x85\xe9\x85\x8d\n    return model\n\n\ndef plot_weights_matrix(model, layer_index, plot_name, file_name, output_dir=OUTPUT_DIR):\n    # \xe7\xbb\x98\xe5\x88\xb6\xe6\x9d\x83\xe5\x80\xbc\xe8\x8c\x83\xe5\x9b\xb4\xe5\x87\xbd\xe6\x95\xb0\n    # \xe6\x8f\x90\xe5\x8f\x96\xe6\x8c\x87\xe5\xae\x9a\xe5\xb1\x82\xe7\x9a\x84\xe6\x9d\x83\xe5\x80\xbc\xe7\x9f\xa9\xe9\x98\xb5\n    weights = model.layers[layer_index].get_weights()[0]\n    shape = weights.shape\n    # \xe7\x94\x9f\xe6\x88\x90\xe5\x92\x8c\xe6\x9d\x83\xe5\x80\xbc\xe7\x9f\xa9\xe9\x98\xb5\xe7\xad\x89\xe5\xa4\xa7\xe5\xb0\x8f\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe5\x9d\x90\xe6\xa0\x87\n    X = np.array(range(shape[1]))\n    Y = np.array(range(shape[0]))\n    X, Y = np.meshgrid(X, Y)\n    # \xe7\xbb\x98\xe5\x88\xb63D\xe5\x9b\xbe\n    fig = plt.figure()\n    ax = fig.gca(projection=\'3d\')\n    ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n    ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n    ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n    plt.title(plot_name, fontsize=20, fontproperties=\'SimHei\')\n    # \xe7\xbb\x98\xe5\x88\xb6\xe6\x9d\x83\xe5\x80\xbc\xe7\x9f\xa9\xe9\x98\xb5\xe8\x8c\x83\xe5\x9b\xb4\n    ax.plot_surface(X, Y, weights, cmap=plt.get_cmap(\'rainbow\'), linewidth=0)\n    # \xe8\xae\xbe\xe7\xbd\xae\xe5\x9d\x90\xe6\xa0\x87\xe8\xbd\xb4\xe5\x90\x8d\n    ax.set_xlabel(\'\xe7\xbd\x91\xe6\xa0\xbcx\xe5\x9d\x90\xe6\xa0\x87\', fontsize=16, rotation=0, fontproperties=\'SimHei\')\n    ax.set_ylabel(\'\xe7\xbd\x91\xe6\xa0\xbcy\xe5\x9d\x90\xe6\xa0\x87\', fontsize=16, rotation=0, fontproperties=\'SimHei\')\n    ax.set_zlabel(\'\xe6\x9d\x83\xe5\x80\xbc\', fontsize=16, rotation=90, fontproperties=\'SimHei\')\n    # \xe4\xbf\x9d\xe5\xad\x98\xe7\x9f\xa9\xe9\x98\xb5\xe8\x8c\x83\xe5\x9b\xb4\xe5\x9b\xbe\n    plt.savefig(output_dir + ""/"" + file_name + "".svg"")\n    plt.close(fig)\n\n\ndef regularizers_influence(X_train, y_train):\n    for _lambda in [1e-5, 1e-3, 1e-1, 0.12, 0.13]:  # \xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\xe7\xb3\xbb\xe6\x95\xb0\n        # \xe5\x88\x9b\xe5\xbb\xba\xe5\xb8\xa6\xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\xe9\xa1\xb9\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n        model = build_model_with_regularization(_lambda)\n        # \xe6\xa8\xa1\xe5\x9e\x8b\xe8\xae\xad\xe7\xbb\x83\n        model.fit(X_train, y_train, epochs=N_EPOCHS, verbose=1)\n        # \xe7\xbb\x98\xe5\x88\xb6\xe6\x9d\x83\xe5\x80\xbc\xe8\x8c\x83\xe5\x9b\xb4\n        layer_index = 2\n        plot_title = ""\xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\xe7\xb3\xbb\xe6\x95\xb0\xef\xbc\x9a{}"".format(_lambda)\n        file_name = ""\xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\xe7\xbd\x91\xe7\xbb\x9c\xe6\x9d\x83\xe5\x80\xbc_"" + str(_lambda)\n        # \xe7\xbb\x98\xe5\x88\xb6\xe7\xbd\x91\xe7\xbb\x9c\xe6\x9d\x83\xe5\x80\xbc\xe8\x8c\x83\xe5\x9b\xb4\xe5\x9b\xbe\n        plot_weights_matrix(model, layer_index, plot_title, file_name, output_dir=OUTPUT_DIR + \'/regularizers\')\n        # \xe7\xbb\x98\xe5\x88\xb6\xe4\xb8\x8d\xe5\x90\x8c\xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\xe7\xb3\xbb\xe6\x95\xb0\xe7\x9a\x84\xe5\x86\xb3\xe7\xad\x96\xe8\xbe\xb9\xe7\x95\x8c\xe7\xba\xbf\n        # \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe7\x9a\x84 x \xe5\x9d\x90\xe6\xa0\x87\xe8\x8c\x83\xe5\x9b\xb4\xe4\xb8\xba[-2, 3]\n        xx = np.arange(-2, 3, 0.01)\n        # \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe7\x9a\x84 y \xe5\x9d\x90\xe6\xa0\x87\xe8\x8c\x83\xe5\x9b\xb4\xe4\xb8\xba[-1.5, 2]\n        yy = np.arange(-1.5, 2, 0.01)\n        # \xe7\x94\x9f\xe6\x88\x90 x-y \xe5\xb9\xb3\xe9\x9d\xa2\xe9\x87\x87\xe6\xa0\xb7\xe7\xbd\x91\xe6\xa0\xbc\xe7\x82\xb9\xef\xbc\x8c\xe6\x96\xb9\xe4\xbe\xbf\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n        XX, YY = np.meshgrid(xx, yy)\n        preds = model.predict_classes(np.c_[XX.ravel(), YY.ravel()])\n        title = ""\xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\xe7\xb3\xbb\xe6\x95\xb0\xef\xbc\x9a{}"".format(_lambda)\n        file = ""\xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96_%g.svg"" % _lambda\n        make_plot(X_train, y_train, title, file, XX, YY, preds, output_dir=OUTPUT_DIR + \'/regularizers\')\n\n\ndef main():\n    X, y, X_train, X_test, y_train, y_test = load_dataset()\n    # \xe7\xbb\x98\xe5\x88\xb6\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x88\x86\xe5\xb8\x83\n    make_plot(X, y, None, ""\xe6\x9c\x88\xe7\x89\x99\xe5\xbd\xa2\xe7\x8a\xb6\xe4\xba\x8c\xe5\x88\x86\xe7\xb1\xbb\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x88\x86\xe5\xb8\x83.svg"")\n    # \xe7\xbd\x91\xe7\xbb\x9c\xe5\xb1\x82\xe6\x95\xb0\xe7\x9a\x84\xe5\xbd\xb1\xe5\x93\x8d\n    network_layers_influence(X_train, y_train)\n    # Dropout\xe7\x9a\x84\xe5\xbd\xb1\xe5\x93\x8d\n    dropout_influence(X_train, y_train)\n    # \xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\xe7\x9a\x84\xe5\xbd\xb1\xe5\x93\x8d\n    regularizers_influence(X_train, y_train)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
ch09-过拟合/compile_fit.py,9,"b'import  tensorflow as tf\nfrom    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n\n\ndef preprocess(x, y):\n    """"""\n    x is a simple image, not a batch\n    """"""\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    x = tf.reshape(x, [28*28])\n    y = tf.cast(y, dtype=tf.int32)\n    y = tf.one_hot(y, depth=10)\n    return x,y\n\n\nbatchsz = 128\n(x, y), (x_val, y_val) = datasets.mnist.load_data()\nprint(\'datasets:\', x.shape, y.shape, x.min(), x.max())\n\n\n\ndb = tf.data.Dataset.from_tensor_slices((x,y))\ndb = db.map(preprocess).shuffle(60000).batch(batchsz)\nds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\nds_val = ds_val.map(preprocess).batch(batchsz) \n\nsample = next(iter(db))\nprint(sample[0].shape, sample[1].shape)\n\n\nnetwork = Sequential([layers.Dense(256, activation=\'relu\'),\n                     layers.Dense(128, activation=\'relu\'),\n                     layers.Dense(64, activation=\'relu\'),\n                     layers.Dense(32, activation=\'relu\'),\n                     layers.Dense(10)])\nnetwork.build(input_shape=(None, 28*28))\nnetwork.summary()\n\n\n\n\nnetwork.compile(optimizer=optimizers.Adam(lr=0.01),\n\t\tloss=tf.losses.CategoricalCrossentropy(from_logits=True),\n\t\tmetrics=[\'accuracy\']\n\t)\n\nnetwork.fit(db, epochs=5, validation_data=ds_val,\n              validation_steps=2)\n \nnetwork.evaluate(ds_val)\n\nsample = next(iter(ds_val))\nx = sample[0]\ny = sample[1] # one-hot\npred = network.predict(x) # [b, 10]\n# convert back to number \ny = tf.argmax(y, axis=1)\npred = tf.argmax(pred, axis=1)\n\nprint(pred)\nprint(y)\n'"
ch09-过拟合/dropout.py,20,"b""import  os\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n\nimport  tensorflow as tf\nfrom    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n\n\ndef preprocess(x, y):\n\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    y = tf.cast(y, dtype=tf.int32)\n\n    return x,y\n\n\nbatchsz = 128\n(x, y), (x_val, y_val) = datasets.mnist.load_data()\nprint('datasets:', x.shape, y.shape, x.min(), x.max())\n\n\n\ndb = tf.data.Dataset.from_tensor_slices((x,y))\ndb = db.map(preprocess).shuffle(60000).batch(batchsz).repeat(10)\n\nds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\nds_val = ds_val.map(preprocess).batch(batchsz) \n\n\n\n\nnetwork = Sequential([layers.Dense(256, activation='relu'),\n                     layers.Dropout(0.5), # 0.5 rate to drop\n                     layers.Dense(128, activation='relu'),\n                     layers.Dropout(0.5), # 0.5 rate to drop\n                     layers.Dense(64, activation='relu'),\n                     layers.Dense(32, activation='relu'),\n                     layers.Dense(10)])\nnetwork.build(input_shape=(None, 28*28))\nnetwork.summary()\n\noptimizer = optimizers.Adam(lr=0.01)\n\n\n\nfor step, (x,y) in enumerate(db):\n\n    with tf.GradientTape() as tape:\n        # [b, 28, 28] => [b, 784]\n        x = tf.reshape(x, (-1, 28*28))\n        # [b, 784] => [b, 10]\n        out = network(x, training=True)\n        # [b] => [b, 10]\n        y_onehot = tf.one_hot(y, depth=10) \n        # [b]\n        loss = tf.reduce_mean(tf.losses.categorical_crossentropy(y_onehot, out, from_logits=True))\n\n\n        loss_regularization = []\n        for p in network.trainable_variables:\n            loss_regularization.append(tf.nn.l2_loss(p))\n        loss_regularization = tf.reduce_sum(tf.stack(loss_regularization))\n\n        loss = loss + 0.0001 * loss_regularization\n \n\n    grads = tape.gradient(loss, network.trainable_variables)\n    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n\n\n    if step % 100 == 0:\n\n        print(step, 'loss:', float(loss), 'loss_regularization:', float(loss_regularization)) \n\n\n    # evaluate\n    if step % 500 == 0:\n        total, total_correct = 0., 0\n\n        for step, (x, y) in enumerate(ds_val): \n            # [b, 28, 28] => [b, 784]\n            x = tf.reshape(x, (-1, 28*28))\n            # [b, 784] => [b, 10] \n            out = network(x, training=True)  \n            # [b, 10] => [b] \n            pred = tf.argmax(out, axis=1) \n            pred = tf.cast(pred, dtype=tf.int32)\n            # bool type \n            correct = tf.equal(pred, y)\n            # bool tensor => int tensor => numpy\n            total_correct += tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()\n            total += x.shape[0]\n\n        print(step, 'Evaluate Acc with drop:', total_correct/total)\n\n        total, total_correct = 0., 0\n\n        for step, (x, y) in enumerate(ds_val): \n            # [b, 28, 28] => [b, 784]\n            x = tf.reshape(x, (-1, 28*28))\n            # [b, 784] => [b, 10] \n            out = network(x, training=False)  \n            # [b, 10] => [b] \n            pred = tf.argmax(out, axis=1) \n            pred = tf.cast(pred, dtype=tf.int32)\n            # bool type \n            correct = tf.equal(pred, y)\n            # bool tensor => int tensor => numpy\n            total_correct += tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()\n            total += x.shape[0]\n\n        print(step, 'Evaluate Acc without drop:', total_correct/total)"""
ch09-过拟合/regularization.py,15,"b""import  tensorflow as tf\nfrom    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n\n\ndef preprocess(x, y):\n\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    y = tf.cast(y, dtype=tf.int32)\n\n    return x,y\n\n\nbatchsz = 128\n(x, y), (x_val, y_val) = datasets.mnist.load_data()\nprint('datasets:', x.shape, y.shape, x.min(), x.max())\n\n\n\ndb = tf.data.Dataset.from_tensor_slices((x,y))\ndb = db.map(preprocess).shuffle(60000).batch(batchsz).repeat(10)\n\nds_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\nds_val = ds_val.map(preprocess).batch(batchsz) \n\n\n\n\nnetwork = Sequential([layers.Dense(256, activation='relu'),\n                     layers.Dense(128, activation='relu'),\n                     layers.Dense(64, activation='relu'),\n                     layers.Dense(32, activation='relu'),\n                     layers.Dense(10)])\nnetwork.build(input_shape=(None, 28*28))\nnetwork.summary()\n\noptimizer = optimizers.Adam(lr=0.01)\n\n\n\nfor step, (x,y) in enumerate(db):\n\n    with tf.GradientTape() as tape:\n        # [b, 28, 28] => [b, 784]\n        x = tf.reshape(x, (-1, 28*28))\n        # [b, 784] => [b, 10]\n        out = network(x)\n        # [b] => [b, 10]\n        y_onehot = tf.one_hot(y, depth=10) \n        # [b]\n        loss = tf.reduce_mean(tf.losses.categorical_crossentropy(y_onehot, out, from_logits=True))\n\n\n        loss_regularization = []\n        for p in network.trainable_variables:\n            loss_regularization.append(tf.nn.l2_loss(p))\n        loss_regularization = tf.reduce_sum(tf.stack(loss_regularization))\n\n        loss = loss + 0.0001 * loss_regularization\n \n\n    grads = tape.gradient(loss, network.trainable_variables)\n    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n\n\n    if step % 100 == 0:\n\n        print(step, 'loss:', float(loss), 'loss_regularization:', float(loss_regularization)) \n\n\n    # evaluate\n    if step % 500 == 0:\n        total, total_correct = 0., 0\n\n        for step, (x, y) in enumerate(ds_val): \n            # [b, 28, 28] => [b, 784]\n            x = tf.reshape(x, (-1, 28*28))\n            # [b, 784] => [b, 10]\n            out = network(x) \n            # [b, 10] => [b] \n            pred = tf.argmax(out, axis=1) \n            pred = tf.cast(pred, dtype=tf.int32)\n            # bool type \n            correct = tf.equal(pred, y)\n            # bool tensor => int tensor => numpy\n            total_correct += tf.reduce_sum(tf.cast(correct, dtype=tf.int32)).numpy()\n            total += x.shape[0]\n\n        print(step, 'Evaluate Acc:', total_correct/total)"""
ch09-过拟合/train_evalute_test.py,14,"b'import  tensorflow as tf\nfrom    tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n\n\ndef preprocess(x, y):\n    """"""\n    x is a simple image, not a batch\n    """"""\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    x = tf.reshape(x, [28*28])\n    y = tf.cast(y, dtype=tf.int32)\n    y = tf.one_hot(y, depth=10)\n    return x,y\n\n\nbatchsz = 128\n(x, y), (x_test, y_test) = datasets.mnist.load_data()\nprint(\'datasets:\', x.shape, y.shape, x.min(), x.max())\n\n\n\nidx = tf.range(60000)\nidx = tf.random.shuffle(idx)\nx_train, y_train = tf.gather(x, idx[:50000]), tf.gather(y, idx[:50000])\nx_val, y_val = tf.gather(x, idx[-10000:]) , tf.gather(y, idx[-10000:])\nprint(x_train.shape, y_train.shape, x_val.shape, y_val.shape)\ndb_train = tf.data.Dataset.from_tensor_slices((x_train,y_train))\ndb_train = db_train.map(preprocess).shuffle(50000).batch(batchsz)\n\ndb_val = tf.data.Dataset.from_tensor_slices((x_val,y_val))\ndb_val = db_val.map(preprocess).shuffle(10000).batch(batchsz)\n\n\n\ndb_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\ndb_test = db_test.map(preprocess).batch(batchsz) \n\nsample = next(iter(db_train))\nprint(sample[0].shape, sample[1].shape)\n\n\nnetwork = Sequential([layers.Dense(256, activation=\'relu\'),\n                     layers.Dense(128, activation=\'relu\'),\n                     layers.Dense(64, activation=\'relu\'),\n                     layers.Dense(32, activation=\'relu\'),\n                     layers.Dense(10)])\nnetwork.build(input_shape=(None, 28*28))\nnetwork.summary()\n\n\n\n\nnetwork.compile(optimizer=optimizers.Adam(lr=0.01),\n\t\tloss=tf.losses.CategoricalCrossentropy(from_logits=True),\n\t\tmetrics=[\'accuracy\']\n\t)\n\nnetwork.fit(db_train, epochs=6, validation_data=db_val, validation_freq=2)\n\nprint(\'Test performance:\') \nnetwork.evaluate(db_test)\n \n\nsample = next(iter(db_test))\nx = sample[0]\ny = sample[1] # one-hot\npred = network.predict(x) # [b, 10]\n# convert back to number \ny = tf.argmax(y, axis=1)\npred = tf.argmax(pred, axis=1)\n\nprint(pred)\nprint(y)\n'"
ch10-卷积神经网络/bn_main.py,3,"b""import tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, optimizers\n\n\n# 2 images with 4x4 size, 3 channels\n# we explicitly enforce the mean and stddev to N(1, 0.5)\nx = tf.random.normal([2,4,4,3], mean=1.,stddev=0.5)\n\nnet = layers.BatchNormalization(axis=-1, center=True, scale=True,\n                                trainable=True)\n\nout = net(x)\nprint('forward in test mode:', net.variables)\n\n\nout = net(x, training=True)\nprint('forward in train mode(1 step):', net.variables)\n\nfor i in range(100):\n    out = net(x, training=True)\nprint('forward in train mode(100 steps):', net.variables)\n\n\noptimizer = optimizers.SGD(lr=1e-2)\nfor i in range(10):\n    with tf.GradientTape() as tape:\n        out = net(x, training=True)\n        loss = tf.reduce_mean(tf.pow(out,2)) - 1\n\n    grads = tape.gradient(loss, net.trainable_variables)\n    optimizer.apply_gradients(zip(grads, net.trainable_variables))\nprint('backward(10 steps):', net.variables)\n\n\n\n\n"""
ch10-卷积神经网络/cifar10_train.py,31,"b'import  tensorflow as tf\nfrom    tensorflow.keras import layers, optimizers, datasets, Sequential\nimport  os\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'2\'\ntf.random.set_seed(2345)\n\nconv_layers = [ # 5 units of conv + max pooling\n    # unit 1\n    layers.Conv2D(64, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\n    layers.Conv2D(64, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\n    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\'same\'),\n\n    # unit 2\n    layers.Conv2D(128, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\n    layers.Conv2D(128, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\n    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\'same\'),\n\n    # unit 3\n    layers.Conv2D(256, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\n    layers.Conv2D(256, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\n    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\'same\'),\n\n    # unit 4\n    layers.Conv2D(512, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\n    layers.Conv2D(512, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\n    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\'same\'),\n\n    # unit 5\n    layers.Conv2D(512, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\n    layers.Conv2D(512, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\n    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\'same\')\n\n]\n\n\n\ndef preprocess(x, y):\n    # [0~1]\n    x = 2*tf.cast(x, dtype=tf.float32) / 255.-1\n    y = tf.cast(y, dtype=tf.int32)\n    return x,y\n\n\n(x,y), (x_test, y_test) = datasets.cifar10.load_data()\ny = tf.squeeze(y, axis=1)\ny_test = tf.squeeze(y_test, axis=1)\nprint(x.shape, y.shape, x_test.shape, y_test.shape)\n\n\ntrain_db = tf.data.Dataset.from_tensor_slices((x,y))\ntrain_db = train_db.shuffle(1000).map(preprocess).batch(128)\n\ntest_db = tf.data.Dataset.from_tensor_slices((x_test,y_test))\ntest_db = test_db.map(preprocess).batch(64)\n\nsample = next(iter(train_db))\nprint(\'sample:\', sample[0].shape, sample[1].shape,\n      tf.reduce_min(sample[0]), tf.reduce_max(sample[0]))\n\n\ndef main():\n\n    # [b, 32, 32, 3] => [b, 1, 1, 512]\n    conv_net = Sequential(conv_layers)\n\n    fc_net = Sequential([\n        layers.Dense(256, activation=tf.nn.relu),\n        layers.Dense(128, activation=tf.nn.relu),\n        layers.Dense(10, activation=None),\n    ])\n\n    conv_net.build(input_shape=[None, 32, 32, 3])\n    fc_net.build(input_shape=[None, 512])\n    conv_net.summary()\n    fc_net.summary()\n    optimizer = optimizers.Adam(lr=1e-4)\n\n    # [1, 2] + [3, 4] => [1, 2, 3, 4]\n    variables = conv_net.trainable_variables + fc_net.trainable_variables\n\n    for epoch in range(50):\n\n        for step, (x,y) in enumerate(train_db):\n\n            with tf.GradientTape() as tape:\n                # [b, 32, 32, 3] => [b, 1, 1, 512]\n                out = conv_net(x)\n                # flatten, => [b, 512]\n                out = tf.reshape(out, [-1, 512])\n                # [b, 512] => [b, 10]\n                logits = fc_net(out)\n                # [b] => [b, 10]\n                y_onehot = tf.one_hot(y, depth=10)\n                # compute loss\n                loss = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\n                loss = tf.reduce_mean(loss)\n\n            grads = tape.gradient(loss, variables)\n            optimizer.apply_gradients(zip(grads, variables))\n\n            if step %100 == 0:\n                print(epoch, step, \'loss:\', float(loss))\n\n\n\n        total_num = 0\n        total_correct = 0\n        for x,y in test_db:\n\n            out = conv_net(x)\n            out = tf.reshape(out, [-1, 512])\n            logits = fc_net(out)\n            prob = tf.nn.softmax(logits, axis=1)\n            pred = tf.argmax(prob, axis=1)\n            pred = tf.cast(pred, dtype=tf.int32)\n\n            correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)\n            correct = tf.reduce_sum(correct)\n\n            total_num += x.shape[0]\n            total_correct += int(correct)\n\n        acc = total_correct / total_num\n        print(epoch, \'acc:\', acc)\n\n\n\nif __name__ == \'__main__\':\n    main()\n'"
ch10-卷积神经网络/nb.py,60,"b'#%%\nimport  os\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\']=\'2\'\n\nimport  tensorflow as tf\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers, optimizers, datasets, Sequential\n\n\n\n#%% \nx = tf.random.normal([2,5,5,3]) # \xe6\xa8\xa1\xe6\x8b\x9f\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8c3\xe9\x80\x9a\xe9\x81\x93\xef\xbc\x8c\xe9\xab\x98\xe5\xae\xbd\xe4\xb8\xba5\n# \xe9\x9c\x80\xe8\xa6\x81\xe6\xa0\xb9\xe6\x8d\xae[k,k,cin,cout]\xe6\xa0\xbc\xe5\xbc\x8f\xe5\x88\x9b\xe5\xbb\xba\xef\xbc\x8c4\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\nw = tf.random.normal([3,3,3,4]) \n# \xe6\xad\xa5\xe9\x95\xbf\xe4\xb8\xba1, padding\xe4\xb8\xba0,\nout = tf.nn.conv2d(x,w,strides=1,padding=[[0,0],[0,0],[0,0],[0,0]])\n\n\n# %%\nx = tf.random.normal([2,5,5,3]) # \xe6\xa8\xa1\xe6\x8b\x9f\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8c3\xe9\x80\x9a\xe9\x81\x93\xef\xbc\x8c\xe9\xab\x98\xe5\xae\xbd\xe4\xb8\xba5\n# \xe9\x9c\x80\xe8\xa6\x81\xe6\xa0\xb9\xe6\x8d\xae[k,k,cin,cout]\xe6\xa0\xbc\xe5\xbc\x8f\xe5\x88\x9b\xe5\xbb\xba\xef\xbc\x8c4\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\nw = tf.random.normal([3,3,3,4])\n# \xe6\xad\xa5\xe9\x95\xbf\xe4\xb8\xba1, padding\xe4\xb8\xba1,\nout = tf.nn.conv2d(x,w,strides=1,padding=[[0,0],[1,1],[1,1],[0,0]])\n\n\n# %%\nx = tf.random.normal([2,5,5,3]) # \xe6\xa8\xa1\xe6\x8b\x9f\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8c3\xe9\x80\x9a\xe9\x81\x93\xef\xbc\x8c\xe9\xab\x98\xe5\xae\xbd\xe4\xb8\xba5\nw = tf.random.normal([3,3,3,4]) # 4\xe4\xb8\xaa3x3\xe5\xa4\xa7\xe5\xb0\x8f\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\n# \xe6\xad\xa5\xe9\x95\xbf\xe4\xb8\xba,padding\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xba\xe8\xbe\x93\xe5\x87\xba\xe3\x80\x81\xe8\xbe\x93\xe5\x85\xa5\xe5\x90\x8c\xe5\xa4\xa7\xe5\xb0\x8f\n# \xe9\x9c\x80\xe8\xa6\x81\xe6\xb3\xa8\xe6\x84\x8f\xe7\x9a\x84\xe6\x98\xaf, padding=same\xe5\x8f\xaa\xe6\x9c\x89\xe5\x9c\xa8strides=1\xe6\x97\xb6\xe6\x89\x8d\xe6\x98\xaf\xe5\x90\x8c\xe5\xa4\xa7\xe5\xb0\x8f\nout = tf.nn.conv2d(x,w,strides=1,padding=\'SAME\')\n\n\n# %%\nx = tf.random.normal([2,5,5,3])\nw = tf.random.normal([3,3,3,4])\n# \xe9\xab\x98\xe5\xae\xbd\xe6\x8c\x893\xe5\x80\x8d\xe5\x87\x8f\xe5\xb0\x91\nout = tf.nn.conv2d(x,w,strides=3,padding=\'SAME\')\nprint(out.shape)\n\n\n# %%\n# \xe6\xa0\xb9\xe6\x8d\xae[cout]\xe6\xa0\xbc\xe5\xbc\x8f\xe5\x88\x9b\xe5\xbb\xba\xe5\x81\x8f\xe7\xbd\xae\xe5\x90\x91\xe9\x87\x8f\nb = tf.zeros([4])\n# \xe5\x9c\xa8\xe5\x8d\xb7\xe7\xa7\xaf\xe8\xbe\x93\xe5\x87\xba\xe4\xb8\x8a\xe5\x8f\xa0\xe5\x8a\xa0\xe5\x81\x8f\xe7\xbd\xae\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x8c\xe5\xae\x83\xe4\xbc\x9a\xe8\x87\xaa\xe5\x8a\xa8broadcasting\xe4\xb8\xba[b,h\',w\',cout]\nout = out + b\n\n\n# %%\n# \xe5\x88\x9b\xe5\xbb\xba\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\xb1\xbb\nlayer = layers.Conv2D(4,kernel_size=(3,4),strides=(2,1),padding=\'SAME\')\nout = layer(x) # \xe5\x89\x8d\xe5\x90\x91\xe8\xae\xa1\xe7\xae\x97\nout.shape\n\n\n# %%\nlayer.kernel,layer.bias\n# \xe8\xbf\x94\xe5\x9b\x9e\xe6\x89\x80\xe6\x9c\x89\xe5\xbe\x85\xe4\xbc\x98\xe5\x8c\x96\xe5\xbc\xa0\xe9\x87\x8f\xe5\x88\x97\xe8\xa1\xa8\nlayer.trainable_variables\n\n# %%\nfrom tensorflow.keras import Sequential\nnetwork = Sequential([ # \xe7\xbd\x91\xe7\xbb\x9c\xe5\xae\xb9\xe5\x99\xa8\n    layers.Conv2D(6,kernel_size=3,strides=1), # \xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82, 6\xe4\xb8\xaa3x3\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\n    layers.MaxPooling2D(pool_size=2,strides=2), # \xe9\xab\x98\xe5\xae\xbd\xe5\x90\x84\xe5\x87\x8f\xe5\x8d\x8a\xe7\x9a\x84\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\n    layers.ReLU(), # \xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\n    layers.Conv2D(16,kernel_size=3,strides=1), # \xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82, 16\xe4\xb8\xaa3x3\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\n    layers.MaxPooling2D(pool_size=2,strides=2), # \xe9\xab\x98\xe5\xae\xbd\xe5\x90\x84\xe5\x87\x8f\xe5\x8d\x8a\xe7\x9a\x84\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\n    layers.ReLU(), # \xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\n    layers.Flatten(), # \xe6\x89\x93\xe5\xb9\xb3\xe5\xb1\x82\xef\xbc\x8c\xe6\x96\xb9\xe4\xbe\xbf\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe5\xa4\x84\xe7\x90\x86\n\n    layers.Dense(120, activation=\'relu\'), # \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xef\xbc\x8c120\xe4\xb8\xaa\xe8\x8a\x82\xe7\x82\xb9\n    layers.Dense(84, activation=\'relu\'), # \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xef\xbc\x8c84\xe8\x8a\x82\xe7\x82\xb9\n    layers.Dense(10) # \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xef\xbc\x8c10\xe4\xb8\xaa\xe8\x8a\x82\xe7\x82\xb9\n                    ])\n# build\xe4\xb8\x80\xe6\xac\xa1\xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe7\xbb\x99\xe8\xbe\x93\xe5\x85\xa5X\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xef\xbc\x8c\xe5\x85\xb6\xe4\xb8\xad4\xe4\xb8\xba\xe9\x9a\x8f\xe6\x84\x8f\xe7\xbb\x99\xe7\x9a\x84batchsz\nnetwork.build(input_shape=(4, 28, 28, 1))\n# \xe7\xbb\x9f\xe8\xae\xa1\xe7\xbd\x91\xe7\xbb\x9c\xe4\xbf\xa1\xe6\x81\xaf\nnetwork.summary()\n\n\n# %%\n# \xe5\xaf\xbc\xe5\x85\xa5\xe8\xaf\xaf\xe5\xb7\xae\xe8\xae\xa1\xe7\xae\x97\xef\xbc\x8c\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe6\xa8\xa1\xe5\x9d\x97\nfrom tensorflow.keras import losses, optimizers\n# \xe5\x88\x9b\xe5\xbb\xba\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe7\xb1\xbb\xef\xbc\x8c\xe5\x9c\xa8\xe5\xae\x9e\xe9\x99\x85\xe8\xae\xa1\xe7\xae\x97\xe6\x97\xb6\xe7\x9b\xb4\xe6\x8e\xa5\xe8\xb0\x83\xe7\x94\xa8\xe7\xb1\xbb\xe5\xae\x9e\xe4\xbe\x8b\xe5\x8d\xb3\xe5\x8f\xaf\ncriteon = losses.CategoricalCrossentropy(from_logits=True)\n\n# %%\n    # \xe6\x9e\x84\xe5\xbb\xba\xe6\xa2\xaf\xe5\xba\xa6\xe8\xae\xb0\xe5\xbd\x95\xe7\x8e\xaf\xe5\xa2\x83\n    with tf.GradientTape() as tape: \n        # \xe6\x8f\x92\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe7\xbb\xb4\xe5\xba\xa6\xef\xbc\x8c=>[b,28,28,1]\n        x = tf.expand_dims(x,axis=3)\n        # \xe5\x89\x8d\xe5\x90\x91\xe8\xae\xa1\xe7\xae\x97\xef\xbc\x8c\xe8\x8e\xb7\xe5\xbe\x9710\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe5\x88\x86\xe5\xb8\x83\xef\xbc\x8c[b, 784] => [b, 10]\n        out = network(x)\n        # \xe7\x9c\x9f\xe5\xae\x9e\xe6\xa0\x87\xe7\xad\xbeone-hot\xe7\xbc\x96\xe7\xa0\x81\xef\xbc\x8c[b] => [b, 10]\n        y_onehot = tf.one_hot(y, depth=10)\n        # \xe8\xae\xa1\xe7\xae\x97\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe6\xa0\x87\xe9\x87\x8f\n        loss = criteon(y_onehot, out)\n    # \xe8\x87\xaa\xe5\x8a\xa8\xe8\xae\xa1\xe7\xae\x97\xe6\xa2\xaf\xe5\xba\xa6\n    grads = tape.gradient(loss, network.trainable_variables)\n    # \xe8\x87\xaa\xe5\x8a\xa8\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\n    optimizer.apply_gradients(zip(grads, network.trainable_variables))\n\n\n# %%\n        # \xe8\xae\xb0\xe5\xbd\x95\xe9\xa2\x84\xe6\xb5\x8b\xe6\xad\xa3\xe7\xa1\xae\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\xef\xbc\x8c\xe6\x80\xbb\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\xe9\x87\x8f\n        correct, total = 0,0\n        for x,y in db_test: # \xe9\x81\x8d\xe5\x8e\x86\xe6\x89\x80\xe6\x9c\x89\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe6\xa0\xb7\xe6\x9c\xac\n            # \xe6\x8f\x92\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe7\xbb\xb4\xe5\xba\xa6\xef\xbc\x8c=>[b,28,28,1]\n            x = tf.expand_dims(x,axis=3)\n            # \xe5\x89\x8d\xe5\x90\x91\xe8\xae\xa1\xe7\xae\x97\xef\xbc\x8c\xe8\x8e\xb7\xe5\xbe\x9710\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe5\x88\x86\xe5\xb8\x83\xef\xbc\x8c[b, 784] => [b, 10]\n            out = network(x)\n            # \xe7\x9c\x9f\xe5\xae\x9e\xe7\x9a\x84\xe6\xb5\x81\xe7\xa8\x8b\xe6\x97\xb6\xe5\x85\x88\xe7\xbb\x8f\xe8\xbf\x87softmax\xef\xbc\x8c\xe5\x86\x8dargmax\n            # \xe4\xbd\x86\xe6\x98\xaf\xe7\x94\xb1\xe4\xba\x8esoftmax\xe4\xb8\x8d\xe6\x94\xb9\xe5\x8f\x98\xe5\x85\x83\xe7\xb4\xa0\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xe7\x9b\xb8\xe5\xaf\xb9\xe5\x85\xb3\xe7\xb3\xbb\xef\xbc\x8c\xe6\x95\x85\xe7\x9c\x81\xe5\x8e\xbb\n            pred = tf.argmax(out, axis=-1)  \n            y = tf.cast(y, tf.int64)\n            # \xe7\xbb\x9f\xe8\xae\xa1\xe9\xa2\x84\xe6\xb5\x8b\xe6\xad\xa3\xe7\xa1\xae\xe6\x95\xb0\xe9\x87\x8f\n            correct += float(tf.reduce_sum(tf.cast(tf.equal(pred, y),tf.float32)))\n            # \xe7\xbb\x9f\xe8\xae\xa1\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa0\xb7\xe6\x9c\xac\xe6\x80\xbb\xe6\x95\xb0\n            total += x.shape[0]\n        # \xe8\xae\xa1\xe7\xae\x97\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\n        print(\'test acc:\', correct/total)\n\n\n# %%\n# \xe6\x9e\x84\xe9\x80\xa0\xe8\xbe\x93\xe5\x85\xa5\nx=tf.random.normal([100,32,32,3])\n# \xe5\xb0\x86\xe5\x85\xb6\xe4\xbb\x96\xe7\xbb\xb4\xe5\xba\xa6\xe5\x90\x88\xe5\xb9\xb6\xef\xbc\x8c\xe4\xbb\x85\xe4\xbf\x9d\xe7\x95\x99\xe9\x80\x9a\xe9\x81\x93\xe7\xbb\xb4\xe5\xba\xa6\nx=tf.reshape(x,[-1,3])\n# \xe8\xae\xa1\xe7\xae\x97\xe5\x85\xb6\xe4\xbb\x96\xe7\xbb\xb4\xe5\xba\xa6\xe7\x9a\x84\xe5\x9d\x87\xe5\x80\xbc\nub=tf.reduce_mean(x,axis=0)\nub\n\n\n# %%\n# \xe5\x88\x9b\xe5\xbb\xbaBN\xe5\xb1\x82\nlayer=layers.BatchNormalization()\n\n# %%\nnetwork = Sequential([ # \xe7\xbd\x91\xe7\xbb\x9c\xe5\xae\xb9\xe5\x99\xa8\n    layers.Conv2D(6,kernel_size=3,strides=1),\n    # \xe6\x8f\x92\xe5\x85\xa5BN\xe5\xb1\x82\n    layers.BatchNormalization(),\n    layers.MaxPooling2D(pool_size=2,strides=2),\n    layers.ReLU(),\n    layers.Conv2D(16,kernel_size=3,strides=1),\n    # \xe6\x8f\x92\xe5\x85\xa5BN\xe5\xb1\x82\n    layers.BatchNormalization(),\n    layers.MaxPooling2D(pool_size=2,strides=2),\n    layers.ReLU(),\n    layers.Flatten(),\n    layers.Dense(120, activation=\'relu\'),\n    # \xe6\xad\xa4\xe5\xa4\x84\xe4\xb9\x9f\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x8f\x92\xe5\x85\xa5BN\xe5\xb1\x82\n    layers.Dense(84, activation=\'relu\'), \n    # \xe6\xad\xa4\xe5\xa4\x84\xe4\xb9\x9f\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x8f\x92\xe5\x85\xa5BN\xe5\xb1\x82\n    layers.Dense(10)\n                    ])\n\n\n# %%\n    with tf.GradientTape() as tape: \n        # \xe6\x8f\x92\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe7\xbb\xb4\xe5\xba\xa6\n        x = tf.expand_dims(x,axis=3)\n        # \xe5\x89\x8d\xe5\x90\x91\xe8\xae\xa1\xe7\xae\x97\xef\xbc\x8c\xe8\xae\xbe\xe7\xbd\xae\xe8\xae\xa1\xe7\xae\x97\xe6\xa8\xa1\xe5\xbc\x8f\xef\xbc\x8c[b, 784] => [b, 10]\n        out = network(x, training=True)\n\n\n# %%\n        for x,y in db_test: # \xe9\x81\x8d\xe5\x8e\x86\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\n            # \xe6\x8f\x92\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe7\xbb\xb4\xe5\xba\xa6\n            x = tf.expand_dims(x,axis=3)\n            # \xe5\x89\x8d\xe5\x90\x91\xe8\xae\xa1\xe7\xae\x97\xef\xbc\x8c\xe6\xb5\x8b\xe8\xaf\x95\xe6\xa8\xa1\xe5\xbc\x8f\n            out = network(x, training=False)\n\n\n# %%\ndef preprocess(x, y):\n    # [0~1]\n    x = 2*tf.cast(x, dtype=tf.float32) / 255.-1\n    y = tf.cast(y, dtype=tf.int32)\n    return x,y\n    \n# \xe5\x9c\xa8\xe7\xba\xbf\xe4\xb8\x8b\xe8\xbd\xbd\xef\xbc\x8c\xe5\x8a\xa0\xe8\xbd\xbdCIFAR10\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n(x,y), (x_test, y_test) = datasets.cifar100.load_data()\n# \xe5\x88\xa0\xe9\x99\xa4y\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbb\xb4\xe5\xba\xa6\xef\xbc\x8c[b,1] => [b]\ny = tf.squeeze(y, axis=1)\ny_test = tf.squeeze(y_test, axis=1)\n# \xe6\x89\x93\xe5\x8d\xb0\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe5\x92\x8c\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\nprint(x.shape, y.shape, x_test.shape, y_test.shape)\n# \xe6\x9e\x84\xe5\xbb\xba\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe5\xaf\xb9\xe8\xb1\xa1\ntrain_db = tf.data.Dataset.from_tensor_slices((x,y))\ntrain_db = train_db.shuffle(1000).map(preprocess).batch(128)\n# \xe6\x9e\x84\xe5\xbb\xba\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe5\xaf\xb9\xe8\xb1\xa1\ntest_db = tf.data.Dataset.from_tensor_slices((x_test,y_test))\ntest_db = test_db.map(preprocess).batch(128)\n# \xe4\xbb\x8e\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe4\xb8\xad\xe9\x87\x87\xe6\xa0\xb7\xe4\xb8\x80\xe4\xb8\xaaBatch\xef\xbc\x8c\xe8\xa7\x82\xe5\xaf\x9f\nsample = next(iter(train_db))\nprint(\'sample:\', sample[0].shape, sample[1].shape,\n      tf.reduce_min(sample[0]), tf.reduce_max(sample[0]))\n\n\n\n# %%\nconv_layers = [ # \xe5\x85\x88\xe5\x88\x9b\xe5\xbb\xba\xe5\x8c\x85\xe5\x90\xab\xe5\xa4\x9a\xe5\xb1\x82\xe7\x9a\x84\xe5\x88\x97\xe8\xa1\xa8\n    # Conv-Conv-Pooling\xe5\x8d\x95\xe5\x85\x831\n    # 64\xe4\xb8\xaa3x3\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8, \xe8\xbe\x93\xe5\x85\xa5\xe8\xbe\x93\xe5\x87\xba\xe5\x90\x8c\xe5\xa4\xa7\xe5\xb0\x8f\n    layers.Conv2D(64, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\n    layers.Conv2D(64, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\n    # \xe9\xab\x98\xe5\xae\xbd\xe5\x87\x8f\xe5\x8d\x8a\n    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\'same\'),\n\n    # Conv-Conv-Pooling\xe5\x8d\x95\xe5\x85\x832,\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x8f\x90\xe5\x8d\x87\xe8\x87\xb3128\xef\xbc\x8c\xe9\xab\x98\xe5\xae\xbd\xe5\xa4\xa7\xe5\xb0\x8f\xe5\x87\x8f\xe5\x8d\x8a\n    layers.Conv2D(128, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\n    layers.Conv2D(128, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\n    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\'same\'),\n\n    # Conv-Conv-Pooling\xe5\x8d\x95\xe5\x85\x833,\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x8f\x90\xe5\x8d\x87\xe8\x87\xb3256\xef\xbc\x8c\xe9\xab\x98\xe5\xae\xbd\xe5\xa4\xa7\xe5\xb0\x8f\xe5\x87\x8f\xe5\x8d\x8a\n    layers.Conv2D(256, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\n    layers.Conv2D(256, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\n    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\'same\'),\n\n    # Conv-Conv-Pooling\xe5\x8d\x95\xe5\x85\x834,\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x8f\x90\xe5\x8d\x87\xe8\x87\xb3512\xef\xbc\x8c\xe9\xab\x98\xe5\xae\xbd\xe5\xa4\xa7\xe5\xb0\x8f\xe5\x87\x8f\xe5\x8d\x8a\n    layers.Conv2D(512, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\n    layers.Conv2D(512, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\n    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\'same\'),\n\n    # Conv-Conv-Pooling\xe5\x8d\x95\xe5\x85\x835,\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x8f\x90\xe5\x8d\x87\xe8\x87\xb3512\xef\xbc\x8c\xe9\xab\x98\xe5\xae\xbd\xe5\xa4\xa7\xe5\xb0\x8f\xe5\x87\x8f\xe5\x8d\x8a\n    layers.Conv2D(512, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\n    layers.Conv2D(512, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),\n    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\'same\')\n]\n# \xe5\x88\xa9\xe7\x94\xa8\xe5\x89\x8d\xe9\x9d\xa2\xe5\x88\x9b\xe5\xbb\xba\xe7\x9a\x84\xe5\xb1\x82\xe5\x88\x97\xe8\xa1\xa8\xe6\x9e\x84\xe5\xbb\xba\xe7\xbd\x91\xe7\xbb\x9c\xe5\xae\xb9\xe5\x99\xa8\nconv_net = Sequential(conv_layers)\n\n\n# %%\n# \xe5\x88\x9b\xe5\xbb\xba3\xe5\xb1\x82\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe5\xad\x90\xe7\xbd\x91\xe7\xbb\x9c\nfc_net = Sequential([\n    layers.Dense(256, activation=tf.nn.relu),\n    layers.Dense(128, activation=tf.nn.relu),\n    layers.Dense(100, activation=None),\n])\n\n\n# %%\n# build2\xe4\xb8\xaa\xe5\xad\x90\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe5\xb9\xb6\xe6\x89\x93\xe5\x8d\xb0\xe7\xbd\x91\xe7\xbb\x9c\xe5\x8f\x82\xe6\x95\xb0\xe4\xbf\xa1\xe6\x81\xaf\nconv_net.build(input_shape=[4, 32, 32, 3])\nfc_net.build(input_shape=[4, 512])\nconv_net.summary()\nfc_net.summary()\n\n\n# %%\n# \xe5\x88\x97\xe8\xa1\xa8\xe5\x90\x88\xe5\xb9\xb6\xef\xbc\x8c\xe5\x90\x88\xe5\xb9\xb62\xe4\xb8\xaa\xe5\xad\x90\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\nvariables = conv_net.trainable_variables + fc_net.trainable_variables\n# \xe5\xaf\xb9\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\x82\xe6\x95\xb0\xe6\xb1\x82\xe6\xa2\xaf\xe5\xba\xa6\ngrads = tape.gradient(loss, variables)\n# \xe8\x87\xaa\xe5\x8a\xa8\xe6\x9b\xb4\xe6\x96\xb0\noptimizer.apply_gradients(zip(grads, variables))\n\n\n# %%\nx = tf.random.normal([1,7,7,1]) # \xe6\xa8\xa1\xe6\x8b\x9f\xe8\xbe\x93\xe5\x85\xa5\n# \xe7\xa9\xba\xe6\xb4\x9e\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x8c1\xe4\xb8\xaa3x3\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\nlayer = layers.Conv2D(1,kernel_size=3,strides=1,dilation_rate=2)\nout = layer(x) # \xe5\x89\x8d\xe5\x90\x91\xe8\xae\xa1\xe7\xae\x97\nout.shape\n\n\n# %%\n# \xe5\x88\x9b\xe5\xbb\xbaX\xe7\x9f\xa9\xe9\x98\xb5\nx = tf.range(25)+1\n# Reshape\xe4\xb8\xba\xe5\x90\x88\xe6\xb3\x95\xe7\xbb\xb4\xe5\xba\xa6\xe7\x9a\x84\xe5\xbc\xa0\xe9\x87\x8f\nx = tf.reshape(x,[1,5,5,1])\nx = tf.cast(x, tf.float32)\n# \xe5\x88\x9b\xe5\xbb\xba\xe5\x9b\xba\xe5\xae\x9a\xe5\x86\x85\xe5\xae\xb9\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe7\x9f\xa9\xe9\x98\xb5\nw = tf.constant([[-1,2,-3.],[4,-5,6],[-7,8,-9]])\n# \xe8\xb0\x83\xe6\x95\xb4\xe4\xb8\xba\xe5\x90\x88\xe6\xb3\x95\xe7\xbb\xb4\xe5\xba\xa6\xe7\x9a\x84\xe5\xbc\xa0\xe9\x87\x8f\nw = tf.expand_dims(w,axis=2)\nw = tf.expand_dims(w,axis=3)\n# \xe8\xbf\x9b\xe8\xa1\x8c\xe6\x99\xae\xe9\x80\x9a\xe5\x8d\xb7\xe7\xa7\xaf\xe8\xbf\x90\xe7\xae\x97\nout = tf.nn.conv2d(x,w,strides=2,padding=\'VALID\')\nout\n#%%\n# \xe6\x99\xae\xe9\x80\x9a\xe5\x8d\xb7\xe7\xa7\xaf\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe4\xbd\x9c\xe4\xb8\xba\xe8\xbd\xac\xe7\xbd\xae\xe5\x8d\xb7\xe7\xa7\xaf\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8c\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xbd\xac\xe7\xbd\xae\xe5\x8d\xb7\xe7\xa7\xaf\xe8\xbf\x90\xe7\xae\x97\nxx = tf.nn.conv2d_transpose(out, w, strides=2, \n    padding=\'VALID\',\n    output_shape=[1,5,5,1])\n\n\n<tf.Tensor: id=117, shape=(5, 5), dtype=float32, numpy=\narray([[   67.,  -134.,   278.,  -154.,   231.],\n       [ -268.,   335.,  -710.,   385.,  -462.],\n       [  586.,  -770.,  1620.,  -870.,  1074.],\n       [ -468.,   585., -1210.,   635.,  -762.],\n       [  819.,  -936.,  1942., -1016.,  1143.]], dtype=float32)>\n\n\n# %%\nx = tf.random.normal([1,6,6,1])\n# 6x6\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe7\xbb\x8f\xe8\xbf\x87\xe6\x99\xae\xe9\x80\x9a\xe5\x8d\xb7\xe7\xa7\xaf\nout = tf.nn.conv2d(x,w,strides=2,padding=\'VALID\')\nout\n<tf.Tensor: id=21, shape=(1, 2, 2, 1), dtype=float32, numpy=\narray([[[[ 20.438847 ],\n         [ 19.160788 ]],\n\n        [[  0.8098897],\n         [-28.30303  ]]]], dtype=float32)>\n# %%\n# \xe6\x81\xa2\xe5\xa4\x8d\xe5\x87\xba6x6\xe5\xa4\xa7\xe5\xb0\x8f\nxx = tf.nn.conv2d_transpose(out, w, strides=2, \n    padding=\'VALID\',\n    output_shape=[1,6,6,1])\nxx\n\n\n\n# %%\n# \xe5\x88\x9b\xe5\xbb\xba\xe8\xbd\xac\xe7\xbd\xae\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xb1\xbb\nlayer = layers.Conv2DTranspose(1,kernel_size=3,strides=2,padding=\'VALID\')\nxx2 = layer(out)\nxx2\n# %%\nclass BasicBlock(layers.Layer):\n    # \xe6\xae\x8b\xe5\xb7\xae\xe6\xa8\xa1\xe5\x9d\x97\xe7\xb1\xbb\n    def __init__(self, filter_num, stride=1):\n        super(BasicBlock, self).__init__()\n        # f(x)\xe5\x8c\x85\xe5\x90\xab\xe4\xba\x862\xe4\xb8\xaa\xe6\x99\xae\xe9\x80\x9a\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xef\xbc\x8c\xe5\x88\x9b\xe5\xbb\xba\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x821\n        self.conv1 = layers.Conv2D(filter_num, (3, 3), strides=stride, padding=\'same\')\n        self.bn1 = layers.BatchNormalization()\n        self.relu = layers.Activation(\'relu\')\n        # \xe5\x88\x9b\xe5\xbb\xba\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x822\n        self.conv2 = layers.Conv2D(filter_num, (3, 3), strides=1, padding=\'same\')\n        self.bn2 = layers.BatchNormalization()\n\n        if stride != 1: # \xe6\x8f\x92\xe5\x85\xa5identity\xe5\xb1\x82\n            self.downsample = Sequential()\n            self.downsample.add(layers.Conv2D(filter_num, (1, 1), strides=stride))\n        else: # \xe5\x90\xa6\xe5\x88\x99\xef\xbc\x8c\xe7\x9b\xb4\xe6\x8e\xa5\xe8\xbf\x9e\xe6\x8e\xa5\n            self.downsample = lambda x:x\n\n\n    def call(self, inputs, training=None):\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe5\x87\xbd\xe6\x95\xb0\n        out = self.conv1(inputs) # \xe9\x80\x9a\xe8\xbf\x87\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out) # \xe9\x80\x9a\xe8\xbf\x87\xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\n        out = self.bn2(out)\n        # \xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe8\xbf\x87identity()\xe8\xbd\xac\xe6\x8d\xa2\n        identity = self.downsample(inputs)\n        # f(x)+x\xe8\xbf\x90\xe7\xae\x97\n        output = layers.add([out, identity])\n        # \xe5\x86\x8d\xe9\x80\x9a\xe8\xbf\x87\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe5\xb9\xb6\xe8\xbf\x94\xe5\x9b\x9e\n        output = tf.nn.relu(output)\n        return output\n'"
ch10-卷积神经网络/resnet.py,1,"b""import  tensorflow as tf\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers, Sequential\n\n\n\nclass BasicBlock(layers.Layer):\n    # \xe6\xae\x8b\xe5\xb7\xae\xe6\xa8\xa1\xe5\x9d\x97\n    def __init__(self, filter_num, stride=1):\n        super(BasicBlock, self).__init__()\n        # \xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x8d\x95\xe5\x85\x83\n        self.conv1 = layers.Conv2D(filter_num, (3, 3), strides=stride, padding='same')\n        self.bn1 = layers.BatchNormalization()\n        self.relu = layers.Activation('relu')\n        # \xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x8d\x95\xe5\x85\x83\n        self.conv2 = layers.Conv2D(filter_num, (3, 3), strides=1, padding='same')\n        self.bn2 = layers.BatchNormalization()\n\n        if stride != 1:# \xe9\x80\x9a\xe8\xbf\x871x1\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xae\x8c\xe6\x88\x90shape\xe5\x8c\xb9\xe9\x85\x8d\n            self.downsample = Sequential()\n            self.downsample.add(layers.Conv2D(filter_num, (1, 1), strides=stride))\n        else:# shape\xe5\x8c\xb9\xe9\x85\x8d\xef\xbc\x8c\xe7\x9b\xb4\xe6\x8e\xa5\xe7\x9f\xad\xe6\x8e\xa5\n            self.downsample = lambda x:x\n\n    def call(self, inputs, training=None):\n\n        # [b, h, w, c]\xef\xbc\x8c\xe9\x80\x9a\xe8\xbf\x87\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x8d\x95\xe5\x85\x83\n        out = self.conv1(inputs)\n        out = self.bn1(out)\n        out = self.relu(out)\n        # \xe9\x80\x9a\xe8\xbf\x87\xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x8d\x95\xe5\x85\x83\n        out = self.conv2(out)\n        out = self.bn2(out)\n        # \xe9\x80\x9a\xe8\xbf\x87identity\xe6\xa8\xa1\xe5\x9d\x97\n        identity = self.downsample(inputs)\n        # 2\xe6\x9d\xa1\xe8\xb7\xaf\xe5\xbe\x84\xe8\xbe\x93\xe5\x87\xba\xe7\x9b\xb4\xe6\x8e\xa5\xe7\x9b\xb8\xe5\x8a\xa0\n        output = layers.add([out, identity])\n        output = tf.nn.relu(output) # \xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\n\n        return output\n\n\nclass ResNet(keras.Model):\n    # \xe9\x80\x9a\xe7\x94\xa8\xe7\x9a\x84ResNet\xe5\xae\x9e\xe7\x8e\xb0\xe7\xb1\xbb\n    def __init__(self, layer_dims, num_classes=10): # [2, 2, 2, 2]\n        super(ResNet, self).__init__()\n        # \xe6\xa0\xb9\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\n        self.stem = Sequential([layers.Conv2D(64, (3, 3), strides=(1, 1)),\n                                layers.BatchNormalization(),\n                                layers.Activation('relu'),\n                                layers.MaxPool2D(pool_size=(2, 2), strides=(1, 1), padding='same')\n                                ])\n        # \xe5\xa0\x86\xe5\x8f\xa04\xe4\xb8\xaaBlock\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaablock\xe5\x8c\x85\xe5\x90\xab\xe4\xba\x86\xe5\xa4\x9a\xe4\xb8\xaaBasicBlock,\xe8\xae\xbe\xe7\xbd\xae\xe6\xad\xa5\xe9\x95\xbf\xe4\xb8\x8d\xe4\xb8\x80\xe6\xa0\xb7\n        self.layer1 = self.build_resblock(64,  layer_dims[0])\n        self.layer2 = self.build_resblock(128, layer_dims[1], stride=2)\n        self.layer3 = self.build_resblock(256, layer_dims[2], stride=2)\n        self.layer4 = self.build_resblock(512, layer_dims[3], stride=2)\n\n        # \xe9\x80\x9a\xe8\xbf\x87Pooling\xe5\xb1\x82\xe5\xb0\x86\xe9\xab\x98\xe5\xae\xbd\xe9\x99\x8d\xe4\xbd\x8e\xe4\xb8\xba1x1\n        self.avgpool = layers.GlobalAveragePooling2D()\n        # \xe6\x9c\x80\xe5\x90\x8e\xe8\xbf\x9e\xe6\x8e\xa5\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe5\x88\x86\xe7\xb1\xbb\n        self.fc = layers.Dense(num_classes)\n\n    def call(self, inputs, training=None):\n        # \xe9\x80\x9a\xe8\xbf\x87\xe6\xa0\xb9\xe7\xbd\x91\xe7\xbb\x9c\n        x = self.stem(inputs)\n        # \xe4\xb8\x80\xe6\xac\xa1\xe9\x80\x9a\xe8\xbf\x874\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9d\x97\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        # \xe9\x80\x9a\xe8\xbf\x87\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\n        x = self.avgpool(x)\n        # \xe9\x80\x9a\xe8\xbf\x87\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\n        x = self.fc(x)\n\n        return x\n\n\n\n    def build_resblock(self, filter_num, blocks, stride=1):\n        # \xe8\xbe\x85\xe5\x8a\xa9\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe5\xa0\x86\xe5\x8f\xa0filter_num\xe4\xb8\xaaBasicBlock\n        res_blocks = Sequential()\n        # \xe5\x8f\xaa\xe6\x9c\x89\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaaBasicBlock\xe7\x9a\x84\xe6\xad\xa5\xe9\x95\xbf\xe5\x8f\xaf\xe8\x83\xbd\xe4\xb8\x8d\xe4\xb8\xba1\xef\xbc\x8c\xe5\xae\x9e\xe7\x8e\xb0\xe4\xb8\x8b\xe9\x87\x87\xe6\xa0\xb7\n        res_blocks.add(BasicBlock(filter_num, stride))\n\n        for _ in range(1, blocks):#\xe5\x85\xb6\xe4\xbb\x96BasicBlock\xe6\xad\xa5\xe9\x95\xbf\xe9\x83\xbd\xe4\xb8\xba1\n            res_blocks.add(BasicBlock(filter_num, stride=1))\n\n        return res_blocks\n\n\ndef resnet18():\n    # \xe9\x80\x9a\xe8\xbf\x87\xe8\xb0\x83\xe6\x95\xb4\xe6\xa8\xa1\xe5\x9d\x97\xe5\x86\x85\xe9\x83\xa8BasicBlock\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\xe5\x92\x8c\xe9\x85\x8d\xe7\xbd\xae\xe5\xae\x9e\xe7\x8e\xb0\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84ResNet\n    return ResNet([2, 2, 2, 2])\n\n\ndef resnet34():\n     # \xe9\x80\x9a\xe8\xbf\x87\xe8\xb0\x83\xe6\x95\xb4\xe6\xa8\xa1\xe5\x9d\x97\xe5\x86\x85\xe9\x83\xa8BasicBlock\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\xe5\x92\x8c\xe9\x85\x8d\xe7\xbd\xae\xe5\xae\x9e\xe7\x8e\xb0\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84ResNet\n    return ResNet([3, 4, 6, 3])"""
ch10-卷积神经网络/resnet18_train.py,17,"b""import  tensorflow as tf\nfrom    tensorflow.keras import layers, optimizers, datasets, Sequential\nimport  os\nfrom    resnet import resnet18\n\nos.environ['TF_CPP_MIN_LOG_LEVEL']='2'\ntf.random.set_seed(2345)\n\n\n\n\n\ndef preprocess(x, y):\n    # \xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe6\x98\xa0\xe5\xb0\x84\xe5\x88\xb0-1~1\n    x = 2*tf.cast(x, dtype=tf.float32) / 255. - 1\n    y = tf.cast(y, dtype=tf.int32) # \xe7\xb1\xbb\xe5\x9e\x8b\xe8\xbd\xac\xe6\x8d\xa2\n    return x,y\n\n\n(x,y), (x_test, y_test) = datasets.cifar10.load_data() # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\ny = tf.squeeze(y, axis=1) # \xe5\x88\xa0\xe9\x99\xa4\xe4\xb8\x8d\xe5\xbf\x85\xe8\xa6\x81\xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\ny_test = tf.squeeze(y_test, axis=1) # \xe5\x88\xa0\xe9\x99\xa4\xe4\xb8\x8d\xe5\xbf\x85\xe8\xa6\x81\xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\nprint(x.shape, y.shape, x_test.shape, y_test.shape)\n\n\ntrain_db = tf.data.Dataset.from_tensor_slices((x,y)) # \xe6\x9e\x84\xe5\xbb\xba\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\n# \xe9\x9a\x8f\xe6\x9c\xba\xe6\x89\x93\xe6\x95\xa3\xef\xbc\x8c\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xef\xbc\x8c\xe6\x89\xb9\xe9\x87\x8f\xe5\x8c\x96\ntrain_db = train_db.shuffle(1000).map(preprocess).batch(512)\n\ntest_db = tf.data.Dataset.from_tensor_slices((x_test,y_test)) #\xe6\x9e\x84\xe5\xbb\xba\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\n# \xe9\x9a\x8f\xe6\x9c\xba\xe6\x89\x93\xe6\x95\xa3\xef\xbc\x8c\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xef\xbc\x8c\xe6\x89\xb9\xe9\x87\x8f\xe5\x8c\x96\ntest_db = test_db.map(preprocess).batch(512)\n# \xe9\x87\x87\xe6\xa0\xb7\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\nsample = next(iter(train_db))\nprint('sample:', sample[0].shape, sample[1].shape,\n      tf.reduce_min(sample[0]), tf.reduce_max(sample[0]))\n\n\ndef main():\n\n    # [b, 32, 32, 3] => [b, 1, 1, 512]\n    model = resnet18() # ResNet18\xe7\xbd\x91\xe7\xbb\x9c\n    model.build(input_shape=(None, 32, 32, 3))\n    model.summary() # \xe7\xbb\x9f\xe8\xae\xa1\xe7\xbd\x91\xe7\xbb\x9c\xe5\x8f\x82\xe6\x95\xb0\n    optimizer = optimizers.Adam(lr=1e-4) # \xe6\x9e\x84\xe5\xbb\xba\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\n\n    for epoch in range(100): # \xe8\xae\xad\xe7\xbb\x83epoch\n\n        for step, (x,y) in enumerate(train_db):\n\n            with tf.GradientTape() as tape:\n                # [b, 32, 32, 3] => [b, 10],\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n                logits = model(x)\n                # [b] => [b, 10],one-hot\xe7\xbc\x96\xe7\xa0\x81\n                y_onehot = tf.one_hot(y, depth=10)\n                # \xe8\xae\xa1\xe7\xae\x97\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\n                loss = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\n                loss = tf.reduce_mean(loss)\n            # \xe8\xae\xa1\xe7\xae\x97\xe6\xa2\xaf\xe5\xba\xa6\xe4\xbf\xa1\xe6\x81\xaf\n            grads = tape.gradient(loss, model.trainable_variables)\n            # \xe6\x9b\xb4\xe6\x96\xb0\xe7\xbd\x91\xe7\xbb\x9c\xe5\x8f\x82\xe6\x95\xb0\n            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n            if step %50 == 0:\n                print(epoch, step, 'loss:', float(loss))\n\n\n\n        total_num = 0\n        total_correct = 0\n        for x,y in test_db:\n\n            logits = model(x)\n            prob = tf.nn.softmax(logits, axis=1)\n            pred = tf.argmax(prob, axis=1)\n            pred = tf.cast(pred, dtype=tf.int32)\n\n            correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)\n            correct = tf.reduce_sum(correct)\n\n            total_num += x.shape[0]\n            total_correct += int(correct)\n\n        acc = total_correct / total_num\n        print(epoch, 'acc:', acc)\n\n\n\nif __name__ == '__main__':\n    main()\n"""
ch11-循环神经网络/nb.py,33,"b""#%%\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport matplotlib.pyplot as plt\n\n#%%\nx = tf.range(10)\nx = tf.random.shuffle(x)\n# \xe5\x88\x9b\xe5\xbb\xba\xe5\x85\xb110\xe4\xb8\xaa\xe5\x8d\x95\xe8\xaf\x8d\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x8d\x95\xe8\xaf\x8d\xe7\x94\xa8\xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\xba4\xe7\x9a\x84\xe5\x90\x91\xe9\x87\x8f\xe8\xa1\xa8\xe7\xa4\xba\xe7\x9a\x84\xe5\xb1\x82\nnet = layers.Embedding(10, 4)\nout = net(x)\n\nout\n#%%\nnet.embeddings\nnet.embeddings.trainable\nnet.trainable = False\n#%%\n# \xe4\xbb\x8e\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb8\xad\xe5\x8a\xa0\xe8\xbd\xbd\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe8\xa1\xa8\nembed_glove = load_embed('glove.6B.50d.txt')\n# \xe7\x9b\xb4\xe6\x8e\xa5\xe5\x88\xa9\xe7\x94\xa8\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe8\xa1\xa8\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96Embedding\xe5\xb1\x82\nnet.set_weights([embed_glove])\n#%%\ncell = layers.SimpleRNNCell(3)\ncell.build(input_shape=(None,4))\ncell.trainable_variables\n\n\n#%%\n# \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\x8a\xb6\xe6\x80\x81\xe5\x90\x91\xe9\x87\x8f\nh0 = [tf.zeros([4, 64])]\nx = tf.random.normal([4, 80, 100])\nxt = x[:,0,:]\n# \xe6\x9e\x84\xe5\xbb\xba\xe8\xbe\x93\xe5\x85\xa5\xe7\x89\xb9\xe5\xbe\x81f=100,\xe5\xba\x8f\xe5\x88\x97\xe9\x95\xbf\xe5\xba\xa6s=80,\xe7\x8a\xb6\xe6\x80\x81\xe9\x95\xbf\xe5\xba\xa6=64\xe7\x9a\x84Cell\ncell = layers.SimpleRNNCell(64)\nout, h1 = cell(xt, h0) # \xe5\x89\x8d\xe5\x90\x91\xe8\xae\xa1\xe7\xae\x97\nprint(out.shape, h1[0].shape)\nprint(id(out), id(h1[0])) \n\n\n#%%\nh = h0\n# \xe5\x9c\xa8\xe5\xba\x8f\xe5\x88\x97\xe9\x95\xbf\xe5\xba\xa6\xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\xe8\xa7\xa3\xe5\xbc\x80\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0xt:[b,f]\nfor xt in tf.unstack(x, axis=1):\n    out, h = cell(xt, h) # \xe5\x89\x8d\xe5\x90\x91\xe8\xae\xa1\xe7\xae\x97\n# \xe6\x9c\x80\xe7\xbb\x88\xe8\xbe\x93\xe5\x87\xba\xe5\x8f\xaf\xe4\xbb\xa5\xe8\x81\x9a\xe5\x90\x88\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x97\xb6\xe9\x97\xb4\xe6\x88\xb3\xe4\xb8\x8a\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x8c\xe4\xb9\x9f\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x8f\xaa\xe5\x8f\x96\xe6\x9c\x80\xe5\x90\x8e\xe6\x97\xb6\xe9\x97\xb4\xe6\x88\xb3\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\nout = out\n\n#%%\nx = tf.random.normal([4,80,100])\nxt = x[:,0,:] # \xe5\x8f\x96\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe6\x97\xb6\xe9\x97\xb4\xe6\x88\xb3\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5x0\n# \xe6\x9e\x84\xe5\xbb\xba2\xe4\xb8\xaaCell,\xe5\x85\x88cell0,\xe5\x90\x8ecell1\ncell0 = layers.SimpleRNNCell(64)\ncell1 = layers.SimpleRNNCell(64)\nh0 = [tf.zeros([4,64])] # cell0\xe7\x9a\x84\xe5\x88\x9d\xe5\xa7\x8b\xe7\x8a\xb6\xe6\x80\x81\xe5\x90\x91\xe9\x87\x8f\nh1 = [tf.zeros([4,64])] # cell1\xe7\x9a\x84\xe5\x88\x9d\xe5\xa7\x8b\xe7\x8a\xb6\xe6\x80\x81\xe5\x90\x91\xe9\x87\x8f\n\nout0, h0 = cell0(xt, h0)\nout1, h1 = cell1(out0, h1)\n\n\n#%%\nfor xt in tf.unstack(x, axis=1):\n    # xtw\xe4\xbd\x9c\xe4\xb8\xba\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe4\xb8\xbaout0\n    out0, h0 = cell0(xt, h0)\n    # \xe4\xb8\x8a\xe4\xb8\x80\xe4\xb8\xaacell\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xbaout0\xe4\xbd\x9c\xe4\xb8\xba\xe6\x9c\xaccell\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\n    out1, h1 = cell1(out0, h1)\n\n\n#%%\nprint(x.shape)\n# \xe4\xbf\x9d\xe5\xad\x98\xe4\xb8\x8a\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe6\x97\xb6\xe9\x97\xb4\xe6\x88\xb3\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\nmiddle_sequences = []\n# \xe8\xae\xa1\xe7\xae\x97\xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe6\x97\xb6\xe9\x97\xb4\xe6\x88\xb3\xe4\xb8\x8a\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x8c\xe5\xb9\xb6\xe4\xbf\x9d\xe5\xad\x98\nfor xt in tf.unstack(x, axis=1):\n    out0, h0 = cell0(xt, h0)\n    middle_sequences.append(out0)\n# \xe8\xae\xa1\xe7\xae\x97\xe7\xac\xac\xe4\xba\x8c\xe5\xb1\x82\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe6\x97\xb6\xe9\x97\xb4\xe6\x88\xb3\xe4\xb8\x8a\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\n# \xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe6\x98\xaf\xe6\x9c\xab\xe5\xb1\x82\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe4\xbf\x9d\xe5\xad\x98\xe6\x89\x80\xe6\x9c\x89\xe6\x97\xb6\xe9\x97\xb4\xe6\x88\xb3\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\nfor xt in middle_sequences:\n    out1, h1 = cell1(xt, h1)\n\n\n#%%\nlayer = layers.SimpleRNN(64)\nx = tf.random.normal([4, 80, 100])\nout = layer(x)\nout.shape\n\n#%%\nlayer = layers.SimpleRNN(64,return_sequences=True)\nout = layer(x) \nout\n\n#%%\nnet = keras.Sequential([ # \xe6\x9e\x84\xe5\xbb\xba2\xe5\xb1\x82RNN\xe7\xbd\x91\xe7\xbb\x9c\n# \xe9\x99\xa4\xe6\x9c\x80\xe6\x9c\xab\xe5\xb1\x82\xe5\xa4\x96\xef\xbc\x8c\xe9\x83\xbd\xe9\x9c\x80\xe8\xa6\x81\xe8\xbf\x94\xe5\x9b\x9e\xe6\x89\x80\xe6\x9c\x89\xe6\x97\xb6\xe9\x97\xb4\xe6\x88\xb3\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\nlayers.SimpleRNN(64, return_sequences=True),\nlayers.SimpleRNN(64),\n])\nout = net(x)\n\n\n\n#%%\nW = tf.ones([2,2]) # \xe4\xbb\xbb\xe6\x84\x8f\xe5\x88\x9b\xe5\xbb\xba\xe6\x9f\x90\xe7\x9f\xa9\xe9\x98\xb5\neigenvalues = tf.linalg.eigh(W)[0] # \xe8\xae\xa1\xe7\xae\x97\xe7\x89\xb9\xe5\xbe\x81\xe5\x80\xbc\neigenvalues\n#%%\nval = [W]\nfor i in range(10): # \xe7\x9f\xa9\xe9\x98\xb5\xe7\x9b\xb8\xe4\xb9\x98n\xe6\xac\xa1\xe6\x96\xb9\n    val.append([val[-1]@W])\n# \xe8\xae\xa1\xe7\xae\x97L2\xe8\x8c\x83\xe6\x95\xb0\nnorm = list(map(lambda x:tf.norm(x).numpy(),val))\nplt.plot(range(1,12),norm)\nplt.xlabel('n times')\nplt.ylabel('L2-norm')\nplt.savefig('w_n_times_1.svg')\n#%%\nW = tf.ones([2,2])*0.4 # \xe4\xbb\xbb\xe6\x84\x8f\xe5\x88\x9b\xe5\xbb\xba\xe6\x9f\x90\xe7\x9f\xa9\xe9\x98\xb5\neigenvalues = tf.linalg.eigh(W)[0] # \xe8\xae\xa1\xe7\xae\x97\xe7\x89\xb9\xe5\xbe\x81\xe5\x80\xbc\nprint(eigenvalues)\nval = [W]\nfor i in range(10):\n    val.append([val[-1]@W])\nnorm = list(map(lambda x:tf.norm(x).numpy(),val))\nplt.plot(range(1,12),norm)\nplt.xlabel('n times')\nplt.ylabel('L2-norm')\nplt.savefig('w_n_times_0.svg')\n#%%\na=tf.random.uniform([2,2])\ntf.clip_by_value(a,0.4,0.6) # \xe6\xa2\xaf\xe5\xba\xa6\xe5\x80\xbc\xe8\xa3\x81\xe5\x89\xaa\n\n#%%\n\n\n\n\n#%%\na=tf.random.uniform([2,2]) * 5\n# \xe6\x8c\x89\xe8\x8c\x83\xe6\x95\xb0\xe6\x96\xb9\xe5\xbc\x8f\xe8\xa3\x81\xe5\x89\xaa\nb = tf.clip_by_norm(a, 5)\ntf.norm(a),tf.norm(b)\n\n#%%\nw1=tf.random.normal([3,3]) # \xe5\x88\x9b\xe5\xbb\xba\xe6\xa2\xaf\xe5\xba\xa6\xe5\xbc\xa0\xe9\x87\x8f1\nw2=tf.random.normal([3,3]) # \xe5\x88\x9b\xe5\xbb\xba\xe6\xa2\xaf\xe5\xba\xa6\xe5\xbc\xa0\xe9\x87\x8f2\n# \xe8\xae\xa1\xe7\xae\x97global norm\nglobal_norm=tf.math.sqrt(tf.norm(w1)**2+tf.norm(w2)**2) \n# \xe6\xa0\xb9\xe6\x8d\xaeglobal norm\xe5\x92\x8cmax norm=2\xe8\xa3\x81\xe5\x89\xaa\n(ww1,ww2),global_norm=tf.clip_by_global_norm([w1,w2],2)\n# \xe8\xae\xa1\xe7\xae\x97\xe8\xa3\x81\xe5\x89\xaa\xe5\x90\x8e\xe7\x9a\x84\xe5\xbc\xa0\xe9\x87\x8f\xe7\xbb\x84\xe7\x9a\x84global norm\nglobal_norm2 = tf.math.sqrt(tf.norm(ww1)**2+tf.norm(ww2)**2)\nprint(global_norm, global_norm2)\n\n#%%\nwith tf.GradientTape() as tape:\n  logits = model(x) # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n  loss = criteon(y, logits) # \xe8\xaf\xaf\xe5\xb7\xae\xe8\xae\xa1\xe7\xae\x97\n# \xe8\xae\xa1\xe7\xae\x97\xe6\xa2\xaf\xe5\xba\xa6\xe5\x80\xbc\ngrads = tape.gradient(loss, model.trainable_variables)\ngrads, _ = tf.clip_by_global_norm(grads, 25) # \xe5\x85\xa8\xe5\xb1\x80\xe6\xa2\xaf\xe5\xba\xa6\xe8\xa3\x81\xe5\x89\xaa\n# \xe5\x88\xa9\xe7\x94\xa8\xe8\xa3\x81\xe5\x89\xaa\xe5\x90\x8e\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe5\xbc\xa0\xe9\x87\x8f\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\noptimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n#%%\nx = tf.random.normal([2,80,100])\nxt = x[:,0,:] # \xe5\xbe\x97\xe5\x88\xb0\xe4\xb8\x80\xe4\xb8\xaa\xe6\x97\xb6\xe9\x97\xb4\xe6\x88\xb3\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\ncell = layers.LSTMCell(64) # \xe5\x88\x9b\xe5\xbb\xbaCell\n# \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\x8a\xb6\xe6\x80\x81\xe5\x92\x8c\xe8\xbe\x93\xe5\x87\xbaList,[h,c]\nstate = [tf.zeros([2,64]),tf.zeros([2,64])]\nout, state = cell(xt, state) # \xe5\x89\x8d\xe5\x90\x91\xe8\xae\xa1\xe7\xae\x97\nid(out),id(state[0]),id(state[1])\n\n\n#%%\nnet = layers.LSTM(4)\nnet.build(input_shape=(None,5,3))\nnet.trainable_variables\n#%%\n\nnet = layers.GRU(4)\nnet.build(input_shape=(None,5,3))\nnet.trainable_variables\n\n#%%\n# \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\x8a\xb6\xe6\x80\x81\xe5\x90\x91\xe9\x87\x8f\nh = [tf.zeros([2,64])]\ncell = layers.GRUCell(64) # \xe6\x96\xb0\xe5\xbb\xbaGRU Cell\nfor xt in tf.unstack(x, axis=1):\n    out, h = cell(xt, h)\nout.shape\n\n\n#%%\n"""
ch11-循环神经网络/pretrained.py,0,"b""import os\nimport sys\nimport numpy as np\nimport tensorflow as tf \nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\n\nBASE_DIR = ''\nGLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\nTEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup')\nMAX_SEQUENCE_LENGTH = 1000\nMAX_NUM_WORDS = 20000\nEMBEDDING_DIM = 100\nVALIDATION_SPLIT = 0.2\n\n# first, build index mapping words in the embeddings set\n# to their embedding vector\n\nprint('Indexing word vectors.')\n\nembeddings_index = {}\nwith open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\nprint('Found %s word vectors.' % len(embeddings_index))\n\n# second, prepare text samples and their labels\nprint('Processing text dataset')\n\ntexts = []  # list of text samples\nlabels_index = {}  # dictionary mapping label name to numeric id\nlabels = []  # list of label ids\nfor name in sorted(os.listdir(TEXT_DATA_DIR)):\n    path = os.path.join(TEXT_DATA_DIR, name)\n    if os.path.isdir(path):\n        label_id = len(labels_index)\n        labels_index[name] = label_id\n        for fname in sorted(os.listdir(path)):\n            if fname.isdigit():\n                fpath = os.path.join(path, fname)\n                args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'}\n                with open(fpath, **args) as f:\n                    t = f.read()\n                    i = t.find('\\n\\n')  # skip header\n                    if 0 < i:\n                        t = t[i:]\n                    texts.append(t)\n                labels.append(label_id)\n\nprint('Found %s texts.' % len(texts))\n\n# finally, vectorize the text samples into a 2D integer tensor\ntokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\ndata = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n\nlabels = to_categorical(np.asarray(labels))\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)\n\n# split the data into a training set and a validation set\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\nnum_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n\nx_train = data[:-num_validation_samples]\ny_train = labels[:-num_validation_samples]\nx_val = data[-num_validation_samples:]\ny_val = labels[-num_validation_samples:]\n\nprint('Preparing embedding matrix.')\n\n# prepare embedding matrix\nnum_words = min(MAX_NUM_WORDS, len(word_index)) + 1\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    if i > MAX_NUM_WORDS:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n\n# load pre-trained word embeddings into an Embedding layer\n# note that we set trainable = False so as to keep the embeddings fixed\nembedding_layer = Embedding(num_words,\n                            EMBEDDING_DIM,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=False)\n\nprint('Training model.')\n\n# train a 1D convnet with global maxpooling\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\nx = Conv1D(128, 5, activation='relu')(embedded_sequences)\nx = MaxPooling1D(5)(x)\nx = Conv1D(128, 5, activation='relu')(x)\nx = MaxPooling1D(5)(x)\nx = Conv1D(128, 5, activation='relu')(x)\nx = GlobalMaxPooling1D()(x)\nx = Dense(128, activation='relu')(x)\npreds = Dense(len(labels_index), activation='softmax')(x)\n\nmodel = Model(sequence_input, preds)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['acc'])\n\nmodel.fit(x_train, y_train,\n          batch_size=128,\n          epochs=10,\n          validation_data=(x_val, y_val))"""
ch11-循环神经网络/sentiment_analysis_cell - GRU.py,9,"b'#%%\nimport  os\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers, losses, optimizers, Sequential\n\n\ntf.random.set_seed(22)\nnp.random.seed(22)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nassert tf.__version__.startswith(\'2.\')\n\nbatchsz = 128 # \xe6\x89\xb9\xe9\x87\x8f\xe5\xa4\xa7\xe5\xb0\x8f\ntotal_words = 10000 # \xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xe5\xa4\xa7\xe5\xb0\x8fN_vocab\nmax_review_len = 80 # \xe5\x8f\xa5\xe5\xad\x90\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6s\xef\xbc\x8c\xe5\xa4\xa7\xe4\xba\x8e\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90\xe9\x83\xa8\xe5\x88\x86\xe5\xb0\x86\xe6\x88\xaa\xe6\x96\xad\xef\xbc\x8c\xe5\xb0\x8f\xe4\xba\x8e\xe7\x9a\x84\xe5\xb0\x86\xe5\xa1\xab\xe5\x85\x85\nembedding_len = 100 # \xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe7\x89\xb9\xe5\xbe\x81\xe9\x95\xbf\xe5\xba\xa6f\n# \xe5\x8a\xa0\xe8\xbd\xbdIMDB\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe6\xad\xa4\xe5\xa4\x84\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x87\x87\xe7\x94\xa8\xe6\x95\xb0\xe5\xad\x97\xe7\xbc\x96\xe7\xa0\x81\xef\xbc\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe5\xad\x97\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\x95\xe8\xaf\x8d\n(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=total_words)\nprint(x_train.shape, len(x_train[0]), y_train.shape)\nprint(x_test.shape, len(x_test[0]), y_test.shape)\n#%%\nx_train[0]\n#%%\n# \xe6\x95\xb0\xe5\xad\x97\xe7\xbc\x96\xe7\xa0\x81\xe8\xa1\xa8\nword_index = keras.datasets.imdb.get_word_index()\n# for k,v in word_index.items():\n#     print(k,v)\n#%%\nword_index = {k:(v+3) for k,v in word_index.items()}\nword_index[""<PAD>""] = 0\nword_index[""<START>""] = 1\nword_index[""<UNK>""] = 2  # unknown\nword_index[""<UNUSED>""] = 3\n# \xe7\xbf\xbb\xe8\xbd\xac\xe7\xbc\x96\xe7\xa0\x81\xe8\xa1\xa8\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_review(text):\n    return \' \'.join([reverse_word_index.get(i, \'?\') for i in text])\n\ndecode_review(x_train[8])\n\n#%%\n\n# x_train:[b, 80]\n# x_test: [b, 80]\n# \xe6\x88\xaa\xe6\x96\xad\xe5\x92\x8c\xe5\xa1\xab\xe5\x85\x85\xe5\x8f\xa5\xe5\xad\x90\xef\xbc\x8c\xe4\xbd\xbf\xe5\xbe\x97\xe7\xad\x89\xe9\x95\xbf\xef\xbc\x8c\xe6\xad\xa4\xe5\xa4\x84\xe9\x95\xbf\xe5\x8f\xa5\xe5\xad\x90\xe4\xbf\x9d\xe7\x95\x99\xe5\x8f\xa5\xe5\xad\x90\xe5\x90\x8e\xe9\x9d\xa2\xe7\x9a\x84\xe9\x83\xa8\xe5\x88\x86\xef\xbc\x8c\xe7\x9f\xad\xe5\x8f\xa5\xe5\xad\x90\xe5\x9c\xa8\xe5\x89\x8d\xe9\x9d\xa2\xe5\xa1\xab\xe5\x85\x85\nx_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_len)\nx_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_len)\n# \xe6\x9e\x84\xe5\xbb\xba\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe6\x89\x93\xe6\x95\xa3\xef\xbc\x8c\xe6\x89\xb9\xe9\x87\x8f\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\xa2\xe6\x8e\x89\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\x8d\xe5\xa4\x9fbatchsz\xe7\x9a\x84batch\ndb_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ndb_train = db_train.shuffle(1000).batch(batchsz, drop_remainder=True)\ndb_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\ndb_test = db_test.batch(batchsz, drop_remainder=True)\nprint(\'x_train shape:\', x_train.shape, tf.reduce_max(y_train), tf.reduce_min(y_train))\nprint(\'x_test shape:\', x_test.shape)\n\n#%%\n\nclass MyRNN(keras.Model):\n    # Cell\xe6\x96\xb9\xe5\xbc\x8f\xe6\x9e\x84\xe5\xbb\xba\xe5\xa4\x9a\xe5\xb1\x82\xe7\xbd\x91\xe7\xbb\x9c\n    def __init__(self, units):\n        super(MyRNN, self).__init__()\n        # [b, 64]\xef\xbc\x8c\xe6\x9e\x84\xe5\xbb\xbaCell\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\x8a\xb6\xe6\x80\x81\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x8c\xe9\x87\x8d\xe5\xa4\x8d\xe4\xbd\xbf\xe7\x94\xa8\n        self.state0 = [tf.zeros([batchsz, units])]\n        self.state1 = [tf.zeros([batchsz, units])]\n        # \xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe7\xbc\x96\xe7\xa0\x81 [b, 80] => [b, 80, 100]\n        self.embedding = layers.Embedding(total_words, embedding_len,\n                                          input_length=max_review_len)\n        # \xe6\x9e\x84\xe5\xbb\xba2\xe4\xb8\xaaCell\n        self.rnn_cell0 = layers.GRUCell(units, dropout=0.5)\n        self.rnn_cell1 = layers.GRUCell(units, dropout=0.5)\n        # \xe6\x9e\x84\xe5\xbb\xba\xe5\x88\x86\xe7\xb1\xbb\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe5\xb0\x86CELL\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe7\x89\xb9\xe5\xbe\x81\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\xef\xbc\x8c2\xe5\x88\x86\xe7\xb1\xbb\n        # [b, 80, 100] => [b, 64] => [b, 1]\n        self.outlayer = Sequential([\n        \tlayers.Dense(units),\n        \tlayers.Dropout(rate=0.5),\n        \tlayers.ReLU(),\n        \tlayers.Dense(1)])\n\n    def call(self, inputs, training=None):\n        x = inputs # [b, 80]\n        # embedding: [b, 80] => [b, 80, 100]\n        x = self.embedding(x)\n        # rnn cell compute,[b, 80, 100] => [b, 64]\n        state0 = self.state0\n        state1 = self.state1\n        for word in tf.unstack(x, axis=1): # word: [b, 100] \n            out0, state0 = self.rnn_cell0(word, state0, training) \n            out1, state1 = self.rnn_cell1(out0, state1, training)\n        # \xe6\x9c\xab\xe5\xb1\x82\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbe\x93\xe5\x87\xba\xe4\xbd\x9c\xe4\xb8\xba\xe5\x88\x86\xe7\xb1\xbb\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5: [b, 64] => [b, 1]\n        x = self.outlayer(out1, training)\n        # p(y is pos|x)\n        prob = tf.sigmoid(x)\n\n        return prob\n\ndef main():\n    units = 64 # RNN\xe7\x8a\xb6\xe6\x80\x81\xe5\x90\x91\xe9\x87\x8f\xe9\x95\xbf\xe5\xba\xa6f\n    epochs = 50 # \xe8\xae\xad\xe7\xbb\x83epochs\n\n    model = MyRNN(units)\n    # \xe8\xa3\x85\xe9\x85\x8d\n    model.compile(optimizer = optimizers.RMSprop(0.001),\n                  loss = losses.BinaryCrossentropy(),\n                  metrics=[\'accuracy\'])\n    # \xe8\xae\xad\xe7\xbb\x83\xe5\x92\x8c\xe9\xaa\x8c\xe8\xaf\x81\n    model.fit(db_train, epochs=epochs, validation_data=db_test)\n    # \xe6\xb5\x8b\xe8\xaf\x95\n    model.evaluate(db_test)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
ch11-循环神经网络/sentiment_analysis_cell - LSTM.py,9,"b'#%%\nimport  os\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers, losses, optimizers, Sequential\n\n\ntf.random.set_seed(22)\nnp.random.seed(22)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nassert tf.__version__.startswith(\'2.\')\n\nbatchsz = 128 # \xe6\x89\xb9\xe9\x87\x8f\xe5\xa4\xa7\xe5\xb0\x8f\ntotal_words = 10000 # \xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xe5\xa4\xa7\xe5\xb0\x8fN_vocab\nmax_review_len = 80 # \xe5\x8f\xa5\xe5\xad\x90\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6s\xef\xbc\x8c\xe5\xa4\xa7\xe4\xba\x8e\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90\xe9\x83\xa8\xe5\x88\x86\xe5\xb0\x86\xe6\x88\xaa\xe6\x96\xad\xef\xbc\x8c\xe5\xb0\x8f\xe4\xba\x8e\xe7\x9a\x84\xe5\xb0\x86\xe5\xa1\xab\xe5\x85\x85\nembedding_len = 100 # \xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe7\x89\xb9\xe5\xbe\x81\xe9\x95\xbf\xe5\xba\xa6f\n# \xe5\x8a\xa0\xe8\xbd\xbdIMDB\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe6\xad\xa4\xe5\xa4\x84\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x87\x87\xe7\x94\xa8\xe6\x95\xb0\xe5\xad\x97\xe7\xbc\x96\xe7\xa0\x81\xef\xbc\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe5\xad\x97\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\x95\xe8\xaf\x8d\n(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=total_words)\nprint(x_train.shape, len(x_train[0]), y_train.shape)\nprint(x_test.shape, len(x_test[0]), y_test.shape)\n#%%\nx_train[0]\n#%%\n# \xe6\x95\xb0\xe5\xad\x97\xe7\xbc\x96\xe7\xa0\x81\xe8\xa1\xa8\nword_index = keras.datasets.imdb.get_word_index()\n# for k,v in word_index.items():\n#     print(k,v)\n#%%\nword_index = {k:(v+3) for k,v in word_index.items()}\nword_index[""<PAD>""] = 0\nword_index[""<START>""] = 1\nword_index[""<UNK>""] = 2  # unknown\nword_index[""<UNUSED>""] = 3\n# \xe7\xbf\xbb\xe8\xbd\xac\xe7\xbc\x96\xe7\xa0\x81\xe8\xa1\xa8\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_review(text):\n    return \' \'.join([reverse_word_index.get(i, \'?\') for i in text])\n\ndecode_review(x_train[8])\n\n#%%\n\n# x_train:[b, 80]\n# x_test: [b, 80]\n# \xe6\x88\xaa\xe6\x96\xad\xe5\x92\x8c\xe5\xa1\xab\xe5\x85\x85\xe5\x8f\xa5\xe5\xad\x90\xef\xbc\x8c\xe4\xbd\xbf\xe5\xbe\x97\xe7\xad\x89\xe9\x95\xbf\xef\xbc\x8c\xe6\xad\xa4\xe5\xa4\x84\xe9\x95\xbf\xe5\x8f\xa5\xe5\xad\x90\xe4\xbf\x9d\xe7\x95\x99\xe5\x8f\xa5\xe5\xad\x90\xe5\x90\x8e\xe9\x9d\xa2\xe7\x9a\x84\xe9\x83\xa8\xe5\x88\x86\xef\xbc\x8c\xe7\x9f\xad\xe5\x8f\xa5\xe5\xad\x90\xe5\x9c\xa8\xe5\x89\x8d\xe9\x9d\xa2\xe5\xa1\xab\xe5\x85\x85\nx_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_len)\nx_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_len)\n# \xe6\x9e\x84\xe5\xbb\xba\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe6\x89\x93\xe6\x95\xa3\xef\xbc\x8c\xe6\x89\xb9\xe9\x87\x8f\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\xa2\xe6\x8e\x89\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\x8d\xe5\xa4\x9fbatchsz\xe7\x9a\x84batch\ndb_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ndb_train = db_train.shuffle(1000).batch(batchsz, drop_remainder=True)\ndb_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\ndb_test = db_test.batch(batchsz, drop_remainder=True)\nprint(\'x_train shape:\', x_train.shape, tf.reduce_max(y_train), tf.reduce_min(y_train))\nprint(\'x_test shape:\', x_test.shape)\n\n#%%\n\nclass MyRNN(keras.Model):\n    # Cell\xe6\x96\xb9\xe5\xbc\x8f\xe6\x9e\x84\xe5\xbb\xba\xe5\xa4\x9a\xe5\xb1\x82\xe7\xbd\x91\xe7\xbb\x9c\n    def __init__(self, units):\n        super(MyRNN, self).__init__()\n        # [b, 64]\xef\xbc\x8c\xe6\x9e\x84\xe5\xbb\xbaCell\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\x8a\xb6\xe6\x80\x81\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x8c\xe9\x87\x8d\xe5\xa4\x8d\xe4\xbd\xbf\xe7\x94\xa8\n        self.state0 = [tf.zeros([batchsz, units]),tf.zeros([batchsz, units])]\n        self.state1 = [tf.zeros([batchsz, units]),tf.zeros([batchsz, units])]\n        # \xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe7\xbc\x96\xe7\xa0\x81 [b, 80] => [b, 80, 100]\n        self.embedding = layers.Embedding(total_words, embedding_len,\n                                          input_length=max_review_len)\n        # \xe6\x9e\x84\xe5\xbb\xba2\xe4\xb8\xaaCell\n        self.rnn_cell0 = layers.LSTMCell(units, dropout=0.5)\n        self.rnn_cell1 = layers.LSTMCell(units, dropout=0.5)\n        # \xe6\x9e\x84\xe5\xbb\xba\xe5\x88\x86\xe7\xb1\xbb\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe5\xb0\x86CELL\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe7\x89\xb9\xe5\xbe\x81\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\xef\xbc\x8c2\xe5\x88\x86\xe7\xb1\xbb\n        # [b, 80, 100] => [b, 64] => [b, 1]\n        self.outlayer = Sequential([\n        \tlayers.Dense(units),\n        \tlayers.Dropout(rate=0.5),\n        \tlayers.ReLU(),\n        \tlayers.Dense(1)])\n\n    def call(self, inputs, training=None):\n        x = inputs # [b, 80]\n        # embedding: [b, 80] => [b, 80, 100]\n        x = self.embedding(x)\n        # rnn cell compute,[b, 80, 100] => [b, 64]\n        state0 = self.state0\n        state1 = self.state1\n        for word in tf.unstack(x, axis=1): # word: [b, 100] \n            out0, state0 = self.rnn_cell0(word, state0, training) \n            out1, state1 = self.rnn_cell1(out0, state1, training)\n        # \xe6\x9c\xab\xe5\xb1\x82\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbe\x93\xe5\x87\xba\xe4\xbd\x9c\xe4\xb8\xba\xe5\x88\x86\xe7\xb1\xbb\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5: [b, 64] => [b, 1]\n        x = self.outlayer(out1,training)\n        # p(y is pos|x)\n        prob = tf.sigmoid(x)\n\n        return prob\n\ndef main():\n    units = 64 # RNN\xe7\x8a\xb6\xe6\x80\x81\xe5\x90\x91\xe9\x87\x8f\xe9\x95\xbf\xe5\xba\xa6f\n    epochs = 50 # \xe8\xae\xad\xe7\xbb\x83epochs\n\n    model = MyRNN(units)\n    # \xe8\xa3\x85\xe9\x85\x8d\n    model.compile(optimizer = optimizers.RMSprop(0.001),\n                  loss = losses.BinaryCrossentropy(),\n                  metrics=[\'accuracy\'])\n    # \xe8\xae\xad\xe7\xbb\x83\xe5\x92\x8c\xe9\xaa\x8c\xe8\xaf\x81\n    model.fit(db_train, epochs=epochs, validation_data=db_test)\n    # \xe6\xb5\x8b\xe8\xaf\x95\n    model.evaluate(db_test)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
ch11-循环神经网络/sentiment_analysis_cell.py,9,"b'#%%\nimport  os\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers, losses, optimizers, Sequential\n\n\ntf.random.set_seed(22)\nnp.random.seed(22)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nassert tf.__version__.startswith(\'2.\')\n\nbatchsz = 128 # \xe6\x89\xb9\xe9\x87\x8f\xe5\xa4\xa7\xe5\xb0\x8f\ntotal_words = 10000 # \xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xe5\xa4\xa7\xe5\xb0\x8fN_vocab\nmax_review_len = 80 # \xe5\x8f\xa5\xe5\xad\x90\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6s\xef\xbc\x8c\xe5\xa4\xa7\xe4\xba\x8e\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90\xe9\x83\xa8\xe5\x88\x86\xe5\xb0\x86\xe6\x88\xaa\xe6\x96\xad\xef\xbc\x8c\xe5\xb0\x8f\xe4\xba\x8e\xe7\x9a\x84\xe5\xb0\x86\xe5\xa1\xab\xe5\x85\x85\nembedding_len = 100 # \xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe7\x89\xb9\xe5\xbe\x81\xe9\x95\xbf\xe5\xba\xa6f\n# \xe5\x8a\xa0\xe8\xbd\xbdIMDB\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe6\xad\xa4\xe5\xa4\x84\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x87\x87\xe7\x94\xa8\xe6\x95\xb0\xe5\xad\x97\xe7\xbc\x96\xe7\xa0\x81\xef\xbc\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe5\xad\x97\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\x95\xe8\xaf\x8d\n(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=total_words)\nprint(x_train.shape, len(x_train[0]), y_train.shape)\nprint(x_test.shape, len(x_test[0]), y_test.shape)\n#%%\nx_train[0]\n#%%\n# \xe6\x95\xb0\xe5\xad\x97\xe7\xbc\x96\xe7\xa0\x81\xe8\xa1\xa8\nword_index = keras.datasets.imdb.get_word_index()\n# for k,v in word_index.items():\n#     print(k,v)\n#%%\nword_index = {k:(v+3) for k,v in word_index.items()}\nword_index[""<PAD>""] = 0\nword_index[""<START>""] = 1\nword_index[""<UNK>""] = 2  # unknown\nword_index[""<UNUSED>""] = 3\n# \xe7\xbf\xbb\xe8\xbd\xac\xe7\xbc\x96\xe7\xa0\x81\xe8\xa1\xa8\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_review(text):\n    return \' \'.join([reverse_word_index.get(i, \'?\') for i in text])\n\ndecode_review(x_train[8])\n\n#%%\n\n# x_train:[b, 80]\n# x_test: [b, 80]\n# \xe6\x88\xaa\xe6\x96\xad\xe5\x92\x8c\xe5\xa1\xab\xe5\x85\x85\xe5\x8f\xa5\xe5\xad\x90\xef\xbc\x8c\xe4\xbd\xbf\xe5\xbe\x97\xe7\xad\x89\xe9\x95\xbf\xef\xbc\x8c\xe6\xad\xa4\xe5\xa4\x84\xe9\x95\xbf\xe5\x8f\xa5\xe5\xad\x90\xe4\xbf\x9d\xe7\x95\x99\xe5\x8f\xa5\xe5\xad\x90\xe5\x90\x8e\xe9\x9d\xa2\xe7\x9a\x84\xe9\x83\xa8\xe5\x88\x86\xef\xbc\x8c\xe7\x9f\xad\xe5\x8f\xa5\xe5\xad\x90\xe5\x9c\xa8\xe5\x89\x8d\xe9\x9d\xa2\xe5\xa1\xab\xe5\x85\x85\nx_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_len)\nx_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_len)\n# \xe6\x9e\x84\xe5\xbb\xba\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe6\x89\x93\xe6\x95\xa3\xef\xbc\x8c\xe6\x89\xb9\xe9\x87\x8f\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\xa2\xe6\x8e\x89\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\x8d\xe5\xa4\x9fbatchsz\xe7\x9a\x84batch\ndb_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ndb_train = db_train.shuffle(1000).batch(batchsz, drop_remainder=True)\ndb_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\ndb_test = db_test.batch(batchsz, drop_remainder=True)\nprint(\'x_train shape:\', x_train.shape, tf.reduce_max(y_train), tf.reduce_min(y_train))\nprint(\'x_test shape:\', x_test.shape)\n\n#%%\n\nclass MyRNN(keras.Model):\n    # Cell\xe6\x96\xb9\xe5\xbc\x8f\xe6\x9e\x84\xe5\xbb\xba\xe5\xa4\x9a\xe5\xb1\x82\xe7\xbd\x91\xe7\xbb\x9c\n    def __init__(self, units):\n        super(MyRNN, self).__init__()\n        # [b, 64]\xef\xbc\x8c\xe6\x9e\x84\xe5\xbb\xbaCell\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\x8a\xb6\xe6\x80\x81\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x8c\xe9\x87\x8d\xe5\xa4\x8d\xe4\xbd\xbf\xe7\x94\xa8\n        self.state0 = [tf.zeros([batchsz, units])]\n        self.state1 = [tf.zeros([batchsz, units])]\n        # \xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe7\xbc\x96\xe7\xa0\x81 [b, 80] => [b, 80, 100]\n        self.embedding = layers.Embedding(total_words, embedding_len,\n                                          input_length=max_review_len)\n        # \xe6\x9e\x84\xe5\xbb\xba2\xe4\xb8\xaaCell\n        self.rnn_cell0 = layers.SimpleRNNCell(units, dropout=0.5)\n        self.rnn_cell1 = layers.SimpleRNNCell(units, dropout=0.5)\n        # \xe6\x9e\x84\xe5\xbb\xba\xe5\x88\x86\xe7\xb1\xbb\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe5\xb0\x86CELL\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe7\x89\xb9\xe5\xbe\x81\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\xef\xbc\x8c2\xe5\x88\x86\xe7\xb1\xbb\n        # [b, 80, 100] => [b, 64] => [b, 1]\n        self.outlayer = Sequential([\n        \tlayers.Dense(units),\n        \tlayers.Dropout(rate=0.5),\n        \tlayers.ReLU(),\n        \tlayers.Dense(1)])\n\n    def call(self, inputs, training=None):\n        x = inputs # [b, 80]\n        # embedding: [b, 80] => [b, 80, 100]\n        x = self.embedding(x)\n        # rnn cell compute,[b, 80, 100] => [b, 64]\n        state0 = self.state0\n        state1 = self.state1\n        for word in tf.unstack(x, axis=1): # word: [b, 100] \n            out0, state0 = self.rnn_cell0(word, state0, training) \n            out1, state1 = self.rnn_cell1(out0, state1, training)\n        # \xe6\x9c\xab\xe5\xb1\x82\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbe\x93\xe5\x87\xba\xe4\xbd\x9c\xe4\xb8\xba\xe5\x88\x86\xe7\xb1\xbb\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5: [b, 64] => [b, 1]\n        x = self.outlayer(out1, training)\n        # p(y is pos|x)\n        prob = tf.sigmoid(x)\n\n        return prob\n\ndef main():\n    units = 64 # RNN\xe7\x8a\xb6\xe6\x80\x81\xe5\x90\x91\xe9\x87\x8f\xe9\x95\xbf\xe5\xba\xa6f\n    epochs = 50 # \xe8\xae\xad\xe7\xbb\x83epochs\n\n    model = MyRNN(units)\n    # \xe8\xa3\x85\xe9\x85\x8d\n    model.compile(optimizer = optimizers.RMSprop(0.001),\n                  loss = losses.BinaryCrossentropy(),\n                  metrics=[\'accuracy\'])\n    # \xe8\xae\xad\xe7\xbb\x83\xe5\x92\x8c\xe9\xaa\x8c\xe8\xaf\x81\n    model.fit(db_train, epochs=epochs, validation_data=db_test)\n    # \xe6\xb5\x8b\xe8\xaf\x95\n    model.evaluate(db_test)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
ch11-循环神经网络/sentiment_analysis_layer - GRU.py,6,"b'#%%\nimport  os\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers, losses, optimizers, Sequential\n\n\ntf.random.set_seed(22)\nnp.random.seed(22)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nassert tf.__version__.startswith(\'2.\')\n\nbatchsz = 128 # \xe6\x89\xb9\xe9\x87\x8f\xe5\xa4\xa7\xe5\xb0\x8f\ntotal_words = 10000 # \xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xe5\xa4\xa7\xe5\xb0\x8fN_vocab\nmax_review_len = 80 # \xe5\x8f\xa5\xe5\xad\x90\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6s\xef\xbc\x8c\xe5\xa4\xa7\xe4\xba\x8e\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90\xe9\x83\xa8\xe5\x88\x86\xe5\xb0\x86\xe6\x88\xaa\xe6\x96\xad\xef\xbc\x8c\xe5\xb0\x8f\xe4\xba\x8e\xe7\x9a\x84\xe5\xb0\x86\xe5\xa1\xab\xe5\x85\x85\nembedding_len = 100 # \xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe7\x89\xb9\xe5\xbe\x81\xe9\x95\xbf\xe5\xba\xa6f\n# \xe5\x8a\xa0\xe8\xbd\xbdIMDB\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe6\xad\xa4\xe5\xa4\x84\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x87\x87\xe7\x94\xa8\xe6\x95\xb0\xe5\xad\x97\xe7\xbc\x96\xe7\xa0\x81\xef\xbc\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe5\xad\x97\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\x95\xe8\xaf\x8d\n(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=total_words)\nprint(x_train.shape, len(x_train[0]), y_train.shape)\nprint(x_test.shape, len(x_test[0]), y_test.shape)\n#%%\nx_train[0]\n#%%\n# \xe6\x95\xb0\xe5\xad\x97\xe7\xbc\x96\xe7\xa0\x81\xe8\xa1\xa8\nword_index = keras.datasets.imdb.get_word_index()\n# for k,v in word_index.items():\n#     print(k,v)\n#%%\nword_index = {k:(v+3) for k,v in word_index.items()}\nword_index[""<PAD>""] = 0\nword_index[""<START>""] = 1\nword_index[""<UNK>""] = 2  # unknown\nword_index[""<UNUSED>""] = 3\n# \xe7\xbf\xbb\xe8\xbd\xac\xe7\xbc\x96\xe7\xa0\x81\xe8\xa1\xa8\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_review(text):\n    return \' \'.join([reverse_word_index.get(i, \'?\') for i in text])\n\ndecode_review(x_train[8])\n\n#%%\n\n# x_train:[b, 80]\n# x_test: [b, 80]\n# \xe6\x88\xaa\xe6\x96\xad\xe5\x92\x8c\xe5\xa1\xab\xe5\x85\x85\xe5\x8f\xa5\xe5\xad\x90\xef\xbc\x8c\xe4\xbd\xbf\xe5\xbe\x97\xe7\xad\x89\xe9\x95\xbf\xef\xbc\x8c\xe6\xad\xa4\xe5\xa4\x84\xe9\x95\xbf\xe5\x8f\xa5\xe5\xad\x90\xe4\xbf\x9d\xe7\x95\x99\xe5\x8f\xa5\xe5\xad\x90\xe5\x90\x8e\xe9\x9d\xa2\xe7\x9a\x84\xe9\x83\xa8\xe5\x88\x86\xef\xbc\x8c\xe7\x9f\xad\xe5\x8f\xa5\xe5\xad\x90\xe5\x9c\xa8\xe5\x89\x8d\xe9\x9d\xa2\xe5\xa1\xab\xe5\x85\x85\nx_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_len)\nx_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_len)\n# \xe6\x9e\x84\xe5\xbb\xba\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe6\x89\x93\xe6\x95\xa3\xef\xbc\x8c\xe6\x89\xb9\xe9\x87\x8f\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\xa2\xe6\x8e\x89\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\x8d\xe5\xa4\x9fbatchsz\xe7\x9a\x84batch\ndb_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ndb_train = db_train.shuffle(1000).batch(batchsz, drop_remainder=True)\ndb_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\ndb_test = db_test.batch(batchsz, drop_remainder=True)\nprint(\'x_train shape:\', x_train.shape, tf.reduce_max(y_train), tf.reduce_min(y_train))\nprint(\'x_test shape:\', x_test.shape)\n\n#%%\n\nclass MyRNN(keras.Model):\n    # Cell\xe6\x96\xb9\xe5\xbc\x8f\xe6\x9e\x84\xe5\xbb\xba\xe5\xa4\x9a\xe5\xb1\x82\xe7\xbd\x91\xe7\xbb\x9c\n    def __init__(self, units):\n        super(MyRNN, self).__init__() \n        # \xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe7\xbc\x96\xe7\xa0\x81 [b, 80] => [b, 80, 100]\n        self.embedding = layers.Embedding(total_words, embedding_len,\n                                          input_length=max_review_len)\n        # \xe6\x9e\x84\xe5\xbb\xbaRNN\n        self.rnn = keras.Sequential([\n            layers.GRU(units, dropout=0.5, return_sequences=True),\n            layers.GRU(units, dropout=0.5)\n        ])\n        # \xe6\x9e\x84\xe5\xbb\xba\xe5\x88\x86\xe7\xb1\xbb\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe5\xb0\x86CELL\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe7\x89\xb9\xe5\xbe\x81\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\xef\xbc\x8c2\xe5\x88\x86\xe7\xb1\xbb\n        # [b, 80, 100] => [b, 64] => [b, 1]\n        self.outlayer = Sequential([\n        \tlayers.Dense(32),\n        \tlayers.Dropout(rate=0.5),\n        \tlayers.ReLU(),\n        \tlayers.Dense(1)])\n\n    def call(self, inputs, training=None):\n        x = inputs # [b, 80]\n        # embedding: [b, 80] => [b, 80, 100]\n        x = self.embedding(x)\n        # rnn cell compute,[b, 80, 100] => [b, 64]\n        x = self.rnn(x)\n        # \xe6\x9c\xab\xe5\xb1\x82\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbe\x93\xe5\x87\xba\xe4\xbd\x9c\xe4\xb8\xba\xe5\x88\x86\xe7\xb1\xbb\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5: [b, 64] => [b, 1]\n        x = self.outlayer(x,training)\n        # p(y is pos|x)\n        prob = tf.sigmoid(x)\n\n        return prob\n\ndef main():\n    units = 32 # RNN\xe7\x8a\xb6\xe6\x80\x81\xe5\x90\x91\xe9\x87\x8f\xe9\x95\xbf\xe5\xba\xa6f\n    epochs = 50 # \xe8\xae\xad\xe7\xbb\x83epochs\n\n    model = MyRNN(units)\n    # \xe8\xa3\x85\xe9\x85\x8d\n    model.compile(optimizer = optimizers.Adam(0.001),\n                  loss = losses.BinaryCrossentropy(),\n                  metrics=[\'accuracy\'])\n    # \xe8\xae\xad\xe7\xbb\x83\xe5\x92\x8c\xe9\xaa\x8c\xe8\xaf\x81\n    model.fit(db_train, epochs=epochs, validation_data=db_test)\n    # \xe6\xb5\x8b\xe8\xaf\x95\n    model.evaluate(db_test)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
ch11-循环神经网络/sentiment_analysis_layer - LSTM - pretrained.py,6,"b'#%%\nimport  os\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers, losses, optimizers, Sequential\n\n\ntf.random.set_seed(22)\nnp.random.seed(22)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nassert tf.__version__.startswith(\'2.\')\n\nbatchsz = 128 # \xe6\x89\xb9\xe9\x87\x8f\xe5\xa4\xa7\xe5\xb0\x8f\ntotal_words = 10000 # \xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xe5\xa4\xa7\xe5\xb0\x8fN_vocab\nmax_review_len = 80 # \xe5\x8f\xa5\xe5\xad\x90\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6s\xef\xbc\x8c\xe5\xa4\xa7\xe4\xba\x8e\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90\xe9\x83\xa8\xe5\x88\x86\xe5\xb0\x86\xe6\x88\xaa\xe6\x96\xad\xef\xbc\x8c\xe5\xb0\x8f\xe4\xba\x8e\xe7\x9a\x84\xe5\xb0\x86\xe5\xa1\xab\xe5\x85\x85\nembedding_len = 100 # \xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe7\x89\xb9\xe5\xbe\x81\xe9\x95\xbf\xe5\xba\xa6f\n# \xe5\x8a\xa0\xe8\xbd\xbdIMDB\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe6\xad\xa4\xe5\xa4\x84\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x87\x87\xe7\x94\xa8\xe6\x95\xb0\xe5\xad\x97\xe7\xbc\x96\xe7\xa0\x81\xef\xbc\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe5\xad\x97\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\x95\xe8\xaf\x8d\n(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=total_words)\nprint(x_train.shape, len(x_train[0]), y_train.shape)\nprint(x_test.shape, len(x_test[0]), y_test.shape)\n#%%\nx_train[0]\n#%%\n# \xe6\x95\xb0\xe5\xad\x97\xe7\xbc\x96\xe7\xa0\x81\xe8\xa1\xa8\nword_index = keras.datasets.imdb.get_word_index()\n# for k,v in word_index.items():\n#     print(k,v)\n#%%\nword_index = {k:(v+3) for k,v in word_index.items()}\nword_index[""<PAD>""] = 0\nword_index[""<START>""] = 1\nword_index[""<UNK>""] = 2  # unknown\nword_index[""<UNUSED>""] = 3\n# \xe7\xbf\xbb\xe8\xbd\xac\xe7\xbc\x96\xe7\xa0\x81\xe8\xa1\xa8\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_review(text):\n    return \' \'.join([reverse_word_index.get(i, \'?\') for i in text])\n\ndecode_review(x_train[8])\n\n#%%\nprint(\'Indexing word vectors.\')\nembeddings_index = {}\nGLOVE_DIR = r\'C:\\Users\\z390\\Downloads\\glove6b50dtxt\'\nwith open(os.path.join(GLOVE_DIR, \'glove.6B.100d.txt\'),encoding=\'utf-8\') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype=\'float32\')\n        embeddings_index[word] = coefs\n\nprint(\'Found %s word vectors.\' % len(embeddings_index))\n#%%\nlen(embeddings_index.keys())\nlen(word_index.keys())\n#%%\nMAX_NUM_WORDS = total_words\n# prepare embedding matrix\nnum_words = min(MAX_NUM_WORDS, len(word_index))\nembedding_matrix = np.zeros((num_words, embedding_len))\napplied_vec_count = 0\nfor word, i in word_index.items():\n    if i >= MAX_NUM_WORDS:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    # print(word,embedding_vector)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n        applied_vec_count += 1\nprint(applied_vec_count, embedding_matrix.shape)\n\n#%%\n# x_train:[b, 80]\n# x_test: [b, 80]\n# \xe6\x88\xaa\xe6\x96\xad\xe5\x92\x8c\xe5\xa1\xab\xe5\x85\x85\xe5\x8f\xa5\xe5\xad\x90\xef\xbc\x8c\xe4\xbd\xbf\xe5\xbe\x97\xe7\xad\x89\xe9\x95\xbf\xef\xbc\x8c\xe6\xad\xa4\xe5\xa4\x84\xe9\x95\xbf\xe5\x8f\xa5\xe5\xad\x90\xe4\xbf\x9d\xe7\x95\x99\xe5\x8f\xa5\xe5\xad\x90\xe5\x90\x8e\xe9\x9d\xa2\xe7\x9a\x84\xe9\x83\xa8\xe5\x88\x86\xef\xbc\x8c\xe7\x9f\xad\xe5\x8f\xa5\xe5\xad\x90\xe5\x9c\xa8\xe5\x89\x8d\xe9\x9d\xa2\xe5\xa1\xab\xe5\x85\x85\nx_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_len)\nx_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_len)\n# \xe6\x9e\x84\xe5\xbb\xba\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe6\x89\x93\xe6\x95\xa3\xef\xbc\x8c\xe6\x89\xb9\xe9\x87\x8f\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\xa2\xe6\x8e\x89\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\x8d\xe5\xa4\x9fbatchsz\xe7\x9a\x84batch\ndb_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ndb_train = db_train.shuffle(1000).batch(batchsz, drop_remainder=True)\ndb_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\ndb_test = db_test.batch(batchsz, drop_remainder=True)\nprint(\'x_train shape:\', x_train.shape, tf.reduce_max(y_train), tf.reduce_min(y_train))\nprint(\'x_test shape:\', x_test.shape)\n\n#%%\n\nclass MyRNN(keras.Model):\n    # Cell\xe6\x96\xb9\xe5\xbc\x8f\xe6\x9e\x84\xe5\xbb\xba\xe5\xa4\x9a\xe5\xb1\x82\xe7\xbd\x91\xe7\xbb\x9c\n    def __init__(self, units):\n        super(MyRNN, self).__init__() \n        # \xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe7\xbc\x96\xe7\xa0\x81 [b, 80] => [b, 80, 100]\n        self.embedding = layers.Embedding(total_words, embedding_len,\n                                          input_length=max_review_len,\n                                          trainable=False)\n        self.embedding.build(input_shape=(None,max_review_len))\n        # self.embedding.set_weights([embedding_matrix])\n        # \xe6\x9e\x84\xe5\xbb\xbaRNN\n        self.rnn = keras.Sequential([\n            layers.LSTM(units, dropout=0.5, return_sequences=True),\n            layers.LSTM(units, dropout=0.5)\n        ])\n        # \xe6\x9e\x84\xe5\xbb\xba\xe5\x88\x86\xe7\xb1\xbb\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe5\xb0\x86CELL\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe7\x89\xb9\xe5\xbe\x81\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\xef\xbc\x8c2\xe5\x88\x86\xe7\xb1\xbb\n        # [b, 80, 100] => [b, 64] => [b, 1]\n        self.outlayer = Sequential([\n            layers.Dense(32),\n            layers.Dropout(rate=0.5),\n            layers.ReLU(),\n            layers.Dense(1)])\n\n    def call(self, inputs, training=None):\n        x = inputs # [b, 80]\n        # embedding: [b, 80] => [b, 80, 100]\n        x = self.embedding(x)\n        # rnn cell compute,[b, 80, 100] => [b, 64]\n        x = self.rnn(x)\n        # \xe6\x9c\xab\xe5\xb1\x82\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbe\x93\xe5\x87\xba\xe4\xbd\x9c\xe4\xb8\xba\xe5\x88\x86\xe7\xb1\xbb\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5: [b, 64] => [b, 1]\n        x = self.outlayer(x,training)\n        # p(y is pos|x)\n        prob = tf.sigmoid(x)\n\n        return prob\n\ndef main():\n    units = 512 # RNN\xe7\x8a\xb6\xe6\x80\x81\xe5\x90\x91\xe9\x87\x8f\xe9\x95\xbf\xe5\xba\xa6f\n    epochs = 50 # \xe8\xae\xad\xe7\xbb\x83epochs\n\n    model = MyRNN(units)\n    # \xe8\xa3\x85\xe9\x85\x8d\n    model.compile(optimizer = optimizers.Adam(0.001),\n                  loss = losses.BinaryCrossentropy(),\n                  metrics=[\'accuracy\'])\n    # \xe8\xae\xad\xe7\xbb\x83\xe5\x92\x8c\xe9\xaa\x8c\xe8\xaf\x81\n    model.fit(db_train, epochs=epochs, validation_data=db_test)\n    # \xe6\xb5\x8b\xe8\xaf\x95\n    model.evaluate(db_test)\n\n\nif __name__ == \'__main__\':\n    main()\n\n\n#%%\n'"
ch11-循环神经网络/sentiment_analysis_layer - LSTM.py,6,"b'#%%\nimport  os\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers, losses, optimizers, Sequential\n\n\ntf.random.set_seed(22)\nnp.random.seed(22)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nassert tf.__version__.startswith(\'2.\')\n\nbatchsz = 128 # \xe6\x89\xb9\xe9\x87\x8f\xe5\xa4\xa7\xe5\xb0\x8f\ntotal_words = 10000 # \xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xe5\xa4\xa7\xe5\xb0\x8fN_vocab\nmax_review_len = 80 # \xe5\x8f\xa5\xe5\xad\x90\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6s\xef\xbc\x8c\xe5\xa4\xa7\xe4\xba\x8e\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90\xe9\x83\xa8\xe5\x88\x86\xe5\xb0\x86\xe6\x88\xaa\xe6\x96\xad\xef\xbc\x8c\xe5\xb0\x8f\xe4\xba\x8e\xe7\x9a\x84\xe5\xb0\x86\xe5\xa1\xab\xe5\x85\x85\nembedding_len = 100 # \xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe7\x89\xb9\xe5\xbe\x81\xe9\x95\xbf\xe5\xba\xa6f\n# \xe5\x8a\xa0\xe8\xbd\xbdIMDB\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe6\xad\xa4\xe5\xa4\x84\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x87\x87\xe7\x94\xa8\xe6\x95\xb0\xe5\xad\x97\xe7\xbc\x96\xe7\xa0\x81\xef\xbc\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe5\xad\x97\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\x95\xe8\xaf\x8d\n(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=total_words)\nprint(x_train.shape, len(x_train[0]), y_train.shape)\nprint(x_test.shape, len(x_test[0]), y_test.shape)\n#%%\nx_train[0]\n#%%\n# \xe6\x95\xb0\xe5\xad\x97\xe7\xbc\x96\xe7\xa0\x81\xe8\xa1\xa8\nword_index = keras.datasets.imdb.get_word_index()\n# for k,v in word_index.items():\n#     print(k,v)\n#%%\nword_index = {k:(v+3) for k,v in word_index.items()}\nword_index[""<PAD>""] = 0\nword_index[""<START>""] = 1\nword_index[""<UNK>""] = 2  # unknown\nword_index[""<UNUSED>""] = 3\n# \xe7\xbf\xbb\xe8\xbd\xac\xe7\xbc\x96\xe7\xa0\x81\xe8\xa1\xa8\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_review(text):\n    return \' \'.join([reverse_word_index.get(i, \'?\') for i in text])\n\ndecode_review(x_train[8])\n\n#%%\n\n# x_train:[b, 80]\n# x_test: [b, 80]\n# \xe6\x88\xaa\xe6\x96\xad\xe5\x92\x8c\xe5\xa1\xab\xe5\x85\x85\xe5\x8f\xa5\xe5\xad\x90\xef\xbc\x8c\xe4\xbd\xbf\xe5\xbe\x97\xe7\xad\x89\xe9\x95\xbf\xef\xbc\x8c\xe6\xad\xa4\xe5\xa4\x84\xe9\x95\xbf\xe5\x8f\xa5\xe5\xad\x90\xe4\xbf\x9d\xe7\x95\x99\xe5\x8f\xa5\xe5\xad\x90\xe5\x90\x8e\xe9\x9d\xa2\xe7\x9a\x84\xe9\x83\xa8\xe5\x88\x86\xef\xbc\x8c\xe7\x9f\xad\xe5\x8f\xa5\xe5\xad\x90\xe5\x9c\xa8\xe5\x89\x8d\xe9\x9d\xa2\xe5\xa1\xab\xe5\x85\x85\nx_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_len)\nx_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_len)\n# \xe6\x9e\x84\xe5\xbb\xba\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe6\x89\x93\xe6\x95\xa3\xef\xbc\x8c\xe6\x89\xb9\xe9\x87\x8f\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\xa2\xe6\x8e\x89\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\x8d\xe5\xa4\x9fbatchsz\xe7\x9a\x84batch\ndb_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ndb_train = db_train.shuffle(1000).batch(batchsz, drop_remainder=True)\ndb_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\ndb_test = db_test.batch(batchsz, drop_remainder=True)\nprint(\'x_train shape:\', x_train.shape, tf.reduce_max(y_train), tf.reduce_min(y_train))\nprint(\'x_test shape:\', x_test.shape)\n\n#%%\n\nclass MyRNN(keras.Model):\n    # Cell\xe6\x96\xb9\xe5\xbc\x8f\xe6\x9e\x84\xe5\xbb\xba\xe5\xa4\x9a\xe5\xb1\x82\xe7\xbd\x91\xe7\xbb\x9c\n    def __init__(self, units):\n        super(MyRNN, self).__init__() \n        # \xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe7\xbc\x96\xe7\xa0\x81 [b, 80] => [b, 80, 100]\n        self.embedding = layers.Embedding(total_words, embedding_len,\n                                          input_length=max_review_len)\n        # \xe6\x9e\x84\xe5\xbb\xbaRNN\n        self.rnn = keras.Sequential([\n            layers.LSTM(units, dropout=0.5, return_sequences=True),\n            layers.LSTM(units, dropout=0.5)\n        ])\n        # \xe6\x9e\x84\xe5\xbb\xba\xe5\x88\x86\xe7\xb1\xbb\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe5\xb0\x86CELL\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe7\x89\xb9\xe5\xbe\x81\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\xef\xbc\x8c2\xe5\x88\x86\xe7\xb1\xbb\n        # [b, 80, 100] => [b, 64] => [b, 1]\n        self.outlayer = Sequential([\n        \tlayers.Dense(32),\n        \tlayers.Dropout(rate=0.5),\n        \tlayers.ReLU(),\n        \tlayers.Dense(1)])\n\n    def call(self, inputs, training=None):\n        x = inputs # [b, 80]\n        # embedding: [b, 80] => [b, 80, 100]\n        x = self.embedding(x)\n        # rnn cell compute,[b, 80, 100] => [b, 64]\n        x = self.rnn(x)\n        # \xe6\x9c\xab\xe5\xb1\x82\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbe\x93\xe5\x87\xba\xe4\xbd\x9c\xe4\xb8\xba\xe5\x88\x86\xe7\xb1\xbb\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5: [b, 64] => [b, 1]\n        x = self.outlayer(x,training)\n        # p(y is pos|x)\n        prob = tf.sigmoid(x)\n\n        return prob\n\ndef main():\n    units = 32 # RNN\xe7\x8a\xb6\xe6\x80\x81\xe5\x90\x91\xe9\x87\x8f\xe9\x95\xbf\xe5\xba\xa6f\n    epochs = 50 # \xe8\xae\xad\xe7\xbb\x83epochs\n\n    model = MyRNN(units)\n    # \xe8\xa3\x85\xe9\x85\x8d\n    model.compile(optimizer = optimizers.Adam(0.001),\n                  loss = losses.BinaryCrossentropy(),\n                  metrics=[\'accuracy\'])\n    # \xe8\xae\xad\xe7\xbb\x83\xe5\x92\x8c\xe9\xaa\x8c\xe8\xaf\x81\n    model.fit(db_train, epochs=epochs, validation_data=db_test)\n    # \xe6\xb5\x8b\xe8\xaf\x95\n    model.evaluate(db_test)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
ch11-循环神经网络/sentiment_analysis_layer.py,6,"b'#%%\nimport  os\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers, losses, optimizers, Sequential\n\n\ntf.random.set_seed(22)\nnp.random.seed(22)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nassert tf.__version__.startswith(\'2.\')\n\nbatchsz = 512 # \xe6\x89\xb9\xe9\x87\x8f\xe5\xa4\xa7\xe5\xb0\x8f\ntotal_words = 10000 # \xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xe5\xa4\xa7\xe5\xb0\x8fN_vocab\nmax_review_len = 80 # \xe5\x8f\xa5\xe5\xad\x90\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6s\xef\xbc\x8c\xe5\xa4\xa7\xe4\xba\x8e\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90\xe9\x83\xa8\xe5\x88\x86\xe5\xb0\x86\xe6\x88\xaa\xe6\x96\xad\xef\xbc\x8c\xe5\xb0\x8f\xe4\xba\x8e\xe7\x9a\x84\xe5\xb0\x86\xe5\xa1\xab\xe5\x85\x85\nembedding_len = 100 # \xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe7\x89\xb9\xe5\xbe\x81\xe9\x95\xbf\xe5\xba\xa6f\n# \xe5\x8a\xa0\xe8\xbd\xbdIMDB\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe6\xad\xa4\xe5\xa4\x84\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x87\x87\xe7\x94\xa8\xe6\x95\xb0\xe5\xad\x97\xe7\xbc\x96\xe7\xa0\x81\xef\xbc\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe5\xad\x97\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\x95\xe8\xaf\x8d\n(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=total_words)\nprint(x_train.shape, len(x_train[0]), y_train.shape)\nprint(x_test.shape, len(x_test[0]), y_test.shape)\n#%%\nx_train[0]\n#%%\n# \xe6\x95\xb0\xe5\xad\x97\xe7\xbc\x96\xe7\xa0\x81\xe8\xa1\xa8\nword_index = keras.datasets.imdb.get_word_index()\n# for k,v in word_index.items():\n#     print(k,v)\n#%%\nword_index = {k:(v+3) for k,v in word_index.items()}\nword_index[""<PAD>""] = 0\nword_index[""<START>""] = 1\nword_index[""<UNK>""] = 2  # unknown\nword_index[""<UNUSED>""] = 3\n# \xe7\xbf\xbb\xe8\xbd\xac\xe7\xbc\x96\xe7\xa0\x81\xe8\xa1\xa8\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_review(text):\n    return \' \'.join([reverse_word_index.get(i, \'?\') for i in text])\n\ndecode_review(x_train[8])\n\n#%%\n\n# x_train:[b, 80]\n# x_test: [b, 80]\n# \xe6\x88\xaa\xe6\x96\xad\xe5\x92\x8c\xe5\xa1\xab\xe5\x85\x85\xe5\x8f\xa5\xe5\xad\x90\xef\xbc\x8c\xe4\xbd\xbf\xe5\xbe\x97\xe7\xad\x89\xe9\x95\xbf\xef\xbc\x8c\xe6\xad\xa4\xe5\xa4\x84\xe9\x95\xbf\xe5\x8f\xa5\xe5\xad\x90\xe4\xbf\x9d\xe7\x95\x99\xe5\x8f\xa5\xe5\xad\x90\xe5\x90\x8e\xe9\x9d\xa2\xe7\x9a\x84\xe9\x83\xa8\xe5\x88\x86\xef\xbc\x8c\xe7\x9f\xad\xe5\x8f\xa5\xe5\xad\x90\xe5\x9c\xa8\xe5\x89\x8d\xe9\x9d\xa2\xe5\xa1\xab\xe5\x85\x85\nx_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_review_len)\nx_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_review_len)\n# \xe6\x9e\x84\xe5\xbb\xba\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe6\x89\x93\xe6\x95\xa3\xef\xbc\x8c\xe6\x89\xb9\xe9\x87\x8f\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\xa2\xe6\x8e\x89\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\x8d\xe5\xa4\x9fbatchsz\xe7\x9a\x84batch\ndb_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ndb_train = db_train.shuffle(1000).batch(batchsz, drop_remainder=True)\ndb_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\ndb_test = db_test.batch(batchsz, drop_remainder=True)\nprint(\'x_train shape:\', x_train.shape, tf.reduce_max(y_train), tf.reduce_min(y_train))\nprint(\'x_test shape:\', x_test.shape)\n\n#%%\n\nclass MyRNN(keras.Model):\n    # Cell\xe6\x96\xb9\xe5\xbc\x8f\xe6\x9e\x84\xe5\xbb\xba\xe5\xa4\x9a\xe5\xb1\x82\xe7\xbd\x91\xe7\xbb\x9c\n    def __init__(self, units):\n        super(MyRNN, self).__init__() \n        # \xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe7\xbc\x96\xe7\xa0\x81 [b, 80] => [b, 80, 100]\n        self.embedding = layers.Embedding(total_words, embedding_len,\n                                          input_length=max_review_len)\n        # \xe6\x9e\x84\xe5\xbb\xbaRNN\n        self.rnn = keras.Sequential([\n            layers.SimpleRNN(units, dropout=0.5, return_sequences=True),\n            layers.SimpleRNN(units, dropout=0.5)\n        ])\n        # \xe6\x9e\x84\xe5\xbb\xba\xe5\x88\x86\xe7\xb1\xbb\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe5\xb0\x86CELL\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe7\x89\xb9\xe5\xbe\x81\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\xef\xbc\x8c2\xe5\x88\x86\xe7\xb1\xbb\n        # [b, 80, 100] => [b, 64] => [b, 1]\n        self.outlayer = Sequential([\n        \tlayers.Dense(32),\n        \tlayers.Dropout(rate=0.5),\n        \tlayers.ReLU(),\n        \tlayers.Dense(1)])\n\n    def call(self, inputs, training=None):\n        x = inputs # [b, 80]\n        # embedding: [b, 80] => [b, 80, 100]\n        x = self.embedding(x)\n        # rnn cell compute,[b, 80, 100] => [b, 64]\n        x = self.rnn(x)\n        # \xe6\x9c\xab\xe5\xb1\x82\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbe\x93\xe5\x87\xba\xe4\xbd\x9c\xe4\xb8\xba\xe5\x88\x86\xe7\xb1\xbb\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5: [b, 64] => [b, 1]\n        x = self.outlayer(x,training)\n        # p(y is pos|x)\n        prob = tf.sigmoid(x)\n\n        return prob\n\ndef main():\n    units = 64 # RNN\xe7\x8a\xb6\xe6\x80\x81\xe5\x90\x91\xe9\x87\x8f\xe9\x95\xbf\xe5\xba\xa6f\n    epochs = 50 # \xe8\xae\xad\xe7\xbb\x83epochs\n\n    model = MyRNN(units)\n    # \xe8\xa3\x85\xe9\x85\x8d\n    model.compile(optimizer = optimizers.Adam(0.001),\n                  loss = losses.BinaryCrossentropy(),\n                  metrics=[\'accuracy\'])\n    # \xe8\xae\xad\xe7\xbb\x83\xe5\x92\x8c\xe9\xaa\x8c\xe8\xaf\x81\n    model.fit(db_train, epochs=epochs, validation_data=db_test)\n    # \xe6\xb5\x8b\xe8\xaf\x95\n    model.evaluate(db_test)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
ch12-自编码器/autoencoder.py,17,"b""import  os\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nfrom    tensorflow.keras import Sequential, layers\nfrom    PIL import Image\nfrom    matplotlib import pyplot as plt\n\n\n\ntf.random.set_seed(22)\nnp.random.seed(22)\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nassert tf.__version__.startswith('2.')\n\n\ndef save_images(imgs, name):\n    new_im = Image.new('L', (280, 280))\n\n    index = 0\n    for i in range(0, 280, 28):\n        for j in range(0, 280, 28):\n            im = imgs[index]\n            im = Image.fromarray(im, mode='L')\n            new_im.paste(im, (i, j))\n            index += 1\n\n    new_im.save(name)\n\n\nh_dim = 20\nbatchsz = 512\nlr = 1e-3\n\n\n(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\nx_train, x_test = x_train.astype(np.float32) / 255., x_test.astype(np.float32) / 255.\n# we do not need label\ntrain_db = tf.data.Dataset.from_tensor_slices(x_train)\ntrain_db = train_db.shuffle(batchsz * 5).batch(batchsz)\ntest_db = tf.data.Dataset.from_tensor_slices(x_test)\ntest_db = test_db.batch(batchsz)\n\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)\n\n\n\nclass AE(keras.Model):\n\n    def __init__(self):\n        super(AE, self).__init__()\n\n        # Encoders\n        self.encoder = Sequential([\n            layers.Dense(256, activation=tf.nn.relu),\n            layers.Dense(128, activation=tf.nn.relu),\n            layers.Dense(h_dim)\n        ])\n\n        # Decoders\n        self.decoder = Sequential([\n            layers.Dense(128, activation=tf.nn.relu),\n            layers.Dense(256, activation=tf.nn.relu),\n            layers.Dense(784)\n        ])\n\n\n    def call(self, inputs, training=None):\n        # [b, 784] => [b, 10]\n        h = self.encoder(inputs)\n        # [b, 10] => [b, 784]\n        x_hat = self.decoder(h)\n\n        return x_hat\n\n\n\nmodel = AE()\nmodel.build(input_shape=(None, 784))\nmodel.summary()\n\noptimizer = tf.optimizers.Adam(lr=lr)\n\nfor epoch in range(100):\n\n    for step, x in enumerate(train_db):\n\n        #[b, 28, 28] => [b, 784]\n        x = tf.reshape(x, [-1, 784])\n\n        with tf.GradientTape() as tape:\n            x_rec_logits = model(x)\n\n            rec_loss = tf.losses.binary_crossentropy(x, x_rec_logits, from_logits=True)\n            rec_loss = tf.reduce_mean(rec_loss)\n\n        grads = tape.gradient(rec_loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n\n        if step % 100 ==0:\n            print(epoch, step, float(rec_loss))\n\n\n        # evaluation\n        x = next(iter(test_db))\n        logits = model(tf.reshape(x, [-1, 784]))\n        x_hat = tf.sigmoid(logits)\n        # [b, 784] => [b, 28, 28]\n        x_hat = tf.reshape(x_hat, [-1, 28, 28])\n\n        # [b, 28, 28] => [2b, 28, 28]\n        x_concat = tf.concat([x, x_hat], axis=0)\n        x_concat = x_hat\n        x_concat = x_concat.numpy() * 255.\n        x_concat = x_concat.astype(np.uint8)\n        save_images(x_concat, 'ae_images/rec_epoch_%d.png'%epoch)\n"""
ch12-自编码器/vae.py,21,"b""import  os\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nfrom    tensorflow.keras import Sequential, layers\nfrom    PIL import Image\nfrom    matplotlib import pyplot as plt\n\n\n\ntf.random.set_seed(22)\nnp.random.seed(22)\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nassert tf.__version__.startswith('2.')\n\n\ndef save_images(imgs, name):\n    new_im = Image.new('L', (280, 280))\n\n    index = 0\n    for i in range(0, 280, 28):\n        for j in range(0, 280, 28):\n            im = imgs[index]\n            im = Image.fromarray(im, mode='L')\n            new_im.paste(im, (i, j))\n            index += 1\n\n    new_im.save(name)\n\n\nh_dim = 20\nbatchsz = 512\nlr = 1e-3\n\n\n(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\nx_train, x_test = x_train.astype(np.float32) / 255., x_test.astype(np.float32) / 255.\n# we do not need label\ntrain_db = tf.data.Dataset.from_tensor_slices(x_train)\ntrain_db = train_db.shuffle(batchsz * 5).batch(batchsz)\ntest_db = tf.data.Dataset.from_tensor_slices(x_test)\ntest_db = test_db.batch(batchsz)\n\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)\n\nz_dim = 10\n\nclass VAE(keras.Model):\n\n    def __init__(self):\n        super(VAE, self).__init__()\n\n        # Encoder\n        self.fc1 = layers.Dense(128)\n        self.fc2 = layers.Dense(z_dim) # get mean prediction\n        self.fc3 = layers.Dense(z_dim)\n\n        # Decoder\n        self.fc4 = layers.Dense(128)\n        self.fc5 = layers.Dense(784)\n\n    def encoder(self, x):\n\n        h = tf.nn.relu(self.fc1(x))\n        # get mean\n        mu = self.fc2(h)\n        # get variance\n        log_var = self.fc3(h)\n\n        return mu, log_var\n\n    def decoder(self, z):\n\n        out = tf.nn.relu(self.fc4(z))\n        out = self.fc5(out)\n\n        return out\n\n    def reparameterize(self, mu, log_var):\n\n        eps = tf.random.normal(log_var.shape)\n\n        std = tf.exp(log_var*0.5)\n\n        z = mu + std * eps\n        return z\n\n    def call(self, inputs, training=None):\n\n        # [b, 784] => [b, z_dim], [b, z_dim]\n        mu, log_var = self.encoder(inputs)\n        # reparameterization trick\n        z = self.reparameterize(mu, log_var)\n\n        x_hat = self.decoder(z)\n\n        return x_hat, mu, log_var\n\n\nmodel = VAE()\nmodel.build(input_shape=(4, 784))\noptimizer = tf.optimizers.Adam(lr)\n\nfor epoch in range(1000):\n\n    for step, x in enumerate(train_db):\n\n        x = tf.reshape(x, [-1, 784])\n\n        with tf.GradientTape() as tape:\n            x_rec_logits, mu, log_var = model(x)\n\n            rec_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=x, logits=x_rec_logits)\n            rec_loss = tf.reduce_sum(rec_loss) / x.shape[0]\n\n            # compute kl divergence (mu, var) ~ N (0, 1)\n            # https://stats.stackexchange.com/questions/7440/kl-divergence-between-two-univariate-gaussians\n            kl_div = -0.5 * (log_var + 1 - mu**2 - tf.exp(log_var))\n            kl_div = tf.reduce_sum(kl_div) / x.shape[0]\n\n            loss = rec_loss + 1. * kl_div\n\n        grads = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n\n        if step % 100 == 0:\n            print(epoch, step, 'kl div:', float(kl_div), 'rec loss:', float(rec_loss))\n\n\n    # evaluation\n    z = tf.random.normal((batchsz, z_dim))\n    logits = model.decoder(z)\n    x_hat = tf.sigmoid(logits)\n    x_hat = tf.reshape(x_hat, [-1, 28, 28]).numpy() *255.\n    x_hat = x_hat.astype(np.uint8)\n    save_images(x_hat, 'vae_images/sampled_epoch%d.png'%epoch)\n\n    x = next(iter(test_db))\n    x = tf.reshape(x, [-1, 784])\n    x_hat_logits, _, _ = model(x)\n    x_hat = tf.sigmoid(x_hat_logits)\n    x_hat = tf.reshape(x_hat, [-1, 28, 28]).numpy() *255.\n    x_hat = x_hat.astype(np.uint8)\n    save_images(x_hat, 'vae_images/rec_epoch%d.png'%epoch)\n\n"""
ch13-生成对抗网络/dataset.py,9,"b'import multiprocessing\n\nimport tensorflow as tf\n\n\ndef make_anime_dataset(img_paths, batch_size, resize=64, drop_remainder=True, shuffle=True, repeat=1):\n\n    # @tf.function\n    def _map_fn(img):\n        img = tf.image.resize(img, [resize, resize])\n        # img = tf.image.random_crop(img,[resize, resize])\n        # img = tf.image.random_flip_left_right(img)\n        # img = tf.image.random_flip_up_down(img)\n        img = tf.clip_by_value(img, 0, 255)\n        img = img / 127.5 - 1 #-1~1\n        return img\n\n    dataset = disk_image_batch_dataset(img_paths,\n                                          batch_size,\n                                          drop_remainder=drop_remainder,\n                                          map_fn=_map_fn,\n                                          shuffle=shuffle,\n                                          repeat=repeat)\n    img_shape = (resize, resize, 3)\n    len_dataset = len(img_paths) // batch_size\n\n    return dataset, img_shape, len_dataset\n\n\ndef batch_dataset(dataset,\n                  batch_size,\n                  drop_remainder=True,\n                  n_prefetch_batch=1,\n                  filter_fn=None,\n                  map_fn=None,\n                  n_map_threads=None,\n                  filter_after_map=False,\n                  shuffle=True,\n                  shuffle_buffer_size=None,\n                  repeat=None):\n    # set defaults\n    if n_map_threads is None:\n        n_map_threads = multiprocessing.cpu_count()\n    if shuffle and shuffle_buffer_size is None:\n        shuffle_buffer_size = max(batch_size * 128, 2048)  # set the minimum buffer size as 2048\n\n    # [*] it is efficient to conduct `shuffle` before `map`/`filter` because `map`/`filter` is sometimes costly\n    if shuffle:\n        dataset = dataset.shuffle(shuffle_buffer_size)\n\n    if not filter_after_map:\n        if filter_fn:\n            dataset = dataset.filter(filter_fn)\n\n        if map_fn:\n            dataset = dataset.map(map_fn, num_parallel_calls=n_map_threads)\n\n    else:  # [*] this is slower\n        if map_fn:\n            dataset = dataset.map(map_fn, num_parallel_calls=n_map_threads)\n\n        if filter_fn:\n            dataset = dataset.filter(filter_fn)\n\n    dataset = dataset.batch(batch_size, drop_remainder=drop_remainder)\n\n    dataset = dataset.repeat(repeat).prefetch(n_prefetch_batch)\n\n    return dataset\n\n\ndef memory_data_batch_dataset(memory_data,\n                              batch_size,\n                              drop_remainder=True,\n                              n_prefetch_batch=1,\n                              filter_fn=None,\n                              map_fn=None,\n                              n_map_threads=None,\n                              filter_after_map=False,\n                              shuffle=True,\n                              shuffle_buffer_size=None,\n                              repeat=None):\n    """"""Batch dataset of memory data.\n\n    Parameters\n    ----------\n    memory_data : nested structure of tensors/ndarrays/lists\n\n    """"""\n    dataset = tf.data.Dataset.from_tensor_slices(memory_data)\n    dataset = batch_dataset(dataset,\n                            batch_size,\n                            drop_remainder=drop_remainder,\n                            n_prefetch_batch=n_prefetch_batch,\n                            filter_fn=filter_fn,\n                            map_fn=map_fn,\n                            n_map_threads=n_map_threads,\n                            filter_after_map=filter_after_map,\n                            shuffle=shuffle,\n                            shuffle_buffer_size=shuffle_buffer_size,\n                            repeat=repeat)\n    return dataset\n\n\ndef disk_image_batch_dataset(img_paths,\n                             batch_size,\n                             labels=None,\n                             drop_remainder=True,\n                             n_prefetch_batch=1,\n                             filter_fn=None,\n                             map_fn=None,\n                             n_map_threads=None,\n                             filter_after_map=False,\n                             shuffle=True,\n                             shuffle_buffer_size=None,\n                             repeat=None):\n    """"""Batch dataset of disk image for PNG and JPEG.\n\n    Parameters\n    ----------\n        img_paths : 1d-tensor/ndarray/list of str\n        labels : nested structure of tensors/ndarrays/lists\n\n    """"""\n    if labels is None:\n        memory_data = img_paths\n    else:\n        memory_data = (img_paths, labels)\n\n    def parse_fn(path, *label):\n        img = tf.io.read_file(path)\n        img = tf.image.decode_jpeg(img, channels=3)  # fix channels to 3\n        return (img,) + label\n\n    if map_fn:  # fuse `map_fn` and `parse_fn`\n        def map_fn_(*args):\n            return map_fn(*parse_fn(*args))\n    else:\n        map_fn_ = parse_fn\n\n    dataset = memory_data_batch_dataset(memory_data,\n                                        batch_size,\n                                        drop_remainder=drop_remainder,\n                                        n_prefetch_batch=n_prefetch_batch,\n                                        filter_fn=filter_fn,\n                                        map_fn=map_fn_,\n                                        n_map_threads=n_map_threads,\n                                        filter_after_map=filter_after_map,\n                                        shuffle=shuffle,\n                                        shuffle_buffer_size=shuffle_buffer_size,\n                                        repeat=repeat)\n\n    return dataset\n'"
ch13-生成对抗网络/gan.py,14,"b""import  tensorflow as tf\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers\n\n\nclass Generator(keras.Model):\n    # \xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe7\xbd\x91\xe7\xbb\x9c\n    def __init__(self):\n        super(Generator, self).__init__()\n        filter = 64\n        # \xe8\xbd\xac\xe7\xbd\xae\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x821,\xe8\xbe\x93\xe5\x87\xbachannel\xe4\xb8\xbafilter*8,\xe6\xa0\xb8\xe5\xa4\xa7\xe5\xb0\x8f4,\xe6\xad\xa5\xe9\x95\xbf1,\xe4\xb8\x8d\xe4\xbd\xbf\xe7\x94\xa8padding,\xe4\xb8\x8d\xe4\xbd\xbf\xe7\x94\xa8\xe5\x81\x8f\xe7\xbd\xae\n        self.conv1 = layers.Conv2DTranspose(filter*8, 4,1, 'valid', use_bias=False)\n        self.bn1 = layers.BatchNormalization()\n        # \xe8\xbd\xac\xe7\xbd\xae\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x822\n        self.conv2 = layers.Conv2DTranspose(filter*4, 4,2, 'same', use_bias=False)\n        self.bn2 = layers.BatchNormalization()\n        # \xe8\xbd\xac\xe7\xbd\xae\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x823\n        self.conv3 = layers.Conv2DTranspose(filter*2, 4,2, 'same', use_bias=False)\n        self.bn3 = layers.BatchNormalization()\n        # \xe8\xbd\xac\xe7\xbd\xae\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x824\n        self.conv4 = layers.Conv2DTranspose(filter*1, 4,2, 'same', use_bias=False)\n        self.bn4 = layers.BatchNormalization()\n        # \xe8\xbd\xac\xe7\xbd\xae\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x825\n        self.conv5 = layers.Conv2DTranspose(3, 4,2, 'same', use_bias=False)\n\n    def call(self, inputs, training=None):\n        x = inputs # [z, 100]\n        # Reshape\xe4\xb9\x984D\xe5\xbc\xa0\xe9\x87\x8f\xef\xbc\x8c\xe6\x96\xb9\xe4\xbe\xbf\xe5\x90\x8e\xe7\xbb\xad\xe8\xbd\xac\xe7\xbd\xae\xe5\x8d\xb7\xe7\xa7\xaf\xe8\xbf\x90\xe7\xae\x97:(b, 1, 1, 100)\n        x = tf.reshape(x, (x.shape[0], 1, 1, x.shape[1]))\n        x = tf.nn.relu(x) # \xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\n        # \xe8\xbd\xac\xe7\xbd\xae\xe5\x8d\xb7\xe7\xa7\xaf-BN-\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0:(b, 4, 4, 512)\n        x = tf.nn.relu(self.bn1(self.conv1(x), training=training))\n        # \xe8\xbd\xac\xe7\xbd\xae\xe5\x8d\xb7\xe7\xa7\xaf-BN-\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0:(b, 8, 8, 256)\n        x = tf.nn.relu(self.bn2(self.conv2(x), training=training))\n        # \xe8\xbd\xac\xe7\xbd\xae\xe5\x8d\xb7\xe7\xa7\xaf-BN-\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0:(b, 16, 16, 128)\n        x = tf.nn.relu(self.bn3(self.conv3(x), training=training))\n        # \xe8\xbd\xac\xe7\xbd\xae\xe5\x8d\xb7\xe7\xa7\xaf-BN-\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0:(b, 32, 32, 64)\n        x = tf.nn.relu(self.bn4(self.conv4(x), training=training))\n        # \xe8\xbd\xac\xe7\xbd\xae\xe5\x8d\xb7\xe7\xa7\xaf-\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0:(b, 64, 64, 3)\n        x = self.conv5(x)\n        x = tf.tanh(x) # \xe8\xbe\x93\xe5\x87\xbax\xe8\x8c\x83\xe5\x9b\xb4-1~1,\xe4\xb8\x8e\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xe4\xb8\x80\xe8\x87\xb4\n\n        return x\n\n\nclass Discriminator(keras.Model):\n    # \xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        filter = 64\n        # \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\n        self.conv1 = layers.Conv2D(filter, 4, 2, 'valid', use_bias=False)\n        self.bn1 = layers.BatchNormalization()\n        # \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\n        self.conv2 = layers.Conv2D(filter*2, 4, 2, 'valid', use_bias=False)\n        self.bn2 = layers.BatchNormalization()\n        # \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\n        self.conv3 = layers.Conv2D(filter*4, 4, 2, 'valid', use_bias=False)\n        self.bn3 = layers.BatchNormalization()\n        # \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\n        self.conv4 = layers.Conv2D(filter*8, 3, 1, 'valid', use_bias=False)\n        self.bn4 = layers.BatchNormalization()\n        # \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\n        self.conv5 = layers.Conv2D(filter*16, 3, 1, 'valid', use_bias=False)\n        self.bn5 = layers.BatchNormalization()\n        # \xe5\x85\xa8\xe5\xb1\x80\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\n        self.pool = layers.GlobalAveragePooling2D()\n        # \xe7\x89\xb9\xe5\xbe\x81\xe6\x89\x93\xe5\xb9\xb3\n        self.flatten = layers.Flatten()\n        # 2\xe5\x88\x86\xe7\xb1\xbb\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\n        self.fc = layers.Dense(1)\n\n\n    def call(self, inputs, training=None):\n        # \xe5\x8d\xb7\xe7\xa7\xaf-BN-\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0:(4, 31, 31, 64)\n        x = tf.nn.leaky_relu(self.bn1(self.conv1(inputs), training=training))\n        # \xe5\x8d\xb7\xe7\xa7\xaf-BN-\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0:(4, 14, 14, 128)\n        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training))\n        # \xe5\x8d\xb7\xe7\xa7\xaf-BN-\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0:(4, 6, 6, 256)\n        x = tf.nn.leaky_relu(self.bn3(self.conv3(x), training=training))\n        # \xe5\x8d\xb7\xe7\xa7\xaf-BN-\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0:(4, 4, 4, 512)\n        x = tf.nn.leaky_relu(self.bn4(self.conv4(x), training=training))\n        # \xe5\x8d\xb7\xe7\xa7\xaf-BN-\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0:(4, 2, 2, 1024)\n        x = tf.nn.leaky_relu(self.bn5(self.conv5(x), training=training))\n        # \xe5\x8d\xb7\xe7\xa7\xaf-BN-\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0:(4, 1024)\n        x = self.pool(x)\n        # \xe6\x89\x93\xe5\xb9\xb3\n        x = self.flatten(x)\n        # \xe8\xbe\x93\xe5\x87\xba\xef\xbc\x8c[b, 1024] => [b, 1]\n        logits = self.fc(x)\n\n        return logits\n\ndef main():\n\n    d = Discriminator()\n    g = Generator()\n\n\n    x = tf.random.normal([2, 64, 64, 3])\n    z = tf.random.normal([2, 100])\n\n    prob = d(x)\n    print(prob)\n    x_hat = g(z)\n    print(x_hat.shape)\n\n\n\n\nif __name__ == '__main__':\n    main()"""
ch13-生成对抗网络/gan_train.py,13,"b""import  os\nimport  numpy as np\nimport  tensorflow as tf\nfrom    tensorflow import keras\nfrom    scipy.misc import toimage\nimport  glob\nfrom    gan import Generator, Discriminator\n\nfrom    dataset import make_anime_dataset\n\n\ndef save_result(val_out, val_block_size, image_path, color_mode):\n    def preprocess(img):\n        img = ((img + 1.0) * 127.5).astype(np.uint8)\n        # img = img.astype(np.uint8)\n        return img\n\n    preprocesed = preprocess(val_out)\n    final_image = np.array([])\n    single_row = np.array([])\n    for b in range(val_out.shape[0]):\n        # concat image into a row\n        if single_row.size == 0:\n            single_row = preprocesed[b, :, :, :]\n        else:\n            single_row = np.concatenate((single_row, preprocesed[b, :, :, :]), axis=1)\n\n        # concat image row to final_image\n        if (b+1) % val_block_size == 0:\n            if final_image.size == 0:\n                final_image = single_row\n            else:\n                final_image = np.concatenate((final_image, single_row), axis=0)\n\n            # reset single row\n            single_row = np.array([])\n\n    if final_image.shape[2] == 1:\n        final_image = np.squeeze(final_image, axis=2)\n    toimage(final_image).save(image_path)\n\n\ndef celoss_ones(logits):\n    # \xe8\xae\xa1\xe7\xae\x97\xe5\xb1\x9e\xe4\xba\x8e\xe4\xb8\x8e\xe6\xa0\x87\xe7\xad\xbe\xe4\xb8\xba1\xe7\x9a\x84\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\n    y = tf.ones_like(logits)\n    loss = keras.losses.binary_crossentropy(y, logits, from_logits=True)\n    return tf.reduce_mean(loss)\n\n\ndef celoss_zeros(logits):\n    # \xe8\xae\xa1\xe7\xae\x97\xe5\xb1\x9e\xe4\xba\x8e\xe4\xb8\x8e\xe4\xbe\xbf\xe7\xad\xbe\xe4\xb8\xba0\xe7\x9a\x84\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\n    y = tf.zeros_like(logits)\n    loss = keras.losses.binary_crossentropy(y, logits, from_logits=True)\n    return tf.reduce_mean(loss)\n\ndef d_loss_fn(generator, discriminator, batch_z, batch_x, is_training):\n    # \xe8\xae\xa1\xe7\xae\x97\xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe7\x9a\x84\xe8\xaf\xaf\xe5\xb7\xae\xe5\x87\xbd\xe6\x95\xb0\n    # \xe9\x87\x87\xe6\xa0\xb7\xe7\x94\x9f\xe6\x88\x90\xe5\x9b\xbe\xe7\x89\x87\n    fake_image = generator(batch_z, is_training)\n    # \xe5\x88\xa4\xe5\xae\x9a\xe7\x94\x9f\xe6\x88\x90\xe5\x9b\xbe\xe7\x89\x87\n    d_fake_logits = discriminator(fake_image, is_training)\n    # \xe5\x88\xa4\xe5\xae\x9a\xe7\x9c\x9f\xe5\xae\x9e\xe5\x9b\xbe\xe7\x89\x87\n    d_real_logits = discriminator(batch_x, is_training)\n    # \xe7\x9c\x9f\xe5\xae\x9e\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\x8e1\xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84\xe8\xaf\xaf\xe5\xb7\xae\n    d_loss_real = celoss_ones(d_real_logits)\n    # \xe7\x94\x9f\xe6\x88\x90\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\x8e0\xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84\xe8\xaf\xaf\xe5\xb7\xae\n    d_loss_fake = celoss_zeros(d_fake_logits)\n    # \xe5\x90\x88\xe5\xb9\xb6\xe8\xaf\xaf\xe5\xb7\xae\n    loss = d_loss_fake + d_loss_real\n\n    return loss\n\n\ndef g_loss_fn(generator, discriminator, batch_z, is_training):\n    # \xe9\x87\x87\xe6\xa0\xb7\xe7\x94\x9f\xe6\x88\x90\xe5\x9b\xbe\xe7\x89\x87\n    fake_image = generator(batch_z, is_training)\n    # \xe5\x9c\xa8\xe8\xae\xad\xe7\xbb\x83\xe7\x94\x9f\xe6\x88\x90\xe7\xbd\x91\xe7\xbb\x9c\xe6\x97\xb6\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe8\xbf\xab\xe4\xbd\xbf\xe7\x94\x9f\xe6\x88\x90\xe5\x9b\xbe\xe7\x89\x87\xe5\x88\xa4\xe5\xae\x9a\xe4\xb8\xba\xe7\x9c\x9f\n    d_fake_logits = discriminator(fake_image, is_training)\n    # \xe8\xae\xa1\xe7\xae\x97\xe7\x94\x9f\xe6\x88\x90\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\x8e1\xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84\xe8\xaf\xaf\xe5\xb7\xae\n    loss = celoss_ones(d_fake_logits)\n\n    return loss\n\ndef main():\n\n    tf.random.set_seed(3333)\n    np.random.seed(3333)\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n    assert tf.__version__.startswith('2.')\n\n\n    z_dim = 100 # \xe9\x9a\x90\xe8\x97\x8f\xe5\x90\x91\xe9\x87\x8fz\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\n    epochs = 3000000 # \xe8\xae\xad\xe7\xbb\x83\xe6\xad\xa5\xe6\x95\xb0\n    batch_size = 64 # batch size\n    learning_rate = 0.0002\n    is_training = True\n\n    # \xe8\x8e\xb7\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe8\xb7\xaf\xe5\xbe\x84\n    # C:\\Users\\z390\\Downloads\\anime-faces\n    # r'C:\\Users\\z390\\Downloads\\faces\\*.jpg'\n    img_path = glob.glob(r'C:\\Users\\z390\\Downloads\\anime-faces\\*\\*.jpg') + \\\n        glob.glob(r'C:\\Users\\z390\\Downloads\\anime-faces\\*\\*.png')\n    # img_path = glob.glob(r'C:\\Users\\z390\\Downloads\\getchu_aligned_with_label\\GetChu_aligned2\\*.jpg')\n    # img_path.extend(img_path2)\n    print('images num:', len(img_path))\n    # \xe6\x9e\x84\xe5\xbb\xba\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\xaf\xb9\xe8\xb1\xa1\n    dataset, img_shape, _ = make_anime_dataset(img_path, batch_size, resize=64)\n    print(dataset, img_shape)\n    sample = next(iter(dataset)) # \xe9\x87\x87\xe6\xa0\xb7\n    print(sample.shape, tf.reduce_max(sample).numpy(),\n          tf.reduce_min(sample).numpy())\n    dataset = dataset.repeat(100) # \xe9\x87\x8d\xe5\xa4\x8d\xe5\xbe\xaa\xe7\x8e\xaf\n    db_iter = iter(dataset)\n\n\n    generator = Generator() # \xe5\x88\x9b\xe5\xbb\xba\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\n    generator.build(input_shape = (4, z_dim))\n    discriminator = Discriminator() # \xe5\x88\x9b\xe5\xbb\xba\xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\n    discriminator.build(input_shape=(4, 64, 64, 3))\n    # \xe5\x88\x86\xe5\x88\xab\xe4\xb8\xba\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe5\x92\x8c\xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe5\x88\x9b\xe5\xbb\xba\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\n    g_optimizer = keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5)\n    d_optimizer = keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5)\n\n    generator.load_weights('generator.ckpt')\n    discriminator.load_weights('discriminator.ckpt')\n    print('Loaded chpt!!')\n\n    d_losses, g_losses = [],[]\n    for epoch in range(epochs): # \xe8\xae\xad\xe7\xbb\x83epochs\xe6\xac\xa1\n        # 1. \xe8\xae\xad\xe7\xbb\x83\xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\n        for _ in range(1):\n            # \xe9\x87\x87\xe6\xa0\xb7\xe9\x9a\x90\xe8\x97\x8f\xe5\x90\x91\xe9\x87\x8f\n            batch_z = tf.random.normal([batch_size, z_dim])\n            batch_x = next(db_iter) # \xe9\x87\x87\xe6\xa0\xb7\xe7\x9c\x9f\xe5\xae\x9e\xe5\x9b\xbe\xe7\x89\x87\n            # \xe5\x88\xa4\xe5\x88\xab\xe5\x99\xa8\xe5\x89\x8d\xe5\x90\x91\xe8\xae\xa1\xe7\xae\x97\n            with tf.GradientTape() as tape:\n                d_loss = d_loss_fn(generator, discriminator, batch_z, batch_x, is_training)\n            grads = tape.gradient(d_loss, discriminator.trainable_variables)\n            d_optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n        # 2. \xe8\xae\xad\xe7\xbb\x83\xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\n        # \xe9\x87\x87\xe6\xa0\xb7\xe9\x9a\x90\xe8\x97\x8f\xe5\x90\x91\xe9\x87\x8f\n        batch_z = tf.random.normal([batch_size, z_dim])\n        batch_x = next(db_iter) # \xe9\x87\x87\xe6\xa0\xb7\xe7\x9c\x9f\xe5\xae\x9e\xe5\x9b\xbe\xe7\x89\x87\n        # \xe7\x94\x9f\xe6\x88\x90\xe5\x99\xa8\xe5\x89\x8d\xe5\x90\x91\xe8\xae\xa1\xe7\xae\x97\n        with tf.GradientTape() as tape:\n            g_loss = g_loss_fn(generator, discriminator, batch_z, is_training)\n        grads = tape.gradient(g_loss, generator.trainable_variables)\n        g_optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n\n        if epoch % 100 == 0:\n            print(epoch, 'd-loss:',float(d_loss), 'g-loss:', float(g_loss))\n            # \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n            z = tf.random.normal([100, z_dim])\n            fake_image = generator(z, training=False)\n            img_path = os.path.join('gan_images', 'gan-%d.png'%epoch)\n            save_result(fake_image.numpy(), 10, img_path, color_mode='P')\n\n            d_losses.append(float(d_loss))\n            g_losses.append(float(g_loss))\n\n            if epoch % 10000 == 1:\n                # print(d_losses)\n                # print(g_losses)\n                generator.save_weights('generator.ckpt')\n                discriminator.save_weights('discriminator.ckpt')\n\n            \n\n\n\nif __name__ == '__main__':\n    main()"""
ch13-生成对抗网络/wgan.py,10,"b""import  tensorflow as tf\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers\n\n\n\n\n\n\nclass Generator(keras.Model):\n\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        # z: [b, 100] => [b, 3*3*512] => [b, 3, 3, 512] => [b, 64, 64, 3]\n        self.fc = layers.Dense(3*3*512)\n\n        self.conv1 = layers.Conv2DTranspose(256, 3, 3, 'valid')\n        self.bn1 = layers.BatchNormalization()\n\n        self.conv2 = layers.Conv2DTranspose(128, 5, 2, 'valid')\n        self.bn2 = layers.BatchNormalization()\n\n        self.conv3 = layers.Conv2DTranspose(3, 4, 3, 'valid')\n\n    def call(self, inputs, training=None):\n        # [z, 100] => [z, 3*3*512]\n        x = self.fc(inputs)\n        x = tf.reshape(x, [-1, 3, 3, 512])\n        x = tf.nn.leaky_relu(x)\n\n        #\n        x = tf.nn.leaky_relu(self.bn1(self.conv1(x), training=training))\n        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training))\n        x = self.conv3(x)\n        x = tf.tanh(x)\n\n        return x\n\n\nclass Discriminator(keras.Model):\n\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        # [b, 64, 64, 3] => [b, 1]\n        self.conv1 = layers.Conv2D(64, 5, 3, 'valid')\n\n        self.conv2 = layers.Conv2D(128, 5, 3, 'valid')\n        self.bn2 = layers.BatchNormalization()\n\n        self.conv3 = layers.Conv2D(256, 5, 3, 'valid')\n        self.bn3 = layers.BatchNormalization()\n\n        # [b, h, w ,c] => [b, -1]\n        self.flatten = layers.Flatten()\n        self.fc = layers.Dense(1)\n\n\n    def call(self, inputs, training=None):\n\n        x = tf.nn.leaky_relu(self.conv1(inputs))\n        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training))\n        x = tf.nn.leaky_relu(self.bn3(self.conv3(x), training=training))\n\n        # [b, h, w, c] => [b, -1]\n        x = self.flatten(x)\n        # [b, -1] => [b, 1]\n        logits = self.fc(x)\n\n        return logits\n\ndef main():\n\n    d = Discriminator()\n    g = Generator()\n\n\n    x = tf.random.normal([2, 64, 64, 3])\n    z = tf.random.normal([2, 100])\n\n    prob = d(x)\n    print(prob)\n    x_hat = g(z)\n    print(x_hat.shape)\n\n\n\n\nif __name__ == '__main__':\n    main()"""
ch13-生成对抗网络/wgan_train.py,24,"b""import  os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nimport  numpy as np\nimport  tensorflow as tf\nfrom    tensorflow import keras\n\nfrom    PIL import Image\nimport  glob\nfrom    gan import Generator, Discriminator\n\nfrom    dataset import make_anime_dataset\n\n\ndef save_result(val_out, val_block_size, image_path, color_mode):\n    def preprocess(img):\n        img = ((img + 1.0) * 127.5).astype(np.uint8)\n        # img = img.astype(np.uint8)\n        return img\n\n    preprocesed = preprocess(val_out)\n    final_image = np.array([])\n    single_row = np.array([])\n    for b in range(val_out.shape[0]):\n        # concat image into a row\n        if single_row.size == 0:\n            single_row = preprocesed[b, :, :, :]\n        else:\n            single_row = np.concatenate((single_row, preprocesed[b, :, :, :]), axis=1)\n\n        # concat image row to final_image\n        if (b+1) % val_block_size == 0:\n            if final_image.size == 0:\n                final_image = single_row\n            else:\n                final_image = np.concatenate((final_image, single_row), axis=0)\n\n            # reset single row\n            single_row = np.array([])\n\n    if final_image.shape[2] == 1:\n        final_image = np.squeeze(final_image, axis=2) \n    Image.fromarray(final_image).save(image_path)\n\n\ndef celoss_ones(logits):\n    # [b, 1]\n    # [b] = [1, 1, 1, 1,]\n    # loss = tf.keras.losses.categorical_crossentropy(y_pred=logits,\n    #                                                y_true=tf.ones_like(logits))\n    return - tf.reduce_mean(logits)\n\n\ndef celoss_zeros(logits):\n    # [b, 1]\n    # [b] = [1, 1, 1, 1,]\n    # loss = tf.keras.losses.categorical_crossentropy(y_pred=logits,\n    #                                                y_true=tf.zeros_like(logits))\n    return tf.reduce_mean(logits)\n\n\ndef gradient_penalty(discriminator, batch_x, fake_image):\n\n    batchsz = batch_x.shape[0]\n\n    # [b, h, w, c]\n    t = tf.random.uniform([batchsz, 1, 1, 1])\n    # [b, 1, 1, 1] => [b, h, w, c]\n    t = tf.broadcast_to(t, batch_x.shape)\n\n    interplate = t * batch_x + (1 - t) * fake_image\n\n    with tf.GradientTape() as tape:\n        tape.watch([interplate])\n        d_interplote_logits = discriminator(interplate, training=True)\n    grads = tape.gradient(d_interplote_logits, interplate)\n\n    # grads:[b, h, w, c] => [b, -1]\n    grads = tf.reshape(grads, [grads.shape[0], -1])\n    gp = tf.norm(grads, axis=1) #[b]\n    gp = tf.reduce_mean( (gp-1)**2 )\n\n    return gp\n\n\n\ndef d_loss_fn(generator, discriminator, batch_z, batch_x, is_training):\n    # 1. treat real image as real\n    # 2. treat generated image as fake\n    fake_image = generator(batch_z, is_training)\n    d_fake_logits = discriminator(fake_image, is_training)\n    d_real_logits = discriminator(batch_x, is_training)\n\n    d_loss_real = celoss_ones(d_real_logits)\n    d_loss_fake = celoss_zeros(d_fake_logits)\n    gp = gradient_penalty(discriminator, batch_x, fake_image)\n\n    loss = d_loss_real + d_loss_fake + 10. * gp\n\n    return loss, gp\n\n\ndef g_loss_fn(generator, discriminator, batch_z, is_training):\n\n    fake_image = generator(batch_z, is_training)\n    d_fake_logits = discriminator(fake_image, is_training)\n    loss = celoss_ones(d_fake_logits)\n\n    return loss\n\n\ndef main():\n\n    tf.random.set_seed(233)\n    np.random.seed(233)\n    assert tf.__version__.startswith('2.')\n\n\n    # hyper parameters\n    z_dim = 100\n    epochs = 3000000\n    batch_size = 512\n    learning_rate = 0.0005\n    is_training = True\n\n\n    img_path = glob.glob(r'C:\\Users\\Jackie\\Downloads\\faces\\*.jpg')\n    assert len(img_path) > 0\n    \n\n    dataset, img_shape, _ = make_anime_dataset(img_path, batch_size)\n    print(dataset, img_shape)\n    sample = next(iter(dataset))\n    print(sample.shape, tf.reduce_max(sample).numpy(),\n          tf.reduce_min(sample).numpy())\n    dataset = dataset.repeat()\n    db_iter = iter(dataset)\n\n\n    generator = Generator() \n    generator.build(input_shape = (None, z_dim))\n    discriminator = Discriminator()\n    discriminator.build(input_shape=(None, 64, 64, 3))\n    z_sample = tf.random.normal([100, z_dim])\n\n\n    g_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5)\n    d_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5)\n\n\n    for epoch in range(epochs):\n\n        for _ in range(5):\n            batch_z = tf.random.normal([batch_size, z_dim])\n            batch_x = next(db_iter)\n\n            # train D\n            with tf.GradientTape() as tape:\n                d_loss, gp = d_loss_fn(generator, discriminator, batch_z, batch_x, is_training)\n            grads = tape.gradient(d_loss, discriminator.trainable_variables)\n            d_optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n        \n        batch_z = tf.random.normal([batch_size, z_dim])\n\n        with tf.GradientTape() as tape:\n            g_loss = g_loss_fn(generator, discriminator, batch_z, is_training)\n        grads = tape.gradient(g_loss, generator.trainable_variables)\n        g_optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n\n        if epoch % 100 == 0:\n            print(epoch, 'd-loss:',float(d_loss), 'g-loss:', float(g_loss),\n                  'gp:', float(gp))\n\n            z = tf.random.normal([100, z_dim])\n            fake_image = generator(z, training=False)\n            img_path = os.path.join('images', 'wgan-%d.png'%epoch)\n            save_result(fake_image.numpy(), 10, img_path, color_mode='P')\n\n\n\nif __name__ == '__main__':\n    main()"""
ch14-强化学习/REINFORCE_tf.py,10,"b'import \tgym,os\nimport  numpy as np\nimport  matplotlib\nfrom \tmatplotlib import pyplot as plt\n# Default parameters for plots\nmatplotlib.rcParams[\'font.size\'] = 18\nmatplotlib.rcParams[\'figure.titlesize\'] = 18\nmatplotlib.rcParams[\'figure.figsize\'] = [9, 7]\nmatplotlib.rcParams[\'font.family\'] = [\'KaiTi\']\nmatplotlib.rcParams[\'axes.unicode_minus\']=False \n\nimport \ttensorflow as tf\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers,optimizers,losses\nfrom    PIL import Image\nenv = gym.make(\'CartPole-v1\')  # \xe5\x88\x9b\xe5\xbb\xba\xe6\xb8\xb8\xe6\x88\x8f\xe7\x8e\xaf\xe5\xa2\x83\nenv.seed(2333)\ntf.random.set_seed(2333)\nnp.random.seed(2333)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nassert tf.__version__.startswith(\'2.\')\n\nlearning_rate = 0.0002\ngamma         = 0.98\n\nclass Policy(keras.Model):\n    # \xe7\xad\x96\xe7\x95\xa5\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe7\x94\x9f\xe6\x88\x90\xe5\x8a\xa8\xe4\xbd\x9c\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\xe5\x88\x86\xe5\xb8\x83\n    def __init__(self):\n        super(Policy, self).__init__()\n        self.data = [] # \xe5\xad\x98\xe5\x82\xa8\xe8\xbd\xa8\xe8\xbf\xb9\n        # \xe8\xbe\x93\xe5\x85\xa5\xe4\xb8\xba\xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\xba4\xe7\x9a\x84\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe4\xb8\xba\xe5\xb7\xa6\xe3\x80\x81\xe5\x8f\xb32\xe4\xb8\xaa\xe5\x8a\xa8\xe4\xbd\x9c\n        self.fc1 = layers.Dense(128, kernel_initializer=\'he_normal\')\n        self.fc2 = layers.Dense(2, kernel_initializer=\'he_normal\')\n        # \xe7\xbd\x91\xe7\xbb\x9c\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\n        self.optimizer = optimizers.Adam(lr=learning_rate)\n\n    def call(self, inputs, training=None):\n        # \xe7\x8a\xb6\xe6\x80\x81\xe8\xbe\x93\xe5\x85\xa5s\xe7\x9a\x84shape\xe4\xb8\xba\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x9a[4]\n        x = tf.nn.relu(self.fc1(inputs))\n        x = tf.nn.softmax(self.fc2(x), axis=1)\n        return x\n\n    def put_data(self, item):\n        # \xe8\xae\xb0\xe5\xbd\x95r,log_P(a|s)\n        self.data.append(item)\n\n    def train_net(self, tape):\n        # \xe8\xae\xa1\xe7\xae\x97\xe6\xa2\xaf\xe5\xba\xa6\xe5\xb9\xb6\xe6\x9b\xb4\xe6\x96\xb0\xe7\xad\x96\xe7\x95\xa5\xe7\xbd\x91\xe7\xbb\x9c\xe5\x8f\x82\xe6\x95\xb0\xe3\x80\x82tape\xe4\xb8\xba\xe6\xa2\xaf\xe5\xba\xa6\xe8\xae\xb0\xe5\xbd\x95\xe5\x99\xa8\n        R = 0 # \xe7\xbb\x88\xe7\xbb\x93\xe7\x8a\xb6\xe6\x80\x81\xe7\x9a\x84\xe5\x88\x9d\xe5\xa7\x8b\xe5\x9b\x9e\xe6\x8a\xa5\xe4\xb8\xba0\n        for r, log_prob in self.data[::-1]:#\xe9\x80\x86\xe5\xba\x8f\xe5\x8f\x96\n            R = r + gamma * R # \xe8\xae\xa1\xe7\xae\x97\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x97\xb6\xe9\x97\xb4\xe6\x88\xb3\xe4\xb8\x8a\xe7\x9a\x84\xe5\x9b\x9e\xe6\x8a\xa5\n            # \xe6\xaf\x8f\xe4\xb8\xaa\xe6\x97\xb6\xe9\x97\xb4\xe6\x88\xb3\xe9\x83\xbd\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x80\xe6\xac\xa1\xe6\xa2\xaf\xe5\xba\xa6\n            # grad_R=-log_P*R*grad_theta\n            loss = -log_prob * R\n            with tape.stop_recording():\n                # \xe4\xbc\x98\xe5\x8c\x96\xe7\xad\x96\xe7\x95\xa5\xe7\xbd\x91\xe7\xbb\x9c\n                grads = tape.gradient(loss, self.trainable_variables)\n                # print(grads)\n                self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n        self.data = [] # \xe6\xb8\x85\xe7\xa9\xba\xe8\xbd\xa8\xe8\xbf\xb9\n\ndef main():\n    pi = Policy() # \xe5\x88\x9b\xe5\xbb\xba\xe7\xad\x96\xe7\x95\xa5\xe7\xbd\x91\xe7\xbb\x9c\n    pi(tf.random.normal((4,4)))\n    pi.summary()\n    score = 0.0 # \xe8\xae\xa1\xe5\x88\x86\n    print_interval = 20 # \xe6\x89\x93\xe5\x8d\xb0\xe9\x97\xb4\xe9\x9a\x94\n    returns = []\n\n    for n_epi in range(400):\n        s = env.reset() # \xe5\x9b\x9e\xe5\x88\xb0\xe6\xb8\xb8\xe6\x88\x8f\xe5\x88\x9d\xe5\xa7\x8b\xe7\x8a\xb6\xe6\x80\x81\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9es0\n        with tf.GradientTape(persistent=True) as tape:\n            for t in range(501): # CartPole-v1 forced to terminates at 500 step.\n                # \xe9\x80\x81\xe5\x85\xa5\xe7\x8a\xb6\xe6\x80\x81\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x8c\xe8\x8e\xb7\xe5\x8f\x96\xe7\xad\x96\xe7\x95\xa5\n                s = tf.constant(s,dtype=tf.float32)\n                # s: [4] => [1,4]\n                s = tf.expand_dims(s, axis=0)\n                prob = pi(s) # \xe5\x8a\xa8\xe4\xbd\x9c\xe5\x88\x86\xe5\xb8\x83:[1,2]\n                # \xe4\xbb\x8e\xe7\xb1\xbb\xe5\x88\xab\xe5\x88\x86\xe5\xb8\x83\xe4\xb8\xad\xe9\x87\x87\xe6\xa0\xb71\xe4\xb8\xaa\xe5\x8a\xa8\xe4\xbd\x9c, shape: [1]\n                a = tf.random.categorical(tf.math.log(prob), 1)[0]\n                a = int(a) # Tensor\xe8\xbd\xac\xe6\x95\xb0\xe5\xad\x97\n                s_prime, r, done, info = env.step(a)\n                # \xe8\xae\xb0\xe5\xbd\x95\xe5\x8a\xa8\xe4\xbd\x9ca\xe5\x92\x8c\xe5\x8a\xa8\xe4\xbd\x9c\xe4\xba\xa7\xe7\x94\x9f\xe7\x9a\x84\xe5\xa5\x96\xe5\x8a\xb1r\n                # prob shape:[1,2]\n                pi.put_data((r, tf.math.log(prob[0][a])))\n                s = s_prime # \xe5\x88\xb7\xe6\x96\xb0\xe7\x8a\xb6\xe6\x80\x81\n                score += r # \xe7\xb4\xaf\xe7\xa7\xaf\xe5\xa5\x96\xe5\x8a\xb1\n\n                if n_epi >1000:\n                    env.render()\n                    # im = Image.fromarray(s)\n                    # im.save(""res/%d.jpg"" % info[\'frames\'][0])\n\n                if done:  # \xe5\xbd\x93\xe5\x89\x8depisode\xe7\xbb\x88\xe6\xad\xa2\n                    break\n            # episode\xe7\xbb\x88\xe6\xad\xa2\xe5\x90\x8e\xef\xbc\x8c\xe8\xae\xad\xe7\xbb\x83\xe4\xb8\x80\xe6\xac\xa1\xe7\xbd\x91\xe7\xbb\x9c\n            pi.train_net(tape)\n        del tape\n\n        if n_epi%print_interval==0 and n_epi!=0:\n            returns.append(score/print_interval)\n            print(f""# of episode :{n_epi}, avg score : {score/print_interval}"")\n            score = 0.0\n    env.close() # \xe5\x85\xb3\xe9\x97\xad\xe7\x8e\xaf\xe5\xa2\x83\n\n    plt.plot(np.arange(len(returns))*print_interval, returns)\n    plt.plot(np.arange(len(returns))*print_interval, returns, \'s\')\n    plt.xlabel(\'\xe5\x9b\x9e\xe5\x90\x88\xe6\x95\xb0\')\n    plt.ylabel(\'\xe6\x80\xbb\xe5\x9b\x9e\xe6\x8a\xa5\')\n    plt.savefig(\'reinforce-tf-cartpole.svg\')\n\nif __name__ == \'__main__\':\n    main()'"
ch14-强化学习/a3c_tf_cartpole.py,18,"b'import  matplotlib\nfrom    matplotlib import pyplot as plt\nmatplotlib.rcParams[\'font.size\'] = 18\nmatplotlib.rcParams[\'figure.titlesize\'] = 18\nmatplotlib.rcParams[\'figure.figsize\'] = [9, 7]\nmatplotlib.rcParams[\'font.family\'] = [\'KaiTi\']\nmatplotlib.rcParams[\'axes.unicode_minus\']=False\n\nplt.figure()\n\nimport os\nos.environ[""CUDA_VISIBLE_DEVICES""] = """"\nimport  threading\nimport  gym\nimport  multiprocessing\nimport  numpy as np\nfrom    queue import Queue\nimport  matplotlib.pyplot as plt\n\nimport  tensorflow as tf\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers,optimizers,losses\n\n\n\ntf.random.set_seed(1231)\nnp.random.seed(1231)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nassert tf.__version__.startswith(\'2.\')\n\n\nclass ActorCritic(keras.Model):\n    # Actor-Critic\xe6\xa8\xa1\xe5\x9e\x8b\n    def __init__(self, state_size, action_size):\n        super(ActorCritic, self).__init__()\n        self.state_size = state_size # \xe7\x8a\xb6\xe6\x80\x81\xe5\x90\x91\xe9\x87\x8f\xe9\x95\xbf\xe5\xba\xa6\n        self.action_size = action_size # \xe5\x8a\xa8\xe4\xbd\x9c\xe6\x95\xb0\xe9\x87\x8f\n        # \xe7\xad\x96\xe7\x95\xa5\xe7\xbd\x91\xe7\xbb\x9cActor\n        self.dense1 = layers.Dense(128, activation=\'relu\')\n        self.policy_logits = layers.Dense(action_size)\n        # V\xe7\xbd\x91\xe7\xbb\x9cCritic\n        self.dense2 = layers.Dense(128, activation=\'relu\')\n        self.values = layers.Dense(1)\n\n    def call(self, inputs):\n        # \xe8\x8e\xb7\xe5\xbe\x97\xe7\xad\x96\xe7\x95\xa5\xe5\x88\x86\xe5\xb8\x83Pi(a|s)\n        x = self.dense1(inputs)\n        logits = self.policy_logits(x)\n        # \xe8\x8e\xb7\xe5\xbe\x97v(s)\n        v = self.dense2(inputs)\n        values = self.values(v)\n        return logits, values\n\n\ndef record(episode,\n           episode_reward,\n           worker_idx,\n           global_ep_reward,\n           result_queue,\n           total_loss,\n           num_steps):\n    # \xe7\xbb\x9f\xe8\xae\xa1\xe5\xb7\xa5\xe5\x85\xb7\xe5\x87\xbd\xe6\x95\xb0\n    if global_ep_reward == 0:\n        global_ep_reward = episode_reward\n    else:\n        global_ep_reward = global_ep_reward * 0.99 + episode_reward * 0.01\n    print(\n        f""{episode} | ""\n        f""Average Reward: {int(global_ep_reward)} | ""\n        f""Episode Reward: {int(episode_reward)} | ""\n        f""Loss: {int(total_loss / float(num_steps) * 1000) / 1000} | ""\n        f""Steps: {num_steps} | ""\n        f""Worker: {worker_idx}""\n    )\n    result_queue.put(global_ep_reward) # \xe4\xbf\x9d\xe5\xad\x98\xe5\x9b\x9e\xe6\x8a\xa5\xef\xbc\x8c\xe4\xbc\xa0\xe7\xbb\x99\xe4\xb8\xbb\xe7\xba\xbf\xe7\xa8\x8b\n    return global_ep_reward\n\nclass Memory:\n    def __init__(self):\n        self.states = []\n        self.actions = []\n        self.rewards = []\n\n    def store(self, state, action, reward):\n        self.states.append(state)\n        self.actions.append(action)\n        self.rewards.append(reward)\n\n    def clear(self):\n        self.states = []\n        self.actions = []\n        self.rewards = []\n\nclass Agent:\n    # \xe6\x99\xba\xe8\x83\xbd\xe4\xbd\x93\xef\xbc\x8c\xe5\x8c\x85\xe5\x90\xab\xe4\xba\x86\xe4\xb8\xad\xe5\xa4\xae\xe5\x8f\x82\xe6\x95\xb0\xe7\xbd\x91\xe7\xbb\x9cserver\n    def __init__(self):\n        # server\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xef\xbc\x8cclient\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xef\xbc\x8c\xe7\x9b\xb4\xe6\x8e\xa5\xe4\xbb\x8eserver\xe6\x8b\x89\xe5\x8f\x96\xe5\x8f\x82\xe6\x95\xb0\n        self.opt = optimizers.Adam(1e-3)\n        # \xe4\xb8\xad\xe5\xa4\xae\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe7\xb1\xbb\xe4\xbc\xbc\xe4\xba\x8e\xe5\x8f\x82\xe6\x95\xb0\xe6\x9c\x8d\xe5\x8a\xa1\xe5\x99\xa8\n        self.server = ActorCritic(4, 2) # \xe7\x8a\xb6\xe6\x80\x81\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x8c\xe5\x8a\xa8\xe4\xbd\x9c\xe6\x95\xb0\xe9\x87\x8f\n        self.server(tf.random.normal((2, 4)))\n    def train(self):\n        res_queue = Queue() # \xe5\x85\xb1\xe4\xba\xab\xe9\x98\x9f\xe5\x88\x97\n        # \xe5\x88\x9b\xe5\xbb\xba\xe5\x90\x84\xe4\xb8\xaa\xe4\xba\xa4\xe4\xba\x92\xe7\x8e\xaf\xe5\xa2\x83\n        workers = [Worker(self.server, self.opt, res_queue, i)\n                   for i in range(multiprocessing.cpu_count())]\n        for i, worker in enumerate(workers):\n            print(""Starting worker {}"".format(i))\n            worker.start()\n        # \xe7\xbb\x9f\xe8\xae\xa1\xe5\xb9\xb6\xe7\xbb\x98\xe5\x88\xb6\xe6\x80\xbb\xe5\x9b\x9e\xe6\x8a\xa5\xe6\x9b\xb2\xe7\xba\xbf\n        returns = []\n        while True:\n            reward = res_queue.get()\n            if reward is not None:\n                returns.append(reward)\n            else: # \xe7\xbb\x93\xe6\x9d\x9f\xe6\xa0\x87\xe5\xbf\x97\n                break\n        [w.join() for w in workers] # \xe7\xad\x89\xe5\xbe\x85\xe7\xba\xbf\xe7\xa8\x8b\xe9\x80\x80\xe5\x87\xba \n\n        print(returns)\n        plt.figure()\n        plt.plot(np.arange(len(returns)), returns)\n        # plt.plot(np.arange(len(moving_average_rewards)), np.array(moving_average_rewards), \'s\')\n        plt.xlabel(\'\xe5\x9b\x9e\xe5\x90\x88\xe6\x95\xb0\')\n        plt.ylabel(\'\xe6\x80\xbb\xe5\x9b\x9e\xe6\x8a\xa5\')\n        plt.savefig(\'a3c-tf-cartpole.svg\')\n\n\nclass Worker(threading.Thread): \n    def __init__(self,  server, opt, result_queue, idx):\n        super(Worker, self).__init__()\n        self.result_queue = result_queue # \xe5\x85\xb1\xe4\xba\xab\xe9\x98\x9f\xe5\x88\x97\n        self.server = server # \xe4\xb8\xad\xe5\xa4\xae\xe6\xa8\xa1\xe5\x9e\x8b\n        self.opt = opt # \xe4\xb8\xad\xe5\xa4\xae\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\n        self.client = ActorCritic(4, 2) # \xe7\xba\xbf\xe7\xa8\x8b\xe7\xa7\x81\xe6\x9c\x89\xe7\xbd\x91\xe7\xbb\x9c\n        self.worker_idx = idx # \xe7\xba\xbf\xe7\xa8\x8bid\n        self.env = gym.make(\'CartPole-v1\').unwrapped\n        self.ep_loss = 0.0\n\n    def run(self): \n        mem = Memory() # \xe6\xaf\x8f\xe4\xb8\xaaworker\xe8\x87\xaa\xe5\xb7\xb1\xe7\xbb\xb4\xe6\x8a\xa4\xe4\xb8\x80\xe4\xb8\xaamemory\n        for epi_counter in range(500): # \xe6\x9c\xaa\xe8\xbe\xbe\xe5\x88\xb0\xe6\x9c\x80\xe5\xa4\xa7\xe5\x9b\x9e\xe5\x90\x88\xe6\x95\xb0\n            current_state = self.env.reset() # \xe5\xa4\x8d\xe4\xbd\x8dclient\xe6\xb8\xb8\xe6\x88\x8f\xe7\x8a\xb6\xe6\x80\x81\n            mem.clear()\n            ep_reward = 0.\n            ep_steps = 0  \n            done = False\n            while not done:\n                # \xe8\x8e\xb7\xe5\xbe\x97Pi(a|s),\xe6\x9c\xaa\xe7\xbb\x8fsoftmax\n                logits, _ = self.client(tf.constant(current_state[None, :],\n                                         dtype=tf.float32))\n                probs = tf.nn.softmax(logits)\n                # \xe9\x9a\x8f\xe6\x9c\xba\xe9\x87\x87\xe6\xa0\xb7\xe5\x8a\xa8\xe4\xbd\x9c\n                action = np.random.choice(2, p=probs.numpy()[0])\n                new_state, reward, done, _ = self.env.step(action) # \xe4\xba\xa4\xe4\xba\x92 \n                ep_reward += reward # \xe7\xb4\xaf\xe5\x8a\xa0\xe5\xa5\x96\xe5\x8a\xb1\n                mem.store(current_state, action, reward) # \xe8\xae\xb0\xe5\xbd\x95\n                ep_steps += 1 # \xe8\xae\xa1\xe7\xae\x97\xe5\x9b\x9e\xe5\x90\x88\xe6\xad\xa5\xe6\x95\xb0\n                current_state = new_state # \xe5\x88\xb7\xe6\x96\xb0\xe7\x8a\xb6\xe6\x80\x81 \n\n                if ep_steps >= 500 or done: # \xe6\x9c\x80\xe9\x95\xbf\xe6\xad\xa5\xe6\x95\xb0500\n                    # \xe8\xae\xa1\xe7\xae\x97\xe5\xbd\x93\xe5\x89\x8dclient\xe4\xb8\x8a\xe7\x9a\x84\xe8\xaf\xaf\xe5\xb7\xae\n                    with tf.GradientTape() as tape:\n                        total_loss = self.compute_loss(done, new_state, mem) \n                    # \xe8\xae\xa1\xe7\xae\x97\xe8\xaf\xaf\xe5\xb7\xae\n                    grads = tape.gradient(total_loss, self.client.trainable_weights)\n                    # \xe6\xa2\xaf\xe5\xba\xa6\xe6\x8f\x90\xe4\xba\xa4\xe5\x88\xb0server\xef\xbc\x8c\xe5\x9c\xa8server\xe4\xb8\x8a\xe6\x9b\xb4\xe6\x96\xb0\xe6\xa2\xaf\xe5\xba\xa6\n                    self.opt.apply_gradients(zip(grads,\n                                                 self.server.trainable_weights))\n                    # \xe4\xbb\x8eserver\xe6\x8b\x89\xe5\x8f\x96\xe6\x9c\x80\xe6\x96\xb0\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n                    self.client.set_weights(self.server.get_weights())\n                    mem.clear() # \xe6\xb8\x85\xe7\xa9\xbaMemory \n                    # \xe7\xbb\x9f\xe8\xae\xa1\xe6\xad\xa4\xe5\x9b\x9e\xe5\x90\x88\xe5\x9b\x9e\xe6\x8a\xa5\n                    self.result_queue.put(ep_reward)\n                    print(self.worker_idx, ep_reward)\n                    break\n        self.result_queue.put(None) # \xe7\xbb\x93\xe6\x9d\x9f\xe7\xba\xbf\xe7\xa8\x8b\n\n    def compute_loss(self,\n                     done,\n                     new_state,\n                     memory,\n                     gamma=0.99):\n        if done:\n            reward_sum = 0. # \xe7\xbb\x88\xe6\xad\xa2\xe7\x8a\xb6\xe6\x80\x81\xe7\x9a\x84v(\xe7\xbb\x88\xe6\xad\xa2)=0\n        else:\n            reward_sum = self.client(tf.constant(new_state[None, :],\n                                     dtype=tf.float32))[-1].numpy()[0]\n        # \xe7\xbb\x9f\xe8\xae\xa1\xe6\x8a\x98\xe6\x89\xa3\xe5\x9b\x9e\xe6\x8a\xa5\n        discounted_rewards = []\n        for reward in memory.rewards[::-1]:  # reverse buffer r\n            reward_sum = reward + gamma * reward_sum\n            discounted_rewards.append(reward_sum)\n        discounted_rewards.reverse()\n        # \xe8\x8e\xb7\xe5\x8f\x96\xe7\x8a\xb6\xe6\x80\x81\xe7\x9a\x84Pi(a|s)\xe5\x92\x8cv(s)\n        logits, values = self.client(tf.constant(np.vstack(memory.states),\n                                 dtype=tf.float32))\n        # \xe8\xae\xa1\xe7\xae\x97advantage = R() - v(s)\n        advantage = tf.constant(np.array(discounted_rewards)[:, None],\n                                         dtype=tf.float32) - values\n        # Critic\xe7\xbd\x91\xe7\xbb\x9c\xe6\x8d\x9f\xe5\xa4\xb1\n        value_loss = advantage ** 2\n        # \xe7\xad\x96\xe7\x95\xa5\xe6\x8d\x9f\xe5\xa4\xb1\n        policy = tf.nn.softmax(logits)\n        policy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n                        labels=memory.actions, logits=logits)\n        # \xe8\xae\xa1\xe7\xae\x97\xe7\xad\x96\xe7\x95\xa5\xe7\xbd\x91\xe7\xbb\x9c\xe6\x8d\x9f\xe5\xa4\xb1\xe6\x97\xb6\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x8d\xe4\xbc\x9a\xe8\xae\xa1\xe7\xae\x97V\xe7\xbd\x91\xe7\xbb\x9c\n        policy_loss = policy_loss * tf.stop_gradient(advantage)\n        # Entropy Bonus\n        entropy = tf.nn.softmax_cross_entropy_with_logits(labels=policy,\n                                                          logits=logits)\n        policy_loss = policy_loss - 0.01 * entropy\n        # \xe8\x81\x9a\xe5\x90\x88\xe5\x90\x84\xe4\xb8\xaa\xe8\xaf\xaf\xe5\xb7\xae\n        total_loss = tf.reduce_mean((0.5 * value_loss + policy_loss))\n        return total_loss\n\n\nif __name__ == \'__main__\':\n    master = Agent()\n    master.train()\n'"
ch14-强化学习/dqn_tf.py,19,"b'import collections\nimport random\nimport gym,os\nimport  numpy as np\nimport  tensorflow as tf\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers,optimizers,losses\n\nenv = gym.make(\'CartPole-v1\')  # \xe5\x88\x9b\xe5\xbb\xba\xe6\xb8\xb8\xe6\x88\x8f\xe7\x8e\xaf\xe5\xa2\x83\nenv.seed(1234)\ntf.random.set_seed(1234)\nnp.random.seed(1234)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nassert tf.__version__.startswith(\'2.\')\n\n# Hyperparameters\nlearning_rate = 0.0002\ngamma = 0.99\nbuffer_limit = 50000\nbatch_size = 32\n\n\nclass ReplayBuffer():\n    # \xe7\xbb\x8f\xe9\xaa\x8c\xe5\x9b\x9e\xe6\x94\xbe\xe6\xb1\xa0\n    def __init__(self):\n        # \xe5\x8f\x8c\xe5\x90\x91\xe9\x98\x9f\xe5\x88\x97\n        self.buffer = collections.deque(maxlen=buffer_limit)\n\n    def put(self, transition):\n        self.buffer.append(transition)\n\n    def sample(self, n):\n        # \xe4\xbb\x8e\xe5\x9b\x9e\xe6\x94\xbe\xe6\xb1\xa0\xe9\x87\x87\xe6\xa0\xb7n\xe4\xb8\xaa5\xe5\x85\x83\xe7\xbb\x84\n        mini_batch = random.sample(self.buffer, n)\n        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n        # \xe6\x8c\x89\xe7\xb1\xbb\xe5\x88\xab\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x95\xb4\xe7\x90\x86\n        for transition in mini_batch:\n            s, a, r, s_prime, done_mask = transition\n            s_lst.append(s)\n            a_lst.append([a])\n            r_lst.append([r])\n            s_prime_lst.append(s_prime)\n            done_mask_lst.append([done_mask])\n        # \xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90Tensor\n        return tf.constant(s_lst, dtype=tf.float32),\\\n                      tf.constant(a_lst, dtype=tf.int32), \\\n                      tf.constant(r_lst, dtype=tf.float32), \\\n                      tf.constant(s_prime_lst, dtype=tf.float32), \\\n                      tf.constant(done_mask_lst, dtype=tf.float32)\n\n\n    def size(self):\n        return len(self.buffer)\n\n\nclass Qnet(keras.Model):\n    def __init__(self):\n        # \xe5\x88\x9b\xe5\xbb\xbaQ\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe8\xbe\x93\xe5\x85\xa5\xe4\xb8\xba\xe7\x8a\xb6\xe6\x80\x81\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe4\xb8\xba\xe5\x8a\xa8\xe4\xbd\x9c\xe7\x9a\x84Q\xe5\x80\xbc\n        super(Qnet, self).__init__()\n        self.fc1 = layers.Dense(256, kernel_initializer=\'he_normal\')\n        self.fc2 = layers.Dense(256, kernel_initializer=\'he_normal\')\n        self.fc3 = layers.Dense(2, kernel_initializer=\'he_normal\')\n\n    def call(self, x, training=None):\n        x = tf.nn.relu(self.fc1(x))\n        x = tf.nn.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n    def sample_action(self, s, epsilon):\n        # \xe9\x80\x81\xe5\x85\xa5\xe7\x8a\xb6\xe6\x80\x81\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x8c\xe8\x8e\xb7\xe5\x8f\x96\xe7\xad\x96\xe7\x95\xa5: [4]\n        s = tf.constant(s, dtype=tf.float32)\n        # s: [4] => [1,4]\n        s = tf.expand_dims(s, axis=0)\n        out = self(s)[0]\n        coin = random.random()\n        # \xe7\xad\x96\xe7\x95\xa5\xe6\x94\xb9\xe8\xbf\x9b\xef\xbc\x9ae-\xe8\xb4\xaa\xe5\xbf\x83\xe6\x96\xb9\xe5\xbc\x8f\n        if coin < epsilon:\n            # epsilon\xe5\xa4\xa7\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe5\x8f\x96\n            return random.randint(0, 1)\n        else:  # \xe9\x80\x89\xe6\x8b\xa9Q\xe5\x80\xbc\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe5\x8a\xa8\xe4\xbd\x9c\n            return int(tf.argmax(out))\n\n\ndef train(q, q_target, memory, optimizer):\n    # \xe9\x80\x9a\xe8\xbf\x87Q\xe7\xbd\x91\xe7\xbb\x9c\xe5\x92\x8c\xe5\xbd\xb1\xe5\xad\x90\xe7\xbd\x91\xe7\xbb\x9c\xe6\x9d\xa5\xe6\x9e\x84\xe9\x80\xa0\xe8\xb4\x9d\xe5\xb0\x94\xe6\x9b\xbc\xe6\x96\xb9\xe7\xa8\x8b\xe7\x9a\x84\xe8\xaf\xaf\xe5\xb7\xae\xef\xbc\x8c\n    # \xe5\xb9\xb6\xe5\x8f\xaa\xe6\x9b\xb4\xe6\x96\xb0Q\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe5\xbd\xb1\xe5\xad\x90\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe6\x9b\xb4\xe6\x96\xb0\xe4\xbc\x9a\xe6\xbb\x9e\xe5\x90\x8eQ\xe7\xbd\x91\xe7\xbb\x9c\n    huber = losses.Huber()\n    for i in range(10):  # \xe8\xae\xad\xe7\xbb\x8310\xe6\xac\xa1\n        # \xe4\xbb\x8e\xe7\xbc\x93\xe5\x86\xb2\xe6\xb1\xa0\xe9\x87\x87\xe6\xa0\xb7\n        s, a, r, s_prime, done_mask = memory.sample(batch_size)\n        with tf.GradientTape() as tape:\n            # s: [b, 4]\n            q_out = q(s)  # \xe5\xbe\x97\xe5\x88\xb0Q(s,a)\xe7\x9a\x84\xe5\x88\x86\xe5\xb8\x83\n            # \xe7\x94\xb1\xe4\xba\x8eTF\xe7\x9a\x84gather_nd\xe4\xb8\x8epytorch\xe7\x9a\x84gather\xe5\x8a\x9f\xe8\x83\xbd\xe4\xb8\x8d\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe6\x9e\x84\xe9\x80\xa0\n            # gather_nd\xe9\x9c\x80\xe8\xa6\x81\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8cindices:[b, 2]\n            # pi_a = pi.gather(1, a) # pytorch\xe5\x8f\xaa\xe9\x9c\x80\xe8\xa6\x81\xe4\xb8\x80\xe8\xa1\x8c\xe5\x8d\xb3\xe5\x8f\xaf\xe5\xae\x9e\xe7\x8e\xb0\n            indices = tf.expand_dims(tf.range(a.shape[0]), axis=1)\n            indices = tf.concat([indices, a], axis=1)\n            q_a = tf.gather_nd(q_out, indices) # \xe5\x8a\xa8\xe4\xbd\x9c\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\xe5\x80\xbc, [b]\n            q_a = tf.expand_dims(q_a, axis=1) # [b]=> [b,1]\n            # \xe5\xbe\x97\xe5\x88\xb0Q(s\',a)\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\xef\xbc\x8c\xe5\xae\x83\xe6\x9d\xa5\xe8\x87\xaa\xe5\xbd\xb1\xe5\xad\x90\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x81 [b,4]=>[b,2]=>[b,1]\n            max_q_prime = tf.reduce_max(q_target(s_prime),axis=1,keepdims=True)\n            # \xe6\x9e\x84\xe9\x80\xa0Q(s,a_t)\xe7\x9a\x84\xe7\x9b\xae\xe6\xa0\x87\xe5\x80\xbc\xef\xbc\x8c\xe6\x9d\xa5\xe8\x87\xaa\xe8\xb4\x9d\xe5\xb0\x94\xe6\x9b\xbc\xe6\x96\xb9\xe7\xa8\x8b\n            target = r + gamma * max_q_prime * done_mask\n            # \xe8\xae\xa1\xe7\xae\x97Q(s,a_t)\xe4\xb8\x8e\xe7\x9b\xae\xe6\xa0\x87\xe5\x80\xbc\xe7\x9a\x84\xe8\xaf\xaf\xe5\xb7\xae\n            loss = huber(q_a, target)\n        # \xe6\x9b\xb4\xe6\x96\xb0\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe4\xbd\xbf\xe5\xbe\x97Q(s,a_t)\xe4\xbc\xb0\xe8\xae\xa1\xe7\xac\xa6\xe5\x90\x88\xe8\xb4\x9d\xe5\xb0\x94\xe6\x9b\xbc\xe6\x96\xb9\xe7\xa8\x8b\n        grads = tape.gradient(loss, q.trainable_variables)\n        # for p in grads:\n        #     print(tf.norm(p))\n        # print(grads)\n        optimizer.apply_gradients(zip(grads, q.trainable_variables))\n\n\ndef main():\n    env = gym.make(\'CartPole-v1\')  # \xe5\x88\x9b\xe5\xbb\xba\xe7\x8e\xaf\xe5\xa2\x83\n    q = Qnet()  # \xe5\x88\x9b\xe5\xbb\xbaQ\xe7\xbd\x91\xe7\xbb\x9c\n    q_target = Qnet()  # \xe5\x88\x9b\xe5\xbb\xba\xe5\xbd\xb1\xe5\xad\x90\xe7\xbd\x91\xe7\xbb\x9c\n    q.build(input_shape=(2,4))\n    q_target.build(input_shape=(2,4))\n    for src, dest in zip(q.variables, q_target.variables):\n        dest.assign(src) # \xe5\xbd\xb1\xe5\xad\x90\xe7\xbd\x91\xe7\xbb\x9c\xe6\x9d\x83\xe5\x80\xbc\xe6\x9d\xa5\xe8\x87\xaaQ\n    memory = ReplayBuffer()  # \xe5\x88\x9b\xe5\xbb\xba\xe5\x9b\x9e\xe6\x94\xbe\xe6\xb1\xa0\n\n    print_interval = 20\n    score = 0.0\n    optimizer = optimizers.Adam(lr=learning_rate)\n\n    for n_epi in range(10000):  # \xe8\xae\xad\xe7\xbb\x83\xe6\xac\xa1\xe6\x95\xb0\n        # epsilon\xe6\xa6\x82\xe7\x8e\x87\xe4\xb9\x9f\xe4\xbc\x9a8%\xe5\x88\xb01%\xe8\xa1\xb0\xe5\x87\x8f\xef\xbc\x8c\xe8\xb6\x8a\xe5\x88\xb0\xe5\x90\x8e\xe9\x9d\xa2\xe8\xb6\x8a\xe4\xbd\xbf\xe7\x94\xa8Q\xe5\x80\xbc\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe5\x8a\xa8\xe4\xbd\x9c\n        epsilon = max(0.01, 0.08 - 0.01 * (n_epi / 200))\n        s = env.reset()  # \xe5\xa4\x8d\xe4\xbd\x8d\xe7\x8e\xaf\xe5\xa2\x83\n        for t in range(600):  # \xe4\xb8\x80\xe4\xb8\xaa\xe5\x9b\x9e\xe5\x90\x88\xe6\x9c\x80\xe5\xa4\xa7\xe6\x97\xb6\xe9\x97\xb4\xe6\x88\xb3\n            # if n_epi>1000:\n            #     env.render()\n            # \xe6\xa0\xb9\xe6\x8d\xae\xe5\xbd\x93\xe5\x89\x8dQ\xe7\xbd\x91\xe7\xbb\x9c\xe6\x8f\x90\xe5\x8f\x96\xe7\xad\x96\xe7\x95\xa5\xef\xbc\x8c\xe5\xb9\xb6\xe6\x94\xb9\xe8\xbf\x9b\xe7\xad\x96\xe7\x95\xa5\n            a = q.sample_action(s, epsilon)\n            # \xe4\xbd\xbf\xe7\x94\xa8\xe6\x94\xb9\xe8\xbf\x9b\xe7\x9a\x84\xe7\xad\x96\xe7\x95\xa5\xe4\xb8\x8e\xe7\x8e\xaf\xe5\xa2\x83\xe4\xba\xa4\xe4\xba\x92\n            s_prime, r, done, info = env.step(a)\n            done_mask = 0.0 if done else 1.0  # \xe7\xbb\x93\xe6\x9d\x9f\xe6\xa0\x87\xe5\xbf\x97\xe6\x8e\xa9\xe7\xa0\x81\n            # \xe4\xbf\x9d\xe5\xad\x985\xe5\x85\x83\xe7\xbb\x84\n            memory.put((s, a, r / 100.0, s_prime, done_mask))\n            s = s_prime  # \xe5\x88\xb7\xe6\x96\xb0\xe7\x8a\xb6\xe6\x80\x81\n            score += r  # \xe8\xae\xb0\xe5\xbd\x95\xe6\x80\xbb\xe5\x9b\x9e\xe6\x8a\xa5\n            if done:  # \xe5\x9b\x9e\xe5\x90\x88\xe7\xbb\x93\xe6\x9d\x9f\n                break\n\n        if memory.size() > 2000:  # \xe7\xbc\x93\xe5\x86\xb2\xe6\xb1\xa0\xe5\x8f\xaa\xe6\x9c\x89\xe5\xa4\xa7\xe4\xba\x8e2000\xe5\xb0\xb1\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xae\xad\xe7\xbb\x83\n            train(q, q_target, memory, optimizer)\n\n        if n_epi % print_interval == 0 and n_epi != 0:\n            for src, dest in zip(q.variables, q_target.variables):\n                dest.assign(src)  # \xe5\xbd\xb1\xe5\xad\x90\xe7\xbd\x91\xe7\xbb\x9c\xe6\x9d\x83\xe5\x80\xbc\xe6\x9d\xa5\xe8\x87\xaaQ\n            print(""# of episode :{}, avg score : {:.1f}, buffer size : {}, "" \\\n                  ""epsilon : {:.1f}%"" \\\n                  .format(n_epi, score / print_interval, memory.size(), epsilon * 100))\n            score = 0.0\n    env.close()\n\n\nif __name__ == \'__main__\':\n    main()'"
ch14-强化学习/ppo_tf_cartpole.py,29,"b'import  matplotlib\nfrom \tmatplotlib import pyplot as plt\nmatplotlib.rcParams[\'font.size\'] = 18\nmatplotlib.rcParams[\'figure.titlesize\'] = 18\nmatplotlib.rcParams[\'figure.figsize\'] = [9, 7]\nmatplotlib.rcParams[\'font.family\'] = [\'KaiTi\']\nmatplotlib.rcParams[\'axes.unicode_minus\']=False\n\nplt.figure()\n\nimport  gym,os\nimport  numpy as np\nimport  tensorflow as tf\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers,optimizers,losses\nfrom    collections import namedtuple\nfrom    torch.utils.data import SubsetRandomSampler,BatchSampler\n\nenv = gym.make(\'CartPole-v1\')  # \xe5\x88\x9b\xe5\xbb\xba\xe6\xb8\xb8\xe6\x88\x8f\xe7\x8e\xaf\xe5\xa2\x83\nenv.seed(2222)\ntf.random.set_seed(2222)\nnp.random.seed(2222)\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\nassert tf.__version__.startswith(\'2.\')\n\n\n\ngamma = 0.98 # \xe6\xbf\x80\xe5\x8a\xb1\xe8\xa1\xb0\xe5\x87\x8f\xe5\x9b\xa0\xe5\xad\x90\nepsilon = 0.2 # PPO\xe8\xaf\xaf\xe5\xb7\xae\xe8\xb6\x85\xe5\x8f\x82\xe6\x95\xb00.8~1.2\nbatch_size = 32 # batch size\n\n\n# \xe5\x88\x9b\xe5\xbb\xba\xe6\xb8\xb8\xe6\x88\x8f\xe7\x8e\xaf\xe5\xa2\x83\nenv = gym.make(\'CartPole-v0\').unwrapped\nTransition = namedtuple(\'Transition\', [\'state\', \'action\', \'a_log_prob\', \'reward\', \'next_state\'])\n\n\nclass Actor(keras.Model):\n    def __init__(self):\n        super(Actor, self).__init__()\n        # \xe7\xad\x96\xe7\x95\xa5\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe4\xb9\x9f\xe5\x8f\xabActor\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe4\xb8\xba\xe6\xa6\x82\xe7\x8e\x87\xe5\x88\x86\xe5\xb8\x83pi(a|s)\n        self.fc1 = layers.Dense(100, kernel_initializer=\'he_normal\')\n        self.fc2 = layers.Dense(2, kernel_initializer=\'he_normal\')\n\n    def call(self, inputs):\n        x = tf.nn.relu(self.fc1(inputs))\n        x = self.fc2(x)\n        x = tf.nn.softmax(x, axis=1) # \xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90\xe6\xa6\x82\xe7\x8e\x87\n        return x\n\nclass Critic(keras.Model):\n    def __init__(self):\n        super(Critic, self).__init__()\n        # \xe5\x81\x8f\xe7\xbd\xaeb\xe7\x9a\x84\xe4\xbc\xb0\xe5\x80\xbc\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe4\xb9\x9f\xe5\x8f\xabCritic\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe4\xb8\xbav(s)\n        self.fc1 = layers.Dense(100, kernel_initializer=\'he_normal\')\n        self.fc2 = layers.Dense(1, kernel_initializer=\'he_normal\')\n\n    def call(self, inputs):\n        x = tf.nn.relu(self.fc1(inputs))\n        x = self.fc2(x)\n        return x\n\n\n\n\nclass PPO():\n    # PPO\xe7\xae\x97\xe6\xb3\x95\xe4\xb8\xbb\xe4\xbd\x93\n    def __init__(self):\n        super(PPO, self).__init__()\n        self.actor = Actor() # \xe5\x88\x9b\xe5\xbb\xbaActor\xe7\xbd\x91\xe7\xbb\x9c\n        self.critic = Critic() # \xe5\x88\x9b\xe5\xbb\xbaCritic\xe7\xbd\x91\xe7\xbb\x9c\n        self.buffer = [] # \xe6\x95\xb0\xe6\x8d\xae\xe7\xbc\x93\xe5\x86\xb2\xe6\xb1\xa0\n        self.actor_optimizer = optimizers.Adam(1e-3) # Actor\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\n        self.critic_optimizer = optimizers.Adam(3e-3) # Critic\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\n\n    def select_action(self, s):\n        # \xe9\x80\x81\xe5\x85\xa5\xe7\x8a\xb6\xe6\x80\x81\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x8c\xe8\x8e\xb7\xe5\x8f\x96\xe7\xad\x96\xe7\x95\xa5: [4]\n        s = tf.constant(s, dtype=tf.float32)\n        # s: [4] => [1,4]\n        s = tf.expand_dims(s, axis=0)\n        # \xe8\x8e\xb7\xe5\x8f\x96\xe7\xad\x96\xe7\x95\xa5\xe5\x88\x86\xe5\xb8\x83: [1, 2]\n        prob = self.actor(s)\n        # \xe4\xbb\x8e\xe7\xb1\xbb\xe5\x88\xab\xe5\x88\x86\xe5\xb8\x83\xe4\xb8\xad\xe9\x87\x87\xe6\xa0\xb71\xe4\xb8\xaa\xe5\x8a\xa8\xe4\xbd\x9c, shape: [1]\n        a = tf.random.categorical(tf.math.log(prob), 1)[0]\n        a = int(a)  # Tensor\xe8\xbd\xac\xe6\x95\xb0\xe5\xad\x97\n        return a, float(prob[0][a]) # \xe8\xbf\x94\xe5\x9b\x9e\xe5\x8a\xa8\xe4\xbd\x9c\xe5\x8f\x8a\xe5\x85\xb6\xe6\xa6\x82\xe7\x8e\x87\n\n    def get_value(self, s):\n        # \xe9\x80\x81\xe5\x85\xa5\xe7\x8a\xb6\xe6\x80\x81\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x8c\xe8\x8e\xb7\xe5\x8f\x96\xe7\xad\x96\xe7\x95\xa5: [4]\n        s = tf.constant(s, dtype=tf.float32)\n        # s: [4] => [1,4]\n        s = tf.expand_dims(s, axis=0)\n        # \xe8\x8e\xb7\xe5\x8f\x96\xe7\xad\x96\xe7\x95\xa5\xe5\x88\x86\xe5\xb8\x83: [1, 2]\n        v = self.critic(s)[0]\n        return float(v) # \xe8\xbf\x94\xe5\x9b\x9ev(s)\n\n    def store_transition(self, transition):\n        # \xe5\xad\x98\xe5\x82\xa8\xe9\x87\x87\xe6\xa0\xb7\xe6\x95\xb0\xe6\x8d\xae\n        self.buffer.append(transition)\n\n    def optimize(self):\n        # \xe4\xbc\x98\xe5\x8c\x96\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xbb\xe5\x87\xbd\xe6\x95\xb0\n        # \xe4\xbb\x8e\xe7\xbc\x93\xe5\xad\x98\xe4\xb8\xad\xe5\x8f\x96\xe5\x87\xba\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90Tensor\n        state = tf.constant([t.state for t in self.buffer], dtype=tf.float32)\n        action = tf.constant([t.action for t in self.buffer], dtype=tf.int32)\n        action = tf.reshape(action,[-1,1])\n        reward = [t.reward for t in self.buffer]\n        old_action_log_prob = tf.constant([t.a_log_prob for t in self.buffer], dtype=tf.float32)\n        old_action_log_prob = tf.reshape(old_action_log_prob, [-1,1])\n        # \xe9\x80\x9a\xe8\xbf\x87MC\xe6\x96\xb9\xe6\xb3\x95\xe5\xbe\xaa\xe7\x8e\xaf\xe8\xae\xa1\xe7\xae\x97R(st)\n        R = 0\n        Rs = []\n        for r in reward[::-1]:\n            R = r + gamma * R\n            Rs.insert(0, R)\n        Rs = tf.constant(Rs, dtype=tf.float32)\n        # \xe5\xaf\xb9\xe7\xbc\x93\xe5\x86\xb2\xe6\xb1\xa0\xe6\x95\xb0\xe6\x8d\xae\xe5\xa4\xa7\xe8\x87\xb4\xe8\xbf\xad\xe4\xbb\xa310\xe9\x81\x8d\n        for _ in range(round(10*len(self.buffer)/batch_size)):\n            # \xe9\x9a\x8f\xe6\x9c\xba\xe4\xbb\x8e\xe7\xbc\x93\xe5\x86\xb2\xe6\xb1\xa0\xe9\x87\x87\xe6\xa0\xb7batch size\xe5\xa4\xa7\xe5\xb0\x8f\xe6\xa0\xb7\xe6\x9c\xac\n            index = np.random.choice(np.arange(len(self.buffer)), batch_size, replace=False)\n            # \xe6\x9e\x84\xe5\xbb\xba\xe6\xa2\xaf\xe5\xba\xa6\xe8\xb7\x9f\xe8\xb8\xaa\xe7\x8e\xaf\xe5\xa2\x83\n            with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n                # \xe5\x8f\x96\xe5\x87\xbaR(st)\xef\xbc\x8c[b,1]\n                v_target = tf.expand_dims(tf.gather(Rs, index, axis=0), axis=1)\n                # \xe8\xae\xa1\xe7\xae\x97v(s)\xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\xbc\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf\xe5\x81\x8f\xe7\xbd\xaeb\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x90\x8e\xe9\x9d\xa2\xe4\xbc\x9a\xe4\xbb\x8b\xe7\xbb\x8d\xe4\xb8\xba\xe4\xbb\x80\xe4\xb9\x88\xe5\x86\x99\xe6\x88\x90v\n                v = self.critic(tf.gather(state, index, axis=0))\n                delta = v_target - v # \xe8\xae\xa1\xe7\xae\x97\xe4\xbc\x98\xe5\x8a\xbf\xe5\x80\xbc\n                advantage = tf.stop_gradient(delta) # \xe6\x96\xad\xe5\xbc\x80\xe6\xa2\xaf\xe5\xba\xa6\xe8\xbf\x9e\xe6\x8e\xa5 \n                # \xe7\x94\xb1\xe4\xba\x8eTF\xe7\x9a\x84gather_nd\xe4\xb8\x8epytorch\xe7\x9a\x84gather\xe5\x8a\x9f\xe8\x83\xbd\xe4\xb8\x8d\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe6\x9e\x84\xe9\x80\xa0\n                # gather_nd\xe9\x9c\x80\xe8\xa6\x81\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8cindices:[b, 2]\n                # pi_a = pi.gather(1, a) # pytorch\xe5\x8f\xaa\xe9\x9c\x80\xe8\xa6\x81\xe4\xb8\x80\xe8\xa1\x8c\xe5\x8d\xb3\xe5\x8f\xaf\xe5\xae\x9e\xe7\x8e\xb0\n                a = tf.gather(action, index, axis=0) # \xe5\x8f\x96\xe5\x87\xbabatch\xe7\x9a\x84\xe5\x8a\xa8\xe4\xbd\x9cat\n                # batch\xe7\x9a\x84\xe5\x8a\xa8\xe4\xbd\x9c\xe5\x88\x86\xe5\xb8\x83pi(a|st)\n                pi = self.actor(tf.gather(state, index, axis=0)) \n                indices = tf.expand_dims(tf.range(a.shape[0]), axis=1)\n                indices = tf.concat([indices, a], axis=1)\n                pi_a = tf.gather_nd(pi, indices)  # \xe5\x8a\xa8\xe4\xbd\x9c\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\xe5\x80\xbcpi(at|st), [b]\n                pi_a = tf.expand_dims(pi_a, axis=1)  # [b]=> [b,1] \n                # \xe9\x87\x8d\xe8\xa6\x81\xe6\x80\xa7\xe9\x87\x87\xe6\xa0\xb7\n                ratio = (pi_a / tf.gather(old_action_log_prob, index, axis=0))\n                surr1 = ratio * advantage\n                surr2 = tf.clip_by_value(ratio, 1 - epsilon, 1 + epsilon) * advantage\n                # PPO\xe8\xaf\xaf\xe5\xb7\xae\xe5\x87\xbd\xe6\x95\xb0\n                policy_loss = -tf.reduce_mean(tf.minimum(surr1, surr2))\n                # \xe5\xaf\xb9\xe4\xba\x8e\xe5\x81\x8f\xe7\xbd\xaev\xe6\x9d\xa5\xe8\xaf\xb4\xef\xbc\x8c\xe5\xb8\x8c\xe6\x9c\x9b\xe4\xb8\x8eMC\xe4\xbc\xb0\xe8\xae\xa1\xe7\x9a\x84R(st)\xe8\xb6\x8a\xe6\x8e\xa5\xe8\xbf\x91\xe8\xb6\x8a\xe5\xa5\xbd\n                value_loss = losses.MSE(v_target, v)\n            # \xe4\xbc\x98\xe5\x8c\x96\xe7\xad\x96\xe7\x95\xa5\xe7\xbd\x91\xe7\xbb\x9c\n            grads = tape1.gradient(policy_loss, self.actor.trainable_variables)\n            self.actor_optimizer.apply_gradients(zip(grads, self.actor.trainable_variables))\n            # \xe4\xbc\x98\xe5\x8c\x96\xe5\x81\x8f\xe7\xbd\xae\xe5\x80\xbc\xe7\xbd\x91\xe7\xbb\x9c\n            grads = tape2.gradient(value_loss, self.critic.trainable_variables)\n            self.critic_optimizer.apply_gradients(zip(grads, self.critic.trainable_variables))\n\n        self.buffer = []  # \xe6\xb8\x85\xe7\xa9\xba\xe5\xb7\xb2\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\n\n\ndef main():\n    agent = PPO()\n    returns = [] # \xe7\xbb\x9f\xe8\xae\xa1\xe6\x80\xbb\xe5\x9b\x9e\xe6\x8a\xa5\n    total = 0 # \xe4\xb8\x80\xe6\xae\xb5\xe6\x97\xb6\xe9\x97\xb4\xe5\x86\x85\xe5\xb9\xb3\xe5\x9d\x87\xe5\x9b\x9e\xe6\x8a\xa5\n    for i_epoch in range(500): # \xe8\xae\xad\xe7\xbb\x83\xe5\x9b\x9e\xe5\x90\x88\xe6\x95\xb0\n        state = env.reset() # \xe5\xa4\x8d\xe4\xbd\x8d\xe7\x8e\xaf\xe5\xa2\x83\n        for t in range(500): # \xe6\x9c\x80\xe5\xa4\x9a\xe8\x80\x83\xe8\x99\x91500\xe6\xad\xa5\n            # \xe9\x80\x9a\xe8\xbf\x87\xe6\x9c\x80\xe6\x96\xb0\xe7\xad\x96\xe7\x95\xa5\xe4\xb8\x8e\xe7\x8e\xaf\xe5\xa2\x83\xe4\xba\xa4\xe4\xba\x92\n            action, action_prob = agent.select_action(state)\n            next_state, reward, done, _ = env.step(action)\n            # \xe6\x9e\x84\xe5\xbb\xba\xe6\xa0\xb7\xe6\x9c\xac\xe5\xb9\xb6\xe5\xad\x98\xe5\x82\xa8\n            trans = Transition(state, action, action_prob, reward, next_state)\n            agent.store_transition(trans)\n            state = next_state # \xe5\x88\xb7\xe6\x96\xb0\xe7\x8a\xb6\xe6\x80\x81\n            total += reward # \xe7\xb4\xaf\xe7\xa7\xaf\xe6\xbf\x80\xe5\x8a\xb1\n            if done: # \xe5\x90\x88\xe9\x80\x82\xe7\x9a\x84\xe6\x97\xb6\xe9\x97\xb4\xe7\x82\xb9\xe8\xae\xad\xe7\xbb\x83\xe7\xbd\x91\xe7\xbb\x9c\n                if len(agent.buffer) >= batch_size:\n                    agent.optimize() # \xe8\xae\xad\xe7\xbb\x83\xe7\xbd\x91\xe7\xbb\x9c\n                break\n\n        if i_epoch % 20 == 0: # \xe6\xaf\x8f20\xe4\xb8\xaa\xe5\x9b\x9e\xe5\x90\x88\xe7\xbb\x9f\xe8\xae\xa1\xe4\xb8\x80\xe6\xac\xa1\xe5\xb9\xb3\xe5\x9d\x87\xe5\x9b\x9e\xe6\x8a\xa5\n            returns.append(total/20)\n            total = 0\n            print(i_epoch, returns[-1])\n\n    print(np.array(returns))\n    plt.figure()\n    plt.plot(np.arange(len(returns))*20, np.array(returns))\n    plt.plot(np.arange(len(returns))*20, np.array(returns), \'s\')\n    plt.xlabel(\'\xe5\x9b\x9e\xe5\x90\x88\xe6\x95\xb0\')\n    plt.ylabel(\'\xe6\x80\xbb\xe5\x9b\x9e\xe6\x8a\xa5\')\n    plt.savefig(\'ppo-tf-cartpole.svg\')\n\n\nif __name__ == \'__main__\':\n    main()\n    print(""end"")'"
ch15-自定义数据集/pokemon.py,13,"b'import  os, glob\nimport  random, csv\nimport tensorflow as tf\n\n\n\ndef load_csv(root, filename, name2label):\n    # \xe4\xbb\x8ecsv\xe6\x96\x87\xe4\xbb\xb6\xe8\xbf\x94\xe5\x9b\x9eimages,labels\xe5\x88\x97\xe8\xa1\xa8\n    # root:\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe6\xa0\xb9\xe7\x9b\xae\xe5\xbd\x95\xef\xbc\x8cfilename:csv\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xef\xbc\x8c name2label:\xe7\xb1\xbb\xe5\x88\xab\xe5\x90\x8d\xe7\xbc\x96\xe7\xa0\x81\xe8\xa1\xa8\n    if not os.path.exists(os.path.join(root, filename)):\n        # \xe5\xa6\x82\xe6\x9e\x9ccsv\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\x8d\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe5\x88\x99\xe5\x88\x9b\xe5\xbb\xba\n        images = []\n        for name in name2label.keys(): # \xe9\x81\x8d\xe5\x8e\x86\xe6\x89\x80\xe6\x9c\x89\xe5\xad\x90\xe7\x9b\xae\xe5\xbd\x95\xef\xbc\x8c\xe8\x8e\xb7\xe5\xbe\x97\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\n            # \xe5\x8f\xaa\xe8\x80\x83\xe8\x99\x91\xe5\x90\x8e\xe7\xbc\x80\xe4\xb8\xbapng,jpg,jpeg\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x9a\'pokemon\\\\mewtwo\\\\00001.png\n            images += glob.glob(os.path.join(root, name, \'*.png\'))\n            images += glob.glob(os.path.join(root, name, \'*.jpg\'))\n            images += glob.glob(os.path.join(root, name, \'*.jpeg\'))\n        # \xe6\x89\x93\xe5\x8d\xb0\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xbf\xa1\xe6\x81\xaf\xef\xbc\x9a1167, \'pokemon\\\\bulbasaur\\\\00000000.png\'\n        print(len(images), images)\n        random.shuffle(images) # \xe9\x9a\x8f\xe6\x9c\xba\xe6\x89\x93\xe6\x95\xa3\xe9\xa1\xba\xe5\xba\x8f\n        # \xe5\x88\x9b\xe5\xbb\xbacsv\xe6\x96\x87\xe4\xbb\xb6\xef\xbc\x8c\xe5\xb9\xb6\xe5\xad\x98\xe5\x82\xa8\xe5\x9b\xbe\xe7\x89\x87\xe8\xb7\xaf\xe5\xbe\x84\xe5\x8f\x8a\xe5\x85\xb6label\xe4\xbf\xa1\xe6\x81\xaf\n        with open(os.path.join(root, filename), mode=\'w\', newline=\'\') as f:\n            writer = csv.writer(f)\n            for img in images:  # \'pokemon\\\\bulbasaur\\\\00000000.png\'\n                name = img.split(os.sep)[-2]\n                label = name2label[name]\n                # \'pokemon\\\\bulbasaur\\\\00000000.png\', 0\n                writer.writerow([img, label])\n            print(\'written into csv file:\', filename)\n\n    # \xe6\xad\xa4\xe6\x97\xb6\xe5\xb7\xb2\xe7\xbb\x8f\xe6\x9c\x89csv\xe6\x96\x87\xe4\xbb\xb6\xef\xbc\x8c\xe7\x9b\xb4\xe6\x8e\xa5\xe8\xaf\xbb\xe5\x8f\x96\n    images, labels = [], []\n    with open(os.path.join(root, filename)) as f:\n        reader = csv.reader(f)\n        for row in reader:\n            # \'pokemon\\\\bulbasaur\\\\00000000.png\', 0\n            img, label = row\n            label = int(label)\n            images.append(img)\n            labels.append(label) \n    # \xe8\xbf\x94\xe5\x9b\x9e\xe5\x9b\xbe\xe7\x89\x87\xe8\xb7\xaf\xe5\xbe\x84list\xe5\x92\x8c\xe6\xa0\x87\xe7\xad\xbelist\n    return images, labels\n\n\ndef load_pokemon(root, mode=\'train\'):\n    # \xe5\x88\x9b\xe5\xbb\xba\xe6\x95\xb0\xe5\xad\x97\xe7\xbc\x96\xe7\xa0\x81\xe8\xa1\xa8\n    name2label = {}  # ""sq..."":0\n    # \xe9\x81\x8d\xe5\x8e\x86\xe6\xa0\xb9\xe7\x9b\xae\xe5\xbd\x95\xe4\xb8\x8b\xe7\x9a\x84\xe5\xad\x90\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xef\xbc\x8c\xe5\xb9\xb6\xe6\x8e\x92\xe5\xba\x8f\xef\xbc\x8c\xe4\xbf\x9d\xe8\xaf\x81\xe6\x98\xa0\xe5\xb0\x84\xe5\x85\xb3\xe7\xb3\xbb\xe5\x9b\xba\xe5\xae\x9a\n    for name in sorted(os.listdir(os.path.join(root))):\n        # \xe8\xb7\xb3\xe8\xbf\x87\xe9\x9d\x9e\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\n        if not os.path.isdir(os.path.join(root, name)):\n            continue\n        # \xe7\xbb\x99\xe6\xaf\x8f\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe7\xbc\x96\xe7\xa0\x81\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe5\xad\x97\n        name2label[name] = len(name2label.keys())\n\n    # \xe8\xaf\xbb\xe5\x8f\x96Label\xe4\xbf\xa1\xe6\x81\xaf\n    # [file1,file2,], [3,1]\n    images, labels = load_csv(root, \'images.csv\', name2label)\n\n    if mode == \'train\':  # 60%\n        images = images[:int(0.6 * len(images))]\n        labels = labels[:int(0.6 * len(labels))]\n    elif mode == \'val\':  # 20% = 60%->80%\n        images = images[int(0.6 * len(images)):int(0.8 * len(images))]\n        labels = labels[int(0.6 * len(labels)):int(0.8 * len(labels))]\n    else:  # 20% = 80%->100%\n        images = images[int(0.8 * len(images)):]\n        labels = labels[int(0.8 * len(labels)):]\n\n    return images, labels, name2label\n\n# \xe8\xbf\x99\xe9\x87\x8c\xe7\x9a\x84mean\xe5\x92\x8cstd\xe6\xa0\xb9\xe6\x8d\xae\xe7\x9c\x9f\xe5\xae\x9e\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe8\xae\xa1\xe7\xae\x97\xe8\x8e\xb7\xe5\xbe\x97\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82ImageNet\nimg_mean = tf.constant([0.485, 0.456, 0.406])\nimg_std = tf.constant([0.229, 0.224, 0.225])\ndef normalize(x, mean=img_mean, std=img_std):\n    # \xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\n    # x: [224, 224, 3]\n    # mean: [224, 224, 3], std: [3]\n    x = (x - mean)/std\n    return x\n\ndef denormalize(x, mean=img_mean, std=img_std):\n    # \xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe7\x9a\x84\xe9\x80\x86\xe8\xbf\x87\xe7\xa8\x8b\n    x = x * std + mean\n    return x\n\ndef preprocess(x,y):\n    # x: \xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84List\xef\xbc\x8cy\xef\xbc\x9a\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe6\x95\xb0\xe5\xad\x97\xe7\xbc\x96\xe7\xa0\x81List\n    x = tf.io.read_file(x) # \xe6\xa0\xb9\xe6\x8d\xae\xe8\xb7\xaf\xe5\xbe\x84\xe8\xaf\xbb\xe5\x8f\x96\xe5\x9b\xbe\xe7\x89\x87\n    x = tf.image.decode_jpeg(x, channels=3) # \xe5\x9b\xbe\xe7\x89\x87\xe8\xa7\xa3\xe7\xa0\x81\n    x = tf.image.resize(x, [244, 244]) # \xe5\x9b\xbe\xe7\x89\x87\xe7\xbc\xa9\xe6\x94\xbe\n\n    # \xe6\x95\xb0\xe6\x8d\xae\xe5\xa2\x9e\xe5\xbc\xba\n    # x = tf.image.random_flip_up_down(x)\n    x= tf.image.random_flip_left_right(x) # \xe5\xb7\xa6\xe5\x8f\xb3\xe9\x95\x9c\xe5\x83\x8f\n    x = tf.image.random_crop(x, [224, 224, 3]) # \xe9\x9a\x8f\xe6\x9c\xba\xe8\xa3\x81\xe5\x89\xaa\n    # \xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90\xe5\xbc\xa0\xe9\x87\x8f\n    # x: [0,255]=> 0~1\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    # 0~1 => D(0,1)\n    x = normalize(x) # \xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\n    y = tf.convert_to_tensor(y) # \xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90\xe5\xbc\xa0\xe9\x87\x8f\n\n    return x, y\n\n\ndef main():\n    import  time\n\n\n\n    # \xe5\x8a\xa0\xe8\xbd\xbdpokemon\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe6\x8c\x87\xe5\xae\x9a\xe5\x8a\xa0\xe8\xbd\xbd\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\n    images, labels, table = load_pokemon(\'pokemon\', \'train\')\n    print(\'images:\', len(images), images)\n    print(\'labels:\', len(labels), labels)\n    print(\'table:\', table)\n\n    # images: string path\n    # labels: number\n    db = tf.data.Dataset.from_tensor_slices((images, labels))\n    db = db.shuffle(1000).map(preprocess).batch(32)\n\n    # \xe5\x88\x9b\xe5\xbb\xbaTensorBoard\xe5\xaf\xb9\xe8\xb1\xa1\n    writter = tf.summary.create_file_writer(\'logs\')\n    for step, (x,y) in enumerate(db):\n        # x: [32, 224, 224, 3]\n        # y: [32]\n        with writter.as_default():\n            x = denormalize(x) # \xe5\x8f\x8d\xe5\x90\x91normalize\xef\xbc\x8c\xe6\x96\xb9\xe4\xbe\xbf\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n            # \xe5\x86\x99\xe5\x85\xa5\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xe6\x8d\xae\n            tf.summary.image(\'img\',x,step=step,max_outputs=9)\n            time.sleep(5)\n\n\n\n\nif __name__ == \'__main__\':\n    main()'"
ch15-自定义数据集/resnet.py,9,"b""import  os\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers\n\n\n\ntf.random.set_seed(22)\nnp.random.seed(22)\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nassert tf.__version__.startswith('2.')\n\n\n\nclass ResnetBlock(keras.Model):\n\n    def __init__(self, channels, strides=1):\n        super(ResnetBlock, self).__init__()\n\n        self.channels = channels\n        self.strides = strides\n\n        self.conv1 = layers.Conv2D(channels, 3, strides=strides,\n                                   padding=[[0,0],[1,1],[1,1],[0,0]])\n        self.bn1 = keras.layers.BatchNormalization()\n        self.conv2 = layers.Conv2D(channels, 3, strides=1,\n                                   padding=[[0,0],[1,1],[1,1],[0,0]])\n        self.bn2 = keras.layers.BatchNormalization()\n\n        if strides!=1:\n            self.down_conv = layers.Conv2D(channels, 1, strides=strides, padding='valid')\n            self.down_bn = tf.keras.layers.BatchNormalization()\n\n    def call(self, inputs, training=None):\n        residual = inputs\n\n        x = self.conv1(inputs)\n        x = tf.nn.relu(x)\n        x = self.bn1(x, training=training)\n        x = self.conv2(x)\n        x = tf.nn.relu(x)\n        x = self.bn2(x, training=training)\n\n        # \xe6\xae\x8b\xe5\xb7\xae\xe8\xbf\x9e\xe6\x8e\xa5\n        if self.strides!=1:\n            residual = self.down_conv(inputs)\n            residual = tf.nn.relu(residual)\n            residual = self.down_bn(residual, training=training)\n\n        x = x + residual\n        x = tf.nn.relu(x)\n        return x\n\n\nclass ResNet(keras.Model):\n\n    def __init__(self, num_classes, initial_filters=16, **kwargs):\n        super(ResNet, self).__init__(**kwargs)\n\n        self.stem = layers.Conv2D(initial_filters, 3, strides=3, padding='valid')\n\n        self.blocks = keras.models.Sequential([\n            ResnetBlock(initial_filters * 2, strides=3),\n            ResnetBlock(initial_filters * 2, strides=1),\n            # layers.Dropout(rate=0.5),\n\n            ResnetBlock(initial_filters * 4, strides=3),\n            ResnetBlock(initial_filters * 4, strides=1),\n\n            ResnetBlock(initial_filters * 8, strides=2),\n            ResnetBlock(initial_filters * 8, strides=1),\n\n            ResnetBlock(initial_filters * 16, strides=2),\n            ResnetBlock(initial_filters * 16, strides=1),\n        ])\n\n        self.final_bn = layers.BatchNormalization()\n        self.avg_pool = layers.GlobalMaxPool2D()\n        self.fc = layers.Dense(num_classes)\n\n    def call(self, inputs, training=None):\n        # print('x:',inputs.shape)\n        out = self.stem(inputs)\n        out = tf.nn.relu(out)\n\n        # print('stem:',out.shape)\n\n        out = self.blocks(out, training=training)\n        # print('res:',out.shape)\n\n        out = self.final_bn(out, training=training)\n        # out = tf.nn.relu(out)\n\n        out = self.avg_pool(out)\n\n        # print('avg_pool:',out.shape)\n        out = self.fc(out)\n\n        # print('out:',out.shape)\n\n        return out\n\n\n\ndef main():\n    num_classes = 5\n\n    resnet18 = ResNet(5)\n    resnet18.build(input_shape=(4,224,224,3))\n    resnet18.summary()\n\n\n\n\n\n\nif __name__ == '__main__':\n    main()"""
ch15-自定义数据集/train_scratch.py,14,"b""import  matplotlib\nfrom    matplotlib import pyplot as plt\nmatplotlib.rcParams['font.size'] = 18\nmatplotlib.rcParams['figure.titlesize'] = 18\nmatplotlib.rcParams['figure.figsize'] = [9, 7]\nmatplotlib.rcParams['font.family'] = ['KaiTi']\nmatplotlib.rcParams['axes.unicode_minus']=False\n\nimport  os\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers,optimizers,losses\nfrom    tensorflow.keras.callbacks import EarlyStopping\n\ntf.random.set_seed(1234)\nnp.random.seed(1234)\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nassert tf.__version__.startswith('2.')\n\n\nfrom pokemon import  load_pokemon,normalize\n\n\n\ndef preprocess(x,y):\n    # x: \xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xef\xbc\x8cy\xef\xbc\x9a\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe6\x95\xb0\xe5\xad\x97\xe7\xbc\x96\xe7\xa0\x81\n    x = tf.io.read_file(x)\n    x = tf.image.decode_jpeg(x, channels=3) # RGBA\n    x = tf.image.resize(x, [244, 244])\n\n    x = tf.image.random_flip_left_right(x)\n    x = tf.image.random_flip_up_down(x)\n    x = tf.image.random_crop(x, [224,224,3])\n\n    # x: [0,255]=> -1~1\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    x = normalize(x)\n    y = tf.convert_to_tensor(y)\n    y = tf.one_hot(y, depth=5)\n\n    return x, y\n\n\nbatchsz = 32\n# \xe5\x88\x9b\xe5\xbb\xba\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86Datset\xe5\xaf\xb9\xe8\xb1\xa1\nimages, labels, table = load_pokemon('pokemon',mode='train')\ndb_train = tf.data.Dataset.from_tensor_slices((images, labels))\ndb_train = db_train.shuffle(1000).map(preprocess).batch(batchsz)\n# \xe5\x88\x9b\xe5\xbb\xba\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86Datset\xe5\xaf\xb9\xe8\xb1\xa1\nimages2, labels2, table = load_pokemon('pokemon',mode='val')\ndb_val = tf.data.Dataset.from_tensor_slices((images2, labels2))\ndb_val = db_val.map(preprocess).batch(batchsz)\n# \xe5\x88\x9b\xe5\xbb\xba\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86Datset\xe5\xaf\xb9\xe8\xb1\xa1\nimages3, labels3, table = load_pokemon('pokemon',mode='test')\ndb_test = tf.data.Dataset.from_tensor_slices((images3, labels3))\ndb_test = db_test.map(preprocess).batch(batchsz)\n\n# \xe5\x8a\xa0\xe8\xbd\xbdDenseNet\xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\xb9\xb6\xe5\x8e\xbb\xe6\x8e\x89\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xef\xbc\x8c\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xbamax pooling\nnet = keras.applications.DenseNet121(include_top=False, pooling='max')\n# \xe8\xae\xbe\xe8\xae\xa1\xe4\xb8\xba\xe4\xb8\x8d\xe5\x8f\x82\xe4\xb8\x8e\xe4\xbc\x98\xe5\x8c\x96\xef\xbc\x8c\xe5\x8d\xb3MobileNet\xe8\xbf\x99\xe9\x83\xa8\xe5\x88\x86\xe5\x8f\x82\xe6\x95\xb0\xe5\x9b\xba\xe5\xae\x9a\xe4\xb8\x8d\xe5\x8a\xa8\nnet.trainable = True\nnewnet = keras.Sequential([\n    net, # \xe5\x8e\xbb\xe6\x8e\x89\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84DenseNet121\n    layers.Dense(1024, activation='relu'), # \xe8\xbf\xbd\xe5\x8a\xa0\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\n    layers.BatchNormalization(), # \xe8\xbf\xbd\xe5\x8a\xa0BN\xe5\xb1\x82\n    layers.Dropout(rate=0.5), # \xe8\xbf\xbd\xe5\x8a\xa0Dropout\xe5\xb1\x82\xef\xbc\x8c\xe9\x98\xb2\xe6\xad\xa2\xe8\xbf\x87\xe6\x8b\x9f\xe5\x90\x88\n    layers.Dense(5) # \xe6\xa0\xb9\xe6\x8d\xae\xe5\xae\x9d\xe5\x8f\xaf\xe6\xa2\xa6\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe4\xbb\xbb\xe5\x8a\xa1\xef\xbc\x8c\xe8\xae\xbe\xe7\xbd\xae\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe8\xbe\x93\xe5\x87\xba\xe8\x8a\x82\xe7\x82\xb9\xe6\x95\xb0\xe4\xb8\xba5\n])\nnewnet.build(input_shape=(4,224,224,3))\nnewnet.summary()\n\n# \xe5\x88\x9b\xe5\xbb\xbaEarly Stopping\xe7\xb1\xbb\xef\xbc\x8c\xe8\xbf\x9e\xe7\xbb\xad3\xe6\xac\xa1\xe4\xb8\x8d\xe4\xb8\x8b\xe9\x99\x8d\xe5\x88\x99\xe7\xbb\x88\xe6\xad\xa2\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    min_delta=0.001,\n    patience=3\n)\n\nnewnet.compile(optimizer=optimizers.Adam(lr=1e-3),\n               loss=losses.CategoricalCrossentropy(from_logits=True),\n               metrics=['accuracy'])\nhistory  = newnet.fit(db_train, validation_data=db_val, validation_freq=1, epochs=100,\n           callbacks=[early_stopping])\nhistory = history.history\nprint(history.keys())\nprint(history['val_accuracy'])\nprint(history['accuracy'])\ntest_acc = newnet.evaluate(db_test)\n\nplt.figure()\nreturns = history['val_accuracy']\nplt.plot(np.arange(len(returns)), returns, label='\xe9\xaa\x8c\xe8\xaf\x81\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87')\nplt.plot(np.arange(len(returns)), returns, 's')\nreturns = history['accuracy']\nplt.plot(np.arange(len(returns)), returns, label='\xe8\xae\xad\xe7\xbb\x83\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87')\nplt.plot(np.arange(len(returns)), returns, 's')\n\nplt.plot([len(returns)-1],[test_acc[-1]], 'D', label='\xe6\xb5\x8b\xe8\xaf\x95\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87')\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87')\nplt.savefig('scratch.svg')"""
ch15-自定义数据集/train_transfer.py,14,"b""import  matplotlib\nfrom    matplotlib import pyplot as plt\nmatplotlib.rcParams['font.size'] = 18\nmatplotlib.rcParams['figure.titlesize'] = 18\nmatplotlib.rcParams['figure.figsize'] = [9, 7]\nmatplotlib.rcParams['font.family'] = ['KaiTi']\nmatplotlib.rcParams['axes.unicode_minus']=False\n\nimport  os\nimport  tensorflow as tf\nimport  numpy as np\nfrom    tensorflow import keras\nfrom    tensorflow.keras import layers,optimizers,losses\nfrom    tensorflow.keras.callbacks import EarlyStopping\n\ntf.random.set_seed(2222)\nnp.random.seed(2222)\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nassert tf.__version__.startswith('2.')\n\n\nfrom pokemon import  load_pokemon,normalize\n\n\n\ndef preprocess(x,y):\n    # x: \xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xef\xbc\x8cy\xef\xbc\x9a\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe6\x95\xb0\xe5\xad\x97\xe7\xbc\x96\xe7\xa0\x81\n    x = tf.io.read_file(x)\n    x = tf.image.decode_jpeg(x, channels=3) # RGBA\n    x = tf.image.resize(x, [244, 244])\n\n    x = tf.image.random_flip_left_right(x)\n    x = tf.image.random_flip_up_down(x)\n    x = tf.image.random_crop(x, [224,224,3])\n\n    # x: [0,255]=> -1~1\n    x = tf.cast(x, dtype=tf.float32) / 255.\n    x = normalize(x)\n    y = tf.convert_to_tensor(y)\n    y = tf.one_hot(y, depth=5)\n\n    return x, y\n\n\nbatchsz = 32\n# \xe5\x88\x9b\xe5\xbb\xba\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86Datset\xe5\xaf\xb9\xe8\xb1\xa1\nimages, labels, table = load_pokemon('pokemon',mode='train')\ndb_train = tf.data.Dataset.from_tensor_slices((images, labels))\ndb_train = db_train.shuffle(1000).map(preprocess).batch(batchsz)\n# \xe5\x88\x9b\xe5\xbb\xba\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86Datset\xe5\xaf\xb9\xe8\xb1\xa1\nimages2, labels2, table = load_pokemon('pokemon',mode='val')\ndb_val = tf.data.Dataset.from_tensor_slices((images2, labels2))\ndb_val = db_val.map(preprocess).batch(batchsz)\n# \xe5\x88\x9b\xe5\xbb\xba\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86Datset\xe5\xaf\xb9\xe8\xb1\xa1\nimages3, labels3, table = load_pokemon('pokemon',mode='test')\ndb_test = tf.data.Dataset.from_tensor_slices((images3, labels3))\ndb_test = db_test.map(preprocess).batch(batchsz)\n\n# \xe5\x8a\xa0\xe8\xbd\xbdDenseNet\xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\xb9\xb6\xe5\x8e\xbb\xe6\x8e\x89\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xef\xbc\x8c\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xbamax pooling\nnet = keras.applications.DenseNet121(weights='imagenet', include_top=False, pooling='max')\n# \xe8\xae\xbe\xe8\xae\xa1\xe4\xb8\xba\xe4\xb8\x8d\xe5\x8f\x82\xe4\xb8\x8e\xe4\xbc\x98\xe5\x8c\x96\xef\xbc\x8c\xe5\x8d\xb3MobileNet\xe8\xbf\x99\xe9\x83\xa8\xe5\x88\x86\xe5\x8f\x82\xe6\x95\xb0\xe5\x9b\xba\xe5\xae\x9a\xe4\xb8\x8d\xe5\x8a\xa8\nnet.trainable = True\nnewnet = keras.Sequential([\n    net, # \xe5\x8e\xbb\xe6\x8e\x89\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84DenseNet121\n    layers.Dense(1024, activation='relu'), # \xe8\xbf\xbd\xe5\x8a\xa0\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\n    layers.BatchNormalization(), # \xe8\xbf\xbd\xe5\x8a\xa0BN\xe5\xb1\x82\n    layers.Dropout(rate=0.5), # \xe8\xbf\xbd\xe5\x8a\xa0Dropout\xe5\xb1\x82\xef\xbc\x8c\xe9\x98\xb2\xe6\xad\xa2\xe8\xbf\x87\xe6\x8b\x9f\xe5\x90\x88\n    layers.Dense(5) # \xe6\xa0\xb9\xe6\x8d\xae\xe5\xae\x9d\xe5\x8f\xaf\xe6\xa2\xa6\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe4\xbb\xbb\xe5\x8a\xa1\xef\xbc\x8c\xe8\xae\xbe\xe7\xbd\xae\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe8\xbe\x93\xe5\x87\xba\xe8\x8a\x82\xe7\x82\xb9\xe6\x95\xb0\xe4\xb8\xba5\n])\nnewnet.build(input_shape=(4,224,224,3))\nnewnet.summary()\n\n# \xe5\x88\x9b\xe5\xbb\xbaEarly Stopping\xe7\xb1\xbb\xef\xbc\x8c\xe8\xbf\x9e\xe7\xbb\xad3\xe6\xac\xa1\xe4\xb8\x8d\xe4\xb8\x8b\xe9\x99\x8d\xe5\x88\x99\xe7\xbb\x88\xe6\xad\xa2\nearly_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    min_delta=0.001,\n    patience=3\n)\n\nnewnet.compile(optimizer=optimizers.Adam(lr=1e-3),\n               loss=losses.CategoricalCrossentropy(from_logits=True),\n               metrics=['accuracy'])\nhistory  = newnet.fit(db_train, validation_data=db_val, validation_freq=1, epochs=100,\n           callbacks=[early_stopping])\nhistory = history.history\nprint(history.keys())\nprint(history['val_accuracy'])\nprint(history['accuracy'])\ntest_acc = newnet.evaluate(db_test)\n\nplt.figure()\nreturns = history['val_accuracy']\nplt.plot(np.arange(len(returns)), returns, label='\xe9\xaa\x8c\xe8\xaf\x81\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87')\nplt.plot(np.arange(len(returns)), returns, 's')\nreturns = history['accuracy']\nplt.plot(np.arange(len(returns)), returns, label='\xe8\xae\xad\xe7\xbb\x83\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87')\nplt.plot(np.arange(len(returns)), returns, 's')\n\nplt.plot([len(returns)-1],[test_acc[-1]], 'D', label='\xe6\xb5\x8b\xe8\xaf\x95\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87')\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87')\nplt.savefig('transfer.svg')"""
