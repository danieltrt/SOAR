file_path,api_count,code
census_income_demo.py,2,"b'""""""\nMulti-gate Mixture-of-Experts demo with census income data.\n\nCopyright (c) 2018 Drawbridge, Inc\nLicensed under the MIT License (see LICENSE for details)\nWritten by Alvin Deng\n""""""\n\nimport random\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras.optimizers import Adam\nfrom keras.initializers import VarianceScaling\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\nfrom keras.utils import to_categorical\nfrom keras.callbacks import Callback\nfrom sklearn.metrics import roc_auc_score\n\nfrom mmoe import MMoE\n\nSEED = 1\n\n# Fix numpy seed for reproducibility\nnp.random.seed(SEED)\n\n# Fix random seed for reproducibility\nrandom.seed(SEED)\n\n# Fix TensorFlow graph-level seed for reproducibility\ntf.set_random_seed(SEED)\ntf_session = tf.Session(graph=tf.get_default_graph())\nK.set_session(tf_session)\n\n\n# Simple callback to print out ROC-AUC\nclass ROCCallback(Callback):\n    def __init__(self, training_data, validation_data, test_data):\n        self.train_X = training_data[0]\n        self.train_Y = training_data[1]\n        self.validation_X = validation_data[0]\n        self.validation_Y = validation_data[1]\n        self.test_X = test_data[0]\n        self.test_Y = test_data[1]\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        train_prediction = self.model.predict(self.train_X)\n        validation_prediction = self.model.predict(self.validation_X)\n        test_prediction = self.model.predict(self.test_X)\n\n        # Iterate through each task and output their ROC-AUC across different datasets\n        for index, output_name in enumerate(self.model.output_names):\n            train_roc_auc = roc_auc_score(self.train_Y[index], train_prediction[index])\n            validation_roc_auc = roc_auc_score(self.validation_Y[index], validation_prediction[index])\n            test_roc_auc = roc_auc_score(self.test_Y[index], test_prediction[index])\n            print(\n                \'ROC-AUC-{}-Train: {} ROC-AUC-{}-Validation: {} ROC-AUC-{}-Test: {}\'.format(\n                    output_name, round(train_roc_auc, 4),\n                    output_name, round(validation_roc_auc, 4),\n                    output_name, round(test_roc_auc, 4)\n                )\n            )\n\n        return\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return\n\n\ndef data_preparation():\n    # The column names are from\n    # https://www2.1010data.com/documentationcenter/prod/Tutorials/MachineLearningExamples/CensusIncomeDataSet.html\n    column_names = [\'age\', \'class_worker\', \'det_ind_code\', \'det_occ_code\', \'education\', \'wage_per_hour\', \'hs_college\',\n                    \'marital_stat\', \'major_ind_code\', \'major_occ_code\', \'race\', \'hisp_origin\', \'sex\', \'union_member\',\n                    \'unemp_reason\', \'full_or_part_emp\', \'capital_gains\', \'capital_losses\', \'stock_dividends\',\n                    \'tax_filer_stat\', \'region_prev_res\', \'state_prev_res\', \'det_hh_fam_stat\', \'det_hh_summ\',\n                    \'instance_weight\', \'mig_chg_msa\', \'mig_chg_reg\', \'mig_move_reg\', \'mig_same\', \'mig_prev_sunbelt\',\n                    \'num_emp\', \'fam_under_18\', \'country_father\', \'country_mother\', \'country_self\', \'citizenship\',\n                    \'own_or_self\', \'vet_question\', \'vet_benefits\', \'weeks_worked\', \'year\', \'income_50k\']\n\n    # Load the dataset in Pandas\n    train_df = pd.read_csv(\n        \'data/census-income.data.gz\',\n        delimiter=\',\',\n        header=None,\n        index_col=None,\n        names=column_names\n    )\n    other_df = pd.read_csv(\n        \'data/census-income.test.gz\',\n        delimiter=\',\',\n        header=None,\n        index_col=None,\n        names=column_names\n    )\n\n    # First group of tasks according to the paper\n    label_columns = [\'income_50k\', \'marital_stat\']\n\n    # One-hot encoding categorical columns\n    categorical_columns = [\'class_worker\', \'det_ind_code\', \'det_occ_code\', \'education\', \'hs_college\', \'major_ind_code\',\n                           \'major_occ_code\', \'race\', \'hisp_origin\', \'sex\', \'union_member\', \'unemp_reason\',\n                           \'full_or_part_emp\', \'tax_filer_stat\', \'region_prev_res\', \'state_prev_res\', \'det_hh_fam_stat\',\n                           \'det_hh_summ\', \'mig_chg_msa\', \'mig_chg_reg\', \'mig_move_reg\', \'mig_same\', \'mig_prev_sunbelt\',\n                           \'fam_under_18\', \'country_father\', \'country_mother\', \'country_self\', \'citizenship\',\n                           \'vet_question\']\n    train_raw_labels = train_df[label_columns]\n    other_raw_labels = other_df[label_columns]\n    transformed_train = pd.get_dummies(train_df.drop(label_columns, axis=1), columns=categorical_columns)\n    transformed_other = pd.get_dummies(other_df.drop(label_columns, axis=1), columns=categorical_columns)\n\n    # Filling the missing column in the other set\n    transformed_other[\'det_hh_fam_stat_ Grandchild <18 ever marr not in subfamily\'] = 0\n\n    # One-hot encoding categorical labels\n    train_income = to_categorical((train_raw_labels.income_50k == \' 50000+.\').astype(int), num_classes=2)\n    train_marital = to_categorical((train_raw_labels.marital_stat == \' Never married\').astype(int), num_classes=2)\n    other_income = to_categorical((other_raw_labels.income_50k == \' 50000+.\').astype(int), num_classes=2)\n    other_marital = to_categorical((other_raw_labels.marital_stat == \' Never married\').astype(int), num_classes=2)\n\n    dict_outputs = {\n        \'income\': train_income.shape[1],\n        \'marital\': train_marital.shape[1]\n    }\n    dict_train_labels = {\n        \'income\': train_income,\n        \'marital\': train_marital\n    }\n    dict_other_labels = {\n        \'income\': other_income,\n        \'marital\': other_marital\n    }\n    output_info = [(dict_outputs[key], key) for key in sorted(dict_outputs.keys())]\n\n    # Split the other dataset into 1:1 validation to test according to the paper\n    validation_indices = transformed_other.sample(frac=0.5, replace=False, random_state=SEED).index\n    test_indices = list(set(transformed_other.index) - set(validation_indices))\n    validation_data = transformed_other.iloc[validation_indices]\n    validation_label = [dict_other_labels[key][validation_indices] for key in sorted(dict_other_labels.keys())]\n    test_data = transformed_other.iloc[test_indices]\n    test_label = [dict_other_labels[key][test_indices] for key in sorted(dict_other_labels.keys())]\n    train_data = transformed_train\n    train_label = [dict_train_labels[key] for key in sorted(dict_train_labels.keys())]\n\n    return train_data, train_label, validation_data, validation_label, test_data, test_label, output_info\n\n\ndef main():\n    # Load the data\n    train_data, train_label, validation_data, validation_label, test_data, test_label, output_info = data_preparation()\n    num_features = train_data.shape[1]\n\n    print(\'Training data shape = {}\'.format(train_data.shape))\n    print(\'Validation data shape = {}\'.format(validation_data.shape))\n    print(\'Test data shape = {}\'.format(test_data.shape))\n\n    # Set up the input layer\n    input_layer = Input(shape=(num_features,))\n\n    # Set up MMoE layer\n    mmoe_layers = MMoE(\n        units=4,\n        num_experts=8,\n        num_tasks=2\n    )(input_layer)\n\n    output_layers = []\n\n    # Build tower layer from MMoE layer\n    for index, task_layer in enumerate(mmoe_layers):\n        tower_layer = Dense(\n            units=8,\n            activation=\'relu\',\n            kernel_initializer=VarianceScaling())(task_layer)\n        output_layer = Dense(\n            units=output_info[index][0],\n            name=output_info[index][1],\n            activation=\'softmax\',\n            kernel_initializer=VarianceScaling())(tower_layer)\n        output_layers.append(output_layer)\n\n    # Compile model\n    model = Model(inputs=[input_layer], outputs=output_layers)\n    adam_optimizer = Adam()\n    model.compile(\n        loss={\'income\': \'binary_crossentropy\', \'marital\': \'binary_crossentropy\'},\n        optimizer=adam_optimizer,\n        metrics=[\'accuracy\']\n    )\n\n    # Print out model architecture summary\n    model.summary()\n\n    # Train the model\n    model.fit(\n        x=train_data,\n        y=train_label,\n        validation_data=(validation_data, validation_label),\n        callbacks=[\n            ROCCallback(\n                training_data=(train_data, train_label),\n                validation_data=(validation_data, validation_label),\n                test_data=(test_data, test_label)\n            )\n        ],\n        epochs=100\n    )\n\n\nif __name__ == \'__main__\':\n    main()\n'"
mmoe.py,1,"b'""""""\nMulti-gate Mixture-of-Experts model implementation.\n\nCopyright (c) 2018 Drawbridge, Inc\nLicensed under the MIT License (see LICENSE for details)\nWritten by Alvin Deng\n""""""\n\nfrom keras import backend as K\nfrom keras import activations, initializers, regularizers, constraints\nfrom keras.engine.topology import Layer, InputSpec\n\n\nclass MMoE(Layer):\n    """"""\n    Multi-gate Mixture-of-Experts model.\n    """"""\n\n    def __init__(self,\n                 units,\n                 num_experts,\n                 num_tasks,\n                 use_expert_bias=True,\n                 use_gate_bias=True,\n                 expert_activation=\'relu\',\n                 gate_activation=\'softmax\',\n                 expert_bias_initializer=\'zeros\',\n                 gate_bias_initializer=\'zeros\',\n                 expert_bias_regularizer=None,\n                 gate_bias_regularizer=None,\n                 expert_bias_constraint=None,\n                 gate_bias_constraint=None,\n                 expert_kernel_initializer=\'VarianceScaling\',\n                 gate_kernel_initializer=\'VarianceScaling\',\n                 expert_kernel_regularizer=None,\n                 gate_kernel_regularizer=None,\n                 expert_kernel_constraint=None,\n                 gate_kernel_constraint=None,\n                 activity_regularizer=None,\n                 **kwargs):\n        """"""\n         Method for instantiating MMoE layer.\n\n        :param units: Number of hidden units\n        :param num_experts: Number of experts\n        :param num_tasks: Number of tasks\n        :param use_expert_bias: Boolean to indicate the usage of bias in the expert weights\n        :param use_gate_bias: Boolean to indicate the usage of bias in the gate weights\n        :param expert_activation: Activation function of the expert weights\n        :param gate_activation: Activation function of the gate weights\n        :param expert_bias_initializer: Initializer for the expert bias\n        :param gate_bias_initializer: Initializer for the gate bias\n        :param expert_bias_regularizer: Regularizer for the expert bias\n        :param gate_bias_regularizer: Regularizer for the gate bias\n        :param expert_bias_constraint: Constraint for the expert bias\n        :param gate_bias_constraint: Constraint for the gate bias\n        :param expert_kernel_initializer: Initializer for the expert weights\n        :param gate_kernel_initializer: Initializer for the gate weights\n        :param expert_kernel_regularizer: Regularizer for the expert weights\n        :param gate_kernel_regularizer: Regularizer for the gate weights\n        :param expert_kernel_constraint: Constraint for the expert weights\n        :param gate_kernel_constraint: Constraint for the gate weights\n        :param activity_regularizer: Regularizer for the activity\n        :param kwargs: Additional keyword arguments for the Layer class\n        """"""\n        # Hidden nodes parameter\n        self.units = units\n        self.num_experts = num_experts\n        self.num_tasks = num_tasks\n\n        # Weight parameter\n        self.expert_kernels = None\n        self.gate_kernels = None\n        self.expert_kernel_initializer = initializers.get(expert_kernel_initializer)\n        self.gate_kernel_initializer = initializers.get(gate_kernel_initializer)\n        self.expert_kernel_regularizer = regularizers.get(expert_kernel_regularizer)\n        self.gate_kernel_regularizer = regularizers.get(gate_kernel_regularizer)\n        self.expert_kernel_constraint = constraints.get(expert_kernel_constraint)\n        self.gate_kernel_constraint = constraints.get(gate_kernel_constraint)\n\n        # Activation parameter\n        self.expert_activation = activations.get(expert_activation)\n        self.gate_activation = activations.get(gate_activation)\n\n        # Bias parameter\n        self.expert_bias = None\n        self.gate_bias = None\n        self.use_expert_bias = use_expert_bias\n        self.use_gate_bias = use_gate_bias\n        self.expert_bias_initializer = initializers.get(expert_bias_initializer)\n        self.gate_bias_initializer = initializers.get(gate_bias_initializer)\n        self.expert_bias_regularizer = regularizers.get(expert_bias_regularizer)\n        self.gate_bias_regularizer = regularizers.get(gate_bias_regularizer)\n        self.expert_bias_constraint = constraints.get(expert_bias_constraint)\n        self.gate_bias_constraint = constraints.get(gate_bias_constraint)\n\n        # Activity parameter\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n        # Keras parameter\n        self.input_spec = InputSpec(min_ndim=2)\n        self.supports_masking = True\n\n        super(MMoE, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        """"""\n        Method for creating the layer weights.\n\n        :param input_shape: Keras tensor (future input to layer)\n                            or list/tuple of Keras tensors to reference\n                            for weight shape computations\n        """"""\n        assert input_shape is not None and len(input_shape) >= 2\n\n        input_dimension = input_shape[-1]\n\n        # Initialize expert weights (number of input features * number of units per expert * number of experts)\n        self.expert_kernels = self.add_weight(\n            name=\'expert_kernel\',\n            shape=(input_dimension, self.units, self.num_experts),\n            initializer=self.expert_kernel_initializer,\n            regularizer=self.expert_kernel_regularizer,\n            constraint=self.expert_kernel_constraint,\n        )\n\n        # Initialize expert bias (number of units per expert * number of experts)\n        if self.use_expert_bias:\n            self.expert_bias = self.add_weight(\n                name=\'expert_bias\',\n                shape=(self.units, self.num_experts),\n                initializer=self.expert_bias_initializer,\n                regularizer=self.expert_bias_regularizer,\n                constraint=self.expert_bias_constraint,\n            )\n\n        # Initialize gate weights (number of input features * number of experts * number of tasks)\n        self.gate_kernels = [self.add_weight(\n            name=\'gate_kernel_task_{}\'.format(i),\n            shape=(input_dimension, self.num_experts),\n            initializer=self.gate_kernel_initializer,\n            regularizer=self.gate_kernel_regularizer,\n            constraint=self.gate_kernel_constraint\n        ) for i in range(self.num_tasks)]\n\n        # Initialize gate bias (number of experts * number of tasks)\n        if self.use_gate_bias:\n            self.gate_bias = [self.add_weight(\n                name=\'gate_bias_task_{}\'.format(i),\n                shape=(self.num_experts,),\n                initializer=self.gate_bias_initializer,\n                regularizer=self.gate_bias_regularizer,\n                constraint=self.gate_bias_constraint\n            ) for i in range(self.num_tasks)]\n\n        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dimension})\n\n        super(MMoE, self).build(input_shape)\n\n    def call(self, inputs, **kwargs):\n        """"""\n        Method for the forward function of the layer.\n\n        :param inputs: Input tensor\n        :param kwargs: Additional keyword arguments for the base method\n        :return: A tensor\n        """"""\n        gate_outputs = []\n        final_outputs = []\n\n        # f_{i}(x) = activation(W_{i} * x + b), where activation is ReLU according to the paper\n        expert_outputs = K.tf.tensordot(a=inputs, b=self.expert_kernels, axes=1)\n        # Add the bias term to the expert weights if necessary\n        if self.use_expert_bias:\n            expert_outputs = K.bias_add(x=expert_outputs, bias=self.expert_bias)\n        expert_outputs = self.expert_activation(expert_outputs)\n\n        # g^{k}(x) = activation(W_{gk} * x + b), where activation is softmax according to the paper\n        for index, gate_kernel in enumerate(self.gate_kernels):\n            gate_output = K.dot(x=inputs, y=gate_kernel)\n            # Add the bias term to the gate weights if necessary\n            if self.use_gate_bias:\n                gate_output = K.bias_add(x=gate_output, bias=self.gate_bias[index])\n            gate_output = self.gate_activation(gate_output)\n            gate_outputs.append(gate_output)\n\n        # f^{k}(x) = sum_{i=1}^{n}(g^{k}(x)_{i} * f_{i}(x))\n        for gate_output in gate_outputs:\n            expanded_gate_output = K.expand_dims(gate_output, axis=1)\n            weighted_expert_output = expert_outputs * K.repeat_elements(expanded_gate_output, self.units, axis=1)\n            final_outputs.append(K.sum(weighted_expert_output, axis=2))\n\n        return final_outputs\n\n    def compute_output_shape(self, input_shape):\n        """"""\n        Method for computing the output shape of the MMoE layer.\n\n        :param input_shape: Shape tuple (tuple of integers)\n        :return: List of input shape tuple where the size of the list is equal to the number of tasks\n        """"""\n        assert input_shape is not None and len(input_shape) >= 2\n\n        output_shape = list(input_shape)\n        output_shape[-1] = self.units\n        output_shape = tuple(output_shape)\n\n        return [output_shape for _ in range(self.num_tasks)]\n\n    def get_config(self):\n        """"""\n        Method for returning the configuration of the MMoE layer.\n\n        :return: Config dictionary\n        """"""\n        config = {\n            \'units\': self.units,\n            \'num_experts\': self.num_experts,\n            \'num_tasks\': self.num_tasks,\n            \'use_expert_bias\': self.use_expert_bias,\n            \'use_gate_bias\': self.use_gate_bias,\n            \'expert_activation\': activations.serialize(self.expert_activation),\n            \'gate_activation\': activations.serialize(self.gate_activation),\n            \'expert_bias_initializer\': initializers.serialize(self.expert_bias_initializer),\n            \'gate_bias_initializer\': initializers.serialize(self.gate_bias_initializer),\n            \'expert_bias_regularizer\': regularizers.serialize(self.expert_bias_regularizer),\n            \'gate_bias_regularizer\': regularizers.serialize(self.gate_bias_regularizer),\n            \'expert_bias_constraint\': constraints.serialize(self.expert_bias_constraint),\n            \'gate_bias_constraint\': constraints.serialize(self.gate_bias_constraint),\n            \'expert_kernel_initializer\': initializers.serialize(self.expert_kernel_initializer),\n            \'gate_kernel_initializer\': initializers.serialize(self.gate_kernel_initializer),\n            \'expert_kernel_regularizer\': regularizers.serialize(self.expert_kernel_regularizer),\n            \'gate_kernel_regularizer\': regularizers.serialize(self.gate_kernel_regularizer),\n            \'expert_kernel_constraint\': constraints.serialize(self.expert_kernel_constraint),\n            \'gate_kernel_constraint\': constraints.serialize(self.gate_kernel_constraint),\n            \'activity_regularizer\': regularizers.serialize(self.activity_regularizer)\n        }\n        base_config = super(MMoE, self).get_config()\n\n        return dict(list(base_config.items()) + list(config.items()))\n'"
synthetic_demo.py,2,"b'""""""\nMulti-gate Mixture-of-Experts demo with census income data.\n\nCopyright (c) 2018 Drawbridge, Inc\nLicensed under the MIT License (see LICENSE for details)\nWritten by Peizhou Liao\n""""""\n\nimport random\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras import metrics\nfrom keras.optimizers import Adam\nfrom keras.initializers import VarianceScaling\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\n\nfrom mmoe import MMoE\n\nSEED = 1\n\n# Fix numpy seed for reproducibility\nnp.random.seed(SEED)\n\n# Fix random seed for reproducibility\nrandom.seed(SEED)\n\n# Fix TensorFlow graph-level seed for reproducibility\ntf.set_random_seed(SEED)\ntf_session = tf.Session(graph=tf.get_default_graph())\nK.set_session(tf_session)\n\n\ndef data_preparation():\n    # Synthetic data parameters\n    num_dimension = 100\n    num_row = 12000\n    c = 0.3\n    rho = 0.8\n    m = 5\n\n    # Initialize vectors u1, u2, w1, and w2 according to the paper\n    mu1 = np.random.normal(size=num_dimension)\n    mu1 = (mu1 - np.mean(mu1)) / (np.std(mu1) * np.sqrt(num_dimension))\n    mu2 = np.random.normal(size=num_dimension)\n    mu2 -= mu2.dot(mu1) * mu1\n    mu2 /= np.linalg.norm(mu2)\n    w1 = c * mu1\n    w2 = c * (rho * mu1 + np.sqrt(1. - rho ** 2) * mu2)\n\n    # Feature and label generation\n    alpha = np.random.normal(size=m)\n    beta = np.random.normal(size=m)\n    y0 = []\n    y1 = []\n    X = []\n\n    for i in range(num_row):\n        x = np.random.normal(size=num_dimension)\n        X.append(x)\n        num1 = w1.dot(x)\n        num2 = w2.dot(x)\n        comp1, comp2 = 0.0, 0.0\n\n        for j in range(m):\n            comp1 += np.sin(alpha[j] * num1 + beta[j])\n            comp2 += np.sin(alpha[j] * num2 + beta[j])\n\n        y0.append(num1 + comp1 + np.random.normal(scale=0.1, size=1))\n        y1.append(num2 + comp2 + np.random.normal(scale=0.1, size=1))\n\n    X = np.array(X)\n    data = pd.DataFrame(\n        data=X,\n        index=range(X.shape[0]),\n        columns=[\'x{}\'.format(it) for it in range(X.shape[1])]\n    )\n\n    train_data = data.iloc[0:10000]\n    train_label = [y0[0:10000], y1[0:10000]]\n    validation_data = data.iloc[10000:11000]\n    validation_label = [y0[10000:11000], y1[10000:11000]]\n    test_data = data.iloc[11000:]\n    test_label = [y0[11000:], y1[11000:]]\n\n    return train_data, train_label, validation_data, validation_label, test_data, test_label\n\n\ndef main():\n    # Load the data\n    train_data, train_label, validation_data, validation_label, test_data, test_label = data_preparation()\n    num_features = train_data.shape[1]\n\n    print(\'Training data shape = {}\'.format(train_data.shape))\n    print(\'Validation data shape = {}\'.format(validation_data.shape))\n    print(\'Test data shape = {}\'.format(test_data.shape))\n\n    # Set up the input layer\n    input_layer = Input(shape=(num_features,))\n\n    # Set up MMoE layer\n    mmoe_layers = MMoE(\n        units=16,\n        num_experts=8,\n        num_tasks=2\n    )(input_layer)\n\n    output_layers = []\n\n    output_info = [\'y0\', \'y1\']\n\n    # Build tower layer from MMoE layer\n    for index, task_layer in enumerate(mmoe_layers):\n        tower_layer = Dense(\n            units=8,\n            activation=\'relu\',\n            kernel_initializer=VarianceScaling())(task_layer)\n        output_layer = Dense(\n            units=1,\n            name=output_info[index],\n            activation=\'linear\',\n            kernel_initializer=VarianceScaling())(tower_layer)\n        output_layers.append(output_layer)\n\n    # Compile model\n    model = Model(inputs=[input_layer], outputs=output_layers)\n    learning_rates = [1e-4, 1e-3, 1e-2]\n    adam_optimizer = Adam(lr=learning_rates[0])\n    model.compile(\n        loss={\'y0\': \'mean_squared_error\', \'y1\': \'mean_squared_error\'},\n        optimizer=adam_optimizer,\n        metrics=[metrics.mae]\n    )\n\n    # Print out model architecture summary\n    model.summary()\n\n    # Train the model\n    model.fit(\n        x=train_data,\n        y=train_label,\n        validation_data=(validation_data, validation_label),\n        epochs=100\n    )\n\n\nif __name__ == \'__main__\':\n    main()\n'"
