file_path,api_count,code
2_Linear_Classifier/code/main.py,11,"b'import tensorflow as tf\nimport numpy as np\nfrom utils import *\nfrom ops import *\n\n# Data Dimensions\nimg_h = img_w = 28  # MNIST images are 28x28\nimg_size_flat = img_h * img_w  # 28x28=784, the total number of pixels\nn_classes = 10  # Number of classes, one class per digit\n\n# Load MNIST data\nx_train, y_train, x_valid, y_valid = load_data(mode=\'train\')\nprint(""Size of:"")\nprint(""- Training-set:\\t\\t{}"".format(len(y_train)))\nprint(""- Validation-set:\\t{}"".format(len(y_valid)))\n\n# Hyper-parameters\nlearning_rate = 0.001  # The optimization initial learning rate\nepochs = 2  # Total number of training epochs\nbatch_size = 100  # Training batch size\ndisplay_freq = 100  # Frequency of displaying the training results\n\n# Create the graph for the linear model\n# Placeholders for inputs (x) and outputs(y)\nx = tf.placeholder(tf.float32, shape=[None, img_size_flat], name=\'X\')\ny = tf.placeholder(tf.float32, shape=[None, n_classes], name=\'Y\')\n\n# create weight matrix initialized randomely from N~(0, 0.01)\nW = weight_variable(shape=[img_size_flat, n_classes])\n\n# create bias vector initialized as zero\nb = bias_variable(shape=[n_classes])\n\noutput_logits = tf.matmul(x, W) + b\ny_pred = tf.nn.softmax(output_logits)\n\n# Model predictions\ncls_prediction = tf.argmax(output_logits, axis=1, name=\'predictions\')\n\n# Define the loss function, optimizer, and accuracy\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits), name=\'loss\')\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name=\'Adam-op\').minimize(loss)\ncorrect_prediction = tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1), name=\'correct_pred\')\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\'accuracy\')\n\n# Creating the op for initializing all variables\ninit = tf.global_variables_initializer()\n\n# Launch the graph (session)\nwith tf.Session() as sess:\n    sess.run(init)\n    global_step = 0\n    # Number of training iterations in each epoch\n    num_tr_iter = int(len(y_train) / batch_size)\n    for epoch in range(epochs):\n        print(\'Training epoch: {}\'.format(epoch + 1))\n        x_train, y_train = randomize(x_train, y_train)\n        for iteration in range(num_tr_iter):\n            global_step += 1\n            start = iteration * batch_size\n            end = (iteration + 1) * batch_size\n            x_batch, y_batch = get_next_batch(x_train, y_train, start, end)\n\n            # Run optimization op (backprop)\n            feed_dict_batch = {x: x_batch, y: y_batch}\n            sess.run(optimizer, feed_dict=feed_dict_batch)\n\n            if iteration % display_freq == 0:\n                # Calculate and display the batch loss and accuracy\n                loss_batch, acc_batch = sess.run([loss, accuracy],\n                                                 feed_dict=feed_dict_batch)\n\n                print(""iter {0:3d}:\\t Loss={1:.2f},\\tTraining Accuracy={2:.01%}"".\n                      format(iteration, loss_batch, acc_batch))\n\n        # Run validation after every epoch\n        feed_dict_valid = {x: x_valid[:1000], y: y_valid[:1000]}\n        loss_valid, acc_valid = sess.run([loss, accuracy], feed_dict=feed_dict_valid)\n        print(\'---------------------------------------------------------\')\n        print(""Epoch: {0}, validation loss: {1:.2f}, validation accuracy: {2:.01%}"".\n              format(epoch + 1, loss_valid, acc_valid))\n        print(\'---------------------------------------------------------\')\n\n    # Test the network after training\n    # Accuracy\n    x_test, y_test = load_data(mode=\'test\')\n    feed_dict_test = {x: x_test[:1000], y: y_test[:1000]}\n    loss_test, acc_test = sess.run([loss, accuracy], feed_dict=feed_dict_test)\n    print(\'---------------------------------------------------------\')\n    print(""Test loss: {0:.2f}, test accuracy: {1:.01%}"".format(loss_test, acc_test))\n    print(\'---------------------------------------------------------\')\n\n    # Plot some of the correct and misclassified examples\n    cls_pred = sess.run(cls_prediction, feed_dict=feed_dict_test)\n    cls_true = np.argmax(y_test[:1000], axis=1)\n    plot_images(x_test, cls_true, cls_pred, title=\'Correct Examples\')\n    plot_example_errors(x_test[:1000], cls_true, cls_pred, title=\'Misclassified Examples\')\n    plt.show()\n'"
2_Linear_Classifier/code/ops.py,6,"b'import tensorflow as tf\n\n\n# weight and bais wrappers\ndef weight_variable(shape):\n    """"""\n    Create a weight variable with appropriate initialization\n    :param name: weight name\n    :param shape: weight shape\n    :return: initialized weight variable\n    """"""\n    initer = tf.truncated_normal_initializer(stddev=0.01)\n    return tf.get_variable(\'W\',\n                           dtype=tf.float32,\n                           shape=shape,\n                           initializer=initer)\n\n\ndef bias_variable(shape):\n    """"""\n    Create a bias variable with appropriate initialization\n    :param name: bias variable name\n    :param shape: bias variable shape\n    :return: initialized bias variable\n    """"""\n    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n    return tf.get_variable(\'b\',\n                           dtype=tf.float32,\n                           initializer=initial)'"
2_Linear_Classifier/code/utils.py,0,"b'import numpy as np\nimport random\nimport scipy\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport matplotlib.pyplot as plt\n\n\ndef load_data(mode=\'train\'):\n    """"""\n    Function to (download and) load the MNIST data\n    :param mode: train or test\n    :return: images and the corresponding labels\n    """"""\n    mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n    if mode == \'train\':\n        x_train, y_train, x_valid, y_valid = mnist.train.images, mnist.train.labels, \\\n                                             mnist.validation.images, mnist.validation.labels\n        return x_train, y_train, x_valid, y_valid\n    elif mode == \'test\':\n        x_test, y_test = mnist.test.images, mnist.test.labels\n    return x_test, y_test\n\n\ndef randomize(x, y):\n    """""" Randomizes the order of data samples and their corresponding labels""""""\n    permutation = np.random.permutation(y.shape[0])\n    shuffled_x = x[permutation, :]\n    shuffled_y = y[permutation]\n    return shuffled_x, shuffled_y\n\n\ndef reformat(x, y):\n    """"""\n    Reformats the data to the format acceptable for convolutional layers\n    :param x: input array\n    :param y: corresponding labels\n    :return: reshaped input and labels\n    """"""\n    img_size, num_ch, num_class = int(np.sqrt(x.shape[-1])), 1, len(np.unique(np.argmax(y, 1)))\n    dataset = x.reshape((-1, img_size, img_size, num_ch)).astype(np.float32)\n    labels = (np.arange(num_class) == y[:, None]).astype(np.float32)\n    return dataset, labels\n\n\ndef get_next_batch(x, y, start, end):\n    x_batch = x[start:end]\n    y_batch = y[start:end]\n    return x_batch, y_batch\n\n\ndef plot_images(images, cls_true, cls_pred=None, title=None):\n    """"""\n    Create figure with 3x3 sub-plots.\n    :param images: array of images to be plotted, (9, img_h*img_w)\n    :param cls_true: corresponding true labels (9,)\n    :param cls_pred: corresponding true labels (9,)\n    """"""\n    fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n    for i, ax in enumerate(axes.flat):\n        # Plot image.\n        ax.imshow(images[i].reshape(28, 28), cmap=\'binary\')\n\n        # Show true and predicted classes.\n        if cls_pred is None:\n            ax_title = ""True: {0}"".format(cls_true[i])\n        else:\n            ax_title = ""True: {0}, Pred: {1}"".format(cls_true[i], cls_pred[i])\n\n        ax.set_title(ax_title)\n\n        # Remove ticks from the plot.\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n    if title:\n        plt.suptitle(title, size=20)\n    plt.show(block=False)\n\n\ndef plot_example_errors(images, cls_true, cls_pred, title=None):\n    """"""\n    Function for plotting examples of images that have been mis-classified\n    :param images: array of all images, (#imgs, img_h*img_w)\n    :param cls_true: corresponding true labels, (#imgs,)\n    :param cls_pred: corresponding predicted labels, (#imgs,)\n    """"""\n    # Negate the boolean array.\n    incorrect = np.logical_not(np.equal(cls_pred, cls_true))\n\n    # Get the images from the test-set that have been\n    # incorrectly classified.\n    incorrect_images = images[incorrect]\n\n    # Get the true and predicted classes for those images.\n    cls_pred = cls_pred[incorrect]\n    cls_true = cls_true[incorrect]\n\n    # Plot the first 9 images.\n    plot_images(images=incorrect_images[0:9],\n                cls_true=cls_true[0:9],\n                cls_pred=cls_pred[0:9],\n                title=title)\n'"
3_Neural_Network/code/main.py,9,"b'# imports\nimport tensorflow as tf\nimport numpy as np\nfrom ops import fc_layer\nfrom utils import *\n\n# Data Dimensions\nimg_h = img_w = 28              # MNIST images are 28x28\nimg_size_flat = img_h * img_w   # 28x28=784, the total number of pixels\nn_classes = 10                  # Number of classes, one class per digit\n\n# Load MNIST data\nx_train, y_train, x_valid, y_valid = load_data(mode=\'train\')\nprint(""Size of:"")\nprint(""- Training-set:\\t\\t{}"".format(len(y_train)))\nprint(""- Validation-set:\\t{}"".format(len(y_valid)))\n\n# Hyper-parameters\nlearning_rate = 0.001   # The optimization initial learning rate\nepochs = 10             # Total number of training epochs\nbatch_size = 100        # Training batch size\ndisplay_freq = 100      # Frequency of displaying the training results\n\n# Network Configuration\nh1 = 200                # Number of units in the first hidden layer\n\n# Create the graph for the linear model\n# Placeholders for inputs (x) and outputs(y)\nx = tf.placeholder(tf.float32, shape=[None, img_size_flat], name=\'X\')\ny = tf.placeholder(tf.float32, shape=[None, n_classes], name=\'Y\')\n\n# Create the network layers\nfc1 = fc_layer(x, h1, \'FC1\', use_relu=True)\noutput_logits = fc_layer(fc1, n_classes, \'OUT\', use_relu=False)\n\n# Network predictions\ncls_prediction = tf.argmax(output_logits, axis=1, name=\'predictions\')\n\n# Define the loss function, optimizer, and accuracy\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits), name=\'loss\')\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name=\'Adam-op\').minimize(loss)\ncorrect_prediction = tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1), name=\'correct_pred\')\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\'accuracy\')\n\n# Create the op for initializing all variables\ninit = tf.global_variables_initializer()\n\n# Launch the graph (session)\nwith tf.Session() as sess:\n    sess.run(init)\n    global_step = 0\n    # Number of training iterations in each epoch\n    num_tr_iter = int(len(y_train) / batch_size)\n    for epoch in range(epochs):\n        print(\'Training epoch: {}\'.format(epoch + 1))\n        x_train, y_train = randomize(x_train, y_train)\n        for iteration in range(num_tr_iter):\n            global_step += 1\n            start = iteration * batch_size\n            end = (iteration + 1) * batch_size\n            x_batch, y_batch = get_next_batch(x_train, y_train, start, end)\n\n            # Run optimization op (backprop)\n            feed_dict_batch = {x: x_batch, y: y_batch}\n            sess.run(optimizer, feed_dict=feed_dict_batch)\n\n            if iteration % display_freq == 0:\n                # Calculate and display the batch loss and accuracy\n                loss_batch, acc_batch = sess.run([loss, accuracy],\n                                                 feed_dict=feed_dict_batch)\n\n                print(""iter {0:3d}:\\t Loss={1:.2f},\\tTraining Accuracy={2:.01%}"".\n                      format(iteration, loss_batch, acc_batch))\n\n        # Run validation after every epoch\n        feed_dict_valid = {x: x_valid[:1000], y: y_valid[:1000]}\n        loss_valid, acc_valid = sess.run([loss, accuracy], feed_dict=feed_dict_valid)\n        print(\'---------------------------------------------------------\')\n        print(""Epoch: {0}, validation loss: {1:.2f}, validation accuracy: {2:.01%}"".\n              format(epoch + 1, loss_valid, acc_valid))\n        print(\'---------------------------------------------------------\')\n\n        # Test the network after training\n        # Accuracy\n        x_test, y_test = load_data(mode=\'test\')\n        feed_dict_test = {x: x_test[:1000], y: y_test[:1000]}\n        loss_test, acc_test = sess.run([loss, accuracy], feed_dict=feed_dict_test)\n        print(\'---------------------------------------------------------\')\n        print(""Test loss: {0:.2f}, test accuracy: {1:.01%}"".format(loss_test, acc_test))\n        print(\'---------------------------------------------------------\')\n\n    # Test the network after training\n    x_test, y_test = load_data(mode=\'test\')\n    feed_dict_test = {x: x_test[:1000], y: y_test[:1000]}\n    loss_test, acc_test = sess.run([loss, accuracy], feed_dict=feed_dict_test)\n    print(\'---------------------------------------------------------\')\n    print(""Test loss: {0:.2f}, test accuracy: {1:.01%}"".format(loss_test, acc_test))\n    print(\'---------------------------------------------------------\')\n\n    # Plot some of the correct and misclassified examples\n    cls_pred = sess.run(cls_prediction, feed_dict=feed_dict_test)\n    cls_true = np.argmax(y_test[:1000], axis=1)\n    plot_images(x_test, cls_true, cls_pred, title=\'Correct Examples\')\n    plot_example_errors(x_test[:1000], cls_true, cls_pred, title=\'Misclassified Examples\')\n    plt.show()\n'"
3_Neural_Network/code/ops.py,8,"b'import tensorflow as tf\n\n\n# weight and bais wrappers\ndef weight_variable(name, shape):\n    """"""\n    Create a weight variable with appropriate initialization\n    :param name: weight name\n    :param shape: weight shape\n    :return: initialized weight variable\n    """"""\n    initer = tf.truncated_normal_initializer(stddev=0.01)\n    return tf.get_variable(\'W_\' + name,\n                           dtype=tf.float32,\n                           shape=shape,\n                           initializer=initer)\n\n\ndef bias_variable(name, shape):\n    """"""\n    Create a bias variable with appropriate initialization\n    :param name: bias variable name\n    :param shape: bias variable shape\n    :return: initialized bias variable\n    """"""\n    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n    return tf.get_variable(\'b_\' + name,\n                           dtype=tf.float32,\n                           initializer=initial)\n\n\ndef fc_layer(x, num_units, name, use_relu=True):\n    """"""\n    Create a fully-connected layer\n    :param x: input from previous layer\n    :param num_units: number of hidden units in the fully-connected layer\n    :param name: layer name\n    :param use_relu: boolean to add ReLU non-linearity (or not)\n    :return: The output array\n    """"""\n    in_dim = x.get_shape()[1]\n    W = weight_variable(name, shape=[in_dim, num_units])\n    b = bias_variable(name, [num_units])\n    layer = tf.matmul(x, W)\n    layer += b\n    if use_relu:\n        layer = tf.nn.relu(layer)\n    return layer\n'"
3_Neural_Network/code/utils.py,0,"b'import matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef load_data(mode=\'train\'):\n    """"""\n    Function to (download and) load the MNIST data\n    :param mode: train or test\n    :return: images and the corresponding labels\n    """"""\n    from tensorflow.examples.tutorials.mnist import input_data\n    mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n    if mode == \'train\':\n        x_train, y_train, x_valid, y_valid = mnist.train.images, mnist.train.labels, \\\n                                             mnist.validation.images, mnist.validation.labels\n        return x_train, y_train, x_valid, y_valid\n    elif mode == \'test\':\n        x_test, y_test = mnist.test.images, mnist.test.labels\n    return x_test, y_test\n\n\ndef randomize(x, y):\n    """""" Randomizes the order of data samples and their corresponding labels""""""\n    permutation = np.random.permutation(y.shape[0])\n    shuffled_x = x[permutation, :]\n    shuffled_y = y[permutation]\n    return shuffled_x, shuffled_y\n\n\ndef get_next_batch(x, y, start, end):\n    x_batch = x[start:end]\n    y_batch = y[start:end]\n    return x_batch, y_batch\n\n\ndef plot_images(images, cls_true, cls_pred=None, title=None):\n    """"""\n    Create figure with 3x3 sub-plots.\n    :param images: array of images to be plotted, (9, img_h*img_w)\n    :param cls_true: corresponding true labels (9,)\n    :param cls_pred: corresponding true labels (9,)\n    """"""\n    fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n    for i, ax in enumerate(axes.flat):\n        # Plot image.\n        ax.imshow(images[i].reshape(28, 28), cmap=\'binary\')\n\n        # Show true and predicted classes.\n        if cls_pred is None:\n            ax_title = ""True: {0}"".format(cls_true[i])\n        else:\n            ax_title = ""True: {0}, Pred: {1}"".format(cls_true[i], cls_pred[i])\n\n        ax.set_title(ax_title)\n\n        # Remove ticks from the plot.\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n    if title:\n        plt.suptitle(title, size=20)\n    plt.show(block=False)\n\n\ndef plot_example_errors(images, cls_true, cls_pred, title=None):\n    """"""\n    Function for plotting examples of images that have been mis-classified\n    :param images: array of all images, (#imgs, img_h*img_w)\n    :param cls_true: corresponding true labels, (#imgs,)\n    :param cls_pred: corresponding predicted labels, (#imgs,)\n    """"""\n    # Negate the boolean array.\n    incorrect = np.logical_not(np.equal(cls_pred, cls_true))\n\n    # Get the images from the test-set that have been\n    # incorrectly classified.\n    incorrect_images = images[incorrect]\n\n    # Get the true and predicted classes for those images.\n    cls_pred = cls_pred[incorrect]\n    cls_true = cls_true[incorrect]\n\n    # Plot the first 9 images.\n    plot_images(images=incorrect_images[0:9],\n                cls_true=cls_true[0:9],\n                cls_pred=cls_pred[0:9],\n                title=title)\n'"
4_Tensorboard/code/main.py,22,"b'# imports\nimport os\nimport tensorflow as tf\nimport numpy as np\nfrom ops import fc_layer\nfrom utils import *\nfrom tensorflow.contrib.tensorboard.plugins import projector\n\n# Import MNIST data\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\nprint(""Size of:"")\nprint(""- Training-set:\\t\\t{}"".format(len(mnist.train.labels)))\nprint(""- Test-set:\\t\\t{}"".format(len(mnist.test.labels)))\nprint(""- Validation-set:\\t{}"".format(len(mnist.validation.labels)))\n\n# hyper-parameters\nlogs_path = ""./logs/full""  # path to the folder that we want to save the logs for TensorBoard\nlearning_rate = 0.001  # The optimization learning rate\nepochs = 10  # Total number of training epochs\nbatch_size = 100  # Training batch size\ndisplay_freq = 100  # Frequency of displaying the training results\n\n# Network Parameters\n# We know that MNIST images are 28 pixels in each dimension.\nimg_h = img_w = 28\n\n# Images are stored in one-dimensional arrays of this length.\nimg_size_flat = img_h * img_w\n\n# Number of classes, one class for each of 10 digits.\nn_classes = 10\n\n# number of units in the first hidden layer\nh1 = 200\n\n# Create graph\n# Placeholders for inputs (x), outputs(y)\nwith tf.variable_scope(\'Input\'):\n    x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name=\'X\')\n    tf.summary.image(\'input_image\', tf.reshape(x, (-1, img_w, img_h, 1)), max_outputs=5)\n    y = tf.placeholder(tf.float32, shape=[None, n_classes], name=\'Y\')\n\nfc1 = fc_layer(x, h1, \'Hidden_layer\', use_relu=True)\noutput_logits = fc_layer(fc1, n_classes, \'Output_layer\', use_relu=False)\n\n# Define the loss function, optimizer, and accuracy\nwith tf.variable_scope(\'Train\'):\n    with tf.variable_scope(\'Loss\'):\n        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits), name=\'loss\')\n        tf.summary.scalar(\'loss\', loss)\n    with tf.variable_scope(\'Optimizer\'):\n        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name=\'Adam-op\').minimize(loss)\n    with tf.variable_scope(\'Accuracy\'):\n        correct_prediction = tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1), name=\'correct_pred\')\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\'accuracy\')\n        tf.summary.scalar(\'accuracy\', accuracy)\n    with tf.variable_scope(\'Prediction\'):\n        # Network predictions\n        cls_prediction = tf.argmax(output_logits, axis=1, name=\'predictions\')\n\n# Initialize the variables\ninit = tf.global_variables_initializer()\n# Merge all summaries\nmerged = tf.summary.merge_all()\n\n# Launch the graph (session)\nwith tf.Session() as sess:\n    sess.run(init)\n    train_writer = tf.summary.FileWriter(logs_path, sess.graph)\n    num_tr_iter = int(mnist.train.num_examples / batch_size)\n    global_step = 0\n    for epoch in range(epochs + 1):\n        print(\'Training epoch: {}\'.format(epoch))\n        for iteration in range(num_tr_iter):\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n            global_step += 1\n            # Run optimization op (backprop)\n            feed_dict_batch = {x: batch_x, y: batch_y}\n            _, summary_tr = sess.run([optimizer, merged], feed_dict=feed_dict_batch)\n            train_writer.add_summary(summary_tr, global_step)\n\n            if iteration % display_freq == 0:\n                # Calculate and display the batch loss and accuracy\n                loss_batch, acc_batch = sess.run([loss, accuracy],\n                                                 feed_dict=feed_dict_batch)\n                print(""iter {0:3d}:\\t Loss={1:.2f},\\tTraining Accuracy={2:.01%}"".\n                      format(iteration, loss_batch, acc_batch))\n\n        # Run validation after every epoch\n        feed_dict_valid = {x: mnist.validation.images, y: mnist.validation.labels}\n        loss_valid, acc_valid = sess.run([loss, accuracy], feed_dict=feed_dict_valid)\n        print(\'---------------------------------------------------------\')\n        print(""Epoch: {0}, validation loss: {1:.2f}, validation accuracy: {2:.01%}"".\n              format(epoch + 1, loss_valid, acc_valid))\n        print(\'---------------------------------------------------------\')\n\n    # Load the test set\n    x_test = mnist.test.images\n    y_test = mnist.test.labels\n\n    # Initialize the embedding variable with the shape of our desired tensor\n    tensor_shape = (x_test.shape[0], fc1.get_shape()[1].value)  # [test_set , h1] = [10000 , 200]\n    embedding_var = tf.Variable(tf.zeros(tensor_shape),\n                                name=\'fc1_embedding\')\n    # assign the tensor that we want to visualize to the embedding variable\n    embedding_assign = embedding_var.assign(fc1)\n\n    # Create a config object to write the configuration parameters\n    config = projector.ProjectorConfig()\n\n    # Add embedding variable\n    embedding = config.embeddings.add()\n    embedding.tensor_name = embedding_var.name\n\n    # Link this tensor to its metadata file (e.g. labels) -> we will create this file later\n    embedding.metadata_path = \'metadata.tsv\'\n\n    # Specify where you find the sprite. -> we will create this image later\n    embedding.sprite.image_path = \'sprite_images.png\'\n    embedding.sprite.single_image_dim.extend([img_w, img_h])\n\n    # Write a projector_config.pbtxt in the logs_path.\n    # TensorBoard will read this file during startup.\n    projector.visualize_embeddings(train_writer, config)\n\n    # Run session to evaluate the tensor\n    x_test_fc1 = sess.run(embedding_assign, feed_dict={x: x_test})\n\n    # Save the tensor in model.ckpt file\n    saver = tf.train.Saver()\n    saver.save(sess, os.path.join(logs_path, ""model.ckpt""), global_step)\n\n    # Reshape images from vector to matrix\n    x_test_images = np.reshape(np.array(x_test), (-1, img_w, img_h))\n    # Reshape labels from one-hot-encode to index\n    x_test_labels = np.argmax(y_test, axis=1)\n\n    write_sprite_image(os.path.join(logs_path, \'sprite_images.png\'), x_test_images)\n    write_metadata(os.path.join(logs_path, \'metadata.tsv\'), x_test_labels)'"
4_Tensorboard/code/ops.py,11,"b'import tensorflow as tf\n\n\n# weight and bais wrappers\ndef weight_variable(name, shape):\n    """"""\n    Create a weight variable with appropriate initialization\n    :param name: weight name\n    :param shape: weight shape\n    :return: initialized weight variable\n    """"""\n    initer = tf.truncated_normal_initializer(stddev=0.01)\n    return tf.get_variable(\'W_\' + name,\n                           dtype=tf.float32,\n                           shape=shape,\n                           initializer=initer)\n\n\ndef bias_variable(name, shape):\n    """"""\n    Create a bias variable with appropriate initialization\n    :param name: bias variable name\n    :param shape: bias variable shape\n    :return: initialized bias variable\n    """"""\n    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n    return tf.get_variable(\'b_\' + name,\n                           dtype=tf.float32,\n                           initializer=initial)\n\n\ndef fc_layer(x, num_units, name, use_relu=True):\n    """"""\n    Create a fully-connected layer\n    :param x: input from previous layer\n    :param num_units: number of hidden units in the fully-connected layer\n    :param name: layer name\n    :param use_relu: boolean to add ReLU non-linearity (or not)\n    :return: The output array\n    """"""\n    with tf.variable_scope(name):\n        in_dim = x.get_shape()[1]\n        W = weight_variable(name, shape=[in_dim, num_units])\n        tf.summary.histogram(\'W\', W)\n        b = bias_variable(name, [num_units])\n        tf.summary.histogram(\'b\', b)\n        layer = tf.matmul(x, W)\n        layer += b\n        if use_relu:\n            layer = tf.nn.relu(layer)\n        return layer\n'"
4_Tensorboard/code/utils.py,0,"b'import matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef write_sprite_image(filename, images):\n    """"""\n        Create a sprite image consisting of sample images\n        :param filename: name of the file to save on disk\n        :param shape: tensor of flattened images\n    """"""\n    num_img, img_w, img_h = images.shape\n    # Invert grayscale image\n    images = 1 - images\n\n    # Calculate number of plot\n    n_plots = int(np.ceil(np.sqrt(num_img)))\n\n    # Make the background of sprite image\n    sprite_image = np.ones((img_h * n_plots, img_w * n_plots))\n\n    for i in range(n_plots):\n        for j in range(n_plots):\n            img_idx = i * n_plots + j\n            if img_idx < images.shape[0]:\n                img = images[img_idx]\n                sprite_image[i * img_h:(i + 1) * img_h,\n                j * img_w:(j + 1) * img_w] = img\n\n    plt.imsave(filename, sprite_image, cmap=\'gray\')\n    print(\'Sprite image saved in {}\'.format(filename))\n\n\ndef write_metadata(filename, labels):\n    """"""\n            Create a metadata file image consisting of sample indices and labels\n            :param filename: name of the file to save on disk\n            :param shape: tensor of labels\n    """"""\n    with open(filename, \'w\') as f:\n        f.write(""Index\\tLabel\\n"")\n        for index, label in enumerate(labels):\n            f.write(""{}\\t{}\\n"".format(index, label))\n\n    print(\'Metadata file saved in {}\'.format(filename))\n\n'"
5_AutoEncoder/code/main.py,18,"b'# imports\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Import MNIST data\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nfrom ops import fc_layer\nfrom utils import plot_max_active, plot_images\n\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n\nprint(""Size of:"")\nprint(""- Training-set:\\t\\t{}"".format(len(mnist.train.labels)))\nprint(""- Test-set:\\t\\t{}"".format(len(mnist.test.labels)))\nprint(""- Validation-set:\\t{}"".format(len(mnist.validation.labels)))\n\n# hyper-parameters\nlogs_path = ""./logs/full""  # path to the folder that we want to save the logs for Tensorboard\nlearning_rate = 0.001  # The optimization learning rate\nepochs = 10  # Total number of training epochs\nbatch_size = 100  # Training batch size\ndisplay_freq = 100  # Frequency of displaying the training results\n\n# Network Parameters\n# We know that MNIST images are 28 pixels in each dimension.\nimg_h = img_w = 28\n\n# Images are stored in one-dimensional arrays of this length.\nimg_size_flat = img_h * img_w\n\n# number of units in the hidden layer\nh1 = 100\n\n# level of the noise in noisy data\nnoise_level = 0.6\n\n# Create graph\n# Placeholders for inputs (x), outputs(y)\nwith tf.variable_scope(\'Input\'):\n    x_original = tf.placeholder(tf.float32, shape=[None, img_size_flat], name=\'X_original\')\n    x_noisy = tf.placeholder(tf.float32, shape=[None, img_size_flat], name=\'X_noisy\')\n\n\nfc1, W1 = fc_layer(x_noisy, h1, \'Hidden_layer\', use_relu=True)\nout, W2 = fc_layer(fc1, img_size_flat, \'Output_layer\', use_relu=False)\n\n# calculate the activation\nh_active = W1 / tf.sqrt(tf.reduce_sum(tf.square(W1), axis=0)) # [784, 100]\n\n# Define the loss function, optimizer, and accuracy\nwith tf.variable_scope(\'Train\'):\n    with tf.variable_scope(\'Loss\'):\n        loss = tf.reduce_mean(tf.losses.mean_squared_error(x_original, out), name=\'loss\')\n        tf.summary.scalar(\'loss\', loss)\n    with tf.variable_scope(\'Optimizer\'):\n        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name=\'Adam-op\').minimize(loss)\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n# Add 5 images from original, noisy and reconstructed samples to summaries\ntf.summary.image(\'original\', tf.reshape(x_original, (-1, img_w, img_h, 1)), max_outputs=5)\ntf.summary.image(\'noisy\', tf.reshape(x_noisy, (-1, img_w, img_h, 1)), max_outputs=5)\ntf.summary.image(\'reconstructed\', tf.reshape(out, (-1, img_w, img_h, 1)), max_outputs=5)\n\nmerged = tf.summary.merge_all()\n\n# Launch the graph (session)\nwith tf.Session() as sess:\n    sess.run(init)\n    train_writer = tf.summary.FileWriter(logs_path, sess.graph)\n    num_tr_iter = int(mnist.train.num_examples / batch_size)\n    global_step = 0\n    for epoch in range(epochs):\n        print(\'Training epoch: {}\'.format(epoch + 1))\n        for iteration in range(num_tr_iter):\n            batch_x, _ = mnist.train.next_batch(batch_size)\n            batch_x_noisy = batch_x + noise_level * np.random.normal(loc=0.0, scale=1.0, size=batch_x.shape)\n\n            global_step += 1\n            # Run optimization op (backprop)\n            feed_dict_batch = {x_original: batch_x, x_noisy: batch_x_noisy}\n            _, summary_tr = sess.run([optimizer, merged], feed_dict=feed_dict_batch)\n            train_writer.add_summary(summary_tr, global_step)\n\n            if iteration % display_freq == 0:\n                # Calculate and display the batch loss and accuracy\n                loss_batch = sess.run(loss,\n                                      feed_dict=feed_dict_batch)\n                print(""iter {0:3d}:\\t Reconstruction loss={1:.3f}"".\n                      format(iteration, loss_batch))\n\n        # Run validation after every epoch\n        x_valid_original  = mnist.validation.images\n        x_valid_noisy = x_valid_original + noise_level * np.random.normal(loc=0.0, scale=1.0, size=x_valid_original.shape)\n\n        feed_dict_valid = {x_original: x_valid_original, x_noisy: x_valid_noisy}\n        loss_valid = sess.run(loss, feed_dict=feed_dict_valid)\n        print(\'---------------------------------------------------------\')\n        print(""Epoch: {0}, validation loss: {1:.3f}"".\n              format(epoch + 1, loss_valid))\n        print(\'---------------------------------------------------------\')\n\n    # Test the network after training\n    # Make a noisy image\n    test_samples = 5\n    x_test = mnist.test.images[:test_samples]\n    x_test_noisy = x_test + noise_level * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n    # Reconstruct a clean image from noisy image\n    x_reconstruct = sess.run(out, feed_dict={x_noisy: x_test_noisy})\n    # Calculate the loss between reconstructed image and original image\n    loss_test = sess.run(loss, feed_dict={x_original: x_test, x_noisy: x_test_noisy})\n    print(\'---------------------------------------------------------\')\n    print(""Test loss of original image compared to reconstructed image : {0:.3f}"".format(loss_test))\n    print(\'---------------------------------------------------------\')\n\n    # Plot original image, noisy image and reconstructed image\n    plot_images(x_test, x_test_noisy, x_reconstruct)\n\n    # Plot the images that maximally activate the hidden units\n    plot_max_active(sess.run(h_active))\n    plt.show()\n\n\n\n\n\n# Load the test set\nx_test = mnist.test.images\ny_test = mnist.test.labels\n\n\n# Initialize the embedding variable with the shape of our desired tensor\ntensor_shape = (x_test.shape[0] , fc1.get_shape()[1].value) # [test_set , h1] = [10000 , 200]\nembedding_var = tf.Variable(tf.zeros(tensor_shape),\n                            name=\'fc1_embedding\')\n# assign the tensor that we want to visualize to the embedding variable\nembedding_assign = embedding_var.assign(fc1)\n\n'"
5_AutoEncoder/code/ops.py,11,"b'import tensorflow as tf\n\n# weight and bais wrappers\ndef weight_variable(name, shape):\n    """"""\n    Create a weight variable with appropriate initialization\n    :param name: weight name\n    :param shape: weight shape\n    :return: initialized weight variable\n    """"""\n    initer = tf.truncated_normal_initializer(stddev=0.01)\n    return tf.get_variable(\'W_\' + name,\n                           dtype=tf.float32,\n                           shape=shape,\n                           initializer=initer)\n\n\ndef bias_variable(name, shape):\n    """"""\n    Create a bias variable with appropriate initialization\n    :param name: bias variable name\n    :param shape: bias variable shape\n    :return: initialized bias variable\n    """"""\n    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n    return tf.get_variable(\'b_\' + name,\n                           dtype=tf.float32,\n                           initializer=initial)\n\n\ndef fc_layer(x, num_units, name, use_relu=True):\n    """"""\n    Create a fully-connected layer\n    :param x: input from previous layer\n    :param num_units: number of hidden units in the fully-connected layer\n    :param name: layer name\n    :param use_relu: boolean to add ReLU non-linearity (or not)\n    :return: The output array\n    """"""\n    with tf.variable_scope(name):\n        in_dim = x.get_shape()[1]\n        W = weight_variable(name, shape=[in_dim, num_units])\n        tf.summary.histogram(\'W\', W)\n        b = bias_variable(name, [num_units])\n        tf.summary.histogram(\'b\', b)\n        layer = tf.matmul(x, W)\n        layer += b\n        if use_relu:\n            layer = tf.nn.relu(layer)\n        return layer, W'"
5_AutoEncoder/code/utils.py,0,"b'import matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef plot_images(original_images, noisy_images, reconstructed_images):\n    """"""\n    Create figure of original and reconstructed image.\n    :param original_image: original images to be plotted, (?, img_h*img_w)\n    :param noisy_image: original images to be plotted, (?, img_h*img_w)\n    :param reconstructed_image: reconstructed images to be plotted, (?, img_h*img_w)\n    """"""\n    num_images = original_images.shape[0]\n    fig, axes = plt.subplots(num_images, 3, figsize=(9, 9))\n    fig.subplots_adjust(hspace=.1, wspace=0)\n\n    img_h = img_w = np.sqrt(original_images.shape[-1]).astype(int)\n    for i, ax in enumerate(axes):\n        # Plot image.\n        ax[0].imshow(original_images[i].reshape((img_h, img_w)), cmap=\'gray\')\n        ax[1].imshow(noisy_images[i].reshape((img_h, img_w)), cmap=\'gray\')\n        ax[2].imshow(reconstructed_images[i].reshape((img_h, img_w)), cmap=\'gray\')\n\n        # Remove ticks from the plot.\n        for sub_ax in ax:\n            sub_ax.set_xticks([])\n            sub_ax.set_yticks([])\n\n    for ax, col in zip(axes[0], [""Original Image"", ""Noisy Image"", ""Reconstructed Image""]):\n        ax.set_title(col)\n\n    fig.tight_layout()\n    plt.show(block=False)\n\n\ndef plot_max_active(x):\n    """"""\n    Plots the images that are maximally activating the hidden units\n    :param x: numpy array of size [input_dim, num_hidden_units]\n    """"""\n    fig, axes = plt.subplots(nrows=10, ncols=10, figsize=(9, 9))\n    fig.subplots_adjust(hspace=.1, wspace=0)\n    img_h = img_w = np.sqrt(x.shape[0]).astype(int)\n    for i, ax in enumerate(axes.flat):\n        # Plot image.\n        ax.imshow(x[:, i].reshape((img_h, img_w)), cmap=\'gray\')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_yticklabels([])\n        ax.set_xticklabels([])\n    plt.show(block=False)\n'"
6_Convolutional_Neural_Network/code/main.py,19,"b'import tensorflow as tf\nimport numpy as np\nfrom utils import *\nfrom ops import *\n\n# Data Dimensions\nimg_h = img_w = 28  # MNIST images are 28x28\nimg_size_flat = img_h * img_w  # 28x28=784, the total number of pixels\nn_classes = 10  # Number of classes, one class per digit\nn_channels = 1\n\n# Load MNIST data\nx_train, y_train, x_valid, y_valid = load_data(mode=\'train\')\nprint(""Size of:"")\nprint(""- Training-set:\\t\\t{}"".format(len(y_train)))\nprint(""- Validation-set:\\t{}"".format(len(y_valid)))\n\n# Hyper-parameters\nlogs_path = ""./logs""  # path to the folder that we want to save the logs for Tensorboard\nlr = 0.001  # The optimization initial learning rate\nepochs = 10  # Total number of training epochs\nbatch_size = 100  # Training batch size\ndisplay_freq = 100  # Frequency of displaying the training results\n\n# Network Configuration\n# 1st Convolutional Layer\nfilter_size1 = 5  # Convolution filters are 5 x 5 pixels.\nnum_filters1 = 16  # There are 16 of these filters.\nstride1 = 1  # The stride of the sliding window\n\n# 2nd Convolutional Layer\nfilter_size2 = 5  # Convolution filters are 5 x 5 pixels.\nnum_filters2 = 32  # There are 32 of these filters.\nstride2 = 1  # The stride of the sliding window\n\n# Fully-connected layer.\nh1 = 128  # Number of neurons in fully-connected layer.\n\n# Create the network graph\n# Placeholders for inputs (x), outputs(y)\nwith tf.name_scope(\'Input\'):\n    x = tf.placeholder(tf.float32, shape=[None, img_h, img_w, n_channels], name=\'X\')\n    y = tf.placeholder(tf.float32, shape=[None, n_classes], name=\'Y\')\n\nconv1 = conv_layer(x, filter_size1, num_filters1, stride1, name=\'conv1\')\npool1 = max_pool(conv1, ksize=2, stride=2, name=\'pool1\')\nconv2 = conv_layer(pool1, filter_size2, num_filters2, stride2, name=\'conv2\')\npool2 = max_pool(conv2, ksize=2, stride=2, name=\'pool2\')\nlayer_flat = flatten_layer(pool2)\nfc1 = fc_layer(layer_flat, h1, \'FC1\', use_relu=True)\noutput_logits = fc_layer(fc1, n_classes, \'OUT\', use_relu=False)\n\n# Define the loss function, optimizer, and accuracy\nwith tf.variable_scope(\'Train\'):\n    with tf.variable_scope(\'Loss\'):\n        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits), name=\'loss\')\n    tf.summary.scalar(\'loss\', loss)\n    with tf.variable_scope(\'Optimizer\'):\n        optimizer = tf.train.AdamOptimizer(learning_rate=lr, name=\'Adam-op\').minimize(loss)\n    with tf.variable_scope(\'Accuracy\'):\n        correct_prediction = tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1), name=\'correct_pred\')\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\'accuracy\')\n    tf.summary.scalar(\'accuracy\', accuracy)\n    with tf.variable_scope(\'Prediction\'):\n        # Network predictions\n        cls_prediction = tf.argmax(output_logits, axis=1, name=\'predictions\')\n\n# Creating the op for initializing all variables\ninit = tf.global_variables_initializer()\n# Merge all summaries\nmerged = tf.summary.merge_all()\n\n# Launch the graph (session)\nwith tf.Session() as sess:\n    sess.run(init)\n    global_step = 0\n    summary_writer = tf.summary.FileWriter(logs_path, sess.graph)\n    # Number of training iterations in each epoch\n    num_tr_iter = int(len(y_train) / batch_size)\n    for epoch in range(epochs):\n        print(\'Training epoch: {}\'.format(epoch + 1))\n        x_train, y_train = randomize(x_train, y_train)\n        for iteration in range(num_tr_iter):\n            global_step += 1\n            start = iteration * batch_size\n            end = (iteration + 1) * batch_size\n            x_batch, y_batch = get_next_batch(x_train, y_train, start, end)\n\n            # Run optimization op (backprop)\n            feed_dict_batch = {x: x_batch, y: y_batch}\n            sess.run(optimizer, feed_dict=feed_dict_batch)\n\n            if iteration % display_freq == 0:\n                # Calculate and display the batch loss and accuracy\n                loss_batch, acc_batch, summary_tr = sess.run([loss, accuracy, merged],\n                                                             feed_dict=feed_dict_batch)\n                summary_writer.add_summary(summary_tr, global_step)\n\n                print(""iter {0:3d}:\\t Loss={1:.2f},\\tTraining Accuracy={2:.01%}"".\n                      format(iteration, loss_batch, acc_batch))\n\n        # Run validation after every epoch\n        feed_dict_valid = {x: x_valid, y: y_valid}\n        loss_valid, acc_valid, summary_val = sess.run([loss, accuracy, merged], feed_dict=feed_dict_valid)\n        summary_writer.add_summary(summary_val, global_step)\n        print(\'---------------------------------------------------------\')\n        print(""Epoch: {0}, validation loss: {1:.2f}, validation accuracy: {2:.01%}"".\n              format(epoch + 1, loss_valid, acc_valid))\n        print(\'---------------------------------------------------------\')\n\n    # Test the network when training is done\n    x_test, y_test = load_data(mode=\'test\')\n    feed_dict_test = {x: x_test, y: y_test}\n    loss_test, acc_test = sess.run([loss, accuracy], feed_dict=feed_dict_test)\n    print(\'---------------------------------------------------------\')\n    print(""Test loss: {0:.2f}, test accuracy: {1:.01%}"".format(loss_test, acc_test))\n    print(\'---------------------------------------------------------\')\n\n    # Plot some of the correct and misclassified examples\n    cls_pred = sess.run(cls_prediction, feed_dict=feed_dict_test)\n    cls_true = np.argmax(y_test, axis=1)\n    plot_images(x_test, cls_true, cls_pred, title=\'Correct Examples\')\n    plot_example_errors(x_test, cls_true, cls_pred, title=\'Misclassified Examples\')\n    plt.show()\n'"
6_Convolutional_Neural_Network/code/ops.py,19,"b'import tensorflow as tf\n\n\n# weight and bais wrappers\ndef weight_variable(shape):\n    """"""\n    Create a weight variable with appropriate initialization\n    :param name: weight name\n    :param shape: weight shape\n    :return: initialized weight variable\n    """"""\n    initer = tf.truncated_normal_initializer(stddev=0.01)\n    return tf.get_variable(\'W\',\n                           dtype=tf.float32,\n                           shape=shape,\n                           initializer=initer)\n\n\ndef bias_variable(shape):\n    """"""\n    Create a bias variable with appropriate initialization\n    :param name: bias variable name\n    :param shape: bias variable shape\n    :return: initialized bias variable\n    """"""\n    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n    return tf.get_variable(\'b\',\n                           dtype=tf.float32,\n                           initializer=initial)\n\n\ndef fc_layer(x, num_units, name, use_relu=True):\n    """"""\n    Create a fully-connected layer\n    :param x: input from previous layer\n    :param num_units: number of hidden units in the fully-connected layer\n    :param name: layer name\n    :param use_relu: boolean to add ReLU non-linearity (or not)\n    :return: The output array\n    """"""\n    with tf.variable_scope(name):\n        in_dim = x.get_shape()[1]\n        W = weight_variable(shape=[in_dim, num_units])\n        tf.summary.histogram(\'weight\', W)\n        b = bias_variable(shape=[num_units])\n        tf.summary.histogram(\'bias\', b)\n        layer = tf.matmul(x, W)\n        layer += b\n        if use_relu:\n            layer = tf.nn.relu(layer)\n        return layer\n\n\ndef conv_layer(x, filter_size, num_filters, stride, name):\n    """"""\n    Create a 2D convolution layer\n    :param x: input from previous layer\n    :param filter_size: size of each filter\n    :param num_filters: number of filters (or output feature maps)\n    :param stride: filter stride\n    :param name: layer name\n    :return: The output array\n    """"""\n    with tf.variable_scope(name):\n        num_in_channel = x.get_shape().as_list()[-1]\n        shape = [filter_size, filter_size, num_in_channel, num_filters]\n        W = weight_variable(shape=shape)\n        tf.summary.histogram(\'weight\', W)\n        b = bias_variable(shape=[num_filters])\n        tf.summary.histogram(\'bias\', b)\n        layer = tf.nn.conv2d(x, W,\n                             strides=[1, stride, stride, 1],\n                             padding=""SAME"")\n        layer += b\n        return tf.nn.relu(layer)\n\n\ndef flatten_layer(layer):\n    """"""\n    Flattens the output of the convolutional layer to be fed into fully-connected layer\n    :param layer: input array\n    :return: flattened array\n    """"""\n    with tf.variable_scope(\'Flatten_layer\'):\n        layer_shape = layer.get_shape()\n        num_features = layer_shape[1:4].num_elements()\n        layer_flat = tf.reshape(layer, [-1, num_features])\n    return layer_flat\n\n\ndef max_pool(x, ksize, stride, name):\n    """"""\n    Create a max pooling layer\n    :param x: input to max-pooling layer\n    :param ksize: size of the max-pooling filter\n    :param stride: stride of the max-pooling filter\n    :param name: layer name\n    :return: The output array\n    """"""\n    return tf.nn.max_pool(x,\n                          ksize=[1, ksize, ksize, 1],\n                          strides=[1, stride, stride, 1],\n                          padding=""SAME"",\n                          name=name)\n'"
6_Convolutional_Neural_Network/code/utils.py,0,"b'import numpy as np\nimport random\nimport scipy\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport matplotlib.pyplot as plt\n\n\ndef load_data(mode=\'train\'):\n    """"""\n    Function to (download and) load the MNIST data\n    :param mode: train or test\n    :return: images and the corresponding labels\n    """"""\n    mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n    if mode == \'train\':\n        x_train, y_train, x_valid, y_valid = mnist.train.images, mnist.train.labels, \\\n                                             mnist.validation.images, mnist.validation.labels\n        x_train, _ = reformat(x_train, y_train)\n        x_valid, _ = reformat(x_valid, y_valid)\n        return x_train, y_train, x_valid, y_valid\n    elif mode == \'test\':\n        x_test, y_test = mnist.test.images, mnist.test.labels\n        x_test, _ = reformat(x_test, y_test)\n    return x_test, y_test\n\n\ndef randomize(x, y):\n    """""" Randomizes the order of data samples and their corresponding labels""""""\n    permutation = np.random.permutation(y.shape[0])\n    shuffled_x = x[permutation, :, :, :]\n    shuffled_y = y[permutation]\n    return shuffled_x, shuffled_y\n\n\ndef reformat(x, y):\n    """"""\n    Reformats the data to the format acceptable for convolutional layers\n    :param x: input array\n    :param y: corresponding labels\n    :return: reshaped input and labels\n    """"""\n    img_size, num_ch, num_class = int(np.sqrt(x.shape[-1])), 1, len(np.unique(np.argmax(y, 1)))\n    dataset = x.reshape((-1, img_size, img_size, num_ch)).astype(np.float32)\n    labels = (np.arange(num_class) == y[:, None]).astype(np.float32)\n    return dataset, labels\n\n\ndef get_next_batch(x, y, start, end):\n    x_batch = x[start:end]\n    y_batch = y[start:end]\n    return x_batch, y_batch\n\ndef plot_images(images, cls_true, cls_pred=None, title=None):\n    """"""\n    Create figure with 3x3 sub-plots.\n    :param images: array of images to be plotted, (9, img_h*img_w)\n    :param cls_true: corresponding true labels (9,)\n    :param cls_pred: corresponding true labels (9,)\n    """"""\n    fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n    for i, ax in enumerate(axes.flat):\n        # Plot image.\n        ax.imshow(np.squeeze(images[i]), cmap=\'binary\')\n\n        # Show true and predicted classes.\n        if cls_pred is None:\n            ax_title = ""True: {0}"".format(cls_true[i])\n        else:\n            ax_title = ""True: {0}, Pred: {1}"".format(cls_true[i], cls_pred[i])\n\n        ax.set_title(ax_title)\n\n        # Remove ticks from the plot.\n        ax.set_xticks([])\n        ax.set_yticks([])\n\n    if title:\n        plt.suptitle(title, size=20)\n    plt.show(block=False)\n\n\ndef plot_example_errors(images, cls_true, cls_pred, title=None):\n    """"""\n    Function for plotting examples of images that have been mis-classified\n    :param images: array of all images, (#imgs, img_h*img_w)\n    :param cls_true: corresponding true labels, (#imgs,)\n    :param cls_pred: corresponding predicted labels, (#imgs,)\n    """"""\n    # Negate the boolean array.\n    incorrect = np.logical_not(np.equal(cls_pred, cls_true))\n\n    # Get the images from the test-set that have been\n    # incorrectly classified.\n    incorrect_images = images[incorrect]\n\n    # Get the true and predicted classes for those images.\n    cls_pred = cls_pred[incorrect]\n    cls_true = cls_true[incorrect]\n\n    # Plot the first 9 images.\n    plot_images(images=incorrect_images[0:9],\n                cls_true=cls_true[0:9],\n                cls_pred=cls_pred[0:9],\n                title=title)\n'"
7_Recurrent_Neural_Network/code/02_Bidirectional_RNN_for_Classification/03_many_to_one.py,0,b''
7_Recurrent_Neural_Network/code/02_Bidirectional_RNN_for_Classification/main.py,10,"b'import tensorflow as tf\n\n# Load MNIST data\nfrom ops import weight_variable, bias_variable, BiRNN\nfrom utils import load_data, randomize, get_next_batch\n\nx_train, y_train, x_valid, y_valid = load_data(mode=\'train\')\nprint(""Size of:"")\nprint(""- Training-set:\\t\\t{}"".format(len(y_train)))\nprint(""- Validation-set:\\t{}"".format(len(y_valid)))\n\n# Hyper-parameters\nlearning_rate = 0.001   # The optimization initial learning rate\nepochs = 10             # Total number of training epochs\nbatch_size = 100        # Training batch size\ndisplay_freq = 100      # Frequency of displaying the training results\n\n# Network Parameters\nnum_input = 28          # MNIST data input (img shape: 28*28)\ntimesteps = 28          # Timesteps\nnum_hidden_units = 128  # Number of hidden units of the RNN\nn_classes = 10          # Number of classes, one class per digit\n\n\n# Create the graph for the linear model\n# Placeholders for inputs (x) and outputs(y)\nx = tf.placeholder(tf.float32, shape=[None, timesteps, num_input], name=\'X\')\ny = tf.placeholder(tf.float32, shape=[None, n_classes], name=\'Y\')\n\n\n# create weight matrix initialized randomely from N~(0, 0.01)\n# Hidden layer weights => 2*n_hidden because of forward + backward cells\nW = weight_variable(shape=[2*num_hidden_units, n_classes])\n\n# create bias vector initialized as zero\nb = bias_variable(shape=[n_classes])\n\noutput_logits = BiRNN(x, W, b, timesteps, num_hidden_units)\ny_pred = tf.nn.softmax(output_logits)\n\n# Model predictions\ncls_prediction = tf.argmax(output_logits, axis=1, name=\'predictions\')\n\n# Define the loss function, optimizer, and accuracy\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits), name=\'loss\')\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name=\'Adam-op\').minimize(loss)\ncorrect_prediction = tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1), name=\'correct_pred\')\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\'accuracy\')\n\n# Creating the op for initializing all variables\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    global_step = 0\n    # Number of training iterations in each epoch\n    num_tr_iter = int(len(y_train) / batch_size)\n    for epoch in range(epochs):\n        print(\'Training epoch: {}\'.format(epoch + 1))\n        x_train, y_train = randomize(x_train, y_train)\n        for iteration in range(num_tr_iter):\n            global_step += 1\n            start = iteration * batch_size\n            end = (iteration + 1) * batch_size\n            x_batch, y_batch = get_next_batch(x_train, y_train, start, end)\n            x_batch = x_batch.reshape((batch_size, timesteps, num_input))\n            # Run optimization op (backprop)\n            feed_dict_batch = {x: x_batch, y: y_batch}\n            sess.run(optimizer, feed_dict=feed_dict_batch)\n\n            if iteration % display_freq == 0:\n                # Calculate and display the batch loss and accuracy\n                loss_batch, acc_batch = sess.run([loss, accuracy],\n                                                 feed_dict=feed_dict_batch)\n\n                print(""iter {0:3d}:\\t Loss={1:.2f},\\tTraining Accuracy={2:.01%}"".\n                      format(iteration, loss_batch, acc_batch))\n\n        # Run validation after every epoch\n\n        feed_dict_valid = {x: x_valid[:1000].reshape((-1, timesteps, num_input)), y: y_valid[:1000]}\n        loss_valid, acc_valid = sess.run([loss, accuracy], feed_dict=feed_dict_valid)\n        print(\'---------------------------------------------------------\')\n        print(""Epoch: {0}, validation loss: {1:.2f}, validation accuracy: {2:.01%}"".\n              format(epoch + 1, loss_valid, acc_valid))\n        print(\'---------------------------------------------------------\')\n\n    # Test the network after training\n    # Accuracy\n    x_test, y_test = load_data(mode=\'test\')\n    feed_dict_test = {x: x_test[:1000].reshape((-1, timesteps, num_input)), y: y_test[:1000]}\n    loss_test, acc_test = sess.run([loss, accuracy], feed_dict=feed_dict_test)\n    print(\'---------------------------------------------------------\')\n    print(""Test loss: {0:.2f}, test accuracy: {1:.01%}"".format(loss_test, acc_test))\n    print(\'---------------------------------------------------------\')\n'"
7_Recurrent_Neural_Network/code/02_Bidirectional_RNN_for_Classification/ops.py,9,"b'import tensorflow as tf\nfrom tensorflow.contrib import rnn\n\n\ndef weight_variable(shape):\n    """"""\n    Create a weight variable with appropriate initialization\n    :param name: weight name\n    :param shape: weight shape\n    :return: initialized weight variable\n    """"""\n    initer = tf.truncated_normal_initializer(stddev=0.01)\n    return tf.get_variable(\'W\',\n                           dtype=tf.float32,\n                           shape=shape,\n                           initializer=initer)\n\n\ndef bias_variable(shape):\n    """"""\n    Create a bias variable with appropriate initialization\n    :param name: bias variable name\n    :param shape: bias variable shape\n    :return: initialized bias variable\n    """"""\n    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n    return tf.get_variable(\'b\',\n                           dtype=tf.float32,\n                           initializer=initial)\n\n\ndef BiRNN(x, weights, biases, timesteps, num_hidden):\n    # Prepare data shape to match `rnn` function requirements\n    # Current data input shape: (batch_size, timesteps, n_input)\n    # Required shape: \'timesteps\' tensors list of shape (batch_size, num_input)\n\n    # Unstack to get a list of \'timesteps\' tensors of shape (batch_size, num_input)\n    x = tf.unstack(x, timesteps, 1)\n\n    # Define lstm cells with tensorflow\n    # Forward direction cell\n    lstm_fw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n    # Backward direction cell\n    lstm_bw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n\n    # Get BiRNN cell output\n    outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n                                                 dtype=tf.float32)\n\n    # Linear activation, using rnn inner loop last output\n    return tf.matmul(outputs[-1], weights) + biases\n'"
7_Recurrent_Neural_Network/code/02_Bidirectional_RNN_for_Classification/utils.py,0,"b'import numpy as np\n\n\ndef load_data(mode=\'train\'):\n    """"""\n    Function to (download and) load the MNIST data\n    :param mode: train or test\n    :return: images and the corresponding labels\n    """"""\n    from tensorflow.examples.tutorials.mnist import input_data\n    mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n    if mode == \'train\':\n        x_train, y_train, x_valid, y_valid = mnist.train.images, mnist.train.labels, \\\n                                             mnist.validation.images, mnist.validation.labels\n        return x_train, y_train, x_valid, y_valid\n    elif mode == \'test\':\n        x_test, y_test = mnist.test.images, mnist.test.labels\n    return x_test, y_test\n\n\ndef randomize(x, y):\n    """""" Randomizes the order of data samples and their corresponding labels""""""\n    permutation = np.random.permutation(y.shape[0])\n    shuffled_x = x[permutation, :]\n    shuffled_y = y[permutation]\n    return shuffled_x, shuffled_y\n\n\ndef reformat(x, y):\n    """"""\n    Reformats the data to the format acceptable for convolutional layers\n    :param x: input array\n    :param y: corresponding labels\n    :return: reshaped input and labels\n    """"""\n    img_size, num_ch, num_class = int(np.sqrt(x.shape[-1])), 1, len(np.unique(np.argmax(y, 1)))\n    dataset = x.reshape((-1, img_size, img_size, num_ch)).astype(np.float32)\n    labels = (np.arange(num_class) == y[:, None]).astype(np.float32)\n    return dataset, labels\n\n\ndef get_next_batch(x, y, start, end):\n    x_batch = x[start:end]\n    y_batch = y[start:end]\n    return x_batch, y_batch'"
7_Recurrent_Neural_Network/code/02_Static_vs_Dynamic_RNN/main.py,6,"b'import tensorflow as tf\nimport numpy as np\nfrom ops import *\n\n\ninput_dim = 1\nmax_time = 4\nnum_hidden_units = 10\nout_dim = 1\n\nx = tf.placeholder(tf.float32, [None, max_time, input_dim])\ny = tf.placeholder(tf.float32, [None])\n\n# create weight matrix initialized randomely from N~(0, 0.01)\nW = weight_variable(shape=[num_hidden_units, out_dim])\n\n# create bias vector initialized as zero\nb = bias_variable(shape=[out_dim])\n\n# pred_out = Dynamic_LSTM(x, W, b, num_hidden_units)\npred_out = Static_LSTM(x, W, b, max_time, num_hidden_units)\n\ncost = tf.reduce_mean(tf.square(pred_out - y))\ntrain_op = tf.train.AdamOptimizer().minimize(cost)\n\n# Creating the op for initializing all variables\ninit = tf.global_variables_initializer()\n\nx_train = np.array([[[1], [2], [5], [6]],\n           [[5], [7], [7], [8]],\n           [[3], [4], [5], [9]]])\ny_train = np.array([[1, 3, 8, 14],\n           [5, 12, 19, 27],\n           [3, 7, 12, 21]])\n\nx_test = np.array([[[1], [2], [3], [4]],\n          [[4], [5], [6], [7]]])\n\ny_test = np.array([[[1], [3], [6], [10]],\n            [[4], [9], [15], [22]]])\n\n\nwith tf.Session() as sess:\n    sess.run(init)\n    for i in range(10000):\n        _, mse = sess.run([train_op, cost], feed_dict={x: x_train, y: y_train})\n        if i % 1000 == 0:\n            print(\'Step {}, MSE={}\'.format(i, mse))\n    # Test\n        y_pred = sess.run(pred_out, feed_dict={x: x_test})\n\n    for i, x in enumerate(x_test):\n        print(""When the input is {}"".format(x))\n        print(""The ground truth output should be {}"".format(y_test[i]))\n        print(""And the model thinks it is {}\\n"".format(y_pred[i]))\n'"
7_Recurrent_Neural_Network/code/02_Static_vs_Dynamic_RNN/ops.py,20,"b'import tensorflow as tf\n\n\n# weight and bais wrappers\ndef weight_variable(shape):\n    """"""\n    Create a weight variable with appropriate initialization\n    :param name: weight name\n    :param shape: weight shape\n    :return: initialized weight variable\n    """"""\n    initer = tf.truncated_normal_initializer(stddev=0.01)\n    return tf.get_variable(\'W\',\n                           dtype=tf.float32,\n                           shape=shape,\n                           initializer=initer)\n\n\ndef bias_variable(shape):\n    """"""\n    Create a bias variable with appropriate initialization\n    :param name: bias variable name\n    :param shape: bias variable shape\n    :return: initialized bias variable\n    """"""\n    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n    return tf.get_variable(\'b\',\n                           dtype=tf.float32,\n                           initializer=initial)\n\n\ndef LSTM(x, weights, biases, num_hidden):\n    """"""\n    :param x: inputs of size [T, batch_size, input_size]\n    :param weights: matrix of fully-connected output layer weights\n    :param biases: vector of fully-connected output layer biases\n    """"""\n    cell = tf.nn.rnn_cell.LSTMCell(num_hidden)\n    outputs, states = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)\n    num_examples = tf.shape(x)[0]\n    w_repeated = tf.tile(tf.expand_dims(weights, 0), [num_examples, 1, 1])\n    out = tf.matmul(outputs, w_repeated) + biases\n    out = tf.squeeze(out)\n    return out\n\n\n# def Static_LSTM(x, weights, biases, timesteps, num_hidden):\n#\n#     # Prepare data shape to match `rnn` function requirements\n#     # Current data input shape: (batch_size, timesteps, n_input)\n#     # Required shape: \'timesteps\' tensors list of shape (batch_size, n_input)\n#\n#     # Unstack to get a list of \'timesteps\' tensors of shape (batch_size, n_input)\n#     num_examples = tf.shape(x)[0]\n#     x = tf.unstack(x, timesteps, 1)\n#\n#     # Define a rnn cell with tensorflow\n#     cell = tf.nn.rnn_cell.LSTMCell(num_hidden)\n#     w_repeated = tf.tile(tf.expand_dims(weights, 0), [num_examples, 1, 1])\n#     # Get lstm cell output\n#     # If no initial_state is provided, dtype must be specified\n#     # If no initial cell state is provided, they will be initialized to zero\n#     outputs_series, current_state = tf.nn.static_rnn(cell, x, dtype=tf.float32)\n#     outputs = tf.stack(outputs_series, axis=1)\n#     out = tf.matmul(outputs, w_repeated) + biases\n#     out = tf.squeeze(out)\n#\n#     # Linear activation, using rnn inner loop last output\n#     return out\n'"
7_Recurrent_Neural_Network/code/03_Many_to_One_with_Fixed_Sequence_Length/main.py,6,"b'import tensorflow as tf\nimport numpy as np\nfrom ops import *\nfrom utils import next_batch\n\n# Data Dimensions\ninput_dim = 1           # input dimension\nseq_max_len = 4         # sequence maximum length\nout_dim = 1             # output dimension\n\n# Parameters\nlearning_rate = 0.01    # The optimization initial learning rate\ntraining_steps = 10000  # Total number of training steps\nbatch_size = 10         # batch size\ndisplay_freq = 1000     # Frequency of displaying the training results\n\n# Network Configuration\nnum_hidden_units = 10   # number of hidden units\n\n# Create the graph for the model\n# Placeholders for inputs(x), input sequence lengths (seqLen) and outputs(y)\nx = tf.placeholder(tf.float32, [None, seq_max_len, input_dim])\ny = tf.placeholder(tf.float32, [None, 1])\n\n# create weight matrix initialized randomly from N~(0, 0.01)\nW = weight_variable(shape=[num_hidden_units, out_dim])\n\n# create bias vector initialized as zero\nb = bias_variable(shape=[out_dim])\n\n# Network predictions\npred_out = RNN(x, W, b, num_hidden_units)\n\n# Define the loss function (i.e. mean-squared error loss) and optimizer\ncost = tf.reduce_mean(tf.square(pred_out - y))\ntrain_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n\n# Creating the op for initializing all variables\ninit = tf.global_variables_initializer()\n\n# ==========\n#  TOY DATA\n# ==========\nx_train = np.random.randint(0, 10, size=(100, 4, 1))\ny_train = np.sum(x_train, axis=1)\n\nx_test = np.random.randint(0, 10, size=(5, 4, 1))\ny_test = np.sum(x_test, axis=1)\n# ==========\n\n# Launch the graph (session)\nwith tf.Session() as sess:\n    sess.run(init)\n    print(\'----------Training---------\')\n    for i in range(training_steps):\n        x_batch, y_batch = next_batch(x_train, y_train, batch_size)\n        _, mse = sess.run([train_op, cost], feed_dict={x: x_batch, y: y_batch})\n        if i % display_freq == 0:\n            print(\'Step {0:<6}, MSE={1:.4f}\'.format(i, mse))\n    # Test\n    y_pred = sess.run(pred_out, feed_dict={x: x_test})\n    print(\'--------Test Results-------\')\n    for i, x in enumerate(y_test):\n        print(""When the ground truth output is {}, the model thinks it is {}""\n              .format(y_test[i], y_pred[i]))\n'"
7_Recurrent_Neural_Network/code/03_Many_to_One_with_Fixed_Sequence_Length/ops.py,9,"b'import tensorflow as tf\n\n\n# weight and bais wrappers\ndef weight_variable(shape):\n    """"""\n    Create a weight variable with appropriate initialization\n    :param shape: weight shape\n    :return: initialized weight variable\n    """"""\n    initer = tf.truncated_normal_initializer(stddev=0.01)\n    return tf.get_variable(\'W\',\n                           dtype=tf.float32,\n                           shape=shape,\n                           initializer=initer)\n\n\ndef bias_variable(shape):\n    """"""\n    Create a bias variable with appropriate initialization\n    :param shape: bias variable shape\n    :return: initialized bias variable\n    """"""\n    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n    return tf.get_variable(\'b\',\n                           dtype=tf.float32,\n                           initializer=initial)\n\n\ndef RNN(x, weights, biases, num_hidden):\n    """"""\n    :param x: inputs of size [batch_size, max_time, input_dim]\n    :param weights: matrix of fully-connected output layer weights\n    :param biases: vector of fully-connected output layer biases\n    :param num_hidden: number of hidden units\n    """"""\n    cell = tf.nn.rnn_cell.BasicRNNCell(num_hidden)\n    outputs, states = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)\n    out = tf.matmul(outputs[:, -1, :], weights) + biases\n    return out\n'"
7_Recurrent_Neural_Network/code/03_Many_to_One_with_Fixed_Sequence_Length/utils.py,0,"b'import numpy as np\n\n\ndef next_batch(x, y, batch_size):\n    N = x.shape[0]\n    batch_indeces = np.random.permutation(N)[:batch_size]\n    x_batch = x[batch_indeces]\n    y_batch = y[batch_indeces]\n    return x_batch, y_batch\n'"
7_Recurrent_Neural_Network/code/04_Many_to_One_with_Variable_Sequence_Length/main.py,7,"b'import tensorflow as tf\nimport numpy as np\nfrom ops import *\nfrom utils import next_batch, generate_data\n\n# Data Dimensions\ninput_dim = 1           # input dimension\nseq_max_len = 4         # sequence maximum length\nout_dim = 1             # output dimension\n\n# Parameters\nlearning_rate = 0.01    # The optimization initial learning rate\ntraining_steps = 10000  # Total number of training steps\nbatch_size = 10         # batch size\ndisplay_freq = 1000     # Frequency of displaying the training results\n\n# Network Configuration\nnum_hidden_units = 10   # number of hidden units\n\n# Create the graph for the model\n# Placeholders for inputs(x), input sequence lengths (seqLen) and outputs(y)\nx = tf.placeholder(tf.float32, [None, seq_max_len, input_dim])\nseqLen = tf.placeholder(tf.int32, [None])\ny = tf.placeholder(tf.float32, [None, 1])\n\n# create weight matrix initialized randomly from N~(0, 0.01)\nW = weight_variable(shape=[num_hidden_units, out_dim])\n\n# create bias vector initialized as zero\nb = bias_variable(shape=[out_dim])\n\n# Network predictions\npred_out = RNN(x, W, b, num_hidden_units, seq_max_len, seqLen)\n# pred_out = LSTM(x, W, b, num_hidden_units, seq_max_len, seqLen)\n\n# Define the loss function (i.e. mean-squared error loss) and optimizer\ncost = tf.reduce_mean(tf.square(pred_out - y))\ntrain_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n# Creating the op for initializing all variables\ninit = tf.global_variables_initializer()\n\n# ==========\n#  TOY DATA\n# ==========\nx_train, y_train, seq_len_train = generate_data(count=1000, max_length=seq_max_len, dim=input_dim)\nx_test, y_test, seq_len_test = generate_data(count=5, max_length=seq_max_len, dim=input_dim)\n\n# x_test = np.array([[[1], [2], [3], [4]],\n#                     [[1], [2], [0], [0]],\n#                    [[4], [5], [3], [9]]])\n# seq_len_test = np.array([4, 2, 4])\n# y_test = np.array([[10], [3], [21]])\n# ==========\n\n# Launch the graph (session)\nwith tf.Session() as sess:\n    sess.run(init)\n    print(\'----------Training---------\')\n    for i in range(training_steps):\n        x_batch, y_batch, seq_len_batch = next_batch(x_train, y_train, seq_len_train, batch_size)\n        _, mse = sess.run([train_op, cost], feed_dict={x: x_batch, y: y_batch, seqLen: seq_len_batch})\n        if i % display_freq == 0:\n            print(\'Step {0:<6}, MSE={1:.4f}\'.format(i, mse))\n    # Test\n    y_pred = sess.run(pred_out, feed_dict={x: x_test, seqLen: seq_len_test})\n    print(\'--------Test Results-------\')\n    for i, x in enumerate(y_test):\n        print(""When the ground truth output is {}, the model thinks it is {}""\n              .format(y_test[i], y_pred[i]))\n'"
7_Recurrent_Neural_Network/code/04_Many_to_One_with_Variable_Sequence_Length/ops.py,18,"b'import tensorflow as tf\n\n\n# weight and bais wrappers\ndef weight_variable(shape):\n    """"""\n    Create a weight variable with appropriate initialization\n    :param shape: weight shape\n    :return: initialized weight variable\n    """"""\n    initer = tf.truncated_normal_initializer(stddev=0.01)\n    return tf.get_variable(\'W\',\n                           dtype=tf.float32,\n                           shape=shape,\n                           initializer=initer)\n\n\ndef bias_variable(shape):\n    """"""\n    Create a bias variable with appropriate initialization\n    :param shape: bias variable shape\n    :return: initialized bias variable\n    """"""\n    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n    return tf.get_variable(\'b\',\n                           dtype=tf.float32,\n                           initializer=initial)\n\n\ndef RNN(x, weights, biases, n_hidden, seq_max_len, seq_len):\n    """"""\n    :param x: inputs of shape [batch_size, max_time, input_dim]\n    :param weights: matrix of fully-connected output layer weights\n    :param biases: vector of fully-connected output layer biases\n    :param n_hidden: number of hidden units\n    :param seq_max_len: sequence maximum length\n    :param seq_len: length of each sequence of shape [batch_size,]\n    """"""\n    cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n    outputs, states = tf.nn.dynamic_rnn(cell, x, sequence_length=seq_len, dtype=tf.float32)\n\n    # Hack to build the indexing and retrieve the right output.\n    batch_size = tf.shape(outputs)[0]\n    # Start indices for each sample\n    index = tf.range(0, batch_size) * seq_max_len + (seq_len - 1)\n    # Indexing\n    outputs = tf.gather(tf.reshape(outputs, [-1, n_hidden]), index)\n    out = tf.matmul(outputs, weights) + biases\n    return out\n\n\ndef LSTM(x, weights, biases, n_hidden, seq_max_len, seq_len):\n    """"""\n    :param x: inputs of shape [batch_size, max_time, input_dim]\n    :param weights: matrix of fully-connected output layer weights\n    :param biases: vector of fully-connected output layer biases\n    :param n_hidden: number of hidden units\n    :param seq_max_len: sequence maximum length\n    :param seq_len: length of each sequence of shape [batch_size,]\n    """"""\n    cell = tf.nn.rnn_cell.LSTMCell(n_hidden)\n    outputs, states = tf.nn.dynamic_rnn(cell, x, sequence_length=seq_len, dtype=tf.float32)\n\n    # Hack to build the indexing and retrieve the right output.\n    batch_size = tf.shape(outputs)[0]\n    # Start indices for each sample\n    index = tf.range(0, batch_size) * seq_max_len + (seq_len - 1)\n    # Indexing\n    outputs = tf.gather(tf.reshape(outputs, [-1, n_hidden]), index)\n    out = tf.matmul(outputs, weights) + biases\n    return out\n'"
7_Recurrent_Neural_Network/code/04_Many_to_One_with_Variable_Sequence_Length/utils.py,0,"b'import numpy as np\n\n\ndef generate_data(count=1000, max_length=4, dim=1):\n    x = np.random.randint(0, 10, size=(count, max_length, dim))\n    length = np.random.randint(1, max_length+1, count)\n    for i in range(count):\n        x[i, length[i]:, :] = 0\n    y = np.sum(x, axis=1)\n    return x, y, length\n\n\ndef next_batch(x, y, seq_len, batch_size):\n    N = x.shape[0]\n    batch_indeces = np.random.permutation(N)[:batch_size]\n    x_batch = x[batch_indeces]\n    y_batch = y[batch_indeces]\n    seq_len_batch = seq_len[batch_indeces]\n    return x_batch, y_batch, seq_len_batch\n'"
7_Recurrent_Neural_Network/code/05_Many_to_Many/main.py,6,"b'import tensorflow as tf\nimport numpy as np\nfrom ops import *\n\ninput_dim = 1\nmax_time = 4\nnum_hidden_units = 10\nout_dim = 1\n\nx = tf.placeholder(tf.float32, [None, max_time, input_dim])\ny = tf.placeholder(tf.float32, [None, max_time])\n\n# create weight matrix initialized randomely from N~(0, 0.01)\nW = weight_variable(shape=[num_hidden_units, out_dim])\n\n# create bias vector initialized as zero\nb = bias_variable(shape=[out_dim])\n\npred_out = LSTM(x, W, b, num_hidden_units)\n\ncost = tf.reduce_mean(tf.square(pred_out - y))\ntrain_op = tf.train.AdamOptimizer().minimize(cost)\n\n# Creating the op for initializing all variables\ninit = tf.global_variables_initializer()\n\nx_train = np.array([[[1], [2], [5], [6]],\n                    [[5], [7], [7], [8]],\n                    [[3], [4], [5], [9]]])\ny_train = np.array([[1, 3, 8, 14],\n                    [5, 12, 19, 27],\n                    [3, 7, 12, 21]])\n\nx_test = np.array([[[1], [2], [3], [4]],\n                   [[4], [5], [6], [7]]])\n\ny_test = np.array([[1, 3, 6, 10],\n                   [4, 9, 15, 22]])\n\nwith tf.Session() as sess:\n    sess.run(init)\n    for i in range(10000):\n        _, mse = sess.run([train_op, cost], feed_dict={x: x_train, y: y_train})\n        if i % 1000 == 0:\n            print(\'Step {}, MSE={}\'.format(i, mse))\n    # Test\n    y_pred = sess.run(pred_out, feed_dict={x: x_test})\n\n    for i, x in enumerate(y_test):\n        print(""When the ground truth output is {}, the model thinks it is {}""\n              .format(y_test[i], y_pred[i]))\n'"
7_Recurrent_Neural_Network/code/05_Many_to_Many/ops.py,12,"b'import tensorflow as tf\n\n\n# weight and bais wrappers\ndef weight_variable(shape):\n    """"""\n    Create a weight variable with appropriate initialization\n    :param shape: weight shape\n    :return: initialized weight variable\n    """"""\n    initer = tf.truncated_normal_initializer(stddev=0.01)\n    return tf.get_variable(\'W\',\n                           dtype=tf.float32,\n                           shape=shape,\n                           initializer=initer)\n\n\ndef bias_variable(shape):\n    """"""\n    Create a bias variable with appropriate initialization\n    :param shape: bias variable shape\n    :return: initialized bias variable\n    """"""\n    initer = tf.constant(0., shape=shape, dtype=tf.float32)\n    return tf.get_variable(\'b\',\n                           dtype=tf.float32,\n                           initializer=initer)\n\n\ndef LSTM(x, weights, biases, num_hidden):\n    """"""\n    :param x: inputs of size [T, batch_size, input_size]\n    :param weights: matrix of fully-connected output layer weights\n    :param biases: vector of fully-connected output layer biases\n    :param num_hidden: number of hidden units\n    """"""\n    cell = tf.nn.rnn_cell.LSTMCell(num_hidden)\n    outputs, states = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)\n    num_examples = tf.shape(x)[0]\n    w_repeated = tf.tile(tf.expand_dims(weights, 0), [num_examples, 1, 1])\n    out = tf.matmul(outputs, w_repeated) + biases\n    out = tf.squeeze(out)\n    return out\n'"
7_Recurrent_Neural_Network/code/05_Many_to_Many/sandbox.py,2,"b""    import tensorflow as tf\nimport numpy as np\n\n# Create input data\n# Lets say you have a batch of two examples, one is of length 10, and the other of length 6.\n# Each one is a vector of 8 numbers (time-steps=8)\n\nX = np.random.randn(2, 10, 8)   # [batch_size, max_time (or length), input_dim]\n\n# The second example is of length 6\nX[1, 6:] = 0\nX_lengths = [10, 6]\n\ncell = tf.nn.rnn_cell.LSTMCell(num_units=100)\noutputs, last_states = tf.nn.dynamic_rnn(cell, X, sequence_length=X_lengths, dtype=tf.float64)\n# X: input of shape [batch_size, max_time, input_dim]\n# sequence_length: (optional) An int32/int64 vector sized `[batch_size]`. Used to copy-through\n# state and zero-out outputs when past a batch element's sequence length.  So it's more for\n# correctness than performance.\n# dtype: (optional) The data type for the initial state and expected output.\n#  (If there is no initial_state, you must give a dtype.)\n\n# outputs: [batch_size, max_time, num_hidden_units] (=ht)\n# last_states: the last state for each example (cT, hT), each of shape [batch_size, num_hidden_units]\n# If cells are `LSTMCells`, `state` will be a tuple containing a `LSTMStateTuple` for each cell.\n\nprint()\n\n"""
7_Recurrent_Neural_Network/code/06_Vanilla_RNN_for_Classification/main.py,10,"b'import tensorflow as tf\n\n# Load MNIST data\nfrom ops import weight_variable, bias_variable, RNN\nfrom utils import load_data, randomize, get_next_batch\n\nx_train, y_train, x_valid, y_valid = load_data(mode=\'train\')\nprint(""Size of:"")\nprint(""- Training-set:\\t\\t{}"".format(len(y_train)))\nprint(""- Validation-set:\\t{}"".format(len(y_valid)))\n\n# Hyper-parameters\nlearning_rate = 0.001   # The optimization initial learning rate\nepochs = 10             # Total number of training epochs\nbatch_size = 100        # Training batch size\ndisplay_freq = 100      # Frequency of displaying the training results\n\n# Network Parameters\ninput_dim = 28          # MNIST data input (img shape: 28*28)\nmax_time = 28           # Timesteps\nnum_hidden_units = 128  # Number of hidden units of the RNN\nn_classes = 10          # Number of classes, one class per digit\n\n\n# Create the graph for the linear model\n# Placeholders for inputs (x) and outputs(y)\nx = tf.placeholder(tf.float32, shape=[None, max_time, input_dim], name=\'X\')\ny = tf.placeholder(tf.float32, shape=[None, n_classes], name=\'Y\')\n\n\n# create weight matrix initialized randomely from N~(0, 0.01)\nW = weight_variable(shape=[num_hidden_units, n_classes])\n\n# create bias vector initialized as zero\nb = bias_variable(shape=[n_classes])\n\noutput_logits = RNN(x, W, b, max_time, num_hidden_units)\ny_pred = tf.nn.softmax(output_logits)\n\n# Model predictions\ncls_prediction = tf.argmax(output_logits, axis=1, name=\'predictions\')\n\n# Define the loss function, optimizer, and accuracy\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output_logits), name=\'loss\')\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name=\'Adam-op\').minimize(loss)\ncorrect_prediction = tf.equal(tf.argmax(output_logits, 1), tf.argmax(y, 1), name=\'correct_pred\')\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\'accuracy\')\n\n# Creating the op for initializing all variables\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    global_step = 0\n    # Number of training iterations in each epoch\n    num_tr_iter = int(len(y_train) / batch_size)\n    for epoch in range(epochs):\n        print(\'Training epoch: {}\'.format(epoch + 1))\n        x_train, y_train = randomize(x_train, y_train)\n        for iteration in range(num_tr_iter):\n            global_step += 1\n            start = iteration * batch_size\n            end = (iteration + 1) * batch_size\n            x_batch, y_batch = get_next_batch(x_train, y_train, start, end)\n            x_batch = x_batch.reshape((batch_size, max_time, input_dim))\n            # Run optimization op (backprop)\n            feed_dict_batch = {x: x_batch, y: y_batch}\n            sess.run(optimizer, feed_dict=feed_dict_batch)\n\n            if iteration % display_freq == 0:\n                # Calculate and display the batch loss and accuracy\n                loss_batch, acc_batch = sess.run([loss, accuracy],\n                                                 feed_dict=feed_dict_batch)\n\n                print(""iter {0:3d}:\\t Loss={1:.2f},\\tTraining Accuracy={2:.01%}"".\n                      format(iteration, loss_batch, acc_batch))\n\n        # Run validation after every epoch\n\n        feed_dict_valid = {x: x_valid[:1000].reshape((-1, max_time, input_dim)), y: y_valid[:1000]}\n        loss_valid, acc_valid = sess.run([loss, accuracy], feed_dict=feed_dict_valid)\n        print(\'---------------------------------------------------------\')\n        print(""Epoch: {0}, validation loss: {1:.2f}, validation accuracy: {2:.01%}"".\n              format(epoch + 1, loss_valid, acc_valid))\n        print(\'---------------------------------------------------------\')\n\n    # Test the network after training\n    # Accuracy\n    x_test, y_test = load_data(mode=\'test\')\n    feed_dict_test = {x: x_test[:1000].reshape((-1, max_time, input_dim)), y: y_test[:1000]}\n    loss_test, acc_test = sess.run([loss, accuracy], feed_dict=feed_dict_test)\n    print(\'---------------------------------------------------------\')\n    print(""Test loss: {0:.2f}, test accuracy: {1:.01%}"".format(loss_test, acc_test))\n    print(\'---------------------------------------------------------\')\n'"
7_Recurrent_Neural_Network/code/06_Vanilla_RNN_for_Classification/ops.py,11,"b'import tensorflow as tf\nfrom tensorflow.contrib import rnn\n\n\ndef weight_variable(shape):\n    """"""\n    Create a weight variable with appropriate initialization\n    :param name: weight name\n    :param shape: weight shape\n    :return: initialized weight variable\n    """"""\n    initer = tf.truncated_normal_initializer(stddev=0.01)\n    return tf.get_variable(\'W\',\n                           dtype=tf.float32,\n                           shape=shape,\n                           initializer=initer)\n\n\ndef bias_variable(shape):\n    """"""\n    Create a bias variable with appropriate initialization\n    :param name: bias variable name\n    :param shape: bias variable shape\n    :return: initialized bias variable\n    """"""\n    initial = tf.constant(0., shape=shape, dtype=tf.float32)\n    return tf.get_variable(\'b\',\n                           dtype=tf.float32,\n                           initializer=initial)\n\n\ndef RNN(x, weights, biases, max_time, num_hidden):\n\n    # Prepare data shape to match `rnn` function requirements\n    # Current data input shape: (batch_size, timesteps, n_input)\n    # Required shape: \'timesteps\' tensors list of shape (batch_size, n_input)\n\n    # Unstack to get a list of \'timesteps\' tensors of shape (batch_size, n_input)\n    x = tf.unstack(x, max_time, 1)\n\n    # Define a rnn cell with tensorflow\n    # rnn_cell = rnn.BasicRNNCell(num_hidden)\n    # Define a lstm cell with tensorflow\n    lstm_cell = rnn.BasicLSTMCell(num_hidden)\n\n    # Get lstm cell output\n    # If no initial_state is provided, dtype must be specified\n    # If no initial cell satte is provided, they will be initialized to zero\n    # states_series, current_state = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n\n    # Linear activation, using rnn inner loop last output\n    # return tf.matmul(current_state, weights) + biases\n    return tf.matmul(outputs[-1], weights) + biases\n\n'"
7_Recurrent_Neural_Network/code/06_Vanilla_RNN_for_Classification/utils.py,0,"b'import numpy as np\n\n\ndef load_data(mode=\'train\'):\n    """"""\n    Function to (download and) load the MNIST data\n    :param mode: train or test\n    :return: images and the corresponding labels\n    """"""\n    from tensorflow.examples.tutorials.mnist import input_data\n    mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n    if mode == \'train\':\n        x_train, y_train, x_valid, y_valid = mnist.train.images, mnist.train.labels, \\\n                                             mnist.validation.images, mnist.validation.labels\n        return x_train, y_train, x_valid, y_valid\n    elif mode == \'test\':\n        x_test, y_test = mnist.test.images, mnist.test.labels\n    return x_test, y_test\n\n\ndef randomize(x, y):\n    """""" Randomizes the order of data samples and their corresponding labels""""""\n    permutation = np.random.permutation(y.shape[0])\n    shuffled_x = x[permutation, :]\n    shuffled_y = y[permutation]\n    return shuffled_x, shuffled_y\n\n\ndef reformat(x, y):\n    """"""\n    Reformats the data to the format acceptable for convolutional layers\n    :param x: input array\n    :param y: corresponding labels\n    :return: reshaped input and labels\n    """"""\n    img_size, num_ch, num_class = int(np.sqrt(x.shape[-1])), 1, len(np.unique(np.argmax(y, 1)))\n    dataset = x.reshape((-1, img_size, img_size, num_ch)).astype(np.float32)\n    labels = (np.arange(num_class) == y[:, None]).astype(np.float32)\n    return dataset, labels\n\n\ndef get_next_batch(x, y, start, end):\n    x_batch = x[start:end]\n    y_batch = y[start:end]\n    return x_batch, y_batch'"
