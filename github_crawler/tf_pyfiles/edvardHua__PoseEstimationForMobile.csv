file_path,api_count,code
training/src/__init__.py,0,b'# -*- coding: utf-8 -*-\n# @Time    : 18-3-9 \xe4\xb8\x8a\xe5\x8d\x8810:45\n# @Author  : zengzihua@huya.com\n# @FileName: __init__.py.py\n# @Software: PyCharm\n'
training/src/benchmark.py,5,"b'# -*- coding: utf-8 -*-\n# @Time    : 18-7-10 \xe4\xb8\x8a\xe5\x8d\x889:41\n# @Author  : zengzihua@huya.com\n# @FileName: benchmark.py\n# @Software: PyCharm\n\nimport tensorflow as tf\nimport numpy as np\nimport json\nimport argparse\nimport cv2\nimport os\nimport math\nimport time\n\nfrom scipy.ndimage.filters import gaussian_filter\n\n\ndef cal_coord(pred_heatmaps, images_anno):\n    coords = {}\n    for img_id in pred_heatmaps.keys():\n        heat_h, heat_w, n_kpoints = pred_heatmaps[img_id].shape\n        scale_h, scale_w = heat_h / images_anno[img_id][\'height\'], heat_w / images_anno[img_id][\'width\']\n        coord = []\n        for p_ind in range(n_kpoints):\n            heat = pred_heatmaps[img_id][:, :, p_ind]\n            heat = gaussian_filter(heat, sigma=5)\n            ind = np.unravel_index(np.argmax(heat), heat.shape)\n            coord_x = int((ind[1] + 1) / scale_w)\n            coord_y = int((ind[0] + 1) / scale_h)\n            coord.append((coord_x, coord_y))\n        coords[img_id] = coord\n    return coords\n\n\ndef infer(frozen_pb_path, output_node_name, img_path, images_anno):\n    with tf.gfile.GFile(frozen_pb_path, ""rb"") as f:\n        restored_graph_def = tf.GraphDef()\n        restored_graph_def.ParseFromString(f.read())\n\n    tf.import_graph_def(\n        restored_graph_def,\n        input_map=None,\n        return_elements=None,\n        name=""""\n    )\n\n    graph = tf.get_default_graph()\n    input_image = graph.get_tensor_by_name(""image:0"")\n    output_heat = graph.get_tensor_by_name(""%s:0"" % output_node_name)\n\n    res = {}\n    use_times = []\n    with tf.Session() as sess:\n        for img_id in images_anno.keys():\n            ori_img = cv2.imread(os.path.join(img_path, images_anno[img_id][\'file_name\']))\n            shape = input_image.get_shape().as_list()\n            inp_img = cv2.resize(ori_img, (shape[1], shape[2]))\n            st = time.time()\n            heat = sess.run(output_heat, feed_dict={input_image: [inp_img]})\n            infer_time = 1000 * (time.time() - st)\n            print(""img_id = %d, cost_time = %.2f ms"" % (img_id, infer_time))\n            use_times.append(infer_time)\n            res[img_id] = np.squeeze(heat)\n    print(""Average inference time = %.2f ms"" % np.mean(use_times))\n    return res\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=""PCKh benchmark"")\n    parser.add_argument(""--frozen_pb_path"", type=str, default="""")\n    parser.add_argument(""--anno_json_path"", type=str, default="""")\n    parser.add_argument(""--img_path"", type=str, default="""")\n    parser.add_argument(""--output_node_name"", type=str, default="""")\n    parser.add_argument(""--gpus"", type=str, default=""1"")\n    args = parser.parse_args()\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = args.gpus\n    anno = json.load(open(args.anno_json_path))\n    print(""Total test example=%d"" % len(anno[\'images\']))\n\n    images_anno = {}\n    keypoint_annos = {}\n    transform = list(zip(\n        [1, 2, 4, 6, 8, 3, 5, 7, 10, 12, 14, 9, 11, 13],\n        [1, 2, 4, 6, 8, 3, 5, 7, 10, 12, 14, 9, 11, 13]\n    ))\n    for img_info, anno_info in zip(anno[\'images\'], anno[\'annotations\']):\n        images_anno[img_info[\'id\']] = img_info\n\n        prev_xs = anno_info[\'keypoints\'][0::3]\n        prev_ys = anno_info[\'keypoints\'][1::3]\n\n        new_kp = []\n        for idx, idy in transform:\n            new_kp.append(\n                (prev_xs[idx-1], prev_ys[idy-1])\n            )\n\n        keypoint_annos[anno_info[\'image_id\']] = new_kp\n\n    pred_heatmap = infer(args.frozen_pb_path, args.output_node_name, args.img_path, images_anno)\n    pred_coords = cal_coord(pred_heatmap, images_anno)\n\n    scores = []\n    for img_id in keypoint_annos.keys():\n        groundtruth_anno = keypoint_annos[img_id]\n\n        head_gt = groundtruth_anno[0]\n        neck_gt = groundtruth_anno[1]\n\n        threshold = math.sqrt((head_gt[0] - neck_gt[0]) ** 2 + (head_gt[1] - neck_gt[1]) ** 2)\n\n        curr_score = []\n        for index, coord in enumerate(pred_coords[img_id]):\n            pred_x, pred_y = coord\n            gt_x, gt_y = groundtruth_anno[index]\n\n            d = math.sqrt((pred_x-gt_x)**2 + (pred_y-gt_y)**2)\n            if d > threshold:\n                curr_score.append(0)\n            else:\n                curr_score.append(1)\n        scores.append(np.mean(curr_score))\n\n    print(""PCKh=%.2f"" % (np.mean(scores) * 100))\n\n'"
training/src/cpm_body.py,127,"b""import tensorflow as tf\nimport pickle\n\n\nclass CPM_Model(object):\n    def __init__(self, stages, joints):\n        self.stages = stages\n        self.stage_heatmap = []\n        self.stage_loss = [0] * stages\n        self.total_loss = 0\n        self.input_image = None\n        self.center_map = None\n        self.gt_heatmap = None\n        self.learning_rate = 0\n        self.merged_summary = None\n        self.joints = joints\n        self.batch_size = 0\n\n    def build_model(self, input_image, center_map, batch_size):\n        self.batch_size = batch_size\n        self.input_image = input_image\n        self.center_map = center_map\n        with tf.variable_scope('pooled_center_map'):\n            self.center_map = tf.layers.average_pooling2d(inputs=self.center_map,\n                                                          pool_size=[9, 9],\n                                                          strides=[8, 8],\n                                                          padding='same',\n                                                          name='center_map')\n        with tf.variable_scope('sub_stages'):\n            sub_conv1 = tf.layers.conv2d(inputs=input_image,\n                                         filters=64,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv1')\n            sub_conv2 = tf.layers.conv2d(inputs=sub_conv1,\n                                         filters=64,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv2')\n            sub_pool1 = tf.layers.max_pooling2d(inputs=sub_conv2,\n                                                pool_size=[2, 2],\n                                                strides=2,\n                                                padding='same',\n                                                name='sub_pool1')\n            sub_conv3 = tf.layers.conv2d(inputs=sub_pool1,\n                                         filters=128,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv3')\n            sub_conv4 = tf.layers.conv2d(inputs=sub_conv3,\n                                         filters=128,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv4')\n            sub_pool2 = tf.layers.max_pooling2d(inputs=sub_conv4,\n                                                pool_size=[2, 2],\n                                                strides=2,\n                                                padding='same',\n                                                name='sub_pool2')\n            sub_conv5 = tf.layers.conv2d(inputs=sub_pool2,\n                                         filters=256,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv5')\n            sub_conv6 = tf.layers.conv2d(inputs=sub_conv5,\n                                         filters=256,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv6')\n            sub_conv7 = tf.layers.conv2d(inputs=sub_conv6,\n                                         filters=256,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv7')\n            sub_conv8 = tf.layers.conv2d(inputs=sub_conv7,\n                                         filters=256,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv8')\n            sub_pool3 = tf.layers.max_pooling2d(inputs=sub_conv8,\n                                                pool_size=[2, 2],\n                                                strides=2,\n                                                padding='same',\n                                                name='sub_pool3')\n            sub_conv9 = tf.layers.conv2d(inputs=sub_pool3,\n                                         filters=512,\n                                         kernel_size=[3, 3],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='sub_conv9')\n            sub_conv10 = tf.layers.conv2d(inputs=sub_conv9,\n                                          filters=512,\n                                          kernel_size=[3, 3],\n                                          strides=[1, 1],\n                                          padding='same',\n                                          activation=tf.nn.relu,\n                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                          name='sub_conv10')\n            sub_conv11 = tf.layers.conv2d(inputs=sub_conv10,\n                                          filters=256,\n                                          kernel_size=[3, 3],\n                                          strides=[1, 1],\n                                          padding='same',\n                                          activation=tf.nn.relu,\n                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                          name='sub_conv11')\n            sub_conv12 = tf.layers.conv2d(inputs=sub_conv11,\n                                          filters=256,\n                                          kernel_size=[3, 3],\n                                          strides=[1, 1],\n                                          padding='same',\n                                          activation=tf.nn.relu,\n                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                          name='sub_conv12')\n            sub_conv13 = tf.layers.conv2d(inputs=sub_conv12,\n                                          filters=256,\n                                          kernel_size=[3, 3],\n                                          strides=[1, 1],\n                                          padding='same',\n                                          activation=tf.nn.relu,\n                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                          name='sub_conv13')\n            sub_conv14 = tf.layers.conv2d(inputs=sub_conv13,\n                                          filters=256,\n                                          kernel_size=[3, 3],\n                                          strides=[1, 1],\n                                          padding='same',\n                                          activation=tf.nn.relu,\n                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                          name='sub_conv14')\n\n            self.sub_stage_img_feature = tf.layers.conv2d(inputs=sub_conv14,\n                                                          filters=128,\n                                                          kernel_size=[3, 3],\n                                                          strides=[1, 1],\n                                                          padding='same',\n                                                          activation=tf.nn.relu,\n                                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                                          name='sub_stage_img_feature')\n\n        with tf.variable_scope('stage_1'):\n            conv1 = tf.layers.conv2d(inputs=self.sub_stage_img_feature,\n                                     filters=512,\n                                     kernel_size=[1, 1],\n                                     strides=[1, 1],\n                                     padding='same',\n                                     activation=tf.nn.relu,\n                                     kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                     name='conv1')\n            self.stage_heatmap.append(tf.layers.conv2d(inputs=conv1,\n                                                       filters=self.joints,\n                                                       kernel_size=[1, 1],\n                                                       strides=[1, 1],\n                                                       padding='same',\n                                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                                       name='stage_heatmap'))\n        for stage in range(2, self.stages + 1):\n            self._middle_conv(stage)\n\n    def _middle_conv(self, stage):\n        with tf.variable_scope('stage_' + str(stage)):\n            self.current_featuremap = tf.concat([self.stage_heatmap[stage - 2],\n                                                 self.sub_stage_img_feature,\n                                                 self.center_map,\n                                                 ],\n                                                axis=3)\n            mid_conv1 = tf.layers.conv2d(inputs=self.current_featuremap,\n                                         filters=128,\n                                         kernel_size=[7, 7],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='mid_conv1')\n            mid_conv2 = tf.layers.conv2d(inputs=mid_conv1,\n                                         filters=128,\n                                         kernel_size=[7, 7],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='mid_conv2')\n            mid_conv3 = tf.layers.conv2d(inputs=mid_conv2,\n                                         filters=128,\n                                         kernel_size=[7, 7],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='mid_conv3')\n            mid_conv4 = tf.layers.conv2d(inputs=mid_conv3,\n                                         filters=128,\n                                         kernel_size=[7, 7],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='mid_conv4')\n            mid_conv5 = tf.layers.conv2d(inputs=mid_conv4,\n                                         filters=128,\n                                         kernel_size=[7, 7],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='mid_conv5')\n            mid_conv6 = tf.layers.conv2d(inputs=mid_conv5,\n                                         filters=128,\n                                         kernel_size=[1, 1],\n                                         strides=[1, 1],\n                                         padding='same',\n                                         activation=tf.nn.relu,\n                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                         name='mid_conv6')\n            self.current_heatmap = tf.layers.conv2d(inputs=mid_conv6,\n                                                    filters=self.joints,\n                                                    kernel_size=[1, 1],\n                                                    strides=[1, 1],\n                                                    padding='same',\n                                                    kernel_initializer=tf.contrib.layers.xavier_initializer(),\n                                                    name='mid_conv7')\n            self.stage_heatmap.append(self.current_heatmap)\n\n    def build_loss(self, gt_heatmap, lr, lr_decay_rate, lr_decay_step):\n        self.gt_heatmap = gt_heatmap\n        self.total_loss = 0\n        self.learning_rate = lr\n        self.lr_decay_rate = lr_decay_rate\n        self.lr_decay_step = lr_decay_step\n\n        for stage in range(self.stages):\n            with tf.variable_scope('stage' + str(stage + 1) + '_loss'):\n                self.stage_loss[stage] = tf.nn.l2_loss(self.stage_heatmap[stage] - self.gt_heatmap,\n                                                       name='l2_loss') / self.batch_size\n            tf.summary.scalar('stage' + str(stage + 1) + '_loss', self.stage_loss[stage])\n\n        with tf.variable_scope('total_loss'):\n            for stage in range(self.stages):\n                self.total_loss += self.stage_loss[stage]\n            tf.summary.scalar('total loss', self.total_loss)\n\n        with tf.variable_scope('train'):\n            self.global_step = tf.contrib.framework.get_or_create_global_step()\n\n            self.lr = tf.train.exponential_decay(self.learning_rate,\n                                                 global_step=self.global_step,\n                                                 decay_rate=self.lr_decay_rate,\n                                                 decay_steps=self.lr_decay_step)\n            tf.summary.scalar('learning rate', self.lr)\n\n            self.train_op = tf.contrib.layers.optimize_loss(loss=self.total_loss,\n                                                            global_step=self.global_step,\n                                                            learning_rate=self.lr,\n                                                            optimizer='Adam')\n        self.merged_summary = tf.summary.merge_all()\n\n    def load_weights_from_file(self, weight_file_path, sess, finetune=True):\n        weights = pickle.load(open(weight_file_path, 'rb'), encoding='latin1')\n\n        with tf.variable_scope('', reuse=True):\n            ## Pre stage conv\n            # conv1\n            for layer in range(1, 3):\n                conv_kernel = tf.get_variable('sub_stages/sub_conv' + str(layer) + '/kernel')\n                conv_bias = tf.get_variable('sub_stages/sub_conv' + str(layer) + '/bias')\n\n                loaded_kernel = weights['conv1_' + str(layer)]\n                loaded_bias = weights['conv1_' + str(layer) + '_b']\n\n                sess.run(tf.assign(conv_kernel, loaded_kernel))\n                sess.run(tf.assign(conv_bias, loaded_bias))\n\n            # conv2\n            for layer in range(1, 3):\n                conv_kernel = tf.get_variable('sub_stages/sub_conv' + str(layer + 2) + '/kernel')\n                conv_bias = tf.get_variable('sub_stages/sub_conv' + str(layer + 2) + '/bias')\n\n                loaded_kernel = weights['conv2_' + str(layer)]\n                loaded_bias = weights['conv2_' + str(layer) + '_b']\n\n                sess.run(tf.assign(conv_kernel, loaded_kernel))\n                sess.run(tf.assign(conv_bias, loaded_bias))\n\n            # conv3\n            for layer in range(1, 5):\n                conv_kernel = tf.get_variable('sub_stages/sub_conv' + str(layer + 4) + '/kernel')\n                conv_bias = tf.get_variable('sub_stages/sub_conv' + str(layer + 4) + '/bias')\n\n                loaded_kernel = weights['conv3_' + str(layer)]\n                loaded_bias = weights['conv3_' + str(layer) + '_b']\n\n                sess.run(tf.assign(conv_kernel, loaded_kernel))\n                sess.run(tf.assign(conv_bias, loaded_bias))\n\n            # conv4\n            for layer in range(1, 3):\n                conv_kernel = tf.get_variable('sub_stages/sub_conv' + str(layer + 8) + '/kernel')\n                conv_bias = tf.get_variable('sub_stages/sub_conv' + str(layer + 8) + '/bias')\n\n                loaded_kernel = weights['conv4_' + str(layer)]\n                loaded_bias = weights['conv4_' + str(layer) + '_b']\n\n                sess.run(tf.assign(conv_kernel, loaded_kernel))\n                sess.run(tf.assign(conv_bias, loaded_bias))\n\n            # conv4_CPM\n            for layer in range(1, 5):\n                conv_kernel = tf.get_variable('sub_stages/sub_conv' + str(layer + 10) + '/kernel')\n                conv_bias = tf.get_variable('sub_stages/sub_conv' + str(layer + 10) + '/bias')\n\n                loaded_kernel = weights['conv4_' + str(2 + layer) + '_CPM']\n                loaded_bias = weights['conv4_' + str(2 + layer) + '_CPM_b']\n\n                sess.run(tf.assign(conv_kernel, loaded_kernel))\n                sess.run(tf.assign(conv_bias, loaded_bias))\n\n            # conv5_3_CPM\n            conv_kernel = tf.get_variable('sub_stages/sub_stage_img_feature/kernel')\n            conv_bias = tf.get_variable('sub_stages/sub_stage_img_feature/bias')\n\n            loaded_kernel = weights['conv4_7_CPM']\n            loaded_bias = weights['conv4_7_CPM_b']\n\n            sess.run(tf.assign(conv_kernel, loaded_kernel))\n            sess.run(tf.assign(conv_bias, loaded_bias))\n\n            ## stage 1\n            conv_kernel = tf.get_variable('stage_1/conv1/kernel')\n            conv_bias = tf.get_variable('stage_1/conv1/bias')\n\n            loaded_kernel = weights['conv5_1_CPM']\n            loaded_bias = weights['conv5_1_CPM_b']\n\n            sess.run(tf.assign(conv_kernel, loaded_kernel))\n            sess.run(tf.assign(conv_bias, loaded_bias))\n\n            if finetune != True:\n                conv_kernel = tf.get_variable('stage_1/stage_heatmap/kernel')\n                conv_bias = tf.get_variable('stage_1/stage_heatmap/bias')\n\n                loaded_kernel = weights['conv5_2_CPM']\n                loaded_bias = weights['conv5_2_CPM_b']\n\n                sess.run(tf.assign(conv_kernel, loaded_kernel))\n                sess.run(tf.assign(conv_bias, loaded_bias))\n\n                ## stage 2 and behind\n                for stage in range(2, self.stages + 1):\n                    for layer in range(1, 8):\n                        conv_kernel = tf.get_variable('stage_' + str(stage) + '/mid_conv' + str(layer) + '/kernel')\n                        conv_bias = tf.get_variable('stage_' + str(stage) + '/mid_conv' + str(layer) + '/bias')\n\n                        loaded_kernel = weights['Mconv' + str(layer) + '_stage' + str(stage)]\n                        loaded_bias = weights['Mconv' + str(layer) + '_stage' + str(stage) + '_b']\n\n                        sess.run(tf.assign(conv_kernel, loaded_kernel))\n                        sess.run(tf.assign(conv_bias, loaded_bias))\n"""
training/src/dataset.py,3,"b'# Copyright 2018 Zihua Zeng (edvard_hua@live.com)\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===================================================================================\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nfrom dataset_augment import pose_random_scale, pose_rotation, pose_flip, pose_resize_shortestedge_random, \\\n    pose_crop_random, pose_to_img\nfrom dataset_prepare import CocoMetadata\nfrom os.path import join\nfrom pycocotools.coco import COCO\nimport multiprocessing\n\nBASE = ""/root/hdd""\nBASE_PATH = """"\nTRAIN_JSON = ""ai_challenger_train.json""\nVALID_JSON = ""ai_challenger_valid.json""\n\nTRAIN_ANNO = None\nVALID_ANNO = None\nCONFIG = None\n\n\ndef set_config(config):\n    global CONFIG, BASE, BASE_PATH\n    CONFIG = config\n    BASE = CONFIG[\'imgpath\']\n    BASE_PATH = CONFIG[\'datapath\']\n\n\ndef _parse_function(imgId, is_train, ann=None):\n    """"""\n    :param imgId:\n    :return:\n    """"""\n\n    global TRAIN_ANNO\n    global VALID_ANNO\n\n    if ann is not None:\n        if is_train == True:\n            TRAIN_ANNO = ann\n        else:\n            VALID_ANNO = ann\n    else:\n        if is_train == True:\n            anno = TRAIN_ANNO\n        else:\n            anno = VALID_ANNO\n\n    img_meta = anno.loadImgs([imgId])[0]\n    anno_ids = anno.getAnnIds(imgIds=imgId)\n    img_anno = anno.loadAnns(anno_ids)\n    idx = img_meta[\'id\']\n    img_path = join(BASE, img_meta[\'file_name\'])\n\n    img_meta_data = CocoMetadata(idx, img_path, img_meta, img_anno, sigma=6.0)\n    img_meta_data = pose_random_scale(img_meta_data)\n    img_meta_data = pose_rotation(img_meta_data)\n    img_meta_data = pose_flip(img_meta_data)\n    img_meta_data = pose_resize_shortestedge_random(img_meta_data)\n    img_meta_data = pose_crop_random(img_meta_data)\n    return pose_to_img(img_meta_data)\n\n\ndef _set_shapes(img, heatmap):\n    img.set_shape([CONFIG[\'input_height\'], CONFIG[\'input_width\'], 3])\n    heatmap.set_shape(\n        [CONFIG[\'input_height\'] / CONFIG[\'scale\'], CONFIG[\'input_width\'] / CONFIG[\'scale\'], CONFIG[\'n_kpoints\']])\n    return img, heatmap\n\n\ndef _get_dataset_pipeline(anno, batch_size, epoch, buffer_size, is_train=True):\n\n    imgIds = anno.getImgIds()\n\n    dataset = tf.data.Dataset.from_tensor_slices(imgIds)\n\n    dataset.shuffle(buffer_size)\n    dataset = dataset.map(\n        lambda imgId: tuple(\n            tf.py_func(\n                func=_parse_function,\n                inp=[imgId, is_train],\n                Tout=[tf.float32, tf.float32]\n            )\n        ), num_parallel_calls=CONFIG[\'multiprocessing_num\'])\n\n    dataset = dataset.map(_set_shapes, num_parallel_calls=CONFIG[\'multiprocessing_num\'])\n    dataset = dataset.batch(batch_size).repeat(epoch)\n    dataset = dataset.prefetch(100)\n\n    return dataset\n\n\ndef get_train_dataset_pipeline(batch_size=32, epoch=10, buffer_size=1):\n    global TRAIN_ANNO\n\n    anno_path = join(BASE_PATH, TRAIN_JSON)\n    print(""preparing annotation from:"", anno_path)\n    TRAIN_ANNO = COCO(\n        anno_path\n    )\n    return _get_dataset_pipeline(TRAIN_ANNO, batch_size, epoch, buffer_size, True)\n\ndef get_valid_dataset_pipeline(batch_size=32, epoch=10, buffer_size=1):\n    global VALID_ANNO\n\n    anno_path = join(BASE_PATH, VALID_JSON)\n    print(""preparing annotation from:"", anno_path)\n    VALID_ANNO = COCO(\n        anno_path\n    )\n    return _get_dataset_pipeline(VALID_ANNO, batch_size, epoch, buffer_size, False)\n'"
training/src/dataset_augment.py,0,"b'# -*- coding: utf-8 -*-\n# @Time    : 18-3-7 \xe4\xb8\x8b\xe5\x8d\x882:36\n# @Author  : edvard_hua@live.com\n# @FileName: dataset_augument.py\n# @Software: PyCharm\n\nimport math\nimport random\n\nimport cv2\nimport numpy as np\nfrom tensorpack.dataflow.imgaug.geometry import RotationAndCropValid\nfrom enum import Enum\n\n_network_w = 256\n_network_h = 256\n_scale = 2\n\n\nclass CocoPart(Enum):\n    Top = 0\n    Neck = 1\n    RShoulder = 2\n    RElbow = 3\n    RWrist = 4\n    LShoulder = 5\n    LElbow = 6\n    LWrist = 7\n    RHip = 8\n    RKnee = 9\n    RAnkle = 10\n    LHip = 11\n    LKnee = 12\n    LAnkle = 13\n    Background = 14\n\n\ndef set_network_input_wh(w, h):\n    global _network_w, _network_h\n    _network_w, _network_h = w, h\n\n\ndef set_network_scale(scale):\n    global _scale\n    _scale = scale\n\n\ndef get_network_output_wh():\n    return _network_w // _scale, _network_h // _scale\n\n\ndef pose_random_scale(meta):\n    scalew = random.uniform(0.8, 1.2)\n    scaleh = random.uniform(0.8, 1.2)\n    neww = int(meta.width * scalew)\n    newh = int(meta.height * scaleh)\n\n    dst = cv2.resize(meta.img, (neww, newh), interpolation=cv2.INTER_AREA)\n\n    # adjust meta data\n    adjust_joint_list = []\n    for joint in meta.joint_list:\n        adjust_joint = []\n        for point in joint:\n            if point[0] < -100 or point[1] < -100:\n                adjust_joint.append((-1000, -1000))\n                continue\n            adjust_joint.append((int(point[0] * scalew + 0.5), int(point[1] * scaleh + 0.5)))\n        adjust_joint_list.append(adjust_joint)\n\n    meta.joint_list = adjust_joint_list\n    meta.width, meta.height = neww, newh\n    meta.img = dst\n    return meta\n\n\ndef pose_rotation(meta):\n    deg = random.uniform(-15.0, 15.0)\n    img = meta.img\n\n    center = (img.shape[1] * 0.5, img.shape[0] * 0.5)  # x, y\n    rot_m = cv2.getRotationMatrix2D((int(center[0]), int(center[1])), deg, 1)\n    ret = cv2.warpAffine(img, rot_m, img.shape[1::-1], flags=cv2.INTER_AREA, borderMode=cv2.BORDER_CONSTANT)\n    if img.ndim == 3 and ret.ndim == 2:\n        ret = ret[:, :, np.newaxis]\n    neww, newh = RotationAndCropValid.largest_rotated_rect(ret.shape[1], ret.shape[0], deg)\n    neww = min(neww, ret.shape[1])\n    newh = min(newh, ret.shape[0])\n    newx = int(center[0] - neww * 0.5)\n    newy = int(center[1] - newh * 0.5)\n    # print(ret.shape, deg, newx, newy, neww, newh)\n    img = ret[newy:newy + newh, newx:newx + neww]\n\n    # adjust meta data\n    adjust_joint_list = []\n    for joint in meta.joint_list:\n        adjust_joint = []\n        for point in joint:\n            if point[0] < -100 or point[1] < -100:\n                adjust_joint.append((-1000, -1000))\n                continue\n            # if point[0] <= 0 or point[1] <= 0:\n            #     adjust_joint.append((-1, -1))\n            #     continue\n            x, y = _rotate_coord((meta.width, meta.height), (newx, newy), point, deg)\n            adjust_joint.append((x, y))\n        adjust_joint_list.append(adjust_joint)\n\n    meta.joint_list = adjust_joint_list\n    meta.width, meta.height = neww, newh\n    meta.img = img\n\n    return meta\n\n\ndef pose_flip(meta):\n    r = random.uniform(0, 1.0)\n    if r > 0.5:\n        return meta\n\n    img = meta.img\n    img = cv2.flip(img, 1)\n\n    # flip meta\n    flip_list = [CocoPart.Top, CocoPart.Neck, CocoPart.LShoulder, CocoPart.LElbow, CocoPart.LWrist, CocoPart.RShoulder,\n                 CocoPart.RElbow, CocoPart.RWrist,\n                 CocoPart.LHip, CocoPart.LKnee, CocoPart.LAnkle, CocoPart.RHip, CocoPart.RKnee, CocoPart.RAnkle\n                 ]\n    adjust_joint_list = []\n    for joint in meta.joint_list:\n        adjust_joint = []\n        for cocopart in flip_list:\n            point = joint[cocopart.value]\n            if point[0] < -100 or point[1] < -100:\n                adjust_joint.append((-1000, -1000))\n                continue\n            # if point[0] <= 0 or point[1] <= 0:\n            #     adjust_joint.append((-1, -1))\n            #     continue\n            adjust_joint.append((meta.width - point[0], point[1]))\n        adjust_joint_list.append(adjust_joint)\n\n    meta.joint_list = adjust_joint_list\n\n    meta.img = img\n    return meta\n\n\ndef pose_resize_shortestedge_random(meta):\n    ratio_w = _network_w / meta.width\n    ratio_h = _network_h / meta.height\n    ratio = min(ratio_w, ratio_h)\n    target_size = int(min(meta.width * ratio + 0.5, meta.height * ratio + 0.5))\n    target_size = int(target_size * random.uniform(0.95, 1.2))\n    # target_size = int(min(_network_w, _network_h) * random.uniform(0.7, 1.5))\n    return pose_resize_shortestedge(meta, target_size)\n\n\ndef _rotate_coord(shape, newxy, point, angle):\n    angle = -1 * angle / 180.0 * math.pi\n\n    ox, oy = shape\n    px, py = point\n\n    ox /= 2\n    oy /= 2\n\n    qx = math.cos(angle) * (px - ox) - math.sin(angle) * (py - oy)\n    qy = math.sin(angle) * (px - ox) + math.cos(angle) * (py - oy)\n\n    new_x, new_y = newxy\n\n    qx += ox - new_x\n    qy += oy - new_y\n\n    return int(qx + 0.5), int(qy + 0.5)\n\n\ndef pose_resize_shortestedge(meta, target_size):\n    global _network_w, _network_h\n    img = meta.img\n\n    # adjust image\n    scale = target_size / min(meta.height, meta.width)\n    if meta.height < meta.width:\n        newh, neww = target_size, int(scale * meta.width + 0.5)\n    else:\n        newh, neww = int(scale * meta.height + 0.5), target_size\n\n    dst = cv2.resize(img, (neww, newh), interpolation=cv2.INTER_AREA)\n\n    pw = ph = 0\n    if neww < _network_w or newh < _network_h:\n        pw = max(0, (_network_w - neww) // 2)\n        ph = max(0, (_network_h - newh) // 2)\n        mw = (_network_w - neww) % 2\n        mh = (_network_h - newh) % 2\n        color1 = random.randint(0, 255)\n        color2 = random.randint(0, 255)\n        color3 = random.randint(0, 255)\n        dst = cv2.copyMakeBorder(dst, ph, ph + mh, pw, pw + mw, cv2.BORDER_CONSTANT, value=(color1, color2, color3))\n\n    # adjust meta data\n    adjust_joint_list = []\n    for joint in meta.joint_list:\n        adjust_joint = []\n        for point in joint:\n            if point[0] < -100 or point[1] < -100:\n                adjust_joint.append((-1000, -1000))\n                continue\n            # if point[0] <= 0 or point[1] <= 0 or int(point[0]*scale+0.5) > neww or int(point[1]*scale+0.5) > newh:\n            #     adjust_joint.append((-1, -1))\n            #     continue\n            adjust_joint.append((int(point[0] * scale + 0.5) + pw, int(point[1] * scale + 0.5) + ph))\n        adjust_joint_list.append(adjust_joint)\n\n    meta.joint_list = adjust_joint_list\n    meta.width, meta.height = neww + pw * 2, newh + ph * 2\n    meta.img = dst\n    return meta\n\n\ndef pose_crop(meta, x, y, w, h):\n    # adjust image\n    target_size = (w, h)\n\n    img = meta.img\n    resized = img[y:y + target_size[1], x:x + target_size[0], :]\n\n    # adjust meta data\n    adjust_joint_list = []\n    for joint in meta.joint_list:\n        adjust_joint = []\n        for point in joint:\n            if point[0] < -100 or point[1] < -100:\n                adjust_joint.append((-1000, -1000))\n                continue\n            # if point[0] <= 0 or point[1] <= 0:\n            #     adjust_joint.append((-1000, -1000))\n            #     continue\n            new_x, new_y = point[0] - x, point[1] - y\n            # if new_x <= 0 or new_y <= 0 or new_x > target_size[0] or new_y > target_size[1]:\n            #     adjust_joint.append((-1, -1))\n            #     continue\n            adjust_joint.append((new_x, new_y))\n        adjust_joint_list.append(adjust_joint)\n\n    meta.joint_list = adjust_joint_list\n    meta.width, meta.height = target_size\n    meta.img = resized\n    return meta\n\n\ndef pose_crop_random(meta):\n    global _network_w, _network_h\n    target_size = (_network_w, _network_h)\n    for _ in range(50):\n        x = random.randrange(0, meta.width - target_size[0]) if meta.width > target_size[0] else 0\n        y = random.randrange(0, meta.height - target_size[1]) if meta.height > target_size[1] else 0\n\n        # check whether any face is inside the box to generate a reasonably-balanced datasets\n        for joint in meta.joint_list:\n            if x <= joint[CocoPart.RKnee.value][0] < x + target_size[0] and \\\n                    y <= joint[CocoPart.RKnee.value][1] < y + target_size[1] and \\\n                    x <= joint[CocoPart.RAnkle.value][0] < x + target_size[0] and \\\n                    y <= joint[CocoPart.RAnkle.value][1] < y + target_size[1] and \\\n                    x <= joint[CocoPart.LKnee.value][0] < x + target_size[0] and \\\n                    y <= joint[CocoPart.LKnee.value][1] < y + target_size[1] and \\\n                    x <= joint[CocoPart.LAnkle.value][0] < x + target_size[0] and \\\n                    y <= joint[CocoPart.LAnkle.value][1] < y + target_size[1]:\n                break\n    return pose_crop(meta, x, y, target_size[0], target_size[1])\n\n\ndef pose_to_img(meta_l):\n    global _network_w, _network_h, _scale\n    return meta_l.img.astype(np.float32), \\\n           meta_l.get_heatmap(target_size=(_network_w // _scale, _network_h // _scale)).astype(np.float32)\n'"
training/src/dataset_prepare.py,0,"b'# -*- coding: utf-8 -*-\n# @Time    : 18-3-6 3:20 PM\n# @Author  : edvard_hua@live.com\n# @FileName: data_prepare.py\n# @Software: PyCharm\n\nimport numpy as np\nimport cv2\nimport struct\nimport math\n\n\nclass CocoPose:\n    @staticmethod\n    def get_bgimg(inp, target_size=None):\n        inp = cv2.cvtColor(inp.astype(np.uint8), cv2.COLOR_BGR2RGB)\n        if target_size:\n            inp = cv2.resize(inp, target_size, interpolation=cv2.INTER_AREA)\n        return inp\n\n    @staticmethod\n    def display_image(inp, heatmap=None, pred_heat=None, as_numpy=False):\n        global mplset\n        mplset = True\n        import matplotlib.pyplot as plt\n\n        fig = plt.figure()\n        if heatmap is not None:\n            a = fig.add_subplot(1, 2, 1)\n            a.set_title(\'True_Heatmap\')\n            plt.imshow(CocoPose.get_bgimg(inp, target_size=(heatmap.shape[1], heatmap.shape[0])), alpha=0.5)\n            tmp = np.amax(heatmap, axis=2)\n            plt.imshow(tmp, cmap=plt.cm.gray, alpha=0.7)\n            plt.colorbar()\n        else:\n            a = fig.add_subplot(1, 2, 1)\n            a.set_title(\'Image\')\n            plt.imshow(CocoPose.get_bgimg(inp))\n\n        if pred_heat is not None:\n            a = fig.add_subplot(1, 2, 2)\n            a.set_title(\'Pred_Heatmap\')\n            plt.imshow(CocoPose.get_bgimg(inp, target_size=(pred_heat.shape[1], pred_heat.shape[0])), alpha=0.5)\n            tmp = np.amax(pred_heat, axis=2)\n            plt.imshow(tmp, cmap=plt.cm.gray, alpha=1)\n            plt.colorbar()\n\n        if not as_numpy:\n            plt.show()\n        else:\n            fig.canvas.draw()\n            data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=\'\')\n            data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n\n            fig.clear()\n            plt.close()\n            return data\n\n\nclass CocoMetadata:\n    __coco_parts = 14\n\n    @staticmethod\n    def parse_float(four_np):\n        assert len(four_np) == 4\n        return struct.unpack(\'<f\', bytes(four_np))[0]\n\n    @staticmethod\n    def parse_floats(four_nps, adjust=0):\n        assert len(four_nps) % 4 == 0\n        return [(CocoMetadata.parse_float(four_nps[x * 4:x * 4 + 4]) + adjust) for x in range(len(four_nps) // 4)]\n\n    def __init__(self, idx, img_path, img_meta, annotations, sigma):\n        self.idx = idx\n        self.img = self.read_image(img_path)\n        self.sigma = sigma\n\n        self.height = int(img_meta[\'height\'])\n        self.width = int(img_meta[\'width\'])\n\n        joint_list = []\n        for ann in annotations:\n            if ann.get(\'num_keypoints\', 0) == 0:\n                continue\n\n            kp = np.array(ann[\'keypoints\'])\n            xs = kp[0::3]\n            ys = kp[1::3]\n            vs = kp[2::3]\n\n            joint_list.append([(x, y) if v >= 1 else (-1000, -1000) for x, y, v in zip(xs, ys, vs)])\n\n        self.joint_list = []\n        transform = list(zip(\n            [1, 2, 4, 6, 8, 3, 5, 7, 10, 12, 14, 9, 11, 13],\n            [1, 2, 4, 6, 8, 3, 5, 7, 10, 12, 14, 9, 11, 13]\n        ))\n        for prev_joint in joint_list:\n            new_joint = []\n            for idx1, idx2 in transform:\n                j1 = prev_joint[idx1 - 1]\n                j2 = prev_joint[idx2 - 1]\n\n                if j1[0] <= 0 or j1[1] <= 0 or j2[0] <= 0 or j2[1] <= 0:\n                    new_joint.append((-1000, -1000))\n                else:\n                    new_joint.append(((j1[0] + j2[0]) / 2, (j1[1] + j2[1]) / 2))\n            # background\n            # new_joint.append((-1000, -1000))\n            self.joint_list.append(new_joint)\n\n    def get_heatmap(self, target_size):\n        heatmap = np.zeros((CocoMetadata.__coco_parts, self.height, self.width), dtype=np.float32)\n\n        for joints in self.joint_list:\n            for idx, point in enumerate(joints):\n                if point[0] < 0 or point[1] < 0:\n                    continue\n                CocoMetadata.put_heatmap(heatmap, idx, point, self.sigma)\n\n        heatmap = heatmap.transpose((1, 2, 0))\n\n        # background\n        # heatmap[:, :, -1] = np.clip(1 - np.amax(heatmap, axis=2), 0.0, 1.0)\n\n        if target_size:\n            heatmap = cv2.resize(heatmap, target_size, interpolation=cv2.INTER_AREA)\n\n        return heatmap.astype(np.float16)\n\n    @staticmethod\n    def put_heatmap(heatmap, plane_idx, center, sigma):\n        center_x, center_y = center\n        _, height, width = heatmap.shape[:3]\n\n        th = 1.6052\n        delta = math.sqrt(th * 2)\n\n        x0 = int(max(0, center_x - delta * sigma))\n        y0 = int(max(0, center_y - delta * sigma))\n\n        x1 = int(min(width, center_x + delta * sigma))\n        y1 = int(min(height, center_y + delta * sigma))\n\n        # gaussian filter\n        for y in range(y0, y1):\n            for x in range(x0, x1):\n                d = (x - center_x) ** 2 + (y - center_y) ** 2\n                exp = d / 2.0 / sigma / sigma\n                if exp > th:\n                    continue\n                heatmap[plane_idx][y][x] = max(heatmap[plane_idx][y][x], math.exp(-exp))\n                heatmap[plane_idx][y][x] = min(heatmap[plane_idx][y][x], 1.0)\n\n    def read_image(self, img_path):\n        img_str = open(img_path, ""rb"").read()\n        if not img_str:\n            print(""image not read, path=%s"" % img_path)\n        nparr = np.fromstring(img_str, np.uint8)\n        return cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n'"
training/src/gen_frozen_pb.py,6,"b'# Copyright 2018 Zihua Zeng (edvard_hua@live.com)\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===================================================================================\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\nimport argparse\nfrom networks import get_network\nimport os\n\nfrom pprint import pprint\n\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = \'\'\nparser = argparse.ArgumentParser(description=\'Tensorflow Pose Estimation Graph Extractor\')\nparser.add_argument(\'--model\', type=str, default=\'mv2_cpm\', help=\'\')\nparser.add_argument(\'--size\', type=int, default=224)\nparser.add_argument(\'--checkpoint\', type=str, default=\'\', help=\'checkpoint path\')\nparser.add_argument(\'--output_node_names\', type=str, default=\'Convolutional_Pose_Machine/stage_5_out\')\nparser.add_argument(\'--output_graph\', type=str, default=\'./model.pb\', help=\'output_freeze_path\')\n\nargs = parser.parse_args()\n\ninput_node = tf.placeholder(tf.float32, shape=[1, args.size, args.size, 3], name=""image"")\n\nwith tf.Session() as sess:\n    net = get_network(args.model, input_node, trainable=False)\n    saver = tf.train.Saver()\n    saver.restore(sess, args.checkpoint)\n\n    input_graph_def = tf.get_default_graph().as_graph_def()\n    output_graph_def = tf.graph_util.convert_variables_to_constants(\n        sess,  # The session\n        input_graph_def,  # input_graph_def is useful for retrieving the nodes\n        args.output_node_names.split("","")\n    )\n\nwith tf.gfile.GFile(args.output_graph, ""wb"") as f:\n    f.write(output_graph_def.SerializeToString())'"
training/src/gen_tflite_coreml.py,1,"b'# -*- coding: utf-8 -*-\n# @Time    : 18-7-16 \xe4\xb8\x8a\xe5\x8d\x8810:26\n# @Author  : edvard_hua@live.com\n# @FileName: gen_tflite_coreml.py\n# @Software: PyCharm\n\nimport argparse\nimport os\n\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = \'\'\nparser = argparse.ArgumentParser(description=""Tools for convert frozen_pb into tflite or coreml."")\nparser.add_argument(""--frozen_pb"", type=str, default=""./hourglass/model-360000.pb"", help=""Path for storing checkpoint."")\nparser.add_argument(""--input_node_name"", type=str, default=""image"", help=""Name of input node name."")\nparser.add_argument(""--output_node_name"", type=str, default=""hourglass_out_3"", help=""Name of output node name."")\nparser.add_argument(""--output_path"", type=str, default=""./hourglass"", help=""Path for storing tflite & coreml"")\nparser.add_argument(""--type"", type=str, default=""coreml"", help=""tflite or coreml"")\n\nargs = parser.parse_args()\n\noutput_filename = args.frozen_pb.rsplit(""/"", 1)[1]\noutput_filename = output_filename.split(""."")[0]\n\n\nif ""tflite"" in args.type:\n    import tensorflow as tf\n    output_filename += "".tflite""\n    converter = tf.contrib.lite.TocoConverter.from_frozen_graph(\n        args.frozen_pb,\n        [args.input_node_name],\n        [args.output_node_name]\n    )\n    tflite_model = converter.convert()\n    open(os.path.join(args.output_path, output_filename), ""wb"").write(tflite_model)\n    print(""Generate tflite success."")\nelif ""coreml"" in args.type:\n    import tfcoreml as tf_converter\n    output_filename += "".mlmodel""\n    tf_converter.convert(tf_model_path=args.frozen_pb,\n                         mlmodel_path = os.path.join(args.output_path, output_filename),\n                         image_input_names = [""%s:0"" % args.input_node_name],\n                         output_feature_names = [\'%s:0\' % args.output_node_name])\n    print(""Generate CoreML success."")\n\n\n\n\n\n'"
training/src/network_base.py,10,"b'# -*- coding: utf-8 -*-\n# @Time    : 18-4-24 5:48 PM\n# @Author  : edvard_hua@live.com\n# @FileName: network_base.py\n# @Software: PyCharm\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\n_init_xavier = tf.contrib.layers.xavier_initializer()\n_init_norm = tf.truncated_normal_initializer(stddev=0.01)\n_init_zero = slim.init_ops.zeros_initializer()\n_l2_regularizer_00004 = tf.contrib.layers.l2_regularizer(0.00004)\n_trainable = True\n\n\ndef is_trainable(trainable=True):\n    global _trainable\n    _trainable = trainable\n\n\ndef max_pool(inputs, k_h, k_w, s_h, s_w, name, padding=""SAME""):\n    return tf.nn.max_pool(inputs,\n                          ksize=[1, k_h, k_w, 1],\n                          strides=[1, s_h, s_w, 1],\n                          padding=padding,\n                          name=name)\n\n\ndef upsample(inputs, factor, name):\n    return tf.image.resize_bilinear(inputs, [int(inputs.get_shape()[1]) * factor, int(inputs.get_shape()[2]) * factor],\n                                    name=name)\n\ndef separable_conv(input, c_o, k_s, stride, scope):\n        with slim.arg_scope([slim.batch_norm],\n                            decay=0.999,\n                            fused=True,\n                            is_training=_trainable,\n                            activation_fn=tf.nn.relu6):\n            output = slim.separable_convolution2d(input,\n                                                  num_outputs=None,\n                                                  stride=stride,\n                                                  trainable=_trainable,\n                                                  depth_multiplier=1.0,\n                                                  kernel_size=[k_s, k_s],\n                                                  weights_initializer=_init_xavier,\n                                                  weights_regularizer=_l2_regularizer_00004,\n                                                  biases_initializer=None,\n                                                  scope=scope + \'_depthwise\')\n\n            output = slim.convolution2d(output,\n                                        c_o,\n                                        stride=1,\n                                        kernel_size=[1, 1],\n                                        weights_initializer=_init_xavier,\n                                        biases_initializer=_init_zero,\n                                        normalizer_fn=slim.batch_norm,\n                                        trainable=_trainable,\n                                        weights_regularizer=None,\n                                        scope=scope + \'_pointwise\')\n\n        return output\n\n\ndef inverted_bottleneck(inputs, up_channel_rate, channels, subsample, k_s=3, scope=""""):\n    with tf.variable_scope(""inverted_bottleneck_%s"" % scope):\n        with slim.arg_scope([slim.batch_norm],\n                            decay=0.999,\n                            fused=True,\n                            is_training=_trainable,\n                            activation_fn=tf.nn.relu6):\n            stride = 2 if subsample else 1\n\n            output = slim.convolution2d(inputs,\n                                        up_channel_rate * inputs.get_shape().as_list()[-1],\n                                        stride=1,\n                                        kernel_size=[1, 1],\n                                        weights_initializer=_init_xavier,\n                                        biases_initializer=_init_zero,\n                                        normalizer_fn=slim.batch_norm,\n                                        weights_regularizer=None,\n                                        scope=scope + \'_up_pointwise\',\n                                        trainable=_trainable)\n\n            output = slim.separable_convolution2d(output,\n                                                  num_outputs=None,\n                                                  stride=stride,\n                                                  depth_multiplier=1.0,\n                                                  kernel_size=k_s,\n                                                  weights_initializer=_init_xavier,\n                                                  weights_regularizer=_l2_regularizer_00004,\n                                                  biases_initializer=None,\n                                                  padding=""SAME"",\n                                                  scope=scope + \'_depthwise\',\n                                                  trainable=_trainable)\n\n            output = slim.convolution2d(output,\n                                        channels,\n                                        stride=1,\n                                        kernel_size=[1, 1],\n                                        activation_fn=None,\n                                        weights_initializer=_init_xavier,\n                                        biases_initializer=_init_zero,\n                                        normalizer_fn=slim.batch_norm,\n                                        weights_regularizer=None,\n                                        scope=scope + \'_pointwise\',\n                                        trainable=_trainable)\n            if inputs.get_shape().as_list()[-1] == channels:\n                output = tf.add(inputs, output)\n\n    return output\n\n\ndef convb(input, k_h, k_w, c_o, stride, name, relu=True):\n    with slim.arg_scope([slim.batch_norm], decay=0.999, fused=True, is_training=_trainable):\n        output = slim.convolution2d(\n            inputs=input,\n            num_outputs=c_o,\n            kernel_size=[k_h, k_w],\n            stride=stride,\n            normalizer_fn=slim.batch_norm,\n            weights_regularizer=_l2_regularizer_00004,\n            weights_initializer=_init_xavier,\n            biases_initializer=_init_zero,\n            activation_fn=tf.nn.relu if relu else None,\n            scope=name,\n            trainable=_trainable)\n    return output\n'"
training/src/network_mv2_cpm.py,4,"b'# Copyright 2018 Zihua Zeng (edvard_hua@live.com)\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===================================================================================\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom network_base import max_pool, upsample, inverted_bottleneck, separable_conv, convb, is_trainable\n\nN_KPOINTS = 14\nSTAGE_NUM = 6\n\nout_channel_ratio = lambda d: max(int(d * 0.75), 8)\nup_channel_ratio = lambda d: int(d * 1.)\nout_channel_cpm = lambda d: max(int(d * 0.75), 8)\n\n\ndef build_network(input, trainable):\n    is_trainable(trainable)\n\n    net = convb(input, 3, 3, out_channel_ratio(32), 2, name=""Conv2d_0"")\n\n    with tf.variable_scope(\'MobilenetV2\'):\n\n        # 128, 112\n        mv2_branch_0 = slim.stack(net, inverted_bottleneck,\n                                  [\n                                      (1, out_channel_ratio(16), 0, 3),\n                                      (1, out_channel_ratio(16), 0, 3)\n                                  ], scope=""MobilenetV2_part_0"")\n\n        # 64, 56\n        mv2_branch_1 = slim.stack(mv2_branch_0, inverted_bottleneck,\n                                  [\n                                      (up_channel_ratio(6), out_channel_ratio(24), 1, 3),\n                                      (up_channel_ratio(6), out_channel_ratio(24), 0, 3),\n                                      (up_channel_ratio(6), out_channel_ratio(24), 0, 3),\n                                      (up_channel_ratio(6), out_channel_ratio(24), 0, 3),\n                                      (up_channel_ratio(6), out_channel_ratio(24), 0, 3),\n                                  ], scope=""MobilenetV2_part_1"")\n\n        # 32, 28\n        mv2_branch_2 = slim.stack(mv2_branch_1, inverted_bottleneck,\n                                  [\n                                      (up_channel_ratio(6), out_channel_ratio(32), 1, 3),\n                                      (up_channel_ratio(6), out_channel_ratio(32), 0, 3),\n                                      (up_channel_ratio(6), out_channel_ratio(32), 0, 3),\n                                      (up_channel_ratio(6), out_channel_ratio(32), 0, 3),\n                                      (up_channel_ratio(6), out_channel_ratio(32), 0, 3),\n                                  ], scope=""MobilenetV2_part_2"")\n\n        # 16, 14\n        mv2_branch_3 = slim.stack(mv2_branch_2, inverted_bottleneck,\n                                  [\n                                      (up_channel_ratio(6), out_channel_ratio(64), 1, 3),\n                                      (up_channel_ratio(6), out_channel_ratio(64), 0, 3),\n                                      (up_channel_ratio(6), out_channel_ratio(64), 0, 3),\n                                      (up_channel_ratio(6), out_channel_ratio(64), 0, 3),\n                                      (up_channel_ratio(6), out_channel_ratio(64), 0, 3),\n                                  ], scope=""MobilenetV2_part_3"")\n\n        # 8, 7\n        mv2_branch_4 = slim.stack(mv2_branch_3, inverted_bottleneck,\n                                  [\n                                      (up_channel_ratio(6), out_channel_ratio(96), 1, 3),\n                                      (up_channel_ratio(6), out_channel_ratio(96), 0, 3),\n                                      (up_channel_ratio(6), out_channel_ratio(96), 0, 3),\n                                      (up_channel_ratio(6), out_channel_ratio(96), 0, 3),\n                                      (up_channel_ratio(6), out_channel_ratio(96), 0, 3)\n                                  ], scope=""MobilenetV2_part_4"")\n\n        cancat_mv2 = tf.concat(\n            [\n                max_pool(mv2_branch_0, 4, 4, 4, 4, name=""mv2_0_max_pool""),\n                max_pool(mv2_branch_1, 2, 2, 2, 2, name=""mv2_1_max_pool""),\n                mv2_branch_2,\n                upsample(mv2_branch_3, 2, name=""mv2_3_upsample""),\n                upsample(mv2_branch_4, 4, name=""mv2_4_upsample"")\n            ]\n            , axis=3)\n\n    with tf.variable_scope(""Convolutional_Pose_Machine""):\n        l2s = []\n        prev = None\n        for stage_number in range(STAGE_NUM):\n            if prev is not None:\n                inputs = tf.concat([cancat_mv2, prev], axis=3)\n            else:\n                inputs = cancat_mv2\n\n            kernel_size = 7\n            lastest_channel_size = 128\n            if stage_number == 0:\n                kernel_size = 3\n                lastest_channel_size = 512\n\n            _ = slim.stack(inputs, inverted_bottleneck,\n                           [\n                               (2, out_channel_cpm(32), 0, kernel_size),\n                               (up_channel_ratio(4), out_channel_cpm(32), 0, kernel_size),\n                               (up_channel_ratio(4), out_channel_cpm(32), 0, kernel_size),\n                           ], scope=""stage_%d_mv2"" % stage_number)\n\n            _ = slim.stack(_, separable_conv,\n                           [\n                               (out_channel_ratio(lastest_channel_size), 1, 1),\n                               (N_KPOINTS, 1, 1)\n                           ], scope=""stage_%d_mv1"" % stage_number)\n\n            prev = _\n            cpm_out = upsample(_, 4, ""stage_%d_out"" % stage_number)\n            l2s.append(cpm_out)\n\n    return cpm_out, l2s\n'"
training/src/network_mv2_hourglass.py,1,"b'# -*- coding: utf-8 -*-\n# @Time    : 18-4-12 5:12 PM\n# @Author  : edvard_hua@live.com\n# @FileName: network_mv2_cpm.py\n# @Software: PyCharm\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom network_base import max_pool, upsample, inverted_bottleneck, separable_conv, convb, is_trainable\n\nN_KPOINTS = 14\nSTAGE_NUM = 4\n\nout_channel_ratio = lambda d: int(d * 1.0)\nup_channel_ratio = lambda d: int(d * 1.0)\n\nl2s = []\n\n\ndef hourglass_module(inp, stage_nums):\n    if stage_nums > 0:\n        down_sample = max_pool(inp, 2, 2, 2, 2, name=""hourglass_downsample_%d"" % stage_nums)\n\n        block_front = slim.stack(down_sample, inverted_bottleneck,\n                                 [\n                                     (up_channel_ratio(6), out_channel_ratio(24), 0, 3),\n                                     (up_channel_ratio(6), out_channel_ratio(24), 0, 3),\n                                     (up_channel_ratio(6), out_channel_ratio(24), 0, 3),\n                                     (up_channel_ratio(6), out_channel_ratio(24), 0, 3),\n                                     (up_channel_ratio(6), out_channel_ratio(24), 0, 3),\n                                 ], scope=""hourglass_front_%d"" % stage_nums)\n        stage_nums -= 1\n        block_mid = hourglass_module(block_front, stage_nums)\n        block_back = inverted_bottleneck(\n            block_mid, up_channel_ratio(6), N_KPOINTS,\n            0, 3, scope=""hourglass_back_%d"" % stage_nums)\n\n        up_sample = upsample(block_back, 2, ""hourglass_upsample_%d"" % stage_nums)\n\n        # jump layer\n        branch_jump = slim.stack(inp, inverted_bottleneck,\n                                 [\n                                     (up_channel_ratio(6), out_channel_ratio(24), 0, 3),\n                                     (up_channel_ratio(6), out_channel_ratio(24), 0, 3),\n                                     (up_channel_ratio(6), out_channel_ratio(24), 0, 3),\n                                     (up_channel_ratio(6), out_channel_ratio(24), 0, 3),\n                                     (up_channel_ratio(6), N_KPOINTS, 0, 3),\n                                 ], scope=""hourglass_branch_jump_%d"" % stage_nums)\n\n        curr_hg_out = tf.add(up_sample, branch_jump, name=""hourglass_out_%d"" % stage_nums)\n        # mid supervise\n        l2s.append(curr_hg_out)\n\n        return curr_hg_out\n\n    _ = inverted_bottleneck(\n        inp, up_channel_ratio(6), out_channel_ratio(24),\n        0, 3, scope=""hourglass_mid_%d"" % stage_nums\n    )\n    return _\n\n\ndef build_network(input, trainable):\n    is_trainable(trainable)\n\n    net = convb(input, 3, 3, out_channel_ratio(16), 2, name=""Conv2d_0"")\n\n    # 128, 112\n    net = slim.stack(net, inverted_bottleneck,\n                     [\n                         (1, out_channel_ratio(16), 0, 3),\n                         (1, out_channel_ratio(16), 0, 3)\n                     ], scope=""Conv2d_1"")\n\n    # 64, 56\n    net = slim.stack(net, inverted_bottleneck,\n                     [\n                         (up_channel_ratio(6), out_channel_ratio(24), 1, 3),\n                         (up_channel_ratio(6), out_channel_ratio(24), 0, 3),\n                         (up_channel_ratio(6), out_channel_ratio(24), 0, 3),\n                         (up_channel_ratio(6), out_channel_ratio(24), 0, 3),\n                         (up_channel_ratio(6), out_channel_ratio(24), 0, 3),\n                     ], scope=""Conv2d_2"")\n\n    net_h_w = int(net.shape[1])\n    # build network recursively\n    hg_out = hourglass_module(net, STAGE_NUM)\n\n    for index, l2 in enumerate(l2s):\n        l2_w_h = int(l2.shape[1])\n        if l2_w_h == net_h_w:\n            continue\n        scale = net_h_w // l2_w_h\n        l2s[index] = upsample(l2, scale, name=""upsample_for_loss_%d"" % index)\n\n    return hg_out, l2s\n'"
training/src/networks.py,0,"b'# -*- coding: utf-8 -*-\n# @Time    : 18-3-6 3:20 PM\n# @Author  : edvard_hua@live.com\n# @FileName: data_filter.py\n# @Software: PyCharm\n\nimport network_mv2_cpm\nimport network_mv2_hourglass\n\ndef get_network(type, input, trainable=True):\n    if type == \'mv2_cpm\':\n        net, loss = network_mv2_cpm.build_network(input, trainable)\n    elif type == ""mv2_hourglass"":\n        net, loss = network_mv2_hourglass.build_network(input, trainable)        \n    return net, loss\n'"
training/src/test.py,19,"b'# Copyright 2018 Zihua Zeng (edvard_hua@live.com)\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===================================================================================\n# -*- coding: utf-8 -*-\n\n\ndef display_image():\n    """"""\n    display heatmap & origin image\n    :return:\n    """"""\n    from dataset_prepare import CocoMetadata, CocoPose\n    from pycocotools.coco import COCO\n    from os.path import join\n    from dataset import _parse_function\n\n    BASE_PATH = ""/root/hdd/ai_challenger""\n\n    import os\n    # os.chdir("".."")\n\n    ANNO = COCO(\n        join(BASE_PATH, ""ai_challenger_valid.json"")\n    )\n    train_imgIds = ANNO.getImgIds()\n\n    img, heat = _parse_function(train_imgIds[100], ANNO)\n\n    CocoPose.display_image(img, heat, pred_heat=heat, as_numpy=False)\n\n    from PIL import Image\n    for _ in range(heat.shape[2]):\n        data = CocoPose.display_image(img, heat, pred_heat=heat[:, :, _:(_ + 1)], as_numpy=True)\n        im = Image.fromarray(data)\n        im.save(""test_heatmap/heat_%d.jpg"" % _)\n\n\ndef saved_model_graph():\n    """"""\n    save the graph of model and check it in tensorboard\n    :return:\n    """"""\n\n    from os.path import join\n    from network_mv2_cpm_2 import build_network\n    import tensorflow as tf\n    import os\n\n    INPUT_WIDTH = 256\n    INPUT_HEIGHT = 256\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'\'\n\n    input_node = tf.placeholder(tf.float32, shape=(1, INPUT_WIDTH, INPUT_HEIGHT, 3),\n                                name=\'image\')\n    build_network(input_node, False)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    with tf.Session(config=config) as sess:\n        train_writer = tf.summary.FileWriter(\n            join(""tensorboard/test_graph/""),\n            sess.graph\n        )\n        sess.run(tf.global_variables_initializer())\n\n\ndef metric_prefix(input_width, input_height):\n    """"""\n    output the calculation of you model\n    :param input_width:\n    :param input_height:\n    :return:\n    """"""\n    import tensorflow as tf\n    from networks import get_network\n    import os\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'\'\n\n    input_node = tf.placeholder(tf.float32, shape=(1, input_width, input_height, 3),\n                                name=\'image\')\n    get_network(""mv2_cpm_2"", input_node, False)\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    run_meta = tf.RunMetadata()\n    with tf.Session(config=config) as sess:\n        opts = tf.profiler.ProfileOptionBuilder.float_operation()\n        flops = tf.profiler.profile(sess.graph, run_meta=run_meta, cmd=\'op\', options=opts)\n\n        opts = tf.profiler.ProfileOptionBuilder.trainable_variables_parameter()\n        params = tf.profiler.profile(sess.graph, run_meta=run_meta, cmd=\'op\', options=opts)\n\n        print(""opts {:,} --- paras {:,}"".format(flops.total_float_ops, params.total_parameters))\n        sess.run(tf.global_variables_initializer())\n\n\ndef run_with_frozen_pb(img_path, input_w_h, frozen_graph, output_node_names):\n    import tensorflow as tf\n    import cv2\n    import numpy as np\n    import os\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'\'\n    from dataset_prepare import CocoPose\n    with tf.gfile.GFile(frozen_graph, ""rb"") as f:\n        restored_graph_def = tf.GraphDef()\n        restored_graph_def.ParseFromString(f.read())\n\n    tf.import_graph_def(\n        restored_graph_def,\n        input_map=None,\n        return_elements=None,\n        name=""""\n    )\n\n    graph = tf.get_default_graph()\n    image = graph.get_tensor_by_name(""image:0"")\n    output = graph.get_tensor_by_name(""%s:0"" % output_node_names)\n\n    image_0 = cv2.imread(img_path)\n    w, h, _ = image_0.shape\n    image_ = cv2.resize(image_0, (input_w_h, input_w_h), interpolation=cv2.INTER_AREA)\n\n    with tf.Session() as sess:\n        heatmaps = sess.run(output, feed_dict={image: [image_]})\n        CocoPose.display_image(\n            # np.reshape(image_, [1, input_w_h, input_w_h, 3]),\n            image_,\n            None,\n            heatmaps[0,:,:,:],\n            False\n        )\n        # save each heatmaps to disk\n        from PIL import Image\n        for _ in range(heatmaps.shape[2]):\n            data = CocoPose.display_image(image_, heatmaps[0,:,:,:], pred_heat=heatmaps[0, :, :, _:(_ + 1)], as_numpy=True)\n            im = Image.fromarray(data)\n            im.save(""test/heat_%d.jpg"" % _)\n\n\nif __name__ == \'__main__\':\n    # saved_model_graph()\n    metric_prefix(256, 256)\n    # run_with_frozen_pb(\n    #     ""/root/hdd/ai_challenger/train/0a9396675bf14580eb08c37e0b8a69a0299afb03.jpg"",\n    #     256,\n    #     ""./model-260000.pb"",\n    #     ""Convolutional_Pose_Machine/stage_5_out""\n    # )\n    # display_image()\n\n'"
training/src/train.py,39,"b'# Copyright 2018 Zihua Zeng (edvard_hua@live.com)\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ===================================================================================\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\nimport os\nimport platform\nimport time\nimport numpy as np\nimport configparser\nimport dataset\n\nfrom datetime import datetime\n\nfrom dataset import get_train_dataset_pipeline, get_valid_dataset_pipeline\nfrom networks import get_network\nfrom dataset_prepare import CocoPose\nfrom dataset_augment import set_network_input_wh, set_network_scale\n\ndef get_input_iter(batchsize, epoch, is_train=True):\n    if is_train is True:\n        input_pipeline = get_train_dataset_pipeline(batch_size=batchsize, epoch=epoch, buffer_size=100)\n    else:\n        input_pipeline = get_valid_dataset_pipeline(batch_size=batchsize, epoch=epoch, buffer_size=100)\n    iter = input_pipeline.make_one_shot_iterator()\n    return iter\n\ndef get_loss_and_output(model, batchsize, input_image, input_heat, reuse_variables=None):\n    losses = []\n\n    with tf.variable_scope(tf.get_variable_scope(), reuse=reuse_variables):\n        _, pred_heatmaps_all = get_network(model, input_image, True)\n\n    for idx, pred_heat in enumerate(pred_heatmaps_all):\n        loss_l2 = tf.nn.l2_loss(tf.concat(pred_heat, axis=0) - input_heat, name=\'loss_heatmap_stage%d\' % idx)\n        losses.append(loss_l2)\n\n    total_loss = tf.reduce_sum(losses) / batchsize\n    total_loss_ll_heat = tf.reduce_sum(loss_l2) / batchsize\n    return total_loss, total_loss_ll_heat, pred_heat\n\n\ndef average_gradients(tower_grads):\n    """"""\n    Get gradients of all variables.\n    :param tower_grads:\n    :return:\n    """"""\n    average_grads = []\n\n    # get variable and gradients in differents gpus\n    for grad_and_vars in zip(*tower_grads):\n        # calculate the average gradient of each gpu\n        grads = []\n        for g, _ in grad_and_vars:\n            expanded_g = tf.expand_dims(g, 0)\n            grads.append(expanded_g)\n        grad = tf.concat(grads, 0)\n        grad = tf.reduce_mean(grad, 0)\n\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        average_grads.append(grad_and_var)\n    return average_grads\n\n\ndef main(argv=None):\n    # load config file and setup\n    params = {}\n    config = configparser.ConfigParser()\n    config_file = ""experiments/mv2_cpm.cfg""\n    if len(argv) != 1:\n        config_file = argv[1]\n    config.read(config_file)\n    for _ in config.options(""Train""):\n        params[_] = eval(config.get(""Train"", _))\n\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = params[\'visible_devices\']\n\n    gpus_index = params[\'visible_devices\'].split("","")\n    params[\'gpus\'] = len(gpus_index)\n\n    if not os.path.exists(params[\'modelpath\']):\n        os.makedirs(params[\'modelpath\'])\n    if not os.path.exists(params[\'logpath\']):\n        os.makedirs(params[\'logpath\'])\n\n    dataset.set_config(params)\n    set_network_input_wh(params[\'input_width\'], params[\'input_height\'])\n    set_network_scale(params[\'scale\'])\n\n    gpus = \'gpus\'\n    if platform.system() == \'Darwin\':\n        gpus = \'cpu\'\n    training_name = \'{}_batch-{}_lr-{}_{}-{}_{}x{}_{}\'.format(\n        params[\'model\'],\n        params[\'batchsize\'],\n        params[\'lr\'],\n        gpus,\n        params[\'gpus\'],\n        params[\'input_width\'], params[\'input_height\'],\n        config_file.replace(""/"", ""-"").replace("".cfg"", """")\n    )\n\n    with tf.Graph().as_default(), tf.device(""/cpu:0""):\n        train_dataset = get_train_dataset_pipeline(params[\'batchsize\'], params[\'max_epoch\'], buffer_size=100)\n        valid_dataset = get_valid_dataset_pipeline(params[\'batchsize\'], params[\'max_epoch\'], buffer_size=100)\n\n        train_iterator = train_dataset.make_one_shot_iterator()\n        valid_iterator = valid_dataset.make_one_shot_iterator()\n        \n        handle = tf.placeholder(tf.string, shape=[])\n        input_iterator = tf.data.Iterator.from_string_handle(handle, train_dataset.output_types, train_dataset.output_shapes)\n\n        global_step = tf.Variable(0, trainable=False)\n        learning_rate = tf.train.exponential_decay(float(params[\'lr\']), global_step,\n                                                   decay_steps=10000, decay_rate=float(params[\'decay_rate\']), staircase=True)\n        opt = tf.train.AdamOptimizer(learning_rate, epsilon=1e-8)\n        tower_grads = []\n        reuse_variable = False\n\n        if platform.system() == \'Darwin\':\n            # cpu (mac only)\n            with tf.device(""/cpu:0""):\n                with tf.name_scope(""CPU_0""):\n                    input_image, input_heat = input_iterator.get_next()\n                    loss, last_heat_loss, pred_heat = get_loss_and_output(params[\'model\'], params[\'batchsize\'],\n                                                                          input_image, input_heat, reuse_variable)\n                    reuse_variable = True\n                    grads = opt.compute_gradients(loss)\n                    tower_grads.append(grads)\n        else:\n            # multiple gpus\n            for i in range(params[\'gpus\']):\n                with tf.device(""/gpu:%d"" % i):\n                    with tf.name_scope(""GPU_%d"" % i):\n                        input_image, input_heat = input_iterator.get_next()\n                        loss, last_heat_loss, pred_heat = get_loss_and_output(params[\'model\'], params[\'batchsize\'], input_image, input_heat, reuse_variable)\n                        reuse_variable = True\n                        grads = opt.compute_gradients(loss)\n                        tower_grads.append(grads)\n\n        grads = average_gradients(tower_grads)\n        for grad, var in grads:\n            if grad is not None:\n                tf.summary.histogram(""gradients_on_average/%s"" % var.op.name, grad)\n\n        apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n        for var in tf.trainable_variables():\n            tf.summary.histogram(var.op.name, var)\n\n        MOVING_AVERAGE_DECAY = 0.99\n        variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n        variable_to_average = (tf.trainable_variables() + tf.moving_average_variables())\n        variables_averages_op = variable_averages.apply(variable_to_average)\n\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n            train_op = tf.group(apply_gradient_op, variables_averages_op)\n\n        saver = tf.train.Saver(max_to_keep=100)\n\n        tf.summary.scalar(""learning_rate"", learning_rate)\n        tf.summary.scalar(""loss"", loss)\n        tf.summary.scalar(""loss_lastlayer_heat"", last_heat_loss)\n        summary_merge_op = tf.summary.merge_all()\n\n        pred_result_image = tf.placeholder(tf.float32, shape=[params[\'batchsize\'], 480, 640, 3])\n        pred_result__summary = tf.summary.image(""pred_result_image"", pred_result_image, params[\'batchsize\'])\n\n        init = tf.global_variables_initializer()\n        config = tf.ConfigProto()\n        # occupy gpu gracefully\n        config.gpu_options.allow_growth = True\n        with tf.Session(config=config) as sess:\n            init.run()\n            train_handle = sess.run(train_iterator.string_handle())\n            valid_handle = sess.run(valid_iterator.string_handle())\n            coord = tf.train.Coordinator()\n            threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n            summary_writer = tf.summary.FileWriter(os.path.join(params[\'logpath\'], training_name), sess.graph)\n            total_step_num = params[\'num_train_samples\'] * params[\'max_epoch\'] // (params[\'batchsize\'] * params[\'gpus\'])\n            print(""Start training..."")\n            for step in range(total_step_num):\n                start_time = time.time()\n                _, loss_value, lh_loss = sess.run([train_op, loss, last_heat_loss],\n                                                  feed_dict={handle: train_handle}\n                )\n                duration = time.time() - start_time\n\n                if step != 0 and step % params[\'per_update_tensorboard_step\'] == 0:\n                    # False will speed up the training time.\n                    if params[\'pred_image_on_tensorboard\'] is True:\n                        valid_loss_value, valid_lh_loss, valid_in_image, valid_in_heat, valid_p_heat = sess.run(\n                            [loss, last_heat_loss, input_image, input_heat, pred_heat],\n                            feed_dict={handle: valid_handle}\n                        )\n                        result = []\n                        for index in range(params[\'batchsize\']):\n                            r = CocoPose.display_image(\n                                    valid_in_image[index,:,:,:],\n                                    valid_in_heat[index,:,:,:],\n                                    valid_p_heat[index,:,:,:],\n                                    True\n                                )\n                            result.append(\n                                r.astype(np.float32)\n                            )\n\n                        comparsion_of_pred_result = sess.run(\n                            pred_result__summary,\n                            feed_dict={\n                                pred_result_image: np.array(result)\n                            }\n                        )\n                        summary_writer.add_summary(comparsion_of_pred_result, step)\n\n                    # print train info\n                    num_examples_per_step = params[\'batchsize\'] * params[\'gpus\']\n                    examples_per_sec = num_examples_per_step / duration\n                    sec_per_batch = duration / params[\'gpus\']\n                    format_str = (\'%s: step %d, loss = %.2f, last_heat_loss = %.2f (%.1f examples/sec; %.3f sec/batch)\')\n                    print(format_str % (datetime.now(), step, loss_value, lh_loss, examples_per_sec, sec_per_batch))\n\n                    # tensorboard visualization\n                    merge_op = sess.run(summary_merge_op, feed_dict={handle: valid_handle})\n                    summary_writer.add_summary(merge_op, step)\n\n                # save model\n                if step != 0 and step % params[\'per_saved_model_step\'] == 0:\n                    checkpoint_path = os.path.join(params[\'modelpath\'], training_name, \'model\')\n                    saver.save(sess, checkpoint_path, global_step=step)\n            coord.request_stop()\n            coord.join(threads)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()'"
training/docker/cocoapi/PythonAPI/setup.py,0,"b'from distutils.core import setup\nfrom Cython.Build import cythonize\nfrom distutils.extension import Extension\nimport numpy as np\n\n# To compile and install locally run ""python setup.py build_ext --inplace""\n# To install library to Python site-packages run ""python setup.py build_ext install""\n\next_modules = [\n    Extension(\n        \'pycocotools._mask\',\n        sources=[\'../common/maskApi.c\', \'pycocotools/_mask.pyx\'],\n        include_dirs = [np.get_include(), \'../common\'],\n        extra_compile_args=[\'-Wno-cpp\', \'-Wno-unused-function\', \'-std=c99\'],\n    )\n]\n\nsetup(name=\'pycocotools\',\n      packages=[\'pycocotools\'],\n      package_dir = {\'pycocotools\': \'pycocotools\'},\n      version=\'2.0\',\n      ext_modules=\n          cythonize(ext_modules)\n      )'"
training/docker/cocoapi/PythonAPI/pycocotools/__init__.py,0,"b""__author__ = 'tylin'\n"""
training/docker/cocoapi/PythonAPI/pycocotools/coco.py,0,"b'__author__ = \'tylin\'\n__version__ = \'2.0\'\n# Interface for accessing the Microsoft COCO dataset.\n\n# Microsoft COCO is a large image dataset designed for object detection,\n# segmentation, and caption generation. pycocotools is a Python API that\n# assists in loading, parsing and visualizing the annotations in COCO.\n# Please visit http://mscoco.org/ for more information on COCO, including\n# for the data, paper, and tutorials. The exact format of the annotations\n# is also described on the COCO website. For example usage of the pycocotools\n# please see pycocotools_demo.ipynb. In addition to this API, please download both\n# the COCO images and annotations in order to run the demo.\n\n# An alternative to using the API is to load the annotations directly\n# into Python dictionary\n# Using the API provides additional utility functions. Note that this API\n# supports both *instance* and *caption* annotations. In the case of\n# captions not all functions are defined (e.g. categories are undefined).\n\n# The following API functions are defined:\n#  COCO       - COCO api class that loads COCO annotation file and prepare data structures.\n#  decodeMask - Decode binary mask M encoded via run-length encoding.\n#  encodeMask - Encode binary mask M using run-length encoding.\n#  getAnnIds  - Get ann ids that satisfy given filter conditions.\n#  getCatIds  - Get cat ids that satisfy given filter conditions.\n#  getImgIds  - Get img ids that satisfy given filter conditions.\n#  loadAnns   - Load anns with the specified ids.\n#  loadCats   - Load cats with the specified ids.\n#  loadImgs   - Load imgs with the specified ids.\n#  annToMask  - Convert segmentation in an annotation to binary mask.\n#  showAnns   - Display the specified annotations.\n#  loadRes    - Load algorithm results and create API for accessing them.\n#  download   - Download COCO images from mscoco.org server.\n# Throughout the API ""ann""=annotation, ""cat""=category, and ""img""=image.\n# Help on each functions can be accessed by: ""help COCO>function"".\n\n# See also COCO>decodeMask,\n# COCO>encodeMask, COCO>getAnnIds, COCO>getCatIds,\n# COCO>getImgIds, COCO>loadAnns, COCO>loadCats,\n# COCO>loadImgs, COCO>annToMask, COCO>showAnns\n\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2014.\n# Licensed under the Simplified BSD License [see bsd.txt]\n\nimport json\nimport time\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import PatchCollection\nfrom matplotlib.patches import Polygon\nimport numpy as np\nimport copy\nimport itertools\nfrom . import mask as maskUtils\nimport os\nfrom collections import defaultdict\nimport sys\nPYTHON_VERSION = sys.version_info[0]\nif PYTHON_VERSION == 2:\n    from urllib import urlretrieve\nelif PYTHON_VERSION == 3:\n    from urllib.request import urlretrieve\n\n\ndef _isArrayLike(obj):\n    return hasattr(obj, \'__iter__\') and hasattr(obj, \'__len__\')\n\n\nclass COCO:\n    def __init__(self, annotation_file=None):\n        """"""\n        Constructor of Microsoft COCO helper class for reading and visualizing annotations.\n        :param annotation_file (str): location of annotation file\n        :param image_folder (str): location to the folder that hosts images.\n        :return:\n        """"""\n        # load dataset\n        self.dataset,self.anns,self.cats,self.imgs = dict(),dict(),dict(),dict()\n        self.imgToAnns, self.catToImgs = defaultdict(list), defaultdict(list)\n        if not annotation_file == None:\n            print(\'loading annotations into memory...\')\n            tic = time.time()\n            dataset = json.load(open(annotation_file, \'r\'))\n            assert type(dataset)==dict, \'annotation file format {} not supported\'.format(type(dataset))\n            print(\'Done (t={:0.2f}s)\'.format(time.time()- tic))\n            self.dataset = dataset\n            self.createIndex()\n\n    def createIndex(self):\n        # create index\n        print(\'creating index...\')\n        anns, cats, imgs = {}, {}, {}\n        imgToAnns,catToImgs = defaultdict(list),defaultdict(list)\n        if \'annotations\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                imgToAnns[ann[\'image_id\']].append(ann)\n                anns[ann[\'id\']] = ann\n\n        if \'images\' in self.dataset:\n            for img in self.dataset[\'images\']:\n                imgs[img[\'id\']] = img\n\n        if \'categories\' in self.dataset:\n            for cat in self.dataset[\'categories\']:\n                cats[cat[\'id\']] = cat\n\n        if \'annotations\' in self.dataset and \'categories\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                catToImgs[ann[\'category_id\']].append(ann[\'image_id\'])\n\n        print(\'index created!\')\n\n        # create class members\n        self.anns = anns\n        self.imgToAnns = imgToAnns\n        self.catToImgs = catToImgs\n        self.imgs = imgs\n        self.cats = cats\n\n    def info(self):\n        """"""\n        Print information about the annotation file.\n        :return:\n        """"""\n        for key, value in self.dataset[\'info\'].items():\n            print(\'{}: {}\'.format(key, value))\n\n    def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n        """"""\n        Get ann ids that satisfy given filter conditions. default skips that filter\n        :param imgIds  (int array)     : get anns for given imgs\n               catIds  (int array)     : get anns for given cats\n               areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n               iscrowd (boolean)       : get anns for given crowd label (False or True)\n        :return: ids (int array)       : integer array of ann ids\n        """"""\n        imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(imgIds) == len(catIds) == len(areaRng) == 0:\n            anns = self.dataset[\'annotations\']\n        else:\n            if not len(imgIds) == 0:\n                lists = [self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns]\n                anns = list(itertools.chain.from_iterable(lists))\n            else:\n                anns = self.dataset[\'annotations\']\n            anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann[\'category_id\'] in catIds]\n            anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann[\'area\'] > areaRng[0] and ann[\'area\'] < areaRng[1]]\n        if not iscrowd == None:\n            ids = [ann[\'id\'] for ann in anns if ann[\'iscrowd\'] == iscrowd]\n        else:\n            ids = [ann[\'id\'] for ann in anns]\n        return ids\n\n    def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n        """"""\n        filtering parameters. default skips that filter.\n        :param catNms (str array)  : get cats for given cat names\n        :param supNms (str array)  : get cats for given supercategory names\n        :param catIds (int array)  : get cats for given cat ids\n        :return: ids (int array)   : integer array of cat ids\n        """"""\n        catNms = catNms if _isArrayLike(catNms) else [catNms]\n        supNms = supNms if _isArrayLike(supNms) else [supNms]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(catNms) == len(supNms) == len(catIds) == 0:\n            cats = self.dataset[\'categories\']\n        else:\n            cats = self.dataset[\'categories\']\n            cats = cats if len(catNms) == 0 else [cat for cat in cats if cat[\'name\']          in catNms]\n            cats = cats if len(supNms) == 0 else [cat for cat in cats if cat[\'supercategory\'] in supNms]\n            cats = cats if len(catIds) == 0 else [cat for cat in cats if cat[\'id\']            in catIds]\n        ids = [cat[\'id\'] for cat in cats]\n        return ids\n\n    def getImgIds(self, imgIds=[], catIds=[]):\n        \'\'\'\n        Get img ids that satisfy given filter conditions.\n        :param imgIds (int array) : get imgs for given ids\n        :param catIds (int array) : get imgs with all given cats\n        :return: ids (int array)  : integer array of img ids\n        \'\'\'\n        imgIds = imgIds if _isArrayLike(imgIds) else [imgIds]\n        catIds = catIds if _isArrayLike(catIds) else [catIds]\n\n        if len(imgIds) == len(catIds) == 0:\n            ids = self.imgs.keys()\n        else:\n            ids = set(imgIds)\n            for i, catId in enumerate(catIds):\n                if i == 0 and len(ids) == 0:\n                    ids = set(self.catToImgs[catId])\n                else:\n                    ids &= set(self.catToImgs[catId])\n        return list(ids)\n\n    def loadAnns(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying anns\n        :return: anns (object array) : loaded ann objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.anns[id] for id in ids]\n        elif type(ids) == int:\n            return [self.anns[ids]]\n\n    def loadCats(self, ids=[]):\n        """"""\n        Load cats with the specified ids.\n        :param ids (int array)       : integer ids specifying cats\n        :return: cats (object array) : loaded cat objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.cats[id] for id in ids]\n        elif type(ids) == int:\n            return [self.cats[ids]]\n\n    def loadImgs(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying img\n        :return: imgs (object array) : loaded img objects\n        """"""\n        if _isArrayLike(ids):\n            return [self.imgs[id] for id in ids]\n        elif type(ids) == int:\n            return [self.imgs[ids]]\n\n    def showAnns(self, anns):\n        """"""\n        Display the specified annotations.\n        :param anns (array of object): annotations to display\n        :return: None\n        """"""\n        if len(anns) == 0:\n            return 0\n        if \'segmentation\' in anns[0] or \'keypoints\' in anns[0]:\n            datasetType = \'instances\'\n        elif \'caption\' in anns[0]:\n            datasetType = \'captions\'\n        else:\n            raise Exception(\'datasetType not supported\')\n        if datasetType == \'instances\':\n            ax = plt.gca()\n            ax.set_autoscale_on(False)\n            polygons = []\n            color = []\n            for ann in anns:\n                c = (np.random.random((1, 3))*0.6+0.4).tolist()[0]\n                if \'segmentation\' in ann:\n                    if type(ann[\'segmentation\']) == list:\n                        # polygon\n                        for seg in ann[\'segmentation\']:\n                            poly = np.array(seg).reshape((int(len(seg)/2), 2))\n                            polygons.append(Polygon(poly))\n                            color.append(c)\n                    else:\n                        # mask\n                        t = self.imgs[ann[\'image_id\']]\n                        if type(ann[\'segmentation\'][\'counts\']) == list:\n                            rle = maskUtils.frPyObjects([ann[\'segmentation\']], t[\'height\'], t[\'width\'])\n                        else:\n                            rle = [ann[\'segmentation\']]\n                        m = maskUtils.decode(rle)\n                        img = np.ones( (m.shape[0], m.shape[1], 3) )\n                        if ann[\'iscrowd\'] == 1:\n                            color_mask = np.array([2.0,166.0,101.0])/255\n                        if ann[\'iscrowd\'] == 0:\n                            color_mask = np.random.random((1, 3)).tolist()[0]\n                        for i in range(3):\n                            img[:,:,i] = color_mask[i]\n                        ax.imshow(np.dstack( (img, m*0.5) ))\n                if \'keypoints\' in ann and type(ann[\'keypoints\']) == list:\n                    # turn skeleton into zero-based index\n                    sks = np.array(self.loadCats(ann[\'category_id\'])[0][\'skeleton\'])-1\n                    kp = np.array(ann[\'keypoints\'])\n                    x = kp[0::3]\n                    y = kp[1::3]\n                    v = kp[2::3]\n                    for sk in sks:\n                        if np.all(v[sk]>0):\n                            plt.plot(x[sk],y[sk], linewidth=3, color=c)\n                    plt.plot(x[v>0], y[v>0],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=\'k\',markeredgewidth=2)\n                    plt.plot(x[v>1], y[v>1],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=c, markeredgewidth=2)\n            p = PatchCollection(polygons, facecolor=color, linewidths=0, alpha=0.4)\n            ax.add_collection(p)\n            p = PatchCollection(polygons, facecolor=\'none\', edgecolors=color, linewidths=2)\n            ax.add_collection(p)\n        elif datasetType == \'captions\':\n            for ann in anns:\n                print(ann[\'caption\'])\n\n    def loadRes(self, resFile):\n        """"""\n        Load result file and return a result api object.\n        :param   resFile (str)     : file name of result file\n        :return: res (obj)         : result api object\n        """"""\n        res = COCO()\n        res.dataset[\'images\'] = [img for img in self.dataset[\'images\']]\n\n        print(\'Loading and preparing results...\')\n        tic = time.time()\n        if type(resFile) == str or type(resFile) == unicode:\n            anns = json.load(open(resFile))\n        elif type(resFile) == np.ndarray:\n            anns = self.loadNumpyAnnotations(resFile)\n        else:\n            anns = resFile\n        assert type(anns) == list, \'results in not an array of objects\'\n        annsImgIds = [ann[\'image_id\'] for ann in anns]\n        assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \\\n               \'Results do not correspond to current coco set\'\n        if \'caption\' in anns[0]:\n            imgIds = set([img[\'id\'] for img in res.dataset[\'images\']]) & set([ann[\'image_id\'] for ann in anns])\n            res.dataset[\'images\'] = [img for img in res.dataset[\'images\'] if img[\'id\'] in imgIds]\n            for id, ann in enumerate(anns):\n                ann[\'id\'] = id+1\n        elif \'bbox\' in anns[0] and not anns[0][\'bbox\'] == []:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                bb = ann[\'bbox\']\n                x1, x2, y1, y2 = [bb[0], bb[0]+bb[2], bb[1], bb[1]+bb[3]]\n                if not \'segmentation\' in ann:\n                    ann[\'segmentation\'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n                ann[\'area\'] = bb[2]*bb[3]\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'segmentation\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                # now only support compressed RLE format as segmentation results\n                ann[\'area\'] = maskUtils.area(ann[\'segmentation\'])\n                if not \'bbox\' in ann:\n                    ann[\'bbox\'] = maskUtils.toBbox(ann[\'segmentation\'])\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'keypoints\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                s = ann[\'keypoints\']\n                x = s[0::3]\n                y = s[1::3]\n                x0,x1,y0,y1 = np.min(x), np.max(x), np.min(y), np.max(y)\n                ann[\'area\'] = (x1-x0)*(y1-y0)\n                ann[\'id\'] = id + 1\n                ann[\'bbox\'] = [x0,y0,x1-x0,y1-y0]\n        print(\'DONE (t={:0.2f}s)\'.format(time.time()- tic))\n\n        res.dataset[\'annotations\'] = anns\n        res.createIndex()\n        return res\n\n    def download(self, tarDir = None, imgIds = [] ):\n        \'\'\'\n        Download COCO images from mscoco.org server.\n        :param tarDir (str): COCO results directory name\n               imgIds (list): images to be downloaded\n        :return:\n        \'\'\'\n        if tarDir is None:\n            print(\'Please specify target directory\')\n            return -1\n        if len(imgIds) == 0:\n            imgs = self.imgs.values()\n        else:\n            imgs = self.loadImgs(imgIds)\n        N = len(imgs)\n        if not os.path.exists(tarDir):\n            os.makedirs(tarDir)\n        for i, img in enumerate(imgs):\n            tic = time.time()\n            fname = os.path.join(tarDir, img[\'file_name\'])\n            if not os.path.exists(fname):\n                urlretrieve(img[\'coco_url\'], fname)\n            print(\'downloaded {}/{} images (t={:0.1f}s)\'.format(i, N, time.time()- tic))\n\n    def loadNumpyAnnotations(self, data):\n        """"""\n        Convert result data from a numpy array [Nx7] where each row contains {imageID,x1,y1,w,h,score,class}\n        :param  data (numpy.ndarray)\n        :return: annotations (python nested list)\n        """"""\n        print(\'Converting ndarray to lists...\')\n        assert(type(data) == np.ndarray)\n        print(data.shape)\n        assert(data.shape[1] == 7)\n        N = data.shape[0]\n        ann = []\n        for i in range(N):\n            if i % 1000000 == 0:\n                print(\'{}/{}\'.format(i,N))\n            ann += [{\n                \'image_id\'  : int(data[i, 0]),\n                \'bbox\'  : [ data[i, 1], data[i, 2], data[i, 3], data[i, 4] ],\n                \'score\' : data[i, 5],\n                \'category_id\': int(data[i, 6]),\n                }]\n        return ann\n\n    def annToRLE(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE to RLE.\n        :return: binary mask (numpy 2D array)\n        """"""\n        t = self.imgs[ann[\'image_id\']]\n        h, w = t[\'height\'], t[\'width\']\n        segm = ann[\'segmentation\']\n        if type(segm) == list:\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            rles = maskUtils.frPyObjects(segm, h, w)\n            rle = maskUtils.merge(rles)\n        elif type(segm[\'counts\']) == list:\n            # uncompressed RLE\n            rle = maskUtils.frPyObjects(segm, h, w)\n        else:\n            # rle\n            rle = ann[\'segmentation\']\n        return rle\n\n    def annToMask(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n        :return: binary mask (numpy 2D array)\n        """"""\n        rle = self.annToRLE(ann)\n        m = maskUtils.decode(rle)\n        return m'"
training/docker/cocoapi/PythonAPI/pycocotools/cocoeval.py,0,"b'__author__ = \'tsungyi\'\n\nimport numpy as np\nimport datetime\nimport time\nfrom collections import defaultdict\nfrom . import mask as maskUtils\nimport copy\n\nclass COCOeval:\n    # Interface for evaluating detection on the Microsoft COCO dataset.\n    #\n    # The usage for CocoEval is as follows:\n    #  cocoGt=..., cocoDt=...       # load dataset and results\n    #  E = CocoEval(cocoGt,cocoDt); # initialize CocoEval object\n    #  E.params.recThrs = ...;      # set parameters as desired\n    #  E.evaluate();                # run per image evaluation\n    #  E.accumulate();              # accumulate per image results\n    #  E.summarize();               # display summary metrics of results\n    # For example usage see evalDemo.m and http://mscoco.org/.\n    #\n    # The evaluation parameters are as follows (defaults in brackets):\n    #  imgIds     - [all] N img ids to use for evaluation\n    #  catIds     - [all] K cat ids to use for evaluation\n    #  iouThrs    - [.5:.05:.95] T=10 IoU thresholds for evaluation\n    #  recThrs    - [0:.01:1] R=101 recall thresholds for evaluation\n    #  areaRng    - [...] A=4 object area ranges for evaluation\n    #  maxDets    - [1 10 100] M=3 thresholds on max detections per image\n    #  iouType    - [\'segm\'] set iouType to \'segm\', \'bbox\' or \'keypoints\'\n    #  iouType replaced the now DEPRECATED useSegm parameter.\n    #  useCats    - [1] if true use category labels for evaluation\n    # Note: if useCats=0 category labels are ignored as in proposal scoring.\n    # Note: multiple areaRngs [Ax2] and maxDets [Mx1] can be specified.\n    #\n    # evaluate(): evaluates detections on every image and every category and\n    # concats the results into the ""evalImgs"" with fields:\n    #  dtIds      - [1xD] id for each of the D detections (dt)\n    #  gtIds      - [1xG] id for each of the G ground truths (gt)\n    #  dtMatches  - [TxD] matching gt id at each IoU or 0\n    #  gtMatches  - [TxG] matching dt id at each IoU or 0\n    #  dtScores   - [1xD] confidence of each dt\n    #  gtIgnore   - [1xG] ignore flag for each gt\n    #  dtIgnore   - [TxD] ignore flag for each dt at each IoU\n    #\n    # accumulate(): accumulates the per-image, per-category evaluation\n    # results in ""evalImgs"" into the dictionary ""eval"" with fields:\n    #  params     - parameters used for evaluation\n    #  date       - date evaluation was performed\n    #  counts     - [T,R,K,A,M] parameter dimensions (see above)\n    #  precision  - [TxRxKxAxM] precision for every evaluation setting\n    #  recall     - [TxKxAxM] max recall for every evaluation setting\n    # Note: precision and recall==-1 for settings with no gt objects.\n    #\n    # See also coco, mask, pycocoDemo, pycocoEvalDemo\n    #\n    # Microsoft COCO Toolbox.      version 2.0\n    # Data, paper, and tutorials available at:  http://mscoco.org/\n    # Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n    # Licensed under the Simplified BSD License [see coco/license.txt]\n    def __init__(self, cocoGt=None, cocoDt=None, iouType=\'segm\'):\n        \'\'\'\n        Initialize CocoEval using coco APIs for gt and dt\n        :param cocoGt: coco object with ground truth annotations\n        :param cocoDt: coco object with detection results\n        :return: None\n        \'\'\'\n        if not iouType:\n            print(\'iouType not specified. use default iouType segm\')\n        self.cocoGt   = cocoGt              # ground truth COCO API\n        self.cocoDt   = cocoDt              # detections COCO API\n        self.params   = {}                  # evaluation parameters\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results [KxAxI] elements\n        self.eval     = {}                  # accumulated evaluation results\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        self.params = Params(iouType=iouType) # parameters\n        self._paramsEval = {}               # parameters for evaluation\n        self.stats = []                     # result summarization\n        self.ious = {}                      # ious between all gts and dts\n        if not cocoGt is None:\n            self.params.imgIds = sorted(cocoGt.getImgIds())\n            self.params.catIds = sorted(cocoGt.getCatIds())\n\n\n    def _prepare(self):\n        \'\'\'\n        Prepare ._gts and ._dts for evaluation based on params\n        :return: None\n        \'\'\'\n        def _toMask(anns, coco):\n            # modify ann[\'segmentation\'] by reference\n            for ann in anns:\n                rle = coco.annToRLE(ann)\n                ann[\'segmentation\'] = rle\n        p = self.params\n        if p.useCats:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n        else:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds))\n\n        # convert ground truth to mask if iouType == \'segm\'\n        if p.iouType == \'segm\':\n            _toMask(gts, self.cocoGt)\n            _toMask(dts, self.cocoDt)\n        # set ignore flag\n        for gt in gts:\n            gt[\'ignore\'] = gt[\'ignore\'] if \'ignore\' in gt else 0\n            gt[\'ignore\'] = \'iscrowd\' in gt and gt[\'iscrowd\']\n            if p.iouType == \'keypoints\':\n                gt[\'ignore\'] = (gt[\'num_keypoints\'] == 0) or gt[\'ignore\']\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        for gt in gts:\n            self._gts[gt[\'image_id\'], gt[\'category_id\']].append(gt)\n        for dt in dts:\n            self._dts[dt[\'image_id\'], dt[\'category_id\']].append(dt)\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results\n        self.eval     = {}                  # accumulated evaluation results\n\n    def evaluate(self):\n        \'\'\'\n        Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n        :return: None\n        \'\'\'\n        tic = time.time()\n        print(\'Running per image evaluation...\')\n        p = self.params\n        # add backward compatibility if useSegm is specified in params\n        if not p.useSegm is None:\n            p.iouType = \'segm\' if p.useSegm == 1 else \'bbox\'\n            print(\'useSegm (deprecated) is not None. Running {} evaluation\'.format(p.iouType))\n        print(\'Evaluate annotation type *{}*\'.format(p.iouType))\n        p.imgIds = list(np.unique(p.imgIds))\n        if p.useCats:\n            p.catIds = list(np.unique(p.catIds))\n        p.maxDets = sorted(p.maxDets)\n        self.params=p\n\n        self._prepare()\n        # loop through images, area range, max detection number\n        catIds = p.catIds if p.useCats else [-1]\n\n        if p.iouType == \'segm\' or p.iouType == \'bbox\':\n            computeIoU = self.computeIoU\n        elif p.iouType == \'keypoints\':\n            computeIoU = self.computeOks\n        self.ious = {(imgId, catId): computeIoU(imgId, catId) \\\n                        for imgId in p.imgIds\n                        for catId in catIds}\n\n        evaluateImg = self.evaluateImg\n        maxDet = p.maxDets[-1]\n        self.evalImgs = [evaluateImg(imgId, catId, areaRng, maxDet)\n                 for catId in catIds\n                 for areaRng in p.areaRng\n                 for imgId in p.imgIds\n             ]\n        self._paramsEval = copy.deepcopy(self.params)\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format(toc-tic))\n\n    def computeIoU(self, imgId, catId):\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return []\n        inds = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in inds]\n        if len(dt) > p.maxDets[-1]:\n            dt=dt[0:p.maxDets[-1]]\n\n        if p.iouType == \'segm\':\n            g = [g[\'segmentation\'] for g in gt]\n            d = [d[\'segmentation\'] for d in dt]\n        elif p.iouType == \'bbox\':\n            g = [g[\'bbox\'] for g in gt]\n            d = [d[\'bbox\'] for d in dt]\n        else:\n            raise Exception(\'unknown iouType for iou computation\')\n\n        # compute iou between each dt and gt region\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        ious = maskUtils.iou(d,g,iscrowd)\n        return ious\n\n    def computeOks(self, imgId, catId):\n        p = self.params\n        # dimention here should be Nxm\n        gts = self._gts[imgId, catId]\n        dts = self._dts[imgId, catId]\n        inds = np.argsort([-d[\'score\'] for d in dts], kind=\'mergesort\')\n        dts = [dts[i] for i in inds]\n        if len(dts) > p.maxDets[-1]:\n            dts = dts[0:p.maxDets[-1]]\n        # if len(gts) == 0 and len(dts) == 0:\n        if len(gts) == 0 or len(dts) == 0:\n            return []\n        ious = np.zeros((len(dts), len(gts)))\n        sigmas = np.array([.26, .25, .25, .35, .35, .79, .79, .72, .72, .62,.62, 1.07, 1.07, .87, .87, .89, .89])/10.0\n        vars = (sigmas * 2)**2\n        k = len(sigmas)\n        # compute oks between each detection and ground truth object\n        for j, gt in enumerate(gts):\n            # create bounds for ignore regions(double the gt bbox)\n            g = np.array(gt[\'keypoints\'])\n            xg = g[0::3]; yg = g[1::3]; vg = g[2::3]\n            k1 = np.count_nonzero(vg > 0)\n            bb = gt[\'bbox\']\n            x0 = bb[0] - bb[2]; x1 = bb[0] + bb[2] * 2\n            y0 = bb[1] - bb[3]; y1 = bb[1] + bb[3] * 2\n            for i, dt in enumerate(dts):\n                d = np.array(dt[\'keypoints\'])\n                xd = d[0::3]; yd = d[1::3]\n                if k1>0:\n                    # measure the per-keypoint distance if keypoints visible\n                    dx = xd - xg\n                    dy = yd - yg\n                else:\n                    # measure minimum distance to keypoints in (x0,y0) & (x1,y1)\n                    z = np.zeros((k))\n                    dx = np.max((z, x0-xd),axis=0)+np.max((z, xd-x1),axis=0)\n                    dy = np.max((z, y0-yd),axis=0)+np.max((z, yd-y1),axis=0)\n                e = (dx**2 + dy**2) / vars / (gt[\'area\']+np.spacing(1)) / 2\n                if k1 > 0:\n                    e=e[vg > 0]\n                ious[i, j] = np.sum(np.exp(-e)) / e.shape[0]\n        return ious\n\n    def evaluateImg(self, imgId, catId, aRng, maxDet):\n        \'\'\'\n        perform evaluation for single category and image\n        :return: dict (single image results)\n        \'\'\'\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return None\n\n        for g in gt:\n            if g[\'ignore\'] or (g[\'area\']<aRng[0] or g[\'area\']>aRng[1]):\n                g[\'_ignore\'] = 1\n            else:\n                g[\'_ignore\'] = 0\n\n        # sort dt highest score first, sort gt ignore last\n        gtind = np.argsort([g[\'_ignore\'] for g in gt], kind=\'mergesort\')\n        gt = [gt[i] for i in gtind]\n        dtind = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in dtind[0:maxDet]]\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        # load computed ious\n        ious = self.ious[imgId, catId][:, gtind] if len(self.ious[imgId, catId]) > 0 else self.ious[imgId, catId]\n\n        T = len(p.iouThrs)\n        G = len(gt)\n        D = len(dt)\n        gtm  = np.zeros((T,G))\n        dtm  = np.zeros((T,D))\n        gtIg = np.array([g[\'_ignore\'] for g in gt])\n        dtIg = np.zeros((T,D))\n        if not len(ious)==0:\n            for tind, t in enumerate(p.iouThrs):\n                for dind, d in enumerate(dt):\n                    # information about best match so far (m=-1 -> unmatched)\n                    iou = min([t,1-1e-10])\n                    m   = -1\n                    for gind, g in enumerate(gt):\n                        # if this gt already matched, and not a crowd, continue\n                        if gtm[tind,gind]>0 and not iscrowd[gind]:\n                            continue\n                        # if dt matched to reg gt, and on ignore gt, stop\n                        if m>-1 and gtIg[m]==0 and gtIg[gind]==1:\n                            break\n                        # continue to next gt unless better match made\n                        if ious[dind,gind] < iou:\n                            continue\n                        # if match successful and best so far, store appropriately\n                        iou=ious[dind,gind]\n                        m=gind\n                    # if match made store id of match for both dt and gt\n                    if m ==-1:\n                        continue\n                    dtIg[tind,dind] = gtIg[m]\n                    dtm[tind,dind]  = gt[m][\'id\']\n                    gtm[tind,m]     = d[\'id\']\n        # set unmatched detections outside of area range to ignore\n        a = np.array([d[\'area\']<aRng[0] or d[\'area\']>aRng[1] for d in dt]).reshape((1, len(dt)))\n        dtIg = np.logical_or(dtIg, np.logical_and(dtm==0, np.repeat(a,T,0)))\n        # store results for given image and category\n        return {\n                \'image_id\':     imgId,\n                \'category_id\':  catId,\n                \'aRng\':         aRng,\n                \'maxDet\':       maxDet,\n                \'dtIds\':        [d[\'id\'] for d in dt],\n                \'gtIds\':        [g[\'id\'] for g in gt],\n                \'dtMatches\':    dtm,\n                \'gtMatches\':    gtm,\n                \'dtScores\':     [d[\'score\'] for d in dt],\n                \'gtIgnore\':     gtIg,\n                \'dtIgnore\':     dtIg,\n            }\n\n    def accumulate(self, p = None):\n        \'\'\'\n        Accumulate per image evaluation results and store the result in self.eval\n        :param p: input params for evaluation\n        :return: None\n        \'\'\'\n        print(\'Accumulating evaluation results...\')\n        tic = time.time()\n        if not self.evalImgs:\n            print(\'Please run evaluate() first\')\n        # allows input customized parameters\n        if p is None:\n            p = self.params\n        p.catIds = p.catIds if p.useCats == 1 else [-1]\n        T           = len(p.iouThrs)\n        R           = len(p.recThrs)\n        K           = len(p.catIds) if p.useCats else 1\n        A           = len(p.areaRng)\n        M           = len(p.maxDets)\n        precision   = -np.ones((T,R,K,A,M)) # -1 for the precision of absent categories\n        recall      = -np.ones((T,K,A,M))\n        scores      = -np.ones((T,R,K,A,M))\n\n        # create dictionary for future indexing\n        _pe = self._paramsEval\n        catIds = _pe.catIds if _pe.useCats else [-1]\n        setK = set(catIds)\n        setA = set(map(tuple, _pe.areaRng))\n        setM = set(_pe.maxDets)\n        setI = set(_pe.imgIds)\n        # get inds to evaluate\n        k_list = [n for n, k in enumerate(p.catIds)  if k in setK]\n        m_list = [m for n, m in enumerate(p.maxDets) if m in setM]\n        a_list = [n for n, a in enumerate(map(lambda x: tuple(x), p.areaRng)) if a in setA]\n        i_list = [n for n, i in enumerate(p.imgIds)  if i in setI]\n        I0 = len(_pe.imgIds)\n        A0 = len(_pe.areaRng)\n        # retrieve E at each category, area range, and max number of detections\n        for k, k0 in enumerate(k_list):\n            Nk = k0*A0*I0\n            for a, a0 in enumerate(a_list):\n                Na = a0*I0\n                for m, maxDet in enumerate(m_list):\n                    E = [self.evalImgs[Nk + Na + i] for i in i_list]\n                    E = [e for e in E if not e is None]\n                    if len(E) == 0:\n                        continue\n                    dtScores = np.concatenate([e[\'dtScores\'][0:maxDet] for e in E])\n\n                    # different sorting method generates slightly different results.\n                    # mergesort is used to be consistent as Matlab implementation.\n                    inds = np.argsort(-dtScores, kind=\'mergesort\')\n                    dtScoresSorted = dtScores[inds]\n\n                    dtm  = np.concatenate([e[\'dtMatches\'][:,0:maxDet] for e in E], axis=1)[:,inds]\n                    dtIg = np.concatenate([e[\'dtIgnore\'][:,0:maxDet]  for e in E], axis=1)[:,inds]\n                    gtIg = np.concatenate([e[\'gtIgnore\'] for e in E])\n                    npig = np.count_nonzero(gtIg==0 )\n                    if npig == 0:\n                        continue\n                    tps = np.logical_and(               dtm,  np.logical_not(dtIg) )\n                    fps = np.logical_and(np.logical_not(dtm), np.logical_not(dtIg) )\n\n                    tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\n                    fp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)\n                    for t, (tp, fp) in enumerate(zip(tp_sum, fp_sum)):\n                        tp = np.array(tp)\n                        fp = np.array(fp)\n                        nd = len(tp)\n                        rc = tp / npig\n                        pr = tp / (fp+tp+np.spacing(1))\n                        q  = np.zeros((R,))\n                        ss = np.zeros((R,))\n\n                        if nd:\n                            recall[t,k,a,m] = rc[-1]\n                        else:\n                            recall[t,k,a,m] = 0\n\n                        # numpy is slow without cython optimization for accessing elements\n                        # use python array gets significant speed improvement\n                        pr = pr.tolist(); q = q.tolist()\n\n                        for i in range(nd-1, 0, -1):\n                            if pr[i] > pr[i-1]:\n                                pr[i-1] = pr[i]\n\n                        inds = np.searchsorted(rc, p.recThrs, side=\'left\')\n                        try:\n                            for ri, pi in enumerate(inds):\n                                q[ri] = pr[pi]\n                                ss[ri] = dtScoresSorted[pi]\n                        except:\n                            pass\n                        precision[t,:,k,a,m] = np.array(q)\n                        scores[t,:,k,a,m] = np.array(ss)\n        self.eval = {\n            \'params\': p,\n            \'counts\': [T, R, K, A, M],\n            \'date\': datetime.datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\'),\n            \'precision\': precision,\n            \'recall\':   recall,\n            \'scores\': scores,\n        }\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format( toc-tic))\n\n    def summarize(self):\n        \'\'\'\n        Compute and display summary metrics for evaluation results.\n        Note this functin can *only* be applied on the default parameter setting\n        \'\'\'\n        def _summarize( ap=1, iouThr=None, areaRng=\'all\', maxDets=100 ):\n            p = self.params\n            iStr = \' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}\'\n            titleStr = \'Average Precision\' if ap == 1 else \'Average Recall\'\n            typeStr = \'(AP)\' if ap==1 else \'(AR)\'\n            iouStr = \'{:0.2f}:{:0.2f}\'.format(p.iouThrs[0], p.iouThrs[-1]) \\\n                if iouThr is None else \'{:0.2f}\'.format(iouThr)\n\n            aind = [i for i, aRng in enumerate(p.areaRngLbl) if aRng == areaRng]\n            mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]\n            if ap == 1:\n                # dimension of precision: [TxRxKxAxM]\n                s = self.eval[\'precision\']\n                # IoU\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,:,aind,mind]\n            else:\n                # dimension of recall: [TxKxAxM]\n                s = self.eval[\'recall\']\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,aind,mind]\n            if len(s[s>-1])==0:\n                mean_s = -1\n            else:\n                mean_s = np.mean(s[s>-1])\n            print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n            return mean_s\n        def _summarizeDets():\n            stats = np.zeros((12,))\n            stats[0] = _summarize(1)\n            stats[1] = _summarize(1, iouThr=.5, maxDets=self.params.maxDets[2])\n            stats[2] = _summarize(1, iouThr=.75, maxDets=self.params.maxDets[2])\n            stats[3] = _summarize(1, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[4] = _summarize(1, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[5] = _summarize(1, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            stats[6] = _summarize(0, maxDets=self.params.maxDets[0])\n            stats[7] = _summarize(0, maxDets=self.params.maxDets[1])\n            stats[8] = _summarize(0, maxDets=self.params.maxDets[2])\n            stats[9] = _summarize(0, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[10] = _summarize(0, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[11] = _summarize(0, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            return stats\n        def _summarizeKps():\n            stats = np.zeros((10,))\n            stats[0] = _summarize(1, maxDets=20)\n            stats[1] = _summarize(1, maxDets=20, iouThr=.5)\n            stats[2] = _summarize(1, maxDets=20, iouThr=.75)\n            stats[3] = _summarize(1, maxDets=20, areaRng=\'medium\')\n            stats[4] = _summarize(1, maxDets=20, areaRng=\'large\')\n            stats[5] = _summarize(0, maxDets=20)\n            stats[6] = _summarize(0, maxDets=20, iouThr=.5)\n            stats[7] = _summarize(0, maxDets=20, iouThr=.75)\n            stats[8] = _summarize(0, maxDets=20, areaRng=\'medium\')\n            stats[9] = _summarize(0, maxDets=20, areaRng=\'large\')\n            return stats\n        if not self.eval:\n            raise Exception(\'Please run accumulate() first\')\n        iouType = self.params.iouType\n        if iouType == \'segm\' or iouType == \'bbox\':\n            summarize = _summarizeDets\n        elif iouType == \'keypoints\':\n            summarize = _summarizeKps\n        self.stats = summarize()\n\n    def __str__(self):\n        self.summarize()\n\nclass Params:\n    \'\'\'\n    Params for coco evaluation api\n    \'\'\'\n    def setDetParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [1, 10, 100]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [0 ** 2, 32 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'small\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def setKpParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [20]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def __init__(self, iouType=\'segm\'):\n        if iouType == \'segm\' or iouType == \'bbox\':\n            self.setDetParams()\n        elif iouType == \'keypoints\':\n            self.setKpParams()\n        else:\n            raise Exception(\'iouType not supported\')\n        self.iouType = iouType\n        # useSegm is deprecated\n        self.useSegm = None'"
training/docker/cocoapi/PythonAPI/pycocotools/mask.py,0,"b'__author__ = \'tsungyi\'\n\nimport pycocotools._mask as _mask\n\n# Interface for manipulating masks stored in RLE format.\n#\n# RLE is a simple yet efficient format for storing binary masks. RLE\n# first divides a vector (or vectorized image) into a series of piecewise\n# constant regions and then for each piece simply stores the length of\n# that piece. For example, given M=[0 0 1 1 1 0 1] the RLE counts would\n# be [2 3 1 1], or for M=[1 1 1 1 1 1 0] the counts would be [0 6 1]\n# (note that the odd counts are always the numbers of zeros). Instead of\n# storing the counts directly, additional compression is achieved with a\n# variable bitrate representation based on a common scheme called LEB128.\n#\n# Compression is greatest given large piecewise constant regions.\n# Specifically, the size of the RLE is proportional to the number of\n# *boundaries* in M (or for an image the number of boundaries in the y\n# direction). Assuming fairly simple shapes, the RLE representation is\n# O(sqrt(n)) where n is number of pixels in the object. Hence space usage\n# is substantially lower, especially for large simple objects (large n).\n#\n# Many common operations on masks can be computed directly using the RLE\n# (without need for decoding). This includes computations such as area,\n# union, intersection, etc. All of these operations are linear in the\n# size of the RLE, in other words they are O(sqrt(n)) where n is the area\n# of the object. Computing these operations on the original mask is O(n).\n# Thus, using the RLE can result in substantial computational savings.\n#\n# The following API functions are defined:\n#  encode         - Encode binary masks using RLE.\n#  decode         - Decode binary masks encoded via RLE.\n#  merge          - Compute union or intersection of encoded masks.\n#  iou            - Compute intersection over union between masks.\n#  area           - Compute area of encoded masks.\n#  toBbox         - Get bounding boxes surrounding encoded masks.\n#  frPyObjects    - Convert polygon, bbox, and uncompressed RLE to encoded RLE mask.\n#\n# Usage:\n#  Rs     = encode( masks )\n#  masks  = decode( Rs )\n#  R      = merge( Rs, intersect=false )\n#  o      = iou( dt, gt, iscrowd )\n#  a      = area( Rs )\n#  bbs    = toBbox( Rs )\n#  Rs     = frPyObjects( [pyObjects], h, w )\n#\n# In the API the following formats are used:\n#  Rs      - [dict] Run-length encoding of binary masks\n#  R       - dict Run-length encoding of binary mask\n#  masks   - [hxwxn] Binary mask(s) (must have type np.ndarray(dtype=uint8) in column-major order)\n#  iscrowd - [nx1] list of np.ndarray. 1 indicates corresponding gt image has crowd region to ignore\n#  bbs     - [nx4] Bounding box(es) stored as [x y w h]\n#  poly    - Polygon stored as [[x1 y1 x2 y2...],[x1 y1 ...],...] (2D list)\n#  dt,gt   - May be either bounding boxes or encoded masks\n# Both poly and bbs are 0-indexed (bbox=[0 0 1 1] encloses first pixel).\n#\n# Finally, a note about the intersection over union (iou) computation.\n# The standard iou of a ground truth (gt) and detected (dt) object is\n#  iou(gt,dt) = area(intersect(gt,dt)) / area(union(gt,dt))\n# For ""crowd"" regions, we use a modified criteria. If a gt object is\n# marked as ""iscrowd"", we allow a dt to match any subregion of the gt.\n# Choosing gt\' in the crowd gt that best matches the dt can be done using\n# gt\'=intersect(dt,gt). Since by definition union(gt\',dt)=dt, computing\n#  iou(gt,dt,iscrowd) = iou(gt\',dt) = area(intersect(gt,dt)) / area(dt)\n# For crowd gt regions we use this modified criteria above for the iou.\n#\n# To compile run ""python setup.py build_ext --inplace""\n# Please do not contact us for help with compiling.\n#\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n# Licensed under the Simplified BSD License [see coco/license.txt]\n\niou         = _mask.iou\nmerge       = _mask.merge\nfrPyObjects = _mask.frPyObjects\n\ndef encode(bimask):\n    if len(bimask.shape) == 3:\n        return _mask.encode(bimask)\n    elif len(bimask.shape) == 2:\n        h, w = bimask.shape\n        return _mask.encode(bimask.reshape((h, w, 1), order=\'F\'))[0]\n\ndef decode(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.decode(rleObjs)\n    else:\n        return _mask.decode([rleObjs])[:,:,0]\n\ndef area(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.area(rleObjs)\n    else:\n        return _mask.area([rleObjs])[0]\n\ndef toBbox(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.toBbox(rleObjs)\n    else:\n        return _mask.toBbox([rleObjs])[0]'"
