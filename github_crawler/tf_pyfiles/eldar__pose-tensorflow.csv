file_path,api_count,code
eval_mscoco.py,0,"b'import os\nimport sys\n\nimport argparse\nimport json\n\nfrom util.config import load_config\n\ndir_path = os.path.dirname(os.path.realpath(__file__))\nsys.path.append(dir_path + \'/lib/coco/PythonAPI\')\nfrom pycocotools.coco import COCO as COCO\nfrom pycocotools.cocoeval import COCOeval\n\ndef apply_threhsold(inFile, threshold):\n    outFile = inFile[:-5] + \'-\' + str(threshold) + \'.json\'\n\n    with open(inFile) as data_file:\n        data = json.load(data_file)\n\n    for person_id in range(len(data)):\n        keypoints = data[person_id][""keypoints""]\n        keypoints = [int(keypoints[i] > threshold) if i % 3 == 2 else int(keypoints[i]) for i in range(len(keypoints))]\n        data[person_id][""keypoints""] = keypoints\n\n    with open(outFile, \'w\') as outfile:\n        json.dump(data, outfile)\n\n    return outFile\n\n\ndef eval_init(cfg):\n    dataset = cfg.dataset\n    dataset_phase = cfg.dataset_phase\n    dataset_ann = cfg.dataset_ann\n    threshold = 0\n\n    # initialize cocoGT api\n    annFile = \'%s/annotations/%s_%s.json\' % (dataset, dataset_ann, dataset_phase)\n    cocoGT = COCO(annFile)\n\n    # initialize cocoPred api\n    inFile = ""predictions_with_segm.json""\n    predFile = apply_threhsold(inFile, threshold)\n    cocoPred = cocoGT.loadRes(predFile)\n\n    return cocoGT, cocoPred\n\n\ndef eval_mscoco_with_segm(cocoGT, cocoPred):\n    # running evaluation\n    cocoEval = COCOeval(cocoGT, cocoPred, ""keypoints"")\n    cocoEval.evaluate()\n    cocoEval.accumulate()\n    cocoEval.summarize()\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    args, unparsed = parser.parse_known_args()\n\n    cfg = load_config()\n\n    cocoGT, cocoPred = eval_init(cfg)\n    eval_mscoco_with_segm(cocoGT, cocoPred)'"
eval_pck.py,0,"b'import argparse\n\nimport numpy as np\nfrom numpy import array as arr\nimport scipy.io as sio\n\nfrom util.config import load_config\nfrom dataset.factory import create as dataset_create\n\n\ndef enclosing_rect(points):\n    xs = points[:, 0]\n    ys = points[:, 1]\n    return np.array([np.amin(xs), np.amin(ys), np.amax(xs), np.amax(ys)])\n\n\ndef rect_size(rect):\n    return np.array([rect[2]-rect[0], rect[3]-rect[1]])\n\n\ndef print_results(pck, cfg):\n    str = """"\n    for heading in (cfg.all_joints_names + [""total""]):\n        str += "" & "" + heading\n    print(str)\n\n    str = """"\n    all_joint_ids = cfg.all_joints + [np.arange(cfg.num_joints)]\n    for j_ids in all_joint_ids:\n        j_ids_np = arr(j_ids)\n        pck_av = np.mean(pck[j_ids_np])\n        str += "" & {0:.1f}"".format(pck_av)\n    print(str)\n\n\ndef eval_pck(cfg):\n    dataset = dataset_create(cfg)\n    filename = \'predictions.mat\'\n    pred = sio.loadmat(filename)\n\n    joints = pred[\'joints\']\n    pck_ratio_thresh = cfg.pck_threshold\n\n    num_joints = cfg.num_joints\n    num_images = joints.shape[1]\n\n    pred_joints = np.zeros((num_images, num_joints, 2))\n    gt_joints = np.zeros((num_images, num_joints, 2))\n    pck_thresh = np.zeros((num_images, 1))\n    gt_present_joints = np.zeros((num_images, num_joints))\n\n    for k in range(num_images):\n        pred = joints[0,k]\n        gt = dataset.data[k].joints[0]\n        if gt.shape[0] == 0:\n            continue\n        gt_joint_ids = gt[:, 0].astype(\'int32\')\n        rect = enclosing_rect(gt[:,1:3])\n        pck_thresh[k] = pck_ratio_thresh*np.amax(rect_size(rect))\n\n        gt_present_joints[k, gt_joint_ids] = 1\n        gt_joints[k, gt_joint_ids, :] = gt[:,1:3]\n        pred_joints[k, :, :] = pred[:,0:2]\n\n    dists = np.sqrt(np.sum((pred_joints - gt_joints)**2, axis=2))\n    correct = dists <= pck_thresh\n\n    num_all = np.sum(gt_present_joints, axis=0)\n\n    num_correct = np.zeros((num_joints, ))\n    for j_id in range(num_joints):\n        num_correct[j_id] = np.sum(correct[gt_present_joints[:,j_id] == 1, j_id], axis=0)\n\n    pck = num_correct/num_all*100.0\n\n    print_results(pck, cfg)\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    args, unparsed = parser.parse_known_args()\n\n    cfg = load_config()\n\n    eval_pck(cfg)'"
pairwise_stats.py,0,"b'import numpy as np\n\nimport scipy.io\n\nfrom util.config import load_config\nfrom dataset.factory import create as create_dataset\nfrom dataset.pose_dataset import Batch\n\n\ndef remap_keys(mapping):\n    return [{\'key\': k, \'value\': v} for k, v in mapping.items()]\n\n\ndef save_stats(stats, cfg):\n    mat_stats = {}\n    mat_stats[""graph""] = []\n    mat_stats[""means""] = []\n    mat_stats[""std_devs""] = []\n    for start in range(cfg.num_joints):\n        for end in range(cfg.num_joints):\n            if start != end:\n                joint_pair = (start, end)\n                mat_stats[""graph""].append([start, end])\n                mat_stats[""means""].append(stats[joint_pair][""mean""])\n                mat_stats[""std_devs""].append(stats[joint_pair][""std""])\n    print(mat_stats)\n    scipy.io.savemat(cfg.pairwise_stats_fn, mat_stats)\n\n\n# Compute pairwise statistics at reference scale\ndef pairwise_stats():\n    cfg = load_config()\n    dataset = create_dataset(cfg)\n    dataset.set_shuffle(True)\n    dataset.set_pairwise_stats_collect(True)\n\n    num_images = dataset.num_images\n    all_pairwise_differences = {}\n\n    if cfg.mirror:\n        num_images *= 2\n\n    for k in range(num_images):\n        print(\'processing image {}/{}\'.format(k, num_images-1))\n\n        batch = dataset.next_batch()\n        batch_stats = batch[Batch.data_item].pairwise_stats\n        for joint_pair in batch_stats:\n            if joint_pair not in all_pairwise_differences:\n                all_pairwise_differences[joint_pair] = []\n            all_pairwise_differences[joint_pair] += batch_stats[joint_pair]\n\n    stats = {}\n    for joint_pair in all_pairwise_differences:\n        stats[joint_pair] = {}\n        stats[joint_pair][""mean""] = np.mean(all_pairwise_differences[joint_pair], axis=0)\n        stats[joint_pair][""std""] = np.std(all_pairwise_differences[joint_pair], axis=0)\n\n    save_stats(stats, cfg)\n\n\nif __name__ == \'__main__\':\n    pairwise_stats()'"
test.py,0,"b""import argparse\nimport logging\nimport os\n\nimport numpy as np\nimport scipy.io\nimport scipy.ndimage\n\nfrom util.config import load_config\nfrom dataset.factory import create as create_dataset\nfrom dataset.pose_dataset import Batch\nfrom nnet.predict import setup_pose_prediction, extract_cnn_output, argmax_pose_predict\nfrom util import visualize\n\n\ndef test_net(visualise, cache_scoremaps):\n    logging.basicConfig(level=logging.INFO)\n\n    cfg = load_config()\n    dataset = create_dataset(cfg)\n    dataset.set_shuffle(False)\n    dataset.set_test_mode(True)\n\n    sess, inputs, outputs = setup_pose_prediction(cfg)\n\n    if cache_scoremaps:\n        out_dir = cfg.scoremap_dir\n        if not os.path.exists(out_dir):\n            os.makedirs(out_dir)\n\n    num_images = dataset.num_images\n    predictions = np.zeros((num_images,), dtype=np.object)\n\n    for k in range(num_images):\n        print('processing image {}/{}'.format(k, num_images-1))\n\n        batch = dataset.next_batch()\n\n        outputs_np = sess.run(outputs, feed_dict={inputs: batch[Batch.inputs]})\n\n        scmap, locref, pairwise_diff = extract_cnn_output(outputs_np, cfg)\n\n        pose = argmax_pose_predict(scmap, locref, cfg.stride)\n\n        pose_refscale = np.copy(pose)\n        pose_refscale[:, 0:2] /= cfg.global_scale\n        predictions[k] = pose_refscale\n\n        if visualise:\n            img = np.squeeze(batch[Batch.inputs]).astype('uint8')\n            visualize.show_heatmaps(cfg, img, scmap, pose)\n            visualize.waitforbuttonpress()\n\n        if cache_scoremaps:\n            base = os.path.basename(batch[Batch.data_item].im_path)\n            raw_name = os.path.splitext(base)[0]\n            out_fn = os.path.join(out_dir, raw_name + '.mat')\n            scipy.io.savemat(out_fn, mdict={'scoremaps': scmap.astype('float32')})\n\n            out_fn = os.path.join(out_dir, raw_name + '_locreg' + '.mat')\n            if cfg.location_refinement:\n                scipy.io.savemat(out_fn, mdict={'locreg_pred': locref.astype('float32')})\n\n    scipy.io.savemat('predictions.mat', mdict={'joints': predictions})\n\n    sess.close()\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--novis', default=False, action='store_true')\n    parser.add_argument('--cache', default=False, action='store_true')\n    args, unparsed = parser.parse_known_args()\n\n    test_net(not args.novis, args.cache)\n"""
test_multiperson.py,0,"b'import argparse\nimport logging\nimport os\n\nimport numpy as np\nimport scipy.io\nimport scipy.ndimage\nimport json\nfrom json import encoder\nencoder.FLOAT_REPR = lambda o: format(o, \'.2f\')\n\nfrom util.config import load_config\nfrom dataset.factory import create as create_dataset\nfrom dataset.pose_dataset import Batch\nfrom util.mscoco_util import pose_predict_with_gt_segm\nfrom nnet.predict import *\nfrom util import visualize\nfrom multiperson.detections import extract_detections\nfrom multiperson.predict import SpatialModel, eval_graph, get_person_conf_multicut\nfrom multiperson.visualize import PersonDraw, visualize_detections\n\nimport matplotlib.pyplot as plt\n\n\ndef test_net(visualise, cache_scoremaps, development):\n    logging.basicConfig(level=logging.INFO)\n\n    cfg = load_config()\n    dataset = create_dataset(cfg)\n    dataset.set_shuffle(False)\n\n    sm = SpatialModel(cfg)\n    sm.load()\n\n    draw_multi = PersonDraw()\n\n    from_cache = ""cached_scoremaps"" in cfg\n    if not from_cache:\n        sess, inputs, outputs = setup_pose_prediction(cfg)\n\n    if cache_scoremaps:\n        out_dir = cfg.scoremap_dir\n        if not os.path.exists(out_dir):\n            os.makedirs(out_dir)\n\n    pairwise_stats = dataset.pairwise_stats\n    num_images = dataset.num_images if not development else min(10, dataset.num_images)\n    coco_results = []\n\n    for k in range(num_images):\n        print(\'processing image {}/{}\'.format(k, num_images-1))\n\n        batch = dataset.next_batch()\n\n        cache_name = ""{}.mat"".format(batch[Batch.data_item].coco_id)\n\n        if not from_cache:\n            outputs_np = sess.run(outputs, feed_dict={inputs: batch[Batch.inputs]})\n            scmap, locref, pairwise_diff = extract_cnn_output(outputs_np, cfg, pairwise_stats)\n\n            if cache_scoremaps:\n                if visualise:\n                    img = np.squeeze(batch[Batch.inputs]).astype(\'uint8\')\n                    pose = argmax_pose_predict(scmap, locref, cfg.stride)\n                    arrows = argmax_arrows_predict(scmap, locref, pairwise_diff, cfg.stride)\n                    visualize.show_arrows(cfg, img, pose, arrows)\n                    visualize.waitforbuttonpress()\n                    continue\n\n                out_fn = os.path.join(out_dir, cache_name)\n                dict = {\'scoremaps\': scmap.astype(\'float32\'),\n                        \'locreg_pred\': locref.astype(\'float32\'),\n                        \'pairwise_diff\': pairwise_diff.astype(\'float32\')}\n                scipy.io.savemat(out_fn, mdict=dict)\n                continue\n        else:\n            #cache_name = \'1.mat\'\n            full_fn = os.path.join(cfg.cached_scoremaps, cache_name)\n            mlab = scipy.io.loadmat(full_fn)\n            scmap = mlab[""scoremaps""]\n            locref = mlab[""locreg_pred""]\n            pairwise_diff = mlab[""pairwise_diff""]\n\n        detections = extract_detections(cfg, scmap, locref, pairwise_diff)\n        unLab, pos_array, unary_array, pwidx_array, pw_array = eval_graph(sm, detections)\n        person_conf_multi = get_person_conf_multicut(sm, unLab, unary_array, pos_array)\n\n        if visualise:\n            img = np.squeeze(batch[Batch.inputs]).astype(\'uint8\')\n            #visualize.show_heatmaps(cfg, img, scmap, pose)\n\n            """"""\n            # visualize part detections after NMS\n            visim_dets = visualize_detections(cfg, img, detections)\n            plt.imshow(visim_dets)\n            plt.show()\n            visualize.waitforbuttonpress()\n            """"""\n\n#            """"""\n            visim_multi = img.copy()\n            draw_multi.draw(visim_multi, dataset, person_conf_multi)\n\n            plt.imshow(visim_multi)\n            plt.show()\n            visualize.waitforbuttonpress()\n#            """"""\n\n\n        if cfg.use_gt_segm:\n            coco_img_results = pose_predict_with_gt_segm(scmap, locref, cfg.stride, batch[Batch.data_item].gt_segm,\n                                                      batch[Batch.data_item].coco_id)\n            coco_results += coco_img_results\n            if len(coco_img_results):\n                dataset.visualize_coco(coco_img_results, batch[Batch.data_item].visibilities)\n\n    if cfg.use_gt_segm:\n        with open(\'predictions_with_segm.json\', \'w\') as outfile:\n            json.dump(coco_results, outfile)\n\n    sess.close()\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--novis\', default=False, action=\'store_true\')\n    parser.add_argument(\'--cache\', default=False, action=\'store_true\')\n    parser.add_argument(\'--dev\', default=False, action=\'store_true\')\n    args, unparsed = parser.parse_known_args()\n\n    test_net(not args.novis, args.cache, args.dev)\n'"
train.py,14,"b'import logging\nimport threading\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nfrom util.config import load_config\nfrom dataset.factory import create as create_dataset\nfrom nnet.net_factory import pose_net\nfrom nnet.pose_net import get_batch_spec\nfrom util.logging import setup_logging\n\n\nclass LearningRate(object):\n    def __init__(self, cfg):\n        self.steps = cfg.multi_step\n        self.current_step = 0\n\n    def get_lr(self, iteration):\n        lr = self.steps[self.current_step][0]\n        if iteration == self.steps[self.current_step][1]:\n            self.current_step += 1\n\n        return lr\n\n\ndef setup_preloading(batch_spec):\n    placeholders = {name: tf.placeholder(tf.float32, shape=spec) for (name, spec) in batch_spec.items()}\n    names = placeholders.keys()\n    placeholders_list = list(placeholders.values())\n\n    QUEUE_SIZE = 20\n\n    q = tf.FIFOQueue(QUEUE_SIZE, [tf.float32]*len(batch_spec))\n    enqueue_op = q.enqueue(placeholders_list)\n    batch_list = q.dequeue()\n\n    batch = {}\n    for idx, name in enumerate(names):\n        batch[name] = batch_list[idx]\n        batch[name].set_shape(batch_spec[name])\n    return batch, enqueue_op, placeholders\n\n\ndef load_and_enqueue(sess, enqueue_op, coord, dataset, placeholders):\n    while not coord.should_stop():\n        batch_np = dataset.next_batch()\n        food = {pl: batch_np[name] for (name, pl) in placeholders.items()}\n        sess.run(enqueue_op, feed_dict=food)\n\n\ndef start_preloading(sess, enqueue_op, dataset, placeholders):\n    coord = tf.train.Coordinator()\n\n    t = threading.Thread(target=load_and_enqueue,\n                         args=(sess, enqueue_op, coord, dataset, placeholders))\n    t.start()\n\n    return coord, t\n\n\ndef get_optimizer(loss_op, cfg):\n    learning_rate = tf.placeholder(tf.float32, shape=[])\n\n    if cfg.optimizer == ""sgd"":\n        optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n    elif cfg.optimizer == ""adam"":\n        optimizer = tf.train.AdamOptimizer(cfg.adam_lr)\n    else:\n        raise ValueError(\'unknown optimizer {}\'.format(cfg.optimizer))\n    train_op = slim.learning.create_train_op(loss_op, optimizer)\n\n    return learning_rate, train_op\n\n\ndef train():\n    setup_logging()\n\n    cfg = load_config()\n    dataset = create_dataset(cfg)\n\n    batch_spec = get_batch_spec(cfg)\n    batch, enqueue_op, placeholders = setup_preloading(batch_spec)\n\n    losses = pose_net(cfg).train(batch)\n    total_loss = losses[\'total_loss\']\n\n    for k, t in losses.items():\n        tf.summary.scalar(k, t)\n    merged_summaries = tf.summary.merge_all()\n\n    variables_to_restore = slim.get_variables_to_restore(include=[""resnet_v1""])\n    restorer = tf.train.Saver(variables_to_restore)\n    saver = tf.train.Saver(max_to_keep=5)\n\n    sess = tf.Session()\n\n    coord, thread = start_preloading(sess, enqueue_op, dataset, placeholders)\n\n    train_writer = tf.summary.FileWriter(cfg.log_dir, sess.graph)\n\n    learning_rate, train_op = get_optimizer(total_loss, cfg)\n\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.local_variables_initializer())\n\n    # Restore variables from disk.\n    restorer.restore(sess, cfg.init_weights)\n\n    max_iter = int(cfg.multi_step[-1][1])\n\n    display_iters = cfg.display_iters\n    cum_loss = 0.0\n    lr_gen = LearningRate(cfg)\n\n    for it in range(max_iter+1):\n        current_lr = lr_gen.get_lr(it)\n        [_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],\n                                          feed_dict={learning_rate: current_lr})\n        cum_loss += loss_val\n        train_writer.add_summary(summary, it)\n\n        if it % display_iters == 0:\n            average_loss = cum_loss / display_iters\n            cum_loss = 0.0\n            logging.info(""iteration: {} loss: {} lr: {}""\n                         .format(it, ""{0:.4f}"".format(average_loss), current_lr))\n\n        # Save snapshot\n        if (it % cfg.save_iters == 0 and it != 0) or it == max_iter:\n            model_name = cfg.snapshot_prefix\n            saver.save(sess, model_name, global_step=it)\n\n    sess.close()\n    coord.request_stop()\n    coord.join([thread])\n\n\nif __name__ == \'__main__\':\n    train()\n'"
vis_dataset.py,0,"b'import logging\n\nimport numpy as np\nfrom scipy.misc import imresize\n\nimport matplotlib\nmatplotlib.use(\'TkAgg\')\nimport matplotlib.pyplot as plt\n\nfrom util.config import load_config\nfrom dataset.pose_dataset import Batch\nfrom dataset.factory import create as dataset_create\n\ndef display_dataset():\n    logging.basicConfig(level=logging.DEBUG)\n\n    cfg = load_config()\n    dataset = dataset_create(cfg)\n    dataset.set_shuffle(False)\n\n    while True:\n        batch = dataset.next_batch()\n\n        for frame_id in range(1):\n            img = batch[Batch.inputs][frame_id,:,:,:]\n            img = np.squeeze(img).astype(\'uint8\')\n\n            scmap = batch[Batch.part_score_targets][frame_id,:,:,:]\n            scmap = np.squeeze(scmap)\n\n            # scmask = batch[Batch.part_score_weights]\n            # if scmask.size > 1:\n            #     scmask = np.squeeze(scmask).astype(\'uint8\')\n            # else:\n            #     scmask = np.zeros(img.shape)\n\n            subplot_height = 4\n            subplot_width = 5\n            num_plots = subplot_width * subplot_height\n            f, axarr = plt.subplots(subplot_height, subplot_width)\n\n            for j in range(num_plots):\n                plot_j = j // subplot_width\n                plot_i = j % subplot_width\n\n                curr_plot = axarr[plot_j, plot_i]\n                curr_plot.axis(\'off\')\n\n                if j >= cfg.num_joints:\n                    continue\n\n                scmap_part = scmap[:,:,j]\n                scmap_part = imresize(scmap_part, 8.0, interp=\'nearest\')\n                scmap_part = np.lib.pad(scmap_part, ((4, 0), (4, 0)), \'minimum\')\n\n                curr_plot.set_title(""{}"".format(j+1))\n                curr_plot.imshow(img)\n                curr_plot.hold(True)\n                curr_plot.imshow(scmap_part, alpha=.5)\n\n        # figure(0)\n        # plt.imshow(np.sum(scmap, axis=2))\n        # plt.figure(100)\n        # plt.imshow(img)\n        # plt.figure(2)\n        # plt.imshow(scmask)\n        plt.show()\n        plt.waitforbuttonpress()\n\n\nif __name__ == \'__main__\':\n    display_dataset()\n'"
dataset/factory.py,0,"b'from dataset.pose_dataset import PoseDataset\n\n\ndef create(cfg):\n    dataset_type = cfg.dataset_type\n    if dataset_type == ""mpii"":\n        from dataset.mpii import MPII\n        data = MPII(cfg)\n    elif dataset_type == ""coco"":\n        from dataset.mscoco import MSCOCO\n        data = MSCOCO(cfg)\n    elif dataset_type == ""penn_action"":\n        from dataset.penn_action import PennAction\n        data = PennAction(cfg)\n    elif dataset_type == ""default"":\n        data = PoseDataset(cfg)\n    else:\n        raise Exception(""Unsupported dataset_type: \\""{}\\"""".format(dataset_type))\n    return data\n'"
dataset/mpii.py,0,"b""from dataset.pose_dataset import PoseDataset\n\n\nclass MPII(PoseDataset):\n    def __init__(self, cfg):\n        cfg.all_joints = [[0, 5], [1, 4], [2, 3], [6, 11], [7, 10], [8, 9], [12], [13]]\n        cfg.all_joints_names = ['ankle', 'knee', 'hip', 'wrist', 'elbow', 'shoulder', 'chin', 'forehead']\n        cfg.num_joints = 14\n        super().__init__(cfg)\n\n    def mirror_joint_coords(self, joints, image_width):\n        joints[:, 1] = image_width - joints[:, 1]\n        return joints\n\n    def get_pose_segments(self):\n       return [[0, 1], [1, 2], [3, 4], [4, 5], [6, 7], [7, 8], [9, 10], [10, 11], [12, 13]]\n"""
dataset/mscoco.py,0,"b'import os\nimport sys\nimport skimage.io as io\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nfrom scipy.misc import imresize\nimport json\n\ndir_path = os.path.dirname(os.path.realpath(__file__))\nsys.path.append(dir_path + \'/../lib/coco/PythonAPI\')\n\nfrom pycocotools.coco import COCO as COCO\nfrom pycocotools import mask as maskUtils\n\nfrom dataset.pose_dataset import PoseDataset, DataItem\n\n\ndef get_gt_visibilities(inFile, visibilities):\n    with open(inFile) as data_file:\n        data = json.load(data_file)\n\n    for person_id in range(len(data)):\n        keypoints = data[person_id][""keypoints""]\n        keypoints = [visibilities[person_id][i//3] if i % 3 == 2 else int(keypoints[i]) for i in\n                     range(len(keypoints))]\n        data[person_id][""keypoints""] = keypoints\n\n    with open(inFile, \'w\') as data_file:\n        json.dump(data, data_file)\n\n\nclass MSCOCO(PoseDataset):\n    def __init__(self, cfg):\n        cfg.all_joints = [[0], [2, 1], [4, 3], [6, 5], [8, 7],[10, 9], [12, 11], [14, 13], [16, 15]]\n        cfg.all_joints_names = [""nose"", \'eye\', \'ear\', \'shoulder\', \'elbow\', \'hand\', \'hip\', \'knee\', \'foot\']\n        cfg.num_joints = 17\n        super().__init__(cfg)\n\n    def load_dataset(self):\n        dataset  = self.cfg.dataset\n        dataset_phase = self.cfg.dataset_phase\n        dataset_ann = self.cfg.dataset_ann\n\n        # initialize COCO api\n        annFile = \'%s/annotations/%s_%s.json\'%(dataset,dataset_ann,dataset_phase)\n        self.coco = COCO(annFile)\n\n        imgIds = self.coco.getImgIds()\n\n        data = []\n\n        # loop through each image\n        for imgId in imgIds:\n            item = DataItem()\n\n            img = self.coco.loadImgs(imgId)[0]\n            item.im_path = ""%s/images/%s/%s""%(dataset, dataset_phase, img[""file_name""])\n            item.im_size = [3, img[""height""], img[""width""]]\n            item.coco_id = imgId\n            annIds = self.coco.getAnnIds(imgIds=img[\'id\'], iscrowd=False)\n            anns = self.coco.loadAnns(annIds)\n\n            all_person_keypoints = []\n            masked_persons_RLE = []\n            visible_persons_RLE = []\n            all_visibilities = []\n\n            # Consider only images with people\n            has_people = len(anns) > 0\n            if not has_people and self.cfg.coco_only_images_with_people:\n                continue\n\n            for ann in anns: # loop through each person\n                person_keypoints = []\n                visibilities = []\n                if ann[""num_keypoints""] != 0:\n                    for i in range(self.cfg.num_joints):\n                        x_coord = ann[""keypoints""][3 * i]\n                        y_coord = ann[""keypoints""][3 * i + 1]\n                        visibility = ann[""keypoints""][3 * i + 2]\n                        visibilities.append(visibility)\n                        if visibility != 0: # i.e. if labeled\n                            person_keypoints.append([i, x_coord, y_coord])\n                    all_person_keypoints.append(np.array(person_keypoints))\n                    visible_persons_RLE.append(maskUtils.decode(self.coco.annToRLE(ann)))\n                    all_visibilities.append(visibilities)\n                if ann[""num_keypoints""] == 0:\n                    masked_persons_RLE.append(self.coco.annToRLE(ann))\n\n            item.joints = all_person_keypoints\n            item.im_neg_mask = maskUtils.merge(masked_persons_RLE)\n            if self.cfg.use_gt_segm:\n                item.gt_segm = np.moveaxis(np.array(visible_persons_RLE), 0, -1)\n                item.visibilities = all_visibilities\n            data.append(item)\n\n        self.has_gt = self.cfg.dataset is not ""image_info""\n        return data\n\n\n    def compute_scmap_weights(self, scmap_shape, joint_id, data_item):\n        size = scmap_shape[0:2]\n        scmask = np.ones(size)\n        m = maskUtils.decode(data_item.im_neg_mask)\n        if m.size:\n            scmask = 1.0 - imresize(m, size)\n        scmask = np.stack([scmask] * self.cfg.num_joints, axis=-1)\n        return scmask\n\n    def get_pose_segments(self):\n       return [[0, 1], [0, 2], [1, 3], [2, 4], [5, 7], [6, 8], [7, 9], [8, 10], [11, 13], [12, 14], [13, 15], [14, 16]]\n\n    def visualize_coco(self, coco_img_results, visibilities):\n        inFile = ""tmp.json""\n        with open(inFile, \'w\') as outfile:\n            json.dump(coco_img_results, outfile)\n        get_gt_visibilities(inFile, visibilities)\n\n        # initialize cocoPred api\n        cocoPred = self.coco.loadRes(inFile)\n        os.remove(inFile)\n\n        imgIds = [coco_img_results[0][""image_id""]]\n\n        for imgId in imgIds:\n            img = cocoPred.loadImgs(imgId)[0]\n            im_path = ""%s/images/%s/%s"" % (self.cfg.dataset, self.cfg.dataset_phase, img[""file_name""])\n            I = io.imread(im_path)\n\n            fig = plt.figure()\n            a = fig.add_subplot(2, 2, 1)\n            plt.imshow(I)\n            a.set_title(\'Initial Image\')\n\n            a = fig.add_subplot(2, 2, 2)\n            plt.imshow(I)\n            a.set_title(\'Predicted Keypoints\')\n            annIds = cocoPred.getAnnIds(imgIds=img[\'id\'])\n            anns = cocoPred.loadAnns(annIds)\n            cocoPred.showAnns(anns)\n\n            a = fig.add_subplot(2, 2, 3)\n            plt.imshow(I)\n            a.set_title(\'GT Keypoints\')\n            annIds = self.coco.getAnnIds(imgIds=img[\'id\'])\n            anns = self.coco.loadAnns(annIds)\n            self.coco.showAnns(anns)\n\n            plt.show()\n'"
dataset/penn_action.py,0,"b'from functools import reduce\nimport numpy as np\n\nfrom dataset.pose_dataset import PoseDataset, Batch\n\n\ndef merge_batch(batches):\n    """"""\n    Merges n=len(batches) batches of size 1 into\n    one batch of size n\n    """"""\n    res = {}\n    for key, tensor in batches[0].items():\n        elements = [batch[key] for batch in batches]\n        if type(tensor) is np.ndarray:\n            elements = reduce(lambda x, y: np.concatenate((x, y), axis=0), elements)\n        res[key] = elements\n    return res\n\n\nclass PennAction(PoseDataset):\n    def __init__(self, cfg):\n        cfg.all_joints = [[0], [1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]]\n        cfg.all_joints_names = [""head"", ""shoulder"", ""elbow"", ""wrist"", ""hip"", ""knee"", ""ankle""]\n        cfg.num_joints = 13\n        super().__init__(cfg)\n        self.add_extra_fields()\n\n    def add_extra_fields(self):\n        dataset = self.raw_data[\'dataset\']\n        for i in range(self.num_images):\n            raw_item = dataset[0, i]\n            item = self.data[i]\n            item.seq_id = raw_item[4][0][0]\n            item.frame_id = raw_item[5][0][0]\n\n    def mirror_joint_coords(self, joints, image_width):\n        joints[:, 1] = image_width - joints[:, 1] + 1  # 1-indexed\n        return joints\n\n    def next_batch(self):\n        while True:\n            imidx, mirror = self.next_training_sample()\n            data_item = self.get_training_sample(imidx)\n\n            scale = self.get_scale()\n            if not self.is_valid_size(data_item.im_size, scale):\n                continue\n\n            if self.cfg.video_batch:\n                sequences = self.raw_data[\'sequences\']\n                seq_ids = sequences[0, data_item.seq_id][0]\n                num_frames = len(seq_ids)\n                start_frame = data_item.frame_id\n                num_frames_model = self.cfg.batch_size\n\n                if start_frame + num_frames_model - 1 >= num_frames:\n                    start_frame = num_frames - num_frames_model\n\n                seq_subset = seq_ids[start_frame:start_frame+num_frames_model]\n                data_items = [self.get_training_sample(imidx) for imidx in seq_subset]\n                batches = [self.make_batch(item, scale, mirror) for item in data_items]\n\n                batch = merge_batch(batches)\n            else:\n                batch = self.make_batch(data_item, scale, mirror)\n\n            return batch\n'"
dataset/pose_dataset.py,0,"b'import logging\nimport random as rand\nfrom enum import Enum\n\nimport numpy as np\nfrom numpy import array as arr\nfrom numpy import concatenate as cat\n\nimport scipy.io as sio\nfrom scipy.misc import imread, imresize\n\n\nclass Batch(Enum):\n    inputs = 0\n    part_score_targets = 1\n    part_score_weights = 2\n    locref_targets = 3\n    locref_mask = 4\n    pairwise_targets = 5\n    pairwise_mask = 6\n    data_item = 7\n\n\ndef mirror_joints_map(all_joints, num_joints):\n    res = np.arange(num_joints)\n    symmetric_joints = [p for p in all_joints if len(p) == 2]\n    for pair in symmetric_joints:\n        res[pair[0]] = pair[1]\n        res[pair[1]] = pair[0]\n    return res\n\n\ndef extend_crop(crop, crop_pad, image_size):\n    crop[0] = max(crop[0] - crop_pad, 0)\n    crop[1] = max(crop[1] - crop_pad, 0)\n    crop[2] = min(crop[2] + crop_pad, image_size[2] - 1)\n    crop[3] = min(crop[3] + crop_pad, image_size[1] - 1)\n    return crop\n\n\ndef data_to_input(data):\n    return np.expand_dims(data, axis=0).astype(float)\n\n\ndef collect_pairwise_stats(joint_id, coords):\n    pairwise_stats = {}\n    for person_id in range(len(coords)):\n        num_joints = len(joint_id[person_id])\n        for k_start in range(num_joints):\n            j_id_start = joint_id[person_id][k_start]\n            joint_pt = coords[person_id][k_start, :]\n            j_x_start = np.asscalar(joint_pt[0])\n            j_y_start = np.asscalar(joint_pt[1])\n            for k_end in range(num_joints):\n                if k_start != k_end:\n                    j_id_end = joint_id[person_id][k_end]\n                    joint_pt = coords[person_id][k_end, :]\n                    j_x_end = np.asscalar(joint_pt[0])\n                    j_y_end = np.asscalar(joint_pt[1])\n                    if (j_id_start, j_id_end) not in pairwise_stats:\n                        pairwise_stats[(j_id_start, j_id_end)] = []\n                    pairwise_stats[(j_id_start, j_id_end)].append([j_x_end - j_x_start, j_y_end - j_y_start])\n    return pairwise_stats\n\n\ndef load_pairwise_stats(cfg):\n    mat_stats = sio.loadmat(cfg.pairwise_stats_fn)\n    pairwise_stats = {}\n    for id in range(len(mat_stats[\'graph\'])):\n        pair = tuple(mat_stats[\'graph\'][id])\n        pairwise_stats[pair] = {""mean"": mat_stats[\'means\'][id], ""std"": mat_stats[\'std_devs\'][id]}\n    for pair in pairwise_stats:\n        pairwise_stats[pair][""mean""] *= cfg.global_scale\n        pairwise_stats[pair][""std""] *= cfg.global_scale\n    return pairwise_stats\n\n\ndef get_pairwise_index(j_id, j_id_end, num_joints):\n    return (num_joints - 1) * j_id + j_id_end - int(j_id < j_id_end)\n\n\nclass DataItem:\n    pass\n\n\nclass PoseDataset:\n    def __init__(self, cfg):\n        self.cfg = cfg\n        self.data = self.load_dataset() if cfg.dataset else []\n        self.num_images = len(self.data)\n        if self.cfg.mirror:\n            self.symmetric_joints = mirror_joints_map(cfg.all_joints, cfg.num_joints)\n        self.curr_img = 0\n        self.set_shuffle(cfg.shuffle)\n        self.set_pairwise_stats_collect(cfg.pairwise_stats_collect)\n        if self.cfg.pairwise_predict:\n            self.pairwise_stats = load_pairwise_stats(self.cfg)\n\n\n    def load_dataset(self):\n        cfg = self.cfg\n        file_name = cfg.dataset\n        # Load Matlab file dataset annotation\n        mlab = sio.loadmat(file_name)\n        self.raw_data = mlab\n        mlab = mlab[\'dataset\']\n\n        num_images = mlab.shape[1]\n        data = []\n        has_gt = True\n\n        for i in range(num_images):\n            sample = mlab[0, i]\n\n            item = DataItem()\n            item.image_id = i\n            item.im_path = sample[0][0]\n            item.im_size = sample[1][0]\n            if len(sample) >= 3:\n                joints = sample[2][0][0]\n                joint_id = joints[:, 0]\n                # make sure joint ids are 0-indexed\n                if joint_id.size != 0:\n                    assert((joint_id < cfg.num_joints).any())\n                joints[:, 0] = joint_id\n                item.joints = [joints]\n            else:\n                has_gt = False\n            if cfg.crop:\n                crop = sample[3][0] - 1\n                item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)\n            data.append(item)\n\n        self.has_gt = has_gt\n        return data\n\n    def num_keypoints(self):\n        return self.cfg.num_joints\n\n    def set_test_mode(self, test_mode):\n        self.has_gt = not test_mode\n\n\n    def set_shuffle(self, shuffle):\n        self.shuffle = shuffle\n        if not shuffle:\n            assert not self.cfg.mirror\n            self.image_indices = np.arange(self.num_images)\n\n\n    def set_pairwise_stats_collect(self, pairwise_stats_collect):\n        self.pairwise_stats_collect = pairwise_stats_collect\n        if self.pairwise_stats_collect:\n            assert self.get_scale() == 1.0\n\n\n    def mirror_joint_coords(self, joints, image_width):\n        # horizontally flip the x-coordinate, keep y unchanged\n        joints[:, 1] = image_width - joints[:, 1] - 1\n        return joints\n\n\n    def mirror_joints(self, joints, symmetric_joints, image_width):\n        # joint ids are 0 indexed\n        res = np.copy(joints)\n        res = self.mirror_joint_coords(res, image_width)\n        # swap the joint_id for a symmetric one\n        joint_id = joints[:, 0].astype(int)\n        res[:, 0] = symmetric_joints[joint_id]\n        return res\n\n\n    def shuffle_images(self):\n        num_images = self.num_images\n        if self.cfg.mirror:\n            image_indices = np.random.permutation(num_images * 2)\n            self.mirrored = image_indices >= num_images\n            image_indices[self.mirrored] = image_indices[self.mirrored] - num_images\n            self.image_indices = image_indices\n        else:\n            self.image_indices = np.random.permutation(num_images)\n\n\n    def num_training_samples(self):\n        num = self.num_images\n        if self.cfg.mirror:\n            num *= 2\n        return num\n\n\n    def next_training_sample(self):\n        if self.curr_img == 0 and self.shuffle:\n            self.shuffle_images()\n\n        curr_img = self.curr_img\n        self.curr_img = (self.curr_img + 1) % self.num_training_samples()\n\n        imidx = self.image_indices[curr_img]\n        mirror = self.cfg.mirror and self.mirrored[curr_img]\n\n        return imidx, mirror\n\n\n    def get_training_sample(self, imidx):\n        return self.data[imidx]\n\n\n    def get_scale(self):\n        cfg = self.cfg\n        scale = cfg.global_scale\n        if hasattr(cfg, \'scale_jitter_lo\') and hasattr(cfg, \'scale_jitter_up\'):\n            scale_jitter = rand.uniform(cfg.scale_jitter_lo, cfg.scale_jitter_up)\n            scale *= scale_jitter\n        return scale\n\n\n    def next_batch(self):\n        while True:\n            imidx, mirror = self.next_training_sample()\n            data_item = self.get_training_sample(imidx)\n            scale = self.get_scale()\n\n            if not self.is_valid_size(data_item.im_size, scale):\n                continue\n\n            return self.make_batch(data_item, scale, mirror)\n\n\n    def is_valid_size(self, image_size, scale):\n        im_width = image_size[2]\n        im_height = image_size[1]\n\n        max_input_size = 100\n        if im_height < max_input_size or im_width < max_input_size:\n            return False\n\n        if hasattr(self.cfg, \'max_input_size\'):\n            max_input_size = self.cfg.max_input_size\n            input_width = im_width * scale\n            input_height = im_height * scale\n            if input_height * input_width > max_input_size * max_input_size:\n                return False\n\n        return True\n\n\n    def make_batch(self, data_item, scale, mirror):\n        im_file = data_item.im_path\n        logging.debug(\'image %s\', im_file)\n        logging.debug(\'mirror %r\', mirror)\n        image = imread(im_file, mode=\'RGB\')\n\n        if self.has_gt:\n            joints = np.copy(data_item.joints)\n\n        if self.cfg.crop:\n            crop = data_item.crop\n            image = image[crop[1]:crop[3] + 1, crop[0]:crop[2] + 1, :]\n            if self.has_gt:\n                joints[:, 1:3] -= crop[0:2].astype(joints.dtype)\n\n        img = imresize(image, scale) if scale != 1 else image\n        scaled_img_size = arr(img.shape[0:2])\n\n        if mirror:\n            img = np.fliplr(img)\n\n        batch = {Batch.inputs: img}\n\n        if self.has_gt:\n            stride = self.cfg.stride\n\n            if mirror:\n                joints = [self.mirror_joints(person_joints, self.symmetric_joints, image.shape[1]) for person_joints in\n                          joints]\n\n            sm_size = np.ceil(scaled_img_size / (stride * 2)).astype(int) * 2\n\n            scaled_joints = [person_joints[:, 1:3] * scale for person_joints in joints]\n\n            joint_id = [person_joints[:, 0].astype(int) for person_joints in joints]\n            batch = self.compute_targets_and_weights(joint_id, scaled_joints, data_item, sm_size, scale, batch)\n\n            if self.pairwise_stats_collect:\n                data_item.pairwise_stats = collect_pairwise_stats(joint_id, scaled_joints)\n\n        batch = {key: data_to_input(data) for (key, data) in batch.items()}\n\n        batch[Batch.data_item] = data_item\n\n        return batch\n\n\n    def set_locref(self, locref_map, locref_mask, locref_scale, i, j, j_id, dx, dy):\n        locref_mask[j, i, j_id * 2 + 0] = 1\n        locref_mask[j, i, j_id * 2 + 1] = 1\n        locref_map[j, i, j_id * 2 + 0] = dx * locref_scale\n        locref_map[j, i, j_id * 2 + 1] = dy * locref_scale\n\n\n    def set_pairwise_map(self, pairwise_map, pairwise_mask, i, j, j_id, j_id_end, coords, pt_x, pt_y, person_id, k_end):\n        num_joints = self.cfg.num_joints\n        joint_pt = coords[person_id][k_end, :]\n        j_x_end = np.asscalar(joint_pt[0])\n        j_y_end = np.asscalar(joint_pt[1])\n        pair_id = get_pairwise_index(j_id, j_id_end, num_joints)\n        stats = self.pairwise_stats[(j_id, j_id_end)]\n        dx = j_x_end - pt_x\n        dy = j_y_end - pt_y\n        pairwise_mask[j, i, pair_id * 2 + 0] = 1\n        pairwise_mask[j, i, pair_id * 2 + 1] = 1\n        pairwise_map[j, i, pair_id * 2 + 0] = (dx - stats[""mean""][0]) / stats[""std""][0]\n        pairwise_map[j, i, pair_id * 2 + 1] = (dy - stats[""mean""][1]) / stats[""std""][1]\n\n\n    def compute_targets_and_weights(self, joint_id, coords, data_item, size, scale, batch):\n        stride = self.cfg.stride\n        dist_thresh = self.cfg.pos_dist_thresh * scale\n        num_joints = self.cfg.num_joints\n        half_stride = stride / 2\n        scmap = np.zeros(cat([size, arr([num_joints])]))\n\n        locref_shape = cat([size, arr([num_joints * 2])])\n        locref_mask = np.zeros(locref_shape)\n        locref_map = np.zeros(locref_shape)\n\n        pairwise_shape = cat([size, arr([num_joints * (num_joints - 1) * 2])])\n        pairwise_mask = np.zeros(pairwise_shape)\n        pairwise_map = np.zeros(pairwise_shape)\n\n        dist_thresh_sq = dist_thresh ** 2\n\n        width = size[1]\n        height = size[0]\n\n        for person_id in range(len(coords)):\n            for k, j_id in enumerate(joint_id[person_id]):\n                joint_pt = coords[person_id][k, :]\n                j_x = np.asscalar(joint_pt[0])\n                j_y = np.asscalar(joint_pt[1])\n\n                # don\'t loop over entire heatmap, but just relevant locations\n                j_x_sm = round((j_x - half_stride) / stride)\n                j_y_sm = round((j_y - half_stride) / stride)\n                min_x = round(max(j_x_sm - dist_thresh - 1, 0))\n                max_x = round(min(j_x_sm + dist_thresh + 1, width - 1))\n                min_y = round(max(j_y_sm - dist_thresh - 1, 0))\n                max_y = round(min(j_y_sm + dist_thresh + 1, height - 1))\n\n                for j in range(min_y, max_y + 1):  # range(height):\n                    pt_y = j * stride + half_stride\n                    for i in range(min_x, max_x + 1):  # range(width):\n                        # pt = arr([i*stride+half_stride, j*stride+half_stride])\n                        # diff = joint_pt - pt\n                        # The code above is too slow in python\n                        pt_x = i * stride + half_stride\n                        dx = j_x - pt_x\n                        dy = j_y - pt_y\n                        dist = dx ** 2 + dy ** 2\n                        # print(la.norm(diff))\n\n                        if dist <= dist_thresh_sq:\n                            dist = dx ** 2 + dy ** 2\n                            locref_scale = 1.0 / self.cfg.locref_stdev\n                            current_normalized_dist = dist * locref_scale ** 2\n                            prev_normalized_dist = locref_map[j, i, j_id * 2 + 0] ** 2 + \\\n                                                   locref_map[j, i, j_id * 2 + 1] ** 2\n                            update_scores = (scmap[j, i, j_id] == 0) or prev_normalized_dist > current_normalized_dist\n                            if self.cfg.location_refinement and update_scores:\n                                self.set_locref(locref_map, locref_mask, locref_scale, i, j, j_id, dx, dy)\n                            if self.cfg.pairwise_predict and update_scores:\n                                for k_end, j_id_end in enumerate(joint_id[person_id]):\n                                    if k != k_end:\n                                        self.set_pairwise_map(pairwise_map, pairwise_mask, i, j, j_id, j_id_end,\n                                                              coords, pt_x, pt_y, person_id, k_end)\n                            scmap[j, i, j_id] = 1\n\n        scmap_weights = self.compute_scmap_weights(scmap.shape, joint_id, data_item)\n\n        # Update batch\n        batch.update({\n            Batch.part_score_targets: scmap,\n            Batch.part_score_weights: scmap_weights\n        })\n        if self.cfg.location_refinement:\n            batch.update({\n                Batch.locref_targets: locref_map,\n                Batch.locref_mask: locref_mask\n            })\n        if self.cfg.pairwise_predict:\n            batch.update({\n                Batch.pairwise_targets: pairwise_map,\n                Batch.pairwise_mask: pairwise_mask\n            })\n\n        return batch\n\n\n    def compute_scmap_weights(self, scmap_shape, joint_id, data_item):\n        cfg = self.cfg\n        if cfg.weigh_only_present_joints:\n            weights = np.zeros(scmap_shape)\n            for person_joint_id in joint_id:\n                for j_id in person_joint_id:\n                    weights[:, :, j_id] = 1.0\n        else:\n            weights = np.ones(scmap_shape)\n        return weights'"
demo/demo_multiperson.py,0,"b'import os\nimport sys\n\nimport numpy as np\n\nsys.path.append(os.path.dirname(__file__) + ""/../"")\n\nfrom scipy.misc import imread, imsave\n\nfrom util.config import load_config\nfrom dataset.factory import create as create_dataset\nfrom nnet import predict\nfrom util import visualize\nfrom dataset.pose_dataset import data_to_input\n\nfrom multiperson.detections import extract_detections\nfrom multiperson.predict import SpatialModel, eval_graph, get_person_conf_multicut\nfrom multiperson.visualize import PersonDraw, visualize_detections\n\nimport matplotlib.pyplot as plt\n\n\ncfg = load_config(""demo/pose_cfg_multi.yaml"")\n\ndataset = create_dataset(cfg)\n\nsm = SpatialModel(cfg)\nsm.load()\n\ndraw_multi = PersonDraw()\n\n# Load and setup CNN part detector\nsess, inputs, outputs = predict.setup_pose_prediction(cfg)\n\n# Read image from file\nfile_name = ""demo/image_multi.png""\nimage = imread(file_name, mode=\'RGB\')\n\nimage_batch = data_to_input(image)\n\n# Compute prediction with the CNN\noutputs_np = sess.run(outputs, feed_dict={inputs: image_batch})\nscmap, locref, pairwise_diff = predict.extract_cnn_output(outputs_np, cfg, dataset.pairwise_stats)\n\ndetections = extract_detections(cfg, scmap, locref, pairwise_diff)\nunLab, pos_array, unary_array, pwidx_array, pw_array = eval_graph(sm, detections)\nperson_conf_multi = get_person_conf_multicut(sm, unLab, unary_array, pos_array)\n\nimg = np.copy(image)\n\nvisim_multi = img.copy()\n\nfig = plt.imshow(visim_multi)\ndraw_multi.draw(visim_multi, dataset, person_conf_multi)\nfig.axes.get_xaxis().set_visible(False)\nfig.axes.get_yaxis().set_visible(False)\n\nplt.show()\nvisualize.waitforbuttonpress()\n'"
demo/singleperson.py,0,"b'import os\nimport sys\n\nsys.path.append(os.path.dirname(__file__) + ""/../"")\n\nfrom scipy.misc import imread\n\nfrom util.config import load_config\nfrom nnet import predict\nfrom util import visualize\nfrom dataset.pose_dataset import data_to_input\n\n\ncfg = load_config(""demo/pose_cfg.yaml"")\n\n# Load and setup CNN part detector\nsess, inputs, outputs = predict.setup_pose_prediction(cfg)\n\n# Read image from file\nfile_name = ""demo/image.png""\nimage = imread(file_name, mode=\'RGB\')\n\nimage_batch = data_to_input(image)\n\n# Compute prediction with the CNN\noutputs_np = sess.run(outputs, feed_dict={inputs: image_batch})\nscmap, locref, _ = predict.extract_cnn_output(outputs_np, cfg)\n\n# Extract maximum scoring location from the heatmap, assume 1 person\npose = predict.argmax_pose_predict(scmap, locref, cfg.stride)\n\n# Visualise\nvisualize.show_heatmaps(cfg, image, scmap, pose)\nvisualize.waitforbuttonpress()\n'"
multiperson/detections.py,0,"b""import math\nimport os\nimport sys\nfrom collections import namedtuple\n\nimport numpy as np\n\ndir_path = os.path.dirname(os.path.realpath(__file__))\nsys.path.append(dir_path + '/../lib/nms_cython/')\nfrom nms_grid import nms_grid\n\nDetections = namedtuple('Detections', ['coord', 'coord_grid', 'conf', 'pairwise'])\n\n\ndef pos_from_grid_raw(cfg, gridpos):\n    return gridpos * cfg.stride + 0.5 * cfg.stride\n\n\ndef pos_from_gridpos_offset(cfg, gridpos, pred_offset):\n    return pos_from_grid_raw(cfg, gridpos) + pred_offset\n\n\ndef make_nms_grid(nms_radius):\n    nms_radius = math.ceil(nms_radius)\n    dist_grid = np.zeros([2 * nms_radius + 1, 2 * nms_radius + 1], dtype=np.uint8)\n    for yidx in range(dist_grid.shape[0]):\n        for xidx in range(dist_grid.shape[1]):\n            if (yidx - nms_radius) ** 2 + (xidx - nms_radius) ** 2 <= nms_radius ** 2:\n                dist_grid[yidx][xidx] = 1\n    return dist_grid\n\n\ndef extract_detections(cfg, scmap, locref, pairwise_diff):\n    num_joints = cfg.num_joints\n    num_pairwise_relations = pairwise_diff.shape[2]\n\n    # get dist_grid\n    dist_grid = make_nms_grid(cfg.nms_radius)\n\n    unProb = [None] * num_joints\n    unPos = [None] * num_joints\n    unPos_grid = [None] * num_joints\n    pairwiseDiff = [None] * num_joints\n\n    # apply nms\n    for p_idx in range(num_joints):\n        # IMPORTANT, as C++ function expects row-major\n        prob_map = np.ascontiguousarray(scmap[:, :, p_idx])\n        # print(prob_map.flags) has to be C_CONTIGUOUS\n\n        dets = nms_grid(prob_map, dist_grid, cfg.det_min_score)\n\n        cur_prob = np.zeros([len(dets), 1], dtype=np.float64)\n        cur_pos = np.zeros([len(dets), 2], dtype=np.float64)\n        cur_pos_grid = np.zeros([len(dets), 2], dtype=np.float64)\n        cur_pairwise = np.zeros([len(dets), num_pairwise_relations, 2], dtype=np.float64)\n\n        for idx, didx in enumerate(dets):\n            ix = didx % scmap.shape[1]\n            iy = didx // scmap.shape[1]\n\n            cur_prob[idx, 0] = scmap[iy, ix, p_idx]\n            cur_pos_grid[idx, :] = pos_from_grid_raw(cfg, np.array([ix, iy]))\n            cur_pos[idx, :] = cur_pos_grid[idx, :] + locref[iy, ix, p_idx, :]\n            cur_pairwise[idx, :, :] = pairwise_diff[iy, ix, :, :]\n\n        unProb[p_idx] = cur_prob\n        unPos[p_idx] = cur_pos\n        unPos_grid[p_idx] = cur_pos_grid\n        pairwiseDiff[p_idx] = cur_pairwise\n\n    return Detections(coord=unPos, coord_grid=unPos_grid, conf=unProb, pairwise=pairwiseDiff)\n"""
multiperson/predict.py,0,"b'import math\nimport time\nimport sys\nimport os\nfrom collections import namedtuple\n\nimport numpy as np\nimport scipy.io as sio\n\ndir_path = os.path.dirname(os.path.realpath(__file__))\nsys.path.append(dir_path + \'/../lib/multicut_cython/\')\nfrom multicut import solve_nl_lmp\n\nfrom dataset.pose_dataset import get_pairwise_index\n\n\ndef logit_transform(p):\n    p = np.minimum(np.maximum(p, 1e-7), 1.0 - 1e-7)\n    return np.log((1-p)/p)\n\n\ndef eval_graph(sm, detections):\n\n    time_start = time.time()\n\n    unary_prob = detections.conf\n    coordinates = detections.coord\n    pairwise_regr = detections.pairwise\n\n    cidx_list = range(0, sm.num_keypoints)\n    #cidx_list = [0, 1, 2]\n\n    unary_counts = []\n\n    for idx, cidx in enumerate(cidx_list):\n        unary_counts.append(unary_prob[cidx].shape[0])\n    \n    num_unary = sum(unary_counts)\n    num_pairwise = 0\n\n    for idx1, cidx1 in enumerate(cidx_list):\n        for cidx2 in cidx_list[idx1:]:\n            if cidx1 == cidx2:\n                num_pairwise += unary_prob[cidx1].shape[0]*(unary_prob[cidx1].shape[0] - 1) // 2\n            else:\n                num_pairwise += unary_prob[cidx1].shape[0]*unary_prob[cidx2].shape[0]\n\n    pos_array = np.zeros([num_unary, 2], dtype=np.float64)\n    unary_array = np.zeros([num_unary, 1], dtype=np.float64)\n    pw_array = np.zeros([num_pairwise, 1], dtype=np.float64)\n    pwidx_array = np.zeros([num_pairwise, 2], dtype=np.uint16)\n\n    firstidx = 0\n    firstidx_list = []\n    for idx, cidx in enumerate(cidx_list):\n        lastidx = firstidx + unary_prob[cidx].shape[0]\n        unary_array[firstidx:lastidx] = unary_prob[cidx]\n        pos_array[firstidx:lastidx] = coordinates[cidx]\n\n        firstidx_list.append(firstidx)\n        firstidx = lastidx\n\n    firstidx = 0\n    for idx1, cidx1 in enumerate(cidx_list):\n        for idx2 in range(idx1, len(cidx_list)):\n\n            if coordinates[cidx1].shape[0] > 0:\n                cidx2 = cidx_list[idx2]\n\n                if coordinates[cidx2].shape[0] > 0:\n\n                    if not sm.need_this_pairwise(cidx1, cidx2):\n                        continue\n\n                    cur_prob, ptidx = sm.eval(cidx1, cidx2, detections)\n                    lastidx = firstidx + cur_prob.shape[0]\n\n                    ptidx[:, 0] += firstidx_list[idx1]\n                    ptidx[:, 1] += firstidx_list[idx2]\n\n                    pw_array[firstidx:lastidx, 0] = cur_prob\n                    pwidx_array[firstidx:lastidx, :] = ptidx\n\n                    firstidx = lastidx\n\n    is_sparse_graph = True\n    solver_type = False\n    do_suppression = True\n    logit_in_solver = False\n\n    if unary_array.shape[0] > 0:\n\n        unary_array_solver = unary_array if logit_in_solver else logit_transform(unary_array)\n        pw_array_solver = pw_array if logit_in_solver else logit_transform(pw_array)\n\n        time_start = time.time()\n\n        res = solve_nl_lmp(unary_array_solver, pwidx_array, pw_array_solver,\n                          is_sparse_graph, solver_type, do_suppression, logit_in_solver)\n\n        unLab = np.array(res, dtype=np.uint64)\n\n        firstidx = 0\n        for cidx in cidx_list:\n            lastidx = firstidx + unary_prob[cidx].shape[0]\n            unLab[firstidx:lastidx, 0] = cidx\n            firstidx = lastidx\n\n    else:\n        unLab = np.array([])\n\n    return unLab, pos_array, unary_array, pwidx_array, pw_array\n\ndef get_person_conf_single(sm, unProb, pos_array, pwidx_array, pw_array):\n\n    assert(len(unProb) == sm.num_keypoints)\n    assert(pwidx_array.shape[0] == pw_array.shape[0])\n    assert(pw_array.shape[1] == 1)\n\n    det_type_idx = []\n    \n    firstidx = 0\n    for pidx in range(len(unProb)):\n        lastidx = firstidx + unProb[pidx].shape[0]\n\n        curidx = np.array([False]*pos_array.shape[0])\n        curidx[firstidx:lastidx] = True\n        \n        firstidx = lastidx\n        det_type_idx.append(curidx)\n\n    # num people == number of heads \n    head_idx = 13\n    num_people = unProb[head_idx].shape[0]\n\n    connect_graph = dict()\n    connect_graph[13] = (12,)\n    connect_graph[12] = (8, 9, 2, 3)\n    connect_graph[8] = (7,)\n    connect_graph[7] = (6,)\n    connect_graph[9] = (10,)\n    connect_graph[10] = (11,)\n    connect_graph[2] = (1,)\n    connect_graph[1] = (0,)\n    connect_graph[3] = (4,)\n    connect_graph[4] = (5,)\n\n    person_conf = np.zeros([num_people, sm.num_keypoints, 2])\n    SearchNode = namedtuple(\'SearchNode\', [\'pidx\', \'kidx\', \'hidx\'])\n    \n    search_queue = []\n\n    for pidx, hidx in enumerate(np.flatnonzero(det_type_idx[head_idx])):\n        search_queue.append(SearchNode(pidx=pidx, kidx=head_idx, hidx=hidx))\n        person_conf[pidx, head_idx, :] = pos_array[hidx, :]\n\n    assert(len(search_queue) == num_people)\n\n    while len(search_queue) > 0:\n        node = search_queue.pop()\n\n        pidx = node.pidx\n        kidx = node.kidx\n        hidx = node.hidx\n\n        # find the closes match for current part \n        if kidx in connect_graph:\n\n            for kidx2 in connect_graph[kidx]:\n                best_hidx = None\n                best_pw = None\n                \n                # search all pairwise with compatible type \n                for idx in range(pwidx_array.shape[0]):\n                    if pwidx_array[idx, 0] == hidx or pwidx_array[idx, 1] == hidx:\n                        idx2 = np.flatnonzero(pwidx_array[idx, :] != hidx)[0]\n                        other_hidx = pwidx_array[idx, idx2]\n\n                        if det_type_idx[kidx2][other_hidx]:\n                            if pw_array[idx] > best_pw:\n                                best_hidx = other_hidx\n                                best_pw = pw_array[idx]\n                        \n\n                if best_pw > 0.5:\n                    person_conf[pidx, kidx2, :] = pos_array[best_hidx, :]\n                    search_queue.append(SearchNode(pidx = pidx, kidx = kidx2, hidx = best_hidx))\n\n    return person_conf\n\n    \n\ndef get_person_conf_multicut(sm, unLab, unary_array, pos_array):\n    if unLab.shape[0] > 0:\n        num_people = int(np.max(unLab[:, 1])) + 1\n    else:\n        num_people = 0\n\n    person_conf = np.zeros([num_people, sm.num_keypoints, 2])\n    sum_prob = np.zeros([num_people, sm.num_keypoints, 1])\n\n    # compute weighted average of keypoints of the same time \n    for didx in range(unLab.shape[0]):\n        kidx = unLab[didx,0]\n        pidx = unLab[didx,1]\n\n        person_conf[pidx, kidx, :] += pos_array[didx, :]*unary_array[didx]\n        sum_prob[pidx, kidx] += unary_array[didx]\n\n    print(""num_people: "", num_people)\n\n    for pidx in range(num_people):\n        for kidx in range(sm.num_keypoints):\n            if sum_prob[pidx, kidx] > 0:\n                person_conf[pidx, kidx, :] /= sum_prob[pidx, kidx]\n\n    return person_conf\n\n\ndef compute_angle(deltaX, deltaY):\n    angle = np.arctan2(deltaY,deltaX)\n\n    # atan2 is between [-pi, pi]\n    # wrapMinusPiPifast(angle)\n\n    assert((angle > math.pi).sum() == 0)\n    assert((angle < -math.pi).sum() == 0)\n\n    return angle\n    #assert(all(angle <= math.pi) && all(angle >= -math.pi))\n\n\ndef wrap_angle(a):\n    larger = a > math.pi\n    smaller = a < -math.pi\n    a[larger]  = a[larger] - 2*math.pi\n    a[smaller] = a[smaller] + 2*math.pi\n\n    return a\n\n\ndef compute_features(delta_real, delta_predicted):\n    a = compute_angle(delta_real[:, 0], delta_real[:, 1])\n    a_forward = compute_angle(delta_predicted[:, 0], delta_predicted[:, 1])\n    delta1 = delta_real - delta_predicted\n\n    dist = np.linalg.norm(delta1, axis=1, ord=2)\n    abs_a = np.abs(wrap_angle(a - a_forward))\n    return dist, abs_a\n\n\nclass SpatialModel:\n\n    def __init__(self, cfg):\n        self.num_keypoints = cfg.num_joints\n        num_keypoints = cfg.num_joints\n        self.cfg = cfg\n\n        self.graph_dict = dict()\n\n        self.same_part_pw_coef = 0.2\n\n        self.X_min = [[None]*num_keypoints for idx in range(num_keypoints)]\n        self.X_max = [[None]*num_keypoints for idx in range(num_keypoints)]\n        self.w = [[None]*num_keypoints for idx in range(num_keypoints)]\n\n    def load(self):\n        for cidx1 in range(self.num_keypoints):\n            for cidx2 in range(cidx1+1, self.num_keypoints):\n                model_name  = ""{}/spatial_model_cidx_{}_{}.mat"".format(self.cfg.pairwise_model_dir, cidx1 + 1, cidx2 + 1)\n                #print ""loading:"", model_name\n                if not os.path.isfile(model_name):\n                    continue\n    \n                spatial_model = sio.loadmat(model_name)\n\n                self.X_max[cidx1][cidx2] = spatial_model[\'spatial_model\'][\'training_opts\'][0][0][0][\'X_max\'][0][0]\n                self.X_min[cidx1][cidx2] = spatial_model[\'spatial_model\'][\'training_opts\'][0][0][0][\'X_min\'][0][0]\n                self.w[cidx1][cidx2] = spatial_model[\'spatial_model\'][\'log_reg\'][0][0][0][\'w\'][0][0][:]\n\n                #self.pair_class[cidx1][cidx2] = PairClassDistAngle(w=w, X_min=X_min, X_max=X_max)\n                \n        if not self.cfg.tensorflow_pairwise_order:\n            tmpval = sio.loadmat(self.cfg.pairwise_stats_fn)\n            self.graph = tmpval[\'graph\']\n            self.pairwise_means = tmpval[\'means\']\n            self.pairwise_std_devs = tmpval[\'std_devs\']\n\n            for gidx in range(self.graph.shape[0]):\n                cidx1 = self.graph[gidx, 0]\n                cidx2 = self.graph[gidx, 1]\n                self.graph_dict[(cidx1, cidx2)] = gidx\n\n    def get_fwd_bwd_index(self, cidx1, cidx2):\n        if self.cfg.tensorflow_pairwise_order:\n            fwd_idx = get_pairwise_index(cidx1, cidx2, self.cfg.num_joints)\n            bwd_idx = get_pairwise_index(cidx2, cidx1, self.cfg.num_joints)\n        else:\n            fwd_idx = self.graph_dict[(cidx1+1, cidx2+1)]\n            bwd_idx = self.graph_dict[(cidx2+1, cidx1+1)]\n        return fwd_idx, bwd_idx\n\n    def need_this_pairwise(self, cidx1, cidx2):\n        if cidx1 == cidx2:\n            return True\n        sparse_graph = self.cfg.sparse_graph\n        return not sparse_graph or [cidx1, cidx2] in sparse_graph\n\n    def eval(self, cidx1, cidx2, detections):\n        unPos = detections.coord\n\n        idx_type1 = np.array(range(unPos[cidx1].shape[0]))\n        idx_type2 = np.array(range(unPos[cidx2].shape[0]))\n\n        assert(idx_type1.shape[0] > 0)\n        assert(idx_type2.shape[0] > 0)\n\n        num_edges = len(idx_type1) * len(idx_type2)\n\n        tmpidx1, tmpidx2 = np.meshgrid(idx_type1, idx_type2)\n        ptidx = np.hstack((tmpidx1.T.reshape((num_edges, 1)), tmpidx2.T.reshape((num_edges, 1))))\n\n        if cidx1 != cidx2:\n            cur_prob = self.compute_different_part_pairwise(cidx1, cidx2, detections, ptidx, num_edges)\n        else:\n            cur_prob = None\n            ptidx = ptidx[ptidx[:, 0] < ptidx[:, 1]]\n\n            delta = unPos[cidx2][ptidx[:, 1], :] - unPos[cidx1][ptidx[:, 0], :]\n            dists = np.linalg.norm(delta, axis=1, ord=2)\n\n            cur_prob = 1./(1 + np.exp(self.same_part_pw_coef*dists-7.5))\n\n        return cur_prob, ptidx\n\n    def compute_different_part_pairwise(self, cidx1, cidx2, detections, ptidx, num_edges):\n        unPos = detections.coord\n        unPos_grid = detections.coord_grid\n        nextReg = detections.pairwise\n\n        fwd_idx, bwd_idx = self.get_fwd_bwd_index(cidx1, cidx2)\n        #print(""cidxs "", cidx1, cidx2)\n        #print(""pairwise index"", fwd_idx, bwd_idx)\n\n        assert (ptidx.shape[0] > 0)\n\n        delta_real_forward = unPos[cidx2][ptidx[:, 1], :] - unPos_grid[cidx1][ptidx[:, 0], :]\n        delta_real_backward = unPos[cidx1][ptidx[:, 0], :] - unPos_grid[cidx2][ptidx[:, 1], :]\n\n        delta_forward = nextReg[cidx1][ptidx[:, 0], fwd_idx, :]\n        delta_backward = nextReg[cidx2][ptidx[:, 1], bwd_idx, :]\n\n        dist1, abs_a1 = compute_features(delta_real_forward, delta_forward)\n        dist2, abs_a2 = compute_features(delta_real_backward, delta_backward)\n\n        featAugm = np.hstack((dist1.reshape(num_edges, 1), abs_a1.reshape(num_edges, 1), dist2.reshape(num_edges, 1),\n                              abs_a2.reshape(num_edges, 1)))\n\n        # print ""features: "", cidx1, cidx2, featAugm\n\n        featAugm = np.hstack((featAugm, np.exp(-featAugm), np.ones((num_edges, 1))))\n\n        featAugm[:, :-1] = (featAugm[:, :-1] - self.X_min[cidx1][cidx2]) / (\n            self.X_max[cidx1][cidx2] - self.X_min[cidx1][cidx2])\n        cur_prob = 1 / (1 + np.exp(-featAugm.dot(self.w[cidx1][cidx2])))\n\n        return cur_prob\n'"
multiperson/visualize.py,0,"b'import math\nimport numpy as np\n\nimport scipy.spatial\n\nimport matplotlib.pyplot as plt\n\nimport munkres\n\nfrom util.visualize import check_point, _npcircle\nfrom util import visualize\n\n\nmin_match_dist = 200\nmarker_size = 5\n\ndraw_conf_min_count = 3\n\n\ndef get_ref_points(person_conf):\n    avg_conf = np.sum(person_conf, axis=1) / person_conf.shape[1]\n\n    # last points is tip of the head -> use it as reference\n    ref_points = person_conf[:, -1, :]\n\n    # use average of other points if head tip is missing\n    emptyidx = (np.sum(ref_points, axis=1) == 0)\n    ref_points[emptyidx, :] = avg_conf[emptyidx, :]\n\n    return ref_points\n\n\nclass PersonDraw:\n    def __init__(self):\n        self.mk = munkres.Munkres()\n\n        self.prev_person_conf = np.zeros([0, 1])\n        self.prev_color_assignment = None\n\n        # generated colors from http://tools.medialab.sciences-po.fr/iwanthue/\n        track_colors_str = [""#F5591E"",\n                            ""#3870FB"",\n                            ""#FE5DB0"",\n                            ""#B4A691"",\n                            ""#43053F"",\n                            ""#3475B1"",\n                            ""#642612"",\n                            ""#B3B43D"",\n                            ""#DD9BFE"",\n                            ""#28948D"",\n                            ""#E99D53"",\n                            ""#012B46"",\n                            ""#9D2DA3"",\n                            ""#04220A"",\n                            ""#62CB22"",\n                            ""#EE8F91"",\n                            ""#D71638"",\n                            ""#00613A"",\n                            ""#318918"",\n                            ""#B770FF"",\n                            ""#82C091"",\n                            ""#6C1333"",\n                            ""#973405"",\n                            ""#B19CB2"",\n                            ""#F6267B"",\n                            ""#284489"",\n                            ""#97BF17"",\n                            ""#3B899C"",\n                            ""#931813"",\n                            ""#FA76B6""]\n\n        self.track_colors = [(int(s[1:3], 16), int(s[3:5], 16), int(s[5:7], 16)) for s in track_colors_str]\n\n    def draw(self, visim, dataset, person_conf):\n        minx = 2 * marker_size\n        miny = 2 * marker_size\n        maxx = visim.shape[1] - 2 * marker_size\n        maxy = visim.shape[0] - 2 * marker_size\n\n        num_people = person_conf.shape[0]\n        color_assignment = dict()\n\n        # MA: assign same color to matching body configurations\n        if self.prev_person_conf.shape[0] > 0 and person_conf.shape[0] > 0:\n            ref_points = get_ref_points(person_conf)\n            prev_ref_points = get_ref_points(self.prev_person_conf)\n\n            # MA: this munkres implementation assumes that num(rows) >= num(columns)\n            if person_conf.shape[0] <= self.prev_person_conf.shape[0]:\n                cost_matrix = scipy.spatial.distance.cdist(ref_points, prev_ref_points)\n            else:\n                cost_matrix = scipy.spatial.distance.cdist(prev_ref_points, ref_points)\n\n            assert (cost_matrix.shape[0] <= cost_matrix.shape[1])\n\n            conf_assign = self.mk.compute(cost_matrix)\n\n            if person_conf.shape[0] > self.prev_person_conf.shape[0]:\n                conf_assign = [(idx2, idx1) for idx1, idx2 in conf_assign]\n                cost_matrix = cost_matrix.T\n\n            for pidx1, pidx2 in conf_assign:\n                if cost_matrix[pidx1][pidx2] < min_match_dist:\n                    color_assignment[pidx1] = self.prev_color_assignment[pidx2]\n\n        print(""#tracked objects:"", len(color_assignment))\n\n        free_coloridx = sorted(list(set(range(len(self.track_colors))).difference(set(color_assignment.values()))),\n                               reverse=True)\n\n        for pidx in range(num_people):\n            # color_idx = pidx % len(self.track_colors)\n            if pidx in color_assignment:\n                color_idx = color_assignment[pidx]\n            else:\n                if len(free_coloridx) > 0:\n                    color_idx = free_coloridx[-1]\n                    free_coloridx = free_coloridx[:-1]\n                else:\n                    color_idx = np.random.randint(len(self.track_colors))\n\n                color_assignment[pidx] = color_idx\n\n            assert (color_idx < len(self.track_colors))\n\n            if np.sum(person_conf[pidx, :, 0] > 0) < draw_conf_min_count:\n                continue\n\n            for kidx1, kidx2 in dataset.get_pose_segments():\n                p1 = (int(math.floor(person_conf[pidx, kidx1, 0])), int(math.floor(person_conf[pidx, kidx1, 1])))\n                p2 = (int(math.floor(person_conf[pidx, kidx2, 0])), int(math.floor(person_conf[pidx, kidx2, 1])))\n\n                if check_point(p1[0], p1[1], minx, miny, maxx, maxy) and check_point(p2[0], p2[1], minx, miny, maxx,\n                                                                                     maxy):\n                    color = np.array(self.track_colors[color_idx][::-1], dtype=np.float64) / 255.0\n                    plt.plot([p1[0], p2[0]], [p1[1], p2[1]], marker=\'o\', linestyle=\'solid\', linewidth=2.0, color=color)\n\n\n        self.prev_person_conf = person_conf\n        self.prev_color_assignment = color_assignment\n\n\n\nkeypoint_colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (0, 255, 255), (255, 0, 255), (255, 255, 0),\n                   (255, 0, 0), (0, 255, 0), (0, 0, 255), (0, 255, 255), (255, 0, 255), (255, 255, 0),\n                   (255, 0, 0), (0, 255, 0), (255, 0, 0), (0, 255, 0), (0, 0, 255)]\n\ndef visualize_detections(cfg, img, detections):\n    vis_scale = 1.0\n    marker_size = 4\n\n    minx = 2 * marker_size\n    miny = 2 * marker_size\n    maxx = img.shape[1] - 2 * marker_size\n    maxy = img.shape[0] - 2 * marker_size\n\n    unPos = detections.coord\n    joints_to_visualise = range(cfg.num_joints)\n    visim_dets = img.copy()\n    for pidx in joints_to_visualise:\n        for didx in range(unPos[pidx].shape[0]):\n            cur_x = unPos[pidx][didx, 0] * vis_scale\n            cur_y = unPos[pidx][didx, 1] * vis_scale\n\n            # / cfg.global_scale\n\n            if check_point(cur_x, cur_y, minx, miny, maxx, maxy):\n                _npcircle(visim_dets,\n                          cur_x, cur_y,\n                          marker_size,\n                          keypoint_colors[pidx])\n    return visim_dets\n'"
nnet/losses.py,4,"b'import tensorflow as tf\n\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.framework import ops\n\nimport tensorflow.contrib.losses as tf_losses\n\n\ndef huber_loss(labels, predictions, weight=1.0, k=1.0, scope=None):\n    """"""Define a huber loss  https://en.wikipedia.org/wiki/Huber_loss\n      tensor: tensor to regularize.\n      k: value of k in the huber loss\n      scope: Optional scope for op_scope.\n\n    Huber loss:\n    f(x) = if |x| <= k:\n              0.5 * x^2\n           else:\n              k * |x| - 0.5 * k^2\n\n    Returns:\n      the L1 loss op.\n\n    http://concise-bio.readthedocs.io/en/latest/_modules/concise/tf_helper.html\n    """"""\n    with ops.name_scope(scope, ""absolute_difference"",\n                        [predictions, labels]) as scope:\n        predictions.get_shape().assert_is_compatible_with(labels.get_shape())\n        if weight is None:\n            raise ValueError(""`weight` cannot be None"")\n        predictions = math_ops.to_float(predictions)\n        labels = math_ops.to_float(labels)\n        diff = math_ops.subtract(predictions, labels)\n        abs_diff = tf.abs(diff)\n        losses = tf.where(abs_diff < k,\n                          0.5 * tf.square(diff),\n                          k * abs_diff - 0.5 * k ** 2)\n        return tf.losses.compute_weighted_loss(losses, weight)\n'"
nnet/net_factory.py,0,b'from nnet.pose_net import PoseNet\n\n\ndef pose_net(cfg):\n    if cfg.video:\n        from nnet.pose_seq_net import PoseSeqNet\n        cls = PoseSeqNet\n    else:\n        cls = PoseNet\n    return cls(cfg)\n'
nnet/pose_net.py,8,"b'import re\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.contrib.slim.nets import resnet_v1\n\nfrom dataset.pose_dataset import Batch\nfrom nnet import losses\n\n\nnet_funcs = {\'resnet_50\': resnet_v1.resnet_v1_50,\n             \'resnet_101\': resnet_v1.resnet_v1_101}\n\n\ndef prediction_layer(cfg, input, name, num_outputs):\n    with slim.arg_scope([slim.conv2d, slim.conv2d_transpose], padding=\'SAME\',\n                        activation_fn=None, normalizer_fn=None,\n                        weights_regularizer=slim.l2_regularizer(cfg.weight_decay)):\n        with tf.variable_scope(name):\n            pred = slim.conv2d_transpose(input, num_outputs,\n                                         kernel_size=[3, 3], stride=2,\n                                         scope=\'block4\')\n            return pred\n\n\ndef get_batch_spec(cfg):\n    num_joints = cfg.num_joints\n    batch_size = cfg.batch_size\n    batch_spec = {\n        Batch.inputs: [batch_size, None, None, 3],\n        Batch.part_score_targets: [batch_size, None, None, num_joints],\n        Batch.part_score_weights: [batch_size, None, None, num_joints]\n    }\n    if cfg.location_refinement:\n        batch_spec[Batch.locref_targets] = [batch_size, None, None, num_joints * 2]\n        batch_spec[Batch.locref_mask] = [batch_size, None, None, num_joints * 2]\n    if cfg.pairwise_predict:\n        batch_spec[Batch.pairwise_targets] = [batch_size, None, None, num_joints * (num_joints - 1) * 2]\n        batch_spec[Batch.pairwise_mask] = [batch_size, None, None, num_joints * (num_joints - 1) * 2]\n    return batch_spec\n\n\nclass PoseNet:\n    def __init__(self, cfg):\n        self.cfg = cfg\n\n    def extract_features(self, inputs):\n        net_fun = net_funcs[self.cfg.net_type]\n\n        mean = tf.constant(self.cfg.mean_pixel,\n                           dtype=tf.float32, shape=[1, 1, 1, 3], name=\'img_mean\')\n        im_centered = inputs - mean\n\n        with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n            net, end_points = net_fun(im_centered, global_pool=False, output_stride=16, is_training=False)\n\n        return net, end_points\n\n    def prediction_layers(self, features, end_points, reuse=None, no_interm=False, scope=\'pose\'):\n        cfg = self.cfg\n\n        num_layers = re.findall(""resnet_([0-9]*)"", cfg.net_type)[0]\n        layer_name = \'resnet_v1_{}\'.format(num_layers) + \'/block{}/unit_{}/bottleneck_v1\'\n\n        out = {}\n        with tf.variable_scope(scope, reuse=reuse):\n            out[\'part_pred\'] = prediction_layer(cfg, features, \'part_pred\',\n                                                cfg.num_joints)\n            if cfg.location_refinement:\n                out[\'locref\'] = prediction_layer(cfg, features, \'locref_pred\',\n                                                 cfg.num_joints * 2)\n            if cfg.pairwise_predict:\n                out[\'pairwise_pred\'] = prediction_layer(cfg, features, \'pairwise_pred\',\n                                                       cfg.num_joints * (cfg.num_joints - 1) * 2)\n            if cfg.intermediate_supervision and not no_interm:\n                interm_name = layer_name.format(3, cfg.intermediate_supervision_layer)\n                block_interm_out = end_points[interm_name]\n                out[\'part_pred_interm\'] = prediction_layer(cfg, block_interm_out,\n                                                           \'intermediate_supervision\',\n                                                           cfg.num_joints)\n\n        return out\n\n    def get_net(self, inputs):\n        net, end_points = self.extract_features(inputs)\n        return self.prediction_layers(net, end_points)\n\n    def test(self, inputs):\n        heads = self.get_net(inputs)\n        return self.add_test_layers(heads)\n\n    def add_test_layers(self, heads):\n        prob = tf.sigmoid(heads[\'part_pred\'])\n        outputs = {\'part_prob\': prob}\n        if self.cfg.location_refinement:\n            outputs[\'locref\'] = heads[\'locref\']\n        if self.cfg.pairwise_predict:\n            outputs[\'pairwise_pred\'] = heads[\'pairwise_pred\']\n        return outputs\n\n    def part_detection_loss(self, heads, batch, locref, pairwise, intermediate):\n        cfg = self.cfg\n\n        weigh_part_predictions = cfg.weigh_part_predictions\n        part_score_weights = batch[Batch.part_score_weights] if weigh_part_predictions else 1.0\n\n        def add_part_loss(pred_layer):\n            return tf.losses.sigmoid_cross_entropy(batch[Batch.part_score_targets],\n                                                   heads[pred_layer],\n                                                   part_score_weights)\n\n        loss = {}\n        loss[\'part_loss\'] = add_part_loss(\'part_pred\')\n        total_loss = loss[\'part_loss\']\n        if intermediate:\n            loss[\'part_loss_interm\'] = add_part_loss(\'part_pred_interm\')\n            total_loss = total_loss + loss[\'part_loss_interm\']\n\n        if locref:\n            locref_pred = heads[\'locref\']\n            locref_targets = batch[Batch.locref_targets]\n            locref_weights = batch[Batch.locref_mask]\n\n            loss_func = losses.huber_loss if cfg.locref_huber_loss else tf.losses.mean_squared_error\n            loss[\'locref_loss\'] = cfg.locref_loss_weight * loss_func(locref_targets, locref_pred, locref_weights)\n            total_loss = total_loss + loss[\'locref_loss\']\n\n        if pairwise:\n            pairwise_pred = heads[\'pairwise_pred\']\n            pairwise_targets = batch[Batch.pairwise_targets]\n            pairwise_weights = batch[Batch.pairwise_mask]\n\n            loss_func = losses.huber_loss if cfg.pairwise_huber_loss else tf.losses.mean_squared_error\n            loss[\'pairwise_loss\'] = cfg.pairwise_loss_weight * loss_func(pairwise_targets, pairwise_pred,\n                                                                         pairwise_weights)\n            total_loss = total_loss + loss[\'pairwise_loss\']\n\n        # loss[\'total_loss\'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize)\n        loss[\'total_loss\'] = total_loss\n        return loss\n\n    def train(self, batch):\n        cfg = self.cfg\n\n        intermediate = cfg.intermediate_supervision\n        locref = cfg.location_refinement\n        pairwise = cfg.pairwise_predict\n\n        heads = self.get_net(batch[Batch.inputs])\n        return self.part_detection_loss(heads, batch, locref, pairwise, intermediate)\n'"
nnet/predict.py,5,"b'import numpy as np\n\nimport tensorflow as tf\n\nfrom nnet.net_factory import pose_net\n\n\ndef setup_pose_prediction(cfg):\n    inputs = tf.placeholder(tf.float32, shape=[cfg.batch_size, None, None, 3])\n\n    outputs = pose_net(cfg).test(inputs)\n\n    restorer = tf.train.Saver()\n\n    sess = tf.Session()\n\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.local_variables_initializer())\n\n    # Restore variables from disk.\n    restorer.restore(sess, cfg.init_weights)\n\n    return sess, inputs, outputs\n\n\ndef extract_cnn_output(outputs_np, cfg, pairwise_stats = None):\n    scmap = outputs_np[\'part_prob\']\n    scmap = np.squeeze(scmap)\n    locref = None\n    pairwise_diff = None\n    if cfg.location_refinement:\n        locref = np.squeeze(outputs_np[\'locref\'])\n        shape = locref.shape\n        locref = np.reshape(locref, (shape[0], shape[1], -1, 2))\n        locref *= cfg.locref_stdev\n    if cfg.pairwise_predict:\n        pairwise_diff = np.squeeze(outputs_np[\'pairwise_pred\'])\n        shape = pairwise_diff.shape\n        pairwise_diff = np.reshape(pairwise_diff, (shape[0], shape[1], -1, 2))\n        num_joints = cfg.num_joints\n        for pair in pairwise_stats:\n            pair_id = (num_joints - 1) * pair[0] + pair[1] - int(pair[0] < pair[1])\n            pairwise_diff[:, :, pair_id, 0] *= pairwise_stats[pair][""std""][0]\n            pairwise_diff[:, :, pair_id, 0] += pairwise_stats[pair][""mean""][0]\n            pairwise_diff[:, :, pair_id, 1] *= pairwise_stats[pair][""std""][1]\n            pairwise_diff[:, :, pair_id, 1] += pairwise_stats[pair][""mean""][1]\n    return scmap, locref, pairwise_diff\n\n\ndef argmax_pose_predict(scmap, offmat, stride):\n    """"""Combine scoremat and offsets to the final pose.""""""\n    num_joints = scmap.shape[2]\n    pose = []\n    for joint_idx in range(num_joints):\n        maxloc = np.unravel_index(np.argmax(scmap[:, :, joint_idx]),\n                                  scmap[:, :, joint_idx].shape)\n        offset = np.array(offmat[maxloc][joint_idx])[::-1] if offmat is not None else 0\n        pos_f8 = (np.array(maxloc).astype(\'float\') * stride + 0.5 * stride +\n                  offset)\n        pose.append(np.hstack((pos_f8[::-1],\n                               [scmap[maxloc][joint_idx]])))\n    return np.array(pose)\n\n\ndef argmax_arrows_predict(scmap, offmat, pairwise_diff, stride):\n    num_joints = scmap.shape[2]\n    arrows = {}\n    for joint_idx in range(num_joints):\n        maxloc = np.unravel_index(np.argmax(scmap[:, :, joint_idx]),\n                                  scmap[:, :, joint_idx].shape)\n        offset = np.array(offmat[maxloc][joint_idx])[::-1] if offmat is not None else 0\n        pos_f8 = (np.array(maxloc).astype(\'float\') * stride + 0.5 * stride +\n                  offset)[::-1]\n        for joint_idx_end in range(num_joints):\n            if joint_idx_end != joint_idx:\n                pair_id = (num_joints - 1) * joint_idx + joint_idx_end - int(joint_idx < joint_idx_end)\n                difference = np.array(pairwise_diff[maxloc][pair_id])[::-1] if pairwise_diff is not None else 0\n                pos_f8_end = (np.array(maxloc).astype(\'float\') * stride + 0.5 * stride + difference)[::-1]\n                arrows[(joint_idx, joint_idx_end)] = (pos_f8, pos_f8_end)\n\n    return arrows'"
util/config.py,0,"b'import os\nimport pprint\nimport logging\n\nimport yaml\nfrom easydict import EasyDict as edict\n\nimport util.default_config\n\n\ncfg = util.default_config.cfg\n\n\ndef _merge_a_into_b(a, b):\n    """"""Merge config dictionary a into config dictionary b, clobbering the\n    options in b whenever they are also specified in a.\n    """"""\n    if type(a) is not edict:\n        return\n\n    for k, v in a.items():\n        # a must specify keys that are in b\n        #if k not in b:\n        #    raise KeyError(\'{} is not a valid config key\'.format(k))\n\n        # recursively merge dicts\n        if type(v) is edict:\n            try:\n                _merge_a_into_b(a[k], b[k])\n            except:\n                print(\'Error under config key: {}\'.format(k))\n                raise\n        else:\n            b[k] = v\n\n\ndef cfg_from_file(filename):\n    """"""Load a config from file filename and merge it into the default options.\n    """"""\n    with open(filename, \'r\') as f:\n        yaml_cfg = edict(yaml.load(f))\n\n    _merge_a_into_b(yaml_cfg, cfg)\n\n    logging.info(""Config:\\n""+pprint.pformat(cfg))\n    return cfg\n\n\ndef load_config(filename = ""pose_cfg.yaml""):\n    if \'POSE_PARAM_PATH\' in os.environ:\n        filename = os.environ[\'POSE_PARAM_PATH\'] + \'/\' + filename\n    return cfg_from_file(filename)\n\n\nif __name__ == ""__main__"":\n    print(load_config())'"
util/default_config.py,0,"b'from easydict import EasyDict as edict\n\ncfg = edict()\n\ncfg.stride = 8.0\ncfg.weigh_part_predictions = False\ncfg.weigh_negatives = False\ncfg.fg_fraction = 0.25\ncfg.weigh_only_present_joints = False\ncfg.mean_pixel = [123.68, 116.779, 103.939]\ncfg.shuffle = True\ncfg.snapshot_prefix = ""snapshot""\ncfg.log_dir = ""log""\ncfg.global_scale = 1.0\ncfg.location_refinement = False\ncfg.locref_stdev = 7.2801\ncfg.locref_loss_weight = 1.0\ncfg.locref_huber_loss = True\ncfg.optimizer = ""sgd""\ncfg.intermediate_supervision = False\ncfg.intermediate_supervision_layer = 12\ncfg.regularize = False\ncfg.weight_decay = 0.0001\ncfg.mirror = False\ncfg.crop = False\ncfg.crop_pad = 0\ncfg.scoremap_dir = ""test""\ncfg.dataset = """"\ncfg.dataset_type = ""default""  # options: ""default"", ""coco""\ncfg.use_gt_segm = False\ncfg.batch_size = 1\ncfg.video = False\ncfg.video_batch = False\ncfg.sparse_graph = []\ncfg.pairwise_stats_collect = False\ncfg.pairwise_stats_fn = ""pairwise_stats.mat""\ncfg.pairwise_predict = False\ncfg.pairwise_huber_loss = True\ncfg.pairwise_loss_weight = 1.0\ncfg.tensorflow_pairwise_order = True\n'"
util/logging.py,0,"b""import logging\n\n\ndef setup_logging():\n    FORMAT = '%(asctime)-15s %(message)s'\n    logging.basicConfig(filename='log.txt', filemode='w',\n                        datefmt='%Y-%m-%d %H:%M:%S',\n                        level=logging.INFO, format=FORMAT)\n    console = logging.StreamHandler()\n    console.setLevel(logging.INFO)\n    logging.getLogger('').addHandler(console)"""
util/mscoco_util.py,0,"b'import numpy as np\nimport scipy.io\n\n\ndef multi_dim_argmax(arr):\n    """""" Getting argmax over the first 2 axes """"""\n    m, n, i, j = arr.shape\n    arr = np.reshape(arr, (m * n, i, j))\n    return np.unravel_index(np.argmax(arr, 0), (m, n))\n\n\ndef interweave_matrices(x, y, z):\n    """"""Combine matrices by concatenating their cols: x.col(1),y.col(1),z.col(1) ... x.col(n),y.col(n),z.col(n) """"""\n    num_joints = x.shape[1]\n    id_x = (np.arange(0, num_joints, 0.5) + 1).astype(\'int\')\n    id_y = (np.arange(0, num_joints, 0.5) + 0.5).astype(\'int\')\n    id_z = (np.arange(0, num_joints, 0.5) + 0).astype(\'int\')\n    x = np.insert(x, id_x, 0, axis=1)\n    y = np.insert(y, id_y, 0, axis=1)\n    z = np.insert(z, id_z, 0, axis=1)\n    return x + y + z\n\n\ndef pose_predict_with_gt_segm(scmap, offmat, stride, gt_segm, coco_id):\n    """"""Generate all poses in an image using segmentations""""""\n    if gt_segm.size == 0:\n        img_keypoints = []\n    else:\n        num_persons = gt_segm.shape[2]\n        num_joints = scmap.shape[2]\n        init_w = gt_segm.shape[1]\n        init_h = gt_segm.shape[0]\n\n        upsized_w = scmap.shape[1] * stride\n        upsized_h = scmap.shape[0] * stride\n\n        diff_w_l = int((upsized_w - init_w) // 2)\n        diff_w_r = int(upsized_w - init_w - diff_w_l)\n\n        diff_h_u = int((upsized_h - init_h) // 2)\n        diff_h_d = int(upsized_h - init_h - diff_h_u)\n\n        padded_gt_segm = np.pad(gt_segm, ((diff_h_u, diff_h_d), (diff_w_l, diff_w_r), (0, 0)), \'constant\')\n        upsized_scmap = scipy.ndimage.zoom(scmap, (stride, stride, 1))\n\n        person_joint_scmap = padded_gt_segm[:, :, :, np.newaxis] * upsized_scmap[:, :, np.newaxis, :]\n        upsized_maxloc = multi_dim_argmax(person_joint_scmap)\n        maxloc_0 = (upsized_maxloc[0]//stride).astype(int)\n        maxloc_1 = (upsized_maxloc[1]//stride).astype(int)\n        indices = np.array([np.arange(num_joints)] * num_persons)\n        offset = np.moveaxis(offmat[(maxloc_0, maxloc_1, indices)][:,:,::-1], -1, 0) if offmat is not None else 0\n        pos_f8 = (np.array((maxloc_0, maxloc_1)).astype(\'float\') * stride + 0.5 * stride + offset)\n        v = scmap[(maxloc_0, maxloc_1, indices)]\n        img_keypoints = (interweave_matrices(pos_f8[1].astype(\'int\'), pos_f8[0].astype(\'int\'), v)).tolist()\n\n    coco_img_results = []\n    for person_keypoints in img_keypoints:\n        person_result = {}\n        person_result[""image_id""] = coco_id\n        person_result[""category_id""] = 1\n        person_result[""keypoints""] = person_keypoints\n        person_result[""score""] = 1\n        coco_img_results.append(person_result)\n\n    return coco_img_results\n\n'"
util/visualize.py,0,"b'import math\n\nimport numpy as np\nfrom scipy.misc import imresize\nimport matplotlib\nmatplotlib.use(\'TkAgg\')\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\n\ndef _npcircle(image, cx, cy, radius, color, transparency=0.0):\n    """"""Draw a circle on an image using only numpy methods.""""""\n    radius = int(radius)\n    cx = int(cx)\n    cy = int(cy)\n    y, x = np.ogrid[-radius: radius, -radius: radius]\n    index = x**2 + y**2 <= radius**2\n    image[cy-radius:cy+radius, cx-radius:cx+radius][index] = (\n        image[cy-radius:cy+radius, cx-radius:cx+radius][index].astype(\'float32\') * transparency +\n        np.array(color).astype(\'float32\') * (1.0 - transparency)).astype(\'uint8\')\n\n\ndef check_point(cur_x, cur_y, minx, miny, maxx, maxy):\n    return minx < cur_x < maxx and miny < cur_y < maxy\n\n\ndef visualize_joints(image, pose):\n    marker_size = 8\n    minx = 2 * marker_size\n    miny = 2 * marker_size\n    maxx = image.shape[1] - 2 * marker_size\n    maxy = image.shape[0] - 2 * marker_size\n    num_joints = pose.shape[0]\n\n    visim = image.copy()\n    colors = [[255, 0, 0], [0, 255, 0], [0, 0, 255], [0, 245, 255], [255, 131, 250], [255, 255, 0],\n              [255, 0, 0], [0, 255, 0], [0, 0, 255], [0, 245, 255], [255, 131, 250], [255, 255, 0],\n              [0, 0, 0], [255, 255, 255], [255, 0, 0], [0, 255, 0], [0, 0, 255]]\n    for p_idx in range(num_joints):\n        cur_x = pose[p_idx, 0]\n        cur_y = pose[p_idx, 1]\n        if check_point(cur_x, cur_y, minx, miny, maxx, maxy):\n            _npcircle(visim,\n                      cur_x, cur_y,\n                      marker_size,\n                      colors[p_idx],\n                      0.0)\n    return visim\n\n\ndef show_heatmaps(cfg, img, scmap, pose, cmap=""jet""):\n    interp = ""bilinear""\n    all_joints = cfg.all_joints\n    all_joints_names = cfg.all_joints_names\n    subplot_width = 3\n    subplot_height = math.ceil((len(all_joints) + 1) / subplot_width)\n    f, axarr = plt.subplots(subplot_height, subplot_width)\n    for pidx, part in enumerate(all_joints):\n        plot_j = (pidx + 1) // subplot_width\n        plot_i = (pidx + 1) % subplot_width\n        scmap_part = np.sum(scmap[:, :, part], axis=2)\n        scmap_part = imresize(scmap_part, 8.0, interp=\'bicubic\')\n        scmap_part = np.lib.pad(scmap_part, ((4, 0), (4, 0)), \'minimum\')\n        curr_plot = axarr[plot_j, plot_i]\n        curr_plot.set_title(all_joints_names[pidx])\n        curr_plot.axis(\'off\')\n        curr_plot.imshow(img, interpolation=interp)\n        curr_plot.imshow(scmap_part, alpha=.5, cmap=cmap, interpolation=interp)\n\n    curr_plot = axarr[0, 0]\n    curr_plot.set_title(\'Pose\')\n    curr_plot.axis(\'off\')\n    curr_plot.imshow(visualize_joints(img, pose))\n\n    plt.show()\n\ndef show_arrows(cfg, img, pose, arrows):\n    fig = plt.figure()\n    a = fig.add_subplot(2, 2, 1)\n    plt.imshow(img)\n    a.set_title(\'Initial Image\')\n\n\n    b = fig.add_subplot(2, 2, 2)\n    plt.imshow(img)\n    b.set_title(\'Predicted Pairwise Differences\')\n\n    color_opt=[\'r\', \'g\', \'b\', \'c\', \'m\', \'y\', \'k\']\n    joint_pairs = [(6, 5), (6, 11), (6, 8), (6, 15), (6, 0)]\n    color_legends = []\n    for id, joint_pair in enumerate(joint_pairs):\n        end_joint_side = (""r "" if joint_pair[1] % 2 == 0 else ""l "") if joint_pair[1] != 0 else """"\n        end_joint_name = end_joint_side + cfg.all_joints_names[int(math.ceil(joint_pair[1] / 2))]\n        start = arrows[joint_pair][0]\n        end = arrows[joint_pair][1]\n        b.arrow(start[0], start[1], end[0]-start[0], end[1]-start[1], head_width=3, head_length=6, fc=color_opt[id], ec=color_opt[id], label=end_joint_name)\n        color_legend = mpatches.Patch(color=color_opt[id], label=end_joint_name)\n        color_legends.append(color_legend)\n\n    plt.legend(handles=color_legends, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n    plt.show()\n\ndef waitforbuttonpress():\n    plt.waitforbuttonpress(timeout=1)'"
lib/multicut_cython/setup.py,0,"b'from setuptools import setup\nfrom Cython.Build import cythonize\nfrom distutils.extension import Extension\nfrom sys import platform as _platform\nimport os\nimport numpy as np\n\n#openmp_arg = \'-fopenmp\'\n#if _platform == ""win32"":\n#    openmp_arg = \'-openmp\'\n\nextensions = [\n  Extension(\n    \'multicut\', [\'multicut.pyx\', \'src/nl-lmp.cxx\'],\n    language=""c++"",\n    include_dirs=[np.get_include(), \'.\', \'include\', \'src\'],\n    extra_compile_args=[\'-std=c++11\',\'-O3\', \'-DHAVE_CPP11_INITIALIZER_LISTS\'],\n    extra_link_args=[\'-std=c++11\', \'-L./\']\n  )\n] \n\nsetup(\n    name = \'multicut\',\n    ext_modules = cythonize(extensions)\n)\n'"
lib/nms_cython/setup.py,0,"b'from setuptools import setup\nfrom Cython.Build import cythonize\nfrom distutils.extension import Extension\nfrom sys import platform as _platform\nimport os\nimport numpy as np\n\n#openmp_arg = \'-fopenmp\'\n#if _platform == ""win32"":\n#    openmp_arg = \'-openmp\'\n\nextensions = [\n  Extension(\n    \'nms_grid\', [\'nms_grid.pyx\'],\n    language=""c++"",\n    include_dirs=[np.get_include(), \'.\',\'include\'],\n    extra_compile_args=[\'-DILOUSESTL\',\'-DIL_STD\',\'-std=c++11\',\'-O3\'],\n    extra_link_args=[\'-std=c++11\']\n  )\n] \n\nsetup(\n    name = \'nms_grid\',\n    ext_modules = cythonize(extensions)\n)\n'"
lib/coco/PythonAPI/setup.py,0,"b'from distutils.core import setup\nfrom Cython.Build import cythonize\nfrom distutils.extension import Extension\nimport numpy as np\n\n# To compile and install locally run ""python setup.py build_ext --inplace""\n# To install library to Python site-packages run ""python setup.py build_ext install""\n\next_modules = [\n    Extension(\n        \'pycocotools._mask\',\n        sources=[\'../common/maskApi.c\', \'pycocotools/_mask.pyx\'],\n        include_dirs = [np.get_include(), \'../common\'],\n        extra_compile_args=[\'-Wno-cpp\', \'-Wno-unused-function\', \'-std=c99\'],\n    )\n]\n\nsetup(name=\'pycocotools\',\n      packages=[\'pycocotools\'],\n      package_dir = {\'pycocotools\': \'pycocotools\'},\n      version=\'2.0\',\n      ext_modules=\n          cythonize(ext_modules)\n      )'"
lib/coco/PythonAPI/pycocotools/__init__.py,0,"b""__author__ = 'tylin'\n"""
lib/coco/PythonAPI/pycocotools/coco.py,0,"b'__author__ = \'tylin\'\n__version__ = \'2.0\'\n# Interface for accessing the Microsoft COCO dataset.\n\n# Microsoft COCO is a large image dataset designed for object detection,\n# segmentation, and caption generation. pycocotools is a Python API that\n# assists in loading, parsing and visualizing the annotations in COCO.\n# Please visit http://mscoco.org/ for more information on COCO, including\n# for the data, paper, and tutorials. The exact format of the annotations\n# is also described on the COCO website. For example usage of the pycocotools\n# please see pycocotools_demo.ipynb. In addition to this API, please download both\n# the COCO images and annotations in order to run the demo.\n\n# An alternative to using the API is to load the annotations directly\n# into Python dictionary\n# Using the API provides additional utility functions. Note that this API\n# supports both *instance* and *caption* annotations. In the case of\n# captions not all functions are defined (e.g. categories are undefined).\n\n# The following API functions are defined:\n#  COCO       - COCO api class that loads COCO annotation file and prepare data structures.\n#  decodeMask - Decode binary mask M encoded via run-length encoding.\n#  encodeMask - Encode binary mask M using run-length encoding.\n#  getAnnIds  - Get ann ids that satisfy given filter conditions.\n#  getCatIds  - Get cat ids that satisfy given filter conditions.\n#  getImgIds  - Get img ids that satisfy given filter conditions.\n#  loadAnns   - Load anns with the specified ids.\n#  loadCats   - Load cats with the specified ids.\n#  loadImgs   - Load imgs with the specified ids.\n#  annToMask  - Convert segmentation in an annotation to binary mask.\n#  showAnns   - Display the specified annotations.\n#  loadRes    - Load algorithm results and create API for accessing them.\n#  download   - Download COCO images from mscoco.org server.\n# Throughout the API ""ann""=annotation, ""cat""=category, and ""img""=image.\n# Help on each functions can be accessed by: ""help COCO>function"".\n\n# See also COCO>decodeMask,\n# COCO>encodeMask, COCO>getAnnIds, COCO>getCatIds,\n# COCO>getImgIds, COCO>loadAnns, COCO>loadCats,\n# COCO>loadImgs, COCO>annToMask, COCO>showAnns\n\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2014.\n# Licensed under the Simplified BSD License [see bsd.txt]\n\nimport json\nimport time\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import PatchCollection\nfrom matplotlib.patches import Polygon\nimport numpy as np\nimport copy\nimport itertools\nfrom . import mask as maskUtils\nimport os\nfrom collections import defaultdict\nimport sys\nPYTHON_VERSION = sys.version_info[0]\nif PYTHON_VERSION == 2:\n    from urllib import urlretrieve\nelif PYTHON_VERSION == 3:\n    from urllib.request import urlretrieve\n\nclass COCO:\n    def __init__(self, annotation_file=None):\n        """"""\n        Constructor of Microsoft COCO helper class for reading and visualizing annotations.\n        :param annotation_file (str): location of annotation file\n        :param image_folder (str): location to the folder that hosts images.\n        :return:\n        """"""\n        # load dataset\n        self.dataset,self.anns,self.cats,self.imgs = dict(),dict(),dict(),dict()\n        self.imgToAnns, self.catToImgs = defaultdict(list), defaultdict(list)\n        if not annotation_file == None:\n            print(\'loading annotations into memory...\')\n            tic = time.time()\n            dataset = json.load(open(annotation_file, \'r\'))\n            assert type(dataset)==dict, \'annotation file format {} not supported\'.format(type(dataset))\n            print(\'Done (t={:0.2f}s)\'.format(time.time()- tic))\n            self.dataset = dataset\n            self.createIndex()\n\n    def createIndex(self):\n        # create index\n        print(\'creating index...\')\n        anns, cats, imgs = {}, {}, {}\n        imgToAnns,catToImgs = defaultdict(list),defaultdict(list)\n        if \'annotations\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                imgToAnns[ann[\'image_id\']].append(ann)\n                anns[ann[\'id\']] = ann\n\n        if \'images\' in self.dataset:\n            for img in self.dataset[\'images\']:\n                imgs[img[\'id\']] = img\n\n        if \'categories\' in self.dataset:\n            for cat in self.dataset[\'categories\']:\n                cats[cat[\'id\']] = cat\n\n        if \'annotations\' in self.dataset and \'categories\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                catToImgs[ann[\'category_id\']].append(ann[\'image_id\'])\n\n        print(\'index created!\')\n\n        # create class members\n        self.anns = anns\n        self.imgToAnns = imgToAnns\n        self.catToImgs = catToImgs\n        self.imgs = imgs\n        self.cats = cats\n\n    def info(self):\n        """"""\n        Print information about the annotation file.\n        :return:\n        """"""\n        for key, value in self.dataset[\'info\'].items():\n            print(\'{}: {}\'.format(key, value))\n\n    def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n        """"""\n        Get ann ids that satisfy given filter conditions. default skips that filter\n        :param imgIds  (int array)     : get anns for given imgs\n               catIds  (int array)     : get anns for given cats\n               areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n               iscrowd (boolean)       : get anns for given crowd label (False or True)\n        :return: ids (int array)       : integer array of ann ids\n        """"""\n        imgIds = imgIds if type(imgIds) == list else [imgIds]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(imgIds) == len(catIds) == len(areaRng) == 0:\n            anns = self.dataset[\'annotations\']\n        else:\n            if not len(imgIds) == 0:\n                lists = [self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns]\n                anns = list(itertools.chain.from_iterable(lists))\n            else:\n                anns = self.dataset[\'annotations\']\n            anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann[\'category_id\'] in catIds]\n            anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann[\'area\'] > areaRng[0] and ann[\'area\'] < areaRng[1]]\n        if not iscrowd == None:\n            ids = [ann[\'id\'] for ann in anns if ann[\'iscrowd\'] == iscrowd]\n        else:\n            ids = [ann[\'id\'] for ann in anns]\n        return ids\n\n    def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n        """"""\n        filtering parameters. default skips that filter.\n        :param catNms (str array)  : get cats for given cat names\n        :param supNms (str array)  : get cats for given supercategory names\n        :param catIds (int array)  : get cats for given cat ids\n        :return: ids (int array)   : integer array of cat ids\n        """"""\n        catNms = catNms if type(catNms) == list else [catNms]\n        supNms = supNms if type(supNms) == list else [supNms]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(catNms) == len(supNms) == len(catIds) == 0:\n            cats = self.dataset[\'categories\']\n        else:\n            cats = self.dataset[\'categories\']\n            cats = cats if len(catNms) == 0 else [cat for cat in cats if cat[\'name\']          in catNms]\n            cats = cats if len(supNms) == 0 else [cat for cat in cats if cat[\'supercategory\'] in supNms]\n            cats = cats if len(catIds) == 0 else [cat for cat in cats if cat[\'id\']            in catIds]\n        ids = [cat[\'id\'] for cat in cats]\n        return ids\n\n    def getImgIds(self, imgIds=[], catIds=[]):\n        \'\'\'\n        Get img ids that satisfy given filter conditions.\n        :param imgIds (int array) : get imgs for given ids\n        :param catIds (int array) : get imgs with all given cats\n        :return: ids (int array)  : integer array of img ids\n        \'\'\'\n        imgIds = imgIds if type(imgIds) == list else [imgIds]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(imgIds) == len(catIds) == 0:\n            ids = self.imgs.keys()\n        else:\n            ids = set(imgIds)\n            for i, catId in enumerate(catIds):\n                if i == 0 and len(ids) == 0:\n                    ids = set(self.catToImgs[catId])\n                else:\n                    ids &= set(self.catToImgs[catId])\n        return list(ids)\n\n    def loadAnns(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying anns\n        :return: anns (object array) : loaded ann objects\n        """"""\n        if type(ids) == list:\n            return [self.anns[id] for id in ids]\n        elif type(ids) == int:\n            return [self.anns[ids]]\n\n    def loadCats(self, ids=[]):\n        """"""\n        Load cats with the specified ids.\n        :param ids (int array)       : integer ids specifying cats\n        :return: cats (object array) : loaded cat objects\n        """"""\n        if type(ids) == list:\n            return [self.cats[id] for id in ids]\n        elif type(ids) == int:\n            return [self.cats[ids]]\n\n    def loadImgs(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying img\n        :return: imgs (object array) : loaded img objects\n        """"""\n        if type(ids) == list:\n            return [self.imgs[id] for id in ids]\n        elif type(ids) == int:\n            return [self.imgs[ids]]\n\n    def showAnns(self, anns):\n        """"""\n        Display the specified annotations.\n        :param anns (array of object): annotations to display\n        :return: None\n        """"""\n        if len(anns) == 0:\n            return 0\n        if \'segmentation\' in anns[0] or \'keypoints\' in anns[0]:\n            datasetType = \'instances\'\n        elif \'caption\' in anns[0]:\n            datasetType = \'captions\'\n        else:\n            raise Exception(\'datasetType not supported\')\n        if datasetType == \'instances\':\n            ax = plt.gca()\n            ax.set_autoscale_on(False)\n            polygons = []\n            color = []\n            for ann in anns:\n                c = (np.random.random((1, 3))*0.6+0.4).tolist()[0]\n                if \'segmentation\' in ann:\n                    if type(ann[\'segmentation\']) == list:\n                        # polygon\n                        for seg in ann[\'segmentation\']:\n                            poly = np.array(seg).reshape((int(len(seg)/2), 2))\n                            polygons.append(Polygon(poly))\n                            color.append(c)\n                    else:\n                        # mask\n                        t = self.imgs[ann[\'image_id\']]\n                        if type(ann[\'segmentation\'][\'counts\']) == list:\n                            rle = maskUtils.frPyObjects([ann[\'segmentation\']], t[\'height\'], t[\'width\'])\n                        else:\n                            rle = [ann[\'segmentation\']]\n                        m = maskUtils.decode(rle)\n                        img = np.ones( (m.shape[0], m.shape[1], 3) )\n                        if ann[\'iscrowd\'] == 1:\n                            color_mask = np.array([2.0,166.0,101.0])/255\n                        if ann[\'iscrowd\'] == 0:\n                            color_mask = np.random.random((1, 3)).tolist()[0]\n                        for i in range(3):\n                            img[:,:,i] = color_mask[i]\n                        ax.imshow(np.dstack( (img, m*0.5) ))\n                if \'keypoints\' in ann and type(ann[\'keypoints\']) == list:\n                    # turn skeleton into zero-based index\n                    sks = np.array(self.loadCats(ann[\'category_id\'])[0][\'skeleton\'])-1\n                    kp = np.array(ann[\'keypoints\'])\n                    x = kp[0::3]\n                    y = kp[1::3]\n                    v = kp[2::3]\n                    for sk in sks:\n                        if np.all(v[sk]>0):\n                            plt.plot(x[sk],y[sk], linewidth=3, color=c)\n                    plt.plot(x[v>0], y[v>0],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=\'k\',markeredgewidth=2)\n                    plt.plot(x[v>1], y[v>1],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=c, markeredgewidth=2)\n            p = PatchCollection(polygons, facecolor=color, linewidths=0, alpha=0.4)\n            ax.add_collection(p)\n            p = PatchCollection(polygons, facecolor=\'none\', edgecolors=color, linewidths=2)\n            ax.add_collection(p)\n        elif datasetType == \'captions\':\n            for ann in anns:\n                print(ann[\'caption\'])\n\n    def loadRes(self, resFile):\n        """"""\n        Load result file and return a result api object.\n        :param   resFile (str)     : file name of result file\n        :return: res (obj)         : result api object\n        """"""\n        res = COCO()\n        res.dataset[\'images\'] = [img for img in self.dataset[\'images\']]\n\n        print(\'Loading and preparing results...\')\n        tic = time.time()\n        if type(resFile) == str or type(resFile) == unicode:\n            anns = json.load(open(resFile))\n        elif type(resFile) == np.ndarray:\n            anns = self.loadNumpyAnnotations(resFile)\n        else:\n            anns = resFile\n        assert type(anns) == list, \'results in not an array of objects\'\n        annsImgIds = [ann[\'image_id\'] for ann in anns]\n        assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \\\n               \'Results do not correspond to current coco set\'\n        if \'caption\' in anns[0]:\n            imgIds = set([img[\'id\'] for img in res.dataset[\'images\']]) & set([ann[\'image_id\'] for ann in anns])\n            res.dataset[\'images\'] = [img for img in res.dataset[\'images\'] if img[\'id\'] in imgIds]\n            for id, ann in enumerate(anns):\n                ann[\'id\'] = id+1\n        elif \'bbox\' in anns[0] and not anns[0][\'bbox\'] == []:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                bb = ann[\'bbox\']\n                x1, x2, y1, y2 = [bb[0], bb[0]+bb[2], bb[1], bb[1]+bb[3]]\n                if not \'segmentation\' in ann:\n                    ann[\'segmentation\'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n                ann[\'area\'] = bb[2]*bb[3]\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'segmentation\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                # now only support compressed RLE format as segmentation results\n                ann[\'area\'] = maskUtils.area(ann[\'segmentation\'])\n                if not \'bbox\' in ann:\n                    ann[\'bbox\'] = maskUtils.toBbox(ann[\'segmentation\'])\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'keypoints\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                s = ann[\'keypoints\']\n                x = s[0::3]\n                y = s[1::3]\n                x0,x1,y0,y1 = np.min(x), np.max(x), np.min(y), np.max(y)\n                ann[\'area\'] = (x1-x0)*(y1-y0)\n                ann[\'id\'] = id + 1\n                ann[\'bbox\'] = [x0,y0,x1-x0,y1-y0]\n        print(\'DONE (t={:0.2f}s)\'.format(time.time()- tic))\n\n        res.dataset[\'annotations\'] = anns\n        res.createIndex()\n        return res\n\n    def download(self, tarDir = None, imgIds = [] ):\n        \'\'\'\n        Download COCO images from mscoco.org server.\n        :param tarDir (str): COCO results directory name\n               imgIds (list): images to be downloaded\n        :return:\n        \'\'\'\n        if tarDir is None:\n            print(\'Please specify target directory\')\n            return -1\n        if len(imgIds) == 0:\n            imgs = self.imgs.values()\n        else:\n            imgs = self.loadImgs(imgIds)\n        N = len(imgs)\n        if not os.path.exists(tarDir):\n            os.makedirs(tarDir)\n        for i, img in enumerate(imgs):\n            tic = time.time()\n            fname = os.path.join(tarDir, img[\'file_name\'])\n            if not os.path.exists(fname):\n                urlretrieve(img[\'coco_url\'], fname)\n            print(\'downloaded {}/{} images (t={:0.1f}s)\'.format(i, N, time.time()- tic))\n\n    def loadNumpyAnnotations(self, data):\n        """"""\n        Convert result data from a numpy array [Nx7] where each row contains {imageID,x1,y1,w,h,score,class}\n        :param  data (numpy.ndarray)\n        :return: annotations (python nested list)\n        """"""\n        print(\'Converting ndarray to lists...\')\n        assert(type(data) == np.ndarray)\n        print(data.shape)\n        assert(data.shape[1] == 7)\n        N = data.shape[0]\n        ann = []\n        for i in range(N):\n            if i % 1000000 == 0:\n                print(\'{}/{}\'.format(i,N))\n            ann += [{\n                \'image_id\'  : int(data[i, 0]),\n                \'bbox\'  : [ data[i, 1], data[i, 2], data[i, 3], data[i, 4] ],\n                \'score\' : data[i, 5],\n                \'category_id\': int(data[i, 6]),\n                }]\n        return ann\n\n    def annToRLE(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE to RLE.\n        :return: binary mask (numpy 2D array)\n        """"""\n        t = self.imgs[ann[\'image_id\']]\n        h, w = t[\'height\'], t[\'width\']\n        segm = ann[\'segmentation\']\n        if type(segm) == list:\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            rles = maskUtils.frPyObjects(segm, h, w)\n            rle = maskUtils.merge(rles)\n        elif type(segm[\'counts\']) == list:\n            # uncompressed RLE\n            rle = maskUtils.frPyObjects(segm, h, w)\n        else:\n            # rle\n            rle = ann[\'segmentation\']\n        return rle\n\n    def annToMask(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n        :return: binary mask (numpy 2D array)\n        """"""\n        rle = self.annToRLE(ann)\n        m = maskUtils.decode(rle)\n        return m'"
lib/coco/PythonAPI/pycocotools/cocoeval.py,0,"b'__author__ = \'tsungyi\'\n\nimport numpy as np\nimport datetime\nimport time\nfrom collections import defaultdict\nfrom . import mask as maskUtils\nimport copy\n\nclass COCOeval:\n    # Interface for evaluating detection on the Microsoft COCO dataset.\n    #\n    # The usage for CocoEval is as follows:\n    #  cocoGt=..., cocoDt=...       # load dataset and results\n    #  E = CocoEval(cocoGt,cocoDt); # initialize CocoEval object\n    #  E.params.recThrs = ...;      # set parameters as desired\n    #  E.evaluate();                # run per image evaluation\n    #  E.accumulate();              # accumulate per image results\n    #  E.summarize();               # display summary metrics of results\n    # For example usage see evalDemo.m and http://mscoco.org/.\n    #\n    # The evaluation parameters are as follows (defaults in brackets):\n    #  imgIds     - [all] N img ids to use for evaluation\n    #  catIds     - [all] K cat ids to use for evaluation\n    #  iouThrs    - [.5:.05:.95] T=10 IoU thresholds for evaluation\n    #  recThrs    - [0:.01:1] R=101 recall thresholds for evaluation\n    #  areaRng    - [...] A=4 object area ranges for evaluation\n    #  maxDets    - [1 10 100] M=3 thresholds on max detections per image\n    #  iouType    - [\'segm\'] set iouType to \'segm\', \'bbox\' or \'keypoints\'\n    #  iouType replaced the now DEPRECATED useSegm parameter.\n    #  useCats    - [1] if true use category labels for evaluation\n    # Note: if useCats=0 category labels are ignored as in proposal scoring.\n    # Note: multiple areaRngs [Ax2] and maxDets [Mx1] can be specified.\n    #\n    # evaluate(): evaluates detections on every image and every category and\n    # concats the results into the ""evalImgs"" with fields:\n    #  dtIds      - [1xD] id for each of the D detections (dt)\n    #  gtIds      - [1xG] id for each of the G ground truths (gt)\n    #  dtMatches  - [TxD] matching gt id at each IoU or 0\n    #  gtMatches  - [TxG] matching dt id at each IoU or 0\n    #  dtScores   - [1xD] confidence of each dt\n    #  gtIgnore   - [1xG] ignore flag for each gt\n    #  dtIgnore   - [TxD] ignore flag for each dt at each IoU\n    #\n    # accumulate(): accumulates the per-image, per-category evaluation\n    # results in ""evalImgs"" into the dictionary ""eval"" with fields:\n    #  params     - parameters used for evaluation\n    #  date       - date evaluation was performed\n    #  counts     - [T,R,K,A,M] parameter dimensions (see above)\n    #  precision  - [TxRxKxAxM] precision for every evaluation setting\n    #  recall     - [TxKxAxM] max recall for every evaluation setting\n    # Note: precision and recall==-1 for settings with no gt objects.\n    #\n    # See also coco, mask, pycocoDemo, pycocoEvalDemo\n    #\n    # Microsoft COCO Toolbox.      version 2.0\n    # Data, paper, and tutorials available at:  http://mscoco.org/\n    # Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n    # Licensed under the Simplified BSD License [see coco/license.txt]\n    def __init__(self, cocoGt=None, cocoDt=None, iouType=\'segm\'):\n        \'\'\'\n        Initialize CocoEval using coco APIs for gt and dt\n        :param cocoGt: coco object with ground truth annotations\n        :param cocoDt: coco object with detection results\n        :return: None\n        \'\'\'\n        if not iouType:\n            print(\'iouType not specified. use default iouType segm\')\n        self.cocoGt   = cocoGt              # ground truth COCO API\n        self.cocoDt   = cocoDt              # detections COCO API\n        self.params   = {}                  # evaluation parameters\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results [KxAxI] elements\n        self.eval     = {}                  # accumulated evaluation results\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        self.params = Params(iouType=iouType) # parameters\n        self._paramsEval = {}               # parameters for evaluation\n        self.stats = []                     # result summarization\n        self.ious = {}                      # ious between all gts and dts\n        if not cocoGt is None:\n            self.params.imgIds = sorted(cocoGt.getImgIds())\n            self.params.catIds = sorted(cocoGt.getCatIds())\n\n\n    def _prepare(self):\n        \'\'\'\n        Prepare ._gts and ._dts for evaluation based on params\n        :return: None\n        \'\'\'\n        def _toMask(anns, coco):\n            # modify ann[\'segmentation\'] by reference\n            for ann in anns:\n                rle = coco.annToRLE(ann)\n                ann[\'segmentation\'] = rle\n        p = self.params\n        if p.useCats:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n        else:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds))\n\n        # convert ground truth to mask if iouType == \'segm\'\n        if p.iouType == \'segm\':\n            _toMask(gts, self.cocoGt)\n            _toMask(dts, self.cocoDt)\n        # set ignore flag\n        for gt in gts:\n            gt[\'ignore\'] = gt[\'ignore\'] if \'ignore\' in gt else 0\n            gt[\'ignore\'] = \'iscrowd\' in gt and gt[\'iscrowd\']\n            if p.iouType == \'keypoints\':\n                gt[\'ignore\'] = (gt[\'num_keypoints\'] == 0) or gt[\'ignore\']\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        for gt in gts:\n            self._gts[gt[\'image_id\'], gt[\'category_id\']].append(gt)\n        for dt in dts:\n            self._dts[dt[\'image_id\'], dt[\'category_id\']].append(dt)\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results\n        self.eval     = {}                  # accumulated evaluation results\n\n    def evaluate(self):\n        \'\'\'\n        Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n        :return: None\n        \'\'\'\n        tic = time.time()\n        print(\'Running per image evaluation...\')\n        p = self.params\n        # add backward compatibility if useSegm is specified in params\n        if not p.useSegm is None:\n            p.iouType = \'segm\' if p.useSegm == 1 else \'bbox\'\n            print(\'useSegm (deprecated) is not None. Running {} evaluation\'.format(p.iouType))\n        print(\'Evaluate annotation type *{}*\'.format(p.iouType))\n        p.imgIds = list(np.unique(p.imgIds))\n        if p.useCats:\n            p.catIds = list(np.unique(p.catIds))\n        p.maxDets = sorted(p.maxDets)\n        self.params=p\n\n        self._prepare()\n        # loop through images, area range, max detection number\n        catIds = p.catIds if p.useCats else [-1]\n\n        if p.iouType == \'segm\' or p.iouType == \'bbox\':\n            computeIoU = self.computeIoU\n        elif p.iouType == \'keypoints\':\n            computeIoU = self.computeOks\n        self.ious = {(imgId, catId): computeIoU(imgId, catId) \\\n                        for imgId in p.imgIds\n                        for catId in catIds}\n\n        evaluateImg = self.evaluateImg\n        maxDet = p.maxDets[-1]\n        self.evalImgs = [evaluateImg(imgId, catId, areaRng, maxDet)\n                 for catId in catIds\n                 for areaRng in p.areaRng\n                 for imgId in p.imgIds\n             ]\n        self._paramsEval = copy.deepcopy(self.params)\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format(toc-tic))\n\n    def computeIoU(self, imgId, catId):\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return []\n        inds = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in inds]\n        if len(dt) > p.maxDets[-1]:\n            dt=dt[0:p.maxDets[-1]]\n\n        if p.iouType == \'segm\':\n            g = [g[\'segmentation\'] for g in gt]\n            d = [d[\'segmentation\'] for d in dt]\n        elif p.iouType == \'bbox\':\n            g = [g[\'bbox\'] for g in gt]\n            d = [d[\'bbox\'] for d in dt]\n        else:\n            raise Exception(\'unknown iouType for iou computation\')\n\n        # compute iou between each dt and gt region\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        ious = maskUtils.iou(d,g,iscrowd)\n        return ious\n\n    def computeOks(self, imgId, catId):\n        p = self.params\n        # dimention here should be Nxm\n        gts = self._gts[imgId, catId]\n        dts = self._dts[imgId, catId]\n        inds = np.argsort([-d[\'score\'] for d in dts], kind=\'mergesort\')\n        dts = [dts[i] for i in inds]\n        if len(dts) > p.maxDets[-1]:\n            dts = dts[0:p.maxDets[-1]]\n        # if len(gts) == 0 and len(dts) == 0:\n        if len(gts) == 0 or len(dts) == 0:\n            return []\n        ious = np.zeros((len(dts), len(gts)))\n        sigmas = np.array([.26, .25, .25, .35, .35, .79, .79, .72, .72, .62,.62, 1.07, 1.07, .87, .87, .89, .89])/10.0\n        vars = (sigmas * 2)**2\n        k = len(sigmas)\n        # compute oks between each detection and ground truth object\n        for j, gt in enumerate(gts):\n            # create bounds for ignore regions(double the gt bbox)\n            g = np.array(gt[\'keypoints\'])\n            xg = g[0::3]; yg = g[1::3]; vg = g[2::3]\n            k1 = np.count_nonzero(vg > 0)\n            bb = gt[\'bbox\']\n            x0 = bb[0] - bb[2]; x1 = bb[0] + bb[2] * 2\n            y0 = bb[1] - bb[3]; y1 = bb[1] + bb[3] * 2\n            for i, dt in enumerate(dts):\n                d = np.array(dt[\'keypoints\'])\n                xd = d[0::3]; yd = d[1::3]\n                if k1>0:\n                    # measure the per-keypoint distance if keypoints visible\n                    dx = xd - xg\n                    dy = yd - yg\n                else:\n                    # measure minimum distance to keypoints in (x0,y0) & (x1,y1)\n                    z = np.zeros((k))\n                    dx = np.max((z, x0-xd),axis=0)+np.max((z, xd-x1),axis=0)\n                    dy = np.max((z, y0-yd),axis=0)+np.max((z, yd-y1),axis=0)\n                e = (dx**2 + dy**2) / vars / (gt[\'area\']+np.spacing(1)) / 2\n                if k1 > 0:\n                    e=e[vg > 0]\n                ious[i, j] = np.sum(np.exp(-e)) / e.shape[0]\n        return ious\n\n    def evaluateImg(self, imgId, catId, aRng, maxDet):\n        \'\'\'\n        perform evaluation for single category and image\n        :return: dict (single image results)\n        \'\'\'\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return None\n\n        for g in gt:\n            if g[\'ignore\'] or (g[\'area\']<aRng[0] or g[\'area\']>aRng[1]):\n                g[\'_ignore\'] = 1\n            else:\n                g[\'_ignore\'] = 0\n\n        # sort dt highest score first, sort gt ignore last\n        gtind = np.argsort([g[\'_ignore\'] for g in gt], kind=\'mergesort\')\n        gt = [gt[i] for i in gtind]\n        dtind = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in dtind[0:maxDet]]\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        # load computed ious\n        ious = self.ious[imgId, catId][:, gtind] if len(self.ious[imgId, catId]) > 0 else self.ious[imgId, catId]\n\n        T = len(p.iouThrs)\n        G = len(gt)\n        D = len(dt)\n        gtm  = np.zeros((T,G))\n        dtm  = np.zeros((T,D))\n        gtIg = np.array([g[\'_ignore\'] for g in gt])\n        dtIg = np.zeros((T,D))\n        if not len(ious)==0:\n            for tind, t in enumerate(p.iouThrs):\n                for dind, d in enumerate(dt):\n                    # information about best match so far (m=-1 -> unmatched)\n                    iou = min([t,1-1e-10])\n                    m   = -1\n                    for gind, g in enumerate(gt):\n                        # if this gt already matched, and not a crowd, continue\n                        if gtm[tind,gind]>0 and not iscrowd[gind]:\n                            continue\n                        # if dt matched to reg gt, and on ignore gt, stop\n                        if m>-1 and gtIg[m]==0 and gtIg[gind]==1:\n                            break\n                        # continue to next gt unless better match made\n                        if ious[dind,gind] < iou:\n                            continue\n                        # if match successful and best so far, store appropriately\n                        iou=ious[dind,gind]\n                        m=gind\n                    # if match made store id of match for both dt and gt\n                    if m ==-1:\n                        continue\n                    dtIg[tind,dind] = gtIg[m]\n                    dtm[tind,dind]  = gt[m][\'id\']\n                    gtm[tind,m]     = d[\'id\']\n        # set unmatched detections outside of area range to ignore\n        a = np.array([d[\'area\']<aRng[0] or d[\'area\']>aRng[1] for d in dt]).reshape((1, len(dt)))\n        dtIg = np.logical_or(dtIg, np.logical_and(dtm==0, np.repeat(a,T,0)))\n        # store results for given image and category\n        return {\n                \'image_id\':     imgId,\n                \'category_id\':  catId,\n                \'aRng\':         aRng,\n                \'maxDet\':       maxDet,\n                \'dtIds\':        [d[\'id\'] for d in dt],\n                \'gtIds\':        [g[\'id\'] for g in gt],\n                \'dtMatches\':    dtm,\n                \'gtMatches\':    gtm,\n                \'dtScores\':     [d[\'score\'] for d in dt],\n                \'gtIgnore\':     gtIg,\n                \'dtIgnore\':     dtIg,\n            }\n\n    def accumulate(self, p = None):\n        \'\'\'\n        Accumulate per image evaluation results and store the result in self.eval\n        :param p: input params for evaluation\n        :return: None\n        \'\'\'\n        print(\'Accumulating evaluation results...\')\n        tic = time.time()\n        if not self.evalImgs:\n            print(\'Please run evaluate() first\')\n        # allows input customized parameters\n        if p is None:\n            p = self.params\n        p.catIds = p.catIds if p.useCats == 1 else [-1]\n        T           = len(p.iouThrs)\n        R           = len(p.recThrs)\n        K           = len(p.catIds) if p.useCats else 1\n        A           = len(p.areaRng)\n        M           = len(p.maxDets)\n        precision   = -np.ones((T,R,K,A,M)) # -1 for the precision of absent categories\n        recall      = -np.ones((T,K,A,M))\n\n        # create dictionary for future indexing\n        _pe = self._paramsEval\n        catIds = _pe.catIds if _pe.useCats else [-1]\n        setK = set(catIds)\n        setA = set(map(tuple, _pe.areaRng))\n        setM = set(_pe.maxDets)\n        setI = set(_pe.imgIds)\n        # get inds to evaluate\n        k_list = [n for n, k in enumerate(p.catIds)  if k in setK]\n        m_list = [m for n, m in enumerate(p.maxDets) if m in setM]\n        a_list = [n for n, a in enumerate(map(lambda x: tuple(x), p.areaRng)) if a in setA]\n        i_list = [n for n, i in enumerate(p.imgIds)  if i in setI]\n        I0 = len(_pe.imgIds)\n        A0 = len(_pe.areaRng)\n        # retrieve E at each category, area range, and max number of detections\n        for k, k0 in enumerate(k_list):\n            Nk = k0*A0*I0\n            for a, a0 in enumerate(a_list):\n                Na = a0*I0\n                for m, maxDet in enumerate(m_list):\n                    E = [self.evalImgs[Nk + Na + i] for i in i_list]\n                    E = [e for e in E if not e is None]\n                    if len(E) == 0:\n                        continue\n                    dtScores = np.concatenate([e[\'dtScores\'][0:maxDet] for e in E])\n\n                    # different sorting method generates slightly different results.\n                    # mergesort is used to be consistent as Matlab implementation.\n                    inds = np.argsort(-dtScores, kind=\'mergesort\')\n\n                    dtm  = np.concatenate([e[\'dtMatches\'][:,0:maxDet] for e in E], axis=1)[:,inds]\n                    dtIg = np.concatenate([e[\'dtIgnore\'][:,0:maxDet]  for e in E], axis=1)[:,inds]\n                    gtIg = np.concatenate([e[\'gtIgnore\'] for e in E])\n                    npig = np.count_nonzero(gtIg==0 )\n                    if npig == 0:\n                        continue\n                    tps = np.logical_and(               dtm,  np.logical_not(dtIg) )\n                    fps = np.logical_and(np.logical_not(dtm), np.logical_not(dtIg) )\n\n                    tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\n                    fp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)\n                    for t, (tp, fp) in enumerate(zip(tp_sum, fp_sum)):\n                        tp = np.array(tp)\n                        fp = np.array(fp)\n                        nd = len(tp)\n                        rc = tp / npig\n                        pr = tp / (fp+tp+np.spacing(1))\n                        q  = np.zeros((R,))\n\n                        if nd:\n                            recall[t,k,a,m] = rc[-1]\n                        else:\n                            recall[t,k,a,m] = 0\n\n                        # numpy is slow without cython optimization for accessing elements\n                        # use python array gets significant speed improvement\n                        pr = pr.tolist(); q = q.tolist()\n\n                        for i in range(nd-1, 0, -1):\n                            if pr[i] > pr[i-1]:\n                                pr[i-1] = pr[i]\n\n                        inds = np.searchsorted(rc, p.recThrs, side=\'left\')\n                        try:\n                            for ri, pi in enumerate(inds):\n                                q[ri] = pr[pi]\n                        except:\n                            pass\n                        precision[t,:,k,a,m] = np.array(q)\n        self.eval = {\n            \'params\': p,\n            \'counts\': [T, R, K, A, M],\n            \'date\': datetime.datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\'),\n            \'precision\': precision,\n            \'recall\':   recall,\n        }\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format( toc-tic))\n\n    def summarize(self):\n        \'\'\'\n        Compute and display summary metrics for evaluation results.\n        Note this functin can *only* be applied on the default parameter setting\n        \'\'\'\n        def _summarize( ap=1, iouThr=None, areaRng=\'all\', maxDets=100 ):\n            p = self.params\n            iStr = \' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}\'\n            titleStr = \'Average Precision\' if ap == 1 else \'Average Recall\'\n            typeStr = \'(AP)\' if ap==1 else \'(AR)\'\n            iouStr = \'{:0.2f}:{:0.2f}\'.format(p.iouThrs[0], p.iouThrs[-1]) \\\n                if iouThr is None else \'{:0.2f}\'.format(iouThr)\n\n            aind = [i for i, aRng in enumerate(p.areaRngLbl) if aRng == areaRng]\n            mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]\n            if ap == 1:\n                # dimension of precision: [TxRxKxAxM]\n                s = self.eval[\'precision\']\n                # IoU\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,:,aind,mind]\n            else:\n                # dimension of recall: [TxKxAxM]\n                s = self.eval[\'recall\']\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,aind,mind]\n            if len(s[s>-1])==0:\n                mean_s = -1\n            else:\n                mean_s = np.mean(s[s>-1])\n            print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n            return mean_s\n        def _summarizeDets():\n            stats = np.zeros((12,))\n            stats[0] = _summarize(1)\n            stats[1] = _summarize(1, iouThr=.5, maxDets=self.params.maxDets[2])\n            stats[2] = _summarize(1, iouThr=.75, maxDets=self.params.maxDets[2])\n            stats[3] = _summarize(1, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[4] = _summarize(1, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[5] = _summarize(1, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            stats[6] = _summarize(0, maxDets=self.params.maxDets[0])\n            stats[7] = _summarize(0, maxDets=self.params.maxDets[1])\n            stats[8] = _summarize(0, maxDets=self.params.maxDets[2])\n            stats[9] = _summarize(0, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[10] = _summarize(0, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[11] = _summarize(0, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            return stats\n        def _summarizeKps():\n            stats = np.zeros((10,))\n            stats[0] = _summarize(1, maxDets=20)\n            stats[1] = _summarize(1, maxDets=20, iouThr=.5)\n            stats[2] = _summarize(1, maxDets=20, iouThr=.75)\n            stats[3] = _summarize(1, maxDets=20, areaRng=\'medium\')\n            stats[4] = _summarize(1, maxDets=20, areaRng=\'large\')\n            stats[5] = _summarize(0, maxDets=20)\n            stats[6] = _summarize(0, maxDets=20, iouThr=.5)\n            stats[7] = _summarize(0, maxDets=20, iouThr=.75)\n            stats[8] = _summarize(0, maxDets=20, areaRng=\'medium\')\n            stats[9] = _summarize(0, maxDets=20, areaRng=\'large\')\n            return stats\n        if not self.eval:\n            raise Exception(\'Please run accumulate() first\')\n        iouType = self.params.iouType\n        if iouType == \'segm\' or iouType == \'bbox\':\n            summarize = _summarizeDets\n        elif iouType == \'keypoints\':\n            summarize = _summarizeKps\n        self.stats = summarize()\n\n    def __str__(self):\n        self.summarize()\n\nclass Params:\n    \'\'\'\n    Params for coco evaluation api\n    \'\'\'\n    def setDetParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [1, 10, 100]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [0 ** 2, 32 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'small\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def setKpParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [20]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def __init__(self, iouType=\'segm\'):\n        if iouType == \'segm\' or iouType == \'bbox\':\n            self.setDetParams()\n        elif iouType == \'keypoints\':\n            self.setKpParams()\n        else:\n            raise Exception(\'iouType not supported\')\n        self.iouType = iouType\n        # useSegm is deprecated\n        self.useSegm = None'"
lib/coco/PythonAPI/pycocotools/mask.py,0,"b'__author__ = \'tsungyi\'\n\nimport pycocotools._mask as _mask\n\n# Interface for manipulating masks stored in RLE format.\n#\n# RLE is a simple yet efficient format for storing binary masks. RLE\n# first divides a vector (or vectorized image) into a series of piecewise\n# constant regions and then for each piece simply stores the length of\n# that piece. For example, given M=[0 0 1 1 1 0 1] the RLE counts would\n# be [2 3 1 1], or for M=[1 1 1 1 1 1 0] the counts would be [0 6 1]\n# (note that the odd counts are always the numbers of zeros). Instead of\n# storing the counts directly, additional compression is achieved with a\n# variable bitrate representation based on a common scheme called LEB128.\n#\n# Compression is greatest given large piecewise constant regions.\n# Specifically, the size of the RLE is proportional to the number of\n# *boundaries* in M (or for an image the number of boundaries in the y\n# direction). Assuming fairly simple shapes, the RLE representation is\n# O(sqrt(n)) where n is number of pixels in the object. Hence space usage\n# is substantially lower, especially for large simple objects (large n).\n#\n# Many common operations on masks can be computed directly using the RLE\n# (without need for decoding). This includes computations such as area,\n# union, intersection, etc. All of these operations are linear in the\n# size of the RLE, in other words they are O(sqrt(n)) where n is the area\n# of the object. Computing these operations on the original mask is O(n).\n# Thus, using the RLE can result in substantial computational savings.\n#\n# The following API functions are defined:\n#  encode         - Encode binary masks using RLE.\n#  decode         - Decode binary masks encoded via RLE.\n#  merge          - Compute union or intersection of encoded masks.\n#  iou            - Compute intersection over union between masks.\n#  area           - Compute area of encoded masks.\n#  toBbox         - Get bounding boxes surrounding encoded masks.\n#  frPyObjects    - Convert polygon, bbox, and uncompressed RLE to encoded RLE mask.\n#\n# Usage:\n#  Rs     = encode( masks )\n#  masks  = decode( Rs )\n#  R      = merge( Rs, intersect=false )\n#  o      = iou( dt, gt, iscrowd )\n#  a      = area( Rs )\n#  bbs    = toBbox( Rs )\n#  Rs     = frPyObjects( [pyObjects], h, w )\n#\n# In the API the following formats are used:\n#  Rs      - [dict] Run-length encoding of binary masks\n#  R       - dict Run-length encoding of binary mask\n#  masks   - [hxwxn] Binary mask(s) (must have type np.ndarray(dtype=uint8) in column-major order)\n#  iscrowd - [nx1] list of np.ndarray. 1 indicates corresponding gt image has crowd region to ignore\n#  bbs     - [nx4] Bounding box(es) stored as [x y w h]\n#  poly    - Polygon stored as [[x1 y1 x2 y2...],[x1 y1 ...],...] (2D list)\n#  dt,gt   - May be either bounding boxes or encoded masks\n# Both poly and bbs are 0-indexed (bbox=[0 0 1 1] encloses first pixel).\n#\n# Finally, a note about the intersection over union (iou) computation.\n# The standard iou of a ground truth (gt) and detected (dt) object is\n#  iou(gt,dt) = area(intersect(gt,dt)) / area(union(gt,dt))\n# For ""crowd"" regions, we use a modified criteria. If a gt object is\n# marked as ""iscrowd"", we allow a dt to match any subregion of the gt.\n# Choosing gt\' in the crowd gt that best matches the dt can be done using\n# gt\'=intersect(dt,gt). Since by definition union(gt\',dt)=dt, computing\n#  iou(gt,dt,iscrowd) = iou(gt\',dt) = area(intersect(gt,dt)) / area(dt)\n# For crowd gt regions we use this modified criteria above for the iou.\n#\n# To compile run ""python setup.py build_ext --inplace""\n# Please do not contact us for help with compiling.\n#\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n# Licensed under the Simplified BSD License [see coco/license.txt]\n\niou         = _mask.iou\nmerge       = _mask.merge\nfrPyObjects = _mask.frPyObjects\n\ndef encode(bimask):\n    if len(bimask.shape) == 3:\n        return _mask.encode(bimask)\n    elif len(bimask.shape) == 2:\n        h, w = bimask.shape\n        return _mask.encode(bimask.reshape((h, w, 1), order=\'F\'))[0]\n\ndef decode(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.decode(rleObjs)\n    else:\n        return _mask.decode([rleObjs])[:,:,0]\n\ndef area(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.area(rleObjs)\n    else:\n        return _mask.area([rleObjs])[0]\n\ndef toBbox(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.toBbox(rleObjs)\n    else:\n        return _mask.toBbox([rleObjs])[0]'"
