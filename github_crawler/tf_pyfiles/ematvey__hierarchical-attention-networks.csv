file_path,api_count,code
HAN_model.py,45,"b'import tensorflow as tf\nimport tensorflow.contrib.layers as layers\nimport numpy as np\nimport data_util\nfrom model_components import task_specific_attention, bidirectional_rnn\n\n\nclass HANClassifierModel():\n  """""" Implementation of document classification model described in\n    `Hierarchical Attention Networks for Document Classification (Yang et al., 2016)`\n    (https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf)""""""\n\n  def __init__(self,\n               vocab_size,\n               embedding_size,\n               classes,\n               word_cell,\n               sentence_cell,\n               word_output_size,\n               sentence_output_size,\n               max_grad_norm,\n               dropout_keep_proba,\n               is_training=None,\n               learning_rate=1e-4,\n               device=\'/cpu:0\',\n               scope=None):\n    self.vocab_size = vocab_size\n    self.embedding_size = embedding_size\n    self.classes = classes\n    self.word_cell = word_cell\n    self.word_output_size = word_output_size\n    self.sentence_cell = sentence_cell\n    self.sentence_output_size = sentence_output_size\n    self.max_grad_norm = max_grad_norm\n    self.dropout_keep_proba = dropout_keep_proba\n\n    with tf.variable_scope(scope or \'tcm\') as scope:\n      self.global_step = tf.Variable(0, name=\'global_step\', trainable=False)\n\n      if is_training is not None:\n        self.is_training = is_training\n      else:\n        self.is_training = tf.placeholder(dtype=tf.bool, name=\'is_training\')\n\n      self.sample_weights = tf.placeholder(shape=(None,), dtype=tf.float32, name=\'sample_weights\')\n\n      # [document x sentence x word]\n      self.inputs = tf.placeholder(shape=(None, None, None), dtype=tf.int32, name=\'inputs\')\n\n      # [document x sentence]\n      self.word_lengths = tf.placeholder(shape=(None, None), dtype=tf.int32, name=\'word_lengths\')\n\n      # [document]\n      self.sentence_lengths = tf.placeholder(shape=(None,), dtype=tf.int32, name=\'sentence_lengths\')\n\n      # [document]\n      self.labels = tf.placeholder(shape=(None,), dtype=tf.int32, name=\'labels\')\n\n      (self.document_size,\n        self.sentence_size,\n        self.word_size) = tf.unstack(tf.shape(self.inputs))\n\n      self._init_embedding(scope)\n\n      # embeddings cannot be placed on GPU\n      with tf.device(device):\n        self._init_body(scope)\n\n    with tf.variable_scope(\'train\'):\n      self.cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.labels, logits=self.logits)\n\n      self.loss = tf.reduce_mean(tf.multiply(self.cross_entropy, self.sample_weights))\n      tf.summary.scalar(\'loss\', self.loss)\n\n      self.accuracy = tf.reduce_mean(tf.cast(tf.nn.in_top_k(self.logits, self.labels, 1), tf.float32))\n      tf.summary.scalar(\'accuracy\', self.accuracy)\n\n      tvars = tf.trainable_variables()\n\n      grads, global_norm = tf.clip_by_global_norm(\n        tf.gradients(self.loss, tvars),\n        self.max_grad_norm)\n      tf.summary.scalar(\'global_grad_norm\', global_norm)\n\n      opt = tf.train.AdamOptimizer(learning_rate)\n\n      self.train_op = opt.apply_gradients(\n        zip(grads, tvars), name=\'train_op\',\n        global_step=self.global_step)\n\n      self.summary_op = tf.summary.merge_all()\n\n  def _init_embedding(self, scope):\n    with tf.variable_scope(scope):\n      with tf.variable_scope(""embedding"") as scope:\n        self.embedding_matrix = tf.get_variable(\n          name=""embedding_matrix"",\n          shape=[self.vocab_size, self.embedding_size],\n          initializer=layers.xavier_initializer(),\n          dtype=tf.float32)\n        self.inputs_embedded = tf.nn.embedding_lookup(\n          self.embedding_matrix, self.inputs)\n\n  def _init_body(self, scope):\n    with tf.variable_scope(scope):\n\n      word_level_inputs = tf.reshape(self.inputs_embedded, [\n        self.document_size * self.sentence_size,\n        self.word_size,\n        self.embedding_size\n      ])\n      word_level_lengths = tf.reshape(\n        self.word_lengths, [self.document_size * self.sentence_size])\n\n      with tf.variable_scope(\'word\') as scope:\n        word_encoder_output, _ = bidirectional_rnn(\n          self.word_cell, self.word_cell,\n          word_level_inputs, word_level_lengths,\n          scope=scope)\n\n        with tf.variable_scope(\'attention\') as scope:\n          word_level_output = task_specific_attention(\n            word_encoder_output,\n            self.word_output_size,\n            scope=scope)\n\n        with tf.variable_scope(\'dropout\'):\n          word_level_output = layers.dropout(\n            word_level_output, keep_prob=self.dropout_keep_proba,\n            is_training=self.is_training,\n          )\n\n      # sentence_level\n\n      sentence_inputs = tf.reshape(\n        word_level_output, [self.document_size, self.sentence_size, self.word_output_size])\n\n      with tf.variable_scope(\'sentence\') as scope:\n        sentence_encoder_output, _ = bidirectional_rnn(\n          self.sentence_cell, self.sentence_cell, sentence_inputs, self.sentence_lengths, scope=scope)\n\n        with tf.variable_scope(\'attention\') as scope:\n          sentence_level_output = task_specific_attention(\n            sentence_encoder_output, self.sentence_output_size, scope=scope)\n\n        with tf.variable_scope(\'dropout\'):\n          sentence_level_output = layers.dropout(\n            sentence_level_output, keep_prob=self.dropout_keep_proba,\n            is_training=self.is_training,\n          )\n\n      with tf.variable_scope(\'classifier\'):\n        self.logits = layers.fully_connected(\n          sentence_level_output, self.classes, activation_fn=None)\n\n        self.prediction = tf.argmax(self.logits, axis=-1)\n\n  def get_feed_data(self, x, y=None, class_weights=None, is_training=True):\n    x_m, doc_sizes, sent_sizes = data_util.batch(x)\n    fd = {\n      self.inputs: x_m,\n      self.sentence_lengths: doc_sizes,\n      self.word_lengths: sent_sizes,\n    }\n    if y is not None:\n      fd[self.labels] = y\n      if class_weights is not None:\n        fd[self.sample_weights] = [class_weights[yy] for yy in y]\n      else:\n        fd[self.sample_weights] = np.ones(shape=[len(x_m)], dtype=np.float32)\n    fd[self.is_training] = is_training\n    return fd\n\n\nif __name__ == \'__main__\':\n  try:\n    from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple, GRUCell\n  except ImportError:\n    LSTMCell = tf.nn.rnn_cell.LSTMCell\n    LSTMStateTuple = tf.nn.rnn_cell.LSTMStateTuple\n    GRUCell = tf.nn.rnn_cell.GRUCell\n\n  tf.reset_default_graph()\n  with tf.Session() as session:\n    model = HANClassifierModel(\n      vocab_size=10,\n      embedding_size=5,\n      classes=2,\n      word_cell=GRUCell(10),\n      sentence_cell=GRUCell(10),\n      word_output_size=10,\n      sentence_output_size=10,\n      max_grad_norm=5.0,\n      dropout_keep_proba=0.5,\n    )\n    session.run(tf.global_variables_initializer())\n\n    fd = {\n      model.is_training: False,\n      model.inputs: [[\n        [5, 4, 1, 0],\n        [3, 3, 6, 7],\n        [6, 7, 0, 0]\n      ],\n        [\n        [2, 2, 1, 0],\n        [3, 3, 6, 7],\n        [0, 0, 0, 0]\n      ]],\n      model.word_lengths: [\n        [3, 4, 2],\n        [3, 4, 0],\n      ],\n      model.sentence_lengths: [3, 2],\n      model.labels: [0, 1],\n    }\n\n    print(session.run(model.logits, fd))\n    session.run(model.train_op, fd)\n'"
bn_lstm.py,40,"b'# borrowed from https://github.com/OlavHN/bnlstm, updated for r1.0\n\nimport math\nimport numpy as np\nimport tensorflow as tf\n\ntry:\n  from tensorflow.contrib.rnn import RNNCell\nexcept ImportError:\n  RNNCell = tf.nn.rnn_cell.RNNCel\n\n\nclass LSTMCell(RNNCell):\n    """"""Vanilla LSTM implemented with same initializations as BN-LSTM""""""\n    def __init__(self, num_units):\n        self.num_units = num_units\n\n    @property\n    def state_size(self):\n        return (self.num_units, self.num_units)\n\n    @property\n    def output_size(self):\n        return self.num_units\n\n    def __call__(self, x, state, scope=None):\n        with tf.variable_scope(scope or type(self).__name__):\n            c, h = state\n\n            # Keep W_xh and W_hh separate here as well to reuse initialization methods\n            x_size = x.get_shape().as_list()[1]\n            W_xh = tf.get_variable(\'W_xh\',\n                [x_size, 4 * self.num_units],\n                initializer=orthogonal_initializer())\n            W_hh = tf.get_variable(\'W_hh\',\n                [self.num_units, 4 * self.num_units],\n                initializer=bn_lstm_identity_initializer(0.95))\n            bias = tf.get_variable(\'bias\', [4 * self.num_units])\n\n            # hidden = tf.matmul(x, W_xh) + tf.matmul(h, W_hh) + bias\n            # improve speed by concat.\n            concat = tf.concat([x, h], 1)\n            W_both = tf.concat([W_xh, W_hh], 0)\n            hidden = tf.matmul(concat, W_both) + bias\n\n            i, j, f, o = tf.split(hidden, 4, axis=1)\n\n            new_c = c * tf.sigmoid(f) + tf.sigmoid(i) * tf.tanh(j)\n            new_h = tf.tanh(new_c) * tf.sigmoid(o)\n\n            return new_h, (new_c, new_h)\n\nclass BNLSTMCell(RNNCell):\n    """"""Batch normalized LSTM as described in http://arxiv.org/abs/1603.09025""""""\n    def __init__(self, num_units, training):\n        self.num_units = num_units\n        self.training = training\n\n    @property\n    def state_size(self):\n        return (self.num_units, self.num_units)\n\n    @property\n    def output_size(self):\n        return self.num_units\n\n    def __call__(self, x, state, scope=None):\n        with tf.variable_scope(scope or \'bn_lstm\'):\n            c, h = state\n\n            x_size = x.get_shape().as_list()[1]\n            W_xh = tf.get_variable(\'W_xh\',\n                [x_size, 4 * self.num_units],\n                initializer=orthogonal_initializer())\n            W_hh = tf.get_variable(\'W_hh\',\n                [self.num_units, 4 * self.num_units],\n                initializer=bn_lstm_identity_initializer(0.95))\n            bias = tf.get_variable(\'bias\', [4 * self.num_units])\n\n            xh = tf.matmul(x, W_xh)\n            hh = tf.matmul(h, W_hh)\n\n            bn_xh = batch_norm(xh, \'xh\', self.training)\n            bn_hh = batch_norm(hh, \'hh\', self.training)\n\n            hidden = bn_xh + bn_hh + bias\n\n            i, j, f, o = tf.split(hidden, 4, axis=1)\n\n            new_c = c * tf.sigmoid(f) + tf.sigmoid(i) * tf.tanh(j)\n            bn_new_c = batch_norm(new_c, \'c\', self.training)\n\n            new_h = tf.tanh(bn_new_c) * tf.sigmoid(o)\n\n            return new_h, (new_c, new_h)\n\ndef orthogonal(shape):\n    flat_shape = (shape[0], np.prod(shape[1:]))\n    a = np.random.normal(0.0, 1.0, flat_shape)\n    u, _, v = np.linalg.svd(a, full_matrices=False)\n    q = u if u.shape == flat_shape else v\n    return q.reshape(shape)\n\ndef bn_lstm_identity_initializer(scale):\n    def _initializer(shape, dtype=tf.float32, partition_info=None):\n        """"""Ugly cause LSTM params calculated in one matrix multiply""""""\n        size = shape[0]\n        # gate (j) is identity\n        t = np.zeros(shape)\n        t[:, size:size * 2] = np.identity(size) * scale\n        t[:, :size] = orthogonal([size, size])\n        t[:, size * 2:size * 3] = orthogonal([size, size])\n        t[:, size * 3:] = orthogonal([size, size])\n        return tf.constant(t, dtype=dtype)\n\n    return _initializer\n\ndef orthogonal_initializer():\n    def _initializer(shape, dtype=tf.float32, partition_info=None):\n        return tf.constant(orthogonal(shape), dtype)\n    return _initializer\n\ndef batch_norm(x, name_scope, training, epsilon=1e-3, decay=0.999):\n    """"""Assume 2d [batch, values] tensor""""""\n\n    with tf.variable_scope(name_scope):\n        size = x.get_shape().as_list()[1]\n\n        scale = tf.get_variable(\'scale\', [size],\n            initializer=tf.constant_initializer(0.1))\n        offset = tf.get_variable(\'offset\', [size])\n\n        pop_mean = tf.get_variable(\'pop_mean\', [size],\n            initializer=tf.zeros_initializer(),\n            trainable=False)\n        pop_var = tf.get_variable(\'pop_var\', [size],\n            initializer=tf.ones_initializer(),\n            trainable=False)\n        batch_mean, batch_var = tf.nn.moments(x, [0])\n\n        train_mean_op = tf.assign(\n            pop_mean,\n            pop_mean * decay + batch_mean * (1 - decay))\n        train_var_op = tf.assign(\n            pop_var,\n            pop_var * decay + batch_var * (1 - decay))\n\n        def batch_statistics():\n            with tf.control_dependencies([train_mean_op, train_var_op]):\n                return tf.nn.batch_normalization(x, batch_mean, batch_var, offset, scale, epsilon)\n\n        def population_statistics():\n            return tf.nn.batch_normalization(x, pop_mean, pop_var, offset, scale, epsilon)\n\n        return tf.cond(training, batch_statistics, population_statistics)\n'"
bn_lstm_test.py,25,"b'import time\nimport uuid\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.ops.rnn import dynamic_rnn\nfrom bn_lstm import LSTMCell, BNLSTMCell, orthogonal_initializer\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nbatch_size = 100\nhidden_size = 100\n\nmnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)\n\nx = tf.placeholder(tf.float32, [None, 784])\ntraining = tf.placeholder(tf.bool)\n\nx_inp = tf.expand_dims(x, -1)\nlstm = BNLSTMCell(hidden_size, training) #LSTMCell(hidden_size)\n\n#c, h\ninitialState = (\n    tf.random_normal([batch_size, hidden_size], stddev=0.1),\n    tf.random_normal([batch_size, hidden_size], stddev=0.1))\n\noutputs, state = dynamic_rnn(lstm, x_inp, initial_state=initialState, dtype=tf.float32)\n\n_, final_hidden = state\n\nW = tf.get_variable(\'W\', [hidden_size, 10], initializer=orthogonal_initializer())\nb = tf.get_variable(\'b\', [10])\n\ny = tf.nn.softmax(tf.matmul(final_hidden, W) + b)\n\ny_ = tf.placeholder(tf.float32, [None, 10])\n\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n\noptimizer = tf.train.AdamOptimizer()\ngvs = optimizer.compute_gradients(cross_entropy)\ncapped_gvs = [(None if grad is None else tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\ntrain_step = optimizer.apply_gradients(capped_gvs)\n\ncorrect_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n# Summaries\ntf.summary.scalar(""accuracy"", accuracy)\ntf.summary.scalar(""xe_loss"", cross_entropy)\nfor (grad, var), (capped_grad, _) in zip(gvs, capped_gvs):\n    if grad is not None:\n        tf.summary.histogram(\'grad/{}\'.format(var.name), capped_grad)\n        tf.summary.histogram(\'capped_fraction/{}\'.format(var.name),\n            tf.nn.zero_fraction(grad - capped_grad))\n        tf.summary.histogram(\'weight/{}\'.format(var.name), var)\n\nmerged = tf.merge_all_summaries()\n\ninit = tf.initialize_all_variables()\n\nsess = tf.Session()\nsess.run(init)\n\nlogdir = \'logs/\' + str(uuid.uuid4())\nos.makedirs(logdir)\nprint(\'logging to \' + logdir)\nwriter = tf.summary.trainWriter(logdir, sess.graph)\n\ncurrent_time = time.time()\nprint(""Using population statistics (training: False) at test time gives worse results than batch statistics"")\n\nfor i in range(100000):\n    batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n    loss, _ = sess.run([cross_entropy, train_step], feed_dict={x: batch_xs, y_: batch_ys, training: True})\n    step_time = time.time() - current_time\n    current_time = time.time()\n    if i % 100 == 0:\n        batch_xs, batch_ys = mnist.validation.next_batch(batch_size)\nsummary.    _str = sess.run(merged, feed_dict={x: batch_xs, y_: batch_ys, training: False})\n        writer.summary.add_str, i)\n    print(loss, step_time)\n'"
data_util.py,0,"b'import numpy as np\n\n\ndef batch(inputs):\n  batch_size = len(inputs)\n\n  document_sizes = np.array([len(doc) for doc in inputs], dtype=np.int32)\n  document_size = document_sizes.max()\n\n  sentence_sizes_ = [[len(sent) for sent in doc] for doc in inputs]\n  sentence_size = max(map(max, sentence_sizes_))\n\n  b = np.zeros(shape=[batch_size, document_size, sentence_size], dtype=np.int32) # == PAD\n\n  sentence_sizes = np.zeros(shape=[batch_size, document_size], dtype=np.int32)\n  for i, document in enumerate(inputs):\n    for j, sentence in enumerate(document):\n      sentence_sizes[i, j] = sentence_sizes_[i][j]\n      for k, word in enumerate(sentence):\n        b[i, j, k] = word\n\n  return b, document_sizes, sentence_sizes'"
model_components.py,17,"b'import tensorflow as tf\nimport tensorflow.contrib.layers as layers\n\ntry:\n    from tensorflow.contrib.rnn import LSTMStateTuple\nexcept ImportError:\n    LSTMStateTuple = tf.nn.rnn_cell.LSTMStateTuple\n\n\ndef bidirectional_rnn(cell_fw, cell_bw, inputs_embedded, input_lengths,\n                      scope=None):\n    """"""Bidirecional RNN with concatenated outputs and states""""""\n    with tf.variable_scope(scope or ""birnn"") as scope:\n        ((fw_outputs,\n          bw_outputs),\n         (fw_state,\n          bw_state)) = (\n            tf.nn.bidirectional_dynamic_rnn(cell_fw=cell_fw,\n                                            cell_bw=cell_bw,\n                                            inputs=inputs_embedded,\n                                            sequence_length=input_lengths,\n                                            dtype=tf.float32,\n                                            swap_memory=True,\n                                            scope=scope))\n        outputs = tf.concat((fw_outputs, bw_outputs), 2)\n\n        def concatenate_state(fw_state, bw_state):\n            if isinstance(fw_state, LSTMStateTuple):\n                state_c = tf.concat(\n                    (fw_state.c, bw_state.c), 1, name=\'bidirectional_concat_c\')\n                state_h = tf.concat(\n                    (fw_state.h, bw_state.h), 1, name=\'bidirectional_concat_h\')\n                state = LSTMStateTuple(c=state_c, h=state_h)\n                return state\n            elif isinstance(fw_state, tf.Tensor):\n                state = tf.concat((fw_state, bw_state), 1,\n                                  name=\'bidirectional_concat\')\n                return state\n            elif (isinstance(fw_state, tuple) and\n                    isinstance(bw_state, tuple) and\n                    len(fw_state) == len(bw_state)):\n                # multilayer\n                state = tuple(concatenate_state(fw, bw)\n                              for fw, bw in zip(fw_state, bw_state))\n                return state\n\n            else:\n                raise ValueError(\n                    \'unknown state type: {}\'.format((fw_state, bw_state)))\n\n\n        state = concatenate_state(fw_state, bw_state)\n        return outputs, state\n\n\ndef task_specific_attention(inputs, output_size,\n                            initializer=layers.xavier_initializer(),\n                            activation_fn=tf.tanh, scope=None):\n    """"""\n    Performs task-specific attention reduction, using learned\n    attention context vector (constant within task of interest).\n\n    Args:\n        inputs: Tensor of shape [batch_size, units, input_size]\n            `input_size` must be static (known)\n            `units` axis will be attended over (reduced from output)\n            `batch_size` will be preserved\n        output_size: Size of output\'s inner (feature) dimension\n\n    Returns:\n        outputs: Tensor of shape [batch_size, output_dim].\n    """"""\n    assert len(inputs.get_shape()) == 3 and inputs.get_shape()[-1].value is not None\n\n    with tf.variable_scope(scope or \'attention\') as scope:\n        attention_context_vector = tf.get_variable(name=\'attention_context_vector\',\n                                                   shape=[output_size],\n                                                   initializer=initializer,\n                                                   dtype=tf.float32)\n        input_projection = layers.fully_connected(inputs, output_size,\n                                                  activation_fn=activation_fn,\n                                                  scope=scope)\n\n        vector_attn = tf.reduce_sum(tf.multiply(input_projection, attention_context_vector), axis=2, keep_dims=True)\n        attention_weights = tf.nn.softmax(vector_attn, dim=1)\n        weighted_projection = tf.multiply(input_projection, attention_weights)\n\n        outputs = tf.reduce_sum(weighted_projection, axis=1)\n\n        return outputs\n'"
worker.py,14,"b'#!/usr/bin/env python3\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--task\', default=\'yelp\', choices=[\'yelp\'])\nparser.add_argument(\'--mode\', default=\'train\', choices=[\'train\', \'eval\'])\nparser.add_argument(\'--checkpoint-frequency\', type=int, default=100)\nparser.add_argument(\'--eval-frequency\', type=int, default=10000)\nparser.add_argument(\'--batch-size\', type=int, default=30)\nparser.add_argument(""--device"", default=""/cpu:0"")\nparser.add_argument(""--max-grad-norm"", type=float, default=5.0)\nparser.add_argument(""--lr"", type=float, default=0.001)\nargs = parser.parse_args()\n\nimport importlib\nimport os\nimport pickle\nimport random\nimport time\nfrom collections import Counter, defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport spacy\nimport tensorflow as tf\nfrom tensorflow.contrib.tensorboard.plugins import projector\nfrom tqdm import tqdm\n\nimport ujson\nfrom data_util import batch\n\ntask_name = args.task\n\ntask = importlib.import_module(task_name)\n\ncheckpoint_dir = os.path.join(task.train_dir, \'checkpoint\')\ntflog_dir = os.path.join(task.train_dir, \'tflog\')\ncheckpoint_name = task_name + \'-model\'\ncheckpoint_dir = os.path.join(task.train_dir, \'checkpoints\')\ncheckpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n\n# @TODO: move calculation into `task file`\ntrainset = task.read_trainset(epochs=1)\nclass_weights = pd.Series(Counter([l for _, l in trainset]))\nclass_weights = 1/(class_weights/class_weights.mean())\nclass_weights = class_weights.to_dict()\n\nvocab = task.read_vocab()\nlabels = task.read_labels()\n\nclasses = max(labels.values())+1\nvocab_size = task.vocab_size\n\nlabels_rev = {int(v): k for k, v in labels.items()}\nvocab_rev = {int(v): k for k, v in vocab.items()}\n\n\ndef HAN_model_1(session, restore_only=False):\n  """"""Hierarhical Attention Network""""""\n  import tensorflow as tf\n  try:\n    from tensorflow.contrib.rnn import GRUCell, MultiRNNCell, DropoutWrapper\n  except ImportError:\n    MultiRNNCell = tf.nn.rnn_cell.MultiRNNCell\n    GRUCell = tf.nn.rnn_cell.GRUCell\n  from bn_lstm import BNLSTMCell\n  from HAN_model import HANClassifierModel\n\n  is_training = tf.placeholder(dtype=tf.bool, name=\'is_training\')\n\n  cell = BNLSTMCell(80, is_training) # h-h batchnorm LSTMCell\n  # cell = GRUCell(30)\n  cell = MultiRNNCell([cell]*5)\n\n  model = HANClassifierModel(\n      vocab_size=vocab_size,\n      embedding_size=200,\n      classes=classes,\n      word_cell=cell,\n      sentence_cell=cell,\n      word_output_size=100,\n      sentence_output_size=100,\n      device=args.device,\n      learning_rate=args.lr,\n      max_grad_norm=args.max_grad_norm,\n      dropout_keep_proba=0.5,\n      is_training=is_training,\n  )\n\n  saver = tf.train.Saver(tf.global_variables())\n  checkpoint = tf.train.get_checkpoint_state(checkpoint_dir)\n  if checkpoint:\n    print(""Reading model parameters from %s"" % checkpoint.model_checkpoint_path)\n    saver.restore(session, checkpoint.model_checkpoint_path)\n  elif restore_only:\n    raise FileNotFoundError(""Cannot restore model"")\n  else:\n    print(""Created model with fresh parameters"")\n    session.run(tf.global_variables_initializer())\n  # tf.get_default_graph().finalize()\n  return model, saver\n\nmodel_fn = HAN_model_1\n\ndef decode(ex):\n  print(\'text: \' + \'\\n\'.join([\' \'.join([vocab_rev.get(wid, \'<?>\') for wid in sent]) for sent in ex[0]]))\n  print(\'label: \', labels_rev[ex[1]])\n\nprint(\'data loaded\')\n\ndef batch_iterator(dataset, batch_size, max_epochs):\n  for i in range(max_epochs):\n    xb = []\n    yb = []\n    for ex in dataset:\n      x, y = ex\n      xb.append(x)\n      yb.append(y)\n      if len(xb) == batch_size:\n        yield xb, yb\n        xb, yb = [], []\n\n\ndef ev(session, model, dataset):\n  predictions = []\n  labels = []\n  examples = []\n  for x, y in tqdm(batch_iterator(dataset, args.batch_size, 1)):\n    examples.extend(x)\n    labels.extend(y)\n    predictions.extend(session.run(model.prediction, model.get_feed_data(x, is_training=False)))\n\n  df = pd.DataFrame({\'predictions\': predictions, \'labels\': labels, \'examples\': examples})\n  return df\n\n\ndef evaluate(dataset):\n  tf.reset_default_graph()\n  config = tf.ConfigProto(allow_soft_placement=True)\n  with tf.Session(config=config) as s:\n    model, _ = model_fn(s, restore_only=True)\n    df = ev(s, model, dataset)\n  print((df[\'predictions\'] == df[\'labels\']).mean())\n  import IPython\n  IPython.embed()\n\n\ndef train():\n  tf.reset_default_graph()\n\n  config = tf.ConfigProto(allow_soft_placement=True)\n\n  with tf.Session(config=config) as s:\n    model, saver = model_fn(s)\n    summary_writer = tf.summary.FileWriter(tflog_dir, graph=tf.get_default_graph())\n\n    # Format: tensorflow/contrib/tensorboard/plugins/projector/projector_config.proto\n    # pconf = projector.ProjectorConfig()\n\n    # # You can add multiple embeddings. Here we add only one.\n    # embedding = pconf.embeddings.add()\n    # embedding.tensor_name = m.embedding_matrix.name\n\n    # # Link this tensor to its metadata file (e.g. labels).\n    # embedding.metadata_path = vocab_tsv\n\n    # print(embedding.tensor_name)\n\n    # Saves a configuration file that TensorBoard will read during startup.\n\n    for i, (x, y) in enumerate(batch_iterator(task.read_trainset(epochs=3), args.batch_size, 300)):\n      fd = model.get_feed_data(x, y, class_weights=class_weights)\n\n      # import IPython\n      # IPython.embed()\n\n      t0 = time.clock()\n      step, summaries, loss, accuracy, _ = s.run([\n          model.global_step,\n          model.summary_op,\n          model.loss,\n          model.accuracy,\n          model.train_op,\n      ], fd)\n      td = time.clock() - t0\n\n      summary_writer.add_summary(summaries, global_step=step)\n      # projector.visualize_embeddings(summary_writer, pconf)\n\n      if step % 1 == 0:\n        print(\'step %s, loss=%s, accuracy=%s, t=%s, inputs=%s\' % (step, loss, accuracy, round(td, 2), fd[model.inputs].shape))\n      if step != 0 and step % args.checkpoint_frequency == 0:\n        print(\'checkpoint & graph meta\')\n        saver.save(s, checkpoint_path, global_step=step)\n        print(\'checkpoint done\')\n      if step != 0 and step % args.eval_frequency == 0:\n        print(\'evaluation at step %s\' % i)\n        dev_df = ev(s, model, task.read_devset(epochs=1))\n        print(\'dev accuracy: %.2f\' % (dev_df[\'predictions\'] == dev_df[\'labels\']).mean())\n\ndef main():\n  if args.mode == \'train\':\n    train()\n  elif args.mode == \'eval\':\n    evaluate(task.read_devset(epochs=1))\n\nif __name__ == \'__main__\':\n  main()\n'"
yelp.py,0,"b""import os\nimport pickle\n\ntrain_dir = os.path.join(os.path.curdir, 'yelp')\ndata_dir = os.path.join(train_dir, 'data')\n\nfor dir in [train_dir, data_dir]:\n  if not os.path.exists(dir):\n    os.makedirs(dir)\n\ntrainset_fn = os.path.join(data_dir, 'train.dataset')\ndevset_fn = os.path.join(data_dir, 'dev.dataset')\ntestset_fn = os.path.join(data_dir, 'test.dataset')\nvocab_fn = os.path.join(data_dir, 'vocab.pickle')\n\nreserved_tokens = 5\nunknown_id = 2\n\nvocab_size = 50001\n\ndef _read_dataset(fn, review_max_sentences=30, sentence_max_length=30, epochs=1):\n  c = 0\n  while 1:\n    c += 1\n    if epochs > 0 and c > epochs:\n      return\n    print('epoch %s' % c)\n    with open(fn, 'rb') as f:\n      try:\n        while 1:\n          x, y = pickle.load(f)\n\n          # clip review to specified max lengths\n          x = x[:review_max_sentences]\n          x = [sent[:sentence_max_length] for sent in x]\n\n          y -= 1\n          assert y >= 0 and y <= 4\n          yield x, y\n      except EOFError:\n        continue\n\ndef read_trainset(epochs=1):\n  return _read_dataset(trainset_fn, epochs=epochs)\n\ndef read_devset(epochs=1):\n  return _read_dataset(devset_fn, epochs=epochs)\n\ndef read_vocab():\n  with open(vocab_fn, 'rb') as f:\n    return pickle.load(f)\n\ndef read_labels():\n  return {i: i for i in range(5)}\n"""
yelp_prepare.py,0,"b'import argparse\nparser = argparse.ArgumentParser()\nparser.add_argument(""review_path"")\nargs = parser.parse_args()\n\nimport os\nimport ujson as json\nimport spacy\nimport pickle\nimport random\nfrom tqdm import tqdm\nfrom collections import defaultdict\nimport numpy as np\nfrom yelp import *\n\nen = spacy.load(\'en\')\n\ndef read_reviews():\n  with open(args.review_path, \'rb\') as f:\n    for line in f:\n      yield json.loads(line)\n\ndef build_word_frequency_distribution():\n  path = os.path.join(data_dir, \'word_freq.pickle\')\n\n  try:\n    with open(path, \'rb\') as freq_dist_f:\n      freq_dist_f = pickle.load(freq_dist_f)\n      print(\'frequency distribution loaded\')\n      return freq_dist_f\n  except IOError:\n    pass\n\n  print(\'building frequency distribution\')\n  freq = defaultdict(int)\n  for i, review in enumerate(read_reviews()):\n    doc = en.tokenizer(review[\'text\'])\n    for token in doc:\n      freq[token.orth_] += 1\n    if i % 10000 == 0:\n      with open(path, \'wb\') as freq_dist_f:\n        pickle.dump(freq, freq_dist_f)\n      print(\'dump at {}\'.format(i))\n  return freq\n\ndef build_vocabulary(lower=3, n=50000):\n  try:\n    with open(vocab_fn, \'rb\') as vocab_file:\n      vocab = pickle.load(vocab_file)\n      print(\'vocabulary loaded\')\n      return vocab\n  except IOError:\n    print(\'building vocabulary\')\n  freq = build_word_frequency_distribution()\n  top_words = list(sorted(freq.items(), key=lambda x: -x[1]))[:n-lower+1]\n  vocab = {}\n  i = lower\n  for w, freq in top_words:\n    vocab[w] = i\n    i += 1\n  with open(vocab_fn, \'wb\') as vocab_file:\n    pickle.dump(vocab, vocab_file)\n  return vocab\n\nUNKNOWN = 2\n\ndef make_data(split_points=(0.8, 0.94)):\n  train_ratio, dev_ratio = split_points\n  vocab = build_vocabulary()\n  train_f = open(trainset_fn, \'wb\')\n  dev_f = open(devset_fn, \'wb\')\n  test_f = open(testset_fn, \'wb\')\n\n  try:\n    for review in tqdm(read_reviews()):\n      x = []\n      for sent in en(review[\'text\']).sents:\n        x.append([vocab.get(tok.orth_, UNKNOWN) for tok in sent])\n      y = review[\'stars\']\n\n      r = random.random()\n      if r < train_ratio:\n        f = train_f\n      elif r < dev_ratio:\n        f = dev_f\n      else:\n        f = test_f\n      pickle.dump((x, y), f)\n  except KeyboardInterrupt:\n    pass\n\n  train_f.close()\n  dev_f.close()\n  test_f.close()\n\nif __name__ == \'__main__\':\n  make_data()'"
