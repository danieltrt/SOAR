file_path,api_count,code
setup.py,0,"b""from setuptools import find_packages\nfrom setuptools import setup\n\nREQUIRED_PACKAGES = ['distance', 'numpy', 'six', 'pillow']\nVERSION = '0.7.6'\ntry:\n    import pypandoc\n    README = pypandoc.convert('README.md', 'rst')\nexcept(IOError, ImportError):\n    README = open('README.md').read()\n\n\nsetup(\n    name='aocr',\n    url='https://github.com/emedvedev/attention-ocr',\n    download_url='https://github.com/emedvedev/attention-ocr/archive/{}.tar.gz'.format(VERSION),\n    author='Ed Medvedev',\n    author_email='edward.medvedev@gmail.com',\n    version=VERSION,\n    install_requires=REQUIRED_PACKAGES,\n    packages=find_packages(),\n    include_package_data=True,\n    license='MIT',\n    description=('''Optical character recognition model '''\n                 '''for Tensorflow based on Visual Attention.'''),\n    long_description=README,\n    entry_points={\n        'console_scripts': ['aocr=aocr.__main__:main'],\n    }\n)\n"""
aocr/__init__.py,0,"b""__author__ = 'emedvedev'\n"""
aocr/__main__.py,2,"b'# TODO: update the readme with new parameters\n# TODO: restoring a model without recreating it (use constants / op names in the code?)\n# TODO: move all the training parameters inside the training parser\n# TODO: switch to https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn instead of buckets\n\nfrom __future__ import absolute_import\n\nimport sys\nimport argparse\nimport logging\n\nimport tensorflow as tf\n\nfrom .model.model import Model\nfrom .defaults import Config\nfrom .util import dataset\nfrom .util.data_gen import DataGen\nfrom .util.export import Exporter\n\ntf.logging.set_verbosity(tf.logging.ERROR)\n\n\ndef process_args(args, defaults):\n\n    parser = argparse.ArgumentParser()\n    parser.prog = \'aocr\'\n    subparsers = parser.add_subparsers()\n\n    # Global arguments\n    parser_base = argparse.ArgumentParser(add_help=False)\n    parser_base.add_argument(\'--log-path\', dest=""log_path"",\n                             metavar=defaults.LOG_PATH,\n                             type=str, default=defaults.LOG_PATH,\n                             help=(\'log file path (default: %s)\'\n                                   % (defaults.LOG_PATH)))\n\n    # Dataset generation\n    parser_dataset = subparsers.add_parser(\'dataset\', parents=[parser_base],\n                                           help=\'create a dataset in the TFRecords format\')\n    parser_dataset.set_defaults(phase=\'dataset\')\n    parser_dataset.add_argument(\'annotations_path\', metavar=\'annotations\',\n                                type=str,\n                                help=(\'path to the annotation file\'))\n    parser_dataset.add_argument(\'output_path\', nargs=\'?\', metavar=\'output\',\n                                type=str, default=defaults.NEW_DATASET_PATH,\n                                help=(\'output path (default: %s)\'\n                                      % defaults.NEW_DATASET_PATH))\n    parser_dataset.add_argument(\'--log-step\', dest=\'log_step\',\n                                type=int, default=defaults.LOG_STEP,\n                                metavar=defaults.LOG_STEP,\n                                help=(\'print log messages every N steps (default: %s)\'\n                                      % defaults.LOG_STEP))\n    parser_dataset.add_argument(\'--no-force-uppercase\', dest=\'force_uppercase\',\n                                action=\'store_false\', default=defaults.FORCE_UPPERCASE,\n                                help=\'do not force uppercase on label values\')\n    parser_dataset.add_argument(\'--save-filename\', dest=\'save_filename\',\n                                action=\'store_true\', default=defaults.SAVE_FILENAME,\n                                help=\'save filename as a field in the dataset\')\n\n    # Shared model arguments\n    parser_model = argparse.ArgumentParser(add_help=False)\n    parser_model.set_defaults(visualize=defaults.VISUALIZE)\n    parser_model.set_defaults(load_model=defaults.LOAD_MODEL)\n    parser_model.add_argument(\'--max-width\', dest=""max_width"",\n                              metavar=defaults.MAX_WIDTH,\n                              type=int, default=defaults.MAX_WIDTH,\n                              help=(\'max image width (default: %s)\'\n                                    % (defaults.MAX_WIDTH)))\n    parser_model.add_argument(\'--max-height\', dest=""max_height"",\n                              metavar=defaults.MAX_HEIGHT,\n                              type=int, default=defaults.MAX_HEIGHT,\n                              help=(\'max image height (default: %s)\'\n                                    % (defaults.MAX_HEIGHT)))\n    parser_model.add_argument(\'--max-prediction\', dest=""max_prediction"",\n                              metavar=defaults.MAX_PREDICTION,\n                              type=int, default=defaults.MAX_PREDICTION,\n                              help=(\'max length of predicted strings (default: %s)\'\n                                    % (defaults.MAX_PREDICTION)))\n    parser_model.add_argument(\'--full-ascii\', dest=\'full_ascii\', action=\'store_true\',\n                              help=(\'use lowercase in addition to uppercase\'))\n    parser_model.set_defaults(full_ascii=defaults.FULL_ASCII)\n    parser_model.add_argument(\'--color\', dest=""channels"", action=\'store_const\', const=3,\n                              default=defaults.CHANNELS,\n                              help=(\'do not convert source images to grayscale\'))\n    parser_model.add_argument(\'--no-distance\', dest=""use_distance"", action=""store_false"",\n                              default=defaults.USE_DISTANCE,\n                              help=(\'require full match when calculating accuracy\'))\n    parser_model.add_argument(\'--gpu-id\', dest=""gpu_id"", metavar=defaults.GPU_ID,\n                              type=int, default=defaults.GPU_ID,\n                              help=\'specify a GPU ID\')\n    parser_model.add_argument(\'--use-gru\', dest=\'use_gru\', action=\'store_true\',\n                              help=\'use GRU instead of LSTM\')\n    parser_model.add_argument(\'--attn-num-layers\', dest=""attn_num_layers"",\n                              type=int, default=defaults.ATTN_NUM_LAYERS,\n                              metavar=defaults.ATTN_NUM_LAYERS,\n                              help=(\'hidden layers in attention decoder cell (default: %s)\'\n                                    % (defaults.ATTN_NUM_LAYERS)))\n    parser_model.add_argument(\'--attn-num-hidden\', dest=""attn_num_hidden"",\n                              type=int, default=defaults.ATTN_NUM_HIDDEN,\n                              metavar=defaults.ATTN_NUM_HIDDEN,\n                              help=(\'hidden units in attention decoder cell (default: %s)\'\n                                    % (defaults.ATTN_NUM_HIDDEN)))\n    parser_model.add_argument(\'--initial-learning-rate\', dest=""initial_learning_rate"",\n                              type=float, default=defaults.INITIAL_LEARNING_RATE,\n                              metavar=defaults.INITIAL_LEARNING_RATE,\n                              help=(\'initial learning rate (default: %s)\'\n                                    % (defaults.INITIAL_LEARNING_RATE)))\n    parser_model.add_argument(\'--model-dir\', \'--job-dir\', dest=""model_dir"",\n                              type=str, default=defaults.MODEL_DIR,\n                              metavar=defaults.MODEL_DIR,\n                              help=(\'directory for the model \'\n                                    \'(default: %s)\' % (defaults.MODEL_DIR)))\n    parser_model.add_argument(\'--target-embedding-size\', dest=""target_embedding_size"",\n                              type=int, default=defaults.TARGET_EMBEDDING_SIZE,\n                              metavar=defaults.TARGET_EMBEDDING_SIZE,\n                              help=(\'embedding dimension for each target (default: %s)\'\n                                    % (defaults.TARGET_EMBEDDING_SIZE)))\n    parser_model.add_argument(\'--output-dir\', dest=""output_dir"",\n                              type=str, default=defaults.OUTPUT_DIR,\n                              metavar=defaults.OUTPUT_DIR,\n                              help=(\'output directory (default: %s)\'\n                                    % (defaults.OUTPUT_DIR)))\n    parser_model.add_argument(\'--max-gradient-norm\', dest=""max_gradient_norm"",\n                              type=int, default=defaults.MAX_GRADIENT_NORM,\n                              metavar=defaults.MAX_GRADIENT_NORM,\n                              help=(\'clip gradients to this norm (default: %s)\'\n                                    % (defaults.MAX_GRADIENT_NORM)))\n    parser_model.add_argument(\'--no-gradient-clipping\', dest=\'clip_gradients\', action=\'store_false\',\n                              help=(\'do not perform gradient clipping\'))\n    parser_model.set_defaults(clip_gradients=defaults.CLIP_GRADIENTS)\n\n    # Training\n    parser_train = subparsers.add_parser(\'train\', parents=[parser_base, parser_model],\n                                         help=\'Train the model and save checkpoints.\')\n    parser_train.set_defaults(phase=\'train\')\n    parser_train.add_argument(\'dataset_path\', metavar=\'dataset\',\n                              type=str, default=defaults.DATA_PATH,\n                              help=(\'training dataset in the TFRecords format\'\n                                    \' (default: %s)\'\n                                    % (defaults.DATA_PATH)))\n    parser_train.add_argument(\'--steps-per-checkpoint\', dest=""steps_per_checkpoint"",\n                              type=int, default=defaults.STEPS_PER_CHECKPOINT,\n                              metavar=defaults.STEPS_PER_CHECKPOINT,\n                              help=(\'steps between saving the model\'\n                                    \' (default: %s)\'\n                                    % (defaults.STEPS_PER_CHECKPOINT)))\n    parser_train.add_argument(\'--batch-size\', dest=""batch_size"",\n                              type=int, default=defaults.BATCH_SIZE,\n                              metavar=defaults.BATCH_SIZE,\n                              help=(\'batch size (default: %s)\'\n                                    % (defaults.BATCH_SIZE)))\n    parser_train.add_argument(\'--num-epoch\', dest=""num_epoch"",\n                              type=int, default=defaults.NUM_EPOCH,\n                              metavar=defaults.NUM_EPOCH,\n                              help=(\'number of training epochs (default: %s)\'\n                                    % (defaults.NUM_EPOCH)))\n    parser_train.add_argument(\'--no-resume\', dest=\'load_model\', action=\'store_false\',\n                              help=(\'create a new model even if checkpoints already exist\'))\n\n    # Testing\n    parser_test = subparsers.add_parser(\'test\', parents=[parser_base, parser_model],\n                                        help=\'Test the saved model.\')\n    parser_test.set_defaults(phase=\'test\', steps_per_checkpoint=0, batch_size=1,\n                             max_width=defaults.MAX_WIDTH, max_height=defaults.MAX_HEIGHT,\n                             max_prediction=defaults.MAX_PREDICTION, full_ascii=defaults.FULL_ASCII)\n    parser_test.add_argument(\'dataset_path\', metavar=\'dataset\',\n                             type=str, default=defaults.DATA_PATH,\n                             help=(\'Testing dataset in the TFRecords format\'\n                                   \', default=%s\'\n                                   % (defaults.DATA_PATH)))\n    parser_test.add_argument(\'--visualize\', dest=\'visualize\', action=\'store_true\',\n                             help=(\'visualize attentions\'))\n\n    # Exporting\n    parser_export = subparsers.add_parser(\'export\', parents=[parser_base, parser_model],\n                                          help=\'Export the model with weights for production use.\')\n    parser_export.set_defaults(phase=\'export\', steps_per_checkpoint=0, batch_size=1)\n    parser_export.add_argument(\'export_path\', nargs=\'?\', metavar=\'dir\',\n                               type=str, default=defaults.EXPORT_PATH,\n                               help=(\'Directory to save the exported model to,\'\n                                     \'default=%s\'\n                                     % (defaults.EXPORT_PATH)))\n    parser_export.add_argument(\'--format\', dest=""format"",\n                               type=str, default=defaults.EXPORT_FORMAT,\n                               choices=[\'frozengraph\', \'savedmodel\'],\n                               help=(\'export format\'\n                                     \' (default: %s)\'\n                                     % (defaults.EXPORT_FORMAT)))\n\n    # Predicting\n    parser_predict = subparsers.add_parser(\'predict\', parents=[parser_base, parser_model],\n                                           help=\'Predict text from files (feed through stdin).\')\n    parser_predict.set_defaults(phase=\'predict\', steps_per_checkpoint=0, batch_size=1)\n\n    parameters = parser.parse_args(args)\n    return parameters\n\n\ndef main(args=None):\n\n    if args is None:\n        args = sys.argv[1:]\n\n    parameters = process_args(args, Config)\n    logging.basicConfig(\n        level=logging.DEBUG,\n        format=\'%(asctime)-15s %(name)-5s %(levelname)-8s %(message)s\',\n        filename=parameters.log_path)\n    console = logging.StreamHandler()\n    console.setLevel(logging.INFO)\n    formatter = logging.Formatter(\'%(asctime)-15s %(name)-5s %(levelname)-8s %(message)s\')\n    console.setFormatter(formatter)\n    logging.getLogger(\'\').addHandler(console)\n\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n\n        if parameters.phase == \'dataset\':\n            dataset.generate(\n                parameters.annotations_path,\n                parameters.output_path,\n                parameters.log_step,\n                parameters.force_uppercase,\n                parameters.save_filename\n            )\n            return\n\n        if parameters.full_ascii:\n            DataGen.set_full_ascii_charmap()\n\n        model = Model(\n            phase=parameters.phase,\n            visualize=parameters.visualize,\n            output_dir=parameters.output_dir,\n            batch_size=parameters.batch_size,\n            initial_learning_rate=parameters.initial_learning_rate,\n            steps_per_checkpoint=parameters.steps_per_checkpoint,\n            model_dir=parameters.model_dir,\n            target_embedding_size=parameters.target_embedding_size,\n            attn_num_hidden=parameters.attn_num_hidden,\n            attn_num_layers=parameters.attn_num_layers,\n            clip_gradients=parameters.clip_gradients,\n            max_gradient_norm=parameters.max_gradient_norm,\n            session=sess,\n            load_model=parameters.load_model,\n            gpu_id=parameters.gpu_id,\n            use_gru=parameters.use_gru,\n            use_distance=parameters.use_distance,\n            max_image_width=parameters.max_width,\n            max_image_height=parameters.max_height,\n            max_prediction_length=parameters.max_prediction,\n            channels=parameters.channels,\n        )\n\n        if parameters.phase == \'train\':\n            model.train(\n                data_path=parameters.dataset_path,\n                num_epoch=parameters.num_epoch\n            )\n        elif parameters.phase == \'test\':\n            model.test(\n                data_path=parameters.dataset_path\n            )\n        elif parameters.phase == \'predict\':\n            for line in sys.stdin:\n                filename = line.rstrip()\n                try:\n                    with open(filename, \'rb\') as img_file:\n                        img_file_data = img_file.read()\n                except IOError:\n                    logging.error(\'Result: error while opening file %s.\', filename)\n                    continue\n                text, probability = model.predict(img_file_data)\n                logging.info(\'Result: OK. %s %s\', \'{:.2f}\'.format(probability), text)\n        elif parameters.phase == \'export\':\n            exporter = Exporter(model)\n            exporter.save(parameters.export_path, parameters.format)\n            return\n        else:\n            raise NotImplementedError\n\n\nif __name__ == ""__main__"":\n    main()\n'"
aocr/defaults.py,0,"b'""""""\nDefault parameters.\n""""""\n\n\nclass Config(object):\n    """"""\n    Default config (see __main__.py or README for documentation).\n    """"""\n\n    GPU_ID = 0\n    VISUALIZE = False\n\n    # I/O\n    NEW_DATASET_PATH = \'./dataset.tfrecords\'\n    DATA_PATH = \'./data.tfrecords\'\n    MODEL_DIR = \'./checkpoints\'\n    LOG_PATH = \'aocr.log\'\n    OUTPUT_DIR = \'./results\'\n    STEPS_PER_CHECKPOINT = 100\n    EXPORT_FORMAT = \'savedmodel\'\n    EXPORT_PATH = \'exported\'\n    FORCE_UPPERCASE = True\n    SAVE_FILENAME = False\n    FULL_ASCII = False\n\n    # Optimization\n    NUM_EPOCH = 1000\n    BATCH_SIZE = 65\n    INITIAL_LEARNING_RATE = 1.0\n\n    # Network parameters\n    CLIP_GRADIENTS = True  # whether to perform gradient clipping\n    MAX_GRADIENT_NORM = 5.0  # Clip gradients to this norm\n    TARGET_EMBEDDING_SIZE = 10  # embedding dimension for each target\n    ATTN_NUM_HIDDEN = 128  # number of hidden units in attention decoder cell\n    ATTN_NUM_LAYERS = 2  # number of layers in attention decoder cell\n    # (Encoder number of hidden units will be ATTN_NUM_HIDDEN*ATTN_NUM_LAYERS)\n    LOAD_MODEL = True\n    OLD_MODEL_VERSION = False\n    TARGET_VOCAB_SIZE = 26+10+3  # 0: PADDING, 1: GO, 2: EOS, >2: 0-9, a-z\n    CHANNELS = 1  # number of color channels from source image (1 = grayscale, 3 = rgb)\n\n    MAX_WIDTH = 160\n    MAX_HEIGHT = 60\n    MAX_PREDICTION = 8\n\n    USE_DISTANCE = True\n\n    # Dataset generation\n    LOG_STEP = 500\n'"
aocr/model/__init__.py,0,b''
aocr/model/cnn.py,19,"b'# pylint: disable=invalid-name\n\nfrom __future__ import absolute_import\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef var_random(name, shape, regularizable=False):\n    \'\'\'\n    Initialize a random variable using xavier initialization.\n    Add regularization if regularizable=True\n    :param name:\n    :param shape:\n    :param regularizable:\n    :return:\n    \'\'\'\n    v = tf.get_variable(name, shape=shape, initializer=tf.contrib.layers.xavier_initializer())\n    if regularizable:\n        with tf.name_scope(name + \'/Regularizer/\'):\n            tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, tf.nn.l2_loss(v))\n    return v\n\n\ndef max_2x2pool(incoming, name):\n    \'\'\'\n    max pooling on 2 dims.\n    :param incoming:\n    :param name:\n    :return:\n    \'\'\'\n    with tf.variable_scope(name):\n        return tf.nn.max_pool(incoming, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1), padding=\'SAME\')\n\n\ndef max_2x1pool(incoming, name):\n    \'\'\'\n    max pooling only on image width\n    :param incoming:\n    :param name:\n    :return:\n    \'\'\'\n    with tf.variable_scope(name):\n        return tf.nn.max_pool(incoming, ksize=(1, 2, 1, 1), strides=(1, 2, 1, 1), padding=\'SAME\')\n\n\ndef ConvRelu(incoming, num_filters, filter_size, name):\n    \'\'\'\n    Add a convolution layer followed by a Relu layer.\n    :param incoming:\n    :param num_filters:\n    :param filter_size:\n    :param name:\n    :return:\n    \'\'\'\n    num_filters_from = incoming.get_shape().as_list()[3]\n    with tf.variable_scope(name):\n        conv_W = var_random(\n            \'W\',\n            tuple(filter_size) + (num_filters_from, num_filters),\n            regularizable=True\n        )\n\n        after_conv = tf.nn.conv2d(incoming, conv_W, strides=(1, 1, 1, 1), padding=\'SAME\')\n\n        return tf.nn.relu(after_conv)\n\n\ndef batch_norm(incoming, is_training):\n    \'\'\'\n    batch normalization\n    :param incoming:\n    :param is_training:\n    :return:\n    \'\'\'\n    return tf.contrib.layers.batch_norm(incoming, is_training=is_training, scale=True, decay=0.99)\n\n\ndef ConvReluBN(incoming, num_filters, filter_size, name, is_training):\n    \'\'\'\n    Convolution -> Batch normalization -> Relu\n    :param incoming:\n    :param num_filters:\n    :param filter_size:\n    :param name:\n    :param is_training:\n    :return:\n    \'\'\'\n    num_filters_from = incoming.get_shape().as_list()[3]\n    with tf.variable_scope(name):\n        conv_W = var_random(\n            \'W\',\n            tuple(filter_size) + (num_filters_from, num_filters),\n            regularizable=True\n        )\n\n        after_conv = tf.nn.conv2d(incoming, conv_W, strides=(1, 1, 1, 1), padding=\'SAME\')\n\n        after_bn = batch_norm(after_conv, is_training)\n\n        return tf.nn.relu(after_bn)\n\n\ndef dropout(incoming, is_training, keep_prob=0.5):\n    return tf.contrib.layers.dropout(incoming, keep_prob=keep_prob, is_training=is_training)\n\n\ndef tf_create_attention_map(incoming):\n    \'\'\'\n    flatten hight and width into one dimention of size attn_length\n    :param incoming: 3D Tensor [batch_size x cur_h x cur_w x num_channels]\n    :return: attention_map: 3D Tensor [batch_size x attn_length x attn_size].\n    \'\'\'\n    shape = incoming.get_shape().as_list()\n    return tf.reshape(incoming, (-1, np.prod(shape[1:3]), shape[3]))\n\n\nclass CNN(object):\n    """"""\n    Usage for tf tensor output:\n    o = CNN(x).tf_output()\n\n    """"""\n\n    def __init__(self, input_tensor, is_training):\n        self._build_network(input_tensor, is_training)\n\n    def _build_network(self, input_tensor, is_training):\n        """"""\n        https://github.com/bgshih/crnn/blob/master/model/crnn_demo/config.lua\n        :return:\n        """"""\n        net = tf.add(input_tensor, (-128.0))\n        net = tf.multiply(net, (1/128.0))\n\n        net = ConvRelu(net, 64, (3, 3), \'conv_conv1\')\n        net = max_2x2pool(net, \'conv_pool1\')\n\n        net = ConvRelu(net, 128, (3, 3), \'conv_conv2\')\n        net = max_2x2pool(net, \'conv_pool2\')\n\n        net = ConvReluBN(net, 256, (3, 3), \'conv_conv3\', is_training)\n        net = ConvRelu(net, 256, (3, 3), \'conv_conv4\')\n        net = max_2x1pool(net, \'conv_pool3\')\n\n        net = ConvReluBN(net, 512, (3, 3), \'conv_conv5\', is_training)\n        net = ConvRelu(net, 512, (3, 3), \'conv_conv6\')\n        net = max_2x1pool(net, \'conv_pool4\')\n\n        net = ConvReluBN(net, 512, (2, 2), \'conv_conv7\', is_training)\n        net = max_2x1pool(net, \'conv_pool5\')\n        net = dropout(net, is_training)\n\n        net = tf.squeeze(net, axis=1)\n\n        self.model = net\n\n    def tf_output(self):\n        # if self.input_tensor is not None:\n        return self.model\n\n    # def __call__(self, input_tensor):\n    #     return self.model(input_tensor)\n\n    def save(self):\n        pass\n'"
aocr/model/model.py,71,"b'""""""Visual Attention Based OCR Model.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\n\nimport time\nimport os\nimport math\nimport logging\nimport sys\n\nimport distance\nimport numpy as np\nimport tensorflow as tf\n\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nfrom .cnn import CNN\nfrom .seq2seq_model import Seq2SeqModel\nfrom ..util.data_gen import DataGen\nfrom ..util.visualizations import visualize_attention\n\n\nclass Model(object):\n    def __init__(self,\n                 phase,\n                 visualize,\n                 output_dir,\n                 batch_size,\n                 initial_learning_rate,\n                 steps_per_checkpoint,\n                 model_dir,\n                 target_embedding_size,\n                 attn_num_hidden,\n                 attn_num_layers,\n                 clip_gradients,\n                 max_gradient_norm,\n                 session,\n                 load_model,\n                 gpu_id,\n                 use_gru,\n                 use_distance=True,\n                 max_image_width=160,\n                 max_image_height=60,\n                 max_prediction_length=8,\n                 channels=1,\n                 reg_val=0):\n\n        self.use_distance = use_distance\n\n        # We need resized width, not the actual width\n        max_resized_width = 1. * max_image_width / max_image_height * DataGen.IMAGE_HEIGHT\n\n        self.max_original_width = max_image_width\n        self.max_width = int(math.ceil(max_resized_width))\n\n        self.encoder_size = int(math.ceil(1. * self.max_width / 4))\n        self.decoder_size = max_prediction_length + 2\n        self.buckets = [(self.encoder_size, self.decoder_size)]\n\n        if gpu_id >= 0:\n            device_id = \'/gpu:\' + str(gpu_id)\n        else:\n            device_id = \'/cpu:0\'\n        self.device_id = device_id\n\n        if not os.path.exists(model_dir):\n            os.makedirs(model_dir)\n\n        if phase == \'test\':\n            batch_size = 1\n\n        logging.info(\'phase: %s\', phase)\n        logging.info(\'model_dir: %s\', model_dir)\n        logging.info(\'load_model: %s\', load_model)\n        logging.info(\'output_dir: %s\', output_dir)\n        logging.info(\'steps_per_checkpoint: %d\', steps_per_checkpoint)\n        logging.info(\'batch_size: %d\', batch_size)\n        logging.info(\'learning_rate: %f\', initial_learning_rate)\n        logging.info(\'reg_val: %d\', reg_val)\n        logging.info(\'max_gradient_norm: %f\', max_gradient_norm)\n        logging.info(\'clip_gradients: %s\', clip_gradients)\n        logging.info(\'max_image_width %f\', max_image_width)\n        logging.info(\'max_prediction_length %f\', max_prediction_length)\n        logging.info(\'channels: %d\', channels)\n        logging.info(\'target_embedding_size: %f\', target_embedding_size)\n        logging.info(\'attn_num_hidden: %d\', attn_num_hidden)\n        logging.info(\'attn_num_layers: %d\', attn_num_layers)\n        logging.info(\'visualize: %s\', visualize)\n\n        if use_gru:\n            logging.info(\'using GRU in the decoder.\')\n\n        self.reg_val = reg_val\n        self.sess = session\n        self.steps_per_checkpoint = steps_per_checkpoint\n        self.model_dir = model_dir\n        self.output_dir = output_dir\n        self.batch_size = batch_size\n        self.global_step = tf.Variable(0, trainable=False)\n        self.phase = phase\n        self.visualize = visualize\n        self.learning_rate = initial_learning_rate\n        self.clip_gradients = clip_gradients\n        self.channels = channels\n\n        if phase == \'train\':\n            self.forward_only = False\n        else:\n            self.forward_only = True\n\n        with tf.device(device_id):\n\n            self.height = tf.constant(DataGen.IMAGE_HEIGHT, dtype=tf.int32)\n            self.height_float = tf.constant(DataGen.IMAGE_HEIGHT, dtype=tf.float64)\n\n            self.img_pl = tf.placeholder(tf.string, name=\'input_image_as_bytes\')\n            self.img_data = tf.cond(\n                tf.less(tf.rank(self.img_pl), 1),\n                lambda: tf.expand_dims(self.img_pl, 0),\n                lambda: self.img_pl\n            )\n            self.img_data = tf.map_fn(self._prepare_image, self.img_data, dtype=tf.float32)\n            num_images = tf.shape(self.img_data)[0]\n\n            # TODO: create a mask depending on the image/batch size\n            self.encoder_masks = []\n            for i in xrange(self.encoder_size + 1):\n                self.encoder_masks.append(\n                    tf.tile([[1.]], [num_images, 1])\n                )\n\n            self.decoder_inputs = []\n            self.target_weights = []\n            for i in xrange(self.decoder_size + 1):\n                self.decoder_inputs.append(\n                    tf.tile([1], [num_images])\n                )\n                if i < self.decoder_size:\n                    self.target_weights.append(tf.tile([1.], [num_images]))\n                else:\n                    self.target_weights.append(tf.tile([0.], [num_images]))\n\n            cnn_model = CNN(self.img_data, not self.forward_only)\n            self.conv_output = cnn_model.tf_output()\n            self.perm_conv_output = tf.transpose(self.conv_output, perm=[1, 0, 2])\n            self.attention_decoder_model = Seq2SeqModel(\n                encoder_masks=self.encoder_masks,\n                encoder_inputs_tensor=self.perm_conv_output,\n                decoder_inputs=self.decoder_inputs,\n                target_weights=self.target_weights,\n                target_vocab_size=len(DataGen.CHARMAP),\n                buckets=self.buckets,\n                target_embedding_size=target_embedding_size,\n                attn_num_layers=attn_num_layers,\n                attn_num_hidden=attn_num_hidden,\n                forward_only=self.forward_only,\n                use_gru=use_gru)\n\n            table = tf.contrib.lookup.MutableHashTable(\n                key_dtype=tf.int64,\n                value_dtype=tf.string,\n                default_value="""",\n                checkpoint=True,\n            )\n\n            insert = table.insert(\n                tf.constant(list(range(len(DataGen.CHARMAP))), dtype=tf.int64),\n                tf.constant(DataGen.CHARMAP),\n            )\n\n            with tf.control_dependencies([insert]):\n                num_feed = []\n                prb_feed = []\n\n                for line in xrange(len(self.attention_decoder_model.output)):\n                    guess = tf.argmax(self.attention_decoder_model.output[line], axis=1)\n                    proba = tf.reduce_max(\n                        tf.nn.softmax(self.attention_decoder_model.output[line]), axis=1)\n                    num_feed.append(guess)\n                    prb_feed.append(proba)\n\n                # Join the predictions into a single output string.\n                trans_output = tf.transpose(num_feed)\n                trans_output = tf.map_fn(\n                    lambda m: tf.foldr(\n                        lambda a, x: tf.cond(\n                            tf.equal(x, DataGen.EOS_ID),\n                            lambda: \'\',\n                            lambda: table.lookup(x) + a  # pylint: disable=undefined-variable\n                        ),\n                        m,\n                        initializer=\'\'\n                    ),\n                    trans_output,\n                    dtype=tf.string\n                )\n\n                # Calculate the total probability of the output string.\n                trans_outprb = tf.transpose(prb_feed)\n                trans_outprb = tf.gather(trans_outprb, tf.range(tf.size(trans_output)))\n                trans_outprb = tf.map_fn(\n                    lambda m: tf.foldr(\n                        lambda a, x: tf.multiply(tf.cast(x, tf.float64), a),\n                        m,\n                        initializer=tf.cast(1, tf.float64)\n                    ),\n                    trans_outprb,\n                    dtype=tf.float64\n                )\n\n                self.prediction = tf.cond(\n                    tf.equal(tf.shape(trans_output)[0], 1),\n                    lambda: trans_output[0],\n                    lambda: trans_output,\n                )\n                self.probability = tf.cond(\n                    tf.equal(tf.shape(trans_outprb)[0], 1),\n                    lambda: trans_outprb[0],\n                    lambda: trans_outprb,\n                )\n\n                self.prediction = tf.identity(self.prediction, name=\'prediction\')\n                self.probability = tf.identity(self.probability, name=\'probability\')\n\n            if not self.forward_only:  # train\n                self.updates = []\n                self.summaries_by_bucket = []\n\n                params = tf.trainable_variables()\n                opt = tf.train.AdadeltaOptimizer(learning_rate=initial_learning_rate)\n                loss_op = self.attention_decoder_model.loss\n\n                if self.reg_val > 0:\n                    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n                    logging.info(\'Adding %s regularization losses\', len(reg_losses))\n                    logging.debug(\'REGULARIZATION_LOSSES: %s\', reg_losses)\n                    loss_op = self.reg_val * tf.reduce_sum(reg_losses) + loss_op\n\n                gradients, params = list(zip(*opt.compute_gradients(loss_op, params)))\n                if self.clip_gradients:\n                    gradients, _ = tf.clip_by_global_norm(gradients, max_gradient_norm)\n\n                # Summaries for loss, variables, gradients, gradient norms and total gradient norm.\n                summaries = [\n                    tf.summary.scalar(""loss"", loss_op),\n                    tf.summary.scalar(""total_gradient_norm"", tf.global_norm(gradients))\n                ]\n                all_summaries = tf.summary.merge(summaries)\n                self.summaries_by_bucket.append(all_summaries)\n\n                # update op - apply gradients\n                update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n                with tf.control_dependencies(update_ops):\n                    self.updates.append(\n                        opt.apply_gradients(\n                            list(zip(gradients, params)),\n                            global_step=self.global_step\n                        )\n                    )\n\n        self.saver_all = tf.train.Saver(tf.all_variables())\n        self.checkpoint_path = os.path.join(self.model_dir, ""model.ckpt"")\n\n        ckpt = tf.train.get_checkpoint_state(model_dir)\n        if ckpt and load_model:\n            # pylint: disable=no-member\n            logging.info(""Reading model parameters from %s"", ckpt.model_checkpoint_path)\n            self.saver_all.restore(self.sess, ckpt.model_checkpoint_path)\n        else:\n            logging.info(""Created model with fresh parameters."")\n            self.sess.run(tf.initialize_all_variables())\n\n    def predict(self, image_file_data):\n        input_feed = {}\n        input_feed[self.img_pl.name] = image_file_data\n\n        output_feed = [self.prediction, self.probability]\n        outputs = self.sess.run(output_feed, input_feed)\n\n        text = outputs[0]\n        probability = outputs[1]\n        if sys.version_info >= (3,):\n            text = text.decode(\'iso-8859-1\')\n\n        return (text, probability)\n\n    def test(self, data_path):\n        current_step = 0\n        num_correct = 0.0\n        num_total = 0.0\n\n        s_gen = DataGen(data_path, self.buckets, epochs=1, max_width=self.max_original_width)\n        for batch in s_gen.gen(1):\n            current_step += 1\n            # Get a batch (one image) and make a step.\n            start_time = time.time()\n            result = self.step(batch, self.forward_only)\n            curr_step_time = (time.time() - start_time)\n\n            num_total += 1\n\n            output = result[\'prediction\']\n            ground = batch[\'labels\'][0]\n            comment = batch[\'comments\'][0]\n            if sys.version_info >= (3,):\n                output = output.decode(\'iso-8859-1\')\n                ground = ground.decode(\'iso-8859-1\')\n                comment = comment.decode(\'iso-8859-1\')\n\n            probability = result[\'probability\']\n\n            if self.use_distance:\n                incorrect = distance.levenshtein(output, ground)\n                if not ground:\n                    if not output:\n                        incorrect = 0\n                    else:\n                        incorrect = 1\n                else:\n                    incorrect = float(incorrect) / len(ground)\n                incorrect = min(1, incorrect)\n            else:\n                incorrect = 0 if output == ground else 1\n\n            num_correct += 1. - incorrect\n\n            if self.visualize:\n                # Attention visualization.\n                threshold = 0.5\n                normalize = True\n                binarize = True\n                attns_list = [[a.tolist() for a in step_attn] for step_attn in result[\'attentions\']]\n                attns = np.array(attns_list).transpose([1, 0, 2])\n                visualize_attention(batch[\'data\'],\n                                    \'out\',\n                                    attns,\n                                    output,\n                                    self.max_width,\n                                    DataGen.IMAGE_HEIGHT,\n                                    threshold=threshold,\n                                    normalize=normalize,\n                                    binarize=binarize,\n                                    ground=ground,\n                                    flag=None)\n\n            step_accuracy = ""{:>4.0%}"".format(1. - incorrect)\n            if incorrect:\n                correctness = step_accuracy + "" ({} vs {}) {}"".format(output, ground, comment)\n            else:\n                correctness = step_accuracy + "" ("" + ground + "")""\n\n            logging.info(\'Step {:.0f} ({:.3f}s). \'\n                         \'Accuracy: {:6.2%}, \'\n                         \'loss: {:f}, perplexity: {:0<7.6}, probability: {:6.2%} {}\'.format(\n                             current_step,\n                             curr_step_time,\n                             num_correct / num_total,\n                             result[\'loss\'],\n                             math.exp(result[\'loss\']) if result[\'loss\'] < 300 else float(\'inf\'),\n                             probability,\n                             correctness))\n\n    def train(self, data_path, num_epoch):\n        logging.info(\'num_epoch: %d\', num_epoch)\n        s_gen = DataGen(\n            data_path, self.buckets,\n            epochs=num_epoch, max_width=self.max_original_width\n        )\n        step_time = 0.0\n        loss = 0.0\n        current_step = 0\n        skipped_counter = 0\n        writer = tf.summary.FileWriter(self.model_dir, self.sess.graph)\n\n        logging.info(\'Starting the training process.\')\n        for batch in s_gen.gen(self.batch_size):\n\n            current_step += 1\n\n            start_time = time.time()\n            # result = self.step(batch, self.forward_only)\n            result = None\n            try:\n                result = self.step(batch, self.forward_only)\n            except Exception as e:\n                skipped_counter += 1\n                logging.info(""Step {} failed, batch skipped."" +\n                             "" Total skipped: {}"".format(current_step, skipped_counter))\n                logging.error(\n                    ""Step {} failed. Exception details: {}"".format(current_step, str(e)))\n                continue\n\n            loss += result[\'loss\'] / self.steps_per_checkpoint\n            curr_step_time = (time.time() - start_time)\n            step_time += curr_step_time / self.steps_per_checkpoint\n\n            # num_correct = 0\n\n            # step_outputs = result[\'prediction\']\n            # grounds = batch[\'labels\']\n            # for output, ground in zip(step_outputs, grounds):\n            #     if self.use_distance:\n            #         incorrect = distance.levenshtein(output, ground)\n            #         incorrect = float(incorrect) / len(ground)\n            #         incorrect = min(1.0, incorrect)\n            #     else:\n            #         incorrect = 0 if output == ground else 1\n            #     num_correct += 1. - incorrect\n\n            writer.add_summary(result[\'summaries\'], current_step)\n\n            # precision = num_correct / len(batch[\'labels\'])\n            step_perplexity = math.exp(result[\'loss\']) if result[\'loss\'] < 300 else float(\'inf\')\n\n            # logging.info(\'Step %i: %.3fs, precision: %.2f, loss: %f, perplexity: %f.\'\n            #              % (current_step, curr_step_time, precision*100,\n            #                 result[\'loss\'], step_perplexity))\n\n            logging.info(\'Step %i: %.3fs, loss: %f, perplexity: %f.\',\n                         current_step, curr_step_time, result[\'loss\'], step_perplexity)\n\n            # Once in a while, we save checkpoint, print statistics, and run evals.\n            if current_step % self.steps_per_checkpoint == 0:\n                perplexity = math.exp(loss) if loss < 300 else float(\'inf\')\n                # Print statistics for the previous epoch.\n                logging.info(""Global step %d. Time: %.3f, loss: %f, perplexity: %.2f."",\n                             self.sess.run(self.global_step), step_time, loss, perplexity)\n                # Save checkpoint and reset timer and loss.\n                logging.info(""Saving the model at step %d."", current_step)\n                self.saver_all.save(self.sess, self.checkpoint_path, global_step=self.global_step)\n                step_time, loss = 0.0, 0.0\n\n        # Print statistics for the previous epoch.\n        perplexity = math.exp(loss) if loss < 300 else float(\'inf\')\n        logging.info(""Global step %d. Time: %.3f, loss: %f, perplexity: %.2f."",\n                     self.sess.run(self.global_step), step_time, loss, perplexity)\n\n        if skipped_counter:\n            logging.info(""Skipped {} batches due to errors."".format(skipped_counter))\n\n        # Save checkpoint and reset timer and loss.\n        logging.info(""Finishing the training and saving the model at step %d."", current_step)\n        self.saver_all.save(self.sess, self.checkpoint_path, global_step=self.global_step)\n\n    # step, read one batch, generate gradients\n    def step(self, batch, forward_only):\n        img_data = batch[\'data\']\n        decoder_inputs = batch[\'decoder_inputs\']\n        target_weights = batch[\'target_weights\']\n\n        # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n        input_feed = {}\n        input_feed[self.img_pl.name] = img_data\n\n        for idx in xrange(self.decoder_size):\n            input_feed[self.decoder_inputs[idx].name] = decoder_inputs[idx]\n            input_feed[self.target_weights[idx].name] = target_weights[idx]\n\n        # Since our targets are decoder inputs shifted by one, we need one more.\n        last_target = self.decoder_inputs[self.decoder_size].name\n        input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n\n        # Output feed: depends on whether we do a backward step or not.\n        output_feed = [\n            self.attention_decoder_model.loss,  # Loss for this batch.\n        ]\n\n        if not forward_only:\n            output_feed += [self.summaries_by_bucket[0],\n                            self.updates[0]]\n        else:\n            output_feed += [self.prediction]\n            output_feed += [self.probability]\n            if self.visualize:\n                output_feed += self.attention_decoder_model.attentions\n\n        outputs = self.sess.run(output_feed, input_feed)\n\n        res = {\n            \'loss\': outputs[0],\n        }\n\n        if not forward_only:\n            res[\'summaries\'] = outputs[1]\n        else:\n            res[\'prediction\'] = outputs[1]\n            res[\'probability\'] = outputs[2]\n            if self.visualize:\n                res[\'attentions\'] = outputs[3:]\n\n        return res\n\n    def _prepare_image(self, image):\n        """"""Resize the image to a maximum height of `self.height` and maximum\n        width of `self.width` while maintaining the aspect ratio. Pad the\n        resized image to a fixed size of ``[self.height, self.width]``.""""""\n        img = tf.image.decode_png(image, channels=self.channels)\n        dims = tf.shape(img)\n        width = self.max_width\n\n        max_width = tf.to_int32(tf.ceil(tf.truediv(dims[1], dims[0]) * self.height_float))\n        max_height = tf.to_int32(tf.ceil(tf.truediv(width, max_width) * self.height_float))\n\n        resized = tf.cond(\n            tf.greater_equal(width, max_width),\n            lambda: tf.cond(\n                tf.less_equal(dims[0], self.height),\n                lambda: tf.to_float(img),\n                lambda: tf.image.resize_images(img, [self.height, max_width],\n                                               method=tf.image.ResizeMethod.BICUBIC),\n            ),\n            lambda: tf.image.resize_images(img, [max_height, width],\n                                           method=tf.image.ResizeMethod.BICUBIC)\n        )\n\n        padded = tf.image.pad_to_bounding_box(resized, 0, 0, self.height, width)\n        return padded\n'"
aocr/model/seq2seq.py,46,"b'# Copyright 2015 Google Inc. All Rights Reserved.  #\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n# pylint: disable=invalid-name\n\n""""""Library for creating sequence-to-sequence models in TensorFlow.\n\nSequence-to-sequence recurrent neural networks can learn complex functions\nthat map input sequences to output sequences. These models yield very good\nresults on a number of tasks, such as speech recognition, parsing, machine\ntranslation, or even constructing automated replies to emails.\n\nBefore using this module, it is recommended to read the TensorFlow tutorial\non sequence-to-sequence models. It explains the basic concepts of this module\nand shows an end-to-end example of how to build a translation model.\n    https://www.tensorflow.org/versions/master/tutorials/seq2seq/index.html\n\nHere is an overview of functions available in this module. They all use\na very similar interface, so after reading the above tutorial and using\none of them, others should be easy to substitute.\n\n* Full sequence-to-sequence models.\n    - basic_rnn_seq2seq: The most basic RNN-RNN model.\n    - tied_rnn_seq2seq: The basic model with tied encoder and decoder weights.\n    - embedding_rnn_seq2seq: The basic model with input embedding.\n    - embedding_tied_rnn_seq2seq: The tied model with input embedding.\n    - embedding_attention_seq2seq: Advanced model with input embedding and\n            the neural attention mechanism; recommended for complex tasks.\n\n* Multi-task sequence-to-sequence models.\n    - one2many_rnn_seq2seq: The embedding model with multiple decoders.\n\n* Decoders (when you write your own encoder, you can use these to decode;\n        e.g., if you want to write a model that generates captions for images).\n    - rnn_decoder: The basic decoder based on a pure RNN.\n    - attention_decoder: A decoder that uses the attention mechanism.\n\n* Losses.\n    - sequence_loss: Loss for a sequence model returning average log-perplexity.\n    - sequence_loss_by_example: As above, but not averaging over all examples.\n\n* model_with_buckets: A convenience function to create models with bucketing\n        (see the tutorial above for an explanation of why and how to use it).\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n# We disable pylint because we need python3 compatibility.\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nfrom six.moves import zip     # pylint: disable=redefined-builtin\n\nimport tensorflow as tf\n\ntry:\n    from tensorflow.contrib.rnn.python.ops import rnn_cell_impl\nexcept ImportError:\n    from tensorflow.python.ops import rnn_cell_impl\n\ntry:\n    linear = rnn_cell_impl._linear  # pylint: disable=protected-access\nexcept AttributeError:\n    # pylint: disable=protected-access,no-name-in-module\n    from tensorflow.contrib.rnn.python.ops import core_rnn_cell\n    linear = core_rnn_cell._linear\n\n\ndef _extract_argmax_and_embed(embedding, output_projection=None,\n                              update_embedding=True):\n    """"""Get a loop_function that extracts the previous symbol and embeds it.\n\n    Args:\n        embedding: embedding tensor for symbols.\n        output_projection: None or a pair (W, B). If provided, each fed previous\n            output will first be multiplied by W and added B.\n        update_embedding: Boolean; if False, the gradients will not propagate\n            through the embeddings.\n\n    Returns:\n        A loop function.\n    """"""\n    def loop_function(prev, _):\n        if output_projection is not None:\n            prev = tf.nn.xw_plus_b(prev,\n                                   output_projection[0], output_projection[1])\n        prev_symbol = tf.argmax(prev, 1)\n        # Note that gradients will not propagate through the second parameter of\n        # embedding_lookup.\n        emb_prev = tf.nn.embedding_lookup(embedding, prev_symbol)\n        if not update_embedding:\n            emb_prev = tf.stop_gradient(emb_prev)\n        return emb_prev\n    return loop_function\n\n\ndef attention_decoder(decoder_inputs, initial_state, attention_states, cell,\n                      output_size=None, num_heads=1, loop_function=None,\n                      dtype=tf.float32, scope=None,\n                      initial_state_attention=False, attn_num_hidden=128):\n    """"""RNN decoder with attention for the sequence-to-sequence model.\n\n    In this context ""attention"" means that, during decoding, the RNN can look up\n    information in the additional tensor attention_states, and it does this by\n    focusing on a few entries from the tensor. This model has proven to yield\n    especially good results in a number of sequence-to-sequence tasks. This\n    implementation is based on http://arxiv.org/abs/1412.7449 (see below for\n    details). It is recommended for complex sequence-to-sequence tasks.\n\n    Args:\n        decoder_inputs: A list of 2D Tensors [batch_size x input_size].\n        initial_state: 2D Tensor [batch_size x cell.state_size].\n        attention_states: 3D Tensor [batch_size x attn_length x attn_size].\n        cell: rnn_cell.RNNCell defining the cell function and size.\n        output_size: Size of the output vectors; if None, we use cell.output_size.\n        num_heads: Number of attention heads that read from attention_states.\n        loop_function: If not None, this function will be applied to i-th output\n            in order to generate i+1-th input, and decoder_inputs will be ignored,\n            except for the first element (""GO"" symbol). This can be used for decoding,\n            but also for training to emulate http://arxiv.org/abs/1506.03099.\n            Signature -- loop_function(prev, i) = next\n                * prev is a 2D Tensor of shape [batch_size x output_size],\n                * i is an integer, the step number (when advanced control is needed),\n                * next is a 2D Tensor of shape [batch_size x input_size].\n        dtype: The dtype to use for the RNN initial state (default: tf.float32).\n        scope: VariableScope for the created subgraph; default: ""attention_decoder"".\n        initial_state_attention: If False (default), initial attentions are zero.\n            If True, initialize the attentions from the initial state and attention\n            states -- useful when we wish to resume decoding from a previously\n            stored decoder state and attention states.\n\n    Returns:\n        A tuple of the form (outputs, state), where:\n            outputs: A list of the same length as decoder_inputs of 2D Tensors of\n                shape [batch_size x output_size]. These represent the generated outputs.\n                Output i is computed from input i (which is either the i-th element\n                of decoder_inputs or loop_function(output {i-1}, i)) as follows.\n                First, we run the cell on a combination of the input and previous\n                attention masks:\n                    cell_output, new_state = cell(linear(input, prev_attn), prev_state).\n                Then, we calculate new attention masks:\n                    new_attn = softmax(V^T * tanh(W * attention_states + U * new_state))\n                and then we calculate the output:\n                    output = linear(cell_output, new_attn).\n            state: The state of each decoder cell the final time-step.\n                It is a 2D Tensor of shape [batch_size x cell.state_size].\n\n    Raises:\n        ValueError: when num_heads is not positive, there are no inputs, or shapes\n            of attention_states are not set.\n    """"""\n    # MODIFIED ADD START\n    assert num_heads == 1, \'We only consider the case where num_heads=1!\'\n    # MODIFIED ADD END\n    if not decoder_inputs:\n        raise ValueError(""Must provide at least 1 input to attention decoder."")\n    if num_heads < 1:\n        raise ValueError(""With less than 1 heads, use a non-attention decoder."")\n    if not attention_states.get_shape()[1:2].is_fully_defined():\n        raise ValueError(""Shape[1] and [2] of attention_states must be known: %s""\n                         % attention_states.get_shape())\n    if output_size is None:\n        output_size = cell.output_size\n\n    with tf.variable_scope(scope or ""attention_decoder""):\n        batch_size = tf.shape(decoder_inputs[0])[0]  # Needed for reshaping.\n        attn_length = attention_states.get_shape()[1].value\n        attn_size = attention_states.get_shape()[2].value\n\n        # To calculate W1 * h_t we use a 1-by-1 convolution, need to reshape before.\n        hidden = tf.reshape(attention_states, [-1, attn_length, 1, attn_size])\n        hidden_features = []\n        v = []\n        attention_vec_size = attn_size  # Size of query vectors for attention.\n        for a in xrange(num_heads):\n            k = tf.get_variable(""AttnW_%d"" % a,\n                                [1, 1, attn_size, attention_vec_size])\n            hidden_features.append(tf.nn.conv2d(hidden, k, [1, 1, 1, 1], ""SAME""))\n            v.append(tf.get_variable(""AttnV_%d"" % a,\n                                     [attention_vec_size]))\n\n        state = initial_state\n\n        # MODIFIED: return both context vector and attention weights\n        def attention(query):\n            """"""Put attention masks on hidden using hidden_features and query.""""""\n            # MODIFIED ADD START\n            ss = None  # record attention weights\n            # MODIFIED ADD END\n            ds = []  # Results of attention reads will be stored here.\n            for a in xrange(num_heads):\n                with tf.variable_scope(""Attention_%d"" % a):\n                    y = linear(query, attention_vec_size, True)\n                    y = tf.reshape(y, [-1, 1, 1, attention_vec_size])\n                    # Attention mask is a softmax of v^T * tanh(...).\n                    s = tf.reduce_sum(v[a] * tf.tanh(hidden_features[a] + y), [2, 3])\n                    a = tf.nn.softmax(s)\n                    ss = a\n                    # a = tf.Print(a, [a], message=""a: "",summarize=30)\n                    # Now calculate the attention-weighted vector d.\n                    d = tf.reduce_sum(\n                        tf.reshape(a, [-1, attn_length, 1, 1]) * hidden,\n                        [1, 2]\n                    )\n                    ds.append(tf.reshape(d, [-1, attn_size]))\n            # MODIFIED DELETED return ds\n            # MODIFIED ADD START\n            return ds, ss\n            # MODIFIED ADD END\n\n        outputs = []\n        # MODIFIED ADD START\n        attention_weights_history = []\n        # MODIFIED ADD END\n        prev = None\n        batch_attn_size = tf.stack([batch_size, attn_size])\n        attns = [tf.zeros(batch_attn_size, dtype=dtype)\n                 for _ in xrange(num_heads)]\n        for a in attns:  # Ensure the second shape of attention vectors is set.\n            a.set_shape([None, attn_size])\n        if initial_state_attention:\n            # MODIFIED DELETED attns = attention(initial_state)\n            # MODIFIED ADD START\n            attns, attn_weights = attention(initial_state)\n            attention_weights_history.append(attn_weights)\n            # MODIFIED ADD END\n        for i, inp in enumerate(decoder_inputs):\n            if i > 0:\n                tf.get_variable_scope().reuse_variables()\n            # If loop_function is set, we use it instead of decoder_inputs.\n            if loop_function is not None and prev is not None:\n                with tf.variable_scope(""loop_function"", reuse=True):\n                    inp = loop_function(prev, i)\n            # Merge input and previous attentions into one vector of the right size.\n            # input_size = inp.get_shape().with_rank(2)[1]\n            # todo: use input_size\n            input_size = attn_num_hidden\n            x = linear([inp] + attns, input_size, True)\n            # Run the RNN.\n            cell_output, state = cell(x, state)\n            # Run the attention mechanism.\n            if i == 0 and initial_state_attention:\n                with tf.variable_scope(tf.get_variable_scope(),\n                                       reuse=True):\n                    # MODIFIED DELETED attns = attention(state)\n                    # MODIFIED ADD START\n                    attns, attn_weights = attention(state)\n                    # MODIFIED ADD END\n            else:\n                # MODIFIED DELETED attns = attention(state)\n                # MODIFIED ADD START\n                attns, attn_weights = attention(state)\n                attention_weights_history.append(attn_weights)\n                # MODIFIED ADD END\n\n            with tf.variable_scope(""AttnOutputProjection""):\n                output = linear([cell_output] + attns, output_size, True)\n            if loop_function is not None:\n                prev = output\n            outputs.append(output)\n\n    # MODIFIED DELETED return outputs, state\n    # MODIFIED ADD START\n    return outputs, state, attention_weights_history\n    # MODIFIED ADD END\n\n\ndef embedding_attention_decoder(decoder_inputs, initial_state, attention_states,\n                                cell, num_symbols, embedding_size, num_heads=1,\n                                output_size=None, output_projection=None,\n                                feed_previous=False,\n                                update_embedding_for_previous=True,\n                                dtype=tf.float32, scope=None,\n                                initial_state_attention=False,\n                                attn_num_hidden=128):\n    """"""RNN decoder with embedding and attention and a pure-decoding option.\n\n    Args:\n        decoder_inputs: A list of 1D batch-sized int32 Tensors (decoder inputs).\n        initial_state: 2D Tensor [batch_size x cell.state_size].\n        attention_states: 3D Tensor [batch_size x attn_length x attn_size].\n        cell: rnn_cell.RNNCell defining the cell function.\n        num_symbols: Integer, how many symbols come into the embedding.\n        embedding_size: Integer, the length of the embedding vector for each symbol.\n        num_heads: Number of attention heads that read from attention_states.\n        output_size: Size of the output vectors; if None, use output_size.\n        output_projection: None or a pair (W, B) of output projection weights and\n            biases; W has shape [output_size x num_symbols] and B has shape\n            [num_symbols]; if provided and feed_previous=True, each fed previous\n            output will first be multiplied by W and added B.\n        feed_previous: Boolean; if True, only the first of decoder_inputs will be\n            used (the ""GO"" symbol), and all other decoder inputs will be generated by:\n                next = embedding_lookup(embedding, argmax(previous_output)),\n            In effect, this implements a greedy decoder. It can also be used\n            during training to emulate http://arxiv.org/abs/1506.03099.\n            If False, decoder_inputs are used as given (the standard decoder case).\n        update_embedding_for_previous: Boolean; if False and feed_previous=True,\n            only the embedding for the first symbol of decoder_inputs (the ""GO""\n            symbol) will be updated by back propagation. Embeddings for the symbols\n            generated from the decoder itself remain unchanged. This parameter has\n            no effect if feed_previous=False.\n        dtype: The dtype to use for the RNN initial states (default: tf.float32).\n        scope: VariableScope for the created subgraph; defaults to\n            ""embedding_attention_decoder"".\n        initial_state_attention: If False (default), initial attentions are zero.\n            If True, initialize the attentions from the initial state and attention\n            states -- useful when we wish to resume decoding from a previously\n            stored decoder state and attention states.\n\n    Returns:\n        A tuple of the form (outputs, state), where:\n            outputs: A list of the same length as decoder_inputs of 2D Tensors with\n                shape [batch_size x output_size] containing the generated outputs.\n            state: The state of each decoder cell at the final time-step.\n                It is a 2D Tensor of shape [batch_size x cell.state_size].\n\n    Raises:\n        ValueError: When output_projection has the wrong shape.\n    """"""\n    if output_size is None:\n        output_size = cell.output_size\n    if output_projection is not None:\n        proj_biases = tf.convert_to_tensor(output_projection[1], dtype=dtype)\n        proj_biases.get_shape().assert_is_compatible_with([num_symbols])\n\n    with tf.variable_scope(scope or ""embedding_attention_decoder""):\n        with tf.device(""/cpu:0""):\n            embedding = tf.get_variable(""embedding"",\n                                        [num_symbols, embedding_size])\n        loop_function = _extract_argmax_and_embed(\n            embedding, output_projection,\n            update_embedding_for_previous) if feed_previous else None\n        emb_inp = [\n            tf.nn.embedding_lookup(embedding, i) for i in decoder_inputs]\n        return attention_decoder(\n            emb_inp, initial_state, attention_states, cell, output_size=output_size,\n            num_heads=num_heads, loop_function=loop_function,\n            initial_state_attention=initial_state_attention, attn_num_hidden=attn_num_hidden)\n\n\ndef sequence_loss_by_example(logits, targets, weights,\n                             average_across_timesteps=True,\n                             softmax_loss_function=None, name=None):\n    """"""Weighted cross-entropy loss for a sequence of logits (per example).\n\n    Args:\n        logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].\n        targets: List of 1D batch-sized int32 Tensors of the same length as logits.\n        weights: List of 1D batch-sized float-Tensors of the same length as logits.\n        average_across_timesteps: If set, divide the returned cost by the total\n            label weight.\n        softmax_loss_function: Function (inputs-batch, labels-batch) -> loss-batch\n            to be used instead of the standard softmax (the default if this is None).\n        name: Optional name for this operation, default: ""sequence_loss_by_example"".\n\n    Returns:\n        1D batch-sized float Tensor: The log-perplexity for each sequence.\n\n    Raises:\n        ValueError: If len(logits) is different from len(targets) or len(weights).\n    """"""\n    if len(targets) != len(logits) or len(weights) != len(logits):\n        raise ValueError(""Lengths of logits, weights, and targets must be the same ""\n                         ""%d, %d, %d."" % (len(logits), len(weights), len(targets)))\n    with tf.name_scope(name, ""sequence_loss_by_example"",\n                       logits + targets + weights):\n        log_perp_list = []\n        for logit, target, weight in zip(logits, targets, weights):\n            if softmax_loss_function is None:\n                # todo(irving,ebrevdo): This reshape is needed because\n                # sequence_loss_by_example is called with scalars sometimes, which\n                # violates our general scalar strictness policy.\n                target = tf.reshape(target, [-1])\n                crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n                    logits=logit, labels=target)\n            else:\n                crossent = softmax_loss_function(logits=logit, labels=target)\n            log_perp_list.append(crossent * weight)\n        log_perps = tf.add_n(log_perp_list)\n        if average_across_timesteps:\n            total_size = tf.add_n(weights)\n            total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.\n            log_perps /= total_size\n    return log_perps\n\n\ndef sequence_loss(logits, targets, weights,\n                  average_across_timesteps=True, average_across_batch=True,\n                  softmax_loss_function=None, name=None):\n    """"""Weighted cross-entropy loss for a sequence of logits, batch-collapsed.\n\n    Args:\n        logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].\n        targets: List of 1D batch-sized int32 Tensors of the same length as logits.\n        weights: List of 1D batch-sized float-Tensors of the same length as logits.\n        average_across_timesteps: If set, divide the returned cost by the total\n            label weight.\n        average_across_batch: If set, divide the returned cost by the batch size.\n        softmax_loss_function: Function (inputs-batch, labels-batch) -> loss-batch\n            to be used instead of the standard softmax (the default if this is None).\n        name: Optional name for this operation, defaults to ""sequence_loss"".\n\n    Returns:\n        A scalar float Tensor: The average log-perplexity per symbol (weighted).\n\n    Raises:\n        ValueError: If len(logits) is different from len(targets) or len(weights).\n    """"""\n    with tf.name_scope(name, ""sequence_loss"", logits + targets + weights):\n        cost = tf.reduce_sum(sequence_loss_by_example(\n            logits, targets, weights,\n            average_across_timesteps=average_across_timesteps,\n            softmax_loss_function=softmax_loss_function))\n        if average_across_batch:\n            batch_size = tf.shape(targets[0])[0]\n            return cost / tf.cast(batch_size, tf.float32)\n\n        return cost\n\n\ndef model_with_buckets(encoder_inputs_tensor, decoder_inputs, targets, weights,\n                       buckets, seq2seq, softmax_loss_function=None,\n                       per_example_loss=False, name=None):\n    """"""Create a sequence-to-sequence model with support for bucketing.\n\n    The seq2seq argument is a function that defines a sequence-to-sequence model,\n    e.g., seq2seq = lambda x, y: basic_rnn_seq2seq(x, y, rnn_cell.GRUCell(24))\n\n    Args:\n        encoder_inputs: A list of Tensors to feed the encoder; first seq2seq input.\n        decoder_inputs: A list of Tensors to feed the decoder; second seq2seq input.\n        targets: A list of 1D batch-sized int32 Tensors (desired output sequence).\n        weights: List of 1D batch-sized float-Tensors to weight the targets.\n        buckets: A list of pairs of (input size, output size) for each bucket.\n        seq2seq: A sequence-to-sequence model function; it takes 2 input that\n            agree with encoder_inputs and decoder_inputs, and returns a pair\n            consisting of outputs and states (as, e.g., basic_rnn_seq2seq).\n        softmax_loss_function: Function (inputs-batch, labels-batch) -> loss-batch\n            to be used instead of the standard softmax (the default if this is None).\n        per_example_loss: Boolean. If set, the returned loss will be a batch-sized\n            tensor of losses for each sequence in the batch. If unset, it will be\n            a scalar with the averaged loss from all examples.\n        name: Optional name for this operation, defaults to ""model_with_buckets"".\n\n    Returns:\n        A tuple of the form (outputs, losses), where:\n            outputs: The outputs for each bucket. Its j\'th element consists of a list\n                of 2D Tensors of shape [batch_size x num_decoder_symbols] (jth outputs).\n            losses: List of scalar Tensors, representing losses for each bucket, or,\n                if per_example_loss is set, a list of 1D batch-sized float Tensors.\n\n    Raises:\n        ValueError: If length of encoder_inputsut, targets, or weights is smaller\n            than the largest (last) bucket.\n    """"""\n    if len(targets) < buckets[-1][1]:\n        raise ValueError(""Length of targets (%d) must be at least that of last""\n                         ""bucket (%d)."" % (len(targets), buckets[-1][1]))\n    if len(weights) < buckets[-1][1]:\n        raise ValueError(""Length of weights (%d) must be at least that of last""\n                         ""bucket (%d)."" % (len(weights), buckets[-1][1]))\n\n    all_inputs = [encoder_inputs_tensor] + decoder_inputs + targets + weights\n    with tf.name_scope(name, ""model_with_buckets"", all_inputs):\n        with tf.variable_scope(tf.get_variable_scope(), reuse=None):\n            bucket = buckets[0]\n            encoder_inputs = tf.split(encoder_inputs_tensor, bucket[0], 0)\n            encoder_inputs = [tf.squeeze(inp, squeeze_dims=[0]) for inp in encoder_inputs]\n            bucket_outputs, attention_weights_history = seq2seq(encoder_inputs[:int(bucket[0])],\n                                                                decoder_inputs[:int(bucket[1])],\n                                                                int(bucket[0]))\n            if per_example_loss:\n                loss = sequence_loss_by_example(\n                    bucket_outputs, targets[:int(bucket[1])], weights[:int(bucket[1])],\n                    average_across_timesteps=True,\n                    softmax_loss_function=softmax_loss_function)\n            else:\n                loss = sequence_loss(\n                    bucket_outputs, targets[:int(bucket[1])], weights[:int(bucket[1])],\n                    average_across_timesteps=True,\n                    softmax_loss_function=softmax_loss_function)\n\n    return bucket_outputs, loss, attention_weights_history\n'"
aocr/model/seq2seq_model.py,11,"b'# Copyright 2015 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Sequence-to-sequence model with an attention mechanism.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\nfrom .seq2seq import model_with_buckets\nfrom .seq2seq import embedding_attention_decoder\n\n\nclass Seq2SeqModel(object):\n    """"""Sequence-to-sequence model with attention and for multiple buckets.\n    This class implements a multi-layer recurrent neural network as encoder,\n    and an attention-based decoder. This is the same as the model described in\n    this paper: http://arxiv.org/abs/1412.7449 - please look there for details,\n    or into the seq2seq library for complete model implementation.\n    This class also allows to use GRU cells in addition to LSTM cells, and\n    sampled softmax to handle large output vocabulary size. A single-layer\n    version of this model, but with bi-directional encoder, was presented in\n      http://arxiv.org/abs/1409.0473\n    and sampled softmax is described in Section 3 of the following paper.\n      http://arxiv.org/abs/1412.2007\n    """"""\n\n    def __init__(self, encoder_masks, encoder_inputs_tensor,\n                 decoder_inputs,\n                 target_weights,\n                 target_vocab_size,\n                 buckets,\n                 target_embedding_size,\n                 attn_num_layers,\n                 attn_num_hidden,\n                 forward_only,\n                 use_gru):\n        """"""Create the model.\n\n        Args:\n          source_vocab_size: size of the source vocabulary.\n          target_vocab_size: size of the target vocabulary.\n          buckets: a list of pairs (I, O), where I specifies maximum input length\n            that will be processed in that bucket, and O specifies maximum output\n            length. Training instances that have inputs longer than I or outputs\n            longer than O will be pushed to the next bucket and padded accordingly.\n            We assume that the list is sorted, e.g., [(2, 4), (8, 16)].\n          size: number of units in each layer of the model.\n          num_layers: number of layers in the model.\n          max_gradient_norm: gradients will be clipped to maximally this norm.\n          learning_rate: learning rate to start with.\n          learning_rate_decay_factor: decay learning rate by this much when needed.\n          use_lstm: if true, we use LSTM cells instead of GRU cells.\n          num_samples: number of samples for sampled softmax.\n          forward_only: if set, we do not construct the backward pass in the model.\n        """"""\n        self.encoder_inputs_tensor = encoder_inputs_tensor\n        self.decoder_inputs = decoder_inputs\n        self.target_weights = target_weights\n        self.target_vocab_size = target_vocab_size\n        self.buckets = buckets\n        self.encoder_masks = encoder_masks\n\n        # Create the internal multi-layer cell for our RNN.\n        single_cell = tf.contrib.rnn.BasicLSTMCell(\n            attn_num_hidden, forget_bias=0.0, state_is_tuple=False\n        )\n        if use_gru:\n            print(""using GRU CELL in decoder"")\n            single_cell = tf.contrib.rnn.GRUCell(attn_num_hidden)\n        cell = single_cell\n\n        if attn_num_layers > 1:\n            cell = tf.contrib.rnn.MultiRNNCell(\n                [single_cell] * attn_num_layers, state_is_tuple=False\n            )\n\n        # The seq2seq function: we use embedding for the input and attention.\n        def seq2seq_f(lstm_inputs, decoder_inputs, seq_length, do_decode):\n\n            num_hidden = attn_num_layers * attn_num_hidden\n            lstm_fw_cell = tf.contrib.rnn.BasicLSTMCell(\n                num_hidden, forget_bias=0.0, state_is_tuple=False\n            )\n            # Backward direction cell\n            lstm_bw_cell = tf.contrib.rnn.BasicLSTMCell(\n                num_hidden, forget_bias=0.0, state_is_tuple=False\n            )\n\n            (pre_encoder_inputs,\n             output_state_fw,\n             output_state_bw) = tf.contrib.rnn.static_bidirectional_rnn(\n                 lstm_fw_cell, lstm_bw_cell, lstm_inputs,\n                 initial_state_fw=None, initial_state_bw=None,\n                 dtype=tf.float32, sequence_length=None, scope=None)\n\n            encoder_inputs = [e*f for e, f in zip(pre_encoder_inputs, encoder_masks[:seq_length])]\n            top_states = [tf.reshape(e, [-1, 1, num_hidden*2])\n                          for e in encoder_inputs]\n            attention_states = tf.concat(top_states, 1)\n            initial_state = tf.concat(axis=1, values=[output_state_fw, output_state_bw])\n            outputs, _, attention_weights_history = embedding_attention_decoder(\n                decoder_inputs, initial_state, attention_states, cell,\n                num_symbols=target_vocab_size,\n                embedding_size=target_embedding_size,\n                num_heads=1,\n                output_size=target_vocab_size,\n                output_projection=None,\n                feed_previous=do_decode,\n                initial_state_attention=False,\n                attn_num_hidden=attn_num_hidden)\n            return outputs, attention_weights_history\n\n        # Our targets are decoder inputs shifted by one.\n        targets = [decoder_inputs[i + 1]\n                   for i in xrange(len(decoder_inputs) - 1)]\n\n        softmax_loss_function = None  # default to tf.nn.sparse_softmax_cross_entropy_with_logits\n\n        # Training outputs and losses.\n        if forward_only:\n            self.output, self.loss, self.attention_weights_history = model_with_buckets(\n                encoder_inputs_tensor, decoder_inputs, targets,\n                self.target_weights, buckets, lambda x, y, z: seq2seq_f(x, y, z, True),\n                softmax_loss_function=softmax_loss_function)\n        else:\n            self.output, self.loss, self.attention_weights_history = model_with_buckets(\n                encoder_inputs_tensor, decoder_inputs, targets,\n                self.target_weights, buckets, lambda x, y, z: seq2seq_f(x, y, z, False),\n                softmax_loss_function=softmax_loss_function)\n\n        self.attentions = self.attention_weights_history\n'"
aocr/util/__init__.py,0,b''
aocr/util/bucketdata.py,0,"b""from __future__ import absolute_import\n\nimport numpy as np\n\n\nclass BucketData(object):\n    def __init__(self):\n        self.data_list = []\n        self.label_list = []\n        self.label_list_plain = []\n        self.comment_list = []\n\n    def append(self, datum, label, label_plain, comment):\n        self.data_list.append(datum)\n        self.label_list.append(label)\n        self.label_list_plain.append(label_plain)\n        self.comment_list.append(comment)\n\n        return len(self.data_list)\n\n    def flush_out(self, bucket_specs, valid_target_length=float('inf'),\n                  go_shift=1):\n        # print self.max_width, self.max_label_len\n        res = {}\n\n        decoder_input_len = bucket_specs[0][1]\n\n        # ENCODER PART\n        res['data'] = np.array(self.data_list)\n        res['labels'] = self.label_list_plain\n        res['comments'] = self.comment_list\n\n        # DECODER PART\n        target_weights = []\n        for l_idx in range(len(self.label_list)):\n            label_len = len(self.label_list[l_idx])\n            if label_len <= decoder_input_len:\n                self.label_list[l_idx] = np.concatenate((\n                    self.label_list[l_idx],\n                    np.zeros(decoder_input_len - label_len, dtype=np.int32)))\n                one_mask_len = min(label_len - go_shift, valid_target_length)\n                target_weights.append(np.concatenate((\n                    np.ones(one_mask_len, dtype=np.float32),\n                    np.zeros(decoder_input_len - one_mask_len,\n                             dtype=np.float32))))\n            else:\n                raise NotImplementedError\n\n        res['decoder_inputs'] = [a.astype(np.int32) for a in\n                                 np.array(self.label_list).T]\n        res['target_weights'] = [a.astype(np.float32) for a in\n                                 np.array(target_weights).T]\n\n        assert len(res['decoder_inputs']) == len(res['target_weights'])\n\n        self.data_list, self.label_list, self.label_list_plain, self.comment_list = [], [], [], []\n\n        return res\n\n    def __len__(self):\n        return len(self.data_list)\n\n    def __iadd__(self, other):\n        self.data_list += other.data_list\n        self.label_list += other.label_list\n        self.label_list_plain += other.label_list_plain\n        self.comment_list += other.comment_list\n\n    def __add__(self, other):\n        res = BucketData()\n        res.data_list = self.data_list + other.data_list\n        res.label_list = self.label_list + other.label_list\n        res.label_list_plain = self.label_list_plain + other.label_list_plain\n        res.comment_list = self.comment_list + other.comment_list\n        return res\n"""
aocr/util/data_gen.py,8,"b'from __future__ import absolute_import\n\nimport sys\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom PIL import Image\nfrom six import BytesIO as IO\n\nfrom .bucketdata import BucketData\n\ntry:\n    TFRecordDataset = tf.data.TFRecordDataset  # pylint: disable=invalid-name\nexcept AttributeError:\n    TFRecordDataset = tf.contrib.data.TFRecordDataset  # pylint: disable=invalid-name\n\n\nclass DataGen(object):\n    GO_ID = 1\n    EOS_ID = 2\n    IMAGE_HEIGHT = 32\n    CHARMAP = [\'\', \'\', \'\'] + list(\'0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ\')\n\n    @staticmethod\n    def set_full_ascii_charmap():\n        DataGen.CHARMAP = [\'\', \'\', \'\'] + [chr(i) for i in range(32, 127)]\n\n    def __init__(self,\n                 annotation_fn,\n                 buckets,\n                 epochs=1000,\n                 max_width=None):\n        """"""\n        :param annotation_fn:\n        :param lexicon_fn:\n        :param valid_target_len:\n        :param img_width_range: only needed for training set\n        :param word_len:\n        :param epochs:\n        :return:\n        """"""\n        self.epochs = epochs\n        self.max_width = max_width\n\n        self.bucket_specs = buckets\n        self.bucket_data = BucketData()\n\n        dataset = TFRecordDataset([annotation_fn])\n        dataset = dataset.map(self._parse_record)\n        dataset = dataset.shuffle(buffer_size=10000)\n        self.dataset = dataset.repeat(self.epochs)\n\n    def clear(self):\n        self.bucket_data = BucketData()\n\n    def gen(self, batch_size):\n\n        dataset = self.dataset.batch(batch_size)\n        iterator = dataset.make_one_shot_iterator()\n\n        images, labels, comments = iterator.get_next()\n        with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n\n            while True:\n                try:\n                    raw_images, raw_labels, raw_comments = sess.run([images, labels, comments])\n                    for img, lex, comment in zip(raw_images, raw_labels, raw_comments):\n\n                        if self.max_width and (Image.open(IO(img)).size[0] <= self.max_width):\n                            word = self.convert_lex(lex)\n\n                            bucket_size = self.bucket_data.append(img, word, lex, comment)\n                            if bucket_size >= batch_size:\n                                bucket = self.bucket_data.flush_out(\n                                    self.bucket_specs,\n                                    go_shift=1)\n                                yield bucket\n\n                except tf.errors.OutOfRangeError:\n                    break\n\n        self.clear()\n\n    def convert_lex(self, lex):\n        if sys.version_info >= (3,):\n            lex = lex.decode(\'iso-8859-1\')\n\n        assert len(lex) < self.bucket_specs[-1][1]\n\n        return np.array(\n            [self.GO_ID] + [self.CHARMAP.index(char) for char in lex] + [self.EOS_ID],\n            dtype=np.int32)\n\n    @staticmethod\n    def _parse_record(example_proto):\n        features = tf.parse_single_example(\n            example_proto,\n            features={\n                \'image\': tf.FixedLenFeature([], tf.string),\n                \'label\': tf.FixedLenFeature([], tf.string),\n                \'comment\': tf.FixedLenFeature([], tf.string, default_value=\'\'),\n            })\n        return features[\'image\'], features[\'label\'], features[\'comment\']\n'"
aocr/util/dataset.py,4,"b""from __future__ import absolute_import\n\nimport logging\nimport re\n\nimport tensorflow as tf\n\nfrom six import b\n\n\ndef _bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef _int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef generate(annotations_path, output_path, log_step=5000,\n             force_uppercase=True, save_filename=False):\n\n    logging.info('Building a dataset from %s.', annotations_path)\n    logging.info('Output file: %s', output_path)\n\n    writer = tf.python_io.TFRecordWriter(output_path)\n    longest_label = ''\n    idx = 0\n\n    with open(annotations_path, 'r') as annotations:\n        for idx, line in enumerate(annotations):\n            line = line.rstrip('\\n')\n\n            # Split the line on the first whitespace character and allow empty values for the label\n            # NOTE: this does not allow whitespace in image paths\n            line_match = re.match(r'(\\S+)\\s(.*)', line)\n            if line_match is None:\n                logging.error('missing filename or label, ignoring line %i: %s', idx+1, line)\n                continue\n            (img_path, label) = line_match.groups()\n\n            with open(img_path, 'rb') as img_file:\n                img = img_file.read()\n\n            if force_uppercase:\n                label = label.upper()\n\n            if len(label) > len(longest_label):\n                longest_label = label\n\n            feature = {}\n            feature['image'] = _bytes_feature(img)\n            feature['label'] = _bytes_feature(b(label))\n            if save_filename:\n                feature['comment'] = _bytes_feature(b(img_path))\n\n            example = tf.train.Example(features=tf.train.Features(feature=feature))\n\n            writer.write(example.SerializeToString())\n\n            if idx % log_step == 0:\n                logging.info('Processed %s pairs.', idx+1)\n\n    if idx:\n        logging.info('Dataset is ready: %i pairs.', idx+1)\n        logging.info('Longest label (%i): %s', len(longest_label), longest_label)\n\n    writer.close()\n"""
aocr/util/export.py,4,"b'from __future__ import absolute_import\n\nimport os\nimport logging\n\nimport tensorflow as tf\n\n\nclass Exporter(object):\n    def __init__(self, model):\n        self.model = model\n\n    def save(self, path, model_format):\n        if model_format == ""savedmodel"":\n            logging.info(""Creating a SavedModel."")\n\n            builder = tf.saved_model.builder.SavedModelBuilder(path)\n            freezing_graph = self.model.sess.graph\n            builder.add_meta_graph_and_variables(\n                self.model.sess,\n                [""serve""],\n                signature_def_map={\n                    \'serving_default\': tf.saved_model.signature_def_utils.predict_signature_def(\n                        {\'input\': freezing_graph.get_tensor_by_name(\'input_image_as_bytes:0\')},\n                        {\n                            \'output\': freezing_graph.get_tensor_by_name(\'prediction:0\'),\n                            \'probability\': freezing_graph.get_tensor_by_name(\'probability:0\')\n                        }\n                    ),\n                },\n                clear_devices=True)\n\n            builder.save()\n\n            logging.info(""Exported SavedModel into %s"", path)\n\n        elif model_format == ""frozengraph"":\n\n            logging.info(""Creating a frozen graph."")\n\n            if not os.path.exists(path):\n                os.makedirs(path)\n\n            output_graph_def = tf.graph_util.convert_variables_to_constants(\n                self.model.sess,\n                self.model.sess.graph.as_graph_def(),\n                [\'prediction\', \'probability\'],\n            )\n\n            with tf.gfile.GFile(path + \'/frozen_graph.pb\', ""wb"") as outfile:\n                outfile.write(output_graph_def.SerializeToString())\n\n            logging.info(""Exported as %s"", path + \'/frozen_graph.pb\')\n'"
aocr/util/visualizations.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\n\nimport math\nimport os\n\nfrom io import BytesIO\n\nimport numpy as np\n\nfrom PIL import Image\n\n\ndef visualize_attention(filename, output_dir, attentions, pred, pad_width,\n                        pad_height, threshold=1, normalize=False,\n                        binarize=True, ground=None, flag=None):\n    """"""Visualize the focus of the attention mechanism on an image.\n\n    Parameters\n    ----------\n    filename : string\n        Input filename.\n    output_dir : string\n        Output directory for visualizations.\n    attentions : array of shape [len(pred), attention_size]\n        Attention weights.\n    pred : string\n        Predicted output.\n    pad_width : int\n        Padded image width in pixels used as model input.\n    pad_height : int\n        Padded image height in pixels used as model input.\n    threshold : int or float, optional (default=1)\n        Threshold of maximum attention weight to display.\n    normalize : bool, optional (default=False)\n        Normalize the attention values to the [0, 1] range.\n    binarize : bool, optional (default=True)\n        If normalized, set attention values below `threshold` to 0.\n        If not normalized, set maximum attention values to 1 and others to 0.\n    ground : string or None, optional (default=None)\n        Ground truth label.\n    flag : bool or None, optional (default=None)\n        Incorrect prediction flag.\n\n    """"""\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    if flag is None:\n        filestring = \'predict-{}\'.format(str(pred))\n        idx = 2\n        while filestring in os.listdir(output_dir):\n            filestring = \'predict-{}-{}\'.format(str(pred), idx)\n            idx += 1\n        out_dir = output_dir\n    elif flag:\n        filestring = os.path.splitext(os.path.basename(filename))[0]\n        out_dir = os.path.join(output_dir, \'incorrect\')\n    else:\n        filestring = os.path.splitext(os.path.basename(filename))[0]\n        out_dir = os.path.join(output_dir, \'correct\')\n    out_dir = os.path.join(out_dir, filestring.replace(\'/\', \'_\'))\n\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n\n    with open(os.path.join(out_dir, \'word.txt\'), \'w\') as fword:\n        fword.write(pred + \'\\n\')\n        if ground is not None:\n            fword.write(ground)\n\n        if isinstance(filename, str):\n            img_file = open(filename, \'rb\')\n            img = Image.open(img_file)\n        else:\n            img = Image.open(BytesIO(filename))\n\n        # Get image sequence with attention applied.\n        img_data = np.asarray(img, dtype=np.uint8)\n        img_out_frames, _ = map_attentions(img_data,\n                                           attentions,\n                                           pred,\n                                           pad_width,\n                                           pad_height,\n                                           threshold=threshold,\n                                           normalize=normalize,\n                                           binarize=binarize)\n\n        # Create initial image frame.\n        img_out_init = (img_data * 0.3).astype(np.uint8)\n        img_out_init = Image.fromarray(img_out_init)\n        img_out_init = img_out_init.convert(\'RGB\')\n\n        # Assemble animation frames.\n        img_out_frames = [img_out_init] + img_out_frames\n\n        # Save cropped and animated images.\n        output_animation = os.path.join(out_dir, \'image.gif\')\n\n        img_out_frames[0].save(output_animation, format=\'gif\', save_all=True, loop=999,\n                               duration=500, append_images=img_out_frames[1:])\n\n        if isinstance(filename, str):\n            img_file.close()\n        img.close()\n\n\ndef map_attentions(img_data, attentions, pred, pad_width, pad_height,\n                   threshold=1, normalize=False, binarize=True):\n    """"""Map the attentions to the image.""""""\n    img_out_agg = np.zeros(img_data.shape)\n    img_out_frames = []\n\n    width, height = img_data.shape[1], img_data.shape[0]\n\n    # Calculate the image resizing proportions.\n    width_resize_ratio, height_resize_ratio = 1, 1\n    max_width = math.ceil((width / height) * pad_height)\n    max_height = math.ceil((pad_width / max_width) * pad_height)\n    if pad_width >= max_width:\n        if pad_height < height:\n            width_resize_ratio = width / max_width\n            height_resize_ratio = height / pad_height\n    else:\n        width_resize_ratio = width / pad_width\n        height_resize_ratio = height / max_height\n\n    # Map the attention for each predicted character.\n    for idx in range(len(pred)):\n        attention = attentions[0][idx]\n\n        # Get maximal attentional focus.\n        score = attention.max()\n\n        # Reshape the attention vector.\n        nrows = 1  # should match number of encoded rows\n        attention = attention.reshape((nrows, -1))\n\n        # Map attention to fixed value.\n        if normalize:\n            attention *= (1.0 / attention.max())\n            if binarize:\n                attention[attention < threshold] = 0\n        elif binarize:\n            attention[attention >= score * threshold] = 1\n            attention[attention < score] = 0\n\n        # Resize attention to the image size, cropping padded regions.\n        attention = Image.fromarray(attention)\n        attention = attention.resize(\n            (int(pad_width*width_resize_ratio), int(pad_height*height_resize_ratio)),\n            Image.NEAREST)\n        attention = attention.crop((0, 0, width, height))\n        attention = np.asarray(attention)\n\n        # Add new axis as needed (e.g., RGB images).\n        if len(img_data.shape) == 3:\n            attention = attention[..., np.newaxis]\n\n        # Update the image frame with attended region(s).\n        img_out_i = (img_data * np.maximum(attention, 0.3)).astype(np.uint8)\n        img_out_i = Image.fromarray(img_out_i)\n        img_out_i = img_out_i.convert(\'RGB\')\n\n        # Add animation frame to list.\n        img_out_frames.append(img_out_i)\n\n        # Add attention to aggregate.\n        img_out_agg += attention\n\n    return img_out_frames, img_out_agg\n'"
