file_path,api_count,code
GAN-version/colorize_base.py,0,"b'# System Modules\nimport os\nimport time\n\n# Extra\nfrom keras.engine.topology import Input\nfrom keras.engine.training import Model\nfrom keras.layers import LeakyReLU, Concatenate, Dropout\nfrom keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\nfrom keras.layers.core import Activation, SpatialDropout2D\nfrom keras.layers.merge import concatenate\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D\nfrom models.utils.instance_normalization import InstanceNormalization\nfrom models.utils.sn import ConvSN2D\nfrom models.utils.calc_output_and_feature_size import calc_output_and_feature_size\nfrom models.utils.attention import Attention\nfrom keras.layers import Conv2D, Lambda, add, AvgPool2D, Activation, UpSampling2D, Input, concatenate, Reshape, LeakyReLU, Reshape, Flatten, concatenate\n\n# Custom Libs\nfrom models.utils.calc_output_and_feature_size import calc_output_and_feature_size\nfrom lib.data_utils import save_sample_images, write_log, generate_training_images\nfrom lib.data_utils import generator, generate_label_data\n\n# Keras Modules\nimport keras\nfrom keras.utils import multi_gpu_model\nfrom keras.layers import Lambda, UpSampling2D, Input, concatenate\nfrom keras.utils.data_utils import  GeneratorEnqueuer\nfrom keras.utils import multi_gpu_model\nfrom keras.callbacks import TensorBoard\nfrom keras.optimizers import Adam\nfrom keras.models import Model, save_model, load_model\nfrom keras import backend as K\nK.clear_session()\n\n# Import models\nfrom models.discriminator_full import DiscriminatorFull\nfrom models.discriminator_low import DiscriminatorLow\nfrom models.discriminator_medium import DiscriminatorMedium\nfrom models.core_generator import CoreGenerator\n\n# Other Modules \nimport tensorflow as tf\nimport numpy as np\n\n# ----------\n#  Settings \n# ----------\n\nheight = 128\nwidth = 128\nchannels = 1\nepochs = 10\ngpus = 1\nbatch_size = 5\ncpus = 2\nuse_multiprocessing = True\nsave_weights_every_n_epochs = 0.01\nmax_queue_size=batch_size * 1\nimg_dir = ""./Train/""\ntest_dir = ""./Test/""\nresource_dir = ""./resources/""\ndataset_len = len(os.listdir(img_dir))\ntestset_len = len(os.listdir(test_dir))\nlearning_rate = 0.0002\nexperiment_name = time.strftime(""%Y-%m-%d-%H-%M"")\ndecay_rate = 0\ndecay_rate = learning_rate / ((dataset_len / batch_size) * epochs)\n\n\n# ----------------------------------\n# Load filenames\n#-----------------------------------\n\nX = []\nfor filename in os.listdir(img_dir):\n    X.append(filename)\n\nTest = []\nfor filename in os.listdir(test_dir):\n    Test.append(filename)    \n    \n# ----------------------------------\n#  Create directory for sample data\n# ----------------------------------\n\nmain_dir = \'./output/256/\' + experiment_name\nsave_sample_images_dir = main_dir + \'/sample_images/\'\nsave_validation_images_dir = main_dir + \'/validation_images/\'\nweights_dir = main_dir +\'/weights/\'\nlog_path = main_dir + \'/logs/\'\nmodel_path = main_dir + \'/models/\'\n\nif not os.path.exists(main_dir):\n    os.makedirs(main_dir)\n    os.makedirs(save_sample_images_dir)\n    os.makedirs(save_validation_images_dir)\n    os.makedirs(log_path)\n    os.makedirs(weights_dir)\n    os.makedirs(model_path)\n\n# ---------------\n#  Import Models \n# ---------------\n    \ncore_generator = CoreGenerator(gpus=gpus, width=width, height=height)\ndiscriminator_full = DiscriminatorFull(gpus=gpus, decay_rate=decay_rate, width=width, height=height)\ndiscriminator_medium = DiscriminatorMedium(gpus=gpus, decay_rate=decay_rate, width=width, height=height)\ndiscriminator_low = DiscriminatorLow(gpus=gpus, decay_rate=decay_rate, width=width, height=height)\n\nif os.path.isdir(""./resources/""):\n    core_generator.model.load_weights(\'./resources/core_generator.h5\')\n    discriminator_full.model.load_weights(\'./resources/discriminator_full.h5\')\n    discriminator_medium.model.load_weights(\'./resources/discriminator_medium.h5\')\n    discriminator_low.model.load_weights(\'./resources/discriminator_low.h5\')\n\n# Create a directory to save weights\nif not os.path.exists(resource_dir):\n    os.makedirs(resource_dir)\n\ndiscriminator_full.trainable = False\ndiscriminator_medium.model.trainable = False\ndiscriminator_full.model.trainable = False\n\n\n# --------------------------------\n#  Create GAN with core generator\n# --------------------------------\n\n# Generate image with core generator\ngan_x = Input(shape=(height, width, channels,))\ngan_y = Input(shape=(height, width, 2,))\n\n# Extract style features and add them to image\ngan_output = core_generator.model(gan_x)\n\n# Extract features and predictions from discriminators\ndisc_input = concatenate([gan_x, gan_output], axis=-1)\npred_full, features_full = discriminator_full.model(disc_input)\npred_medium, features_medium = discriminator_medium.model(disc_input)\npred_low, features_low = discriminator_low.model(disc_input)\n\n# Compile GAN\ngan_core = Model(inputs=gan_x, outputs=[gan_output, features_full, features_medium, features_low, pred_full, pred_medium, pred_low])                  \n\ngan_core.name = ""gan_core""\noptimizer = Adam(learning_rate, 0.5, decay=decay_rate)\nloss_gan = [\'mae\', \'mae\', \'mae\', \'mae\', \'mse\', \'mse\', \'mse\']\nloss_weights_gan = [1, 3.33, 3.33, 3.33, 0.33, 0.33, 0.33]\n\n# gan_core = multi_gpu_model(gan_core_org)\ngan_core.compile(optimizer=optimizer, loss_weights=loss_weights_gan, loss=loss_gan)\n\n\n# --------------------------------\n#  Compile Discriminator\n# --------------------------------\n\ndiscriminator_full.model.trainable = True\ndiscriminator_medium.model.trainable = True\ndiscriminator_low.model.trainable = True\n\ndef zero_loss(y_true, y_pred):\n    return K.zeros_like(y_true)\n\nloss_d = [\'mse\', zero_loss]\nloss_weights_d = [1, 0]\noptimizer_dis = Adam(learning_rate, 0.5, decay=decay_rate)\n\ndiscriminator_full_multi = discriminator_full.model\ndiscriminator_medium_multi = discriminator_medium.model\ndiscriminator_low_multi = discriminator_low.model\n\ndiscriminator_full_multi.compile(optimizer=optimizer_dis, loss_weights=loss_weights_d, loss=loss_d)\ndiscriminator_medium_multi.compile(optimizer=optimizer_dis, loss_weights=loss_weights_d, loss=loss_d)\ndiscriminator_low_multi.compile(optimizer=optimizer_dis, loss_weights=loss_weights_d, loss=loss_d)\n\n\n# --------------------------------------------------\n#  Initiate Generator Queue\n# --------------------------------------------------\n\nenqueuer = GeneratorEnqueuer(generator(X, img_dir, batch_size, dataset_len, width, height), use_multiprocessing=use_multiprocessing, wait_time=0.01)\n\nenqueuer.start(workers=cpus, max_queue_size=max_queue_size)\noutput_generator = enqueuer.get()\n\n# ---------------------------------\n#  Initiate values for Tensorboard\n# ---------------------------------\n\ncallback_Full = TensorBoard(log_path)\ncallback_Medium = TensorBoard(log_path)\ncallback_Low = TensorBoard(log_path)\ncallback_gan = TensorBoard(log_path)\n\ncallback_Full.set_model(discriminator_full.model)\ncallback_Medium.set_model(discriminator_medium.model)\ncallback_Low.set_model(discriminator_low.model)\ncallback_gan.set_model(gan_core)\n\ncallback_Full_names = [\'weighted_loss_real_full\', \'disc_loss_real_full\', \'zero_1\', \'weighted_loss_fake_full\', \'disc_loss_fake_full\', \'zero_2\']\ncallback_Medium_names = [\'weighted_loss_real_low\', \'disc_loss_real_medium\', \'zero_3\', \'weighted_loss_fake_medium\', \'disc_loss_fake_medium\', \'zero_4\']\ncallback_Low_names = [\'weighted_loss_real_low\', \'disc_loss_real_low\', \'zero_3\', \'weighted_loss_fake_low\', \'disc_loss_fake_low\', \'zero_4\']\ncallback_gan_names = [\'total_gan_loss\', \'image_diff\', \'feature_diff_disc_full\', \'feature_diff_disc_low\', \'predictions_full\', \'predictions_low\']\n\n# Decide how often to create sample images, save log data, and weights. \ncycles = int(epochs * (dataset_len / batch_size))\nsave_images_cycle = int((dataset_len / batch_size))\nsave_weights_cycle = int((dataset_len / batch_size))\n\n# Calculate the discriminator output size for features and image predictions\npred_size_f, feat_size_f = calc_output_and_feature_size(width, height)\npred_size_m, feat_size_m = calc_output_and_feature_size(width/2, height/2)\npred_size_l, feat_size_l = calc_output_and_feature_size(width/4, height/4)\n\n# Create benchmark to see progress\nstart = time.time()\n\ndef concatenateNumba(x, y):\n    return np.concatenate([x, y], axis=-1)\n\nfor i in range(0, cycles):\n    start_c = time.time()\n    # ------------------------\n    #  Generate Training Data\n    # ------------------------\n\n    # Discriminator data\n    x_full, y_full, x_and_y_full = next(output_generator)\n    x_medium, y_medium, x_and_y_medium = next(output_generator)\n    x_low, y_low, x_and_y_low = next(output_generator)\n    \n    # Fixed data\n    fake_labels_f, true_labels_f, dummy_f = generate_label_data(batch_size, pred_size_f, feat_size_f)\n    fake_labels_m, true_labels_m, dummy_m = generate_label_data(batch_size, pred_size_m, feat_size_m)\n    fake_labels_l, true_labels_l, dummy_l = generate_label_data(batch_size, pred_size_l, feat_size_l)\n  \n    # GAN data\n    x_gan, y_gan, x_and_y_gan = next(output_generator)\n\n    # ----------------------\n    #  Train Discriminators \n    # ----------------------\n\n    y_gen_full, _, _, _, _, _, _ = gan_core.predict(x_full)\n    x_and_y_gen_full = concatenateNumba(x_full, y_gen_full)\n    \n    # Prepare data for Medium Resolution Discriminator \n    y_gen_medium, _, _, _, _ , _, _= gan_core.predict(x_medium)\n    x_and_y_gen_medium = concatenateNumba(x_medium, y_gen_medium)\n    \n    # Prepare data for Low Resolution Discriminator \n    y_gen_low, _, _, _, _ , _, _= gan_core.predict(x_low)\n    x_and_y_gen_low = concatenateNumba(x_low, y_gen_low)\n\n    # Train Discriminators \n    d_loss_fake_full = discriminator_full_multi.train_on_batch(x_and_y_gen_full, [fake_labels_f, dummy_f])\n    d_loss_real_full = discriminator_full_multi.train_on_batch(x_and_y_full, [true_labels_f, dummy_f])\n    \n    d_loss_fake_medium = discriminator_medium_multi.train_on_batch(x_and_y_gen_medium, [fake_labels_m, dummy_m])\n    d_loss_real_medium = discriminator_medium_multi.train_on_batch(x_and_y_medium, [true_labels_m, dummy_m])\n   \n    d_loss_fake_low = discriminator_low_multi.train_on_batch(x_and_y_gen_low, [fake_labels_l, dummy_l])\n    d_loss_real_low = discriminator_low_multi.train_on_batch(x_and_y_low, [true_labels_l, dummy_l])\n\n    # -----------\n    #  Train GAN\n    # -----------\n    \n\n    # Extract featuers from discriminators \n    _, real_features_full = discriminator_full_multi.predict(x_and_y_gan)\n    _, real_features_medium = discriminator_medium_multi.predict(x_and_y_gan)\n    _, real_features_low = discriminator_low_multi.predict(x_and_y_gan)\n    \n    # Train GAN on one batch\n    gan_core_loss = gan_core.train_on_batch(x_gan, [y_gan, \n                                                    real_features_full,\n                                                    real_features_medium,\n                                                    real_features_low,\n                                                    true_labels_f,\n                                                    true_labels_m,\n                                                    true_labels_l])\n\n    # -------------------------------------------\n    #  Save image samples, weights, and log data\n    # -------------------------------------------\n    \n    # Print log data to tensorboard\n    write_log(callback_Full, callback_Full_names, d_loss_fake_full + d_loss_real_full, i)\n    write_log(callback_Medium, callback_Medium_names, d_loss_fake_medium + d_loss_real_medium, i)\n    write_log(callback_Low, callback_Low_names, d_loss_fake_low + d_loss_real_low, i)\n    write_log(callback_gan, callback_gan_names, gan_core_loss, i)\n    \n    end_c = time.time()\n    print(""\\n\\nCycle:"", i)\n    print(""Time:"", end_c - start_c)\n    print(""Total images:"", batch_size * i)\n\n    # Save sample images\n    if i % save_images_cycle == 0:\n        print(\'Print those bad boys:\', i)\n        end = time.time()\n        hours, rem = divmod(end-start, 3600)\n        minutes, seconds = divmod(rem, 60)\n        print(""{:0>2}:{:0>2}:{:05.2f}"".format(int(hours),int(minutes),seconds))\n        x_val, y_val, x_y_val = generate_training_images(Test, 5, testset_len, width, height, test_dir)\n        output_benchmark, _, _, _, _, _ ,_ = gan_core.predict(x_val)\n        save_sample_images(output_benchmark, x_val, \'b-\' + str(i), save_validation_images_dir)\n        save_sample_images(y_gen_full, x_full, str(i), save_sample_images_dir)\n        start = time.time()\n\n    #  Save weights\n    if i % save_weights_cycle == 0:\n        discriminator_full.model.save_weights(weights_dir + str(i) + ""-discriminator_full.h5"")\n        discriminator_medium.model.save_weights(weights_dir + str(i) + ""-discriminator_medium.h5"")\n        discriminator_low.model.save_weights(weights_dir + str(i) + ""-discriminator_low.h5"")\n        core_generator.model.save_weights(weights_dir + str(i) + ""-core_generator.h5"")\n\n        \n        discriminator_full.model.save_weights(resource_dir + ""discriminator_full.h5"")\n        discriminator_medium.model.save_weights(resource_dir + ""discriminator_medium.h5"")\n        discriminator_low.model.save_weights(resource_dir + ""discriminator_low.h5"")\n        core_generator.model.save_weights(resource_dir + ""core_generator.h5"")\n\n\n'"
download_and_clean_data_scripts/multi_crop_images.py,0,"b'import os\nfrom PIL import Image\nfrom multiprocessing import Pool\n\n\ndef crop_image(path_and_file):\n\twith Image.open(path_and_file[0]) as im:\n\t\tx, y = im.size\n\t\tim.crop((0, 18, x - 18, y)).save(\'/home/userai/jobs/drawing2logo/data/screenshots_382/\' + path_and_file[1], ""PNG"")\n\nif __name__ == ""__main__"":\n\timg_dir = r""./screenshots/""\n\timages = []\n\t\n\tfor filename in os.listdir(img_dir):\n\t\tfilepath = os.path.join(img_dir, filename)\n\t\timages.append([filepath, filename])\n\t\n\tpool = Pool(processes=120) \n\tpool.map(crop_image, images)\n\n\tprint(""Done!"") '"
download_and_clean_data_scripts/multi_resize.py,0,"b'import os\nfrom PIL import Image\nfrom multiprocessing import Pool\n\nsize = 224, 224\n\ndef resize_image(path_and_file):\n\twith Image.open(path_and_file[0]) as im:\n\t\tim.thumbnail(size)\n\t\tim.save(\'/home/userai/jobs/drawing2logo/data/screenshots_224/\' + path_and_file[1], ""PNG"")\n\nif __name__ == ""__main__"":\n\timg_dir = r""./screenshots_382/""\n\timages = []\n\t\n\tfor filename in os.listdir(img_dir):\n\t\tfilepath = os.path.join(img_dir, filename)\n\t\timages.append([filepath, filename])\n\t\n\tpool = Pool(processes=125) \n\tpool.map(resize_image, images)\n\n\tprint(""Done!"") '"
GAN-version/lib/data_utils.py,1,"b""import numpy as np\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom keras.preprocessing import image\nimport tensorflow as tf\nfrom keras.callbacks import TensorBoard\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\nimport os\nfrom skimage.transform import resize, rotate, rescale\nfrom skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\nfrom skimage.io import imsave\nimport random\n\n\ndef turn_filename_into_image(filenames, batch_size, width, height, img_dir):\n    \n    empty_x = []\n    rot_value = random.randint(-20,20)\n    flip_lr = bool(random.getrandbits(1))\n    flip_ud = bool(random.getrandbits(1))\n    \n    for name in filenames:\n        image_x = img_to_array(load_img(img_dir + name, target_size=(width, height)))\n        image_x = np.array(image_x, dtype='float32')\n        image_x = (1.0/(255./2))*image_x - 1\n     \n        image_x = rotate(image_x, rot_value, mode='reflect')\n\n        if flip_lr:\n            image_x = np.fliplr(image_x)\n        \n        empty_x.append(image_x)\n        \n    empty_x = np.array(empty_x, dtype='float32')\n    lab_batch = rgb2lab(empty_x)\n    X_batch = lab_batch[:,:,:,0] / 100\n    X_batch = X_batch.reshape(X_batch.shape+(1,))\n    Y_batch = lab_batch[:,:,:,1:] / 128\n    \n    return np.array(X_batch, dtype='float32'), np.array(Y_batch, dtype='float32')\n\n\ndef random_image_index(dataset_len, batchsize):\n    start = random.randint(0,(dataset_len - (batchsize + 1)))\n    end = start + batchsize\n    return start, end\n\ndef generate_training_images(filenames, batch_size, dataset_len, width, height, img_dir):\n    \n    start, end = random_image_index(dataset_len, batch_size)\n    names = filenames[start:end]\n    x, y = turn_filename_into_image(names, batch_size, width, height, img_dir)\n    x_and_y = np.concatenate([x, y], axis=-1)\n    \n    return x, y, x_and_y\n\ndef generator(X, img_dir, batch_size, dataset_len, width, height):\n    while True:\n        x, y, x_and_y = generate_training_images(X, batch_size, dataset_len, width, height, img_dir)\n        yield x, y, x_and_y\n\ndef generate_label_data(batch_size, output_size_pred, output_size_features):\n\n    fake_labels = np.zeros((batch_size, output_size_pred, 1))\n    true_labels = np.ones((batch_size, output_size_pred, 1))\n    placeholder_input = np.zeros((batch_size, output_size_features, 1))\n    \n    return fake_labels, true_labels, placeholder_input\n\n\ndef save_each_image(colored_layers, BW_layer, cycle, nr, path, ending):\n    \n    cur = np.zeros((128, 128, 3))\n    cur[:,:,0] = BW_layer[:,:,0] * 100\n    cur[:,:,1:] = colored_layers * 128\n    imsave(os.path.join(path, cycle + nr + ending), lab2rgb(cur))\n\ndef save_sample_images(colored_layers, BW_layer, cycle, path):\n    for i in range(len(colored_layers)):\n        save_each_image(colored_layers[i], BW_layer[i], cycle, str(i), path, '-gen.png')\n\n        \ndef write_log(callback, names, logs, batch_no):\n    for name, value in zip(names, logs):\n        summary = tf.Summary()\n        summary_value = summary.value.add()\n        summary_value.simple_value = value\n        summary_value.tag = name\n        callback.writer.add_summary(summary, batch_no)\n        callback.writer.flush()\n"""
GAN-version/models/core_generator.py,0,"b'from keras.engine.topology import Input\nfrom keras.engine.training import Model\nfrom keras.layers import LeakyReLU, Concatenate, Dropout\nfrom keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\nfrom keras.layers.core import Activation, SpatialDropout2D\nfrom keras.layers.merge import concatenate\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D\nfrom .utils.instance_normalization import InstanceNormalization\nfrom .utils.sn import ConvSN2D\nfrom .utils.attention import Attention\nimport keras\n\n\nclass CoreGenerator():\n    """"""Core Generator.\n        \n    # Arguments\n        width: Width of image in pixels\n        height: Height of image in pixels\n        channels: Channels for the input image\n        gpus: The number of gpus you will be using. \n    """"""\n\n    def __init__(self,\n                 width=256,\n                 height=256,\n                 channels=1,\n                 gpus = 0):\n        \n        self.width = width\n        self.height = height\n        self.channels = channels\n        self.gpus = gpus\n        self.gf = 64\n\n        \n        # -------------------------------------------------------------------------------------\n        #  Core Generator \n        #  The U-net structure is from Erik Linder-Noren\'s brilliant pix2pix model\n        #  Source: https://github.com/eriklindernoren/Keras-GAN/blob/master/pix2pix/pix2pix.py\n        #  Modifications: Thinner to enable 128x128 images, Spectral Normalization and \n        #  an Attention layer. \n        # -------------------------------------------------------------------------------------\n        \n\n        def conv2d(layer_input, filters, f_size=4):\n            """"""Layers used during downsampling""""""\n            d = ConvSN2D(filters, kernel_size=f_size, strides=2, padding=\'same\')(layer_input)\n            d = LeakyReLU(alpha=0.2)(d)\n\n            return d\n\n        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n            """"""Layers used during upsampling""""""\n            u = UpSampling2D(size=2)(layer_input)\n            u = ConvSN2D(filters, kernel_size=f_size, strides=1, padding=\'same\', activation=\'relu\')(u)\n            if dropout_rate:\n                u = Dropout(dropout_rate)(u)\n            u = Concatenate()([u, skip_input])\n            return u\n\n        # Image input\n        d1 = Input(shape=(width, height, channels))\n\n        # Downsampling\n        d2 = conv2d(d1, self.gf*2)\n        d3 = conv2d(d2, self.gf*4)\n        d4 = conv2d(d3, self.gf*8)\n        d5 = conv2d(d4, self.gf*8)\n        d6 = conv2d(d5, self.gf*8)\n        d7 = conv2d(d6, self.gf*8)\n\n        # Upsampling\n        u1 = deconv2d(d7, d6, self.gf*8)\n        u2 = deconv2d(u1, d5, self.gf*8)\n        u3 = deconv2d(u2, d4, self.gf*8)\n        u4 = deconv2d(u3, d3, self.gf*4)\n        u4_att = Attention(512)(u4)\n        u5 = deconv2d(u4_att, d2, self.gf*2)\n\n        u6 = UpSampling2D(size=2)(u5)\n        output = ConvSN2D(2, kernel_size=(7,7), strides=1, padding=\'same\', activation=\'tanh\')(u6)\n        \n        core_generator = Model(d1, output)\n        core_generator.name = ""core_generator""\n        \n        # --------------\n        #  Compile Model\n        # --------------\n        \n        if self.gpus < 2:\n            self.model = core_generator\n            self.save_model = self.model\n        else:\n            self.save_model = core_generator\n            self.model = multi_gpu_model(self.save_model, gpus=gpus)\n'"
GAN-version/models/discriminator_full.py,0,"b'from .utils.calc_output_and_feature_size import calc_output_and_feature_size\nfrom .utils.instance_normalization import InstanceNormalization\nfrom keras.models import model_from_json, Model\nfrom .utils.sn import ConvSN2D\nfrom keras.optimizers import Adam\nfrom keras import backend as K\nfrom .utils.attention import Attention\nfrom keras.utils import multi_gpu_model\nfrom keras.layers import Conv2D, Lambda, add, AvgPool2D, Activation, UpSampling2D, Input, concatenate, Reshape, LeakyReLU, Reshape, Flatten, concatenate\n\n\nclass DiscriminatorFull():\n    """"""Full Resolution Discriminator.\n        \n    # Arguments\n        width: Width of image in pixels\n        height: Height of image in pixels\n        channels: Channels for the input image and the generated image\n        gpus: The number of gpus you will be using.\n        learning_rate: Learning rate\n        decay_rate: The amount of learning decay for each training update\n    """"""\n\n    def __init__(self,\n                 width=256,\n                 height=256,\n                 channels=3,\n                 learning_rate=0.0002,\n                 decay_rate=2e-6,\n                 gpus = 0):\n        \n        self.width = width\n        self.height = height\n        self.channels = channels\n        self.gpus = gpus\n        self.learning_rate = learning_rate\n        self.decay_rate = decay_rate\n\n\n        # ----------------------\n        #  Discriminator Fullres\n        # ----------------------\n\n        output_size_full_picture, output_size_full_features = calc_output_and_feature_size(self.width, self.height)\n\n        discriminator_input = Input(shape=(self.height, self.width, self.channels,))\n\n        x_1 = ConvSN2D(64, 4, padding=\'same\', strides=2)(discriminator_input)\n        x = LeakyReLU(alpha=0.2)(x_1)\n\n        x_2 = ConvSN2D(128, 4, padding=\'same\', strides=2)(x)\n        x = LeakyReLU(alpha=0.2)(x_2)\n        \n        x_2_att = Attention(128)(x)\n        \n        x_3 = ConvSN2D(256, 4, padding=\'same\', strides=2)(x_2_att)\n        x = LeakyReLU(alpha=0.2)(x_3)\n\n        x_4 = ConvSN2D(512, 4, padding=\'same\', strides=1)(x)\n        x = LeakyReLU(alpha=0.2)(x_4)\n\n        x = ConvSN2D(1, 4, padding=\'same\', strides=1)(x)\n        x = Reshape([output_size_full_picture, 1])(x)\n\n        discriminator_features = concatenate([Flatten()(x_1), Flatten()(x_2), Flatten()(x_3), Flatten()(x_4)], axis=1)\n        discriminator_features = Reshape([output_size_full_features, 1])(discriminator_features)\n\n\n        def zero_loss(y_true, y_pred):\n            return K.zeros_like(y_true)\n\n        loss_d = [\'mse\', zero_loss]\n        \n        if self.gpus < 2:\n            self.model = Model(discriminator_input, [x, discriminator_features])\n            self.save_model = self.model\n        else:\n            self.save_model = Model(discriminator_input, [x, discriminator_features])\n            self.model = multi_gpu_model(self.save_model, gpus=gpus)\n\n        loss_weights_d = [1, 0]\n        optimizer = Adam(self.learning_rate, 0.5, decay=self.decay_rate)\n        self.model.compile(optimizer=optimizer, loss_weights=loss_weights_d, loss=loss_d)\n\n        \n'"
GAN-version/models/discriminator_low.py,0,"b'from .utils.calc_output_and_feature_size import calc_output_and_feature_size\nfrom .utils.sn import ConvSN2D\nfrom .utils.instance_normalization import InstanceNormalization\nfrom keras.models import model_from_json, Model\nfrom keras.optimizers import Adam\nfrom keras import backend as K\nfrom .utils.attention import Attention\nfrom keras.utils import multi_gpu_model\nfrom keras.layers import Conv2D, Lambda, add, AvgPool2D, Activation, UpSampling2D, Input, concatenate, Reshape, LeakyReLU, Reshape, Flatten, concatenate\n\nclass DiscriminatorLow():\n    """"""1/4 Resolution Discriminator.\n        \n    # Arguments\n        width: Width of image in pixels\n        height: Height of image in pixels\n        channels: Channels for the input image \n        gpus: The number of gpus you will be using.\n        learning_rate: Learning rate\n        decay_rate: The amount of learning decay for each training update\n    """"""\n\n    def __init__(self,\n                 width=256,\n                 height=256,\n                 channels=3,\n                 learning_rate=0.0002,\n                 decay_rate=2e-6,\n                 gpus = 0):\n        \n        self.width = width\n        self.height = height\n        self.channels = channels\n        self.gpus = gpus\n        self.learning_rate = learning_rate\n        self.decay_rate = decay_rate\n        \n        # -----------------------------\n        #  Discriminator Low resolution\n        # -----------------------------\n\n        output_size_low_picture, output_size_low_features = calc_output_and_feature_size(self.height/4, self.width/4)\n\n        discriminator_low_res_input = Input(shape=(self.height, self.width, self.channels,))\n        discriminator_low_res_input_downsample = AvgPool2D(2, padding=\'same\')(discriminator_low_res_input)\n        discriminator_low_res_input_downsample = AvgPool2D(2, padding=\'same\')(discriminator_low_res_input_downsample)\n\n        x_1 = ConvSN2D(64, 4, padding=\'same\', strides=2)(discriminator_low_res_input_downsample)\n        x = LeakyReLU(alpha=0.2)(x_1)\n        \n        x_1_att = Attention(64)(x)\n        \n        x_2 = ConvSN2D(128, 4, padding=\'same\', strides=2)(x_1_att)\n        x = LeakyReLU(alpha=0.2)(x_2)\n\n        x_3 = ConvSN2D(256, 4, padding=\'same\', strides=2)(x)\n        x = LeakyReLU(alpha=0.2)(x_3)\n\n        x_4 = ConvSN2D(512, 4, padding=\'same\', strides=1)(x)\n        x = LeakyReLU(alpha=0.2)(x_4)\n\n        x = ConvSN2D(1, 4, padding=\'same\', strides=1)(x)\n        x = Reshape([output_size_low_picture, 1])(x)\n\n        discriminator_low_features = concatenate([Flatten()(x_1), Flatten()(x_2), Flatten()(x_3), Flatten()(x_4)], axis=1)\n        discriminator_low_features = Reshape([output_size_low_features, 1])(discriminator_low_features)\n\n        def zero_loss(y_true, y_pred):\n            return K.zeros_like(y_true)\n\n        loss_d = [\'mse\', zero_loss]\n        loss_weights_d = [1, 0]\n        optimizer = Adam(self.learning_rate, 0.5, decay=self.decay_rate)\n        \n        if self.gpus < 2:\n            self.model = Model(discriminator_low_res_input, [x, discriminator_low_features])\n            self.save_model = self.model\n        else:\n            self.save_model = Model(discriminator_low_res_input, [x, discriminator_low_features])\n            self.model = multi_gpu_model(self.save_model, gpus=self.gpus)\n        \n        self.model.compile(optimizer=optimizer, loss_weights=loss_weights_d, loss=loss_d)\n'"
GAN-version/models/discriminator_medium.py,0,"b'from .utils.calc_output_and_feature_size import calc_output_and_feature_size\nfrom .utils.sn import ConvSN2D\nfrom .utils.instance_normalization import InstanceNormalization\nfrom keras.models import model_from_json, Model\nfrom keras.optimizers import Adam\nfrom keras import backend as K\nfrom .utils.attention import Attention\nfrom keras.utils import multi_gpu_model\nfrom keras.layers import Conv2D, Lambda, add, AvgPool2D, Activation, UpSampling2D, Input, concatenate, Reshape, LeakyReLU, Reshape, Flatten, concatenate\n\nclass DiscriminatorMedium():\n    """"""1/2 Resolution Discriminator.\n        \n    # Arguments\n        width: Width of image in pixels\n        height: Height of image in pixels\n        channels: Channels for the input image\n        gpus: The number of gpus you will be using.\n        learning_rate: Learning rate\n        decay_rate: The amount of learning decay for each training update\n    """"""\n\n    def __init__(self,\n                 width=256,\n                 height=256,\n                 channels=3,\n                 learning_rate=0.0002,\n                 decay_rate=2e-6,\n                 gpus = 0):\n        \n        self.width = width\n        self.height = height\n        self.channels = channels\n        self.gpus = gpus\n        self.learning_rate = learning_rate\n        self.decay_rate = decay_rate\n        \n        # -----------------------------\n        #  Discriminator Low resolution\n        # -----------------------------\n\n        output_size_low_picture, output_size_low_features = calc_output_and_feature_size(self.height/2, self.width/2)\n\n        discriminator_low_res_input = Input(shape=(self.height, self.width, self.channels,))\n        discriminator_low_res_input_downsample = AvgPool2D(2, padding=\'same\')(discriminator_low_res_input)\n\n        x_1 = ConvSN2D(64, 4, padding=\'same\', strides=2)(discriminator_low_res_input_downsample)\n        x = LeakyReLU(alpha=0.2)(x_1)\n        \n        x_1_att = Attention(64)(x)\n\n        x_2 = ConvSN2D(128, 4, padding=\'same\', strides=2)(x_1_att)\n        x = LeakyReLU(alpha=0.2)(x_2)\n\n        x_3 = ConvSN2D(256, 4, padding=\'same\', strides=2)(x)\n        x = LeakyReLU(alpha=0.2)(x_3)\n\n        x_4 = ConvSN2D(512, 4, padding=\'same\', strides=1)(x)\n        x = LeakyReLU(alpha=0.2)(x_4)\n\n        x = ConvSN2D(1, 4, padding=\'same\', strides=1)(x)\n        x = Reshape([output_size_low_picture, 1])(x)\n\n        discriminator_low_features = concatenate([Flatten()(x_1), Flatten()(x_2), Flatten()(x_3), Flatten()(x_4)], axis=1)\n        discriminator_low_features = Reshape([output_size_low_features, 1])(discriminator_low_features)\n\n        def zero_loss(y_true, y_pred):\n            return K.zeros_like(y_true)\n\n        loss_d = [\'mse\', zero_loss]\n        loss_weights_d = [1, 0]\n        optimizer = Adam(self.learning_rate, 0.5, decay=self.decay_rate)\n        \n        if self.gpus < 2:\n            self.model = Model(discriminator_low_res_input, [x, discriminator_low_features])\n            self.save_model = self.model\n        else:\n            self.save_model = Model(discriminator_low_res_input, [x, discriminator_low_features])\n            self.model = multi_gpu_model(self.save_model, gpus=self.gpus)\n        \n        self.model.compile(optimizer=optimizer, loss_weights=loss_weights_d, loss=loss_d)\n'"
download_and_clean_data_scripts/pixabay/pixabay_main_custom.py,0,"b'# -*- coding: utf-8 -*-\r\n# @Time    : 2018/8/16 10:59\r\n# @Author  : \xe9\x99\x88\xe5\xad\x90\xe6\x98\x82\r\nimport os\r\nimport requests\r\nfrom bs4 import BeautifulSoup\r\nfrom tqdm import tqdm\r\nimport sys\r\nfrom utils import save_img, path_processor, img_name_processor\r\n\r\n\r\ndef pexels(keyword):\r\n    img_cnt = 0\r\n    if not keyword: sys.exit(\'\xe7\xa8\x8b\xe5\xba\x8f\xe9\x80\x80\xe5\x87\xba\xef\xbc\x9a\xe6\x9c\xaa\xe8\xbe\x93\xe5\x85\xa5\xe5\x85\xb3\xe9\x94\xae\xe5\xad\x97\xef\xbc\x81\')\r\n    for page in tqdm(range(1, 50)):\r\n        print(f\'\\n-----[{keyword}]\xe6\xad\xa3\xe5\x9c\xa8\xe7\x88\xac\xe5\x8f\x96\xe7\xac\xac{page}\xe9\xa1\xb5-----\')\r\n        pexels_url = ""https://www.pexels.com/search/%s/?page=%s"" % (keyword, page)\r\n\r\n        headers = {\'User-Agent\':\'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36\'}\r\n        res = requests.get(pexels_url,headers=headers,verify=False)\r\n\r\n        # print(res.text)\r\n        if \'Sorry, no pictures found!\' in res.text:\r\n            print(\'-*--*--*-\xe7\x88\xac\xe5\x8f\x96\xe5\xae\x8c\xe6\xaf\x95-*--*--*-\')\r\n            sys.exit(0)\r\n\r\n        soup = BeautifulSoup(res.text, \'lxml\')\r\n        # print(soup)\r\n        articles = soup.find_all(\'article\')\r\n        # print(len(articles))\r\n        for article in articles:\r\n            src = article.img.attrs[\'src\']\r\n            print(src)\r\n            path = rf\'D://\xe4\xba\xba\xe8\x84\xb8\xe7\x9b\xb8\xe5\x85\xb3\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87//pexels//{keyword}\'\r\n            if not os.path.exists(path):\r\n                os.makedirs(path)\r\n            filename = img_name_processor(src)\r\n            file = os.path.join(path, filename)\r\n            save_img(file=file, src=src)\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    \r\n    categories = [\'male\', \'old\', \'vintage\', \'dog\', \'cat\', \'building\', \'nature\', \'castle\', \'water\', \'ocean\', \'cities\', \'body\', \'hands\', \'people\', \'culture\', \'religion\', \'color\', \'patterns\', \'houses\', \'vintage\', \'river\', \'landscape\', \'lights\', \'animals\', \'wallpaper\', \'texture\', \'current events\', \'architecture\', \'business\', \'work\', \'travel\', \'fashion\', \'food\', \'drink\', \'spirituality\', \'experimental\', \'health\', \'arts\', \'culture\', \'children\', \'people\', \'events\', \'trees\', \'green\', \'yellow\', \'pink\', \'blue\', \'red\', \'minimal\', \'hands\', \'head\', \'eyes\', \'mouth\', \'eating\', \'playing\', \'sports\']\r\n    for i in categories:\r\n        pexels(i)\r\n'"
download_and_clean_data_scripts/pixabay/utils.py,0,"b'# -*- coding: utf-8 -*-\r\nimport os\r\nimport requests\r\nimport hashlib\r\nimport time\r\nfrom random import random\r\nfrom datetime import datetime\r\nimport logging\r\n\r\ntoday = datetime.today().date()\r\n\r\nlogging.basicConfig(level=logging.INFO,\r\n    format=\'%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s\',\r\n    datefmt=\'%Y-%m-%d %H:%M:%S\',\r\n    filename=\'log/%s.log\' % today,\r\n    filemode=\'a\')\r\n\r\ndef save_img(file, src):\r\n    \'\'\'\r\n    This function is used to save pictures.\r\n    Initiates an HTTP request to the picture URL,\r\n    gets the binary code,\r\n    writes the code to the local file,\r\n    and completes the preservation of a picture.\r\n    :param file:folder path\r\n    :param src: image url\r\n    :return:\r\n    \'\'\'\r\n    if os.path.exists(file):\r\n        print(f\'-{file}\xe5\xb7\xb2\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe8\xb7\xb3\xe8\xbf\x87\xe3\x80\x82-\')\r\n    else: # This is done simply to dedup process\r\n        try:\r\n            headers = {\r\n                \'User-Agent\': \'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36\'}\r\n            res = requests.get(src, timeout=3, verify=False,headers=headers)\r\n            # print(res.content)\r\n        except Exception as e:\r\n            print(f\'--{e}--\')\r\n            logging.warning(f\'{os.path.split(__file__)[1]} - {src} - {e}\')\r\n            return False\r\n        else:\r\n            if res.status_code == 200:\r\n                img = res.content\r\n                open(file, \'wb\').write(img)\r\n                time.sleep(random())\r\n                return True\r\n\r\n\r\ndef path_processor(site, folder):\r\n    \'\'\'\r\n    :param site: site name\xef\xbc\x8cpexels\xef\xbc\x8cpixabay\xef\xbc\x8cgoogle\r\n    :param folder: category name\r\n    :return path: path\r\n    \'\'\'\r\n    categories = [[\'\xe4\xb9\x90\xe5\x99\xa8\', \'\xe7\xac\x9b\xe5\xad\x90\', \'\xe9\xbc\x93\', \'\xe9\x95\xbf\xe5\x8f\xb7\', \'\xe9\x92\xa2\xe7\x90\xb4\', \'\xe5\xb0\x8f\xe6\x8f\x90\xe7\x90\xb4\',\'\xe5\xa5\xb3\xe8\x84\xb8\'],\r\n                  [\'\xe4\xba\xa4\xe9\x80\x9a\xe5\xb7\xa5\xe5\x85\xb7\', \'\xe9\x9d\xa2\xe5\x8c\x85\xe8\xbd\xa6\', \'\xe6\x91\xa9\xe6\x89\x98\xe8\xbd\xa6\', \'\xe8\xbd\xbf\xe8\xbd\xa6\', \'SUV\', \'\xe7\x94\xb5\xe7\x93\xb6\xe8\xbd\xa6\', \'\xe4\xb8\x89\xe8\xbd\xae\xe8\xbd\xa6\', \'\xe8\x87\xaa\xe8\xa1\x8c\xe8\xbd\xa6\', \'\xe8\x88\xb9\', \'\xe5\xa4\xa7\xe5\xae\xa2\xe8\xbd\xa6\', \'\xe5\xbe\xae\xe5\x9e\x8b\xe8\xbd\xa6\'],\r\n                  [\'\xe5\x8a\x9e\xe5\x85\xac\xe4\xba\xa7\xe5\x93\x81\', \'\xe6\x98\xbe\xe7\xa4\xba\xe5\xb1\x8f\', \'\xe9\xbc\xa0\xe6\xa0\x87\', \'\xe5\x9e\x83\xe5\x9c\xbe\xe7\xaf\x93\', \'\xe8\xb7\xaf\xe7\x94\xb1\xe5\x99\xa8\', \'\xe6\x8a\x98\xe5\x8f\xa0\xe5\xba\x8a\', \'\xe5\x8a\x9e\xe5\x85\xac\xe6\xa1\x8c\', \'\xe7\x94\xb5\xe8\xaf\x9d\', \'\xe6\x89\x93\xe5\x8d\xb0\xe6\x9c\xba\', \'\xe9\x94\xae\xe7\x9b\x98\', \'\xe4\xb9\xa6\xe6\x9c\xac\', \'\xe7\x94\xb5\xe8\x84\x91\xe6\xa4\x85\', \'\xe6\x8a\x95\xe5\xbd\xb1\xe4\xbb\xaa\', \'\xe7\xbb\xbf\xe6\xa4\x8d\xe7\x9b\x86\xe6\xa0\xbd\', \'\xe6\x9c\xac\xe5\xad\x90\',\r\n                   \'\xe7\xac\x94\xe7\xb1\xbb\', \'\xe6\x8e\xa5\xe7\xba\xbf\xe6\x9d\xbf\', \'\xe7\xac\x94\xe8\xae\xb0\xe6\x9c\xac\xe7\x94\xb5\xe8\x84\x91\', \'\xe6\x96\x87\xe4\xbb\xb6\xe6\x94\xb6\xe7\xba\xb3\', \'\xe5\xa4\x9a\xe8\x82\x89\xe7\x9b\x86\xe6\xa0\xbd\', \'\xe6\x96\x87\xe4\xbb\xb6\xe6\x9f\x9c\', \'\xe7\xa2\x8e\xe7\xba\xb8\xe6\x9c\xba\', \'\xe5\xb9\xb3\xe6\x9d\xbf\xe7\x94\xb5\xe8\x84\x91\', \'\xe8\xae\xa2\xe4\xb9\xa6\xe6\x9c\xba\', \'\xe4\xbf\x9d\xe9\x99\xa9\xe6\x9f\x9c\', \'\xe8\xae\xa1\xe7\xae\x97\xe5\x99\xa8\'],\r\n                  [\'\xe5\x9c\xba\xe6\x99\xaf\', \'\xe5\x95\x86\xe5\x9c\xba\xe5\x86\x85\xe6\x99\xaf\', \'\xe9\x85\x92\xe5\x90\xa7\xe5\xa4\x9c\xe5\xba\x97\', \'\xe5\x8d\xa7\xe5\xae\xa4\', \'\xe6\xb9\x96\xe6\xb3\x8a\', \'\xe5\xb1\xb1\', \'\xe5\x9c\xb0\xe9\x93\x81\xe5\x86\x85\xe6\x99\xaf\', \'\xe5\x8e\xa2\xe5\xbc\x8f\xe7\x94\xb5\xe6\xa2\xaf\xe5\xa4\x96\xe6\x99\xaf\', \'\xe6\xb2\x99\xe6\xbb\xa9\', \'\xe8\xbd\xbf\xe8\xbd\xa6\xe5\x86\x85\xe6\x99\xaf\', \'\xe7\xaf\xae\xe7\x90\x83\xe5\x9c\xba\', \'\xe5\x9b\xbe\xe4\xb9\xa6\xe9\xa6\x86\xe5\x86\x85\xe6\x99\xaf\', \'\xe8\xb7\x91\xe9\x81\x93\', \'\xe5\xb9\xbf\xe5\x9c\xba\',\r\n                   \'\xe5\xae\xa2\xe5\x8e\x85\',\r\n                   \'\xe7\x94\xb0\xe9\x87\x8e\', \'\xe5\x85\xac\xe8\xb7\xaf\', \'\xe5\x8d\xab\xe7\x94\x9f\xe9\x97\xb4\', \'\xe8\xb6\x85\xe5\xb8\x82\xe5\x86\x85\xe6\x99\xaf\', \'\xe5\xa4\xa7\xe9\x97\xa8\xe5\x8f\xa3\', \'\xe8\xa1\x97\xe9\x81\x93\', \'\xe7\x94\xb5\xe5\xbd\xb1\xe9\x99\xa2\xe5\x86\x85\xe6\x99\xaf\', \'\xe8\x8d\x89\xe5\x9d\xaa\', \'\xe5\x8e\xa8\xe6\x88\xbf\', \'\xe5\x8e\xa2\xe5\xbc\x8f\xe7\x94\xb5\xe6\xa2\xaf\xe5\x86\x85\xe6\x99\xaf\', \'\xe5\x86\x99\xe5\xad\x97\xe6\xa5\xbc\xe5\xa4\x96\xe6\x99\xaf\', \'\xe7\x80\x91\xe5\xb8\x83\', \'\xe8\xb6\xb3\xe7\x90\x83\xe5\x9c\xba\', \'\xe9\xb2\x9c\xe8\x8a\xb1\',\r\n                   \'\xe5\xa4\xa9\xe7\xa9\xba\',\r\n                   \'\xe5\x8a\x9e\xe5\x85\xac\xe5\xae\xa4\', \'\xe6\xa0\x91\xe6\x9c\xa8\', \'\xe6\x89\x8b\xe6\x89\xb6\xe7\x94\xb5\xe6\xa2\xaf\', \'\xe9\xa4\x90\xe5\x8e\x85\xe5\x86\x85\xe6\x99\xaf\', \'\xe5\x81\xa5\xe8\xba\xab\xe6\x88\xbf\xe5\x86\x85\xe6\x99\xaf\'],\r\n                  [\'\xe5\xae\xb6\xe7\x94\xa8\xe7\x94\xb5\xe5\x99\xa8\', \'\xe6\xb4\x97\xe8\xa1\xa3\xe6\x9c\xba\', \'\xe5\xa3\x81\xe6\x8c\x82\xe7\xa9\xba\xe8\xb0\x83\', \'\xe7\x94\xb5\xe7\xa3\x81\xe7\x82\x89\', \'\xe8\xb6\x85\xe8\x96\x84\xe7\x94\xb5\xe8\xa7\x86\', \'\xe5\xbe\xae\xe6\xb3\xa2\xe7\x82\x89\', \'\xe5\x90\xb8\xe5\xb0\x98\xe5\x99\xa8\', \'\xe7\x94\xb5\xe9\xa5\xad\xe7\x85\xb2\', \'\xe5\x8a\xa0\xe6\xb9\xbf\xe5\x99\xa8\', \'\xe7\x94\xb5\xe7\x83\xad\xe7\x89\x87\', \'\xe7\x87\x83\xe6\xb0\x94\xe7\x81\xb6\', \'\xe7\x94\xb5\xe9\xa3\x8e\xe6\x89\x87\', \'\xe6\x9f\x9c\xe5\xbc\x8f\xe7\xa9\xba\xe8\xb0\x83\', \'\xe5\x92\x96\xe5\x95\xa1\xe6\x9c\xba\',\r\n                   \'\xe6\xa6\xa8\xe6\xb1\x81\xe6\x9c\xba\', \'\xe5\x89\x83\xe9\xa1\xbb\xe5\x88\x80\', \'\xe6\x89\xab\xe5\x9c\xb0\xe6\x9c\xba\xe5\x99\xa8\xe4\xba\xba\', \'\xe9\x9d\xa2\xe5\x8c\x85\xe6\x9c\xba\', \'\xe7\x94\xb5\xe6\xb0\xb4\xe5\xa3\xb6\', \'\xe7\x94\xb5\xe5\x90\xb9\xe9\xa3\x8e\', \'\xe5\x86\xb0\xe7\xae\xb1\', \'\xe9\xa5\xae\xe6\xb0\xb4\xe6\x9c\xba\', \'\xe7\x86\xa8\xe6\x96\x97\', \'\xe6\xb2\xb9\xe7\x83\x9f\xe6\x9c\xba\'],\r\n                  [\'\xe6\x95\xb0\xe7\xa0\x81\xe4\xba\xa7\xe5\x93\x81\', \'\xe6\x89\x8b\xe6\x9c\xba\', \'\xe9\x9f\xb3\xe7\xae\xb1\', \'\xe7\x9b\xb8\xe6\x9c\xba\', \'VR\xe7\x9c\xbc\xe9\x95\x9c\', \'\xe4\xb8\x89\xe8\x84\x9a\xe6\x9e\xb6\', \'\xe4\xbd\x93\xe6\x84\x9f\xe8\xbd\xa6\', \'\xe6\x89\x8b\xe8\xa1\xa8\', \'\xe6\x97\xa0\xe4\xba\xba\xe6\x9c\xba\', \'\xe8\x80\xb3\xe6\x9c\xba\xe8\x80\xb3\xe9\xba\xa6\'],\r\n                  [\'\xe6\x9c\x8d\xe9\xa5\xb0\', \'\xe7\x9f\xad\xe8\xa3\xa4\', \'\xe8\xbf\x9e\xe8\xa1\xa3\xe8\xa3\x99\', \'\xe4\xbc\x91\xe9\x97\xb2\xe8\xa3\xa4\', \'\xe8\xa1\xac\xe8\xa1\xab\', \'\xe8\xbf\x90\xe5\x8a\xa8\xe9\x9e\x8b\', \'\xe5\xa4\x96\xe5\xa5\x97\', \'T\xe6\x81\xa4\', \'\xe5\x87\x89\xe9\x9e\x8b\', \'\xe7\x9a\xae\xe9\x9e\x8b\', \'\xe7\x89\x9b\xe4\xbb\x94\xe8\xa3\xa4\', \'\xe6\x8b\x96\xe9\x9e\x8b\'],\r\n                  [\'\xe6\xb4\xbb\xe5\x8a\xa8\', \'\xe8\xbf\x90\xe5\x8a\xa8\xe4\xbc\x9a\', \'\xe5\xa9\x9a\xe7\xa4\xbc\', \'\xe8\x81\x9a\xe9\xa4\x90\'],\r\n                  [\'\xe7\x94\x9f\xe6\xb4\xbb\xe7\x94\xa8\xe5\x93\x81\', \'\xe7\x8e\xbb\xe7\x92\x83\xe6\x9d\xaf\', \'\xe7\xa2\x97\', \'\xe8\xbf\x90\xe5\x8a\xa8\xe6\xb0\xb4\xe5\xa3\xb6\', \'\xe4\xbf\x9d\xe9\xb2\x9c\xe7\x9b\x92\', \'\xe9\x94\x85\xe5\x85\xb7\', \'\xe7\x93\x9c\xe6\x9e\x9c\xe5\x88\xa8\', \'\xe8\x8f\x9c\xe5\x88\x80\', \'\xe5\x89\xaa\xe5\x88\x80\', \'\xe7\xad\xb7\xe5\xad\x90\', \'\xe5\x8f\x89\', \'\xe6\xa4\x85\xe5\xad\x90\', \'\xe6\xa2\xaf\xe5\xad\x90\', \'\xe6\xb2\x99\xe5\x8f\x91\', \'\xe9\xa9\xac\xe5\x85\x8b\xe6\x9d\xaf\', \'\xe8\xa1\xa3\xe6\x9e\xb6\',\r\n                   \'\xe7\x9b\x98\xe5\xad\x90\', \'\xe4\xbc\x9e\', \'\xe5\x8b\xba\xe5\xad\x90\', \'\xe9\xa4\x90\xe6\xa1\x8c\'],\r\n                  [\'\xe7\xae\xb1\xe5\x8c\x85\xe8\xa3\x85\xe9\xa5\xb0\', \'\xe5\x8f\x8c\xe8\x82\xa9\xe5\x8c\x85\', \'\xe5\x8c\x96\xe5\xa6\x86\xe5\x93\x81\', \'\xe7\x8f\xa0\xe5\xae\x9d\', \'\xe5\xa5\xb3\xe5\xbc\x8f\xe6\x8c\x8e\xe5\x8c\x85\', \'\xe7\x9c\xbc\xe9\x95\x9c\', \'\xe6\x8b\x89\xe6\x9d\x86\xe7\xae\xb1\', \'\xe6\x89\x8b\xe6\x8f\x90\xe5\x8c\x85\', \'\xe9\x92\xb1\xe5\x8c\x85\', \'\xe8\x85\xb0\xe5\xb8\xa6\'],\r\n                  [\'\xe9\xa3\x9f\xe5\x93\x81\', \'\xe8\xbd\xa6\xe5\x8e\x98\xe5\xad\x90 \xe6\xa8\xb1\xe6\xa1\x83\', \'\xe4\xb8\x89\xe6\x96\x87\xe9\xb1\xbc\', \'\xe7\x81\xab\xe9\x94\x85\', \'\xe7\x9f\xbf\xe6\xb3\x89\xe6\xb0\xb4\', \'\xe4\xbc\x91\xe9\x97\xb2\xe9\x9b\xb6\xe9\xa3\x9f\', \'\xe7\x81\xab\xe9\xbe\x99\xe6\x9e\x9c\', \'\xe9\xa6\x99\xe8\x95\x89\', \'\xe6\xa4\xb0\xe5\xad\x90\', \'\xe9\xb1\xbf\xe9\xb1\xbc \xe7\xab\xa0\xe9\xb1\xbc\', \'\xe9\x9d\xa2\xe5\x8c\x85\', \'\xe9\xa5\xbc\xe5\xb9\xb2\', \'\xe7\x83\xa7\xe7\x83\xa4\',\r\n                   \'\xe7\xb3\x96\xe6\x9e\x9c \xe5\xb7\xa7\xe5\x85\x8b\xe5\x8a\x9b\',\r\n                   \'\xe6\xb5\xb7\xe5\x8f\x82\', \'\xe5\x9d\x9a\xe6\x9e\x9c\xe7\x82\x92\xe8\xb4\xa7\', \'\xe8\xb4\x9d\xe7\xb1\xbb\', \'\xe6\xb5\xb7\xe4\xba\xa7\xe5\xb9\xb2\xe8\xb4\xa7\', \'\xe9\xb8\xa1\xe7\xbf\x85\', \'\xe7\x89\x9b\xe5\xa5\xb6\', \'\xe8\x8a\x92\xe6\x9e\x9c\', \'\xe9\xa3\x9f\xe7\x94\xa8\xe6\xb2\xb9\', \'\xe7\x8c\x95\xe7\x8c\xb4\xe6\xa1\x83\', \'\xe7\x89\x9b\xe6\x8e\x92\', \'\xe8\x99\xbe\xe7\xb1\xbb\', \'\xe8\x9b\x8b\xe7\xb3\x95\', \'\xe6\xa9\x99\xe5\xad\x90\', \'\xe8\xa5\xbf\xe9\xa4\x90\', \'\xe9\xa5\xae\xe6\x96\x99\',\r\n                   \'\xe6\x96\xb9\xe4\xbe\xbf\xe9\x9d\xa2\',\r\n                   \'\xe9\xb1\xbc\xe7\xb1\xbb\', \'\xe8\x86\xa8\xe5\x8c\x96\xe9\xa3\x9f\xe5\x93\x81\', \'\xe7\x89\x9b\xe6\xb2\xb9\xe6\x9e\x9c\', \'\xe5\xb0\x8f\xe9\xbe\x99\xe8\x99\xbe\', \'\xe7\xb1\xb3\xe9\x9d\xa2\', \'\xe8\x93\x9d\xe8\x8e\x93\', \'\xe8\x8f\xa0\xe8\x90\x9d\', \'\xe7\xba\xa2\xe9\x85\x92\', \'\xe5\x92\x96\xe5\x95\xa1\xe7\xb2\x89\', \'\xe5\x92\x96\xe5\x95\xa1\xe8\xb1\x86\', \'\xe6\xa6\xb4\xe8\x8e\xb2\', \'\xe7\x99\xbd\xe9\x85\x92\', \'\xe8\x8b\xb9\xe6\x9e\x9c\', \'\xe8\x82\x89\', \'\xe8\x9f\xb9\xe7\xb1\xbb\']]\r\n    for cat in categories:\r\n        if folder in cat:\r\n            path = f\'{site}/{cat[0]}/{folder}/\'\r\n            break\r\n    else:\r\n        raise NameError(""Please input correct category name\xef\xbc\x81"")\r\n    if not os.path.exists(path):\r\n        os.makedirs(path)\r\n    return path\r\n\r\n\r\ndef img_name_processor(src):\r\n    """"""\r\n    This function is used to handle the file name of the saved picture.\r\n    Hash the URL of the picture as its filename.\r\n    :param src: image url\r\n    :return: image filename\r\n    """"""\r\n    h5 = hashlib.md5()\r\n    h5.update(src.encode(\'utf-8\'))\r\n    img = h5.hexdigest() + \'.jpg\'\r\n    return img\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    save_img(\'test.jpg\',\'https://images.pexels.com/photos/458766/pexels-photo-458766.jpeg?auto=compress&cs=tinysrgb&h=350\')\r\n'"
download_and_clean_data_scripts/yahoo_fcc100m_dataset/parse100m.py,0,"b'""""""\nparser.py: A basic parser for the YFCC100M dataset.\n\nauthor: Frank Liu - frank.zijie@gmail.com\nlast modified: 05/30/2015\n\nCopyright (c) 2015, Frank Liu\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n    * Redistributions of source code must retain the above copyright\n      notice, this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above copyright\n      notice, this list of conditions and the following disclaimer in the\n      documentation and/or other materials provided with the distribution.\n    * Neither the name of the Frank Liu (fzliu) nor the\n      names of its contributors may be used to endorse or promote products\n      derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL Frank Liu (fzliu) BE LIABLE FOR ANY\nDIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\nON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n""""""\n\nfrom io import BytesIO\nimport random\nimport os\nimport sys\nimport time\n\n\nimport re\nfrom random import randint\nimport uuid\nfrom multiprocessing import Pool\n\n\n# library imports (install with pip)\nimport numpy as np\nfrom PIL import Image\nimport requests\n\n# directory which contains the tab-separated YFCC100M data\n# more info @ download at http://labs.yahoo.com/news/yfcc100m/\nYFCC100M_DIR = ""yfcc100m_dataset""\n\n# keys for the YFCC100M data\nYFCC100M_KEYS = [\n    ""photo_id"",\n    ""identifier"",\n    ""hash"",\n    ""user_id"",\n    ""username"",\n    ""date_taken"",\n    ""upload_time"",\n    ""camera_type"",\n    ""title"",\n    ""description"",\n    ""user_tags"",\n    ""machine_tags"",\n    ""longitude"",\n    ""latitude"",\n    ""accuracy"",\n    ""page_url"",\n    ""download_url"",\n    ""license"",\n    ""license_url"",\n    ""server"",\n    ""farm"",\n    ""secret"",\n    ""original"",\n    ""extension"",\n    ""image_or_video""\n]\n\n\ndef image_from_url(url):\n    """"""\n        Downloads an image in numpy array format, given a URL.\n    """"""\n\n    # loop until the image is successfully downloaded\n    status = None\n    while status != 200:\n        response = requests.get(url)\n        status = response.status_code\n    pimg = Image.open(BytesIO(response.content))\n    pimg = pimg.convert(""RGB"")\n    \n    pimg.save(\'/home/ubuntu/storage/yahoo/yfcc100m-tools/peoplenet/\' + str(uuid.uuid4()) + \'.jpg\')\n\n    return True\n\ndef download_images(line):\n        try:\n\n            # fit the data into a dictionary\n            values = [item.strip() for item in line.split(""\\t"")]\n            data = dict(zip(YFCC100M_KEYS, values))\n            \n            people = False\n            if bool(re.search(""people"", data[""machine_tags""])) or bool(re.search(""people"", data[""user_tags""])):\n                people = True\n\n            if data[""image_or_video""] == ""0"" and people:\n                image_from_url(data[""download_url""])\n        \n        except IOError:\n            print(\'Error!\')\n        \n        \nif __name__ == ""__main__"":\n    YFCC100M_DIR = \'/home/ubuntu/storage/yahoo/yfcc100m-tools/parts/\'\n    \n    for part in os.listdir(YFCC100M_DIR):\n        fh =  open(os.path.join(YFCC100M_DIR, part), ""r"").readlines()\n        \n        print(part)\n        pool = Pool(processes=16) \n        pool.map(download_images, fh)\n\n    print(""Done!"")'"
GAN-version/models/pix2pixHD/core_generator_load.py,0,"b'from keras.models import model_from_json, Model\nfrom keras.layers import Conv2D, Lambda, add, AvgPool2D, Activation, UpSampling2D, Input, concatenate, Reshape, Flatten, Dense\nfrom .utils.conv2d_r import Conv2D_r\nfrom keras.utils import multi_gpu_model\nfrom .utils.instance_normalization import InstanceNormalization\n\n\nclass CoreGenerator():\n    """"""Core Generator.\n        \n    # Arguments\n        width: Width of image in pixels\n        height: Height of image in pixels\n        channels: Channels for the input image and the generated image\n        gpus: The number of gpus you will be using. \n    """"""\n\n    def __init__(self,\n                 width=384,\n                 height=384,\n                 channels=1,\n                 gpus = 0):\n        \n        self.width = width\n        self.height = height\n        self.input_channels = channels \n        self.channels = channels\n        self.gpus = gpus\n        \n        # -----------------------\n        #  Core Generator Encoder\n        # -----------------------\n\n        core_generator_idea = Input(shape=(self.width, self.height, self.input_channels,))\n        core_generator_idea_downsample = AvgPool2D(2, padding=\'same\')(core_generator_idea)\n        \n        core_generator_style = Input(shape=(self.width/(2**7), self.height/(2**7), self.input_channels,))\n        \n        # -----------------------\n        #  Idea Head\n        # -----------------------\n        \n        \n        encoder = Conv2D_r(64, 7, 1, core_generator_idea_downsample)\n        encoder = InstanceNormalization(axis=-1)(encoder)\n        encoder = Activation(\'relu\')(encoder)\n\n        encoder = Conv2D_r(128, 3, 2, encoder)\n        encoder = InstanceNormalization(axis=-1)(encoder)\n        encoder = Activation(\'relu\')(encoder)\n\n        encoder = Conv2D_r(256, 3, 2, encoder)\n        encoder = InstanceNormalization(axis=-1)(encoder)\n        encoder = Activation(\'relu\')(encoder)\n\n        encoder = Conv2D_r(512, 3, 2, encoder)\n        encoder = InstanceNormalization(axis=-1)(encoder)\n        encoder = Activation(\'relu\')(encoder)\n\n        encoder = Conv2D_r(512, 3, 2, encoder)\n        encoder = InstanceNormalization(axis=-1)(encoder)\n        encoder = Activation(\'relu\')(encoder)\n        \n        # -----------------------\n        #  Style Head\n        # -----------------------\n        \n        \n        style = Conv2D_r(128, 3, 1, core_generator_style)\n        style = InstanceNormalization(axis=-1)(style)\n        style = Activation(\'relu\')(style)\n        \n        style = UpSampling2D(2)(style)\n        style = Conv2D_r(256, 3, 1, style)\n        style = InstanceNormalization(axis=-1)(style)\n        style = Activation(\'relu\')(style)\n        \n        style = UpSampling2D(2)(style)\n        style = Conv2D_r(512, 3, 1, style)\n        style = InstanceNormalization(axis=-1)(style)\n        style = Activation(\'relu\')(style)\n     \n       \n        # -----------------------\n        #  Merge Style and Idea\n        # -----------------------\n        \n        style_and_idea = concatenate([encoder, style], axis=-1)\n        style_and_idea = Conv2D_r(1024, 3, 1,  style_and_idea)\n        style_and_idea = InstanceNormalization(axis=-1)(style_and_idea)\n        style_and_idea = Activation(\'relu\')(style_and_idea)\n        \n        style_and_idea = Conv2D_r(512, 3, 1,  style_and_idea)\n        style_and_idea = InstanceNormalization(axis=-1)( style_and_idea)\n        style_and_idea = Activation(\'relu\')(style_and_idea)\n        \n        # -------------------------------\n        #   Core Generator Residual Block\n        # -------------------------------\n\n        def ResidualUnit(input_features):\n            output_features = Conv2D_r(512, 3, 1, input_features)\n            output_features = InstanceNormalization(axis=-1)(output_features) \n            output_features = Activation(\'relu\')(output_features)\n            output_features = Conv2D_r(512, 3, 1, output_features)\n            output_features = InstanceNormalization(axis=-1)(output_features)\n            output_features = add([input_features, output_features])\n            output_features = Activation(\'relu\')(output_features)\n            return output_features\n\n        resnet = ResidualUnit(style_and_idea)\n        resnet = ResidualUnit(resnet)\n        resnet = ResidualUnit(resnet)\n        resnet = ResidualUnit(resnet)\n        resnet = ResidualUnit(resnet)\n        resnet = ResidualUnit(resnet)\n        resnet = ResidualUnit(resnet)\n        resnet = ResidualUnit(resnet)\n        resnet = ResidualUnit(resnet)\n\n        \n        # -------------\n        #  Core Decoder\n        # -------------\n\n        decoder = UpSampling2D(2)(resnet)\n        decoder = Conv2D_r(512, 3, 1, decoder)\n        decoder = InstanceNormalization(axis=-1)(decoder)\n        decoder = Activation(\'relu\')(decoder)\n\n        decoder = UpSampling2D(2)(decoder)\n        decoder = Conv2D_r(256, 3, 1, decoder)\n        decoder = InstanceNormalization(axis=-1)(decoder)\n        decoder = Activation(\'relu\')(decoder)\n\n        decoder = UpSampling2D(2)(decoder)\n        decoder = Conv2D_r(128, 3, 1, decoder)\n        decoder = InstanceNormalization(axis=-1)(decoder)\n        decoder = Activation(\'relu\')(decoder)\n\n        decoder = UpSampling2D(2)(decoder)\n        decoder = Conv2D_r(64, 3, 1, decoder)\n        features = Lambda(lambda x: x, name=\'core_features_org\')(decoder)\n        decoder = InstanceNormalization(axis=-1)(decoder)\n        decoder = Activation(\'relu\')(decoder)\n\n        decoder = Conv2D_r(channels, 7, 1, decoder)\n        picture_lowres = Activation(\'tanh\')(decoder)\n        \n        core_generator = Model([core_generator_idea, core_generator_style], [picture_lowres, features])\n        core_generator.name = ""core_generator""\n        \n        # --------------\n        #  Compile Model\n        # --------------\n        \n        if self.gpus < 2:\n            self.model = core_generator\n            self.save_model = self.model\n        else:\n            self.save_model = core_generator\n            self.model = multi_gpu_model(self.save_model, gpus=gpus)\n'"
GAN-version/models/pix2pixHD/enhancer.py,0,"b'from keras.models import model_from_json, Model\nfrom keras.layers import Conv2D, Lambda, add, AvgPool2D, Activation, UpSampling2D, Input, concatenate, Reshape\nfrom .utils.conv2d_r import Conv2D_r\nfrom .utils.instance_normalization import InstanceNormalization\nfrom keras.utils import multi_gpu_model\n\n\n\nclass Enhancer():\n    """"""Enhancer.\n        \n    # Arguments\n        width: Width of image in pixels\n        height: Height of image in pixels\n        channels: Channels for the input image and the generated image\n        gpus: The number of gpus you will be using. \n    """"""\n\n    def __init__(self,\n                 width=256,\n                 height=256,\n                 channels=1,\n                 gpus = 0):\n        \n        self.width = width\n        self.height = height\n        self.channels = channels\n        self.gpus = gpus\n    \n        # ---------------------------\n        #  Enhancer Generator Encoder\n        # ---------------------------\n\n        enhancer_generator_input = Input(shape=(self.width, self.height, channels,))\n        enhancer_core_features = Input(shape=(self.width/2, self.height/2, 64,))\n\n        encoder = Conv2D_r(32, 7, 1, enhancer_generator_input)\n        encoder = InstanceNormalization(axis=-1)(encoder)\n        encoder = Activation(\'relu\')(encoder)\n\n        encoder = Conv2D_r(64, 3, 2, encoder)\n        enhancer_and_core = concatenate([encoder, enhancer_core_features], axis=-1)\n        enhancer_and_core = InstanceNormalization(axis=-1)(enhancer_and_core)\n        enhancer_and_core = Activation(\'relu\')(enhancer_and_core)\n        \n        enhancer_and_core = Conv2D_r(64, 3, 1,  enhancer_and_core)\n        enhancer_and_core = InstanceNormalization(axis=-1)(enhancer_and_core)\n        enhancer_and_core = Activation(\'relu\')(enhancer_and_core)\n        \n        \n        # ----------------------------------\n        #  Enhancer Generator Residual Block\n        # ----------------------------------\n\n        def ResidualUnitLocal(input_features):\n            output_features = Conv2D_r(64, 3, 1, input_features) \n            output_features = InstanceNormalization(axis=-1)(output_features) \n            output_features = Activation(\'relu\')(output_features)\n            output_features = Conv2D_r(64, 3, 1, output_features)\n            output_features = InstanceNormalization(axis=-1)(output_features) \n            output_features = add([input_features, output_features])\n            output_features = Activation(\'relu\')(output_features)\n            return output_features\n\n        resnet = ResidualUnitLocal(enhancer_and_core)\n        resnet = ResidualUnitLocal(resnet)\n        resnet = ResidualUnitLocal(resnet)\n\n        # ---------------------------\n        #  Enhancer Generator Decoder\n        # ---------------------------\n\n        decoder = UpSampling2D(2)(resnet)\n        decoder = Conv2D_r(64, 3, 1, decoder)\n        decoder = InstanceNormalization(axis=-1)(decoder)\n        decoder = Activation(\'relu\')(decoder)\n\n        decoder = Conv2D_r(channels, 7, 2, decoder)\n        enhanced_picture = Activation(\'tanh\')(decoder)\n\n        # -----------------\n        #  Save model\n        # -----------------\n        \n        if self.gpus < 2:\n            self.model = Model([enhancer_generator_input, enhancer_core_features], enhanced_picture) \n            self.save_model = self.model\n        else:\n            self.save_model = Model([enhancer_generator_input, enhancer_core_features], enhanced_picture) \n            self.model = multi_gpu_model(self.save_model, gpus=gpus)\n'"
GAN-version/models/pix2pixHD/load_trained_models.py,0,"b'from keras.models import Model, save_model, load_model\nfrom keras.optimizers import Adam\nfrom .utils.conv2d_r import Conv2D_r\nfrom keras.utils import multi_gpu_model\nfrom .utils.instance_normalization import InstanceNormalization\nimport tensorflow as tf\nfrom keras import backend as K\nfrom .utils.sn import ConvSN2D, DenseSN\n\ndef zero_loss(y_true, y_pred):\n            return K.zeros_like(y_true)\n\nclass CoreGeneratorEnhancer():\n    """"""Core Generator.\n        \n    # Arguments\n        width: Width of image in pixels\n        height: Height of image in pixels\n        channels: Channels for the input image and the generated image\n        gpus: The number of gpus you will be using. \n    """"""\n\n    def __init__(self,\n                 resource_path=\'./resources/\',\n                 gpus = 0):\n        \n        self.gpus = gpus\n        \n        core_generator_original = load_model(resource_path + \'core_generator.h5\', custom_objects={\'Conv2D_r\': Conv2D_r, \'InstanceNormalization\': InstanceNormalization, \'tf\': tf, \'ConvSN2D\': ConvSN2D, \'DenseSN\': DenseSN})\n        core_generator = Model(inputs=core_generator_original.input, outputs=[core_generator_original.output, core_generator_original.get_layer(\'core_features_org\').output])\n        core_generator.name = ""core_generator""\n        core_generator.trainable = True\n\t\n        self.model = core_generator        \n        self.save_model = core_generator \n    \nclass CoreGenerator():\n    """"""Core Generator.\n        \n    # Arguments\n        width: Width of image in pixels\n        height: Height of image in pixels\n        channels: Channels for the input image and the generated image\n        gpus: The number of gpus you will be using. \n    """"""\n\n    def __init__(self,\n                 resource_path=\'./resources/\',\n                 gpus = 0):\n        \n        self.gpus = gpus\n        \n        core_generator = load_model(resource_path + \'core_generator.h5\', custom_objects={\'Conv2D_r\': Conv2D_r, \'InstanceNormalization\': InstanceNormalization, \'tf\': tf, \'ConvSN2D\': ConvSN2D, \'DenseSN\': DenseSN})\n        #core_generator = Model(inputs=core_generator_original.input, \n        #                    outputs=[core_generator_original.get_layer(\'core_features_org\').output, # core_generator_original.get_layer(\'core_features_true\').output])\n        core_generator.name = ""core_generator""\n        core_generator.trainable = True\n\t        \n        self.model = core_generator\n        self.save_model = core_generator\n            \nclass Enhancer():\n    """"""Enhancer.\n        \n    # Arguments\n        width: Width of image in pixels\n        height: Height of image in pixels\n        channels: Channels for the input image and the generated image\n        gpus: The number of gpus you will be using. \n    """"""\n\n    def __init__(self,\n                 resource_path=\'./resources/\',\n                 gpus = 0):\n        \n        self.gpus = gpus\n        \n        enhancer = load_model(resource_path + \'enhancer.h5\', custom_objects={\'Conv2D_r\': Conv2D_r, \'InstanceNormalization\': InstanceNormalization, \'tf\': tf, \'ConvSN2D\': ConvSN2D, \'DenseSN\': DenseSN})\n        enhancer.name = \'enhancer\'\n        enhancer.trainable = True\n        \n        self.model = enhancer\n        self.save_model = enhancer\n\nclass DiscriminatorFull():\n    """"""Core Generator.\n        \n    # Arguments\n        width: Width of image in pixels\n        height: Height of image in pixels\n        channels: Channels for the input image and the generated image\n        gpus: The number of gpus you will be using.\n        learning_rate: learning rate\n        decay_rate: \n    """"""\n\n    def __init__(self,\n                 resource_path=\'./resources/\',\n                 learning_rate=0.0002,\n                 decay_rate=2e-6,\n                 gpus = 1):\n        \n        self.gpus = gpus\n        self.learning_rate = learning_rate\n        self.decay_rate = decay_rate\n        \n\n        def zero_loss(y_true, y_pred):\n        \treturn K.zeros_like(y_true)\n               \n        discriminator_full = load_model(resource_path + \'discriminator_full.h5\', custom_objects={\'Conv2D_r\': Conv2D_r, \'InstanceNormalization\': InstanceNormalization, \'tf\': tf, \'zero_loss\': zero_loss, \'ConvSN2D\': ConvSN2D, \'DenseSN\': DenseSN})\n        \n        discriminator_full.trainable = True\n        discriminator_full.name = ""discriminator_full""\n        \n        self.model = discriminator_full\n        self.save_model = discriminator_full\n        \n        \nclass DiscriminatorLow():\n    """"""Core Generator.\n        \n    # Arguments\n        width: Width of image in pixels\n        height: Height of image in pixels\n        channels: Channels for the input image and the generated image\n        gpus: The number of gpus you will be using. \n        learning_rate: learning rate\n        decay_rate: \n    """"""\n\n    def __init__(self,\n                 resource_path=\'./resources/\',\n                 learning_rate=0.0002,\n                 decay_rate=2e-6,\n                 gpus = 0):\n        \n        self.gpus = gpus\n        self.learning_rate = learning_rate\n        self.decay_rate = decay_rate\n        \n        def zero_loss(y_true, y_pred):\n        \treturn K.zeros_like(y_true)\n       \n        discriminator_low = load_model(resource_path + \'discriminator_low.h5\', custom_objects={\'Conv2D_r\': Conv2D_r, \'InstanceNormalization\': InstanceNormalization, \'tf\': tf,\'zero_loss\': zero_loss, \'ConvSN2D\': ConvSN2D, \'DenseSN\': DenseSN})\n        \n        discriminator_low.trainable = True\n        discriminator_low.name = ""discriminator_low""\n\n        self.model = discriminator_low\n        self.save_model = discriminator_low  \n\nclass StyleFeatures():\n    """"""Core Generator.\n        \n    # Arguments\n        width: Width of image in pixels\n        height: Height of image in pixels\n        channels: Channels for the input image and the generated image\n        gpus: The number of gpus you will be using. \n        learning_rate: learning rate\n        decay_rate: \n    """"""\n\n    def __init__(self,\n                 resource_path=\'./resources/\',\n                 gpus = 0):\n        \n        self.gpus = gpus\n        \n        style_features = load_model(resource_path + \'style_features.h5\', custom_objects={\'Conv2D_r\': Conv2D_r, \'InstanceNormalization\': InstanceNormalization, \'tf\': tf, \'ConvSN2D\': ConvSN2D, \'DenseSN\': DenseSN})\n        \n        style_features.trainable = True\n        style_features.name = ""style_features""\n        \n        self.model = style_features\n        self.save_model = style_features\n\n'"
GAN-version/models/pix2pixHD/load_weights.py,0,"b'import keras\nfrom keras.models import Model, save_model, load_model\nfrom core_generator_load import CoreGenerator\nfrom discriminator_full import DiscriminatorFull\nfrom discriminator_low import DiscriminatorLow\nfrom style_features import StyleFeatures\nfrom enhancer import Enhancer\n\nfrom keras.optimizers import Adam\nfrom keras.models import model_from_json\nfrom utils.conv2d_r import Conv2D_r\nfrom keras.utils import multi_gpu_model\nfrom utils.instance_normalization import InstanceNormalization\nimport tensorflow as tf\nfrom keras import backend as K\nfrom utils.sn import ConvSN2D, DenseSN\nfrom keras.models import Model, save_model, load_model\n\ndef zero_loss(y_true, y_pred):\n            return K.zeros_like(y_true)\n\n\nstyle_features = StyleFeatures(gpus=1)\ncore_generator = CoreGenerator(gpus=1)\ndiscriminator_full = DiscriminatorFull(gpus=1, decay_rate=0)\ndiscriminator_low = DiscriminatorLow(gpus=1, decay_rate=0)\nenhancer = Enhancer(gpus=1)\n\nresource_path=\'./weights/\'\nsave_path = \'./resources/\'\nlearning_rate=0.0002,\ndecay_rate=0\n\ncore_generator.model.load_weights(resource_path + ""core_generator.h5"")\nenhancer.model.load_weights(resource_path + \'enhancer.h5\')\ndiscriminator_full.model.load_weights(resource_path + \'discriminator_full.h5\')\ndiscriminator_low.model.load_weights(resource_path + \'discriminator_low.h5\')\nstyle_features.model.load_weights(resource_path + \'style_features.h5\')        \n\n     \nsave_model(discriminator_full.model, save_path + ""discriminator_full.h5"")\nsave_model(discriminator_low.model, save_path + ""discriminator_low.h5"")\nsave_model(enhancer.model, save_path + ""enhancer.h5"")\nsave_model(core_generator.model, save_path + ""core_generator.h5"")\nsave_model(style_features.model, save_path + ""style_features.h5"")   '"
GAN-version/models/utils/AdamAccumulate.py,1,"b""import keras.backend as K\nfrom keras.legacy import interfaces\nfrom keras.optimizers import Optimizer\n\n\nclass AdamAccumulate(Optimizer):\n\n    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n                 epsilon=None, decay=0., amsgrad=False, accum_iters=1, **kwargs):\n        if accum_iters < 1:\n            raise ValueError('accum_iters must be >= 1')\n        super(AdamAccumulate, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n            self.lr = K.variable(lr, name='lr')\n            self.beta_1 = K.variable(beta_1, name='beta_1')\n            self.beta_2 = K.variable(beta_2, name='beta_2')\n            self.decay = K.variable(decay, name='decay')\n        if epsilon is None:\n            epsilon = K.epsilon()\n        self.epsilon = epsilon\n        self.initial_decay = decay\n        self.amsgrad = amsgrad\n        self.accum_iters = K.variable(accum_iters, K.dtype(self.iterations))\n        self.accum_iters_float = K.cast(self.accum_iters, K.floatx())\n\n    @interfaces.legacy_get_updates_support\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n\n        completed_updates = K.cast(K.tf.floor(self.iterations / self.accum_iters), K.floatx())\n\n        if self.initial_decay > 0:\n            lr = lr * (1. / (1. + self.decay * completed_updates))\n\n        t = completed_updates + 1\n\n        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) / (1. - K.pow(self.beta_1, t)))\n\n        # self.iterations incremented after processing a batch\n        # batch:              1 2 3 4 5 6 7 8 9\n        # self.iterations:    0 1 2 3 4 5 6 7 8\n        # update_switch = 1:        x       x    (if accum_iters=4)  \n        update_switch = K.equal((self.iterations + 1) % self.accum_iters, 0)\n        update_switch = K.cast(update_switch, K.floatx())\n\n        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        gs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n\n        if self.amsgrad:\n            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        else:\n            vhats = [K.zeros(1) for _ in params]\n\n        self.weights = [self.iterations] + ms + vs + vhats\n\n        for p, g, m, v, vhat, tg in zip(params, grads, ms, vs, vhats, gs):\n\n            sum_grad = tg + g\n            avg_grad = sum_grad / self.accum_iters_float\n\n            m_t = (self.beta_1 * m) + (1. - self.beta_1) * avg_grad\n            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(avg_grad)\n\n            if self.amsgrad:\n                vhat_t = K.maximum(vhat, v_t)\n                p_t = p - lr_t * m_t / (K.sqrt(vhat_t) + self.epsilon)\n                self.updates.append(K.update(vhat, (1 - update_switch) * vhat + update_switch * vhat_t))\n            else:\n                p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n\n            self.updates.append(K.update(m, (1 - update_switch) * m + update_switch * m_t))\n            self.updates.append(K.update(v, (1 - update_switch) * v + update_switch * v_t))\n            self.updates.append(K.update(tg, (1 - update_switch) * sum_grad))\n            new_p = p_t\n\n            # Apply constraints.\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, (1 - update_switch) * p + update_switch * new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'beta_1': float(K.get_value(self.beta_1)),\n                  'beta_2': float(K.get_value(self.beta_2)),\n                  'decay': float(K.get_value(self.decay)),\n                  'epsilon': self.epsilon,\n                  'amsgrad': self.amsgrad}\n        base_config = super(AdamAccumulate, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))"""
GAN-version/models/utils/attention.py,1,"b""from keras import backend as K\nfrom keras.layers import InputSpec\nimport tensorflow as tf\nfrom keras.engine.topology import Layer\n\n# -------------------------------------------------------------------------------------\n#  Attention Layer from Self-Attention Generative Adversarial Networks\n#  Paper: https://arxiv.org/abs/1805.08318\n#  Author of the layer: Hao Chen\n#  Source: https://stackoverflow.com/questions/50819931/self-attention-gan-in-keras\n# -------------------------------------------------------------------------------------\n\nclass Attention(Layer):\n    def __init__(self, ch, **kwargs):\n        super(Attention, self).__init__(**kwargs)\n        self.channels = ch\n        self.filters_f_g = self.channels // 8\n        self.filters_h = self.channels\n\n    def build(self, input_shape):\n        kernel_shape_f_g = (1, 1) + (self.channels, self.filters_f_g)\n        kernel_shape_h = (1, 1) + (self.channels, self.filters_h)\n\n        # Create a trainable weight variable for this layer:\n        self.gamma = self.add_weight(name='gamma', shape=[1], initializer='zeros', trainable=True)\n        self.kernel_f = self.add_weight(shape=kernel_shape_f_g,\n                                        initializer='glorot_uniform',\n                                        name='kernel_f')\n        self.kernel_g = self.add_weight(shape=kernel_shape_f_g,\n                                        initializer='glorot_uniform',\n                                        name='kernel_g')\n        self.kernel_h = self.add_weight(shape=kernel_shape_h,\n                                        initializer='glorot_uniform',\n                                        name='kernel_h')\n        self.bias_f = self.add_weight(shape=(self.filters_f_g,),\n                                      initializer='zeros',\n                                      name='bias_F')\n        self.bias_g = self.add_weight(shape=(self.filters_f_g,),\n                                      initializer='zeros',\n                                      name='bias_g')\n        self.bias_h = self.add_weight(shape=(self.filters_h,),\n                                      initializer='zeros',\n                                      name='bias_h')\n        super(Attention, self).build(input_shape)\n        # Set input spec.\n        self.input_spec = InputSpec(ndim=4,\n                                    axes={3: input_shape[-1]})\n        self.built = True\n\n\n    def call(self, x):\n        def hw_flatten(x):\n            return K.reshape(x, shape=[K.shape(x)[0], K.shape(x)[1]*K.shape(x)[2], K.shape(x)[-1]])\n\n        f = K.conv2d(x,\n                     kernel=self.kernel_f,\n                     strides=(1, 1), padding='same')  # [bs, h, w, c']\n        f = K.bias_add(f, self.bias_f)\n        g = K.conv2d(x,\n                     kernel=self.kernel_g,\n                     strides=(1, 1), padding='same')  # [bs, h, w, c']\n        g = K.bias_add(g, self.bias_g)\n        h = K.conv2d(x,\n                     kernel=self.kernel_h,\n                     strides=(1, 1), padding='same')  # [bs, h, w, c]\n        h = K.bias_add(h, self.bias_h)\n\n        s = tf.matmul(hw_flatten(g), hw_flatten(f), transpose_b=True)  # # [bs, N, N]\n\n        beta = K.softmax(s, axis=-1)  # attention map\n\n        o = K.batch_dot(beta, hw_flatten(h))  # [bs, N, C]\n\n        o = K.reshape(o, shape=K.shape(x))  # [bs, h, w, C]\n        x = self.gamma * o + x\n\n        return x\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n"""
GAN-version/models/utils/calc_output_and_feature_size.py,0,"b'def calc_output_and_feature_size(height, width):\n    output = (height/(2**3))*(width/(2**3))\n    features = ((height/2**1)*(width/2**1)) * 64 + \\\n               ((height/2**2)*(width/2**2)) * 128 + \\\n               ((height/2**3)*(width/2**3)) * 256 + \\\n               ((height/2**3)*(width/2**3)) * 512\n    return int(output), int(features)\n'"
GAN-version/models/utils/conv2d_r.py,1,"b""import tensorflow as tf\nfrom keras.layers import Conv2D, Lambda\nfrom .sn import ConvSN2D\n\ndef Conv2D_r(channels, filter_size, strides, features):\n    padding = [[0, 0], [filter_size // 2, filter_size // 2],\n               [filter_size // 2, filter_size // 2], [0, 0]]\n    \n    out = Lambda(lambda net: tf.pad(net, padding, 'REFLECT'))(features)\n    out = ConvSN2D(channels, filter_size, strides=strides, padding='valid')(out)\n    return out"""
GAN-version/models/utils/instance_normalization.py,0,"b'#Source: https://github.com/keras-team/keras-contrib/\n\nfrom keras.engine import Layer, InputSpec\nfrom keras import initializers \nfrom keras import regularizers\nfrom keras import constraints\nfrom keras import backend as K\nfrom keras.utils.generic_utils import get_custom_objects\n\nimport numpy as np\n\n\nclass InstanceNormalization(Layer):\n    """"""Instance normalization layer (Lei Ba et al, 2016, Ulyanov et al., 2016).\n    Normalize the activations of the previous layer at each step,\n    i.e. applies a transformation that maintains the mean activation\n    close to 0 and the activation standard deviation close to 1.\n    # Arguments\n        axis: Integer, the axis that should be normalized\n            (typically the features axis).\n            For instance, after a `Conv2D` layer with\n            `data_format=""channels_first""`,\n            set `axis=1` in `InstanceNormalization`.\n            Setting `axis=None` will normalize all values in each instance of the batch.\n            Axis 0 is the batch dimension. `axis` cannot be set to 0 to avoid errors.\n        epsilon: Small float added to variance to avoid dividing by zero.\n        center: If True, add offset of `beta` to normalized tensor.\n            If False, `beta` is ignored.\n        scale: If True, multiply by `gamma`.\n            If False, `gamma` is not used.\n            When the next layer is linear (also e.g. `nn.relu`),\n            this can be disabled since the scaling\n            will be done by the next layer.\n        beta_initializer: Initializer for the beta weight.\n        gamma_initializer: Initializer for the gamma weight.\n        beta_regularizer: Optional regularizer for the beta weight.\n        gamma_regularizer: Optional regularizer for the gamma weight.\n        beta_constraint: Optional constraint for the beta weight.\n        gamma_constraint: Optional constraint for the gamma weight.\n    # Input shape\n        Arbitrary. Use the keyword argument `input_shape`\n        (tuple of integers, does not include the samples axis)\n        when using this layer as the first layer in a model.\n    # Output shape\n        Same shape as input.\n    # References\n        - [Layer Normalization](https://arxiv.org/abs/1607.06450)\n        - [Instance Normalization: The Missing Ingredient for Fast Stylization](https://arxiv.org/abs/1607.08022)\n    """"""\n    def __init__(self,\n                 axis=None,\n                 epsilon=1e-3,\n                 center=True,\n                 scale=True,\n                 beta_initializer=\'zeros\',\n                 gamma_initializer=\'ones\',\n                 beta_regularizer=None,\n                 gamma_regularizer=None,\n                 beta_constraint=None,\n                 gamma_constraint=None,\n                 **kwargs):\n        super(InstanceNormalization, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.axis = axis\n        self.epsilon = epsilon\n        self.center = center\n        self.scale = scale\n        self.beta_initializer = initializers.get(beta_initializer)\n        self.gamma_initializer = initializers.get(gamma_initializer)\n        self.beta_regularizer = regularizers.get(beta_regularizer)\n        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n        self.beta_constraint = constraints.get(beta_constraint)\n        self.gamma_constraint = constraints.get(gamma_constraint)\n\n    def build(self, input_shape):\n        ndim = len(input_shape)\n        if self.axis == 0:\n            raise ValueError(\'Axis cannot be zero\')\n\n        if (self.axis is not None) and (ndim == 2):\n            raise ValueError(\'Cannot specify axis for rank 1 tensor\')\n\n        self.input_spec = InputSpec(ndim=ndim)\n\n        if self.axis is None:\n            shape = (1,)\n        else:\n            shape = (input_shape[self.axis],)\n\n        if self.scale:\n            self.gamma = self.add_weight(shape=shape,\n                                         name=\'gamma\',\n                                         initializer=self.gamma_initializer,\n                                         regularizer=self.gamma_regularizer,\n                                         constraint=self.gamma_constraint)\n        else:\n            self.gamma = None\n        if self.center:\n            self.beta = self.add_weight(shape=shape,\n                                        name=\'beta\',\n                                        initializer=self.beta_initializer,\n                                        regularizer=self.beta_regularizer,\n                                        constraint=self.beta_constraint)\n        else:\n            self.beta = None\n        self.built = True\n\n    def call(self, inputs, training=None):\n        input_shape = K.int_shape(inputs)\n        reduction_axes = list(range(0, len(input_shape)))\n\n        if (self.axis is not None):\n            del reduction_axes[self.axis]\n\n        del reduction_axes[0]\n\n        mean = K.mean(inputs, reduction_axes, keepdims=True)\n        stddev = K.std(inputs, reduction_axes, keepdims=True) + self.epsilon\n        normed = (inputs - mean) / stddev\n\n        broadcast_shape = [1] * len(input_shape)\n        if self.axis is not None:\n            broadcast_shape[self.axis] = input_shape[self.axis]\n\n        if self.scale:\n            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n            normed = normed * broadcast_gamma\n        if self.center:\n            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n            normed = normed + broadcast_beta\n        return normed\n\n    def get_config(self):\n        config = {\n            \'axis\': self.axis,\n            \'epsilon\': self.epsilon,\n            \'center\': self.center,\n            \'scale\': self.scale,\n            \'beta_initializer\': initializers.serialize(self.beta_initializer),\n            \'gamma_initializer\': initializers.serialize(self.gamma_initializer),\n            \'beta_regularizer\': regularizers.serialize(self.beta_regularizer),\n            \'gamma_regularizer\': regularizers.serialize(self.gamma_regularizer),\n            \'beta_constraint\': constraints.serialize(self.beta_constraint),\n            \'gamma_constraint\': constraints.serialize(self.gamma_constraint)\n        }\n        base_config = super(InstanceNormalization, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\nget_custom_objects().update({\'InstanceNormalization\': InstanceNormalization})'"
GAN-version/models/utils/sn.py,7,"b""from keras import backend as K\nfrom keras.engine import *\nfrom keras.legacy import interfaces\nfrom keras import activations\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras.utils.generic_utils import func_dump\nfrom keras.utils.generic_utils import func_load\nfrom keras.utils.generic_utils import deserialize_keras_object\nfrom keras.utils.generic_utils import has_arg\nfrom keras.utils import conv_utils\nfrom keras.legacy import interfaces\nfrom keras.layers import Dense, Conv1D, Conv2D, Conv3D, Conv2DTranspose, Embedding\nimport tensorflow as tf\n\n# -------------------------------------------------------------------------------------------------------------\n#  SN Layer from Spectral Normalization for Generative Adversarial Networks\n#  Paper: https://arxiv.org/abs/1802.05957\n#  Author of the layer: I-Sheng Fang\n#  Source: https://github.com/IShengFang/SpectralNormalizationKeras/blob/master/SpectralNormalizationKeras.py\n# -------------------------------------------------------------------------------------------------------------\n\nclass DenseSN(Dense):\n    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        input_dim = input_shape[-1]\n        self.kernel = self.add_weight(shape=(input_dim, self.units),\n                                      initializer=self.kernel_initializer,\n                                      name='kernel',\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.units,),\n                                        initializer=self.bias_initializer,\n                                        name='bias',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.u = self.add_weight(shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n                                 initializer=initializers.RandomNormal(0, 1),\n                                 name='sn',\n                                 trainable=False)\n        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n        self.built = True\n        \n    def call(self, inputs, training=None):\n        def _l2normalize(v, eps=1e-12):\n            return v / (K.sum(v ** 2) ** 0.5 + eps)\n        def power_iteration(W, u):\n            _u = u\n            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n            _u = _l2normalize(K.dot(_v, W))\n            return _u, _v\n        W_shape = self.kernel.shape.as_list()\n        #Flatten the Tensor\n        W_reshaped = K.reshape(self.kernel, [-1, W_shape[-1]])\n        _u, _v = power_iteration(W_reshaped, self.u)\n        #Calculate Sigma\n        sigma=K.dot(_v, W_reshaped)\n        sigma=K.dot(sigma, K.transpose(_u))\n        #normalize it\n        W_bar = W_reshaped / sigma\n        #reshape weight tensor\n        if training in {0, False}:\n            W_bar = K.reshape(W_bar, W_shape)\n        else:\n            with tf.control_dependencies([self.u.assign(_u)]):\n                 W_bar = K.reshape(W_bar, W_shape)  \n        output = K.dot(inputs, W_bar)\n        if self.use_bias:\n            output = K.bias_add(output, self.bias, data_format='channels_last')\n        if self.activation is not None:\n            output = self.activation(output)\n        return output \n        \nclass _ConvSN(Layer):\n\n    def __init__(self, rank,\n                 filters,\n                 kernel_size,\n                 strides=1,\n                 padding='valid',\n                 data_format=None,\n                 dilation_rate=1,\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 spectral_normalization=True,\n                 **kwargs):\n        super(_ConvSN, self).__init__(**kwargs)\n        self.rank = rank\n        self.filters = filters\n        self.kernel_size = conv_utils.normalize_tuple(kernel_size, rank, 'kernel_size')\n        self.strides = conv_utils.normalize_tuple(strides, rank, 'strides')\n        self.padding = conv_utils.normalize_padding(padding)\n        self.data_format = conv_utils.normalize_data_format(data_format)\n        self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, rank, 'dilation_rate')\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n        self.input_spec = InputSpec(ndim=self.rank + 2)\n        self.spectral_normalization = spectral_normalization\n        self.u = None\n        \n    def _l2normalize(self, v, eps=1e-12):\n        return v / (K.sum(v ** 2) ** 0.5 + eps)\n    \n    def power_iteration(self, u, W):\n        '''\n        Accroding the paper, we only need to do power iteration one time.\n        '''\n        v = self._l2normalize(K.dot(u, K.transpose(W)))\n        u = self._l2normalize(K.dot(v, W))\n        return u, v\n    def build(self, input_shape):\n        if self.data_format == 'channels_first':\n            channel_axis = 1\n        else:\n            channel_axis = -1\n        if input_shape[channel_axis] is None:\n            raise ValueError('The channel dimension of the inputs '\n                             'should be defined. Found `None`.')\n        input_dim = input_shape[channel_axis]\n        kernel_shape = self.kernel_size + (input_dim, self.filters)\n\n        self.kernel = self.add_weight(shape=kernel_shape,\n                                      initializer=self.kernel_initializer,\n                                      name='kernel',\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n\n        #Spectral Normalization\n        if self.spectral_normalization:\n            self.u = self.add_weight(shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n                                     initializer=initializers.RandomNormal(0, 1),\n                                     name='sn',\n                                     trainable=False)\n        \n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.filters,),\n                                        initializer=self.bias_initializer,\n                                        name='bias',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        # Set input spec.\n        self.input_spec = InputSpec(ndim=self.rank + 2,\n                                    axes={channel_axis: input_dim})\n        self.built = True\n\n    def call(self, inputs):\n        def _l2normalize(v, eps=1e-12):\n            return v / (K.sum(v ** 2) ** 0.5 + eps)\n        def power_iteration(W, u):\n            _u = u\n            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n            _u = _l2normalize(K.dot(_v, W))\n            return _u, _v\n        \n        if self.spectral_normalization:\n            W_shape = self.kernel.shape.as_list()\n            #Flatten the Tensor\n            W_reshaped = K.reshape(self.kernel, [-1, W_shape[-1]])\n            _u, _v = power_iteration(W_reshaped, self.u)\n            #Calculate Sigma\n            sigma=K.dot(_v, W_reshaped)\n            sigma=K.dot(sigma, K.transpose(_u))\n            #normalize it\n            W_bar = W_reshaped / sigma\n            #reshape weight tensor\n            if training in {0, False}:\n                W_bar = K.reshape(W_bar, W_shape)\n            else:\n                with tf.control_dependencies([self.u.assign(_u)]):\n                    W_bar = K.reshape(W_bar, W_shape)\n\n            #update weitht\n            self.kernel = W_bar\n        \n        if self.rank == 1:\n            outputs = K.conv1d(\n                inputs,\n                self.kernel,\n                strides=self.strides[0],\n                padding=self.padding,\n                data_format=self.data_format,\n                dilation_rate=self.dilation_rate[0])\n        if self.rank == 2:\n            outputs = K.conv2d(\n                inputs,\n                self.kernel,\n                strides=self.strides,\n                padding=self.padding,\n                data_format=self.data_format,\n                dilation_rate=self.dilation_rate)\n        if self.rank == 3:\n            outputs = K.conv3d(\n                inputs,\n                self.kernel,\n                strides=self.strides,\n                padding=self.padding,\n                data_format=self.data_format,\n                dilation_rate=self.dilation_rate)\n\n        if self.use_bias:\n            outputs = K.bias_add(\n                outputs,\n                self.bias,\n                data_format=self.data_format)\n\n        if self.activation is not None:\n            return self.activation(outputs)\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        if self.data_format == 'channels_last':\n            space = input_shape[1:-1]\n            new_space = []\n            for i in range(len(space)):\n                new_dim = conv_utils.conv_output_length(\n                    space[i],\n                    self.kernel_size[i],\n                    padding=self.padding,\n                    stride=self.strides[i],\n                    dilation=self.dilation_rate[i])\n                new_space.append(new_dim)\n            return (input_shape[0],) + tuple(new_space) + (self.filters,)\n        if self.data_format == 'channels_first':\n            space = input_shape[2:]\n            new_space = []\n            for i in range(len(space)):\n                new_dim = conv_utils.conv_output_length(\n                    space[i],\n                    self.kernel_size[i],\n                    padding=self.padding,\n                    stride=self.strides[i],\n                    dilation=self.dilation_rate[i])\n                new_space.append(new_dim)\n            return (input_shape[0], self.filters) + tuple(new_space)\n\n    def get_config(self):\n        config = {\n            'rank': self.rank,\n            'filters': self.filters,\n            'kernel_size': self.kernel_size,\n            'strides': self.strides,\n            'padding': self.padding,\n            'data_format': self.data_format,\n            'dilation_rate': self.dilation_rate,\n            'activation': activations.serialize(self.activation),\n            'use_bias': self.use_bias,\n            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n            'bias_initializer': initializers.serialize(self.bias_initializer),\n            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n            'bias_constraint': constraints.serialize(self.bias_constraint)\n        }\n        base_config = super(_Conv, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n    \nclass ConvSN2D(Conv2D):\n\n    def build(self, input_shape):\n        if self.data_format == 'channels_first':\n            channel_axis = 1\n        else:\n            channel_axis = -1\n        if input_shape[channel_axis] is None:\n            raise ValueError('The channel dimension of the inputs '\n                             'should be defined. Found `None`.')\n        input_dim = input_shape[channel_axis]\n        kernel_shape = self.kernel_size + (input_dim, self.filters)\n\n        self.kernel = self.add_weight(shape=kernel_shape,\n                                      initializer=self.kernel_initializer,\n                                      name='kernel',\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.filters,),\n                                        initializer=self.bias_initializer,\n                                        name='bias',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n            \n        self.u = self.add_weight(shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n                         initializer=initializers.RandomNormal(0, 1),\n                         name='sn',\n                         trainable=False)\n        \n        # Set input spec.\n        self.input_spec = InputSpec(ndim=self.rank + 2,\n                                    axes={channel_axis: input_dim})\n        self.built = True\n    def call(self, inputs, training=None):\n        def _l2normalize(v, eps=1e-12):\n            return v / (K.sum(v ** 2) ** 0.5 + eps)\n        def power_iteration(W, u):\n            #Accroding the paper, we only need to do power iteration one time.\n            _u = u\n            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n            _u = _l2normalize(K.dot(_v, W))\n            return _u, _v\n        #Spectral Normalization\n        W_shape = self.kernel.shape.as_list()\n        #Flatten the Tensor\n        W_reshaped = K.reshape(self.kernel, [-1, W_shape[-1]])\n        _u, _v = power_iteration(W_reshaped, self.u)\n        #Calculate Sigma\n        sigma=K.dot(_v, W_reshaped)\n        sigma=K.dot(sigma, K.transpose(_u))\n        #normalize it\n        W_bar = W_reshaped / sigma\n        #reshape weight tensor\n        if training in {0, False}:\n            W_bar = K.reshape(W_bar, W_shape)\n        else:\n            with tf.control_dependencies([self.u.assign(_u)]):\n                W_bar = K.reshape(W_bar, W_shape)\n                \n        outputs = K.conv2d(\n                inputs,\n                W_bar,\n                strides=self.strides,\n                padding=self.padding,\n                data_format=self.data_format,\n                dilation_rate=self.dilation_rate)\n        if self.use_bias:\n            outputs = K.bias_add(\n                outputs,\n                self.bias,\n                data_format=self.data_format)\n        if self.activation is not None:\n            return self.activation(outputs)\n        return outputs\n    \nclass ConvSN1D(Conv1D):\n    \n    def build(self, input_shape):\n        if self.data_format == 'channels_first':\n            channel_axis = 1\n        else:\n            channel_axis = -1\n        if input_shape[channel_axis] is None:\n            raise ValueError('The channel dimension of the inputs '\n                             'should be defined. Found `None`.')\n        input_dim = input_shape[channel_axis]\n        kernel_shape = self.kernel_size + (input_dim, self.filters)\n\n        self.kernel = self.add_weight(shape=kernel_shape,\n                                      initializer=self.kernel_initializer,\n                                      name='kernel',\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.filters,),\n                                        initializer=self.bias_initializer,\n                                        name='bias',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n            \n        self.u = self.add_weight(shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n                 initializer=initializers.RandomNormal(0, 1),\n                 name='sn',\n                 trainable=False)\n        # Set input spec.\n        self.input_spec = InputSpec(ndim=self.rank + 2,\n                                    axes={channel_axis: input_dim})\n        self.built = True\n        \n    def call(self, inputs, training=None):\n        def _l2normalize(v, eps=1e-12):\n            return v / (K.sum(v ** 2) ** 0.5 + eps)\n        def power_iteration(W, u):\n            #Accroding the paper, we only need to do power iteration one time.\n            _u = u\n            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n            _u = _l2normalize(K.dot(_v, W))\n            return _u, _v\n        #Spectral Normalization\n        W_shape = self.kernel.shape.as_list()\n        #Flatten the Tensor\n        W_reshaped = K.reshape(self.kernel, [-1, W_shape[-1]])\n        _u, _v = power_iteration(W_reshaped, self.u)\n        #Calculate Sigma\n        sigma=K.dot(_v, W_reshaped)\n        sigma=K.dot(sigma, K.transpose(_u))\n        #normalize it\n        W_bar = W_reshaped / sigma\n        #reshape weight tensor\n        if training in {0, False}:\n            W_bar = K.reshape(W_bar, W_shape)\n        else:\n            with tf.control_dependencies([self.u.assign(_u)]):\n                W_bar = K.reshape(W_bar, W_shape)\n                \n        outputs = K.conv1d(\n                inputs,\n                W_bar,\n                strides=self.strides,\n                padding=self.padding,\n                data_format=self.data_format,\n                dilation_rate=self.dilation_rate)\n        if self.use_bias:\n            outputs = K.bias_add(\n                outputs,\n                self.bias,\n                data_format=self.data_format)\n        if self.activation is not None:\n            return self.activation(outputs)\n        return outputs\n\nclass ConvSN3D(Conv3D):    \n    def build(self, input_shape):\n        if self.data_format == 'channels_first':\n            channel_axis = 1\n        else:\n            channel_axis = -1\n        if input_shape[channel_axis] is None:\n            raise ValueError('The channel dimension of the inputs '\n                             'should be defined. Found `None`.')\n        input_dim = input_shape[channel_axis]\n        kernel_shape = self.kernel_size + (input_dim, self.filters)\n\n        self.kernel = self.add_weight(shape=kernel_shape,\n                                      initializer=self.kernel_initializer,\n                                      name='kernel',\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        \n        self.u = self.add_weight(shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n                         initializer=initializers.RandomNormal(0, 1),\n                         name='sn',\n                         trainable=False)\n        \n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.filters,),\n                                        initializer=self.bias_initializer,\n                                        name='bias',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        # Set input spec.\n        self.input_spec = InputSpec(ndim=self.rank + 2,\n                                    axes={channel_axis: input_dim})\n        self.built = True\n\n    def call(self, inputs, training=None):\n        def _l2normalize(v, eps=1e-12):\n            return v / (K.sum(v ** 2) ** 0.5 + eps)\n        def power_iteration(W, u):\n            #Accroding the paper, we only need to do power iteration one time.\n            _u = u\n            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n            _u = _l2normalize(K.dot(_v, W))\n            return _u, _v\n        #Spectral Normalization\n        W_shape = self.kernel.shape.as_list()\n        #Flatten the Tensor\n        W_reshaped = K.reshape(self.kernel, [-1, W_shape[-1]])\n        _u, _v = power_iteration(W_reshaped, self.u)\n        #Calculate Sigma\n        sigma=K.dot(_v, W_reshaped)\n        sigma=K.dot(sigma, K.transpose(_u))\n        #normalize it\n        W_bar = W_reshaped / sigma\n        #reshape weight tensor\n        if training in {0, False}:\n            W_bar = K.reshape(W_bar, W_shape)\n        else:\n            with tf.control_dependencies([self.u.assign(_u)]):\n                W_bar = K.reshape(W_bar, W_shape)\n                \n        outputs = K.conv3d(\n                inputs,\n                W_bar,\n                strides=self.strides,\n                padding=self.padding,\n                data_format=self.data_format,\n                dilation_rate=self.dilation_rate)\n        if self.use_bias:\n            outputs = K.bias_add(\n                outputs,\n                self.bias,\n                data_format=self.data_format)\n        if self.activation is not None:\n            return self.activation(outputs)\n        return outputs\n\n        \nclass EmbeddingSN(Embedding):\n    \n    def build(self, input_shape):\n        self.embeddings = self.add_weight(\n            shape=(self.input_dim, self.output_dim),\n            initializer=self.embeddings_initializer,\n            name='embeddings',\n            regularizer=self.embeddings_regularizer,\n            constraint=self.embeddings_constraint,\n            dtype=self.dtype)\n        \n        self.u = self.add_weight(shape=tuple([1, self.embeddings.shape.as_list()[-1]]),\n                         initializer=initializers.RandomNormal(0, 1),\n                         name='sn',\n                         trainable=False)\n        \n        self.built = True\n        \n    def call(self, inputs):\n        if K.dtype(inputs) != 'int32':\n            inputs = K.cast(inputs, 'int32')\n            \n        def _l2normalize(v, eps=1e-12):\n            return v / (K.sum(v ** 2) ** 0.5 + eps)\n        def power_iteration(W, u):\n            #Accroding the paper, we only need to do power iteration one time.\n            _u = u\n            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n            _u = _l2normalize(K.dot(_v, W))\n            return _u, _v\n        W_shape = self.embeddings.shape.as_list()\n        #Flatten the Tensor\n        W_reshaped = K.reshape(self.embeddings, [-1, W_shape[-1]])\n        _u, _v = power_iteration(W_reshaped, self.u)\n        #Calculate Sigma\n        sigma=K.dot(_v, W_reshaped)\n        sigma=K.dot(sigma, K.transpose(_u))\n        #normalize it\n        W_bar = W_reshaped / sigma\n        #reshape weight tensor\n        if training in {0, False}:\n            W_bar = K.reshape(W_bar, W_shape)\n        else:\n            with tf.control_dependencies([self.u.assign(_u)]):\n                W_bar = K.reshape(W_bar, W_shape)\n        self.embeddings = W_bar\n            \n        out = K.gather(self.embeddings, inputs)\n        return out \n\nclass ConvSN2DTranspose(Conv2DTranspose):\n\n    def build(self, input_shape):\n        if len(input_shape) != 4:\n            raise ValueError('Inputs should have rank ' +\n                             str(4) +\n                             '; Received input shape:', str(input_shape))\n        if self.data_format == 'channels_first':\n            channel_axis = 1\n        else:\n            channel_axis = -1\n        if input_shape[channel_axis] is None:\n            raise ValueError('The channel dimension of the inputs '\n                             'should be defined. Found `None`.')\n        input_dim = input_shape[channel_axis]\n        kernel_shape = self.kernel_size + (self.filters, input_dim)\n\n        self.kernel = self.add_weight(shape=kernel_shape,\n                                      initializer=self.kernel_initializer,\n                                      name='kernel',\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.filters,),\n                                        initializer=self.bias_initializer,\n                                        name='bias',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n            \n        self.u = self.add_weight(shape=tuple([1, self.kernel.shape.as_list()[-1]]),\n                         initializer=initializers.RandomNormal(0, 1),\n                         name='sn',\n                         trainable=False)\n        \n        # Set input spec.\n        self.input_spec = InputSpec(ndim=4, axes={channel_axis: input_dim})\n        self.built = True  \n    \n    def call(self, inputs):\n        input_shape = K.shape(inputs)\n        batch_size = input_shape[0]\n        if self.data_format == 'channels_first':\n            h_axis, w_axis = 2, 3\n        else:\n            h_axis, w_axis = 1, 2\n\n        height, width = input_shape[h_axis], input_shape[w_axis]\n        kernel_h, kernel_w = self.kernel_size\n        stride_h, stride_w = self.strides\n        if self.output_padding is None:\n            out_pad_h = out_pad_w = None\n        else:\n            out_pad_h, out_pad_w = self.output_padding\n\n        # Infer the dynamic output shape:\n        out_height = conv_utils.deconv_length(height,\n                                              stride_h, kernel_h,\n                                              self.padding,\n                                              out_pad_h)\n        out_width = conv_utils.deconv_length(width,\n                                             stride_w, kernel_w,\n                                             self.padding,\n                                             out_pad_w)\n        if self.data_format == 'channels_first':\n            output_shape = (batch_size, self.filters, out_height, out_width)\n        else:\n            output_shape = (batch_size, out_height, out_width, self.filters)\n            \n        #Spectral Normalization    \n        def _l2normalize(v, eps=1e-12):\n            return v / (K.sum(v ** 2) ** 0.5 + eps)\n        def power_iteration(W, u):\n            #Accroding the paper, we only need to do power iteration one time.\n            _u = u\n            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n            _u = _l2normalize(K.dot(_v, W))\n            return _u, _v\n        W_shape = self.kernel.shape.as_list()\n        #Flatten the Tensor\n        W_reshaped = K.reshape(self.kernel, [-1, W_shape[-1]])\n        _u, _v = power_iteration(W_reshaped, self.u)\n        #Calculate Sigma\n        sigma=K.dot(_v, W_reshaped)\n        sigma=K.dot(sigma, K.transpose(_u))\n        #normalize it\n        W_bar = W_reshaped / sigma\n        #reshape weight tensor\n        if training in {0, False}:\n            W_bar = K.reshape(W_bar, W_shape)\n        else:\n            with tf.control_dependencies([self.u.assign(_u)]):\n                W_bar = K.reshape(W_bar, W_shape)\n        self.kernel = W_bar\n        \n        outputs = K.conv2d_transpose(\n            inputs,\n            self.kernel,\n            output_shape,\n            self.strides,\n            padding=self.padding,\n            data_format=self.data_format)\n\n        if self.use_bias:\n            outputs = K.bias_add(\n                outputs,\n                self.bias,\n                data_format=self.data_format)\n\n        if self.activation is not None:\n            return self.activation(outputs)\n        return outputs\n"""
