file_path,api_count,code
lib/setup.py,0,"b'import numpy as np\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\ndef customize_compiler(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to gcc/nvcc works.\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        print(extra_postargs)\n        postargs = extra_postargs[\'gcc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        customize_compiler(self.compiler)\n        build_ext.build_extensions(self)\n\nextensions = [\n    Extension(\n        ""utils.cython_bbox"",\n        [""utils/bbox.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs = [numpy_include]\n    ),\n]\n\nsetup(\n    name=\'iter-reason\',\n    ext_modules=extensions,\n    # inject our custom trigger\n    cmdclass={\'build_ext\': custom_build_ext}\n)\n'"
tools/_init_paths.py,0,"b""import os.path as osp\nimport sys\n\ndef add_path(path):\n    if path not in sys.path:\n        sys.path.insert(0, path)\n\nthis_dir = osp.dirname(__file__)\n\n# Add lib to PYTHONPATH\nlib_path = osp.join(this_dir, '..', 'lib')\nadd_path(lib_path)\n"""
tools/reval.py,0,"b'#!/usr/bin/env python\n# Reval = re-eval. Re-evaluate saved detections.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport _init_paths\nfrom model.config import cfg\nfrom datasets.factory import get_imdb\ntry:\n  import cPickle as pickle\nexcept ImportError:\n  import pickle\nimport os, sys, argparse\nimport numpy as np\n\n\ndef parse_args():\n  """"""\n  Parse input arguments\n  """"""\n  parser = argparse.ArgumentParser(description=\'Re-evaluate results\')\n  parser.add_argument(\'output_dir\', nargs=1, help=\'results directory\',\n                      type=str)\n  parser.add_argument(\'--imdb\', dest=\'imdb_name\',\n                      help=\'dataset to re-evaluate\',\n                      default=\'ade_mtest_5\', type=str)\n\n  if len(sys.argv) == 1:\n    parser.print_help()\n    sys.exit(1)\n\n  args = parser.parse_args()\n  return args\n\n\ndef from_results(imdb_name, output_dir, args):\n  imdb = get_imdb(imdb_name)\n  with open(os.path.join(output_dir, \'results.pkl\'), \'rb\') as f:\n    results = pickle.load(f)\n\n  print(\'Evaluating detections\')\n  imdb.evaluate(results, output_dir)\n\n\nif __name__ == \'__main__\':\n  args = parse_args()\n\n  output_dir = os.path.abspath(args.output_dir[0])\n  imdb_name = args.imdb_name\n  from_results(imdb_name, output_dir, args)\n'"
tools/test_memory.py,4,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport _init_paths\nfrom model.test import test_net\nfrom model.config import cfg, cfg_from_file, cfg_from_list\nimport nets.attend_memory as attend_memory\nfrom datasets.factory import get_imdb\nimport argparse\nimport pprint\nimport time, os, sys\n\nimport tensorflow as tf\n\n\ndef parse_args():\n  """"""\n  Parse input arguments\n  """"""\n  parser = argparse.ArgumentParser(description=\'Test a region classification network\')\n  parser.add_argument(\'--cfg\', dest=\'cfg_file\',\n                        help=\'optional config file\', default=None, type=str)\n  parser.add_argument(\'--model\', dest=\'model\',\n                        help=\'model to test\',\n                        default=None, type=str)\n  parser.add_argument(\'--visualize\', dest=\'visualize\', help=\'whether to show results\',\n                        action=\'store_true\')\n  parser.add_argument(\'--imdb\', dest=\'imdb_name\',\n                        help=\'dataset to test\',\n                        default=\'voc_2007_test\', type=str)\n  parser.add_argument(\'--tag\', dest=\'tag\',\n                        help=\'tag of the model\',\n                        default=\'\', type=str)\n  parser.add_argument(\'--net\', dest=\'net\',\n                        help=\'vgg16, res50, res101, res152, mobile\',\n                        default=\'res50\', type=str)\n  parser.add_argument(\'--set\', dest=\'set_cfgs\',\n                        help=\'set config keys\', default=None,\n                        nargs=argparse.REMAINDER)\n\n  if len(sys.argv) == 1:\n    parser.print_help()\n    sys.exit(1)\n\n  args = parser.parse_args()\n  return args\n\nif __name__ == \'__main__\':\n  args = parse_args()\n\n  print(\'Called with args:\')\n  print(args)\n\n  if args.cfg_file is not None:\n    cfg_from_file(args.cfg_file)\n  if args.set_cfgs is not None:\n    cfg_from_list(args.set_cfgs)\n\n  print(\'Using config:\')\n  pprint.pprint(cfg)\n\n  # if has model, get the name from it\n  # if does not, then just use the initialization weights\n  if args.model:\n    filename = os.path.splitext(os.path.basename(args.model))[0]\n  else:\n    filename = os.path.splitext(os.path.basename(args.weight))[0]\n\n  tag = args.tag\n  tag = tag if tag else \'default\'\n  filename = tag + \'/\' + filename\n\n  imdb = get_imdb(args.imdb_name)\n  cfg.CLASSES = imdb.classes\n\n  for i in range(len(imdb.image_index)):\n    imdb.roidb[i][\'image\'] = imdb.image_path_at(i)\n\n  tfconfig = tf.ConfigProto(allow_soft_placement=True)\n  tfconfig.gpu_options.allow_growth=True\n\n  # init session\n  sess = tf.Session(config=tfconfig)\n  \n  net_base, net_tag = args.net.split(\'_\')\n\n  if net_tag == \'local\':\n    memory = attend_memory\n    iter_test = False\n  else:\n    raise NotImplementedError\n\n  # load network\n  if net_base == \'vgg16\':\n    net = memory.vgg16_memory()\n  elif net_base == \'res50\':\n    net = memory.resnetv1_memory(num_layers=50)\n  elif net_base == \'res101\':\n    net = memory.resnetv1_memory(num_layers=101)\n  elif net_base == \'res152\':\n    net = memory.resnetv1_memory(num_layers=152)\n  elif net_base == \'mobile\':\n    net = memory.mobilenetv1_memory()\n  else:\n    raise NotImplementedError\n\n  # load model\n  net.create_architecture(""TEST"", imdb.num_classes, tag=\'default\')\n\n  if args.model:\n    print((\'Loading model check point from {:s}\').format(args.model))\n    saver = tf.train.Saver()\n    saver.restore(sess, args.model)\n    print(\'Loaded.\')\n  else:\n    print((\'Loading initial weights from {:s}\').format(args.weight))\n    sess.run(tf.global_variables_initializer())\n    print(\'Loaded.\')\n\n  test_net(sess, net, imdb, imdb.roidb, filename, args.visualize, iter_test=iter_test)\n\n  sess.close()\n'"
tools/test_net.py,4,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport _init_paths\nfrom model.test import test_net_base\nfrom model.config import cfg, cfg_from_file, cfg_from_list\nfrom datasets.factory import get_imdb\nimport argparse\nimport pprint\nimport time, os, sys\n\nimport tensorflow as tf\nfrom nets.vgg16 import vgg16\nfrom nets.resnet_v1 import resnetv1\nfrom nets.mobilenet_v1 import mobilenetv1\n\n\ndef parse_args():\n  """"""\n  Parse input arguments\n  """"""\n  parser = argparse.ArgumentParser(description=\'Test a region classification network\')\n  parser.add_argument(\'--cfg\', dest=\'cfg_file\',\n                        help=\'optional config file\', default=None, type=str)\n  parser.add_argument(\'--model\', dest=\'model\',\n                        help=\'model to test\',\n                        default=None, type=str)\n  parser.add_argument(\'--imdb\', dest=\'imdb_name\',\n                        help=\'dataset to test\',\n                        default=\'voc_2007_test\', type=str)\n  parser.add_argument(\'--visualize\', dest=\'visualize\', help=\'whether to show results\',\n                        action=\'store_true\')\n  parser.add_argument(\'--tag\', dest=\'tag\',\n                        help=\'tag of the model\',\n                        default=\'\', type=str)\n  parser.add_argument(\'--net\', dest=\'net\',\n                        help=\'vgg16, res50, res101, res152, mobile\',\n                        default=\'res50\', type=str)\n  parser.add_argument(\'--set\', dest=\'set_cfgs\',\n                        help=\'set config keys\', default=None,\n                        nargs=argparse.REMAINDER)\n\n  if len(sys.argv) == 1:\n    parser.print_help()\n    sys.exit(1)\n\n  args = parser.parse_args()\n  return args\n\nif __name__ == \'__main__\':\n  args = parse_args()\n\n  print(\'Called with args:\')\n  print(args)\n\n  if args.cfg_file is not None:\n    cfg_from_file(args.cfg_file)\n  if args.set_cfgs is not None:\n    cfg_from_list(args.set_cfgs)\n\n  print(\'Using config:\')\n  pprint.pprint(cfg)\n\n  # if has model, get the name from it\n  # if does not, then just use the initialization weights\n  if args.model:\n    filename = os.path.splitext(os.path.basename(args.model))[0]\n  else:\n    filename = os.path.splitext(os.path.basename(args.weight))[0]\n\n  tag = args.tag\n  tag = tag if tag else \'default\'\n  filename = tag + \'/\' + filename\n\n  imdb = get_imdb(args.imdb_name)\n  cfg.CLASSES = imdb.classes\n  for i in range(len(imdb.image_index)):\n    imdb.roidb[i][\'image\'] = imdb.image_path_at(i)\n\n  tfconfig = tf.ConfigProto(allow_soft_placement=True)\n  tfconfig.gpu_options.allow_growth=True\n\n  # init session\n  sess = tf.Session(config=tfconfig)\n  # load network\n  if args.net == \'vgg16\':\n    net = vgg16()\n  elif args.net == \'res50\':\n    net = resnetv1(num_layers=50)\n  elif args.net == \'res101\':\n    net = resnetv1(num_layers=101)\n  elif args.net == \'res152\':\n    net = resnetv1(num_layers=152)\n  elif args.net == \'mobile\':\n    net = mobilenetv1()\n  else:\n    raise NotImplementedError\n\n  # load model\n  net.create_architecture(""TEST"", imdb.num_classes, tag=\'default\')\n\n  if args.model:\n    print((\'Loading model check point from {:s}\').format(args.model))\n    saver = tf.train.Saver()\n    saver.restore(sess, args.model)\n    print(\'Loaded.\')\n  else:\n    print((\'Loading initial weights from {:s}\').format(args.weight))\n    sess.run(tf.global_variables_initializer())\n    print(\'Loaded.\')\n\n  test_net_base(sess, net, imdb, imdb.roidb, filename, args.visualize)\n\n  sess.close()\n'"
tools/trainval_memory.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport _init_paths\nfrom model.train_val import get_training_roidb\nfrom model.train_val_memory import train_net\nfrom model.config import cfg, cfg_from_file, cfg_from_list, get_output_dir, get_output_tb_dir\nimport nets.attend_memory as attend_memory\nfrom datasets.factory import get_imdb\nimport datasets.imdb\nimport argparse\nimport pprint\nimport numpy as np\nimport sys\n\nimport tensorflow as tf\n\n\ndef parse_args():\n  """"""\n  Parse input arguments\n  """"""\n  parser = argparse.ArgumentParser(description=\'Train a region classification network with memory\')\n  parser.add_argument(\'--cfg\', dest=\'cfg_file\',\n                      help=\'optional config file\',\n                      default=None, type=str)\n  parser.add_argument(\'--weight\', dest=\'weight\',\n                      help=\'initialize with pretrained model weights\',\n                      type=str)\n  parser.add_argument(\'--imdb\', dest=\'imdb_name\',\n                      help=\'dataset to train on\',\n                      default=\'voc_2007_trainval\', type=str)\n  parser.add_argument(\'--imdbval\', dest=\'imdbval_name\',\n                      help=\'dataset to validate on\',\n                      default=\'voc_2007_test\', type=str)\n  parser.add_argument(\'--iters\', dest=\'max_iters\',\n                      help=\'number of iterations to train\',\n                      default=70000, type=int)\n  parser.add_argument(\'--tag\', dest=\'tag\',\n                      help=\'tag of the model\',\n                      default=None, type=str)\n  parser.add_argument(\'--net\', dest=\'net\',\n                      help=\'vgg16, res50, res101, res152, mobile\',\n                      default=\'res50\', type=str)\n  parser.add_argument(\'--set\', dest=\'set_cfgs\',\n                      help=\'set config keys\', default=None,\n                      nargs=argparse.REMAINDER)\n\n  if len(sys.argv) == 1:\n    parser.print_help()\n    sys.exit(1)\n\n  args = parser.parse_args()\n  return args\n\n\ndef combined_roidb(imdb_names):\n  """"""\n  Combine multiple roidbs\n  """"""\n\n  def get_roidb(imdb_name):\n    imdb = get_imdb(imdb_name)\n    print(\'Loaded dataset `{:s}` for training\'.format(imdb.name))\n    roidb = get_training_roidb(imdb)\n    return roidb\n\n  roidbs = [get_roidb(s) for s in imdb_names.split(\'+\')]\n  roidb = roidbs[0]\n  if len(roidbs) > 1:\n    for r in roidbs[1:]:\n      roidb.extend(r)\n    tmp = get_imdb(imdb_names.split(\'+\')[1])\n    imdb = datasets.imdb.imdb(imdb_names, tmp.classes)\n  else:\n    imdb = get_imdb(imdb_names)\n  return imdb, roidb\n\n\nif __name__ == \'__main__\':\n  args = parse_args()\n\n  print(\'Called with args:\')\n  print(args)\n\n  if args.cfg_file is not None:\n    cfg_from_file(args.cfg_file)\n  if args.set_cfgs is not None:\n    cfg_from_list(args.set_cfgs)\n\n  print(\'Using config:\')\n  pprint.pprint(cfg)\n\n  np.random.seed(cfg.RNG_SEED)\n\n  # train set\n  imdb, roidb = combined_roidb(args.imdb_name)\n  print(\'{:d} roidb entries\'.format(len(roidb)))\n  cfg.CLASSES = imdb.classes\n\n  # output directory where the models are saved\n  output_dir = get_output_dir(imdb, args.tag)\n  print(\'Output will be saved to `{:s}`\'.format(output_dir))\n\n  # tensorboard directory where the summaries are saved during training\n  tb_dir = get_output_tb_dir(imdb, args.tag)\n  print(\'TensorFlow summaries will be saved to `{:s}`\'.format(tb_dir))\n\n  # also add the validation set, but with no flipping images\n  orgflip = cfg.TRAIN.USE_FLIPPED\n  cfg.TRAIN.USE_FLIPPED = False\n  _, valroidb = combined_roidb(args.imdbval_name)\n  print(\'{:d} validation roidb entries\'.format(len(valroidb)))\n  cfg.TRAIN.USE_FLIPPED = orgflip\n\n  net_base, net_tag = args.net.split(\'_\')\n\n  if net_tag == \'local\':\n    memory = attend_memory\n  else:\n    raise NotImplementedError\n\n  # load network\n  if net_base == \'vgg16\':\n    net = memory.vgg16_memory()\n  elif net_base == \'res50\':\n    net = memory.resnetv1_memory(num_layers=50)\n  elif net_base == \'res101\':\n    net = memory.resnetv1_memory(num_layers=101)\n  elif net_base == \'res152\':\n    net = memory.resnetv1_memory(num_layers=152)\n  elif net_base == \'mobile\':\n    net = memory.mobilenetv1_memory()\n  else:\n    raise NotImplementedError\n    \n  train_net(net, imdb, roidb, valroidb, output_dir, tb_dir,\n            pretrained_model=args.weight,\n            max_iters=args.max_iters)\n'"
tools/trainval_net.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport _init_paths\nfrom model.train_val import get_training_roidb, train_net\nfrom model.config import cfg, cfg_from_file, cfg_from_list, get_output_dir, get_output_tb_dir\nfrom datasets.factory import get_imdb\nimport datasets.imdb\nimport argparse\nimport pprint\nimport numpy as np\nimport sys\n\nimport tensorflow as tf\nfrom nets.vgg16 import vgg16\nfrom nets.resnet_v1 import resnetv1\nfrom nets.mobilenet_v1 import mobilenetv1\n\ndef parse_args():\n  """"""\n  Parse input arguments\n  """"""\n  parser = argparse.ArgumentParser(description=\'Train a region classification network\')\n  parser.add_argument(\'--cfg\', dest=\'cfg_file\',\n                      help=\'optional config file\',\n                      default=None, type=str)\n  parser.add_argument(\'--weight\', dest=\'weight\',\n                      help=\'initialize with pretrained model weights\',\n                      type=str)\n  parser.add_argument(\'--imdb\', dest=\'imdb_name\',\n                      help=\'dataset to train on\',\n                      default=\'voc_2007_trainval\', type=str)\n  parser.add_argument(\'--imdbval\', dest=\'imdbval_name\',\n                      help=\'dataset to validate on\',\n                      default=\'voc_2007_test\', type=str)\n  parser.add_argument(\'--iters\', dest=\'max_iters\',\n                      help=\'number of iterations to train\',\n                      default=70000, type=int)\n  parser.add_argument(\'--tag\', dest=\'tag\',\n                      help=\'tag of the model\',\n                      default=None, type=str)\n  parser.add_argument(\'--net\', dest=\'net\',\n                      help=\'vgg16, res50, res101, res152, mobile\',\n                      default=\'res50\', type=str)\n  parser.add_argument(\'--set\', dest=\'set_cfgs\',\n                      help=\'set config keys\', default=None,\n                      nargs=argparse.REMAINDER)\n\n  if len(sys.argv) == 1:\n    parser.print_help()\n    sys.exit(1)\n\n  args = parser.parse_args()\n  return args\n\n\ndef combined_roidb(imdb_names):\n  """"""\n  Combine multiple roidbs\n  """"""\n\n  def get_roidb(imdb_name):\n    imdb = get_imdb(imdb_name)\n    print(\'Loaded dataset `{:s}` for training\'.format(imdb.name))\n    roidb = get_training_roidb(imdb)\n    return roidb\n\n  roidbs = [get_roidb(s) for s in imdb_names.split(\'+\')]\n  roidb = roidbs[0]\n  if len(roidbs) > 1:\n    for r in roidbs[1:]:\n      roidb.extend(r)\n    tmp = get_imdb(imdb_names.split(\'+\')[1])\n    imdb = datasets.imdb.imdb(imdb_names, tmp.classes)\n  else:\n    imdb = get_imdb(imdb_names)\n  return imdb, roidb\n\n\nif __name__ == \'__main__\':\n  args = parse_args()\n\n  print(\'Called with args:\')\n  print(args)\n\n  if args.cfg_file is not None:\n    cfg_from_file(args.cfg_file)\n  if args.set_cfgs is not None:\n    cfg_from_list(args.set_cfgs)\n\n  print(\'Using config:\')\n  pprint.pprint(cfg)\n\n  np.random.seed(cfg.RNG_SEED)\n\n  # train set\n  imdb, roidb = combined_roidb(args.imdb_name)\n  print(\'{:d} roidb entries\'.format(len(roidb)))\n  cfg.CLASSES = imdb.classes\n\n  # output directory where the models are saved\n  output_dir = get_output_dir(imdb, args.tag)\n  print(\'Output will be saved to `{:s}`\'.format(output_dir))\n\n  # tensorboard directory where the summaries are saved during training\n  tb_dir = get_output_tb_dir(imdb, args.tag)\n  print(\'TensorFlow summaries will be saved to `{:s}`\'.format(tb_dir))\n\n  # also add the validation set, but with no flipping images\n  orgflip = cfg.TRAIN.USE_FLIPPED\n  cfg.TRAIN.USE_FLIPPED = False\n  _, valroidb = combined_roidb(args.imdbval_name)\n  print(\'{:d} validation roidb entries\'.format(len(valroidb)))\n  cfg.TRAIN.USE_FLIPPED = orgflip\n\n  # load network\n  if args.net == \'vgg16\':\n    net = vgg16()\n  elif args.net == \'res50\':\n    net = resnetv1(num_layers=50)\n  elif args.net == \'res101\':\n    net = resnetv1(num_layers=101)\n  elif args.net == \'res152\':\n    net = resnetv1(num_layers=152)\n  elif args.net == \'mobile\':\n    net = mobilenetv1()\n  else:\n    raise NotImplementedError\n    \n  train_net(net, imdb, roidb, valroidb, output_dir, tb_dir,\n            pretrained_model=args.weight,\n            max_iters=args.max_iters)\n'"
lib/data/__init__.py,0,b''
lib/data/layer.py,0,"b'""""""The data layer used during training to train the region classifier.\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom model.config import cfg\nfrom data.minibatch import get_minibatch\nimport numpy as np\nimport time\n\nclass DataLayer(object):\n  """"""Data layer used for training.""""""\n\n  def __init__(self, roidb, num_classes, random=False):\n    """"""Set the roidb to be used by this layer during training.""""""\n    self._roidb = roidb\n    self._num_classes = num_classes\n    # Also set a random flag\n    self._random = random\n    self._shuffle_roidb_inds()\n\n  def _shuffle_roidb_inds(self):\n    """"""Randomly permute the training roidb.""""""\n    # If the random flag is set, \n    # then the database is shuffled according to system time\n    # Useful for the validation set\n    if self._random:\n      st0 = np.random.get_state()\n      millis = int(round(time.time() * 1000)) % 4294967295\n      np.random.seed(millis)\n    \n    self._perm = np.random.permutation(np.arange(len(self._roidb)))\n    # Restore the random state\n    if self._random:\n      np.random.set_state(st0)\n      \n    self._cur = 0\n\n  def _get_next_minibatch_inds(self):\n    """"""Return the roidb indices for the next minibatch.""""""\n    \n    if self._cur + cfg.TRAIN.IMS_PER_BATCH >= len(self._roidb):\n      self._shuffle_roidb_inds()\n\n    db_inds = self._perm[self._cur:self._cur + cfg.TRAIN.IMS_PER_BATCH]\n    self._cur += cfg.TRAIN.IMS_PER_BATCH\n\n    return db_inds\n\n  def forward(self):\n    """"""Return the blobs to be used for the next minibatch.\n\n    If cfg.TRAIN.USE_PREFETCH is True, then blobs will be computed in a\n    separate process and made available through self._blob_queue.\n    """"""\n    db_inds = self._get_next_minibatch_inds()\n    minibatch_db = [self._roidb[i] for i in db_inds]\n    return get_minibatch(minibatch_db)\n      \n'"
lib/data/minibatch.py,0,"b'""""""Compute minibatch blobs for training a region classification network.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport numpy.random as npr\nimport cv2\nfrom model.config import cfg\nfrom utils.blob import prep_im_for_blob, im_list_to_blob\n\ndef get_minibatch(roidb, is_training=True):\n  """"""Given a roidb, construct a minibatch sampled from it.""""""\n  num_images = len(roidb)\n  # Sample random scales to use for each image in this batch\n  scales = cfg.TRAIN.SCALES if is_training else cfg.TEST.SCALES\n  max_scale = cfg.TRAIN.MAX_SIZE if is_training else cfg.TEST.MAX_SIZE\n  random_scale_inds = npr.randint(0, high=len(scales), size=num_images)\n\n  # Get the input image blob, formatted for caffe\n  im_blob, im_scales = get_image_blob(roidb, random_scale_inds, scales, max_scale)\n\n  blobs = {\'data\': im_blob}\n  \n  # gt boxes: (x1, y1, x2, y2, cls)\n  gt_inds = np.where(roidb[0][\'gt_classes\'] != 0)[0]\n  gt_boxes = np.empty((len(gt_inds), 5), dtype=np.float32)\n  gt_boxes[:, 0:4] = roidb[0][\'boxes\'][gt_inds, :] * im_scales[0]\n  gt_boxes[:, 4] = roidb[0][\'gt_classes\'][gt_inds]\n  blobs[\'gt_boxes\'] = gt_boxes\n  # height, width, scale\n  blobs[\'im_info\'] = np.array([im_blob.shape[1], \n                              im_blob.shape[2], \n                              im_scales[0]], dtype=np.float32)\n  blobs[\'memory_size\'] = np.ceil(blobs[\'im_info\'][:2] / cfg.BOTTLE_SCALE).astype(np.int32)\n  blobs[\'num_gt\'] = np.int32(gt_boxes.shape[0])\n\n  return blobs\n\ndef get_image_blob(roidb, scale_inds, scales, max_scale):\n  """"""Builds an input blob from the images in the roidb at the specified\n  scales.\n  """"""\n  num_images = len(roidb)\n  processed_ims = []\n  im_scales = []\n  for i in range(num_images):\n    im = cv2.imread(roidb[i][\'image\'])\n    if roidb[i][\'flipped\']:\n      im = im[:, ::-1, :]\n    target_size = scales[scale_inds[i]]\n    im, im_scale = prep_im_for_blob(im, cfg.PIXEL_MEANS, target_size,\n                    max_scale)\n    im_scales.append(im_scale)\n    processed_ims.append(im)\n\n  # Create a blob to hold the input images\n  blob = im_list_to_blob(processed_ims)\n\n  return blob, im_scales\n'"
lib/datasets/__init__.py,0,b''
lib/datasets/ade.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport os.path as osp\ntry:\n  import cPickle as pickle\nexcept ImportError:\n  import pickle\nimport json\nimport cv2\nimport numpy as np\n\nfrom datasets.imdb import imdb\nfrom model.config import cfg\n\nclass ade(imdb):\n  def __init__(self, image_set, count=5):\n    imdb.__init__(self, 'ade_%s_%d' % (image_set,count))\n    self._image_set = image_set\n    self._root_path = osp.join(cfg.DATA_DIR, 'ADE')\n    self._name_file = osp.join(self._root_path, 'objectnames.txt')\n    self._count_file = osp.join(self._root_path, 'objectcounts.txt')\n    self._anno_file = osp.join(self._root_path, self._image_set + '.txt')\n    with open(self._anno_file) as fid:\n      image_index = fid.readlines()\n      self._image_index = [ii.strip() for ii in image_index]\n    with open(self._name_file) as fid:\n      raw_names = fid.readlines()\n      self._raw_names = [n.strip().replace(' ', '_') for n in raw_names]\n      self._len_raw = len(self._raw_names)\n    with open(self._count_file) as fid:\n      raw_counts = fid.readlines()\n      self._raw_counts = np.array([int(n.strip()) for n in raw_counts])\n\n    # First class is always background\n    self._ade_inds = [0] + list(np.where(self._raw_counts >= count)[0])\n    self._classes = ['__background__']\n\n    for idx in self._ade_inds:\n      if idx == 0:\n        continue\n      ade_name = self._raw_names[idx]\n      self._classes.append(ade_name)\n\n    self._classes = tuple(self._classes)\n    self._class_to_ind = dict(list(zip(self.classes, list(range(self.num_classes)))))\n    self.set_proposal_method('gt')\n\n  def _load_text(self, text_path):\n    class_keys = {}\n    with open(text_path) as fid:\n      lines = fid.readlines()\n      for line in lines:\n        columns = line.split('#')\n        key = '%s_%s' % (columns[0].strip(), columns[1].strip())\n        # Just get the class ID\n        class_name = columns[4].strip().replace(' ', '_')\n        if class_name in self._class_to_ind:\n          class_keys[key] = self._class_to_ind[class_name]\n      total_num_ins = len(lines)\n\n    return class_keys, total_num_ins\n\n  def _load_annotation(self):\n    gt_roidb = []\n\n    for i in xrange(self.num_images):\n      image_path = self.image_path_at(i)\n      if i % 10 == 0:\n        print(image_path)\n      # Estimate the number of objects from text file\n      text_path = image_path.replace('.jpg', '_atr.txt')\n      class_keys, total_num_ins = self._load_text(text_path)\n\n      valid_num_ins = 0\n      boxes = np.zeros((total_num_ins, 4), dtype=np.uint16)\n      gt_classes = np.zeros((total_num_ins), dtype=np.int32)\n      seg_areas = np.zeros((total_num_ins), dtype=np.float32)\n\n      # First, whole objects\n      label_path = image_path.replace('.jpg', '_seg.png')\n      seg = cv2.imread(label_path)\n      height, width, _ = seg.shape\n\n      # OpenCV has reversed RGB\n      instances = seg[:, :, 0]\n      unique_ins = np.unique(instances)\n\n      for t, ins in enumerate(list(unique_ins)):\n        if ins == 0:\n          continue\n        key = '%03d_%d' % (t, 0)\n        if key in class_keys:\n          ins_seg = np.where(instances == ins)\n          x1 = ins_seg[1].min()\n          x2 = ins_seg[1].max()\n          y1 = ins_seg[0].min()\n          y2 = ins_seg[0].max()\n          boxes[valid_num_ins, :] = [x1, y1, x2, y2]\n          gt_classes[valid_num_ins] = class_keys[key]\n          seg_areas[valid_num_ins] = ins_seg[0].shape[0]\n          valid_num_ins += 1\n\n      # Then deal with parts\n      level = 1\n      while True:\n        part_path = image_path.replace('.jpg', '_parts_%d.png' % level)\n        if osp.exists(part_path):\n          seg = cv2.imread(part_path)\n          instances = seg[:, :, 0]\n          unique_ins = np.unique(instances)\n\n          for t, ins in enumerate(list(unique_ins)):\n            if ins == 0:\n              continue\n            key = '%03d_%d' % (t, level)\n            if key in class_keys:\n              ins_seg = np.where(instances == ins)\n              x1 = ins_seg[1].min()\n              x2 = ins_seg[1].max()\n              y1 = ins_seg[0].min()\n              y2 = ins_seg[0].max()\n              boxes[valid_num_ins, :] = [x1, y1, x2, y2]\n              gt_classes[valid_num_ins] = class_keys[key]\n              seg_areas[valid_num_ins] = ins_seg[0].shape[0]\n              valid_num_ins += 1\n\n          level += 1\n        else:\n          break\n\n      boxes = boxes[:valid_num_ins, :]\n      gt_classes = gt_classes[:valid_num_ins]\n      seg_areas = seg_areas[:valid_num_ins]\n\n      gt_roidb.append({'width': width,\n                      'height': height,\n                      'boxes' : boxes,\n                      'gt_classes': gt_classes,\n                      'flipped' : False,\n                      'seg_areas': seg_areas})\n    return gt_roidb\n\n  def image_path_at(self, i):\n    return osp.join(self._root_path, self._image_index[i])\n\n  def gt_roidb(self):\n    cache_file = osp.join(self.cache_path, self.name + '_gt_roidb.pkl')\n    image_file = osp.join(self.cache_path, self.name + '_gt_image.pkl')\n    if osp.exists(cache_file) and osp.exists(image_file):\n      with open(cache_file, 'rb') as fid:\n        gt_roidb = pickle.load(fid)\n      print('{} gt roidb loaded from {}'.format(self.name, cache_file))\n      with open(image_file, 'rb') as fid:\n        self._image_index = pickle.load(fid)\n      print('{} gt image loaded from {}'.format(self.name, image_file))\n      return gt_roidb\n\n    gt_roidb = self._load_annotation()\n    with open(cache_file, 'wb') as fid:\n      pickle.dump(gt_roidb, fid, pickle.HIGHEST_PROTOCOL)\n    print('wrote gt roidb to {}'.format(cache_file))\n    with open(image_file, 'wb') as fid:\n      pickle.dump(self._image_index, fid, pickle.HIGHEST_PROTOCOL)\n    print('wrote gt image to {}'.format(image_file))\n    return gt_roidb\n\n  # Do some left-right flipping here\n  def _find_flipped_classes(self):\n    self._flipped_classes = np.arange(self.num_classes, dtype=np.int32)\n    for i, cls_name in enumerate(self.classes):\n      if cls_name.startswith('left_'):\n        query = cls_name.replace('left_', 'right_')\n        idx = self._class_to_ind[query]\n        # Swap for both left and right\n        self._flipped_classes[idx] = i\n        self._flipped_classes[i] = idx\n\n  def append_flipped_images(self):\n    self._find_flipped_classes()\n    num_images = self.num_images\n    widths = self._get_widths()\n    for i in range(num_images):\n      boxes = self.roidb[i]['boxes'].copy()\n      oldx1 = boxes[:, 0].copy()\n      oldx2 = boxes[:, 2].copy()\n      boxes[:, 0] = widths[i] - oldx2 - 1\n      boxes[:, 2] = widths[i] - oldx1 - 1\n      assert (boxes[:, 2] >= boxes[:, 0]).all()\n      entry = {'width': widths[i],\n               'height': self.roidb[i]['height'],\n               'boxes': boxes,\n               'gt_classes': self._flipped_classes[self.roidb[i]['gt_classes']],\n               'flipped': True,\n               'seg_areas': self.roidb[i]['seg_areas']}\n      self.roidb.append(entry)\n    self._image_index = self._image_index * 2\n"""
lib/datasets/coco.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datasets.imdb import imdb\nimport datasets.ds_utils as ds_utils\nfrom model.config import cfg\nimport os.path as osp\nimport sys\nimport os\nimport numpy as np\nimport scipy.io as sio\ntry:\n  import cPickle as pickle\nexcept ImportError:\n  import pickle\nimport json\nimport uuid\n# COCO API\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nfrom pycocotools import mask as COCOmask\n\nclass coco(imdb):\n  def __init__(self, image_set, year):\n    imdb.__init__(self, \'coco_\' + year + \'_\' + image_set)\n    # COCO specific config options\n    self.config = {\'use_crowd\': False,\n                   \'use_salt\': True,\n                   \'cleanup\': True}\n    # name, paths\n    self._year = year\n    self._image_set = image_set\n    self._data_path = osp.join(cfg.DATA_DIR, \'coco\')\n    # load COCO API, classes, class <-> id mappings\n    self._COCO = COCO(self._get_ann_file())\n    cats = self._COCO.loadCats(self._COCO.getCatIds())\n    self._classes = tuple([\'__background__\'] + [c[\'name\'] for c in cats])\n    self._class_to_ind = dict(list(zip(self.classes, list(range(self.num_classes)))))\n    self._class_to_coco_cat_id = dict(list(zip([c[\'name\'] for c in cats],\n                                               self._COCO.getCatIds())))\n    self._image_index = self._load_image_set_index()\n    # Default to roidb handler\n    self.set_proposal_method(\'gt\')\n\n    # Some image sets are ""views"" (i.e. subsets) into others.\n    # For example, minival2014 is a random 5000 image subset of val2014.\n    # This mapping tells us where the view\'s images and proposals come from.\n    self._view_map = {\n      \'minival2014\': \'val2014\',  # 5k val2014 subset\n      \'valminusminival2014\': \'val2014\',  # val2014 \\setminus minival2014\n      \'test-dev2015\': \'test2015\',\n    }\n    coco_name = image_set + year  # e.g., ""val2014""\n    self._data_name = (self._view_map[coco_name]\n                       if coco_name in self._view_map\n                       else coco_name)\n    # Dataset splits that have ground-truth annotations (test splits\n    # do not have gt annotations)\n    self._gt_splits = (\'train\', \'val\', \'minival\')\n\n  def _get_ann_file(self):\n    prefix = \'instances\' if self._image_set.find(\'test\') == -1 \\\n      else \'image_info\'\n    return osp.join(self._data_path, \'annotations\',\n                    prefix + \'_\' + self._image_set + self._year + \'.json\')\n\n  def _load_image_set_index(self):\n    """"""\n    Load image ids.\n    """"""\n    image_ids = self._COCO.getImgIds()\n    return image_ids\n\n  def _get_widths(self):\n    anns = self._COCO.loadImgs(self._image_index)\n    widths = [ann[\'width\'] for ann in anns]\n    return widths\n\n  def image_path_at(self, i):\n    """"""\n    Return the absolute path to image i in the image sequence.\n    """"""\n    return self.image_path_from_index(self._image_index[i])\n\n  def image_path_from_index(self, index):\n    """"""\n    Construct an image path from the image\'s ""index"" identifier.\n    """"""\n    # Example image path for index=119993:\n    #   images/train2014/COCO_train2014_000000119993.jpg\n    file_name = (\'COCO_\' + self._data_name + \'_\' +\n                 str(index).zfill(12) + \'.jpg\')\n    image_path = osp.join(self._data_path, \'images\',\n                          self._data_name, file_name)\n    assert osp.exists(image_path), \\\n      \'Path does not exist: {}\'.format(image_path)\n    return image_path\n\n  def gt_roidb(self):\n    """"""\n    Return the database of ground-truth regions of interest.\n    This function loads/saves from/to a cache file to speed up future calls.\n    """"""\n    cache_file = osp.join(self.cache_path, self.name + \'_gt_roidb.pkl\')\n    if osp.exists(cache_file):\n      with open(cache_file, \'rb\') as fid:\n        roidb = pickle.load(fid)\n      print(\'{} gt roidb loaded from {}\'.format(self.name, cache_file))\n      return roidb\n\n    gt_roidb = [self._load_coco_annotation(index)\n                for index in self._image_index]\n\n    with open(cache_file, \'wb\') as fid:\n      pickle.dump(gt_roidb, fid, pickle.HIGHEST_PROTOCOL)\n    print(\'wrote gt roidb to {}\'.format(cache_file))\n    return gt_roidb\n\n  def _load_coco_annotation(self, index):\n    """"""\n    Loads COCO bounding-box instance annotations. Crowd instances are\n    handled by marking their overlaps (with all categories) to -1. This\n    overlap value means that crowd ""instances"" are excluded from training.\n    """"""\n    im_ann = self._COCO.loadImgs(index)[0]\n    width = im_ann[\'width\']\n    height = im_ann[\'height\']\n\n    # Whether to use crowd annotations\n    iscrowd = None\n    if self.config[\'use_crowd\']:\n      iscrowd = False\n    annIds = self._COCO.getAnnIds(imgIds=index, iscrowd=iscrowd)\n    objs = self._COCO.loadAnns(annIds)\n    # Sanitize bboxes -- some are invalid\n    valid_objs = []\n    for obj in objs:\n      x1 = np.max((0, obj[\'bbox\'][0]))\n      y1 = np.max((0, obj[\'bbox\'][1]))\n      x2 = np.min((width - 1, x1 + np.max((0, obj[\'bbox\'][2] - 1))))\n      y2 = np.min((height - 1, y1 + np.max((0, obj[\'bbox\'][3] - 1))))\n      if obj[\'area\'] > 0 and x2 >= x1 and y2 >= y1:\n        obj[\'clean_bbox\'] = [x1, y1, x2, y2]\n        valid_objs.append(obj)\n    objs = valid_objs\n    num_objs = len(objs)\n\n    boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n    gt_classes = np.zeros((num_objs), dtype=np.int32)\n    seg_areas = np.zeros((num_objs), dtype=np.float32)\n\n    # Lookup table to map from COCO category ids to our internal class\n    # indices\n    coco_cat_id_to_class_ind = dict([(self._class_to_coco_cat_id[cls],\n                                      self._class_to_ind[cls])\n                                     for cls in self._classes[1:]])\n\n    for ix, obj in enumerate(objs):\n      cls = coco_cat_id_to_class_ind[obj[\'category_id\']]\n      boxes[ix, :] = obj[\'clean_bbox\']\n      gt_classes[ix] = cls\n      seg_areas[ix] = obj[\'area\']\n\n    ds_utils.validate_boxes(boxes, width=width, height=height)\n    return {\'width\': width,\n            \'height\': height,\n            \'boxes\': boxes,\n            \'gt_classes\': gt_classes,\n            \'flipped\': False,\n            \'seg_areas\': seg_areas}\n\n  def _print_detection_eval_metrics(self, coco_eval):\n    IoU_lo_thresh = 0.5\n    IoU_hi_thresh = 0.95\n\n    def _get_thr_ind(coco_eval, thr):\n      ind = np.where((coco_eval.params.iouThrs > thr - 1e-5) &\n                     (coco_eval.params.iouThrs < thr + 1e-5))[0][0]\n      iou_thr = coco_eval.params.iouThrs[ind]\n      assert np.isclose(iou_thr, thr)\n      return ind\n\n    ind_lo = _get_thr_ind(coco_eval, IoU_lo_thresh)\n    ind_hi = _get_thr_ind(coco_eval, IoU_hi_thresh)\n    # precision has dims (iou, recall, cls, area range, max dets)\n    # area range index 0: all area ranges\n    # max dets index 2: 100 per image\n    precision = \\\n      coco_eval.eval[\'precision\'][ind_lo:(ind_hi + 1), :, :, 0, 2]\n    ap_default = np.mean(precision[precision > -1])\n    print((\'~~~~ Mean and per-category AP @ IoU=[{:.2f},{:.2f}] \'\n           \'~~~~\').format(IoU_lo_thresh, IoU_hi_thresh))\n    print(\'{:.1f}\'.format(100 * ap_default))\n    for cls_ind, cls in enumerate(self.classes):\n      if cls == \'__background__\':\n        continue\n      # minus 1 because of __background__\n      precision = coco_eval.eval[\'precision\'][ind_lo:(ind_hi + 1), :, cls_ind - 1, 0, 2]\n      ap = np.mean(precision[precision > -1])\n      print(\'{:.1f}\'.format(100 * ap))\n\n    print(\'~~~~ Summary metrics ~~~~\')\n    coco_eval.summarize()\n\n  def _do_detection_eval(self, res_file, output_dir):\n    ann_type = \'bbox\'\n    coco_dt = self._COCO.loadRes(res_file)\n    coco_eval = COCOeval(self._COCO, coco_dt)\n    coco_eval.params.useSegm = (ann_type == \'segm\')\n    coco_eval.evaluate()\n    coco_eval.accumulate()\n    self._print_detection_eval_metrics(coco_eval)\n    eval_file = osp.join(output_dir, \'detection_results.pkl\')\n    with open(eval_file, \'wb\') as fid:\n      pickle.dump(coco_eval, fid, pickle.HIGHEST_PROTOCOL)\n    print(\'Wrote COCO eval results to: {}\'.format(eval_file))\n\n  def _coco_results_one_category(self, boxes, cat_id):\n    results = []\n    for im_ind, index in enumerate(self.image_index):\n      dets = boxes[im_ind].astype(np.float)\n      if dets == []:\n        continue\n      scores = dets[:, -1]\n      xs = dets[:, 0]\n      ys = dets[:, 1]\n      ws = dets[:, 2] - xs + 1\n      hs = dets[:, 3] - ys + 1\n      results.extend(\n        [{\'image_id\': index,\n          \'category_id\': cat_id,\n          \'bbox\': [xs[k], ys[k], ws[k], hs[k]],\n          \'score\': scores[k]} for k in range(dets.shape[0])])\n    return results\n\n  def _write_coco_results_file(self, all_boxes, res_file):\n    # [{""image_id"": 42,\n    #   ""category_id"": 18,\n    #   ""bbox"": [258.15,41.29,348.26,243.78],\n    #   ""score"": 0.236}, ...]\n    results = []\n    for cls_ind, cls in enumerate(self.classes):\n      if cls == \'__background__\':\n        continue\n      print(\'Collecting {} results ({:d}/{:d})\'.format(cls, cls_ind,\n                                                       self.num_classes - 1))\n      coco_cat_id = self._class_to_coco_cat_id[cls]\n      results.extend(self._coco_results_one_category(all_boxes[cls_ind],\n                                                     coco_cat_id))\n    print(\'Writing results json to {}\'.format(res_file))\n    with open(res_file, \'w\') as fid:\n      json.dump(results, fid)\n\n  def evaluate_detections(self, all_boxes, output_dir):\n    res_file = osp.join(output_dir, (\'detections_\' +\n                                     self._image_set +\n                                     self._year +\n                                     \'_results\'))\n    if self.config[\'use_salt\']:\n      res_file += \'_{}\'.format(str(uuid.uuid4()))\n    res_file += \'.json\'\n    self._write_coco_results_file(all_boxes, res_file)\n    # Only do evaluation on non-test sets\n    if self._image_set.find(\'test\') == -1:\n      self._do_detection_eval(res_file, output_dir)\n    # Optionally cleanup results json file\n    if self.config[\'cleanup\']:\n      os.remove(res_file)\n\n'"
lib/datasets/ds_utils.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\n\ndef unique_boxes(boxes, scale=1.0):\n  """"""Return indices of unique boxes.""""""\n  v = np.array([1, 1e3, 1e6, 1e9])\n  hashes = np.round(boxes * scale).dot(v)\n  _, index = np.unique(hashes, return_index=True)\n  return np.sort(index)\n\n\ndef xywh_to_xyxy(boxes):\n  """"""Convert [x y w h] box format to [x1 y1 x2 y2] format.""""""\n  return np.hstack((boxes[:, 0:2], boxes[:, 0:2] + boxes[:, 2:4] - 1))\n\n\ndef xyxy_to_xywh(boxes):\n  """"""Convert [x1 y1 x2 y2] box format to [x y w h] format.""""""\n  return np.hstack((boxes[:, 0:2], boxes[:, 2:4] - boxes[:, 0:2] + 1))\n\n\ndef validate_boxes(boxes, width=0, height=0):\n  """"""Check that a set of boxes are valid.""""""\n  x1 = boxes[:, 0]\n  y1 = boxes[:, 1]\n  x2 = boxes[:, 2]\n  y2 = boxes[:, 3]\n  assert (x1 >= 0).all()\n  assert (y1 >= 0).all()\n  assert (x2 >= x1).all()\n  assert (y2 >= y1).all()\n  assert (x2 < width).all()\n  assert (y2 < height).all()\n\n\ndef filter_small_boxes(boxes, min_size):\n  w = boxes[:, 2] - boxes[:, 0]\n  h = boxes[:, 3] - boxes[:, 1]\n  keep = np.where((w >= min_size) & (h > min_size))[0]\n  return keep\n'"
lib/datasets/factory.py,0,"b'""""""Factory method for easily getting imdbs by name.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n__sets = {}\nfrom datasets.coco import coco\nfrom datasets.ade import ade\nfrom datasets.visual_genome import visual_genome\n\nimport numpy as np\n\n# Set up coco_2014_<split>\nfor year in [\'2014\']:\n  for split in [\'train\', \'val\', \'minival\', \'valminusminival\', \'trainval\']:\n    name = \'coco_{}_{}\'.format(year, split)\n    __sets[name] = (lambda split=split, year=year: coco(split, year))\n\n# Set up coco_2015_<split>\nfor year in [\'2015\']:\n  for split in [\'test\', \'test-dev\']:\n    name = \'coco_{}_{}\'.format(year, split)\n    __sets[name] = (lambda split=split, year=year: coco(split, year))\n\n# Set up ade_<split>_5\nfor split in [\'train\', \'val\', \'mval\', \'mtest\']:\n  name = \'ade_{}_5\'.format(split)\n  __sets[name] = (lambda split=split: ade(split))\n\n# Set up vg_<split>_5,10\nfor split in [\'train\', \'val\', \'test\']:\n  name = \'visual_genome_{}_5\'.format(split)\n  __sets[name] = (lambda split=split: visual_genome(split))\n  \n\ndef get_imdb(name):\n  """"""Get an imdb (image database) by name.""""""\n  if name not in __sets:\n    raise KeyError(\'Unknown dataset: {}\'.format(name))\n  return __sets[name]()\n\n\ndef list_imdbs():\n  """"""List all registered imdbs.""""""\n  return list(__sets.keys())\n'"
lib/datasets/imdb.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport os.path as osp\nfrom utils.cython_bbox import bbox_overlaps\nimport numpy as np\nimport scipy.sparse\nfrom datasets.voc_eval import voc_ap\nfrom model.config import cfg\nfrom data.layer import DataLayer\nfrom data.minibatch import get_minibatch\n\nclass imdb(object):\n  """"""Image database.""""""\n\n  def __init__(self, name, classes=None):\n    self._name = name\n    if not classes:\n      self._classes = []\n    else:\n      self._classes = classes\n    self._image_index = []\n    self._obj_proposer = \'gt\'\n    self._roidb = None\n    self._roidb_handler = self.default_roidb\n    # Use this dict for storing dataset specific config options\n    self._data_layer = DataLayer\n    self._minibatch = get_minibatch\n    self.config = {}\n\n  @property\n  def name(self):\n    return self._name\n\n  @property\n  def num_classes(self):\n    return len(self._classes)\n\n  @property\n  def classes(self):\n    return self._classes\n\n  @property\n  def image_index(self):\n    return self._image_index\n\n  @property\n  def data_layer(self):\n    return self._data_layer\n\n  @property\n  def minibatch(self):\n    return self._minibatch\n\n  @property\n  def roidb_handler(self):\n    return self._roidb_handler\n\n  @roidb_handler.setter\n  def roidb_handler(self, val):\n    self._roidb_handler = val\n\n  def set_proposal_method(self, method):\n    method = eval(\'self.\' + method + \'_roidb\')\n    self.roidb_handler = method\n\n  @property\n  def roidb(self):\n    # A roidb is a list of dictionaries, each with the following keys:\n    #   boxes\n    #   gt_overlaps\n    #   gt_classes\n    #   flipped\n    if self._roidb is not None:\n      return self._roidb\n    self._roidb = self.roidb_handler()\n    return self._roidb\n\n  @property\n  def cache_path(self):\n    cache_path = osp.abspath(osp.join(cfg.DATA_DIR, \'cache\'))\n    if not os.path.exists(cache_path):\n      os.makedirs(cache_path)\n    return cache_path\n\n  @property\n  def num_images(self):\n    return len(self.image_index)\n\n  def image_path_at(self, i):\n    raise NotImplementedError\n\n  def default_roidb(self):\n    raise NotImplementedError\n\n  def _score(self, all_scores):\n    scs = [0.] * self.num_classes\n    scs_all = [0.] * self.num_classes\n    valid = [0] * self.num_classes\n    for i in xrange(1, self.num_classes):\n      ind_this = np.where(self.gt_classes == i)[0]  \n      scs_all[i] = np.sum(all_scores[ind_this, i])\n      if ind_this.shape[0] > 0:\n        valid[i] = ind_this.shape[0]\n        scs[i] = scs_all[i] / ind_this.shape[0]\n\n    mcls_sc = np.mean([s for s, v in zip(scs,valid) if v])\n    mins_sc = np.sum(scs_all) / self.gt_classes.shape[0]\n    return scs[1:], mcls_sc, mins_sc, valid[1:]\n\n  def _accuracy(self, all_scores):\n    acs = [0.] * self.num_classes\n    acs_all = [0.] * self.num_classes\n    valid = [0] * self.num_classes\n\n    # Need to remove the background class\n    max_inds = np.argmax(all_scores[:, 1:], axis=1) + 1\n    max_scores = np.empty_like(all_scores)\n    max_scores[:] = 0.\n    max_scores[np.arange(self.gt_classes.shape[0]), max_inds] = 1.\n\n    for i in xrange(1, self.num_classes):\n      ind_this = np.where(self.gt_classes == i)[0]\n      acs_all[i] = np.sum(max_scores[ind_this, i])\n      if ind_this.shape[0] > 0:\n        valid[i] = ind_this.shape[0]\n        acs[i] = acs_all[i] / ind_this.shape[0]\n\n    mcls_ac = np.mean([s for s, v in zip(acs,valid) if v])\n    mins_ac = np.sum(acs_all) / self.gt_classes.shape[0]\n    return acs[1:], mcls_ac, mins_ac\n\n  def _average_precision(self, all_scores):\n    aps = [0.] * self.num_classes\n    valid = [0] * self.num_classes\n\n    ind_all = np.arange(self.gt_classes.shape[0])\n    num_cls = self.num_classes\n    num_ins = ind_all.shape[0]\n\n    for i, c in enumerate(self._classes):\n      if i == 0:\n        continue\n      gt_this = (self.gt_classes == i).astype(np.float32)\n      num_this = np.sum(gt_this)\n      if i % 10 == 0:\n        print(\'AP for %s: %d/%d\' % (c, i, num_cls))\n      if num_this > 0:\n        valid[i] = num_this\n        sco_this = all_scores[ind_all, i]\n\n        ind_sorted = np.argsort(-sco_this)\n\n        tp = gt_this[ind_sorted]\n        max_ind = num_ins - np.argmax(tp[::-1])\n        tp = tp[:max_ind]\n        fp = 1. - tp\n\n        tp = np.cumsum(tp)\n        fp = np.cumsum(fp)\n        rec = tp / float(num_this)\n        prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n\n        aps[i] = voc_ap(rec, prec)\n\n    mcls_ap = np.mean([s for s, v in zip(aps,valid) if v])\n\n    # Compute the overall score\n    max_inds = np.argmax(all_scores[:, 1:], axis=1) + 1\n    max_scores = np.empty_like(all_scores)\n    max_scores[:] = 0.\n    max_scores[ind_all, max_inds] = 1.\n    pred_all = max_scores[ind_all, self.gt_classes]\n    sco_all = all_scores[ind_all, self.gt_classes]\n    ind_sorted = np.argsort(-sco_all)\n\n    tp = pred_all[ind_sorted]\n    fp = 1. - tp\n\n    tp = np.cumsum(tp)\n    fp = np.cumsum(fp)\n    rec = tp / float(num_ins)\n    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n\n    mins_ap = voc_ap(rec, prec)\n    return aps[1:], mcls_ap, mins_ap\n\n  def evaluate_detail(self, all_scores, output_dir, roidb=None):\n    if roidb is None:\n      roidb = self.roidb\n    all_scores = np.vstack(all_scores)\n    # all_scores = np.minimum(all_scores, 1.0)\n    self.gt_classes = np.hstack([r[\'gt_classes\'] for r in roidb])\n\n    scs, mcls_sc, mins_sc, valid = self._score(all_scores)\n    acs, mcls_ac, mins_ac = self._accuracy(all_scores)\n    aps, mcls_ap, mins_ap = self._average_precision(all_scores)\n\n    for i, cls in enumerate(self._classes):\n      if cls == \'__background__\' or not valid[i-1]:\n        continue\n      if valid[i-1]:\n        print((\'{} {:d} {:.4f} {:.4f}\'.format(cls, \n                                              valid[i-1], \n                                              acs[i-1], \n                                              aps[i-1])))\n\n    print(\'~~~~~~~~\')\n    # print(\'Accuracies | APs:\')\n    # for ac, ap, vl in zip(acs, aps, valid):\n    #   if vl:\n    #     print((\'{:.3f} {:.3f}\'.format(ac, ap)))\n    print((\'mean-cls: {:.3f} {:.3f}\'.format(mcls_ac, mcls_ap)))\n    print((\'mean-ins: {:.3f} {:.3f}\'.format(mins_ac, mins_ap)))\n    print(\'~~~~~~~~\')\n    print((\'{:.3f} {:.3f} {:.3f} {:.3f} {:.3f} {:.3f}\'.format(mcls_sc, \n                                                              mcls_ac, \n                                                              mcls_ap, \n                                                              mins_sc, \n                                                              mins_ac, \n                                                              mins_ap)))\n    print(\'~~~~~~~~\')\n\n    return acs, aps\n\n  def evaluate(self, all_scores, output_dir, roidb=None):\n    if roidb is None:\n      roidb = self.roidb\n    all_scores = np.vstack(all_scores)\n    # all_scores = np.minimum(all_scores, 1.0)\n    self.gt_classes = np.hstack([r[\'gt_classes\'] for r in roidb])\n\n    scs, mcls_sc, mins_sc, valid = self._score(all_scores)\n    acs, mcls_ac, mins_ac = self._accuracy(all_scores)\n    aps, mcls_ap, mins_ap = self._average_precision(all_scores)\n\n    for i, cls in enumerate(self._classes):\n      if cls == \'__background__\' or not valid[i-1]:\n        continue\n      print((\'{} {:d} {:.4f} {:.4f} {:.4f}\'.format(cls, \n                                                  valid[i-1], \n                                                  scs[i-1], \n                                                  acs[i-1], \n                                                  aps[i-1])))\n\n    print(\'~~~~~~~~\')\n    # print(\'Scores | Accuracies | APs:\')\n    # for sc, ac, ap, vl in zip(scs, acs, aps, valid):\n    #   if vl:\n    #     print((\'{:.3f} {:.3f} {:.3f}\'.format(sc, ac, ap)))\n    print((\'mean-cls: {:.3f} {:.3f} {:.3f}\'.format(mcls_sc, mcls_ac, mcls_ap)))\n    print((\'mean-ins: {:.3f} {:.3f} {:.3f}\'.format(mins_sc, mins_ac, mins_ap)))\n    print(\'~~~~~~~~\')\n    print((\'{:.3f} {:.3f} {:.3f} {:.3f} {:.3f} {:.3f}\'.format(mcls_sc, \n                                                              mcls_ac, \n                                                              mcls_ap, \n                                                              mins_sc, \n                                                              mins_ac, \n                                                              mins_ap)))\n    print(\'~~~~~~~~\')\n\n    return mcls_sc, mcls_ac, mcls_ap, mins_sc, mins_ac, mins_ap\n\n  def _get_widths(self):\n    return [r[\'width\'] for r in self.roidb]\n\n  def append_flipped_images(self):\n    num_images = self.num_images\n    widths = self._get_widths()\n    for i in range(num_images):\n      boxes = self.roidb[i][\'boxes\'].copy()\n      oldx1 = boxes[:, 0].copy()\n      oldx2 = boxes[:, 2].copy()\n      boxes[:, 0] = widths[i] - oldx2 - 1\n      boxes[:, 2] = widths[i] - oldx1 - 1\n      assert (boxes[:, 2] >= boxes[:, 0]).all()\n      entry = {\'width\': widths[i],\n               \'height\': self.roidb[i][\'height\'],\n               \'boxes\': boxes,\n               \'gt_classes\': self.roidb[i][\'gt_classes\'],\n               \'flipped\': True,\n               \'seg_areas\': self.roidb[i][\'seg_areas\']}\n      self.roidb.append(entry)\n    self._image_index = self._image_index * 2\n\n'"
lib/datasets/visual_genome.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport os.path as osp\ntry:\n  import cPickle as pickle\nexcept ImportError:\n  import pickle\nimport json\nimport cv2\nimport numpy as np\n\nfrom datasets.imdb import imdb\nfrom model.config import cfg\n\nclass visual_genome(imdb):\n  def __init__(self, image_set, count=5):\n    imdb.__init__(self, 'visual_genome_%s_%d' % (image_set, count))\n    self._image_set = image_set\n    self._root_path = osp.join(cfg.DATA_DIR, 'visual_genome')\n    self._name_file = osp.join(self._root_path, 'synsets.txt')\n    self._anno_file = osp.join(self._root_path, self._image_set + '.json')\n    self._image_file = osp.join(self._root_path, 'image_data.json')\n    with open(self._name_file) as fid:\n      lines = fid.readlines()\n      self._raw_names = []\n      self._raw_counts = []\n      for line in lines:\n        name, cc = line.strip().split(':')\n        cc = int(cc)\n        self._raw_names.append(name)\n        self._raw_counts.append(cc)\n      self._len_raw = len(self._raw_names)\n\n    self._raw_counts = np.array(self._raw_counts)\n    # First class is always background\n    self._vg_inds = [0] + list(np.where(self._raw_counts >= count)[0])\n    self._classes = ['__background__']\n\n    for idx in self._vg_inds:\n      if idx == 0:\n        continue\n      vg_name = self._raw_names[idx]\n      self._classes.append(vg_name)\n\n    self._classes = tuple(self._classes)\n    self._class_to_ind = dict(list(zip(self.classes, list(range(self.num_classes)))))\n    self.set_proposal_method('gt')\n    # Call to get one\n    self.roidb\n\n  def _load_annotation(self):\n    gt_roidb = []\n\n    with open(self._anno_file) as fid:\n      annos = json.load(fid)\n\n    with open(self._image_file) as fid:\n      images = json.load(fid)\n      image_ids = [item['image_id'] for item in images]\n      image_dicts = dict(zip(image_ids, range(len(image_ids))))\n\n    for i in xrange(len(annos)):\n      anno = annos[i]\n      idx = image_dicts[anno['image_id']]\n      image = images[idx]\n      image_path = image['url'].replace('https://cs.stanford.edu/people/rak248/','VG/').encode('ascii')\n      self._image_index.append(image_path)\n      if i % 100 == 0:\n        print('%d: %s' % (i, image_path))\n      width = image['width']\n      height = image['height']\n\n      objects = anno['objects']\n      num_objs = sum([len(x['synsets']) for x in objects])\n\n      boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n      gt_classes = np.zeros((num_objs), dtype=np.int32)\n      seg_areas = np.zeros((num_objs), dtype=np.float32)\n\n      ix = 0\n      for obj in objects:\n        names = obj['synsets']\n        x = obj['x']\n        y = obj['y']\n        w = obj['w']\n        h = obj['h']\n\n        # need to verify if x and y are 0 based\n        x1 = max(x, 0)\n        y1 = max(y, 0)\n        x2 = min(x + w - 1, width - 1)\n        y2 = min(y + h - 1, height - 1)\n\n        if x1 > x2 or y1 > y2:\n          continue\n\n        for n in names:\n          if n in self.classes:\n            cls = self._class_to_ind[n]\n            boxes[ix, :] = [x1, y1, x2, y2]\n            gt_classes[ix] = cls\n            seg_areas[ix] = (x2 - x1 + 1) * (y2 - y1 + 1)\n            ix += 1\n\n      boxes = boxes[:ix,:]\n      gt_classes = gt_classes[:ix]\n      seg_areas = seg_areas[:ix]\n\n      gt_roidb.append({'width': width,\n                      'height': height,\n                      'boxes' : boxes,\n                      'gt_classes': gt_classes,\n                      'flipped' : False,\n                      'seg_areas': seg_areas})\n    return gt_roidb\n\n  def image_path_at(self, i):\n    return osp.join(self._root_path, self._image_index[i])\n\n  def gt_roidb(self):\n    cache_file = osp.join(self.cache_path, self.name + '_gt_roidb.pkl')\n    image_file = osp.join(self.cache_path, self.name + '_gt_image.pkl')\n    if osp.exists(cache_file) and osp.exists(image_file):\n      with open(cache_file, 'rb') as fid:\n        gt_roidb = pickle.load(fid)\n      print('{} gt roidb lovgd from {}'.format(self.name, cache_file))\n      with open(image_file, 'rb') as fid:\n        self._image_index = pickle.load(fid)\n      print('{} gt image lovgd from {}'.format(self.name, image_file))\n      return gt_roidb\n\n    gt_roidb = self._load_annotation()\n    with open(cache_file, 'wb') as fid:\n      pickle.dump(gt_roidb, fid, pickle.HIGHEST_PROTOCOL)\n    print('wrote gt roidb to {}'.format(cache_file))\n    with open(image_file, 'wb') as fid:\n      pickle.dump(self._image_index, fid, pickle.HIGHEST_PROTOCOL)\n    print('wrote gt image to {}'.format(image_file))\n    return gt_roidb\n"""
lib/datasets/voc_eval.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport xml.etree.ElementTree as ET\nimport os\ntry:\n  import cPickle as pickle\nexcept ImportError:\n  import pickle\nimport numpy as np\n\ndef parse_rec(filename):\n  """""" Parse a PASCAL VOC xml file """"""\n  tree = ET.parse(filename)\n  objects = []\n  for obj in tree.findall(\'object\'):\n    obj_struct = {}\n    obj_struct[\'name\'] = obj.find(\'name\').text\n    obj_struct[\'pose\'] = obj.find(\'pose\').text\n    obj_struct[\'truncated\'] = int(obj.find(\'truncated\').text)\n    obj_struct[\'difficult\'] = int(obj.find(\'difficult\').text)\n    bbox = obj.find(\'bndbox\')\n    obj_struct[\'bbox\'] = [int(bbox.find(\'xmin\').text),\n                          int(bbox.find(\'ymin\').text),\n                          int(bbox.find(\'xmax\').text),\n                          int(bbox.find(\'ymax\').text)]\n    objects.append(obj_struct)\n\n  return objects\n\n\ndef voc_ap(rec, prec, use_07_metric=False):\n  """""" ap = voc_ap(rec, prec, [use_07_metric])\n  Compute VOC AP given precision and recall.\n  If use_07_metric is true, uses the\n  VOC 07 11 point method (default:False).\n  """"""\n  if use_07_metric:\n    # 11 point metric\n    ap = 0.\n    for t in np.arange(0., 1.1, 0.1):\n      if np.sum(rec >= t) == 0:\n        p = 0\n      else:\n        p = np.max(prec[rec >= t])\n      ap = ap + p / 11.\n  else:\n    # correct AP calculation\n    # first append sentinel values at the end\n    mrec = np.concatenate(([0.], rec, [1.]))\n    mpre = np.concatenate(([0.], prec, [0.]))\n\n    # compute the precision envelope\n    for i in range(mpre.size - 1, 0, -1):\n      mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n    # to calculate area under PR curve, look for points\n    # where X axis (recall) changes value\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n\n    # and sum (\\Delta recall) * prec\n    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n  return ap\n\n\ndef voc_eval(detpath,\n             annopath,\n             imagesetfile,\n             classname,\n             cachedir,\n             ovthresh=0.5,\n             use_07_metric=False,\n             use_diff=False):\n  """"""rec, prec, ap = voc_eval(detpath,\n                              annopath,\n                              imagesetfile,\n                              classname,\n                              [ovthresh],\n                              [use_07_metric])\n\n  Top level function that does the PASCAL VOC evaluation.\n\n  detpath: Path to detections\n      detpath.format(classname) should produce the detection results file.\n  annopath: Path to annotations\n      annopath.format(imagename) should be the xml annotations file.\n  imagesetfile: Text file containing the list of images, one image per line.\n  classname: Category name (duh)\n  cachedir: Directory for caching the annotations\n  [ovthresh]: Overlap threshold (default = 0.5)\n  [use_07_metric]: Whether to use VOC07\'s 11 point AP computation\n      (default False)\n  """"""\n  # assumes detections are in detpath.format(classname)\n  # assumes annotations are in annopath.format(imagename)\n  # assumes imagesetfile is a text file with each line an image name\n  # cachedir caches the annotations in a pickle file\n\n  # first load gt\n  if not os.path.isdir(cachedir):\n    os.mkdir(cachedir)\n  cachefile = os.path.join(cachedir, \'%s_annots.pkl\' % imagesetfile)\n  # read list of images\n  with open(imagesetfile, \'r\') as f:\n    lines = f.readlines()\n  imagenames = [x.strip() for x in lines]\n\n  if not os.path.isfile(cachefile):\n    # load annotations\n    recs = {}\n    for i, imagename in enumerate(imagenames):\n      recs[imagename] = parse_rec(annopath.format(imagename))\n      if i % 100 == 0:\n        print(\'Reading annotation for {:d}/{:d}\'.format(\n          i + 1, len(imagenames)))\n    # save\n    print(\'Saving cached annotations to {:s}\'.format(cachefile))\n    with open(cachefile, \'w\') as f:\n      pickle.dump(recs, f)\n  else:\n    # load\n    with open(cachefile, \'rb\') as f:\n      try:\n        recs = pickle.load(f)\n      except:\n        recs = pickle.load(f, encoding=\'bytes\')\n\n  # extract gt objects for this class\n  class_recs = {}\n  npos = 0\n  for imagename in imagenames:\n    R = [obj for obj in recs[imagename] if obj[\'name\'] == classname]\n    bbox = np.array([x[\'bbox\'] for x in R])\n    if use_diff:\n      difficult = np.array([False for x in R]).astype(np.bool)\n    else:\n      difficult = np.array([x[\'difficult\'] for x in R]).astype(np.bool)\n    det = [False] * len(R)\n    npos = npos + sum(~difficult)\n    class_recs[imagename] = {\'bbox\': bbox,\n                             \'difficult\': difficult,\n                             \'det\': det}\n\n  # read dets\n  detfile = detpath.format(classname)\n  with open(detfile, \'r\') as f:\n    lines = f.readlines()\n\n  splitlines = [x.strip().split(\' \') for x in lines]\n  image_ids = [x[0] for x in splitlines]\n  confidence = np.array([float(x[1]) for x in splitlines])\n  BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n\n  nd = len(image_ids)\n  tp = np.zeros(nd)\n  fp = np.zeros(nd)\n\n  if BB.shape[0] > 0:\n    # sort by confidence\n    sorted_ind = np.argsort(-confidence)\n    sorted_scores = np.sort(-confidence)\n    BB = BB[sorted_ind, :]\n    image_ids = [image_ids[x] for x in sorted_ind]\n\n    # go down dets and mark TPs and FPs\n    for d in range(nd):\n      R = class_recs[image_ids[d]]\n      bb = BB[d, :].astype(float)\n      ovmax = -np.inf\n      BBGT = R[\'bbox\'].astype(float)\n\n      if BBGT.size > 0:\n        # compute overlaps\n        # intersection\n        ixmin = np.maximum(BBGT[:, 0], bb[0])\n        iymin = np.maximum(BBGT[:, 1], bb[1])\n        ixmax = np.minimum(BBGT[:, 2], bb[2])\n        iymax = np.minimum(BBGT[:, 3], bb[3])\n        iw = np.maximum(ixmax - ixmin + 1., 0.)\n        ih = np.maximum(iymax - iymin + 1., 0.)\n        inters = iw * ih\n\n        # union\n        uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n               (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n               (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n\n        overlaps = inters / uni\n        ovmax = np.max(overlaps)\n        jmax = np.argmax(overlaps)\n\n      if ovmax > ovthresh:\n        if not R[\'difficult\'][jmax]:\n          if not R[\'det\'][jmax]:\n            tp[d] = 1.\n            R[\'det\'][jmax] = 1\n          else:\n            fp[d] = 1.\n      else:\n        fp[d] = 1.\n\n  # compute precision recall\n  fp = np.cumsum(fp)\n  tp = np.cumsum(tp)\n  rec = tp / float(npos)\n  # avoid divide by zero in case the first detection matches a difficult\n  # ground truth\n  prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n  ap = voc_ap(rec, prec, use_07_metric)\n\n  return rec, prec, ap\n'"
lib/model/__init__.py,0,b'from . import config\n'
lib/model/bbox_transform.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\ndef bbox_transform(ex_rois, gt_rois):\n  ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0\n  ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0\n  ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\n  ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n\n  gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + 1.0\n  gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + 1.0\n  gt_ctr_x = gt_rois[:, 0] + 0.5 * gt_widths\n  gt_ctr_y = gt_rois[:, 1] + 0.5 * gt_heights\n\n  targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n  targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n  targets_dw = np.log(gt_widths / ex_widths)\n  targets_dh = np.log(gt_heights / ex_heights)\n\n  targets = np.vstack(\n    (targets_dx, targets_dy, targets_dw, targets_dh)).transpose()\n  return targets\n\n\ndef bbox_transform_inv(boxes, deltas):\n  if boxes.shape[0] == 0:\n    return np.zeros((0, deltas.shape[1]), dtype=deltas.dtype)\n\n  boxes = boxes.astype(deltas.dtype, copy=False)\n  widths = boxes[:, 2] - boxes[:, 0] + 1.0\n  heights = boxes[:, 3] - boxes[:, 1] + 1.0\n  ctr_x = boxes[:, 0] + 0.5 * widths\n  ctr_y = boxes[:, 1] + 0.5 * heights\n\n  dx = deltas[:, 0::4]\n  dy = deltas[:, 1::4]\n  dw = deltas[:, 2::4]\n  dh = deltas[:, 3::4]\n  \n  pred_ctr_x = dx * widths[:, np.newaxis] + ctr_x[:, np.newaxis]\n  pred_ctr_y = dy * heights[:, np.newaxis] + ctr_y[:, np.newaxis]\n  pred_w = np.exp(dw) * widths[:, np.newaxis]\n  pred_h = np.exp(dh) * heights[:, np.newaxis]\n\n  pred_boxes = np.zeros(deltas.shape, dtype=deltas.dtype)\n  # x1\n  pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n  # y1\n  pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n  # x2\n  pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n  # y2\n  pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n  return pred_boxes\n\n\ndef clip_boxes(boxes, im_shape):\n  """"""\n  Clip boxes to image boundaries.\n  """"""\n\n  # x1 >= 0\n  boxes[:, 0::4] = np.maximum(np.minimum(boxes[:, 0::4], im_shape[1] - 1), 0)\n  # y1 >= 0\n  boxes[:, 1::4] = np.maximum(np.minimum(boxes[:, 1::4], im_shape[0] - 1), 0)\n  # x2 < im_shape[1]\n  boxes[:, 2::4] = np.maximum(np.minimum(boxes[:, 2::4], im_shape[1] - 1), 0)\n  # y2 < im_shape[0]\n  boxes[:, 3::4] = np.maximum(np.minimum(boxes[:, 3::4], im_shape[0] - 1), 0)\n  return boxes\n'"
lib/model/config.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport os.path as osp\nimport numpy as np\n# `pip install easydict` if you don\'t have it\nfrom easydict import EasyDict as edict\n\n__C = edict()\n# Consumers can get config by:\n#   from fast_rcnn_config import cfg\ncfg = __C\n\n#\n# Memory options\n#\n__C.MEM = edict()\n\n# Number of memory iterations\n__C.MEM.ITER = 2\n\n# Height of the memory\n__C.MEM.INIT_H = 20\n# Width of the memory\n__C.MEM.INIT_W = 20\n\n# Channel of the memory\n__C.MEM.C = 512\n\n# Basic stds in the memory\n__C.MEM.STD = 0.01\n# Base stds in the memory update function for input features\n__C.MEM.U_STD = 0.01\n# Region classification\n__C.MEM.C_STD = 0.01\n\n# Feature to memory ratio\n__C.MEM.FM_R = 1.\n# Value to gate ratio\n__C.MEM.VG_R = 1.\n# FC to Pool ratio when combing the input\n__C.MEM.FP_R = 1.\n\n# Conv kernel size for memory\n__C.MEM.CONV = 3\n\n# Canonical region size\n__C.MEM.CROP_SIZE = 7\n\n# Context aggregation\n__C.MEM.CT_L = 3\n__C.MEM.CT_CONV = 3\n__C.MEM.CT_FCONV = 3\n\n# Input feature\n__C.MEM.IN_L = 2\n__C.MEM.IN_CONV = 3\n\n# Memory final fc layer channels\n__C.MEM.FC_C = 4096\n__C.MEM.FC_L = 2\n\n# The weight for the memory based prediction\n__C.MEM.WEIGHT = 1.\n# Final supervision weight\n__C.MEM.WEIGHT_FINAL = 1.\n# The threshold to control the entropy of the distribution\n__C.MEM.BETA = .5\n\n#\n# Training options\n#\n__C.TRAIN = edict()\n\n# Initial learning rate\n__C.TRAIN.RATE = 0.0005\n\n# Momentum\n__C.TRAIN.MOMENTUM = 0.9\n\n# Weight decay, for regularization\n__C.TRAIN.WEIGHT_DECAY = 0.0001\n\n# Factor for reducing the learning rate\n__C.TRAIN.GAMMA = 0.1\n\n# Step size for reducing the learning rate, currently only support one step\n__C.TRAIN.STEPSIZE = [30000]\n\n# Iteration intervals for showing the loss during training, on command line interface\n__C.TRAIN.DISPLAY = 20\n\n# Whether to double the learning rate for bias\n__C.TRAIN.DOUBLE_BIAS = True\n\n# Whether to have weight decay on bias as well\n__C.TRAIN.BIAS_DECAY = False\n\n# The number of snapshots kept, older ones are deleted to save space\n__C.TRAIN.SNAPSHOT_KEPT = 2\n\n# The time interval for saving tensorflow summaries\n__C.TRAIN.SUMMARY_ITERS = 500\n\n# The time interval for saving tensorflow summaries\n__C.TRAIN.SUMMARY_INTERVAL = 180\n\n# Scale to use during training (can list multiple scales)\n# The scale is the pixel size of an image\'s shortest side\n__C.TRAIN.SCALES = (600,)\n\n# Max pixel size of the longest side of a scaled input image\n__C.TRAIN.MAX_SIZE = 1000\n\n# Images to use per minibatch\n__C.TRAIN.IMS_PER_BATCH = 1\n\n# Use horizontally-flipped images during training?\n__C.TRAIN.USE_FLIPPED = True\n\n# How much to jitter the ground-truth bounding box, right now closed\n__C.TRAIN.BBOX_THRESH = 1.\n\n# Iterations between snapshots\n__C.TRAIN.SNAPSHOT_ITERS = 10000\n\n# solver.prototxt specifies the snapshot path prefix, this adds an optional\n# infix to yield the path: <prefix>[_<infix>]_iters_XYZ.caffemodel\n__C.TRAIN.SNAPSHOT_PREFIX = \'res101\'\n\n#\n# Testing options\n#\n__C.TEST = edict()\n\n# Scale to use during testing (can NOT list multiple scales)\n# The scale is the pixel size of an image\'s shortest side\n__C.TEST.SCALES = (600,)\n\n# Max pixel size of the longest side of a scaled input image\n__C.TEST.MAX_SIZE = 1000\n\n#\n# ResNet options\n#\n\n__C.RESNET = edict()\n\n# Option to set if max-pooling is appended after crop_and_resize. \n# if true, the region will be resized to a square of 2xPOOLING_SIZE, \n# then 2x2 max-pooling is applied; otherwise the region will be directly\n# resized to a square of POOLING_SIZE\n__C.RESNET.MAX_POOL = False\n\n# Number of fixed blocks during training, by default the first of all 4 blocks is fixed\n# Range: 0 (none) to 3 (all)\n__C.RESNET.FIXED_BLOCKS = 1\n\n#\n# MobileNet options\n#\n\n__C.MOBILENET = edict()\n\n# Whether to regularize the depth-wise filters during training\n__C.MOBILENET.REGU_DEPTH = False\n\n# Number of fixed layers during training, by default the bottom 5 of 14 layers is fixed\n# Range: 0 (none) to 12 (all)\n__C.MOBILENET.FIXED_LAYERS = 5\n\n# Weight decay for the mobilenet weights\n__C.MOBILENET.WEIGHT_DECAY = 0.00004\n\n# Depth multiplier\n__C.MOBILENET.DEPTH_MULTIPLIER = 1.\n\n#\n# MISC\n#\n\n# Class names, for visualization purposes\n__C.CLASSES = None\n\n# Pixel mean values (BGR order) as a (1, 1, 3) array\n# We use the same pixel mean for all networks even though it\'s not exactly what\n# they were trained with\n__C.PIXEL_MEANS = np.array([[[102.9801, 115.9465, 122.7717]]])\n\n# For reproducibility\n__C.RNG_SEED = 3\n\n# Root directory of project\n__C.ROOT_DIR = osp.abspath(osp.join(osp.dirname(__file__), \'..\', \'..\'))\n\n# Data directory\n__C.DATA_DIR = osp.abspath(osp.join(__C.ROOT_DIR, \'data\'))\n\n# Place outputs under an experiments directory\n__C.EXP_DIR = \'default\'\n\n# Size of the pooled region after RoI pooling\n__C.POOLING_SIZE = 7\n\n# Scale of the head spatially\n__C.BOTTLE_SCALE = 16.\n\n# EPS, a small number for numerical issue\n__C.EPS = 1e-14\n\n\ndef get_output_dir(imdb, weights_filename):\n  """"""Return the directory where experimental artifacts are placed.\n  If the directory does not exist, it is created.\n\n  A canonical path is built using the name from an imdb and a network\n  (if not None).\n  """"""\n  outdir = osp.abspath(osp.join(__C.ROOT_DIR, \'output\', __C.EXP_DIR, imdb.name))\n  if weights_filename is None:\n    weights_filename = \'default\'\n  outdir = osp.join(outdir, weights_filename)\n  if not os.path.exists(outdir):\n    os.makedirs(outdir)\n  return outdir\n\n\ndef get_output_tb_dir(imdb, weights_filename):\n  """"""Return the directory where tensorflow summaries are placed.\n  If the directory does not exist, it is created.\n\n  A canonical path is built using the name from an imdb and a network\n  (if not None).\n  """"""\n  outdir = osp.abspath(osp.join(__C.ROOT_DIR, \'tensorboard\', __C.EXP_DIR, imdb.name))\n  if weights_filename is None:\n    weights_filename = \'default\'\n  outdir = osp.join(outdir, weights_filename)\n  if not os.path.exists(outdir):\n    os.makedirs(outdir)\n  return outdir\n\n\ndef _merge_a_into_b(a, b):\n  """"""Merge config dictionary a into config dictionary b, clobbering the\n  options in b whenever they are also specified in a.\n  """"""\n  if type(a) is not edict:\n    return\n\n  for k, v in a.items():\n    # a must specify keys that are in b\n    if k not in b:\n      raise KeyError(\'{} is not a valid config key\'.format(k))\n\n    # the types must match, too\n    old_type = type(b[k])\n    if old_type is not type(v):\n      if isinstance(b[k], np.ndarray):\n        v = np.array(v, dtype=b[k].dtype)\n      else:\n        raise ValueError((\'Type mismatch ({} vs. {}) \'\n                          \'for config key: {}\').format(type(b[k]),\n                                                       type(v), k))\n\n    # recursively merge dicts\n    if type(v) is edict:\n      try:\n        _merge_a_into_b(a[k], b[k])\n      except:\n        print((\'Error under config key: {}\'.format(k)))\n        raise\n    else:\n      b[k] = v\n\n\ndef cfg_from_file(filename):\n  """"""Load a config file and merge it into the default options.""""""\n  import yaml\n  with open(filename, \'r\') as f:\n    yaml_cfg = edict(yaml.load(f))\n\n  _merge_a_into_b(yaml_cfg, __C)\n\n\ndef cfg_from_list(cfg_list):\n  """"""Set config keys via list (e.g., from command line).""""""\n  from ast import literal_eval\n  assert len(cfg_list) % 2 == 0\n  for k, v in zip(cfg_list[0::2], cfg_list[1::2]):\n    key_list = k.split(\'.\')\n    d = __C\n    for subkey in key_list[:-1]:\n      assert subkey in d\n      d = d[subkey]\n    subkey = key_list[-1]\n    assert subkey in d\n    try:\n      value = literal_eval(v)\n    except:\n      # handle the case when v is a string literal\n      value = v\n    assert type(value) == type(d[subkey]), \\\n      \'type {} does not match original type {}\'.format(\n        type(value), type(d[subkey]))\n    d[subkey] = value\n'"
lib/model/test.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cv2\nimport numpy as np\ntry:\n  import cPickle as pickle\nexcept ImportError:\n  import pickle\nimport os\n\nfrom model.config import cfg, get_output_dir\nfrom utils.timer import Timer\nfrom utils.cython_bbox import bbox_overlaps\nfrom utils.visualization import draw_predicted_boxes_test\n\n\ndef im_detect(sess, imdb, net, roidb):\n  blobs = imdb.minibatch(roidb, is_training=False)\n  _, scores = net.test_image(sess, blobs)\n\n  return scores, blobs\n\ndef im_detect_iter(sess, imdb, net, roidb, iter):\n  blobs = imdb.minibatch(roidb, is_training=False)\n  _, scores = net.test_image_iter(sess, blobs, iter)\n\n  return scores, blobs\n\ndef test_net(sess, net, imdb, roidb, weights_filename, visualize=False, iter_test=False):\n  if iter_test:\n    for iter in xrange(cfg.MEM.ITER):\n      test_net_memory(sess, net, imdb, roidb, weights_filename, visualize, iter)\n  else:\n    test_net_base(sess, net, imdb, roidb, weights_filename, visualize)\n\ndef test_net_base(sess, net, imdb, roidb, weights_filename, visualize=False):\n  np.random.seed(cfg.RNG_SEED)\n  """"""Test a Fast R-CNN network on an image database.""""""\n  num_images = len(roidb)\n  output_dir = get_output_dir(imdb, weights_filename)\n  output_dir_image = os.path.join(output_dir, \'images\')\n  if visualize and not os.path.exists(output_dir_image):\n    os.makedirs(output_dir_image)\n  all_scores = [[] for _ in range(num_images)]\n\n  # timers\n  _t = {\'score\' : Timer()}\n\n  for i in range(num_images):\n    _t[\'score\'].tic()\n    all_scores[i], blobs = im_detect(sess, imdb, net, [roidb[i]])\n    _t[\'score\'].toc()\n\n    print(\'score: {:d}/{:d} {:.3f}s\' \\\n        .format(i + 1, num_images, _t[\'score\'].average_time))\n\n    if visualize and i % 10 == 0:\n      basename = os.path.basename(imdb.image_path_at(i)).split(\'.\')[0]\n      im_vis, wrong = draw_predicted_boxes_test(blobs[\'data\'], all_scores[i], blobs[\'gt_boxes\'])\n      if wrong:\n        out_image = os.path.join(output_dir_image, basename + \'.jpg\')\n        print(out_image)\n        cv2.imwrite(out_image, im_vis)\n\n  res_file = os.path.join(output_dir, \'results.pkl\')\n  with open(res_file, \'wb\') as f:\n    pickle.dump(all_scores, f, pickle.HIGHEST_PROTOCOL)\n\n  print(\'Evaluating detections\')\n  mcls_sc, mcls_ac, mcls_ap, mins_sc, mins_ac, mins_ap = imdb.evaluate(all_scores, output_dir)\n  eval_file = os.path.join(output_dir, \'results.txt\')\n  with open(eval_file, \'w\') as f:\n    f.write(\'{:.3f} {:.3f} {:.3f} {:.3f}\'.format(mins_ap, mins_ac, mcls_ap, mcls_ac))\n\ndef test_net_memory(sess, net, imdb, roidb, weights_filename, visualize=False, iter=0):\n  np.random.seed(cfg.RNG_SEED)\n  """"""Test a Fast R-CNN network on an image database.""""""\n  num_images = len(roidb)\n  output_dir = get_output_dir(imdb, weights_filename + ""_iter%02d"" % iter)\n  output_dir_image = os.path.join(output_dir, \'images\')\n  if visualize and not os.path.exists(output_dir_image):\n    os.makedirs(output_dir_image)\n  all_scores = [[] for _ in range(num_images)]\n\n  # timers\n  _t = {\'score\' : Timer()}\n\n  for i in range(num_images):\n    _t[\'score\'].tic()\n    all_scores[i], blobs = im_detect_iter(sess, imdb, net, [roidb[i]], iter)\n    _t[\'score\'].toc()\n\n    print(\'score: {:d}/{:d} {:.3f}s\' \\\n        .format(i + 1, num_images, _t[\'score\'].average_time))\n\n    if visualize and i % 10 == 0:\n      basename = os.path.basename(imdb.image_path_at(i)).split(\'.\')[0]\n      im_vis, wrong = draw_predicted_boxes_test(blobs[\'data\'], all_scores[i], blobs[\'gt_boxes\'])\n      if wrong:\n        out_image = os.path.join(output_dir_image, basename + \'.jpg\')\n        print(out_image)\n        cv2.imwrite(out_image, im_vis)\n\n  res_file = os.path.join(output_dir, \'results.pkl\')\n  with open(res_file, \'wb\') as f:\n    pickle.dump(all_scores, f, pickle.HIGHEST_PROTOCOL)\n\n  print(\'Evaluating detections\')\n  mcls_sc, mcls_ac, mcls_ap, mins_sc, mins_ac, mins_ap = imdb.evaluate(all_scores, output_dir)\n  eval_file = os.path.join(output_dir, \'results.txt\')\n  with open(eval_file, \'w\') as f:\n    f.write(\'{:.3f} {:.3f} {:.3f} {:.3f}\'.format(mins_ap, mins_ac, mcls_ap, mcls_ac))\n\n'"
lib/model/train_val.py,17,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom model.config import cfg\nfrom utils.timer import Timer\ntry:\n  import cPickle as pickle\nexcept ImportError:\n  import pickle\nimport numpy as np\nimport os\nimport sys\nimport glob\nimport time\n\nimport tensorflow as tf\nfrom tensorflow.python import pywrap_tensorflow\n\nclass SolverWrapper(object):\n  """"""\n    A wrapper class for the training process\n  """"""\n\n  def __init__(self, sess, network, imdb, roidb, valroidb, output_dir, tbdir, pretrained_model=None):\n    self.net = network\n    self.imdb = imdb\n    self.roidb = roidb\n    self.valroidb = valroidb\n    self.output_dir = output_dir\n    self.tbdir = tbdir\n    # Simply put \'_val\' at the end to save the summaries from the validation set\n    self.tbvaldir = tbdir + \'_val\'\n    if not os.path.exists(self.tbvaldir):\n      os.makedirs(self.tbvaldir)\n    self.pretrained_model = pretrained_model\n\n  def snapshot(self, sess, iter):\n    net = self.net\n\n    if not os.path.exists(self.output_dir):\n      os.makedirs(self.output_dir)\n\n    # Store the model snapshot\n    filename = cfg.TRAIN.SNAPSHOT_PREFIX + \'_iter_{:d}\'.format(iter) + \'.ckpt\'\n    filename = os.path.join(self.output_dir, filename)\n    self.saver.save(sess, filename)\n    print(\'Wrote snapshot to: {:s}\'.format(filename))\n\n    # Also store some meta information, random state, etc.\n    nfilename = cfg.TRAIN.SNAPSHOT_PREFIX + \'_iter_{:d}\'.format(iter) + \'.pkl\'\n    nfilename = os.path.join(self.output_dir, nfilename)\n    # current state of numpy random\n    st0 = np.random.get_state()\n    # current position in the database\n    cur = self.data_layer._cur\n    # current shuffled indexes of the database\n    perm = self.data_layer._perm\n    # current position in the validation database\n    cur_val = self.data_layer_val._cur\n    # current shuffled indexes of the validation database\n    perm_val = self.data_layer_val._perm\n\n    # Dump the meta info\n    with open(nfilename, \'wb\') as fid:\n      pickle.dump(st0, fid, pickle.HIGHEST_PROTOCOL)\n      pickle.dump(cur, fid, pickle.HIGHEST_PROTOCOL)\n      pickle.dump(perm, fid, pickle.HIGHEST_PROTOCOL)\n      pickle.dump(cur_val, fid, pickle.HIGHEST_PROTOCOL)\n      pickle.dump(perm_val, fid, pickle.HIGHEST_PROTOCOL)\n      pickle.dump(iter, fid, pickle.HIGHEST_PROTOCOL)\n\n    return filename, nfilename\n\n  def from_snapshot(self, sess, sfile, nfile):\n    print(\'Restoring model snapshots from {:s}\'.format(sfile))\n    self.saver.restore(sess, sfile)\n    print(\'Restored.\')\n    # Needs to restore the other hyper-parameters/states for training, (TODO xinlei) I have\n    # tried my best to find the random states so that it can be recovered exactly\n    # However the Tensorflow state is currently not available\n    with open(nfile, \'rb\') as fid:\n      st0 = pickle.load(fid)\n      cur = pickle.load(fid)\n      perm = pickle.load(fid)\n      cur_val = pickle.load(fid)\n      perm_val = pickle.load(fid)\n      last_snapshot_iter = pickle.load(fid)\n\n      np.random.set_state(st0)\n      self.data_layer._cur = cur\n      self.data_layer._perm = perm\n      self.data_layer_val._cur = cur_val\n      self.data_layer_val._perm = perm_val\n\n    return last_snapshot_iter\n\n  def get_variables_in_checkpoint_file(self, file_name):\n    try:\n      reader = pywrap_tensorflow.NewCheckpointReader(file_name)\n      var_to_shape_map = reader.get_variable_to_shape_map()\n      return var_to_shape_map \n    except Exception as e:  # pylint: disable=broad-except\n      print(str(e))\n      if ""corrupted compressed block contents"" in str(e):\n        print(""It\'s likely that your checkpoint file has been compressed ""\n              ""with SNAPPY."")\n\n  def construct_graph(self, sess):\n    with sess.graph.as_default():\n      # Set the random seed for tensorflow\n      tf.set_random_seed(cfg.RNG_SEED)\n      # Build the main computation graph\n      layers = self.net.create_architecture(\'TRAIN\', self.imdb.num_classes, tag=\'default\')\n      # Define the loss\n      loss = layers[\'total_loss\']\n      # Set learning rate and momentum\n      lr = tf.Variable(cfg.TRAIN.RATE, trainable=False)\n      self.optimizer = tf.train.MomentumOptimizer(lr, cfg.TRAIN.MOMENTUM)\n\n      # Compute the gradients with regard to the loss\n      gvs = self.optimizer.compute_gradients(loss)\n      grad_summaries = []\n      for grad, var in gvs:\n        if grad == None:\n          continue\n        grad_summaries.append(tf.summary.histogram(\'TRAIN/\' + var.name, var))\n        grad_summaries.append(tf.summary.histogram(\'GRAD/\' + var.name, grad))\n\n      # Double the gradient of the bias if set\n      if cfg.TRAIN.DOUBLE_BIAS:\n        final_gvs = []\n        with tf.variable_scope(\'Gradient_Mult\') as scope:\n          for grad, var in gvs:\n            scale = 1.\n            if cfg.TRAIN.DOUBLE_BIAS and \'/biases:\' in var.name:\n              scale *= 2.\n            if not np.allclose(scale, 1.0):\n              grad = tf.multiply(grad, scale)\n            final_gvs.append((grad, var))\n        train_op = self.optimizer.apply_gradients(final_gvs)\n      else:\n        train_op = self.optimizer.apply_gradients(gvs)\n      self.summary_grads = tf.summary.merge(grad_summaries)\n\n      # We will handle the snapshots ourselves\n      self.saver = tf.train.Saver(max_to_keep=100000)\n      # Write the train and validation information to tensorboard\n      self.writer = tf.summary.FileWriter(self.tbdir, sess.graph)\n      self.valwriter = tf.summary.FileWriter(self.tbvaldir)\n\n    return lr, train_op\n\n  def find_previous(self):\n    sfiles = os.path.join(self.output_dir, cfg.TRAIN.SNAPSHOT_PREFIX + \'_iter_*.ckpt.meta\')\n    sfiles = glob.glob(sfiles)\n    sfiles.sort(key=os.path.getmtime)\n    # Get the snapshot name in TensorFlow\n    redfiles = []\n    for stepsize in cfg.TRAIN.STEPSIZE:\n      redfiles.append(os.path.join(self.output_dir, \n                      cfg.TRAIN.SNAPSHOT_PREFIX + \'_iter_{:d}.ckpt.meta\'.format(stepsize+1)))\n    if len(sfiles) > 1:\n      sfiles = [ss.replace(\'.meta\', \'\') for ss in sfiles if ss not in redfiles]\n    else:\n      sfiles = [ss.replace(\'.meta\', \'\') for ss in sfiles]\n\n    nfiles = os.path.join(self.output_dir, cfg.TRAIN.SNAPSHOT_PREFIX + \'_iter_*.pkl\')\n    nfiles = glob.glob(nfiles)\n    nfiles.sort(key=os.path.getmtime)\n    redfiles = [redfile.replace(\'.ckpt.meta\', \'.pkl\') for redfile in redfiles]\n    if len(nfiles) > 1:\n      nfiles = [nn for nn in nfiles if nn not in redfiles]\n\n    lsf = len(sfiles)\n    assert len(nfiles) == lsf\n\n    return lsf, nfiles, sfiles\n\n  def initialize(self, sess):\n    # Fresh train directly from ImageNet weights\n    print(\'Loading initial model weights from {:s}\'.format(self.pretrained_model))\n    variables = tf.global_variables()\n    # Initialize all variables first\n    sess.run(tf.variables_initializer(variables, name=\'init\'))\n    var_keep_dic = self.get_variables_in_checkpoint_file(self.pretrained_model)\n    # Get the variables to restore, ignoring the variables to fix\n    variables_to_restore = self.net.get_variables_to_restore(variables, var_keep_dic)\n\n    restorer = tf.train.Saver(variables_to_restore)\n    restorer.restore(sess, self.pretrained_model)\n    print(\'Loaded.\')\n    # Need to fix the variables before loading, so that the RGB weights are changed to BGR\n    # For VGG16 it also changes the convolutional weights fc6 and fc7 to\n    # fully connected weights\n    self.net.fix_variables(sess, self.pretrained_model)\n    self.net.fixed_parameters(sess)\n    print(\'Fixed.\')\n    rate = cfg.TRAIN.RATE\n    last_snapshot_iter = 0\n    stepsizes = list(cfg.TRAIN.STEPSIZE)\n    # Initial file lists are empty\n    np_paths = []\n    ss_paths = []\n\n    return rate, last_snapshot_iter, stepsizes, np_paths, ss_paths\n\n  def restore(self, sess, sfile, nfile):\n    # Get the most recent snapshot and restore\n    np_paths = [nfile]\n    ss_paths = [sfile]\n    # Restore model from snapshots\n    last_snapshot_iter = self.from_snapshot(sess, sfile, nfile)\n    # Set the learning rate\n    rate = cfg.TRAIN.RATE\n    stepsizes = []\n    for stepsize in cfg.TRAIN.STEPSIZE:\n      if last_snapshot_iter > stepsize:\n        rate *= cfg.TRAIN.GAMMA\n      else:\n        stepsizes.append(stepsize)\n\n    return rate, last_snapshot_iter, stepsizes, np_paths, ss_paths\n\n  def remove_snapshot(self, np_paths, ss_paths):\n    to_remove = len(np_paths) - cfg.TRAIN.SNAPSHOT_KEPT\n    for c in range(to_remove):\n      nfile = np_paths[0]\n      os.remove(str(nfile))\n      np_paths.remove(nfile)\n\n    to_remove = len(ss_paths) - cfg.TRAIN.SNAPSHOT_KEPT\n    for c in range(to_remove):\n      sfile = ss_paths[0]\n      # To make the code compatible to earlier versions of Tensorflow,\n      # where the naming tradition for checkpoints are different\n      if os.path.exists(str(sfile)):\n        os.remove(str(sfile))\n      else:\n        os.remove(str(sfile + \'.data-00000-of-00001\'))\n        os.remove(str(sfile + \'.index\'))\n      sfile_meta = sfile + \'.meta\'\n      os.remove(str(sfile_meta))\n      ss_paths.remove(sfile)\n\n  def train_model(self, sess, max_iters):\n    # Build data layers for both training and validation set\n    self.data_layer = self.imdb.data_layer(self.roidb, self.imdb.num_classes)\n    self.data_layer_val = self.imdb.data_layer(self.valroidb, self.imdb.num_classes, random=True)\n\n    # Construct the computation graph\n    lr, train_op = self.construct_graph(sess)\n\n    # Find previous snapshots if there is any to restore from\n    lsf, nfiles, sfiles = self.find_previous()\n\n    # Initialize the variables or restore them from the last snapshot\n    if lsf == 0:\n      rate, last_snapshot_iter, stepsizes, np_paths, ss_paths = self.initialize(sess)\n    else:\n      rate, last_snapshot_iter, stepsizes, np_paths, ss_paths = self.restore(sess, \n                                                                            str(sfiles[-1]), \n                                                                            str(nfiles[-1]))\n    timer = Timer()\n    iter = last_snapshot_iter + 1\n    last_summary_iter = iter\n    last_summary_time = time.time()\n    # Make sure the lists are not empty\n    stepsizes.append(max_iters)\n    stepsizes.reverse()\n    next_stepsize = stepsizes.pop()\n    while iter < max_iters + 1:\n      # Learning rate\n      if iter == next_stepsize + 1:\n        # Add snapshot here before reducing the learning rate\n        self.snapshot(sess, iter)\n        rate *= cfg.TRAIN.GAMMA\n        sess.run(tf.assign(lr, rate))\n        next_stepsize = stepsizes.pop()\n\n      timer.tic()\n      # Get training data, one batch at a time\n      blobs = self.data_layer.forward()\n\n      now = time.time()\n      if iter == 1 or \\\n          (now - last_summary_time > cfg.TRAIN.SUMMARY_INTERVAL and \\\n          iter - last_summary_iter > cfg.TRAIN.SUMMARY_ITERS):\n        # Compute the graph with summary\n        loss_cls, total_loss, summary, gsummary = \\\n          self.net.train_step_with_summary(sess, blobs, train_op, self.summary_grads)\n        self.writer.add_summary(summary, float(iter))\n        self.writer.add_summary(gsummary, float(iter+1))\n        # Also check the summary on the validation set\n        blobs_val = self.data_layer_val.forward()\n        summary_val = self.net.get_summary(sess, blobs_val)\n        self.valwriter.add_summary(summary_val, float(iter))\n        last_summary_iter = iter\n        last_summary_time = now\n      else:\n        # Compute the graph without summary\n        loss_cls, total_loss = self.net.train_step(sess, blobs, train_op)\n      timer.toc()\n\n      # Display training information\n      if iter % (cfg.TRAIN.DISPLAY) == 0:\n        print(\'iter: %d / %d, total loss: %.6f\\n >>> loss_cls: %.6f\\n >>> lr: %f\' % \\\n              (iter, max_iters, total_loss, loss_cls, lr.eval()))\n        print(\'speed: {:.3f}s / iter\'.format(timer.average_time))\n\n      # Snapshotting\n      if iter % cfg.TRAIN.SNAPSHOT_ITERS == 0:\n        last_snapshot_iter = iter\n        ss_path, np_path = self.snapshot(sess, iter)\n        np_paths.append(np_path)\n        ss_paths.append(ss_path)\n\n        # Remove the old snapshots if there are too many\n        if len(np_paths) > cfg.TRAIN.SNAPSHOT_KEPT:\n          self.remove_snapshot(np_paths, ss_paths)\n\n      iter += 1\n\n    if last_snapshot_iter != iter - 1:\n      self.snapshot(sess, iter - 1)\n\n    self.writer.close()\n    self.valwriter.close()\n\n\ndef get_training_roidb(imdb):\n  """"""Returns a roidb (Region of Interest database) for use in training.""""""\n  if cfg.TRAIN.USE_FLIPPED:\n    print(\'Appending horizontally-flipped training examples...\')\n    imdb.append_flipped_images()\n    print(\'done\')\n\n  for i in range(len(imdb.image_index)):\n    imdb.roidb[i][\'image\'] = imdb.image_path_at(i)\n\n  return imdb.roidb\n\n\ndef filter_roidb(roidb):\n  """"""Remove roidb entries that have no usable RoIs.""""""\n\n  def is_valid(entry):\n    # Valid images have at least one ground truth labeled\n    valid = len(entry[\'gt_classes\']) > 0\n    return valid\n\n  num = len(roidb)\n  filtered_roidb = [entry for entry in roidb if is_valid(entry)]\n  num_after = len(filtered_roidb)\n  print(\'Filtered {} roidb entries: {} -> {}\'.format(num - num_after,\n                                                     num, num_after))\n  return filtered_roidb\n\n\ndef train_net(network, imdb, roidb, valroidb, output_dir, tb_dir,\n              pretrained_model=None,\n              max_iters=40000):\n  """"""Train a Faster R-CNN network.""""""\n  roidb = filter_roidb(roidb)\n  valroidb = filter_roidb(valroidb)\n\n  tfconfig = tf.ConfigProto(allow_soft_placement=True)\n  tfconfig.gpu_options.allow_growth = True\n\n  with tf.Session(config=tfconfig) as sess:\n    sw = SolverWrapper(sess, network, imdb, roidb, valroidb, output_dir, tb_dir,\n                       pretrained_model=pretrained_model)\n    print(\'Solving...\')\n    sw.train_model(sess, max_iters)\n    print(\'done solving\')\n'"
lib/model/train_val_memory.py,14,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom model.config import cfg\nfrom model.train_val import filter_roidb, SolverWrapper\n\nfrom utils.timer import Timer\ntry:\n  import cPickle as pickle\nexcept ImportError:\n  import pickle\nimport numpy as np\nimport os\nimport sys\nimport glob\nimport time\n\nimport tensorflow as tf\nfrom tensorflow.python import pywrap_tensorflow\n\nclass MemorySolverWrapper(SolverWrapper):\n  """"""\n    A wrapper class for the training process of spatial memory\n  """"""\n\n  def construct_graph(self, sess):\n    with sess.graph.as_default():\n      # Set the random seed for tensorflow\n      tf.set_random_seed(cfg.RNG_SEED)\n      # Build the main computation graph\n      layers = self.net.create_architecture(\'TRAIN\', self.imdb.num_classes, tag=\'default\')\n      # Define the loss\n      loss = layers[\'total_loss\']\n      # Set learning rate and momentum\n      lr = tf.Variable(cfg.TRAIN.RATE, trainable=False)\n      self.optimizer = tf.train.MomentumOptimizer(lr, cfg.TRAIN.MOMENTUM)\n\n      # Compute the gradients with regard to the loss\n      gvs = self.optimizer.compute_gradients(loss)\n      grad_summaries = []\n      for grad, var in gvs:\n        if \'SMN\' not in var.name and \'GMN\' not in var.name:\n          continue\n        grad_summaries.append(tf.summary.histogram(\'TRAIN/\' + var.name, var))\n        if grad is not None:\n          grad_summaries.append(tf.summary.histogram(\'GRAD/\' + var.name, grad))\n\n      # Double the gradient of the bias if set\n      if cfg.TRAIN.DOUBLE_BIAS:\n        final_gvs = []\n        with tf.variable_scope(\'Gradient_Mult\') as scope:\n          for grad, var in gvs:\n            scale = 1.\n            if cfg.TRAIN.DOUBLE_BIAS and \'/biases:\' in var.name:\n              scale *= 2.\n            if not np.allclose(scale, 1.0):\n              grad = tf.multiply(grad, scale)\n            final_gvs.append((grad, var))\n        train_op = self.optimizer.apply_gradients(final_gvs)\n      else:\n        train_op = self.optimizer.apply_gradients(gvs)\n      self.summary_grads = tf.summary.merge(grad_summaries)\n\n      # We will handle the snapshots ourselves\n      self.saver = tf.train.Saver(max_to_keep=100000)\n      # Write the train and validation information to tensorboard\n      self.writer = tf.summary.FileWriter(self.tbdir, sess.graph)\n      self.valwriter = tf.summary.FileWriter(self.tbvaldir)\n\n    return lr, train_op\n\n  def train_model(self, sess, max_iters):\n    # Build data layers for both training and validation set\n    self.data_layer = self.imdb.data_layer(self.roidb, self.imdb.num_classes)\n    self.data_layer_val = self.imdb.data_layer(self.valroidb, self.imdb.num_classes, random=True)\n\n    # Construct the computation graph\n    lr, train_op = self.construct_graph(sess)\n\n    # Find previous snapshots if there is any to restore from\n    lsf, nfiles, sfiles = self.find_previous()\n\n    # Initialize the variables or restore them from the last snapshot\n    if lsf == 0:\n      rate, last_snapshot_iter, stepsizes, np_paths, ss_paths = self.initialize(sess)\n    else:\n      rate, last_snapshot_iter, stepsizes, np_paths, ss_paths = self.restore(sess, \n                                                                            str(sfiles[-1]), \n                                                                            str(nfiles[-1]))\n    timer = Timer()\n    iter = last_snapshot_iter + 1\n    last_summary_iter = iter\n    last_summary_time = time.time()\n    # Make sure the lists are not empty\n    stepsizes.append(max_iters)\n    stepsizes.reverse()\n    next_stepsize = stepsizes.pop()\n    while iter < max_iters + 1:\n      # Learning rate\n      if iter == next_stepsize + 1:\n        # Add snapshot here before reducing the learning rate\n        self.snapshot(sess, iter)\n        rate *= cfg.TRAIN.GAMMA\n        sess.run(tf.assign(lr, rate))\n        next_stepsize = stepsizes.pop()\n\n      timer.tic()\n      # Get training data, one batch at a time\n      blobs = self.data_layer.forward()\n\n      now = time.time()\n      if iter == 1 or \\\n          (now - last_summary_time > cfg.TRAIN.SUMMARY_INTERVAL and \\\n          iter - last_summary_iter > cfg.TRAIN.SUMMARY_ITERS):\n        # Compute the graph with summary\n        loss_cls, total_loss, summary, gsummary = \\\n          self.net.train_step_with_summary(sess, blobs, train_op, self.summary_grads)\n        self.writer.add_summary(summary, float(iter))\n        self.writer.add_summary(gsummary, float(iter+1))\n        # Also check the summary on the validation set\n       \tblobs_val = self.data_layer_val.forward()\n        summary_val = self.net.get_summary(sess, blobs_val)\n        self.valwriter.add_summary(summary_val, float(iter))\n        last_summary_iter = iter\n        last_summary_time = now\n      else:\n        # Compute the graph without summary\n        loss_cls, total_loss = self.net.train_step(sess, blobs, train_op)\n      timer.toc()\n\n      # Display training information\n      if iter % (cfg.TRAIN.DISPLAY) == 0:\n        print(\'iter: %d / %d, total loss: %.6f\\n >>> loss_cls: %.6f\\n >>> lr: %f\' % \\\n              (iter, max_iters, total_loss, loss_cls, lr.eval()))\n        print(\'speed: {:.3f}s / iter\'.format(timer.average_time))\n\n      # Snapshotting\n      if iter % cfg.TRAIN.SNAPSHOT_ITERS == 0:\n        last_snapshot_iter = iter\n        ss_path, np_path = self.snapshot(sess, iter)\n        np_paths.append(np_path)\n        ss_paths.append(ss_path)\n\n        # Remove the old snapshots if there are too many\n        if len(np_paths) > cfg.TRAIN.SNAPSHOT_KEPT:\n          self.remove_snapshot(np_paths, ss_paths)\n\n      iter += 1\n\n    if last_snapshot_iter != iter - 1:\n      self.snapshot(sess, iter - 1)\n\n    self.writer.close()\n    self.valwriter.close()\n\n\ndef train_net(network, imdb, roidb, valroidb, output_dir, tb_dir,\n              pretrained_model=None,\n              max_iters=40000):\n  """"""Train a Faster R-CNN network with memory.""""""\n  roidb = filter_roidb(roidb)\n  valroidb = filter_roidb(valroidb)\n\n  tfconfig = tf.ConfigProto(allow_soft_placement=True)\n  tfconfig.gpu_options.allow_growth = True\n\n  with tf.Session(config=tfconfig) as sess:\n    sw = MemorySolverWrapper(sess, network, imdb, roidb, valroidb, \n                            output_dir, tb_dir,\n                            pretrained_model=pretrained_model)\n    print(\'Solving...\')\n    sw.train_model(sess, max_iters)\n    print(\'done solving\')\n'"
lib/nets/__init__.py,0,b''
lib/nets/attend_memory.py,47,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nimport numpy as np\n\nfrom nets.base_memory import BaseMemory\nfrom nets.vgg16 import vgg16\nfrom nets.resnet_v1 import resnetv1\nfrom nets.mobilenet_v1 import mobilenetv1\n\nfrom model.config import cfg\nfrom utils.snippets import update_weights\nfrom utils.visualization import draw_predicted_boxes, draw_predicted_boxes_attend, draw_weights\n\nclass AttendMemory(BaseMemory):\n  def __init__(self):\n    BaseMemory.__init__(self)\n    self._predictions[""confid""] = []\n    self._predictions[""weights""] = []\n    # self._attends = []\n    self._attend_means = []\n\n  def _update_weights(self, labels, cls_prob, iter):\n    weights = tf.py_func(update_weights,\n                        [labels, cls_prob],\n                        tf.float32, \n                        name=""weights_%02d"" % iter)\n\n    weights.set_shape([None])\n    self._predictions[""weights""].append(weights)\n\n    # It is the label for the next iteration\n    self._score_summaries[iter+1].append(weights)\n\n  def _add_attend_mean_summary(self, iter):\n    image = tf.py_func(draw_weights,\n                      [self._attend_means[iter]],\n                      tf.uint8,\n                      name=""attend"")\n    return tf.summary.image(\'MEAN-%02d\' % iter, image)\n\n  def _add_pred_memory_summary(self, iter):\n    # also visualize the predictions of the network\n    if self._gt_image is None:\n      self._add_gt_image()\n    if iter == 0:\n      image = tf.py_func(draw_predicted_boxes_attend,\n                         [self._gt_image,\n                         self._predictions[\'cls_prob\'][iter],\n                         self._gt_boxes,\n                         self._predictions[\'confid_prob\'][iter]],\n                         tf.float32, name=""pred_boxes"")\n    else:\n      image = tf.py_func(draw_predicted_boxes_attend,\n                         [self._gt_image,\n                         self._predictions[\'cls_prob\'][iter],\n                         self._gt_boxes,\n                         self._predictions[\'confid_prob\'][iter],\n                         self._predictions[""weights""][iter-1]],\n                         tf.float32, name=""pred_boxes"")\n    return tf.summary.image(\'PRED-%02d\' % iter, image)\n\n  def _add_pred_attend_summary(self):\n    # also visualize the predictions of the network\n    if self._gt_image is None:\n      self._add_gt_image()\n    image = tf.py_func(draw_predicted_boxes,\n                       [self._gt_image,\n                       self._predictions[\'attend_cls_prob\'],\n                       self._gt_boxes],\n                       tf.float32, name=""attend_boxes"")\n    return tf.summary.image(\'PRED-attend\', image)\n\n  def _confidence_init(self, fc7, is_training):\n    initializer = tf.random_normal_initializer(mean=0.0, stddev=0.01)\n    confid = slim.fully_connected(fc7, 1, \n                                 weights_initializer=initializer,\n                                 trainable=is_training,\n                                 activation_fn=None, \n                                 biases_initializer=None,\n                                 scope=\'confid\')\n    self._base_confidence = confid\n    self._score_summaries[0].append(confid)\n\n  def _confidence_iter(self, mem_fc7, is_training, name, iter):\n    initializer = tf.random_normal_initializer(mean=0.0, stddev=cfg.MEM.C_STD)\n    with tf.variable_scope(name):\n      confid_mem = slim.fully_connected(mem_fc7, 1, \n                                        weights_initializer=initializer,\n                                        trainable=is_training,\n                                        activation_fn=None,\n                                        biases_initializer=None,\n                                        scope=""confid_mem"")\n      self._predictions[\'confid\'].append(confid_mem)\n      self._score_summaries[iter].append(confid_mem)\n\n  def _aggregate_pred(self, name):\n    with tf.variable_scope(name):\n      comb_confid = tf.stack(self._predictions[\'confid\'], axis=2, name=\'comb_confid\')\n      comb_attend = tf.nn.softmax(comb_confid, dim=2, name=\'comb_attend\')\n      self._predictions[\'confid_prob\'] = tf.unstack(comb_attend, axis=2, name=\'unstack_attend\')\n      comb_score = tf.stop_gradient(tf.stack(self._predictions[""cls_score""], \n                                    axis=2, name=\'comb_score\'),\n                                    name=""comb_score_nb"")\n      cls_score = tf.reduce_sum(tf.multiply(comb_score, comb_attend, name=\'weighted_cls_score\'), \n                                axis=2, name=\'attend_cls_score\')\n      cls_prob = tf.nn.softmax(cls_score, name=""cls_prob"")\n      cls_pred = tf.argmax(cls_score, axis=1, name=""cls_pred"")\n\n      self._predictions[""attend_cls_score""] = cls_score\n      self._predictions[""attend_cls_prob""] = cls_prob\n      self._predictions[""attend_cls_pred""] = cls_pred\n      self._predictions[\'attend\'] = self._predictions[\'confid_prob\']\n\n    return cls_prob\n\n  def _build_conv(self, is_training):\n    # Get the head\n    net_conv = self._image_to_head(is_training)\n    with tf.variable_scope(self._scope, self._scope):\n      # get the region of interest\n      rois, batch_ids, inv_rois, inv_batch_ids = self._target_memory_layer(""target"")\n      # region of interest pooling\n      pool5 = self._crop_rois(net_conv, rois, batch_ids, ""pool5"")\n      pool5_nb = tf.stop_gradient(pool5, name=""pool5_nb"")\n\n    # initialize the normalization vector, note here it is the batch ids\n    count_matrix_raw, self._count_crops = self._inv_crops(self._count_base, inv_rois, batch_ids, ""count_matrix"")\n    self._count_matrix = tf.stop_gradient(count_matrix_raw, name=\'cm_nb\')\n    self._count_matrix_eps = tf.maximum(self._count_matrix, cfg.EPS, name=\'count_eps\')\n    self._score_summaries[0].append(self._count_matrix)\n\n    fc7 = self._head_to_tail(pool5, is_training)\n    # First iteration\n    with tf.variable_scope(self._scope, self._scope):\n      # region classification\n      cls_score_conv, cls_prob_conv = self._cls_init(fc7, is_training)\n\n    return cls_score_conv, pool5_nb, \\\n           rois, batch_ids, inv_rois, inv_batch_ids\n\n  def _build_pred(self, is_training, mem, cls_score_conv, rois, batch_ids, iter):\n    if cfg.MEM.CT_L:\n      mem_net = self._context(mem, is_training, ""context"", iter)\n    else:\n      mem_net = mem\n    mem_ct_pool5 = self._crop_rois(mem_net, rois, batch_ids, ""mem_ct_pool5"")\n    mem_fc7 = self._fc_iter(mem_ct_pool5, is_training, ""fc7"", iter) \n    cls_score_mem = self._cls_iter(mem_fc7, is_training, ""cls_iter"", iter)\n    self._confidence_iter(mem_fc7, is_training, ""confid_iter"", iter)\n    cls_score, cls_prob, cls_pred = self._comb_conv_mem(cls_score_conv, cls_score_mem, \n                                                        ""comb_conv_mem"", iter)\n\n    return cls_score, cls_prob, cls_pred\n\n  def _build_memory(self, is_training, is_testing):\n    # initialize memory\n    mem = self._mem_init(is_training, ""mem_init"")\n    # convolution related stuff\n    cls_score_conv, pool5_nb, \\\n    rois, batch_ids, inv_rois, inv_batch_ids = self._build_conv(is_training)\n    # Separate first prediction\n    reuse = None\n    # Memory iterations\n    self._labels = tf.reshape(self._targets[""labels""], [-1])\n    for iter in xrange(cfg.MEM.ITER):\n      print(\'ITERATION: %02d\' % iter)\n      self._mems.append(mem)\n      with tf.variable_scope(\'SMN\', reuse=reuse):\n        # Use memory to predict the output\n        cls_score, cls_prob, cls_pred = self._build_pred(is_training, \n                                                         mem, \n                                                         cls_score_conv, \n                                                         rois, batch_ids, iter)\n\n        if iter == cfg.MEM.ITER - 1:\n          break\n\n        # Update the memory with all the regions\n        mem = self._build_update(is_training, mem, \n                                pool5_nb, cls_score, cls_prob, cls_pred,\n                                rois, batch_ids, inv_rois, inv_batch_ids, \n                                iter)\n\n        if is_training:\n          self._update_weights(self._labels, cls_prob, iter)\n\n      if iter == 0:\n        reuse = True\n\n    # Need to finalize the class scores, regardless of whether loss is computed\n    cls_prob = self._aggregate_pred(""aggregate"")\n\n    return rois, cls_prob\n\n  def _add_memory_losses(self, name):\n    cross_entropy = []\n    assert len(self._predictions[""cls_score""]) == cfg.MEM.ITER\n    assert len(self._predictions[""weights""]) == cfg.MEM.ITER - 1\n    with tf.variable_scope(name):\n      # Then add ones for later iterations\n      for iter in xrange(cfg.MEM.ITER):\n        # RCNN, class loss\n        cls_score = self._predictions[""cls_score""][iter]\n        ce_ins = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=cls_score, \n                                                                labels=self._labels,\n                                                                name=""ce_ins_%02d"" % iter)\n        if iter > 0:\n          weight = self._predictions[""weights""][iter-1]\n          ce = tf.reduce_sum(tf.multiply(weight, ce_ins, name=""weight_%02d"" % iter), name=""ce_%02d"" % iter)\n        else:\n          ce = tf.reduce_mean(ce_ins, name=""ce_%02d"" % iter)\n        cross_entropy.append(ce)\n\n      # The final most important score\n      cls_score = self._predictions[""attend_cls_score""]\n      ce_final = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=cls_score, \n                                                                               labels=self._labels,\n                                                                               name=""ce_ins""), name=""ce"")\n      self._losses[\'cross_entropy_final\'] = ce_final\n \n      ce_rest = tf.stack(cross_entropy[1:], name=""cross_entropy_rest"")\n      self._losses[\'cross_entropy_image\'] = cross_entropy[0]\n      self._losses[\'cross_entropy_memory\'] = tf.reduce_mean(ce_rest, name=\'cross_entropy\')\n      self._losses[\'cross_entropy\'] = self._losses[\'cross_entropy_image\'] \\\n                                    + cfg.MEM.WEIGHT * self._losses[\'cross_entropy_memory\'] \\\n                                    + cfg.MEM.WEIGHT_FINAL * self._losses[\'cross_entropy_final\']\n\n      loss = self._losses[\'cross_entropy\']\n      regularization_loss = tf.add_n(tf.losses.get_regularization_losses(), \'regu\')\n      self._losses[\'total_loss\'] = loss + regularization_loss\n\n      self._event_summaries.update(self._losses)\n\n    return loss\n\n  def _add_attend(self, iter):\n    attend_reshape = tf.reshape(self._predictions[\'attend\'][iter], [-1, 1, 1, 1], name=""reshape_attend_%d"" % iter)\n    attend = tf.multiply(self._count_crops, attend_reshape, name=""attend_%d"" % iter)\n    sum_attend = tf.reduce_sum(attend, axis=0, keep_dims=True, name=""sum_attend_%d"" % iter)\n    # self._attends.append(sum_attend)\n    self._attend_means.append(tf.div(sum_attend, self._count_matrix_eps, name=""mean_attend_%d"" % iter))\n\n  def _create_summary(self):\n    val_summaries = []\n    with tf.device(""/cpu:0""):\n      val_summaries.append(self._add_gt_image_summary())\n      val_summaries.append(self._add_pred_attend_summary())\n      for iter in xrange(cfg.MEM.ITER):\n        self._add_attend(iter)\n      for iter in xrange(cfg.MEM.ITER):\n        val_summaries.append(self._add_pred_memory_summary(iter))\n        val_summaries.append(self._add_memory_summary(iter))\n        # val_summaries.append(self._add_attend_summary(iter))\n        val_summaries.append(self._add_attend_mean_summary(iter))\n        for var in self._score_summaries[iter]:\n          self._add_score_iter_summary(iter, var)\n      for key, var in self._event_summaries.items():\n        val_summaries.append(tf.summary.scalar(key, var))\n      for var in self._act_summaries:\n        self._add_zero_summary(var)\n\n    self._summary_op = tf.summary.merge_all()\n    self._summary_op_val = tf.summary.merge(val_summaries)\n\n  # take the last predicted output\n  def test_image(self, sess, blobs):\n    cls_score, cls_prob = sess.run([self._predictions[""attend_cls_score""],\n                                    self._predictions[\'attend_cls_prob\']],\n                                   feed_dict=self._parse_dict(blobs))\n    return cls_score, cls_prob\n\nclass vgg16_memory(AttendMemory, vgg16):\n  def __init__(self):\n    AttendMemory.__init__(self)\n    self._feat_stride = [16, ]\n    self._feat_compress = [1. / float(self._feat_stride[0]), ]\n    self._scope = \'vgg_16\'\n\nclass resnetv1_memory(AttendMemory, resnetv1):\n  def __init__(self, num_layers=50):\n    AttendMemory.__init__(self)\n    self._feat_stride = [16, ]\n    self._feat_compress = [1. / float(self._feat_stride[0]), ]\n    self._num_layers = num_layers\n    self._scope = \'resnet_v1_%d\' % num_layers\n    resnetv1._decide_blocks(self)\n\nclass mobilenetv1_memory(AttendMemory, mobilenetv1):\n  def __init__(self):\n    AttendMemory.__init__(self)\n    self._feat_stride = [16, ]\n    self._feat_compress = [1. / float(self._feat_stride[0]), ]\n    self._depth_multiplier = cfg.MOBILENET.DEPTH_MULTIPLIER\n    self._scope = \'MobilenetV1\'\n'"
lib/nets/base_memory.py,110,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nimport numpy as np\n\nfrom nets.network import Network\nfrom nets.vgg16 import vgg16\nfrom nets.resnet_v1 import resnetv1\nfrom nets.mobilenet_v1 import mobilenetv1\n\nfrom model.config import cfg\nfrom utils.snippets import compute_target_memory\nfrom utils.visualization import draw_predicted_boxes, draw_memory\n\nclass BaseMemory(Network):\n  def __init__(self):\n    self._predictions = {}\n    self._predictions[""cls_score""] = []\n    self._predictions[""cls_prob""] = []\n    self._predictions[""cls_pred""] = []\n\n    self._losses = {}\n    self._targets = {}\n    self._layers = {}\n    self._gt_image = None\n    self._mems = []\n    self._act_summaries = []\n    self._score_summaries = [[] for _ in xrange(cfg.MEM.ITER)]\n    self._event_summaries = {}\n    self._variables_to_fix = {}\n\n  def _add_score_iter_summary(self, iter, tensor):\n    tf.summary.histogram(\'SCORE-%02d/\' % iter + tensor.op.name, tensor)\n\n  def _add_memory_summary(self, iter):\n    image = tf.py_func(draw_memory,\n                      [self._mems[iter]],\n                      tf.float32,\n                      name=""memory"")\n    return tf.summary.image(\'MEM-%02d\' % iter, image)\n\n  def _add_pred_memory_summary(self, iter):\n    # also visualize the predictions of the network\n    if self._gt_image is None:\n      self._add_gt_image()\n    image = tf.py_func(draw_predicted_boxes,\n                       [self._gt_image,\n                       self._predictions[\'cls_prob\'][iter],\n                       self._gt_boxes],\n                       tf.float32, name=""pred_boxes"")\n    return tf.summary.image(\'PRED-%02d\' % iter, image)\n\n  def _target_memory_layer(self, name):\n    with tf.variable_scope(name):\n      rois, batch_ids, labels, inv_rois, inv_batch_ids = tf.py_func(compute_target_memory,\n                                                          [self._memory_size, self._gt_boxes, self._feat_stride[0]],\n                                                          [tf.float32, tf.int32, tf.int32, tf.float32, tf.int32],\n                                                          name=""target_memory_layer"")\n\n      rois.set_shape([None, 4])\n      labels.set_shape([None, 1])\n      inv_rois.set_shape([None, 4])\n\n      self._targets[\'rois\'] = rois\n      self._targets[\'batch_ids\'] = batch_ids\n      self._targets[\'labels\'] = labels\n      self._targets[\'inv_rois\'] = inv_rois\n      self._targets[\'inv_batch_ids\'] = inv_batch_ids\n\n      self._score_summaries[0].append(rois)\n      self._score_summaries[0].append(labels)\n      self._score_summaries[0].append(inv_rois)\n\n    return rois, batch_ids, inv_rois, inv_batch_ids\n\n  def _crop_rois(self, bottom, rois, batch_ids, name, iter=0):\n    with tf.variable_scope(name):\n      crops = tf.image.crop_and_resize(bottom, rois, batch_ids, \n                                       [cfg.MEM.CROP_SIZE, cfg.MEM.CROP_SIZE],\n                                       name=""crops"")\n      self._score_summaries[iter].append(crops)\n    return crops\n\n  def _inv_crops(self, pool5, inv_rois, inv_batch_ids, name):\n    with tf.variable_scope(name):\n      inv_crops = tf.image.crop_and_resize(pool5, inv_rois, inv_batch_ids, self._memory_size,\n                                           extrapolation_value=0, # difference is 0 outside\n                                           name=""inv_crops"")\n      # Add things up (make sure it is relu)\n      inv_crop = tf.reduce_sum(inv_crops, axis=0, keep_dims=True, name=""reduce_sum"")\n\n    return inv_crop, inv_crops\n\n  # The initial classes, only use output from the conv features\n  def _cls_init(self, fc7, is_training):\n    initializer = tf.random_normal_initializer(mean=0.0, stddev=0.01)\n    cls_score = slim.fully_connected(fc7, self._num_classes, \n                                       weights_initializer=initializer,\n                                       trainable=is_training,\n                                       activation_fn=None, scope=\'cls_score\')\n    cls_prob = tf.nn.softmax(cls_score, name=""cls_prob"")\n    cls_pred = tf.argmax(cls_score, axis=1, name=""cls_pred"")\n\n    self._score_summaries[0].append(cls_score)\n    self._score_summaries[0].append(cls_pred)\n    self._score_summaries[0].append(cls_prob)\n\n    return cls_score, cls_prob\n\n  def _mem_init(self, is_training, name):\n    mem_initializer = tf.constant_initializer(0.0)\n    # Kinda like bias\n    if cfg.TRAIN.BIAS_DECAY:\n      mem_regularizer = tf.contrib.layers.l2_regularizer(cfg.TRAIN.WEIGHT_DECAY)\n    else:\n      mem_regularizer = tf.no_regularizer\n\n    with tf.variable_scope(\'SMN\'):\n      with tf.variable_scope(name):\n        mem_init = tf.get_variable(\'mem_init\', \n                                   [1, cfg.MEM.INIT_H, cfg.MEM.INIT_W, cfg.MEM.C], \n                                   initializer=mem_initializer, \n                                   trainable=is_training,\n                                   regularizer=mem_regularizer)\n        self._score_summaries[0].append(mem_init)\n        # resize it to the image-specific size\n        mem_init = tf.image.resize_bilinear(mem_init, self._memory_size, \n                                            name=""resize_init"")\n\n    return mem_init\n\n  def _context_conv(self, net, conv, scope):\n    net = slim.conv2d(net, cfg.MEM.C, [conv, conv], scope=scope)\n    return net\n\n  def _context(self, net, is_training, name, iter):\n    num_layers = cfg.MEM.CT_L\n    xavier = tf.contrib.layers.variance_scaling_initializer()\n\n    assert num_layers % 2 == 1\n    conv = cfg.MEM.CT_CONV\n    with tf.variable_scope(name):\n      with slim.arg_scope([slim.conv2d, slim.separable_conv2d], \n                          activation_fn=None, \n                          trainable=is_training,\n                          weights_initializer=xavier,\n                          biases_initializer=tf.constant_initializer(0.0)):\n        net = self._context_conv(net, cfg.MEM.CT_FCONV, ""conv1"")\n        for i in xrange(2, num_layers+1, 2):\n          net1 = tf.nn.relu(net, name=""relu%02d"" % (i-1))\n          self._act_summaries.append(net1)\n          self._score_summaries[iter].append(net1)\n          net1 = self._context_conv(net1, conv, ""conv%02d"" % i)\n          net2 = tf.nn.relu(net1, name=""relu%02d"" % i)\n          self._act_summaries.append(net2)\n          self._score_summaries[iter].append(net2)\n          net2 = self._context_conv(net2, conv, ""conv%02d"" % (i+1))\n          net = tf.add(net, net2, ""residual%02d"" % i)\n\n    return net\n\n  def _fc_iter(self, mem_pool5, is_training, name, iter):\n    xavier = tf.contrib.layers.variance_scaling_initializer()\n    \n    with tf.variable_scope(name):\n      mem_fc7 = slim.flatten(mem_pool5, scope=\'flatten\')\n      with slim.arg_scope([slim.fully_connected], \n                          activation_fn=tf.nn.relu, \n                          trainable=is_training,\n                          weights_initializer=xavier,\n                          biases_initializer=tf.constant_initializer(0.0)):\n        for i in xrange(cfg.MEM.FC_L):\n          mem_fc7 = slim.fully_connected(mem_fc7, \n                                        cfg.MEM.FC_C, \n                                        scope=""mem_fc%d"" % (i+6))\n          self._act_summaries.append(mem_fc7)\n          self._score_summaries[iter].append(mem_fc7)\n\n    return mem_fc7\n\n  def _cls_iter(self, mem_fc7, is_training, name, iter):\n    initializer = tf.random_normal_initializer(mean=0.0, stddev=cfg.MEM.C_STD)\n    with tf.variable_scope(name):\n      cls_score_mem = slim.fully_connected(mem_fc7, self._num_classes, \n                                          weights_initializer=initializer,\n                                          activation_fn=None,\n                                          trainable=is_training,\n                                          biases_initializer=tf.constant_initializer(0.0),\n                                          scope=""cls_score_mem"")\n      self._score_summaries[iter].append(cls_score_mem)\n\n    return cls_score_mem\n\n  def _comb_conv_mem(self, cls_score_conv, cls_score_mem, name, iter):\n    with tf.variable_scope(name):\n      # take the output directly from each iteration\n      if iter == 0:\n        cls_score = cls_score_conv\n      else:\n        cls_score = cls_score_mem\n      cls_prob = tf.nn.softmax(cls_score, name=""cls_prob"")\n      cls_pred = tf.argmax(cls_score, axis=1, name=""cls_pred"")\n\n      self._predictions[\'cls_score\'].append(cls_score)\n      self._predictions[\'cls_pred\'].append(cls_pred)\n      self._predictions[\'cls_prob\'].append(cls_prob)\n\n      self._score_summaries[iter].append(cls_score)\n      self._score_summaries[iter].append(cls_pred)\n      self._score_summaries[iter].append(cls_prob)\n\n    return cls_score, cls_prob, cls_pred\n\n  def _bottomtop(self, pool5, cls_prob, is_training, name, iter):\n    initializer=tf.random_normal_initializer(mean=0.0, stddev=cfg.MEM.STD)\n    initializer_fc=tf.random_normal_initializer(mean=0.0, stddev=cfg.MEM.STD * cfg.MEM.FP_R)\n    with tf.variable_scope(name):\n      with slim.arg_scope([slim.fully_connected, slim.conv2d], \n                          activation_fn=None,\n                          trainable=is_training):\n        # just make the representation more dense\n        map_prob = slim.fully_connected(cls_prob, \n                                        cfg.MEM.C,\n                                        weights_initializer=initializer_fc,\n                                        biases_initializer=None,\n                                        scope=""map_prob"")\n        map_comp = tf.reshape(map_prob, [-1, 1, 1, cfg.MEM.C], name=""map_comp"")\n        pool5_comp = slim.conv2d(pool5, \n                                 cfg.MEM.C, \n                                 [1, 1], \n                                 weights_initializer=initializer,\n                                 biases_initializer=tf.constant_initializer(0.0),\n                                 scope=""pool5_comp"")\n\n        pool5_comb = tf.add(map_comp, pool5_comp, name=""addition"")\n        pool5_comb = tf.nn.relu(pool5_comb, name=""pool5_comb"")\n\n        self._score_summaries[iter].append(map_prob)\n        self._score_summaries[iter].append(pool5_comp)\n        self._score_summaries[iter].append(pool5_comb)\n        self._act_summaries.append(pool5_comb)\n\n    return pool5_comb\n\n  def _bottom(self, pool5, is_training, name, iter):\n    initializer=tf.random_normal_initializer(mean=0.0, stddev=cfg.MEM.STD)\n    with tf.variable_scope(name):\n      with slim.arg_scope([slim.fully_connected, slim.conv2d], \n                          activation_fn=None,\n                          trainable=is_training):\n        # just make the representation more dense\n        pool5_comp = slim.conv2d(pool5, \n                                 cfg.MEM.C, \n                                 [1, 1], \n                                 activation_fn=tf.nn.relu,\n                                 weights_initializer=initializer,\n                                 biases_initializer=tf.constant_initializer(0.0),\n                                 scope=""pool5_comp"")\n        self._score_summaries[iter].append(pool5_comp)\n        self._act_summaries.append(pool5_comp)\n\n    return pool5_comp\n\n  def _topprob(self, cls_prob, is_training, name, iter):\n    initializer_fc=tf.random_normal_initializer(mean=0.0, \n                                                stddev=cfg.MEM.STD * cfg.MEM.FP_R)\n    with tf.variable_scope(name):\n      # just make the representation more dense\n      map_prob = slim.fully_connected(cls_prob, \n                                      cfg.MEM.C,\n                                      activation_fn=tf.nn.relu,\n                                      trainable=is_training,\n                                      weights_initializer=initializer_fc,\n                                      biases_initializer=tf.constant_initializer(0.0),\n                                      scope=""map_prob"")\n      map_comp = tf.reshape(map_prob, [-1, 1, 1, cfg.MEM.C], name=""map_comp"")\n      map_pool = tf.tile(map_comp, [1, cfg.MEM.CROP_SIZE, cfg.MEM.CROP_SIZE, 1], name=""map_pool"")\n      self._score_summaries[iter].append(map_prob)\n      self._act_summaries.append(map_prob)\n\n    return map_pool\n\n  def _toppred(self, cls_pred, is_training, name, iter):\n    initializer_fc=tf.random_normal_initializer(mean=0.0, \n                                                stddev=cfg.MEM.STD * cfg.MEM.FP_R)\n    with tf.variable_scope(name):\n      cls_pred_hot = tf.one_hot(cls_pred, self._num_classes, name=""encode"")\n      # just make the representation more dense\n      map_pred = slim.fully_connected(cls_pred_hot, \n                                      cfg.MEM.C,\n                                      activation_fn=tf.nn.relu,\n                                      trainable=is_training,\n                                      weights_initializer=initializer_fc,\n                                      biases_initializer=tf.constant_initializer(0.0),\n                                      scope=""map_pred"")\n      map_comp = tf.reshape(map_pred, [-1, 1, 1, cfg.MEM.C], name=""map_comp"")\n      map_pool = tf.tile(map_comp, [1, cfg.MEM.CROP_SIZE, cfg.MEM.CROP_SIZE, 1], name=""map_pool"")\n      self._score_summaries[iter].append(map_pred)\n      self._act_summaries.append(map_pred)\n\n    return map_pool\n\n  def _input(self, net, is_training, name, iter):\n    num_layers = cfg.MEM.IN_L\n    in_conv = cfg.MEM.IN_CONV\n    xavier = tf.contrib.layers.variance_scaling_initializer()\n    with tf.variable_scope(name): \n      # the first part is already done\n      for i in xrange(2, num_layers+1):\n        net = slim.conv2d(net, cfg.MEM.C, [in_conv, in_conv], \n                          activation_fn=tf.nn.relu, \n                          trainable=is_training,\n                          weights_initializer=xavier,\n                          biases_initializer=tf.constant_initializer(0.0),\n                          scope=""conv%02d"" % i)\n        self._score_summaries[iter].append(net)\n        self._act_summaries.append(net)\n\n    return net\n\n  def _mem_update(self, pool5_mem, pool5_input, is_training, name, iter):\n    feat_initializer = tf.random_normal_initializer(mean=0.0, \n                          stddev=cfg.MEM.U_STD)\n    mem_initializer = tf.random_normal_initializer(mean=0.0, \n                          stddev=cfg.MEM.U_STD * cfg.MEM.FM_R)\n    feat_gate_initializer = tf.random_normal_initializer(mean=0.0, \n                          stddev=cfg.MEM.U_STD / cfg.MEM.VG_R)\n    mem_gate_initializer = tf.random_normal_initializer(mean=0.0, \n                          stddev=cfg.MEM.U_STD * cfg.MEM.FM_R / cfg.MEM.VG_R)\n    mconv = cfg.MEM.CONV\n    with tf.variable_scope(name):\n      with slim.arg_scope([slim.conv2d], \n                          activation_fn=None, \n                          biases_initializer=tf.constant_initializer(0.0),\n                          trainable=is_training): \n        # compute the gates and features\n        p_input = slim.conv2d(pool5_input, \n                              cfg.MEM.C, \n                              [mconv, mconv], \n                              weights_initializer=feat_initializer,\n                              scope=""input_p"")\n        p_reset = slim.conv2d(pool5_input, \n                              1, \n                              [mconv, mconv],\n                              weights_initializer=feat_gate_initializer,\n                              scope=""reset_p"")\n        p_update = slim.conv2d(pool5_input, \n                              1, \n                              [mconv, mconv],\n                              weights_initializer=feat_gate_initializer,\n                              scope=""update_p"")\n        # compute the gates and features from the hidden memory\n        m_reset = slim.conv2d(pool5_mem, \n                              1,\n                              [mconv, mconv],\n                              weights_initializer=mem_gate_initializer,\n                              biases_initializer=None,\n                              scope=""reset_m"")\n        m_update = slim.conv2d(pool5_mem, \n                              1,\n                              [mconv, mconv], \n                              weights_initializer=mem_gate_initializer,\n                              biases_initializer=None,\n                              scope=""update_m"")\n        # get the reset gate, the portion that is kept from the previous step\n        reset_gate = tf.sigmoid(p_reset + m_reset, name=""reset_gate"")\n        reset_res = tf.multiply(pool5_mem, reset_gate, name=""m_input_reset"")\n        m_input = slim.conv2d(reset_res, \n                              cfg.MEM.C,\n                              [mconv, mconv], \n                              weights_initializer=mem_initializer,\n                              biases_initializer=None,\n                              scope=""input_m"")\n        # Non-linear activation\n        pool5_new = tf.nn.relu(p_input + m_input, name=""pool5_new"")\n        # get the update gate, the portion that is taken to update the new memory\n        update_gate = tf.sigmoid(p_update + m_update, name=""update_gate"")\n        # the update is done in a difference manner\n        mem_diff = tf.multiply(update_gate, pool5_new - pool5_mem, \n                              name=""mem_diff"") \n\n      self._score_summaries[iter].append(p_reset)\n      self._score_summaries[iter].append(p_update)\n      self._score_summaries[iter].append(m_reset)\n      self._score_summaries[iter].append(m_update)\n      self._score_summaries[iter].append(reset_gate)\n      self._score_summaries[iter].append(update_gate)\n      self._score_summaries[iter].append(mem_diff)\n\n    return mem_diff\n\n  def _input_module(self, pool5_nb, \n                    cls_score_nb, cls_prob_nb, cls_pred_nb, \n                    is_training, iter):\n    pool5_comb = self._bottomtop(pool5_nb, cls_score_nb, is_training, ""bottom_top"", iter)\n    pool5_input = self._input(pool5_comb, is_training, ""pool5_input"", iter)\n\n    return pool5_input\n\n  def _build_conv(self, is_training):\n    # Get the head\n    net_conv = self._image_to_head(is_training)\n    with tf.variable_scope(self._scope, self._scope):\n      # get the region of interest\n      rois, batch_ids, inv_rois, inv_batch_ids = self._target_memory_layer(""target"")\n      # region of interest pooling\n      pool5 = self._crop_rois(net_conv, rois, batch_ids, ""pool5"")\n      pool5_nb = tf.stop_gradient(pool5, name=""pool5_nb"")\n\n    # initialize the normalization vector, note here it is the batch ids\n    count_matrix_raw, self._count_crops = self._inv_crops(self._count_base, inv_rois, batch_ids, ""count_matrix"")\n    self._count_matrix = tf.stop_gradient(count_matrix_raw, name=\'cm_nb\')\n    self._count_matrix_eps = tf.maximum(self._count_matrix, cfg.EPS, name=\'count_eps\')\n    self._score_summaries[0].append(self._count_matrix)\n\n    fc7 = self._head_to_tail(pool5, is_training)\n    # First iteration\n    with tf.variable_scope(self._scope, self._scope):\n      # region classification\n      cls_score_conv, cls_prob_conv = self._cls_init(fc7, is_training)\n\n    return cls_score_conv, pool5_nb, \\\n           rois, batch_ids, inv_rois, inv_batch_ids\n\n  def _build_pred(self, is_training, mem, cls_score_conv, rois, batch_ids, iter):\n    if cfg.MEM.CT_L:\n      mem_net = self._context(mem, is_training, ""context"", iter)\n    else:\n      mem_net = mem\n    mem_ct_pool5 = self._crop_rois(mem_net, rois, batch_ids, ""mem_ct_pool5"", iter)\n    mem_fc7 = self._fc_iter(mem_ct_pool5, is_training, ""fc7"", iter) \n    cls_score_mem = self._cls_iter(mem_fc7, is_training, ""cls_iter"", iter) \n    cls_score, cls_prob, cls_pred = self._comb_conv_mem(cls_score_conv, cls_score_mem, \n                                                        ""comb_conv_mem"", iter)\n\n    return cls_score, cls_prob, cls_pred\n\n  def _build_update(self, is_training, mem, pool5_nb, cls_score, cls_prob, cls_pred,\n                    rois, batch_ids, inv_rois, inv_batch_ids, iter):\n    cls_score_nb = tf.stop_gradient(cls_score, name=""cls_score_nb"")\n    cls_prob_nb = tf.stop_gradient(cls_prob, name=""cls_prob_nb"")\n    cls_pred_nb = tf.stop_gradient(cls_pred, name=""cls_pred_nb"")\n    pool5_mem = self._crop_rois(mem, rois, batch_ids, ""pool5_mem"", iter)\n    pool5_input = self._input_module(pool5_nb, \n                                    cls_score_nb, cls_prob_nb, \n                                    cls_pred_nb, is_training, iter)\n    mem_update = self._mem_update(pool5_mem, pool5_input, is_training, ""mem_update"", iter) \n    mem_diff, _ = self._inv_crops(mem_update, inv_rois, inv_batch_ids, ""inv_crop"")\n    self._score_summaries[iter].append(mem_diff)\n    # Update the memory\n    mem_div = tf.div(mem_diff, self._count_matrix_eps, name=""div"")\n    mem = tf.add(mem, mem_div, name=""add"")\n    self._score_summaries[iter].append(mem)\n\n    return mem\n\n  def _build_memory(self, is_training, is_testing):\n    # initialize memory\n    mem = self._mem_init(is_training, ""mem_init"")\n    # convolution related stuff\n    cls_score_conv, pool5_nb, \\\n    rois, batch_ids, inv_rois, inv_batch_ids = self._build_conv(is_training)\n    # Separate first prediction\n    reuse = None\n    # Memory iterations\n    for iter in xrange(cfg.MEM.ITER):\n      print(\'ITERATION: %02d\' % iter)\n      self._mems.append(mem)\n      with tf.variable_scope(\'SMN\', reuse=reuse):\n        # Use memory to predict the output\n        cls_score, cls_prob, cls_pred = self._build_pred(is_training, \n                                                         mem, \n                                                         cls_score_conv, \n                                                         rois, batch_ids, iter)\n        if iter == cfg.MEM.ITER - 1:\n          break\n        # Update the memory with all the regions\n        mem = self._build_update(is_training, mem, \n                                pool5_nb, cls_score, cls_prob, cls_pred,\n                                rois, batch_ids, inv_rois, inv_batch_ids, \n                                iter)\n        \n      if iter == 0:\n        reuse = True\n\n    return rois, cls_prob\n\n  def _add_memory_losses(self, name):\n    cross_entropy = []\n    assert len(self._predictions[""cls_score""]) == cfg.MEM.ITER\n    with tf.variable_scope(name):\n      label = tf.reshape(self._targets[""labels""], [-1])\n      for iter in xrange(cfg.MEM.ITER):\n        # RCNN, class loss\n        cls_score = self._predictions[""cls_score""][iter]\n        ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=cls_score, \n                                                                           labels=label))\n        cross_entropy.append(ce)\n\n      ce_rest = tf.stack(cross_entropy[1:], name=""cross_entropy_rest"")\n      self._losses[\'cross_entropy_image\'] = cross_entropy[0]\n      self._losses[\'cross_entropy_memory\'] = tf.reduce_mean(ce_rest, name=\'cross_entropy\')\n      self._losses[\'cross_entropy\'] = self._losses[\'cross_entropy_image\'] \\\n                                    + cfg.MEM.WEIGHT * self._losses[\'cross_entropy_memory\']\n\n      loss = self._losses[\'cross_entropy\']\n      regularization_loss = tf.add_n(tf.losses.get_regularization_losses(), \'regu\')\n      self._losses[\'total_loss\'] = loss + regularization_loss\n\n      self._event_summaries.update(self._losses)\n\n    return loss\n\n  def _create_summary(self):\n    val_summaries = []\n    with tf.device(""/cpu:0""):\n      val_summaries.append(self._add_gt_image_summary())\n      for iter in xrange(cfg.MEM.ITER):\n        val_summaries.append(self._add_pred_memory_summary(iter))\n        val_summaries.append(self._add_memory_summary(iter))\n        for var in self._score_summaries[iter]:\n          self._add_score_iter_summary(iter, var)\n      for key, var in self._event_summaries.items():\n        val_summaries.append(tf.summary.scalar(key, var))\n      for var in self._act_summaries:\n        self._add_zero_summary(var)\n\n    self._summary_op = tf.summary.merge_all()\n    self._summary_op_val = tf.summary.merge(val_summaries)\n\n  def create_architecture(self, mode, num_classes, tag=None):\n    self._image = tf.placeholder(tf.float32, shape=[1, None, None, 3])\n    self._im_info = tf.placeholder(tf.float32, shape=[3])\n    self._memory_size = tf.placeholder(tf.int32, shape=[2])\n    self._gt_boxes = tf.placeholder(tf.float32, shape=[None, 5])\n    self._count_base = tf.ones([1, cfg.MEM.CROP_SIZE, cfg.MEM.CROP_SIZE, 1])\n    self._num_gt = tf.placeholder(tf.int32, shape=[])\n    self._tag = tag\n\n    self._num_classes = num_classes\n    self._mode = mode\n\n    training = mode == \'TRAIN\'\n    testing = mode == \'TEST\'\n\n    assert tag is not None\n\n    # handle most of the regularizers here\n    weights_regularizer = tf.contrib.layers.l2_regularizer(cfg.TRAIN.WEIGHT_DECAY)\n    if cfg.TRAIN.BIAS_DECAY:\n      biases_regularizer = weights_regularizer\n    else:\n      biases_regularizer = tf.no_regularizer\n\n    # list as many types of layers as possible, even if they are not used now\n    with slim.arg_scope([slim.conv2d, slim.conv2d_in_plane, \\\n                        slim.conv2d_transpose, slim.separable_conv2d, slim.fully_connected], \n                        weights_regularizer=weights_regularizer,\n                        biases_regularizer=biases_regularizer, \n                        biases_initializer=tf.constant_initializer(0.0)): \n      rois = self._build_memory(training, testing)\n\n    layers_to_output = {\'rois\': rois}\n\n    if not testing:\n      self._add_memory_losses(""loss"")\n      layers_to_output.update(self._losses)\n      self._create_summary()\n\n    layers_to_output.update(self._predictions)\n\n    return layers_to_output\n\n  # take the last predicted output\n  def test_image(self, sess, blobs):\n    cls_score, cls_prob = sess.run([self._predictions[""cls_score""][-1],\n                                    self._predictions[\'cls_prob\'][-1]],\n                                   feed_dict=self._parse_dict(blobs))\n    return cls_score, cls_prob\n\n  # Test the base output\n  def test_image_iter(self, sess, blobs, iter):\n    cls_score, cls_prob = sess.run([self._predictions[""cls_score""][iter],\n                                    self._predictions[\'cls_prob\'][iter]],\n                                   feed_dict=self._parse_dict(blobs))\n    return cls_score, cls_prob\n\nclass vgg16_memory(BaseMemory, vgg16):\n  def __init__(self):\n    BaseMemory.__init__(self)\n    self._feat_stride = [16, ]\n    self._feat_compress = [1. / float(self._feat_stride[0]), ]\n    self._scope = \'vgg_16\'\n\nclass resnetv1_memory(BaseMemory, resnetv1):\n  def __init__(self, num_layers=50):\n    BaseMemory.__init__(self)\n    self._feat_stride = [16, ]\n    self._feat_compress = [1. / float(self._feat_stride[0]), ]\n    self._num_layers = num_layers\n    self._scope = \'resnet_v1_%d\' % num_layers\n    resnetv1._decide_blocks(self)\n\nclass mobilenetv1_memory(BaseMemory, mobilenetv1):\n  def __init__(self):\n    BaseMemory.__init__(self)\n    self._feat_stride = [16, ]\n    self._feat_compress = [1. / float(self._feat_stride[0]), ]\n    self._depth_multiplier = cfg.MOBILENET.DEPTH_MULTIPLIER\n    self._scope = \'MobilenetV1\'\n'"
lib/nets/mobilenet_v1.py,12,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.contrib.slim.python.slim.nets import resnet_utils\nimport numpy as np\nfrom collections import namedtuple\n\nfrom nets.network import Network\nfrom model.config import cfg\n\ndef separable_conv2d_same(inputs, kernel_size, stride, rate=1, scope=None):\n  """"""Strided 2-D separable convolution with \'SAME\' padding.\n  Args:\n    inputs: A 4-D tensor of size [batch, height_in, width_in, channels].\n    kernel_size: An int with the kernel_size of the filters.\n    stride: An integer, the output stride.\n    rate: An integer, rate for atrous convolution.\n    scope: Scope.\n  Returns:\n    output: A 4-D tensor of size [batch, height_out, width_out, channels] with\n      the convolution output.\n  """"""\n\n  # By passing filters=None\n  # separable_conv2d produces only a depth-wise convolution layer\n  if stride == 1:\n    return slim.separable_conv2d(inputs, None, kernel_size, \n                                  depth_multiplier=1, stride=1, rate=rate,\n                                  padding=\'SAME\', scope=scope)\n  else:\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    inputs = tf.pad(inputs,\n                    [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n    return slim.separable_conv2d(inputs, None, kernel_size, \n                                  depth_multiplier=1, stride=stride, rate=rate, \n                                  padding=\'VALID\', scope=scope)\n\n# The following is adapted from:\n# https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.py\n\n# Conv and DepthSepConv named tuple define layers of the MobileNet architecture\n# Conv defines 3x3 convolution layers\n# DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.\n# stride is the stride of the convolution\n# depth is the number of channels or filters in a layer\nConv = namedtuple(\'Conv\', [\'kernel\', \'stride\', \'depth\'])\nDepthSepConv = namedtuple(\'DepthSepConv\', [\'kernel\', \'stride\', \'depth\'])\n\n# _CONV_DEFS specifies the MobileNet body\n_CONV_DEFS = [\n    Conv(kernel=3, stride=2, depth=32),\n    DepthSepConv(kernel=3, stride=1, depth=64),\n    DepthSepConv(kernel=3, stride=2, depth=128),\n    DepthSepConv(kernel=3, stride=1, depth=128),\n    DepthSepConv(kernel=3, stride=2, depth=256),\n    DepthSepConv(kernel=3, stride=1, depth=256),\n    DepthSepConv(kernel=3, stride=2, depth=512),\n    DepthSepConv(kernel=3, stride=1, depth=512),\n    DepthSepConv(kernel=3, stride=1, depth=512),\n    DepthSepConv(kernel=3, stride=1, depth=512),\n    DepthSepConv(kernel=3, stride=1, depth=512),\n    DepthSepConv(kernel=3, stride=1, depth=512),\n    # use stride 1 for the 13th layer\n    DepthSepConv(kernel=3, stride=1, depth=1024),\n    DepthSepConv(kernel=3, stride=1, depth=1024)\n]\n\n# Modified mobilenet_v1\ndef mobilenet_v1_base(inputs,\n                      conv_defs,\n                      starting_layer=0,\n                      min_depth=8,\n                      depth_multiplier=1.0,\n                      output_stride=None,\n                      reuse=None,\n                      scope=None):\n  """"""Mobilenet v1.\n  Constructs a Mobilenet v1 network from inputs to the given final endpoint.\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    starting_layer: specifies the current starting layer. For region proposal \n      network it is 0, for region classification it is 12 by default.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    conv_defs: A list of ConvDef named tuples specifying the net architecture.\n    output_stride: An integer that specifies the requested ratio of input to\n      output spatial resolution. If not None, then we invoke atrous convolution\n      if necessary to prevent the network from reducing the spatial resolution\n      of the activation maps. \n    scope: Optional variable_scope.\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n  Raises:\n    ValueError: if depth_multiplier <= 0, or convolution type is not defined.\n  """"""\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n  end_points = {}\n\n  # Used to find thinned depths for each layer.\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n\n  with tf.variable_scope(scope, \'MobilenetV1\', [inputs], reuse=reuse):\n    # The current_stride variable keeps track of the output stride of the\n    # activations, i.e., the running product of convolution strides up to the\n    # current network layer. This allows us to invoke atrous convolution\n    # whenever applying the next convolution would result in the activations\n    # having output stride larger than the target output_stride.\n    current_stride = 1\n\n    # The atrous convolution rate parameter.\n    rate = 1\n\n    net = inputs\n    for i, conv_def in enumerate(conv_defs):\n      end_point_base = \'Conv2d_%d\' % (i + starting_layer)\n\n      if output_stride is not None and current_stride == output_stride:\n        # If we have reached the target output_stride, then we need to employ\n        # atrous convolution with stride=1 and multiply the atrous rate by the\n        # current unit\'s stride for use in subsequent layers.\n        layer_stride = 1\n        layer_rate = rate\n        rate *= conv_def.stride\n      else:\n        layer_stride = conv_def.stride\n        layer_rate = 1\n        current_stride *= conv_def.stride\n\n      if isinstance(conv_def, Conv):\n        end_point = end_point_base\n        net = resnet_utils.conv2d_same(net, depth(conv_def.depth), conv_def.kernel,\n                          stride=conv_def.stride,\n                          scope=end_point)\n\n      elif isinstance(conv_def, DepthSepConv):\n        end_point = end_point_base + \'_depthwise\'\n        \n        net = separable_conv2d_same(net, conv_def.kernel,\n                                    stride=layer_stride,\n                                    rate=layer_rate,\n                                    scope=end_point)\n\n        end_point = end_point_base + \'_pointwise\'\n\n        net = slim.conv2d(net, depth(conv_def.depth), [1, 1],\n                          stride=1,\n                          scope=end_point)\n\n      else:\n        raise ValueError(\'Unknown convolution type %s for layer %d\'\n                         % (conv_def.ltype, i))\n\n    return net\n\n# Modified arg_scope to incorporate configs\ndef mobilenet_v1_arg_scope(is_training=True,\n                           stddev=0.09):\n  batch_norm_params = {\n      \'is_training\': False,\n      \'center\': True,\n      \'scale\': True,\n      \'decay\': 0.9997,\n      \'epsilon\': 0.001,\n      \'trainable\': False,\n  }\n\n  # Set weight_decay for weights in Conv and DepthSepConv layers.\n  weights_init = tf.truncated_normal_initializer(stddev=stddev)\n  regularizer = tf.contrib.layers.l2_regularizer(cfg.MOBILENET.WEIGHT_DECAY)\n  if cfg.MOBILENET.REGU_DEPTH:\n    depthwise_regularizer = regularizer\n  else:\n    depthwise_regularizer = None\n\n  with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                      trainable=is_training,\n                      weights_initializer=weights_init,\n                      activation_fn=tf.nn.relu6, \n                      normalizer_fn=slim.batch_norm,\n                      padding=\'SAME\'):\n    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n      with slim.arg_scope([slim.conv2d], weights_regularizer=regularizer):\n        with slim.arg_scope([slim.separable_conv2d],\n                            weights_regularizer=depthwise_regularizer) as sc:\n          return sc\n\nclass mobilenetv1(Network):\n  def __init__(self):\n    Network.__init__(self)\n    self._feat_stride = [16, ]\n    self._feat_compress = [1. / float(self._feat_stride[0]), ]\n    self._depth_multiplier = cfg.MOBILENET.DEPTH_MULTIPLIER\n    self._scope = \'MobilenetV1\'\n\n  def _image_to_head(self, is_training, reuse=None):\n    # Base bottleneck\n    assert (0 <= cfg.MOBILENET.FIXED_LAYERS <= 12)\n    net_conv = self._image\n    if cfg.MOBILENET.FIXED_LAYERS > 0:\n      with slim.arg_scope(mobilenet_v1_arg_scope(is_training=False)):\n        net_conv = mobilenet_v1_base(net_conv,\n                                      _CONV_DEFS[:cfg.MOBILENET.FIXED_LAYERS],\n                                      starting_layer=0,\n                                      depth_multiplier=self._depth_multiplier,\n                                      reuse=reuse,\n                                      scope=self._scope)\n    if cfg.MOBILENET.FIXED_LAYERS < 12:\n      with slim.arg_scope(mobilenet_v1_arg_scope(is_training=is_training)):\n        net_conv = mobilenet_v1_base(net_conv,\n                                      _CONV_DEFS[cfg.MOBILENET.FIXED_LAYERS:12],\n                                      starting_layer=cfg.MOBILENET.FIXED_LAYERS,\n                                      depth_multiplier=self._depth_multiplier,\n                                      reuse=reuse,\n                                      scope=self._scope)\n\n    self._act_summaries.append(net_conv)\n    self._layers[\'head\'] = net_conv\n\n    return net_conv\n\n  def _head_to_tail(self, pool5, is_training, reuse=None):\n    with slim.arg_scope(mobilenet_v1_arg_scope(is_training=is_training)):\n      fc7 = mobilenet_v1_base(pool5,\n                              _CONV_DEFS[12:],\n                              starting_layer=12,\n                              depth_multiplier=self._depth_multiplier,\n                              reuse=reuse,\n                              scope=self._scope)\n      # average pooling done by reduce_mean\n      fc7 = tf.reduce_mean(fc7, axis=[1, 2])\n    return fc7\n\n  def get_variables_to_restore(self, variables, var_keep_dic):\n    variables_to_restore = []\n\n    for v in variables:\n      # exclude the first conv layer to swap RGB to BGR\n      if v.name == (self._scope + \'/Conv2d_0/weights:0\'):\n        self._variables_to_fix[v.name] = v\n        continue\n      if v.name.split(\':\')[0] in var_keep_dic:\n        print(\'Variables restored: %s\' % v.name)\n        variables_to_restore.append(v)\n\n    return variables_to_restore\n\n  def fix_variables(self, sess, pretrained_model):\n    print(\'Fix MobileNet V1 layers..\')\n    with tf.variable_scope(\'Fix_MobileNet_V1\'):\n      with tf.device(""/cpu:0""):\n        # fix RGB to BGR, and match the scale by (255.0 / 2.0)\n        Conv2d_0_rgb = tf.get_variable(""Conv2d_0_rgb"", \n                                    [3, 3, 3, max(int(32 * self._depth_multiplier), 8)], \n                                    trainable=False)\n        restorer_fc = tf.train.Saver({self._scope + ""/Conv2d_0/weights"": Conv2d_0_rgb})\n        restorer_fc.restore(sess, pretrained_model)\n\n        sess.run(tf.assign(self._variables_to_fix[self._scope + ""/Conv2d_0/weights:0""], \n                           tf.reverse(Conv2d_0_rgb / (255.0 / 2.0), [2])))\n'"
lib/nets/network.py,42,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\nimport numpy as np\n\nfrom model.config import cfg\nfrom utils.snippets import compute_target\nfrom utils.visualization import draw_gt_boxes, draw_predicted_boxes\n\nclass Network(object):\n  def __init__(self):\n    self._predictions = {}\n    self._losses = {}\n    self._targets = {}\n    self._layers = {}\n    self._gt_image = None\n    self._act_summaries = []\n    self._score_summaries = {}\n    self._event_summaries = {}\n    self._variables_to_fix = {}\n\n  def _add_gt_image(self):\n    # add back mean\n    image = self._image + cfg.PIXEL_MEANS\n    # BGR to RGB (opencv uses BGR)\n    self._gt_image = tf.reverse(image, axis=[-1])\n\n  def _add_org_image_summary(self):\n    if self._gt_image is None:\n      self._add_gt_image()\n    \n    return tf.summary.image(\'ORG\', self._gt_image)\n\n  def _add_gt_image_summary(self):\n    # use a customized visualization function to visualize the boxes\n    if self._gt_image is None:\n      self._add_gt_image()\n    image = tf.py_func(draw_gt_boxes, \n                      [self._gt_image, self._gt_boxes],\n                      tf.float32, name=""gt_boxes"")\n    \n    return tf.summary.image(\'GROUND_TRUTH\', image)\n\n  def _add_pred_summary(self):\n    # also visualize the predictions of the network\n    if self._gt_image is None:\n      self._add_gt_image()\n    image = tf.py_func(draw_predicted_boxes,\n                       [self._gt_image,\n                       self._predictions[\'cls_prob\'],\n                       self._gt_boxes],\n                       tf.float32, name=""pred_boxes"")\n    return tf.summary.image(\'PRED\', image)\n\n  def _add_zero_summary(self, tensor):\n    tf.summary.scalar(\'ACT/\' + tensor.op.name + \'/zero_fraction\',\n                      tf.nn.zero_fraction(tensor))\n\n  def _add_act_summary(self, tensor):\n    tf.summary.histogram(\'ACT/\' + tensor.op.name + \'/activations\', tensor)\n    tf.summary.scalar(\'ACT/\' + tensor.op.name + \'/zero_fraction\',\n                      tf.nn.zero_fraction(tensor))\n\n  def _add_score_summary(self, key, tensor):\n    tf.summary.histogram(\'SCORE/\' + tensor.op.name + \'/\' + key + \'/scores\', tensor)\n\n  def _add_train_summary(self, var):\n    tf.summary.histogram(\'TRAIN/\' + var.op.name, var)\n\n  def _crop_pool_layer(self, bottom, rois, batch_ids, name):\n    with tf.variable_scope(name):\n      pre_pool_size = cfg.POOLING_SIZE * 2\n      crops = tf.image.crop_and_resize(bottom, \n                                       rois, \n                                       batch_ids, \n                                       [pre_pool_size, pre_pool_size], \n                                       name=""crops"")\n\n    return slim.max_pool2d(crops, [2, 2], padding=\'SAME\')\n\n  def _dropout_layer(self, bottom, name, ratio=0.5):\n    return tf.nn.dropout(bottom, ratio, name=name)\n\n  def _target_layer(self, name):\n    with tf.variable_scope(name):\n      rois, batch_ids, roi_overlaps, labels = tf.py_func(compute_target,\n                                                        [self._memory_size, self._gt_boxes, self._feat_stride[0]],\n                                                        [tf.float32, tf.int32, tf.float32, tf.int32],\n                                                        name=""target_layer"")\n\n      rois.set_shape([None, 4])\n      labels.set_shape([None, 1])\n\n      self._targets[\'rois\'] = rois\n      self._targets[\'roi_overlaps\'] = roi_overlaps\n      self._targets[\'batch_ids\'] = batch_ids\n      self._targets[\'labels\'] = labels\n\n      self._score_summaries.update(self._targets)\n\n      return rois, batch_ids\n\n  def _region_classification(self, fc7, is_training, initializer):\n    cls_score = slim.fully_connected(fc7, self._num_classes, \n                                       weights_initializer=initializer,\n                                       trainable=is_training,\n                                       activation_fn=None, \n                                       scope=\'cls_score\')\n    cls_prob = tf.nn.softmax(cls_score, name=""cls_prob"")\n    cls_pred = tf.argmax(cls_score, axis=1, name=""cls_pred"")\n\n    self._predictions[""cls_score""] = cls_score\n    self._predictions[""cls_pred""] = cls_pred\n    self._predictions[""cls_prob""] = cls_prob\n\n    return cls_prob\n\n  def _image_to_head(self, is_training, reuse=None):\n    raise NotImplementedError\n\n  def _head_to_tail(self, pool5, is_training, reuse=None):\n    raise NotImplementedError\n\n  def _build_network(self, is_training=True):\n    # select initializers\n    initializer = tf.random_normal_initializer(mean=0.0, stddev=0.01)\n\n    net_conv = self._image_to_head(is_training)\n    with tf.variable_scope(self._scope, self._scope):\n      # get the region of interest\n      rois, batch_ids = self._target_layer(""target"")\n      # region of interest pooling\n      pool5 = self._crop_pool_layer(net_conv, rois, batch_ids, ""pool5"")\n\n    fc7 = self._head_to_tail(pool5, is_training)\n    with tf.variable_scope(self._scope, self._scope):\n      # region classification\n      cls_prob = self._region_classification(fc7, is_training, initializer)\n\n    self._score_summaries.update(self._predictions)\n\n    return rois, cls_prob\n\n  def _add_losses(self):\n    with tf.variable_scope(\'loss\') as scope:\n      # RCNN, class loss\n      cls_score = self._predictions[""cls_score""]\n      label = tf.reshape(self._targets[""labels""], [-1])\n      cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=cls_score, labels=label))\n\n      self._losses[\'cross_entropy\'] = cross_entropy\n      loss = cross_entropy\n      regularization_loss = tf.add_n(tf.losses.get_regularization_losses(), \'regu\')\n      self._losses[\'total_loss\'] = loss + regularization_loss\n\n      self._event_summaries.update(self._losses)\n\n    return loss\n\n  def create_architecture(self, mode, num_classes, tag=None):\n    self._image = tf.placeholder(tf.float32, shape=[1, None, None, 3])\n    self._im_info = tf.placeholder(tf.float32, shape=[3])\n    self._memory_size = tf.placeholder(tf.int32, shape=[2])\n    self._gt_boxes = tf.placeholder(tf.float32, shape=[None, 5])\n    self._num_gt = tf.placeholder(tf.int32, shape=[])\n    self._tag = tag\n\n    self._num_classes = num_classes\n    self._mode = mode\n\n    training = mode == \'TRAIN\'\n    testing = mode == \'TEST\'\n\n    assert tag is not None\n\n    # handle most of the regularizers here\n    weights_regularizer = tf.contrib.layers.l2_regularizer(cfg.TRAIN.WEIGHT_DECAY)\n    if cfg.TRAIN.BIAS_DECAY:\n      biases_regularizer = weights_regularizer\n    else:\n      biases_regularizer = tf.no_regularizer\n\n    # list as many types of layers as possible, even if they are not used now\n    with slim.arg_scope([slim.conv2d, slim.conv2d_in_plane, \\\n                    slim.conv2d_transpose, slim.separable_conv2d, slim.fully_connected], \n                    weights_regularizer=weights_regularizer,\n                    biases_regularizer=biases_regularizer, \n                    biases_initializer=tf.constant_initializer(0.0)): \n      rois, cls_prob = self._build_network(training)\n\n    layers_to_output = {\'rois\': rois}\n\n    if not testing:\n      self._add_losses()\n      layers_to_output.update(self._losses)\n      val_summaries = []\n      with tf.device(""/cpu:0""):\n        val_summaries.append(self._add_gt_image_summary())\n        val_summaries.append(self._add_pred_summary())\n        for key, var in self._event_summaries.items():\n          val_summaries.append(tf.summary.scalar(key, var))\n        for key, var in self._score_summaries.items():\n          self._add_score_summary(key, var)\n        for var in self._act_summaries:\n          self._add_act_summary(var)\n\n      self._summary_op = tf.summary.merge_all()\n      self._summary_op_val = tf.summary.merge(val_summaries)\n\n    layers_to_output.update(self._predictions)\n\n    return layers_to_output\n\n  def get_variables_to_restore(self, variables, var_keep_dic):\n    raise NotImplementedError\n\n  def fix_variables(self, sess, pretrained_model):\n    raise NotImplementedError\n\n  # Extract the head feature maps, for example for vgg16 it is conv5_3\n  # only useful during testing mode\n  def extract_head(self, sess, image):\n    feed_dict = {self._image: image}\n    feat = sess.run(self._layers[""head""], feed_dict=feed_dict)\n    return feat\n\n  def _parse_dict(self, blobs):\n    feed_dict = {self._image: blobs[\'data\'], self._im_info: blobs[\'im_info\'],\n                 self._gt_boxes: blobs[\'gt_boxes\'], \n                 self._memory_size: blobs[\'memory_size\'],\n                 self._num_gt: blobs[\'num_gt\']}\n    return feed_dict\n\n  # only useful during testing mode\n  def test_image(self, sess, blobs):\n    cls_score, cls_prob = sess.run([self._predictions[""cls_score""],\n                                    self._predictions[\'cls_prob\']],\n                                   feed_dict=self._parse_dict(blobs))\n    return cls_score, cls_prob\n\n  def get_summary(self, sess, blobs):\n    summary = sess.run(self._summary_op_val, feed_dict=self._parse_dict(blobs))\n    return summary\n\n  def train_step(self, sess, blobs, train_op):\n    loss_cls, loss, _ = sess.run([self._losses[\'cross_entropy\'],\n                                  self._losses[\'total_loss\'],\n                                  train_op],\n                                 feed_dict=self._parse_dict(blobs))\n    return loss_cls, loss\n\n  def train_step_with_summary(self, sess, blobs, train_op, summary_grads):\n    loss_cls, loss, summary, gsummary, _ = sess.run([self._losses[\'cross_entropy\'],\n                                           self._losses[\'total_loss\'],\n                                           self._summary_op,\n                                           summary_grads,\n                                           train_op],\n                                          feed_dict=self._parse_dict(blobs))\n    return loss_cls, loss, summary, gsummary\n\n  def train_step_no_return(self, sess, blobs, train_op):\n    sess.run([train_op], feed_dict=self._parse_dict(blobs))\n\n  # Empty function here, just in case\n  def fixed_parameters(self, sess):\n    return\n\n'"
lib/nets/resnet_v1.py,14,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.contrib.slim.python.slim.nets import resnet_utils\nfrom tensorflow.contrib.slim.python.slim.nets import resnet_v1\nfrom tensorflow.contrib.slim.python.slim.nets.resnet_v1 import resnet_v1_block\nimport numpy as np\n\nfrom nets.network import Network\nfrom model.config import cfg\n\ndef resnet_arg_scope(is_training=True,\n                     batch_norm_decay=0.997,\n                     batch_norm_epsilon=1e-5,\n                     batch_norm_scale=True):\n  batch_norm_params = {\n    \'is_training\': False,\n    \'decay\': batch_norm_decay,\n    \'epsilon\': batch_norm_epsilon,\n    \'scale\': batch_norm_scale,\n    \'trainable\': False,\n    \'updates_collections\': tf.GraphKeys.UPDATE_OPS\n  }\n\n  with slim.arg_scope(\n      [slim.conv2d],\n      weights_regularizer=slim.l2_regularizer(cfg.TRAIN.WEIGHT_DECAY),\n      weights_initializer=slim.variance_scaling_initializer(),\n      trainable=is_training,\n      activation_fn=tf.nn.relu,\n      normalizer_fn=slim.batch_norm,\n      normalizer_params=batch_norm_params):\n    with slim.arg_scope([slim.batch_norm], **batch_norm_params) as arg_sc:\n      return arg_sc\n\nclass resnetv1(Network):\n  def __init__(self, num_layers=50):\n    Network.__init__(self)\n    self._feat_stride = [16, ]\n    self._feat_compress = [1. / float(self._feat_stride[0]), ]\n    self._num_layers = num_layers\n    self._scope = \'resnet_v1_%d\' % num_layers\n    self._decide_blocks()\n\n  def _crop_pool_layer(self, bottom, rois, batch_ids, name):\n    with tf.variable_scope(name):\n      if cfg.RESNET.MAX_POOL:\n        pre_pool_size = cfg.POOLING_SIZE * 2\n        crops = tf.image.crop_and_resize(bottom, rois, batch_ids, [pre_pool_size, pre_pool_size],\n                                         name=""crops"")\n        crops = slim.max_pool2d(crops, [2, 2], padding=\'SAME\')\n      else:\n        crops = tf.image.crop_and_resize(bottom, rois, batch_ids, [cfg.POOLING_SIZE, cfg.POOLING_SIZE],\n                                         name=""crops"")\n    return crops\n\n  # Do the first few layers manually, because \'SAME\' padding can behave inconsistently\n  # for images of different sizes: sometimes 0, sometimes 1\n  def _build_base(self):\n    with tf.variable_scope(self._scope, self._scope):\n      net = resnet_utils.conv2d_same(self._image, 64, 7, stride=2, scope=\'conv1\')\n      net = tf.pad(net, [[0, 0], [1, 1], [1, 1], [0, 0]])\n      net = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\', scope=\'pool1\')\n\n    return net\n\n  def _image_to_head(self, is_training, reuse=None):\n    assert (0 <= cfg.RESNET.FIXED_BLOCKS <= 3)\n    # Now the base is always fixed during training\n    with slim.arg_scope(resnet_arg_scope(is_training=False)):\n      net_conv = self._build_base()\n    if cfg.RESNET.FIXED_BLOCKS > 0:\n      with slim.arg_scope(resnet_arg_scope(is_training=False)):\n        net_conv, _ = resnet_v1.resnet_v1(net_conv,\n                                           self._blocks[0:cfg.RESNET.FIXED_BLOCKS],\n                                           global_pool=False,\n                                           include_root_block=False,\n                                           reuse=reuse,\n                                           scope=self._scope)\n    if cfg.RESNET.FIXED_BLOCKS < 3:\n      with slim.arg_scope(resnet_arg_scope(is_training=is_training)):\n        net_conv, _ = resnet_v1.resnet_v1(net_conv,\n                                           self._blocks[cfg.RESNET.FIXED_BLOCKS:-1],\n                                           global_pool=False,\n                                           include_root_block=False,\n                                           reuse=reuse,\n                                           scope=self._scope)\n\n    self._act_summaries.append(net_conv)\n    self._layers[\'head\'] = net_conv\n\n    return net_conv\n\n  def _head_to_tail(self, pool5, is_training, reuse=None):\n    with slim.arg_scope(resnet_arg_scope(is_training=is_training)):\n      fc7, _ = resnet_v1.resnet_v1(pool5,\n                                   self._blocks[-1:],\n                                   global_pool=False,\n                                   include_root_block=False,\n                                   reuse=reuse,\n                                   scope=self._scope)\n      # average pooling done by reduce_mean\n      fc7 = tf.reduce_mean(fc7, axis=[1, 2])\n    return fc7\n\n  def _decide_blocks(self):\n    # choose different blocks for different number of layers\n    if self._num_layers == 50:\n      self._blocks = [resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n                      resnet_v1_block(\'block2\', base_depth=128, num_units=4, stride=2),\n                      # use stride 1 for the last conv4 layer\n                      resnet_v1_block(\'block3\', base_depth=256, num_units=6, stride=1),\n                      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1)]\n\n    elif self._num_layers == 101:\n      self._blocks = [resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n                      resnet_v1_block(\'block2\', base_depth=128, num_units=4, stride=2),\n                      # use stride 1 for the last conv4 layer\n                      resnet_v1_block(\'block3\', base_depth=256, num_units=23, stride=1),\n                      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1)]\n\n    elif self._num_layers == 152:\n      self._blocks = [resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n                      resnet_v1_block(\'block2\', base_depth=128, num_units=8, stride=2),\n                      # use stride 1 for the last conv4 layer\n                      resnet_v1_block(\'block3\', base_depth=256, num_units=36, stride=1),\n                      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1)]\n\n    else:\n      # other numbers are not supported\n      raise NotImplementedError\n\n  def get_variables_to_restore(self, variables, var_keep_dic):\n    variables_to_restore = []\n\n    for v in variables:\n      # exclude the first conv layer to swap RGB to BGR\n      if v.name == (self._scope + \'/conv1/weights:0\'):\n        self._variables_to_fix[v.name] = v\n        continue\n      if v.name.split(\':\')[0] in var_keep_dic:\n        print(\'Variables restored: %s\' % v.name)\n        variables_to_restore.append(v)\n\n    return variables_to_restore\n\n  def fix_variables(self, sess, pretrained_model):\n    print(\'Fix Resnet V1 layers..\')\n    with tf.variable_scope(\'Fix_Resnet_V1\'):\n      with tf.device(""/cpu:0""):\n        # fix RGB to BGR\n        conv1_rgb = tf.get_variable(""conv1_rgb"", [7, 7, 3, 64], trainable=False)\n        restorer_fc = tf.train.Saver({self._scope + ""/conv1/weights"": conv1_rgb})\n        restorer_fc.restore(sess, pretrained_model)\n\n        sess.run(tf.assign(self._variables_to_fix[self._scope + \'/conv1/weights:0\'], \n                           tf.reverse(conv1_rgb, [2])))\n'"
lib/nets/vgg16.py,12,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\n\nfrom nets.network import Network\nfrom model.config import cfg\n\nclass vgg16(Network):\n  def __init__(self):\n    Network.__init__(self)\n    self._feat_stride = [16, ]\n    self._feat_compress = [1. / float(self._feat_stride[0]), ]\n    self._scope = \'vgg_16\'\n\n  def _image_to_head(self, is_training, reuse=None):\n    with tf.variable_scope(self._scope, self._scope, reuse=reuse):\n      net = slim.repeat(self._image, 2, slim.conv2d, 64, [3, 3],\n                          trainable=False, scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], padding=\'SAME\', scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3],\n                        trainable=False, scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], padding=\'SAME\', scope=\'pool2\')\n      net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3],\n                        trainable=is_training, scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], padding=\'SAME\', scope=\'pool3\')\n      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3],\n                        trainable=is_training, scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], padding=\'SAME\', scope=\'pool4\')\n      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3],\n                        trainable=is_training, scope=\'conv5\')\n\n    self._act_summaries.append(net)\n    self._layers[\'head\'] = net\n    \n    return net\n\n  def _head_to_tail(self, pool5, is_training, reuse=None):\n    with tf.variable_scope(self._scope, self._scope, reuse=reuse):\n      pool5_flat = slim.flatten(pool5, scope=\'flatten\')\n      fc6 = slim.fully_connected(pool5_flat, 4096, scope=\'fc6\')\n      if is_training:\n        fc6 = slim.dropout(fc6, keep_prob=0.5, is_training=True, \n                            scope=\'dropout6\')\n      fc7 = slim.fully_connected(fc6, 4096, scope=\'fc7\')\n      if is_training:\n        fc7 = slim.dropout(fc7, keep_prob=0.5, is_training=True, \n                            scope=\'dropout7\')\n\n    return fc7\n\n  def get_variables_to_restore(self, variables, var_keep_dic):\n    variables_to_restore = []\n\n    for v in variables:\n      # exclude the conv weights that are fc weights in vgg16\n      if v.name == (self._scope + \'/fc6/weights:0\') or \\\n         v.name == (self._scope + \'/fc7/weights:0\'):\n        self._variables_to_fix[v.name] = v\n        continue\n      # exclude the first conv layer to swap RGB to BGR\n      if v.name == (self._scope + \'/conv1/conv1_1/weights:0\'):\n        self._variables_to_fix[v.name] = v\n        continue\n      if v.name.split(\':\')[0] in var_keep_dic:\n        print(\'Variables restored: %s\' % v.name)\n        variables_to_restore.append(v)\n\n    return variables_to_restore\n\n  def fix_variables(self, sess, pretrained_model):\n    print(\'Fix VGG16 layers..\')\n    with tf.variable_scope(\'Fix_VGG16\'):\n      with tf.device(""/cpu:0""):\n        # fix the vgg16 issue from conv weights to fc weights\n        # fix RGB to BGR\n        fc6_conv = tf.get_variable(""fc6_conv"", [7, 7, 512, 4096], trainable=False)\n        fc7_conv = tf.get_variable(""fc7_conv"", [1, 1, 4096, 4096], trainable=False)\n        conv1_rgb = tf.get_variable(""conv1_rgb"", [3, 3, 3, 64], trainable=False)\n        restorer_fc = tf.train.Saver({self._scope + ""/fc6/weights"": fc6_conv, \n                                      self._scope + ""/fc7/weights"": fc7_conv,\n                                      self._scope + ""/conv1/conv1_1/weights"": conv1_rgb})\n        restorer_fc.restore(sess, pretrained_model)\n\n        sess.run(tf.assign(self._variables_to_fix[self._scope + \'/fc6/weights:0\'], tf.reshape(fc6_conv, \n                            self._variables_to_fix[self._scope + \'/fc6/weights:0\'].get_shape())))\n        sess.run(tf.assign(self._variables_to_fix[self._scope + \'/fc7/weights:0\'], tf.reshape(fc7_conv, \n                            self._variables_to_fix[self._scope + \'/fc7/weights:0\'].get_shape())))\n        sess.run(tf.assign(self._variables_to_fix[self._scope + \'/conv1/conv1_1/weights:0\'], \n                            tf.reverse(conv1_rgb, [2])))\n'"
lib/utils/__init__.py,0,b''
lib/utils/blob.py,0,"b'""""""Blob helper functions.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport cv2\n\n\ndef im_list_to_blob(ims):\n  """"""Convert a list of images into a network input.\n\n  Assumes images are already prepared (means subtracted, BGR order, ...).\n  """"""\n  max_shape = np.array([im.shape for im in ims]).max(axis=0)\n  num_images = len(ims)\n  blob = np.zeros((num_images, max_shape[0], max_shape[1], 3),\n                  dtype=np.float32)\n  for i in range(num_images):\n    im = ims[i]\n    blob[i, 0:im.shape[0], 0:im.shape[1], :] = im\n\n  return blob\n\n\ndef prep_im_for_blob(im, pixel_means, target_size, max_size):\n  """"""Mean subtract and scale an image for use in a blob.""""""\n  im = im.astype(np.float32, copy=False)\n  im -= pixel_means\n  im_shape = im.shape\n  im_size_min = np.min(im_shape[0:2])\n  im_size_max = np.max(im_shape[0:2])\n  im_scale = float(target_size) / float(im_size_min)\n  # Prevent the biggest axis from being more than MAX_SIZE\n  if np.round(im_scale * im_size_max) > max_size:\n    im_scale = float(max_size) / float(im_size_max)\n  im = cv2.resize(im, None, None, fx=im_scale, fy=im_scale,\n                  interpolation=cv2.INTER_LINEAR)\n\n  return im, im_scale\n'"
lib/utils/snippets.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom model.config import cfg\nfrom utils.cython_bbox import bbox_overlaps\ntry:\n  import cPickle as pickle\nexcept ImportError:\n  import pickle\n\n# Just return the ground truth boxes for a single image\ndef compute_target(memory_size, gt_boxes, feat_stride):\n  factor_h = (memory_size[0] - 1.) * feat_stride\n  factor_w = (memory_size[1] - 1.) * feat_stride\n  num_gt = gt_boxes.shape[0]\n\n  x1 = gt_boxes[:, [0]] / factor_w\n  y1 = gt_boxes[:, [1]] / factor_h\n  x2 = gt_boxes[:, [2]] / factor_w\n  y2 = gt_boxes[:, [3]] / factor_h\n\n  rois = np.hstack((y1, x1, y2, x2))\n  batch_ids = np.zeros((num_gt), dtype=np.int32)\n  # overlap to regions of interest\n  roi_overlaps = np.ones((num_gt), dtype=np.float32)\n  labels = np.array(gt_boxes[:, 4], dtype=np.int32)\n\n  return rois, batch_ids, roi_overlaps, labels\n\n# Also return the reverse index of rois\ndef compute_target_memory(memory_size, gt_boxes, feat_stride):\n  minus_h = memory_size[0] - 1.\n  minus_w = memory_size[1] - 1.\n  num_gt = gt_boxes.shape[0]\n\n  x1 = gt_boxes[:, [0]] / feat_stride\n  y1 = gt_boxes[:, [1]] / feat_stride\n  x2 = gt_boxes[:, [2]] / feat_stride\n  y2 = gt_boxes[:, [3]] / feat_stride\n\n  # h, w, h, w\n  rois = np.hstack((y1, x1, y2, x2))\n  rois[:, 0::2] /= minus_h\n  rois[:, 1::2] /= minus_w\n  batch_ids = np.zeros((num_gt), dtype=np.int32)\n  labels = np.array(gt_boxes[:, 4], dtype=np.int32)\n\n  # h, w, h, w\n  inv_rois = np.empty_like(rois)\n  inv_rois[:, 0:2] = 0.\n  inv_rois[:, 2] = minus_h\n  inv_rois[:, 3] = minus_w\n  inv_rois[:, 0::2] -= y1\n  inv_rois[:, 1::2] -= x1\n\n  # normalize coordinates\n  inv_rois[:, 0::2] /= np.maximum(y2 - y1, cfg.EPS)\n  inv_rois[:, 1::2] /= np.maximum(x2 - x1, cfg.EPS)\n\n  inv_batch_ids = np.arange((num_gt), dtype=np.int32)\n\n  return rois, batch_ids, labels, inv_rois, inv_batch_ids\n\n# Update weights for the target\ndef update_weights(labels, cls_prob):\n  num_gt = labels.shape[0]\n  index = np.arange(num_gt)\n  cls_score = cls_prob[index, labels]\n  big_ones = cls_score >= 1. - cfg.MEM.BETA\n  # Focus on the hard examples\n  weights = 1. - cls_score\n  weights[big_ones] = cfg.MEM.BETA\n  weights /= np.maximum(np.sum(weights), cfg.EPS)\n  \n  return weights\n'"
lib/utils/timer.py,0,"b'import time\n\nclass Timer(object):\n    """"""A simple timer.""""""\n    def __init__(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n\n    def tic(self):\n        # using time.time instead of time.clock because time time.clock\n        # does not normalize for multithreading\n        self.start_time = time.time()\n\n    def toc(self, average=True):\n        self.diff = time.time() - self.start_time\n        self.total_time += self.diff\n        self.calls += 1\n        self.average_time = self.total_time / self.calls\n        if average:\n            return self.average_time\n        else:\n            return self.diff\n'"
lib/utils/visualization.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport os.path as osp\nfrom random import shuffle\nimport PIL.Image as Image\nimport PIL.ImageColor as ImageColor\nimport PIL.ImageDraw as ImageDraw\nimport PIL.ImageFont as ImageFont\n\nfrom model.config import cfg\n\nSTANDARD_COLORS = [\n    'AliceBlue', 'Chartreuse', 'Aqua', 'Aquamarine', 'Azure', 'Beige', 'Bisque',\n    'BlanchedAlmond', 'BlueViolet', 'BurlyWood', 'CadetBlue', 'AntiqueWhite',\n    'Chocolate', 'Coral', 'CornflowerBlue', 'Cornsilk', 'Crimson', 'Cyan',\n    'DarkCyan', 'DarkGoldenRod', 'DarkGrey', 'DarkKhaki', 'DarkOrange',\n    'DarkOrchid', 'DarkSalmon', 'DarkSeaGreen', 'DarkTurquoise', 'DarkViolet',\n    'DeepPink', 'DeepSkyBlue', 'DodgerBlue', 'FireBrick', 'FloralWhite',\n    'ForestGreen', 'Fuchsia', 'Gainsboro', 'GhostWhite', 'Gold', 'GoldenRod',\n    'Salmon', 'Tan', 'HoneyDew', 'HotPink', 'IndianRed', 'Ivory', 'Khaki',\n    'Lavender', 'LavenderBlush', 'LawnGreen', 'LemonChiffon', 'LightBlue',\n    'LightCoral', 'LightCyan', 'LightGoldenRodYellow', 'LightGray', 'LightGrey',\n    'LightGreen', 'LightPink', 'LightSalmon', 'LightSeaGreen', 'LightSkyBlue',\n    'LightSlateGray', 'LightSlateGrey', 'LightSteelBlue', 'LightYellow', 'Lime',\n    'LimeGreen', 'Linen', 'Magenta', 'MediumAquaMarine', 'MediumOrchid',\n    'MediumPurple', 'MediumSeaGreen', 'MediumSlateBlue', 'MediumSpringGreen',\n    'MediumTurquoise', 'MediumVioletRed', 'MintCream', 'MistyRose', 'Moccasin',\n    'NavajoWhite', 'OldLace', 'Olive', 'OliveDrab', 'Orange', 'OrangeRed',\n    'Orchid', 'PaleGoldenRod', 'PaleGreen', 'PaleTurquoise', 'PaleVioletRed',\n    'PapayaWhip', 'PeachPuff', 'Peru', 'Pink', 'Plum', 'PowderBlue', 'Purple',\n    'Red', 'RosyBrown', 'RoyalBlue', 'SaddleBrown', 'Green', 'SandyBrown',\n    'SeaGreen', 'SeaShell', 'Sienna', 'Silver', 'SkyBlue', 'SlateBlue',\n    'SlateGray', 'SlateGrey', 'Snow', 'SpringGreen', 'SteelBlue', 'GreenYellow',\n    'Teal', 'Thistle', 'Tomato', 'Turquoise', 'Violet', 'Wheat', 'White',\n    'WhiteSmoke', 'Yellow', 'YellowGreen'\n]\n\nNUM_COLORS = len(STANDARD_COLORS)\nthis_dir = osp.dirname(__file__)\n\ntry:\n  FONT = ImageFont.truetype(osp.join(this_dir, '..', '..', 'data', 'helveticaneue.ttf'), 12)\nexcept IOError:\n  FONT = ImageFont.load_default()\n\ntry:\n  FONT_BIG = ImageFont.truetype(osp.join(this_dir, '..', '..', 'data', 'helveticaneue.ttf'), 24)\nexcept IOError:\n  FONT_BIG = ImageFont.load_default()\n\ndef _draw_single_box(image, xmin, ymin, xmax, ymax, display_str, font, color='black', color_text='black', thickness=2):\n  draw = ImageDraw.Draw(image)\n  (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n  draw.line([(left, top), (left, bottom), (right, bottom),\n             (right, top), (left, top)], width=thickness, fill=color)\n  text_bottom = bottom\n  # Reverse list and print from bottom to top.\n  text_width, text_height = font.getsize(display_str)\n  margin = np.ceil(0.05 * text_height)\n  draw.rectangle(\n      [(left, text_bottom - text_height - 2 * margin), (left + text_width,\n                                                        text_bottom)],\n      fill=color)\n  draw.text(\n      (left + margin, text_bottom - text_height - margin),\n      display_str,\n      fill=color_text,\n      font=font)\n\n  return image\n\ndef draw_gt_boxes(image, gt_boxes):\n  num_boxes = gt_boxes.shape[0]\n  disp_image = Image.fromarray(np.uint8(image[0]))\n\n  list_gt = range(num_boxes)\n  shuffle(list_gt)\n  for i in list_gt:\n    this_class = int(gt_boxes[i, 4])\n    disp_image = _draw_single_box(disp_image, \n                                gt_boxes[i, 0],\n                                gt_boxes[i, 1],\n                                gt_boxes[i, 2],\n                                gt_boxes[i, 3],\n                                '%s' % (cfg.CLASSES[this_class]),\n                                FONT,\n                                color=STANDARD_COLORS[this_class % NUM_COLORS])\n\n  new_image = np.empty_like(image)\n  new_image[0, :] = np.array(disp_image)\n  return new_image\n\ndef draw_predicted_boxes(image, scores, gt_boxes, labels=None):\n  disp_image = Image.fromarray(np.uint8(image[0]))\n  num_boxes = gt_boxes.shape[0]\n  preds = np.argmax(scores, axis=1)\n  if labels is None:\n    labels = gt_boxes[:, 4]\n\n  list_gt = range(num_boxes)\n  shuffle(list_gt)\n  for i in list_gt:\n    this_class = int(labels[i])\n    pred_class = preds[i]\n    this_conf = scores[i, this_class]\n    pred_conf = scores[i, pred_class]\n    this_text = '%s|%.2f' % (cfg.CLASSES[this_class], this_conf)\n    if this_class != pred_class:\n      this_text += '(%s|%.2f)' % (cfg.CLASSES[pred_class], pred_conf)\n    elif this_class == 0:\n      this_text = '(X)'\n    disp_image = _draw_single_box(disp_image, \n                                gt_boxes[i, 0],\n                                gt_boxes[i, 1],\n                                gt_boxes[i, 2],\n                                gt_boxes[i, 3],\n                                this_text,\n                                FONT,\n                                color=STANDARD_COLORS[this_class % NUM_COLORS])\n\n  new_image = np.empty_like(image)\n  new_image[0, :] = np.array(disp_image)\n  return new_image\n\ndef draw_predicted_boxes_attend(image, scores, gt_boxes, attend, weight=None):\n  disp_image = Image.fromarray(np.uint8(image[0]))\n  num_boxes = gt_boxes.shape[0]\n  preds = np.argmax(scores, axis=1)\n  labels = gt_boxes[:, 4]\n\n  list_gt = range(num_boxes)\n  shuffle(list_gt)\n  for i in list_gt:\n    this_class = int(labels[i])\n    pred_class = preds[i]\n    this_conf = scores[i, this_class]\n    pred_conf = scores[i, pred_class]\n    this_text = '%.2f&' % attend[i, 0]\n    if weight is not None:\n      this_text += '%.2f&' % (weight[i] * num_boxes)\n    this_text += '%s|%.2f' % (cfg.CLASSES[this_class], this_conf)\n    if this_class != pred_class:\n      this_text += '(%s|%.2f)' % (cfg.CLASSES[pred_class], pred_conf)\n    elif this_class == 0:\n      this_text = '(X)'\n    disp_image = _draw_single_box(disp_image, \n                                gt_boxes[i, 0],\n                                gt_boxes[i, 1],\n                                gt_boxes[i, 2],\n                                gt_boxes[i, 3],\n                                this_text,\n                                FONT,\n                                color=STANDARD_COLORS[this_class % NUM_COLORS])\n\n  new_image = np.empty_like(image)\n  new_image[0, :] = np.array(disp_image)\n  return new_image\n\ndef draw_predicted_boxes_test(image, scores, gt_boxes):\n  disp_image = Image.fromarray(np.uint8(image[0] + cfg.PIXEL_MEANS))\n  num_boxes = gt_boxes.shape[0]\n  # Avoid background class\n  preds = np.argmax(scores[:,1:], axis=1)+1\n  wrong = False\n  list_gt = range(num_boxes)\n  shuffle(list_gt)\n  for i in list_gt:\n    this_class = int(gt_boxes[i, 4])\n    pred_class = preds[i]\n    this_conf = scores[i, this_class]\n    pred_conf = scores[i, pred_class]\n    if this_class != pred_class:\n      this_text = '%s|%.2f' % (cfg.CLASSES[this_class], this_conf)\n      this_text += '(%s|%.2f)' % (cfg.CLASSES[pred_class], pred_conf)\n      wrong = True\n    else:\n      # this_text = '(X)'\n      this_text = '%s|%.2f' % (cfg.CLASSES[this_class], this_conf)\n    disp_image = _draw_single_box(disp_image, \n                                gt_boxes[i, 0],\n                                gt_boxes[i, 1],\n                                gt_boxes[i, 2],\n                                gt_boxes[i, 3],\n                                this_text,\n                                FONT,\n                                color=STANDARD_COLORS[this_class % NUM_COLORS])\n\n  new_image = np.array(disp_image)\n  return new_image, wrong\n\ndef draw_memory(mem, scale=1.0):\n  # Set the boundary\n  mem_image = np.minimum(np.mean(np.absolute(mem.squeeze(axis=0)), axis=2) * (255. / scale), 255.)\n  # Just visualization\n  mem_image = np.tile(np.expand_dims(mem_image, axis=2), [1,1,3])\n\n  return mem_image[np.newaxis]\n\ndef draw_weights(mem, scale=1.0):\n  # Set the boundary\n  mem_image = np.minimum(np.mean(np.absolute(mem.squeeze(axis=0)), axis=2) * (255. / scale), 255.)\n  # Just visualization\n  mem_image = np.tile(np.expand_dims(np.uint8(mem_image), axis=2), [1,1,3])\n\n  return mem_image[np.newaxis]\n\n\n\n\n  \n"""
