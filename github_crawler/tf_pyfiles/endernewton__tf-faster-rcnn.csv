file_path,api_count,code
lib/setup.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nfrom os.path import join as pjoin\nimport numpy as np\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\n\ndef find_in_path(name, path):\n    ""Find a file in a search path""\n    #adapted fom http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\ndef locate_cuda():\n    """"""Locate the CUDA environment on the system\n\n    Returns a dict with keys \'home\', \'nvcc\', \'include\', and \'lib64\'\n    and values giving the absolute path to each directory.\n\n    Starts by looking for the CUDAHOME env variable. If not found, everything\n    is based on finding \'nvcc\' in the PATH.\n    """"""\n\n    # first check if the CUDAHOME env variable is in use\n    if \'CUDAHOME\' in os.environ:\n        home = os.environ[\'CUDAHOME\']\n        nvcc = pjoin(home, \'bin\', \'nvcc\')\n    else:\n        # otherwise, search the PATH for NVCC\n        default_path = pjoin(os.sep, \'usr\', \'local\', \'cuda\', \'bin\')\n        nvcc = find_in_path(\'nvcc\', os.environ[\'PATH\'] + os.pathsep + default_path)\n        if nvcc is None:\n            raise EnvironmentError(\'The nvcc binary could not be \'\n                \'located in your $PATH. Either add it to your path, or set $CUDAHOME\')\n        home = os.path.dirname(os.path.dirname(nvcc))\n\n    cudaconfig = {\'home\':home, \'nvcc\':nvcc,\n                  \'include\': pjoin(home, \'include\'),\n                  \'lib64\': pjoin(home, \'lib64\')}\n    for k, v in cudaconfig.items():\n        if not os.path.exists(v):\n            raise EnvironmentError(\'The CUDA %s path could not be located in %s\' % (k, v))\n\n    return cudaconfig\nCUDA = locate_cuda()\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to gcc/nvcc works.\n\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        print(extra_postargs)\n        if os.path.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', CUDA[\'nvcc\'])\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'gcc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\next_modules = [\n    Extension(\n        ""utils.cython_bbox"",\n        [""utils/bbox.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs = [numpy_include]\n    ),\n    Extension(\n        ""nms.cpu_nms"",\n        [""nms/cpu_nms.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs = [numpy_include]\n    ),\n    Extension(\'nms.gpu_nms\',\n        [\'nms/nms_kernel.cu\', \'nms/gpu_nms.pyx\'],\n        library_dirs=[CUDA[\'lib64\']],\n        libraries=[\'cudart\'],\n        language=\'c++\',\n        runtime_library_dirs=[CUDA[\'lib64\']],\n        # this syntax is specific to this build system\n        # we\'re only going to use certain compiler args with nvcc and not with gcc\n        # the implementation of this trick is in customize_compiler() below\n        extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n                            \'nvcc\': [\'-arch=sm_52\',\n                                     \'--ptxas-options=-v\',\n                                     \'-c\',\n                                     \'--compiler-options\',\n                                     ""\'-fPIC\'""]},\n        include_dirs = [numpy_include, CUDA[\'include\']]\n    )\n]\n\nsetup(\n    name=\'tf_faster_rcnn\',\n    ext_modules=ext_modules,\n    # inject our custom trigger\n    cmdclass={\'build_ext\': custom_build_ext},\n)\n'"
tools/_init_paths.py,0,"b""import os.path as osp\nimport sys\n\ndef add_path(path):\n    if path not in sys.path:\n        sys.path.insert(0, path)\n\nthis_dir = osp.dirname(__file__)\n\n# Add lib to PYTHONPATH\nlib_path = osp.join(this_dir, '..', 'lib')\nadd_path(lib_path)\n\ncoco_path = osp.join(this_dir, '..', 'data', 'coco', 'PythonAPI')\nadd_path(coco_path)\n"""
tools/convert_from_depre.py,10,"b'# --------------------------------------------------------\n# Tensorflow Faster R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Xinlei Chen\n# --------------------------------------------------------\n""""""\nConvert depreciated VGG16 snapshots to the ones that support tensorflow format\n\nIt will check the specific snapshot at the vgg16_depre folder, and copy it to the same location at vgg16 folder\nSee experimental/scripts/convert_vgg16.sh for how to use it.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport _init_paths\nfrom model.config import cfg, cfg_from_file, cfg_from_list, get_output_dir\nfrom model.train_val import filter_roidb, get_training_roidb\nfrom datasets.factory import get_imdb\nimport datasets.imdb\nimport argparse\nimport pprint\nimport numpy as np\nimport sys\nimport os\nimport os.path as osp\nimport shutil\n\ntry:\n  import cPickle as pickle\nexcept ImportError:\n  import pickle\n\nimport tensorflow as tf\nfrom tensorflow.python import pywrap_tensorflow\n\nfrom nets.vgg16 import vgg16\n\ndef parse_args():\n  """"""\n  Parse input arguments\n  """"""\n  parser = argparse.ArgumentParser(description=\'Convert an old VGG16 snapshot to new format\')\n  parser.add_argument(\'--cfg\', dest=\'cfg_file\',\n                      help=\'optional config file\',\n                      default=None, type=str)\n  parser.add_argument(\'--snapshot\', dest=\'snapshot\',\n                      help=\'vgg snapshot prefix\',\n                      type=str)\n  parser.add_argument(\'--imdb\', dest=\'imdb_name\',\n                      help=\'dataset to train on\',\n                      default=\'voc_2007_trainval\', type=str)\n  parser.add_argument(\'--iters\', dest=\'max_iters\',\n                      help=\'number of iterations to train\',\n                      default=70000, type=int)\n  parser.add_argument(\'--tag\', dest=\'tag\',\n                      help=\'tag of the model\',\n                      default=None, type=str)\n  parser.add_argument(\'--set\', dest=\'set_cfgs\',\n                      help=\'set config keys\', default=None,\n                      nargs=argparse.REMAINDER)\n\n  if len(sys.argv) == 1:\n    parser.print_help()\n    sys.exit(1)\n\n  args = parser.parse_args()\n  return args\n\ndef combined_roidb(imdb_names):\n  """"""\n  Combine multiple roidbs\n  """"""\n\n  def get_roidb(imdb_name):\n    imdb = get_imdb(imdb_name)\n    print(\'Loaded dataset `{:s}` for training\'.format(imdb.name))\n    imdb.set_proposal_method(cfg.TRAIN.PROPOSAL_METHOD)\n    print(\'Set proposal method: {:s}\'.format(cfg.TRAIN.PROPOSAL_METHOD))\n    roidb = get_training_roidb(imdb)\n    return roidb\n\n  roidbs = [get_roidb(s) for s in imdb_names.split(\'+\')]\n  roidb = roidbs[0]\n  if len(roidbs) > 1:\n    for r in roidbs[1:]:\n      roidb.extend(r)\n    tmp = get_imdb(imdb_names.split(\'+\')[1])\n    imdb = datasets.imdb.imdb(imdb_names, tmp.classes)\n  else:\n    imdb = get_imdb(imdb_names)\n  return imdb, roidb\n\ndef get_variables_in_checkpoint_file(file_name):\n  try:\n    reader = pywrap_tensorflow.NewCheckpointReader(file_name)\n    var_to_shape_map = reader.get_variable_to_shape_map()\n    return var_to_shape_map \n  except Exception as e:  # pylint: disable=broad-except\n    print(str(e))\n    if ""corrupted compressed block contents"" in str(e):\n      print(""It\'s likely that your checkpoint file has been compressed ""\n            ""with SNAPPY."")\n\ndef convert_names(name):\n  # removing :0\n  name = name.replace(\':0\', \'\')\n  # replace\n  name = name.replace(\'vgg_16/\', \'vgg16_default/\')\n  name = name.replace(\'/biases\', \'/bias\')\n  name = name.replace(\'/weights\', \'/weight\')\n  name = name.replace(\'/conv1/\', \'/\')\n  name = name.replace(\'/conv2/\', \'/\')\n  name = name.replace(\'/conv3/\', \'/\')\n  name = name.replace(\'/conv4/\', \'/\')\n  name = name.replace(\'/conv5/\', \'/\')\n\n  return name\n\n# Just build the graph, load the weights/statistics, and save them\ndef convert_from_depre(net, imdb, input_dir, output_dir, snapshot, max_iters):\n  if not osp.exists(output_dir):\n    os.makedirs(output_dir)\n\n  tfconfig = tf.ConfigProto(allow_soft_placement=True)\n  tfconfig.gpu_options.allow_growth = True\n  sess = tf.Session(config=tfconfig)\n\n  num_classes = imdb.num_classes\n  with sess.graph.as_default():\n    tf.set_random_seed(cfg.RNG_SEED)\n    layers = net.create_architecture(sess, \'TRAIN\', num_classes, tag=\'default\',\n                                            anchor_scales=cfg.ANCHOR_SCALES,\n                                            anchor_ratios=cfg.ANCHOR_RATIOS)\n    loss = layers[\'total_loss\']\n    # Learning rate should be reduced already\n    lr = tf.Variable(cfg.TRAIN.LEARNING_RATE * cfg.TRAIN.GAMMA, trainable=False)\n    momentum = cfg.TRAIN.MOMENTUM\n    optimizer = tf.train.MomentumOptimizer(lr, momentum)\n    gvs = optimizer.compute_gradients(loss)\n    if cfg.TRAIN.DOUBLE_BIAS:\n      final_gvs = []\n      with tf.variable_scope(\'Gradient_Mult\') as scope:\n        for grad, var in gvs:\n          scale = 1.\n          if cfg.TRAIN.DOUBLE_BIAS and \'/biases:\' in var.name:\n            scale *= 2.\n          if not np.allclose(scale, 1.0):\n            grad = tf.multiply(grad, scale)\n          final_gvs.append((grad, var))\n      train_op = optimizer.apply_gradients(final_gvs)\n    else:\n      train_op = optimizer.apply_gradients(gvs)\n\n    checkpoint = osp.join(input_dir, snapshot + \'.ckpt\')\n    variables = tf.global_variables()\n    name2var = {convert_names(v.name): v for v in variables}\n    target_names = get_variables_in_checkpoint_file(checkpoint)\n    restorer = tf.train.Saver(name2var)\n    saver = tf.train.Saver()\n\n    print(\'Importing...\')\n    restorer.restore(sess, checkpoint)\n    checkpoint = osp.join(output_dir, snapshot + \'.ckpt\')\n    print(\'Exporting...\')\n    saver.save(sess, checkpoint)\n\n    # also copy the pkl file\n    index = osp.join(input_dir, snapshot + \'.pkl\')\n    outdex = osp.join(output_dir, snapshot + \'.pkl\')\n    shutil.copy(index, outdex)\n\n  sess.close()\n\n\nif __name__ == \'__main__\':\n  args = parse_args()\n\n  print(\'Called with args:\')\n  print(args)\n\n  if args.cfg_file is not None:\n    cfg_from_file(args.cfg_file)\n  if args.set_cfgs is not None:\n    cfg_from_list(args.set_cfgs)\n\n  print(\'Using config:\')\n  pprint.pprint(cfg)\n\n  np.random.seed(cfg.RNG_SEED)\n\n  # train set\n  imdb, _ = combined_roidb(args.imdb_name)\n\n  # output directory where the snapshot will be exported\n  output_dir = get_output_dir(imdb, args.tag)\n  print(\'Output will be exported to `{:s}`\'.format(output_dir))\n\n  # input directory where the snapshot will be imported\n  input_dir = output_dir.replace(\'/vgg16/\', \'/vgg16_depre/\')\n  print(\'Input will be imported from `{:s}`\'.format(input_dir))\n\n  net = vgg16()\n\n  convert_from_depre(net, imdb, input_dir, output_dir, args.snapshot, args.max_iters)\n'"
tools/demo.py,3,"b'#!/usr/bin/env python\n\n# --------------------------------------------------------\n# Tensorflow Faster R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Xinlei Chen, based on code from Ross Girshick\n# --------------------------------------------------------\n\n""""""\nDemo script showing detections in sample images.\n\nSee README.md for installation instructions before running.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport _init_paths\nfrom model.config import cfg\nfrom model.test import im_detect\nfrom model.nms_wrapper import nms\n\nfrom utils.timer import Timer\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os, cv2\nimport argparse\n\nfrom nets.vgg16 import vgg16\nfrom nets.resnet_v1 import resnetv1\n\nCLASSES = (\'__background__\',\n           \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n           \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n           \'cow\', \'diningtable\', \'dog\', \'horse\',\n           \'motorbike\', \'person\', \'pottedplant\',\n           \'sheep\', \'sofa\', \'train\', \'tvmonitor\')\n\nNETS = {\'vgg16\': (\'vgg16_faster_rcnn_iter_70000.ckpt\',),\'res101\': (\'res101_faster_rcnn_iter_110000.ckpt\',)}\nDATASETS= {\'pascal_voc\': (\'voc_2007_trainval\',),\'pascal_voc_0712\': (\'voc_2007_trainval+voc_2012_trainval\',)}\n\ndef vis_detections(im, class_name, dets, thresh=0.5):\n    """"""Draw detected bounding boxes.""""""\n    inds = np.where(dets[:, -1] >= thresh)[0]\n    if len(inds) == 0:\n        return\n\n    im = im[:, :, (2, 1, 0)]\n    fig, ax = plt.subplots(figsize=(12, 12))\n    ax.imshow(im, aspect=\'equal\')\n    for i in inds:\n        bbox = dets[i, :4]\n        score = dets[i, -1]\n\n        ax.add_patch(\n            plt.Rectangle((bbox[0], bbox[1]),\n                          bbox[2] - bbox[0],\n                          bbox[3] - bbox[1], fill=False,\n                          edgecolor=\'red\', linewidth=3.5)\n            )\n        ax.text(bbox[0], bbox[1] - 2,\n                \'{:s} {:.3f}\'.format(class_name, score),\n                bbox=dict(facecolor=\'blue\', alpha=0.5),\n                fontsize=14, color=\'white\')\n\n    ax.set_title((\'{} detections with \'\n                  \'p({} | box) >= {:.1f}\').format(class_name, class_name,\n                                                  thresh),\n                  fontsize=14)\n    plt.axis(\'off\')\n    plt.tight_layout()\n    plt.draw()\n\ndef demo(sess, net, image_name):\n    """"""Detect object classes in an image using pre-computed object proposals.""""""\n\n    # Load the demo image\n    im_file = os.path.join(cfg.DATA_DIR, \'demo\', image_name)\n    im = cv2.imread(im_file)\n\n    # Detect all object classes and regress object bounds\n    timer = Timer()\n    timer.tic()\n    scores, boxes = im_detect(sess, net, im)\n    timer.toc()\n    print(\'Detection took {:.3f}s for {:d} object proposals\'.format(timer.total_time, boxes.shape[0]))\n\n    # Visualize detections for each class\n    CONF_THRESH = 0.8\n    NMS_THRESH = 0.3\n    for cls_ind, cls in enumerate(CLASSES[1:]):\n        cls_ind += 1 # because we skipped background\n        cls_boxes = boxes[:, 4*cls_ind:4*(cls_ind + 1)]\n        cls_scores = scores[:, cls_ind]\n        dets = np.hstack((cls_boxes,\n                          cls_scores[:, np.newaxis])).astype(np.float32)\n        keep = nms(dets, NMS_THRESH)\n        dets = dets[keep, :]\n        vis_detections(im, cls, dets, thresh=CONF_THRESH)\n\ndef parse_args():\n    """"""Parse input arguments.""""""\n    parser = argparse.ArgumentParser(description=\'Tensorflow Faster R-CNN demo\')\n    parser.add_argument(\'--net\', dest=\'demo_net\', help=\'Network to use [vgg16 res101]\',\n                        choices=NETS.keys(), default=\'res101\')\n    parser.add_argument(\'--dataset\', dest=\'dataset\', help=\'Trained dataset [pascal_voc pascal_voc_0712]\',\n                        choices=DATASETS.keys(), default=\'pascal_voc_0712\')\n    args = parser.parse_args()\n\n    return args\n\nif __name__ == \'__main__\':\n    cfg.TEST.HAS_RPN = True  # Use RPN for proposals\n    args = parse_args()\n\n    # model path\n    demonet = args.demo_net\n    dataset = args.dataset\n    tfmodel = os.path.join(\'output\', demonet, DATASETS[dataset][0], \'default\',\n                              NETS[demonet][0])\n\n\n    if not os.path.isfile(tfmodel + \'.meta\'):\n        raise IOError((\'{:s} not found.\\nDid you download the proper networks from \'\n                       \'our server and place them properly?\').format(tfmodel + \'.meta\'))\n\n    # set config\n    tfconfig = tf.ConfigProto(allow_soft_placement=True)\n    tfconfig.gpu_options.allow_growth=True\n\n    # init session\n    sess = tf.Session(config=tfconfig)\n    # load network\n    if demonet == \'vgg16\':\n        net = vgg16()\n    elif demonet == \'res101\':\n        net = resnetv1(num_layers=101)\n    else:\n        raise NotImplementedError\n    net.create_architecture(""TEST"", 21,\n                          tag=\'default\', anchor_scales=[8, 16, 32])\n    saver = tf.train.Saver()\n    saver.restore(sess, tfmodel)\n\n    print(\'Loaded network {:s}\'.format(tfmodel))\n\n    im_names = [\'000456.jpg\', \'000542.jpg\', \'001150.jpg\',\n                \'001763.jpg\', \'004545.jpg\']\n    for im_name in im_names:\n        print(\'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\')\n        print(\'Demo for data/demo/{}\'.format(im_name))\n        demo(sess, net, im_name)\n\n    plt.show()\n'"
tools/reval.py,0,"b'#!/usr/bin/env python\n\n# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n# Reval = re-eval. Re-evaluate saved detections.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport _init_paths\nfrom model.test import apply_nms\nfrom model.config import cfg\nfrom datasets.factory import get_imdb\nimport pickle\nimport os, sys, argparse\nimport numpy as np\n\n\ndef parse_args():\n  """"""\n  Parse input arguments\n  """"""\n  parser = argparse.ArgumentParser(description=\'Re-evaluate results\')\n  parser.add_argument(\'output_dir\', nargs=1, help=\'results directory\',\n                      type=str)\n  parser.add_argument(\'--imdb\', dest=\'imdb_name\',\n                      help=\'dataset to re-evaluate\',\n                      default=\'voc_2007_test\', type=str)\n  parser.add_argument(\'--matlab\', dest=\'matlab_eval\',\n                      help=\'use matlab for evaluation\',\n                      action=\'store_true\')\n  parser.add_argument(\'--comp\', dest=\'comp_mode\', help=\'competition mode\',\n                      action=\'store_true\')\n  parser.add_argument(\'--nms\', dest=\'apply_nms\', help=\'apply nms\',\n                      action=\'store_true\')\n\n  if len(sys.argv) == 1:\n    parser.print_help()\n    sys.exit(1)\n\n  args = parser.parse_args()\n  return args\n\n\ndef from_dets(imdb_name, output_dir, args):\n  imdb = get_imdb(imdb_name)\n  imdb.competition_mode(args.comp_mode)\n  imdb.config[\'matlab_eval\'] = args.matlab_eval\n  with open(os.path.join(output_dir, \'detections.pkl\'), \'rb\') as f:\n    dets = pickle.load(f)\n\n  if args.apply_nms:\n    print(\'Applying NMS to all detections\')\n    nms_dets = apply_nms(dets, cfg.TEST.NMS)\n  else:\n    nms_dets = dets\n\n  print(\'Evaluating detections\')\n  imdb.evaluate_detections(nms_dets, output_dir)\n\n\nif __name__ == \'__main__\':\n  args = parse_args()\n\n  output_dir = os.path.abspath(args.output_dir[0])\n  imdb_name = args.imdb_name\n  from_dets(imdb_name, output_dir, args)\n'"
tools/test_net.py,4,"b'# --------------------------------------------------------\n# Tensorflow Faster R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Zheqi he, Xinlei Chen, based on code from Ross Girshick\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport _init_paths\nfrom model.test import test_net\nfrom model.config import cfg, cfg_from_file, cfg_from_list\nfrom datasets.factory import get_imdb\nimport argparse\nimport pprint\nimport time, os, sys\n\nimport tensorflow as tf\nfrom nets.vgg16 import vgg16\nfrom nets.resnet_v1 import resnetv1\nfrom nets.mobilenet_v1 import mobilenetv1\n\ndef parse_args():\n  """"""\n  Parse input arguments\n  """"""\n  parser = argparse.ArgumentParser(description=\'Test a Fast R-CNN network\')\n  parser.add_argument(\'--cfg\', dest=\'cfg_file\',\n            help=\'optional config file\', default=None, type=str)\n  parser.add_argument(\'--model\', dest=\'model\',\n            help=\'model to test\',\n            default=None, type=str)\n  parser.add_argument(\'--imdb\', dest=\'imdb_name\',\n            help=\'dataset to test\',\n            default=\'voc_2007_test\', type=str)\n  parser.add_argument(\'--comp\', dest=\'comp_mode\', help=\'competition mode\',\n            action=\'store_true\')\n  parser.add_argument(\'--num_dets\', dest=\'max_per_image\',\n            help=\'max number of detections per image\',\n            default=100, type=int)\n  parser.add_argument(\'--tag\', dest=\'tag\',\n                        help=\'tag of the model\',\n                        default=\'\', type=str)\n  parser.add_argument(\'--net\', dest=\'net\',\n                      help=\'vgg16, res50, res101, res152, mobile\',\n                      default=\'res50\', type=str)\n  parser.add_argument(\'--set\', dest=\'set_cfgs\',\n                        help=\'set config keys\', default=None,\n                        nargs=argparse.REMAINDER)\n\n  if len(sys.argv) == 1:\n    parser.print_help()\n    sys.exit(1)\n\n  args = parser.parse_args()\n  return args\n\nif __name__ == \'__main__\':\n  args = parse_args()\n\n  print(\'Called with args:\')\n  print(args)\n\n  if args.cfg_file is not None:\n    cfg_from_file(args.cfg_file)\n  if args.set_cfgs is not None:\n    cfg_from_list(args.set_cfgs)\n\n  print(\'Using config:\')\n  pprint.pprint(cfg)\n\n  # if has model, get the name from it\n  # if does not, then just use the initialization weights\n  if args.model:\n    filename = os.path.splitext(os.path.basename(args.model))[0]\n  else:\n    filename = os.path.splitext(os.path.basename(args.weight))[0]\n\n  tag = args.tag\n  tag = tag if tag else \'default\'\n  filename = tag + \'/\' + filename\n\n  imdb = get_imdb(args.imdb_name)\n  imdb.competition_mode(args.comp_mode)\n\n  tfconfig = tf.ConfigProto(allow_soft_placement=True)\n  tfconfig.gpu_options.allow_growth=True\n\n  # init session\n  sess = tf.Session(config=tfconfig)\n  # load network\n  if args.net == \'vgg16\':\n    net = vgg16()\n  elif args.net == \'res50\':\n    net = resnetv1(num_layers=50)\n  elif args.net == \'res101\':\n    net = resnetv1(num_layers=101)\n  elif args.net == \'res152\':\n    net = resnetv1(num_layers=152)\n  elif args.net == \'mobile\':\n    net = mobilenetv1()\n  else:\n    raise NotImplementedError\n\n  # load model\n  net.create_architecture(""TEST"", imdb.num_classes, tag=\'default\',\n                          anchor_scales=cfg.ANCHOR_SCALES,\n                          anchor_ratios=cfg.ANCHOR_RATIOS)\n\n  if args.model:\n    print((\'Loading model check point from {:s}\').format(args.model))\n    saver = tf.train.Saver()\n    saver.restore(sess, args.model)\n    print(\'Loaded.\')\n  else:\n    print((\'Loading initial weights from {:s}\').format(args.weight))\n    sess.run(tf.global_variables_initializer())\n    print(\'Loaded.\')\n\n  test_net(sess, net, imdb, filename, max_per_image=args.max_per_image)\n\n  sess.close()\n'"
tools/trainval_net.py,0,"b'# --------------------------------------------------------\n# Tensorflow Faster R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Zheqi He, Xinlei Chen, based on code from Ross Girshick\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport _init_paths\nfrom model.train_val import get_training_roidb, train_net\nfrom model.config import cfg, cfg_from_file, cfg_from_list, get_output_dir, get_output_tb_dir\nfrom datasets.factory import get_imdb\nimport datasets.imdb\nimport argparse\nimport pprint\nimport numpy as np\nimport sys\n\nimport tensorflow as tf\nfrom nets.vgg16 import vgg16\nfrom nets.resnet_v1 import resnetv1\nfrom nets.mobilenet_v1 import mobilenetv1\n\ndef parse_args():\n  """"""\n  Parse input arguments\n  """"""\n  parser = argparse.ArgumentParser(description=\'Train a Fast R-CNN network\')\n  parser.add_argument(\'--cfg\', dest=\'cfg_file\',\n                      help=\'optional config file\',\n                      default=None, type=str)\n  parser.add_argument(\'--weight\', dest=\'weight\',\n                      help=\'initialize with pretrained model weights\',\n                      type=str)\n  parser.add_argument(\'--imdb\', dest=\'imdb_name\',\n                      help=\'dataset to train on\',\n                      default=\'voc_2007_trainval\', type=str)\n  parser.add_argument(\'--imdbval\', dest=\'imdbval_name\',\n                      help=\'dataset to validate on\',\n                      default=\'voc_2007_test\', type=str)\n  parser.add_argument(\'--iters\', dest=\'max_iters\',\n                      help=\'number of iterations to train\',\n                      default=70000, type=int)\n  parser.add_argument(\'--tag\', dest=\'tag\',\n                      help=\'tag of the model\',\n                      default=None, type=str)\n  parser.add_argument(\'--net\', dest=\'net\',\n                      help=\'vgg16, res50, res101, res152, mobile\',\n                      default=\'res50\', type=str)\n  parser.add_argument(\'--set\', dest=\'set_cfgs\',\n                      help=\'set config keys\', default=None,\n                      nargs=argparse.REMAINDER)\n\n  if len(sys.argv) == 1:\n    parser.print_help()\n    sys.exit(1)\n\n  args = parser.parse_args()\n  return args\n\n\ndef combined_roidb(imdb_names):\n  """"""\n  Combine multiple roidbs\n  """"""\n\n  def get_roidb(imdb_name):\n    imdb = get_imdb(imdb_name)\n    print(\'Loaded dataset `{:s}` for training\'.format(imdb.name))\n    imdb.set_proposal_method(cfg.TRAIN.PROPOSAL_METHOD)\n    print(\'Set proposal method: {:s}\'.format(cfg.TRAIN.PROPOSAL_METHOD))\n    roidb = get_training_roidb(imdb)\n    return roidb\n\n  roidbs = [get_roidb(s) for s in imdb_names.split(\'+\')]\n  roidb = roidbs[0]\n  if len(roidbs) > 1:\n    for r in roidbs[1:]:\n      roidb.extend(r)\n    tmp = get_imdb(imdb_names.split(\'+\')[1])\n    imdb = datasets.imdb.imdb(imdb_names, tmp.classes)\n  else:\n    imdb = get_imdb(imdb_names)\n  return imdb, roidb\n\n\nif __name__ == \'__main__\':\n  args = parse_args()\n\n  print(\'Called with args:\')\n  print(args)\n\n  if args.cfg_file is not None:\n    cfg_from_file(args.cfg_file)\n  if args.set_cfgs is not None:\n    cfg_from_list(args.set_cfgs)\n\n  print(\'Using config:\')\n  pprint.pprint(cfg)\n\n  np.random.seed(cfg.RNG_SEED)\n\n  # train set\n  imdb, roidb = combined_roidb(args.imdb_name)\n  print(\'{:d} roidb entries\'.format(len(roidb)))\n\n  # output directory where the models are saved\n  output_dir = get_output_dir(imdb, args.tag)\n  print(\'Output will be saved to `{:s}`\'.format(output_dir))\n\n  # tensorboard directory where the summaries are saved during training\n  tb_dir = get_output_tb_dir(imdb, args.tag)\n  print(\'TensorFlow summaries will be saved to `{:s}`\'.format(tb_dir))\n\n  # also add the validation set, but with no flipping images\n  orgflip = cfg.TRAIN.USE_FLIPPED\n  cfg.TRAIN.USE_FLIPPED = False\n  _, valroidb = combined_roidb(args.imdbval_name)\n  print(\'{:d} validation roidb entries\'.format(len(valroidb)))\n  cfg.TRAIN.USE_FLIPPED = orgflip\n\n  # load network\n  if args.net == \'vgg16\':\n    net = vgg16()\n  elif args.net == \'res50\':\n    net = resnetv1(num_layers=50)\n  elif args.net == \'res101\':\n    net = resnetv1(num_layers=101)\n  elif args.net == \'res152\':\n    net = resnetv1(num_layers=152)\n  elif args.net == \'mobile\':\n    net = mobilenetv1()\n  else:\n    raise NotImplementedError\n    \n  train_net(net, imdb, roidb, valroidb, output_dir, tb_dir,\n            pretrained_model=args.weight,\n            max_iters=args.max_iters)\n'"
lib/datasets/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n'
lib/datasets/coco.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Xinlei Chen\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datasets.imdb import imdb\nimport datasets.ds_utils as ds_utils\nfrom model.config import cfg\nimport os.path as osp\nimport sys\nimport os\nimport numpy as np\nimport scipy.sparse\nimport scipy.io as sio\nimport pickle\nimport json\nimport uuid\n# COCO API\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nfrom pycocotools import mask as COCOmask\n\nclass coco(imdb):\n  def __init__(self, image_set, year):\n    imdb.__init__(self, \'coco_\' + year + \'_\' + image_set)\n    # COCO specific config options\n    self.config = {\'use_salt\': True,\n                   \'cleanup\': True}\n    # name, paths\n    self._year = year\n    self._image_set = image_set\n    self._data_path = osp.join(cfg.DATA_DIR, \'coco\')\n    # load COCO API, classes, class <-> id mappings\n    self._COCO = COCO(self._get_ann_file())\n    cats = self._COCO.loadCats(self._COCO.getCatIds())\n    self._classes = tuple([\'__background__\'] + [c[\'name\'] for c in cats])\n    self._class_to_ind = dict(list(zip(self.classes, list(range(self.num_classes)))))\n    self._class_to_coco_cat_id = dict(list(zip([c[\'name\'] for c in cats],\n                                               self._COCO.getCatIds())))\n    self._image_index = self._load_image_set_index()\n    # Default to roidb handler\n    self.set_proposal_method(\'gt\')\n    self.competition_mode(False)\n\n    # Some image sets are ""views"" (i.e. subsets) into others.\n    # For example, minival2014 is a random 5000 image subset of val2014.\n    # This mapping tells us where the view\'s images and proposals come from.\n    self._view_map = {\n      \'minival2014\': \'val2014\',  # 5k val2014 subset\n      \'valminusminival2014\': \'val2014\',  # val2014 \\setminus minival2014\n      \'test-dev2015\': \'test2015\',\n    }\n    coco_name = image_set + year  # e.g., ""val2014""\n    self._data_name = (self._view_map[coco_name]\n                       if coco_name in self._view_map\n                       else coco_name)\n    # Dataset splits that have ground-truth annotations (test splits\n    # do not have gt annotations)\n    self._gt_splits = (\'train\', \'val\', \'minival\')\n\n  def _get_ann_file(self):\n    prefix = \'instances\' if self._image_set.find(\'test\') == -1 \\\n      else \'image_info\'\n    return osp.join(self._data_path, \'annotations\',\n                    prefix + \'_\' + self._image_set + self._year + \'.json\')\n\n  def _load_image_set_index(self):\n    """"""\n    Load image ids.\n    """"""\n    image_ids = self._COCO.getImgIds()\n    return image_ids\n\n  def _get_widths(self):\n    anns = self._COCO.loadImgs(self._image_index)\n    widths = [ann[\'width\'] for ann in anns]\n    return widths\n\n  def image_path_at(self, i):\n    """"""\n    Return the absolute path to image i in the image sequence.\n    """"""\n    return self.image_path_from_index(self._image_index[i])\n\n  def image_path_from_index(self, index):\n    """"""\n    Construct an image path from the image\'s ""index"" identifier.\n    """"""\n    # Example image path for index=119993:\n    #   images/train2014/COCO_train2014_000000119993.jpg\n    file_name = (\'COCO_\' + self._data_name + \'_\' +\n                 str(index).zfill(12) + \'.jpg\')\n    image_path = osp.join(self._data_path, \'images\',\n                          self._data_name, file_name)\n    assert osp.exists(image_path), \\\n      \'Path does not exist: {}\'.format(image_path)\n    return image_path\n\n  def gt_roidb(self):\n    """"""\n    Return the database of ground-truth regions of interest.\n    This function loads/saves from/to a cache file to speed up future calls.\n    """"""\n    cache_file = osp.join(self.cache_path, self.name + \'_gt_roidb.pkl\')\n    if osp.exists(cache_file):\n      with open(cache_file, \'rb\') as fid:\n        roidb = pickle.load(fid)\n      print(\'{} gt roidb loaded from {}\'.format(self.name, cache_file))\n      return roidb\n\n    gt_roidb = [self._load_coco_annotation(index)\n                for index in self._image_index]\n\n    with open(cache_file, \'wb\') as fid:\n      pickle.dump(gt_roidb, fid, pickle.HIGHEST_PROTOCOL)\n    print(\'wrote gt roidb to {}\'.format(cache_file))\n    return gt_roidb\n\n  def _load_coco_annotation(self, index):\n    """"""\n    Loads COCO bounding-box instance annotations. Crowd instances are\n    handled by marking their overlaps (with all categories) to -1. This\n    overlap value means that crowd ""instances"" are excluded from training.\n    """"""\n    im_ann = self._COCO.loadImgs(index)[0]\n    width = im_ann[\'width\']\n    height = im_ann[\'height\']\n\n    annIds = self._COCO.getAnnIds(imgIds=index, iscrowd=None)\n    objs = self._COCO.loadAnns(annIds)\n    # Sanitize bboxes -- some are invalid\n    valid_objs = []\n    for obj in objs:\n      x1 = np.max((0, obj[\'bbox\'][0]))\n      y1 = np.max((0, obj[\'bbox\'][1]))\n      x2 = np.min((width - 1, x1 + np.max((0, obj[\'bbox\'][2] - 1))))\n      y2 = np.min((height - 1, y1 + np.max((0, obj[\'bbox\'][3] - 1))))\n      if obj[\'area\'] > 0 and x2 >= x1 and y2 >= y1:\n        obj[\'clean_bbox\'] = [x1, y1, x2, y2]\n        valid_objs.append(obj)\n    objs = valid_objs\n    num_objs = len(objs)\n\n    boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n    gt_classes = np.zeros((num_objs), dtype=np.int32)\n    overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n    seg_areas = np.zeros((num_objs), dtype=np.float32)\n\n    # Lookup table to map from COCO category ids to our internal class\n    # indices\n    coco_cat_id_to_class_ind = dict([(self._class_to_coco_cat_id[cls],\n                                      self._class_to_ind[cls])\n                                     for cls in self._classes[1:]])\n\n    for ix, obj in enumerate(objs):\n      cls = coco_cat_id_to_class_ind[obj[\'category_id\']]\n      boxes[ix, :] = obj[\'clean_bbox\']\n      gt_classes[ix] = cls\n      seg_areas[ix] = obj[\'area\']\n      if obj[\'iscrowd\']:\n        # Set overlap to -1 for all classes for crowd objects\n        # so they will be excluded during training\n        overlaps[ix, :] = -1.0\n      else:\n        overlaps[ix, cls] = 1.0\n\n    ds_utils.validate_boxes(boxes, width=width, height=height)\n    overlaps = scipy.sparse.csr_matrix(overlaps)\n    return {\'width\': width,\n            \'height\': height,\n            \'boxes\': boxes,\n            \'gt_classes\': gt_classes,\n            \'gt_overlaps\': overlaps,\n            \'flipped\': False,\n            \'seg_areas\': seg_areas}\n\n  def _get_widths(self):\n    return [r[\'width\'] for r in self.roidb]\n\n  def append_flipped_images(self):\n    num_images = self.num_images\n    widths = self._get_widths()\n    for i in range(num_images):\n      boxes = self.roidb[i][\'boxes\'].copy()\n      oldx1 = boxes[:, 0].copy()\n      oldx2 = boxes[:, 2].copy()\n      boxes[:, 0] = widths[i] - oldx2 - 1\n      boxes[:, 2] = widths[i] - oldx1 - 1\n      assert (boxes[:, 2] >= boxes[:, 0]).all()\n      entry = {\'width\': widths[i],\n               \'height\': self.roidb[i][\'height\'],\n               \'boxes\': boxes,\n               \'gt_classes\': self.roidb[i][\'gt_classes\'],\n               \'gt_overlaps\': self.roidb[i][\'gt_overlaps\'],\n               \'flipped\': True,\n               \'seg_areas\': self.roidb[i][\'seg_areas\']}\n\n      self.roidb.append(entry)\n    self._image_index = self._image_index * 2\n\n  def _get_box_file(self, index):\n    # first 14 chars / first 22 chars / all chars + .mat\n    # COCO_val2014_0/COCO_val2014_000000447/COCO_val2014_000000447991.mat\n    file_name = (\'COCO_\' + self._data_name +\n                 \'_\' + str(index).zfill(12) + \'.mat\')\n    return osp.join(file_name[:14], file_name[:22], file_name)\n\n  def _print_detection_eval_metrics(self, coco_eval):\n    IoU_lo_thresh = 0.5\n    IoU_hi_thresh = 0.95\n\n    def _get_thr_ind(coco_eval, thr):\n      ind = np.where((coco_eval.params.iouThrs > thr - 1e-5) &\n                     (coco_eval.params.iouThrs < thr + 1e-5))[0][0]\n      iou_thr = coco_eval.params.iouThrs[ind]\n      assert np.isclose(iou_thr, thr)\n      return ind\n\n    ind_lo = _get_thr_ind(coco_eval, IoU_lo_thresh)\n    ind_hi = _get_thr_ind(coco_eval, IoU_hi_thresh)\n    # precision has dims (iou, recall, cls, area range, max dets)\n    # area range index 0: all area ranges\n    # max dets index 2: 100 per image\n    precision = \\\n      coco_eval.eval[\'precision\'][ind_lo:(ind_hi + 1), :, :, 0, 2]\n    ap_default = np.mean(precision[precision > -1])\n    print((\'~~~~ Mean and per-category AP @ IoU=[{:.2f},{:.2f}] \'\n           \'~~~~\').format(IoU_lo_thresh, IoU_hi_thresh))\n    print(\'{:.1f}\'.format(100 * ap_default))\n    for cls_ind, cls in enumerate(self.classes):\n      if cls == \'__background__\':\n        continue\n      # minus 1 because of __background__\n      precision = coco_eval.eval[\'precision\'][ind_lo:(ind_hi + 1), :, cls_ind - 1, 0, 2]\n      ap = np.mean(precision[precision > -1])\n      print(\'{:.1f}\'.format(100 * ap))\n\n    print(\'~~~~ Summary metrics ~~~~\')\n    coco_eval.summarize()\n\n  def _do_detection_eval(self, res_file, output_dir):\n    ann_type = \'bbox\'\n    coco_dt = self._COCO.loadRes(res_file)\n    coco_eval = COCOeval(self._COCO, coco_dt)\n    coco_eval.params.useSegm = (ann_type == \'segm\')\n    coco_eval.evaluate()\n    coco_eval.accumulate()\n    self._print_detection_eval_metrics(coco_eval)\n    eval_file = osp.join(output_dir, \'detection_results.pkl\')\n    with open(eval_file, \'wb\') as fid:\n      pickle.dump(coco_eval, fid, pickle.HIGHEST_PROTOCOL)\n    print(\'Wrote COCO eval results to: {}\'.format(eval_file))\n\n  def _coco_results_one_category(self, boxes, cat_id):\n    results = []\n    for im_ind, index in enumerate(self.image_index):\n      dets = boxes[im_ind].astype(np.float)\n      if dets == []:\n        continue\n      scores = dets[:, -1]\n      xs = dets[:, 0]\n      ys = dets[:, 1]\n      ws = dets[:, 2] - xs + 1\n      hs = dets[:, 3] - ys + 1\n      results.extend(\n        [{\'image_id\': index,\n          \'category_id\': cat_id,\n          \'bbox\': [xs[k], ys[k], ws[k], hs[k]],\n          \'score\': scores[k]} for k in range(dets.shape[0])])\n    return results\n\n  def _write_coco_results_file(self, all_boxes, res_file):\n    # [{""image_id"": 42,\n    #   ""category_id"": 18,\n    #   ""bbox"": [258.15,41.29,348.26,243.78],\n    #   ""score"": 0.236}, ...]\n    results = []\n    for cls_ind, cls in enumerate(self.classes):\n      if cls == \'__background__\':\n        continue\n      print(\'Collecting {} results ({:d}/{:d})\'.format(cls, cls_ind,\n                                                       self.num_classes - 1))\n      coco_cat_id = self._class_to_coco_cat_id[cls]\n      results.extend(self._coco_results_one_category(all_boxes[cls_ind],\n                                                     coco_cat_id))\n    print(\'Writing results json to {}\'.format(res_file))\n    with open(res_file, \'w\') as fid:\n      json.dump(results, fid)\n\n  def evaluate_detections(self, all_boxes, output_dir):\n    res_file = osp.join(output_dir, (\'detections_\' +\n                                     self._image_set +\n                                     self._year +\n                                     \'_results\'))\n    if self.config[\'use_salt\']:\n      res_file += \'_{}\'.format(str(uuid.uuid4()))\n    res_file += \'.json\'\n    self._write_coco_results_file(all_boxes, res_file)\n    # Only do evaluation on non-test sets\n    if self._image_set.find(\'test\') == -1:\n      self._do_detection_eval(res_file, output_dir)\n    # Optionally cleanup results json file\n    if self.config[\'cleanup\']:\n      os.remove(res_file)\n\n  def competition_mode(self, on):\n    if on:\n      self.config[\'use_salt\'] = False\n      self.config[\'cleanup\'] = False\n    else:\n      self.config[\'use_salt\'] = True\n      self.config[\'cleanup\'] = True\n'"
lib/datasets/ds_utils.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\n\ndef unique_boxes(boxes, scale=1.0):\n  """"""Return indices of unique boxes.""""""\n  v = np.array([1, 1e3, 1e6, 1e9])\n  hashes = np.round(boxes * scale).dot(v)\n  _, index = np.unique(hashes, return_index=True)\n  return np.sort(index)\n\n\ndef xywh_to_xyxy(boxes):\n  """"""Convert [x y w h] box format to [x1 y1 x2 y2] format.""""""\n  return np.hstack((boxes[:, 0:2], boxes[:, 0:2] + boxes[:, 2:4] - 1))\n\n\ndef xyxy_to_xywh(boxes):\n  """"""Convert [x1 y1 x2 y2] box format to [x y w h] format.""""""\n  return np.hstack((boxes[:, 0:2], boxes[:, 2:4] - boxes[:, 0:2] + 1))\n\n\ndef validate_boxes(boxes, width=0, height=0):\n  """"""Check that a set of boxes are valid.""""""\n  x1 = boxes[:, 0]\n  y1 = boxes[:, 1]\n  x2 = boxes[:, 2]\n  y2 = boxes[:, 3]\n  assert (x1 >= 0).all()\n  assert (y1 >= 0).all()\n  assert (x2 >= x1).all()\n  assert (y2 >= y1).all()\n  assert (x2 < width).all()\n  assert (y2 < height).all()\n\n\ndef filter_small_boxes(boxes, min_size):\n  w = boxes[:, 2] - boxes[:, 0]\n  h = boxes[:, 3] - boxes[:, 1]\n  keep = np.where((w >= min_size) & (h > min_size))[0]\n  return keep\n'"
lib/datasets/factory.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Factory method for easily getting imdbs by name.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\n__sets = {}\nfrom datasets.pascal_voc import pascal_voc\nfrom datasets.coco import coco\n\nimport numpy as np\n\n# Set up voc_<year>_<split> \nfor year in [\'2007\', \'2012\']:\n  for split in [\'train\', \'val\', \'trainval\', \'test\']:\n    name = \'voc_{}_{}\'.format(year, split)\n    __sets[name] = (lambda split=split, year=year: pascal_voc(split, year))\n\nfor year in [\'2007\', \'2012\']:\n  for split in [\'train\', \'val\', \'trainval\', \'test\']:\n    name = \'voc_{}_{}_diff\'.format(year, split)\n    __sets[name] = (lambda split=split, year=year: pascal_voc(split, year, use_diff=True))\n\n# Set up coco_2014_<split>\nfor year in [\'2014\']:\n  for split in [\'train\', \'val\', \'minival\', \'valminusminival\', \'trainval\']:\n    name = \'coco_{}_{}\'.format(year, split)\n    __sets[name] = (lambda split=split, year=year: coco(split, year))\n\n# Set up coco_2015_<split>\nfor year in [\'2015\']:\n  for split in [\'test\', \'test-dev\']:\n    name = \'coco_{}_{}\'.format(year, split)\n    __sets[name] = (lambda split=split, year=year: coco(split, year))\n\n\ndef get_imdb(name):\n  """"""Get an imdb (image database) by name.""""""\n  if name not in __sets:\n    raise KeyError(\'Unknown dataset: {}\'.format(name))\n  return __sets[name]()\n\n\ndef list_imdbs():\n  """"""List all registered imdbs.""""""\n  return list(__sets.keys())\n'"
lib/datasets/imdb.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Xinlei Chen\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport os.path as osp\nimport PIL\nfrom utils.cython_bbox import bbox_overlaps\nimport numpy as np\nimport scipy.sparse\nfrom model.config import cfg\n\n\nclass imdb(object):\n  """"""Image database.""""""\n\n  def __init__(self, name, classes=None):\n    self._name = name\n    self._num_classes = 0\n    if not classes:\n      self._classes = []\n    else:\n      self._classes = classes\n    self._image_index = []\n    self._obj_proposer = \'gt\'\n    self._roidb = None\n    self._roidb_handler = self.default_roidb\n    # Use this dict for storing dataset specific config options\n    self.config = {}\n\n  @property\n  def name(self):\n    return self._name\n\n  @property\n  def num_classes(self):\n    return len(self._classes)\n\n  @property\n  def classes(self):\n    return self._classes\n\n  @property\n  def image_index(self):\n    return self._image_index\n\n  @property\n  def roidb_handler(self):\n    return self._roidb_handler\n\n  @roidb_handler.setter\n  def roidb_handler(self, val):\n    self._roidb_handler = val\n\n  def set_proposal_method(self, method):\n    method = eval(\'self.\' + method + \'_roidb\')\n    self.roidb_handler = method\n\n  @property\n  def roidb(self):\n    # A roidb is a list of dictionaries, each with the following keys:\n    #   boxes\n    #   gt_overlaps\n    #   gt_classes\n    #   flipped\n    if self._roidb is not None:\n      return self._roidb\n    self._roidb = self.roidb_handler()\n    return self._roidb\n\n  @property\n  def cache_path(self):\n    cache_path = osp.abspath(osp.join(cfg.DATA_DIR, \'cache\'))\n    if not os.path.exists(cache_path):\n      os.makedirs(cache_path)\n    return cache_path\n\n  @property\n  def num_images(self):\n    return len(self.image_index)\n\n  def image_path_at(self, i):\n    raise NotImplementedError\n\n  def default_roidb(self):\n    raise NotImplementedError\n\n  def evaluate_detections(self, all_boxes, output_dir=None):\n    """"""\n    all_boxes is a list of length number-of-classes.\n    Each list element is a list of length number-of-images.\n    Each of those list elements is either an empty list []\n    or a numpy array of detection.\n\n    all_boxes[class][image] = [] or np.array of shape #dets x 5\n    """"""\n    raise NotImplementedError\n\n  def _get_widths(self):\n    return [PIL.Image.open(self.image_path_at(i)).size[0]\n            for i in range(self.num_images)]\n\n  def append_flipped_images(self):\n    num_images = self.num_images\n    widths = self._get_widths()\n    for i in range(num_images):\n      boxes = self.roidb[i][\'boxes\'].copy()\n      oldx1 = boxes[:, 0].copy()\n      oldx2 = boxes[:, 2].copy()\n      boxes[:, 0] = widths[i] - oldx2 - 1\n      boxes[:, 2] = widths[i] - oldx1 - 1\n      assert (boxes[:, 2] >= boxes[:, 0]).all()\n      entry = {\'boxes\': boxes,\n               \'gt_overlaps\': self.roidb[i][\'gt_overlaps\'],\n               \'gt_classes\': self.roidb[i][\'gt_classes\'],\n               \'flipped\': True}\n      self.roidb.append(entry)\n    self._image_index = self._image_index * 2\n\n  def evaluate_recall(self, candidate_boxes=None, thresholds=None,\n                      area=\'all\', limit=None):\n    """"""Evaluate detection proposal recall metrics.\n\n    Returns:\n        results: dictionary of results with keys\n            \'ar\': average recall\n            \'recalls\': vector recalls at each IoU overlap threshold\n            \'thresholds\': vector of IoU overlap thresholds\n            \'gt_overlaps\': vector of all ground-truth overlaps\n    """"""\n    # Record max overlap value for each gt box\n    # Return vector of overlap values\n    areas = {\'all\': 0, \'small\': 1, \'medium\': 2, \'large\': 3,\n             \'96-128\': 4, \'128-256\': 5, \'256-512\': 6, \'512-inf\': 7}\n    area_ranges = [[0 ** 2, 1e5 ** 2],  # all\n                   [0 ** 2, 32 ** 2],  # small\n                   [32 ** 2, 96 ** 2],  # medium\n                   [96 ** 2, 1e5 ** 2],  # large\n                   [96 ** 2, 128 ** 2],  # 96-128\n                   [128 ** 2, 256 ** 2],  # 128-256\n                   [256 ** 2, 512 ** 2],  # 256-512\n                   [512 ** 2, 1e5 ** 2],  # 512-inf\n                   ]\n    assert area in areas, \'unknown area range: {}\'.format(area)\n    area_range = area_ranges[areas[area]]\n    gt_overlaps = np.zeros(0)\n    num_pos = 0\n    for i in range(self.num_images):\n      # Checking for max_overlaps == 1 avoids including crowd annotations\n      # (...pretty hacking :/)\n      max_gt_overlaps = self.roidb[i][\'gt_overlaps\'].toarray().max(axis=1)\n      gt_inds = np.where((self.roidb[i][\'gt_classes\'] > 0) &\n                         (max_gt_overlaps == 1))[0]\n      gt_boxes = self.roidb[i][\'boxes\'][gt_inds, :]\n      gt_areas = self.roidb[i][\'seg_areas\'][gt_inds]\n      valid_gt_inds = np.where((gt_areas >= area_range[0]) &\n                               (gt_areas <= area_range[1]))[0]\n      gt_boxes = gt_boxes[valid_gt_inds, :]\n      num_pos += len(valid_gt_inds)\n\n      if candidate_boxes is None:\n        # If candidate_boxes is not supplied, the default is to use the\n        # non-ground-truth boxes from this roidb\n        non_gt_inds = np.where(self.roidb[i][\'gt_classes\'] == 0)[0]\n        boxes = self.roidb[i][\'boxes\'][non_gt_inds, :]\n      else:\n        boxes = candidate_boxes[i]\n      if boxes.shape[0] == 0:\n        continue\n      if limit is not None and boxes.shape[0] > limit:\n        boxes = boxes[:limit, :]\n\n      overlaps = bbox_overlaps(boxes.astype(np.float),\n                               gt_boxes.astype(np.float))\n\n      _gt_overlaps = np.zeros((gt_boxes.shape[0]))\n      for j in range(gt_boxes.shape[0]):\n        # find which proposal box maximally covers each gt box\n        argmax_overlaps = overlaps.argmax(axis=0)\n        # and get the iou amount of coverage for each gt box\n        max_overlaps = overlaps.max(axis=0)\n        # find which gt box is \'best\' covered (i.e. \'best\' = most iou)\n        gt_ind = max_overlaps.argmax()\n        gt_ovr = max_overlaps.max()\n        assert (gt_ovr >= 0)\n        # find the proposal box that covers the best covered gt box\n        box_ind = argmax_overlaps[gt_ind]\n        # record the iou coverage of this gt box\n        _gt_overlaps[j] = overlaps[box_ind, gt_ind]\n        assert (_gt_overlaps[j] == gt_ovr)\n        # mark the proposal box and the gt box as used\n        overlaps[box_ind, :] = -1\n        overlaps[:, gt_ind] = -1\n      # append recorded iou coverage level\n      gt_overlaps = np.hstack((gt_overlaps, _gt_overlaps))\n\n    gt_overlaps = np.sort(gt_overlaps)\n    if thresholds is None:\n      step = 0.05\n      thresholds = np.arange(0.5, 0.95 + 1e-5, step)\n    recalls = np.zeros_like(thresholds)\n    # compute recall for each iou threshold\n    for i, t in enumerate(thresholds):\n      recalls[i] = (gt_overlaps >= t).sum() / float(num_pos)\n    # ar = 2 * np.trapz(recalls, thresholds)\n    ar = recalls.mean()\n    return {\'ar\': ar, \'recalls\': recalls, \'thresholds\': thresholds,\n            \'gt_overlaps\': gt_overlaps}\n\n  def create_roidb_from_box_list(self, box_list, gt_roidb):\n    assert len(box_list) == self.num_images, \\\n      \'Number of boxes must match number of ground-truth images\'\n    roidb = []\n    for i in range(self.num_images):\n      boxes = box_list[i]\n      num_boxes = boxes.shape[0]\n      overlaps = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n\n      if gt_roidb is not None and gt_roidb[i][\'boxes\'].size > 0:\n        gt_boxes = gt_roidb[i][\'boxes\']\n        gt_classes = gt_roidb[i][\'gt_classes\']\n        gt_overlaps = bbox_overlaps(boxes.astype(np.float),\n                                    gt_boxes.astype(np.float))\n        argmaxes = gt_overlaps.argmax(axis=1)\n        maxes = gt_overlaps.max(axis=1)\n        I = np.where(maxes > 0)[0]\n        overlaps[I, gt_classes[argmaxes[I]]] = maxes[I]\n\n      overlaps = scipy.sparse.csr_matrix(overlaps)\n      roidb.append({\n        \'boxes\': boxes,\n        \'gt_classes\': np.zeros((num_boxes,), dtype=np.int32),\n        \'gt_overlaps\': overlaps,\n        \'flipped\': False,\n        \'seg_areas\': np.zeros((num_boxes,), dtype=np.float32),\n      })\n    return roidb\n\n  @staticmethod\n  def merge_roidbs(a, b):\n    assert len(a) == len(b)\n    for i in range(len(a)):\n      a[i][\'boxes\'] = np.vstack((a[i][\'boxes\'], b[i][\'boxes\']))\n      a[i][\'gt_classes\'] = np.hstack((a[i][\'gt_classes\'],\n                                      b[i][\'gt_classes\']))\n      a[i][\'gt_overlaps\'] = scipy.sparse.vstack([a[i][\'gt_overlaps\'],\n                                                 b[i][\'gt_overlaps\']])\n      a[i][\'seg_areas\'] = np.hstack((a[i][\'seg_areas\'],\n                                     b[i][\'seg_areas\']))\n    return a\n\n  def competition_mode(self, on):\n    """"""Turn competition mode on or off.""""""\n    pass\n'"
lib/datasets/pascal_voc.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Xinlei Chen\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom datasets.imdb import imdb\nimport datasets.ds_utils as ds_utils\nimport xml.etree.ElementTree as ET\nimport numpy as np\nimport scipy.sparse\nimport scipy.io as sio\nimport utils.cython_bbox\nimport pickle\nimport subprocess\nimport uuid\nfrom .voc_eval import voc_eval\nfrom model.config import cfg\n\n\nclass pascal_voc(imdb):\n  def __init__(self, image_set, year, use_diff=False):\n    name = \'voc_\' + year + \'_\' + image_set\n    if use_diff:\n      name += \'_diff\'\n    imdb.__init__(self, name)\n    self._year = year\n    self._image_set = image_set\n    self._devkit_path = self._get_default_path()\n    self._data_path = os.path.join(self._devkit_path, \'VOC\' + self._year)\n    self._classes = (\'__background__\',  # always index 0\n                     \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n                     \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n                     \'cow\', \'diningtable\', \'dog\', \'horse\',\n                     \'motorbike\', \'person\', \'pottedplant\',\n                     \'sheep\', \'sofa\', \'train\', \'tvmonitor\')\n    self._class_to_ind = dict(list(zip(self.classes, list(range(self.num_classes)))))\n    self._image_ext = \'.jpg\'\n    self._image_index = self._load_image_set_index()\n    # Default to roidb handler\n    self._roidb_handler = self.gt_roidb\n    self._salt = str(uuid.uuid4())\n    self._comp_id = \'comp4\'\n\n    # PASCAL specific config options\n    self.config = {\'cleanup\': True,\n                   \'use_salt\': True,\n                   \'use_diff\': use_diff,\n                   \'matlab_eval\': False,\n                   \'rpn_file\': None}\n\n    assert os.path.exists(self._devkit_path), \\\n      \'VOCdevkit path does not exist: {}\'.format(self._devkit_path)\n    assert os.path.exists(self._data_path), \\\n      \'Path does not exist: {}\'.format(self._data_path)\n\n  def image_path_at(self, i):\n    """"""\n    Return the absolute path to image i in the image sequence.\n    """"""\n    return self.image_path_from_index(self._image_index[i])\n\n  def image_path_from_index(self, index):\n    """"""\n    Construct an image path from the image\'s ""index"" identifier.\n    """"""\n    image_path = os.path.join(self._data_path, \'JPEGImages\',\n                              index + self._image_ext)\n    assert os.path.exists(image_path), \\\n      \'Path does not exist: {}\'.format(image_path)\n    return image_path\n\n  def _load_image_set_index(self):\n    """"""\n    Load the indexes listed in this dataset\'s image set file.\n    """"""\n    # Example path to image set file:\n    # self._devkit_path + /VOCdevkit2007/VOC2007/ImageSets/Main/val.txt\n    image_set_file = os.path.join(self._data_path, \'ImageSets\', \'Main\',\n                                  self._image_set + \'.txt\')\n    assert os.path.exists(image_set_file), \\\n      \'Path does not exist: {}\'.format(image_set_file)\n    with open(image_set_file) as f:\n      image_index = [x.strip() for x in f.readlines()]\n    return image_index\n\n  def _get_default_path(self):\n    """"""\n    Return the default path where PASCAL VOC is expected to be installed.\n    """"""\n    return os.path.join(cfg.DATA_DIR, \'VOCdevkit\' + self._year)\n\n  def gt_roidb(self):\n    """"""\n    Return the database of ground-truth regions of interest.\n\n    This function loads/saves from/to a cache file to speed up future calls.\n    """"""\n    cache_file = os.path.join(self.cache_path, self.name + \'_gt_roidb.pkl\')\n    if os.path.exists(cache_file):\n      with open(cache_file, \'rb\') as fid:\n        try:\n          roidb = pickle.load(fid)\n        except:\n          roidb = pickle.load(fid, encoding=\'bytes\')\n      print(\'{} gt roidb loaded from {}\'.format(self.name, cache_file))\n      return roidb\n\n    gt_roidb = [self._load_pascal_annotation(index)\n                for index in self.image_index]\n    with open(cache_file, \'wb\') as fid:\n      pickle.dump(gt_roidb, fid, pickle.HIGHEST_PROTOCOL)\n    print(\'wrote gt roidb to {}\'.format(cache_file))\n\n    return gt_roidb\n\n  def rpn_roidb(self):\n    if int(self._year) == 2007 or self._image_set != \'test\':\n      gt_roidb = self.gt_roidb()\n      rpn_roidb = self._load_rpn_roidb(gt_roidb)\n      roidb = imdb.merge_roidbs(gt_roidb, rpn_roidb)\n    else:\n      roidb = self._load_rpn_roidb(None)\n\n    return roidb\n\n  def _load_rpn_roidb(self, gt_roidb):\n    filename = self.config[\'rpn_file\']\n    print(\'loading {}\'.format(filename))\n    assert os.path.exists(filename), \\\n      \'rpn data not found at: {}\'.format(filename)\n    with open(filename, \'rb\') as f:\n      box_list = pickle.load(f)\n    return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n  def _load_pascal_annotation(self, index):\n    """"""\n    Load image and bounding boxes info from XML file in the PASCAL VOC\n    format.\n    """"""\n    filename = os.path.join(self._data_path, \'Annotations\', index + \'.xml\')\n    tree = ET.parse(filename)\n    objs = tree.findall(\'object\')\n    if not self.config[\'use_diff\']:\n      # Exclude the samples labeled as difficult\n      non_diff_objs = [\n        obj for obj in objs if int(obj.find(\'difficult\').text) == 0]\n      # if len(non_diff_objs) != len(objs):\n      #     print \'Removed {} difficult objects\'.format(\n      #         len(objs) - len(non_diff_objs))\n      objs = non_diff_objs\n    num_objs = len(objs)\n\n    boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n    gt_classes = np.zeros((num_objs), dtype=np.int32)\n    overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n    # ""Seg"" area for pascal is just the box area\n    seg_areas = np.zeros((num_objs), dtype=np.float32)\n\n    # Load object bounding boxes into a data frame.\n    for ix, obj in enumerate(objs):\n      bbox = obj.find(\'bndbox\')\n      # Make pixel indexes 0-based\n      x1 = float(bbox.find(\'xmin\').text) - 1\n      y1 = float(bbox.find(\'ymin\').text) - 1\n      x2 = float(bbox.find(\'xmax\').text) - 1\n      y2 = float(bbox.find(\'ymax\').text) - 1\n      cls = self._class_to_ind[obj.find(\'name\').text.lower().strip()]\n      boxes[ix, :] = [x1, y1, x2, y2]\n      gt_classes[ix] = cls\n      overlaps[ix, cls] = 1.0\n      seg_areas[ix] = (x2 - x1 + 1) * (y2 - y1 + 1)\n\n    overlaps = scipy.sparse.csr_matrix(overlaps)\n\n    return {\'boxes\': boxes,\n            \'gt_classes\': gt_classes,\n            \'gt_overlaps\': overlaps,\n            \'flipped\': False,\n            \'seg_areas\': seg_areas}\n\n  def _get_comp_id(self):\n    comp_id = (self._comp_id + \'_\' + self._salt if self.config[\'use_salt\']\n               else self._comp_id)\n    return comp_id\n\n  def _get_voc_results_file_template(self):\n    # VOCdevkit/results/VOC2007/Main/<comp_id>_det_test_aeroplane.txt\n    filename = self._get_comp_id() + \'_det_\' + self._image_set + \'_{:s}.txt\'\n    path = os.path.join(\n      self._devkit_path,\n      \'results\',\n      \'VOC\' + self._year,\n      \'Main\',\n      filename)\n    return path\n\n  def _write_voc_results_file(self, all_boxes):\n    for cls_ind, cls in enumerate(self.classes):\n      if cls == \'__background__\':\n        continue\n      print(\'Writing {} VOC results file\'.format(cls))\n      filename = self._get_voc_results_file_template().format(cls)\n      with open(filename, \'wt\') as f:\n        for im_ind, index in enumerate(self.image_index):\n          dets = all_boxes[cls_ind][im_ind]\n          if dets == []:\n            continue\n          # the VOCdevkit expects 1-based indices\n          for k in range(dets.shape[0]):\n            f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                    format(index, dets[k, -1],\n                           dets[k, 0] + 1, dets[k, 1] + 1,\n                           dets[k, 2] + 1, dets[k, 3] + 1))\n\n  def _do_python_eval(self, output_dir=\'output\'):\n    annopath = os.path.join(\n      self._devkit_path,\n      \'VOC\' + self._year,\n      \'Annotations\',\n      \'{:s}.xml\')\n    imagesetfile = os.path.join(\n      self._devkit_path,\n      \'VOC\' + self._year,\n      \'ImageSets\',\n      \'Main\',\n      self._image_set + \'.txt\')\n    cachedir = os.path.join(self._devkit_path, \'annotations_cache\')\n    aps = []\n    # The PASCAL VOC metric changed in 2010\n    use_07_metric = True if int(self._year) < 2010 else False\n    print(\'VOC07 metric? \' + (\'Yes\' if use_07_metric else \'No\'))\n    if not os.path.isdir(output_dir):\n      os.mkdir(output_dir)\n    for i, cls in enumerate(self._classes):\n      if cls == \'__background__\':\n        continue\n      filename = self._get_voc_results_file_template().format(cls)\n      rec, prec, ap = voc_eval(\n        filename, annopath, imagesetfile, cls, cachedir, ovthresh=0.5,\n        use_07_metric=use_07_metric, use_diff=self.config[\'use_diff\'])\n      aps += [ap]\n      print((\'AP for {} = {:.4f}\'.format(cls, ap)))\n      with open(os.path.join(output_dir, cls + \'_pr.pkl\'), \'wb\') as f:\n        pickle.dump({\'rec\': rec, \'prec\': prec, \'ap\': ap}, f)\n    print((\'Mean AP = {:.4f}\'.format(np.mean(aps))))\n    print(\'~~~~~~~~\')\n    print(\'Results:\')\n    for ap in aps:\n      print((\'{:.3f}\'.format(ap)))\n    print((\'{:.3f}\'.format(np.mean(aps))))\n    print(\'~~~~~~~~\')\n    print(\'\')\n    print(\'--------------------------------------------------------------\')\n    print(\'Results computed with the **unofficial** Python eval code.\')\n    print(\'Results should be very close to the official MATLAB eval code.\')\n    print(\'Recompute with `./tools/reval.py --matlab ...` for your paper.\')\n    print(\'-- Thanks, The Management\')\n    print(\'--------------------------------------------------------------\')\n\n  def _do_matlab_eval(self, output_dir=\'output\'):\n    print(\'-----------------------------------------------------\')\n    print(\'Computing results with the official MATLAB eval code.\')\n    print(\'-----------------------------------------------------\')\n    path = os.path.join(cfg.ROOT_DIR, \'lib\', \'datasets\',\n                        \'VOCdevkit-matlab-wrapper\')\n    cmd = \'cd {} && \'.format(path)\n    cmd += \'{:s} -nodisplay -nodesktop \'.format(cfg.MATLAB)\n    cmd += \'-r ""dbstop if error; \'\n    cmd += \'voc_eval(\\\'{:s}\\\',\\\'{:s}\\\',\\\'{:s}\\\',\\\'{:s}\\\'); quit;""\' \\\n      .format(self._devkit_path, self._get_comp_id(),\n              self._image_set, output_dir)\n    print((\'Running:\\n{}\'.format(cmd)))\n    status = subprocess.call(cmd, shell=True)\n\n  def evaluate_detections(self, all_boxes, output_dir):\n    self._write_voc_results_file(all_boxes)\n    self._do_python_eval(output_dir)\n    if self.config[\'matlab_eval\']:\n      self._do_matlab_eval(output_dir)\n    if self.config[\'cleanup\']:\n      for cls in self._classes:\n        if cls == \'__background__\':\n          continue\n        filename = self._get_voc_results_file_template().format(cls)\n        os.remove(filename)\n\n  def competition_mode(self, on):\n    if on:\n      self.config[\'use_salt\'] = False\n      self.config[\'cleanup\'] = False\n    else:\n      self.config[\'use_salt\'] = True\n      self.config[\'cleanup\'] = True\n\n\nif __name__ == \'__main__\':\n  from datasets.pascal_voc import pascal_voc\n\n  d = pascal_voc(\'trainval\', \'2007\')\n  res = d.roidb\n  from IPython import embed;\n\n  embed()\n'"
lib/datasets/voc_eval.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Bharath Hariharan\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport xml.etree.ElementTree as ET\nimport os\nimport pickle\nimport numpy as np\n\ndef parse_rec(filename):\n  """""" Parse a PASCAL VOC xml file """"""\n  tree = ET.parse(filename)\n  objects = []\n  for obj in tree.findall(\'object\'):\n    obj_struct = {}\n    obj_struct[\'name\'] = obj.find(\'name\').text\n    obj_struct[\'pose\'] = obj.find(\'pose\').text\n    obj_struct[\'truncated\'] = int(obj.find(\'truncated\').text)\n    obj_struct[\'difficult\'] = int(obj.find(\'difficult\').text)\n    bbox = obj.find(\'bndbox\')\n    obj_struct[\'bbox\'] = [int(bbox.find(\'xmin\').text),\n                          int(bbox.find(\'ymin\').text),\n                          int(bbox.find(\'xmax\').text),\n                          int(bbox.find(\'ymax\').text)]\n    objects.append(obj_struct)\n\n  return objects\n\n\ndef voc_ap(rec, prec, use_07_metric=False):\n  """""" ap = voc_ap(rec, prec, [use_07_metric])\n  Compute VOC AP given precision and recall.\n  If use_07_metric is true, uses the\n  VOC 07 11 point method (default:False).\n  """"""\n  if use_07_metric:\n    # 11 point metric\n    ap = 0.\n    for t in np.arange(0., 1.1, 0.1):\n      if np.sum(rec >= t) == 0:\n        p = 0\n      else:\n        p = np.max(prec[rec >= t])\n      ap = ap + p / 11.\n  else:\n    # correct AP calculation\n    # first append sentinel values at the end\n    mrec = np.concatenate(([0.], rec, [1.]))\n    mpre = np.concatenate(([0.], prec, [0.]))\n\n    # compute the precision envelope\n    for i in range(mpre.size - 1, 0, -1):\n      mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n    # to calculate area under PR curve, look for points\n    # where X axis (recall) changes value\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n\n    # and sum (\\Delta recall) * prec\n    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n  return ap\n\n\ndef voc_eval(detpath,\n             annopath,\n             imagesetfile,\n             classname,\n             cachedir,\n             ovthresh=0.5,\n             use_07_metric=False,\n             use_diff=False):\n  """"""rec, prec, ap = voc_eval(detpath,\n                              annopath,\n                              imagesetfile,\n                              classname,\n                              [ovthresh],\n                              [use_07_metric])\n\n  Top level function that does the PASCAL VOC evaluation.\n\n  detpath: Path to detections\n      detpath.format(classname) should produce the detection results file.\n  annopath: Path to annotations\n      annopath.format(imagename) should be the xml annotations file.\n  imagesetfile: Text file containing the list of images, one image per line.\n  classname: Category name (duh)\n  cachedir: Directory for caching the annotations\n  [ovthresh]: Overlap threshold (default = 0.5)\n  [use_07_metric]: Whether to use VOC07\'s 11 point AP computation\n      (default False)\n  """"""\n  # assumes detections are in detpath.format(classname)\n  # assumes annotations are in annopath.format(imagename)\n  # assumes imagesetfile is a text file with each line an image name\n  # cachedir caches the annotations in a pickle file\n\n  # first load gt\n  if not os.path.isdir(cachedir):\n    os.mkdir(cachedir)\n  cachefile = os.path.join(cachedir, \'%s_annots.pkl\' % imagesetfile)\n  # read list of images\n  with open(imagesetfile, \'r\') as f:\n    lines = f.readlines()\n  imagenames = [x.strip() for x in lines]\n\n  if not os.path.isfile(cachefile):\n    # load annotations\n    recs = {}\n    for i, imagename in enumerate(imagenames):\n      recs[imagename] = parse_rec(annopath.format(imagename))\n      if i % 100 == 0:\n        print(\'Reading annotation for {:d}/{:d}\'.format(\n          i + 1, len(imagenames)))\n    # save\n    print(\'Saving cached annotations to {:s}\'.format(cachefile))\n    with open(cachefile, \'w\') as f:\n      pickle.dump(recs, f)\n  else:\n    # load\n    with open(cachefile, \'rb\') as f:\n      try:\n        recs = pickle.load(f)\n      except:\n        recs = pickle.load(f, encoding=\'bytes\')\n\n  # extract gt objects for this class\n  class_recs = {}\n  npos = 0\n  for imagename in imagenames:\n    R = [obj for obj in recs[imagename] if obj[\'name\'] == classname]\n    bbox = np.array([x[\'bbox\'] for x in R])\n    if use_diff:\n      difficult = np.array([False for x in R]).astype(np.bool)\n    else:\n      difficult = np.array([x[\'difficult\'] for x in R]).astype(np.bool)\n    det = [False] * len(R)\n    npos = npos + sum(~difficult)\n    class_recs[imagename] = {\'bbox\': bbox,\n                             \'difficult\': difficult,\n                             \'det\': det}\n\n  # read dets\n  detfile = detpath.format(classname)\n  with open(detfile, \'r\') as f:\n    lines = f.readlines()\n\n  splitlines = [x.strip().split(\' \') for x in lines]\n  image_ids = [x[0] for x in splitlines]\n  confidence = np.array([float(x[1]) for x in splitlines])\n  BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n\n  nd = len(image_ids)\n  tp = np.zeros(nd)\n  fp = np.zeros(nd)\n\n  if BB.shape[0] > 0:\n    # sort by confidence\n    sorted_ind = np.argsort(-confidence)\n    sorted_scores = np.sort(-confidence)\n    BB = BB[sorted_ind, :]\n    image_ids = [image_ids[x] for x in sorted_ind]\n\n    # go down dets and mark TPs and FPs\n    for d in range(nd):\n      R = class_recs[image_ids[d]]\n      bb = BB[d, :].astype(float)\n      ovmax = -np.inf\n      BBGT = R[\'bbox\'].astype(float)\n\n      if BBGT.size > 0:\n        # compute overlaps\n        # intersection\n        ixmin = np.maximum(BBGT[:, 0], bb[0])\n        iymin = np.maximum(BBGT[:, 1], bb[1])\n        ixmax = np.minimum(BBGT[:, 2], bb[2])\n        iymax = np.minimum(BBGT[:, 3], bb[3])\n        iw = np.maximum(ixmax - ixmin + 1., 0.)\n        ih = np.maximum(iymax - iymin + 1., 0.)\n        inters = iw * ih\n\n        # union\n        uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n               (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n               (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n\n        overlaps = inters / uni\n        ovmax = np.max(overlaps)\n        jmax = np.argmax(overlaps)\n\n      if ovmax > ovthresh:\n        if not R[\'difficult\'][jmax]:\n          if not R[\'det\'][jmax]:\n            tp[d] = 1.\n            R[\'det\'][jmax] = 1\n          else:\n            fp[d] = 1.\n      else:\n        fp[d] = 1.\n\n  # compute precision recall\n  fp = np.cumsum(fp)\n  tp = np.cumsum(tp)\n  rec = tp / float(npos)\n  # avoid divide by zero in case the first detection matches a difficult\n  # ground truth\n  prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n  ap = voc_ap(rec, prec, use_07_metric)\n\n  return rec, prec, ap\n'"
lib/layer_utils/__init__.py,0,b''
lib/layer_utils/anchor_target_layer.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Xinlei Chen\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom model.config import cfg\nimport numpy as np\nimport numpy.random as npr\nfrom utils.cython_bbox import bbox_overlaps\nfrom model.bbox_transform import bbox_transform\n\ndef anchor_target_layer(rpn_cls_score, gt_boxes, im_info, _feat_stride, all_anchors, num_anchors):\n  """"""Same as the anchor target layer in original Fast/er RCNN """"""\n  A = num_anchors\n  total_anchors = all_anchors.shape[0]\n  K = total_anchors / num_anchors\n\n  # allow boxes to sit over the edge by a small amount\n  _allowed_border = 0\n\n  # map of shape (..., H, W)\n  height, width = rpn_cls_score.shape[1:3]\n\n  # only keep anchors inside the image\n  inds_inside = np.where(\n    (all_anchors[:, 0] >= -_allowed_border) &\n    (all_anchors[:, 1] >= -_allowed_border) &\n    (all_anchors[:, 2] < im_info[1] + _allowed_border) &  # width\n    (all_anchors[:, 3] < im_info[0] + _allowed_border)  # height\n  )[0]\n\n  # keep only inside anchors\n  anchors = all_anchors[inds_inside, :]\n\n  # label: 1 is positive, 0 is negative, -1 is dont care\n  labels = np.empty((len(inds_inside),), dtype=np.float32)\n  labels.fill(-1)\n\n  # overlaps between the anchors and the gt boxes\n  # overlaps (ex, gt)\n  overlaps = bbox_overlaps(\n    np.ascontiguousarray(anchors, dtype=np.float),\n    np.ascontiguousarray(gt_boxes, dtype=np.float))\n  argmax_overlaps = overlaps.argmax(axis=1)\n  max_overlaps = overlaps[np.arange(len(inds_inside)), argmax_overlaps]\n  gt_argmax_overlaps = overlaps.argmax(axis=0)\n  gt_max_overlaps = overlaps[gt_argmax_overlaps,\n                             np.arange(overlaps.shape[1])]\n  gt_argmax_overlaps = np.where(overlaps == gt_max_overlaps)[0]\n\n  if not cfg.TRAIN.RPN_CLOBBER_POSITIVES:\n    # assign bg labels first so that positive labels can clobber them\n    # first set the negatives\n    labels[max_overlaps < cfg.TRAIN.RPN_NEGATIVE_OVERLAP] = 0\n\n  # fg label: for each gt, anchor with highest overlap\n  labels[gt_argmax_overlaps] = 1\n\n  # fg label: above threshold IOU\n  labels[max_overlaps >= cfg.TRAIN.RPN_POSITIVE_OVERLAP] = 1\n\n  if cfg.TRAIN.RPN_CLOBBER_POSITIVES:\n    # assign bg labels last so that negative labels can clobber positives\n    labels[max_overlaps < cfg.TRAIN.RPN_NEGATIVE_OVERLAP] = 0\n\n  # subsample positive labels if we have too many\n  num_fg = int(cfg.TRAIN.RPN_FG_FRACTION * cfg.TRAIN.RPN_BATCHSIZE)\n  fg_inds = np.where(labels == 1)[0]\n  if len(fg_inds) > num_fg:\n    disable_inds = npr.choice(\n      fg_inds, size=(len(fg_inds) - num_fg), replace=False)\n    labels[disable_inds] = -1\n\n  # subsample negative labels if we have too many\n  num_bg = cfg.TRAIN.RPN_BATCHSIZE - np.sum(labels == 1)\n  bg_inds = np.where(labels == 0)[0]\n  if len(bg_inds) > num_bg:\n    disable_inds = npr.choice(\n      bg_inds, size=(len(bg_inds) - num_bg), replace=False)\n    labels[disable_inds] = -1\n\n  bbox_targets = np.zeros((len(inds_inside), 4), dtype=np.float32)\n  bbox_targets = _compute_targets(anchors, gt_boxes[argmax_overlaps, :])\n\n  bbox_inside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n  # only the positive ones have regression targets\n  bbox_inside_weights[labels == 1, :] = np.array(cfg.TRAIN.RPN_BBOX_INSIDE_WEIGHTS)\n\n  bbox_outside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n  if cfg.TRAIN.RPN_POSITIVE_WEIGHT < 0:\n    # uniform weighting of examples (given non-uniform sampling)\n    num_examples = np.sum(labels >= 0)\n    positive_weights = np.ones((1, 4)) * 1.0 / num_examples\n    negative_weights = np.ones((1, 4)) * 1.0 / num_examples\n  else:\n    assert ((cfg.TRAIN.RPN_POSITIVE_WEIGHT > 0) &\n            (cfg.TRAIN.RPN_POSITIVE_WEIGHT < 1))\n    positive_weights = (cfg.TRAIN.RPN_POSITIVE_WEIGHT /\n                        np.sum(labels == 1))\n    negative_weights = ((1.0 - cfg.TRAIN.RPN_POSITIVE_WEIGHT) /\n                        np.sum(labels == 0))\n  bbox_outside_weights[labels == 1, :] = positive_weights\n  bbox_outside_weights[labels == 0, :] = negative_weights\n\n  # map up to original set of anchors\n  labels = _unmap(labels, total_anchors, inds_inside, fill=-1)\n  bbox_targets = _unmap(bbox_targets, total_anchors, inds_inside, fill=0)\n  bbox_inside_weights = _unmap(bbox_inside_weights, total_anchors, inds_inside, fill=0)\n  bbox_outside_weights = _unmap(bbox_outside_weights, total_anchors, inds_inside, fill=0)\n\n  # labels\n  labels = labels.reshape((1, height, width, A)).transpose(0, 3, 1, 2)\n  labels = labels.reshape((1, 1, A * height, width))\n  rpn_labels = labels\n\n  # bbox_targets\n  bbox_targets = bbox_targets \\\n    .reshape((1, height, width, A * 4))\n\n  rpn_bbox_targets = bbox_targets\n  # bbox_inside_weights\n  bbox_inside_weights = bbox_inside_weights \\\n    .reshape((1, height, width, A * 4))\n\n  rpn_bbox_inside_weights = bbox_inside_weights\n\n  # bbox_outside_weights\n  bbox_outside_weights = bbox_outside_weights \\\n    .reshape((1, height, width, A * 4))\n\n  rpn_bbox_outside_weights = bbox_outside_weights\n  return rpn_labels, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights\n\n\ndef _unmap(data, count, inds, fill=0):\n  """""" Unmap a subset of item (data) back to the original set of items (of\n  size count) """"""\n  if len(data.shape) == 1:\n    ret = np.empty((count,), dtype=np.float32)\n    ret.fill(fill)\n    ret[inds] = data\n  else:\n    ret = np.empty((count,) + data.shape[1:], dtype=np.float32)\n    ret.fill(fill)\n    ret[inds, :] = data\n  return ret\n\n\ndef _compute_targets(ex_rois, gt_rois):\n  """"""Compute bounding-box regression targets for an image.""""""\n\n  assert ex_rois.shape[0] == gt_rois.shape[0]\n  assert ex_rois.shape[1] == 4\n  assert gt_rois.shape[1] == 5\n\n  return bbox_transform(ex_rois, gt_rois[:, :4]).astype(np.float32, copy=False)\n'"
lib/layer_utils/generate_anchors.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\n\n# Verify that we compute the same anchors as Shaoqing\'s matlab implementation:\n#\n#    >> load output/rpn_cachedir/faster_rcnn_VOC2007_ZF_stage1_rpn/anchors.mat\n#    >> anchors\n#\n#    anchors =\n#\n#       -83   -39   100    56\n#      -175   -87   192   104\n#      -359  -183   376   200\n#       -55   -55    72    72\n#      -119  -119   136   136\n#      -247  -247   264   264\n#       -35   -79    52    96\n#       -79  -167    96   184\n#      -167  -343   184   360\n\n# array([[ -83.,  -39.,  100.,   56.],\n#       [-175.,  -87.,  192.,  104.],\n#       [-359., -183.,  376.,  200.],\n#       [ -55.,  -55.,   72.,   72.],\n#       [-119., -119.,  136.,  136.],\n#       [-247., -247.,  264.,  264.],\n#       [ -35.,  -79.,   52.,   96.],\n#       [ -79., -167.,   96.,  184.],\n#       [-167., -343.,  184.,  360.]])\n\ndef generate_anchors(base_size=16, ratios=[0.5, 1, 2],\n                     scales=2 ** np.arange(3, 6)):\n  """"""\n  Generate anchor (reference) windows by enumerating aspect ratios X\n  scales wrt a reference (0, 0, 15, 15) window.\n  """"""\n\n  base_anchor = np.array([1, 1, base_size, base_size]) - 1\n  ratio_anchors = _ratio_enum(base_anchor, ratios)\n  anchors = np.vstack([_scale_enum(ratio_anchors[i, :], scales)\n                       for i in range(ratio_anchors.shape[0])])\n  return anchors\n\n\ndef _whctrs(anchor):\n  """"""\n  Return width, height, x center, and y center for an anchor (window).\n  """"""\n\n  w = anchor[2] - anchor[0] + 1\n  h = anchor[3] - anchor[1] + 1\n  x_ctr = anchor[0] + 0.5 * (w - 1)\n  y_ctr = anchor[1] + 0.5 * (h - 1)\n  return w, h, x_ctr, y_ctr\n\n\ndef _mkanchors(ws, hs, x_ctr, y_ctr):\n  """"""\n  Given a vector of widths (ws) and heights (hs) around a center\n  (x_ctr, y_ctr), output a set of anchors (windows).\n  """"""\n\n  ws = ws[:, np.newaxis]\n  hs = hs[:, np.newaxis]\n  anchors = np.hstack((x_ctr - 0.5 * (ws - 1),\n                       y_ctr - 0.5 * (hs - 1),\n                       x_ctr + 0.5 * (ws - 1),\n                       y_ctr + 0.5 * (hs - 1)))\n  return anchors\n\n\ndef _ratio_enum(anchor, ratios):\n  """"""\n  Enumerate a set of anchors for each aspect ratio wrt an anchor.\n  """"""\n\n  w, h, x_ctr, y_ctr = _whctrs(anchor)\n  size = w * h\n  size_ratios = size / ratios\n  ws = np.round(np.sqrt(size_ratios))\n  hs = np.round(ws * ratios)\n  anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n  return anchors\n\n\ndef _scale_enum(anchor, scales):\n  """"""\n  Enumerate a set of anchors for each scale wrt an anchor.\n  """"""\n\n  w, h, x_ctr, y_ctr = _whctrs(anchor)\n  ws = w * scales\n  hs = h * scales\n  anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n  return anchors\n\n\nif __name__ == \'__main__\':\n  import time\n\n  t = time.time()\n  a = generate_anchors()\n  print(time.time() - t)\n  print(a)\n  from IPython import embed;\n\n  embed()\n'"
lib/layer_utils/proposal_layer.py,9,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Xinlei Chen\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nfrom model.config import cfg\nfrom model.bbox_transform import bbox_transform_inv, clip_boxes, bbox_transform_inv_tf, clip_boxes_tf\nfrom model.nms_wrapper import nms\n\ndef proposal_layer(rpn_cls_prob, rpn_bbox_pred, im_info, cfg_key, _feat_stride, anchors, num_anchors):\n  """"""A simplified version compared to fast/er RCNN\n     For details please see the technical report\n  """"""\n  if type(cfg_key) == bytes:\n      cfg_key = cfg_key.decode(\'utf-8\')\n  pre_nms_topN = cfg[cfg_key].RPN_PRE_NMS_TOP_N\n  post_nms_topN = cfg[cfg_key].RPN_POST_NMS_TOP_N\n  nms_thresh = cfg[cfg_key].RPN_NMS_THRESH\n\n  # Get the scores and bounding boxes\n  scores = rpn_cls_prob[:, :, :, num_anchors:]\n  rpn_bbox_pred = rpn_bbox_pred.reshape((-1, 4))\n  scores = scores.reshape((-1, 1))\n  proposals = bbox_transform_inv(anchors, rpn_bbox_pred)\n  proposals = clip_boxes(proposals, im_info[:2])\n\n  # Pick the top region proposals\n  order = scores.ravel().argsort()[::-1]\n  if pre_nms_topN > 0:\n    order = order[:pre_nms_topN]\n  proposals = proposals[order, :]\n  scores = scores[order]\n\n  # Non-maximal suppression\n  keep = nms(np.hstack((proposals, scores)), nms_thresh)\n\n  # Pick th top region proposals after NMS\n  if post_nms_topN > 0:\n    keep = keep[:post_nms_topN]\n  proposals = proposals[keep, :]\n  scores = scores[keep]\n\n  # Only support single image as input\n  batch_inds = np.zeros((proposals.shape[0], 1), dtype=np.float32)\n  blob = np.hstack((batch_inds, proposals.astype(np.float32, copy=False)))\n\n  return blob, scores\n\n\ndef proposal_layer_tf(rpn_cls_prob, rpn_bbox_pred, im_info, cfg_key, _feat_stride, anchors, num_anchors):\n  if type(cfg_key) == bytes:\n    cfg_key = cfg_key.decode(\'utf-8\')\n  pre_nms_topN = cfg[cfg_key].RPN_PRE_NMS_TOP_N\n  post_nms_topN = cfg[cfg_key].RPN_POST_NMS_TOP_N\n  nms_thresh = cfg[cfg_key].RPN_NMS_THRESH\n\n  # Get the scores and bounding boxes\n  scores = rpn_cls_prob[:, :, :, num_anchors:]\n  scores = tf.reshape(scores, shape=(-1,))\n  rpn_bbox_pred = tf.reshape(rpn_bbox_pred, shape=(-1, 4))\n\n  proposals = bbox_transform_inv_tf(anchors, rpn_bbox_pred)\n  proposals = clip_boxes_tf(proposals, im_info[:2])\n\n  # Non-maximal suppression\n  indices = tf.image.non_max_suppression(proposals, scores, max_output_size=post_nms_topN, iou_threshold=nms_thresh)\n\n  boxes = tf.gather(proposals, indices)\n  boxes = tf.to_float(boxes)\n  scores = tf.gather(scores, indices)\n  scores = tf.reshape(scores, shape=(-1, 1))\n\n  # Only support single image as input\n  batch_inds = tf.zeros((tf.shape(indices)[0], 1), dtype=tf.float32)\n  blob = tf.concat([batch_inds, boxes], 1)\n\n  return blob, scores\n\n\n'"
lib/layer_utils/proposal_target_layer.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick, Sean Bell and Xinlei Chen\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport numpy.random as npr\nfrom model.config import cfg\nfrom model.bbox_transform import bbox_transform\nfrom utils.cython_bbox import bbox_overlaps\n\n\ndef proposal_target_layer(rpn_rois, rpn_scores, gt_boxes, _num_classes):\n  """"""\n  Assign object detection proposals to ground-truth targets. Produces proposal\n  classification labels and bounding-box regression targets.\n  """"""\n\n  # Proposal ROIs (0, x1, y1, x2, y2) coming from RPN\n  # (i.e., rpn.proposal_layer.ProposalLayer), or any other source\n  all_rois = rpn_rois\n  all_scores = rpn_scores\n\n  # Include ground-truth boxes in the set of candidate rois\n  if cfg.TRAIN.USE_GT:\n    zeros = np.zeros((gt_boxes.shape[0], 1), dtype=gt_boxes.dtype)\n    all_rois = np.vstack(\n      (all_rois, np.hstack((zeros, gt_boxes[:, :-1])))\n    )\n    # not sure if it a wise appending, but anyway i am not using it\n    all_scores = np.vstack((all_scores, zeros))\n\n  num_images = 1\n  rois_per_image = cfg.TRAIN.BATCH_SIZE / num_images\n  fg_rois_per_image = np.round(cfg.TRAIN.FG_FRACTION * rois_per_image)\n\n  # Sample rois with classification labels and bounding box regression\n  # targets\n  labels, rois, roi_scores, bbox_targets, bbox_inside_weights = _sample_rois(\n    all_rois, all_scores, gt_boxes, fg_rois_per_image,\n    rois_per_image, _num_classes)\n\n  rois = rois.reshape(-1, 5)\n  roi_scores = roi_scores.reshape(-1)\n  labels = labels.reshape(-1, 1)\n  bbox_targets = bbox_targets.reshape(-1, _num_classes * 4)\n  bbox_inside_weights = bbox_inside_weights.reshape(-1, _num_classes * 4)\n  bbox_outside_weights = np.array(bbox_inside_weights > 0).astype(np.float32)\n\n  return rois, roi_scores, labels, bbox_targets, bbox_inside_weights, bbox_outside_weights\n\n\ndef _get_bbox_regression_labels(bbox_target_data, num_classes):\n  """"""Bounding-box regression targets (bbox_target_data) are stored in a\n  compact form N x (class, tx, ty, tw, th)\n\n  This function expands those targets into the 4-of-4*K representation used\n  by the network (i.e. only one class has non-zero targets).\n\n  Returns:\n      bbox_target (ndarray): N x 4K blob of regression targets\n      bbox_inside_weights (ndarray): N x 4K blob of loss weights\n  """"""\n\n  clss = bbox_target_data[:, 0]\n  bbox_targets = np.zeros((clss.size, 4 * num_classes), dtype=np.float32)\n  bbox_inside_weights = np.zeros(bbox_targets.shape, dtype=np.float32)\n  inds = np.where(clss > 0)[0]\n  for ind in inds:\n    cls = clss[ind]\n    start = int(4 * cls)\n    end = start + 4\n    bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]\n    bbox_inside_weights[ind, start:end] = cfg.TRAIN.BBOX_INSIDE_WEIGHTS\n  return bbox_targets, bbox_inside_weights\n\n\ndef _compute_targets(ex_rois, gt_rois, labels):\n  """"""Compute bounding-box regression targets for an image.""""""\n\n  assert ex_rois.shape[0] == gt_rois.shape[0]\n  assert ex_rois.shape[1] == 4\n  assert gt_rois.shape[1] == 4\n\n  targets = bbox_transform(ex_rois, gt_rois)\n  if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n    # Optionally normalize targets by a precomputed mean and stdev\n    targets = ((targets - np.array(cfg.TRAIN.BBOX_NORMALIZE_MEANS))\n               / np.array(cfg.TRAIN.BBOX_NORMALIZE_STDS))\n  return np.hstack(\n    (labels[:, np.newaxis], targets)).astype(np.float32, copy=False)\n\n\ndef _sample_rois(all_rois, all_scores, gt_boxes, fg_rois_per_image, rois_per_image, num_classes):\n  """"""Generate a random sample of RoIs comprising foreground and background\n  examples.\n  """"""\n  # overlaps: (rois x gt_boxes)\n  overlaps = bbox_overlaps(\n    np.ascontiguousarray(all_rois[:, 1:5], dtype=np.float),\n    np.ascontiguousarray(gt_boxes[:, :4], dtype=np.float))\n  gt_assignment = overlaps.argmax(axis=1)\n  max_overlaps = overlaps.max(axis=1)\n  labels = gt_boxes[gt_assignment, 4]\n\n  # Select foreground RoIs as those with >= FG_THRESH overlap\n  fg_inds = np.where(max_overlaps >= cfg.TRAIN.FG_THRESH)[0]\n  # Guard against the case when an image has fewer than fg_rois_per_image\n  # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n  bg_inds = np.where((max_overlaps < cfg.TRAIN.BG_THRESH_HI) &\n                     (max_overlaps >= cfg.TRAIN.BG_THRESH_LO))[0]\n\n  # Small modification to the original version where we ensure a fixed number of regions are sampled\n  if fg_inds.size > 0 and bg_inds.size > 0:\n    fg_rois_per_image = min(fg_rois_per_image, fg_inds.size)\n    fg_inds = npr.choice(fg_inds, size=int(fg_rois_per_image), replace=False)\n    bg_rois_per_image = rois_per_image - fg_rois_per_image\n    to_replace = bg_inds.size < bg_rois_per_image\n    bg_inds = npr.choice(bg_inds, size=int(bg_rois_per_image), replace=to_replace)\n  elif fg_inds.size > 0:\n    to_replace = fg_inds.size < rois_per_image\n    fg_inds = npr.choice(fg_inds, size=int(rois_per_image), replace=to_replace)\n    fg_rois_per_image = rois_per_image\n  elif bg_inds.size > 0:\n    to_replace = bg_inds.size < rois_per_image\n    bg_inds = npr.choice(bg_inds, size=int(rois_per_image), replace=to_replace)\n    fg_rois_per_image = 0\n  else:\n    import pdb\n    pdb.set_trace()\n\n  # The indices that we\'re selecting (both fg and bg)\n  keep_inds = np.append(fg_inds, bg_inds)\n  # Select sampled values from various arrays:\n  labels = labels[keep_inds]\n  # Clamp labels for the background RoIs to 0\n  labels[int(fg_rois_per_image):] = 0\n  rois = all_rois[keep_inds]\n  roi_scores = all_scores[keep_inds]\n\n  bbox_target_data = _compute_targets(\n    rois[:, 1:5], gt_boxes[gt_assignment[keep_inds], :4], labels)\n\n  bbox_targets, bbox_inside_weights = \\\n    _get_bbox_regression_labels(bbox_target_data, num_classes)\n\n  return labels, rois, roi_scores, bbox_targets, bbox_inside_weights\n'"
lib/layer_utils/proposal_top_layer.py,9,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Xinlei Chen\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom model.config import cfg\nfrom model.bbox_transform import bbox_transform_inv, clip_boxes, bbox_transform_inv_tf, clip_boxes_tf\n\nimport tensorflow as tf\nimport numpy as np\nimport numpy.random as npr\n\ndef proposal_top_layer(rpn_cls_prob, rpn_bbox_pred, im_info, _feat_stride, anchors, num_anchors):\n  """"""A layer that just selects the top region proposals\n     without using non-maximal suppression,\n     For details please see the technical report\n  """"""\n  rpn_top_n = cfg.TEST.RPN_TOP_N\n\n  scores = rpn_cls_prob[:, :, :, num_anchors:]\n\n  rpn_bbox_pred = rpn_bbox_pred.reshape((-1, 4))\n  scores = scores.reshape((-1, 1))\n\n  length = scores.shape[0]\n  if length < rpn_top_n:\n    # Random selection, maybe unnecessary and loses good proposals\n    # But such case rarely happens\n    top_inds = npr.choice(length, size=rpn_top_n, replace=True)\n  else:\n    top_inds = scores.argsort(0)[::-1]\n    top_inds = top_inds[:rpn_top_n]\n    top_inds = top_inds.reshape(rpn_top_n, )\n\n  # Do the selection here\n  anchors = anchors[top_inds, :]\n  rpn_bbox_pred = rpn_bbox_pred[top_inds, :]\n  scores = scores[top_inds]\n\n  # Convert anchors into proposals via bbox transformations\n  proposals = bbox_transform_inv(anchors, rpn_bbox_pred)\n\n  # Clip predicted boxes to image\n  proposals = clip_boxes(proposals, im_info[:2])\n\n  # Output rois blob\n  # Our RPN implementation only supports a single input image, so all\n  # batch inds are 0\n  batch_inds = np.zeros((proposals.shape[0], 1), dtype=np.float32)\n  blob = np.hstack((batch_inds, proposals.astype(np.float32, copy=False)))\n  return blob, scores\n\n\ndef proposal_top_layer_tf(rpn_cls_prob, rpn_bbox_pred, im_info, _feat_stride, anchors, num_anchors):\n  """"""A layer that just selects the top region proposals\n     without using non-maximal suppression,\n     For details please see the technical report\n  """"""\n  rpn_top_n = cfg.TEST.RPN_TOP_N\n\n  scores = rpn_cls_prob[:, :, :, num_anchors:]\n  rpn_bbox_pred = tf.reshape(rpn_bbox_pred, shape=(-1, 4))\n  scores = tf.reshape(scores, shape=(-1,))\n\n  # Do the selection here\n  top_scores, top_inds = tf.nn.top_k(scores, k=rpn_top_n)\n  top_scores = tf.reshape(top_scores, shape=(-1, 1))\n  top_anchors = tf.gather(anchors, top_inds)\n  top_rpn_bbox = tf.gather(rpn_bbox_pred, top_inds)\n  proposals = bbox_transform_inv_tf(top_anchors, top_rpn_bbox)\n\n  # Clip predicted boxes to image\n  proposals = clip_boxes_tf(proposals, im_info[:2])\n\n  # Output rois blob\n  # Our RPN implementation only supports a single input image, so all\n  # batch inds are 0\n  proposals = tf.to_float(proposals)\n  batch_inds = tf.zeros((rpn_top_n, 1))\n  blob = tf.concat([batch_inds, proposals], 1)\n  return blob, top_scores\n'"
lib/layer_utils/snippets.py,11,"b'# --------------------------------------------------------\n# Tensorflow Faster R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Xinlei Chen\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nfrom layer_utils.generate_anchors import generate_anchors\n\ndef generate_anchors_pre(height, width, feat_stride, anchor_scales=(8,16,32), anchor_ratios=(0.5,1,2)):\n  """""" A wrapper function to generate anchors given different scales\n    Also return the number of anchors in variable \'length\'\n  """"""\n  anchors = generate_anchors(ratios=np.array(anchor_ratios), scales=np.array(anchor_scales))\n  A = anchors.shape[0]\n  shift_x = np.arange(0, width) * feat_stride\n  shift_y = np.arange(0, height) * feat_stride\n  shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n  shifts = np.vstack((shift_x.ravel(), shift_y.ravel(), shift_x.ravel(), shift_y.ravel())).transpose()\n  K = shifts.shape[0]\n  # width changes faster, so here it is H, W, C\n  anchors = anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2))\n  anchors = anchors.reshape((K * A, 4)).astype(np.float32, copy=False)\n  length = np.int32(anchors.shape[0])\n\n  return anchors, length\n\ndef generate_anchors_pre_tf(height, width, feat_stride=16, anchor_scales=(8, 16, 32), anchor_ratios=(0.5, 1, 2)):\n  shift_x = tf.range(width) * feat_stride # width\n  shift_y = tf.range(height) * feat_stride # height\n  shift_x, shift_y = tf.meshgrid(shift_x, shift_y)\n  sx = tf.reshape(shift_x, shape=(-1,))\n  sy = tf.reshape(shift_y, shape=(-1,))\n  shifts = tf.transpose(tf.stack([sx, sy, sx, sy]))\n  K = tf.multiply(width, height)\n  shifts = tf.transpose(tf.reshape(shifts, shape=[1, K, 4]), perm=(1, 0, 2))\n\n  anchors = generate_anchors(ratios=np.array(anchor_ratios), scales=np.array(anchor_scales))\n  A = anchors.shape[0]\n  anchor_constant = tf.constant(anchors.reshape((1, A, 4)), dtype=tf.int32)\n\n  length = K * A\n  anchors_tf = tf.reshape(tf.add(anchor_constant, shifts), shape=(length, 4))\n  \n  return tf.cast(anchors_tf, dtype=tf.float32), length\n'"
lib/model/__init__.py,0,b'from . import config\n'
lib/model/bbox_transform.py,19,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\ndef bbox_transform(ex_rois, gt_rois):\n  ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0\n  ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0\n  ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\n  ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n\n  gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + 1.0\n  gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + 1.0\n  gt_ctr_x = gt_rois[:, 0] + 0.5 * gt_widths\n  gt_ctr_y = gt_rois[:, 1] + 0.5 * gt_heights\n\n  targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n  targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n  targets_dw = np.log(gt_widths / ex_widths)\n  targets_dh = np.log(gt_heights / ex_heights)\n\n  targets = np.vstack(\n    (targets_dx, targets_dy, targets_dw, targets_dh)).transpose()\n  return targets\n\n\ndef bbox_transform_inv(boxes, deltas):\n  if boxes.shape[0] == 0:\n    return np.zeros((0, deltas.shape[1]), dtype=deltas.dtype)\n\n  boxes = boxes.astype(deltas.dtype, copy=False)\n  widths = boxes[:, 2] - boxes[:, 0] + 1.0\n  heights = boxes[:, 3] - boxes[:, 1] + 1.0\n  ctr_x = boxes[:, 0] + 0.5 * widths\n  ctr_y = boxes[:, 1] + 0.5 * heights\n\n  dx = deltas[:, 0::4]\n  dy = deltas[:, 1::4]\n  dw = deltas[:, 2::4]\n  dh = deltas[:, 3::4]\n  \n  pred_ctr_x = dx * widths[:, np.newaxis] + ctr_x[:, np.newaxis]\n  pred_ctr_y = dy * heights[:, np.newaxis] + ctr_y[:, np.newaxis]\n  pred_w = np.exp(dw) * widths[:, np.newaxis]\n  pred_h = np.exp(dh) * heights[:, np.newaxis]\n\n  pred_boxes = np.zeros(deltas.shape, dtype=deltas.dtype)\n  # x1\n  pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n  # y1\n  pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n  # x2\n  pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n  # y2\n  pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n  return pred_boxes\n\n\ndef clip_boxes(boxes, im_shape):\n  """"""\n  Clip boxes to image boundaries.\n  """"""\n\n  # x1 >= 0\n  boxes[:, 0::4] = np.maximum(np.minimum(boxes[:, 0::4], im_shape[1] - 1), 0)\n  # y1 >= 0\n  boxes[:, 1::4] = np.maximum(np.minimum(boxes[:, 1::4], im_shape[0] - 1), 0)\n  # x2 < im_shape[1]\n  boxes[:, 2::4] = np.maximum(np.minimum(boxes[:, 2::4], im_shape[1] - 1), 0)\n  # y2 < im_shape[0]\n  boxes[:, 3::4] = np.maximum(np.minimum(boxes[:, 3::4], im_shape[0] - 1), 0)\n  return boxes\n\n\n\ndef bbox_transform_inv_tf(boxes, deltas):\n  boxes = tf.cast(boxes, deltas.dtype)\n  widths = tf.subtract(boxes[:, 2], boxes[:, 0]) + 1.0\n  heights = tf.subtract(boxes[:, 3], boxes[:, 1]) + 1.0\n  ctr_x = tf.add(boxes[:, 0], widths * 0.5)\n  ctr_y = tf.add(boxes[:, 1], heights * 0.5)\n\n  dx = deltas[:, 0]\n  dy = deltas[:, 1]\n  dw = deltas[:, 2]\n  dh = deltas[:, 3]\n\n  pred_ctr_x = tf.add(tf.multiply(dx, widths), ctr_x)\n  pred_ctr_y = tf.add(tf.multiply(dy, heights), ctr_y)\n  pred_w = tf.multiply(tf.exp(dw), widths)\n  pred_h = tf.multiply(tf.exp(dh), heights)\n\n  pred_boxes0 = tf.subtract(pred_ctr_x, pred_w * 0.5)\n  pred_boxes1 = tf.subtract(pred_ctr_y, pred_h * 0.5)\n  pred_boxes2 = tf.add(pred_ctr_x, pred_w * 0.5)\n  pred_boxes3 = tf.add(pred_ctr_y, pred_h * 0.5)\n\n  return tf.stack([pred_boxes0, pred_boxes1, pred_boxes2, pred_boxes3], axis=1)\n\n\ndef clip_boxes_tf(boxes, im_info):\n  b0 = tf.maximum(tf.minimum(boxes[:, 0], im_info[1] - 1), 0)\n  b1 = tf.maximum(tf.minimum(boxes[:, 1], im_info[0] - 1), 0)\n  b2 = tf.maximum(tf.minimum(boxes[:, 2], im_info[1] - 1), 0)\n  b3 = tf.maximum(tf.minimum(boxes[:, 3], im_info[0] - 1), 0)\n  return tf.stack([b0, b1, b2, b3], axis=1)\n\n\n'"
lib/model/config.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport os.path as osp\nimport numpy as np\n# `pip install easydict` if you don\'t have it\nfrom easydict import EasyDict as edict\n\n__C = edict()\n# Consumers can get config by:\n#   from fast_rcnn_config import cfg\ncfg = __C\n\n#\n# Training options\n#\n__C.TRAIN = edict()\n\n# Initial learning rate\n__C.TRAIN.LEARNING_RATE = 0.001\n\n# Momentum\n__C.TRAIN.MOMENTUM = 0.9\n\n# Weight decay, for regularization\n__C.TRAIN.WEIGHT_DECAY = 0.0001\n\n# Factor for reducing the learning rate\n__C.TRAIN.GAMMA = 0.1\n\n# Step size for reducing the learning rate, currently only support one step\n__C.TRAIN.STEPSIZE = [30000]\n\n# Iteration intervals for showing the loss during training, on command line interface\n__C.TRAIN.DISPLAY = 10\n\n# Whether to double the learning rate for bias\n__C.TRAIN.DOUBLE_BIAS = True\n\n# Whether to initialize the weights with truncated normal distribution \n__C.TRAIN.TRUNCATED = False\n\n# Whether to have weight decay on bias as well\n__C.TRAIN.BIAS_DECAY = False\n\n# Whether to add ground truth boxes to the pool when sampling regions\n__C.TRAIN.USE_GT = False\n\n# Whether to use aspect-ratio grouping of training images, introduced merely for saving\n# GPU memory\n__C.TRAIN.ASPECT_GROUPING = False\n\n# The number of snapshots kept, older ones are deleted to save space\n__C.TRAIN.SNAPSHOT_KEPT = 3\n\n# The time interval for saving tensorflow summaries\n__C.TRAIN.SUMMARY_INTERVAL = 180\n\n# Scale to use during training (can list multiple scales)\n# The scale is the pixel size of an image\'s shortest side\n__C.TRAIN.SCALES = (600,)\n\n# Max pixel size of the longest side of a scaled input image\n__C.TRAIN.MAX_SIZE = 1000\n\n# Images to use per minibatch\n__C.TRAIN.IMS_PER_BATCH = 1\n\n# Minibatch size (number of regions of interest [ROIs])\n__C.TRAIN.BATCH_SIZE = 128\n\n# Fraction of minibatch that is labeled foreground (i.e. class > 0)\n__C.TRAIN.FG_FRACTION = 0.25\n\n# Overlap threshold for a ROI to be considered foreground (if >= FG_THRESH)\n__C.TRAIN.FG_THRESH = 0.5\n\n# Overlap threshold for a ROI to be considered background (class = 0 if\n# overlap in [LO, HI))\n__C.TRAIN.BG_THRESH_HI = 0.5\n__C.TRAIN.BG_THRESH_LO = 0.1\n\n# Use horizontally-flipped images during training?\n__C.TRAIN.USE_FLIPPED = True\n\n# Train bounding-box regressors\n__C.TRAIN.BBOX_REG = True\n\n# Overlap required between a ROI and ground-truth box in order for that ROI to\n# be used as a bounding-box regression training example\n__C.TRAIN.BBOX_THRESH = 0.5\n\n# Iterations between snapshots\n__C.TRAIN.SNAPSHOT_ITERS = 5000\n\n# solver.prototxt specifies the snapshot path prefix, this adds an optional\n# infix to yield the path: <prefix>[_<infix>]_iters_XYZ.caffemodel\n__C.TRAIN.SNAPSHOT_PREFIX = \'res101_faster_rcnn\'\n\n# Normalize the targets (subtract empirical mean, divide by empirical stddev)\n__C.TRAIN.BBOX_NORMALIZE_TARGETS = True\n\n# Deprecated (inside weights)\n__C.TRAIN.BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n\n# Normalize the targets using ""precomputed"" (or made up) means and stdevs\n# (BBOX_NORMALIZE_TARGETS must also be True)\n__C.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED = True\n\n__C.TRAIN.BBOX_NORMALIZE_MEANS = (0.0, 0.0, 0.0, 0.0)\n\n__C.TRAIN.BBOX_NORMALIZE_STDS = (0.1, 0.1, 0.2, 0.2)\n\n# Train using these proposals\n__C.TRAIN.PROPOSAL_METHOD = \'gt\'\n\n# Make minibatches from images that have similar aspect ratios (i.e. both\n# tall and thin or both short and wide) in order to avoid wasting computation\n# on zero-padding.\n\n# Use RPN to detect objects\n__C.TRAIN.HAS_RPN = True\n\n# IOU >= thresh: positive example\n__C.TRAIN.RPN_POSITIVE_OVERLAP = 0.7\n\n# IOU < thresh: negative example\n__C.TRAIN.RPN_NEGATIVE_OVERLAP = 0.3\n\n# If an anchor satisfied by positive and negative conditions set to negative\n__C.TRAIN.RPN_CLOBBER_POSITIVES = False\n\n# Max number of foreground examples\n__C.TRAIN.RPN_FG_FRACTION = 0.5\n\n# Total number of examples\n__C.TRAIN.RPN_BATCHSIZE = 256\n\n# NMS threshold used on RPN proposals\n__C.TRAIN.RPN_NMS_THRESH = 0.7\n\n# Number of top scoring boxes to keep before apply NMS to RPN proposals\n__C.TRAIN.RPN_PRE_NMS_TOP_N = 12000\n\n# Number of top scoring boxes to keep after applying NMS to RPN proposals\n__C.TRAIN.RPN_POST_NMS_TOP_N = 2000\n\n# Deprecated (outside weights)\n__C.TRAIN.RPN_BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n\n# Give the positive RPN examples weight of p * 1 / {num positives}\n# and give negatives a weight of (1 - p)\n# Set to -1.0 to use uniform example weighting\n__C.TRAIN.RPN_POSITIVE_WEIGHT = -1.0\n\n# Whether to use all ground truth bounding boxes for training, \n# For COCO, setting USE_ALL_GT to False will exclude boxes that are flagged as \'\'iscrowd\'\'\n__C.TRAIN.USE_ALL_GT = True\n\n#\n# Testing options\n#\n__C.TEST = edict()\n\n# Scale to use during testing (can NOT list multiple scales)\n# The scale is the pixel size of an image\'s shortest side\n__C.TEST.SCALES = (600,)\n\n# Max pixel size of the longest side of a scaled input image\n__C.TEST.MAX_SIZE = 1000\n\n# Overlap threshold used for non-maximum suppression (suppress boxes with\n# IoU >= this threshold)\n__C.TEST.NMS = 0.3\n\n# Experimental: treat the (K+1) units in the cls_score layer as linear\n# predictors (trained, eg, with one-vs-rest SVMs).\n__C.TEST.SVM = False\n\n# Test using bounding-box regressors\n__C.TEST.BBOX_REG = True\n\n# Propose boxes\n__C.TEST.HAS_RPN = False\n\n# Test using these proposals\n__C.TEST.PROPOSAL_METHOD = \'gt\'\n\n## NMS threshold used on RPN proposals\n__C.TEST.RPN_NMS_THRESH = 0.7\n\n# Number of top scoring boxes to keep before apply NMS to RPN proposals\n__C.TEST.RPN_PRE_NMS_TOP_N = 6000\n\n# Number of top scoring boxes to keep after applying NMS to RPN proposals\n__C.TEST.RPN_POST_NMS_TOP_N = 300\n\n# Proposal height and width both need to be greater than RPN_MIN_SIZE (at orig image scale)\n# __C.TEST.RPN_MIN_SIZE = 16\n\n# Testing mode, default to be \'nms\', \'top\' is slower but better\n# See report for details\n__C.TEST.MODE = \'nms\'\n\n# Only useful when TEST.MODE is \'top\', specifies the number of top proposals to select\n__C.TEST.RPN_TOP_N = 5000\n\n#\n# ResNet options\n#\n\n__C.RESNET = edict()\n\n# Option to set if max-pooling is appended after crop_and_resize. \n# if true, the region will be resized to a square of 2xPOOLING_SIZE, \n# then 2x2 max-pooling is applied; otherwise the region will be directly\n# resized to a square of POOLING_SIZE\n__C.RESNET.MAX_POOL = False\n\n# Number of fixed blocks during training, by default the first of all 4 blocks is fixed\n# Range: 0 (none) to 3 (all)\n__C.RESNET.FIXED_BLOCKS = 1\n\n#\n# MobileNet options\n#\n\n__C.MOBILENET = edict()\n\n# Whether to regularize the depth-wise filters during training\n__C.MOBILENET.REGU_DEPTH = False\n\n# Number of fixed layers during training, by default the bottom 5 of 14 layers is fixed\n# Range: 0 (none) to 12 (all)\n__C.MOBILENET.FIXED_LAYERS = 5\n\n# Weight decay for the mobilenet weights\n__C.MOBILENET.WEIGHT_DECAY = 0.00004\n\n# Depth multiplier\n__C.MOBILENET.DEPTH_MULTIPLIER = 1.\n\n#\n# MISC\n#\n\n# Pixel mean values (BGR order) as a (1, 1, 3) array\n# We use the same pixel mean for all networks even though it\'s not exactly what\n# they were trained with\n__C.PIXEL_MEANS = np.array([[[102.9801, 115.9465, 122.7717]]])\n\n# For reproducibility\n__C.RNG_SEED = 3\n\n# Root directory of project\n__C.ROOT_DIR = osp.abspath(osp.join(osp.dirname(__file__), \'..\', \'..\'))\n\n# Data directory\n__C.DATA_DIR = osp.abspath(osp.join(__C.ROOT_DIR, \'data\'))\n\n# Name (or path to) the matlab executable\n__C.MATLAB = \'matlab\'\n\n# Place outputs under an experiments directory\n__C.EXP_DIR = \'default\'\n\n# Use GPU implementation of non-maximum suppression\n__C.USE_GPU_NMS = True\n\n# Use an end-to-end tensorflow model.\n# Note: models in E2E tensorflow mode have only been tested in feed-forward mode,\n#       but these models are exportable to other tensorflow instances as GraphDef files.\n__C.USE_E2E_TF = True\n\n# Default pooling mode, only \'crop\' is available\n__C.POOLING_MODE = \'crop\'\n\n# Size of the pooled region after RoI pooling\n__C.POOLING_SIZE = 7\n\n# Anchor scales for RPN\n__C.ANCHOR_SCALES = [8,16,32]\n\n# Anchor ratios for RPN\n__C.ANCHOR_RATIOS = [0.5,1,2]\n\n# Number of filters for the RPN layer\n__C.RPN_CHANNELS = 512\n\n\ndef get_output_dir(imdb, weights_filename):\n  """"""Return the directory where experimental artifacts are placed.\n  If the directory does not exist, it is created.\n\n  A canonical path is built using the name from an imdb and a network\n  (if not None).\n  """"""\n  outdir = osp.abspath(osp.join(__C.ROOT_DIR, \'output\', __C.EXP_DIR, imdb.name))\n  if weights_filename is None:\n    weights_filename = \'default\'\n  outdir = osp.join(outdir, weights_filename)\n  if not os.path.exists(outdir):\n    os.makedirs(outdir)\n  return outdir\n\n\ndef get_output_tb_dir(imdb, weights_filename):\n  """"""Return the directory where tensorflow summaries are placed.\n  If the directory does not exist, it is created.\n\n  A canonical path is built using the name from an imdb and a network\n  (if not None).\n  """"""\n  outdir = osp.abspath(osp.join(__C.ROOT_DIR, \'tensorboard\', __C.EXP_DIR, imdb.name))\n  if weights_filename is None:\n    weights_filename = \'default\'\n  outdir = osp.join(outdir, weights_filename)\n  if not os.path.exists(outdir):\n    os.makedirs(outdir)\n  return outdir\n\n\ndef _merge_a_into_b(a, b):\n  """"""Merge config dictionary a into config dictionary b, clobbering the\n  options in b whenever they are also specified in a.\n  """"""\n  if type(a) is not edict:\n    return\n\n  for k, v in a.items():\n    # a must specify keys that are in b\n    if k not in b:\n      raise KeyError(\'{} is not a valid config key\'.format(k))\n\n    # the types must match, too\n    old_type = type(b[k])\n    if old_type is not type(v):\n      if isinstance(b[k], np.ndarray):\n        v = np.array(v, dtype=b[k].dtype)\n      else:\n        raise ValueError((\'Type mismatch ({} vs. {}) \'\n                          \'for config key: {}\').format(type(b[k]),\n                                                       type(v), k))\n\n    # recursively merge dicts\n    if type(v) is edict:\n      try:\n        _merge_a_into_b(a[k], b[k])\n      except:\n        print((\'Error under config key: {}\'.format(k)))\n        raise\n    else:\n      b[k] = v\n\n\ndef cfg_from_file(filename):\n  """"""Load a config file and merge it into the default options.""""""\n  import yaml\n  with open(filename, \'r\') as f:\n    yaml_cfg = edict(yaml.load(f))\n\n  _merge_a_into_b(yaml_cfg, __C)\n\n\ndef cfg_from_list(cfg_list):\n  """"""Set config keys via list (e.g., from command line).""""""\n  from ast import literal_eval\n  assert len(cfg_list) % 2 == 0\n  for k, v in zip(cfg_list[0::2], cfg_list[1::2]):\n    key_list = k.split(\'.\')\n    d = __C\n    for subkey in key_list[:-1]:\n      assert subkey in d\n      d = d[subkey]\n    subkey = key_list[-1]\n    assert subkey in d\n    try:\n      value = literal_eval(v)\n    except:\n      # handle the case when v is a string literal\n      value = v\n    assert type(value) == type(d[subkey]), \\\n      \'type {} does not match original type {}\'.format(\n        type(value), type(d[subkey]))\n    d[subkey] = value\n'"
lib/model/nms_wrapper.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom model.config import cfg\nfrom nms.gpu_nms import gpu_nms\nfrom nms.cpu_nms import cpu_nms\n\ndef nms(dets, thresh, force_cpu=False):\n  """"""Dispatch to either CPU or GPU NMS implementations.""""""\n\n  if dets.shape[0] == 0:\n    return []\n  if cfg.USE_GPU_NMS and not force_cpu:\n    return gpu_nms(dets, thresh, device_id=0)\n  else:\n    return cpu_nms(dets, thresh)\n'"
lib/model/test.py,0,"b'# --------------------------------------------------------\n# Tensorflow Faster R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Xinlei Chen\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport cv2\nimport numpy as np\ntry:\n  import cPickle as pickle\nexcept ImportError:\n  import pickle\nimport os\nimport math\n\nfrom utils.timer import Timer\nfrom utils.blob import im_list_to_blob\n\nfrom model.config import cfg, get_output_dir\nfrom model.bbox_transform import clip_boxes, bbox_transform_inv\nfrom model.nms_wrapper import nms\n\ndef _get_image_blob(im):\n  """"""Converts an image into a network input.\n  Arguments:\n    im (ndarray): a color image in BGR order\n  Returns:\n    blob (ndarray): a data blob holding an image pyramid\n    im_scale_factors (list): list of image scales (relative to im) used\n      in the image pyramid\n  """"""\n  im_orig = im.astype(np.float32, copy=True)\n  im_orig -= cfg.PIXEL_MEANS\n\n  im_shape = im_orig.shape\n  im_size_min = np.min(im_shape[0:2])\n  im_size_max = np.max(im_shape[0:2])\n\n  processed_ims = []\n  im_scale_factors = []\n\n  for target_size in cfg.TEST.SCALES:\n    im_scale = float(target_size) / float(im_size_min)\n    # Prevent the biggest axis from being more than MAX_SIZE\n    if np.round(im_scale * im_size_max) > cfg.TEST.MAX_SIZE:\n      im_scale = float(cfg.TEST.MAX_SIZE) / float(im_size_max)\n    im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale,\n            interpolation=cv2.INTER_LINEAR)\n    im_scale_factors.append(im_scale)\n    processed_ims.append(im)\n\n  # Create a blob to hold the input images\n  blob = im_list_to_blob(processed_ims)\n\n  return blob, np.array(im_scale_factors)\n\ndef _get_blobs(im):\n  """"""Convert an image and RoIs within that image into network inputs.""""""\n  blobs = {}\n  blobs[\'data\'], im_scale_factors = _get_image_blob(im)\n\n  return blobs, im_scale_factors\n\ndef _clip_boxes(boxes, im_shape):\n  """"""Clip boxes to image boundaries.""""""\n  # x1 >= 0\n  boxes[:, 0::4] = np.maximum(boxes[:, 0::4], 0)\n  # y1 >= 0\n  boxes[:, 1::4] = np.maximum(boxes[:, 1::4], 0)\n  # x2 < im_shape[1]\n  boxes[:, 2::4] = np.minimum(boxes[:, 2::4], im_shape[1] - 1)\n  # y2 < im_shape[0]\n  boxes[:, 3::4] = np.minimum(boxes[:, 3::4], im_shape[0] - 1)\n  return boxes\n\ndef _rescale_boxes(boxes, inds, scales):\n  """"""Rescale boxes according to image rescaling.""""""\n  for i in range(boxes.shape[0]):\n    boxes[i,:] = boxes[i,:] / scales[int(inds[i])]\n\n  return boxes\n\ndef im_detect(sess, net, im):\n  blobs, im_scales = _get_blobs(im)\n  assert len(im_scales) == 1, ""Only single-image batch implemented""\n\n  im_blob = blobs[\'data\']\n  blobs[\'im_info\'] = np.array([im_blob.shape[1], im_blob.shape[2], im_scales[0]], dtype=np.float32)\n\n  _, scores, bbox_pred, rois = net.test_image(sess, blobs[\'data\'], blobs[\'im_info\'])\n  \n  boxes = rois[:, 1:5] / im_scales[0]\n  scores = np.reshape(scores, [scores.shape[0], -1])\n  bbox_pred = np.reshape(bbox_pred, [bbox_pred.shape[0], -1])\n  if cfg.TEST.BBOX_REG:\n    # Apply bounding-box regression deltas\n    box_deltas = bbox_pred\n    pred_boxes = bbox_transform_inv(boxes, box_deltas)\n    pred_boxes = _clip_boxes(pred_boxes, im.shape)\n  else:\n    # Simply repeat the boxes, once for each class\n    pred_boxes = np.tile(boxes, (1, scores.shape[1]))\n\n  return scores, pred_boxes\n\ndef apply_nms(all_boxes, thresh):\n  """"""Apply non-maximum suppression to all predicted boxes output by the\n  test_net method.\n  """"""\n  num_classes = len(all_boxes)\n  num_images = len(all_boxes[0])\n  nms_boxes = [[[] for _ in range(num_images)] for _ in range(num_classes)]\n  for cls_ind in range(num_classes):\n    for im_ind in range(num_images):\n      dets = all_boxes[cls_ind][im_ind]\n      if dets == []:\n        continue\n\n      x1 = dets[:, 0]\n      y1 = dets[:, 1]\n      x2 = dets[:, 2]\n      y2 = dets[:, 3]\n      scores = dets[:, 4]\n      inds = np.where((x2 > x1) & (y2 > y1))[0]\n      dets = dets[inds,:]\n      if dets == []:\n        continue\n\n      keep = nms(dets, thresh)\n      if len(keep) == 0:\n        continue\n      nms_boxes[cls_ind][im_ind] = dets[keep, :].copy()\n  return nms_boxes\n\ndef test_net(sess, net, imdb, weights_filename, max_per_image=100, thresh=0.):\n  np.random.seed(cfg.RNG_SEED)\n  """"""Test a Fast R-CNN network on an image database.""""""\n  num_images = len(imdb.image_index)\n  # all detections are collected into:\n  #  all_boxes[cls][image] = N x 5 array of detections in\n  #  (x1, y1, x2, y2, score)\n  all_boxes = [[[] for _ in range(num_images)]\n         for _ in range(imdb.num_classes)]\n\n  output_dir = get_output_dir(imdb, weights_filename)\n  # timers\n  _t = {\'im_detect\' : Timer(), \'misc\' : Timer()}\n\n  for i in range(num_images):\n    im = cv2.imread(imdb.image_path_at(i))\n\n    _t[\'im_detect\'].tic()\n    scores, boxes = im_detect(sess, net, im)\n    _t[\'im_detect\'].toc()\n\n    _t[\'misc\'].tic()\n\n    # skip j = 0, because it\'s the background class\n    for j in range(1, imdb.num_classes):\n      inds = np.where(scores[:, j] > thresh)[0]\n      cls_scores = scores[inds, j]\n      cls_boxes = boxes[inds, j*4:(j+1)*4]\n      cls_dets = np.hstack((cls_boxes, cls_scores[:, np.newaxis])) \\\n        .astype(np.float32, copy=False)\n      keep = nms(cls_dets, cfg.TEST.NMS)\n      cls_dets = cls_dets[keep, :]\n      all_boxes[j][i] = cls_dets\n\n    # Limit to max_per_image detections *over all classes*\n    if max_per_image > 0:\n      image_scores = np.hstack([all_boxes[j][i][:, -1]\n                    for j in range(1, imdb.num_classes)])\n      if len(image_scores) > max_per_image:\n        image_thresh = np.sort(image_scores)[-max_per_image]\n        for j in range(1, imdb.num_classes):\n          keep = np.where(all_boxes[j][i][:, -1] >= image_thresh)[0]\n          all_boxes[j][i] = all_boxes[j][i][keep, :]\n    _t[\'misc\'].toc()\n\n    print(\'im_detect: {:d}/{:d} {:.3f}s {:.3f}s\' \\\n        .format(i + 1, num_images, _t[\'im_detect\'].average_time,\n            _t[\'misc\'].average_time))\n\n  det_file = os.path.join(output_dir, \'detections.pkl\')\n  with open(det_file, \'wb\') as f:\n    pickle.dump(all_boxes, f, pickle.HIGHEST_PROTOCOL)\n\n  print(\'Evaluating detections\')\n  imdb.evaluate_detections(all_boxes, output_dir)\n\n'"
lib/model/train_val.py,14,"b'# --------------------------------------------------------\n# Tensorflow Faster R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Xinlei Chen and Zheqi He\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom model.config import cfg\nimport roi_data_layer.roidb as rdl_roidb\nfrom roi_data_layer.layer import RoIDataLayer\nfrom utils.timer import Timer\ntry:\n  import cPickle as pickle\nexcept ImportError:\n  import pickle\nimport numpy as np\nimport os\nimport sys\nimport glob\nimport time\n\nimport tensorflow as tf\nfrom tensorflow.python import pywrap_tensorflow\n\nclass SolverWrapper(object):\n  """"""\n    A wrapper class for the training process\n  """"""\n\n  def __init__(self, sess, network, imdb, roidb, valroidb, output_dir, tbdir, pretrained_model=None):\n    self.net = network\n    self.imdb = imdb\n    self.roidb = roidb\n    self.valroidb = valroidb\n    self.output_dir = output_dir\n    self.tbdir = tbdir\n    # Simply put \'_val\' at the end to save the summaries from the validation set\n    self.tbvaldir = tbdir + \'_val\'\n    if not os.path.exists(self.tbvaldir):\n      os.makedirs(self.tbvaldir)\n    self.pretrained_model = pretrained_model\n\n  def snapshot(self, sess, iter):\n    net = self.net\n\n    if not os.path.exists(self.output_dir):\n      os.makedirs(self.output_dir)\n\n    # Store the model snapshot\n    filename = cfg.TRAIN.SNAPSHOT_PREFIX + \'_iter_{:d}\'.format(iter) + \'.ckpt\'\n    filename = os.path.join(self.output_dir, filename)\n    self.saver.save(sess, filename)\n    print(\'Wrote snapshot to: {:s}\'.format(filename))\n\n    # Also store some meta information, random state, etc.\n    nfilename = cfg.TRAIN.SNAPSHOT_PREFIX + \'_iter_{:d}\'.format(iter) + \'.pkl\'\n    nfilename = os.path.join(self.output_dir, nfilename)\n    # current state of numpy random\n    st0 = np.random.get_state()\n    # current position in the database\n    cur = self.data_layer._cur\n    # current shuffled indexes of the database\n    perm = self.data_layer._perm\n    # current position in the validation database\n    cur_val = self.data_layer_val._cur\n    # current shuffled indexes of the validation database\n    perm_val = self.data_layer_val._perm\n\n    # Dump the meta info\n    with open(nfilename, \'wb\') as fid:\n      pickle.dump(st0, fid, pickle.HIGHEST_PROTOCOL)\n      pickle.dump(cur, fid, pickle.HIGHEST_PROTOCOL)\n      pickle.dump(perm, fid, pickle.HIGHEST_PROTOCOL)\n      pickle.dump(cur_val, fid, pickle.HIGHEST_PROTOCOL)\n      pickle.dump(perm_val, fid, pickle.HIGHEST_PROTOCOL)\n      pickle.dump(iter, fid, pickle.HIGHEST_PROTOCOL)\n\n    return filename, nfilename\n\n  def from_snapshot(self, sess, sfile, nfile):\n    print(\'Restoring model snapshots from {:s}\'.format(sfile))\n    self.saver.restore(sess, sfile)\n    print(\'Restored.\')\n    # Needs to restore the other hyper-parameters/states for training, (TODO xinlei) I have\n    # tried my best to find the random states so that it can be recovered exactly\n    # However the Tensorflow state is currently not available\n    with open(nfile, \'rb\') as fid:\n      st0 = pickle.load(fid)\n      cur = pickle.load(fid)\n      perm = pickle.load(fid)\n      cur_val = pickle.load(fid)\n      perm_val = pickle.load(fid)\n      last_snapshot_iter = pickle.load(fid)\n\n      np.random.set_state(st0)\n      self.data_layer._cur = cur\n      self.data_layer._perm = perm\n      self.data_layer_val._cur = cur_val\n      self.data_layer_val._perm = perm_val\n\n    return last_snapshot_iter\n\n  def get_variables_in_checkpoint_file(self, file_name):\n    try:\n      reader = pywrap_tensorflow.NewCheckpointReader(file_name)\n      var_to_shape_map = reader.get_variable_to_shape_map()\n      return var_to_shape_map \n    except Exception as e:  # pylint: disable=broad-except\n      print(str(e))\n      if ""corrupted compressed block contents"" in str(e):\n        print(""It\'s likely that your checkpoint file has been compressed ""\n              ""with SNAPPY."")\n\n  def construct_graph(self, sess):\n    with sess.graph.as_default():\n      # Set the random seed for tensorflow\n      tf.set_random_seed(cfg.RNG_SEED)\n      # Build the main computation graph\n      layers = self.net.create_architecture(\'TRAIN\', self.imdb.num_classes, tag=\'default\',\n                                            anchor_scales=cfg.ANCHOR_SCALES,\n                                            anchor_ratios=cfg.ANCHOR_RATIOS)\n      # Define the loss\n      loss = layers[\'total_loss\']\n      # Set learning rate and momentum\n      lr = tf.Variable(cfg.TRAIN.LEARNING_RATE, trainable=False)\n      self.optimizer = tf.train.MomentumOptimizer(lr, cfg.TRAIN.MOMENTUM)\n\n      # Compute the gradients with regard to the loss\n      gvs = self.optimizer.compute_gradients(loss)\n      # Double the gradient of the bias if set\n      if cfg.TRAIN.DOUBLE_BIAS:\n        final_gvs = []\n        with tf.variable_scope(\'Gradient_Mult\') as scope:\n          for grad, var in gvs:\n            scale = 1.\n            if cfg.TRAIN.DOUBLE_BIAS and \'/biases:\' in var.name:\n              scale *= 2.\n            if not np.allclose(scale, 1.0):\n              grad = tf.multiply(grad, scale)\n            final_gvs.append((grad, var))\n        train_op = self.optimizer.apply_gradients(final_gvs)\n      else:\n        train_op = self.optimizer.apply_gradients(gvs)\n\n      # We will handle the snapshots ourselves\n      self.saver = tf.train.Saver(max_to_keep=100000)\n      # Write the train and validation information to tensorboard\n      self.writer = tf.summary.FileWriter(self.tbdir, sess.graph)\n      self.valwriter = tf.summary.FileWriter(self.tbvaldir)\n\n    return lr, train_op\n\n  def find_previous(self):\n    sfiles = os.path.join(self.output_dir, cfg.TRAIN.SNAPSHOT_PREFIX + \'_iter_*.ckpt.meta\')\n    sfiles = glob.glob(sfiles)\n    sfiles.sort(key=os.path.getmtime)\n    # Get the snapshot name in TensorFlow\n    redfiles = []\n    for stepsize in cfg.TRAIN.STEPSIZE:\n      redfiles.append(os.path.join(self.output_dir, \n                      cfg.TRAIN.SNAPSHOT_PREFIX + \'_iter_{:d}.ckpt.meta\'.format(stepsize+1)))\n    sfiles = [ss.replace(\'.meta\', \'\') for ss in sfiles if ss not in redfiles]\n\n    nfiles = os.path.join(self.output_dir, cfg.TRAIN.SNAPSHOT_PREFIX + \'_iter_*.pkl\')\n    nfiles = glob.glob(nfiles)\n    nfiles.sort(key=os.path.getmtime)\n    redfiles = [redfile.replace(\'.ckpt.meta\', \'.pkl\') for redfile in redfiles]\n    nfiles = [nn for nn in nfiles if nn not in redfiles]\n\n    lsf = len(sfiles)\n    assert len(nfiles) == lsf\n\n    return lsf, nfiles, sfiles\n\n  def initialize(self, sess):\n    # Initial file lists are empty\n    np_paths = []\n    ss_paths = []\n    # Fresh train directly from ImageNet weights\n    print(\'Loading initial model weights from {:s}\'.format(self.pretrained_model))\n    variables = tf.global_variables()\n    # Initialize all variables first\n    sess.run(tf.variables_initializer(variables, name=\'init\'))\n    var_keep_dic = self.get_variables_in_checkpoint_file(self.pretrained_model)\n    # Get the variables to restore, ignoring the variables to fix\n    variables_to_restore = self.net.get_variables_to_restore(variables, var_keep_dic)\n\n    restorer = tf.train.Saver(variables_to_restore)\n    restorer.restore(sess, self.pretrained_model)\n    print(\'Loaded.\')\n    # Need to fix the variables before loading, so that the RGB weights are changed to BGR\n    # For VGG16 it also changes the convolutional weights fc6 and fc7 to\n    # fully connected weights\n    self.net.fix_variables(sess, self.pretrained_model)\n    print(\'Fixed.\')\n    last_snapshot_iter = 0\n    rate = cfg.TRAIN.LEARNING_RATE\n    stepsizes = list(cfg.TRAIN.STEPSIZE)\n\n    return rate, last_snapshot_iter, stepsizes, np_paths, ss_paths\n\n  def restore(self, sess, sfile, nfile):\n    # Get the most recent snapshot and restore\n    np_paths = [nfile]\n    ss_paths = [sfile]\n    # Restore model from snapshots\n    last_snapshot_iter = self.from_snapshot(sess, sfile, nfile)\n    # Set the learning rate\n    rate = cfg.TRAIN.LEARNING_RATE\n    stepsizes = []\n    for stepsize in cfg.TRAIN.STEPSIZE:\n      if last_snapshot_iter > stepsize:\n        rate *= cfg.TRAIN.GAMMA\n      else:\n        stepsizes.append(stepsize)\n\n    return rate, last_snapshot_iter, stepsizes, np_paths, ss_paths\n\n  def remove_snapshot(self, np_paths, ss_paths):\n    to_remove = len(np_paths) - cfg.TRAIN.SNAPSHOT_KEPT\n    for c in range(to_remove):\n      nfile = np_paths[0]\n      os.remove(str(nfile))\n      np_paths.remove(nfile)\n\n    to_remove = len(ss_paths) - cfg.TRAIN.SNAPSHOT_KEPT\n    for c in range(to_remove):\n      sfile = ss_paths[0]\n      # To make the code compatible to earlier versions of Tensorflow,\n      # where the naming tradition for checkpoints are different\n      if os.path.exists(str(sfile)):\n        os.remove(str(sfile))\n      else:\n        os.remove(str(sfile + \'.data-00000-of-00001\'))\n        os.remove(str(sfile + \'.index\'))\n      sfile_meta = sfile + \'.meta\'\n      os.remove(str(sfile_meta))\n      ss_paths.remove(sfile)\n\n  def train_model(self, sess, max_iters):\n    # Build data layers for both training and validation set\n    self.data_layer = RoIDataLayer(self.roidb, self.imdb.num_classes)\n    self.data_layer_val = RoIDataLayer(self.valroidb, self.imdb.num_classes, random=True)\n\n    # Construct the computation graph\n    lr, train_op = self.construct_graph(sess)\n\n    # Find previous snapshots if there is any to restore from\n    lsf, nfiles, sfiles = self.find_previous()\n\n    # Initialize the variables or restore them from the last snapshot\n    if lsf == 0:\n      rate, last_snapshot_iter, stepsizes, np_paths, ss_paths = self.initialize(sess)\n    else:\n      rate, last_snapshot_iter, stepsizes, np_paths, ss_paths = self.restore(sess, \n                                                                            str(sfiles[-1]), \n                                                                            str(nfiles[-1]))\n    timer = Timer()\n    iter = last_snapshot_iter + 1\n    last_summary_time = time.time()\n    # Make sure the lists are not empty\n    stepsizes.append(max_iters)\n    stepsizes.reverse()\n    next_stepsize = stepsizes.pop()\n    while iter < max_iters + 1:\n      # Learning rate\n      if iter == next_stepsize + 1:\n        # Add snapshot here before reducing the learning rate\n        self.snapshot(sess, iter)\n        rate *= cfg.TRAIN.GAMMA\n        sess.run(tf.assign(lr, rate))\n        next_stepsize = stepsizes.pop()\n\n      timer.tic()\n      # Get training data, one batch at a time\n      blobs = self.data_layer.forward()\n\n      now = time.time()\n      if iter == 1 or now - last_summary_time > cfg.TRAIN.SUMMARY_INTERVAL:\n        # Compute the graph with summary\n        rpn_loss_cls, rpn_loss_box, loss_cls, loss_box, total_loss, summary = \\\n          self.net.train_step_with_summary(sess, blobs, train_op)\n        self.writer.add_summary(summary, float(iter))\n        # Also check the summary on the validation set\n        blobs_val = self.data_layer_val.forward()\n        summary_val = self.net.get_summary(sess, blobs_val)\n        self.valwriter.add_summary(summary_val, float(iter))\n        last_summary_time = now\n      else:\n        # Compute the graph without summary\n        rpn_loss_cls, rpn_loss_box, loss_cls, loss_box, total_loss = \\\n          self.net.train_step(sess, blobs, train_op)\n      timer.toc()\n\n      # Display training information\n      if iter % (cfg.TRAIN.DISPLAY) == 0:\n        print(\'iter: %d / %d, total loss: %.6f\\n >>> rpn_loss_cls: %.6f\\n \'\n              \'>>> rpn_loss_box: %.6f\\n >>> loss_cls: %.6f\\n >>> loss_box: %.6f\\n >>> lr: %f\' % \\\n              (iter, max_iters, total_loss, rpn_loss_cls, rpn_loss_box, loss_cls, loss_box, lr.eval()))\n        print(\'speed: {:.3f}s / iter\'.format(timer.average_time))\n\n      # Snapshotting\n      if iter % cfg.TRAIN.SNAPSHOT_ITERS == 0:\n        last_snapshot_iter = iter\n        ss_path, np_path = self.snapshot(sess, iter)\n        np_paths.append(np_path)\n        ss_paths.append(ss_path)\n\n        # Remove the old snapshots if there are too many\n        if len(np_paths) > cfg.TRAIN.SNAPSHOT_KEPT:\n          self.remove_snapshot(np_paths, ss_paths)\n\n      iter += 1\n\n    if last_snapshot_iter != iter - 1:\n      self.snapshot(sess, iter - 1)\n\n    self.writer.close()\n    self.valwriter.close()\n\n\ndef get_training_roidb(imdb):\n  """"""Returns a roidb (Region of Interest database) for use in training.""""""\n  if cfg.TRAIN.USE_FLIPPED:\n    print(\'Appending horizontally-flipped training examples...\')\n    imdb.append_flipped_images()\n    print(\'done\')\n\n  print(\'Preparing training data...\')\n  rdl_roidb.prepare_roidb(imdb)\n  print(\'done\')\n\n  return imdb.roidb\n\n\ndef filter_roidb(roidb):\n  """"""Remove roidb entries that have no usable RoIs.""""""\n\n  def is_valid(entry):\n    # Valid images have:\n    #   (1) At least one foreground RoI OR\n    #   (2) At least one background RoI\n    overlaps = entry[\'max_overlaps\']\n    # find boxes with sufficient overlap\n    fg_inds = np.where(overlaps >= cfg.TRAIN.FG_THRESH)[0]\n    # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n    bg_inds = np.where((overlaps < cfg.TRAIN.BG_THRESH_HI) &\n                       (overlaps >= cfg.TRAIN.BG_THRESH_LO))[0]\n    # image is only valid if such boxes exist\n    valid = len(fg_inds) > 0 or len(bg_inds) > 0\n    return valid\n\n  num = len(roidb)\n  filtered_roidb = [entry for entry in roidb if is_valid(entry)]\n  num_after = len(filtered_roidb)\n  print(\'Filtered {} roidb entries: {} -> {}\'.format(num - num_after,\n                                                     num, num_after))\n  return filtered_roidb\n\n\ndef train_net(network, imdb, roidb, valroidb, output_dir, tb_dir,\n              pretrained_model=None,\n              max_iters=40000):\n  """"""Train a Faster R-CNN network.""""""\n  roidb = filter_roidb(roidb)\n  valroidb = filter_roidb(valroidb)\n\n  tfconfig = tf.ConfigProto(allow_soft_placement=True)\n  tfconfig.gpu_options.allow_growth = True\n\n  with tf.Session(config=tfconfig) as sess:\n    sw = SolverWrapper(sess, network, imdb, roidb, valroidb, output_dir, tb_dir,\n                       pretrained_model=pretrained_model)\n    print(\'Solving...\')\n    sw.train_model(sess, max_iters)\n    print(\'done solving\')\n'"
lib/nets/__init__.py,0,b''
lib/nets/mobilenet_v1.py,12,"b'# --------------------------------------------------------\n# Tensorflow Faster R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Xinlei Chen\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.contrib.slim import losses\nfrom tensorflow.contrib.slim import arg_scope\nfrom tensorflow.contrib.slim.python.slim.nets import resnet_utils\nimport numpy as np\nfrom collections import namedtuple\n\nfrom nets.network import Network\nfrom model.config import cfg\n\ndef separable_conv2d_same(inputs, kernel_size, stride, rate=1, scope=None):\n  """"""Strided 2-D separable convolution with \'SAME\' padding.\n  Args:\n    inputs: A 4-D tensor of size [batch, height_in, width_in, channels].\n    kernel_size: An int with the kernel_size of the filters.\n    stride: An integer, the output stride.\n    rate: An integer, rate for atrous convolution.\n    scope: Scope.\n  Returns:\n    output: A 4-D tensor of size [batch, height_out, width_out, channels] with\n      the convolution output.\n  """"""\n\n  # By passing filters=None\n  # separable_conv2d produces only a depth-wise convolution layer\n  if stride == 1:\n    return slim.separable_conv2d(inputs, None, kernel_size, \n                                  depth_multiplier=1, stride=1, rate=rate,\n                                  padding=\'SAME\', scope=scope)\n  else:\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    inputs = tf.pad(inputs,\n                    [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n    return slim.separable_conv2d(inputs, None, kernel_size, \n                                  depth_multiplier=1, stride=stride, rate=rate, \n                                  padding=\'VALID\', scope=scope)\n\n# The following is adapted from:\n# https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.py\n\n# Conv and DepthSepConv named tuple define layers of the MobileNet architecture\n# Conv defines 3x3 convolution layers\n# DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.\n# stride is the stride of the convolution\n# depth is the number of channels or filters in a layer\nConv = namedtuple(\'Conv\', [\'kernel\', \'stride\', \'depth\'])\nDepthSepConv = namedtuple(\'DepthSepConv\', [\'kernel\', \'stride\', \'depth\'])\n\n# _CONV_DEFS specifies the MobileNet body\n_CONV_DEFS = [\n    Conv(kernel=3, stride=2, depth=32),\n    DepthSepConv(kernel=3, stride=1, depth=64),\n    DepthSepConv(kernel=3, stride=2, depth=128),\n    DepthSepConv(kernel=3, stride=1, depth=128),\n    DepthSepConv(kernel=3, stride=2, depth=256),\n    DepthSepConv(kernel=3, stride=1, depth=256),\n    DepthSepConv(kernel=3, stride=2, depth=512),\n    DepthSepConv(kernel=3, stride=1, depth=512),\n    DepthSepConv(kernel=3, stride=1, depth=512),\n    DepthSepConv(kernel=3, stride=1, depth=512),\n    DepthSepConv(kernel=3, stride=1, depth=512),\n    DepthSepConv(kernel=3, stride=1, depth=512),\n    # use stride 1 for the 13th layer\n    DepthSepConv(kernel=3, stride=1, depth=1024),\n    DepthSepConv(kernel=3, stride=1, depth=1024)\n]\n\n# Modified mobilenet_v1\ndef mobilenet_v1_base(inputs,\n                      conv_defs,\n                      starting_layer=0,\n                      min_depth=8,\n                      depth_multiplier=1.0,\n                      output_stride=None,\n                      reuse=None,\n                      scope=None):\n  """"""Mobilenet v1.\n  Constructs a Mobilenet v1 network from inputs to the given final endpoint.\n  Args:\n    inputs: a tensor of shape [batch_size, height, width, channels].\n    starting_layer: specifies the current starting layer. For region proposal \n      network it is 0, for region classification it is 12 by default.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    conv_defs: A list of ConvDef named tuples specifying the net architecture.\n    output_stride: An integer that specifies the requested ratio of input to\n      output spatial resolution. If not None, then we invoke atrous convolution\n      if necessary to prevent the network from reducing the spatial resolution\n      of the activation maps. \n    scope: Optional variable_scope.\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n  Raises:\n    ValueError: if depth_multiplier <= 0, or convolution type is not defined.\n  """"""\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n  end_points = {}\n\n  # Used to find thinned depths for each layer.\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n\n  with tf.variable_scope(scope, \'MobilenetV1\', [inputs], reuse=reuse):\n    # The current_stride variable keeps track of the output stride of the\n    # activations, i.e., the running product of convolution strides up to the\n    # current network layer. This allows us to invoke atrous convolution\n    # whenever applying the next convolution would result in the activations\n    # having output stride larger than the target output_stride.\n    current_stride = 1\n\n    # The atrous convolution rate parameter.\n    rate = 1\n\n    net = inputs\n    for i, conv_def in enumerate(conv_defs):\n      end_point_base = \'Conv2d_%d\' % (i + starting_layer)\n\n      if output_stride is not None and current_stride == output_stride:\n        # If we have reached the target output_stride, then we need to employ\n        # atrous convolution with stride=1 and multiply the atrous rate by the\n        # current unit\'s stride for use in subsequent layers.\n        layer_stride = 1\n        layer_rate = rate\n        rate *= conv_def.stride\n      else:\n        layer_stride = conv_def.stride\n        layer_rate = 1\n        current_stride *= conv_def.stride\n\n      if isinstance(conv_def, Conv):\n        end_point = end_point_base\n        net = resnet_utils.conv2d_same(net, depth(conv_def.depth), conv_def.kernel,\n                          stride=conv_def.stride,\n                          scope=end_point)\n\n      elif isinstance(conv_def, DepthSepConv):\n        end_point = end_point_base + \'_depthwise\'\n        \n        net = separable_conv2d_same(net, conv_def.kernel,\n                                    stride=layer_stride,\n                                    rate=layer_rate,\n                                    scope=end_point)\n\n        end_point = end_point_base + \'_pointwise\'\n\n        net = slim.conv2d(net, depth(conv_def.depth), [1, 1],\n                          stride=1,\n                          scope=end_point)\n\n      else:\n        raise ValueError(\'Unknown convolution type %s for layer %d\'\n                         % (conv_def.ltype, i))\n\n    return net\n\n# Modified arg_scope to incorporate configs\ndef mobilenet_v1_arg_scope(is_training=True,\n                           stddev=0.09):\n  batch_norm_params = {\n      \'is_training\': False,\n      \'center\': True,\n      \'scale\': True,\n      \'decay\': 0.9997,\n      \'epsilon\': 0.001,\n      \'trainable\': False,\n  }\n\n  # Set weight_decay for weights in Conv and DepthSepConv layers.\n  weights_init = tf.truncated_normal_initializer(stddev=stddev)\n  regularizer = tf.contrib.layers.l2_regularizer(cfg.MOBILENET.WEIGHT_DECAY)\n  if cfg.MOBILENET.REGU_DEPTH:\n    depthwise_regularizer = regularizer\n  else:\n    depthwise_regularizer = None\n\n  with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n                      trainable=is_training,\n                      weights_initializer=weights_init,\n                      activation_fn=tf.nn.relu6, \n                      normalizer_fn=slim.batch_norm,\n                      padding=\'SAME\'):\n    with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n      with slim.arg_scope([slim.conv2d], weights_regularizer=regularizer):\n        with slim.arg_scope([slim.separable_conv2d],\n                            weights_regularizer=depthwise_regularizer) as sc:\n          return sc\n\nclass mobilenetv1(Network):\n  def __init__(self):\n    Network.__init__(self)\n    self._feat_stride = [16, ]\n    self._feat_compress = [1. / float(self._feat_stride[0]), ]\n    self._depth_multiplier = cfg.MOBILENET.DEPTH_MULTIPLIER\n    self._scope = \'MobilenetV1\'\n\n  def _image_to_head(self, is_training, reuse=None):\n    # Base bottleneck\n    assert (0 <= cfg.MOBILENET.FIXED_LAYERS <= 12)\n    net_conv = self._image\n    if cfg.MOBILENET.FIXED_LAYERS > 0:\n      with slim.arg_scope(mobilenet_v1_arg_scope(is_training=False)):\n        net_conv = mobilenet_v1_base(net_conv,\n                                      _CONV_DEFS[:cfg.MOBILENET.FIXED_LAYERS],\n                                      starting_layer=0,\n                                      depth_multiplier=self._depth_multiplier,\n                                      reuse=reuse,\n                                      scope=self._scope)\n    if cfg.MOBILENET.FIXED_LAYERS < 12:\n      with slim.arg_scope(mobilenet_v1_arg_scope(is_training=is_training)):\n        net_conv = mobilenet_v1_base(net_conv,\n                                      _CONV_DEFS[cfg.MOBILENET.FIXED_LAYERS:12],\n                                      starting_layer=cfg.MOBILENET.FIXED_LAYERS,\n                                      depth_multiplier=self._depth_multiplier,\n                                      reuse=reuse,\n                                      scope=self._scope)\n\n    self._act_summaries.append(net_conv)\n    self._layers[\'head\'] = net_conv\n\n    return net_conv\n\n  def _head_to_tail(self, pool5, is_training, reuse=None):\n    with slim.arg_scope(mobilenet_v1_arg_scope(is_training=is_training)):\n      fc7 = mobilenet_v1_base(pool5,\n                              _CONV_DEFS[12:],\n                              starting_layer=12,\n                              depth_multiplier=self._depth_multiplier,\n                              reuse=reuse,\n                              scope=self._scope)\n      # average pooling done by reduce_mean\n      fc7 = tf.reduce_mean(fc7, axis=[1, 2])\n    return fc7\n\n  def get_variables_to_restore(self, variables, var_keep_dic):\n    variables_to_restore = []\n\n    for v in variables:\n      # exclude the first conv layer to swap RGB to BGR\n      if v.name == (self._scope + \'/Conv2d_0/weights:0\'):\n        self._variables_to_fix[v.name] = v\n        continue\n      if v.name.split(\':\')[0] in var_keep_dic:\n        print(\'Variables restored: %s\' % v.name)\n        variables_to_restore.append(v)\n\n    return variables_to_restore\n\n  def fix_variables(self, sess, pretrained_model):\n    print(\'Fix MobileNet V1 layers..\')\n    with tf.variable_scope(\'Fix_MobileNet_V1\') as scope:\n      with tf.device(""/cpu:0""):\n        # fix RGB to BGR, and match the scale by (255.0 / 2.0)\n        Conv2d_0_rgb = tf.get_variable(""Conv2d_0_rgb"", \n                                    [3, 3, 3, max(int(32 * self._depth_multiplier), 8)], \n                                    trainable=False)\n        restorer_fc = tf.train.Saver({self._scope + ""/Conv2d_0/weights"": Conv2d_0_rgb})\n        restorer_fc.restore(sess, pretrained_model)\n\n        sess.run(tf.assign(self._variables_to_fix[self._scope + ""/Conv2d_0/weights:0""], \n                           tf.reverse(Conv2d_0_rgb / (255.0 / 2.0), [2])))\n'"
lib/nets/network.py,90,"b'# --------------------------------------------------------\n# Tensorflow Faster R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Xinlei Chen\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.contrib.slim import losses\nfrom tensorflow.contrib.slim import arg_scope\n\nimport numpy as np\n\nfrom layer_utils.snippets import generate_anchors_pre, generate_anchors_pre_tf\nfrom layer_utils.proposal_layer import proposal_layer, proposal_layer_tf\nfrom layer_utils.proposal_top_layer import proposal_top_layer, proposal_top_layer_tf\nfrom layer_utils.anchor_target_layer import anchor_target_layer\nfrom layer_utils.proposal_target_layer import proposal_target_layer\nfrom utils.visualization import draw_bounding_boxes\n\nfrom model.config import cfg\n\nclass Network(object):\n  def __init__(self):\n    self._predictions = {}\n    self._losses = {}\n    self._anchor_targets = {}\n    self._proposal_targets = {}\n    self._layers = {}\n    self._gt_image = None\n    self._act_summaries = []\n    self._score_summaries = {}\n    self._train_summaries = []\n    self._event_summaries = {}\n    self._variables_to_fix = {}\n\n  def _add_gt_image(self):\n    # add back mean\n    image = self._image + cfg.PIXEL_MEANS\n    # BGR to RGB (opencv uses BGR)\n    resized = tf.image.resize_bilinear(image, tf.to_int32(self._im_info[:2] / self._im_info[2]))\n    self._gt_image = tf.reverse(resized, axis=[-1])\n\n  def _add_gt_image_summary(self):\n    # use a customized visualization function to visualize the boxes\n    if self._gt_image is None:\n      self._add_gt_image()\n    image = tf.py_func(draw_bounding_boxes, \n                      [self._gt_image, self._gt_boxes, self._im_info],\n                      tf.float32, name=""gt_boxes"")\n    \n    return tf.summary.image(\'GROUND_TRUTH\', image)\n\n  def _add_act_summary(self, tensor):\n    tf.summary.histogram(\'ACT/\' + tensor.op.name + \'/activations\', tensor)\n    tf.summary.scalar(\'ACT/\' + tensor.op.name + \'/zero_fraction\',\n                      tf.nn.zero_fraction(tensor))\n\n  def _add_score_summary(self, key, tensor):\n    tf.summary.histogram(\'SCORE/\' + tensor.op.name + \'/\' + key + \'/scores\', tensor)\n\n  def _add_train_summary(self, var):\n    tf.summary.histogram(\'TRAIN/\' + var.op.name, var)\n\n  def _reshape_layer(self, bottom, num_dim, name):\n    input_shape = tf.shape(bottom)\n    with tf.variable_scope(name) as scope:\n      # change the channel to the caffe format\n      to_caffe = tf.transpose(bottom, [0, 3, 1, 2])\n      # then force it to have channel 2\n      reshaped = tf.reshape(to_caffe,\n                            tf.concat(axis=0, values=[[1, num_dim, -1], [input_shape[2]]]))\n      # then swap the channel back\n      to_tf = tf.transpose(reshaped, [0, 2, 3, 1])\n      return to_tf\n\n  def _softmax_layer(self, bottom, name):\n    if name.startswith(\'rpn_cls_prob_reshape\'):\n      input_shape = tf.shape(bottom)\n      bottom_reshaped = tf.reshape(bottom, [-1, input_shape[-1]])\n      reshaped_score = tf.nn.softmax(bottom_reshaped, name=name)\n      return tf.reshape(reshaped_score, input_shape)\n    return tf.nn.softmax(bottom, name=name)\n\n  def _proposal_top_layer(self, rpn_cls_prob, rpn_bbox_pred, name):\n    with tf.variable_scope(name) as scope:\n      if cfg.USE_E2E_TF:\n        rois, rpn_scores = proposal_top_layer_tf(\n          rpn_cls_prob,\n          rpn_bbox_pred,\n          self._im_info,\n          self._feat_stride,\n          self._anchors,\n          self._num_anchors\n        )\n      else:\n        rois, rpn_scores = tf.py_func(proposal_top_layer,\n                              [rpn_cls_prob, rpn_bbox_pred, self._im_info,\n                               self._feat_stride, self._anchors, self._num_anchors],\n                              [tf.float32, tf.float32], name=""proposal_top"")\n        \n      rois.set_shape([cfg.TEST.RPN_TOP_N, 5])\n      rpn_scores.set_shape([cfg.TEST.RPN_TOP_N, 1])\n\n    return rois, rpn_scores\n\n  def _proposal_layer(self, rpn_cls_prob, rpn_bbox_pred, name):\n    with tf.variable_scope(name) as scope:\n      if cfg.USE_E2E_TF:\n        rois, rpn_scores = proposal_layer_tf(\n          rpn_cls_prob,\n          rpn_bbox_pred,\n          self._im_info,\n          self._mode,\n          self._feat_stride,\n          self._anchors,\n          self._num_anchors\n        )\n      else:\n        rois, rpn_scores = tf.py_func(proposal_layer,\n                              [rpn_cls_prob, rpn_bbox_pred, self._im_info, self._mode,\n                               self._feat_stride, self._anchors, self._num_anchors],\n                              [tf.float32, tf.float32], name=""proposal"")\n\n      rois.set_shape([None, 5])\n      rpn_scores.set_shape([None, 1])\n\n    return rois, rpn_scores\n\n  # Only use it if you have roi_pooling op written in tf.image\n  def _roi_pool_layer(self, bootom, rois, name):\n    with tf.variable_scope(name) as scope:\n      return tf.image.roi_pooling(bootom, rois,\n                                  pooled_height=cfg.POOLING_SIZE,\n                                  pooled_width=cfg.POOLING_SIZE,\n                                  spatial_scale=1. / 16.)[0]\n\n  def _crop_pool_layer(self, bottom, rois, name):\n    with tf.variable_scope(name) as scope:\n      batch_ids = tf.squeeze(tf.slice(rois, [0, 0], [-1, 1], name=""batch_id""), [1])\n      # Get the normalized coordinates of bounding boxes\n      bottom_shape = tf.shape(bottom)\n      height = (tf.to_float(bottom_shape[1]) - 1.) * np.float32(self._feat_stride[0])\n      width = (tf.to_float(bottom_shape[2]) - 1.) * np.float32(self._feat_stride[0])\n      x1 = tf.slice(rois, [0, 1], [-1, 1], name=""x1"") / width\n      y1 = tf.slice(rois, [0, 2], [-1, 1], name=""y1"") / height\n      x2 = tf.slice(rois, [0, 3], [-1, 1], name=""x2"") / width\n      y2 = tf.slice(rois, [0, 4], [-1, 1], name=""y2"") / height\n      # Won\'t be back-propagated to rois anyway, but to save time\n      bboxes = tf.stop_gradient(tf.concat([y1, x1, y2, x2], axis=1))\n      pre_pool_size = cfg.POOLING_SIZE * 2\n      crops = tf.image.crop_and_resize(bottom, bboxes, tf.to_int32(batch_ids), [pre_pool_size, pre_pool_size], name=""crops"")\n\n    return slim.max_pool2d(crops, [2, 2], padding=\'SAME\')\n\n  def _dropout_layer(self, bottom, name, ratio=0.5):\n    return tf.nn.dropout(bottom, ratio, name=name)\n\n  def _anchor_target_layer(self, rpn_cls_score, name):\n    with tf.variable_scope(name) as scope:\n      rpn_labels, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights = tf.py_func(\n        anchor_target_layer,\n        [rpn_cls_score, self._gt_boxes, self._im_info, self._feat_stride, self._anchors, self._num_anchors],\n        [tf.float32, tf.float32, tf.float32, tf.float32],\n        name=""anchor_target"")\n\n      rpn_labels.set_shape([1, 1, None, None])\n      rpn_bbox_targets.set_shape([1, None, None, self._num_anchors * 4])\n      rpn_bbox_inside_weights.set_shape([1, None, None, self._num_anchors * 4])\n      rpn_bbox_outside_weights.set_shape([1, None, None, self._num_anchors * 4])\n\n      rpn_labels = tf.to_int32(rpn_labels, name=""to_int32"")\n      self._anchor_targets[\'rpn_labels\'] = rpn_labels\n      self._anchor_targets[\'rpn_bbox_targets\'] = rpn_bbox_targets\n      self._anchor_targets[\'rpn_bbox_inside_weights\'] = rpn_bbox_inside_weights\n      self._anchor_targets[\'rpn_bbox_outside_weights\'] = rpn_bbox_outside_weights\n\n      self._score_summaries.update(self._anchor_targets)\n\n    return rpn_labels\n\n  def _proposal_target_layer(self, rois, roi_scores, name):\n    with tf.variable_scope(name) as scope:\n      rois, roi_scores, labels, bbox_targets, bbox_inside_weights, bbox_outside_weights = tf.py_func(\n        proposal_target_layer,\n        [rois, roi_scores, self._gt_boxes, self._num_classes],\n        [tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32],\n        name=""proposal_target"")\n\n      rois.set_shape([cfg.TRAIN.BATCH_SIZE, 5])\n      roi_scores.set_shape([cfg.TRAIN.BATCH_SIZE])\n      labels.set_shape([cfg.TRAIN.BATCH_SIZE, 1])\n      bbox_targets.set_shape([cfg.TRAIN.BATCH_SIZE, self._num_classes * 4])\n      bbox_inside_weights.set_shape([cfg.TRAIN.BATCH_SIZE, self._num_classes * 4])\n      bbox_outside_weights.set_shape([cfg.TRAIN.BATCH_SIZE, self._num_classes * 4])\n\n      self._proposal_targets[\'rois\'] = rois\n      self._proposal_targets[\'labels\'] = tf.to_int32(labels, name=""to_int32"")\n      self._proposal_targets[\'bbox_targets\'] = bbox_targets\n      self._proposal_targets[\'bbox_inside_weights\'] = bbox_inside_weights\n      self._proposal_targets[\'bbox_outside_weights\'] = bbox_outside_weights\n\n      self._score_summaries.update(self._proposal_targets)\n\n      return rois, roi_scores\n\n  def _anchor_component(self):\n    with tf.variable_scope(\'ANCHOR_\' + self._tag) as scope:\n      # just to get the shape right\n      height = tf.to_int32(tf.ceil(self._im_info[0] / np.float32(self._feat_stride[0])))\n      width = tf.to_int32(tf.ceil(self._im_info[1] / np.float32(self._feat_stride[0])))\n      if cfg.USE_E2E_TF:\n        anchors, anchor_length = generate_anchors_pre_tf(\n          height,\n          width,\n          self._feat_stride,\n          self._anchor_scales,\n          self._anchor_ratios\n        )\n      else:\n        anchors, anchor_length = tf.py_func(generate_anchors_pre,\n                                            [height, width,\n                                             self._feat_stride, self._anchor_scales, self._anchor_ratios],\n                                            [tf.float32, tf.int32], name=""generate_anchors"")\n      anchors.set_shape([None, 4])\n      anchor_length.set_shape([])\n      self._anchors = anchors\n      self._anchor_length = anchor_length\n\n  def _build_network(self, is_training=True):\n    # select initializers\n    if cfg.TRAIN.TRUNCATED:\n      initializer = tf.truncated_normal_initializer(mean=0.0, stddev=0.01)\n      initializer_bbox = tf.truncated_normal_initializer(mean=0.0, stddev=0.001)\n    else:\n      initializer = tf.random_normal_initializer(mean=0.0, stddev=0.01)\n      initializer_bbox = tf.random_normal_initializer(mean=0.0, stddev=0.001)\n\n    net_conv = self._image_to_head(is_training)\n    with tf.variable_scope(self._scope, self._scope):\n      # build the anchors for the image\n      self._anchor_component()\n      # region proposal network\n      rois = self._region_proposal(net_conv, is_training, initializer)\n      # region of interest pooling\n      if cfg.POOLING_MODE == \'crop\':\n        pool5 = self._crop_pool_layer(net_conv, rois, ""pool5"")\n      else:\n        raise NotImplementedError\n\n    fc7 = self._head_to_tail(pool5, is_training)\n    with tf.variable_scope(self._scope, self._scope):\n      # region classification\n      cls_prob, bbox_pred = self._region_classification(fc7, is_training, \n                                                        initializer, initializer_bbox)\n\n    self._score_summaries.update(self._predictions)\n\n    return rois, cls_prob, bbox_pred\n\n  def _smooth_l1_loss(self, bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights, sigma=1.0, dim=[1]):\n    sigma_2 = sigma ** 2\n    box_diff = bbox_pred - bbox_targets\n    in_box_diff = bbox_inside_weights * box_diff\n    abs_in_box_diff = tf.abs(in_box_diff)\n    smoothL1_sign = tf.stop_gradient(tf.to_float(tf.less(abs_in_box_diff, 1. / sigma_2)))\n    in_loss_box = tf.pow(in_box_diff, 2) * (sigma_2 / 2.) * smoothL1_sign \\\n                  + (abs_in_box_diff - (0.5 / sigma_2)) * (1. - smoothL1_sign)\n    out_loss_box = bbox_outside_weights * in_loss_box\n    loss_box = tf.reduce_mean(tf.reduce_sum(\n      out_loss_box,\n      axis=dim\n    ))\n    return loss_box\n\n  def _add_losses(self, sigma_rpn=3.0):\n    with tf.variable_scope(\'LOSS_\' + self._tag) as scope:\n      # RPN, class loss\n      rpn_cls_score = tf.reshape(self._predictions[\'rpn_cls_score_reshape\'], [-1, 2])\n      rpn_label = tf.reshape(self._anchor_targets[\'rpn_labels\'], [-1])\n      rpn_select = tf.where(tf.not_equal(rpn_label, -1))\n      rpn_cls_score = tf.reshape(tf.gather(rpn_cls_score, rpn_select), [-1, 2])\n      rpn_label = tf.reshape(tf.gather(rpn_label, rpn_select), [-1])\n      rpn_cross_entropy = tf.reduce_mean(\n        tf.nn.sparse_softmax_cross_entropy_with_logits(logits=rpn_cls_score, labels=rpn_label))\n\n      # RPN, bbox loss\n      rpn_bbox_pred = self._predictions[\'rpn_bbox_pred\']\n      rpn_bbox_targets = self._anchor_targets[\'rpn_bbox_targets\']\n      rpn_bbox_inside_weights = self._anchor_targets[\'rpn_bbox_inside_weights\']\n      rpn_bbox_outside_weights = self._anchor_targets[\'rpn_bbox_outside_weights\']\n      rpn_loss_box = self._smooth_l1_loss(rpn_bbox_pred, rpn_bbox_targets, rpn_bbox_inside_weights,\n                                          rpn_bbox_outside_weights, sigma=sigma_rpn, dim=[1, 2, 3])\n\n      # RCNN, class loss\n      cls_score = self._predictions[""cls_score""]\n      label = tf.reshape(self._proposal_targets[""labels""], [-1])\n      cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=cls_score, labels=label))\n\n      # RCNN, bbox loss\n      bbox_pred = self._predictions[\'bbox_pred\']\n      bbox_targets = self._proposal_targets[\'bbox_targets\']\n      bbox_inside_weights = self._proposal_targets[\'bbox_inside_weights\']\n      bbox_outside_weights = self._proposal_targets[\'bbox_outside_weights\']\n      loss_box = self._smooth_l1_loss(bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights)\n\n      self._losses[\'cross_entropy\'] = cross_entropy\n      self._losses[\'loss_box\'] = loss_box\n      self._losses[\'rpn_cross_entropy\'] = rpn_cross_entropy\n      self._losses[\'rpn_loss_box\'] = rpn_loss_box\n\n      loss = cross_entropy + loss_box + rpn_cross_entropy + rpn_loss_box\n      regularization_loss = tf.add_n(tf.losses.get_regularization_losses(), \'regu\')\n      self._losses[\'total_loss\'] = loss + regularization_loss\n\n      self._event_summaries.update(self._losses)\n\n    return loss\n\n  def _region_proposal(self, net_conv, is_training, initializer):\n    rpn = slim.conv2d(net_conv, cfg.RPN_CHANNELS, [3, 3], trainable=is_training, weights_initializer=initializer,\n                        scope=""rpn_conv/3x3"")\n    self._act_summaries.append(rpn)\n    rpn_cls_score = slim.conv2d(rpn, self._num_anchors * 2, [1, 1], trainable=is_training,\n                                weights_initializer=initializer,\n                                padding=\'VALID\', activation_fn=None, scope=\'rpn_cls_score\')\n    # change it so that the score has 2 as its channel size\n    rpn_cls_score_reshape = self._reshape_layer(rpn_cls_score, 2, \'rpn_cls_score_reshape\')\n    rpn_cls_prob_reshape = self._softmax_layer(rpn_cls_score_reshape, ""rpn_cls_prob_reshape"")\n    rpn_cls_pred = tf.argmax(tf.reshape(rpn_cls_score_reshape, [-1, 2]), axis=1, name=""rpn_cls_pred"")\n    rpn_cls_prob = self._reshape_layer(rpn_cls_prob_reshape, self._num_anchors * 2, ""rpn_cls_prob"")\n    rpn_bbox_pred = slim.conv2d(rpn, self._num_anchors * 4, [1, 1], trainable=is_training,\n                                weights_initializer=initializer,\n                                padding=\'VALID\', activation_fn=None, scope=\'rpn_bbox_pred\')\n    if is_training:\n      rois, roi_scores = self._proposal_layer(rpn_cls_prob, rpn_bbox_pred, ""rois"")\n      rpn_labels = self._anchor_target_layer(rpn_cls_score, ""anchor"")\n      # Try to have a deterministic order for the computing graph, for reproducibility\n      with tf.control_dependencies([rpn_labels]):\n        rois, _ = self._proposal_target_layer(rois, roi_scores, ""rpn_rois"")\n    else:\n      if cfg.TEST.MODE == \'nms\':\n        rois, _ = self._proposal_layer(rpn_cls_prob, rpn_bbox_pred, ""rois"")\n      elif cfg.TEST.MODE == \'top\':\n        rois, _ = self._proposal_top_layer(rpn_cls_prob, rpn_bbox_pred, ""rois"")\n      else:\n        raise NotImplementedError\n\n    self._predictions[""rpn_cls_score""] = rpn_cls_score\n    self._predictions[""rpn_cls_score_reshape""] = rpn_cls_score_reshape\n    self._predictions[""rpn_cls_prob""] = rpn_cls_prob\n    self._predictions[""rpn_cls_pred""] = rpn_cls_pred\n    self._predictions[""rpn_bbox_pred""] = rpn_bbox_pred\n    self._predictions[""rois""] = rois\n\n    return rois\n\n  def _region_classification(self, fc7, is_training, initializer, initializer_bbox):\n    cls_score = slim.fully_connected(fc7, self._num_classes, \n                                       weights_initializer=initializer,\n                                       trainable=is_training,\n                                       activation_fn=None, scope=\'cls_score\')\n    cls_prob = self._softmax_layer(cls_score, ""cls_prob"")\n    cls_pred = tf.argmax(cls_score, axis=1, name=""cls_pred"")\n    bbox_pred = slim.fully_connected(fc7, self._num_classes * 4, \n                                     weights_initializer=initializer_bbox,\n                                     trainable=is_training,\n                                     activation_fn=None, scope=\'bbox_pred\')\n\n    self._predictions[""cls_score""] = cls_score\n    self._predictions[""cls_pred""] = cls_pred\n    self._predictions[""cls_prob""] = cls_prob\n    self._predictions[""bbox_pred""] = bbox_pred\n\n    return cls_prob, bbox_pred\n\n  def _image_to_head(self, is_training, reuse=None):\n    raise NotImplementedError\n\n  def _head_to_tail(self, pool5, is_training, reuse=None):\n    raise NotImplementedError\n\n  def create_architecture(self, mode, num_classes, tag=None,\n                          anchor_scales=(8, 16, 32), anchor_ratios=(0.5, 1, 2)):\n    self._image = tf.placeholder(tf.float32, shape=[1, None, None, 3])\n    self._im_info = tf.placeholder(tf.float32, shape=[3])\n    self._gt_boxes = tf.placeholder(tf.float32, shape=[None, 5])\n    self._tag = tag\n\n    self._num_classes = num_classes\n    self._mode = mode\n    self._anchor_scales = anchor_scales\n    self._num_scales = len(anchor_scales)\n\n    self._anchor_ratios = anchor_ratios\n    self._num_ratios = len(anchor_ratios)\n\n    self._num_anchors = self._num_scales * self._num_ratios\n\n    training = mode == \'TRAIN\'\n    testing = mode == \'TEST\'\n\n    assert tag != None\n\n    # handle most of the regularizers here\n    weights_regularizer = tf.contrib.layers.l2_regularizer(cfg.TRAIN.WEIGHT_DECAY)\n    if cfg.TRAIN.BIAS_DECAY:\n      biases_regularizer = weights_regularizer\n    else:\n      biases_regularizer = tf.no_regularizer\n\n    # list as many types of layers as possible, even if they are not used now\n    with arg_scope([slim.conv2d, slim.conv2d_in_plane, \\\n                    slim.conv2d_transpose, slim.separable_conv2d, slim.fully_connected], \n                    weights_regularizer=weights_regularizer,\n                    biases_regularizer=biases_regularizer, \n                    biases_initializer=tf.constant_initializer(0.0)): \n      rois, cls_prob, bbox_pred = self._build_network(training)\n\n    layers_to_output = {\'rois\': rois}\n\n    for var in tf.trainable_variables():\n      self._train_summaries.append(var)\n\n    if testing:\n      stds = np.tile(np.array(cfg.TRAIN.BBOX_NORMALIZE_STDS), (self._num_classes))\n      means = np.tile(np.array(cfg.TRAIN.BBOX_NORMALIZE_MEANS), (self._num_classes))\n      self._predictions[""bbox_pred""] *= stds\n      self._predictions[""bbox_pred""] += means\n    else:\n      self._add_losses()\n      layers_to_output.update(self._losses)\n\n      val_summaries = []\n      with tf.device(""/cpu:0""):\n        val_summaries.append(self._add_gt_image_summary())\n        for key, var in self._event_summaries.items():\n          val_summaries.append(tf.summary.scalar(key, var))\n        for key, var in self._score_summaries.items():\n          self._add_score_summary(key, var)\n        for var in self._act_summaries:\n          self._add_act_summary(var)\n        for var in self._train_summaries:\n          self._add_train_summary(var)\n\n      self._summary_op = tf.summary.merge_all()\n      self._summary_op_val = tf.summary.merge(val_summaries)\n\n    layers_to_output.update(self._predictions)\n\n    return layers_to_output\n\n  def get_variables_to_restore(self, variables, var_keep_dic):\n    raise NotImplementedError\n\n  def fix_variables(self, sess, pretrained_model):\n    raise NotImplementedError\n\n  # Extract the head feature maps, for example for vgg16 it is conv5_3\n  # only useful during testing mode\n  def extract_head(self, sess, image):\n    feed_dict = {self._image: image}\n    feat = sess.run(self._layers[""head""], feed_dict=feed_dict)\n    return feat\n\n  # only useful during testing mode\n  def test_image(self, sess, image, im_info):\n    feed_dict = {self._image: image,\n                 self._im_info: im_info}\n\n    cls_score, cls_prob, bbox_pred, rois = sess.run([self._predictions[""cls_score""],\n                                                     self._predictions[\'cls_prob\'],\n                                                     self._predictions[\'bbox_pred\'],\n                                                     self._predictions[\'rois\']],\n                                                    feed_dict=feed_dict)\n    return cls_score, cls_prob, bbox_pred, rois\n\n  def get_summary(self, sess, blobs):\n    feed_dict = {self._image: blobs[\'data\'], self._im_info: blobs[\'im_info\'],\n                 self._gt_boxes: blobs[\'gt_boxes\']}\n    summary = sess.run(self._summary_op_val, feed_dict=feed_dict)\n\n    return summary\n\n  def train_step(self, sess, blobs, train_op):\n    feed_dict = {self._image: blobs[\'data\'], self._im_info: blobs[\'im_info\'],\n                 self._gt_boxes: blobs[\'gt_boxes\']}\n    rpn_loss_cls, rpn_loss_box, loss_cls, loss_box, loss, _ = sess.run([self._losses[""rpn_cross_entropy""],\n                                                                        self._losses[\'rpn_loss_box\'],\n                                                                        self._losses[\'cross_entropy\'],\n                                                                        self._losses[\'loss_box\'],\n                                                                        self._losses[\'total_loss\'],\n                                                                        train_op],\n                                                                       feed_dict=feed_dict)\n    return rpn_loss_cls, rpn_loss_box, loss_cls, loss_box, loss\n\n  def train_step_with_summary(self, sess, blobs, train_op):\n    feed_dict = {self._image: blobs[\'data\'], self._im_info: blobs[\'im_info\'],\n                 self._gt_boxes: blobs[\'gt_boxes\']}\n    rpn_loss_cls, rpn_loss_box, loss_cls, loss_box, loss, summary, _ = sess.run([self._losses[""rpn_cross_entropy""],\n                                                                                 self._losses[\'rpn_loss_box\'],\n                                                                                 self._losses[\'cross_entropy\'],\n                                                                                 self._losses[\'loss_box\'],\n                                                                                 self._losses[\'total_loss\'],\n                                                                                 self._summary_op,\n                                                                                 train_op],\n                                                                                feed_dict=feed_dict)\n    return rpn_loss_cls, rpn_loss_box, loss_cls, loss_box, loss, summary\n\n  def train_step_no_return(self, sess, blobs, train_op):\n    feed_dict = {self._image: blobs[\'data\'], self._im_info: blobs[\'im_info\'],\n                 self._gt_boxes: blobs[\'gt_boxes\']}\n    sess.run([train_op], feed_dict=feed_dict)\n\n'"
lib/nets/resnet_v1.py,23,"b'# --------------------------------------------------------\n# Tensorflow Faster R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Zheqi He and Xinlei Chen\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.contrib.slim import losses\nfrom tensorflow.contrib.slim import arg_scope\nfrom tensorflow.contrib.slim.python.slim.nets import resnet_utils\nfrom tensorflow.contrib.slim.python.slim.nets import resnet_v1\nfrom tensorflow.contrib.slim.python.slim.nets.resnet_v1 import resnet_v1_block\nimport numpy as np\n\nfrom nets.network import Network\nfrom model.config import cfg\n\ndef resnet_arg_scope(is_training=True,\n                     batch_norm_decay=0.997,\n                     batch_norm_epsilon=1e-5,\n                     batch_norm_scale=True):\n  batch_norm_params = {\n    \'is_training\': False,\n    \'decay\': batch_norm_decay,\n    \'epsilon\': batch_norm_epsilon,\n    \'scale\': batch_norm_scale,\n    \'trainable\': False,\n    \'updates_collections\': tf.GraphKeys.UPDATE_OPS\n  }\n\n  with arg_scope(\n      [slim.conv2d],\n      weights_regularizer=slim.l2_regularizer(cfg.TRAIN.WEIGHT_DECAY),\n      weights_initializer=slim.variance_scaling_initializer(),\n      trainable=is_training,\n      activation_fn=tf.nn.relu,\n      normalizer_fn=slim.batch_norm,\n      normalizer_params=batch_norm_params):\n    with arg_scope([slim.batch_norm], **batch_norm_params) as arg_sc:\n      return arg_sc\n\nclass resnetv1(Network):\n  def __init__(self, num_layers=50):\n    Network.__init__(self)\n    self._feat_stride = [16, ]\n    self._feat_compress = [1. / float(self._feat_stride[0]), ]\n    self._num_layers = num_layers\n    self._scope = \'resnet_v1_%d\' % num_layers\n    self._decide_blocks()\n\n  def _crop_pool_layer(self, bottom, rois, name):\n    with tf.variable_scope(name) as scope:\n      batch_ids = tf.squeeze(tf.slice(rois, [0, 0], [-1, 1], name=""batch_id""), [1])\n      # Get the normalized coordinates of bboxes\n      bottom_shape = tf.shape(bottom)\n      height = (tf.to_float(bottom_shape[1]) - 1.) * np.float32(self._feat_stride[0])\n      width = (tf.to_float(bottom_shape[2]) - 1.) * np.float32(self._feat_stride[0])\n      x1 = tf.slice(rois, [0, 1], [-1, 1], name=""x1"") / width\n      y1 = tf.slice(rois, [0, 2], [-1, 1], name=""y1"") / height\n      x2 = tf.slice(rois, [0, 3], [-1, 1], name=""x2"") / width\n      y2 = tf.slice(rois, [0, 4], [-1, 1], name=""y2"") / height\n      # Won\'t be back-propagated to rois anyway, but to save time\n      bboxes = tf.stop_gradient(tf.concat([y1, x1, y2, x2], 1))\n      if cfg.RESNET.MAX_POOL:\n        pre_pool_size = cfg.POOLING_SIZE * 2\n        crops = tf.image.crop_and_resize(bottom, bboxes, tf.to_int32(batch_ids), [pre_pool_size, pre_pool_size],\n                                         name=""crops"")\n        crops = slim.max_pool2d(crops, [2, 2], padding=\'SAME\')\n      else:\n        crops = tf.image.crop_and_resize(bottom, bboxes, tf.to_int32(batch_ids), [cfg.POOLING_SIZE, cfg.POOLING_SIZE],\n                                         name=""crops"")\n    return crops\n\n  # Do the first few layers manually, because \'SAME\' padding can behave inconsistently\n  # for images of different sizes: sometimes 0, sometimes 1\n  def _build_base(self):\n    with tf.variable_scope(self._scope, self._scope):\n      net = resnet_utils.conv2d_same(self._image, 64, 7, stride=2, scope=\'conv1\')\n      net = tf.pad(net, [[0, 0], [1, 1], [1, 1], [0, 0]])\n      net = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\', scope=\'pool1\')\n\n    return net\n\n  def _image_to_head(self, is_training, reuse=None):\n    assert (0 <= cfg.RESNET.FIXED_BLOCKS <= 3)\n    # Now the base is always fixed during training\n    with slim.arg_scope(resnet_arg_scope(is_training=False)):\n      net_conv = self._build_base()\n    if cfg.RESNET.FIXED_BLOCKS > 0:\n      with slim.arg_scope(resnet_arg_scope(is_training=False)):\n        net_conv, _ = resnet_v1.resnet_v1(net_conv,\n                                           self._blocks[0:cfg.RESNET.FIXED_BLOCKS],\n                                           global_pool=False,\n                                           include_root_block=False,\n                                           reuse=reuse,\n                                           scope=self._scope)\n    if cfg.RESNET.FIXED_BLOCKS < 3:\n      with slim.arg_scope(resnet_arg_scope(is_training=is_training)):\n        net_conv, _ = resnet_v1.resnet_v1(net_conv,\n                                           self._blocks[cfg.RESNET.FIXED_BLOCKS:-1],\n                                           global_pool=False,\n                                           include_root_block=False,\n                                           reuse=reuse,\n                                           scope=self._scope)\n\n    self._act_summaries.append(net_conv)\n    self._layers[\'head\'] = net_conv\n\n    return net_conv\n\n  def _head_to_tail(self, pool5, is_training, reuse=None):\n    with slim.arg_scope(resnet_arg_scope(is_training=is_training)):\n      fc7, _ = resnet_v1.resnet_v1(pool5,\n                                   self._blocks[-1:],\n                                   global_pool=False,\n                                   include_root_block=False,\n                                   reuse=reuse,\n                                   scope=self._scope)\n      # average pooling done by reduce_mean\n      fc7 = tf.reduce_mean(fc7, axis=[1, 2])\n    return fc7\n\n  def _decide_blocks(self):\n    # choose different blocks for different number of layers\n    if self._num_layers == 50:\n      self._blocks = [resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n                      resnet_v1_block(\'block2\', base_depth=128, num_units=4, stride=2),\n                      # use stride 1 for the last conv4 layer\n                      resnet_v1_block(\'block3\', base_depth=256, num_units=6, stride=1),\n                      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1)]\n\n    elif self._num_layers == 101:\n      self._blocks = [resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n                      resnet_v1_block(\'block2\', base_depth=128, num_units=4, stride=2),\n                      # use stride 1 for the last conv4 layer\n                      resnet_v1_block(\'block3\', base_depth=256, num_units=23, stride=1),\n                      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1)]\n\n    elif self._num_layers == 152:\n      self._blocks = [resnet_v1_block(\'block1\', base_depth=64, num_units=3, stride=2),\n                      resnet_v1_block(\'block2\', base_depth=128, num_units=8, stride=2),\n                      # use stride 1 for the last conv4 layer\n                      resnet_v1_block(\'block3\', base_depth=256, num_units=36, stride=1),\n                      resnet_v1_block(\'block4\', base_depth=512, num_units=3, stride=1)]\n\n    else:\n      # other numbers are not supported\n      raise NotImplementedError\n\n  def get_variables_to_restore(self, variables, var_keep_dic):\n    variables_to_restore = []\n\n    for v in variables:\n      # exclude the first conv layer to swap RGB to BGR\n      if v.name == (self._scope + \'/conv1/weights:0\'):\n        self._variables_to_fix[v.name] = v\n        continue\n      if v.name.split(\':\')[0] in var_keep_dic:\n        print(\'Variables restored: %s\' % v.name)\n        variables_to_restore.append(v)\n\n    return variables_to_restore\n\n  def fix_variables(self, sess, pretrained_model):\n    print(\'Fix Resnet V1 layers..\')\n    with tf.variable_scope(\'Fix_Resnet_V1\') as scope:\n      with tf.device(""/cpu:0""):\n        # fix RGB to BGR\n        conv1_rgb = tf.get_variable(""conv1_rgb"", [7, 7, 3, 64], trainable=False)\n        restorer_fc = tf.train.Saver({self._scope + ""/conv1/weights"": conv1_rgb})\n        restorer_fc.restore(sess, pretrained_model)\n\n        sess.run(tf.assign(self._variables_to_fix[self._scope + \'/conv1/weights:0\'], \n                           tf.reverse(conv1_rgb, [2])))\n'"
lib/nets/vgg16.py,12,"b'# --------------------------------------------------------\n# Tensorflow Faster R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Xinlei Chen\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom tensorflow.contrib.slim import losses\nfrom tensorflow.contrib.slim import arg_scope\nimport numpy as np\n\nfrom nets.network import Network\nfrom model.config import cfg\n\nclass vgg16(Network):\n  def __init__(self):\n    Network.__init__(self)\n    self._feat_stride = [16, ]\n    self._feat_compress = [1. / float(self._feat_stride[0]), ]\n    self._scope = \'vgg_16\'\n\n  def _image_to_head(self, is_training, reuse=None):\n    with tf.variable_scope(self._scope, self._scope, reuse=reuse):\n      net = slim.repeat(self._image, 2, slim.conv2d, 64, [3, 3],\n                          trainable=False, scope=\'conv1\')\n      net = slim.max_pool2d(net, [2, 2], padding=\'SAME\', scope=\'pool1\')\n      net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3],\n                        trainable=False, scope=\'conv2\')\n      net = slim.max_pool2d(net, [2, 2], padding=\'SAME\', scope=\'pool2\')\n      net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3],\n                        trainable=is_training, scope=\'conv3\')\n      net = slim.max_pool2d(net, [2, 2], padding=\'SAME\', scope=\'pool3\')\n      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3],\n                        trainable=is_training, scope=\'conv4\')\n      net = slim.max_pool2d(net, [2, 2], padding=\'SAME\', scope=\'pool4\')\n      net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3],\n                        trainable=is_training, scope=\'conv5\')\n\n    self._act_summaries.append(net)\n    self._layers[\'head\'] = net\n    \n    return net\n\n  def _head_to_tail(self, pool5, is_training, reuse=None):\n    with tf.variable_scope(self._scope, self._scope, reuse=reuse):\n      pool5_flat = slim.flatten(pool5, scope=\'flatten\')\n      fc6 = slim.fully_connected(pool5_flat, 4096, scope=\'fc6\')\n      if is_training:\n        fc6 = slim.dropout(fc6, keep_prob=0.5, is_training=True, \n                            scope=\'dropout6\')\n      fc7 = slim.fully_connected(fc6, 4096, scope=\'fc7\')\n      if is_training:\n        fc7 = slim.dropout(fc7, keep_prob=0.5, is_training=True, \n                            scope=\'dropout7\')\n\n    return fc7\n\n  def get_variables_to_restore(self, variables, var_keep_dic):\n    variables_to_restore = []\n\n    for v in variables:\n      # exclude the conv weights that are fc weights in vgg16\n      if v.name == (self._scope + \'/fc6/weights:0\') or \\\n         v.name == (self._scope + \'/fc7/weights:0\'):\n        self._variables_to_fix[v.name] = v\n        continue\n      # exclude the first conv layer to swap RGB to BGR\n      if v.name == (self._scope + \'/conv1/conv1_1/weights:0\'):\n        self._variables_to_fix[v.name] = v\n        continue\n      if v.name.split(\':\')[0] in var_keep_dic:\n        print(\'Variables restored: %s\' % v.name)\n        variables_to_restore.append(v)\n\n    return variables_to_restore\n\n  def fix_variables(self, sess, pretrained_model):\n    print(\'Fix VGG16 layers..\')\n    with tf.variable_scope(\'Fix_VGG16\') as scope:\n      with tf.device(""/cpu:0""):\n        # fix the vgg16 issue from conv weights to fc weights\n        # fix RGB to BGR\n        fc6_conv = tf.get_variable(""fc6_conv"", [7, 7, 512, 4096], trainable=False)\n        fc7_conv = tf.get_variable(""fc7_conv"", [1, 1, 4096, 4096], trainable=False)\n        conv1_rgb = tf.get_variable(""conv1_rgb"", [3, 3, 3, 64], trainable=False)\n        restorer_fc = tf.train.Saver({self._scope + ""/fc6/weights"": fc6_conv, \n                                      self._scope + ""/fc7/weights"": fc7_conv,\n                                      self._scope + ""/conv1/conv1_1/weights"": conv1_rgb})\n        restorer_fc.restore(sess, pretrained_model)\n\n        sess.run(tf.assign(self._variables_to_fix[self._scope + \'/fc6/weights:0\'], tf.reshape(fc6_conv, \n                            self._variables_to_fix[self._scope + \'/fc6/weights:0\'].get_shape())))\n        sess.run(tf.assign(self._variables_to_fix[self._scope + \'/fc7/weights:0\'], tf.reshape(fc7_conv, \n                            self._variables_to_fix[self._scope + \'/fc7/weights:0\'].get_shape())))\n        sess.run(tf.assign(self._variables_to_fix[self._scope + \'/conv1/conv1_1/weights:0\'], \n                            tf.reverse(conv1_rgb, [2])))\n'"
lib/nms/__init__.py,0,b''
lib/nms/py_cpu_nms.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\n\ndef py_cpu_nms(dets, thresh):\n    """"""Pure Python NMS baseline.""""""\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n'"
lib/roi_data_layer/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n'
lib/roi_data_layer/layer.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Xinlei Chen\n# --------------------------------------------------------\n\n""""""The data layer used during training to train a Fast R-CNN network.\n\nRoIDataLayer implements a Caffe Python layer.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom model.config import cfg\nfrom roi_data_layer.minibatch import get_minibatch\nimport numpy as np\nimport time\n\nclass RoIDataLayer(object):\n  """"""Fast R-CNN data layer used for training.""""""\n\n  def __init__(self, roidb, num_classes, random=False):\n    """"""Set the roidb to be used by this layer during training.""""""\n    self._roidb = roidb\n    self._num_classes = num_classes\n    # Also set a random flag\n    self._random = random\n    self._shuffle_roidb_inds()\n\n  def _shuffle_roidb_inds(self):\n    """"""Randomly permute the training roidb.""""""\n    # If the random flag is set, \n    # then the database is shuffled according to system time\n    # Useful for the validation set\n    if self._random:\n      st0 = np.random.get_state()\n      millis = int(round(time.time() * 1000)) % 4294967295\n      np.random.seed(millis)\n    \n    if cfg.TRAIN.ASPECT_GROUPING:\n      widths = np.array([r[\'width\'] for r in self._roidb])\n      heights = np.array([r[\'height\'] for r in self._roidb])\n      horz = (widths >= heights)\n      vert = np.logical_not(horz)\n      horz_inds = np.where(horz)[0]\n      vert_inds = np.where(vert)[0]\n      inds = np.hstack((\n          np.random.permutation(horz_inds),\n          np.random.permutation(vert_inds)))\n      inds = np.reshape(inds, (-1, 2))\n      row_perm = np.random.permutation(np.arange(inds.shape[0]))\n      inds = np.reshape(inds[row_perm, :], (-1,))\n      self._perm = inds\n    else:\n      self._perm = np.random.permutation(np.arange(len(self._roidb)))\n    # Restore the random state\n    if self._random:\n      np.random.set_state(st0)\n      \n    self._cur = 0\n\n  def _get_next_minibatch_inds(self):\n    """"""Return the roidb indices for the next minibatch.""""""\n    \n    if self._cur + cfg.TRAIN.IMS_PER_BATCH >= len(self._roidb):\n      self._shuffle_roidb_inds()\n\n    db_inds = self._perm[self._cur:self._cur + cfg.TRAIN.IMS_PER_BATCH]\n    self._cur += cfg.TRAIN.IMS_PER_BATCH\n\n    return db_inds\n\n  def _get_next_minibatch(self):\n    """"""Return the blobs to be used for the next minibatch.\n\n    If cfg.TRAIN.USE_PREFETCH is True, then blobs will be computed in a\n    separate process and made available through self._blob_queue.\n    """"""\n    db_inds = self._get_next_minibatch_inds()\n    minibatch_db = [self._roidb[i] for i in db_inds]\n    return get_minibatch(minibatch_db, self._num_classes)\n      \n  def forward(self):\n    """"""Get blobs and copy them into this layer\'s top blob vector.""""""\n    blobs = self._get_next_minibatch()\n    return blobs\n'"
lib/roi_data_layer/minibatch.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Xinlei Chen\n# --------------------------------------------------------\n\n""""""Compute minibatch blobs for training a Fast R-CNN network.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport numpy.random as npr\nimport cv2\nfrom model.config import cfg\nfrom utils.blob import prep_im_for_blob, im_list_to_blob\n\ndef get_minibatch(roidb, num_classes):\n  """"""Given a roidb, construct a minibatch sampled from it.""""""\n  num_images = len(roidb)\n  # Sample random scales to use for each image in this batch\n  random_scale_inds = npr.randint(0, high=len(cfg.TRAIN.SCALES),\n                  size=num_images)\n  assert(cfg.TRAIN.BATCH_SIZE % num_images == 0), \\\n    \'num_images ({}) must divide BATCH_SIZE ({})\'. \\\n    format(num_images, cfg.TRAIN.BATCH_SIZE)\n\n  # Get the input image blob, formatted for caffe\n  im_blob, im_scales = _get_image_blob(roidb, random_scale_inds)\n\n  blobs = {\'data\': im_blob}\n\n  assert len(im_scales) == 1, ""Single batch only""\n  assert len(roidb) == 1, ""Single batch only""\n  \n  # gt boxes: (x1, y1, x2, y2, cls)\n  if cfg.TRAIN.USE_ALL_GT:\n    # Include all ground truth boxes\n    gt_inds = np.where(roidb[0][\'gt_classes\'] != 0)[0]\n  else:\n    # For the COCO ground truth boxes, exclude the ones that are \'\'iscrowd\'\' \n    gt_inds = np.where(roidb[0][\'gt_classes\'] != 0 & np.all(roidb[0][\'gt_overlaps\'].toarray() > -1.0, axis=1))[0]\n  gt_boxes = np.empty((len(gt_inds), 5), dtype=np.float32)\n  gt_boxes[:, 0:4] = roidb[0][\'boxes\'][gt_inds, :] * im_scales[0]\n  gt_boxes[:, 4] = roidb[0][\'gt_classes\'][gt_inds]\n  blobs[\'gt_boxes\'] = gt_boxes\n  blobs[\'im_info\'] = np.array(\n    [im_blob.shape[1], im_blob.shape[2], im_scales[0]],\n    dtype=np.float32)\n\n  return blobs\n\ndef _get_image_blob(roidb, scale_inds):\n  """"""Builds an input blob from the images in the roidb at the specified\n  scales.\n  """"""\n  num_images = len(roidb)\n  processed_ims = []\n  im_scales = []\n  for i in range(num_images):\n    im = cv2.imread(roidb[i][\'image\'])\n    if roidb[i][\'flipped\']:\n      im = im[:, ::-1, :]\n    target_size = cfg.TRAIN.SCALES[scale_inds[i]]\n    im, im_scale = prep_im_for_blob(im, cfg.PIXEL_MEANS, target_size,\n                    cfg.TRAIN.MAX_SIZE)\n    im_scales.append(im_scale)\n    processed_ims.append(im)\n\n  # Create a blob to hold the input images\n  blob = im_list_to_blob(processed_ims)\n\n  return blob, im_scales\n'"
lib/roi_data_layer/roidb.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Transform a roidb into a trainable roidb by adding a bunch of metadata.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom model.config import cfg\nfrom model.bbox_transform import bbox_transform\nfrom utils.cython_bbox import bbox_overlaps\nimport PIL\n\ndef prepare_roidb(imdb):\n  """"""Enrich the imdb\'s roidb by adding some derived quantities that\n  are useful for training. This function precomputes the maximum\n  overlap, taken over ground-truth boxes, between each ROI and\n  each ground-truth box. The class with maximum overlap is also\n  recorded.\n  """"""\n  roidb = imdb.roidb\n  if not (imdb.name.startswith(\'coco\')):\n    sizes = [PIL.Image.open(imdb.image_path_at(i)).size\n         for i in range(imdb.num_images)]\n  for i in range(len(imdb.image_index)):\n    roidb[i][\'image\'] = imdb.image_path_at(i)\n    if not (imdb.name.startswith(\'coco\')):\n      roidb[i][\'width\'] = sizes[i][0]\n      roidb[i][\'height\'] = sizes[i][1]\n    # need gt_overlaps as a dense array for argmax\n    gt_overlaps = roidb[i][\'gt_overlaps\'].toarray()\n    # max overlap with gt over classes (columns)\n    max_overlaps = gt_overlaps.max(axis=1)\n    # gt class that had the max overlap\n    max_classes = gt_overlaps.argmax(axis=1)\n    roidb[i][\'max_classes\'] = max_classes\n    roidb[i][\'max_overlaps\'] = max_overlaps\n    # sanity checks\n    # max overlap of 0 => class should be zero (background)\n    zero_inds = np.where(max_overlaps == 0)[0]\n    assert all(max_classes[zero_inds] == 0)\n    # max overlap > 0 => class should not be zero (must be a fg class)\n    nonzero_inds = np.where(max_overlaps > 0)[0]\n    assert all(max_classes[nonzero_inds] != 0)\n'"
lib/utils/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n'
lib/utils/blob.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Blob helper functions.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport cv2\n\n\ndef im_list_to_blob(ims):\n  """"""Convert a list of images into a network input.\n\n  Assumes images are already prepared (means subtracted, BGR order, ...).\n  """"""\n  max_shape = np.array([im.shape for im in ims]).max(axis=0)\n  num_images = len(ims)\n  blob = np.zeros((num_images, max_shape[0], max_shape[1], 3),\n                  dtype=np.float32)\n  for i in range(num_images):\n    im = ims[i]\n    blob[i, 0:im.shape[0], 0:im.shape[1], :] = im\n\n  return blob\n\n\ndef prep_im_for_blob(im, pixel_means, target_size, max_size):\n  """"""Mean subtract and scale an image for use in a blob.""""""\n  im = im.astype(np.float32, copy=False)\n  im -= pixel_means\n  im_shape = im.shape\n  im_size_min = np.min(im_shape[0:2])\n  im_size_max = np.max(im_shape[0:2])\n  im_scale = float(target_size) / float(im_size_min)\n  # Prevent the biggest axis from being more than MAX_SIZE\n  if np.round(im_scale * im_size_max) > max_size:\n    im_scale = float(max_size) / float(im_size_max)\n  im = cv2.resize(im, None, None, fx=im_scale, fy=im_scale,\n                  interpolation=cv2.INTER_LINEAR)\n\n  return im, im_scale\n'"
lib/utils/timer.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport time\n\nclass Timer(object):\n    """"""A simple timer.""""""\n    def __init__(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n\n    def tic(self):\n        # using time.time instead of time.clock because time time.clock\n        # does not normalize for multithreading\n        self.start_time = time.time()\n\n    def toc(self, average=True):\n        self.diff = time.time() - self.start_time\n        self.total_time += self.diff\n        self.calls += 1\n        self.average_time = self.total_time / self.calls\n        if average:\n            return self.average_time\n        else:\n            return self.diff\n'"
lib/utils/visualization.py,0,"b""# --------------------------------------------------------\n# Tensorflow Faster R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Xinlei Chen\n# --------------------------------------------------------\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom six.moves import range\nimport PIL.Image as Image\nimport PIL.ImageColor as ImageColor\nimport PIL.ImageDraw as ImageDraw\nimport PIL.ImageFont as ImageFont\n\nSTANDARD_COLORS = [\n    'AliceBlue', 'Chartreuse', 'Aqua', 'Aquamarine', 'Azure', 'Beige', 'Bisque',\n    'BlanchedAlmond', 'BlueViolet', 'BurlyWood', 'CadetBlue', 'AntiqueWhite',\n    'Chocolate', 'Coral', 'CornflowerBlue', 'Cornsilk', 'Crimson', 'Cyan',\n    'DarkCyan', 'DarkGoldenRod', 'DarkGrey', 'DarkKhaki', 'DarkOrange',\n    'DarkOrchid', 'DarkSalmon', 'DarkSeaGreen', 'DarkTurquoise', 'DarkViolet',\n    'DeepPink', 'DeepSkyBlue', 'DodgerBlue', 'FireBrick', 'FloralWhite',\n    'ForestGreen', 'Fuchsia', 'Gainsboro', 'GhostWhite', 'Gold', 'GoldenRod',\n    'Salmon', 'Tan', 'HoneyDew', 'HotPink', 'IndianRed', 'Ivory', 'Khaki',\n    'Lavender', 'LavenderBlush', 'LawnGreen', 'LemonChiffon', 'LightBlue',\n    'LightCoral', 'LightCyan', 'LightGoldenRodYellow', 'LightGray', 'LightGrey',\n    'LightGreen', 'LightPink', 'LightSalmon', 'LightSeaGreen', 'LightSkyBlue',\n    'LightSlateGray', 'LightSlateGrey', 'LightSteelBlue', 'LightYellow', 'Lime',\n    'LimeGreen', 'Linen', 'Magenta', 'MediumAquaMarine', 'MediumOrchid',\n    'MediumPurple', 'MediumSeaGreen', 'MediumSlateBlue', 'MediumSpringGreen',\n    'MediumTurquoise', 'MediumVioletRed', 'MintCream', 'MistyRose', 'Moccasin',\n    'NavajoWhite', 'OldLace', 'Olive', 'OliveDrab', 'Orange', 'OrangeRed',\n    'Orchid', 'PaleGoldenRod', 'PaleGreen', 'PaleTurquoise', 'PaleVioletRed',\n    'PapayaWhip', 'PeachPuff', 'Peru', 'Pink', 'Plum', 'PowderBlue', 'Purple',\n    'Red', 'RosyBrown', 'RoyalBlue', 'SaddleBrown', 'Green', 'SandyBrown',\n    'SeaGreen', 'SeaShell', 'Sienna', 'Silver', 'SkyBlue', 'SlateBlue',\n    'SlateGray', 'SlateGrey', 'Snow', 'SpringGreen', 'SteelBlue', 'GreenYellow',\n    'Teal', 'Thistle', 'Tomato', 'Turquoise', 'Violet', 'Wheat', 'White',\n    'WhiteSmoke', 'Yellow', 'YellowGreen'\n]\n\nNUM_COLORS = len(STANDARD_COLORS)\n\ntry:\n  FONT = ImageFont.truetype('arial.ttf', 24)\nexcept IOError:\n  FONT = ImageFont.load_default()\n\ndef _draw_single_box(image, xmin, ymin, xmax, ymax, display_str, font, color='black', thickness=4):\n  draw = ImageDraw.Draw(image)\n  (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n  draw.line([(left, top), (left, bottom), (right, bottom),\n             (right, top), (left, top)], width=thickness, fill=color)\n  text_bottom = bottom\n  # Reverse list and print from bottom to top.\n  text_width, text_height = font.getsize(display_str)\n  margin = np.ceil(0.05 * text_height)\n  draw.rectangle(\n      [(left, text_bottom - text_height - 2 * margin), (left + text_width,\n                                                        text_bottom)],\n      fill=color)\n  draw.text(\n      (left + margin, text_bottom - text_height - margin),\n      display_str,\n      fill='black',\n      font=font)\n\n  return image\n\ndef draw_bounding_boxes(image, gt_boxes, im_info):\n  num_boxes = gt_boxes.shape[0]\n  gt_boxes_new = gt_boxes.copy()\n  gt_boxes_new[:,:4] = np.round(gt_boxes_new[:,:4].copy() / im_info[2])\n  disp_image = Image.fromarray(np.uint8(image[0]))\n\n  for i in range(num_boxes):\n    this_class = int(gt_boxes_new[i, 4])\n    disp_image = _draw_single_box(disp_image, \n                                gt_boxes_new[i, 0],\n                                gt_boxes_new[i, 1],\n                                gt_boxes_new[i, 2],\n                                gt_boxes_new[i, 3],\n                                'N%02d-C%02d' % (i, this_class),\n                                FONT,\n                                color=STANDARD_COLORS[this_class % NUM_COLORS])\n\n  image[0, :] = np.array(disp_image)\n  return image\n"""
lib/datasets/tools/mcg_munge.py,0,"b'import os\nimport sys\n\n""""""Hacky tool to convert file system layout of MCG boxes downloaded from\nhttp://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/\nso that it\'s consistent with those computed by Jan Hosang (see:\nhttp://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-\n  computing/research/object-recognition-and-scene-understanding/how-\n  good-are-detection-proposals-really/)\n\nNB: Boxes from the MCG website are in (y1, x1, y2, x2) order.\nBoxes from Hosang et al. are in (x1, y1, x2, y2) order.\n""""""\n\ndef munge(src_dir):\n    # stored as: ./MCG-COCO-val2014-boxes/COCO_val2014_000000193401.mat\n    # want:      ./MCG/mat/COCO_val2014_0/COCO_val2014_000000141/COCO_val2014_000000141334.mat\n\n    files = os.listdir(src_dir)\n    for fn in files:\n        base, ext = os.path.splitext(fn)\n        # first 14 chars / first 22 chars / all chars + .mat\n        # COCO_val2014_0/COCO_val2014_000000447/COCO_val2014_000000447991.mat\n        first = base[:14]\n        second = base[:22]\n        dst_dir = os.path.join(\'MCG\', \'mat\', first, second)\n        if not os.path.exists(dst_dir):\n            os.makedirs(dst_dir)\n        src = os.path.join(src_dir, fn)\n        dst = os.path.join(dst_dir, fn)\n        print \'MV: {} -> {}\'.format(src, dst)\n        os.rename(src, dst)\n\nif __name__ == \'__main__\':\n    # src_dir should look something like:\n    #  src_dir = \'MCG-COCO-val2014-boxes\'\n    src_dir = sys.argv[1]\n    munge(src_dir)\n'"
