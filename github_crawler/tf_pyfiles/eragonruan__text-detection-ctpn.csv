file_path,api_count,code
main/demo.py,14,"b'# coding=utf-8\nimport os\nimport shutil\nimport sys\nimport time\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\n\nsys.path.append(os.getcwd())\nfrom nets import model_train as model\nfrom utils.rpn_msr.proposal_layer import proposal_layer\nfrom utils.text_connector.detectors import TextDetector\n\ntf.app.flags.DEFINE_string(\'test_data_path\', \'data/demo/\', \'\')\ntf.app.flags.DEFINE_string(\'output_path\', \'data/res/\', \'\')\ntf.app.flags.DEFINE_string(\'gpu\', \'0\', \'\')\ntf.app.flags.DEFINE_string(\'checkpoint_path\', \'checkpoints_mlt/\', \'\')\nFLAGS = tf.app.flags.FLAGS\n\n\ndef get_images():\n    files = []\n    exts = [\'jpg\', \'png\', \'jpeg\', \'JPG\']\n    for parent, dirnames, filenames in os.walk(FLAGS.test_data_path):\n        for filename in filenames:\n            for ext in exts:\n                if filename.endswith(ext):\n                    files.append(os.path.join(parent, filename))\n                    break\n    print(\'Find {} images\'.format(len(files)))\n    return files\n\n\ndef resize_image(img):\n    img_size = img.shape\n    im_size_min = np.min(img_size[0:2])\n    im_size_max = np.max(img_size[0:2])\n\n    im_scale = float(600) / float(im_size_min)\n    if np.round(im_scale * im_size_max) > 1200:\n        im_scale = float(1200) / float(im_size_max)\n    new_h = int(img_size[0] * im_scale)\n    new_w = int(img_size[1] * im_scale)\n\n    new_h = new_h if new_h // 16 == 0 else (new_h // 16 + 1) * 16\n    new_w = new_w if new_w // 16 == 0 else (new_w // 16 + 1) * 16\n\n    re_im = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n    return re_im, (new_h / img_size[0], new_w / img_size[1])\n\n\ndef main(argv=None):\n    if os.path.exists(FLAGS.output_path):\n        shutil.rmtree(FLAGS.output_path)\n    os.makedirs(FLAGS.output_path)\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = FLAGS.gpu\n\n    with tf.get_default_graph().as_default():\n        input_image = tf.placeholder(tf.float32, shape=[None, None, None, 3], name=\'input_image\')\n        input_im_info = tf.placeholder(tf.float32, shape=[None, 3], name=\'input_im_info\')\n\n        global_step = tf.get_variable(\'global_step\', [], initializer=tf.constant_initializer(0), trainable=False)\n\n        bbox_pred, cls_pred, cls_prob = model.model(input_image)\n\n        variable_averages = tf.train.ExponentialMovingAverage(0.997, global_step)\n        saver = tf.train.Saver(variable_averages.variables_to_restore())\n\n        with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n            ckpt_state = tf.train.get_checkpoint_state(FLAGS.checkpoint_path)\n            model_path = os.path.join(FLAGS.checkpoint_path, os.path.basename(ckpt_state.model_checkpoint_path))\n            print(\'Restore from {}\'.format(model_path))\n            saver.restore(sess, model_path)\n\n            im_fn_list = get_images()\n            for im_fn in im_fn_list:\n                print(\'===============\')\n                print(im_fn)\n                start = time.time()\n                try:\n                    im = cv2.imread(im_fn)[:, :, ::-1]\n                except:\n                    print(""Error reading image {}!"".format(im_fn))\n                    continue\n\n                img, (rh, rw) = resize_image(im)\n                h, w, c = img.shape\n                im_info = np.array([h, w, c]).reshape([1, 3])\n                bbox_pred_val, cls_prob_val = sess.run([bbox_pred, cls_prob],\n                                                       feed_dict={input_image: [img],\n                                                                  input_im_info: im_info})\n\n                textsegs, _ = proposal_layer(cls_prob_val, bbox_pred_val, im_info)\n                scores = textsegs[:, 0]\n                textsegs = textsegs[:, 1:5]\n\n                textdetector = TextDetector(DETECT_MODE=\'H\')\n                boxes = textdetector.detect(textsegs, scores[:, np.newaxis], img.shape[:2])\n                boxes = np.array(boxes, dtype=np.int)\n\n                cost_time = (time.time() - start)\n                print(""cost time: {:.2f}s"".format(cost_time))\n\n                for i, box in enumerate(boxes):\n                    cv2.polylines(img, [box[:8].astype(np.int32).reshape((-1, 1, 2))], True, color=(0, 255, 0),\n                                  thickness=2)\n                img = cv2.resize(img, None, None, fx=1.0 / rh, fy=1.0 / rw, interpolation=cv2.INTER_LINEAR)\n                cv2.imwrite(os.path.join(FLAGS.output_path, os.path.basename(im_fn)), img[:, :, ::-1])\n\n                with open(os.path.join(FLAGS.output_path, os.path.splitext(os.path.basename(im_fn))[0]) + "".txt"",\n                          ""w"") as f:\n                    for i, box in enumerate(boxes):\n                        line = "","".join(str(box[k]) for k in range(8))\n                        line += "","" + str(scores[i]) + ""\\r\\n""\n                        f.writelines(line)\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
main/train.py,36,"b'import datetime\nimport os\nimport sys\nimport time\n\nimport tensorflow as tf\n\nsys.path.append(os.getcwd())\nfrom tensorflow.contrib import slim\nfrom nets import model_train as model\nfrom utils.dataset import data_provider as data_provider\n\ntf.app.flags.DEFINE_float(\'learning_rate\', 1e-5, \'\')\ntf.app.flags.DEFINE_integer(\'max_steps\', 50000, \'\')\ntf.app.flags.DEFINE_integer(\'decay_steps\', 30000, \'\')\ntf.app.flags.DEFINE_integer(\'decay_rate\', 0.1, \'\')\ntf.app.flags.DEFINE_float(\'moving_average_decay\', 0.997, \'\')\ntf.app.flags.DEFINE_integer(\'num_readers\', 4, \'\')\ntf.app.flags.DEFINE_string(\'gpu\', \'0\', \'\')\ntf.app.flags.DEFINE_string(\'checkpoint_path\', \'checkpoints_mlt/\', \'\')\ntf.app.flags.DEFINE_string(\'logs_path\', \'logs_mlt/\', \'\')\ntf.app.flags.DEFINE_string(\'pretrained_model_path\', \'data/vgg_16.ckpt\', \'\')\ntf.app.flags.DEFINE_boolean(\'restore\', True, \'\')\ntf.app.flags.DEFINE_integer(\'save_checkpoint_steps\', 2000, \'\')\nFLAGS = tf.app.flags.FLAGS\n\n\ndef main(argv=None):\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = FLAGS.gpu\n    now = datetime.datetime.now()\n    StyleTime = now.strftime(""%Y-%m-%d-%H-%M-%S"")\n    os.makedirs(FLAGS.logs_path + StyleTime)\n    if not os.path.exists(FLAGS.checkpoint_path):\n        os.makedirs(FLAGS.checkpoint_path)\n\n    input_image = tf.placeholder(tf.float32, shape=[None, None, None, 3], name=\'input_image\')\n    input_bbox = tf.placeholder(tf.float32, shape=[None, 5], name=\'input_bbox\')\n    input_im_info = tf.placeholder(tf.float32, shape=[None, 3], name=\'input_im_info\')\n\n    global_step = tf.get_variable(\'global_step\', [], initializer=tf.constant_initializer(0), trainable=False)\n    learning_rate = tf.Variable(FLAGS.learning_rate, trainable=False)\n    tf.summary.scalar(\'learning_rate\', learning_rate)\n    opt = tf.train.AdamOptimizer(learning_rate)\n\n    gpu_id = int(FLAGS.gpu)\n    with tf.device(\'/gpu:%d\' % gpu_id):\n        with tf.name_scope(\'model_%d\' % gpu_id) as scope:\n            bbox_pred, cls_pred, cls_prob = model.model(input_image)\n            total_loss, model_loss, rpn_cross_entropy, rpn_loss_box = model.loss(bbox_pred, cls_pred, input_bbox,\n                                                                                 input_im_info)\n            batch_norm_updates_op = tf.group(*tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope))\n            grads = opt.compute_gradients(total_loss)\n\n    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n\n    summary_op = tf.summary.merge_all()\n    variable_averages = tf.train.ExponentialMovingAverage(\n        FLAGS.moving_average_decay, global_step)\n    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n    with tf.control_dependencies([variables_averages_op, apply_gradient_op, batch_norm_updates_op]):\n        train_op = tf.no_op(name=\'train_op\')\n\n    saver = tf.train.Saver(tf.global_variables(), max_to_keep=100)\n    summary_writer = tf.summary.FileWriter(FLAGS.logs_path + StyleTime, tf.get_default_graph())\n\n    init = tf.global_variables_initializer()\n\n    if FLAGS.pretrained_model_path is not None:\n        variable_restore_op = slim.assign_from_checkpoint_fn(FLAGS.pretrained_model_path,\n                                                             slim.get_trainable_variables(),\n                                                             ignore_missing_vars=True)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    config.gpu_options.per_process_gpu_memory_fraction = 0.95\n    config.allow_soft_placement = True\n    with tf.Session(config=config) as sess:\n        if FLAGS.restore:\n            ckpt = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n            restore_step = int(ckpt.split(\'.\')[0].split(\'_\')[-1])\n            print(""continue training from previous checkpoint {}"".format(restore_step))\n            saver.restore(sess, ckpt)\n        else:\n            sess.run(init)\n            restore_step = 0\n            if FLAGS.pretrained_model_path is not None:\n                variable_restore_op(sess)\n\n        data_generator = data_provider.get_batch(num_workers=FLAGS.num_readers)\n        start = time.time()\n        for step in range(restore_step, FLAGS.max_steps):\n            data = next(data_generator)\n            ml, tl, _, summary_str = sess.run([model_loss, total_loss, train_op, summary_op],\n                                              feed_dict={input_image: data[0],\n                                                         input_bbox: data[1],\n                                                         input_im_info: data[2]})\n\n            summary_writer.add_summary(summary_str, global_step=step)\n\n            if step != 0 and step % FLAGS.decay_steps == 0:\n                sess.run(tf.assign(learning_rate, learning_rate.eval() * FLAGS.decay_rate))\n\n            if step % 10 == 0:\n                avg_time_per_step = (time.time() - start) / 10\n                start = time.time()\n                print(\'Step {:06d}, model loss {:.4f}, total loss {:.4f}, {:.2f} seconds/step, LR: {:.6f}\'.format(\n                    step, ml, tl, avg_time_per_step, learning_rate.eval()))\n\n            if (step + 1) % FLAGS.save_checkpoint_steps == 0:\n                filename = (\'ctpn_{:d}\'.format(step + 1) + \'.ckpt\')\n                filename = os.path.join(FLAGS.checkpoint_path, filename)\n                saver.save(sess, filename)\n                print(\'Write model to: {:s}\'.format(filename))\n\n\nif __name__ == \'__main__\':\n    tf.app.run()\n'"
nets/model_train.py,60,"b'import tensorflow as tf\nfrom tensorflow.contrib import slim\n\nfrom nets import vgg\nfrom utils.rpn_msr.anchor_target_layer import anchor_target_layer as anchor_target_layer_py\n\n\ndef mean_image_subtraction(images, means=[123.68, 116.78, 103.94]):\n    num_channels = images.get_shape().as_list()[-1]\n    if len(means) != num_channels:\n        raise ValueError(\'len(means) must match the number of channels\')\n    channels = tf.split(axis=3, num_or_size_splits=num_channels, value=images)\n    for i in range(num_channels):\n        channels[i] -= means[i]\n    return tf.concat(axis=3, values=channels)\n\n\ndef make_var(name, shape, initializer=None):\n    return tf.get_variable(name, shape, initializer=initializer)\n\n\ndef Bilstm(net, input_channel, hidden_unit_num, output_channel, scope_name):\n    # width--->time step\n    with tf.variable_scope(scope_name) as scope:\n        shape = tf.shape(net)\n        N, H, W, C = shape[0], shape[1], shape[2], shape[3]\n        net = tf.reshape(net, [N * H, W, C])\n        net.set_shape([None, None, input_channel])\n\n        lstm_fw_cell = tf.contrib.rnn.LSTMCell(hidden_unit_num, state_is_tuple=True)\n        lstm_bw_cell = tf.contrib.rnn.LSTMCell(hidden_unit_num, state_is_tuple=True)\n\n        lstm_out, last_state = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, net, dtype=tf.float32)\n        lstm_out = tf.concat(lstm_out, axis=-1)\n\n        lstm_out = tf.reshape(lstm_out, [N * H * W, 2 * hidden_unit_num])\n\n        init_weights = tf.contrib.layers.variance_scaling_initializer(factor=0.01, mode=\'FAN_AVG\', uniform=False)\n        init_biases = tf.constant_initializer(0.0)\n        weights = make_var(\'weights\', [2 * hidden_unit_num, output_channel], init_weights)\n        biases = make_var(\'biases\', [output_channel], init_biases)\n\n        outputs = tf.matmul(lstm_out, weights) + biases\n\n        outputs = tf.reshape(outputs, [N, H, W, output_channel])\n        return outputs\n\n\ndef lstm_fc(net, input_channel, output_channel, scope_name):\n    with tf.variable_scope(scope_name) as scope:\n        shape = tf.shape(net)\n        N, H, W, C = shape[0], shape[1], shape[2], shape[3]\n        net = tf.reshape(net, [N * H * W, C])\n\n        init_weights = tf.contrib.layers.variance_scaling_initializer(factor=0.01, mode=\'FAN_AVG\', uniform=False)\n        init_biases = tf.constant_initializer(0.0)\n        weights = make_var(\'weights\', [input_channel, output_channel], init_weights)\n        biases = make_var(\'biases\', [output_channel], init_biases)\n\n        output = tf.matmul(net, weights) + biases\n        output = tf.reshape(output, [N, H, W, output_channel])\n    return output\n\n\ndef model(image):\n    image = mean_image_subtraction(image)\n    with slim.arg_scope(vgg.vgg_arg_scope()):\n        conv5_3 = vgg.vgg_16(image)\n\n    rpn_conv = slim.conv2d(conv5_3, 512, 3)\n\n    lstm_output = Bilstm(rpn_conv, 512, 128, 512, scope_name=\'BiLSTM\')\n\n    bbox_pred = lstm_fc(lstm_output, 512, 10 * 4, scope_name=""bbox_pred"")\n    cls_pred = lstm_fc(lstm_output, 512, 10 * 2, scope_name=""cls_pred"")\n\n    # transpose: (1, H, W, A x d) -> (1, H, WxA, d)\n    cls_pred_shape = tf.shape(cls_pred)\n    cls_pred_reshape = tf.reshape(cls_pred, [cls_pred_shape[0], cls_pred_shape[1], -1, 2])\n\n    cls_pred_reshape_shape = tf.shape(cls_pred_reshape)\n    cls_prob = tf.reshape(tf.nn.softmax(tf.reshape(cls_pred_reshape, [-1, cls_pred_reshape_shape[3]])),\n                          [-1, cls_pred_reshape_shape[1], cls_pred_reshape_shape[2], cls_pred_reshape_shape[3]],\n                          name=""cls_prob"")\n\n    return bbox_pred, cls_pred, cls_prob\n\n\ndef anchor_target_layer(cls_pred, bbox, im_info, scope_name):\n    with tf.variable_scope(scope_name) as scope:\n        # \'rpn_cls_score\', \'gt_boxes\', \'im_info\'\n        rpn_labels, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights = \\\n            tf.py_func(anchor_target_layer_py,\n                       [cls_pred, bbox, im_info, [16, ], [16]],\n                       [tf.float32, tf.float32, tf.float32, tf.float32])\n\n        rpn_labels = tf.convert_to_tensor(tf.cast(rpn_labels, tf.int32),\n                                          name=\'rpn_labels\')\n        rpn_bbox_targets = tf.convert_to_tensor(rpn_bbox_targets,\n                                                name=\'rpn_bbox_targets\')\n        rpn_bbox_inside_weights = tf.convert_to_tensor(rpn_bbox_inside_weights,\n                                                       name=\'rpn_bbox_inside_weights\')\n        rpn_bbox_outside_weights = tf.convert_to_tensor(rpn_bbox_outside_weights,\n                                                        name=\'rpn_bbox_outside_weights\')\n\n        return [rpn_labels, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights]\n\n\ndef smooth_l1_dist(deltas, sigma2=9.0, name=\'smooth_l1_dist\'):\n    with tf.name_scope(name=name) as scope:\n        deltas_abs = tf.abs(deltas)\n        smoothL1_sign = tf.cast(tf.less(deltas_abs, 1.0 / sigma2), tf.float32)\n        return tf.square(deltas) * 0.5 * sigma2 * smoothL1_sign + \\\n               (deltas_abs - 0.5 / sigma2) * tf.abs(smoothL1_sign - 1)\n\n\ndef loss(bbox_pred, cls_pred, bbox, im_info):\n    rpn_data = anchor_target_layer(cls_pred, bbox, im_info, ""anchor_target_layer"")\n\n    # classification loss\n    # transpose: (1, H, W, A x d) -> (1, H, WxA, d)\n    cls_pred_shape = tf.shape(cls_pred)\n    cls_pred_reshape = tf.reshape(cls_pred, [cls_pred_shape[0], cls_pred_shape[1], -1, 2])\n    rpn_cls_score = tf.reshape(cls_pred_reshape, [-1, 2])\n    rpn_label = tf.reshape(rpn_data[0], [-1])\n    # ignore_label(-1)\n    fg_keep = tf.equal(rpn_label, 1)\n    rpn_keep = tf.where(tf.not_equal(rpn_label, -1))\n    rpn_cls_score = tf.gather(rpn_cls_score, rpn_keep)\n    rpn_label = tf.gather(rpn_label, rpn_keep)\n    rpn_cross_entropy_n = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=rpn_label, logits=rpn_cls_score)\n\n    # box loss\n    rpn_bbox_pred = bbox_pred\n    rpn_bbox_targets = rpn_data[1]\n    rpn_bbox_inside_weights = rpn_data[2]\n    rpn_bbox_outside_weights = rpn_data[3]\n\n    rpn_bbox_pred = tf.gather(tf.reshape(rpn_bbox_pred, [-1, 4]), rpn_keep)  # shape (N, 4)\n    rpn_bbox_targets = tf.gather(tf.reshape(rpn_bbox_targets, [-1, 4]), rpn_keep)\n    rpn_bbox_inside_weights = tf.gather(tf.reshape(rpn_bbox_inside_weights, [-1, 4]), rpn_keep)\n    rpn_bbox_outside_weights = tf.gather(tf.reshape(rpn_bbox_outside_weights, [-1, 4]), rpn_keep)\n\n    rpn_loss_box_n = tf.reduce_sum(rpn_bbox_outside_weights * smooth_l1_dist(\n        rpn_bbox_inside_weights * (rpn_bbox_pred - rpn_bbox_targets)), reduction_indices=[1])\n\n    rpn_loss_box = tf.reduce_sum(rpn_loss_box_n) / (tf.reduce_sum(tf.cast(fg_keep, tf.float32)) + 1)\n    rpn_cross_entropy = tf.reduce_mean(rpn_cross_entropy_n)\n\n    model_loss = rpn_cross_entropy + rpn_loss_box\n\n    regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n    total_loss = tf.add_n(regularization_losses) + model_loss\n\n    tf.summary.scalar(\'model_loss\', model_loss)\n    tf.summary.scalar(\'total_loss\', total_loss)\n    tf.summary.scalar(\'rpn_cross_entropy\', rpn_cross_entropy)\n    tf.summary.scalar(\'rpn_loss_box\', rpn_loss_box)\n\n    return total_loss, model_loss, rpn_cross_entropy, rpn_loss_box\n'"
nets/vgg.py,4,"b""import tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef vgg_arg_scope(weight_decay=0.0005):\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                        activation_fn=tf.nn.relu,\n                        weights_regularizer=slim.l2_regularizer(weight_decay),\n                        biases_initializer=tf.zeros_initializer()):\n        with slim.arg_scope([slim.conv2d], padding='SAME') as arg_sc:\n            return arg_sc\n\n\ndef vgg_16(inputs, scope='vgg_16'):\n    with tf.variable_scope(scope, 'vgg_16', [inputs]) as sc:\n        with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d]):\n            net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n            net = slim.max_pool2d(net, [2, 2], scope='pool1')\n            net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n            net = slim.max_pool2d(net, [2, 2], scope='pool2')\n            net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')\n            net = slim.max_pool2d(net, [2, 2], scope='pool3')\n            net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')\n            net = slim.max_pool2d(net, [2, 2], scope='pool4')\n            net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')\n\n    return net\n"""
utils/bbox/bbox_transform.py,0,"b'import numpy as np\n\n\ndef bbox_transform(ex_rois, gt_rois):\n    """"""\n    computes the distance from ground-truth boxes to the given boxes, normed by their size\n    :param ex_rois: n * 4 numpy array, given boxes\n    :param gt_rois: n * 4 numpy array, ground-truth boxes\n    :return: deltas: n * 4 numpy array, ground-truth boxes\n    """"""\n    ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0\n    ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0\n    ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\n    ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n\n    assert np.min(ex_widths) > 0.1 and np.min(ex_heights) > 0.1, \\\n        \'Invalid boxes found: {} {}\'.format(ex_rois[np.argmin(ex_widths), :], ex_rois[np.argmin(ex_heights), :])\n\n    gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + 1.0\n    gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + 1.0\n    gt_ctr_x = gt_rois[:, 0] + 0.5 * gt_widths\n    gt_ctr_y = gt_rois[:, 1] + 0.5 * gt_heights\n\n    # warnings.catch_warnings()\n    # warnings.filterwarnings(\'error\')\n    targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n    targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n    targets_dw = np.log(gt_widths / ex_widths)\n    targets_dh = np.log(gt_heights / ex_heights)\n\n    targets = np.vstack(\n        (targets_dx, targets_dy, targets_dw, targets_dh)).transpose()\n\n    return targets\n\n\ndef bbox_transform_inv(boxes, deltas):\n    boxes = boxes.astype(deltas.dtype, copy=False)\n\n    widths = boxes[:, 2] - boxes[:, 0] + 1.0\n    heights = boxes[:, 3] - boxes[:, 1] + 1.0\n    ctr_x = boxes[:, 0] + 0.5 * widths\n    ctr_y = boxes[:, 1] + 0.5 * heights\n\n    dx = deltas[:, 0::4]\n    dy = deltas[:, 1::4]\n    dw = deltas[:, 2::4]\n    dh = deltas[:, 3::4]\n\n    pred_ctr_x = ctr_x[:, np.newaxis]\n    pred_ctr_y = dy * heights[:, np.newaxis] + ctr_y[:, np.newaxis]\n    pred_w = widths[:, np.newaxis]\n    pred_h = np.exp(dh) * heights[:, np.newaxis]\n\n    pred_boxes = np.zeros(deltas.shape, dtype=deltas.dtype)\n    # x1\n    pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n    # y1\n    pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n    # x2\n    pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n    # y2\n    pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n    return pred_boxes\n\n\ndef clip_boxes(boxes, im_shape):\n    """"""\n    Clip boxes to image boundaries.\n    """"""\n    # x1 >= 0\n    boxes[:, 0::4] = np.maximum(np.minimum(boxes[:, 0::4], im_shape[1] - 1), 0)\n    # y1 >= 0\n    boxes[:, 1::4] = np.maximum(np.minimum(boxes[:, 1::4], im_shape[0] - 1), 0)\n    # x2 < im_shape[1]\n    boxes[:, 2::4] = np.maximum(np.minimum(boxes[:, 2::4], im_shape[1] - 1), 0)\n    # y2 < im_shape[0]\n    boxes[:, 3::4] = np.maximum(np.minimum(boxes[:, 3::4], im_shape[0] - 1), 0)\n    return boxes\n'"
utils/bbox/setup.py,0,"b'from distutils.core import setup\n\nimport numpy as np\nfrom Cython.Build import cythonize\n\nnumpy_include = np.get_include()\nsetup(ext_modules=cythonize(""bbox.pyx""), include_dirs=[numpy_include])\nsetup(ext_modules=cythonize(""nms.pyx""), include_dirs=[numpy_include])\n'"
utils/dataset/data_provider.py,0,"b'# encoding:utf-8\nimport os\nimport time\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom utils.dataset.data_util import GeneratorEnqueuer\n\nDATA_FOLDER = ""data/dataset/mlt/""\n\n\ndef get_training_data():\n    img_files = []\n    exts = [\'jpg\', \'png\', \'jpeg\', \'JPG\']\n    for parent, dirnames, filenames in os.walk(os.path.join(DATA_FOLDER, ""image"")):\n        for filename in filenames:\n            for ext in exts:\n                if filename.endswith(ext):\n                    img_files.append(os.path.join(parent, filename))\n                    break\n    print(\'Find {} images\'.format(len(img_files)))\n    return img_files\n\n\ndef load_annoataion(p):\n    bbox = []\n    with open(p, ""r"") as f:\n        lines = f.readlines()\n    for line in lines:\n        line = line.strip().split("","")\n        x_min, y_min, x_max, y_max = map(int, line)\n        bbox.append([x_min, y_min, x_max, y_max, 1])\n    return bbox\n\n\ndef generator(vis=False):\n    image_list = np.array(get_training_data())\n    print(\'{} training images in {}\'.format(image_list.shape[0], DATA_FOLDER))\n    index = np.arange(0, image_list.shape[0])\n    while True:\n        np.random.shuffle(index)\n        for i in index:\n            try:\n                im_fn = image_list[i]\n                im = cv2.imread(im_fn)\n                h, w, c = im.shape\n                im_info = np.array([h, w, c]).reshape([1, 3])\n\n                _, fn = os.path.split(im_fn)\n                fn, _ = os.path.splitext(fn)\n                txt_fn = os.path.join(DATA_FOLDER, ""label"", fn + \'.txt\')\n                if not os.path.exists(txt_fn):\n                    print(""Ground truth for image {} not exist!"".format(im_fn))\n                    continue\n                bbox = load_annoataion(txt_fn)\n                if len(bbox) == 0:\n                    print(""Ground truth for image {} empty!"".format(im_fn))\n                    continue\n\n                if vis:\n                    for p in bbox:\n                        cv2.rectangle(im, (p[0], p[1]), (p[2], p[3]), color=(0, 0, 255), thickness=1)\n                    fig, axs = plt.subplots(1, 1, figsize=(30, 30))\n                    axs.imshow(im[:, :, ::-1])\n                    axs.set_xticks([])\n                    axs.set_yticks([])\n                    plt.tight_layout()\n                    plt.show()\n                    plt.close()\n                yield [im], bbox, im_info\n\n            except Exception as e:\n                print(e)\n                continue\n\n\ndef get_batch(num_workers, **kwargs):\n    try:\n        enqueuer = GeneratorEnqueuer(generator(**kwargs), use_multiprocessing=True)\n        enqueuer.start(max_queue_size=24, workers=num_workers)\n        generator_output = None\n        while True:\n            while enqueuer.is_running():\n                if not enqueuer.queue.empty():\n                    generator_output = enqueuer.queue.get()\n                    break\n                else:\n                    time.sleep(0.01)\n            yield generator_output\n            generator_output = None\n    finally:\n        if enqueuer is not None:\n            enqueuer.stop()\n\n\nif __name__ == \'__main__\':\n    gen = get_batch(num_workers=2, vis=True)\n    while True:\n        image, bbox, im_info = next(gen)\n        print(\'done\')\n'"
utils/dataset/data_util.py,0,"b'import multiprocessing\nimport threading\nimport time\n\nimport numpy as np\n\ntry:\n    import queue\nexcept ImportError:\n    import Queue as queue\n\n\nclass GeneratorEnqueuer():\n    def __init__(self, generator,\n                 use_multiprocessing=False,\n                 wait_time=0.05,\n                 random_seed=None):\n        self.wait_time = wait_time\n        self._generator = generator\n        self._use_multiprocessing = use_multiprocessing\n        self._threads = []\n        self._stop_event = None\n        self.queue = None\n        self.random_seed = random_seed\n\n    def start(self, workers=1, max_queue_size=10):\n        def data_generator_task():\n            while not self._stop_event.is_set():\n                try:\n                    if self._use_multiprocessing or self.queue.qsize() < max_queue_size:\n                        generator_output = next(self._generator)\n                        self.queue.put(generator_output)\n                    else:\n                        time.sleep(self.wait_time)\n                except Exception:\n                    self._stop_event.set()\n                    raise\n\n        try:\n            if self._use_multiprocessing:\n                self.queue = multiprocessing.Queue(maxsize=max_queue_size)\n                self._stop_event = multiprocessing.Event()\n            else:\n                self.queue = queue.Queue()\n                self._stop_event = threading.Event()\n\n            for _ in range(workers):\n                if self._use_multiprocessing:\n                    # Reset random seed else all children processes\n                    # share the same seed\n                    np.random.seed(self.random_seed)\n                    thread = multiprocessing.Process(target=data_generator_task)\n                    thread.daemon = True\n                    if self.random_seed is not None:\n                        self.random_seed += 1\n                else:\n                    thread = threading.Thread(target=data_generator_task)\n                self._threads.append(thread)\n                thread.start()\n        except:\n            self.stop()\n            raise\n\n    def is_running(self):\n        return self._stop_event is not None and not self._stop_event.is_set()\n\n    def stop(self, timeout=None):\n        if self.is_running():\n            self._stop_event.set()\n\n        for thread in self._threads:\n            if thread.is_alive():\n                if self._use_multiprocessing:\n                    thread.terminate()\n                else:\n                    thread.join(timeout)\n\n        if self._use_multiprocessing:\n            if self.queue is not None:\n                self.queue.close()\n\n        self._threads = []\n        self._stop_event = None\n        self.queue = None\n\n    def get(self):\n        while self.is_running():\n            if not self.queue.empty():\n                inputs = self.queue.get()\n                if inputs is not None:\n                    yield inputs\n            else:\n                time.sleep(self.wait_time)\n'"
utils/prepare/split_label.py,0,"b'import os\nimport sys\n\nimport cv2 as cv\nimport numpy as np\nfrom tqdm import tqdm\n\nsys.path.append(os.getcwd())\nfrom utils.prepare.utils import orderConvex, shrink_poly\n\nDATA_FOLDER = ""/media/D/DataSet/mlt_selected/""\nOUTPUT = ""data/dataset/mlt/""\nMAX_LEN = 1200\nMIN_LEN = 600\n\nim_fns = os.listdir(os.path.join(DATA_FOLDER, ""image""))\nim_fns.sort()\n\nif not os.path.exists(os.path.join(OUTPUT, ""image"")):\n    os.makedirs(os.path.join(OUTPUT, ""image""))\nif not os.path.exists(os.path.join(OUTPUT, ""label"")):\n    os.makedirs(os.path.join(OUTPUT, ""label""))\n\nfor im_fn in tqdm(im_fns):\n    try:\n        _, fn = os.path.split(im_fn)\n        bfn, ext = os.path.splitext(fn)\n        if ext.lower() not in [\'.jpg\', \'.png\']:\n            continue\n\n        gt_path = os.path.join(DATA_FOLDER, ""label"", \'gt_\' + bfn + \'.txt\')\n        img_path = os.path.join(DATA_FOLDER, ""image"", im_fn)\n\n        img = cv.imread(img_path)\n        img_size = img.shape\n        im_size_min = np.min(img_size[0:2])\n        im_size_max = np.max(img_size[0:2])\n\n        im_scale = float(600) / float(im_size_min)\n        if np.round(im_scale * im_size_max) > 1200:\n            im_scale = float(1200) / float(im_size_max)\n        new_h = int(img_size[0] * im_scale)\n        new_w = int(img_size[1] * im_scale)\n\n        new_h = new_h if new_h // 16 == 0 else (new_h // 16 + 1) * 16\n        new_w = new_w if new_w // 16 == 0 else (new_w // 16 + 1) * 16\n\n        re_im = cv.resize(img, (new_w, new_h), interpolation=cv.INTER_LINEAR)\n        re_size = re_im.shape\n\n        polys = []\n        with open(gt_path, \'r\') as f:\n            lines = f.readlines()\n        for line in lines:\n            splitted_line = line.strip().lower().split(\',\')\n            x1, y1, x2, y2, x3, y3, x4, y4 = map(float, splitted_line[:8])\n            poly = np.array([x1, y1, x2, y2, x3, y3, x4, y4]).reshape([4, 2])\n            poly[:, 0] = poly[:, 0] / img_size[1] * re_size[1]\n            poly[:, 1] = poly[:, 1] / img_size[0] * re_size[0]\n            poly = orderConvex(poly)\n            polys.append(poly)\n\n            # cv.polylines(re_im, [poly.astype(np.int32).reshape((-1, 1, 2))], True,color=(0, 255, 0), thickness=2)\n\n        res_polys = []\n        for poly in polys:\n            # delete polys with width less than 10 pixel\n            if np.linalg.norm(poly[0] - poly[1]) < 10 or np.linalg.norm(poly[3] - poly[0]) < 10:\n                continue\n\n            res = shrink_poly(poly)\n            # for p in res:\n            #    cv.polylines(re_im, [p.astype(np.int32).reshape((-1, 1, 2))], True, color=(0, 255, 0), thickness=1)\n\n            res = res.reshape([-1, 4, 2])\n            for r in res:\n                x_min = np.min(r[:, 0])\n                y_min = np.min(r[:, 1])\n                x_max = np.max(r[:, 0])\n                y_max = np.max(r[:, 1])\n\n                res_polys.append([x_min, y_min, x_max, y_max])\n\n        cv.imwrite(os.path.join(OUTPUT, ""image"", fn), re_im)\n        with open(os.path.join(OUTPUT, ""label"", bfn) + "".txt"", ""w"") as f:\n            for p in res_polys:\n                line = "","".join(str(p[i]) for i in range(4))\n                f.writelines(line + ""\\r\\n"")\n                # for p in res_polys:\n                #    cv.rectangle(re_im,(p[0],p[1]),(p[2],p[3]),color=(0,0,255),thickness=1)\n\n                # cv.imshow(""demo"",re_im)\n                # cv.waitKey(0)\n    except:\n        print(""Error processing {}"".format(im_fn))\n'"
utils/prepare/utils.py,0,"b'import numpy as np\nfrom shapely.geometry import Polygon\n\n\ndef pickTopLeft(poly):\n    idx = np.argsort(poly[:, 0])\n    if poly[idx[0], 1] < poly[idx[1], 1]:\n        s = idx[0]\n    else:\n        s = idx[1]\n\n    return poly[(s, (s + 1) % 4, (s + 2) % 4, (s + 3) % 4), :]\n\n\ndef orderConvex(p):\n    points = Polygon(p).convex_hull\n    points = np.array(points.exterior.coords)[:4]\n    points = points[::-1]\n    points = pickTopLeft(points)\n    points = np.array(points).reshape([4, 2])\n    return points\n\n\ndef shrink_poly(poly, r=16):\n    # y = kx + b\n    x_min = int(np.min(poly[:, 0]))\n    x_max = int(np.max(poly[:, 0]))\n\n    k1 = (poly[1][1] - poly[0][1]) / (poly[1][0] - poly[0][0])\n    b1 = poly[0][1] - k1 * poly[0][0]\n\n    k2 = (poly[2][1] - poly[3][1]) / (poly[2][0] - poly[3][0])\n    b2 = poly[3][1] - k2 * poly[3][0]\n\n    res = []\n\n    start = int((x_min // 16 + 1) * 16)\n    end = int((x_max // 16) * 16)\n\n    p = x_min\n    res.append([p, int(k1 * p + b1),\n                start - 1, int(k1 * (p + 15) + b1),\n                start - 1, int(k2 * (p + 15) + b2),\n                p, int(k2 * p + b2)])\n\n    for p in range(start, end + 1, r):\n        res.append([p, int(k1 * p + b1),\n                    (p + 15), int(k1 * (p + 15) + b1),\n                    (p + 15), int(k2 * (p + 15) + b2),\n                    p, int(k2 * p + b2)])\n    return np.array(res, dtype=np.int).reshape([-1, 8])\n'"
utils/rpn_msr/__init__.py,0,b''
utils/rpn_msr/anchor_target_layer.py,0,"b'# -*- coding:utf-8 -*-\nimport numpy as np\nimport numpy.random as npr\nfrom utils.bbox.bbox import bbox_overlaps\n\nfrom utils.bbox.bbox_transform import bbox_transform\nfrom utils.rpn_msr.config import Config as cfg\nfrom utils.rpn_msr.generate_anchors import generate_anchors\n\nDEBUG = False\n\n\ndef anchor_target_layer(rpn_cls_score, gt_boxes, im_info, _feat_stride=[16, ], anchor_scales=[16, ]):\n    """"""\n    Assign anchors to ground-truth targets. Produces anchor classification\n    labels and bounding-box regression targets.\n    Parameters\n    ----------\n    rpn_cls_score: (1, H, W, Ax2) bg/fg scores of previous conv layer\n    gt_boxes: (G, 5) vstack of [x1, y1, x2, y2, class]\n    im_info: a list of [image_height, image_width, scale_ratios]\n    _feat_stride: the downsampling ratio of feature map to the original input image\n    anchor_scales: the scales to the basic_anchor (basic anchor is [16, 16])\n    ----------\n    Returns\n    ----------\n    rpn_labels : (HxWxA, 1), for each anchor, 0 denotes bg, 1 fg, -1 dontcare\n    rpn_bbox_targets: (HxWxA, 4), distances of the anchors to the gt_boxes(may contains some transform)\n                            that are the regression objectives\n    rpn_bbox_inside_weights: (HxWxA, 4) weights of each boxes, mainly accepts hyper param in cfg\n    rpn_bbox_outside_weights: (HxWxA, 4) used to balance the fg/bg,\n                            beacuse the numbers of bgs and fgs mays significiantly different\n    """"""\n    _anchors = generate_anchors(scales=np.array(anchor_scales))  # \xe7\x94\x9f\xe6\x88\x90\xe5\x9f\xba\xe6\x9c\xac\xe7\x9a\x84anchor,\xe4\xb8\x80\xe5\x85\xb19\xe4\xb8\xaa\n    _num_anchors = _anchors.shape[0]  # 9\xe4\xb8\xaaanchor\n\n    if DEBUG:\n        print(\'anchors:\')\n        print(_anchors)\n        print(\'anchor shapes:\')\n        print(np.hstack((\n            _anchors[:, 2::4] - _anchors[:, 0::4],\n            _anchors[:, 3::4] - _anchors[:, 1::4],\n        )))\n        _counts = cfg.EPS\n        _sums = np.zeros((1, 4))\n        _squared_sums = np.zeros((1, 4))\n        _fg_sum = 0\n        _bg_sum = 0\n        _count = 0\n\n    # allow boxes to sit over the edge by a small amount\n    _allowed_border = 0\n    # map of shape (..., H, W)\n    # height, width = rpn_cls_score.shape[1:3]\n\n    im_info = im_info[0]  # \xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe9\xab\x98\xe5\xae\xbd\xe5\x8f\x8a\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    if DEBUG:\n        print(""im_info: "", im_info)\n    # \xe5\x9c\xa8feature-map\xe4\xb8\x8a\xe5\xae\x9a\xe4\xbd\x8danchor\xef\xbc\x8c\xe5\xb9\xb6\xe5\x8a\xa0\xe4\xb8\x8adelta\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe5\x9c\xa8\xe5\xae\x9e\xe9\x99\x85\xe5\x9b\xbe\xe5\x83\x8f\xe4\xb8\xadanchor\xe7\x9a\x84\xe7\x9c\x9f\xe5\xae\x9e\xe5\x9d\x90\xe6\xa0\x87\n    # Algorithm:\n    # for each (H, W) location i\n    #   generate 9 anchor boxes centered on cell i\n    #   apply predicted bbox deltas at cell i to each of the 9 anchors\n    # filter out-of-image anchors\n    # measure GT overlap\n\n    assert rpn_cls_score.shape[0] == 1, \'Only single item batches are supported\'\n\n    # map of shape (..., H, W)\n    height, width = rpn_cls_score.shape[1:3]  # feature-map\xe7\x9a\x84\xe9\xab\x98\xe5\xae\xbd\n\n    if DEBUG:\n        print(\'AnchorTargetLayer: height\', height, \'width\', width)\n        print(\'\')\n        print(\'im_size: ({}, {})\'.format(im_info[0], im_info[1]))\n        print(\'scale: {}\'.format(im_info[2]))\n        print(\'height, width: ({}, {})\'.format(height, width))\n        print(\'rpn: gt_boxes.shape\', gt_boxes.shape)\n        print(\'rpn: gt_boxes\', gt_boxes)\n\n    # 1. Generate proposals from bbox deltas and shifted anchors\n    shift_x = np.arange(0, width) * _feat_stride\n    shift_y = np.arange(0, height) * _feat_stride\n    shift_x, shift_y = np.meshgrid(shift_x, shift_y)  # in W H order\n    # K is H x W\n    shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                        shift_x.ravel(), shift_y.ravel())).transpose()  # \xe7\x94\x9f\xe6\x88\x90feature-map\xe5\x92\x8c\xe7\x9c\x9f\xe5\xae\x9eimage\xe4\xb8\x8aanchor\xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84\xe5\x81\x8f\xe7\xa7\xbb\xe9\x87\x8f\n    # add A anchors (1, A, 4) to\n    # cell K shifts (K, 1, 4) to get\n    # shift anchors (K, A, 4)\n    # reshape to (K*A, 4) shifted anchors\n    A = _num_anchors  # 9\xe4\xb8\xaaanchor\n    K = shifts.shape[0]  # 50*37\xef\xbc\x8cfeature-map\xe7\x9a\x84\xe5\xae\xbd\xe4\xb9\x98\xe9\xab\x98\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\n    all_anchors = (_anchors.reshape((1, A, 4)) +\n                   shifts.reshape((1, K, 4)).transpose((1, 0, 2)))  # \xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe5\xa4\x8d\xe5\x88\xb6\xe5\xae\xbd\xe9\xab\x98\xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe7\x9b\xb8\xe5\x8a\xa0\n    all_anchors = all_anchors.reshape((K * A, 4))\n    total_anchors = int(K * A)\n\n    # only keep anchors inside the image\n    # \xe4\xbb\x85\xe4\xbf\x9d\xe7\x95\x99\xe9\x82\xa3\xe4\xba\x9b\xe8\xbf\x98\xe5\x9c\xa8\xe5\x9b\xbe\xe5\x83\x8f\xe5\x86\x85\xe9\x83\xa8\xe7\x9a\x84anchor\xef\xbc\x8c\xe8\xb6\x85\xe5\x87\xba\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe9\x83\xbd\xe5\x88\xa0\xe6\x8e\x89\n    inds_inside = np.where(\n        (all_anchors[:, 0] >= -_allowed_border) &\n        (all_anchors[:, 1] >= -_allowed_border) &\n        (all_anchors[:, 2] < im_info[1] + _allowed_border) &  # width\n        (all_anchors[:, 3] < im_info[0] + _allowed_border)  # height\n    )[0]\n\n    if DEBUG:\n        print(\'total_anchors\', total_anchors)\n        print(\'inds_inside\', len(inds_inside))\n\n    # keep only inside anchors\n    anchors = all_anchors[inds_inside, :]  # \xe4\xbf\x9d\xe7\x95\x99\xe9\x82\xa3\xe4\xba\x9b\xe5\x9c\xa8\xe5\x9b\xbe\xe5\x83\x8f\xe5\x86\x85\xe7\x9a\x84anchor\n    if DEBUG:\n        print(\'anchors.shape\', anchors.shape)\n\n    # \xe8\x87\xb3\xe6\xad\xa4\xef\xbc\x8canchor\xe5\x87\x86\xe5\xa4\x87\xe5\xa5\xbd\xe4\xba\x86\n    # --------------------------------------------------------------\n    # label: 1 is positive, 0 is negative, -1 is dont care\n    # (A)\n    labels = np.empty((len(inds_inside),), dtype=np.float32)\n    labels.fill(-1)  # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96label\xef\xbc\x8c\xe5\x9d\x87\xe4\xb8\xba-1\n\n    # overlaps between the anchors and the gt boxes\n    # overlaps (ex, gt), shape is A x G\n    # \xe8\xae\xa1\xe7\xae\x97anchor\xe5\x92\x8cgt-box\xe7\x9a\x84overlap\xef\xbc\x8c\xe7\x94\xa8\xe6\x9d\xa5\xe7\xbb\x99anchor\xe4\xb8\x8a\xe6\xa0\x87\xe7\xad\xbe\n    overlaps = bbox_overlaps(\n        np.ascontiguousarray(anchors, dtype=np.float),\n        np.ascontiguousarray(gt_boxes, dtype=np.float))  # \xe5\x81\x87\xe8\xae\xbeanchors\xe6\x9c\x89x\xe4\xb8\xaa\xef\xbc\x8cgt_boxes\xe6\x9c\x89y\xe4\xb8\xaa\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe7\x9a\x84\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xef\xbc\x88x,y\xef\xbc\x89\xe7\x9a\x84\xe6\x95\xb0\xe7\xbb\x84\n    # \xe5\xad\x98\xe6\x94\xbe\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaaanchor\xe5\x92\x8c\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaagtbox\xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84overlap\n    argmax_overlaps = overlaps.argmax(axis=1)  # (A)#\xe6\x89\xbe\xe5\x88\xb0\xe5\x92\x8c\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaagtbox\xef\xbc\x8coverlap\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe9\x82\xa3\xe4\xb8\xaaanchor\n    max_overlaps = overlaps[np.arange(len(inds_inside)), argmax_overlaps]\n    gt_argmax_overlaps = overlaps.argmax(axis=0)  # G#\xe6\x89\xbe\xe5\x88\xb0\xe6\xaf\x8f\xe4\xb8\xaa\xe4\xbd\x8d\xe7\xbd\xae\xe4\xb8\x8a9\xe4\xb8\xaaanchor\xe4\xb8\xad\xe4\xb8\x8egtbox\xef\xbc\x8coverlap\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe9\x82\xa3\xe4\xb8\xaa\n    gt_max_overlaps = overlaps[gt_argmax_overlaps,\n                               np.arange(overlaps.shape[1])]\n    gt_argmax_overlaps = np.where(overlaps == gt_max_overlaps)[0]\n\n    if not cfg.RPN_CLOBBER_POSITIVES:\n        # assign bg labels first so that positive labels can clobber them\n        labels[max_overlaps < cfg.RPN_NEGATIVE_OVERLAP] = 0  # \xe5\x85\x88\xe7\xbb\x99\xe8\x83\x8c\xe6\x99\xaf\xe4\xb8\x8a\xe6\xa0\x87\xe7\xad\xbe\xef\xbc\x8c\xe5\xb0\x8f\xe4\xba\x8e0.3overlap\xe7\x9a\x84\n\n    # fg label: for each gt, anchor with highest overlap\n    labels[gt_argmax_overlaps] = 1  # \xe6\xaf\x8f\xe4\xb8\xaa\xe4\xbd\x8d\xe7\xbd\xae\xe4\xb8\x8a\xe7\x9a\x849\xe4\xb8\xaaanchor\xe4\xb8\xadoverlap\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe8\xae\xa4\xe4\xb8\xba\xe6\x98\xaf\xe5\x89\x8d\xe6\x99\xaf\n    # fg label: above threshold IOU\n    labels[max_overlaps >= cfg.RPN_POSITIVE_OVERLAP] = 1  # overlap\xe5\xa4\xa7\xe4\xba\x8e0.7\xe7\x9a\x84\xe8\xae\xa4\xe4\xb8\xba\xe6\x98\xaf\xe5\x89\x8d\xe6\x99\xaf\n\n    if cfg.RPN_CLOBBER_POSITIVES:\n        # assign bg labels last so that negative labels can clobber positives\n        labels[max_overlaps < cfg.RPN_NEGATIVE_OVERLAP] = 0\n\n    # subsample positive labels if we have too many\n    # \xe5\xaf\xb9\xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xe8\xbf\x9b\xe8\xa1\x8c\xe9\x87\x87\xe6\xa0\xb7\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\xe5\xa4\xaa\xe5\xa4\x9a\xe7\x9a\x84\xe8\xaf\x9d\n    # \xe9\x99\x90\xe5\x88\xb6\xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\xe4\xb8\x8d\xe8\xb6\x85\xe8\xbf\x87128\xe4\xb8\xaa\n    num_fg = int(cfg.RPN_FG_FRACTION * cfg.RPN_BATCHSIZE)\n    fg_inds = np.where(labels == 1)[0]\n    if len(fg_inds) > num_fg:\n        disable_inds = npr.choice(\n            fg_inds, size=(len(fg_inds) - num_fg), replace=False)  # \xe9\x9a\x8f\xe6\x9c\xba\xe5\x8e\xbb\xe9\x99\xa4\xe6\x8e\x89\xe4\xb8\x80\xe4\xba\x9b\xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\n        labels[disable_inds] = -1  # \xe5\x8f\x98\xe4\xb8\xba-1\n\n    # subsample negative labels if we have too many\n    # \xe5\xaf\xb9\xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\xe8\xbf\x9b\xe8\xa1\x8c\xe9\x87\x87\xe6\xa0\xb7\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\xe5\xa4\xaa\xe5\xa4\x9a\xe7\x9a\x84\xe8\xaf\x9d\n    # \xe6\xad\xa3\xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\xe6\x80\xbb\xe6\x95\xb0\xe6\x98\xaf256\xef\xbc\x8c\xe9\x99\x90\xe5\x88\xb6\xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\xe7\x9b\xae\xe6\x9c\x80\xe5\xa4\x9a128\xef\xbc\x8c\n    # \xe5\xa6\x82\xe6\x9e\x9c\xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\xe9\x87\x8f\xe5\xb0\x8f\xe4\xba\x8e128\xef\xbc\x8c\xe5\xb7\xae\xe7\x9a\x84\xe9\x82\xa3\xe4\xba\x9b\xe5\xb0\xb1\xe7\x94\xa8\xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\xe8\xa1\xa5\xe4\xb8\x8a\xef\xbc\x8c\xe5\x87\x91\xe9\xbd\x90256\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\n    num_bg = cfg.RPN_BATCHSIZE - np.sum(labels == 1)\n    bg_inds = np.where(labels == 0)[0]\n    if len(bg_inds) > num_bg:\n        disable_inds = npr.choice(\n            bg_inds, size=(len(bg_inds) - num_bg), replace=False)\n        labels[disable_inds] = -1\n        # print ""was %s inds, disabling %s, now %s inds"" % (\n        # len(bg_inds), len(disable_inds), np.sum(labels == 0))\n\n    # \xe8\x87\xb3\xe6\xad\xa4\xef\xbc\x8c \xe4\xb8\x8a\xe5\xa5\xbd\xe6\xa0\x87\xe7\xad\xbe\xef\xbc\x8c\xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xa1\xe7\xae\x97rpn-box\xe7\x9a\x84\xe7\x9c\x9f\xe5\x80\xbc\n    # --------------------------------------------------------------\n    bbox_targets = np.zeros((len(inds_inside), 4), dtype=np.float32)\n    bbox_targets = _compute_targets(anchors, gt_boxes[argmax_overlaps, :])  # \xe6\xa0\xb9\xe6\x8d\xaeanchor\xe5\x92\x8cgtbox\xe8\xae\xa1\xe7\xae\x97\xe5\xbe\x97\xe7\x9c\x9f\xe5\x80\xbc\xef\xbc\x88anchor\xe5\x92\x8cgtbox\xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84\xe5\x81\x8f\xe5\xb7\xae\xef\xbc\x89\n\n    bbox_inside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n    bbox_inside_weights[labels == 1, :] = np.array(cfg.RPN_BBOX_INSIDE_WEIGHTS)  # \xe5\x86\x85\xe9\x83\xa8\xe6\x9d\x83\xe9\x87\x8d\xef\xbc\x8c\xe5\x89\x8d\xe6\x99\xaf\xe5\xb0\xb1\xe7\xbb\x991\xef\xbc\x8c\xe5\x85\xb6\xe4\xbb\x96\xe6\x98\xaf0\n\n    bbox_outside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n    if cfg.RPN_POSITIVE_WEIGHT < 0:  # \xe6\x9a\x82\xe6\x97\xb6\xe4\xbd\xbf\xe7\x94\xa8uniform \xe6\x9d\x83\xe9\x87\x8d\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf\xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xe6\x98\xaf1\xef\xbc\x8c\xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\xe6\x98\xaf0\n        # uniform weighting of examples (given non-uniform sampling)\n        num_examples = np.sum(labels >= 0) + 1\n        # positive_weights = np.ones((1, 4)) * 1.0 / num_examples\n        # negative_weights = np.ones((1, 4)) * 1.0 / num_examples\n        positive_weights = np.ones((1, 4))\n        negative_weights = np.zeros((1, 4))\n    else:\n        assert ((cfg.RPN_POSITIVE_WEIGHT > 0) &\n                (cfg.RPN_POSITIVE_WEIGHT < 1))\n        positive_weights = (cfg.RPN_POSITIVE_WEIGHT /\n                            (np.sum(labels == 1)) + 1)\n        negative_weights = ((1.0 - cfg.RPN_POSITIVE_WEIGHT) /\n                            (np.sum(labels == 0)) + 1)\n    bbox_outside_weights[labels == 1, :] = positive_weights  # \xe5\xa4\x96\xe9\x83\xa8\xe6\x9d\x83\xe9\x87\x8d\xef\xbc\x8c\xe5\x89\x8d\xe6\x99\xaf\xe6\x98\xaf1\xef\xbc\x8c\xe8\x83\x8c\xe6\x99\xaf\xe6\x98\xaf0\n    bbox_outside_weights[labels == 0, :] = negative_weights\n\n    if DEBUG:\n        _sums += bbox_targets[labels == 1, :].sum(axis=0)\n        _squared_sums += (bbox_targets[labels == 1, :] ** 2).sum(axis=0)\n        _counts += np.sum(labels == 1)\n        means = _sums / _counts\n        stds = np.sqrt(_squared_sums / _counts - means ** 2)\n        print(\'means:\')\n        print(means)\n        print(\'stdevs:\')\n        print(stds)\n\n    # map up to original set of anchors\n    # \xe4\xb8\x80\xe5\xbc\x80\xe5\xa7\x8b\xe6\x98\xaf\xe5\xb0\x86\xe8\xb6\x85\xe5\x87\xba\xe5\x9b\xbe\xe5\x83\x8f\xe8\x8c\x83\xe5\x9b\xb4\xe7\x9a\x84anchor\xe7\x9b\xb4\xe6\x8e\xa5\xe4\xb8\xa2\xe6\x8e\x89\xe7\x9a\x84\xef\xbc\x8c\xe7\x8e\xb0\xe5\x9c\xa8\xe5\x9c\xa8\xe5\x8a\xa0\xe5\x9b\x9e\xe6\x9d\xa5\n    labels = _unmap(labels, total_anchors, inds_inside, fill=-1)  # \xe8\xbf\x99\xe4\xba\x9banchor\xe7\x9a\x84label\xe6\x98\xaf-1\xef\xbc\x8c\xe4\xb9\x9f\xe5\x8d\xb3dontcare\n    bbox_targets = _unmap(bbox_targets, total_anchors, inds_inside, fill=0)  # \xe8\xbf\x99\xe4\xba\x9banchor\xe7\x9a\x84\xe7\x9c\x9f\xe5\x80\xbc\xe6\x98\xaf0\xef\xbc\x8c\xe4\xb9\x9f\xe5\x8d\xb3\xe6\xb2\xa1\xe6\x9c\x89\xe5\x80\xbc\n    bbox_inside_weights = _unmap(bbox_inside_weights, total_anchors, inds_inside, fill=0)  # \xe5\x86\x85\xe9\x83\xa8\xe6\x9d\x83\xe9\x87\x8d\xe4\xbb\xa50\xe5\xa1\xab\xe5\x85\x85\n    bbox_outside_weights = _unmap(bbox_outside_weights, total_anchors, inds_inside, fill=0)  # \xe5\xa4\x96\xe9\x83\xa8\xe6\x9d\x83\xe9\x87\x8d\xe4\xbb\xa50\xe5\xa1\xab\xe5\x85\x85\n\n    if DEBUG:\n        print(\'rpn: max max_overlap\', np.max(max_overlaps))\n        print(\'rpn: num_positive\', np.sum(labels == 1))\n        print(\'rpn: num_negative\', np.sum(labels == 0))\n        _fg_sum += np.sum(labels == 1)\n        _bg_sum += np.sum(labels == 0)\n        _count += 1\n        print(\'rpn: num_positive avg\', _fg_sum / _count)\n        print(\'rpn: num_negative avg\', _bg_sum / _count)\n\n    # labels\n    labels = labels.reshape((1, height, width, A))  # reshap\xe4\xb8\x80\xe4\xb8\x8blabel\n    rpn_labels = labels\n\n    # bbox_targets\n    bbox_targets = bbox_targets \\\n        .reshape((1, height, width, A * 4))  # reshape\n\n    rpn_bbox_targets = bbox_targets\n    # bbox_inside_weights\n    bbox_inside_weights = bbox_inside_weights \\\n        .reshape((1, height, width, A * 4))\n\n    rpn_bbox_inside_weights = bbox_inside_weights\n\n    # bbox_outside_weights\n    bbox_outside_weights = bbox_outside_weights \\\n        .reshape((1, height, width, A * 4))\n    rpn_bbox_outside_weights = bbox_outside_weights\n\n    if DEBUG:\n        print(""anchor target set"")\n    return rpn_labels, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights\n\n\ndef _unmap(data, count, inds, fill=0):\n    """""" Unmap a subset of item (data) back to the original set of items (of\n    size count) """"""\n    if len(data.shape) == 1:\n        ret = np.empty((count,), dtype=np.float32)\n        ret.fill(fill)\n        ret[inds] = data\n    else:\n        ret = np.empty((count,) + data.shape[1:], dtype=np.float32)\n        ret.fill(fill)\n        ret[inds, :] = data\n    return ret\n\n\ndef _compute_targets(ex_rois, gt_rois):\n    """"""Compute bounding-box regression targets for an image.""""""\n\n    assert ex_rois.shape[0] == gt_rois.shape[0]\n    assert ex_rois.shape[1] == 4\n    assert gt_rois.shape[1] == 5\n\n    return bbox_transform(ex_rois, gt_rois[:, :4]).astype(np.float32, copy=False)\n'"
utils/rpn_msr/config.py,0,"b'class Config:\n    EPS = 1e-14\n    RPN_CLOBBER_POSITIVES = False\n    RPN_NEGATIVE_OVERLAP = 0.3\n    RPN_POSITIVE_OVERLAP = 0.7\n    RPN_FG_FRACTION = 0.5\n    RPN_BATCHSIZE = 300\n    RPN_BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n    RPN_POSITIVE_WEIGHT = -1.0\n\n    RPN_PRE_NMS_TOP_N = 12000\n    RPN_POST_NMS_TOP_N = 1000\n    RPN_NMS_THRESH = 0.7\n    RPN_MIN_SIZE = 8\n'"
utils/rpn_msr/generate_anchors.py,0,"b""import numpy as np\n\n\ndef generate_basic_anchors(sizes, base_size=16):\n    base_anchor = np.array([0, 0, base_size - 1, base_size - 1], np.int32)\n    anchors = np.zeros((len(sizes), 4), np.int32)\n    index = 0\n    for h, w in sizes:\n        anchors[index] = scale_anchor(base_anchor, h, w)\n        index += 1\n    return anchors\n\n\ndef scale_anchor(anchor, h, w):\n    x_ctr = (anchor[0] + anchor[2]) * 0.5\n    y_ctr = (anchor[1] + anchor[3]) * 0.5\n    scaled_anchor = anchor.copy()\n    scaled_anchor[0] = x_ctr - w / 2  # xmin\n    scaled_anchor[2] = x_ctr + w / 2  # xmax\n    scaled_anchor[1] = y_ctr - h / 2  # ymin\n    scaled_anchor[3] = y_ctr + h / 2  # ymax\n    return scaled_anchor\n\n\ndef generate_anchors(base_size=16, ratios=[0.5, 1, 2],\n                     scales=2 ** np.arange(3, 6)):\n    heights = [11, 16, 23, 33, 48, 68, 97, 139, 198, 283]\n    widths = [16]\n    sizes = []\n    for h in heights:\n        for w in widths:\n            sizes.append((h, w))\n    return generate_basic_anchors(sizes)\n\n\nif __name__ == '__main__':\n    import time\n\n    t = time.time()\n    a = generate_anchors()\n    print(time.time() - t)\n    print(a)\n    from IPython import embed;\n\n    embed()\n"""
utils/rpn_msr/proposal_layer.py,0,"b'# -*- coding:utf-8 -*-\nimport numpy as np\nfrom utils.bbox.nms import nms\n\nfrom utils.bbox.bbox_transform import bbox_transform_inv, clip_boxes\nfrom utils.rpn_msr.config import Config as cfg\nfrom utils.rpn_msr.generate_anchors import generate_anchors\n\nDEBUG = False\n\n\ndef proposal_layer(rpn_cls_prob_reshape, rpn_bbox_pred, im_info, _feat_stride=[16, ], anchor_scales=[16, ]):\n    """"""\n    Parameters\n    ----------\n    rpn_cls_prob_reshape: (1 , H , W , Ax2) outputs of RPN, prob of bg or fg\n                         NOTICE: the old version is ordered by (1, H, W, 2, A) !!!!\n    rpn_bbox_pred: (1 , H , W , Ax4), rgs boxes output of RPN\n    im_info: a list of [image_height, image_width, scale_ratios]\n    _feat_stride: the downsampling ratio of feature map to the original input image\n    anchor_scales: the scales to the basic_anchor (basic anchor is [16, 16])\n    ----------\n    Returns\n    ----------\n    rpn_rois : (1 x H x W x A, 5) e.g. [0, x1, y1, x2, y2]\n    # Algorithm:\n    #\n    # for each (H, W) location i\n    #   generate A anchor boxes centered on cell i\n    #   apply predicted bbox deltas at cell i to each of the A anchors\n    # clip predicted boxes to image\n    # remove predicted boxes with either height or width < threshold\n    # sort all (proposal, score) pairs by score from highest to lowest\n    # take top pre_nms_topN proposals before NMS\n    # apply NMS with threshold 0.7 to remaining proposals\n    # take after_nms_topN proposals after NMS\n    # return the top proposals (-> RoIs top, scores top)\n    #layer_params = yaml.load(self.param_str_)\n    """"""\n\n    _anchors = generate_anchors(scales=np.array(anchor_scales))  # \xe7\x94\x9f\xe6\x88\x90\xe5\x9f\xba\xe6\x9c\xac\xe7\x9a\x849\xe4\xb8\xaaanchor\n    _num_anchors = _anchors.shape[0]  # 9\xe4\xb8\xaaanchor\n\n    im_info = im_info[0]  # \xe5\x8e\x9f\xe5\xa7\x8b\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe9\xab\x98\xe5\xae\xbd\xe3\x80\x81\xe7\xbc\xa9\xe6\x94\xbe\xe5\xb0\xba\xe5\xba\xa6\n\n    assert rpn_cls_prob_reshape.shape[0] == 1, \\\n        \'Only single item batches are supported\'\n\n    pre_nms_topN = cfg.RPN_PRE_NMS_TOP_N  # 12000,\xe5\x9c\xa8\xe5\x81\x9anms\xe4\xb9\x8b\xe5\x89\x8d\xef\xbc\x8c\xe6\x9c\x80\xe5\xa4\x9a\xe4\xbf\x9d\xe7\x95\x99\xe7\x9a\x84\xe5\x80\x99\xe9\x80\x89box\xe6\x95\xb0\xe7\x9b\xae\n    post_nms_topN = cfg.RPN_POST_NMS_TOP_N  # 2000\xef\xbc\x8c\xe5\x81\x9a\xe5\xae\x8cnms\xe4\xb9\x8b\xe5\x90\x8e\xef\xbc\x8c\xe6\x9c\x80\xe5\xa4\x9a\xe4\xbf\x9d\xe7\x95\x99\xe7\x9a\x84box\xe7\x9a\x84\xe6\x95\xb0\xe7\x9b\xae\n    nms_thresh = cfg.RPN_NMS_THRESH  # nms\xe7\x94\xa8\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe9\x98\x88\xe5\x80\xbc\xe6\x98\xaf0.7\n    min_size = cfg.RPN_MIN_SIZE  # \xe5\x80\x99\xe9\x80\x89box\xe7\x9a\x84\xe6\x9c\x80\xe5\xb0\x8f\xe5\xb0\xba\xe5\xaf\xb8\xef\xbc\x8c\xe7\x9b\xae\xe5\x89\x8d\xe6\x98\xaf16\xef\xbc\x8c\xe9\xab\x98\xe5\xae\xbd\xe5\x9d\x87\xe8\xa6\x81\xe5\xa4\xa7\xe4\xba\x8e16\n\n    height, width = rpn_cls_prob_reshape.shape[1:3]  # feature-map\xe7\x9a\x84\xe9\xab\x98\xe5\xae\xbd\n    width = width // 10\n\n    # the first set of _num_anchors channels are bg probs\n    # the second set are the fg probs, which we want\n    # (1, H, W, A)\n    scores = np.reshape(np.reshape(rpn_cls_prob_reshape, [1, height, width, _num_anchors, 2])[:, :, :, :, 1],\n                        [1, height, width, _num_anchors])\n    # \xe6\x8f\x90\xe5\x8f\x96\xe5\x88\xb0object\xe7\x9a\x84\xe5\x88\x86\xe6\x95\xb0\xef\xbc\x8cnon-object\xe7\x9a\x84\xe6\x88\x91\xe4\xbb\xac\xe4\xb8\x8d\xe5\x85\xb3\xe5\xbf\x83\n\n    bbox_deltas = rpn_bbox_pred  # \xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84pred\xe6\x98\xaf\xe7\x9b\xb8\xe5\xaf\xb9\xe5\x80\xbc\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe8\xbf\x9b\xe4\xb8\x80\xe6\xad\xa5\xe5\xa4\x84\xe7\x90\x86\xe6\x88\x90\xe7\x9c\x9f\xe5\xae\x9e\xe5\x9b\xbe\xe5\x83\x8f\xe4\xb8\xad\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\n    # im_info = bottom[2].data[0, :]\n\n    if DEBUG:\n        print(\'im_size: ({}, {})\'.format(im_info[0], im_info[1]))\n        print(\'scale: {}\'.format(im_info[2]))\n\n    # 1. Generate proposals from bbox deltas and shifted anchors\n    if DEBUG:\n        print(\'score map size: {}\'.format(scores.shape))\n\n    # Enumerate all shifts\n    # \xe5\x90\x8canchor-target-layer-tf\xe8\xbf\x99\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe7\x94\x9f\xe6\x88\x90anchor\xe7\x9a\x84shift\xef\xbc\x8c\xe8\xbf\x9b\xe4\xb8\x80\xe6\xad\xa5\xe5\xbe\x97\xe5\x88\xb0\xe6\x95\xb4\xe5\xbc\xa0\xe5\x9b\xbe\xe5\x83\x8f\xe4\xb8\x8a\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89anchor\n    shift_x = np.arange(0, width) * _feat_stride\n    shift_y = np.arange(0, height) * _feat_stride\n    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n    shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                        shift_x.ravel(), shift_y.ravel())).transpose()\n\n    # Enumerate all shifted anchors:\n    #\n    # add A anchors (1, A, 4) to\n    # cell K shifts (K, 1, 4) to get\n    # shift anchors (K, A, 4)\n    # reshape to (K*A, 4) shifted anchors\n    A = _num_anchors\n    K = shifts.shape[0]\n    anchors = _anchors.reshape((1, A, 4)) + \\\n              shifts.reshape((1, K, 4)).transpose((1, 0, 2))\n    anchors = anchors.reshape((K * A, 4))  # \xe8\xbf\x99\xe9\x87\x8c\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84anchor\xe5\xb0\xb1\xe6\x98\xaf\xe6\x95\xb4\xe5\xbc\xa0\xe5\x9b\xbe\xe5\x83\x8f\xe4\xb8\x8a\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89anchor\n\n    # Transpose and reshape predicted bbox transformations to get them\n    # into the same order as the anchors:\n    # bbox deltas will be (1, 4 * A, H, W) format\n    # transpose to (1, H, W, 4 * A)\n    # reshape to (1 * H * W * A, 4) where rows are ordered by (h, w, a)\n    # in slowest to fastest order\n    bbox_deltas = bbox_deltas.reshape((-1, 4))  # (HxWxA, 4)\n\n    # Same story for the scores:\n    scores = scores.reshape((-1, 1))\n\n    # Convert anchors into proposals via bbox transformations\n    proposals = bbox_transform_inv(anchors, bbox_deltas)  # \xe5\x81\x9a\xe9\x80\x86\xe5\x8f\x98\xe6\x8d\xa2\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0box\xe5\x9c\xa8\xe5\x9b\xbe\xe5\x83\x8f\xe4\xb8\x8a\xe7\x9a\x84\xe7\x9c\x9f\xe5\xae\x9e\xe5\x9d\x90\xe6\xa0\x87\n\n    # 2. clip predicted boxes to image\n    proposals = clip_boxes(proposals, im_info[:2])  # \xe5\xb0\x86\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84proposal\xe4\xbf\xae\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\x8b\xef\xbc\x8c\xe8\xb6\x85\xe5\x87\xba\xe5\x9b\xbe\xe5\x83\x8f\xe8\x8c\x83\xe5\x9b\xb4\xe7\x9a\x84\xe5\xb0\x86\xe4\xbc\x9a\xe8\xa2\xab\xe4\xbf\xae\xe5\x89\xaa\xe6\x8e\x89\n\n    # 3. remove predicted boxes with either height or width < threshold\n    # (NOTE: convert min_size to input image scale stored in im_info[2])\n    keep = _filter_boxes(proposals, min_size)  # \xe7\xa7\xbb\xe9\x99\xa4\xe9\x82\xa3\xe4\xba\x9bproposal\xe5\xb0\x8f\xe4\xba\x8e\xe4\xb8\x80\xe5\xae\x9a\xe5\xb0\xba\xe5\xaf\xb8\xe7\x9a\x84proposal\n    proposals = proposals[keep, :]  # \xe4\xbf\x9d\xe7\x95\x99\xe5\x89\xa9\xe4\xb8\x8b\xe7\x9a\x84proposal\n    scores = scores[keep]\n    bbox_deltas = bbox_deltas[keep, :]\n\n    # # remove irregular boxes, too fat too tall\n    # keep = _filter_irregular_boxes(proposals)\n    # proposals = proposals[keep, :]\n    # scores = scores[keep]\n\n    # 4. sort all (proposal, score) pairs by score from highest to lowest\n    # 5. take top pre_nms_topN (e.g. 6000)\n    order = scores.ravel().argsort()[::-1]  # score\xe6\x8c\x89\xe5\xbe\x97\xe5\x88\x86\xe7\x9a\x84\xe9\xab\x98\xe4\xbd\x8e\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x8e\x92\xe5\xba\x8f\n    if pre_nms_topN > 0:  # \xe4\xbf\x9d\xe7\x95\x9912000\xe4\xb8\xaaproposal\xe8\xbf\x9b\xe5\x8e\xbb\xe5\x81\x9anms\n        order = order[:pre_nms_topN]\n    proposals = proposals[order, :]\n    scores = scores[order]\n    bbox_deltas = bbox_deltas[order, :]\n\n    # 6. apply nms (e.g. threshold = 0.7)\n    # 7. take after_nms_topN (e.g. 300)\n    # 8. return the top proposals (-> RoIs top)\n    keep = nms(np.hstack((proposals, scores)), nms_thresh)  # \xe8\xbf\x9b\xe8\xa1\x8cnms\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe4\xbf\x9d\xe7\x95\x992000\xe4\xb8\xaaproposal\n    if post_nms_topN > 0:\n        keep = keep[:post_nms_topN]\n    proposals = proposals[keep, :]\n    scores = scores[keep]\n    bbox_deltas = bbox_deltas[keep, :]\n\n    # Output rois blob\n    # Our RPN implementation only supports a single input image, so all\n    # batch inds are 0\n    blob = np.hstack((scores.astype(np.float32, copy=False), proposals.astype(np.float32, copy=False)))\n\n    return blob, bbox_deltas\n\n\ndef _filter_boxes(boxes, min_size):\n    """"""Remove all boxes with any side smaller than min_size.""""""\n    ws = boxes[:, 2] - boxes[:, 0] + 1\n    hs = boxes[:, 3] - boxes[:, 1] + 1\n    keep = np.where((ws >= min_size) & (hs >= min_size))[0]\n    return keep\n\n\ndef _filter_irregular_boxes(boxes, min_ratio=0.2, max_ratio=5):\n    """"""Remove all boxes with any side smaller than min_size.""""""\n    ws = boxes[:, 2] - boxes[:, 0] + 1\n    hs = boxes[:, 3] - boxes[:, 1] + 1\n    rs = ws / hs\n    keep = np.where((rs <= max_ratio) & (rs >= min_ratio))[0]\n    return keep\n'"
utils/text_connector/__init__.py,0,b''
utils/text_connector/detectors.py,0,"b'# coding:utf-8\nimport numpy as np\nfrom utils.bbox.nms import nms\n\nfrom .text_connect_cfg import Config as TextLineCfg\nfrom .text_proposal_connector import TextProposalConnector\nfrom .text_proposal_connector_oriented import TextProposalConnector as TextProposalConnectorOriented\n\n\nclass TextDetector:\n    def __init__(self, DETECT_MODE=""H""):\n        self.mode = DETECT_MODE\n        if self.mode == ""H"":\n            self.text_proposal_connector = TextProposalConnector()\n        elif self.mode == ""O"":\n            self.text_proposal_connector = TextProposalConnectorOriented()\n\n    def detect(self, text_proposals, scores, size):\n        # \xe5\x88\xa0\xe9\x99\xa4\xe5\xbe\x97\xe5\x88\x86\xe8\xbe\x83\xe4\xbd\x8e\xe7\x9a\x84proposal\n        keep_inds = np.where(scores > TextLineCfg.TEXT_PROPOSALS_MIN_SCORE)[0]\n        text_proposals, scores = text_proposals[keep_inds], scores[keep_inds]\n\n        # \xe6\x8c\x89\xe5\xbe\x97\xe5\x88\x86\xe6\x8e\x92\xe5\xba\x8f\n        sorted_indices = np.argsort(scores.ravel())[::-1]\n        text_proposals, scores = text_proposals[sorted_indices], scores[sorted_indices]\n\n        # \xe5\xaf\xb9proposal\xe5\x81\x9anms\n        keep_inds = nms(np.hstack((text_proposals, scores)), TextLineCfg.TEXT_PROPOSALS_NMS_THRESH)\n        text_proposals, scores = text_proposals[keep_inds], scores[keep_inds]\n\n        # \xe8\x8e\xb7\xe5\x8f\x96\xe6\xa3\x80\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\n        text_recs = self.text_proposal_connector.get_text_lines(text_proposals, scores, size)\n        keep_inds = self.filter_boxes(text_recs)\n        return text_recs[keep_inds]\n\n    def filter_boxes(self, boxes):\n        heights = np.zeros((len(boxes), 1), np.float)\n        widths = np.zeros((len(boxes), 1), np.float)\n        scores = np.zeros((len(boxes), 1), np.float)\n        index = 0\n        for box in boxes:\n            heights[index] = (abs(box[5] - box[1]) + abs(box[7] - box[3])) / 2.0 + 1\n            widths[index] = (abs(box[2] - box[0]) + abs(box[6] - box[4])) / 2.0 + 1\n            scores[index] = box[8]\n            index += 1\n\n        return np.where((widths / heights > TextLineCfg.MIN_RATIO) & (scores > TextLineCfg.LINE_MIN_SCORE) &\n                        (widths > (TextLineCfg.TEXT_PROPOSALS_WIDTH * TextLineCfg.MIN_NUM_PROPOSALS)))[0]\n'"
utils/text_connector/other.py,0,"b'import numpy as np\n\n\ndef threshold(coords, min_, max_):\n    return np.maximum(np.minimum(coords, max_), min_)\n\n\ndef clip_boxes(boxes, im_shape):\n    """"""\n    Clip boxes to image boundaries.\n    """"""\n    boxes[:, 0::2] = threshold(boxes[:, 0::2], 0, im_shape[1] - 1)\n    boxes[:, 1::2] = threshold(boxes[:, 1::2], 0, im_shape[0] - 1)\n    return boxes\n\n\nclass Graph:\n    def __init__(self, graph):\n        self.graph = graph\n\n    def sub_graphs_connected(self):\n        sub_graphs = []\n        for index in range(self.graph.shape[0]):\n            if not self.graph[:, index].any() and self.graph[index, :].any():\n                v = index\n                sub_graphs.append([v])\n                while self.graph[v, :].any():\n                    v = np.where(self.graph[v, :])[0][0]\n                    sub_graphs[-1].append(v)\n        return sub_graphs\n'"
utils/text_connector/text_connect_cfg.py,0,b'class Config:\n    MAX_HORIZONTAL_GAP = 50\n    TEXT_PROPOSALS_MIN_SCORE = 0.7\n    TEXT_PROPOSALS_NMS_THRESH = 0.2\n    MIN_V_OVERLAPS = 0.7\n    MIN_SIZE_SIM = 0.7\n    MIN_RATIO = 0.5\n    LINE_MIN_SCORE = 0.9\n    TEXT_PROPOSALS_WIDTH = 16\n    MIN_NUM_PROPOSALS = 2\n'
utils/text_connector/text_proposal_connector.py,0,"b'import numpy as np\n\nfrom utils.text_connector.other import clip_boxes\nfrom utils.text_connector.text_proposal_graph_builder import TextProposalGraphBuilder\n\n\nclass TextProposalConnector:\n    def __init__(self):\n        self.graph_builder = TextProposalGraphBuilder()\n\n    def group_text_proposals(self, text_proposals, scores, im_size):\n        graph = self.graph_builder.build_graph(text_proposals, scores, im_size)\n        return graph.sub_graphs_connected()\n\n    def fit_y(self, X, Y, x1, x2):\n        len(X) != 0\n        # if X only include one point, the function will get line y=Y[0]\n        if np.sum(X == X[0]) == len(X):\n            return Y[0], Y[0]\n        p = np.poly1d(np.polyfit(X, Y, 1))\n        return p(x1), p(x2)\n\n    def get_text_lines(self, text_proposals, scores, im_size):\n        # tp=text proposal\n        tp_groups = self.group_text_proposals(text_proposals, scores, im_size)\n        text_lines = np.zeros((len(tp_groups), 5), np.float32)\n\n        for index, tp_indices in enumerate(tp_groups):\n            text_line_boxes = text_proposals[list(tp_indices)]\n\n            x0 = np.min(text_line_boxes[:, 0])\n            x1 = np.max(text_line_boxes[:, 2])\n\n            offset = (text_line_boxes[0, 2] - text_line_boxes[0, 0]) * 0.5\n\n            lt_y, rt_y = self.fit_y(text_line_boxes[:, 0], text_line_boxes[:, 1], x0 + offset, x1 - offset)\n            lb_y, rb_y = self.fit_y(text_line_boxes[:, 0], text_line_boxes[:, 3], x0 + offset, x1 - offset)\n\n            # the score of a text line is the average score of the scores\n            # of all text proposals contained in the text line\n            score = scores[list(tp_indices)].sum() / float(len(tp_indices))\n\n            text_lines[index, 0] = x0\n            text_lines[index, 1] = min(lt_y, rt_y)\n            text_lines[index, 2] = x1\n            text_lines[index, 3] = max(lb_y, rb_y)\n            text_lines[index, 4] = score\n\n        text_lines = clip_boxes(text_lines, im_size)\n\n        text_recs = np.zeros((len(text_lines), 9), np.float)\n        index = 0\n        for line in text_lines:\n            xmin, ymin, xmax, ymax = line[0], line[1], line[2], line[3]\n            text_recs[index, 0] = xmin\n            text_recs[index, 1] = ymin\n            text_recs[index, 2] = xmax\n            text_recs[index, 3] = ymin\n            text_recs[index, 4] = xmax\n            text_recs[index, 5] = ymax\n            text_recs[index, 6] = xmin\n            text_recs[index, 7] = ymax\n            text_recs[index, 8] = line[4]\n            index = index + 1\n\n        return text_recs\n'"
utils/text_connector/text_proposal_connector_oriented.py,0,"b'# coding:utf-8\nimport numpy as np\n\nfrom utils.text_connector.text_proposal_graph_builder import TextProposalGraphBuilder\n\n\nclass TextProposalConnector:\n    """"""\n        Connect text proposals into text lines\n    """"""\n\n    def __init__(self):\n        self.graph_builder = TextProposalGraphBuilder()\n\n    def group_text_proposals(self, text_proposals, scores, im_size):\n        graph = self.graph_builder.build_graph(text_proposals, scores, im_size)\n        return graph.sub_graphs_connected()\n\n    def fit_y(self, X, Y, x1, x2):\n        len(X) != 0\n        # if X only include one point, the function will get line y=Y[0]\n        if np.sum(X == X[0]) == len(X):\n            return Y[0], Y[0]\n        p = np.poly1d(np.polyfit(X, Y, 1))\n        return p(x1), p(x2)\n\n    def get_text_lines(self, text_proposals, scores, im_size):\n        """"""\n        text_proposals:boxes\n        \n        """"""\n        # tp=text proposal\n        tp_groups = self.group_text_proposals(text_proposals, scores, im_size)  # \xe9\xa6\x96\xe5\x85\x88\xe8\xbf\x98\xe6\x98\xaf\xe5\xbb\xba\xe5\x9b\xbe\xef\xbc\x8c\xe8\x8e\xb7\xe5\x8f\x96\xe5\x88\xb0\xe6\x96\x87\xe6\x9c\xac\xe8\xa1\x8c\xe7\x94\xb1\xe5\x93\xaa\xe5\x87\xa0\xe4\xb8\xaa\xe5\xb0\x8f\xe6\xa1\x86\xe6\x9e\x84\xe6\x88\x90\n\n        text_lines = np.zeros((len(tp_groups), 8), np.float32)\n\n        for index, tp_indices in enumerate(tp_groups):\n            text_line_boxes = text_proposals[list(tp_indices)]  # \xe6\xaf\x8f\xe4\xb8\xaa\xe6\x96\x87\xe6\x9c\xac\xe8\xa1\x8c\xe7\x9a\x84\xe5\x85\xa8\xe9\x83\xa8\xe5\xb0\x8f\xe6\xa1\x86\n            X = (text_line_boxes[:, 0] + text_line_boxes[:, 2]) / 2  # \xe6\xb1\x82\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb0\x8f\xe6\xa1\x86\xe7\x9a\x84\xe4\xb8\xad\xe5\xbf\x83x\xef\xbc\x8cy\xe5\x9d\x90\xe6\xa0\x87\n            Y = (text_line_boxes[:, 1] + text_line_boxes[:, 3]) / 2\n\n            z1 = np.polyfit(X, Y, 1)  # \xe5\xa4\x9a\xe9\xa1\xb9\xe5\xbc\x8f\xe6\x8b\x9f\xe5\x90\x88\xef\xbc\x8c\xe6\xa0\xb9\xe6\x8d\xae\xe4\xb9\x8b\xe5\x89\x8d\xe6\xb1\x82\xe7\x9a\x84\xe4\xb8\xad\xe5\xbf\x83\xe5\xba\x97\xe6\x8b\x9f\xe5\x90\x88\xe4\xb8\x80\xe6\x9d\xa1\xe7\x9b\xb4\xe7\xba\xbf\xef\xbc\x88\xe6\x9c\x80\xe5\xb0\x8f\xe4\xba\x8c\xe4\xb9\x98\xef\xbc\x89\n\n            x0 = np.min(text_line_boxes[:, 0])  # \xe6\x96\x87\xe6\x9c\xac\xe8\xa1\x8cx\xe5\x9d\x90\xe6\xa0\x87\xe6\x9c\x80\xe5\xb0\x8f\xe5\x80\xbc\n            x1 = np.max(text_line_boxes[:, 2])  # \xe6\x96\x87\xe6\x9c\xac\xe8\xa1\x8cx\xe5\x9d\x90\xe6\xa0\x87\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\n\n            offset = (text_line_boxes[0, 2] - text_line_boxes[0, 0]) * 0.5  # \xe5\xb0\x8f\xe6\xa1\x86\xe5\xae\xbd\xe5\xba\xa6\xe7\x9a\x84\xe4\xb8\x80\xe5\x8d\x8a\n\n            # \xe4\xbb\xa5\xe5\x85\xa8\xe9\x83\xa8\xe5\xb0\x8f\xe6\xa1\x86\xe7\x9a\x84\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xe8\xbf\x99\xe4\xb8\xaa\xe7\x82\xb9\xe5\x8e\xbb\xe6\x8b\x9f\xe5\x90\x88\xe4\xb8\x80\xe6\x9d\xa1\xe7\x9b\xb4\xe7\xba\xbf\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x80\xe4\xb8\x8b\xe6\x96\x87\xe6\x9c\xac\xe8\xa1\x8cx\xe5\x9d\x90\xe6\xa0\x87\xe7\x9a\x84\xe6\x9e\x81\xe5\xb7\xa6\xe6\x9e\x81\xe5\x8f\xb3\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84y\xe5\x9d\x90\xe6\xa0\x87\n            lt_y, rt_y = self.fit_y(text_line_boxes[:, 0], text_line_boxes[:, 1], x0 + offset, x1 - offset)\n            # \xe4\xbb\xa5\xe5\x85\xa8\xe9\x83\xa8\xe5\xb0\x8f\xe6\xa1\x86\xe7\x9a\x84\xe5\xb7\xa6\xe4\xb8\x8b\xe8\xa7\x92\xe8\xbf\x99\xe4\xb8\xaa\xe7\x82\xb9\xe5\x8e\xbb\xe6\x8b\x9f\xe5\x90\x88\xe4\xb8\x80\xe6\x9d\xa1\xe7\x9b\xb4\xe7\xba\xbf\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x80\xe4\xb8\x8b\xe6\x96\x87\xe6\x9c\xac\xe8\xa1\x8cx\xe5\x9d\x90\xe6\xa0\x87\xe7\x9a\x84\xe6\x9e\x81\xe5\xb7\xa6\xe6\x9e\x81\xe5\x8f\xb3\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84y\xe5\x9d\x90\xe6\xa0\x87\n            lb_y, rb_y = self.fit_y(text_line_boxes[:, 0], text_line_boxes[:, 3], x0 + offset, x1 - offset)\n\n            score = scores[list(tp_indices)].sum() / float(len(tp_indices))  # \xe6\xb1\x82\xe5\x85\xa8\xe9\x83\xa8\xe5\xb0\x8f\xe6\xa1\x86\xe5\xbe\x97\xe5\x88\x86\xe7\x9a\x84\xe5\x9d\x87\xe5\x80\xbc\xe4\xbd\x9c\xe4\xb8\xba\xe6\x96\x87\xe6\x9c\xac\xe8\xa1\x8c\xe7\x9a\x84\xe5\x9d\x87\xe5\x80\xbc\n\n            text_lines[index, 0] = x0\n            text_lines[index, 1] = min(lt_y, rt_y)  # \xe6\x96\x87\xe6\x9c\xac\xe8\xa1\x8c\xe4\xb8\x8a\xe7\xab\xaf \xe7\xba\xbf\xe6\xae\xb5 \xe7\x9a\x84y\xe5\x9d\x90\xe6\xa0\x87\xe7\x9a\x84\xe5\xb0\x8f\xe5\x80\xbc\n            text_lines[index, 2] = x1\n            text_lines[index, 3] = max(lb_y, rb_y)  # \xe6\x96\x87\xe6\x9c\xac\xe8\xa1\x8c\xe4\xb8\x8b\xe7\xab\xaf \xe7\xba\xbf\xe6\xae\xb5 \xe7\x9a\x84y\xe5\x9d\x90\xe6\xa0\x87\xe7\x9a\x84\xe5\xa4\xa7\xe5\x80\xbc\n            text_lines[index, 4] = score  # \xe6\x96\x87\xe6\x9c\xac\xe8\xa1\x8c\xe5\xbe\x97\xe5\x88\x86\n            text_lines[index, 5] = z1[0]  # \xe6\xa0\xb9\xe6\x8d\xae\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe6\x8b\x9f\xe5\x90\x88\xe7\x9a\x84\xe7\x9b\xb4\xe7\xba\xbf\xe7\x9a\x84k\xef\xbc\x8cb\n            text_lines[index, 6] = z1[1]\n            height = np.mean((text_line_boxes[:, 3] - text_line_boxes[:, 1]))  # \xe5\xb0\x8f\xe6\xa1\x86\xe5\xb9\xb3\xe5\x9d\x87\xe9\xab\x98\xe5\xba\xa6\n            text_lines[index, 7] = height + 2.5\n\n        text_recs = np.zeros((len(text_lines), 9), np.float)\n        index = 0\n        for line in text_lines:\n            b1 = line[6] - line[7] / 2  # \xe6\xa0\xb9\xe6\x8d\xae\xe9\xab\x98\xe5\xba\xa6\xe5\x92\x8c\xe6\x96\x87\xe6\x9c\xac\xe8\xa1\x8c\xe4\xb8\xad\xe5\xbf\x83\xe7\xba\xbf\xef\xbc\x8c\xe6\xb1\x82\xe5\x8f\x96\xe6\x96\x87\xe6\x9c\xac\xe8\xa1\x8c\xe4\xb8\x8a\xe4\xb8\x8b\xe4\xb8\xa4\xe6\x9d\xa1\xe7\xba\xbf\xe7\x9a\x84b\xe5\x80\xbc\n            b2 = line[6] + line[7] / 2\n            x1 = line[0]\n            y1 = line[5] * line[0] + b1  # \xe5\xb7\xa6\xe4\xb8\x8a\n            x2 = line[2]\n            y2 = line[5] * line[2] + b1  # \xe5\x8f\xb3\xe4\xb8\x8a\n            x3 = line[0]\n            y3 = line[5] * line[0] + b2  # \xe5\xb7\xa6\xe4\xb8\x8b\n            x4 = line[2]\n            y4 = line[5] * line[2] + b2  # \xe5\x8f\xb3\xe4\xb8\x8b\n            disX = x2 - x1\n            disY = y2 - y1\n            width = np.sqrt(disX * disX + disY * disY)  # \xe6\x96\x87\xe6\x9c\xac\xe8\xa1\x8c\xe5\xae\xbd\xe5\xba\xa6\n\n            fTmp0 = y3 - y1  # \xe6\x96\x87\xe6\x9c\xac\xe8\xa1\x8c\xe9\xab\x98\xe5\xba\xa6\n            fTmp1 = fTmp0 * disY / width\n            x = np.fabs(fTmp1 * disX / width)  # \xe5\x81\x9a\xe8\xa1\xa5\xe5\x81\xbf\n            y = np.fabs(fTmp1 * disY / width)\n            if line[5] < 0:\n                x1 -= x\n                y1 += y\n                x4 += x\n                y4 -= y\n            else:\n                x2 += x\n                y2 += y\n                x3 -= x\n                y3 -= y\n            text_recs[index, 0] = x1\n            text_recs[index, 1] = y1\n            text_recs[index, 2] = x2\n            text_recs[index, 3] = y2\n            text_recs[index, 4] = x4\n            text_recs[index, 5] = y4\n            text_recs[index, 6] = x3\n            text_recs[index, 7] = y3\n            text_recs[index, 8] = line[4]\n            index = index + 1\n\n        return text_recs\n'"
utils/text_connector/text_proposal_graph_builder.py,0,"b'import numpy as np\n\nfrom utils.text_connector.other import Graph\nfrom utils.text_connector.text_connect_cfg import Config as TextLineCfg\n\n\nclass TextProposalGraphBuilder:\n    """"""\n        Build Text proposals into a graph.\n    """"""\n\n    def get_successions(self, index):\n        box = self.text_proposals[index]\n        results = []\n        for left in range(int(box[0]) + 1, min(int(box[0]) + TextLineCfg.MAX_HORIZONTAL_GAP + 1, self.im_size[1])):\n            adj_box_indices = self.boxes_table[left]\n            for adj_box_index in adj_box_indices:\n                if self.meet_v_iou(adj_box_index, index):\n                    results.append(adj_box_index)\n            if len(results) != 0:\n                return results\n        return results\n\n    def get_precursors(self, index):\n        box = self.text_proposals[index]\n        results = []\n        for left in range(int(box[0]) - 1, max(int(box[0] - TextLineCfg.MAX_HORIZONTAL_GAP), 0) - 1, -1):\n            adj_box_indices = self.boxes_table[left]\n            for adj_box_index in adj_box_indices:\n                if self.meet_v_iou(adj_box_index, index):\n                    results.append(adj_box_index)\n            if len(results) != 0:\n                return results\n        return results\n\n    def is_succession_node(self, index, succession_index):\n        precursors = self.get_precursors(succession_index)\n        if self.scores[index] >= np.max(self.scores[precursors]):\n            return True\n        return False\n\n    def meet_v_iou(self, index1, index2):\n        def overlaps_v(index1, index2):\n            h1 = self.heights[index1]\n            h2 = self.heights[index2]\n            y0 = max(self.text_proposals[index2][1], self.text_proposals[index1][1])\n            y1 = min(self.text_proposals[index2][3], self.text_proposals[index1][3])\n            return max(0, y1 - y0 + 1) / min(h1, h2)\n\n        def size_similarity(index1, index2):\n            h1 = self.heights[index1]\n            h2 = self.heights[index2]\n            return min(h1, h2) / max(h1, h2)\n\n        return overlaps_v(index1, index2) >= TextLineCfg.MIN_V_OVERLAPS and \\\n               size_similarity(index1, index2) >= TextLineCfg.MIN_SIZE_SIM\n\n    def build_graph(self, text_proposals, scores, im_size):\n        self.text_proposals = text_proposals\n        self.scores = scores\n        self.im_size = im_size\n        self.heights = text_proposals[:, 3] - text_proposals[:, 1] + 1\n\n        boxes_table = [[] for _ in range(self.im_size[1])]\n        for index, box in enumerate(text_proposals):\n            boxes_table[int(box[0])].append(index)\n        self.boxes_table = boxes_table\n\n        graph = np.zeros((text_proposals.shape[0], text_proposals.shape[0]), np.bool)\n\n        for index, box in enumerate(text_proposals):\n            successions = self.get_successions(index)\n            if len(successions) == 0:\n                continue\n            succession_index = successions[np.argmax(scores[successions])]\n            if self.is_succession_node(index, succession_index):\n                # NOTE: a box can have multiple successions(precursors) if multiple successions(precursors)\n                # have equal scores.\n                graph[index, succession_index] = True\n        return Graph(graph)\n'"
