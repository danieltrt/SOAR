file_path,api_count,code
A3C_class.py,36,"b'# -*- coding: utf-8 -*-\nfrom configs import EXTRA_DENSE, N_HIDDEN, DROPOUT, COOL_V, COOL_A, dep, gamma, training\n\n# \xd0\xbc\xd0\xb0\xd0\xb3\xd0\xb8\xd1\x87\xd0\xb5\xd1\x81\xd0\xba\xd0\xb0\xd1\x8f \xd0\xba\xd0\xbe\xd0\xbd\xd1\x81\xd1\x82\xd0\xb0\xd0\xbd\xd1\x82\xd0\xb0 37 - \xd1\x87\xd0\xb8\xd1\x81\xd0\xbb\xd0\xbe \xd1\x84\xd0\xb8\xd1\x87\ns_size = 38 * dep\na_size = 3\nmodel_path = \'model\'\nimport os\nimport pandas as pd\nimport numpy as np\nimport scipy\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n# %matplotlib inline\nfrom random import choice\nfrom trader_gym import environment\n# %load_ext ipycache\n\n\ndef update_target_graph(from_scope, to_scope):\n    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n\n    op_holder = []\n    for from_var, to_var in zip(from_vars, to_vars):\n        op_holder.append(to_var.assign(from_var))\n    return op_holder\n\n\ndef discount(x, gamma):\n    return scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n\n\ndef normalized_columns_initializer(std=1.0):\n    def _initializer(shape, dtype=None, partition_info=None):\n        out = np.random.randn(*shape).astype(np.float32)\n        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n        return tf.constant(out)\n    return _initializer\n\n\nclass AC_Network():\n    def __init__(self, s_size, a_size, scope, trainer):\n        with tf.variable_scope(scope):\n            self.inputs = tf.placeholder(shape=[None, s_size], dtype=tf.float32)\n            self.imageIn = tf.reshape(self.inputs, shape=[-1, s_size])\n            if(EXTRA_DENSE):\n                hidden = slim.fully_connected(slim.flatten(self.imageIn), N_HIDDEN, activation_fn=tf.nn.tanh)\n            else:\n                hidden = slim.flatten(self.imageIn)\n\n            if(DROPOUT):\n                rnn_in = tf.layers.dropout(\n                    hidden,\n                    rate=0.5,\n                    noise_shape=None,\n                    seed=None,\n                    training=training,\n                    name=\'drop1\')\n            else:\n                rnn_in = hidden\n\n            lstm_cell = tf.contrib.rnn.BasicLSTMCell(N_HIDDEN, state_is_tuple=True)\n            c_init = np.zeros((1, lstm_cell.state_size.c), np.float32)\n            h_init = np.zeros((1, lstm_cell.state_size.h), np.float32)\n            self.state_init = [c_init, h_init]\n            c_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.c])\n            h_in = tf.placeholder(tf.float32, [1, lstm_cell.state_size.h])\n            self.state_in = (c_in, h_in)\n            rnn_in = tf.expand_dims(rnn_in, [0])\n            step_size = tf.shape(self.imageIn)[:1]\n            state_in = tf.contrib.rnn.LSTMStateTuple(c_in, h_in)\n            lstm_outputs, lstm_state = tf.nn.dynamic_rnn(\n                lstm_cell, rnn_in, initial_state=state_in, sequence_length=step_size,\n                time_major=False)\n            lstm_c, lstm_h = lstm_state\n            self.state_out = (lstm_c[:1, :], lstm_h[:1, :])\n            rnn_out = tf.reshape(lstm_outputs, [-1, N_HIDDEN])\n\n            if(COOL_A):\n                a_in = slim.fully_connected(slim.flatten(rnn_out), 32, activation_fn=tf.nn.tanh)\n            else:\n                a_in = rnn_out\n\n            self.policy = slim.fully_connected(a_in, a_size,\n                                               activation_fn=tf.nn.softmax,\n                                               weights_initializer=normalized_columns_initializer(0.01),\n                                               biases_initializer=None)\n            if(COOL_V):\n                v_in = slim.fully_connected(slim.flatten(rnn_out), 32, activation_fn=tf.nn.tanh)\n            else:\n                v_in = rnn_out\n\n            self.value = slim.fully_connected(v_in, 1,\n                                              activation_fn=None,\n                                              weights_initializer=normalized_columns_initializer(0.01),\n                                              biases_initializer=None)\n\n            if scope != \'global\':\n                self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n                self.actions_onehot = tf.one_hot(self.actions, a_size, dtype=tf.float32)\n                self.target_v = tf.placeholder(shape=[None], dtype=tf.float32)\n                self.advantages = tf.placeholder(shape=[None], dtype=tf.float32)\n                self.responsible_outputs = tf.reduce_sum(self.policy * self.actions_onehot, [1])\n\n                self.value_loss = 0.5 * tf.reduce_sum(tf.square(self.target_v - tf.reshape(self.value, [-1])))\n                self.entropy = - tf.reduce_sum(self.policy * tf.log(self.policy))\n                self.policy_loss = -tf.reduce_sum(tf.log(self.responsible_outputs) * self.advantages)\n                self.loss = 0.5 * self.value_loss + self.policy_loss - self.entropy * 0.01\n\n                local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n                self.gradients = tf.gradients(self.loss, local_vars)\n                self.var_norms = tf.global_norm(local_vars)\n                grads, self.grad_norms = tf.clip_by_global_norm(self.gradients, 40.0)\n\n                global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \'global\')\n                self.apply_grads = trainer.apply_gradients(zip(grads, global_vars))\n\n\nclass Worker():\n    def __init__(self, env, name, s_size, a_size, trainer, model_path, global_episodes):\n        self.name = ""worker_"" + str(name)\n        self.number = name\n        self.model_path = model_path\n        self.trainer = trainer\n        self.global_episodes = global_episodes\n        self.increment = self.global_episodes.assign_add(1)\n        self.episode_rewards = []\n        self.episode_lengths = []\n        self.episode_mean_values = []\n        self.summary_writer = tf.summary.FileWriter(\'tb/train_\' + str(self.number))\n\n        self.local_AC = AC_Network(s_size, a_size, self.name, trainer)\n        self.update_local_ops = update_target_graph(\'global\', self.name)\n        self.env = env\n        self.actions = [-1, 0, 1]\n\n    def train(self, rollout, sess, gamma, bootstrap_value):\n        rollout = np.array(rollout)\n        observations = rollout[:, 0]\n        actions = rollout[:, 1]\n        rewards = rollout[:, 2]\n        next_observations = rollout[:, 3]\n        values = rollout[:, 5]\n\n        self.rewards_plus = np.asarray(rewards.tolist() + [bootstrap_value])\n        discounted_rewards = discount(self.rewards_plus, gamma)[:-1]\n        self.value_plus = np.asarray(values.tolist() + [bootstrap_value])\n        advantages = rewards + gamma * self.value_plus[1:] - self.value_plus[:-1]\n        advantages = discount(advantages, gamma)\n        # \xd0\xbb\xd0\xbe\xd0\xbb, \xd1\x8d\xd1\x82\xd0\xbe \xd1\x80\xd0\xb0\xd0\xb2\xd0\xbd\xd0\xbe discounted_rewaeds - sefl.value_plus\n\n        rnn_state = self.local_AC.state_init\n        feed_dict = {self.local_AC.target_v: discounted_rewards,\n                     self.local_AC.inputs: np.vstack(observations),\n                     self.local_AC.actions: actions,\n                     self.local_AC.advantages: advantages,\n                     self.local_AC.state_in[0]: rnn_state[0],\n                     self.local_AC.state_in[1]: rnn_state[1]}\n        v_l, p_l, e_l, g_n, v_n, _ = sess.run([self.local_AC.value_loss,\n                                               self.local_AC.policy_loss,\n                                               self.local_AC.entropy,\n                                               self.local_AC.grad_norms,\n                                               self.local_AC.var_norms,\n                                               self.local_AC.apply_grads],\n                                              feed_dict=feed_dict)\n        return v_l / len(rollout), p_l / len(rollout), e_l / len(rollout), g_n, v_n\n\n    def work(self, max_episode_length, gamma, sess, coord, saver, dep):\n        episode_count = sess.run(self.global_episodes)\n        total_steps = 0\n        print(""Starting worker "" + str(self.number))\n        with sess.as_default(), sess.graph.as_default():\n            while not coord.should_stop():\n                sess.run(self.update_local_ops)\n                episode_buffer = []\n                episode_values = []\n                episode_frames = []\n                action_buffer = [0] * dep\n                episode_reward = 0\n                episode_step_count = 0\n                d = False\n                s = self.env.reset()\n                s = np.concatenate((s, action_buffer))\n                episode_frames.append(s)\n                rnn_state = self.local_AC.state_init\n                summary = tf.Summary()\n\n                while d == False:\n                    a_dist, v, rnn_state = sess.run([self.local_AC.policy, self.local_AC.value, self.local_AC.state_out],\n                                                    feed_dict={self.local_AC.inputs: [s],\n                                                               self.local_AC.state_in[0]: rnn_state[0],\n                                                               self.local_AC.state_in[1]: rnn_state[1]})\n                    a = np.random.choice(a_dist[0], p=a_dist[0])\n                    a = np.argmax(a_dist == a)\n                    s1, r, d, _ = self.env.step(self.actions[a])\n                    # \xd0\x94\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xb8\xd0\xbc \xd0\xbd\xd0\xbe\xd0\xb2\xd0\xbe\xd0\xb5 \xd0\xb4\xd0\xb5\xd0\xb9\xd1\x81\xd1\x82\xd0\xb2\xd0\xb8\xd0\xb5 \xd0\xb2 \xd0\xba\xd0\xbe\xd0\xbd\xd0\xb5\xd1\x86 \xd0\xb1\xd1\x83\xd1\x84\xd0\xb5\xd1\x80\xd0\xb0\n                    action_buffer[:] = np.concatenate((action_buffer[1:], [self.actions[a]]))\n                    s1 = np.concatenate((s1, action_buffer))\n                    if not d:\n                        episode_frames.append(s1)\n                    else:\n                        s1 = s\n\n                    episode_buffer.append([s, a, r, s1, d, v[0, 0]])\n                    episode_values.append(v[0, 0])\n                    episode_reward += r\n                    s = s1\n                    total_steps += 1\n                    episode_step_count += 1\n#                     Save history of boss actions\n\n                    if len(episode_buffer) == 200 and d != True and episode_step_count != max_episode_length - 1:\n                        v1 = sess.run(self.local_AC.value,\n                                      feed_dict={self.local_AC.inputs: [s],\n                                                 self.local_AC.state_in[0]: rnn_state[0],\n                                                 self.local_AC.state_in[1]: rnn_state[1]})[0, 0]\n                        v_l, p_l, e_l, g_n, v_n = self.train(episode_buffer, sess, gamma, v1)\n                        episode_buffer = []\n                        sess.run(self.update_local_ops)\n                    if d:\n                        break\n\n                self.episode_rewards.append(episode_reward)\n                self.episode_lengths.append(episode_step_count)\n                self.episode_mean_values.append(np.mean(episode_values))\n\n                if len(episode_buffer) != 0:\n                    v_l, p_l, e_l, g_n, v_n = self.train(episode_buffer, sess, gamma, 0.0)\n\n                if episode_count % 50 == 0 and self.name == \'worker_0\':\n                    saver.save(sess, self.model_path + \'/model-\' + str(episode_count) + \'.cptk\')\n                    print(""Saved Model"")\n\n                mean_reward = np.mean(self.episode_rewards[-5:])\n                mean_length = np.mean(self.episode_lengths[-5:])\n                mean_value = np.mean(self.episode_mean_values[-5:])\n\n                summary.value.add(tag=\'env/shares\', simple_value=float(self.env.n_shares))\n                summary.value.add(tag=\'Perf/Act\', simple_value=float(a))\n                summary.value.add(tag=\'Perf/Episode_reward\', simple_value=float(episode_reward))\n                summary.value.add(tag=\'Perf/Reward\', simple_value=float(mean_reward))\n                summary.value.add(tag=\'Perf/Length\', simple_value=float(mean_length))\n                summary.value.add(tag=\'Perf/Value\', simple_value=float(mean_value))\n                summary.value.add(tag=\'Losses/Value Loss\', simple_value=float(v_l))\n                summary.value.add(tag=\'Losses/Policy Loss\', simple_value=float(p_l))\n                summary.value.add(tag=\'Losses/Entropy\', simple_value=float(e_l))\n                summary.value.add(tag=\'Losses/Grad Norm\', simple_value=float(g_n))\n                summary.value.add(tag=\'Losses/Var Norm\', simple_value=float(v_n))\n                self.summary_writer.add_summary(summary, episode_count)\n                self.summary_writer.flush()\n                if self.name == \'worker_0\':\n                    sess.run(self.increment)\n                episode_count += 1\n                print(episode_count, episode_reward)\n\n\nclass Test_Worker():\n    def __init__(self, env, name, s_size, a_size, trainer, model_path, global_episodes):\n        self.name = ""worker_"" + str(name)\n        self.number = name\n        self.model_path = model_path\n        self.trainer = trainer\n        self.global_episodes = global_episodes\n        self.increment = self.global_episodes.assign_add(1)\n        self.acts = []\n        self.rewards = []\n        self.summary_writer = tf.summary.FileWriter(\'tb/train_\' + str(self.number))\n        self.local_AC = AC_Network(s_size, a_size, self.name, trainer)\n        self.update_local_ops = update_target_graph(\'global\', self.name)\n        self.env = env\n        self.actions = [-1, 0, 1]\n        self.prev_act = 1\n\n    def work(self, max_episode_length, gamma, sess, coord, saver, dep, tresh):\n        episode_count = sess.run(self.global_episodes)\n        total_steps = 0\n        print(""Starting worker "" + str(self.number))\n        with sess.as_default(), sess.graph.as_default():\n            sess.run(self.update_local_ops)\n            d = False\n            episode_reward = 0\n            s0 = self.env.reset()\n            action_buffer = [0] * dep\n            s = np.concatenate((s0, action_buffer))\n            rnn_state = self.local_AC.state_init\n            summary = tf.Summary()\n\n            while d == False:\n                a_dist, v, rnn_state = sess.run([self.local_AC.policy, self.local_AC.value, self.local_AC.state_out],\n                                                feed_dict={self.local_AC.inputs: [s],\n                                                           self.local_AC.state_in[0]: rnn_state[0],\n                                                           self.local_AC.state_in[1]: rnn_state[1]})\n                # print(""A"",a_dist[0], a_dist)\n                p = np.amax(a_dist[0])\n                a = np.argmax(a_dist[0])\n                # print(\'argmax\',p,\'max\',a)\n                if(p < tresh):\n                    a = self.prev_act\n                self.prev_act = a\n                # print(a)\n                # a = np.random.choice(a_dist[0],p=a_dist[0])\n                # print(""RC"",a)\n                # a = np.argmax(a_dist == a)\n                # print(""ARGMAX"",a)\n                s1, r, d, _ = self.env.step(self.actions[a])\n                action_buffer[:] = np.concatenate((action_buffer[1:], [self.actions[a]]))\n                s1 = np.concatenate((s1, action_buffer))\n                if d:\n                    s1 = s\n                total_steps += 1\n                episode_reward += r\n                s = s1\n                self.acts.append(self.actions[a])\n                self.rewards.append(r)\n\n            self.episode_reward = np.cumsum(self.rewards)\n            # for i in range(len(self.actions)):\n            #     summary.value.add(tag=\'test/a\', simple_value=float(self.actions[i]))\n            #     summary.value.add(tag=\'test/r\', simple_value=float(self.rewards[i]))\n            #     summary.value.add(tag=\'test/equity\', simple_value=float(self.episode_reward[i]))\n            #     self.summary_writer.add_summary(summary, i)\n            #     self.summary_writer.flush()\n            if self.name == \'worker_0\':\n                sess.run(self.increment)\n            episode_count += 1\n            print(total_steps, episode_reward)\n'"
A3C_trading.py,11,"b'# -*- coding: utf-8 -*-\nimport os\nimport pandas as pd\nfrom multiprocessing import Pool\nimport warnings\nimport numpy as np\nimport scipy\nimport pybacktest as pb\nimport matplotlib.pyplot as plt\nimport threading\nimport multiprocessing\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport scipy.signal\nfrom random import choice\nfrom time import sleep\nfrom time import time\nimport sys\nfrom trader_gym import environment\nfrom A3C_class import *\nfrom configs import TRAIN_DATA, LOAD_MODEL, LR, FRAMES_STACKED, NUM_WORKERS, MODEL_DIR\nwarnings.filterwarnings(""ignore"")\n\n# \xd0\x95\xd1\x81\xd0\xbb\xd0\xb8 \xd0\xbd\xd0\xb5\xd1\x82 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd0\xb0, \xd1\x82\xd0\xbe \xd0\xb5\xd0\xb3\xd0\xbe \xd0\xbd\xd1\x83\xd0\xb6\xd0\xbd\xd0\xbe \xd1\x81\xd0\xbe\xd0\xb7\xd0\xb4\xd0\xb0\xd1\x82\xd1\x8c \xd1\x81 \xd0\xbf\xd0\xbe\xd0\xbc\xd0\xbe\xd1\x89\xd1\x8c\xd1\x8e load_data.ipynb\n\ntrain_df = pd.read_pickle(TRAIN_DATA)\n\nif FRAMES_STACKED > 1:\n    data = np.hstack([train_df.values[i:-FRAMES_STACKED + i - 1, :] for i in range(FRAMES_STACKED, 0, -1)])\nelse:\n    data = train_df.values\n\ntrain_df = pd.DataFrame(data, train_df[FRAMES_STACKED:-1].index)\nmax_episode_len = train_df.shape[0]\n\ntf.reset_default_graph()\nif not os.path.exists(MODEL_DIR):\n    os.makedirs(MODEL_DIR)\n\nwith tf.device(USE_DEVICE):\n    global_episodes = tf.Variable(0, dtype=tf.int32, name=\'global_episodes\', trainable=False)\n    trainer = tf.train.RMSPropOptimizer(learning_rate=LR, decay=0.99, epsilon=1e-6)\n    master_network = AC_Network(s_size, a_size, \'global\', None)\n    workers = []\n    for i in range(NUM_WORKERS):\n        env = environment(train_df, max_episode_len)\n        workers.append(Worker(env, i, s_size, a_size, trainer, MODEL_DIR, global_episodes))\n    saver = tf.train.Saver(max_to_keep=25)\n\nif \'session\' in locals() and session is not None:\n    print(\'Close interactive session\')\n    session.close()\nif \'sess\' in locals() and sess is not None:\n    print(\'Close interactive session\')\n    sess.close()\n\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0)\n\nwith tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True, gpu_options=gpu_options)) as sess:\n    coord = tf.train.Coordinator()\n    if LOAD_MODEL:\n        print(\'Loading Model...\')\n        ckpt = tf.train.get_checkpoint_state(MODEL_DIR)\n        saver.restore(sess, ckpt.model_checkpoint_path)\n    else:\n        sess.run(tf.global_variables_initializer())\n    summary_writer = tf.summary.FileWriter(""tb/train"", graph=sess.graph)\n    worker_threads = []\n    for worker in workers:\n        def worker_work(): return worker.work(max_episode_len, gamma, sess, coord, saver, FRAMES_STACKED)\n        t = threading.Thread(target=(worker_work))\n        t.start()\n        worker_threads.append(t)\n    coord.join(worker_threads)\n'"
configs.py,0,"b'# Directories\nTENSORBOARD_DIR = \'/mnt/a3c_data/tb/\'\nMODEL_DIR = \'/mnt/a3c_data/model/\'\nDATA_DIR = \'/mnt/a3c_data/data/\'\nPLOTS_DIR = \'/mnt/a3c_data/plots/\'\n\n# Data files\nPOSTFIX = ""[\'RTS-12.15\']w5k""\nTEST_POSTFIX = ""[\'RTS-3.16\', \'RTS-6.16\']w5k""\nPOSTFIX_REAL = ""[\'RTS-12.15\']""\nTEST_POSTFIX_REAL = ""[\'RTS-3.16\', \'RTS-6.16\']""\n\n# Network architechture\nEXTRA_DENSE = False\nN_HIDDEN = 64\nDROPOUT = True\nTRAINING = False\nCOOL_V = True\nCOOL_A = False\nDEP = 1\nGAMMA = .8\nLOAD_MODEL = False\nLR = 1e-4\nCOMISSION = 20  # rubls\nPRICE_MAG = 1 / 5000  # \xd1\x81\xd1\x80\xd0\xb5\xd0\xb4\xd0\xbd\xd0\xbd\xd0\xb5 \xd1\x80\xd0\xb0\xd0\xb7\xd0\xbb\xd0\xb8\xd1\x87\xd0\xb8\xd0\xb5 \xd0\xbc\xd0\xb5\xd0\xb6\xd0\xb4\xd1\x83 \xd0\xbc\xd0\xb0\xd0\xba\xd1\x81 \xd0\xb8 \xd0\xbc\xd0\xb8\xd0\xbd \xd1\x86\xd0\xb5\xd0\xbd\xd0\xbe\xd0\xb9 \xd0\xb7\xd0\xb0 5000 \xd1\x88\xd0\xb0\xd0\xb3\xd0\xbe\xd0\xb2\n'"
test_trading.py,22,"b'# -*- coding: utf-8 -*-\nCUDA_VISIBLE_DEVICES = -1\nimport os\nimport pandas as pd\nfrom multiprocessing import Pool\nimport warnings\nwarnings.filterwarnings(""ignore"")\nimport numpy as np\nimport scipy\nimport matplotlib as mpl\nmpl.use(\'Agg\')\nimport pybacktest as pb\nimport matplotlib.pyplot as plt\n\nimport threading\nimport multiprocessing\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport scipy.signal\nfrom random import choice\nfrom time import sleep\nfrom time import time\nimport sys\nfrom trader_gym import environment\nfrom A3C_class import *\n# %load_ext ipycache\nmax_episode_length = 300\nimport matplotlib\nmatplotlib.rcParams.update({\'font.size\': 21})\nfrom configs import postfix, test_postfix, load_model, LR, dep, test_postfix_real, postfix_real\n\ndata_dir_path = \'/mnt/a3c_data/\'\n\n# data\nprint(data_dir_path + str(test_postfix))\nR = pd.read_pickle(data_dir_path + str(test_postfix))\nR = R[False == R.index.duplicated()]\ntest_R = pd.read_pickle(data_dir_path + str(test_postfix_real))\ntest_R = test_R[False == test_R.index.duplicated()]\n# print (R.mean()[0], R.max()[0], (R.max()[0] - R.min()[0]), (R.max()[0] - R.mean()[0])/(R.max()[0] - R.min()[0]), (R.min()[0] - R.mean()[0])/(R.max()[0] - R.min()[0]))\nR = (R - R.mean()) / (R.max() - R.min())\nold_R = R.copy()\nvals = R.values\nD = np.hstack([vals[i:-dep + i - 1, :] for i in range(dep, 0, -1)])\nR = pd.DataFrame(D, R[dep:-1].index)\n# R = R[:10000]\n\n\n# Main\ndef run_trades(max_episode_length, gamma, s_size, a_size, load_model, model_path, length, env):\n    tf.reset_default_graph()\n    with tf.device(\'/cpu:0\'):\n        global_episodes = tf.Variable(0, dtype=tf.int32, name=\'global_episodes\', trainable=False)\n        trainer = tf.train.RMSPropOptimizer(learning_rate=1e-3, decay=0.99, epsilon=1e-6)\n        master_network = AC_Network(s_size, a_size, \'global\', None)\n        num_workers = 1\n        workers = []\n        for i in range(num_workers):\n            workers.append(Test_Worker(env, i, s_size, a_size, trainer, model_path, global_episodes))\n        saver = tf.train.Saver(max_to_keep=5)\n\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0)\n    if not os.path.exists(model_path):\n        os.makedirs(model_path)\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=False, log_device_placement=False, gpu_options=gpu_options)) as sess:\n        coord = tf.train.Coordinator()\n\n        if load_model:\n            print(\'Loading Model...\')\n            ckpt = tf.train.get_checkpoint_state(model_path)\n            saver.restore(sess, ckpt.model_checkpoint_path)\n        else:\n            sess.run(tf.global_variables_initializer())\n\n        summary_writer = tf.summary.FileWriter(\'tb/train\', graph=sess.graph)\n        worker_threads = []\n        for worker in workers:\n            def worker_work(): return worker.work(max_episode_length, gamma, sess, coord, saver, dep, 0.33)\n            t = threading.Thread(target=(worker_work))\n            t.start()\n            worker_threads.append(t)\n\n        coord.join(worker_threads)\n        acts = workers[0].acts\n        rews = workers[0].rewards\n    return acts, rews\n\n\n# \xd0\xa2\xd0\xbe\xd1\x80\xd0\xb3\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x8f \xd1\x81\xd0\xb8\xd1\x81\xd1\x82\xd0\xb5\xd0\xbc\xd0\xb0 - \xd0\xbf\xd0\xbe\xd0\xba\xd1\x83\xd0\xbf\xd0\xb0\xd0\xb5\xd0\xbc/\xd0\xbf\xd1\x80\xd0\xbe\xd0\xb4\xd0\xb0\xd0\xb5\xd0\xbc \xd0\xb2 \xd0\xb7\xd0\xb0\xd0\xb2\xd0\xb8\xd1\x81\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd0\xb8 \xd0\xbe\xd1\x82 \xd0\xbf\xd1\x80\xd0\xb5\xd0\xb4\xd1\x81\xd0\xba\xd0\xb0\xd0\xb7\xd0\xb0\xd0\xbd\xd0\xbd\xd0\xbe\xd0\xb3\xd0\xbe \xd0\xba\xd0\xbb\xd0\xb0\xd1\x81\xd1\x81\xd0\xb0\ndef backtest(acts, o_R):\n    pacts = np.asarray(acts).copy()\n    for i in range(1, pacts.shape[0]):\n        if pacts[i] == 0:\n            if pacts[i - 1] == -1:\n                # from short\n                pacts[i] = 0.5\n            elif pacts[i - 1] == 1:\n                pacts[i] = -0.5\n\n    # plt.plot(pacts, \'.\')\n    data = o_R[dep - 1:-3]\n\n    buy = cover = pd.Series(pacts == 1, index=data.index)\n    short = sell = pd.Series(pacts == -1, index=data.index)\n    cover = pd.Series(pacts > 0, index=data.index)\n    sell = pd.Series(pacts < 0, index=data.index)\n    # print(np.sum(buy-sell))\n    OHLC = data[[(\'DealPrice\', \'close\'), (\'DealPrice\', \'close\'), (\'DealPrice\', \'close\'), (\'DealPrice\', \'close\')]]\n    OHLC.columns = [\'O\', \'H\', \'L\', \'C\']\n    # \xd0\x9e\xd1\x81\xd0\xbd\xd0\xbe\xd0\xb2\xd0\xbd\xd0\xbe\xd0\xb9 \xd0\xb3\xd1\x80\xd0\xb0\xd1\x84\xd0\xb8\xd0\xba \xd1\x8d\xd0\xba\xd0\xb2\xd0\xb8\xd1\x82\xd0\xb8\n    bt = pb.Backtest(locals())\n    return bt\n\n\nload_model = True\nmodel_path = \'model\'\nlength = R.shape[0]\nnoise_level = 1e-5\nprice = R.values[:, 3]\nn_noises = 1\nn_runs = 1\n\ntest_acts = np.zeros((n_noises, n_runs, length - 1))\ntest_rews = np.zeros((n_noises, n_runs, length - 1))\n\nfor j in range(n_noises):\n    np.random.seed(j)\n    env = environment(R + (np.random.rand(R.shape[0], R.shape[1]) - 0.5) * (1e-5 * (0**j)), length)\n    for i in range(n_runs):\n        np.random.seed(i + 1377)\n        test_acts[j, i, :], test_rews[j, i, :] = run_trades(\n            max_episode_length, gamma, s_size, a_size, load_model, model_path, length, env)\n        print(j, i)\n\n# \xd0\x9f\xd0\xa0\xd0\x98 \xd0\x9d\xd0\x95\xd0\xa3\xd0\x92\xd0\x95\xd0\xa0\xd0\x95\xd0\x9d\xd0\x9d\xd0\x9e\xd0\xa1\xd0\xa2\xd0\x98 \xd0\x9d\xd0\xa3\xd0\x96\xd0\x9d\xd0\x9e \xd0\x94\xd0\x95\xd0\x9b\xd0\x90\xd0\xa2\xd0\xac \xd0\x9f\xd0\xa0\xd0\x95\xd0\x94\xd0\xab\xd0\x94\xd0\xa3\xd0\xa9\xd0\x95\xd0\x95 \xd0\x94\xd0\x95\xd0\x99\xd0\xa1\xd0\xa2\xd0\x92\xd0\x98\xd0\x95, \xd0\x90 \xd0\x9d\xd0\x95 0\n\ntest_acts_probs = np.zeros((n_noises, a_size, length - 1))\ntresh = 0.0\nacts = np.zeros((n_noises, length - 1))\ntest_acts = test_acts.astype(int)\nfor k in range(n_noises):\n    for j in range(n_runs):\n        for i in range(length - 1):\n            a = test_acts[k, j, i]\n            test_acts_probs[k, a + 1, i] += 1\n    test_acts_probs[k, :, :] = test_acts_probs[k, :, :] / n_runs\n    acts[k, :] = [np.argmax(x) - 1 if max(x) > tresh else 0 for x in test_acts_probs[k, :, :].T]\n# print(test_acts_probs)\n# plt.plot(acts.T)\n# plt.show()\n\n\ndef write_report(r, filename):\n    with open(filename, ""a"") as input_file:\n        for k, v in r.items():\n            line = \'{}, {}\'.format(k, v)\n            print(line, file=input_file)\n\n\nfolder = os.path.relpath(""."", "".."")\nfor i in range(2):\n    # [ R.index.get_loc(\'2015-12-29 10:06:00\')[0], R.index.get_loc(\'2016-01-15 10:06:00\')[0], \\\n    lengths = [R.index.get_loc(\'2016-03-15 10:06:00\')[0], R.index.get_loc(\'2016-06-15 10:06:00\')[0]]\n    #names = [\'2 weeks\', \'1 month\', \'3 month\', \'6 months\']\n    names = [\'3 month\', \'6 months\']\n    bt = backtest(acts[-1, :lengths[i]], test_R[:lengths[i] + dep + 2])\n    fig = plt.figure(figsize=(12, 10))\n    plt.ylim(60000, 140000)\n    bt.plot_trades()\n    plt.legend([\'long enter\', \'short enter\', \'long exit\', \'short exit\', \'equity\', \'price\'], loc=\'best\')\n    plt.title(folder + \' test on \' + names[i], fontname=""Times New Roman"")\n    plt.ylabel(\'Rubles\', fontname=""Times New Roman"")\n    plt.xlabel(\'Time\', fontname=""Liberation Serif"")\n    plt.plot()\n    # plt.show()\n    plt.savefig(\'../new_plots/\' + folder + \'_\' + names[i] + \'.pdf\', bbox_inches=\'tight\', format=\'pdf\')\n\n    # \xd0\x9e\xd1\x82\xd1\x87\xd0\xb5\xd1\x82 \xd0\xbf\xd0\xbe \xd1\x81\xd0\xb8\xd1\x81\xd1\x82\xd0\xb5\xd0\xbc\xd0\xb5\n    B = bt.report\n    write_report(B, \'../new_plots/\' + folder + \'_.txt\')\n\n\n# data\nR = pd.read_pickle(data_dir_path + str(postfix))\nR = R[False == R.index.duplicated()]\n\ntest_R = pd.read_pickle(data_dir_path + str(postfix_real))\ntest_R = test_R[False == test_R.index.duplicated()]\n# print (R.mean()[0], R.max()[0], (R.max()[0] - R.min()[0]), (R.max()[0] - R.mean()[0])/(R.max()[0] - R.min()[0]), (R.min()[0] - R.mean()[0])/(R.max()[0] - R.min()[0]))\nR = (R - R.mean()) / (R.max() - R.min())\nold_R = R.copy()\nvals = R.values\nD = np.hstack([vals[i:-dep + i - 1, :] for i in range(dep, 0, -1)])\nR = pd.DataFrame(D, R[dep:-1].index)\n# R = R[:10000]\n\n\n# Main\ndef run_trades(max_episode_length, gamma, s_size, a_size, load_model, model_path, length, env):\n    tf.reset_default_graph()\n    with tf.device(\'/cpu:0\'):\n        global_episodes = tf.Variable(0, dtype=tf.int32, name=\'global_episodes\', trainable=False)\n        trainer = tf.train.RMSPropOptimizer(learning_rate=1e-3, decay=0.99, epsilon=1e-6)\n        master_network = AC_Network(s_size, a_size, \'global\', None)\n        num_workers = 1\n        workers = []\n        for i in range(num_workers):\n            workers.append(Test_Worker(env, i, s_size, a_size, trainer, model_path, global_episodes))\n        saver = tf.train.Saver(max_to_keep=5)\n\n    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0)\n    if not os.path.exists(model_path):\n        os.makedirs(model_path)\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=False, log_device_placement=False, gpu_options=gpu_options)) as sess:\n        coord = tf.train.Coordinator()\n\n        if load_model:\n            print(\'Loading Model...\')\n            ckpt = tf.train.get_checkpoint_state(model_path)\n            saver.restore(sess, ckpt.model_checkpoint_path)\n        else:\n            sess.run(tf.global_variables_initializer())\n\n        summary_writer = tf.summary.FileWriter(\'tb/train\', graph=sess.graph)\n        worker_threads = []\n        for worker in workers:\n            def worker_work(): return worker.work(max_episode_length, gamma, sess, coord, saver, dep, 0.33)\n            t = threading.Thread(target=(worker_work))\n            t.start()\n            worker_threads.append(t)\n\n        coord.join(worker_threads)\n        acts = workers[0].acts\n        rews = workers[0].rewards\n    return acts, rews\n\n\n# \xd0\xa2\xd0\xbe\xd1\x80\xd0\xb3\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x8f \xd1\x81\xd0\xb8\xd1\x81\xd1\x82\xd0\xb5\xd0\xbc\xd0\xb0 - \xd0\xbf\xd0\xbe\xd0\xba\xd1\x83\xd0\xbf\xd0\xb0\xd0\xb5\xd0\xbc/\xd0\xbf\xd1\x80\xd0\xbe\xd0\xb4\xd0\xb0\xd0\xb5\xd0\xbc \xd0\xb2 \xd0\xb7\xd0\xb0\xd0\xb2\xd0\xb8\xd1\x81\xd0\xb8\xd0\xbc\xd0\xbe\xd1\x81\xd1\x82\xd0\xb8 \xd0\xbe\xd1\x82 \xd0\xbf\xd1\x80\xd0\xb5\xd0\xb4\xd1\x81\xd0\xba\xd0\xb0\xd0\xb7\xd0\xb0\xd0\xbd\xd0\xbd\xd0\xbe\xd0\xb3\xd0\xbe \xd0\xba\xd0\xbb\xd0\xb0\xd1\x81\xd1\x81\xd0\xb0\ndef backtest(acts, o_R):\n    pacts = np.asarray(acts).copy()\n    for i in range(1, pacts.shape[0]):\n        if pacts[i] == 0:\n            if pacts[i - 1] == -1:\n                # from short\n                pacts[i] = 0.5\n            elif pacts[i - 1] == 1:\n                pacts[i] = -0.5\n\n    # plt.plot(pacts, \'.\')\n    data = o_R[dep - 1:-3]\n\n    buy = cover = pd.Series(pacts == 1, index=data.index)\n    short = sell = pd.Series(pacts == -1, index=data.index)\n    cover = pd.Series(pacts > 0, index=data.index)\n    sell = pd.Series(pacts < 0, index=data.index)\n    # print(np.sum(buy-sell))\n    OHLC = data[[(\'DealPrice\', \'close\'), (\'DealPrice\', \'close\'), (\'DealPrice\', \'close\'), (\'DealPrice\', \'close\')]]\n    OHLC.columns = [\'O\', \'H\', \'L\', \'C\']\n    # \xd0\x9e\xd1\x81\xd0\xbd\xd0\xbe\xd0\xb2\xd0\xbd\xd0\xbe\xd0\xb9 \xd0\xb3\xd1\x80\xd0\xb0\xd1\x84\xd0\xb8\xd0\xba \xd1\x8d\xd0\xba\xd0\xb2\xd0\xb8\xd1\x82\xd0\xb8\n    bt = pb.Backtest(locals())\n    return bt\n\n\nload_model = True\nmodel_path = \'model\'\nlength = R.shape[0]\nnoise_level = 1e-5\nprice = R.values[:, 3]\nn_noises = 1\nn_runs = 1\n\ntest_acts = np.zeros((n_noises, n_runs, length - 1))\ntest_rews = np.zeros((n_noises, n_runs, length - 1))\n\nfor j in range(n_noises):\n    np.random.seed(j)\n    env = environment(R + (np.random.rand(R.shape[0], R.shape[1]) - 0.5) * (1e-5 * (0**j)), length)\n    for i in range(n_runs):\n        np.random.seed(i + 1377)\n        test_acts[j, i, :], test_rews[j, i, :] = run_trades(\n            max_episode_length, gamma, s_size, a_size, load_model, model_path, length, env)\n        print(j, i)\n\n# \xd0\x9f\xd0\xa0\xd0\x98 \xd0\x9d\xd0\x95\xd0\xa3\xd0\x92\xd0\x95\xd0\xa0\xd0\x95\xd0\x9d\xd0\x9d\xd0\x9e\xd0\xa1\xd0\xa2\xd0\x98 \xd0\x9d\xd0\xa3\xd0\x96\xd0\x9d\xd0\x9e \xd0\x94\xd0\x95\xd0\x9b\xd0\x90\xd0\xa2\xd0\xac \xd0\x9f\xd0\xa0\xd0\x95\xd0\x94\xd0\xab\xd0\x94\xd0\xa3\xd0\xa9\xd0\x95\xd0\x95 \xd0\x94\xd0\x95\xd0\x99\xd0\xa1\xd0\xa2\xd0\x92\xd0\x98\xd0\x95, \xd0\x90 \xd0\x9d\xd0\x95 0\n\ntest_acts_probs = np.zeros((n_noises, a_size, length - 1))\ntresh = 0.0\nacts = np.zeros((n_noises, length - 1))\ntest_acts = test_acts.astype(int)\nfor k in range(n_noises):\n    for j in range(n_runs):\n        for i in range(length - 1):\n            a = test_acts[k, j, i]\n            test_acts_probs[k, a + 1, i] += 1\n    test_acts_probs[k, :, :] = test_acts_probs[k, :, :] / n_runs\n    acts[k, :] = [np.argmax(x) - 1 if max(x) > tresh else 0 for x in test_acts_probs[k, :, :].T]\n# print(test_acts_probs)\n# plt.plot(acts.T)\n# plt.show()\n\n\ndef write_report(r, filename):\n    with open(filename, ""a"") as input_file:\n        for k, v in r.items():\n            line = \'{}, {}\'.format(k, v)\n            print(line, file=input_file)\n\n\nfolder = os.path.relpath(""."", "".."")\nfor i in range(1):\n    lengths = [R.values.shape[0]]\n    names = [\' train on 3 months\']\n    bt = backtest(acts[-1, :lengths[i]], test_R[:lengths[i] + dep + 2])\n    fig = plt.figure(figsize=(12, 10))\n    plt.ylim(60000, 1400000)\n    bt.plot_trades()\n    plt.legend([\'long enter\', \'short enter\', \'long exit\', \'short exit\', \'equity\', \'price\'], loc=\'best\')\n    plt.title(folder + names[i], fontname=""Times New Roman"")\n    plt.ylabel(\'Rubles\', fontname=""Times New Roman"")\n    plt.xlabel(\'Time\', fontname=""Times New Roman"")\n    plt.yscale(\'log\')\n\n    plt.plot()\n    # plt.show()\n    plt.savefig(\'../new_plots/\' + folder + \'_\' + names[i] + \'.pdf\', bbox_inches=\'tight\', format=\'pdf\')\n\n    # \xd0\x9e\xd1\x82\xd1\x87\xd0\xb5\xd1\x82 \xd0\xbf\xd0\xbe \xd1\x81\xd0\xb8\xd1\x81\xd1\x82\xd0\xb5\xd0\xbc\xd0\xb5\n    B = bt.report\n    write_report(B, \'../new_plots/\' + folder + \'_train.txt\')\n'"
trader_gym.py,0,"b'# -*- coding: utf-8 -*-\n# \xd0\x94\xd0\xb0\xd0\xbd\xd0\xbd\xd1\x8b\xd0\xb9 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb \xd1\x81\xd0\xbe\xd0\xb4\xd0\xb5\xd1\x80\xd0\xb6\xd0\xb8\xd1\x82 envinroment \xd0\xba\xd0\xb0\xd0\xba \xd0\xb2 \xd0\xb3\xd1\x83\xd0\xbc\xd0\xb5\n# \xd0\x93\xd0\xbe\xd1\x82\xd0\xbe\xd0\xb2\xd0\xb8\xd1\x82 \xd1\x81\xd0\xbf\xd0\xb8\xd1\x81\xd0\xbe\xd0\xba \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd0\xbe\xd0\xb2 \xd1\x81 \xd0\xbf\xd0\xbe\xd0\xbb\xd0\xbd\xd1\x8b\xd0\xbc\xd0\xb8 \xd0\xb8\xd0\xbc\xd0\xb5\xd0\xbd\xd0\xb0\xd0\xbc\xd0\xb8 \xd0\xb4\xd0\xbb\xd1\x8f \xd0\xb7\xd0\xb0\xd0\xb3\xd1\x80\xd1\x83\xd0\xb7\xd0\xba\xd0\xb8\nimport numpy as np\nfrom configs import comission, PRICE_MAG\n\n# \xd0\xa0\xd0\xb5\xd0\xb0\xd0\xbb\xd0\xb8\xd0\xb7\xd0\xb0\xd1\x86\xd0\xb8\xd1\x8f \xd0\xba\xd0\xbb\xd0\xb0\xd1\x81\xd1\x81\xd0\xb0 environment \xd0\xbd\xd0\xb0\xd0\xbf\xd0\xbe\xd0\xb4\xd0\xbe\xd0\xb1\xd0\xb8\xd0\xb5 \xd1\x82\xd0\xb0\xd0\xba\xd0\xbe\xd0\xb3\xd0\xbe\xd0\xb3\xd0\xbe \xd0\xb8\xd0\xb7 openai.gym\n\n\nclass environment:\n    def __init__(self, data_frame, leng):\n        self.big_data = data_frame.values\n        self.len = leng\n        self.start_len = self.big_data.shape[0] - leng\n        #print(self.start_len, leng)\n        start_point = int(np.random.rand() * self.start_len)\n        self.data = self.big_data[start_point:start_point + leng, :]\n        # close_prices\n        self.prices = self.data[:, 3]\n        self.iter = 0\n        self.n_shares = 0\n        self.cash = 0\n        self.max_shares = 1\n        self.max_iter = self.prices.shape[0]\n        self.done = False\n        self.prev_equity = 0\n        self.equity = 0\n        self.comission = comission\n        # \xd0\xa8\xd1\x82\xd1\x80\xd0\xb0\xd1\x84 \xd0\xb7\xd0\xb0 \xd0\xbf\xd0\xbe\xd0\xb2\xd1\x82\xd0\xbe\xd1\x80\xd1\x8b\n        self.same_steps = 0\n        self.prev_act = 0\n\n    def calc_reward(self, act):\n        # \xd0\x94\xd0\xb5\xd0\xb9\xd1\x81\xd1\x82\xd0\xb2\xd0\xb8\xd0\xb5 act is -1(sell) 0 (nothing) and +1(buy)\n        # if(act != self.n_shares\n        # if abs(self.n_shares + act) <= self.max_shares:\n        # print(PRICE_MAG)\n        if(self.n_shares != act):\n            self.cash = self.cash - self.prices[self.iter - 1] * \\\n                (act - self.n_shares) - self.comission * PRICE_MAG * (1 + 0 * (self.same_steps < 3))\n        self.n_shares = act\n        # \xd0\xad\xd0\xba\xd0\xb2\xd0\xb8\xd1\x82\xd0\xb8 - \xd1\x81\xd1\x83\xd0\xbc\xd0\xbc\xd0\xb0\xd1\x80\xd0\xbd\xd1\x8b\xd0\xb9 \xd0\xbe\xd0\xb1\xd1\x8a\xd0\xb5\xd0\xbc \xd0\xb4\xd0\xb5\xd0\xbd\xd0\xb5\xd0\xb3, \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 \xd1\x81\xd0\xb5\xd0\xb9\xd1\x87\xd0\xb0\xd1\x81 \xd0\xb2\xd1\x81\xd0\xb5 \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb4\xd0\xb0\xd1\x82\xd1\x8c\n        self.equity = self.cash + self.prices[self.iter] * self.n_shares\n        reward = self.equity - self.prev_equity\n        self.prev_equity = self.equity\n        # \xd0\x9c\xd0\xb0\xd0\xb3\xd0\xb8\xd1\x87\xd0\xb5\xd1\x81\xd0\xba\xd0\xb8\xd0\xb5 \xd0\xba\xd0\xbe\xd0\xbd\xd1\x81\xd1\x82\xd0\xb0\xd0\xbd\xd1\x82\xd1\x8b - \xd1\x88\xd1\x82\xd1\x80\xd0\xb0\xd1\x84 \xd1\x80\xd0\xb0\xd0\xb2\xd0\xb5\xd0\xbd 0.01% \xd0\xb7\xd0\xb0 \xd1\x85\xd0\xbe\xd0\xb4 \xd0\xbd\xd0\xb0 10 \xd0\xbe\xd0\xb4\xd0\xb8\xd0\xbd\xd0\xb0\xd0\xba\xd0\xbe\xd0\xb2\xd1\x8b\xd1\x85 \xd0\xb4\xd0\xb5\xd0\xb9\xd1\x81\xd1\x82\xd0\xb2\xd0\xb8\xd0\xb9\n        return reward - self.comission * PRICE_MAG * (int(self.same_steps / 1000))\n\n    def step(self, act):\n        # \xd0\x9e\xd0\xb4\xd0\xb8\xd0\xbd \xd1\x88\xd0\xb0\xd0\xb3 \xd1\x81\xd0\xb8\xd1\x81\xd1\x82\xd0\xb5\xd0\xbc\xd1\x8b - \xd0\xbf\xd0\xbe\xd0\xbb\xd1\x83\xd1\x87\xd0\xb8\xd1\x82\xd1\x8c \xd0\xbd\xd0\xb0 \xd0\xb2\xd1\x85\xd0\xbe\xd0\xb4 act = [-1,0,1][a]\n        # \xd0\x95\xd1\x81\xd0\xbb\xd0\xb8 \xd0\xbd\xd0\xb5 \xd0\xba\xd0\xbe\xd0\xbd\xd0\xb5\xd1\x86 \xd0\xb8\xd0\xb3\xd1\x80\xd1\x8b:\n        if not self.done:\n            self.iter += 1\n        # \xd0\x98\xd0\xb7\xd0\xb2\xd0\xbb\xd0\xb5\xd1\x87\xd1\x8c \xd1\x81\xd0\xbb\xd0\xb5\xd0\xb4\xd1\x83\xd1\x8e\xd1\x89\xd0\xb8\xd0\xb5 \xd0\xbd\xd0\xb0\xd0\xb1\xd0\xbb\xd1\x8e\xd0\xb4\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8f\n        # \xd0\xa1\xd0\xbe\xd1\x81\xd1\x82\xd0\xbe\xd1\x8f\xd0\xbd\xd0\xb8\xd0\xb5 \xd1\x81\xd0\xb8\xd1\x81\xd1\x82\xd0\xb5\xd0\xbc\xd1\x8b - \xd0\xbe\xd0\xb4\xd0\xbd\xd0\xbe \xd1\x87\xd0\xb8\xd1\x81\xd0\xbb\xd0\xbe self.iter\n        observation = self.data[self.iter]\n        reward = self.calc_reward(act)\n        # \xd0\xa1\xd1\x87\xd0\xb8\xd1\x82\xd0\xb0\xd0\xb5\xd0\xbc \xd1\x87\xd0\xb8\xd1\x81\xd0\xbb\xd0\xbe \xd0\xbe\xd0\xb4\xd0\xb8\xd0\xbd\xd0\xb0\xd0\xba\xd0\xbe\xd0\xb2\xd1\x8b\xd1\x85 \xd0\xb4\xd0\xb5\xd0\xb9\xd1\x81\xd1\x82\xd0\xb2\xd0\xb8\xd0\xb9 \xd0\xbf\xd0\xbe\xd0\xb4\xd1\x80\xd1\x8f\xd0\xb4\n        self.same_steps += 1\n        if act != self.prev_act:\n            self.same_steps = 0\n\n        if self.iter >= self.max_iter - 1:\n            self.done = True\n        else:\n            self.done = False\n        self.prev_act = act\n        return observation, reward, self.done, self.n_shares\n\n    def reset(self):\n        self.iter = 0\n        self.done = False\n        start_point = int(np.random.rand() * self.start_len)\n        self.data = self.big_data[start_point:start_point + self.len, :]\n        observation = self.data[self.iter]\n        self.prices = self.data[:, 3]\n        self.n_shares = 0\n        self.cash = 0\n        self.prev_equity = 0\n        self.equity = 0\n        return observation\n\n    # \xd0\x93\xd0\xb5\xd0\xbd\xd0\xb5\xd1\x80\xd0\xb8\xd1\x80\xd1\x83\xd0\xb5\xd1\x82 shifted_act\n    def sample(self):\n        return np.random.randint(0, 3)\n'"
