file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\nfrom ergo.version import __version__, __author__, __email__, __license__\n\nimport os\n\n\ndesc = \'ergo is a tool that makes deep learning with Keras easier.\'\n\nrequired = []\nwith open(\'requirements.txt\') as fp:\n    for line in fp:\n        line = line.strip()\n        if line != """":\n            required.append(line)\n\nsetup( name                 = \'ergo-ai\',\n       version              = __version__,\n       description          = desc,\n       long_description     = desc,\n       long_description_content_type = \'text/plain\',\n       author               = __author__,\n       author_email         = __email__,\n       url                  = \'http://www.github.com/evilsocket/ergo\',\n       packages             = find_packages(),\n       install_requires     = required,\n       scripts              = [ \'bin/ergo\' ],\n       license              = __license__,\n       classifiers          = [\n            \'Programming Language :: Python :: 3\',\n            \'Development Status :: 5 - Production/Stable\',\n            \'License :: OSI Approved :: GNU General Public License (GPL)\',\n            \'Environment :: Console\',\n      ],\n)\n'"
ergo/__init__.py,0,b''
ergo/dataset.py,0,"b'import os\nimport logging as log\n\nimport numpy as np\nimport pandas as pd\n\nfrom keras.utils import to_categorical\n\nfrom ergo.core.saver import Saver\nfrom ergo.core.loader import Loader\n\nclass Dataset(object):\n    @staticmethod\n    def split_row(row, n_labels, flat):\n        x = row.iloc[:,1:].copy()\n        if not flat:\n            if len(row) == 1:\n                # this check is to prevent the list comprehension to fail\n                # shouldn\'t happen in production but can fail on test\n                log.error(""Dataset size must be greater than 1"")\n                quit()\n            x = [ np.squeeze(np.array( [ x[i][:]] ), axis = 0)\n                   for i in x.columns ]\n        y = to_categorical(row.values[:,0], n_labels)\n        return x, y\n\n    def __init__(self, path):\n        self.path       = os.path.abspath(path)\n        self.train_path = os.path.join(self.path, \'data-train.csv\')\n        self.test_path  = os.path.join(self.path, \'data-test.csv\')\n        self.valid_path = os.path.join(self.path, \'data-validation.csv\')\n        self.saver      = Saver(self)\n        self.loader     = Loader(self)\n        self.do_save    = True\n        self.is_flat    = True\n        self.n_labels   = 0\n        self.train      = None\n        self.test       = None\n        self.validation = None\n        self.X_train    = None\n        self.Y_train    = None\n        self.X_test     = None\n        self.Y_test     = None\n        self.X_val      = None\n        self.Y_val      = None\n        self.X          = None\n        self.Y          = None\n\n    def has_train(self):\n        return os.path.exists(self.train_path) or os.path.exists(self.train_path.replace(\'.csv\', \'.pkl\'))\n\n    def has_test(self):\n        return os.path.exists(self.test_path) or os.path.exists(self.test_path.replace(\'.csv\', \'.pkl\'))\n\n    def has_validation(self):\n        return os.path.exists(self.valid_path) or os.path.exists(self.valid_path.replace(\'.csv\', \'.pkl\'))\n\n    def exists(self):\n        return self.has_train() and \\\n               self.has_test() and \\\n               self.has_validation()\n\n    def _set_xys(self, for_training = True):\n        if for_training:\n            self.X_train, self.Y_train = Dataset.split_row(self.train, self.n_labels, self.is_flat)\n            self.X_test,  self.Y_test  = Dataset.split_row(self.test, self.n_labels, self.is_flat)\n            self.X_val,   self.Y_val   = Dataset.split_row(self.validation, self.n_labels, self.is_flat)\n        else:\n            self.X, self.Y = Dataset.split_row(self.train, self.n_labels, self.is_flat)\n\n    def _set_xys_test(self):\n        self.X_test, self.Y_test = Dataset.split_row(self.test, self.n_labels, self.is_flat)\n\n    def _check_encoding(self):\n        pkl_test = self.train_path.replace(\'.csv\', \'.pkl\')\n        if os.path.exists(pkl_test):\n            log.info(""detected pickle encoded dataset"")\n            self.is_flat = False\n            self.train_path = self.train_path.replace(\'.csv\', \'.pkl\')\n            self.test_path = self.test_path.replace(\'.csv\', \'.pkl\')\n            self.valid_path = self.valid_path.replace(\'.csv\', \'.pkl\')\n\n    def load_test(self):\n        self._check_encoding()\n        self.loader.load_test()\n        self._set_xys_test()\n\n    def load(self):\n        self._check_encoding()\n        self.loader.load()\n        self._set_xys()\n\n    def _is_scalar_value(self, v):\n        try:\n            return not (len(v) >= 0)\n        except TypeError:\n            # TypeError: object of type \'X\' has no len()\n            return True\n        except:\n            raise\n\n    def source(self, data, p_test = 0.0, p_val = 0.0, shuffle = True, n_labels=None):\n        if shuffle:\n            # reset indexes and resample data just in case\n            dataset = data.sample(frac = 1).reset_index(drop = True)\n        else:\n            dataset = data\n\n        # check if the input vectors are made of scalars or other vectors\n        self.is_flat = True\n        for x in dataset.iloc[0,:]:\n            if not self._is_scalar_value(x):\n                log.info(""detected non scalar input: %s"", x.shape)\n                self.is_flat = False\n                break\n\n        # count unique labels on first column if no counter is provided externally\n        self.n_labels = n_labels if n_labels is not None else len(dataset.iloc[:,0].unique())\n        # if both values are zero, we\'re just loading a single file,\n        # otherwise we want to generate training temporary datasets.\n        for_training = p_test > 0.0 and p_val > 0.0\n        if for_training:\n            log.info(""generating train, test and validation datasets (test=%f validation=%f) ..."",\n                    p_test,\n                    p_val)\n\n            n_tot   = len(dataset)\n            n_train = int(n_tot * ( 1 - p_test - p_val))\n            n_test  = int(n_tot * p_test)\n            n_val   = int(n_tot * p_val)\n\n            self.train      = dataset.head(n_train)\n            self.test       = dataset.head(n_train + n_test).tail(n_test)\n            self.validation = dataset.tail(n_val)\n\n            if self.do_save:\n                self.saver.save()\n        else:\n            self.train = dataset\n\n        self._set_xys(for_training)\n\n    def subsample(self, ratio):\n        X = self.X.values if self.is_flat else self.X\n        y = self.Y\n        if ratio < 1.0:\n            log.info(""selecting a randomized sample of %d%% ..."", ratio * 100)\n\n            tot_rows = X.shape[0] if self.is_flat else X[0].shape[0]\n            num      = int(tot_rows * ratio)\n            indexes  = np.random.choice(tot_rows, num, replace = False)\n\n            X = X[indexes] if self.is_flat else [ i[indexes] for i in X ]\n            y = y[indexes]\n\n        return X, y\n'"
ergo/project.py,0,"b'import os\nimport sys\nimport json\nimport re\nimport gc\nimport logging as log\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nfrom keras.models import model_from_yaml, load_model\nfrom keras.utils.training_utils import multi_gpu_model\nfrom keras import backend as K\n\nfrom ergo.core.utils import serialize_classification_report, serialize_cm\nfrom ergo.core.logic import Logic\nfrom ergo.dataset import Dataset\nfrom ergo.core.multi_model import multi_model\n\nclass Project(object):\n    def __init__(self, path):\n        # base info\n        self.path  = os.path.abspath(path)\n        self.logic = Logic(self.path)\n        # model related data\n        self.model          = None\n        self.accu           = None\n        self.model_path     = os.path.join(self.path, \'model.yml\')\n        self.weights_path   = os.path.join(self.path, \'model.h5\')\n        self.fdeep_path     = os.path.join(self.path, \'model.fdeep\')\n        # training related data\n        self.dataset         = Dataset(self.path)\n        self.txt_stats_path  = os.path.join(self.path, \'stats.txt\')\n        self.json_stats_path = os.path.join(self.path, \'stats.json\')\n        self.history_path    = os.path.join(self.path, \'history.json\')\n        self.classes_path    = os.path.join(self.path, \'classes.json\')\n        self.history         = None\n        self.classes         = None\n        self.what            = {\n            \'train\' : ""Training --------------------------------------------\\n"",\n            \'val\'   : ""Validation ------------------------------------------\\n"",\n            \'test\'  : ""Test ------------------------------------------------\\n""\n        }\n\n    def exists(self):\n        return os.path.exists(self.path)\n\n    def is_trained(self):\n        return os.path.exists(self.weights_path)\n\n    def load(self):\n        log.info(""loading project %s ..."" % self.path)\n\n        if not self.exists():\n            return ""%s does not exist"" % self.path\n\n        err = self.logic.load()\n        if err is not None:\n            return err\n\n        if os.path.exists(self.weights_path):\n            log.debug(""loading model from %s ..."", self.weights_path)\n            self.model = load_model(self.weights_path)\n            # https://github.com/keras-team/keras/issues/6462\n            self.model._make_predict_function()\n\n        elif os.path.exists(self.model_path):\n            log.debug(""loading model from %s ..."", self.model_path)\n            with open(self.model_path, \'r\') as fp:\n                self.model = model_from_yaml(fp.read())\n\n        else:\n            self.model = self.logic.builder(True)\n\n        if os.path.exists(self.history_path):\n            log.debug(""loading history from %s ..."", self.history_path)\n            with open(self.history_path, \'r\') as fp:\n                self.history = json.load(fp)\n\n\n        if os.path.exists(self.classes_path):\n            log.debug(""loading classes from %s ..."", self.classes_path)\n            with open(self.classes_path, \'r\') as fp:\n                self.classes = {int(k) : v for k, v in json.load(fp).items()}\n\n        return None\n\n    def accuracy_for(self, X, Y, repo_as_dict = False):\n        Y_tpred = np.argmax(self.model.predict(X), axis = 1)\n        repo    = classification_report(np.argmax(Y, axis = 1), Y_tpred, output_dict = repo_as_dict, digits = 5)\n        cm      = confusion_matrix(np.argmax(Y, axis = 1), Y_tpred)\n        return repo, cm\n\n    def accuracy(self):\n        train, tr_cm = self.accuracy_for(self.dataset.X_train, self.dataset.Y_train)\n        test,  ts_cm = self.accuracy_for(self.dataset.X_test, self.dataset.Y_test)\n        val,  val_cm = self.accuracy_for(self.dataset.X_val, self.dataset.Y_val)\n        return {\'train\': (train, tr_cm),\n                \'test\': (test, ts_cm),\n                \'val\': (val, val_cm)}\n\n    def reload_model(self):\n        K.clear_session()\n\n        if os.path.exists(self.weights_path):\n            self.model = load_model(self.weights_path)\n            # https://github.com/keras-team/keras/issues/6462\n            self.model._make_predict_function()\n        elif os.path.exists(self.model_path):\n            with open(self.model_path, \'r\') as fp:\n                self.model = model_from_yaml(fp.read())\n        else:\n            self.model = self.logic.builder(True)\n\n        gc.collect()\n\n    def _save_model(self):\n        log.info(""updating %s ..."", self.model_path)\n        with open( self.model_path, \'w\' ) as fp:\n            fp.write(self.model.to_yaml())\n\n        log.info(""updating %s ..."", self.weights_path)\n        self.model.save(self.weights_path)\n\n    def _save_history(self):\n        log.info(""updating %s ..."", self.history_path)\n        with open(self.history_path, \'w\') as fp:\n            json.dump(self.history, fp)\n\n    def _emit_txt_stats(self, where):\n        for who, header in self.what.items():\n            vals = self.accu[who]\n            where.write( header )\n            where.write( vals[0] )\n            where.write(""\\n\\n"")\n            where.write(""confusion matrix:"")\n            where.write(""\\n\\n"")\n            where.write(""%s\\n"" % vals[1])\n            where.write(""\\n"")\n\n    def _emit_json_stats(self, where):\n        stats = {}\n        for who in self.what:\n            report, cm = self.accu[who]\n            stats[who] = {\n                \'accuracy\': serialize_classification_report(report),\n                \'cm\': serialize_cm(cm)\n            }\n        json.dump(stats, where)\n\n    def _save_stats(self):\n        log.info(""updating %s ..."", self.txt_stats_path)\n        with open(self.txt_stats_path, \'w\') as fp:\n            self._emit_txt_stats(fp)\n\n        log.info(""updating %s ..."", self.json_stats_path)\n        with open(self.json_stats_path, \'wt\') as fp:\n            self._emit_json_stats(fp)\n\n    def _from_file(self, filename):\n        log.info(""preparing data from %s ..."", filename)\n        return self.logic.prepare_dataset(filename)\n\n    def prepare(self, source, p_test, p_val, shuffle = True):\n        data = self._from_file(source)\n        num_labels = None\n        if self.model is not None:\n            # assuming only one single dense output layer\n            num_labels = self.model.outputs[-1].shape[1]\n        log.info(""data shape: %s"", data.shape)\n        return self.dataset.source(data, p_test, p_val, shuffle, num_labels)\n\n    def train(self, gpus):\n        # async datasets saver might be running, wait before training\n        self.dataset.saver.wait()\n\n        # train\n        if self.model is None:\n            self.model = self.logic.builder(True)\n\n        to_train = multi_model(self.model, None)\n        if gpus > 1:\n            log.info(""training with %d GPUs"", gpus)\n            to_train = multi_model(self.model, multi_gpu_model(self.model, gpus=gpus))\n\n        past = self.history.copy() if self.history is not None else None\n        present = self.logic.trainer(to_train, self.dataset).history\n\n        if past is None:\n            self.history = present\n        else:\n            self.history = {}\n            for name, past_values in past.items():\n                self.history[name] = past_values + present[name]\n\n        self.accu = self.accuracy()\n\n        print("""")\n        self._emit_txt_stats(sys.stdout)\n\n        # save model structure and weights\n        self._save_model()\n        # save training history\n        self._save_history()\n        # save model accuracy statistics\n        self._save_stats()\n\n    def view(self, img_only = False):\n        import ergo.views as views\n\n        views.model(self, img_only)\n        views.roc(self, img_only)\n        views.stats(self, img_only)\n        views.history(self, img_only)\n        views.show(img_only)\n'"
ergo/templates.py,0,"b'from ergo.core.template import Template\n\nprepare = \\\n""""""\nimport pandas as pd\n\n# this function is called whenever the `ergo train <project> --dataset file.csv`\n# command is executed, the first argument is the dataset and it must return\n# a pandas.DataFrame object.\ndef prepare_dataset(filename):\n    # simply read as csv\n    return pd.read_csv(filename, sep = \',\', header = None)\n\n# this function is called to process a single input into a vector\n# that can be used for training or to run an inference.\n# it is called from `ergo encode ...` with `is_encoding` set to True,\n# in which case you can add additional metadata to the vector, or\n# from the `ergo serve ...` API, in which case the vector will be\n# used for inference and can\'t contain metadata.\ndef prepare_input(x, is_encoding = False):\n    # simply read as csv\n    return pd.read_csv( pd.compat.StringIO(x), sep = \',\', header = None)\n""""""\n\nmodel = \\\n""""""\nimport logging as log\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\n\n# build the model\ndef build_model(is_train):  \n    n_inputs       = {NUM_INPUTS}\n    n_hidden       = [{HIDDEN}]\n    dropout        = 0.4\n    activation     = \'relu\'\n    out_activation = \'softmax\'\n  \n    log.info(""building model for %s ..."" % \'training\' if is_train else \'evaluation\')\n\n    model = Sequential()\n    for i, n_neurons in enumerate(n_hidden):\n        # setup the input layer\n        if i == 0:\n            model.add(Dense(n_neurons, input_shape = (n_inputs,), activation = activation))\n        else:\n            model.add(Dense(n_neurons, activation = activation))\n        # add dropout\n        if is_train:\n            model.add(Dropout(dropout))\n    # setup output layer\n    model.add(Dense({NUM_OUTPUTS}, activation = out_activation))\n    \n    return model\n""""""\n\ntrain = \\\n""""""\nimport logging as log\n\nfrom keras.callbacks import EarlyStopping\n\n# define training strategy\ndef train_model(model, dataset):\n    log.info(""training model (train on %d samples, validate on %d) ..."" % ( \\\\\n            len(dataset.Y_train), \n            len(dataset.Y_val) ) )\n    \n    loss      = \'categorical_crossentropy\'\n    optimizer = \'adam\'\n    metrics   = [\'accuracy\']\n    \n    model.compile(loss = loss, optimizer = optimizer, metrics = metrics)\n\n    earlyStop = EarlyStopping(monitor = \'val_acc\', min_delta=0.0001, patience = 5, mode = \'auto\')\n    return model.fit( dataset.X_train, dataset.Y_train,\n            batch_size = {BATCH_SIZE},\n            epochs = {MAX_EPOCHS},\n            verbose = 2,\n            validation_data = (dataset.X_val, dataset.Y_val),\n            callbacks = [earlyStop])\n""""""\n\ndeps = \\\n""""""\nergo-ai\n""""""\n\ngitignore = \\\n""""""\n*.pyc\n*.csv\n__pycache__\n""""""\n\nTemplates = [ \n    Template(\'prepare.py\', prepare),\n    Template(\'model.py\', model),\n    Template(\'train.py\', train),\n    Template(\'requirements.txt\', deps),\n    Template(\'.gitignore\', gitignore)\n]\n'"
ergo/version.py,0,"b'__author__    = \'Simone Margaritelli\'\n__email__     = \'evilsocket@gmail.com\'\n__copyright__ = \'Copyright 2019, Simone Margaritelli\'\n__credits__   = [\'Nico Chiaraviglio\', \'waxwing\']\n__license__   = \'GPL\'\n__version__   = \'1.12.9\'\n__status__    = \'Production\'\n\nbanner = """"""\n                  ,.::XXXXXXX*:.,,,,                                                 \n              .:(XXX(`        ` `*XXXX&:.,                                           \n         ,.:(XX*\'`                    `)*XXX::...,                                   \n       *&X* `                              `#XXXXXXX*,.                              \n    .X#``     .:&XX*#*:.,                       ``^:*XXXX&:.                         \n  .*X`    ,:XXXXXXXXXXXXXX#.                           `^XXXX&,                      \n.:X`    .XXXXXXXXXXXXXXXXXX*.                             `^XX&&&.,                  \n:`     XXXXXXXXXXXXXXXXXXXXX                   .,            `XXXXX:.                \n      XXXXXXXXXXXXXXXXXXXXXXX                .XXXXXX#,           `XXXXX,             \n     XXXXXXXXXXXXXXXXXXXXXXXX,               .&XXX, `^X#:.          `^&XX.           \n    &XXXXXXXXXXXXXXXXXX&`   `XX,             .:XXXX      `^X*.,        `^XX.         \n   #XXXXXXXXXXXXXXXX&`        `XX,            `XXXX,.XXXXXXXXXXX&.,      `XX&        \n   XXXXXXXXXXXXXXX(`    .u      `XXX*:.         `*X*.XXXXXXXXXXXXXXX\\.    `XX&.      \n  ,XXXXXXXXXXXXXX`   ud8888.        `\'\\*XXX&.      `*XX,XXXXXXXXXXXXXXX&,   `XX&     \n  *XXXXXXXXXXXX`   :888\'8888.               `XXX.     `*XX&X^`XXXXXXXXXX*""X**XXX&    \n  XXXXXXXXXXX`     d888 \'88*""   .u    .        `(X#,     `X&,  `XXXXXXXXX  `XXXXX&.  \n  XXXXXXXX(`       8888.+""     .d88B :@8c         `XX      `X    XXXXXXXX    `XXXX&. \n .XXXXXX:`         8888L      =""8888f8888r          `X*,     X   \'XXXXXXX,      `""`  \n *XXX:`            \'8888c. .+   4888>\'88""      uL     `X*   &X   XXXXXXXX&,          \n XX`                ""88888*     4888> \'    .ue888Nc..    ""*X*`   ""XXX^XXX&:,         \n&X`                   ""YP\'      4888>     d88E`""888E`       u.      `   `""`          \n`                              .d888L .+  888E  888E   ...ue888b                     \n                               ^""8888*""   888E  888E   888R Y888r                    \n         ergo    %10s       ""Y""     888E  888E   888R I888>                    \n         keras   %10s               888& .888E   888R I888>                    \n         tf      %10s               *888"" 888&   888R I888>                    \n         sklearn %10s                      `""   ""888E  8888cJ8WX                     \n                                          .dWi   `88E  ""*888*P""                      \n                                          4888~  J8*     \'Y""                         \n                                           ^""===*""`                                  \n""""""\n'"
ergo/views.py,0,"b'import os\nimport json\nimport itertools\nimport numpy as np\nimport logging as log\n\ndef model(prj, img_only):\n    if prj.model is not None:\n        prj.model.summary()\n\ndef roc(prj, img_only):\n    if prj.dataset.has_test():\n        import matplotlib.pyplot as plt\n        from sklearn.metrics import roc_curve, auc\n\n        log.info(""found %s, loading ..."", prj.dataset.test_path)\n        prj.dataset.load_test()\n        log.info(""computing ROC curve on %d samples ..."", len(prj.dataset.X_test))\n\n        y_pred = prj.model.predict(prj.dataset.X_test)\n        fpr, tpr, thresholds = roc_curve(prj.dataset.Y_test.ravel(), y_pred.ravel())\n\n        plt.figure(""ROC Curve"")\n        plt.title(""ROC Curve"")\n        plt.plot([0, 1], [0, 1], \'k--\')\n        plt.plot(fpr, tpr, label=\'AUC = {:.3f}\'.format(auc(fpr, tpr)))\n        plt.xlabel(\'FPR\')\n        plt.ylabel(\'TPR\')\n        plt.legend()\n\n        plt.savefig( os.path.join(prj.path, \'roc.png\') )\n\ndef stats(prj, img_only):\n    if os.path.exists(prj.txt_stats_path):\n        with open(prj.txt_stats_path, \'rt\') as fp:\n            print(fp.read().strip())\n\n    if os.path.exists(prj.json_stats_path):\n        import matplotlib.pyplot as plt\n\n        with open(prj.json_stats_path, \'rt\') as fp:\n            stats = json.load(fp)\n            for who, header in prj.what.items():\n                orig = np.array(stats[who][\'cm\'])\n                cm = np.array(stats[who][\'cm\'])\n                tot = cm.sum()\n                cm = cm.astype(\'float\') / cm.sum(axis=1)[:, np.newaxis]\n\n                name = header.strip("" -\\n"").lower()\n                title = ""%s confusion matrix (%d samples)"" % (name, tot)\n                filename = os.path.join(prj.path, ""%s_cm.png"" % name)\n\n                plt.figure(title)\n                plt.imshow(cm, interpolation=\'nearest\', cmap=plt.cm.Reds)\n                plt.title(title)\n                plt.colorbar()\n                classes = range(0, cm.shape[0])\n                tick_marks = np.arange(len(classes))\n                plt.xticks(tick_marks, classes, rotation=45)\n                plt.yticks(tick_marks, classes)\n                thresh = cm.max() / 2.\n                for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n                    plt.text(j, i, ""%.1f%% (%d)"" % (cm[i, j] * 100, orig[i, j]),\n                             horizontalalignment=""center"",\n                             color=""white"" if cm[i, j] > thresh else ""black"")\n\n                plt.tight_layout()\n                plt.ylabel(\'truth\')\n                plt.xlabel(\'prediction\')\n                plt.savefig(filename)\n\ndef history(prj, img_only):\n    if prj.history is not None:\n        import matplotlib.pyplot as plt\n\n        plt.figure(""training history"")\n        # Plot training & validation accuracy values\n        plt.subplot(2,1,1)\n        plt.plot(prj.history[\'acc\'])\n        plt.plot(prj.history[\'val_acc\'])\n        plt.title(\'Model accuracy\')\n        plt.ylabel(\'Accuracy\')\n        plt.xlabel(\'Epoch\')\n        plt.legend([\'Train\', \'Test\'], loc=\'lower right\')\n\n        # Plot training & validation loss values\n        plt.subplot(2,1,2)\n        plt.plot(prj.history[\'loss\'])\n        plt.plot(prj.history[\'val_loss\'])\n        plt.title(\'Model loss\')\n        plt.ylabel(\'Loss\')\n        plt.xlabel(\'Epoch\')\n        plt.legend([\'Train\', \'Test\'], loc=\'upper right\')\n        plt.tight_layout()\n        plt.savefig( os.path.join(prj.path, \'history.png\') )\n\ndef plot_clusters(prj, pca, X, y, ca, D3 = False):\n    import matplotlib.pyplot as plt\n    from numpy import argmax\n    from matplotlib.lines import Line2D\n    from mpl_toolkits.mplot3d import axes3d\n\n    Xt = pca.transform(X)\n    Xt = Xt[:, :3]\n    y = argmax(y, axis=1)\n\n    clusters = ca.labels_\n    cmap = plt.cm.jet\n    cmaplist = [cmap(i) for i in range(0, cmap.N, cmap.N//len(set(clusters)))]\n    cmap = cmap.from_list(\'Custom cmap\', cmaplist, cmap.N)\n\n    if D3:\n        fig = plt.figure(\'Clustering Analisys - 3D projection\')\n        ax = fig.add_subplot(1, 1, 1, projection = \'3d\')\n        ax.set_zlabel(\'Principal component 3\')\n    else:\n        fig = plt.figure(\'Clustering Analisys\')\n        ax = fig.add_subplot(1, 1, 1)\n\n    ax.set_xlabel(\'Principal component 1\')\n    ax.set_ylabel(\'Principal component 2\')\n    markers = [m for m, func in Line2D.markers.items()\n                        if func != \'nothing\' and m in Line2D.filled_markers]\n    _legends = []\n    for cl in list(set(y)):\n        idx =  np.where(y == cl)[0]\n        if D3:\n            scatter = ax.scatter(Xt[idx, 0], Xt[idx, 1], Xt[idx,2], cmap=cmap, s=40, alpha=0.5,\n                                 c=clusters[idx],\n                                 marker=markers[cl],\n                                 label=y[idx],\n                                 edgecolor=\'black\'\n                                 )\n        else:\n            scatter = ax.scatter(Xt[idx,0], Xt[idx,1], cmap = cmap, s = 40, alpha = 0.5,\n                                 c = clusters[idx],\n                                 marker = markers[cl],\n                                 label=y[idx],\n                                 edgecolor=\'black\'\n                                )\n        l = Line2D([], [], color=\'black\', marker=markers[cl], linestyle=\'None\',\n                          markersize=5, mfc = \'w\', markeredgecolor = \'black\', label=\'Class %d\' % cl)\n        _legends.append(l)\n\n    for cl in list(set(clusters)):\n        label = \'Cluster %d\' % cl\n        if cl == -1:\n            label = \'Noisy samples\'\n        l = Line2D([], [], color= cmaplist[cl], marker=\'o\', linestyle=\'None\',\n                          markersize=5,  label=label)\n        _legends.append(l)\n\n    legend1 = ax.legend(handles=_legends, loc=\'upper left\')\n    ax.add_artist(legend1)\n\n    fig.tight_layout()\n    if D3:\n        name = \'clusters_projection_3D.png\'\n    else:\n        name = \'clusters_projection.png\'\n    fig.savefig(os.path.join(prj.path, name))\n\ndef plot_intertia(prj, x, y):\n    import matplotlib.pyplot as plt\n\n    fig = plt.figure(""Inertia of Kmeans clustering algorithm"")\n    ax = fig.add_subplot(1,1,1)\n    ax.set_xlabel(\'# of clusters\')\n    ax.set_ylabel(\'Intertia\')\n    ax.plot(x,y, \'*-\')\n    fig.savefig(os.path.join(prj.path, \'kmeans_inertia.png\'))\n\n\ndef pca_projection(prj, pca, X, y, D3):\n    import matplotlib.pyplot as plt\n    from numpy import argmax\n    from mpl_toolkits.mplot3d import axes3d\n\n    Xt = pca.transform(X)\n    Xt = Xt[:,:3]\n    y = argmax(y, axis=1)\n\n    cmap = plt.cm.jet\n    cmaplist = [cmap(i) for i in range(cmap.N)]\n    cmap = cmap.from_list(\'Custom cmap\', cmaplist, cmap.N)\n\n    if D3:\n        fig = plt.figure(\'PCA decomposition - 3D projection\')\n        ax = fig.add_subplot(1, 1, 1, projection = \'3d\')\n        ax.set_zlabel(\'Principal component 3\')\n    else:\n        fig = plt.figure(\'PCA decomposition\')\n        ax = fig.add_subplot(1, 1, 1)\n\n    ax.set_xlabel(\'Principal component 1\')\n    ax.set_ylabel(\'Principal component 2\')\n\n    if D3:\n        scatter = ax.scatter(Xt[:, 0], Xt[:, 1], Xt[:,2], c=y, cmap=cmap, label = y, s=5, alpha=0.5)\n    else:\n        scatter = ax.scatter(Xt[:, 0], Xt[:, 1], c=y, cmap=cmap, label=y, s=5, alpha=0.5)\n    legend1 = ax.legend(*scatter.legend_elements(), loc = \'upper right\', title = \'Class\')\n    ax.add_artist(legend1)\n    fig.tight_layout()\n    if D3:\n        name = \'pca_projection_3d.png\'\n    else:\n        name = \'pca_projection.png\'\n    fig.savefig( os.path.join(prj.path, name))\n\n\ndef pca_explained_variance(prj, pca, img_only):\n    import matplotlib.pyplot as plt\n    exp = pca.explained_variance_ratio_.cumsum()\n\n    exp90, exp95, exp99 = -1, -1, -1\n    for i,j in enumerate(exp):\n        if j >= 0.9 and exp90 == -1:\n            exp90 = i\n        elif j >= 0.95 and exp95 == -1:\n            exp95 = i\n        elif j >= 0.99 and exp99 == -1:\n            exp99 = i\n\n    fig = plt.figure(\'PCA explained variance\')\n    ax = fig.add_subplot(1,1,1)\n    ax.set_xlabel(\'Principal component number\')\n    ax.set_ylabel(\'Explained variance\')\n    ax.plot(exp, \'-\')\n\n    # show 90, 95 and 99 % explanation\n    ax.axvline(x=exp90, label=\'%d PC > 90%%\' % exp90, linestyle = \'--\', c=\'k\')\n    ax.axvline(x=exp95, label=\'%d PC > 95%%\' % exp95, linestyle = \'--\', c=\'b\')\n    ax.axvline(x=exp99, label=\'%d PC > 99%%\' % exp99, linestyle = \'--\', c=\'r\')\n    ax.legend(title = \'Required components\')\n\n    fig.tight_layout()\n    fig.savefig(os.path.join(prj.path, \'pca_explained_ratio.png\'))\n\n\ndef correlation_matrix(prj, corr, img_only):\n    import numpy as np\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n\n    sns.set(style=""white"")\n    mask = np.zeros_like(corr, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n\n    fig, ax = plt.subplots()\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n    sns.heatmap(corr, square = True, mask=mask, cmap=cmap,cbar_kws={""shrink"": .7})\n    fig.savefig(os.path.join(prj.path, \'corr_matrix.png\'))\n\ndef show(img_only):\n    if not img_only:\n        import matplotlib.pyplot as plt\n        plt.show()\n\n'"
ergo/actions/__init__.py,0,b''
ergo/actions/clean.py,0,"b'import os\nimport argparse\n\nfrom ergo.core.utils import clean_if_exist\n\ndef parse_args(argv):\n    parser = argparse.ArgumentParser(prog=""ergo clean"", description=""Clean a project from temporary datasets and optionally reset it to its initial state."",\n            formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(""path"", help=""Path of the project to clean."")\n    parser.add_argument( ""-a"", ""--all"", dest=""all"", action=""store_true"", default=False,\n        help=""Remove model weights and training data."")\n\n    args = parser.parse_args(argv)\n    return args\n\ndef action_clean(argc, argv):\n    args = parse_args(argv)\n    files = [ \\\n        \'data-train.csv\',\n        \'data-test.csv\',\n        \'data-validation.csv\',\n        \'data-train.pkl\',\n        \'data-test.pkl\',\n        \'data-validation.pkl\']\n\n    if args.all:\n        files += [ \\\n            \'__pycache__\',\n            \'logs\',\n            \'model.yml\',\n            \'model.h5\',\n            \'model.fdeep\',\n            \'model.stats\', # legacy\n            \'model.png\', # legacy\n            \'test_cm.png\',\n            \'training_cm.png\',\n            \'validation_cm.png\',\n            \'history.png\',\n            \'roc.png\',\n            \'stats.txt\',\n            \'stats.json\',\n            \'history.json\',\n            \'pca_projection.png\',\n            \'pca_explained_ratio.png\',\n            \'corr_matrix.png\'\n        ]\n    \n    clean_if_exist(args.path, files)\n'"
ergo/actions/compare.py,0,"b'import os\nimport json\nimport argparse\nimport logging as log\nimport numpy as np\nimport tempfile\n\nfrom terminaltables import AsciiTable\n\nfrom ergo.project import Project\n\n\ndef parse_args(argv):\n    parser = argparse.ArgumentParser(prog=""ergo cmp"",\n                                     description=""Compare the performances of two models against a given dataset."",\n                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(""path_1"", help=""Path of the first project to compare."")\n    parser.add_argument(""path_2"", help=""Path of the second project to compare."")\n\n    parser.add_argument(""-d"", ""--dataset"", dest=""dataset"", action=""store"", type=str, required=True,\n                        help=""The dataset file to use to compare the models."")\n    parser.add_argument(""-j"", ""--to-json"", dest=""to_json"", action=""store"", type=str, required=False,\n                        help=""Output the comparision results to this json file."")\n\n    args = parser.parse_args(argv)\n    return args\n\n\ndef red(s):\n    return ""\\033[31m"" + s + ""\\033[0m""\n\n\ndef green(s):\n    return ""\\033[32m"" + s + ""\\033[0m""\n\n\n# https://stackoverflow.com/questions/11942364/typeerror-integer-is-not-json-serializable-when-serializing-json-in-python\ndef default(o):\n    if isinstance(o, np.int64): return int(o)\n    raise TypeError\n\n\ndef generate_reduced_dataset(dataset, size=10):\n    temp_name = next(tempfile._get_candidate_names())\n    tmpfile = open(temp_name, \'w\')\n    count = 0\n    with open(dataset, \'r\') as inpfile:\n        for line in inpfile: count += 1\n\n    size = min(count, 100)\n    size = max(count // 100, size)\n    log.info(""creating temporary dataset %s of size %d"", temp_name, size)\n\n    with open(dataset, \'r\') as inpfile:\n        p = float(size) / count\n        # assuming header:\n        tmpfile.write(inpfile.readline())\n        for i in inpfile:\n            line = inpfile.readline()\n            if np.random.random() < p:\n                tmpfile.write(line)\n    tmpfile.close()\n    return temp_name\n\n\ndef are_preparation_equal(projects, dataset):\n    inp_shape = None\n    for prj in projects:\n        if inp_shape is None:\n            inp_shape = prj.model.input_shape\n        elif inp_shape != prj.model.input_shape:\n            return False\n        prj.prepare(dataset, 0.0, 0.0, False)\n    return compare_datasets(projects[0].dataset, projects[1].dataset)\n\n\ndef compare_datasets(ds1, ds2):\n    if ds1.is_flat:\n        return np.array_equal(ds1.X, ds2.X)\n    for i, j in zip(ds1.X, ds2.X):\n        if not np.array_equal(i, j):\n            return False\n    return True\n\n\ndef action_compare(argc, argv):\n    args = parse_args(argv)\n    metrics = {}\n    projects = { \\\n        args.path_1: None,\n        args.path_2: None,\n    }\n    ref = None\n    inp_shape = None\n    out_shape = None\n    is_prepared = None\n    prjs = []\n\n    for path in projects:\n        prj = Project(path)\n        err = prj.load()\n        if err is not None:\n            log.error(""error while loading project %s: %s"", path, err)\n            quit()\n        prjs.append(prj)\n        if not is_prepared:\n            is_prepared = True\n        else:\n            small_dataset = generate_reduced_dataset(args.dataset)\n            are_equal = are_preparation_equal(prjs, small_dataset)\n            log.info(""deleting temporal file %s"", small_dataset)\n            os.remove(small_dataset)\n        if out_shape is None:\n            out_shape = prj.model.output_shape\n        elif out_shape != prj.model.output_shape:\n            log.error(""model %s output shape is %s, expected %s"", path, prj.model.output_shape, out_shape)\n            quit()\n        projects[path] = prj\n\n    for prj, path in zip(prjs, projects):\n        prj = Project(path)\n        err = prj.load()\n\n        if err is not None:\n            log.error(""error while loading project %s: %s"", path, err)\n            quit()\n\n        if ref is None:\n            prj.prepare(args.dataset, 0, 0, False)\n            ref = prj\n            is_prepared = True\n        else:\n            if are_equal:\n                log.info(""Projects use same prepare.py file ..."")\n                prj.dataset.X, prj.dataset.Y, prj.dataset.n_labels = ref.dataset.X.copy(), ref.dataset.Y.copy(), ref.dataset.n_labels\n            else:\n                log.info(""Projects use different prepare.py files, reloading dataset ..."")\n                prj.prepare(args.dataset, 0., 0., False)\n\n        # TODO: Run in parallel?\n        log.debug(""running %s ..."", path)\n        metrics[path] = prj.accuracy_for(prj.dataset.X, prj.dataset.Y, repo_as_dict=True)\n\n    prev = None\n    for path, m in metrics.items():\n        if prev is None:\n            prev = m\n            continue\n\n        ref_repo, ref_cm = prev\n        new_repo, new_cm = m\n        diffs = {\n            \'report\': [],\n            \'cm\': [],\n            \'cm_stats\': {}\n        }\n\n        table = [[""Name"", ""Ref"", ""New"", ""Delta""]]\n        for label, ref_run in ref_repo.items():\n            for name, ref_value in ref_run.items():\n                new_value = new_repo[label][name]\n                if new_value != ref_value:\n                    delta = new_value - ref_value\n                    sign, fn = (\'+\', green) if delta >= 0 else (\'\', red)\n                    diffs[\'report\'].append({\n                        \'name\': \'%s / %s\' % (label, name),\n                        \'delta\': delta,\n                    })\n                    table.append([ \\\n                        ""%s / %s"" % (label, name),\n                        ""%.2f"" % ref_value,\n                        ""%.2f"" % new_value,\n                        fn(""%s%.2f"" % (sign, delta))])\n        print("""")\n        print(AsciiTable(table).table)\n\n        heads = [""""]\n        for i in range(0, ref_cm.shape[0]):\n            heads.append(""class %d"" % i)\n\n        table = [heads]\n        total = 0\n        impr = 0\n        regr = 0\n        for i in range(0, ref_cm.shape[0]):\n            row = [""class %d"" % i]\n            row_diffs = []\n            for j in range(0, ref_cm.shape[1]):\n                ref_v = ref_cm[i][j]\n                new_v = new_cm[i][j]\n                total = total + new_v\n                delta = new_v - ref_v\n\n                if ref_v != new_v:\n                    sign = \'+\' if delta >= 0 else \'\'\n\n                    if i == j:\n                        fn = green if delta >= 0 else red\n                    else:\n                        fn = red if delta >= 0 else green\n\n                    if fn == green:\n                        impr += abs(delta)\n                    else:\n                        regr += abs(delta)\n\n                    cell = fn(""%d (%s%d)"" % (new_v, sign, delta))\n                else:\n                    cell = ""%d"" % ref_v\n\n                row.append(cell)\n                row_diffs.append(delta)\n\n            diffs[\'cm\'].append(row_diffs)\n            table.append(row)\n\n        print("""")\n        print(AsciiTable(table).table)\n\n        diffs[\'cm_stats\'] = {\n            \'improvements\': {\n                \'total\': impr,\n                \'perc\': impr / float(total) * 100.0\n            },\n            \'regressions\': {\n                \'total\': regr,\n                \'perc\': regr / float(total) * 100.0\n            }\n        }\n\n        print("""")\n        print(""Improvements: %d ( %.2f %% )"" % (impr, impr / float(total) * 100.0))\n        print(""Regressions : %d ( %.2f %% )"" % (regr, regr / float(total) * 100.0))\n\n        if args.to_json is not None:\n            print("""")\n            log.info(""creating %s ..."", args.to_json)\n            with open(args.to_json, \'w+\') as fp:\n                json.dump(diffs, fp, default=default)\n'"
ergo/actions/create.py,0,"b'import sys\nimport os\nimport argparse\nimport logging as log\n\nfrom ergo.project import Project\nfrom ergo.templates import Templates\n\ndef parse_args(argv):\n    parser = argparse.ArgumentParser(prog=""ergo create"", description=""Create a new ergo project."",\n            formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(""path"", help=""Path of the project to create."")\n    \n    parser.add_argument(""-i"", ""--inputs"", dest=""num_inputs"", action=""store"", type=int, default=10,\n        help=""Number of inputs of the model."")\n    parser.add_argument(""-o"", ""--outputs"", dest=""num_outputs"", action=""store"", type=int, default=2,\n        help=""Number of outputs of the model."")\n    parser.add_argument(""-l"", ""--layers"", dest=""hidden"", action=""store"", type=str, default=""30, 30"",\n        help=""Comma separated list of positive integers, one per each hidden layer representing its size."")\n    parser.add_argument(""-b"", ""--batch-size"", dest=""batch_size"", action=""store"", type=int, default=64,\n        help=""Batch size parameter for training."")\n    parser.add_argument(""-e"", ""--epochs"", dest=""max_epochs"", action=""store"", type=int, default=50,\n        help=""Maximum number of epochs to train the model."")\n\n    args = parser.parse_args(argv)\n    return args\n\ndef action_create(argc, argv):\n    args = parse_args(argv)\n    if os.path.exists(args.path):\n        log.error(""path %s already exists"" % args.path)\n        quit()\n\n    check = [n for n in [int(s.strip()) for s in args.hidden.split(\',\') if s.strip() != """"] if n > 0]\n    if len(check) < 1:\n        log.error(""the --hidden argument must be a comma separated list of at least one positive integer"")\n        quit()\n\n    ctx = {\n        \'NUM_INPUTS\': args.num_inputs,\n        \'HIDDEN\':     \', \'.join([str(n) for n in check]),\n        \'NUM_OUTPUTS\': args.num_outputs,\n        \'BATCH_SIZE\': args.batch_size,\n        \'MAX_EPOCHS\': args.max_epochs,\n    }\n\n    log.info(""initializing project %s with ANN %d(%s)%d ..."", args.path, ctx[\'NUM_INPUTS\'], ctx[\'HIDDEN\'], ctx[\'NUM_OUTPUTS\'])\n    os.makedirs(args.path, exist_ok=True)\n    for tpl in Templates:\n        log.info( ""creating %s"", tpl.name)\n        with open( os.path.join(args.path, tpl.name), \'wt\' ) as fp:\n            data = tpl.compile(ctx)  \n            fp.write(data)\n'"
ergo/actions/encode.py,0,"b'import os\nimport time\nimport sys\nimport glob\nimport argparse\nimport logging as log\nimport multiprocessing\n\nfrom ergo.project import Project\nfrom ergo.core.queue import TaskQueue\n\ndef parse_args(argv):\n    parser = argparse.ArgumentParser(prog=""ergo encode"", description=""Encode one or more files to vectors and create or update a csv dataset for training."",\n            formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(""project"", help=""The path containing the model definition."")\n    parser.add_argument(""path"", help=""Path of a single file or of a folder of files."")\n\n    parser.add_argument( ""-l"", ""--label"", dest=""label"", default=\'auto\',\n        help=""The class of the file(s) or \'auto\' to use the name of the containing folder."")\n    parser.add_argument( ""-o"", ""--output"", dest=""output"", default=\'dataset.csv\',\n        help=""Output CSV file names."")\n    parser.add_argument( ""-f"", ""--filter"", dest=""filter"", default=\'*.*\',\n        help=""If PATH is a folder, this flag determines which files are going to be selected."")\n    parser.add_argument( ""-m"", ""--multi"", dest=""multi"", default=False, action=""store_true"",\n        help=""If PATH is a single file, this flag will enable line by line reading of multiple inputs from it."")\n    parser.add_argument( ""-w"", ""--workers"", dest=""workers"", default=0, type=int,\n        help=""Number of concurrent workers to use for encoding, or zero to run two workers per available CPU core."")\n    parser.add_argument( ""-d"", ""--delete"", dest=""delete"", default=False, action=""store_true"",\n        help=""Delete each file after encoding it."")\n\n    args = parser.parse_args(argv)\n    return args\n\n# given the arguments setup and a path, return the label\ndef label_of(args, path):\n    if args.label == \'auto\':\n        return os.path.basename(os.path.dirname(path))\n    else:\n        return args.label\n\nprogress_at = None\nprev_done = 0\nprev_speed = 0\n\ndef get_speed(done):\n    global progress_at, prev_done, prev_speed\n\n    speed = 0\n    now   = time.time()\n    if progress_at is not None:\n        delta_v = int(done - prev_done)\n        if delta_v >= 10:\n            delta_t = now - progress_at\n            speed = delta_v / delta_t\n        else:\n            speed = prev_speed\n            now   = progress_at\n            done  = prev_done\n\n    progress_at = now\n    prev_done = done\n    prev_speed = speed\n\n    return speed\n\n# simple progress bar\ndef on_progress(done, total):\n    maxsz = 80\n    perc  = done / total\n    sp    = "" "" if perc > 0 else """"\n    bar   = (""\xe2\x96\x88"" * int(maxsz * perc)) + sp\n\n    sys.stdout.write(""\\r%d/%d (%d/s) %s%.1f%%"" % (done, total, get_speed(done), bar, perc * 100.0))\n    if perc == 1.0:\n        sys.stdout.write(""\\n"")\n\n# wait for lines on the results queue and append them to filepath\ndef appender(filepath, total, results):\n    log.debug(""file appender for %s started ..."", filepath)\n\n    print()\n    with open(filepath, \'a+t\') as fp:\n        done = 0\n        while True: \n            on_progress(done, total)\n\n            line = results.get()\n            if line is None:\n                break\n\n            fp.write(line.strip() + ""\\n"")\n            done += 1\n\n# use the project prepare_input function to encode a single\n# input into a CSV line\ndef parse_input(prj, inp, label, results, do_delete = False):\n    log.debug(""encoding \'%s\' as \'%s\' ..."", inp, label)\n    x = prj.logic.prepare_input(inp, is_encoding = True)\n    results.put(""%s,%s"" % (str(label), \',\'.join(map(str,x))))\n    if do_delete and os.path.isfile(inp): \n        os.remove(inp)\n\ndef action_encode(argc, argv):\n    args = parse_args(argv)\n    if not os.path.exists(args.path):\n        log.error(""%s does not exist."", args.path)\n        quit()\n\n    prj = Project(args.project)\n    err = prj.load()\n    if err is not None:\n        log.error(""error while loading project: %s"", err)\n        quit()\n    \n    args.label = args.label.strip().lower()\n    log.info(""using %s labeling"", \'auto\' if args.label == \'auto\' else \'hardcoded\')\n\n    inputs = []\n    if os.path.isdir(args.path):\n        in_files = []\n        if args.label == \'auto\':\n            # the label is inferred from the dirname, so we expect\n            # args.path to contain multiple subfolders\n            for subfolder in glob.glob(os.path.join(args.path, ""*"")):\n                log.info(""enumerating %s ..."", subfolder)\n                in_filter = os.path.join(subfolder, args.filter)\n                in_sub    = glob.glob(in_filter)\n                n_sub     = len(in_sub)\n                if n_sub > 0:\n                    log.info(""collected %d inputs from %s"", n_sub, subfolder)\n                    in_files.extend(in_sub)\n        else:\n            # grab files directly from args.path\n            in_filter = os.path.join(args.path, args.filter)\n            in_files.extend(glob.glob(in_filter))\n            log.info(""collected %d inputs from %s"", len(in_files), args.path)\n\n        log.info(""labeling %d files ..."", len(in_files))\n        for filepath in in_files:\n            if os.path.isfile(filepath):\n                inputs.append((label_of(args, filepath), filepath))\n\n    elif args.multi:\n        log.info(""parsing multiple inputs from %s ..."", args.path)\n        label = label_of(args, args.path)\n        with open(args.path, \'rt\') as fp:\n            for line in fp:\n                inputs.append((label, line))\n\n    else:\n        label = label_of(args, args.path)\n        inputs.append((label, args.path))\n\n    # one encoding queue that pushes to another queue that centralizes\n    # append operations to a single writer process\n    num_in = len(inputs)\n    enc_q = TaskQueue(\'encoding\', args.workers)\n    res_q = multiprocessing.Queue()\n    app_p = multiprocessing.Process(target=appender, args=(args.output, num_in, res_q))\n\n    # open the output file and start waiting for lines to append\n    app_p.start()\n\n    log.info(""encoding %d inputs to %s ..."", num_in, args.output)\n    for (y, x) in inputs:\n        enc_q.add_task(parse_input, prj, x, y, res_q, args.delete)\n\n    # wait for all inputs to be encoded\n    enc_q.join()\n    # let the writer know there are no more inputs to read\n    res_q.put(None)\n    # wait for the writer to finish\n    app_p.join()\n\n'"
ergo/actions/explore.py,0,"b'import argparse\nfrom terminaltables import AsciiTable\nimport logging as log\nimport numpy as np\nimport pandas as pd\nnp.seterr(divide=\'ignore\', invalid=\'ignore\')\n\nfrom ergo.project import Project\nimport ergo.views as views\n\ndef validate_ratio(args):\n    if args.ratio > 1.0 or args.ratio <= 0:\n        log.error(""ratio must be in the (0.0, 1.0] interval"")\n        quit()\n\ndef parse_args(argv):\n    parser = argparse.ArgumentParser(prog=""ergo explore"", description=""Explore dataset properties"",\n            formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(""path"", help=""Path of the project to explore."")\n    parser.add_argument(""-a"", ""--attributes"", dest=""attributes"", action=""store"", type=str, required=False,\n                        help=""Optional file containing the attribute names, one per line."")\n    parser.add_argument(""-r"", ""--ratio"", dest=""ratio"", action=""store"", type=validate_ratio, required=False, default=1.0,\n                        help=""Size of the subset of the dataset to use in the (0,1] interval."")\n    parser.add_argument(""-d"", ""--dataset"", dest=""dataset"", action=""store"", type=str, required=True,\n                        help=""Dataset file to use."")\n    parser.add_argument(""--img-only"", dest=""img_only"", default=False, action=""store_true"",\n                        help=""Save plots as PNG files but don\'t show them in a UI."")\n    parser.add_argument(""-s"", dest=""stats"", action=""store_true"", default=False,\n                        help=""Show dataset statistics"")\n    parser.add_argument(""-c"", dest=""correlations"", action=""store_true"", default=False,\n                        help=""Show correlation tables and graphs"")\n    parser.add_argument(""-p"", dest=""pca"", action=""store_true"", default=False,\n                        help=""Calculate pca decomposition and explained variance"")\n    parser.add_argument(""-k"", dest=""cluster"", action=""store_true"", default=False,\n                        help=""Perform clustering analysis"")\n    parser.add_argument(""--algorithm"", dest=""cluster_alg"", choices=[\'kmeans\', \'dbscan\'], default=\'kmeans\', const=\'kmeans\', nargs=\'?\',\n                        help=""Algorithm for clustering analysis"")\n    parser.add_argument(""-n"", dest=""nclusters"", action=""store"", required = False, type=float,\n                        help=""Number of clusters for Kmeans algorithm"")\n    parser.add_argument(""--nmax"", dest=""nmaxclusters"", action=""store"", required=False, type=int,\n                        help=""Perform inertia analysis using nmax clusters at most"")\n    parser.add_argument(""--3D"", dest=""D3"", action=""store_true"", default=False,\n                        help=""Plot 3D projections of the data"")\n    parser.add_argument(""--all"", dest=""all"", action=""store_true"", default=False,\n                        help=""Process all capabilities of ergo explore (can be time consuming deppending on dataset"")\n    parser.add_argument(""-w"", ""--workers"", dest=""workers"", action=""store"", type=int, default=0,\n                        help=""If 0, all the algorithms will run a single thread. Otherwise all algorithms will use w number of jobs ""+ \\\n                             ""to distribute the computation among, -1 to use the number of logical CPU cores available."")\n    return parser.parse_args(argv)\n\ndef red(s):\n    return ""\\033[31m"" + s + ""\\033[0m""\n\ndef terminal(s):\n    return s\n\ndef get_attributes(filename, ncols):\n    attributes = []\n    if filename is not None:\n        with open(filename) as f:\n            attributes = f.readlines()\n        attributes = [name.strip() for name in attributes]\n    else:\n        attributes = [""feature %d"" % i for i in range(0, ncols)]\n\n    return attributes\n\nprj = None\nncols = 0\nnrows = 0\nattributes = None\nn_jobs = 1\n\n\ndef compute_correlations_with_target (X,y):\n    global nrows, ncols, attributes\n    y = np.argmax(y, axis=1)\n    corr = [(attributes[i], np.corrcoef(X[:, i], y)[0,1], i)\n            for i in range(0, ncols)]\n    corr = [(i, j, k) for i, j, k in corr if not np.isnan(j)]\n    corr = sorted(corr, key=lambda x: abs(x[1]), reverse=True)\n    return corr\n\n\ndef print_target_correlation_table(corr, min_corr = 0.3):\n    table = [(""Column"", ""Feature"", ""Correlation with target"")]\n    not_relevant = 0\n    for at, c, idx in corr:\n        if abs(c) < min_corr:\n            not_relevant += 1\n            continue\n        row = ( ""%d"" % idx, ""%s"" % at, ""%.4f"" % c)\n        table.append(row)\n\n    print("""")\n    print(AsciiTable(table).table)\n    print("""")\n\n    if not_relevant > 0:\n        log.info(""%d features have correlation lower than %f with target attribute."", not_relevant, min_corr)\n\ndef calculate_pca(X):\n    from sklearn.decomposition import PCA\n    dec = PCA()\n    dec.fit(X)\n    return dec\n\n\ndef calculate_corr(X):\n    global attributes\n    return pd.DataFrame(np.corrcoef(X, rowvar=False), columns=attributes, index=attributes)\n\ndef is_in_table(table, entry):\n    for row in table:\n        if row[1] == entry[0] and row[0] == entry[1]:\n            return True\n    return False\n\n\ndef print_correlation_table(corr, min_corr=0.6):\n    abs_corr = corr.abs().unstack()\n    sorted_corr = abs_corr.sort_values(kind = ""quicksort"", ascending=False)\n    table = [(""Feature"", ""Feature"", ""Correlation"")]\n    for idx in sorted_corr.index:\n        if idx[0] == idx[1]:\n            continue\n        if sorted_corr.loc[idx] <= min_corr:\n            break\n        if is_in_table(table, idx):\n            continue\n        table.append(( idx[0], idx[1], ""%.3f"" % sorted_corr.loc[idx]))\n    print("""")\n    log.info(""Showing variables with a correlation higher than %f"" % min_corr)\n    print("""")\n    print(AsciiTable(table).table)\n    print("""")\n\n\ndef print_stats_table(X):\n    global attributes\n    minv = X.min(axis = 0)\n    maxv = X.max(axis = 0)\n    stdv = X.std(axis = 0)\n\n    table = [(""Feature"", ""Min value"", ""Max value"", ""Standard deviation"")]\n    constant = []\n    unormalized = []\n    for a, mi, ma, st in zip(attributes, minv, maxv, stdv ):\n        color = terminal\n        if mi == ma:\n            constant.append(a)\n            continue\n        if mi < -1 or ma > 1:\n            color = red\n            unormalized.append(color(a))\n\n        table.append( (color(a), color(""%.2e"" % mi) , color(""%.2e"" % ma), color( ""%.2e""% st)))\n\n    print("""")\n    log.info(""Features distribution:"")\n    print("""")\n    print(AsciiTable(table).table)\n    print("""")\n    log.warning(""The following attributes have constant values: %s"" % \', \'.join(constant))\n    print("""")\n    log.warning(""The following features are not normalized: %s"" % \', \'.join(unormalized))\n\n\ndef kmeans_clustering(X, n_clusters):\n    global n_jobs\n    from sklearn.cluster import KMeans\n    km = KMeans(n_clusters = n_clusters, n_jobs=n_jobs)\n    km.fit(X)\n    return km\n\n\ndef dbscan_clustering(X, var):\n    global n_jobs\n    from sklearn.cluster import DBSCAN\n    db = DBSCAN(var, min_samples=10, n_jobs=n_jobs)\n    db.fit(X)\n    return db\n\n\ndef n_clusters_analysis(X, nmax = 10, nmin = 2):\n    global prj, n_jobs\n    from sklearn.cluster import KMeans\n\n    inertia = []\n    for n in range(nmin, nmax + 1):\n        log.info(""fitting %d clusters to data"" % n)\n        km = KMeans(n_clusters = n, n_jobs=n_jobs).fit(X)\n        inertia.append(km.inertia_)\n    views.plot_intertia(prj, range(nmin, nmax + 1), inertia)\n\n\ndef action_explore(argc, argv):\n    global prj, nrows, ncols, attributes, n_jobs\n\n    args     = parse_args(argv)\n\n    if args.all:\n        args.pca = True\n        args.correlations = True\n        args.stats = True\n        args.cluster = True\n        args.D3 = True\n\n    if args.workers == -1:\n        import multiprocessing\n        n_jobs = multiprocessing.cpu_count()\n    elif args.workers != 0:\n        n_jobs = args.workers\n    log.info(""using %d workers"" % n_jobs)\n\n    if args.nclusters and not args.cluster:\n        log.warning(""number of clusters specified but clustering won\'t be perfomed"")\n\n    if not (args.pca or args.correlations or args.stats or args.cluster):\n        log.error(""No exploration action was specified"")\n        print("""")\n        parse_args([""-h""])\n        quit()\n\n    prj = Project(args.path)\n    err = prj.load()\n    if err is not None:\n        log.error(""error while loading project: %s"", err)\n        quit()\n\n    prj.prepare(args.dataset, 0.0, 0.0)\n    if not prj.dataset.is_flat:\n        log.error(""data exploration can only be applied to flat inputs"")\n        quit()\n\n    X, y = prj.dataset.subsample(args.ratio)\n    nrows, ncols = X.shape\n    attributes = get_attributes(args.attributes, ncols)\n\n    if args.correlations:\n        log.info(""computing correlations of each feature with target"")\n        corr = compute_correlations_with_target(X,y)\n        print_target_correlation_table(corr)\n        log.info(""computing features crosscorrelation"")\n        corr = calculate_corr(X)\n        print_correlation_table(corr, min_corr=0.7)\n        views.correlation_matrix(prj, corr, args.img_only)\n\n    if args.pca:\n        log.info(""computing pca"")\n        pca = calculate_pca(X)\n        log.info(""computing pca projection"")\n        views.pca_projection(prj, pca, X, y, False)\n        if args.D3:\n            views.pca_projection(prj, pca, X, y, args.D3)\n        views.pca_explained_variance(prj, pca, args.img_only)\n\n    if args.stats:\n        log.info(""computing features stats"")\n        print_stats_table(X)\n\n    inertia = False\n    if args.cluster:\n        if args.cluster_alg == \'kmeans\':\n            cluster_alg = kmeans_clustering\n            if not args.nclusters:\n                args.nclusters = len(set(np.argmax(y, axis=1)))\n            args.nclusters = int(args.nclusters)\n            if args.nmaxclusters:\n                log.info(""performing inertia analysis with clusters in the range (%d, %d)"" % (args.nclusters, args.nmaxclusters))\n                inertia = True\n                n_clusters_analysis(X, args.nmaxclusters, args.nclusters)\n            else:\n                log.info(""computing kmeans clustering with k=%d"" % args.nclusters)\n        elif args.cluster_alg == \'dbscan\':\n            cluster_alg = dbscan_clustering\n            if not args.nclusters:\n                args.nclusters = 2\n            log.info(""computing dbscan clustering with eps=%f"" % args.nclusters)\n            if args.nmaxclusters:\n                log.warning(""nmax specified but not used. Inertia analysis only available for Kmeans."")\n        if not args.pca and not inertia:\n            log.info(""computing pca to plot clusters"")\n            pca = calculate_pca(X)\n        if not inertia:\n            ca = cluster_alg(X, args.nclusters)\n            if len(set(ca.labels_)) == 1:\n                log.error(""clustering failed. Check input parameter."")\n                quit()\n            views.plot_clusters(prj, pca, X, y, ca, False)\n            if args.D3:\n                views.plot_clusters(prj, pca, X, y, ca, args.D3)\n\n    views.show(args.img_only)\n'"
ergo/actions/info.py,2,"b'import argparse\nimport json\nimport sys\nstderr = sys.stderr\nsys.stderr = open(\'/dev/null\', \'w\')\nimport keras\nimport sklearn\nsys.stderr = stderr \n\nimport tensorflow as tf\nfrom tensorflow.python.client import device_lib\n\nfrom ergo.version import banner, __version__\n\ndef get_pads(devs):\n    namepad = 0\n    typepad = 0\n    for dev in devs:\n        lname = len(dev.name)\n        ltype = len(dev.device_type)\n\n        if lname > namepad:\n            namepad = lname\n\n        if ltype > typepad:\n            typepad = ltype\n\n    return namepad, typepad\n\n# https://stackoverflow.com/questions/1094841/reusable-library-to-get-human-readable-version-of-file-size\ndef mem_fmt(num, suffix=\'B\'):\n    num = int(num)\n    for unit in [\'\',\'Ki\',\'Mi\',\'Gi\',\'Ti\',\'Pi\',\'Ei\',\'Zi\']:\n        if abs(num) < 1024.0:\n            return ""%3.1f %s%s"" % (num, unit, suffix)\n        num /= 1024.0\n    return ""%.1f %s%s"" % (num, \'Yi\', suffix)\n\ndef parse_args(argv):\n    parser = argparse.ArgumentParser(prog=""ergo info"", description=""Print library versions and hardware info."",\n            formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(""-j"", ""--to-json"", dest=""to_json"", action=""store_true"", default=False,\n        help=""Output the information to json instead of text."")\n\n    args = parser.parse_args(argv)\n    return args\n\ndef action_info(argc, argv):\n    args = parse_args(argv)\n\n    if args.to_json is True:\n        devices = []\n        info = {\n            ""version"": __version__,\n            ""keras_version"": keras.__version__,\n            ""tf_version"": tf.__version__,\n            \'sklean_version\': sklearn.__version__,\n            ""devices"":[]\n        }\n        devs = device_lib.list_local_devices()\n        for dev in devs:\n            info[\'devices\'].append({\n                \'name\': dev.name,\n                \'type\': dev.device_type,\n                \'memory\': dev.memory_limit,\n                \'description\': dev.physical_device_desc\n            })\n        \n        print(json.dumps(info)) \n    else:\n        print(banner.strip(""\\n"") % (__version__, keras.__version__, tf.__version__, sklearn.__version__))\n        print("""")\n        print(""Hardware:\\n"")\n\n        devs = device_lib.list_local_devices()\n        npad, tpad  = get_pads(devs)\n        for dev in devs:\n            print( ""%s (%s) - %s"" % (dev.name.ljust(npad,\' \'), dev.device_type.ljust(tpad,\' \'), mem_fmt(dev.memory_limit)))\n'"
ergo/actions/optimize.py,0,"b'import os\nimport argparse\nimport logging as log\n\nfrom ergo.core.optimizer import optimize_dataset\nfrom ergo.dataset import Dataset\n\ndef probability(x):\n    x = float(x)\n    if x < 0 or x > 1:\n        raise argparse.ArgumentTypeError(""%r not in range [0.0 - 1.0]"" % x )\n    return x\n\ndef parse_args(argv):\n    parser = argparse.ArgumentParser(prog=""ergo optimize-dataset"", description=""Remove duplicates from the dataset and only allow them within a given reuse ratio."",\n            formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(""path"", help=""Path of the project to create."")\n\n    parser.add_argument(""-r"", ""--reuse-ratio"", dest=""reuse"", action=""store"", type=probability, default=0.15,\n        help=""Reuse ratio of duplicates in the [0,1] range."")\n    parser.add_argument(""-o"", ""--output"", dest=""output"", action=""store"", default=None,\n        help=""Output dataset file."")\n\n    args = parser.parse_args(argv)\n    return args\n\ndef action_optimize_dataset(argc, argv):\n    args = parse_args(argv)\n    path = os.path.abspath(args.path)\n    if not os.path.exists(path):\n        log.error(""dataset file %s does not exist"", path)\n        quit()\n    \n    optimize_dataset(path, args.reuse, args.output)\n'"
ergo/actions/prepare.py,0,"b'import argparse\nimport os\nimport logging as log\n\nfrom ergo.project import Project\nfrom ergo.core.utils import clean_if_exist\n\ndef probability(x):\n    x = float(x)\n    if x < 0 or x > 1:\n        raise argparse.ArgumentTypeError(""%r not in range [0.0 - 1.0]"" % x)\n    return x\n\n\ndef validate_args(args):\n    if args.dataset is not None and (not args.dataset.startswith(\'sum://\') and not os.path.exists(args.dataset)):\n        log.error(""file %s does not exist"" % args.dataset)\n        quit()\n\n    p_train = 1 - args.test - args.validation\n    if p_train < 0:\n        log.error(""validation and test proportion are bigger than 1"")\n        quit()\n    elif p_train < 0.5:\n        log.error(""using less than 50% of the dataset for training"")\n        quit()\n\ndef clean_dataset(path):\n    files = [ \\\n        \'data-train.csv\',\n        \'data-test.csv\',\n        \'data-validation.csv\',\n        \'data-train.pkl\',\n        \'data-test.pkl\',\n        \'data-validation.pkl\']\n\n    clean_if_exist(path, files)\n\ndef parse_args(argv):\n    parser = argparse.ArgumentParser(prog=""ergo prepare"", description=""Create train, test and validation sets."",\n                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(""path"", help=""The path containing the model definition."")\n\n    parser.add_argument(""-d"", ""--dataset"", action=""store"", dest=""dataset"",\n                        help=""Path of the dataset to use or a SUM server address. Leave empty to reuse previously generated subsets."")\n    parser.add_argument(""-t"", ""--test"", action=""store"", dest=""test"", type=probability, default=0.15,\n                        help=""Proportion of the test set in the (0,1] interval."")\n    parser.add_argument(""-v"", ""--validation"", action=""store"", dest=""validation"", type=probability, default=0.15,\n                        help=""Proportion of the validation set in the (0,1] interval."")\n    parser.add_argument(""--no-shuffle"", dest=""no_shuffle"", action=""store_true"", default=False,\n                        help=""Do not shuffle dataset during preparation."")\n\n    args = parser.parse_args(argv)\n    validate_args(args)\n    return args\n\n\ndef action_prepare(argc, argv):\n    args = parse_args(argv)\n    prj = Project(args.path)\n    err = prj.load()\n    if err is not None:\n        log.error(""error while loading project: %s"", err)\n        quit()\n\n    if args.dataset is None:\n        log.error(""no --dataset argument specified"", args.path)\n        quit()\n\n    if prj.dataset.exists():\n        log.info(""removing previously generated datasets"")\n        clean_dataset(args.path)\n\n    prj.dataset.do_save = True\n    prj.prepare(args.dataset, args.test, args.validation, not args.no_shuffle)'"
ergo/actions/relevance.py,0,"b'import argparse\nimport json\nimport logging as log\nimport time\nimport numpy as np\n\nfrom terminaltables import AsciiTable\n\nfrom ergo.core.queue import TaskQueue\nfrom ergo.project import Project\n\n# https://stackoverflow.com/questions/11942364/typeerror-integer-is-not-json-serializable-when-serializing-json-in-python\ndef default(o):\n    if isinstance(o, np.int64): return int(o)\n    raise TypeError\n\ndef validate_args(args):\n    if args.ratio > 1.0 or args.ratio <= 0:\n        log.error(""ratio must be in the (0.0, 1.0] interval"")\n        quit()\n\ndef parse_args(argv):\n    parser = argparse.ArgumentParser(prog=""ergo relevance"", description=""Compute the relevance of each feature of the dataset by differential evaluation."",\n            formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(""path"", help=""Path of the project."")\n\n    parser.add_argument(""-d"", ""--dataset"", dest=""dataset"", action=""store"", type=str, required=True,\n        help=""Dataset file to use."")\n    parser.add_argument(""-m"", ""--metric"", dest=""metric"", action=""store"", type=str, default=\'precision\',\n        help=""Which metric to track changes of while performing differential evaluation."")\n    parser.add_argument(""-a"", ""--attributes"", dest=""attributes"", action=""store"", type=str, required=False,\n        help=""Optional file containing the attribute names, one per line."")\n    parser.add_argument(""-r"", ""--ratio"", dest=""ratio"", action=""store"", type=float, required=False, default=1.0,\n        help=""Size of the subset of the dataset to use in the (0,1] interval."")\n    parser.add_argument(""-j"", ""--to-json"", dest=""to_json"", action=""store"", type=str, required=False,\n        help=""Output the relevances to this json file."")\n    parser.add_argument(""-w"", ""--workers"", dest=""workers"", action=""store"", type=int, default=0,\n        help=""If 0, the algorithm will run a single thread iteratively, otherwise create a number of workers "" + \\\n             ""to distribute the computation among, -1 to use the number of logical CPU cores available. WARNING: "" + \\\n             ""The dataset will be copied in memory for each worker.""   )\n\n    args = parser.parse_args(argv)\n    return args\n\ndef get_attributes(filename, ncols):\n    attributes = []\n    if filename is not None:\n        with open(filename) as f:\n            attributes = f.readlines()\n        attributes = [name.strip() for name in attributes]\n    else:\n        attributes = [""feature %d"" % i for i in range(0, ncols)]\n\n    return attributes\n\ndef zeroize_feature(X, col, is_scalar_input):\n    if is_scalar_input:\n        backup = X[:, col].copy()\n        X[:,col] = 0\n    else:\n        backup = X[col].copy()\n        X[col] = np.zeros_like(backup)\n\n    return backup\n\ndef restore_feature(X, col, backup, is_scalar_input):\n    if is_scalar_input:\n        X[:,col] = backup\n    else:\n        X[col] = backup\n\n\nprj    = None\ndeltas = []\ntot    = 0\nncols  = 0\nnrows  = 0\nstart  = None\nspeed  = 0\nattributes = None\n\ndef run_inference_without(X, y, col, flat, ref, metric):\n    global prj, deltas, tot, start, speed, nrows, ncols, attributes\n\n    log.info(""[%.2f evals/s] computing relevance for attribute [%d/%d] %s ..."", speed, col + 1, ncols, attributes[col])\n\n    copy = X.copy()\n    zeroize_feature(copy, col, flat)\n\n    start = time.time()\n\n    accu, cm = prj.accuracy_for(copy, y, repo_as_dict = True)\n\n    speed = (1.0 / (time.time() - start)) * nrows\n\n    delta = ref - accu[\'weighted avg\'][metric]\n    tot  += delta\n\n    deltas.append((col, delta))\n\ndef action_relevance(argc, argv):\n    global prj, deltas, tot, start, speed, nrows, ncols, attributes\n\n    args = parse_args(argv)\n    prj = Project(args.path)\n    err = prj.load()\n    if err is not None:\n        log.error(""error while loading project: %s"", err)\n        quit()\n    elif not prj.is_trained():\n        log.error(""no trained Keras model found for this project"")\n        quit()\n\n    prj.prepare(args.dataset, 0.0, 0.0)\n\n    # one single worker in blocking mode = serial\n    if args.workers == 0:\n        args.workers = 1\n\n    X, y         = prj.dataset.subsample(args.ratio)\n    nrows, ncols = X.shape if prj.dataset.is_flat else (X[0].shape[0], len(X))\n    attributes   = get_attributes(args.attributes, ncols)\n    queue        = TaskQueue(\'relevance\', num_workers=args.workers, blocking=True)\n   \n    if args.workers == 1:\n        log.info(""computing relevance of %d attributes on %d samples using \'%s\' metric (slow mode) ..."", ncols, nrows, args.metric)\n    else:\n        log.info(""computing relevance of %d attributes on %d samples using \'%s\' metric (parallel with %d workers) ..."", ncols, nrows, args.metric, queue.num_workers)\n\n    start = time.time()\n    ref_accu, ref_cm = prj.accuracy_for(X, y, repo_as_dict = True)\n    speed = (1.0 / (time.time() - start)) * nrows\n\n    for col in range(0, ncols):\n        queue.add_task( run_inference_without, \n                X, y, col, prj.dataset.is_flat, \n                ref_accu[\'weighted avg\'][args.metric], \n                args.metric) \n\n    # wait for all inferences to finish\n    queue.join()\n    # sort relevances by absolute value\n    deltas = sorted(deltas, key = lambda x: abs(x[1]), reverse = True)\n\n    rels     = []\n    num_zero = 0\n    table    = [(""Column"", ""Feature"", ""Relevance"")]\n\n    for delta in deltas:\n        col, d = delta\n        colname = attributes[col]\n        rel = {\n            ""attribute"": colname,\n            ""index"": col, \n            ""relevance"": 0.0     \n        }\n\n        if d != 0.0:\n            relevance = (d / tot) * 100.0\n            row       = (""%d"" % col, attributes[col], ""%.2f%%"" % relevance)\n            row       = [""\\033[31m%s\\033[0m"" % e for e in row] if relevance < 0.0 else row\n            table.append(row)\n            rel[\'relevance\'] = relevance\n        else:\n            num_zero += 1\n\n        rels.append(rel)\n\n    print("""")\n    print(AsciiTable(table).table)\n    print("""")\n\n    if num_zero > 0:\n        log.info(""%d features have 0 relevance."", num_zero)\n\n    if args.to_json is not None:\n        print("""")\n        log.info(""creating %s ..."", args.to_json)\n        with open(args.to_json, \'w+\') as fp:\n            json.dump(rels, fp, default=default)\n'"
ergo/actions/serve.py,0,"b'import os\nimport argparse\nimport traceback\nimport threading\nimport logging as log\nimport pandas as pd\nimport numpy as np\nfrom flask import Flask, request, jsonify\n\nfrom ergo.project import Project\n\nreqs        = 0\nreload_lock = threading.Lock()\nprj         = None\nclasses     = None\nnum_outputs = 0\napp         = Flask(__name__)\n\ndef get_input(req):\n    try:\n        # search in query parameters\n        x = request.args.get(\'x\')\n        if x is not None and x != """":\n            return x\n    except:\n        pass\n\n    try:\n        # search in form body\n        x = request.form.get(\'x\')\n        if x is not None and x != """":\n            return x\n    except:\n        pass\n\n\n    try:\n        # search as file upload\n        x = request.files[\'x\']\n        if x is not None:\n            return x\n    except:\n        pass\n\n    try:\n        # search for direct json body\n        x = req.get_json(silent=True)\n        if x is not None:\n            return x\n    except:\n        pass\n\n    return None\n\n@app.route(\'/encode\', methods=[\'POST\', \'GET\'])\ndef encode_route():\n    global prj, classes, num_outputs\n\n    try:\n        xin = get_input(request)\n        if xin is None:\n            return ""missing \'x\' parameter"", 400\n\n        x = prj.logic.prepare_input(xin)\n\n        return jsonify(list(x)), 200\n    except Exception as e:\n        return str(e), 400\n\n@app.route(\'/\', methods=[\'POST\', \'GET\'])\ndef infer_route():\n    global reqs, reload_lock, prj, classes, num_outputs\n    try:\n        xin = get_input(request)\n        if xin is None:\n            return ""missing \'x\' parameter"", 400\n\n        with reload_lock:\n            # encode the input\n            x = prj.logic.prepare_input(xin)\n            # run inference\n            y = prj.model.predict(np.array([x]))[0].tolist()\n            # decode results\n            num_y = len(y)\n            resp = {}\n\n            if num_y != num_outputs:\n                return ""expected %d output classes, got inference with %d results"" % (num_outputs, num_y), 500\n\n            resp = { classes[i] : y[i] for i in range(num_y) }\n            reqs += 1\n            if reqs >= 25:\n                prj.reload_model()\n                reqs = 0\n\n        return jsonify(resp), 200\n    except Exception as e:\n        log.exception(""error while running inference"")\n        return str(e), 400\n\ndef parse_args(argv):\n    parser = argparse.ArgumentParser(prog=""ergo serve"", description=""Load a model and expose an API that can be used to run its inference."",\n            formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(""path"", help=""Path of the project."")\n\n    parser.add_argument(""-a"", ""--address"", dest=""address"", action=""store"", type=str,\n        help=""IP address or hostname to bind to."")\n    parser.add_argument(""-p"", ""--port"", dest=""port"", action=""store"", type=int, default=8080,\n        help=""TCP port to bind to."")\n    parser.add_argument(""--debug"", dest=""debug"", action=""store_true"", default=False,\n        help=""Enable debug messages."")\n\n    parser.add_argument(""--profile"", dest=""profile"", action=""store_true"", default=False,\n        help=""Enable profiler."")\n    parser.add_argument(""--prof-max"", dest=""restrictions"", type=int, default=30,\n        help=""Maximum number of calls to profile."")\n\n    parser.add_argument(""--classes"", dest=""classes"", default=None,\n        help=""Optional comma separated list of output classes."")\n\n    args = parser.parse_args(argv)\n    return args\n\ndef action_serve(argc, argv):\n    global prj, app, classes, num_outputs\n\n    args = parse_args(argv)\n    prj = Project(args.path)\n    err = prj.load()\n    if err is not None:\n        log.error(""error while loading project: %s"", err)\n        quit()\n    elif not prj.is_trained():\n        log.error(""no trained Keras model found for this project"")\n        quit()\n\n    if args.classes is None:\n        num_outputs = prj.model.output.shape[1]\n        if prj.classes is None:\n            classes = [""class_%d"" % i for i in range(num_outputs)]\n        else:\n            classes = [prj.classes[i] for i in range(num_outputs)]\n    else:\n        classes = [s.strip() for s in args.classes.split(\',\') if s.strip() != """"]\n        num_outputs = len(classes)\n\n    if args.profile:\n        from werkzeug.contrib.profiler import ProfilerMiddleware\n        args.debug = True\n        app.config[\'PROFILE\'] = True\n        app.wsgi_app = ProfilerMiddleware(app.wsgi_app, restrictions=[args.restrictions])\n\n    app.run(host=args.address, port=args.port, debug=args.debug)\n'"
ergo/actions/to_fdeep.py,0,"b'""""""Convert a Keras model to frugally-deep format.\n""""""\n\nimport base64\nimport datetime\nimport hashlib\nimport json\nimport sys\nimport argparse\n\nimport keras\nimport numpy as np\nfrom keras import backend as K\nfrom keras.layers import Input\nfrom keras.models import Model, load_model\n\nimport logging as log\nfrom ergo.project import Project\n\n__author__ = ""Tobias Hermann""\n__copyright__ = ""Copyright 2017, Tobias Hermann""\n__license__ = ""MIT""\n__maintainer__ = ""Tobias Hermann, https://github.com/Dobiasd/frugally-deep""\n__email__ = ""editgym@gmail.com""\n\nSTORE_FLOATS_HUMAN_READABLE = False\n\ndef transform_input_kernel(kernel):\n    """"""Transforms weights of a single CuDNN input kernel into the regular Keras format.""""""\n    return kernel.T.reshape(kernel.shape, order=\'F\')\n\n\ndef transform_recurrent_kernel(kernel):\n    """"""Transforms weights of a single CuDNN recurrent kernel into the regular Keras format.""""""\n    return kernel.T\n\n\ndef transform_kernels(kernels, n_gates, transform_func):\n    """"""\n    Transforms CuDNN kernel matrices (either LSTM or GRU) into the regular Keras format.\n\n    Parameters\n    ----------\n    kernels : numpy.ndarray\n        Composite matrix of input or recurrent kernels.\n    n_gates : int\n        Number of recurrent unit gates, 3 for GRU, 4 for LSTM.\n    transform_func: function(numpy.ndarray)\n        Function to apply to each input or recurrent kernel.\n\n    Returns\n    -------\n    numpy.ndarray\n        Transformed composite matrix of input or recurrent kernels in C-contiguous layout.\n    """"""\n    return np.require(np.hstack([transform_func(kernel) for kernel in np.hsplit(kernels, n_gates)]), requirements=\'C\')\n\n\ndef transform_bias(bias):\n    """"""Transforms bias weights of an LSTM layer into the regular Keras format.""""""\n    return np.sum(np.split(bias, 2, axis=0), axis=0)\n\n\ndef write_text_file(path, text):\n    """"""Write a string to a file""""""\n    with open(path, ""w"") as text_file:\n        print(text, file=text_file)\n\n\ndef arr_as_arr5(arr):\n    """"""Convert an n-tensor to a 5-tensor""""""\n    depth = len(arr.shape)\n    if depth == 1:\n        return arr.reshape(1, 1, 1, 1, arr.shape[0])\n    if depth == 2:\n        return arr.reshape(1, 1, 1, arr.shape[0], arr.shape[1])\n    if depth == 3:\n        return arr.reshape(1, 1, arr.shape[0], arr.shape[1], arr.shape[2])\n    if depth == 4:\n        return arr.reshape(1, arr.shape[0], arr.shape[1], arr.shape[2], arr.shape[3])\n    if depth == 5:\n        return arr\n    if depth == 6 and arr.shape[0] in [None, 1]:  # todo: Is this still needed?\n        return arr.reshape(arr.shape[1:])\n    raise ValueError(\'invalid number of dimensions\')\n\n\ndef get_layer_input_shape_shape5(layer):\n    """"""Convert a keras shape to an fdeep shape""""""\n    shape = layer.input_shape[1:]\n    depth = len(shape)\n    if depth == 1:\n        return (1, 1, 1, 1, shape[0])\n    if depth == 2:\n        return (1, 1, 1, shape[0], shape[1])\n    if depth == 3:\n        return (1, 1, shape[0], shape[1], shape[2])\n    if depth == 4:\n        return (1, shape[0], shape[1], shape[2], shape[3])\n    if depth == 5:\n        return shape\n    raise ValueError(\'invalid number of dimensions\')\n\n\ndef show_tensor5(tens):\n    """"""Serialize 3-tensor to a dict""""""\n    values = tens.flatten()\n    return {\n        \'shape\': tens.shape,\n        \'values\': encode_floats(values)\n    }\n\n\ndef show_test_data_as_tensor5(arr):\n    """"""Serialize model test data""""""\n    return show_tensor5(arr_as_arr5(arr))\n\n\ndef get_model_input_layers(model):\n    """"""Works for different Keras version.""""""\n    if hasattr(model, \'_input_layers\'):\n        return model._input_layers\n    if hasattr(model, \'input_layers\'):\n        return model.input_layers\n    raise ValueError(\'can not get (_)input_layers from model\')\n\n\ndef measure_predict(model, data_in):\n    """"""Returns output and duration in seconds""""""\n    start_time = datetime.datetime.now()\n    data_out = model.predict(data_in)\n    end_time = datetime.datetime.now()\n    duration = end_time - start_time\n    print(\'Forward pass took {} s.\'.format(duration.total_seconds()))\n    return data_out, duration.total_seconds()\n\n\ndef replace_none_with(value, shape):\n    """"""Replace every None with a fixed value.""""""\n    return tuple(list(map(lambda x: x if x is not None else value, shape)))\n\n\ndef are_embedding_layer_positions_ok_for_testing(model):\n    """"""\n    Test data can only be generated if all embeddings layers\n    are positioned directly behind the input nodes\n    """"""\n\n    def count_embedding_layers(model):\n        layers = model.layers\n        result = 0\n        for layer in layers:\n            if isinstance(layer, keras.layers.Embedding):\n                result += 1\n        layer_type = type(layer).__name__\n        if layer_type in [\'Model\', \'Sequential\']:\n            result += count_embedding_layers(layer)\n        return result\n\n    def count_embedding_layers_at_input_nodes(model):\n        result = 0\n        for input_layer in get_model_input_layers(model):\n            if input_layer._outbound_nodes and isinstance(\n                    input_layer._outbound_nodes[0].outbound_layer, keras.layers.Embedding):\n                result += 1\n        return result\n\n    return count_embedding_layers(model) == count_embedding_layers_at_input_nodes(model)\n\n\ndef gen_test_data(model):\n    """"""Generate data for model verification test.""""""\n\n    def set_shape_idx_0_to_1_if_none(shape):\n        """"""Change first element in tuple to 1.""""""\n        if shape[0] is not None:\n            return shape\n        shape_lst = list(shape)\n        shape_lst[0] = 1\n        shape = tuple(shape_lst)\n        return shape\n\n    def generate_input_data(input_layer):\n        """"""Random data fitting the input shape of a layer.""""""\n        if input_layer._outbound_nodes and isinstance(\n                input_layer._outbound_nodes[0].outbound_layer, keras.layers.Embedding):\n            random_fn = lambda size: np.random.randint(\n                0, input_layer._outbound_nodes[0].outbound_layer.input_dim, size)\n        else:\n            random_fn = np.random.normal\n        try:\n            shape = input_layer.batch_input_shape\n        except AttributeError:\n            shape = input_layer.input_shape\n        return random_fn(\n            size=replace_none_with(32, set_shape_idx_0_to_1_if_none(shape))).astype(np.float32)\n\n    assert are_embedding_layer_positions_ok_for_testing(\n        model), ""Test data can only be generated if embedding layers are positioned directly after input nodes.""\n\n    data_in = list(map(generate_input_data, get_model_input_layers(model)))\n\n    warm_up_runs = 3\n    test_runs = 5\n    data_out = None\n    for _ in range(warm_up_runs):\n        measure_predict(model, data_in)\n    duration_sum = 0\n    print(\'Starting performance measurements.\')\n    for _ in range(test_runs):\n        data_out, duration = measure_predict(model, data_in)\n        duration_sum = duration_sum + duration\n    duration_avg = duration_sum / test_runs\n    print(\'Forward pass took {} s on average.\'.format(duration_avg))\n\n    return {\n        \'inputs\': list(map(show_test_data_as_tensor5, data_in)),\n        \'outputs\': list(map(show_test_data_as_tensor5, data_out))\n    }\n\n\ndef split_every(size, seq):\n    """"""Split a sequence every seq elements.""""""\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n\n\ndef encode_floats(arr):\n    """"""Serialize a sequence of floats.""""""\n    if STORE_FLOATS_HUMAN_READABLE:\n        return arr.flatten().tolist()\n    return list(split_every(1024, base64.b64encode(arr).decode(\'ascii\')))\n\n\ndef prepare_filter_weights_conv_2d(weights):\n    """"""Change dimension order of 2d filter weights to the one used in fdeep""""""\n    assert len(weights.shape) == 4\n    return np.moveaxis(weights, [0, 1, 2, 3], [1, 2, 3, 0]).flatten()\n\n\ndef prepare_filter_weights_slice_conv_2d(weights):\n    """"""Change dimension order of 2d filter weights to the one used in fdeep""""""\n    assert len(weights.shape) == 4\n    return np.moveaxis(weights, [0, 1, 2, 3], [1, 2, 0, 3]).flatten()\n\n\ndef prepare_filter_weights_conv_1d(weights):\n    """"""Change dimension order of 1d filter weights to the one used in fdeep""""""\n    assert len(weights.shape) == 3\n    return np.moveaxis(weights, [0, 1, 2], [1, 2, 0]).flatten()\n\n\ndef show_conv_1d_layer(layer):\n    """"""Serialize Conv1D layer to dict""""""\n    weights = layer.get_weights()\n    assert len(weights) == 1 or len(weights) == 2\n    assert len(weights[0].shape) == 3\n    weights_flat = prepare_filter_weights_conv_1d(weights[0])\n    assert layer.padding in [\'valid\', \'same\']\n    assert len(layer.input_shape) == 3\n    assert layer.input_shape[0] is None\n    result = {\n        \'weights\': encode_floats(weights_flat)\n    }\n    if len(weights) == 2:\n        bias = weights[1]\n        result[\'bias\'] = encode_floats(bias)\n    return result\n\n\ndef show_conv_2d_layer(layer):\n    """"""Serialize Conv2D layer to dict""""""\n    weights = layer.get_weights()\n    assert len(weights) == 1 or len(weights) == 2\n    assert len(weights[0].shape) == 4\n    weights_flat = prepare_filter_weights_conv_2d(weights[0])\n    assert layer.padding in [\'valid\', \'same\']\n    assert len(layer.input_shape) == 4\n    assert layer.input_shape[0] is None\n    result = {\n        \'weights\': encode_floats(weights_flat)\n    }\n    if len(weights) == 2:\n        bias = weights[1]\n        result[\'bias\'] = encode_floats(bias)\n    return result\n\n\ndef show_separable_conv_2d_layer(layer):\n    """"""Serialize SeparableConv2D layer to dict""""""\n    weights = layer.get_weights()\n    assert layer.depth_multiplier == 1\n    assert len(weights) == 2 or len(weights) == 3\n    assert len(weights[0].shape) == 4\n    assert len(weights[1].shape) == 4\n\n    # probably incorrect for depth_multiplier > 1?\n    slice_weights = prepare_filter_weights_slice_conv_2d(weights[0])\n    stack_weights = prepare_filter_weights_conv_2d(weights[1])\n\n    assert layer.padding in [\'valid\', \'same\']\n    assert len(layer.input_shape) == 4\n    assert layer.input_shape[0] is None\n    result = {\n        \'slice_weights\': encode_floats(slice_weights),\n        \'stack_weights\': encode_floats(stack_weights),\n    }\n    if len(weights) == 3:\n        bias = weights[2]\n        result[\'bias\'] = encode_floats(bias)\n    return result\n\n\ndef show_depthwise_conv_2d_layer(layer):\n    """"""Serialize DepthwiseConv2D layer to dict""""""\n    weights = layer.get_weights()\n    assert layer.depth_multiplier == 1\n    assert len(weights) in [1, 2]\n    assert len(weights[0].shape) == 4\n\n    # probably incorrect for depth_multiplier > 1?\n    slice_weights = prepare_filter_weights_slice_conv_2d(weights[0])\n\n    assert layer.padding in [\'valid\', \'same\']\n    assert len(layer.input_shape) == 4\n    assert layer.input_shape[0] is None\n    result = {\n        \'slice_weights\': encode_floats(slice_weights),\n    }\n    if len(weights) == 2:\n        bias = weights[1]\n        result[\'bias\'] = encode_floats(bias)\n    return result\n\n\ndef show_batch_normalization_layer(layer):\n    """"""Serialize batch normalization layer to dict""""""\n    layer_axis = None\n    if isinstance(layer.axis, int):\n        layer_axis = layer.axis\n    else:\n        assert len(layer.axis) == 1\n        layer_axis = layer.axis[0]\n    assert layer_axis == -1 or layer_axis + 1 == len(layer.input_shape)\n    moving_mean = K.get_value(layer.moving_mean)\n    moving_variance = K.get_value(layer.moving_variance)\n    result = {}\n    result[\'moving_mean\'] = encode_floats(moving_mean)\n    result[\'moving_variance\'] = encode_floats(moving_variance)\n    if layer.center:\n        beta = K.get_value(layer.beta)\n        result[\'beta\'] = encode_floats(beta)\n    if layer.scale:\n        gamma = K.get_value(layer.gamma)\n        result[\'gamma\'] = encode_floats(gamma)\n    return result\n\n\ndef show_dense_layer(layer):\n    """"""Serialize dense layer to dict""""""\n    weights = layer.get_weights()\n    assert len(weights) == 1 or len(weights) == 2\n    assert len(weights[0].shape) == 2\n    weights_flat = weights[0].flatten()\n    result = {\n        \'weights\': encode_floats(weights_flat)\n    }\n    if len(weights) == 2:\n        bias = weights[1]\n        result[\'bias\'] = encode_floats(bias)\n    return result\n\n\ndef show_prelu_layer(layer):\n    """"""Serialize prelu layer to dict""""""\n    weights = layer.get_weights()\n    assert len(weights) == 1\n    weights_flat = weights[0].flatten()\n    result = {\n        \'alpha\': encode_floats(weights_flat)\n    }\n    return result\n\n\ndef show_embedding_layer(layer):\n    """"""Serialize Embedding layer to dict""""""\n    weights = layer.get_weights()\n    assert len(weights) == 1\n    result = {\n        \'weights\': encode_floats(weights[0])\n    }\n    return result\n\n\ndef show_lstm_layer(layer):\n    """"""Serialize LSTM layer to dict""""""\n    weights = layer.get_weights()\n    assert len(weights) == 2 or len(weights) == 3\n    result = {\'weights\': encode_floats(weights[0]),\n              \'recurrent_weights\': encode_floats(weights[1])}\n\n    if len(weights) == 3:\n        result[\'bias\'] = encode_floats(weights[2])\n\n    return result\n\n\ndef show_gru_layer(layer):\n    """"""Serialize GRU layer to dict""""""\n    weights = layer.get_weights()\n    assert len(weights) == 2 or len(weights) == 3\n    result = {\'weights\': encode_floats(weights[0]),\n              \'recurrent_weights\': encode_floats(weights[1])}\n\n    if len(weights) == 3:\n        result[\'bias\'] = encode_floats(weights[2])\n\n    return result\n\n\ndef transform_cudnn_weights(input_weights, recurrent_weights, n_gates):\n    return transform_kernels(input_weights, n_gates, transform_input_kernel), \\\n           transform_kernels(recurrent_weights, n_gates, transform_recurrent_kernel)\n\n\ndef show_cudnn_lstm_layer(layer):\n    """"""Serialize a GPU-trained LSTM layer to dict""""""\n    weights = layer.get_weights()\n    assert len(weights) == 3  # CuDNN LSTM always has a bias\n\n    n_gates = 4\n    input_weights, recurrent_weights = transform_cudnn_weights(weights[0], weights[1], n_gates)\n\n    result = {\'weights\': encode_floats(input_weights),\n              \'recurrent_weights\': encode_floats(recurrent_weights),\n              \'bias\': encode_floats(transform_bias(weights[2]))}\n\n    return result\n\n\ndef show_cudnn_gru_layer(layer):\n    """"""Serialize a GPU-trained GRU layer to dict""""""\n    weights = layer.get_weights()\n    assert len(weights) == 3  # CuDNN GRU always has a bias\n\n    n_gates = 3\n    input_weights, recurrent_weights = transform_cudnn_weights(weights[0], weights[1], n_gates)\n\n    result = {\'weights\': encode_floats(input_weights),\n              \'recurrent_weights\': encode_floats(recurrent_weights),\n              \'bias\': encode_floats(weights[2])}\n\n    return result\n\n\ndef get_transform_func(layer):\n    """"""Returns functions that can be applied to layer weights to transform them into the standard Keras format, if applicable.""""""\n    if layer.__class__.__name__ in [\'CuDNNGRU\', \'CuDNNLSTM\']:\n        if layer.__class__.__name__ == \'CuDNNGRU\':\n            n_gates = 3\n        elif layer.__class__.__name__ == \'CuDNNLSTM\':\n            n_gates = 4\n\n        input_transform_func = lambda kernels: transform_kernels(kernels, n_gates, transform_input_kernel)\n        recurrent_transform_func = lambda kernels: transform_kernels(kernels, n_gates, transform_recurrent_kernel)\n    else:\n        input_transform_func = lambda kernels: kernels\n        recurrent_transform_func = lambda kernels: kernels\n\n    if layer.__class__.__name__ == \'CuDNNLSTM\':\n        bias_transform_func = transform_bias\n    else:\n        bias_transform_func = lambda bias: bias\n\n    return input_transform_func, recurrent_transform_func, bias_transform_func\n\n\ndef show_bidirectional_layer(layer):\n    """"""Serialize Bidirectional layer to dict""""""\n    forward_weights = layer.forward_layer.get_weights()\n    assert len(forward_weights) == 2 or len(forward_weights) == 3\n    forward_input_transform_func, forward_recurrent_transform_func, forward_bias_transform_func = get_transform_func(\n        layer.forward_layer)\n\n    backward_weights = layer.backward_layer.get_weights()\n    assert len(backward_weights) == 2 or len(backward_weights) == 3\n    backward_input_transform_func, backward_recurrent_transform_func, backward_bias_transform_func = get_transform_func(\n        layer.backward_layer)\n\n    result = {\'forward_weights\': encode_floats(forward_input_transform_func(forward_weights[0])),\n              \'forward_recurrent_weights\': encode_floats(forward_recurrent_transform_func(forward_weights[1])),\n              \'backward_weights\': encode_floats(backward_input_transform_func(backward_weights[0])),\n              \'backward_recurrent_weights\': encode_floats(backward_recurrent_transform_func(backward_weights[1]))}\n\n    if len(forward_weights) == 3:\n        result[\'forward_bias\'] = encode_floats(forward_bias_transform_func(forward_weights[2]))\n    if len(backward_weights) == 3:\n        result[\'backward_bias\'] = encode_floats(backward_bias_transform_func(backward_weights[2]))\n\n    return result\n\n\ndef get_layer_functions_dict():\n    return {\n        \'Conv1D\': show_conv_1d_layer,\n        \'Conv2D\': show_conv_2d_layer,\n        \'SeparableConv2D\': show_separable_conv_2d_layer,\n        \'DepthwiseConv2D\': show_depthwise_conv_2d_layer,\n        \'BatchNormalization\': show_batch_normalization_layer,\n        \'Dense\': show_dense_layer,\n        \'PReLU\': show_prelu_layer,\n        \'Embedding\': show_embedding_layer,\n        \'LSTM\': show_lstm_layer,\n        \'GRU\': show_gru_layer,\n        \'CuDNNLSTM\': show_cudnn_lstm_layer,\n        \'CuDNNGRU\': show_cudnn_gru_layer,\n        \'Bidirectional\': show_bidirectional_layer,\n        \'TimeDistributed\': show_time_distributed_layer,\n        \'UpSampling2D\': check_upsampling_2d_layer\n    }\n\n\ndef check_upsampling_2d_layer(layer):\n    print(layer.get_config())\n    assert layer.get_config()[\'interpolation\'] == \'nearest\', \\\n        \'Only interpolation nearest is currently supported by frugally-deep.\'\n    return None\n\n\ndef show_time_distributed_layer(layer):\n    show_layer_functions = get_layer_functions_dict()\n    config = layer.get_config()\n    class_name = config[\'layer\'][\'class_name\']\n\n    if class_name in show_layer_functions:\n\n        if len(layer.input_shape) == 3:\n            input_shape_new = (layer.input_shape[0], layer.input_shape[2])\n        elif len(layer.input_shape) == 4:\n            input_shape_new = (layer.input_shape[0], layer.input_shape[2], layer.input_shape[3])\n        elif len(layer.input_shape) == 5:\n            input_shape_new = (layer.input_shape[0], layer.input_shape[2], layer.input_shape[3], layer.input_shape[4])\n        elif len(layer.input_shape) == 6:\n            input_shape_new = (layer.input_shape[0], layer.input_shape[2], layer.input_shape[3], layer.input_shape[4],\n                               layer.input_shape[5])\n        else:\n            raise Exception(\'Wrong input shape\')\n\n        layer_function = show_layer_functions[class_name]\n        attributes = dir(layer.layer)\n\n        class CopiedLayer:\n            pass\n\n        copied_layer = CopiedLayer()\n\n        for attr in attributes:\n            try:\n                if attr not in [\'input_shape\', \'__class__\']:\n                    setattr(copied_layer, attr, getattr(layer.layer, attr))\n                elif attr == \'input_shape\':\n                    setattr(copied_layer, \'input_shape\', input_shape_new)\n            except Exception:\n                continue\n\n        setattr(copied_layer, ""output_shape"", getattr(layer, ""output_shape""))\n\n        return layer_function(copied_layer)\n\n    else:\n        return None\n\n\ndef get_dict_keys(d):\n    """"""Return keys of a dictionary""""""\n    return [key for key in d]\n\n\ndef merge_two_disjunct_dicts(x, y):\n    """"""Given two dicts, merge them into a new dict as a shallow copy.\n    No Key is allowed to be present in both dictionaries.\n    """"""\n    assert set(get_dict_keys(x)).isdisjoint(get_dict_keys(y))\n    z = x.copy()\n    z.update(y)\n    return z\n\n\ndef is_ascii(some_string):\n    """"""Check if a string only contains ascii characters""""""\n    try:\n        some_string.encode(\'ascii\')\n    except UnicodeEncodeError:\n        return False\n    else:\n        return True\n\n\ndef get_all_weights(model):\n    """"""Serialize all weights of the models layers""""""\n    show_layer_functions = get_layer_functions_dict()\n    result = {}\n    layers = model.layers\n    assert K.image_data_format() == \'channels_last\'\n    for layer in layers:\n        layer_type = type(layer).__name__\n        if layer_type in [\'Model\', \'Sequential\']:\n            result = merge_two_disjunct_dicts(result, get_all_weights(layer))\n        else:\n            if hasattr(layer, \'data_format\'):\n                if layer_type in [\'AveragePooling1D\', \'MaxPooling1D\', \'AveragePooling2D\', \'MaxPooling2D\',\n                                  \'GlobalAveragePooling1D\', \'GlobalMaxPooling1D\', \'GlobalAveragePooling2D\',\n                                  \'GlobalMaxPooling2D\']:\n                    assert layer.data_format == \'channels_last\' or layer.data_format == \'channels_first\'\n                else:\n                    assert layer.data_format == \'channels_last\'\n\n            show_func = show_layer_functions.get(layer_type, None)\n            name = layer.name\n            assert is_ascii(name)\n            if name in result:\n                raise ValueError(\'duplicate layer name \' + name)\n            shown_layer = None\n            if show_func:\n                shown_layer = show_func(layer)\n            if shown_layer:\n                result[name] = shown_layer\n            if show_func and layer_type == \'TimeDistributed\':\n                if name not in result:\n                    result[name] = {}\n\n                result[name][\'td_input_len\'] = encode_floats(np.array([len(layer.input_shape) - 1], dtype=np.float32))\n                result[name][\'td_output_len\'] = encode_floats(np.array([len(layer.output_shape) - 1], dtype=np.float32))\n    return result\n\n\ndef get_model_name(model):\n    """"""Return .name or ._name or \'dummy_model_name\'""""""\n    if hasattr(model, \'name\'):\n        return model.name\n    if hasattr(model, \'_name\'):\n        return model._name\n    return \'dummy_model_name\'\n\n\ndef set_model_name(model, name):\n    """"""Overwrite .name or ._name\'""""""\n    if hasattr(model, \'name\'):\n        model.name = name\n    elif hasattr(model, \'_name\'):\n        model._name = name\n    else:\n        pass  # Model has no name property.\n\n\ndef convert_sequential_to_model(model):\n    """"""Convert a sequential model to the underlying functional format""""""\n    if type(model).__name__ == \'Sequential\':\n        name = get_model_name(model)\n        if hasattr(model, \'_inbound_nodes\'):\n            inbound_nodes = model._inbound_nodes\n        elif hasattr(model, \'inbound_nodes\'):\n            inbound_nodes = model.inbound_nodes\n        else:\n            raise ValueError(\'can not get (_)inbound_nodes from model\')\n        # Since Keras 2.2.0\n        if model.model == model:\n            input_layer = Input(batch_shape=model.layers[0].input_shape)\n            prev_layer = input_layer\n            for layer in model.layers:\n                prev_layer = layer(prev_layer)\n            funcmodel = Model([input_layer], [prev_layer])\n            model = funcmodel\n        else:\n            model = model.model\n        set_model_name(model, name)\n        if hasattr(model, \'_inbound_nodes\'):\n            model._inbound_nodes = inbound_nodes\n        elif hasattr(model, \'inbound_nodes\'):\n            model.inbound_nodes = inbound_nodes\n    assert model.layers\n    for i in range(len(model.layers)):\n        if type(model.layers[i]).__name__ in [\'Model\', \'Sequential\']:\n            model.layers[i] = convert_sequential_to_model(model.layers[i])\n    return model\n\n\ndef offset_conv2d_eval(depth, padding, x):\n    """"""Perform a conv2d on x with a given padding""""""\n    kernel = K.variable(value=np.array([[[[1]] + [[0]] * (depth - 1)]]),\n                        dtype=\'float32\')\n    return K.conv2d(x, kernel, strides=(3, 3), padding=padding)\n\n\ndef offset_sep_conv2d_eval(depth, padding, x):\n    """"""Perform a separable conv2d on x with a given padding""""""\n    depthwise_kernel = K.variable(value=np.array([[[[1]] * depth]]),\n                                  dtype=\'float32\')\n    pointwise_kernel = K.variable(value=np.array([[[[1]] + [[0]] * (depth - 1)]]),\n                                  dtype=\'float32\')\n    return K.separable_conv2d(x, depthwise_kernel,\n                              pointwise_kernel, strides=(3, 3), padding=padding)\n\n\ndef conv2d_offset_max_pool_eval(_, padding, x):\n    """"""Perform a max pooling operation on x""""""\n    return K.pool2d(x, (1, 1), strides=(3, 3), padding=padding, pool_mode=\'max\')\n\n\ndef conv2d_offset_average_pool_eval(_, padding, x):\n    """"""Perform an average pooling operation on x""""""\n    return K.pool2d(x, (1, 1), strides=(3, 3), padding=padding, pool_mode=\'avg\')\n\n\ndef check_operation_offset(depth, eval_f, padding):\n    """"""Check if backend used an offset while placing the filter\n    e.g. during a convolution.\n    TensorFlow is inconsistent in doing so depending\n    on the type of operation, the used device (CPU/GPU) and the input depth.\n    """"""\n    in_arr = np.array([[[[i] * depth for i in range(6)]]])\n    input_data = K.variable(value=in_arr, dtype=\'float32\')\n    output = eval_f(depth, padding, input_data)\n    result = K.eval(output).flatten().tolist()\n    assert result in [[0, 3], [1, 4]]\n    return result == [1, 4]\n\n\ndef get_shapes(tensor5s):\n    """"""Return shapes of a list of tensors""""""\n    return [t[\'shape\'] for t in tensor5s]\n\n\ndef calculate_hash(model):\n    layers = model.layers\n    hash_m = hashlib.sha256()\n    for layer in layers:\n        for weights in layer.get_weights():\n            assert isinstance(weights, np.ndarray)\n            hash_m.update(weights.tobytes())\n        hash_m.update(layer.name.encode(\'ascii\'))\n    return hash_m.hexdigest()\n\n\ndef convert(in_path, out_path, no_tests=False, metadata=None):\n    """"""Convert any Keras model to the frugally-deep model format.""""""\n\n    assert K.backend() == ""tensorflow""\n    assert K.floatx() == ""float32""\n    assert K.image_data_format() == \'channels_last\'\n\n    print(\'loading {}\'.format(in_path))\n    model = load_model(in_path)\n\n    # Force creation of underlying functional model.\n    # see: https://github.com/fchollet/keras/issues/8136\n    # Loss and optimizer type do not matter, since we do not train the model.\n    model.compile(loss=\'mse\', optimizer=\'sgd\')\n\n    model = convert_sequential_to_model(model)\n\n    test_data = None if no_tests else gen_test_data(model)\n\n    json_output = {}\n    if metadata is not None:\n        with open(metadata, \'r\') as metadata_file:\n            meta_info = json.load(metadata_file)\n        json_output.update(meta_info)\n    json_output[\'architecture\'] = json.loads(model.to_json())\n\n    json_output[\'image_data_format\'] = K.image_data_format()\n    for depth in range(1, 3, 1):\n        json_output[\'conv2d_valid_offset_depth_\' + str(depth)] = \\\n            check_operation_offset(depth, offset_conv2d_eval, \'valid\')\n        json_output[\'conv2d_same_offset_depth_\' + str(depth)] = \\\n            check_operation_offset(depth, offset_conv2d_eval, \'same\')\n        json_output[\'separable_conv2d_valid_offset_depth_\' + str(depth)] = \\\n            check_operation_offset(depth, offset_sep_conv2d_eval, \'valid\')\n        json_output[\'separable_conv2d_same_offset_depth_\' + str(depth)] = \\\n            check_operation_offset(depth, offset_sep_conv2d_eval, \'same\')\n    json_output[\'max_pooling_2d_valid_offset\'] = \\\n        check_operation_offset(1, conv2d_offset_max_pool_eval, \'valid\')\n    json_output[\'max_pooling_2d_same_offset\'] = \\\n        check_operation_offset(1, conv2d_offset_max_pool_eval, \'same\')\n    json_output[\'average_pooling_2d_valid_offset\'] = \\\n        check_operation_offset(1, conv2d_offset_average_pool_eval, \'valid\')\n    json_output[\'average_pooling_2d_same_offset\'] = \\\n        check_operation_offset(1, conv2d_offset_average_pool_eval, \'same\')\n    json_output[\'input_shapes\'] = list(map(get_layer_input_shape_shape5, get_model_input_layers(model)))\n    if test_data:\n        json_output[\'tests\'] = [test_data]\n    json_output[\'trainable_params\'] = get_all_weights(model)\n    json_output[\'hash\'] = calculate_hash(model)\n\n    print(\'writing {}\'.format(out_path))\n    write_text_file(out_path, json.dumps(\n        json_output, allow_nan=False, indent=2, sort_keys=True))\n\ndef parse_args(argv):\n    parser = argparse.ArgumentParser(prog=""ergo to-fdeep"", description=""Convert the model inside an ergo project to the frugally-deep library format."",\n            formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(""path"", help=""Path of the project containing the model."")\n    parser.add_argument(""--no-tests"", dest=""no_tests"", action=""store_true"", default=False,\n            help=""Don\'t generate tests in fdeep"")\n    parser.add_argument(""--metadata"", dest=""metadata"", action=""store"", required=False,\n            help=""Add metadata to fdeep model (json file)"")\n    args = parser.parse_args(argv)\n    return args\n\ndef action_to_fdeep(argc, argv):\n    args = parse_args(argv)\n    prj = Project(args.path)\n    err = prj.load()\n    if err is not None:\n        log.error(""error while loading project: %s"", err)\n        quit()\n    elif not prj.is_trained():\n        log.error(""no trained model found for this project"")\n        quit()\n\n    convert(prj.weights_path, prj.fdeep_path, args.no_tests, args.metadata)\n\n'"
ergo/actions/to_tf.py,4,"b'""""""Convert a Keras model to tensorflow protobuf format.\n""""""\nimport os\nimport argparse\n\nimport tensorflow as tf\nfrom keras import backend as K\n\nimport logging as log\n\nfrom ergo.project import Project\n\n# @credits to https://stackoverflow.com/questions/45466020/how-to-export-keras-h5-to-tensorflow-pb\ndef freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n    """"""\n    Freezes the state of a session into a pruned computation graph.\n\n    Creates a new computation graph where variable nodes are replaced by\n    constants taking their current value in the session. The new graph will be\n    pruned so subgraphs that are not necessary to compute the requested\n    outputs are removed.\n    @param session The TensorFlow session to be frozen.\n    @param keep_var_names A list of variable names that should not be frozen,\n                          or None to freeze all the variables in the graph.\n    @param output_names Names of the relevant graph outputs.\n    @param clear_devices Remove the device directives from the graph for better portability.\n    @return The frozen graph definition.\n    """"""\n    log.info(""freezing session and creating pruned computation graph ..."")\n\n    graph = session.graph\n    with graph.as_default():\n        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n        output_names = output_names or []\n        output_names += [v.op.name for v in tf.global_variables()]\n        input_graph_def = graph.as_graph_def()\n        if clear_devices:\n            for node in input_graph_def.node:\n                node.device = """"\n        frozen_graph = tf.graph_util.convert_variables_to_constants(\n            session, input_graph_def, output_names, freeze_var_names)\n        return frozen_graph\n\ndef parse_args(argv):\n    parser = argparse.ArgumentParser(prog=""ergo to-tf"", description=""Convert the model inside an ergo project to TensorFlow format."",\n            formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(""path"", help=""Path of the project containing the model."")\n    args = parser.parse_args(argv)\n    return args\n\ndef action_to_tf(argc, argv):\n    args = parse_args(argv)\n    prj = Project(args.path)\n    err = prj.load()\n    if err is not None:\n        log.error(""error while loading project: %s"", err)\n        quit()\n    elif not prj.is_trained():\n        log.error(""no trained Keras model found for this projec"")\n        quit()\n\n    frozen_graph = freeze_session(K.get_session(),\n                              output_names=[out.op.name for out in prj.model.outputs])\n\n    log.info(""saving protobuf to %s ..."", os.path.join(prj.path, \'model.pb\'))\n\n    tf.train.write_graph(frozen_graph, prj.path, ""model.pb"", as_text=False)\n'"
ergo/actions/train.py,0,"b'import os\nimport argparse\nimport logging as log\n\nfrom ergo.project import Project\n\ndef probability(x):\n    x = float(x)\n    if x < 0 or x > 1:\n        raise argparse.ArgumentTypeError(""%r not in range [0.0 - 1.0]"" % x )\n    return x\n\ndef validate_args(args):\n    if args.dataset is not None and (not args.dataset.startswith(\'sum://\') and not os.path.exists(args.dataset)):\n        log.error(""file %s does not exist"" % args.dataset)\n        quit()\n\n    p_train = 1 - args.test - args.validation\n    if p_train < 0:\n        log.error(""validation and test proportion are bigger than 1"")\n        quit()\n    elif p_train < 0.5:\n        log.error(""using less than 50% of the dataset for training"")\n        quit()\n\ndef parse_args(argv):\n    parser = argparse.ArgumentParser(prog=""ergo train"", description=""Start the training phase of a model."",\n            formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(""path"", help=""The path containing the model definition."")\n\n    parser.add_argument( ""-d"", ""--dataset"", action=""store"", dest=""dataset"", \n        help=""Path of the dataset to use or a SUM server address. Leave empty to reuse previously generated subsets."")\n    parser.add_argument(""-g"", ""--gpus"", action=""store"", dest=""gpus"", type=int, default=0, \n        help=""Number of GPU devices to use, leave to 0 to use all the available ones."")\n    parser.add_argument(""-t"", ""--test"", action=""store"", dest=""test"", type=probability, default=0.15, \n        help=""Proportion of the test set in the (0,1] interval."")\n    parser.add_argument(""-v"", ""--validation"", action=""store"", dest=""validation"", type=probability, default=0.15, \n        help=""Proportion of the validation set in the (0,1] interval."")\n    \n    parser.add_argument(""--no-save"", dest=""no_save"", action=""store_true"", default=False, \n        help=""Do not save temporary datasets on disk."")\n    parser.add_argument(""--no-shuffle"", dest=""no_shuffle"", action=""store_true"", default=False, \n        help=""Do not shuffle dataset during preparation."")\n\n    args = parser.parse_args(argv)\n    validate_args(args)\n    return args\n\ndef action_train(argc, argv):\n    args = parse_args(argv)\n    prj = Project(args.path)\n    err = prj.load()\n    if err is not None:\n        log.error(""error while loading project: %s"", err)\n        quit()\n\n    if args.dataset is not None:\n        # a dataset was specified, split it and generate\n        # the subsets\n        prj.dataset.do_save = not args.no_save\n        prj.prepare(args.dataset, args.test, args.validation, not args.no_shuffle)\n    elif prj.dataset.exists():\n        # no dataset passed, attempt to use the previously\n        # generated subsets\n        prj.dataset.load()\n    else:\n        log.error(""no test/train/validation subsets found in %s, please specify a --dataset argument"", args.path)\n        quit()\n\n    prj.train(args.gpus)\n'"
ergo/actions/view.py,0,"b'import os\nimport argparse\nimport logging as log\n\nfrom ergo.project import Project\n\ndef parse_args(argv):\n    parser = argparse.ArgumentParser(prog=""ergo view"", description=""View the model struture and training statistics."",\n            formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(""path"", help=""Path of the project."")\n    parser.add_argument( ""--img-only"", dest=""img_only"", default=False, action=""store_true"",\n        help=""Save plots as PNG files but don\'t show them in a UI."")\n    args = parser.parse_args(argv)\n    return args\n\ndef action_view(argc, argv):\n    args = parse_args(argv)\n    prj = Project(args.path)\n    err = prj.load()\n    if err is not None:\n        log.error(""error while loading project: %s"", err)\n        quit()\n\n    prj.view(args.img_only)\n'"
ergo/core/__init__.py,0,b''
ergo/core/action.py,0,"b'class Action(object):\n    def __init__(self, name, description, cb):\n        self.name = name\n        self.description = description\n        self.cb = cb\n'"
ergo/core/loader.py,0,"b'import logging as log\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport threading\n\nclass Loader(object):\n    def __init__(self, dataset):\n        self.dataset = dataset\n\n    def _reader(self, path):\n        log.info(""loading %s ..."" % path)\n        if self.dataset.is_flat:\n            return pd.read_csv(path, sep = \',\', header = None)\n        else:\n            return pickle.load( open( path, ""rb"" ) )\n\n    def _worker(self, idx):\n        if idx == 0:\n            self.dataset.train = self._reader(self.dataset.train_path)\n        elif idx == 1:\n            self.dataset.test = self._reader(self.dataset.test_path)\n        elif idx == 2:\n            self.dataset.validation = self._reader(self.dataset.valid_path)\n\n    def load_test(self):\n        self.dataset.test = self._reader(self.dataset.test_path)\n        self.dataset.n_labels = len(np.unique(self.dataset.test.iloc[:,0].unique()))\n        log.info(""detected %d distinct labels"", self.dataset.n_labels)\n\n    def load(self):\n        threads = ( \\\n          threading.Thread(target=self._worker, args=(0,)),\n          threading.Thread(target=self._worker, args=(1,)),\n          threading.Thread(target=self._worker, args=(2,)) \n        )\n        for t in threads:\n            t.start()\n        for t in threads:\n            t.join() \n\n        u = np.concatenate( \\\n                (self.dataset.validation.iloc[:,0].unique(), \n                self.dataset.test.iloc[:,0].unique(),\n                self.dataset.train.iloc[:,0].unique()) )\n        self.dataset.n_labels = len(np.unique(u))\n\n        log.info(""detected %d distinct labels"", self.dataset.n_labels)\n'"
ergo/core/logic.py,0,"b'import os\nimport logging as log\nfrom importlib.machinery import SourceFileLoader\n\nclass Logic(object):\n    @staticmethod\n    def get_doer(name):\n        log.debug(""loading %s ..."" % name)\n        return SourceFileLoader("""",name).load_module()\n\n    @staticmethod\n    def get_symbol(name, symbol):\n        doer = Logic.get_doer(name)\n        if symbol not in doer.__dict__:\n            return None, ""%s does not define a %s function"" % (name, symbol)\n        return doer.__dict__[symbol], None\n\n    @staticmethod\n    def get_symbols(name, symbols):\n        doer = Logic.get_doer(name)\n        ret  = []\n        for symbol in symbols:\n            if symbol not in doer.__dict__:\n                return None, ""%s does not define a %s function"" % (name, symbol)\n            else:\n                ret.append(doer.__dict__[symbol])\n        return ret, None\n\n    def __init__(self, path):\n        self.path             = os.path.abspath(path)\n        self.preparer_path    = os.path.join(self.path, \'prepare.py\')\n        self.dataset_preparer = None\n        self.input_preparer   = None\n        self.builder_path     = os.path.join(self.path, \'model.py\')\n        self.builder          = None\n        self.trainer_path     = os.path.join(self.path, \'train.py\')\n        self.trainer          = None\n\n    def load(self):\n        preparers, err = Logic.get_symbols(self.preparer_path, (\'prepare_dataset\', \'prepare_input\')) \n        if err is not None:\n            return err\n        else:\n            self.dataset_preparer, self.input_preparer = preparers\n\n        self.builder, err = Logic.get_symbol(self.builder_path, \'build_model\')\n        if err is not None:\n            return err\n\n        self.trainer, err = Logic.get_symbol(self.trainer_path, \'train_model\')\n        if err is not None:\n            return err\n\n        return None\n\n    def prepare_dataset(self, filename):\n        return self.dataset_preparer(filename)\n\n    def prepare_input(self, x, is_encoding = False):\n        return self.input_preparer(x, is_encoding)\n\n\n\n\n\n'"
ergo/core/multi_model.py,0,"b'\nclass multi_model():\n    def __init__ (self, cpu_model, gpu_model = None):\n        self.cpu_model = cpu_model\n        self.gpu_model = gpu_model\n\n    def fit (self, x, y, **kwargs):\n        if self.gpu_model is not None:\n            return self.gpu_model.fit(x, y, **kwargs)\n        return self.cpu_model.fit(x,y, **kwargs)\n\n    def compile(self, **kwargs):\n        if self.gpu_model is not None:\n            return self.gpu_model.compile(**kwargs)\n        return self.cpu_model.compile(**kwargs)\n\n    def __getattr__(self, name, *args, **kwargs):\n        if hasattr(self.cpu_model, name):\n            def wrapper(*args, **kwargs):\n                return getattr(self.cpu_model, name)(*args, **kwargs)\n            return wrapper\n        raise AttributeError(""Method/Attribute %s not found for keras model"" % name)\n'"
ergo/core/optimizer.py,0,"b'import logging as log\nimport pandas as pd\n\ndef optimize_dataset(path, reuse = 0.15, output = None):\n    log.info(""optimizing dataset %s (reuse ratio is %.1f%%) ..."", path, reuse * 100.0)\n\n    data  = pd.read_csv(path, sep = \',\', header = None)\n    n_tot = len(data)\n\n    log.info(""loaded %d total samples"", n_tot)\n\n    unique  = data.drop_duplicates()\n    n_uniq  = len(unique)\n    n_reuse = int( n_uniq * reuse )\n    reuse   = data.sample(n=n_reuse).reset_index(drop = True)\n\n    log.info(""found %d unique samples, reusing %d samples from the main dataset"", n_uniq, n_reuse)\n\n    out          = pd.concat([reuse, unique]).sample(frac=1).reset_index(drop=True)\n    outpath      = output if output is not None else path\n    n_out        = len(out)\n    optimization = 100.0 - (n_out * 100.0) / float(n_tot)\n\n    log.info(""optimized dataset has %d records, optimization is %.2f%%"", n_out, optimization)\n\n    log.info(""saving %s ..."", outpath)\n    out.to_csv( outpath, sep = \',\', header = None, index = None)\n'"
ergo/core/queue.py,0,"b'from threading import Thread\nimport multiprocessing\nimport logging as log\nimport queue \nimport time\nimport os\n\nclass TaskQueue(queue.Queue):\n    def __init__(self, name, num_workers=-1, blocking=False):\n        queue.Queue.__init__(self)\n        self.name = name\n        self.blocking = blocking\n        self.num_workers = num_workers if num_workers > 0 else (multiprocessing.cpu_count() * 2)\n        self._start_workers()\n\n    def add_task(self, task, *args, **kwargs):\n        args = args or ()\n        kwargs = kwargs or {}\n        self.put((task, args, kwargs), block=self.blocking)\n\n    def _start_workers(self):\n        log.info(""starting %d workers for %s"" % (self.num_workers, self.name))\n        for i in range(self.num_workers):\n            t = Thread(target=self._worker)\n            t.daemon = True\n            t.start()\n\n    def _worker(self):\n        log.debug(""task queue worker started"")\n        while True:\n            item, args, kwargs = self.get()\n            item(*args, **kwargs)  \n            self.task_done()\n\n'"
ergo/core/saver.py,0,"b'import threading\nimport logging as log\nimport pickle\n\nclass Saver(object):\n    def __init__(self, dataset):\n        self.dataset = dataset\n        self.threads = None\n\n    @staticmethod\n    def _worker(v, filename, flat):\n        log.info(""saving %s ..."" % filename)\n\n        if not flat:\n            pickle.dump( v, open( filename, ""wb"" ) )\n        else:\n            v.to_csv(filename, sep = \',\', header = None, index = None, chunksize=512)\n\n    def save(self):\n        train_path = self.dataset.train_path\n        test_path  = self.dataset.test_path\n        valid_path = self.dataset.valid_path\n        flat = self.dataset.is_flat\n\n        if not self.dataset.is_flat:\n            train_path = train_path.replace(\'.csv\', \'.pkl\')\n            test_path = test_path.replace(\'.csv\', \'.pkl\')\n            valid_path = valid_path.replace(\'.csv\', \'.pkl\')\n\n        self.threads = [ \\\n          threading.Thread(target=Saver._worker, args=( self.dataset.train, train_path, flat, )),\n          threading.Thread(target=Saver._worker, args=( self.dataset.test, test_path, flat, )),\n          threading.Thread(target=Saver._worker, args=( self.dataset.validation, valid_path, flat, )) \n        ]\n\n        for t in self.threads:\n            t.start()\n    \n    def wait(self):\n        if self.threads is not None:\n            log.info(""waiting for datasets saving to complete ..."")\n            for t in self.threads:\n                t.join()\n'"
ergo/core/template.py,0,"b'class Template(object):\n    def __init__(self, name, code):\n        self.name = name\n        self.code = code.strip()\n\n    def compile(self, ctx):\n        compiled = self.code\n        for key, val in ctx.items():\n            compiled = compiled.replace(""{%s}"" % key, str(val))\n        return compiled\n'"
ergo/core/utils.py,0,"b'import os\nimport shutil\nimport logging as log\nimport numpy as np\n\nfrom collections import defaultdict\n\ndef clean_if_exist(path, files):\n    path  = os.path.abspath(path)\n    for filename in files:\n        filename = os.path.join(path, filename)\n        if os.path.exists(filename):\n            if os.path.isdir(filename):\n                log.info(""removing folder %s"", filename)\n                shutil.rmtree(filename)\n            else:\n                log.info(""removing file %s"", filename)\n                os.remove(filename)\n\ndef is_number(s):\n    try:\n        float(s)\n        return True\n    except ValueError:\n        return False\n\ndef serialize_classification_report(cr):\n    tmp = list()\n    for row in cr.split(""\\n""):\n        parsed_row = [x.strip() for x in row.split(""  "") if len(x.strip()) > 0]\n        if len(parsed_row) > 0:\n            tmp.append(parsed_row)\n    \n    measures = tmp[0]\n    out = defaultdict(dict)\n    for row in tmp[1:]:\n        columns      = len(row)\n        class_label  = row[0].strip()\n        num_measures = len(measures)\n        \n        # fixes https://github.com/evilsocket/ergo/issues/5\n        while columns < num_measures + 1:\n            row.insert(1, None)\n            columns += 1\n\n        for j, m in enumerate(measures):\n            v = row[j + 1]\n            value = float(v.strip()) if v is not None else None\n            metric = m.strip()\n            out[class_label][metric] = value\n\n    return out\n\ndef serialize_cm(cm):\n    return cm.tolist()\n'"
