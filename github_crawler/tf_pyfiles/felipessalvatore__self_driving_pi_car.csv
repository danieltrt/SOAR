file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\n\nsetup(\n    name=\'self-driving-project\',\n    version=\'0.1\',\n    description=\'Autonomous car in rasberry pi with tensorflow\',\n    author=\'Paula Moraes, Felipe Salvatore\',\n    packages=find_packages(),\n    test_suite=""tests""\n)\n'"
self_driving/DiffController.py,5,"b'import os\nimport argparse\nimport time\nimport tensorflow as tf\nimport numpy as np\nimport keyboard as key\nfrom ml_training.DataHolder import DataHolder\nfrom ml_training.Config import Config\nfrom ml_training.Trainer import Trainer\nfrom ml_training.DFN import DFN\nfrom ml_training.CNN import CNN\nfrom nxt_car.DiffCar import DiffCar\nfrom vision.Camera import Camera\nfrom ml_training.util import int2command\nfrom vision.util import write_img\n\n\nclass DiffController():\n    """"""\n    Class that controls the Diffcar robot using one DFN.\n\n    :param height: image height\n    :type heights: int\n    :param width: image width\n    :type width: int\n    :param channels: image channels\n    :type channels: int\n    :param architecture: network architecture\n    :type architecture: list of int\n    :param activations: list of different tf functions\n    :type activations: list of tf.nn.sigmoid, tf.nn.relu, tf.nn.tanh\n    :param conv_architecture: convolutional architecture\n    :type conv_architecture: list of int\n    :param kernel_sizes: filter sizes\n    :type kernel_sizes: list of int\n    :param pool_kernel: pooling filter sizes\n    :type pool_kernel: list of int\n    :param resize: param to control the image\'s resize ratio\n    :type resize: int\n    :param conv: param to control if the model will be a CNN\n                 or DFN\n    :type conv: bool\n    :param mode: param to control type of image\n    :type mode: str\n    :param bluetooth: param to control if the bluetooth will be used.\n    :type bluetooth: bool\n    :param debug: param to enter debug mode\n    :type debug: bool\n    """"""\n    def __init__(self,\n                 height,\n                 width,\n                 architecture,\n                 activations,\n                 conv_architecture,\n                 kernel_sizes,\n                 pool_kernel,\n                 resize,\n                 conv,\n                 mode,\n                 bluetooth,\n                 debug):\n        assert mode == ""pure"" or mode == ""green"" or mode == ""bin"" or mode == ""gray""  # noqa\n        self.robot = DiffCar(bluetooth=bluetooth)\n        self.cam = Camera(mode=mode,\n                          debug=debug,\n                          resize=resize / 100.0)\n        self.mode = mode\n        if mode == ""pure"":\n            channels = 3\n        else:\n            channels = 1\n        config = Config(channels=channels,\n                        height=height,\n                        width=width,\n                        architecture=architecture,\n                        activations=activations,\n                        conv_architecture=conv_architecture,\n                        kernel_sizes=kernel_sizes,\n                        pool_kernel=pool_kernel)\n        data = DataHolder(config)\n        graph = tf.Graph()\n        if conv:\n            network = CNN(graph, config)\n        else:\n            network = DFN(graph, config)\n        self.trainer = Trainer(graph, config, network, data)\n\n    def get_command(self, img, label_dict=int2command):\n        """"""\n        Get command from model\'s prediction\n\n        :param img: image\n        :type img: np.array\n        :param label_dict: dict translating label to command\n        :type label_dict: dict\n        :return: command\n        :rtype: int\n        """"""\n        command_int = int(self.trainer.predict(img)[0])\n        command_int = label_dict[command_int]\n        return command_int\n\n    def get_command_and_prob(self, img, label_dict=int2command):\n        """"""\n        Get command from model\'s prediction\n\n        :param img: image\n        :type img: np.array\n        :param label_dict: dict translating label to command\n        :type label_dict: dict\n        :return: command, probability\n        :rtype: int, np.array\n        """"""\n        prob = self.trainer.predict_prob(img)[0]\n        result = np.argmax(prob, axis=0)\n        result = result.astype(np.int32)\n        command_int = int(result)\n        command_int = label_dict[command_int]\n        return command_int, prob\n\n    def image2float(self, img):\n        """"""\n        Change type and shape of the image\'s array\n        according to self.mode.\n\n        :param img: image\n        :type img: np.array\n        :return: image\n        :rtype: np.array\n        """"""\n        if self.mode == ""pure"":\n            img = img.astype(np.float32) / 255\n            img = img.reshape((1, img.shape[0] * img.shape[1] * img.shape[2]))\n            return img\n        else:\n            img = img.astype(np.float32) / 255\n            img = img.reshape((1, img.shape[0] * img.shape[1]))\n        return img\n\n    def drive(self):\n        """"""\n        The car drives itself until the key ""q"" is pressed.\n        """"""\n        last_command = None\n        while True:\n            img = self.cam.take_picture()\n\n            if key.is_pressed(\'q\'):\n                print(\'Exiting...\')\n                self.robot.idle()\n                break\n            else:\n                img = self.image2float(img)\n                command = self.get_command(img)\n                print(command)\n                if command == \'up\':\n                    self.robot.move_up()\n                    time.sleep(0.05)\n                elif command == \'down\':\n                    self.robot.move_down()\n                    time.sleep(0.05)\n                elif command == \'left\':\n                    self.robot.move_left()\n                elif command == \'right\':\n                    self.robot.move_right()\n                if last_command is not None:\n                    if last_command != command:\n                        self.robot.idle()\n                        last_command = command\n\n    def drive_debug(self):\n        """"""\n        The car drives itself until the key ""q"" is pressed.\n        All captured images are stored in the folder ""debug_run""\n        to check the model\'s performance.\n        """"""\n        if not os.path.exists(""debug_run""):\n            os.makedirs(""debug_run"")\n        last_command = None\n        count = 0\n        while True:\n            init = time.time()\n            img, original_img = self.cam.take_picture()\n            init = time.time() - init\n            print(""take_picture_time = {0:.3f}"".format(init))\n\n            if key.is_pressed(\'q\'):\n                print(\'Exiting...\')\n                self.robot.idle()\n                break\n            else:\n                img_flatt = self.image2float(img)\n                init = time.time()\n                command, prob = self.get_command_and_prob(img_flatt)\n                init = time.time() - init\n                print(""foward_time = {0:.3f}"".format(init))\n                commands = [\'up\', \'left\', \'right\']\n                commands_prob = []\n                for i, com in enumerate(commands):\n                    commands_prob.append(com + "":{0:.2f}"".format(prob[i]))\n                print(commands_prob)\n                print(command)\n                name = os.path.join(""debug_run"", str(count) + "".png"")\n                write_img(original_img, commands_prob, name)\n                if command == \'up\':\n                    self.robot.move_up()\n                    time.sleep(0.05)\n                elif command == \'down\':\n                    self.robot.move_down()\n                    time.sleep(0.05)\n                elif command == \'left\':\n                    self.robot.move_left()\n                elif command == \'right\':\n                    self.robot.move_right()\n                if last_command is not None:\n                    if last_command != command:\n                        self.robot.idle()\n                        last_command = command\n            count += 1\n\n\ndef main():\n    """"""\n    Drive the robot car using one trained model\n    """"""\n    parser = argparse.ArgumentParser(description=""Drive the robot car using one trained model"")  # noqa\n\n    parser.add_argument(""-m"",\n                        ""--mode"",\n                        type=str,\n                        default=""pure"",\n                        help=""image mode (default=\'pure\')"")\n    parser.add_argument(""-b"",\n                        ""--bluetooth"",\n                        type=bool,\n                        default=False,\n                        help=""bluetooth control (default=False)"")\n    parser.add_argument(""-d"",\n                        ""--debug"",\n                        action=""store_true"",\n                        default=False,\n                        help=""debug (default=False)"")\n    parser.add_argument(""-he"",\n                        ""--height"",\n                        type=int,\n                        default=90,\n                        help=""image height (default=90)"")\n    parser.add_argument(""-w"",\n                        ""--width"",\n                        type=int,\n                        default=160,\n                        help=""image width (default=160)"")\n    parser.add_argument(\'-a\',\n                        \'--architecture\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'sizes for hidden layers and output layer, should end with ""4"" !, (default=[4])\',  # noqa\n                        default=[4])\n    parser.add_argument(\'-conva\',\n                        \'--conv_architecture\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'filters for conv layers (default=[32, 64])\',\n                        default=[32, 64])\n    parser.add_argument(\'-k\',\n                        \'--kernel_sizes\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'kernel sizes for conv layers (default=None - 5 for every layer)\',  # noqa\n                        default=None)\n    parser.add_argument(\'-p\',\n                        \'--pool_kernel\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'kernel sizes for pooling layers (default=None - 2 for every layer)\',  # noqa\n                        default=None)\n    parser.add_argument(\'-ac\',\n                        \'--activations\',\n                        type=str,\n                        nargs=\'+\',\n                        help=\'activations: relu, sigmoid, tanh (defaul=None)\',\n                        default=None)\n    parser.add_argument(""-conv"",\n                        ""--conv"",\n                        action=""store_true"",\n                        default=False,\n                        help=""Use convolutional network (default=False)"")\n    parser.add_argument(\'-r\',\n                        \'--resize\',\n                        type=int,\n                        default=100,\n                        help=\'resize percentage, (default=100)\')\n\n    args = parser.parse_args()\n    activations_dict = {""relu"": tf.nn.relu,\n                        ""sigmoid"": tf.nn.sigmoid,\n                        ""tanh"": tf.nn.tanh}\n\n    if args.activations is not None:\n        activations = [activations_dict[act] for act in args.activations]\n    else:\n        activations = args.activations\n\n    car = DiffController(height=args.height,\n                         width=args.width,\n                         architecture=args.architecture,\n                         activations=activations,\n                         conv_architecture=args.conv_architecture,\n                         kernel_sizes=args.kernel_sizes,\n                         pool_kernel=args.pool_kernel,\n                         resize=args.resize,\n                         conv=args.conv,\n                         mode=args.mode,\n                         bluetooth=args.bluetooth,\n                         debug=args.debug)\n    if args.debug:\n        car.drive_debug()\n    else:\n        car.drive()\n    if car.robot.btCon:\n        car.robot.disconnect(car.robot.sock)\n    time.sleep(0.3)  # waits for keyboard thread to shutdown\n\n\nif __name__ == \'__main__\':\n    main()\n'"
self_driving/simulation.py,4,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport argparse\nimport os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom ml_training.DataHolder import DataHolder\nfrom ml_training.Config import Config\nfrom ml_training.Trainer import Trainer\nfrom ml_training.DFN import DFN\nfrom ml_training.CNN import CNN\nfrom vision.image_manipulation import binarize_image, grayscale_image\nfrom vision.util import write_img\n\n\ndef simulate_run(folder_path,\n                 output_path,\n                 mode,\n                 trainer,\n                 verbose,\n                 resize=100,\n                 commands=[\'up\', \'left\', \'right\']):\n    """"""\n    Function to simulate one driving using one of folder images.\n\n    :param folder_path: path to folder containing images\n    :type folder_path: str\n    :param output_path: path to save the images with the respective probability\n    :type output_path: str\n    :param mode: image mode\n    :type mode: str\n    :param trainer: trainer object to run prediction\n    :type trainer: ml_training.Trainer\n    :param verbose: param to control print\n    :type verbose: bool\n    :param resize: param to control the image\'s resize ratio\n    :type resize: int\n    :param commands: list of commands in sting format\n    :type commands: list of str\n    """"""\n    if verbose:\n        print(""Trying to run simulator in images from {} \\n"".format(folder_path))  # noqa\n    resize = resize / 100.0\n    for filename in os.listdir(folder_path):\n        if verbose:\n            print(filename)\n        image_path = os.path.join(folder_path, filename)\n        image_path_output = os.path.join(output_path, filename)\n        image_raw = cv2.imread(image_path)\n        image = cv2.resize(image_raw, (0, 0), fx=resize, fy=resize)\n        image = image2float(image, mode)\n        prob = trainer.predict_prob(image)[0]\n        commands_prob = []\n        for i, com in enumerate(commands):\n            commands_prob.append(com + "":{0:.2f}"".format(prob[i]))\n        write_img(image_raw, commands_prob, image_path_output)\n\n\ndef image2float(img, mode=""pure""):\n    """"""\n    Change type and shape of the image\'s array\n    according to self.mode.\n\n    :param img: image\n    :type img: np.array\n    :param mode: param to contro the image\'s form\n    :type mode: str\n    :return: image\n    :rtype: np.array\n    """"""\n    if mode == ""pure"":\n        img = img.astype(np.float32) / 255\n        img = img.reshape((1, img.shape[0] * img.shape[1] * img.shape[2]))\n        return img\n    else:\n        if mode == ""green"":\n            img = img[1]\n        elif mode == ""bin"":\n            img = binarize_image(img)\n        elif mode == ""gray"":\n            img = grayscale_image(img)\n        img = img.astype(np.float32) / 255\n        img = img.reshape((1, img.shape[0] * img.shape[1]))\n        return img\n\n\ndef main():\n    """"""\n    Script to run one simmulation on a trained model.\n    """"""\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'img_folder_path\',\n                        type=str, help=\'path to image folder\')\n    parser.add_argument(\'output_path\',\n                        type=str, help=\'path to simulate data to be saved\')\n    parser.add_argument(\'-a\',\n                        \'--architecture\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'sizes for hidden layers and output layer, should end with ""4"" !, (default=[4])\',  # noqa\n                        default=[4])\n    parser.add_argument(\'-conva\',\n                        \'--conv_architecture\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'filters for conv layers (default=[32, 64])\',\n                        default=[32, 64])\n    parser.add_argument(\'-k\',\n                        \'--kernel_sizes\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'kernel sizes for conv layers (default=None - 5 for every layer)\',  # noqa\n                        default=None)\n    parser.add_argument(\'-p\',\n                        \'--pool_kernel\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'kernel sizes for pooling layers (default=None - 2 for every layer)\',  # noqa\n                        default=None)\n    parser.add_argument(\'-ac\',\n                        \'--activations\',\n                        type=str,\n                        nargs=\'+\',\n                        help=\'activations: relu, sigmoid, tanh (defaul=None)\',\n                        default=None)\n    parser.add_argument(""-conv"",\n                        ""--conv"",\n                        action=""store_true"",\n                        default=False,\n                        help=""Use convolutional network (default=False)"")\n    parser.add_argument(""-m"",\n                        ""--mode"",\n                        type=str,\n                        default=""pure"",\n                        help=""image type: \'pure\', \'bin\', \'gray\', \'green\' --only-- (default=pure)"")  # noqa\n    parser.add_argument(""-he"",\n                        ""--height"",\n                        type=int,\n                        default=90,\n                        help=""image height (default=90)"")\n    parser.add_argument(""-w"",\n                        ""--width"",\n                        type=int,\n                        default=160,\n                        help=""image width (default=160)"")\n    parser.add_argument(""-r"",\n                        ""--resize"",\n                        type=int,\n                        default=100,\n                        help=""percentage to resize images in dataset (default=100)"")  # noqa\n    parser.add_argument(""-v"",\n                        ""--verbose"",\n                        action=""store_true"",\n                        default=False,\n                        help=""print training results and calculate confusion matrix (default=False)"")  # noqa\n    args = parser.parse_args()\n\n    if not os.path.exists(args.output_path):\n        os.makedirs(args.output_path)\n\n    if args.mode == ""bin"" or args.mode == ""gray"" or args.mode == ""green"":\n        channels = 1\n    else:\n        channels = 3\n\n    activations_dict = {""relu"": tf.nn.relu,\n                        ""sigmoid"": tf.nn.sigmoid,\n                        ""tanh"": tf.nn.tanh}\n\n    if args.activations is not None:\n        activations = [activations_dict[act] for act in args.activations]\n    else:\n        activations = args.activations\n\n    config = Config(height=args.height,\n                    width=args.width,\n                    channels=channels,\n                    architecture=args.architecture,\n                    activations=activations,\n                    conv_architecture=args.conv_architecture,\n                    kernel_sizes=args.kernel_sizes,\n                    pool_kernel=args.pool_kernel)\n\n    data = DataHolder(config)\n    graph = tf.Graph()\n    if args.conv:\n        network = CNN(graph, config)\n    else:\n        network = DFN(graph, config)\n    trainer = Trainer(graph, config, network, data)\n    print(""\\nSimulating in the {} data\\n"".format(args.mode))\n    print(""params:\\n{}\\n"".format(str(config)))\n    if not os.path.exists(""checkpoints""):\n        print(""===Simulation of a non trained model==="")\n    simulate_run(args.img_folder_path,\n                 args.output_path,\n                 args.mode,\n                 trainer,\n                 args.verbose,\n                 args.resize)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tests/__init__.py,0,b''
tests/test_bluetooth.py,0,"b""import unittest\nimport bluetooth as blue\n\n\nclass BluetoothTest(unittest.TestCase):\n\n    @classmethod\n    def setUp(cls):\n        cls.ID = '00:16:53:17:EF:0A'  # MAC address NXT11\n\n    def test_bluetooth_2_raspberry_can_find_nxt(self):\n        blue_list = blue.discover_devices()\n        self.assertIn(self.ID, blue_list)\n\n    def test_bluetooth_1_connection(self):\n        socket = blue.BluetoothSocket(blue.RFCOMM)\n        socket.connect((self.ID, 1))\n        peer_name = socket.getpeername()\n        self.assertEqual(self.ID, peer_name[0])\n"""
tests/test_cam.py,0,"b'import unittest\nimport cv2\nimport os\n\n\nclass CamTest(unittest.TestCase):\n\n    @classmethod\n    def tearDown(cls):\n        if os.path.exists(cls.img_path):\n            os.remove(cls.img_path)\n\n    @classmethod\n    def setUp(cls):\n        cls.img_path = os.path.join(os.getcwd(),\n                                    ""img.png"")\n        cls.cam = cv2.VideoCapture(0)\n        height_param = 4\n        width_param = 3\n        width_size = 160\n        height_size = 90\n        cls.cam.set(width_param, width_size)\n        cls.cam.set(height_param, height_size)\n\n    def test_take_and_save_image_as_grayscale(self):\n        _, img = self.cam.read()\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        cv2.imwrite(self.img_path, img)\n        test_shape = (90, 160)\n        self.assertEqual(test_shape, img.shape)\n        self.assertTrue(os.path.exists(self.img_path))\n'"
tests/test_motor.py,0,"b""import unittest\nfrom nxt.bluesock import BlueSock\nimport nxt\nimport time\n\n\nclass NxtTest(unittest.TestCase):\n\n    @classmethod\n    def tearDown(cls):\n        cls.leftMotor.idle()\n        cls.rightMotor.idle()\n        cls.leftMotor.brake()\n        cls.rightMotor.brake()\n        cls.sock.close()\n\n    @classmethod\n    def setUp(cls):\n        ID = '00:16:53:17:EF:0A'  # MAC address NXT11\n        cls.sock = BlueSock(ID)\n        brick = cls.sock.connect()\n        cls.leftMotor = nxt.Motor(brick, nxt.PORT_B)\n        cls.rightMotor = nxt.Motor(brick, nxt.PORT_A)\n        cls.leftMotor.reset_position(False)\n        cls.rightMotor.reset_position(False)\n\n    def test_motor_left_right(self):\n        turnDegrees = 360\n        self.rightMotor.weak_turn(20, turnDegrees)\n        self.leftMotor.weak_turn(- 20, turnDegrees)\n        time.sleep(3)  # wait for the motors to stop\n        rtc = abs(self.rightMotor.get_tacho().tacho_count)\n        ltc = abs(self.leftMotor.get_tacho().tacho_count)\n        self.assertAlmostEqual(rtc, turnDegrees, delta=3)\n        self.assertAlmostEqual(ltc, turnDegrees, delta=3)\n"""
tests/test_usb.py,0,"b'import unittest\nfrom nxt import locator\nfrom nxt import usbsock\n\n\nclass USBTest(unittest.TestCase):\n\n    @classmethod\n    def setUp(cls):\n        cls.connection_method = locator.Method(bluetooth=False)\n        cls.generator = locator.find_bricks(method=cls.connection_method)\n\n    def test_usb_1_raspberry_is_not_connected(self):\n        #self.assertRaises(StopIteration, self.generator.next)\n        raise_exception = False\n        try:\n            next_item = next(self.generator)\n        except StopIteration:\n            raise_exception = True\n        self.assertFalse(raise_exception)\n\n    def test_usb_2_check_usbsock_nxt(self):\n        get_result = next(self.generator, None)\n        self.assertIsInstance(get_result, usbsock.USBSock)\n'"
self_driving/data_collection/DataCollector.py,0,"b'import sys\nimport os\nimport inspect\nimport keyboard as key\nfrom util import get_date\nimport abc\nimport time\nimport pickle\nimport argparse\n\nalmost_current = os.path.abspath(inspect.getfile(inspect.currentframe()))\ncurrentdir = os.path.dirname(almost_current)\nparentdir = os.path.dirname(currentdir)\nsys.path.insert(0, parentdir)\n\nfrom nxt_car.DiffCar import DiffCar  # noqa\nfrom vision.Camera import Camera  # noqa\n\n\nclass DataCollector():\n    __metaclass__ = abc.ABCMeta\n\n    """"""\n    Abstract class to collect images and commands,\n    both to the classification setting such as the\n    regression setting.\n\n    :param robot: robot class to control the nxt car\n    :type robot: nxt_car.DiffCar\n    :param cam: camera class to take pictures\n    :type cam: vision.Camera\n    :param name: name the folder in which all the pictures\n                 will be saved\n    :type name: None or str\n    """"""\n    def __init__(self, robot, cam, name=None):\n        date = get_date()\n        if name is not None:\n            if not os.path.exists(name):\n                os.mkdir(name)\n            self.dir_name = os.path.join(name, date)\n            self.pickle_name = os.path.join(name,\n                                            date + \'_pickle\')\n        else:\n            self.dir_name = date\n            self.pickle_name = os.path.join(date + \'_pickle\')\n        os.mkdir(self.dir_name)\n        self.data_dict = {}\n        self.count = 0\n        self.robot = robot\n        self.camera = cam\n\n    def save_image_write_dict(self, image, command):\n        """"""\n        Given one image and one command this method,\n        stores the image with name ""self.count"".png\n        and stores the given command in the dict\n        {name: command}.\n\n        :param image: image taked by the camera\n        :type image: np.array\n        :param command: the real command passed to the robot,\n                        it can be a class (""up"", ""down"", etc.)\n                        or a vector ([acceleration, steering wheel angle])\n        :type command: int or np.array\n        """"""\n        name = str(self.count) + "".png""\n        name = os.path.join(self.dir_name, name)\n        self.camera.save_image(name, image)\n        self.data_dict[str(self.count)] = command\n        self.count += 1\n\n    @abc.abstractmethod\n    def generate(self):\n        """"""\n        Method to generate the dataset.\n        """"""\n        return\n\n\nclass BasicDiffCollector(DataCollector):\n    """"""\n    Collector class for the differential model.\n    In this case each image will be classified\n    as ""up"", ""down"", ""left"" and ""right"".\n\n    :param robot: robot class to controll the nxt car\n    :type robot: nxt_car.DiffCar\n    :param cam: camera class to take picture\n    :type cam: vision.Camera\n    :param name: name the folder in which all the pictures\n                 will be saved\n    :type name: str\n    """"""\n\n    def __init__(self, robot, cam, name):\n        super(BasicDiffCollector, self).__init__(robot, cam, name)\n\n    def generate(self):\n        """"""\n        Method to generate the dataset.\n        The car is controlled with the keyboard\n        using the arrow keys, to exit just type ""q"".\n        """"""\n        while True:\n            img = self.camera.take_picture_rgb()\n\n            print ""Working""\n\n            if key.is_pressed(\'q\'):\n                print(\'Exiting...\')\n                break\n\n            elif key.is_pressed(\'up\'):\n                self.robot.move_up()\n                self.save_image_write_dict(img, \'up\')\n                time.sleep(0.05)\n\n            elif key.is_pressed(\'down\'):\n                self.robot.move_down()\n                time.sleep(0.05)\n\n            elif key.is_pressed(\'left\'):\n                self.robot.move_left()\n                self.save_image_write_dict(img, \'left\')\n\n            elif key.is_pressed(\'right\'):\n                self.robot.move_right()\n                self.save_image_write_dict(img, \'right\')\n            else:\n                self.robot.idle()\n        with open(self.pickle_name, ""wb"") as f:\n            pickle.dump(self.data_dict, f)\n\n\ndef main():\n    """"""\n    Script to collect data of the Diffcar robot.\n    """"""\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(""-n"",\n                        ""--name"",\n                        type=str,\n                        default=""pista"",\n                        help=""folder name (default=\'pista\')"")\n    user_args = parser.parse_args()\n    robot = DiffCar(bluetooth=False)\n    cam = Camera()\n    dc = BasicDiffCollector(robot, cam, user_args.name)\n    dc.generate()\n    if robot.btCon:\n        robot.disconnect(robot.sock)\n    print(dc.data_dict)\n    time.sleep(0.3)  # waits\n\n\nif __name__ == \'__main__\':\n    main()\n'"
self_driving/data_collection/__init__.py,0,b''
self_driving/data_collection/util.py,0,"b'import time\n\n\ndef get_date():\n    """"""\n    Gives you the date in form:\n    year-month-day-hours-minutes-second\n\n    :return: current date\n    :rtype: str\n    """"""\n    return time.strftime(\'%Y-%m-%d-%H-%-M-%S\')\n'"
self_driving/data_manipulation/__init__.py,0,b''
self_driving/data_manipulation/data_aug.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport argparse\nimport numpy as np\nimport sys\nimport inspect\ntry:\n    from util import get_image_and_command, get_image, get_flat_shape\n    from util import load_dataset, save_dataset\nexcept ImportError:\n    from data_manipulation.util import get_image_and_command, get_image, get_flat_shape  # noqa\n    from data_manipulation.util import load_dataset, save_dataset\n\nalmost_current = os.path.abspath(inspect.getfile(inspect.currentframe()))\ncurrentdir = os.path.dirname(almost_current)\nparentdir = os.path.dirname(currentdir)\nsys.path.insert(0, parentdir)\n\nimport vision.image_manipulation as img_mani  # noqa\n\n\ndef extend_dataset_flip_axis(data,\n                             labels,\n                             height=90,\n                             width=160,\n                             channels=3):\n    """"""\n    Balance and extend dataset\n    by generating new images flipping the horizontal\n    axis (only applicable to images labeled \'left\' or \'right\').\n    This function is hard-coded, it assumes the following codification:\n        - ""up"": 0\n        - ""left"": 1\n        - ""right"": 2\n\n    :param data: dataset\n    :type data: np.array\n    :param label: labels\n    :type label: np.array\n    :param height: image height\n    :type height: int\n    :param width: image width\n    :type width: int\n    :param channels: image channels\n    :type channels: int\n    :return: extended images, extended labels\n    :rtype: np.array, np.array\n    """"""\n    all_images = []\n    all_labels = []\n    flat_shape = data.shape[1]\n    for i in range(data.shape[0]):\n        orig_label = labels[i]\n        if orig_label == 0:\n            continue\n        frame, cmd = get_image_and_command(data[i],\n                                           labels[i],\n                                           height,\n                                           width,\n                                           channels)\n        if orig_label == 1:\n            flip_cmd = 2\n        else:\n            flip_cmd = 1\n        flip = np.flip(frame, axis=1)\n        flip = np.array(flip.reshape(flat_shape))\n        all_images.append(flip)\n        all_labels.append(flip_cmd)\n    all_labels = np.array(all_labels).astype(\'uint8\')\n    all_labels = all_labels.reshape((all_labels.shape[0], 1))\n    extended_images = np.concatenate((data, all_images), axis=0)\n    extended_labels = np.concatenate((labels, all_labels), axis=0)\n    return extended_images, extended_labels\n\n\ndef transfor_dataset_with_one_channel(data,\n                                      transformation,\n                                      height=90,\n                                      width=160,\n                                      channels=3):\n    """"""\n    Create a new dataset by applying a function ""transformation""\n    available at vision.image_manipulation.\n    Returns a new dataset and the new shape of its contents.\n    The new shape will have only height and width.\n\n    :param transformation: function\n    :type transformation: np.array -> np.array\n    :param data: dataset\n    :type data: np.array\n    :param height: image height\n    :type height: int\n    :param width: image width\n    :type width: int\n    :param channels: image channels\n    :type channels: int\n    :return: transformed dataset, shape\n    :rtype: np.array, tuple\n    """"""\n    new_dataset = []\n    new_shape = ()\n    for i in range(data.shape[0]):\n        image = get_image(data[i],\n                          height,\n                          width,\n                          channels)\n        new_image = transformation(image)\n        if new_shape == ():\n            new_shape = new_image.shape\n        new_image = new_image.reshape(get_flat_shape(new_image))\n        new_dataset.append(new_image)\n    new_dataset = np.array(new_dataset).astype(\'uint8\')\n    return new_dataset, new_shape\n\n\ndef binarize_dataset(data,\n                     height=90,\n                     width=160,\n                     channels=3):\n    """"""\n    Create a new dataset by applying the function binarize_image.\n\n    :param data: dataset\n    :type data: np.array\n    :param height: image height\n    :type height: int\n    :param width: image width\n    :type width: int\n    :param channels: image channels\n    :type channels: int\n    :return: transformed dataset, shape\n    :rtype: np.array, tuple\n    """"""\n    data, shape = transfor_dataset_with_one_channel(data,\n                                                    img_mani.binarize_image,\n                                                    height,\n                                                    width,\n                                                    channels)\n    return data, shape\n\n\ndef gray_dataset(data,\n                 height=90,\n                 width=160,\n                 channels=3):\n    """"""\n    Create a new dataset by applying the function grayscale_image.\n\n    :param data: dataset\n    :type data: np.array\n    :param height: image height\n    :type height: int\n    :param width: image width\n    :type width: int\n    :param channels: image channels\n    :type channels: int\n    :return: transformed dataset, shape\n    :rtype: np.array, tuple\n    """"""\n    data, shape = transfor_dataset_with_one_channel(data,\n                                                    img_mani.grayscale_image,\n                                                    height,\n                                                    width,\n                                                    channels)\n    return data, shape\n\n\ndef green_dataset(data,\n                  height=90,\n                  width=160,\n                  channels=3):\n    """"""\n    Create a new dataset by applying the function green_channel.\n\n    :param data: dataset\n    :type data: np.array\n    :param height: image height\n    :type height: int\n    :param width: image width\n    :type width: int\n    :param channels: image channels\n    :type channels: int\n    :return: transformed dataset, shape\n    :rtype: np.array, tuple\n    """"""\n    data, shape = transfor_dataset_with_one_channel(data,\n                                                    img_mani.green_channel,\n                                                    height,\n                                                    width,\n                                                    channels)\n    return data, shape\n\n\ndef dataset_augmentation(data, labels, height=90, width=160, channels=3):\n    """"""\n    Augment a dataset by inserting a vertical random shadow and\n    by bluring with a Gaussian convolution\n\n    :param data: dataset\n    :type data: np.array\n    :param labels: labels\n    :type labels: np.array\n    :param width: image width\n    :type width: int\n    :param height: image height\n    :type heights: int\n    :param channels: image channels\n    :type channels: int\n    :return: extended images, extended labels\n    :rtype: np.array, np.array\n    """"""\n    all_images = []\n    all_labels = []\n    size = data.shape[0]\n    flat_shape = data.shape[1]\n    for i in range(size):\n        image = get_image(data[i], height, width, channels)\n        new_image = img_mani.random_shadow(image)\n        new_image = new_image.reshape(flat_shape)\n        new_label = labels[i]\n        all_images.append(new_image)\n        all_labels.append(new_label)\n        new_image = img_mani.gaussian_blur(image)\n        new_image = new_image.reshape(flat_shape)\n        all_images.append(new_image)\n        all_labels.append(new_label)\n    all_labels = np.array(all_labels).astype(\'uint8\')\n    all_labels = all_labels.reshape((all_labels.shape[0], 1))\n    extended_images = np.concatenate((data, all_images), axis=0)\n    extended_labels = np.concatenate((labels, all_labels), axis=0)\n    return extended_images, extended_labels\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'data_path\',\n                        type=str, help=\'path to current data\')\n    parser.add_argument(\'labels_path\',\n                        type=str, help=\'path to current labels\')\n    parser.add_argument(\'new_data_folder_path\',\n                        type=str, help=\'path to data and labels to be saved\')  # noqa\n    parser.add_argument(\'dataset_name\',\n                        nargs=\'?\', default=\'dataset\', type=str, help=\'name for dataset. (Default) dataset\')  # noqa\n\n    user_args = parser.parse_args()\n\n    data, labels = load_dataset(user_args.data_path,\n                                user_args.labels_path)\n    data, labels = extend_dataset_flip_axis(data,\n                                            labels)\n    data, labels = dataset_augmentation(data, labels)\n    data_shape = (90, 160, 3)\n    save_dataset(data,\n                 labels,\n                 user_args.new_data_folder_path,\n                 data_shape,\n                 user_args.dataset_name)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
self_driving/data_manipulation/data_mani.py,6,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef _bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef _int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef randomize_in_place(list1, list2, init):\n    """"""\n    Function to randomize two lists the same way.\n    Usualy this functions is used when list1 = dataset,\n    and list2 = labels.\n\n    :param list1: list\n    :type list1: list or np.array\n    :param list2: list\n    :type list2: list or np.array\n    :param init: seed\n    :type init: int\n    """"""\n    np.random.seed(seed=init)\n    np.random.shuffle(list1)\n    np.random.seed(seed=init)\n    np.random.shuffle(list2)\n\n\ndef data_cut(data, labels, init=0):\n    """"""\n    Given the data and the labels this function shuffles them together\n    and separetes four fifths of the data (that is why we use the\n    variable ff) to be the traing data; the rest of the data\n    is divide into valid data and test data. If the size of the\n    data is odd we add one observation copy to the dataset.\n\n    :param data: dataset\n    :type data: np.array\n    :param labels: labels\n    :type labels: np.array\n    :param init: seed\n    :type init: int\n    :return: train, test and valid dataset and labels\n    :rtype: np.array, np.array, np.array, np.array, np.array, np.array\n    """"""\n    randomize_in_place(data, labels, init)\n    data_size = data.shape[0]\n    ff = int((4 / 5.0) * data_size)\n    rest = data_size - ff\n    if rest % 2 == 1:\n        new_data = data[-1]\n        new_label = labels[-1]\n        data = np.vstack([data, new_data])\n        labels = np.vstack([labels, new_label])\n        rest += 1\n    rest = int(rest / 2.0)\n    train_data, train_labels = data[0:ff], labels[0:ff]\n    valid_data, valid_labels = data[ff: ff + rest], labels[ff:ff + rest]\n    ff = ff + rest\n    test_data, test_labels = data[ff: ff + rest], labels[ff: ff + rest]\n    return train_data, train_labels, valid_data, valid_labels, test_data, test_labels  # noqa\n\n\ndef create_record(record_path,\n                  data,\n                  labels,\n                  height,\n                  width,\n                  channels):\n    """"""\n    Fuction to create one tf.record using two numpy arrays.\n    The array in data is expected to be flat.\n\n    :param record_path: path to save the tf.record\n    :type record_path: str\n    :param data: dataset\n    :type data: np.array\n    :param label: labels\n    :type label: np.array\n    :param height: image height\n    :type height: int\n    :param width: image width\n    :type width: int\n    :param channels: image channels\n    :type channels: int\n    """"""\n    assert data.shape[1] == height * width * channels\n    writer = tf.python_io.TFRecordWriter(record_path)\n    for i, e in enumerate(data):\n        img_str = data[i].tostring()\n        label_str = labels[i].tostring()\n        example = tf.train.Example(features=tf.train.Features(feature={\n            \'height\': _int64_feature(height),\n            \'width\': _int64_feature(width),\n            \'channels\': _int64_feature(channels),\n            \'image_raw\': _bytes_feature(img_str),\n            \'labels_raw\': _bytes_feature(label_str)}))\n\n        writer.write(example.SerializeToString())\n    writer.close()\n'"
self_driving/data_manipulation/img2array.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport argparse\nimport pickle\nimport os\nimport cv2\nimport numpy as np\nfrom util import command2int, save_dataset\n\n\ndef folder2array(folder_path,\n                 pickle_path,\n                 height,\n                 width,\n                 channels,\n                 resize,\n                 verbose):\n    """"""\n    Function to transform all images from the folder folder_name\n    into a tuple of np arrays.\n\n    :param folder_path: path to folder containing images\n    :type folder_path: str\n    :param pickle_path: path to pickle containing the labels\n    :type pickle_path: str\n    :param height: image height\n    :type height: int\n    :param width: image width\n    :type width: int\n    :param channels: image channels\n    :type channels: int\n    :param resize: percentage to scale the image\n    :type resize: int\n    :param verbose: param to control the print function\n    :type verbose: bool\n    :return: data, labels, shape\n    :rtype: np.array, np.array, tuple\n    """"""\n    all_images = []\n    all_labels = []\n    flat_shape = height * width * channels\n    shape = (height, width, channels)\n    if resize != 100:\n        resized_shape = (int((height * resize) / 100.0),\n                         int((width * resize) / 100.0),\n                         channels)\n        flat_shape = resized_shape[0] * resized_shape[1] * resized_shape[2]\n        shape = resized_shape\n    resize = resize / 100.0\n    with open(pickle_path, ""rb"") as f:\n        label_dict = pickle.load(f)\n    if verbose:\n        print(""Trying to convert images from {} \\n"".format(folder_path))\n    for filename in os.listdir(folder_path):\n        key = filename[:- 4]\n        label = command2int[label_dict[key]]\n        image_path = os.path.join(folder_path, filename)\n        image = change_type_to_uint8(cv2.imread(image_path))\n        image = cv2.resize(image, (0, 0), fx=resize, fy=resize)\n        image = image.reshape(flat_shape)\n        all_images.append(image)\n        all_labels.append(label)\n    all_labels = change_type_to_uint8(np.array(all_labels))\n    all_images = np.array(all_images)\n    return all_images, all_labels, shape\n\n\ndef change_type_to_uint8(image):\n    """"""\n    Change type to uint8 Unsigned integer (0 to 255)\n\n    :param image: image\n    :type image: np.array\n    :return: image\n    :rtype: np.array\n    """"""\n    image = image.astype(\'uint8\')\n    return image\n\n\ndef create_data_set_as_np_array(folder_path,\n                                npy_path,\n                                npy_name=""data"",\n                                height=90,\n                                width=160,\n                                channels=3,\n                                resize=100,\n                                verbose=True):\n    """"""\n    Giving one path to a folder of folders of images,\n    this function transform all images in two arrays\n    one with all the flatted images \'npy_name\'_<np.shape>_data.npy\n    and other with all the respective labels \'npy_name\'_<np.shape>_labels.npy\n    both saved in \'npy_path\'.\n\n    :param folder_path: path to folder containing folders of images\n                        and pickles\n    :type folder_path: str\n    :param npy_path: name of the data and labels array to be saved\n    :type npy_path: str\n    :param npy_name: path to data and labels array to be saved\n    :type npy_name: str\n    :param height: image height\n    :type heights: int\n    :param width: image width\n    :type width: int\n    :param channels: image channels\n    :type channels: int\n    :param resize: percentage to scale the image\n    :type resize: int\n    :param verbose: param to print path information\n    :type verbose: bool\n    """"""\n    assert os.path.exists(folder_path)\n    all_images = []\n    all_labels = []\n    for folder in os.listdir(folder_path):\n        folder = os.path.join(folder_path, folder)\n        if os.path.isdir(folder):\n            pickle_path = folder + ""_pickle""\n            images, labels, shape = folder2array(folder,\n                                                 pickle_path,\n                                                 height,\n                                                 width,\n                                                 channels,\n                                                 resize,\n                                                 verbose)\n            all_images.append(images)\n            all_labels.append(labels)\n    all_images = np.concatenate(all_images, axis=0)\n    all_labels = np.concatenate(all_labels, axis=0)\n    all_labels = all_labels.reshape((all_labels.shape[0], 1))\n    save_dataset(all_images, all_labels, npy_path, shape, npy_name)\n\n\ndef main():\n    """"""\n    Script to transform one folder containing folders of images\n    and pickles to a tuple of np.arrays\n    """"""\n    description = ""Transform one folder containing folders\\\n    of images and pickles to a tuple of np.arrays""\n    parser = argparse.ArgumentParser(description=description)\n\n    parser.add_argument(\'img_folder_path\',\n                        type=str, help=\'path to image folder\')\n    parser.add_argument(\'npy_folder_path\',\n                        type=str, help=\'path to npy files to be saved\')\n    parser.add_argument(\'npy_name\',\n                        type=str, nargs=\'?\', default=""data"", help=\'name of npy files (default=""data"")\')  # noqa\n    parser.add_argument(""-he"",\n                        ""--image_height"",\n                        type=int,\n                        default=90,\n                        help=""original height number (default=90)"")\n    parser.add_argument(""-w"",\n                        ""--image_width"",\n                        type=int,\n                        default=160,\n                        help=""original width number (default=160)"")\n    parser.add_argument(""-c"",\n                        ""--image_channels"",\n                        type=int,\n                        default=3,\n                        help=""number of channels (default=3)"")\n    parser.add_argument(""-r"",\n                        ""--resize"",\n                        type=int,\n                        default=100,\n                        help=""percentage to resize images in dataset (default=100)"")  # noqa\n    user_args = parser.parse_args()\n    create_data_set_as_np_array(user_args.img_folder_path,\n                                user_args.npy_folder_path,\n                                user_args.npy_name,\n                                user_args.image_height,\n                                user_args.image_width,\n                                user_args.image_channels,\n                                user_args.resize)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
self_driving/data_manipulation/util.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport unittest\nimport numpy as np\nimport os\n\ncommand2int = {""up"": 0, ""left"": 1, ""right"": 2}\nint2command = {i[1]: i[0] for i in command2int.items()}\n\n\ndef run_test(testClass):\n    """"""\n    Function to run all the tests from a class of tests.\n\n    :param testClass: class for testing\n    :type testClass: unittest.TesCase\n    """"""\n    suite = unittest.TestLoader().loadTestsFromTestCase(testClass)\n    unittest.TextTestRunner(verbosity=2).run(suite)\n\n\ndef get_image_and_command(data_index,\n                          label_index,\n                          height=90,\n                          width=160,\n                          channels=3):\n    """"""\n    Get and reshape image\n    with parameters: 90(height), 160(width), 3(channels)\n    from data_index and get it\'s label\n    in label_index (e.g. \'right\')\n\n    :param data_index: index on the dataset array\n    :type data_index: numpy.ndarray\n    :param label_index: index on the labels array\n    :type label_index: numpy.ndarray\n    :param height: image height\n    :type heights: int\n    :param width: image width\n    :type width: int\n    :param channels: image channels\n    :type channels: int\n    :return: image, command\n    :rtype: numpy.ndarray, str\n    """"""\n    img_array = data_index.reshape((height, width, channels))\n    command = int2command[label_index[0]]\n    return img_array, command\n\n\ndef get_image(data_index,\n              height=90,\n              width=160,\n              channels=3):\n    """"""\n    Get and reshape image with parameters:\n    90(height), 160(width), 3(channels)\n    from data_index\n\n    :param data_index: index on the dataset array\n    :type data_index: numpy.ndarray\n    :param height: image height\n    :type heights: int\n    :param width: image width\n    :type width: int\n    :param channels: image channels\n    :type channels: int\n    :return: image\n    :rtype: numpy.ndarray\n    """"""\n    return data_index.reshape((height, width, channels))\n\n\ndef get_flat_shape(image):\n    """"""\n    Multiply each shape\n    component of image (tuple of array dimensions)\n\n    :param image: image\n    :type image: numpy.ndarray\n    :return: image\'s flat shape\n    :rtype: int\n    """"""\n    flat = 1\n    for i in range(len(image.shape)):\n        flat *= image.shape[i]\n    return flat\n\n\ndef shape2filename(data_shape):\n    """"""\n    Get each shape component and return a string\n    formatted to \'height_width_channels_\'\n\n    :param data_shape: dataset shape\n    :type data_shape: tuple\n    :return: shape as string\n    :rtype: str\n    """"""\n    name = """"\n    for i in data_shape:\n        name += ""{}_"".format(i)\n    return name\n\n\ndef load_dataset(data_path,\n                 labels_path):\n    """"""\n    Load and return dataset\n    arrays from data_path and\n    label arrays from labels_path\n\n    :param data_path: path to dataset\n    :type data_path: str (.npy file)\n    :param labels_path: path to labels\n    :type labels_path: str (.npy file)\n    :return: dataset, labels\n    :rtype: numpy.ndarray, numpy.ndarray\n    """"""\n    data = np.load(data_path)\n    labels = np.load(labels_path)\n    return data, labels\n\n\ndef save_dataset(data,\n                 labels,\n                 folder_path,\n                 data_shape,\n                 name):\n    """"""\n    Save data and labels in a directory as a numpy array binary file (NPY)\n\n    :param data: dataset\n    :type data: numpy.ndarray\n    :param labels: labels\n    :type labels: numpy.ndarray\n    :param folder_path: path to save dataset and labels\n    :type folder_path: str\n    :param data_shape: shape of numpy array dimensions\n    :type data_shape: tuple\n    :param name: name to save data and labels\n    :type name: str\n    """"""\n    shape = shape2filename(data_shape)\n    data_name = ""{}_{}data.npy"".format(name, shape)\n    label_name = ""{}_{}labels.npy"".format(name, shape)\n    data_path = os.path.join(folder_path, data_name)\n    labels_path = os.path.join(folder_path, label_name)\n    np.save(data_path, data)\n    np.save(labels_path, labels)\n'"
self_driving/ml_training/CNN.py,14,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\n\nclass CNN():\n    """"""\n    A general Convolutional Neural Network (CNN)\n\n    :param graph: computation graph\n    :type graph: tf.Graph\n    :param config:  config class holding info about the\n                    number of hidden layers (size of the list)\n                    and the number of neurons in each\n                    layer (number in the list), and\n                    the different activation functions.\n\n    :type config: Config\n    """"""\n    def __init__(self,\n                 graph,\n                 config):\n        self.activations = config.activations\n        self.architecture = config.architecture\n        if self.activations is not None:\n            assert len(self.architecture) - 1 == len(self.activations)\n        self.conv_architecture = config.conv_architecture\n        self.kernel_sizes = config.kernel_sizes\n        if self.kernel_sizes is None:\n            self.kernel_sizes = [5] * len(self.conv_architecture)\n        assert len(self.kernel_sizes) == len(self.conv_architecture)\n        self.pool_kernel = config.pool_kernel\n        if self.pool_kernel is None:\n            self.pool_kernel = [2] * len(self.conv_architecture)\n        assert len(self.pool_kernel) == len(self.conv_architecture)\n        self.graph = graph\n        self.height = config.height\n        self.width = config.width\n        self.channels = config.channels\n\n    def get_logits(self,\n                   img_input,\n                   reuse=None):\n        """"""\n        Get logits from img_input.\n\n        :param img_input: input image\n        :type img_input: tf.Tensor(shape=(None,height*width*channels),\n                                          dype=tf.float32)\n        :param reuse: param to control reuse variables\n        :type reuse: None or True\n        :return: logits\n        :rtype: tf.Tensor(shape=(None, categories),\n                          dype=tf.float32)\n        """"""\n        with self.graph.as_default():\n            with tf.variable_scope(""logits"", reuse=reuse):\n                img_input = tf.reshape(img_input,\n                                       [-1, self.height,\n                                        self.width,\n                                        self.channels])\n                # Convolutional and Pooling Layers\n                for i, units in enumerate(self.conv_architecture):\n                    kernel_size = [self.kernel_sizes[i], self.kernel_sizes[i]]\n                    conv_kernel_size = [self.pool_kernel[i],\n                                        self.pool_kernel[i]]\n                    conv = tf.contrib.layers.conv2d(inputs=img_input,\n                                                    num_outputs=units,\n                                                    kernel_size=kernel_size,\n                                                    padding=\'SAME\',\n                                                    activation_fn=tf.nn.relu)\n                    img_input = tf.contrib.layers.max_pool2d(inputs=conv,\n                                                             kernel_size=conv_kernel_size)  # noqa\n                # Reshaping\n                shape = img_input.get_shape()\n                flat_shape = int(shape[1] * shape[2] * shape[3])\n                tf_input = tf.reshape(img_input, (-1, flat_shape))\n\n                # Dense Layers\n                architecture_size = len(self.architecture)\n                for i, units in enumerate(self.architecture):\n                    if i != architecture_size - 1:\n                        if self.activations is None:\n                            activation = tf.nn.relu\n                        else:\n                            activation = self.activations[i]\n                        tf_input = tf.contrib.layers.fully_connected(inputs=tf_input,  # noqa\n                                                                     num_outputs=units,  # noqa\n                                                                     activation_fn=activation)  # noqa\n                    else:\n                        tf_input = tf.contrib.layers.fully_connected(inputs=tf_input,  # noqa\n                                                                     num_outputs=units,  # noqa\n                                                                     activation_fn=None)  # noqa\n            return tf_input\n'"
self_driving/ml_training/Config.py,12,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\n\nclass Config(object):\n    """"""\n    Holds model hyperparams.\n\n    :param height: image height\n    :type heights: int\n    :param width: image width\n    :type width: int\n    :param channels: image channels\n    :type channels: int\n    :param architecture: network dense architecture\n    :type architecture: list of int\n    :param activations: list of different tf functions\n    :param conv_architecture: convolutional architecture\n    :type conv_architecture: list of int\n    :param kernel_sizes: filter sizes\n    :type kernel_sizes: list of int\n    :param pool_kernel: pooling filter sizes\n    :type pool_kernel: list of int\n    :type activations: list of tf.nn.sigmoid, tf.nn.relu, tf.nn.tanh\n    :param batch_size: batch size for training\n    :type batch_size: int\n    :param epochs: number of epochs\n    :type epochs: int\n    :param num_steps: number of iterations for each epoch\n    :type num_steps: int\n    :param save_step: when step % save_step == 0, the model\n                      parameters are saved.\n    :type save_step: int\n    :param learning_rate: learning rate for the optimizer\n    :type learning_rate: float\n    :param optimizer: a optimizer from tensorflow.\n    :type optimizer: tf.train.GradientDescentOptimizer,\n                     tf.train.AdadeltaOptimizer,\n                     tf.train.AdagradOptimizer,\n                     tf.train.AdagradDAOptimizer,\n                     tf.train.MomentumOptimizer,\n                     tf.train.AdamOptimizer,\n                     tf.train.FtrlOptimizer,\n                     tf.train.ProximalGradientDescentOptimizer,\n                     tf.train.ProximalAdagradOptimizer,\n                     tf.train.RMSPropOptimizer\n    """"""\n    def __init__(self,\n                 height=90,\n                 width=160,\n                 channels=3,\n                 architecture=[722, 3],\n                 activations=None,\n                 conv_architecture=[32, 64],\n                 kernel_sizes=[5, 5],\n                 pool_kernel=None,\n                 batch_size=32,\n                 epochs=5,\n                 num_steps=1000,\n                 save_step=100,\n                 learning_rate=0.0054,\n                 optimizer=tf.train.GradientDescentOptimizer):\n        self.height = height\n        self.width = width\n        self.channels = channels\n        self.architecture = architecture\n        self.activations = activations\n        self.conv_architecture = conv_architecture\n        self.kernel_sizes = kernel_sizes\n        self.pool_kernel = pool_kernel\n        self.batch_size = batch_size\n        self.epochs = epochs\n        self.num_steps = num_steps\n        self.save_step = save_step\n        self.learning_rate = learning_rate\n        self.optimizer = optimizer\n\n    def __str__(self):\n        """"""\n        Get all attributs values.\n\n        :return: all hyperparams as a string\n        :rtype: str\n        """"""\n        if self.kernel_sizes is None:\n            kernel_sizes = [5] * len(self.conv_architecture)\n        else:\n            kernel_sizes = self.kernel_sizes\n        if self.pool_kernel is None:\n            pool_kernel = [2] * len(self.conv_architecture)\n        else:\n            pool_kernel = self.pool_kernel\n        if self.activations is None:\n            activations = [""relu""] * len(self.architecture)\n        else:\n            activations = self.activations\n        status = ""height = {}\\n"".format(self.height)\n        status += ""width = {}\\n"".format(self.width)\n        status += ""channels = {}\\n"".format(self.channels)\n        status += ""architecture = {}\\n"".format(self.architecture)\n        status += ""activations = {}\\n"".format(activations)\n        status += ""batch_size = {}\\n"".format(self.batch_size)\n        status += ""conv_architecture = {}\\n"".format(self.conv_architecture)\n        status += ""kernel_sizes = {}\\n"".format(kernel_sizes)\n        status += ""pool_kernel = {}\\n"".format(pool_kernel)\n        status += ""batch_size = {}\\n"".format(self.batch_size)\n        status += ""epochs = {}\\n"".format(self.epochs)\n        status += ""num_steps = {}\\n"".format(self.num_steps)\n        status += ""save_step = {}\\n"".format(self.save_step)\n        status += ""learning_rate = {}\\n"".format(self.learning_rate)\n        status += ""optimizer = {}\\n"".format(self.optimizer)\n        return status\n'"
self_driving/ml_training/DFN.py,9,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\n\nclass DFN():\n    """"""\n    A general Deep Feedforward Network (DFN)\n\n    :param graph: computation graph\n    :type graph: tf.Graph\n    :param config:  config class holding info about the\n                    number of hidden layers (size of the list)\n                    and the number of neurons in each\n                    layer (number in the list), and\n                    the different activation functions.\n\n    :type config: Config\n    """"""\n    def __init__(self,\n                 graph,\n                 config):\n        self.activations = config.activations\n        self.architecture = config.architecture\n        if self.activations is not None:\n            assert len(self.architecture) - 1 == len(self.activations)\n        self.graph = graph\n\n    def get_logits(self,\n                   img_input,\n                   reuse=None):\n        """"""\n        Get logits from img_input.\n\n        :param img_input: input image\n        :type img_input: tf.Tensor(shape=(None,height*width*channels),\n                                          dype=tf.float32)\n        :param reuse: param to control reuse variables\n        :type reuse: None or True\n        :return: logits\n        :rtype: tf.Tensor(shape=(None, categories),\n                          dype=tf.float32)\n        """"""\n        with self.graph.as_default():\n            with tf.variable_scope(""logits"", reuse=reuse):\n                tf_input = img_input\n                architecture_size = len(self.architecture)\n                for i, units in enumerate(self.architecture):\n                    if i != architecture_size - 1:\n                        if self.activations is None:\n                            activation = tf.nn.relu\n                        else:\n                            activation = self.activations[i]\n                        tf_input = tf.contrib.layers.fully_connected(inputs=tf_input,  # noqa\n                                                                     num_outputs=units,  # noqa\n                                                                     activation_fn=activation)  # noqa\n                    else:\n                        tf_input = tf.contrib.layers.fully_connected(inputs=tf_input,  # noqa\n                                                                     num_outputs=units,  # noqa\n                                                                     activation_fn=None)  # noqa\n                return tf_input\n'"
self_driving/ml_training/DataHolder.py,1,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport numpy as np\nimport os\nimport sys\nimport inspect\n\nalmost_current = os.path.abspath(inspect.getfile(inspect.currentframe()))\ncurrentdir = os.path.dirname(almost_current)\nparentdir = os.path.dirname(currentdir)\nsys.path.insert(0, parentdir)\n\nfrom data_manipulation.data_aug import extend_dataset_flip_axis, dataset_augmentation  # noqa\nfrom data_manipulation.data_aug import binarize_dataset, gray_dataset, green_dataset  # noqa\nfrom data_manipulation.data_mani import data_cut, create_record  # noqa\n\n\nclass DataHolder():\n    """"""\n    Class that preprocess all data.\n    If the data is already processed, i.e., there is already\n    three tfrecords files, you can pass the files as a list\n    of paths as the variable ""records"". Note that in this\n    list the expected order is train.tfrecords, valid.tfrecords,\n    test.tfrecords.\n\n    :param config: config class with all hyper param information\n    :type config: Config\n    :param data_path: path to load data np.array\n    :type data_path: str\n    :param record_path: path to load labels np.array\n    :type label_path: str\n    :param record_path: path to save tfrecord\n    :type record_path: str\n    :param flip: param to control if the data\n                         will be flipped\n    :type flip: boolean\n    :param augmentation: param to control if the data\n                         will augmented\n    :type augmentation: boolean\n    :param gray: param to control if the data\n                 will be grayscale images\n    :type gray: boolean\n    :param green: param to control if the data will use only\n                  the green channel\n    :type green: boolean\n    :param binary: param to control if the data will be binarized\n    :type binary: boolean\n    :param records: list of paths to tfrecords\n    :type records: list of str\n    """"""\n    def __init__(self,\n                 config,\n                 data_path=None,\n                 label_path=None,\n                 record_path=None,\n                 flip=False,\n                 augmentation=False,\n                 gray=False,\n                 green=False,\n                 binary=False,\n                 records=None):\n        self.config = config\n        self.data_path = data_path\n        self.label_path = label_path\n        self.record_path = record_path\n        self.flip = flip\n        self.augmentation = augmentation\n        self.gray = gray\n        self.green = green\n        self.binary = binary\n        self.records = records\n\n    def create_records(self):\n        """"""\n        Take the arrays in self.data_path and self.label_path,\n        divide them into train, test and valid dataset, shufle them,\n        and processed them (flip, add augmentation, transform in grayscale,\n        transform in image with only the green channel, transform\n        in binarized image) and save all records as:\n              self.record_path + \'_train.tfrecords\'\n              self.record_path + \'_valid.tfrecords\'\n              self.record_path + \'_test.tfrecords\'\n        """"""\n        if self.gray or self.green or self.binary:\n            msg = ""only one condition should be True""\n            assert self.gray ^ self.green ^ self.binary, msg\n\n        data = np.load(self.data_path)\n        labels = np.load(self.label_path)\n\n        # fliping the original data\n        # and dividing it into train, test and valid\n        if self.flip:\n            data, labels = extend_dataset_flip_axis(data,\n                                                    labels,\n                                                    self.config.height,\n                                                    self.config.width,\n                                                    self.config.channels)\n        train_data, train_labels, valid_data, valid_labels, test_data, test_labels = data_cut(data, labels)  # noqa\n\n        # applying data augmentation to the train dataset\n        if self.augmentation:\n            train_data, train_labels = dataset_augmentation(train_data,\n                                                            train_labels,\n                                                            self.config.height,\n                                                            self.config.width,\n                                                            self.config.channels)  # noqa\n        # transforming all images into grayscale\n        if self.gray:\n            train_data, _ = gray_dataset(train_data,\n                                         self.config.height,\n                                         self.config.width,\n                                         self.config.channels)\n            valid_data, _ = gray_dataset(valid_data,\n                                         self.config.height,\n                                         self.config.width,\n                                         self.config.channels)\n            test_data, _ = gray_dataset(test_data,\n                                        self.config.height,\n                                        self.config.width,\n                                        self.config.channels)\n            self.config.channels = 1\n\n        # transforming all images into images with only green channel\n        if self.green:\n            train_data, _ = green_dataset(train_data,\n                                          self.config.height,\n                                          self.config.width,\n                                          self.config.channels)\n            valid_data, _ = green_dataset(valid_data,\n                                          self.config.height,\n                                          self.config.width,\n                                          self.config.channels)\n            test_data, _ = green_dataset(test_data,\n                                         self.config.height,\n                                         self.config.width,\n                                         self.config.channels)\n            self.config.channels = 1\n\n        # transforming all images into binary images\n        if self.binary:\n            train_data, _ = binarize_dataset(train_data,\n                                             self.config.height,\n                                             self.config.width,\n                                             self.config.channels)\n            valid_data, _ = binarize_dataset(valid_data,\n                                             self.config.height,\n                                             self.config.width,\n                                             self.config.channels)\n            test_data, _ = binarize_dataset(test_data,\n                                            self.config.height,\n                                            self.config.width,\n                                            self.config.channels)\n            self.config.channels = 1\n\n        # transforming all data into tf.records\n        tfrecords_filename_train = self.record_path + \'_train.tfrecords\'\n        tfrecords_filename_valid = self.record_path + \'_valid.tfrecords\'\n        tfrecords_filename_test = self.record_path + \'_test.tfrecords\'\n        create_record(tfrecords_filename_train,\n                      train_data,\n                      train_labels,\n                      self.config.height,\n                      self.config.width,\n                      self.config.channels)\n        create_record(tfrecords_filename_valid,\n                      valid_data,\n                      valid_labels,\n                      self.config.height,\n                      self.config.width,\n                      self.config.channels)\n        create_record(tfrecords_filename_test,\n                      test_data,\n                      test_labels,\n                      self.config.height,\n                      self.config.width,\n                      self.config.channels)\n        self.records = [tfrecords_filename_train,\n                        tfrecords_filename_valid,\n                        tfrecords_filename_test]\n\n    def get_train_tfrecord(self):\n        """"""\n        retun path to train tfrecords\n\n        :return: path to train tfrecords\n        :rtype: str\n        """"""\n        if self.records is None:\n            return ""train.tfrecords""\n        else:\n            return self.records[0]\n\n    def get_valid_tfrecord(self):\n        """"""\n        retun path to valid tf records\n\n        :return: path to valid tfrecords\n        :rtype: str\n        """"""\n        if self.records is None:\n            return ""valid.tfrecords""\n        else:\n            return self.records[1]\n\n    def get_test_tfrecord(self):\n        """"""\n        retun path to test tf records\n\n        :return: path to test tfrecords\n        :rtype: str\n        """"""\n        if self.records is None:\n            return ""test.tfrecords""\n        else:\n            return self.records[2]\n'"
self_driving/ml_training/Trainer.py,33,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function\nimport os\nimport numpy as np\nimport tensorflow as tf\ntry:\n    from util import get_iterator, parser_with_normalization\nexcept ImportError:\n    from ml_training.util import get_iterator, parser_with_normalization\n\n\nclass Trainer():\n    """"""\n    Class that trains a model and uses a model to predict\n    images.\n\n    :param graph: computation graph\n    :type graph: tf.Graph\n    :param config:  config class holding info about the\n                    number of hidden layers (size of the list)\n                    and the number of neurons in each\n                    layer (number in the list), and\n                    the different activation functions.\n\n    :type config: Config\n    :param model: deep learning model\n    :type model: DFN or CNN\n    :param dataholder: dataholder class containing all\n                      the data information\n    :type dataholder: DataHolder\n    :param save_dir: folder\'s name to save model\'s params\n    :type save_dir: str\n    """"""\n    def __init__(self,\n                 graph,\n                 config,\n                 model,\n                 dataholder,\n                 save_dir=\'checkpoints/\'):\n        self.tf_optimizer = config.optimizer\n        self.batch_size = config.batch_size\n        self.epochs = config.epochs\n        self.iterations = config.num_steps\n        self.learning_rate = config.learning_rate\n        self.height = config.height\n        self.width = config.width\n        self.channels = config.channels\n        self.show_step = config.save_step\n        self.tfrecords_train = dataholder.get_train_tfrecord()\n        self.tfrecords_valid = dataholder.get_valid_tfrecord()\n        self.tfrecords_test = dataholder.get_test_tfrecord()\n        self.graph = graph\n        self.model = model\n        self.save_dir = save_dir\n        if not os.path.exists(self.save_dir):\n            os.makedirs(self.save_dir)\n        self.build_graph()\n\n    def build_graph(self):\n        """"""\n        Build tensforflow graph. Main tensors:\n\n            self.tf_prediction: model prediction.\n            self.tf_train_loss: loss of a batch of the train dataset.\n            self.valid_accuracy: accuracy of batch of the valid dataset.\n        """"""\n        flat_size = self.height * self.width * self.channels\n        with self.graph.as_default():\n            with tf.name_scope(""placeholders""):\n                self.input_image = tf.placeholder(tf.float32,\n                                                  shape=(None, flat_size),\n                                                  name=""input_image"")\n            with tf.name_scope(""iterators""):\n                self.iterator_train = get_iterator(self.tfrecords_train,\n                                                   self.batch_size,\n                                                   parser_with_normalization)\n                self.iterator_valid = get_iterator(self.tfrecords_valid,\n                                                   self.batch_size,\n                                                   parser_with_normalization)\n            with tf.name_scope(""train_loss""):\n                train_images, train_labels = self.iterator_train.get_next()\n                train_images = tf.reshape(train_images,\n                                          (self.batch_size, flat_size))\n                train_labels = tf.reshape(train_labels, (self.batch_size,))\n                train_logits = self.model.get_logits(train_images)\n                tf_train_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=train_labels,  # noqa\n                                                                           logits=train_logits)  # noqa\n                self.tf_train_loss = tf.reduce_mean(tf_train_loss)\n\n            with tf.name_scope(""optimization""):\n                optimizer = self.tf_optimizer(self.learning_rate)\n                self.update_weights = optimizer.minimize(self.tf_train_loss)\n\n            with tf.name_scope(""valid_loss""):\n                valid_images, valid_labels = self.iterator_valid.get_next()\n                valid_images = tf.reshape(valid_images,\n                                          (self.batch_size, flat_size))\n                valid_labels = tf.reshape(valid_labels, (self.batch_size,))\n                valid_logits = self.model.get_logits(valid_images, reuse=True)\n                valid_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=valid_labels,  # noqa\n                                                                           logits=valid_logits)  # noqa\n                self.tf_valid_loss = tf.reduce_mean(valid_loss)\n\n            with tf.name_scope(""valid_accuracy""):\n                valid_prediction = tf.nn.softmax(valid_logits)\n                valid_prediction = tf.argmax(valid_prediction, axis=1)\n                valid_prediction = tf.cast(valid_prediction, dtype=tf.int32)\n                valid_prediction = tf.equal(valid_prediction,\n                                            valid_labels)\n                self.valid_accuracy = tf.reduce_mean(tf.cast(valid_prediction,\n                                                             \'float\'),\n                                                     name=\'valid_accuracy\')\n            with tf.name_scope(""prediction""):\n                tf_prediction = self.model.get_logits(self.input_image,\n                                                      reuse=True)\n                self.tf_prediction = tf.nn.softmax(tf_prediction)\n\n            with tf.name_scope(""saver""):\n                self.saver = tf.train.Saver()\n                self.save_path = os.path.join(self.save_dir, \'best_validation\')\n\n    def get_accuracy(self, iterator_initializer, accuracy_tensor, iterations):\n        """"""\n        Method to compute accuracy.\n\n        :param iterator_initializer: initializer of the iterator\n        :type iterator_initializer: tf.contrib.data.Iterator method\n        :param accuracy_tensor: tensor with accuracy information\n        :type accuracy_tensor: tf.Tensor(shape=(), dype=tf.float32)\n        :param iterations: number of iterations\n        :type iterations: int\n        :return: accuracy on a dataset\n        :rtype: float\n        """"""\n        with tf.Session(graph=self.graph) as sess:\n            sess.run(iterator_initializer)\n            if os.listdir(self.save_dir) == []:\n                sess.run(tf.global_variables_initializer())\n            else:\n                self.saver.restore(sess=sess, save_path=self.save_path)\n            acc = 0\n            for _ in range(iterations):\n                acc += sess.run(accuracy_tensor)\n        return acc / iterations\n\n    def get_valid_accuracy(self,\n                           iterations=50):\n        """"""\n        Method to compute the accuracy of the model\'s predictions\n        on the valid dataset.\n\n        :param iterations: number of iterations\n        :type iterations: int\n        :return: accuracy on the valid dataset\n        :rtype: float\n        """"""\n        return self.get_accuracy(self.iterator_valid.initializer,\n                                 self.valid_accuracy,\n                                 iterations)\n\n    def fit(self, verbose=True):\n        """"""\n        Fitting the data.\n\n        :param verbose: param to control printing\n        :type verbose: bool\n        """"""\n        best_valid_loss = float(""inf"")\n        with tf.Session(graph=self.graph) as sess:\n            sess.run(self.iterator_train.initializer)\n            sess.run(self.iterator_valid.initializer)\n            sess.run(tf.global_variables_initializer())\n            show_loss = sess.run(self.tf_train_loss)\n            for epoch in range(self.epochs):\n                for step in range(self.iterations):\n                    _, loss = sess.run([self.update_weights,\n                                        self.tf_train_loss])\n                    show_loss = loss\n                    if step % self.show_step == 0:\n                        if verbose:\n                            info = \'Epoch {0:5},\'.format(epoch + 1)\n                            info += \' step {0:5}:\'.format(step + 1)\n                            info += \' train_loss = {0:.6f} |\'.format(show_loss)\n                            info += \' valid_loss = {0:.6f}\\n\'.format(best_valid_loss)  # noqa\n                            print(info, end=\'\')\n                        valid_loss = sess.run(self.tf_valid_loss)\n                        if valid_loss < best_valid_loss:\n                            self.saver.save(sess=sess,\n                                            save_path=self.save_path)\n                            best_valid_loss = valid_loss\n\n    def predict_prob(self, img):\n        """"""\n        Predict the category using img as input.\n\n        :param img: image\n        :type img: np.array\n        :return: batch of probabilities\n        :rtype: np.array\n        """"""\n        type_msg = ""not in the correct type""\n        assert img.dtype == np.float32, type_msg\n        with tf.Session(graph=self.graph) as sess:\n            if os.listdir(self.save_dir) == []:\n                sess.run(tf.global_variables_initializer())\n            else:\n                self.saver.restore(sess=sess, save_path=self.save_path)\n            feed_dict = {self.input_image: img}\n            result = sess.run(self.tf_prediction,\n                              feed_dict=feed_dict)\n            result = result.astype(np.float32)\n        return result\n\n    def predict(self, img):\n        """"""\n        Predict the category using img as input.\n\n        :param img: image\n        :type img: np.array\n        :return: batch of predictions\n        :rtype: np.array\n        """"""\n        result = self.predict_prob(img)\n        result = np.argmax(result, axis=1)\n        result = result.astype(np.int32)\n        return result\n'"
self_driving/ml_training/__init__.py,0,b''
self_driving/ml_training/acc_test.py,5,"b'import tensorflow as tf\nimport os\nimport numpy as np\nimport argparse\nimport sys\nimport inspect\n\nfrom DataHolder import DataHolder\nfrom Config import Config\nfrom Trainer import Trainer\nfrom DFN import DFN\nfrom CNN import CNN\nfrom util import reconstruct_from_record\nfrom util import int2command\n\n\nalmost_current = os.path.abspath(inspect.getfile(inspect.currentframe()))\ncurrentdir = os.path.dirname(almost_current)\nparentdir = os.path.dirname(currentdir)\nsys.path.insert(0, parentdir)\n\nfrom plot.util import plotconfusion  # noqa\n\n\ndef acc(name_tfrecords,\n        records,\n        height,\n        width,\n        channels,\n        architecture,\n        activations,\n        conv_architecture,\n        kernel_sizes,\n        pool_kernel,\n        test,\n        name,\n        conv):\n    """"""\n    Checks model\'s accuracy\n\n    :param name_tfrecords: name of the used tfrecords\n    :type name_tfrecords: str\n    :param records: list of paths to train, test, and valid tfrecords\n    :type records: list of str\n    :param height: image height\n    :type heights: int\n    :param width: image width\n    :type width: int\n    :param channels: image channels\n    :type channels: int\n    :param architecture: network architecture\n    :type architecture: list of int\n    :param activations: list of different tf functions\n    :type activations: list of tf.nn.sigmoid, tf.nn.relu, tf.nn.tanh\n    :param conv_architecture: convolutional architecture\n    :type conv_architecture: list of int\n    :param kernel_sizes: filter sizes\n    :type kernel_sizes: list of int\n    :param pool_kernel: pooling filter sizes\n    :type pool_kernel: list of int\n    :param test: param to control if the test accuracy will be printed.\n    :type test: bool\n    :param name: name to save the confusion matrix plot.\n    :type name: str\n    :param conv: param to control if the model will be a CNN\n                 or DFN\n    :type conv: bool\n    """"""\n\n    config = Config(height=height,\n                    width=width,\n                    channels=channels,\n                    architecture=architecture,\n                    activations=activations,\n                    conv_architecture=conv_architecture,\n                    kernel_sizes=kernel_sizes,\n                    pool_kernel=pool_kernel)\n\n    data = DataHolder(config,\n                      records=records)\n\n    graph = tf.Graph()\n    if conv:\n        net_name = ""CNN""\n        network = CNN(graph, config)\n    else:\n        net_name = ""DFN""\n        network = DFN(graph, config)\n    trainer = Trainer(graph, config, network, data)\n    print(""\\nAccuracy of the {} model in the {} data\\n"".format(net_name,\n                                                               name_tfrecords))\n    print(""params:\\n{}\\n"".format(str(config)))\n    if not os.path.exists(""checkpoints""):\n        print(""===Accuracy of a non trained model==="")\n\n    valid_images, valid_labels, _ = reconstruct_from_record(data.get_valid_tfrecord(), bound=10000)  # noqa\n    valid_images = valid_images.astype(np.float32) / 255\n    valid_pred = trainer.predict(valid_images)\n    valid_labels = valid_labels.reshape((valid_labels.shape[0],))\n    plotconfusion(valid_labels,\n                  valid_pred,\n                  name + ""_valid.png"",\n                  int2command,\n                  classes=[""left"", ""right"", ""up""])\n\n    if test:\n        test_images, test_labels, _ = reconstruct_from_record(data.get_test_tfrecord(), bound=10000)  # noqa\n        test_images = test_images.astype(np.float32) / 255\n        test_pred = trainer.predict(test_images)\n        test_labels = test_labels.reshape((test_labels.shape[0],))\n        plotconfusion(test_labels,\n                      test_pred,\n                      name + ""_test.png"",\n                      int2command,\n                      classes=[""left"", ""right"", ""up""])\n\n\ndef main():\n    """"""\n    Main script to check model\'s accuracy using one kind of data.\n    """"""\n    parser = argparse.ArgumentParser(description=""Checks model\'s accuracy"")\n    parser.add_argument(""-n"",\n                        ""--name_tfrecords"",\n                        type=str,\n                        default=""data"",\n                        help=""name for tfrecords (default=data)"")  # noqa\n    parser.add_argument(""-c"",\n                        ""--channels"",\n                        type=int,\n                        default=3,\n                        help=""number of channels (default=3)"")\n    parser.add_argument(""-he"",\n                        ""--height"",\n                        type=int,\n                        default=90,\n                        help=""image height (default=90)"")\n    parser.add_argument(""-w"",\n                        ""--width"",\n                        type=int,\n                        default=160,\n                        help=""image width (default=160)"")\n    parser.add_argument(\'-a\',\n                        \'--architecture\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'sizes for hidden layers and output layer, should end with least ""3"" !, (default=[3])\',  # noqa\n                        default=[3])\n    parser.add_argument(\'-ac\',\n                        \'--activations\',\n                        type=str,\n                        nargs=\'+\',\n                        help=\'activations: relu, sigmoid, tanh (defaul=None)\',\n                        default=None)\n    parser.add_argument(\'-conva\',\n                        \'--conv_architecture\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'filters for conv layers (default=[32, 64])\',\n                        default=[32, 64])\n    parser.add_argument(\'-k\',\n                        \'--kernel_sizes\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'kernel sizes for conv layers (default=None - 5 for every layer)\',  # noqa\n                        default=None)\n    parser.add_argument(\'-p\',\n                        \'--pool_kernel\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'kernel sizes for pooling layers (default=None - 2 for every layer)\',  # noqa\n                        default=None)\n    parser.add_argument(""-t"",\n                        ""--test"",\n                        action=""store_true"",\n                        default=False,\n                        help=""print test results and calculate confusion matrix (default=False)"")  # noqa\n    parser.add_argument(""-cm"",\n                        ""--cm_name"",\n                        type=str,\n                        default=""Confusion_Matrix"",\n                        help=""name to save confusion matrix plot (default=Confusion_Matrix)"")  # noqa\n    parser.add_argument(""-conv"",\n                        ""--conv"",\n                        action=""store_true"",\n                        default=False,\n                        help=""Use convolutional network (default=False)"")\n    args = parser.parse_args()\n    records = [""_train.tfrecords"", ""_valid.tfrecords"", ""_test.tfrecords""]\n    new_records = []\n    for record in records:\n        record = args.name_tfrecords + record\n        new_records.append(record)\n\n    activations_dict = {""relu"": tf.nn.relu,\n                        ""sigmoid"": tf.nn.sigmoid,\n                        ""tanh"": tf.nn.tanh}\n    if args.activations is not None:\n        activations = [activations_dict[act] for act in args.activations]\n    else:\n        activations = args.activations\n\n    acc(name_tfrecords=args.name_tfrecords,\n        records=new_records,\n        height=args.height,\n        width=args.width,\n        channels=args.channels,\n        architecture=args.architecture,\n        activations=activations,\n        conv_architecture=args.conv_architecture,\n        kernel_sizes=args.kernel_sizes,\n        pool_kernel=args.pool_kernel,\n        test=args.test,\n        name=args.cm_name,\n        conv=args.conv)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
self_driving/ml_training/best_architecture.py,18,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\nimport os\nimport numpy as np\nimport shutil\nimport argparse\nfrom DataHolder import DataHolder\nfrom Config import Config\nfrom Trainer import Trainer\nfrom DFN import DFN\nfrom CNN import CNN\nfrom util import reconstruct_from_record, accuracy_per_category\nfrom util import int2command, get_random_architecture_and_activations\n\n\ndef architecture_search(name_tfrecords,\n                        records,\n                        height,\n                        width,\n                        channels,\n                        conv_architecture,\n                        kernel_sizes,\n                        pool_kernel,\n                        batch_size,\n                        epochs,\n                        num_steps,\n                        save_step,\n                        learning_rate,\n                        optimizer,\n                        experiments,\n                        deepest_net_size,\n                        conv):\n    """"""\n    Script to search different architectures for a DFN,\n    the result is saved on the file architecture_results.txt\n\n    :param name_tfrecords: name of the used tfrecords\n    :type name_tfrecords: str\n    :param records: list of paths to train, test, and valid tfrecords\n    :type records: list of str\n    :param height: image height\n    :type heights: int\n    :param width: image width\n    :type width: int\n    :param channels: image channels\n    :type channels: int\n    :param conv_architecture: convolutional architecture\n    :type conv_architecture: list of int\n    :param kernel_sizes: filter sizes\n    :type kernel_sizes: list of int\n    :param pool_kernel: pooling filter sizes\n    :type pool_kernel: list of int\n    :param batch_size: batch size for training\n    :type batch_size: int\n    :param epochs: number of epochs\n    :type epochs: int\n    :param num_steps: number of iterations for each epoch\n    :type num_steps: int\n    :param save_step: when step % save_step == 0, the model\n                      parameters are saved.\n    :type save_step: int\n    :param learning_rate: learning rate for the optimizer\n    :type learning_rate: float\n    :param optimizer: a optimizer from tensorflow.\n    :type optimizer: tf.train.GradientDescentOptimizer,\n                     tf.train.AdadeltaOptimizer,\n                     tf.train.AdagradOptimizer,\n                     tf.train.AdagradDAOptimizer,\n                     tf.train.AdamOptimizer,\n                     tf.train.FtrlOptimizer,\n                     tf.train.ProximalGradientDescentOptimizer,\n                     tf.train.ProximalAdagradOptimizer,\n                     tf.train.RMSPropOptimizer\n    :param experiments: number of experiments to be made\n    :type experiments: int\n    :param deepest_net_size: size of the deepest network\n    :type deepest_net_size: int\n    :param conv: param to control if the model will be a CNN\n                 or DFN\n    :type conv: bool\n    """"""\n    sizes = np.random.randint(1, deepest_net_size, experiments)\n    hidden_layers, activations = get_random_architecture_and_activations(network_sizes=sizes)  # noqa\n    numeric_result = []\n    results = []\n    info = []\n    if conv:\n        net_name = ""CNN""\n    else:\n        net_name = ""DFN""\n\n    header = ""\\nSearching {} architecture in the {} data\\n"".format(net_name,\n                                                                   name_tfrecords)  # noqa\n    print(header)\n    for arch, act in zip(hidden_layers, activations):\n        config = Config(height=height,\n                        width=width,\n                        channels=channels,\n                        architecture=arch,\n                        activations=act,\n                        conv_architecture=conv_architecture,\n                        kernel_sizes=kernel_sizes,\n                        pool_kernel=pool_kernel,\n                        batch_size=batch_size,\n                        epochs=epochs,\n                        num_steps=num_steps,\n                        save_step=save_step,\n                        learning_rate=learning_rate,\n                        optimizer=optimizer)\n\n        data = DataHolder(config,\n                          records=records)\n        name = str(arch)\n        print(name + "":\\n"")\n        graph = tf.Graph()\n        if conv:\n            network = CNN(graph, config)\n        else:\n            network = DFN(graph, config)\n        trainer = Trainer(graph, config, network, data)\n        trainer.fit(verbose=True)\n        valid_acc = trainer.get_valid_accuracy()\n        numeric_result.append(valid_acc)\n        name += \': valid_acc = {0:.6f} | \'.format(valid_acc)\n        valid_images, valid_labels, _ = reconstruct_from_record(data.get_valid_tfrecord())  # noqa\n        valid_images = valid_images.astype(np.float32) / 255\n        valid_pred = trainer.predict(valid_images)\n        acc_cat = accuracy_per_category(valid_pred, valid_labels, categories=3)\n        for i, cat_result in enumerate(acc_cat):\n            name += int2command[i] + "": = {0:.6f}, "".format(cat_result)\n        results.append(name)\n        if os.path.exists(""checkpoints""):\n            shutil.rmtree(""checkpoints"")\n        info.append(str(config))\n\n    best_result = max(list(zip(numeric_result, hidden_layers, info)))\n    result_string = """"""In an experiment with {0} architectures\n    the best one is {1} with valid accuracy of {2}.\n    \\nThe training uses the following params:\n    \\n{3}\\n"""""".format(experiments,\n                      best_result[1],\n                      best_result[0],\n                      best_result[2])\n    file = open(""architecture_results.txt"", ""w"")\n    file.write(header)\n    file.write(""Results for different architectures\\n"")\n    for result in results:\n        result += ""\\n""\n        file.write(result)\n    file.write(""\\n"")\n    file.write(result_string)\n    file.close()\n\n\ndef main():\n    """"""\n    Main script to perform architecture search.\n    """"""\n    parser = argparse.ArgumentParser(description=\'Perform architecture search\')\n    parser.add_argument(""-n"",\n                        ""--name_tfrecords"",\n                        type=str,\n                        default=""data"",\n                        help=""name for tfrecords (default=data)"")  # noqa\n    parser.add_argument(""-ex"",\n                        ""--experiments"",\n                        type=int,\n                        default=10,\n                        help=""number of experiments to be done (default=10)"")  # noqa\n    parser.add_argument(""-d"",\n                        ""--deep"",\n                        type=int,\n                        default=4,\n                        help=""deep of the model (default=4)"")\n    parser.add_argument(""-he"",\n                        ""--height"",\n                        type=int,\n                        default=90,\n                        help=""image height (default=90)"")\n    parser.add_argument(""-w"",\n                        ""--width"",\n                        type=int,\n                        default=160,\n                        help=""image width (default=160)"")\n    parser.add_argument(""-c"",\n                        ""--channels"",\n                        type=int,\n                        default=3,\n                        help=""number of channels (default=3)"")\n    parser.add_argument(\'-conva\',\n                        \'--conv_architecture\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'filters for conv layers (default=[32, 64])\',  # noqa\n                        default=[32, 64])\n    parser.add_argument(\'-k\',\n                        \'--kernel_sizes\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'kernel sizes for conv layers (default=None - 5 for every layer)\',  # noqa\n                        default=None)\n    parser.add_argument(\'-p\',\n                        \'--pool_kernel\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'kernel sizes for pooling layers (default=None - 2 for every layer)\',  # noqa\n                        default=None)\n    parser.add_argument(""-b"",\n                        ""--batch_size"",\n                        type=int,\n                        default=32,\n                        help=""batch size (default=32)"")\n    parser.add_argument(""-e"",\n                        ""--epochs"",\n                        type=int,\n                        default=5,\n                        help=""epochs for training (default=5)"")\n    parser.add_argument(""-ns"",\n                        ""--num_steps"",\n                        type=int,\n                        default=1000,\n                        help=""number of steps for each epoch (default=1000)"")\n    parser.add_argument(""-ss"",\n                        ""--save_step"",\n                        type=int,\n                        default=100,\n                        help=""number of steps to save variables (default=100)"")\n    parser.add_argument(""-lr"",\n                        ""--learning_rate"",\n                        type=float,\n                        default=0.02,\n                        help=""learning rate (default=0.02)"")\n    opt_list = """"""optimizers: GradientDescent,\n                              Adadelta,\n                              Adagrad,\n                              Adam,\n                              Ftrl,\n                              ProximalGradientDescent,\n                              ProximalAdagrad,\n                              RMSProp""""""\n    parser.add_argument(""-o"",\n                        ""--optimizer"",\n                        type=str,\n                        default=""GradientDescent"",\n                        help=opt_list + ""(default=GradientDescent)"")\n    parser.add_argument(""-conv"",\n                        ""--conv"",\n                        action=""store_true"",\n                        default=False,\n                        help=""Use convolutional network (default=False)"")\n    args = parser.parse_args()\n    records = [""_train.tfrecords"", ""_valid.tfrecords"", ""_test.tfrecords""]\n    new_records = []\n    for record in records:\n        record = args.name_tfrecords + record\n        new_records.append(record)\n\n    optimizer_dict = {""GradientDescent"": tf.train.GradientDescentOptimizer,  # noqa\n                      ""Adadelta"": tf.train.AdadeltaOptimizer,\n                      ""Adagrad"": tf.train.AdagradOptimizer,\n                      ""Adam"": tf.train.AdamOptimizer,\n                      ""Ftrl"": tf.train.FtrlOptimizer,\n                      ""ProximalGradientDescent"": tf.train.ProximalGradientDescentOptimizer,  # noqa\n                      ""ProximalAdagrad"": tf.train.ProximalAdagradOptimizer,  # noqa\n                      ""RMSProp"": tf.train.RMSPropOptimizer}  # noqa\n    optimizer = optimizer_dict[args.optimizer]\n    architecture_search(name_tfrecords=args.name_tfrecords,\n                        records=new_records,\n                        height=args.height,\n                        width=args.width,\n                        channels=args.channels,\n                        experiments=args.experiments,\n                        deepest_net_size=args.deep,\n                        conv_architecture=args.conv_architecture,\n                        kernel_sizes=args.kernel_sizes,\n                        pool_kernel=args.pool_kernel,\n                        batch_size=args.batch_size,\n                        epochs=args.epochs,\n                        num_steps=args.num_steps,\n                        save_step=args.save_step,\n                        learning_rate=args.learning_rate,\n                        optimizer=optimizer,\n                        conv=args.conv)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
self_driving/ml_training/best_learning_rate.py,22,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\nimport os\nimport numpy as np\nimport shutil\nimport argparse\nfrom DataHolder import DataHolder\nfrom Config import Config\nfrom Trainer import Trainer\nfrom DFN import DFN\nfrom CNN import CNN\nfrom util import reconstruct_from_record, accuracy_per_category\nfrom util import int2command\n\n\ndef lr_search(name_tfrecords,\n              records,\n              height,\n              width,\n              channels,\n              architecture,\n              activations,\n              conv_architecture,\n              kernel_sizes,\n              pool_kernel,\n              batch_size,\n              epochs,\n              num_steps,\n              save_step,\n              optimizer,\n              experiments,\n              conv,\n              divisor):\n    """"""\n    Script to run different experiments\n    to search a learning rate value,\n    the result is saved on the file learning_rate_results.txt\n\n    :param name_tfrecords: name of the used tfrecords\n    :type name_tfrecords: str\n    :param records: list of paths to train, test, and valid tfrecords\n    :type records: list of str\n    :param height: image height\n    :type heights: int\n    :param width: image width\n    :type width: int\n    :param channels: image channels\n    :type channels: int\n    :param architecture: network architecture\n    :type architecture: list of int\n    :param activations: list of different tf functions\n    :type activations: list of tf.nn.sigmoid, tf.nn.relu, tf.nn.tanh\n    :param conv_architecture: convolutional architecture\n    :type conv_architecture: list of int\n    :param kernel_sizes: filter sizes\n    :type kernel_sizes: list of int\n    :param pool_kernel: pooling filter sizes\n    :type pool_kernel: list of int\n    :param batch_size: batch size for training\n    :type batch_size: int\n    :param epochs: number of epochs\n    :type epochs: int\n    :param num_steps: number of iterations for each epoch\n    :type num_steps: int\n    :param save_step: when step % save_step == 0, the model\n                      parameters are saved.\n    :type save_step: int\n    :param optimizer: a optimizer from tensorflow.\n    :type optimizer: tf.train.GradientDescentOptimizer,\n                     tf.train.AdadeltaOptimizer,\n                     tf.train.AdagradOptimizer,\n                     tf.train.AdagradDAOptimizer,\n                     tf.train.AdamOptimizer,\n                     tf.train.FtrlOptimizer,\n                     tf.train.ProximalGradientDescentOptimizer,\n                     tf.train.ProximalAdagradOptimizer,\n                     tf.train.RMSPropOptimizer\n    :param experiments: number of experiments to be made\n    :type experiments: int\n    :param conv: param to control if the model will be a CNN\n                 or DFN\n    :type conv: bool\n    :param divisor: param to resize the learning rate\n    :type divisor: float\n    """"""\n    LR = np.random.random_sample([experiments]) / divisor\n    LR.sort()\n    numeric_result = []\n    results = []\n    info = []\n    LR = list(LR)\n    if conv:\n        net_name = ""CNN""\n    else:\n        net_name = ""DFN""\n\n    header = ""\\nSearching learning rate for the model {} in the {} data\\n"".format(net_name,  # noqa\n                                                                                  name_tfrecords)  # noqa\n    print(header)\n    for lr in LR:\n        config = Config(height=height,\n                        width=width,\n                        channels=channels,\n                        learning_rate=lr,\n                        architecture=architecture,\n                        activations=activations,\n                        conv_architecture=conv_architecture,\n                        kernel_sizes=kernel_sizes,\n                        pool_kernel=pool_kernel,\n                        batch_size=batch_size,\n                        epochs=epochs,\n                        num_steps=num_steps,\n                        save_step=save_step,\n                        optimizer=optimizer)\n\n        data = DataHolder(config,\n                          records=records)\n        name = ""lr = {0:.6f}"".format(lr)\n        print(name + "":\\n"")\n        graph = tf.Graph()\n        if conv:\n            network = CNN(graph, config)\n        else:\n            network = DFN(graph, config)\n        trainer = Trainer(graph, config, network, data)\n        trainer.fit(verbose=True)\n        valid_acc = trainer.get_valid_accuracy()\n        numeric_result.append(valid_acc)\n        name += \': valid_acc = {0:.6f} | \'.format(valid_acc)\n        test_images, test_labels, _ = reconstruct_from_record(data.get_test_tfrecord())  # noqa\n        test_images = test_images.astype(np.float32) / 255\n        test_pred = trainer.predict(test_images)\n        acc_cat = accuracy_per_category(test_pred, test_labels, categories=3)\n        for i, cat_result in enumerate(acc_cat):\n            name += int2command[i] + "": = {0:.6f}, "".format(cat_result)\n        results.append(name)\n        if os.path.exists(""checkpoints""):\n            shutil.rmtree(""checkpoints"")\n        info.append(str(config))\n\n    best_result = max(list(zip(numeric_result, LR, info)))\n    result_string = """"""In an experiment with {0} learning rate values\n    the best one is {1} with valid accuracy of {2}.\n    \\nThe training uses the following params:\n    \\n{3}\\n"""""".format(experiments,\n                      best_result[1],\n                      best_result[0],\n                      best_result[2])\n    file = open(""learning_rate_results.txt"", ""w"")\n    file.write(header)\n    file.write(""Results with different values for learning rate\\n"")\n    for result in results:\n        result += ""\\n""\n        file.write(result)\n    file.write(""\\n"")\n    file.write(result_string)\n    file.close()\n\n\ndef main():\n    """"""\n    Main script to perform learnig rate search.\n    """"""\n    parser = argparse.ArgumentParser(description=\'Perform learnig rate search\')\n    parser.add_argument(""-n"",\n                        ""--name_tfrecords"",\n                        type=str,\n                        default=""data"",\n                        help=""name for tfrecords (default=data)"")  # noqa\n    parser.add_argument(""-ex"",\n                        ""--experiments"",\n                        type=int,\n                        default=10,\n                        help=""number of experiments"")\n    parser.add_argument(\'-a\',\n                        \'--architecture\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'sizes for hidden layers and output layer, should end with at least ""3"" !, (default=[3])\',  # noqa\n                        default=[3])\n    parser.add_argument(\'-ac\',\n                        \'--activations\',\n                        type=str,\n                        nargs=\'+\',\n                        help=\'activations: relu, sigmoid, tanh (defaul=None)\',\n                        default=None)\n    parser.add_argument(""-he"",\n                        ""--height"",\n                        type=int,\n                        default=90,\n                        help=""image height (default=90)"")\n    parser.add_argument(""-w"",\n                        ""--width"",\n                        type=int,\n                        default=160,\n                        help=""image width (default=160)"")\n    parser.add_argument(""-c"",\n                        ""--channels"",\n                        type=int,\n                        default=3,\n                        help=""number of channels (default=3)"")\n    parser.add_argument(\'-conva\',\n                        \'--conv_architecture\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'filters for conv layers (default=[32, 64])\',  # noqa\n                        default=[32, 64])\n    parser.add_argument(\'-k\',\n                        \'--kernel_sizes\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'kernel sizes for conv layers (default=None - 5 for every layer)\',  # noqa\n                        default=None)\n    parser.add_argument(\'-p\',\n                        \'--pool_kernel\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'kernel sizes for pooling layers (default=None - 2 for every layer)\',  # noqa\n                        default=None)\n    parser.add_argument(""-b"",\n                        ""--batch_size"",\n                        type=int,\n                        default=32,\n                        help=""batch size (default=32)"")\n    parser.add_argument(""-e"",\n                        ""--epochs"",\n                        type=int,\n                        default=5,\n                        help=""epochs for training (default=5)"")\n    parser.add_argument(""-ns"",\n                        ""--num_steps"",\n                        type=int,\n                        default=1000,\n                        help=""number of steps for each epoch (default=1000)"")\n    parser.add_argument(""-ss"",\n                        ""--save_step"",\n                        type=int,\n                        default=100,\n                        help=""number of steps to save variables (default=100)"")\n    opt_list = """"""optimizers: GradientDescent,\n                              Adadelta,\n                              Adagrad,\n                              Adam,\n                              Ftrl,\n                              ProximalGradientDescent,\n                              ProximalAdagrad,\n                              RMSProp""""""\n    parser.add_argument(""-o"",\n                        ""--optimizer"",\n                        type=str,\n                        default=""GradientDescent"",\n                        help=opt_list + ""(default=GradientDescent)"")\n    parser.add_argument(""-conv"",\n                        ""--conv"",\n                        action=""store_true"",\n                        default=False,\n                        help=""Use convolutional network (default=False)"")\n    parser.add_argument(""-di"",\n                        ""--divisor"",\n                        type=float,\n                        default=100.0,\n                        help=""value to divide the learning rate array (default=100.0)"")  # noqa\n    args = parser.parse_args()\n    records = [""_train.tfrecords"", ""_valid.tfrecords"", ""_test.tfrecords""]\n    new_records = []\n    for record in records:\n        record = args.name_tfrecords + record\n        new_records.append(record)\n\n    optimizer_dict = {""GradientDescent"": tf.train.GradientDescentOptimizer,  # noqa\n                      ""Adadelta"": tf.train.AdadeltaOptimizer,\n                      ""Adagrad"": tf.train.AdagradOptimizer,\n                      ""Adam"": tf.train.AdamOptimizer,\n                      ""Ftrl"": tf.train.FtrlOptimizer,\n                      ""ProximalGradientDescent"": tf.train.ProximalGradientDescentOptimizer,  # noqa\n                      ""ProximalAdagrad"": tf.train.ProximalAdagradOptimizer,  # noqa\n                      ""RMSProp"": tf.train.RMSPropOptimizer}  # noqa\n\n    activations_dict = {""relu"": tf.nn.relu,\n                        ""sigmoid"": tf.nn.sigmoid,\n                        ""tanh"": tf.nn.tanh}\n    if args.activations is not None:\n        activations = [activations_dict[act] for act in args.activations]\n    else:\n        activations = args.activations\n    optimizer = optimizer_dict[args.optimizer]\n\n    lr_search(name_tfrecords=args.name_tfrecords,\n              records=new_records,\n              height=args.height,\n              width=args.width,\n              channels=args.channels,\n              experiments=args.experiments,\n              architecture=args.architecture,\n              activations=activations,\n              conv_architecture=args.conv_architecture,\n              kernel_sizes=args.kernel_sizes,\n              pool_kernel=args.pool_kernel,\n              batch_size=args.batch_size,\n              epochs=args.epochs,\n              num_steps=args.num_steps,\n              save_step=args.save_step,\n              optimizer=optimizer,\n              conv=args.conv,\n              divisor=args.divisor)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
self_driving/ml_training/best_optimizer.py,13,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\nimport os\nimport numpy as np\nimport shutil\nimport argparse\nfrom DataHolder import DataHolder\nfrom Config import Config\nfrom Trainer import Trainer\nfrom DFN import DFN\nfrom CNN import CNN\nfrom util import reconstruct_from_record, accuracy_per_category\nfrom util import int2command\n\n\ndef optmizers_search(name_tfrecords,\n                     records,\n                     height,\n                     width,\n                     channels,\n                     architecture,\n                     activations,\n                     conv_architecture,\n                     kernel_sizes,\n                     pool_kernel,\n                     batch_size,\n                     epochs,\n                     num_steps,\n                     save_step,\n                     learning_rate,\n                     conv):\n    """"""\n    Script to run optmizers search,\n    the result is saved on the file optmizers_results.txt\n\n    :param name_tfrecords: name of the used tfrecords\n    :type name_tfrecords: str\n    :param records: list of paths to train, test, and valid tfrecords\n    :type records: list of str\n    :param height: image height\n    :type heights: int\n    :param width: image width\n    :type width: int\n    :param channels: image channels\n    :type channels: int\n    :param architecture: network architecture\n    :type architecture: list of int\n    :param activations: list of different tf functions\n    :type activations: list of tf.nn.sigmoid, tf.nn.relu, tf.nn.tanh\n    :param conv_architecture: convolutional architecture\n    :type conv_architecture: list of int\n    :param kernel_sizes: filter sizes\n    :type kernel_sizes: list of int\n    :param pool_kernel: pooling filter sizes\n    :type pool_kernel: list of int\n    :param batch_size: batch size for training\n    :type batch_size: int\n    :param epochs: number of epochs\n    :type epochs: int\n    :param num_steps: number of iterations for each epoch\n    :type num_steps: int\n    :param save_step: when step % save_step == 0, the model\n                      parameters are saved.\n    :type save_step: int\n    :param learning_rate: learning rate for the optimizer\n    :type learning_rate: float\n    :param conv: param to control if the model will be a CNN\n                 or DFN\n    :type conv: bool\n    """"""\n    OT = [tf.train.GradientDescentOptimizer,\n          tf.train.AdadeltaOptimizer,\n          tf.train.AdagradOptimizer,\n          tf.train.AdamOptimizer,\n          tf.train.FtrlOptimizer,\n          tf.train.ProximalGradientDescentOptimizer,\n          tf.train.ProximalAdagradOptimizer,\n          tf.train.RMSPropOptimizer]\n\n    OT_name = [""GradientDescentOptimizer"",\n               ""AdadeltaOptimizer"",\n               ""AdagradOptimizer"",\n               ""AdamOptimizer"",\n               ""FtrlOptimizer"",\n               ""ProximalGradientDescentOptimizer"",\n               ""ProximalAdagradOptimizer"",\n               ""RMSPropOptimizer""]\n    numeric_result = []\n    results = []\n    info = []\n    if conv:\n        net_name = ""CNN""\n    else:\n        net_name = ""DFN""\n\n    header = ""\\nSearching optimizer for the {} model in the {} data\\n"".format(net_name,  # noqa\n                                                                              name_tfrecords)  # noqa\n    print(header)\n\n    for name, opt in zip(OT_name, OT):\n        config = Config(height=height,\n                        width=width,\n                        channels=channels,\n                        architecture=architecture,\n                        activations=activations,\n                        conv_architecture=conv_architecture,\n                        kernel_sizes=kernel_sizes,\n                        pool_kernel=pool_kernel,\n                        batch_size=batch_size,\n                        epochs=epochs,\n                        num_steps=num_steps,\n                        save_step=save_step,\n                        learning_rate=learning_rate,\n                        optimizer=opt)\n        data = DataHolder(config,\n                          records=records)\n        print(name + "":\\n"")\n        graph = tf.Graph()\n        if conv:\n            network = CNN(graph, config)\n        else:\n            network = DFN(graph, config)\n        trainer = Trainer(graph, config, network, data)\n        trainer.fit(verbose=True)\n        valid_acc = trainer.get_valid_accuracy()\n        numeric_result.append(valid_acc)\n        name += \': valid_acc = {0:.6f} | \'.format(valid_acc)\n        test_images, test_labels, _ = reconstruct_from_record(data.get_test_tfrecord())  # noqa\n        test_images = test_images.astype(np.float32) / 255\n        test_pred = trainer.predict(test_images)\n        acc_cat = accuracy_per_category(test_pred, test_labels, categories=3)\n        for i, cat_result in enumerate(acc_cat):\n            name += int2command[i] + "": = {0:.6f}, "".format(cat_result)\n        results.append(name)\n        if os.path.exists(""checkpoints""):\n            shutil.rmtree(""checkpoints"")\n        info.append(str(config))\n\n    best_result = max(list(zip(numeric_result, OT_name, info)))\n    result_string = """"""In an experiment with different optmizers\n    the best one is {0} with valid accuracy of {1}.\n    \\nThe training uses the following params:\n    \\n{2}\\n"""""".format(best_result[1],\n                      best_result[0],\n                      best_result[2])\n    file = open(""optmizers_results.txt"", ""w"")\n    file.write(header)\n    file.write(""Results for different optmizers\\n"")\n    for result in results:\n        result += ""\\n""\n        file.write(result)\n    file.write(""\\n"")\n    file.write(result_string)\n    file.close()\n\n\ndef main():\n    """"""\n    Main script to perform optmizer search.\n    """"""\n    parser = argparse.ArgumentParser(description=\'Perform optmizer search\')\n    parser.add_argument(""-n"",\n                        ""--name_tfrecords"",\n                        type=str,\n                        default=""data"",\n                        help=""name for tfrecords (default=data)"")  # noqa\n    parser.add_argument(\'-a\',\n                        \'--architecture\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'sizes for hidden layers and output layer, should end with at least ""3"" !, (default=[3])\',  # noqa\n                        default=[3])\n    parser.add_argument(\'-ac\',\n                        \'--activations\',\n                        type=str,\n                        nargs=\'+\',\n                        help=\'activations: relu, sigmoid, tanh (defaul=None)\',\n                        default=None)\n    parser.add_argument(""-he"",\n                        ""--height"",\n                        type=int,\n                        default=90,\n                        help=""image height (default=90)"")\n    parser.add_argument(""-w"",\n                        ""--width"",\n                        type=int,\n                        default=160,\n                        help=""image width (default=160)"")\n    parser.add_argument(""-c"",\n                        ""--channels"",\n                        type=int,\n                        default=3,\n                        help=""number of channels (default=3)"")\n    parser.add_argument(""-lr"",\n                        ""--learning_rate"",\n                        type=float,\n                        default=0.02,\n                        help=""learning rate (default=0.02)"")\n    parser.add_argument(\'-conva\',\n                        \'--conv_architecture\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'filters for conv layers (default=[32, 64])\',  # noqa\n                        default=[32, 64])\n    parser.add_argument(\'-k\',\n                        \'--kernel_sizes\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'kernel sizes for conv layers (default=None - 5 for every layer)\',  # noqa\n                        default=None)\n    parser.add_argument(\'-p\',\n                        \'--pool_kernel\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'kernel sizes for pooling layers (default=None - 2 for every layer)\',  # noqa\n                        default=None)\n    parser.add_argument(""-b"",\n                        ""--batch_size"",\n                        type=int,\n                        default=32,\n                        help=""batch size (default=32)"")\n    parser.add_argument(""-e"",\n                        ""--epochs"",\n                        type=int,\n                        default=5,\n                        help=""epochs for training (default=5)"")\n    parser.add_argument(""-ns"",\n                        ""--num_steps"",\n                        type=int,\n                        default=1000,\n                        help=""number of steps for each epoch (default=1000)"")\n    parser.add_argument(""-ss"",\n                        ""--save_step"",\n                        type=int,\n                        default=100,\n                        help=""number of steps to save variables (default=100)"")\n    parser.add_argument(""-conv"",\n                        ""--conv"",\n                        action=""store_true"",\n                        default=False,\n                        help=""Use convolutional network (default=False)"")\n    args = parser.parse_args()\n    records = [""_train.tfrecords"", ""_valid.tfrecords"", ""_test.tfrecords""]\n    new_records = []\n    for record in records:\n        record = args.name_tfrecords + record\n        new_records.append(record)\n\n    activations_dict = {""relu"": tf.nn.relu,\n                        ""sigmoid"": tf.nn.sigmoid,\n                        ""tanh"": tf.nn.tanh}\n    if args.activations is not None:\n        activations = [activations_dict[act] for act in args.activations]\n    else:\n        activations = args.activations\n    optmizers_search(name_tfrecords=args.name_tfrecords,\n                     records=new_records,\n                     height=args.height,\n                     width=args.width,\n                     channels=args.channels,\n                     architecture=args.architecture,\n                     activations=activations,\n                     conv_architecture=args.conv_architecture,\n                     kernel_sizes=args.kernel_sizes,\n                     pool_kernel=args.pool_kernel,\n                     batch_size=args.batch_size,\n                     epochs=args.epochs,\n                     learning_rate=args.learning_rate,\n                     num_steps=args.num_steps,\n                     save_step=args.save_step,\n                     conv=args.conv)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
self_driving/ml_training/generate_tfrecords.py,0,"b'import argparse\nfrom Config import Config\nfrom DataHolder import DataHolder\n\n\ndef records_generator(height,\n                      width,\n                      channels,\n                      data_path,\n                      label_path,\n                      name,\n                      flip=False,\n                      augmentation=False,\n                      gray=False,\n                      green=False,\n                      binary=False):\n    """"""\n    Generates tfrecords.\n\n    :param height: image height\n    :type heights: int\n    :param width: image width\n    :type width: int\n    :param channels: image channels\n    :type channels: int\n    :param data_path: path to load data np.array\n    :type data_path: str\n    :param record_path: path to load labels np.array\n    :type label_path: str\n    :param name: path to save tfrecord\n    :type name: str\n    :param flip: param to control if the data\n                         will be flipped\n    :type flip: boolean\n    :param augmentation: param to control if the data\n                         will augmented\n    :type augmentation: boolean\n    :param gray: param to control if the data\n                 will be grayscale images\n    :type gray: boolean\n    :param green: param to control if the data will use only\n                  the green channel\n    :type green: boolean\n    :param binary: param to control if the data will be binarized\n    :type binary: boolean\n    """"""\n\n    config = Config(height=height,\n                    width=width,\n                    channels=channels)\n    data = DataHolder(config,\n                      data_path=data_path,\n                      label_path=label_path,\n                      record_path=name,\n                      flip=flip,\n                      augmentation=augmentation,\n                      gray=gray,\n                      green=green,\n                      binary=binary,\n                      records=None)\n    data.create_records()\n\n\ndef main():\n    """"""\n    Main script to generate tfrecords from a tuple of np.arrays.\n    """"""\n    description = ""Generates tfrecords from a tuple of np.arrays""\n    parser = argparse.ArgumentParser(description=description)\n    parser.add_argument(\'data_path\',\n                        type=str, help=\'path to data array\')\n    parser.add_argument(\'label_path\',\n                        type=str, help=\'path to labels array\')\n    parser.add_argument(""-he"",\n                        ""--height"",\n                        type=int,\n                        default=90,\n                        help=""height number (default=90)"")\n    parser.add_argument(""-w"",\n                        ""--width"",\n                        type=int,\n                        default=160,\n                        help=""width number (default=160)"")\n    parser.add_argument(""-c"",\n                        ""--channels"",\n                        type=int,\n                        default=3,\n                        help=""number of channels (default=3)"")\n\n    parser.add_argument(""-n"",\n                        ""--name"",\n                        type=str,\n                        default=""data"",\n                        help=""name for tfrecords e.g. pure, flip, aug, bin, gray, green (default=data)"")  # noqa\n\n    parser.add_argument(""-f"",\n                        ""--flip"",\n                        action=""store_true"",\n                        default=False,\n                        help=""flag to flip x-axis (default=False)"")\n\n    parser.add_argument(""-p"",\n                        ""--pure"",\n                        action=""store_true"",\n                        default=False,\n                        help=""flag to pure (default=False)"")\n\n    parser.add_argument(""-a"",\n                        ""--augmentation"",\n                        action=""store_true"",\n                        default=False,\n                        help=""flag to augment dataset (default=False)"")\n\n    parser.add_argument(""-gy"",\n                        ""--gray"",\n                        action=""store_true"",\n                        default=False,\n                        help=""flag to transform dataset in grayscale (default=False)"")  # noqa\n\n    parser.add_argument(""-gr"",\n                        ""--green"",\n                        action=""store_true"",\n                        default=False,\n                        help=""flag to keep only the green channel (default=False)"")  # noqa\n\n    parser.add_argument(""-b"",\n                        ""--binary"",\n                        action=""store_true"",\n                        default=False,\n                        help=""flag to binarize dataset (default=False)"")\n\n    args = parser.parse_args()\n    records_generator(args.height,\n                      args.width,\n                      args.channels,\n                      args.data_path,\n                      args.label_path,\n                      args.name,\n                      args.flip,\n                      args.augmentation,\n                      args.gray,\n                      args.green,\n                      args.binary)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
self_driving/ml_training/train.py,22,"b'import tensorflow as tf\nimport os\nimport numpy as np\nimport shutil\nimport argparse\nimport sys\nimport inspect\n\nfrom DataHolder import DataHolder\nfrom Config import Config\nfrom Trainer import Trainer\nfrom DFN import DFN\nfrom CNN import CNN\nfrom util import reconstruct_from_record\nfrom util import int2command\n\n\nalmost_current = os.path.abspath(inspect.getfile(inspect.currentframe()))\ncurrentdir = os.path.dirname(almost_current)\nparentdir = os.path.dirname(currentdir)\nsys.path.insert(0, parentdir)\n\nfrom plot.util import plotconfusion  # noqa\n\n\ndef train(name_tfrecords,\n          records,\n          height,\n          width,\n          channels,\n          architecture,\n          activations,\n          conv_architecture,\n          kernel_sizes,\n          pool_kernel,\n          batch_size,\n          epochs,\n          num_steps,\n          save_step,\n          learning_rate,\n          optimizer,\n          verbose,\n          name,\n          move,\n          conv=False):\n    """"""\n    Trains a model\n\n    :param name_tfrecords: name of the used tfrecords\n    :type name_tfrecords: str\n    :param records: list of paths to train, test, and valid tfrecords\n    :type records: list of str\n    :param height: image height\n    :type heights: int\n    :param width: image width\n    :type width: int\n    :param channels: image channels\n    :type channels: int\n    :param architecture: network architecture\n    :type architecture: list of int\n    :param activations: list of different tf functions\n    :type activations: list of tf.nn.sigmoid, tf.nn.relu, tf.nn.tanh\n    :param conv_architecture: convolutional architecture\n    :type conv_architecture: list of int\n    :param kernel_sizes: filter sizes\n    :type kernel_sizes: list of int\n    :param pool_kernel: pooling filter sizes\n    :type pool_kernel: list of int\n    :param batch_size: batch size for training\n    :type batch_size: int\n    :param epochs: number of epochs\n    :type epochs: int\n    :param num_steps: number of iterations for each epoch\n    :type num_steps: int\n    :param save_step: when step % save_step == 0, the model\n                      parameters are saved.\n    :type save_step: int\n    :param learning_rate: learning rate for the optimizer\n    :type learning_rate: float\n    :param optimizer: a optimizer from tensorflow.\n    :type optimizer: tf.train.GradientDescentOptimizer,\n                     tf.train.AdadeltaOptimizer,\n                     tf.train.AdagradOptimizer,\n                     tf.train.AdagradDAOptimizer,\n                     tf.train.AdamOptimizer,\n                     tf.train.FtrlOptimizer,\n                     tf.train.ProximalGradientDescentOptimizer,\n                     tf.train.ProximalAdagradOptimizer,\n                     tf.train.RMSPropOptimizer\n    :param verbose: param to control if the trainig will be printed\n                    and if the confusion matrix will be calculated.\n    :type verbose: bool\n    :param name: name to save the confusion matrix plot.\n    :type name: str\n    :param move: param to control if the checkpoints path\n                 will be moved to the parent folder.\n    :type move: bool\n    :param conv: param to control if the model will be a CNN\n                 or DFN\n    :type conv: bool\n    """"""\n\n    if os.path.exists(""checkpoints""):\n        shutil.rmtree(""checkpoints"")\n\n    config = Config(height=height,\n                    width=width,\n                    channels=channels,\n                    architecture=architecture,\n                    activations=activations,\n                    conv_architecture=conv_architecture,\n                    kernel_sizes=kernel_sizes,\n                    pool_kernel=pool_kernel,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    num_steps=num_steps,\n                    save_step=save_step,\n                    learning_rate=learning_rate,\n                    optimizer=optimizer)\n\n    data = DataHolder(config,\n                      records=records)\n\n    graph = tf.Graph()\n    if conv:\n        network_name = ""CNN""\n        network = CNN(graph, config)\n    else:\n        network_name = ""DFN""\n        network = DFN(graph, config)\n    trainer = Trainer(graph, config, network, data)\n    print(""\\nTraining in the {} data using one {}\\n"".format(name_tfrecords,\n                                                            network_name))\n    print(""params:\\n{}\\n"".format(str(config)))\n    trainer.fit(verbose=verbose)\n    if verbose:\n        valid_images, valid_labels, _ = reconstruct_from_record(data.get_valid_tfrecord())  # noqa\n        valid_images = valid_images.astype(np.float32) / 255\n        valid_pred = trainer.predict(valid_images)\n        valid_labels = valid_labels.reshape((valid_labels.shape[0],))\n        plotconfusion(valid_labels,\n                      valid_pred,\n                      name + "".png"",\n                      int2command,\n                      classes=[""left"", ""right"", ""up""])\n    if move:\n        dst = os.path.join(parentdir, ""checkpoints"")\n        shutil.move(""checkpoints"", dst)\n\n\ndef main():\n    """"""\n    Main script to train one model using one kind of data.\n    """"""\n    parser = argparse.ArgumentParser(description=\'Train a model\')\n    parser.add_argument(""-n"",\n                        ""--name_tfrecords"",\n                        type=str,\n                        default=""data"",\n                        help=""name for tfrecords (default=data)"")  # noqa\n    parser.add_argument(""-c"",\n                        ""--channels"",\n                        type=int,\n                        default=3,\n                        help=""number of channels (default=3)"")\n    parser.add_argument(""-he"",\n                        ""--height"",\n                        type=int,\n                        default=90,\n                        help=""image height (default=90)"")\n    parser.add_argument(""-w"",\n                        ""--width"",\n                        type=int,\n                        default=160,\n                        help=""image width (default=160)"")\n    parser.add_argument(\'-a\',\n                        \'--architecture\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'sizes for hidden layers and output layer, should end with at least ""3"" !, (default=[3])\',  # noqa\n                        default=[3])\n    parser.add_argument(\'-ac\',\n                        \'--activations\',\n                        type=str,\n                        nargs=\'+\',\n                        help=\'activations: relu, sigmoid, tanh (defaul=None)\',\n                        default=None)\n    parser.add_argument(\'-conva\',\n                        \'--conv_architecture\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'filters for conv layers (default=[32, 64])\',  # noqa\n                        default=[32, 64])\n    parser.add_argument(\'-k\',\n                        \'--kernel_sizes\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'kernel sizes for conv layers (default=None - 5 for every layer)\',  # noqa\n                        default=None)\n    parser.add_argument(\'-p\',\n                        \'--pool_kernel\',\n                        type=int,\n                        nargs=\'+\',\n                        help=\'kernel sizes for pooling layers (default=None - 2 for every layer)\',  # noqa\n                        default=None)\n    parser.add_argument(""-b"",\n                        ""--batch_size"",\n                        type=int,\n                        default=32,\n                        help=""batch size (default=32)"")\n    parser.add_argument(""-e"",\n                        ""--epochs"",\n                        type=int,\n                        default=5,\n                        help=""epochs for training (default=5)"")\n    parser.add_argument(""-ns"",\n                        ""--num_steps"",\n                        type=int,\n                        default=1000,\n                        help=""number of steps for each epoch (default=1000)"")\n    parser.add_argument(""-ss"",\n                        ""--save_step"",\n                        type=int,\n                        default=100,\n                        help=""number of steps to save variables (default=100)"")\n    parser.add_argument(""-lr"",\n                        ""--learning_rate"",\n                        type=float,\n                        default=0.02,\n                        help=""learning rate (default=0.02)"")\n    opt_list = """"""optimizers: GradientDescent,\n                              Adadelta,\n                              Adagrad,\n                              Adam,\n                              Ftrl,\n                              ProximalGradientDescent,\n                              ProximalAdagrad,\n                              RMSProp""""""\n    parser.add_argument(""-o"",\n                        ""--optimizer"",\n                        type=str,\n                        default=""GradientDescent"",\n                        help=opt_list + ""(default=GradientDescent)"")\n    parser.add_argument(""-v"",\n                        ""--verbose"",\n                        action=""store_true"",\n                        default=False,\n                        help=""print training results and calculate confusion matrix (default=False)"")  # noqa\n    parser.add_argument(""-cm"",\n                        ""--cm_name"",\n                        type=str,\n                        default=""Confusion_Matrix"",\n                        help=""name to save confusion matrix plot (default=Confusion_Matrix)"")  # noqa\n    parser.add_argument(""-mv"",\n                        ""--move"",\n                        action=""store_true"",\n                        default=False,\n                        help=""move checkpoits to parent folder (default=False)"")  # noqa\n    parser.add_argument(""-conv"",\n                        ""--conv"",\n                        action=""store_true"",\n                        default=False,\n                        help=""Use convolutional network (default=False)"")  # noqa\n    args = parser.parse_args()\n    records = [""_train.tfrecords"", ""_valid.tfrecords"", ""_test.tfrecords""]\n    new_records = []\n    for record in records:\n        record = args.name_tfrecords + record\n        new_records.append(record)\n\n    optimizer_dict = {""GradientDescent"": tf.train.GradientDescentOptimizer,  # noqa\n                      ""Adadelta"": tf.train.AdadeltaOptimizer,\n                      ""Adagrad"": tf.train.AdagradOptimizer,\n                      ""Adam"": tf.train.AdamOptimizer,\n                      ""Ftrl"": tf.train.FtrlOptimizer,\n                      ""ProximalGradientDescent"": tf.train.ProximalGradientDescentOptimizer,  # noqa\n                      ""ProximalAdagrad"": tf.train.ProximalAdagradOptimizer,  # noqa\n                      ""RMSProp"": tf.train.RMSPropOptimizer}  # noqa\n\n    activations_dict = {""relu"": tf.nn.relu,\n                        ""sigmoid"": tf.nn.sigmoid,\n                        ""tanh"": tf.nn.tanh}\n    if args.activations is not None:\n        activations = [activations_dict[act] for act in args.activations]\n    else:\n        activations = args.activations\n    optimizer = optimizer_dict[args.optimizer]\n\n    train(name_tfrecords=args.name_tfrecords,\n          records=new_records,\n          height=args.height,\n          width=args.width,\n          channels=args.channels,\n          architecture=args.architecture,\n          activations=activations,\n          conv_architecture=args.conv_architecture,\n          kernel_sizes=args.kernel_sizes,\n          pool_kernel=args.pool_kernel,\n          batch_size=args.batch_size,\n          epochs=args.epochs,\n          num_steps=args.num_steps,\n          save_step=args.save_step,\n          learning_rate=args.learning_rate,\n          optimizer=optimizer,\n          verbose=args.verbose,\n          name=args.cm_name,\n          move=args.move,\n          conv=args.conv)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
self_driving/ml_training/util.py,25,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport unittest\nimport tensorflow as tf\nimport numpy as np\n\ncommand2int = {""up"": 0, ""left"": 1, ""right"": 2}\nint2command = {i[1]: i[0] for i in command2int.items()}\n\n\ndef run_test(testClass):\n    """"""\n    Function to run all the tests from a class of tests.\n\n    :param testClass: class for testing\n    :type testClass: unittest.TesCase\n    """"""\n    suite = unittest.TestLoader().loadTestsFromTestCase(testClass)\n    unittest.TextTestRunner(verbosity=2).run(suite)\n\n\ndef reconstruct_from_record(record_path, bound=1000):\n    """"""\n    Function to transform a tf records into a tuple of\n    np arrays. The size is controled by the param ""bound"".\n\n    :param record_path: path to tf_record\n    :type record_path: str\n    :param bound: number of examples to be read\n    :type bound: int\n    :return: images, labels, shape\n    :rtype: np.array, np.array, tuple\n    """"""\n    reconstructed_images = []\n    reconstructed_labels = []\n    record_iterator = tf.python_io.tf_record_iterator(path=record_path)\n\n    for i, string_record in enumerate(record_iterator):\n        if i <= bound:\n            example = tf.train.Example()\n            example.ParseFromString(string_record)\n            height = int(example.features.feature[\'height\'].int64_list.value[0])  # noqa\n            width = int(example.features.feature[\'width\'].int64_list.value[0])  # noqa\n            channels = int(example.features.feature[\'channels\'].int64_list.value[0])  # noqa\n            img_string = (example.features.feature[\'image_raw\']\n                                          .bytes_list\n                                          .value[0])\n            annotation_string = (example.features.feature[\'labels_raw\']\n                                        .bytes_list\n                                        .value[0])\n\n            reconstructed_img = np.fromstring(img_string, dtype=np.uint8)\n            reconstructed_annotation = np.fromstring(annotation_string,\n                                                     dtype=np.uint8)\n            reconstructed_images.append(reconstructed_img)\n            reconstructed_labels.append(reconstructed_annotation)\n        else:\n            break\n    shape = (height, width, channels)\n    reconstructed_images = np.array(reconstructed_images)\n    reconstructed_labels = np.array(reconstructed_labels)\n    return reconstructed_images, reconstructed_labels, shape\n\n\ndef accuracy_per_category(pred, label, categories):\n    """"""\n    Function to give the model\'s accuracy for each category.\n\n    :param pred: model\'s prediction\n    :type pred: np.array\n    :param label: true labels\n    :type label: np.array\n    :param categories: number of categories\n    :type categories: int\n    :return: accuracy\'s list\n    :rtype: list of float\n    """"""\n    pred, label = list(pred), list(label)\n    results = []\n    for cat in range(categories):\n        vfunc = np.vectorize(lambda x: 1 if x == cat else 0)\n        mapped_pred = vfunc(pred)\n        mapped_labels = vfunc(label)\n        right = float(np.dot(mapped_pred, mapped_labels))\n        total = np.sum(mapped_labels)\n        if total == 0:\n            results.append(0.0)\n        else:\n            results.append((right / total))\n    return results\n\n\ndef get_random_architecture_and_activations(network_sizes,\n                                            categories=3,\n                                            upper_bound=6000):\n    """"""\n    Creates a random architecture list and activations list\n    using a list of sizes for different networks.\n\n    :param network_sizes: list of network\'s size\n    :type network_sizes: list of int\n    :param categories: number of categories\n    :type categories: int\n    :param upper_bound: max number of nodes per layer\n    :type upper_bound: int\n    :return: list of hidden layer sizes, list of activation functions\n    :rtype: list of int, list of function tensorflow\n    """"""\n    activations_dict = {0: tf.nn.relu,\n                        1: tf.nn.sigmoid,\n                        2: tf.nn.tanh}\n    hidden_layers = []\n    activations = []\n    lower_bound = categories\n\n    for size in network_sizes:\n        hidden_sizes = []\n        last = upper_bound\n        for _ in range(size):\n            if lower_bound < last / 2:\n                new_size = np.random.randint(lower_bound, last / 2)\n            else:\n                new_size = np.random.randint(lower_bound, lower_bound + 1)\n            hidden_sizes.append(new_size)\n            last = new_size\n        hidden_layers.append(hidden_sizes)\n\n    for hidden in hidden_layers:\n        activ = np.random.randint(0, 3, len(hidden))\n        activ = list(map(lambda x: activations_dict[x], activ))\n        activations.append(activ)\n\n    for hidden in hidden_layers:\n        hidden.append(categories)\n\n    return hidden_layers, activations\n\n\ndef parser_with_normalization(tfrecord):\n    """"""\n    Parser function, transforming string into\n    a tuple of tensors.\n\n    :param tfrecord: a single binary serialized\n    :type tfrecord: tf.Tensor(shape=(), dype=tf.string)\n    :return: image, label\n    :rtype: tf.Tensor(shape=(1, height*width*channels),\n                      dtype=tf.float32),\n            tf.Tensor(shape=(1,), dty\xe1\xb9\x95e=tf.int32)\n    """"""\n    features = {\'height\': tf.FixedLenFeature([], tf.int64),\n                \'width\': tf.FixedLenFeature([], tf.int64),\n                \'channels\': tf.FixedLenFeature([], tf.int64),\n                \'image_raw\': tf.FixedLenFeature([], tf.string),\n                \'labels_raw\': tf.FixedLenFeature([], tf.string)}\n\n    tfrecord_parsed = tf.parse_single_sequence_example(\n        tfrecord, features)\n\n    image = tf.decode_raw(tfrecord_parsed[0][\'image_raw\'], tf.uint8)\n    image = tf.cast(image, tf.float32) / 255\n\n    label = tf.decode_raw(tfrecord_parsed[0][\'labels_raw\'], tf.uint8)\n    label = tf.cast(label, tf.int32)\n\n    return image, label\n\n\ndef get_iterator(filename, batch_size, parser):\n    """"""\n    Function to get an interator.\n\n    :param filename: path to tfrecord dataset\n    :type filename: str\n    :param batch_size: size of the batch\n    :type batch_size: int\n    :param parser: function to parse a string\n                   into a tensor\n    :type parser: tf.Tensor(shape=(), dype=tf.string)\n                  ->\n                  tf.Tensor(shape=(1, height*width*channels),\n                      dtype=tf.float32),\n                  tf.Tensor(shape=(1,), dty\xe1\xb9\x95e=tf.int32)\n    :return: data iterator\n    :rtype: tf.contrib.data.Iterator\n    """"""\n    dataset = tf.contrib.data.TFRecordDataset(filename)\n    dataset = dataset.map(parser)\n    dataset = dataset.repeat()\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.shuffle(batch_size * 2)\n    iterator = dataset.make_initializable_iterator()\n    return iterator\n'"
self_driving/nxt_car/DiffCar.py,0,"b'import nxt\nimport nxt_bluetooth\n\n\nclass DiffCar(object):\n    """"""\n    Class to communicate with the nxt differential car.\n\n    :param turn_ratio: motor\'s ratio\n    :type turn_ratio: int\n    :param power_up: power to be used in the left and right motors\n                     to move up\n    :type power_up: int\n    :param power_down: power to be used in the left and right motors\n                       to move down\n    :type power_down: int\n    :param power_left: power to be used in the left motor\n    :type power_left: int\n    :param tacho_left: tacho count of the left motor\n    :type tacho_left: int\n    :param power_right: power to be used in the right motor\n    :type power_right: int\n    :param tacho_right: tacho count of the right motor\n    :type tacho_right: int\n    :param bluetooth: param to control if the bluetooth will be used.\n    :type bluetooth: bool\n    """"""\n    def __init__(self,\n                 turn_ratio=0,\n                 power_up=10,\n                 power_down=-10,\n                 power_left=20,\n                 tacho_left=30,\n                 power_right=20,\n                 tacho_right=30,\n                 bluetooth=False):\n        if bluetooth:\n            self.sock, self.brick = nxt_bluetooth.connectCar()  # noqa PKSM \'00:16:53:17:B4:04\'\n        else:\n            self.brick = nxt.locator.find_one_brick()\n        self.leftMotor = nxt.Motor(self.brick, nxt.PORT_B)\n        self.rightMotor = nxt.Motor(self.brick, nxt.PORT_A)\n        self.both = nxt.SynchronizedMotors(self.leftMotor,\n                                           self.rightMotor,\n                                           turn_ratio)\n        self.power_up = power_up\n        self.power_down = power_down\n        self.power_left = power_left\n        self.tacho_left = tacho_left\n        self.power_right = power_right\n        self.tacho_right = tacho_right\n        self.btCon = bluetooth\n\n    def move_up(self):\n        """"""\n        Execute one action of moving up\n        """"""\n        self.both.run(self.power_up)\n\n    def move_left(self):\n        """"""\n        Execute one action of moving left\n        """"""\n        self.rightMotor.weak_turn(self.power_left, self.tacho_left)\n        self.leftMotor.weak_turn(- self.power_left, self.tacho_left)\n\n    def move_right(self):\n        """"""\n        Execute one action of moving rigth\n        """"""\n        self.rightMotor.weak_turn(- self.power_right, self.tacho_right)\n        self.leftMotor.weak_turn(self.power_right, self.tacho_right)\n\n    def move_down(self):\n        """"""\n        Execute one action of moving down\n        """"""\n        self.both.run(self.power_down)\n\n    def idle(self):\n        """"""\n        Rest motors\n        """"""\n        self.leftMotor.idle()\n        self.rightMotor.idle()\n\n    def disconnect(self, socket):\n        """"""\n        Disconnect from bluetooth\n        """"""\n        nxt_bluetooth.disconnectCar(socket)\n'"
self_driving/nxt_car/__init__.py,0,b''
self_driving/nxt_car/keyboard-nxt.py,0,"b""from nxt.bluesock import BlueSock\nimport nxt\nimport keyboard as key\n\nID = '00:16:53:17:EF:0A'  # MAC address NXT11\n\nsock = BlueSock(ID)\n\n\ndef moveCar(bk):\n    '''\n    Remote control function to NXT robot with keyboard inputs.\n\n    :param bk: brick\n    :type bk: brick object\n    '''\n    leftMotor = nxt.Motor(bk, nxt.PORT_B)\n    rightMotor = nxt.Motor(bk, nxt.PORT_A)\n    # both = nxt.SynchronizedMotors(leftMotor, rightMotor, 0)\n    # rightboth = nxt.SynchronizedMotors(leftMotor, rightMotor, 100)\n    # leftboth = nxt.SynchronizedMotors(rightMotor, leftMotor, 100)\n    while True:\n\n        try:\n            if key.is_pressed('q'):\n                print('Exiting...')\n                break\n            elif key.is_pressed('up'):\n                rightMotor.weak_turn(20, 100)\n                leftMotor.weak_turn(20, 100)\n            elif key.is_pressed('down'):\n                rightMotor.weak_turn(-20, 30)\n                leftMotor.weak_turn(-20, 30)\n            elif key.is_pressed('left'):\n                rightMotor.weak_turn(20, 30)\n                leftMotor.weak_turn(-20, 30)\n            elif key.is_pressed('right'):\n                rightMotor.weak_turn(-20, 30)\n                leftMotor.weak_turn(20, 30)\n\n            elif key.is_pressed('space'):\n                leftMotor.idle()\n                rightMotor.idle()\n                leftMotor.brake()\n                rightMotor.brake()\n            else:\n                pass\n        except:\n            break\n\n\nif sock:\n    # Connect to brick\n    print('READY')\n    brick = sock.connect()\n\n    moveCar(brick)\n    # Close socket\n    sock.close()\n\n# Failure\nelse:\n    print 'No NXT bricks found'\n"""
self_driving/nxt_car/nxt_bluetooth.py,0,"b'from nxt import bluesock\n\nNXT_ID = \'00:16:53:17:EF:0A\'  # MAC address NXT11\n\n\ndef connectCar(blue_id=NXT_ID):\n    \'\'\'\n    Connect to nxt brick using the id \'blue_id\'. The default\n    id is the global variable NXT_ID. It returs one\n    object for bluethoot connection \'sock\' and one\n    object for NXT control \'brick\'.\n\n    :param blue_id: Bluethoot MAC address\n    :type blue_id: str\n    :rtype: (nxt.bluesock.BlueSock, nxt.brick)\n    \'\'\'\n    try:\n        sock = bluesock.BlueSock(blue_id)\n        brick = sock.connect()\n        return sock, brick\n    except:\n        print(""NO connection with {}"".format(NXT_ID))\n\n\ndef disconnectCar(open_sock):\n    \'\'\'\n    Disconnect from NXT-car\n\n    :param open_sock: opened Bluetooth socket communication\n    :type open_sock: nxt.bluesock.BlueSock\n    \'\'\'\n    open_sock.close()\n'"
self_driving/plot/__init__.py,0,b''
self_driving/plot/dataset_histogram.py,0,"b'\'\'\'\nDataset histogram\n\'\'\'\nimport argparse\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef plot_histogram(labels, path):\n    """"""\n    Plot dataset histogram\n\n    :param label_path: array of labels\n    :type label_path: np.array\n    :param path: name to save histogram\n    :type path: np.str\n    """"""\n\n    data_hist = plt.hist(labels, bins=np.arange(4) - 0.5, edgecolor=\'black\')\n    axes = plt.gca()  # Get Current Axes\n    axes.set_ylim([0, len(labels)])\n\n    plt.title(""Histogram of {} images"".format(len(labels)))\n    plt.xticks(np.arange(4), [\'up\', \'left\', \'right\'])\n    plt.xlabel(""Label"")\n    plt.ylabel(""Frequency"")\n\n    for i in range(3):\n        plt.text(data_hist[1][i] + 0.25,\n                 data_hist[0][i] + (data_hist[0][i] * 0.01),\n                 str(int(data_hist[0][i])))\n\n    plt.savefig(path)\n\n\ndef main():\n    """"""\n    Plot label\'s histogram\n    """"""\n    description = ""plot label\'s histogram""\n    parser = argparse.ArgumentParser(description=description)\n    parser.add_argument(\'labels_path\',\n                        type=str, help=\'path to labels\')\n    parser.add_argument(""-n"",\n                        ""--name"",\n                        type=str,\n                        default=""histogram"",\n                        help=""name to save histogram plot (default=histogram)"")  # noqa\n    args = parser.parse_args()\n    labels = np.load(args.labels_path)\n    plot_histogram(labels, args.name)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
self_driving/plot/util.py,0,"b'import matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt  # noqa\nimport numpy as np  # noqa\nfrom pandas_ml import ConfusionMatrix  # noqa\nimport itertools  # noqa\n\n\ndef plot_confusion_matrix(cm,\n                          classes,\n                          title,\n                          normalize=False,\n                          cmap=plt.cm.Oranges,\n                          path=""confusion_matrix.png""):\n    """"""\n    This function plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \'cmap\' controls the color plot. colors:\n\n    https://matplotlib.org/1.3.1/examples/color/colormaps_reference.html\n\n    :param cm: confusion matrix\n    :type cm: np array\n    :param classes: number of classes\n    :type classes: int\n    :param title: image title\n    :type title: str\n    :param cmap: plt color map\n    :type cmap: plt.cm\n    :param path: path to save image\n    :type path: str\n    """"""\n    if normalize:\n        cm = cm.astype(\'float\') / cm.sum(axis=1)[:, np.newaxis]\n    plt.figure(figsize=(9, 9))\n    plt.imshow(cm, interpolation=\'nearest\', cmap=cmap)\n    plt.title(title, fontsize=24, fontweight=\'bold\')\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = \'.2f\' if normalize else \'d\'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=""center"",\n                 color=""white"" if cm[i, j] > thresh else ""black"")\n\n    plt.tight_layout()\n    plt.ylabel(\'True label\', fontweight=\'bold\')\n    plt.xlabel(\'Predicted label\', fontweight=\'bold\')\n    plt.savefig(path)\n\n\ndef plotconfusion(truth, predictions, path, label_dict, classes):\n    """"""\n    This function plots the confusion matrix and\n    also prints useful statistics.\n\n    :param truth: true labels\n    :type truth: np array\n    :param predictions: model predictions\n    :type predictions: np array\n    :param path: path to save image\n    :type path: str\n    :param label_dict: dict to transform int to str\n    :type label_dict: dict\n    :param classes: number of classes\n    :type classes: int\n    """"""\n    acc = np.array(truth) == np.array(predictions)\n    size = float(acc.shape[0])\n    acc = np.sum(acc.astype(""int32"")) / size\n    truth = [label_dict[i] for i in truth]\n    predictions = [label_dict[i] for i in predictions]\n    cm = ConfusionMatrix(truth, predictions)\n    cm_array = cm.to_array()\n    cm_diag = np.diag(cm_array)\n    sizes_per_cat = []\n    for n in range(cm_array.shape[0]):\n        sizes_per_cat.append(np.sum(cm_array[n]))\n    sizes_per_cat = np.array(sizes_per_cat)\n    sizes_per_cat = sizes_per_cat.astype(np.float32) ** -1\n    recall = np.multiply(cm_diag, sizes_per_cat)\n    print(""\\nRecall:{}"".format(recall))\n    print(""\\nRecall stats: mean = {0:.6f}, std = {1:.6f}\\n"".format(np.mean(recall),  # noqa\n                                                                    np.std(recall)))  # noqa\n    title = ""Confusion matrix of {0} examples\\n accuracy = {1:.6f}"".format(int(size),  # noqa\n                                                                           acc)\n    plot_confusion_matrix(cm_array, classes, title=title, path=path)\n    cm.print_stats()\n'"
self_driving/vision/Camera.py,0,"b'import cv2\r\nfrom image_manipulation import binarize_image, grayscale_image\r\n\r\n\r\nclass Camera(object):\r\n    """"""\r\n    Class to take pictures.\r\n\r\n    :param width_size: camera\'s image width\r\n    :type width_size: int\r\n    :param height_size: camera\'s image height\r\n    :type height_size: int\r\n    :param input_cam_device: param to control camera\'s input\r\n    :type input_cam_device: int\r\n    :param height_param: param to set height on camera\r\n    :type height_param: int\r\n    :param width_param: param to set width on camera\r\n    :type width_param: int\r\n    :param mode: param to control type of image\r\n    :type mode: str\r\n    :param debug: param to enter debug mode\r\n    :type debug: bool\r\n    :param resize: param to control the image resizing\r\n    :type resize: float\r\n    """"""\r\n    def __init__(self,\r\n                 width_size=160,\r\n                 height_size=90,\r\n                 input_cam_device=0,\r\n                 height_param=4,\r\n                 width_param=3,\r\n                 mode=""pure"",\r\n                 debug=False,\r\n                 resize=1.0):\r\n        self.cam = cv2.VideoCapture(input_cam_device)\r\n        self.cam.set(width_param, width_size)\r\n        self.cam.set(height_param, height_size)\r\n        assert mode == ""pure"" or mode == ""green"" or mode == ""bin"" or mode == ""gray""  # noqa\r\n        self.mode = mode\r\n        self.resize = resize\r\n        self.debug = debug\r\n\r\n    def save_image(self, path, img):\r\n        """"""\r\n        Save image in path ""path"".\r\n\r\n        :param path: path to save image\r\n        :type path: str\r\n        :param img: image\r\n        :type img: np.ndarray\r\n        """"""\r\n        cv2.imwrite(path, img)\r\n\r\n    def take_picture(self):\r\n        """"""\r\n        Take picture according to the mode param.\r\n        :rtype: np.ndarray\r\n        """"""\r\n        if self.mode == ""pure"":\r\n            return self.take_picture_rgb()\r\n        elif self.mode == ""green"":\r\n            return self.take_picture_green()\r\n        elif self.mode == ""bin"":\r\n            return self.take_picture_bin()\r\n        elif self.mode == ""gray"":\r\n            return self.take_picture_gray()\r\n\r\n    def take_picture_rgb(self):\r\n        """"""\r\n        Take picture with no transformation.\r\n\r\n        :return: resized image\r\n        :rtype: np.ndarray, np.ndarray\r\n        """"""\r\n        _, img = self.cam.read()\r\n        res = cv2.resize(img, (0, 0), fx=self.resize, fy=self.resize)\r\n        if self.debug:\r\n            return res, img\r\n        return res\r\n\r\n    def take_picture_gray(self):\r\n        """"""\r\n        Take grayscale picture.\r\n\r\n        :return: gray and resized image\r\n        :rtype: np.ndarray, np.ndarray\r\n        """"""\r\n        _, orig = self.cam.read()\r\n        img = cv2.resize(orig, (0, 0), fx=self.resize, fy=self.resize)\r\n        img = grayscale_image(img)\r\n        if self.debug:\r\n            return img, orig\r\n        return img\r\n\r\n    def take_picture_bin(self):\r\n        """"""\r\n        Take binarized picture.\r\n\r\n        :return: binary and resized image\r\n        :rtype: np.ndarray, np.ndarray\r\n        """"""\r\n        _, orig = self.cam.read()\r\n        img = cv2.resize(orig, (0, 0), fx=self.resize, fy=self.resize)\r\n        img = binarize_image(img)\r\n        if self.debug:\r\n            return img, orig\r\n        return img\r\n\r\n    def take_picture_green(self):\r\n        """"""\r\n        Take picture with only the green channel.\r\n\r\n        :return: green and resized image\r\n        :rtype: np.ndarray, np.ndarray\r\n        """"""\r\n        _, orig = self.cam.read()\r\n        img = cv2.resize(orig, (0, 0), fx=self.resize, fy=self.resize)\r\n        if self.debug:\r\n            return img[1], orig\r\n        return img[1]\r\n'"
self_driving/vision/__init__.py,0,b''
self_driving/vision/image_manipulation.py,0,"b'\'\'\'\nUseful functions for data augmentation of images\n\'\'\'\n\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\n\ndef grayscale_image(input_image):\n    """"""\n    Convert input_image to grayscale\n\n    :param input_image: image\n    :type input_image: numpy.ndarray\n    :return: image in grayscale\n    :rtype: numpy.ndarray\n    """"""\n    return cv2.cvtColor(input_image, cv2.COLOR_BGR2GRAY)\n\n\ndef binarize_image(input_image, threshold_value=177):\n    """"""\n    Convert input_image to binary representation\n\n    :param input_image: image\n    :type input_image: numpy.ndarray\n    :param threshold_value: value to be used as a\n                          threshold\n    :type threshold_value: int\n    :return: image in binary form\n    :rtype: numpy.ndarray\n    """"""\n    gray_image = grayscale_image(input_image)\n    bin_image = cv2.GaussianBlur(gray_image, (5, 5), 0)\n    _, bin_image = cv2.threshold(bin_image,\n                                 threshold_value,\n                                 255,\n                                 cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    return bin_image\n\n\ndef green_channel(input_image):\n    """"""\n    Split input_image channels and return only the green channel\n\n    :param input_image: image\n    :type input_image: numpy.ndarray\n    :return: image with only the green channel\n    :rtype: numpy.ndarray\n    """"""\n    return input_image[:, :, 1]\n\n\ndef top_bottom_cut(input_image):\n    """"""\n    Cut off randomly part\n    of the top and bottom of\n    input_image and reshape it to the original dimensions\n\n    :param input_image: image\n    :type input_image: numpy.ndarray\n    :return: cropped image\n    :rtype: numpy.ndarray\n    """"""\n    height = input_image.shape[0]\n    width = input_image.shape[1]\n    input_dtype = input_image.dtype\n    top = int(np.random.uniform(.325, .425) * height)\n    bottom = int(np.random.uniform(.075, .175) * height)\n    input_image = input_image[top:-bottom, :]\n    img = Image.fromarray(input_image)\n    img = img.resize((width, height), Image.LANCZOS)\n    cut_image = np.array(img).astype(input_dtype)\n    return cut_image\n\n\ndef random_shadow(input_image):\n    """"""\n    Insert a vertical random shadow in an input_image\n\n    :param input_image: image\n    :type input_image: numpy.ndarray\n    :return: image with shadow\n    :rtype: numpy.ndarray\n    """"""\n    height, width = input_image.shape[0], input_image.shape[1]\n    [x1, x2] = np.random.choice(width, size=2, replace=False)\n    k = height / float(x2 - x1)\n    b = - k * x1\n    im_array = input_image.copy()\n    for i in range(height):\n        c = int((i - b) / k)\n        im_array[i, :c, :] = (im_array[i, :c, :] * .5).astype(np.uint8)\n    return im_array\n\n\ndef gaussian_blur(input_image,\n                  kernel_size=5):\n    """"""\n    Blur input_image with a Gaussian convolution\n\n    :param input_image: image\n    :type input_image: numpy.ndarray\n    :param kernel_size: size of the kernel\n    :type kernel_size: int\n    :return: blured image\n    :rtype: numpy.ndarray\n    """"""\n    blur = cv2.GaussianBlur(input_image, (kernel_size, kernel_size), 0)\n    return blur\n'"
self_driving/vision/util.py,0,"b'import cv2\n\n\ndef write_img(img,\n              prob,\n              path,\n              font=cv2.FONT_HERSHEY_PLAIN,\n              fontScale=1,\n              fontColor=(255, 25, 55),\n              lineType=2,\n              position1=(15, 25),\n              position2=(15, 38),\n              position3=(15, 52)):\n    """"""\n    Write probabilistic distribution on an image.\n    This function is hard-coded for 3 classes only.\n\n    :param img: image\n    :type img: np.array\n    :param prob: probability of classess\n    :type prob: np.array\n    :param font: type of font\n    :type font: cv2.FONT_HERSHEY_PLAIN\n    :param fontScale: scale of font\n    :type fontScale: float\n    :param fontColor: font\'s Color\n    :type fontColor: tuple\n    :param lineType: type of the line\n    :type lineType: int\n    :param position1: position of the first prob\n    :type position1: tuple\n    :param position2: position of the second prob\n    :type position2: tuple\n    :param position3: position of the third prob\n    :type position3: tuple\n    """"""\n\n    cv2.putText(img, prob[0],\n                position1,\n                font,\n                fontScale,\n                fontColor,\n                lineType)\n    cv2.putText(img, prob[1],\n                position2,\n                font,\n                fontScale,\n                fontColor,\n                lineType)\n    cv2.putText(img, prob[2],\n                position3,\n                font,\n                fontScale,\n                fontColor,\n                lineType)\n    cv2.imwrite(path, img)\n'"
self_driving/data_manipulation/test/TestDataAug.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport unittest\nimport os\nimport sys\nimport inspect\nimport numpy as np\n\nalmost_current = os.path.abspath(inspect.getfile(inspect.currentframe()))\ncurrentdir = os.path.dirname(almost_current)\nparentdir = os.path.dirname(currentdir)\nsys.path.insert(0, parentdir)\n\n\nfrom util import run_test, get_image  # noqa\nfrom img2array import create_data_set_as_np_array  # noqa\nfrom data_aug import extend_dataset_flip_axis, binarize_dataset  # noqa\nfrom data_aug import gray_dataset, green_dataset, dataset_augmentation  # noqa\n\n\nclass TestDataAug(unittest.TestCase):\n    """"""\n    Class that test the data augmentation functions\n    """"""\n    @classmethod\n    def setUpClass(cls):\n        cls.image_folder = os.path.join(currentdir, ""pictures_for_test"")\n        cls.width = 160\n        cls.height = 90\n        cls.channels = 3\n        cls.data_name = os.path.join(currentdir, ""toy_160_90_3_data.npy"")\n        cls.label_name = os.path.join(currentdir, ""toy_160_90_3_labels.npy"")\n        create_data_set_as_np_array(cls.image_folder,\n                                    currentdir,\n                                    ""toy"",\n                                    cls.width,\n                                    cls.height,\n                                    cls.channels,\n                                    verbose=False)\n        cls.data = np.load(cls.data_name)\n        cls.labels = np.load(cls.label_name)\n\n    @classmethod\n    def tearDown(cls):\n        if os.path.exists(cls.data_name):\n            os.remove(cls.data_name)\n        if os.path.exists(cls.label_name):\n            os.remove(cls.label_name)\n\n    def test_flip_data(self):\n        """"""\n        In the toy dataset we have only 7 pictures classified\n        as ""right"". This test checks if the flip function is working\n        adding new 7 images with label ""left"" (2) to the dataset.\n        """"""\n        aug_data, aug_labels = extend_dataset_flip_axis(self.data, self.labels)\n        data_expected_shape = (25 + 7,\n                               self.width * self.height * self.channels)\n        self.assertEqual(aug_data.shape, data_expected_shape)\n        self.assertEqual(aug_labels.shape, (25 + 7, 1))\n        self.assertEqual(np.uint8, aug_labels.dtype)\n        self.assertEqual(np.uint8, aug_data.dtype)\n        one_right_image = 0\n        one_left_image = 25\n        original_image = get_image(self.data[one_right_image])\n        original_image = np.flip(original_image, axis=1)\n        fliped_image = get_image(aug_data[one_left_image])\n        condition = np.all(np.equal(original_image, fliped_image))\n        msg = ""images: {} (orignal) and {} (augmentaded) are not equal"".format(one_right_image, one_left_image)  # noqa\n        self.assertTrue(condition, msg=msg)\n        only_left = aug_labels[25: 25 + 7]\n        only_left = only_left.flatten()\n        self.assertEqual(np.min(only_left), np.max(only_left))\n\n    def test_one_channel_transformation(self):\n        one_channel_shape = (self.data.shape[0],\n                             self.width * self.height)\n        new_data, shape = binarize_dataset(self.data)\n        self.assertEqual(new_data.shape, one_channel_shape)\n        self.assertEqual(shape, (self.height, self.width))\n        self.assertEqual(np.uint8, new_data.dtype)\n        new_data, shape = gray_dataset(self.data)\n        self.assertEqual(new_data.shape, one_channel_shape)\n        self.assertEqual(shape, (self.height, self.width))\n        self.assertEqual(np.uint8, new_data.dtype)\n        new_data, shape = green_dataset(self.data)\n        self.assertEqual(new_data.shape, one_channel_shape)\n        self.assertEqual(shape, (self.height, self.width))\n        self.assertEqual(np.uint8, new_data.dtype)\n\n    def test_data_augmentation(self):\n        aug_data, aug_labels = dataset_augmentation(self.data, self.labels)\n        data_expected_shape = (25 * 3,\n                               self.width * self.height * self.channels)\n        self.assertEqual(aug_data.shape, data_expected_shape)\n        self.assertEqual(aug_labels.shape, (25 * 3, 1))\n        self.assertEqual(np.uint8, aug_labels.dtype)\n        self.assertEqual(np.uint8, aug_data.dtype)\n\n\nif __name__ == ""__main__"":\n    run_test(TestDataAug)\n'"
self_driving/data_manipulation/test/TestDataMani.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport unittest\nimport os\nimport sys\nimport inspect\nimport numpy as np\n\nalmost_current = os.path.abspath(inspect.getfile(inspect.currentframe()))\ncurrentdir = os.path.dirname(almost_current)\nparentdir = os.path.dirname(currentdir)\nsys.path.insert(0, parentdir)\n\n\nfrom util import run_test  # noqa\nfrom img2array import create_data_set_as_np_array  # noqa\n\n\nclass TestDataMani(unittest.TestCase):\n    """"""\n    Class that test the data manipulation functions\n    """"""\n    @classmethod\n    def setUpClass(cls):\n        cls.image_folder = os.path.join(currentdir, ""pictures_for_test"")\n        cls.width = 160\n        cls.height = 90\n        cls.channels = 3\n        cls.data_name = os.path.join(currentdir, ""toy_160_90_3_data.npy"")\n        cls.label_name = os.path.join(currentdir, ""toy_160_90_3_labels.npy"")\n\n    @classmethod\n    def tearDown(cls):\n        if os.path.exists(cls.data_name):\n            os.remove(cls.data_name)\n        if os.path.exists(cls.label_name):\n            os.remove(cls.label_name)\n\n    def test_data_is_created_from_image_folder_and_pickle(self):\n        create_data_set_as_np_array(self.image_folder,\n                                    currentdir,\n                                    ""toy"",\n                                    self.width,\n                                    self.height,\n                                    self.channels,\n                                    verbose=False)\n        data = np.load(self.data_name)\n        labels = np.load(self.label_name)\n        data_expected_shape = (25,\n                               self.width * self.height * self.channels)\n        self.assertEqual(data.shape, data_expected_shape)\n        self.assertEqual(labels.shape, (25, 1))\n        self.assertEqual(np.uint8, labels.dtype)\n        self.assertEqual(np.uint8, data.dtype)\n\n\nif __name__ == ""__main__"":\n    run_test(TestDataMani)\n'"
self_driving/data_manipulation/test/test_all.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport sys\nimport inspect\nfrom TestDataMani import TestDataMani\nfrom TestDataAug import TestDataAug\n\nalmost_current = os.path.abspath(inspect.getfile(inspect.currentframe()))\ncurrentdir = os.path.dirname(almost_current)\nparentdir = os.path.dirname(currentdir)\nsys.path.insert(0, parentdir)\n\nfrom util import run_test  # noqa\n\n\ndef main():\n    run_test(TestDataMani)\n    run_test(TestDataAug)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
self_driving/ml_training/test/TestDataHolder.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport unittest\nimport os\nimport sys\nimport inspect\nimport numpy as np\nimport itertools\n\nalmost_current = os.path.abspath(inspect.getfile(inspect.currentframe()))\ncurrentdir = os.path.dirname(almost_current)\nparentdir = os.path.dirname(currentdir)\nsys.path.insert(0, parentdir)\n\n\nfrom util import run_test, reconstruct_from_record  # noqa\nfrom Config import Config  # noqa\nfrom DataHolder import DataHolder  # noqa\n\n\nclass TestDataHolder(unittest.TestCase):\n    """"""\n    Class that test the if the data manipulation functions\n    """"""\n    @classmethod\n    def setUpClass(cls):\n        config3 = Config()\n        config_gray = Config()\n        config_green = Config()\n        config_bin = Config()\n        data_name = ""toy_160_90_3_data.npy""\n        label_name = ""toy_160_90_3_labels.npy""\n\n        cls.original_dh = DataHolder(config3,\n                                     data_name,\n                                     label_name,\n                                     record_path=""toy"")\n        cls.original_flip = DataHolder(config3,\n                                       data_name,\n                                       label_name,\n                                       record_path=""toy_flip"",\n                                       flip=True)\n        cls.original_aug = DataHolder(config3,\n                                      data_name,\n                                      label_name,\n                                      record_path=""toy_aug"",\n                                      augmentation=True)\n        cls.original_gray = DataHolder(config_gray,\n                                       data_name,\n                                       label_name,\n                                       record_path=""toy_gray"",\n                                       gray=True)\n        cls.original_green = DataHolder(config_green,\n                                        data_name,\n                                        label_name,\n                                        record_path=""toy_green"",\n                                        green=True)\n        cls.original_binary = DataHolder(config_bin,\n                                         data_name,\n                                         label_name,\n                                         record_path=""toy_bin"",\n                                         binary=True)\n        cls.all_dataholders_no_new = [cls.original_dh,\n                                      cls.original_gray,\n                                      cls.original_green,\n                                      cls.original_binary]\n        cls.all_paths = [""toy"",\n                         ""toy_flip"",\n                         ""toy_aug"",\n                         ""toy_gray"",\n                         ""toy_green"",\n                         ""toy_bin""]\n        cls.original_dh.create_records()\n        cls.original_flip.create_records()\n        cls.original_aug.create_records()\n        cls.original_gray.create_records()\n        cls.original_green.create_records()\n        cls.original_binary.create_records()\n\n    @classmethod\n    def tearDown(cls):\n        sufixes = [\'_train.tfrecords\', \'_valid.tfrecords\', \'_test.tfrecords\']\n        for car, cdr in itertools.product(cls.all_paths, sufixes):\n            file_name = car + cdr\n            if os.path.exists(file_name):\n                os.remove(file_name)\n\n    def check_size_and_type_data_holder(self,\n                                        dataholder,\n                                        mode=""train"",\n                                        sizes=[20, 3, 3]):\n        if mode == ""train"":\n            size = sizes[0]\n            record_path = dataholder.get_train_tfrecord()\n        elif mode == ""valid"":\n            size = sizes[1]\n            record_path = dataholder.get_valid_tfrecord()\n        elif mode == ""test"":\n            size = sizes[2]\n            record_path = dataholder.get_test_tfrecord()\n        images, labels, shape = reconstruct_from_record(record_path)\n        self.assertEqual(images.shape, (size, shape[0] * shape[1] * shape[2]))\n        self.assertEqual(labels.shape, (size, 1))\n        self.assertEqual(np.uint8, labels.dtype)\n        self.assertEqual(np.uint8, images.dtype)\n\n    def test_tf_record_is_created_and_can_be_restored(self):\n\n        modes = [""train"", ""valid"", ""test""]\n\n        for dh, mode in itertools.product(self.all_dataholders_no_new, modes):\n            self.check_size_and_type_data_holder(dh, mode=mode)\n        for mode in modes:\n            self.check_size_and_type_data_holder(self.original_flip,\n                                                 mode=mode,\n                                                 sizes=[25, 4, 4])\n            self.check_size_and_type_data_holder(self.original_aug,\n                                                 mode=mode,\n                                                 sizes=[60, 3, 3])\n\n\nif __name__ == ""__main__"":\n    run_test(TestDataHolder)\n'"
self_driving/ml_training/test/TestTrainer.py,2,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport unittest\nimport os\nimport sys\nimport inspect\nimport numpy as np\nimport tensorflow as tf\nimport itertools\nimport shutil\n\nalmost_current = os.path.abspath(inspect.getfile(inspect.currentframe()))\ncurrentdir = os.path.dirname(almost_current)\nparentdir = os.path.dirname(currentdir)\nsys.path.insert(0, parentdir)\n\n\nfrom util import run_test, reconstruct_from_record  # noqa\nfrom Config import Config  # noqa\nfrom DataHolder import DataHolder  # noqa\nfrom DFN import DFN  # noqa\nfrom Trainer import Trainer  # noqa\n\n\nclass TestTrainer(unittest.TestCase):\n    """"""\n    Class that test the Trainer class in optimization and prediction\n    """"""\n    @classmethod\n    def setUpClass(cls):\n        data_name = ""toy_160_90_3_data.npy""\n        label_name = ""toy_160_90_3_labels.npy""\n\n        cls.config3d = Config(epochs=1,\n                              architecture=[4],\n                              num_steps=100,\n                              save_step=10)\n        cls.config_green = Config(epochs=1,\n                                  architecture=[4],\n                                  num_steps=100,\n                                  save_step=10)\n        cls.config_gray = Config(epochs=1,\n                                 architecture=[4],\n                                 num_steps=100,\n                                 save_step=10)\n        cls.config_bin = Config(epochs=1,\n                                architecture=[4],\n                                num_steps=100,\n                                save_step=10)\n        cls.data_aug = DataHolder(cls.config3d,\n                                  data_name,\n                                  label_name,\n                                  record_path=""toy_aug"",\n                                  flip=True,\n                                  augmentation=True)\n        cls.data_gray = DataHolder(cls.config_gray,\n                                   data_name,\n                                   label_name,\n                                   record_path=""toy_gray"",\n                                   flip=True,\n                                   augmentation=False,\n                                   gray=True)\n        cls.data_green = DataHolder(cls.config_green,\n                                    data_name,\n                                    label_name,\n                                    record_path=""toy_green"",\n                                    flip=True,\n                                    augmentation=False,\n                                    green=True)\n        cls.data_binary = DataHolder(cls.config_bin,\n                                     data_name,\n                                     label_name,\n                                     flip=True,\n                                     augmentation=False,\n                                     record_path=""toy_bin"",\n                                     binary=True)\n        cls.data_aug.create_records()\n        cls.data_gray.create_records()\n        cls.data_green.create_records()\n        cls.data_binary.create_records()\n        cls.all_paths = [""toy_aug"",\n                         ""toy_gray"",\n                         ""toy_green"",\n                         ""toy_bin""]\n        cls.data_list = [cls.data_gray, cls.data_green, cls.data_binary]\n        cls.end = False\n\n    @classmethod\n    def tearDown(cls):\n        if cls.end:\n            sufixes = [\'_train.tfrecords\', \'_valid.tfrecords\', \'_test.tfrecords\']  # noqa\n            for car, cdr in itertools.product(cls.all_paths, sufixes):\n                file_name = car + cdr\n                if os.path.exists(file_name):\n                    os.remove(file_name)\n\n        if os.path.exists(""checkpoints""):\n            shutil.rmtree(""checkpoints"")\n\n    def check_overfitting_valid_data(self,\n                                     config,\n                                     dataholder):\n        if os.path.exists(""checkpoints""):\n            shutil.rmtree(""checkpoints"")\n\n        graph = tf.Graph()\n        network = DFN(graph, config)\n        trainer = Trainer(graph,\n                          config,\n                          network,\n                          dataholder)\n        non_trained_acc = trainer.get_valid_accuracy()\n        trainer.fit(verbose=False)\n        trained_acc = trainer.get_valid_accuracy()\n        condition = non_trained_acc < trained_acc\n        msg = ""Performance on valid data not better after training\\n""\n        msg += "" non_trained_acc = {0:.6f}"".format(non_trained_acc)\n        msg += "" | trained_acc = {0:.6f}"".format(trained_acc)\n        self.assertTrue(condition, msg=msg)\n\n    def check_prediction(self, config, dataholder, num_classes=4):\n        if os.path.exists(""checkpoints""):\n            shutil.rmtree(""checkpoints"")\n        record_path = dataholder.get_test_tfrecord()\n        images, _, shape = reconstruct_from_record(record_path)\n        images = images.astype(np.float32) / 255\n        num_images = images.shape[0]\n        graph = tf.Graph()\n        network = DFN(graph, config)\n        trainer = Trainer(graph,\n                          config,\n                          network,\n                          dataholder)\n        non_trained_predictions = trainer.predict(images)\n        trainer.fit(verbose=False)\n        trained_predictions = trainer.predict(images)\n        image = images[0].reshape((1, images[0].shape[0]))\n        single_prediction = trainer.predict(image)\n        self.assertEqual(non_trained_predictions.shape, (num_images,))\n        self.assertEqual(trained_predictions.shape, (num_images,))\n        self.assertEqual(np.int32, non_trained_predictions.dtype)\n        self.assertEqual(np.int32, trained_predictions.dtype)\n        self.assertEqual(np.int32, single_prediction.dtype)\n\n    def test_model_is_fitting_valid_dataset(self):\n        self.check_overfitting_valid_data(self.config3d,\n                                          self.data_aug)\n        for dh in self.data_list:\n            self.check_overfitting_valid_data(self.config_gray,\n                                              dh)\n\n    def test_prediction(self):\n        self.check_prediction(self.config3d,\n                              self.data_aug)\n        for dh in self.data_list:\n            self.check_prediction(self.config_gray,\n                                  dh)\n        TestTrainer.end = True  # hack to use TearDown only here\n\n\nif __name__ == ""__main__"":\n    run_test(TestTrainer)\n'"
self_driving/ml_training/test/test_all.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport os\nimport sys\nimport inspect\nfrom TestDataHolder import TestDataHolder\nfrom TestTrainer import TestTrainer\n\nalmost_current = os.path.abspath(inspect.getfile(inspect.currentframe()))\ncurrentdir = os.path.dirname(almost_current)\nparentdir = os.path.dirname(currentdir)\nsys.path.insert(0, parentdir)\n\nfrom util import run_test  # noqa\n\n\ndef main():\n    run_test(TestDataHolder)\n    run_test(TestTrainer)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
