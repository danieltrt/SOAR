file_path,api_count,code
data_helpers.py,2,"b'import numpy as np\nimport re\nimport itertools\nimport codecs\nfrom collections import Counter\nimport jieba\n\ndef clean_str(string):\n  """"""\n  Tokenization/string cleaning for all datasets except for SST.\n  Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n  """"""\n  string = re.sub(r""[^A-Za-z0-9(),!?\\\'\\`]"", "" "", string)\n  string = re.sub(r""\\\'s"", "" \\\'s"", string)\n  string = re.sub(r""\\\'ve"", "" \\\'ve"", string)\n  string = re.sub(r""n\\\'t"", "" n\\\'t"", string)\n  string = re.sub(r""\\\'re"", "" \\\'re"", string)\n  string = re.sub(r""\\\'d"", "" \\\'d"", string)\n  string = re.sub(r""\\\'ll"", "" \\\'ll"", string)\n  string = re.sub(r"","", "" , "", string)\n  string = re.sub(r""!"", "" ! "", string)\n  string = re.sub(r""\\("", "" \\( "", string)\n  string = re.sub(r""\\)"", "" \\) "", string)\n  string = re.sub(r""\\?"", "" \\? "", string)\n  string = re.sub(r""\\s{2,}"", "" "", string)\n  return string.strip().lower()\n\n\ndef load_data_and_labels(pos=None,neg=None):\n  """"""\n  Loads MR polarity data from files, splits the data into words and generates labels.\n  Returns split sentences and labels.\n  """"""\n  # Load data from files\n  positive_examples = list(codecs.open(""./data/chinese/pos.txt"", ""r"", ""utf-8"").readlines())\n  positive_examples = [[item for item in jieba.cut(s, cut_all=False)] for s in positive_examples]\n  negative_examples = list(codecs.open(""./data/chinese/neg.txt"", ""r"", ""utf-8"").readlines())\n  negative_examples = [[item for item in jieba.cut(s, cut_all=False)] for s in negative_examples]\n  # Split by words\n  x_text = positive_examples + negative_examples\n\n  # Generate labels\n  positive_labels = [[0, 1] for _ in positive_examples]\n  negative_labels = [[1, 0] for _ in negative_examples]\n  y = np.concatenate([positive_labels, negative_labels], 0)\n  return [x_text, y]\n\n\n\n\ndef load_test_data_and_labels(pos=None,neg=None):\n  """"""\n  Loads MR polarity data from files, splits the data into words and generates labels.\n  Returns split sentences and labels.\n  tf.flags.DEFINE_string(""positive_data_file"", ""./data/test_text/pos.txt"", ""Data source for the positive data."")\n  tf.flags.DEFINE_string(""negative_data_file"", ""./data/test_text/neg.txt"", ""Data source for the negative data."")\n  """"""\n  # Load data from files\n  positive_examples = list(codecs.open(""./data/test_text/pos.txt"", ""r"", ""utf-8"").readlines())\n  positive_examples = [[item for item in jieba.cut(s, cut_all=False)] for s in positive_examples]\n  negative_examples = list(codecs.open(""./data/test_text/neg.txt"", ""r"", ""utf-8"").readlines())\n  negative_examples = [[item for item in jieba.cut(s, cut_all=False)] for s in negative_examples]\n  # Split by words\n  x_text = positive_examples + negative_examples\n\n  # Generate labels\n  positive_labels = [[0, 1] for _ in positive_examples]\n  negative_labels = [[1, 0] for _ in negative_examples]\n  y = np.concatenate([positive_labels, negative_labels], 0)\n  return [x_text, y]\n\ndef pad_sentences(sentences, padding_word=""<PAD/>""):\n  """"""\n  Pads all sentences to the same length. The length is defined by the longest sentence.\n  Returns padded sentences.\n  """"""\n  sequence_length = max(len(x) for x in sentences)\n  padded_sentences = []\n  for i in range(len(sentences)):\n    sentence = sentences[i]\n    num_padding = sequence_length - len(sentence)\n    new_sentence = sentence + [padding_word] * num_padding\n    padded_sentences.append(new_sentence)\n  return padded_sentences\n\n\ndef build_vocab(sentences):\n  """"""\n  Builds a vocabulary mapping from word to index based on the sentences.\n  Returns vocabulary mapping and inverse vocabulary mapping.\n  """"""\n  # Build vocabulary\n  word_counts = Counter(itertools.chain(*sentences))\n  # Mapping from index to word\n  vocabulary_inv = [x[0] for x in word_counts.most_common()]\n  # Mapping from word to index\n  vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n  return [vocabulary, vocabulary_inv]\n\n\ndef build_input_data(sentences, labels, vocabulary):\n  """"""\n  Maps sentencs and labels to vectors based on a vocabulary.\n  """"""\n  x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n  y = np.array(labels)\n  return [x, y]\n\n\ndef load_data():\n  """"""\n  Loads and preprocessed data for the MR dataset.\n  Returns input vectors, labels, vocabulary, and inverse vocabulary.\n  """"""\n  # Load and preprocess data\n  sentences, labels = load_data_and_labels()\n  sentences_padded = pad_sentences(sentences)\n  vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n  x, y = build_input_data(sentences_padded, labels, vocabulary)\n  return [x, y, vocabulary, vocabulary_inv]\n\n\ndef batch_iter(data, batch_size, num_epochs):\n  """"""\n  Generates a batch iterator for a dataset.\n  """"""\n  data = np.array(data)\n  data_size = len(data)\n  num_batches_per_epoch = int(len(data)/batch_size) + 1\n  for epoch in range(num_epochs):\n    # Shuffle the data at each epoch\n    shuffle_indices = np.random.permutation(np.arange(data_size))\n    shuffled_data = data[shuffle_indices]\n    for batch_num in range(num_batches_per_epoch):\n      start_index = batch_num * batch_size\n      end_index = min((batch_num + 1) * batch_size, data_size)\n      yield shuffled_data[start_index:end_index]\n'"
eval.py,13,"b'#! /usr/bin/env python\n\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport time\nimport datetime\nimport data_helpers\nfrom text_cnn import TextCNN\nfrom tensorflow.contrib import learn\nimport csv\n\n# Parameters\n# ==================================================\n\n# Data Parameters\ntf.flags.DEFINE_string(""positive_data_file"", ""./data/test_text/pos.txt"", ""Data source for the positive data."")\ntf.flags.DEFINE_string(""negative_data_file"", ""./data/test_text/neg.txt"", ""Data source for the negative data."")\n\n\n# Eval Parameters\ntf.flags.DEFINE_integer(""batch_size"", 64, ""Batch Size (default: 64)"")\ntf.flags.DEFINE_string(""checkpoint_dir"", """", ""Checkpoint directory from training run"")\ntf.flags.DEFINE_boolean(""eval_train"", False, ""Evaluate on all training data"")\n\n# Misc Parameters\ntf.flags.DEFINE_boolean(""allow_soft_placement"", True, ""Allow device soft device placement"")\ntf.flags.DEFINE_boolean(""log_device_placement"", False, ""Log placement of ops on devices"")\n\n\nFLAGS = tf.flags.FLAGS\nFLAGS._parse_flags()\nprint(""\\nParameters:"")\nfor attr, value in sorted(FLAGS.__flags.items()):\n    print(""{}={}"".format(attr.upper(), value))\nprint("""")\n\n# CHANGE THIS: Load data. Load your own data here\nif FLAGS.eval_train:\n    x_raw, y_test = data_helpers.load_test_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n    y_test = np.argmax(y_test, axis=1)\nelse:\n    x_raw = [""a masterpiece four years in the making"", ""everything is off.""]\n    y_test = [1, 0]\n\n# Map data into vocabulary\nvocab_path = os.path.join(FLAGS.checkpoint_dir, "".."", ""vocab"")\nvocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n\ntext_list=[]\n\nfor text in x_raw:\n    text_list.append(\' \'.join(text))\n\nx_test = np.array(list(vocab_processor.transform(text_list)))\n\nprint(""\\nEvaluating...\\n"")\n\n# Evaluation\n# ==================================================\ncheckpoint_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\ngraph = tf.Graph()\nwith graph.as_default():\n    session_conf = tf.ConfigProto(\n      allow_soft_placement=FLAGS.allow_soft_placement,\n      log_device_placement=FLAGS.log_device_placement)\n    sess = tf.Session(config=session_conf)\n    with sess.as_default():\n        # Load the saved meta graph and restore variables\n        saver = tf.train.import_meta_graph(""{}.meta"".format(checkpoint_file))\n        saver.restore(sess, checkpoint_file)\n\n        # Get the placeholders from the graph by name\n        input_x = graph.get_operation_by_name(""input_x"").outputs[0]\n        # input_y = graph.get_operation_by_name(""input_y"").outputs[0]\n        dropout_keep_prob = graph.get_operation_by_name(""dropout_keep_prob"").outputs[0]\n\n        # Tensors we want to evaluate\n        predictions = graph.get_operation_by_name(""output/predictions"").outputs[0]\n\n        # Generate batches for one epoch\n        batches = data_helpers.batch_iter(list(x_test), FLAGS.batch_size, 1)\n\n        # Collect the predictions here\n        all_predictions = []\n\n        for x_test_batch in batches:\n            batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n            all_predictions = np.concatenate([all_predictions, batch_predictions])\n\n# Print accuracy if y_test is defined\nif y_test is not None:\n    correct_predictions = float(sum(all_predictions == y_test))\n    print(""Total number of test examples: {}"".format(len(y_test)))\n    print(""Accuracy: {:g}"".format(correct_predictions/float(len(y_test))))\n\n# Save the evaluation to a csv\npredictions_human_readable = np.column_stack((np.array(x_raw), all_predictions))\nout_path = os.path.join(FLAGS.checkpoint_dir, "".."", ""prediction.csv"")\nprint(""Saving evaluation to {0}"".format(out_path))\nwith open(out_path, \'w\') as f:\n    csv.writer(f).writerows(predictions_human_readable)'"
text_cnn.py,33,"b'import tensorflow as tf\nimport numpy as np\n\n\nclass TextCNN(object):\n    """"""\n    A CNN for text classification.\n    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n    """"""\n    def __init__(\n      self, sequence_length, num_classes, vocab_size,\n      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n\n        # Placeholders for input, output and dropout\n        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=""input_x"")\n        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=""input_y"")\n        self.dropout_keep_prob = tf.placeholder(tf.float32, name=""dropout_keep_prob"")\n\n        # Keeping track of l2 regularization loss (optional)\n        l2_loss = tf.constant(0.0)\n\n        # Embedding layer\n        with tf.device(\'/cpu:0\'), tf.name_scope(""embedding""):\n            self.W = tf.Variable(\n                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n                name=""W"")\n            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n\n        # Create a convolution + maxpool layer for each filter size\n        pooled_outputs = []\n        for i, filter_size in enumerate(filter_sizes):\n            with tf.name_scope(""conv-maxpool-%s"" % filter_size):\n                # Convolution Layer\n                filter_shape = [filter_size, embedding_size, 1, num_filters]\n                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=""W"")\n                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=""b"")\n                conv = tf.nn.conv2d(\n                    self.embedded_chars_expanded,\n                    W,\n                    strides=[1, 1, 1, 1],\n                    padding=""VALID"",\n                    name=""conv"")\n                # Apply nonlinearity\n                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=""relu"")\n                # Maxpooling over the outputs\n                pooled = tf.nn.max_pool(\n                    h,\n                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n                    strides=[1, 1, 1, 1],\n                    padding=\'VALID\',\n                    name=""pool"")\n                pooled_outputs.append(pooled)\n\n        # Combine all the pooled features\n        num_filters_total = num_filters * len(filter_sizes)\n        self.h_pool = tf.concat(pooled_outputs, 3)\n        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n\n        # Add dropout\n        with tf.name_scope(""dropout""):\n            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n\n        # Final (unnormalized) scores and predictions\n        with tf.name_scope(""output""):\n            W = tf.get_variable(\n                ""W"",\n                shape=[num_filters_total, num_classes],\n                initializer=tf.contrib.layers.xavier_initializer())\n            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=""b"")\n            l2_loss += tf.nn.l2_loss(W)\n            l2_loss += tf.nn.l2_loss(b)\n            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=""scores"")\n            self.predictions = tf.argmax(self.scores, 1, name=""predictions"")\n\n        # CalculateMean cross-entropy loss\n        with tf.name_scope(""loss""):\n            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n\n        # Accuracy\n        with tf.name_scope(""accuracy""):\n            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, ""float""), name=""accuracy"")\n'"
train.py,33,"b'#! /usr/bin/env python\n\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport time\nimport datetime\nimport data_helpers\nfrom text_cnn import TextCNN\nfrom tensorflow.contrib import learn\nimport jieba\n# Parameters\n# ==================================================\n\n# Data loading params\ntf.flags.DEFINE_float(""dev_sample_percentage"", .1, ""Percentage of the training data to use for validation"")\ntf.flags.DEFINE_string(""positive_data_file"", ""./data/chinese/pos.txt"", ""Data source for the positive data."")\ntf.flags.DEFINE_string(""negative_data_file"", ""./data/chinese/neg.txt"", ""Data source for the negative data."")\n\n# Model Hyperparameters\ntf.flags.DEFINE_integer(""embedding_dim"", 128, ""Dimensionality of character embedding (default: 128)"")\ntf.flags.DEFINE_string(""filter_sizes"", ""3,4,5"", ""Comma-separated filter sizes (default: \'3,4,5\')"")\ntf.flags.DEFINE_integer(""num_filters"", 128, ""Number of filters per filter size (default: 128)"")\ntf.flags.DEFINE_float(""dropout_keep_prob"", 0.5, ""Dropout keep probability (default: 0.5)"")\ntf.flags.DEFINE_float(""l2_reg_lambda"", 0.0, ""L2 regularization lambda (default: 0.0)"")\n\n# Training parameters\ntf.flags.DEFINE_integer(""batch_size"", 64, ""Batch Size (default: 64)"")\ntf.flags.DEFINE_integer(""num_epochs"", 200, ""Number of training epochs (default: 200)"")\ntf.flags.DEFINE_integer(""evaluate_every"", 100, ""Evaluate model on dev set after this many steps (default: 100)"")\ntf.flags.DEFINE_integer(""checkpoint_every"", 100, ""Save model after this many steps (default: 100)"")\ntf.flags.DEFINE_integer(""num_checkpoints"", 5, ""Number of checkpoints to store (default: 5)"")\n# Misc Parameters\ntf.flags.DEFINE_boolean(""allow_soft_placement"", True, ""Allow device soft device placement"")\ntf.flags.DEFINE_boolean(""log_device_placement"", False, ""Log placement of ops on devices"")\n\nFLAGS = tf.flags.FLAGS\nFLAGS._parse_flags()\nprint(""\\nParameters:"")\nfor attr, value in sorted(FLAGS.__flags.items()):\n    print(""{}={}"".format(attr.upper(), value))\nprint("""")\n\n\n# Data Preparation\n# ==================================================\n\n# Load data\nprint(""Loading data..."")\nx_text, y = data_helpers.load_data_and_labels()\nprint(x_text)\n# Build vocabulary\nmax_document_length = max([len(x) for x in x_text])\nvocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\ntext_list=[]\nfor text in x_text:\n    text_list.append(\' \'.join(text))\nx = np.array(list(vocab_processor.fit_transform(text_list)))\nprint(x)\n# Randomly shuffle data\nnp.random.seed(10)\nshuffle_indices = np.random.permutation(np.arange(len(y)))\nx_shuffled = x[shuffle_indices]\ny_shuffled = y[shuffle_indices]\n\n# Split train/test set\n# TODO: This is very crude, should use cross-validation\ndev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\nx_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\ny_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\nprint(""Vocabulary Size: {:d}"".format(len(vocab_processor.vocabulary_)))\nprint(""Train/Dev split: {:d}/{:d}"".format(len(y_train), len(y_dev)))\n\n\n# Training\n# ==================================================\n\nwith tf.Graph().as_default():\n    session_conf = tf.ConfigProto(\n      allow_soft_placement=FLAGS.allow_soft_placement,\n      log_device_placement=FLAGS.log_device_placement)\n    sess = tf.Session(config=session_conf)\n    with sess.as_default():\n        cnn = TextCNN(\n            sequence_length=x_train.shape[1],\n            num_classes=y_train.shape[1],\n            vocab_size=len(vocab_processor.vocabulary_),\n            embedding_size=FLAGS.embedding_dim,\n            filter_sizes=list(map(int, FLAGS.filter_sizes.split("",""))),\n            num_filters=FLAGS.num_filters,\n            l2_reg_lambda=FLAGS.l2_reg_lambda)\n\n        # Define Training procedure\n        global_step = tf.Variable(0, name=""global_step"", trainable=False)\n        optimizer = tf.train.AdamOptimizer(1e-3)\n        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n\n        # Keep track of gradient values and sparsity (optional)\n        grad_summaries = []\n        for g, v in grads_and_vars:\n            if g is not None:\n                grad_hist_summary = tf.summary.histogram(""{}/grad/hist"".format(v.name), g)\n                sparsity_summary = tf.summary.scalar(""{}/grad/sparsity"".format(v.name), tf.nn.zero_fraction(g))\n                grad_summaries.append(grad_hist_summary)\n                grad_summaries.append(sparsity_summary)\n        grad_summaries_merged = tf.summary.merge(grad_summaries)\n\n        # Output directory for models and summaries\n        timestamp = str(int(time.time()))\n        out_dir = os.path.abspath(os.path.join(os.path.curdir, ""runs"", timestamp))\n        print(""Writing to {}\\n"".format(out_dir))\n\n        # Summaries for loss and accuracy\n        loss_summary = tf.summary.scalar(""loss"", cnn.loss)\n        acc_summary = tf.summary.scalar(""accuracy"", cnn.accuracy)\n\n        # Train Summaries\n        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n        train_summary_dir = os.path.join(out_dir, ""summaries"", ""train"")\n        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n\n        # Dev summaries\n        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n        dev_summary_dir = os.path.join(out_dir, ""summaries"", ""dev"")\n        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n\n        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n        checkpoint_dir = os.path.abspath(os.path.join(out_dir, ""checkpoints""))\n        checkpoint_prefix = os.path.join(checkpoint_dir, ""model"")\n        if not os.path.exists(checkpoint_dir):\n            os.makedirs(checkpoint_dir)\n        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n\n        # Write vocabulary\n        vocab_processor.save(os.path.join(out_dir, ""vocab""))\n\n        # Initialize all variables\n        sess.run(tf.global_variables_initializer())\n\n        def train_step(x_batch, y_batch):\n            """"""\n            A single training step\n            """"""\n            feed_dict = {\n              cnn.input_x: x_batch,\n              cnn.input_y: y_batch,\n              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n            }\n            _, step, summaries, loss, accuracy = sess.run(\n                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n                feed_dict)\n            time_str = datetime.datetime.now().isoformat()\n            print(""{}: step {}, loss {:g}, acc {:g}"".format(time_str, step, loss, accuracy))\n            train_summary_writer.add_summary(summaries, step)\n\n        def dev_step(x_batch, y_batch, writer=None):\n            """"""\n            Evaluates model on a dev set\n            """"""\n            feed_dict = {\n              cnn.input_x: x_batch,\n              cnn.input_y: y_batch,\n              cnn.dropout_keep_prob: 1.0\n            }\n            step, summaries, loss, accuracy = sess.run(\n                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n                feed_dict)\n            time_str = datetime.datetime.now().isoformat()\n            print(""{}: step {}, loss {:g}, acc {:g}"".format(time_str, step, loss, accuracy))\n            if writer:\n                writer.add_summary(summaries, step)\n\n        # Generate batches\n        batches = data_helpers.batch_iter(\n            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n        # Training loop. For each batch...\n        for batch in batches:\n            x_batch, y_batch = zip(*batch)\n            train_step(x_batch, y_batch)\n            current_step = tf.train.global_step(sess, global_step)\n            if current_step % FLAGS.evaluate_every == 0:\n                print(""\\nEvaluation:"")\n                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n                print("""")\n            if current_step % FLAGS.checkpoint_every == 0:\n                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n                print(""Saved model checkpoint to {}\\n"".format(path))\n'"
demo/jieba_demo.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Shared by http://www.tensorflownews.com/\n# Github:https://github.com/TensorFlowNews\nimport jieba\nfrom tensorflow.contrib import learn\nimport numpy as np\n\nstr_neg='''\xe7\xbb\x93\xe5\xb7\xb4\xe4\xb8\xad\xe6\x96\x87\xe5\x88\x86\xe8\xaf\x8d'''\nseg_list = jieba.cut(str_neg, cut_all=False)\nword_list=[item for item in seg_list]\nprint(word_list)\nword_str=' '.join(word_list)\nprint(word_str)\nmax_document_length = len(word_list)\nvocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\nx = np.array(list(vocab_processor.fit_transform([word_str])))\n\nprint(vocab_processor.vocabulary_)\nprint(vocab_processor)\n\n"""
