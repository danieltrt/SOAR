file_path,api_count,code
model.py,26,"b""import tensorflow as tf\n\nclass SelfAttentive(object):\n  '''\n  Tensorflow implementation of 'A Structured Self Attentive Sentence Embedding'\n  (https://arxiv.org/pdf/1703.03130.pdf)\n  '''\n  def build_graph(self, n=60, d=100, u=128, d_a=350, r=30, reuse=False):\n    with tf.variable_scope('SelfAttentive', reuse=reuse):\n      # Hyperparmeters from paper\n      self.n = n\n      self.d = d\n      self.d_a = d_a\n      self.u = u\n      self.r = r\n\n      initializer = tf.contrib.layers.xavier_initializer()\n\n      embedding = tf.get_variable('embedding', shape=[100000, self.d],\n          initializer=initializer)\n      self.input_pl = tf.placeholder(tf.int32, shape=[None, self.n])\n      input_embed = tf.nn.embedding_lookup(embedding, self.input_pl)\n\n      # Declare trainable variables\n      # shape(W_s1) = d_a * 2u\n      self.W_s1 = tf.get_variable('W_s1', shape=[self.d_a, 2*self.u],\n          initializer=initializer)\n      # shape(W_s2) = r * d_a\n      self.W_s2 = tf.get_variable('W_s2', shape=[self.r, self.d_a],\n          initializer=initializer)\n\n      # BiRNN\n      self.batch_size = batch_size = tf.shape(self.input_pl)[0]\n\n      cell_fw = tf.contrib.rnn.LSTMCell(u)\n      cell_bw = tf.contrib.rnn.LSTMCell(u)\n\n      H, _ = tf.nn.bidirectional_dynamic_rnn(\n          cell_fw,\n          cell_bw,\n          input_embed,\n          dtype=tf.float32)\n      H = tf.concat([H[0], H[1]], axis=2)\n\n      self.A = A = tf.nn.softmax(\n          tf.map_fn(\n            lambda x: tf.matmul(self.W_s2, x), \n            tf.tanh(\n              tf.map_fn(\n                lambda x: tf.matmul(self.W_s1, tf.transpose(x)),\n                H))))\n\n      self.M = tf.matmul(A, H)\n\n      A_T = tf.transpose(A, perm=[0, 2, 1])\n      tile_eye = tf.tile(tf.eye(r), [batch_size, 1])\n      tile_eye = tf.reshape(tile_eye, [-1, r, r])\n      AA_T = tf.matmul(A, A_T) - tile_eye\n      self.P = tf.square(tf.norm(AA_T, axis=[-2, -1], ord='fro'))\n\n  def trainable_vars(self):\n    return [var for var in\n        tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='SelfAttentive')]\n"""
reader.py,0,"b'"""""" Code from https://github.com/flrngel/TagSpace-tensorflow/blob/master/reader.py\n""""""\n\nimport csv\nimport numpy as np\n\nclass VocabDict(object):\n  def __init__(self):\n    self.dict = {\'<unk>\': 0}\n\n  def fit(self, word):\n    if word not in self.dict:\n      self.dict[word] = len(self.dict)\n\n  def size(self):\n    return len(self.dict)\n\n  def transform(self, word):\n    if word in self.dict:\n      return self.dict[word]\n    return 0\n\n  def fit_and_transform(self, word):\n    self.fit(word)\n    return self.transform(word)\n\ndef to_categorical(y, target_dict, mode_transform=False):\n  result = []\n  if mode_transform == False:\n    l = len(np.unique(y)) + 1\n  else:\n    l = target_dict.size()\n\n  for i, d in enumerate(y):\n    tmp = [0.] * l\n    for _i, _d in enumerate(d):\n      if mode_transform == False:\n        tmp[target_dict.fit_and_transform(_d)] = 1.\n      else:\n        tmp[target_dict.transform(_d)] = 1.\n    result.append(tmp)\n  return result\n\ndef load_csv(filepath, target_columns=-1, columns_to_ignore=None,\n    has_header=True, n_classes=None, target_dict=None, mode_transform=False):\n\n  if isinstance(target_columns, list) and len(target_columns) < 1:\n    raise Exception(\'target_columns must be list with one value at least\')\n\n  from tensorflow.python.platform import gfile\n  with gfile.Open(filepath) as csv_file:\n    data_file = csv.reader(csv_file)\n    if not columns_to_ignore:\n      columns_to_ignore = []\n    if has_header:\n      header = next(data_file)\n\n    data, target = [], []\n    for i, d in enumerate(data_file):\n      data.append([_d for _i, _d in enumerate(d) if _i not in target_columns and _i not in columns_to_ignore])\n      target.append([_d+str(_i) for _i, _d in enumerate(d) if _i in target_columns])\n\n    if target_dict is None:\n      target_dict = VocabDict()\n    target = to_categorical(target, target_dict=target_dict, mode_transform=mode_transform)\n    return data, target\n'"
train.py,27,"b'import tensorflow as tf\nimport tflearn\nimport numpy as np\nimport re\nfrom model import SelfAttentive\nfrom sklearn.utils import shuffle\nfrom reader import load_csv, VocabDict\n\n\'\'\'\nparse\n\'\'\'\n\ntf.app.flags.DEFINE_integer(\'num_epochs\', 5, \'number of epochs to train\')\ntf.app.flags.DEFINE_integer(\'batch_size\', 20, \'batch size to train in one step\')\ntf.app.flags.DEFINE_integer(\'labels\', 5, \'number of label classes\')\ntf.app.flags.DEFINE_integer(\'word_pad_length\', 60, \'word pad length for training\')\ntf.app.flags.DEFINE_integer(\'decay_step\', 500, \'decay steps\')\ntf.app.flags.DEFINE_float(\'learn_rate\', 1e-2, \'learn rate for training optimization\')\ntf.app.flags.DEFINE_boolean(\'shuffle\', True, \'shuffle data FLAG\')\ntf.app.flags.DEFINE_boolean(\'train\', True, \'train mode FLAG\')\ntf.app.flags.DEFINE_boolean(\'visualize\', False, \'visualize FLAG\')\ntf.app.flags.DEFINE_boolean(\'penalization\', True, \'penalization FLAG\')\n\nFLAGS = tf.app.flags.FLAGS\n\nnum_epochs = FLAGS.num_epochs\nbatch_size = FLAGS.batch_size\ntag_size = FLAGS.labels\nword_pad_length = FLAGS.word_pad_length\nlr = FLAGS.learn_rate\n\nTOKENIZER_RE = re.compile(r""[A-Z]{2,}(?![a-z])|[A-Z][a-z]+(?=[A-Z])|[\\\'\\w\\-]+"", re.UNICODE)\ndef token_parse(iterator):\n  for value in iterator:\n    return TOKENIZER_RE.findall(value)\n\ntokenizer = tflearn.data_utils.VocabularyProcessor(word_pad_length, tokenizer_fn=lambda tokens: [token_parse(x) for x in tokens])\nlabel_dict = VocabDict()\n\ndef string_parser(arr, fit):\n  if fit == False:\n    return list(tokenizer.transform(arr))\n  else:\n    return list(tokenizer.fit_transform(arr))\n\nmodel = SelfAttentive()\nwith tf.Session() as sess:\n  # build graph\n  model.build_graph(n=word_pad_length)\n  # Downstream Application\n  with tf.variable_scope(\'DownstreamApplication\'):\n    global_step = tf.Variable(0, trainable=False, name=\'global_step\')\n    learn_rate = tf.train.exponential_decay(lr, global_step, FLAGS.decay_step, 0.95, staircase=True)\n    labels = tf.placeholder(\'float32\', shape=[None, tag_size])\n    net = tflearn.fully_connected(model.M, 2000, activation=\'relu\')\n    logits = tflearn.fully_connected(net, tag_size, activation=None)\n    loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits), axis=1)\n    if FLAGS.penalization == True:\n      p_coef = 0.004\n      p_loss = p_coef * model.P\n      loss = loss + p_loss\n      p_loss = tf.reduce_mean(p_loss)\n    loss = tf.reduce_mean(loss)\n    params = tf.trainable_variables()\n    #clipped_gradients = [tf.clip_by_value(x, -0.5, 0.5) for x in gradients]\n    optimizer = tf.train.AdamOptimizer(learn_rate)\n    grad_and_vars = tf.gradients(loss, params)\n    clipped_gradients, _ = tf.clip_by_global_norm(grad_and_vars, 0.5)\n    opt = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=global_step)\n\n  # Start Training\n  sess.run(tf.global_variables_initializer())\n\n  words, tags = load_csv(\'./data/ag_news_csv/train.csv\', target_columns=[0], columns_to_ignore=[1], target_dict=label_dict)\n  words = string_parser(words, fit=True)\n  if FLAGS.shuffle == True:\n    words, tags = shuffle(words, tags)\n  word_input = tflearn.data_utils.pad_sequences(words, maxlen=word_pad_length)\n  total = len(word_input)\n  step_print = int((total/batch_size) / 13)\n\n  if FLAGS.train == True:\n    print(\'start training\')\n    for epoch_num in range(num_epochs):\n      epoch_loss = 0\n      step_loss = 0\n      for i in range(int(total/batch_size)):\n        batch_input, batch_tags = (word_input[i*batch_size:(i+1)*batch_size], tags[i*batch_size:(i+1)*batch_size])\n        train_ops = [opt, loss, learn_rate, global_step]\n        if FLAGS.penalization == True:\n          train_ops += [p_loss]\n        result = sess.run(train_ops, feed_dict={model.input_pl: batch_input, labels: batch_tags})\n        step_loss += result[1]\n        epoch_loss += result[1]\n        if i % step_print == (step_print-step_print):\n          if FLAGS.penalization == True:\n            print(f\'step_log: (epoch: {epoch_num}, step: {i}, global_step: {result[3]}, learn_rate: {result[2]}), Loss: {step_loss/step_print}, Penalization: {result[4]})\')\n          else:\n            print(f\'step_log: (epoch: {epoch_num}, step: {i}, global_step: {result[3]}, learn_rate: {result[2]}), Loss: {step_loss/step_print})\')\n          #print(f\'{result[4]}\')\n          step_loss = 0\n      print(\'***\')\n      print(f\'epoch {epoch_num}: (global_step: {result[3]}), Average Loss: {epoch_loss/(total/batch_size)})\')\n      print(\'***\\n\')\n    saver = tf.train.Saver()\n    saver.save(sess, \'./model.ckpt\')\n  else:\n    saver = tf.train.Saver()\n    saver.restore(sess, \'./model.ckpt\')\n  \n  words, tags = load_csv(\'./data/ag_news_csv/test.csv\', target_columns=[0], columns_to_ignore=[1], target_dict=label_dict)\n  words_with_index = string_parser(words, fit=True)\n  word_input = tflearn.data_utils.pad_sequences(words_with_index, maxlen=word_pad_length)\n  total = len(word_input)\n  rs = 0.\n\n  if FLAGS.visualize == True:\n    f = open(\'visualize.html\', \'w\')\n    f.write(\'<html style=""margin:0;padding:0;""><body style=""margin:0;padding:0;"">\\n\')\n\n  for i in range(int(total/batch_size)):\n    batch_input, batch_tags = (word_input[i*batch_size:(i+1)*batch_size], tags[i*batch_size:(i+1)*batch_size])\n    result = sess.run([logits, model.A], feed_dict={model.input_pl: batch_input, labels: batch_tags})\n    arr = result[0]\n    for j in range(len(batch_tags)):\n      rs+=np.sum(np.argmax(arr[j]) == np.argmax(batch_tags[j]))\n\n    if FLAGS.visualize == True:\n      f.write(\'<div style=""margin:25px;"">\\n\')\n      for k in range(len(result[1][0])):\n        f.write(\'<p style=""margin:10px;"">\\n\')\n        ww = TOKENIZER_RE.findall(words[i*batch_size][0])\n        for j in range(word_pad_length):\n          alpha = ""{:.2f}"".format(result[1][0][k][j])\n          if len(ww) <= j:\n            w = ""___""\n          else:\n            w = ww[j]\n          f.write(f\'\\t<span style=""margin-left:3px;background-color:rgba(255,0,0,{alpha})"">{w}</span>\\n\')\n        f.write(\'</p>\\n\')\n      f.write(\'</div>\\n\')\n\n  if FLAGS.visualize == True:\n    f.write(\'</body></html>\')\n    f.close()\n  print(f\'Test accuracy: {rs/total}\')\n\n  sess.close()\n'"
