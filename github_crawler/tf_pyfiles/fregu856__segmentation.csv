file_path,api_count,code
model.py,79,"b'import tensorflow as tf\nimport os\nimport cPickle\n\nfrom utilities import PReLU, spatial_dropout, max_unpool\n\nclass ENet_model(object):\n\n    def __init__(self, model_id, img_height=512, img_width=1024, batch_size=4):\n        self.model_id = model_id\n\n        self.project_dir = ""/root/segmentation/""\n\n        self.logs_dir = self.project_dir + ""training_logs/""\n        if not os.path.exists(self.logs_dir):\n            os.makedirs(self.logs_dir)\n\n        self.batch_size = batch_size\n        self.img_height = img_height\n        self.img_width = img_width\n\n        self.no_of_classes = 20\n        self.class_weights = cPickle.load(open(""data/class_weights.pkl""))\n\n        self.wd = 2e-4 # (weight decay)\n        self.lr = 5e-4 # (learning rate)\n\n        # create all dirs for storing checkpoints and other log data:\n        self.create_model_dirs()\n\n        # add placeholders to the comp. graph:\n        self.add_placeholders()\n\n        # define the forward pass, compute logits and add to the comp. graph:\n        self.add_logits()\n\n        # compute the batch loss and add to the comp. graph:\n        self.add_loss_op()\n\n        # add a training operation (for minimizing the loss) to the comp. graph:\n        self.add_train_op()\n\n    def create_model_dirs(self):\n        self.model_dir = self.logs_dir + ""model_%s"" % self.model_id + ""/""\n        self.checkpoints_dir = self.model_dir + ""checkpoints/""\n        self.debug_imgs_dir = self.model_dir + ""imgs/""\n        if not os.path.exists(self.model_dir):\n            os.makedirs(self.model_dir)\n            os.makedirs(self.checkpoints_dir)\n            os.makedirs(self.debug_imgs_dir)\n\n    def add_placeholders(self):\n        self.imgs_ph = tf.placeholder(tf.float32,\n                    shape=[self.batch_size, self.img_height, self.img_width, 3],\n                    name=""imgs_ph"")\n\n        self.onehot_labels_ph = tf.placeholder(tf.float32,\n                    shape=[self.batch_size, self.img_height, self.img_width, self.no_of_classes],\n                    name=""onehot_labels_ph"")\n\n        # dropout probability in the early layers of the network:\n        self.early_drop_prob_ph = tf.placeholder(tf.float32, name=""early_drop_prob_ph"")\n\n        # dropout probability in the later layers of the network:\n        self.late_drop_prob_ph = tf.placeholder(tf.float32, name=""late_drop_prob_ph"")\n\n    def create_feed_dict(self, imgs_batch, early_drop_prob, late_drop_prob, onehot_labels_batch=None):\n        # return a feed_dict mapping the placeholders to the actual input data:\n        feed_dict = {}\n        feed_dict[self.imgs_ph] = imgs_batch\n        feed_dict[self.early_drop_prob_ph] = early_drop_prob\n        feed_dict[self.late_drop_prob_ph] = late_drop_prob\n        if onehot_labels_batch is not None:\n            # only add the labels data if it\'s specified (during inference, we\n            # won\'t have any labels):\n            feed_dict[self.onehot_labels_ph] = onehot_labels_batch\n\n        return feed_dict\n\n    def add_logits(self):\n        # encoder:\n        # # initial block:\n        network = self.initial_block(x=self.imgs_ph, scope=""inital"")\n        print network.get_shape().as_list()\n\n\n        # # layer 1:\n        # # # save the input shape to use in max_unpool in the decoder:\n        inputs_shape_1 = network.get_shape().as_list()\n        network, pooling_indices_1 = self.encoder_bottleneck_regular(x=network,\n                    output_depth=64, drop_prob=self.early_drop_prob_ph,\n                    scope=""bottleneck_1_0"", downsampling=True)\n        print network.get_shape().as_list()\n\n        network = self.encoder_bottleneck_regular(x=network, output_depth=64,\n                    drop_prob=self.early_drop_prob_ph, scope=""bottleneck_1_1"")\n        print network.get_shape().as_list()\n\n        network = self.encoder_bottleneck_regular(x=network, output_depth=64,\n                    drop_prob=self.early_drop_prob_ph, scope=""bottleneck_1_2"")\n        print network.get_shape().as_list()\n\n        network = self.encoder_bottleneck_regular(x=network, output_depth=64,\n                    drop_prob=self.early_drop_prob_ph, scope=""bottleneck_1_3"")\n        print network.get_shape().as_list()\n\n        network = self.encoder_bottleneck_regular(x=network, output_depth=64,\n                    drop_prob=self.early_drop_prob_ph, scope=""bottleneck_1_4"")\n        print network.get_shape().as_list()\n\n\n        # # layer 2:\n        # # # save the input shape to use in max_unpool in the decoder:\n        inputs_shape_2 = network.get_shape().as_list()\n        network, pooling_indices_2 = self.encoder_bottleneck_regular(x=network,\n                    output_depth=128, drop_prob=self.late_drop_prob_ph,\n                    scope=""bottleneck_2_0"", downsampling=True)\n        print network.get_shape().as_list()\n\n        network = self.encoder_bottleneck_regular(x=network, output_depth=128,\n                        drop_prob=self.late_drop_prob_ph, scope=""bottleneck_2_1"")\n        print network.get_shape().as_list()\n\n        network = self.encoder_bottleneck_dilated(x=network, output_depth=128,\n                    drop_prob=self.late_drop_prob_ph, scope=""bottleneck_2_2"",\n                    dilation_rate=2)\n        print network.get_shape().as_list()\n\n        network = self.encoder_bottleneck_asymmetric(x=network, output_depth=128,\n                    drop_prob=self.late_drop_prob_ph, scope=""bottleneck_2_3"")\n        print network.get_shape().as_list()\n\n        network = self.encoder_bottleneck_dilated(x=network, output_depth=128,\n                    drop_prob=self.late_drop_prob_ph, scope=""bottleneck_2_4"",\n                    dilation_rate=4)\n        print network.get_shape().as_list()\n\n        network = self.encoder_bottleneck_regular(x=network, output_depth=128,\n                    drop_prob=self.late_drop_prob_ph, scope=""bottleneck_2_5"")\n        print network.get_shape().as_list()\n\n        network = self.encoder_bottleneck_dilated(x=network, output_depth=128,\n                    drop_prob=self.late_drop_prob_ph, scope=""bottleneck_2_6"",\n                    dilation_rate=8)\n        print network.get_shape().as_list()\n\n        network = self.encoder_bottleneck_asymmetric(x=network, output_depth=128,\n                    drop_prob=self.late_drop_prob_ph, scope=""bottleneck_2_7"")\n        print network.get_shape().as_list()\n\n        network = self.encoder_bottleneck_dilated(x=network, output_depth=128,\n                    drop_prob=self.late_drop_prob_ph, scope=""bottleneck_2_8"",\n                    dilation_rate=16)\n        print network.get_shape().as_list()\n\n\n        # layer 3:\n        network = self.encoder_bottleneck_regular(x=network, output_depth=128,\n                    drop_prob=self.late_drop_prob_ph, scope=""bottleneck_3_1"")\n        print network.get_shape().as_list()\n\n        network = self.encoder_bottleneck_dilated(x=network, output_depth=128,\n                    drop_prob=self.late_drop_prob_ph, scope=""bottleneck_3_2"",\n                    dilation_rate=2)\n        print network.get_shape().as_list()\n\n        network = self.encoder_bottleneck_asymmetric(x=network, output_depth=128,\n                    drop_prob=self.late_drop_prob_ph, scope=""bottleneck_3_3"")\n        print network.get_shape().as_list()\n\n        network = self.encoder_bottleneck_dilated(x=network, output_depth=128,\n                    drop_prob=self.late_drop_prob_ph, scope=""bottleneck_3_4"",\n                    dilation_rate=4)\n        print network.get_shape().as_list()\n\n        network = self.encoder_bottleneck_regular(x=network, output_depth=128,\n                    drop_prob=self.late_drop_prob_ph, scope=""bottleneck_3_5"")\n        print network.get_shape().as_list()\n\n        network = self.encoder_bottleneck_dilated(x=network, output_depth=128,\n                    drop_prob=self.late_drop_prob_ph, scope=""bottleneck_3_6"",\n                    dilation_rate=8)\n        print network.get_shape().as_list()\n\n        network = self.encoder_bottleneck_asymmetric(x=network, output_depth=128,\n                    drop_prob=self.late_drop_prob_ph, scope=""bottleneck_3_7"")\n        print network.get_shape().as_list()\n\n        network = self.encoder_bottleneck_dilated(x=network, output_depth=128,\n                    drop_prob=self.late_drop_prob_ph, scope=""bottleneck_3_8"",\n                    dilation_rate=16)\n        print network.get_shape().as_list()\n\n\n\n        # decoder:\n        # # layer 4:\n        network = self.decoder_bottleneck(x=network, output_depth=64,\n                    scope=""bottleneck_4_0"", upsampling=True,\n                    pooling_indices=pooling_indices_2, output_shape=inputs_shape_2)\n        print network.get_shape().as_list()\n\n        network = self.decoder_bottleneck(x=network, output_depth=64,\n                    scope=""bottleneck_4_1"")\n        print network.get_shape().as_list()\n\n        network = self.decoder_bottleneck(x=network, output_depth=64,\n                    scope=""bottleneck_4_2"")\n        print network.get_shape().as_list()\n\n\n        # # layer 5:\n        network = self.decoder_bottleneck(x=network, output_depth=16,\n                    scope=""bottleneck_5_0"", upsampling=True,\n                    pooling_indices=pooling_indices_1, output_shape=inputs_shape_1)\n        print network.get_shape().as_list()\n\n        network = self.decoder_bottleneck(x=network, output_depth=16,\n                    scope=""bottleneck_5_1"")\n        print network.get_shape().as_list()\n\n\n\n        # fullconv:\n        network = tf.contrib.slim.conv2d_transpose(network, self.no_of_classes,\n                    [2, 2], stride=2, scope=""fullconv"", padding=""SAME"")\n        print network.get_shape().as_list()\n\n        self.logits = network\n\n    def add_loss_op(self):\n        # compute the weight tensor:\n        weights = self.onehot_labels_ph*self.class_weights\n        weights = tf.reduce_sum(weights, 3)\n\n        # compute the weighted cross-entropy segmentation loss for each pixel:\n        seg_loss_per_pixel = tf.losses.softmax_cross_entropy(\n                    onehot_labels=self.onehot_labels_ph, logits=self.logits,\n                    weights=weights)\n\n        # average the loss over all pixels to get the batch segmentation loss:\n        self.seg_loss = tf.reduce_mean(seg_loss_per_pixel)\n\n        # compute the total loss by summing the segmentation loss and all\n        # variable weight decay losses:\n        self.loss = (self.seg_loss +\n                    tf.add_n(tf.get_collection(""encoder_wd_losses"")) +\n                    tf.add_n(tf.get_collection(""decoder_wd_losses"")))\n\n    def add_train_op(self):\n        # create the train op:\n        optimizer = tf.train.AdamOptimizer(self.lr)\n        self.train_op = optimizer.minimize(self.loss)\n\n    def initial_block(self, x, scope):\n        # convolution branch:\n        W_conv = self.get_variable_weight_decay(scope + ""/W"",\n                    shape=[3, 3, 3, 13], # ([filter_height, filter_width, in_depth, out_depth])\n                    initializer=tf.contrib.layers.xavier_initializer(),\n                    loss_category=""encoder_wd_losses"")\n        b_conv = self.get_variable_weight_decay(scope + ""/b"", shape=[13], # ([out_depth])\n                    initializer=tf.constant_initializer(0),\n                    loss_category=""encoder_wd_losses"")\n        conv_branch = tf.nn.conv2d(x, W_conv, strides=[1, 2, 2, 1],\n                    padding=""SAME"") + b_conv\n\n        # max pooling branch:\n        pool_branch = tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1], padding=""VALID"")\n\n        # concatenate the branches:\n        concat = tf.concat([conv_branch, pool_branch], axis=3) # (3: the depth axis)\n\n        # apply batch normalization and PReLU:\n        output = tf.contrib.slim.batch_norm(concat)\n        output = PReLU(output, scope=scope)\n\n        return output\n\n    def encoder_bottleneck_regular(self, x, output_depth, drop_prob, scope,\n                proj_ratio=4, downsampling=False):\n        input_shape = x.get_shape().as_list()\n        input_depth = input_shape[3]\n\n        internal_depth = int(output_depth/proj_ratio)\n\n        # convolution branch:\n        conv_branch = x\n\n        # # 1x1 projection:\n        if downsampling:\n            W_conv = self.get_variable_weight_decay(scope + ""/W_proj"",\n                        shape=[2, 2, input_depth, internal_depth], # ([filter_height, filter_width, in_depth, out_depth])\n                        initializer=tf.contrib.layers.xavier_initializer(),\n                        loss_category=""encoder_wd_losses"")\n            conv_branch = tf.nn.conv2d(conv_branch, W_conv, strides=[1, 2, 2, 1],\n                        padding=""VALID"") # NOTE! no bias terms\n        else:\n            W_proj = self.get_variable_weight_decay(scope + ""/W_proj"",\n                        shape=[1, 1, input_depth, internal_depth], # ([filter_height, filter_width, in_depth, out_depth])\n                        initializer=tf.contrib.layers.xavier_initializer(),\n                        loss_category=""encoder_wd_losses"")\n            conv_branch = tf.nn.conv2d(conv_branch, W_proj, strides=[1, 1, 1, 1],\n                        padding=""VALID"") # NOTE! no bias terms\n        # # # batch norm and PReLU:\n        conv_branch = tf.contrib.slim.batch_norm(conv_branch)\n        conv_branch = PReLU(conv_branch, scope=scope + ""/proj"")\n\n        # # conv:\n        W_conv = self.get_variable_weight_decay(scope + ""/W_conv"",\n                    shape=[3, 3, internal_depth, internal_depth], # ([filter_height, filter_width, in_depth, out_depth])\n                    initializer=tf.contrib.layers.xavier_initializer(),\n                    loss_category=""encoder_wd_losses"")\n        b_conv = self.get_variable_weight_decay(scope + ""/b_conv"", shape=[internal_depth], # ([out_depth])\n                    initializer=tf.constant_initializer(0),\n                    loss_category=""encoder_wd_losses"")\n        conv_branch = tf.nn.conv2d(conv_branch, W_conv, strides=[1, 1, 1, 1],\n                    padding=""SAME"") + b_conv\n        # # # batch norm and PReLU:\n        conv_branch = tf.contrib.slim.batch_norm(conv_branch)\n        conv_branch = PReLU(conv_branch, scope=scope + ""/conv"")\n\n        # # 1x1 expansion:\n        W_exp = self.get_variable_weight_decay(scope + ""/W_exp"",\n                    shape=[1, 1, internal_depth, output_depth], # ([filter_height, filter_width, in_depth, out_depth])\n                    initializer=tf.contrib.layers.xavier_initializer(),\n                    loss_category=""encoder_wd_losses"")\n        conv_branch = tf.nn.conv2d(conv_branch, W_exp, strides=[1, 1, 1, 1],\n                    padding=""VALID"") # NOTE! no bias terms\n        # # # batch norm:\n        conv_branch = tf.contrib.slim.batch_norm(conv_branch)\n        # NOTE! no PReLU here\n\n        # # regularizer:\n        conv_branch = spatial_dropout(conv_branch, drop_prob)\n\n\n        # main branch:\n        main_branch = x\n\n        if downsampling:\n            # max pooling with argmax (for use in max_unpool in the decoder):\n            main_branch, pooling_indices = tf.nn.max_pool_with_argmax(main_branch,\n                        ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=""SAME"")\n            # (everytime we downsample, we also increase the feature block depth)\n\n            # pad with zeros so that the feature block depth matches:\n            depth_to_pad = output_depth - input_depth\n            paddings = tf.convert_to_tensor([[0, 0], [0, 0], [0, 0], [0, depth_to_pad]])\n            # (paddings is an integer tensor of shape [4, 2] where 4 is the rank\n            # of main_branch. For each dimension D (D = 0, 1, 2, 3) of main_branch,\n            # paddings[D, 0] is the no of values to add before the contents of\n            # main_branch in that dimension, and paddings[D, 0] is the no of\n            # values to add after the contents of main_branch in that dimension)\n            main_branch = tf.pad(main_branch, paddings=paddings, mode=""CONSTANT"")\n\n\n        # add the branches:\n        merged = conv_branch + main_branch\n\n        # apply PReLU:\n        output = PReLU(merged, scope=scope + ""/output"")\n\n        if downsampling:\n            return output, pooling_indices\n        else:\n            return output\n\n    def encoder_bottleneck_dilated(self, x, output_depth, drop_prob, scope,\n                dilation_rate, proj_ratio=4):\n        input_shape = x.get_shape().as_list()\n        input_depth = input_shape[3]\n\n        internal_depth = int(output_depth/proj_ratio)\n\n        # convolution branch:\n        conv_branch = x\n\n        # # 1x1 projection:\n        W_proj = self.get_variable_weight_decay(scope + ""/W_proj"",\n                    shape=[1, 1, input_depth, internal_depth], # ([filter_height, filter_width, in_depth, out_depth])\n                    initializer=tf.contrib.layers.xavier_initializer(),\n                    loss_category=""encoder_wd_losses"")\n        conv_branch = tf.nn.conv2d(conv_branch, W_proj, strides=[1, 1, 1, 1],\n                    padding=""VALID"") # NOTE! no bias terms\n        # # # batch norm and PReLU:\n        conv_branch = tf.contrib.slim.batch_norm(conv_branch)\n        conv_branch = PReLU(conv_branch, scope=scope + ""/proj"")\n\n        # # dilated conv:\n        W_conv = self.get_variable_weight_decay(scope + ""/W_conv"",\n                    shape=[3, 3, internal_depth, internal_depth], # ([filter_height, filter_width, in_depth, out_depth])\n                    initializer=tf.contrib.layers.xavier_initializer(),\n                    loss_category=""encoder_wd_losses"")\n        b_conv = self.get_variable_weight_decay(scope + ""/b_conv"", shape=[internal_depth], # ([out_depth])\n                    initializer=tf.constant_initializer(0),\n                    loss_category=""encoder_wd_losses"")\n        conv_branch = tf.nn.atrous_conv2d(conv_branch, W_conv, rate=dilation_rate,\n                    padding=""SAME"") + b_conv\n        # # # batch norm and PReLU:\n        conv_branch = tf.contrib.slim.batch_norm(conv_branch)\n        conv_branch = PReLU(conv_branch, scope=scope + ""/conv"")\n\n        # # 1x1 expansion:\n        W_exp = self.get_variable_weight_decay(scope + ""/W_exp"",\n                    shape=[1, 1, internal_depth, output_depth], # ([filter_height, filter_width, in_depth, out_depth])\n                    initializer=tf.contrib.layers.xavier_initializer(),\n                    loss_category=""encoder_wd_losses"")\n        conv_branch = tf.nn.conv2d(conv_branch, W_exp, strides=[1, 1, 1, 1],\n                    padding=""VALID"") # NOTE! no bias terms\n        # # # batch norm:\n        conv_branch = tf.contrib.slim.batch_norm(conv_branch)\n        # NOTE! no PReLU here\n\n        # # regularizer:\n        conv_branch = spatial_dropout(conv_branch, drop_prob)\n\n\n        # main branch:\n        main_branch = x\n\n\n        # add the branches:\n        merged = conv_branch + main_branch\n\n        # apply PReLU:\n        output = PReLU(merged, scope=scope + ""/output"")\n\n        return output\n\n    def encoder_bottleneck_asymmetric(self, x, output_depth, drop_prob, scope, proj_ratio=4):\n        input_shape = x.get_shape().as_list()\n        input_depth = input_shape[3]\n\n        internal_depth = int(output_depth/proj_ratio)\n\n        # convolution branch:\n        conv_branch = x\n\n        # # 1x1 projection:\n        W_proj = self.get_variable_weight_decay(scope + ""/W_proj"",\n                    shape=[1, 1, input_depth, internal_depth], # ([filter_height, filter_width, in_depth, out_depth])\n                    initializer=tf.contrib.layers.xavier_initializer(),\n                    loss_category=""encoder_wd_losses"")\n        conv_branch = tf.nn.conv2d(conv_branch, W_proj, strides=[1, 1, 1, 1],\n                    padding=""VALID"") # NOTE! no bias terms\n        # # # batch norm and PReLU:\n        conv_branch = tf.contrib.slim.batch_norm(conv_branch)\n        conv_branch = PReLU(conv_branch, scope=scope + ""/proj"")\n\n        # # asymmetric conv:\n        # # # asymmetric conv 1:\n        W_conv1 = self.get_variable_weight_decay(scope + ""/W_conv1"",\n                    shape=[5, 1, internal_depth, internal_depth], # ([filter_height, filter_width, in_depth, out_depth])\n                    initializer=tf.contrib.layers.xavier_initializer(),\n                    loss_category=""encoder_wd_losses"")\n        conv_branch = tf.nn.conv2d(conv_branch, W_conv1, strides=[1, 1, 1, 1],\n                    padding=""SAME"") # NOTE! no bias terms\n        # # # asymmetric conv 2:\n        W_conv2 = self.get_variable_weight_decay(scope + ""/W_conv2"",\n                    shape=[1, 5, internal_depth, internal_depth], # ([filter_height, filter_width, in_depth, out_depth])\n                    initializer=tf.contrib.layers.xavier_initializer(),\n                    loss_category=""encoder_wd_losses"")\n        b_conv2 = self.get_variable_weight_decay(scope + ""/b_conv2"", shape=[internal_depth], # ([out_depth])\n                    initializer=tf.constant_initializer(0),\n                    loss_category=""encoder_wd_losses"")\n        conv_branch = tf.nn.conv2d(conv_branch, W_conv2, strides=[1, 1, 1, 1],\n                    padding=""SAME"") + b_conv2\n        # # # batch norm and PReLU:\n        conv_branch = tf.contrib.slim.batch_norm(conv_branch)\n        conv_branch = PReLU(conv_branch, scope=scope + ""/conv"")\n\n        # # 1x1 expansion:\n        W_exp = self.get_variable_weight_decay(scope + ""/W_exp"",\n                    shape=[1, 1, internal_depth, output_depth], # ([filter_height, filter_width, in_depth, out_depth])\n                    initializer=tf.contrib.layers.xavier_initializer(),\n                    loss_category=""encoder_wd_losses"")\n        conv_branch = tf.nn.conv2d(conv_branch, W_exp, strides=[1, 1, 1, 1],\n                    padding=""VALID"") # NOTE! no bias terms\n        # # # batch norm:\n        conv_branch = tf.contrib.slim.batch_norm(conv_branch)\n        # NOTE! no PReLU here\n\n        # # regularizer:\n        conv_branch = spatial_dropout(conv_branch, drop_prob)\n\n\n        # main branch:\n        main_branch = x\n\n\n        # add the branches:\n        merged = conv_branch + main_branch\n\n        # apply PReLU:\n        output = PReLU(merged, scope=scope + ""/output"")\n\n        return output\n\n    def decoder_bottleneck(self, x, output_depth, scope, proj_ratio=4,\n                upsampling=False, pooling_indices=None, output_shape=None):\n        # NOTE! decoder uses ReLU instead of PReLU\n\n        input_shape = x.get_shape().as_list()\n        input_depth = input_shape[3]\n\n        internal_depth = int(output_depth/proj_ratio)\n\n        # main branch:\n        main_branch = x\n\n        if upsampling:\n            # # 1x1 projection (to decrease depth to the same value as before downsampling):\n            W_upsample = self.get_variable_weight_decay(scope + ""/W_upsample"",\n                        shape=[1, 1, input_depth, output_depth], # ([filter_height, filter_width, in_depth, out_depth])\n                        initializer=tf.contrib.layers.xavier_initializer(),\n                        loss_category=""decoder_wd_losses"")\n            main_branch = tf.nn.conv2d(main_branch, W_upsample, strides=[1, 1, 1, 1],\n                        padding=""VALID"") # NOTE! no bias terms\n            # # # batch norm:\n            main_branch = tf.contrib.slim.batch_norm(main_branch)\n            # NOTE! no ReLU here\n\n            # # max unpooling:\n            main_branch = max_unpool(main_branch, pooling_indices, output_shape)\n\n        main_branch = tf.cast(main_branch, tf.float32)\n\n\n        # convolution branch:\n        conv_branch = x\n\n        # # 1x1 projection:\n        W_proj = self.get_variable_weight_decay(scope + ""/W_proj"",\n                    shape=[1, 1, input_depth, internal_depth], # ([filter_height, filter_width, in_depth, out_depth])\n                    initializer=tf.contrib.layers.xavier_initializer(),\n                    loss_category=""decoder_wd_losses"")\n        conv_branch = tf.nn.conv2d(conv_branch, W_proj, strides=[1, 1, 1, 1],\n                    padding=""VALID"") # NOTE! no bias terms\n        # # # batch norm and ReLU:\n        conv_branch = tf.contrib.slim.batch_norm(conv_branch)\n        conv_branch = tf.nn.relu(conv_branch)\n\n        # # conv:\n        if upsampling:\n            # deconvolution:\n            W_conv = self.get_variable_weight_decay(scope + ""/W_conv"",\n                        shape=[3, 3, internal_depth, internal_depth], # ([filter_height, filter_width, in_depth, out_depth])\n                        initializer=tf.contrib.layers.xavier_initializer(),\n                        loss_category=""decoder_wd_losses"")\n            b_conv = self.get_variable_weight_decay(scope + ""/b_conv"", shape=[internal_depth], # ([out_depth]], one bias weight per out depth layer),\n                        initializer=tf.constant_initializer(0),\n                        loss_category=""decoder_wd_losses"")\n            main_branch_shape = main_branch.get_shape().as_list()\n            output_shape = tf.convert_to_tensor([main_branch_shape[0],\n                        main_branch_shape[1], main_branch_shape[2], internal_depth])\n            conv_branch = tf.nn.conv2d_transpose(conv_branch, W_conv, output_shape=output_shape,\n                        strides=[1, 2, 2, 1], padding=""SAME"") + b_conv\n        else:\n            W_conv = self.get_variable_weight_decay(scope + ""/W_conv"",\n                        shape=[3, 3, internal_depth, internal_depth], # ([filter_height, filter_width, in_depth, out_depth])\n                        initializer=tf.contrib.layers.xavier_initializer(),\n                        loss_category=""decoder_wd_losses"")\n            b_conv = self.get_variable_weight_decay(scope + ""/b_conv"", shape=[internal_depth], # ([out_depth])\n                        initializer=tf.constant_initializer(0),\n                        loss_category=""decoder_wd_losses"")\n            conv_branch = tf.nn.conv2d(conv_branch, W_conv, strides=[1, 1, 1, 1],\n                        padding=""SAME"") + b_conv\n        # # # batch norm and ReLU:\n        conv_branch = tf.contrib.slim.batch_norm(conv_branch)\n        conv_branch = tf.nn.relu(conv_branch)\n\n        # # 1x1 expansion:\n        W_exp = self.get_variable_weight_decay(scope + ""/W_exp"",\n                    shape=[1, 1, internal_depth, output_depth], # ([filter_height, filter_width, in_depth, out_depth])\n                    initializer=tf.contrib.layers.xavier_initializer(),\n                    loss_category=""decoder_wd_losses"")\n        conv_branch = tf.nn.conv2d(conv_branch, W_exp, strides=[1, 1, 1, 1],\n                    padding=""VALID"") # NOTE! no bias terms\n        # # # batch norm:\n        conv_branch = tf.contrib.slim.batch_norm(conv_branch)\n        # NOTE! no ReLU here\n\n        # NOTE! no regularizer\n\n\n        # add the branches:\n        merged = conv_branch + main_branch\n\n        # apply ReLU:\n        output = tf.nn.relu(merged)\n\n        return output\n\n    def get_variable_weight_decay(self, name, shape, initializer, loss_category,\n                dtype=tf.float32):\n        variable = tf.get_variable(name, shape=shape, dtype=dtype,\n                    initializer=initializer)\n\n        # add a variable weight decay loss:\n        weight_decay = self.wd*tf.nn.l2_loss(variable)\n        tf.add_to_collection(loss_category, weight_decay)\n\n        return variable\n'"
preprocess_data.py,0,"b'import cv2\nimport cPickle\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom collections import namedtuple\nimport random\n\nproject_dir = ""/root/segmentation/""\ndata_dir = ""/root/data/""\n\n# (NOTE! this is taken from the official Cityscapes scripts:)\nLabel = namedtuple( \'Label\' , [\n\n    \'name\'        , # The identifier of this label, e.g. \'car\', \'person\', ... .\n                    # We use them to uniquely name a class\n\n    \'id\'          , # An integer ID that is associated with this label.\n                    # The IDs are used to represent the label in ground truth images\n                    # An ID of -1 means that this label does not have an ID and thus\n                    # is ignored when creating ground truth images (e.g. license plate).\n                    # Do not modify these IDs, since exactly these IDs are expected by the\n                    # evaluation server.\n\n    \'trainId\'     , # Feel free to modify these IDs as suitable for your method. Then create\n                    # ground truth images with train IDs, using the tools provided in the\n                    # \'preparation\' folder. However, make sure to validate or submit results\n                    # to our evaluation server using the regular IDs above!\n                    # For trainIds, multiple labels might have the same ID. Then, these labels\n                    # are mapped to the same class in the ground truth images. For the inverse\n                    # mapping, we use the label that is defined first in the list below.\n                    # For example, mapping all void-type classes to the same ID in training,\n                    # might make sense for some approaches.\n                    # Max value is 255!\n\n    \'category\'    , # The name of the category that this label belongs to\n\n    \'categoryId\'  , # The ID of this category. Used to create ground truth images\n                    # on category level.\n\n    \'hasInstances\', # Whether this label distinguishes between single instances or not\n\n    \'ignoreInEval\', # Whether pixels having this class as ground truth label are ignored\n                    # during evaluations or not\n\n    \'color\'       , # The color of this label\n    ] )\n\n# (NOTE! this is taken from the official Cityscapes scripts:)\nlabels = [\n    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n    Label(  \'unlabeled\'            ,  0 ,      19 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'ego vehicle\'          ,  1 ,      19 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'rectification border\' ,  2 ,      19 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'out of roi\'           ,  3 ,      19 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'static\'               ,  4 ,      19 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'dynamic\'              ,  5 ,      19 , \'void\'            , 0       , False        , True         , (111, 74,  0) ),\n    Label(  \'ground\'               ,  6 ,      19 , \'void\'            , 0       , False        , True         , ( 81,  0, 81) ),\n    Label(  \'road\'                 ,  7 ,        0 , \'flat\'            , 1       , False        , False        , (128, 64,128) ),\n    Label(  \'sidewalk\'             ,  8 ,        1 , \'flat\'            , 1       , False        , False        , (244, 35,232) ),\n    Label(  \'parking\'              ,  9 ,      19 , \'flat\'            , 1       , False        , True         , (250,170,160) ),\n    Label(  \'rail track\'           , 10 ,      19 , \'flat\'            , 1       , False        , True         , (230,150,140) ),\n    Label(  \'building\'             , 11 ,        2 , \'construction\'    , 2       , False        , False        , ( 70, 70, 70) ),\n    Label(  \'wall\'                 , 12 ,        3 , \'construction\'    , 2       , False        , False        , (102,102,156) ),\n    Label(  \'fence\'                , 13 ,        4 , \'construction\'    , 2       , False        , False        , (190,153,153) ),\n    Label(  \'guard rail\'           , 14 ,      19 , \'construction\'    , 2       , False        , True         , (180,165,180) ),\n    Label(  \'bridge\'               , 15 ,      19 , \'construction\'    , 2       , False        , True         , (150,100,100) ),\n    Label(  \'tunnel\'               , 16 ,      19 , \'construction\'    , 2       , False        , True         , (150,120, 90) ),\n    Label(  \'pole\'                 , 17 ,        5 , \'object\'          , 3       , False        , False        , (153,153,153) ),\n    Label(  \'polegroup\'            , 18 ,      19 , \'object\'          , 3       , False        , True         , (153,153,153) ),\n    Label(  \'traffic light\'        , 19 ,        6 , \'object\'          , 3       , False        , False        , (250,170, 30) ),\n    Label(  \'traffic sign\'         , 20 ,        7 , \'object\'          , 3       , False        , False        , (220,220,  0) ),\n    Label(  \'vegetation\'           , 21 ,        8 , \'nature\'          , 4       , False        , False        , (107,142, 35) ),\n    Label(  \'terrain\'              , 22 ,        9 , \'nature\'          , 4       , False        , False        , (152,251,152) ),\n    Label(  \'sky\'                  , 23 ,       10 , \'sky\'             , 5       , False        , False        , ( 70,130,180) ),\n    Label(  \'person\'               , 24 ,       11 , \'human\'           , 6       , True         , False        , (220, 20, 60) ),\n    Label(  \'rider\'                , 25 ,       12 , \'human\'           , 6       , True         , False        , (255,  0,  0) ),\n    Label(  \'car\'                  , 26 ,       13 , \'vehicle\'         , 7       , True         , False        , (  0,  0,142) ),\n    Label(  \'truck\'                , 27 ,       14 , \'vehicle\'         , 7       , True         , False        , (  0,  0, 70) ),\n    Label(  \'bus\'                  , 28 ,       15 , \'vehicle\'         , 7       , True         , False        , (  0, 60,100) ),\n    Label(  \'caravan\'              , 29 ,      19 , \'vehicle\'         , 7       , True         , True         , (  0,  0, 90) ),\n    Label(  \'trailer\'              , 30 ,      19 , \'vehicle\'         , 7       , True         , True         , (  0,  0,110) ),\n    Label(  \'train\'                , 31 ,       16 , \'vehicle\'         , 7       , True         , False        , (  0, 80,100) ),\n    Label(  \'motorcycle\'           , 32 ,       17 , \'vehicle\'         , 7       , True         , False        , (  0,  0,230) ),\n    Label(  \'bicycle\'              , 33 ,       18 , \'vehicle\'         , 7       , True         , False        , (119, 11, 32) ),\n    Label(  \'license plate\'        , -1 ,       -1 , \'vehicle\'         , 7       , False        , True         , (  0,  0,142) ),\n]\n\n# create a function mapping id to trainId:\nid_to_trainId = {label.id: label.trainId for label in labels}\nid_to_trainId_map_func = np.vectorize(id_to_trainId.get)\n\nnew_img_height = 512 # (the height all images fed to the model will be resized to)\nnew_img_width = 1024 # (the width all images fed to the model will be resized to)\nno_of_classes = 20 # (number of object classes (road, sidewalk, car etc.))\n\ncityscapes_dir = data_dir + ""cityscapes/""\n\ntrain_imgs_dir = cityscapes_dir + ""leftImg8bit/train/""\ntrain_gt_dir = cityscapes_dir + ""gtFine/train/""\n\nval_imgs_dir = cityscapes_dir + ""leftImg8bit/val/""\nval_gt_dir = cityscapes_dir + ""gtFine/val/""\n\ntrain_dirs = [""jena/"", ""zurich/"", ""weimar/"", ""ulm/"", ""tubingen/"", ""stuttgart/"",\n            ""strasbourg/"", ""monchengladbach/"", ""krefeld/"", ""hanover/"",\n            ""hamburg/"", ""erfurt/"", ""dusseldorf/"", ""darmstadt/"", ""cologne/"",\n            ""bremen/"", ""bochum/"", ""aachen/""]\nval_dirs = [""frankfurt/"", ""munster/"", ""lindau/""]\n\n\n# get the path to all training images and their corresponding label image:\ntrain_img_paths = []\ntrain_trainId_label_paths = []\nfor dir_step, dir in enumerate(train_dirs):\n    img_dir = train_imgs_dir + dir\n\n    file_names = os.listdir(img_dir)\n    for step, file_name in enumerate(file_names):\n        if step % 10 == 0:\n            print (""train dir %d/%d, step %d/%d"" % (dir_step, len(train_dirs)-1,\n                        step, len(file_names)-1))\n\n        img_id = file_name.split(""_left"")[0]\n\n        # read the image:\n        img_path = img_dir + file_name\n        img = cv2.imread(img_path, -1)\n\n        # resize the image without interpolation (want the image to still match\n        # the corresponding label image which we reisize below) and save to\n        # project_dir/data:\n        img_small = cv2.resize(img, (new_img_width, new_img_height),\n                    interpolation=cv2.INTER_NEAREST)\n        img_small_path = project_dir + ""data/"" + img_id + "".png""\n        cv2.imwrite(img_small_path, img_small)\n        train_img_paths.append(img_small_path)\n\n        # read and resize the corresponding label image without interpolation\n        # (want the resulting image to still only contain pixel values\n        # corresponding to an object class):\n        gt_img_path = train_gt_dir + dir + img_id + ""_gtFine_labelIds.png""\n        gt_img = cv2.imread(gt_img_path, -1)\n        gt_img_small = cv2.resize(gt_img, (new_img_width, new_img_height),\n                        interpolation=cv2.INTER_NEAREST)\n\n        # convert the label image from id to trainId pixel values:\n        id_label = gt_img_small\n        trainId_label = id_to_trainId_map_func(id_label)\n\n        # save the label image to project_dir/data:\n        trainId_label_path = project_dir + ""data/"" + img_id + ""_trainId_label.png""\n        cv2.imwrite(trainId_label_path, trainId_label)\n        train_trainId_label_paths.append(trainId_label_path)\n\n\n# compute the mean color channels of the train imgs:\nprint ""computing mean color channels of the train imgs""\nno_of_train_imgs = len(train_img_paths)\nmean_channels = np.zeros((3, ))\nfor step, img_path in enumerate(train_img_paths):\n    if step % 100 == 0:\n        print step\n\n    img = cv2.imread(img_path, -1)\n\n    img_mean_channels = np.mean(img, axis=0)\n    img_mean_channels = np.mean(img_mean_channels, axis=0)\n\n    mean_channels += img_mean_channels\n\nmean_channels = mean_channels/float(no_of_train_imgs)\n\n# # save to disk:\ncPickle.dump(mean_channels, open(project_dir + ""data/mean_channels.pkl"", ""w""))\n\n\n# compute the class weights:\nprint ""computing class weights""\ntrainId_to_count = {}\nfor trainId in range(no_of_classes):\n    trainId_to_count[trainId] = 0\n\n# # get the total number of pixels in all train labels that are of each\n# # object class:\nfor step, trainId_label_path in enumerate(train_trainId_label_paths):\n    if step % 100 == 0:\n        print step\n\n    # read the label image:\n    trainId_label = cv2.imread(trainId_label_path, -1)\n\n    for trainId in range(no_of_classes):\n        # count how many pixels in the label image are of object class trainId:\n        trainId_mask = np.equal(trainId_label, trainId)\n        label_trainId_count = np.sum(trainId_mask)\n\n        # add to the total count:\n        trainId_to_count[trainId] += label_trainId_count\n\n# # compute the class weights according to the paper:\nclass_weights = []\ntotal_count = sum(trainId_to_count.values())\nfor trainId, count in trainId_to_count.items():\n    trainId_prob = float(count)/float(total_count)\n    trainId_weight = 1/np.log(1.02 + trainId_prob)\n    class_weights.append(trainId_weight)\n\n# # save to disk:\ncPickle.dump(class_weights, open(project_dir + ""data/class_weights.pkl"", ""w""))\n\n\n# get the path to all validation images and their corresponding label image:\nval_img_paths = []\nval_trainId_label_paths = []\nfor dir_step, dir in enumerate(val_dirs):\n    img_dir = val_imgs_dir + dir\n\n    file_names = os.listdir(img_dir)\n    for step, file_name in enumerate(file_names):\n        if step % 10 == 0:\n            print ""val dir %d/%d, step %d/%d"" % (dir_step, len(val_dirs)-1,\n                        step, len(file_names)-1)\n\n        img_id = file_name.split(""_left"")[0]\n\n        # read the image:\n        img_path = img_dir + file_name\n        img = cv2.imread(img_path, -1)\n\n        # resize the image without interpolation (want the image to still match\n        # the corresponding label image which we reisize below) and save to\n        # project_dir/data:\n        img_small = cv2.resize(img, (new_img_width, new_img_height),\n                    interpolation=cv2.INTER_NEAREST)\n        img_small_path = project_dir + ""data/"" + img_id + "".png""\n        cv2.imwrite(img_small_path, img_small)\n        val_img_paths.append(img_small_path)\n\n        # read and resize the corresponding label image without interpolation\n        # (want the resulting image to still only contain pixel values\n        # corresponding to an object class):\n        gt_img_path = val_gt_dir + dir + img_id + ""_gtFine_labelIds.png""\n        gt_img = cv2.imread(gt_img_path, -1)\n        gt_img_small = cv2.resize(gt_img, (new_img_width, new_img_height),\n                    interpolation=cv2.INTER_NEAREST)\n\n        # convert the label image from id to trainId pixel values:\n        id_label = gt_img_small\n        trainId_label = id_to_trainId_map_func(id_label)\n\n        # save the label image to project_dir/data:\n        trainId_label_path = project_dir + ""data/"" + img_id + ""_trainId_label.png""\n        cv2.imwrite(trainId_label_path, trainId_label)\n        val_trainId_label_paths.append(trainId_label_path)\n\n# # save the validation data to disk:\ncPickle.dump(val_trainId_label_paths,\n            open(project_dir + ""data/val_trainId_label_paths.pkl"", ""w""))\ncPickle.dump(val_img_paths,\n            open(project_dir + ""data/val_img_paths.pkl"", ""w""))\n# val_trainId_label_paths = cPickle.load(open(project_dir + ""data/val_trainId_label_paths.pkl""))\n# val_img_paths = cPickle.load(open(project_dir + ""data/val_img_paths.pkl""))\n\n\n# augment the train data by flipping all train imgs:\nno_of_train_imgs = len(train_img_paths)\nprint ""number of train imgs before augmentation: %d "" % no_of_train_imgs\n\naugmented_train_img_paths = []\naugmented_train_trainId_label_paths = []\nfor step, (img_path, label_path) in enumerate(zip(train_img_paths, train_trainId_label_paths)):\n    if step % 100 == 0:\n        print step\n\n    augmented_train_img_paths.append(img_path)\n    augmented_train_trainId_label_paths.append(label_path)\n\n    # read the image:\n    img = cv2.imread(img_path, -1)\n\n    # flip the image and save to project_dir/data:\n    img_flipped = cv2.flip(img, 1)\n    img_flipped_path = img_path.split("".png"")[0] + ""_flipped.png""\n    cv2.imwrite(img_flipped_path, img_flipped)\n    augmented_train_img_paths.append(img_flipped_path)\n\n    # read the corresponding label image:\n    label_img = cv2.imread(label_path, -1)\n\n    # flip the label image and save to project_dir/data:\n    label_img_flipped = cv2.flip(label_img, 1)\n    label_img_flipped_path = label_path.split("".png"")[0] + ""_flipped.png""\n    cv2.imwrite(label_img_flipped_path, label_img_flipped)\n    augmented_train_trainId_label_paths.append(label_img_flipped_path)\n\n# # randomly shuffle the augmented train data:\naugmented_train_data = zip(augmented_train_img_paths, augmented_train_trainId_label_paths)\nrandom.shuffle(augmented_train_data)\nrandom.shuffle(augmented_train_data)\nrandom.shuffle(augmented_train_data)\nrandom.shuffle(augmented_train_data)\n\n# # save the augmented train data to disk:\ntrain_data = augmented_train_data\ntrain_img_paths, train_trainId_label_paths = zip(*train_data)\ncPickle.dump(train_img_paths,\n            open(project_dir + ""data/train_img_paths.pkl"", ""w""))\ncPickle.dump(train_trainId_label_paths,\n            open(project_dir + ""data/train_trainId_label_paths.pkl"", ""w""))\n# train_img_paths = cPickle.load(open(project_dir + ""data/train_img_paths.pkl""))\n# train_trainId_label_paths = cPickle.load(open(project_dir + ""data/train_trainId_label_paths.pkl""))\n\nno_of_train_imgs = len(train_img_paths)\nprint ""number of train imgs after augmentation: %d "" % no_of_train_imgs\n'"
run_on_sequence.py,3,"b'import numpy as np\nimport cPickle\nimport tensorflow as tf\nimport cv2\nimport os\n\nfrom utilities import label_img_to_color\n\nfrom model import ENet_model\n\nproject_dir = ""/root/segmentation/""\n\ndata_dir = project_dir + ""data/""\n\nmodel_id = ""sequence_run""\n\nbatch_size = 4\nimg_height = 512\nimg_width = 1024\n\nmodel = ENet_model(model_id, img_height=img_height, img_width=img_width,\n            batch_size=batch_size)\n\nno_of_classes = model.no_of_classes\n\n# load the mean color channels of the train imgs:\ntrain_mean_channels = cPickle.load(open(""data/mean_channels.pkl""))\n\n# load the sequence data:\nseq_frames_dir = ""/root/data/cityscapes/leftImg8bit/demoVideo/stuttgart_02/""\nseq_frame_paths = []\nframe_names = sorted(os.listdir(seq_frames_dir))\nfor step, frame_name in enumerate(frame_names):\n    if step % 100 == 0:\n        print step\n\n    frame_path = seq_frames_dir + frame_name\n    seq_frame_paths.append(frame_path)\n\n# compute the number of batches needed to iterate through the data:\nno_of_frames = len(seq_frame_paths)\nno_of_batches = int(no_of_frames/batch_size)\n\n# define where to place the resulting images:\nresults_dir = model.project_dir + ""results_on_seq/""\n\n# create a saver for restoring variables/parameters:\nsaver = tf.train.Saver(tf.trainable_variables(), write_version=tf.train.SaverDef.V2)\n\nwith tf.Session() as sess:\n    # initialize all variables/parameters:\n    init = tf.global_variables_initializer()\n    sess.run(init)\n\n    # restore the best trained model:\n    saver.restore(sess, project_dir + ""training_logs/best_model/model_1_epoch_23.ckpt"")\n\n    batch_pointer = 0\n    for step in range(no_of_batches):\n        batch_imgs = np.zeros((batch_size, img_height, img_width, 3), dtype=np.float32)\n        img_paths = []\n\n        for i in range(batch_size):\n            img_path = seq_frame_paths[batch_pointer + i]\n            img_paths.append(img_path)\n\n            # read the image:\n            img = cv2.imread(img_path, -1)\n            img = cv2.resize(img, (img_width, img_height))\n            img = img - train_mean_channels\n            batch_imgs[i] = img\n\n        batch_pointer += batch_size\n\n        batch_feed_dict = model.create_feed_dict(imgs_batch=batch_imgs,\n                    early_drop_prob=0.0, late_drop_prob=0.0)\n\n        # run a forward pass and get the logits:\n        logits = sess.run(model.logits, feed_dict=batch_feed_dict)\n\n        print ""step: %d/%d"" % (step+1, no_of_batches)\n\n        # save all predicted label images overlayed on the input frames to results_dir:\n        predictions = np.argmax(logits, axis=3)\n        for i in range(batch_size):\n            pred_img = predictions[i]\n            pred_img_color = label_img_to_color(pred_img)\n\n            img = batch_imgs[i] + train_mean_channels\n\n            img_file_name = img_paths[i].split(""/"")[-1]\n            img_name = img_file_name.split("".png"")[0]\n            pred_path = results_dir + img_name + ""_pred.png""\n\n            overlayed_img = 0.3*img + 0.7*pred_img_color\n\n            cv2.imwrite(pred_path, overlayed_img)\n\n# create a video of all the resulting overlayed images:\nfourcc = cv2.cv.CV_FOURCC(""M"", ""J"", ""P"", ""G"")\nout = cv2.VideoWriter(results_dir + ""cityscapes_stuttgart_02_pred.avi"", fourcc,\n            20.0, (img_width, img_height))\n\nframe_names = sorted(os.listdir(results_dir))\nfor step, frame_name in enumerate(frame_names):\n    if step % 100 == 0:\n        print step\n\n    if "".png"" in frame_name:\n        frame_path = results_dir + frame_name\n        frame = cv2.imread(frame_path, -1)\n\n        out.write(frame)\n'"
train.py,3,"b'import numpy as np\nimport cPickle\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport cv2\nimport random\n\nfrom utilities import label_img_to_color\n\nfrom model import ENet_model\n\nproject_dir = ""/root/segmentation/""\n\ndata_dir = project_dir + ""data/""\n\n# change this to not overwrite all log data when you train the model:\nmodel_id = ""1""\n\nbatch_size = 4\nimg_height = 512\nimg_width = 1024\n\nmodel = ENet_model(model_id, img_height=img_height, img_width=img_width,\n            batch_size=batch_size)\n\nno_of_classes = model.no_of_classes\n\n# load the mean color channels of the train imgs:\ntrain_mean_channels = cPickle.load(open(""data/mean_channels.pkl""))\n\n# load the training data from disk:\ntrain_img_paths = cPickle.load(open(data_dir + ""train_img_paths.pkl""))\ntrain_trainId_label_paths = cPickle.load(open(data_dir + ""train_trainId_label_paths.pkl""))\ntrain_data = zip(train_img_paths, train_trainId_label_paths)\n\n# compute the number of batches needed to iterate through the training data:\nno_of_train_imgs = len(train_img_paths)\nno_of_batches = int(no_of_train_imgs/batch_size)\n\n# load the validation data from disk:\nval_img_paths = cPickle.load(open(data_dir + ""val_img_paths.pkl""))\nval_trainId_label_paths = cPickle.load(open(data_dir + ""val_trainId_label_paths.pkl""))\nval_data = zip(val_img_paths, val_trainId_label_paths)\n\n# compute the number of batches needed to iterate through the val data:\nno_of_val_imgs = len(val_img_paths)\nno_of_val_batches = int(no_of_val_imgs/batch_size)\n\n# define params needed for label to onehot label conversion:\nlayer_idx = np.arange(img_height).reshape(img_height, 1)\ncomponent_idx = np.tile(np.arange(img_width), (img_height, 1))\n\ndef evaluate_on_val():\n    random.shuffle(val_data)\n    val_img_paths, val_trainId_label_paths = zip(*val_data)\n\n    val_batch_losses = []\n    batch_pointer = 0\n    for step in range(no_of_val_batches):\n        batch_imgs = np.zeros((batch_size, img_height, img_width, 3), dtype=np.float32)\n        batch_onehot_labels = np.zeros((batch_size, img_height, img_width,\n                    no_of_classes), dtype=np.float32)\n\n        for i in range(batch_size):\n            # read the next img:\n            img = cv2.imread(val_img_paths[batch_pointer + i], -1)\n            img = img - train_mean_channels\n            batch_imgs[i] = img\n\n            # read the next label:\n            trainId_label = cv2.imread(val_trainId_label_paths[batch_pointer + i], -1)\n\n            # convert the label to onehot:\n            onehot_label = np.zeros((img_height, img_width, no_of_classes), dtype=np.float32)\n            onehot_label[layer_idx, component_idx, trainId_label] = 1\n            batch_onehot_labels[i] = onehot_label\n\n        batch_pointer += batch_size\n\n        batch_feed_dict = model.create_feed_dict(imgs_batch=batch_imgs,\n                    early_drop_prob=0.0, late_drop_prob=0.0,\n                    onehot_labels_batch=batch_onehot_labels)\n\n        # run a forward pass, get the batch loss and the logits:\n        batch_loss, logits = sess.run([model.loss, model.logits],\n                    feed_dict=batch_feed_dict)\n\n        val_batch_losses.append(batch_loss)\n        print (""epoch: %d/%d, val step: %d/%d, val batch loss: %g"" % (epoch+1,\n                    no_of_epochs, step+1, no_of_val_batches, batch_loss))\n\n        if step < 4:\n            # save the predicted label images to disk for debugging and\n            # qualitative evaluation:\n            predictions = np.argmax(logits, axis=3)\n            for i in range(batch_size):\n                pred_img = predictions[i]\n                label_img_color = label_img_to_color(pred_img)\n                cv2.imwrite((model.debug_imgs_dir + ""val_"" + str(epoch) + ""_"" +\n                            str(step) + ""_"" + str(i) + "".png""), label_img_color)\n\n    val_loss = np.mean(val_batch_losses)\n    return val_loss\n\ndef train_data_iterator():\n    random.shuffle(train_data)\n    train_img_paths, train_trainId_label_paths = zip(*train_data)\n\n    batch_pointer = 0\n    for step in range(no_of_batches):\n        # get and yield the next batch_size imgs and onehot labels from the train data:\n        batch_imgs = np.zeros((batch_size, img_height, img_width, 3), dtype=np.float32)\n        batch_onehot_labels = np.zeros((batch_size, img_height, img_width,\n                    no_of_classes), dtype=np.float32)\n\n        for i in range(batch_size):\n            # read the next img:\n            img = cv2.imread(train_img_paths[batch_pointer + i], -1)\n            img = img - train_mean_channels\n            batch_imgs[i] = img\n\n            # read the next label:\n            trainId_label = cv2.imread(train_trainId_label_paths[batch_pointer + i], -1)\n\n            # convert the label to onehot:\n            onehot_label = np.zeros((img_height, img_width, no_of_classes), dtype=np.float32)\n            onehot_label[layer_idx, component_idx, trainId_label] = 1\n            batch_onehot_labels[i] = onehot_label\n\n        batch_pointer += batch_size\n\n        yield (batch_imgs, batch_onehot_labels)\n\nno_of_epochs = 100\n\n# create a saver for saving all model variables/parameters:\nsaver = tf.train.Saver(tf.trainable_variables(), write_version=tf.train.SaverDef.V2)\n\n# initialize all log data containers:\ntrain_loss_per_epoch = []\nval_loss_per_epoch = []\n\n# initialize a list containing the 5 best val losses (is used to tell when to\n# save a model checkpoint):\nbest_epoch_losses = [1000, 1000, 1000, 1000, 1000]\n\nwith tf.Session() as sess:\n    # initialize all variables/parameters:\n    init = tf.global_variables_initializer()\n    sess.run(init)\n\n    for epoch in range(no_of_epochs):\n        print ""###########################""\n        print ""######## NEW EPOCH ########""\n        print ""###########################""\n        print ""epoch: %d/%d"" % (epoch+1, no_of_epochs)\n\n        # run an epoch and get all batch losses:\n        batch_losses = []\n        for step, (imgs, onehot_labels) in enumerate(train_data_iterator()):\n            # create a feed dict containing the batch data:\n            batch_feed_dict = model.create_feed_dict(imgs_batch=imgs,\n                        early_drop_prob=0.01, late_drop_prob=0.1,\n                        onehot_labels_batch=onehot_labels)\n\n            # compute the batch loss and compute & apply all gradients w.r.t to\n            # the batch loss (without model.train_op in the call, the network\n            # would NOT train, we would only compute the batch loss):\n            batch_loss, _ = sess.run([model.loss, model.train_op],\n                        feed_dict=batch_feed_dict)\n            batch_losses.append(batch_loss)\n\n            print ""step: %d/%d, training batch loss: %g"" % (step+1, no_of_batches, batch_loss)\n\n        # compute the train epoch loss:\n        train_epoch_loss = np.mean(batch_losses)\n        # save the train epoch loss:\n        train_loss_per_epoch.append(train_epoch_loss)\n        # save the train epoch losses to disk:\n        cPickle.dump(train_loss_per_epoch, open(""%strain_loss_per_epoch.pkl""\n                    % model.model_dir, ""w""))\n        print ""training loss: %g"" % train_epoch_loss\n\n        # run the model on the validation data:\n        val_loss = evaluate_on_val()\n\n        # save the val epoch loss:\n        val_loss_per_epoch.append(val_loss)\n        # save the val epoch losses to disk:\n        cPickle.dump(val_loss_per_epoch, open(""%sval_loss_per_epoch.pkl""\\\n                    % model.model_dir, ""w""))\n        print ""validation loss: %g"" % val_loss\n\n        if val_loss < max(best_epoch_losses): # (if top 5 performance on val:)\n            # save the model weights to disk:\n            checkpoint_path = (model.checkpoints_dir + ""model_"" +\n                        model.model_id + ""_epoch_"" + str(epoch + 1) + "".ckpt"")\n            saver.save(sess, checkpoint_path)\n            print ""checkpoint saved in file: %s"" % checkpoint_path\n\n            # update the top 5 val losses:\n            index = best_epoch_losses.index(max(best_epoch_losses))\n            best_epoch_losses[index] = val_loss\n\n        # plot the training loss vs epoch and save to disk:\n        plt.figure(1)\n        plt.plot(train_loss_per_epoch, ""k^"")\n        plt.plot(train_loss_per_epoch, ""k"")\n        plt.ylabel(""loss"")\n        plt.xlabel(""epoch"")\n        plt.title(""training loss per epoch"")\n        plt.savefig(""%strain_loss_per_epoch.png"" % model.model_dir)\n        plt.close(1)\n\n        # plot the val loss vs epoch and save to disk:\n        plt.figure(1)\n        plt.plot(val_loss_per_epoch, ""k^"")\n        plt.plot(val_loss_per_epoch, ""k"")\n        plt.ylabel(""loss"")\n        plt.xlabel(""epoch"")\n        plt.title(""validation loss per epoch"")\n        plt.savefig(""%sval_loss_per_epoch.png"" % model.model_dir)\n        plt.close(1)\n'"
utilities.py,15,"b'import tensorflow as tf\nimport cv2\nimport numpy as np\n\ndef PReLU(x, scope):\n    # PReLU(x) = x if x > 0, alpha*x otherwise\n\n    alpha = tf.get_variable(scope + ""/alpha"", shape=[1],\n                initializer=tf.constant_initializer(0), dtype=tf.float32)\n\n    output = tf.nn.relu(x) + alpha*(x - abs(x))*0.5\n\n    return output\n\n# function for 2D spatial dropout:\ndef spatial_dropout(x, drop_prob):\n    # x is a tensor of shape [batch_size, height, width, channels]\n\n    keep_prob = 1.0 - drop_prob\n    input_shape = x.get_shape().as_list()\n\n    batch_size = input_shape[0]\n    channels = input_shape[3]\n\n    # drop each channel with probability drop_prob:\n    noise_shape = tf.constant(value=[batch_size, 1, 1, channels])\n    x_drop = tf.nn.dropout(x, keep_prob, noise_shape=noise_shape)\n\n    output = x_drop\n\n    return output\n\n# function for unpooling max_pool:\ndef max_unpool(inputs, pooling_indices, output_shape=None, k_size=[1, 2, 2, 1]):\n    # NOTE! this function is based on the implementation by kwotsin in\n    # https://github.com/kwotsin/TensorFlow-ENet\n\n    # inputs has shape [batch_size, height, width, channels]\n\n    # pooling_indices: pooling indices of the previously max_pooled layer\n\n    # output_shape: what shape the returned tensor should have\n\n    pooling_indices = tf.cast(pooling_indices, tf.int32)\n    input_shape = tf.shape(inputs, out_type=tf.int32)\n\n    one_like_pooling_indices = tf.ones_like(pooling_indices, dtype=tf.int32)\n    batch_shape = tf.concat([[input_shape[0]], [1], [1], [1]], 0)\n    batch_range = tf.reshape(tf.range(input_shape[0], dtype=tf.int32), shape=batch_shape)\n    b = one_like_pooling_indices*batch_range\n    y = pooling_indices//(output_shape[2]*output_shape[3])\n    x = (pooling_indices//output_shape[3]) % output_shape[2]\n    feature_range = tf.range(output_shape[3], dtype=tf.int32)\n    f = one_like_pooling_indices*feature_range\n\n    inputs_size = tf.size(inputs)\n    indices = tf.transpose(tf.reshape(tf.stack([b, y, x, f]), [4, inputs_size]))\n    values = tf.reshape(inputs, [inputs_size])\n\n    ret = tf.scatter_nd(indices, values, output_shape)\n\n    return ret\n\n# function for colorizing a label image:\ndef label_img_to_color(img):\n    label_to_color = {\n        0: [128, 64,128],\n        1: [244, 35,232],\n        2: [ 70, 70, 70],\n        3: [102,102,156],\n        4: [190,153,153],\n        5: [153,153,153],\n        6: [250,170, 30],\n        7: [220,220,  0],\n        8: [107,142, 35],\n        9: [152,251,152],\n        10: [ 70,130,180],\n        11: [220, 20, 60],\n        12: [255,  0,  0],\n        13: [  0,  0,142],\n        14: [  0,  0, 70],\n        15: [  0, 60,100],\n        16: [  0, 80,100],\n        17: [  0,  0,230],\n        18: [119, 11, 32],\n        19: [81,  0, 81]\n        }\n\n    img_height, img_width = img.shape\n\n    img_color = np.zeros((img_height, img_width, 3))\n    for row in range(img_height):\n        for col in range(img_width):\n            label = img[row, col]\n\n            img_color[row, col] = np.array(label_to_color[label])\n\n    return img_color\n'"
