file_path,api_count,code
control.py,0,"b'from tensorflow_serving.apis import get_model_status_pb2, model_service_pb2_grpc, model_management_pb2\nimport grpc\nfrom absl import app,flags\nfrom enum import unique,Enum\nfrom kazoo.client import KazooClient\nfrom yolo3.utils import get_classes\n\n@unique\nclass MODE(Enum):\n    STATUS=1\n    CONFIG=2\n    ZOOKEEPER=3\n\nFLAGS=flags.FLAGS\nflags.DEFINE_enum_class(""mode"",default=MODE.ZOOKEEPER,enum_class=MODE,help=\'exec mode\')\nflags.DEFINE_multi_string(""addresses"",default=[""10.12.102.32:8500"",""10.12.102.33:8500"",""10.12.102.52:8500"",""10.12.102.53:8500""],help=\'grpc servers address\')\n\ndef get_config(*args):\n    return bytes(\'#\'.join(str(arg) for arg in args),encoding=""utf8"")\n\ndef main(_):\n    if MODE.STATUS == FLAGS.mode:\n        request = get_model_status_pb2.GetModelStatusRequest()\n        request.model_spec.name = \'detection\'\n        request.model_spec.signature_name = \'serving_default\'\n    elif MODE.CONFIG == FLAGS.mode:\n        request = model_management_pb2.ReloadConfigRequest()\n        config = request.config.model_config_list.config.add()\n        config.name = \'detection\'\n        config.base_path = \'/models/detection/detection\'\n        config.model_platform = \'tensorflow\'\n        config.model_version_policy.specific.versions.append(5)\n        config.model_version_policy.specific.versions.append(7)\n        config2 = request.config.model_config_list.config.add()\n        config2.name = \'pascal\'\n        config2.base_path = \'/models/detection/pascal\'\n        config2.model_platform = \'tensorflow\'\n    elif MODE.ZOOKEEPER==FLAGS.mode:\n        zk = KazooClient(hosts=""10.10.67.225:2181"")\n        zk.start()\n        zk.ensure_path(\'/serving/cunan\')\n        zk.set(\'/serving/cunan\',get_config(\'detection\',5,224,\'serving_default\',\',\'.join(get_classes(\'model_data/cci.names\')),""10.12.102.32:8000""))\n        return\n    for address in FLAGS.addresses:\n        channel = grpc.insecure_channel(address)\n        stub = model_service_pb2_grpc.ModelServiceStub(channel)\n        if MODE.STATUS==FLAGS.mode:\n            result = stub.GetModelStatus(request)\n        elif MODE.CONFIG==FLAGS.mode:\n            result = stub.HandleReloadConfigRequest(request)\n        print(result)\n\n\nif __name__ == \'__main__\':\n    app.run(main)\n'"
convert.py,12,"b'#! /usr/bin/env python\n""""""\nReads Darknet config and weights and creates Keras model with TF backend.\n\n""""""\n\nimport argparse\nimport configparser\nimport io\nimport os\nfrom collections import defaultdict\n\nimport numpy as np\nimport tensorflow as tf\n\n#os.environ[""CUDA_VISIBLE_DEVICES""] = """"\nparser = argparse.ArgumentParser(description=\'Darknet To Keras Converter.\')\nparser.add_argument(\'config_path\', help=\'Path to Darknet cfg file.\')\nparser.add_argument(\'weights_path\', help=\'Path to Darknet weights file.\')\nparser.add_argument(\'output_path\', help=\'Path to output Keras model file.\')\nparser.add_argument(\'-p\',\n                    \'--plot_model\',\n                    help=\'Plot generated Keras model and save as image.\',\n                    action=\'store_true\')\nparser.add_argument(\'-w\',\n                    \'--weights_only\',\n                    help=\'Save as Keras weights file instead of model file.\',\n                    action=\'store_true\')\n\n\ndef unique_config_sections(config_file):\n    """"""Convert all config sections to have unique names.\n\n    Adds unique suffixes to config sections for compability with configparser.\n    """"""\n    section_counters = defaultdict(int)\n    output_stream = io.StringIO()\n    with open(config_file) as fin:\n        for line in fin:\n            if line.startswith(\'[\'):\n                section = line.strip().strip(\'[]\')\n                _section = section + \'_\' + str(section_counters[section])\n                section_counters[section] += 1\n                line = line.replace(section, _section)\n            output_stream.write(line)\n    output_stream.seek(0)\n    return output_stream\n\n\n# %%\ndef _main(args):\n    config_path = os.path.expanduser(args.config_path)\n    weights_path = os.path.expanduser(args.weights_path)\n    assert config_path.endswith(\'.cfg\'), \'{} is not a .cfg file\'.format(\n        config_path)\n    assert weights_path.endswith(\n        \'.weights\'), \'{} is not a .weights file\'.format(weights_path)\n\n    output_path = os.path.expanduser(args.output_path)\n    assert output_path.endswith(\n        \'.h5\'), \'output path {} is not a .h5 file\'.format(output_path)\n    output_root = os.path.splitext(output_path)[0]\n\n    # Load weights and config.\n    print(\'Loading weights.\')\n    weights_file = open(weights_path, \'rb\')\n    major, minor, revision = np.ndarray(shape=(3,),\n                                        dtype=\'int32\',\n                                        buffer=weights_file.read(12))\n    if (major * 10 + minor) >= 2 and major < 1000 and minor < 1000:\n        seen = np.ndarray(shape=(1,),\n                          dtype=\'int64\',\n                          buffer=weights_file.read(8))\n    else:\n        seen = np.ndarray(shape=(1,),\n                          dtype=\'int32\',\n                          buffer=weights_file.read(4))\n    print(\'Weights Header: \', major, minor, revision, seen)\n\n    print(\'Parsing Darknet config.\')\n    unique_config_file = unique_config_sections(config_path)\n    cfg_parser = configparser.ConfigParser()\n    cfg_parser.read_file(unique_config_file)\n\n    print(\'Creating Keras model.\')\n    input_layer = tf.keras.layers.Input(shape=(None, None, 3))\n    prev_layer = input_layer\n    all_layers = []\n\n    weight_decay = float(cfg_parser[\'net_0\'][\'decay\']\n                        ) if \'net_0\' in cfg_parser.sections() else 5e-4\n    count = 0\n    out_index = []\n    for section in cfg_parser.sections():\n        print(\'Parsing section {}\'.format(section))\n        if section.startswith(\'convolutional\'):\n            filters = int(cfg_parser[section][\'filters\'])\n            size = int(cfg_parser[section][\'size\'])\n            stride = int(cfg_parser[section][\'stride\'])\n            pad = int(cfg_parser[section][\'pad\'])\n            activation = cfg_parser[section][\'activation\']\n            batch_normalize = \'batch_normalize\' in cfg_parser[section]\n\n            padding = \'same\' if pad == 1 and stride == 1 else \'valid\'\n\n            # Setting weights.\n            # Darknet serializes convolutional weights as:\n            # [bias/beta, [gamma, mean, variance], conv_weights]\n            prev_layer_shape = prev_layer.shape\n\n            weights_shape = (size, size, prev_layer_shape[-1], filters)\n            darknet_w_shape = (filters, weights_shape[2], size, size)\n            weights_size = np.product(weights_shape)\n\n            print(\'conv2d\', \'bn\' if batch_normalize else \'  \', activation,\n                  weights_shape)\n\n            conv_bias = np.ndarray(shape=(filters,),\n                                   dtype=\'float32\',\n                                   buffer=weights_file.read(filters * 4))\n            count += filters\n\n            if batch_normalize:\n                bn_weights = np.ndarray(shape=(3, filters),\n                                        dtype=\'float32\',\n                                        buffer=weights_file.read(filters * 12))\n                count += 3 * filters\n\n                bn_weight_list = [\n                    bn_weights[0],  # scale gamma\n                    conv_bias,  # shift beta\n                    bn_weights[1],  # running mean\n                    bn_weights[2]  # running var\n                ]\n\n            conv_weights = np.ndarray(shape=darknet_w_shape,\n                                      dtype=\'float32\',\n                                      buffer=weights_file.read(weights_size *\n                                                               4))\n            count += weights_size\n\n            # DarkNet conv_weights are serialized Caffe-style:\n            # (out_dim, in_dim, height, width)\n            # We would like to set these to Tensorflow order:\n            # (height, width, in_dim, out_dim)\n            conv_weights = np.transpose(conv_weights, [2, 3, 1, 0])\n            conv_weights = [conv_weights\n                           ] if batch_normalize else [conv_weights, conv_bias]\n\n            # Handle activation.\n            act_fn = None\n            if activation == \'leaky\':\n                pass  # Add advanced activation later.\n            elif activation != \'linear\':\n                raise ValueError(\n                    \'Unknown activation function `{}` in section {}\'.format(\n                        activation, section))\n\n            # Create Conv2D layer\n            if stride > 1:\n                # Darknet uses left and top padding instead of \'same\' mode\n                prev_layer = tf.keras.layers.ZeroPadding2D(\n                    ((1, 0), (1, 0)))(prev_layer)\n            conv_layer = (tf.keras.layers.Conv2D(\n                filters, (size, size),\n                strides=(stride, stride),\n                kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n                use_bias=not batch_normalize,\n                weights=conv_weights,\n                activation=act_fn,\n                padding=padding))(prev_layer)\n\n            if batch_normalize:\n                conv_layer = (tf.keras.layers.BatchNormalization(\n                    weights=bn_weight_list))(conv_layer)\n            prev_layer = conv_layer\n\n            if activation == \'linear\':\n                all_layers.append(prev_layer)\n            elif activation == \'leaky\':\n                act_layer = tf.keras.layers.LeakyReLU(alpha=0.1)(prev_layer)\n                prev_layer = act_layer\n                all_layers.append(act_layer)\n\n        elif section.startswith(\'route\'):\n            ids = [int(i) for i in cfg_parser[section][\'layers\'].split(\',\')]\n            layers = [all_layers[i] for i in ids]\n            if len(layers) > 1:\n                print(\'Concatenating route layers:\', layers)\n                concatenate_layer = tf.keras.layers.Concatenate()(layers)\n                all_layers.append(concatenate_layer)\n                prev_layer = concatenate_layer\n            else:\n                skip_layer = layers[0]  # only one layer to route\n                all_layers.append(skip_layer)\n                prev_layer = skip_layer\n\n        elif section.startswith(\'maxpool\'):\n            size = int(cfg_parser[section][\'size\'])\n            stride = int(cfg_parser[section][\'stride\'])\n            all_layers.append(\n                tf.keras.layers.MaxPooling2D(pool_size=(size, size),\n                                             strides=(stride, stride),\n                                             padding=\'same\')(prev_layer))\n            prev_layer = all_layers[-1]\n\n        elif section.startswith(\'shortcut\'):\n            index = int(cfg_parser[section][\'from\'])\n            activation = cfg_parser[section][\'activation\']\n            assert activation == \'linear\', \'Only linear activation supported.\'\n            all_layers.append(\n                tf.keras.layers.Add()([all_layers[index], prev_layer]))\n            prev_layer = all_layers[-1]\n\n        elif section.startswith(\'upsample\'):\n            stride = int(cfg_parser[section][\'stride\'])\n            assert stride == 2, \'Only stride=2 supported.\'\n            all_layers.append(tf.keras.layers.UpSampling2D(stride)(prev_layer))\n            prev_layer = all_layers[-1]\n\n        elif section.startswith(\'yolo\'):\n            out_index.append(len(all_layers) - 1)\n            all_layers.append(None)\n            prev_layer = all_layers[-1]\n\n        elif section.startswith(\'net\'):\n            pass\n\n        else:\n            raise ValueError(\n                \'Unsupported section header type: {}\'.format(section))\n\n    # Create and save model.\n    if len(out_index) == 0: out_index.append(len(all_layers) - 1)\n    model = tf.keras.models.Model(inputs=input_layer,\n                                  outputs=[all_layers[i] for i in out_index])\n    print(model.summary())\n    if args.weights_only:\n        model.save_weights(\'{}\'.format(output_path))\n        print(\'Saved Keras weights to {}\'.format(output_path))\n    else:\n        model.save(\'{}\'.format(output_path))\n        print(\'Saved Keras model to {}\'.format(output_path))\n\n    # Check to see if all weights have been read.\n    remaining_weights = len(weights_file.read()) / 4\n    weights_file.close()\n    print(\'Read {} of {} from Darknet weights.\'.format(\n        count, count + remaining_weights))\n    if remaining_weights > 0:\n        print(\'Warning: {} unused weights\'.format(remaining_weights))\n\n    if args.plot_model:\n        tf.keras.utils.plot_model(model,\n                                  to_file=\'{}.png\'.format(output_root),\n                                  show_shapes=True)\n        print(\'Saved model plot to {}.png\'.format(output_root))\n\n\nif __name__ == \'__main__\':\n    _main(parser.parse_args())\n'"
kmeans.py,13,"b'import numpy as np\nimport matplotlib.pyplot as plt\nfrom yolo3.utils import bind\nimport tensorflow as tf\nfrom yolo3.data import Dataset\nfrom yolo3.enum import DATASET_MODE\n\nif hasattr(tf, \'enable_eager_execution\'):\n    tf.enable_eager_execution()\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\n\nclass YOLO_Kmeans:\n\n    def parse_tfrecord(self, example_proto):\n        feature_description = {\n            \'image/object/bbox/xmin\': tf.io.VarLenFeature(tf.float32),\n            \'image/object/bbox/xmax\': tf.io.VarLenFeature(tf.float32),\n            \'image/object/bbox/ymin\': tf.io.VarLenFeature(tf.float32),\n            \'image/object/bbox/ymax\': tf.io.VarLenFeature(tf.float32)\n        }\n        features = tf.io.parse_single_example(example_proto,\n                                              feature_description)\n        xmins = features[\'image/object/bbox/xmin\'].values\n        xmaxs = features[\'image/object/bbox/xmax\'].values\n        ymins = features[\'image/object/bbox/ymin\'].values\n        ymaxs = features[\'image/object/bbox/ymax\'].values\n        return xmins, xmaxs, ymins, ymaxs\n    def parse_text(self, line):\n        values = tf.strings.split([line],\' \').values\n        reshaped_data = tf.reshape(values[1:], [-1, 5])\n        xmins = tf.strings.to_number(reshaped_data[:, 0], tf.float32)\n        xmaxs = tf.strings.to_number(reshaped_data[:, 2], tf.float32)\n        ymins = tf.strings.to_number(reshaped_data[:, 1], tf.float32)\n        ymaxs = tf.strings.to_number(reshaped_data[:, 3], tf.float32)\n        return xmins, xmaxs, ymins, ymaxs\n\n    def __init__(self, cluster_number, glob_path):\n        self.cluster_number = cluster_number\n        self.glob_path = glob_path\n\n    def iou(self, boxes, clusters):  # 1 box -> k clusters\n        n = boxes.shape[0]\n        k = self.cluster_number\n\n        box_area = boxes[:, 0] * boxes[:, 1]\n        box_area = box_area.repeat(k)\n        box_area = np.reshape(box_area, (n, k))\n\n        cluster_area = clusters[:, 0] * clusters[:, 1]\n        cluster_area = np.tile(cluster_area, [1, n])\n        cluster_area = np.reshape(cluster_area, (n, k))\n\n        box_w_matrix = np.reshape(boxes[:, 0].repeat(k), (n, k))\n        cluster_w_matrix = np.reshape(np.tile(clusters[:, 0], (1, n)), (n, k))\n        min_w_matrix = np.minimum(cluster_w_matrix, box_w_matrix)\n\n        box_h_matrix = np.reshape(boxes[:, 1].repeat(k), (n, k))\n        cluster_h_matrix = np.reshape(np.tile(clusters[:, 1], (1, n)), (n, k))\n        min_h_matrix = np.minimum(cluster_h_matrix, box_h_matrix)\n        inter_area = np.multiply(min_w_matrix, min_h_matrix)\n\n        result = inter_area / (box_area + cluster_area - inter_area)\n        return result\n\n    def avg_iou(self, boxes, clusters):\n        accuracy = np.mean([np.max(self.iou(boxes, clusters), axis=1)])\n        return accuracy\n\n    def kmeans(self, boxes, k, dist=np.median):\n        box_number = boxes.shape[0]\n        distances = np.empty((box_number, k))\n        last_nearest = np.zeros((box_number,))\n        np.random.seed()\n        clusters = boxes[np.random.choice(box_number, k,\n                                          replace=False)]  # init k clusters\n        while True:\n\n            distances = 1 - self.iou(boxes, clusters)\n\n            current_nearest = np.argmin(distances, axis=1)\n            if (last_nearest == current_nearest).all():\n                break  # clusters won\'t change\n            for cluster in range(k):\n                clusters[cluster] = dist(  # update clusters\n                    boxes[current_nearest == cluster],\n                    axis=0)\n\n            last_nearest = current_nearest\n\n        return clusters\n\n    def result2txt(self, data):\n        f = open(""yolo_anchors.txt"", \'w\')\n        row = np.shape(data)[0]\n        for i in range(row):\n            if i == 0:\n                x_y = ""%d,%d"" % (data[i][0], data[i][1])\n            else:\n                x_y = "", %d,%d"" % (data[i][0], data[i][1])\n            f.write(x_y)\n        f.close()\n\n    def txt2boxes(self):\n        train_dataset_builder = Dataset(self.glob_path,\n                                        1,\n                                        mode=DATASET_MODE.TEST)\n        bind(train_dataset_builder, self.parse_tfrecord)\n        bind(train_dataset_builder, self.parse_text)\n        train_dataset, train_num = train_dataset_builder.build()\n        result = []\n        for xmins, xmaxs, ymins, ymaxs in train_dataset:\n            width = xmaxs - xmins\n            height = ymaxs - ymins\n            wh = np.transpose(np.concatenate([width, height], 0))\n            result.append(wh)\n        return np.concatenate(result, 0).astype(np.int32)\n\n    def txt2clusters(self):\n        all_boxes = self.txt2boxes()\n        plt.scatter(all_boxes[:1000, 0], all_boxes[:1000, 1], c=\'r\')\n        result = self.kmeans(all_boxes, k=self.cluster_number)\n        result = result[np.lexsort(result.T[0, None])]\n        plt.scatter(result[:, 0], result[:, 1], c=\'b\')\n        plt.show()\n        self.result2txt(result)\n        print(""K anchors:\\n {}"".format(result))\n        print(""Accuracy: {:.2f}%"".format(self.avg_iou(all_boxes, result) * 100))\n\n\nif __name__ == ""__main__"":\n    cluster_number = 9\n    glob = ""../pascal/VOCdevkit/train/*2007*.tfrecords""\n    kmeans = YOLO_Kmeans(cluster_number, glob)\n    kmeans.txt2clusters()\n'"
main.py,10,"b'from absl import app, flags, logging\nimport tensorflow as tf\ntf.compat.v1.enable_eager_execution()\ntf.compat.v1.enable_v2_behavior()\ntf.compat.v1.enable_v2_tensorshape()\nfrom yolo3.enums import BACKBONE, MODE, OPT\nfrom train import train\nfrom train_backbone import train as train_backbone\nfrom yolo import YOLO, detect_video, detect_img, export_tflite_model, export_serving_model, calculate_map, export_tfjs_model\n\nFLAGS = flags.FLAGS\n\nflags.DEFINE_enum_class(\n    \'backbone\',\n    default=BACKBONE.MOBILENETV2,\n    enum_class=BACKBONE,\n    help=\n    ""Select network backbone, One of {\'MOBILENETV2\',\'DARKNET53\',\'EFFICIENTNET\'}""\n)\nflags.DEFINE_integer(\'batch_size\',\n                     default=8,\n                     lower_bound=0,\n                     help=""Train batch size"")\nflags.DEFINE_string(\'config\', default=None, help=""Config path"")\nflags.DEFINE_multi_integer(\'epochs\',\n                           default=[10, 10],\n                           lower_bound=0,\n                           help=""Frozen train epochs and Full train epochs"")\nflags.DEFINE_string(\'export\', default=\'export_model/8\', help=""Export path"")\nflags.DEFINE_string(\'input\', default=None, help=""Input data for various mode"")\nflags.DEFINE_multi_integer(\'input_size\',\n                           default=(380, 380),\n                           lower_bound=0,\n                           help=""Input size"")\nflags.DEFINE_string(\'log_directory\', default=None, help=""Log directory"")\nflags.DEFINE_string(\n    \'model\',\n    default=None,\n    help=""Model path"")\nflags.DEFINE_enum_class(\n    \'mode\',\n    default=MODE.TRAIN,\n    enum_class=MODE,\n    help=\n    ""Select exec mode, One of {\'TRAIN\',\'TRAIN_BACKBONE\',\'IMAGE\',\'VIDEO\',\'TFLITE\',\'SERVING\',\'MAP\',\'PRUNE\'}""\n)\n\nflags.DEFINE_multi_integer(\'gpus\', default=[0,1], help=""Specific gpu indexes to run"")\nflags.DEFINE_string(\'train_dataset\',\n                    default=\'/usr/local/srv/tfrecords/train/*2007*.tfrecords\',\n                    help=""Dataset glob for train"")\nflags.DEFINE_string(\'val_dataset\',\n                    default=\'/usr/local/srv/tfrecords/val/*2007*.tfrecords\',\n                    help=""Dataset glob for validate"")\nflags.DEFINE_string(\'test_dataset\',\n                    default=\'/usr/local/srv/tfrecords/test/*2007*.tfrecords\',\n                    help=""Dataset glob for test"")\nflags.DEFINE_string(\'anchors_path\',\n                    default=\'model_data/yolo_anchors.txt\',\n                    help=""Anchors path"")\nflags.DEFINE_string(\'classes_path\',\n                    default=\'model_data/voc_classes.txt\',\n                    help=""Classes Path"")\nflags.DEFINE_multi_float(\'learning_rate\',\n                         default=[1e-3, 1e-4],\n                         lower_bound=0,\n                         help=""Learning rate"")\nflags.DEFINE_enum_class(\n    \'opt\',\n    default=None,\n    enum_class=OPT,\n    help=""Select optimization, One of {\'XLA\',\'DEBUG\',\'MKL\'}"")\nflags.DEFINE_string(\'tpu_address\', default=None, help=""TPU address"")\nflags.DEFINE_bool(\'freeze\', default=False,help=""Whether freeze backbone"")\nflags.DEFINE_bool(\'prune\', default=False, help=""Whether prune model"")\n\n\ndef parse_tuple(val):\n    if isinstance(val, str):\n        return tuple([int(num) for num in val[1:-1].split(\',\')])\n    return tuple(val)\n\ndef log(msg):\n    logging.info(msg)\n\ndef get_gpu_name(valid_gpus):\n    return [\':\'.join(gpu.name.split(\':\')[1:]) for gpu in valid_gpus]\n\ndef main(_):\n    flags_dict = FLAGS.flag_values_dict()\n    if FLAGS.config is not None:\n        import yaml\n        with open(FLAGS.config) as stream:\n            config = yaml.safe_load(stream)\n            if \'backbone\' in config:\n                config[\'backbone\'] = BACKBONE[config[\'backbone\']]\n            if \'opt\' in config:\n                config[\'opt\'] = OPT[config[\'opt\']]\n            if \'input_size\' in config:\n                if isinstance(config[\'input_size\'], str):\n                    config[\'input_size\'] = parse_tuple(config[\'input_size\'])\n                elif isinstance(config[\'input_size\'], list):\n                    config[\'input_size\'] = [\n                        parse_tuple(size) for size in config[\'input_size\']\n                    ]\n                else:\n                    raise ValueError(\n                        \'Please use array or tuple to define input_size\')\n            if \'learning_rate\' in config:\n                config[\'learning_rate\'] = [\n                    float(lr) for lr in config[\'learning_rate\']\n                ]\n            flags_dict.update(config)\n\n    opt = flags_dict.get(\'opt\', None)\n    if opt == OPT.XLA:\n        tf.config.optimizer.set_jit(True)\n    elif opt == OPT.DEBUG:\n        tf.compat.v2.random.set_seed(111111);\n        tf.debugging.set_log_device_placement(True)\n        tf.config.experimental_run_functions_eagerly(True)\n        logging.set_verbosity(logging.DEBUG)\n\n    gpus = tf.config.experimental.list_physical_devices(\'GPU\')\n    if gpus:\n        gpu_indexs=[int(gpu.name.split(\':\')[-1]) for gpu in gpus]\n        valid_gpu_indexs=list(filter(lambda gpu: gpu in flags_dict[\'gpus\'],gpu_indexs))\n        valid_gpus=[gpus[index] for index in valid_gpu_indexs]\n        tf.config.experimental.set_visible_devices(valid_gpus, \'GPU\')\n        flags_dict[\'gpus\']=get_gpu_name(valid_gpus)\n    if flags_dict[\'backbone\'] is None:\n        raise ValueError(""Please select your model\'s backbone"")\n    if FLAGS.mode == MODE.TRAIN:\n        log(\'Train mode\')\n        train(flags_dict)\n    elif FLAGS.mode == MODE.TRAIN_BACKBONE:\n        log(\'Train backbone mode\')\n        train_backbone(flags_dict)\n    elif FLAGS.mode == MODE.IMAGE:\n        if flags_dict[\'model\'] is None:\n            raise ValueError(\'Please enter your model path\')\n        log(\'Image detection mode\')\n        detect_img(YOLO(flags_dict))\n    elif FLAGS.mode == MODE.VIDEO:\n        if flags_dict[\'model\'] is None:\n            raise ValueError(\'Please enter your model path\')\n        log(\'Video detection mode\')\n        detect_video(YOLO(flags_dict), FLAGS.input, FLAGS.output)\n    elif FLAGS.mode == MODE.MAP:\n        if flags_dict[\'model\'] is None:\n            raise ValueError(\'Please enter your model path\')\n        log(\'Calculate test dataset map\')\n        flags_dict[\'score\']=0.0\n        calculate_map(YOLO(flags_dict), FLAGS.test_dataset)\n    elif FLAGS.mode == MODE.SERVING:\n        tf.disable_eager_execution()\n        log(\'Export hdf5 model to serving model\')\n        export_serving_model(YOLO(flags_dict), FLAGS.export)\n    elif FLAGS.mode == MODE.TFLITE:\n        log(\'Export hdf5 model to tflite model\')\n        export_tflite_model(YOLO(flags_dict), FLAGS.export)\n    elif FLAGS.mode == MODE.TFJS:\n        log(\'Export hdf5 model to tensorflow.js model\')\n        export_tfjs_model(YOLO(flags_dict), FLAGS.export)\n\n\nif __name__ == \'__main__\':\n    logging.set_verbosity(logging.INFO)\n    app.run(main)\n'"
train.py,17,"b'""""""\nRetrain the YOLO model for your own dataset.\n""""""\nimport tensorflow as tf\nimport datetime\nfrom yolo3.model import darknet_yolo_body, YoloLoss, mobilenetv2_yolo_body, efficientnet_yolo_body\nfrom yolo3.data import Dataset\nfrom yolo3.enums import BACKBONE, DATASET_MODE\nfrom yolo3.map import MAPCallback\nfrom yolo3.utils import get_anchors, get_classes, ModelFactory\nimport os\nimport numpy as np\nimport neural_structured_learning as nsl\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\ntf.keras.backend.set_learning_phase(1)\n\n\ndef train(FLAGS):\n    """"""Train yolov3 with different backbone\n    """"""\n    prune = FLAGS[\'prune\']\n    opt = FLAGS[\'opt\']\n    backbone = FLAGS[\'backbone\']\n    log_dir = os.path.join(\n        \'logs\',\n        str(backbone).split(\'.\')[1].lower() + \'_\' + str(datetime.date.today()))\n\n    batch_size = FLAGS[\'batch_size\']\n    train_dataset_glob = FLAGS[\'train_dataset\']\n    val_dataset_glob = FLAGS[\'val_dataset\']\n    test_dataset_glob = FLAGS[\'test_dataset\']\n    freeze = FLAGS[\'freeze\']\n    epochs = FLAGS[\'epochs\'][0] if freeze else FLAGS[\'epochs\'][1]\n\n    class_names = get_classes(FLAGS[\'classes_path\'])\n    num_classes = len(class_names)\n    anchors = get_anchors(FLAGS[\'anchors_path\'])\n    input_shape = FLAGS[\'input_size\']  # multiple of 32, hw\n    model_path = FLAGS[\'model\']\n    if model_path and model_path.endswith(\n            \'.h5\') is not True:\n        model_path = tf.train.latest_checkpoint(model_path)\n    lr = FLAGS[\'learning_rate\']\n    tpu_address=FLAGS[\'tpu_address\']\n    if tpu_address is not None:\n        cluster_resolver=tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_address)\n        tf.config.experimental_connect_to_host(cluster_resolver.master())\n        tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n        strategy=tf.distribute.experimental.TPUStrategy(cluster_resolver)\n    else:\n        strategy = tf.distribute.MirroredStrategy(devices=FLAGS[\'gpus\'])\n    batch_size = batch_size * strategy.num_replicas_in_sync\n\n    train_dataset_builder = Dataset(train_dataset_glob, batch_size, anchors,\n                                     num_classes, input_shape)\n    train_dataset, train_num = train_dataset_builder.build(epochs)\n    val_dataset_builder = Dataset(val_dataset_glob,\n                                  batch_size,\n                                  anchors,\n                                  num_classes,\n                                  input_shape,\n                                  mode=DATASET_MODE.VALIDATE)\n    val_dataset, val_num = val_dataset_builder.build(epochs)\n    map_callback = MAPCallback(test_dataset_glob, input_shape, anchors,\n                               class_names)\n    tensorboard = tf.keras.callbacks.TensorBoard(write_graph=False,\n                                             log_dir=log_dir,\n                                             write_images=True)\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(os.path.join(\n        log_dir, \'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5\'),\n                                                    monitor=\'val_loss\',\n                                                    save_weights_only=True,\n                                                    save_best_only=True,\n                                                    period=3)\n    cos_lr = tf.keras.callbacks.LearningRateScheduler(\n        lambda epoch, _: tf.keras.experimental.CosineDecay(lr[1], epochs)(\n            epoch).numpy(),1)\n    early_stopping = tf.keras.callbacks.EarlyStopping(\n        monitor=\'val_loss\',\n        min_delta=0,\n        patience=epochs // 5,\n        verbose=1)\n\n    loss = [YoloLoss(idx, anchors, print_loss=False) for idx in range(len(anchors) // 3)]\n\n    adv_config = nsl.configs.make_adv_reg_config(\n        multiplier=0.2, adv_step_size=0.2, adv_grad_norm=\'infinity\')\n    train_dataset = strategy.experimental_distribute_dataset(train_dataset)\n    val_dataset = strategy.experimental_distribute_dataset(val_dataset)\n\n    with strategy.scope():\n        factory = ModelFactory(tf.keras.layers.Input(shape=(*input_shape, 3)),\n                               weights_path=model_path)\n        if backbone == BACKBONE.MOBILENETV2:\n            model = factory.build(mobilenetv2_yolo_body,\n                                  155,\n                                  len(anchors) // 3,\n                                  num_classes,\n                                  alpha=FLAGS[\'alpha\'])\n        elif backbone == BACKBONE.DARKNET53:\n            model = factory.build(darknet_yolo_body, 185,\n                                  len(anchors) // 3, num_classes)\n        elif backbone == BACKBONE.EFFICIENTNET:\n            model = factory.build(efficientnet_yolo_body,\n                                  499,\n                                  FLAGS[\'model_name\'],\n                                  len(anchors) // 3,\n                                  batch_norm_momentum=0.9,\n                                  batch_norm_epsilon=1e-3,\n                                  num_classes=num_classes,\n                                  drop_connect_rate=0.2,\n                                  data_format=""channels_first"")\n\n    # Train with frozen layers first, to get a stable loss.\n    # Adjust num epochs to your dataset. This step is enough to obtain a not bad model.\n    if freeze is True:\n        with strategy.scope():\n            model.compile(optimizer=tf.keras.optimizers.Adam(lr[0],\n                                                             epsilon=1e-8),\n                          loss=loss)\n        model.fit(epochs, [checkpoint, tensorboard, tf.keras.callbacks.LearningRateScheduler(\n        (lambda _, lr:lr),1)], train_dataset, val_dataset)\n        model.save_weights(\n            os.path.join(\n                log_dir,\n                str(backbone).split(\'.\')[1].lower() +\n                \'_trained_weights_stage_1.h5\'))\n    # Unfreeze and continue training, to fine-tune.\n    # Train longer if the result is not good.\n    else:\n        for i in range(len(model.layers)):\n            model.layers[i].trainable = True\n        with strategy.scope():\n            model.compile(optimizer=tf.keras.optimizers.Adam(lr[1],\n                                                             epsilon=1e-8),\n                          loss=loss)  # recompile to apply the change\n        print(\'Unfreeze all of the layers.\')\n        model.fit(epochs, [checkpoint, cos_lr, tensorboard, early_stopping], train_dataset,\n                      val_dataset,use_adv=False)\n        model.save_weights(\n            os.path.join(\n                log_dir,\n                str(backbone).split(\'.\')[1].lower() +\n                \'_trained_weights_final.h5\'))\n\n    # Further training if needed.\n'"
train_backbone.py,20,"b'import tensorflow as tf\nfrom yolo3.data import Dataset\nfrom yolo3.override import mobilenet_v2\nfrom yolo3.darknet import darknet_body\nfrom yolo3.efficientnet import EfficientNetB4\nfrom yolo3.utils import get_classes, ModelFactory\nfrom yolo3.enum import BACKBONE\nimport os\nimport datetime\n\n\nclass BackboneDataset(Dataset):\n    """"""Backbone\'s Dataset extends Dataset,only support txt files now.\n    """"""\n    def parse_tfrecord(self, example_proto):\n        pass\n\n    def parse_text(self, line):\n        values = tf.strings.split([line], \' \').values\n        image = tf.image.decode_image(tf.io.read_file(values[0]),\n                                      channels=3,\n                                      dtype=tf.float32)\n        image.set_shape([None, None, 3])\n        image = tf.image.random_brightness(image, 0.1)\n        image = tf.image.random_hue(image, 0.1)\n        image = tf.image.random_flip_left_right(image)\n        image = tf.image.resize(image, self.input_shape)\n        label = tf.strings.to_number(values[1], tf.int64)\n        return image, label\n\n\ndef mobilenetv2(inputs, alpha, classes):\n    """"""MobilenetV2 wrapper function\n    \n    Arguments:\n        inputs {np.array} -- [train images]\n        alpha {float} -- [controls the width of the network. This is known as the\n        width multiplier in the MobileNetV2 paper.\n            - If `alpha` < 1.0, proportionally decreases the number\n                of filters in each layer.\n            - If `alpha` > 1.0, proportionally increases the number\n                of filters in each layer.\n            - If `alpha` = 1, default number of filters from the paper\n                 are used at each layer.]\n        classes {int} -- [classes total number]\n    \n    Returns:\n        [tf.keras.Model] -- [mobilenetv2 model]\n    """"""\n    return mobilenet_v2(default_batchnorm_momentum=0.9,\n                        alpha=alpha,\n                        input_tensor=inputs,\n                        classes=classes)\n\n\ndef EfficientNet(inputs, classes, input_shape):\n    return EfficientNetB4(classes=classes,\n                          input_shape=input_shape,\n                          input_tensor=inputs)\n\n\ndef train(FLAGS):\n    batch_size = FLAGS[\'batch_size\']\n    use_tpu = FLAGS[\'use_tpu\']\n    class_names = get_classes(FLAGS[\'classes_path\'])\n    epochs = FLAGS[\'epochs\'][0]\n    input_size = FLAGS[\'input_size\']\n    model_path = FLAGS[\'model\']\n    backbone = FLAGS[\'backbone\']\n    train_dataset_glob = FLAGS[\'train_dataset\']\n    val_dataset_glob = FLAGS[\'val_dataset\']\n    log_dir = FLAGS[\'log_directory\'] or os.path.join(\n        \'logs\',\n        str(backbone).split(\'.\')[1].lower() + str(datetime.date.today()))\n    strategy = tf.distribute.MirroredStrategy()\n    batch_size = batch_size * strategy.num_replicas_in_sync\n    with strategy.scope():\n        factory = ModelFactory(weights_path=model_path)\n        if backbone == BACKBONE.MOBILENETV2:\n            model = factory.build(mobilenetv2,\n                                  0,\n                                  alpha=1.4,\n                                  classes=len(class_names))\n        elif backbone == BACKBONE.DARKNET53:\n            model = factory.build(darknet_body, 0, classes=len(class_names))\n        elif backbone == BACKBONE.EFFICIENTNET:\n            model = factory.build(EfficientNet,\n                                  0,\n                                  classes=len(class_names),\n                                  input_shape=(*input_size, 3))\n        model.compile(tf.keras.optimizers.Adam(1e-3),\n                      loss=tf.keras.losses.sparse_categorical_crossentropy,\n                      metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n    if use_tpu:\n        tpu = tf.contrib.cluster_resolver.TPUClusterResolver()\n        tpu_strategy = tf.contrib.tpu.TPUDistributionStrategy(tpu)\n        model = tf.contrib.tpu.keras_to_tpu_model(model, strategy=tpu_strategy)\n\n    train_dataset, train_num = BackboneDataset(train_dataset_glob,\n                                               batch_size,\n                                               num_classes=len(class_names),\n                                               input_shapes=input_size).build()\n    val_dataset, val_num = BackboneDataset(val_dataset_glob,\n                                           batch_size,\n                                           num_classes=len(class_names),\n                                           input_shapes=input_size).build()\n\n    cos_lr = tf.keras.callbacks.LearningRateScheduler(\n        lambda epoch, _: tf.train.cosine_decay(1e-3, epoch, epochs)().numpy(),\n        1)\n    logging = tf.keras.callbacks.TensorBoard(log_dir=log_dir, write_images=True)\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(\n        log_dir, \'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5\'),\n                                                    save_weights_only=True,\n                                                    verbose=1,\n                                                    period=3)\n    model.fit(train_dataset,\n              epochs=epochs,\n              steps_per_epoch=max(1, train_num // batch_size),\n              validation_data=val_dataset,\n              validation_steps=max(1, val_num // batch_size),\n              callbacks=[cos_lr, logging, checkpoint])\n    model.save_weights(\n        os.path.join(\n            log_dir,\n            str(backbone).split(\'.\')[1].lower() + \'_trained_weights.h5\'))\n'"
voc_annotation.py,17,"b'import xml.etree.ElementTree as ET\nimport tensorflow as tf\nfrom os import path\nimport numpy as np\n\nclasses = [""aeroplane"", ""bicycle"", ""bird"", ""boat"", ""bottle"", ""bus"", ""car"", ""cat"", ""chair"", ""cow"", ""diningtable"", ""dog"", ""horse"", ""motorbike"", ""person"", ""pottedplant"", ""sheep"", ""sofa"", ""train"", ""tvmonitor""]\ntfrecords_size = 1000\n\n\ndef convert_to_tfrecord(xml, record_writer):\n    name, _ = xml.split(\'/\')[-1].split(\'.\')\n    root = ET.parse(xml.encode(\'utf-8\')).getroot()\n    xmins = []\n    ymins = []\n    xmaxs = []\n    ymaxs = []\n    labels = []\n    for obj in root.iter(\'object\'):\n        difficult = obj.find(\'difficult\').text\n        cls = obj.find(\'name\').text\n        if cls not in classes or int(difficult) == 1:\n            continue\n        cls_id = classes.index(cls)\n        xmlbox = obj.find(\'bndbox\')\n        xmins.append(float(xmlbox.find(\'xmin\').text))\n        ymins.append(float(xmlbox.find(\'ymin\').text))\n        xmaxs.append(float(xmlbox.find(\'xmax\').text))\n        ymaxs.append(float(xmlbox.find(\'ymax\').text))\n        labels.append(int(cls_id))\n\n    image_data = tf.io.read_file(\n        tf.io.gfile.glob(\'%s/%s/**/%s.jp*g\' % (clazz, file, name))[0])\n    example = tf.train.Example(features=tf.train.Features(\n        feature={\n            \'image/encoded\':\n            tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_data])),\n            \'image/object/bbox/name\':\n            tf.train.Feature(bytes_list=tf.train.BytesList(value=[name])),\n            \'image/object/bbox/xmin\':\n            tf.train.Feature(float_list=tf.train.FloatList(value=xmins)),\n            \'image/object/bbox/xmax\':\n            tf.train.Feature(float_list=tf.train.FloatList(value=xmaxs)),\n            \'image/object/bbox/ymin\':\n            tf.train.Feature(float_list=tf.train.FloatList(value=ymins)),\n            \'image/object/bbox/ymax\':\n            tf.train.Feature(float_list=tf.train.FloatList(value=ymaxs)),\n            \'image/object/bbox/label\':\n            tf.train.Feature(float_list=tf.train.FloatList(value=labels))\n        }))\n    record_writer.write(example.SerializeToString())\n\n\nfor clazz in classes:\n    index_records = 1\n    num = 1\n    record_writer = tf.io.TFRecordWriter(\n        path.join(\'./\', \'cci_%d_%s.tfrecords\' % (index_records, clazz)))\n\n    for file in tf.io.gfile.listdir(clazz):\n        if tf.io.gfile.isdir(\'%s/%s\' % (clazz, file)):\n            xmls = tf.io.gfile.glob(\'%s/%s/**/*.xml\' % (clazz, file))\n            np.random.shuffle(xmls)\n            for xml in xmls:\n                if num >= tfrecords_size:\n                    tf.io.gfile.rename(\n                        \'cci_%d_%s.tfrecords\' % (index_records, clazz),\n                        \'cci_%d_%s_%d.tfrecords\' % (index_records, clazz, num))\n                    index_records += 1\n                    num = 1\n                    record_writer.close()\n                    record_writer = tf.io.TFRecordWriter(\n                        path.join(\n                            \'./\',\n                            \'cci_%d_%s.tfrecords\' % (index_records, clazz)))\n                convert_to_tfrecord(xml, record_writer)\n                num += 1\n            tf.io.gfile.rename(\n                \'cci_%d_%s.tfrecords\' % (index_records, clazz),\n                \'cci_%d_%s_%d.tfrecords\' % (index_records, clazz, num))\n\n            record_writer.close()\n\n'"
voc_text.py,3,"b'import tensorflow as tf\nimport argparse\nimport xml.etree.ElementTree as ET\nimport os\nimport threading\nimport glob\nfrom io import StringIO\nimport shutil\nos.environ[""CUDA_VISIBLE_DEVICES""] = """"\nif hasattr(tf, \'enable_eager_execution\'):\n    tf.enable_eager_execution()\n\nparser=argparse.ArgumentParser(argument_default=argparse.SUPPRESS)\n\nparser.add_argument(\'--folder\',type=str,help=\'parse folder path\',default=\'./\')\nthreads=4\n\nFLAGS = parser.parse_args()\nxmls=glob.glob(os.path.join(FLAGS.folder,\'**/*.xml\'),recursive=True)\nchunk_num=len(xmls)//threads\nclasses = [""aeroplane"", ""bicycle"", ""bird"", ""boat"", ""bottle"", ""bus"", ""car"", ""cat"", ""chair"", ""cow"", ""diningtable"", ""dog"", ""horse"", ""motorbike"", ""person"", ""pottedplant"", ""sheep"", ""sofa"", ""train"", ""tvmonitor""]\ntrain_file = StringIO()\nval_file = StringIO()\ntest_file = StringIO()\nnums=[0,0,0]\nsplits=[0.8,0.9,1]\nmutex = threading.Lock()\ndef write_xmls(xmls):\n    for xml_path in xmls:\n        image_path=glob.glob(os.path.join(\'/\'.join(xml_path.split(\'/\')[:-2]),\'**\',xml_path.split(\'/\')[-1].split(\'.\')[0]+\'.jp*g\'),recursive=True)[0]\n        if tf.io.gfile.exists(image_path) is not True:\n            continue\n        rand = tf.random.uniform([],0,1)\n        xml_root=ET.parse(xml_path).getroot()\n        if rand < splits[0]:\n            file=train_file\n            index=0\n        elif rand >= splits[0] and rand < splits[1]:\n            file=val_file\n            index = 1\n        else:\n            file=test_file\n            index = 2\n        label=image_path\n        objects=0\n        for obj in xml_root.iter(\'object\'):\n            difficult = obj.find(\'difficult\').text\n            cls = obj.find(\'name\').text\n            if cls not in classes or int(difficult) == 1:\n                continue\n            cls_id = classes.index(cls)\n            xmlbox = obj.find(\'bndbox\')\n            b = (int(xmlbox.find(\'xmin\').text), int(xmlbox.find(\'ymin\').text), int(xmlbox.find(\'xmax\').text),\n                 int(xmlbox.find(\'ymax\').text))\n            label+=\' \'+\' \'.join([str(a) for a in b])+\' \'+ str(cls_id)\n            objects+=1\n        if objects<=0:\n            continue\n        label+=\'\\n\'\n        mutex.acquire()\n        file.write(label)\n        nums[index]+=1\n        mutex.release()\npool=[]\nfor idx in range(threads):\n    thread = threading.Thread(target=write_xmls, args=[xmls[idx*chunk_num:(idx+1)*chunk_num]])\n    pool.append(thread)\nfor thread in pool:\n    thread.start()\nfor thread in pool:\n    thread.join()\ntrain_file.close()\nval_file.close()\ntest_file.close()\nwith open (\'voc_train_\'+str(nums[0])+\'.txt\', \'w\') as fd:\n    train_file.seek(0)\n    shutil.copyfileobj(train_file, fd)\nwith open (\'voc_val_\'+str(nums[1])+\'.txt\', \'w\') as fd:\n    val_file.seek(0)\n    shutil.copyfileobj(val_file, fd)\nwith open (\'voc_test_\'+str(nums[2])+\'.txt\', \'w\') as fd:\n    test_file.seek(0)\n    shutil.copyfileobj(test_file, fd)\n'"
yolo.py,27,"b'# -*- coding: utf-8 -*-\n""""""Class definition of YOLO_v3 style detection model on image and video.""""""\n\nimport colorsys\nfrom timeit import default_timer as timer\nimport numpy as np\nfrom PIL import Image, ImageFont, ImageDraw\nimport cv2\nimport tensorflow as tf\nfrom yolo3.model import darknet_yolo_body, mobilenetv2_yolo_body, efficientnet_yolo_body, YoloEval\nfrom yolo3.utils import letterbox_image, get_anchors, get_classes\nfrom yolo3.enums import BACKBONE\nfrom yolo3.map import MAPCallback\nimport os\nfrom typing import List, Tuple\nfrom tensorflow_serving.apis import prediction_log_pb2, predict_pb2\nfrom functools import partial\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\n\ntf.keras.backend.set_learning_phase(0)\n\n\nclass YoloModel(tf.keras.Model):\n    def __init__(self,\n                 model_body,\n                 num_anchors,\n                 classes,\n                 model_path,\n                 anchors,\n                 input_shape,\n                 score=0.2,\n                 nms=0.5,\n                 with_classes=False,\n                 name=None,\n                 **kwargs):\n        super(YoloModel, self).__init__(name=name, **kwargs)\n        self.model_body = model_body\n        self.num_anchors = num_anchors\n        self.classes = classes\n        self.with_classes = with_classes\n        self.num_classes = len(classes)\n        self.model_path = model_path\n        self.anchors = anchors\n        self.score = score\n        self.nms = nms\n        self.input_shapes = input_shape\n        self.model = self.model_body(\n            tf.keras.layers.Input(\n                shape=[*input_shape, 3], batch_size=1, dtype=tf.float32),\n            num_anchors=self.num_anchors // 3,\n            num_classes=self.num_classes)\n        self.model.load_weights(self.model_path)\n        self.yolo_eval=YoloEval(\n            self.anchors,\n            self.num_classes,\n            score_threshold=self.score,\n            iou_threshold=self.nms,\n            name=\'yolo\')\n\n    def parse_image(self, image):\n        decoded_image = tf.io.decode_image(image, channels=3, dtype=tf.float32)\n        decoded_image.set_shape([None, None, 3])\n        letterboxed_image = letterbox_image(decoded_image,\n                                               self.input_shapes)\n        return decoded_image, letterboxed_image\n\n    @tf.function(input_signature=[\n        tf.TensorSpec(shape=(1), dtype=tf.string, name=\'predict_image\')\n    ])\n    def call(self, input):\n        decoded_image, input_image = self.parse_image(input[0])\n        decoded_image_shape = tf.shape(decoded_image)[0:2]\n        input_image = tf.reshape(input_image, [-1, *self.input_shapes, 3])\n        input_image = tf.cast(input_image, tf.float32)\n        out_boxes, out_scores, out_classes = self.yolo_eval(self.model(input_image),decoded_image_shape)\n        if self.with_classes:\n            out_classes = tf.gather(self.classes, out_classes)\n        return out_boxes, out_scores, out_classes\n\n\nclass YOLO(object):\n    def __init__(self, FLAGS):\n        self.backbone = FLAGS.get(\'backbone\', BACKBONE.MOBILENETV2)\n        self.class_names = get_classes(\n            FLAGS.get(\'classes_path\', \'model_data/voc_classes.txt\'))\n        self.anchors = get_anchors(\n            FLAGS.get(\'anchors_path\', \'model_data/yolo_anchors\'))\n        self.input_shape = FLAGS.get(\'input_size\', (416, 416))\n        self.score = FLAGS.get(\'score\', 0.2)\n        self.nms = FLAGS.get(\'nms\', 0.5)\n        self.with_classes = FLAGS.get(\'with_classes\', False)\n\n        self.generate(FLAGS)\n\n    def generate(self, FLAGS):\n        model_path = os.path.expanduser(FLAGS[\'model\'])\n        if model_path.endswith(\'.h5\') is not True:\n            model_path = tf.train.latest_checkpoint(model_path)\n\n        # Load model, or construct model and load weights.\n        num_anchors = len(self.anchors)\n        num_classes = len(self.class_names)\n        try:\n            self.yolo_model = tf.keras.models.load_model(\n                model_path, compile=False)\n        except:\n            if self.backbone == BACKBONE.MOBILENETV2:\n                model_body = partial(\n                    mobilenetv2_yolo_body, alpha=FLAGS.get(\'alpha\', 1.4))\n            elif self.backbone == BACKBONE.DARKNET53:\n                model_body = darknet_yolo_body\n            elif self.backbone == BACKBONE.EFFICIENTNET:\n                model_body = partial(\n                    efficientnet_yolo_body,\n                    num_anchors=num_anchors // 3,\n                    num_classes=num_classes,\n                    drop_rate=0.2,\n                    data_format=""channels_last"")\n\n            self.yolo_model = YoloModel(\n                model_body, num_anchors, self.class_names, model_path,\n                self.anchors, self.input_shape, self.score, self.nms,\n                self.with_classes)\n\n        else:\n            assert self.yolo_model.layers[-1].output_shape[-1] == \\\n                   num_anchors / len(self.yolo_model.output) * (num_classes + 5), \\\n                \'Mismatch between model and given anchor and class sizes\'\n        print(\'{} model, anchors, and classes loaded.\'.format(model_path))\n        # Generate output tensor targets for filtered bounding boxes.\n        hsv_tuples: List[Tuple[float, float, float]] = [\n            (x / len(self.class_names), 1., 1.)\n            for x in range(len(self.class_names))\n        ]\n        self.colors: List[Tuple[float, float, float]] = list(\n            map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n        self.colors: List[Tuple[int, int, int]] = list(\n            map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)),\n                self.colors))\n        np.random.seed(10101)  # Fixed seed for consistent colors across runs.\n        np.random.shuffle(\n            self.colors)  # Shuffle colors to decorrelate adjacent classes.\n        np.random.seed(None)  # Reset seed to default.\n\n    def detect_image(self, image, draw=True):\n        image_data = image\n        if isinstance(image, bytes) is False:\n            image_data = image.read()\n        start = timer()\n        out_boxes, out_scores, out_classes = self.yolo_model([image_data])\n        if tf.executing_eagerly():\n            out_boxes = out_boxes.numpy()\n            out_scores = out_scores.numpy()\n            out_classes = out_classes.numpy()\n        else:\n            start = timer()\n            out_boxes, out_scores, out_classes = tf.compat.v1.keras.backend.get_session(\n            ).run([out_boxes, out_scores, out_classes])\n        end = timer()\n\n        print(\'Found {} boxes for {}\'.format(len(out_boxes), \'img\'))\n        if draw:\n            image = Image.open(image)\n            font = ImageFont.truetype(\n                font=\'font/FiraMono-Medium.otf\',\n                size=np.floor(3e-2 * image.size[1] + 0.5).astype(\'int32\'))\n            thickness = (image.size[1] + image.size[0]) // 300\n            draw = ImageDraw.Draw(image)\n            for i, c in reversed(list(enumerate(out_classes))):\n                if self.with_classes:\n                    c = self.class_names.index(str(c, encoding=""utf-8""))\n                predicted_class = self.class_names[c]\n                box = out_boxes[i]\n                score = out_scores[i]\n\n                label = \'{} {:.2f}\'.format(predicted_class, score)\n\n                label_size = draw.textsize(label, font)\n\n                top, left, bottom, right = box\n                print(label, (left, top), (right, bottom))\n\n                if top - label_size[1] >= 0:\n                    text_origin = np.array([left, top - label_size[1]])\n                else:\n                    text_origin = np.array([left, top + 1])\n\n                # My kingdom for a good redistributable image drawing library.\n                for i in range(thickness):\n                    draw.rectangle([left + i, top + i, right - i, bottom - i],\n                                   outline=self.colors[c])\n                draw.rectangle(\n                    [tuple(text_origin),\n                     tuple(text_origin + label_size)],\n                    fill=self.colors[c])\n                draw.text(text_origin, label, fill=(0, 0, 0), font=font)\n            del draw\n            print(end - start)\n            return image\n        else:\n            return out_boxes, out_scores, out_classes\n\n\ndef overwrite_path(path):\n    if tf.io.gfile.exists(path):\n        while True:\n            overwrite = input(""Overwrite existed model(yes/no):"")\n            if overwrite == \'yes\':\n                tf.io.gfile.rmtree(path)\n                break\n            elif overwrite == \'no\':\n                raise ValueError(\n                    ""Export directory already exists, and isn\'t empty. Please choose a different export directory, or delete all the contents of the specified directory: ""\n                    + path)\n            else:\n                print(\'Please input yes/no\')\n\n\ndef export_tfjs_model(yolo, path):\n    import tensorflowjs as tfjs\n    import tempfile\n    overwrite_path(path)\n\n    temp_savedmodel_dir = tempfile.mktemp(suffix=\'.savedmodel\')\n    tf.keras.experimental.export_saved_model(\n        yolo.yolo_model, temp_savedmodel_dir, serving_only=True)\n\n    tfjs.converters.tf_saved_model_conversion_v2.convert_tf_saved_model(\n        temp_savedmodel_dir,\n        path,\n        signature_def=\'serving_default\',\n        saved_model_tags=\'serve\')\n    # tfjs.converters.save_keras_model(yolo.yolo_model,\n    #                                  path)\n\n\ndef export_serving_model(yolo, path, warmup_path=None,with_tensorrt=False):\n    overwrite_path(path)\n    tf.saved_model.save(yolo.yolo_model, path)\n    if with_tensorrt:\n        params=trt.TrtConversionParams(\n            rewriter_config_template=None,\n            max_workspace_size_bytes=trt.DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,\n            precision_mode=trt.TrtPrecisionMode.FP16,\n            minimum_segment_size=3,\n            is_dynamic_op=True,\n            maximum_cached_engines=1,\n            use_calibration=True,\n            max_batch_size=1)\n        converter = trt.TrtGraphConverterV2(input_saved_model_dir=path,conversion_params=params)\n        converter.convert()\n        tf.io.gfile.rmtree(path)\n        converter.save(path)\n    asset_extra = os.path.join(path, ""assets.extra"")\n    tf.io.gfile.mkdir(asset_extra)\n    with tf.io.TFRecordWriter(\n            os.path.join(asset_extra, ""tf_serving_warmup_requests"")) as writer:\n        request = predict_pb2.PredictRequest()\n        request.model_spec.name = \'detection\'\n        request.model_spec.signature_name = \'serving_default\'\n        if warmup_path is None:\n            warmup_path = input(\'Please enter warm up image path:\')\n        image = open(warmup_path, \'rb\').read()\n        image_data = np.expand_dims(image, 0)\n        request.inputs[\'predict_image\'].CopyFrom(\n            tf.compat.v1.make_tensor_proto(image_data))\n        log = prediction_log_pb2.PredictionLog(\n            predict_log=prediction_log_pb2.PredictLog(request=request))\n        writer.write(log.SerializeToString())\n\n\ndef export_tflite_model(yolo, path):\n    overwrite_path(path)\n    converter = tf.lite.TFLiteConverter.from_keras_model(yolo.yolo_model)\n    # converter.allow_custom_ops = True\n    # converter.experimental_enable_mlir_converter=True\n    converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n    tflite_model = converter.convert()\n    tf.io.write_file(os.path.join(path,\'model.tflite\'),tflite_model)\n\n\ndef calculate_map(yolo, glob):\n    mAP = MAPCallback(glob, yolo.input_shape, yolo.class_names)\n    mAP.set_model(yolo.yolo_model)\n    APs = mAP.calculate_aps()\n    for cls in range(len(yolo.class_names)):\n        if cls in APs:\n            print(yolo.class_names[cls] + \' ap: \', APs[cls])\n    mAP = np.mean([APs[cls] for cls in APs])\n    print(\'mAP: \', mAP)\n\n\ndef inference_img(yolo, image_path, draw=True):\n    try:\n        image = open(image_path, \'rb\')\n    except:\n        print(\'Open Error! Try again!\')\n    else:\n        r_image = yolo.detect_image(image, draw)\n        r_image.show()\n\n\ndef detect_img(yolo):\n    while True:\n        inputs = input(\'Input image filename:\')\n        if inputs.endswith(\'.txt\'):\n            with open(input) as file:\n                for image_path in file.readlines():\n                    image_path = image_path.strip()\n                    inference_img(yolo, image_path, False)\n        else:\n            inference_img(yolo, inputs)\n    yolo.close_session()\n\n\ndef detect_video(yolo: YOLO, video_path: str, output_path: str = """"):\n    video_path_formatted = video_path\n    if video_path.isdigit():\n        video_path_formatted = int(video_path)\n    vid = cv2.VideoCapture(video_path_formatted)\n\n    if not vid.isOpened():\n        raise IOError(""Couldn\'t open webcam or video"")\n    video_FourCC = int(vid.get(cv2.CAP_PROP_FOURCC))\n    video_fps = vid.get(cv2.CAP_PROP_FPS)\n    video_size = (int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)),\n                  int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n    isOutput = True if output_path != """" else False\n    if isOutput:\n        print(""!!! TYPE:"", type(output_path), type(video_FourCC),\n              type(video_fps), type(video_size))\n        out = cv2.VideoWriter(output_path, video_FourCC, video_fps, video_size)\n    accum_time = 0\n    curr_fps = 0\n    fps = ""FPS: ??""\n    prev_time = timer()\n\n    trackers = {}\n    font = ImageFont.truetype(font=\'font/FiraMono-Medium.otf\', size=30)\n    thickness = 1\n    frame_count = 0\n    while True:\n        return_value, frame = vid.read()\n        image = Image.fromarray(frame)\n        img_str = cv2.imencode(\'.jpg\', np.array(image))[1].tostring()\n        draw = ImageDraw.Draw(image)\n        if len(trackers) > 0:\n            for tracker in trackers:\n                success, box = tracker.update(frame)\n                if success is not True:\n                    trackers.pop(tracker)\n                    continue\n                left, top, width, height = box\n                right = left + width\n                bottom = top + height\n\n                label = \'{}\'.format(trackers[tracker])\n\n                label_size = draw.textsize(label, font)\n                if top - label_size[1] >= 0:\n                    text_origin = np.array([left, top - label_size[1]])\n                else:\n                    text_origin = np.array([left, top + 1])\n\n                # My kingdom for a good redistributable image drawing library.\n                for i in range(thickness):\n                    draw.rectangle([left + i, top + i, right - i, bottom - i],\n                                   outline=yolo.colors[c])\n                draw.rectangle(\n                    [tuple(text_origin),\n                     tuple(text_origin + label_size)],\n                    fill=yolo.colors[c])\n                draw.text(text_origin, label, fill=(0, 0, 0), font=font)\n                frame_count += 1\n                if frame_count == 100:\n                    for tracker in trackers:\n                        del tracker\n                    trackers = {}\n                    frame_count = 0\n        else:\n            boxes, scores, classes = yolo.detect_image(img_str, False)\n            for i, c in enumerate(classes):\n                predicted_class = yolo.class_names[c]\n                top, left, bottom, right = boxes[i]\n                height = abs(bottom - top)\n                width = abs(right - left)\n                tracker = cv2.TrackerCSRT_create()\n                #tracker = cv2.TrackerKCF_create()\n                #tracker = cv2.TrackerMOSSE_create()\n                tracker.init(frame, (left, top, width, height))\n                trackers[tracker] = predicted_class\n\n                label = \'{}\'.format(predicted_class)\n                label_size = draw.textsize(label, font)\n                if top - label_size[1] >= 0:\n                    text_origin = np.array([left, top - label_size[1]])\n                else:\n                    text_origin = np.array([left, top + 1])\n\n                # My kingdom for a good redistributable image drawing library.\n                for i in range(thickness):\n                    draw.rectangle([left + i, top + i, right - i, bottom - i],\n                                   outline=yolo.colors[c])\n                draw.rectangle(\n                    [tuple(text_origin),\n                     tuple(text_origin + label_size)],\n                    fill=yolo.colors[c])\n                draw.text(text_origin, label, fill=(0, 0, 0), font=font)\n        del draw\n        result = np.asarray(image)\n        curr_time = timer()\n        exec_time = curr_time - prev_time\n        prev_time = curr_time\n        accum_time = accum_time + exec_time\n        curr_fps = curr_fps + 1\n        if accum_time > 1:\n            accum_time = accum_time - 1\n            fps = ""FPS: "" + str(curr_fps)\n            curr_fps = 0\n        cv2.putText(\n            result,\n            text=fps,\n            org=(3, 15),\n            fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n            fontScale=0.50,\n            color=(255, 0, 0),\n            thickness=2)\n        cv2.namedWindow(""result"", cv2.WINDOW_NORMAL)\n        cv2.imshow(""result"", result)\n\n        if isOutput:\n            out.write(result)\n        if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n            break\n    yolo.close_session()\n'"
yolo3/__init__.py,0,b''
yolo3/autoaugment_v1.py,254,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""AutoAugment util file.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport inspect\nimport math\nimport tensorflow as tf\n\n\n# This signifies the max integer that the controller RNN could predict for the\n# augmentation scheme.\n_MAX_LEVEL = 10.\n\n\n# Represents an invalid bounding box that is used for checking for padding\n# lists of bounding box coordinates for a few augmentation operations\n_INVALID_BOX = [[-1.0, -1.0, -1.0, -1.0]]\n\n\ndef policy_v0():\n  """"""Autoaugment policy that was used in AutoAugment Detection Paper.""""""\n  # Each tuple is an augmentation operation of the form\n  # (operation, probability, magnitude). Each element in policy is a\n  # sub-policy that will be applied sequentially on the image.\n  policy = [\n      [(\'TranslateX_BBox\', 0.6, 4), (\'Equalize\', 0.8, 10)],\n      [(\'TranslateY_Only_BBoxes\', 0.2, 2), (\'Cutout\', 0.8, 8)],\n      [(\'Sharpness\', 0.0, 8), (\'ShearX_BBox\', 0.4, 0)],\n      [(\'ShearY_BBox\', 1.0, 2), (\'TranslateY_Only_BBoxes\', 0.6, 6)],\n      [(\'Rotate_BBox\', 0.6, 10), (\'Color\', 1.0, 6)],\n  ]\n  return policy\n\n\ndef policy_v1():\n  """"""Autoaugment policy that was used in AutoAugment Detection Paper.""""""\n  # Each tuple is an augmentation operation of the form\n  # (operation, probability, magnitude). Each element in policy is a\n  # sub-policy that will be applied sequentially on the image.\n  policy = [\n      [(\'TranslateX_BBox\', 0.6, 4), (\'Equalize\', 0.8, 10)],\n      [(\'TranslateY_Only_BBoxes\', 0.2, 2), (\'Cutout\', 0.8, 8)],\n      [(\'Sharpness\', 0.0, 8), (\'ShearX_BBox\', 0.4, 0)],\n      [(\'ShearY_BBox\', 1.0, 2), (\'TranslateY_Only_BBoxes\', 0.6, 6)],\n      [(\'Rotate_BBox\', 0.6, 10), (\'Color\', 1.0, 6)],\n      [(\'Color\', 0.0, 0), (\'ShearX_Only_BBoxes\', 0.8, 4)],\n      [(\'ShearY_Only_BBoxes\', 0.8, 2), (\'Flip_Only_BBoxes\', 0.0, 10)],\n      [(\'Equalize\', 0.6, 10), (\'TranslateX_BBox\', 0.2, 2)],\n      [(\'Color\', 1.0, 10), (\'TranslateY_Only_BBoxes\', 0.4, 6)],\n      [(\'Rotate_BBox\', 0.8, 10), (\'Contrast\', 0.0, 10)],\n      [(\'Cutout\', 0.2, 2), (\'Brightness\', 0.8, 10)],\n      [(\'Color\', 1.0, 6), (\'Equalize\', 1.0, 2)],\n      [(\'Cutout_Only_BBoxes\', 0.4, 6), (\'TranslateY_Only_BBoxes\', 0.8, 2)],\n      [(\'Color\', 0.2, 8), (\'Rotate_BBox\', 0.8, 10)],\n      [(\'Sharpness\', 0.4, 4), (\'TranslateY_Only_BBoxes\', 0.0, 4)],\n      [(\'Sharpness\', 1.0, 4), (\'SolarizeAdd\', 0.4, 4)],\n      [(\'Rotate_BBox\', 1.0, 8), (\'Sharpness\', 0.2, 8)],\n      [(\'ShearY_BBox\', 0.6, 10), (\'Equalize_Only_BBoxes\', 0.6, 8)],\n      [(\'ShearX_BBox\', 0.2, 6), (\'TranslateY_Only_BBoxes\', 0.2, 10)],\n      [(\'SolarizeAdd\', 0.6, 8), (\'Brightness\', 0.8, 10)],\n  ]\n  return policy\n\n\ndef policy_vtest():\n  """"""Autoaugment test policy for debugging.""""""\n  # Each tuple is an augmentation operation of the form\n  # (operation, probability, magnitude). Each element in policy is a\n  # sub-policy that will be applied sequentially on the image.\n  policy = [\n      [(\'TranslateX_BBox\', 1.0, 4), (\'Equalize\', 1.0, 10)],\n  ]\n  return policy\n\n\ndef policy_v2():\n  """"""Additional policy that performs well on object detection.""""""\n  # Each tuple is an augmentation operation of the form\n  # (operation, probability, magnitude). Each element in policy is a\n  # sub-policy that will be applied sequentially on the image.\n  policy = [\n      [(\'Color\', 0.0, 6), (\'Cutout\', 0.6, 8), (\'Sharpness\', 0.4, 8)],\n      [(\'Rotate_BBox\', 0.4, 8), (\'Sharpness\', 0.4, 2),\n       (\'Rotate_BBox\', 0.8, 10)],\n      [(\'TranslateY_BBox\', 1.0, 8), (\'AutoContrast\', 0.8, 2)],\n      [(\'AutoContrast\', 0.4, 6), (\'ShearX_BBox\', 0.8, 8),\n       (\'Brightness\', 0.0, 10)],\n      [(\'SolarizeAdd\', 0.2, 6), (\'Contrast\', 0.0, 10),\n       (\'AutoContrast\', 0.6, 0)],\n      [(\'Cutout\', 0.2, 0), (\'Solarize\', 0.8, 8), (\'Color\', 1.0, 4)],\n      [(\'TranslateY_BBox\', 0.0, 4), (\'Equalize\', 0.6, 8),\n       (\'Solarize\', 0.0, 10)],\n      [(\'TranslateY_BBox\', 0.2, 2), (\'ShearY_BBox\', 0.8, 8),\n       (\'Rotate_BBox\', 0.8, 8)],\n      [(\'Cutout\', 0.8, 8), (\'Brightness\', 0.8, 8), (\'Cutout\', 0.2, 2)],\n      [(\'Color\', 0.8, 4), (\'TranslateY_BBox\', 1.0, 6), (\'Rotate_BBox\', 0.6, 6)],\n      [(\'Rotate_BBox\', 0.6, 10), (\'BBox_Cutout\', 1.0, 4), (\'Cutout\', 0.2, 8)],\n      [(\'Rotate_BBox\', 0.0, 0), (\'Equalize\', 0.6, 6), (\'ShearY_BBox\', 0.6, 8)],\n      [(\'Brightness\', 0.8, 8), (\'AutoContrast\', 0.4, 2),\n       (\'Brightness\', 0.2, 2)],\n      [(\'TranslateY_BBox\', 0.4, 8), (\'Solarize\', 0.4, 6),\n       (\'SolarizeAdd\', 0.2, 10)],\n      [(\'Contrast\', 1.0, 10), (\'SolarizeAdd\', 0.2, 8), (\'Equalize\', 0.2, 4)],\n  ]\n  return policy\n\n\ndef policy_v3():\n  """"""""Additional policy that performs well on object detection.""""""\n  # Each tuple is an augmentation operation of the form\n  # (operation, probability, magnitude). Each element in policy is a\n  # sub-policy that will be applied sequentially on the image.\n  policy = [\n      [(\'Posterize\', 0.8, 2), (\'TranslateX_BBox\', 1.0, 8)],\n      [(\'BBox_Cutout\', 0.2, 10), (\'Sharpness\', 1.0, 8)],\n      [(\'Rotate_BBox\', 0.6, 8), (\'Rotate_BBox\', 0.8, 10)],\n      [(\'Equalize\', 0.8, 10), (\'AutoContrast\', 0.2, 10)],\n      [(\'SolarizeAdd\', 0.2, 2), (\'TranslateY_BBox\', 0.2, 8)],\n      [(\'Sharpness\', 0.0, 2), (\'Color\', 0.4, 8)],\n      [(\'Equalize\', 1.0, 8), (\'TranslateY_BBox\', 1.0, 8)],\n      [(\'Posterize\', 0.6, 2), (\'Rotate_BBox\', 0.0, 10)],\n      [(\'AutoContrast\', 0.6, 0), (\'Rotate_BBox\', 1.0, 6)],\n      [(\'Equalize\', 0.0, 4), (\'Cutout\', 0.8, 10)],\n      [(\'Brightness\', 1.0, 2), (\'TranslateY_BBox\', 1.0, 6)],\n      [(\'Contrast\', 0.0, 2), (\'ShearY_BBox\', 0.8, 0)],\n      [(\'AutoContrast\', 0.8, 10), (\'Contrast\', 0.2, 10)],\n      [(\'Rotate_BBox\', 1.0, 10), (\'Cutout\', 1.0, 10)],\n      [(\'SolarizeAdd\', 0.8, 6), (\'Equalize\', 0.8, 8)],\n  ]\n  return policy\n\n\ndef blend(image1, image2, factor):\n  """"""Blend image1 and image2 using \'factor\'.\n\n  Factor can be above 0.0.  A value of 0.0 means only image1 is used.\n  A value of 1.0 means only image2 is used.  A value between 0.0 and\n  1.0 means we linearly interpolate the pixel values between the two\n  images.  A value greater than 1.0 ""extrapolates"" the difference\n  between the two pixel values, and we clip the results to values\n  between 0 and 255.\n\n  Args:\n    image1: An image Tensor of type uint8.\n    image2: An image Tensor of type uint8.\n    factor: A floating point value above 0.0.\n\n  Returns:\n    A blended image Tensor of type uint8.\n  """"""\n  if factor == 0.0:\n    return tf.convert_to_tensor(image1)\n  if factor == 1.0:\n    return tf.convert_to_tensor(image2)\n\n  image1 = tf.to_float(image1)\n  image2 = tf.to_float(image2)\n\n  difference = image2 - image1\n  scaled = factor * difference\n\n  # Do addition in float.\n  temp = tf.to_float(image1) + scaled\n\n  # Interpolate\n  if factor > 0.0 and factor < 1.0:\n    # Interpolation means we always stay within 0 and 255.\n    return tf.cast(temp, tf.uint8)\n\n  # Extrapolate:\n  #\n  # We need to clip and then cast.\n  return tf.cast(tf.clip_by_value(temp, 0.0, 255.0), tf.uint8)\n\n\ndef cutout(image, pad_size, replace=0):\n  """"""Apply cutout (https://arxiv.org/abs/1708.04552) to image.\n\n  This operation applies a (2*pad_size x 2*pad_size) mask of zeros to\n  a random location within `img`. The pixel values filled in will be of the\n  value `replace`. The located where the mask will be applied is randomly\n  chosen uniformly over the whole image.\n\n  Args:\n    image: An image Tensor of type uint8.\n    pad_size: Specifies how big the zero mask that will be generated is that\n      is applied to the image. The mask will be of size\n      (2*pad_size x 2*pad_size).\n    replace: What pixel value to fill in the image in the area that has\n      the cutout mask applied to it.\n\n  Returns:\n    An image Tensor that is of type uint8.\n  """"""\n  image_height = tf.shape(image)[0]\n  image_width = tf.shape(image)[1]\n\n  # Sample the center location in the image where the zero mask will be applied.\n  cutout_center_height = tf.random_uniform(\n      shape=[], minval=0, maxval=image_height,\n      dtype=tf.int32)\n\n  cutout_center_width = tf.random_uniform(\n      shape=[], minval=0, maxval=image_width,\n      dtype=tf.int32)\n\n  lower_pad = tf.maximum(0, cutout_center_height - pad_size)\n  upper_pad = tf.maximum(0, image_height - cutout_center_height - pad_size)\n  left_pad = tf.maximum(0, cutout_center_width - pad_size)\n  right_pad = tf.maximum(0, image_width - cutout_center_width - pad_size)\n\n  cutout_shape = [image_height - (lower_pad + upper_pad),\n                  image_width - (left_pad + right_pad)]\n  padding_dims = [[lower_pad, upper_pad], [left_pad, right_pad]]\n  mask = tf.pad(\n      tf.zeros(cutout_shape, dtype=image.dtype),\n      padding_dims, constant_values=1)\n  mask = tf.expand_dims(mask, -1)\n  mask = tf.tile(mask, [1, 1, 3])\n  image = tf.where(\n      tf.equal(mask, 0),\n      tf.ones_like(image, dtype=image.dtype) * replace,\n      image)\n  return image\n\n\ndef solarize(image, threshold=128):\n  # For each pixel in the image, select the pixel\n  # if the value is less than the threshold.\n  # Otherwise, subtract 255 from the pixel.\n  return tf.where(image < threshold, image, 255 - image)\n\n\ndef solarize_add(image, addition=0, threshold=128):\n  # For each pixel in the image less than threshold\n  # we add \'addition\' amount to it and then clip the\n  # pixel value to be between 0 and 255. The value\n  # of \'addition\' is between -128 and 128.\n  added_image = tf.cast(image, tf.int64) + addition\n  added_image = tf.cast(tf.clip_by_value(added_image, 0, 255), tf.uint8)\n  return tf.where(image < threshold, added_image, image)\n\n\ndef color(image, factor):\n  """"""Equivalent of PIL Color.""""""\n  degenerate = tf.image.grayscale_to_rgb(tf.image.rgb_to_grayscale(image))\n  return blend(degenerate, image, factor)\n\n\ndef contrast(image, factor):\n  """"""Equivalent of PIL Contrast.""""""\n  degenerate = tf.image.rgb_to_grayscale(image)\n  # Cast before calling tf.histogram.\n  degenerate = tf.cast(degenerate, tf.int32)\n\n  # Compute the grayscale histogram, then compute the mean pixel value,\n  # and create a constant image size of that value.  Use that as the\n  # blending degenerate target of the original image.\n  hist = tf.histogram_fixed_width(degenerate, [0, 255], nbins=256)\n  mean = tf.reduce_sum(tf.cast(hist, tf.float32)) / 256.0\n  degenerate = tf.ones_like(degenerate, dtype=tf.float32) * mean\n  degenerate = tf.clip_by_value(degenerate, 0.0, 255.0)\n  degenerate = tf.image.grayscale_to_rgb(tf.cast(degenerate, tf.uint8))\n  return blend(degenerate, image, factor)\n\n\ndef brightness(image, factor):\n  """"""Equivalent of PIL Brightness.""""""\n  degenerate = tf.zeros_like(image)\n  return blend(degenerate, image, factor)\n\n\ndef posterize(image, bits):\n  """"""Equivalent of PIL Posterize.""""""\n  shift = 8 - bits\n  return tf.bitwise.left_shift(tf.bitwise.right_shift(image, shift), shift)\n\n\ndef rotate(image, degrees, replace):\n  """"""Rotates the image by degrees either clockwise or counterclockwise.\n\n  Args:\n    image: An image Tensor of type uint8.\n    degrees: Float, a scalar angle in degrees to rotate all images by. If\n      degrees is positive the image will be rotated clockwise otherwise it will\n      be rotated counterclockwise.\n    replace: A one or three value 1D tensor to fill empty pixels caused by\n      the rotate operation.\n\n  Returns:\n    The rotated version of image.\n  """"""\n  # Convert from degrees to radians.\n  degrees_to_radians = math.pi / 180.0\n  radians = degrees * degrees_to_radians\n\n  # In practice, we should randomize the rotation degrees by flipping\n  # it negatively half the time, but that\'s done on \'degrees\' outside\n  # of the function.\n  image = tf.contrib.image.rotate(wrap(image), radians)\n  return unwrap(image, replace)\n\n\ndef random_shift_bbox(image, bbox, pixel_scaling, replace,\n                      new_min_bbox_coords=None):\n  """"""Move the bbox and the image content to a slightly new random location.\n\n  Args:\n    image: 3D uint8 Tensor.\n    bbox: 1D Tensor that has 4 elements (min_y, min_x, max_y, max_x)\n      of type float that represents the normalized coordinates between 0 and 1.\n      The potential values for the new min corner of the bbox will be between\n      [old_min - pixel_scaling * bbox_height/2,\n       old_min - pixel_scaling * bbox_height/2].\n    pixel_scaling: A float between 0 and 1 that specifies the pixel range\n      that the new bbox location will be sampled from.\n    replace: A one or three value 1D tensor to fill empty pixels.\n    new_min_bbox_coords: If not None, then this is a tuple that specifies the\n      (min_y, min_x) coordinates of the new bbox. Normally this is randomly\n      specified, but this allows it to be manually set. The coordinates are\n      the absolute coordinates between 0 and image height/width and are int32.\n\n  Returns:\n    The new image that will have the shifted bbox location in it along with\n    the new bbox that contains the new coordinates.\n  """"""\n  # Obtains image height and width and create helper clip functions.\n  image_height = tf.to_float(tf.shape(image)[0])\n  image_width = tf.to_float(tf.shape(image)[1])\n  def clip_y(val):\n    return tf.clip_by_value(val, 0, tf.to_int32(image_height) - 1)\n  def clip_x(val):\n    return tf.clip_by_value(val, 0, tf.to_int32(image_width) - 1)\n\n  # Convert bbox to pixel coordinates.\n  min_y = tf.to_int32(image_height * bbox[0])\n  min_x = tf.to_int32(image_width * bbox[1])\n  max_y = clip_y(tf.to_int32(image_height * bbox[2]))\n  max_x = clip_x(tf.to_int32(image_width * bbox[3]))\n  bbox_height, bbox_width = (max_y - min_y + 1, max_x - min_x + 1)\n  image_height = tf.to_int32(image_height)\n  image_width = tf.to_int32(image_width)\n\n  # Select the new min/max bbox ranges that are used for sampling the\n  # new min x/y coordinates of the shifted bbox.\n  minval_y = clip_y(\n      min_y - tf.to_int32(pixel_scaling * tf.to_float(bbox_height) / 2.0))\n  maxval_y = clip_y(\n      min_y + tf.to_int32(pixel_scaling * tf.to_float(bbox_height) / 2.0))\n  minval_x = clip_x(\n      min_x - tf.to_int32(pixel_scaling * tf.to_float(bbox_width) / 2.0))\n  maxval_x = clip_x(\n      min_x + tf.to_int32(pixel_scaling * tf.to_float(bbox_width) / 2.0))\n\n  # Sample and calculate the new unclipped min/max coordinates of the new bbox.\n  if new_min_bbox_coords is None:\n    unclipped_new_min_y = tf.random_uniform(\n        shape=[], minval=minval_y, maxval=maxval_y,\n        dtype=tf.int32)\n    unclipped_new_min_x = tf.random_uniform(\n        shape=[], minval=minval_x, maxval=maxval_x,\n        dtype=tf.int32)\n  else:\n    unclipped_new_min_y, unclipped_new_min_x = (\n        clip_y(new_min_bbox_coords[0]), clip_x(new_min_bbox_coords[1]))\n  unclipped_new_max_y = unclipped_new_min_y + bbox_height - 1\n  unclipped_new_max_x = unclipped_new_min_x + bbox_width - 1\n\n  # Determine if any of the new bbox was shifted outside the current image.\n  # This is used for determining if any of the original bbox content should be\n  # discarded.\n  new_min_y, new_min_x, new_max_y, new_max_x = (\n      clip_y(unclipped_new_min_y), clip_x(unclipped_new_min_x),\n      clip_y(unclipped_new_max_y), clip_x(unclipped_new_max_x))\n  shifted_min_y = (new_min_y - unclipped_new_min_y) + min_y\n  shifted_max_y = max_y - (unclipped_new_max_y - new_max_y)\n  shifted_min_x = (new_min_x - unclipped_new_min_x) + min_x\n  shifted_max_x = max_x - (unclipped_new_max_x - new_max_x)\n\n  # Create the new bbox tensor by converting pixel integer values to floats.\n  new_bbox = tf.stack([\n      tf.to_float(new_min_y) / tf.to_float(image_height),\n      tf.to_float(new_min_x) / tf.to_float(image_width),\n      tf.to_float(new_max_y) / tf.to_float(image_height),\n      tf.to_float(new_max_x) / tf.to_float(image_width)])\n\n  # Copy the contents in the bbox and fill the old bbox location\n  # with gray (128).\n  bbox_content = image[shifted_min_y:shifted_max_y + 1,\n                       shifted_min_x:shifted_max_x + 1, :]\n\n  def mask_and_add_image(\n      min_y_, min_x_, max_y_, max_x_, mask, content_tensor, image_):\n    """"""Applies mask to bbox region in image then adds content_tensor to it.""""""\n    mask = tf.pad(mask,\n                  [[min_y_, (image_height - 1) - max_y_],\n                   [min_x_, (image_width - 1) - max_x_],\n                   [0, 0]], constant_values=1)\n    content_tensor = tf.pad(content_tensor,\n                            [[min_y_, (image_height - 1) - max_y_],\n                             [min_x_, (image_width - 1) - max_x_],\n                             [0, 0]], constant_values=0)\n    return image_ * mask + content_tensor\n\n  # Zero out original bbox location.\n  mask = tf.zeros_like(image)[min_y:max_y+1, min_x:max_x+1, :]\n  grey_tensor = tf.zeros_like(mask) + replace[0]\n  image = mask_and_add_image(min_y, min_x, max_y, max_x, mask,\n                             grey_tensor, image)\n\n  # Fill in bbox content to new bbox location.\n  mask = tf.zeros_like(bbox_content)\n  image = mask_and_add_image(new_min_y, new_min_x, new_max_y, new_max_x, mask,\n                             bbox_content, image)\n\n  return image, new_bbox\n\n\ndef _clip_bbox(min_y, min_x, max_y, max_x):\n  """"""Clip bounding box coordinates between 0 and 1.\n\n  Args:\n    min_y: Normalized bbox coordinate of type float between 0 and 1.\n    min_x: Normalized bbox coordinate of type float between 0 and 1.\n    max_y: Normalized bbox coordinate of type float between 0 and 1.\n    max_x: Normalized bbox coordinate of type float between 0 and 1.\n\n  Returns:\n    Clipped coordinate values between 0 and 1.\n  """"""\n  min_y = tf.clip_by_value(min_y, 0.0, 1.0)\n  min_x = tf.clip_by_value(min_x, 0.0, 1.0)\n  max_y = tf.clip_by_value(max_y, 0.0, 1.0)\n  max_x = tf.clip_by_value(max_x, 0.0, 1.0)\n  return min_y, min_x, max_y, max_x\n\n\ndef _check_bbox_area(min_y, min_x, max_y, max_x, delta=0.05):\n  """"""Adjusts bbox coordinates to make sure the area is > 0.\n\n  Args:\n    min_y: Normalized bbox coordinate of type float between 0 and 1.\n    min_x: Normalized bbox coordinate of type float between 0 and 1.\n    max_y: Normalized bbox coordinate of type float between 0 and 1.\n    max_x: Normalized bbox coordinate of type float between 0 and 1.\n    delta: Float, this is used to create a gap of size 2 * delta between\n      bbox min/max coordinates that are the same on the boundary.\n      This prevents the bbox from having an area of zero.\n\n  Returns:\n    Tuple of new bbox coordinates between 0 and 1 that will now have a\n    guaranteed area > 0.\n  """"""\n  height = max_y - min_y\n  width = max_x - min_x\n  def _adjust_bbox_boundaries(min_coord, max_coord):\n    # Make sure max is never 0 and min is never 1.\n    max_coord = tf.maximum(max_coord, 0.0 + delta)\n    min_coord = tf.minimum(min_coord, 1.0 - delta)\n    return min_coord, max_coord\n  min_y, max_y = tf.cond(tf.equal(height, 0.0),\n                         lambda: _adjust_bbox_boundaries(min_y, max_y),\n                         lambda: (min_y, max_y))\n  min_x, max_x = tf.cond(tf.equal(width, 0.0),\n                         lambda: _adjust_bbox_boundaries(min_x, max_x),\n                         lambda: (min_x, max_x))\n  return min_y, min_x, max_y, max_x\n\n\ndef _scale_bbox_only_op_probability(prob):\n  """"""Reduce the probability of the bbox-only operation.\n\n  Probability is reduced so that we do not distort the content of too many\n  bounding boxes that are close to each other. The value of 3.0 was a chosen\n  hyper parameter when designing the autoaugment algorithm that we found\n  empirically to work well.\n\n  Args:\n    prob: Float that is the probability of applying the bbox-only operation.\n\n  Returns:\n    Reduced probability.\n  """"""\n  return prob / 3.0\n\n\ndef _apply_bbox_augmentation(image, bbox, augmentation_func, *args):\n  """"""Applies augmentation_func to the subsection of image indicated by bbox.\n\n  Args:\n    image: 3D uint8 Tensor.\n    bbox: 1D Tensor that has 4 elements (min_y, min_x, max_y, max_x)\n      of type float that represents the normalized coordinates between 0 and 1.\n    augmentation_func: Augmentation function that will be applied to the\n      subsection of image.\n    *args: Additional parameters that will be passed into augmentation_func\n      when it is called.\n\n  Returns:\n    A modified version of image, where the bbox location in the image will\n    have `ugmentation_func applied to it.\n  """"""\n  image_height = tf.to_float(tf.shape(image)[0])\n  image_width = tf.to_float(tf.shape(image)[1])\n  min_y = tf.to_int32(image_height * bbox[0])\n  min_x = tf.to_int32(image_width * bbox[1])\n  max_y = tf.to_int32(image_height * bbox[2])\n  max_x = tf.to_int32(image_width * bbox[3])\n  image_height = tf.to_int32(image_height)\n  image_width = tf.to_int32(image_width)\n\n  # Clip to be sure the max values do not fall out of range.\n  max_y = tf.minimum(max_y, image_height - 1)\n  max_x = tf.minimum(max_x, image_width - 1)\n\n  # Get the sub-tensor that is the image within the bounding box region.\n  bbox_content = image[min_y:max_y + 1, min_x:max_x + 1, :]\n\n  # Apply the augmentation function to the bbox portion of the image.\n  augmented_bbox_content = augmentation_func(bbox_content, *args)\n\n  # Pad the augmented_bbox_content and the mask to match the shape of original\n  # image.\n  augmented_bbox_content = tf.pad(augmented_bbox_content,\n                                  [[min_y, (image_height - 1) - max_y],\n                                   [min_x, (image_width - 1) - max_x],\n                                   [0, 0]])\n\n  # Create a mask that will be used to zero out a part of the original image.\n  mask_tensor = tf.zeros_like(bbox_content)\n\n  mask_tensor = tf.pad(mask_tensor,\n                       [[min_y, (image_height - 1) - max_y],\n                        [min_x, (image_width - 1) - max_x],\n                        [0, 0]],\n                       constant_values=1)\n  # Replace the old bbox content with the new augmented content.\n  image = image * mask_tensor + augmented_bbox_content\n  return image\n\n\ndef _concat_bbox(bbox, bboxes):\n  """"""Helper function that concates bbox to bboxes along the first dimension.""""""\n\n  # Note if all elements in bboxes are -1 (_INVALID_BOX), then this means\n  # we discard bboxes and start the bboxes Tensor with the current bbox.\n  bboxes_sum_check = tf.reduce_sum(bboxes)\n  bbox = tf.expand_dims(bbox, 0)\n  # This check will be true when it is an _INVALID_BOX\n  bboxes = tf.cond(tf.equal(bboxes_sum_check, -4.0),\n                   lambda: bbox,\n                   lambda: tf.concat([bboxes, bbox], 0))\n  return bboxes\n\n\ndef _apply_bbox_augmentation_wrapper(image, bbox, new_bboxes, prob,\n                                     augmentation_func, func_changes_bbox,\n                                     *args):\n  """"""Applies _apply_bbox_augmentation with probability prob.\n\n  Args:\n    image: 3D uint8 Tensor.\n    bbox: 1D Tensor that has 4 elements (min_y, min_x, max_y, max_x)\n      of type float that represents the normalized coordinates between 0 and 1.\n    new_bboxes: 2D Tensor that is a list of the bboxes in the image after they\n      have been altered by aug_func. These will only be changed when\n      func_changes_bbox is set to true. Each bbox has 4 elements\n      (min_y, min_x, max_y, max_x) of type float that are the normalized\n      bbox coordinates between 0 and 1.\n    prob: Float that is the probability of applying _apply_bbox_augmentation.\n    augmentation_func: Augmentation function that will be applied to the\n      subsection of image.\n    func_changes_bbox: Boolean. Does augmentation_func return bbox in addition\n      to image.\n    *args: Additional parameters that will be passed into augmentation_func\n      when it is called.\n\n  Returns:\n    A tuple. Fist element is a modified version of image, where the bbox\n    location in the image will have augmentation_func applied to it if it is\n    chosen to be called with probability `prob`. The second element is a\n    Tensor of Tensors of length 4 that will contain the altered bbox after\n    applying augmentation_func.\n  """"""\n  should_apply_op = tf.cast(\n      tf.floor(tf.random_uniform([], dtype=tf.float32) + prob), tf.bool)\n  if func_changes_bbox:\n    augmented_image, bbox = tf.cond(\n        should_apply_op,\n        lambda: augmentation_func(image, bbox, *args),\n        lambda: (image, bbox))\n  else:\n    augmented_image = tf.cond(\n        should_apply_op,\n        lambda: _apply_bbox_augmentation(image, bbox, augmentation_func, *args),\n        lambda: image)\n  new_bboxes = _concat_bbox(bbox, new_bboxes)\n  return augmented_image, new_bboxes\n\n\ndef _apply_multi_bbox_augmentation(image, bboxes, prob, aug_func,\n                                   func_changes_bbox, *args):\n  """"""Applies aug_func to the image for each bbox in bboxes.\n\n  Args:\n    image: 3D uint8 Tensor.\n    bboxes: 2D Tensor that is a list of the bboxes in the image. Each bbox\n      has 4 elements (min_y, min_x, max_y, max_x) of type float.\n    prob: Float that is the probability of applying aug_func to a specific\n      bounding box within the image.\n    aug_func: Augmentation function that will be applied to the\n      subsections of image indicated by the bbox values in bboxes.\n    func_changes_bbox: Boolean. Does augmentation_func return bbox in addition\n      to image.\n    *args: Additional parameters that will be passed into augmentation_func\n      when it is called.\n\n  Returns:\n    A modified version of image, where each bbox location in the image will\n    have augmentation_func applied to it if it is chosen to be called with\n    probability prob independently across all bboxes. Also the final\n    bboxes are returned that will be unchanged if func_changes_bbox is set to\n    false and if true, the new altered ones will be returned.\n  """"""\n  # Will keep track of the new altered bboxes after aug_func is repeatedly\n  # applied. The -1 values are a dummy value and this first Tensor will be\n  # removed upon appending the first real bbox.\n  new_bboxes = tf.constant(_INVALID_BOX)\n\n  # If the bboxes are empty, then just give it _INVALID_BOX. The result\n  # will be thrown away.\n  bboxes = tf.cond(tf.equal(tf.size(bboxes), 0),\n                   lambda: tf.constant(_INVALID_BOX),\n                   lambda: bboxes)\n\n  bboxes = tf.ensure_shape(bboxes, (None, 4))\n\n  # pylint:disable=g-long-lambda\n  # pylint:disable=line-too-long\n  wrapped_aug_func = lambda _image, bbox, _new_bboxes: _apply_bbox_augmentation_wrapper(\n      _image, bbox, _new_bboxes, prob, aug_func, func_changes_bbox, *args)\n  # pylint:enable=g-long-lambda\n  # pylint:enable=line-too-long\n\n  # Setup the while_loop.\n  num_bboxes = tf.shape(bboxes)[0]  # We loop until we go over all bboxes.\n  idx = tf.constant(0)  # Counter for the while loop.\n\n  # Conditional function when to end the loop once we go over all bboxes\n  # images_and_bboxes contain (_image, _new_bboxes)\n  cond = lambda _idx, _images_and_bboxes: tf.less(_idx, num_bboxes)\n\n  # Shuffle the bboxes so that the augmentation order is not deterministic if\n  # we are not changing the bboxes with aug_func.\n  if not func_changes_bbox:\n    loop_bboxes = tf.random.shuffle(bboxes)\n  else:\n    loop_bboxes = bboxes\n\n  # Main function of while_loop where we repeatedly apply augmentation on the\n  # bboxes in the image.\n  # pylint:disable=g-long-lambda\n  body = lambda _idx, _images_and_bboxes: [\n      _idx + 1, wrapped_aug_func(_images_and_bboxes[0],\n                                 loop_bboxes[_idx],\n                                 _images_and_bboxes[1])]\n  # pylint:enable=g-long-lambda\n\n  _, (image, new_bboxes) = tf.while_loop(\n      cond, body, [idx, (image, new_bboxes)],\n      shape_invariants=[idx.get_shape(),\n                        (image.get_shape(), tf.TensorShape([None, 4]))])\n\n  # Either return the altered bboxes or the original ones depending on if\n  # we altered them in anyway.\n  if func_changes_bbox:\n    final_bboxes = new_bboxes\n  else:\n    final_bboxes = bboxes\n  return image, final_bboxes\n\n\ndef _apply_multi_bbox_augmentation_wrapper(image, bboxes, prob, aug_func,\n                                           func_changes_bbox, *args):\n  """"""Checks to be sure num bboxes > 0 before calling inner function.""""""\n  num_bboxes = tf.shape(bboxes)[0]\n  image, bboxes = tf.cond(\n      tf.equal(num_bboxes, 0),\n      lambda: (image, bboxes),\n      # pylint:disable=g-long-lambda\n      lambda: _apply_multi_bbox_augmentation(\n          image, bboxes, prob, aug_func, func_changes_bbox, *args))\n      # pylint:enable=g-long-lambda\n  return image, bboxes\n\n\ndef rotate_only_bboxes(image, bboxes, prob, degrees, replace):\n  """"""Apply rotate to each bbox in the image with probability prob.""""""\n  func_changes_bbox = False\n  prob = _scale_bbox_only_op_probability(prob)\n  return _apply_multi_bbox_augmentation_wrapper(\n      image, bboxes, prob, rotate, func_changes_bbox, degrees, replace)\n\n\ndef shear_x_only_bboxes(image, bboxes, prob, level, replace):\n  """"""Apply shear_x to each bbox in the image with probability prob.""""""\n  func_changes_bbox = False\n  prob = _scale_bbox_only_op_probability(prob)\n  return _apply_multi_bbox_augmentation_wrapper(\n      image, bboxes, prob, shear_x, func_changes_bbox, level, replace)\n\n\ndef shear_y_only_bboxes(image, bboxes, prob, level, replace):\n  """"""Apply shear_y to each bbox in the image with probability prob.""""""\n  func_changes_bbox = False\n  prob = _scale_bbox_only_op_probability(prob)\n  return _apply_multi_bbox_augmentation_wrapper(\n      image, bboxes, prob, shear_y, func_changes_bbox, level, replace)\n\n\ndef translate_x_only_bboxes(image, bboxes, prob, pixels, replace):\n  """"""Apply translate_x to each bbox in the image with probability prob.""""""\n  func_changes_bbox = False\n  prob = _scale_bbox_only_op_probability(prob)\n  return _apply_multi_bbox_augmentation_wrapper(\n      image, bboxes, prob, translate_x, func_changes_bbox, pixels, replace)\n\n\ndef translate_y_only_bboxes(image, bboxes, prob, pixels, replace):\n  """"""Apply translate_y to each bbox in the image with probability prob.""""""\n  func_changes_bbox = False\n  prob = _scale_bbox_only_op_probability(prob)\n  return _apply_multi_bbox_augmentation_wrapper(\n      image, bboxes, prob, translate_y, func_changes_bbox, pixels, replace)\n\n\ndef flip_only_bboxes(image, bboxes, prob):\n  """"""Apply flip_lr to each bbox in the image with probability prob.""""""\n  func_changes_bbox = False\n  prob = _scale_bbox_only_op_probability(prob)\n  return _apply_multi_bbox_augmentation_wrapper(\n      image, bboxes, prob, tf.image.flip_left_right, func_changes_bbox)\n\n\ndef solarize_only_bboxes(image, bboxes, prob, threshold):\n  """"""Apply solarize to each bbox in the image with probability prob.""""""\n  func_changes_bbox = False\n  prob = _scale_bbox_only_op_probability(prob)\n  return _apply_multi_bbox_augmentation_wrapper(\n      image, bboxes, prob, solarize, func_changes_bbox, threshold)\n\n\ndef equalize_only_bboxes(image, bboxes, prob):\n  """"""Apply equalize to each bbox in the image with probability prob.""""""\n  func_changes_bbox = False\n  prob = _scale_bbox_only_op_probability(prob)\n  return _apply_multi_bbox_augmentation_wrapper(\n      image, bboxes, prob, equalize, func_changes_bbox)\n\n\ndef cutout_only_bboxes(image, bboxes, prob, pad_size, replace):\n  """"""Apply cutout to each bbox in the image with probability prob.""""""\n  func_changes_bbox = False\n  prob = _scale_bbox_only_op_probability(prob)\n  return _apply_multi_bbox_augmentation_wrapper(\n      image, bboxes, prob, cutout, func_changes_bbox, pad_size, replace)\n\n\ndef _rotate_bbox(bbox, image_height, image_width, degrees):\n  """"""Rotates the bbox coordinated by degrees.\n\n  Args:\n    bbox: 1D Tensor that has 4 elements (min_y, min_x, max_y, max_x)\n      of type float that represents the normalized coordinates between 0 and 1.\n    image_height: Int, height of the image.\n    image_width: Int, height of the image.\n    degrees: Float, a scalar angle in degrees to rotate all images by. If\n      degrees is positive the image will be rotated clockwise otherwise it will\n      be rotated counterclockwise.\n\n  Returns:\n    A tensor of the same shape as bbox, but now with the rotated coordinates.\n  """"""\n  image_height, image_width = (\n      tf.to_float(image_height), tf.to_float(image_width))\n\n  # Convert from degrees to radians.\n  degrees_to_radians = math.pi / 180.0\n  radians = degrees * degrees_to_radians\n\n  # Translate the bbox to the center of the image and turn the normalized 0-1\n  # coordinates to absolute pixel locations.\n  # Y coordinates are made negative as the y axis of images goes down with\n  # increasing pixel values, so we negate to make sure x axis and y axis points\n  # are in the traditionally positive direction.\n  min_y = -tf.to_int32(image_height * (bbox[0] - 0.5))\n  min_x = tf.to_int32(image_width * (bbox[1] - 0.5))\n  max_y = -tf.to_int32(image_height * (bbox[2] - 0.5))\n  max_x = tf.to_int32(image_width * (bbox[3] - 0.5))\n  coordinates = tf.stack(\n      [[min_y, min_x], [min_y, max_x], [max_y, min_x], [max_y, max_x]])\n  coordinates = tf.cast(coordinates, tf.float32)\n  # Rotate the coordinates according to the rotation matrix clockwise if\n  # radians is positive, else negative\n  rotation_matrix = tf.stack(\n      [[tf.cos(radians), tf.sin(radians)],\n       [-tf.sin(radians), tf.cos(radians)]])\n  new_coords = tf.cast(\n      tf.matmul(rotation_matrix, tf.transpose(coordinates)), tf.int32)\n  # Find min/max values and convert them back to normalized 0-1 floats.\n  min_y = -(tf.to_float(tf.reduce_max(new_coords[0, :])) / image_height - 0.5)\n  min_x = tf.to_float(tf.reduce_min(new_coords[1, :])) / image_width + 0.5\n  max_y = -(tf.to_float(tf.reduce_min(new_coords[0, :])) / image_height - 0.5)\n  max_x = tf.to_float(tf.reduce_max(new_coords[1, :])) / image_width + 0.5\n\n  # Clip the bboxes to be sure the fall between [0, 1].\n  min_y, min_x, max_y, max_x = _clip_bbox(min_y, min_x, max_y, max_x)\n  min_y, min_x, max_y, max_x = _check_bbox_area(min_y, min_x, max_y, max_x)\n  return tf.stack([min_y, min_x, max_y, max_x])\n\n\ndef rotate_with_bboxes(image, bboxes, degrees, replace):\n  """"""Equivalent of PIL Rotate that rotates the image and bbox.\n\n  Args:\n    image: 3D uint8 Tensor.\n    bboxes: 2D Tensor that is a list of the bboxes in the image. Each bbox\n      has 4 elements (min_y, min_x, max_y, max_x) of type float.\n    degrees: Float, a scalar angle in degrees to rotate all images by. If\n      degrees is positive the image will be rotated clockwise otherwise it will\n      be rotated counterclockwise.\n    replace: A one or three value 1D tensor to fill empty pixels.\n\n  Returns:\n    A tuple containing a 3D uint8 Tensor that will be the result of rotating\n    image by degrees. The second element of the tuple is bboxes, where now\n    the coordinates will be shifted to reflect the rotated image.\n  """"""\n  # Rotate the image.\n  image = rotate(image, degrees, replace)\n\n  # Convert bbox coordinates to pixel values.\n  image_height = tf.shape(image)[0]\n  image_width = tf.shape(image)[1]\n  # pylint:disable=g-long-lambda\n  wrapped_rotate_bbox = lambda bbox: _rotate_bbox(\n      bbox, image_height, image_width, degrees)\n  # pylint:enable=g-long-lambda\n  bboxes = tf.map_fn(wrapped_rotate_bbox, bboxes)\n  return image, bboxes\n\n\ndef translate_x(image, pixels, replace):\n  """"""Equivalent of PIL Translate in X dimension.""""""\n  image = tf.contrib.image.translate(wrap(image), [-pixels, 0])\n  return unwrap(image, replace)\n\n\ndef translate_y(image, pixels, replace):\n  """"""Equivalent of PIL Translate in Y dimension.""""""\n  image = tf.contrib.image.translate(wrap(image), [0, -pixels])\n  return unwrap(image, replace)\n\n\ndef _shift_bbox(bbox, image_height, image_width, pixels, shift_horizontal):\n  """"""Shifts the bbox coordinates by pixels.\n\n  Args:\n    bbox: 1D Tensor that has 4 elements (min_y, min_x, max_y, max_x)\n      of type float that represents the normalized coordinates between 0 and 1.\n    image_height: Int, height of the image.\n    image_width: Int, width of the image.\n    pixels: An int. How many pixels to shift the bbox.\n    shift_horizontal: Boolean. If true then shift in X dimension else shift in\n      Y dimension.\n\n  Returns:\n    A tensor of the same shape as bbox, but now with the shifted coordinates.\n  """"""\n  pixels = tf.to_int32(pixels)\n  # Convert bbox to integer pixel locations.\n  min_y = tf.to_int32(tf.to_float(image_height) * bbox[0])\n  min_x = tf.to_int32(tf.to_float(image_width) * bbox[1])\n  max_y = tf.to_int32(tf.to_float(image_height) * bbox[2])\n  max_x = tf.to_int32(tf.to_float(image_width) * bbox[3])\n\n  if shift_horizontal:\n    min_x = tf.maximum(0, min_x - pixels)\n    max_x = tf.minimum(image_width, max_x - pixels)\n  else:\n    min_y = tf.maximum(0, min_y - pixels)\n    max_y = tf.minimum(image_height, max_y - pixels)\n\n  # Convert bbox back to floats.\n  min_y = tf.to_float(min_y) / tf.to_float(image_height)\n  min_x = tf.to_float(min_x) / tf.to_float(image_width)\n  max_y = tf.to_float(max_y) / tf.to_float(image_height)\n  max_x = tf.to_float(max_x) / tf.to_float(image_width)\n\n  # Clip the bboxes to be sure the fall between [0, 1].\n  min_y, min_x, max_y, max_x = _clip_bbox(min_y, min_x, max_y, max_x)\n  min_y, min_x, max_y, max_x = _check_bbox_area(min_y, min_x, max_y, max_x)\n  return tf.stack([min_y, min_x, max_y, max_x])\n\n\ndef translate_bbox(image, bboxes, pixels, replace, shift_horizontal):\n  """"""Equivalent of PIL Translate in X/Y dimension that shifts image and bbox.\n\n  Args:\n    image: 3D uint8 Tensor.\n    bboxes: 2D Tensor that is a list of the bboxes in the image. Each bbox\n      has 4 elements (min_y, min_x, max_y, max_x) of type float with values\n      between [0, 1].\n    pixels: An int. How many pixels to shift the image and bboxes\n    replace: A one or three value 1D tensor to fill empty pixels.\n    shift_horizontal: Boolean. If true then shift in X dimension else shift in\n      Y dimension.\n\n  Returns:\n    A tuple containing a 3D uint8 Tensor that will be the result of translating\n    image by pixels. The second element of the tuple is bboxes, where now\n    the coordinates will be shifted to reflect the shifted image.\n  """"""\n  if shift_horizontal:\n    image = translate_x(image, pixels, replace)\n  else:\n    image = translate_y(image, pixels, replace)\n\n  # Convert bbox coordinates to pixel values.\n  image_height = tf.shape(image)[0]\n  image_width = tf.shape(image)[1]\n  # pylint:disable=g-long-lambda\n  wrapped_shift_bbox = lambda bbox: _shift_bbox(\n      bbox, image_height, image_width, pixels, shift_horizontal)\n  # pylint:enable=g-long-lambda\n  bboxes = tf.map_fn(wrapped_shift_bbox, bboxes)\n  return image, bboxes\n\n\ndef shear_x(image, level, replace):\n  """"""Equivalent of PIL Shearing in X dimension.""""""\n  # Shear parallel to x axis is a projective transform\n  # with a matrix form of:\n  # [1  level\n  #  0  1].\n  image = tf.contrib.image.transform(\n      wrap(image), [1., level, 0., 0., 1., 0., 0., 0.])\n  return unwrap(image, replace)\n\n\ndef shear_y(image, level, replace):\n  """"""Equivalent of PIL Shearing in Y dimension.""""""\n  # Shear parallel to y axis is a projective transform\n  # with a matrix form of:\n  # [1  0\n  #  level  1].\n  image = tf.contrib.image.transform(\n      wrap(image), [1., 0., 0., level, 1., 0., 0., 0.])\n  return unwrap(image, replace)\n\n\ndef _shear_bbox(bbox, image_height, image_width, level, shear_horizontal):\n  """"""Shifts the bbox according to how the image was sheared.\n\n  Args:\n    bbox: 1D Tensor that has 4 elements (min_y, min_x, max_y, max_x)\n      of type float that represents the normalized coordinates between 0 and 1.\n    image_height: Int, height of the image.\n    image_width: Int, height of the image.\n    level: Float. How much to shear the image.\n    shear_horizontal: If true then shear in X dimension else shear in\n      the Y dimension.\n\n  Returns:\n    A tensor of the same shape as bbox, but now with the shifted coordinates.\n  """"""\n  image_height, image_width = (\n      tf.to_float(image_height), tf.to_float(image_width))\n\n  # Change bbox coordinates to be pixels.\n  min_y = tf.to_int32(image_height * bbox[0])\n  min_x = tf.to_int32(image_width * bbox[1])\n  max_y = tf.to_int32(image_height * bbox[2])\n  max_x = tf.to_int32(image_width * bbox[3])\n  coordinates = tf.stack(\n      [[min_y, min_x], [min_y, max_x], [max_y, min_x], [max_y, max_x]])\n  coordinates = tf.cast(coordinates, tf.float32)\n\n  # Shear the coordinates according to the translation matrix.\n  if shear_horizontal:\n    translation_matrix = tf.stack(\n        [[1, 0], [-level, 1]])\n  else:\n    translation_matrix = tf.stack(\n        [[1, -level], [0, 1]])\n  translation_matrix = tf.cast(translation_matrix, tf.float32)\n  new_coords = tf.cast(\n      tf.matmul(translation_matrix, tf.transpose(coordinates)), tf.int32)\n\n  # Find min/max values and convert them back to floats.\n  min_y = tf.to_float(tf.reduce_min(new_coords[0, :])) / image_height\n  min_x = tf.to_float(tf.reduce_min(new_coords[1, :])) / image_width\n  max_y = tf.to_float(tf.reduce_max(new_coords[0, :])) / image_height\n  max_x = tf.to_float(tf.reduce_max(new_coords[1, :])) / image_width\n\n  # Clip the bboxes to be sure the fall between [0, 1].\n  min_y, min_x, max_y, max_x = _clip_bbox(min_y, min_x, max_y, max_x)\n  min_y, min_x, max_y, max_x = _check_bbox_area(min_y, min_x, max_y, max_x)\n  return tf.stack([min_y, min_x, max_y, max_x])\n\n\ndef shear_with_bboxes(image, bboxes, level, replace, shear_horizontal):\n  """"""Applies Shear Transformation to the image and shifts the bboxes.\n\n  Args:\n    image: 3D uint8 Tensor.\n    bboxes: 2D Tensor that is a list of the bboxes in the image. Each bbox\n      has 4 elements (min_y, min_x, max_y, max_x) of type float with values\n      between [0, 1].\n    level: Float. How much to shear the image. This value will be between\n      -0.3 to 0.3.\n    replace: A one or three value 1D tensor to fill empty pixels.\n    shear_horizontal: Boolean. If true then shear in X dimension else shear in\n      the Y dimension.\n\n  Returns:\n    A tuple containing a 3D uint8 Tensor that will be the result of shearing\n    image by level. The second element of the tuple is bboxes, where now\n    the coordinates will be shifted to reflect the sheared image.\n  """"""\n  if shear_horizontal:\n    image = shear_x(image, level, replace)\n  else:\n    image = shear_y(image, level, replace)\n\n  # Convert bbox coordinates to pixel values.\n  image_height = tf.shape(image)[0]\n  image_width = tf.shape(image)[1]\n  # pylint:disable=g-long-lambda\n  wrapped_shear_bbox = lambda bbox: _shear_bbox(\n      bbox, image_height, image_width, level, shear_horizontal)\n  # pylint:enable=g-long-lambda\n  bboxes = tf.map_fn(wrapped_shear_bbox, bboxes)\n  return image, bboxes\n\n\ndef autocontrast(image):\n  """"""Implements Autocontrast function from PIL using TF ops.\n\n  Args:\n    image: A 3D uint8 tensor.\n\n  Returns:\n    The image after it has had autocontrast applied to it and will be of type\n    uint8.\n  """"""\n\n  def scale_channel(image):\n    """"""Scale the 2D image using the autocontrast rule.""""""\n    # A possibly cheaper version can be done using cumsum/unique_with_counts\n    # over the histogram values, rather than iterating over the entire image.\n    # to compute mins and maxes.\n    lo = tf.to_float(tf.reduce_min(image))\n    hi = tf.to_float(tf.reduce_max(image))\n\n    # Scale the image, making the lowest value 0 and the highest value 255.\n    def scale_values(im):\n      scale = 255.0 / (hi - lo)\n      offset = -lo * scale\n      im = tf.to_float(im) * scale + offset\n      im = tf.clip_by_value(im, 0.0, 255.0)\n      return tf.cast(im, tf.uint8)\n\n    result = tf.cond(hi > lo, lambda: scale_values(image), lambda: image)\n    return result\n\n  # Assumes RGB for now.  Scales each channel independently\n  # and then stacks the result.\n  s1 = scale_channel(image[:, :, 0])\n  s2 = scale_channel(image[:, :, 1])\n  s3 = scale_channel(image[:, :, 2])\n  image = tf.stack([s1, s2, s3], 2)\n  return image\n\n\ndef sharpness(image, factor):\n  """"""Implements Sharpness function from PIL using TF ops.""""""\n  orig_image = image\n  image = tf.cast(image, tf.float32)\n  # Make image 4D for conv operation.\n  image = tf.expand_dims(image, 0)\n  # SMOOTH PIL Kernel.\n  kernel = tf.constant(\n      [[1, 1, 1], [1, 5, 1], [1, 1, 1]], dtype=tf.float32,\n      shape=[3, 3, 1, 1]) / 13.\n  # Tile across channel dimension.\n  kernel = tf.tile(kernel, [1, 1, 3, 1])\n  strides = [1, 1, 1, 1]\n  degenerate = tf.nn.depthwise_conv2d(\n      image, kernel, strides, padding=\'VALID\', rate=[1, 1])\n  degenerate = tf.clip_by_value(degenerate, 0.0, 255.0)\n  degenerate = tf.squeeze(tf.cast(degenerate, tf.uint8), [0])\n\n  # For the borders of the resulting image, fill in the values of the\n  # original image.\n  mask = tf.ones_like(degenerate)\n  padded_mask = tf.pad(mask, [[1, 1], [1, 1], [0, 0]])\n  padded_degenerate = tf.pad(degenerate, [[1, 1], [1, 1], [0, 0]])\n  result = tf.where(tf.equal(padded_mask, 1), padded_degenerate, orig_image)\n\n  # Blend the final result.\n  return blend(result, orig_image, factor)\n\n\ndef equalize(image):\n  """"""Implements Equalize function from PIL using TF ops.""""""\n  def scale_channel(im, c):\n    """"""Scale the data in the channel to implement equalize.""""""\n    im = tf.cast(im[:, :, c], tf.int32)\n    # Compute the histogram of the image channel.\n    histo = tf.histogram_fixed_width(im, [0, 255], nbins=256)\n\n    # For the purposes of computing the step, filter out the nonzeros.\n    nonzero = tf.where(tf.not_equal(histo, 0))\n    nonzero_histo = tf.reshape(tf.gather(histo, nonzero), [-1])\n    step = (tf.reduce_sum(nonzero_histo) - nonzero_histo[-1]) // 255\n\n    def build_lut(histo, step):\n      # Compute the cumulative sum, shifting by step // 2\n      # and then normalization by step.\n      lut = (tf.cumsum(histo) + (step // 2)) // step\n      # Shift lut, prepending with 0.\n      lut = tf.concat([[0], lut[:-1]], 0)\n      # Clip the counts to be in range.  This is done\n      # in the C code for image.point.\n      return tf.clip_by_value(lut, 0, 255)\n\n    # If step is zero, return the original image.  Otherwise, build\n    # lut from the full histogram and step and then index from it.\n    result = tf.cond(tf.equal(step, 0),\n                     lambda: im,\n                     lambda: tf.gather(build_lut(histo, step), im))\n\n    return tf.cast(result, tf.uint8)\n\n  # Assumes RGB for now.  Scales each channel independently\n  # and then stacks the result.\n  s1 = scale_channel(image, 0)\n  s2 = scale_channel(image, 1)\n  s3 = scale_channel(image, 2)\n  image = tf.stack([s1, s2, s3], 2)\n  return image\n\n\ndef wrap(image):\n  """"""Returns \'image\' with an extra channel set to all 1s.""""""\n  shape = tf.shape(image)\n  extended_channel = tf.ones([shape[0], shape[1], 1], image.dtype)\n  extended = tf.concat([image, extended_channel], 2)\n  return extended\n\n\ndef unwrap(image, replace):\n  """"""Unwraps an image produced by wrap.\n\n  Where there is a 0 in the last channel for every spatial position,\n  the rest of the three channels in that spatial dimension are grayed\n  (set to 128).  Operations like translate and shear on a wrapped\n  Tensor will leave 0s in empty locations.  Some transformations look\n  at the intensity of values to do preprocessing, and we want these\n  empty pixels to assume the \'average\' value, rather than pure black.\n\n\n  Args:\n    image: A 3D Image Tensor with 4 channels.\n    replace: A one or three value 1D tensor to fill empty pixels.\n\n  Returns:\n    image: A 3D image Tensor with 3 channels.\n  """"""\n  image_shape = tf.shape(image)\n  # Flatten the spatial dimensions.\n  flattened_image = tf.reshape(image, [-1, image_shape[2]])\n\n  # Find all pixels where the last channel is zero.\n  alpha_channel = flattened_image[:, 3]\n\n  replace = tf.concat([replace, tf.ones([1], image.dtype)], 0)\n\n  # Where they are zero, fill them in with \'replace\'.\n  flattened_image = tf.where(\n      tf.equal(alpha_channel, 0),\n      tf.ones_like(flattened_image, dtype=image.dtype) * replace,\n      flattened_image)\n\n  image = tf.reshape(flattened_image, image_shape)\n  image = tf.slice(image, [0, 0, 0], [image_shape[0], image_shape[1], 3])\n  return image\n\n\ndef _cutout_inside_bbox(image, bbox, pad_fraction):\n  """"""Generates cutout mask and the mean pixel value of the bbox.\n\n  First a location is randomly chosen within the image as the center where the\n  cutout mask will be applied. Note this can be towards the boundaries of the\n  image, so the full cutout mask may not be applied.\n\n  Args:\n    image: 3D uint8 Tensor.\n    bbox: 1D Tensor that has 4 elements (min_y, min_x, max_y, max_x)\n      of type float that represents the normalized coordinates between 0 and 1.\n    pad_fraction: Float that specifies how large the cutout mask should be in\n      in reference to the size of the original bbox. If pad_fraction is 0.25,\n      then the cutout mask will be of shape\n      (0.25 * bbox height, 0.25 * bbox width).\n\n  Returns:\n    A tuple. Fist element is a tensor of the same shape as image where each\n    element is either a 1 or 0 that is used to determine where the image\n    will have cutout applied. The second element is the mean of the pixels\n    in the image where the bbox is located.\n  """"""\n  image_height = tf.shape(image)[0]\n  image_width = tf.shape(image)[1]\n  # Transform from shape [1, 4] to [4].\n  bbox = tf.squeeze(bbox)\n\n  min_y = tf.to_int32(tf.to_float(image_height) * bbox[0])\n  min_x = tf.to_int32(tf.to_float(image_width) * bbox[1])\n  max_y = tf.to_int32(tf.to_float(image_height) * bbox[2])\n  max_x = tf.to_int32(tf.to_float(image_width) * bbox[3])\n\n  # Calculate the mean pixel values in the bounding box, which will be used\n  # to fill the cutout region.\n  mean = tf.reduce_mean(image[min_y:max_y + 1, min_x:max_x + 1],\n                        reduction_indices=[0, 1])\n\n  # Cutout mask will be size pad_size_heigh * 2 by pad_size_width * 2 if the\n  # region lies entirely within the bbox.\n  box_height = max_y - min_y + 1\n  box_width = max_x - min_x + 1\n  pad_size_height = tf.to_int32(pad_fraction * (box_height / 2))\n  pad_size_width = tf.to_int32(pad_fraction * (box_width / 2))\n\n  # Sample the center location in the image where the zero mask will be applied.\n  cutout_center_height = tf.random_uniform(\n      shape=[], minval=min_y, maxval=max_y+1,\n      dtype=tf.int32)\n\n  cutout_center_width = tf.random_uniform(\n      shape=[], minval=min_x, maxval=max_x+1,\n      dtype=tf.int32)\n\n  lower_pad = tf.maximum(\n      0, cutout_center_height - pad_size_height)\n  upper_pad = tf.maximum(\n      0, image_height - cutout_center_height - pad_size_height)\n  left_pad = tf.maximum(\n      0, cutout_center_width - pad_size_width)\n  right_pad = tf.maximum(\n      0, image_width - cutout_center_width - pad_size_width)\n\n  cutout_shape = [image_height - (lower_pad + upper_pad),\n                  image_width - (left_pad + right_pad)]\n  padding_dims = [[lower_pad, upper_pad], [left_pad, right_pad]]\n\n  mask = tf.pad(\n      tf.zeros(cutout_shape, dtype=image.dtype),\n      padding_dims, constant_values=1)\n\n  mask = tf.expand_dims(mask, 2)\n  mask = tf.tile(mask, [1, 1, 3])\n\n  return mask, mean\n\n\ndef bbox_cutout(image, bboxes, pad_fraction, replace_with_mean):\n  """"""Applies cutout to the image according to bbox information.\n\n  This is a cutout variant that using bbox information to make more informed\n  decisions on where to place the cutout mask.\n\n  Args:\n    image: 3D uint8 Tensor.\n    bboxes: 2D Tensor that is a list of the bboxes in the image. Each bbox\n      has 4 elements (min_y, min_x, max_y, max_x) of type float with values\n      between [0, 1].\n    pad_fraction: Float that specifies how large the cutout mask should be in\n      in reference to the size of the original bbox. If pad_fraction is 0.25,\n      then the cutout mask will be of shape\n      (0.25 * bbox height, 0.25 * bbox width).\n    replace_with_mean: Boolean that specified what value should be filled in\n      where the cutout mask is applied. Since the incoming image will be of\n      uint8 and will not have had any mean normalization applied, by default\n      we set the value to be 128. If replace_with_mean is True then we find\n      the mean pixel values across the channel dimension and use those to fill\n      in where the cutout mask is applied.\n\n  Returns:\n    A tuple. First element is a tensor of the same shape as image that has\n    cutout applied to it. Second element is the bboxes that were passed in\n    that will be unchanged.\n  """"""\n  def apply_bbox_cutout(image, bboxes, pad_fraction):\n    """"""Applies cutout to a single bounding box within image.""""""\n    # Choose a single bounding box to apply cutout to.\n    random_index = tf.random_uniform(\n        shape=[], maxval=tf.shape(bboxes)[0], dtype=tf.int32)\n    # Select the corresponding bbox and apply cutout.\n    chosen_bbox = tf.gather(bboxes, random_index)\n    mask, mean = _cutout_inside_bbox(image, chosen_bbox, pad_fraction)\n\n    # When applying cutout we either set the pixel value to 128 or to the mean\n    # value inside the bbox.\n    replace = mean if replace_with_mean else 128\n\n    # Apply the cutout mask to the image. Where the mask is 0 we fill it with\n    # `replace`.\n    image = tf.where(\n        tf.equal(mask, 0),\n        tf.cast(tf.ones_like(image, dtype=image.dtype) * replace,\n                dtype=image.dtype),\n        image)\n    return image\n\n  # Check to see if there are boxes, if so then apply boxcutout.\n  image = tf.cond(tf.equal(tf.size(bboxes), 0), lambda: image,\n                  lambda: apply_bbox_cutout(image, bboxes, pad_fraction))\n\n  return image, bboxes\n\ndef shear_wrapper(shear_horizontal):\n    def _wrapper(image, bboxes, level, replace):\n        return shear_with_bboxes(image, bboxes, level, replace, shear_horizontal=shear_horizontal)\n\n    return _wrapper\n\ndef translate_wrapper(shift_horizontal):\n    def _wrapper(image, bboxes, pixels, replace):\n        return translate_bbox(\n            image, bboxes, pixels, replace, shift_horizontal=shift_horizontal)\n\n    return _wrapper\n\n\nNAME_TO_FUNC = {\n    \'AutoContrast\':\n    autocontrast,\n    \'Equalize\':\n    equalize,\n    \'Posterize\':\n    posterize,\n    \'Solarize\':\n    solarize,\n    \'SolarizeAdd\':\n    solarize_add,\n    \'Color\':\n    color,\n    \'Contrast\':\n    contrast,\n    \'Brightness\':\n    brightness,\n    \'Sharpness\':\n    sharpness,\n    \'Cutout\':\n    cutout,\n    \'BBox_Cutout\':\n    bbox_cutout,\n    \'Rotate_BBox\':\n    rotate_with_bboxes,\n    # pylint:disable=g-long-lambda\n    \'TranslateX_BBox\':\n        translate_wrapper(True),\n    \'TranslateY_BBox\':\n        translate_wrapper(False),\n    \'ShearX_BBox\':\n        shear_wrapper(True),\n    \'ShearY_BBox\':\n        shear_wrapper(False),\n    # pylint:enable=g-long-lambda\n    \'Rotate_Only_BBoxes\':\n    rotate_only_bboxes,\n    \'ShearX_Only_BBoxes\':\n    shear_x_only_bboxes,\n    \'ShearY_Only_BBoxes\':\n    shear_y_only_bboxes,\n    \'TranslateX_Only_BBoxes\':\n    translate_x_only_bboxes,\n    \'TranslateY_Only_BBoxes\':\n    translate_y_only_bboxes,\n    \'Flip_Only_BBoxes\':\n    flip_only_bboxes,\n    \'Solarize_Only_BBoxes\':\n    solarize_only_bboxes,\n    \'Equalize_Only_BBoxes\':\n    equalize_only_bboxes,\n    \'Cutout_Only_BBoxes\':\n    cutout_only_bboxes,\n}\n\ndef _randomly_negate_tensor(tensor):\n  """"""With 50% prob turn the tensor negative.""""""\n  should_flip = tf.cast(tf.floor(tf.random_uniform([]) + 0.5), tf.bool)\n  final_tensor = tf.cond(should_flip, lambda: tensor, lambda: -tensor)\n  return final_tensor\n\n\ndef _rotate_level_to_arg(level):\n  level = (level/_MAX_LEVEL) * 30.\n  level = _randomly_negate_tensor(level)\n  return (level,)\n\n\ndef _shrink_level_to_arg(level):\n  """"""Converts level to ratio by which we shrink the image content.""""""\n  if level == 0:\n    return (1.0,)  # if level is zero, do not shrink the image\n  # Maximum shrinking ratio is 2.9.\n  level = 2. / (_MAX_LEVEL / level) + 0.9\n  return (level,)\n\n\ndef _enhance_level_to_arg(level):\n  return ((level/_MAX_LEVEL) * 1.8 + 0.1,)\n\n\ndef _shear_level_to_arg(level):\n  level = (level/_MAX_LEVEL) * 0.3\n  # Flip level to negative with 50% chance.\n  level = _randomly_negate_tensor(level)\n  return (level,)\n\n\ndef _translate_level_to_arg(level, translate_const):\n  level = (level/_MAX_LEVEL) * float(translate_const)\n  # Flip level to negative with 50% chance.\n  level = _randomly_negate_tensor(level)\n  return (level,)\n\n\ndef _bbox_cutout_level_to_arg(level, hparams):\n  cutout_pad_fraction = (level/_MAX_LEVEL) * hparams.cutout_max_pad_fraction\n  return (cutout_pad_fraction,\n          hparams.cutout_bbox_replace_with_mean)\n\n\ndef level_to_arg(hparams):\n    def _return_empty(level):\n        return ()\n\n    def _translate_level_to_arg_wrapper(level):\n        return _translate_level_to_arg(level, hparams.translate_bbox_const)\n\n    def _level_wrapper(num):\n        def _wrapper(level):\n            return (int((level / _MAX_LEVEL) * num),)\n        return _wrapper\n\n    def _cutout_wrapper(level):\n        return _bbox_cutout_level_to_arg(level,hparams)\n\n    return {\n        \'AutoContrast\':_return_empty,\n        \'Equalize\':_return_empty,\n        \'Posterize\':\n            _level_wrapper(4),\n        \'Solarize\':\n            _level_wrapper(256),\n        \'SolarizeAdd\':\n            _level_wrapper(110),\n        \'Color\':\n        _enhance_level_to_arg,\n        \'Contrast\':\n        _enhance_level_to_arg,\n        \'Brightness\':\n        _enhance_level_to_arg,\n        \'Sharpness\':\n        _enhance_level_to_arg,\n        \'Cutout\':\n            _level_wrapper(hparams.cutout_const),\n        # pylint:disable=g-long-lambda\n        \'BBox_Cutout\':\n            _cutout_wrapper,\n        \'TranslateX_BBox\':\n            _translate_level_to_arg_wrapper,\n        \'TranslateY_BBox\':\n            _translate_level_to_arg_wrapper,\n        # pylint:enable=g-long-lambda\n        \'ShearX_BBox\':\n        _shear_level_to_arg,\n        \'ShearY_BBox\':\n        _shear_level_to_arg,\n        \'Rotate_BBox\':\n        _rotate_level_to_arg,\n        \'Rotate_Only_BBoxes\':\n        _rotate_level_to_arg,\n        \'ShearX_Only_BBoxes\':\n        _shear_level_to_arg,\n        \'ShearY_Only_BBoxes\':\n        _shear_level_to_arg,\n        # pylint:disable=g-long-lambda\n        \'TranslateX_Only_BBoxes\':\n            _translate_level_to_arg_wrapper,\n        \'TranslateY_Only_BBoxes\':\n            _translate_level_to_arg_wrapper,\n        # pylint:enable=g-long-lambda\n        \'Flip_Only_BBoxes\':\n            _return_empty,\n        \'Solarize_Only_BBoxes\':\n            _level_wrapper(256),\n        \'Equalize_Only_BBoxes\':\n            _return_empty,\n        # pylint:disable=g-long-lambda\n        \'Cutout_Only_BBoxes\':\n            _level_wrapper(hparams.cutout_const),\n        # pylint:enable=g-long-lambda\n    }\n\n\ndef bbox_wrapper(func):\n  """"""Adds a bboxes function argument to func and returns unchanged bboxes.""""""\n  def wrapper(images, bboxes, *args, **kwargs):\n    return (func(images, *args, **kwargs), bboxes)\n  return wrapper\n\n\ndef _parse_policy_info(name, prob, level, replace_value, augmentation_hparams):\n  """"""Return the function that corresponds to `name` and update `level` param.""""""\n  func = NAME_TO_FUNC[name]\n  args = level_to_arg(augmentation_hparams)[name](level)\n\n  # Check to see if prob is passed into function. This is used for operations\n  # where we alter bboxes independently.\n  # pytype:disable=wrong-arg-types\n  if \'prob\' in inspect.getargspec(func)[0]:\n    args = tuple([prob] + list(args))\n  # pytype:enable=wrong-arg-types\n\n  # Add in replace arg if it is required for the function that is being called.\n  if \'replace\' in inspect.getargspec(func)[0]:\n    # Make sure replace is the final argument\n    assert \'replace\' == inspect.getargspec(func)[0][-1]\n    args = tuple(list(args) + [replace_value])\n\n  # Add bboxes as the second positional argument for the function if it does\n  # not already exist.\n  if \'bboxes\' not in inspect.getargspec(func)[0]:\n    func = bbox_wrapper(func)\n  return (func, prob, args)\n\n\ndef _apply_func_with_prob(func, image, args, prob, bboxes):\n  """"""Apply `func` to image w/ `args` as input with probability `prob`.""""""\n  assert isinstance(args, tuple)\n  assert \'bboxes\' == inspect.getargspec(func)[0][1]\n\n  # If prob is a function argument, then this randomness is being handled\n  # inside the function, so make sure it is always called.\n  if \'prob\' in inspect.getargspec(func)[0]:\n    prob = 1.0\n\n  # Apply the function with probability `prob`.\n  should_apply_op = tf.cast(\n      tf.floor(tf.random_uniform([], dtype=tf.float32) + prob), tf.bool)\n  augmented_image, augmented_bboxes = tf.cond(\n      should_apply_op,\n      lambda: func(image, bboxes, *args),\n      lambda: (image, bboxes))\n  return augmented_image, augmented_bboxes\n\n\ndef select_and_apply_random_policy(policies, image, bboxes):\n  """"""Select a random policy from `policies` and apply it to `image`.""""""\n  policy_to_select = tf.random_uniform([], maxval=len(policies), dtype=tf.int32)\n  # Note that using tf.case instead of tf.conds would result in significantly\n  # larger graphs and would even break export for some larger policies.\n  for (i, policy) in enumerate(policies):\n    image, bboxes = tf.cond(\n        tf.equal(i, policy_to_select),\n        lambda selected_policy=policy: selected_policy(image, bboxes),\n        lambda: (image, bboxes))\n  return (image, bboxes)\n\n\ndef build_and_apply_nas_policy(policies, image, bboxes,\n                               augmentation_hparams):\n  """"""Build a policy from the given policies passed in and apply to image.\n\n  Args:\n    policies: list of lists of tuples in the form `(func, prob, level)`, `func`\n      is a string name of the augmentation function, `prob` is the probability\n      of applying the `func` operation, `level` is the input argument for\n      `func`.\n    image: tf.Tensor that the resulting policy will be applied to.\n    bboxes:\n    augmentation_hparams: Hparams associated with the NAS learned policy.\n\n  Returns:\n    A version of image that now has data augmentation applied to it based on\n    the `policies` pass into the function. Additionally, returns bboxes if\n    a value for them is passed in that is not None\n  """"""\n  replace_value = [128, 128, 128]\n\n  # func is the string name of the augmentation function, prob is the\n  # probability of applying the operation and level is the parameter associated\n  # with the tf op.\n\n  # tf_policies are functions that take in an image and return an augmented\n  # image.\n  tf_policies = []\n  for policy in policies:\n    tf_policy = []\n    # Link string name to the correct python function and make sure the correct\n    # argument is passed into that function.\n    for policy_info in policy:\n      policy_info = list(policy_info) + [replace_value, augmentation_hparams]\n\n      tf_policy.append(_parse_policy_info(*policy_info))\n    # Now build the tf policy that will apply the augmentation procedue\n    # on image.\n    def make_final_policy(tf_policy_):\n      def final_policy(image_, bboxes_):\n        for func, prob, args in tf_policy_:\n          image_, bboxes_ = _apply_func_with_prob(\n              func, image_, args, prob, bboxes_)\n        return image_, bboxes_\n      return final_policy\n    tf_policies.append(make_final_policy(tf_policy))\n\n  augmented_images, augmented_bboxes = select_and_apply_random_policy(\n      tf_policies, image, bboxes)\n  # If no bounding boxes were specified, then just return the images.\n  return (augmented_images, augmented_bboxes)\n\n\n# TODO(barretzoph): Add in ArXiv link once paper is out.\ndef distort_image_with_autoaugment(image, bboxes, augmentation_name):\n  """"""Applies the AutoAugment policy to `image` and `bboxes`.\n\n  Args:\n    image: `Tensor` of shape [height, width, 3] representing an image.\n    bboxes: `Tensor` of shape [N, 4] representing ground truth boxes that are\n      normalized between [0, 1].\n    augmentation_name: The name of the AutoAugment policy to use. The available\n      options are `v0`, `v1`, `v2`, `v3` and `test`. `v0` is the policy used for\n      all of the results in the paper and was found to achieve the best results\n      on the COCO dataset. `v1`, `v2` and `v3` are additional good policies\n      found on the COCO dataset that have slight variation in what operations\n      were used during the search procedure along with how many operations are\n      applied in parallel to a single image (2 vs 3).\n\n  Returns:\n    A tuple containing the augmented versions of `image` and `bboxes`.\n  """"""\n  available_policies = {\'v0\': policy_v0, \'v1\': policy_v1, \'v2\': policy_v2,\n                        \'v3\': policy_v3, \'test\': policy_vtest}\n  if augmentation_name not in available_policies:\n    raise ValueError(\'Invalid augmentation_name: {}\'.format(augmentation_name))\n\n  policy = available_policies[augmentation_name]()\n  # Hparams that will be used for AutoAugment.\n  augmentation_hparams = tf.contrib.training.HParams(\n      cutout_max_pad_fraction=0.75, cutout_bbox_replace_with_mean=False,\n      cutout_const=100, translate_const=250, cutout_bbox_const=50,\n      translate_bbox_const=120)\n\n  return build_and_apply_nas_policy(policy, image, bboxes, augmentation_hparams)\n'"
yolo3/darknet.py,10,"b'from functools import wraps\nimport tensorflow as tf\nfrom yolo3.utils import compose\n\n\n@wraps(tf.keras.layers.Conv2D)\ndef DarknetConv2D(*args, **kwargs):\n    """"""Wrapper to set Darknet parameters for Convolution2D.""""""\n    # darknet_conv_kwargs = {\'kernel_regularizer\': tf.keras.regularizers.l2(5e-4)}\n    darknet_conv_kwargs = {}\n    darknet_conv_kwargs[\'padding\'] = \'valid\' if kwargs.get(\'strides\') == (\n        2, 2) else \'same\'\n    darknet_conv_kwargs.update(kwargs)\n    return tf.keras.layers.Conv2D(*args, **darknet_conv_kwargs)\n\n\ndef DarknetConv2D_BN_Leaky(*args, **kwargs):\n    """"""Darknet Convolution2D followed by BatchNormalization and LeakyReLU.""""""\n    no_bias_kwargs = {\'use_bias\': False}\n    no_bias_kwargs.update(kwargs)\n    return compose(DarknetConv2D(*args, **no_bias_kwargs),\n                   tf.keras.layers.BatchNormalization(),\n                   tf.keras.layers.LeakyReLU(alpha=0.1))\n\n\ndef resblock_body(x, num_filters, num_blocks):\n    \'\'\'A series of resblocks starting with a downsampling Convolution2D\'\'\'\n    # Darknet uses left and top padding instead of \'same\' mode\n    x = tf.keras.layers.ZeroPadding2D(((1, 0), (1, 0)))(x)\n    x = DarknetConv2D_BN_Leaky(num_filters, (3, 3), strides=(2, 2))(x)\n    for i in range(num_blocks):\n        y = compose(DarknetConv2D_BN_Leaky(num_filters // 2, (1, 1)),\n                    DarknetConv2D_BN_Leaky(num_filters, (3, 3)))(x)\n        x = tf.keras.layers.Add()([x, y])\n    return x\n\n\ndef darknet_body(inputs, include_top=True, classes=1000):\n    \'\'\'Darknent body having 52 Convolution2D layers\'\'\'\n    x = DarknetConv2D_BN_Leaky(32, (3, 3))(inputs)\n    x = resblock_body(x, 64, 1)\n    x = resblock_body(x, 128, 2)\n    x = resblock_body(x, 256, 8)\n    x = resblock_body(x, 512, 8)\n    x = resblock_body(x, 1024, 4)\n    if include_top:\n        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n        x = tf.keras.layers.Dense(classes,\n                                  activation=\'softmax\',\n                                  use_bias=True,\n                                  name=\'Logits\')(x)\n    return tf.keras.Model(inputs, x)\n'"
yolo3/data.py,27,"b""import tensorflow as tf\nfrom functools import reduce\nfrom yolo3.utils import get_random_data, preprocess_true_boxes\nfrom yolo3.enums import DATASET_MODE\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\n\nclass Dataset(object):\n\n    def parse_tfrecord(self, example_proto):\n        feature_description = {\n            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n            'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n            'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n            'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n            'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n            'image/object/bbox/label': tf.io.VarLenFeature(tf.int64)\n        }\n        features = tf.io.parse_single_example(example_proto,\n                                              feature_description)\n        image = tf.image.decode_image(features['image/encoded'],\n                                      channels=3,\n                                      dtype=tf.float32)\n        image.set_shape([None, None, 3])\n        xmins = features['image/object/bbox/xmin'].values\n        xmaxs = features['image/object/bbox/xmax'].values\n        ymins = features['image/object/bbox/ymin'].values\n        ymaxs = features['image/object/bbox/ymax'].values\n        labels = features['image/object/bbox/label'].values\n        image, bbox = get_random_data(image,\n                                      xmins,\n                                      xmaxs,\n                                      ymins,\n                                      ymaxs,\n                                      labels,\n                                      self.input_shape,\n                                      train=self.mode == DATASET_MODE.TRAIN)\n        y1, y2, y3 = tf.py_function(\n            preprocess_true_boxes,\n            [bbox, self.input_shape, self.anchors, self.num_classes],\n            [tf.float32, tf.float32, tf.float32])\n        y1.set_shape([None, None, len(self.anchors) // 3, self.num_classes + 5])\n        y2.set_shape([None, None, len(self.anchors) // 3, self.num_classes + 5])\n        y3.set_shape([None, None, len(self.anchors) // 3, self.num_classes + 5])\n\n        return image, (y1, y2, y3)\n\n    def parse_text(self, line):\n        values = tf.strings.split([line], ' ').values\n        image = tf.image.decode_image(tf.io.read_file(values[0]),\n                                      channels=3,\n                                      dtype=tf.float32)\n        image.set_shape([None, None, 3])\n        reshaped_data = tf.reshape(values[1:], [-1, 5])\n        xmins = tf.strings.to_number(reshaped_data[:, 0], tf.float32)\n        xmaxs = tf.strings.to_number(reshaped_data[:, 2], tf.float32)\n        ymins = tf.strings.to_number(reshaped_data[:, 1], tf.float32)\n        ymaxs = tf.strings.to_number(reshaped_data[:, 3], tf.float32)\n        labels = tf.strings.to_number(reshaped_data[:, 4], tf.int64)\n\n        image, bbox = get_random_data(image,\n                                      xmins,\n                                      xmaxs,\n                                      ymins,\n                                      ymaxs,\n                                      labels,\n                                      self.input_shape,\n                                      train=self.mode == DATASET_MODE.TRAIN)\n        y1, y2, y3 = tf.py_function(\n            preprocess_true_boxes,\n            [bbox, self.input_shape, self.anchors, self.num_classes],\n            [tf.float32, tf.float32, tf.float32])\n        y1.set_shape([None, None, len(self.anchors) // 3, self.num_classes + 5])\n        y2.set_shape([None, None, len(self.anchors) // 3, self.num_classes + 5])\n        y3.set_shape([None, None, len(self.anchors) // 3, self.num_classes + 5])\n\n        return image, (y1, y2, y3)\n\n    def _dataset_internal(self, files, dataset_builder, parser):\n        dataset = tf.data.Dataset.from_tensor_slices(files)\n        if self.mode == DATASET_MODE.TRAIN:\n            train_num = reduce(\n                lambda x, y: x + y,\n                map(lambda file: int(self._get_num_from_name(file)), files))\n            dataset = dataset.interleave(\n                lambda file: dataset_builder(file),\n                cycle_length=AUTOTUNE,\n                num_parallel_calls=AUTOTUNE).shuffle(train_num).map(\n                    parser, num_parallel_calls=AUTOTUNE).prefetch(\n                        self.batch_size).batch(self.batch_size)\n        elif self.mode == DATASET_MODE.VALIDATE:\n            dataset = dataset.interleave(\n                lambda file: dataset_builder(file),\n                cycle_length=AUTOTUNE,\n                num_parallel_calls=AUTOTUNE).map(\n                    parser, num_parallel_calls=AUTOTUNE).prefetch(\n                        self.batch_size).batch(self.batch_size)\n        elif self.mode == DATASET_MODE.TEST:\n            dataset = dataset.interleave(\n                lambda file: dataset_builder(file),\n                cycle_length=AUTOTUNE,\n                num_parallel_calls=AUTOTUNE).map(\n                    parser, num_parallel_calls=AUTOTUNE).prefetch(\n                        self.batch_size).batch(self.batch_size)\n        return dataset\n\n    def __init__(self,\n                 glob_path: str,\n                 batch_size: int,\n                 anchors=None,\n                 num_classes=None,\n                 input_shape=None,\n                 mode=DATASET_MODE.TRAIN):\n        self.glob_path = glob_path\n        self.batch_size = batch_size\n        self.input_shape = input_shape\n        self.anchors = anchors\n        self.num_classes = num_classes\n        self.mode = mode\n\n    def _get_num_from_name(self, name):\n        return int(name.split('/')[-1].split('.')[0].split('_')[-1])\n\n    def build(self, split=None):\n        if self.glob_path is None:\n            return None,0\n        files = tf.io.gfile.glob(self.glob_path)\n        if len(files) == 0:\n            raise ValueError('No file found')\n        try:\n            num = reduce(lambda x, y: x + y,\n                         map(lambda file: self._get_num_from_name(file), files))\n        except Exception:\n            raise ValueError(\n                'Please format file name like <name>_<number>.<extension>')\n        else:\n            tfrecords = list(\n                filter(lambda file: file.endswith('.tfrecords'), files))\n            txts = list(filter(lambda file: file.endswith('.txt'), files))\n            if len(tfrecords) > 0:\n                tfrecords_dataset = self._dataset_internal(\n                    tfrecords, tf.data.TFRecordDataset, self.parse_tfrecord)\n            if len(txts) > 0:\n                txts_dataset = self._dataset_internal(txts,\n                                                      tf.data.TextLineDataset,\n                                                      self.parse_text)\n            if len(tfrecords) > 0 and len(txts) > 0:\n                return tfrecords_dataset.concatenate(txts_dataset), num\n            elif len(tfrecords) > 0:\n                return tfrecords_dataset, num\n            elif len(txts) > 0:\n                return txts_dataset, num\n"""
yolo3/efficientnet.py,48,"b'# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains definitions for EfficientNet model.\n\n[1] Mingxing Tan, Quoc V. Le\n  EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.\n  ICML\'19, https://arxiv.org/abs/1905.11946\n""""""\n\nimport re\nimport collections\nimport math\nimport numpy as np\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\n\n__all__ = [\n    \'EfficientNet\', \'EfficientNetB0\', \'EfficientNetB1\', \'EfficientNetB2\',\n    \'EfficientNetB3\', \'EfficientNetB4\', \'EfficientNetB5\', \'EfficientNetB6\',\n    \'EfficientNetB7\'\n]\n\nIMAGENET_WEIGHTS = {\n    \'efficientnet-b0\': {\n        \'name\': \'efficientnet-b0_imagenet_1000.h5\',\n        \'url\':\n        \'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000.h5\',\n        \'md5\': \'bca04d16b1b8a7c607b1152fe9261af7\',\n    },\n    \'efficientnet-b0-notop\': {\n        \'name\': \'efficientnet-b0_imagenet_1000_notop.h5\',\n        \'url\':\n        \'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b0_imagenet_1000_notop.h5\',\n        \'md5\': \'45d2f3b6330c2401ef66da3961cad769\',\n    },\n    \'efficientnet-b1\': {\n        \'name\': \'efficientnet-b1_imagenet_1000.h5\',\n        \'url\':\n        \'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b1_imagenet_1000.h5\',\n        \'md5\': \'bd4a2b82f6f6bada74fc754553c464fc\',\n    },\n    \'efficientnet-b1-notop\': {\n        \'name\': \'efficientnet-b1_imagenet_1000_notop.h5\',\n        \'url\':\n        \'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b1_imagenet_1000_notop.h5\',\n        \'md5\': \'884aed586c2d8ca8dd15a605ec42f564\',\n    },\n    \'efficientnet-b2\': {\n        \'name\': \'efficientnet-b2_imagenet_1000.h5\',\n        \'url\':\n        \'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b2_imagenet_1000.h5\',\n        \'md5\': \'45b28b26f15958bac270ab527a376999\',\n    },\n    \'efficientnet-b2-notop\': {\n        \'name\': \'efficientnet-b2_imagenet_1000_notop.h5\',\n        \'url\':\n        \'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b2_imagenet_1000_notop.h5\',\n        \'md5\': \'42fb9f2d9243d461d62b4555d3a53b7b\',\n    },\n    \'efficientnet-b3\': {\n        \'name\': \'efficientnet-b3_imagenet_1000.h5\',\n        \'url\':\n        \'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b3_imagenet_1000.h5\',\n        \'md5\': \'decd2c8a23971734f9d3f6b4053bf424\',\n    },\n    \'efficientnet-b3-notop\': {\n        \'name\': \'efficientnet-b3_imagenet_1000_notop.h5\',\n        \'url\':\n        \'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b3_imagenet_1000_notop.h5\',\n        \'md5\': \'1f7d9a8c2469d2e3d3b97680d45df1e1\',\n    },\n    \'efficientnet-b4\': {\n        \'name\': \'efficientnet-b4_imagenet_1000.h5\',\n        \'url\':\n        \'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b4_imagenet_1000.h5\',\n        \'md5\': \'01df77157a86609530aeb4f1f9527949\',\n    },\n    \'efficientnet-b4-notop\': {\n        \'name\': \'efficientnet-b4_imagenet_1000_notop.h5\',\n        \'url\':\n        \'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b4_imagenet_1000_notop.h5\',\n        \'md5\': \'e7c3b780f050f8f49c800f23703f285c\',\n    },\n    \'efficientnet-b5\': {\n        \'name\': \'efficientnet-b5_imagenet_1000.h5\',\n        \'url\':\n        \'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b5_imagenet_1000.h5\',\n        \'md5\': \'c31311a1a38b5111e14457145fccdf32\',\n    },\n    \'efficientnet-b5-notop\': {\n        \'name\': \'efficientnet-b5_imagenet_1000_notop.h5\',\n        \'url\':\n        \'https://github.com/qubvel/efficientnet/releases/download/v0.0.1/efficientnet-b5_imagenet_1000_notop.h5\',\n        \'md5\': \'a09b36129b41196e0bb659fd84fbdd5f\',\n    },\n}\n\nGlobalParams = collections.namedtuple(\'GlobalParams\', [\n    \'batch_norm_momentum\',\n    \'batch_norm_epsilon\',\n    \'dropout_rate\',\n    \'data_format\',\n    \'num_classes\',\n    \'width_coefficient\',\n    \'depth_coefficient\',\n    \'depth_divisor\',\n    \'min_depth\',\n    \'drop_connect_rate\',\n])\nGlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\n\nBlockArgs = collections.namedtuple(\'BlockArgs\', [\n    \'kernel_size\', \'num_repeat\', \'input_filters\', \'output_filters\',\n    \'expand_ratio\', \'id_skip\', \'strides\', \'se_ratio\'\n])\n# defaults will be a public argument for namedtuple in Python 3.7\n# https://docs.python.org/3/library/collections.html#collections.namedtuple\nBlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n\n\nclass BlockDecoder(object):\n    """"""Block Decoder for readability.""""""\n\n    def _decode_block_string(self, block_string):\n        """"""Gets a block through a string notation of arguments.""""""\n        assert isinstance(block_string, str)\n        ops = block_string.split(\'_\')\n        options = {}\n        for op in ops:\n            splits = re.split(r\'(\\d.*)\', op)\n            if len(splits) >= 2:\n                key, value = splits[:2]\n                options[key] = value\n\n        if \'s\' not in options or len(options[\'s\']) != 2:\n            raise ValueError(\'Strides options should be a pair of integers.\')\n\n        return BlockArgs(\n            kernel_size=int(options[\'k\']),\n            num_repeat=int(options[\'r\']),\n            input_filters=int(options[\'i\']),\n            output_filters=int(options[\'o\']),\n            expand_ratio=int(options[\'e\']),\n            id_skip=(\'noskip\' not in block_string),\n            se_ratio=float(options[\'se\']) if \'se\' in options else None,\n            strides=[int(options[\'s\'][0]),\n                     int(options[\'s\'][1])])\n\n    def _encode_block_string(self, block):\n        """"""Encodes a block to a string.""""""\n        args = [\n            \'r%d\' % block.num_repeat,\n            \'k%d\' % block.kernel_size,\n            \'s%d%d\' % (block.strides[0], block.strides[1]),\n            \'e%s\' % block.expand_ratio,\n            \'i%d\' % block.input_filters,\n            \'o%d\' % block.output_filters\n        ]\n        if block.se_ratio > 0 and block.se_ratio <= 1:\n            args.append(\'se%s\' % block.se_ratio)\n        if block.id_skip is False:\n            args.append(\'noskip\')\n        return \'_\'.join(args)\n\n    def decode(self, string_list):\n        """"""Decodes a list of string notations to specify blocks inside the network.\n    Args:\n      string_list: a list of strings, each string is a notation of block.\n    Returns:\n      A list of namedtuples to represent blocks arguments.\n    """"""\n        assert isinstance(string_list, list)\n        blocks_args = []\n        for block_string in string_list:\n            blocks_args.append(self._decode_block_string(block_string))\n        return blocks_args\n\n    def encode(self, blocks_args):\n        """"""Encodes a list of Blocks to a list of strings.\n    Args:\n      blocks_args: A list of namedtuples to represent blocks arguments.\n    Returns:\n      a list of strings, each string is a notation of block.\n    """"""\n        block_strings = []\n        for block in blocks_args:\n            block_strings.append(self._encode_block_string(block))\n        return block_strings\n\n\ndef efficientnet(width_coefficient=None,\n                 depth_coefficient=None,\n                 dropout_rate=0.2,\n                 drop_connect_rate=0.2):\n    """"""Creates a efficientnet model.""""""\n    blocks_args = [\n        \'r1_k3_s11_e1_i32_o16_se0.25\',\n        \'r2_k3_s22_e6_i16_o24_se0.25\',\n        \'r2_k5_s22_e6_i24_o40_se0.25\',\n        \'r3_k3_s22_e6_i40_o80_se0.25\',\n        \'r3_k5_s11_e6_i80_o112_se0.25\',\n        \'r4_k5_s22_e6_i112_o192_se0.25\',\n        \'r1_k3_s11_e6_i192_o320_se0.25\',\n    ]\n    global_params = GlobalParams(batch_norm_momentum=0.99,\n                                 batch_norm_epsilon=1e-3,\n                                 dropout_rate=dropout_rate,\n                                 drop_connect_rate=drop_connect_rate,\n                                 data_format=\'channels_last\',\n                                 num_classes=1000,\n                                 width_coefficient=width_coefficient,\n                                 depth_coefficient=depth_coefficient,\n                                 depth_divisor=8,\n                                 min_depth=None)\n    decoder = BlockDecoder()\n    return decoder.decode(blocks_args), global_params\n\n\ndef efficientnet_params(model_name):\n    """"""Get efficientnet params based on model name.""""""\n    params_dict = {\n        # (width_coefficient, depth_coefficient, resolution, dropout_rate)\n        \'efficientnet-b0\': (1.0, 1.0, 224, 0.2),\n        \'efficientnet-b1\': (1.0, 1.1, 240, 0.2),\n        \'efficientnet-b2\': (1.1, 1.2, 260, 0.3),\n        \'efficientnet-b3\': (1.2, 1.4, 300, 0.3),\n        \'efficientnet-b4\': (1.4, 1.8, 380, 0.4),\n        \'efficientnet-b5\': (1.6, 2.2, 456, 0.4),\n        \'efficientnet-b6\': (1.8, 2.6, 528, 0.5),\n        \'efficientnet-b7\': (2.0, 3.1, 600, 0.5),\n    }\n    return params_dict[model_name]\n\n\ndef get_model_params(model_name, override_params=None):\n    """"""Get the block args and global params for a given model.""""""\n    if model_name.startswith(\'efficientnet\'):\n        width_coefficient, depth_coefficient, input_shape, dropout_rate = (\n            efficientnet_params(model_name))\n        blocks_args, global_params = efficientnet(width_coefficient,\n                                                  depth_coefficient,\n                                                  dropout_rate)\n    else:\n        raise NotImplementedError(\'model name is not pre-defined: %s\' %\n                                  model_name)\n\n    if override_params:\n        # ValueError will be raised here if override_params has fields not included\n        # in global_params.\n        global_params = global_params._replace(**override_params)\n\n    return blocks_args, global_params, input_shape\n\n\nclass EfficientConv2DKernelInitializer(tf.keras.initializers.Initializer):\n    """"""Initialization for convolutional kernels.\n    The main difference with tf.variance_scaling_initializer is that\n    tf.variance_scaling_initializer uses a truncated normal with an uncorrected\n    standard deviation, whereas here we use a normal distribution. Similarly,\n    tf.contrib.layers.variance_scaling_initializer uses a truncated normal with\n    a corrected standard deviation.\n    Args:\n      shape: shape of variable\n      dtype: dtype of variable\n      partition_info: unused\n    Returns:\n      an initialization for the variable\n    """"""\n\n    def __call__(self, shape, dtype=tf.float32, **kwargs):\n        kernel_height, kernel_width, _, out_filters = shape\n        fan_out = int(kernel_height * kernel_width * out_filters)\n        return tf.random.normal(shape,\n                                mean=0.0,\n                                stddev=np.sqrt(2.0 / fan_out),\n                                dtype=dtype)\n\n\nclass EfficientDenseKernelInitializer(tf.keras.initializers.Initializer):\n    """"""Initialization for dense kernels.\n    This initialization is equal to\n      tf.variance_scaling_initializer(scale=1.0/3.0, mode=\'fan_out\',\n                                      distribution=\'uniform\').\n    It is written out explicitly here for clarity.\n    Args:\n      shape: shape of variable\n      dtype: dtype of variable\n    Returns:\n      an initialization for the variable\n    """"""\n\n    def __call__(self, shape, dtype=tf.float32, **kwargs):\n        """"""Initialization for dense kernels.\n        This initialization is equal to\n          tf.variance_scaling_initializer(scale=1.0/3.0, mode=\'fan_out\',\n                                          distribution=\'uniform\').\n        It is written out explicitly here for clarity.\n        Args:\n          shape: shape of variable\n          dtype: dtype of variable\n        Returns:\n          an initialization for the variable\n        """"""\n        init_range = 1.0 / np.sqrt(shape[1])\n        return tf.random.uniform(shape, -init_range, init_range, dtype=dtype)\n\n\nconv_kernel_initializer = EfficientConv2DKernelInitializer()\ndense_kernel_initializer = EfficientDenseKernelInitializer()\n\n\nclass Swish(tf.keras.layers.Layer):\n\n    def call(self, inputs):\n        inputs = tf.convert_to_tensor(inputs)\n        return inputs * tf.sigmoid(inputs)\n\n\nclass DropConnect(tf.keras.layers.Layer):\n\n    def __init__(self, drop_connect_rate=0., **kwargs):\n        super().__init__(**kwargs)\n        self.drop_connect_rate = drop_connect_rate\n\n    def call(self, inputs, training=None):\n\n        def drop_connect():\n            keep_prob = 1.0 - self.drop_connect_rate\n\n            # Compute drop_connect tensor\n            batch_size = tf.shape(inputs)[0]\n            random_tensor = keep_prob\n            random_tensor += tf.random.uniform([batch_size, 1, 1, 1],\n                                               dtype=inputs.dtype)\n            binary_tensor = tf.floor(random_tensor)\n            output = tf.div(inputs, keep_prob) * binary_tensor\n            return output\n\n        return tf.keras.backend.in_train_phase(drop_connect,\n                                               inputs,\n                                               training=training)\n\n    def get_config(self):\n        config = super().get_config()\n        config[\'drop_connect_rate\'] = self.drop_connect_rate\n        return config\n\n\ndef round_filters(filters, global_params):\n    """"""Round number of filters based on depth multiplier.""""""\n    multiplier = global_params.width_coefficient\n    divisor = global_params.depth_divisor\n    min_depth = global_params.min_depth\n    if not multiplier:\n        return filters\n\n    filters *= multiplier\n    min_depth = min_depth or divisor\n    new_filters = max(min_depth,\n                      int(filters + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_filters < 0.9 * filters:\n        new_filters += divisor\n\n    return int(new_filters)\n\n\ndef round_repeats(repeats, global_params):\n    """"""Round number of filters based on depth multiplier.""""""\n    multiplier = global_params.depth_coefficient\n    if not multiplier:\n        return repeats\n    return int(math.ceil(multiplier * repeats))\n\n\nclass Mean(tf.keras.layers.Layer):\n\n    def __init__(self, spatial_dims, **kwargs):\n        super(Mean, self).__init__(**kwargs)\n        self.spatial_dims = spatial_dims\n\n    def call(self, inputs):\n        return tf.reduce_mean(inputs, axis=self.spatial_dims, keepdims=True)\n\n    def get_config(self):\n        config = super().get_config()\n        config[\'spatial_dims\'] = self.spatial_dims\n        return config\n\n\ndef SEBlock(block_args, global_params):\n    num_reduced_filters = max(\n        1, int(block_args.input_filters * block_args.se_ratio))\n    filters = block_args.input_filters * block_args.expand_ratio\n    if global_params.data_format == \'channels_first\':\n        spatial_dims = [2, 3]\n    else:\n        spatial_dims = [1, 2]\n\n    def block(inputs):\n        x = inputs\n        x = Mean(spatial_dims)(x)\n        #x = tf.keras.layers.Lambda(lambda a: tf.reduce_mean(a, axis=spatial_dims, keepdims=True))(x)\n        x = tf.keras.layers.Conv2D(num_reduced_filters,\n                                   kernel_size=[1, 1],\n                                   strides=[1, 1],\n                                   kernel_initializer=conv_kernel_initializer,\n                                   padding=\'same\',\n                                   use_bias=True)(x)\n        x = Swish()(x)\n\n        # Excite\n        x = tf.keras.layers.Conv2D(filters,\n                                   kernel_size=[1, 1],\n                                   strides=[1, 1],\n                                   kernel_initializer=conv_kernel_initializer,\n                                   padding=\'same\',\n                                   use_bias=True)(x)\n        x = tf.keras.layers.Activation(\'sigmoid\')(x)\n        out = tf.keras.layers.Multiply()([x, inputs])\n        return out\n\n    return block\n\n\ndef MBConvBlock(block_args, global_params, drop_connect_rate=None):\n    batch_norm_momentum = global_params.batch_norm_momentum\n    batch_norm_epsilon = global_params.batch_norm_epsilon\n\n    if global_params.data_format == \'channels_first\':\n        channel_axis = 1\n    else:\n        channel_axis = -1\n\n    has_se = (block_args.se_ratio is not None) and (\n        block_args.se_ratio > 0) and (block_args.se_ratio <= 1)\n\n    filters = block_args.input_filters * block_args.expand_ratio\n    kernel_size = block_args.kernel_size\n\n    def block(inputs):\n\n        if block_args.expand_ratio != 1:\n            x = tf.keras.layers.Conv2D(\n                filters,\n                kernel_size=[1, 1],\n                strides=[1, 1],\n                kernel_initializer=conv_kernel_initializer,\n                padding=\'same\',\n                use_bias=False)(inputs)\n            x = tf.keras.layers.BatchNormalization(\n                axis=channel_axis,\n                momentum=batch_norm_momentum,\n                epsilon=batch_norm_epsilon)(x)\n            x = Swish()(x)\n\n        else:\n            x = inputs\n\n        x = tf.keras.layers.DepthwiseConv2D(\n            [kernel_size, kernel_size],\n            strides=block_args.strides,\n            depthwise_initializer=conv_kernel_initializer,\n            padding=\'same\',\n            use_bias=False)(x)\n        x = tf.keras.layers.BatchNormalization(axis=channel_axis,\n                                               momentum=batch_norm_momentum,\n                                               epsilon=batch_norm_epsilon)(x)\n        x = Swish()(x)\n\n        if has_se:\n            x = SEBlock(block_args, global_params)(x)\n\n        # output phase\n\n        x = tf.keras.layers.Conv2D(block_args.output_filters,\n                                   kernel_size=[1, 1],\n                                   strides=[1, 1],\n                                   kernel_initializer=conv_kernel_initializer,\n                                   padding=\'same\',\n                                   use_bias=False)(x)\n        x = tf.keras.layers.BatchNormalization(axis=channel_axis,\n                                               momentum=batch_norm_momentum,\n                                               epsilon=batch_norm_epsilon)(x)\n\n        if block_args.id_skip:\n            if all(s == 1 for s in block_args.strides\n                  ) and block_args.input_filters == block_args.output_filters:\n                # only apply drop_connect if skip presents.\n                if drop_connect_rate:\n                    x = DropConnect(drop_connect_rate)(x)\n                x = tf.keras.layers.Add()([x, inputs])\n        return x\n\n    return block\n\n\ndef EfficientNet(input_shape,\n                 block_args_list,\n                 global_params,\n                 include_top=True,\n                 pooling=None,\n                 input_tensor=None):\n    batch_norm_momentum = global_params.batch_norm_momentum\n    batch_norm_epsilon = global_params.batch_norm_epsilon\n\n    # Stem part\n    if input_tensor is not None:\n        if not hasattr(input_tensor, \'_keras_history\'):\n            inputs = tf.keras.layers.Input(tensor=input_tensor,\n                                           shape=input_shape)\n        else:\n            inputs = input_tensor\n    else:\n        inputs = tf.keras.layers.Input(shape=input_shape)\n\n    if global_params.data_format == \'channels_first\':\n        channel_axis = 1\n    else:\n        channel_axis = -1\n    x = inputs\n    x = tf.keras.layers.Conv2D(filters=round_filters(32, global_params),\n                               kernel_size=[3, 3],\n                               strides=[2, 2],\n                               kernel_initializer=conv_kernel_initializer,\n                               padding=\'same\',\n                               use_bias=False)(x)\n    x = tf.keras.layers.BatchNormalization(axis=channel_axis,\n                                           momentum=batch_norm_momentum,\n                                           epsilon=batch_norm_epsilon)(x)\n    x = Swish()(x)\n\n    # Blocks part\n    block_idx = 1\n    n_blocks = sum([block_args.num_repeat for block_args in block_args_list])\n    drop_rate = global_params.drop_connect_rate or 0\n    drop_rate_dx = drop_rate / n_blocks\n\n    for block_args in block_args_list:\n        assert block_args.num_repeat > 0\n        # Update block input and output filters based on depth multiplier.\n        block_args = block_args._replace(\n            input_filters=round_filters(block_args.input_filters,\n                                        global_params),\n            output_filters=round_filters(block_args.output_filters,\n                                         global_params),\n            num_repeat=round_repeats(block_args.num_repeat, global_params))\n\n        # The first block needs to take care of stride and filter size increase.\n        x = MBConvBlock(block_args,\n                        global_params,\n                        drop_connect_rate=drop_rate_dx * block_idx)(x)\n        block_idx += 1\n\n        if block_args.num_repeat > 1:\n            block_args = block_args._replace(\n                input_filters=block_args.output_filters, strides=[1, 1])\n\n        for _ in xrange(block_args.num_repeat - 1):\n            x = MBConvBlock(block_args,\n                            global_params,\n                            drop_connect_rate=drop_rate_dx * block_idx)(x)\n            block_idx += 1\n\n    # Head part\n    x = tf.keras.layers.Conv2D(filters=round_filters(1280, global_params),\n                               kernel_size=[1, 1],\n                               strides=[1, 1],\n                               kernel_initializer=conv_kernel_initializer,\n                               padding=\'same\',\n                               use_bias=False)(x)\n    x = tf.keras.layers.BatchNormalization(axis=channel_axis,\n                                           momentum=batch_norm_momentum,\n                                           epsilon=batch_norm_epsilon)(x)\n    x = Swish()(x)\n\n    if include_top:\n        x = tf.keras.layers.GlobalAveragePooling2D(\n            data_format=global_params.data_format)(x)\n        if global_params.dropout_rate > 0:\n            x = tf.keras.layers.Dropout(global_params.dropout_rate)(x)\n        x = tf.keras.layers.Dense(\n            global_params.num_classes,\n            kernel_initializer=dense_kernel_initializer)(x)\n        x = tf.keras.layers.Softmax()(x)\n    else:\n        if pooling == \'avg\':\n            x = tf.keras.layers.GlobalAveragePooling2D(\n                data_format=global_params.data_format)(x)\n        elif pooling == \'max\':\n            x = tf.keras.layers.GlobalMaxPooling2D(\n                data_format=global_params.data_format)(x)\n\n    outputs = x\n    model = tf.keras.Model(inputs, outputs)\n    return model\n\n\ndef _get_model_by_name(model_name,\n                       input_shape=None,\n                       include_top=True,\n                       weights=None,\n                       classes=1000,\n                       pooling=None,\n                       input_tensor=None):\n    """"""Re-Implementation of EfficientNet for Keras\n\n    Reference:\n        https://arxiv.org/abs/1807.11626\n\n    Args:\n        input_shape: optional, if ``None`` default_input_shape is used\n            EfficientNetB0 - (224, 224, 3)\n            EfficientNetB1 - (240, 240, 3)\n            EfficientNetB2 - (260, 260, 3)\n            EfficientNetB3 - (300, 300, 3)\n            EfficientNetB4 - (380, 380, 3)\n            EfficientNetB5 - (456, 456, 3)\n            EfficientNetB6 - (528, 528, 3)\n            EfficientNetB7 - (600, 600, 3)\n        include_top: whether to include the fully-connected\n            layer at the top of the network.\n        weights: one of `None` (random initialization),\n              \'imagenet\' (pre-training on ImageNet).\n        classes: optional number of classes to classify images\n            into, only to be specified if `include_top` is True, and\n            if no `weights` argument is specified.\n        pooling: optional [None, \'avg\', \'max\'], if ``include_top=False``\n            add global pooling on top of the network\n            - avg: GlobalAveragePooling2D\n            - max: GlobalMaxPooling2D\n\n    Returns:\n        A Keras model instance.\n\n    """"""\n    if weights not in {None, \'imagenet\'}:\n        raise ValueError(\n            \'Parameter `weights` should be one of [None, ""imagenet""]\')\n\n    if weights == \'imagenet\' and model_name not in IMAGENET_WEIGHTS:\n        raise ValueError(\n            \'There are not pretrained weights for {} model.\'.format(model_name))\n\n    if weights == \'imagenet\' and include_top and classes != 1000:\n        raise ValueError(\'If using `weights` and `include_top`\'\n                         \' `classes` should be 1000\')\n\n    block_agrs_list, global_params, default_input_shape = get_model_params(\n        model_name, override_params={\'num_classes\': classes})\n\n    if input_shape is None:\n        input_shape = (default_input_shape, default_input_shape, 3)\n\n    model = EfficientNet(input_shape,\n                         block_agrs_list,\n                         global_params,\n                         include_top=include_top,\n                         pooling=pooling,\n                         input_tensor=input_tensor)\n\n    if weights:\n        if not include_top:\n            weights_name = model_name + \'-notop\'\n        else:\n            weights_name = model_name\n        weights = IMAGENET_WEIGHTS[weights_name]\n        weights_path = tf.keras.utils.get_file(\n            weights[\'name\'],\n            weights[\'url\'],\n            cache_subdir=\'models\',\n            md5_hash=weights[\'md5\'],\n        )\n        model.load_weights(weights_path)\n\n    return model\n\n\ndef EfficientNetB0(include_top=True,\n                   input_shape=None,\n                   weights=None,\n                   classes=1000,\n                   pooling=None):\n    return _get_model_by_name(\'efficientnet-b0\',\n                              include_top=include_top,\n                              input_shape=input_shape,\n                              weights=weights,\n                              classes=classes,\n                              pooling=pooling)\n\n\ndef EfficientNetB1(include_top=True,\n                   input_shape=None,\n                   weights=None,\n                   classes=1000,\n                   pooling=None):\n    return _get_model_by_name(\'efficientnet-b1\',\n                              include_top=include_top,\n                              input_shape=input_shape,\n                              weights=weights,\n                              classes=classes,\n                              pooling=pooling)\n\n\ndef EfficientNetB2(include_top=True,\n                   input_shape=None,\n                   weights=None,\n                   classes=1000,\n                   pooling=None):\n    return _get_model_by_name(\'efficientnet-b2\',\n                              include_top=include_top,\n                              input_shape=input_shape,\n                              weights=weights,\n                              classes=classes,\n                              pooling=pooling)\n\n\ndef EfficientNetB3(include_top=True,\n                   input_shape=None,\n                   weights=None,\n                   classes=1000,\n                   pooling=None):\n    return _get_model_by_name(\'efficientnet-b3\',\n                              include_top=include_top,\n                              input_shape=input_shape,\n                              weights=weights,\n                              classes=classes,\n                              pooling=pooling)\n\n\ndef EfficientNetB4(include_top=True,\n                   input_shape=None,\n                   weights=None,\n                   classes=1000,\n                   pooling=None,\n                   input_tensor=None):\n    return _get_model_by_name(\'efficientnet-b4\',\n                              include_top=include_top,\n                              input_shape=input_shape,\n                              weights=weights,\n                              classes=classes,\n                              pooling=pooling,\n                              input_tensor=input_tensor)\n\n\ndef EfficientNetB5(include_top=True,\n                   input_shape=None,\n                   weights=None,\n                   classes=1000,\n                   pooling=None):\n    return _get_model_by_name(\'efficientnet-b5\',\n                              include_top=include_top,\n                              input_shape=input_shape,\n                              weights=weights,\n                              classes=classes,\n                              pooling=pooling)\n\n\ndef EfficientNetB6(include_top=True,\n                   input_shape=None,\n                   weights=None,\n                   classes=1000,\n                   pooling=None):\n    return _get_model_by_name(\'efficientnet-b6\',\n                              include_top=include_top,\n                              input_shape=input_shape,\n                              weights=weights,\n                              classes=classes,\n                              pooling=pooling)\n\n\ndef EfficientNetB7(include_top=True,\n                   input_shape=None,\n                   weights=None,\n                   classes=1000,\n                   pooling=None):\n    return _get_model_by_name(\'efficientnet-b7\',\n                              include_top=include_top,\n                              input_shape=input_shape,\n                              weights=weights,\n                              classes=classes,\n                              pooling=pooling)\n'"
yolo3/enums.py,0,"b'from enum import Enum, unique\n\n\n@unique\nclass MODE(Enum):\n    TRAIN = 0\n    IMAGE = 1\n    VIDEO = 2\n    TFLITE = 3\n    SERVING = 4\n    MAP = 5\n    PRUNE = 6\n    TFJS = 7\n    TRAIN_BACKBONE = 8\n\n\n@unique\nclass OPT(Enum):\n    XLA = 0\n    DEBUG = 1\n    MKL = 2\n\n\n@unique\nclass BACKBONE(Enum):\n    MOBILENETV2 = 0\n    EFFICIENTNET = 1\n    DARKNET53 = 2\n\n\n@unique\nclass BOX_LOSS(Enum):\n    MSE = 0\n    GIOU = 1\n\n\n@unique\nclass DATASET_MODE(Enum):\n    TRAIN = 0\n    VALIDATE = 1\n    TEST = 2\n'"
yolo3/map.py,36,"b'import tensorflow as tf\nimport numpy as np\nfrom yolo3.utils import bind\nfrom timeit import default_timer as timer\nfrom yolo3.data import Dataset\nfrom yolo3.enums import DATASET_MODE\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\n\nclass MAPCallback(tf.keras.callbacks.Callback):\n    """"""Calculate the AP given the recall and precision array 1st) We compute a\n    version of the measured precision/recall curve with precision monotonically\n    decreasing 2nd) We compute the AP as the area under this curve by numerical\n    integration.""""""\n\n    def _voc_ap(self, rec, prec):\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n        return ap\n\n    def parse_tfrecord(self, example_proto):\n        feature_description = {\n            \'image/encoded\': tf.io.FixedLenFeature([], tf.string),\n            \'image/object/bbox/xmin\': tf.io.VarLenFeature(tf.float32),\n            \'image/object/bbox/xmax\': tf.io.VarLenFeature(tf.float32),\n            \'image/object/bbox/ymin\': tf.io.VarLenFeature(tf.float32),\n            \'image/object/bbox/ymax\': tf.io.VarLenFeature(tf.float32),\n            \'image/object/bbox/label\': tf.io.VarLenFeature(tf.int64)\n        }\n        features = tf.io.parse_single_example(example_proto,\n                                              feature_description)\n        image = features[\'image/encoded\']\n        xmins = tf.expand_dims(features[\'image/object/bbox/xmin\'].values, 0)\n        xmaxs = tf.expand_dims(features[\'image/object/bbox/xmax\'].values, 0)\n        ymins = tf.expand_dims(features[\'image/object/bbox/ymin\'].values, 0)\n        ymaxs = tf.expand_dims(features[\'image/object/bbox/ymax\'].values, 0)\n        labels = tf.expand_dims(features[\'image/object/bbox/label\'].values, 0)\n        bbox = tf.concat([xmins, ymins, xmaxs, ymaxs,\n                          tf.cast(labels, tf.float32)], 0)\n        return image, tf.transpose(bbox)\n\n    def parse_text(self, line):\n        values = tf.strings.split([line], \' \').values\n        image = tf.io.read_file(values[0])\n        reshaped_data = tf.reshape(values[1:], [-1, 5])\n        xmins = tf.strings.to_number(reshaped_data[:, 0], tf.float32)\n        xmaxs = tf.strings.to_number(reshaped_data[:, 2], tf.float32)\n        ymins = tf.strings.to_number(reshaped_data[:, 1], tf.float32)\n        ymaxs = tf.strings.to_number(reshaped_data[:, 3], tf.float32)\n        labels = tf.strings.to_number(reshaped_data[:, 4], tf.int64)\n        xmins=tf.expand_dims(xmins, 0)\n        xmaxs=tf.expand_dims(xmaxs, 0)\n        ymins=tf.expand_dims(ymins, 0)\n        ymaxs=tf.expand_dims(ymaxs, 0)\n        labels=tf.expand_dims(labels, 0)\n        \n        bbox = tf.concat(\n            [xmins, ymins, xmaxs, ymaxs,\n             tf.cast(labels, tf.float32)], 0)\n        return image, tf.transpose(bbox)\n\n    def calculate_aps(self):\n        test_dataset_builder = Dataset(\n            self.glob_path,\n            self.batch_size,\n            input_shape=self.input_shape,\n            mode=DATASET_MODE.TEST)\n        bind(test_dataset_builder, self.parse_tfrecord)\n        bind(test_dataset_builder, self.parse_text)\n        test_dataset, test_num = test_dataset_builder.build()\n        true_res = {}\n        pred_res = []\n        idx = 0\n        APs = {}\n        start = timer()\n        for image, bbox in test_dataset:\n            out_boxes, out_scores, out_classes = self.model(image)\n            out_boxes = out_boxes.numpy()\n            out_scores = out_scores.numpy()\n            out_classes = out_classes.numpy()\n            if len(out_classes) > 0:\n                for out_box, out_score, out_class in zip(\n                        out_boxes, out_scores, out_classes):\n                    top, left, bottom, right = out_box\n                    pred_res.append(\n                        [idx, out_class, out_score, left, top, right, bottom])\n            true_res[idx] = bbox[0].numpy()\n            idx += 1\n        end = timer()\n        print((end - start) / test_num)\n        for cls in range(self.num_classes):\n            pred_res_cls = [x for x in pred_res if x[1] == cls]\n            if len(pred_res_cls) == 0:\n                APs[cls]=0\n                continue\n            true_res_cls = {}\n            npos = 0\n            for index in true_res:\n                objs = [obj for obj in true_res[index] if obj[4] == cls]\n                npos += len(objs)\n                BBGT = np.array([x[:4] for x in objs])\n                true_res_cls[index] = {\n                    \'bbox\': BBGT,\n                    \'difficult\': [False] * len(objs),\n                    \'det\': [False] * len(objs)\n                }\n            ids = [x[0] for x in pred_res_cls]\n            scores = np.array([x[2] for x in pred_res_cls])\n            bboxs = np.array([x[3:] for x in pred_res_cls])\n            sorted_ind = np.argsort(-scores)\n            bboxs = bboxs[sorted_ind, :]\n            ids = [ids[x] for x in sorted_ind]\n\n            nd = len(ids)\n            tp = np.zeros(nd)\n            fp = np.zeros(nd)\n            for j in range(nd):\n                res = true_res_cls[ids[j]]\n                bbox = bboxs[j, :].astype(float)\n                ovmax = -np.inf\n                BBGT = res[\'bbox\'].astype(float)\n                if BBGT.size > 0:\n                    ixmin = np.maximum(BBGT[:, 0], bbox[0])\n                    iymin = np.maximum(BBGT[:, 1], bbox[1])\n                    ixmax = np.minimum(BBGT[:, 2], bbox[2])\n                    iymax = np.minimum(BBGT[:, 3], bbox[3])\n                    iw = np.maximum(ixmax - ixmin + 1., 0.)\n                    ih = np.maximum(iymax - iymin + 1., 0.)\n                    inters = iw * ih\n\n                    # union\n                    uni = ((bbox[2] - bbox[0] + 1.) * (bbox[3] - bbox[1] + 1.)\n                           + (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n                           (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n\n                    overlaps = inters / uni\n                    ovmax = np.max(overlaps)\n                    jmax = np.argmax(overlaps)\n                if ovmax > self.iou:\n                    if not res[\'difficult\'][jmax]:\n                        if not res[\'det\'][jmax]:\n                            tp[j] = 1.\n                            res[\'det\'][jmax] = 1\n                        else:\n                            fp[j] = 1.\n                else:\n                    fp[j] = 1.\n\n            fp = np.cumsum(fp)\n            tp = np.cumsum(tp)\n            rec = tp / np.maximum(float(npos), np.finfo(np.float64).eps)\n            prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n            ap = self._voc_ap(rec, prec)\n            APs[cls] = ap\n        return APs\n\n    def __init__(self,\n                 glob_path,\n                 input_shape,\n                 class_names,\n                 iou=.5,\n                 batch_size=1):\n        self.input_shape = input_shape\n        self.class_names = class_names\n        self.num_classes = len(class_names)\n        self.glob_path = glob_path\n        self.iou = iou\n        self.batch_size = batch_size\n\n    def on_train_end(self, logs={}):\n        logs = logs or {}\n        origin_learning_phase = tf.keras.backend.learning_phase()\n        tf.keras.backend.set_learning_phase(0)\n        APs = self.calculate_aps()\n        tf.keras.backend.set_learning_phase(origin_learning_phase)\n        for cls in range(self.num_classes):\n            if cls in APs:\n                print(self.class_names[cls] + \' ap: \', APs[cls])\n        mAP = np.mean([APs[cls] for cls in APs])\n        print(\'mAP: \', mAP)\n        logs[\'mAP\'] = mAP\n'"
yolo3/model.py,158,"b'""""""YOLO_v3 Model Defined in Keras.""""""\n\nfrom yolo3.enums import BOX_LOSS\nimport numpy as np\nimport tensorflow as tf\nfrom typing import List, Tuple\nfrom yolo3.utils import compose,do_giou_calculate\nfrom yolo3.override import mobilenet_v2\nfrom yolo3.darknet import DarknetConv2D_BN_Leaky, DarknetConv2D, darknet_body\nfrom yolo3.efficientnet import EfficientNetB4, MBConvBlock, get_model_params, BlockArgs\nfrom yolo3.train import AdvLossModel\n\n\ndef make_last_layers(x, num_filters, out_filters):\n    \'\'\'6 Conv2D_BN_Leaky layers followed by a Conv2D_linear layer\'\'\'\n    x = compose(DarknetConv2D_BN_Leaky(num_filters, (1, 1)),\n                DarknetConv2D_BN_Leaky(num_filters * 2, (3, 3)),\n                DarknetConv2D_BN_Leaky(num_filters, (1, 1)),\n                DarknetConv2D_BN_Leaky(num_filters * 2, (3, 3)),\n                DarknetConv2D_BN_Leaky(num_filters, (1, 1)))(x)\n    y = compose(DarknetConv2D_BN_Leaky(num_filters * 2, (3, 3)),\n                DarknetConv2D(out_filters, (1, 1)))(x)\n    return x, y\n\n\ndef darknet_yolo_body(inputs, num_anchors, num_classes):\n    """"""Create YOLO_V3 model CNN body in Keras.""""""\n    if not hasattr(inputs, \'_keras_history\'):\n        inputs = tf.keras.layers.Input(tensor=inputs)\n    darknet = darknet_body(inputs, include_top=False)\n    x, y1 = make_last_layers(darknet.output, 512,\n                             num_anchors * (num_classes + 5))\n\n    x = compose(DarknetConv2D_BN_Leaky(256, (1, 1)),\n                tf.keras.layers.UpSampling2D(2))(x)\n    x = tf.keras.layers.Concatenate()([x, darknet.layers[152].output])\n    x, y2 = make_last_layers(x, 256, num_anchors * (num_classes + 5))\n\n    x = compose(DarknetConv2D_BN_Leaky(128, (1, 1)),\n                tf.keras.layers.UpSampling2D(2))(x)\n    x = tf.keras.layers.Concatenate()([x, darknet.layers[92].output])\n    x, y3 = make_last_layers(x, 128, num_anchors * (num_classes + 5))\n    y1 = tf.keras.layers.Lambda(\n        lambda y: tf.reshape(y, [\n            -1,\n            tf.shape(y)[1],\n            tf.shape(y)[2], num_anchors, num_classes + 5\n        ]),\n        name=\'y1\')(y1)\n    y2 = tf.keras.layers.Lambda(\n        lambda y: tf.reshape(y, [\n            -1,\n            tf.shape(y)[1],\n            tf.shape(y)[2], num_anchors, num_classes + 5\n        ]),\n        name=\'y2\')(y2)\n    y3 = tf.keras.layers.Lambda(\n        lambda y: tf.reshape(y, [\n            -1,\n            tf.shape(y)[1],\n            tf.shape(y)[2], num_anchors, num_classes + 5\n        ]),\n        name=\'y3\')(y3)\n    return AdvLossModel(inputs, [y1, y2, y3])\n\n\ndef MobilenetSeparableConv2D(filters,\n                             kernel_size,\n                             strides=(1, 1),\n                             padding=\'valid\',\n                             use_bias=True):\n    return compose(\n        tf.keras.layers.DepthwiseConv2D(kernel_size,\n                                        padding=padding,\n                                        use_bias=use_bias,\n                                        strides=strides),\n        tf.keras.layers.BatchNormalization(), tf.keras.layers.ReLU(6.),\n        tf.keras.layers.Conv2D(filters,\n                               1,\n                               padding=\'same\',\n                               use_bias=use_bias,\n                               strides=1), tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.ReLU(6.))\n\n\ndef make_last_layers_mobilenet(x, id, num_filters, out_filters):\n    x = compose(\n        tf.keras.layers.Conv2D(num_filters,\n                               kernel_size=1,\n                               padding=\'same\',\n                               use_bias=False,\n                               name=\'block_\' + str(id) + \'_conv\'),\n        tf.keras.layers.BatchNormalization(momentum=0.9,\n                                           name=\'block_\' + str(id) + \'_BN\'),\n        tf.keras.layers.ReLU(6., name=\'block_\' + str(id) + \'_relu6\'),\n        MobilenetSeparableConv2D(2 * num_filters,\n                                 kernel_size=(3, 3),\n                                 use_bias=False,\n                                 padding=\'same\'),\n        tf.keras.layers.Conv2D(num_filters,\n                               kernel_size=1,\n                               padding=\'same\',\n                               use_bias=False,\n                               name=\'block_\' + str(id + 1) + \'_conv\'),\n        tf.keras.layers.BatchNormalization(momentum=0.9,\n                                           name=\'block_\' + str(id + 1) + \'_BN\'),\n        tf.keras.layers.ReLU(6., name=\'block_\' + str(id + 1) + \'_relu6\'),\n        MobilenetSeparableConv2D(2 * num_filters,\n                                 kernel_size=(3, 3),\n                                 use_bias=False,\n                                 padding=\'same\'),\n        tf.keras.layers.Conv2D(num_filters,\n                               kernel_size=1,\n                               padding=\'same\',\n                               use_bias=False,\n                               name=\'block_\' + str(id + 2) + \'_conv\'),\n        tf.keras.layers.BatchNormalization(momentum=0.9,\n                                           name=\'block_\' + str(id + 2) + \'_BN\'),\n        tf.keras.layers.ReLU(6., name=\'block_\' + str(id + 2) + \'_relu6\'))(x)\n    y = compose(\n        MobilenetSeparableConv2D(2 * num_filters,\n                                 kernel_size=(3, 3),\n                                 use_bias=False,\n                                 padding=\'same\'),\n        tf.keras.layers.Conv2D(out_filters,\n                               kernel_size=1,\n                               padding=\'same\',\n                               use_bias=False))(x)\n    return x, y\n\n\ndef _make_divisible(v, divisor, min_value=None):\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\ndef MobilenetConv2D(kernel, alpha, filters):\n    last_block_filters = _make_divisible(filters * alpha, 8)\n    return compose(\n        tf.keras.layers.Conv2D(last_block_filters,\n                               kernel,\n                               padding=\'same\',\n                               use_bias=False),\n        tf.keras.layers.BatchNormalization(), tf.keras.layers.ReLU(6.))\n\n\ndef mobilenetv2_yolo_body(inputs, num_anchors, num_classes, alpha=1.0):\n    mobilenetv2 = mobilenet_v2(default_batchnorm_momentum=0.9,\n                               alpha=alpha,\n                               input_tensor=inputs,\n                               include_top=False,\n                               weights=\'imagenet\')\n    x, y1 = make_last_layers_mobilenet(mobilenetv2.output, 17, 512,\n                                       num_anchors * (num_classes + 5))\n    x = compose(\n        tf.keras.layers.Conv2D(256,\n                               kernel_size=1,\n                               padding=\'same\',\n                               use_bias=False,\n                               name=\'block_20_conv\'),\n        tf.keras.layers.BatchNormalization(momentum=0.9, name=\'block_20_BN\'),\n        tf.keras.layers.ReLU(6., name=\'block_20_relu6\'),\n        tf.keras.layers.UpSampling2D(2))(x)\n    x = tf.keras.layers.Concatenate()([\n        x,\n        MobilenetConv2D(\n            (1, 1), alpha,\n            384)(mobilenetv2.get_layer(\'block_12_project_BN\').output)\n    ])\n    x, y2 = make_last_layers_mobilenet(x, 21, 256,\n                                       num_anchors * (num_classes + 5))\n    x = compose(\n        tf.keras.layers.Conv2D(128,\n                               kernel_size=1,\n                               padding=\'same\',\n                               use_bias=False,\n                               name=\'block_24_conv\'),\n        tf.keras.layers.BatchNormalization(momentum=0.9, name=\'block_24_BN\'),\n        tf.keras.layers.ReLU(6., name=\'block_24_relu6\'),\n        tf.keras.layers.UpSampling2D(2))(x)\n    x = tf.keras.layers.Concatenate()([\n        x,\n        MobilenetConv2D((1, 1), alpha,\n                        128)(mobilenetv2.get_layer(\'block_5_project_BN\').output)\n    ])\n    x, y3 = make_last_layers_mobilenet(x, 25, 128,\n                                       num_anchors * (num_classes + 5))\n    y1 = tf.keras.layers.Lambda(lambda y: tf.reshape(y, [\n        -1, tf.shape(y)[1],\n        tf.shape(y)[2], num_anchors, num_classes + 5\n    ]),\n                                name=\'y1\')(y1)\n    y2 = tf.keras.layers.Lambda(lambda y: tf.reshape(y, [\n        -1, tf.shape(y)[1],\n        tf.shape(y)[2], num_anchors, num_classes + 5\n    ]),\n                                name=\'y2\')(y2)\n    y3 = tf.keras.layers.Lambda(lambda y: tf.reshape(y, [\n        -1, tf.shape(y)[1],\n        tf.shape(y)[2], num_anchors, num_classes + 5\n    ]),\n                                name=\'y3\')(y3)\n    return AdvLossModel(mobilenetv2.inputs, [y1, y2, y3])\n\n\ndef make_last_layers_efficientnet(x, block_args, global_params):\n    if global_params.data_format == \'channels_first\':\n        channel_axis = 1\n    else:\n        channel_axis = -1\n    num_filters = block_args.input_filters * block_args.expand_ratio\n    x = compose(\n        tf.keras.layers.Conv2D(num_filters,\n                               kernel_size=1,\n                               padding=\'same\',\n                               use_bias=False),\n        tf.keras.layers.BatchNormalization(\n            axis=channel_axis,\n            epsilon=global_params.batch_norm_epsilon,\n            momentum=global_params.batch_norm_momentum),\n        tf.keras.layers.ReLU(6.),\n        MBConvBlock(block_args,\n                    global_params,\n                    drop_connect_rate=global_params.drop_connect_rate),\n        tf.keras.layers.Conv2D(num_filters,\n                               kernel_size=1,\n                               padding=\'same\',\n                               use_bias=False),\n        tf.keras.layers.BatchNormalization(\n            axis=channel_axis,\n            epsilon=global_params.batch_norm_epsilon,\n            momentum=global_params.batch_norm_momentum),\n        tf.keras.layers.ReLU(6.),\n        MBConvBlock(block_args,\n                    global_params,\n                    drop_connect_rate=global_params.drop_connect_rate),\n        tf.keras.layers.Conv2D(num_filters,\n                               kernel_size=1,\n                               padding=\'same\',\n                               use_bias=False),\n        tf.keras.layers.BatchNormalization(\n            axis=channel_axis,\n            epsilon=global_params.batch_norm_epsilon,\n            momentum=global_params.batch_norm_momentum),\n        tf.keras.layers.ReLU(6.))(x)\n    y = compose(\n        MBConvBlock(block_args,\n                    global_params,\n                    drop_connect_rate=global_params.drop_connect_rate),\n        tf.keras.layers.Conv2D(block_args.output_filters,\n                               kernel_size=1,\n                               padding=\'same\',\n                               use_bias=False))(x)\n    return x, y\n\n\ndef efficientnet_yolo_body(inputs, model_name, num_anchors, **kwargs):\n    _, global_params, input_shape = get_model_params(model_name, kwargs)\n    num_classes = global_params.num_classes\n    if global_params.data_format == \'channels_first\':\n        channel_axis = 1\n    else:\n        channel_axis = -1\n    efficientnet = EfficientNetB4(include_top=False,\n                                  weights=\'imagenet\',\n                                  input_shape=(input_shape, input_shape, 3),\n                                  input_tensor=inputs)\n    block_args = BlockArgs(kernel_size=3,\n                           num_repeat=1,\n                           input_filters=512,\n                           output_filters=num_anchors * (num_classes + 5),\n                           expand_ratio=1,\n                           id_skip=True,\n                           se_ratio=0.25,\n                           strides=[1, 1])\n    x, y1 = make_last_layers_efficientnet(efficientnet.output, block_args,\n                                          global_params)\n    x = compose(\n        tf.keras.layers.Conv2D(256,\n                               kernel_size=1,\n                               padding=\'same\',\n                               use_bias=False,\n                               name=\'block_20_conv\'),\n        tf.keras.layers.BatchNormalization(axis=channel_axis,\n                                           momentum=0.9,\n                                           name=\'block_20_BN\'),\n        tf.keras.layers.ReLU(6., name=\'block_20_relu6\'),\n        tf.keras.layers.UpSampling2D(2))(x)\n    block_args = block_args._replace(input_filters=256)\n    x = tf.keras.layers.Concatenate()(\n        [x, efficientnet.get_layer(\'swish_65\').output])\n    x, y2 = make_last_layers_efficientnet(x, block_args, global_params)\n    x = compose(\n        tf.keras.layers.Conv2D(128,\n                               kernel_size=1,\n                               padding=\'same\',\n                               use_bias=False,\n                               name=\'block_24_conv\'),\n        tf.keras.layers.BatchNormalization(axis=channel_axis,\n                                           momentum=0.9,\n                                           name=\'block_24_BN\'),\n        tf.keras.layers.ReLU(6., name=\'block_24_relu6\'),\n        tf.keras.layers.UpSampling2D(2))(x)\n    block_args = block_args._replace(input_filters=128)\n    x = tf.keras.layers.Concatenate()(\n        [x, efficientnet.get_layer(\'swish_29\').output])\n    x, y3 = make_last_layers_efficientnet(x, block_args, global_params)\n    y1 = tf.keras.layers.Reshape(\n        (y1.shape[1], y1.shape[2], num_anchors, num_classes + 5), name=\'y1\')(y1)\n    y2 = tf.keras.layers.Reshape(\n        (y2.shape[1], y2.shape[2], num_anchors, num_classes + 5), name=\'y2\')(y2)\n    y3 = tf.keras.layers.Reshape(\n        (y3.shape[1], y3.shape[2], num_anchors, num_classes + 5), name=\'y3\')(y3)\n\n    return AdvLossModel(efficientnet.inputs, [y1, y2, y3])\n\n\ndef yolo_head(feats: tf.Tensor,\n              anchors: np.ndarray,\n              input_shape: tf.Tensor,\n              calc_loss: bool = False\n             ) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]:\n    """"""Convert final layer features to bounding box parameters.""""""\n    num_anchors = len(anchors)\n    # Reshape to batch, height, width, num_anchors, box_params.\n    anchors_tensor = tf.reshape(tf.constant(anchors), [1, 1, 1, num_anchors, 2])\n    grid_shape = tf.shape(feats)[1:3]\n    grid_y = tf.tile(tf.reshape(tf.range(0, grid_shape[0]), [-1, 1, 1, 1]),\n                     [1, grid_shape[1], 1, 1])\n    grid_x = tf.tile(tf.reshape(tf.range(0, grid_shape[1]), [1, -1, 1, 1]),\n                     [grid_shape[0], 1, 1, 1])\n    grid = tf.concat([grid_x, grid_y], -1)\n    grid = tf.cast(grid, feats.dtype)\n\n    # Adjust preditions to each spatial grid point and anchor size.\n    box_xy = (tf.sigmoid(feats[..., :2]) + grid) / tf.cast(\n        grid_shape[::-1], feats.dtype)\n    box_wh = tf.exp(feats[..., 2:4]) * tf.cast(\n        anchors_tensor, feats.dtype) / tf.cast(input_shape[::-1], feats.dtype)\n    box_confidence = tf.sigmoid(feats[..., 4:5])\n    if calc_loss == True:\n        return grid, box_xy, box_wh, box_confidence\n    box_class_probs = tf.sigmoid(feats[..., 5:])\n    return box_xy, box_wh, box_confidence, box_class_probs\n\n\ndef yolo_correct_boxes(box_xy: tf.Tensor, box_wh: tf.Tensor,\n                       input_shape: tf.Tensor, image_shape) -> tf.Tensor:\n    \'\'\'Get corrected boxes\'\'\'\n    box_yx = box_xy[..., ::-1]\n    box_hw = box_wh[..., ::-1]\n    input_shape = tf.cast(input_shape, box_yx.dtype)\n    image_shape = tf.cast(image_shape, box_yx.dtype)\n    max_shape = tf.maximum(image_shape[0], image_shape[1])\n    ratio = image_shape / max_shape\n    boxed_shape = input_shape * ratio\n    offset = (input_shape - boxed_shape) / 2.\n    scale = image_shape / boxed_shape\n    box_yx = (box_yx * input_shape - offset) * scale\n    box_hw *= input_shape * scale\n\n    box_mins = box_yx - (box_hw / 2.)\n    box_maxes = box_yx + (box_hw / 2.)\n    boxes = tf.concat(\n        [\n            tf.clip_by_value(box_mins[..., 0:1], 0, image_shape[0]),  # y_min\n            tf.clip_by_value(box_mins[..., 1:2], 0, image_shape[1]),  # x_min\n            tf.clip_by_value(box_maxes[..., 0:1], 0, image_shape[0]),  # y_max\n            tf.clip_by_value(box_maxes[..., 1:2], 0, image_shape[1])  # x_max\n        ],\n        -1)\n    return boxes\n\n\ndef yolo_boxes_and_scores(feats: tf.Tensor, anchors: List[Tuple[float, float]],\n                          num_classes: int, input_shape: Tuple[int, int],\n                          image_shape) -> Tuple[tf.Tensor, tf.Tensor]:\n    \'\'\'Process Conv layer output\'\'\'\n    box_xy, box_wh, box_confidence, box_class_probs = yolo_head(\n        feats, anchors, input_shape)\n    boxes = yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape)\n    boxes = tf.reshape(boxes, [-1, 4])\n    box_scores = box_confidence * box_class_probs\n    box_scores = tf.reshape(box_scores, [-1, num_classes])\n    return boxes, box_scores\n\n\ndef yolo_eval(yolo_outputs: List[tf.Tensor],\n              anchors: np.ndarray,\n              num_classes: int,\n              image_shape,\n              max_boxes: int = 20,\n              score_threshold: float = .6,\n              iou_threshold: float = .5\n             ) -> Tuple[List[tf.Tensor], List[tf.Tensor], List[tf.Tensor]]:\n    """"""Evaluate YOLO model on given input and return filtered boxes.""""""\n\n    num_layers = len(yolo_outputs)\n    anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n\n    input_shape = tf.shape(yolo_outputs[0])[1:3] * 32\n    boxes = []\n    box_scores = []\n    for l in range(num_layers):\n        _boxes, _box_scores = yolo_boxes_and_scores(yolo_outputs[l],\n                                                    anchors[anchor_mask[l]],\n                                                    num_classes, input_shape,\n                                                    image_shape)\n        boxes.append(_boxes)\n        box_scores.append(_box_scores)\n    boxes = tf.concat(boxes, axis=0)\n    box_scores = tf.concat(box_scores, axis=0)\n    max_boxes_tensor = tf.constant(max_boxes, dtype=tf.int32)\n    boxes_ = []\n    scores_ = []\n    classes_ = []\n    for c in range(num_classes):\n        nms_index = tf.image.non_max_suppression(\n            boxes,\n            box_scores[:, c],\n            max_boxes_tensor,\n            iou_threshold=iou_threshold,\n            score_threshold=score_threshold)\n        class_boxes = tf.gather(boxes, nms_index)\n        class_box_scores = tf.gather(box_scores[:, c], nms_index)\n        classes = tf.ones_like(class_box_scores, tf.int32) * c\n        boxes_.append(class_boxes)\n        scores_.append(class_box_scores)\n        classes_.append(classes)\n    boxes_ = tf.concat(boxes_, axis=0)\n    scores_ = tf.concat(scores_, axis=0, name=\'scores\')\n    classes_ = tf.concat(classes_, axis=0, name=\'classes\')\n    boxes_ = tf.cast(boxes_, tf.int32, name=\'boxes\')\n    return boxes_, scores_, classes_\n\n\nclass YoloEval(tf.keras.layers.Layer):\n\n    def __init__(self,\n                 anchors,\n                 num_classes,\n                 max_boxes=20,\n                 score_threshold=.6,\n                 iou_threshold=.5,\n                 **kwargs):\n        super(YoloEval, self).__init__(**kwargs)\n        self.anchors = anchors\n        self.num_classes = num_classes\n        self.max_boxes = max_boxes\n        self.score_threshold = score_threshold\n        self.iou_threshold = iou_threshold\n\n    def call(self, yolo_outputs,image_shape):\n        return yolo_eval(yolo_outputs, self.anchors, self.num_classes,\n                         image_shape, self.max_boxes, self.score_threshold,\n                         self.iou_threshold)\n\n    def get_config(self):\n        config = super(YoloEval, self).get_config()\n        config[\'anchors\'] = self.anchors\n        config[\'num_classes\'] = self.num_classes\n        config[\'max_boxes\'] = self.max_boxes\n        config[\'score_threshold\'] = self.score_threshold\n        config[\'iou_threshold\'] = self.iou_threshold\n\n        return config\n\n\nclass YoloLoss(tf.keras.losses.Loss):\n\n    def __init__(self,\n                    idx,\n                    anchors,\n                    ignore_thresh=.5,\n                    box_loss=BOX_LOSS.GIOU,\n                    print_loss=True):\n        super(YoloLoss, self).__init__(reduction=tf.losses.Reduction.NONE,name=\'yolo_loss\')\n        grid_steps = [32, 16, 8]\n        anchor_masks = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n        self.idx = idx\n        self.ignore_thresh = ignore_thresh\n        self.box_loss = box_loss\n        self.print_loss = print_loss\n        self.grid_step = grid_steps[self.idx]\n        self.anchor = anchors[anchor_masks[idx]]\n\n    def call(self, y_true, yolo_output):\n        \'\'\'Return yolo_loss tensor\n\n        Parameters\n        ----------\n        yolo_output: the output of yolo_body\n        y_true: the output of preprocess_true_boxes\n        anchors: array, shape=(N, 2), wh\n        num_classes: integer\n        ignore_thresh: float, the iou threshold whether to ignore object confidence loss\n\n        Returns\n        -------\n        loss: tensor, shape=(1,)\n\n        \'\'\'\n        loss = 0\n        m = tf.shape(yolo_output)[0]  # batch size, tensor\n        mf = tf.cast(m, yolo_output.dtype)\n        object_mask = y_true[..., 4:5]\n        true_class_probs = y_true[..., 5:]\n        input_shape = tf.shape(yolo_output)[1:3] * self.grid_step\n        grid, pred_xy, pred_wh, box_confidence = yolo_head(\n            yolo_output, self.anchor, input_shape, calc_loss=True)\n        pred_max = tf.reverse(pred_xy + pred_wh / 2., [-1])\n        pred_min = tf.reverse(pred_xy - pred_wh / 2., [-1])\n        pred_box = tf.concat([pred_min, pred_max], -1)\n        \n        true_xy = y_true[..., :2]\n        true_wh = y_true[..., 2:4]\n        true_max = tf.reverse(true_xy + true_wh / 2., [-1])\n        true_min = tf.reverse(true_xy - true_wh / 2., [-1])\n        true_box = tf.concat([true_min, true_max], -1)\n        true_box = tf.clip_by_value(true_box, 0, 1)\n        object_mask_bool = tf.cast(object_mask, \'bool\')\n\n        masked_true_box = tf.boolean_mask(true_box, object_mask_bool[..., 0])\n        iou = do_giou_calculate(\n            tf.expand_dims(pred_box, -2),\n            tf.expand_dims(masked_true_box, 0),\n            mode=\'iou\')\n        best_iou = tf.reduce_max(iou, axis=-1)\n        ignore_mask = tf.cast(best_iou < self.ignore_thresh, masked_true_box.dtype)\n\n        ignore_mask = tf.expand_dims(ignore_mask, -1)\n        # focal_loss = focal(object_mask, box_confidence)\n        confidence_loss = (object_mask * tf.nn.sigmoid_cross_entropy_with_logits(labels=object_mask,\n                                                                                logits=yolo_output[..., 4:5]) + \\\n                        (1 - object_mask) * tf.nn.sigmoid_cross_entropy_with_logits(labels=object_mask,\n                                                                                    logits=yolo_output[...,\n                                                                                            4:5]) * ignore_mask)\n        class_loss = object_mask * tf.nn.sigmoid_cross_entropy_with_logits(\n            labels=true_class_probs, logits=yolo_output[..., 5:])\n        confidence_loss = tf.reduce_sum(confidence_loss) / mf\n        class_loss = tf.reduce_sum(class_loss) / mf\n\n        if self.box_loss == BOX_LOSS.GIOU:\n            giou = do_giou_calculate(pred_box, true_box)\n            giou_loss = object_mask * (1 - tf.expand_dims(giou, -1))\n            giou_loss = tf.reduce_sum(giou_loss) / mf\n            loss += giou_loss + confidence_loss + class_loss\n            if self.print_loss:\n                tf.print(str(self.idx)+\':\',giou_loss, confidence_loss, class_loss,tf.reduce_sum(ignore_mask))\n        elif self.box_loss == BOX_LOSS.MSE:\n            grid_shape = tf.cast(tf.shape(yolo_output)[1:3], y_true.dtype)\n            raw_true_xy = y_true[..., :2] * grid_shape[::-1] - grid\n            raw_true_wh = tf.math.log(y_true[..., 2:4] /\n                                    tf.cast(anchors[anchor_mask[idx]], y_true.dtype)*\n                                    tf.cast(input_shape[::-1], y_true.dtype) )\n            raw_true_wh = tf.keras.backend.switch(object_mask, raw_true_wh,\n                                                tf.zeros_like(raw_true_wh))\n            box_loss_scale = 2 - y_true[..., 2:3] * y_true[..., 3:4]\n            xy_loss = object_mask * box_loss_scale * tf.nn.sigmoid_cross_entropy_with_logits(\n                labels=raw_true_xy, logits=yolo_output[..., 0:2])\n            wh_loss = object_mask * box_loss_scale * 0.5 * tf.square(\n                raw_true_wh - yolo_output[..., 2:4])\n            xy_loss = tf.reduce_sum(xy_loss) / mf\n            wh_loss = tf.reduce_sum(wh_loss) / mf\n            loss += xy_loss + wh_loss + confidence_loss + class_loss\n            if print_loss:\n                tf.print(loss, xy_loss, wh_loss, confidence_loss, class_loss,\n                        tf.reduce_sum(ignore_mask))\n        return loss'"
yolo3/override.py,23,"b'import tensorflow as tf\n\n# class FreezableBatchNorm(tf.keras.layers.BatchNormalization):\n#   """"""Batch normalization layer (Ioffe and Szegedy, 2014).\n#\n#   This is a `freezable` batch norm layer that supports setting the `training`\n#   parameter in the __init__ method rather than having to set it either via\n#   the Keras learning phase or via the `call` method parameter. This layer will\n#   forward all other parameters to the default Keras `BatchNormalization`\n#   layer\n#\n#   This is class is necessary because Object Detection model training sometimes\n#   requires batch normalization layers to be `frozen` and used as if it was\n#   evaluation time, despite still training (and potentially using dropout layers)\n#\n#   Like the default Keras BatchNormalization layer, this will normalize the\n#   activations of the previous layer at each batch,\n#   i.e. applies a transformation that maintains the mean activation\n#   close to 0 and the activation standard deviation close to 1.\n#\n#   Arguments:\n#     training: Boolean or None. If True, the batch normalization layer will\n#       normalize the input batch using the batch mean and standard deviation,\n#       and update the total moving mean and standard deviations. If False, the\n#       layer will normalize using the moving average and std. dev, without\n#       updating the learned avg and std. dev.\n#       If None, the layer will follow the keras BatchNormalization layer\n#       strategy of checking the Keras learning phase at `call` time to decide\n#       what to do.\n#     **kwargs: The keyword arguments to forward to the keras BatchNormalization\n#         layer constructor.\n#\n#   Input shape:\n#       Arbitrary. Use the keyword argument `input_shape`\n#       (tuple of integers, does not include the samples axis)\n#       when using this layer as the first layer in a model.\n#\n#   Output shape:\n#       Same shape as input.\n#\n#   References:\n#       - [Batch Normalization: Accelerating Deep Network Training by Reducing\n#         Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n#   """"""\n#\n#   def __init__(self, training=None, **kwargs):\n#     super(FreezableBatchNorm, self).__init__(**kwargs)\n#     self._training = training\n#\n#   def call(self, inputs, training=None):\n#     if training is None:\n#       training = self._training\n#     return super(FreezableBatchNorm, self).call(inputs, training=training)\n\n\ndef _make_divisible(v, divisor, min_value=None):\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\ndef _fixed_padding(inputs, kernel_size, rate=1):\n    """"""Pads the input along the spatial dimensions independently of input size.\n\n  Args:\n    inputs: A tensor of size [batch, height_in, width_in, channels].\n    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.\n                 Should be a positive integer.\n    rate: An integer, rate for atrous convolution.\n\n  Returns:\n    output: A tensor of size [batch, height_out, width_out, channels] with the\n      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).\n  """"""\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    padded_inputs = tf.pad(\n        inputs, [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])\n    return padded_inputs\n\n\nclass _LayersOverride:\n\n    def __init__(self,\n                 default_batchnorm_momentum=0.999,\n                 conv_hyperparams=None,\n                 use_explicit_padding=False,\n                 alpha=1.0,\n                 min_depth=None):\n        """"""Alternative tf.keras.layers interface, for use by the Keras MobileNetV2.\n\n        It is used by the Keras applications kwargs injection API to\n        modify the Mobilenet v2 Keras application with changes required by\n        the Object Detection API.\n\n        These injected interfaces make the following changes to the network:\n\n        - Applies the Object Detection hyperparameter configuration\n        - Supports FreezableBatchNorms\n        - Adds support for a min number of filters for each layer\n        - Makes the `alpha` parameter affect the final convolution block even if it\n            is less than 1.0\n        - Adds support for explicit padding of convolutions\n\n        Args:\n          batchnorm_training: Bool. Assigned to Batch norm layer `training` param\n            when constructing `freezable_batch_norm.FreezableBatchNorm` layers.\n          default_batchnorm_momentum: Float. When \'conv_hyperparams\' is None,\n            batch norm layers will be constructed using this value as the momentum.\n          conv_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object\n            containing hyperparameters for convolution ops. Optionally set to `None`\n            to use default mobilenet_v2 layer builders.\n          use_explicit_padding: If True, use \'valid\' padding for convolutions,\n            but explicitly pre-pads inputs so that the output dimensions are the\n            same as if \'same\' padding were used. Off by default.\n          alpha: The width multiplier referenced in the MobileNetV2 paper. It\n            modifies the number of filters in each convolutional layer.\n          min_depth: Minimum number of filters in the convolutional layers.\n        """"""\n        self._default_batchnorm_momentum = default_batchnorm_momentum\n        self._conv_hyperparams = conv_hyperparams\n        self._use_explicit_padding = use_explicit_padding\n        self._alpha = alpha\n        self._min_depth = min_depth\n        self._regularizer = tf.keras.regularizers.l2(0.00004)\n        self._initializer = tf.random_normal_initializer(stddev=0.03)\n\n    # def _FixedPaddingLayer(self,kernel_size):\n    #     return tf.keras.layers.Lambda(lambda x: _fixed_padding(x, kernel_size))\n\n    # def Conv2D(self,filters,**kwargs):\n    #     """"""Builds a Conv2D layer according to the current Object Detection config.\n    #\n    #     Overrides the Keras MobileNetV2 application\'s convolutions with ones that\n    #     follow the spec specified by the Object Detection hyperparameters.\n    #\n    #     Args:\n    #       filters: The number of filters to use for the convolution.\n    #       **kwargs: Keyword args specified by the Keras application for\n    #         constructing the convolution.\n    #\n    #     Returns:\n    #       A one-arg callable that will either directly apply a Keras Conv2D layer to\n    #       the input argument, or that will first pad the input then apply a Conv2D\n    #       layer.\n    #     """"""\n    #     if kwargs.get(\'name\')==\'Conv_1\' and self._alpha<1.0:\n    #         filters=_make_divisible(1280*self._alpha,8)\n    #\n    #     if self._min_depth and (filters<self._min_depth) and not kwargs.get(\'name\').endswith(\'expand\'):\n    #         filters=self._min_depth\n    #\n    #     # if self._conv_hyperparams:\n    #     #     kwargs=self._conv_hyperparams.params(**kwargs)\n    #     # else:\n    #     #     kwargs[\'kernel_regularizer\']=self._regularizer\n    #     #     kwargs[\'kernel_initializer\']=self._initializer\n    #\n    #     kwargs[\'padding\']=\'same\'\n    #     kernel_size=kwargs.get(\'kernel_size\')\n    #     if self._use_explicit_padding and kernel_size>1:\n    #         kwargs[\'padding\']=\'valid\'\n    #         def padded_conv(features):\n    #             padded_features=self._FixedPaddingLayer(kernel_size)(features)\n    #             return tf.keras.layers.Conv2D(filters,**kwargs)(padded_features)\n    #         return padded_conv\n    #     else:\n    #         return tf.keras.layers.Conv2D(filters,**kwargs)\n\n    # def DepthwiseConv2D(self,**kwargs):\n    #     """"""Builds a DepthwiseConv2D according to the Object Detection config.\n    #\n    #     Overrides the Keras MobileNetV2 application\'s convolutions with ones that\n    #     follow the spec specified by the Object Detection hyperparameters.\n    #\n    #     Args:\n    #       **kwargs: Keyword args specified by the Keras application for\n    #         constructing the convolution.\n    #\n    #     Returns:\n    #       A one-arg callable that will either directly apply a Keras DepthwiseConv2D\n    #       layer to the input argument, or that will first pad the input then apply\n    #       the depthwise convolution.\n    #     """"""\n    #     if self._conv_hyperparams:\n    #         kwargs=self._conv_hyperparams.params(**kwargs)\n    #     else:\n    #         kwargs[\'depthwise_initializer\']=self._initializer\n    #\n    #     kwargs[\'padding\']=\'same\'\n    #     kernel_size=kwargs.get(\'kernel_size\')\n    #     if self._use_explicit_padding and kernel_size>1:\n    #         kwargs[\'padding\']=\'valid\'\n    #         def padded_depthwise_conv(features):\n    #             padded_features=self._FixedPaddingLayer(kernel_size)(features)\n    #             return tf.keras.layers.DepthwiseConv2D(**kwargs)(padded_features)\n    #         return padded_depthwise_conv\n    #     else:\n    #         return tf.keras.layers.DepthwiseConv2D(**kwargs)\n\n    def BatchNormalization(self, **kwargs):\n        """"""Builds a normalization layer.\n\n        Overrides the Keras application batch norm with the norm specified by the\n        Object Detection configuration.\n\n        Args:\n          **kwargs: Only the name is used, all other params ignored.\n            Required for matching `layers.BatchNormalization` calls in the Keras\n            application.\n\n        Returns:\n          A normalization layer specified by the Object Detection hyperparameter\n          configurations.\n        """"""\n        name = kwargs.get(\'name\')\n        if self._conv_hyperparams:\n            return self._conv_hyperparams.build_batch_norm(name=name)\n        else:\n            return tf.keras.layers.BatchNormalization(\n                momentum=self._default_batchnorm_momentum, name=name)\n\n    # def Input(self,shape):\n    #     """"""Builds an Input layer.\n    #\n    #     Overrides the Keras application Input layer with one that uses a\n    #     tf.placeholder_with_default instead of a tf.placeholder. This is necessary\n    #     to ensure the application works when run on a TPU.\n    #\n    #     Args:\n    #       shape: The shape for the input layer to use. (Does not include a dimension\n    #         for the batch size).\n    #     Returns:\n    #       An input layer for the specified shape that internally uses a\n    #       placeholder_with_default.\n    #     """"""\n    #     default_size = 224\n    #     default_batch_size = 1\n    #     shape = list(shape)\n    #     default_shape = [default_size if dim is None else dim for dim in shape]\n    #\n    #     input_tensor = tf.constant(0.0, shape=[default_batch_size] + default_shape)\n    #\n    #     placeholder_with_default = tf.placeholder_with_default(\n    #         input=input_tensor, shape=[None] + shape)\n    #     return tf.keras.layers.Input(tensor=placeholder_with_default)\n\n    # def ReLU(self, *args, **kwargs):\n    #     """"""Builds an activation layer.\n    #\n    #     Overrides the Keras application ReLU with the activation specified by the\n    #     Object Detection configuration.\n    #\n    #     Args:\n    #       *args: Ignored, required to match the `tf.keras.ReLU` interface\n    #       **kwargs: Only the name is used,\n    #         required to match `tf.keras.ReLU` interface\n    #\n    #     Returns:\n    #       An activation layer specified by the Object Detection hyperparameter\n    #       configurations.\n    #     """"""\n    #     name = kwargs.get(\'name\')\n    #     if self._conv_hyperparams:\n    #         return self._conv_hyperparams.build_activation_layer(name=name)\n    #     else:\n    #         return tf.keras.layers.Lambda(tf.nn.relu6, name=name)\n\n    # def ZeroPadding2D(self, **kwargs):\n    #     """"""Replaces explicit padding in the Keras application with a no-op.\n    #\n    #     Args:\n    #       **kwargs: Ignored, required to match the Keras applications usage.\n    #\n    #     Returns:\n    #       A no-op identity lambda.\n    #     """"""\n    #     return lambda x: x\n\n    def __getattr__(self, item):\n        return getattr(tf.keras.layers, item)\n\n\ndef mobilenet_v2(default_batchnorm_momentum=0.999,\n                 conv_hyperparams=None,\n                 use_explicit_padding=False,\n                 alpha=1.0,\n                 min_depth=None,\n                 **kwargs):\n    """"""Instantiates the MobileNetV2 architecture, modified for object detection.\n\n      This wraps the MobileNetV2 tensorflow Keras application, but uses the\n      Keras application\'s kwargs-based monkey-patching API to override the Keras\n      architecture with the following changes:\n\n      - Changes the default batchnorm momentum to 0.9997\n      - Applies the Object Detection hyperparameter configuration\n      - Supports FreezableBatchNorms\n      - Adds support for a min number of filters for each layer\n      - Makes the `alpha` parameter affect the final convolution block even if it\n          is less than 1.0\n      - Adds support for explicit padding of convolutions\n      - Makes the Input layer use a tf.placeholder_with_default instead of a\n          tf.placeholder, to work on TPUs.\n\n      Args:\n          batchnorm_training: Bool. Assigned to Batch norm layer `training` param\n            when constructing `freezable_batch_norm.FreezableBatchNorm` layers.\n          default_batchnorm_momentum: Float. When \'conv_hyperparams\' is None,\n            batch norm layers will be constructed using this value as the momentum.\n          conv_hyperparams: A `hyperparams_builder.KerasLayerHyperparams` object\n            containing hyperparameters for convolution ops. Optionally set to `None`\n            to use default mobilenet_v2 layer builders.\n          use_explicit_padding: If True, use \'valid\' padding for convolutions,\n            but explicitly pre-pads inputs so that the output dimensions are the\n            same as if \'same\' padding were used. Off by default.\n          alpha: The width multiplier referenced in the MobileNetV2 paper. It\n            modifies the number of filters in each convolutional layer.\n          min_depth: Minimum number of filters in the convolutional layers.\n          **kwargs: Keyword arguments forwarded directly to the\n            `tf.keras.applications.MobilenetV2` method that constructs the Keras\n            model.\n\n      Returns:\n          A Keras model instance.\n      """"""\n    layers_override = _LayersOverride(\n        default_batchnorm_momentum=default_batchnorm_momentum,\n        conv_hyperparams=conv_hyperparams,\n        use_explicit_padding=use_explicit_padding,\n        min_depth=min_depth,\n        alpha=alpha)\n    return tf.keras.applications.MobileNetV2(alpha=alpha,\n                                             layers=layers_override,\n                                             **kwargs)\n'"
yolo3/train.py,10,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport neural_structured_learning as nsl\nimport numpy as np\n\n\nclass AdvLossModel(tf.keras.Model):\n    def _compute_total_loss(self, y_trues, y_preds, sample_weights=None):\n        loss = 0\n        for y_true, y_pred, loss_object in zip(y_trues, y_preds, self.loss):\n            loss += loss_object(y_true, y_pred)\n        return loss\n\n    def _train_step(self, inputs):\n        images, y_trues = inputs\n        with tf.GradientTape() as tape_w:\n            tape_w.watch(self.trainable_variables)\n            if self.use_adv:\n                with tf.GradientTape() as tape_x:\n                    tape_x.watch(images)\n                    y_preds = self(images, training=True)\n                    loss = self._compute_total_loss(y_trues, y_preds)\n                    adv_loss = nsl.keras.adversarial_loss(\n                        images,\n                        y_trues,\n                        self,\n                        self._compute_total_loss,\n                        labeled_loss=loss,\n                        gradient_tape=tape_x,\n                        adv_config=self.adv_config)\n                    loss += self.adv_config.multiplier * adv_loss\n            else:\n                y_preds = self(images, training=True)\n                loss = self._compute_total_loss(y_trues, y_preds)\n        gradients = tape_w.gradient(loss, self.trainable_variables)\n        optimizer_op = self.optimizer.apply_gradients(\n            zip(gradients, self.trainable_variables))\n        if self.use_ema:\n            ema = tf.train.ExponentialMovingAverage(decay=0.9999,zero_debias=True)\n            with tf.control_dependencies([optimizer_op]):\n                ema.apply(self.trainable_variables)\n        return loss\n\n    def _val_step(self, inputs):\n        images, y_trues = inputs\n        y_preds = self(images, training=False)\n        loss = self._compute_total_loss(y_trues, y_preds)\n        return loss\n\n    @tf.function\n    def _distributed_epoch(self, dataset, step):\n        total_loss = 0.0\n        num_batches = 0.0\n        for batch in dataset:\n            if self.writer is not None:\n                with self.writer.as_default():\n                    tf.summary.image(\n                        ""Training data"",\n                        tf.cast(batch[0] * 255, tf.uint8),\n                        max_outputs=8)\n            per_replica_loss = self._distribution_strategy.experimental_run_v2(\n                self._train_step if step else self._val_step, args=(batch,))\n            total_loss += self._distribution_strategy.reduce(\n                tf.distribute.ReduceOp.SUM, per_replica_loss,\n                axis=None)\n            num_batches += 1.0\n            tf.print(num_batches, \':\', total_loss / num_batches, sep=\'\')\n        total_loss = total_loss / num_batches\n        return total_loss\n\n    def _configure_callbacks(self, callbacks):\n        for callback in callbacks:\n            callback.set_model(self)\n\n    def fit(\n            self,\n            epochs,\n            callbacks,\n            train_dataset,\n            val_dataset,\n            writer=None,\n            use_ema=False,\n            use_adv=False,\n            adv_config=nsl.configs.make_adv_reg_config(\n                multiplier=0.2, adv_step_size=0.2, adv_grad_norm=\'infinity\'),\n    ):\n        self.writer = writer\n        self.use_ema = use_ema\n        self.use_adv = use_adv\n        self.adv_config = adv_config\n        self._configure_callbacks(callbacks)\n        logs = {}\n        self.stop_training=False\n        for callback in callbacks:\n            callback.on_train_begin(logs)\n        for epoch in range(epochs):\n            if self.stop_training:\n                break\n            for callback in callbacks:\n                callback.on_epoch_begin(epoch, logs)\n            train_loss = self._distributed_epoch(\n                train_dataset, True)\n            val_loss = self._distributed_epoch(\n                val_dataset, False)\n            logs[\'loss\'] = train_loss\n            logs[\'val_loss\'] = val_loss\n            for callback in callbacks:\n                callback.on_epoch_end(epoch, logs)\n\n        for callback in callbacks:\n            callback.on_train_end(logs)'"
yolo3/utils.py,103,"b'""""""Miscellaneous utility functions.""""""\n\nfrom functools import reduce\nimport tensorflow as tf\nimport numpy as np\n\ndef do_giou_calculate(b1, b2, mode=\'giou\'):\n    """"""\n    Args:\n        b1: bounding box. The coordinates of the each bounding box in boxes are\n        encoded as [y_min, x_min, y_max, x_max].\n        b2: the other bounding box. The coordinates of the each bounding box\n        in boxes are encoded as [y_min, x_min, y_max, x_max].\n        mode: one of [\'giou\', \'iou\'],\n        decided to calculate giou loss or iou loss.\n    Returns:\n        GIoU loss float `Tensor`.\n    """"""\n    zero = tf.convert_to_tensor(0., b1.dtype)\n    b1_ymin, b1_xmin, b1_ymax, b1_xmax = tf.unstack(b1, 4, axis=-1)\n    b2_ymin, b2_xmin, b2_ymax, b2_xmax = tf.unstack(b2, 4, axis=-1)\n    b1_width = tf.maximum(zero, b1_xmax - b1_xmin)\n    b1_height = tf.maximum(zero, b1_ymax - b1_ymin)\n    b2_width = tf.maximum(zero, b2_xmax - b2_xmin)\n    b2_height = tf.maximum(zero, b2_ymax - b2_ymin)\n    b1_area = b1_width * b1_height\n    b2_area = b2_width * b2_height\n\n    intersect_ymin = tf.maximum(b1_ymin, b2_ymin)\n    intersect_xmin = tf.maximum(b1_xmin, b2_xmin)\n    intersect_ymax = tf.minimum(b1_ymax, b2_ymax)\n    intersect_xmax = tf.minimum(b1_xmax, b2_xmax)\n    intersect_width = tf.maximum(zero, intersect_xmax - intersect_xmin)\n    intersect_height = tf.maximum(zero, intersect_ymax - intersect_ymin)\n    intersect_area = intersect_width * intersect_height\n\n    union_area = b1_area + b2_area - intersect_area\n    iou = tf.math.divide_no_nan(intersect_area, union_area)\n    if mode == \'iou\':\n        return iou\n\n    enclose_ymin = tf.minimum(b1_ymin, b2_ymin)\n    enclose_xmin = tf.minimum(b1_xmin, b2_xmin)\n    enclose_ymax = tf.maximum(b1_ymax, b2_ymax)\n    enclose_xmax = tf.maximum(b1_xmax, b2_xmax)\n    enclose_width = tf.maximum(zero, enclose_xmax - enclose_xmin)\n    enclose_height = tf.maximum(zero, enclose_ymax - enclose_ymin)\n    enclose_area = enclose_width * enclose_height\n    giou = iou - tf.math.divide_no_nan(\n        (enclose_area - union_area), enclose_area)\n    return giou\n\n\ndef compose(*funcs):\n    """"""Compose arbitrarily many functions, evaluated left to right.\n\n    Reference: https://mathieularose.com/function-composition-in-python/\n    """"""\n    if funcs:\n        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)\n    else:\n        raise ValueError(\'Composition of empty sequence not supported.\')\n\n\ndef letterbox_image(image, size):\n    \'\'\'resize image with unchanged aspect ratio using padding\'\'\'\n    if len(image.shape) == 4:\n        iw, ih = tf.cast(tf.shape(image)[2],\n                         tf.int32), tf.cast(tf.shape(image)[1], tf.int32)\n    elif len(image.shape) == 3:\n        iw, ih = tf.cast(tf.shape(image)[1],\n                         tf.int32), tf.cast(tf.shape(image)[0], tf.int32)\n    w, h = tf.cast(size[1], tf.int32), tf.cast(size[0], tf.int32)\n    nh = tf.cast(tf.cast(ih, tf.float64) * tf.minimum(w / iw, h / ih), tf.int32)\n    nw = tf.cast(tf.cast(iw, tf.float64) * tf.minimum(w / iw, h / ih), tf.int32)\n    dx = (w - nw) // 2\n    dy = (h - nh) // 2\n\n    resized_image = tf.image.resize(image, [nh, nw])\n    new_image = tf.image.pad_to_bounding_box(resized_image, dy, dx, h, w)\n    return new_image\n\n\ndef random_gamma(image, min, max):\n    val = tf.random.uniform([], min, max)\n    return tf.image.adjust_gamma(image, val)\n\n\ndef random_blur(image):\n    import cv2\n    gaussian_blur = lambda image: cv2.GaussianBlur(image.numpy(), (5, 5), 0)\n    h, w = image.shape.as_list()[:2]\n    image = tf.py_function(gaussian_blur, [image], tf.float32)\n    image.set_shape([h, w, 3])\n    return image\n\n\ndef get_anchors(anchors_path):\n    with open(anchors_path) as f:\n        anchors = f.readline()\n    anchors = [float(x) for x in anchors.split(\',\')]\n    return np.array(anchors, np.float32).reshape(-1, 2)\n\n\ndef bind(instance, func, as_name=None):\n    if as_name is None:\n        as_name = func.__name__\n    bound_method = func.__get__(instance, instance.__class__)\n    setattr(instance, as_name, bound_method)\n    return bound_method\n\n\ndef get_classes(classes_path):\n    """"""loads the classes""""""\n    with open(classes_path) as f:\n        class_names = f.readlines()\n    class_names = [c.strip() for c in class_names]\n    return class_names\n\n\ndef get_random_data(image,\n                    xmins,\n                    xmaxs,\n                    ymins,\n                    ymaxs,\n                    labels,\n                    input_shape,\n                    min_scale=0.25,\n                    max_scale=2,\n                    jitter=0.3,\n                    min_gamma=0.8,\n                    max_gamma=2,\n                    blur=False,\n                    flip=True,\n                    hue=.5,\n                    sat=.5,\n                    val=0.,\n                    cont=.1,\n                    noise=0,\n                    max_boxes=20,\n                    min_jpeg_quality=80,\n                    max_jpeg_quality=100,\n                    train: bool = True):\n    \'\'\'random preprocessing for real-time data augmentation\'\'\'\n    input_shape=tf.keras.backend.get_value(input_shape)\n    iw, ih = tf.cast(tf.shape(image)[1],\n                     tf.float32), tf.cast(tf.shape(image)[0], tf.float32)\n    w, h = tf.cast(input_shape[1], tf.float32), tf.cast(input_shape[0],\n                                                        tf.float32)\n    xmaxs = tf.expand_dims(xmaxs, 0)\n    xmins = tf.expand_dims(xmins, 0)\n    ymaxs = tf.expand_dims(ymaxs, 0)\n    ymins = tf.expand_dims(ymins, 0)\n    labels = tf.expand_dims(labels, 0)\n    if train:\n        new_ar = (w / h) * (tf.random.uniform([], 1 - jitter, 1 + jitter) /\n                            tf.random.uniform([], 1 - jitter, 1 + jitter))\n        scale = tf.random.uniform([], min_scale, max_scale)\n        ratio = tf.cond(tf.less(\n            new_ar, 1), lambda: scale * new_ar, lambda: scale / new_ar)\n        ratio = tf.maximum(ratio, 1)\n        nw, nh = tf.cond(tf.less(\n            new_ar,\n            1), lambda: (ratio * h, scale * h), lambda: (scale * w, ratio * w))\n        dx = tf.random.uniform([], 0, w - nw)\n        dy = tf.random.uniform([], 0, h - nh)\n        image = tf.image.resize(image,\n                                [tf.cast(nh, tf.int32),\n                                 tf.cast(nw, tf.int32)])\n\n\n        \n        def crop_and_pad(image, dx, dy):\n            dy_t = tf.cast(tf.math.maximum(-dy, 0), tf.int32)\n            dx_t = tf.cast(tf.math.maximum(-dx, 0), tf.int32)\n            image = tf.image.crop_to_bounding_box(\n                image, dy_t, dx_t,\n                tf.math.minimum(tf.cast(h, tf.int32), tf.cast(nh, tf.int32)),\n                tf.math.minimum(tf.cast(w, tf.int32), tf.cast(nw, tf.int32)))\n            image = tf.image.pad_to_bounding_box(\n                image, tf.cast(tf.math.maximum(dy, 0), tf.int32),\n                tf.cast(tf.math.maximum(dx, 0), tf.int32), tf.cast(\n                    h, tf.int32), tf.cast(w, tf.int32))\n            return image\n\n        image = tf.cond(\n            tf.logical_or(nw > w, nh > h), lambda: crop_and_pad(image, dx, dy),\n            lambda: tf.image.pad_to_bounding_box(\n                image, tf.cast(tf.math.maximum(dy, 0), tf.int32),\n                tf.cast(tf.math.maximum(dx, 0), tf.int32), tf.cast(\n                    h, tf.int32), tf.cast(w, tf.int32)))\n\n        xmins = xmins * nw / iw + dx\n        xmaxs = xmaxs * nw / iw + dx\n        ymins = ymins * nh / ih + dy\n        ymaxs = ymaxs * nh / ih + dy\n        if flip:\n            image, xmins, xmaxs = tf.cond(\n                tf.less(\n                    tf.random.uniform([]),\n                    0.5), lambda: (tf.image.flip_left_right(image), w - xmaxs, w\n                                   - xmins), lambda: (image, xmins, xmaxs))\n        if hue > 0:\n            image = tf.image.random_hue(image, hue)\n        if sat > 0:\n            image = tf.image.random_saturation(image, 1 - sat, 1 + sat)\n        if val > 0:\n            image = tf.image.random_brightness(image, val)\n        if min_gamma < max_gamma:\n            image = random_gamma(image, min_gamma, max_gamma)\n        if cont > 0:\n            image = tf.image.random_contrast(image, 1 - cont, 1 + cont)\n        if min_jpeg_quality < max_jpeg_quality:\n            image = tf.image.random_jpeg_quality(image, min_jpeg_quality,\n                                                 max_jpeg_quality)\n        if noise > 0:\n            image = image + tf.cast(\n                tf.random.uniform(shape=[input_shape[0], input_shape[1], 3],\n                                  minval=0,\n                                  maxval=noise), tf.float32)\n        if blur:\n            image = random_blur(image)\n    else:\n        nh = ih * tf.minimum(w / iw, h / ih)\n        nw = iw * tf.minimum(w / iw, h / ih)\n        dx = (w - nw) / 2\n        dy = (h - nh) / 2\n        image = tf.image.resize(image,\n                                [tf.cast(nh, tf.int32),\n                                 tf.cast(nw, tf.int32)])\n        image = tf.image.pad_to_bounding_box(image, tf.cast(dy, tf.int32),\n                                                 tf.cast(dx, tf.int32),\n                                                 tf.cast(h, tf.int32),\n                                                 tf.cast(w, tf.int32))\n        xmins = xmins * nw / iw + dx\n        xmaxs = xmaxs * nw / iw + dx\n        ymins = ymins * nh / ih + dy\n        ymaxs = ymaxs * nh / ih + dy\n\n    bbox = tf.concat([xmins, ymins, xmaxs, ymaxs,\n                      tf.cast(labels, tf.float32)], 0)\n    bbox = tf.transpose(bbox, [1, 0])\n    image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n    bbox = tf.clip_by_value(bbox,\n                            clip_value_min=0,\n                            clip_value_max=tf.cast(input_shape[0] - 1,\n                                                   tf.float32))\n    bbox_w = bbox[..., 2] - bbox[..., 0]\n    bbox_h = bbox[..., 3] - bbox[..., 1]\n    bbox = tf.boolean_mask(bbox, tf.logical_and(bbox_w > 1, bbox_h > 1))\n    bbox = tf.cond(tf.greater(\n        tf.shape(bbox)[0], max_boxes), lambda: bbox[:max_boxes], lambda: bbox)\n\n    return image, bbox\n\n\ndef preprocess_true_boxes(true_boxes, input_shape, anchors, num_classes):\n    \'\'\'Preprocess true boxes to training input format\n\n    Parameters\n    ----------\n    true_boxes: array, shape=(m, T, 5)\n        Absolute x_min, y_min, x_max, y_max, class_id relative to input_shape.\n    input_shape: array-like, wh, multiples of 32\n    anchors: array, shape=(N, 2), wh\n    num_classes: integer\n\n    Returns\n    -------\n    y_true: list of array, shape like yolo_outputs, xywh are reletive value\n\n    \'\'\'\n    num_layers = len(anchors) // 3  # default setting\n    anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]]\n    true_boxes = np.array(true_boxes, dtype=\'float32\')\n    input_shape = np.array(input_shape, dtype=\'int32\')\n    boxes_xy = (true_boxes[..., 0:2] + true_boxes[..., 2:4]) // 2\n    boxes_wh = true_boxes[..., 2:4] - true_boxes[..., 0:2]\n    true_boxes[..., 0:2] = boxes_xy / input_shape[::-1]\n    true_boxes[..., 2:4] = boxes_wh / input_shape[::-1]\n\n    grid_shapes = [\n        np.round(input_shape / [32, 16, 8][l]).astype(np.int32)\n        for l in range(num_layers)\n    ]\n    y_true = [\n        np.zeros((grid_shapes[l][0], grid_shapes[l][1], len(\n            anchor_mask[l]), 5 + num_classes),\n                 dtype=\'float32\') for l in range(num_layers)\n    ]\n\n    # Expand dim to apply broadcasting.\n    anchors = np.expand_dims(anchors, 0)\n    anchor_maxes = anchors / 2.\n    anchor_mins = -anchor_maxes\n    valid_mask = boxes_wh[..., 0] > 0\n    wh = boxes_wh[valid_mask]\n    # Expand dim to apply broadcasting.\n    wh = np.expand_dims(wh, -2)\n    box_maxes = wh / 2.\n    box_mins = -box_maxes\n\n    anchor_box=tf.stack([anchor_mins[...,1],anchor_mins[...,0],anchor_maxes[...,1],anchor_maxes[...,0]],axis=-1)\n    bbox=tf.stack([box_mins[...,1],box_mins[...,0],box_maxes[...,1],box_maxes[...,0]],axis=-1)\n    iou=do_giou_calculate(anchor_box,bbox,mode=\'iou\').numpy()\n\n    # Find best anchor for each true box\n    best_anchor = np.argmax(iou, axis=-1)\n\n    for t, n in enumerate(best_anchor):\n        for l in range(num_layers):\n            if n in anchor_mask[l]:\n                i = np.floor(true_boxes[t, 0] *\n                             grid_shapes[l][1]).astype(\'int32\')\n                j = np.floor(true_boxes[t, 1] *\n                             grid_shapes[l][0]).astype(\'int32\')\n                k = anchor_mask[l].index(n)\n                c = true_boxes[t, 4].astype(\'int32\')\n                y_true[l][j, i, k, 0:4] = true_boxes[t, 0:4]\n                y_true[l][j, i, k, 4] = 1.\n                y_true[l][j, i, k, 5 + c] = 1.\n\n    return y_true[0], y_true[1], y_true[2]\n\n\nclass ModelFactory(object):\n\n    def __init__(self,\n                 input=tf.keras.layers.Input(shape=(None, None, 3)),\n                 weights_path=None):\n        self.input = input\n        self.weights_path = weights_path\n\n    def build(self, model_builder, freeze_layers=None, *args, **kwargs):\n        model_body = model_builder(self.input, *args, **kwargs)\n        if self.weights_path is not None:\n            model_body.load_weights(self.weights_path, by_name=True)\n            print(\'Load weights {}.\'.format(self.weights_path))\n        # Freeze the darknet body or freeze all but 2 output layers.\n        freeze_layers = freeze_layers or len(model_body.layers) - 3\n        for i in range(freeze_layers):\n            model_body.layers[i].trainable = False\n        print(\'Freeze the first {} layers of total {} layers.\'.format(\n            freeze_layers, len(model_body.layers)))\n        return model_body\n'"
