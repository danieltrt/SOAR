file_path,api_count,code
setup.py,0,"b""from setuptools import setup, find_packages\n\nsetup(\n    name='yadlt',\n    version='0.0.6',\n    url='https://github.com/blackecho/Deep-Learning-Tensorflow',\n    download_url='https://github.com/blackecho/Deep-Learning-TensorFlow/tarball/0.0.6',\n    author='Gabriele Angeletti',\n    author_email='angeletti.gabriele@gmail.com',\n    description='Implementation of various deep learning algorithms using Tensorflow. Class interfaces is sklearn-like.',\n    packages=find_packages(exclude=['tests']),\n    zip_safe=False,\n    include_package_data=True,\n    platforms='any',\n    license='MIT',\n    install_requires=[],\n)\n"""
cmd_line/__init__.py,0,"b'""""""Scripts to run models implemented in yadlt from the command line.""""""'"
tests/__init__.py,0,b''
tests/test_utils.py,0,"b'import numpy as np\nimport unittest\n\nimport utils\n\n\nclass TestUtilsMethods(unittest.TestCase):\n    """""" Test the utils method in the utils module.\n    """"""\n\n    def setUp(self):\n        """""" Setup values for testing.\n        """"""\n        self.v = 13\n        self.x = np.random.rand(39, 58)\n\n    def test_masking_noise(self):\n        """""" test masking noise function.\n        """"""\n        x_noise = utils.masking_noise(self.x, self.v)\n\n        for sample in x_noise:\n            self.assertEqual(sum([i == 0 for i in sample]), self.v)\n\n    def test_salt_and_pepper_noise_with_min_max(self):\n        """""" test the salt and pepper function with specified min and max values.\n        """"""\n        x_sp = utils.salt_and_pepper_noise(self.x, self.v, 0, 1)\n\n        for sample in x_sp:\n            salted_elems = sum([i == 0 or i == 1 for i in sample])\n            self.assertEqual(salted_elems, self.v)\n\n    def test_salt_and_pepper_noise_without_min_max(self):\n        """""" test the salt and pepper function without specified min and max values.\n        """"""\n        x_sp = utils.salt_and_pepper_noise(self.x, self.v)\n\n        mn = self.x.min()\n        mx = self.x.max()\n\n        for sample in x_sp:\n            salted_elements = sum([i == mn or i == mx for i in sample])\n            self.assertAlmostEqual(salted_elements, self.v, delta=2)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
yadlt/__init__.py,0,"b'""""""Yadlt: Yet Another Deep Learnnig Tool.""""""\n'"
cmd_line/autoencoders/__init__.py,0,b''
cmd_line/autoencoders/run_autoencoder.py,1,"b'import numpy as np\nimport tensorflow as tf\n\nfrom yadlt.models.autoencoders import denoising_autoencoder\nfrom yadlt.utils import datasets, utilities\n\n# #################### #\n#   Flags definition   #\n# #################### #\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\n# Global configuration\nflags.DEFINE_string(\'dataset\', \'mnist\', \'Which dataset to use. [""mnist"", ""cifar10"", ""custom""]\')\nflags.DEFINE_string(\'train_dataset\', \'\', \'Path to train set .npy file.\')\nflags.DEFINE_string(\'valid_dataset\', \'\', \'Path to valid set .npy file.\')\nflags.DEFINE_string(\'test_dataset\', \'\', \'Path to test set .npy file.\')\nflags.DEFINE_string(\'name\', \'dae\', \'Model name.\')\nflags.DEFINE_string(\'cifar_dir\', \'\', \'Path to the cifar 10 dataset directory.\')\nflags.DEFINE_boolean(\'encode_train\', False, \'Whether to encode and store the training set.\')\nflags.DEFINE_boolean(\'encode_valid\', False, \'Whether to encode and store the validation set.\')\nflags.DEFINE_boolean(\'encode_test\', False, \'Whether to encode and store the test set.\')\nflags.DEFINE_string(\'save_reconstructions\', \'\', \'Path to a .npy file to save the reconstructions of the model.\')\nflags.DEFINE_string(\'save_parameters\', \'\', \'Path to save the parameters of the model.\')\nflags.DEFINE_string(\'weights\', None, \'Path to a numpy array containing the weights of the autoencoder.\')\nflags.DEFINE_string(\'h_bias\', None, \'Path to a numpy array containing the encoder bias vector.\')\nflags.DEFINE_string(\'v_bias\', None, \'Path to a numpy array containing the decoder bias vector.\')\nflags.DEFINE_integer(\'seed\', -1, \'Seed for the random generators (>= 0). Useful for testing hyperparameters.\')\n\n# Stacked Denoising Autoencoder specific parameters\nflags.DEFINE_integer(\'n_components\', 256, \'Number of hidden units in the dae.\')\nflags.DEFINE_float(\'regcoef\', 5e-4, \'Regularization parameter. If 0, no regularization.\')\nflags.DEFINE_string(\'corr_type\', \'none\', \'Type of input corruption. [""none"", ""masking"", ""salt_and_pepper""]\')\nflags.DEFINE_float(\'corr_frac\', 0., \'Fraction of the input to corrupt.\')\nflags.DEFINE_string(\'enc_act_func\', \'tanh\', \'Activation function for the encoder. [""sigmoid"", ""tanh""]\')\nflags.DEFINE_string(\'dec_act_func\', \'none\', \'Activation function for the decoder. [""sigmoid"", ""tanh"", ""none""]\')\nflags.DEFINE_string(\'loss_func\', \'mse\', \'Loss function. [""mse"" or ""cross_entropy""]\')\nflags.DEFINE_string(\'opt\', \'sgd\', \'[""sgd"", ""adagrad"", ""momentum"", ""adam""]\')\nflags.DEFINE_float(\'learning_rate\', 0.01, \'Initial learning rate.\')\nflags.DEFINE_float(\'momentum\', 0.5, \'Momentum parameter.\')\nflags.DEFINE_integer(\'num_epochs\', 10, \'Number of epochs.\')\nflags.DEFINE_integer(\'batch_size\', 10, \'Size of each mini-batch.\')\n\nassert FLAGS.dataset in [\'mnist\', \'cifar10\', \'custom\']\nassert FLAGS.train_dataset != \'\' if FLAGS.dataset == \'custom\' else True\nassert FLAGS.enc_act_func in [\'sigmoid\', \'tanh\']\nassert FLAGS.dec_act_func in [\'sigmoid\', \'tanh\', \'none\']\nassert FLAGS.corr_type in [\'masking\', \'salt_and_pepper\', \'none\']\nassert 0. <= FLAGS.corr_frac <= 1.\n\nif __name__ == \'__main__\':\n\n    utilities.random_seed_np_tf(FLAGS.seed)\n\n    if FLAGS.dataset == \'mnist\':\n\n        # ################# #\n        #   MNIST Dataset   #\n        # ################# #\n\n        trX, vlX, teX = datasets.load_mnist_dataset(mode=\'unsupervised\')\n\n    elif FLAGS.dataset == \'cifar10\':\n\n        # ################### #\n        #   Cifar10 Dataset   #\n        # ################### #\n\n        trX, teX = datasets.load_cifar10_dataset(FLAGS.cifar_dir, mode=\'unsupervised\')\n        vlX = teX[:5000]  # Validation set is the first half of the test set\n\n    elif FLAGS.dataset == \'custom\':\n\n        # ################## #\n        #   Custom Dataset   #\n        # ################## #\n\n        def load_from_np(dataset_path):\n            if dataset_path != \'\':\n                return np.load(dataset_path)\n            else:\n                return None\n\n        trX = load_from_np(FLAGS.train_dataset)\n        vlX = load_from_np(FLAGS.valid_dataset)\n        teX = load_from_np(FLAGS.test_dataset)\n\n    else:\n        trX = None\n        vlX = None\n        teX = None\n\n    # Create the object\n    enc_act_func = utilities.str2actfunc(FLAGS.enc_act_func)\n    dec_act_func = utilities.str2actfunc(FLAGS.dec_act_func)\n\n    dae = denoising_autoencoder.DenoisingAutoencoder(\n        name=FLAGS.name, n_components=FLAGS.n_components,\n        enc_act_func=enc_act_func, dec_act_func=dec_act_func,\n        corr_type=FLAGS.corr_type, corr_frac=FLAGS.corr_frac,\n        loss_func=FLAGS.loss_func, opt=FLAGS.opt, regcoef=FLAGS.regcoef,\n        learning_rate=FLAGS.learning_rate, momentum=FLAGS.momentum,\n        num_epochs=FLAGS.num_epochs,\n        batch_size=FLAGS.batch_size)\n\n    # Fit the model\n    W = None\n    if FLAGS.weights:\n        W = np.load(FLAGS.weights)\n\n    bh = None\n    if FLAGS.h_bias:\n        bh = np.load(FLAGS.h_bias)\n\n    bv = None\n    if FLAGS.v_bias:\n        bv = np.load(FLAGS.v_bias)\n\n    dae.fit(trX, trX, vlX, vlX)\n\n    # Save the model paramenters\n    if FLAGS.save_parameters:\n        print(\'Saving the parameters of the model...\')\n        params = dae.get_parameters()\n        for p in params:\n            np.save(FLAGS.save_parameters + \'-\' + p, params[p])\n\n    # Save the reconstructions of the model\n    if FLAGS.save_reconstructions:\n        print(\'Saving the reconstructions for the test set...\')\n        np.save(FLAGS.save_reconstructions, dae.reconstruct(teX))\n'"
cmd_line/autoencoders/run_stacked_autoencoder_supervised.py,1,"b'import numpy as np\nimport tensorflow as tf\n\nfrom yadlt.models.autoencoders import stacked_denoising_autoencoder\nfrom yadlt.utils import datasets, utilities\n\n# #################### #\n#   Flags definition   #\n# #################### #\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\n# Global configuration\nflags.DEFINE_string(\'dataset\', \'mnist\', \'Which dataset to use. [""mnist"", ""cifar10"", ""custom""]\')\nflags.DEFINE_string(\'train_dataset\', \'\', \'Path to train set .npy file.\')\nflags.DEFINE_string(\'train_labels\', \'\', \'Path to train labels .npy file.\')\nflags.DEFINE_string(\'valid_dataset\', \'\', \'Path to valid set .npy file.\')\nflags.DEFINE_string(\'valid_labels\', \'\', \'Path to valid labels .npy file.\')\nflags.DEFINE_string(\'test_dataset\', \'\', \'Path to test set .npy file.\')\nflags.DEFINE_string(\'test_labels\', \'\', \'Path to test labels .npy file.\')\nflags.DEFINE_string(\'cifar_dir\', \'\', \'Path to the cifar 10 dataset directory.\')\nflags.DEFINE_boolean(\'do_pretrain\', True, \'Whether or not doing unsupervised pretraining.\')\nflags.DEFINE_string(\'save_predictions\', \'\', \'Path to a .npy file to save predictions of the model.\')\nflags.DEFINE_string(\'save_layers_output_test\', \'\', \'Path to a .npy file to save test set output from all the layers of the model.\')\nflags.DEFINE_string(\'save_layers_output_train\', \'\', \'Path to a .npy file to save train set output from all the layers of the model.\')\nflags.DEFINE_integer(\'seed\', -1, \'Seed for the random generators (>= 0). Useful for testing hyperparameters.\')\nflags.DEFINE_string(\'name\', \'sdae\', \'Name for the model.\')\nflags.DEFINE_float(\'momentum\', 0.5, \'Momentum parameter.\')\n\n# Supervised fine tuning parameters\nflags.DEFINE_string(\'finetune_loss_func\', \'softmax_cross_entropy\', \'Last Layer Loss function. [""softmax_cross_entropy"", ""mse""]\')\nflags.DEFINE_integer(\'finetune_num_epochs\', 30, \'Number of epochs for the fine-tuning phase.\')\nflags.DEFINE_float(\'finetune_learning_rate\', 0.001, \'Learning rate for the fine-tuning phase.\')\nflags.DEFINE_string(\'finetune_act_func\', \'relu\', \'Activation function for the fine-tuning phase. [""sigmoid, ""tanh"", ""relu""]\')\nflags.DEFINE_float(\'finetune_dropout\', 1, \'Dropout parameter.\')\nflags.DEFINE_string(\'finetune_opt\', \'sgd\', \'[""sgd"", ""adagrad"", ""momentum"", ""adam""]\')\nflags.DEFINE_integer(\'finetune_batch_size\', 20, \'Size of each mini-batch for the fine-tuning phase.\')\n# Autoencoder layers specific parameters\nflags.DEFINE_string(\'dae_layers\', \'256,\', \'Comma-separated values for the layers in the sdae.\')\nflags.DEFINE_string(\'dae_regcoef\', \'5e-4,\', \'Regularization parameter for the autoencoders. If 0, no regularization.\')\nflags.DEFINE_string(\'dae_enc_act_func\', \'sigmoid,\', \'Activation function for the encoder. [""sigmoid"", ""tanh""]\')\nflags.DEFINE_string(\'dae_dec_act_func\', \'none,\', \'Activation function for the decoder. [""sigmoid"", ""tanh"", ""none""]\')\nflags.DEFINE_string(\'dae_loss_func\', \'mse,\', \'Loss function. [""mse"" or ""cross_entropy""]\')\nflags.DEFINE_string(\'dae_opt\', \'sgd,\', \'[""sgd"", ""ada_grad"", ""momentum"", ""adam""]\')\nflags.DEFINE_string(\'dae_learning_rate\', \'0.01,\', \'Initial learning rate.\')\nflags.DEFINE_string(\'dae_num_epochs\', \'10,\', \'Number of epochs.\')\nflags.DEFINE_string(\'dae_batch_size\', \'10,\', \'Size of each mini-batch.\')\nflags.DEFINE_string(\'dae_corr_type\', \'none,\', \'Type of input corruption. [""none"", ""masking"", ""salt_and_pepper""]\')\nflags.DEFINE_string(\'dae_corr_frac\', \'0.0,\', \'Fraction of the input to corrupt.\')\n\n# Conversion of Autoencoder layers parameters from string to their specific type\ndae_layers = utilities.flag_to_list(FLAGS.dae_layers, \'int\')\ndae_enc_act_func = utilities.flag_to_list(FLAGS.dae_enc_act_func, \'str\')\ndae_dec_act_func = utilities.flag_to_list(FLAGS.dae_dec_act_func, \'str\')\ndae_opt = utilities.flag_to_list(FLAGS.dae_opt, \'str\')\ndae_loss_func = utilities.flag_to_list(FLAGS.dae_loss_func, \'str\')\ndae_learning_rate = utilities.flag_to_list(FLAGS.dae_learning_rate, \'float\')\ndae_regcoef = utilities.flag_to_list(FLAGS.dae_regcoef, \'float\')\ndae_corr_type = utilities.flag_to_list(FLAGS.dae_corr_type, \'str\')\ndae_corr_frac = utilities.flag_to_list(FLAGS.dae_corr_frac, \'float\')\ndae_num_epochs = utilities.flag_to_list(FLAGS.dae_num_epochs, \'int\')\ndae_batch_size = utilities.flag_to_list(FLAGS.dae_batch_size, \'int\')\n\n# Parameters validation\nassert all([0. <= cf <= 1. for cf in dae_corr_frac])\nassert all([ct in [\'masking\', \'salt_and_pepper\', \'none\'] for ct in dae_corr_type])\nassert FLAGS.dataset in [\'mnist\', \'cifar10\', \'custom\']\nassert len(dae_layers) > 0\nassert all([af in [\'sigmoid\', \'tanh\'] for af in dae_enc_act_func])\nassert all([af in [\'sigmoid\', \'tanh\', \'none\'] for af in dae_dec_act_func])\n\nif __name__ == \'__main__\':\n\n    utilities.random_seed_np_tf(FLAGS.seed)\n\n    if FLAGS.dataset == \'mnist\':\n\n        # ################# #\n        #   MNIST Dataset   #\n        # ################# #\n\n        trX, trY, vlX, vlY, teX, teY = datasets.load_mnist_dataset(mode=\'supervised\')\n\n    elif FLAGS.dataset == \'cifar10\':\n\n        # ################### #\n        #   Cifar10 Dataset   #\n        # ################### #\n\n        trX, trY, teX, teY = datasets.load_cifar10_dataset(FLAGS.cifar_dir, mode=\'supervised\')\n        # Validation set is the first half of the test set\n        vlX = teX[:5000]\n        vlY = teY[:5000]\n\n    elif FLAGS.dataset == \'custom\':\n\n        # ################## #\n        #   Custom Dataset   #\n        # ################## #\n\n        def load_from_np(dataset_path):\n            if dataset_path != \'\':\n                return np.load(dataset_path)\n            else:\n                return None\n\n        trX, trY = load_from_np(FLAGS.train_dataset), load_from_np(FLAGS.train_labels)\n        vlX, vlY = load_from_np(FLAGS.valid_dataset), load_from_np(FLAGS.valid_labels)\n        teX, teY = load_from_np(FLAGS.test_dataset), load_from_np(FLAGS.test_labels)\n\n    else:\n        trX = None\n        trY = None\n        vlX = None\n        vlY = None\n        teX = None\n        teY = None\n\n    # Create the object\n    sdae = None\n\n    dae_enc_act_func = [utilities.str2actfunc(af) for af in dae_enc_act_func]\n    dae_dec_act_func = [utilities.str2actfunc(af) for af in dae_dec_act_func]\n    finetune_act_func = utilities.str2actfunc(FLAGS.finetune_act_func)\n\n    sdae = stacked_denoising_autoencoder.StackedDenoisingAutoencoder(\n        do_pretrain=FLAGS.do_pretrain, name=FLAGS.name,\n        layers=dae_layers, finetune_loss_func=FLAGS.finetune_loss_func,\n        finetune_learning_rate=FLAGS.finetune_learning_rate, finetune_num_epochs=FLAGS.finetune_num_epochs,\n        finetune_opt=FLAGS.finetune_opt, finetune_batch_size=FLAGS.finetune_batch_size,\n        finetune_dropout=FLAGS.finetune_dropout,\n        enc_act_func=dae_enc_act_func, dec_act_func=dae_dec_act_func,\n        corr_type=dae_corr_type, corr_frac=dae_corr_frac, regcoef=dae_regcoef,\n        loss_func=dae_loss_func, opt=dae_opt,\n        learning_rate=dae_learning_rate, momentum=FLAGS.momentum,\n        num_epochs=dae_num_epochs, batch_size=dae_batch_size,\n        finetune_act_func=finetune_act_func)\n\n    # Fit the model (unsupervised pretraining)\n    if FLAGS.do_pretrain:\n        encoded_X, encoded_vX = sdae.pretrain(trX, vlX)\n\n    # Supervised finetuning\n    sdae.fit(trX, trY, vlX, vlY)\n\n    # Compute the accuracy of the model\n    print(\'Test set accuracy: {}\'.format(sdae.score(teX, teY)))\n\n    # Save the predictions of the model\n    if FLAGS.save_predictions:\n        print(\'Saving the predictions for the test set...\')\n        np.save(FLAGS.save_predictions, sdae.predict(teX))\n\n    def save_layers_output(which_set):\n        if which_set == \'train\':\n            trout = sdae.get_layers_output(trX)\n            for i, o in enumerate(trout):\n                np.save(FLAGS.save_layers_output_train + \'-layer-\' + str(i + 1) + \'-train\', o)\n\n        elif which_set == \'test\':\n            teout = sdae.get_layers_output(teX)\n            for i, o in enumerate(teout):\n                np.save(FLAGS.save_layers_output_test + \'-layer-\' + str(i + 1) + \'-test\', o)\n\n    # Save output from each layer of the model\n    if FLAGS.save_layers_output_test:\n        print(\'Saving the output of each layer for the test set\')\n        save_layers_output(\'test\')\n\n    # Save output from each layer of the model\n    if FLAGS.save_layers_output_train:\n        print(\'Saving the output of each layer for the train set\')\n        save_layers_output(\'train\')\n'"
cmd_line/autoencoders/run_stacked_autoencoder_unsupervised.py,1,"b'import numpy as np\nimport os\nimport tensorflow as tf\n\nfrom yadlt.models.autoencoders import deep_autoencoder\nfrom yadlt.utils import datasets, utilities\n\n# #################### #\n#   Flags definition   #\n# #################### #\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\n# Global configuration\nflags.DEFINE_string(\'dataset\', \'mnist\', \'Which dataset to use. [""mnist"", ""cifar10"", ""custom""]\')\nflags.DEFINE_string(\'train_dataset\', \'\', \'Path to train set data .npy file.\')\nflags.DEFINE_string(\'train_ref\', \'\', \'Path to train reference .npy file.\')\nflags.DEFINE_string(\'valid_dataset\', \'\', \'Path to valid set .npy file.\')\nflags.DEFINE_string(\'valid_ref\', \'\', \'Path to valid reference data .npy file.\')\nflags.DEFINE_string(\'test_dataset\', \'\', \'Path to test set .npy file.\')\nflags.DEFINE_string(\'test_ref\', \'\', \'Path to test reference data .npy file.\')\nflags.DEFINE_string(\'cifar_dir\', \'\', \'Path to the cifar 10 dataset directory.\')\nflags.DEFINE_integer(\'seed\', -1, \'Seed for the random generators (>= 0). Useful for testing hyperparameters.\')\nflags.DEFINE_boolean(\'do_pretrain\', True, \'Whether or not doing unsupervised pretraining.\')\nflags.DEFINE_string(\'save_reconstructions\', \'\', \'Path to a .npy file to save the reconstructions of the model.\')\nflags.DEFINE_string(\'save_layers_output_test\', \'\', \'Path to a .npy file to save test set output from all the layers of the model.\')\nflags.DEFINE_string(\'save_layers_output_train\', \'\', \'Path to a .npy file to save train set output from all the layers of the model.\')\nflags.DEFINE_string(\'save_model_parameters\', \'\', \'Path to a directory to save the parameters of the model. One .npy file per layer.\')\nflags.DEFINE_string(\'name\', \'un_sdae\', \'Name for the model.\')\nflags.DEFINE_float(\'momentum\', 0.5, \'Momentum parameter.\')\nflags.DEFINE_boolean(\'tied_weights\', True, \'Whether to use tied weights for the decoders.\')\n\n# Supervised fine tuning parameters\nflags.DEFINE_string(\'finetune_loss_func\', \'cross_entropy\', \'Last Layer Loss function.[""cross_entropy"", ""mse""]\')\nflags.DEFINE_integer(\'finetune_num_epochs\', 30, \'Number of epochs for the fine-tuning phase.\')\nflags.DEFINE_float(\'finetune_learning_rate\', 0.001, \'Learning rate for the fine-tuning phase.\')\nflags.DEFINE_string(\'finetune_enc_act_func\', \'relu,\', \'Activation function for the encoder fine-tuning phase. [""sigmoid, ""tanh"", ""relu""]\')\nflags.DEFINE_string(\'finetune_dec_act_func\', \'sigmoid,\', \'Activation function for the decoder fine-tuning phase. [""sigmoid, ""tanh"", ""relu""]\')\nflags.DEFINE_float(\'finetune_dropout\', 1, \'Dropout parameter.\')\nflags.DEFINE_string(\'finetune_opt\', \'sgd\', \'[""sgd"", ""ada_grad"", ""momentum""]\')\nflags.DEFINE_integer(\'finetune_batch_size\', 20, \'Size of each mini-batch for the fine-tuning phase.\')\n\n# Autoencoder layers specific parameters\nflags.DEFINE_string(\'dae_layers\', \'256,\', \'Comma-separated values for the layers in the sdae.\')\nflags.DEFINE_string(\'dae_regcoef\', \'5e-4,\', \'Regularization parameter for the autoencoders. If 0, no regularization.\')\nflags.DEFINE_string(\'dae_enc_act_func\', \'sigmoid,\', \'Activation function for the encoder. [""sigmoid"", ""tanh""]\')\nflags.DEFINE_string(\'dae_dec_act_func\', \'none,\', \'Activation function for the decoder. [""sigmoid"", ""tanh"", ""none""]\')\nflags.DEFINE_string(\'dae_loss_func\', \'mse,\', \'Loss function. [""mse"" or ""cross_entropy""]\')\nflags.DEFINE_string(\'dae_opt\', \'sgd,\', \'[""sgd"", ""ada_grad"", ""momentum"", ""adam""]\')\nflags.DEFINE_string(\'dae_learning_rate\', \'0.01,\', \'Initial learning rate.\')\nflags.DEFINE_string(\'dae_num_epochs\', \'10,\', \'Number of epochs.\')\nflags.DEFINE_string(\'dae_batch_size\', \'10,\', \'Size of each mini-batch.\')\nflags.DEFINE_string(\'dae_corr_type\', \'none,\', \'Type of input corruption. [""none"", ""masking"", ""salt_and_pepper""]\')\nflags.DEFINE_string(\'dae_corr_frac\', \'0.0,\', \'Fraction of the input to corrupt.\')\n\n# Conversion of Autoencoder layers parameters from string to their specific type\ndae_layers = utilities.flag_to_list(FLAGS.dae_layers, \'int\')\ndae_enc_act_func = utilities.flag_to_list(FLAGS.dae_enc_act_func, \'str\')\ndae_dec_act_func = utilities.flag_to_list(FLAGS.dae_dec_act_func, \'str\')\ndae_opt = utilities.flag_to_list(FLAGS.dae_opt, \'str\')\ndae_loss_func = utilities.flag_to_list(FLAGS.dae_loss_func, \'str\')\ndae_learning_rate = utilities.flag_to_list(FLAGS.dae_learning_rate, \'float\')\ndae_regcoef = utilities.flag_to_list(FLAGS.dae_regcoef, \'float\')\ndae_corr_type = utilities.flag_to_list(FLAGS.dae_corr_type, \'str\')\ndae_corr_frac = utilities.flag_to_list(FLAGS.dae_corr_frac, \'float\')\ndae_num_epochs = utilities.flag_to_list(FLAGS.dae_num_epochs, \'int\')\ndae_batch_size = utilities.flag_to_list(FLAGS.dae_batch_size, \'int\')\n\nfinetune_enc_act_func = utilities.flag_to_list(FLAGS.finetune_enc_act_func, \'str\')\nfinetune_dec_act_func = utilities.flag_to_list(FLAGS.finetune_dec_act_func, \'str\')\n\n# Parameters validation\nassert all([0. <= cf <= 1. for cf in dae_corr_frac])\nassert all([ct in [\'masking\', \'salt_and_pepper\', \'none\'] for ct in dae_corr_type])\nassert FLAGS.dataset in [\'mnist\', \'cifar10\', \'custom\']\nassert len(dae_layers) > 0\nassert all([af in [\'sigmoid\', \'tanh\'] for af in dae_enc_act_func])\nassert all([af in [\'sigmoid\', \'tanh\', \'none\'] for af in dae_dec_act_func])\n\nif __name__ == \'__main__\':\n\n    utilities.random_seed_np_tf(FLAGS.seed)\n\n    if FLAGS.dataset == \'mnist\':\n\n        # ################# #\n        #   MNIST Dataset   #\n        # ################# #\n\n        trX, vlX, teX = datasets.load_mnist_dataset(mode=\'unsupervised\')\n        trRef = trX\n        vlRef = vlX\n        teRef = teX\n\n    elif FLAGS.dataset == \'cifar10\':\n\n        # ################### #\n        #   Cifar10 Dataset   #\n        # ################### #\n\n        trX, teX = datasets.load_cifar10_dataset(FLAGS.cifar_dir, mode=\'unsupervised\')\n        # Validation set is the first half of the test set\n        vlX = teX[:5000]\n        trRef = trX\n        vlRef = vlX\n        teRef = teX\n\n    elif FLAGS.dataset == \'custom\':\n\n        # ################## #\n        #   Custom Dataset   #\n        # ################## #\n\n        def load_from_np(dataset_path):\n            if dataset_path != \'\':\n                return np.load(dataset_path)\n            else:\n                return None\n\n        trX, trRef = load_from_np(FLAGS.train_dataset), load_from_np(FLAGS.train_ref)\n        vlX, vlRef = load_from_np(FLAGS.valid_dataset), load_from_np(FLAGS.valid_ref)\n        teX, teRef = load_from_np(FLAGS.test_dataset), load_from_np(FLAGS.test_ref)\n\n        if not trRef:\n            trRef = trX\n        if not vlRef:\n            vlRef = vlX\n        if not teRef:\n            teRef = teX\n\n    else:\n        trX = None\n        trRef = None\n        vlX = None\n        vlRef = None\n        teX = None\n        teRef = None\n\n    # Create the object\n    sdae = None\n\n    dae_enc_act_func = [utilities.str2actfunc(af) for af in dae_enc_act_func]\n    dae_dec_act_func = [utilities.str2actfunc(af) for af in dae_dec_act_func]\n    finetune_enc_act_func = [utilities.str2actfunc(af) for af in finetune_enc_act_func]\n    finetune_dec_act_func = [utilities.str2actfunc(af) for af in finetune_dec_act_func]\n\n    sdae = deep_autoencoder.DeepAutoencoder(\n        do_pretrain=FLAGS.do_pretrain, name=FLAGS.name,\n        layers=dae_layers, finetune_loss_func=FLAGS.finetune_loss_func,\n        finetune_learning_rate=FLAGS.finetune_learning_rate, finetune_num_epochs=FLAGS.finetune_num_epochs,\n        finetune_opt=FLAGS.finetune_opt, finetune_batch_size=FLAGS.finetune_batch_size,\n        finetune_dropout=FLAGS.finetune_dropout,\n        enc_act_func=dae_enc_act_func, dec_act_func=dae_dec_act_func,\n        corr_type=dae_corr_type, corr_frac=dae_corr_frac, regcoef=dae_regcoef,\n        loss_func=dae_loss_func,\n        opt=dae_opt, tied_weights=FLAGS.tied_weights,\n        learning_rate=dae_learning_rate, momentum=FLAGS.momentum,\n        num_epochs=dae_num_epochs, batch_size=dae_batch_size,\n        finetune_enc_act_func=finetune_enc_act_func, finetune_dec_act_func=finetune_dec_act_func)\n\n    def load_params_npz(npzfilepath):\n        params = []\n        npzfile = np.load(npzfilepath)\n        for f in npzfile.files:\n            params.append(npzfile[f])\n        return params\n\n    encodingw = None\n    encodingb = None\n\n    # Fit the model (unsupervised pretraining)\n    if FLAGS.do_pretrain:\n        encoded_X, encoded_vX = sdae.pretrain(trX, vlX)\n\n    # Supervised finetuning\n    sdae.fit(trX, trRef, vlX, vlRef)\n\n    # Compute the reconstruction loss of the model\n    print(\'Test set reconstruction loss: {}\'.format(sdae.score(teX, teRef)))\n\n    # Save the predictions of the model\n    if FLAGS.save_reconstructions:\n        print(\'Saving the reconstructions for the test set...\')\n        np.save(FLAGS.save_reconstructions, sdae.reconstruct(teX))\n\n    def save_layers_output(which_set):\n        if which_set == \'train\':\n            trout = sdae.get_layers_output(trX)\n            for i, o in enumerate(trout):\n                np.save(FLAGS.save_layers_output_train + \'-layer-\' + str(i + 1) + \'-train\', o)\n\n        elif which_set == \'test\':\n            teout = sdae.get_layers_output(teX)\n            for i, o in enumerate(teout):\n                np.save(FLAGS.save_layers_output_test + \'-layer-\' + str(i + 1) + \'-test\', o)\n\n\n    # Save output from each layer of the model\n    if FLAGS.save_layers_output_test:\n        print(\'Saving the output of each layer for the test set\')\n        save_layers_output(\'test\')\n\n    # Save output from each layer of the model\n    if FLAGS.save_layers_output_train:\n        print(\'Saving the output of each layer for the train set\')\n        save_layers_output(\'train\')\n\n    # Save parameters of the model\n    if FLAGS.save_model_parameters:\n        print(\'Saving the parameters of the model\')\n        param_dir = FLAGS.save_model_parameters\n        model_params = sdae.get_parameters(\n            {\n                \'enc-weights-layer\': sdae.encoding_w_,\n                \'enc-biases-layer\': sdae.encoding_b_,\n                \'dec-weights-layer\': sdae.decoding_w,\n                \'dec-biases-layer\': sdae.decoding_b\n            })\n        for p in model_params:\n            np.save(os.path.join(param_dir, p), model_params[p])\n'"
cmd_line/boltzmann/__init__.py,0,b''
cmd_line/boltzmann/run_dbn.py,1,"b'import numpy as np\nimport tensorflow as tf\n\nfrom yadlt.models.boltzmann import dbn\nfrom yadlt.utils import datasets, utilities\n\n# #################### #\n#   Flags definition   #\n# #################### #\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\n# Global configuration\nflags.DEFINE_string(\'dataset\', \'mnist\', \'Which dataset to use. [""mnist"", ""cifar10"", ""custom""]\')\nflags.DEFINE_string(\'train_dataset\', \'\', \'Path to train set .npy file.\')\nflags.DEFINE_string(\'train_labels\', \'\', \'Path to train labels .npy file.\')\nflags.DEFINE_string(\'valid_dataset\', \'\', \'Path to valid set .npy file.\')\nflags.DEFINE_string(\'valid_labels\', \'\', \'Path to valid labels .npy file.\')\nflags.DEFINE_string(\'test_dataset\', \'\', \'Path to test set .npy file.\')\nflags.DEFINE_string(\'test_labels\', \'\', \'Path to test labels .npy file.\')\nflags.DEFINE_string(\'cifar_dir\', \'\', \'Path to the cifar 10 dataset directory.\')\nflags.DEFINE_string(\'name\', \'dbn\', \'Name of the model.\')\nflags.DEFINE_string(\'save_predictions\', \'\', \'Path to a .npy file to save predictions of the model.\')\nflags.DEFINE_string(\'save_layers_output_test\', \'\', \'Path to a .npy file to save test set output from all the layers of the model.\')\nflags.DEFINE_string(\'save_layers_output_train\', \'\', \'Path to a .npy file to save train set output from all the layers of the model.\')\nflags.DEFINE_boolean(\'do_pretrain\', True, \'Whether or not pretrain the network.\')\nflags.DEFINE_integer(\'seed\', -1, \'Seed for the random generators (>= 0). Useful for testing hyperparameters.\')\nflags.DEFINE_float(\'momentum\', 0.5, \'Momentum parameter.\')\n\n# RBMs layers specific parameters\nflags.DEFINE_string(\'rbm_layers\', \'256,\', \'Comma-separated values for the layers in the sdae.\')\nflags.DEFINE_boolean(\'rbm_gauss_visible\', False, \'Whether to use Gaussian units for the visible layer.\')\nflags.DEFINE_float(\'rbm_stddev\', 0.1, \'Standard deviation for Gaussian visible units.\')\nflags.DEFINE_string(\'rbm_learning_rate\', \'0.001,\', \'Initial learning rate.\')\nflags.DEFINE_string(\'rbm_num_epochs\', \'10,\', \'Number of epochs.\')\nflags.DEFINE_string(\'rbm_batch_size\', \'32,\', \'Size of each mini-batch.\')\nflags.DEFINE_string(\'rbm_gibbs_k\', \'1,\', \'Gibbs sampling steps.\')\n\n# Supervised fine tuning parameters\nflags.DEFINE_string(\'finetune_act_func\', \'relu\', \'Activation function.\')\nflags.DEFINE_float(\'finetune_learning_rate\', 0.01, \'Learning rate.\')\nflags.DEFINE_float(\'finetune_momentum\', 0.9, \'Momentum parameter.\')\nflags.DEFINE_integer(\'finetune_num_epochs\', 10, \'Number of epochs.\')\nflags.DEFINE_integer(\'finetune_batch_size\', 32, \'Size of each mini-batch.\')\nflags.DEFINE_string(\'finetune_opt\', \'momentum\', \'[""sgd"", ""ada_grad"", ""momentum"", ""adam""]\')\nflags.DEFINE_string(\'finetune_loss_func\', \'softmax_cross_entropy\', \'Loss function. [""mse"", ""softmax_cross_entropy""]\')\nflags.DEFINE_float(\'finetune_dropout\', 1, \'Dropout parameter.\')\n\n# Conversion of Autoencoder layers parameters from string to their specific type\nrbm_layers = utilities.flag_to_list(FLAGS.rbm_layers, \'int\')\nrbm_learning_rate = utilities.flag_to_list(FLAGS.rbm_learning_rate, \'float\')\nrbm_num_epochs = utilities.flag_to_list(FLAGS.rbm_num_epochs, \'int\')\nrbm_batch_size = utilities.flag_to_list(FLAGS.rbm_batch_size, \'int\')\nrbm_gibbs_k = utilities.flag_to_list(FLAGS.rbm_gibbs_k, \'int\')\n\n# Parameters validation\nassert FLAGS.dataset in [\'mnist\', \'cifar10\', \'custom\']\nassert FLAGS.finetune_act_func in [\'sigmoid\', \'tanh\', \'relu\']\nassert len(rbm_layers) > 0\n\nif __name__ == \'__main__\':\n\n    utilities.random_seed_np_tf(FLAGS.seed)\n\n    if FLAGS.dataset == \'mnist\':\n\n        # ################# #\n        #   MNIST Dataset   #\n        # ################# #\n\n        trX, trY, vlX, vlY, teX, teY = datasets.load_mnist_dataset(mode=\'supervised\')\n\n    elif FLAGS.dataset == \'cifar10\':\n\n        # ################### #\n        #   Cifar10 Dataset   #\n        # ################### #\n\n        trX, trY, teX, teY = datasets.load_cifar10_dataset(FLAGS.cifar_dir, mode=\'supervised\')\n        vlX = teX[:5000]  # Validation set is the first half of the test set\n        vlY = teY[:5000]\n\n    elif FLAGS.dataset == \'custom\':\n\n        # ################## #\n        #   Custom Dataset   #\n        # ################## #\n\n        def load_from_np(dataset_path):\n            if dataset_path != \'\':\n                return np.load(dataset_path)\n            else:\n                return None\n\n        trX, trY = load_from_np(FLAGS.train_dataset), load_from_np(FLAGS.train_labels)\n        vlX, vlY = load_from_np(FLAGS.valid_dataset), load_from_np(FLAGS.valid_labels)\n        teX, teY = load_from_np(FLAGS.test_dataset), load_from_np(FLAGS.test_labels)\n\n    else:\n        trX, trY, vlX, vlY, teX, teY = None, None, None, None, None, None\n\n    # Create the object\n    finetune_act_func = utilities.str2actfunc(FLAGS.finetune_act_func)\n\n    srbm = dbn.DeepBeliefNetwork(\n        name=FLAGS.name, do_pretrain=FLAGS.do_pretrain,\n        rbm_layers=rbm_layers,\n        finetune_act_func=finetune_act_func, rbm_learning_rate=rbm_learning_rate,\n        rbm_num_epochs=rbm_num_epochs, rbm_gibbs_k = rbm_gibbs_k,\n        rbm_gauss_visible=FLAGS.rbm_gauss_visible, rbm_stddev=FLAGS.rbm_stddev,\n        momentum=FLAGS.momentum, rbm_batch_size=rbm_batch_size, finetune_learning_rate=FLAGS.finetune_learning_rate,\n        finetune_num_epochs=FLAGS.finetune_num_epochs, finetune_batch_size=FLAGS.finetune_batch_size,\n        finetune_opt=FLAGS.finetune_opt, finetune_loss_func=FLAGS.finetune_loss_func,\n        finetune_dropout=FLAGS.finetune_dropout)\n\n    # Fit the model (unsupervised pretraining)\n    if FLAGS.do_pretrain:\n        srbm.pretrain(trX, vlX)\n\n    # finetuning\n    print(\'Start deep belief net finetuning...\')\n    srbm.fit(trX, trY, vlX, vlY)\n\n    # Test the model\n    print(\'Test set accuracy: {}\'.format(srbm.score(teX, teY)))\n\n    # Save the predictions of the model\n    if FLAGS.save_predictions:\n        print(\'Saving the predictions for the test set...\')\n        np.save(FLAGS.save_predictions, srbm.predict(teX))\n\n\n    def save_layers_output(which_set):\n        if which_set == \'train\':\n            trout = srbm.get_layers_output(trX)\n            for i, o in enumerate(trout):\n                np.save(FLAGS.save_layers_output_train + \'-layer-\' + str(i + 1) + \'-train\', o)\n\n        elif which_set == \'test\':\n            teout = srbm.get_layers_output(teX)\n            for i, o in enumerate(teout):\n                np.save(FLAGS.save_layers_output_test + \'-layer-\' + str(i + 1) + \'-test\', o)\n\n\n    # Save output from each layer of the model\n    if FLAGS.save_layers_output_test:\n        print(\'Saving the output of each layer for the test set\')\n        save_layers_output(\'test\')\n\n    # Save output from each layer of the model\n    if FLAGS.save_layers_output_train:\n        print(\'Saving the output of each layer for the train set\')\n        save_layers_output(\'train\')\n'"
cmd_line/boltzmann/run_deep_autoencoder.py,1,"b'import numpy as np\nimport tensorflow as tf\n\nfrom yadlt.models.boltzmann import deep_autoencoder\nfrom yadlt.utils import datasets, utilities\n\n# #################### #\n#   Flags definition   #\n# #################### #\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\n# Global configuration\nflags.DEFINE_string(\'dataset\', \'mnist\', \'Which dataset to use. [""mnist"", ""cifar10"", ""custom""]\')\nflags.DEFINE_string(\'train_dataset\', \'\', \'Path to train set data .npy file.\')\nflags.DEFINE_string(\'train_ref\', \'\', \'Path to train reference .npy file.\')\nflags.DEFINE_string(\'valid_dataset\', \'\', \'Path to valid set .npy file.\')\nflags.DEFINE_string(\'valid_ref\', \'\', \'Path to valid reference data .npy file.\')\nflags.DEFINE_string(\'test_dataset\', \'\', \'Path to test set .npy file.\')\nflags.DEFINE_string(\'test_ref\', \'\', \'Path to test reference data .npy file.\')\nflags.DEFINE_string(\'cifar_dir\', \'\', \'Path to the cifar 10 dataset directory.\')\nflags.DEFINE_string(\'name\', \'srbm\', \'Name of the model.\')\nflags.DEFINE_boolean(\'do_pretrain\', True, \'Whether or not pretrain the network.\')\nflags.DEFINE_string(\'save_layers_output_test\', \'\', \'Path to a .npy file to save test set output from all the layers of the model.\')\nflags.DEFINE_string(\'save_layers_output_train\', \'\', \'Path to a .npy file to save train set output from all the layers of the model.\')\nflags.DEFINE_integer(\'seed\', -1, \'Seed for the random generators (>= 0). Useful for testing hyperparameters.\')\nflags.DEFINE_float(\'momentum\', 0.7, \'Momentum parameter.\')\nflags.DEFINE_string(\'save_reconstructions\', \'\', \'Path to a .npy file to save the reconstructions of the model.\')\n\n# RBMs layers specific parameters\nflags.DEFINE_string(\'rbm_names\', \'rbm\', \'Name for the rbm stored_models.\')\nflags.DEFINE_string(\'rbm_layers\', \'256,\', \'Comma-separated values for the layers in the srbm.\')\nflags.DEFINE_string(\'rbm_noise\', \'gauss\', \'Type of noise. Default: ""gauss"".\')\nflags.DEFINE_float(\'rbm_stddev\', 0.1, \'Standard deviation for Gaussian visible units.\')\nflags.DEFINE_string(\'rbm_learning_rate\', \'0.01,\', \'Initial learning rate.\')\nflags.DEFINE_string(\'rbm_num_epochs\', \'10,\', \'Number of epochs.\')\nflags.DEFINE_string(\'rbm_batch_size\', \'10,\', \'Size of each mini-batch.\')\nflags.DEFINE_string(\'rbm_gibbs_k\', \'1,\', \'Gibbs sampling steps.\')\n# Supervised fine tuning parameters\nflags.DEFINE_float(\'finetune_learning_rate\', 0.01, \'Learning rate.\')\nflags.DEFINE_string(\'finetune_enc_act_func\', \'relu,\', \'Activation function for the encoder fine-tuning phase. [""sigmoid, ""tanh"", ""relu""]\')\nflags.DEFINE_string(\'finetune_dec_act_func\', \'sigmoid,\', \'Activation function for the decoder fine-tuning phase. [""sigmoid, ""tanh"", ""relu""]\')\nflags.DEFINE_integer(\'finetune_num_epochs\', 10, \'Number of epochs.\')\nflags.DEFINE_integer(\'finetune_batch_size\', 10, \'Size of each mini-batch.\')\nflags.DEFINE_string(\'finetune_opt\', \'sgd\', \'[""sgd"", ""adagrad"", ""momentum"", ""adam""]\')\nflags.DEFINE_string(\'finetune_loss_func\', \'mse\', \'Loss function.\')\nflags.DEFINE_float(\'finetune_dropout\', 1, \'Dropout parameter.\')\n\n# Conversion of Autoencoder layers parameters from string to their specific type\nrbm_names = utilities.flag_to_list(FLAGS.rbm_names, \'str\')\nrbm_layers = utilities.flag_to_list(FLAGS.rbm_layers, \'int\')\nrbm_noise = utilities.flag_to_list(FLAGS.rbm_noise, \'str\')\nrbm_learning_rate = utilities.flag_to_list(FLAGS.rbm_learning_rate, \'float\')\nrbm_num_epochs = utilities.flag_to_list(FLAGS.rbm_num_epochs, \'int\')\nrbm_batch_size = utilities.flag_to_list(FLAGS.rbm_batch_size, \'int\')\nrbm_gibbs_k = utilities.flag_to_list(FLAGS.rbm_gibbs_k, \'int\')\n\nfinetune_enc_act_func = utilities.flag_to_list(FLAGS.finetune_enc_act_func, \'str\')\nfinetune_dec_act_func = utilities.flag_to_list(FLAGS.finetune_dec_act_func, \'str\')\n\n# Parameters validation\nassert FLAGS.dataset in [\'mnist\', \'cifar10\', \'custom\']\nassert len(rbm_layers) > 0\n\nif __name__ == \'__main__\':\n\n    utilities.random_seed_np_tf(FLAGS.seed)\n\n    if FLAGS.dataset == \'mnist\':\n\n        # ################# #\n        #   MNIST Dataset   #\n        # ################# #\n\n        trX, vlX, teX = datasets.load_mnist_dataset(mode=\'unsupervised\')\n        trRef = trX\n        vlRef = vlX\n        teRef = teX\n\n    elif FLAGS.dataset == \'cifar10\':\n\n        # ################### #\n        #   Cifar10 Dataset   #\n        # ################### #\n\n        trX, teX = datasets.load_cifar10_dataset(FLAGS.cifar_dir, mode=\'unsupervised\')\n        # Validation set is the first half of the test set\n        vlX = teX[:5000]\n        trRef = trX\n        vlRef = vlX\n        teRef = teX\n\n    elif FLAGS.dataset == \'custom\':\n\n        # ################## #\n        #   Custom Dataset   #\n        # ################## #\n\n        def load_from_np(dataset_path):\n            if dataset_path != \'\':\n                return np.load(dataset_path)\n            else:\n                return None\n\n\n        trX, trRef = load_from_np(FLAGS.train_dataset), load_from_np(FLAGS.train_ref)\n        vlX, vlRef = load_from_np(FLAGS.valid_dataset), load_from_np(FLAGS.valid_ref)\n        teX, teRef = load_from_np(FLAGS.test_dataset), load_from_np(FLAGS.test_ref)\n\n        if not trRef:\n            trRef = trX\n        if not vlRef:\n            vlRef = vlX\n        if not teRef:\n            teRef = teX\n\n    else:\n        trX = None\n        trRef = None\n        vlX = None\n        vlRef = None\n        teX = None\n        teRef = None\n\n    finetune_enc_act_func = [utilities.str2actfunc(af) for af in finetune_enc_act_func]\n    finetune_dec_act_func = [utilities.str2actfunc(af) for af in finetune_dec_act_func]\n\n    # Create the object\n    srbm = deep_autoencoder.DeepAutoencoder(\n        name=FLAGS.name, do_pretrain=FLAGS.do_pretrain,\n        layers=rbm_layers,\n        learning_rate=rbm_learning_rate, gibbs_k=rbm_gibbs_k,\n        num_epochs=rbm_num_epochs, momentum=FLAGS.momentum,\n        batch_size=rbm_batch_size, finetune_learning_rate=FLAGS.finetune_learning_rate,\n        finetune_enc_act_func=finetune_enc_act_func, finetune_dec_act_func=finetune_dec_act_func,\n        finetune_num_epochs=FLAGS.finetune_num_epochs, finetune_batch_size=FLAGS.finetune_batch_size,\n        finetune_opt=FLAGS.finetune_opt, finetune_loss_func=FLAGS.finetune_loss_func, finetune_dropout=FLAGS.finetune_dropout,\n        noise=rbm_noise, stddev=FLAGS.rbm_stddev)\n\n\n    def load_params_npz(npzfilepath):\n        params = []\n        npzfile = np.load(npzfilepath)\n        for f in npzfile.files:\n            params.append(npzfile[f])\n        return params\n\n    if FLAGS.do_pretrain:\n        encoded_X, encoded_vX = srbm.pretrain(trX, vlX)\n\n    # Supervised finetuning\n    srbm.fit(trX, trRef, vlX, vlRef)\n\n    # Compute the reconstruction loss of the model\n    print(\'Test set reconstruction loss: {}\'.format(srbm.score(teX, teRef)))\n\n    # Save the predictions of the model\n    if FLAGS.save_reconstructions:\n        print(\'Saving the reconstructions for the test set...\')\n        np.save(FLAGS.save_reconstructions, srbm.reconstruct(teX))\n\n\n    def save_layers_output(which_set):\n        if which_set == \'train\':\n            trout = srbm.get_layers_output(trX)\n            for i, o in enumerate(trout):\n                np.save(FLAGS.save_layers_output_train + \'-layer-\' + str(i + 1) + \'-train\', o)\n\n        elif which_set == \'test\':\n            teout = srbm.get_layers_output(teX)\n            for i, o in enumerate(teout):\n                np.save(FLAGS.save_layers_output_test + \'-layer-\' + str(i + 1) + \'-test\', o)\n\n\n    # Save output from each layer of the model\n    if FLAGS.save_layers_output_test:\n        print(\'Saving the output of each layer for the test set\')\n        save_layers_output(\'test\')\n\n    # Save output from each layer of the model\n    if FLAGS.save_layers_output_train:\n        print(\'Saving the output of each layer for the train set\')\n        save_layers_output(\'train\')\n'"
cmd_line/boltzmann/run_rbm.py,1,"b'import numpy as np\nimport tensorflow as tf\n\nfrom yadlt.models.boltzmann import rbm\nfrom yadlt.utils import datasets, utilities\n\n# #################### #\n#   Flags definition   #\n# #################### #\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\n# Global configuration\nflags.DEFINE_boolean(\'encode_train\', False, \'Whether to encode and store the training set.\')\nflags.DEFINE_boolean(\'encode_valid\', False, \'Whether to encode and store the validation set.\')\nflags.DEFINE_boolean(\'encode_test\', False, \'Whether to encode and store the test set.\')\nflags.DEFINE_string(\'dataset\', \'mnist\', \'Which dataset to use. [""mnist"", ""cifar10"", ""custom""]\')\nflags.DEFINE_string(\'train_dataset\', \'\', \'Path to train set .npy file.\')\nflags.DEFINE_string(\'valid_dataset\', \'\', \'Path to valid set .npy file.\')\nflags.DEFINE_string(\'test_dataset\', \'\', \'Path to test set .npy file.\')\nflags.DEFINE_string(\'cifar_dir\', \'\', \'Path to the cifar 10 dataset directory.\')\nflags.DEFINE_string(\'save_reconstructions\', \'\', \'Path to a .npy file to save the reconstructions of the model.\')\nflags.DEFINE_string(\'save_parameters\', \'\', \'Path to save the parameters of the model.\')\nflags.DEFINE_integer(\'seed\', -1, \'Seed for the random generators (>= 0). Useful for testing hyperparameters.\')\n\n# RBM configuration\nflags.DEFINE_integer(\'num_hidden\', 250, \'Number of hidden units.\')\nflags.DEFINE_string(\'visible_unit_type\', \'bin\', \'Type of visible units. [""bin"", ""gauss""]\')\nflags.DEFINE_string(\'name\', \'rbm_model\', \'Name for the model.\')\nflags.DEFINE_integer(\'gibbs_sampling_steps\', 1, \'Number of gibbs sampling steps in Contrastive Divergence.\')\nflags.DEFINE_float(\'learning_rate\', 0.001, \'Initial learning rate.\')\nflags.DEFINE_float(\'stddev\', 0.1, \'Standard deviation for the Gaussian visible units.\')\nflags.DEFINE_integer(\'num_epochs\', 10, \'Number of epochs.\')\nflags.DEFINE_integer(\'batch_size\', 32, \'Size of each mini-batch.\')\nflags.DEFINE_integer(\'transform_gibbs_sampling_steps\', 10, \'Gibbs sampling steps for the transformation of data.\')\n\nassert FLAGS.dataset in [\'mnist\', \'cifar10\', \'custom\']\nassert FLAGS.cifar_dir != \'\' if FLAGS.dataset == \'cifar10\' else True\nassert FLAGS.visible_unit_type in [\'bin\', \'gauss\']\n\nif __name__ == \'__main__\':\n\n    utilities.random_seed_np_tf(FLAGS.seed)\n\n    if FLAGS.dataset == \'mnist\':\n\n        # ################# #\n        #   MNIST Dataset   #\n        # ################# #\n\n        trX, vlX, teX = datasets.load_mnist_dataset(mode=\'unsupervised\')\n        width, height = 28, 28\n\n    elif FLAGS.dataset == \'cifar10\':\n\n        # ################### #\n        #   Cifar10 Dataset   #\n        # ################### #\n\n        trX, teX = datasets.load_cifar10_dataset(FLAGS.cifar_dir, mode=\'unsupervised\')\n        vlX = teX[:5000]  # Validation set is the first half of the test set\n        width, height = 32, 32\n\n    elif FLAGS.dataset == \'custom\':\n\n        # ################## #\n        #   Custom Dataset   #\n        # ################## #\n\n        def load_from_np(dataset_path):\n            if dataset_path != \'\':\n                return np.load(dataset_path)\n            else:\n                return None\n\n        trX = load_from_np(FLAGS.train_dataset)\n        vlX = load_from_np(FLAGS.valid_dataset)\n        teX = load_from_np(FLAGS.test_dataset)\n\n    else:\n        trX, vlX, teX, width, height = None, None, None, None, None\n\n    # Create the object\n    r = rbm.RBM(num_hidden=FLAGS.num_hidden,\n                visible_unit_type=FLAGS.visible_unit_type, learning_rate=FLAGS.learning_rate,\n                num_epochs=FLAGS.num_epochs, batch_size=FLAGS.batch_size, stddev=FLAGS.stddev,\n                gibbs_sampling_steps=FLAGS.gibbs_sampling_steps, name=FLAGS.name)\n\n    # Fit the model\n    print(\'Start training...\')\n    r.fit(trX, teX)\n\n    # Save the model paramenters\n    if FLAGS.save_parameters:\n        print(\'Saving the parameters of the model...\')\n        params = r.get_parameters()\n        for p in params:\n            np.save(FLAGS.save_parameters + \'-\' + p, params[p])\n\n    # Save the reconstructions of the model\n    if FLAGS.save_reconstructions:\n        print(\'Saving the reconstructions for the test set...\')\n        np.save(FLAGS.save_reconstructions, r.reconstruct(teX))\n\n    # Encode the training data and store it\n    if FLAGS.encode_train:\n        print(\'Transforming training data...\')\n        r.transform(trX, name=\'train\', save=FLAGS.encode_train)\n\n    if FLAGS.encode_valid:\n        print(\'Transforming validation data...\')\n        r.transform(vlX, name=\'validation\', save=FLAGS.encode_valid)\n\n    if FLAGS.encode_test:\n        print(\'Transforming test data...\')\n        r.transform(teX, name=\'test\', save=FLAGS.encode_test)\n'"
cmd_line/convolutional/__init__.py,0,b''
cmd_line/convolutional/run_conv_net.py,1,"b'import numpy as np\nimport tensorflow as tf\n\nfrom yadlt.models.convolutional import conv_net\nfrom yadlt.utils import datasets, utilities\n\n# #################### #\n#   Flags definition   #\n# #################### #\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\n# Global configuration\nflags.DEFINE_string(\'dataset\', \'mnist\', \'Which dataset to use. [""mnist"", ""cifar10"", ""custom""]\')\nflags.DEFINE_string(\'original_shape\', \'28,28,1\', \'Original shape of the images in the dataset.\')\nflags.DEFINE_string(\'train_dataset\', \'\', \'Path to train set .npy file.\')\nflags.DEFINE_string(\'train_labels\', \'\', \'Path to train labels .npy file.\')\nflags.DEFINE_string(\'valid_dataset\', \'\', \'Path to valid set .npy file.\')\nflags.DEFINE_string(\'valid_labels\', \'\', \'Path to valid labels .npy file.\')\nflags.DEFINE_string(\'test_dataset\', \'\', \'Path to test set .npy file.\')\nflags.DEFINE_string(\'test_labels\', \'\', \'Path to test labels .npy file.\')\nflags.DEFINE_string(\'cifar_dir\', \'\', \'Path to the cifar 10 dataset directory.\')\nflags.DEFINE_string(\'name\', \'convnet\', \'Model name.\')\nflags.DEFINE_integer(\'seed\', -1, \'Seed for the random generators (>= 0). Useful for testing hyperparameters.\')\n\n# Convolutional Net parameters\nflags.DEFINE_string(\'layers\', \'\', \'String representing the architecture of the network.\')\nflags.DEFINE_string(\'loss_func\',  \'softmax_cross_entropy\', \'Loss function. [""mse"" or ""softmax_cross_entropy""]\')\nflags.DEFINE_integer(\'num_epochs\', 10, \'Number of epochs.\')\nflags.DEFINE_integer(\'batch_size\', 10, \'Size of each mini-batch.\')\nflags.DEFINE_string(\'opt\', \'sgd\', \'[""sgd"", ""ada_grad"", ""momentum"", ""adam""]\')\nflags.DEFINE_float(\'learning_rate\', 0.01, \'Initial learning rate.\')\nflags.DEFINE_float(\'momentum\', 0.5, \'Momentum parameter.\')\nflags.DEFINE_float(\'dropout\', 1, \'Dropout parameter.\')\n\nassert FLAGS.dataset in [\'mnist\', \'cifar10\', \'custom\']\n\nif __name__ == \'__main__\':\n\n    utilities.random_seed_np_tf(FLAGS.seed)\n\n    if FLAGS.dataset == \'mnist\':\n\n        # ################# #\n        #   MNIST Dataset   #\n        # ################# #\n\n        trX, trY, vlX, vlY, teX, teY = datasets.load_mnist_dataset(mode=\'supervised\')\n\n    elif FLAGS.dataset == \'cifar10\':\n\n        # ################### #\n        #   Cifar10 Dataset   #\n        # ################### #\n\n        trX, trY, teX, teY = datasets.load_cifar10_dataset(FLAGS.cifar_dir, mode=\'supervised\')\n        vlX = teX[:5000]  # Validation set is the first half of the test set\n        vlY = teY[:5000]\n\n    elif FLAGS.dataset == \'custom\':\n\n        # ################## #\n        #   Custom Dataset   #\n        # ################## #\n\n        def load_from_np(dataset_path):\n            if dataset_path != \'\':\n                return np.load(dataset_path)\n            else:\n                return None\n\n\n        trX, trY = load_from_np(FLAGS.train_dataset), load_from_np(FLAGS.train_labels)\n        vlX, vlY = load_from_np(FLAGS.valid_dataset), load_from_np(FLAGS.valid_labels)\n        teX, teY = load_from_np(FLAGS.test_dataset), load_from_np(FLAGS.test_labels)\n\n    else:\n        trX, trY, vlX, vlY, teX, teY = None, None, None, None, None, None\n\n    # Create the model object\n    convnet = conv_net.ConvolutionalNetwork(\n        original_shape=[int(i) for i in FLAGS.original_shape.split(\',\')],\n        layers=FLAGS.layers, name=FLAGS.name, loss_func=FLAGS.loss_func,\n        num_epochs=FLAGS.num_epochs, batch_size=FLAGS.batch_size, opt=FLAGS.opt,\n        learning_rate=FLAGS.learning_rate, momentum=FLAGS.momentum, dropout=FLAGS.dropout\n    )\n\n    # Model training\n    print(\'Start Convolutional Network training...\')\n    convnet.fit(trX, trY, vlX, vlY)\n\n    # Test the model\n    print(\'Test set accuracy: {}\'.format(convnet.score(teX, teY)))\n'"
cmd_line/linear/__init__.py,0,b''
cmd_line/linear/run_logistic_regression.py,1,"b'import numpy as np\nimport tensorflow as tf\n\nfrom yadlt.models.linear import logistic_regression\nfrom yadlt.utils import datasets, utilities\n\n# #################### #\n#   Flags definition   #\n# #################### #\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\n# Global configuration\nflags.DEFINE_string(\'dataset\', \'mnist\', \'Which dataset to use. [""mnist"", ""cifar10"", ""custom""]\')\nflags.DEFINE_string(\'train_dataset\', \'\', \'Path to train set .npy file.\')\nflags.DEFINE_string(\'train_labels\', \'\', \'Path to train labels .npy file.\')\nflags.DEFINE_string(\'valid_dataset\', \'\', \'Path to valid set .npy file.\')\nflags.DEFINE_string(\'valid_labels\', \'\', \'Path to valid labels .npy file.\')\nflags.DEFINE_string(\'test_dataset\', \'\', \'Path to test set .npy file.\')\nflags.DEFINE_string(\'test_labels\', \'\', \'Path to test labels .npy file.\')\nflags.DEFINE_string(\'cifar_dir\', \'\', \'Path to the cifar 10 dataset directory.\')\nflags.DEFINE_string(\'name\', \'logreg\', \'Name for the model.\')\nflags.DEFINE_string(\'loss_func\', \'cross_entropy\', \'Loss function. [""mse"" or ""cross_entropy""]\')\nflags.DEFINE_float(\'learning_rate\', 0.01, \'Initial learning rate.\')\nflags.DEFINE_integer(\'num_epochs\', 10, \'Number of epochs.\')\nflags.DEFINE_integer(\'batch_size\', 10, \'Size of each mini-batch.\')\nflags.DEFINE_integer(\'seed\', -1, \'Seed for the random generators (>= 0). Useful for testing hyperparameters.\')\n\nassert FLAGS.dataset in [\'mnist\', \'cifar10\', \'custom\']\n\nif __name__ == \'__main__\':\n\n    utilities.random_seed_np_tf(FLAGS.seed)\n\n    if FLAGS.dataset == \'mnist\':\n\n        # ################# #\n        #   MNIST Dataset   #\n        # ################# #\n\n        trX, trY, vlX, vlY, teX, teY = datasets.load_mnist_dataset(mode=\'supervised\')\n\n    elif FLAGS.dataset == \'cifar10\':\n\n        # ################### #\n        #   Cifar10 Dataset   #\n        # ################### #\n\n        trX, trY, teX, teY = datasets.load_cifar10_dataset(FLAGS.cifar_dir, mode=\'supervised\')\n        # Validation set is the first half of the test set\n        vlX = teX[:5000]\n        vlY = teY[:5000]\n\n    elif FLAGS.dataset == \'custom\':\n\n        # ################## #\n        #   Custom Dataset   #\n        # ################## #\n\n        def load_from_np(dataset_path):\n            if dataset_path != \'\':\n                return np.load(dataset_path)\n            else:\n                return None\n\n        trX, trY = load_from_np(FLAGS.train_dataset), load_from_np(FLAGS.train_labels)\n        vlX, vlY = load_from_np(FLAGS.valid_dataset), load_from_np(FLAGS.valid_labels)\n        teX, teY = load_from_np(FLAGS.test_dataset), load_from_np(FLAGS.test_labels)\n\n    else:\n        trX = None\n        trY = None\n        vlX = None\n        vlY = None\n        teX = None\n        teY = None\n\n    # Create the object\n    l = logistic_regression.LogisticRegression(\n        name=FLAGS.name, loss_func=FLAGS.loss_func,\n        learning_rate=FLAGS.learning_rate, num_epochs=FLAGS.num_epochs,\n        batch_size=FLAGS.batch_size)\n\n    # Fit the model\n    l.fit(trX, trY, vlX, vlY)\n\n    # Test the model\n    print(\'Test set accuracy: {}\'.format(l.score(teX, teY)))\n'"
cmd_line/recurrent/__init__.py,0,b''
cmd_line/recurrent/run_lstm.py,1,"b'""""""Command line script to run LSTM model.""""""\n\nimport tensorflow as tf\n\nfrom yadlt.models.recurrent.lstm import LSTM\nfrom yadlt.utils import datasets\nfrom yadlt.utils import utilities\n\n# #################### #\n#   Flags definition   #\n# #################### #\nflags = tf.app.flags\nFLAGS = flags.FLAGS\n\n# Global configuration\nflags.DEFINE_string(\'dataset\', \'mnist\', \'Which dataset to use. [""ptb""]\')\nflags.DEFINE_string(\'ptb_dir\', \'\', \'Path to the ptb dataset directory.\')\nflags.DEFINE_string(\'name\', \'lstm\', \'Model name.\')\nflags.DEFINE_integer(\'seed\', -1, \'Seed for the random generators (>= 0).\\\n    Useful for testing hyperparameters.\')\n\n# LSTM specific parameters\nflags.DEFINE_integer(\'num_layers\', 2, \'Number of layers.\')\nflags.DEFINE_integer(\'num_hidden\', 200, \'Number of hidden units.\')\nflags.DEFINE_integer(\'vocab_size\', 10000, \'Vocabulary size.\')\nflags.DEFINE_integer(\'batch_size\', 20, \'Size of each mini-batch.\')\nflags.DEFINE_integer(\'num_steps\', 35, \'Number of unrolled steps of LSTM.\')\nflags.DEFINE_integer(\'num_epochs\', 10, \'Number of epochs.\')\nflags.DEFINE_float(\'learning_rate\', 1.0, \'Initial learning rate.\')\nflags.DEFINE_float(\'dropout\', 0.5, \'Dropout parameter.\')\nflags.DEFINE_float(\'init_scale\', 0.05, \'initial scale of the weights.\')\nflags.DEFINE_integer(\'max_grad_norm\', 5, \'Max norm of the gradient.\')\nflags.DEFINE_float(\'lr_decay\', 0.8, \'lr decay after num_epochs/3.\')\n\nassert FLAGS.dataset in [\'ptb\']\n\nif __name__ == \'__main__\':\n\n    utilities.random_seed_np_tf(FLAGS.seed)\n\n    if FLAGS.dataset == \'ptb\':\n\n        # ############### #\n        #   PTB Dataset   #\n        # ############### #\n\n        trX, vlX, teX = datasets.load_ptb_dataset(FLAGS.ptb_dir)\n\n    else:\n        trX, vlX, teX = None, None, None\n\n    model = LSTM(\n        FLAGS.num_layers, FLAGS.num_hidden, FLAGS.vocab_size,\n        FLAGS.batch_size, FLAGS.num_steps, FLAGS.num_epochs,\n        FLAGS.learning_rate, FLAGS.dropout, FLAGS.init_scale,\n        FLAGS.max_grad_norm, FLAGS.lr_decay\n    )\n\n    model.fit(trX, teX)\n'"
yadlt/core/__init__.py,0,"b'""""""Yadlt core package.""""""\n\nfrom __future__ import absolute_import\n\nfrom .config import *\nfrom .layers import *\nfrom .trainers import *\nfrom .model import *\nfrom .supervised_model import *\nfrom .unsupervised_model import *\n'"
yadlt/core/config.py,0,"b'""""""Library-wise configurations.""""""\n\nimport errno\nimport os\n\n\nclass Config(object):\n    """"""Configuration class.""""""\n\n    class __Singleton(object):\n        """"""Singleton design pattern.""""""\n\n        def __init__(self, models_dir=\'models/\', data_dir=\'data/\',\n                     logs_dir=\'logs/\'):\n            """"""Constructor.\n\n            Parameters\n            ----------\n            models_dir : string, optional (default=\'models/\')\n                directory path to store trained models.\n                Path is relative to ~/.yadlt\n            data_dir : string, optional (default=\'data/\')\n                directory path to store model generated data.\n                Path is relative to ~/.yadlt\n            logs_dir : string, optional (default=\'logs/\')\n                directory path to store yadlt and tensorflow logs.\n                Path is relative to ~/.yadlt\n            """"""\n            self.home_dir = os.path.join(os.path.expanduser(""~""), \'.yadlt\')\n            self.models_dir = os.path.join(self.home_dir, models_dir)\n            self.data_dir = os.path.join(self.home_dir, data_dir)\n            self.logs_dir = os.path.join(self.home_dir, logs_dir)\n            self.mkdir_p(self.home_dir)\n            self.mkdir_p(self.models_dir)\n            self.mkdir_p(self.data_dir)\n            self.mkdir_p(self.logs_dir)\n\n        def mkdir_p(self, path):\n            """"""Recursively create directories.""""""\n            try:\n                os.makedirs(path)\n            except OSError as exc:  # Python >2.5\n                if exc.errno == errno.EEXIST and os.path.isdir(path):\n                    pass\n                else:\n                    raise\n\n    instance = None\n\n    def __new__(cls):\n        """"""Return singleton instance.""""""\n        if not Config.instance:\n            Config.instance = Config.__Singleton()\n        return Config.instance\n\n    def __getattr__(self, name):\n        """"""Get singleton instance\'s attribute.""""""\n        return getattr(self.instance, name)\n\n    def __setattr__(self, name):\n        """"""Set singleton instance\'s attribute.""""""\n        return setattr(self.instance, name)\n'"
yadlt/core/layers.py,23,"b'""""""Collection of common layers.""""""\n\nimport tensorflow as tf\n\n\nclass Layers(object):\n    """"""Collection of computational NN layers.""""""\n\n    @staticmethod\n    def linear(prev_layer, out_dim, name=""linear""):\n        """"""Create a linear fully-connected layer.\n\n        Parameters\n        ----------\n\n        prev_layer : tf.Tensor\n            Last layer\'s output tensor.\n\n        out_dim : int\n            Number of output units.\n\n        Returns\n        -------\n\n        tuple (\n            tf.Tensor : Linear output tensor\n            tf.Tensor : Linear weights variable\n            tf.Tensor : Linear biases variable\n        )\n        """"""\n        with tf.name_scope(name):\n            in_dim = prev_layer.get_shape()[1].value\n            W = tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=0.1))\n            b = tf.Variable(tf.constant(0.1, shape=[out_dim]))\n            out = tf.add(tf.matmul(prev_layer, W), b)\n            return (out, W, b)\n\n    @staticmethod\n    def regularization(variables, regtype, regcoef, name=""regularization""):\n        """"""Compute the regularization tensor.\n\n        Parameters\n        ----------\n\n        variables : list of tf.Variable\n            List of model variables.\n\n        regtype : str\n            Type of regularization. Can be [""none"", ""l1"", ""l2""]\n\n        regcoef : float,\n            Regularization coefficient.\n\n        name : str, optional (default = ""regularization"")\n            Name for the regularization op.\n\n        Returns\n        -------\n\n        tf.Tensor : Regularization tensor.\n        """"""\n        with tf.name_scope(name):\n            if regtype != \'none\':\n                regs = tf.constant(0.0)\n                for v in variables:\n                    if regtype == \'l2\':\n                        regs = tf.add(regs, tf.nn.l2_loss(v))\n                    elif regtype == \'l1\':\n                        regs = tf.add(regs, tf.reduce_sum(tf.abs(v)))\n\n                return tf.multiply(regcoef, regs)\n            else:\n                return None\n\n\nclass Evaluation(object):\n    """"""Collection of evaluation methods.""""""\n\n    @staticmethod\n    def accuracy(mod_y, ref_y, summary=True, name=""accuracy""):\n        """"""Accuracy computation op.\n\n        Parameters\n        ----------\n\n        mod_y : tf.Tensor\n            Model output tensor.\n\n        ref_y : tf.Tensor\n            Reference input tensor.\n\n        summary : bool, optional (default = True)\n            Whether to save tf summary for the op.\n\n        Returns\n        -------\n\n        tf.Tensor : accuracy op. tensor\n        """"""\n        with tf.name_scope(name):\n            mod_pred = tf.argmax(mod_y, 1)\n            correct_pred = tf.equal(mod_pred, tf.argmax(ref_y, 1))\n            accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n            if summary:\n                tf.summary.scalar(\'accuracy\', accuracy)\n            return accuracy\n'"
yadlt/core/model.py,4,"b'""""""Model scheleton.""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom .config import Config\n\n\nclass Model(object):\n    """"""Class representing an abstract Model.""""""\n\n    def __init__(self, name):\n        """"""Constructor.\n\n        :param name: name of the model, used as filename.\n            string, default \'dae\'\n        """"""\n        self.name = name\n        self.model_path = os.path.join(Config().models_dir, self.name)\n\n        self.input_data = None\n        self.input_labels = None\n        self.keep_prob = None\n        self.layer_nodes = []  # list of layers of the final network\n        self.train_step = None\n        self.cost = None\n\n        # tensorflow objects\n        self.tf_graph = tf.Graph()\n        self.tf_session = None\n        self.tf_saver = None\n        self.tf_merged_summaries = None\n        self.tf_summary_writer = None\n\n    def pretrain_procedure(self, layer_objs, layer_graphs, set_params_func,\n                           train_set, validation_set=None):\n        """"""Perform unsupervised pretraining of the model.\n\n        :param layer_objs: list of model objects (autoencoders or rbms)\n        :param layer_graphs: list of model tf.Graph objects\n        :param set_params_func: function used to set the parameters after\n            pretraining\n        :param train_set: training set\n        :param validation_set: validation set\n        :return: return data encoded by the last layer\n        """"""\n        next_train = train_set\n        next_valid = validation_set\n\n        for l, layer_obj in enumerate(layer_objs):\n            print(\'Training layer {}...\'.format(l + 1))\n            next_train, next_valid = self._pretrain_layer_and_gen_feed(\n                layer_obj, set_params_func, next_train, next_valid,\n                layer_graphs[l])\n\n        return next_train, next_valid\n\n    def _pretrain_layer_and_gen_feed(self, layer_obj, set_params_func,\n                                     train_set, validation_set, graph):\n        """"""Pretrain a single autoencoder and encode the data for the next layer.\n\n        :param layer_obj: layer model\n        :param set_params_func: function used to set the parameters after\n            pretraining\n        :param train_set: training set\n        :param validation_set: validation set\n        :param graph: tf object for the rbm\n        :return: encoded train data, encoded validation data\n        """"""\n        layer_obj.fit(train_set, train_set,\n                      validation_set, validation_set, graph=graph)\n\n        with graph.as_default():\n            set_params_func(layer_obj, graph)\n\n            next_train = layer_obj.transform(train_set, graph=graph)\n            if validation_set is not None:\n                next_valid = layer_obj.transform(validation_set, graph=graph)\n            else:\n                next_valid = None\n\n        return next_train, next_valid\n\n    def get_layers_output(self, dataset):\n        """"""Get output from each layer of the network.\n\n        :param dataset: input data\n        :return: list of np array, element i is the output of layer i\n        """"""\n        layers_out = []\n\n        with self.tf_graph.as_default():\n            with tf.Session() as self.tf_session:\n                self.tf_saver.restore(self.tf_session, self.model_path)\n                for l in self.layer_nodes:\n                    layers_out.append(l.eval({self.input_data: dataset,\n                                              self.keep_prob: 1}))\n\n        if layers_out == []:\n            raise Exception(""This method is not implemented for this model"")\n        else:\n            return layers_out\n\n    def get_parameters(self, params, graph=None):\n        """"""Get the parameters of the model.\n\n        :param params: dictionary of keys (str names) and values (tensors).\n        :return: evaluated tensors in params\n        """"""\n        g = graph if graph is not None else self.tf_graph\n\n        with g.as_default():\n            with tf.Session() as self.tf_session:\n                self.tf_saver.restore(self.tf_session, self.model_path)\n                out = {}\n                for par in params:\n                    if type(params[par]) == list:\n                        for i, p in enumerate(params[par]):\n                            out[par + \'-\' + str(i+1)] = p.eval()\n                    else:\n                        out[par] = params[par].eval()\n                return out\n'"
yadlt/core/supervised_model.py,4,"b'""""""Supervised Model scheleton.""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom yadlt.core.model import Model\nfrom yadlt.utils import tf_utils\n\n\nclass SupervisedModel(Model):\n    """"""Supervised Model scheleton.\n\n    The interface of the class is sklearn-like.\n\n    Methods\n    -------\n\n    * fit(): model training procedure.\n    * predict(): model inference procedure (predict labels).\n    * score(): model scoring procedure (mean accuracy).\n    """"""\n\n    def __init__(self, name):\n        """"""Constructor.""""""\n        Model.__init__(self, name)\n\n    def fit(self, train_X, train_Y, val_X=None, val_Y=None, graph=None):\n        """"""Fit the model to the data.\n\n        Parameters\n        ----------\n\n        train_X : array_like, shape (n_samples, n_features)\n            Training data.\n\n        train_Y : array_like, shape (n_samples, n_classes)\n            Training labels.\n\n        val_X : array_like, shape (N, n_features) optional, (default = None).\n            Validation data.\n\n        val_Y : array_like, shape (N, n_classes) optional, (default = None).\n            Validation labels.\n\n        graph : tf.Graph, optional (default = None)\n            Tensorflow Graph object.\n\n        Returns\n        -------\n        """"""\n        if len(train_Y.shape) != 1:\n            num_classes = train_Y.shape[1]\n        else:\n            raise Exception(""Please convert the labels with one-hot encoding."")\n\n        g = graph if graph is not None else self.tf_graph\n\n        with g.as_default():\n            # Build model\n            self.build_model(train_X.shape[1], num_classes)\n            with tf.Session() as self.tf_session:\n                # Initialize tf stuff\n                summary_objs = tf_utils.init_tf_ops(self.tf_session)\n                self.tf_merged_summaries = summary_objs[0]\n                self.tf_summary_writer = summary_objs[1]\n                self.tf_saver = summary_objs[2]\n                # Train model\n                self._train_model(train_X, train_Y, val_X, val_Y)\n                # Save model\n                self.tf_saver.save(self.tf_session, self.model_path)\n\n    def predict(self, test_X):\n        """"""Predict the labels for the test set.\n\n        Parameters\n        ----------\n\n        test_X : array_like, shape (n_samples, n_features)\n            Test data.\n\n        Returns\n        -------\n\n        array_like, shape (n_samples,) : predicted labels.\n        """"""\n        with self.tf_graph.as_default():\n            with tf.Session() as self.tf_session:\n                self.tf_saver.restore(self.tf_session, self.model_path)\n                feed = {\n                    self.input_data: test_X,\n                    self.keep_prob: 1\n                }\n                return self.mod_y.eval(feed)\n\n    def score(self, test_X, test_Y):\n        """"""Compute the mean accuracy over the test set.\n\n        Parameters\n        ----------\n\n        test_X : array_like, shape (n_samples, n_features)\n            Test data.\n\n        test_Y : array_like, shape (n_samples, n_features)\n            Test labels.\n\n        Returns\n        -------\n\n        float : mean accuracy over the test set\n        """"""\n        with self.tf_graph.as_default():\n            with tf.Session() as self.tf_session:\n                self.tf_saver.restore(self.tf_session, self.model_path)\n                feed = {\n                    self.input_data: test_X,\n                    self.input_labels: test_Y,\n                    self.keep_prob: 1\n                }\n                return self.accuracy.eval(feed)\n'"
yadlt/core/trainers.py,18,"b'""""""Trainers module.""""""\n\nimport tensorflow as tf\n\n\nclass Trainer(object):\n    """"""Wrapper of Tensorflow Optimizers.""""""\n\n    def __init__(self, optimizer, **kw):\n        """"""Constructor.\n\n        Parameters\n        ----------\n        optimizer : string\n            Which optimizer to use. Possible values are [""sgd"", ""adagrad"",\n            ""adam"", ""momentum""]\n        kw :\n            the following arguments should be provided:\n                * sgd: learning_rate (float)\n                * adagrad: learning_rate (float), initial_accumulator_value\n                (float, default=0.1)\n                * adam: learning_rate (float, default=0.001), beta1 (float,\n                default=0.9), beta2 (float, default=0.999), epsilon (float,\n                default 1e-08)\n                * momentum: learning_rate (float), use_nesterov (bool)\n        """"""\n        assert optimizer in [""sgd"", ""adagrad"", ""adam"", ""momentum""]\n\n        def d(k, other=None):\n            if other is not None:\n                return kw[k] if k in kw else other\n            else:\n                return kw[k]\n\n        if optimizer == ""sgd"":\n            self.opt_ = tf.train.GradientDescentOptimizer(d(""learning_rate""))\n\n        elif optimizer == ""adagrad"":\n            self.opt_ = tf.train.AdagradOptimizer(\n                d(""learning_rate""), d(""initial_accumulator_value"", 0.1))\n\n        elif optimizer == ""adam"":\n            self.opt_ = tf.train.AdamOptimizer(d(""learning_rate"", 0.001),\n                                               d(""beta1"", 0.9),\n                                               d(""beta2"", 0.9),\n                                               d(""epsilon"", 1e-08))\n\n        elif optimizer == ""momentum"":\n            self.opt_ = tf.train.MomentumOptimizer(\n                d(""learning_rate""), d(""momentum""),\n                use_nesterov=d(""use_nesterov"", False))\n\n    def compile(self, cost, name_scope=""train""):\n        """"""Compile the optimizer with the given training parameters.\n\n        Parameters\n        ----------\n        cost : Tensor\n            A Tensor containing the value to minimize.\n        name_scope : str , optional (default=""train"")\n            Optional name scope for the optimizer graph ops.\n        """"""\n        with tf.name_scope(name_scope):\n            return self.opt_.minimize(cost)\n\n\nclass Loss(object):\n    """"""Collection of loss functions.""""""\n\n    def __init__(self, lfunc, summary=True, name=""loss""):\n        """"""Constructor.\n\n        Parameters\n        ----------\n\n        lfunc : str\n            Loss function type. Types supported:\n            ""cross_entropy"", ""softmax_cross_entropy"" and ""mse"".\n\n        summary : bool, optional (default = True)\n            Whether to attach a tf scalar summary to the op.\n\n        name : str, optional (default = ""loss"")\n            Name for the loss op.\n        """"""\n        assert lfunc in [""cross_entropy"",\n                         ""softmax_cross_entropy"",\n                         ""mse""]\n\n        self.lfunc = lfunc\n        self.summary = summary\n        self.name = name\n\n    def compile(self, mod_y, ref_y, regterm=None):\n        """"""Compute the loss function tensor.\n\n        Parameters\n        ----------\n\n        mode_y : tf.Tensor\n            model output tensor\n\n        ref_y : tf.Tensor\n            reference input tensor\n\n        regterm : tf.Tensor, optional (default = None)\n            Regularization term tensor\n\n        Returns\n        -------\n\n        Loss function tensor.\n        """"""\n        with tf.name_scope(self.name):\n            if self.lfunc == \'cross_entropy\':\n                clip_inf = tf.clip_by_value(mod_y, 1e-10, float(\'inf\'))\n                clip_sup = tf.clip_by_value(1 - mod_y, 1e-10, float(\'inf\'))\n\n                cost = - tf.reduce_mean(tf.add(\n                        tf.multiply(ref_y, tf.log(clip_inf)),\n                        tf.multiply(tf.subtract(1.0, ref_y), tf.log(clip_sup))))\n\n            elif self.lfunc == \'softmax_cross_entropy\':\n                cost = tf.losses.softmax_cross_entropy(ref_y, mod_y)\n\n            elif self.lfunc == \'mse\':\n                cost = tf.sqrt(tf.reduce_mean(\n                    tf.square(tf.subtract(ref_y, mod_y))))\n\n            else:\n                cost = None\n\n        if cost is not None:\n            cost = cost + regterm if regterm is not None else cost\n            tf.summary.scalar(self.lfunc, cost)\n        else:\n            cost = None\n\n        return cost\n'"
yadlt/core/unsupervised_model.py,7,"b'""""""Unsupervised Model scheleton.""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom yadlt.core.model import Model\nfrom yadlt.utils import tf_utils\n\n\nclass UnsupervisedModel(Model):\n    """"""Unsupervised Model scheleton class.\n\n    The interface of the class is sklearn-like.\n\n    Methods\n    -------\n\n    * fit(): model training procedure.\n    * transform(): model inference procedure.\n    * reconstruct(): model reconstruction procedure (autoencoders).\n    * score(): model scoring procedure (mean error).\n    """"""\n\n    def __init__(self, name):\n        """"""Constructor.""""""\n        Model.__init__(self, name)\n\n    def fit(self, train_X, train_Y=None, val_X=None, val_Y=None, graph=None):\n        """"""Fit the model to the data.\n\n        Parameters\n        ----------\n\n        train_X : array_like, shape (n_samples, n_features)\n            Training data.\n\n        train_Y : array_like, shape (n_samples, n_features)\n            Training reference data.\n\n        val_X : array_like, shape (N, n_features) optional, (default = None).\n            Validation data.\n\n        val_Y : array_like, shape (N, n_features) optional, (default = None).\n            Validation reference data.\n\n        graph : tf.Graph, optional (default = None)\n            Tensorflow Graph object.\n\n        Returns\n        -------\n        """"""\n        g = graph if graph is not None else self.tf_graph\n\n        with g.as_default():\n            # Build model\n            self.build_model(train_X.shape[1])\n            with tf.Session() as self.tf_session:\n                # Initialize tf stuff\n                summary_objs = tf_utils.init_tf_ops(self.tf_session)\n                self.tf_merged_summaries = summary_objs[0]\n                self.tf_summary_writer = summary_objs[1]\n                self.tf_saver = summary_objs[2]\n                # Train model\n                self._train_model(train_X, train_Y, val_X, val_Y)\n                # Save model\n                self.tf_saver.save(self.tf_session, self.model_path)\n\n    def transform(self, data, graph=None):\n        """"""Transform data according to the model.\n\n        Parameters\n        ----------\n\n        data : array_like, shape (n_samples, n_features)\n            Data to transform.\n\n        graph : tf.Graph, optional (default = None)\n            Tensorflow Graph object\n\n        Returns\n        -------\n        array_like, transformed data\n        """"""\n        g = graph if graph is not None else self.tf_graph\n\n        with g.as_default():\n            with tf.Session() as self.tf_session:\n                self.tf_saver.restore(self.tf_session, self.model_path)\n                feed = {self.input_data: data, self.keep_prob: 1}\n                return self.encode.eval(feed)\n\n    def reconstruct(self, data, graph=None):\n        """"""Reconstruct data according to the model.\n\n        Parameters\n        ----------\n\n        data : array_like, shape (n_samples, n_features)\n            Data to transform.\n\n        graph : tf.Graph, optional (default = None)\n            Tensorflow Graph object\n\n        Returns\n        -------\n        array_like, transformed data\n        """"""\n        g = graph if graph is not None else self.tf_graph\n\n        with g.as_default():\n            with tf.Session() as self.tf_session:\n                self.tf_saver.restore(self.tf_session, self.model_path)\n                feed = {self.input_data: data, self.keep_prob: 1}\n                return self.reconstruction.eval(feed)\n\n    def score(self, data, data_ref, graph=None):\n        """"""Compute the reconstruction loss over the test set.\n\n        Parameters\n        ----------\n\n        data : array_like\n            Data to reconstruct.\n\n        data_ref : array_like\n            Reference data.\n\n        Returns\n        -------\n\n        float: Mean error.\n        """"""\n        g = graph if graph is not None else self.tf_graph\n\n        with g.as_default():\n            with tf.Session() as self.tf_session:\n                self.tf_saver.restore(self.tf_session, self.model_path)\n                feed = {\n                    self.input_data: data,\n                    self.input_labels: data_ref,\n                    self.keep_prob: 1\n                }\n                return self.cost.eval(feed)\n'"
yadlt/models/__init__.py,0,"b'""""""Models package.""""""\n'"
yadlt/utils/__init__.py,0,"b'""""""Yadlt utilities package.""""""\n'"
yadlt/utils/datasets.py,0,"b'""""""Datasets module. Provides utilities to load popular datasets.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ntry:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle\n\nimport numpy as np\nimport os\n\n\ndef load_mnist_dataset(mode=\'supervised\', one_hot=True):\n    """"""Load the MNIST handwritten digits dataset.\n\n    :param mode: \'supervised\' or \'unsupervised\' mode\n    :param one_hot: whether to get one hot encoded labels\n    :return: train, validation, test data:\n            for (X, y) if \'supervised\',\n            for (X) if \'unsupervised\'\n    """"""\n    mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=one_hot)\n\n    # Training set\n    trX = mnist.train.images\n    trY = mnist.train.labels\n\n    # Validation set\n    vlX = mnist.validation.images\n    vlY = mnist.validation.labels\n\n    # Test set\n    teX = mnist.test.images\n    teY = mnist.test.labels\n\n    if mode == \'supervised\':\n        return trX, trY, vlX, vlY, teX, teY\n\n    elif mode == \'unsupervised\':\n        return trX, vlX, teX\n\n\ndef load_cifar10_dataset(cifar_dir, mode=\'supervised\'):\n    """"""Load the cifar10 dataset.\n\n    :param cifar_dir: path to the dataset directory\n        (cPicle format from: https://www.cs.toronto.edu/~kriz/cifar.html)\n    :param mode: \'supervised\' or \'unsupervised\' mode\n\n    :return: train, test data:\n            for (X, y) if \'supervised\',\n            for (X) if \'unsupervised\'\n    """"""\n    # Training set\n    trX = None\n    trY = np.array([])\n\n    # Test set\n    teX = np.array([])\n    teY = np.array([])\n\n    for fn in os.listdir(cifar_dir):\n\n        if not fn.startswith(\'batches\') and not fn.startswith(\'readme\'):\n            fo = open(os.path.join(cifar_dir, fn), \'rb\')\n            data_batch = pickle.load(fo)\n            fo.close()\n\n            if fn.startswith(\'data\'):\n\n                if trX is None:\n                    trX = data_batch[\'data\']\n                    trY = data_batch[\'labels\']\n                else:\n                    trX = np.concatenate((trX, data_batch[\'data\']), axis=0)\n                    trY = np.concatenate((trY, data_batch[\'labels\']), axis=0)\n\n            if fn.startswith(\'test\'):\n                teX = data_batch[\'data\']\n                teY = data_batch[\'labels\']\n\n    trX = trX.astype(np.float32) / 255.\n    teX = teX.astype(np.float32) / 255.\n\n    if mode == \'supervised\':\n        return trX, trY, teX, teY\n\n    elif mode == \'unsupervised\':\n        return trX, teX\n\n'"
yadlt/utils/tf_utils.py,8,"b'""""""Collection of Tensorflow specific utilities.""""""\n\nimport os\nimport tensorflow as tf\n\nfrom ..core.config import Config\n\n\ndef init_tf_ops(sess):\n    """"""Initialize TensorFlow operations.\n\n    This function initialize the following tensorflow ops:\n        * init variables ops\n        * summary ops\n        * create model saver\n\n    Parameters\n    ----------\n\n    sess : object\n        Tensorflow `Session` object\n\n    Returns\n    -------\n\n    tuple : (summary_merged, summary_writer)\n        * tf merged summaries object\n        * tf summary writer object\n        * tf saver object\n    """"""\n    summary_merged = tf.summary.merge_all()\n    init_op = tf.global_variables_initializer()\n    saver = tf.train.Saver()\n\n    sess.run(init_op)\n\n    # Retrieve run identifier\n    run_id = 0\n    for e in os.listdir(Config().logs_dir):\n        if e[:3] == \'run\':\n            r = int(e[3:])\n            if r > run_id:\n                run_id = r\n    run_id += 1\n    run_dir = os.path.join(Config().logs_dir, \'run\' + str(run_id))\n    print(\'Tensorboard logs dir for this run is %s\' % (run_dir))\n\n    summary_writer = tf.summary.FileWriter(run_dir, sess.graph)\n\n    return (summary_merged, summary_writer, saver)\n\n\ndef run_summaries(\n        sess, merged_summaries, summary_writer, epoch, feed, tens):\n    """"""Run the summaries and error computation on the validation set.\n\n    Parameters\n    ----------\n\n    sess : tf.Session\n        Tensorflow session object.\n\n    merged_summaries : tf obj\n        Tensorflow merged summaries obj.\n\n    summary_writer : tf.summary.FileWriter\n        Tensorflow summary writer obj.\n\n    epoch : int\n        Current training epoch.\n\n    feed : dict\n        Validation feed dict.\n\n    tens : tf.Tensor\n        Tensor to display and evaluate during training.\n        Can be self.accuracy for SupervisedModel or self.cost for\n        UnsupervisedModel.\n\n    Returns\n    -------\n\n    err : float, mean error over the validation set.\n    """"""\n    try:\n        result = sess.run([merged_summaries, tens], feed_dict=feed)\n        summary_str = result[0]\n        out = result[1]\n        summary_writer.add_summary(summary_str, epoch)\n    except tf.errors.InvalidArgumentError:\n        out = sess.run(tens, feed_dict=feed)\n\n    return out\n'"
yadlt/utils/utilities.py,8,"b'""""""Utitilies module.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom scipy import misc\nimport tensorflow as tf\n\n# ################### #\n#   Network helpers   #\n# ################### #\n\n\ndef sample_prob(probs, rand):\n    """"""Get samples from a tensor of probabilities.\n\n    :param probs: tensor of probabilities\n    :param rand: tensor (of the same shape as probs) of random values\n    :return: binary sample of probabilities\n    """"""\n    return tf.nn.relu(tf.sign(probs - rand))\n\n\ndef corrupt_input(data, sess, corrtype, corrfrac):\n    """"""Corrupt a fraction of data according to the chosen noise method.\n\n    :return: corrupted data\n    """"""\n    corruption_ratio = np.round(corrfrac * data.shape[1]).astype(np.int)\n\n    if corrtype == \'none\':\n        return np.copy(data)\n\n    if corrfrac > 0.0:\n        if corrtype == \'masking\':\n            return masking_noise(data, sess, corrfrac)\n\n        elif corrtype == \'salt_and_pepper\':\n            return salt_and_pepper_noise(data, corruption_ratio)\n    else:\n        return np.copy(data)\n\n\ndef xavier_init(fan_in, fan_out, const=1):\n    """"""Xavier initialization of network weights.\n\n    https://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n\n    :param fan_in: fan in of the network (n_features)\n    :param fan_out: fan out of the network (n_components)\n    :param const: multiplicative constant\n    """"""\n    low = -const * np.sqrt(6.0 / (fan_in + fan_out))\n    high = const * np.sqrt(6.0 / (fan_in + fan_out))\n    return tf.random_uniform((fan_in, fan_out), minval=low, maxval=high)\n\n\ndef seq_data_iterator(raw_data, batch_size, num_steps):\n    """"""Sequence data iterator.\n\n    Taken from tensorflow/models/rnn/ptb/reader.py\n    """"""\n    raw_data = np.array(raw_data, dtype=np.int32)\n\n    data_len = len(raw_data)\n    batch_len = data_len // batch_size\n    data = np.zeros([batch_size, batch_len], dtype=np.int32)\n    for i in range(batch_size):\n        data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\n\n    epoch_size = (batch_len - 1) // num_steps\n\n    if epoch_size == 0:\n        raise ValueError(""epoch_size == 0, decrease batch_size or num_steps"")\n\n    for i in range(epoch_size):\n        x = data[:, i * num_steps: (i+1) * num_steps]\n        y = data[:, i * num_steps + 1: (i+1) * num_steps + 1]\n    yield (x, y)\n\n\n# ################ #\n#   Data helpers   #\n# ################ #\n\n\ndef gen_batches(data, batch_size):\n    """"""Divide input data into batches.\n\n    :param data: input data\n    :param batch_size: size of each batch\n    :return: data divided into batches\n    """"""\n    data = np.array(data)\n\n    for i in range(0, data.shape[0], batch_size):\n        yield data[i:i + batch_size]\n\n\ndef to_one_hot(dataY):\n    """"""Convert the vector of labels dataY into one-hot encoding.\n\n    :param dataY: vector of labels\n    :return: one-hot encoded labels\n    """"""\n    nc = 1 + np.max(dataY)\n    onehot = [np.zeros(nc, dtype=np.int8) for _ in dataY]\n    for i, j in enumerate(dataY):\n        onehot[i][j] = 1\n    return onehot\n\n\ndef conv2bin(data):\n    """"""Convert a matrix of probabilities into binary values.\n\n    If the matrix has values <= 0 or >= 1, the values are\n    normalized to be in [0, 1].\n\n    :type data: numpy array\n    :param data: input matrix\n    :return: converted binary matrix\n    """"""\n    if data.min() < 0 or data.max() > 1:\n        data = normalize(data)\n\n    out_data = data.copy()\n\n    for i, sample in enumerate(out_data):\n\n        for j, val in enumerate(sample):\n\n            if np.random.random() <= val:\n                out_data[i][j] = 1\n            else:\n                out_data[i][j] = 0\n\n    return out_data\n\n\ndef normalize(data):\n    """"""Normalize the data to be in the [0, 1] range.\n\n    :param data:\n    :return: normalized data\n    """"""\n    out_data = data.copy()\n\n    for i, sample in enumerate(out_data):\n        out_data[i] /= sum(out_data[i])\n\n    return out_data\n\n\ndef masking_noise(data, sess, v):\n    """"""Apply masking noise to data in X.\n\n    In other words a fraction v of elements of X\n    (chosen at random) is forced to zero.\n    :param data: array_like, Input data\n    :param sess: TensorFlow session\n    :param v: fraction of elements to distort, float\n    :return: transformed data\n    """"""\n    data_noise = data.copy()\n    rand = tf.random_uniform(data.shape)\n    data_noise[sess.run(tf.nn.relu(tf.sign(v - rand))).astype(np.bool)] = 0\n\n    return data_noise\n\n\ndef salt_and_pepper_noise(X, v):\n    """"""Apply salt and pepper noise to data in X.\n\n    In other words a fraction v of elements of X\n    (chosen at random) is set to its maximum or minimum value according to a\n    fair coin flip.\n    If minimum or maximum are not given, the min (max) value in X is taken.\n    :param X: array_like, Input data\n    :param v: int, fraction of elements to distort\n    :return: transformed data\n    """"""\n    X_noise = X.copy()\n    n_features = X.shape[1]\n\n    mn = X.min()\n    mx = X.max()\n\n    for i, sample in enumerate(X):\n        mask = np.random.randint(0, n_features, v)\n\n        for m in mask:\n\n            if np.random.random() < 0.5:\n                X_noise[i][m] = mn\n            else:\n                X_noise[i][m] = mx\n\n    return X_noise\n\n# ############# #\n#   Utilities   #\n# ############# #\n\n\ndef expand_args(**args_to_expand):\n    """"""Expand the given lists into the length of the layers.\n\n    This is used as a convenience so that the user does not need to specify the\n    complete list of parameters for model initialization.\n    IE the user can just specify one parameter and this function will expand it\n    """"""\n    layers = args_to_expand[\'layers\']\n    try:\n        items = args_to_expand.iteritems()\n    except AttributeError:\n        items = args_to_expand.items()\n\n    for key, val in items:\n        if isinstance(val, list) and len(val) != len(layers):\n            args_to_expand[key] = [val[0] for _ in layers]\n\n    return args_to_expand\n\n\ndef flag_to_list(flagval, flagtype):\n    """"""Convert a string of comma-separated tf flags to a list of values.""""""\n    if flagtype == \'int\':\n        return [int(_) for _ in flagval.split(\',\') if _]\n\n    elif flagtype == \'float\':\n        return [float(_) for _ in flagval.split(\',\') if _]\n\n    elif flagtype == \'str\':\n        return [_ for _ in flagval.split(\',\') if _]\n\n    else:\n        raise Exception(""incorrect type"")\n\n\ndef str2actfunc(act_func):\n    """"""Convert activation function name to tf function.""""""\n    if act_func == \'sigmoid\':\n        return tf.nn.sigmoid\n\n    elif act_func == \'tanh\':\n        return tf.nn.tanh\n\n    elif act_func == \'relu\':\n        return tf.nn.relu\n\n\ndef random_seed_np_tf(seed):\n    """"""Seed numpy and tensorflow random number generators.\n\n    :param seed: seed parameter\n    """"""\n    if seed >= 0:\n        np.random.seed(seed)\n        tf.set_random_seed(seed)\n        return True\n    else:\n        return False\n\n\ndef gen_image(img, width, height, outfile, img_type=\'grey\'):\n    """"""Save an image with the given parameters.""""""\n    assert len(img) == width * height or len(img) == width * height * 3\n\n    if img_type == \'grey\':\n        misc.imsave(outfile, img.reshape(width, height))\n\n    elif img_type == \'color\':\n        misc.imsave(outfile, img.reshape(3, width, height))\n\n\ndef get_weights_as_images(weights_npy, width, height, outdir=\'img/\',\n                          n_images=10, img_type=\'grey\'):\n    """"""Create and save the weights of the hidden units as images.\n    :param weights_npy: path to the weights .npy file\n    :param width: width of the images\n    :param height: height of the images\n    :param outdir: output directory\n    :param n_images: number of images to generate\n    :param img_type: \'grey\' or \'color\' (RGB)\n    """"""\n    weights = np.load(weights_npy)\n    perm = np.random.permutation(weights.shape[1])[:n_images]\n\n    for p in perm:\n        w = np.array([i[p] for i in weights])\n        image_path = outdir + \'w_{}.png\'.format(p)\n        gen_image(w, width, height, image_path, img_type)\n'"
yadlt/models/autoencoders/__init__.py,0,"b'""""""Autoencoder Models package.""""""\n\nfrom __future__ import absolute_import\n'"
yadlt/models/autoencoders/deep_autoencoder.py,33,"b'""""""Implementation of Stacked Denoising Autoencoders for Unsup. Learning.""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import tqdm\n\nfrom yadlt.core import Layers, Loss, Trainer\nfrom yadlt.core import UnsupervisedModel\nfrom yadlt.models.autoencoders import denoising_autoencoder\nfrom yadlt.utils import tf_utils, utilities\n\n\nclass DeepAutoencoder(UnsupervisedModel):\n    """"""Implementation of Stacked Denoising Autoencoders using TensorFlow.\n\n    The interface of the class is sklearn-like.\n    """"""\n\n    def __init__(\n        self, layers, name=\'sdae\',\n        enc_act_func=[tf.nn.tanh], dec_act_func=[None],\n        loss_func=[\'cross_entropy\'], num_epochs=[10], batch_size=[10],\n        opt=[\'sgd\'], regtype=[\'none\'], regcoef=[5e-4],\n        learning_rate=[0.01], momentum=0.5, finetune_dropout=1,\n        corr_type=[\'none\'], finetune_regtype=\'none\', corr_frac=[0.],\n        finetune_loss_func=\'cross_entropy\', finetune_enc_act_func=[tf.nn.relu],\n        tied_weights=True, finetune_dec_act_func=[tf.nn.sigmoid],\n        finetune_batch_size=20, do_pretrain=False,\n        finetune_opt=\'sgd\', finetune_learning_rate=0.001,\n            finetune_num_epochs=10):\n        """"""Constructor.\n\n        :param layers: list containing the hidden units for each layer\n        :param enc_act_func: Activation function for the encoder.\n            [tf.nn.tanh, tf.nn.sigmoid]\n        :param dec_act_func: Activation function for the decoder.\n            [tf.nn.tanh, tf.nn.sigmoid, None]\n        :param regtype: regularization type, can be \'l2\',\'l1\' and \'none\'\n        :param finetune_regtype: regularization type for finetuning\n        :param finetune_loss_func: Loss function for the softmax layer.\n            string, default [\'cross_entropy\', \'mse\']\n        :param finetune_dropout: dropout parameter\n        :param finetune_learning_rate: learning rate for the finetuning.\n            float, default 0.001\n        :param finetune_enc_act_func: activation function for the encoder\n            finetuning phase\n        :param finetune_dec_act_func: activation function for the decoder\n            finetuning phase\n        :param finetune_opt: optimizer for the finetuning phase\n        :param finetune_num_epochs: Number of epochs for the finetuning.\n            int, default 20\n        :param finetune_batch_size: Size of each mini-batch for the finetuning.\n            int, default 20\n        :param tied_weights: if True, the decoder layers weights are\n            constrained to be the transpose of the encoder layers\n        :param corr_type: Type of input corruption. string, default \'none\'.\n            [""none"", ""masking"", ""salt_and_pepper""]\n        :param corr_frac: Fraction of the input to corrupt. float, default 0.0\n        :param do_pretrain: True: uses variables from pretraining,\n            False: initialize new variables.\n        """"""\n        # WARNING! This must be the first expression in the function or else it\n        # will send other variables to expanded_args()\n        # This function takes all the passed parameters that are lists and\n        # expands them to the number of layers, if the number\n        # of layers is more than the list of the parameter.\n        expanded_args = utilities.expand_args(**locals())\n\n        UnsupervisedModel.__init__(self, name)\n\n        self.loss_func = finetune_loss_func\n        self.learning_rate = finetune_learning_rate\n        self.opt = finetune_opt\n        self.num_epochs = finetune_num_epochs\n        self.batch_size = finetune_batch_size\n        self.momentum = momentum\n        self.dropout = finetune_dropout\n        self.regtype = finetune_regtype\n        self.regcoef = regcoef\n\n        self.loss = Loss(self.loss_func)\n        self.trainer = Trainer(\n            finetune_opt, learning_rate=finetune_learning_rate,\n            momentum=momentum)\n\n        self.do_pretrain = do_pretrain\n        self.layers = layers\n        self.tied_weights = tied_weights\n\n        self.finetune_enc_act_func = expanded_args[\'finetune_enc_act_func\']\n        self.finetune_dec_act_func = expanded_args[\'finetune_dec_act_func\']\n\n        self.input_ref = None\n\n        # Model parameters\n        self.encoding_w_ = []  # list of matrices of encoding weights per layer\n        self.encoding_b_ = []  # list of arrays of encoding biases per layer\n\n        self.decoding_w = []  # list of matrices of decoding weights per layer\n        self.decoding_b = []  # list of arrays of decoding biases per layer\n\n        self.reconstruction = None\n        self.autoencoders = []\n        self.autoencoder_graphs = []\n\n        for l, layer in enumerate(layers):\n            dae_str = \'dae-\' + str(l + 1)\n\n            self.autoencoders.append(\n                denoising_autoencoder.DenoisingAutoencoder(\n                    n_components=layer,\n                    name=self.name + \'-\' + dae_str,\n                    enc_act_func=expanded_args[\'enc_act_func\'][l],\n                    dec_act_func=expanded_args[\'dec_act_func\'][l],\n                    loss_func=expanded_args[\'loss_func\'][l],\n                    regtype=expanded_args[\'regtype\'][l],\n                    opt=expanded_args[\'opt\'][l],\n                    learning_rate=expanded_args[\'learning_rate\'][l],\n                    regcoef=expanded_args[\'regcoef\'],\n                    momentum=self.momentum,\n                    corr_type=expanded_args[\'corr_type\'][l],\n                    corr_frac=expanded_args[\'corr_frac\'][l],\n                    num_epochs=expanded_args[\'num_epochs\'][l],\n                    batch_size=expanded_args[\'batch_size\'][l]))\n\n            self.autoencoder_graphs.append(tf.Graph())\n\n    def pretrain(self, train_set, validation_set=None):\n        """"""Perform Unsupervised pretraining of the autoencoder.""""""\n        self.do_pretrain = True\n\n        def set_params_func(autoenc, autoencgraph):\n            params = autoenc.get_parameters(graph=autoencgraph)\n            self.encoding_w_.append(params[\'enc_w\'])\n            self.encoding_b_.append(params[\'enc_b\'])\n\n        return UnsupervisedModel.pretrain_procedure(\n            self, self.autoencoders, self.autoencoder_graphs,\n            set_params_func=set_params_func, train_set=train_set,\n            validation_set=validation_set)\n\n    def _train_model(self, train_set, train_ref,\n                     validation_set, validation_ref):\n        """"""Train the model.\n\n        :param train_set: training set\n        :param train_ref: training reference data\n        :param validation_set: validation set\n        :param validation_ref: validation reference data\n        :return: self\n        """"""\n        shuff = list(zip(train_set, train_ref))\n\n        pbar = tqdm(range(self.num_epochs))\n        for i in pbar:\n\n            np.random.shuffle(shuff)\n            batches = [_ for _ in utilities.gen_batches(\n                shuff, self.batch_size)]\n\n            for batch in batches:\n                x_batch, y_batch = zip(*batch)\n                self.tf_session.run(\n                    self.train_step,\n                    feed_dict={self.input_data: x_batch,\n                               self.input_labels: y_batch,\n                               self.keep_prob: self.dropout})\n\n            if validation_set is not None:\n                feed = {self.input_data: validation_set,\n                        self.input_labels: validation_ref, self.keep_prob: 1}\n                err = tf_utils.run_summaries(\n                    self.tf_session, self.tf_merged_summaries,\n                    self.tf_summary_writer, i, feed, self.cost)\n                pbar.set_description(""Reconstruction loss: %s"" % (err))\n\n    def build_model(self, n_features, encoding_w=None, encoding_b=None):\n        """"""Create the computational graph for the reconstruction task.\n\n        :param n_features: Number of features\n        :param encoding_w: list of weights for the encoding layers.\n        :param encoding_b: list of biases for the encoding layers.\n        :return: self\n        """"""\n        self._create_placeholders(n_features, n_features)\n\n        if encoding_w and encoding_b:\n            self.encoding_w_ = encoding_w\n            self.encoding_b_ = encoding_b\n        else:\n            self._create_variables(n_features)\n\n        self._create_encoding_layers()\n        self._create_decoding_layers()\n\n        variables = []\n        variables.extend(self.encoding_w_)\n        variables.extend(self.encoding_b_)\n        regterm = Layers.regularization(variables, self.regtype, self.regcoef)\n\n        self.cost = self.loss.compile(\n            self.reconstruction, self.input_labels, regterm=regterm)\n        self.train_step = self.trainer.compile(self.cost)\n\n    def _create_placeholders(self, n_features, n_classes):\n        """"""Create the TensorFlow placeholders for the model.\n\n        :param n_features: number of features of the first layer\n        :param n_classes: number of classes\n        :return: self\n        """"""\n        self.input_data = tf.placeholder(\n            tf.float32, [None, n_features], name=\'x-input\')\n        self.input_labels = tf.placeholder(\n            tf.float32, [None, n_classes], name=\'y-input\')\n        self.keep_prob = tf.placeholder(\n            tf.float32, name=\'keep-probs\')\n\n    def _create_variables(self, n_features):\n        """"""Create the TensorFlow variables for the model.\n\n        :param n_features: number of features\n        :return: self\n        """"""\n        if self.do_pretrain:\n            self._create_variables_pretrain()\n        else:\n            self._create_variables_no_pretrain(n_features)\n\n    def _create_variables_no_pretrain(self, n_features):\n        """"""Create model variables (no previous unsupervised pretraining).\n\n        :param n_features: number of features\n        :return: self\n        """"""\n        self.encoding_w_ = []\n        self.encoding_b_ = []\n\n        for l, layer in enumerate(self.layers):\n\n            if l == 0:\n                self.encoding_w_.append(tf.Variable(\n                    tf.truncated_normal(\n                        shape=[n_features, self.layers[l]], stddev=0.1)))\n                self.encoding_b_.append(tf.Variable(\n                    tf.truncated_normal([self.layers[l]], stddev=0.1)))\n            else:\n                self.encoding_w_.append(tf.Variable(\n                    tf.truncated_normal(\n                        shape=[self.layers[l - 1], self.layers[l]],\n                        stddev=0.1)))\n                self.encoding_b_.append(tf.Variable(\n                    tf.truncated_normal([self.layers[l]], stddev=0.1)))\n\n    def _create_variables_pretrain(self):\n        """"""Create model variables (previous unsupervised pretraining).\n\n        :return: self\n        """"""\n        for l, layer in enumerate(self.layers):\n            self.encoding_w_[l] = tf.Variable(\n                self.encoding_w_[l], name=\'enc-w-{}\'.format(l))\n            self.encoding_b_[l] = tf.Variable(\n                self.encoding_b_[l], name=\'enc-b-{}\'.format(l))\n\n    def _create_encoding_layers(self):\n        """"""Create the encoding layers for supervised finetuning.\n\n        :return: output of the final encoding layer.\n        """"""\n        next_train = self.input_data\n        self.layer_nodes = []\n\n        for l, layer in enumerate(self.layers):\n\n            with tf.name_scope(""encode-{}"".format(l)):\n\n                y_act = tf.add(\n                    tf.matmul(next_train, self.encoding_w_[l]),\n                    self.encoding_b_[l]\n                )\n\n                if self.finetune_enc_act_func[l] is not None:\n                    layer_y = self.finetune_enc_act_func[l](y_act)\n                else:\n                    layer_y = None\n\n                # the input to the next layer is the output of this layer\n                next_train = tf.nn.dropout(layer_y, self.keep_prob)\n\n            self.layer_nodes.append(next_train)\n\n        self.encode = next_train\n\n    def _create_decoding_layers(self):\n        """"""Create the decoding layers for reconstruction finetuning.\n\n        :return: output of the final encoding layer.\n        """"""\n        next_decode = self.encode\n\n        for l, layer in reversed(list(enumerate(self.layers))):\n\n            with tf.name_scope(""decode-{}"".format(l)):\n\n                # Create decoding variables\n                if self.tied_weights:\n                    dec_w = tf.transpose(self.encoding_w_[l])\n                else:\n                    dec_w = tf.Variable(tf.transpose(\n                        self.encoding_w_[l].initialized_value()))\n\n                dec_b = tf.Variable(tf.constant(\n                    0.1, shape=[dec_w.get_shape().dims[1].value]))\n                self.decoding_w.append(dec_w)\n                self.decoding_b.append(dec_b)\n\n                y_act = tf.add(\n                    tf.matmul(next_decode, dec_w),\n                    dec_b\n                )\n\n                if self.finetune_dec_act_func[l] is not None:\n                    layer_y = self.finetune_dec_act_func[l](y_act)\n                else:\n                    layer_y = None\n\n                # the input to the next layer is the output of this layer\n                next_decode = tf.nn.dropout(layer_y, self.keep_prob)\n\n            self.layer_nodes.append(next_decode)\n\n        self.reconstruction = next_decode\n'"
yadlt/models/autoencoders/denoising_autoencoder.py,24,"b'""""""Implementation of Denoising Autoencoder using TensorFlow.""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import tqdm\n\nfrom yadlt.core import Layers, Loss, Trainer\nfrom yadlt.core import UnsupervisedModel\nfrom yadlt.utils import tf_utils, utilities\n\n\nclass DenoisingAutoencoder(UnsupervisedModel):\n    """"""Implementation of Denoising Autoencoders using TensorFlow.\n\n    The interface of the class is sklearn-like.\n    """"""\n\n    def __init__(\n        self, n_components, name=\'dae\', loss_func=\'mse\',\n        enc_act_func=tf.nn.tanh, dec_act_func=None, num_epochs=10,\n        batch_size=10, opt=\'sgd\', learning_rate=0.01, momentum=0.9,\n            corr_type=\'none\', corr_frac=0., regtype=\'none\', regcoef=5e-4):\n        """"""Constructor.\n\n        Parameters\n        ----------\n\n        n_components : int\n            Number of hidden units.\n\n        name : str, optional (default = ""dae"")\n            Model name (used for save/load from disk).\n\n        loss_func : str, optional (default = ""mse"")\n            Loss function. [\'mse\', \'cross_entropy\']\n\n        enc_act_func : tf.nn.[activation]\n            Activation function for the encoder.\n\n        dec_act_func : tf.nn.[activation]\n            Activation function for the decoder.\n\n        num_epochs : int, optional (default = 10)\n            Number of epochs.\n\n        batch_size : int, optional (default = 10)\n            Size of each mini-batch.\n\n        opt : str, optional (default = ""sgd"")\n            Which tensorflow optimizer to use.\n            Possible values: [\'sgd\', \'momentum\', \'adagrad\', \'adam\']\n\n        learning_rate : float, optional (default = 0.01)\n            Initial learning rate.\n\n        momentum : float, optional (default = 0.9)\n            Momentum parameter (only used if opt = ""momentum"").\n\n        corr_type : str, optional (default = ""none"")\n            Type of input corruption.\n            Can be one of: [""none"", ""masking"", ""salt_and_pepper""]\n\n        corr_frac : float, optional (default = 0.0)\n            Fraction of the input to corrupt.\n\n        regtype : str, optional (default = ""none"")\n            Type of regularization to apply.\n            Can be one of: [""none"", ""l1"", ""l2""].\n\n        regcoef : float, optional (default = 5e-4)\n            Regularization parameter. If 0, no regularization.\n            Only considered if regtype != ""none"".\n        """"""\n        UnsupervisedModel.__init__(self, name)\n\n        self.loss_func = loss_func\n        self.learning_rate = learning_rate\n        self.opt = opt\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.momentum = momentum\n        self.regtype = regtype\n        self.regcoef = regcoef\n\n        self.loss = Loss(self.loss_func)\n        self.trainer = Trainer(\n            opt, learning_rate=learning_rate, momentum=momentum)\n\n        self.n_components = n_components\n        self.enc_act_func = enc_act_func\n        self.dec_act_func = dec_act_func\n        self.corr_type = corr_type\n        self.corr_frac = corr_frac\n\n        self.input_data_orig = None\n        self.input_data = None\n\n        self.W_ = None\n        self.bh_ = None\n        self.bv_ = None\n\n    def _train_model(self, train_X, train_Y=None, val_X=None, val_Y=None):\n        """"""Train the model.\n\n        Parameters\n        ----------\n\n        train_X : array_like\n            Training data, shape (num_samples, num_features).\n\n        train_Y : array_like, optional (default = None)\n            Reference training data, shape (num_samples, num_features).\n\n        val_X : array_like, optional, default None\n            Validation data, shape (num_val_samples, num_features).\n\n        val_Y : array_like, optional, default None\n            Reference validation data, shape (num_val_samples, num_features).\n\n        Returns\n        -------\n\n        self : trained model instance\n        """"""\n        pbar = tqdm(range(self.num_epochs))\n        for i in pbar:\n            self._run_train_step(train_X)\n            if val_X is not None:\n                feed = {self.input_data_orig: val_X,\n                        self.input_data: val_X}\n                err = tf_utils.run_summaries(\n                    self.tf_session, self.tf_merged_summaries,\n                    self.tf_summary_writer, i, feed, self.cost)\n                pbar.set_description(""Reconstruction loss: %s"" % (err))\n        return self\n\n    def _run_train_step(self, train_X):\n        """"""Run a training step.\n\n        A training step is made by randomly corrupting the training set,\n        randomly shuffling it,  divide it into batches and run the optimizer\n        for each batch.\n\n        Parameters\n        ----------\n\n        train_X : array_like\n            Training data, shape (num_samples, num_features).\n\n        Returns\n        -------\n\n        self\n        """"""\n        x_corrupted = utilities.corrupt_input(\n            train_X, self.tf_session, self.corr_type, self.corr_frac)\n\n        shuff = list(zip(train_X, x_corrupted))\n        np.random.shuffle(shuff)\n\n        batches = [_ for _ in utilities.gen_batches(shuff, self.batch_size)]\n\n        for batch in batches:\n            x_batch, x_corr_batch = zip(*batch)\n            tr_feed = {self.input_data_orig: x_batch,\n                       self.input_data: x_corr_batch}\n            self.tf_session.run(self.train_step, feed_dict=tr_feed)\n\n    def build_model(self, n_features, W_=None, bh_=None, bv_=None):\n        """"""Create the computational graph.\n\n        Parameters\n        ----------\n\n        n_features : int\n            Number of units in the input layer.\n\n        W_ : array_like, optional (default = None)\n            Weight matrix np array.\n\n        bh_ : array_like, optional (default = None)\n            Hidden bias np array.\n\n        bv_ : array_like, optional (default = None)\n            Visible bias np array.\n\n        Returns\n        -------\n\n        self\n        """"""\n        self._create_placeholders(n_features)\n        self._create_variables(n_features, W_, bh_, bv_)\n\n        self._create_encode_layer()\n        self._create_decode_layer()\n\n        variables = [self.W_, self.bh_, self.bv_]\n        regterm = Layers.regularization(variables, self.regtype, self.regcoef)\n\n        self.cost = self.loss.compile(\n            self.reconstruction, self.input_data_orig, regterm=regterm)\n        self.train_step = self.trainer.compile(self.cost)\n\n    def _create_placeholders(self, n_features):\n        """"""Create the TensorFlow placeholders for the model.\n\n        :return: self\n        """"""\n        self.input_data_orig = tf.placeholder(\n            tf.float32, [None, n_features], name=\'x-input\')\n        self.input_data = tf.placeholder(\n            tf.float32, [None, n_features], name=\'x-corr-input\')\n        # not used in this model, created just to comply\n        # with unsupervised_model.py\n        self.input_labels = tf.placeholder(tf.float32)\n        self.keep_prob = tf.placeholder(tf.float32, name=\'keep-probs\')\n\n    def _create_variables(self, n_features, W_=None, bh_=None, bv_=None):\n        """"""Create the TensorFlow variables for the model.\n\n        :return: self\n        """"""\n        if W_:\n            self.W_ = tf.Variable(W_, name=\'enc-w\')\n        else:\n            self.W_ = tf.Variable(\n                tf.truncated_normal(\n                    shape=[n_features, self.n_components], stddev=0.1),\n                name=\'enc-w\')\n\n        if bh_:\n            self.bh_ = tf.Variable(bh_, name=\'hidden-bias\')\n        else:\n            self.bh_ = tf.Variable(tf.constant(\n                0.1, shape=[self.n_components]), name=\'hidden-bias\')\n\n        if bv_:\n            self.bv_ = tf.Variable(bv_, name=\'visible-bias\')\n        else:\n            self.bv_ = tf.Variable(tf.constant(\n                0.1, shape=[n_features]), name=\'visible-bias\')\n\n    def _create_encode_layer(self):\n        """"""Create the encoding layer of the network.\n\n        Returns\n        -------\n\n        self\n        """"""\n        with tf.name_scope(""encoder""):\n\n            activation = tf.add(\n                tf.matmul(self.input_data, self.W_),\n                self.bh_\n            )\n\n            if self.enc_act_func:\n                self.encode = self.enc_act_func(activation)\n            else:\n                self.encode = activation\n\n            return self\n\n    def _create_decode_layer(self):\n        """"""Create the decoding layer of the network.\n\n        Returns\n        -------\n\n        self\n        """"""\n        with tf.name_scope(""decoder""):\n\n            activation = tf.add(\n                tf.matmul(self.encode, tf.transpose(self.W_)),\n                self.bv_\n            )\n\n            if self.dec_act_func:\n                self.reconstruction = self.dec_act_func(activation)\n            else:\n                self.reconstruction = activation\n\n            return self\n\n    def get_parameters(self, graph=None):\n        """"""Return the model parameters in the form of numpy arrays.\n\n        Parameters\n        ----------\n\n        graph : tf.Graph, optional (default = None)\n            Tensorflow graph object.\n\n        Returns\n        -------\n\n        dict : model parameters dictionary.\n        """"""\n        g = graph if graph is not None else self.tf_graph\n\n        with g.as_default():\n            with tf.Session() as self.tf_session:\n                self.tf_saver.restore(self.tf_session, self.model_path)\n\n                return {\n                    \'enc_w\': self.W_.eval(),\n                    \'enc_b\': self.bh_.eval(),\n                    \'dec_b\': self.bv_.eval()\n                }\n'"
yadlt/models/autoencoders/stacked_denoising_autoencoder.py,25,"b'""""""Implementation of Stacked Denoising Autoencoders for Supervised Learning.""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import tqdm\n\nfrom yadlt.core import SupervisedModel\nfrom yadlt.core import Evaluation, Layers, Loss, Trainer\nfrom yadlt.models.autoencoders import denoising_autoencoder\nfrom yadlt.utils import tf_utils, utilities\n\n\nclass StackedDenoisingAutoencoder(SupervisedModel):\n    """"""Implementation of Stacked Denoising Autoencoders using TensorFlow.\n\n    The interface of the class is sklearn-like.\n    """"""\n\n    def __init__(\n        self, layers, name=\'sdae\',\n        enc_act_func=[tf.nn.tanh], dec_act_func=[None],\n        loss_func=[\'cross_entropy\'], num_epochs=[10], batch_size=[10],\n        opt=[\'sgd\'], regcoef=[5e-4], learning_rate=[0.01], momentum=0.5,\n        finetune_dropout=1, corr_type=[\'none\'], corr_frac=[0.],\n        finetune_loss_func=\'softmax_cross_entropy\',\n        finetune_act_func=tf.nn.relu, finetune_opt=\'sgd\',\n        finetune_learning_rate=0.001, finetune_num_epochs=10,\n            finetune_batch_size=20, do_pretrain=False):\n        """"""Constructor.\n\n        :param layers: list containing the hidden units for each layer\n        :param enc_act_func: Activation function for the encoder.\n            [tf.nn.tanh, tf.nn.sigmoid]\n        :param dec_act_func: Activation function for the decoder.\n            [tf.nn.tanh, tf.nn.sigmoid, None]\n        :param finetune_loss_func: Loss function for the softmax layer.\n            string, default [\'softmax_cross_entropy\', \'mse\']\n        :param finetune_dropout: dropout parameter\n        :param finetune_learning_rate: learning rate for the finetuning.\n            float, default 0.001\n        :param finetune_act_func: activation function for the finetuning phase\n        :param finetune_opt: optimizer for the finetuning phase\n        :param finetune_num_epochs: Number of epochs for the finetuning.\n            int, default 20\n        :param finetune_batch_size: Size of each mini-batch for the finetuning.\n            int, default 20\n        :param corr_type: Type of input corruption. string, default \'none\'.\n            [""none"", ""masking"", ""salt_and_pepper""]\n        :param corr_frac: Fraction of the input to corrupt. float, default 0.0\n        :param do_pretrain: True: uses variables from pretraining,\n            False: initialize new variables.\n        """"""\n        # WARNING! This must be the first expression in the function or else it\n        # will send other variables to expanded_args()\n        # This function takes all the passed parameters that are lists and\n        # expands them to the number of layers, if the number\n        # of layers is more than the list of the parameter.\n        expanded_args = utilities.expand_args(**locals())\n\n        SupervisedModel.__init__(self, name)\n\n        self.loss_func = finetune_loss_func\n        self.learning_rate = finetune_learning_rate\n        self.opt = finetune_opt\n        self.num_epochs = finetune_num_epochs\n        self.batch_size = finetune_batch_size\n        self.momentum = momentum\n        self.dropout = finetune_dropout\n\n        self.loss = Loss(self.loss_func)\n        self.trainer = Trainer(\n            finetune_opt, learning_rate=finetune_learning_rate,\n            momentum=momentum)\n\n        self.do_pretrain = do_pretrain\n        self.layers = layers\n        self.finetune_act_func = finetune_act_func\n\n        # Model parameters\n        self.encoding_w_ = []  # list of matrices of encoding weights per layer\n        self.encoding_b_ = []  # list of arrays of encoding biases per layer\n\n        self.last_W = None\n        self.last_b = None\n\n        self.autoencoders = []\n        self.autoencoder_graphs = []\n\n        for l, layer in enumerate(layers):\n            dae_str = \'dae-\' + str(l + 1)\n\n            self.autoencoders.append(\n                denoising_autoencoder.DenoisingAutoencoder(\n                    n_components=layer,\n                    name=self.name + \'-\' + dae_str,\n                    enc_act_func=expanded_args[\'enc_act_func\'][l],\n                    dec_act_func=expanded_args[\'dec_act_func\'][l],\n                    loss_func=expanded_args[\'loss_func\'][l],\n                    opt=expanded_args[\'opt\'][l], regcoef=expanded_args[\'regcoef\'],\n                    learning_rate=expanded_args[\'learning_rate\'][l],\n                    momentum=self.momentum,\n                    corr_type=expanded_args[\'corr_type\'][l],\n                    corr_frac=expanded_args[\'corr_frac\'][l],\n                    num_epochs=expanded_args[\'num_epochs\'][l],\n                    batch_size=expanded_args[\'batch_size\'][l]))\n\n            self.autoencoder_graphs.append(tf.Graph())\n\n    def pretrain(self, train_set, validation_set=None):\n        """"""Perform Unsupervised pretraining of the autoencoder.""""""\n        self.do_pretrain = True\n\n        def set_params_func(autoenc, autoencgraph):\n            params = autoenc.get_parameters(graph=autoencgraph)\n            self.encoding_w_.append(params[\'enc_w\'])\n            self.encoding_b_.append(params[\'enc_b\'])\n\n        return SupervisedModel.pretrain_procedure(\n            self, self.autoencoders, self.autoencoder_graphs,\n            set_params_func=set_params_func, train_set=train_set,\n            validation_set=validation_set)\n\n    def _train_model(self, train_set, train_labels,\n                     validation_set, validation_labels):\n        """"""Train the model.\n\n        :param train_set: training set\n        :param train_labels: training labels\n        :param validation_set: validation set\n        :param validation_labels: validation labels\n        :return: self\n        """"""\n        shuff = list(zip(train_set, train_labels))\n\n        pbar = tqdm(range(self.num_epochs))\n        for i in pbar:\n\n            np.random.shuffle(shuff)\n\n            batches = [_ for _ in utilities.gen_batches(\n                shuff, self.batch_size)]\n\n            for batch in batches:\n                x_batch, y_batch = zip(*batch)\n                self.tf_session.run(\n                    self.train_step,\n                    feed_dict={self.input_data: x_batch,\n                               self.input_labels: y_batch,\n                               self.keep_prob: self.dropout})\n\n            if validation_set is not None:\n                feed = {self.input_data: validation_set,\n                        self.input_labels: validation_labels,\n                        self.keep_prob: 1}\n                acc = tf_utils.run_summaries(\n                    self.tf_session, self.tf_merged_summaries,\n                    self.tf_summary_writer, i, feed, self.accuracy)\n                pbar.set_description(""Accuracy: %s"" % (acc))\n\n    def build_model(self, n_features, n_classes):\n        """"""Create the computational graph.\n\n        This graph is intented to be created for finetuning,\n        i.e. after unsupervised pretraining.\n        :param n_features: Number of features.\n        :param n_classes: number of classes.\n        :return: self\n        """"""\n        self._create_placeholders(n_features, n_classes)\n        self._create_variables(n_features)\n\n        next_train = self._create_encoding_layers()\n        self.mod_y, _, _ = Layers.linear(next_train, n_classes)\n        self.layer_nodes.append(self.mod_y)\n\n        self.cost = self.loss.compile(self.mod_y, self.input_labels)\n        self.train_step = self.trainer.compile(self.cost)\n        self.accuracy = Evaluation.accuracy(self.mod_y, self.input_labels)\n\n    def _create_placeholders(self, n_features, n_classes):\n        """"""Create the TensorFlow placeholders for the model.\n\n        :param n_features: number of features of the first layer\n        :param n_classes: number of classes\n        :return: self\n        """"""\n        self.input_data = tf.placeholder(\n            tf.float32, [None, n_features], name=\'x-input\')\n\n        self.input_labels = tf.placeholder(\n            tf.float32, [None, n_classes], name=\'y-input\')\n\n        self.keep_prob = tf.placeholder(\n            tf.float32, name=\'keep-probs\')\n\n    def _create_variables(self, n_features):\n        """"""Create the TensorFlow variables for the model.\n\n        :param n_features: number of features\n        :return: self\n        """"""\n        if self.do_pretrain:\n            self._create_variables_pretrain()\n        else:\n            self._create_variables_no_pretrain(n_features)\n\n    def _create_variables_no_pretrain(self, n_features):\n        """"""Create model variables (no previous unsupervised pretraining).\n\n        :param n_features: number of features\n        :return: self\n        """"""\n        self.encoding_w_ = []\n        self.encoding_b_ = []\n\n        for l, layer in enumerate(self.layers):\n\n            w_name = \'enc-w-{}\'.format(l)\n            b_name = \'enc-b-{}\'.format(l)\n\n            if l == 0:\n                w_shape = [n_features, self.layers[l]]\n            else:\n                w_shape = [self.layers[l - 1], self.layers[l]]\n\n            w_init = tf.truncated_normal(shape=w_shape, stddev=0.1)\n            W = tf.Variable(w_init, name=w_name)\n            tf.summary.histogram(w_name, W)\n            self.encoding_w_.append(W)\n\n            b_init = tf.constant(0.1, shape=[self.layers[l]])\n            b = tf.Variable(b_init, name=b_name)\n            tf.summary.histogram(b_name, b)\n            self.encoding_b_.append(b)\n\n    def _create_variables_pretrain(self):\n        """"""Create model variables (previous unsupervised pretraining).\n\n        :return: self\n        """"""\n        for l, layer in enumerate(self.layers):\n            w_name = \'enc-w-{}\'.format(l)\n            b_name = \'enc-b-{}\'.format(l)\n\n            self.encoding_w_[l] = tf.Variable(\n                self.encoding_w_[l], name=w_name)\n            tf.summary.histogram(w_name, self.encoding_w_[l])\n\n            self.encoding_b_[l] = tf.Variable(\n                self.encoding_b_[l], name=b_name)\n            tf.summary.histogram(b_name, self.encoding_b_[l])\n\n    def _create_encoding_layers(self):\n        """"""Create the encoding layers for supervised finetuning.\n\n        :return: output of the final encoding layer.\n        """"""\n        next_train = self.input_data\n        self.layer_nodes = []\n\n        for l, layer in enumerate(self.layers):\n\n            with tf.name_scope(""encode-{}"".format(l)):\n\n                y_act = tf.add(\n                    tf.matmul(next_train, self.encoding_w_[l]),\n                    self.encoding_b_[l]\n                )\n\n                if self.finetune_act_func:\n                    layer_y = self.finetune_act_func(y_act)\n                else:\n                    layer_y = None\n\n                # the input to the next layer is the output of this layer\n                next_train = tf.nn.dropout(layer_y, self.keep_prob)\n\n            self.layer_nodes.append(next_train)\n\n        return next_train\n'"
yadlt/models/boltzmann/__init__.py,0,"b'""""""RBM Models package.""""""\n\nfrom __future__ import absolute_import\n'"
yadlt/models/boltzmann/dbn.py,21,"b'""""""Implementation of Deep Belief Network Model using TensorFlow.""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import tqdm\n\nfrom yadlt.core import SupervisedModel\nfrom yadlt.core import Evaluation, Layers, Loss, Trainer\nfrom yadlt.models.boltzmann import rbm\nfrom yadlt.utils import tf_utils, utilities\n\n\nclass DeepBeliefNetwork(SupervisedModel):\n    """"""Implementation of Deep Belief Network for Supervised Learning.\n\n    The interface of the class is sklearn-like.\n    """"""\n\n    def __init__(\n        self, rbm_layers, name=\'dbn\', do_pretrain=False,\n        rbm_num_epochs=[10], rbm_gibbs_k=[1],\n        rbm_gauss_visible=False, rbm_stddev=0.1, rbm_batch_size=[10],\n        rbm_learning_rate=[0.01], finetune_dropout=1,\n        finetune_loss_func=\'softmax_cross_entropy\',\n        finetune_act_func=tf.nn.sigmoid, finetune_opt=\'sgd\',\n        finetune_learning_rate=0.001, finetune_num_epochs=10,\n            finetune_batch_size=20, momentum=0.5):\n        """"""Constructor.\n\n        :param rbm_layers: list containing the hidden units for each layer\n        :param finetune_loss_func: Loss function for the softmax layer.\n            string, default [\'softmax_cross_entropy\', \'mse\']\n        :param finetune_dropout: dropout parameter\n        :param finetune_learning_rate: learning rate for the finetuning.\n            float, default 0.001\n        :param finetune_act_func: activation function for the finetuning phase\n        :param finetune_opt: optimizer for the finetuning phase\n        :param finetune_num_epochs: Number of epochs for the finetuning.\n            int, default 20\n        :param finetune_batch_size: Size of each mini-batch for the finetuning.\n            int, default 20\n        :param do_pretrain: True: uses variables from pretraining,\n            False: initialize new variables.\n        """"""\n        SupervisedModel.__init__(self, name)\n\n        self.loss_func = finetune_loss_func\n        self.learning_rate = finetune_learning_rate\n        self.opt = finetune_opt\n        self.num_epochs = finetune_num_epochs\n        self.batch_size = finetune_batch_size\n        self.momentum = momentum\n        self.dropout = finetune_dropout\n\n        self.loss = Loss(self.loss_func)\n        self.trainer = Trainer(\n            finetune_opt, learning_rate=finetune_learning_rate,\n            momentum=momentum)\n\n        self.do_pretrain = do_pretrain\n        self.layers = rbm_layers\n        self.finetune_act_func = finetune_act_func\n\n        # Model parameters\n        self.encoding_w_ = []  # list of matrices of encoding weights per layer\n        self.encoding_b_ = []  # list of arrays of encoding biases per layer\n\n        self.softmax_W = None\n        self.softmax_b = None\n\n        rbm_params = {\n            \'num_epochs\': rbm_num_epochs, \'gibbs_k\': rbm_gibbs_k,\n            \'batch_size\': rbm_batch_size, \'learning_rate\': rbm_learning_rate}\n\n        for p in rbm_params:\n            if len(rbm_params[p]) != len(rbm_layers):\n                # The current parameter is not specified by the user,\n                # should default it for all the layers\n                rbm_params[p] = [rbm_params[p][0] for _ in rbm_layers]\n\n        self.rbms = []\n        self.rbm_graphs = []\n\n        for l, layer in enumerate(rbm_layers):\n            rbm_str = \'rbm-\' + str(l+1)\n\n            if l == 0 and rbm_gauss_visible:\n                self.rbms.append(\n                    rbm.RBM(\n                        name=self.name + \'-\' + rbm_str,\n                        num_hidden=layer,\n                        learning_rate=rbm_params[\'learning_rate\'][l],\n                        num_epochs=rbm_params[\'num_epochs\'][l],\n                        batch_size=rbm_params[\'batch_size\'][l],\n                        gibbs_sampling_steps=rbm_params[\'gibbs_k\'][l],\n                        visible_unit_type=\'gauss\', stddev=rbm_stddev))\n\n            else:\n                self.rbms.append(\n                    rbm.RBM(\n                        name=self.name + \'-\' + rbm_str,\n                        num_hidden=layer,\n                        learning_rate=rbm_params[\'learning_rate\'][l],\n                        num_epochs=rbm_params[\'num_epochs\'][l],\n                        batch_size=rbm_params[\'batch_size\'][l],\n                        gibbs_sampling_steps=rbm_params[\'gibbs_k\'][l]))\n\n            self.rbm_graphs.append(tf.Graph())\n\n    def pretrain(self, train_set, validation_set=None):\n        """"""Perform Unsupervised pretraining of the DBN.""""""\n        self.do_pretrain = True\n\n        def set_params_func(rbmmachine, rbmgraph):\n            params = rbmmachine.get_parameters(graph=rbmgraph)\n            self.encoding_w_.append(params[\'W\'])\n            self.encoding_b_.append(params[\'bh_\'])\n\n        return SupervisedModel.pretrain_procedure(\n            self, self.rbms, self.rbm_graphs, set_params_func=set_params_func,\n            train_set=train_set, validation_set=validation_set)\n\n    def _train_model(self, train_set, train_labels,\n                     validation_set, validation_labels):\n        """"""Train the model.\n\n        :param train_set: training set\n        :param train_labels: training labels\n        :param validation_set: validation set\n        :param validation_labels: validation labels\n        :return: self\n        """"""\n        shuff = list(zip(train_set, train_labels))\n\n        pbar = tqdm(range(self.num_epochs))\n        for i in pbar:\n\n            np.random.shuffle(shuff)\n            batches = [_ for _ in utilities.gen_batches(\n                shuff, self.batch_size)]\n\n            for batch in batches:\n                x_batch, y_batch = zip(*batch)\n                self.tf_session.run(\n                    self.train_step, feed_dict={\n                        self.input_data: x_batch,\n                        self.input_labels: y_batch,\n                        self.keep_prob: self.dropout})\n\n            if validation_set is not None:\n                feed = {self.input_data: validation_set,\n                        self.input_labels: validation_labels,\n                        self.keep_prob: 1}\n                acc = tf_utils.run_summaries(\n                    self.tf_session, self.tf_merged_summaries,\n                    self.tf_summary_writer, i, feed, self.accuracy)\n                pbar.set_description(""Accuracy: %s"" % (acc))\n\n    def build_model(self, n_features, n_classes):\n        """"""Create the computational graph.\n\n        This graph is intented to be created for finetuning,\n        i.e. after unsupervised pretraining.\n        :param n_features: Number of features.\n        :param n_classes: number of classes.\n        :return: self\n        """"""\n        self._create_placeholders(n_features, n_classes)\n        self._create_variables(n_features)\n\n        next_train = self._create_encoding_layers()\n        self.mod_y, _, _ = Layers.linear(next_train, n_classes)\n        self.layer_nodes.append(self.mod_y)\n\n        self.cost = self.loss.compile(self.mod_y, self.input_labels)\n        self.train_step = self.trainer.compile(self.cost)\n        self.accuracy = Evaluation.accuracy(self.mod_y, self.input_labels)\n\n    def _create_placeholders(self, n_features, n_classes):\n        """"""Create the TensorFlow placeholders for the model.\n\n        :param n_features: number of features of the first layer\n        :param n_classes: number of classes\n        :return: self\n        """"""\n        self.input_data = tf.placeholder(\n            tf.float32, [None, n_features], name=\'x-input\')\n\n        self.input_labels = tf.placeholder(\n            tf.float32, [None, n_classes], name=\'y-input\')\n\n        self.keep_prob = tf.placeholder(tf.float32, name=\'keep-probs\')\n\n    def _create_variables(self, n_features):\n        """"""Create the TensorFlow variables for the model.\n\n        :param n_features: number of features\n        :return: self\n        """"""\n        if self.do_pretrain:\n            self._create_variables_pretrain()\n        else:\n            self._create_variables_no_pretrain(n_features)\n\n    def _create_variables_no_pretrain(self, n_features):\n        """"""Create model variables (no previous unsupervised pretraining).\n\n        :param n_features: number of features\n        :return: self\n        """"""\n        self.encoding_w_ = []\n        self.encoding_b_ = []\n\n        for l, layer in enumerate(self.layers):\n\n            w_name = \'enc-w-{}\'.format(l)\n            b_name = \'enc-b-{}\'.format(l)\n\n            if l == 0:\n                w_shape = [n_features, self.layers[l]]\n            else:\n                w_shape = [self.layers[l - 1], self.layers[l]]\n\n            w_init = tf.truncated_normal(shape=w_shape, stddev=0.1)\n            W = tf.Variable(w_init, name=w_name)\n            tf.summary.histogram(w_name, W)\n            self.encoding_w_.append(W)\n\n            b_init = tf.constant(0.1, shape=[self.layers[l]])\n            b = tf.Variable(b_init, name=b_name)\n            tf.summary.histogram(b_name, b)\n            self.encoding_b_.append(b)\n\n    def _create_variables_pretrain(self):\n        """"""Create model variables (previous unsupervised pretraining).\n\n        :return: self\n        """"""\n        for l, layer in enumerate(self.layers):\n\n            w_name = \'enc-w-{}\'.format(l)\n            b_name = \'enc-b-{}\'.format(l)\n\n            self.encoding_w_[l] = tf.Variable(\n                self.encoding_w_[l], name=w_name)\n            tf.summary.histogram(w_name, self.encoding_w_[l])\n\n            self.encoding_b_[l] = tf.Variable(\n                self.encoding_b_[l], name=b_name)\n            tf.summary.histogram(b_name, self.encoding_w_[l])\n\n    def _create_encoding_layers(self):\n        """"""Create the encoding layers for supervised finetuning.\n\n        :return: output of the final encoding layer.\n        """"""\n        next_train = self.input_data\n        self.layer_nodes = []\n\n        for l, layer in enumerate(self.layers):\n\n            with tf.name_scope(""encode-{}"".format(l)):\n\n                y_act = tf.add(\n                    tf.matmul(next_train, self.encoding_w_[l]),\n                    self.encoding_b_[l]\n                )\n\n                if self.finetune_act_func:\n                    layer_y = self.finetune_act_func(y_act)\n                else:\n                    layer_y = None\n\n                # the input to the next layer is the output of this layer\n                next_train = tf.nn.dropout(layer_y, self.keep_prob)\n\n            self.layer_nodes.append(next_train)\n\n        return next_train\n'"
yadlt/models/boltzmann/deep_autoencoder.py,26,"b'""""""Implementation of a Deep Unsupervised Autoencoder as a stack of RBMs.""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import tqdm\n\nfrom yadlt.core import Layers, Loss, Trainer\nfrom yadlt.core import UnsupervisedModel\nfrom yadlt.models.boltzmann import rbm\nfrom yadlt.utils import tf_utils, utilities\n\n\nclass DeepAutoencoder(UnsupervisedModel):\n    """"""Implementation of a Deep Unsupervised Autoencoder as a stack of RBMs.\n\n    The interface of the class is sklearn-like.\n    """"""\n\n    def __init__(\n        self, layers, name=\'srbm\',\n        num_epochs=[10], batch_size=[10],\n        learning_rate=[0.01], gibbs_k=[1], loss_func=[\'mse\'],\n        momentum=0.5, finetune_dropout=1,\n        finetune_loss_func=\'cross_entropy\', finetune_enc_act_func=[tf.nn.relu],\n        finetune_dec_act_func=[tf.nn.sigmoid], finetune_opt=\'sgd\',\n        finetune_learning_rate=0.001, regcoef=5e-4, finetune_num_epochs=10,\n        noise=[\'gauss\'], stddev=0.1, finetune_batch_size=20, do_pretrain=False,\n            tied_weights=False, regtype=[\'none\'], finetune_regtype=\'none\'):\n        """"""Constructor.\n\n        :param layers: list containing the hidden units for each layer\n        :param finetune_loss_func: Loss function for the softmax layer.\n            string, default [\'cross_entropy\', \'mse\']\n        :param finetune_dropout: dropout parameter\n        :param finetune_learning_rate: learning rate for the finetuning.\n            float, default 0.001\n        :param finetune_enc_act_func: activation function for the encoder\n            finetuning phase\n        :param finetune_dec_act_func: activation function for the decoder\n            finetuning phase\n        :param finetune_opt: optimizer for the finetuning phase\n        :param finetune_num_epochs: Number of epochs for the finetuning.\n            int, default 20\n        :param finetune_batch_size: Size of each mini-batch for the finetuning.\n            int, default 20\n        :param do_pretrain: True: uses variables from pretraining,\n            False: initialize new variables.\n        """"""\n        # WARNING! This must be the first expression in the function or else it\n        # will send other variables to expanded_args()\n        # This function takes all the passed parameters that are lists and\n        # expands them to the number of layers, if the number\n        # of layers is more than the list of the parameter.\n        expanded_args = utilities.expand_args(**locals())\n\n        UnsupervisedModel.__init__(self, name)\n\n        self.loss_func = finetune_loss_func\n        self.learning_rate = finetune_learning_rate\n        self.opt = finetune_opt\n        self.num_epochs = finetune_num_epochs\n        self.batch_size = finetune_batch_size\n        self.momentum = momentum\n        self.dropout = finetune_dropout\n        self.regtype = finetune_regtype\n        self.regcoef = regcoef\n\n        self.loss = Loss(self.loss_func)\n        self.trainer = Trainer(\n            finetune_opt, learning_rate=finetune_learning_rate,\n            momentum=momentum)\n\n        self.do_pretrain = do_pretrain\n        self.layers = layers\n        self.tied_weights = tied_weights\n\n        self.finetune_enc_act_func = expanded_args[\'finetune_enc_act_func\']\n        self.finetune_dec_act_func = expanded_args[\'finetune_dec_act_func\']\n\n        self.input_ref = None\n\n        # Model parameters\n        self.encoding_w_ = []  # list of matrices of encoding weights per layer\n        self.encoding_b_ = []  # list of arrays of encoding biases per layer\n\n        self.decoding_w = []  # list of matrices of decoding weights per layer\n        self.decoding_b = []  # list of arrays of decoding biases per layer\n\n        self.reconstruction = None\n        self.rbms = []\n        self.rbm_graphs = []\n\n        for l, layer in enumerate(layers):\n            rbm_str = \'rbm-\' + str(l + 1)\n            new_rbm = rbm.RBM(\n                name=self.name + \'-\' + rbm_str,\n                loss_func=expanded_args[\'loss_func\'][l],\n                visible_unit_type=expanded_args[\'noise\'][l], stddev=stddev,\n                num_hidden=expanded_args[\'layers\'][l],\n                learning_rate=expanded_args[\'learning_rate\'][l],\n                gibbs_sampling_steps=expanded_args[\'gibbs_k\'][l],\n                num_epochs=expanded_args[\'num_epochs\'][l],\n                batch_size=expanded_args[\'batch_size\'][l],\n                regtype=expanded_args[\'regtype\'][l])\n            self.rbms.append(new_rbm)\n            self.rbm_graphs.append(tf.Graph())\n\n    def pretrain(self, train_set, validation_set=None):\n        """"""Perform Unsupervised pretraining of the autoencoder.""""""\n        self.do_pretrain = True\n\n        def set_params_func(rbmmachine, rbmgraph):\n            params = rbmmachine.get_parameters(graph=rbmgraph)\n            self.encoding_w_.append(params[\'W\'])\n            self.encoding_b_.append(params[\'bh_\'])\n\n        return UnsupervisedModel.pretrain_procedure(\n            self, self.rbms, self.rbm_graphs, set_params_func=set_params_func,\n            train_set=train_set, validation_set=validation_set)\n\n    def _train_model(self, train_set, train_ref,\n                     validation_set, validation_ref):\n        """"""Train the model.\n\n        :param train_set: training set\n        :param train_ref: training reference data\n        :param validation_set: validation set\n        :param validation_ref: validation reference data\n        :return: self\n        """"""\n        shuff = list(zip(train_set, train_ref))\n\n        pbar = tqdm(range(self.num_epochs))\n        for i in pbar:\n\n            np.random.shuffle(shuff)\n            batches = [_ for _ in utilities.gen_batches(\n                shuff, self.batch_size)]\n\n            for batch in batches:\n                x_batch, y_batch = zip(*batch)\n                self.tf_session.run(\n                    self.train_step,\n                    feed_dict={self.input_data: x_batch,\n                               self.input_labels: y_batch,\n                               self.keep_prob: self.dropout})\n\n            if validation_set is not None:\n                feed = {self.input_data: validation_set,\n                        self.input_labels: validation_ref,\n                        self.keep_prob: 1}\n                err = tf_utils.run_summaries(\n                    self.tf_session, self.tf_merged_summaries,\n                    self.tf_summary_writer, i, feed, self.cost)\n                pbar.set_description(""Reconstruction loss: %s"" % (err))\n\n    def build_model(self, n_features, regtype=\'none\',\n                    encoding_w=None, encoding_b=None):\n        """"""Create the computational graph for the reconstruction task.\n\n        :param n_features: Number of features\n        :param regtype: regularization type\n        :param encoding_w: list of weights for the encoding layers.\n        :param encoding_b: list of biases for the encoding layers.\n        :return: self\n        """"""\n        self._create_placeholders(n_features, n_features)\n\n        if encoding_w and encoding_b:\n            self.encoding_w_ = encoding_w\n            self.encoding_b_ = encoding_b\n        else:\n            self._create_variables(n_features)\n\n        self._create_encoding_layers()\n        self._create_decoding_layers()\n\n        variables = []\n        variables.extend(self.encoding_w_)\n        variables.extend(self.encoding_b_)\n        regterm = Layers.regularization(variables, self.regtype, self.regcoef)\n\n        self.cost = self.loss.compile(\n            self.reconstruction, self.input_labels, regterm=regterm)\n        self.train_step = self.trainer.compile(self.cost)\n\n    def _create_placeholders(self, n_features, n_classes):\n        """"""Create the TensorFlow placeholders for the model.\n\n        :param n_features: number of features of the first layer\n        :param n_classes: number of classes\n        :return: self\n        """"""\n        self.input_data = tf.placeholder(\n            tf.float32, [None, n_features], name=\'x-input\')\n        self.input_labels = tf.placeholder(\n            tf.float32, [None, n_classes], name=\'y-input\')\n        self.keep_prob = tf.placeholder(\n            tf.float32, name=\'keep-probs\')\n\n    def _create_variables(self, n_features):\n        """"""Create the TensorFlow variables for the model.\n\n        :param n_features: number of features\n        :return: self\n        """"""\n        if self.do_pretrain:\n            self._create_variables_pretrain()\n        else:\n            self._create_variables_no_pretrain(n_features)\n\n    def _create_variables_no_pretrain(self, n_features):\n        """"""Create model variables (no previous unsupervised pretraining).\n\n        :param n_features: number of features\n        :return: self\n        """"""\n        self.encoding_w_ = []\n        self.encoding_b_ = []\n\n        for l, layer in enumerate(self.layers):\n\n            if l == 0:\n                self.encoding_w_.append(tf.Variable(tf.truncated_normal(\n                    shape=[n_features, self.layers[l]], stddev=0.1)))\n                self.encoding_b_.append(tf.Variable(tf.truncated_normal(\n                    [self.layers[l]], stddev=0.1)))\n            else:\n                self.encoding_w_.append(tf.Variable(tf.truncated_normal(\n                    shape=[self.layers[l - 1], self.layers[l]], stddev=0.1)))\n                self.encoding_b_.append(tf.Variable(tf.truncated_normal(\n                    [self.layers[l]], stddev=0.1)))\n\n    def _create_variables_pretrain(self):\n        """"""Create model variables (previous unsupervised pretraining).\n\n        :return: self\n        """"""\n        for l, layer in enumerate(self.layers):\n            self.encoding_w_[l] = tf.Variable(\n                self.encoding_w_[l], name=\'enc-w-{}\'.format(l))\n            self.encoding_b_[l] = tf.Variable(\n                self.encoding_b_[l], name=\'enc-b-{}\'.format(l))\n\n    def _create_encoding_layers(self):\n        """"""Create the encoding layers for supervised finetuning.\n\n        :return: output of the final encoding layer.\n        """"""\n        next_train = self.input_data\n        self.layer_nodes = []\n\n        for l, layer in enumerate(self.layers):\n\n            with tf.name_scope(""encode-{}"".format(l)):\n\n                y_act = tf.add(\n                    tf.matmul(next_train, self.encoding_w_[l]),\n                    self.encoding_b_[l]\n                )\n\n                if self.finetune_enc_act_func[l] is not None:\n                    layer_y = self.finetune_enc_act_func[l](y_act)\n\n                else:\n                    layer_y = None\n\n                # the input to the next layer is the output of this layer\n                next_train = tf.nn.dropout(layer_y, self.keep_prob)\n\n            self.layer_nodes.append(next_train)\n\n        self.encode = next_train\n\n    def _create_decoding_layers(self):\n        """"""Create the decoding layers for reconstruction finetuning.\n\n        :return: output of the final encoding layer.\n        """"""\n        next_decode = self.encode\n\n        for l, layer in reversed(list(enumerate(self.layers))):\n\n            with tf.name_scope(""decode-{}"".format(l)):\n\n                # Create decoding variables\n                if self.tied_weights:\n                    dec_w = tf.transpose(self.encoding_w_[l])\n                else:\n                    dec_w = tf.Variable(tf.transpose(\n                        self.encoding_w_[l].initialized_value()))\n\n                dec_b = tf.Variable(tf.constant(\n                    0.1, shape=[dec_w.get_shape().dims[1].value]))\n                self.decoding_w.append(dec_w)\n                self.decoding_b.append(dec_b)\n\n                y_act = tf.add(\n                    tf.matmul(next_decode, dec_w),\n                    dec_b\n                )\n\n                if self.finetune_dec_act_func[l] is not None:\n                    layer_y = self.finetune_dec_act_func[l](y_act)\n\n                else:\n                    layer_y = None\n\n                # the input to the next layer is the output of this layer\n                next_decode = tf.nn.dropout(layer_y, self.keep_prob)\n\n            self.layer_nodes.append(next_decode)\n\n        self.reconstruction = next_decode\n'"
yadlt/models/boltzmann/rbm.py,27,"b'""""""Restricted Boltzmann Machine TensorFlow implementation.""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import tqdm\n\nfrom yadlt.core import Layers, Loss\nfrom yadlt.core import UnsupervisedModel\nfrom yadlt.utils import tf_utils, utilities\n\n\nclass RBM(UnsupervisedModel):\n    """"""Restricted Boltzmann Machine implementation using TensorFlow.\n\n    The interface of the class is sklearn-like.\n    """"""\n\n    def __init__(\n        self, num_hidden, visible_unit_type=\'bin\',\n        name=\'rbm\', loss_func=\'mse\', learning_rate=0.01,\n        regcoef=5e-4, regtype=\'none\', gibbs_sampling_steps=1,\n            batch_size=10, num_epochs=10, stddev=0.1):\n        """"""Constructor.\n\n        :param num_hidden: number of hidden units\n        :param loss_function: type of loss function\n        :param visible_unit_type: type of the visible units (bin or gauss)\n        :param gibbs_sampling_steps: optional, default 1\n        :param stddev: default 0.1. Ignored if visible_unit_type is not \'gauss\'\n        """"""\n        UnsupervisedModel.__init__(self, name)\n\n        self.loss_func = loss_func\n        self.learning_rate = learning_rate\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.regtype = regtype\n        self.regcoef = regcoef\n\n        self.loss = Loss(self.loss_func)\n\n        self.num_hidden = num_hidden\n        self.visible_unit_type = visible_unit_type\n        self.gibbs_sampling_steps = gibbs_sampling_steps\n        self.stddev = stddev\n\n        self.W = None\n        self.bh_ = None\n        self.bv_ = None\n\n        self.w_upd8 = None\n        self.bh_upd8 = None\n        self.bv_upd8 = None\n\n        self.cost = None\n\n        self.input_data = None\n        self.hrand = None\n        self.vrand = None\n\n    def _train_model(self, train_set, train_ref=None, validation_set=None,\n                      Validation_ref=None):\n        """"""Train the model.\n\n        :param train_set: training set\n        :param validation_set: validation set. optional, default None\n        :return: self\n        """"""\n        pbar = tqdm(range(self.num_epochs))\n        for i in pbar:\n            self._run_train_step(train_set)\n\n            if validation_set is not None:\n                feed = self._create_feed_dict(validation_set)\n                err = tf_utils.run_summaries(\n                    self.tf_session, self.tf_merged_summaries,\n                    self.tf_summary_writer, i, feed, self.cost)\n                pbar.set_description(""Reconstruction loss: %s"" % (err))\n\n    def _run_train_step(self, train_set):\n        """"""Run a training step.\n\n        A training step is made by randomly shuffling the training set,\n        divide into batches and run the variable update nodes for each batch.\n        :param train_set: training set\n        :return: self\n        """"""\n        np.random.shuffle(train_set)\n\n        batches = [_ for _ in utilities.gen_batches(train_set,\n                                                    self.batch_size)]\n        updates = [self.w_upd8, self.bh_upd8, self.bv_upd8]\n\n        for batch in batches:\n            self.tf_session.run(updates,\n                                feed_dict=self._create_feed_dict(batch))\n\n    def _create_feed_dict(self, data):\n        """"""Create the dictionary of data to feed to tf session during training.\n\n        :param data: training/validation set batch\n        :return: dictionary(self.input_data: data, self.hrand: random_uniform,\n                            self.vrand: random_uniform)\n        """"""\n        return {\n            self.input_data: data,\n            self.hrand: np.random.rand(data.shape[0], self.num_hidden),\n            self.vrand: np.random.rand(data.shape[0], data.shape[1])\n        }\n\n    def build_model(self, n_features, regtype=\'none\'):\n        """"""Build the Restricted Boltzmann Machine model in TensorFlow.\n\n        :param n_features: number of features\n        :param regtype: regularization type\n        :return: self\n        """"""\n        self._create_placeholders(n_features)\n        self._create_variables(n_features)\n        self.encode = self.sample_hidden_from_visible(self.input_data)[0]\n        self.reconstruction = self.sample_visible_from_hidden(\n            self.encode, n_features)\n\n        hprob0, hstate0, vprob, hprob1, hstate1 = self.gibbs_sampling_step(\n            self.input_data, n_features)\n        positive = self.compute_positive_association(self.input_data,\n                                                     hprob0, hstate0)\n\n        nn_input = vprob\n\n        for step in range(self.gibbs_sampling_steps - 1):\n            hprob, hstate, vprob, hprob1, hstate1 = self.gibbs_sampling_step(\n                nn_input, n_features)\n            nn_input = vprob\n\n        negative = tf.matmul(tf.transpose(vprob), hprob1)\n\n        self.w_upd8 = self.W.assign_add(\n            self.learning_rate * (positive - negative) / self.batch_size)\n\n        self.bh_upd8 = self.bh_.assign_add(tf.multiply(self.learning_rate, tf.reduce_mean(\n            tf.subtract(hprob0, hprob1), 0)))\n\n        self.bv_upd8 = self.bv_.assign_add(tf.multiply(self.learning_rate, tf.reduce_mean(\n            tf.subtract(self.input_data, vprob), 0)))\n\n        variables = [self.W, self.bh_, self.bv_]\n        regterm = Layers.regularization(variables, self.regtype, self.regcoef)\n\n        self.cost = self.loss.compile(vprob, self.input_data, regterm=regterm)\n\n    def _create_placeholders(self, n_features):\n        """"""Create the TensorFlow placeholders for the model.\n\n        :param n_features: number of features\n        :return: self\n        """"""\n        self.input_data = tf.placeholder(tf.float32, [None, n_features],\n                                         name=\'x-input\')\n        self.hrand = tf.placeholder(tf.float32, [None, self.num_hidden],\n                                    name=\'hrand\')\n        self.vrand = tf.placeholder(tf.float32, [None, n_features],\n                                    name=\'vrand\')\n        # not used in this model, created just to comply with\n        # unsupervised_model.py\n        self.input_labels = tf.placeholder(tf.float32)\n        self.keep_prob = tf.placeholder(tf.float32, name=\'keep-probs\')\n\n    def _create_variables(self, n_features):\n        """"""Create the TensorFlow variables for the model.\n\n        :param n_features: number of features\n        :return: self\n        """"""\n        w_name = \'weights\'\n        self.W = tf.Variable(tf.truncated_normal(\n            shape=[n_features, self.num_hidden], stddev=0.1), name=w_name)\n        tf.summary.histogram(w_name, self.W)\n\n        bh_name = \'hidden-bias\'\n        self.bh_ = tf.Variable(tf.constant(0.1, shape=[self.num_hidden]),\n                               name=bh_name)\n        tf.summary.histogram(bh_name, self.bh_)\n\n        bv_name = \'visible-bias\'\n        self.bv_ = tf.Variable(tf.constant(0.1, shape=[n_features]),\n                               name=bv_name)\n        tf.summary.histogram(bv_name, self.bv_)\n\n    def gibbs_sampling_step(self, visible, n_features):\n        """"""Perform one step of gibbs sampling.\n\n        :param visible: activations of the visible units\n        :param n_features: number of features\n        :return: tuple(hidden probs, hidden states, visible probs,\n                       new hidden probs, new hidden states)\n        """"""\n        hprobs, hstates = self.sample_hidden_from_visible(visible)\n        vprobs = self.sample_visible_from_hidden(hprobs, n_features)\n        hprobs1, hstates1 = self.sample_hidden_from_visible(vprobs)\n\n        return hprobs, hstates, vprobs, hprobs1, hstates1\n\n    def sample_hidden_from_visible(self, visible):\n        """"""Sample the hidden units from the visible units.\n\n        This is the Positive phase of the Contrastive Divergence algorithm.\n\n        :param visible: activations of the visible units\n        :return: tuple(hidden probabilities, hidden binary states)\n        """"""\n        hprobs = tf.nn.sigmoid(tf.add(tf.matmul(visible, self.W), self.bh_))\n        hstates = utilities.sample_prob(hprobs, self.hrand)\n\n        return hprobs, hstates\n\n    def sample_visible_from_hidden(self, hidden, n_features):\n        """"""Sample the visible units from the hidden units.\n\n        This is the Negative phase of the Contrastive Divergence algorithm.\n        :param hidden: activations of the hidden units\n        :param n_features: number of features\n        :return: visible probabilities\n        """"""\n        visible_activation = tf.add(\n            tf.matmul(hidden, tf.transpose(self.W)),\n            self.bv_\n        )\n\n        if self.visible_unit_type == \'bin\':\n            vprobs = tf.nn.sigmoid(visible_activation)\n\n        elif self.visible_unit_type == \'gauss\':\n            vprobs = tf.truncated_normal(\n                (1, n_features), mean=visible_activation, stddev=self.stddev)\n\n        else:\n            vprobs = None\n\n        return vprobs\n\n    def compute_positive_association(self, visible,\n                                     hidden_probs, hidden_states):\n        """"""Compute positive associations between visible and hidden units.\n\n        :param visible: visible units\n        :param hidden_probs: hidden units probabilities\n        :param hidden_states: hidden units states\n        :return: positive association = dot(visible.T, hidden)\n        """"""\n        if self.visible_unit_type == \'bin\':\n            positive = tf.matmul(tf.transpose(visible), hidden_states)\n\n        elif self.visible_unit_type == \'gauss\':\n            positive = tf.matmul(tf.transpose(visible), hidden_probs)\n\n        else:\n            positive = None\n\n        return positive\n\n    def load_model(self, shape, gibbs_sampling_steps, model_path):\n        """"""Load a trained model from disk.\n\n        The shape of the model (num_visible, num_hidden) and the number\n        of gibbs sampling steps must be known in order to restore the model.\n        :param shape: tuple(num_visible, num_hidden)\n        :param gibbs_sampling_steps:\n        :param model_path:\n        :return: self\n        """"""\n        n_features, self.num_hidden = shape[0], shape[1]\n        self.gibbs_sampling_steps = gibbs_sampling_steps\n\n        self.build_model(n_features)\n\n        init_op = tf.global_variables_initializer()\n        self.tf_saver = tf.train.Saver()\n\n        with tf.Session() as self.tf_session:\n\n            self.tf_session.run(init_op)\n            self.tf_saver.restore(self.tf_session, model_path)\n\n    def get_parameters(self, graph=None):\n        """"""Return the model parameters in the form of numpy arrays.\n\n        :param graph: tf graph object\n        :return: model parameters\n        """"""\n        g = graph if graph is not None else self.tf_graph\n\n        with g.as_default():\n            with tf.Session() as self.tf_session:\n                self.tf_saver.restore(self.tf_session, self.model_path)\n\n                return {\n                    \'W\': self.W.eval(),\n                    \'bh_\': self.bh_.eval(),\n                    \'bv_\': self.bv_.eval()\n                }\n'"
yadlt/models/convolutional/__init__.py,0,"b'""""""Convolutional Models package.""""""\n\nfrom __future__ import absolute_import\n'"
yadlt/models/convolutional/conv_net.py,22,"b'""""""Implementation of Convolutional Neural Networks using TensorFlow.""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import tqdm\n\nfrom yadlt.core import SupervisedModel\nfrom yadlt.core import Evaluation, Loss, Trainer\nfrom yadlt.utils import tf_utils, utilities\n\n\nclass ConvolutionalNetwork(SupervisedModel):\n    """"""Implementation of Convolutional Neural Networks using TensorFlow.\n\n    The interface of the class is sklearn-like.\n    """"""\n\n    def __init__(\n        self, layers, original_shape, name=\'convnet\',\n        loss_func=\'softmax_cross_entropy\', num_epochs=10, batch_size=10,\n            opt=\'sgd\', learning_rate=0.01, momentum=0.5, dropout=0.5):\n        """"""Constructor.\n\n        :param layers: string used to build the model.\n            This string is a comma-separate specification of the layers.\n            Supported values:\n                conv2d-FX-FY-Z-S: 2d convolution with Z feature maps as output\n                    and FX x FY filters. S is the strides size\n                maxpool-X: max pooling on the previous layer. X is the size of\n                    the max pooling\n                full-X: fully connected layer with X units\n                softmax: softmax layer\n            For example:\n                conv2d-5-5-32,maxpool-2,conv2d-5-5-64,maxpool-2,full-128,full-128,softmax\n\n        :param original_shape: original shape of the images in the dataset\n        :param dropout: Dropout parameter\n        """"""\n        assert(layers.split("","")[-1] == ""softmax"")\n\n        SupervisedModel.__init__(self, name)\n\n        self.loss_func = loss_func\n        self.learning_rate = learning_rate\n        self.opt = opt\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.momentum = momentum\n\n        self.loss = Loss(self.loss_func)\n        self.trainer = Trainer(\n            opt, learning_rate=learning_rate,\n            momentum=momentum)\n\n        self.layers = layers\n        self.original_shape = original_shape\n        self.dropout = dropout\n\n        self.W_vars = None\n        self.B_vars = None\n\n        self.accuracy = None\n\n    def _train_model(self, train_set, train_labels,\n                     validation_set, validation_labels):\n        """"""Train the model.\n\n        :param train_set: training set\n        :param train_labels: training labels\n        :param validation_set: validation set\n        :param validation_labels: validation labels\n        :return: self\n        """"""\n        shuff = list(zip(train_set, train_labels))\n\n        pbar = tqdm(range(self.num_epochs))\n        for i in pbar:\n\n            np.random.shuffle(list(shuff))\n            batches = [_ for _ in utilities.gen_batches(\n                shuff, self.batch_size)]\n\n            for batch in batches:\n                x_batch, y_batch = zip(*batch)\n                self.tf_session.run(\n                    self.train_step,\n                    feed_dict={self.input_data: x_batch,\n                               self.input_labels: y_batch,\n                               self.keep_prob: self.dropout})\n\n            if validation_set is not None:\n                feed = {self.input_data: validation_set,\n                        self.input_labels: validation_labels,\n                        self.keep_prob: 1}\n                acc = tf_utils.run_summaries(\n                    self.tf_session, self.tf_merged_summaries,\n                    self.tf_summary_writer, i, feed, self.accuracy)\n                pbar.set_description(""Accuracy: %s"" % (acc))\n\n    def build_model(self, n_features, n_classes):\n        """"""Create the computational graph of the model.\n\n        :param n_features: Number of features.\n        :param n_classes: number of classes.\n        :return: self\n        """"""\n        self._create_placeholders(n_features, n_classes)\n        self._create_layers(n_classes)\n\n        self.cost = self.loss.compile(self.mod_y, self.input_labels)\n        self.train_step = self.trainer.compile(self.cost)\n        self.accuracy = Evaluation.accuracy(self.mod_y, self.input_labels)\n\n    def _create_placeholders(self, n_features, n_classes):\n        """"""Create the TensorFlow placeholders for the model.\n\n        :param n_features: number of features of the first layer\n        :param n_classes: number of classes\n        :return: self\n        """"""\n        self.input_data = tf.placeholder(\n            tf.float32, [None, n_features], name=\'x-input\')\n        self.input_labels = tf.placeholder(\n            tf.float32, [None, n_classes], name=\'y-input\')\n        self.keep_prob = tf.placeholder(\n            tf.float32, name=\'keep-probs\')\n\n    def _create_layers(self, n_classes):\n        """"""Create the layers of the model from self.layers.\n\n        :param n_classes: number of classes\n        :return: self\n        """"""\n        next_layer_feed = tf.reshape(self.input_data,\n                                     [-1, self.original_shape[0],\n                                      self.original_shape[1],\n                                      self.original_shape[2]])\n        prev_output_dim = self.original_shape[2]\n        # this flags indicates whether we are building the first dense layer\n        first_full = True\n\n        self.W_vars = []\n        self.B_vars = []\n\n        for i, l in enumerate(self.layers.split(\',\')):\n\n            node = l.split(\'-\')\n            node_type = node[0]\n\n            if node_type == \'conv2d\':\n\n                # ################### #\n                # Convolutional Layer #\n                # ################### #\n\n                # fx, fy = shape of the convolutional filter\n                # feature_maps = number of output dimensions\n                fx, fy, feature_maps, stride = int(node[1]),\\\n                     int(node[2]), int(node[3]), int(node[4])\n\n                print(\'Building Convolutional layer with %d input channels\\\n                      and %d %dx%d filters with stride %d\' %\n                      (prev_output_dim, feature_maps, fx, fy, stride))\n\n                # Create weights and biases\n                W_conv = self.weight_variable(\n                    [fx, fy, prev_output_dim, feature_maps])\n                b_conv = self.bias_variable([feature_maps])\n                self.W_vars.append(W_conv)\n                self.B_vars.append(b_conv)\n\n                # Convolution and Activation function\n                h_conv = tf.nn.relu(\n                    self.conv2d(next_layer_feed, W_conv, stride) + b_conv)\n\n                # keep track of the number of output dims of the previous layer\n                prev_output_dim = feature_maps\n                # output node of the last layer\n                next_layer_feed = h_conv\n\n            elif node_type == \'maxpool\':\n\n                # ################# #\n                # Max Pooling Layer #\n                # ################# #\n\n                ksize = int(node[1])\n\n                print(\'Building Max Pooling layer with size %d\' % ksize)\n\n                next_layer_feed = self.max_pool(next_layer_feed, ksize)\n\n            elif node_type == \'full\':\n\n                # ####################### #\n                # Densely Connected Layer #\n                # ####################### #\n\n                if first_full:  # first fully connected layer\n\n                    dim = int(node[1])\n                    shp = next_layer_feed.get_shape()\n                    tmpx = shp[1].value\n                    tmpy = shp[2].value\n                    fanin = tmpx * tmpy * prev_output_dim\n\n                    print(\'Building fully connected layer with %d in units\\\n                          and %d out units\' % (fanin, dim))\n\n                    W_fc = self.weight_variable([fanin, dim])\n                    b_fc = self.bias_variable([dim])\n                    self.W_vars.append(W_fc)\n                    self.B_vars.append(b_fc)\n\n                    h_pool_flat = tf.reshape(next_layer_feed, [-1, fanin])\n                    h_fc = tf.nn.relu(tf.add(\n                        tf.matmul(h_pool_flat, W_fc),\n                        b_fc))\n                    h_fc_drop = tf.nn.dropout(h_fc, self.keep_prob)\n\n                    prev_output_dim = dim\n                    next_layer_feed = h_fc_drop\n\n                    first_full = False\n\n                else:  # not first fully connected layer\n\n                    dim = int(node[1])\n                    W_fc = self.weight_variable([prev_output_dim, dim])\n                    b_fc = self.bias_variable([dim])\n                    self.W_vars.append(W_fc)\n                    self.B_vars.append(b_fc)\n\n                    h_fc = tf.nn.relu(tf.add(\n                        tf.matmul(next_layer_feed, W_fc), b_fc))\n                    h_fc_drop = tf.nn.dropout(h_fc, self.keep_prob)\n\n                    prev_output_dim = dim\n                    next_layer_feed = h_fc_drop\n\n            elif node_type == \'softmax\':\n\n                # ############# #\n                # Softmax Layer #\n                # ############# #\n\n                print(\'Building softmax layer with %d in units and\\\n                      %d out units\' % (prev_output_dim, n_classes))\n\n                W_sm = self.weight_variable([prev_output_dim, n_classes])\n                b_sm = self.bias_variable([n_classes])\n                self.W_vars.append(W_sm)\n                self.B_vars.append(b_sm)\n\n                self.mod_y = tf.add(tf.matmul(next_layer_feed, W_sm), b_sm)\n\n    @staticmethod\n    def weight_variable(shape):\n        """"""Create a weight variable.""""""\n        initial = tf.truncated_normal(shape=shape, stddev=0.1)\n        return tf.Variable(initial)\n\n    @staticmethod\n    def bias_variable(shape):\n        """"""Create a bias variable.""""""\n        initial = tf.constant(0.1, shape=shape)\n        return tf.Variable(initial)\n\n    @staticmethod\n    def conv2d(x, W, stride):\n        """"""2D Convolution operation.""""""\n        return tf.nn.conv2d(\n            x, W, strides=[1, stride, stride, 1], padding=\'SAME\')\n\n    @staticmethod\n    def max_pool(x, dim):\n        """"""Max pooling operation.""""""\n        return tf.nn.max_pool(\n            x, ksize=[1, dim, dim, 1], strides=[1, dim, dim, 1],\n            padding=\'SAME\')\n'"
yadlt/models/linear/__init__.py,0,"b'""""""Linear Models package.""""""\n\nfrom __future__ import absolute_import\n'"
yadlt/models/linear/logistic_regression.py,13,"b'""""""Softmax classifier implementation using Tensorflow.""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import tqdm\n\nfrom yadlt.core import Evaluation, Loss\nfrom yadlt.core import SupervisedModel\nfrom yadlt.utils import tf_utils, utilities\n\n\nclass LogisticRegression(SupervisedModel):\n    """"""Simple Logistic Regression using TensorFlow.\n\n    The interface of the class is sklearn-like.\n    """"""\n\n    def __init__(self, name=\'lr\', loss_func=\'cross_entropy\',\n                 learning_rate=0.01, num_epochs=10, batch_size=10):\n        """"""Constructor.""""""\n        SupervisedModel.__init__(self, name)\n\n        self.loss_func = loss_func\n        self.learning_rate = learning_rate\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n\n        self.loss = Loss(self.loss_func)\n\n        # Computational graph nodes\n        self.input_data = None\n        self.input_labels = None\n\n        self.W_ = None\n        self.b_ = None\n\n        self.accuracy = None\n\n    def build_model(self, n_features, n_classes):\n        """"""Create the computational graph.\n\n        :param n_features: number of features\n        :param n_classes: number of classes\n        :return: self\n        """"""\n        self._create_placeholders(n_features, n_classes)\n        self._create_variables(n_features, n_classes)\n\n        self.mod_y = tf.nn.softmax(\n            tf.add(tf.matmul(self.input_data, self.W_), self.b_))\n\n        self.cost = self.loss.compile(self.mod_y, self.input_labels)\n        self.train_step = tf.train.GradientDescentOptimizer(\n            self.learning_rate).minimize(self.cost)\n        self.accuracy = Evaluation.accuracy(self.mod_y, self.input_labels)\n\n    def _create_placeholders(self, n_features, n_classes):\n        """"""Create the TensorFlow placeholders for the model.\n\n        :param n_features: number of features\n        :param n_classes: number of classes\n        :return: self\n        """"""\n        self.input_data = tf.placeholder(\n            tf.float32, [None, n_features], name=\'x-input\')\n        self.input_labels = tf.placeholder(\n            tf.float32, [None, n_classes], name=\'y-input\')\n        self.keep_prob = tf.placeholder(\n            tf.float32, name=\'keep-probs\')\n\n    def _create_variables(self, n_features, n_classes):\n        """"""Create the TensorFlow variables for the model.\n\n        :param n_features: number of features\n        :param n_classes: number of classes\n        :return: self\n        """"""\n        self.W_ = tf.Variable(\n            tf.zeros([n_features, n_classes]), name=\'weights\')\n        self.b_ = tf.Variable(\n            tf.zeros([n_classes]), name=\'biases\')\n\n    def _train_model(self, train_set, train_labels,\n                     validation_set, validation_labels):\n        """"""Train the model.\n\n        :param train_set: training set\n        :param train_labels: training labels\n        :param validation_set: validation set\n        :param validation_labels: validation labels\n        :return: self\n        """"""\n        pbar = tqdm(range(self.num_epochs))\n        for i in pbar:\n\n            shuff = list(zip(train_set, train_labels))\n            np.random.shuffle(shuff)\n\n            batches = [_ for _ in utilities.gen_batches(shuff, self.batch_size)]\n\n            for batch in batches:\n                x_batch, y_batch = zip(*batch)\n                self.tf_session.run(\n                    self.train_step,\n                    feed_dict={self.input_data: x_batch,\n                               self.input_labels: y_batch})\n\n            if validation_set is not None:\n                feed = {self.input_data: validation_set,\n                        self.input_labels: validation_labels}\n                acc = tf_utils.run_summaries(\n                    self.tf_session, self.tf_merged_summaries,\n                    self.tf_summary_writer, i, feed, self.accuracy)\n                pbar.set_description(""Accuracy: %s"" % (acc))\n'"
yadlt/models/recurrent/__init__.py,0,"b'""""""Recurrent Neural Network Models package.""""""\n\nfrom __future__ import absolute_import\n'"
yadlt/models/recurrent/lstm.py,31,"b'""""""LSTM Tensorflow implementation.""""""\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom yadlt.core import Model\nfrom yadlt.utils import utilities\n\n\nclass LSTM(Model):\n    """"""Long Short-Term Memory Network tensorflow implementation.\n\n    The interface of the class is sklearn-like.\n    """"""\n\n    def __init__(self, num_layers=2, num_hidden=200, vocab_size=10000,\n                 batch_size=20, num_steps=35, num_epochs=10, learning_rate=1.0,\n                 dropout=0.5, init_scale=0.05, max_grad_norm=5,\n                 lr_decay=0.8):\n        """"""Constructor.\n\n        :param num_layers: number of LSTM layers\n        :param num_hidden: number of LSTM units\n        :param vocab_size: size of the vocabulary\n        :param batch_size: size of each mini batch\n        :param num_steps: number of unrolled steps of LSTM\n        :param num_epochs: number of training epochs\n        :param learning_rate: learning rate parameter\n        :param dropout: probability of the dropout layer\n        :param init_scale: initial scale of the weights\n        :param max_grad_norm: maximum permissible norm of the gradient\n        :param lr_decay: learning rate decay for each epoch after num_epochs/3\n        """"""\n        self.num_layers = num_layers\n        self.num_hidden = num_hidden\n        self.vocab_size = vocab_size\n        self.batch_size = batch_size\n        self.num_steps = num_steps\n        self.num_epochs = num_epochs\n        self.learning_rate = learning_rate\n        self.dropout = dropout\n        self.init_scale = init_scale\n        self.max_grad_norm = max_grad_norm\n        self.lr_decay = lr_decay\n\n        self.initializer = tf.random_uniform_initializer(\n            -self.init_scale, self.init_scale)\n\n    def fit(self, train_set, test_set):\n        """"""Fit the model to the given data.\n\n        :param train_set: training data\n        :param test_set: test data\n        """"""\n        with tf.Graph().as_default(), tf.Session() as self.tf_session:\n            self.build_model()\n            tf.global_variables_initializer().run()\n            third = self.num_epochs // 3\n\n            for i in range(self.num_epochs):\n                lr_decay = self.lr_decay ** max(i - third, 0.0)\n                self.tf_session.run(\n                    tf.assign(self.lr_var, tf.multiply(self.learning_rate, lr_decay)))\n\n                train_perplexity = self._run_train_step(train_set, \'train\')\n                print(""Epoch: %d Train Perplexity: %.3f""\n                      % (i + 1, train_perplexity))\n\n            test_perplexity = self._run_train_step(test_set, \'test\')\n            print(""Test Perplexity: %.3f"" % test_perplexity)\n\n    def _run_train_step(self, data, mode=\'train\'):\n        """"""Run a single training step.\n\n        :param data: input data\n        :param mode: \'train\' or \'test\'.\n        """"""\n        epoch_size = ((len(data) // self.batch_size) - 1) // self.num_steps\n        costs = 0.0\n        iters = 0\n        step = 0\n        state = self._init_state.eval()\n        op = self._train_op if mode == \'train\' else tf.no_op()\n\n        for step, (x, y) in enumerate(\n            utilities.seq_data_iterator(\n                data, self.batch_size, self.num_steps)):\n            cost, state, _ = self.tf_session.run(\n                [self.cost, self.final_state, op],\n                {self.input_data: x,\n                 self.input_labels: y,\n                 self._init_state: state})\n\n            costs += cost\n            iters += self.num_steps\n\n        if step % (epoch_size // 10) == 10:\n            print(""%.3f perplexity"" % (step * 1.0 / epoch_size))\n\n        return np.exp(costs / iters)\n\n    def build_model(self):\n        """"""Build the model\'s computational graph.""""""\n        with tf.variable_scope(\n                ""model"", reuse=None, initializer=self.initializer):\n            self._create_placeholders()\n            self._create_rnn_cells()\n            self._create_initstate_and_embeddings()\n            self._create_rnn_architecture()\n            self._create_optimizer_node()\n\n    def _create_placeholders(self):\n        """"""Create the computational graph\'s placeholders.""""""\n        self.input_data = tf.placeholder(\n            tf.int32, [self.batch_size, self.num_steps])\n        self.input_labels = tf.placeholder(\n            tf.int32, [self.batch_size, self.num_steps])\n\n    def _create_rnn_cells(self):\n        """"""Create the LSTM cells.""""""\n        lstm_cell = tf.nn.rnn_cell.LSTMCell(\n            self.num_hidden, forget_bias=0.0)\n        lstm_cell = tf.nn.rnn_cell.DropoutWrapper(\n            lstm_cell, output_keep_prob=self.dropout)\n        self.cell = tf.nn.rnn_cell.MultiRNNCell(\n            [lstm_cell] * self.num_layers)\n\n    def _create_initstate_and_embeddings(self):\n        """"""Create the initial state for the cell and the data embeddings.""""""\n        self._init_state = self.cell.zero_state(self.batch_size, tf.float32)\n        embedding = tf.get_variable(\n            ""embedding"", [self.vocab_size, self.num_hidden])\n        inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n        self.inputs = tf.nn.dropout(inputs, self.dropout)\n\n    def _create_rnn_architecture(self):\n        """"""Create the training architecture and the last layer of the LSTM.""""""\n        self.inputs = [tf.squeeze(i, [1]) for i in tf.split(\n            axis=1, num_or_size_splits=self.num_steps, value=self.inputs)]\n        outputs, state = tf.nn.rnn(\n            self.cell, self.inputs, initial_state=self._init_state)\n\n        output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, self.num_hidden])\n        softmax_w = tf.get_variable(\n            ""softmax_w"", [self.num_hidden, self.vocab_size])\n        softmax_b = tf.get_variable(""softmax_b"", [self.vocab_size])\n        logits = tf.add(tf.matmul(output, softmax_w), softmax_b)\n        loss = tf.nn.seq2seq.sequence_loss_by_example(\n            [logits],\n            [tf.reshape(self.input_labels, [-1])],\n            [tf.ones([self.batch_size * self.num_steps])])\n\n        self.cost = tf.div(tf.reduce_sum(loss), self.batch_size)\n        self.final_state = state\n\n    def _create_optimizer_node(self):\n        """"""Create the optimizer node of the graph.""""""\n        self.lr_var = tf.Variable(0.0, trainable=False)\n        tvars = tf.trainable_variables()\n        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),\n                                          self.max_grad_norm)\n        optimizer = tf.train.GradientDescentOptimizer(self.lr_var)\n        self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n'"
