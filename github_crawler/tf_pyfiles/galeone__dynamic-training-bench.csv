file_path,api_count,code
setup.py,0,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Setup file to make dytb installable via pip""""""\n\nimport io\nimport re\nfrom setuptools import setup\nfrom setuptools import find_packages\n\nINIT_PY = io.open(\'dytb/__init__.py\').read()\nMETADATA = dict(re.findall(""__([a-z]+)__ = \'([^\']+)\'"", INIT_PY))\nMETADATA[\'doc\'] = re.findall(\'""""""(.+)""""""\', INIT_PY)[0]\n\nsetup(\n    name=\'dytb\',\n    version=METADATA[\'version\'],\n    description=METADATA[\'doc\'],\n    author=METADATA[\'author\'],\n    author_email=METADATA[\'email\'],\n    url=METADATA[\'url\'],\n    download_url=\'/\'.join((METADATA[\'url\'].rstrip(\'/\'), \'tarball\',\n                           METADATA[\'version\'])),\n    license=\'MPL\',\n    scripts=[\'scripts/dytb_evaluate\', \'scripts/dytb_train\'],\n    packages=find_packages())\n'"
dytb/__init__.py,0,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Simplify the trainining and tuning of Tensorflow models""""""\n\nfrom . import inputs\nfrom . import models\n\n__version__ = \'0.7.4\'\n__url__ = \'https://github.com/galeone/dynamic-training-bench\'\n__author__ = \'Paolo Galeone\'\n__email__ = \'nessuno@nerdz.eu\'\n'"
dytb/evaluate.py,0,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Evaluation method and utilities""""""\n\nfrom .inputs.interfaces import InputType\n\n\ndef evaluate(metric,\n             checkpoint_path,\n             model,\n             dataset,\n             input_type,\n             batch_size,\n             augmentation_fn=None):\n    """"""Eval the model, restoring weight found in checkpoint_path, using the dataset.\n    Args:\n        metric: the metric to evaluate. The usual dictionary with the fn and its properties\n        checkpoint_path: path of the trained model checkpoint directory\n        model: implementation of the Model interface\n        dataset: implementation of the Input interface\n        input_type: InputType enum\n        batch_size: evaluate in batch of size batch_size\n        augmentation_fn: if present, applies the augmentation to the input data\n    Returns:\n        value: scalar value representing the evaluation of the model,\n               on the dataset, fetching values of the specified input_type\n    """"""\n    InputType.check(input_type)\n    model.evaluator.dataset = dataset\n    return model.evaluator.eval(metric, checkpoint_path, input_type, batch_size,\n                                augmentation_fn)\n\n\ndef stats(checkpoint_path, model, dataset, batch_size, augmentation_fn=None):\n    """"""Eval the model, restoring weight found in checkpoint_path, using the dataset.\n    Args:\n        checkpoint_path: path of the trained model checkpoint directory\n        model: implementation of the Model interface\n        dataset: implementation of the Input interface\n        batch_size: evaluate in batch of size batch_size\n        augmentation_fn: if present, applies the augmentation to the input data\n    Returns:\n        dict: a dictionary with the statistics measured\n    """"""\n    model.evaluator.dataset = dataset\n    return model.evaluator.stats(checkpoint_path, batch_size, augmentation_fn)\n'"
dytb/train.py,7,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Train method and utilities""""""\n\nimport os\nimport tensorflow as tf\nfrom .inputs.interfaces import InputType\nfrom .trainer.Trainer import Trainer\n\n\ndef _build_name(args, dataset):\n    """"""Build method name parsing args.\n    Args:\n        args: the training parameter\n        dataset: the dataset object\n    Returns:\n        name: the ID for the current training process""""""\n    optimizer = args[""gd""][""optimizer""](**args[""gd""][""args""])\n    name = ""{}_{}_"".format(dataset.name, optimizer.get_name())\n\n    if args[""lr_decay""][""enabled""]:\n        name += ""lr_decay_""\n    if args[""regularizations""][""l2""]:\n        name += ""l2={}_"".format(args[""regularizations""][""l2""])\n    if args[""regularizations""][""augmentation""][""name""].lower() != ""identity"":\n        name += ""{}_"".format(\n            args[""regularizations""][""augmentation""][""name""].lower())\n    if args[""comment""] != """":\n        name += ""{}_"".format(args[""comment""])\n\n    return name.rstrip(""_"")\n\n\ndef _parse_hyperparameters(hyperparams=None):\n    """"""Check if every parameter passed in hyperparams\n    is a valid hyperparameter.\n    Returns:\n        hyperparams: the same dictionary with default values added if optionals\n    Raises:\n        ValueError if hyperparams is not valid\n    """"""\n\n    if hyperparams is None:\n        hyperparams = {}\n\n    hp_available_keys = {\n        ""batch_size"", ""epochs"", ""gd"", ""lr_decay"", ""regularizations"", ""seed""\n    }\n\n    difference = hyperparams.keys() - hp_available_keys\n    if difference:\n        raise ValueError(\n            ""{} are not valid keys for {}. Valid keys are: {}"".format(\n                difference, ""hyperparameters"", hp_available_keys))\n\n    # Instantiate with default values if not specified\n    args = {\n        # The size of the trainign batch\n        ""batch_size"":\n        hyperparams.get(""batch_size"", 128),\n        # The number of epochs to train\n        # where an epoch is the training set cardinality * the augmentation factor\n        ""epochs"":\n        hyperparams.get(""epochs"", 150),\n        # Gradient descent parameters\n        ""gd"":\n        hyperparams.get(\n            ""gd"",\n            {\n                # The optimizer to use\n                ""optimizer"": tf.train.MomentumOptimizer,\n                # The arguments of the optimizer\n                ""args"": {\n                    ""learning_rate"": 1e-3,\n                    ""momentum"": 0.9,\n                    ""use_nesterov"": False\n                }\n            }),\n        # The learning rate decay\n        ""lr_decay"":\n        hyperparams.get(""lr_decay"", {\n            ""enabled"": False,\n            ""epochs"": 25,\n            ""factor"": .1\n        }),\n        # The regularization to apply\n        ""regularizations"":\n        hyperparams.get(\n            ""regularizations"",\n            {\n                # L2 on the model weights\n                ""l2"": 0.0,\n                # The augmentation on the input data: online augmentation\n                ""augmentation"": {\n                    # The name of the augmentation: identity disables the augmentations\n                    ""name"": ""identity"",\n                    # The function of the augmentation: fn(x) where x is the orignnal sample\n                    ""fn"": lambda x: x,\n                    # The multiplicative factor of the training set: online data augmentation\n                    # can generate a potentially infinite number of training samples.\n                    # However, the generated samples starts to look ""similar"" after\n                    # being generated for a lot of times.\n                    # What we do applying augmentations is to pick samples from the input\n                    # distrubution.\n                    # If we have enough samples (in a single epoch), we have sampled the\n                    # distribution densely enough that the next epoch, altough the samples\n                    # are still online generated, will look similar to the previous one.\n\n                    # In short, this is a multiplicative factor that changes the effective\n                    # training set size:\n                    # 1 means no augmentation.\n                    # A rule of thumb is to set this value to a power of 10.\n                    ""factor"": 1,\n                }\n            }),\n        # seed is the graph level and op level seed.\n        # None means that random seed is used.\n        # Otherwise the specified value is used.\n        ""seed"":\n        hyperparams.get(""seed"", None),\n    }\n\n    def _check_keys(dict_key, available_keys, sub_key=None):\n        inserted_keys = args[dict_key] if not sub_key else args[dict_key][\n            sub_key]\n        diff = inserted_keys.keys() - available_keys\n        if diff:\n            raise ValueError(\n                ""{} are not valid keys for {}. Valid keys are: {}"".format(\n                    diff, dict_key, available_keys))\n\n    _check_keys(""gd"", {""optimizer"", ""args""})\n    _check_keys(""lr_decay"", {""enabled"", ""epochs"", ""factor""})\n    _check_keys(""regularizations"", {""l2"", ""augmentation""})\n    _check_keys(""regularizations"", {""name"", ""fn"", ""factor""}, ""augmentation"")\n\n    # Check numeric fields\n    if args[""epochs""] <= 0:\n        raise ValueError(""epochs <= 0"")\n    if args[""batch_size""] <= 0:\n        raise ValueError(""batch_size <= 0"")\n    # The other fields will be used at runtime.\n    # If they\'re wrong, the training process can\'t start\n    # and tensorflow will raise errors\n    return args\n\n\ndef _parse_surgery(surgery=None):\n    """"""Check if every parameter passed in surgery is valid\n    for network surgery purposes.\n\n    Returns:\n        surgery: the same dictionary with defautl values added if needed\n    Raises:\n        ValueError if surgery values are not valid\n    """"""\n    if surgery is None:\n        surgery = {}\n\n    args = {\n        ""checkpoint_path"": surgery.get(""checkpoint_path"", """"),\n        ""exclude_scopes"": surgery.get(""exclude_scopes"", None),\n        ""trainable_scopes"": surgery.get(""trainable_scopes"", None),\n    }\n\n    if args[""checkpoint_path""] != """":\n        if not tf.train.latest_checkpoint(args[""checkpoint_path""]):\n            raise ValueError(""Invalid {}"".format(args[""checkpoint_path""]))\n    # The other fields will be used at runtime.\n    # If they\'re wrong, the training process can\'t start\n    # and tensorflow will raise errors\n    return args\n\n\ndef train(model,\n          dataset,\n          hyperparameters=None,\n          surgery=None,\n          force_restart=False,\n          comment=""""):\n    """"""Train the model using the provided dataset and the specifiied hyperparameters.\n    Args:\n        model: instance of a model interface\n        dataset: instance of the Input interface\n        hyperparameters: dictionary of the hyperparameter to use to train the model\n        surgery: dictionary of options related to the network surgery, fine tuning and transfer\n                 learning\n        force_restart: boolean, indicates if restart the train from 0 removing the old model\n                       or continue the training.\n        comment: string to append at the log dir name\n    Returns:\n        info dict containing the information of the trained model\n    """"""\n    hyperparameters = _parse_hyperparameters(hyperparameters)\n    surgery = _parse_surgery(surgery)\n    args = {\n        **hyperparameters,\n        **surgery,\n        ""force_restart"": force_restart,\n        ""model"": model,\n        ""dataset"": dataset,\n        ""comment"": comment}\n\n    name = _build_name(args, dataset)\n\n    #### Training constants ####\n    float_steps_per_epoch = dataset.num_examples(InputType.train) * args[\n        ""regularizations""][""augmentation""][""factor""] / args[""batch_size""]\n    steps_per_epoch = 1 if float_steps_per_epoch < 1. else round(\n        float_steps_per_epoch)\n\n    steps = {\n        ""epoch"": steps_per_epoch,\n        ""log"": 1 if steps_per_epoch < 10 else steps_per_epoch // 10,\n        ""max"": int(float_steps_per_epoch * args[""epochs""]),\n        ""decay"": int(float_steps_per_epoch * args[""lr_decay""][""epochs""]),\n    }\n\n    #### Model logs and checkpoint constants ####\n    current_dir = os.getcwd()\n    log_dir = os.path.join(current_dir, ""log"", args[""model""].name, name)\n    best_dir = os.path.join(log_dir, ""best"")\n    paths = {""current"": current_dir, ""log"": log_dir, ""best"": best_dir}\n\n    if tf.gfile.Exists(log_dir) and force_restart:\n        tf.gfile.DeleteRecursively(log_dir)\n    tf.gfile.MakeDirs(log_dir)\n    if not tf.gfile.Exists(best_dir):\n        tf.gfile.MakeDirs(best_dir)\n\n    if args[""regularizations""][""augmentation""][""factor""] != 1:\n        print(""Original training set size {}. Augmented training set size: {}"".\n              format(\n                  dataset.num_examples(InputType.train),\n                  args[""regularizations""][""augmentation""][""factor""] *\n                  dataset.num_examples(InputType.train)))\n    return Trainer(model, dataset, args, steps, paths).train()\n'"
tests/extract_features.py,2,"b'import unittest\nimport tensorflow as tf\n\nfrom dytb.models.predefined.VGG import VGG\nfrom dytb.inputs.images import read_image\n\n\nclass TestFeatureExtractors(unittest.TestCase):\n\n    def test_classifier(self):\n        model = VGG()\n        image = tf.image.resize_bilinear(\n            tf.expand_dims(\n                read_image(""images/nocat.png"", channel=3, image_type=""png""),\n                axis=0), (32, 32))\n        features = model.evaluator.extract_features(\n            checkpoint_path=""../log/VGG/CIFAR-10_Momentum/best/"",\n            inputs=image,\n            layer_name=""VGG/pool1/MaxPool:0"",\n            num_classes=10)\n        self.assertEqual(features.shape, (1, 16, 16, 64))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
dytb/evaluators/AutoencoderEvaluator.py,0,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n"""""" Evaluate Autoencoding models """"""\n\nfrom .Evaluator import Evaluator\n\n\nclass AutoencoderEvaluator(Evaluator):\n    """"""AutoencoderEvaluator is the evaluation object for a Autoencoder model""""""\n\n    @property\n    def metrics(self):\n        """"""Returns a list of dict with keys:\n        {\n            ""fn"": function\n            ""name"": name\n            ""positive_trend_sign"": sign that we like to see when things go well\n            ""model_selection"": boolean, True if the metric has to be measured to select the model\n            ""average"": boolean, true if the metric should be computed as average over the batches.\n                       If false the results over the batches are just added\n            ""tensorboard"": boolean. True if the metric is a scalar and can be logged in tensoboard\n        }\n        """"""\n        return [{\n            ""fn"": self._model.loss,\n            ""name"": ""error"",\n            ""positive_trend_sign"": -1,\n            ""model_selection"": True,\n            ""average"": True,\n            ""tensorboard"": True,\n        }]\n'"
dytb/evaluators/ClassifierEvaluator.py,0,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n"""""" Evaluate Classification models """"""\n\nfrom .Evaluator import Evaluator\nfrom .metrics import accuracy_op, confusion_matrix_op\n\n\nclass ClassifierEvaluator(Evaluator):\n    """"""ClassifierEvaluator is the evaluation object for a Classifier model""""""\n\n    @property\n    def metrics(self):\n        """"""Returns a list of dict with keys:\n        {\n            ""fn"": function\n            ""name"": name\n            ""positive_trend_sign"": sign that we like to see when things go well\n            ""model_selection"": boolean, True if the metric has to be measured to select the model\n            ""average"": boolean, true if the metric should be computed as average over the batches.\n                       If false the results over the batches are just added\n            ""tensorboard"": boolean. True if the metric is a scalar and can be logged in tensoboard\n        }\n        """"""\n        return [{\n            ""fn"": accuracy_op,\n            ""name"": ""accuracy"",\n            ""positive_trend_sign"": +1,\n            ""model_selection"": True,\n            ""average"": True,\n            ""tensorboard"": True,\n        }, {\n            ""fn"":\n            lambda logits, labels: confusion_matrix_op(logits, labels, self.dataset.num_classes),\n            ""name"":\n            ""confusion_matrix"",\n            ""positive_trend_sign"":\n            0,\n            ""model_selection"":\n            False,\n            ""average"":\n            False,\n            ""tensorboard"":\n            False,\n        }]\n'"
dytb/evaluators/DetectorEvaluator.py,0,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n"""""" Evaluate Detection models """"""\n\nfrom .Evaluator import Evaluator\nfrom .metrics import iou_op\n\n\nclass DetectorEvaluator(Evaluator):\n    """"""DetectorEvaluator is the evaluation object for a Detector model""""""\n\n    @property\n    def metrics(self):\n        """"""Returns a list of dict with keys:\n        {\n            ""fn"": function\n            ""name"": name\n            ""positive_trend_sign"": sign that we like to see when things go well\n            ""model_selection"": boolean, True if the metric has to be measured to select the model\n            ""average"": boolean, true if the metric should be computed as average over the batches.\n                       If false the results over the batches are just added\n            ""tensorboard"": boolean. True if the metric is a scalar and can be logged in tensoboard\n        }\n        """"""\n        return [{\n            ""fn"": iou_op,\n            ""name"": ""IoU"",\n            ""positive_trend_sign"": +1,\n            ""model_selection"": True,\n            ""average"": True,\n            ""tensorboard"": True,\n        }]\n'"
dytb/evaluators/Evaluator.py,31,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Define the interface to implement to define an evaluator""""""\n\nimport math\nfrom abc import abstractproperty, ABCMeta\nimport numpy as np\nimport tensorflow as tf\nfrom ..inputs.interfaces import InputType\nfrom ..models.utils import variables_to_restore\n\n\nclass Evaluator(object, metaclass=ABCMeta):\n    """"""Evaluator is the class in charge of evaluate the models""""""\n\n    def __init__(self):\n        self._model = None\n        self._dataset = None\n        self._visualizations = []\n\n    @property\n    def model(self):\n        """"""Returns the model to evaluate""""""\n        return self._model\n\n    @model.setter\n    def model(self, model):\n        """"""Set the model to evaluate.\n        Args:\n            model: implementation of the Model interface\n        """"""\n        self._model = model\n\n    @property\n    def dataset(self):\n        """"""Returns the dataset to use to evaluate the model""""""\n        return self._dataset\n\n    @dataset.setter\n    def dataset(self, dataset):\n        """"""Set the dataset to use to evaluate the model\n        Args:\n            dataset: implementation of the Input interface\n        """"""\n        self._dataset = dataset\n\n    @property\n    def visualizations(self):\n        """"""Returns a list of dict with keys:\n        {\n            ""fn"": function(inputs, predictions, targets) that returns an image\n            ""name"": name\n        }\n        """"""\n        return self._visualizations\n\n    @visualizations.setter\n    def visualizations(self, visualizations):\n        """"""Set the visualization list to disply\n        Args:\n            visualizations: the list of visualizations\n        """"""\n        self._visualizations = visualizations\n\n    @abstractproperty\n    def metrics(self):\n        """"""Returns a list of dict with keys:\n        {\n            ""fn"": function(predictions, targets)\n            ""name"": name\n            ""positive_trend_sign"": sign that we like to see when things go well\n            ""model_selection"": boolean, True if the metric has to be measured to select the model\n            ""average"": boolean, true if the metric should be computed as average over the batches.\n                       If false the results over the batches are just added\n            ""tensorboard"": boolean. True if the metric is a scalar and can be logged in tensoboard\n        }\n        """"""\n\n    def eval(self,\n             metric,\n             checkpoint_path,\n             input_type,\n             batch_size,\n             augmentation_fn=None):\n        """"""Eval the model, restoring weight found in checkpoint_path, using the dataset.\n        Args:\n            metric: the metric to evaluate, a single element of self.metrics\n            checkpoint_path: path of the trained model checkpoint directory\n            input_type: InputType enum\n            batch_size: evaluate in batch of size batch_size\n            augmentation_fn: if present, applies the augmentation to the input data\n\n        Returns:\n            value: scalar value representing the evaluation of the metric on the restored model\n                   on the dataset, fetching values of the specified input_type.\n        """"""\n        InputType.check(input_type)\n\n        with tf.Graph().as_default():\n            # Get inputs and targets: inputs is an input batch\n            # target could be either an array of elements or a tensor.\n            # it could be [label] or [label, attr1, attr2, ...]\n            # or Tensor, where tensor is a standard tensorflow Tensor with\n            # its own shape\n            with tf.device(\'/cpu:0\'):\n                inputs, *targets = self.dataset.inputs(\n                    input_type=input_type,\n                    batch_size=batch_size,\n                    augmentation_fn=augmentation_fn)\n\n            # Build a Graph that computes the predictions from the\n            # inference model.\n            # Preditions is an array of predictions with the same cardinality of\n            # targets\n            _, *predictions = self._model.get(\n                inputs,\n                self.dataset.num_classes,\n                train_phase=False,\n                l2_penalty=0.0)\n\n            if len(predictions) != len(targets):\n                print((""{}.get 2nd return value and {}.inputs 2nd return ""\n                       ""value must have the same cardinality but got: {} vs {}""\n                      ).format(self._model.name, self.dataset.name,\n                               len(predictions), len(targets)))\n                return\n\n            if len(predictions) == 1:\n                predictions = predictions[0]\n                targets = targets[0]\n\n            metric_fn = metric[""fn""](predictions, targets)\n\n            saver = tf.train.Saver(variables_to_restore())\n            init = [\n                tf.variables_initializer(\n                    tf.global_variables() + tf.local_variables()),\n                tf.tables_initializer()\n            ]\n            with tf.Session(config=tf.ConfigProto(\n                    allow_soft_placement=True)) as sess:\n                ckpt = tf.train.get_checkpoint_state(checkpoint_path)\n                if ckpt and ckpt.model_checkpoint_path:\n                    saver.restore(sess, ckpt.model_checkpoint_path)\n                else:\n                    print(\'[!] No checkpoint file found\')\n                    sign = math.copysign(1, metric[""positive_trend_sign""])\n                    return float(\'inf\') if sign < 0 else float(""-inf"")\n\n                # Start the queue runners\n                coord = tf.train.Coordinator()\n                try:\n                    threads = []\n                    for queue_runner in tf.get_collection(\n                            tf.GraphKeys.QUEUE_RUNNERS):\n                        threads.extend(\n                            queue_runner.create_threads(\n                                sess, coord=coord, daemon=True, start=True))\n                    sess.run(init)\n\n                    num_iter = int(\n                        math.ceil(\n                            self.dataset.num_examples(input_type) / batch_size))\n                    step = 0\n                    metric_value_sum = 0.0\n                    while step < num_iter and not coord.should_stop():\n                        step += 1\n                        value = sess.run(metric_fn)\n                        # metrics can sometimes have NaN\n                        # (think about a metric that excludes a certain class and the input batch\n                        # has only element of that class into)\n                        # NaN, not being a number, are excluded from the calculation of the metric\n                        if np.any(np.isnan(value)):\n                            step -= 1\n                        else:\n                            metric_value_sum += value\n                    avg_metric_value = metric_value_sum / step if metric[\n                        ""average""] else metric_value_sum\n                except Exception as exc:\n                    coord.request_stop(exc)\n                finally:\n                    coord.request_stop()\n\n                coord.join(threads)\n            return avg_metric_value\n\n    def stats(self, checkpoint_path, batch_size, augmentation_fn=None):\n        """"""Run the eval method on the model, see eval for arguments\n        and return value description.\n        Moreover, adds informations about the model and returns the whole information\n        in a dictionary.\n        Returns:\n            dict\n        """"""\n        return {\n            ""train"": {\n                metric[""name""]:\n                self.eval(metric, checkpoint_path, InputType.train, batch_size,\n                          augmentation_fn)\n                for metric in self.metrics\n            },\n            ""validation"": {\n                metric[""name""]:\n                self.eval(metric, checkpoint_path, InputType.validation,\n                          batch_size, augmentation_fn)\n                for metric in self.metrics\n            },\n            ""test"": {\n                metric[""name""]:\n                self.eval(metric, checkpoint_path, InputType.test, batch_size,\n                          augmentation_fn)\n                for metric in self.metrics\n            },\n        }\n\n    def visualize(self,\n                  viz,\n                  checkpoint_path,\n                  input_type,\n                  batch_size,\n                  augmentation_fn=None):\n        """"""Restore the model, restoring weight found in checkpoint_path, using the dataset.\n        Execute the function **for a single step**.\n        Args:\n            viz: the function to evaluate, a single element of self.visualizations\n            checkpoint_path: path of the trained model checkpoint directory\n            input_type: InputType enum\n            batch_size: evaluate in batch of size batch_size\n            augmentation_fn: if present, applies the augmentation to the input data\n\n        Returns:\n            image: a numpy batch of images\n        """"""\n        InputType.check(input_type)\n\n        with tf.Graph().as_default():\n            # Get inputs and targets: inputs is an input batch\n            # target could be either an array of elements or a tensor.\n            # it could be [label] or [label, attr1, attr2, ...]\n            # or Tensor, where tensor is a standard tensorflow Tensor with\n            # its own shape\n            with tf.device(\'/cpu:0\'):\n                inputs, *targets = self.dataset.inputs(\n                    input_type=input_type,\n                    batch_size=batch_size,\n                    augmentation_fn=augmentation_fn)\n\n            # Build a Graph that computes the predictions from the\n            # inference model.\n            # Preditions is an array of predictions with the same cardinality of\n            # targets\n            _, *predictions = self._model.get(\n                inputs,\n                self.dataset.num_classes,\n                train_phase=False,\n                l2_penalty=0.0)\n\n            if len(predictions) != len(targets):\n                print((""{}.get 2nd return value and {}.inputs 2nd return ""\n                       ""value must have the same cardinality but got: {} vs {}""\n                      ).format(self._model.name, self.dataset.name,\n                               len(predictions), len(targets)))\n                return\n\n            if len(predictions) == 1:\n                predictions = predictions[0]\n                targets = targets[0]\n\n            saver = tf.train.Saver(variables_to_restore())\n\n            viz_fn = viz[""fn""](inputs, predictions, targets)\n            init = [\n                tf.variables_initializer(\n                    tf.global_variables() + tf.local_variables()),\n                tf.tables_initializer()\n            ]\n            with tf.Session(config=tf.ConfigProto(\n                    allow_soft_placement=True)) as sess:\n                ckpt = tf.train.get_checkpoint_state(checkpoint_path)\n                if ckpt and ckpt.model_checkpoint_path:\n                    saver.restore(sess, ckpt.model_checkpoint_path)\n                else:\n                    print(\'[!] No checkpoint file found\')\n                    return None\n\n                # Start the queue runners\n                coord = tf.train.Coordinator()\n                try:\n                    threads = []\n                    for queue_runner in tf.get_collection(\n                            tf.GraphKeys.QUEUE_RUNNERS):\n                        threads.extend(\n                            queue_runner.create_threads(\n                                sess, coord=coord, daemon=True, start=True))\n\n                    sess.run(init)\n                    return sess.run(viz_fn)\n                except Exception as exc:\n                    coord.request_stop(exc)\n                finally:\n                    coord.request_stop()\n\n                coord.join(threads)\n        return None\n\n    def extract_features(self,\n                         checkpoint_path,\n                         inputs,\n                         layer_name,\n                         num_classes=0):\n        """"""Restore model parameters from checkpoint_path. Search in the model\n        the layer with name `layer_name`. If found places `inputs` as input to the model\n        and returns the values extracted by the layer.\n        Args:\n            checkpoint_path: path of the trained model checkpoint directory\n            inputs: a Tensor with a shape compatible with the model\'s input\n            layer_name: a string, the name of the layer to extract from model\n            num_classes: number of classes to classify, this number must be equal to the number\n            of classes the classifier was trained on, if the model is a classifier or however is\n            a model class aware, otherwise let the number = 0\n        Returns:\n            features: a numpy ndarray that contains the extracted features\n        """"""\n\n        # Evaluate the inputs in the current default graph\n        # then user a placeholder to inject the computed values into the new graph\n        with tf.Session(config=tf.ConfigProto(\n                allow_soft_placement=True)) as sess:\n            evaluated_inputs = sess.run(inputs)\n\n        # Create a new graph to not making dirty the default graph after subsequent\n        # calls\n        with tf.Graph().as_default() as graph:\n            inputs_ = tf.placeholder(inputs.dtype, shape=inputs.shape)\n\n            # Build a Graph that computes the predictions from the inference model.\n            _ = self._model.get(\n                inputs_, num_classes, train_phase=False, l2_penalty=0.0)\n\n            # This will raise an exception if layer_name is not found\n            layer = graph.get_tensor_by_name(layer_name)\n\n            saver = tf.train.Saver(variables_to_restore())\n            init = [\n                tf.variables_initializer(\n                    tf.global_variables() + tf.local_variables()),\n                tf.tables_initializer()\n            ]\n            features = np.zeros(layer.shape)\n            with tf.Session(config=tf.ConfigProto(\n                    allow_soft_placement=True)) as sess:\n                ckpt = tf.train.get_checkpoint_state(checkpoint_path)\n                if ckpt and ckpt.model_checkpoint_path:\n                    # Restores from checkpoint\n                    saver.restore(sess, ckpt.model_checkpoint_path)\n                else:\n                    print(\'[!] No checkpoint file found\')\n                    return features\n                sess.run(init)\n                features = sess.run(\n                    layer, feed_dict={\n                        inputs_: evaluated_inputs\n                    })\n\n            return features\n'"
dytb/evaluators/Metric.py,0,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Utility functions for model training and evaluation""""""\n\n# TODO: understand if metrics can be formalized using an abstract class\n# or something different is better\n\nfrom abc import ABCMeta, abstractproperty\n\n\nclass Metric(object, metaclass=ABCMeta):\n    """"""Metric is a metric to measure and defined by its properties""""""\n\n    @staticmethod\n    @abstractproperty\n    def func(outputs, targets):\n        """"""Metric to measure between outputs and targets""""""\n\n    @staticmethod\n    @abstractproperty\n    def name():\n        """"""Name of the metric""""""\n\n    @staticmethod\n    @abstractproperty\n    def positive_trend_sign():\n        """"""+1 or -1 depending on the expected trend when the\n        metric goes well""""""\n\n    @staticmethod\n    @abstractproperty\n    def model_selection():\n        """"""Boolean: true if the best model should be choosen looking\n        at the trend of this metric""""""\n\n    @staticmethod\n    @abstractproperty\n    def average():\n        """"""Boolean: true if the metric should be averaged over different\n        measures among the dataset. If false the values are added""""""\n\n    @staticmethod\n    @abstractproperty\n    def tensorboard():\n        """"""Boolean: True if the metric should be logged in Tensorboard.\n        The metric should output a scalar to be logged""""""\n'"
dytb/evaluators/RegressorEvaluator.py,0,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n"""""" Evaluate Regression models """"""\n\nfrom .Evaluator import Evaluator\n\n\nclass RegressorEvaluator(Evaluator):\n    """"""RegressorEvaluator is the evaluation object for a Regressor model""""""\n\n    @property\n    def metrics(self):\n        """"""Returns a list of dict with keys:\n        {\n            ""fn"": function\n            ""name"": name\n            ""positive_trend_sign"": sign that we like to see when things go well\n            ""model_selection"": boolean, True if the metric has to be measured to select the model\n            ""average"": boolean, true if the metric should be computed as average over the batches.\n                       If false the results over the batches are just added\n            ""tensorboard"": boolean. True if the metric is a scalar and can be logged in tensoboard\n        }\n        """"""\n        return [{\n            ""fn"": self._model.loss,\n            ""name"": ""error"",\n            ""positive_trend_sign"": -1,\n            ""model_selection"": True,\n            ""average"": True,\n            ""tensorboard"": True,\n        }]\n'"
dytb/evaluators/__init__.py,0,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n'"
dytb/evaluators/metrics.py,18,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Utility functions for model training and evaluation""""""\n\nimport tensorflow as tf\n\n\ndef accuracy_op(logits, labels):\n    """"""Define the accuracy between predictions (logits) and labels.\n    Args:\n        logits: a [batch_size, 1,1, num_classes] tensor or\n                a [batch_size, num_classes] tensor\n        labels: a [batch_size] tensor\n    Returns:\n        accuracy: the accuracy op\n    """"""\n\n    with tf.variable_scope(\'accuracy\'):\n        # handle fully convolutional classifiers\n        logits_shape = logits.shape\n        if len(logits_shape) == 4 and logits_shape[1:3] == [1, 1]:\n            top_k_logits = tf.squeeze(logits, [1, 2])\n        else:\n            top_k_logits = logits\n        top_k_op = tf.nn.in_top_k(top_k_logits, labels, 1)\n        accuracy = tf.reduce_mean(tf.cast(top_k_op, tf.float32))\n\n    return accuracy\n\n\ndef confusion_matrix_op(logits, labels, num_classes):\n    """"""Creates the operation to build the confusion matrix between the\n    predictions and the labels. The number of classes are required to build\n    the matrix correctly.\n    Args:\n        logits: a [batch_size, 1,1, num_classes] tensor or\n                a [batch_size, num_classes] tensor\n        labels: a [batch_size] tensor\n    Returns:\n        confusion_matrix_op: the confusion matrix tf op\n    """"""\n    with tf.variable_scope(\'confusion_matrix\'):\n        # handle fully convolutional classifiers\n        logits_shape = logits.shape\n        if len(logits_shape) == 4 and logits_shape[1:3] == [1, 1]:\n            top_k_logits = tf.squeeze(logits, [1, 2])\n        else:\n            top_k_logits = logits\n\n        # Extract the predicted label (top-1)\n        _, top_predicted_label = tf.nn.top_k(top_k_logits, k=1, sorted=False)\n        # (batch_size, k) -> k = 1 -> (batch_size)\n        top_predicted_label = tf.squeeze(top_predicted_label, axis=1)\n\n        return tf.confusion_matrix(\n            labels, top_predicted_label, num_classes=num_classes)\n\n\ndef iou_op(real_coordinates, coordinates):\n    """"""Returns the average interserction over union operation between a batch of\n    real_coordinates and a batch of coordinates.\n    Args:\n        real_coordinates: a tensor with shape [batch_size, 4]\n        coordinates: a tensor with shape [batch_size, 4]\n    Returns:\n        iou: avewrage interserction over union in the batch\n    """"""\n\n    with tf.variable_scope(\'iou\'):\n        ymin_orig = real_coordinates[:, 0]\n        xmin_orig = real_coordinates[:, 1]\n        ymax_orig = real_coordinates[:, 2]\n        xmax_orig = real_coordinates[:, 3]\n        area_orig = (ymax_orig - ymin_orig) * (xmax_orig - xmin_orig)\n\n        ymin = coordinates[:, 0]\n        xmin = coordinates[:, 1]\n        ymax = coordinates[:, 2]\n        xmax = coordinates[:, 3]\n        area_pred = (ymax - ymin) * (xmax - xmin)\n\n        intersection_ymin = tf.maximum(ymin, ymin_orig)\n        intersection_xmin = tf.maximum(xmin, xmin_orig)\n        intersection_ymax = tf.minimum(ymax, ymax_orig)\n        intersection_xmax = tf.minimum(xmax, xmax_orig)\n\n        intersection_area = tf.maximum(\n            intersection_ymax - intersection_ymin,\n            tf.zeros_like(intersection_ymax)) * tf.maximum(\n                intersection_xmax - intersection_xmin,\n                tf.zeros_like(intersection_ymax))\n\n        iou = tf.reduce_mean(intersection_area /\n                             (area_orig + area_pred - intersection_area))\n        return iou\n'"
dytb/inputs/__init__.py,0,b''
dytb/inputs/images.py,28,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Utils for image processing""""""\n\nimport tensorflow as tf\n\n\n# Adapted from\n# https://github.com/pavelgonchar/colornet/blob/master/train.py\ndef rgb2yuv(rgb):\n    """"""\n    Convert RGB image into YUV https://en.wikipedia.org/wiki/YUV\n    """"""\n    rgb2yuv_filter = tf.constant([[[[0.299, -0.169,\n                                     0.499], [0.587, -0.331, -0.418],\n                                    [0.114, 0.499, -0.0813]]]])\n    rgb2yuv_bias = tf.constant([0., 0.5, 0.5])\n\n    rgb = tf.expand_dims(rgb, 0)\n\n    temp = tf.nn.conv2d(rgb, rgb2yuv_filter, [1, 1, 1, 1], \'SAME\')\n    temp = tf.nn.bias_add(temp, rgb2yuv_bias)\n    temp = tf.squeeze(temp, [0])\n\n    return temp\n\n\n# Adapted from\n# https://github.com/pavelgonchar/colornet/blob/master/train.py\ndef yuv2rgb(yuv):\n    """"""\n    Convert YUV image into RGB https://en.wikipedia.org/wiki/YUV\n    """"""\n    yuv = tf.multiply(yuv, 255)\n    yuv2rgb_filter = tf.constant([[[[1., 1., 1.], [0., -0.34413999, 1.77199996],\n                                    [1.40199995, -0.71414, 0.]]]])\n    yuv2rgb_bias = tf.constant([-179.45599365, 135.45983887, -226.81599426])\n\n    yuv = tf.expand_dims(yuv, 0)\n    temp = tf.nn.conv2d(yuv, yuv2rgb_filter, [1, 1, 1, 1], \'SAME\')\n    temp = tf.nn.bias_add(temp, yuv2rgb_bias)\n    temp = tf.maximum(temp, tf.zeros(temp.get_shape(), dtype=tf.float32))\n    temp = tf.minimum(temp,\n                      tf.multiply(\n                          tf.ones(temp.get_shape(), dtype=tf.float32), 255))\n    temp = tf.divide(temp, 255)\n    temp = tf.squeeze(temp, [0])\n    return temp\n\n\ndef scale_image(image):\n    """"""Returns the image tensor with values in [-1, 1].\n    Args:\n        image: [height, width, depth] tensor with values in [0,1]\n    """"""\n    image = tf.subtract(image, 0.5)\n    # now image has values with zero mean in range [-0.5, 0.5]\n    image = tf.multiply(image, 2.0)\n    # now image has values with zero mean in range [-1, 1]\n    return image\n\n\ndef read_image_jpg(image_path, depth=3, scale=True):\n    """"""Reads the image from image_path (tf.string tensor) [jpg image].\n    Cast the result to float32 and if scale=True scale it in [-1,1]\n    using scale_image. Otherwise the values are in [0,1]\n    Reuturn:\n        the decoded jpeg image, casted to float32\n    """"""\n\n    image = tf.image.convert_image_dtype(\n        tf.image.decode_jpeg(tf.read_file(image_path), channels=depth),\n        dtype=tf.float32)\n    if scale:\n        image = scale_image(image)\n    return image\n\n\ndef read_image_png(image_path, depth=3, scale=True):\n    """"""Reads the image from image_path (tf.string tensor) [jpg image].\n    Cast the result to float32 and if scale=True scale it in [-1,1]\n    using scale_image. Otherwise the values are in [0,1]\n    Reuturn:\n        the decoded jpeg image, casted to float32\n    """"""\n    image = tf.image.convert_image_dtype(\n        tf.image.decode_png(tf.read_file(image_path), channels=depth),\n        dtype=tf.float32)\n    if scale:\n        image = scale_image(image)\n    return image\n\n\ndef read_image(image_path, channel, image_type, scale=True):\n    """"""Wrapper around read_image_{jpg,png}""""""\n    if image_type == ""jpg"":\n        image = read_image_jpg(image_path, channel, scale)\n    else:\n        image = read_image_png(image_path, channel, scale)\n    return image\n'"
dytb/inputs/interfaces.py,0,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Define the interface to implement to define an input""""""\n\nfrom abc import ABCMeta, abstractmethod, abstractproperty\nfrom enum import Enum, unique\n\n\nclass Input(object, metaclass=ABCMeta):\n    """"""Input is the interface that classifiers must implement""""""\n\n    @abstractmethod\n    def inputs(self, input_type, batch_size, augmentation_fn=None):\n        """"""Construct input for evaluation using the Reader ops.\n\n        Args:\n            input_type: InputType enum\n            batch_size: Number of elements per batch.\n            augmentation_fn: function that accepts an input value,\n                perform augmentation and returns the value\n\n        Returns:\n            elements:  tensor of with batch_size elements\n            ground_truth: tensor with batch_size elements\n        """"""\n        pass\n\n    @abstractmethod\n    def num_examples(self, input_type):\n        """"""Returns the number of examples for the specified input_type\n\n        Args:\n            input_type: InputType enum\n        """"""\n        pass\n\n    @abstractproperty\n    def num_classes(self):\n        """"""Returns the number of classes""""""\n        pass\n\n    @abstractproperty\n    def name(self):\n        """"""Returns the name of the input source""""""\n        pass\n\n\n@unique\nclass InputType(Enum):\n    """"""Enum to specify the data type requested""""""\n    validation = \'validation\'\n    train = \'train\'\n    test = \'test\'\n\n    def __str__(self):\n        """"""Return the string representation of the enum""""""\n        return self.value\n\n    @staticmethod\n    def check(input_type):\n        """"""Check if input_type is an element of this Enum""""""\n        if not isinstance(input_type, InputType):\n            raise ValueError(""Invalid input_type, required a valid InputType"")\n'"
dytb/inputs/processing.py,7,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Utils to dataset preprocessing""""""\n\nimport os\nimport multiprocessing\nimport tensorflow as tf\n\n\ndef build_batch(image, label, min_queue_examples, batch_size, shuffle):\n    """"""Construct a queued batch of images and labels.\n    Args:\n        image: 3-D Tensor of [height, width, 3] of type.float32.\n        label: 1-D Tensor or a list of tensors like [label, attrA, ... ]\n        min_queue_examples: int32, minimum number of samples to retain\n           in the queue that provides of batches of examples.\n        batch_size: Number of images per batch.\n        shuffle: boolean indicating whether to use a shuffling queue.\n\n    Returns:\n        images: Images. 4D tensor of [batch_size, height, width, 3] size.\n        labels: Labels. 1D tensor of [batch_size] size containing the elements of labels\n    """"""\n    # Create a queue that shuffles the examples, and then\n    # read \'batch_size\' images + labels from the example queue.\n    num_preprocess_threads = multiprocessing.cpu_count()\n    if num_preprocess_threads > 2:\n        num_preprocess_threads -= 2\n\n    if isinstance(label, list):\n        row = [image] + label\n    else:\n        row = [image, label]\n\n    if shuffle:\n        return tf.train.shuffle_batch(\n            row,\n            batch_size=batch_size,\n            num_threads=num_preprocess_threads,\n            capacity=min_queue_examples + 3 * batch_size,\n            min_after_dequeue=min_queue_examples)\n\n    return tf.train.batch(\n        row,\n        batch_size=batch_size,\n        num_threads=num_preprocess_threads,\n        capacity=min_queue_examples + 3 * batch_size)\n\n\ndef convert_to_tfrecords(dataset, name, data_dir):\n    """""" Converts the dataset in a TFRecord file with name.tfrecords.\n    Save it into data_dir.""""""\n\n    def _int64_feature(value):\n        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n    def _bytes_feature(value):\n        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n    if dataset.images.shape[0] != dataset.num_examples:\n        raise ValueError(\'Images size {} does not match label size {}.\'.format(\n            dataset.images.shape[0], dataset.num_examples))\n    rows = dataset.images.shape[1]\n    cols = dataset.images.shape[2]\n    depth = dataset.images.shape[3]\n\n    filename = os.path.join(data_dir, name + \'.tfrecords\')\n    print(\'Writing\', filename)\n    writer = tf.python_io.TFRecordWriter(filename)\n    for index in range(dataset.num_examples):\n        image_raw = dataset.images[index].tostring()\n        example = tf.train.Example(\n            features=tf.train.Features(\n                feature={\n                    \'height\': _int64_feature(rows),\n                    \'width\': _int64_feature(cols),\n                    \'depth\': _int64_feature(depth),\n                    \'label\': _int64_feature(int(dataset.labels[index])),\n                    \'image_raw\': _bytes_feature(image_raw)\n                }))\n        writer.write(example.SerializeToString())\n    writer.close()\n'"
dytb/models/__init__.py,0,b''
dytb/models/collections.py,0,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Collections""""""\n\n# name of the collection that holds non trainable\n# but required variables for the current model\nREQUIRED_NON_TRAINABLES = \'required_vars_collection\'\n\n# name of the collection that holds the scalar summaries\nSCALAR_SUMMARIES = \'scalar_summaries\'\n\n# name of the collection that holds the media summaries\n# media = not scalar\nMEDIA_SUMMARIES = \'media_summaries\'\n\n# losses collection\nLOSSES = \'losses\'\n'"
dytb/models/interfaces.py,5,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Define the model interfaces""""""\n\nfrom abc import ABCMeta, abstractmethod, abstractproperty\n# Evaluators\nfrom ..evaluators.AutoencoderEvaluator import AutoencoderEvaluator\nfrom ..evaluators.ClassifierEvaluator import ClassifierEvaluator\nfrom ..evaluators.DetectorEvaluator import DetectorEvaluator\nfrom ..evaluators.RegressorEvaluator import RegressorEvaluator\n\n\nclass Autoencoder(object, metaclass=ABCMeta):\n    """"""Autoencoder is the interface that classifiers must implement""""""\n\n    def __init__(self):\n        self._info = {}\n        self._seed = None\n        self._evaluator = None\n\n    @abstractmethod\n    def get(self, inputs, num_classes, train_phase=False, l2_penalty=0.0):\n        """""" define the model with its inputs.\n        Use this function to define the model in training and when exporting the model\n        in the protobuf format.\n\n        Args:\n            inputs: model input\n            num_classes: number of classes to predict. If the model doesn\'t use it,\n                         just pass any value.\n            train_phase: set it to True when defining the model, during train\n            l2_penalty: float value, weight decay (l2) penalty\n\n        Returns:\n            is_training_: tf.bool placeholder enable/disable training ops at run time\n            predictions: the model output\n        """"""\n\n    @abstractmethod\n    def loss(self, predictions, real_values):\n        """"""Return the loss operation between predictions and real_values\n        Args:\n            predictions: predicted values\n            labels: real_values\n\n        Returns:\n            Loss tensor of type float.\n        """"""\n\n    @property\n    def name(self):\n        """"""Returns the name of the model""""""\n        return self.__class__.__name__\n\n    @property\n    def info(self):\n        """"""Returns the inforation about the trained model""""""\n        return self._info\n\n    @info.setter\n    def info(self, info):\n        """"""Save the training info\n        Args:\n            info: dict of training info\n        """"""\n        self._info = info\n\n    @property\n    def seed(self):\n        """"""Returns the seed used for weight initialization""""""\n        return self._seed\n\n    @seed.setter\n    def seed(self, seed):\n        """"""Set the seed to use for weight initialization\n        Args:\n            seed\n        """"""\n        self._seed = seed\n\n    @property\n    def evaluator(self):\n        """"""Returns the evaluator associated to the model""""""\n        if self._evaluator is None:\n            obj = AutoencoderEvaluator()\n            obj.model = self\n            self._evaluator = obj\n\n        return self._evaluator\n\n\nclass Classifier(object, metaclass=ABCMeta):\n    """"""Classifier is the interface that classifiers must implement""""""\n\n    def __init__(self):\n        self._info = {}\n        self._seed = None\n        self._evaluator = None\n\n    @abstractmethod\n    def get(self, inputs, num_classes, train_phase=False, l2_penalty=0.0):\n        """"""Define the model with its inputs.\n        Use this function to define the model in training and when exporting the model\n        in the protobuf format.\n\n        Args:\n            inputs: model input\n            num_classes: number of classes to predict\n            train_phase: set it to True when defining the model, during train\n            l2_penalty: float value, weight decay (l2) penalty\n\n        Returns:\n            is_training_: tf.bool placeholder enable/disable training ops at run time\n            logits: the model output\n        """"""\n\n    @abstractmethod\n    def loss(self, logits, labels):\n        """"""Return the loss operation between logits and labels\n        Args:\n            logits: Logits from get().\n            labels: Labels from train_inputs or inputs(). 1-D tensor\n                  of shape [batch_size]\n\n        Returns:\n            Loss tensor of type float.\n        """"""\n\n    @property\n    def name(self):\n        """"""Returns the name of the model""""""\n        return self.__class__.__name__\n\n    @property\n    def info(self):\n        """"""Returns the inforation about the trained model""""""\n        return self._info\n\n    @info.setter\n    def info(self, info):\n        """"""Save the training info\n        Args:\n            info: dict of training info\n        """"""\n        self._info = info\n\n    @property\n    def seed(self):\n        """"""Returns the seed used for weight initialization""""""\n        return self._seed\n\n    @seed.setter\n    def seed(self, seed):\n        """"""Set the seed to use for weight initialization\n        Args:\n            seed\n        """"""\n        self._seed = seed\n\n    @property\n    def evaluator(self):\n        """"""Returns the evaluator associated to the model""""""\n        if self._evaluator is None:\n            obj = ClassifierEvaluator()\n            obj.model = self\n            self._evaluator = obj\n        return self._evaluator\n\n\nclass Detector(object, metaclass=ABCMeta):\n    """"""Detector is the interface that detectors must implement""""""\n\n    def __init__(self):\n        self._info = {}\n        self._seed = None\n        self._evaluator = None\n\n    @abstractmethod\n    def get(self, inputs, num_classes, train_phase=False, l2_penalty=0.0):\n        """""" define the model with its inputs.\n        Use this function to define the model in training and when exporting the model\n        in the protobuf format.\n\n        Args:\n            inputs: model input, tensor with batch_size elements\n            num_classes: number of classes to predict. If the model doesn\'t use it,\n                         just pass any value.\n            train_phase: set it to True when defining the model, during train\n            l2_penalty: float value, weight decay (l2) penalty\n\n        Returns:\n            is_training_: tf.bool placeholder enable/disable training ops at run time\n            logits: the unscaled prediction for a class specific detector\n            bboxes: the predicted coordinates for every detected object in the input image\n                    this must have the same number of rows of logits\n        """"""\n\n    @abstractmethod\n    def loss(self, label_relations, bboxes_relations):\n        """"""Return the loss operation.\n        Args:\n            label_relations: a tuple with 2 elements, usually the pair\n            (labels, logits), each one a tensor of batch_size elements\n            bboxes_relations: a tuple with 2 elements, usually the pair\n            (coordinates, bboxes) where coordinates are the\n            ground truth coordinates ad bboxes the predicted one\n        Returns:\n            Loss tensor of type float.\n        """"""\n\n    @property\n    def name(self):\n        """"""Returns the name of the model""""""\n        return self.__class__.__name__\n\n    @property\n    def info(self):\n        """"""Returns the inforation about the trained model""""""\n        return self._info\n\n    @info.setter\n    def info(self, info):\n        """"""Save the training info\n        Args:\n            info: dict of training info\n        """"""\n        self._info = info\n\n    @property\n    def seed(self):\n        """"""Returns the seed used for weight initialization""""""\n        return self._seed\n\n    @seed.setter\n    def seed(self, seed):\n        """"""Set the seed to use for weight initialization\n        Args:\n            seed\n        """"""\n        self._seed = seed\n\n    @property\n    def evaluator(self):\n        """"""Returns the evaluator associated to the model""""""\n        if self._evaluator is None:\n            obj = DetectorEvaluator()\n            obj.model = self\n            self._evaluator = obj\n        return self._evaluator\n\n\nclass Regressor(object, metaclass=ABCMeta):\n    """"""Regressor is the interface that regressors must implement""""""\n\n    def __init__(self):\n        self._info = {}\n        self._seed = None\n        self._evaluator = None\n\n    @abstractmethod\n    def get(self, inputs, num_classes, train_phase=False, l2_penalty=0.0):\n        """""" define the model with its inputs.\n        Use this function to define the model in training and when exporting the model\n        in the protobuf format.\n\n        Args:\n            inputs: model input\n            num_classes: number of classes to predict. If the model doesn\'t use it,\n                         just pass any value.\n            train_phase: set it to True when defining the model, during train\n            l2_penalty: float value, weight decay (l2) penalty\n\n        Returns:\n            is_training_: tf.bool placeholder enable/disable training ops at run time\n            predictions: the model output\n        """"""\n\n    @abstractmethod\n    def loss(self, predictions, labels):\n        """"""Return the loss operation between predictions and labels\n        Args:\n            predictions: Predictions from get().\n            labels: Labels from train_inputs or inputs(). 1-D tensor\n                  of shape [batch_size]\n\n        Returns:\n            Loss tensor of type float.\n        """"""\n\n    @property\n    def name(self):\n        """"""Returns the name of the model""""""\n        return self.__class__.__name__\n\n    @property\n    def info(self):\n        """"""Returns the inforation about the trained model""""""\n        return self._info\n\n    @info.setter\n    def info(self, info):\n        """"""Save the training info\n        Args:\n            info: dict of training info\n        """"""\n        self._info = info\n\n    @property\n    def seed(self):\n        """"""Returns the seed used for weight initialization""""""\n        return self._seed\n\n    @seed.setter\n    def seed(self, seed):\n        """"""Set the seed to use for weight initialization\n        Args:\n            seed\n        """"""\n        self._seed = seed\n\n    @property\n    def evaluator(self):\n        """"""Returns the evaluator associated to the model""""""\n        if self._evaluator is None:\n            obj = RegressorEvaluator()\n            obj.model = self\n            self._evaluator = obj\n        return self._evaluator\n\n\nclass Custom(object, metaclass=ABCMeta):\n    """"""Custom is the interface that custom models must implement""""""\n\n    def __init__(self):\n        self._info = {}\n        self._seed = None\n        self._evaluator = None\n\n    @abstractmethod\n    def get(self, inputs, num_classes, **kwargs):\n        """""" define the model with its inputs.\n        Use this function to define the model in training and when exporting the model\n        in the protobuf format.\n\n        Args:\n            inputs: model input\n            num_classes: number of classes to predict. If the model doesn\'t use it,\n                         just pass any value.\n            kwargs:\n                train_phase: set it to True when defining the model, during train\n                l2_penalty: float value, weight decay (l2) penalty\n\n        Returns:\n            is_training_: tf.bool placeholder enable/disable training ops at run time\n            predictions: the model output\n        """"""\n\n    @abstractmethod\n    def loss(self, predictions, real_values):\n        """"""Return the loss operation between predictions and real_values\n        Args:\n            predictions: a list of predicted values eg [predicted_labels_batch, ...]\n            labels: a list of real_values, eg [ labels_batch, attributeA_batch, ...]\n\n        Returns:\n            Loss tensor of type float.\n        """"""\n\n    @abstractproperty\n    def evaluator(self):\n        """"""Returns the evaluator associated to the model""""""\n\n    # Below implemented properties\n\n    @property\n    def name(self):\n        """"""Returns the name of the model""""""\n        return self.__class__.__name__\n\n    @property\n    def info(self):\n        """"""Returns the inforation about the trained model""""""\n        return self._info\n\n    @info.setter\n    def info(self, info):\n        """"""Save the training info\n        Args:\n            info: dict of training info\n        """"""\n        self._info = info\n\n    @property\n    def seed(self):\n        """"""Returns the seed used for weight initialization""""""\n        return self._seed\n\n    @seed.setter\n    def seed(self, seed):\n        """"""Set the seed to use for weight initialization\n        Args:\n            seed\n        """"""\n        self._seed = seed\n'"
dytb/models/layers.py,61,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Layers to easy create models and nice visualizations""""""\n\nimport math\nimport numbers\nimport tensorflow as tf\nfrom .utils import legalize_name, tf_log\nfrom .visualization import on_grid\nfrom .collections import LOSSES, REQUIRED_NON_TRAINABLES, MEDIA_SUMMARIES\n\n\ndef _shape_list(shape):\n    if isinstance(shape, tf.TensorShape):\n        return shape.as_list()\n    return shape\n\n\ndef weight(name,\n           shape,\n           train_phase,\n           initializer=tf.contrib.layers.variance_scaling_initializer(\n               factor=2.0,\n               mode=\'FAN_IN\',\n               uniform=False,\n               seed=None,\n               dtype=tf.float32),\n           wd=0.0):\n    """"""Returns a tensor with the requested shape, initialized\n      using the provided intitializer (default: He init).\n      Applies L2 weight decay penalyt using wd term.\n      Enables visualizations when in train_phase=True.\n      Args:\n          name: the name of the weight\n          shape: the shape of the tensor, a python list or tuple\n          train_phase: boolean, to enable/disable visualization and L2 decay\n          initializer: the initializer to use\n          wd: when train_phase=True uses `wd` as decay penalty\n      Returns:\n        weights: the weight tensor initialized.\n    """"""\n    shape = _shape_list(shape)\n    weights = tf.get_variable(\n        name, shape=shape, initializer=initializer, dtype=tf.float32)\n\n    # show weights of the first layer\n    first = len(shape) == 4 and shape[2] in (1, 3, 4)\n    if first and train_phase:\n        with tf.variable_scope(""visualization""):\n            num_kernels = shape[3]\n            # check if is a perfect square\n            grid_side = math.floor(math.sqrt(num_kernels))\n            tf_log(\n                tf.summary.image(\n                    legalize_name(name),\n                    on_grid(weights[:, :, :, 0:grid_side**2], grid_side,\n                            grid_side)),\n                collection=MEDIA_SUMMARIES)\n\n    if train_phase:\n        # Add weight decay to W\n        tf.add_to_collection(LOSSES, tf.multiply(tf.nn.l2_loss(weights), wd))\n        tf_log(tf.summary.histogram(legalize_name(name), weights))\n    return weights\n\n\ndef bias(name,\n         shape,\n         train_phase,\n         initializer=tf.constant_initializer(value=0.0)):\n    """"""Returns a bias variabile initializeted wuth the provided initializer.\n    No weight decay applied to the bias terms.\n    Enables visualization when in train_phase.\n    Args:\n        name: name for the bias variable\n        shape: shape of the variabile\n        train_phase: boolean, to enable/disable visualization\n        initializer: the initializer to use\n    Returns:\n        bias: the vias variable correctly initialized\n    """"""\n    shape = _shape_list(shape)\n    return weight(name, shape, train_phase, initializer=initializer, wd=0.0)\n\n\ndef atrous_conv(input_x,\n                shape,\n                rate,\n                padding,\n                train_phase,\n                bias_term=True,\n                activation=tf.identity,\n                wd=0.0,\n                initializer=tf.contrib.layers.variance_scaling_initializer(\n                    factor=2.0,\n                    mode=\'FAN_IN\',\n                    uniform=False,\n                    seed=None,\n                    dtype=tf.float32)):\n    """""" Define an atrous conv layer.\n    Args:\n         input_x: a 4D tensor\n         shape: weight shape\n         rate: : A positive int32. The stride with which we sample input values\n            cross the height and width dimensions. Equivalently, the rate by which\n            we upsample the filter values by inserting zeros\n            across the height and width dimensions. In the literature, the same\n            parameter is sometimes called input stride or dilation\n         padding: \'VALID\' or \'SAME\'\n         train_phase: boolean that enables/diables visualizations and train-only specific ops\n         bias_term: a boolean to add (if True) the bias term. Usually disable when\n             the layer is wrapped in a batch norm layer\n         activation: activation function. Default linear\n         wd: weight decay\n         initializer: the initializer to use\n    Rerturns:\n        op: the conv2d op""""""\n    shape = _shape_list(shape)\n    W = weight(""W"", shape, train_phase, initializer=initializer, wd=wd)\n    result = tf.nn.atrous_conv2d(input_x, W, rate, padding)\n    if bias_term:\n        b = bias(""b"", [shape[3]], train_phase)\n        result = tf.nn.bias_add(result, b)\n\n    # apply nonlinearity\n    out = activation(result)\n\n    if train_phase:\n        with tf.variable_scope(""visualization""):\n            # log convolution result pre-activation function\n            # on a single image, the first of the batch\n            conv_results = tf.split(\n                value=result[0], num_or_size_splits=shape[3], axis=2)\n            grid_side = math.floor(math.sqrt(shape[3]))\n\n            pre_activation = on_grid(\n                tf.transpose(conv_results, perm=(1, 2, 3,\n                                                 0))[:, :, :, 0:grid_side**2],\n                grid_side, grid_side)\n\n            # log post-activation\n            conv_results = tf.split(\n                value=out[0], num_or_size_splits=shape[3], axis=2)\n            post_activation = on_grid(\n                tf.transpose(conv_results, perm=(1, 2, 3,\n                                                 0))[:, :, :, 0:grid_side**2],\n                grid_side, grid_side)\n\n            tf_log(\n                tf.summary.image(\n                    legalize_name(result.name + \'/pre_post_activation\'),\n                    tf.concat([pre_activation, post_activation], axis=2),\n                    max_outputs=1),\n                collection=MEDIA_SUMMARIES)\n    return out\n\n\ndef conv(input_x,\n         shape,\n         stride,\n         padding,\n         train_phase,\n         bias_term=True,\n         activation=tf.identity,\n         wd=0.0,\n         initializer=tf.contrib.layers.variance_scaling_initializer(\n             factor=2.0,\n             mode=\'FAN_IN\',\n             uniform=False,\n             seed=None,\n             dtype=tf.float32)):\n    """""" Define a conv layer.\n    Args:\n        input_x: a 4D tensor\n        shape: weight shape\n        stride: a single value supposing equal stride along X and Y\n        padding: \'VALID\' or \'SAME\'\n        train_phase: boolean that enables/diables visualizations and train-only specific ops\n        bias_term: a boolean to add (if True) the bias term. Usually disable when\n                   the layer is wrapped in a batch norm layer\n        activation: activation function. Default linear\n        train_phase: boolean that enables/diables visualizations and train-only specific ops\n        wd: weight decay\n        initializer: the initializer to use\n    Rerturns:\n        op: the conv2d op\n    """"""\n    shape = _shape_list(shape)\n    W = weight(""W"", shape, train_phase, initializer=initializer, wd=wd)\n    result = tf.nn.conv2d(input_x, W, [1, stride, stride, 1], padding)\n    if bias_term:\n        b = bias(""b"", [shape[3]], train_phase)\n        result = tf.nn.bias_add(result, b)\n\n    # apply nonlinearity\n    out = activation(result)\n\n    if train_phase:\n        with tf.variable_scope(""visualization""):\n            # log convolution result pre-activation function\n            # on a single image, the first of the batch\n            conv_results = tf.split(\n                value=result[0], num_or_size_splits=shape[3], axis=2)\n            grid_side = math.floor(math.sqrt(shape[3]))\n\n            pre_activation = on_grid(\n                tf.transpose(conv_results, perm=(1, 2, 3,\n                                                 0))[:, :, :, 0:grid_side**2],\n                grid_side, grid_side)\n\n            # log post-activation\n            conv_results = tf.split(\n                value=out[0], num_or_size_splits=shape[3], axis=2)\n            post_activation = on_grid(\n                tf.transpose(conv_results, perm=(1, 2, 3,\n                                                 0))[:, :, :, 0:grid_side**2],\n                grid_side, grid_side)\n\n            tf_log(\n                tf.summary.image(\n                    legalize_name(result.name + \'/pre_post_activation\'),\n                    tf.concat([pre_activation, post_activation], axis=2),\n                    max_outputs=1),\n                collection=MEDIA_SUMMARIES)\n    return out\n\n\ndef conv_transpose(input_x,\n                   shape,\n                   stride,\n                   padding,\n                   output_shape,\n                   train_phase,\n                   bias_term=True,\n                   activation=tf.identity,\n                   wd=0.0,\n                   initializer=tf.contrib.layers.variance_scaling_initializer(\n                       factor=2.0,\n                       mode=\'FAN_IN\',\n                       uniform=False,\n                       seed=None,\n                       dtype=tf.float32)):\n    """""" Define a conv_transpose layer.\n    Args:\n        input_x: a 4D tensor\n        shape: weight shape: [height, width, output_channels, in_channels]\n        stride: a single value supposing equal stride along X and Y\n        padding: \'VALID\' or \'SAME\'\n        output_shape: a 4D tensor, that\'s the expected output of the conv_transpose operation\n                     (this parameter is needed because the shape of the output can\'t necessarily be\n                     computed from the shape of the input)\n        train_phase: boolean that enables/diables visualizations and train-only specific ops\n        bias_term: a boolean to add (if True) the bias term. Usually disable when\n                   the layer is wrapped in a batch norm layer\n        activation: activation function. Default linear\n        train_phase: boolean that enables/diables visualizations and train-only specific ops\n        wd: weight decay\n        initializer: the initializer to use\n    Rerturns:\n        op: the conv2d_transpose op\n    """"""\n    shape = _shape_list(shape)\n    output_shape = _shape_list(output_shape)\n    W = weight(""W"", shape, train_phase, initializer=initializer, wd=wd)\n    result = tf.nn.conv2d_transpose(input_x, W, output_shape,\n                                    [1, stride, stride, 1], padding)\n    if bias_term:\n        b = bias(""b"", [shape[2]], train_phase)\n        result = tf.nn.bias_add(result, b)\n\n    # apply nonlinearity\n    out = activation(result)\n\n    if train_phase:\n        with tf.variable_scope(""visualization""):\n            # log convolution result pre-activation function\n            # on a single image, the first of the batch\n            conv_results = tf.split(\n                value=result[0], num_or_size_splits=shape[2], axis=2)\n            grid_side = math.floor(math.sqrt(shape[2]))\n\n            pre_activation = on_grid(\n                tf.transpose(conv_results, perm=(1, 2, 3,\n                                                 0))[:, :, :, 0:grid_side**2],\n                grid_side, grid_side)\n\n            # log post-activation\n            conv_results = tf.split(\n                value=out[0], num_or_size_splits=shape[2], axis=2)\n            post_activation = on_grid(\n                tf.transpose(conv_results, perm=(1, 2, 3,\n                                                 0))[:, :, :, 0:grid_side**2],\n                grid_side, grid_side)\n\n            tf_log(\n                tf.summary.image(\n                    legalize_name(result.name + \'/pre_post_activation\'),\n                    tf.concat([pre_activation, post_activation], axis=2),\n                    max_outputs=1),\n                collection=MEDIA_SUMMARIES)\n    return out\n\n\ndef fc(input_x,\n       shape,\n       train_phase,\n       bias_term=True,\n       activation=tf.identity,\n       wd=0.0,\n       initializer=tf.contrib.layers.variance_scaling_initializer(\n           factor=2.0,\n           mode=\'FAN_IN\',\n           uniform=False,\n           seed=None,\n           dtype=tf.float32)):\n    """""" Define a fully connected layer.\n    Args:\n        input_x: a 4d tensor\n        shape: weight shape\n        train_phase: boolean that enables/diables visualizations and train-only specific ops\n        bias_term: a boolean to add (if True) the bias term. Usually disable when\n             the layer is wrapped in a batch norm layer\n        activation: activation function. Default linear\n        wd: weight decay\n        initializer: the initializer to use\n    Returns:\n        fc: the fc layer\n    """"""\n    shape = _shape_list(shape)\n    W = weight(""W"", shape, train_phase, initializer=initializer, wd=wd)\n    result = tf.matmul(input_x, W)\n    if bias_term:\n        b = bias(""b"", [shape[1]], train_phase)\n        result = tf.nn.bias_add(result, b)\n\n    return activation(result)\n\n\ndef batch_norm(layer_output, is_training_, decay=0.9):\n    """"""Applies batch normalization to the layer output.\n    Args:\n        layer_output: 4-d tensor, output of a FC/convolutional layer\n        is_training_: placeholder or boolean variable to set to True when training\n        decay:        decay for the moving average.\n    Returns:\n        bn: the batch normalization layer\n    """"""\n    return tf.contrib.layers.batch_norm(\n        inputs=layer_output,\n        decay=decay,\n        center=True,\n        scale=True,\n        epsilon=1e-3,\n        activation_fn=None,\n        # update moving mean and variance in place\n        updates_collections=None,\n        is_training=is_training_,\n        reuse=None,\n        # create a collections of varialbes to save\n        # (moving mean and moving variance)\n        variables_collections=[REQUIRED_NON_TRAINABLES],\n        outputs_collections=None,\n        trainable=True,\n        batch_weights=None,\n        fused=True,\n        scope=None)\n\n\ndef direct_dropout(x, keep_prob, noise_shape=None, seed=None, name=None):\n    """"""Computes dropout.\n    The original dropout as described in the paper, not the inverted version.\n    Thus it requires to scale the activation AT TEST TIME.\n    Args:\n        x: A tensor.\n        keep_prob: A scalar `Tensor` with the same type as x. The probability\n        that each element is kept.\n        noise_shape: A 1-D `Tensor` of type `int32`, representing the\n          shape for randomly generated keep/drop flags.\n        seed: A Python integer. Used to create random seeds.\n        name: A name for this operation (optional).\n    Returns:\n        A Tensor of the same shape of `x`.\n    Raises:\n        ValueError: If `keep_prob` is not in `(0, 1]`.\n    """"""\n    with tf.name_scope(name, ""direct_dropout"", [x]):\n        x = tf.convert_to_tensor(x, name=""x"")\n        if isinstance(keep_prob, numbers.Real) and not 0 < keep_prob <= 1:\n            raise ValueError(\n                ""keep_prob must be a scalar tensor or a float in the ""\n                ""range (0, 1], got %g"" % keep_prob)\n        keep_prob = tf.convert_to_tensor(\n            keep_prob, dtype=x.dtype, name=""keep_prob"")\n        keep_prob.get_shape().assert_is_compatible_with(tf.TensorShape([]))\n\n        # Do nothing if we know keep_prob == 1\n        if tf.contrib.util.constant_value(keep_prob) == 1:\n            return x\n\n        noise_shape = noise_shape if noise_shape is not None else tf.shape(x)\n        # uniform [keep_prob, 1.0 + keep_prob)\n        random_tensor = keep_prob\n        random_tensor += tf.random_uniform(\n            noise_shape, seed=seed, dtype=x.dtype)\n        # 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)\n        binary_tensor = tf.floor(random_tensor)\n        # Do not scale the activation in train time\n        ret = tf.multiply(x, binary_tensor)\n        ret.set_shape(x.get_shape())\n        return ret\n'"
dytb/models/utils.py,12,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Utils for models creation""""""\n\nimport re\nimport tensorflow as tf\nfrom .collections import SCALAR_SUMMARIES, REQUIRED_NON_TRAINABLES\n\n\ndef legalize_name(name):\n    """"""Made name a legal name to be used in tensorflow summaries\n    Args:\n        name: string\n    Returns:\n        name_legal\n    """"""\n    return re.sub(r""[^\\w|/]"", ""_"", name)\n\n\ndef tf_log(summary, collection=SCALAR_SUMMARIES):\n    """"""Add tf.summary object to collection named collection""""""\n    tf.add_to_collection(collection, summary)\n\n\ndef training_process_variables():\n    """"""global variables - trainable variables:\n    it contains the variable defined by the optimizer and the others\n    defined durint the training process.\n    Those variables are useful when restoring a training process.\n    Those variables are not trainable. can be saved and restored.\n    """"""\n\n    return [\n        variable for variable in tf.global_variables()\n        if variable.name not in (var.name for var in tf.trainable_variables())\n    ]\n\n\ndef variables_to_save(add_list=None):\n    """"""Returns a list of variables to save.\n    add_list variables are always added to the list\n    Args:\n        add_list: a list of variables\n    Returns:\n        list: list of tensors to save\n    """"""\n    if add_list is None:\n        add_list = []\n    return tf.trainable_variables() + tf.get_collection_ref(\n        REQUIRED_NON_TRAINABLES) + add_list + training_process_variables()\n\n\ndef variables_to_restore(add_list=None, exclude_scope_list=None):\n    """"""Returns a list of variables to restore to made the model working\n    properly.\n    The list is made by the trainable variables + required non trainable variables\n    such as statistics of batch norm layers.\n    Remove from the list variables that are in the exclude_scope_list.\n    Add variables in the add_list\n\n    Args:\n        add_list: a list of variables\n        exclude_scope_list: a list of scopes to exclude\n    Returns:\n        list: list of tensors to restore\n    """"""\n\n    if add_list is None:\n        add_list = []\n    if exclude_scope_list is None:\n        exclude_scope_list = []\n\n    variables = variables_to_save()\n    if exclude_scope_list:\n        variables[:] = [\n            variable for variable in variables if not variable.name.startswith(\n                tuple(scope for scope in exclude_scope_list))\n        ]\n    return variables + add_list\n\n\ndef variables_to_train(scope_list=None):\n    """"""Returns a list of variables to train, filtered by the scopes.\n    Args:\n        scope_list: a list of scope to train\n    Returns:\n        the list of variables to train by the optimizer\n    """"""\n    if scope_list is None:\n        return tf.trainable_variables()\n    vars_to_train = []\n    for scope in scope_list:\n        variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n        vars_to_train.extend(variables)\n    return vars_to_train\n\n\ndef num_neurons_and_shape(layer):\n    """"""Count the number of neurons in a single element of the layer, returns this\n    number and the shape of the single layer.\n    Args:\n        layer: [batch_size, widht, height, depth] if the layer is convolutional\n               [batch_size, num_neruons] if the layer is fully connected\n    Returns:\n        num_neurons, shape\n        Where num_neurons is the number of neurons in a single elment of the input batch,\n        shape is the shape of the single element""""""\n    # extract the number of neurons in x\n    # and the number of neurons kept on\n    input_shape = layer.get_shape()\n    if len(input_shape) == 4:  # conv layer\n        num_neurons = input_shape[1].value * input_shape[2].value * input_shape[\n            3].value\n        shape = [\n            -1, input_shape[1].value, input_shape[2].value, input_shape[3].value\n        ]\n    else:  #fc layer\n        num_neurons = input_shape[1].value\n        shape = [-1, input_shape[1].value]\n\n    return num_neurons, shape\n\n\ndef active_neurons(layer, off_value=0):\n    """"""Count the number of active (> off_value) neurons in a single element of the layer.\n    Args:\n        layer: [batch_size, widht, height, depth] if the layer is convolutional\n               [batch_size, num_neruons] if the layer is fully connected\n    Returns:\n        kept_on: [batch_size, 1] tf.int32, number of active neurons\n    """"""\n    binary_tensor = tf.cast(tf.greater(layer, off_value), tf.int32)\n    return tf.reduce_sum(binary_tensor, [1, 2, 3]\n                         if len(layer.get_shape()) == 4 else [1])\n\n\ndef count_trainable_parameters(print_model=False):\n    """"""Count the number of trainable parameters is the current graph.\n    Returns:\n        count: the number of trainable parameters""""""\n    total_parameters = 0\n    for variable in tf.trainable_variables():\n        # shape is an array of tf.Dimension\n        shape = variable.get_shape()\n        if print_model:\n            print(variable)\n        variable_parametes = 1\n        for dim in shape:\n            variable_parametes *= dim.value\n        total_parameters += variable_parametes\n    return total_parameters\n'"
dytb/models/visualization.py,21,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""utility methods to create visualizations in tensorboard""""""\n\nimport math\nimport tensorflow as tf\nfrom .utils import tf_log\nfrom .collections import MEDIA_SUMMARIES\n\n\n# Adapeted from\n# https://gist.github.com/kukuruza/03731dc494603ceab0c5#gistcomment-1879326\ndef on_grid(kernel, grid_side, pad=1):\n    """"""Visualize conv. features as an image (mostly for the 1st layer).\n    Place kernel into a grid, with some paddings between adjacent filters.\n\n    Args:\n        kernel:    tensor of shape [Y, X, NumChannels, NumKernels]\n        grid_side: side of the grid. Require: NumKernels == grid_side**2\n        pad:       number of black pixels around each filter (between them)\n\n    Returns:\n        An image Tensor with shape [(Y+2*pad)*grid_side, (X+2*pad)*grid_side, NumChannels, 1].\n    """"""\n\n    x_min = tf.reduce_min(kernel)\n    x_max = tf.reduce_max(kernel)\n\n    kernel1 = (kernel - x_min) / (x_max - x_min)\n\n    # pad X and Y\n    x1 = tf.pad(\n        kernel1,\n        tf.constant([[pad, pad], [pad, pad], [0, 0], [0, 0]]),\n        mode=\'CONSTANT\')\n\n    # X and Y dimensions, w.r.t. padding\n    Y = kernel1.get_shape()[0] + 2 * pad\n    X = kernel1.get_shape()[1] + 2 * pad\n\n    channels = kernel1.get_shape()[2]\n\n    # put NumKernels to the 1st dimension\n    x2 = tf.transpose(x1, (3, 0, 1, 2))\n    # organize grid on Y axis\n    x3 = tf.reshape(x2,\n                    tf.stack(\n                        values=[grid_side, Y * grid_side, X, channels],\n                        axis=0))  #3\n\n    # switch X and Y axes\n    x4 = tf.transpose(x3, (0, 2, 1, 3))\n    # organize grid on X axis\n    x5 = tf.reshape(x4,\n                    tf.stack(\n                        values=[1, X * grid_side, Y * grid_side, channels],\n                        axis=0))  #3\n\n    # back to normal order (not combining with the next step for clarity)\n    x6 = tf.transpose(x5, (2, 1, 3, 0))\n\n    # to tf.image_summary order [batch_size, height, width, channels],\n    #   where in this case batch_size == 1\n    x7 = tf.transpose(x6, (3, 0, 1, 2))\n\n    # scale to [0, 255] and convert to uint8\n    return tf.image.convert_image_dtype(x7, dtype=tf.uint8)\n\n\ndef log_images(name, inputs, outputs=None):\n    """"""Log inputs and outputs batch of images. Display images in grids\n    Args:\n        name: name of the summary\n        inputs: tensor with shape [batch_size, height, widht, depth]\n        outputs: if present must have the same dimensions as inputs\n    """"""\n\n    with tf.variable_scope(\'visualization\'):\n        batch_size = inputs.get_shape()[0].value\n        grid_side = math.floor(math.sqrt(batch_size))\n        inputs = on_grid(\n            tf.transpose(inputs, perm=(1, 2, 3, 0))[:, :, :, 0:grid_side**2],\n            grid_side)\n\n        if outputs is None:\n            tf_log(\n                tf.summary.image(name, inputs, max_outputs=1),\n                collection=MEDIA_SUMMARIES)\n            return\n\n        inputs = tf.pad(inputs, [[0, 0], [0, 0], [0, 10], [0, 0]])\n        outputs = on_grid(\n            tf.transpose(outputs, perm=(1, 2, 3, 0))[:, :, :, 0:grid_side**2],\n            grid_side)\n        tf_log(\n            tf.summary.image(\n                name, tf.concat([inputs, outputs], axis=2), max_outputs=1),\n            collection=MEDIA_SUMMARIES)\n'"
dytb/trainer/Trainer.py,19,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Trainer for the model""""""\n\nimport time\nimport os\nimport math\nfrom datetime import datetime\nimport numpy as np\nimport tensorflow as tf\nfrom .utils import builders, flow\n\nfrom ..inputs.interfaces import InputType\nfrom ..models.utils import tf_log, variables_to_train, count_trainable_parameters\nfrom ..models.collections import SCALAR_SUMMARIES, MEDIA_SUMMARIES\nfrom ..models.visualization import log_images\n\n\nclass Trainer(object):\n    """"""Trainer for a custom model""""""\n\n    def __init__(self, model, dataset, args, steps, paths):\n        """"""Initialize the trainer.\n        Args:\n            model: the model to train\n            dataset: implementation of the Input interface\n            args: dictionary of hyperparameters a train parameters\n            steps: dictionary of the training steps\n            paths: dictionary of the paths\n        """"""\n        self._model = model\n        self._dataset = dataset\n        self._model.evaluator.dataset = dataset\n        self._args = args\n        self._steps = steps\n        self._paths = paths\n\n    def train(self):\n        """"""Train the model\n        Returns:\n            info: dict containing the information of the trained model\n        Side effect:\n            saves the latest checkpoints and the best model in its own folder\n        """"""\n\n        with tf.Graph().as_default():\n            tf.set_random_seed(self._args[""seed""])\n            self._model.seed = self._args[""seed""]\n            global_step = tf.Variable(0, trainable=False, name=\'global_step\')\n\n            # Get inputs and targets: inputs is an input batch\n            # target could be either an array of elements or a tensor.\n            # it could be [label] or [label, attr1, attr2, ...]\n            # or Tensor, where tensor is a standard tensorflow Tensor with\n            # its own shape\n            with tf.device(\'/cpu:0\'):\n                inputs, *targets = self._dataset.inputs(\n                    input_type=InputType.train,\n                    batch_size=self._args[""batch_size""],\n                    augmentation_fn=self._args[""regularizations""][\n                        ""augmentation""][""fn""])\n\n            # Build a Graph that computes the predictions from the\n            # inference model.\n            # Preditions is an array of predictions with the same cardinality of\n            # targets\n            is_training_, *predictions = self._model.get(\n                inputs,\n                self._dataset.num_classes,\n                train_phase=True,\n                l2_penalty=self._args[""regularizations""][""l2""])\n\n            if len(predictions) != len(targets):\n                print((""{}.get 2nd return value and {}.inputs 2nd return ""\n                       ""value must have the same cardinality but got: {} vs {}""\n                      ).format(self._model.name, self._dataset.name,\n                               len(predictions), len(targets)))\n                return\n\n            if len(predictions) == 1:\n                predictions = predictions[0]\n                targets = targets[0]\n\n            if len(inputs.shape) == 4 and inputs.shape[3].value in (1, 3, 4):\n                log_images(""inputs"", inputs)\n\n            num_of_parameters = count_trainable_parameters(print_model=True)\n            print(""Model {}: trainable parameters: {}. Size: {} KB"".format(\n                self._model.name, num_of_parameters,\n                num_of_parameters * 4 / 1000))\n\n            # Calculate & log loss\n            loss = self._model.loss(predictions, targets)\n            tf_log(tf.summary.scalar(\'loss\', loss))\n\n            # Create optimizer and log learning rate\n            optimizer = builders.build_optimizer(self._args, self._steps,\n                                                 global_step)\n            train_op = optimizer.minimize(\n                loss,\n                global_step=global_step,\n                var_list=variables_to_train(self._args[""trainable_scopes""]))\n\n            model_selection_idx = -1\n            # validation metrics arrays\n            metrics_to_measure = []\n            metric_values_ = []\n            metric_summaries = []\n            for idx, metric in enumerate(self._model.evaluator.metrics):\n                if metric[""model_selection""]:\n                    model_selection_idx = idx\n\n                if metric[""tensorboard""]:\n                    # Build tensorboard scalar visualizations using placeholder\n                    metric_values_.append(tf.placeholder(tf.float32, shape=()))\n                    metric_summaries.append(\n                        tf.summary.scalar(metric[""name""], metric_values_[idx]))\n\n                    metrics_to_measure.append(metric)\n\n            if model_selection_idx == -1:\n                print(\n                    ""Please specify a metric in the evaluator with \'model_selection\' not None""\n                )\n                return\n\n            # visualizations\n            visualizations_to_measure = []\n            visualization_values_ = []\n            visualization_summaries = []\n            for idx, viz in enumerate(self._model.evaluator.visualizations):\n                visualization_values_.append(\n                    tf.placeholder(tf.float32, shape=None))\n                visualization_summaries.append(\n                    tf.summary.image(viz[""name""], visualization_values_[idx]))\n                visualizations_to_measure.append(viz)\n\n            # read collection after that every op added its own\n            # summaries in the train_summaries collection.\n            # No metrics are addded to the SCALAR_SUMMARIES collection\n            # SCALAR SUMMARIES are logged each dataset_size/10 iteration\n            scalar_summaries = tf.summary.merge(\n                tf.get_collection_ref(SCALAR_SUMMARIES))\n\n            # MEDIA SUMMARIES are logged each dataset_size iteration\n            # because when the logged data is big, the wasted space for data vis\n            # is too high. Hence log images at the end of every epoch\n            media_summaries = tf.summary.merge(\n                tf.get_collection_ref(MEDIA_SUMMARIES))\n\n            # Build an initialization operation to run below.\n            init = [\n                tf.variables_initializer(\n                    tf.global_variables() + tf.local_variables()),\n                tf.tables_initializer()\n            ]\n\n            # Start running operations on the Graph.\n            with tf.Session(config=tf.ConfigProto(\n                    allow_soft_placement=True)) as sess:\n                sess.run(init)\n\n                # Start the queue runners with a coordinator\n                coord = tf.train.Coordinator()\n                threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n                # Create the savers.\n                train_saver, best_saver = builders.build_train_savers()\n                flow.restore_or_restart(self._args, self._paths, sess)\n                train_log, validation_log = builders.build_loggers(\n                    sess.graph, self._paths)\n\n                # If a best model already exists (thus we\'re continuing a train\n                # process) then restore the best validation metric reached\n                # and place it into best_model_selection_measure\n                best_model_selection_measure = self._model.evaluator.eval(\n                    self._model.evaluator.metrics[model_selection_idx],\n                    self._paths[""best""],\n                    input_type=InputType.validation,\n                    batch_size=self._args[""batch_size""])\n\n                # Extract previous global step value\n                old_gs = sess.run(global_step)\n\n                # Restart from where we were\n                for step in range(old_gs, self._steps[""max""] + 1):\n                    start_time = time.time()\n                    _, loss_value = sess.run(\n                        [train_op, loss], feed_dict={\n                            is_training_: True\n                        })\n\n                    duration = time.time() - start_time\n\n                    if np.isnan(loss_value):\n                        print(\'Model diverged with loss = NaN\')\n                        break\n\n                    # update logs every 10 iterations\n                    if step % self._steps[""log""] == 0:\n                        examples_per_sec = self._args[""batch_size""] / duration\n                        sec_per_batch = float(duration)\n\n                        format_str = (\'{}: step {}, loss = {:.4f} \'\n                                      \'({:.1f} examples/sec; {:.3f} sec/batch)\')\n                        print(\n                            format_str.format(datetime.now(), step, loss_value,\n                                              examples_per_sec, sec_per_batch))\n                        # log train values\n                        summary_lines = sess.run(\n                            scalar_summaries, feed_dict={\n                                is_training_: True\n                            })\n                        train_log.add_summary(summary_lines, global_step=step)\n\n                    # Save the model checkpoint at the end of every epoch\n                    # evaluate train and validation performance\n                    if (step > 0 and step % self._steps[""epoch""] == 0\n                       ) or step == self._steps[""max""]:\n                        checkpoint_path = os.path.join(self._paths[""log""],\n                                                       \'model.ckpt\')\n                        train_saver.save(\n                            sess, checkpoint_path, global_step=step)\n\n                        # execute media summaries and the end of every epoch\n                        # keeping the model in train mode\n                        summary_lines = sess.run(\n                            media_summaries, feed_dict={\n                                is_training_: True\n                            })\n                        train_log.add_summary(summary_lines, global_step=step)\n\n                        # arrays of validation measures\n                        validation_measured_metrics = []\n                        # ta value is the model selection metric evaluate on the training\n                        # set, useful just to output on the CLI\n                        ta_value = 0\n                        for idx, metric in enumerate(metrics_to_measure):\n                            # validation metrics\n                            validation_measured_metrics.append(\n                                self._model.evaluator.eval(\n                                    metric,\n                                    self._paths[""log""],\n                                    input_type=InputType.validation,\n                                    batch_size=self._args[""batch_size""]))\n                            validation_log.add_summary(\n                                sess.run(\n                                    metric_summaries[idx],\n                                    feed_dict={\n                                        metric_values_[idx]:\n                                        validation_measured_metrics[idx]\n                                    }),\n                                global_step=step)\n\n                            # Repeat measurement on the training set\n                            measure = self._model.evaluator.eval(\n                                metric,\n                                self._paths[""log""],\n                                input_type=InputType.train,\n                                batch_size=self._args[""batch_size""])\n                            train_log.add_summary(\n                                sess.run(\n                                    metric_summaries[idx],\n                                    feed_dict={\n                                        metric_values_[idx]: measure\n                                    }),\n                                global_step=step)\n\n                            # fill ta_value\n                            if idx == model_selection_idx:\n                                ta_value = measure\n\n                        # visualization\n                        for idx, viz in enumerate(visualizations_to_measure):\n                            # validation metrics\n                            measured_viz = self._model.evaluator.visualize(\n                                viz,\n                                self._paths[""log""],\n                                input_type=InputType.validation,\n                                batch_size=self._args[""batch_size""])\n                            validation_log.add_summary(\n                                sess.run(\n                                    visualization_summaries[idx],\n                                    feed_dict={\n                                        visualization_values_[idx]: measured_viz\n                                    }),\n                                global_step=step)\n\n                            # Repeat measurement on the training set\n                            measured_viz = self._model.evaluator.visualize(\n                                viz,\n                                self._paths[""log""],\n                                input_type=InputType.train,\n                                batch_size=self._args[""batch_size""])\n                            train_log.add_summary(\n                                sess.run(\n                                    visualization_summaries[idx],\n                                    feed_dict={\n                                        visualization_values_[idx]: measured_viz\n                                    }),\n                                global_step=step)\n\n                        name = self._model.evaluator.metrics[\n                            model_selection_idx][""name""]\n\n                        print(\n                            \'{} ({}): train {} = {:.3f} validation {} = {:.3f}\'.\n                            format(datetime.now(),\n                                   int(step / self._steps[""epoch""]), name,\n                                   ta_value, name, validation_measured_metrics[\n                                       model_selection_idx]))\n\n                        # save best model\n                        sign = math.copysign(\n                            1,\n                            validation_measured_metrics[model_selection_idx] -\n                            best_model_selection_measure)\n                        if sign == self._model.evaluator.metrics[\n                                model_selection_idx][""positive_trend_sign""]:\n                            best_model_selection_measure = validation_measured_metrics[\n                                model_selection_idx]\n                            best_saver.save(\n                                sess,\n                                os.path.join(self._paths[""best""], \'model.ckpt\'),\n                                global_step=step)\n                        # end of metrics\n\n                        # end of visualizations\n                        # end of for\n                validation_log.close()\n                train_log.close()\n\n                # When done, ask the threads to stop.\n                coord.request_stop()\n                # Wait for threads to finish.\n                coord.join(threads)\n\n            stats = self._model.evaluator.stats(\n                self._paths[""best""], batch_size=self._args[""batch_size""])\n            self._model.info = {\n                ""args"": self._args,\n                ""paths"": self._paths,\n                ""steps"": self._steps,\n                ""stats"": stats\n            }\n            return self._model.info\n'"
dytb/trainer/__init__.py,0,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n'"
dytb/utils/CLIArgs.py,1,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Class that defines and parse CLI arguments""""""\n\nimport os\nimport glob\nimport argparse\nimport json\nimport importlib\nimport pprint\nimport sys\nimport tensorflow as tf\n\n\nclass CLIArgs(object):\n    """"""Class that defines and parse CLI arguments""""""\n\n    def __init__(self, description=""Train the model""):\n        """"""Initialize variables:\n        Args:\n            description: The description to show when the help is displayed""""""\n        self._description = description\n        self._args = None\n\n    @staticmethod\n    def get_dytb_models():\n        """"""Returns the avaiable dytb modules filename, without the .py ext""""""\n        dytbmodels_dir = os.path.join(\n            os.path.dirname(os.path.abspath(__file__)), os.path.pardir,\n            \'models\', \'predefined\')\n        dytbmodels = [\n            model[len(dytbmodels_dir) + 1:-3]\n            for model in glob.glob(\'{}/*.py\'.format(dytbmodels_dir))\n            if ""__init__.py"" not in model\n        ]\n        return dytbmodels\n\n    @staticmethod\n    def get_dytb_datasets():\n        """"""Returns the avaiable dytb datasets filename, without the .py ext""""""\n        dytbdatasets_dir = os.path.join(\n            os.path.dirname(os.path.abspath(__file__)), os.path.pardir,\n            \'inputs\', \'predefined\')\n        dytbdatasets = [\n            dataset[len(dytbdatasets_dir) + 1:-3]\n            for dataset in glob.glob(\'{}/*.py\'.format(dytbdatasets_dir))\n            if ""__init__.py"" not in dataset\n        ]\n        return dytbdatasets\n\n    @staticmethod\n    def get_local_models():\n        """"""Returns the avaiable modules filename, without the .py ext""""""\n        models_dir = os.path.join(os.getcwd(), \'models\')\n        return [\n            model[len(models_dir) + 1:-3]\n            for model in glob.glob(\'{}/*.py\'.format(models_dir))\n            if ""__init__.py"" not in model\n        ]\n\n    @staticmethod\n    def get_local_datasets():\n        """"""Returns the avaiable datasets filename, without the .py ext""""""\n        datasets_dir = os.path.join(os.getcwd(), \'inputs\')\n        return [\n            dataset[len(datasets_dir) + 1:-3]\n            for dataset in glob.glob(\'{}/*.py\'.format(datasets_dir))\n            if ""__init__.py"" not in dataset\n        ]\n\n    @staticmethod\n    def get_optimizers():\n        """"""Returns the avaiable Tensorflow optimizer""""""\n        return [\n            optimizer for optimizer in dir(tf.train)\n            if optimizer.endswith(""Optimizer"")\n        ]\n\n    def _init_parser(self):\n        """"""Parse CLI flags shared by train & eval proceudres.\n        Returns:\n            parser: parser object""""""\n\n        # CLI arguments\n        parser = argparse.ArgumentParser(description=self._description)\n\n        # Required arguments\n        parser.add_argument(\n            \'--model\',\n            required=True,\n            choices=self.get_dytb_models() + self.get_local_models())\n        parser.add_argument(\n            \'--dataset\',\n            required=True,\n            choices=self.get_dytb_datasets() + self.get_local_datasets())\n        parser.add_argument(\'--batch_size\', type=int, default=128)\n\n        return parser\n\n    def _get_model_dataset(self):\n        """"""Return the model object and the dataset object.\n        Returns:\n            model: model object instantiated\n            dataset: input object instantiated""""""\n\n        sys.path.append(os.getcwd())\n\n        # Instantiate the model object\n        # Give the precedence to local models\n        if self._args.model in self.get_local_models():\n            model = getattr(\n                importlib.import_module(\'models.\' + self._args.model),\n                self._args.model)()\n        else:\n            model = getattr(\n                importlib.import_module(\n                    \'dytb.models.predefined.\' + self._args.model),\n                self._args.model)()\n\n        # Instantiate the input object\n        # Give the precedente to local datasets\n        if self._args.dataset in self.get_local_datasets():\n            dataset = getattr(\n                importlib.import_module(\'inputs.\' + self._args.dataset),\n                self._args.dataset)()\n        else:\n            dataset = getattr(\n                importlib.import_module(\n                    \'dytb.inputs.predefined.\' + self._args.dataset),\n                self._args.dataset)()\n\n        return model, dataset\n\n    def parse_eval(self):\n        """"""Parser the CLI arguments for the evaluation procedure\n        and return\n        Returns:\n            args: args object\n            model: model object instantiated\n            dataset: input object instantiated""""""\n\n        parser = self._init_parser()\n        parser.add_argument(\n            ""--checkpoint_path"",\n            required=True,\n            help=\'the path to a checkpoint from which load the model\')\n        parser.add_argument(""--test"", action=""store_true"", help=\'use test set\')\n\n        # Hardware\n        parser.add_argument(\'--eval_device\', default=\'/gpu:0\')\n        self._args = parser.parse_args()\n        # Get model and dataset objects\n        model, dataset = self._get_model_dataset()\n        return self._args, model, dataset\n\n    def parse_train(self):\n        """"""Parser the CLI arguments for the training procedure\n        and return\n        Returns:\n            args: args object\n            model: model object instantiated\n            dataset: input object instantiated\n        """"""\n\n        parser = self._init_parser()\n\n        # Restart train or continue\n        parser.add_argument(\n            \'--restart\',\n            action=\'store_true\',\n            help=\'restart the training process DELETING the old checkpoint files\'\n        )\n\n        # Learning rate decay arguments\n        parser.add_argument(\n            \'--lr_decay\',\n            action=\'store_true\',\n            help=\'enable the learning rate decay\')\n        parser.add_argument(\n            \'--lr_decay_epochs\',\n            type=int,\n            default=25,\n            help=\'decay the learning rate every lr_decay_epochs epochs\')\n        parser.add_argument(\n            \'--lr_decay_factor\',\n            type=float,\n            default=0.1,\n            help=\n            \'decay of lr_decay_factor the initial learning rate after lr_decay_epochs epochs\'\n        )\n\n        # L2 regularization arguments\n        parser.add_argument(\n            \'--l2_penalty\',\n            type=float,\n            default=0.0,\n            help=\'L2 penalty term to apply ad the trained parameters\')\n\n        # Optimization arguments\n        parser.add_argument(\n            \'--optimizer\',\n            choices=self.get_optimizers(),\n            default=\'MomentumOptimizer\',\n            help=\'the optimizer to use\')\n        parser.add_argument(\n            \'--optimizer_args\',\n            type=json.loads,\n            default=\'\'\'\n        {\n            ""learning_rate"": 1e-2,\n            ""momentum"": 0.9\n        }\'\'\',\n            help=\'the optimizer parameters\')\n        parser.add_argument(\n            \'--epochs\',\n            type=int,\n            default=150,\n            help=\'number of epochs to train the model\')\n\n        # Hardware\n        parser.add_argument(\n            \'--train_device\',\n            default=\'/gpu:0\',\n            help=\n            \'the device on which place the the model during the trining phase\')\n\n        # Optional comment\n        parser.add_argument(\n            \'--comment\',\n            default=\'\',\n            help=\'comment string to preprend to the model name\')\n\n        # Fine tuning & graph manipulation\n        parser.add_argument(\n            \'--exclude_scopes\',\n            help=\'comma separated list of scopes of variables to exclude from the checkpoint restoring.\',\n            default=None,\n            type=lambda scope_list: [scope.strip() for scope in scope_list.split(\',\')])\n\n        parser.add_argument(\n            \'--trainable_scopes\',\n            help=\'comma separated list of scopes of variables to train. If empty every variable is trained\',\n            default=None,\n            type=lambda scope_list: [scope.strip() for scope in scope_list.split(\',\')])\n\n        parser.add_argument(\n            ""--checkpoint_path"",\n            required=False,\n            default=\'\',\n            help=\'the path to a checkpoint from which load the model\')\n\n        # Build the object\n        self._args = parser.parse_args()\n\n        # Get model and dataset objects\n        model, dataset = self._get_model_dataset()\n\n        print(\'Args: {}\'.format(pprint.pformat(vars(self._args), indent=4)))\n\n        return self._args, model, dataset\n'"
dytb/utils/__init__.py,0,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n'"
scripts/inputs/__init__.py,0,b''
scripts/models/__init__.py,0,b''
dytb/inputs/predefined/Cifar10.py,11,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n# Adapted from:\n# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_input.py\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Routine for decoding the CIFAR-10 binary file format.""""""\n\nimport os\nimport sys\nimport tarfile\n\nfrom six.moves import urllib\nimport tensorflow as tf\nfrom ..processing import build_batch\nfrom ..images import scale_image\nfrom ..interfaces import Input, InputType\n\n\nclass Cifar10(Input):\n    """"""Routine for decoding the CIFAR-10 binary file format.""""""\n\n    def __init__(self, add_input_to_label=False):\n        # Global constants describing the CIFAR-10 data set.\n        self._name = \'CIFAR-10\'\n        self._image_height = 32\n        self._image_width = 32\n        self._image_depth = 3\n\n        self._num_classes = 10\n        self._num_examples_per_epoch_for_train = 50000\n        self._num_examples_per_epoch_for_eval = 10000\n        self._num_examples_per_epoch_for_test = self._num_examples_per_epoch_for_eval\n\n        self._data_dir = os.path.join(\n            os.path.dirname(os.path.abspath(__file__)), \'data\', \'Cifar10\')\n        self._data_url = \'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\'\n        self._maybe_download_and_extract()\n        self._add_input_to_label = add_input_to_label\n\n    def num_examples(self, input_type):\n        """"""Returns the number of examples per the specified input_type\n\n        Args:\n            input_type: InputType enum\n        """"""\n        InputType.check(input_type)\n\n        if input_type == InputType.train:\n            return self._num_examples_per_epoch_for_train\n        elif input_type == InputType.test:\n            return self._num_examples_per_epoch_for_test\n        return self._num_examples_per_epoch_for_eval\n\n    @property\n    def num_classes(self):\n        """"""Returns the number of classes""""""\n        return self._num_classes\n\n    @property\n    def name(self):\n        """"""Returns the name of the input source""""""\n        return self._name\n\n    def _read(self, filename_queue):\n        """"""Reads and parses examples from CIFAR10 data files.\n\n      Recommendation: if you want N-way read parallelism, call this function\n      N times.  This will give you N independent Readers reading different\n      files & positions within those files, which will give better mixing of\n      examples.\n\n      Args:\n        filename_queue: A queue of strings with the filenames to read from.\n\n      Returns:\n        An object representing a single example, with the following fields:\n          height: number of rows in the result (32)\n          width: number of columns in the result (32)\n          depth: number of color channels in the result (3)\n          key: a scalar string Tensor describing the filename & record number\n            for this example.\n          label: an int32 Tensor with the label in the range 0..9.\n          image: a [height, width, depth] uint8 Tensor with the image data\n      """"""\n\n        # Dimensions of the images in the CIFAR-10 dataset.\n        # See http://www.cs.toronto.edu/~kriz/cifar.html for a description of the\n        # input format.\n        result = {\n            ""height"": self._image_height,\n            ""width"": self._image_width,\n            ""depth"": self._image_depth,\n            ""label"": None,\n            ""image"": None\n        }\n\n        image_bytes = result[""height""] * result[""width""] * result[""depth""]\n        # Every record consists of a label followed by the image, with a\n        # fixed number of bytes for each.\n        label_bytes = 1  # 2 for CIFAR-100\n        record_bytes = label_bytes + image_bytes\n\n        # Read a record, getting filenames from the filename_queue.  No\n        # header or footer in the CIFAR-10 format, so we leave header_bytes\n        # and footer_bytes at their default of 0.\n        reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\n        _, value = reader.read(filename_queue)\n\n        # Convert from a string to a vector of uint8 that is record_bytes long.\n        record_bytes = tf.decode_raw(value, tf.uint8)\n\n        # The first bytes represent the label, which we convert from uint8->int32.\n        result[""label""] = tf.squeeze(\n            tf.cast(tf.slice(record_bytes, [0], [label_bytes]), tf.int32))\n\n        # The remaining bytes after the label represent the image, which we reshape\n        # from [depth * height * width] to [depth, height, width].\n        depth_major = tf.reshape(\n            tf.slice(record_bytes, [label_bytes], [image_bytes]),\n            [result[""depth""], result[""height""], result[""width""]])\n\n        # Convert from [depth, height, width] to [height, width, depth].\n        image = tf.cast(tf.transpose(depth_major, [1, 2, 0]), tf.float32)\n\n        # Convert from [0, 255] -> [0, 1]\n        image = tf.divide(image, 255.0)\n\n        # Convert from [0, 1] -> [-1, 1]\n        result[""image""] = scale_image(image)\n\n        return result\n\n    def inputs(self, input_type, batch_size, augmentation_fn=None):\n        """"""Construct input for CIFAR evaluation using the Reader ops.\n\n        Args:\n            input_type: InputType enum\n            batch_size: Number of images per batch.\n\n        Returns:\n            images: Images. 4D tensor of [batch_size, self._image_height, self._image_width, self._image_depth] size.\n            labels: Labels. 1D tensor of [batch_size] size.\n        """"""\n        InputType.check(input_type)\n\n        if input_type == InputType.train:\n            filenames = [\n                os.path.join(self._data_dir,\n                             \'cifar-10-batches-bin/data_batch_%d.bin\' % i)\n                for i in range(1, 6)\n            ]\n            num_examples_per_epoch = self._num_examples_per_epoch_for_train\n        else:\n            filenames = [\n                os.path.join(self._data_dir,\n                             \'cifar-10-batches-bin/test_batch.bin\')\n            ]\n            num_examples_per_epoch = self._num_examples_per_epoch_for_eval\n\n        for name in filenames:\n            if not tf.gfile.Exists(name):\n                raise ValueError(\'Failed to find file: \' + name)\n\n        with tf.variable_scope(""{}_input"".format(input_type)):\n            # Create a queue that produces the filenames to read.\n            filename_queue = tf.train.string_input_producer(filenames)\n\n            # Read examples from files in the filename queue.\n            read_input = self._read(filename_queue)\n            if augmentation_fn:\n                read_input[""image""] = augmentation_fn(read_input[""image""])\n\n            # Ensure that the random shuffling has good mixing properties.\n            min_fraction_of_examples_in_queue = 0.4\n            min_queue_examples = int(\n                num_examples_per_epoch * min_fraction_of_examples_in_queue)\n\n            # Generate a batch of images and labels by building up a queue of examples.\n            return build_batch(\n                read_input[""image""],\n                read_input[""label""] if not self._add_input_to_label else\n                [read_input[""label""], read_input[""image""]],\n                min_queue_examples,\n                batch_size,\n                shuffle=input_type == InputType.train)\n\n    def _maybe_download_and_extract(self):\n        """"""Download and extract the tarball from Alex\'s website.""""""\n        dest_directory = self._data_dir\n        if not os.path.exists(dest_directory):\n            os.makedirs(dest_directory)\n        filename = self._data_url.split(\'/\')[-1]\n        filepath = os.path.join(dest_directory, filename)\n        if not os.path.exists(filepath):\n\n            def _progress(count, block_size, total_size):\n                sys.stdout.write(\n                    \'\\r>> Downloading %s %.1f%%\' %\n                    (filename,\n                     float(count * block_size) / float(total_size) * 100.0))\n                sys.stdout.flush()\n\n            filepath, _ = urllib.request.urlretrieve(self._data_url, filepath,\n                                                     _progress)\n            print()\n            statinfo = os.stat(filepath)\n            print(\'Successfully downloaded\', filename, statinfo.st_size,\n                  \'bytes.\')\n            tarfile.open(filepath, \'r:gz\').extractall(dest_directory)\n'"
dytb/inputs/predefined/Cifar100.py,11,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n# Conversion of cifar10_input:\n# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_input.py\n# For the cifar100 dataset.\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Routine for decoding the CIFAR-100 binary file format.""""""\n\nimport os\nimport sys\nimport tarfile\n\nfrom six.moves import urllib\nimport tensorflow as tf\nfrom ..processing import build_batch\nfrom ..images import scale_image\nfrom ..interfaces import Input, InputType\n\n\nclass Cifar100(Input):\n    """"""Routine for decoding the CIFAR-100 binary file format.""""""\n\n    def __init__(self, add_input_to_label=False):\n        # Global constants describing the CIFAR-100 data set.\n        self._name = \'CIFAR-100\'\n        self._image_height = 32\n        self._image_width = 32\n        self._image_depth = 3\n\n        self._num_classes = 100\n        self._num_examples_per_epoch_for_train = 50000\n        self._num_examples_per_epoch_for_eval = 10000\n        self._num_examples_per_epoch_for_test = self._num_examples_per_epoch_for_eval\n\n        self._data_dir = os.path.join(\n            os.path.dirname(os.path.abspath(__file__)), \'data\', \'Cifar100\')\n        self._data_url = \'http://www.cs.toronto.edu/~kriz/cifar-100-binary.tar.gz\'\n        self._maybe_download_and_extract()\n        self._add_input_to_label = add_input_to_label\n\n    def num_examples(self, input_type):\n        """"""Returns the number of examples per the specified input_type\n\n        Args:\n            input_type: InputType enum\n        """"""\n        InputType.check(input_type)\n\n        if input_type == InputType.train:\n            return self._num_examples_per_epoch_for_train\n        elif input_type == InputType.test:\n            return self._num_examples_per_epoch_for_test\n        return self._num_examples_per_epoch_for_eval\n\n    @property\n    def num_classes(self):\n        """"""Returns the number of classes""""""\n        return self._num_classes\n\n    @property\n    def name(self):\n        """"""Returns the name of the input source""""""\n        return self._name\n\n    def _read(self, filename_queue):\n        """"""Reads and parses examples from CIFAR10 data files.\n\n      Recommendation: if you want N-way read parallelism, call this function\n      N times.  This will give you N independent Readers reading different\n      files & positions within those files, which will give better mixing of\n      examples.\n\n      Args:\n        filename_queue: A queue of strings with the filenames to read from.\n\n      Returns:\n        An object representing a single example, with the following fields:\n          height: number of rows in the result (32)\n          width: number of columns in the result (32)\n          depth: number of color channels in the result (3)\n          key: a scalar string Tensor describing the filename & record number\n            for this example.\n          label: an int32 Tensor with the label in the range 0..9.\n          image: a [height, width, depth] uint8 Tensor with the image data\n      """"""\n\n        # Dimensions of the images in the CIFAR-10 dataset.\n        # See http://www.cs.toronto.edu/~kriz/cifar.html for a description of the\n        # input format.\n        result = {\n            ""height"": self._image_height,\n            ""width"": self._image_width,\n            ""depth"": self._image_depth,\n            ""label"": None,\n            ""image"": None\n        }\n\n        image_bytes = result[""height""] * result[""width""] * result[""depth""]\n        # Every record consists of a label followed by the image, with a\n        # fixed number of bytes for each.\n        label_bytes = 2  # 2 for CIFAR-100\n        record_bytes = label_bytes + image_bytes\n\n        # Read a record, getting filenames from the filename_queue.  No\n        # header or footer in the CIFAR-100 format, so we leave header_bytes\n        # and footer_bytes at their default of 0.\n        reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\n        _, value = reader.read(filename_queue)\n\n        # Convert from a string to a vector of uint8 that is record_bytes long.\n        record_bytes = tf.decode_raw(value, tf.uint8)\n\n        # The first byte represent the coarse-label.\n        # Extract the second byte that\'s the fine-label and convert it from uint8->int32.\n        result[""label""] = tf.squeeze(\n            tf.cast(tf.slice(record_bytes, [1], [label_bytes - 1]), tf.int32))\n\n        # The remaining bytes after the label represent the image, which we reshape\n        # from [depth * height * width] to [depth, height, width].\n        depth_major = tf.reshape(\n            tf.slice(record_bytes, [label_bytes], [image_bytes]),\n            [result[""depth""], result[""height""], result[""width""]])\n\n        # Convert from [depth, height, width] to [height, width, depth].\n        image = tf.cast(tf.transpose(depth_major, [1, 2, 0]), tf.float32)\n\n        # Convert from [0, 255] -> [0, 1]\n        image = tf.divide(image, 255.0)\n\n        # Convert from [0, 1] -> [-1, 1]\n        result[""image""] = scale_image(image)\n\n        return result\n\n    def inputs(self, input_type, batch_size, augmentation_fn=None):\n        """"""Construct input for CIFAR evaluation using the Reader ops.\n\n        Args:\n            input_type: InputType enum\n            batch_size: Number of images per batch.\n\n        Returns:\n            images: Images. 4D tensor of [batch_size, self._image_height, self._image_width, self._image_depth] size.\n            labels: Labels. 1D tensor of [batch_size] size.\n        """"""\n        InputType.check(input_type)\n\n        if input_type == InputType.train:\n            filename = os.path.join(self._data_dir,\n                                    \'cifar-100-binary/train.bin\')\n            num_examples_per_epoch = self._num_examples_per_epoch_for_train\n        else:\n            filename = os.path.join(self._data_dir, \'cifar-100-binary/test.bin\')\n            num_examples_per_epoch = self._num_examples_per_epoch_for_eval\n\n        if not tf.gfile.Exists(filename):\n            raise ValueError(\'Failed to find file: \' + filename)\n\n        with tf.variable_scope(""{}_input"".format(input_type)):\n            # Create a queue that produces the filenames to read.\n            filename_queue = tf.train.string_input_producer([filename])\n\n            # Read examples from files in the filename queue.\n            read_input = self._read(filename_queue)\n            if augmentation_fn:\n                read_input[""image""] = augmentation_fn(read_input[""image""])\n\n            # Ensure that the random shuffling has good mixing properties.\n            min_fraction_of_examples_in_queue = 0.4\n            min_queue_examples = int(\n                num_examples_per_epoch * min_fraction_of_examples_in_queue)\n\n            # Generate a batch of images and labels by building up a queue of examples.\n            return build_batch(\n                read_input[""image""],\n                read_input[""label""] if not self._add_input_to_label else\n                [read_input[""label""], read_input[""image""]],\n                min_queue_examples,\n                batch_size,\n                shuffle=input_type == InputType.train)\n\n    def _maybe_download_and_extract(self):\n        """"""Download and extract the tarball from Alex\'s website.""""""\n        dest_directory = self._data_dir\n        if not os.path.exists(dest_directory):\n            os.makedirs(dest_directory)\n        filename = self._data_url.split(\'/\')[-1]\n        filepath = os.path.join(dest_directory, filename)\n        if not os.path.exists(filepath):\n\n            def _progress(count, block_size, total_size):\n                sys.stdout.write(\n                    \'\\r>> Downloading %s %.1f%%\' %\n                    (filename,\n                     float(count * block_size) / float(total_size) * 100.0))\n                sys.stdout.flush()\n\n            filepath, _ = urllib.request.urlretrieve(self._data_url, filepath,\n                                                     _progress)\n            print()\n            statinfo = os.stat(filepath)\n            print(\'Successfully downloaded\', filename, statinfo.st_size,\n                  \'bytes.\')\n            tarfile.open(filepath, \'r:gz\').extractall(dest_directory)\n'"
dytb/inputs/predefined/MNIST.py,17,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Routine for decoding the MNIST binary file format.""""""\n\nimport os\n\nimport tensorflow as tf\nfrom tensorflow.contrib.learn.python.learn.datasets import mnist\nfrom ..processing import convert_to_tfrecords, build_batch\nfrom ..images import scale_image\nfrom ..interfaces import Input, InputType\n\n\nclass MNIST(Input):\n    """"""Routine for decoding the MNIST binary file format.""""""\n\n    def __init__(self, resize=(28, 28, 1), add_input_to_label=False):\n        # Global constants describing the MNIST data set.\n        self._name = \'MNIST\'\n        self._original_shape = (28, 28, 1)\n        mnist.IMAGE_PIXELS = 28 * 28\n        self._image_width = resize[0]\n        self._image_height = resize[1]\n        self._image_depth = resize[2]\n\n        self._num_classes = 10\n        self._num_examples_per_epoch_for_train = 55000\n        self._num_examples_per_epoch_for_eval = 5000\n        self._num_examples_per_epoch_for_test = 10000\n\n        self._data_dir = os.path.join(\n            os.path.dirname(os.path.abspath(__file__)), \'data\', \'MNIST\')\n        self._maybe_download_and_extract()\n        self._add_input_to_label = add_input_to_label\n\n    def num_examples(self, input_type):\n        """"""Returns the number of examples per the specified input_type\n\n        Args:\n            input_type: InputType enum\n        """"""\n        InputType.check(input_type)\n\n        if input_type == InputType.train:\n            return self._num_examples_per_epoch_for_train\n        elif input_type == InputType.test:\n            return self._num_examples_per_epoch_for_test\n        return self._num_examples_per_epoch_for_eval\n\n    @property\n    def num_classes(self):\n        """"""Returns the number of classes""""""\n        return self._num_classes\n\n    @property\n    def name(self):\n        """"""Returns the name of the input source""""""\n        return self._name\n\n    # adapted from:\n    # https://github.com/tensorflow/tensorflow/blob/r0.11/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py\n    def _read(self, filename_queue):\n        """"""Reads and parses examples from MNIST data files.\n        Recommendation: if you want N-way read parallelism, call this function\n        N times.  This will give you N independent Readers reading different\n        files & positions within those files, which will give better mixing of\n        examples.\n\n        Args:\n            filename_queue: A queue of strings with the filenames to read from.\n\n        Returns:\n          An object representing a single example, with the following fields:\n              label: an int32 Tensor with the label in the range 0..9.\n              image: a [height, width, depth] uint8 Tensor with the image data\n        """"""\n\n        result = {\'image\': None, \'label\': None}\n\n        reader = tf.TFRecordReader()\n        _, value = reader.read(filename_queue)\n        features = tf.parse_single_example(\n            value,\n            features={\n                \'image_raw\': tf.FixedLenFeature([], tf.string),\n                # int64 required\n                \'label\': tf.FixedLenFeature([], tf.int64)\n            })\n\n        # Convert from a scalar string tensor (whose single string has\n        # length mnist.IMAGE_PIXELS) to a uint8 tensor with shape\n        # [mnist.IMAGE_PIXELS].\n        image = tf.decode_raw(features[\'image_raw\'], tf.uint8)\n        image.set_shape([mnist.IMAGE_PIXELS])\n\n        # Reshape to a valid image\n        image = tf.reshape(image, self._original_shape)\n        # Resize to the selected shape\n        image = tf.squeeze(\n            tf.image.resize_bilinear(\n                tf.expand_dims(image, axis=0),\n                [self._image_height, self._image_width]),\n            axis=[0])\n\n        # Convert from [0, 255] -> [0, 1]\n        image = tf.divide(tf.cast(image, tf.float32), 255.0)\n        # Convert from [0, 1] -> [-1, 1]\n        result[""image""] = scale_image(image)\n\n        # Convert label from a scalar uint8 tensor to an int32 scalar.\n        result[""label""] = tf.cast(features[\'label\'], tf.int32)\n        return result\n\n    def inputs(self, input_type, batch_size, augmentation_fn=None):\n        """"""Construct input for MNIST evaluation using the Reader ops.\n\n        Args:\n            input_type: InputType enum.\n            batch_size: Number of images per batch.\n\n        Returns:\n            images: Images. 4D tensor of [batch_size, resize[0], resize[1], resize[2]] size.\n            labels: Labels. 1D tensor of [batch_size] size.\n        """"""\n        InputType.check(input_type)\n\n        if input_type == InputType.train:\n            filename = os.path.join(self._data_dir, \'train.tfrecords\')\n            num_examples_per_epoch = self._num_examples_per_epoch_for_train\n        elif input_type == InputType.validation:\n            filename = os.path.join(self._data_dir, \'validation.tfrecords\')\n            num_examples_per_epoch = self._num_examples_per_epoch_for_eval\n        elif input_type == InputType.test:\n            filename = os.path.join(self._data_dir, \'test.tfrecords\')\n            num_examples_per_epoch = self._num_examples_per_epoch_for_test\n\n        with tf.variable_scope(""{}_input"".format(input_type)):\n            # Create a queue that produces the filenames to read.\n            filename_queue = tf.train.string_input_producer([filename])\n\n            # Read examples from files in the filename queue.\n            read_input = self._read(filename_queue)\n            if augmentation_fn:\n                read_input[""image""] = augmentation_fn(read_input[""image""])\n\n            # Ensure that the random shuffling has good mixing properties.\n            min_fraction_of_examples_in_queue = 0.4\n            min_queue_examples = int(\n                num_examples_per_epoch * min_fraction_of_examples_in_queue)\n\n            # Generate a batch of images and labels by building up a queue of examples.\n            return build_batch(\n                read_input[""image""],\n                read_input[""label""] if not self._add_input_to_label else\n                [read_input[""label""], read_input[""image""]],\n                min_queue_examples,\n                batch_size,\n                shuffle=input_type == InputType.train)\n\n    def _maybe_download_and_extract(self):\n        """"""Download and extract the MNIST dataset""""""\n        data_sets = mnist.read_data_sets(\n            self._data_dir,\n            dtype=tf.uint8,\n            reshape=False,\n            validation_size=self._num_examples_per_epoch_for_eval)\n\n        # Convert to Examples and write the result to TFRecords.\n        if not tf.gfile.Exists(os.path.join(self._data_dir, \'train.tfrecords\')):\n            convert_to_tfrecords(data_sets.train, \'train\', self._data_dir)\n\n        if not tf.gfile.Exists(\n                os.path.join(self._data_dir, \'validation.tfrecords\')):\n            convert_to_tfrecords(data_sets.validation, \'validation\',\n                                 self._data_dir)\n\n        if not tf.gfile.Exists(os.path.join(self._data_dir, \'test.tfrecords\')):\n            convert_to_tfrecords(data_sets.test, \'test\', self._data_dir)\n'"
dytb/inputs/predefined/ORLFaces.py,11,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""ORL Faces database input""""""\n\nimport os\nimport sys\nimport zipfile\nimport glob\nfrom PIL import Image\n\nfrom six.moves import urllib\nimport tensorflow as tf\nimport numpy as np\nfrom ..processing import convert_to_tfrecords, build_batch\nfrom ..images import scale_image\nfrom ..interfaces import Input, InputType\n\n\nclass ORLFaces(Input):\n    """"""ORL Faces database input""""""\n\n    def __init__(self, add_input_to_label=False):\n        # Global constants describing the ORL Faces data set.\n        self._name = \'ORL-Faces\'\n        self._image_width = 92\n        self._image_height = 112\n        self._image_depth = 1\n\n        self._num_classes = 40\n        self._num_examples_per_epoch_for_train = 400\n        self._num_examples_per_epoch_for_eval = 0\n        self._num_examples_per_epoch_for_test = 0\n\n        self._data_dir = os.path.join(\n            os.path.dirname(os.path.abspath(__file__)), \'data\', \'ORLFaces\')\n        self._data_url = \'http://www.cl.cam.ac.uk/Research/DTG/attarchive/pub/data/att_faces.zip\'\n        self._maybe_download_and_extract()\n        self._add_input_to_label = add_input_to_label\n\n    def num_examples(self, input_type):\n        """"""Returns the number of examples per the specified input_type\n\n        Args:\n            input_type: InputType enum\n        """"""\n        InputType.check(input_type)\n\n        if input_type == InputType.train:\n            return self._num_examples_per_epoch_for_train\n        elif input_type == InputType.test:\n            return self._num_examples_per_epoch_for_test\n        return self._num_examples_per_epoch_for_eval\n\n    @property\n    def num_classes(self):\n        """"""Returns the number of classes""""""\n        return self._num_classes\n\n    @property\n    def name(self):\n        """"""Returns the name of the input source""""""\n        return self._name\n\n    # adapted from:\n    # https://github.com/tensorflow/tensorflow/blob/r0.11/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py\n    def _read(self, filename_queue):\n        """"""Reads and parses examples from MNIST data files.\n        Recommendation: if you want N-way read parallelism, call this function\n        N times.  This will give you N independent Readers reading different\n        files & positions within those files, which will give better mixing of\n        examples.\n\n        Args:\n            filename_queue: A queue of strings with the filenames to read from.\n\n        Returns:\n          An object representing a single example, with the following fields:\n              label: an int32 Tensor with the label in the range 0..9.\n              image: a [height, width, depth] uint8 Tensor with the image data\n        """"""\n\n        result = {\'image\': None, \'label\': None}\n\n        reader = tf.TFRecordReader()\n        _, value = reader.read(filename_queue)\n        features = tf.parse_single_example(\n            value,\n            features={\n                \'image_raw\': tf.FixedLenFeature([], tf.string),\n                # int64 required\n                \'label\': tf.FixedLenFeature([], tf.int64)\n            })\n\n        # Convert from a scalar string tensor (whose single string has\n        # length IMAGE_WIDHT * self._image_height) to a uint8 tensor with\n        # the same shape\n        image = tf.decode_raw(features[\'image_raw\'], tf.uint8)\n        image.set_shape([self._image_width * self._image_height])\n\n        #`Reshape to a valid image\n        image = tf.reshape(\n            image, (self._image_height, self._image_width, self._image_depth))\n\n        # Convert from [0, 255] -> [0, 1] floats.\n        image = tf.divide(tf.cast(image, tf.float32), 255.0)\n\n        # Convert from [0, 1] -> [-1, 1]\n        result[""image""] = scale_image(image)\n\n        # Convert label from a scalar uint8 tensor to an int32 scalar.\n        result[""label""] = tf.cast(features[\'label\'], tf.int32)\n\n        return result\n\n    def inputs(self, input_type, batch_size, augmentation_fn=None):\n        """"""Construct input for ORL Faces evaluation using the Reader ops.\n\n        Args:\n            input_type: InputType enum.\n            batch_size: Number of images per batch.\n\n        Returns:\n            images: Images. 4D tensor of [batch_size, self._image_width, self._image_height, self._image_depth] size.\n            labels: Labels. 1D tensor of [batch_size] size.\n        """"""\n        InputType.check(input_type)\n\n        with tf.variable_scope(""{}_input"".format(input_type)):\n            filename = os.path.join(self._data_dir, \'faces.tfrecords\')\n            num_examples_per_epoch = self._num_examples_per_epoch_for_train\n\n            # Create a queue that produces the filenames to read.\n            filename_queue = tf.train.string_input_producer([filename])\n\n            # Read examples from files in the filename queue.\n            read_input = self._read(filename_queue)\n            if augmentation_fn:\n                read_input[""image""] = augmentation_fn(read_input[""image""])\n\n            # Ensure that the random shuffling has good mixing properties.\n            min_fraction_of_examples_in_queue = 0.4\n            min_queue_examples = int(\n                num_examples_per_epoch * min_fraction_of_examples_in_queue)\n\n            # Generate a batch of images and labels by building up a queue of examples.\n            return build_batch(\n                read_input[""image""],\n                read_input[""label""] if not self._add_input_to_label else\n                [read_input[""label""], read_input[""image""]],\n                min_queue_examples,\n                batch_size,\n                shuffle=input_type == InputType.train)\n\n    def _maybe_download_and_extract(self):\n        """"""Download and extract the ORL Faces dataset""""""\n\n        dest_directory = self._data_dir\n        if not os.path.exists(dest_directory):\n            os.makedirs(dest_directory)\n        filename = self._data_url.split(\'/\')[-1]\n        filepath = os.path.join(dest_directory, filename)\n        if not os.path.exists(filepath):\n\n            def _progress(count, block_size, total_size):\n                sys.stdout.write(\n                    \'\\r>> Downloading %s %.1f%%\' %\n                    (filename,\n                     float(count * block_size) / float(total_size) * 100.0))\n                sys.stdout.flush()\n\n            filepath, _ = urllib.request.urlretrieve(self._data_url, filepath,\n                                                     _progress)\n            print()\n            statinfo = os.stat(filepath)\n            print(\'Successfully downloaded\', filename, statinfo.st_size,\n                  \'bytes.\')\n            with zipfile.ZipFile(filepath) as zip_f:\n                zip_f.extractall(\n                    os.path.join(dest_directory,\n                                 filename.split(\'.\')[-2]))\n\n        # Convert to Examples and write the result to TFRecords.\n        if not tf.gfile.Exists(os.path.join(self._data_dir, \'faces.tfrecords\')):\n            images = []\n            labels = []\n\n            for pgm in glob.glob(""{}/*/*.pgm"".format(\n                    os.path.join(dest_directory,\n                                 filename.split(\'.\')[-2]))):\n                images.append(\n                    np.expand_dims(np.asarray(Image.open(pgm)), axis=2))\n                labels.append(int(pgm.split(""/"")[-2].strip(""s"")))\n\n            # Create dataset object\n            dataset = lambda: None\n            dataset.num_examples = self._num_examples_per_epoch_for_train\n            dataset.images = np.array(images)\n            dataset.labels = np.array(labels)\n            convert_to_tfrecords(dataset, \'faces\', self._data_dir)\n'"
dytb/inputs/predefined/PASCALVOC2012Classification.py,11,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""PASCAL VOC 2012""""""\n\nimport os\nimport sys\nimport tarfile\nimport xml.etree.ElementTree as etree\nimport csv\nfrom collections import defaultdict\n\nfrom six.moves import urllib\nimport tensorflow as tf\nfrom ..processing import build_batch\nfrom ..images import read_image_jpg\nfrom ..interfaces.Input import Input\nfrom ..interfaces.InputType import InputType\n\n\nclass PASCALVOC2012Classification(Input):\n    """"""Routine for decoding the PASCAL VOC 2012 binary file format.""""""\n\n    def __init__(self, add_input_to_label=False):\n        # Global constants describing the PASCAL VOC 2012 data set.\n        # resize image to a fixed size\n        # the resize dimension is an hyperparameter\n        self._name = \'PASCAL-VOC-2012-Classification\'\n        self._image_height = 150\n        self._image_width = 150\n        self._image_depth = 3\n\n        # multiple boxes enable the return of a tensor\n        # of boxes instead of a single box per image\n        self._multiple_bboxes = False\n\n        self.CLASSES = [\n            ""aeroplane"", ""bicycle"", ""bird"", ""boat"", ""bottle"", ""bus"", ""car"",\n            ""cat"", ""chair"", ""cow"", ""diningtable"", ""dog"", ""horse"", ""motorbike"",\n            ""person"", ""pottedplant"", ""sheep"", ""sofa"", ""train"", ""tvmonitor""\n        ]\n        self._bboxes = {""train"": defaultdict(list), ""val"": defaultdict(list)}\n        self._tf_bboxes = {""train"": None, ""val"": None}\n        self._num_classes = 20\n        self._num_examples_per_epoch_for_train = 13609\n        self._num_examples_per_epoch_for_eval = 13841\n        self._num_examples_per_epoch_for_test = self._num_examples_per_epoch_for_eval\n\n        self._data_dir = os.path.join(\n            os.path.dirname(os.path.abspath(__file__)), \'data\', \'PASCALVOC2012\')\n        self._data_url = \'http://pjreddie.com/media/files/VOCtrainval_11-May-2012.tar\'\n        self._maybe_download_and_extract()\n        self._add_input_to_label = add_input_to_label\n\n    @property\n    def name(self):\n        """"""Returns the name of the input source""""""\n        return self._name\n\n    def num_examples(self, input_type):\n        """"""Returns the number of examples per the specified input_type\n\n        Args:\n            input_type: InputType enum\n        """"""\n        InputType.check(input_type)\n\n        if input_type == InputType.train:\n            return self._num_examples_per_epoch_for_train\n        elif input_type == InputType.test:\n            return self._num_examples_per_epoch_for_test\n        return self._num_examples_per_epoch_for_eval\n\n    @property\n    def num_classes(self):\n        """"""Returns the number of classes""""""\n        return self._num_classes\n\n    def _read_image_and_box(self, bboxes_csv):\n        """"""Extract the filename from the queue, read the image and\n        produce a single box\n        Returns:\n            image, box\n        """"""\n\n        reader = tf.TextLineReader(skip_header_lines=True)\n        _, row = reader.read(bboxes_csv)\n        # file ,y_min, x_min, y_max, x_max, label\n        record_defaults = [[""""], [0.], [0.], [0.], [0.], [0.]]\n        # eg:\n        # 2008_000033,0.1831831831831832,0.208,0.7717717717717718,0.952,0\n        filename, y_min, x_min, y_max, x_max, label = tf.decode_csv(\n            row, record_defaults)\n        image_path = os.path.join(self._data_dir, \'VOCdevkit\', \'VOC2012\',\n                                  \'JPEGImages\') + ""/"" + filename + "".jpg""\n\n        # image is normalized in [-1,1], convert to #_image_depth depth\n        image = read_image_jpg(image_path, depth=self._image_depth)\n        return image, tf.stack([y_min, x_min, y_max, x_max, label])\n\n    def _read(self, filename_queue):\n        image, bbox_and_label = self._read_image_and_box(\n            filename_queue)  #bbox is a single box\n\n        bbox = bbox_and_label[:4]\n        label = tf.cast(bbox_and_label[-1], tf.int32)\n\n        image = tf.squeeze(\n            tf.image.crop_and_resize(\n                tf.expand_dims(image, axis=0),\n                tf.expand_dims(bbox, axis=0),\n                box_ind=[0],\n                crop_size=[self._image_height, self._image_width]),\n            axis=[0])\n        return image, label\n\n    def inputs(self, input_type, batch_size, augmentation_fn=None):\n        """"""Construct input for PASCALVOC2012Classification evaluation using the Reader ops.\n\n        Args:\n            input_type: InputType enum\n            batch_size: Number of images per batch.\n        Returns:\n            images: Images. 4D tensor of [batch_size, self._image_height, self._image_width, self._image_depth] size.\n            labels: tensor with batch_size labels\n        """"""\n        InputType.check(input_type)\n\n        if input_type == InputType.train:\n            filenames = [os.path.join(self._data_dir, \'train.csv\')]\n            num_examples_per_epoch = self._num_examples_per_epoch_for_train\n        else:\n            filenames = [os.path.join(self._data_dir, \'val.csv\')]\n            num_examples_per_epoch = self._num_examples_per_epoch_for_eval\n\n        for name in filenames:\n            if not tf.gfile.Exists(name):\n                raise ValueError(\'Failed to find file: \' + name)\n\n        # Ensure that the random shuffling has good mixing properties.\n        min_fraction_of_examples_in_queue = 0.4\n        min_queue_examples = int(\n            num_examples_per_epoch * min_fraction_of_examples_in_queue)\n\n        with tf.variable_scope(""{}_input"".format(input_type)):\n            # Create a queue that produces the filenames to read.\n            filename_queue = tf.train.string_input_producer(filenames)\n\n            image, label = self._read(filename_queue)\n            if augmentation_fn:\n                image = augmentation_fn(image)\n\n            return build_batch(\n                image,\n                label if not self._add_input_to_label else [label, image],\n                min_queue_examples,\n                batch_size,\n                shuffle=input_type == InputType.train)\n\n    def _maybe_download_and_extract(self):\n        """"""Download and extract the tarball""""""\n        dest_directory = self._data_dir\n        if not os.path.exists(dest_directory):\n            os.makedirs(dest_directory)\n        filename = self._data_url.split(\'/\')[-1]\n        archivepath = os.path.join(dest_directory, filename)\n        if not os.path.exists(archivepath):\n\n            def _progress(count, block_size, total_size):\n                sys.stdout.write(\n                    \'\\r>> Downloading %s %.1f%%\' %\n                    (filename,\n                     float(count * block_size) / float(total_size) * 100.0))\n                sys.stdout.flush()\n\n            archivepath, _ = urllib.request.urlretrieve(self._data_url,\n                                                        archivepath, _progress)\n            print()\n            statinfo = os.stat(archivepath)\n            print(\'Successfully downloaded\', filename, statinfo.st_size,\n                  \'bytes.\')\n            tarfile.open(archivepath, \'r\').extractall(dest_directory)\n            print(\'Sucessfully extracted.\')\n\n        # Now self._data dir contains VOCDevkit folder\n        # Build train.csv and val.csv file in self._data_dir\n        csv_header = [""filename"", ""y_min"", ""x_min"", ""y_max"", ""x_max"", ""label""]\n        if os.path.exists(os.path.join(\n                self._data_dir, \'train.csv\')) and os.path.exists(\n                    os.path.join(self._data_dir, \'val.csv\')):\n            return\n\n        base_dir = os.path.join(\n            self._data_dir,\n            \'VOCdevkit\',\n            \'VOC2012\',\n        )\n\n        for current_set in [\'train\', \'val\']:\n            csv_path = os.path.join(self._data_dir,\n                                    \'{}.csv\'.format(current_set))\n            with open(csv_path, mode=\'w\') as csv_file:\n                # header\n                writer = csv.DictWriter(csv_file, csv_header)\n                writer.writeheader()\n                for current_class in self.CLASSES:\n                    lines = open(\n                        os.path.join(\n                            base_dir, \'ImageSets\', \'Main\', \'{}_{}.txt\'.format(\n                                current_class,\n                                current_set))).read().strip().split(""\\n"")\n                    for line in lines:\n                        splitted = line.split()\n                        if len(splitted) < 1:\n                            print(splitted, line, current_class)\n                        if splitted[1] == ""-1"":\n                            continue\n\n                        image_xml = os.path.join(base_dir, \'Annotations\',\n                                                 \'{}.xml\'.format(splitted[0]))\n                        image_filename = splitted[0]\n\n                        # parse XML\n                        tree = etree.parse(image_xml)\n                        root = tree.getroot()\n                        size = root.find(\'size\')\n                        width = float(size.find(\'width\').text)\n                        height = float(size.find(\'height\').text)\n\n                        for obj in root.iter(\'object\'):\n                            # skip difficult & object.name not in current class\n                            label = obj.find(\'name\').text\n                            if label != current_class:\n                                continue\n\n                            difficult = obj.find(\'difficult\').text\n                            if int(difficult) == 1:\n                                continue\n\n                            bndbox = obj.find(\'bndbox\')\n                            normalized_bbox = [\n                                # y_min\n                                float(bndbox.find(\'ymin\').text) / height,\n                                # x_min\n                                float(bndbox.find(\'xmin\').text) / width,\n                                # y_max\n                                float(bndbox.find(\'ymax\').text) / height,\n                                # x_max\n                                float(bndbox.find(\'xmax\').text) / width\n                            ]\n\n                            label_id = self.CLASSES.index(current_class)\n                            writer.writerow({\n                                ""filename"": image_filename,\n                                ""y_min"": normalized_bbox[0],\n                                ""x_min"": normalized_bbox[1],\n                                ""y_max"": normalized_bbox[2],\n                                ""x_max"": normalized_bbox[3],\n                                ""label"": label_id\n                            })\n            print(\'{}.csv created\'.format(current_set))\n'"
dytb/inputs/predefined/PASCALVOC2012Localization.py,6,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""PASCAL VOC 2012""""""\n\nimport os\nimport sys\nimport tarfile\nimport xml.etree.ElementTree as etree\nimport csv\nfrom collections import defaultdict\n\nfrom six.moves import urllib\nimport tensorflow as tf\nfrom ..processing import build_batch\nfrom ..images import read_image_jpg\nfrom ..interfaces import Input, InputType\nfrom ..PASCALVOC2012Classification import PASCALVOC2012Classification\n\n\nclass PASCALVOC2012Localization(Input):\n    """"""Routine for decoding the PASCAL VOC 2012 binary file format.""""""\n\n    def __init__(self):\n        self._name = \'PASCAL-VOC-2012-Localization\'\n        # multiple boxes enable the return of a tensor\n        # of boxes instead of a single box per image\n        self._multiple_bboxes = False\n\n        # Use Classification dataset\n        # to extract shared features and download the dataset\n        self._pascal = PASCALVOC2012Classification()\n\n    def num_examples(self, input_type):\n        """"""Returns the number of examples per the specified input_type\n\n        Args:\n            input_type: InputType enum\n        """"""\n        return self._pascal.num_examples(input_type)\n\n    @property\n    def num_classes(self):\n        """"""Returns the number of classes""""""\n        return self._pascal.num_classes\n\n    @property\n    def name(self):\n        """"""Returns the name of the input source""""""\n        return self._name\n\n    def _read_image_and_box(self, bboxes_csv):\n        """"""Extract the filename from the queue, read the image and\n        produce a single box\n        Returns:\n            image, [y_min, x_min, y_max, x_max, label]\n        """"""\n\n        reader = tf.TextLineReader(skip_header_lines=True)\n        _, row = reader.read(bboxes_csv)\n        # file ,y_min, x_min, y_max, x_max, label\n        record_defaults = [[""""], [0.], [0.], [0.], [0.], [0.]]\n        # eg:\n        # 2008_000033,0.1831831831831832,0.208,0.7717717717717718,0.952,0\n        filename, y_min, x_min, y_max, x_max, label = tf.decode_csv(\n            row, record_defaults)\n        image_path = os.path.join(self._data_dir, \'VOCdevkit\', \'VOC2012\',\n                                  \'JPEGImages\') + ""/"" + filename + "".jpg""\n\n        # image is normalized in [-1,1]\n        image = read_image_jpg(image_path)\n        return image, tf.stack([y_min, x_min, y_max, x_max, label])\n\n    def inputs(self, input_type, batch_size, augmentation_fn=None):\n        """"""Construct input for PASCALVOC2012 evaluation using the Reader ops.\n\n        Args:\n            input_type: InputType enum\n            batch_size: Number of images per batch.\n        Returns:\n            images: Images. 4D tensor of [batch_size, self._image_height, self._image_width, self._image_depth] size.\n            labels: A tensor with shape [batch_size, num_bboxes_max, 5]. num_bboxes_max are the maximum bboxes found in the\n            requested set (train/test/validation). Where the bbox is fake, a -1,-1,-1,-1,-1 value is present\n        """"""\n        InputType.check(input_type)\n\n        if input_type == InputType.train:\n            filenames = [\n                os.path.join(self._data_dir, \'VOCdevkit\', \'VOC2012\',\n                             \'ImageSets\', \'Main\', \'train.txt\')\n            ]\n            num_examples_per_epoch = self._num_examples_per_epoch_for_train\n        else:\n            filenames = [\n                os.path.join(self._data_dir, \'VOCdevkit\', \'VOC2012\',\n                             \'ImageSets\', \'Main\', \'val.txt\')\n            ]\n            num_examples_per_epoch = self._num_examples_per_epoch_for_eval\n\n        for name in filenames:\n            if not tf.gfile.Exists(name):\n                raise ValueError(\'Failed to find file: \' + name)\n\n        # Ensure that the random shuffling has good mixing properties.\n        min_fraction_of_examples_in_queue = 0.4\n        min_queue_examples = int(\n            num_examples_per_epoch * min_fraction_of_examples_in_queue)\n\n        with tf.variable_scope(""{}_input"".format(input_type)):\n            # Create a queue that produces the filenames to read.\n            filename_queue = tf.train.string_input_producer(filenames)\n\n            image, bbox = self._read_image_and_box(filename_queue)\n\n            if augmentation_fn:\n                image = augmentation_fn(image)\n            return build_batch(\n                image,\n                bbox,\n                min_queue_examples,\n                batch_size,\n                shuffle=input_type == InputType.train)\n'"
dytb/inputs/predefined/__init__.py,0,b''
dytb/models/predefined/LeNet.py,23,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Build a LeNet-like network without additional layers""""""\n\nimport tensorflow as tf\nfrom ..collections import LOSSES\nfrom ..layers import conv, fc\nfrom ..interfaces import Classifier\n\n\nclass LeNet(Classifier):\n    """"""Build a LeNet-like network without additional layers""""""\n\n    def _inference(self,\n                   images,\n                   num_classes,\n                   is_training_,\n                   train_phase=False,\n                   l2_penalty=0.0):\n        """"""Build the LeNet-like network.\n\n        Args:\n          images: Images returned from train_inputs() or inputs().\n          num_classes: Number of classes to predict\n          is_training_: enable/disable training ops at run time\n          train_phase: Boolean to enable/disable training ops at build time\n          l2_penalty: float value, weight decay (l2) penalty\n\n        Returns:\n          Logits.\n        """"""\n\n        # Initializer with seed\n        initializer = tf.contrib.layers.variance_scaling_initializer(\n            factor=2.0,\n            mode=\'FAN_IN\',\n            uniform=False,\n            seed=self.seed,\n            dtype=tf.float32)\n\n        with tf.variable_scope(self.__class__.__name__):\n            with tf.variable_scope(""conv1""):\n                conv1 = conv(\n                    images, [5, 5, 1, 32],\n                    1,\n                    \'SAME\',\n                    train_phase,\n                    activation=tf.nn.relu,\n                    wd=l2_penalty,\n                    initializer=initializer)\n\n            with tf.variable_scope(""pool1""):\n                pool1 = tf.nn.max_pool(\n                    conv1,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n\n            with tf.variable_scope(""conv2""):\n                conv2 = conv(\n                    pool1, [5, 5, 32, 64],\n                    1,\n                    \'SAME\',\n                    train_phase,\n                    activation=tf.nn.relu,\n                    wd=l2_penalty,\n                    initializer=initializer)\n\n            with tf.variable_scope(""pool2""):\n                pool2 = tf.nn.max_pool(\n                    conv2,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n                pool2 = tf.reshape(pool2, [-1, 7 * 7 * 64])\n\n            with tf.variable_scope(""fc1""):\n                fc1 = fc(\n                    pool2, [7 * 7 * 64, 1024],\n                    train_phase,\n                    activation=tf.nn.relu,\n                    wd=l2_penalty,\n                    initializer=initializer)\n\n            with tf.variable_scope(""softmax_linear""):\n                logits = fc(fc1, [1024, num_classes], train_phase)\n            return logits\n\n    def loss(self, logits, labels):\n        """"""Add L2Loss to all the trainable variables.\n        Args:\n          logits: Logits from get().\n          labels: Labels from train_inputs or inputs(). 1-D tensor\n                  of shape [batch_size]\n\n        Returns:\n          Loss tensor of type float.\n        """"""\n        with tf.variable_scope(\'loss\'):\n            # Calculate the average cross entropy loss across the batch.\n            labels = tf.cast(labels, tf.int64)\n            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n                logits=logits, labels=labels, name=\'cross_entropy_per_example\')\n            cross_entropy_mean = tf.reduce_mean(\n                cross_entropy, name=\'cross_entropy\')\n            tf.add_to_collection(LOSSES, cross_entropy_mean)\n\n            # The total loss is defined as the cross entropy loss plus all of the weight\n            # decay terms (L2 loss).\n            error = tf.add_n(tf.get_collection(LOSSES), name=\'total_loss\')\n        return error\n\n    def get(self, images, num_classes, train_phase=False, l2_penalty=0.0):\n        """""" define the model with its inputs.\n        Use this function to define the model in training and when exporting the model\n        in the protobuf format.\n\n        Args:\n            images: model input\n            num_classes: number of classes to predict\n            train_phase: set it to True when defining the model, during train\n            l2_penalty: float value, weight decay (l2) penalty\n\n        Returns:\n            is_training_: tf.bool placeholder enable/disable training ops at run time\n            logits: the model output\n        """"""\n        is_training_ = tf.placeholder_with_default(\n            False, shape=(), name=""is_training_"")\n        # build a graph that computes the logits predictions from the images\n        logits = self._inference(images, num_classes, is_training_, train_phase,\n                                 l2_penalty)\n\n        return is_training_, logits\n'"
dytb/models/predefined/LeNetBN.py,23,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Build a LeNet-like network with BN layers""""""\n\nimport tensorflow as tf\nfrom ..collections import LOSSES\nfrom ..layers import conv, fc, batch_norm\nfrom ..interfaces import Classifier\n\n\nclass LeNetBN(Classifier):\n    """"""Build a LeNet-like network with BN layers""""""\n\n    def _inference(self,\n                   images,\n                   num_classes,\n                   is_training_,\n                   train_phase=False,\n                   l2_penalty=0.0):\n        """"""Build the LeNet-like network.\n\n        Args:\n          images: Images returned from train_inputs() or inputs().\n          num_classes: Number of classes to predict\n          is_training_: enable/disable training ops at run time\n          train_phase: Boolean to enable/disable training ops at build time\n          l2_penalty: float value, weight decay (l2) penalty\n\n        Returns:\n          Logits.\n        """"""\n\n        # Initializer with seed\n        initializer = tf.contrib.layers.variance_scaling_initializer(\n            factor=2.0,\n            mode=\'FAN_IN\',\n            uniform=False,\n            seed=self.seed,\n            dtype=tf.float32)\n\n        with tf.variable_scope(self.__class__.__name__):\n            with tf.variable_scope(""conv1""):\n                conv1 = tf.nn.relu(\n                    batch_norm(\n                        conv(\n                            images, [5, 5, 1, 32],\n                            1,\n                            \'SAME\',\n                            train_phase,\n                            bias_term=False,\n                            wd=l2_penalty,\n                            initializer=initializer), is_training_\n                        if train_phase else False))\n\n            with tf.variable_scope(""pool1""):\n                pool1 = tf.nn.max_pool(\n                    conv1,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n\n            with tf.variable_scope(""conv2""):\n                conv2 = tf.nn.relu(\n                    batch_norm(\n                        conv(\n                            pool1, [5, 5, 32, 64],\n                            1,\n                            \'SAME\',\n                            train_phase,\n                            bias_term=False,\n                            wd=l2_penalty,\n                            initializer=initializer), is_training_\n                        if train_phase else False))\n\n            with tf.variable_scope(""pool2""):\n                pool2 = tf.nn.max_pool(\n                    conv2,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n                pool2 = tf.reshape(pool2, [-1, 7 * 7 * 64])\n\n            with tf.variable_scope(""fc1""):\n                fc1 = tf.nn.relu(\n                    batch_norm(\n                        fc(pool2, [7 * 7 * 64, 1024],\n                           train_phase,\n                           bias_term=False,\n                           wd=l2_penalty,\n                           initializer=initializer), is_training_\n                        if train_phase else False))\n\n            with tf.variable_scope(""softmax_linear""):\n                logits = fc(\n                    fc1, [1024, num_classes],\n                    train_phase,\n                    initializer=initializer)\n            return logits\n\n    def loss(self, logits, labels):\n        """"""Add L2Loss to all the trainable variables.\n        Args:\n          logits: Logits from get().\n          labels: Labels from train_inputs or inputs(). 1-D tensor\n                  of shape [batch_size]\n\n        Returns:\n          Loss tensor of type float.\n        """"""\n        with tf.variable_scope(\'loss\'):\n            # Calculate the average cross entropy loss across the batch.\n            labels = tf.cast(labels, tf.int64)\n            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n                logits=logits, labels=labels, name=\'cross_entropy_per_example\')\n            cross_entropy_mean = tf.reduce_mean(\n                cross_entropy, name=\'cross_entropy\')\n            tf.add_to_collection(LOSSES, cross_entropy_mean)\n\n            # The total loss is defined as the cross entropy loss plus all of the weight\n            # decay terms (L2 loss).\n            error = tf.add_n(tf.get_collection(LOSSES), name=\'total_loss\')\n\n        return error\n\n    def get(self, images, num_classes, train_phase=False, l2_penalty=0.0):\n        """""" define the model with its inputs.\n        Use this function to define the model in training and when exporting the model\n        in the protobuf format.\n\n        Args:\n            images: model input\n            num_classes: number of classes to predict\n            train_phase: set it to True when defining the model, during train\n            l2_penalty: float value, weight decay (l2) penalty\n\n        Returns:\n            is_training_: tf.bool placeholder enable/disable training ops at run time\n            logits: the model output\n        """"""\n        is_training_ = tf.placeholder_with_default(\n            False, shape=(), name=""is_training_"")\n        # build a graph that computes the logits predictions from the images\n        logits = self._inference(images, num_classes, is_training_, train_phase,\n                                 l2_penalty)\n\n        return is_training_, logits\n'"
dytb/models/predefined/LeNetDirectDropout.py,25,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Build a LeNet-like network with direct dropout layers""""""\n\nimport tensorflow as tf\nfrom ..collections import LOSSES\nfrom ..layers import conv, fc, direct_dropout\nfrom ..interfaces import Classifier\n\n\nclass LeNetDirectDropout(Classifier):\n    """"""Build a LeNet-like network with direct dropout layers""""""\n\n    def _inference(self,\n                   images,\n                   num_classes,\n                   is_training_,\n                   train_phase=False,\n                   l2_penalty=0.0):\n        """"""Build the LeNet-like network.\n\n        Args:\n          images: Images returned from train_inputs() or inputs().\n          num_classes: Number of classes to predict\n          is_training_: enable/disable training ops at run time\n          train_phase: Boolean to enable/disable training ops at build time\n          l2_penalty: float value, weight decay (l2) penalty\n\n        Returns:\n          Logits.\n        """"""\n\n        # Initializer with seed\n        initializer = tf.contrib.layers.variance_scaling_initializer(\n            factor=2.0,\n            mode=\'FAN_IN\',\n            uniform=False,\n            seed=self.seed,\n            dtype=tf.float32)\n\n        def direct_drop(layer, prob):\n            """""" Build a condition node if we are in train_phase. thus we can use the\n            is_training_ placeholder to switch.\n            Build a prob*layer node when we\'re not in train_phase.\n            Returns the correct node""""""\n\n            if train_phase:\n                layer = tf.cond(\n                    tf.equal(is_training_, True),\n                    lambda: direct_dropout(layer, prob), lambda: prob * layer)\n            else:\n                layer = prob * layer\n            return layer\n\n        with tf.variable_scope(self.__class__.__name__):\n            with tf.variable_scope(""conv1""):\n                conv1 = conv(\n                    images, [5, 5, 1, 32],\n                    1,\n                    \'SAME\',\n                    train_phase,\n                    activation=tf.nn.relu,\n                    wd=l2_penalty,\n                    initializer=initializer)\n                conv1 = direct_drop(conv1, 0.7)\n\n            with tf.variable_scope(""pool1""):\n                pool1 = tf.nn.max_pool(\n                    conv1,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n\n            with tf.variable_scope(""conv2""):\n                conv2 = conv(\n                    pool1, [5, 5, 32, 64],\n                    1,\n                    \'SAME\',\n                    train_phase,\n                    activation=tf.nn.relu,\n                    wd=l2_penalty,\n                    initializer=initializer)\n                conv2 = direct_drop(conv2, 0.6)\n\n            with tf.variable_scope(""pool2""):\n                pool2 = tf.nn.max_pool(\n                    conv2,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n                pool2 = tf.reshape(pool2, [-1, 7 * 7 * 64])\n\n            with tf.variable_scope(""fc1""):\n                fc1 = fc(\n                    pool2, [7 * 7 * 64, 1024],\n                    train_phase,\n                    activation=tf.nn.relu,\n                    wd=l2_penalty,\n                    initializer=initializer)\n\n                fc1 = direct_drop(fc1, 0.5)\n\n            with tf.variable_scope(""softmax_linear""):\n                logits = fc(\n                    fc1, [1024, num_classes],\n                    train_phase,\n                    initializer=initializer)\n        return logits\n\n    def loss(self, logits, labels):\n        """"""Add L2Loss to all the trainable variables.\n        Args:\n          logits: Logits from get().\n          labels: Labels from train_inputs or inputs(). 1-D tensor\n                  of shape [batch_size]\n\n        Returns:\n          Loss tensor of type float.\n        """"""\n        with tf.variable_scope(\'loss\'):\n            # Calculate the average cross entropy loss across the batch.\n            labels = tf.cast(labels, tf.int64)\n            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n                logits=logits, labels=labels, name=\'cross_entropy_per_example\')\n            cross_entropy_mean = tf.reduce_mean(\n                cross_entropy, name=\'cross_entropy\')\n            tf.add_to_collection(LOSSES, cross_entropy_mean)\n\n            # The total loss is defined as the cross entropy loss plus all of the weight\n            # decay terms (L2 loss).\n            error = tf.add_n(tf.get_collection(LOSSES), name=\'total_loss\')\n\n        return error\n\n    def get(self, images, num_classes, train_phase=False, l2_penalty=0.0):\n        """""" define the model with its inputs.\n        Use this function to define the model in training and when exporting the model\n        in the protobuf format.\n\n        Args:\n            images: model input\n            num_classes: number of classes to predict\n            train_phase: set it to True when defining the model, during train\n            l2_penalty: float value, weight decay (l2) penalty\n\n        Returns:\n            is_training_: tf.bool placeholder enable/disable training ops at run time\n            logits: the model output\n        """"""\n        is_training_ = tf.placeholder_with_default(\n            False, shape=(), name=""is_training_"")\n        # build a graph that computes the logits predictions from the images\n        logits = self._inference(images, num_classes, is_training_, train_phase,\n                                 l2_penalty)\n\n        return is_training_, logits\n'"
dytb/models/predefined/LeNetDropout.py,32,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Build the LeNet-like network with dropout layers""""""\n\nimport tensorflow as tf\nfrom ..collections import LOSSES\nfrom ..layers import conv, fc\nfrom ..interfaces import Classifier\n\n\nclass LeNetDropout(Classifier):\n    """"""Build the LeNet-like network with dropout layers""""""\n\n    def _inference(self,\n                   images,\n                   num_classes,\n                   is_training_,\n                   train_phase=False,\n                   l2_penalty=0.0):\n        """"""Build the LeNet-like network.\n\n        Args:\n          images: Images returned from train_inputs() or inputs().\n          num_classes: Number of classes to predict\n          is_training_: enable/disable training ops at run time\n          train_phase: Boolean to enable/disable training ops at build time\n          l2_penalty: float value, weight decay (l2) penalty\n\n        Returns:\n          Logits.\n        """"""\n\n        # Initializer with seed\n        initializer = tf.contrib.layers.variance_scaling_initializer(\n            factor=2.0,\n            mode=\'FAN_IN\',\n            uniform=False,\n            seed=self.seed,\n            dtype=tf.float32)\n\n        with tf.variable_scope(self.__class__.__name__):\n            with tf.variable_scope(""conv1""):\n                conv1 = conv(\n                    images, [5, 5, 1, 32],\n                    1,\n                    \'SAME\',\n                    train_phase,\n                    activation=tf.nn.relu,\n                    wd=l2_penalty,\n                    initializer=initializer)\n                if train_phase:\n                    conv1 = tf.cond(\n                        tf.equal(is_training_, True),\n                        lambda: tf.nn.dropout(conv1, 0.7), lambda: conv1)\n\n            with tf.variable_scope(""pool1""):\n                pool1 = tf.nn.max_pool(\n                    conv1,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n\n            with tf.variable_scope(""conv2""):\n                conv2 = conv(\n                    pool1, [5, 5, 32, 64],\n                    1,\n                    \'SAME\',\n                    train_phase,\n                    activation=tf.nn.relu,\n                    wd=l2_penalty,\n                    initializer=initializer)\n                if train_phase:\n                    conv2 = tf.cond(\n                        tf.equal(is_training_, True),\n                        lambda: tf.nn.dropout(conv2, 0.6), lambda: conv2)\n\n            with tf.variable_scope(""pool2""):\n                pool2 = tf.nn.max_pool(\n                    conv2,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n                pool2 = tf.reshape(pool2, [-1, 7 * 7 * 64])\n\n            with tf.variable_scope(""fc1""):\n                fc1 = fc(\n                    pool2, [7 * 7 * 64, 1024],\n                    train_phase,\n                    activation=tf.nn.relu,\n                    wd=l2_penalty,\n                    initializer=initializer)\n\n                if train_phase:\n                    fc1 = tf.cond(\n                        tf.equal(is_training_, True),\n                        lambda: tf.nn.dropout(fc1, 0.5), lambda: fc1)\n\n            with tf.variable_scope(""softmax_linear""):\n                logits = fc(\n                    fc1, [1024, num_classes],\n                    train_phase,\n                    initializer=initializer)\n        return logits\n\n    def loss(self, logits, labels):\n        """"""Add L2Loss to all the trainable variables.\n        Args:\n          logits: Logits from get().\n          labels: Labels from train_inputs or inputs(). 1-D tensor\n                  of shape [batch_size]\n\n        Returns:\n          Loss tensor of type float.\n        """"""\n        with tf.variable_scope(\'loss\'):\n            # Calculate the average cross entropy loss across the batch.\n            labels = tf.cast(labels, tf.int64)\n            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n                logits=logits, labels=labels, name=\'cross_entropy_per_example\')\n            cross_entropy_mean = tf.reduce_mean(\n                cross_entropy, name=\'cross_entropy\')\n            tf.add_to_collection(LOSSES, cross_entropy_mean)\n\n            # The total loss is defined as the cross entropy loss plus all of the weight\n            # decay terms (L2 loss).\n            error = tf.add_n(tf.get_collection(LOSSES), name=\'total_loss\')\n\n        return error\n\n    def get(self, images, num_classes, train_phase=False, l2_penalty=0.0):\n        """""" define the model with its inputs.\n        Use this function to define the model in training and when exporting the model\n        in the protobuf format.\n\n        Args:\n            images: model input\n            num_classes: number of classes to predict\n            train_phase: set it to True when defining the model, during train\n            l2_penalty: float value, weight decay (l2) penalty\n\n        Returns:\n            is_training_: tf.bool placeholder enable/disable training ops at run time\n            logits: the model output\n        """"""\n        is_training_ = tf.placeholder_with_default(\n            False, shape=(), name=""is_training_"")\n        # build a graph that computes the logits predictions from the images\n        logits = self._inference(images, num_classes, is_training_, train_phase,\n                                 l2_penalty)\n\n        return is_training_, logits\n'"
dytb/models/predefined/SingleLayerCAE.py,16,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Build a single layer CAE""""""\n\nimport tensorflow as tf\nfrom ..collections import LOSSES\nfrom ..layers import conv\nfrom ..interfaces import Autoencoder\n\n\nclass SingleLayerCAE(Autoencoder):\n    """""" Build a single layer CAE""""""\n\n    def _pad(self, input_x, filter_side):\n        """"""\n        pads input_x with the right amount of zeros.\n        Args:\n            input_x: 4-D tensor, [batch_side, widht, height, depth]\n            filter_side: used to dynamically determine the padding amount\n        Returns:\n            input_x padded\n        """"""\n        # calculate the padding amount for each side\n        amount = filter_side - 1\n        # pad the input on top, bottom, left, right, with amount zeros\n        return tf.pad(input_x,\n                      [[0, 0], [amount, amount], [amount, amount], [0, 0]])\n\n    def get(self, images, num_classes, train_phase=False, l2_penalty=0.0):\n        """""" define the model with its inputs.\n        Use this function to define the model in training and when exporting the model\n        in the protobuf format.\n        Args:\n            images: model input\n            num_classes: number of classes to predict. If the model doesn\'t use it,\n                         just pass any value.\n            train_phase: set it to True when defining the model, during train\n            l2_penalty: float value, weight decay (l2) penalty\n        Returns:\n            is_training_: tf.bool placeholder enable/disable training ops at run time\n            predictions: the model output\n        """"""\n\n        # Initializer with seed\n        initializer = tf.contrib.layers.variance_scaling_initializer(\n            factor=2.0,\n            mode=\'FAN_IN\',\n            uniform=False,\n            seed=self.seed,\n            dtype=tf.float32)\n\n        filter_side = 3\n        filters_number = 32\n        with tf.variable_scope(self.__class__.__name__):\n            input_x = self._pad(images, filter_side)\n\n            with tf.variable_scope(""encode""):\n                # the encoding convolutions is a [3 x 3 x input_depth] x 32 convolution\n                # the activation function chosen is the tanh\n                # 32 is the number of feature extracted. It\'s completely arbitrary as is\n                # the side of the convolutional filter and the activation function used\n                encoding = conv(\n                    input_x, [\n                        filter_side, filter_side,\n                        input_x.get_shape()[3].value, filters_number\n                    ],\n                    1,\n                    \'VALID\',\n                    train_phase,\n                    activation=tf.nn.tanh,\n                    wd=l2_penalty,\n                    initializer=initializer)\n\n            with tf.variable_scope(""decode""):\n                # the decoding convolution is a [3 x 3 x 32] x input_depth convolution\n                # the activation function chosen is the tanh\n                # The dimenensions of the convolutional filter in the decoding convolution,\n                # differently from the encoding, are constrained by the\n                # choices made in the encoding layer\n                # The only degree of freedom is the chose of the activation function.\n                # We have to choose an activation function that constraints the outputs\n                # to live in the same space of the input values.\n                # Since the input values are between -1 and 1, we can use the tanh function\n                # directly, or we could use the sigmoid and then scale the output\n                output_x = conv(\n                    encoding, [\n                        filter_side, filter_side, filters_number,\n                        input_x.get_shape()[3].value\n                    ],\n                    1,\n                    \'VALID\',\n                    train_phase,\n                    activation=tf.nn.tanh,\n                    initializer=initializer)\n\n        # The is_training_ placeholder is not used, but we define and return it\n        # in order to respect the expected output cardinality of the get method\n        is_training_ = tf.placeholder_with_default(\n            False, shape=(), name=""is_training_"")\n        return is_training_, output_x\n\n    def loss(self, predictions, real_values):\n        """"""Return the loss operation between predictions and real_values.\n        Add L2 weight decay term if any.\n        Args:\n            predictions: predicted values\n            real_values: real values\n        Returns:\n            Loss tensor of type float.\n        """"""\n        with tf.variable_scope(\'loss\'):\n            # 1/2n \\sum^{n}_{i=i}{(x_i - x\'_i)^2}\n            mse = tf.divide(\n                tf.reduce_mean(\n                    tf.square(tf.subtract(predictions, real_values))),\n                2.,\n                name=""mse"")\n            tf.add_to_collection(LOSSES, mse)\n\n            # mse + weight_decay per layer\n            error = tf.add_n(tf.get_collection(LOSSES), name=\'total_loss\')\n\n        return error\n'"
dytb/models/predefined/StackedCAE.py,20,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Build a stacked CAE""""""\n\nimport tensorflow as tf\nfrom ..collections import LOSSES\nfrom ..layers import conv\nfrom ..interfaces import Autoencoder\n\n\nclass StackedCAE(Autoencoder):\n    """"""Build a stacked CAE""""""\n\n    def _pad(self, input_x, filter_side):\n        """"""\n        pads input_x with the right amount of zeros.\n        Args:\n            input_x: 4-D tensor, [batch_side, widht, height, depth]\n            filter_side: used to dynamically determine the padding amount\n        Returns:\n            input_x padded\n        """"""\n        # calculate the padding amount for each side\n        amount = filter_side - 1\n        # pad the input on top, bottom, left, right, with amount zeros\n        return tf.pad(input_x,\n                      [[0, 0], [amount, amount], [amount, amount], [0, 0]])\n\n    def get(self, images, num_classes, train_phase=False, l2_penalty=0.0):\n        """""" define the model with its inputs.\n        Use this function to define the model in training and when exporting the model\n        in the protobuf format.\n        Args:\n            images: model input\n            num_classes: number of classes to predict. If the model doesn\'t use it,\n                         just pass any value.\n            train_phase: set it to True when defining the model, during train\n            l2_penalty: float value, weight decay (l2) penalty\n        Returns:\n            is_training_: tf.bool placeholder enable/disable training ops at run time\n            predictions: the model output\n        """"""\n\n        # Initializer with seed\n        initializer = tf.contrib.layers.variance_scaling_initializer(\n            factor=2.0,\n            mode=\'FAN_IN\',\n            uniform=False,\n            seed=self.seed,\n            dtype=tf.float32)\n\n        num_layers = 9\n        filter_side = 3\n        filters_number = 9\n        with tf.variable_scope(self.__class__.__name__):\n            input_x = tf.identity(images)\n            input_padded = self._pad(input_x, filter_side)\n            for layer in range(num_layers):\n                with tf.variable_scope(""layer_"" + str(layer)):\n                    with tf.variable_scope(""encode""):\n                        encoding = conv(\n                            input_padded, [\n                                filter_side, filter_side,\n                                input_padded.get_shape()[3].value,\n                                filters_number\n                            ],\n                            1,\n                            \'VALID\',\n                            train_phase,\n                            activation=tf.nn.tanh,\n                            wd=l2_penalty,\n                            initializer=initializer)\n                        if train_phase:\n                            encoding = tf.nn.dropout(encoding, 0.5)\n\n                    with tf.variable_scope(""decode""):\n                        output_x = conv(\n                            encoding, [\n                                filter_side, filter_side, filters_number,\n                                images.get_shape()[3].value\n                            ],\n                            1,\n                            \'VALID\',\n                            train_phase,\n                            activation=tf.nn.tanh,\n                            initializer=initializer)\n\n                        tf.add_to_collection(LOSSES, self._mse(\n                            input_x, output_x))\n                        input_x = tf.stop_gradient(output_x)\n                        input_padded = self._pad(input_x, filter_side)\n\n        # The is_training_ placeholder is not used, but we define and return it\n        # in order to respect the expected output cardinality of the get method\n        is_training_ = tf.placeholder_with_default(\n            False, shape=(), name=""is_training_"")\n        return is_training_, output_x\n\n    def _mse(self, input_x, output_x):\n        # 1/2n \\sum^{n}_{i=i}{(x_i - x\'_i)^2}\n        return tf.divide(\n            tf.reduce_mean(tf.square(tf.subtract(input_x, output_x))),\n            2.,\n            name=""mse"")\n\n    def loss(self, predictions, real_values):\n        """"""Return the loss operation between predictions and real_values.\n        Add L2 weight decay term if any.\n        Args:\n            predictions: predicted values\n            real_values: real values\n        Returns:\n            Loss tensor of type float.\n        """"""\n        with tf.variable_scope(\'loss\'):\n            #tf.add_to_collection(LOSSES, self._mse(real_values, predictions))\n            # mse + weight_decay per layer\n            error = tf.add_n(tf.get_collection(LOSSES), name=\'total_loss\')\n\n        return error\n'"
dytb/models/predefined/StackedDenoisingCAE.py,22,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Build a stacked denoising CAE""""""\n\nimport tensorflow as tf\nfrom ..collections import LOSSES\nfrom ..layers import conv\nfrom ..interfaces import Autoencoder\n\n\nclass StackedDenoisingCAE(Autoencoder):\n    """"""Build a stacked denoising CAE""""""\n\n    def _pad(self, input_x, filter_side):\n        """"""\n        pads input_x with the right amount of zeros.\n        Args:\n            input_x: 4-D tensor, [batch_side, widht, height, depth]\n            filter_side: used to dynamically determine the padding amount\n        Returns:\n            input_x padded\n        """"""\n        # calculate the padding amount for each side\n        amount = filter_side - 1\n        # pad the input on top, bottom, left, right, with amount zeros\n        return tf.pad(input_x,\n                      [[0, 0], [amount, amount], [amount, amount], [0, 0]])\n\n    def get(self, images, num_classes, train_phase=False, l2_penalty=0.0):\n        """""" define the model with its inputs.\n        Use this function to define the model in training and when exporting the model\n        in the protobuf format.\n        Args:\n            images: model input\n            num_classes: number of classes to predict. If the model doesn\'t use it,\n                         just pass any value.\n            train_phase: set it to True when defining the model, during train\n            l2_penalty: float value, weight decay (l2) penalty\n        Returns:\n            is_training_: tf.bool placeholder enable/disable training ops at run time\n            predictions: the model output\n        """"""\n\n        # Initializer with seed\n        initializer = tf.contrib.layers.variance_scaling_initializer(\n            factor=2.0,\n            mode=\'FAN_IN\',\n            uniform=False,\n            seed=self.seed,\n            dtype=tf.float32)\n\n        num_layers = 9\n        filter_side = 3\n        filters_number = 9\n        with tf.variable_scope(self.__class__.__name__):\n            input_x = tf.identity(images)\n            if train_phase:\n                input_x_noise = tf.clip_by_value(input_x + tf.random_uniform(\n                    input_x.get_shape(),\n                    minval=-0.5,\n                    maxval=0.5,\n                    dtype=input_x.dtype,\n                    seed=None), -1.0, 1.0)\n            else:\n                input_x_noise = input_x\n            input_padded_noise = self._pad(input_x_noise, filter_side)\n\n            for layer in range(num_layers):\n                with tf.variable_scope(""layer_"" + str(layer)):\n                    with tf.variable_scope(""encode""):\n                        encoding = conv(\n                            input_padded_noise, [\n                                filter_side, filter_side,\n                                input_padded_noise.get_shape()[3].value,\n                                filters_number\n                            ],\n                            1,\n                            \'VALID\',\n                            train_phase,\n                            activation=tf.nn.relu,\n                            wd=l2_penalty,\n                            initializer=initializer)\n\n                        if train_phase:\n                            encoding = tf.nn.dropout(encoding, 0.5)\n\n                    with tf.variable_scope(""decode""):\n                        output_x_noise = conv(\n                            encoding, [\n                                filter_side, filter_side, filters_number,\n                                images.get_shape()[3].value\n                            ],\n                            1,\n                            \'VALID\',\n                            train_phase,\n                            activation=tf.nn.tanh,\n                            initializer=initializer)\n\n                        last = layer == num_layers - 1\n                        if train_phase and not last:\n                            output_x_noise = tf.nn.dropout(output_x_noise, 0.5)\n\n                        # loss between input without noise and output computed\n                        # on noisy values\n                        tf.add_to_collection(LOSSES,\n                                             self._mse(output_x_noise, input_x))\n                        input_x_noise = tf.stop_gradient(output_x_noise)\n                        input_padded_noise = self._pad(input_x_noise,\n                                                       filter_side)\n\n        # The is_training_ placeholder is not used, but we define and return it\n        # in order to respect the expected output cardinality of the get method\n        is_training_ = tf.placeholder_with_default(\n            False, shape=(), name=""is_training_"")\n        return is_training_, output_x_noise\n\n    def _mse(self, input_x, output_x):\n        # 1/2n \\sum^{n}_{i=i}{(x_i - x\'_i)^2}\n        return tf.divide(\n            tf.reduce_mean(tf.square(tf.subtract(input_x, output_x))),\n            2.,\n            name=""mse"")\n\n    def loss(self, predictions, real_values):\n        """"""Return the loss operation between predictions and real_values.\n        Add L2 weight decay term if any.\n        Args:\n            predictions: predicted values\n            real_values: real values\n        Returns:\n            Loss tensor of type float.\n        """"""\n        with tf.variable_scope(\'loss\'):\n            #tf.add_to_collection(LOSSES, self._mse(real_values, predictions))\n            # mse + weight_decay per layer\n            error = tf.add_n(tf.get_collection(LOSSES), name=\'total_loss\')\n\n        return error\n'"
dytb/models/predefined/VGG.py,56,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Build the VGG-like network without additional layers""""""\n\nimport tensorflow as tf\nfrom ..collections import LOSSES\nfrom ..layers import conv, fc\nfrom ..interfaces import Classifier\n\n\nclass VGG(Classifier):\n    """"""Build the VGG-like network without additional layers""""""\n\n    def _inference(self,\n                   images,\n                   num_classes,\n                   is_training_,\n                   train_phase=False,\n                   l2_penalty=0.0):\n        """"""Build the VGG-like network without additional layers.\n\n        Args:\n          images: Images returned from train_inputs() or inputs().\n          num_classes: Number of classes to predict\n          is_training_: enable/disable training ops at run time\n          train_phase: Boolean to enable/disable training ops at build time\n          l2_penalty: float value, weight decay (l2) penalty\n\n        Returns:\n          Logits.\n        """"""\n\n        # Initializer with seed\n        initializer = tf.contrib.layers.variance_scaling_initializer(\n            factor=2.0,\n            mode=\'FAN_IN\',\n            uniform=False,\n            seed=self.seed,\n            dtype=tf.float32)\n\n        with tf.variable_scope(self.__class__.__name__):\n            with tf.variable_scope(\'64\'):\n                with tf.variable_scope(\'conv1\'):\n                    conv1 = conv(\n                        images, [3, 3, 3, 64],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                with tf.variable_scope(\'conv2\'):\n                    conv2 = conv(\n                        conv1, [3, 3, 64, 64],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n            with tf.variable_scope(\'pool1\'):\n                pool1 = tf.nn.max_pool(\n                    conv2,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n\n            with tf.variable_scope(\'128\'):\n                with tf.variable_scope(\'conv3\'):\n                    conv3 = conv(\n                        pool1, [3, 3, 64, 128],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                with tf.variable_scope(\'conv4\'):\n                    conv4 = conv(\n                        conv3, [3, 3, 128, 128],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n            with tf.variable_scope(\'pool2\'):\n                pool2 = tf.nn.max_pool(\n                    conv4,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n\n            with tf.variable_scope(\'256\'):\n                with tf.variable_scope(\'conv5\'):\n                    conv5 = conv(\n                        pool2, [3, 3, 128, 256],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                with tf.variable_scope(\'conv6\'):\n                    conv6 = conv(\n                        conv5, [3, 3, 256, 256],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                with tf.variable_scope(\'conv7\'):\n                    conv7 = conv(\n                        conv6, [3, 3, 256, 256],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n            with tf.variable_scope(\'pool3\'):\n                pool3 = tf.nn.max_pool(\n                    conv7,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n\n            with tf.variable_scope(\'512\'):\n                with tf.variable_scope(\'conv8\'):\n                    conv8 = conv(\n                        pool3, [3, 3, 256, 512],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                with tf.variable_scope(\'conv9\'):\n                    conv9 = conv(\n                        conv8, [3, 3, 512, 512],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                with tf.variable_scope(\'conv10\'):\n                    conv10 = conv(\n                        conv9, [3, 3, 512, 512],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n            with tf.variable_scope(\'pool4\'):\n                pool4 = tf.nn.max_pool(\n                    conv10,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n\n            with tf.variable_scope(\'512b2\'):\n                with tf.variable_scope(\'conv11\'):\n                    conv11 = conv(\n                        pool4, [3, 3, 512, 512],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                with tf.variable_scope(\'conv12\'):\n                    conv12 = conv(\n                        conv11, [3, 3, 512, 512],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                with tf.variable_scope(\'conv13\'):\n                    conv13 = conv(\n                        conv12, [3, 3, 512, 512],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n            with tf.variable_scope(\'pool5\'):\n                pool5 = tf.nn.max_pool(\n                    conv13,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n                pool5 = tf.reshape(pool5, [-1, 512])\n\n            with tf.variable_scope(\'fc\'):\n                fc1 = fc(\n                    pool5, [512, 512],\n                    train_phase,\n                    activation=tf.nn.relu,\n                    wd=l2_penalty,\n                    initializer=initializer)\n\n            with tf.variable_scope(\'softmax_linear\'):\n                logits = fc(\n                    fc1, [512, num_classes],\n                    train_phase,\n                    initializer=initializer)\n        return logits\n\n    def loss(self, logits, labels):\n        """"""Add L2Loss to all the trainable variables.\n        Args:\n          logits: Logits from get().\n          labels: Labels from train_inputs or inputs(). 1-D tensor\n                  of shape [batch_size]\n\n        Returns:\n          Loss tensor of type float.\n        """"""\n        with tf.variable_scope(\'loss\'):\n            # Calculate the average cross entropy loss across the batch.\n            labels = tf.cast(labels, tf.int64)\n            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n                logits=logits, labels=labels, name=\'cross_entropy_per_example\')\n            cross_entropy_mean = tf.reduce_mean(\n                cross_entropy, name=\'cross_entropy\')\n            tf.add_to_collection(LOSSES, cross_entropy_mean)\n\n            # The total loss is defined as the cross entropy loss plus all of the weight\n            # decay terms (L2 loss).\n            error = tf.add_n(tf.get_collection(LOSSES), name=\'total_loss\')\n\n        return error\n\n    def get(self, images, num_classes, train_phase=False, l2_penalty=0.0):\n        """""" define the model with its inputs.\n        Use this function to define the model in training and when exporting the model\n        in the protobuf format.\n\n        Args:\n            images: model input\n            num_classes: number of classes to predict\n            train_phase: set it to True when defining the model, during train\n            l2_penalty: float value, weight decay (l2) penalty\n\n        Returns:\n            is_training_: tf.bool placeholder enable/disable training ops at run time\n            logits: the model output\n        """"""\n        is_training_ = tf.placeholder_with_default(\n            False, shape=(), name=""is_training_"")\n        # build a graph that computes the logits predictions from the images\n        logits = self._inference(images, num_classes, is_training_, train_phase,\n                                 l2_penalty)\n\n        return is_training_, logits\n'"
dytb/models/predefined/VGGBN.py,55,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Builds the VGG-like network with BN applied to every layer""""""\n\nimport tensorflow as tf\nfrom ..collections import LOSSES\nfrom ..layers import conv, fc, batch_norm\nfrom ..interfaces import Classifier\n\n\nclass VGGBN(Classifier):\n    """"""Builds the VGG-like network with BN applied to every layer""""""\n\n    def _inference(self,\n                   images,\n                   num_classes,\n                   is_training_,\n                   train_phase=False,\n                   l2_penalty=0.0):\n        """"""Builds the VGG-like network with BN applied to every layer.\n\n        Args:\n            images: Images returned from train_inputs() or inputs().\n            num_classes: Number of classes to predict\n            is_training_: enable/disable training ops at run time\n            train_phase: Boolean to enable/disable train ops at build time\n            l2_penalty: float value, weight decay (l2) penalty\n\n        Returns:\n          Logits.\n        """"""\n\n        # Initializer with seed\n        initializer = tf.contrib.layers.variance_scaling_initializer(\n            factor=2.0,\n            mode=\'FAN_IN\',\n            uniform=False,\n            seed=self.seed,\n            dtype=tf.float32)\n\n        with tf.variable_scope(self.__class__.__name__):\n            with tf.variable_scope(\'64\'):\n                with tf.variable_scope(\'conv1\'):\n                    conv1 = tf.nn.relu(\n                        batch_norm(\n                            conv(\n                                images, [3, 3, 3, 64],\n                                1,\n                                \'SAME\',\n                                train_phase,\n                                bias_term=False,\n                                wd=l2_penalty,\n                                initializer=initializer), is_training_\n                            if train_phase else False))\n\n                with tf.variable_scope(\'conv2\'):\n                    conv2 = tf.nn.relu(\n                        batch_norm(\n                            conv(\n                                conv1, [3, 3, 64, 64],\n                                1,\n                                \'SAME\',\n                                train_phase,\n                                bias_term=False,\n                                wd=l2_penalty,\n                                initializer=initializer), is_training_\n                            if train_phase else False))\n\n            with tf.variable_scope(\'pool1\'):\n                pool1 = tf.nn.max_pool(\n                    conv2,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n\n            with tf.variable_scope(\'128\'):\n                with tf.variable_scope(\'conv3\'):\n                    conv3 = tf.nn.relu(\n                        batch_norm(\n                            conv(\n                                pool1, [3, 3, 64, 128],\n                                1,\n                                \'SAME\',\n                                train_phase,\n                                bias_term=False,\n                                wd=l2_penalty,\n                                initializer=initializer), is_training_\n                            if train_phase else False))\n\n                with tf.variable_scope(\'conv4\'):\n                    conv4 = tf.nn.relu(\n                        batch_norm(\n                            conv(\n                                conv3, [3, 3, 128, 128],\n                                1,\n                                \'SAME\',\n                                train_phase,\n                                bias_term=False,\n                                wd=l2_penalty,\n                                initializer=initializer), is_training_\n                            if train_phase else False))\n\n            with tf.variable_scope(\'pool2\'):\n                pool2 = tf.nn.max_pool(\n                    conv4,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n\n            with tf.variable_scope(\'256\'):\n                with tf.variable_scope(\'conv5\'):\n                    conv5 = tf.nn.relu(\n                        batch_norm(\n                            conv(\n                                pool2, [3, 3, 128, 256],\n                                1,\n                                \'SAME\',\n                                train_phase,\n                                bias_term=False,\n                                wd=l2_penalty,\n                                initializer=initializer), is_training_\n                            if train_phase else False))\n\n                with tf.variable_scope(\'conv6\'):\n                    conv6 = tf.nn.relu(\n                        batch_norm(\n                            conv(\n                                conv5, [3, 3, 256, 256],\n                                1,\n                                \'SAME\',\n                                train_phase,\n                                bias_term=False,\n                                wd=l2_penalty,\n                                initializer=initializer), is_training_\n                            if train_phase else False))\n\n                with tf.variable_scope(\'conv7\'):\n                    conv7 = tf.nn.relu(\n                        batch_norm(\n                            conv(\n                                conv6, [3, 3, 256, 256],\n                                1,\n                                \'SAME\',\n                                train_phase,\n                                bias_term=False,\n                                wd=l2_penalty,\n                                initializer=initializer), is_training_\n                            if train_phase else False))\n\n            with tf.variable_scope(\'pool3\'):\n                pool3 = tf.nn.max_pool(\n                    conv7,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n\n            with tf.variable_scope(\'512\'):\n                with tf.variable_scope(\'conv8\'):\n                    conv8 = tf.nn.relu(\n                        batch_norm(\n                            conv(\n                                pool3, [3, 3, 256, 512],\n                                1,\n                                \'SAME\',\n                                train_phase,\n                                bias_term=False,\n                                wd=l2_penalty,\n                                initializer=initializer), is_training_\n                            if train_phase else False))\n\n                with tf.variable_scope(\'conv9\'):\n                    conv9 = tf.nn.relu(\n                        batch_norm(\n                            conv(\n                                conv8, [3, 3, 512, 512],\n                                1,\n                                \'SAME\',\n                                train_phase,\n                                bias_term=False,\n                                wd=l2_penalty,\n                                initializer=initializer), is_training_\n                            if train_phase else False))\n\n                with tf.variable_scope(\'conv10\'):\n                    conv10 = tf.nn.relu(\n                        batch_norm(\n                            conv(\n                                conv9, [3, 3, 512, 512],\n                                1,\n                                \'SAME\',\n                                train_phase,\n                                bias_term=False,\n                                wd=l2_penalty,\n                                initializer=initializer), is_training_\n                            if train_phase else False))\n\n            with tf.variable_scope(\'pool4\'):\n                pool4 = tf.nn.max_pool(\n                    conv10,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n\n            with tf.variable_scope(\'512b2\'):\n                with tf.variable_scope(\'conv11\'):\n                    conv11 = tf.nn.relu(\n                        batch_norm(\n                            conv(\n                                pool4, [3, 3, 512, 512],\n                                1,\n                                \'SAME\',\n                                train_phase,\n                                bias_term=False,\n                                wd=l2_penalty,\n                                initializer=initializer), is_training_\n                            if train_phase else False))\n\n                with tf.variable_scope(\'conv12\'):\n                    conv12 = tf.nn.relu(\n                        batch_norm(\n                            conv(\n                                conv11, [3, 3, 512, 512],\n                                1,\n                                \'SAME\',\n                                train_phase,\n                                bias_term=False,\n                                wd=l2_penalty,\n                                initializer=initializer), is_training_\n                            if train_phase else False))\n\n                with tf.variable_scope(\'conv13\'):\n                    conv13 = tf.nn.relu(\n                        batch_norm(\n                            conv(\n                                conv12, [3, 3, 512, 512],\n                                1,\n                                \'SAME\',\n                                train_phase,\n                                bias_term=False,\n                                wd=l2_penalty,\n                                initializer=initializer), is_training_\n                            if train_phase else False))\n\n            with tf.variable_scope(\'pool5\'):\n                pool5 = tf.nn.max_pool(\n                    conv13,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n                pool5 = tf.reshape(pool5, [-1, 512])\n\n            with tf.variable_scope(\'fc\'):\n                fc1 = tf.nn.relu(\n                    batch_norm(\n                        fc(pool5, [512, 512],\n                           train_phase,\n                           bias_term=False,\n                           wd=l2_penalty,\n                           initializer=initializer), is_training_\n                        if train_phase else False))\n\n            with tf.variable_scope(\'softmax_linear\'):\n                # no batch norm in the classification head\n                logits = fc(\n                    fc1, [512, num_classes],\n                    train_phase,\n                    initializer=initializer)\n        return logits\n\n    def loss(self, logits, labels):\n        """"""Add L2Loss to all the trainable variables.\n        Args:\n          logits: Logits from get().\n          labels: Labels from train_inputs or inputs(). 1-D tensor\n                  of shape [batch_size]\n\n        Returns:\n          Loss tensor of type float.\n        """"""\n        with tf.variable_scope(\'loss\'):\n            # Calculate the average cross entropy loss across the batch.\n            labels = tf.cast(labels, tf.int64)\n            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n                logits=logits, labels=labels, name=\'cross_entropy_per_example\')\n            cross_entropy_mean = tf.reduce_mean(\n                cross_entropy, name=\'cross_entropy\')\n            tf.add_to_collection(LOSSES, cross_entropy_mean)\n\n            # The total loss is defined as the cross entropy loss plus all of the weight\n            # decay terms (L2 loss).\n            error = tf.add_n(tf.get_collection(LOSSES), name=\'total_loss\')\n\n        return error\n\n    def get(self, images, num_classes, train_phase=False, l2_penalty=0.0):\n        """""" define the model with its inputs.\n        Use this function to define the model in training and when exporting the model\n        in the protobuf format.\n\n        Args:\n            images: model input\n            num_classes: number of classes to predict\n            train_phase: set it to True when defining the model, during train\n            l2_penalty: float value, weight decay (l2) penalty\n\n        Returns:\n            is_training_: enable/disable training ops at run time\n            logits: the model output\n        """"""\n        is_training_ = tf.placeholder_with_default(\n            False, shape=(), name=""is_training_"")\n        # build a graph that computes the logits predictions from the images\n        logits = self._inference(images, num_classes, is_training_, train_phase,\n                                 l2_penalty)\n\n        return is_training_, logits\n'"
dytb/models/predefined/VGGDirectDropout.py,58,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Builds the VGG-like network with direct dropout layers\napplyed after avery layer of neurons""""""\n\nimport tensorflow as tf\nfrom ..collections import LOSSES\nfrom ..layers import conv, fc, direct_dropout\nfrom ..interfaces import Classifier\n\n\nclass VGGDirectDropout(Classifier):\n    """"""Builds the VGG-like network with direct dropout layers\n    applyed after avery layer of neurons""""""\n\n    def _inference(self,\n                   images,\n                   num_classes,\n                   is_training_,\n                   train_phase=False,\n                   l2_penalty=0.0):\n        """"""Builds the VGG-like network with direct dropout layers\n        applyed after avery layer of neurons.\n\n        Args:\n          images: Images returned from train_inputs() or inputs().\n          num_classes: Number of classes to predict\n          is_training_: enable/disable training ops at run time\n          train_phase: Boolean to enable/disable training ops at build time\n          l2_penalty: float value, weight decay (l2) penalty\n\n        Returns:\n          Logits.\n        """"""\n\n        # Initializer with seed\n        initializer = tf.contrib.layers.variance_scaling_initializer(\n            factor=2.0,\n            mode=\'FAN_IN\',\n            uniform=False,\n            seed=self.seed,\n            dtype=tf.float32)\n\n        def direct_drop(layer, prob):\n            """""" Build a condition node if we are in train_phase. thus we can use the\n            is_training_ placeholder to switch.\n            Build a prob*layer node when we\'re not in train_phase.\n            Returns the correct node""""""\n\n            if train_phase:\n                layer = tf.cond(\n                    tf.equal(is_training_, True),\n                    lambda: direct_dropout(layer, prob), lambda: prob * layer)\n            else:\n                layer = prob * layer\n            return layer\n\n        with tf.variable_scope(self.__class__.__name__):\n            with tf.variable_scope(\'64\'):\n                with tf.variable_scope(\'conv1\'):\n                    conv1 = conv(\n                        images, [3, 3, 3, 64],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n                    direct_drop(conv1, 0.7)\n\n                with tf.variable_scope(\'conv2\'):\n                    conv2 = conv(\n                        conv1, [3, 3, 64, 64],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n                    direct_drop(conv2, 0.6)\n\n            with tf.variable_scope(\'pool1\'):\n                pool1 = tf.nn.max_pool(\n                    conv2,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n\n            with tf.variable_scope(\'128\'):\n                with tf.variable_scope(\'conv3\'):\n                    conv3 = conv(\n                        pool1, [3, 3, 64, 128],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                    conv3 = direct_drop(conv3, 0.6)\n\n                with tf.variable_scope(\'conv4\'):\n                    conv4 = conv(\n                        conv3, [3, 3, 128, 128],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                    conv4 = direct_drop(conv4, 0.6)\n\n            with tf.variable_scope(\'pool2\'):\n                pool2 = tf.nn.max_pool(\n                    conv4,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n\n            with tf.variable_scope(\'256\'):\n                with tf.variable_scope(\'conv5\'):\n                    conv5 = conv(\n                        pool2, [3, 3, 128, 256],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                    conv5 = direct_drop(conv5, 0.6)\n\n                with tf.variable_scope(\'conv6\'):\n                    conv6 = conv(\n                        conv5, [3, 3, 256, 256],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                    conv6 = direct_drop(conv6, 0.6)\n\n                with tf.variable_scope(\'conv7\'):\n                    conv7 = conv(\n                        conv6, [3, 3, 256, 256],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                    conv7 = direct_drop(conv7, 0.6)\n\n            with tf.variable_scope(\'pool3\'):\n                pool3 = tf.nn.max_pool(\n                    conv7,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n\n            with tf.variable_scope(\'512\'):\n                with tf.variable_scope(\'conv8\'):\n                    conv8 = conv(\n                        pool3, [3, 3, 256, 512],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                    conv8 = direct_drop(conv8, 0.6)\n\n                with tf.variable_scope(\'conv9\'):\n                    conv9 = conv(\n                        conv8, [3, 3, 512, 512],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                    conv9 = direct_drop(conv9, 0.6)\n\n                with tf.variable_scope(\'conv10\'):\n                    conv10 = conv(\n                        conv9, [3, 3, 512, 512],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                    conv10 = direct_drop(conv10, 0.6)\n\n            with tf.variable_scope(\'pool4\'):\n                pool4 = tf.nn.max_pool(\n                    conv10,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n\n            with tf.variable_scope(\'512b2\'):\n                with tf.variable_scope(\'conv11\'):\n                    conv11 = conv(\n                        pool4, [3, 3, 512, 512],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                    conv11 = direct_drop(conv11, 0.6)\n\n                with tf.variable_scope(\'conv12\'):\n                    conv12 = conv(\n                        conv11, [3, 3, 512, 512],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                    conv12 = direct_drop(conv12, 0.6)\n\n                with tf.variable_scope(\'conv13\'):\n                    conv13 = conv(\n                        conv12, [3, 3, 512, 512],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                    conv13 = direct_drop(conv13, 0.6)\n\n            with tf.variable_scope(\'pool5\'):\n                pool5 = tf.nn.max_pool(\n                    conv13,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n\n                pool5 = tf.reshape(pool5, [-1, 512])\n\n            with tf.variable_scope(\'fc\'):\n                fc1 = fc(\n                    pool5, [512, 512],\n                    train_phase,\n                    activation=tf.nn.relu,\n                    wd=l2_penalty,\n                    initializer=initializer)\n                fc1 = direct_drop(fc1, 0.5)\n\n            with tf.variable_scope(\'softmax_linear\'):\n                logits = fc(\n                    fc1, [512, num_classes],\n                    train_phase,\n                    initializer=initializer)\n        return logits\n\n    def loss(self, logits, labels):\n        """"""Add L2Loss to all the trainable variables.\n        Args:\n          logits: Logits from get().\n          labels: Labels from train_inputs or inputs(). 1-D tensor\n                  of shape [batch_size]\n\n        Returns:\n          Loss tensor of type float.\n        """"""\n        with tf.variable_scope(\'loss\'):\n            # Calculate the average cross entropy loss across the batch.\n            labels = tf.cast(labels, tf.int64)\n            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n                logits=logits, labels=labels, name=\'cross_entropy_per_example\')\n            cross_entropy_mean = tf.reduce_mean(\n                cross_entropy, name=\'cross_entropy\')\n            tf.add_to_collection(LOSSES, cross_entropy_mean)\n\n            # The total loss is defined as the cross entropy loss plus all of the weight\n            # decay terms (L2 loss).\n            error = tf.add_n(tf.get_collection(LOSSES), name=\'total_loss\')\n\n        return error\n\n    def get(self, images, num_classes, train_phase=False, l2_penalty=0.0):\n        """""" define the model with its inputs.\n        Use this function to define the model in training and when exporting the model\n        in the protobuf format.\n\n        Args:\n            images: model input\n            num_classes: number of classes to predict\n            train_phase: set it to True when defining the model, during train\n            l2_penalty: float value, weight decay (l2) penalty\n\n        Returns:\n            is_training_: tf.bool placeholder to enable/disable training ops at run time\n            logits: the model output\n        """"""\n        is_training_ = tf.placeholder_with_default(\n            False, shape=(), name=""is_training_"")\n        # build a graph that computes the logits predictions from the images\n        logits = self._inference(images, num_classes, is_training_, train_phase,\n                                 l2_penalty)\n\n        return is_training_, logits\n'"
dytb/models/predefined/VGGDropout.py,69,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Builds the VGG-like network with dropout layers\napplyed after avery layer of neurons""""""\n\nimport tensorflow as tf\nfrom ..collections import LOSSES\nfrom ..layers import conv, fc\nfrom ..interfaces import Classifier\n\n\nclass VGGDropout(Classifier):\n    """"""Builds the VGG-like network with dropout layers applyed\n    after avery layer of neurons""""""\n\n    def _inference(self,\n                   images,\n                   num_classes,\n                   is_training_,\n                   train_phase=False,\n                   l2_penalty=0.0):\n        """"""Builds the VGG-like network with inverted dropout layers\n        applyed after avery layer of neurons.\n\n        Args:\n          images: Images returned from train_inputs() or inputs().\n          num_classes: Number of classes to predict\n          is_training_: enable/disable training ops at run time\n          train_phase: Boolean to enable/disable training ops at build time\n          l2_penalty: float value, weight decay (l2) penalty\n\n        Returns:\n          Logits.\n        """"""\n\n        # Initializer with seed\n        initializer = tf.contrib.layers.variance_scaling_initializer(\n            factor=2.0,\n            mode=\'FAN_IN\',\n            uniform=False,\n            seed=self.seed,\n            dtype=tf.float32)\n\n        with tf.variable_scope(self.__class__.__name__):\n            with tf.variable_scope(\'64\'):\n                with tf.variable_scope(\'conv1\'):\n                    conv1 = conv(\n                        images, [3, 3, 3, 64],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n                    if train_phase:\n                        conv1 = tf.nn.dropout(conv1, 0.7)\n\n                with tf.variable_scope(\'conv2\'):\n                    conv2 = conv(\n                        conv1, [3, 3, 64, 64],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                    if train_phase:\n                        conv2 = tf.nn.dropout(conv2, 0.6)\n\n            with tf.variable_scope(\'pool1\'):\n                pool1 = tf.nn.max_pool(\n                    conv2,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n\n            with tf.variable_scope(\'128\'):\n                with tf.variable_scope(\'conv3\'):\n                    conv3 = conv(\n                        pool1, [3, 3, 64, 128],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                    if train_phase:\n                        conv3 = tf.nn.dropout(conv3, 0.6)\n\n                with tf.variable_scope(\'conv4\'):\n                    conv4 = conv(\n                        conv3, [3, 3, 128, 128],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                    if train_phase:\n                        conv4 = tf.nn.dropout(conv4, 0.6)\n\n            with tf.variable_scope(\'pool2\'):\n                pool2 = tf.nn.max_pool(\n                    conv4,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n\n            with tf.variable_scope(\'256\'):\n                with tf.variable_scope(\'conv5\'):\n                    conv5 = conv(\n                        pool2, [3, 3, 128, 256],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                    if train_phase:\n                        conv5 = tf.nn.dropout(conv5, 0.6)\n\n                with tf.variable_scope(\'conv6\'):\n                    conv6 = conv(\n                        conv5, [3, 3, 256, 256],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                    if train_phase:\n                        conv6 = tf.nn.dropout(conv6, 0.6)\n\n                with tf.variable_scope(\'conv7\'):\n                    conv7 = conv(\n                        conv6, [3, 3, 256, 256],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                    if train_phase:\n                        conv7 = tf.nn.dropout(conv7, 0.6)\n\n            with tf.variable_scope(\'pool3\'):\n                pool3 = tf.nn.max_pool(\n                    conv7,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n\n            with tf.variable_scope(\'512\'):\n                with tf.variable_scope(\'conv8\'):\n                    conv8 = conv(\n                        pool3, [3, 3, 256, 512],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                    if train_phase:\n                        conv8 = tf.nn.dropout(conv8, 0.6)\n\n                with tf.variable_scope(\'conv9\'):\n                    conv9 = conv(\n                        conv8, [3, 3, 512, 512],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                    if train_phase:\n                        conv9 = tf.nn.dropout(conv9, 0.6)\n\n                with tf.variable_scope(\'conv10\'):\n                    conv10 = conv(\n                        conv9, [3, 3, 512, 512],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                    if train_phase:\n                        conv10 = tf.nn.dropout(conv10, 0.6)\n\n            with tf.variable_scope(\'pool4\'):\n                pool4 = tf.nn.max_pool(\n                    conv10,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n\n            with tf.variable_scope(\'512b2\'):\n                with tf.variable_scope(\'conv11\'):\n                    conv11 = conv(\n                        pool4, [3, 3, 512, 512],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                    if train_phase:\n                        conv11 = tf.nn.dropout(conv11, 0.6)\n\n                with tf.variable_scope(\'conv12\'):\n                    conv12 = conv(\n                        conv11, [3, 3, 512, 512],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                    if train_phase:\n                        conv12 = tf.nn.dropout(conv12, 0.6)\n\n                with tf.variable_scope(\'conv13\'):\n                    conv13 = conv(\n                        conv12, [3, 3, 512, 512],\n                        1,\n                        \'SAME\',\n                        train_phase,\n                        activation=tf.nn.relu,\n                        wd=l2_penalty,\n                        initializer=initializer)\n\n                    if train_phase:\n                        conv13 = tf.nn.dropout(conv13, 0.6)\n\n            with tf.variable_scope(\'pool5\'):\n                pool5 = tf.nn.max_pool(\n                    conv13,\n                    ksize=[1, 2, 2, 1],\n                    strides=[1, 2, 2, 1],\n                    padding=\'VALID\')\n\n                pool5 = tf.reshape(pool5, [-1, 512])\n\n            with tf.variable_scope(\'fc\'):\n                fc1 = fc(\n                    pool5, [512, 512],\n                    train_phase,\n                    activation=tf.nn.relu,\n                    wd=l2_penalty,\n                    initializer=initializer)\n\n                if train_phase:\n                    fc1 = tf.nn.dropout(fc1, 0.5)\n\n            with tf.variable_scope(\'softmax_linear\'):\n                logits = fc(\n                    fc1, [512, num_classes],\n                    train_phase,\n                    initializer=initializer)\n        return logits\n\n    def loss(self, logits, labels):\n        """"""Add L2Loss to all the trainable variables.\n        Args:\n          logits: Logits from get().\n          labels: Labels from train_inputs or inputs(). 1-D tensor\n                  of shape [batch_size]\n\n        Returns:\n          Loss tensor of type float.\n        """"""\n        with tf.variable_scope(\'loss\'):\n            # Calculate the average cross entropy loss across the batch.\n            labels = tf.cast(labels, tf.int64)\n            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n                logits=logits, labels=labels, name=\'cross_entropy_per_example\')\n            cross_entropy_mean = tf.reduce_mean(\n                cross_entropy, name=\'cross_entropy\')\n            tf.add_to_collection(LOSSES, cross_entropy_mean)\n\n            # The total loss is defined as the cross entropy loss plus all of the weight\n            # decay terms (L2 loss).\n            error = tf.add_n(tf.get_collection(LOSSES), name=\'total_loss\')\n\n        return error\n\n    def get(self, images, num_classes, train_phase=False, l2_penalty=0.0):\n        """""" define the model with its inputs.\n        Use this function to define the model in training and when exporting the model\n        in the protobuf format.\n\n        Args:\n            images: model input\n            num_classes: number of classes to predict\n            train_phase: set it to True when defining the model, during train\n            l2_penalty: float value, weight decay (l2) penalty\n\n        Returns:\n            is_training_: enable/disable training ops at run time\n            logits: the model output\n        """"""\n        is_training_ = tf.placeholder_with_default(\n            False, shape=(), name=""is_training_"")\n        # build a graph that computes the logits predictions from the images\n        logits = self._inference(images, num_classes, is_training_, train_phase,\n                                 l2_penalty)\n\n        return is_training_, logits\n'"
dytb/models/predefined/__init__.py,0,b''
dytb/trainer/utils/__init__.py,0,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n'"
dytb/trainer/utils/builders.py,11,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Utilities used by the trainers""""""\n\nimport os\nimport tensorflow as tf\n\nfrom ...models.utils import variables_to_save, variables_to_restore, tf_log\n\n\ndef build_optimizer(args, steps, global_step):\n    """"""Build the specified optimizer, log the learning rate and enalble\n    learning rate decay is specified.\n    Args:\n        args: the optimization argument dict\n        global_step: integer tensor, the current training step\n    Returns:\n        optimizer: tf.Optimizer object initialized\n    """"""\n    # Extract the initial learning rate\n    initial_lr = float(args[""gd""][""args""][\'learning_rate\'])\n\n    if args[""lr_decay""][""enabled""]:\n        # Decay the learning rate exponentially based on the number of steps.\n        learning_rate = tf.train.exponential_decay(\n            initial_lr,\n            global_step,\n            steps[""decay""],\n            args[""lr_decay""][""factor""],\n            staircase=True)\n        # Update the learning rate parameter of the optimizer\n        args[""gd""][""args""][\'learning_rate\'] = learning_rate\n        # Log the learning rate\n        tf_log(tf.summary.scalar(\'learning_rate\', learning_rate))\n    else:\n        learning_rate = tf.constant(initial_lr)\n\n    # Instantiate the optimizer\n    optimizer = args[""gd""][""optimizer""](**args[""gd""][""args""])\n    return optimizer\n\n\ndef build_restore_saver(variables_to_add=None, scopes_to_remove=None):\n    """"""Return a saver that restores every trainable variable that\'s not\n    under a scope to remove.\n    Args:\n        variables_to_add: list of variables to add\n        scopes_to_remove: list of scopes to remove\n    """"""\n    if variables_to_add is None:\n        variables_to_add = []\n\n    if scopes_to_remove is None:\n        scopes_to_remove = []\n\n    restore_saver = tf.train.Saver(\n        variables_to_restore(variables_to_add, scopes_to_remove))\n    return restore_saver\n\n\ndef build_train_savers(variables_to_add=None):\n    """"""Add variables_to_add to the collection of variables to save.\n    Args:\n        variables_to_add: list of variables to add\n    Returns:\n        train_saver: saver to use to log the training model\n        best_saver: saver used to save the best model\n    """"""\n    if variables_to_add is None:\n        variables_to_add = []\n    variables = variables_to_save(variables_to_add)\n    train_saver = tf.train.Saver(variables, max_to_keep=2)\n    best_saver = tf.train.Saver(variables, max_to_keep=1)\n    return train_saver, best_saver\n\n\ndef build_loggers(graph, paths):\n    """"""Build the FileWriter object used to log summaries.\n    Args:\n        graph: the graph which operations to log refers to\n        paths: dict of paths\n    Returns:\n        train_log: tf.summary.FileWriter object to log train op\n        validation_log: tf.summary.FileWriter object to log validation op\n    """"""\n    train_log = tf.summary.FileWriter(\n        os.path.join(paths[""log""], \'train\'), graph=graph)\n    validation_log = tf.summary.FileWriter(\n        os.path.join(paths[""log""], \'validation\'), graph=graph)\n    return train_log, validation_log\n'"
dytb/trainer/utils/flow.py,2,"b'#Copyright (C) 2017 Paolo Galeone <nessuno@nerdz.eu>\n#\n#This Source Code Form is subject to the terms of the Mozilla Public\n#License, v. 2.0. If a copy of the MPL was not distributed with this\n#file, you can obtain one at http://mozilla.org/MPL/2.0/.\n#Exhibit B is not attached; this software is compatible with the\n#licenses expressed under Section 1.12 of the MPL v2.\n""""""Utilities to control to flow execution of the trainers""""""\n\nimport sys\nimport tensorflow as tf\n\nfrom .builders import build_restore_saver\n\n\ndef restore_or_restart(args, paths, sess):\n    """"""Restore actual session or restart the training.\n    If SESS.checkpoint_path is setted, start a new train\n    loading the weight from the lastest checkpoint in that path\n    Args:\n        sess: session\n        paths: dict of paths\n    """"""\n\n    # first check if exists and checkpoint_path passed\n    # from where to load the weights.\n    # Return error if there\'s not\n    pretrained_checkpoint = None\n    if args[""checkpoint_path""] != \'\':\n        pretrained_checkpoint = tf.train.latest_checkpoint(\n            args[""checkpoint_path""])\n        if not pretrained_checkpoint:\n            print(""[E] {} not valid"".format(args[""checkpoint_path""]))\n            sys.exit(-1)\n\n    if not args[""force_restart""]:\n        # continue training checkpoint\n        continue_checkpoint = tf.train.latest_checkpoint(paths[""log""])\n        if continue_checkpoint:\n            restore_saver = build_restore_saver(\n                None, scopes_to_remove=args[""exclude_scopes""])\n            restore_saver.restore(sess, continue_checkpoint)\n        # else if the continue checkpoint does not exists\n        # and the pretrained checkpoint has been specified\n        # load the weights from the pretrained checkpoint\n        elif pretrained_checkpoint:\n            restore_saver = build_restore_saver(\n                [], scopes_to_remove=args[""exclude_scopes""])\n            restore_saver.restore(sess, pretrained_checkpoint)\n        else:\n            print(\'[!] No checkpoint file found\')\n'"
