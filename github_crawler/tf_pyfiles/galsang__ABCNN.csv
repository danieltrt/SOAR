file_path,api_count,code
ABCNN.py,53,"b'import tensorflow as tf\nimport numpy as np\n\n\nclass ABCNN():\n    def __init__(self, s, w, l2_reg, model_type, num_features, d0=300, di=50, num_classes=2, num_layers=2):\n        """"""\n        Implmenentaion of ABCNNs\n        (https://arxiv.org/pdf/1512.05193.pdf)\n\n        :param s: sentence length\n        :param w: filter width\n        :param l2_reg: L2 regularization coefficient\n        :param model_type: Type of the network(BCNN, ABCNN1, ABCNN2, ABCNN3).\n        :param num_features: The number of pre-set features(not coming from CNN) used in the output layer.\n        :param d0: dimensionality of word embedding(default: 300)\n        :param di: The number of convolution kernels (default: 50)\n        :param num_classes: The number of classes for answers.\n        :param num_layers: The number of convolution layers.\n        """"""\n\n        self.x1 = tf.placeholder(tf.float32, shape=[None, d0, s], name=""x1"")\n        self.x2 = tf.placeholder(tf.float32, shape=[None, d0, s], name=""x2"")\n        self.y = tf.placeholder(tf.int32, shape=[None], name=""y"")\n        self.features = tf.placeholder(tf.float32, shape=[None, num_features], name=""features"")\n\n        # zero padding to inputs for wide convolution\n        def pad_for_wide_conv(x):\n            return tf.pad(x, np.array([[0, 0], [0, 0], [w - 1, w - 1], [0, 0]]), ""CONSTANT"", name=""pad_wide_conv"")\n\n        def cos_sim(v1, v2):\n            norm1 = tf.sqrt(tf.reduce_sum(tf.square(v1), axis=1))\n            norm2 = tf.sqrt(tf.reduce_sum(tf.square(v2), axis=1))\n            dot_products = tf.reduce_sum(v1 * v2, axis=1, name=""cos_sim"")\n\n            return dot_products / (norm1 * norm2)\n\n        def euclidean_score(v1, v2):\n            euclidean = tf.sqrt(tf.reduce_sum(tf.square(v1 - v2), axis=1))\n            return 1 / (1 + euclidean)\n\n        def make_attention_mat(x1, x2):\n            # x1, x2 = [batch, height, width, 1] = [batch, d, s, 1]\n            # x2 => [batch, height, 1, width]\n            # [batch, width, wdith] = [batch, s, s]\n            euclidean = tf.sqrt(tf.reduce_sum(tf.square(x1 - tf.matrix_transpose(x2)), axis=1))\n            return 1 / (1 + euclidean)\n\n        def convolution(name_scope, x, d, reuse):\n            with tf.name_scope(name_scope + ""-conv""):\n                with tf.variable_scope(""conv"") as scope:\n                    conv = tf.contrib.layers.conv2d(\n                        inputs=x,\n                        num_outputs=di,\n                        kernel_size=(d, w),\n                        stride=1,\n                        padding=""VALID"",\n                        activation_fn=tf.nn.tanh,\n                        weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n                        weights_regularizer=tf.contrib.layers.l2_regularizer(scale=l2_reg),\n                        biases_initializer=tf.constant_initializer(1e-04),\n                        reuse=reuse,\n                        trainable=True,\n                        scope=scope\n                    )\n                    # Weight: [filter_height, filter_width, in_channels, out_channels]\n                    # output: [batch, 1, input_width+filter_Width-1, out_channels] == [batch, 1, s+w-1, di]\n\n                    # [batch, di, s+w-1, 1]\n                    conv_trans = tf.transpose(conv, [0, 3, 2, 1], name=""conv_trans"")\n                    return conv_trans\n\n        def w_pool(variable_scope, x, attention):\n            # x: [batch, di, s+w-1, 1]\n            # attention: [batch, s+w-1]\n            with tf.variable_scope(variable_scope + ""-w_pool""):\n                if model_type == ""ABCNN2"" or model_type == ""ABCNN3"":\n                    pools = []\n                    # [batch, s+w-1] => [batch, 1, s+w-1, 1]\n                    attention = tf.transpose(tf.expand_dims(tf.expand_dims(attention, -1), -1), [0, 2, 1, 3])\n\n                    for i in range(s):\n                        # [batch, di, w, 1], [batch, 1, w, 1] => [batch, di, 1, 1]\n                        pools.append(tf.reduce_sum(x[:, :, i:i + w, :] * attention[:, :, i:i + w, :],\n                                                   axis=2,\n                                                   keep_dims=True))\n\n                    # [batch, di, s, 1]\n                    w_ap = tf.concat(pools, axis=2, name=""w_ap"")\n                else:\n                    w_ap = tf.layers.average_pooling2d(\n                        inputs=x,\n                        # (pool_height, pool_width)\n                        pool_size=(1, w),\n                        strides=1,\n                        padding=""VALID"",\n                        name=""w_ap""\n                    )\n                    # [batch, di, s, 1]\n\n                return w_ap\n\n        def all_pool(variable_scope, x):\n            with tf.variable_scope(variable_scope + ""-all_pool""):\n                if variable_scope.startswith(""input""):\n                    pool_width = s\n                    d = d0\n                else:\n                    pool_width = s + w - 1\n                    d = di\n\n                all_ap = tf.layers.average_pooling2d(\n                    inputs=x,\n                    # (pool_height, pool_width)\n                    pool_size=(1, pool_width),\n                    strides=1,\n                    padding=""VALID"",\n                    name=""all_ap""\n                )\n                # [batch, di, 1, 1]\n\n                # [batch, di]\n                all_ap_reshaped = tf.reshape(all_ap, [-1, d])\n                #all_ap_reshaped = tf.squeeze(all_ap, [2, 3])\n\n                return all_ap_reshaped\n\n        def CNN_layer(variable_scope, x1, x2, d):\n            # x1, x2 = [batch, d, s, 1]\n            with tf.variable_scope(variable_scope):\n                if model_type == ""ABCNN1"" or model_type == ""ABCNN3"":\n                    with tf.name_scope(""att_mat""):\n                        aW = tf.get_variable(name=""aW"",\n                                             shape=(s, d),\n                                             initializer=tf.contrib.layers.xavier_initializer(),\n                                             regularizer=tf.contrib.layers.l2_regularizer(scale=l2_reg))\n\n                        # [batch, s, s]\n                        att_mat = make_attention_mat(x1, x2)\n\n                        # [batch, s, s] * [s,d] => [batch, s, d]\n                        # matrix transpose => [batch, d, s]\n                        # expand dims => [batch, d, s, 1]\n                        x1_a = tf.expand_dims(tf.matrix_transpose(tf.einsum(""ijk,kl->ijl"", att_mat, aW)), -1)\n                        x2_a = tf.expand_dims(tf.matrix_transpose(\n                            tf.einsum(""ijk,kl->ijl"", tf.matrix_transpose(att_mat), aW)), -1)\n\n                        # [batch, d, s, 2]\n                        x1 = tf.concat([x1, x1_a], axis=3)\n                        x2 = tf.concat([x2, x2_a], axis=3)\n\n                left_conv = convolution(name_scope=""left"", x=pad_for_wide_conv(x1), d=d, reuse=False)\n                right_conv = convolution(name_scope=""right"", x=pad_for_wide_conv(x2), d=d, reuse=True)\n\n                left_attention, right_attention = None, None\n\n                if model_type == ""ABCNN2"" or model_type == ""ABCNN3"":\n                    # [batch, s+w-1, s+w-1]\n                    att_mat = make_attention_mat(left_conv, right_conv)\n                    # [batch, s+w-1], [batch, s+w-1]\n                    left_attention, right_attention = tf.reduce_sum(att_mat, axis=2), tf.reduce_sum(att_mat, axis=1)\n\n                left_wp = w_pool(variable_scope=""left"", x=left_conv, attention=left_attention)\n                left_ap = all_pool(variable_scope=""left"", x=left_conv)\n                right_wp = w_pool(variable_scope=""right"", x=right_conv, attention=right_attention)\n                right_ap = all_pool(variable_scope=""right"", x=right_conv)\n\n                return left_wp, left_ap, right_wp, right_ap\n\n        x1_expanded = tf.expand_dims(self.x1, -1)\n        x2_expanded = tf.expand_dims(self.x2, -1)\n\n        LO_0 = all_pool(variable_scope=""input-left"", x=x1_expanded)\n        RO_0 = all_pool(variable_scope=""input-right"", x=x2_expanded)\n\n        LI_1, LO_1, RI_1, RO_1 = CNN_layer(variable_scope=""CNN-1"", x1=x1_expanded, x2=x2_expanded, d=d0)\n        sims = [cos_sim(LO_0, RO_0), cos_sim(LO_1, RO_1)]\n\n        if num_layers > 1:\n            _, LO_2, _, RO_2 = CNN_layer(variable_scope=""CNN-2"", x1=LI_1, x2=RI_1, d=di)\n            self.test = LO_2\n            self.test2 = RO_2\n            sims.append(cos_sim(LO_2, RO_2))\n\n        with tf.variable_scope(""output-layer""):\n            self.output_features = tf.concat([self.features, tf.stack(sims, axis=1)], axis=1, name=""output_features"")\n\n            self.estimation = tf.contrib.layers.fully_connected(\n                inputs=self.output_features,\n                num_outputs=num_classes,\n                activation_fn=None,\n                weights_initializer=tf.contrib.layers.xavier_initializer(),\n                weights_regularizer=tf.contrib.layers.l2_regularizer(scale=l2_reg),\n                biases_initializer=tf.constant_initializer(1e-04),\n                scope=""FC""\n            )\n\n        self.prediction = tf.contrib.layers.softmax(self.estimation)[:, 1]\n\n        self.cost = tf.add(\n            tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.estimation, labels=self.y)),\n            tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)),\n            name=""cost"")\n\n        tf.summary.scalar(""cost"", self.cost)\n        self.merged = tf.summary.merge_all()\n\n        print(""="" * 50)\n        print(""List of Variables:"")\n        for v in tf.trainable_variables():\n            print(v.name)\n        print(""="" * 50)\n'"
preprocess.py,0,"b'import numpy as np\nimport nltk\nimport gensim\n\n\nclass Word2Vec():\n    def __init__(self):\n        # Load Google\'s pre-trained Word2Vec model.\n        self.model = gensim.models.KeyedVectors.load_word2vec_format(\'./GoogleNews-vectors-negative300.bin\',\n                                                                     binary=True)\n        self.unknowns = np.random.uniform(-0.01, 0.01, 300).astype(""float32"")\n\n    def get(self, word):\n        if word not in self.model.vocab:\n            return self.unknowns\n        else:\n            return self.model.word_vec(word)\n\n\nclass Data():\n    def __init__(self, word2vec, max_len=0):\n        self.s1s, self.s2s, self.labels, self.features = [], [], [], []\n        self.index, self.max_len, self.word2vec = 0, max_len, word2vec\n\n    def open_file(self):\n        pass\n\n    def is_available(self):\n        if self.index < self.data_size:\n            return True\n        else:\n            return False\n\n    def reset_index(self):\n        self.index = 0\n\n    def next(self):\n        if (self.is_available()):\n            self.index += 1\n            return self.data[self.index - 1]\n        else:\n            return\n\n    def next_batch(self, batch_size):\n        batch_size = min(self.data_size - self.index, batch_size)\n        s1_mats, s2_mats = [], []\n\n        for i in range(batch_size):\n            s1 = self.s1s[self.index + i]\n            s2 = self.s2s[self.index + i]\n\n            # [1, d0, s]\n            s1_mats.append(np.expand_dims(np.pad(np.column_stack([self.word2vec.get(w) for w in s1]),\n                                                 [[0, 0], [0, self.max_len - len(s1)]],\n                                                 ""constant""), axis=0))\n\n            s2_mats.append(np.expand_dims(np.pad(np.column_stack([self.word2vec.get(w) for w in s2]),\n                                                 [[0, 0], [0, self.max_len - len(s2)]],\n                                                 ""constant""), axis=0))\n\n        # [batch_size, d0, s]\n        batch_s1s = np.concatenate(s1_mats, axis=0)\n        batch_s2s = np.concatenate(s2_mats, axis=0)\n        batch_labels = self.labels[self.index:self.index + batch_size]\n        batch_features = self.features[self.index:self.index + batch_size]\n\n        self.index += batch_size\n\n        return batch_s1s, batch_s2s, batch_labels, batch_features\n\n\nclass MSRP(Data):\n    def open_file(self, mode, parsing_method=""normal""):\n        with open(""./MSRP_Corpus/msr_paraphrase_"" + mode + "".txt"", ""r"", encoding=""utf-8"") as f:\n            f.readline()\n\n            for line in f:\n                items = line[:-1].split(""\\t"")\n                label = int(items[0])\n                if parsing_method == ""NLTK"":\n                    s1 = nltk.word_tokenize(items[3])\n                    s2 = nltk.word_tokenize(items[4])\n                else:\n                    s1 = items[3].strip().split()\n                    s2 = items[4].strip().split()\n\n                # bleu_score = nltk.translate.bleu_score.sentence_bleu(s1, s2)\n                # sentence_bleu(s1, s2, smoothing_function=nltk.translate.bleu_score.SmoothingFunction.method1)\n\n                self.s1s.append(s1)\n                self.s2s.append(s2)\n                self.labels.append(label)\n                self.features.append([len(s1), len(s2)])\n\n                # double use training data\n                """"""\n                if mode == ""train"":\n                    self.s1s.append(s2)\n                    self.s2s.append(s1)\n                    self.labels.append(label)\n                    self.features.append([len(s2), len(s1)])\n                """"""\n\n                local_max_len = max(len(s1), len(s2))\n                if local_max_len > self.max_len:\n                    self.max_len = local_max_len\n\n        self.data_size = len(self.s1s)\n        self.num_features = len(self.features[0])\n\n\nclass WikiQA(Data):\n    def open_file(self, mode):\n        with open(""./WikiQA_Corpus/WikiQA-"" + mode + "".txt"", ""r"", encoding=""utf-8"") as f:\n            stopwords = nltk.corpus.stopwords.words(""english"")\n\n            for line in f:\n                items = line[:-1].split(""\\t"")\n\n                s1 = items[0].lower().split()\n                # truncate answers to 40 tokens.\n                s2 = items[1].lower().split()[:40]\n                label = int(items[2])\n\n                self.s1s.append(s1)\n                self.s2s.append(s2)\n                self.labels.append(label)\n                word_cnt = len([word for word in s1 if (word not in stopwords) and (word in s2)])\n                self.features.append([len(s1), len(s2), word_cnt])\n\n                local_max_len = max(len(s1), len(s2))\n                if local_max_len > self.max_len:\n                    self.max_len = local_max_len\n\n        self.data_size = len(self.s1s)\n\n        flatten = lambda l: [item for sublist in l for item in sublist]\n        q_vocab = list(set(flatten(self.s1s)))\n        idf = {}\n        for w in q_vocab:\n            idf[w] = np.log(self.data_size / len([1 for s1 in self.s1s if w in s1]))\n\n        for i in range(self.data_size):\n            wgt_word_cnt = sum([idf[word] for word in self.s1s[i] if (word not in stopwords) and (word in self.s2s[i])])\n            self.features[i].append(wgt_word_cnt)\n\n        self.num_features = len(self.features[0])\n'"
show.py,0,"b'\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nwith open(""./experiments/WikiQA-ABCNN1-2-LR.txt"", ""r"", encoding=""utf-8"") as f:\n    f.readline()\n    MAPs, MRRs = [], []\n\n    for line in f:\n        MAP = line[:-1].split(""\\t"")[1]\n        MRR = line[:-1].split(""\\t"")[2]\n\n        MAPs.append(MAP)\n        MRRs.append(MRR)\n\nprint(""max:"", max(MAPs), max(MRRs))\n\nplt.plot(np.arange(1, len(MAPs)+1, 1), MAPs, \'r\')\nplt.plot(np.arange(1, len(MAPs)+1, 1), MRRs, \'b\')\nplt.legend([""MAP"", ""MRR""])\nplt.show()\n\n'"
test.py,4,"b'import tensorflow as tf\nimport sys\nimport numpy as np\n\nfrom preprocess import Word2Vec, MSRP, WikiQA\nfrom ABCNN import ABCNN\nfrom utils import build_path\nfrom sklearn.externals import joblib\n\n\ndef test(w, l2_reg, epoch, max_len, model_type, num_layers, data_type, classifier, word2vec, num_classes=2):\n    if data_type == ""WikiQA"":\n        test_data = WikiQA(word2vec=word2vec, max_len=max_len)\n    else:\n        test_data = MSRP(word2vec=word2vec, max_len=max_len)\n\n    test_data.open_file(mode=""test"")\n\n    model = ABCNN(s=max_len, w=w, l2_reg=l2_reg, model_type=model_type,\n                  num_features=test_data.num_features, num_classes=num_classes, num_layers=num_layers)\n\n    model_path = build_path(""./models/"", data_type, model_type, num_layers)\n    MAPs, MRRs = [], []\n\n    print(""="" * 50)\n    print(""test data size:"", test_data.data_size)\n\n    # Due to GTX 970 memory issues\n    #gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7)\n\n    for e in range(1, epoch + 1):\n        test_data.reset_index()\n\n        #with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n        with tf.Session() as sess:\n            saver = tf.train.Saver()\n            saver.restore(sess, model_path + ""-"" + str(e))\n            print(model_path + ""-"" + str(e), ""restored."")\n\n            if classifier == ""LR"" or classifier == ""SVM"":\n                clf_path = build_path(""./models/"", data_type, model_type, num_layers,\n                                      ""-"" + str(e) + ""-"" + classifier + "".pkl"")\n                clf = joblib.load(clf_path)\n                print(clf_path, ""restored."")\n\n            QA_pairs = {}\n            s1s, s2s, labels, features = test_data.next_batch(batch_size=test_data.data_size)\n\n            for i in range(test_data.data_size):\n                pred, clf_input = sess.run([model.prediction, model.output_features],\n                                           feed_dict={model.x1: np.expand_dims(s1s[i], axis=0),\n                                                      model.x2: np.expand_dims(s2s[i], axis=0),\n                                                      model.y: np.expand_dims(labels[i], axis=0),\n                                                      model.features: np.expand_dims(features[i], axis=0)})\n\n                if classifier == ""LR"":\n                    clf_pred = clf.predict_proba(clf_input)[:, 1]\n                    pred = clf_pred\n                elif classifier == ""SVM"":\n                    clf_pred = clf.decision_function(clf_input)\n                    pred = clf_pred\n\n                s1 = "" "".join(test_data.s1s[i])\n                s2 = "" "".join(test_data.s2s[i])\n\n                if s1 in QA_pairs:\n                    QA_pairs[s1].append((s2, labels[i], np.asscalar(pred)))\n                else:\n                    QA_pairs[s1] = [(s2, labels[i], np.asscalar(pred))]\n\n            # Calculate MAP and MRR for comparing performance\n            MAP, MRR = 0, 0\n            for s1 in QA_pairs.keys():\n                p, AP = 0, 0\n                MRR_check = False\n\n                QA_pairs[s1] = sorted(QA_pairs[s1], key=lambda x: x[-1], reverse=True)\n\n                for idx, (s2, label, prob) in enumerate(QA_pairs[s1]):\n                    if label == 1:\n                        if not MRR_check:\n                            MRR += 1 / (idx + 1)\n                            MRR_check = True\n\n                        p += 1\n                        AP += p / (idx + 1)\n\n                AP /= p\n                MAP += AP\n\n            num_questions = len(QA_pairs.keys())\n            MAP /= num_questions\n            MRR /= num_questions\n\n            MAPs.append(MAP)\n            MRRs.append(MRR)\n\n            print(""[Epoch "" + str(e) + ""] MAP:"", MAP, ""/ MRR:"", MRR)\n\n    print(""="" * 50)\n    print(""max MAP:"", max(MAPs), ""max MRR:"", max(MRRs))\n    print(""="" * 50)\n\n    exp_path = build_path(""./experiments/"", data_type, model_type, num_layers, ""-"" + classifier + "".txt"")\n    with open(exp_path, ""w"", encoding=""utf-8"") as f:\n        print(""Epoch\\tMAP\\tMRR"", file=f)\n        for i in range(e):\n            print(str(i + 1) + ""\\t"" + str(MAPs[i]) + ""\\t"" + str(MRRs[i]), file=f)\n\n\nif __name__ == ""__main__"":\n\n    # Paramters\n    # --ws: window_size\n    # --l2_reg: l2_reg modifier\n    # --epoch: epoch\n    # --max_len: max sentence length\n    # --model_type: model type\n    # --num_layers: number of convolution layers\n    # --data_type: MSRP or WikiQA data\n    # --classifier: Final layout classifier(model, LR, SVM)\n\n    # default parameters\n    params = {\n        ""ws"": 4,\n        ""l2_reg"": 0.0004,\n        ""epoch"": 50,\n        ""max_len"": 40,\n        ""model_type"": ""BCNN"",\n        ""num_layers"": 2,\n        ""data_type"": ""WikiQA"",\n        ""classifier"": ""LR"",\n        ""word2vec"": Word2Vec()\n    }\n\n    if len(sys.argv) > 1:\n        for arg in sys.argv[1:]:\n            k = arg.split(""="")[0][2:]\n            v = arg.split(""="")[1]\n            params[k] = v\n\n    test(w=int(params[""ws""]), l2_reg=float(params[""l2_reg""]), epoch=int(params[""epoch""]),\n         max_len=int(params[""max_len""]), model_type=params[""model_type""],\n         num_layers=int(params[""num_layers""]), data_type=params[""data_type""],\n         classifier=params[""classifier""], word2vec=params[""word2vec""])\n'"
train.py,7,"b'import tensorflow as tf\nimport numpy as np\nimport sys\n\nfrom preprocess import Word2Vec, MSRP, WikiQA\nfrom ABCNN import ABCNN\nfrom utils import build_path\nfrom sklearn import linear_model, svm\nfrom sklearn.externals import joblib\n\n\ndef train(lr, w, l2_reg, epoch, batch_size, model_type, num_layers, data_type, word2vec, num_classes=2):\n    if data_type == ""WikiQA"":\n        train_data = WikiQA(word2vec=word2vec)\n    else:\n        train_data = MSRP(word2vec=word2vec)\n\n    train_data.open_file(mode=""train"")\n\n    print(""="" * 50)\n    print(""training data size:"", train_data.data_size)\n    print(""training max len:"", train_data.max_len)\n    print(""="" * 50)\n\n    model = ABCNN(s=train_data.max_len, w=w, l2_reg=l2_reg, model_type=model_type,\n                  num_features=train_data.num_features, num_classes=num_classes, num_layers=num_layers)\n\n    optimizer = tf.train.AdagradOptimizer(lr, name=""optimizer"").minimize(model.cost)\n\n    # Due to GTX 970 memory issues\n    #gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7)\n\n    # Initialize all variables\n    init = tf.global_variables_initializer()\n\n    # model(parameters) saver\n    saver = tf.train.Saver(max_to_keep=100)\n\n    #with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n    with tf.Session() as sess:\n        #train_summary_writer = tf.summary.FileWriter(""C:/tf_logs/train"", sess.graph)\n\n        sess.run(init)\n\n        print(""="" * 50)\n        for e in range(1, epoch + 1):\n            print(""[Epoch "" + str(e) + ""]"")\n\n            train_data.reset_index()\n            i = 0\n\n            LR = linear_model.LogisticRegression()\n            SVM = svm.LinearSVC()\n            clf_features = []\n\n            while train_data.is_available():\n                i += 1\n\n                batch_x1, batch_x2, batch_y, batch_features = train_data.next_batch(batch_size=batch_size)\n\n                merged, _, c, features = sess.run([model.merged, optimizer, model.cost, model.output_features],\n                                                  feed_dict={model.x1: batch_x1,\n                                                             model.x2: batch_x2,\n                                                             model.y: batch_y,\n                                                             model.features: batch_features})\n\n                clf_features.append(features)\n\n                if i % 100 == 0:\n                    print(""[batch "" + str(i) + ""] cost:"", c)\n                #train_summary_writer.add_summary(merged, i)\n\n            save_path = saver.save(sess, build_path(""./models/"", data_type, model_type, num_layers), global_step=e)\n            print(""model saved as"", save_path)\n\n            clf_features = np.concatenate(clf_features)\n            LR.fit(clf_features, train_data.labels)\n            SVM.fit(clf_features, train_data.labels)\n\n            LR_path = build_path(""./models/"", data_type, model_type, num_layers, ""-"" + str(e) + ""-LR.pkl"")\n            SVM_path = build_path(""./models/"", data_type, model_type, num_layers, ""-"" + str(e) + ""-SVM.pkl"")\n            joblib.dump(LR, LR_path)\n            joblib.dump(SVM, SVM_path)\n\n            print(""LR saved as"", LR_path)\n            print(""SVM saved as"", SVM_path)\n\n        print(""training finished!"")\n        print(""="" * 50)\n\n\nif __name__ == ""__main__"":\n\n    # Paramters\n    # --lr: learning rate\n    # --ws: window_size\n    # --l2_reg: l2_reg modifier\n    # --epoch: epoch\n    # --batch_size: batch size\n    # --model_type: model type\n    # --num_layers: number of convolution layers\n    # --data_type: MSRP or WikiQA data\n\n    # default parameters\n    params = {\n        ""lr"": 0.08,\n        ""ws"": 4,\n        ""l2_reg"": 0.0004,\n        ""epoch"": 50,\n        ""batch_size"": 64,\n        ""model_type"": ""BCNN"",\n        ""num_layers"": 2,\n        ""data_type"": ""WikiQA"",\n        ""word2vec"": Word2Vec()\n    }\n\n    print(""="" * 50)\n    print(""Parameters:"")\n    for k in sorted(params.keys()):\n        print(k, "":"", params[k])\n\n    if len(sys.argv) > 1:\n        for arg in sys.argv[1:]:\n            k = arg.split(""="")[0][2:]\n            v = arg.split(""="")[1]\n            params[k] = v\n\n    train(lr=float(params[""lr""]), w=int(params[""ws""]), l2_reg=float(params[""l2_reg""]), epoch=int(params[""epoch""]),\n          batch_size=int(params[""batch_size""]), model_type=params[""model_type""], num_layers=int(params[""num_layers""]),\n          data_type=params[""data_type""], word2vec=params[""word2vec""])\n'"
utils.py,0,"b'\ndef build_path(prefix, data_type, model_type, num_layers, postpix=""""):\n    return prefix + data_type + ""-"" + model_type + ""-"" + str(num_layers) + postpix\n'"
MSRP_Corpus/make_dev.py,0,"b'import numpy as np\n\nwith open(""msr_paraphrase_train_original.txt"", ""r"", encoding=""utf-8"") as f:\n    attributes = f.readline()\n    lines = f.readlines()\n    np.random.shuffle(lines)\n\n    with open(""msr_paraphrase_dev.txt"", ""w"", encoding=""utf-8"") as f2:\n        print(attributes, file=f2, end="""")\n        for l in lines[:400]:\n            print(l, file=f2, end="""")\n\n    with open(""msr_paraphrase_train.txt"",""w"", encoding=""utf-8"") as f3:\n        print(attributes, file=f3, end="""")\n        for l in lines[400:]:\n            print(l, file=f3, end="""")\n'"
MSRP_Corpus/test.py,0,"b'\nimport random\nwith open(""msr_paraphrase_train.txt"", ""r"", encoding=""utf-8"") as f:\n    attributes = f.readline()\n    sentences = []\n\n    for line in f:\n        if int(line.split(""\\t"")[0]) == 1:\n            sentences.append(line.split(""\\t"")[3])\n\n    selected = random.sample(sentences, k=100)\n\n    with open(""result.txt"", ""w"", encoding=""utf-8"") as f2:\n        print(attributes, file=f2)\n\n        for s in selected:\n            print(""1\\t1\\t1\\t"" + s + ""\\t"" + s, file=f2)\n            print(""0\\t1\\t1\\t"" + s + ""\\t"" + ""I have no idea."", file=f2)\n\n\n\n'"
WikiQA_Corpus/extract.py,0,"b'\nwith open(""WikiQA-train.txt"", ""r"", encoding=""utf-8"") as f:\n    QA_pairs = {}\n\n    for line in f:\n        s1 = line.split(""\\t"")[0]\n        s2 = line.split(""\\t"")[1]\n        label = int(line.split(""\\t"")[2])\n\n        if s1 in QA_pairs.keys():\n            QA_pairs[s1].append((s2,label))\n        else:\n            QA_pairs[s1] = [(s2,label)]\n\n    with open(""WikiQA-train_real.txt"", ""w"", encoding=""utf-8"") as f2:\n        c = 0\n\n        for s1 in QA_pairs:\n            has_answers = 0\n\n            for s2, label in QA_pairs[s1]:\n                if label == 1 :\n                    c += len(QA_pairs[s1])\n                    has_answers = 1\n                    break\n\n            if has_answers:\n                for s2, label in QA_pairs[s1]:\n                    print(s1 + ""\\t"" + s2 + ""\\t"" + str(label), file=f2)\n\n    print(""count:"", c)'"
