file_path,api_count,code
cnn_model.py,22,"b'# coding: utf-8\n\nimport tensorflow as tf\n\n\nclass TCNNConfig(object):\n    """"""CNN\xe9\x85\x8d\xe7\xbd\xae\xe5\x8f\x82\xe6\x95\xb0""""""\n\n    embedding_dim = 64  # \xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe7\xbb\xb4\xe5\xba\xa6\n    seq_length = 600  # \xe5\xba\x8f\xe5\x88\x97\xe9\x95\xbf\xe5\xba\xa6\n    num_classes = 10  # \xe7\xb1\xbb\xe5\x88\xab\xe6\x95\xb0\n    num_filters = 256  # \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe6\x95\xb0\xe7\x9b\xae\n    kernel_size = 5  # \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe5\xb0\xba\xe5\xaf\xb8\n    vocab_size = 5000  # \xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xe8\xbe\xbe\xe5\xb0\x8f\n\n    hidden_dim = 128  # \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\n\n    dropout_keep_prob = 0.5  # dropout\xe4\xbf\x9d\xe7\x95\x99\xe6\xaf\x94\xe4\xbe\x8b\n    learning_rate = 1e-3  # \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n\n    batch_size = 64  # \xe6\xaf\x8f\xe6\x89\xb9\xe8\xae\xad\xe7\xbb\x83\xe5\xa4\xa7\xe5\xb0\x8f\n    num_epochs = 10  # \xe6\x80\xbb\xe8\xbf\xad\xe4\xbb\xa3\xe8\xbd\xae\xe6\xac\xa1\n\n    print_per_batch = 100  # \xe6\xaf\x8f\xe5\xa4\x9a\xe5\xb0\x91\xe8\xbd\xae\xe8\xbe\x93\xe5\x87\xba\xe4\xb8\x80\xe6\xac\xa1\xe7\xbb\x93\xe6\x9e\x9c\n    save_per_batch = 10  # \xe6\xaf\x8f\xe5\xa4\x9a\xe5\xb0\x91\xe8\xbd\xae\xe5\xad\x98\xe5\x85\xa5tensorboard\n\n\nclass TextCNN(object):\n    """"""\xe6\x96\x87\xe6\x9c\xac\xe5\x88\x86\xe7\xb1\xbb\xef\xbc\x8cCNN\xe6\xa8\xa1\xe5\x9e\x8b""""""\n\n    def __init__(self, config):\n        self.config = config\n\n        # \xe4\xb8\x89\xe4\xb8\xaa\xe5\xbe\x85\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n        self.input_x = tf.placeholder(tf.int32, [None, self.config.seq_length], name=\'input_x\')\n        self.input_y = tf.placeholder(tf.float32, [None, self.config.num_classes], name=\'input_y\')\n        self.keep_prob = tf.placeholder(tf.float32, name=\'keep_prob\')\n\n        self.cnn()\n\n    def cnn(self):\n        """"""CNN\xe6\xa8\xa1\xe5\x9e\x8b""""""\n        # \xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe6\x98\xa0\xe5\xb0\x84\n        with tf.device(\'/cpu:0\'):\n            embedding = tf.get_variable(\'embedding\', [self.config.vocab_size, self.config.embedding_dim])\n            embedding_inputs = tf.nn.embedding_lookup(embedding, self.input_x)\n\n        with tf.name_scope(""cnn""):\n            # CNN layer\n            conv = tf.layers.conv1d(embedding_inputs, self.config.num_filters, self.config.kernel_size, name=\'conv\')\n            # global max pooling layer\n            gmp = tf.reduce_max(conv, reduction_indices=[1], name=\'gmp\')\n\n        with tf.name_scope(""score""):\n            # \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xef\xbc\x8c\xe5\x90\x8e\xe9\x9d\xa2\xe6\x8e\xa5dropout\xe4\xbb\xa5\xe5\x8f\x8arelu\xe6\xbf\x80\xe6\xb4\xbb\n            fc = tf.layers.dense(gmp, self.config.hidden_dim, name=\'fc1\')\n            fc = tf.contrib.layers.dropout(fc, self.keep_prob)\n            fc = tf.nn.relu(fc)\n\n            # \xe5\x88\x86\xe7\xb1\xbb\xe5\x99\xa8\n            self.logits = tf.layers.dense(fc, self.config.num_classes, name=\'fc2\')\n            self.y_pred_cls = tf.argmax(tf.nn.softmax(self.logits), 1)  # \xe9\xa2\x84\xe6\xb5\x8b\xe7\xb1\xbb\xe5\x88\xab\n\n        with tf.name_scope(""optimize""):\n            # \xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\n            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.input_y)\n            self.loss = tf.reduce_mean(cross_entropy)\n            # \xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\n            self.optim = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(self.loss)\n\n        with tf.name_scope(""accuracy""):\n            # \xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\n            correct_pred = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_cls)\n            self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n'"
predict.py,3,"b""# coding: utf-8\n\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\nimport tensorflow.contrib.keras as kr\n\nfrom cnn_model import TCNNConfig, TextCNN\nfrom data.cnews_loader import read_category, read_vocab\n\ntry:\n    bool(type(unicode))\nexcept NameError:\n    unicode = str\n\nbase_dir = 'data/cnews'\nvocab_dir = os.path.join(base_dir, 'cnews.vocab.txt')\n\nsave_dir = 'checkpoints/textcnn'\nsave_path = os.path.join(save_dir, 'best_validation')  # \xe6\x9c\x80\xe4\xbd\xb3\xe9\xaa\x8c\xe8\xaf\x81\xe7\xbb\x93\xe6\x9e\x9c\xe4\xbf\x9d\xe5\xad\x98\xe8\xb7\xaf\xe5\xbe\x84\n\n\nclass CnnModel:\n    def __init__(self):\n        self.config = TCNNConfig()\n        self.categories, self.cat_to_id = read_category()\n        self.words, self.word_to_id = read_vocab(vocab_dir)\n        self.config.vocab_size = len(self.words)\n        self.model = TextCNN(self.config)\n\n        self.session = tf.Session()\n        self.session.run(tf.global_variables_initializer())\n        saver = tf.train.Saver()\n        saver.restore(sess=self.session, save_path=save_path)  # \xe8\xaf\xbb\xe5\x8f\x96\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n\n    def predict(self, message):\n        # \xe6\x94\xaf\xe6\x8c\x81\xe4\xb8\x8d\xe8\xae\xba\xe5\x9c\xa8python2\xe8\xbf\x98\xe6\x98\xafpython3\xe4\xb8\x8b\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe9\x83\xbd\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x9c\xa82\xe6\x88\x96\xe8\x80\x853\xe7\x9a\x84\xe7\x8e\xaf\xe5\xa2\x83\xe4\xb8\x8b\xe8\xbf\x90\xe8\xa1\x8c\n        content = unicode(message)\n        data = [self.word_to_id[x] for x in content if x in self.word_to_id]\n\n        feed_dict = {\n            self.model.input_x: kr.preprocessing.sequence.pad_sequences([data], self.config.seq_length),\n            self.model.keep_prob: 1.0\n        }\n\n        y_pred_cls = self.session.run(self.model.y_pred_cls, feed_dict=feed_dict)\n        return self.categories[y_pred_cls[0]]\n\n\nif __name__ == '__main__':\n    cnn_model = CnnModel()\n    test_demo = ['\xe4\xb8\x89\xe6\x98\x9fST550\xe4\xbb\xa5\xe5\x85\xa8\xe6\x96\xb0\xe7\x9a\x84\xe6\x8b\x8d\xe6\x91\x84\xe6\x96\xb9\xe5\xbc\x8f\xe8\xb6\x85\xe8\xb6\x8a\xe4\xba\x86\xe4\xbb\xa5\xe5\xbe\x80\xe4\xbb\xbb\xe4\xbd\x95\xe4\xb8\x80\xe6\xac\xbe\xe6\x95\xb0\xe7\xa0\x81\xe7\x9b\xb8\xe6\x9c\xba',\n                 '\xe7\x83\xad\xe7\x81\xabvs\xe9\xaa\x91\xe5\xa3\xab\xe5\x89\x8d\xe7\x9e\xbb\xef\xbc\x9a\xe7\x9a\x87\xe5\xb8\x9d\xe5\x9b\x9e\xe4\xb9\xa1\xe4\xba\x8c\xe7\x95\xaa\xe6\x88\x98 \xe4\xb8\x9c\xe9\x83\xa8\xe6\xac\xa1\xe5\xb8\xad\xe5\x94\xbe\xe6\x89\x8b\xe5\x8f\xaf\xe5\xbe\x97\xe6\x96\xb0\xe6\xb5\xaa\xe4\xbd\x93\xe8\x82\xb2\xe8\xae\xaf\xe5\x8c\x97\xe4\xba\xac\xe6\x97\xb6\xe9\x97\xb43\xe6\x9c\x8830\xe6\x97\xa57:00']\n    for i in test_demo:\n        print(cnn_model.predict(i))\n"""
rnn_model.py,25,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nclass TRNNConfig(object):\n    """"""RNN\xe9\x85\x8d\xe7\xbd\xae\xe5\x8f\x82\xe6\x95\xb0""""""\n\n    # \xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\n    embedding_dim = 64      # \xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe7\xbb\xb4\xe5\xba\xa6\n    seq_length = 600        # \xe5\xba\x8f\xe5\x88\x97\xe9\x95\xbf\xe5\xba\xa6\n    num_classes = 10        # \xe7\xb1\xbb\xe5\x88\xab\xe6\x95\xb0\n    vocab_size = 5000       # \xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xe8\xbe\xbe\xe5\xb0\x8f\n\n    num_layers= 2           # \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe5\xb1\x82\xe6\x95\xb0\n    hidden_dim = 128        # \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\n    rnn = \'gru\'             # lstm \xe6\x88\x96 gru\n\n    dropout_keep_prob = 0.8 # dropout\xe4\xbf\x9d\xe7\x95\x99\xe6\xaf\x94\xe4\xbe\x8b\n    learning_rate = 1e-3    # \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n\n    batch_size = 128         # \xe6\xaf\x8f\xe6\x89\xb9\xe8\xae\xad\xe7\xbb\x83\xe5\xa4\xa7\xe5\xb0\x8f\n    num_epochs = 10          # \xe6\x80\xbb\xe8\xbf\xad\xe4\xbb\xa3\xe8\xbd\xae\xe6\xac\xa1\n\n    print_per_batch = 100    # \xe6\xaf\x8f\xe5\xa4\x9a\xe5\xb0\x91\xe8\xbd\xae\xe8\xbe\x93\xe5\x87\xba\xe4\xb8\x80\xe6\xac\xa1\xe7\xbb\x93\xe6\x9e\x9c\n    save_per_batch = 10      # \xe6\xaf\x8f\xe5\xa4\x9a\xe5\xb0\x91\xe8\xbd\xae\xe5\xad\x98\xe5\x85\xa5tensorboard\n\n\nclass TextRNN(object):\n    """"""\xe6\x96\x87\xe6\x9c\xac\xe5\x88\x86\xe7\xb1\xbb\xef\xbc\x8cRNN\xe6\xa8\xa1\xe5\x9e\x8b""""""\n    def __init__(self, config):\n        self.config = config\n\n        # \xe4\xb8\x89\xe4\xb8\xaa\xe5\xbe\x85\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n        self.input_x = tf.placeholder(tf.int32, [None, self.config.seq_length], name=\'input_x\')\n        self.input_y = tf.placeholder(tf.float32, [None, self.config.num_classes], name=\'input_y\')\n        self.keep_prob = tf.placeholder(tf.float32, name=\'keep_prob\')\n\n        self.rnn()\n\n    def rnn(self):\n        """"""rnn\xe6\xa8\xa1\xe5\x9e\x8b""""""\n\n        def lstm_cell():   # lstm\xe6\xa0\xb8\n            return tf.contrib.rnn.BasicLSTMCell(self.config.hidden_dim, state_is_tuple=True)\n\n        def gru_cell():  # gru\xe6\xa0\xb8\n            return tf.contrib.rnn.GRUCell(self.config.hidden_dim)\n\n        def dropout(): # \xe4\xb8\xba\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaarnn\xe6\xa0\xb8\xe5\x90\x8e\xe9\x9d\xa2\xe5\x8a\xa0\xe4\xb8\x80\xe4\xb8\xaadropout\xe5\xb1\x82\n            if (self.config.rnn == \'lstm\'):\n                cell = lstm_cell()\n            else:\n                cell = gru_cell()\n            return tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=self.keep_prob)\n\n        # \xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe6\x98\xa0\xe5\xb0\x84\n        with tf.device(\'/cpu:0\'):\n            embedding = tf.get_variable(\'embedding\', [self.config.vocab_size, self.config.embedding_dim])\n            embedding_inputs = tf.nn.embedding_lookup(embedding, self.input_x)\n\n        with tf.name_scope(""rnn""):\n            # \xe5\xa4\x9a\xe5\xb1\x82rnn\xe7\xbd\x91\xe7\xbb\x9c\n            cells = [dropout() for _ in range(self.config.num_layers)]\n            rnn_cell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n\n            _outputs, _ = tf.nn.dynamic_rnn(cell=rnn_cell, inputs=embedding_inputs, dtype=tf.float32)\n            last = _outputs[:, -1, :]  # \xe5\x8f\x96\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe6\x97\xb6\xe5\xba\x8f\xe8\xbe\x93\xe5\x87\xba\xe4\xbd\x9c\xe4\xb8\xba\xe7\xbb\x93\xe6\x9e\x9c\n\n        with tf.name_scope(""score""):\n            # \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xef\xbc\x8c\xe5\x90\x8e\xe9\x9d\xa2\xe6\x8e\xa5dropout\xe4\xbb\xa5\xe5\x8f\x8arelu\xe6\xbf\x80\xe6\xb4\xbb\n            fc = tf.layers.dense(last, self.config.hidden_dim, name=\'fc1\')\n            fc = tf.contrib.layers.dropout(fc, self.keep_prob)\n            fc = tf.nn.relu(fc)\n\n            # \xe5\x88\x86\xe7\xb1\xbb\xe5\x99\xa8\n            self.logits = tf.layers.dense(fc, self.config.num_classes, name=\'fc2\')\n            self.y_pred_cls = tf.argmax(tf.nn.softmax(self.logits), 1)  # \xe9\xa2\x84\xe6\xb5\x8b\xe7\xb1\xbb\xe5\x88\xab\n\n        with tf.name_scope(""optimize""):\n            # \xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\n            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.input_y)\n            self.loss = tf.reduce_mean(cross_entropy)\n            # \xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\n            self.optim = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(self.loss)\n\n        with tf.name_scope(""accuracy""):\n            # \xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\n            correct_pred = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_cls)\n            self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n'"
run_cnn.py,10,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport time\nfrom datetime import timedelta\n\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn import metrics\n\nfrom cnn_model import TCNNConfig, TextCNN\nfrom data.cnews_loader import read_vocab, read_category, batch_iter, process_file, build_vocab\n\nbase_dir = \'data/cnews\'\ntrain_dir = os.path.join(base_dir, \'cnews.train.txt\')\ntest_dir = os.path.join(base_dir, \'cnews.test.txt\')\nval_dir = os.path.join(base_dir, \'cnews.val.txt\')\nvocab_dir = os.path.join(base_dir, \'cnews.vocab.txt\')\n\nsave_dir = \'checkpoints/textcnn\'\nsave_path = os.path.join(save_dir, \'best_validation\')  # \xe6\x9c\x80\xe4\xbd\xb3\xe9\xaa\x8c\xe8\xaf\x81\xe7\xbb\x93\xe6\x9e\x9c\xe4\xbf\x9d\xe5\xad\x98\xe8\xb7\xaf\xe5\xbe\x84\n\n\ndef get_time_dif(start_time):\n    """"""\xe8\x8e\xb7\xe5\x8f\x96\xe5\xb7\xb2\xe4\xbd\xbf\xe7\x94\xa8\xe6\x97\xb6\xe9\x97\xb4""""""\n    end_time = time.time()\n    time_dif = end_time - start_time\n    return timedelta(seconds=int(round(time_dif)))\n\n\ndef feed_data(x_batch, y_batch, keep_prob):\n    feed_dict = {\n        model.input_x: x_batch,\n        model.input_y: y_batch,\n        model.keep_prob: keep_prob\n    }\n    return feed_dict\n\n\ndef evaluate(sess, x_, y_):\n    """"""\xe8\xaf\x84\xe4\xbc\xb0\xe5\x9c\xa8\xe6\x9f\x90\xe4\xb8\x80\xe6\x95\xb0\xe6\x8d\xae\xe4\xb8\x8a\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe5\x92\x8c\xe6\x8d\x9f\xe5\xa4\xb1""""""\n    data_len = len(x_)\n    batch_eval = batch_iter(x_, y_, 128)\n    total_loss = 0.0\n    total_acc = 0.0\n    for x_batch, y_batch in batch_eval:\n        batch_len = len(x_batch)\n        feed_dict = feed_data(x_batch, y_batch, 1.0)\n        loss, acc = sess.run([model.loss, model.acc], feed_dict=feed_dict)\n        total_loss += loss * batch_len\n        total_acc += acc * batch_len\n\n    return total_loss / data_len, total_acc / data_len\n\n\ndef train():\n    print(""Configuring TensorBoard and Saver..."")\n    # \xe9\x85\x8d\xe7\xbd\xae Tensorboard\xef\xbc\x8c\xe9\x87\x8d\xe6\x96\xb0\xe8\xae\xad\xe7\xbb\x83\xe6\x97\xb6\xef\xbc\x8c\xe8\xaf\xb7\xe5\xb0\x86tensorboard\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xe5\x88\xa0\xe9\x99\xa4\xef\xbc\x8c\xe4\xb8\x8d\xe7\x84\xb6\xe5\x9b\xbe\xe4\xbc\x9a\xe8\xa6\x86\xe7\x9b\x96\n    tensorboard_dir = \'tensorboard/textcnn\'\n    if not os.path.exists(tensorboard_dir):\n        os.makedirs(tensorboard_dir)\n\n    tf.summary.scalar(""loss"", model.loss)\n    tf.summary.scalar(""accuracy"", model.acc)\n    merged_summary = tf.summary.merge_all()\n    writer = tf.summary.FileWriter(tensorboard_dir)\n\n    # \xe9\x85\x8d\xe7\xbd\xae Saver\n    saver = tf.train.Saver()\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    print(""Loading training and validation data..."")\n    # \xe8\xbd\xbd\xe5\x85\xa5\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe4\xb8\x8e\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\n    start_time = time.time()\n    x_train, y_train = process_file(train_dir, word_to_id, cat_to_id, config.seq_length)\n    x_val, y_val = process_file(val_dir, word_to_id, cat_to_id, config.seq_length)\n    time_dif = get_time_dif(start_time)\n    print(""Time usage:"", time_dif)\n\n    # \xe5\x88\x9b\xe5\xbb\xbasession\n    session = tf.Session()\n    session.run(tf.global_variables_initializer())\n    writer.add_graph(session.graph)\n\n    print(\'Training and evaluating...\')\n    start_time = time.time()\n    total_batch = 0  # \xe6\x80\xbb\xe6\x89\xb9\xe6\xac\xa1\n    best_acc_val = 0.0  # \xe6\x9c\x80\xe4\xbd\xb3\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\n    last_improved = 0  # \xe8\xae\xb0\xe5\xbd\x95\xe4\xb8\x8a\xe4\xb8\x80\xe6\xac\xa1\xe6\x8f\x90\xe5\x8d\x87\xe6\x89\xb9\xe6\xac\xa1\n    require_improvement = 1000  # \xe5\xa6\x82\xe6\x9e\x9c\xe8\xb6\x85\xe8\xbf\x871000\xe8\xbd\xae\xe6\x9c\xaa\xe6\x8f\x90\xe5\x8d\x87\xef\xbc\x8c\xe6\x8f\x90\xe5\x89\x8d\xe7\xbb\x93\xe6\x9d\x9f\xe8\xae\xad\xe7\xbb\x83\n\n    flag = False\n    for epoch in range(config.num_epochs):\n        print(\'Epoch:\', epoch + 1)\n        batch_train = batch_iter(x_train, y_train, config.batch_size)\n        for x_batch, y_batch in batch_train:\n            feed_dict = feed_data(x_batch, y_batch, config.dropout_keep_prob)\n\n            if total_batch % config.save_per_batch == 0:\n                # \xe6\xaf\x8f\xe5\xa4\x9a\xe5\xb0\x91\xe8\xbd\xae\xe6\xac\xa1\xe5\xb0\x86\xe8\xae\xad\xe7\xbb\x83\xe7\xbb\x93\xe6\x9e\x9c\xe5\x86\x99\xe5\x85\xa5tensorboard scalar\n                s = session.run(merged_summary, feed_dict=feed_dict)\n                writer.add_summary(s, total_batch)\n\n            if total_batch % config.print_per_batch == 0:\n                # \xe6\xaf\x8f\xe5\xa4\x9a\xe5\xb0\x91\xe8\xbd\xae\xe6\xac\xa1\xe8\xbe\x93\xe5\x87\xba\xe5\x9c\xa8\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe5\x92\x8c\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\x8a\xe7\x9a\x84\xe6\x80\xa7\xe8\x83\xbd\n                feed_dict[model.keep_prob] = 1.0\n                loss_train, acc_train = session.run([model.loss, model.acc], feed_dict=feed_dict)\n                loss_val, acc_val = evaluate(session, x_val, y_val)  # todo\n\n                if acc_val > best_acc_val:\n                    # \xe4\xbf\x9d\xe5\xad\x98\xe6\x9c\x80\xe5\xa5\xbd\xe7\xbb\x93\xe6\x9e\x9c\n                    best_acc_val = acc_val\n                    last_improved = total_batch\n                    saver.save(sess=session, save_path=save_path)\n                    improved_str = \'*\'\n                else:\n                    improved_str = \'\'\n\n                time_dif = get_time_dif(start_time)\n                msg = \'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%},\' \\\n                      + \' Val Loss: {3:>6.2}, Val Acc: {4:>7.2%}, Time: {5} {6}\'\n                print(msg.format(total_batch, loss_train, acc_train, loss_val, acc_val, time_dif, improved_str))\n\n            feed_dict[model.keep_prob] = config.dropout_keep_prob\n            session.run(model.optim, feed_dict=feed_dict)  # \xe8\xbf\x90\xe8\xa1\x8c\xe4\xbc\x98\xe5\x8c\x96\n            total_batch += 1\n\n            if total_batch - last_improved > require_improvement:\n                # \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe6\xad\xa3\xe7\xa1\xae\xe7\x8e\x87\xe9\x95\xbf\xe6\x9c\x9f\xe4\xb8\x8d\xe6\x8f\x90\xe5\x8d\x87\xef\xbc\x8c\xe6\x8f\x90\xe5\x89\x8d\xe7\xbb\x93\xe6\x9d\x9f\xe8\xae\xad\xe7\xbb\x83\n                print(""No optimization for a long time, auto-stopping..."")\n                flag = True\n                break  # \xe8\xb7\xb3\xe5\x87\xba\xe5\xbe\xaa\xe7\x8e\xaf\n        if flag:  # \xe5\x90\x8c\xe4\xb8\x8a\n            break\n\n\ndef test():\n    print(""Loading test data..."")\n    start_time = time.time()\n    x_test, y_test = process_file(test_dir, word_to_id, cat_to_id, config.seq_length)\n\n    session = tf.Session()\n    session.run(tf.global_variables_initializer())\n    saver = tf.train.Saver()\n    saver.restore(sess=session, save_path=save_path)  # \xe8\xaf\xbb\xe5\x8f\x96\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n\n    print(\'Testing...\')\n    loss_test, acc_test = evaluate(session, x_test, y_test)\n    msg = \'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}\'\n    print(msg.format(loss_test, acc_test))\n\n    batch_size = 128\n    data_len = len(x_test)\n    num_batch = int((data_len - 1) / batch_size) + 1\n\n    y_test_cls = np.argmax(y_test, 1)\n    y_pred_cls = np.zeros(shape=len(x_test), dtype=np.int32)  # \xe4\xbf\x9d\xe5\xad\x98\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\n    for i in range(num_batch):  # \xe9\x80\x90\xe6\x89\xb9\xe6\xac\xa1\xe5\xa4\x84\xe7\x90\x86\n        start_id = i * batch_size\n        end_id = min((i + 1) * batch_size, data_len)\n        feed_dict = {\n            model.input_x: x_test[start_id:end_id],\n            model.keep_prob: 1.0\n        }\n        y_pred_cls[start_id:end_id] = session.run(model.y_pred_cls, feed_dict=feed_dict)\n\n    # \xe8\xaf\x84\xe4\xbc\xb0\n    print(""Precision, Recall and F1-Score..."")\n    print(metrics.classification_report(y_test_cls, y_pred_cls, target_names=categories))\n\n    # \xe6\xb7\xb7\xe6\xb7\x86\xe7\x9f\xa9\xe9\x98\xb5\n    print(""Confusion Matrix..."")\n    cm = metrics.confusion_matrix(y_test_cls, y_pred_cls)\n    print(cm)\n\n    time_dif = get_time_dif(start_time)\n    print(""Time usage:"", time_dif)\n\n\nif __name__ == \'__main__\':\n    if len(sys.argv) != 2 or sys.argv[1] not in [\'train\', \'test\']:\n        raise ValueError(""""""usage: python run_cnn.py [train / test]"""""")\n\n    print(\'Configuring CNN model...\')\n    config = TCNNConfig()\n    if not os.path.exists(vocab_dir):  # \xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe5\xad\x98\xe5\x9c\xa8\xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xef\xbc\x8c\xe9\x87\x8d\xe5\xbb\xba\n        build_vocab(train_dir, vocab_dir, config.vocab_size)\n    categories, cat_to_id = read_category()\n    words, word_to_id = read_vocab(vocab_dir)\n    config.vocab_size = len(words)\n    model = TextCNN(config)\n\n    if sys.argv[1] == \'train\':\n        train()\n    else:\n        test()\n'"
run_rnn.py,10,"b'# coding: utf-8\n\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport time\nfrom datetime import timedelta\n\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn import metrics\n\nfrom rnn_model import TRNNConfig, TextRNN\nfrom data.cnews_loader import read_vocab, read_category, batch_iter, process_file, build_vocab\n\nbase_dir = \'data/cnews\'\ntrain_dir = os.path.join(base_dir, \'cnews.train.txt\')\ntest_dir = os.path.join(base_dir, \'cnews.test.txt\')\nval_dir = os.path.join(base_dir, \'cnews.val.txt\')\nvocab_dir = os.path.join(base_dir, \'cnews.vocab.txt\')\n\nsave_dir = \'checkpoints/textrnn\'\nsave_path = os.path.join(save_dir, \'best_validation\')  # \xe6\x9c\x80\xe4\xbd\xb3\xe9\xaa\x8c\xe8\xaf\x81\xe7\xbb\x93\xe6\x9e\x9c\xe4\xbf\x9d\xe5\xad\x98\xe8\xb7\xaf\xe5\xbe\x84\n\n\ndef get_time_dif(start_time):\n    """"""\xe8\x8e\xb7\xe5\x8f\x96\xe5\xb7\xb2\xe4\xbd\xbf\xe7\x94\xa8\xe6\x97\xb6\xe9\x97\xb4""""""\n    end_time = time.time()\n    time_dif = end_time - start_time\n    return timedelta(seconds=int(round(time_dif)))\n\n\ndef feed_data(x_batch, y_batch, keep_prob):\n    feed_dict = {\n        model.input_x: x_batch,\n        model.input_y: y_batch,\n        model.keep_prob: keep_prob\n    }\n    return feed_dict\n\n\ndef evaluate(sess, x_, y_):\n    """"""\xe8\xaf\x84\xe4\xbc\xb0\xe5\x9c\xa8\xe6\x9f\x90\xe4\xb8\x80\xe6\x95\xb0\xe6\x8d\xae\xe4\xb8\x8a\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe5\x92\x8c\xe6\x8d\x9f\xe5\xa4\xb1""""""\n    data_len = len(x_)\n    batch_eval = batch_iter(x_, y_, 128)\n    total_loss = 0.0\n    total_acc = 0.0\n    for x_batch, y_batch in batch_eval:\n        batch_len = len(x_batch)\n        feed_dict = feed_data(x_batch, y_batch, 1.0)\n        loss, acc = sess.run([model.loss, model.acc], feed_dict=feed_dict)\n        total_loss += loss * batch_len\n        total_acc += acc * batch_len\n\n    return total_loss / data_len, total_acc / data_len\n\n\ndef train():\n    print(""Configuring TensorBoard and Saver..."")\n    # \xe9\x85\x8d\xe7\xbd\xae Tensorboard\xef\xbc\x8c\xe9\x87\x8d\xe6\x96\xb0\xe8\xae\xad\xe7\xbb\x83\xe6\x97\xb6\xef\xbc\x8c\xe8\xaf\xb7\xe5\xb0\x86tensorboard\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xe5\x88\xa0\xe9\x99\xa4\xef\xbc\x8c\xe4\xb8\x8d\xe7\x84\xb6\xe5\x9b\xbe\xe4\xbc\x9a\xe8\xa6\x86\xe7\x9b\x96\n    tensorboard_dir = \'tensorboard/textrnn\'\n    if not os.path.exists(tensorboard_dir):\n        os.makedirs(tensorboard_dir)\n\n    tf.summary.scalar(""loss"", model.loss)\n    tf.summary.scalar(""accuracy"", model.acc)\n    merged_summary = tf.summary.merge_all()\n    writer = tf.summary.FileWriter(tensorboard_dir)\n\n    # \xe9\x85\x8d\xe7\xbd\xae Saver\n    saver = tf.train.Saver()\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    print(""Loading training and validation data..."")\n    # \xe8\xbd\xbd\xe5\x85\xa5\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe4\xb8\x8e\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\n    start_time = time.time()\n    x_train, y_train = process_file(train_dir, word_to_id, cat_to_id, config.seq_length)\n    x_val, y_val = process_file(val_dir, word_to_id, cat_to_id, config.seq_length)\n    time_dif = get_time_dif(start_time)\n    print(""Time usage:"", time_dif)\n\n    # \xe5\x88\x9b\xe5\xbb\xbasession\n    session = tf.Session()\n    session.run(tf.global_variables_initializer())\n    writer.add_graph(session.graph)\n\n    print(\'Training and evaluating...\')\n    start_time = time.time()\n    total_batch = 0  # \xe6\x80\xbb\xe6\x89\xb9\xe6\xac\xa1\n    best_acc_val = 0.0  # \xe6\x9c\x80\xe4\xbd\xb3\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\n    last_improved = 0  # \xe8\xae\xb0\xe5\xbd\x95\xe4\xb8\x8a\xe4\xb8\x80\xe6\xac\xa1\xe6\x8f\x90\xe5\x8d\x87\xe6\x89\xb9\xe6\xac\xa1\n    require_improvement = 1000  # \xe5\xa6\x82\xe6\x9e\x9c\xe8\xb6\x85\xe8\xbf\x871000\xe8\xbd\xae\xe6\x9c\xaa\xe6\x8f\x90\xe5\x8d\x87\xef\xbc\x8c\xe6\x8f\x90\xe5\x89\x8d\xe7\xbb\x93\xe6\x9d\x9f\xe8\xae\xad\xe7\xbb\x83\n\n    flag = False\n    for epoch in range(config.num_epochs):\n        print(\'Epoch:\', epoch + 1)\n        batch_train = batch_iter(x_train, y_train, config.batch_size)\n        for x_batch, y_batch in batch_train:\n            feed_dict = feed_data(x_batch, y_batch, config.dropout_keep_prob)\n\n            if total_batch % config.save_per_batch == 0:\n                # \xe6\xaf\x8f\xe5\xa4\x9a\xe5\xb0\x91\xe8\xbd\xae\xe6\xac\xa1\xe5\xb0\x86\xe8\xae\xad\xe7\xbb\x83\xe7\xbb\x93\xe6\x9e\x9c\xe5\x86\x99\xe5\x85\xa5tensorboard scalar\n                s = session.run(merged_summary, feed_dict=feed_dict)\n                writer.add_summary(s, total_batch)\n\n            if total_batch % config.print_per_batch == 0:\n                # \xe6\xaf\x8f\xe5\xa4\x9a\xe5\xb0\x91\xe8\xbd\xae\xe6\xac\xa1\xe8\xbe\x93\xe5\x87\xba\xe5\x9c\xa8\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe5\x92\x8c\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\x8a\xe7\x9a\x84\xe6\x80\xa7\xe8\x83\xbd\n                feed_dict[model.keep_prob] = 1.0\n                loss_train, acc_train = session.run([model.loss, model.acc], feed_dict=feed_dict)\n                loss_val, acc_val = evaluate(session, x_val, y_val)  # todo\n\n                if acc_val > best_acc_val:\n                    # \xe4\xbf\x9d\xe5\xad\x98\xe6\x9c\x80\xe5\xa5\xbd\xe7\xbb\x93\xe6\x9e\x9c\n                    best_acc_val = acc_val\n                    last_improved = total_batch\n                    saver.save(sess=session, save_path=save_path)\n                    improved_str = \'*\'\n                else:\n                    improved_str = \'\'\n\n                time_dif = get_time_dif(start_time)\n                msg = \'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%},\' \\\n                      + \' Val Loss: {3:>6.2}, Val Acc: {4:>7.2%}, Time: {5} {6}\'\n                print(msg.format(total_batch, loss_train, acc_train, loss_val, acc_val, time_dif, improved_str))\n            \n            feed_dict[model.keep_prob] = config.dropout_keep_prob\n            session.run(model.optim, feed_dict=feed_dict)  # \xe8\xbf\x90\xe8\xa1\x8c\xe4\xbc\x98\xe5\x8c\x96\n            total_batch += 1\n\n            if total_batch - last_improved > require_improvement:\n                # \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe6\xad\xa3\xe7\xa1\xae\xe7\x8e\x87\xe9\x95\xbf\xe6\x9c\x9f\xe4\xb8\x8d\xe6\x8f\x90\xe5\x8d\x87\xef\xbc\x8c\xe6\x8f\x90\xe5\x89\x8d\xe7\xbb\x93\xe6\x9d\x9f\xe8\xae\xad\xe7\xbb\x83\n                print(""No optimization for a long time, auto-stopping..."")\n                flag = True\n                break  # \xe8\xb7\xb3\xe5\x87\xba\xe5\xbe\xaa\xe7\x8e\xaf\n        if flag:  # \xe5\x90\x8c\xe4\xb8\x8a\n            break\n\n\ndef test():\n    print(""Loading test data..."")\n    start_time = time.time()\n    x_test, y_test = process_file(test_dir, word_to_id, cat_to_id, config.seq_length)\n\n    session = tf.Session()\n    session.run(tf.global_variables_initializer())\n    saver = tf.train.Saver()\n    saver.restore(sess=session, save_path=save_path)  # \xe8\xaf\xbb\xe5\x8f\x96\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n\n    print(\'Testing...\')\n    loss_test, acc_test = evaluate(session, x_test, y_test)\n    msg = \'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}\'\n    print(msg.format(loss_test, acc_test))\n\n    batch_size = 128\n    data_len = len(x_test)\n    num_batch = int((data_len - 1) / batch_size) + 1\n\n    y_test_cls = np.argmax(y_test, 1)\n    y_pred_cls = np.zeros(shape=len(x_test), dtype=np.int32)  # \xe4\xbf\x9d\xe5\xad\x98\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\n    for i in range(num_batch):  # \xe9\x80\x90\xe6\x89\xb9\xe6\xac\xa1\xe5\xa4\x84\xe7\x90\x86\n        start_id = i * batch_size\n        end_id = min((i + 1) * batch_size, data_len)\n        feed_dict = {\n            model.input_x: x_test[start_id:end_id],\n            model.keep_prob: 1.0\n        }\n        y_pred_cls[start_id:end_id] = session.run(model.y_pred_cls, feed_dict=feed_dict)\n\n    # \xe8\xaf\x84\xe4\xbc\xb0\n    print(""Precision, Recall and F1-Score..."")\n    print(metrics.classification_report(y_test_cls, y_pred_cls, target_names=categories))\n\n    # \xe6\xb7\xb7\xe6\xb7\x86\xe7\x9f\xa9\xe9\x98\xb5\n    print(""Confusion Matrix..."")\n    cm = metrics.confusion_matrix(y_test_cls, y_pred_cls)\n    print(cm)\n\n    time_dif = get_time_dif(start_time)\n    print(""Time usage:"", time_dif)\n\n\nif __name__ == \'__main__\':\n    if len(sys.argv) != 2 or sys.argv[1] not in [\'train\', \'test\']:\n        raise ValueError(""""""usage: python run_rnn.py [train / test]"""""")\n\n    print(\'Configuring RNN model...\')\n    config = TRNNConfig()\n    if not os.path.exists(vocab_dir):  # \xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe5\xad\x98\xe5\x9c\xa8\xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xef\xbc\x8c\xe9\x87\x8d\xe5\xbb\xba\n        build_vocab(train_dir, vocab_dir, config.vocab_size)\n    categories, cat_to_id = read_category()\n    words, word_to_id = read_vocab(vocab_dir)\n    config.vocab_size = len(words)\n    model = TextRNN(config)\n\n    if sys.argv[1] == \'train\':\n        train()\n    else:\n        test()\n'"
data/__init__.py,0,b''
data/cnews_loader.py,0,"b'# coding: utf-8\n\nimport sys\nfrom collections import Counter\n\nimport numpy as np\nimport tensorflow.keras as kr\n\nif sys.version_info[0] > 2:\n    is_py3 = True\nelse:\n    reload(sys)\n    sys.setdefaultencoding(""utf-8"")\n    is_py3 = False\n\n\ndef native_word(word, encoding=\'utf-8\'):\n    """"""\xe5\xa6\x82\xe6\x9e\x9c\xe5\x9c\xa8python2\xe4\xb8\x8b\xe9\x9d\xa2\xe4\xbd\xbf\xe7\x94\xa8python3\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\x8f\xaf\xe8\x80\x83\xe8\x99\x91\xe8\xb0\x83\xe7\x94\xa8\xe6\xad\xa4\xe5\x87\xbd\xe6\x95\xb0\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\x80\xe4\xb8\x8b\xe5\xad\x97\xe7\xac\xa6\xe7\xbc\x96\xe7\xa0\x81""""""\n    if not is_py3:\n        return word.encode(encoding)\n    else:\n        return word\n\n\ndef native_content(content):\n    if not is_py3:\n        return content.decode(\'utf-8\')\n    else:\n        return content\n\n\ndef open_file(filename, mode=\'r\'):\n    """"""\n    \xe5\xb8\xb8\xe7\x94\xa8\xe6\x96\x87\xe4\xbb\xb6\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe5\x8f\xaf\xe5\x9c\xa8python2\xe5\x92\x8cpython3\xe9\x97\xb4\xe5\x88\x87\xe6\x8d\xa2.\n    mode: \'r\' or \'w\' for read or write\n    """"""\n    if is_py3:\n        return open(filename, mode, encoding=\'utf-8\', errors=\'ignore\')\n    else:\n        return open(filename, mode)\n\n\ndef read_file(filename):\n    """"""\xe8\xaf\xbb\xe5\x8f\x96\xe6\x96\x87\xe4\xbb\xb6\xe6\x95\xb0\xe6\x8d\xae""""""\n    contents, labels = [], []\n    with open_file(filename) as f:\n        for line in f:\n            try:\n                label, content = line.strip().split(\'\\t\')\n                if content:\n                    contents.append(list(native_content(content)))\n                    labels.append(native_content(label))\n            except:\n                pass\n    return contents, labels\n\n\ndef build_vocab(train_dir, vocab_dir, vocab_size=5000):\n    """"""\xe6\xa0\xb9\xe6\x8d\xae\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe6\x9e\x84\xe5\xbb\xba\xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xef\xbc\x8c\xe5\xad\x98\xe5\x82\xa8""""""\n    data_train, _ = read_file(train_dir)\n\n    all_data = []\n    for content in data_train:\n        all_data.extend(content)\n\n    counter = Counter(all_data)\n    count_pairs = counter.most_common(vocab_size - 1)\n    words, _ = list(zip(*count_pairs))\n    # \xe6\xb7\xbb\xe5\x8a\xa0\xe4\xb8\x80\xe4\xb8\xaa <PAD> \xe6\x9d\xa5\xe5\xb0\x86\xe6\x89\x80\xe6\x9c\x89\xe6\x96\x87\xe6\x9c\xacpad\xe4\xb8\xba\xe5\x90\x8c\xe4\xb8\x80\xe9\x95\xbf\xe5\xba\xa6\n    words = [\'<PAD>\'] + list(words)\n    open_file(vocab_dir, mode=\'w\').write(\'\\n\'.join(words) + \'\\n\')\n\n\ndef read_vocab(vocab_dir):\n    """"""\xe8\xaf\xbb\xe5\x8f\x96\xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8""""""\n    # words = open_file(vocab_dir).read().strip().split(\'\\n\')\n    with open_file(vocab_dir) as fp:\n        # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xafpy2 \xe5\x88\x99\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x80\xbc\xe9\x83\xbd\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xbaunicode\n        words = [native_content(_.strip()) for _ in fp.readlines()]\n    word_to_id = dict(zip(words, range(len(words))))\n    return words, word_to_id\n\n\ndef read_category():\n    """"""\xe8\xaf\xbb\xe5\x8f\x96\xe5\x88\x86\xe7\xb1\xbb\xe7\x9b\xae\xe5\xbd\x95\xef\xbc\x8c\xe5\x9b\xba\xe5\xae\x9a""""""\n    categories = [\'\xe4\xbd\x93\xe8\x82\xb2\', \'\xe8\xb4\xa2\xe7\xbb\x8f\', \'\xe6\x88\xbf\xe4\xba\xa7\', \'\xe5\xae\xb6\xe5\xb1\x85\', \'\xe6\x95\x99\xe8\x82\xb2\', \'\xe7\xa7\x91\xe6\x8a\x80\', \'\xe6\x97\xb6\xe5\xb0\x9a\', \'\xe6\x97\xb6\xe6\x94\xbf\', \'\xe6\xb8\xb8\xe6\x88\x8f\', \'\xe5\xa8\xb1\xe4\xb9\x90\']\n\n    categories = [native_content(x) for x in categories]\n\n    cat_to_id = dict(zip(categories, range(len(categories))))\n\n    return categories, cat_to_id\n\n\ndef to_words(content, words):\n    """"""\xe5\xb0\x86id\xe8\xa1\xa8\xe7\xa4\xba\xe7\x9a\x84\xe5\x86\x85\xe5\xae\xb9\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xba\xe6\x96\x87\xe5\xad\x97""""""\n    return \'\'.join(words[x] for x in content)\n\n\ndef process_file(filename, word_to_id, cat_to_id, max_length=600):\n    """"""\xe5\xb0\x86\xe6\x96\x87\xe4\xbb\xb6\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xbaid\xe8\xa1\xa8\xe7\xa4\xba""""""\n    contents, labels = read_file(filename)\n\n    data_id, label_id = [], []\n    for i in range(len(contents)):\n        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n        label_id.append(cat_to_id[labels[i]])\n\n    # \xe4\xbd\xbf\xe7\x94\xa8keras\xe6\x8f\x90\xe4\xbe\x9b\xe7\x9a\x84pad_sequences\xe6\x9d\xa5\xe5\xb0\x86\xe6\x96\x87\xe6\x9c\xacpad\xe4\xb8\xba\xe5\x9b\xba\xe5\xae\x9a\xe9\x95\xbf\xe5\xba\xa6\n    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, max_length)\n    y_pad = kr.utils.to_categorical(label_id, num_classes=len(cat_to_id))  # \xe5\xb0\x86\xe6\xa0\x87\xe7\xad\xbe\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xbaone-hot\xe8\xa1\xa8\xe7\xa4\xba\n\n    return x_pad, y_pad\n\n\ndef batch_iter(x, y, batch_size=64):\n    """"""\xe7\x94\x9f\xe6\x88\x90\xe6\x89\xb9\xe6\xac\xa1\xe6\x95\xb0\xe6\x8d\xae""""""\n    data_len = len(x)\n    num_batch = int((data_len - 1) / batch_size) + 1\n\n    indices = np.random.permutation(np.arange(data_len))\n    x_shuffle = x[indices]\n    y_shuffle = y[indices]\n\n    for i in range(num_batch):\n        start_id = i * batch_size\n        end_id = min((i + 1) * batch_size, data_len)\n        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]\n'"
helper/__init__.py,0,b''
helper/cnews_group.py,0,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n""""""\n\xe5\xb0\x86\xe6\x96\x87\xe6\x9c\xac\xe6\x95\xb4\xe5\x90\x88\xe5\x88\xb0 train\xe3\x80\x81test\xe3\x80\x81val \xe4\xb8\x89\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\n""""""\n\nimport os\n\ndef _read_file(filename):\n    """"""\xe8\xaf\xbb\xe5\x8f\x96\xe4\xb8\x80\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe5\xb9\xb6\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xba\xe4\xb8\x80\xe8\xa1\x8c""""""\n    with open(filename, \'r\', encoding=\'utf-8\') as f:\n        return f.read().replace(\'\\n\', \'\').replace(\'\\t\', \'\').replace(\'\\u3000\', \'\')\n\ndef save_file(dirname):\n    """"""\n    \xe5\xb0\x86\xe5\xa4\x9a\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe6\x95\xb4\xe5\x90\x88\xe5\xb9\xb6\xe5\xad\x98\xe5\x88\xb03\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\n    dirname: \xe5\x8e\x9f\xe6\x95\xb0\xe6\x8d\xae\xe7\x9b\xae\xe5\xbd\x95\n    \xe6\x96\x87\xe4\xbb\xb6\xe5\x86\x85\xe5\xae\xb9\xe6\xa0\xbc\xe5\xbc\x8f:  \xe7\xb1\xbb\xe5\x88\xab\\t\xe5\x86\x85\xe5\xae\xb9\n    """"""\n    f_train = open(\'data/cnews/cnews.train.txt\', \'w\', encoding=\'utf-8\')\n    f_test = open(\'data/cnews/cnews.test.txt\', \'w\', encoding=\'utf-8\')\n    f_val = open(\'data/cnews/cnews.val.txt\', \'w\', encoding=\'utf-8\')\n    for category in os.listdir(dirname):   # \xe5\x88\x86\xe7\xb1\xbb\xe7\x9b\xae\xe5\xbd\x95\n        cat_dir = os.path.join(dirname, category)\n        if not os.path.isdir(cat_dir):\n            continue\n        files = os.listdir(cat_dir)\n        count = 0\n        for cur_file in files:\n            filename = os.path.join(cat_dir, cur_file)\n            content = _read_file(filename)\n            if count < 5000:\n                f_train.write(category + \'\\t\' + content + \'\\n\')\n            elif count < 6000:\n                f_test.write(category + \'\\t\' + content + \'\\n\')\n            else:\n                f_val.write(category + \'\\t\' + content + \'\\n\')\n            count += 1\n\n        print(\'Finished:\', category)\n\n    f_train.close()\n    f_test.close()\n    f_val.close()\n\n\nif __name__ == \'__main__\':\n    save_file(\'data/thucnews\')\n    print(len(open(\'data/cnews/cnews.train.txt\', \'r\', encoding=\'utf-8\').readlines()))\n    print(len(open(\'data/cnews/cnews.test.txt\', \'r\', encoding=\'utf-8\').readlines()))\n    print(len(open(\'data/cnews/cnews.val.txt\', \'r\', encoding=\'utf-8\').readlines()))\n'"
