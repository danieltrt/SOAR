file_path,api_count,code
make_data_set/gen_200_images.py,0,"b'import os\nimport numpy as np\nfrom PIL import Image\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n""""""\ntensorflow 1.4\nPIL 4.3.0 (pip install pillow)\nnumpy 1.13.1\n""""""\n\nif not os.path.exists(\'./images\'):\n    os.mkdir(\'./images\')\n\n\ndef gen_image(arr, index, label):\n    # \xe7\x9b\xb4\xe6\x8e\xa5\xe4\xbf\x9d\xe5\xad\x98 arr\xef\xbc\x8c\xe6\x98\xaf\xe9\xbb\x91\xe5\xba\x95\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c1.0 - arr \xe6\x98\xaf\xe7\x99\xbd\xe5\xba\x95\xe5\x9b\xbe\xe7\x89\x87\n    matrix = (np.reshape(1.0 - arr, (28, 28)) * 255).astype(np.uint8)\n    img = Image.fromarray(matrix, \'L\')\n    # \xe5\xad\x98\xe5\x82\xa8\xe5\x9b\xbe\xe7\x89\x87\xe6\x97\xb6\xef\xbc\x8clabel_index\xe7\x9a\x84\xe6\xa0\xbc\xe5\xbc\x8f\xef\xbc\x8c\xe6\x96\xb9\xe4\xbe\xbf\xe5\x9c\xa8\xe5\x88\xb6\xe4\xbd\x9c\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe6\x97\xb6\xef\xbc\x8c\xe4\xbb\x8e\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe5\x8d\xb3\xe5\x8f\xaf\xe7\x9f\xa5\xe9\x81\x93label\n    img.save(""./images/{}_{}.png"".format(label, index))\n\n\ndata = input_data.read_data_sets(\'../mnist/data_set\')\nx, y = data.train.next_batch(200)\nfor i, (arr, label) in enumerate(zip(x, y)):\n    print(i, label)\n    gen_image(arr, i, label)\n'"
make_data_set/make_hdf5_data_set.py,0,"b'import os\nimport h5py\nimport numpy as np\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\n\n""""""\nh5py: 2.7.1\nPIL 4.3.0 (pip install pillow)\nnumpy 1.13.1\nsklearn: 0.19.1\n""""""\nif not os.path.exists(\'./data_set\'):\n    os.mkdir(\'./data_set\')\n\n\ndef make_hdf5_data_set():\n    x, y = [], []\n\n    for i, image_path in enumerate(os.listdir(\'./images\')):\n        # label\xe8\xbd\xac\xe4\xb8\xba\xe7\x8b\xac\xe7\x83\xad\xe7\xbc\x96\xe7\xa0\x81\xe5\x90\x8e\xe5\x86\x8d\xe4\xbf\x9d\xe5\xad\x98\n        label = int(image_path.split(\'_\')[0])\n        label_one_hot = [0 if i != label else 1 for i in range(10)]\n        y.append(label_one_hot)\n\n        # \xe5\x9b\xbe\xe7\x89\x87\xe5\x83\x8f\xe7\xb4\xa0\xe5\x80\xbc\xe6\x98\xa0\xe5\xb0\x84\xe5\x88\xb0 0 - 1\xe4\xb9\x8b\xe9\x97\xb4\n        image = Image.open(\'./images/{}\'.format(image_path)).convert(\'L\')\n        image_arr = 1 - np.reshape(image, 784) / 255.0\n        x.append(image_arr)\n\n    with h5py.File(\'./data_set/data.h5\', \'w\') as f:\n        f.create_dataset(\'x_data\', data=np.array(x))\n        f.create_dataset(\'y_data\', data=np.array(y))\n\n\nclass DataSet:\n    def __init__(self):\n        with h5py.File(\'./data_set/data.h5\', \'r\') as f:\n            x, y = f[\'x_data\'].value, f[\'y_data\'].value\n\n        self.train_x, self.test_x, self.train_y, self.test_y = \\\n            train_test_split(x, y, test_size=0.2, random_state=0)\n\n        self.train_size = len(self.train_x)\n\n    def get_train_batch(self, batch_size=64):\n        # \xe9\x9a\x8f\xe6\x9c\xba\xe8\x8e\xb7\xe5\x8f\x96batch_size\xe4\xb8\xaa\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\n        choice = np.random.randint(self.train_size, size=batch_size)\n        batch_x = self.train_x[choice, :]\n        batch_y = self.train_y[choice, :]\n\n        return batch_x, batch_y\n\n    def get_test_set(self):\n        return self.test_x, self.test_y\n\n\nif __name__ == \'__main__\':\n    make_hdf5_data_set()\n    import time\n\n    s = time.time()\n    for i in range(1000):\n        data_set = DataSet()\n        train_x, train_y = data_set.get_train_batch()\n        test_x, test_y = data_set.get_test_set()\n\n    print(time.time() - s)\n'"
make_data_set/make_npy_data_set.py,0,"b'import os\nimport numpy as np\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\n\n""""""\nPIL 4.3.0 (pip install pillow)\nnumpy 1.13.1\nsklearn: 0.19.1\n""""""\nif not os.path.exists(\'./data_set\'):\n    os.mkdir(\'./data_set\')\n\n\ndef make_npy_data_set():\n    x, y = [], []\n\n    for i, image_path in enumerate(os.listdir(\'./images\')):\n        # label\xe8\xbd\xac\xe4\xb8\xba\xe7\x8b\xac\xe7\x83\xad\xe7\xbc\x96\xe7\xa0\x81\xe5\x90\x8e\xe5\x86\x8d\xe4\xbf\x9d\xe5\xad\x98\n        label = int(image_path.split(\'_\')[0])\n        label_one_hot = [0 if i != label else 1 for i in range(10)]\n        y.append(label_one_hot)\n\n        # \xe5\x9b\xbe\xe7\x89\x87\xe5\x83\x8f\xe7\xb4\xa0\xe5\x80\xbc\xe6\x98\xa0\xe5\xb0\x84\xe5\x88\xb0 0 - 1\xe4\xb9\x8b\xe9\x97\xb4\n        image = Image.open(\'./images/{}\'.format(image_path)).convert(\'L\')\n        image_arr = 1 - np.reshape(image, 784) / 255.0\n        x.append(image_arr)\n\n    np.save(\'data_set/X.npy\', np.array(x))\n    np.save(\'data_set/Y.npy\', np.array(y))\n\n\nclass DataSet:\n    def __init__(self):\n        x, y = np.load(\'data_set/X.npy\'), np.load(\'data_set/Y.npy\')\n        self.train_x, self.test_x, self.train_y, self.test_y = \\\n            train_test_split(x, y, test_size=0.2, random_state=0)\n\n        self.train_size = len(self.train_x)\n\n    def get_train_batch(self, batch_size=64):\n        # \xe9\x9a\x8f\xe6\x9c\xba\xe8\x8e\xb7\xe5\x8f\x96batch_size\xe4\xb8\xaa\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\n        choice = np.random.randint(self.train_size, size=batch_size)\n        batch_x = self.train_x[choice, :]\n        batch_y = self.train_y[choice, :]\n\n        return batch_x, batch_y\n\n    def get_test_set(self):\n        return self.test_x, self.test_y\n\n\nif __name__ == \'__main__\':\n    make_npy_data_set()\n\n    import time\n\n    s = time.time()\n    for i in range(1000):\n        data_set = DataSet()\n        train_x, train_y = data_set.get_train_batch()\n        test_x, test_y = data_set.get_test_set()\n\n    print(time.time() - s)\n'"
gym/CartPole-v0-nn/predict.py,0,"b'# predict.py\n# https://geektutu.com\nimport time\nimport numpy as np\nimport gym\nfrom tensorflow.keras import models\n\n\nsaved_model = models.load_model(\'CartPole-v0-nn.h5\')  # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\xa8\xa1\xe5\x9e\x8b\nenv = gym.make(""CartPole-v0"")  # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\xb8\xb8\xe6\x88\x8f\xe7\x8e\xaf\xe5\xa2\x83\n\nfor i in range(5):\n    state = env.reset()\n    score = 0\n    while True:\n        time.sleep(0.01)\n        env.render()   # \xe6\x98\xbe\xe7\xa4\xba\xe7\x94\xbb\xe9\x9d\xa2\n        action = np.argmax(saved_model.predict(np.array([state]))[0])  # \xe9\xa2\x84\xe6\xb5\x8b\xe5\x8a\xa8\xe4\xbd\x9c\n        state, reward, done, _ = env.step(action)  # \xe6\x89\xa7\xe8\xa1\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe5\x8a\xa8\xe4\xbd\x9c\n        score += reward     # \xe6\xaf\x8f\xe5\x9b\x9e\xe5\x90\x88\xe7\x9a\x84\xe5\xbe\x97\xe5\x88\x86\n        if done:       # \xe6\xb8\xb8\xe6\x88\x8f\xe7\xbb\x93\xe6\x9d\x9f\n            print(\'using nn, score: \', score)  # \xe6\x89\x93\xe5\x8d\xb0\xe5\x88\x86\xe6\x95\xb0\n            break\nenv.close()\n'"
gym/CartPole-v0-nn/train.py,0,"b'# train.py\n# https://geektutu.com\nimport random\nimport gym\nimport numpy as np\nfrom tensorflow.keras import models, layers\n\nenv = gym.make(""CartPole-v0"")  # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\xb8\xb8\xe6\x88\x8f\xe7\x8e\xaf\xe5\xa2\x83\n\nSTATE_DIM, ACTION_DIM = 4, 2  # State \xe7\xbb\xb4\xe5\xba\xa6 4, Action \xe7\xbb\xb4\xe5\xba\xa6 2\nmodel = models.Sequential([\n    layers.Dense(64, input_dim=STATE_DIM, activation=\'relu\'),\n    layers.Dense(20, activation=\'relu\'),\n    layers.Dense(ACTION_DIM, activation=\'linear\')\n])\nmodel.summary()  # \xe6\x89\x93\xe5\x8d\xb0\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe4\xbf\xa1\xe6\x81\xaf\n\n\ndef generate_data_one_episode():\n    \'\'\'\xe7\x94\x9f\xe6\x88\x90\xe5\x8d\x95\xe6\xac\xa1\xe6\xb8\xb8\xe6\x88\x8f\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\'\'\'\n    x, y, score = [], [], 0\n    state = env.reset()\n    while True:\n        action = random.randrange(0, 2)\n        x.append(state)\n        y.append([1, 0] if action == 0 else [0, 1]) # \xe8\xae\xb0\xe5\xbd\x95\xe6\x95\xb0\xe6\x8d\xae\n        state, reward, done, _ = env.step(action) # \xe6\x89\xa7\xe8\xa1\x8c\xe5\x8a\xa8\xe4\xbd\x9c\n        score += reward\n        if done:\n            break\n    return x, y, score\n\n\ndef generate_training_data(expected_score=100):\n    \'\'\'# \xe7\x94\x9f\xe6\x88\x90N\xe6\xac\xa1\xe6\xb8\xb8\xe6\x88\x8f\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe5\xb9\xb6\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xad\x9b\xe9\x80\x89\xef\xbc\x8c\xe9\x80\x89\xe6\x8b\xa9 > 100 \xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe4\xbd\x9c\xe4\xb8\xba\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\'\'\'\n    data_X, data_Y, scores = [], [], []\n    for i in range(10000):\n        x, y, score = generate_data_one_episode()\n        if score > expected_score:\n            data_X += x\n            data_Y += y\n            scores.append(score)\n    print(\'dataset size: {}, max score: {}\'.format(len(data_X), max(scores)))\n    return np.array(data_X), np.array(data_Y)\n\n\ndata_X, data_Y = generate_training_data()\nmodel.compile(loss=\'mse\', optimizer=\'adam\')\nmodel.fit(data_X, data_Y, epochs=5)\nmodel.save(\'CartPole-v0-nn.h5\')  # \xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\n'"
gym/CartPole-v0-nn/try_gym.py,0,"b'# try_gym.py\n# https://geektutu.com\nimport gym  # 0.12.5\nimport random\nimport time\n\nenv = gym.make(""CartPole-v0"")  # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\xb8\xb8\xe6\x88\x8f\xe7\x8e\xaf\xe5\xa2\x83\n\nstate = env.reset()\nscore = 0\nwhile True:\n    time.sleep(0.1)\n    env.render()   # \xe6\x98\xbe\xe7\xa4\xba\xe7\x94\xbb\xe9\x9d\xa2\n    action = random.randint(0, 1)  # \xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe6\x8b\xa9\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8a\xa8\xe4\xbd\x9c 0 \xe6\x88\x96 1\n    state, reward, done, _ = env.step(action)  # \xe6\x89\xa7\xe8\xa1\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe5\x8a\xa8\xe4\xbd\x9c\n    score += reward     # \xe6\xaf\x8f\xe5\x9b\x9e\xe5\x90\x88\xe7\x9a\x84\xe5\xbe\x97\xe5\x88\x86\n    if done:       # \xe6\xb8\xb8\xe6\x88\x8f\xe7\xbb\x93\xe6\x9d\x9f\n        print(\'score: \', score)  # \xe6\x89\x93\xe5\x8d\xb0\xe5\x88\x86\xe6\x95\xb0\n        break\nenv.close()\n'"
gym/CartPole-v0-policy-gradient/policy_gradient.py,0,"b'# policy_gradient.py\n# https://geektutu.com\nimport matplotlib.pyplot as plt\nimport gym\nimport numpy as np\nfrom tensorflow.keras import models, layers, optimizers\n\nenv = gym.make(\'CartPole-v0\')\n\nSTATE_DIM, ACTION_DIM = 4, 2\nmodel = models.Sequential([\n    layers.Dense(100, input_dim=STATE_DIM, activation=\'relu\'),\n    layers.Dropout(0.1),\n    layers.Dense(ACTION_DIM, activation=""softmax"")\n])\nmodel.compile(loss=\'mean_squared_error\',\n              optimizer=optimizers.Adam(0.001))\n\n\ndef choose_action(s):\n    """"""\xe9\xa2\x84\xe6\xb5\x8b\xe5\x8a\xa8\xe4\xbd\x9c""""""\n    prob = model.predict(np.array([s]))[0]\n    return np.random.choice(len(prob), p=prob)\n\n\ndef discount_rewards(rewards, gamma=0.95):\n    """"""\xe8\xae\xa1\xe7\xae\x97\xe8\xa1\xb0\xe5\x87\x8freward\xe7\x9a\x84\xe7\xb4\xaf\xe5\x8a\xa0\xe6\x9c\x9f\xe6\x9c\x9b\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\xad\xe5\xbf\x83\xe5\x8c\x96\xe5\x92\x8c\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe5\xa4\x84\xe7\x90\x86""""""\n    prior = 0\n    out = np.zeros_like(rewards)\n    for i in reversed(range(len(rewards))):\n        prior = prior * gamma + rewards[i]\n        out[i] = prior\n    return out / np.std(out - np.mean(out))\n\n\ndef train(records):\n    s_batch = np.array([record[0] for record in records])\n    # action \xe7\x8b\xac\xe7\x83\xad\xe7\xbc\x96\xe7\xa0\x81\xe5\xa4\x84\xe7\x90\x86\xef\xbc\x8c\xe6\x96\xb9\xe4\xbe\xbf\xe6\xb1\x82\xe5\x8a\xa8\xe4\xbd\x9c\xe6\xa6\x82\xe7\x8e\x87\xef\xbc\x8c\xe5\x8d\xb3 prob_batch\n    a_batch = np.array([[1 if record[1] == i else 0 for i in range(ACTION_DIM)]\n                        for record in records])\n    # \xe5\x81\x87\xe8\xae\xbepredict\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\xe6\x98\xaf [0.3, 0.7]\xef\xbc\x8c\xe9\x80\x89\xe6\x8b\xa9\xe7\x9a\x84\xe5\x8a\xa8\xe4\xbd\x9c\xe6\x98\xaf [0, 1]\n    # \xe5\x88\x99\xe5\x8a\xa8\xe4\xbd\x9c[0, 1]\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\xe7\xad\x89\xe4\xba\x8e [0, 0.7] = [0.3, 0.7] * [0, 1]\n    prob_batch = model.predict(s_batch) * a_batch\n    r_batch = discount_rewards([record[2] for record in records])\n\n    model.fit(s_batch, prob_batch, sample_weight=r_batch, verbose=0)\n\n\nepisodes = 2000  # \xe8\x87\xb3\xe5\xa4\x9a2000\xe6\xac\xa1\nscore_list = []  # \xe8\xae\xb0\xe5\xbd\x95\xe6\x89\x80\xe6\x9c\x89\xe5\x88\x86\xe6\x95\xb0\nfor i in range(episodes):\n    s = env.reset()\n    score = 0\n    replay_records = []\n    while True:\n        a = choose_action(s)\n        next_s, r, done, _ = env.step(a)\n        replay_records.append((s, a, r))\n\n        score += r\n        s = next_s\n        if done:\n            train(replay_records)\n            score_list.append(score)\n            print(\'episode:\', i, \'score:\', score, \'max:\', max(score_list))\n            break\n    # \xe6\x9c\x80\xe5\x90\x8e10\xe6\xac\xa1\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe5\x88\x86\xe5\xa4\xa7\xe4\xba\x8e 195 \xe6\x97\xb6\xef\xbc\x8c\xe5\x81\x9c\xe6\xad\xa2\xe5\xb9\xb6\xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\n    if np.mean(score_list[-10:]) > 195:\n        model.save(\'CartPole-v0-pg.h5\')\n        break\nenv.close()\n\n\n# \xe7\x94\xbb\xe5\x9b\xbe\nplt.plot(score_list)\nx = np.array(range(len(score_list)))\nsmooth_func = np.poly1d(np.polyfit(x, score_list, 3))\nplt.plot(x, smooth_func(x), label=\'Mean\', linestyle=\'--\')\nplt.show()\n'"
gym/CartPole-v0-policy-gradient/test_policy_gradient.py,0,"b'# test_policy_gradient.py\n# https://geektutu.com\nimport time\nimport numpy as np\nimport gym\nfrom tensorflow.keras import models\n\n\nsaved_model = models.load_model(\'CartPole-v0-pg.h5\')\nenv = gym.make(""CartPole-v0"")\n\nfor i in range(5):\n    s = env.reset()\n    score = 0\n    while True:\n        time.sleep(0.01)\n        env.render()\n        prob = saved_model.predict(np.array([s]))[0]\n        a = np.random.choice(len(prob), p=prob)\n        s, r, done, _ = env.step(a)\n        score += r\n        if done:\n            print(\'using policy gradient, score: \', score)  # \xe6\x89\x93\xe5\x8d\xb0\xe5\x88\x86\xe6\x95\xb0\n            break\nenv.close()\n'"
gym/MountainCar-v0-dqn/dqn.py,0,"b'# dqn.py\n# https://geektutu.com\nfrom collections import deque\nimport random\nimport gym\nimport numpy as np\nfrom tensorflow.keras import models, layers, optimizers\n\n\nclass DQN(object):\n    def __init__(self):\n        self.step = 0\n        self.update_freq = 200  # \xe6\xa8\xa1\xe5\x9e\x8b\xe6\x9b\xb4\xe6\x96\xb0\xe9\xa2\x91\xe7\x8e\x87\n        self.replay_size = 2000  # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe5\xa4\xa7\xe5\xb0\x8f\n        self.replay_queue = deque(maxlen=self.replay_size)\n        self.model = self.create_model()\n        self.target_model = self.create_model()\n\n    def create_model(self):\n        """"""\xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe4\xb8\xba100\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c""""""\n        STATE_DIM, ACTION_DIM = 2, 3\n        model = models.Sequential([\n            layers.Dense(100, input_dim=STATE_DIM, activation=\'relu\'),\n            layers.Dense(ACTION_DIM, activation=""linear"")\n        ])\n        model.compile(loss=\'mean_squared_error\',\n                      optimizer=optimizers.Adam(0.001))\n        return model\n\n    def act(self, s, epsilon=0.1):\n        """"""\xe9\xa2\x84\xe6\xb5\x8b\xe5\x8a\xa8\xe4\xbd\x9c""""""\n        # \xe5\x88\x9a\xe5\xbc\x80\xe5\xa7\x8b\xe6\x97\xb6\xef\xbc\x8c\xe5\x8a\xa0\xe4\xb8\x80\xe7\x82\xb9\xe9\x9a\x8f\xe6\x9c\xba\xe6\x88\x90\xe5\x88\x86\xef\xbc\x8c\xe4\xba\xa7\xe7\x94\x9f\xe6\x9b\xb4\xe5\xa4\x9a\xe7\x9a\x84\xe7\x8a\xb6\xe6\x80\x81\n        if np.random.uniform() < epsilon - self.step * 0.0002:\n            return np.random.choice([0, 1, 2])\n        return np.argmax(self.model.predict(np.array([s]))[0])\n\n    def save_model(self, file_path=\'MountainCar-v0-dqn.h5\'):\n        print(\'model saved\')\n        self.model.save(file_path)\n\n    def remember(self, s, a, next_s, reward):\n        """"""\xe5\x8e\x86\xe5\x8f\xb2\xe8\xae\xb0\xe5\xbd\x95\xef\xbc\x8cposition >= 0.4\xe6\x97\xb6\xe7\xbb\x99\xe9\xa2\x9d\xe5\xa4\x96\xe7\x9a\x84reward\xef\xbc\x8c\xe5\xbf\xab\xe9\x80\x9f\xe6\x94\xb6\xe6\x95\x9b""""""\n        if next_s[0] >= 0.4:\n            reward += 1\n        self.replay_queue.append((s, a, next_s, reward))\n\n    def train(self, batch_size=64, lr=1, factor=0.95):\n        if len(self.replay_queue) < self.replay_size:\n            return\n        self.step += 1\n        # \xe6\xaf\x8f update_freq \xe6\xad\xa5\xef\xbc\x8c\xe5\xb0\x86 model \xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\xe8\xb5\x8b\xe5\x80\xbc\xe7\xbb\x99 target_model\n        if self.step % self.update_freq == 0:\n            self.target_model.set_weights(self.model.get_weights())\n\n        replay_batch = random.sample(self.replay_queue, batch_size)\n        s_batch = np.array([replay[0] for replay in replay_batch])\n        next_s_batch = np.array([replay[2] for replay in replay_batch])\n\n        Q = self.model.predict(s_batch)\n        Q_next = self.target_model.predict(next_s_batch)\n\n        # \xe4\xbd\xbf\xe7\x94\xa8\xe5\x85\xac\xe5\xbc\x8f\xe6\x9b\xb4\xe6\x96\xb0\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe4\xb8\xad\xe7\x9a\x84Q\xe5\x80\xbc\n        for i, replay in enumerate(replay_batch):\n            _, a, _, reward = replay\n            Q[i][a] = (1 - lr) * Q[i][a] + lr * (reward + factor * np.amax(Q_next[i]))\n \n        # \xe4\xbc\xa0\xe5\x85\xa5\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xae\xad\xe7\xbb\x83\n        self.model.fit(s_batch, Q, verbose=0)\n\n\nenv = gym.make(\'MountainCar-v0\')\nepisodes = 1000  # \xe8\xae\xad\xe7\xbb\x831000\xe6\xac\xa1\nscore_list = []  # \xe8\xae\xb0\xe5\xbd\x95\xe6\x89\x80\xe6\x9c\x89\xe5\x88\x86\xe6\x95\xb0\nagent = DQN()\nfor i in range(episodes):\n    s = env.reset()\n    score = 0\n    while True:\n        a = agent.act(s)\n        next_s, reward, done, _ = env.step(a)\n        agent.remember(s, a, next_s, reward)\n        agent.train()\n        score += reward\n        s = next_s\n        if done:\n            score_list.append(score)\n            print(\'episode:\', i, \'score:\', score, \'max:\', max(score_list))\n            break\n    # \xe6\x9c\x80\xe5\x90\x8e10\xe6\xac\xa1\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe5\x88\x86\xe5\xa4\xa7\xe4\xba\x8e -160 \xe6\x97\xb6\xef\xbc\x8c\xe5\x81\x9c\xe6\xad\xa2\xe5\xb9\xb6\xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\n    if np.mean(score_list[-10:]) > -160:\n        agent.save_model()\n        break\nenv.close()\n\nimport matplotlib.pyplot as plt\n\nplt.plot(score_list, color=\'green\')\nplt.show()'"
gym/MountainCar-v0-dqn/test_dqn.py,0,"b""# test_dqn.py\n# https://geektutu.com\nimport time\nimport gym\nimport numpy as np\nfrom tensorflow.keras import models\nenv = gym.make('MountainCar-v0')\nmodel = models.load_model('MountainCar-v0-dqn.h5')\ns = env.reset()\nscore = 0\nwhile True:\n    env.render()\n    time.sleep(0.01)\n    a = np.argmax(model.predict(np.array([s]))[0])\n    s, reward, done, _ = env.step(a)\n    score += reward\n    if done:\n        print('score:', score)\n        break\nenv.close()"""
gym/MountainCar-v0-q-learning/q_learning.py,0,"b'# q_learning.py\n# https://geektutu.com\nimport pickle\nfrom collections import defaultdict\nimport gym  # 0.12.5\nimport numpy as np\n\n# \xe9\xbb\x98\xe8\xae\xa4\xe5\xb0\x86Action 0,1,2\xe7\x9a\x84\xe4\xbb\xb7\xe5\x80\xbc\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xb8\xba0\nQ = defaultdict(lambda: [0, 0, 0])\n\nenv = gym.make(\'MountainCar-v0\')\n\n\ndef transform_state(state):\n    """"""\xe5\xb0\x86 position, velocity \xe9\x80\x9a\xe8\xbf\x87\xe7\xba\xbf\xe6\x80\xa7\xe8\xbd\xac\xe6\x8d\xa2\xe6\x98\xa0\xe5\xb0\x84\xe5\x88\xb0 [0, 40] \xe8\x8c\x83\xe5\x9b\xb4\xe5\x86\x85""""""\n    pos, v = state\n    pos_low, v_low = env.observation_space.low\n    pos_high, v_high = env.observation_space.high\n\n    a = 40 * (pos - pos_low) / (pos_high - pos_low)\n    b = 40 * (v - v_low) / (v_high - v_low)\n\n    return int(a), int(b)\n\n# print(transform_state([-1.0, 0.01]))\n# eg: (4, 22)\n\n\nlr, factor = 0.7, 0.95\nepisodes = 10000  # \xe8\xae\xad\xe7\xbb\x8310000\xe6\xac\xa1\nscore_list = []  # \xe8\xae\xb0\xe5\xbd\x95\xe6\x89\x80\xe6\x9c\x89\xe5\x88\x86\xe6\x95\xb0\nfor i in range(episodes):\n    s = transform_state(env.reset())\n    score = 0\n    while True:\n        a = np.argmax(Q[s])\n        # \xe8\xae\xad\xe7\xbb\x83\xe5\x88\x9a\xe5\xbc\x80\xe5\xa7\x8b\xef\xbc\x8c\xe5\xa4\x9a\xe4\xb8\x80\xe7\x82\xb9\xe9\x9a\x8f\xe6\x9c\xba\xe6\x80\xa7\xef\xbc\x8c\xe4\xbb\xa5\xe4\xbe\xbf\xe6\x9c\x89\xe6\x9b\xb4\xe5\xa4\x9a\xe7\x9a\x84\xe7\x8a\xb6\xe6\x80\x81\n        if np.random.random() > i * 3 / episodes:\n            a = np.random.choice([0, 1, 2])\n        # \xe6\x89\xa7\xe8\xa1\x8c\xe5\x8a\xa8\xe4\xbd\x9c\n        next_s, reward, done, _ = env.step(a)\n        next_s = transform_state(next_s)\n        # \xe6\xa0\xb9\xe6\x8d\xae\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe5\x85\xac\xe5\xbc\x8f\xe6\x9b\xb4\xe6\x96\xb0Q-Table\n        Q[s][a] = (1 - lr) * Q[s][a] + lr * (reward + factor * max(Q[next_s]))\n        score += reward\n        s = next_s\n        if done:\n            score_list.append(score)\n            print(\'episode:\', i, \'score:\', score, \'max:\', max(score_list))\n            break\nenv.close()\n\n# \xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\nwith open(\'MountainCar-v0-q-learning.pickle\', \'wb\') as f:\n    pickle.dump(dict(Q), f)\n    print(\'model saved\')\n'"
gym/MountainCar-v0-q-learning/test_q_learning.py,0,"b'# test_q_learning.py\n# https://geektutu.com\nimport time\nimport pickle\nimport gym\nimport numpy as np\n\n\ndef transform_state(state):\n    """"""\xe5\xb0\x86 position, velocity \xe9\x80\x9a\xe8\xbf\x87\xe7\xba\xbf\xe6\x80\xa7\xe8\xbd\xac\xe6\x8d\xa2\xe6\x98\xa0\xe5\xb0\x84\xe5\x88\xb0 [0, 40] \xe8\x8c\x83\xe5\x9b\xb4\xe5\x86\x85""""""\n    pos, v = state\n    pos_low, v_low = env.observation_space.low\n    pos_high, v_high = env.observation_space.high\n\n    a = 40 * (pos - pos_low) / (pos_high - pos_low)\n    b = 40 * (v - v_low) / (v_high - v_low)\n\n    return int(a), int(b)\n\n\n# \xe5\x8a\xa0\xe8\xbd\xbd\xe6\xa8\xa1\xe5\x9e\x8b\nwith open(\'MountainCar-v0-q-learning.pickle\', \'rb\') as f:\n    Q = pickle.load(f)\n    print(\'model loaded\')\n\nenv = gym.make(\'MountainCar-v0\')\ns = env.reset()\nscore = 0\nwhile True:\n    env.render()\n    time.sleep(0.01)\n    # transform_state\xe5\x87\xbd\xe6\x95\xb0 \xe4\xb8\x8e \xe8\xae\xad\xe7\xbb\x83\xe6\x97\xb6\xe7\x9a\x84\xe4\xb8\x80\xe8\x87\xb4\n    s = transform_state(s)\n    a = np.argmax(Q[s]) if s in Q else 0\n    s, reward, done, _ = env.step(a)\n    score += reward\n    if done:\n        print(\'score:\', score)\n        break\nenv.close()\n'"
gym/MountainCar-v0-q-learning/try_gym.py,0,"b""import time\nimport random\nimport gym  # 0.12.5\n\nenv = gym.make('MountainCar-v0')\nenv.reset()\nscore = 0\nfor i in range(2):\n    while True:\n        env.render()\n        time.sleep(0.01)\n        a = random.randint(0, 2)\n        s, reward, done, _ = env.step(a)\n        score += reward\n        if done:\n            print('score:', score)\n            break\n    env.close()"""
mnist/v1/model.py,9,"b'import tensorflow as tf\n\n\nclass Network:\n    def __init__(self):\n        # \xe5\xad\xa6\xe4\xb9\xa0\xe9\x80\x9f\xe7\x8e\x87\xef\xbc\x8c\xe4\xb8\x80\xe8\x88\xac\xe5\x9c\xa8 0.00001 - 0.5 \xe4\xb9\x8b\xe9\x97\xb4\n        self.learning_rate = 0.001\n\n        # \xe8\xbe\x93\xe5\x85\xa5\xe5\xbc\xa0\xe9\x87\x8f 28 * 28 = 784\xe4\xb8\xaa\xe5\x83\x8f\xe7\xb4\xa0\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\x80\xe7\xbb\xb4\xe5\x90\x91\xe9\x87\x8f\n        self.x = tf.placeholder(tf.float32, [None, 784])\n\n        # \xe6\xa0\x87\xe7\xad\xbe\xe5\x80\xbc\xef\xbc\x8c\xe5\x8d\xb3\xe5\x9b\xbe\xe5\x83\x8f\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe5\xaf\xb9\xe5\xba\x94\xe6\x95\xb0\xe5\xad\x97\xe6\x98\xaf8\xef\xbc\x8c\xe5\x88\x99\xe5\xaf\xb9\xe5\xba\x94label\xe6\x98\xaf [0,0,0,0,0,0,0,0,1,0]\n        # \xe8\xbf\x99\xe7\xa7\x8d\xe6\x96\xb9\xe5\xbc\x8f\xe7\xa7\xb0\xe4\xb8\xba one-hot\xe7\xbc\x96\xe7\xa0\x81\n        # \xe6\xa0\x87\xe7\xad\xbe\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\xba10\xe7\x9a\x84\xe4\xb8\x80\xe7\xbb\xb4\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x8c\xe5\x80\xbc\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe4\xb8\x8b\xe6\xa0\x87\xe5\x8d\xb3\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\x8a\xe5\x86\x99\xe7\x9a\x84\xe6\x95\xb0\xe5\xad\x97\n        self.label = tf.placeholder(tf.float32, [None, 10])\n\n        # \xe6\x9d\x83\xe9\x87\x8d\xef\xbc\x8c\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x85\xa8 0\n        self.w = tf.Variable(tf.zeros([784, 10]))\n        # \xe5\x81\x8f\xe7\xbd\xae bias\xef\xbc\x8c \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x85\xa8 0\n        self.b = tf.Variable(tf.zeros([10]))\n        # \xe8\xbe\x93\xe5\x87\xba y = softmax(X * w + b)\n        self.y = tf.nn.softmax(tf.matmul(self.x, self.w) + self.b)\n\n        # \xe6\x8d\x9f\xe5\xa4\xb1\xef\xbc\x8c\xe5\x8d\xb3\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xef\xbc\x8c\xe6\x9c\x80\xe5\xb8\xb8\xe7\x94\xa8\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe6\xa0\x87\xe7\xad\xbe(label)\xe4\xb8\x8e\xe8\xbe\x93\xe5\x87\xba(y)\xe4\xb9\x8b\xe9\x97\xb4\xe5\xb7\xae\xe5\x88\xab\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\n        self.loss = -tf.reduce_sum(self.label * tf.log(self.y + 1e-10))\n\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe9\x87\x87\xe7\x94\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\xe3\x80\x82\xe8\xb0\x83\xe6\x95\xb4w\xe4\xb8\x8eb\xef\xbc\x8c\xe4\xbd\xbf\xe5\xbe\x97\xe6\x8d\x9f\xe5\xa4\xb1(loss)\xe6\x9c\x80\xe5\xb0\x8f\n        # loss\xe8\xb6\x8a\xe5\xb0\x8f\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe8\xae\xa1\xe7\xae\x97\xe5\x87\xba\xe6\x9d\xa5\xe7\x9a\x84y\xe5\x80\xbc\xe4\xb8\x8e \xe6\xa0\x87\xe7\xad\xbe(label)\xe5\x80\xbc\xe8\xb6\x8a\xe6\x8e\xa5\xe8\xbf\x91\xef\xbc\x8c\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe8\xb6\x8a\xe9\xab\x98\n        self.train = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss)\n\n        # \xe4\xbb\xa5\xe4\xb8\x8b\xe4\xbb\xa3\xe7\xa0\x81\xe9\xaa\x8c\xe8\xaf\x81\xe6\xad\xa3\xe7\xa1\xae\xe7\x8e\x87\xe6\x97\xb6\xe4\xbd\xbf\xe7\x94\xa8\n        # argmax \xe8\xbf\x94\xe5\x9b\x9e\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\xe7\x9a\x84\xe4\xb8\x8b\xe6\xa0\x87\xef\xbc\x8c\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\xe7\x9a\x84\xe4\xb8\x8b\xe6\xa0\x87\xe5\x8d\xb3\xe7\xad\x94\xe6\xa1\x88\n        # \xe4\xbe\x8b\xe5\xa6\x82 [0,0,0,0.9,0,0.1,0,0,0,0] \xe4\xbb\xa3\xe8\xa1\xa8\xe6\x95\xb0\xe5\xad\x973\n        predict = tf.equal(tf.argmax(self.label, 1), tf.argmax(self.y, 1))\n\n        # predict -> [true, true, true, false, false, true]\n        # reduce_mean\xe5\x8d\xb3\xe6\xb1\x82predict\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe6\x95\xb0 \xe5\x8d\xb3 \xe6\xad\xa3\xe7\xa1\xae\xe4\xb8\xaa\xe6\x95\xb0 / \xe6\x80\xbb\xe6\x95\xb0\xef\xbc\x8c\xe5\x8d\xb3\xe6\xad\xa3\xe7\xa1\xae\xe7\x8e\x87\n        self.accuracy = tf.reduce_mean(tf.cast(predict, ""float""))\n'"
mnist/v1/train.py,2,"b'import tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nfrom model import Network\n\n\'\'\'\npython 3.6\ntensorflow 1.4\n\'\'\'\n\n\nclass Train:\n    def __init__(self):\n        self.net = Network()\n\n        # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96 session\n        # Network() \xe5\x8f\xaa\xe6\x98\xaf\xe6\x9e\x84\xe9\x80\xa0\xe4\xba\x86\xe4\xb8\x80\xe5\xbc\xa0\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97\xe9\x9c\x80\xe8\xa6\x81\xe6\x94\xbe\xe5\x88\xb0\xe4\xbc\x9a\xe8\xaf\x9d(session)\xe4\xb8\xad\n        self.sess = tf.Session()\n        # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x8f\x98\xe9\x87\x8f\n        self.sess.run(tf.global_variables_initializer())\n\n        # \xe8\xaf\xbb\xe5\x8f\x96\xe8\xae\xad\xe7\xbb\x83\xe5\x92\x8c\xe6\xb5\x8b\xe8\xaf\x95\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe8\xbf\x99\xe6\x98\xaftensorflow\xe5\xba\x93\xe8\x87\xaa\xe5\xb8\xa6\xe7\x9a\x84\xef\xbc\x8c\xe4\xb8\x8d\xe5\xad\x98\xe5\x9c\xa8\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe4\xbc\x9a\xe8\x87\xaa\xe5\x8a\xa8\xe4\xb8\x8b\xe8\xbd\xbd\n        # \xe9\xa1\xb9\xe7\x9b\xae\xe7\x9b\xae\xe5\xbd\x95\xe4\xb8\x8b\xe5\xb7\xb2\xe7\xbb\x8f\xe4\xb8\x8b\xe8\xbd\xbd\xe5\xa5\xbd\xef\xbc\x8c\xe5\x88\xa0\xe6\x8e\x89\xe5\x90\x8e\xef\xbc\x8c\xe9\x87\x8d\xe6\x96\xb0\xe8\xbf\x90\xe8\xa1\x8c\xe4\xbb\xa3\xe7\xa0\x81\xe4\xbc\x9a\xe8\x87\xaa\xe5\x8a\xa8\xe4\xb8\x8b\xe8\xbd\xbd\n        # data_set/train-images-idx3-ubyte.gz\n        # data_set/train-labels-idx1-ubyte.gz\n        # data_set/t10k-images-idx3-ubyte.gz\n        # data_set/t10k-labels-idx1-ubyte.gz\n        self.data = input_data.read_data_sets(\'../data_set\', one_hot=True)\n\n    def train(self):\n        # batch_size \xe6\x98\xaf\xe6\x8c\x87\xe6\xaf\x8f\xe6\xac\xa1\xe8\xbf\xad\xe4\xbb\xa3\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe4\xbc\xa0\xe5\x85\xa5\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe5\xbc\xa0\xe6\x95\xb0\xe3\x80\x82\n        # \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\xb0\x8f\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8\xe5\x85\xa8\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe6\x95\xb0\xe6\x8d\xae\xe5\xa4\xa7\xe7\x9a\x84\xe6\x83\x85\xe5\x86\xb5\xe4\xb8\x8b\xef\xbc\x8c\n        # \xe4\xb8\xba\xe4\xba\x86\xe6\x8f\x90\xe9\xab\x98\xe8\xae\xad\xe7\xbb\x83\xe9\x80\x9f\xe5\xba\xa6\xef\xbc\x8c\xe7\x94\xa8\xe9\x9a\x8f\xe6\x9c\xba\xe6\x8a\xbd\xe5\x8f\x96\xe7\x9a\x84n\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe6\x9d\xa5\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe6\x95\x88\xe6\x9e\x9c\xe4\xb8\x8e\xe5\x85\xa8\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\x9b\xb8\xe8\xbf\x91\n        # https://www.zhihu.com/question/32673260\n        batch_size = 64\n\n        # \xe6\x80\xbb\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe6\xac\xa1\xe6\x95\xb0\n        train_step = 2000\n\n        # \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\n        for i in range(train_step):\n            # \xe4\xbb\x8e\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xb8\xad\xe8\x8e\xb7\xe5\x8f\x96 \xe8\xbe\x93\xe5\x85\xa5\xe5\x92\x8c\xe6\xa0\x87\xe7\xad\xbe(\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf\xe7\xad\x94\xe6\xa1\x88)\n            x, label = self.data.train.next_batch(batch_size)\n            # \xe6\xaf\x8f\xe6\xac\xa1\xe8\xae\xa1\xe7\xae\x97train\xef\xbc\x8c\xe6\x9b\xb4\xe6\x96\xb0\xe6\x95\xb4\xe4\xb8\xaa\xe7\xbd\x91\xe7\xbb\x9c\n            # loss\xe5\x8f\xaa\xe6\x98\xaf\xe4\xb8\xba\xe4\xba\x86\xe7\x9c\x8b\xe5\x88\xb0\xe6\x8d\x9f\xe5\xa4\xb1\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xef\xbc\x8c\xe6\x96\xb9\xe4\xbe\xbf\xe6\x89\x93\xe5\x8d\xb0\n            _, loss = self.sess.run([self.net.train, self.net.loss],\n                                    feed_dict={self.net.x: x, self.net.label: label})\n\n            # \xe6\x89\x93\xe5\x8d\xb0 loss\xef\xbc\x8c\xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad\xe5\xb0\x86\xe4\xbc\x9a\xe7\x9c\x8b\xe5\x88\xb0\xef\xbc\x8closs\xe6\x9c\x89\xe5\x8f\x98\xe5\xb0\x8f\xe7\x9a\x84\xe8\xb6\x8b\xe5\x8a\xbf\n            # \xe4\xbb\xa3\xe8\xa1\xa8\xe9\x9a\x8f\xe7\x9d\x80\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe8\xbf\x9b\xe8\xa1\x8c\xef\xbc\x8c\xe7\xbd\x91\xe7\xbb\x9c\xe8\xaf\x86\xe5\x88\xab\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe8\x83\xbd\xe5\x8a\x9b\xe6\x8f\x90\xe9\xab\x98\n            # \xe4\xbd\x86\xe6\x98\xaf\xe7\x94\xb1\xe4\xba\x8e\xe7\xbd\x91\xe7\xbb\x9c\xe8\xa7\x84\xe6\xa8\xa1\xe8\xbe\x83\xe5\xb0\x8f\xef\xbc\x8c\xe5\x90\x8e\xe6\x9c\x9f\xe6\xb2\xa1\xe6\x9c\x89\xe6\x98\x8e\xe6\x98\xbe\xe4\xb8\x8b\xe9\x99\x8d\xef\xbc\x8c\xe8\x80\x8c\xe6\x98\xaf\xe6\x9c\x89\xe6\x98\x8e\xe6\x98\xbe\xe6\xb3\xa2\xe5\x8a\xa8\n            if (i + 1) % 10 == 0:\n                print(\'\xe7\xac\xac%5d\xe6\xad\xa5\xef\xbc\x8c\xe5\xbd\x93\xe5\x89\x8dloss\xef\xbc\x9a%.2f\' % (i + 1, loss))\n\n    def calculate_accuracy(self):\n        test_x = self.data.test.images\n        test_label = self.data.test.labels\n        # \xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x9a\xe4\xb8\x8e\xe8\xae\xad\xe7\xbb\x83\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe6\x98\xaf\xef\xbc\x8c\xe5\xb9\xb6\xe6\xb2\xa1\xe6\x9c\x89\xe8\xae\xa1\xe7\xae\x97 self.net.train\n        # \xe5\x8f\xaa\xe8\xae\xa1\xe7\xae\x97\xe4\xba\x86accuracy\xe8\xbf\x99\xe4\xb8\xaa\xe5\xbc\xa0\xe9\x87\x8f\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe4\xb8\x8d\xe4\xbc\x9a\xe6\x9b\xb4\xe6\x96\xb0\xe7\xbd\x91\xe7\xbb\x9c\n        # \xe6\x9c\x80\xe7\xbb\x88\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe7\xba\xa6\xe4\xb8\xba0.91\n        accuracy = self.sess.run(self.net.accuracy,\n                                 feed_dict={self.net.x: test_x, self.net.label: test_label})\n        print(""\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87: %.2f\xef\xbc\x8c\xe5\x85\xb1\xe6\xb5\x8b\xe8\xaf\x95\xe4\xba\x86%d\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87 "" % (accuracy, len(test_label)))\n\n\nif __name__ == ""__main__"":\n    app = Train()\n    app.train()\n    app.calculate_accuracy()\n'"
mnist/v2/model.py,10,"b'import tensorflow as tf\n\n\nclass Network:\n    def __init__(self):\n        self.learning_rate = 0.001\n        # \xe8\xae\xb0\xe5\xbd\x95\xe5\xb7\xb2\xe7\xbb\x8f\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\xac\xa1\xe6\x95\xb0\n        self.global_step = tf.Variable(0, trainable=False)\n\n        self.x = tf.placeholder(tf.float32, [None, 784])\n        self.label = tf.placeholder(tf.float32, [None, 10])\n\n        self.w = tf.Variable(tf.zeros([784, 10]))\n        self.b = tf.Variable(tf.zeros([10]))\n        self.y = tf.nn.softmax(tf.matmul(self.x, self.w) + self.b)\n\n        self.loss = -tf.reduce_sum(self.label * tf.log(self.y + 1e-10))\n\n        # minimize \xe5\x8f\xaf\xe4\xbc\xa0\xe5\x85\xa5\xe5\x8f\x82\xe6\x95\xb0 global_step\xef\xbc\x8c \xe6\xaf\x8f\xe6\xac\xa1\xe8\xae\xad\xe7\xbb\x83 global_step\xe7\x9a\x84\xe5\x80\xbc\xe4\xbc\x9a\xe5\xa2\x9e\xe5\x8a\xa01\n        # \xe5\x9b\xa0\xe6\xad\xa4\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x80\x9a\xe8\xbf\x87\xe8\xae\xa1\xe7\xae\x97self.global_step\xe8\xbf\x99\xe4\xb8\xaa\xe5\xbc\xa0\xe9\x87\x8f\xe7\x9a\x84\xe5\x80\xbc\xef\xbc\x8c\xe7\x9f\xa5\xe9\x81\x93\xe5\xbd\x93\xe5\x89\x8d\xe8\xae\xad\xe7\xbb\x83\xe4\xba\x86\xe5\xa4\x9a\xe5\xb0\x91\xe6\xad\xa5\n        self.train = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(\n            self.loss, global_step=self.global_step)\n\n        predict = tf.equal(tf.argmax(self.label, 1), tf.argmax(self.y, 1))\n        self.accuracy = tf.reduce_mean(tf.cast(predict, ""float""))\n'"
mnist/v2/predict.py,4,"b'import tensorflow as tf\nimport numpy as np\nfrom PIL import Image\n\nfrom model import Network\n\n\'\'\'\npython 3.6\ntensorflow 1.4\npillow(PIL) 4.3.0\n\xe4\xbd\xbf\xe7\x94\xa8tensorflow\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x9d\xa5\xe9\xa2\x84\xe6\xb5\x8b\xe6\x89\x8b\xe5\x86\x99\xe6\x95\xb0\xe5\xad\x97\n\xe8\xbe\x93\xe5\x85\xa5\xe6\x98\xaf28 * 28\xe5\x83\x8f\xe7\xb4\xa0\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe6\x98\xaf\xe4\xb8\xaa\xe5\x85\xb7\xe4\xbd\x93\xe7\x9a\x84\xe6\x95\xb0\xe5\xad\x97\n\'\'\'\n\nCKPT_DIR = \'ckpt\'\n\n\nclass Predict:\n    def __init__(self):\n        self.net = Network()\n        self.sess = tf.Session()\n        self.sess.run(tf.global_variables_initializer())\n\n        # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x88\xb0sess\xe4\xb8\xad\n        self.restore()\n\n    def restore(self):\n        saver = tf.train.Saver()\n        ckpt = tf.train.get_checkpoint_state(CKPT_DIR)\n        if ckpt and ckpt.model_checkpoint_path:\n            saver.restore(self.sess, ckpt.model_checkpoint_path)\n        else:\n            raise FileNotFoundError(""\xe6\x9c\xaa\xe4\xbf\x9d\xe5\xad\x98\xe4\xbb\xbb\xe4\xbd\x95\xe6\xa8\xa1\xe5\x9e\x8b"")\n\n    def predict(self, image_path):\n        # \xe8\xaf\xbb\xe5\x9b\xbe\xe7\x89\x87\xe5\xb9\xb6\xe8\xbd\xac\xe4\xb8\xba\xe9\xbb\x91\xe7\x99\xbd\xe7\x9a\x84\n        img = Image.open(image_path).convert(\'L\')\n        flatten_img = np.reshape(img, 784)\n        x = np.array([1 - flatten_img])\n        y = self.sess.run(self.net.y, feed_dict={self.net.x: x})\n\n        # \xe5\x9b\xa0\xe4\xb8\xbax\xe5\x8f\xaa\xe4\xbc\xa0\xe5\x85\xa5\xe4\xba\x86\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe5\x8f\x96y[0]\xe5\x8d\xb3\xe5\x8f\xaf\n        # np.argmax()\xe5\x8f\x96\xe5\xbe\x97\xe7\x8b\xac\xe7\x83\xad\xe7\xbc\x96\xe7\xa0\x81\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\xe7\x9a\x84\xe4\xb8\x8b\xe6\xa0\x87\xef\xbc\x8c\xe5\x8d\xb3\xe4\xbb\xa3\xe8\xa1\xa8\xe7\x9a\x84\xe6\x95\xb0\xe5\xad\x97\n        print(image_path)\n        print(\'        -> Predict digit\', np.argmax(y[0]))\n\n\nif __name__ == ""__main__"":\n    app = Predict()\n    app.predict(\'../test_images/0.png\')\n    app.predict(\'../test_images/1.png\')\n    app.predict(\'../test_images/4.png\')\n'"
mnist/v2/train.py,5,"b'import tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nfrom model import Network\n\n\'\'\'\npython 3.6\ntensorflow 1.4\n\n\xe9\x87\x8d\xe7\x82\xb9\xe5\xaf\xb9\xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe9\x83\xa8\xe5\x88\x86\xe6\xb7\xbb\xe5\x8a\xa0\xe4\xba\x86\xe6\xb3\xa8\xe9\x87\x8a\n\xe5\xa6\x82\xe6\x9e\x9c\xe6\x83\xb3\xe7\x9c\x8b\xe5\x85\xb6\xe4\xbb\x96\xe4\xbb\xa3\xe7\xa0\x81\xe7\x9a\x84\xe6\xb3\xa8\xe9\x87\x8a\xef\xbc\x8c\xe8\xaf\xb7\xe7\xa7\xbb\xe6\xad\xa5 v1\nv2 \xe7\x89\x88\xe6\x9c\xac\xe6\xaf\x94 v1 \xe7\x89\x88\xe6\x9c\xac\xe5\xa2\x9e\xe5\x8a\xa0\xe4\xba\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe4\xbf\x9d\xe5\xad\x98\xe5\x92\x8c\xe7\xbb\xa7\xe7\xbb\xad\xe8\xae\xad\xe7\xbb\x83\n\'\'\'\n\nCKPT_DIR = \'ckpt\'\n\n\nclass Train:\n    def __init__(self):\n        self.net = Network()\n        self.sess = tf.Session()\n        self.sess.run(tf.global_variables_initializer())\n        self.data = input_data.read_data_sets(\'../data_set\', one_hot=True)\n\n    def train(self):\n        batch_size = 64\n        train_step = 10000\n        # \xe8\xae\xb0\xe5\xbd\x95\xe8\xae\xad\xe7\xbb\x83\xe6\xac\xa1\xe6\x95\xb0, \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xb8\xba0\n        step = 0\n\n        # \xe6\xaf\x8f\xe9\x9a\x941000\xe6\xad\xa5\xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\n        save_interval = 1000\n\n        # tf.train.Saver\xe6\x98\xaf\xe7\x94\xa8\xe6\x9d\xa5\xe4\xbf\x9d\xe5\xad\x98\xe8\xae\xad\xe7\xbb\x83\xe7\xbb\x93\xe6\x9e\x9c\xe7\x9a\x84\xe3\x80\x82\n        # max_to_keep \xe7\x94\xa8\xe6\x9d\xa5\xe8\xae\xbe\xe7\xbd\xae\xe6\x9c\x80\xe5\xa4\x9a\xe4\xbf\x9d\xe5\xad\x98\xe5\xa4\x9a\xe5\xb0\x91\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe9\xbb\x98\xe8\xae\xa4\xe6\x98\xaf5\n        # \xe5\xa6\x82\xe6\x9e\x9c\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xb6\x85\xe8\xbf\x87\xe8\xbf\x99\xe4\xb8\xaa\xe5\x80\xbc\xef\xbc\x8c\xe6\x9c\x80\xe6\x97\xa7\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe5\xb0\x86\xe8\xa2\xab\xe5\x88\xa0\xe9\x99\xa4\n        saver = tf.train.Saver(max_to_keep=10)\n\n        ckpt = tf.train.get_checkpoint_state(CKPT_DIR)\n        if ckpt and ckpt.model_checkpoint_path:\n            saver.restore(self.sess, ckpt.model_checkpoint_path)\n            # \xe8\xaf\xbb\xe5\x8f\x96\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xe7\x9a\x84global_step\xe7\x9a\x84\xe5\x80\xbc\xef\xbc\x8c\xe5\x8d\xb3\xe5\xbd\x93\xe5\x89\x8d\xe5\xb7\xb2\xe7\xbb\x8f\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\xac\xa1\xe6\x95\xb0\n            step = self.sess.run(self.net.global_step)\n            print(\'Continue from\')\n            print(\'        -> Minibatch update : \', step)\n\n        while step < train_step:\n            x, label = self.data.train.next_batch(batch_size)\n            _, loss = self.sess.run([self.net.train, self.net.loss],\n                                    feed_dict={self.net.x: x, self.net.label: label})\n            step = self.sess.run(self.net.global_step)\n            if step % 1000 == 0:\n                print(\'\xe7\xac\xac%5d\xe6\xad\xa5\xef\xbc\x8c\xe5\xbd\x93\xe5\x89\x8dloss\xef\xbc\x9a%.2f\' % (step, loss))\n\n            # \xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbf\x9d\xe5\xad\x98\xe5\x9c\xa8ckpt\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xe4\xb8\x8b\n            # \xe6\xa8\xa1\xe5\x9e\x8b\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe6\x9c\x80\xe5\x90\x8e\xe4\xbc\x9a\xe5\xa2\x9e\xe5\x8a\xa0global_step\xe7\x9a\x84\xe5\x80\xbc\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x821000\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe4\xb8\xba model-1000\n            if step % save_interval == 0:\n                saver.save(self.sess, CKPT_DIR + \'/model\', global_step=step)\n\n    def calculate_accuracy(self):\n        test_x = self.data.test.images\n        test_label = self.data.test.labels\n        accuracy = self.sess.run(self.net.accuracy,\n                                 feed_dict={self.net.x: test_x, self.net.label: test_label})\n        print(""\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87: %.2f\xef\xbc\x8c\xe5\x85\xb1\xe6\xb5\x8b\xe8\xaf\x95\xe4\xba\x86%d\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87 "" % (accuracy, len(test_label)))\n\n\nif __name__ == ""__main__"":\n    app = Train()\n    app.train()\n    app.calculate_accuracy()\n'"
mnist/v3/model.py,14,"b'import tensorflow as tf\n\n\nclass Network:\n    def __init__(self):\n        self.learning_rate = 0.001\n        self.global_step = tf.Variable(0, trainable=False, name=""global_step"")\n\n        self.x = tf.placeholder(tf.float32, [None, 784], name=""x"")\n        self.label = tf.placeholder(tf.float32, [None, 10], name=""label"")\n\n        self.w = tf.Variable(tf.zeros([784, 10]), name=""fc/weight"")\n        self.b = tf.Variable(tf.zeros([10]), name=""fc/bias"")\n        self.y = tf.nn.softmax(tf.matmul(self.x, self.w) + self.b, name=""y"")\n\n        self.loss = -tf.reduce_sum(self.label * tf.log(self.y + 1e-10))\n        self.train = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(\n            self.loss, global_step=self.global_step)\n\n        predict = tf.equal(tf.argmax(self.label, 1), tf.argmax(self.y, 1))\n        self.accuracy = tf.reduce_mean(tf.cast(predict, ""float""))\n\n        # \xe5\x88\x9b\xe5\xbb\xba summary node\n        # w, b \xe7\x94\xbb\xe7\x9b\xb4\xe6\x96\xb9\xe5\x9b\xbe\n        # loss, accuracy\xe7\x94\xbb\xe6\xa0\x87\xe9\x87\x8f\xe5\x9b\xbe\n        tf.summary.histogram(\'weight\', self.w)\n        tf.summary.histogram(\'bias\', self.b)\n        tf.summary.scalar(\'loss\', self.loss)\n        tf.summary.scalar(\'accuracy\', self.accuracy)\n'"
mnist/v3/predict.py,4,"b'import tensorflow as tf\nimport numpy as np\nfrom PIL import Image\n\nfrom model import Network\n\n\'\'\'\npython 3.6\ntensorflow 1.4\npillow(PIL) 4.3.0\n\xe4\xbd\xbf\xe7\x94\xa8tensorflow\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x9d\xa5\xe9\xa2\x84\xe6\xb5\x8b\xe6\x89\x8b\xe5\x86\x99\xe6\x95\xb0\xe5\xad\x97\n\xe8\xbe\x93\xe5\x85\xa5\xe6\x98\xaf28 * 28\xe5\x83\x8f\xe7\xb4\xa0\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe6\x98\xaf\xe4\xb8\xaa\xe5\x85\xb7\xe4\xbd\x93\xe7\x9a\x84\xe6\x95\xb0\xe5\xad\x97\n\'\'\'\n\nCKPT_DIR = \'ckpt\'\n\n\nclass Predict:\n    def __init__(self):\n        self.net = Network()\n        self.sess = tf.Session()\n        self.sess.run(tf.global_variables_initializer())\n\n        # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x88\xb0sess\xe4\xb8\xad\n        self.restore()\n\n    def restore(self):\n        saver = tf.train.Saver()\n        ckpt = tf.train.get_checkpoint_state(CKPT_DIR)\n        if ckpt and ckpt.model_checkpoint_path:\n            saver.restore(self.sess, ckpt.model_checkpoint_path)\n        else:\n            raise FileNotFoundError(""\xe6\x9c\xaa\xe4\xbf\x9d\xe5\xad\x98\xe4\xbb\xbb\xe4\xbd\x95\xe6\xa8\xa1\xe5\x9e\x8b"")\n\n    def predict(self, image_path):\n        # \xe8\xaf\xbb\xe5\x9b\xbe\xe7\x89\x87\xe5\xb9\xb6\xe8\xbd\xac\xe4\xb8\xba\xe9\xbb\x91\xe7\x99\xbd\xe7\x9a\x84\n        img = Image.open(image_path).convert(\'L\')\n        flatten_img = np.reshape(img, 784)\n        x = np.array([1 - flatten_img])\n        y = self.sess.run(self.net.y, feed_dict={self.net.x: x})\n\n        # \xe5\x9b\xa0\xe4\xb8\xbax\xe5\x8f\xaa\xe4\xbc\xa0\xe5\x85\xa5\xe4\xba\x86\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe5\x8f\x96y[0]\xe5\x8d\xb3\xe5\x8f\xaf\n        # np.argmax()\xe5\x8f\x96\xe5\xbe\x97\xe7\x8b\xac\xe7\x83\xad\xe7\xbc\x96\xe7\xa0\x81\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\xe7\x9a\x84\xe4\xb8\x8b\xe6\xa0\x87\xef\xbc\x8c\xe5\x8d\xb3\xe4\xbb\xa3\xe8\xa1\xa8\xe7\x9a\x84\xe6\x95\xb0\xe5\xad\x97\n        print(image_path)\n        print(\'        -> Predict digit\', np.argmax(y[0]))\n\n\nif __name__ == ""__main__"":\n    app = Predict()\n    app.predict(\'../test_images/0.png\')\n    app.predict(\'../test_images/1.png\')\n    app.predict(\'../test_images/4.png\')\n'"
mnist/v3/train.py,6,"b'import tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nfrom model import Network\n\n\'\'\'\npython 3.6\ntensorflow 1.4\n\n\xe9\x87\x8d\xe7\x82\xb9\xe5\xaf\xb9\xe8\xae\xad\xe7\xbb\x83\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe7\x9a\x84\xe9\x83\xa8\xe5\x88\x86\xe6\xb7\xbb\xe5\x8a\xa0\xe4\xba\x86\xe6\xb3\xa8\xe9\x87\x8a\n\xe5\xa6\x82\xe6\x9e\x9c\xe6\x83\xb3\xe7\x9c\x8b\xe5\x85\xb6\xe4\xbb\x96\xe4\xbb\xa3\xe7\xa0\x81\xe7\x9a\x84\xe6\xb3\xa8\xe9\x87\x8a\xef\xbc\x8c\xe8\xaf\xb7\xe7\xa7\xbb\xe6\xad\xa5 v1, v2\nv3\xe6\xaf\x94v2\xe5\xa2\x9e\xe5\x8a\xa0\xe4\xba\x86loss\xe5\x92\x8caccuracy\xe7\x9a\x84\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n\'\'\'\n\nCKPT_DIR = \'ckpt\'\n\n\nclass Train:\n    def __init__(self):\n        self.net = Network()\n        self.sess = tf.Session()\n        self.sess.run(tf.global_variables_initializer())\n        self.data = input_data.read_data_sets(\'../data_set\', one_hot=True)\n\n    def train(self):\n        batch_size = 64\n        train_step = 20000\n        step = 0\n        save_interval = 1000\n        saver = tf.train.Saver(max_to_keep=5)\n\n        # merge\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84summary node\n        merged_summary_op = tf.summary.merge_all()\n        # \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe5\xad\x98\xe5\x82\xa8\xe7\x9b\xae\xe5\xbd\x95\xe4\xb8\xba\xe5\xbd\x93\xe5\x89\x8d\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xe4\xb8\x8b\xe7\x9a\x84 log\n        merged_writer = tf.summary.FileWriter(""./log"", self.sess.graph)\n\n        ckpt = tf.train.get_checkpoint_state(CKPT_DIR)\n        if ckpt and ckpt.model_checkpoint_path:\n            saver.restore(self.sess, ckpt.model_checkpoint_path)\n            # \xe8\xaf\xbb\xe5\x8f\x96\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xe7\x9a\x84global_step\xe7\x9a\x84\xe5\x80\xbc\xef\xbc\x8c\xe5\x8d\xb3\xe5\xbd\x93\xe5\x89\x8d\xe5\xb7\xb2\xe7\xbb\x8f\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\xac\xa1\xe6\x95\xb0\n            step = self.sess.run(self.net.global_step)\n            print(\'Continue from\')\n            print(\'        -> Minibatch update : \', step)\n\n        while step < train_step:\n            x, label = self.data.train.next_batch(batch_size)\n            _, loss, merged_summary = self.sess.run(\n                [self.net.train, self.net.loss, merged_summary_op],\n                feed_dict={self.net.x: x, self.net.label: label}\n            )\n            step = self.sess.run(self.net.global_step)\n\n            if step % 100 == 0:\n                merged_writer.add_summary(merged_summary, step)\n\n            if step % save_interval == 0:\n                saver.save(self.sess, CKPT_DIR + \'/model\', global_step=step)\n                print(\'%s/model-%d saved\' % (CKPT_DIR, step))\n\n    def calculate_accuracy(self):\n        test_x = self.data.test.images\n        test_label = self.data.test.labels\n        accuracy = self.sess.run(self.net.accuracy,\n                                 feed_dict={self.net.x: test_x, self.net.label: test_label})\n        print(""\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87: %.2f\xef\xbc\x8c\xe5\x85\xb1\xe6\xb5\x8b\xe8\xaf\x95\xe4\xba\x86%d\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87 "" % (accuracy, len(test_label)))\n\n\nif __name__ == ""__main__"":\n    app = Train()\n    app.train()\n    app.calculate_accuracy()\n'"
mnist/v4_cnn/predict.py,1,"b'import tensorflow as tf\nfrom PIL import Image\nimport numpy as np\n\nfrom train import CNN\n\n\'\'\'\npython 3.7\ntensorflow 2.0.0b0\npillow(PIL) 4.3.0\n\'\'\'\n\n\nclass Predict(object):\n    def __init__(self):\n        latest = tf.train.latest_checkpoint(\'./ckpt\')\n        self.cnn = CNN()\n        # \xe6\x81\xa2\xe5\xa4\x8d\xe7\xbd\x91\xe7\xbb\x9c\xe6\x9d\x83\xe9\x87\x8d\n        self.cnn.model.load_weights(latest)\n\n    def predict(self, image_path):\n        # \xe4\xbb\xa5\xe9\xbb\x91\xe7\x99\xbd\xe6\x96\xb9\xe5\xbc\x8f\xe8\xaf\xbb\xe5\x8f\x96\xe5\x9b\xbe\xe7\x89\x87\n        img = Image.open(image_path).convert(\'L\')\n        flatten_img = np.reshape(img, (28, 28, 1))\n        x = np.array([1 - flatten_img])\n\n        # API refer: https://keras.io/models/model/\n        y = self.cnn.model.predict(x)\n\n        # \xe5\x9b\xa0\xe4\xb8\xbax\xe5\x8f\xaa\xe4\xbc\xa0\xe5\x85\xa5\xe4\xba\x86\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe5\x8f\x96y[0]\xe5\x8d\xb3\xe5\x8f\xaf\n        # np.argmax()\xe5\x8f\x96\xe5\xbe\x97\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\xe7\x9a\x84\xe4\xb8\x8b\xe6\xa0\x87\xef\xbc\x8c\xe5\x8d\xb3\xe4\xbb\xa3\xe8\xa1\xa8\xe7\x9a\x84\xe6\x95\xb0\xe5\xad\x97\n        print(image_path)\n        print(y[0])\n        print(\'        -> Predict digit\', np.argmax(y[0]))\n\n\nif __name__ == ""__main__"":\n    app = Predict()\n    app.predict(\'../test_images/0.png\')\n    app.predict(\'../test_images/1.png\')\n    app.predict(\'../test_images/4.png\')\n'"
mnist/v4_cnn/train.py,1,"b'import os\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\n\n\'\'\'\npython 3.7\ntensorflow 2.0.0b0\n\'\'\'\n\n\nclass CNN(object):\n    def __init__(self):\n        model = models.Sequential()\n        # \xe7\xac\xac1\xe5\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x8c\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xb8\xba3*3\xef\xbc\x8c32\xe4\xb8\xaa\xef\xbc\x8c28*28\xe4\xb8\xba\xe5\xbe\x85\xe8\xae\xad\xe7\xbb\x83\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\n        model.add(layers.Conv2D(\n            32, (3, 3), activation=\'relu\', input_shape=(28, 28, 1)))\n        model.add(layers.MaxPooling2D((2, 2)))\n        # \xe7\xac\xac2\xe5\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x8c\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xb8\xba3*3\xef\xbc\x8c64\xe4\xb8\xaa\n        model.add(layers.Conv2D(64, (3, 3), activation=\'relu\'))\n        model.add(layers.MaxPooling2D((2, 2)))\n        # \xe7\xac\xac3\xe5\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x8c\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xb8\xba3*3\xef\xbc\x8c64\xe4\xb8\xaa\n        model.add(layers.Conv2D(64, (3, 3), activation=\'relu\'))\n\n        model.add(layers.Flatten())\n        model.add(layers.Dense(64, activation=\'relu\'))\n        model.add(layers.Dense(10, activation=\'softmax\'))\n\n        model.summary()\n\n        self.model = model\n\n\nclass DataSource(object):\n    def __init__(self):\n        # mnist\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\xad\x98\xe5\x82\xa8\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\xef\xbc\x8c\xe5\xa6\x82\xe4\xbd\x95\xe4\xb8\x8d\xe5\xad\x98\xe5\x9c\xa8\xe5\xb0\x86\xe8\x87\xaa\xe5\x8a\xa8\xe4\xb8\x8b\xe8\xbd\xbd\n        data_path = os.path.abspath(os.path.dirname(\n            __file__)) + \'/../data_set_tf2/mnist.npz\'\n        (train_images, train_labels), (test_images,\n                                       test_labels) = datasets.mnist.load_data(path=data_path)\n        # 6\xe4\xb8\x87\xe5\xbc\xa0\xe8\xae\xad\xe7\xbb\x83\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c1\xe4\xb8\x87\xe5\xbc\xa0\xe6\xb5\x8b\xe8\xaf\x95\xe5\x9b\xbe\xe7\x89\x87\n        train_images = train_images.reshape((60000, 28, 28, 1))\n        test_images = test_images.reshape((10000, 28, 28, 1))\n        # \xe5\x83\x8f\xe7\xb4\xa0\xe5\x80\xbc\xe6\x98\xa0\xe5\xb0\x84\xe5\x88\xb0 0 - 1 \xe4\xb9\x8b\xe9\x97\xb4\n        train_images, test_images = train_images / 255.0, test_images / 255.0\n\n        self.train_images, self.train_labels = train_images, train_labels\n        self.test_images, self.test_labels = test_images, test_labels\n\nclass Train:\n    def __init__(self):\n        self.cnn = CNN()\n        self.data = DataSource()\n\n    def train(self):\n        check_path = \'./ckpt/cp-{epoch:04d}.ckpt\'\n        # period \xe6\xaf\x8f\xe9\x9a\x945epoch\xe4\xbf\x9d\xe5\xad\x98\xe4\xb8\x80\xe6\xac\xa1\n        save_model_cb = tf.keras.callbacks.ModelCheckpoint(\n            check_path, save_weights_only=True, verbose=1, period=5)\n\n        self.cnn.model.compile(optimizer=\'adam\',\n                               loss=\'sparse_categorical_crossentropy\',\n                               metrics=[\'accuracy\'])\n        self.cnn.model.fit(self.data.train_images, self.data.train_labels,\n                           epochs=5, callbacks=[save_model_cb])\n\n        test_loss, test_acc = self.cnn.model.evaluate(\n            self.data.test_images, self.data.test_labels)\n        print(""\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87: %.4f\xef\xbc\x8c\xe5\x85\xb1\xe6\xb5\x8b\xe8\xaf\x95\xe4\xba\x86%d\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87 "" % (test_acc, len(self.data.test_labels)))\n\n\nif __name__ == ""__main__"":\n    app = Train()\n    app.train()\n'"
