file_path,api_count,code
setup.py,0,"b'import os\n\nfrom setuptools import find_packages, setup\n\n\ndef read(fname):\n    with open(os.path.join(os.path.dirname(__file__), fname)) as opened:\n        return opened.read()\n\n\ndef main():\n    setup(\n        name=\'tffm\',\n        version=\'1.0.1\',\n        author=""Mikhail Trofimov"",\n        author_email=""mikhail.trofimov@phystech.edu"",\n        url=\'https://github.com/geffy/tffm\',\n        download_url=\'https://github.com/geffy/tffm/archive/1.0.1.tar.gz\',\n        description=(\'TensforFlow implementation of arbitrary order \'\n                     \'Factorization Machine\'),\n        classifiers=[\n            \'Development Status :: 3 - Alpha\',\n            \'Intended Audience :: Science/Research\',\n            \'Topic :: Scientific/Engineering\',\n            \'License :: OSI Approved :: MIT License\',\n            \'Programming Language :: Python :: 2.7\',\n            \'Programming Language :: Python :: 3\',\n        ],\n        license=\'MIT\',\n        install_requires=[\n            \'scikit-learn\',\n            \'numpy\',\n            \'tqdm\'\n        ],\n        packages=find_packages()\n    )\n\n\nif __name__ == ""__main__"":\n    main()\n'"
test.py,1,"b""import unittest\nimport numpy as np\nfrom tffm import TFFMClassifier\nfrom scipy import sparse as sp\nimport tensorflow as tf\nimport pickle\n\n\nclass TestFM(unittest.TestCase):\n\n    def setUp(self):\n        # Reproducibility.\n        np.random.seed(0)\n\n        n_samples = 20\n        n_features = 10\n\n        self.X = np.random.randn(n_samples, n_features)\n        self.y = np.random.binomial(1, 0.5, size=n_samples)\n\n    def decision_function_order_4(self, input_type, use_diag=False):\n        # Explanation for init_std=1.0.\n        # With small init_std the contribution of higher order terms is\n        # neglectable, so we would essentially test only low-order implementation.\n        # That's why a relatively high init_std=1.0 here.\n        model = TFFMClassifier(\n            order=4,\n            rank=10,\n            optimizer=tf.train.AdamOptimizer(learning_rate=0.1),\n            n_epochs=0,\n            input_type=input_type,\n            init_std=1.0,\n            seed=0,\n            use_diag=use_diag\n        )\n\n        if input_type == 'dense':\n            X = self.X\n        else:\n            X = sp.csr_matrix(self.X)\n\n        model.fit(X, self.y)\n        b = model.intercept\n        w = model.weights\n\n        desired = self.bruteforce_inference(self.X, w, b, use_diag=use_diag)\n\n        actual = model.decision_function(X)\n        model.destroy()\n\n        np.testing.assert_almost_equal(actual, desired, decimal=4)\n\n    def test_dense_FM(self):\n        self.decision_function_order_4(input_type='dense', use_diag=False)\n\n    def test_dense_PN(self):\n        self.decision_function_order_4(input_type='dense', use_diag=True)\n\n    def test_sparse_FM(self):\n        self.decision_function_order_4(input_type='sparse', use_diag=False)\n\n    def test_sparse_PN(self):\n        self.decision_function_order_4(input_type='sparse', use_diag=True)\n\n\n    def bruteforce_inference_one_interaction(self, X, w, order, use_diag):\n        n_obj, n_feat = X.shape\n        ans = np.zeros(n_obj)\n        if order == 2:\n            for i in range(n_feat):\n                for j in range(0 if use_diag else i+1, n_feat):\n                    x_prod = X[:, i] * X[:, j]\n                    w_prod = np.sum(w[1][i, :] * w[1][j, :])\n                    denominator = 2.0**(order-1) if use_diag else 1.0\n                    ans += x_prod * w_prod / denominator\n        elif order == 3:\n            for i in range(n_feat):\n                for j in range(0 if use_diag else i+1, n_feat):\n                    for k in range(0 if use_diag else j+1, n_feat):\n                        x_prod = X[:, i] * X[:, j] * X[:, k]\n                        w_prod = np.sum(w[2][i, :] * w[2][j, :] * w[2][k, :])\n                        denominator = 2.0**(order-1) if use_diag else 1.0\n                        ans += x_prod * w_prod / denominator\n        elif order == 4:\n            for i in range(n_feat):\n                for j in range(0 if use_diag else i+1, n_feat):\n                    for k in range(0 if use_diag else j+1, n_feat):\n                        for ell in range(0 if use_diag else k+1, n_feat):\n                            x_prod = X[:, i] * X[:, j] * X[:, k] * X[:, ell]\n                            w_prod = np.sum(w[3][i, :] * w[3][j, :] * w[3][k, :] * w[3][ell, :])\n                            denominator = 2.0**(order-1) if use_diag else 1.0\n                            ans += x_prod * w_prod / denominator\n        else:\n            assert False\n        return ans\n\n    def bruteforce_inference(self, X, w, b, use_diag):\n        assert len(w) <= 4\n        ans = X.dot(w[0]).flatten() + b\n        if len(w) > 1:\n            ans += self.bruteforce_inference_one_interaction(X, w, 2, use_diag)\n        if len(w) > 2:\n            ans += self.bruteforce_inference_one_interaction(X, w, 3, use_diag)\n        if len(w) > 3:\n            ans += self.bruteforce_inference_one_interaction(X, w, 4, use_diag)\n        return ans\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tffm/__init__.py,0,"b""from .models import TFFMClassifier, TFFMRegressor\n\n__all__ = ['TFFMClassifier', 'TFFMRegressor']\n"""
tffm/base.py,9,"b'import tensorflow as tf\nfrom .core import TFFMCore\nfrom sklearn.base import BaseEstimator\nfrom abc import ABCMeta, abstractmethod\nimport six\nfrom tqdm import tqdm\nimport numpy as np\nimport os\n\n\ndef batcher(X_, y_=None, w_=None, batch_size=-1):\n    """"""Split data to mini-batches.\n\n    Parameters\n    ----------\n    X_ : {numpy.array, scipy.sparse.csr_matrix}, shape (n_samples, n_features)\n        Training vector, where n_samples in the number of samples and\n        n_features is the number of features.\n\n    y_ : np.array or None, shape (n_samples,)\n        Target vector relative to X.\n\n    w_ : np.array or None, shape (n_samples,)\n        Vector of sample weights.\n\n    batch_size : int\n        Size of batches.\n        Use -1 for full-size batches\n\n    Yields\n    -------\n    ret_x : {numpy.array, scipy.sparse.csr_matrix}, shape (batch_size, n_features)\n        Same type as input\n\n    ret_y : np.array or None, shape (batch_size,)\n\n    ret_w : np.array or None, shape (batch_size,)\n    """"""\n    n_samples = X_.shape[0]\n\n    if batch_size == -1:\n        batch_size = n_samples\n    if batch_size < 1:\n       raise ValueError(\'Parameter batch_size={} is unsupported\'.format(batch_size))\n\n    for i in range(0, n_samples, batch_size):\n        upper_bound = min(i + batch_size, n_samples)\n        ret_x = X_[i:upper_bound]\n        ret_y = None\n        ret_w = None\n        if y_ is not None:\n            ret_y = y_[i:i + batch_size]\n        if w_ is not None:\n            ret_w = w_[i:i + batch_size]\n        yield (ret_x, ret_y, ret_w)\n\n\ndef batch_to_feeddict(X, y, w, core):\n    """"""Prepare feed dict for session.run() from mini-batch.\n    Convert sparse format into tuple (indices, values, shape) for tf.SparseTensor\n    Parameters\n    ----------\n    X : {numpy.array, scipy.sparse.csr_matrix}, shape (batch_size, n_features)\n        Training vector, where batch_size in the number of samples and\n        n_features is the number of features.\n    y : np.array, shape (batch_size,)\n        Target vector relative to X.\n    core : TFFMCore\n        Core used for extract appropriate placeholders\n    Returns\n    -------\n    fd : dict\n        Dict with formatted placeholders\n    """"""\n    fd = {}\n    if core.input_type == \'dense\':\n        fd[core.train_x] = X.astype(np.float32)\n    else:\n        # sparse case\n        X_sparse = X.tocoo()\n        fd[core.raw_indices] = np.hstack(\n            (X_sparse.row[:, np.newaxis], X_sparse.col[:, np.newaxis])\n        ).astype(np.int64)\n        fd[core.raw_values] = X_sparse.data.astype(np.float32)\n        fd[core.raw_shape] = np.array(X_sparse.shape).astype(np.int64)\n\n    if y is not None:\n        fd[core.train_y] = y.astype(np.float32)\n    if w is not None:\n        fd[core.train_w] = w.astype(np.float32)\n\n    return fd\n\n\nclass TFFMBaseModel(six.with_metaclass(ABCMeta, BaseEstimator)):\n    """"""Base class for FM.\n    This class implements L2-regularized arbitrary order FM model.\n\n    It supports arbitrary order of interactions and has linear complexity in the\n    number of features (a generalization of the approach described in Lemma 3.1\n    in the referenced paper, details will be added soon).\n\n    It can handle both dense and sparse input. Only numpy.array and CSR matrix are\n    allowed as inputs; any other input format should be explicitly converted.\n\n    Support logging/visualization with TensorBoard.\n\n\n    Parameters (for initialization)\n    ----------\n    batch_size : int, default: -1\n        Number of samples in mini-batches. Shuffled every epoch.\n        Use -1 for full gradient (whole training set in each batch).\n\n    n_epoch : int, default: 100\n        Default number of epoches.\n        It can be overrived by explicitly provided value in fit() method.\n\n    log_dir : str or None, default: None\n        Path for storing model stats during training. Used only if is not None.\n        WARNING: If such directory already exists, it will be removed!\n        You can use TensorBoard to visualize the stats:\n        `tensorboard --logdir={log_dir}`\n\n    session_config : tf.ConfigProto or None, default: None\n        Additional setting passed to tf.Session object.\n        Useful for CPU/GPU switching, setting number of threads and so on,\n        `tf.ConfigProto(device_count = {\'GPU\': 0})` will disable GPU (if enabled)\n\n    verbose : int, default: 0\n        Level of verbosity.\n        Set 1 for tensorboard info only and 2 for additional stats every epoch.\n\n    kwargs : dict, default: {}\n        Arguments for TFFMCore constructor.\n        See TFFMCore\'s doc for details.\n\n    Attributes\n    ----------\n    core : TFFMCore or None\n        Computational graph with internal utils.\n        Will be initialized during first call .fit()\n\n    session : tf.Session or None\n        Current execution session or None.\n        Should be explicitly terminated via calling destroy() method.\n\n    steps : int\n        Counter of passed lerning epochs, used as step number for writing stats\n\n    n_features : int\n        Number of features used in this dataset.\n        Inferred during the first call of fit() method.\n\n    intercept : float, shape: [1]\n        Intercept (bias) term.\n\n    weights : array of np.array, shape: [order]\n        Array of underlying representations.\n        First element will have shape [n_features, 1],\n        all the others -- [n_features, rank].\n\n    Notes\n    -----\n    You should explicitly call destroy() method to release resources.\n    See TFFMCore\'s doc for details.\n    """"""\n\n\n    def init_basemodel(self, n_epochs=100, batch_size=-1,\n                       log_dir=None,session_config=None,\n                       verbose=0, seed=None,sample_weight=None,\n                       pos_class_weight=None,**core_arguments):\n        core_arguments[\'seed\'] = seed\n        self.core = TFFMCore(**core_arguments)\n        self.batch_size = batch_size\n        self.n_epochs = n_epochs\n        self.need_logs = log_dir is not None\n        self.log_dir = log_dir\n        self.session_config = session_config\n        self.verbose = verbose\n        self.steps = 0\n        self.seed = seed\n        self.sample_weight = sample_weight\n        self.pos_class_weight = pos_class_weight\n\n    def initialize_session(self):\n        """"""Start computational session on builded graph.\n        Initialize summary logger (if needed).\n        """"""\n        if self.core.graph is None:\n            raise \'Graph not found. Try call .core.build_graph() before .initialize_session()\'\n        if self.need_logs:\n            self.summary_writer = tf.summary.FileWriter(self.log_dir, self.core.graph)\n            if self.verbose > 0:\n                full_log_path = os.path.abspath(self.log_dir)\n                print(\'Initialize logs, use: \\ntensorboard --logdir={}\'.format(full_log_path))\n        self.session = tf.Session(config=self.session_config, graph=self.core.graph)\n        self.session.run(self.core.init_all_vars)\n\n\n    def _fit(self, X_, y_, w_, n_epochs=None, show_progress=False):\n        if self.core.n_features is None:\n            self.core.set_num_features(X_.shape[1])\n\n        assert self.core.n_features==X_.shape[1], \'Different num of features in initialized graph and input\'\n\n        if self.core.graph is None:\n            self.core.build_graph()\n            self.initialize_session()\n\n        if n_epochs is None:\n            n_epochs = self.n_epochs\n\n        # For reproducible results\n        if self.seed:\n            np.random.seed(self.seed)\n        \n        # Training cycle\n        for epoch in tqdm(range(n_epochs), unit=\'epoch\', disable=(not show_progress)):\n            # generate permutation\n            perm = np.random.permutation(X_.shape[0])\n            epoch_loss = []\n            # iterate over batches\n            for bX, bY, bW in batcher(X_[perm], y_=y_[perm], w_=w_[perm], batch_size=self.batch_size):\n                fd = batch_to_feeddict(bX, bY, bW, core=self.core)\n                ops_to_run = [self.core.trainer, self.core.target, self.core.summary_op]\n                result = self.session.run(ops_to_run, feed_dict=fd)\n                _, batch_target_value, summary_str = result\n                epoch_loss.append(batch_target_value)\n                # write stats \n                if self.need_logs:\n                    self.summary_writer.add_summary(summary_str, self.steps)\n                    self.summary_writer.flush()\n                self.steps += 1\n            if self.verbose > 1:\n                    print(\'[epoch {}]: mean target value: {}\'.format(epoch, np.mean(epoch_loss)))\n\n    def decision_function(self, X, pred_batch_size=None):\n        if self.core.graph is None:\n            raise sklearn.exceptions.NotFittedError(""Call fit before prediction"")\n        output = []\n        if pred_batch_size is None:\n            pred_batch_size = self.batch_size\n\n        for bX, bY, bW in batcher(X, y_=None, w_=None, batch_size=pred_batch_size):\n            fd = batch_to_feeddict(bX, bY, bW, core=self.core)\n            output.append(self.session.run(self.core.outputs, feed_dict=fd))\n        distances = np.concatenate(output).reshape(-1)\n        # WARNING: be careful with this reshape in case of multiclass\n        return distances\n\n    @abstractmethod\n    def predict(self, X, pred_batch_size=None):\n        """"""Predict target values for X.""""""\n\n    @property\n    def intercept(self):\n        """"""Export bias term from tf.Variable to float.""""""\n        return self.core.b.eval(session=self.session)\n\n    @property\n    def weights(self):\n        """"""Export underlying weights from tf.Variables to np.arrays.""""""\n        return [x.eval(session=self.session) for x in self.core.w]\n\n    def save_state(self, path):\n        self.core.saver.save(self.session, path)\n\n    def load_state(self, path):\n        if self.core.graph is None:\n            self.core.build_graph()\n            self.initialize_session()\n        self.core.saver.restore(self.session, path)\n\n    def destroy(self):\n        """"""Terminates session and destroyes graph.""""""\n        self.session.close()\n        self.core.graph = None\n'"
tffm/core.py,58,"b'import tensorflow as tf\nfrom . import utils\nimport math\n\n\nclass TFFMCore():\n    """"""This class implements underlying routines about creating computational graph.\n\n    Its required `n_features` to be set at graph building time.\n\n\n    Parameters\n    ----------\n    order : int, default: 2\n        Order of corresponding polynomial model.\n        All interaction from bias and linear to order will be included.\n\n    rank : int, default: 5\n        Number of factors in low-rank appoximation.\n        This value is shared across different orders of interaction.\n\n    input_type : str, \'dense\' or \'sparse\', default: \'dense\'\n        Type of input data. Only numpy.array allowed for \'dense\' and\n        scipy.sparse.csr_matrix for \'sparse\'. This affects construction of\n        computational graph and cannot be changed during training/testing.\n\n    loss_function : function: (tf.Op, tf.Op) -> tf.Op, default: None\n        Loss function.\n        Take 2 tf.Ops: outputs and targets and should return tf.Op of loss\n        See examples: .utils.loss_mse, .utils.loss_logistic\n\n    optimizer : tf.train.Optimizer, default: Adam(learning_rate=0.01)\n        Optimization method used for training\n\n    reg : float, default: 0\n        Strength of L2 regularization\n\n    use_diag : bool, default: False\n        Use diagonal elements of weights matrix or not.\n        In the other words, should terms like x^2 be included.\n        Ofter reffered as a ""Polynomial Network"".\n        Default value (False) corresponds to FM.\n\n    reweight_reg : bool, default: False\n        Use frequency of features as weights for regularization or not.\n        Should be usefull for very sparse data and/or small batches\n\n    init_std : float, default: 0.01\n        Amplitude of random initialization\n\n    seed : int or None, default: None\n        Random seed used at graph creating time\n\n\n    Attributes\n    ----------\n    graph : tf.Graph or None\n        Initialized computational graph or None\n\n    trainer : tf.Op\n        TensorFlow operation node to perform learning on single batch\n\n    n_features : int\n        Number of features used in this dataset.\n        Inferred during the first call of fit() method.\n\n    saver : tf.Op\n        tf.train.Saver instance, connected to graph\n\n    summary_op : tf.Op\n        tf.merge_all_summaries instance for export logging\n\n    b : tf.Variable, shape: [1]\n        Bias term.\n\n    w : array of tf.Variable, shape: [order]\n        Array of underlying representations.\n        First element will have shape [n_features, 1],\n        all the others -- [n_features, rank].\n\n    Notes\n    -----\n    Parameter `rank` is shared across all orders of interactions (except bias and\n    linear parts).\n    tf.sparse_reorder doesn\'t requied since COO format is lexigraphical ordered.\n    This implementation uses a generalized approach from referenced paper along\n    with caching.\n\n    References\n    ----------\n    Steffen Rendle, Factorization Machines\n        http://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf\n\n    """"""\n    def __init__(self, order=2, rank=2, input_type=\'dense\', loss_function=None, \n                optimizer=tf.train.Adam(learning_rate=0.01), reg=0,\n                init_std=0.01, use_diag=False, reweight_reg=False,\n                seed=None):\n        self.order = order\n        self.rank = rank\n        self.use_diag = use_diag\n        self.input_type = input_type\n        self.optimizer = optimizer\n        self.reg = reg\n        self.reweight_reg = reweight_reg\n        self.init_std = init_std\n        self.seed = seed\n        self.n_features = None\n        self.graph = None\n        self.loss_function = loss_function\n\n\n    def set_num_features(self, n_features):\n        self.n_features = n_features\n\n    def init_learnable_params(self):\n        self.w = [None] * self.order\n        for i in range(1, self.order + 1):\n            r = self.rank\n            if i == 1:\n                r = 1\n            rnd_weights = tf.random_uniform([self.n_features, r], -self.init_std, self.init_std)\n            self.w[i - 1] = tf.verify_tensor_all_finite(\n                tf.Variable(rnd_weights, trainable=True, name=\'embedding_\' + str(i)),\n                msg=\'NaN or Inf in w[{}].\'.format(i-1))\n        self.b = tf.Variable(self.init_std, trainable=True, name=\'bias\')\n        tf.summary.scalar(\'bias\', self.b)\n\n    def init_placeholders(self):\n        if self.input_type == \'dense\':\n            self.train_x = tf.placeholder(tf.float32, shape=[None, self.n_features], name=\'x\')\n        else:\n            with tf.name_scope(\'sparse_placeholders\') as scope:\n                self.raw_indices = tf.placeholder(tf.int64, shape=[None, 2], name=\'raw_indices\')\n                self.raw_values = tf.placeholder(tf.float32, shape=[None], name=\'raw_data\')\n                self.raw_shape = tf.placeholder(tf.int64, shape=[2], name=\'raw_shape\')\n            # tf.sparse_reorder is not needed since scipy return COO in canonical order\n            self.train_x = tf.SparseTensor(self.raw_indices, self.raw_values, self.raw_shape)\n        self.train_y = tf.placeholder(tf.float32, shape=[None], name=\'Y\')\n        self.train_w = tf.placeholder(tf.float32, shape=[None], name=\'sample_weights\')\n\n    def pow_matmul(self, order, pow):\n        if pow not in self.x_pow_cache:\n            x_pow = utils.pow_wrapper(self.train_x, pow, self.input_type)\n            self.x_pow_cache[pow] = x_pow\n        if order not in self.matmul_cache:\n            self.matmul_cache[order] = {}\n        if pow not in self.matmul_cache[order]:\n            w_pow = tf.pow(self.w[order - 1], pow)\n            dot = utils.matmul_wrapper(self.x_pow_cache[pow], w_pow, self.input_type)\n            self.matmul_cache[order][pow] = dot\n        return self.matmul_cache[order][pow]\n\n    def init_main_block(self):\n        self.x_pow_cache = {}\n        self.matmul_cache = {}\n        self.outputs = self.b\n        with tf.name_scope(\'linear_part\') as scope:\n            contribution = utils.matmul_wrapper(self.train_x, self.w[0], self.input_type)\n        self.outputs += contribution\n        for i in range(2, self.order + 1):\n            with tf.name_scope(\'order_{}\'.format(i)) as scope:\n                raw_dot = utils.matmul_wrapper(self.train_x, self.w[i - 1], self.input_type)\n                dot = tf.pow(raw_dot, i)\n                if self.use_diag:\n                    contribution = tf.reshape(tf.reduce_sum(dot, [1]), [-1, 1])\n                    contribution /= 2.0**(i-1)\n                else:\n                    initialization_shape = tf.shape(dot)\n                    for in_pows, out_pows, coef in utils.powers_and_coefs(i):\n                        product_of_pows = tf.ones(initialization_shape)\n                        for pow_idx in range(len(in_pows)):\n                            pmm = self.pow_matmul(i, in_pows[pow_idx])\n                            product_of_pows *= tf.pow(pmm, out_pows[pow_idx])\n                        dot -= coef * product_of_pows\n                    contribution = tf.reshape(tf.reduce_sum(dot, [1]), [-1, 1])\n                    contribution /= float(math.factorial(i))\n            self.outputs += contribution\n\n    def init_regularization(self):\n        with tf.name_scope(\'regularization\') as scope:\n            self.regularization = 0\n            with tf.name_scope(\'reweights\') as scope:\n                if self.reweight_reg:\n                    counts = utils.count_nonzero_wrapper(self.train_x, self.input_type)\n                    sqrt_counts = tf.transpose(tf.sqrt(tf.to_float(counts)))\n                else:\n                    sqrt_counts = tf.ones_like(self.w[0])\n                self.reweights = sqrt_counts / tf.reduce_sum(sqrt_counts)\n            for order in range(1, self.order + 1):\n                node_name = \'regularization_penalty_\' + str(order)\n                norm = tf.reduce_mean(tf.pow(self.w[order - 1]*self.reweights, 2), name=node_name)\n                tf.summary.scalar(\'penalty_W_{}\'.format(order), norm)\n                self.regularization += norm\n            tf.summary.scalar(\'regularization_penalty\', self.regularization)\n\n    def init_loss(self):\n        with tf.name_scope(\'loss\') as scope:\n            self.loss = self.loss_function(self.outputs, self.train_y) * self.train_w\n            self.reduced_loss = tf.reduce_mean(self.loss)\n            tf.summary.scalar(\'loss\', self.reduced_loss)\n\n    def init_target(self):\n        with tf.name_scope(\'target\') as scope:\n            self.target = self.reduced_loss + self.reg * self.regularization\n            self.checked_target = tf.verify_tensor_all_finite(\n                self.target,\n                msg=\'NaN or Inf in target value\', \n                name=\'target\')\n            tf.summary.scalar(\'target\', self.checked_target)\n\n    def build_graph(self):\n        """"""Build computational graph according to params.""""""\n        assert self.n_features is not None, \'Number of features is unknown. It can be set explicitly by .core.set_num_features\'\n        self.graph = tf.Graph()\n        self.graph.seed = self.seed\n        with self.graph.as_default():\n            with tf.name_scope(\'learnable_params\') as scope:\n                self.init_learnable_params()\n            with tf.name_scope(\'input_block\') as scope:\n                self.init_placeholders()\n            with tf.name_scope(\'main_block\') as scope:\n                self.init_main_block()\n            with tf.name_scope(\'optimization_criterion\') as scope:\n                self.init_regularization()\n                self.init_loss()\n                self.init_target()\n            self.trainer = self.optimizer.minimize(self.checked_target)\n            self.init_all_vars = tf.global_variables_initializer()\n            self.summary_op = tf.summary.merge_all()\n            self.saver = tf.train.Saver()\n'"
tffm/models.py,0,"b'""""""Implementation of an arbitrary order Factorization Machines.""""""\n\nimport numpy as np\nfrom .base import TFFMBaseModel\nfrom .utils import loss_logistic, loss_mse,  sigmoid\n\n\n\nclass TFFMClassifier(TFFMBaseModel):\n    """"""Factorization Machine (aka FM).\n\n    This class implements L2-regularized arbitrary order FM model with logistic\n    loss and gradient-based optimization.\n\n    Only binary classification with 0/1 labels supported.\n\n    See TFFMBaseModel and TFFMCore docs for details about parameters.\n    """"""\n\n    def __init__(self, **init_params):\n\n        assert \'loss_function\' not in init_params, """"""Parameter \'loss_function\' is\n        not supported for TFFMClassifier. For custom loss function, extend the\n        base class TFFMBaseModel.""""""\n\n        init_params[\'loss_function\'] = loss_logistic\n        self.init_basemodel(**init_params)\n\n    def _preprocess_sample_weights(self, sample_weight, pos_class_weight, used_y):\n        assert sample_weight is None or pos_class_weight is None, ""sample_weight and pos_class_weight are mutually exclusive parameters""\n        used_w = np.ones_like(used_y)\n        if sample_weight is None and pos_class_weight is None:\n            return used_w\n        if type(pos_class_weight) == float:\n            used_w[used_y > 0] = pos_class_weight\n        elif sample_weight == ""balanced"":\n            pos_rate = np.mean(used_y > 0)\n            neg_rate = 1 - pos_rate\n            used_w[used_y > 0] = neg_rate / pos_rate\n            used_w[used_y < 0] = 1.0\n            return used_w\n        elif type(sample_weight) == np.ndarray and len(sample_weight.shape)==1:\n            used_w = sample_weight\n        else:\n            raise ValueError(""Unexpected type for sample_weight or pos_class_weight parameters."")\n\n        return used_w\n\n\n    def fit(self, X, y, sample_weight=None, pos_class_weight=None, n_epochs=None, show_progress=False):\n        # preprocess Y: suppose input {0, 1}, but internally will use {-1, 1} labels instead\n        if not (set(y) == set([0, 1])):\n            raise ValueError(""Input labels must be in set {0,1}."")\n        used_y = y * 2 - 1\n        if sample_weight is not None:\n            self.sample_weight = sample_weight\n        if pos_class_weight is not None:\n            self.pos_class_weight = pos_class_weight\n        used_w = self._preprocess_sample_weights(self.sample_weight, self.pos_class_weight, used_y)\n        self._fit(X_=X, y_=used_y, w_=used_w, n_epochs=n_epochs, show_progress=show_progress)\n\n    def predict(self, X, pred_batch_size=None):\n        """"""Predict using the FM model\n\n        Parameters\n        ----------\n        X : {numpy.array, scipy.sparse.csr_matrix}, shape = (n_samples, n_features)\n            Samples.\n        pred_batch_size : int batch size for prediction (default None)\n\n        Returns\n        -------\n        predictions : array, shape = (n_samples,)\n            Returns predicted values.\n        """"""\n        raw_output = self.decision_function(X, pred_batch_size)\n        predictions = (raw_output > 0).astype(int)\n        return predictions\n\n    def predict_proba(self, X, pred_batch_size=None):\n        """"""Probability estimates.\n\n        The returned estimates for all 2 classes are ordered by the\n        label of classes.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n        pred_batch_size : int batch size for prediction (default None)\n\n        Returns\n        -------\n        probs : array-like, shape = [n_samples, 2]\n            Returns the probability of the sample for each class in the model.\n        """"""\n        outputs = self.decision_function(X, pred_batch_size)\n        probs_positive = sigmoid(outputs)\n        probs_negative = 1 - probs_positive\n        probs = np.vstack((probs_negative.T, probs_positive.T))\n        return probs.T\n\n\nclass TFFMRegressor(TFFMBaseModel):\n    """"""Factorization Machine (aka FM).\n\n    This class implements L2-regularized arbitrary order FM model with MSE\n    loss and gradient-based optimization.\n\n    Custom loss functions are not supported, mean squared error is always\n    used. Any loss function provided in parameters will be overwritten.\n\n    See TFFMBaseModel and TFFMCore docs for details about parameters.\n    """"""\n\n    def __init__(self, **init_params):\n\n        assert \'loss_function\' not in init_params, """"""Parameter \'loss_function\' is\n        not supported for TFFMRegressor. For custom loss function, extend the\n        base class TFFMBaseModel.""""""\n        \n        init_params[\'loss_function\'] = loss_mse\n        self.init_basemodel(**init_params)\n\n    def fit(self, X, y, sample_weight=None, n_epochs=None, show_progress=False):\n        sample_weight = np.ones_like(y) if sample_weight is None else sample_weight\n        self._fit(X_=X, y_=y, w_=sample_weight, n_epochs=n_epochs, show_progress=show_progress)\n\n    def predict(self, X, pred_batch_size=None):\n        """"""Predict using the FM model\n\n        Parameters\n        ----------\n        X : {numpy.array, scipy.sparse.csr_matrix}, shape = (n_samples, n_features)\n            Samples.\n        pred_batch_size : int batch size for prediction (default None)\n\n        Returns\n        -------\n        predictions : array, shape = (n_samples,)\n            Returns predicted values.\n        """"""\n        predictions = self.decision_function(X, pred_batch_size)\n        return predictions\n'"
tffm/utils.py,26,"b'""""""Supporting functions for arbitrary order Factorization Machines.""""""\n\nimport math\nimport numpy as np\nimport tensorflow as tf\nimport itertools\nfrom itertools import combinations_with_replacement, takewhile, count\nfrom collections import defaultdict\n\n\ndef get_shorter_decompositions(basic_decomposition):\n    """"""Returns all arrays simpler than basic_decomposition.\n\n    Returns all arrays that can be constructed from basic_decomposition\n    via joining (summing) its elements.\n\n    Parameters\n    ----------\n    basic_decomposition : list or np.array\n        The array from which to build subsequent ones.\n\n    Returns\n    -------\n    decompositions : list of tuples\n        All possible arrays that can be constructed from basic_decomposition.\n    counts : np.array\n        counts[i] equals to the number of ways to build decompositions[i] from\n        basic_decomposition.\n\n    Example\n    -------\n    decompositions, counts = get_shorter_decompositions([1, 2, 3])\n        decompositions == [(1, 5), (2, 4), (3, 3), (6,)]\n        counts == [ 2.,  1.,  1.,  2.]\n    """"""\n    order = int(np.sum(basic_decomposition))\n    decompositions = []\n    variations = defaultdict(lambda: [])\n    for curr_len in range(1, len(basic_decomposition)):\n        for sum_rule in combinations_with_replacement(range(curr_len), order):\n            sum_rule = np.array(sum_rule)\n            curr_pows = np.array([np.sum(sum_rule == i) for i in range(curr_len)])\n            curr_pows = curr_pows[curr_pows != 0]\n            sorted_pow = tuple(np.sort(curr_pows))\n            variations[sorted_pow].append(tuple(curr_pows))\n            decompositions.append(sorted_pow)\n    if len(decompositions) > 1:\n        decompositions = np.unique(decompositions)\n        counts = np.zeros(decompositions.shape[0])\n        for i, dec in enumerate(decompositions):\n            counts[i] = len(np.unique(variations[dec]))\n    else:\n        counts = np.ones(1)\n    return decompositions, counts\n\ndef sort_topologically(children_by_node, node_list):\n    """"""Topological sort of a graph.\n\n    Parameters\n    ----------\n    children_by_node : dict\n        Children for any node.\n    node_list : list\n        All nodes (some nodes may not have children and thus a separate\n        parameter is needed).\n\n    Returns\n    -------\n    list, nodes in the topological order\n    """"""\n    levels_by_node = {}\n    nodes_by_level = defaultdict(set)\n\n    def walk_depth_first(node):\n        if node in levels_by_node:\n            return levels_by_node[node]\n        children = children_by_node[node]\n        level = 0 if not children else (1 + max(walk_depth_first(lname) for lname, _ in children))\n        levels_by_node[node] = level\n        nodes_by_level[level].add(node)\n        return level\n\n    for node in node_list:\n        walk_depth_first(node)\n\n    nodes_by_level = list(takewhile(lambda x: x != [],\n                                    (list(nodes_by_level[i]) for i in count())))\n    return list(itertools.chain.from_iterable(nodes_by_level))\n\ndef initial_coefficient(decomposition):\n    """"""Compute initial coefficient of the decomposition.""""""\n    order = np.sum(decomposition)\n    coef = math.factorial(order)\n    coef /= np.prod([math.factorial(x) for x in decomposition])\n    _, counts = np.unique(decomposition, return_counts=True)\n    coef /= np.prod([math.factorial(c) for c in counts])\n    return coef\n\ndef powers_and_coefs(order):\n    """"""For a `order`-way FM returns the powers and their coefficients needed to\n    compute model equation efficiently\n    """"""\n    decompositions, _ = get_shorter_decompositions(np.ones(order))\n    graph = defaultdict(lambda: list())\n    graph_reversed = defaultdict(lambda: list())\n    for dec in decompositions:\n        parents, weights = get_shorter_decompositions(dec)\n        for i in range(len(parents)):\n            graph[parents[i]].append((dec, weights[i]))\n            graph_reversed[dec].append((parents[i], weights[i]))\n\n    topo_order = sort_topologically(graph, decompositions)\n\n    final_coefs = defaultdict(lambda: 0)\n    for node in topo_order:\n        final_coefs[node] += initial_coefficient(node)\n        for p, w in graph_reversed[node]:\n            final_coefs[p] -= w * final_coefs[node]\n    powers_and_coefs_list = []\n    # for dec, c in final_coefs.iteritems():\n    for dec, c in final_coefs.items():\n        in_pows, out_pows = np.unique(dec, return_counts=True)\n        powers_and_coefs_list.append((in_pows, out_pows, c))\n\n    return powers_and_coefs_list\n\n\ndef matmul_wrapper(A, B, optype):\n    """"""Wrapper for handling sparse and dense versions of `tf.matmul` operation.\n\n    Parameters\n    ----------\n    A : tf.Tensor\n    B : tf.Tensor\n    optype : str, {\'dense\', \'sparse\'}\n\n    Returns\n    -------\n    tf.Tensor\n    """"""\n    with tf.name_scope(\'matmul_wrapper\') as scope:\n        if optype == \'dense\':\n            return tf.matmul(A, B)\n        elif optype == \'sparse\':\n            return tf.sparse_tensor_dense_matmul(A, B)\n        else:\n            raise NameError(\'Unknown input type in matmul_wrapper\')\n\n\ndef pow_wrapper(X, p, optype):\n    """"""Wrapper for handling sparse and dense versions of `tf.pow` operation.\n\n    Parameters\n    ----------\n    X : tf.Tensor\n    p : int\n    optype : str, {\'dense\', \'sparse\'}\n\n    Returns\n    -------\n    tf.Tensor\n    """"""\n    with tf.name_scope(\'pow_wrapper\') as scope:\n        if optype == \'dense\':\n            return tf.pow(X, p)\n        elif optype == \'sparse\':\n            return tf.SparseTensor(X.indices, tf.pow(X.values, p), X.dense_shape)\n        else:\n            raise NameError(\'Unknown input type in pow_wrapper\')\n\n\ndef count_nonzero_wrapper(X, optype):\n    """"""Wrapper for handling sparse and dense versions of `tf.count_nonzero`.\n\n    Parameters\n    ----------\n    X : tf.Tensor (N, K)\n    optype : str, {\'dense\', \'sparse\'}\n\n    Returns\n    -------\n    tf.Tensor (1,K)\n    """"""\n    with tf.name_scope(\'count_nonzero_wrapper\') as scope:\n        if optype == \'dense\':\n            return tf.count_nonzero(X, axis=0, keep_dims=True)\n        elif optype == \'sparse\':\n            indicator_X = tf.SparseTensor(X.indices, tf.ones_like(X.values), X.dense_shape)\n            return tf.sparse_reduce_sum(indicator_X, axis=0, keep_dims=True)\n        else:\n            raise NameError(\'Unknown input type in count_nonzero_wrapper\')\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\n# Predefined loss functions\n# Should take 2 tf.Ops: outputs, targets and should return tf.Op of element-wise losses\n# Be careful about dimensionality -- maybe tf.transpose(outputs) is needed\n\ndef loss_logistic(outputs, y):\n    margins = -y * tf.transpose(outputs)\n    raw_loss = tf.log(tf.add(1.0, tf.exp(margins)))\n    return tf.minimum(raw_loss, 100, name=\'truncated_log_loss\')\n\ndef loss_mse(outputs, y):\n    return tf.pow(y -  tf.transpose(outputs), 2, name=\'mse_loss\')\n    \n\n'"
