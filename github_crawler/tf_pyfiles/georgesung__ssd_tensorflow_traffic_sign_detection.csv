file_path,api_count,code
data_prep.py,0,"b'\'\'\'\nData preparation\n\'\'\'\nfrom settings import *\nimport numpy as np\nimport pickle\n\n\ndef calc_iou(box_a, box_b):\n\t""""""\n\tCalculate the Intersection Over Union of two boxes\n\tEach box specified by upper left corner and lower right corner:\n\t(x1, y1, x2, y2), where 1 denotes upper left corner, 2 denotes lower right corner\n\n\tReturns IOU value\n\t""""""\n\t# Calculate intersection, i.e. area of overlap between the 2 boxes (could be 0)\n\t# http://math.stackexchange.com/a/99576\n\tx_overlap = max(0, min(box_a[2], box_b[2]) - max(box_a[0], box_b[0]))\n\ty_overlap = max(0, min(box_a[3], box_b[3]) - max(box_a[1], box_b[1]))\n\tintersection = x_overlap * y_overlap\n\n\t# Calculate union\n\tarea_box_a = (box_a[2] - box_a[0]) * (box_a[3] - box_a[1])\n\tarea_box_b = (box_b[2] - box_b[0]) * (box_b[3] - box_b[1])\n\tunion = area_box_a + area_box_b - intersection\n\n\tiou = intersection / union\n\treturn iou\n\n\ndef find_gt_boxes(data_raw, image_file):\n\t""""""\n\tGiven (global) feature map sizes, and single training example,\n\tfind all default boxes that exceed Jaccard overlap threshold\n\n\tReturns y_true array that flags the matching default boxes with class ID (-1 means nothing there)\n\t""""""\n\t# Pre-process ground-truth data\n\t# Convert absolute coordinates to relative coordinates ranging from 0 to 1\n\t# Read the sign class label (note background class label is 0, sign labels are ints >=1)\n\tsigns_data = data_raw[image_file]\n\n\tsigns_class = []\n\tsigns_box_coords = []  # relative coordinates\n\tfor sign_data in signs_data:\n\t\t# Find class label\n\t\tsign_class = sign_data[\'class\']\n\t\tsigns_class.append(sign_class)\n\n\t\t# Calculate relative coordinates\n\t\t# (x1, y1, x2, y2), where 1 denotes upper left corner, 2 denotes lower right corner\n\t\tabs_box_coords = sign_data[\'box_coords\']\n\t\tscale = np.array([IMG_W, IMG_H, IMG_W, IMG_H])\n\t\tbox_coords = np.array(abs_box_coords) / scale\n\t\tsigns_box_coords.append(box_coords)\n\n\t# Initialize y_true to all 0s (0 -> background)\n\ty_true_len = 0\n\tfor fm_size in FM_SIZES:\n\t\ty_true_len += fm_size[0] * fm_size[1] * NUM_DEFAULT_BOXES\n\ty_true_conf = np.zeros(y_true_len)\n\ty_true_loc = np.zeros(y_true_len * 4)\n\n\t# For each GT box, for each feature map, for each feature map cell, for each default box:\n\t# 1) Calculate the Jaccard overlap (IOU) and annotate the class label\n\t# 2) Count how many box matches we got\n\t# 3) If we got a match, calculate normalized box coordinates and updte y_true_loc\n\tmatch_counter = 0\n\tfor i, gt_box_coords in enumerate(signs_box_coords):\n\t\ty_true_idx = 0\n\t\t#for fm_idx, fm_size in enumerate(FM_SIZES):\n\t\tfor fm_size in FM_SIZES:\n\t\t\tfm_h, fm_w = fm_size  # feature map height and width\n\t\t\tfor row in range(fm_h):\n\t\t\t\tfor col in range(fm_w):\n\t\t\t\t\tfor db in DEFAULT_BOXES:\n\t\t\t\t\t\t# Calculate relative box coordinates for this default box\n\t\t\t\t\t\tx1_offset, y1_offset, x2_offset, y2_offset = db\n\t\t\t\t\t\tabs_db_box_coords = np.array([\n\t\t\t\t\t\t\tmax(0, col + x1_offset),\n\t\t\t\t\t\t\tmax(0, row + y1_offset),\n\t\t\t\t\t\t\tmin(fm_w, col+1 + x2_offset),\n\t\t\t\t\t\t\tmin(fm_h, row+1 + y2_offset)\n\t\t\t\t\t\t])\n\t\t\t\t\t\tscale = np.array([fm_w, fm_h, fm_w, fm_h])\n\t\t\t\t\t\tdb_box_coords = abs_db_box_coords / scale\n\n\t\t\t\t\t\t# Calculate Jaccard overlap (i.e. Intersection Over Union, IOU) of GT box and default box\n\t\t\t\t\t\tiou = calc_iou(gt_box_coords, db_box_coords)\n\n\t\t\t\t\t\t# If box matches, i.e. IOU threshold met\n\t\t\t\t\t\tif iou >= IOU_THRESH:\n\t\t\t\t\t\t\t# Update y_true_conf to reflect we found a match, and increment match_counter\n\t\t\t\t\t\t\ty_true_conf[y_true_idx] = signs_class[i]\n\t\t\t\t\t\t\tmatch_counter += 1\n\n\t\t\t\t\t\t\t# Calculate normalized box coordinates and update y_true_loc\n\t\t\t\t\t\t\tabs_box_center = np.array([col + 0.5, row + 0.5])  # absolute coordinates of center of feature map cell\n\t\t\t\t\t\t\tabs_gt_box_coords = gt_box_coords * scale  # absolute ground truth box coordinates (in feature map grid)\n\t\t\t\t\t\t\tnorm_box_coords = abs_gt_box_coords - np.concatenate((abs_box_center, abs_box_center))\n\t\t\t\t\t\t\ty_true_loc[y_true_idx*4 : y_true_idx*4 + 4] = norm_box_coords\n\n\t\t\t\t\t\ty_true_idx += 1\n\n\treturn y_true_conf, y_true_loc, match_counter\n\n\ndef do_data_prep(data_raw):\n\t""""""\n\tCreate the y_true array\n\tdata_raw is the dict mapping image_file -> [{\'class\': class_int, \'box_coords\': (x1, y1, x2, y2)}, {...}, ...]\n\n\tReturn a dict {image_file1: {\'y_true_conf\': y_true_conf, \'y_true_loc\': y_true_loc}, image_file2: ...}\n\t""""""\n\t# Prepare the data by populating y_true appropriately\n\tdata_prep = {}\n\tfor image_file in data_raw.keys():\n\t\t# Find groud-truth boxes based on Jaccard overlap,\n\t\t# populate y_true_conf (class labels) and y_true_loc (normalized box coordinates)\n\t\ty_true_conf, y_true_loc, match_counter = find_gt_boxes(data_raw, image_file)\n\n\t\t# Only want data points where we have matching default boxes\n\t\tif match_counter > 0:\n\t\t\tdata_prep[image_file] = {\'y_true_conf\': y_true_conf, \'y_true_loc\': y_true_loc}\n\n\treturn data_prep\n\n\nif __name__ == \'__main__\':\n\twith open(\'data_raw_%sx%s.p\' % (IMG_W, IMG_H), \'rb\') as f:\n\t\tdata_raw = pickle.load(f)\n\n\tprint(\'Preparing data (i.e. matching boxes)\')\n\tdata_prep = do_data_prep(data_raw)\n\n\twith open(\'data_prep_%sx%s.p\' % (IMG_W, IMG_H), \'wb\') as f:\n\t\tpickle.dump(data_prep, f)\n\n\tprint(\'Done. Saved prepared data to data_prep_%sx%s.p\' % (IMG_W, IMG_H))\n\tprint(\'Total images with >=1 matching box: %d\' % len(data_prep.keys()))\n'"
inference.py,2,"b'\'\'\'\nRun inference using trained model\n\'\'\'\nimport tensorflow as tf\nfrom settings import *\nfrom model import SSDModel\nfrom model import ModelHelper\nfrom model import nms\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport cv2\nimport math\nimport os\nimport time\nimport pickle\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom moviepy.editor import VideoFileClip\nfrom optparse import OptionParser\nimport glob\n\n\ndef run_inference(image, model, sess, mode, sign_map):\n\t""""""\n\tRun inference on a given image\n\n\tArguments:\n\t\t* image: Numpy array representing a single RGB image\n\t\t* model: Dict of tensor references returned by SSDModel()\n\t\t* sess: TensorFlow session reference\n\t\t* mode: String of either ""image"", ""video"", or ""demo""\n\n\tReturns:\n\t\t* Numpy array representing annotated image\n\t""""""\n\t# Save original image in memory\n\timage = np.array(image)\n\timage_orig = np.copy(image)\n\n\t# Get relevant tensors\n\tx = model[\'x\']\n\tis_training = model[\'is_training\']\n\tpreds_conf = model[\'preds_conf\']\n\tpreds_loc = model[\'preds_loc\']\n\tprobs = model[\'probs\']\n\n\t# Convert image to PIL Image, resize it, convert to grayscale (if necessary), convert back to numpy array\n\timage = Image.fromarray(image)\n\torig_w, orig_h = image.size\n\tif NUM_CHANNELS == 1:\n\t\timage = image.convert(\'L\')  # 8-bit grayscale\n\timage = image.resize((IMG_W, IMG_H), Image.LANCZOS)  # high-quality downsampling filter\n\timage = np.asarray(image)\n\n\timages = np.array([image])  # create a ""batch"" of 1 image\n\tif NUM_CHANNELS == 1:\n\t\timages = np.expand_dims(images, axis=-1)  # need extra dimension of size 1 for grayscale\n\n\t# Perform object detection\n\tt0 = time.time()  # keep track of duration of object detection + NMS\n\tpreds_conf_val, preds_loc_val, probs_val = sess.run([preds_conf, preds_loc, probs], feed_dict={x: images, is_training: False})\n\tif mode != \'video\':\n\t\tprint(\'Inference took %.1f ms (%.2f fps)\' % ((time.time() - t0)*1000, 1/(time.time() - t0)))\n\n\t# Gather class predictions and confidence values\n\ty_pred_conf = preds_conf_val[0]  # batch size of 1, so just take [0]\n\ty_pred_conf = y_pred_conf.astype(\'float32\')\n\tprob = probs_val[0]\n\n\t# Gather localization predictions\n\ty_pred_loc = preds_loc_val[0]\n\n\t# Perform NMS\n\tboxes = nms(y_pred_conf, y_pred_loc, prob)\n\tif mode != \'video\':\n\t\tprint(\'Inference + NMS took %.1f ms (%.2f fps)\' % ((time.time() - t0)*1000, 1/(time.time() - t0)))\n\n\t# Rescale boxes\' coordinates back to original image\'s dimensions\n\t# Recall boxes = [[x1, y1, x2, y2, cls, cls_prob], [...], ...]\n\tscale = np.array([orig_w/IMG_W, orig_h/IMG_H, orig_w/IMG_W, orig_h/IMG_H])\n\tif len(boxes) > 0:\n\t\tboxes[:, :4] = boxes[:, :4] * scale\n\n\t# Draw and annotate boxes over original image, and return annotated image\n\timage = image_orig\n\tfor box in boxes:\n\t\t# Get box parameters\n\t\tbox_coords = [int(round(x)) for x in box[:4]]\n\t\tcls = int(box[4])\n\t\tcls_prob = box[5]\n\n\t\t# Annotate image\n\t\timage = cv2.rectangle(image, tuple(box_coords[:2]), tuple(box_coords[2:]), (0,255,0))\n\t\tlabel_str = \'%s %.2f\' % (sign_map[cls], cls_prob)\n\t\timage = cv2.putText(image, label_str, (box_coords[0], box_coords[1]), 0, 0.5, (0,255,0), 1, cv2.LINE_AA)\n\n\treturn image\n\n\ndef generate_output(input_files, mode):\n\t""""""\n\tGenerate annotated images, videos, or sample images, based on mode\n\t""""""\n\t# First, load mapping from integer class ID to sign name string\n\tsign_map = {}\n\twith open(\'signnames.csv\', \'r\') as f:\n\t\tfor line in f:\n\t\t\tline = line[:-1]  # strip newline at the end\n\t\t\tsign_id, sign_name = line.split(\',\')\n\t\t\tsign_map[int(sign_id)] = sign_name\n\tsign_map[0] = \'background\'  # class ID 0 reserved for background class\n\n\t# Create output directory \'inference_out/\' if needed\n\tif mode == \'image\' or mode == \'video\':\n\t\tif not os.path.isdir(\'./inference_out\'):\n\t\t\ttry:\n\t\t\t\tos.mkdir(\'./inference_out\')\n\t\t\texcept FileExistsError:\n\t\t\t\tprint(\'Error: Cannot mkdir ./inference_out\')\n\t\t\t\treturn\n\n\t# Launch the graph\n\twith tf.Graph().as_default(), tf.Session() as sess:\n\t\t# ""Instantiate"" neural network, get relevant tensors\n\t\tmodel = SSDModel()\n\n\t\t# Load trained model\n\t\tsaver = tf.train.Saver()\n\t\tprint(\'Restoring previously trained model at %s\' % MODEL_SAVE_PATH)\n\t\tsaver.restore(sess, MODEL_SAVE_PATH)\n\n\t\tif mode == \'image\':\n\t\t\tfor image_file in input_files:\n\t\t\t\tprint(\'Running inference on %s\' % image_file)\n\t\t\t\timage_orig = np.asarray(Image.open(image_file))\n\t\t\t\timage = run_inference(image_orig, model, sess, mode, sign_map)\n\n\t\t\t\thead, tail = os.path.split(image_file)\n\t\t\t\tplt.imsave(\'./inference_out/%s\' % tail, image)\n\t\t\tprint(\'Output saved in inference_out/\')\n\n\t\telif mode == \'video\':\n\t\t\tfor video_file in input_files:\n\t\t\t\tprint(\'Running inference on %s\' % video_file)\n\t\t\t\tvideo = VideoFileClip(video_file)\n\t\t\t\tvideo = video.fl_image(lambda x: run_inference(x, model, sess, mode, sign_map))\n\n\t\t\t\thead, tail = os.path.split(video_file)\n\t\t\t\tvideo.write_videofile(\'./inference_out/%s\' % tail, audio=False)\n\t\t\tprint(\'Output saved in inference_out/\')\n\n\t\telif mode == \'demo\':\n\t\t\tprint(\'Demo mode: Running inference on images in sample_images/\')\n\t\t\timage_files = os.listdir(\'sample_images/\')\n\n\t\t\tfor image_file in image_files:\n\t\t\t\tprint(\'Running inference on sample_images/%s\' % image_file)\n\t\t\t\timage_orig = np.asarray(Image.open(\'sample_images/\' + image_file))\n\t\t\t\timage = run_inference(image_orig, model, sess, mode, sign_map)\n\t\t\t\tplt.imshow(image)\n\t\t\t\tplt.show()\n\n\t\telse:\n\t\t\traise ValueError(\'Invalid mode: %s\' % mode)\n\n\nif __name__ == \'__main__\':\n\t# Configure command line options\n\tparser = OptionParser()\n\tparser.add_option(\'-i\', \'--input_dir\', dest=\'input_dir\',\n\t\thelp=\'Directory of input videos/images (ignored for ""demo"" mode). Will run inference on all videos/images in that dir\')\n\tparser.add_option(\'-m\', \'--mode\', dest=\'mode\', default=\'image\',\n\t\thelp=\'Operating mode, could be ""image"", ""video"", or ""demo""; ""demo"" mode displays annotated images from sample_images/\')\n\n\t# Get and parse command line options\n\toptions, args = parser.parse_args()\n\n\tinput_dir = options.input_dir\n\tmode = options.mode\n\n\tif mode != \'video\' and mode != \'image\' and mode != \'demo\':\n\t\tassert ValueError(\'Invalid mode: %s\' % mode)\n\n\tif mode != \'demo\':\n\t\tinput_files = glob.glob(input_dir + \'/*.*\')\n\telse:\n\t\tinput_files = []\n\n\tgenerate_output(input_files, mode)\n'"
model.py,27,"b'\'\'\'\nModel definition\n\'\'\'\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nfrom settings import *\nfrom data_prep import calc_iou\n\n\ndef SSDHook(feature_map, hook_id):\n\t""""""\n\tTakes input feature map, output the predictions tensor\n\thook_id is for variable_scope unqie string ID\n\t""""""\n\twith tf.variable_scope(\'ssd_hook_\' + hook_id):\n\t\t# Note we have linear activation (i.e. no activation function)\n\t\tnet_conf = slim.conv2d(feature_map, NUM_PRED_CONF, [3, 3], activation_fn=None, scope=\'conv_conf\')\n\t\tnet_conf = tf.contrib.layers.flatten(net_conf)\n\n\t\tnet_loc = slim.conv2d(feature_map, NUM_PRED_LOC, [3, 3], activation_fn=None, scope=\'conv_loc\')\n\t\tnet_loc = tf.contrib.layers.flatten(net_loc)\n\n\treturn net_conf, net_loc\n\n\ndef ModelHelper(y_pred_conf, y_pred_loc):\n\t""""""\n\tDefine loss function, optimizer, predictions, and accuracy metric\n\tLoss includes confidence loss and localization loss\n\n\tconf_loss_mask is created at batch generation time, to mask the confidence losses\n\tIt has 1 at locations w/ positives, and 1 at select negative locations\n\tsuch that negative-to-positive ratio of NEG_POS_RATIO is satisfied\n\n\tArguments:\n\t\t* y_pred_conf: Class predictions from model,\n\t\t\ta tensor of shape [batch_size, num_feature_map_cells * num_defaul_boxes * num_classes]\n\t\t* y_pred_loc: Localization predictions from model,\n\t\t\ta tensor of shape [batch_size, num_feature_map_cells * num_defaul_boxes * 4]\n\n\tReturns relevant tensor references\n\t""""""\n\tnum_total_preds = 0\n\tfor fm_size in FM_SIZES:\n\t\tnum_total_preds += fm_size[0] * fm_size[1] * NUM_DEFAULT_BOXES\n\tnum_total_preds_conf = num_total_preds * NUM_CLASSES\n\tnum_total_preds_loc  = num_total_preds * 4\n\n\t# Input tensors\n\ty_true_conf = tf.placeholder(tf.int32, [None, num_total_preds], name=\'y_true_conf\')  # classification ground-truth labels\n\ty_true_loc  = tf.placeholder(tf.float32, [None, num_total_preds_loc], name=\'y_true_loc\')  # localization ground-truth labels\n\tconf_loss_mask = tf.placeholder(tf.float32, [None, num_total_preds], name=\'conf_loss_mask\')  # 1 mask ""bit"" per def. box\n\n\t# Confidence loss\n\tlogits = tf.reshape(y_pred_conf, [-1, num_total_preds, NUM_CLASSES])\n\tconf_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y_true_conf)\n\tconf_loss = conf_loss_mask * conf_loss  # ""zero-out"" the loss for don\'t-care negatives\n\tconf_loss = tf.reduce_sum(conf_loss)\n\n\t# Localization loss (smooth L1 loss)\n\t# loc_loss_mask is analagous to conf_loss_mask, except 4 times the size\n\tdiff = y_true_loc - y_pred_loc\n\t\n\tloc_loss_l2 = 0.5 * (diff**2.0)\n\tloc_loss_l1 = tf.abs(diff) - 0.5\n\tsmooth_l1_condition = tf.less(tf.abs(diff), 1.0)\n\tloc_loss = tf.select(smooth_l1_condition, loc_loss_l2, loc_loss_l1)\n\t\n\tloc_loss_mask = tf.minimum(y_true_conf, 1)  # have non-zero localization loss only where we have matching ground-truth box\n\tloc_loss_mask = tf.to_float(loc_loss_mask)\n\tloc_loss_mask = tf.stack([loc_loss_mask] * 4, axis=2)  # [0, 1, 1] -> [[[0, 0, 0, 0], [1, 1, 1, 1], [1, 1, 1, 1]], ...]\n\tloc_loss_mask = tf.reshape(loc_loss_mask, [-1, num_total_preds_loc])  # removing the inner-most dimension of above\n\tloc_loss = loc_loss_mask * loc_loss\n\tloc_loss = tf.reduce_sum(loc_loss)\n\n\t# Weighted average of confidence loss and localization loss\n\t# Also add regularization loss\n\tloss = conf_loss + LOC_LOSS_WEIGHT * loc_loss + tf.reduce_sum(slim.losses.get_regularization_losses())\n\toptimizer = OPT.minimize(loss)\n\n\t#reported_loss = loss #tf.reduce_sum(loss, 1)  # DEBUG\n\n\t# Class probabilities and predictions\n\tprobs_all = tf.nn.softmax(logits)\n\tprobs, preds_conf = tf.nn.top_k(probs_all)  # take top-1 probability, and the index is the predicted class\n\tprobs = tf.reshape(probs, [-1, num_total_preds])\n\tpreds_conf = tf.reshape(preds_conf, [-1, num_total_preds])\n\n\t# Return a dictionary of {tensor_name: tensor_reference}\n\tret_dict = {\n\t\t\'y_true_conf\': y_true_conf,\n\t\t\'y_true_loc\': y_true_loc,\n\t\t\'conf_loss_mask\': conf_loss_mask,\n\t\t\'optimizer\': optimizer,\n\t\t\'conf_loss\': conf_loss,\n\t\t\'loc_loss\': loc_loss,\n\t\t\'loss\': loss,\n\t\t\'probs\': probs,\n\t\t\'preds_conf\': preds_conf,\n\t\t\'preds_loc\': y_pred_loc,\n\t}\n\treturn ret_dict\n\n\ndef AlexNet():\n\t""""""\n\tAlexNet\n\t""""""\n\t# Image batch tensor and dropout keep prob placeholders\n\tx = tf.placeholder(tf.float32, [None, IMG_H, IMG_W, NUM_CHANNELS], name=\'x\')\n\tis_training = tf.placeholder(tf.bool, name=\'is_training\')\n\n\t# Classification and localization predictions\n\tpreds_conf = []  # conf -> classification b/c confidence loss -> classification loss\n\tpreds_loc = []\n\n\t# Use batch normalization for all convolution layers\n\t# FIXME: Not sure why setting is_training is not working well\n\t#with slim.arg_scope([slim.conv2d], normalizer_fn=slim.batch_norm, normalizer_params={\'is_training\': is_training}):\n\twith slim.arg_scope([slim.conv2d], normalizer_fn=slim.batch_norm, normalizer_params={\'is_training\': True},\\\n\t\t\tweights_regularizer=slim.l2_regularizer(scale=REG_SCALE)):\n\t\tnet = slim.conv2d(x, 64, [11, 11], 4, padding=\'VALID\', scope=\'conv1\')\n\t\tnet = slim.max_pool2d(net, [3, 3], 2, scope=\'pool1\')\n\t\tnet = slim.conv2d(net, 192, [5, 5], scope=\'conv2\')\n\n\t\tnet_conf, net_loc = SSDHook(net, \'conv2\')\n\t\tpreds_conf.append(net_conf)\n\t\tpreds_loc.append(net_loc)\n\n\t\tnet = slim.max_pool2d(net, [3, 3], 2, scope=\'pool2\')\n\t\tnet = slim.conv2d(net, 384, [3, 3], scope=\'conv3\')\n\t\tnet = slim.conv2d(net, 384, [3, 3], scope=\'conv4\')\n\t\tnet = slim.conv2d(net, 256, [3, 3], scope=\'conv5\')\n\n\t\t# The following layers added for SSD\n\t\tnet = slim.conv2d(net, 1024, [3, 3], scope=\'conv6\')\n\t\tnet = slim.conv2d(net, 1024, [1, 1], scope=\'conv7\')\n\n\t\tnet_conf, net_loc = SSDHook(net, \'conv7\')\n\t\tpreds_conf.append(net_conf)\n\t\tpreds_loc.append(net_loc)\n\n\t\tnet = slim.conv2d(net, 256, [1, 1], scope=\'conv8\')\n\t\tnet = slim.conv2d(net, 512, [3, 3], 2, scope=\'conv8_2\')\n\n\t\tnet_conf, net_loc = SSDHook(net, \'conv8_2\')\n\t\tpreds_conf.append(net_conf)\n\t\tpreds_loc.append(net_loc)\n\n\t\tnet = slim.conv2d(net, 128, [1, 1], scope=\'conv9\')\n\t\tnet = slim.conv2d(net, 256, [3, 3], 2, scope=\'conv9_2\')\n\n\t\tnet_conf, net_loc = SSDHook(net, \'conv9_2\')\n\t\tpreds_conf.append(net_conf)\n\t\tpreds_loc.append(net_loc)\n\n\t# Concatenate all preds together into 1 vector, for both classification and localization predictions\n\tfinal_pred_conf = tf.concat(1, preds_conf)\n\tfinal_pred_loc = tf.concat(1, preds_loc)\n\n\t# Return a dictionary of {tensor_name: tensor_reference}\n\tret_dict = {\n\t\t\'x\': x,\n\t\t\'y_pred_conf\': final_pred_conf,\n\t\t\'y_pred_loc\': final_pred_loc,\n\t\t\'is_training\': is_training,\n\t}\n\treturn ret_dict\n\n\ndef SSDModel():\n\t""""""\n\tWrapper around the model and model helper\n\tReturns dict of relevant tensor references\n\t""""""\n\tif MODEL == \'AlexNet\':\n\t\tmodel = AlexNet()\n\telse:\n\t\traise NotImplementedError(\'Model %s not supported\' % MODEL)\n\n\tmodel_helper = ModelHelper(model[\'y_pred_conf\'], model[\'y_pred_loc\'])\n\n\tssd_model = {}\n\tfor k in model.keys():\n\t\tssd_model[k] = model[k]\n\tfor k in model_helper.keys():\n\t\tssd_model[k] = model_helper[k]\n\n\treturn ssd_model\n\n\ndef nms(y_pred_conf, y_pred_loc, prob):\n\t""""""\n\tNon-Maximum Suppression (NMS)\n\tPerforms NMS on all boxes of each class where predicted probability > CONF_THRES\n\tFor all boxes exceeding IOU threshold, select the box with highest confidence\n\tReturns a lsit of box coordinates post-NMS\n\n\tArguments:\n\t\t* y_pred_conf: Class predictions, numpy array of shape (num_feature_map_cells * num_defaul_boxes,)\n\t\t* y_pred_loc: Bounding box coordinates, numpy array of shape (num_feature_map_cells * num_defaul_boxes * 4,)\n\t\t\tThese coordinates are normalized coordinates relative to center of feature map cell\n\t\t* prob: Class probabilities, numpy array of shape (num_feature_map_cells * num_defaul_boxes,)\n\n\tReturns:\n\t\t* boxes: Numpy array of boxes, with shape (num_boxes, 6). shape[0] is interpreted as:\n\t\t\t[x1, y1, x2, y2, class, probability], where x1/y1/x2/y2 are the coordinates of the\n\t\t\tupper-left and lower-right corners. Box coordinates assume the image size is IMG_W x IMG_H.\n\t\t\tRemember to rescale box coordinates if your target image has different dimensions.\n\t""""""\n\t# Keep track of boxes for each class\n\tclass_boxes = {}  # class -> [(x1, y1, x2, y2, prob), (...), ...]\n\twith open(\'signnames.csv\', \'r\') as f:\n\t\tfor line in f:\n\t\t\tcls, _ = line.split(\',\')\n\t\t\tclass_boxes[float(cls)] = []\n\n\t# Go through all possible boxes and perform class-based greedy NMS (greedy based on class prediction confidence)\n\ty_idx = 0\n\tfor fm_size in FM_SIZES:\n\t\tfm_h, fm_w = fm_size  # feature map height and width\n\t\tfor row in range(fm_h):\n\t\t\tfor col in range(fm_w):\n\t\t\t\tfor db in DEFAULT_BOXES:\n\t\t\t\t\t# Only perform calculations if class confidence > CONF_THRESH and not background class\n\t\t\t\t\tif prob[y_idx] > CONF_THRESH and y_pred_conf[y_idx] > 0.:\n\t\t\t\t\t\t# Calculate absolute coordinates of predicted bounding box\n\t\t\t\t\t\txc, yc = col + 0.5, row + 0.5  # center of current feature map cell\n\t\t\t\t\t\tcenter_coords = np.array([xc, yc, xc, yc])\n\t\t\t\t\t\tabs_box_coords = center_coords + y_pred_loc[y_idx*4 : y_idx*4 + 4]  # predictions are offsets to center of fm cell\n\n\t\t\t\t\t\t# Calculate predicted box coordinates in actual image\n\t\t\t\t\t\tscale = np.array([IMG_W/fm_w, IMG_H/fm_h, IMG_W/fm_w, IMG_H/fm_h])\n\t\t\t\t\t\tbox_coords = abs_box_coords * scale\n\t\t\t\t\t\tbox_coords = [int(round(x)) for x in box_coords]\n\n\t\t\t\t\t\t# Compare this box to all previous boxes of this class\n\t\t\t\t\t\tcls = y_pred_conf[y_idx]\n\t\t\t\t\t\tcls_prob = prob[y_idx]\n\t\t\t\t\t\tbox = (*box_coords, cls, cls_prob)\n\t\t\t\t\t\tif len(class_boxes[cls]) == 0:\n\t\t\t\t\t\t\tclass_boxes[cls].append(box)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tsuppressed = False  # did this box suppress other box(es)?\n\t\t\t\t\t\t\toverlapped = False  # did this box overlap with other box(es)?\n\t\t\t\t\t\t\tfor other_box in class_boxes[cls]:\n\t\t\t\t\t\t\t\tiou = calc_iou(box[:4], other_box[:4])\n\t\t\t\t\t\t\t\tif iou > NMS_IOU_THRESH:\n\t\t\t\t\t\t\t\t\toverlapped = True\n\t\t\t\t\t\t\t\t\t# If current box has higher confidence than other box\n\t\t\t\t\t\t\t\t\tif box[5] > other_box[5]:\n\t\t\t\t\t\t\t\t\t\tclass_boxes[cls].remove(other_box)\n\t\t\t\t\t\t\t\t\t\tsuppressed = True\n\t\t\t\t\t\t\tif suppressed or not overlapped:\n\t\t\t\t\t\t\t\tclass_boxes[cls].append(box)\n\n\t\t\t\t\ty_idx += 1\n\n\t# Gather all the pruned boxes and return them\n\tboxes = []\n\tfor cls in class_boxes.keys():\n\t\tfor class_box in class_boxes[cls]:\n\t\t\tboxes.append(class_box)\n\tboxes = np.array(boxes)\n\n\treturn boxes\n'"
settings.py,1,"b""'''\nGlobal settings\n'''\nimport tensorflow as tf\n\n\n# Default boxes\n# DEFAULT_BOXES = ((x1_offset, y1_offset, x2_offset, y2_offset), (...), ...)\n# Offset is relative to upper-left-corner and lower-right-corner of the feature map cell\nDEFAULT_BOXES = ((-0.5, -0.5, 0.5, 0.5), (0.2, 0.2, -0.2, -0.2), (-0.8, -0.2, 0.8, 0.2), (-0.2, -0.8, 0.2, 0.8))\nNUM_DEFAULT_BOXES = len(DEFAULT_BOXES)\n\n# Constants (TODO: Keep this updated as we go along)\nNUM_CLASSES = 3  # 2 signs + 1 background class\nNUM_CHANNELS = 1  # grayscale->1, RGB->3\nNUM_PRED_CONF = NUM_DEFAULT_BOXES * NUM_CLASSES  # number of class predictions per feature map cell\nNUM_PRED_LOC  = NUM_DEFAULT_BOXES * 4  # number of localization regression predictions per feature map cell\n\n# Bounding box parameters\nIOU_THRESH = 0.5  # match ground-truth box to default boxes exceeding this IOU threshold, during data prep\nNMS_IOU_THRESH = 0.2  # IOU threshold for non-max suppression\n\n# Negatives-to-positives ratio used to filter training data\nNEG_POS_RATIO = 5  # negative:positive = NEG_POS_RATIO:1\n\n# Class confidence threshold to count as detection\nCONF_THRESH = 0.9\n\n# Model selection and dependent parameters\nMODEL = 'AlexNet'  # AlexNet/VGG16/ResNet50\nif MODEL == 'AlexNet':\n\t#IMG_H, IMG_W = 300, 300\n\t#FM_SIZES = [[36, 36], [17, 17], [9, 9], [5, 5]]  # feature map sizes for SSD hooks via TensorBoard visualization (HxW)\n\n\tIMG_H, IMG_W = 260, 400\n\tFM_SIZES = [[31, 48], [15, 23], [8, 12], [4, 6]]\nelse:\n\traise NotImplementedError('Model not implemented')\n\n# Model hyper-parameters\nOPT = tf.train.AdadeltaOptimizer()\nREG_SCALE = 1e-2  # L2 regularization strength\nLOC_LOSS_WEIGHT = 1.  # weight of localization loss: loss = conf_loss + LOC_LOSS_WEIGHT * loc_loss\n\n# Training process\nRESUME = False  # resume training from previously saved model?\nNUM_EPOCH = 200\nBATCH_SIZE = 32  # batch size for training (relatively small)\nVALIDATION_SIZE = 0.05  # fraction of total training set to use as validation set\nSAVE_MODEL = True  # save trained model to disk?\nMODEL_SAVE_PATH = './model.ckpt'  # where to save trained model\n"""
train.py,3,"b'\'\'\'\nTrain the model on dataset\n\'\'\'\nimport tensorflow as tf\nfrom settings import *\nfrom model import SSDModel\nfrom model import ModelHelper\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport cv2\nimport math\nimport os\nimport time\nimport pickle\nfrom PIL import Image\n\n\ndef next_batch(X, y_conf, y_loc, batch_size):\n\t""""""\n\tNext batch generator\n\tArguments:\n\t\t* X: List of image file names\n\t\t* y_conf: List of ground-truth vectors for class labels\n\t\t* y_loc: List of ground-truth vectors for localization\n\t\t* batch_size: Batch size\n\n\tYields:\n\t\t* images: Batch numpy array representation of batch of images\n\t\t* y_true_conf: Batch numpy array of ground-truth class labels\n\t\t* y_true_loc: Batch numpy array of ground-truth localization\n\t\t* conf_loss_mask: Loss mask for confidence loss, to set NEG_POS_RATIO\n\t""""""\n\tstart_idx = 0\n\twhile True:\n\t\timage_files = X[start_idx : start_idx + batch_size]\n\t\ty_true_conf = np.array(y_conf[start_idx : start_idx + batch_size])\n\t\ty_true_loc  = np.array(y_loc[start_idx : start_idx + batch_size])\n\n\t\t# Read images from image_files\n\t\timages = []\n\t\tfor image_file in image_files:\n\t\t\timage = Image.open(\'resized_images_%sx%s/%s\' % (IMG_W, IMG_H, image_file))\n\t\t\timage = np.asarray(image)\n\t\t\timages.append(image)\n\n\t\timages = np.array(images, dtype=\'float32\')\n\n\t\t# Grayscale images have array shape (H, W), but we want shape (H, W, 1)\n\t\tif NUM_CHANNELS == 1:\n\t\t\timages = np.expand_dims(images, axis=-1)\n\n\t\t# Normalize pixel values (scale them between -1 and 1)\n\t\timages = images/127.5 - 1.\n\n\t\t# For y_true_conf, calculate how many negative examples we need to satisfy NEG_POS_RATIO\n\t\tnum_pos = np.where(y_true_conf > 0)[0].shape[0]\n\t\tnum_neg = NEG_POS_RATIO * num_pos\n\t\ty_true_conf_size = np.sum(y_true_conf.shape)\n\n\t\t# Create confidence loss mask to satisfy NEG_POS_RATIO\n\t\tif num_pos + num_neg < y_true_conf_size:\n\t\t\tconf_loss_mask = np.copy(y_true_conf)\n\t\t\tconf_loss_mask[np.where(conf_loss_mask > 0)] = 1.\n\n\t\t\t# Find all (i,j) tuples where y_true_conf[i][j]==0\n\t\t\tzero_indices = np.where(conf_loss_mask == 0.)  # ([i1, i2, ...], [j1, j2, ...])\n\t\t\tzero_indices = np.transpose(zero_indices)  # [[i1, j1], [i2, j2], ...]\n\n\t\t\t# Randomly choose num_neg rows from zero_indices, w/o replacement\n\t\t\tchosen_zero_indices = zero_indices[np.random.choice(zero_indices.shape[0], int(num_neg), False)]\n\n\t\t\t# ""Enable"" chosen negative examples, specified by chosen_zero_indices\n\t\t\tfor zero_idx in chosen_zero_indices:\n\t\t\t\ti, j = zero_idx\n\t\t\t\tconf_loss_mask[i][j] = 1.\n\n\t\telse:\n\t\t\t# If we have so many positive examples such that num_pos+num_neg >= y_true_conf_size,\n\t\t\t# no need to prune negative data\n\t\t\tconf_loss_mask = np.ones_like(y_true_conf)\n\n\t\tyield (images, y_true_conf, y_true_loc, conf_loss_mask)\n\n\t\t# Update start index for the next batch\n\t\tstart_idx += batch_size\n\t\tif start_idx >= X.shape[0]:\n\t\t\tstart_idx = 0\n\n\ndef run_training():\n\t""""""\n\tLoad training and test data\n\tRun training process\n\tPlot train/validation losses\n\tReport test loss\n\tSave model\n\t""""""\n\t# Load training and test data\n\twith open(\'data_prep_%sx%s.p\' % (IMG_W, IMG_H), mode=\'rb\') as f:\n\t\ttrain = pickle.load(f)\n\t#with open(\'test.p\', mode=\'rb\') as f:\n\t#\ttest = pickle.load(f)\n\n\t# Format the data\n\tX_train = []\n\ty_train_conf = []\n\ty_train_loc = []\n\tfor image_file in train.keys():\n\t\tX_train.append(image_file)\n\t\ty_train_conf.append(train[image_file][\'y_true_conf\'])\n\t\ty_train_loc.append(train[image_file][\'y_true_loc\'])\n\tX_train = np.array(X_train)\n\ty_train_conf = np.array(y_train_conf)\n\ty_train_loc = np.array(y_train_loc)\n\n\t# Train/validation split\n\tX_train, X_valid, y_train_conf, y_valid_conf, y_train_loc, y_valid_loc = train_test_split(\\\n\t\tX_train, y_train_conf, y_train_loc, test_size=VALIDATION_SIZE, random_state=1)\n\n\t# Launch the graph\n\twith tf.Graph().as_default(), tf.Session() as sess:\n\t\t# ""Instantiate"" neural network, get relevant tensors\n\t\tmodel = SSDModel()\n\t\tx = model[\'x\']\n\t\ty_true_conf = model[\'y_true_conf\']\n\t\ty_true_loc = model[\'y_true_loc\']\n\t\tconf_loss_mask = model[\'conf_loss_mask\']\n\t\tis_training = model[\'is_training\']\n\t\toptimizer = model[\'optimizer\']\n\t\treported_loss = model[\'loss\']\n\n\t\t# Training process\n\t\t# TF saver to save/restore trained model\n\t\tsaver = tf.train.Saver()\n\n\t\tif RESUME:\n\t\t\tprint(\'Restoring previously trained model at %s\' % MODEL_SAVE_PATH)\n\t\t\tsaver.restore(sess, MODEL_SAVE_PATH)\n\n\t\t\t# Restore previous loss history\n\t\t\twith open(\'loss_history.p\', \'rb\') as f:\n\t\t\t\tloss_history = pickle.load(f)\n\t\telse:\n\t\t\tprint(\'Training model from scratch\')\n\t\t\t# Variable initialization\n\t\t\tsess.run(tf.global_variables_initializer())\n\n\t\t\t# For book-keeping, keep track of training and validation loss over epochs, like such:\n\t\t\t# [(train_acc_epoch1, valid_acc_epoch1), (train_acc_epoch2, valid_acc_epoch2), ...]\n\t\t\tloss_history = []\n\n\t\t# Record time elapsed for performance check\n\t\tlast_time = time.time()\n\t\ttrain_start_time = time.time()\n\n\t\t# Run NUM_EPOCH epochs of training\n\t\tfor epoch in range(NUM_EPOCH):\n\t\t\ttrain_gen = next_batch(X_train, y_train_conf, y_train_loc, BATCH_SIZE)\n\t\t\tnum_batches_train = math.ceil(X_train.shape[0] / BATCH_SIZE)\n\t\t\tlosses = []  # list of loss values for book-keeping\n\n\t\t\t# Run training on each batch\n\t\t\tfor _ in range(num_batches_train):\n\t\t\t\t# Obtain the training data and labels from generator\n\t\t\t\timages, y_true_conf_gen, y_true_loc_gen, conf_loss_mask_gen = next(train_gen)\n\n\t\t\t\t# Perform gradient update (i.e. training step) on current batch\n\t\t\t\t_, loss = sess.run([optimizer, reported_loss], feed_dict={\n\t\t\t\t#_, loss, loc_loss_dbg, loc_loss_mask, loc_loss = sess.run([optimizer, reported_loss, model[\'loc_loss_dbg\'], model[\'loc_loss_mask\'], model[\'loc_loss\']],feed_dict={  # DEBUG\n\t\t\t\t\tx: images,\n\t\t\t\t\ty_true_conf: y_true_conf_gen,\n\t\t\t\t\ty_true_loc: y_true_loc_gen,\n\t\t\t\t\tconf_loss_mask: conf_loss_mask_gen,\n\t\t\t\t\tis_training: True\n\t\t\t\t})\n\t\t\t\t\n\t\t\t\tlosses.append(loss)  # TODO: Need mAP metric instead of raw loss\n\n\t\t\t# A rough estimate of loss for this epoch (overweights the last batch)\n\t\t\ttrain_loss = np.mean(losses)\n\n\t\t\t# Calculate validation loss at the end of the epoch\n\t\t\tvalid_gen = next_batch(X_valid, y_valid_conf, y_valid_loc, BATCH_SIZE)\n\t\t\tnum_batches_valid = math.ceil(X_valid.shape[0] / BATCH_SIZE)\n\t\t\tlosses = []\n\t\t\tfor _ in range(num_batches_valid):\n\t\t\t\timages, y_true_conf_gen, y_true_loc_gen, conf_loss_mask_gen = next(valid_gen)\n\n\t\t\t\t# Perform forward pass and calculate loss\n\t\t\t\tloss = sess.run(reported_loss, feed_dict={\n\t\t\t\t\tx: images,\n\t\t\t\t\ty_true_conf: y_true_conf_gen,\n\t\t\t\t\ty_true_loc: y_true_loc_gen,\n\t\t\t\t\tconf_loss_mask: conf_loss_mask_gen,\n\t\t\t\t\tis_training: False\n\t\t\t\t})\n\t\t\t\tlosses.append(loss)\n\t\t\tvalid_loss = np.mean(losses)\n\n\t\t\t# Record and report train/validation/test losses for this epoch\n\t\t\tloss_history.append((train_loss, valid_loss))\n\n\t\t\t# Print accuracy every epoch\n\t\t\tprint(\'Epoch %d -- Train loss: %.4f, Validation loss: %.4f, Elapsed time: %.2f sec\' %\\\n\t\t\t\t(epoch+1, train_loss, valid_loss, time.time() - last_time))\n\t\t\tlast_time = time.time()\n\n\t\ttotal_time = time.time() - train_start_time\n\t\tprint(\'Total elapsed time: %d min %d sec\' % (total_time/60, total_time%60))\n\n\t\ttest_loss = 0.  # TODO: Add test set\n\t\t\'\'\'\n\t\t# After training is complete, evaluate accuracy on test set\n\t\tprint(\'Calculating test accuracy...\')\n\t\ttest_gen = next_batch(X_test, y_test, BATCH_SIZE)\n\t\ttest_size = X_test.shape[0]\n\t\ttest_acc = calculate_accuracy(test_gen, test_size, BATCH_SIZE, accuracy, x, y, keep_prob, sess)\n\t\tprint(\'Test acc.: %.4f\' % (test_acc,))\n\t\t\'\'\'\n\n\t\tif SAVE_MODEL:\n\t\t\t# Save model to disk\n\t\t\tsave_path = saver.save(sess, MODEL_SAVE_PATH)\n\t\t\tprint(\'Trained model saved at: %s\' % save_path)\n\n\t\t\t# Also save accuracy history\n\t\t\tprint(\'Loss history saved at loss_history.p\')\n\t\t\twith open(\'loss_history.p\', \'wb\') as f:\n\t\t\t\tpickle.dump(loss_history, f)\n\n\t# Return final test accuracy and accuracy_history\n\treturn test_loss, loss_history\n\n\nif __name__ == \'__main__\':\n\trun_training()\n'"
viz_model.py,4,"b""'''\nVisualize the model using TensorBoard\n'''\nimport tensorflow as tf\nfrom settings import *\nfrom model import SSDModel\n\nFM_ONLY = False  # Only want to see feature map sizes?\n\nwith tf.Graph().as_default(), tf.Session() as sess:\n\tif FM_ONLY:\n\t\t# Only want to see feature map sizes (e.g. loss function and vector concatenation not yet set up)\n\t\tif MODEL == 'AlexNet':\n\t\t\tfrom model import AlexNet as MyModel\n\t\telse:\n\t\t\traise NotImplementedError('Model %s not supported' % MODEL)\n\t\t_ = MyModel()\n\telse:\n\t\t# This includes the entire graph, e.g. loss function, optimizer, etc.\n\t\t_ = SSDModel()\n\n\ttf.summary.merge_all()\n\twriter = tf.summary.FileWriter('./tensorboard_out', sess.graph)\n\ttf.global_variables_initializer().run()\n"""
data_gathering/create_pickle.py,0,"b""'''\nCreate raw data pickle file\ndata_raw is a dict mapping image_filename -> [{'class': class_int, 'box_coords': (x1, y1, x2, y2)}, {...}, ...]\n'''\nimport numpy as np\nimport pickle\nimport re\nimport os\nfrom PIL import Image\n\n# Script config\nRESIZE_IMAGE = True  # resize the images and write to 'resized_images/'\nGRAYSCALE = True  # convert image to grayscale? this option is only valid if RESIZE_IMAGE==True (FIXME)\nTARGET_W, TARGET_H = 400, 260  # 1.74 is weighted avg ratio, but 1.65 aspect ratio is close enough (1.65 was for stop signs)\n\n###########################\n# Execute main script\n###########################\n\n# First get mapping from sign name string to integer label\nsign_map = {'stop': 1, 'pedestrianCrossing': 2}  # only 2 sign classes (background class is 0)\n'''\nsign_map = {}  # sign_name -> integer_label\nwith open('signnames.csv', 'r') as f:\n\tfor line in f:\n\t\tline = line[:-1]  # strip newline at the end\n\t\tinteger_label, sign_name = line.split(',')\n\t\tsign_map[sign_name] = int(integer_label)\n'''\n\n# Create raw data pickle file\ndata_raw = {}\n\n# For speed, put entire contents of mergedAnnotations.csv in memory\nmerged_annotations = []\nwith open('mergedAnnotations.csv', 'r') as f:\n\tfor line in f:\n\t\tline = line[:-1]  # strip trailing newline\n\t\tmerged_annotations.append(line)\n\n# Create pickle file to represent dataset\nimage_files = os.listdir('annotations')\nfor image_file in image_files:\n\t# Find box coordinates for all signs in this image\n\tclass_list = []\n\tbox_coords_list = []\n\tfor line in merged_annotations:\n\t\tif re.search(image_file, line):\n\t\t\tfields = line.split(';')\n\n\t\t\t# Get sign name and assign class label\n\t\t\tsign_name = fields[1]\n\t\t\tif sign_name != 'stop' and sign_name != 'pedestrianCrossing':\n\t\t\t\tcontinue  # ignore signs that are neither stop nor pedestrianCrossing signs\n\t\t\tsign_class = sign_map[sign_name]\n\t\t\tclass_list.append(sign_class)\n\n\t\t\t# Resize image, get rescaled box coordinates\n\t\t\tbox_coords = np.array([int(x) for x in fields[2:6]])\n\n\t\t\tif RESIZE_IMAGE:\n\t\t\t\t# Resize the images and write to 'resized_images/'\n\t\t\t\timage = Image.open('annotations/' + image_file)\n\t\t\t\torig_w, orig_h = image.size\n\n\t\t\t\tif GRAYSCALE:\n\t\t\t\t\timage = image.convert('L')  # 8-bit grayscale\n\t\t\t\timage = image.resize((TARGET_W, TARGET_H), Image.LANCZOS)  # high-quality downsampling filter\n\n\t\t\t\tresized_dir = 'resized_images_%dx%d/' % (TARGET_W, TARGET_H)\n\t\t\t\tif not os.path.exists(resized_dir):\n\t\t\t\t\tos.makedirs(resized_dir)\n\n\t\t\t\timage.save(os.path.join(resized_dir, image_file))\n\n\t\t\t\t# Rescale box coordinates\n\t\t\t\tx_scale = TARGET_W / orig_w\n\t\t\t\ty_scale = TARGET_H / orig_h\n\n\t\t\t\tulc_x, ulc_y, lrc_x, lrc_y = box_coords\n\t\t\t\tnew_box_coords = (ulc_x * x_scale, ulc_y * y_scale, lrc_x * x_scale, lrc_y * y_scale)\n\t\t\t\tnew_box_coords = [round(x) for x in new_box_coords]\n\t\t\t\tbox_coords = np.array(new_box_coords)\n\n\t\t\tbox_coords_list.append(box_coords)\n\n\tif len(class_list) == 0:\n\t\tcontinue  # ignore images with no signs-of-interest\n\tclass_list = np.array(class_list)\n\tbox_coords_list = np.array(box_coords_list)\n\n\t# Create the list of dicts\n\tthe_list = []\n\tfor i in range(len(box_coords_list)):\n\t\td = {'class': class_list[i], 'box_coords': box_coords_list[i]}\n\t\tthe_list.append(d)\n\n\tdata_raw[image_file] = the_list\n\nwith open('data_raw_%dx%d.p' % (TARGET_W, TARGET_H), 'wb') as f:\n\tpickle.dump(data_raw, f)\n"""
