file_path,api_count,code
script/download_dataset.py,0,"b'#!/usr/bin/env python\n""""""\nUsage:\n    download_dataset.py DESTINATION_DIR\n\nOptions:\n    -h --help   Show this screen.\n""""""\n\nimport os\nfrom subprocess import call\n\nfrom docopt import docopt\n\n\nif __name__ == \'__main__\':\n    args = docopt(__doc__)\n\n    destination_dir = os.path.abspath(args[\'DESTINATION_DIR\'])\n    if not os.path.exists(destination_dir):\n        os.makedirs(destination_dir)\n    os.chdir(destination_dir)\n\n    for language in (\'python\', \'javascript\', \'java\', \'ruby\', \'php\', \'go\'):\n        call([\'wget\', \'https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/{}.zip\'.format(language), \'-P\', destination_dir, \'-O\', \'{}.zip\'.format(language)])\n        call([\'unzip\', \'{}.zip\'.format(language)])\n        call([\'rm\', \'{}.zip\'.format(language)])\n'"
src/__init__.py,0,b''
src/error_analysis.py,0,"b'#!/usr/bin/env python\r\n""""""\r\nUsage:\r\n    error_analysis.py [options] MODEL_PATH (--standard-dataset | --method2code-dataset) DATA_PATH OUT_FILE\r\n\r\nOptions:\r\n    -h --help                        Show this screen.\r\n    --max-num-epochs EPOCHS          The maximum number of epochs to run [default: 300]\r\n    --max-num-files INT              Number of files to load.\r\n    --max-num-examples INT           Randomly sample examples from the dataset to display.\r\n    --hypers-override HYPERS         JSON dictionary overriding hyperparameter values.\r\n    --hypers-override-file FILE      JSON file overriding hyperparameter values.\r\n    --test-batch-size SIZE           The size of the batches in which to compute MRR. [default: 1000]\r\n    --distance-metric METRIC         The distance metric to use [default: cosine]\r\n    --quiet                          Less output (not one per line per minibatch). [default: False]\r\n    --azure-info PATH                Azure authentication information file (JSON). Used to load data from Azure storage.\r\n    --debug                          Enable debug routines. [default: False]\r\n    --standard-dataset               The DATA_PATH is to a standard dataset.\r\n    --method2code-dataset            The DATA_PATH is to a standard dataset but will be used for method2code tasks.\r\n    --language-to-analyze LANG       The language to analyze. Defaults to all.\r\n""""""\r\nimport io\r\nimport json\r\nfrom typing import List, Dict, Any, Optional\r\nfrom tqdm import tqdm\r\n\r\nfrom pygments import highlight\r\nfrom pygments.lexers import get_lexer_by_name\r\nfrom pygments.formatters import HtmlFormatter\r\nfrom docopt import docopt\r\nfrom dpu_utils.utils import run_and_debug, RichPath\r\n\r\nimport model_test\r\nfrom model_test import expand_data_path, MrrSearchTester\r\nfrom random import sample\r\n\r\n\r\n## Default Bootstrap headers\r\nHEADER=f""""""\r\n<!doctype html>\r\n<html lang=""en"">\r\n  <head>\r\n    <!-- Required meta tags -->\r\n    <meta charset=""utf-8"">\r\n    <meta name=""viewport"" content=""width=device-width, initial-scale=1, shrink-to-fit=no"">\r\n\r\n    <!-- Bootstrap CSS -->\r\n    <link rel=""stylesheet"" href=""https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css"" integrity=""sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO"" crossorigin=""anonymous"">\r\n\r\n    <title>Error Analysis</title>\r\n    \r\n    <style>\r\n        {HtmlFormatter().get_style_defs(\'.highlight\')}\r\n    </style>\r\n  </head>\r\n  <body>\r\n""""""\r\nFOOTER=""""""\r\n</body></html>\r\n""""""\r\n\r\ndef to_highlighted_html(code:str, language: str) -> str:\r\n    lexer = get_lexer_by_name(language, stripall=True)\r\n    formatter = HtmlFormatter(linenos=True)\r\n    return highlight(code, lexer, formatter)\r\n\r\ndef generate_html_error_report(tester: MrrSearchTester,\r\n                               data:  List[Dict[str, Any]],\r\n                               max_num_examples: Optional[int],\r\n                               outfile: str,\r\n                               filter_language: Optional[str] = None) -> None:\r\n\r\n    error_log = []  # type: List[MrrSearchTester.QueryResult]\r\n    # Sample the data if requested\r\n    data = sample_data(data=data,\r\n                       max_num_examples=max_num_examples)\r\n\r\n    # generate error logs\r\n    tester.update_test_batch_size(max_num_examples)\r\n    tester.evaluate(data, \'Error Analysis Run\', error_log, filter_language=filter_language)\r\n\r\n    ""Generates HTML Report of Errors.""\r\n    print(\'Generating Report\')\r\n    with open(outfile, \'w\') as f:\r\n        f.write(HEADER)\r\n        for query_result in tqdm(error_log, total=len(error_log)):\r\n            with io.StringIO() as sb:\r\n                target_code = data[query_result.target_idx][\'code\']\r\n                target_query = data[query_result.target_idx][\'docstring\'].replace(\'\\n\', \' \')\r\n                language = data[query_result.target_idx][\'language\']\r\n                sb.write(f\'<h2> Query: ""{target_query}""</h2>\\n\\n\')\r\n                sb.write(f\'<strong>Target Snippet</strong>\\n{to_highlighted_html(target_code, language=language)}\\n\')\r\n                sb.write(f\'Target snippet was ranked at position <strong>{query_result.target_rank}</strong>.\\n\')\r\n\r\n                sb.write(\'<div class=""row"">\\n\')\r\n                for pos, sample_idx in enumerate(query_result.top_ranked_idxs):\r\n                    sb.write(\'<div class=""col-sm"">\\n\')\r\n                    sb.write(f\'<strong>Result at {pos+1}</strong>\\n\')\r\n                    sb.write(f\'{data[sample_idx][""repo""]} {data[sample_idx][""path""]}:{data[sample_idx][""lineno""]}\\n\')\r\n                    result_docstring = data[sample_idx][\'docstring\']\r\n                    result_code = data[sample_idx][\'code\']\r\n                    lang = data[sample_idx][\'language\']\r\n                    sb.write(f\'<blockquote><p>  Docstring: <em>{result_docstring}</em></blockquote>\\n{to_highlighted_html(result_code, language=lang)}\\n\\n\')\r\n                    sb.write(\'</div>\\n\')\r\n                sb.write(\'</div>\\n<hr/>\\n\')\r\n                f.write(sb.getvalue())\r\n        f.write(FOOTER)\r\n\r\n\r\ndef sample_data(data: List[Dict[str, Any]],\r\n                max_num_examples: Optional[int]) -> List[Dict[str, Any]]:\r\n    """"""\r\n    Sample max_num_examples from the data.\r\n\r\n    Args:\r\n        data: List[Dict[str, Any]]\r\n        max_num_examples:  either an int or if a string will attempt conversion to an int.\r\n\r\n    Returns:\r\n        data: List[Dict[str, Any]]\r\n    """"""\r\n    if max_num_examples:\r\n        num_elements = min(len(data), max_num_examples)\r\n        print(f\'Extracting {num_elements} random samples from dataset.\')\r\n        data = sample(data, num_elements)\r\n\r\n    return data\r\n\r\n\r\ndef run(arguments):\r\n    max_num_examples = int(arguments.get(\'--max-num-examples\')) if arguments.get(\'--max-num-examples\') else None\r\n    azure_info_path = arguments.get(\'--azure-info\', None)\r\n    test_data_dirs = expand_data_path(arguments[\'DATA_PATH\'], azure_info_path)\r\n\r\n    if arguments[\'--hypers-override\'] is not None:\r\n        hypers_override = json.loads(arguments[\'--hypers-override\'])\r\n    elif arguments[\'--hypers-override-file\'] is not None:\r\n        with open(arguments[\'--hypers-override-file\']) as f:\r\n            hypers_override = json.load(f)\r\n    else:\r\n        hypers_override = {}\r\n\r\n    model_path = RichPath.create(arguments[\'MODEL_PATH\'], azure_info_path=azure_info_path)\r\n\r\n    tester = MrrSearchTester(model_path, test_batch_size=int(arguments[\'--test-batch-size\']),\r\n                             distance_metric=arguments[\'--distance-metric\'], hypers_override=hypers_override)\r\n\r\n    # Load dataset\r\n    if arguments[\'--standard-dataset\'] or arguments[\'--method2code-dataset\']:\r\n        data = model_test.get_dataset_from(test_data_dirs, use_func_names=arguments[\'--method2code-dataset\'])\r\n    else:\r\n        raise Exception(f\'No dataset option seems to have been passed in.\')\r\n\r\n    generate_html_error_report(tester=tester,\r\n                               data=data,\r\n                               max_num_examples=max_num_examples,\r\n                               outfile=arguments[\'OUT_FILE\'],\r\n                               filter_language=arguments.get(\'--language-to-analyze\'))\r\n\r\nif __name__ == \'__main__\':\r\n    args = docopt(__doc__)\r\n    run_and_debug(lambda: run(args), args.get(\'--debug\', False))\r\n'"
src/model_restore_helper.py,3,"b'from typing import Dict, Any, Optional, Type\n\nimport tensorflow as tf\nfrom dpu_utils.utils import RichPath\n\nfrom models import Model, NeuralBoWModel, RNNModel, SelfAttentionModel, ConvolutionalModel, ConvSelfAttentionModel\n\n\ndef get_model_class_from_name(model_name: str) -> Type[Model]:\n    model_name = model_name.lower()\n    if model_name in [\'neuralbow\', \'neuralbowmodel\']:\n        return NeuralBoWModel\n    elif model_name in [\'rnn\', \'rnnmodel\']:\n        return RNNModel\n    elif model_name in {\'selfatt\', \'selfattention\', \'selfattentionmodel\'}:\n        return SelfAttentionModel\n    elif model_name in {\'1dcnn\', \'convolutionalmodel\'}:\n        return ConvolutionalModel\n    elif model_name in {\'convselfatt\', \'convselfattentionmodel\'}:\n        return ConvSelfAttentionModel\n    else:\n        raise Exception(""Unknown model \'%s\'!"" % model_name)\n\n\ndef restore(path: RichPath, is_train: bool, hyper_overrides: Optional[Dict[str, Any]]=None) -> Model:\n    saved_data = path.read_as_pickle()\n\n    if hyper_overrides is not None:\n        saved_data[\'hyperparameters\'].update(hyper_overrides)\n\n    model_class = get_model_class_from_name(saved_data[\'model_type\'])\n    model = model_class(saved_data[\'hyperparameters\'], saved_data.get(\'run_name\'))\n    model.query_metadata.update(saved_data[\'query_metadata\'])\n    for (language, language_metadata) in saved_data[\'per_code_language_metadata\'].items():\n        model.per_code_language_metadata[language] = language_metadata\n    model.make_model(is_train=is_train)\n\n    variables_to_initialize = []\n    with model.sess.graph.as_default():\n        with tf.name_scope(""restore""):\n            restore_ops = []\n            used_vars = set()\n            for variable in sorted(model.sess.graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES), key=lambda v: v.name):\n                used_vars.add(variable.name)\n                if variable.name in saved_data[\'weights\']:\n                    # print(\'Initializing %s from saved value.\' % variable.name)\n                    restore_ops.append(variable.assign(saved_data[\'weights\'][variable.name]))\n                else:\n                    print(\'Freshly initializing %s since no saved value was found.\' % variable.name)\n                    variables_to_initialize.append(variable)\n            for var_name in sorted(saved_data[\'weights\']):\n                if var_name not in used_vars:\n                    if var_name.endswith(\'Adam:0\') or var_name.endswith(\'Adam_1:0\') or var_name in [\'beta1_power:0\', \'beta2_power:0\']:\n                        continue\n                    print(\'Saved weights for %s not used by model.\' % var_name)\n            restore_ops.append(tf.variables_initializer(variables_to_initialize))\n            model.sess.run(restore_ops)\n    return model\n'"
src/model_test.py,0,"b'from collections import defaultdict\nfrom itertools import chain\nfrom typing import Optional, List, Dict, Any, NamedTuple, Iterable, Tuple\nimport logging\nimport random\n\nfrom dpu_utils.mlutils import Vocabulary\nfrom dpu_utils.utils import RichPath\nimport numpy as np\nfrom more_itertools import chunked, flatten\nfrom scipy.spatial.distance import cdist\nimport wandb\n\nimport model_restore_helper\nfrom models.model import get_data_files_from_directory, Model\nfrom dataextraction.python.parse_python_data import tokenize_python_from_string\nfrom dataextraction.utils import tokenize_docstring_from_string\nfrom dpu_utils.codeutils import split_identifier_into_parts\n\n\ndef compute_ranks(src_representations: np.ndarray,\n                  tgt_representations: np.ndarray,\n                  distance_metric: str) -> Tuple[np.array, np.array]:\n    distances = cdist(src_representations, tgt_representations,\n                      metric=distance_metric)\n    # By construction the diagonal contains the correct elements\n    correct_elements = np.expand_dims(np.diag(distances), axis=-1)\n    return np.sum(distances <= correct_elements, axis=-1), distances\n\n\nclass MrrSearchTester:\n    def __init__(self, model_path: RichPath, test_batch_size: int=1000, distance_metric: str=\'cosine\',\n                 quiet: bool=False, hypers_override: Optional[Dict[str, Any]]=None) -> None:\n        self.__model = model_restore_helper.restore(path=model_path,\n                                                    is_train=False,\n                                                    hyper_overrides=hypers_override)\n        self.__quiet = quiet\n        self.__test_batch_size = test_batch_size\n        self.__distance_metric = distance_metric\n\n    @property\n    def model(self) -> Model:\n        return self.__model\n\n    @property\n    def test_batch_size(self)-> int:\n        return self.__test_batch_size\n\n    def update_test_batch_size(self, test_batch_size: int)-> None:\n        self.__test_batch_size = test_batch_size\n\n    QueryResult = NamedTuple(\'QueryResult\', [\n        (\'target_idx\', int),\n        (\'target_rank\', int),\n        (\'top_ranked_idxs\', List[int])\n    ])\n\n    def evaluate(self, data: List[Dict[str, Any]], data_label_name: str,\n                 error_log: Optional[List[\'MrrSearchTester.QueryResult\']]=None,\n                 error_log_rank_threshold: int=10,\n                 filter_language: Optional[str]=None)-> float:\n        """"""\n        Evaluate the MRR on the given dataset.\n\n        :param data: the data to test on.\n        :param data_label_name: A label used when printing the result output.\n        :param error_log: If not null, store in the log, results where the target rank is above the threshold.\n        :param error_log_rank_threshold: The threshold for logging into error_log (used only if error_log is not None)\n        :return: the mean reciprocal rank (MRR) score\n        """"""\n        assert len(data) > 0, \'data must have more than 0 rows.\'\n        np.random.seed(0)  # set random seed so that random things are reproducible\n\n        if filter_language is None:\n            idxs = np.arange(len(data))\n        else:\n            idxs = np.array([i for i in range(len(data)) if data[i][\'language\'] == filter_language])\n        if len(idxs) == 0:\n            print(\'Warning: Trying to test on empty dataset. Skipping.\')\n            return float(\'nan\')\n        data = np.array(data, dtype=np.object)\n        np.random.shuffle(idxs)\n        data = data[idxs]\n\n        if len(data) < self.__test_batch_size:\n            logging.warning(f\'the size of the total data {len(data):,} is less than the batch_size: {self.__test_batch_size:,} adjusting batch size to equal data size.\')\n            self.update_test_batch_size(len(data))\n\n        def self_or_random_representation(representation: Optional[np.ndarray]) -> np.ndarray:\n            if representation is not None:\n                return representation\n            else:\n                return np.random.randn(self.__model.representation_size)\n\n        # Determine random sample of examples before chunking into batches.\n        # sample only from full batches\n        max_samples = 50\n        full_batch_len = len(data) // self.__test_batch_size * self.__test_batch_size\n        examples_sample = np.zeros(len(data), dtype=bool)\n        examples_sample[np.random.choice(np.arange(full_batch_len), replace=False, size=min(full_batch_len, max_samples))] = True\n        examples_table = []\n\n        sum_mrr = 0.0\n        num_batches = 0\n        batched_data = chunked(data, self.__test_batch_size)\n        batched_sample = chunked(examples_sample, self.__test_batch_size)\n        for batch_idx, (batch_data, batch_sample) in enumerate(zip(batched_data, batched_sample)):\n            if len(batch_data) < self.__test_batch_size:\n                break  # the last batch is smaller than the others, exclude.\n            num_batches += 1\n\n            code_representations = self.__model.get_code_representations(batch_data)\n            query_representations = self.__model.get_query_representations(batch_data)\n            assert len(code_representations) == len(query_representations) == self.__test_batch_size\n\n            # Construct numpy batch\n            num_uncomputed_representations = sum(1 for i in range(self.__test_batch_size)\n                                                 if code_representations[i] is None or query_representations[i] is None)\n            if num_uncomputed_representations > 0:\n                print(f\'Ignoring {num_uncomputed_representations} samples whose representation could not be computed\')\n\n            # Design decision: If a representation cannot be computed assign a random representation. This keeps\n            # the batch size identical across all models.\n            batch_code_representations = np.array(\n                [self_or_random_representation(code_representations[i]) for i in range(self.__test_batch_size)],\n                dtype=np.float32)\n            batch_query_representations = np.array(\n                [self_or_random_representation(query_representations[i]) for i in range(self.__test_batch_size)],\n                dtype=np.float32)\n\n            ranks, distances = compute_ranks(batch_code_representations,\n                                             batch_query_representations,\n                                             self.__distance_metric)\n\n            # Log example tables for a sample of rankings of queries for each dataset\n            if wandb.run:\n                examples_table_name = data_label_name.rstrip(""-All"")\n                examples_table_columns = [""Rank"", ""Language"", ""Query"", ""Code""]\n                for example, sample, rank in zip(batch_data, batch_sample, ranks):\n                    if not sample:\n                        continue\n                    language = example[\'language\']\n                    markdown_code = ""```%s\\n"" % language + example[\'code\'].strip(""\\n"") + ""\\n```""\n                    examples_table.append([rank, language, example[\'func_name\'], markdown_code])\n\n            sum_mrr += np.mean(1.0 / ranks)\n\n            if error_log is not None:\n                batch_sample_idxs = idxs[batch_idx*self.__test_batch_size:(batch_idx+1)*self.__test_batch_size]\n                for i in range(len(ranks)):\n                    if ranks[i] >= error_log_rank_threshold:\n                        result = MrrSearchTester.QueryResult(\n                            target_idx=batch_sample_idxs[i],\n                            target_rank=ranks[i],\n                            top_ranked_idxs=batch_sample_idxs[np.argsort(distances[i])[:3]]\n                        )\n                        error_log.append(result)\n\n            if self.__quiet and batch_idx % 100 == 99:\n                print(f\'Tested on {batch_idx + 1} batches so far.\')\n\n        if wandb.run and examples_table:\n            wandb.log({""Examples-%s"" % examples_table_name: wandb.Table(columns=examples_table_columns, rows=examples_table)})\n\n        eval_mrr = sum_mrr / num_batches\n        log_label = f\'{data_label_name} MRR (bs={self.__test_batch_size:,})\'\n        print(f\'{log_label}: {eval_mrr: .3f}\')\n        if wandb.run:\n            wandb.run.summary[f\'{log_label}\'] = eval_mrr\n        return eval_mrr\n\n\ndef expand_data_path(data_path: str, azure_info_path: Optional[str]) -> List[RichPath]:\n    """"""\n    Args:\n        data_path: A path to either a file or a directory. If it\'s a file, we interpret it as a list of\n            data directories.\n\n    Returns:\n        List of data directories (potentially just data_path)\n    """"""\n    data_rpath = RichPath.create(data_path, azure_info_path)\n\n    if data_rpath.is_dir():\n        return [data_rpath]\n\n    return [RichPath.create(data_dir, azure_info_path)\n            for data_dir in data_rpath.read_as_text().splitlines()]\n\n\ndef filter_untokenizable_code(data: Iterable[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    """"""Filter out data where field code_tokens is empty.""""""\n    return [d for d in data if d[\'code_tokens\']]\n\ndef log_row_count_diff(original_data: Iterable[Any], filtered_data:Iterable[Any], label: str) -> None:\n    """"""Compute the difference between row counts and log appropriately.""""""\n    original_row_count = len(list(original_data))\n    filtered_row_count = len(list(filtered_data))\n\n    assert original_row_count > 0, \'original_data does not contain any rows.\'\n    assert filtered_row_count <= original_row_count, f\'filtered_data {filtered_row_count:,} has a larger row count than original_data {original_row_count:,}.\'\n\n    pcnt_parsed = filtered_row_count / original_row_count\n    print(f\'{label}: parsed {filtered_row_count:,} out of {original_row_count:,} rows. ({pcnt_parsed*100:.1f}%)\')\n    if wandb.run:\n        wandb.run.summary[f\'{label} Parsed Pct\'] = pcnt_parsed\n\n\ndef get_dataset_from(data_dirs: List[RichPath], \n                     use_func_names: bool=False, \n                     max_files_per_dir: Optional[int] = None) -> List[Dict[str, Any]]:\n    data_files = sorted(get_data_files_from_directory(data_dirs, max_files_per_dir))\n    data = list(chain(*chain(list(f.read_by_file_suffix()) for f in data_files)))\n\n    if use_func_names:\n        # This task tries to match the function name to the code, by setting the function name as the query\n        for sample in data:\n            # Replace the query tokens with the function name, broken up into its sub-tokens:\n            sample[\'docstring_tokens\'] = split_identifier_into_parts(sample[\'func_name\'])\n\n            # In the code, replace the function name with the out-of-vocab token everywhere it appears:\n            sample[\'code_tokens\'] = [Vocabulary.get_unk() if token == sample[\'func_name\'] else token\n                                     for token in sample[\'code_tokens\']]\n    return data\n\n\ndef compute_evaluation_metrics(model_path: RichPath, arguments, \n                               azure_info_path: str,\n                               valid_data_dirs: List[RichPath], \n                               test_data_dirs: List[RichPath],\n                               max_files_per_dir: Optional[int] = None):\n\n    tester = MrrSearchTester(model_path, test_batch_size=int(arguments[\'--test-batch-size\']),\n                                  distance_metric=arguments[\'--distance-metric\'])\n    test_data = get_dataset_from(test_data_dirs, max_files_per_dir=max_files_per_dir)\n    # Get all languages in test_data\n    dataset_languages = set(d[\'language\'] for d in test_data)\n    evaluation_sets = list((l, True) for l in dataset_languages)  # type: List[Tuple[str, bool]]\n    if set(tester.model.per_code_language_metadata.keys()) == dataset_languages:\n        evaluation_sets = [(\'All\', False)] + evaluation_sets\n    final_eval = {}  # type: Dict[str, float]\n    for language_name, filter_language in evaluation_sets:\n        if filter_language and language_name not in tester.model.per_code_language_metadata:\n            continue\n        mrr = tester.evaluate(test_data, f\'Test-{language_name}\', filter_language=language_name if filter_language else None)\n        if language_name == ""All"":\n            final_eval[\'Primary MRR\'] = mrr\n\n        # run test using the function name as the query\n        mrr = tester.evaluate(get_dataset_from(test_data_dirs, use_func_names=True, max_files_per_dir=max_files_per_dir), f\'FuncNameTest-{language_name}\',\n                              filter_language=language_name if filter_language else None)\n        if language_name == ""All"":\n            final_eval[\'FuncName MRR\'] = mrr\n\n        # run the test procedure on the validation set (with same batch size as test, so that MRR is comparable)\n        tester.evaluate(get_dataset_from(valid_data_dirs, max_files_per_dir=max_files_per_dir), f\'Validation-{language_name}\',\n                        filter_language=language_name if filter_language else None)\n\n    if wandb.run and final_eval:\n        wandb.run.summary[\'Eval\'] = final_eval\n'"
src/predict.py,0,"b'#!/usr/bin/env python\n""""""\nRun predictions on a CodeSearchNet model.\n\nUsage:\n    predict.py -m MODEL_FILE [-p PREDICTIONS_CSV]\n    predict.py -r RUN_ID     [-p PREDICTIONS_CSV]\n    predict.py -h | --help\n\nOptions:\n    -h --help                       Show this screen\n    -m, --model_file FILENAME       Local path to a saved model file (filename.pkl.gz)\n    -r, --wandb_run_id RUN_ID       wandb run ID, [username]/codesearchnet/[hash string id], viewable from run overview page via info icon\n    -p, --predictions_csv FILENAME  CSV filename for model predictions (note: W&B benchmark submission requires the default name)\n                                    [default: ../resources/model_predictions.csv]\n\nExamples:\n    ./predict.py -r username/codesearchnet/0123456\n    ./predict.py -m ../resources/saved_models/neuralbowmodel-2019-10-31-12-00-00_model_best.pkl.gz\n""""""\n\n""""""\nThis script tests a model on the CodeSearchNet Challenge, given\n- a particular model as a local file (-m, --model_file MODEL_FILENAME.pkl.gz), OR\n- as a Weights & Biases run id (-r, --wandb_run_id [username]/codesearchnet/0123456), which you can find\non the /overview page or by clicking the \'info\' icon on a given run.\nRun with ""-h"" to see full command line options.\nNote that this takes around 2 hours to make predictions on the baseline model.\n\nThis script generates ranking results over the CodeSearchNet corpus for a given model by scoring their relevance\n(using that model) to 99 search queries of the CodeSearchNet Challenge. We use cosine distance between the learned \nrepresentations of the natural language queries and the code, which is stored in jsonlines files with this format:\nhttps://github.com/github/CodeSearchNet#preprocessed-data-format. The 99 challenge queries are located in \nthis file: https://github.com/github/CodeSearchNet/blob/master/resources/queries.csv. \nTo download the full CodeSearchNet corpus, see the README at the root of this repository.\n\nNote that this script is specific to methods and code in our baseline model and may not generalize to new models. \nWe provide it as a reference and in order to be transparent about our baseline submission to the CodeSearchNet Challenge.\n\nThis script produces a CSV file of model predictions with the following fields: \'query\', \'language\', \'identifier\', and \'url\':\n      * language: the programming language for the given query, e.g. ""python"".  This information is available as a field in the data to be scored.\n      * query: the textual representation of the query, e.g. ""int to string"" .  \n      * identifier: this is an optional field that can help you track your data\n      * url: the unique GitHub URL to the returned results, e.g. ""https://github.com/JamesClonk/vultr/blob/fed59ad207c9bda0a5dfe4d18de53ccbb3d80c91/cmd/commands.go#L12-L190"". This information is available as a field in the data to be scored.\n\nThe schema of the output CSV file constitutes a valid submission to the CodeSearchNet Challenge hosted on Weights & Biases. See further background and instructions on the submission process in the root README.\n\nThe row order corresponds to the result ranking in the search task. For example, if in row 5 there is an entry for the Python query ""read properties file"", and in row 60 another result for the Python query ""read properties file"", then the URL in row 5 is considered to be ranked higher than the URL in row 60 for that query and language.\n""""""\n\nimport pickle\nimport re\nimport shutil\nimport sys\n\nfrom annoy import AnnoyIndex\nfrom docopt import docopt\nfrom dpu_utils.utils import RichPath\nimport pandas as pd\nfrom tqdm import tqdm\nimport wandb\nfrom wandb.apis import InternalApi\n\nfrom dataextraction.python.parse_python_data import tokenize_docstring_from_string\nimport model_restore_helper\n\ndef query_model(query, model, indices, language, topk=100):\n    query_embedding = model.get_query_representations([{\'docstring_tokens\': tokenize_docstring_from_string(query),\n                                                        \'language\': language}])[0]\n    idxs, distances = indices.get_nns_by_vector(query_embedding, topk, include_distances=True)\n    return idxs, distances\n\n\nif __name__ == \'__main__\':\n    args = docopt(__doc__)\n    \n    queries = pd.read_csv(\'../resources/queries.csv\')\n    queries = list(queries[\'query\'].values)\n\n    run_id = None\n    args_wandb_run_id = args.get(\'--wandb_run_id\')\n    local_model_path = args.get(\'--model_file\')\n    predictions_csv = args.get(\'--predictions_csv\')\n\n    if args_wandb_run_id:\n        # validate format of runid:\n        if len(args_wandb_run_id.split(\'/\')) != 3:\n            print(""ERROR: Invalid wandb_run_id format: %s (Expecting: user/project/hash)"" % args_wandb_run_id, file=sys.stderr)\n            sys.exit(1)\n        wandb_api = wandb.Api()\n        # retrieve saved model from W&B for this run\n        print(""Fetching run from W&B..."")\n        try:\n            run = wandb_api.run(args_wandb_run_id)\n        except wandb.CommError as e:\n            print(""ERROR: Problem querying W&B for wandb_run_id: %s"" % args_wandb_run_id, file=sys.stderr)\n            sys.exit(1)\n\n        print(""Fetching run files from W&B..."")\n        gz_run_files = [f for f in run.files() if f.name.endswith(\'gz\')]\n        if not gz_run_files:\n            print(""ERROR: Run contains no model-like files"")\n            sys.exit(1)\n        model_file = gz_run_files[0].download(replace=True)\n        local_model_path = model_file.name\n        run_id = args_wandb_run_id.split(\'/\')[-1]\n\n    model_path = RichPath.create(local_model_path, None)\n    print(""Restoring model from %s"" % model_path)\n    model = model_restore_helper.restore(\n        path=model_path,\n        is_train=False,\n        hyper_overrides={})\n    \n    predictions = []\n    for language in (\'python\', \'go\', \'javascript\', \'java\', \'php\', \'ruby\'):\n        print(""Evaluating language: %s"" % language)\n        definitions = pickle.load(open(\'../resources/data/{}_dedupe_definitions_v2.pkl\'.format(language), \'rb\'))\n        indexes = [{\'code_tokens\': d[\'function_tokens\'], \'language\': d[\'language\']} for d in tqdm(definitions)]\n        code_representations = model.get_code_representations(indexes)\n\n        indices = AnnoyIndex(code_representations[0].shape[0], \'angular\')\n        for index, vector in tqdm(enumerate(code_representations)):\n            if vector is not None:\n                indices.add_item(index, vector)\n        indices.build(200)\n\n        for query in queries:\n            for idx, _ in zip(*query_model(query, model, indices, language)):\n                predictions.append((query, language, definitions[idx][\'identifier\'], definitions[idx][\'url\']))\n\n    df = pd.DataFrame(predictions, columns=[\'query\', \'language\', \'identifier\', \'url\'])\n    df.to_csv(predictions_csv, index=False)\n\n\n    if run_id:\n        print(\'Uploading predictions to W&B\')\n        # upload model predictions CSV file to W&B\n\n        # we checked that there are three path components above\n        entity, project, name = args_wandb_run_id.split(\'/\')\n\n        # make sure the file is in our cwd, with the correct name\n        predictions_base_csv = ""model_predictions.csv""\n        shutil.copyfile(predictions_csv, predictions_base_csv)\n\n        # Using internal wandb API. TODO: Update when available as a public API\n        internal_api = InternalApi()\n        internal_api.push([predictions_base_csv], run=name, entity=entity, project=project)\n'"
src/relevanceeval.py,0,"b'#!/usr/bin/env python\r\n""""""\r\nUsage:\r\n    relevanceeval.py [options] RELEVANCE_ANNOTATIONS_CSV_PATH MODEL_PREDICTIONS_CSV\r\n\r\nStandalone relevance evaluation script that outputs evaluation statistics for a set of predictions of a given model.\r\nThe input formats of the files is described below.\r\n\r\nThe model predictions MODEL_PREDICTIONS_CSV file has the following format:\r\n    A comma-separated file with (at least) the fields and headers ""language"", ""query"", ""url"". Each row represents\r\n    a single result for a given query and a given programming language.\r\n\r\n    * language: the programming language for the given query, e.g. ""python""\r\n    * query: the textual representation of the query, e.g. ""int to string""\r\n    * url: the unique GitHub URL to the returned results, e.g. ""https://github.com/JamesClonk/vultr/blob/fed59ad207c9bda0a5dfe4d18de53ccbb3d80c91/cmd/commands.go#L12-L190""\r\n\r\n     The order of the rows imply the ranking of the results in the search task. For example, if in row 5 there is\r\n     an entry for the Python query ""read properties file"" and then in row 60 appears another result for the\r\n     Python query ""read properties file"", then the URL in row 5 is considered to be ranked higher than the\r\n     URL in row 60 for that query and language.\r\n\r\nOptions:\r\n    --debug                          Run in debug mode, falling into pdb on exceptions.\r\n    -h --help                        Show this screen.\r\n""""""\r\nfrom collections import defaultdict\r\nfrom typing import Dict, List\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom docopt import docopt\r\nfrom dpu_utils.utils import run_and_debug\r\n\r\ndef load_relevances(filepath: str) -> Dict[str, Dict[str, Dict[str, float]]]:\r\n    relevance_annotations = pd.read_csv(filepath)\r\n    per_query_language = relevance_annotations.pivot_table(\r\n        index=[\'Query\', \'Language\', \'GitHubUrl\'], values=\'Relevance\', aggfunc=np.mean)\r\n\r\n    # Map language -> query -> url -> float\r\n    relevances = defaultdict(lambda: defaultdict(dict))  # type: Dict[str, Dict[str, Dict[str, float]]]\r\n    for (query, language, url), relevance in per_query_language[\'Relevance\'].items():\r\n        relevances[language.lower()][query.lower()][url] = relevance\r\n    return relevances\r\n\r\ndef load_predictions(filepath: str, max_urls_per_language: int=300) -> Dict[str, Dict[str, List[str]]]:\r\n    prediction_data = pd.read_csv(filepath)\r\n\r\n    # Map language -> query -> Ranked List of URL\r\n    predictions = defaultdict(lambda: defaultdict(list))\r\n    for _, row in prediction_data.iterrows():\r\n        predictions[row[\'language\'].lower()][row[\'query\'].lower()].append(row[\'url\'])\r\n    for query_data in predictions.values():\r\n        for query, ranked_urls in query_data.items():\r\n            query_data[query] = ranked_urls[:max_urls_per_language]\r\n\r\n    return predictions\r\n\r\ndef coverage_per_language(predictions: Dict[str, List[str]],\r\n                          relevance_scores: Dict[str, Dict[str, float]], with_positive_relevance: bool=False) -> float:\r\n    """"""\r\n    Compute the % of annotated URLs that appear in the algorithm\'s predictions.\r\n    """"""\r\n    num_annotations = 0\r\n    num_covered = 0\r\n    for query, url_data in relevance_scores.items():\r\n        urls_in_predictions = set(predictions[query])\r\n        for url, relevance in url_data.items():\r\n            if not with_positive_relevance or relevance > 0:\r\n                num_annotations += 1\r\n                if url in urls_in_predictions:\r\n                    num_covered += 1\r\n\r\n    return num_covered / num_annotations\r\n\r\ndef ndcg(predictions: Dict[str, List[str]], relevance_scores: Dict[str, Dict[str, float]],\r\n         ignore_rank_of_non_annotated_urls: bool=True) -> float:\r\n    num_results = 0\r\n    ndcg_sum = 0\r\n\r\n    for query, query_relevance_annotations in relevance_scores.items():\r\n        current_rank = 1\r\n        query_dcg = 0\r\n        for url in predictions[query]:\r\n            if url in query_relevance_annotations:\r\n                query_dcg += (2**query_relevance_annotations[url] - 1) / np.log2(current_rank + 1)\r\n                current_rank += 1\r\n            elif not ignore_rank_of_non_annotated_urls:\r\n                current_rank += 1\r\n\r\n        query_idcg = 0\r\n        for i, ideal_relevance in enumerate(sorted(query_relevance_annotations.values(), reverse=True), start=1):\r\n            query_idcg += (2 ** ideal_relevance - 1) / np.log2(i + 1)\r\n        if query_idcg == 0:\r\n            # We have no positive annotations for the given query, so we should probably not penalize anyone about this.\r\n            continue\r\n        num_results += 1\r\n        ndcg_sum += query_dcg / query_idcg\r\n    return ndcg_sum / num_results\r\n\r\n\r\n\r\ndef run(arguments):\r\n    relevance_scores = load_relevances(arguments[\'RELEVANCE_ANNOTATIONS_CSV_PATH\'])\r\n    predictions = load_predictions(arguments[\'MODEL_PREDICTIONS_CSV\'])\r\n\r\n    languages_predicted = sorted(set(predictions.keys()))\r\n\r\n    # Now Compute the various evaluation results\r\n    print(\'% of URLs in predictions that exist in the annotation dataset:\')\r\n    for language in languages_predicted:\r\n        print(f\'\\t{language}: {coverage_per_language(predictions[language], relevance_scores[language])*100:.2f}%\')\r\n\r\n    print(\'% of URLs in predictions that exist in the annotation dataset (avg relevance > 0):\')\r\n    for language in languages_predicted:\r\n        print(f\'\\t{language}: {coverage_per_language(predictions[language], relevance_scores[language], with_positive_relevance=True) * 100:.2f}%\')\r\n\r\n    print(\'NDCG:\')\r\n    for language in languages_predicted:\r\n        print(f\'\\t{language}: {ndcg(predictions[language], relevance_scores[language]):.3f}\')\r\n\r\n    print(\'NDCG (full ranking):\')\r\n    for language in languages_predicted:\r\n        print(f\'\\t{language}: {ndcg(predictions[language], relevance_scores[language], ignore_rank_of_non_annotated_urls=False):.3f}\')\r\n\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    args = docopt(__doc__)\r\n    run_and_debug(lambda: run(args), args[\'--debug\'])'"
src/test.py,0,"b'#!/usr/bin/env python\r\n""""""\r\nUsage:\r\n    test.py [options] MODEL_PATH VALID_DATA_PATH TEST_DATA_PATH\r\n    test.py [options] MODEL_PATH\r\n\r\nStandalone testing script\r\n\r\nOptions:\r\n    -h --help                        Show this screen.\r\n    --test-batch-size SIZE           The size of the batches in which to compute MRR. [default: 1000]\r\n    --distance-metric METRIC         The distance metric to use [default: cosine]\r\n    --run-name NAME                  Picks a name for the trained model.\r\n    --quiet                          Less output (not one per line per minibatch). [default: False]\r\n    --dryrun                         Do not log run into logging database. [default: False]\r\n    --azure-info PATH                Azure authentication information file (JSON). Used to load data from Azure storage.\r\n    --sequential                     Do not parallelise data-loading. Simplifies debugging. [default: False]\r\n    --debug                          Enable debug routines. [default: False]\r\n""""""\r\nfrom pathlib import Path\r\n\r\nfrom docopt import docopt\r\nfrom dpu_utils.utils import run_and_debug, RichPath\r\n\r\nimport model_test as test\r\n\r\n\r\ndef run(arguments):\r\n    azure_info_path = arguments.get(\'--azure-info\', None)\r\n\r\n    # if you do not pass arguments for train/valid/test data default to files checked into repo.\r\n    if not arguments[\'VALID_DATA_PATH\']:\r\n        dir_path = Path(__file__).parent.absolute()\r\n        print(dir_path)\r\n        arguments[\'VALID_DATA_PATH\'] = str(dir_path / \'data_dirs_valid.txt\')\r\n        arguments[\'TEST_DATA_PATH\'] = str(dir_path / \'data_dirs_test.txt\')\r\n\r\n    valid_data_dirs = test.expand_data_path(arguments[\'VALID_DATA_PATH\'], azure_info_path)\r\n    test_data_dirs = test.expand_data_path(arguments[\'TEST_DATA_PATH\'], azure_info_path)\r\n    test.compute_evaluation_metrics(RichPath.create(arguments[\'MODEL_PATH\'], azure_info_path=azure_info_path),\r\n                                    arguments, azure_info_path, valid_data_dirs, test_data_dirs)\r\n\r\nif __name__ == \'__main__\':\r\n    args = docopt(__doc__)\r\n    run_and_debug(lambda: run(args), args[\'--debug\'])'"
src/train.py,0,"b'#!/usr/bin/env python\n""""""\nUsage:\n    train.py [options] SAVE_FOLDER TRAIN_DATA_PATH VALID_DATA_PATH TEST_DATA_PATH\n    train.py [options] [SAVE_FOLDER]\n\n*_DATA_PATH arguments may either accept (1) directory filled with .jsonl.gz files that we use as data,\nor a (2) plain text file containing a list of such directories (used for multi-language training).\n\nIn the case that you supply a (2) plain text file, all directory names must be separated by a newline.\nFor example, if you want to read from multiple directories you might have a plain text file called\ndata_dirs_train.txt with the below contents:\n\n> cat ~/src/data_dirs_train.txt\nazure://semanticcodesearch/pythondata/Processed_Data/jsonl/train\nazure://semanticcodesearch/csharpdata/split/csharpCrawl-train\n\nOptions:\n    -h --help                        Show this screen.\n    --max-num-epochs EPOCHS          The maximum number of epochs to run [default: 300]\n    --max-files-per-dir INT          Maximum number of files per directory to load for training data.\n    --hypers-override HYPERS         JSON dictionary overriding hyperparameter values.\n    --hypers-override-file FILE      JSON file overriding hyperparameter values.\n    --model MODELNAME                Choose model type. [default: neuralbowmodel]\n    --test-batch-size SIZE           The size of the batches in which to compute MRR. [default: 1000]\n    --distance-metric METRIC         The distance metric to use [default: cosine]\n    --run-name NAME                  Picks a name for the trained model.\n    --quiet                          Less output (not one per line per minibatch). [default: False]\n    --dryrun                         Do not log run into logging database. [default: False]\n    --testrun                        Do a model run on a small dataset for testing purposes. [default: False]\n    --azure-info PATH                Azure authentication information file (JSON). Used to load data from Azure storage.\n    --evaluate-model PATH            Run evaluation on previously trained model.\n    --sequential                     Do not parallelise data-loading. Simplifies debugging. [default: False]\n    --debug                          Enable debug routines. [default: False]\n""""""\nimport json\nimport os\nimport sys\nimport time\nfrom typing import Type, Dict, Any, Optional, List\nfrom pathlib import Path\n\nfrom docopt import docopt\nfrom dpu_utils.utils import RichPath, git_tag_run, run_and_debug\nimport wandb\n\nimport model_restore_helper\nfrom model_test import compute_evaluation_metrics\nfrom models.model import Model\nimport model_test as test\n\ndef run_train(model_class: Type[Model],\n              train_data_dirs: List[RichPath],\n              valid_data_dirs: List[RichPath],\n              save_folder: str,\n              hyperparameters: Dict[str, Any],\n              azure_info_path: Optional[str],\n              run_name: str,\n              quiet: bool = False,\n              max_files_per_dir: Optional[int] = None,\n              parallelize: bool = True) -> RichPath:\n    model = model_class(hyperparameters, run_name=run_name, model_save_dir=save_folder, log_save_dir=save_folder)\n    if os.path.exists(model.model_save_path):\n        model = model_restore_helper.restore(RichPath.create(model.model_save_path), is_train=True)\n        model.train_log(""Resuming training run %s of model %s with following hypers:\\n%s"" % (run_name,\n                                                                                             model.__class__.__name__,\n                                                                                             str(hyperparameters)))\n        resume = True\n    else:\n        model.train_log(""Tokenizing and building vocabulary for code snippets and queries.  This step may take several hours."")\n        model.load_metadata(train_data_dirs, max_files_per_dir=max_files_per_dir, parallelize=parallelize)\n        model.make_model(is_train=True)\n        model.train_log(""Starting training run %s of model %s with following hypers:\\n%s"" % (run_name,\n                                                                                             model.__class__.__name__,\n                                                                                             str(hyperparameters)))\n        resume = False\n\n    philly_job_id = os.environ.get(\'PHILLY_JOB_ID\')\n    if philly_job_id is not None:\n        # We are in Philly write out the model name in an auxiliary file\n        with open(os.path.join(save_folder, philly_job_id+\'.job\'), \'w\') as f:\n            f.write(os.path.basename(model.model_save_path))\n    \n    wandb.config.update(model.hyperparameters)\n    model.train_log(""Loading training and validation data."")\n    train_data = model.load_data_from_dirs(train_data_dirs, is_test=False, max_files_per_dir=max_files_per_dir, parallelize=parallelize)\n    valid_data = model.load_data_from_dirs(valid_data_dirs, is_test=False, max_files_per_dir=max_files_per_dir, parallelize=parallelize)\n    model.train_log(""Begin Training."")\n    model_path = model.train(train_data, valid_data, azure_info_path, quiet=quiet, resume=resume)\n    return model_path\n\n\ndef make_run_id(arguments: Dict[str, Any]) -> str:\n    """"""Choose a run ID, based on the --save-name parameter, the PHILLY_JOB_ID and the current time.""""""\n    philly_id = os.environ.get(\'PHILLY_JOB_ID\')\n    if philly_id is not None:\n        return philly_id\n    user_save_name = arguments.get(\'--run-name\')\n    if user_save_name is not None:\n        user_save_name = user_save_name[:-len(\'.pkl\')] if user_save_name.endswith(\'.pkl\') else user_save_name\n    else:\n        user_save_name = arguments[\'--model\']\n    return ""%s-%s"" % (user_save_name, time.strftime(""%Y-%m-%d-%H-%M-%S""))\n\n\ndef run(arguments, tag_in_vcs=False) -> None:\n    azure_info_path = arguments.get(\'--azure-info\', None)\n    testrun = arguments.get(\'--testrun\')\n    max_files_per_dir=arguments.get(\'--max-files-per-dir\')\n\n    dir_path = Path(__file__).parent.absolute()\n\n    # if you do not pass arguments for train/valid/test data default to files checked into repo.\n    if not arguments[\'TRAIN_DATA_PATH\']:\n        arguments[\'TRAIN_DATA_PATH\'] = str(dir_path/\'data_dirs_train.txt\')\n        arguments[\'VALID_DATA_PATH\'] = str(dir_path/\'data_dirs_valid.txt\')\n        arguments[\'TEST_DATA_PATH\'] = str(dir_path/\'data_dirs_test.txt\')\n\n    train_data_dirs = test.expand_data_path(arguments[\'TRAIN_DATA_PATH\'], azure_info_path)\n    valid_data_dirs = test.expand_data_path(arguments[\'VALID_DATA_PATH\'], azure_info_path)\n    test_data_dirs = test.expand_data_path(arguments[\'TEST_DATA_PATH\'], azure_info_path)\n    \n    # default model save location\n    if not arguments[\'SAVE_FOLDER\']:\n        arguments[\'SAVE_FOLDER\'] =  str(dir_path.parent/\'resources/saved_models/\')\n\n    save_folder = arguments[\'SAVE_FOLDER\']\n\n    model_class = model_restore_helper.get_model_class_from_name(arguments[\'--model\'])\n\n    hyperparameters = model_class.get_default_hyperparameters()\n    run_name = make_run_id(arguments)\n\n    # make name of wandb run = run_id (Doesn\'t populate yet)\n    hyperparameters[\'max_epochs\'] = int(arguments.get(\'--max-num-epochs\'))\n\n    if testrun:\n        hyperparameters[\'max_epochs\'] = 2\n        if not max_files_per_dir:\n            max_files_per_dir = 1\n\n    # override hyperparams if flag is passed\n    hypers_override = arguments.get(\'--hypers-override\')\n    if hypers_override is not None:\n        hyperparameters.update(json.loads(hypers_override))\n    elif arguments.get(\'--hypers-override-file\') is not None:\n        with open(arguments.get(\'--hypers-override-file\')) as f:\n            hyperparameters.update(json.load(f))\n\n    os.makedirs(save_folder, exist_ok=True)\n\n    if tag_in_vcs:\n        hyperparameters[\'git_commit\'] = git_tag_run(run_name)\n\n    # turns off wandb if you don\'t want to log anything\n    if arguments.get(\'--dryrun\'):\n        os.environ[""WANDB_MODE""] = \'dryrun\'\n    # save hyperparams to logging\n    # must filter out type=set from logging when as that is not json serializable\n    wandb.init(name=run_name, config={k: v for k, v in hyperparameters.items() if not isinstance(v, set)})\n    wandb.config.update({\'model-class\': arguments[\'--model\'],\n                         \'train_folder\': str(train_data_dirs),\n                         \'valid_folder\': str(valid_data_dirs),\n                         \'save_folder\': str(save_folder),\n                         \'test_folder\': str(test_data_dirs),\n                         \'CUDA_VISIBLE_DEVICES\': os.environ.get(""CUDA_VISIBLE_DEVICES"", \'Not Set\'),\n                         \'run-name\': arguments.get(\'--run-name\'),\n                         \'CLI-command\': \' \'.join(sys.argv)})\n\n\n    if arguments.get(\'--evaluate-model\'):\n        model_path = RichPath.create(arguments[\'--evaluate-model\'])\n    else:\n        model_path = run_train(model_class, train_data_dirs, valid_data_dirs, save_folder, hyperparameters,\n                               azure_info_path, run_name, arguments[\'--quiet\'],\n                               max_files_per_dir=max_files_per_dir,\n                               parallelize=not(arguments[\'--sequential\']))\n\n    wandb.config[\'best_model_path\'] = str(model_path)\n    wandb.save(str(model_path.to_local_path()))\n\n    # only limit files in test run if `--testrun` flag is passed by user.\n    if testrun:\n        compute_evaluation_metrics(model_path, arguments, azure_info_path, valid_data_dirs, test_data_dirs, max_files_per_dir)\n    else:\n        compute_evaluation_metrics(model_path, arguments, azure_info_path, valid_data_dirs, test_data_dirs)\n\n\nif __name__ == \'__main__\':\n    args = docopt(__doc__)\n    run_and_debug(lambda: run(args), args[\'--debug\'])\n'"
function_parser/function_parser/__init__.py,0,b''
function_parser/function_parser/fetch_licenses.py,0,"b""import glob\nfrom itertools import chain\nimport os\nimport pickle\nimport re\n\nfrom dask.distributed import Client\nimport dask.distributed\nfrom tqdm import tqdm\n\nfrom language_data import LANGUAGE_METADATA\nfrom utils import download\n\n# Gets notices\nLEGAL_FILES_REGEX ='(AUTHORS|NOTICE|LEGAL)(?:\\..*)?\\Z'\n\nPREFERRED_EXT_REGEX = '\\.[md|markdown|txt|html]\\Z'\n\n# Regex to match any extension except .spdx or .header\nOTHER_EXT_REGEX = '\\.(?!spdx|header|gemspec)[^./]+\\Z'\n\n# Regex to match, LICENSE, LICENCE, unlicense, etc.\nLICENSE_REGEX = '(un)?licen[sc]e'\n\n# Regex to match COPYING, COPYRIGHT, etc.\nCOPYING_REGEX = 'copy(ing|right)'\n\n# Regex to match OFL.\nOFL_REGEX = 'ofl'\n\n# BSD + PATENTS patent file\nPATENTS_REGEX = 'patents'\n\n\ndef match_license_file(filename):\n    for regex in [LEGAL_FILES_REGEX, \n                  LICENSE_REGEX + '\\Z',\n                  LICENSE_REGEX + PREFERRED_EXT_REGEX,\n                  COPYING_REGEX + '\\Z',\n                  COPYING_REGEX + PREFERRED_EXT_REGEX,\n                  LICENSE_REGEX + OTHER_EXT_REGEX,\n                  COPYING_REGEX + OTHER_EXT_REGEX,\n                  LICENSE_REGEX + '[-_]',\n                  COPYING_REGEX + '[-_]',\n                  '[-_]' + LICENSE_REGEX,\n                  '[-_]' + COPYING_REGEX,\n                  OFL_REGEX + PREFERRED_EXT_REGEX,\n                  OFL_REGEX + OTHER_EXT_REGEX,\n                  OFL_REGEX + '\\Z',\n                  PATENTS_REGEX + '\\Z',\n                  PATENTS_REGEX + OTHER_EXT_REGEX]:\n        if re.match(regex, filename.lower()):\n            return filename\n    return None\n\ndef flattenlist(listoflists):\n    return list(chain.from_iterable(listoflists))\n\ndef fetch_license(nwo):\n    licenses = []\n    tmp_dir = download(nwo)\n    for f in sorted(glob.glob(tmp_dir.name + '/**/*', recursive=True), key=lambda x: len(x)):\n        if not os.path.isdir(f):\n            if match_license_file(f.split('/')[-1]):\n                licenses.append((nwo, f.replace(tmp_dir.name + '/', ''), open(f, errors='surrogateescape').read()))\n    \n    return licenses\n\n\nclient = Client()\n\nfor language in LANGUAGE_METADATA.keys():\n    definitions = pickle.load(open('../data/{}_dedupe_definitions_v2.pkl'.format(language), 'rb'))\n    nwos = list(set([d['nwo'] for d in definitions]))\n    \n    futures = client.map(fetch_license, nwos)\n    results = []\n    for r in tqdm(futures):\n        try:\n            results.append(r.result(2))\n        except dask.distributed.TimeoutError:\n            continue\n    \n    flat_results = flattenlist(results)\n    licenses = dict()\n    for nwo, path, content in flat_results:\n        if content:\n            licenses[nwo] = licenses.get(nwo, []) + [(path, content)]\n    pickle.dump(licenses, open('../data/{}_licenses.pkl'.format(language), 'wb'))\n"""
function_parser/function_parser/language_data.py,0,"b""from parsers.go_parser import GoParser\r\nfrom parsers.java_parser import JavaParser\r\nfrom parsers.javascript_parser import JavascriptParser\r\nfrom parsers.php_parser import PhpParser\r\nfrom parsers.python_parser import PythonParser\r\nfrom parsers.ruby_parser import RubyParser\r\n\r\n\r\nLANGUAGE_METADATA = {\r\n    'python': {\r\n        'platform': 'pypi',\r\n        'ext': 'py',\r\n        'language_parser': PythonParser\r\n    },\r\n    'java': {\r\n        'platform': 'maven',\r\n        'ext': 'java',\r\n        'language_parser': JavaParser\r\n    },\r\n    'go': {\r\n        'platform': 'go',\r\n        'ext': 'go',\r\n        'language_parser': GoParser\r\n    },\r\n    'javascript': {\r\n        'platform': 'npm',\r\n        'ext': 'js',\r\n        'language_parser': JavascriptParser\r\n    },\r\n    'php': {\r\n        'platform': 'packagist',\r\n        'ext': 'php',\r\n        'language_parser': PhpParser\r\n    },\r\n    'ruby': {\r\n        'platform': 'rubygems',\r\n        'ext': 'rb',\r\n        'language_parser': RubyParser\r\n    }\r\n}\r\n"""
function_parser/function_parser/parser_cli.py,0,"b'""""""\r\nUsage:\r\n    parser_cli.py [options] INPUT_FILEPATH\r\n\r\nOptions:\r\n    -h --help\r\n    --language LANGUAGE             Language\r\n""""""\r\nimport json\r\n\r\nfrom docopt import docopt\r\nfrom tree_sitter import Language\r\n\r\nfrom language_data import LANGUAGE_METADATA\r\nfrom process import DataProcessor\r\n\r\nif __name__ == \'__main__\':\r\n    args = docopt(__doc__)\r\n\r\n    DataProcessor.PARSER.set_language(Language(\'/src/build/py-tree-sitter-languages.so\', args[\'--language\']))\r\n    processor = DataProcessor(language=args[\'--language\'],\r\n                              language_parser=LANGUAGE_METADATA[args[\'--language\']][\'language_parser\'])\r\n\r\n    functions = processor.process_single_file(args[\'INPUT_FILEPATH\'])\r\n    print(json.dumps(functions, indent=2))\r\n'"
function_parser/function_parser/process.py,0,"b'""""""\nUsage:\n    process.py [options] INPUT_DIR OUTPUT_DIR\n\nOptions:\n    -h --help\n    --language LANGUAGE             Language\n    --processes PROCESSES           # of processes to use [default: 16]\n    --license-filter FILE           License metadata to filter, every row contains [nwo, license, language, score] (e.g. [\'pandas-dev/pandas\', \'bsd-3-clause\', \'Python\', 0.9997])\n    --tree-sitter-build FILE        [default: /src/build/py-tree-sitter-languages.so]\n""""""\nimport functools\nfrom multiprocessing import Pool\nimport pickle\nfrom os import PathLike\nfrom typing import Optional, Tuple, Type, List, Dict, Any\n\nfrom docopt import docopt\nfrom dpu_utils.codeutils.deduplication import DuplicateDetector\nimport pandas as pd\nfrom tree_sitter import Language, Parser\n\nfrom language_data import LANGUAGE_METADATA\nfrom parsers.language_parser import LanguageParser, tokenize_docstring\nfrom utils import download, get_sha, flatten, remap_nwo, walk\n\nclass DataProcessor:\n\n    PARSER = Parser()\n\n    def __init__(self, language: str, language_parser: Type[LanguageParser]):\n        self.language = language\n        self.language_parser = language_parser\n\n    def process_dee(self, nwo, ext) -> List[Dict[str, Any]]:\n        # Process dependees (libraries) to get function implementations\n        indexes = []\n        _, nwo = remap_nwo(nwo)\n        if nwo is None:\n            return indexes\n\n        tmp_dir = download(nwo)\n        files = walk(tmp_dir, ext)\n        # files = glob.iglob(tmp_dir.name + \'/**/*.{}\'.format(ext), recursive=True)\n        sha = None\n\n        for f in files:\n            definitions = self.get_function_definitions(f)\n            if definitions is None:\n                continue\n            if sha is None:\n                sha = get_sha(tmp_dir, nwo)\n\n            nwo, path, functions = definitions\n            indexes.extend((self.extract_function_data(func, nwo, path, sha) for func in functions if len(func[\'function_tokens\']) > 1))\n        return indexes\n\n    def process_dent(self, nwo, ext, library_candidates) -> Tuple[List[Dict[str, Any]], List[Tuple[str, str]]]:\n        # Process dependents (applications) to get function calls\n        dents = []\n        edges = []\n        _, nwo = remap_nwo(nwo)\n        if nwo is None:\n            return dents, edges\n\n        tmp_dir = download(nwo)\n        files = walk(tmp_dir, ext)\n        sha = None\n\n        for f in files:\n            context_and_calls = self.get_context_and_function_calls(f)\n            if context_and_calls is None:\n                continue\n            if sha is None:\n                sha = get_sha(tmp_dir, nwo)\n\n            nwo, path, context, calls = context_and_calls\n            libraries = []\n            for cxt in context:\n                if type(cxt) == dict:\n                    libraries.extend([v.split(\'.\')[0] for v in cxt.values()])\n                elif type(cxt) == list:\n                    libraries.extend(cxt)\n\n            match_scopes = {}\n            for cxt in set(libraries):\n                if cxt in library_candidates:\n                    match_scopes[cxt] = library_candidates[cxt]\n\n            for call in calls:\n                for depended_library_name, dependend_library_functions in match_scopes.items():\n                    for depended_library_function in dependend_library_functions:\n                        # Other potential filters: len(call[\'identifier\']) > 6 or len(call[\'identifier\'].split(\'_\')) > 1\n                        if (call[\'identifier\'] not in self.language_parser.STOPWORDS and\n                            ((depended_library_function[\'identifier\'].split(\'.\')[-1] == \'__init__\' and\n                              call[\'identifier\'] == depended_library_function[\'identifier\'].split(\'.\')[0]) or\n                             ((len(call[\'identifier\']) > 9 or\n                               (not call[\'identifier\'].startswith(\'_\') and len(call[\'identifier\'].split(\'_\')) > 1)) and\n                              call[\'identifier\'] == depended_library_function[\'identifier\'])\n                            )):\n                            dent = {\n                                \'nwo\': nwo,\n                                \'sha\': sha,\n                                \'path\': path,\n                                \'language\': self.language,\n                                \'identifier\': call[\'identifier\'],\n                                \'argument_list\': call[\'argument_list\'],\n                                \'url\': \'https://github.com/{}/blob/{}/{}#L{}-L{}\'.format(nwo, sha, path,\n                                                                                         call[\'start_point\'][0] + 1,\n                                                                                         call[\'end_point\'][0] + 1)\n                            }\n                            dents.append(dent)\n                            edges.append((dent[\'url\'], depended_library_function[\'url\']))\n        return dents, edges\n\n    def process_single_file(self, filepath: PathLike) -> List[Dict[str, Any]]:\n        definitions = self.get_function_definitions(filepath)\n        if definitions is None:\n            return []\n        _, _, functions = definitions\n\n        return [self.extract_function_data(func, \'\', \'\', \'\') for func in functions if len(func[\'function_tokens\']) > 1]\n\n    def extract_function_data(self, function: Dict[str, Any], nwo, path: str, sha: str):\n        return {\n            \'nwo\': nwo,\n            \'sha\': sha,\n            \'path\': path,\n            \'language\': self.language,\n            \'identifier\': function[\'identifier\'],\n            \'parameters\': function.get(\'parameters\', \'\'),\n            \'argument_list\': function.get(\'argument_list\', \'\'),\n            \'return_statement\': function.get(\'return_statement\', \'\'),\n            \'docstring\': function[\'docstring\'].strip(),\n            \'docstring_summary\': function[\'docstring_summary\'].strip(),\n            \'docstring_tokens\': tokenize_docstring(function[\'docstring_summary\']),\n            \'function\': function[\'function\'].strip(),\n            \'function_tokens\': function[\'function_tokens\'],\n            \'url\': \'https://github.com/{}/blob/{}/{}#L{}-L{}\'.format(nwo, sha, path, function[\'start_point\'][0] + 1,\n                                                                     function[\'end_point\'][0] + 1)\n        }\n\n    def get_context_and_function_calls(self, filepath: str) -> Optional[Tuple[str, str, List, List]]:\n        nwo = \'/\'.join(filepath.split(\'/\')[3:5])\n        path = \'/\'.join(filepath.split(\'/\')[5:])\n        if any(fp in path.lower() for fp in self.language_parser.FILTER_PATHS):\n            return None\n        try:\n            with open(filepath) as source_code:\n                blob = source_code.read()\n            tree = DataProcessor.PARSER.parse(blob.encode())\n            return (nwo, path, self.language_parser.get_context(tree, blob), self.language_parser.get_calls(tree, blob))\n        except (UnicodeDecodeError, FileNotFoundError, IsADirectoryError, ValueError, OSError):\n            return None\n\n    def get_function_definitions(self, filepath: str) -> Optional[Tuple[str, str, List]]:\n        nwo = \'/\'.join(filepath.split(\'/\')[3:5])\n        path = \'/\'.join(filepath.split(\'/\')[5:])\n        if any(fp in path.lower() for fp in self.language_parser.FILTER_PATHS):\n            return None\n        try:\n            with open(filepath) as source_code:\n                blob = source_code.read()\n            tree = DataProcessor.PARSER.parse(blob.encode())\n            return (nwo, path, self.language_parser.get_definition(tree, blob))\n        except (UnicodeDecodeError, FileNotFoundError, IsADirectoryError, ValueError, OSError):\n            return None\n\n\nif __name__ == \'__main__\':\n    args = docopt(__doc__)\n\n    repository_dependencies = pd.read_csv(args[\'INPUT_DIR\'] + \'repository_dependencies-1.4.0-2018-12-22.csv\', index_col=False)\n    projects = pd.read_csv(args[\'INPUT_DIR\'] + \'projects_with_repository_fields-1.4.0-2018-12-22.csv\', index_col=False)\n\n    repository_dependencies[\'Manifest Platform\'] = repository_dependencies[\'Manifest Platform\'].apply(lambda x: x.lower())\n    id_to_nwo = {project[\'ID\']: project[\'Repository Name with Owner\'] for project in projects[[\'ID\', \'Repository Name with Owner\']].dropna().to_dict(orient=\'records\')}\n    nwo_to_name = {project[\'Repository Name with Owner\']: project[\'Name\'] for project in projects[[\'Repository Name with Owner\', \'Name\']].dropna().to_dict(orient=\'records\')}\n\n    filtered = repository_dependencies[(repository_dependencies[\'Host Type\'] == \'GitHub\') & (repository_dependencies[\'Manifest Platform\'] == LANGUAGE_METADATA[args[\'--language\']][\'platform\'])][[\'Repository Name with Owner\', \'Dependency Project ID\']].dropna().to_dict(orient=\'records\')\n\n    dependency_pairs = [(rd[\'Repository Name with Owner\'], id_to_nwo[int(rd[\'Dependency Project ID\'])])\n                        for rd in filtered if int(rd[\'Dependency Project ID\']) in id_to_nwo]\n\n    dependency_pairs = list(set(dependency_pairs))\n\n    dents, dees = zip(*dependency_pairs)\n    # dents = list(set(dents))\n    dees = list(set(dees))\n\n    DataProcessor.PARSER.set_language(Language(args[\'--tree-sitter-build\'], args[\'--language\']))\n\n    processor = DataProcessor(language=args[\'--language\'],\n                              language_parser=LANGUAGE_METADATA[args[\'--language\']][\'language_parser\'])\n\n    with Pool(processes=int(args[\'--processes\'])) as pool:\n        output = pool.imap_unordered(functools.partial(processor.process_dee,\n                                                       ext=LANGUAGE_METADATA[args[\'--language\']][\'ext\']),\n                                     dees)\n\n    definitions = list(flatten(output))\n    with open(args[\'OUTPUT_DIR\'] + \'{}_definitions.pkl\'.format(args[\'--language\']), \'wb\') as f:\n        pickle.dump(definitions, f)\n\n    license_filter_file = args.get(\'--license-filter\')\n    if license_filter_file is not None:\n        with open(license_filter_file, \'rb\') as f:\n            license_filter = pickle.load(f)\n        valid_nwos = dict([(l[0], l[3]) for l in license_filter])\n\n        # Sort function definitions with repository popularity\n        definitions = [dict(list(d.items()) + [(\'score\', valid_nwos[d[\'nwo\']])]) for d in definitions if d[\'nwo\'] in valid_nwos]\n        definitions = sorted(definitions, key=lambda x: -x[\'score\'])\n\n        # dedupe\n        seen = set()\n        filtered = []\n        for d in definitions:\n            if \' \'.join(d[\'function_tokens\']) not in seen:\n                filtered.append(d)\n                seen.add(\' \'.join(d[\'function_tokens\']))\n\n        dd = DuplicateDetector(min_num_tokens_per_document=10)\n        filter_mask = [dd.add_file(id=idx,\n                                   tokens=d[\'function_tokens\'],\n                                   language=d[\'language\']) for idx, d in enumerate(filtered)]\n        exclusion_set = dd.compute_ids_to_exclude()\n        exclusion_mask = [idx not in exclusion_set for idx, _ in enumerate(filtered)]\n        filtered = [d for idx, d in enumerate(filtered) if filter_mask[idx] & exclusion_mask[idx]]\n\n        with open(args[\'OUTPUT_DIR\'] + \'{}_dedupe_definitions.pkl\'.format(args[\'--language\']), \'wb\') as f:\n            pickle.dump(filtered, f)\n'"
function_parser/function_parser/process_calls.py,0,"b'""""""\nUsage:\n    process_calls.py [options] INPUT_DIR DEFINITION_FILE OUTPUT_DIR\n\nOptions:\n    -h --help\n    --language LANGUAGE             Language\n    --processes PROCESSES           # of processes to use [default: 16]\n    --tree-sitter-build FILE        [default: /src/build/py-tree-sitter-languages.so]\n""""""\nfrom collections import Counter, defaultdict\nimport functools\nimport gzip\nfrom multiprocessing import Pool\nimport pandas as pd\nimport pickle\n\nfrom docopt import docopt\nfrom tree_sitter import Language\n\nfrom language_data import LANGUAGE_METADATA\nfrom process import DataProcessor\n\n\nif __name__ == \'__main__\':\n    args = docopt(__doc__)\n\n    repository_dependencies = pd.read_csv(args[\'INPUT_DIR\'] + \'repository_dependencies-1.4.0-2018-12-22.csv\', index_col=False)\n    projects = pd.read_csv(args[\'INPUT_DIR\'] + \'projects_with_repository_fields-1.4.0-2018-12-22.csv\', index_col=False)\n\n    repository_dependencies[\'Manifest Platform\'] = repository_dependencies[\'Manifest Platform\'].apply(lambda x: x.lower())\n    id_to_nwo = {project[\'ID\']: project[\'Repository Name with Owner\'] for project in projects[[\'ID\', \'Repository Name with Owner\']].dropna().to_dict(orient=\'records\')}\n    nwo_to_name = {project[\'Repository Name with Owner\']: project[\'Name\'] for project in projects[[\'Repository Name with Owner\', \'Name\']].dropna().to_dict(orient=\'records\')}\n\n    filtered = repository_dependencies[(repository_dependencies[\'Host Type\'] == \'GitHub\') & (repository_dependencies[\'Manifest Platform\'] == LANGUAGE_METADATA[args[\'--language\']][\'platform\'])][[\'Repository Name with Owner\', \'Dependency Project ID\']].dropna().to_dict(orient=\'records\')\n\n    dependency_pairs = [(rd[\'Repository Name with Owner\'], id_to_nwo[int(rd[\'Dependency Project ID\'])])\n                        for rd in filtered if int(rd[\'Dependency Project ID\']) in id_to_nwo]\n\n    dependency_pairs = list(set(dependency_pairs))\n\n    dents, dees = zip(*dependency_pairs)\n    dents = list(set(dents))\n\n    definitions = defaultdict(list)\n    with open(args[\'DEFINITION_FILE\'], \'rb\') as f:\n        for d in pickle.load(f)\n            definitions[d[\'nwo\']].append(d)\n    definitions = dict(definitions)\n\n    # Fill candidates from most depended libraries\n    c = Counter(dees)\n    library_candidates = {}\n    for nwo, _ in c.most_common(len(c)):\n        if nwo.split(\'/\')[-1] not in library_candidates and nwo in definitions:\n            # Approximate library name with the repository name from nwo\n            library_candidates[nwo.split(\'/\')[-1]] = definitions[nwo]\n\n    DataProcessor.PARSER.set_language(Language(args[\'--tree-sitter-build\'], args[\'--language\']))\n    processor = DataProcessor(language=args[\'--language\'],\n                              language_parser=LANGUAGE_METADATA[args[\'--language\']][\'language_parser\'])\n\n    with Pool(processes=int(args[\'--processes\'])) as pool:\n        output = pool.imap_unordered(functools.partial(processor.process_dent,\n                                                       ext=LANGUAGE_METADATA[args[\'--language\']][\'ext\']),\n                                     dents)\n\n    dent_definitions, edges = map(list, map(flatten, zip(*output)))\n\n    with gzip.GzipFile(args[\'OUTPUT_DIR\'] + \'{}_dent_definitions.pkl.gz\'.format(args[\'--language\']), \'wb\') as outfile:\n        pickle.dump(dent_definitions, outfile)\n    with gzip.GzipFile(args[\'OUTPUT_DIR\'] + \'{}_edges.pkl.gz\'.format(args[\'--language\']), \'wb\') as outfile:\n        pickle.dump(edges, outfile)\n'"
function_parser/function_parser/utils.py,0,"b'import itertools\nimport os\nimport re\nimport subprocess\nimport tempfile\nfrom typing import List, Tuple\n\nimport requests\n\n\ndef flatten(l):\n    """"""Flatten list of lists.\n    Args:\n        l: A list of lists\n    Returns: A flattened iterable\n    """"""\n    return itertools.chain.from_iterable(l)\n\n\ndef chunks(l: List, n: int):\n    """"""Yield successive n-sized chunks from l.""""""\n    for i in range(0, len(l), n):\n        yield l[i:i + n]\n\n\ndef remap_nwo(nwo: str) -> Tuple[str, str]:\n    r = requests.get(\'https://github.com/{}\'.format(nwo))\n    if r.status_code not in (404, 451, 502): # DMCA\n        if \'migrated\' not in r.text:\n            if r.history:\n                return (nwo, \'/\'.join(re.findall(r\'""https://github.com/.+""\', r.history[0].text)[0].strip(\'""\').split(\'/\')[-2:]))\n            return (nwo, nwo)\n    return (nwo, None)\n\n\ndef get_sha(tmp_dir: tempfile.TemporaryDirectory, nwo: str):\n    os.chdir(os.path.join(tmp_dir.name, nwo))\n    # git rev-parse HEAD\n    cmd = [\'git\', \'rev-parse\', \'HEAD\']\n    sha = subprocess.check_output(cmd).strip().decode(\'utf-8\')\n    os.chdir(\'/tmp\')\n    return sha\n\n\ndef download(nwo: str):\n    os.environ[\'GIT_TERMINAL_PROMPT\'] = \'0\'\n    tmp_dir = tempfile.TemporaryDirectory()\n    cmd = [\'git\', \'clone\', \'--depth=1\', \'https://github.com/{}.git\'.format(nwo), \'{}/{}\'.format(tmp_dir.name, nwo)]\n    subprocess.run(cmd, stdin=subprocess.DEVNULL, stderr=subprocess.DEVNULL, stdout=subprocess.DEVNULL)\n    return tmp_dir\n\n\ndef walk(tmp_dir: tempfile.TemporaryDirectory, ext: str):\n    results = []\n    for root, _, files in os.walk(tmp_dir.name):\n        for f in files:\n            if f.endswith(\'.\' + ext):\n                results.append(os.path.join(root, f))\n    return results\n'"
function_parser/script/setup.py,0,"b""import glob\n\nfrom tree_sitter import Language\n\nlanguages = [\n    '/src/vendor/tree-sitter-python',\n    '/src/vendor/tree-sitter-javascript',\n    # '/src/vendor/tree-sitter-typescript/typescript',\n    # '/src/vendor/tree-sitter-typescript/tsx',\n    '/src/vendor/tree-sitter-go',\n    '/src/vendor/tree-sitter-ruby',\n    '/src/vendor/tree-sitter-java',\n    '/src/vendor/tree-sitter-cpp',\n    '/src/vendor/tree-sitter-php',\n]\n\nLanguage.build_library(\n    # Store the library in the directory\n    '/src/build/py-tree-sitter-languages.so',\n    # Include one or more languages\n    languages\n)\n"""
src/dataextraction/__init__.py,0,b'\n'
src/dataextraction/dedup_split.py,0,"b'#!/usr/bin/env python\n""""""\nRemove near duplicates from data and perform train/test/validation/holdout splits.\n\nUsage:\n    dedup_split.py [options] INPUT_FILENAME OUTPUT_FOLDER\n\nArguments:\n    INPUT_FOLDER               directory w/ compressed jsonl files that have a .jsonl.gz a file extension\n    OUTPUT_FOLDER              directory where you want to save data to.\n\nOptions:\n    -h --help                    Show this screen.\n    --azure-info=<path>          Azure authentication information file (JSON).\n    --train-ratio FLOAT          Ratio of files for training set. [default: 0.6]\n    --valid-ratio FLOAT          Ratio of files for validation set. [default: 0.15]\n    --test-ratio FLOAT           Ratio of files for test set. [default: 0.15]\n    --holdout-ratio FLOAT        Ratio of files for test set. [default: 0.1]\n    --debug                      Enable debug routines. [default: False]\n\nExample:\n\n    python dedup_split.py \\\n    --azure-info /ds/hamel/azure_auth.json \\\n    azure://semanticcodesearch/pythondata/raw_v2  \\\n    azure://semanticcodesearch/pythondata/Processed_Data_v2\n\n""""""\n\nfrom docopt import docopt\nimport hashlib\nimport pandas as pd\nfrom utils.pkldf2jsonl import chunked_save_df_to_jsonl\nfrom dpu_utils.utils import RichPath, run_and_debug\nfrom dpu_utils.codeutils.deduplication import DuplicateDetector\nimport os\nfrom tqdm import tqdm\n\n\ndef jsonl_to_df(input_folder: RichPath) -> pd.DataFrame:\n    ""Concatenates all jsonl files from path and returns them as a single pandas.DataFrame .""\n\n    assert input_folder.is_dir(), \'Argument supplied must be a directory\'\n    dfs = []\n    files = list(input_folder.iterate_filtered_files_in_dir(\'*.jsonl.gz\'))\n    assert files, \'There were no jsonl.gz files in the specified directory.\'\n    print(f\'reading files from {input_folder.path}\')\n    for f in tqdm(files, total=len(files)):\n        dfs.append(pd.DataFrame(list(f.read_as_jsonl(error_handling=lambda m,e: print(f\'Error while loading {m} : {e}\')))))\n    return pd.concat(dfs)\n\n\ndef remove_duplicate_code_df(df: pd.DataFrame) -> pd.DataFrame:\n    ""Resolve near duplicates based upon code_tokens field in data.""\n    assert \'code_tokens\' in df.columns.values, \'Data must contain field code_tokens\'\n    assert \'language\' in df.columns.values, \'Data must contain field language\'\n    df.reset_index(inplace=True, drop=True)\n    df[\'doc_id\'] = df.index.values\n    dd = DuplicateDetector(min_num_tokens_per_document=10)\n    filter_mask = df.apply(lambda x: dd.add_file(id=x.doc_id,\n                                                 tokens=x.code_tokens,\n                                                 language=x.language),\n                           axis=1)\n    # compute fuzzy duplicates\n    exclusion_set = dd.compute_ids_to_exclude()\n    # compute pandas.series of type boolean which flags whether or not code should be discarded\n    # in order to resolve duplicates (discards all but one in each set of duplicate functions)\n    exclusion_mask = df[\'doc_id\'].apply(lambda x: x not in exclusion_set)\n\n    # filter the data\n    print(f\'Removed {sum(~(filter_mask & exclusion_mask)):,} fuzzy duplicates out of {df.shape[0]:,} rows.\')\n    return df[filter_mask & exclusion_mask]\n\n\ndef label_folds(df: pd.DataFrame, train_ratio: float, valid_ratio: float, test_ratio: float, holdout_ratio: float) -> pd.DataFrame:\n    ""Adds a partition column to DataFrame with values: {train, valid, test, holdout}.""\n    assert abs(train_ratio + valid_ratio + test_ratio + holdout_ratio - 1) < 1e-5,  \'Ratios must sum up to 1.\'\n    # code in the same file will always go to the same split\n    df[\'hash_key\'] = df.apply(lambda x: f\'{x.repo}:{x.path}\', axis=1)\n    df[\'hash_val\'] = df[\'hash_key\'].apply(lambda x: int(hashlib.md5(x.encode()).hexdigest(), 16) % (2**16))\n\n    train_bound = int(2**16 * train_ratio)\n    valid_bound = train_bound + int(2**16 * valid_ratio)\n    test_bound = valid_bound + int(2**16 * test_ratio)\n\n    def label_splits(hash_val: int) -> str:\n        if hash_val <= train_bound:\n            return ""train""\n        elif hash_val <= valid_bound:\n            return ""valid""\n        elif hash_val <= test_bound:\n            return ""test""\n        else:\n            return ""holdout""\n\n    # apply partition logic\n    df[\'partition\'] = df[\'hash_val\'].apply(lambda x: label_splits(x))\n    # display summary statistics\n    counts = df.groupby(\'partition\')[\'repo\'].count().rename(\'count\')\n    summary_df = pd.concat([counts, (counts / counts.sum()).rename(\'pct\')], axis=1)\n    print(summary_df)\n\n    return df\n\n\ndef run(args):\n\n    azure_info_path = args.get(\'--azure-info\', None)\n    input_path = RichPath.create(args[\'INPUT_FILENAME\'], azure_info_path)\n    output_folder = RichPath.create(args[\'OUTPUT_FOLDER\'], azure_info_path)\n    train = float(args[\'--train-ratio\'])\n    valid = float(args[\'--valid-ratio\'])\n    test = float(args[\'--test-ratio\'])\n    holdout = float(args[\'--holdout-ratio\'])\n\n    # get data and process it\n    df = jsonl_to_df(input_path)\n    print(\'Removing fuzzy duplicates ... this may take some time.\')\n    df = remove_duplicate_code_df(df)\n    df = df.sample(frac=1, random_state=20181026)  # shuffle order of files\n    df = label_folds(df, train_ratio=train, valid_ratio=valid, test_ratio=test, holdout_ratio=holdout)\n    splits = [\'train\', \'valid\', \'test\', \'holdout\']\n\n    for split in splits:\n        split_df = df[df.partition == split]\n\n        # save dataframes as chunked jsonl files\n        jsonl_save_folder = output_folder.join(f\'jsonl/{split}\')\n        print(f\'Uploading data to {str(jsonl_save_folder)}\')\n        chunked_save_df_to_jsonl(split_df, jsonl_save_folder)\n\n        # Upload dataframes to Azure\n        filename = f\'/tmp/{split}_df.pkl\'\n        df_save_path = output_folder.join(f\'DataFrame/{split}_df.pkl\')\n        split_df.to_pickle(filename)\n        print(f\'Uploading data to {str(df_save_path)}\')\n        df_save_path.copy_from(RichPath.create(filename))\n        os.unlink(filename)\n\n\nif __name__ == \'__main__\':\n    args = docopt(__doc__)\n    run_and_debug(lambda: run(args), args.get(\'--debug\'))\n'"
src/dataextraction/utils.py,0,"b'import re\r\nfrom typing import List\r\n\r\nDOCSTRING_REGEX_TOKENIZER = re.compile(r""[^\\s,\'\\""`.():\\[\\]=*;>{\\}+-/\\\\]+|\\\\+|\\.+|\\(\\)|{\\}|\\[\\]|\\(+|\\)+|:+|\\[+|\\]+|{+|\\}+|=+|\\*+|;+|>+|\\++|-+|/+"")\r\n\r\n\r\ndef tokenize_docstring_from_string(docstr: str) -> List[str]:\r\n    return [t for t in DOCSTRING_REGEX_TOKENIZER.findall(docstr) if t is not None and len(t) > 0]'"
src/encoders/__init__.py,0,"b'from .encoder import Encoder, QueryType\nfrom .nbow_seq_encoder import NBoWEncoder\nfrom .rnn_seq_encoder import RNNEncoder\nfrom .self_att_encoder import SelfAttentionEncoder\nfrom .conv_seq_encoder import ConvolutionSeqEncoder\nfrom .conv_self_att_encoder import ConvSelfAttentionEncoder'"
src/encoders/conv_self_att_encoder.py,5,"b'from typing import Dict, Any\n\nimport tensorflow as tf\n\nfrom .utils.bert_self_attention import BertConfig, BertModel\nfrom .masked_seq_encoder import MaskedSeqEncoder\nfrom utils.tfutils import get_activation, pool_sequence_embedding\n\n\nclass ConvSelfAttentionEncoder(MaskedSeqEncoder):\n    @classmethod\n    def get_default_hyperparameters(cls) -> Dict[str, Any]:\n        encoder_hypers = {\'1dcnn_position_encoding\': \'none\',  # One of {\'none\', \'learned\'}\n                          \'1dcnn_layer_list\': [128, 128],\n                          \'1dcnn_kernel_width\': [8, 8],  # Has to have same length as 1dcnn_layer_list\n                          \'1dcnn_add_residual_connections\': True,\n                          \'1dcnn_activation\': \'tanh\',\n\n                          \'self_attention_activation\': \'gelu\',\n                          \'self_attention_hidden_size\': 128,\n                          \'self_attention_intermediate_size\': 512,\n                          \'self_attention_num_layers\': 2,\n                          \'self_attention_num_heads\': 8,\n                          \'self_attention_pool_mode\': \'weighted_mean\',\n                          }\n        hypers = super().get_default_hyperparameters()\n        hypers.update(encoder_hypers)\n        return hypers\n\n    def __init__(self, label: str, hyperparameters: Dict[str, Any], metadata: Dict[str, Any]):\n        super().__init__(label, hyperparameters, metadata)\n\n    @property\n    def output_representation_size(self):\n        return self.get_hyper(\'self_attention_hidden_size\')\n\n    def make_model(self, is_train: bool = False) -> tf.Tensor:\n        with tf.variable_scope(""self_attention_encoder""):\n            self._make_placeholders()\n\n            seq_tokens_embeddings = self.embedding_layer(self.placeholders[\'tokens\'])\n\n            activation_fun = get_activation(self.get_hyper(\'1dcnn_activation\'))\n            current_embeddings = seq_tokens_embeddings\n            num_filters_and_width = zip(self.get_hyper(\'1dcnn_layer_list\'), self.get_hyper(\'1dcnn_kernel_width\'))\n            for (layer_idx, (num_filters, kernel_width)) in enumerate(num_filters_and_width):\n                next_embeddings = tf.layers.conv1d(\n                    inputs=current_embeddings,\n                    filters=num_filters,\n                    kernel_size=kernel_width,\n                    padding=""same"")\n\n                # Add residual connections past the first layer.\n                if self.get_hyper(\'1dcnn_add_residual_connections\') and layer_idx > 0:\n                    next_embeddings += current_embeddings\n\n                current_embeddings = activation_fun(next_embeddings)\n\n                current_embeddings = tf.nn.dropout(current_embeddings,\n                                                   keep_prob=self.placeholders[\'dropout_keep_rate\'])\n\n            config = BertConfig(vocab_size=self.get_hyper(\'token_vocab_size\'),\n                                hidden_size=self.get_hyper(\'self_attention_hidden_size\'),\n                                num_hidden_layers=self.get_hyper(\'self_attention_num_layers\'),\n                                num_attention_heads=self.get_hyper(\'self_attention_num_heads\'),\n                                intermediate_size=self.get_hyper(\'self_attention_intermediate_size\'))\n\n            model = BertModel(config=config,\n                              is_training=is_train,\n                              input_ids=self.placeholders[\'tokens\'],\n                              input_mask=self.placeholders[\'tokens_mask\'],\n                              use_one_hot_embeddings=False,\n                              embedded_input=current_embeddings)\n\n            output_pool_mode = self.get_hyper(\'self_attention_pool_mode\').lower()\n            if output_pool_mode == \'bert\':\n                return model.get_pooled_output()\n            else:\n                seq_token_embeddings = model.get_sequence_output()\n                seq_token_masks = self.placeholders[\'tokens_mask\']\n                seq_token_lengths = tf.reduce_sum(seq_token_masks, axis=1)  # B\n                return pool_sequence_embedding(output_pool_mode,\n                                               sequence_token_embeddings=seq_token_embeddings,\n                                               sequence_lengths=seq_token_lengths,\n                                               sequence_token_masks=seq_token_masks)\n'"
src/encoders/conv_seq_encoder.py,9,"b'from typing import Dict, Any\n\nimport tensorflow as tf\n\nfrom .masked_seq_encoder import MaskedSeqEncoder\nfrom utils.tfutils import get_activation, pool_sequence_embedding\n\n\nclass ConvolutionSeqEncoder(MaskedSeqEncoder):\n    @classmethod\n    def get_default_hyperparameters(cls) -> Dict[str, Any]:\n        encoder_hypers = {\'1dcnn_position_encoding\': \'learned\',  # One of {\'none\', \'learned\'}\n                          \'1dcnn_layer_list\': [128, 128, 128],\n                          \'1dcnn_kernel_width\': [16, 16, 16],  # Has to have same length as 1dcnn_layer_list\n                          \'1dcnn_add_residual_connections\': True,\n                          \'1dcnn_activation\': \'tanh\',\n                          \'1dcnn_pool_mode\': \'weighted_mean\',\n                         }\n        hypers = super().get_default_hyperparameters()\n        hypers.update(encoder_hypers)\n        return hypers\n\n    def __init__(self, label: str, hyperparameters: Dict[str, Any], metadata: Dict[str, Any]):\n        super().__init__(label, hyperparameters, metadata)\n\n    @property\n    def output_representation_size(self):\n        return self.get_hyper(\'1dcnn_layer_list\')[-1]\n\n    def make_model(self, is_train: bool=False) -> tf.Tensor:\n        with tf.variable_scope(""1dcnn_encoder""):\n            self._make_placeholders()\n\n            seq_tokens_embeddings = self.embedding_layer(self.placeholders[\'tokens\'])\n            seq_tokens_embeddings = self.__add_position_encoding(seq_tokens_embeddings)\n\n            activation_fun = get_activation(self.get_hyper(\'1dcnn_activation\'))\n            current_embeddings = seq_tokens_embeddings\n            num_filters_and_width = zip(self.get_hyper(\'1dcnn_layer_list\'), self.get_hyper(\'1dcnn_kernel_width\'))\n            for (layer_idx, (num_filters, kernel_width)) in enumerate(num_filters_and_width):\n                next_embeddings = tf.layers.conv1d(\n                    inputs=current_embeddings,\n                    filters=num_filters,\n                    kernel_size=kernel_width,\n                    padding=""same"")\n\n                # Add residual connections past the first layer.\n                if self.get_hyper(\'1dcnn_add_residual_connections\') and layer_idx > 0:\n                    next_embeddings += current_embeddings\n\n                current_embeddings = activation_fun(next_embeddings)\n\n                current_embeddings = tf.nn.dropout(current_embeddings,\n                                                   keep_prob=self.placeholders[\'dropout_keep_rate\'])\n\n            seq_token_mask = self.placeholders[\'tokens_mask\']\n            seq_token_lengths = tf.reduce_sum(seq_token_mask, axis=1)  # B\n            return pool_sequence_embedding(self.get_hyper(\'1dcnn_pool_mode\').lower(),\n                                           sequence_token_embeddings=current_embeddings,\n                                           sequence_lengths=seq_token_lengths,\n                                           sequence_token_masks=seq_token_mask)\n\n    def __add_position_encoding(self, seq_inputs: tf.Tensor) -> tf.Tensor:\n        position_encoding = self.get_hyper(\'1dcnn_position_encoding\').lower()\n        if position_encoding == \'none\':\n            return seq_inputs\n        elif position_encoding == \'learned\':\n            position_embeddings = \\\n                tf.get_variable(name=\'position_embeddings\',\n                                initializer=tf.truncated_normal_initializer(stddev=0.02),\n                                shape=[self.get_hyper(\'max_num_tokens\'),\n                                       self.get_hyper(\'token_embedding_size\')],\n                                )\n            # Add batch dimension to position embeddings to make broadcasting work, then add:\n            return seq_inputs + tf.expand_dims(position_embeddings, axis=0)\n        else:\n            raise ValueError(""Unknown position encoding \'%s\'!"" % position_encoding)\n'"
src/encoders/encoder.py,4,"b'from abc import ABC, abstractmethod\nfrom enum import Enum\nfrom typing import List, Dict, Any, Optional, Tuple\n\nimport tensorflow as tf\n\n\nclass QueryType(Enum):\n    DOCSTRING = \'docstring_as_query\'\n    FUNCTION_NAME = \'func_name_as_query\'\n\n\nclass Encoder(ABC):\n    @classmethod\n    @abstractmethod\n    def get_default_hyperparameters(cls) -> Dict[str, Any]:\n        """"""\n        Returns:\n             Default set of hyperparameters for encoder.\n             Note that at use, the hyperparameters names will be prefixed with \'${label}_\' for the\n             chosen encoder label.\n        """"""\n        return {}\n\n    def __init__(self, label: str, hyperparameters: Dict[str, Any], metadata: Dict[str, Any]):\n        """"""\n        Args:\n            label: Label for the encoder, used in names of hyperparameters.\n            hyperparameters: Hyperparameters used.\n            metadata: Dictionary with metadata (e.g., vocabularies) used by this encoder.\n        """"""\n        self.__label = label\n        self.__hyperparameters = hyperparameters\n        self.__metadata = metadata\n        self.__placeholders = {}\n\n    @property\n    def label(self):\n        return self.__label\n\n    @property\n    def hyperparameters(self):\n        return self.__hyperparameters\n\n    @property\n    def metadata(self):\n        return self.__metadata\n\n    @property\n    def placeholders(self):\n        return self.__placeholders\n\n    @property\n    @abstractmethod\n    def output_representation_size(self) -> int:\n        raise Exception(""Encoder.output_representation_size not implemented!"")\n\n    def get_hyper(self, hyper_name: str) -> Any:\n        """"""\n        Retrieve hyper parameter, prefixing the given name with the label of the encoder.\n\n        Args:\n            hyper_name: Some hyperparameter name.\n\n        Returns:\n            self.hyperparameters[\'%s_%s\' % (self.label, hyper_name)]\n        """"""\n        return self.hyperparameters[\'%s_%s\' % (self.label, hyper_name)]\n\n    def _make_placeholders(self):\n        """"""\n        Creates placeholders for encoders.\n        """"""\n        self.__placeholders[\'dropout_keep_rate\'] = \\\n            tf.placeholder(tf.float32,\n                           shape=(),\n                           name=\'dropout_keep_rate\')\n\n    @abstractmethod\n    def make_model(self, is_train: bool=False) -> tf.Tensor:\n        """"""\n        Create the actual encoder model, including necessary placeholders and parameters.\n\n        Args:\n            is_train: Bool flag indicating if the model is used for training or inference.\n\n        Returns:\n            A tensor encoding the passed data.\n        """"""\n        pass\n\n    @classmethod\n    @abstractmethod\n    def init_metadata(cls) -> Dict[str, Any]:\n        """"""\n        Called to initialise the metadata before looking at actual data (i.e., set up Counters, lists, sets, ...)\n\n        Returns:\n            A dictionary that will be used to collect the raw metadata (token counts, ...).\n        """"""\n        return {}\n\n    @classmethod\n    @abstractmethod\n    def load_metadata_from_sample(cls, data_to_load: Any, raw_metadata: Dict[str, Any],\n                                  use_subtokens: bool=False, mark_subtoken_end: bool=False) -> None:\n        """"""\n        Called to load metadata from a single sample.\n\n        Args:\n            data_to_load: Raw data to load; type depens on encoder. Usually comes from a data parser such as\n             tokenize_python_from_string or tokenize_docstring_from_string.\n            raw_metadata: A dictionary that will be used to collect the raw metadata (token counts, ...).\n            use_subtokens: subtokenize identifiers\n            mark_subtoken_end: add a special marker for subtoken ends. Used only if use_subtokens=True\n        """"""\n        pass\n\n    @classmethod\n    @abstractmethod\n    def finalise_metadata(cls, encoder_label: str, hyperparameters: Dict[str, Any], raw_metadata_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n        """"""\n        Called to finalise the metadata after looking at actual data (i.e., compute vocabularies, ...)\n\n        Args:\n            encoder_label: Label used for this encoder.\n            hyperparameters: Hyperparameters used.\n            raw_metadata_list: List of dictionaries used to collect the raw metadata (token counts, ...) (one per file).\n\n        Returns:\n            Finalised metadata (vocabs, ...).\n        """"""\n        return {}\n\n    @classmethod\n    @abstractmethod\n    def load_data_from_sample(cls,\n                              encoder_label: str,\n                              hyperparameters: Dict[str, Any],\n                              metadata: Dict[str, Any],\n                              data_to_load: Any,\n                              function_name: Optional[str],\n                              result_holder: Dict[str, Any],\n                              is_test: bool=True) -> bool:\n        """"""\n        Called to convert a raw sample into the internal format, allowing for preprocessing.\n        Result will eventually be fed again into the split_data_into_minibatches pipeline.\n\n        Args:\n            encoder_label: Label used for this encoder.\n            hyperparameters: Hyperparameters used to load data.\n            metadata: Computed metadata (e.g. vocabularies).\n            data_to_load: Raw data to load; type depens on encoder. Usually comes from a data parser such as\n             tokenize_python_from_string or tokenize_docstring_from_string.\n             function_name: The name of the function.\n            result_holder: Dictionary used to hold the prepared data.\n            is_test: Flag marking if we are handling training data or not.\n\n        Returns:\n            Flag indicating if the example should be used (True) or dropped (False)\n        """"""\n        return True\n\n    @abstractmethod\n    def init_minibatch(self, batch_data: Dict[str, Any]) -> None:\n        """"""\n        Initialise a minibatch that will be constructed.\n\n        Args:\n            batch_data: The minibatch data.\n        """"""\n        pass\n\n    @abstractmethod\n    def extend_minibatch_by_sample(self, batch_data: Dict[str, Any], sample: Dict[str, Any], is_train: bool=False,\n                                   query_type: QueryType=QueryType.DOCSTRING.value) -> bool:\n        """"""\n        Extend a minibatch under construction by one sample. This is where the data may be randomly perturbed in each\n        epoch for data augmentation.\n\n        Args:\n            batch_data: The minibatch data.\n            sample: The sample to add.\n            is_train: Flag indicating if we are in train mode (which causes data augmentation)\n            query_type: Indicates what should be used as the query, the docstring or the function name.\n\n        Returns:\n            True iff the minibatch is full after this sample.\n        """"""\n        return True\n\n    @abstractmethod\n    def minibatch_to_feed_dict(self, batch_data: Dict[str, Any], feed_dict: Dict[tf.Tensor, Any], is_train: bool) -> None:\n        """"""\n        Take a collected minibatch and add it to a feed dict that can be fed directly to the constructed model.\n\n        Args:\n            batch_data: The minibatch data.\n            feed_dict: The feed dictionary that we will send to tensorflow.\n            is_train: Flag indicating if we are in training mode.\n        """"""\n        feed_dict[self.placeholders[\'dropout_keep_rate\']] = self.hyperparameters[\'dropout_keep_rate\'] if is_train else 1.0\n\n    @abstractmethod\n    def get_token_embeddings(self) -> Tuple[tf.Tensor, List[str]]:\n        """"""Returns the tensorflow embeddings tensor (VxD) along with a list (of size V) of the names of the\n        embedded elements.""""""\n        pass\n'"
src/encoders/masked_seq_encoder.py,2,"b'from typing import Dict, Any, Iterable, Optional\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom .seq_encoder import SeqEncoder\nfrom utils.tfutils import write_to_feed_dict, pool_sequence_embedding\n\n\nclass MaskedSeqEncoder(SeqEncoder):\n    @classmethod\n    def get_default_hyperparameters(cls) -> Dict[str, Any]:\n        encoder_hypers = {\n                         }\n        hypers = super().get_default_hyperparameters()\n        hypers.update(encoder_hypers)\n        return hypers\n\n    def __init__(self, label: str, hyperparameters: Dict[str, Any], metadata: Dict[str, Any]):\n        super().__init__(label, hyperparameters, metadata)\n\n    def _make_placeholders(self):\n        """"""\n        Creates placeholders ""tokens"" and ""tokens_mask"" for masked sequence encoders.\n        """"""\n        super()._make_placeholders()\n        self.placeholders[\'tokens_mask\'] = \\\n            tf.placeholder(tf.float32,\n                           shape=[None, self.get_hyper(\'max_num_tokens\')],\n                           name=\'tokens_mask\')\n\n    def init_minibatch(self, batch_data: Dict[str, Any]) -> None:\n        super().init_minibatch(batch_data)\n        batch_data[\'tokens\'] = []\n        batch_data[\'tokens_mask\'] = []\n\n    def minibatch_to_feed_dict(self, batch_data: Dict[str, Any], feed_dict: Dict[tf.Tensor, Any], is_train: bool) -> None:\n        super().minibatch_to_feed_dict(batch_data, feed_dict, is_train)\n        write_to_feed_dict(feed_dict, self.placeholders[\'tokens\'], batch_data[\'tokens\'])\n        write_to_feed_dict(feed_dict, self.placeholders[\'tokens_mask\'], batch_data[\'tokens_mask\'])\n'"
src/encoders/nbow_seq_encoder.py,3,"b'from typing import Dict, Any\n\nimport tensorflow as tf\n\nfrom .masked_seq_encoder import MaskedSeqEncoder\nfrom utils.tfutils import pool_sequence_embedding\n\n\nclass NBoWEncoder(MaskedSeqEncoder):\n    @classmethod\n    def get_default_hyperparameters(cls) -> Dict[str, Any]:\n        encoder_hypers = { \'nbow_pool_mode\': \'weighted_mean\',\n                         }\n        hypers = super().get_default_hyperparameters()\n        hypers.update(encoder_hypers)\n        return hypers\n\n    def __init__(self, label: str, hyperparameters: Dict[str, Any], metadata: Dict[str, Any]):\n        super().__init__(label, hyperparameters, metadata)\n\n    @property\n    def output_representation_size(self):\n        return self.get_hyper(\'token_embedding_size\')\n\n    def make_model(self, is_train: bool=False) -> tf.Tensor:\n        with tf.variable_scope(""nbow_encoder""):\n            self._make_placeholders()\n\n            seq_tokens_embeddings = self.embedding_layer(self.placeholders[\'tokens\'])\n            seq_token_mask = self.placeholders[\'tokens_mask\']\n            seq_token_lengths = tf.reduce_sum(seq_token_mask, axis=1)  # B\n            return pool_sequence_embedding(self.get_hyper(\'nbow_pool_mode\').lower(),\n                                           sequence_token_embeddings=seq_tokens_embeddings,\n                                           sequence_lengths=seq_token_lengths,\n                                           sequence_token_masks=seq_token_mask)\n'"
src/encoders/rnn_seq_encoder.py,33,"b'from typing import Dict, Any, Union, Tuple\n\nimport tensorflow as tf\n\nfrom .seq_encoder import SeqEncoder\nfrom utils.tfutils import write_to_feed_dict, pool_sequence_embedding\n\n\ndef __make_rnn_cell(cell_type: str,\n                    hidden_size: int,\n                    dropout_keep_rate: Union[float, tf.Tensor]=1.0,\n                    recurrent_dropout_keep_rate: Union[float, tf.Tensor]=1.0) \\\n        -> tf.nn.rnn_cell.RNNCell:\n    """"""\n    Args:\n        cell_type: ""lstm"", ""gru"", or \'rnn\' (any casing)\n        hidden_size: size for the underlying recurrent unit\n        dropout_keep_rate: output-vector dropout prob\n        recurrent_dropout_keep_rate:  state-vector dropout prob\n\n    Returns:\n        RNNCell of the desired type.\n    """"""\n    cell_type = cell_type.lower()\n    if cell_type == \'lstm\':\n        cell = tf.nn.rnn_cell.LSTMCell(hidden_size)\n    elif cell_type == \'gru\':\n        cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n    elif cell_type == \'rnn\':\n        cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)\n    else:\n        raise ValueError(""Unknown RNN cell type \'%s\'!"" % cell_type)\n\n    return tf.contrib.rnn.DropoutWrapper(cell,\n                                         output_keep_prob=dropout_keep_rate,\n                                         state_keep_prob=recurrent_dropout_keep_rate)\n\n\ndef _make_deep_rnn_cell(num_layers: int,\n                        cell_type: str,\n                        hidden_size: int,\n                        dropout_keep_rate: Union[float, tf.Tensor]=1.0,\n                        recurrent_dropout_keep_rate: Union[float, tf.Tensor]=1.0) \\\n        -> tf.nn.rnn_cell.RNNCell:\n    """"""\n    Args:\n        num_layers: number of layers in result\n        cell_type: ""lstm"" or ""gru"" (any casing)\n        hidden_size: size for the underlying recurrent unit\n        dropout_keep_rate: output-vector dropout prob\n        recurrent_dropout_keep_rate: state-vector dropout prob\n\n    Returns:\n        (Multi)RNNCell of the desired type.\n    """"""\n    if num_layers == 1:\n        return __make_rnn_cell(cell_type, hidden_size, dropout_keep_rate, recurrent_dropout_keep_rate)\n    else:\n        cells = [__make_rnn_cell(cell_type, hidden_size, dropout_keep_rate, recurrent_dropout_keep_rate)\n                 for _ in range(num_layers)]\n        return tf.nn.rnn_cell.MultiRNNCell(cells)\n\n\nclass RNNEncoder(SeqEncoder):\n    @classmethod\n    def get_default_hyperparameters(cls) -> Dict[str, Any]:\n        encoder_hypers = {\'rnn_num_layers\': 2,\n                          \'rnn_hidden_dim\': 64,\n                          \'rnn_cell_type\': \'LSTM\',  # One of [LSTM, GRU, RNN]\n                          \'rnn_is_bidirectional\': True,\n                          \'rnn_dropout_keep_rate\': 0.8,\n                          \'rnn_recurrent_dropout_keep_rate\': 1.0,\n                          \'rnn_pool_mode\': \'weighted_mean\',\n                          }\n        hypers = super().get_default_hyperparameters()\n        hypers.update(encoder_hypers)\n        return hypers\n\n    def __init__(self, label: str, hyperparameters: Dict[str, Any], metadata: Dict[str, Any]):\n        super().__init__(label, hyperparameters, metadata)\n\n    @property\n    def output_representation_size(self):\n        if self.get_hyper(\'rnn_is_bidirectional\'):\n            return 2 * self.get_hyper(\'rnn_hidden_dim\')\n        else:\n            return self.get_hyper(\'rnn_hidden_dim\')\n\n    def _encode_with_rnn(self,\n                         inputs: tf.Tensor,\n                         input_lengths: tf.Tensor) \\\n            -> Tuple[tf.Tensor, tf.Tensor]:\n        cell_type = self.get_hyper(\'rnn_cell_type\').lower()\n        rnn_cell_fwd = _make_deep_rnn_cell(num_layers=self.get_hyper(\'rnn_num_layers\'),\n                                           cell_type=cell_type,\n                                           hidden_size=self.get_hyper(\'rnn_hidden_dim\'),\n                                           dropout_keep_rate=self.placeholders[\'rnn_dropout_keep_rate\'],\n                                           recurrent_dropout_keep_rate=self.placeholders[\'rnn_recurrent_dropout_keep_rate\'],\n                                           )\n        if not self.get_hyper(\'rnn_is_bidirectional\'):\n            (outputs, final_states) = tf.nn.dynamic_rnn(cell=rnn_cell_fwd,\n                                                        inputs=inputs,\n                                                        sequence_length=input_lengths,\n                                                        dtype=tf.float32,\n                                                        )\n\n            if cell_type == \'lstm\':\n                final_state = tf.concat([tf.concat(layer_final_state, axis=-1)  # concat c & m of LSTM cell\n                                         for layer_final_state in final_states],\n                                        axis=-1)  # concat across layers\n            elif cell_type == \'gru\' or cell_type == \'rnn\':\n                final_state = tf.concat(final_states, axis=-1)\n            else:\n                raise ValueError(""Unknown RNN cell type \'%s\'!"" % cell_type)\n        else:\n            rnn_cell_bwd = _make_deep_rnn_cell(num_layers=self.get_hyper(\'rnn_num_layers\'),\n                                               cell_type=cell_type,\n                                               hidden_size=self.get_hyper(\'rnn_hidden_dim\'),\n                                               dropout_keep_rate=self.placeholders[\'rnn_dropout_keep_rate\'],\n                                               recurrent_dropout_keep_rate=self.placeholders[\'rnn_recurrent_dropout_keep_rate\'],\n                                               )\n\n            (outputs, final_states) = tf.nn.bidirectional_dynamic_rnn(cell_fw=rnn_cell_fwd,\n                                                                      cell_bw=rnn_cell_bwd,\n                                                                      inputs=inputs,\n                                                                      sequence_length=input_lengths,\n                                                                      dtype=tf.float32,\n                                                                      )\n            # Merge fwd/bwd outputs:\n            if cell_type == \'lstm\':\n                final_state = tf.concat([tf.concat([tf.concat(layer_final_state, axis=-1)  # concat c & m of LSTM cell\n                                                    for layer_final_state in layer_final_states],\n                                                   axis=-1)  # concat across layers\n                                        for layer_final_states in final_states],\n                                        axis=-1)  # concat fwd & bwd\n            elif cell_type == \'gru\' or cell_type == \'rnn\':\n                final_state = tf.concat([tf.concat(layer_final_states, axis=-1)  # concat across layers\n                                         for layer_final_states in final_states],\n                                        axis=-1)  # concat fwd & bwd\n            else:\n                raise ValueError(""Unknown RNN cell type \'%s\'!"" % cell_type)\n            outputs = tf.concat(outputs, axis=-1)  # concat fwd & bwd\n\n        return final_state, outputs\n\n    def make_model(self, is_train: bool=False) -> tf.Tensor:\n        with tf.variable_scope(""rnn_encoder""):\n            self._make_placeholders()\n\n            self.placeholders[\'tokens_lengths\'] = \\\n                tf.placeholder(tf.int32,\n                               shape=[None],\n                               name=\'tokens_lengths\')\n\n            self.placeholders[\'rnn_dropout_keep_rate\'] = \\\n                tf.placeholder(tf.float32,\n                               shape=[],\n                               name=\'rnn_dropout_keep_rate\')\n\n            self.placeholders[\'rnn_recurrent_dropout_keep_rate\'] = \\\n                tf.placeholder(tf.float32,\n                               shape=[],\n                               name=\'rnn_recurrent_dropout_keep_rate\')\n\n            seq_tokens = self.placeholders[\'tokens\']\n            seq_tokens_embeddings = self.embedding_layer(seq_tokens)\n            seq_tokens_lengths = self.placeholders[\'tokens_lengths\']\n\n            rnn_final_state, token_embeddings = self._encode_with_rnn(seq_tokens_embeddings, seq_tokens_lengths)\n\n            output_pool_mode = self.get_hyper(\'rnn_pool_mode\').lower()\n            if output_pool_mode == \'rnn_final\':\n                return rnn_final_state\n            else:\n                token_mask = tf.expand_dims(tf.range(tf.shape(seq_tokens)[1]), axis=0)            # 1 x T\n                token_mask = tf.tile(token_mask, multiples=(tf.shape(seq_tokens_lengths)[0], 1))  # B x T\n                token_mask = tf.cast(token_mask < tf.expand_dims(seq_tokens_lengths, axis=-1),\n                                     dtype=tf.float32)                                            # B x T\n                return pool_sequence_embedding(output_pool_mode,\n                                               sequence_token_embeddings=token_embeddings,\n                                               sequence_lengths=seq_tokens_lengths,\n                                               sequence_token_masks=token_mask)\n\n    def init_minibatch(self, batch_data: Dict[str, Any]) -> None:\n        super().init_minibatch(batch_data)\n        batch_data[\'tokens\'] = []\n        batch_data[\'tokens_lengths\'] = []\n\n    def minibatch_to_feed_dict(self, batch_data: Dict[str, Any], feed_dict: Dict[tf.Tensor, Any], is_train: bool) -> None:\n        super().minibatch_to_feed_dict(batch_data, feed_dict, is_train)\n        feed_dict[self.placeholders[\'rnn_dropout_keep_rate\']] = \\\n            self.get_hyper(\'rnn_dropout_keep_rate\') if is_train else 1.0\n        feed_dict[self.placeholders[\'rnn_recurrent_dropout_keep_rate\']] = \\\n            self.get_hyper(\'rnn_recurrent_dropout_keep_rate\') if is_train else 1.0\n\n        write_to_feed_dict(feed_dict, self.placeholders[\'tokens\'], batch_data[\'tokens\'])\n        write_to_feed_dict(feed_dict, self.placeholders[\'tokens_lengths\'], batch_data[\'tokens_lengths\'])\n'"
src/encoders/self_att_encoder.py,3,"b'from typing import Dict, Any\n\nimport tensorflow as tf\n\nfrom .utils.bert_self_attention import BertConfig, BertModel\nfrom .masked_seq_encoder import MaskedSeqEncoder\nfrom utils.tfutils import pool_sequence_embedding\n\n\nclass SelfAttentionEncoder(MaskedSeqEncoder):\n    @classmethod\n    def get_default_hyperparameters(cls) -> Dict[str, Any]:\n        encoder_hypers = {\'self_attention_activation\': \'gelu\',\n                          \'self_attention_hidden_size\': 128,\n                          \'self_attention_intermediate_size\': 512,\n                          \'self_attention_num_layers\': 3,\n                          \'self_attention_num_heads\': 8,\n                          \'self_attention_pool_mode\': \'weighted_mean\',\n                          }\n        hypers = super().get_default_hyperparameters()\n        hypers.update(encoder_hypers)\n        return hypers\n\n    def __init__(self, label: str, hyperparameters: Dict[str, Any], metadata: Dict[str, Any]):\n        super().__init__(label, hyperparameters, metadata)\n\n    @property\n    def output_representation_size(self):\n        return self.get_hyper(\'self_attention_hidden_size\')\n\n    def make_model(self, is_train: bool = False) -> tf.Tensor:\n        with tf.variable_scope(""self_attention_encoder""):\n            self._make_placeholders()\n\n            config = BertConfig(vocab_size=self.get_hyper(\'token_vocab_size\'),\n                                hidden_size=self.get_hyper(\'self_attention_hidden_size\'),\n                                num_hidden_layers=self.get_hyper(\'self_attention_num_layers\'),\n                                num_attention_heads=self.get_hyper(\'self_attention_num_heads\'),\n                                intermediate_size=self.get_hyper(\'self_attention_intermediate_size\'))\n\n            model = BertModel(config=config,\n                              is_training=is_train,\n                              input_ids=self.placeholders[\'tokens\'],\n                              input_mask=self.placeholders[\'tokens_mask\'],\n                              use_one_hot_embeddings=False)\n\n            output_pool_mode = self.get_hyper(\'self_attention_pool_mode\').lower()\n            if output_pool_mode == \'bert\':\n                return model.get_pooled_output()\n            else:\n                seq_token_embeddings = model.get_sequence_output()\n                seq_token_masks = self.placeholders[\'tokens_mask\']\n                seq_token_lengths = tf.reduce_sum(seq_token_masks, axis=1)  # B\n                return pool_sequence_embedding(output_pool_mode,\n                                               sequence_token_embeddings=seq_token_embeddings,\n                                               sequence_lengths=seq_token_lengths,\n                                               sequence_token_masks=seq_token_masks)\n'"
src/encoders/seq_encoder.py,7,"b'from collections import Counter\nimport numpy as np\nfrom typing import Dict, Any, List, Iterable, Optional, Tuple\nimport random\nimport re\n\nfrom utils.bpevocabulary import BpeVocabulary\nfrom utils.tfutils import convert_and_pad_token_sequence\n\nimport tensorflow as tf\nfrom dpu_utils.codeutils import split_identifier_into_parts\nfrom dpu_utils.mlutils import Vocabulary\n\nfrom .encoder import Encoder, QueryType\n\n\nclass SeqEncoder(Encoder):\n    @classmethod\n    def get_default_hyperparameters(cls) -> Dict[str, Any]:\n        encoder_hypers = { \'token_vocab_size\': 10000,\n                           \'token_vocab_count_threshold\': 10,\n                           \'token_embedding_size\': 128,\n\n                           \'use_subtokens\': False,\n                           \'mark_subtoken_end\': False,\n\n                           \'max_num_tokens\': 200,\n\n                           \'use_bpe\': True,\n                           \'pct_bpe\': 0.5\n                         }\n        hypers = super().get_default_hyperparameters()\n        hypers.update(encoder_hypers)\n        return hypers\n\n    IDENTIFIER_TOKEN_REGEX = re.compile(\'[_a-zA-Z][_a-zA-Z0-9]*\')\n\n    def __init__(self, label: str, hyperparameters: Dict[str, Any], metadata: Dict[str, Any]):\n        super().__init__(label, hyperparameters, metadata)\n        if hyperparameters[\'%s_use_bpe\' % label]:\n            assert not hyperparameters[\'%s_use_subtokens\' % label], \'Subtokens cannot be used along with BPE.\'\n        elif hyperparameters[\'%s_use_subtokens\' % label]:\n            assert not hyperparameters[\'%s_use_bpe\' % label], \'Subtokens cannot be used along with BPE.\'\n\n    def _make_placeholders(self):\n        """"""\n        Creates placeholders ""tokens"" for sequence encoders.\n        """"""\n        super()._make_placeholders()\n        self.placeholders[\'tokens\'] = \\\n            tf.placeholder(tf.int32,\n                           shape=[None, self.get_hyper(\'max_num_tokens\')],\n                           name=\'tokens\')\n\n    def embedding_layer(self, token_inp: tf.Tensor) -> tf.Tensor:\n        """"""\n        Creates embedding layer that is in common between many encoders.\n\n        Args:\n            token_inp:  2D tensor that is of shape (batch size, sequence length)\n\n        Returns:\n            3D tensor of shape (batch size, sequence length, embedding dimension)\n        """"""\n\n        token_embeddings = tf.get_variable(name=\'token_embeddings\',\n                                           initializer=tf.glorot_uniform_initializer(),\n                                           shape=[len(self.metadata[\'token_vocab\']),\n                                                  self.get_hyper(\'token_embedding_size\')],\n                                           )\n        self.__embeddings = token_embeddings\n\n        token_embeddings = tf.nn.dropout(token_embeddings,\n                                         keep_prob=self.placeholders[\'dropout_keep_rate\'])\n\n        return tf.nn.embedding_lookup(params=token_embeddings, ids=token_inp)\n\n    @classmethod\n    def init_metadata(cls) -> Dict[str, Any]:\n        raw_metadata = super().init_metadata()\n        raw_metadata[\'token_counter\'] = Counter()\n        return raw_metadata\n\n    @classmethod\n    def _to_subtoken_stream(cls, input_stream: Iterable[str], mark_subtoken_end: bool) -> Iterable[str]:\n        for token in input_stream:\n            if SeqEncoder.IDENTIFIER_TOKEN_REGEX.match(token):\n                yield from split_identifier_into_parts(token)\n                if mark_subtoken_end:\n                    yield \'</id>\'\n            else:\n                yield token\n\n    @classmethod\n    def load_metadata_from_sample(cls, data_to_load: Iterable[str], raw_metadata: Dict[str, Any],\n                                  use_subtokens: bool=False, mark_subtoken_end: bool=False) -> None:\n        if use_subtokens:\n            data_to_load = cls._to_subtoken_stream(data_to_load, mark_subtoken_end=mark_subtoken_end)\n        raw_metadata[\'token_counter\'].update(data_to_load)\n\n    @classmethod\n    def finalise_metadata(cls, encoder_label: str, hyperparameters: Dict[str, Any], raw_metadata_list: List[Dict[str, Any]]) -> Dict[str, Any]:\n        final_metadata = super().finalise_metadata(encoder_label, hyperparameters, raw_metadata_list)\n        merged_token_counter = Counter()\n        for raw_metadata in raw_metadata_list:\n            merged_token_counter += raw_metadata[\'token_counter\']\n\n        if hyperparameters[\'%s_use_bpe\' % encoder_label]:\n            token_vocabulary = BpeVocabulary(vocab_size=hyperparameters[\'%s_token_vocab_size\' % encoder_label],\n                                             pct_bpe=hyperparameters[\'%s_pct_bpe\' % encoder_label]\n                                             )\n            token_vocabulary.fit(merged_token_counter)\n        else:\n            token_vocabulary = Vocabulary.create_vocabulary(tokens=merged_token_counter,\n                                                            max_size=hyperparameters[\'%s_token_vocab_size\' % encoder_label],\n                                                            count_threshold=hyperparameters[\'%s_token_vocab_count_threshold\' % encoder_label])\n\n        final_metadata[\'token_vocab\'] = token_vocabulary\n        # Save the most common tokens for use in data augmentation:\n        final_metadata[\'common_tokens\'] = merged_token_counter.most_common(50)\n        return final_metadata\n\n    @classmethod\n    def load_data_from_sample(cls,\n                              encoder_label: str,\n                              hyperparameters: Dict[str, Any],\n                              metadata: Dict[str, Any],\n                              data_to_load: Any,\n                              function_name: Optional[str],\n                              result_holder: Dict[str, Any],\n                              is_test: bool = True) -> bool:\n        """"""\n        Saves two versions of both the code and the query: one using the docstring as the query and the other using the\n        function-name as the query, and replacing the function name in the code with an out-of-vocab token.\n        Sub-tokenizes, converts, and pads both versions, and rejects empty samples.\n        """"""\n        # Save the two versions of the code and query:\n        data_holder = {QueryType.DOCSTRING.value: data_to_load, QueryType.FUNCTION_NAME.value: None}\n        # Skip samples where the function name is very short, because it probably has too little information\n        # to be a good search query.\n        if not is_test and hyperparameters[\'fraction_using_func_name\'] > 0. and function_name and \\\n                len(function_name) >= hyperparameters[\'min_len_func_name_for_query\']:\n            if encoder_label == \'query\':\n                # Set the query tokens to the function name, broken up into its sub-tokens:\n                data_holder[QueryType.FUNCTION_NAME.value] = split_identifier_into_parts(function_name)\n            elif encoder_label == \'code\':\n                # In the code, replace the function name with the out-of-vocab token everywhere it appears:\n                data_holder[QueryType.FUNCTION_NAME.value] = [Vocabulary.get_unk() if token == function_name else token\n                                                              for token in data_to_load]\n\n        # Sub-tokenize, convert, and pad both versions:\n        for key, data in data_holder.items():\n            if not data:\n                result_holder[f\'{encoder_label}_tokens_{key}\'] = None\n                result_holder[f\'{encoder_label}_tokens_mask_{key}\'] = None\n                result_holder[f\'{encoder_label}_tokens_length_{key}\'] = None\n                continue\n            if hyperparameters[f\'{encoder_label}_use_subtokens\']:\n                data = cls._to_subtoken_stream(data,\n                                               mark_subtoken_end=hyperparameters[\n                                                   f\'{encoder_label}_mark_subtoken_end\'])\n            tokens, tokens_mask = \\\n                convert_and_pad_token_sequence(metadata[\'token_vocab\'], list(data),\n                                               hyperparameters[f\'{encoder_label}_max_num_tokens\'])\n            # Note that we share the result_holder with different encoders, and so we need to make our identifiers\n            # unique-ish\n            result_holder[f\'{encoder_label}_tokens_{key}\'] = tokens\n            result_holder[f\'{encoder_label}_tokens_mask_{key}\'] = tokens_mask\n            result_holder[f\'{encoder_label}_tokens_length_{key}\'] = int(np.sum(tokens_mask))\n\n        if result_holder[f\'{encoder_label}_tokens_mask_{QueryType.DOCSTRING.value}\'] is None or \\\n                int(np.sum(result_holder[f\'{encoder_label}_tokens_mask_{QueryType.DOCSTRING.value}\'])) == 0:\n            return False\n\n        return True\n\n    def extend_minibatch_by_sample(self, batch_data: Dict[str, Any], sample: Dict[str, Any], is_train: bool=False,\n                                   query_type: QueryType = QueryType.DOCSTRING.value) -> bool:\n        """"""\n        Implements various forms of data augmentation.\n        """"""\n        current_sample = dict()\n\n        # Train with some fraction of samples having their query set to the function name instead of the docstring, and\n        # their function name replaced with out-of-vocab in the code:\n        current_sample[\'tokens\'] = sample[f\'{self.label}_tokens_{query_type}\']\n        current_sample[\'tokens_mask\'] = sample[f\'{self.label}_tokens_mask_{query_type}\']\n        current_sample[\'tokens_lengths\'] = sample[f\'{self.label}_tokens_length_{query_type}\']\n\n        # In the query, randomly add high-frequency tokens:\n        # TODO: Add tokens with frequency proportional to their frequency in the vocabulary\n        if is_train and self.label == \'query\' and self.hyperparameters[\'query_random_token_frequency\'] > 0.:\n            total_length = len(current_sample[\'tokens\'])\n            length_without_padding = current_sample[\'tokens_lengths\']\n            # Generate a list of places in which to insert tokens:\n            insert_indices = np.array([random.uniform(0., 1.) for _ in range(length_without_padding)])  # don\'t allow insertions in the padding\n            insert_indices = insert_indices < self.hyperparameters[\'query_random_token_frequency\']  # insert at the correct frequency\n            insert_indices = np.flatnonzero(insert_indices)\n            if len(insert_indices) > 0:\n                # Generate the random tokens to add:\n                tokens_to_add = [random.randrange(0, len(self.metadata[\'common_tokens\']))\n                                 for _ in range(len(insert_indices))]  # select one of the most common tokens for each location\n                tokens_to_add = [self.metadata[\'common_tokens\'][token][0] for token in tokens_to_add]  # get the word corresponding to the token we\'re adding\n                tokens_to_add = [self.metadata[\'token_vocab\'].get_id_or_unk(token) for token in tokens_to_add]  # get the index within the vocab of the token we\'re adding\n                # Efficiently insert the added tokens, leaving the total length the same:\n                to_insert = 0\n                output_query = np.zeros(total_length, dtype=int)\n                for idx in range(min(length_without_padding, total_length - len(insert_indices))):  # iterate only through the beginning of the array where changes are being made\n                    if to_insert < len(insert_indices) and idx == insert_indices[to_insert]:\n                        output_query[idx + to_insert] = tokens_to_add[to_insert]\n                        to_insert += 1\n                    output_query[idx + to_insert] = current_sample[\'tokens\'][idx]\n                current_sample[\'tokens\'] = output_query\n                # Add the needed number of non-padding values to the mask:\n                current_sample[\'tokens_mask\'][length_without_padding:length_without_padding + len(tokens_to_add)] = 1.\n                current_sample[\'tokens_lengths\'] += len(tokens_to_add)\n\n        # Add the current sample to the minibatch:\n        [batch_data[key].append(current_sample[key]) for key in current_sample.keys() if key in batch_data.keys()]\n\n        return False\n\n    def get_token_embeddings(self) -> Tuple[tf.Tensor, List[str]]:\n        return (self.__embeddings,\n                list(self.metadata[\'token_vocab\'].id_to_token))\n'"
src/models/__init__.py,0,b'from .model import Model\nfrom .nbow_model import NeuralBoWModel\nfrom .rnn_model import RNNModel\nfrom .self_att_model import SelfAttentionModel\nfrom .conv_model import ConvolutionalModel\nfrom .conv_self_att_model import ConvSelfAttentionModel'
src/models/conv_model.py,0,"b'from typing import Any, Dict, Optional\n\nfrom encoders import ConvolutionSeqEncoder\nfrom .model import Model\n\n\nclass ConvolutionalModel(Model):\n    @classmethod\n    def get_default_hyperparameters(cls) -> Dict[str, Any]:\n        hypers = {}\n        for label in [""code"", ""query""]:\n            hypers.update({f\'{label}_{key}\': value\n                           for key, value in ConvolutionSeqEncoder.get_default_hyperparameters().items()})\n        model_hypers = {\n            \'learning_rate\': 5e-4,\n            \'code_use_subtokens\': False,\n            \'code_mark_subtoken_end\': False,\n            \'batch_size\': 1000,\n        }\n        hypers.update(super().get_default_hyperparameters())\n        hypers.update(model_hypers)\n        return hypers\n\n    def __init__(self,\n                 hyperparameters: Dict[str, Any],\n                 run_name: str = None,\n                 model_save_dir: Optional[str] = None,\n                 log_save_dir: Optional[str] = None):\n        super().__init__(\n            hyperparameters,\n            code_encoder_type=ConvolutionSeqEncoder,\n            query_encoder_type=ConvolutionSeqEncoder,\n            run_name=run_name,\n            model_save_dir=model_save_dir,\n            log_save_dir=log_save_dir)\n'"
src/models/conv_self_att_model.py,0,"b'from typing import Any, Dict, Optional\n\nfrom encoders import ConvSelfAttentionEncoder\nfrom .model import Model\n\n\nclass ConvSelfAttentionModel(Model):\n    @classmethod\n    def get_default_hyperparameters(cls) -> Dict[str, Any]:\n        hypers = {}\n        for label in [""code"", ""query""]:\n            hypers.update({f\'{label}_{key}\': value\n                           for key, value in ConvSelfAttentionEncoder.get_default_hyperparameters().items()})\n        model_hypers = {\n            \'learning_rate\': 5e-4,\n            \'code_use_subtokens\': False,\n            \'code_mark_subtoken_end\': False,\n            \'batch_size\': 650,\n        }\n        hypers.update(super().get_default_hyperparameters())\n        hypers.update(model_hypers)\n        return hypers\n\n    def __init__(self,\n                 hyperparameters: Dict[str, Any],\n                 run_name: str = None,\n                 model_save_dir: Optional[str] = None,\n                 log_save_dir: Optional[str] = None):\n        super().__init__(\n            hyperparameters,\n            code_encoder_type=ConvSelfAttentionEncoder,\n            query_encoder_type=ConvSelfAttentionEncoder,\n            run_name=run_name,\n            model_save_dir=model_save_dir,\n            log_save_dir=log_save_dir)\n'"
src/models/model.py,62,"b'import os\nimport itertools\nimport multiprocessing\nimport random\nimport time\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict, OrderedDict\nfrom enum import Enum, auto\nfrom typing import List, Dict, Any, Iterable, Tuple, Optional, Union, Callable, Type, DefaultDict\n\nimport numpy as np\nimport wandb\nimport tensorflow as tf\nfrom dpu_utils.utils import RichPath\n\nfrom utils.py_utils import run_jobs_in_parallel\nfrom encoders import Encoder, QueryType\n\n\nLoadedSamples = Dict[str, List[Dict[str, Any]]]\nSampleId = Tuple[str, int]\n\n\nclass RepresentationType(Enum):\n    CODE = auto()\n    QUERY = auto()\n\n\ndef get_data_files_from_directory(data_dirs: List[RichPath],\n                                  max_files_per_dir: Optional[int] = None) -> List[RichPath]:\n    files = []  # type: List[str]\n    for data_dir in data_dirs:\n        dir_files = data_dir.get_filtered_files_in_dir(\'*.jsonl.gz\')\n        if max_files_per_dir:\n            dir_files = sorted(dir_files)[:int(max_files_per_dir)]\n        files += dir_files\n\n    np.random.shuffle(files)  # This avoids having large_file_0, large_file_1, ... subsequences\n    return files\n\n\ndef parse_data_file(hyperparameters: Dict[str, Any],\n                    code_encoder_class: Type[Encoder],\n                    per_code_language_metadata: Dict[str, Dict[str, Any]],\n                    query_encoder_class: Type[Encoder],\n                    query_metadata: Dict[str, Any],\n                    is_test: bool,\n                    data_file: RichPath) -> Dict[str, List[Tuple[bool, Dict[str, Any]]]]:\n    results: DefaultDict[str, List] = defaultdict(list)\n    for raw_sample in data_file.read_by_file_suffix():\n        sample: Dict = {}\n        language = raw_sample[\'language\']\n        if language.startswith(\'python\'):  # In some datasets, we use \'python-2.7\' and \'python-3\'\n            language = \'python\'\n\n        # the load_data_from_sample method call places processed data into sample, and\n        # returns a boolean flag indicating if sample should be used\n        function_name = raw_sample.get(\'func_name\')\n        use_code_flag = code_encoder_class.load_data_from_sample(""code"",\n                                                                 hyperparameters,\n                                                                 per_code_language_metadata[language],\n                                                                 raw_sample[\'code_tokens\'],\n                                                                 function_name,\n                                                                 sample,\n                                                                 is_test)\n\n        use_query_flag = query_encoder_class.load_data_from_sample(""query"",\n                                                                   hyperparameters,\n                                                                   query_metadata,\n                                                                   [d.lower() for d in raw_sample[\'docstring_tokens\']],\n                                                                   function_name,\n                                                                   sample,\n                                                                   is_test)\n        use_example = use_code_flag and use_query_flag\n        results[language].append((use_example, sample))\n    return results\n\n\nclass Model(ABC):\n    @classmethod\n    @abstractmethod\n    def get_default_hyperparameters(cls) -> Dict[str, Any]:\n        return {\n                \'batch_size\': 200,\n\n                \'optimizer\': \'Adam\',\n                \'seed\': 0,\n                \'dropout_keep_rate\': 0.9,\n                \'learning_rate\': 0.01,\n                \'learning_rate_code_scale_factor\': 1.,\n                \'learning_rate_query_scale_factor\': 1.,\n                \'learning_rate_decay\': 0.98,\n                \'momentum\': 0.85,\n                \'gradient_clip\': 1,\n                \'loss\': \'softmax\',  # One of softmax, cosine, max-margin\n                \'margin\': 1,\n                \'max_epochs\': 500,\n                \'patience\': 5,\n\n                # Fraction of samples for which the query should be the function name instead of the docstring:\n                \'fraction_using_func_name\': 0.1,\n                # Only functions with a name at least this long will be candidates for training with the function name\n                # as the query instead of the docstring:\n                \'min_len_func_name_for_query\': 12,\n                # Frequency at which random, common tokens are added into the query:\n                \'query_random_token_frequency\': 0.,\n\n                # Maximal number of tokens considered to compute a representation for code/query:\n                \'code_max_num_tokens\': 200,\n                \'query_max_num_tokens\': 30,\n               }\n\n    def __init__(self,\n                 hyperparameters: Dict[str, Any],\n                 code_encoder_type: Type[Encoder],\n                 query_encoder_type: Type[Encoder],\n                 run_name: Optional[str] = None,\n                 model_save_dir: Optional[str] = None,\n                 log_save_dir: Optional[str] = None) -> None:\n        self.__code_encoder_type = code_encoder_type\n        self.__code_encoders: OrderedDict[str, Any] = OrderedDict()  # OrderedDict as we are using the order of languages a few times...\n\n        self.__query_encoder_type = query_encoder_type\n        self.__query_encoder: Any = None\n\n        # start with default hyper-params and then override them\n        self.hyperparameters = self.get_default_hyperparameters()\n        self.hyperparameters.update(hyperparameters)\n\n        self.__query_metadata: Dict[str, Any] = {}\n        self.__per_code_language_metadata: Dict[str, Any] = {}\n        self.__placeholders: Dict[str, Union[tf.placeholder, tf.placeholder_with_default]] = {}\n        self.__ops: Dict[str, Any] = {}\n        if run_name is None:\n            run_name = type(self).__name__\n        self.__run_name = run_name\n\n        if model_save_dir is None:\n            self.__model_save_dir = os.environ.get(\'PHILLY_MODEL_DIRECTORY\', default=\'.\')  # type: str\n        else:\n            self.__model_save_dir = model_save_dir  # type: str\n\n        if log_save_dir is None:\n            self.__log_save_dir = os.environ.get(\'PHILLY_LOG_DIRECTORY\', default=\'.\')  # type: str\n        else:\n            self.__log_save_dir = log_save_dir  # type: str\n\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        if ""gpu_device_id"" in self.hyperparameters:\n            config.gpu_options.visible_device_list = str(self.hyperparameters[""gpu_device_id""])\n\n        graph = tf.Graph()\n        self.__sess = tf.Session(graph=graph, config=config)\n\n        # save directory as tensorboard.\n        self.__tensorboard_dir = log_save_dir\n\n    @property\n    def query_metadata(self):\n        return self.__query_metadata\n\n    @property\n    def per_code_language_metadata(self):\n        return self.__per_code_language_metadata\n\n    @property\n    def placeholders(self):\n        return self.__placeholders\n\n    @property\n    def ops(self):\n        return self.__ops\n\n    @property\n    def sess(self):\n        return self.__sess\n\n    @property\n    def run_name(self):\n        return self.__run_name\n\n    @property\n    def representation_size(self) -> int:\n        return self.__query_encoder.output_representation_size\n\n    def _log_tensorboard_scalar(self, tag:str, value:float, step:int) -> None:\n        """"""Log scalar values that are not ops to tensorboard.""""""\n        summary = tf.Summary(value=[tf.Summary.Value(tag=tag,\n                                                     simple_value=value)])\n        self.__summary_writer.add_summary(summary, step)\n        self.__summary_writer.flush()\n\n    def save(self, path: RichPath) -> None:\n        variables_to_save = list(set(self.__sess.graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)))\n        weights_to_save = self.__sess.run(variables_to_save)\n        weights_to_save = {var.name: value\n                           for (var, value) in zip(variables_to_save, weights_to_save)}\n\n        data_to_save = {\n                         ""model_type"": type(self).__name__,\n                         ""hyperparameters"": self.hyperparameters,\n                         ""query_metadata"": self.__query_metadata,\n                         ""per_code_language_metadata"": self.__per_code_language_metadata,\n                         ""weights"": weights_to_save,\n                         ""run_name"": self.__run_name,\n                       }\n\n        path.save_as_compressed_file(data_to_save)\n\n    def train_log(self, msg) -> None:\n        log_path = os.path.join(self.__log_save_dir,\n                                f\'{self.run_name}.train_log\')\n        with open(log_path, mode=\'a\', encoding=\'utf-8\') as f:\n            f.write(msg + ""\\n"")\n        print(msg.encode(\'ascii\', errors=\'replace\').decode())\n\n    def test_log(self, msg) -> None:\n        log_path = os.path.join(self.__log_save_dir,\n                                f\'{self.run_name}.test_log\')\n        with open(log_path, mode=\'a\', encoding=\'utf-8\') as f:\n            f.write(msg + ""\\n"")\n        print(msg.encode(\'ascii\', errors=\'replace\').decode())\n\n    def make_model(self, is_train: bool):\n        with self.__sess.graph.as_default():\n            random.seed(self.hyperparameters[\'seed\'])\n            np.random.seed(self.hyperparameters[\'seed\'])\n            tf.set_random_seed(self.hyperparameters[\'seed\'])\n\n            self._make_model(is_train=is_train)\n            self._make_loss()\n            if is_train:\n                self._make_training_step()\n                self.__summary_writer = tf.summary.FileWriter(self.__tensorboard_dir, self.__sess.graph)\n\n    def _make_model(self, is_train: bool) -> None:\n        """"""\n        Create the actual model.\n\n        Note: This has to create self.ops[\'code_representations\'] and self.ops[\'query_representations\'],\n        tensors of the same shape and rank 2.\n        """"""\n        self.__placeholders[\'dropout_keep_rate\'] = tf.placeholder(tf.float32,\n                                                                  shape=(),\n                                                                  name=\'dropout_keep_rate\')\n        self.__placeholders[\'sample_loss_weights\'] = \\\n            tf.placeholder_with_default(input=np.ones(shape=[self.hyperparameters[\'batch_size\']],\n                                                      dtype=np.float32),\n                                        shape=[self.hyperparameters[\'batch_size\']],\n                                        name=\'sample_loss_weights\')\n\n        with tf.variable_scope(""code_encoder""):\n            language_encoders = []\n            for (language, language_metadata) in sorted(self.__per_code_language_metadata.items(), key=lambda kv: kv[0]):\n                with tf.variable_scope(language):\n                    self.__code_encoders[language] = self.__code_encoder_type(label=""code"",\n                                                                              hyperparameters=self.hyperparameters,\n                                                                              metadata=language_metadata)\n                    language_encoders.append(self.__code_encoders[language].make_model(is_train=is_train))\n            self.ops[\'code_representations\'] = tf.concat(language_encoders, axis=0)\n        with tf.variable_scope(""query_encoder""):\n            self.__query_encoder = self.__query_encoder_type(label=""query"",\n                                                             hyperparameters=self.hyperparameters,\n                                                             metadata=self.__query_metadata)\n            self.ops[\'query_representations\'] = self.__query_encoder.make_model(is_train=is_train)\n\n        code_representation_size = next(iter(self.__code_encoders.values())).output_representation_size\n        query_representation_size = self.__query_encoder.output_representation_size\n        assert code_representation_size == query_representation_size, \\\n            f\'Representations produced for code ({code_representation_size}) and query ({query_representation_size}) cannot differ!\'\n\n    def get_code_token_embeddings(self, language: str) -> Tuple[tf.Tensor, List[str]]:\n        with self.__sess.graph.as_default():\n            with tf.variable_scope(""code_encoder""):\n                return self.__code_encoders[language].get_token_embeddings()\n\n    def get_query_token_embeddings(self) -> Tuple[tf.Tensor, List[str]]:\n        with self.__sess.graph.as_default():\n            with tf.variable_scope(""query_encoder""):\n                return self.__query_encoder.get_token_embeddings()\n\n    def _make_loss(self) -> None:\n        if self.hyperparameters[\'loss\'] == \'softmax\':\n            logits = tf.matmul(self.ops[\'query_representations\'],\n                               self.ops[\'code_representations\'],\n                               transpose_a=False,\n                               transpose_b=True,\n                               name=\'code_query_cooccurrence_logits\',\n                               )  # B x B\n\n            similarity_scores = logits\n\n            per_sample_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n                labels=tf.range(tf.shape(self.ops[\'code_representations\'])[0]),  # [0, 1, 2, 3, ..., n]\n                logits=logits\n            )\n        elif self.hyperparameters[\'loss\'] == \'cosine\':\n            query_norms = tf.norm(self.ops[\'query_representations\'], axis=-1, keep_dims=True) + 1e-10\n            code_norms = tf.norm(self.ops[\'code_representations\'], axis=-1, keep_dims=True) + 1e-10\n            cosine_similarities = tf.matmul(self.ops[\'query_representations\'] / query_norms,\n                                            self.ops[\'code_representations\'] / code_norms,\n                                            transpose_a=False,\n                                            transpose_b=True,\n                                            name=\'code_query_cooccurrence_logits\',\n                                            )  # B x B\n            similarity_scores = cosine_similarities\n\n            # A max-margin-like loss, but do not penalize negative cosine similarities.\n            neg_matrix = tf.diag(tf.fill(dims=[tf.shape(cosine_similarities)[0]], value=float(\'-inf\')))\n            per_sample_loss = tf.maximum(0., self.hyperparameters[\'margin\']\n                                             - tf.diag_part(cosine_similarities)\n                                             + tf.reduce_max(tf.nn.relu(cosine_similarities + neg_matrix),\n                                                             axis=-1))\n        elif self.hyperparameters[\'loss\'] == \'max-margin\':\n            logits = tf.matmul(self.ops[\'query_representations\'],\n                               self.ops[\'code_representations\'],\n                               transpose_a=False,\n                               transpose_b=True,\n                               name=\'code_query_cooccurrence_logits\',\n                               )  # B x B\n            similarity_scores = logits\n            logprobs = tf.nn.log_softmax(logits)\n\n            min_inf_matrix = tf.diag(tf.fill(dims=[tf.shape(logprobs)[0]], value=float(\'-inf\')))\n            per_sample_loss = tf.maximum(0., self.hyperparameters[\'margin\']\n                                             - tf.diag_part(logprobs)\n                                             + tf.reduce_max(logprobs + min_inf_matrix, axis=-1))\n        elif self.hyperparameters[\'loss\'] == \'triplet\':\n            query_reps = self.ops[\'query_representations\']  # BxD\n            code_reps = self.ops[\'code_representations\']    # BxD\n\n            query_reps = tf.broadcast_to(query_reps, shape=[tf.shape(query_reps)[0], tf.shape(query_reps)[0],tf.shape(query_reps)[1]])  # B*xBxD\n            code_reps = tf.broadcast_to(code_reps, shape=[tf.shape(code_reps)[0], tf.shape(code_reps)[0],tf.shape(code_reps)[1]])  # B*xBxD\n            code_reps = tf.transpose(code_reps, perm=(1, 0, 2))  # BxB*xD\n\n            all_pair_distances = tf.norm(query_reps - code_reps, axis=-1)  # BxB\n            similarity_scores = -all_pair_distances\n\n            correct_distances = tf.expand_dims(tf.diag_part(all_pair_distances), axis=-1)  # Bx1\n\n            pointwise_loss = tf.nn.relu(correct_distances - all_pair_distances + self.hyperparameters[\'margin\']) # BxB\n            pointwise_loss *= (1 - tf.eye(tf.shape(pointwise_loss)[0]))\n\n            per_sample_loss = tf.reduce_sum(pointwise_loss, axis=-1) / (tf.reduce_sum(tf.cast(tf.greater(pointwise_loss, 0), dtype=tf.float32), axis=-1) + 1e-10)  # B\n        else:\n            raise Exception(f\'Unrecognized loss-type ""{self.hyperparameters[""loss""]}""\')\n\n        per_sample_loss = per_sample_loss * self.placeholders[\'sample_loss_weights\']\n        self.ops[\'loss\'] = tf.reduce_sum(per_sample_loss) / tf.reduce_sum(self.placeholders[\'sample_loss_weights\'])\n\n        # extract the logits from the diagonal of the matrix, which are the logits corresponding to the ground-truth\n        correct_scores = tf.diag_part(similarity_scores)\n        # compute how many queries have bigger logits than the ground truth (the diagonal) -> which will be incorrectly ranked\n        compared_scores = similarity_scores >= tf.expand_dims(correct_scores, axis=-1)\n        # for each row of the matrix (query), sum how many logits are larger than the ground truth\n        # ...then take the reciprocal of that to get the MRR for each individual query (you will need to take the mean later)\n        self.ops[\'mrr\'] = 1 / tf.reduce_sum(tf.to_float(compared_scores), axis=1)\n\n    def _make_training_step(self) -> None:\n        """"""\n        Constructs self.ops[\'train_step\'] from self.ops[\'loss\'] and hyperparameters.\n        """"""\n        optimizer_name = self.hyperparameters[\'optimizer\'].lower()\n        if optimizer_name == \'sgd\':\n            optimizer = tf.train.GradientDescentOptimizer(learning_rate=self.hyperparameters[\'learning_rate\'])\n        elif optimizer_name == \'rmsprop\':\n            optimizer = tf.train.RMSPropOptimizer(learning_rate=self.hyperparameters[\'learning_rate\'],\n                                                  decay=self.hyperparameters[\'learning_rate_decay\'],\n                                                  momentum=self.hyperparameters[\'momentum\'])\n        elif optimizer_name == \'adam\':\n            optimizer = tf.train.AdamOptimizer(learning_rate=self.hyperparameters[\'learning_rate\'])\n        else:\n            raise Exception(\'Unknown optimizer ""%s"".\' % (self.hyperparameters[\'optimizer\']))\n\n        # Calculate and clip gradients\n        trainable_vars = self.sess.graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n        gradients = tf.gradients(self.ops[\'loss\'], trainable_vars)\n        clipped_gradients, _ = tf.clip_by_global_norm(gradients, self.hyperparameters[\'gradient_clip\'])\n        pruned_clipped_gradients = []\n        for (gradient, trainable_var) in zip(clipped_gradients, trainable_vars):\n            if gradient is None:\n                continue\n            if trainable_var.name.startswith(""code_encoder/""):\n                gradient *= tf.constant(self.hyperparameters[\'learning_rate_code_scale_factor\'],\n                                        dtype=tf.float32)\n            elif trainable_var.name.startswith(""query_encoder/""):\n                gradient *= tf.constant(self.hyperparameters[\'learning_rate_query_scale_factor\'],\n                                        dtype=tf.float32)\n\n            pruned_clipped_gradients.append((gradient, trainable_var))\n        self.ops[\'train_step\'] = optimizer.apply_gradients(pruned_clipped_gradients)\n\n    def load_metadata(self, data_dirs: List[RichPath], max_files_per_dir: Optional[int] = None, parallelize: bool = True) -> None:\n        raw_query_metadata_list = []\n        raw_code_language_metadata_lists: DefaultDict[str, List] = defaultdict(list)\n\n        def metadata_parser_fn(_, file_path: RichPath) -> Iterable[Tuple[Dict[str, Any], Dict[str, Dict[str, Any]]]]:\n            raw_query_metadata = self.__query_encoder_type.init_metadata()\n            per_code_language_metadata: DefaultDict[str, Dict[str, Any]] = defaultdict(self.__code_encoder_type.init_metadata)\n\n            for raw_sample in file_path.read_by_file_suffix():\n                sample_language = raw_sample[\'language\']\n                self.__code_encoder_type.load_metadata_from_sample(raw_sample[\'code_tokens\'],\n                                                                   per_code_language_metadata[sample_language],\n                                                                   self.hyperparameters[\'code_use_subtokens\'],\n                                                                   self.hyperparameters[\'code_mark_subtoken_end\'])\n                self.__query_encoder_type.load_metadata_from_sample([d.lower() for d in raw_sample[\'docstring_tokens\']],\n                                                                    raw_query_metadata)\n            yield (raw_query_metadata, per_code_language_metadata)\n\n        def received_result_callback(metadata_parser_result: Tuple[Dict[str, Any], Dict[str, Dict[str, Any]]]):\n            (raw_query_metadata, per_code_language_metadata) = metadata_parser_result\n            raw_query_metadata_list.append(raw_query_metadata)\n            for (metadata_language, raw_code_language_metadata) in per_code_language_metadata.items():\n                raw_code_language_metadata_lists[metadata_language].append(raw_code_language_metadata)\n\n        def finished_callback():\n            pass\n\n        if parallelize:\n            run_jobs_in_parallel(get_data_files_from_directory(data_dirs, max_files_per_dir),\n                                 metadata_parser_fn,\n                                 received_result_callback,\n                                 finished_callback)\n        else:\n            for (idx, file) in enumerate(get_data_files_from_directory(data_dirs, max_files_per_dir)):\n                for res in metadata_parser_fn(idx, file):\n                    received_result_callback(res)\n\n        self.__query_metadata = self.__query_encoder_type.finalise_metadata(""query"", self.hyperparameters, raw_query_metadata_list)\n        for (language, raw_per_language_metadata) in raw_code_language_metadata_lists.items():\n            self.__per_code_language_metadata[language] = \\\n                self.__code_encoder_type.finalise_metadata(""code"", self.hyperparameters, raw_per_language_metadata)\n\n    def load_existing_metadata(self, metadata_path: RichPath):\n        saved_data = metadata_path.read_by_file_suffix()\n\n        hyper_names = set(self.hyperparameters.keys())\n        hyper_names.update(saved_data[\'hyperparameters\'].keys())\n        for hyper_name in hyper_names:\n            old_hyper_value = saved_data[\'hyperparameters\'].get(hyper_name)\n            new_hyper_value = self.hyperparameters.get(hyper_name)\n            if old_hyper_value != new_hyper_value:\n                self.train_log(""I: Hyperparameter %s now has value \'%s\' but was \'%s\' when tensorising data.""\n                               % (hyper_name, new_hyper_value, old_hyper_value))\n\n        self.__query_metadata = saved_data[\'query_metadata\']\n        self.__per_code_language_metadata = saved_data[\'per_code_language_metadata\']\n\n    def load_data_from_dirs(self, data_dirs: List[RichPath], is_test: bool,\n                            max_files_per_dir: Optional[int] = None,\n                            return_num_original_samples: bool = False, \n                            parallelize: bool = True) -> Union[LoadedSamples, Tuple[LoadedSamples, int]]:\n        return self.load_data_from_files(data_files=list(get_data_files_from_directory(data_dirs, max_files_per_dir)),\n                                         is_test=is_test,\n                                         return_num_original_samples=return_num_original_samples,\n                                         parallelize=parallelize)\n\n    def load_data_from_files(self, data_files: Iterable[RichPath], is_test: bool,\n                             return_num_original_samples: bool = False, parallelize: bool = True) -> Union[LoadedSamples, Tuple[LoadedSamples, int]]:\n        tasks_as_args = [(self.hyperparameters,\n                          self.__code_encoder_type,\n                          self.__per_code_language_metadata,\n                          self.__query_encoder_type,\n                          self.__query_metadata,\n                          is_test,\n                          data_file)\n                         for data_file in data_files]\n\n        if parallelize:\n            with multiprocessing.Pool() as pool:\n                per_file_results = pool.starmap(parse_data_file, tasks_as_args)\n        else:\n            per_file_results = [parse_data_file(*task_args) for task_args in tasks_as_args]\n        samples: DefaultDict[str, List] = defaultdict(list)\n        num_all_samples = 0\n        for per_file_result in per_file_results:\n            for (language, parsed_samples) in per_file_result.items():\n                for (use_example, parsed_sample) in parsed_samples:\n                    num_all_samples += 1\n                    if use_example:\n                        samples[language].append(parsed_sample)\n        if return_num_original_samples:\n            return samples, num_all_samples\n        return samples\n\n    def __init_minibatch(self) -> Dict[str, Any]:\n        """"""\n        Returns:\n            An empty data holder for minibatch construction.\n        """"""\n        batch_data: Dict[str, Any] = dict()\n        batch_data[\'samples_in_batch\'] = 0\n        batch_data[\'batch_finished\'] = False\n        # This bit is a bit tricky. To keep the alignment between code and query bits, we need\n        # to keep everything separate here (including the query data). When we finalise a minibatch,\n        # we will join (concatenate) all the query info and send it to the query encoder. The\n        # language-specific bits get sent to the language-specific encoders, but are ordered such\n        # that concatenating the results of the code encoders gives us something that is aligned\n        # with the query encoder output.\n        batch_data[\'per_language_query_data\'] = {}\n        batch_data[\'per_language_code_data\'] = {}\n        for (language, language_encoder) in self.__code_encoders.items():\n            batch_data[\'per_language_query_data\'][language] = {}\n            batch_data[\'per_language_query_data\'][language][\'query_sample_ids\'] = []\n            self.__query_encoder.init_minibatch(batch_data[\'per_language_query_data\'][language])\n            batch_data[\'per_language_code_data\'][language] = {}\n            batch_data[\'per_language_code_data\'][language][\'code_sample_ids\'] = []\n            language_encoder.init_minibatch(batch_data[\'per_language_code_data\'][language])\n        return batch_data\n\n    def __extend_minibatch_by_sample(self,\n                                     batch_data: Dict[str, Any],\n                                     sample: Dict[str, Any],\n                                     language: str,\n                                     sample_id: SampleId,\n                                     include_query: bool = True,\n                                     include_code: bool = True,\n                                     is_train: bool = False) -> bool:\n        """"""\n        Extend a minibatch under construction by one sample.\n\n        Args:\n            batch_data: The minibatch data.\n            sample: The sample to add.\n            language: The (programming) language of the same to add.\n            sample_id: Unique identifier of the example.\n            include_code: Flag indicating if the code data needs to be added.\n            include_query: Flag indicating if the query data needs to be added.\n            is_train: Flag indicating if we are in train mode (which causes data augmentation)\n\n        Returns:\n            True iff the minibatch is full after this sample.\n        """"""\n        minibatch_is_full = False\n\n        # Train with some fraction of samples having their query set to the function name instead of the docstring, and\n        # their function name replaced with out-of-vocab in the code:\n        query_type = QueryType.DOCSTRING.value\n        if is_train and sample[f\'query_tokens_{QueryType.FUNCTION_NAME.value}\'] is not None and \\\n                random.uniform(0., 1.) < self.hyperparameters[\'fraction_using_func_name\']:\n            query_type = QueryType.FUNCTION_NAME.value\n\n        if include_query:\n            batch_data[\'per_language_query_data\'][language][\'query_sample_ids\'].append(sample_id)\n            minibatch_is_full |= self.__query_encoder.extend_minibatch_by_sample(\n                batch_data[\'per_language_query_data\'][language], sample, is_train=is_train, query_type=query_type)\n        if include_code:\n            batch_data[\'per_language_code_data\'][language][\'code_sample_ids\'].append(sample_id)\n            minibatch_is_full |= self.__code_encoders[language].extend_minibatch_by_sample(\n                batch_data[\'per_language_code_data\'][language], sample, is_train=is_train, query_type=query_type)\n        return minibatch_is_full or batch_data[\'samples_in_batch\'] >= self.hyperparameters[\'batch_size\']\n\n    def __minibatch_to_feed_dict(self,\n                                 batch_data: Dict[str, Any],\n                                 language_to_reweighting_factor: Optional[Dict[str, float]],\n                                 is_train: bool) -> Tuple[Dict[tf.Tensor, Any], List[SampleId]]:\n        """"""\n        Take a collected minibatch and turn it into something that can be fed directly to the constructed model\n\n        Args:\n            batch_data: The minibatch data (initialised by __init_minibatch and repeatedly filled by __extend_minibatch_by_sample)\n            language_to_reweighting_factor: Optional map from language to the language-specific weighting factor. If not present,\n              no reweighting will be performed.\n            is_train: Flag indicating if we are in train mode (to set dropout properly)\n\n        Returns:\n            A pair of a map from model placeholders to appropriate data structures and a list of sample ids\n            such that id_list[i] = id means that the i-th minibatch entry corresponds to the sample identified by id.\n        """"""\n        final_minibatch = {self.__placeholders[\'dropout_keep_rate\']: self.hyperparameters[\'dropout_keep_rate\'] if is_train else 1.0}\n\n        # Finalise the code representations while joining the query information:\n        full_query_batch_data: Dict[str, Any] = {\'code_sample_ids\': []}\n        language_weights = []\n        for (language, language_encoder) in self.__code_encoders.items():\n            language_encoder.minibatch_to_feed_dict(batch_data[\'per_language_code_data\'][language], final_minibatch, is_train)\n            full_query_batch_data[\'code_sample_ids\'].extend(batch_data[\'per_language_code_data\'][language][\'code_sample_ids\'])\n\n            for (key, value) in batch_data[\'per_language_query_data\'][language].items():\n                if key in full_query_batch_data:\n                    if isinstance(value, list):\n                        full_query_batch_data[key].extend(value)\n                    elif isinstance(value, int):\n                        full_query_batch_data[key] += value\n                    else:\n                        raise ValueError()\n                else:\n                    full_query_batch_data[key] = value\n            if language_to_reweighting_factor is not None:\n                language_weights.extend([language_to_reweighting_factor[language]] * len(batch_data[\'per_language_code_data\'][language][\'tokens\']))\n\n        self.__query_encoder.minibatch_to_feed_dict(full_query_batch_data, final_minibatch, is_train)\n        if language_to_reweighting_factor is not None:\n            final_minibatch[self.__placeholders[\'sample_loss_weights\']] = language_weights\n        if len(full_query_batch_data[\'query_sample_ids\']) > 0:  # If we are only computing code representations, this will be empty\n            return final_minibatch, full_query_batch_data[\'query_sample_ids\']\n        else:\n            return final_minibatch, full_query_batch_data[\'code_sample_ids\']\n\n    def __split_data_into_minibatches(self,\n                                      data: LoadedSamples,\n                                      is_train: bool = False,\n                                      include_query: bool = True,\n                                      include_code: bool = True,\n                                      drop_incomplete_final_minibatch: bool = True,\n                                      compute_language_weightings: bool = False) \\\n            -> Iterable[Tuple[Dict[tf.Tensor, Any], Any, int, List[SampleId]]]:\n        """"""\n        Take tensorised data and chunk into feed dictionaries corresponding to minibatches.\n\n        Args:\n            data: The tensorised input data.\n            is_train: Flag indicating if we are in train mode (which causes shuffling and the use of dropout)\n            include_query: Flag indicating if query data should be included.\n            include_code: Flag indicating if code data should be included.\n            drop_incomplete_final_minibatch: If True, all returned minibatches will have the configured size\n             and some examples from data may not be considered at all. If False, the final minibatch will\n             be shorter than the configured size.\n            compute_language_weightings: If True, produces weights for samples that normalise the loss\n             contribution of each language to be 1/num_languages.\n\n        Returns:\n            Iterable sequence of 4-tuples:\n              (1) A feed dict mapping placeholders to values,\n              (2) Number of samples in the batch\n              (3) Total number of datapoints processed\n              (4) List of IDs that connect the minibatch elements to the inputs. Concretely,\n                  element id_list[i] = (lang, j) indicates that the i-th result in the batch\n                  corresponds to the sample data[lang][j].\n        """"""\n        # We remove entries from language_to_num_remaining_samples once None are remaining:\n        language_to_num_remaining_samples, language_to_idx_list = {}, {}\n        for (language, samples) in data.items():\n            num_samples = len(samples)\n            language_to_num_remaining_samples[language] = num_samples\n            sample_idx_list = np.arange(num_samples)\n            if is_train:\n                np.random.shuffle(sample_idx_list)\n            language_to_idx_list[language] = sample_idx_list\n\n        if compute_language_weightings:\n            # We want to weigh languages equally, and thus normalise the loss of their samples with\n            # total_num_samples * 1/num_languages * 1/num_samples_per_language.\n            # Then, assuming a loss of 1 per sample for simplicity, the total loss attributed to a language is\n            #    \\sum_{1 \\leq i \\leq num_samples_per_language} total_num_samples / (num_languages * num_samples_per_language)\n            #  =  num_samples_per_language * total_num_samples / (num_languages * num_samples_per_language)\n            #  =                             total_num_samples / num_languages\n            total_num_samples = sum(language_to_num_remaining_samples.values())\n            num_languages = len(language_to_num_remaining_samples)\n            language_to_reweighting_factor = {language: float(total_num_samples)/(num_languages * num_samples_per_language)\n                                              for (language, num_samples_per_language) in language_to_num_remaining_samples.items()}\n        else:\n            language_to_reweighting_factor = None  # type: ignore\n\n        total_samples_used = 0\n        batch_data = self.__init_minibatch()\n\n        while len(language_to_num_remaining_samples) > 0:\n            # Pick a language for the sample, by weighted sampling over the remaining data points:\n            remaining_languages = list(language_to_num_remaining_samples.keys())\n            total_num_remaining_samples = sum(language_to_num_remaining_samples.values())\n            picked_language = np.random.choice(a=remaining_languages,\n                                               p=[float(language_to_num_remaining_samples[lang]) / total_num_remaining_samples\n                                                  for lang in remaining_languages])\n\n            # Pick an example for the given language, and update counters:\n            picked_example_idx = language_to_num_remaining_samples[picked_language] - 1  # Note that indexing is 0-based and counting 1-based...\n            language_to_num_remaining_samples[picked_language] -= 1\n            if language_to_num_remaining_samples[picked_language] == 0:\n                del(language_to_num_remaining_samples[picked_language])  # We are done with picked_language now\n            picked_sample = data[picked_language][language_to_idx_list[picked_language][picked_example_idx]]\n\n            # Add the example to the current minibatch under preparation:\n            batch_data[\'samples_in_batch\'] += 1\n            batch_finished = self.__extend_minibatch_by_sample(batch_data,\n                                                               picked_sample,\n                                                               language=picked_language,\n                                                               sample_id=(picked_language, language_to_idx_list[picked_language][picked_example_idx]),\n                                                               include_query=include_query,\n                                                               include_code=include_code,\n                                                               is_train=is_train\n                                                               )\n            total_samples_used += 1\n\n            if batch_finished:\n                feed_dict, original_sample_ids = self.__minibatch_to_feed_dict(batch_data, language_to_reweighting_factor, is_train)\n                yield feed_dict, batch_data[\'samples_in_batch\'], total_samples_used, original_sample_ids\n                batch_data = self.__init_minibatch()\n\n        if not drop_incomplete_final_minibatch and batch_data[\'samples_in_batch\'] > 0:\n            feed_dict, original_sample_ids = self.__minibatch_to_feed_dict(batch_data, language_to_reweighting_factor, is_train)\n            yield feed_dict, batch_data[\'samples_in_batch\'], total_samples_used, original_sample_ids\n\n    def __run_epoch_in_batches(self, data: LoadedSamples, epoch_name: str, is_train: bool, quiet: bool = False) -> Tuple[float, float, float]:\n        """"""\n        Args:\n            data: Data to run on; will be broken into minibatches.\n            epoch_name: Name to use in CLI output.\n            is_train: Flag indicating if we should training ops (updating weights) as well.\n            quiet: Flag indicating that we should print only few lines (useful if not run in interactive shell)\n\n        Returns:\n            Triple of epoch loss (average over samples), MRR (average over batches), total time used for epoch (in s)\n        """"""\n        """"""Run the training ops and return the loss and the MRR.""""""\n        epoch_loss, loss = 0.0, 0.0\n        mrr_sum, mrr = 0.0, 0.0\n        epoch_start = time.time()\n        data_generator = self.__split_data_into_minibatches(data, is_train=is_train, compute_language_weightings=True)\n        samples_used_so_far = 0\n        printed_one_line = False\n        for minibatch_counter, (batch_data_dict, samples_in_batch, samples_used_so_far, _) in enumerate(data_generator):\n            if not quiet or (minibatch_counter % 100) == 99:\n                print(""%s: Batch %5i (has %i samples). Processed %i samples. Loss so far: %.4f.  MRR so far: %.4f ""\n                      % (epoch_name, minibatch_counter, samples_in_batch,\n                         samples_used_so_far - samples_in_batch, loss, mrr),\n                      flush=True,\n                      end=""\\r"" if not quiet else \'\\n\')\n                printed_one_line = True\n            ops_to_run = {\'loss\': self.__ops[\'loss\'], \'mrr\': self.__ops[\'mrr\']}\n            if is_train:\n                ops_to_run[\'train_step\'] = self.__ops[\'train_step\']\n            op_results = self.__sess.run(ops_to_run, feed_dict=batch_data_dict)\n            assert not np.isnan(op_results[\'loss\'])\n\n            epoch_loss += op_results[\'loss\'] * samples_in_batch\n            mrr_sum += np.sum(op_results[\'mrr\'])\n\n            loss = epoch_loss / max(1, samples_used_so_far)\n            mrr = mrr_sum / max(1, samples_used_so_far)\n\n            # additional training logs\n            if (minibatch_counter % 100) == 0 and is_train:\n                wandb.log({\'train-loss\': op_results[\'loss\'],\n                           \'train-mrr\': op_results[\'mrr\']})\n\n            minibatch_counter += 1\n\n        used_time = time.time() - epoch_start\n        if printed_one_line:\n            print(""\\r\\x1b[K"", end=\'\')\n        self.train_log(""  Epoch %s took %.2fs [processed %s samples/second]""\n                       % (epoch_name, used_time, int(samples_used_so_far/used_time)))\n\n        return loss, mrr, used_time\n\n    @property\n    def model_save_path(self) -> str:\n        return os.path.join(self.__model_save_dir,\n                            f\'{self.run_name}_model_best.pkl.gz\')\n\n    def train(self,\n              train_data: LoadedSamples,\n              valid_data: LoadedSamples,\n              azure_info_path: Optional[str],\n              quiet: bool = False,\n              resume: bool = False) -> RichPath:\n        model_path = RichPath.create(self.model_save_path, azure_info_path)\n        with self.__sess.as_default():\n            tf.set_random_seed(self.hyperparameters[\'seed\'])\n            train_data_per_lang_nums = {language: len(samples) for language, samples in train_data.items()}\n            print(\'Training on %s samples.\' % ("", "".join(""%i %s"" % (num, lang) for (lang, num) in train_data_per_lang_nums.items())))\n            valid_data_per_lang_nums = {language: len(samples) for language, samples in valid_data.items()}\n            print(\'Validating on %s samples.\' % ("", "".join(""%i %s"" % (num, lang) for (lang, num) in valid_data_per_lang_nums.items())))\n\n            if resume:\n                # Variables should have been restored.\n                best_val_mrr_loss, best_val_mrr, _ = self.__run_epoch_in_batches(valid_data, ""RESUME (valid)"", is_train=False, quiet=quiet)\n                self.train_log(\'Validation Loss on Resume: %.6f\' % (best_val_mrr_loss,))\n            else:\n                init_op = tf.variables_initializer(self.__sess.graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))\n                self.__sess.run(init_op)\n                self.save(model_path)\n                best_val_mrr = 0\n            no_improvement_counter = 0\n            epoch_number = 0\n            while (epoch_number < self.hyperparameters[\'max_epochs\']\n                   and no_improvement_counter < self.hyperparameters[\'patience\']):\n\n                self.train_log(\'==== Epoch %i ====\' % (epoch_number,))\n\n                # run training loop and log metrics\n                train_loss, train_mrr, train_time = self.__run_epoch_in_batches(train_data, ""%i (train)"" % (epoch_number,),\n                                                                                is_train=True,\n                                                                                quiet=quiet)\n                self.train_log(\' Training Loss: %.6f\' % (train_loss,))\n\n                # run validation calcs and log metrics\n                val_loss, val_mrr, val_time = self.__run_epoch_in_batches(valid_data, ""%i (valid)"" % (epoch_number,),\n                                                                          is_train=False,\n                                                                          quiet=quiet)\n                self.train_log(\' Validation:  Loss: %.6f | MRR: %.6f\' % (val_loss, val_mrr,))\n\n                log = {\'epoch\': epoch_number,\n                       \'train-loss\': train_loss,\n                       \'train-mrr\': train_mrr,\n                       \'train-time-sec\': train_time,\n                       \'val-loss\': val_loss,\n                       \'val-mrr\': val_mrr,\n                       \'val-time-sec\': val_time}\n                \n                # log to wandb\n                wandb.log(log)\n            \n                # log to tensorboard\n                for key in log:\n                    if key != \'epoch\':\n                        self._log_tensorboard_scalar(tag=key, \n                                                     value=log[key],\n                                                     step=epoch_number)\n\n                #  log the final epoch number\n                wandb.run.summary[\'epoch\'] = epoch_number\n\n                if val_mrr > best_val_mrr:\n                    # save the best val_mrr encountered\n                    best_val_mrr_loss, best_val_mrr = val_loss, val_mrr\n\n                    wandb.run.summary[\'best_val_mrr_loss\'] = best_val_mrr_loss\n                    wandb.run.summary[\'best_val_mrr\'] = val_mrr\n                    wandb.run.summary[\'best_epoch\'] = epoch_number\n\n                    no_improvement_counter = 0\n                    self.save(model_path)\n                    self.train_log(""  Best result so far -- saved model as \'%s\'."" % (model_path,))\n                else:\n                    # record epochs without improvement for early stopping\n                    no_improvement_counter += 1\n                epoch_number += 1\n\n            log_path = os.path.join(self.__log_save_dir,\n                                    f\'{self.run_name}.train_log\')\n            wandb.save(log_path)\n            tf.io.write_graph(self.__sess.graph,\n                              logdir=wandb.run.dir,\n                              name=f\'{self.run_name}-graph.pbtxt\')\n\n        self.__summary_writer.close()\n        return model_path\n\n    def __compute_representations_batched(self,\n                                          raw_data: List[Dict[str, Any]],\n                                          data_loader_fn: Callable[[Dict[str, Any], Dict[str, Any]], bool],\n                                          model_representation_op: tf.Tensor,\n                                          representation_type: RepresentationType) -> List[Optional[np.ndarray]]:\n        """"""Return a list of vector representation of each datapoint or None if the representation for that datapoint\n        cannot be computed.\n\n        Args:\n            raw_data: a list of raw data point as dictionanries.\n            data_loader_fn: A function f(in, out) that attempts to load/preprocess the necessary data from\n             in and store it in out, returning a boolean success value. If it returns False, the sample is\n             skipped and no representation is computed.\n            model_representation_op: An op in the computation graph that represents the desired\n             representations.\n            representation_type: type of the representation we are interested in (either code or query)\n\n        Returns:\n             A list of either a 1D numpy array of the representation of the i-th element in data or None if a\n             representation could not be computed.\n        """"""\n        tensorized_data = defaultdict(list)  # type: Dict[str, List[Dict[str, Any]]]\n        sample_to_tensorised_data_id = []  # type: List[Optional[SampleId]]\n        for raw_sample in raw_data:\n            language = raw_sample[\'language\']\n            if language.startswith(\'python\'):\n                language = \'python\'\n            sample: Dict = {}\n            valid_example = data_loader_fn(raw_sample, sample)\n            if valid_example:\n                sample_to_tensorised_data_id.append((language, len(tensorized_data[language])))\n                tensorized_data[language].append(sample)\n            else:\n                sample_to_tensorised_data_id.append(None)\n        assert len(sample_to_tensorised_data_id) == len(raw_data)\n\n        data_generator = self.__split_data_into_minibatches(tensorized_data,\n                                                            is_train=False,\n                                                            include_query=representation_type == RepresentationType.QUERY,\n                                                            include_code=representation_type == RepresentationType.CODE,\n                                                            drop_incomplete_final_minibatch=False)\n\n        computed_representations = []\n        original_tensorised_data_ids = []  # type: List[SampleId]\n        for minibatch_counter, (batch_data_dict, samples_in_batch, samples_used_so_far, batch_original_tensorised_data_ids) in enumerate(data_generator):\n            op_results = self.__sess.run(model_representation_op, feed_dict=batch_data_dict)\n            computed_representations.append(op_results)\n            original_tensorised_data_ids.extend(batch_original_tensorised_data_ids)\n\n        computed_representations = np.concatenate(computed_representations, axis=0)\n        tensorised_data_id_to_representation_idx = {tensorised_data_id: repr_idx\n                                                    for (repr_idx, tensorised_data_id) in enumerate(original_tensorised_data_ids)}\n        reordered_representations: List = []\n        for tensorised_data_id in sample_to_tensorised_data_id:\n            if tensorised_data_id is None:\n                reordered_representations.append(None)\n            else:\n                reordered_representations.append(computed_representations[tensorised_data_id_to_representation_idx[tensorised_data_id]])\n        return reordered_representations\n\n    def get_query_representations(self, query_data: List[Dict[str, Any]]) -> List[Optional[np.ndarray]]:\n        def query_data_loader(sample_to_parse, result_holder):\n            function_name = sample_to_parse.get(\'func_name\')\n            return self.__query_encoder_type.load_data_from_sample(\n                ""query"",\n                self.hyperparameters,\n                self.__query_metadata,\n                [d.lower() for d in sample_to_parse[\'docstring_tokens\']],\n                function_name,\n                result_holder=result_holder,\n                is_test=True)\n\n        return self.__compute_representations_batched(query_data,\n                                                      data_loader_fn=query_data_loader,\n                                                      model_representation_op=self.__ops[\'query_representations\'],\n                                                      representation_type=RepresentationType.QUERY)\n\n    def get_code_representations(self, code_data: List[Dict[str, Any]]) -> List[Optional[np.ndarray]]:\n        def code_data_loader(sample_to_parse, result_holder):\n            code_tokens = sample_to_parse[\'code_tokens\']\n            language = sample_to_parse[\'language\']\n            if language.startswith(\'python\'):\n                language = \'python\'\n\n            if code_tokens is not None:\n                function_name = sample_to_parse.get(\'func_name\')\n                return self.__code_encoder_type.load_data_from_sample(\n                    ""code"",\n                    self.hyperparameters,\n                    self.__per_code_language_metadata[language],\n                    code_tokens,\n                    function_name,\n                    result_holder=result_holder,\n                    is_test=True)\n            else:\n                return False\n\n        return self.__compute_representations_batched(code_data,\n                                                      data_loader_fn=code_data_loader,\n                                                      model_representation_op=self.__ops[\'code_representations\'],\n                                                      representation_type=RepresentationType.CODE)\n'"
src/models/nbow_model.py,0,"b'from typing import Any, Dict, Optional\n\nfrom encoders import NBoWEncoder\nfrom .model import Model\n\n\nclass NeuralBoWModel(Model):\n    @classmethod\n    def get_default_hyperparameters(cls) -> Dict[str, Any]:\n        hypers = {}\n        for label in [""code"", ""query""]:\n            hypers.update({f\'{label}_{key}\': value\n                           for key, value in NBoWEncoder.get_default_hyperparameters().items()})\n        model_hypers = {\n            \'code_use_subtokens\': False,\n            \'code_mark_subtoken_end\': False,\n            \'loss\': \'cosine\',\n            \'batch_size\': 1000\n        }\n        hypers.update(super().get_default_hyperparameters())\n        hypers.update(model_hypers)\n        return hypers\n\n    def __init__(self,\n                 hyperparameters: Dict[str, Any],\n                 run_name: str = None,\n                 model_save_dir: Optional[str] = None,\n                 log_save_dir: Optional[str] = None):\n        super().__init__(\n            hyperparameters,\n            code_encoder_type=NBoWEncoder,\n            query_encoder_type=NBoWEncoder,\n            run_name=run_name,\n            model_save_dir=model_save_dir,\n            log_save_dir=log_save_dir)\n'"
src/models/rnn_model.py,0,"b'from typing import Any, Dict, Optional\n\nfrom encoders import RNNEncoder\nfrom models import Model\n\n\nclass RNNModel(Model):\n    @classmethod\n    def get_default_hyperparameters(cls) -> Dict[str, Any]:\n        hypers = {}\n        for label in [""code"", ""query""]:\n            hypers.update({f\'{label}_{key}\': value\n                           for key, value in RNNEncoder.get_default_hyperparameters().items()})\n        model_hypers = {\n            \'code_use_subtokens\': False,\n            \'code_mark_subtoken_end\': True,\n            \'batch_size\': 1000\n        }\n        hypers.update(super().get_default_hyperparameters())\n        hypers.update(model_hypers)\n        return hypers\n\n    def __init__(self,\n                 hyperparameters: Dict[str, Any],\n                 run_name: str = None,\n                 model_save_dir: Optional[str] = None,\n                 log_save_dir: Optional[str] = None):\n        super().__init__(\n            hyperparameters,\n            code_encoder_type=RNNEncoder,\n            query_encoder_type=RNNEncoder,\n            run_name=run_name,\n            model_save_dir=model_save_dir,\n            log_save_dir=log_save_dir)\n'"
src/models/self_att_model.py,0,"b'from typing import Any, Dict, Optional\n\nfrom encoders import SelfAttentionEncoder\nfrom models import Model\n\n\nclass SelfAttentionModel(Model):\n    @classmethod\n    def get_default_hyperparameters(cls) -> Dict[str, Any]:\n        hypers = {}\n        for label in [""code"", ""query""]:\n            hypers.update({f\'{label}_{key}\': value\n                           for key, value in SelfAttentionEncoder.get_default_hyperparameters().items()})\n        model_hypers = {\n            \'learning_rate\': 5e-4,\n            \'code_use_subtokens\': False,\n            \'code_mark_subtoken_end\': False,\n            \'batch_size\': 450,\n        }\n        hypers.update(super().get_default_hyperparameters())\n        hypers.update(model_hypers)\n        return hypers\n\n    def __init__(self,\n                 hyperparameters: Dict[str, Any],\n                 run_name: str = None,\n                 model_save_dir: Optional[str] = None,\n                 log_save_dir: Optional[str] = None):\n        super().__init__(\n            hyperparameters,\n            code_encoder_type=SelfAttentionEncoder,\n            query_encoder_type=SelfAttentionEncoder,\n            run_name=run_name,\n            model_save_dir=model_save_dir,\n            log_save_dir=log_save_dir)\n'"
src/utils/__init__.py,0,b''
src/utils/bpevocabulary.py,0,"b'## Code adapted from https://github.com/soaxelbrooke/python-bpe/blob/master/bpe/encoder.py\r\n## MIT License (see repository)\r\n\r\n\r\n"""""" An encoder which learns byte pair encodings for white-space separated text.  Can tokenize, encode, and decode. """"""\r\nimport typing\r\nfrom typing import Optional\r\nfrom collections import Counter\r\n\r\nimport toolz\r\n\r\ntry:\r\n    from typing import Dict, Iterable, Callable, List, Any, Iterator\r\nexcept ImportError:\r\n    pass\r\n\r\n\r\nDEFAULT_EOW = \'__eow\'\r\nDEFAULT_SOW = \'__sow\'\r\nDEFAULT_UNK = \'__unk\'\r\nDEFAULT_PAD = \'__pad\'\r\n\r\n\r\nclass BpeVocabulary(typing.Sized):\r\n    """"""\r\n    Encodes white-space separated text using byte-pair encoding.  See https://arxiv.org/abs/1508.07909 for details.\r\n    """"""\r\n\r\n    def __init__(self, vocab_size: int=8192, pct_bpe: float=0.2,\r\n                 ngram_min: int=2, ngram_max: int=8, required_tokens: Optional[Iterable[str]]=None, strict=True,\r\n                 EOW=DEFAULT_EOW, SOW=DEFAULT_SOW, UNK=DEFAULT_UNK, PAD=DEFAULT_PAD):\r\n        if vocab_size < 1:\r\n            raise ValueError(\'vocab size must be greater than 0.\')\r\n\r\n        self.EOW = EOW\r\n        self.SOW = SOW\r\n        self.eow_len = len(EOW)\r\n        self.sow_len = len(SOW)\r\n        self.UNK = UNK\r\n        self.PAD = PAD\r\n        self.required_tokens = list(set(required_tokens or []).union({self.UNK, self.PAD}))\r\n        self.vocab_size = vocab_size\r\n        self.pct_bpe = pct_bpe\r\n        self.word_vocab_size = max([int(vocab_size * (1 - pct_bpe)), len(self.required_tokens or [])])\r\n        self.bpe_vocab_size = vocab_size - self.word_vocab_size\r\n        self.word_vocab = {}  # type: Dict[str, int]\r\n        self.bpe_vocab = {}  # type: Dict[str, int]\r\n        self.inverse_word_vocab = {}  # type: Dict[int, str]\r\n        self.inverse_bpe_vocab = {}  # type: Dict[int, str]\r\n        self.ngram_min = ngram_min\r\n        self.ngram_max = ngram_max\r\n        self.strict = strict\r\n\r\n    def __len__(self):\r\n        return self.vocab_size\r\n\r\n    def byte_pair_counts(self, words: Iterable[str]) -> Iterable[typing.Counter]:\r\n        """""" Counts space separated token character pairs:\r\n            [(\'T h i s </w>\', 4}] -> {\'Th\': 4, \'hi\': 4, \'is\': 4}\r\n        """"""\r\n        for token, count in self.count_tokens(words).items():\r\n            bp_counts = Counter()  # type: Counter\r\n            sub_tokens = token.split(\' \')\r\n            joined_tokens = \'\'.join(sub_tokens)\r\n            token_offsets = [0]\r\n            length = 0\r\n            for ngram in sub_tokens:\r\n                bp_counts[ngram] += count\r\n                length += len(ngram)\r\n                token_offsets += [length]\r\n            for ngram_size in range(self.ngram_min, min(self.ngram_max, len(sub_tokens)) + 1):\r\n                for i in range(len(sub_tokens) - ngram_size + 1):\r\n                    bp_counts[joined_tokens[token_offsets[i]:token_offsets[i+ngram_size]]] += count\r\n\r\n            yield bp_counts\r\n\r\n    def count_tokens(self, words: Iterable[str]) -> Dict[str, int]:\r\n        """""" Count tokens into a BPE vocab """"""\r\n        token_counts = Counter(words)\r\n        return {\' \'.join(token): count for token, count in token_counts.items()}\r\n\r\n    def learn_word_vocab(self, word_counts: typing.Counter[str]) -> Dict[str, int]:\r\n        """""" Build vocab from self.word_vocab_size most common tokens in provided sentences """"""\r\n        for token in set(self.required_tokens or []):\r\n            word_counts[token] = int(2**31)\r\n        word_counts[self.PAD] = int(2**32)  # Make sure that PAD gets id=0\r\n        sorted_word_counts = sorted(word_counts.items(), key=lambda p: -p[1])\r\n        return {word: idx for idx, (word, count) in enumerate(sorted_word_counts[:self.word_vocab_size])}\r\n\r\n    def learn_bpe_vocab(self, words: Iterable[str]) -> Dict[str, int]:\r\n        """""" Learns a vocab of byte pair encodings """"""\r\n        vocab = Counter()  # type: typing.Counter\r\n        for token in {self.SOW, self.EOW}:\r\n            vocab[token] = int(2**63)\r\n        for idx, byte_pair_count in enumerate(self.byte_pair_counts(words)):\r\n            vocab.update(byte_pair_count)\r\n            if (idx + 1) % 10000 == 0:\r\n                self.trim_vocab(10 * self.bpe_vocab_size, vocab)\r\n\r\n        sorted_bpe_counts = sorted(vocab.items(), key=lambda p: -p[1])[:self.bpe_vocab_size]\r\n        return {bp: idx + self.word_vocab_size for idx, (bp, count) in enumerate(sorted_bpe_counts)}\r\n\r\n    def fit(self, word_counts: typing.Counter[str]) -> None:\r\n        """""" Learn vocab from text. """"""\r\n\r\n        # First, learn word vocab\r\n        self.word_vocab = self.learn_word_vocab(word_counts)\r\n\r\n        remaining_words = Counter({word: count for word, count in word_counts.items()\r\n                           if word not in self.word_vocab})\r\n        self.bpe_vocab = self.learn_bpe_vocab(remaining_words.elements())\r\n\r\n        self.inverse_word_vocab = {idx: token for token, idx in self.word_vocab.items()}\r\n        self.inverse_bpe_vocab = {idx: token for token, idx in self.bpe_vocab.items()}\r\n\r\n    @staticmethod\r\n    def trim_vocab(n: int, vocab: Dict[str, int]) -> None:\r\n        """"""  Deletes all pairs below 10 * vocab size to prevent memory problems """"""\r\n        pair_counts = sorted(vocab.items(), key=lambda p: -p[1])\r\n        pairs_to_trim = [pair for pair, count in pair_counts[n:]]\r\n        for pair in pairs_to_trim:\r\n            del vocab[pair]\r\n\r\n    def subword_tokenize(self, word: str) -> List[str]:\r\n        """""" Tokenizes inside an unknown token using BPE """"""\r\n        end_idx = min([len(word), self.ngram_max])\r\n        sw_tokens = [self.SOW]\r\n        start_idx = 0\r\n\r\n        while start_idx < len(word):\r\n            subword = word[start_idx:end_idx]\r\n            if subword in self.bpe_vocab:\r\n                sw_tokens.append(subword)\r\n                start_idx = end_idx\r\n                end_idx = min([len(word), start_idx + self.ngram_max])\r\n            elif len(subword) == 1:\r\n                sw_tokens.append(self.UNK)\r\n                start_idx = end_idx\r\n                end_idx = min([len(word), start_idx + self.ngram_max])\r\n            else:\r\n                end_idx -= 1\r\n\r\n        sw_tokens.append(self.EOW)\r\n        return sw_tokens\r\n\r\n    def tokenize(self, word_tokens: List[str]) -> List[str]:\r\n        """""" Split a sentence into word and subword tokens """"""\r\n\r\n        tokens = []\r\n        for word_token in word_tokens:\r\n            if word_token in self.word_vocab:\r\n                tokens.append(word_token)\r\n            else:\r\n                tokens.extend(self.subword_tokenize(word_token))\r\n\r\n        return tokens\r\n\r\n    def transform(self, sentences: Iterable[List[str]], reverse=False, fixed_length=None)-> Iterable[List[str]]:\r\n        """""" Turns tokens into vocab idxs """"""\r\n        direction = -1 if reverse else 1\r\n        for sentence in sentences:\r\n            encoded = []\r\n            tokens = list(self.tokenize(sentence))\r\n            for token in tokens:\r\n                if token in self.word_vocab:\r\n                    encoded.append(self.word_vocab[token])\r\n                elif token in self.bpe_vocab:\r\n                    encoded.append(self.bpe_vocab[token])\r\n                else:\r\n                    encoded.append(self.word_vocab[self.UNK])\r\n\r\n            if fixed_length is not None:\r\n                encoded = encoded[:fixed_length]\r\n                while len(encoded) < fixed_length:\r\n                    encoded.append(self.word_vocab[self.PAD])\r\n\r\n            yield encoded[::direction]\r\n\r\n    def inverse_transform(self, rows: Iterable[List[int]]) -> Iterator[str]:\r\n        """""" Turns token indexes back into space-joined text. """"""\r\n        for row in rows:\r\n            words = []\r\n\r\n            rebuilding_word = False\r\n            current_word = \'\'\r\n            for idx in row:\r\n                if self.inverse_bpe_vocab.get(idx) == self.SOW:\r\n                    if rebuilding_word and self.strict:\r\n                        raise ValueError(\'Encountered second SOW token before EOW.\')\r\n                    rebuilding_word = True\r\n\r\n                elif self.inverse_bpe_vocab.get(idx) == self.EOW:\r\n                    if not rebuilding_word and self.strict:\r\n                        raise ValueError(\'Encountered EOW without matching SOW.\')\r\n                    rebuilding_word = False\r\n                    words.append(current_word)\r\n                    current_word = \'\'\r\n\r\n                elif rebuilding_word and (idx in self.inverse_bpe_vocab):\r\n                    current_word += self.inverse_bpe_vocab[idx]\r\n\r\n                elif rebuilding_word and (idx in self.inverse_word_vocab):\r\n                    current_word += self.inverse_word_vocab[idx]\r\n\r\n                elif idx in self.inverse_word_vocab:\r\n                    words.append(self.inverse_word_vocab[idx])\r\n\r\n                elif idx in self.inverse_bpe_vocab:\r\n                    if self.strict:\r\n                        raise ValueError(""Found BPE index {} when not rebuilding word!"".format(idx))\r\n                    else:\r\n                        words.append(self.inverse_bpe_vocab[idx])\r\n\r\n                else:\r\n                    raise ValueError(""Got index {} that was not in word or BPE vocabs!"".format(idx))\r\n\r\n            yield \' \'.join(w for w in words if w != \'\')\r\n'"
src/utils/embeddingvis.py,0,"b'#!/usr/bin/env python\r\n""""""\r\nUsage:\r\n    embeddingvis.py [options] plot-tsne (--code | --query) MODEL_PATH\r\n    embeddingvis.py [options] print-nns (--code | --query) MODEL_PATH DISTANCE_THRESHOLD\r\n\r\nOptions:\r\n    --azure-info=<path>        Azure authentication information file (JSON). Used to load data from Azure storage.\r\n    --distance-metric METRIC   The distance metric to use [default: cosine]\r\n    --num-nns NUM              The number of nearest neighbors to show when print-nns. [default: 2]\r\n    --lim-items NUM            Maximum number of items to use. Useful when memory is limited. [default: -1]\r\n    -h --help                  Show this screen.\r\n    --hypers-override HYPERS   JSON dictionary overriding hyperparameter values.\r\n    --language LANG            The code language to use. Only when --code option is given. [default: python]\r\n    --debug                    Enable debug routines. [default: False]\r\n""""""\r\nfrom docopt import docopt\r\nfrom dpu_utils.utils import RichPath, run_and_debug\r\nfrom sklearn.manifold import TSNE\r\nimport numpy as np\r\nfrom scipy.spatial.distance import pdist\r\nimport matplotlib.pyplot as plt\r\n\r\n\r\nimport model_restore_helper\r\nfrom utils.visutils import square_to_condensed\r\n\r\n\r\ndef run(arguments) -> None:\r\n    azure_info_path = arguments.get(\'--azure-info\', None)\r\n\r\n    model_path = RichPath.create(arguments[\'MODEL_PATH\'], azure_info_path=azure_info_path)\r\n\r\n    model = model_restore_helper.restore(\r\n        path=model_path,\r\n        is_train=False)\r\n\r\n    if arguments[\'--query\']:\r\n        embeddings, elements = model.get_query_token_embeddings()\r\n    else:\r\n        embeddings, elements = model.get_code_token_embeddings(arguments[\'--language\'])\r\n\r\n    max_num_elements = int(arguments[\'--lim-items\'])\r\n    if max_num_elements > 0:\r\n        embeddings, elements = embeddings[:max_num_elements], elements[:max_num_elements]\r\n\r\n    print(f\'Collected {len(elements)} elements to visualize.\')\r\n\r\n    embeddings = model.sess.run(fetches=embeddings)\r\n\r\n    if arguments[\'plot-tsne\']:\r\n        emb_2d = TSNE(n_components=2, verbose=1, metric=arguments[\'--distance-metric\']).fit_transform(embeddings)\r\n\r\n        plt.scatter(emb_2d[:, 0], emb_2d[:, 1])\r\n        for i in range(len(elements)):\r\n            plt.annotate(elements[i], xy=(emb_2d[i,0], emb_2d[i,1]))\r\n\r\n        plt.show()\r\n    elif arguments[\'print-nns\']:\r\n        flat_distances = pdist(embeddings, arguments[\'--distance-metric\'])\r\n        num_nns = int(arguments[\'--num-nns\'])\r\n\r\n        for i, element in enumerate(elements):\r\n            distance_from_i = np.fromiter(\r\n                (flat_distances[square_to_condensed(i, j, len(elements))] if i != j else float(\'inf\') for j in\r\n                 range(len(elements))), dtype=np.float)\r\n\r\n            nns = [int(k) for k in np.argsort(distance_from_i)[:num_nns]]  # The first two NNs\r\n\r\n            if distance_from_i[nns[0]] > float(arguments[\'DISTANCE_THRESHOLD\']):\r\n                continue\r\n            try:\r\n                print(f\'{element} --> \' + \', \'.join(f\'{elements[n]} ({distance_from_i[n]:.2f})\' for n in nns))\r\n            except:\r\n                print(\'Error printing token for nearest neighbors pair.\')\r\n\r\n\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    args = docopt(__doc__)\r\n    run_and_debug(lambda: run(args), args.get(\'--debug\', False))'"
src/utils/general_utils.py,0,"b'from typing import List, Any\nimport pickle\nimport pandas as pd\n\n\ndef save_file_pickle(fname: str, obj: Any) -> None:\n    with open(fname, \'wb\') as f:\n        pickle.dump(obj, f)\n\n\ndef load_file_pickle(fname: str) -> None:\n    with open(fname, \'rb\') as f:\n        obj = pickle.load(f)\n        return obj\n\n\ndef chunkify(df: pd.DataFrame, n: int) -> List[pd.DataFrame]:\n    ""turn pandas.dataframe into equal size n chunks.""\n    return [df[i::n] for i in range(n)]\n'"
src/utils/jsonl2iddata.py,0,"b'#!/usr/bin/env python\r\n""""""A utility tool for extracting the identifier tokens from existing .jsonl.gz data. Primarily used for exporting\r\ndata for MSR\'s tool for dataset deduplication at https://github.com/Microsoft/near-duplicate-code-detector.\r\n\r\nUsage:\r\n    jsonl2iddata.py [options] INPUT_PATH OUTPUT_PATH\r\n\r\nOptions:\r\n    -h --help                  Show this screen.\r\n    --azure-info=<path>        Azure authentication information file (JSON). Used to load data from Azure storage.\r\n    --debug                    Enable debug routines. [default: False]\r\n""""""\r\nfrom docopt import docopt\r\n\r\nfrom dpu_utils.utils import run_and_debug, RichPath, ChunkWriter\r\n\r\n\r\ndef run(arguments):\r\n    azure_info_path = arguments.get(\'--azure-info\', None)\r\n    input_folder = RichPath.create(arguments[\'INPUT_PATH\'], azure_info_path)\r\n    output_folder = RichPath.create(arguments[\'OUTPUT_PATH\'], azure_info_path)\r\n\r\n    with ChunkWriter(output_folder, file_prefix=\'codedata\', max_chunk_size=500, file_suffix=\'.jsonl.gz\') as chunked_writer:\r\n        for file in input_folder.iterate_filtered_files_in_dir(\'*.jsonl.gz\'):\r\n            for line in file.read_by_file_suffix():\r\n                tokens=line[\'code_tokens\']\r\n                chunked_writer.add(dict(filename=\'%s:%s:%s\' % (line[\'repo\'], line[\'path\'], line[\'lineno\']), tokens=tokens))\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    args = docopt(__doc__)\r\n    run_and_debug(lambda: run(args), args.get(\'--debug\', False))\r\n'"
src/utils/nearestneighbor.py,0,"b'#!/usr/bin/env python\r\n""""""\r\nUsage:\r\n    nearestneighbor.py [options] (--code | --query | --both) MODEL_PATH DATA_PATH\r\n\r\nOptions:\r\n    --azure-info=<path>        Azure authentication information file (JSON). Used to load data from Azure storage.\r\n    --distance-metric METRIC   The distance metric to use [default: cosine]\r\n    -h --help                  Show this screen.\r\n    --hypers-override HYPERS   JSON dictionary overriding hyperparameter values.\r\n    --debug                    Enable debug routines. [default: False]\r\n    --num-nns NUM              The number of NNs to visualize [default: 2]\r\n    --distance-threshold TH    The distance threshold above which to ignore [default: 0.2]\r\n    --max-num-items LIMIT      The maximum number of items to use. Use zero for all. [default: 0]\r\n""""""\r\nimport json\r\nfrom itertools import chain\r\nfrom typing import Any, Dict, List\r\n\r\nfrom docopt import docopt\r\nfrom dpu_utils.utils import RichPath, run_and_debug\r\nimport numpy as np\r\nfrom more_itertools import take\r\nfrom scipy.spatial.distance import pdist\r\n\r\nfrom pygments import highlight\r\nfrom pygments.lexers import get_lexer_by_name\r\nfrom pygments.formatters import TerminalFormatter\r\n\r\nimport model_restore_helper\r\n\r\n# Condensed to square from\r\n# http://stackoverflow.com/questions/13079563/how-does-condensed-distance-matrix-work-pdist\r\nfrom utils.visutils import square_to_condensed\r\n\r\n\r\ndef to_string(code: str, language: str) -> str:\r\n    lexer = get_lexer_by_name(language, stripall=True)\r\n    formatter = TerminalFormatter(linenos=True)\r\n    return highlight(code, lexer, formatter)\r\n\r\ndef run(arguments) -> None:\r\n    azure_info_path = arguments.get(\'--azure-info\', None)\r\n    data_path = RichPath.create(arguments[\'DATA_PATH\'], azure_info_path)\r\n    assert data_path.is_dir(), ""%s is not a folder"" % (data_path,)\r\n\r\n    hypers_override = arguments.get(\'--hypers-override\')\r\n    if hypers_override is not None:\r\n        hypers_override = json.loads(hypers_override)\r\n    else:\r\n        hypers_override = {}\r\n\r\n    model_path = RichPath.create(arguments[\'MODEL_PATH\'], azure_info_path=azure_info_path)\r\n\r\n    model = model_restore_helper.restore(\r\n        path=model_path,\r\n        is_train=False,\r\n        hyper_overrides=hypers_override)\r\n\r\n    num_elements_to_take = int(arguments[\'--max-num-items\'])\r\n    data = chain(*chain(list(f.read_by_file_suffix()) for f in data_path.iterate_filtered_files_in_dir(\'*.jsonl.gz\')))\r\n    if num_elements_to_take == 0:  # Take all\r\n        data = list(data)\r\n    else:\r\n        assert num_elements_to_take > 0\r\n        data = take(num_elements_to_take, data)\r\n\r\n\r\n    num_nns = int(arguments[\'--num-nns\'])\r\n\r\n    if arguments[\'--code\']:\r\n        representations = model.get_code_representations(data)\r\n    elif arguments[\'--query\']:\r\n        representations = model.get_query_representations(data)\r\n    else:\r\n        code_representations = model.get_code_representations(data)\r\n        query_representations = model.get_query_representations(data)\r\n        representations = np.concatenate([code_representations, query_representations], axis=-1)\r\n\r\n    filtered_representations = []\r\n    filtered_data = []  # type: List[Dict[str, Any]]\r\n    for i, representation in enumerate(representations):\r\n        if representation is None:\r\n            continue\r\n        filtered_representations.append(representation)\r\n        filtered_data.append(data[i])\r\n\r\n    filtered_representations = np.stack(filtered_representations, axis=0)\r\n    flat_distances = pdist(filtered_representations, arguments[\'--distance-metric\'])\r\n\r\n    for i, data in enumerate(filtered_data):\r\n        distance_from_i = np.fromiter(\r\n            (flat_distances[square_to_condensed(i, j, len(filtered_data))] if i != j else float(\'inf\') for j in\r\n             range(len(filtered_data))), dtype=np.float)\r\n\r\n        nns = [int(k) for k in np.argsort(distance_from_i)[:num_nns]]  # The first two NNs\r\n\r\n        if distance_from_i[nns[0]] > float(arguments[\'--distance-threshold\']):\r\n            continue\r\n\r\n        print(\'===============================================================\')\r\n        print(f""{data[\'repo\']}:{data[\'path\']}:{data[\'lineno\']}"")\r\n        print(to_string(data[\'original_string\'], language=data[\'language\']))\r\n\r\n        for j in range(num_nns):\r\n            print()\r\n            print(f\'Nearest Neighbour {j+1}: {filtered_data[nns[j]][""repo""]}:{filtered_data[nns[j]][""path""]}:{filtered_data[nns[j]][""lineno""]} (distance {distance_from_i[nns[j]]})\')\r\n            print(to_string(filtered_data[nns[j]][\'original_string\'], language=filtered_data[nns[j]][\'language\']))\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    args = docopt(__doc__)\r\n    run_and_debug(lambda: run(args), args.get(\'--debug\', False))\r\n'"
src/utils/pkldf2jsonl.py,0,"b'import pandas as pd\nfrom .general_utils import chunkify\nfrom dpu_utils.utils import RichPath\nfrom multiprocessing import Pool, cpu_count\n\n\ndef df_to_jsonl(df: pd.DataFrame, RichPath_obj: RichPath, i: int, basefilename=\'codedata\') -> str:\n    dest_filename = f\'{basefilename}_{str(i).zfill(5)}.jsonl.gz\'\n    RichPath_obj.join(dest_filename).save_as_compressed_file(df.to_dict(orient=\'records\'))\n    return str(RichPath_obj.join(dest_filename))\n\n\ndef chunked_save_df_to_jsonl(df: pd.DataFrame,\n                             output_folder: RichPath,\n                             num_chunks: int=None,\n                             parallel: bool=True) -> None:\n    ""Chunk DataFrame (n chunks = num cores) and save as jsonl files.""\n\n    df.reset_index(drop=True, inplace=True)\n    # parallel saving to jsonl files on azure\n    n = cpu_count() if num_chunks is None else num_chunks\n    dfs = chunkify(df, n)\n    args = zip(dfs, [output_folder]*len(dfs), range(len(dfs)))\n\n    if not parallel:\n        for arg in args:\n            dest_filename = df_to_jsonl(*arg)\n            print(f\'Wrote chunk to {dest_filename}\')\n    else:\n        with Pool(cpu_count()) as pool:\n            pool.starmap(df_to_jsonl, args)\n\n'"
src/utils/py_utils.py,0,"b'import multiprocessing\nfrom typing import List, Iterable, Callable, TypeVar\n\n\nJobType = TypeVar(""JobType"")\nResultType = TypeVar(""ResultType"")\n\n\ndef __parallel_queue_worker(worker_id: int,\n                            job_queue: multiprocessing.Queue,\n                            result_queue: multiprocessing.Queue,\n                            worker_fn: Callable[[int, JobType], Iterable[ResultType]]):\n    while True:\n        job = job_queue.get()\n\n        # ""None"" is the signal for last job, put that back in for other workers and stop:\n        if job is None:\n            job_queue.put(job)\n            break\n\n        for result in worker_fn(worker_id, job):\n            result_queue.put(result)\n    result_queue.put(None)\n\n\ndef run_jobs_in_parallel(all_jobs: List[JobType],\n                         worker_fn: Callable[[int, JobType], Iterable[ResultType]],\n                         received_result_callback: Callable[[ResultType], None],\n                         finished_callback: Callable[[], None],\n                         result_queue_size: int=100) -> None:\n    """"""\n    Runs jobs in parallel and uses callbacks to collect results.\n    :param all_jobs: Job descriptions; one at a time will be parsed into worker_fn.\n    :param worker_fn: Worker function receiving a job; many copies may run in parallel.\n      Can yield results, which will be processed (one at a time) by received_result_callback.\n    :param received_result_callback: Called when a result was produced by any worker. Only one will run at a time.\n    :param finished_callback: Called when all jobs have been processed.\n    """"""\n    job_queue = multiprocessing.Queue(len(all_jobs) + 1)\n    for job in all_jobs:\n        job_queue.put(job)\n    job_queue.put(None)  # Marker that we are done\n\n    # This will hold the actual results:\n    result_queue = multiprocessing.Queue(result_queue_size)\n\n    # Create workers:\n    num_workers = multiprocessing.cpu_count() - 1\n    workers = [multiprocessing.Process(target=__parallel_queue_worker,\n                                       args=(worker_id, job_queue, result_queue, worker_fn))\n               for worker_id in range(num_workers)]\n    for worker in workers:\n        worker.start()\n\n    num_workers_finished = 0\n    while True:\n        result = result_queue.get()\n        if result is None:\n            num_workers_finished += 1\n            if num_workers_finished == len(workers):\n                finished_callback()\n                break\n        else:\n            received_result_callback(result)\n\n    for worker in workers:\n        worker.join()\n'"
src/utils/repo_helper.py,0,"b'from subprocess import check_call\nfrom pathlib import Path\nfrom typing import List\nfrom json import loads\nfrom requests import get\n\nclass Repo():\n    """"""\n    Because we don\'t have a good way to query the content of live repos.\n\n    Example usage:\n\n    # Instantiate object and retrieve code from repo: tensorflow/tensorflow\n    > rc = Repo(org=\'tensorflow\', repo_name=\'tensorflow\', dest_path=\'/some/existing/folder\')\n    > rc.clone()  # gets the code by cloning if does not exist, or optionally pull the latest code.\n\n    # returns list of Path objects in repo that end in \'.py\'\n    > rc.get_filenames_with_extension(\'.py\')\n    """"""\n\n    def __init__(self, org: str, repo_name: str, dest_path: str):\n        self.metadata = self.__get_metadata(org, repo_name)\n        assert Path(dest_path).is_dir(), f\'Argument dest_path should be an existing directory: {dest_path}\'\n        self.dest_path = Path(dest_path)\n        self.repo_save_folder = self.dest_path/self.metadata[\'full_name\']\n\n    def __get_metadata(self, org: str, repo_name: str) -> dict:\n        ""Validates github org and repo_name, and returns metadata about the repo.""\n\n        resp = get(f\'https://api.github.com/repos/{org}/{repo_name}\')\n        resp.raise_for_status()\n        info = loads(resp.text or resp.content)\n        if \'clone_url\' not in info:\n            raise Exception(f\'Cannot find repository {org}/{repo_name}\')\n        return info\n\n    def clone(self, refresh: bool=True) -> Path:\n        ""Will clone a repo (default branch only) into the desired path, or if already exists will optionally pull latest code.""\n        default_branch = self.metadata[\'default_branch\']\n        clone_url = self.metadata[\'clone_url\']\n\n        if not self.repo_save_folder.exists():\n            cmd = f\'git clone --depth 1 -b {default_branch} --single-branch {clone_url} {str(self.repo_save_folder.absolute())}\'\n            print(f\'Cloning repo:\\n {cmd}\')\n            check_call(cmd, shell=True)\n\n        elif refresh:\n            cmd = f\'git -C {str(self.repo_save_folder.absolute())} pull\'\n            print(f\'Pulling latest code from repo:\\n {cmd}\')\n            check_call(cmd, shell=True)\n\n    def get_filenames_with_extension(self, extension: str=\'.py\') -> List[Path]:\n        ""Return a list of filenames in the repo that end in the supplied extension.""\n        files = self.repo_save_folder.glob(\'**/*\')\n        return [f for f in files if f.is_file() and f.name.endswith(extension)]\n'"
src/utils/tfutils.py,22,"b'from typing import List, Tuple, Dict, Any, Optional, Union\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.ops.init_ops import Initializer\n\nfrom dpu_utils.mlutils import Vocabulary\n\nfrom utils.bpevocabulary import BpeVocabulary\n\nBIG_NUMBER = 1e7\n\n\ndef convert_and_pad_token_sequence(token_vocab: Union[Vocabulary, BpeVocabulary],\n                                   token_sequence: List[str],\n                                   output_tensor_size: int,\n                                   pad_from_left: bool = False) \\\n        -> Tuple[np.ndarray, np.ndarray]:\n    """"""\n    Tensorise token sequence with padding; returning a mask for used elements as well.\n\n    Args:\n        token_vocab: Vocabulary or BPE encoder to use. We assume that token_vocab[0] is the padding symbol.\n        token_sequence: List of tokens in string form\n        output_tensor_size: Size of the resulting tensor (i.e., length up which we pad / down to which we truncate.\n        pad_from_left: Indicate if we are padding/truncating on the left side of string. [Default: False]\n\n    Returns:\n        Pair of numpy arrays. First is the actual tensorised token sequence, the second is a masking tensor\n        that is 1.0 for those token indices that are actually used.\n    """"""\n    if isinstance(token_vocab, BpeVocabulary):\n        token_ids = np.array(list(token_vocab.transform([token_sequence], fixed_length=output_tensor_size))[0])\n        token_mask = np.array([1 if token_ids[i] > 0 else 0 for i in range(len(token_ids))])\n        return token_ids, token_mask\n\n    if pad_from_left:\n        token_sequence = token_sequence[-output_tensor_size:]\n    else:\n        token_sequence = token_sequence[:output_tensor_size]\n\n    sequence_length = len(token_sequence)\n    if pad_from_left:\n        start_idx = output_tensor_size - sequence_length\n    else:\n        start_idx = 0\n\n    token_ids = np.zeros(output_tensor_size, dtype=np.int32)\n    token_mask = np.zeros(output_tensor_size, dtype=np.float32)\n    for i, token in enumerate(token_sequence, start=start_idx):\n        token_ids[i] = token_vocab.get_id_or_unk(token)\n        token_mask[i] = True\n\n    return token_ids, token_mask\n\n\ndef write_to_feed_dict(feed_dict: Dict[tf.Tensor, Any], placeholder, val) -> None:\n    if len(val) == 0:\n        ph_shape = [dim if dim is not None else 0 for dim in placeholder.shape.as_list()]\n        feed_dict[placeholder] = np.empty(ph_shape)\n    else:\n        feed_dict[placeholder] = val\n\n\nclass NoisyIdentityInitializer(Initializer):\n    def __init__(self, noise: float=1e-1):\n        self.__noise = noise\n        self.__identity_initializer = tf.initializers.identity()\n        self.__noise_initializer = tf.initializers.random_uniform(minval=-self.__noise, maxval=self.__noise)\n\n    def set_config(self):\n        return {\n            ""noise"": self.__noise,\n        }\n\n    def __call__(self, shape, dtype=None, partition_info=None):\n        identity = self.__identity_initializer(shape=shape, dtype=dtype, partition_info=partition_info)\n        noise = self.__noise_initializer(shape=shape, dtype=dtype, partition_info=partition_info)\n        return identity + noise\n\n\ndef get_activation(activation_fun: Optional[str]):\n    if activation_fun is None:\n        return None\n    activation_fun = activation_fun.lower()\n    if activation_fun == \'linear\':\n        return None\n    if activation_fun == \'tanh\':\n        return tf.tanh\n    if activation_fun == \'relu\':\n        return tf.nn.relu\n    if activation_fun == \'leaky_relu\':\n        return tf.nn.leaky_relu\n    if activation_fun == \'elu\':\n        return tf.nn.elu\n    if activation_fun == \'selu\':\n        return tf.nn.selu\n    if activation_fun == \'gelu\':\n        def gelu(input_tensor):\n            cdf = 0.5 * (1.0 + tf.erf(input_tensor / tf.sqrt(2.0)))\n            return input_tensor * cdf\n        return gelu\n    else:\n        raise ValueError(""Unknown activation function \'%s\'!"" % activation_fun)\n\n\ndef pool_sequence_embedding(pool_mode: str,\n                            sequence_token_embeddings: tf.Tensor,\n                            sequence_lengths: tf.Tensor,\n                            sequence_token_masks: tf.Tensor) -> tf.Tensor:\n    """"""\n    Takes a batch of sequences of token embeddings and applies a pooling function,\n    returning one representation for each sequence.\n\n    Args:\n        pool_mode: The pooling mode, one of ""mean"", ""max"", ""weighted_mean"". For\n         the latter, a weight network is introduced that computes a score (from [0,1])\n         for each token, and embeddings are weighted by that score when computing\n         the mean.\n        sequence_token_embeddings: A float32 tensor of shape [B, T, D], where B is the\n         batch dimension, T is the maximal number of tokens per sequence, and D is\n         the embedding size.\n        sequence_lengths: An int32 tensor of shape [B].\n        sequence_token_masks: A float32 tensor of shape [B, T] with 0/1 values used\n         for masking out unused entries in sequence_embeddings.\n    Returns:\n        A tensor of shape [B, D], containing the pooled representation for each\n        sequence.\n    """"""\n    if pool_mode == \'mean\':\n        seq_token_embeddings_masked = \\\n            sequence_token_embeddings * tf.expand_dims(sequence_token_masks, axis=-1)  # B x T x D\n        seq_token_embeddings_sum = tf.reduce_sum(seq_token_embeddings_masked, axis=1)  # B x D\n        sequence_lengths = tf.expand_dims(tf.cast(sequence_lengths, dtype=tf.float32), axis=-1)  # B x 1\n        return seq_token_embeddings_sum / sequence_lengths\n    elif pool_mode == \'max\':\n        sequence_token_masks = -BIG_NUMBER * (1 - sequence_token_masks)  # B x T\n        sequence_token_masks = tf.expand_dims(sequence_token_masks, axis=-1)  # B x T x 1\n        return tf.reduce_max(sequence_token_embeddings + sequence_token_masks, axis=1)\n    elif pool_mode == \'weighted_mean\':\n        token_weights = tf.layers.dense(sequence_token_embeddings,\n                                        units=1,\n                                        activation=tf.sigmoid,\n                                        use_bias=False)  # B x T x 1\n        token_weights *= tf.expand_dims(sequence_token_masks, axis=-1)  # B x T x 1\n        seq_embedding_weighted_sum = tf.reduce_sum(sequence_token_embeddings * token_weights, axis=1)  # B x D\n        return seq_embedding_weighted_sum / (tf.reduce_sum(token_weights, axis=1) + 1e-8)  # B x D\n    else:\n        raise ValueError(""Unknown sequence pool mode \'%s\'!"" % pool_mode)\n'"
src/utils/visutils.py,0,"b'def square_to_condensed(i, j, n):\r\n    assert i != j, ""no diagonal elements in condensed matrix""\r\n    if i < j:\r\n        i, j = j, i\r\n    return int(n * j - j * (j + 1) / 2 + i - 1 - j)'"
function_parser/function_parser/parsers/__init__.py,0,b''
function_parser/function_parser/parsers/commentutils.py,0,"b'def strip_c_style_comment_delimiters(comment: str) -> str:\r\n    comment_lines = comment.split(\'\\n\')\r\n    cleaned_lines = []\r\n    for l in comment_lines:\r\n        l = l.strip()\r\n        if l.endswith(\'*/\'):\r\n            l = l[:-2]\r\n        if l.startswith(\'*\'):\r\n            l = l[1:]\r\n        elif l.startswith(\'/**\'):\r\n            l = l[3:]\r\n        elif l.startswith(\'//\'):\r\n            l = l[2:]\r\n        cleaned_lines.append(l.strip())\r\n    return \'\\n\'.join(cleaned_lines)\r\n\r\n\r\ndef get_docstring_summary(docstring: str) -> str:\r\n    """"""Get the first lines of the documentation comment up to the empty lines.""""""\r\n    if \'\\n\\n\' in docstring:\r\n        return docstring.split(\'\\n\\n\')[0]\r\n    elif \'@\' in docstring:\r\n        return docstring[:docstring.find(\'@\')]  # This usually is the start of a JavaDoc-style @param comment.\r\n    return docstring'"
function_parser/function_parser/parsers/go_parser.py,0,"b""from typing import List, Dict, Any\n\nfrom parsers.language_parser import LanguageParser, match_from_span, tokenize_code\nfrom parsers.commentutils import get_docstring_summary, strip_c_style_comment_delimiters\n\n\nclass GoParser(LanguageParser):\n\n    FILTER_PATHS = ('test', 'vendor')\n\n    @staticmethod\n    def get_definition(tree, blob: str) -> List[Dict[str, Any]]:\n        definitions = []\n        comment_buffer = []\n        for child in tree.root_node.children:\n            if child.type == 'comment':\n                comment_buffer.append(child)\n            elif child.type in ('method_declaration', 'function_declaration'):\n                docstring = '\\n'.join([match_from_span(comment, blob) for comment in comment_buffer])\n                docstring_summary = strip_c_style_comment_delimiters((get_docstring_summary(docstring)))\n\n                metadata = GoParser.get_function_metadata(child, blob)\n                definitions.append({\n                    'type': child.type,\n                    'identifier': metadata['identifier'],\n                    'parameters': metadata['parameters'],\n                    'function': match_from_span(child, blob),\n                    'function_tokens': tokenize_code(child, blob),\n                    'docstring': docstring,\n                    'docstring_summary': docstring_summary,\n                    'start_point': child.start_point,\n                    'end_point': child.end_point     \n                })\n                comment_buffer = []\n            else:\n                comment_buffer = []\n        return definitions\n\n\n    @staticmethod\n    def get_function_metadata(function_node, blob: str) -> Dict[str, str]:\n        metadata = {\n            'identifier': '',\n            'parameters': '',\n        }\n        if function_node.type == 'function_declaration':\n            metadata['identifier'] = match_from_span(function_node.children[1], blob)\n            metadata['parameters'] = match_from_span(function_node.children[2], blob)\n        elif function_node.type == 'method_declaration':\n            metadata['identifier'] = match_from_span(function_node.children[2], blob)\n            metadata['parameters'] = ' '.join([match_from_span(function_node.children[1], blob),\n                                               match_from_span(function_node.children[3], blob)])\n        return metadata\n"""
function_parser/function_parser/parsers/java_parser.py,0,"b""from typing import List, Dict, Any\r\n\r\nfrom parsers.language_parser import LanguageParser, match_from_span, tokenize_code, traverse_type\r\nfrom parsers.commentutils import strip_c_style_comment_delimiters, get_docstring_summary\r\n\r\n\r\nclass JavaParser(LanguageParser):\r\n\r\n    FILTER_PATHS = ('test', 'tests')\r\n\r\n    BLACKLISTED_FUNCTION_NAMES = {'toString', 'hashCode', 'equals', 'finalize', 'notify', 'notifyAll', 'clone'}\r\n\r\n    @staticmethod\r\n    def get_definition(tree, blob: str) -> List[Dict[str, Any]]:\r\n        classes = (node for node in tree.root_node.children if node.type == 'class_declaration')\r\n\r\n        definitions = []\r\n        for _class in classes:\r\n            class_identifier = match_from_span([child for child in _class.children if child.type == 'identifier'][0], blob).strip()\r\n            for child in (child for child in _class.children if child.type == 'class_body'):\r\n                for idx, node in enumerate(child.children):\r\n                    if node.type == 'method_declaration':\r\n                        if JavaParser.is_method_body_empty(node):\r\n                            continue\r\n                        docstring = ''\r\n                        if idx - 1 >= 0 and child.children[idx-1].type == 'comment':\r\n                            docstring = match_from_span(child.children[idx - 1], blob)\r\n                            docstring = strip_c_style_comment_delimiters(docstring)\r\n                        docstring_summary = get_docstring_summary(docstring)\r\n\r\n                        metadata = JavaParser.get_function_metadata(node, blob)\r\n                        if metadata['identifier'] in JavaParser.BLACKLISTED_FUNCTION_NAMES:\r\n                            continue\r\n                        definitions.append({\r\n                            'type': node.type,\r\n                            'identifier': '{}.{}'.format(class_identifier, metadata['identifier']),\r\n                            'parameters': metadata['parameters'],\r\n                            'function': match_from_span(node, blob),\r\n                            'function_tokens': tokenize_code(node, blob),\r\n                            'docstring': docstring,\r\n                            'docstring_summary': docstring_summary,\r\n                            'start_point': node.start_point,\r\n                            'end_point': node.end_point\r\n                        })\r\n        return definitions\r\n\r\n    @staticmethod\r\n    def get_class_metadata(class_node, blob: str) -> Dict[str, str]:\r\n        metadata = {\r\n            'identifier': '',\r\n            'argument_list': '',\r\n        }\r\n        is_header = False\r\n        for n in class_node.children:\r\n            if is_header:\r\n                if n.type == 'identifier':\r\n                    metadata['identifier'] = match_from_span(n, blob).strip('(:')\r\n                elif n.type == 'argument_list':\r\n                    metadata['argument_list'] = match_from_span(n, blob)\r\n            if n.type == 'class':\r\n                is_header = True\r\n            elif n.type == ':':\r\n                break\r\n        return metadata\r\n\r\n    @staticmethod\r\n    def is_method_body_empty(node):\r\n        for c in node.children:\r\n            if c.type in {'method_body', 'constructor_body'}:\r\n                if c.start_point[0] == c.end_point[0]:\r\n                    return True\r\n\r\n    @staticmethod\r\n    def get_function_metadata(function_node, blob: str) -> Dict[str, str]:\r\n        metadata = {\r\n            'identifier': '',\r\n            'parameters': '',\r\n        }\r\n\r\n        declarators = []\r\n        traverse_type(function_node, declarators, '{}_declarator'.format(function_node.type.split('_')[0]))\r\n        parameters = []\r\n        for n in declarators[0].children:\r\n            if n.type == 'identifier':\r\n                metadata['identifier'] = match_from_span(n, blob).strip('(')\r\n            elif n.type == 'formal_parameter':\r\n                parameters.append(match_from_span(n, blob))\r\n        metadata['parameters'] = ' '.join(parameters)\r\n        return metadata\r\n"""
function_parser/function_parser/parsers/javascript_parser.py,0,"b""from typing import List, Dict, Any\n\nfrom parsers.language_parser import LanguageParser, match_from_span, tokenize_code, traverse_type, previous_sibling, \\\n    node_parent\nfrom parsers.commentutils import get_docstring_summary, strip_c_style_comment_delimiters\n\n\nclass JavascriptParser(LanguageParser):\n\n    FILTER_PATHS = ('test', 'node_modules')\n\n    BLACKLISTED_FUNCTION_NAMES = {'toString', 'toLocaleString', 'valueOf'}\n\n    @staticmethod\n    def get_docstring(tree, node, blob: str) -> str:\n        docstring = ''\n        parent_node = node_parent(tree, node)\n\n        if parent_node.type == 'variable_declarator':\n            base_node = node_parent(tree, parent_node)  # Get the variable declaration\n        elif parent_node.type == 'pair':\n            base_node = parent_node  # This is a common pattern where a function is assigned as a value to a dictionary.\n        else:\n            base_node = node\n\n        prev_sibling = previous_sibling(tree, base_node)\n        if prev_sibling is not None and prev_sibling.type == 'comment':\n            all_prev_comment_nodes = [prev_sibling]\n            prev_sibling = previous_sibling(tree, prev_sibling)\n            while prev_sibling is not None and prev_sibling.type == 'comment':\n                all_prev_comment_nodes.append(prev_sibling)\n                last_comment_start_line = prev_sibling.start_point[0]\n                prev_sibling = previous_sibling(tree, prev_sibling)\n                if prev_sibling.end_point[0] + 1 < last_comment_start_line:\n                    break  # if there is an empty line, stop expanding.\n\n            docstring = ' '.join((strip_c_style_comment_delimiters(match_from_span(s, blob)) for s in all_prev_comment_nodes[::-1]))\n        return docstring\n        \n    @staticmethod\n    def get_definition(tree, blob: str) -> List[Dict[str, Any]]:\n        function_nodes = []\n        functions = []\n        traverse_type(tree.root_node, function_nodes, 'function')\n        for function in function_nodes:\n            if function.children is None or len(function.children) == 0:\n                continue\n            parent_node = node_parent(tree, function)\n            functions.append((parent_node.type, function, JavascriptParser.get_docstring(tree, function, blob)))\n\n        definitions = []\n        for node_type, function_node, docstring in functions:\n            metadata = JavascriptParser.get_function_metadata(function_node, blob)\n            docstring_summary = get_docstring_summary(docstring)\n\n            if metadata['identifier'] in JavascriptParser.BLACKLISTED_FUNCTION_NAMES:\n                continue\n            definitions.append({\n                'type': node_type,\n                'identifier': metadata['identifier'],\n                'parameters': metadata['parameters'],\n                'function': match_from_span(function_node, blob),\n                'function_tokens': tokenize_code(function_node, blob),\n                'docstring': docstring,\n                'docstring_summary': docstring_summary,\n                'start_point': function_node.start_point,\n                'end_point': function_node.end_point     \n            })\n        return definitions\n\n\n    @staticmethod\n    def get_function_metadata(function_node, blob: str) -> Dict[str, str]:\n        metadata = {\n            'identifier': '',\n            'parameters': '',\n        }\n        identifier_nodes = [child for child in function_node.children if child.type == 'identifier']\n        formal_parameters_nodes = [child for child in function_node.children if child.type == 'formal_parameters']\n        if identifier_nodes:\n            metadata['identifier'] = match_from_span(identifier_nodes[0], blob)\n        if formal_parameters_nodes:\n            metadata['parameters'] = match_from_span(formal_parameters_nodes[0], blob)\n        return metadata\n"""
function_parser/function_parser/parsers/language_parser.py,0,"b'import re\r\nfrom abc import ABC, abstractmethod\r\nfrom typing import List, Dict, Any, Set, Optional\r\n\r\nDOCSTRING_REGEX_TOKENIZER = re.compile(r""[^\\s,\'\\""`.():\\[\\]=*;>{\\}+-/\\\\]+|\\\\+|\\.+|\\(\\)|{\\}|\\[\\]|\\(+|\\)+|:+|\\[+|\\]+|{+|\\}+|=+|\\*+|;+|>+|\\++|-+|/+"")\r\n\r\n\r\ndef tokenize_docstring(docstring: str) -> List[str]:\r\n    return [t for t in DOCSTRING_REGEX_TOKENIZER.findall(docstring) if t is not None and len(t) > 0]\r\n\r\n\r\ndef tokenize_code(node, blob: str, nodes_to_exclude: Optional[Set]=None) -> List:\r\n    tokens = []\r\n    traverse(node, tokens)\r\n    return [match_from_span(token, blob) for token in tokens if nodes_to_exclude is None or token not in nodes_to_exclude]\r\n\r\n\r\ndef traverse(node, results: List) -> None:\r\n    if node.type == \'string\':\r\n        results.append(node)\r\n        return\r\n    for n in node.children:\r\n        traverse(n, results)\r\n    if not node.children:\r\n        results.append(node)\r\n\r\ndef nodes_are_equal(n1, n2):\r\n    return n1.type == n2.type and n1.start_point == n2.start_point and n1.end_point == n2.end_point\r\n\r\ndef previous_sibling(tree, node):\r\n    """"""\r\n    Search for the previous sibling of the node.\r\n\r\n    TODO: C TreeSitter should support this natively, but not its Python bindings yet. Replace later.\r\n    """"""\r\n    to_visit = [tree.root_node]\r\n    while len(to_visit) > 0:\r\n        next_node = to_visit.pop()\r\n        for i, node_at_i in enumerate(next_node.children):\r\n            if nodes_are_equal(node, node_at_i):\r\n                if i > 0:\r\n                    return next_node.children[i-1]\r\n                return None\r\n        else:\r\n            to_visit.extend(next_node.children)\r\n    return ValueError(""Could not find node in tree."")\r\n\r\n\r\ndef node_parent(tree, node):\r\n    to_visit = [tree.root_node]\r\n    while len(to_visit) > 0:\r\n        next_node = to_visit.pop()\r\n        for child in next_node.children:\r\n            if nodes_are_equal(child, node):\r\n                return next_node\r\n        else:\r\n            to_visit.extend(next_node.children)\r\n    raise ValueError(""Could not find node in tree."")\r\n\r\n\r\ndef match_from_span(node, blob: str) -> str:\r\n    lines = blob.split(\'\\n\')\r\n    line_start = node.start_point[0]\r\n    line_end = node.end_point[0]\r\n    char_start = node.start_point[1]\r\n    char_end = node.end_point[1]\r\n    if line_start != line_end:\r\n        return \'\\n\'.join([lines[line_start][char_start:]] + lines[line_start+1:line_end] + [lines[line_end][:char_end]])\r\n    else:\r\n        return lines[line_start][char_start:char_end]\r\n\r\n\r\ndef traverse_type(node, results: List, kind: str) -> None:\r\n    if node.type == kind:\r\n        results.append(node)\r\n    if not node.children:\r\n        return\r\n    for n in node.children:\r\n        traverse_type(n, results, kind)\r\n\r\n\r\nclass LanguageParser(ABC):\r\n    @staticmethod\r\n    @abstractmethod\r\n    def get_definition(tree, blob: str) -> List[Dict[str, Any]]:\r\n        pass\r\n\r\n    @staticmethod\r\n    @abstractmethod\r\n    def get_class_metadata(class_node, blob):\r\n        pass\r\n\r\n    @staticmethod\r\n    @abstractmethod\r\n    def get_function_metadata(function_node, blob) -> Dict[str, str]:\r\n        pass\r\n\r\n    @staticmethod\r\n    @abstractmethod\r\n    def get_context(tree, blob):\r\n        raise NotImplementedError\r\n\r\n    @staticmethod\r\n    @abstractmethod\r\n    def get_calls(tree, blob):\r\n        raise NotImplementedError\r\n'"
function_parser/function_parser/parsers/php_parser.py,0,"b""from typing import List, Dict, Any\n\nfrom parsers.language_parser import LanguageParser, match_from_span, tokenize_code, traverse_type\nfrom parsers.commentutils import strip_c_style_comment_delimiters, get_docstring_summary\n\n\nclass PhpParser(LanguageParser):\n\n    FILTER_PATHS = ('test', 'tests')\n\n    BLACKLISTED_FUNCTION_NAMES = {'__construct', '__destruct', '__call', '__callStatic',\n                                  '__get', '__set', '__isset', '__unset',\n                                  '__sleep', '__wakeup', '__toString', '__invoke',\n                                  '__set_state', '__clone', '__debugInfo', '__serialize',\n                                  '__unserialize'}\n\n    @staticmethod\n    def get_docstring(trait_node, blob: str, idx: int) -> str:\n        docstring = ''\n        if idx - 1 >= 0 and trait_node.children[idx-1].type == 'comment':\n            docstring = match_from_span(trait_node.children[idx-1], blob)\n            docstring = strip_c_style_comment_delimiters(docstring)\n        return docstring\n\n\n    @staticmethod\n    def get_declarations(declaration_node, blob: str, node_type: str) -> List[Dict[str, Any]]:\n        declarations = []\n        for idx, child in enumerate(declaration_node.children):\n            if child.type == 'name':\n                declaration_name = match_from_span(child, blob)\n            elif child.type == 'method_declaration':\n                docstring = PhpParser.get_docstring(declaration_node, blob, idx)\n                docstring_summary = get_docstring_summary(docstring)\n                function_nodes = []\n                traverse_type(child, function_nodes, 'function_definition')\n                if function_nodes:\n                    function_node = function_nodes[0]\n                    metadata = PhpParser.get_function_metadata(function_node, blob)\n\n                    if metadata['identifier'] in PhpParser.BLACKLISTED_FUNCTION_NAMES:\n                        continue\n\n                    declarations.append({\n                        'type': node_type,\n                        'identifier': '{}.{}'.format(declaration_name, metadata['identifier']),\n                        'parameters': metadata['parameters'],\n                        'function': match_from_span(child, blob),\n                        'function_tokens': tokenize_code(child, blob),\n                        'docstring': docstring,\n                        'docstring_summary': docstring_summary,\n                        'start_point': function_node.start_point,\n                        'end_point': function_node.end_point\n                    })\n        return declarations\n\n\n    @staticmethod\n    def get_definition(tree, blob: str) -> List[Dict[str, Any]]:\n        trait_declarations = [child for child in tree.root_node.children if child.type == 'trait_declaration']\n        class_declarations = [child for child in tree.root_node.children if child.type == 'class_declaration']\n        definitions = []\n        for trait_declaration in trait_declarations:\n            definitions.extend(PhpParser.get_declarations(trait_declaration, blob, trait_declaration.type))\n        for class_declaration in class_declarations:\n            definitions.extend(PhpParser.get_declarations(class_declaration, blob, class_declaration.type))\n        return definitions\n\n\n    @staticmethod\n    def get_function_metadata(function_node, blob: str) -> Dict[str, str]:\n        metadata = {\n            'identifier': '',\n            'parameters': '',\n        }\n        metadata['identifier'] = match_from_span(function_node.children[1], blob)\n        metadata['parameters'] = match_from_span(function_node.children[2], blob)\n        return metadata\n"""
function_parser/function_parser/parsers/python_parser.py,0,"b'from typing import Dict, Iterable, Optional, Iterator, Any, List\r\n\r\nfrom parsers.language_parser import LanguageParser, match_from_span, tokenize_code, traverse_type\r\nfrom parsers.commentutils import get_docstring_summary\r\n\r\n\r\nclass PythonParser(LanguageParser):\r\n\r\n    FILTER_PATHS = (\'test\',)\r\n    STOPWORDS = ()\r\n\r\n    # Get function calls\r\n    @staticmethod\r\n    def get_context(tree, blob):\r\n        def _get_import_from(import_from_statement, blob):\r\n            context = {}\r\n            mode = \'from\'\r\n            library = \'\'\r\n            for n in import_from_statement.children:\r\n                if n.type == \'from\':\r\n                    mode = \'from\'\r\n                elif n.type == \'import\':\r\n                    mode = \'import\'\r\n                elif n.type == \'dotted_name\':\r\n                    if mode == \'from\':\r\n                        library = match_from_span(n, blob).strip()\r\n                    elif mode == \'import\':\r\n                        if library:\r\n                            context[match_from_span(n, blob).strip().strip(\',\')] = library\r\n            return context\r\n\r\n        def _get_import(import_statement, blob):\r\n            context = []\r\n            for n in import_statement.children:\r\n                if n.type == \'dotted_name\':\r\n                    context.append(match_from_span(n, blob).strip())\r\n                if n.type == \'aliased_import\':\r\n                    for a in n.children:\r\n                        if a.type == \'dotted_name\':\r\n                            context.append(match_from_span(a, blob).strip())\r\n            return context\r\n\r\n        import_from_statements = []\r\n        traverse_type(tree.root_node, import_from_statements, \'import_from_statement\')\r\n\r\n        import_statements = []\r\n        traverse_type(tree.root_node, import_statements, \'import_statement\')\r\n\r\n        context = []\r\n        context.extend((_get_import_from(i, blob) for i in import_from_statements))\r\n        context.extend((_get_import(i, blob) for i in import_statements))\r\n        return context\r\n\r\n    @staticmethod\r\n    def get_calls(tree, blob):\r\n        calls = []\r\n        traverse_type(tree.root_node, calls, \'call\')\r\n\r\n        def _traverse_calls(node, identifiers):\r\n            if node.type == \'identifier\':\r\n                identifiers.append(node)\r\n            if not node.children or node.type == \'argument_list\':\r\n                return\r\n            for n in node.children:\r\n                _traverse_calls(n, identifiers)\r\n\r\n        results = []\r\n        for call in calls:\r\n            identifiers = []\r\n            _traverse_calls(call, identifiers)\r\n\r\n            if identifiers:\r\n                identifier = identifiers[-1]\r\n                argument_lists = [n for n in call.children if n.type == \'argument_list\']\r\n                argument_list = \'\'\r\n                if argument_lists:\r\n                    argument_list = match_from_span(argument_lists[-1], blob)\r\n                results.append({\r\n                    \'identifier\': match_from_span(identifier, blob),\r\n                    \'argument_list\': argument_list,\r\n                    \'start_point\': identifier.start_point,\r\n                    \'end_point\': identifier.end_point,\r\n                })\r\n        return results\r\n\r\n    @staticmethod\r\n    def __get_docstring_node(function_node):\r\n        docstring_node = [node for node in function_node.children if\r\n                          node.type == \'expression_statement\' and node.children[0].type == \'string\']\r\n        if len(docstring_node) > 0:\r\n            return docstring_node[0].children[0]\r\n        return None\r\n\r\n    @staticmethod\r\n    def get_docstring(docstring_node, blob: str) -> str:\r\n        docstring = \'\'\r\n        if docstring_node is not None:\r\n            docstring = match_from_span(docstring_node, blob)\r\n            docstring = docstring.strip().strip(\'""\').strip(""\'"")\r\n        return docstring\r\n\r\n    @staticmethod\r\n    def get_function_metadata(function_node, blob: str) -> Dict[str, str]:\r\n        metadata = {\r\n            \'identifier\': \'\',\r\n            \'parameters\': \'\',\r\n            \'return_statement\': \'\'\r\n        }\r\n        is_header = False\r\n        for child in function_node.children:\r\n            if is_header:\r\n                if child.type == \'identifier\':\r\n                    metadata[\'identifier\'] = match_from_span(child, blob)\r\n                elif child.type == \'parameters\':\r\n                    metadata[\'parameters\'] = match_from_span(child, blob)\r\n            if child.type == \'def\':\r\n                is_header = True\r\n            elif child.type == \':\':\r\n                is_header = False\r\n            elif child.type == \'return_statement\':\r\n                metadata[\'return_statement\'] = match_from_span(child, blob)\r\n        return metadata\r\n\r\n    @staticmethod\r\n    def get_class_metadata(class_node, blob: str) -> Dict[str, str]:\r\n        metadata = {\r\n            \'identifier\': \'\',\r\n            \'argument_list\': \'\',\r\n        }\r\n        is_header = False\r\n        for child in class_node.children:\r\n            if is_header:\r\n                if child.type == \'identifier\':\r\n                    metadata[\'identifier\'] = match_from_span(child, blob)\r\n                elif child.type == \'argument_list\':\r\n                    metadata[\'argument_list\'] = match_from_span(child, blob)\r\n            if child.type == \'class\':\r\n                is_header = True\r\n            elif child.type == \':\':\r\n                break\r\n        return metadata\r\n\r\n    @staticmethod\r\n    def is_function_empty(function_node) -> bool:\r\n        seen_header_end = False\r\n        for child in function_node.children:\r\n            if seen_header_end and (child.type==\'pass_statement\' or child.type==\'raise_statement\'):\r\n                return True\r\n            elif seen_header_end:\r\n                return False\r\n\r\n            if child.type == \':\':\r\n                seen_header_end = True\r\n        return False\r\n\r\n    @staticmethod\r\n    def __process_functions(functions: Iterable, blob: str, func_identifier_scope: Optional[str]=None) -> Iterator[Dict[str, Any]]:\r\n        for function_node in functions:\r\n            if PythonParser.is_function_empty(function_node):\r\n                continue\r\n            function_metadata = PythonParser.get_function_metadata(function_node, blob)\r\n            if func_identifier_scope is not None:\r\n                function_metadata[\'identifier\'] = \'{}.{}\'.format(func_identifier_scope,\r\n                                                                 function_metadata[\'identifier\'])\r\n                if function_metadata[\'identifier\'].startswith(\'__\') and function_metadata[\'identifier\'].endswith(\'__\'):\r\n                    continue  # Blacklist built-in functions\r\n            docstring_node = PythonParser.__get_docstring_node(function_node)\r\n            function_metadata[\'docstring\'] = PythonParser.get_docstring(docstring_node, blob)\r\n            function_metadata[\'docstring_summary\'] = get_docstring_summary(function_metadata[\'docstring\'])\r\n            function_metadata[\'function\'] = match_from_span(function_node, blob)\r\n            function_metadata[\'function_tokens\'] = tokenize_code(function_node, blob, {docstring_node})\r\n            function_metadata[\'start_point\'] = function_node.start_point\r\n            function_metadata[\'end_point\'] = function_node.end_point\r\n\r\n            yield function_metadata\r\n\r\n    @staticmethod\r\n    def get_function_definitions(node):\r\n        for child in node.children:\r\n            if child.type == \'function_definition\':\r\n                yield child\r\n            elif child.type == \'decorated_definition\':\r\n                for c in child.children:\r\n                    if c.type == \'function_definition\':\r\n                        yield c\r\n\r\n    @staticmethod\r\n    def get_definition(tree, blob: str) -> List[Dict[str, Any]]:\r\n        functions = PythonParser.get_function_definitions(tree.root_node)\r\n        classes = (node for node in tree.root_node.children if node.type == \'class_definition\')\r\n\r\n        definitions = list(PythonParser.__process_functions(functions, blob))\r\n\r\n        for _class in classes:\r\n            class_metadata = PythonParser.get_class_metadata(_class, blob)\r\n            docstring_node = PythonParser.__get_docstring_node(_class)\r\n            class_metadata[\'docstring\'] = PythonParser.get_docstring(docstring_node, blob)\r\n            class_metadata[\'docstring_summary\'] = get_docstring_summary(class_metadata[\'docstring\'])\r\n            class_metadata[\'function\'] = \'\'\r\n            class_metadata[\'function_tokens\'] = []\r\n            class_metadata[\'start_point\'] = _class.start_point\r\n            class_metadata[\'end_point\'] = _class.end_point\r\n            definitions.append(class_metadata)\r\n\r\n            functions = PythonParser.get_function_definitions(_class)\r\n            definitions.extend(PythonParser.__process_functions(functions, blob, class_metadata[\'identifier\']))\r\n\r\n        return definitions\r\n'"
function_parser/function_parser/parsers/ruby_parser.py,0,"b'from typing import List, Dict, Any\n\nfrom parsers.language_parser import LanguageParser, match_from_span, tokenize_code\nfrom parsers.commentutils import get_docstring_summary\n\n\nclass RubyParser(LanguageParser):\n\n    FILTER_PATHS = (\'test\', \'vendor\')\n\n    BLACKLISTED_FUNCTION_NAMES = {\'initialize\', \'to_text\', \'display\', \'dup\', \'clone\', \'equal?\', \'==\', \'<=>\',\n                                  \'===\', \'<=\', \'<\', \'>\', \'>=\', \'between?\', \'eql?\', \'hash\'}\n\n    @staticmethod\n    def get_docstring(trait_node, blob: str, idx: int) -> str:\n        raise NotImplementedError(""Not used for Ruby."")\n\n\n    @staticmethod\n    def get_methods(module_or_class_node, blob: str, module_name: str, node_type: str) -> List[Dict[str, Any]]:\n        definitions = []\n        comment_buffer = []\n        module_or_class_name = match_from_span(module_or_class_node.children[1], blob)\n        for child in module_or_class_node.children:\n            if child.type == \'comment\':\n                comment_buffer.append(child)\n            elif child.type == \'method\':\n                docstring = \'\\n\'.join([match_from_span(comment, blob).strip().strip(\'#\') for comment in comment_buffer])\n                docstring_summary = get_docstring_summary(docstring)\n\n                metadata = RubyParser.get_function_metadata(child, blob)\n                if metadata[\'identifier\'] in RubyParser.BLACKLISTED_FUNCTION_NAMES:\n                    continue\n                definitions.append({\n                    \'type\': \'class\',\n                    \'identifier\': \'{}.{}.{}\'.format(module_name, module_or_class_name, metadata[\'identifier\']),\n                    \'parameters\': metadata[\'parameters\'],\n                    \'function\': match_from_span(child, blob),\n                    \'function_tokens\': tokenize_code(child, blob),\n                    \'docstring\': docstring,\n                    \'docstring_summary\': docstring_summary,\n                    \'start_point\': child.start_point,\n                    \'end_point\': child.end_point\n                })\n                comment_buffer = []\n            else:\n                comment_buffer = []\n        return definitions\n\n\n    @staticmethod\n    def get_definition(tree, blob: str) -> List[Dict[str, Any]]:\n        definitions = []\n        if \'ERROR\' not in set([child.type for child in tree.root_node.children]):\n            modules = [child for child in tree.root_node.children if child.type == \'module\']\n            for module in modules:\n                if module.children:\n                    module_name = match_from_span(module.children[1], blob)\n                    sub_modules = [child for child in module.children if child.type == \'module\' and child.children]\n                    classes = [child for child in module.children if child.type == \'class\']\n                    for sub_module_node in sub_modules:\n                        definitions.extend(RubyParser.get_methods(sub_module_node, blob, module_name, sub_module_node.type))\n                    for class_node in classes:\n                        definitions.extend(RubyParser.get_methods(class_node, blob, module_name, class_node.type))\n        return definitions\n\n\n    @staticmethod\n    def get_function_metadata(function_node, blob: str) -> Dict[str, str]:\n        metadata = {\n            \'identifier\': \'\',\n            \'parameters\': \'\',\n        }\n        metadata[\'identifier\'] = match_from_span(function_node.children[1], blob)\n        if function_node.children[2].type == \'method_parameters\':\n            metadata[\'parameters\'] = match_from_span(function_node.children[2], blob)\n        return metadata\n'"
src/dataextraction/python/__init__.py,0,b''
src/dataextraction/python/parse_python_data.py,0,"b'#!/usr/bin/env python\n""""""\nAcquires python data from local disk or GCP and performs parsing, cleaning and tokenization steps\nto form a parallel corpus of (code, docstring) pairs with additional metadata.  Processed data\nis saved as multi-part jsonl files to the OUTPUT_PATH.\n\nUsage:\n    parse_python_data.py [options] OUTPUT_PATH\n\nOptions:\n    -h --help                  Show this screen.\n    --input-folder=<path>      Use the given input folder instead of downloading.\n    --azure-info=<path>        Azure authentication information file (JSON). Used to load data from Azure storage\n    --debug                    Enable debug routines. [default: False]\n\n""""""\nimport re\nimport os\nfrom multiprocessing import Pool\nfrom typing import List, NamedTuple\n\nimport pandas as pd\nimport parso\nfrom docopt import docopt\nfrom dpu_utils.utils import RichPath, run_and_debug\nfrom tqdm import tqdm\n\nfrom dataextraction.utils import tokenize_docstring_from_string\nfrom utils.pkldf2jsonl import chunked_save_df_to_jsonl\n\n\nIS_WHITESPACE_REGEX = re.compile(r\'\\s+\')\n\n\nclass ParsedCode(NamedTuple):\n    code_tokens: List[str]\n    comment_tokens: List[str]\n\n\ndef tokenize_python_from_string(code: str,\n                                func_only: bool=True,\n                                report_errors: bool=False,\n                                only_ids: bool=False,\n                                add_keywords: bool=True) -> ParsedCode:\n    """"""\n    Tokenize Python code given a string.\n\n    Args:\n        code: The input code\n        func_only: if you want to only parse functions in code.\n        report_errors: Flag that turns on verbose error reporting\n        only_ids: Return only the identifiers within the code\n        add_keywords: Return keywords (used only when only_ids=True)\n\n    Returns:\n        Pair of lists. First list is sequence of code tokens; second list is sequence of tokens in comments.\n    """"""\n    try:\n        try:\n            parsed_ast = parso.parse(code, error_recovery=False, version=""2.7"")\n        except parso.parser.ParserSyntaxError:\n            parsed_ast = parso.parse(code, error_recovery=False, version=""3.7"")\n        code_tokens, comment_tokens = [], []\n\n        func_nodes = list(parsed_ast.iter_funcdefs())\n\n        # parse arbitrary snippets of code that are not functions if func_only = False\n        if not func_only:\n            func_nodes = [parsed_ast]\n        \n        for func_node in func_nodes:  # There should only be one, but we can process more...\n            doc_node = func_node.get_doc_node()\n            leaf_node = func_node.get_first_leaf()\n            while True:\n                # Skip over the docstring:\n                if leaf_node is doc_node:\n                    leaf_node = leaf_node.get_next_leaf()\n\n                # First, retrieve comment tokens:\n                for prefix in leaf_node._split_prefix():\n                    if prefix.type == \'comment\':\n                        comment_text = prefix.value[1:]  # Split off the leading ""#""\n                        comment_tokens.extend(tokenize_docstring_from_string(comment_text))\n\n                # Second, stop if we\'ve reached the end:\n                if leaf_node.type == \'endmarker\':\n                    break\n\n                # Third, record code tokens:\n                if not(IS_WHITESPACE_REGEX.match(leaf_node.value)):\n                    if only_ids:\n                        if leaf_node.type == \'name\':\n                            code_tokens.append(leaf_node.value)\n                    else:\n                        if leaf_node.type == \'keyword\':\n                            if add_keywords:\n                                code_tokens.append(leaf_node.value)\n                        else:\n                            code_tokens.append(leaf_node.value)\n                leaf_node = leaf_node.get_next_leaf()\n        return ParsedCode(code_tokens=code_tokens, comment_tokens=comment_tokens)\n    except Exception as e:\n        if report_errors:\n            print(\'Error tokenizing: %s\' % (e,))\n        return ParsedCode(code_tokens=[], comment_tokens=[])\n\n\ndef download_files_into_pandas(i: int=10) -> pd.DataFrame:\n    """"""Get files from Google Cloud Platform, there are 10 files.\n\n    Args:\n        i : int between 1 and 10 that specifies how many of the 10 files you\n            want to download.  You should only use this argument for testing.\n\n\n    Files are obtained by this query: https://console.cloud.google.com/bigquery?sq=235037502967:58a5d62f75f34d22b0f70d38b9352a85\n    """"""\n    frames = []\n    for i in tqdm(range(i), total=i):\n        success = False\n        while not success:\n            try:\n                frame = pd.read_csv(f\'https://storage.googleapis.com/kubeflow-examples/code_search_new/python_raw_v2/00000000000{i}.csv\', encoding=\'utf-8\')\n                frames.append(frame)\n                success = True\n            except Exception as e:\n                print(f\'Error downloading file {i}:\\n {e}, retrying...\')\n\n    df = pd.concat(frames)\n\n    df[\'repo\'] = df[\'repo_path\'].apply(lambda r: r.split()[0])\n    df[\'path\'] = df[\'repo_path\'].apply(lambda r: r.split()[1])\n    df.drop(columns=[\'repo_path\'], inplace=True)\n    df = df[[\'repo\', \'path\', \'content\']]\n    return df\n\n\ndef load_files_into_pandas(input_folder: str) -> pd.DataFrame:\n    """"""Get files from a local directory.\n\n    Args:\n        input_folder: the folder containing the .csv files\n    """"""\n    frames = []\n    for file in os.listdir(input_folder):\n        if not file.endswith(\'.csv\'):\n            continue\n        frame = pd.read_csv(os.path.join(input_folder, file), encoding=\'utf-8\')\n        frames.append(frame)\n\n    df = pd.concat(frames)\n\n    df[\'repo\'] = df[\'repo_path\'].apply(lambda r: r.split()[0])\n    df[\'path\'] = df[\'repo_path\'].apply(lambda r: r.split()[1])\n    df.drop(columns=[\'repo_path\'], inplace=True)\n    df = df[[\'repo\', \'path\', \'content\']]\n    return df\n\n\ndef parse_raw_data_into_function_list(blob, require_docstring: bool=True):\n    """"""Extract per-function data from a given code blob.\n\n    Filters out undesirable function types. Keep only the first line of the docstring, and remove all internal comments from\n    the code.\n\n    Args:\n        blob: String containing some python code.\n\n    Returns:\n        List of functions represented by dictionaries containing the code, docstring and metadata.\n    """"""\n    parsed_data_list = []\n    try:\n        try:\n            parsed_module = parso.parse(blob, error_recovery=False, version=""2.7"")\n        except parso.parser.ParserSyntaxError:\n            parsed_module = parso.parse(blob, error_recovery=False, version=""3.7"")\n\n        function_defs = list(parsed_module.iter_funcdefs())\n        for class_def in parsed_module.iter_classdefs():\n            function_defs.extend(class_def.iter_funcdefs())\n\n        for function_def in function_defs:\n            function_name = function_def.name.value\n            docstring_node = function_def.get_doc_node()\n            if docstring_node is None:\n                docstring = \'\'\n            else:\n                docstring = docstring_node.value\n            first_docstring_line = docstring.split(\'\\n\\s*\\n\')[0]\n\n            # We now need to un-indent the code which may have come from a class. For that, identify how far\n            # we are indented, and try to to remove that from all lines:\n            function_code = function_def.get_code()\n            def_prefix = list(function_def.get_first_leaf()._split_prefix())[-1].value\n            trimmed_lines = []\n            for line in function_code.splitlines():\n                if line.startswith(def_prefix):\n                    trimmed_lines.append(line[len(def_prefix):])\n            function_code = \'\\n\'.join(trimmed_lines)\n\n            should_use_function = not (re.search(r\'(__.+__)|(.*test.*)|(.*Test.*)\', function_name) or  # skip __*__ methods and test code\n                                       re.search(r\'NotImplementedException|@abstractmethod\', function_code) or\n                                       len(function_code.split(\'\\n\')) <= 2 or  # should have more than 1 line of code (the declaration is one line)\n                                       (len(first_docstring_line.split()) <= 2) and require_docstring)  # docstring should have at least 3 words.\n\n            if should_use_function:\n                parsed_data_list.append({\'code\': function_code,\n                                         \'docstring\': first_docstring_line,\n                                         \'language\': \'python\',\n                                         \'lineno\': function_def.start_pos[0],\n                                         \'func_name\': function_name,\n                                         })\n\n    except parso.parser.ParserSyntaxError:\n        pass\n    return parsed_data_list\n\n\ndef listlen(x):\n    if not isinstance(x, list):\n        return 0\n    return len(x)\n\n\ndef run(args):\n    azure_info_path = args.get(\'--azure-info\')\n    output_folder = RichPath.create(args[\'OUTPUT_PATH\'], azure_info_path)\n\n    # Download / read the data files:\n    if args[\'--input-folder\'] is None:\n        print(\'Downloading data...\')\n        raw_code_data_df = download_files_into_pandas()\n    else:\n        print(\'Loading data...\')\n        raw_code_data_df = load_files_into_pandas(args[\'--input-folder\'])\n    print(\'Data loaded.\')\n\n    # Find all the functions and methods, filter out ones that don\'t meet requirements,\n    # separate the code from the docstring and produce a list of functions that includes the code,\n    # the first line of the docstring, and metadata of each:\n    with Pool() as pool:\n        function_data = pool.map(parse_raw_data_into_function_list, raw_code_data_df.content.tolist())\n    assert len(function_data) == raw_code_data_df.shape[0], \\\n        f\'Row count mismatch. `raw_code_data_df` has {raw_code_data_df.shape[0]} rows; `function_data` has {len(function_data)} rows.\'\n    raw_code_data_df[\'function_data\'] = function_data\n    print(f\'Split {raw_code_data_df.shape[0]} blobs into {sum(len(fun_data) for fun_data in function_data)} documented individual functions.\')\n\n    # Flatten function data out:\n    # TODO: We should also have access to the SHA of the objects here.\n    raw_code_data_df = raw_code_data_df.set_index([\'repo\', \'path\'])[\'function_data\'].apply(pd.Series).stack()\n    raw_code_data_df = raw_code_data_df.reset_index()\n    raw_code_data_df.columns = [\'repo\', \'path\', \'_\', \'function_data\']\n\n    # Extract meta-data and format dataframe.\n    function_data_df = pd.DataFrame(raw_code_data_df.function_data.values.tolist())\n    assert len(raw_code_data_df) == len(function_data_df), \\\n        f\'Row count mismatch. `raw_code_data_df` has {len(raw_code_data_df)} rows; `function_data_df` has {len(function_data_df)} rows.\'\n    function_data_df = pd.concat([raw_code_data_df[[\'repo\', \'path\']], function_data_df], axis=1)\n\n    # remove observations where the same code appears more than once\n    num_before_dedup = len(function_data_df)\n    function_data_df = function_data_df.drop_duplicates([\'code\'])\n    num_after_dedup = len(function_data_df)\n\n    print(f\'Removed {num_before_dedup - num_after_dedup} exact duplicate rows.\')\n\n    print(\'Tokenizing code, comments and docstrings ...\')\n    with Pool() as pool:\n        code_tokenization_results: List[ParsedCode] = pool.map(tokenize_python_from_string,\n                                                               function_data_df[\'code\'].tolist())\n\n        code_tokens_list, comment_tokens_list = list(zip(*code_tokenization_results))\n        function_data_df[\'code_tokens\'] = code_tokens_list\n        function_data_df[\'comment_tokens\'] = comment_tokens_list\n        function_data_df[\'docstring_tokens\'] = pool.map(tokenize_docstring_from_string,\n                                                        function_data_df[\'docstring\'].tolist())\n    function_data_df.dropna(subset=[\'code_tokens\', \'comment_tokens\', \'docstring_tokens\'], inplace=True)\n    function_data_df.reset_index(inplace=True, drop=True)\n\n    cols_to_keep = [\'repo\', \'path\', \'lineno\', \'func_name\', \'language\',\n                    \'code\', \'code_tokens\', \'comment_tokens\',\n                    \'docstring\', \'docstring_tokens\',\n                    ]\n    # write data to jsonl\n    print(f\'Count by language:\\n{function_data_df.language.value_counts()}\')\n    chunked_save_df_to_jsonl(df=function_data_df[cols_to_keep],\n                             output_folder=output_folder,\n                             parallel=True)\n    print(f\'Wrote {function_data_df.shape[0]} rows to {str(output_folder)}.\')\n\n\nif __name__ == \'__main__\':\n    args = docopt(__doc__)\n    run_and_debug(lambda: run(args), args.get(\'--debug\'))\n'"
src/encoders/utils/__init__.py,0,b''
src/encoders/utils/bert_self_attention.py,74,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""The main BERT model and related functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport copy\nimport json\nimport math\nimport re\nimport six\nimport tensorflow as tf\n\nfrom utils.tfutils import get_activation\n\n\nclass BertConfig(object):\n  """"""Configuration for `BertModel`.""""""\n\n  def __init__(self,\n               vocab_size,\n               hidden_size=768,\n               num_hidden_layers=12,\n               num_attention_heads=12,\n               intermediate_size=3072,\n               hidden_act=""gelu"",\n               hidden_dropout_prob=0.1,\n               attention_probs_dropout_prob=0.1,\n               max_position_embeddings=512,\n               type_vocab_size=16,\n               initializer_range=0.02):\n    """"""Constructs BertConfig.\n\n    Args:\n      vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n      hidden_size: Size of the encoder layers and the pooler layer.\n      num_hidden_layers: Number of hidden layers in the Transformer encoder.\n      num_attention_heads: Number of attention heads for each attention layer in\n        the Transformer encoder.\n      intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)\n        layer in the Transformer encoder.\n      hidden_act: The non-linear activation function (function or string) in the\n        encoder and pooler.\n      hidden_dropout_prob: The dropout probability for all fully connected\n        layers in the embeddings, encoder, and pooler.\n      attention_probs_dropout_prob: The dropout ratio for the attention\n        probabilities.\n      max_position_embeddings: The maximum sequence length that this model might\n        ever be used with. Typically set this to something large just in case\n        (e.g., 512 or 1024 or 2048).\n      type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n        `BertModel`.\n      initializer_range: The stdev of the truncated_normal_initializer for\n        initializing all weight matrices.\n    """"""\n    self.vocab_size = vocab_size\n    self.hidden_size = hidden_size\n    self.num_hidden_layers = num_hidden_layers\n    self.num_attention_heads = num_attention_heads\n    self.hidden_act = hidden_act\n    self.intermediate_size = intermediate_size\n    self.hidden_dropout_prob = hidden_dropout_prob\n    self.attention_probs_dropout_prob = attention_probs_dropout_prob\n    self.max_position_embeddings = max_position_embeddings\n    self.type_vocab_size = type_vocab_size\n    self.initializer_range = initializer_range\n\n  @classmethod\n  def from_dict(cls, json_object):\n    """"""Constructs a `BertConfig` from a Python dictionary of parameters.""""""\n    config = BertConfig(vocab_size=None)\n    for (key, value) in six.iteritems(json_object):\n      config.__dict__[key] = value\n    return config\n\n  @classmethod\n  def from_json_file(cls, json_file):\n    """"""Constructs a `BertConfig` from a json file of parameters.""""""\n    with tf.gfile.GFile(json_file, ""r"") as reader:\n      text = reader.read()\n    return cls.from_dict(json.loads(text))\n\n  def to_dict(self):\n    """"""Serializes this instance to a Python dictionary.""""""\n    output = copy.deepcopy(self.__dict__)\n    return output\n\n  def to_json_string(self):\n    """"""Serializes this instance to a JSON string.""""""\n    return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n\nclass BertModel(object):\n  """"""BERT model (""Bidirectional Embedding Representations from a Transformer"").\n\n  Example usage:\n\n  ```python\n  # Already been converted into WordPiece token ids\n  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])\n  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])\n  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])\n\n  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n\n  model = modeling.BertModel(config=config, is_training=True,\n    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)\n\n  label_embeddings = tf.get_variable(...)\n  pooled_output = model.get_pooled_output()\n  logits = tf.matmul(pooled_output, label_embeddings)\n  ...\n  ```\n  """"""\n\n  def __init__(self,\n               config,\n               is_training,\n               input_ids,\n               input_mask=None,\n               token_type_ids=None,\n               use_one_hot_embeddings=True,\n               scope=None,\n               embedded_input=None):\n    """"""Constructor for BertModel.\n\n    Args:\n      config: `BertConfig` instance.\n      is_training: bool. rue for training model, false for eval model. Controls\n        whether dropout will be applied.\n      input_ids: int32 Tensor of shape [batch_size, seq_length].\n      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n        embeddings or tf.embedding_lookup() for the word embeddings. On the TPU,\n        it is must faster if this is True, on the CPU or GPU, it is faster if\n        this is False.\n      scope: (optional) variable scope. Defaults to ""bert"".\n      embedded_input: (optional) If provided, the embedding layer here is\n        skipped and the passed embeddings are passed into the self-attentional\n        layers.\n\n    Raises:\n      ValueError: The config is invalid or one of the input tensor shapes\n        is invalid.\n    """"""\n    config = copy.deepcopy(config)\n    if not is_training:\n      config.hidden_dropout_prob = 0.0\n      config.attention_probs_dropout_prob = 0.0\n\n    input_shape = get_shape_list(input_ids, expected_rank=2)\n    batch_size = input_shape[0]\n    seq_length = input_shape[1]\n\n    if input_mask is None:\n      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    if token_type_ids is None:\n      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)\n\n    with tf.variable_scope(""bert"", scope):\n      with tf.variable_scope(""embeddings""):\n        if embedded_input is None:\n          # Perform embedding lookup on the word ids.\n          (self.embedding_output, self.embedding_table) = embedding_lookup(\n              input_ids=input_ids,\n              vocab_size=config.vocab_size,\n              embedding_size=config.hidden_size,\n              initializer_range=config.initializer_range,\n              word_embedding_name=""word_embeddings"",\n              use_one_hot_embeddings=use_one_hot_embeddings)\n        else:\n          self.embedding_output = embedded_input\n\n        # Add positional embeddings and token type embeddings, then layer\n        # normalize and perform dropout.\n        self.embedding_output = embedding_postprocessor(\n            input_tensor=self.embedding_output,\n            use_token_type=True,\n            token_type_ids=token_type_ids,\n            token_type_vocab_size=config.type_vocab_size,\n            token_type_embedding_name=""token_type_embeddings"",\n            use_position_embeddings=True,\n            position_embedding_name=""position_embeddings"",\n            initializer_range=config.initializer_range,\n            max_position_embeddings=config.max_position_embeddings,\n            dropout_prob=config.hidden_dropout_prob)\n\n      with tf.variable_scope(""encoder""):\n        # This converts a 2D mask of shape [batch_size, seq_length] to a 3D\n        # mask of shape [batch_size, seq_length, seq_length] which is used\n        # for the attention scores.\n        attention_mask = create_attention_mask_from_input_mask(\n            input_ids, input_mask)\n\n        # Run the stacked transformer.\n        # `sequence_output` shape = [batch_size, seq_length, hidden_size].\n        self.all_encoder_layers = transformer_model(\n            input_tensor=self.embedding_output,\n            attention_mask=attention_mask,\n            hidden_size=config.hidden_size,\n            num_hidden_layers=config.num_hidden_layers,\n            num_attention_heads=config.num_attention_heads,\n            intermediate_size=config.intermediate_size,\n            intermediate_act_fn=get_activation(config.hidden_act),\n            hidden_dropout_prob=config.hidden_dropout_prob,\n            attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n            initializer_range=config.initializer_range,\n            do_return_all_layers=True)\n\n      self.sequence_output = self.all_encoder_layers[-1]\n      # The ""pooler"" converts the encoded sequence tensor of shape\n      # [batch_size, seq_length, hidden_size] to a tensor of shape\n      # [batch_size, hidden_size]. This is necessary for segment-level\n      # (or segment-pair-level) classification tasks where we need a fixed\n      # dimensional representation of the segment.\n      with tf.variable_scope(""pooler""):\n        # We ""pool"" the model by simply taking the hidden state corresponding\n        # to the first token. We assume that this has been pre-trained\n        first_token_tensor = tf.squeeze(self.sequence_output[:, 0:1, :], axis=1)\n        self.pooled_output = tf.layers.dense(\n            first_token_tensor,\n            config.hidden_size,\n            activation=tf.tanh,\n            kernel_initializer=create_initializer(config.initializer_range))\n\n  def get_pooled_output(self):\n    return self.pooled_output\n\n  def get_sequence_output(self):\n    """"""Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    """"""\n    return self.sequence_output\n\n  def get_all_encoder_layers(self):\n    return self.all_encoder_layers\n\n  def get_embedding_output(self):\n    """"""Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    """"""\n    return self.embedding_output\n\n  def get_embedding_table(self):\n    return self.embedding_table\n\n\ndef get_assigment_map_from_checkpoint(tvars, init_checkpoint):\n  """"""Compute the union of the current variables and checkpoint variables.""""""\n  assignment_map = {}\n  initialized_variable_names = {}\n\n  name_to_variable = collections.OrderedDict()\n  for var in tvars:\n    name = var.name\n    m = re.match(""^(.*):\\\\d+$"", name)\n    if m is not None:\n      name = m.group(1)\n    name_to_variable[name] = var\n\n  init_vars = tf.train.list_variables(init_checkpoint)\n\n  assignment_map = collections.OrderedDict()\n  for x in init_vars:\n    (name, var) = (x[0], x[1])\n    if name not in name_to_variable:\n      continue\n    assignment_map[name] = name\n    initialized_variable_names[name] = 1\n    initialized_variable_names[name + "":0""] = 1\n\n  return (assignment_map, initialized_variable_names)\n\n\ndef dropout(input_tensor, dropout_prob):\n  """"""Perform dropout.\n\n  Args:\n    input_tensor: float Tensor.\n    dropout_prob: Python float. The probability of dropping out a value (NOT of\n      *keeping* a dimension as in `tf.nn.dropout`).\n\n  Returns:\n    A version of `input_tensor` with dropout applied.\n  """"""\n  if dropout_prob is None or dropout_prob == 0.0:\n    return input_tensor\n\n  output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)\n  return output\n\n\ndef layer_norm(input_tensor, name=None):\n  """"""Run layer normalization on the last dimension of the tensor.""""""\n  return tf.contrib.layers.layer_norm(\n      inputs=input_tensor, begin_norm_axis=-1, begin_params_axis=-1, scope=name)\n\n\ndef layer_norm_and_dropout(input_tensor, dropout_prob, name=None):\n  """"""Runs layer normalization followed by dropout.""""""\n  output_tensor = layer_norm(input_tensor, name)\n  output_tensor = dropout(output_tensor, dropout_prob)\n  return output_tensor\n\n\ndef create_initializer(initializer_range=0.02):\n  """"""Creates a `truncated_normal_initializer` with the given range.""""""\n  return tf.truncated_normal_initializer(stddev=initializer_range)\n\n\ndef embedding_lookup(input_ids,\n                     vocab_size,\n                     embedding_size=128,\n                     initializer_range=0.02,\n                     word_embedding_name=""word_embeddings"",\n                     use_one_hot_embeddings=False):\n  """"""Looks up words embeddings for id tensor.\n\n  Args:\n    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n      ids.\n    vocab_size: int. Size of the embedding vocabulary.\n    embedding_size: int. Width of the word embeddings.\n    initializer_range: float. Embedding initialization range.\n    word_embedding_name: string. Name of the embedding table.\n    use_one_hot_embeddings: bool. If True, use one-hot method for word\n      embeddings. If False, use `tf.nn.embedding_lookup()`. One hot is better\n      for TPUs.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, embedding_size].\n  """"""\n  # This function assumes that the input is of shape [batch_size, seq_length,\n  # num_inputs].\n  #\n  # If the input is a 2D tensor of shape [batch_size, seq_length], we\n  # reshape to [batch_size, seq_length, 1].\n  if input_ids.shape.ndims == 2:\n    input_ids = tf.expand_dims(input_ids, axis=[-1])\n\n  embedding_table = tf.get_variable(\n      name=word_embedding_name,\n      shape=[vocab_size, embedding_size],\n      initializer=create_initializer(initializer_range))\n\n  if use_one_hot_embeddings:\n    flat_input_ids = tf.reshape(input_ids, [-1])\n    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)\n    output = tf.matmul(one_hot_input_ids, embedding_table)\n  else:\n    output = tf.nn.embedding_lookup(embedding_table, input_ids)\n\n  input_shape = get_shape_list(input_ids)\n\n  output = tf.reshape(output,\n                      input_shape[0:-1] + [input_shape[-1] * embedding_size])\n  return (output, embedding_table)\n\n\ndef embedding_postprocessor(input_tensor,\n                            use_token_type=False,\n                            token_type_ids=None,\n                            token_type_vocab_size=16,\n                            token_type_embedding_name=""token_type_embeddings"",\n                            use_position_embeddings=True,\n                            position_embedding_name=""position_embeddings"",\n                            initializer_range=0.02,\n                            max_position_embeddings=512,\n                            dropout_prob=0.1):\n  """"""Performs various post-processing on a word embedding tensor.\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length,\n      embedding_size].\n    use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      Must be specified if `use_token_type` is True.\n    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.\n    token_type_embedding_name: string. The name of the embedding table variable\n      for token type ids.\n    use_position_embeddings: bool. Whether to add position embeddings for the\n      position of each token in the sequence.\n    position_embedding_name: string. The name of the embedding table variable\n      for positional embeddings.\n    initializer_range: float. Range of the weight initialization.\n    max_position_embeddings: int. Maximum sequence length that might ever be\n      used with this model. This can be longer than the sequence length of\n      input_tensor, but cannot be shorter.\n    dropout_prob: float. Dropout probability applied to the final output tensor.\n\n  Returns:\n    float tensor with same shape as `input_tensor`.\n\n  Raises:\n    ValueError: One of the tensor shapes or input values is invalid.\n  """"""\n  input_shape = get_shape_list(input_tensor, expected_rank=3)\n  batch_size = input_shape[0]\n  seq_length = input_shape[1]\n  width = input_shape[2]\n\n  if seq_length > max_position_embeddings:\n    raise ValueError(""The seq length (%d) cannot be greater than ""\n                     ""`max_position_embeddings` (%d)"" %\n                     (seq_length, max_position_embeddings))\n\n  output = input_tensor\n\n  if use_token_type:\n    if token_type_ids is None:\n      raise ValueError(""`token_type_ids` must be specified if""\n                       ""`use_token_type` is True."")\n    token_type_table = tf.get_variable(\n        name=token_type_embedding_name,\n        shape=[token_type_vocab_size, width],\n        initializer=create_initializer(initializer_range))\n    # This vocab will be small so we always do one-hot here, since it is always\n    # faster for a small vocabulary.\n    flat_token_type_ids = tf.reshape(token_type_ids, [-1])\n    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)\n    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)\n    token_type_embeddings = tf.reshape(token_type_embeddings,\n                                       [batch_size, seq_length, width])\n    output += token_type_embeddings\n\n  if use_position_embeddings:\n    full_position_embeddings = tf.get_variable(\n        name=position_embedding_name,\n        shape=[max_position_embeddings, width],\n        initializer=create_initializer(initializer_range))\n    # Since the position embedding table is a learned variable, we create it\n    # using a (long) sequence length `max_position_embeddings`. The actual\n    # sequence length might be shorter than this, for faster training of\n    # tasks that do not have long sequences.\n    #\n    # So `full_position_embeddings` is effectively an embedding table\n    # for position [0, 1, 2, ..., max_position_embeddings-1], and the current\n    # sequence has positions [0, 1, 2, ... seq_length-1], so we can just\n    # perform a slice.\n    if seq_length < max_position_embeddings:\n      position_embeddings = tf.slice(full_position_embeddings, [0, 0],\n                                     [seq_length, -1])\n    else:\n      position_embeddings = full_position_embeddings\n\n    num_dims = len(output.shape.as_list())\n\n    # Only the last two dimensions are relevant (`seq_length` and `width`), so\n    # we broadcast among the first dimensions, which is typically just\n    # the batch size.\n    position_broadcast_shape = []\n    for _ in range(num_dims - 2):\n      position_broadcast_shape.append(1)\n    position_broadcast_shape.extend([seq_length, width])\n    position_embeddings = tf.reshape(position_embeddings,\n                                     position_broadcast_shape)\n    output += position_embeddings\n\n  output = layer_norm_and_dropout(output, dropout_prob)\n  return output\n\n\ndef create_attention_mask_from_input_mask(from_tensor, to_mask):\n  """"""Create 3D attention mask from a 2D tensor mask.\n\n  Args:\n    from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n    to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n  """"""\n  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n  batch_size = from_shape[0]\n  from_seq_length = from_shape[1]\n\n  to_shape = get_shape_list(to_mask, expected_rank=2)\n  to_seq_length = to_shape[1]\n\n  to_mask = tf.cast(\n      tf.reshape(to_mask, [batch_size, 1, to_seq_length]), tf.float32)\n\n  # We don\'t assume that `from_tensor` is a mask (although it could be). We\n  # don\'t actually care if we attend *from* padding tokens (only *to* padding)\n  # tokens so we create a tensor of all ones.\n  #\n  # `broadcast_ones` = [batch_size, from_seq_length, 1]\n  broadcast_ones = tf.ones(\n      shape=[batch_size, from_seq_length, 1], dtype=tf.float32)\n\n  # Here we broadcast along two dimensions to create the mask.\n  mask = broadcast_ones * to_mask\n\n  return mask\n\n\ndef attention_layer(from_tensor,\n                    to_tensor,\n                    attention_mask=None,\n                    num_attention_heads=1,\n                    size_per_head=512,\n                    query_act=None,\n                    key_act=None,\n                    value_act=None,\n                    attention_probs_dropout_prob=0.0,\n                    initializer_range=0.02,\n                    do_return_2d_tensor=False,\n                    batch_size=None,\n                    from_seq_length=None,\n                    to_seq_length=None):\n  """"""Performs multi-headed attention from `from_tensor` to `to_tensor`.\n\n  This is an implementation of multi-headed attention based on ""Attention\n  is all you Need"". If `from_tensor` and `to_tensor` are the same, then\n  this is self-attention. Each timestep in `from_tensor` attends to the\n  corresponding sequence in `to_tensor`, and returns a fixed-with vector.\n\n  This function first projects `from_tensor` into a ""query"" tensor and\n  `to_tensor` into ""key"" and ""value"" tensors. These are (effectively) a list\n  of tensors of length `num_attention_heads`, where each tensor is of shape\n  [batch_size, seq_length, size_per_head].\n\n  Then, the query and key tensors are dot-producted and scaled. These are\n  softmaxed to obtain attention probabilities. The value tensors are then\n  interpolated by these probabilities, then concatenated back to a single\n  tensor and returned.\n\n  In practice, the multi-headed attention are done with transposes and\n  reshapes rather than actual separate tensors.\n\n  Args:\n    from_tensor: float Tensor of shape [batch_size, from_seq_length,\n      from_width].\n    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n    attention_mask: (optional) int32 Tensor of shape [batch_size,\n      from_seq_length, to_seq_length]. The values should be 1 or 0. The\n      attention scores will effectively be set to -infinity for any positions in\n      the mask that are 0, and will be unchanged for positions that are 1.\n    num_attention_heads: int. Number of attention heads.\n    size_per_head: int. Size of each attention head.\n    query_act: (optional) Activation function for the query transform.\n    key_act: (optional) Activation function for the key transform.\n    value_act: (optional) Activation function for the value transform.\n    attention_probs_dropout_prob: (optional) float. Dropout probability of the\n      attention probabilities.\n    initializer_range: float. Range of the weight initializer.\n    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size\n      * from_seq_length, num_attention_heads * size_per_head]. If False, the\n      output will be of shape [batch_size, from_seq_length, num_attention_heads\n      * size_per_head].\n    batch_size: (Optional) int. If the input is 2D, this might be the batch size\n      of the 3D version of the `from_tensor` and `to_tensor`.\n    from_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `from_tensor`.\n    to_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `to_tensor`.\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length,\n      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is\n      true, this will be of shape [batch_size * from_seq_length,\n      num_attention_heads * size_per_head]).\n\n  Raises:\n    ValueError: Any of the arguments or tensor shapes are invalid.\n  """"""\n\n  def transpose_for_scores(input_tensor, batch_size, num_attention_heads,\n                           seq_length, width):\n    output_tensor = tf.reshape(\n        input_tensor, [batch_size, seq_length, num_attention_heads, width])\n\n    output_tensor = tf.transpose(output_tensor, [0, 2, 1, 3])\n    return output_tensor\n\n  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])\n  to_shape = get_shape_list(to_tensor, expected_rank=[2, 3])\n\n  if len(from_shape) != len(to_shape):\n    raise ValueError(\n        ""The rank of `from_tensor` must match the rank of `to_tensor`."")\n\n  if len(from_shape) == 3:\n    batch_size = from_shape[0]\n    from_seq_length = from_shape[1]\n    to_seq_length = to_shape[1]\n  elif len(from_shape) == 2:\n    if (batch_size is None or from_seq_length is None or to_seq_length is None):\n      raise ValueError(\n          ""When passing in rank 2 tensors to attention_layer, the values ""\n          ""for `batch_size`, `from_seq_length`, and `to_seq_length` ""\n          ""must all be specified."")\n\n  # Scalar dimensions referenced here:\n  #   B = batch size (number of sequences)\n  #   F = `from_tensor` sequence length\n  #   T = `to_tensor` sequence length\n  #   N = `num_attention_heads`\n  #   H = `size_per_head`\n\n  from_tensor_2d = reshape_to_matrix(from_tensor)\n  to_tensor_2d = reshape_to_matrix(to_tensor)\n\n  # `query_layer` = [B*F, N*H]\n  query_layer = tf.layers.dense(\n      from_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=query_act,\n      name=""query"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  # `key_layer` = [B*T, N*H]\n  key_layer = tf.layers.dense(\n      to_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=key_act,\n      name=""key"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  # `value_layer` = [B*T, N*H]\n  value_layer = tf.layers.dense(\n      to_tensor_2d,\n      num_attention_heads * size_per_head,\n      activation=value_act,\n      name=""value"",\n      kernel_initializer=create_initializer(initializer_range))\n\n  # `query_layer` = [B, N, F, H]\n  query_layer = transpose_for_scores(query_layer, batch_size,\n                                     num_attention_heads, from_seq_length,\n                                     size_per_head)\n\n  # `key_layer` = [B, N, T, H]\n  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,\n                                   to_seq_length, size_per_head)\n\n  # Take the dot product between ""query"" and ""key"" to get the raw\n  # attention scores.\n  # `attention_scores` = [B, N, F, T]\n  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)\n  attention_scores = tf.multiply(attention_scores,\n                                 1.0 / math.sqrt(float(size_per_head)))\n\n  if attention_mask is not None:\n    # `attention_mask` = [B, 1, F, T]\n    attention_mask = tf.expand_dims(attention_mask, axis=[1])\n\n    # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n    # masked positions, this operation will create a tensor which is 0.0 for\n    # positions we want to attend and -10000.0 for masked positions.\n    adder = (1.0 - tf.cast(attention_mask, tf.float32)) * -10000.0\n\n    # Since we are adding it to the raw scores before the softmax, this is\n    # effectively the same as removing these entirely.\n    attention_scores += adder\n\n  # Normalize the attention scores to probabilities.\n  # `attention_probs` = [B, N, F, T]\n  attention_probs = tf.nn.softmax(attention_scores)\n\n  # This is actually dropping out entire tokens to attend to, which might\n  # seem a bit unusual, but is taken from the original Transformer paper.\n  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)\n\n  # `value_layer` = [B, T, N, H]\n  value_layer = tf.reshape(\n      value_layer,\n      [batch_size, to_seq_length, num_attention_heads, size_per_head])\n\n  # `value_layer` = [B, N, T, H]\n  value_layer = tf.transpose(value_layer, [0, 2, 1, 3])\n\n  # `context_layer` = [B, N, F, H]\n  context_layer = tf.matmul(attention_probs, value_layer)\n\n  # `context_layer` = [B, F, N, H]\n  context_layer = tf.transpose(context_layer, [0, 2, 1, 3])\n\n  if do_return_2d_tensor:\n    # `context_layer` = [B*F, N*V]\n    context_layer = tf.reshape(\n        context_layer,\n        [batch_size * from_seq_length, num_attention_heads * size_per_head])\n  else:\n    # `context_layer` = [B, F, N*V]\n    context_layer = tf.reshape(\n        context_layer,\n        [batch_size, from_seq_length, num_attention_heads * size_per_head])\n\n  return context_layer\n\n\ndef transformer_model(input_tensor,\n                      attention_mask=None,\n                      hidden_size=768,\n                      num_hidden_layers=12,\n                      num_attention_heads=12,\n                      intermediate_size=3072,\n                      intermediate_act_fn=get_activation(\'gelu\'),\n                      hidden_dropout_prob=0.1,\n                      attention_probs_dropout_prob=0.1,\n                      initializer_range=0.02,\n                      do_return_all_layers=False):\n  """"""Multi-headed, multi-layer Transformer from ""Attention is All You Need"".\n\n  This is almost an exact implementation of the original Transformer encoder.\n\n  See the original paper:\n  https://arxiv.org/abs/1706.03762\n\n  Also see:\n  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n      seq_length], with 1 for positions that can be attended to and 0 in\n      positions that should not be.\n    hidden_size: int. Hidden size of the Transformer.\n    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n    num_attention_heads: int. Number of attention heads in the Transformer.\n    intermediate_size: int. The size of the ""intermediate"" (a.k.a., feed\n      forward) layer.\n    intermediate_act_fn: function. The non-linear activation function to apply\n      to the output of the intermediate/feed-forward layer.\n    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n    attention_probs_dropout_prob: float. Dropout probability of the attention\n      probabilities.\n    initializer_range: float. Range of the initializer (stddev of truncated\n      normal).\n    do_return_all_layers: Whether to also return all layers or just the final\n      layer.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n    hidden layer of the Transformer.\n\n  Raises:\n    ValueError: A Tensor shape or parameter is invalid.\n  """"""\n  if hidden_size % num_attention_heads != 0:\n    raise ValueError(\n        ""The hidden size (%d) is not a multiple of the number of attention ""\n        ""heads (%d)"" % (hidden_size, num_attention_heads))\n\n  attention_head_size = int(hidden_size / num_attention_heads)\n  input_shape = get_shape_list(input_tensor, expected_rank=3)\n  batch_size = input_shape[0]\n  seq_length = input_shape[1]\n  input_width = input_shape[2]\n\n  # The Transformer performs sum residuals on all layers so the input needs\n  # to be the same as the hidden size.\n  if input_width != hidden_size:\n    raise ValueError(""The width of the input tensor (%d) != hidden size (%d)"" %\n                     (input_width, hidden_size))\n\n  # We keep the representation as a 2D tensor to avoid re-shaping it back and\n  # forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on\n  # the GPU/CPU but may not be free on the TPU, so we want to minimize them to\n  # help the optimizer.\n  prev_output = reshape_to_matrix(input_tensor)\n\n  all_layer_outputs = []\n  for layer_idx in range(num_hidden_layers):\n    with tf.variable_scope(""layer_%d"" % layer_idx):\n      layer_input = prev_output\n\n      with tf.variable_scope(""attention""):\n        attention_heads = []\n        with tf.variable_scope(""self""):\n          attention_head = attention_layer(\n              from_tensor=layer_input,\n              to_tensor=layer_input,\n              attention_mask=attention_mask,\n              num_attention_heads=num_attention_heads,\n              size_per_head=attention_head_size,\n              attention_probs_dropout_prob=attention_probs_dropout_prob,\n              initializer_range=initializer_range,\n              do_return_2d_tensor=True,\n              batch_size=batch_size,\n              from_seq_length=seq_length,\n              to_seq_length=seq_length)\n          attention_heads.append(attention_head)\n\n        attention_output = None\n        if len(attention_heads) == 1:\n          attention_output = attention_heads[0]\n        else:\n          # In the case where we have other sequences, we just concatenate\n          # them to the self-attention head before the projection.\n          attention_output = tf.concat(attention_heads, axis=-1)\n\n        # Run a linear projection of `hidden_size` then add a residual\n        # with `layer_input`.\n        with tf.variable_scope(""output""):\n          attention_output = tf.layers.dense(\n              attention_output,\n              hidden_size,\n              kernel_initializer=create_initializer(initializer_range))\n          attention_output = dropout(attention_output, hidden_dropout_prob)\n          attention_output = layer_norm(attention_output + layer_input)\n\n      # The activation is only applied to the ""intermediate"" hidden layer.\n      with tf.variable_scope(""intermediate""):\n        intermediate_output = tf.layers.dense(\n            attention_output,\n            intermediate_size,\n            activation=intermediate_act_fn,\n            kernel_initializer=create_initializer(initializer_range))\n\n      # Down-project back to `hidden_size` then add the residual.\n      with tf.variable_scope(""output""):\n        layer_output = tf.layers.dense(\n            intermediate_output,\n            hidden_size,\n            kernel_initializer=create_initializer(initializer_range))\n        layer_output = dropout(layer_output, hidden_dropout_prob)\n        layer_output = layer_norm(layer_output + attention_output)\n        prev_output = layer_output\n        all_layer_outputs.append(layer_output)\n\n  if do_return_all_layers:\n    final_outputs = []\n    for layer_output in all_layer_outputs:\n      final_output = reshape_from_matrix(layer_output, input_shape)\n      final_outputs.append(final_output)\n    return final_outputs\n  else:\n    final_output = reshape_from_matrix(prev_output, input_shape)\n    return final_output\n\n\ndef get_shape_list(tensor, expected_rank=None, name=None):\n  """"""Returns a list of the shape of tensor, preferring static dimensions.\n\n  Args:\n    tensor: A tf.Tensor object to find the shape of.\n    expected_rank: (optional) int. The expected rank of `tensor`. If this is\n      specified and the `tensor` has a different rank, and exception will be\n      thrown.\n    name: Optional name of the tensor for the error message.\n\n  Returns:\n    A list of dimensions of the shape of tensor. All static dimensions will\n    be returned as python integers, and dynamic dimensions will be returned\n    as tf.Tensor scalars.\n  """"""\n  if name is None:\n    name = tensor.name\n\n  if expected_rank is not None:\n    assert_rank(tensor, expected_rank, name)\n\n  shape = tensor.shape.as_list()\n\n  non_static_indexes = []\n  for (index, dim) in enumerate(shape):\n    if dim is None:\n      non_static_indexes.append(index)\n\n  if not non_static_indexes:\n    return shape\n\n  dyn_shape = tf.shape(tensor)\n  for index in non_static_indexes:\n    shape[index] = dyn_shape[index]\n  return shape\n\n\ndef reshape_to_matrix(input_tensor):\n  """"""Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).""""""\n  ndims = input_tensor.shape.ndims\n  if ndims < 2:\n    raise ValueError(""Input tensor must have at least rank 2. Shape = %s"" %\n                     (input_tensor.shape))\n  if ndims == 2:\n    return input_tensor\n\n  width = input_tensor.shape[-1]\n  output_tensor = tf.reshape(input_tensor, [-1, width])\n  return output_tensor\n\n\ndef reshape_from_matrix(output_tensor, orig_shape_list):\n  """"""Reshapes a rank 2 tensor back to its original rank >= 2 tensor.""""""\n  if len(orig_shape_list) == 2:\n    return output_tensor\n\n  output_shape = get_shape_list(output_tensor)\n\n  orig_dims = orig_shape_list[0:-1]\n  width = output_shape[-1]\n\n  return tf.reshape(output_tensor, orig_dims + [width])\n\n\ndef assert_rank(tensor, expected_rank, name=None):\n  """"""Raises an exception if the tensor rank is not of the expected rank.\n\n  Args:\n    tensor: A tf.Tensor to check the rank of.\n    expected_rank: Python integer or list of integers, expected rank.\n    name: Optional name of the tensor for the error message.\n\n  Raises:\n    ValueError: If the expected shape doesn\'t match the actual shape.\n  """"""\n  if name is None:\n    name = tensor.name\n\n  expected_rank_dict = {}\n  if isinstance(expected_rank, six.integer_types):\n    expected_rank_dict[expected_rank] = True\n  else:\n    for x in expected_rank:\n      expected_rank_dict[x] = True\n\n  actual_rank = tensor.shape.ndims\n  if actual_rank not in expected_rank_dict:\n    scope_name = tf.get_variable_scope().name\n    raise ValueError(\n        ""For the tensor `%s` in scope `%s`, the actual rank ""\n        ""`%d` (shape = %s) is not equal to the expected rank `%s`"" %\n        (name, scope_name, actual_rank, str(tensor.shape), str(expected_rank)))'"
