file_path,api_count,code
py/Beam.py,0,"b'from __future__ import division\nfrom __future__ import print_function\n\nimport copy\n\n\nclass Optical:\n    ""optical score of beam""\n\n    def __init__(self, prBlank=0, prNonBlank=0):\n        self.prBlank = prBlank  # prob of ending with a blank\n        self.prNonBlank = prNonBlank  # prob of ending with a non-blank\n\n\nclass Textual:\n    ""textual score of beam""\n\n    def __init__(self, text=\'\'):\n        self.text = text\n        self.wordHist = []  # history of words so far\n        self.wordDev = \'\'  # developing word\n        self.prUnnormalized = 1.0\n        self.prTotal = 1.0\n\n\nclass Beam:\n    ""beam with text, optical and textual score""\n\n    def __init__(self, lm, useNGrams):\n        ""creates genesis beam""\n        self.optical = Optical(1.0, 0.0)\n        self.textual = Textual(\'\')\n        self.lm = lm\n        self.useNGrams = useNGrams\n\n    def mergeBeam(self, beam):\n        ""merge probabilities of two beams with same text""\n\n        if self.getText() != beam.getText():\n            raise Exception(\'mergeBeam: texts differ\')\n\n        self.optical.prNonBlank += beam.getPrNonBlank()\n        self.optical.prBlank += beam.getPrBlank()\n\n    def getText(self):\n        return self.textual.text\n\n    def getPrBlank(self):\n        return self.optical.prBlank\n\n    def getPrNonBlank(self):\n        return self.optical.prNonBlank\n\n    def getPrTotal(self):\n        return self.getPrBlank() + self.getPrNonBlank()\n\n    def getPrTextual(self):\n        return self.textual.prTotal\n\n    def getNextChars(self):\n        return self.lm.getNextChars(self.textual.wordDev)\n\n    def createChildBeam(self, newChar, prBlank, prNonBlank):\n        ""extend beam by new character and set optical score""\n        beam = Beam(self.lm, self.useNGrams)\n\n        # copy textual information\n        beam.textual = copy.deepcopy(self.textual)\n        beam.textual.text += newChar\n\n        # do textual calculations only if beam gets extended\n        if newChar != \'\':\n            if self.useNGrams:  # use unigrams and bigrams\n\n                # if new char occurs inside a word\n                if newChar in beam.lm.getWordChars():\n                    beam.textual.wordDev += newChar\n                    nextWords = beam.lm.getNextWords(beam.textual.wordDev)\n\n                    # no complete word in text, then use unigram of all possible next words\n                    numWords = len(beam.textual.wordHist)\n                    prSum = 0\n                    if numWords == 0:\n                        for w in nextWords:\n                            prSum += beam.lm.getUnigramProb(w)\n                    # take last complete word and sum up bigrams of all possible next words\n                    else:\n                        lastWord = beam.textual.wordHist[-1]\n                        for w in nextWords:\n                            prSum += beam.lm.getBigramProb(lastWord, w)\n                    beam.textual.prTotal = beam.textual.prUnnormalized * prSum\n                    beam.textual.prTotal = beam.textual.prTotal ** (\n                            1 / (numWords + 1)) if numWords >= 1 else beam.textual.prTotal\n\n                # if new char does not occur inside a word\n                else:\n                    # if current word is not empty, add it to history\n                    if beam.textual.wordDev != \'\':\n                        beam.textual.wordHist.append(beam.textual.wordDev)\n                        beam.textual.wordDev = \'\'\n\n                        # score with unigram (first word) or bigram (all other words) probability\n                        numWords = len(beam.textual.wordHist)\n                        if numWords == 1:\n                            beam.textual.prUnnormalized *= beam.lm.getUnigramProb(beam.textual.wordHist[-1])\n                            beam.textual.prTotal = beam.textual.prUnnormalized\n                        elif numWords >= 2:\n                            beam.textual.prUnnormalized *= beam.lm.getBigramProb(beam.textual.wordHist[-2],\n                                                                                 beam.textual.wordHist[-1])\n                            beam.textual.prTotal = beam.textual.prUnnormalized ** (1 / numWords)\n\n            else:  # don\'t use unigrams and bigrams, just keep wordDev up to date\n                if newChar in beam.lm.getWordChars():\n                    beam.textual.wordDev += newChar\n                else:\n                    beam.textual.wordDev = \'\'\n\n        # set optical information\n        beam.optical.prBlank = prBlank\n        beam.optical.prNonBlank = prNonBlank\n        return beam\n\n    def __str__(self):\n        return \'""\' + self.getText() + \'""\' + \';\' + str(self.getPrTotal()) + \';\' + str(self.getPrTextual()) + \';\' + str(\n            self.textual.prUnnormalized)\n\n\nclass BeamList:\n    ""list of beams at specific time-step""\n\n    def __init__(self):\n        self.beams = {}\n\n    def addBeam(self, beam):\n        ""add or merge new beam into list""\n        # add if text not yet known\n        if beam.getText() not in self.beams:\n            self.beams[beam.getText()] = beam\n        # otherwise merge with existing beam\n        else:\n            self.beams[beam.getText()].mergeBeam(beam)\n\n    def getBestBeams(self, num):\n        ""return best beams, specify the max. number of beams to be returned (beam width)""\n        u = [v for (_, v) in self.beams.items()]\n        lmWeight = 1\n        return sorted(u, reverse=True, key=lambda x: x.getPrTotal() * (x.getPrTextual() ** lmWeight))[:num]\n\n    def deletePartialBeams(self, lm):\n        ""delete beams for which last word is not finished""\n        for (k, v) in self.beams.items():\n            lastWord = v.textual.wordDev\n            if (lastWord != \'\') and (not lm.isWord(lastWord)):\n                del self.beams[k]\n\n    def completeBeams(self, lm):\n        ""complete beams such that last word is complete word""\n        for (_, v) in self.beams.items():\n            lastPrefix = v.textual.wordDev\n            if lastPrefix == \'\' or lm.isWord(lastPrefix):\n                continue\n\n            # get word candidates for this prefix\n            words = lm.getNextWords(lastPrefix)\n            # if there is just one candidate, then the last prefix can be extended to\n            if len(words) == 1:\n                word = words[0]\n                v.textual.text += word[len(lastPrefix) - len(word):]\n\n    def dump(self):\n        for k in self.beams.keys():\n            print(unicode(self.beams[k]).encode(\'ascii\', \'replace\'))  # map to ascii if possible (for py2 and windows)\n'"
py/DataLoader.py,0,"b'from __future__ import division\nfrom __future__ import print_function\n\nimport codecs\nimport os.path\n\nimport numpy as np\n\nfrom LanguageModel import LanguageModel\n\n\ndef softmax(mat):\n    ""calc softmax such that labels per time-step form probability distribution""\n    # dim0=t, dim1=c\n    maxT, _ = mat.shape\n    res = np.zeros(mat.shape)\n    for t in range(maxT):\n        y = mat[t, :]\n        maxValue = np.max(y)\n        e = np.exp(y - maxValue)\n        s = np.sum(e)\n        res[t, :] = e / s\n\n    return res\n\n\ndef loadFromCSV(fn):\n    ""load matrix from csv file. Last entry in row terminated by semicolon.""\n    mat = np.genfromtxt(fn, delimiter=\';\')[:, :-1]\n    mat = softmax(mat)\n    return mat\n\n\nclass Data:\n    ""holds matrix, ground truth and filenames of a sample""\n\n    def __init__(self):\n        self.gt = \'\'\n        self.mat = None\n        self.fn = \'\'\n\n\nclass DataLoader:\n    ""load data from a given directory""\n\n    def __init__(self, dataset, sampleEach=1):\n        self.path = \'../data/\' + dataset + \'/\'\n        self.chars = codecs.open(self.path + \'chars.txt\', \'r\', \'utf8\').read()\n        self.wordChars = codecs.open(self.path + \'wordChars.txt\', \'r\', \'utf8\').read()\n        self.lm = LanguageModel(codecs.open(self.path + \'corpus.txt\', \'r\', \'utf8\').read(), self.chars, self.wordChars)\n        self.mats = []\n        self.gts = []\n        self.fns = []\n\n        i = 0\n        while True:\n            fnMat = self.path + \'mat_\' + str(i) + \'.csv\'\n            fnGT = self.path + \'gt_\' + str(i) + \'.txt\'\n            i += 1\n\n            # file not found\n            if (not os.path.isfile(fnMat)) or (not os.path.isfile(fnGT)):\n                break\n\n            # ignore this sample\n            if (i - 1) % sampleEach != 0:\n                continue\n\n            # put into result\n            self.mats.append(fnMat)\n            self.gts.append(fnGT)\n            self.fns.append(fnMat + \'|\' + fnGT)\n\n        self.currIdx = 0\n\n    def getNumSamples(self):\n        return len(self.mats)\n\n    def __next__(self):\n        if self.currIdx >= len(self.mats):\n            raise StopIteration()\n\n        data = Data()\n        data.mat = loadFromCSV(self.mats[self.currIdx])\n        data.gt = codecs.open(self.gts[self.currIdx], \'r\', \'utf8\').read()\n        data.fn = self.fns[self.currIdx]\n\n        self.currIdx += 1\n        return data\n\n    # python2 needs next, not __next__\n    next = __next__\n\n    def __iter__(self):\n        return self\n'"
py/LanguageModel.py,0,"b'from __future__ import division\nfrom __future__ import print_function\n\nimport re\n\nfrom PrefixTree import PrefixTree\n\n\nclass LanguageModel:\n    ""unigram/bigram LM, add-k smoothing""\n\n    def __init__(self, corpus, chars, wordChars):\n        ""read text from filename, specify chars which are contained in dataset, specify chars which form words""\n        # read from file\n        self.wordCharPattern = \'[\' + wordChars + \']\'\n        self.wordPattern = self.wordCharPattern + \'+\'\n        words = re.findall(self.wordPattern, corpus)\n        uniqueWords = list(set(words))  # make unique\n        self.numWords = len(words)\n        self.numUniqueWords = len(uniqueWords)\n        self.smoothing = True\n        self.addK = 1.0 if self.smoothing else 0.0\n\n        # create unigrams\n        self.unigrams = {}\n        for w in words:\n            w = w.lower()\n            if w not in self.unigrams:\n                self.unigrams[w] = 0\n            self.unigrams[w] += 1 / self.numWords\n\n        # create unnormalized bigrams\n        bigrams = {}\n        for i in range(len(words) - 1):\n            w1 = words[i].lower()\n            w2 = words[i + 1].lower()\n            if w1 not in bigrams:\n                bigrams[w1] = {}\n            if w2 not in bigrams[w1]:\n                bigrams[w1][w2] = self.addK  # add-K\n            bigrams[w1][w2] += 1\n\n        # normalize bigrams\n        for w1 in bigrams.keys():\n            # sum up\n            probSum = self.numUniqueWords * self.addK  # add-K smoothing\n            for w2 in bigrams[w1].keys():\n                probSum += bigrams[w1][w2]\n            # and divide\n            for w2 in bigrams[w1].keys():\n                bigrams[w1][w2] /= probSum\n        self.bigrams = bigrams\n\n        # create prefix tree\n        self.tree = PrefixTree()  # create empty tree\n        self.tree.addWords(uniqueWords)  # add all unique words to tree\n\n        # list of all chars, word chars and nonword chars\n        self.allChars = chars\n        self.wordChars = wordChars\n        self.nonWordChars = str().join(\n            set(chars) - set(re.findall(self.wordCharPattern, chars)))  # else calculate those chars\n\n    def getNextWords(self, text):\n        ""text must be prefix of a word""\n        return self.tree.getNextWords(text)\n\n    def getNextChars(self, text):\n        ""text must be prefix of a word""\n        nextChars = str().join(self.tree.getNextChars(text))\n\n        # if in between two words or if word ends, add non-word chars\n        if (text == \'\') or (self.isWord(text)):\n            nextChars += self.getNonWordChars()\n\n        return nextChars\n\n    def getWordChars(self):\n        return self.wordChars\n\n    def getNonWordChars(self):\n        return self.nonWordChars\n\n    def getAllChars(self):\n        return self.allChars\n\n    def isWord(self, text):\n        return self.tree.isWord(text)\n\n    def getUnigramProb(self, w):\n        ""prob of seeing word w.""\n        w = w.lower()\n        val = self.unigrams.get(w)\n        if val != None:\n            return val\n        return 0\n\n    def getBigramProb(self, w1, w2):\n        ""prob of seeing words w1 w2 next to each other.""\n        w1 = w1.lower()\n        w2 = w2.lower()\n        val1 = self.bigrams.get(w1)\n        if val1 != None:\n            val2 = val1.get(w2)\n            if val2 != None:\n                return val2\n            return self.addK / (self.getUnigramProb(w1) * self.numUniqueWords + self.numUniqueWords)\n        return 0\n\n\nif __name__ == \'__main__\':\n    lm = LanguageModel(\'12 1 13 12 15 234 2526\', \' ,.:0123456789\', \'0123456789\')\n    prefix = \'1\'\n    print(\'getNextChars:\', lm.getNextChars(prefix))\n    print(\'getNonWordChars:\', lm.getNonWordChars())\n    print(\'getNextWords:\', lm.getNextWords(prefix))\n    print(\'isWord:\', lm.isWord(prefix))\n    print(\'getBigramProb:\', lm.getBigramProb(\'12\', \'15\'))\n'"
py/Metrics.py,0,"b'from __future__ import division\nfrom __future__ import print_function\n\nimport re\n\nimport editdistance\n\n\nclass Metrics:\n    ""CER and WER""\n\n    def __init__(self, wordChars=r\'\\w\'):\n        self.numWords = 0\n        self.numChars = 0\n\n        self.edWords = 0\n        self.edChars = 0\n\n        self.pattern = \'[\' + wordChars + \']+\'\n\n    def getWordIDStrings(self, s1, s2):\n        # get words in string 1 and string 2\n        words1 = re.findall(self.pattern, s1)\n        words2 = re.findall(self.pattern, s2)\n\n        # find unique words\n        allWords = list(set(words1 + words2))\n\n        # list of word ids for string 1\n        idStr1 = []\n        for w in words1:\n            idStr1.append(allWords.index(w))\n\n        # list of word ids for string 2\n        idStr2 = []\n        for w in words2:\n            idStr2.append(allWords.index(w))\n\n        return (idStr1, idStr2)\n\n    def addSample(self, gt, rec):\n        ""insert result and ground truth for next sample""\n        # chars\n        self.edChars += editdistance.eval(gt, rec)\n        self.numChars += len(gt)\n\n        # words\n        (idStrGt, idStrRec) = self.getWordIDStrings(gt, rec)\n        self.edWords += editdistance.eval(idStrGt, idStrRec)\n        self.numWords += len(idStrGt)\n\n    def getCER(self):\n        ""get character error rate""\n        return self.edChars / self.numChars\n\n    def getWER(self):\n        ""get word error rate""\n        return self.edWords / self.numWords\n\n\nif __name__ == \'__main__\':\n    m = Metrics()\n    m.addSample(\'hxllo world\', \'hello world\')\n    m.addSample(\'yes we cxn\', \'yes we can\')\n    print(\'CER:\', m.getCER())\n    print(\'WER:\', m.getWER())\n'"
py/PrefixTree.py,0,"b'from __future__ import division\nfrom __future__ import print_function\n\n\nclass Node:\n    ""class representing nodes in a prefix tree""\n\n    def __init__(self):\n        self.children = {}  # all child elements beginning with current prefix\n        self.isWord = False  # does this prefix represent a word\n\n    def __str__(self):\n        s = \'\'\n        for k in self.children.keys():\n            s += k\n        return \'isWord: \' + str(self.isWord) + \'; children: \' + s\n\n\nclass PrefixTree:\n    ""prefix tree""\n\n    def __init__(self):\n        self.root = Node()\n\n    def addWord(self, text):\n        ""add word to prefix tree""\n        node = self.root\n        for i in range(len(text)):\n            c = text[i]  # current char\n            if c not in node.children:\n                node.children[c] = Node()\n            node = node.children[c]\n            isLast = (i + 1 == len(text))\n            if isLast:\n                node.isWord = True\n\n    def addWords(self, words):\n        for w in words:\n            self.addWord(w)\n\n    def getNode(self, text):\n        ""get node representing given text""\n        node = self.root\n        for c in text:\n            if c in node.children:\n                node = node.children[c]\n            else:\n                return None\n        return node\n\n    def isWord(self, text):\n        node = self.getNode(text)\n        if node:\n            return node.isWord\n        return False\n\n    def getNextChars(self, text):\n        ""get all characters which may directly follow given text""\n        chars = []\n        node = self.getNode(text)\n        if node:\n            for k in node.children.keys():\n                chars.append(k)\n        return chars\n\n    def getNextWords(self, text):\n        ""get all words of which given text is a prefix (including the text itself, it is a word)""\n        words = []\n        node = self.getNode(text)\n        if node:\n            nodes = [node]\n            prefixes = [text]\n            while len(nodes) > 0:\n                # put all children into list\n                for k, v in nodes[0].children.items():\n                    nodes.append(v)\n                    prefixes.append(prefixes[0] + k)\n\n                # is current node a word\n                if nodes[0].isWord:\n                    words.append(prefixes[0])\n\n                # remove current node\n                del nodes[0]\n                del prefixes[0]\n\n        return words\n\n    def dump(self):\n        nodes = [self.root]\n        while len(nodes) > 0:\n            # put all children into list\n            for v in nodes[0].children.values():\n                nodes.append(v)\n\n            # dump current node\n            print(nodes[0])\n\n            # remove from list\n            del nodes[0]\n\n\nif __name__ == \'__main__\':\n    t = PrefixTree()  # create tree\n    t.addWords([\'this\', \'that\'])  # add words\n    print(t.getNextChars(\'th\'))  # chars following \'th\'\n    print(t.getNextWords(\'tha\'))  # all words of which \'th\' is prefix\n    t.dump()  # dump all nodes\n'"
py/Utils.py,0,"b'from __future__ import division\nfrom __future__ import print_function\n\nimport csv\nimport sys\n\n\ndef redirectToFile():\n    sys.stdout = open(\'out.txt\', \'w+\')\n\n\ndef flushToFile():\n    sys.stdout.flush()\n\n\nclass CSVWriter:\n    ""log to csv file""\n\n    def __init__(self):\n        self.file = open(\'out.csv\', \'w+\')\n        self.writer = csv.writer(self.file, lineterminator=\'\\n\')\n\n    def write(self, line):\n        line = [x.encode(\'ascii\', \'replace\') for x in line]  # map to ascii if possible (for py2 and windows)\n        self.writer.writerow(line)\n        self.file.flush()\n'"
py/WordBeamSearch.py,0,"b'from __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom Beam import Beam, BeamList\nfrom LanguageModel import LanguageModel\n\n\ndef wordBeamSearch(mat, beamWidth, lm, useNGrams):\n    ""decode matrix, use given beam width and language model""\n    chars = lm.getAllChars()\n    blankIdx = len(chars)  # blank label is supposed to be last label in RNN output\n    maxT, _ = mat.shape  # shape of RNN output: TxC\n\n    genesisBeam = Beam(lm, useNGrams)  # empty string\n    last = BeamList()  # list of beams at time-step before beginning of RNN output\n    last.addBeam(genesisBeam)  # start with genesis beam\n\n    # go over all time-steps\n    for t in range(maxT):\n        curr = BeamList()  # list of beams at current time-step\n\n        # go over best beams\n        bestBeams = last.getBestBeams(beamWidth)  # get best beams\n        for beam in bestBeams:\n            # calc probability that beam ends with non-blank\n            prNonBlank = 0\n            if beam.getText() != \'\':\n                # char at time-step t must also occur at t-1\n                labelIdx = chars.index(beam.getText()[-1])\n                prNonBlank = beam.getPrNonBlank() * mat[t, labelIdx]\n\n            # calc probability that beam ends with blank\n            prBlank = beam.getPrTotal() * mat[t, blankIdx]\n\n            # save result\n            curr.addBeam(beam.createChildBeam(\'\', prBlank, prNonBlank))\n\n            # extend current beam with characters according to language model\n            nextChars = beam.getNextChars()\n            for c in nextChars:\n                # extend current beam with new character\n                labelIdx = chars.index(c)\n                if beam.getText() != \'\' and beam.getText()[-1] == c:\n                    prNonBlank = mat[t, labelIdx] * beam.getPrBlank()  # same chars must be separated by blank\n                else:\n                    prNonBlank = mat[t, labelIdx] * beam.getPrTotal()  # different chars can be neighbours\n\n                # save result\n                curr.addBeam(beam.createChildBeam(c, 0, prNonBlank))\n\n        # move current beams to next time-step\n        last = curr\n\n    # return most probable beam\n    last.completeBeams(lm)\n    bestBeams = last.getBestBeams(1)  # sort by probability\n    return bestBeams[0].getText()\n\n\nif __name__ == \'__main__\':\n    testLM = LanguageModel(\'a b aa ab ba bb\', \'ab \', \'ab\')\n    testMat = np.array([[0.3, 0.1, 0, 0.6], [0.3, 0.1, 0, 0.6]])\n    testBW = 25\n    res = wordBeamSearch(testMat, testBW, testLM, False)\n    print(\'Result: ""\' + res + \'""\')\n'"
py/main.py,0,"b'from __future__ import division\nfrom __future__ import print_function\n\nimport editdistance\n\nimport Utils\nfrom DataLoader import DataLoader\nfrom Metrics import Metrics\nfrom WordBeamSearch import wordBeamSearch\n\n# Settings\nsampleEach = 1\ndataset = \'bentham\'\nuseNGrams = True\n\n# main\nif __name__ == \'__main__\':\n\n    # load dataset\n    loader = DataLoader(dataset, sampleEach)\n    print(\'Decoding \' + str(loader.getNumSamples()) + \' samples now.\')\n    print(\'\')\n\n    # metrics calculates CER and WER for dataset\n    m = Metrics(loader.lm.getWordChars())\n\n    # write results to csv\n    csv = Utils.CSVWriter()\n\n    # decode each sample from dataset\n    for (idx, data) in enumerate(loader):\n        # decode matrix\n        res = wordBeamSearch(data.mat, 10, loader.lm, useNGrams)\n        print(\'Sample: \' + str(idx + 1))\n        print(\'Filenames: \' + data.fn)\n        print(\'Result:       ""\' + res + \'""\')\n        print(\'Ground Truth: ""\' + data.gt + \'""\')\n        strEditDist = str(editdistance.eval(res, data.gt))\n        print(\'Editdistance: \' + strEditDist)\n\n        # output CER and WER\n        m.addSample(data.gt, res)\n        print(\'Accumulated CER and WER so far:\', \'CER:\', m.getCER(), \'WER:\', m.getWER())\n        print(\'\')\n\n        # output to csv\n        csv.write([res, data.gt, strEditDist])\n'"
tf/testCustomOp.py,4,"b'from __future__ import print_function\nfrom __future__ import division\nimport numpy as np\nimport tensorflow as tf\nimport codecs\n\n\ndef testCustomOp(feedMat, corpus, chars, wordChars):\n\t""decode using word beam search. Result is tuple, first entry is label string, second entry is char string.""\n\n\t# TF session\n\tsess = tf.Session()\n\tsess.run(tf.global_variables_initializer())\n\n\t# load custom TF op\n\tword_beam_search_module = tf.load_op_library(\'../cpp/proj/TFWordBeamSearch.so\')\n\n\t# input with shape TxBxC\n\tmat=tf.placeholder(tf.float32, shape=feedMat.shape)\n\n\t# decode using the ""Words"" mode of word beam search with beam width set to 25 and add-k smoothing to 0.0\n\tassert len(chars) + 1 == mat.shape[2]\n\tdecode = word_beam_search_module.word_beam_search(mat, 25, \'Words\', 0.0, corpus.encode(\'utf8\'), chars.encode(\'utf8\'), wordChars.encode(\'utf8\'))\n\n\t# feed matrix of shape TxBxC and evaluate TF graph\n\tres = sess.run(decode, { mat:feedMat })\n\n\t# result is string of labels terminated by blank (similar to C-strings) if shorter than T\n\tblank = len(chars)\n\ts = \'\'\n\tfor label in res[0]:\n\t\tif label == blank:\n\t\t\tbreak\n\t\ts += chars[label] # map label to char\n\treturn (res[0], s)\n\n\ndef loadMat(fn):\n\t""load matrix from csv and apply softmax""\n\n\tmat = np.genfromtxt(fn, delimiter=\';\')[:,:-1] #load matrix from file\n\tmaxT, _ = mat.shape # dim0=t, dim1=c\n\t\n\t# apply softmax\n\tres = np.zeros(mat.shape)\n\tfor t in range(maxT):\n\t\ty = mat[t, :]\n\t\te = np.exp(y)\n\t\ts = np.sum(e)\n\t\tres[t, :] = e / s\n\n\t# expand to TxBxC\n\treturn np.expand_dims(res, 1)\n\n\ndef testMiniExample():\n\t""mini example, just to check that everything is working""\n\tcorpus = \'a ba\' # two words ""a"" and ""ba"", separated by whitespace\n\tchars = \'ab \' # the first three characters which occur in the matrix (in this ordering)\n\twordChars = \'ab\' # whitespace not included which serves as word-separating character\n\tmat = np.array([[[0.9, 0.1, 0.0, 0.0]],[[0.0, 0.0, 0.0, 1.0]],[[0.6, 0.4, 0.0, 0.0]]]) # 3 time-steps and 4 characters per time time (""a"", ""b"", "" "", blank)\n\n\tres = testCustomOp(mat, corpus, chars, wordChars)\t\n\tprint(\'\')\n\tprint(\'Mini example:\')\n\tprint(\'Label string:\', res[0])\n\tprint(\'Char string:\', \'""\' + res[1] + \'""\')\n\tassert res[1] == \'ba\'\n\n\ndef testRealExample():\n\t""real example using a sample from a HTR dataset""\n\tdataPath = \'../data/bentham/\'\n\tcorpus = codecs.open(dataPath + \'corpus.txt\', \'r\', \'utf8\').read()\n\tchars = codecs.open(dataPath + \'chars.txt\', \'r\', \'utf8\').read()\n\twordChars = codecs.open(dataPath + \'wordChars.txt\', \'r\', \'utf8\').read()\n\tmat = loadMat(dataPath + \'mat_2.csv\')\n\n\tres = testCustomOp(mat, corpus, chars, wordChars)\n\tprint(\'\')\n\tprint(\'Real example:\')\n\tprint(\'Label string:\', res[0])\n\tprint(\'Char string:\', \'""\' + res[1] + \'""\')\n\tassert res[1] == \'submitt both mental and corporeal, is far beyond any idea\'\n\n\nif __name__==\'__main__\':\n\t# test custom op\n\ttestMiniExample()\n\ttestRealExample()\n\n'"
