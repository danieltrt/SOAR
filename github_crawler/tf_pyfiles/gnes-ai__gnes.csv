file_path,api_count,code
setup.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nfrom os import path\n\nfrom setuptools import setup, find_packages\nfrom setuptools.extension import Extension\n\ntry:\n    pkg_name = \'gnes\'\n    libinfo_py = path.join(pkg_name, \'__init__.py\')\n    libinfo_content = open(libinfo_py, \'r\', encoding=\'utf8\').readlines()\n    version_line = [l.strip() for l in libinfo_content if l.startswith(\'__version__\')][0]\n    exec(version_line)  # produce __version__\nexcept FileNotFoundError:\n    __version__ = \'0.0.0\'\n\ntry:\n    with open(\'README.md\', encoding=\'utf8\') as fp:\n        _long_description = fp.read()\nexcept FileNotFoundError:\n    _long_description = \'\'\n\nextensions = [\n    Extension(\n        \'gnes.indexer.chunk.bindexer.cython\',\n        [\'gnes/indexer/chunk/bindexer/bindexer.pyx\'],\n        extra_compile_args=[\'-O3\', \'-g0\'],\n    ),\n    Extension(\n        \'gnes.indexer.chunk.hbindexer.cython\',\n        [\'gnes/indexer/chunk/hbindexer/hbindexer.pyx\'],\n        extra_compile_args=[\'-O3\', \'-g0\'],\n    ),\n]\n\nbase_dep = [\n    \'numpy\',\n    \'termcolor\',\n    \'protobuf\',\n    \'grpcio\',\n    \'ruamel.yaml>=0.15.89\',\n    \'pyzmq>=17.1.0\']\n\n# using pip install gnes[xx] is depreciated\n# extras_dep is kept for legacy issue, will be removed soon\n\nextras_dep = {\n    \'bert\': [\'bert-serving-server>=1.8.6\', \'bert-serving-client>=1.8.6\'],\n    # \'elmo\': [\n    #     \'elmoformanylangs @ git+https://github.com/HIT-SCIR/ELMoForManyLangs.git@master#egg=elmoformanylangs-0.0.2\',\n    #     \'paramiko\', \'pattern3\'],\n    \'flair\': [\'flair>=0.4.1\'],\n    \'annoy\': [\'annoy==1.15.2\'],\n    \'chinese\': [\'jieba\'],\n    \'vision\': [\'opencv-python>=4.0.0\', \'imagehash>=4.0\', \'image\', \'peakutils\'],\n    \'leveldb\': [\'plyvel>=1.0.5\'],\n    \'test\': [\'pylint\', \'memory_profiler>=0.55.0\', \'psutil>=5.6.1\', \'gputil>=1.4.0\'],\n    \'transformers\': [\'pytorch-transformers\'],\n    \'onnx\': [\'onnxruntime\'],\n    \'audio\': [\'librosa>=0.7.0\'],\n    \'scipy\': [\'scipy\', \'sklearn\'],\n    \'flask\': [\'flask\'],\n    \'aiohttp\': [\'aiohttp\'],\n    \'http\': [\'flask\', \'aiohttp\']\n}\n\n\ndef combine_dep(new_key, base_keys):\n    extras_dep[new_key] = list(set(k for v in base_keys for k in extras_dep[v]))\n\n\ncombine_dep(\'nlp\', [\'bert\', \'flair\', \'transformers\'])\ncombine_dep(\'cn_nlp\', [\'chinese\', \'nlp\'])\ncombine_dep(\'all\', [k for k in extras_dep if k != \'elmo\'])\n\nsetup(\n    name=pkg_name,\n    packages=find_packages(),\n    version=__version__,\n    include_package_data=True,\n    description=\'GNES is Generic Neural Elastic Search,\'\n                \' a cloud-native semantic search system based on deep neural network.\',\n    author=\'GNES team\',\n    author_email=\'team@gnes.ai\',\n    license=\'Apache 2.0\',\n    url=\'https://gnes.ai\',\n    download_url=\'https://github.com/gnes-ai/gnes/tags\',\n    long_description=_long_description,\n    long_description_content_type=\'text/markdown\',\n    zip_safe=False,\n    setup_requires=[\n        \'setuptools>=18.0\',\n        \'cython\',\n    ],\n    ext_modules=extensions,\n    install_requires=base_dep,\n    extras_require=extras_dep,\n    entry_points={\n        \'console_scripts\': [\'gnes=gnes.cli:main\'],\n    },\n    classifiers=(\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Education\',\n        \'Intended Audience :: Science/Research\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'Programming Language :: Cython\',\n        \'Programming Language :: Unix Shell\',\n        \'Environment :: Console\',\n        \'License :: OSI Approved :: Apache Software License\',\n        \'Operating System :: OS Independent\',\n        \'Topic :: Database :: Database Engines/Servers\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        \'Topic :: Internet :: WWW/HTTP :: Indexing/Search\',\n        \'Topic :: Scientific/Engineering :: Image Recognition\',\n        \'Topic :: Multimedia :: Video\',\n        \'Topic :: Scientific/Engineering\',\n        \'Topic :: Scientific/Engineering :: Mathematics\',\n        \'Topic :: Software Development\',\n        \'Topic :: Software Development :: Libraries\',\n        \'Topic :: Software Development :: Libraries :: Python Modules\',\n    ),\n    keywords=\'gnes cloud-native semantic search elastic neural-network encoding embedding serving\',\n)\n'"
docs/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nfrom os import path\n\nsys.path.insert(0, os.path.abspath(\'..\'))\n\n# -- Project information -----------------------------------------------------\n\nproject = \'GNES Documentation\'\ncopyright = \'2019. Han Xiao, Jianfeng Yan, Feng Wang, Jie Fu\'\nauthor = \'Han Xiao (https://hanxiao.github.io)\'\n\n\ntry:\n    pkg_name = \'gnes\'\n    libinfo_py = path.join(\'..\', pkg_name, \'__init__.py\')\n    libinfo_content = open(libinfo_py, \'r\').readlines()\n    version_line = [l.strip() for l in libinfo_content if l.startswith(\'__version__\')][0]\n    exec(version_line)  # produce __version__\nexcept FileNotFoundError:\n    __version__ = \'0.0.0\'\n\n\n# The short X.Y version\nversion = __version__\n# The full version, including alpha/beta/rc tags\nrelease = __version__\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx_autodoc_typehints\',\n    \'sphinx.ext.viewcode\',\n    \'sphinxcontrib.apidoc\',\n    \'sphinxarg.ext\',\n    \'recommonmark\',\n]\n\n\napidoc_module_dir = \'../gnes\'\napidoc_output_dir = \'api\'\napidoc_excluded_paths = [\'tests\']\napidoc_separate_modules = True\n\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\nsource_suffix = [\'.rst\', \'.md\']\n# source_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\nhtml_logo = \'../.github/gnes-logo-square-blue-on-white-tight.svg\'\n\nhtml_css_files = [\'main.css\']\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'GNESdoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'GNES.tex\', \'GNES Documentation\',\n     \'Han Xiao, Feng Wang, Jianfeng Yan, Jie Fu\', \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'gnes\', \'GNES Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'GNES\', \'GNES Documentation\',\n     author, \'GNES\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n# -- Options for Epub output -------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [\'search.html\']\n\n\n# -- Extension configuration -------------------------------------------------\n\nautoclass_content = \'both\''"
docs/make-req-table.py,0,"b""extras_dep = {\n    'bert': ['bert-serving-server>=1.8.6', 'bert-serving-client>=1.8.6'],\n    # 'elmo': [\n    #     'elmoformanylangs @ git+https://github.com/HIT-SCIR/ELMoForManyLangs.git@master#egg=elmoformanylangs-0.0.2',\n    #     'paramiko', 'pattern3'],\n    'flair': ['flair>=0.4.1'],\n    'annoy': ['annoy==1.15.2'],\n    'chinese': ['jieba'],\n    'vision': ['opencv-python>=4.0.0', 'imagehash>=4.0'],\n    'leveldb': ['plyvel>=1.0.5'],\n    'test': ['pylint', 'memory_profiler>=0.55.0', 'psutil>=5.6.1', 'gputil>=1.4.0'],\n    'transformers': ['pytorch-transformers'],\n    'onnx': ['onnxruntime'],\n    'audio': ['librosa>=0.7.0'],\n    'scipy': ['scipy']\n}\n\n\ndef combine_dep(new_key, base_keys):\n    extras_dep[new_key] = list(set(k for v in base_keys for k in extras_dep[v]))\n\n\ncombine_dep('nlp', ['bert', 'flair', 'transformers'])\ncombine_dep('cn_nlp', ['chinese', 'nlp'])\ncombine_dep('all', [k for k in extras_dep if k != 'elmo'])\n\nfor k, v in extras_dep.items():\n    print('<tr><td><pre>%s</pre></td><td>%s</td>' % ('pip install gnes[%s]' % k, ', '.join(v)))\n"""
gnes/__init__.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\n# do not change this line manually\n# this is managed by git tag and updated on every release\n__version__ = \'0.0.46\'\n\n# do not change this line manually\n# this is managed by shell/make-proto.sh and updated on every execution\n__proto_version__ = \'0.0.10\'\n'"
gnes/component.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nfrom .encoder import base as encoder_base\nfrom .indexer import base as indexer_base\nfrom .preprocessor import base as prep_base\nfrom .router import base as router_base\nfrom .score_fn import base as score_base\n\n# Encoder\nBaseEncoder = encoder_base.BaseEncoder\nBaseTextEncoder = encoder_base.BaseTextEncoder\nBaseAudioEncoder = encoder_base.BaseAudioEncoder\nBaseImageEncoder = encoder_base.BaseImageEncoder\nBaseVideoEncoder = encoder_base.BaseVideoEncoder\nBaseBinaryEncoder = encoder_base.BaseBinaryEncoder\nBaseNumericEncoder = encoder_base.BaseNumericEncoder\nPipelineEncoder = encoder_base.PipelineEncoder\n\n# Indexer\nBaseChunkIndexer = indexer_base.BaseChunkIndexer\nBaseIndexer = indexer_base.BaseIndexer\nBaseDocIndexer = indexer_base.BaseDocIndexer\nBaseKeyIndexer = indexer_base.BaseChunkIndexerHelper\nJointIndexer = indexer_base.JointIndexer\n\n# Preprocessor\nBasePreprocessor = prep_base.BasePreprocessor\nBaseImagePreprocessor = prep_base.BaseImagePreprocessor\nBaseTextPreprocessor = prep_base.BaseTextPreprocessor\nBaseAudioPreprocessor = prep_base.BaseAudioPreprocessor\nBaseVideoPreprocessor = prep_base.BaseVideoPreprocessor\nPipelinePreprocessor = prep_base.PipelinePreprocessor\nUnaryPreprocessor = prep_base.UnaryPreprocessor\n\n# Router\nBaseReduceRouter = router_base.BaseReduceRouter\nBaseRouter = router_base.BaseRouter\nBaseTopkReduceRouter = router_base.BaseTopkReduceRouter\nBaseMapRouter = router_base.BaseMapRouter\nPipelineRouter = router_base.PipelineRouter\n\n# Score_Fn\nBaseScoreFn = score_base.BaseScoreFn\nModifierScoreFn = score_base.ModifierScoreFn\nCombinedScoreFn = score_base.CombinedScoreFn\n'"
gnes/helper.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport fcntl\nimport importlib.util\nimport logging\nimport os\nimport sys\nimport threading\nimport time\nfrom copy import copy\nfrom functools import wraps\nfrom itertools import islice\nfrom logging import Formatter\nfrom typing import Iterator, Any, Union, List, Callable\n\nimport numpy as np\n\ntry:\n    from memory_profiler import memory_usage\nexcept ImportError:\n    memory_usage = lambda: [0]\n\nfrom ruamel.yaml import YAML\nfrom termcolor import colored\n\n__all__ = [\'get_sys_info\', \'get_optimal_sample_size\',\n           \'get_perm\', \'time_profile\', \'set_logger\',\n           \'batch_iterator\', \'batching\', \'yaml\',\n           \'profile_logger\', \'load_contrib_module\',\n           \'parse_arg\', \'profiling\', \'FileLock\',\n           \'train_required\', \'get_first_available_gpu\',\n           \'PathImporter\', \'progressbar\', \'Singleton\']\n\n\nclass Singleton:\n    """"""\n    Make your class singeton\n    """"""\n    def __init__(self, cls):\n        self.__instance = None\n        self.__cls = cls\n        self._lock = threading.Lock()\n\n    def __call__(self, *args, **kwargs):\n        self._lock.acquire()\n        if self.__instance is None:\n            self.__instance = self.__cls(*args, **kwargs)\n        self._lock.release()\n        return self.__instance\n\n\ndef progressbar(i, prefix="""", suffix="""", count=100, size=60):\n    """"""\n\n    Example:\n\n    for i in range(10000):\n        progressbar(i, prefix=""computing: "", count=100, size=60)\n\n    The resulted output is:\n        computing: [###########################################################.] 99/100\n        computing: [###########################################################.] 199/200\n        computing: [###########################################################.] 299/300\n        computing: [###########################################################.] 399/400\n        computing: [###########################################################.] 499/500\n        computing: [###########################################################.] 599/600\n        computing: [###########################################################.] 699/700\n        computing: [###########################################################.] 799/800\n        computing: [###########################################################.] 899/900\n        computing: [#############################...............................] 950/1000\n    """"""\n    step = int(i / count)\n    _i = i\n    i = i % count\n    if step > 0 and i == 0:\n        sys.stdout.write(\'\\n\')\n    x = int(size * i / count)\n    sys.stdout.write(\n        ""%s[%s%s] %i/%i %s\\r"" % (prefix, ""#"" * x, ""."" * (size - x), _i,\n                                 (step + 1) * count, suffix))\n    sys.stdout.flush()\n\n\ndef get_first_available_gpu():\n    try:\n        import GPUtil\n        r = GPUtil.getAvailable(order=\'random\',\n                                maxMemory=0.5,\n                                maxLoad=0.5,\n                                limit=1)\n        if r:\n            return r[0]\n        raise ValueError\n    except Exception:\n        return -1\n\n\nclass FileLock:\n    """"""\n    Implements the Posix based file locking (Linux, Ubuntu, MacOS, etc.)\n    """"""\n\n    def __init__(self, lock_file: str = ""LOCK""):\n        self._lock_file = lock_file\n        self._lock_file_fd = None\n\n    @property\n    def is_locked(self):\n        return self._lock_file_fd is not None\n\n    def acquire(self):\n        open_mode = os.O_RDWR | os.O_CREAT | os.O_TRUNC\n        fd = os.open(self._lock_file, open_mode)\n\n        try:\n            fcntl.flock(fd, fcntl.LOCK_EX | fcntl.LOCK_NB)\n            self._lock_file_fd = fd\n            return fd\n        except (IOError, OSError):\n            os.close(fd)\n        return None\n\n    def release(self):\n        if self.is_locked:\n            fd = self._lock_file_fd\n            self._lock_file_fd = None\n            fcntl.flock(fd, fcntl.LOCK_UN)\n            os.close(fd)\n\n\ndef get_sys_info():\n    from psutil import virtual_memory\n    mem = virtual_memory()\n    # get available memory in (M)\n    avai = mem.available / 1e6\n\n    def timer(x, y):\n        stime = time.time()\n        np.matmul(x, y)\n        return time.time() - stime\n\n    x = np.random.random([1000, 1000])\n    y = np.random.random([1000, 1000])\n    unit_time = timer(x, y)\n    return avai, unit_time\n\n\ndef touch_dir(base_dir: str) -> None:\n    if not os.path.exists(base_dir):\n        os.makedirs(base_dir)\n\n\ndef ralloc_estimator(n_lines, num_dim, unit_time, max_mem, max_time=60):\n    est_time = num_dim * num_dim * n_lines / 1e9 * unit_time * 2\n    est_mem = 60 + 30 * (n_lines * num_dim / 768 / 10000)\n    if (est_time < max_time) and (est_mem < max_mem * 0.5):\n        return n_lines\n    return ralloc_estimator(int(n_lines * 0.9), num_dim, unit_time, max_mem, max_time)\n\n\ndef get_optimal_sample_size(x):\n    max_mem, unit_time = get_sys_info()\n    num_samples, num_dim = x.shape\n    return ralloc_estimator(num_samples, num_dim, unit_time, max_mem, 30)\n\n\ndef get_perm(L, m):\n    n = int(len(L) / m)\n    avg = sum(L) / len(L) * m\n    LR = sorted(enumerate(L), key=lambda x: -x[1])\n    L = np.reshape([i[1] for i in LR], [m, n])\n    R = np.reshape([i[0] for i in LR], [m, n])\n    F = np.zeros([m, n])\n\n    reranked = []\n    for _ in range(n):\n        ind = 0\n        for i in range(m):\n            if i % 2 == 0:\n                start, direction = 0, 1\n            else:\n                start, direction = n - 1, -1\n            while F[i, start] == 1:\n                start += direction\n            if (ind + L[i, start] < avg) or (direction == 1):\n                ind += L[i, start]\n                F[i, start] = 1\n                reranked.append(R[i, start])\n            else:\n                start, direction = n - 1, -1\n                while F[i, start] == 1:\n                    start += direction\n                ind += L[i, start]\n                F[i, start] = 1\n                reranked.append(R[i, start])\n\n    return reranked\n\n\ndef time_profile(func):\n    @wraps(func)\n    def arg_wrapper(*args, **kwargs):\n        if os.environ.get(\'GNES_PROFILING\', False):\n            start_t = time.perf_counter()\n            if os.environ.get(\'GNES_PROFILING_MEM\', False):\n                start_mem = memory_usage()[0]\n            r = func(*args, **kwargs)\n            elapsed = time.perf_counter() - start_t\n            if os.environ.get(\'GNES_PROFILING_MEM\', False):\n                end_mem = memory_usage()[0]\n            # level_prefix = \'\'.join(\'-\' for v in inspect.stack() if v and v.index is not None and v.index >= 0)\n            level_prefix = \'\'\n            if os.environ.get(\'GNES_PROFILING_MEM\', False):\n                mem_status = \'memory: %4.2fM -> %4.2fM\' % (start_mem, end_mem)\n            else:\n                mem_status = \'\'\n            profile_logger.info(\'%s%s: %3.3fs. %s\' % (level_prefix, func.__qualname__, elapsed, mem_status))\n        else:\n            r = func(*args, **kwargs)\n        return r\n\n    return arg_wrapper\n\n\nclass ColoredFormatter(Formatter):\n    MAPPING = {\n        \'DEBUG\': dict(color=\'white\', on_color=None),  # white\n        \'INFO\': dict(color=\'white\', on_color=None),  # cyan\n        \'WARNING\': dict(color=\'yellow\', on_color=\'on_grey\'),  # yellow\n        \'ERROR\': dict(color=\'white\', on_color=\'on_red\'),  # 31 for red\n        \'CRITICAL\': dict(color=\'white\', on_color=\'on_green\'),  # white on red bg\n    }\n\n    PREFIX = \'\\033[\'\n    SUFFIX = \'\\033[0m\'\n\n    def format(self, record):\n        cr = copy(record)\n        seq = self.MAPPING.get(cr.levelname, self.MAPPING[\'INFO\'])  # default white\n        cr.msg = colored(cr.msg, **seq)\n        return super().format(cr)\n\n\ndef set_logger(context, verbose=False):\n    if os.name == \'nt\':  # for Windows\n        return NTLogger(context, verbose)\n\n    # Remove all handlers associated with the root logger object.\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n\n    logger = logging.getLogger(context)\n    logger.propagate = False\n    if not logger.handlers:\n        logger.setLevel(logging.DEBUG if verbose else logging.INFO)\n        formatter = ColoredFormatter(\n            \'%(levelname)-.1s:\' + context + \':[%(filename).3s:%(funcName).3s:%(lineno)3d]:%(message)s\', datefmt=\n            \'%m-%d %H:%M:%S\')\n        console_handler = logging.StreamHandler()\n        console_handler.setLevel(logging.DEBUG if verbose else logging.INFO)\n        console_handler.setFormatter(formatter)\n        logger.handlers = []\n        logger.addHandler(console_handler)\n\n    return logger\n\n\nclass NTLogger:\n    def __init__(self, context, verbose):\n        self.context = context\n        self.verbose = verbose\n\n    def info(self, msg, **kwargs):\n        print(\'I:%s:%s\' % (self.context, msg), flush=True)\n\n    def debug(self, msg, **kwargs):\n        if self.verbose:\n            print(\'D:%s:%s\' % (self.context, msg), flush=True)\n\n    def error(self, msg, **kwargs):\n        print(\'E:%s:%s\' % (self.context, msg), flush=True)\n\n    def warning(self, msg, **kwargs):\n        print(\'W:%s:%s\' % (self.context, msg), flush=True)\n\n\nclass TimeContext:\n    def __init__(self, msg: str, logger=None):\n        self._msg = msg\n        self._logger = logger\n        self.duration = 0\n\n    def __enter__(self):\n        self.start = time.perf_counter()\n        if not self._logger:\n            print(self._msg, end=\' ...\\t\', flush=True)\n        return self\n\n    def __exit__(self, typ, value, traceback):\n        self.duration = time.perf_counter() - self.start\n        if self._logger:\n            self._logger.info(\'%s takes %3.3f secs\' % (self._msg, self.duration))\n        else:\n            print(colored(\'    [%3.3f secs]\' % self.duration, \'green\'), flush=True)\n\n\nclass Tokenizer:\n    def __init__(self, dict_path: str = None):\n        import jieba\n        self._jieba = jieba.Tokenizer()\n        self._jieba.cache_file = ""gnes.jieba_wrapper.cache""\n\n        if dict_path is not None:\n            self._jieba.load_userdict(dict_path)\n\n    def tokenize(self, text, with_position=False):\n        if not with_position:\n            return self._jieba.lcut(text)  # resulted token list\n        else:\n            return self._jieba.tokenize(text)  # triple data consisting of (token, start_pos, end_pos)\n\n\ndef batch_iterator(data: Union[Iterator[Any], List[Any], np.ndarray], batch_size: int, axis: int = 0) -> Iterator[Any]:\n    if not batch_size or batch_size <= 0:\n        yield data\n        return\n    if isinstance(data, np.ndarray):\n        if batch_size >= data.shape[axis]:\n            yield data\n            return\n        for _ in range(0, data.shape[axis], batch_size):\n            start = _\n            end = min(len(data), _ + batch_size)\n            yield np.take(data, range(start, end), axis, mode=\'clip\')\n    elif hasattr(data, \'__len__\'):\n        if batch_size >= len(data):\n            yield data\n            return\n        for _ in range(0, len(data), batch_size):\n            yield data[_:_ + batch_size]\n    elif isinstance(data, Iterator):\n        # as iterator, there is no way to know the length of it\n        while True:\n            chunk = tuple(islice(data, batch_size))\n            if not chunk:\n                return\n            yield chunk\n    else:\n        raise TypeError(\'unsupported type: %s\' % type(data))\n\n\ndef get_size(data: Union[Iterator[Any], List[Any], np.ndarray], axis: int = 0) -> int:\n    if isinstance(data, np.ndarray):\n        total_size = data.shape[axis]\n    elif hasattr(data, \'__len__\'):\n        total_size = len(data)\n    else:\n        total_size = None\n    return total_size\n\n\ndef pooling_simple(data_array, pooling_strategy):\n    if pooling_strategy == \'REDUCE_MEAN\':\n        _pooled_data = sum(data_array) / (len(data_array) + 1e-10)\n    elif pooling_strategy == \'REDUCE_MAX\':\n        _pooled_data = max(data_array) / (len(data_array) + 1e-10)\n    elif pooling_strategy == \'REDUCE_MEAN_MAX\':\n        _pooled_data = np.concatenate(\n            (sum(data_array) / (len(data_array) + 1e-10),\n             max(data_array) / (len(data_array) + 1e-10)), axis=0)\n    else:\n        raise ValueError(\'pooling_strategy: %s has not been implemented\' % pooling_strategy)\n    return _pooled_data\n\n\ndef pooling_torch(data_tensor, mask_tensor, pooling_strategy):\n    import torch\n\n    minus_mask = lambda x, m: x - (1.0 - m).unsqueeze(2) * 1e30\n    mul_mask = lambda x, m: torch.mul(x, m.unsqueeze(2))\n\n    masked_reduce_mean = lambda x, m: torch.div(torch.sum(mul_mask(x, m), dim=1),\n                                                torch.sum(m.unsqueeze(2), dim=1) + 1e-10)\n    masked_reduce_max = lambda x, m: torch.max(minus_mask(x, m), 1)[0]\n\n    if pooling_strategy == \'REDUCE_MEAN\':\n        output_tensor = masked_reduce_mean(data_tensor, mask_tensor)\n    elif pooling_strategy == \'REDUCE_MAX\':\n        output_tensor = masked_reduce_max(data_tensor, mask_tensor)\n    elif pooling_strategy == \'REDUCE_MEAN_MAX\':\n        output_tensor = torch.cat(\n            (masked_reduce_mean(data_tensor, mask_tensor),\n             masked_reduce_max(data_tensor, mask_tensor)), dim=1)\n    else:\n        raise ValueError(\'pooling_strategy: %s has not been implemented\' % pooling_strategy)\n\n    return output_tensor\n\n\ndef batching(func: Callable[[Any], np.ndarray] = None, *,\n             batch_size: Union[int, Callable] = None, num_batch=None,\n             iter_axis: int = 0, concat_axis: int = 0, chunk_dim=-1):\n    def _batching(func):\n        @wraps(func)\n        def arg_wrapper(self, data, label=None, *args, **kwargs):\n            # priority: decorator > class_attribute\n            b_size = (batch_size(data) if callable(batch_size) else batch_size) or getattr(self, \'batch_size\', None)\n            # no batching if b_size is None\n            if b_size is None:\n                if label is None:\n                    return func(self, data, *args, **kwargs)\n                else:\n                    return func(self, data, label, *args, **kwargs)\n\n            if hasattr(self, \'logger\'):\n                self.logger.info(\n                    \'batching enabled for %s(). batch_size=%s\\tnum_batch=%s\\taxis=%s\' % (\n                        func.__qualname__, b_size, num_batch, iter_axis))\n\n            total_size1 = get_size(data, iter_axis)\n            total_size2 = b_size * num_batch if num_batch else None\n\n            if total_size1 is not None and total_size2 is not None:\n                total_size = min(total_size1, total_size2)\n            else:\n                total_size = total_size1 or total_size2\n\n            final_result = []\n\n            if label is not None:\n                data = (data, label)\n\n            for b in batch_iterator(data[:total_size], b_size, iter_axis):\n                if label is None:\n                    r = func(self, b, *args, **kwargs)\n                else:\n                    r = func(self, b[0], b[1], *args, **kwargs)\n\n                if r is not None:\n                    final_result.append(r)\n\n            if len(final_result) == 1:\n                # the only result of one batch\n                return final_result[0]\n\n            if len(final_result) and concat_axis is not None:\n                if isinstance(final_result[0], np.ndarray):\n                    final_result = np.concatenate(final_result, concat_axis)\n                    if chunk_dim != -1:\n                        final_result = final_result.reshape((-1, chunk_dim, final_result.shape[1]))\n                elif isinstance(final_result[0], tuple):\n                    reduced_result = []\n                    num_cols = len(final_result[0])\n                    for col in range(num_cols):\n                        reduced_result.append(np.concatenate([row[col] for row in final_result], concat_axis))\n                    if chunk_dim != -1:\n                        for col in range(num_cols):\n                            reduced_result[col] = reduced_result[col].reshape(\n                                (-1, chunk_dim, reduced_result[col].shape[1]))\n                    final_result = tuple(reduced_result)\n\n            if len(final_result):\n                return final_result\n\n        return arg_wrapper\n\n    if func:\n        return _batching(func)\n    else:\n        return _batching\n\n\ndef _get_yaml():\n    y = YAML(typ=\'safe\')\n    y.default_flow_style = False\n    return y\n\n\ndef parse_arg(v: str):\n    if v.startswith(\'[\') and v.endswith(\']\'):\n        # function args must be immutable tuples not list\n        tmp = v.replace(\'[\', \'\').replace(\']\', \'\').strip().split(\',\')\n        if len(tmp) > 0:\n            return [parse_arg(vv.strip()) for vv in tmp]\n        else:\n            return []\n    try:\n        v = int(v)  # parse int parameter\n    except ValueError:\n        try:\n            v = float(v)  # parse float parameter\n        except ValueError:\n            if len(v) == 0:\n                # ignore it when the parameter is empty\n                v = None\n            elif v.lower() == \'true\':  # parse boolean parameter\n                v = True\n            elif v.lower() == \'false\':\n                v = False\n    return v\n\n\ndef countdown(t: int, logger=None, reason: str = \'I am blocking this thread\'):\n    if not logger:\n        sys.stdout.write(\'\\n\')\n        sys.stdout.flush()\n    while t > 0:\n        t -= 1\n        msg = \'%ss left: %s\' % (colored(\'%3d\' % t, \'yellow\'), reason)\n        if logger:\n            logger.info(msg)\n        else:\n            sys.stdout.write(\'\\r%s\' % msg)\n            sys.stdout.flush()\n        time.sleep(1)\n    sys.stdout.write(\'\\n\')\n    sys.stdout.flush()\n\n\ndef as_numpy_array(func, dtype=np.float32):\n    @wraps(func)\n    def arg_wrapper(self, *args, **kwargs):\n        r = func(self, *args, **kwargs)\n        r_type = type(r).__name__\n        if r_type in {\'ndarray\', \'EagerTensor\', \'Tensor\', \'list\'}:\n            return np.array(r, dtype)\n        else:\n            raise TypeError(\'unrecognized type %s: %s\' % (r_type, type(r)))\n\n    return arg_wrapper\n\n\ndef train_required(func):\n    @wraps(func)\n    def arg_wrapper(self, *args, **kwargs):\n        if hasattr(self, \'is_trained\'):\n            if self.is_trained:\n                return func(self, *args, **kwargs)\n            else:\n                raise RuntimeError(\'training is required before calling ""%s""\' % func.__name__)\n        else:\n            raise AttributeError(\'%r has no attribute ""is_trained""\' % self)\n\n    return arg_wrapper\n\n\ndef load_contrib_module():\n    if not os.getenv(\'GNES_CONTRIB_MODULE_IS_LOADING\'):\n        import importlib.util\n\n        contrib = os.getenv(\'GNES_CONTRIB_MODULE\')\n        os.environ[\'GNES_CONTRIB_MODULE_IS_LOADING\'] = \'true\'\n\n        modules = []\n\n        if contrib:\n            default_logger.info(\n                \'find a value in $GNES_CONTRIB_MODULE=%s, will load them as external modules\' % contrib)\n            for p in contrib.split(\',\'):\n                m = PathImporter.add_modules(p)\n                modules.append(m)\n                default_logger.info(\'successfully registered %s class, you can now use it via yaml.\' % m)\n        return modules\n\n\nclass PathImporter:\n\n    @staticmethod\n    def _get_module_name(absolute_path):\n        module_name = os.path.basename(absolute_path)\n        module_name = module_name.replace(\'.py\', \'\')\n        return module_name\n\n    @staticmethod\n    def add_modules(*paths):\n        for p in paths:\n            if not os.path.exists(p):\n                raise FileNotFoundError(\'cannot import module from %s, file not exist\', p)\n            module, spec = PathImporter._path_import(p)\n        return module\n\n    @staticmethod\n    def _path_import(absolute_path):\n        module_name = PathImporter._get_module_name(absolute_path)\n        spec = importlib.util.spec_from_file_location(module_name, absolute_path)\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n        sys.modules[spec.name] = module\n        return module, spec\n\n\ndef make_route_table(routes, include_frontend: bool = False, jitter: float = 1e-8):\n    route_time = []\n    if include_frontend:\n        total_duration = get_duration(routes[0].start_time, routes[0].end_time) + jitter\n    else:\n        total_duration = get_duration(routes[0].start_time, routes[-1].end_time) + jitter\n    sum_duration = 0\n    for k in routes:\n        if k.service == \'FrontEndService\':\n            continue\n        d = get_duration(k.start_time, k.end_time)\n        route_time.append((k.service, d))\n        sum_duration += d\n\n    def get_table_str(time_table):\n        return \'\\n\'.join(\n            [\'%40s\\t%3.3fs\\t%3d%%\' % (k[0], k[1], k[1] / total_duration * 100) for k in\n             sorted(time_table, key=lambda x: x[1], reverse=True)])\n\n    summary = [(\'system\', total_duration - sum_duration),\n               (\'total\', total_duration),\n               (\'job\', sum_duration),\n               (\'parallel\', max(sum_duration - total_duration, 0))]\n    route_table = (\'\\n%s\\n\' % (\'-\' * 80)).join(\n        [\'%40s\\t%-6s\\t%3s\' % (\'Breakdown\', \'Time\', \'Percent\'), get_table_str(route_time),\n         get_table_str(summary)])\n    return route_table\n\n\ndef get_duration(start_time, end_time):\n    if not start_time or not end_time:\n        return -1\n    d_s = end_time.seconds - start_time.seconds\n    d_n = end_time.nanos - start_time.nanos\n    if d_s < 0 and d_n > 0:\n        d_s = max(d_s + 1, 0)\n        d_n = max(d_n - 1e9, 0)\n    elif d_s > 0 and d_n < 0:\n        d_s = max(d_s - 1, 0)\n        d_n = max(d_n + 1e9, 0)\n    return max(d_s + d_n / 1e9, 0)\n\n\nprofile_logger = set_logger(\'PROFILE\')\ndefault_logger = set_logger(\'GNES\')\nprofiling = time_profile\n\nyaml = _get_yaml()\n'"
gnes/uuid.py,0,"b'import threading\nimport time\nfrom datetime import datetime\n\nfrom . import helper\n\n\n@helper.Singleton\nclass BaseIDGenerator(object):\n    """"""\n    Thread-safe (auto incremental) uuid generator\n    """"""\n\n    def __init__(self, start_id: int = 0, *args, **kwargs):\n        self.args = args\n        self.kwargs = kwargs\n        self._lock = threading.Lock()\n        self._next_id = start_id\n\n    def reset(self, start_id: int = 0):\n        with self._lock:\n            self._next_id = start_id\n\n    def next(self) -> int:\n        with self._lock:\n            temp = self._next_id\n            self._next_id += 1\n            return temp\n\n\n@helper.Singleton\nclass SnowflakeIDGenerator(object):\n\n    def __init__(self,\n                 machine_id: int = 0,\n                 datacenter_id: int = 0,\n                 *args,\n                 **kwargs):\n        self._lock = threading.Lock()\n        self._next_id = 0\n\n        self.machine_id = machine_id\n        self.datacenter_id = datacenter_id\n\n        self.machine_bits = 5\n        self.datacenter_bits = 5\n        self.max_machine_id = -1 ^ -1 << self.machine_bits\n        self.max_datacenter_id = -1 ^ (-1 << self.datacenter_bits)\n\n        self.counter_bits = 12\n        self.max_counter_mask = -1 ^ -1 << self.counter_bits\n\n        self.machine_shift = self.counter_bits\n        self.datacenter_shift = self.counter_bits + self.machine_bits\n        self.timestamp_shift = self.counter_bits + self.machine_bits + self.datacenter_bits\n\n        self.twepoch = int(time.mktime(time.strptime(\'2019-01-01 00:00:00\', ""%Y-%m-%d %H:%M:%S"")))\n        self.last_timestamp = -1\n        self.current_timestamp = lambda: int(datetime.now().timestamp() * 1000)\n\n    # def _get_timestamp(self) -> int:\n    #     return int(datetime.now().timestamp() * 1000)\n\n    def _get_next_timestamp(self, last_timestamp) -> int:\n        timestamp = self.current_timestamp()\n        while timestamp <= last_timestamp:\n            timestamp = self.current_timestamp()\n        return timestamp\n\n    def next(self) -> int:\n        with self._lock:\n            timestamp = self.current_timestamp()\n            if self.last_timestamp == timestamp:\n                self._next_id = (self._next_id + 1) & self.max_counter_mask\n                if self._next_id == 0:\n                    timestamp = self._get_next_timestamp(self.last_timestamp)\n            else:\n                self._next_id = 0\n\n            if timestamp < self.last_timestamp:\n                raise ValueError(\n                    \'the current timestamp is smaller than the last timestamp\')\n\n            self.last_timestamp = timestamp\n            uuid = ((timestamp - self.twepoch) << self.timestamp_shift) \\\n                    | (self.datacenter_id << self.datacenter_shift) \\\n                    | (self.machine_id << self.machine_shift) \\\n                    | self._next_id\n            return uuid\n'"
tests/__init__.py,0,"b'import os\nimport re\nfrom typing import TextIO, List\n\nfrom gnes.proto import gnes_pb2\n\n\ndef txt_file2pb_docs(fp: TextIO, start_id: int = 0) -> List[\'gnes_pb2.Document\']:\n    data = [v for v in fp if v.strip()]\n    docs = []\n    for doc_id, doc_txt in enumerate(data, start_id):\n        doc = line2pb_doc(doc_txt, doc_id)\n        docs.append(doc)\n    return docs\n\n\ndef line2pb_doc(line: str, doc_id: int = 0, deliminator: str = r\'[.\xe3\x80\x82\xef\xbc\x81\xef\xbc\x9f!?]+\') -> \'gnes_pb2.Document\':\n    doc = gnes_pb2.Document()\n    doc.doc_id = doc_id\n    doc.doc_type = gnes_pb2.Document.TEXT\n    doc.meta_info = line.encode()\n    if deliminator:\n        for ci, s in enumerate(re.split(deliminator, line)):\n            if s.strip():\n                c = doc.chunks.add()\n                c.doc_id = doc_id\n                c.text = s\n                c.offset = ci\n    else:\n        c = doc.chunks.add()\n        c.doc_id = doc_id\n        c.text = line\n        c.offset = 0\n    return doc\n\n\nenv_dict = {\n    \'orange-ci\': {\n        \'BERT_CI_PORT\': 7125,\n        \'BERT_CI_PORT_OUT\': 7126,\n        \'BERT_CI_MODEL\': \'/chinese_L-12_H-768_A-12\',\n        \'ELMO_CI_MODEL\': \'/zhs.model\',\n        \'FLAIR_CI_MODEL\': \'/multi-forward-fast\',\n        \'GPT_CI_MODEL\': \'/openai_gpt\',\n        \'GPT2_CI_MODEL\': \'/openai_gpt2\',\n        \'XL_CI_MODEL\': \'/transformer_xl_wt103\',\n        \'WORD2VEC_MODEL\': \'/sgns.wiki.bigram-char.sample\',\n        \'VGG_MODEL\': \'/\',\n        \'RESNET_MODEL\': \'/\',\n        \'INCEPTION_MODEL\': \'/\',\n        \'MOBILENET_MODEL\': \'/\',\n        \'FASTERRCNN_MODEL\': \'/\',\n        \'GNES_PROFILING\': \'\',\n        \'TORCH_TRANSFORMERS_MODEL\': \'/torch_transformer\'\n        # \'VGGISH_MODEL\': \'/lab/vggish\',\n        # \'YT8M_INCEPTION\': \'/lab/yt8m_incep_v3\',\n        # \'YT8M_PCA_MODEL\': \'/lab/yt8m_pca\',\n        # \'YT8M_MODEL\': \'/lab/yt8m_model\'\n    },\n    \'idc-165\': {\n        \'BERT_CI_PORT\': 7125,\n        \'BERT_CI_PORT_OUT\': 7126,\n        \'BERT_CI_MODEL\': \'/ext_data/chinese_L-12_H-768_A-12\',\n        \'ELMO_CI_MODEL\': \'/ext_data/zhs.model\',\n        \'FLAIR_CI_MODEL\': \'/ext_data/multi-forward-fast\',\n        \'GPT_CI_MODEL\': \'/ext_data/openai_gpt\',\n        \'GPT2_CI_MODEL\': \'/ext_data/openai_gpt2\',\n        \'XL_CI_MODEL\': \'/ext_data/transformer_xl_wt103\',\n        \'WORD2VEC_MODEL\': \'/ext_data/sgns.wiki.bigram-char.sample\',\n        \'VGG_MODEL\': \'/ext_data/image_encoder\',\n        \'RESNET_MODEL\': \'/ext_data/image_encoder\',\n        \'INCEPTION_MODEL\': \'/ext_data/image_encoder\',\n        \'MOBILENET_MODEL\': \'/ext_data/image_encoder\',\n        \'FASTERRCNN_MODEL\': \'/ext_data/image_preprocessor\',\n        \'GNES_PROFILING\': \'\',\n        \'TORCH_TRANSFORMERS_MODEL\': \'/ext_data/torch_transformer\'\n        # \'VGGISH_MODEL\': \'/ext_data/lab/vggish\',\n        # \'YT8M_INCEPTION\': \'/ext_data/lab/yt8m_incep_v3\',\n        # \'YT8M_PCA_MODEL\': \'/ext_data/lab/yt8m_pca\',\n        # \'YT8M_MODEL\': \'/ext_data/lab/yt8m_model\'\n    }\n\n}\n\nfor k, v in env_dict[os.environ.get(\'GNES_ENV_SET\', \'idc-165\')].items():\n    if k not in os.environ:\n        os.environ[k] = str(v)\n    else:\n        print(\'os.environ[""%s""]=%s exists already, i wont set it to %s\' % (k, os.environ[k], str(v)))\n'"
tests/test_annoyindexer.py,0,"b""import os\nimport unittest\n\nimport numpy as np\n\nfrom gnes.indexer.chunk.annoy import AnnoyIndexer\nfrom gnes.indexer.chunk.numpy import NumpyIndexer\n\n\nclass TestAnnoyIndexer(unittest.TestCase):\n    def setUp(self):\n        self.toy_data = np.random.random([10, 5]).astype(np.float32)\n\n        dirname = os.path.dirname(__file__)\n        self.dump_path = os.path.join(dirname, 'indexer.pkl')\n\n    def tearDown(self):\n        if os.path.exists(self.dump_path):\n            os.remove(self.dump_path)\n\n    def test_search(self):\n        a = AnnoyIndexer(5, self.dump_path)\n        a.add(list(zip(list(range(10)), list(range(10)))), self.toy_data, [1.] * 10)\n        self.assertEqual(a.num_chunks, 10)\n        self.assertEqual(a.num_docs, 10)\n        top_1 = [i[0][0] for i in a.query(self.toy_data, top_k=1)]\n        self.assertEqual(top_1, list(range(10)))\n        a.close()\n        a.dump()\n        a.dump_yaml()\n\n    def test_numpy_indexer(self):\n        a = NumpyIndexer()\n        a.add(list(zip(list(range(10)), list(range(10)))), self.toy_data, [1.] * 10)\n        self.assertEqual(a.num_chunks, 10)\n        self.assertEqual(a.num_docs, 10)\n        top_1 = [i[0][0] for i in a.query(self.toy_data, top_k=1)]\n        self.assertEqual(top_1, list(range(10)))\n        a.close()\n        a.dump()\n        a.dump_yaml()\n        b = NumpyIndexer.load_yaml(a.yaml_full_path)\n        self.assertEqual(b.num_chunks, 10)\n        self.assertEqual(b.num_docs, 10)\n        top_1 = [i[0][0] for i in b.query(self.toy_data, top_k=1)]\n        self.assertEqual(top_1, list(range(10)))\n"""
tests/test_audio_preprocessor.py,0,"b""import os\nimport unittest\n\nfrom gnes.cli.parser import set_preprocessor_parser, _set_client_parser\nfrom gnes.client.base import ZmqClient\nfrom gnes.proto import gnes_pb2, RequestGenerator, blob2array\nfrom gnes.service.preprocessor import PreprocessorService\n\n\nclass TestAudioPreprocessor(unittest.TestCase):\n\n    def setUp(self):\n        self.dirname = os.path.dirname(__file__)\n        self.yml_path = os.path.join(self.dirname, 'yaml', 'preprocessor-audio.yml')\n        self.video_path = os.path.join(self.dirname, 'videos')\n        self.video_bytes = [open(os.path.join(self.video_path, _), 'rb').read()\n                            for _ in os.listdir(self.video_path)]\n\n    def test_video_preprocessor_service_empty(self):\n        args = set_preprocessor_parser().parse_args([\n            '--yaml_path', self.yml_path\n        ])\n        with PreprocessorService(args):\n            pass\n\n    def test_video_preprocessor_service_realdata(self):\n        args = set_preprocessor_parser().parse_args([\n            '--yaml_path', self.yml_path\n        ])\n\n        c_args = _set_client_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', str(args.port_in)\n        ])\n\n        with PreprocessorService(args), ZmqClient(c_args) as client:\n            for req in RequestGenerator.index(self.video_bytes):\n                msg = gnes_pb2.Message()\n                msg.request.index.CopyFrom(req.index)\n                client.send_message(msg)\n                r = client.recv_message()\n                for d in r.request.index.docs:\n                    self.assertGreater(len(d.chunks), 0)\n                    for _ in range(len(d.chunks)):\n                        shape = blob2array(d.chunks[_].blob).shape\n                        self.assertEqual(len(shape), 1)\n\n"""
tests/test_batching.py,0,"b""import unittest\n\nimport numpy as np\nfrom numpy.testing import assert_array_equal\n\nfrom gnes.encoder.base import BaseEncoder\nfrom gnes.helper import batching, batch_iterator\n\nget_batch_size = lambda x: 2\n\n\nclass bar(BaseEncoder):\n    def __init__(self):\n        super().__init__()\n        self.batch_size = None\n\n    @batching(batch_size=2, num_batch=4)\n    def foo(self, data):\n        return np.array(data)\n\n    @batching(batch_size=2)\n    def bar(self, data):\n        return np.array(data)\n\n    @batching(batch_size=8, num_batch=8)\n    def foo1(self, data):\n        return np.array(data)\n\n    @batching\n    def foo2(self, data):\n        return np.array(data)\n\n    @batching(batch_size=get_batch_size)\n    def foo3(self, data):\n        return np.array(data)\n\n    @batching(batch_size=get_batch_size)\n    def train(self, data):\n        print('train: %s' % data)\n\n\nclass TestBatching(unittest.TestCase):\n    def test_iterator(self):\n        a = [1, 2, 3, 4, 5, 6, 7]\n        b = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n\n        num_batches = np.ceil(len(a) / 2)\n        act_batch = 0\n        for j in batch_iterator(a, 2):\n            self.assertLessEqual(len(j), 2)\n            self.assertEqual(type(j), list)\n            act_batch += 1\n        self.assertEqual(num_batches, act_batch)\n\n        num_batches = np.ceil(len(b) / 2)\n        act_batch = 0\n        for j in batch_iterator(b, 2):\n            self.assertLessEqual(j.shape[0], 2)\n            self.assertEqual(type(j), np.ndarray)\n            act_batch += 1\n        self.assertEqual(num_batches, act_batch)\n\n        num_batches = np.ceil(len(a) / 2)\n        act_batch = 0\n        for j in batch_iterator(iter(a), 2):\n            self.assertLessEqual(len(j), 2)\n            act_batch += 1\n        self.assertEqual(num_batches, act_batch)\n\n    def test_decorator(self):\n        b = bar()\n\n        def _test_fn(fn):\n            self.assertEqual(fn([1]), np.array([1, ]))\n            self.assertSequenceEqual(fn([1, 2]).tolist(), [1, 2])\n            self.assertSequenceEqual(fn(list(range(1, 30))).tolist(), list(range(1, 30)))\n            self.assertSequenceEqual(fn(range(1, 30)).tolist(), list(range(1, 30)))\n\n        _test_fn(b.foo1)\n        _test_fn(b.foo2)\n        _test_fn(b.bar)\n\n        self.assertSequenceEqual(b.foo(range(1, 30)).tolist(), list(range(1, 9)))\n        t = np.random.randint(0, 255, [32, 10])\n        assert_array_equal(b.foo1(t), t)\n        assert_array_equal(b.bar(t), t)\n        assert_array_equal(b.foo(t), t[:8])\n\n        b.batch_size = 8\n        _test_fn(b.foo2)\n        _test_fn(b.foo3)\n        self.assertEqual(b.train([1]), None)\n\n    def test_mini_batch(self):\n        x = list(range(10))\n\n        @batching(batch_size=4)\n        def _do_mini_batch(_, y):\n            return y\n\n        # this will follow self.batch_size, which is None\n        @batching\n        def _do_mini_batch2(_, y):\n            return y\n\n        self.assertEqual(_do_mini_batch(None, x), [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9]])\n        self.assertEqual(_do_mini_batch2(self, x), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n        self.batch_size = 4\n\n        self.assertEqual(_do_mini_batch2(self, x), [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9]])\n"""
tests/test_bindexer.py,0,"b'import os\nimport unittest\n\nimport numpy as np\n\nfrom gnes.indexer.chunk.bindexer import BIndexer\n\n\n@unittest.SkipTest\nclass TestBIndexer(unittest.TestCase):\n    def setUp(self):\n        self.toy_data = np.array([[1, 2, 1, 2],\n                                  [2, 1, 3, 4],\n                                  [1, 2, 1, 2],\n                                  [2, 1, 4, 3],\n                                  [2, 1, 3, 4],\n                                  [23, 32, 21, 33],\n                                  [123, 132, 1, 1]]).astype(np.uint8)\n        self.toy_label = [(234, 0), (432, 0), (123, 1), (321, 0), (1, 0), (2, 0), (6, 0)]\n\n        self.toy_query = np.array([[1, 2, 1, 2],\n                                   [2, 1, 3, 4],\n                                   [3, 2, 1, 2]]).astype(np.uint8)\n\n        self.toy_exp = [[(234, 0, 1., 1,), (123, 1, 1., 1)], [(432, 0, 1., 1), (1, 0, 1., 1)],\n                        [(234, 0, 1., 0.75), (123, 1, 1., 0.75)]]\n        self.weights = [1.] * len(self.toy_label)\n\n        dirname = os.path.dirname(__file__)\n        self.dump_path = os.path.join(dirname, \'test-indexer.bin\')\n\n    def tearDown(self):\n        if os.path.exists(self.dump_path):\n            os.remove(self.dump_path)\n\n    def test_nsw_search(self):\n        fd = BIndexer(self.toy_data.shape[1], data_path=self.dump_path + \'_1\')\n        fd.add(self.toy_label, self.toy_data, self.weights)\n        self.assertEqual(fd.num_doc, 7)\n        self.assertEqual(fd.num_chunks, 7)\n        self.assertEqual(fd.num_chunks_avg, 1)\n\n        rs = fd.query(self.toy_query, 2, method=\'nsw\', normalized_score=False)\n        for i in range(len(rs)):\n            rs[i] = sorted(rs[i], key=lambda x: (x[3], x[0]))\n        fd.close()\n        self.assertEqual(rs, self.toy_exp)\n\n    def test_force_search(self):\n        fd = BIndexer(self.toy_data.shape[1], data_path=self.dump_path + \'_2\')\n        fd.add(self.toy_label, self.toy_data, self.weights)\n        rs = fd.query(self.toy_query, 2, method=\'force\', normalized_score=False)\n        for i in range(len(rs)):\n            rs[i] = sorted(rs[i], key=lambda x: (x[3], x[0]))\n        fd.close()\n        self.assertEqual(rs, self.toy_exp)\n\n    def test_dump_load(self):\n        fd = BIndexer(self.toy_data.shape[1], data_path=self.dump_path + \'_3\')\n        fd.add(self.toy_label, self.toy_data, self.weights)\n        fd.dump()\n        fd.close()\n        # shutil.rmtree(self.data_path + ""_3"")\n\n        fd2 = BIndexer.load(fd.dump_full_path)\n        rs = fd2.query(self.toy_query, 2, normalized_score=False)\n        for i in range(len(rs)):\n            rs[i] = sorted(rs[i], key=lambda x: (x[3], x[0]))\n        fd2.close()\n\n        self.assertEqual(rs, self.toy_exp)\n        os.remove(self.dump_path + \'_3\')\n'"
tests/test_client_cli.py,0,"b""import os\nimport unittest\n\nfrom gnes.cli.parser import set_frontend_parser, set_router_parser, set_client_cli_parser\nfrom gnes.client.cli import CLIClient\nfrom gnes.service.base import SocketType\nfrom gnes.service.frontend import FrontendService\nfrom gnes.service.router import RouterService\n\n\nclass TestCLI(unittest.TestCase):\n    def setUp(self):\n        self.dirname = os.path.dirname(__file__)\n        self.test_file = os.path.join(self.dirname, 'sonnets_small.txt')\n        self.train_args = set_client_cli_parser().parse_args([\n            '--mode', 'train',\n            '--txt_file', self.test_file,\n            '--batch_size', '4'\n        ])\n        self.index_args = set_client_cli_parser().parse_args([\n            '--mode', 'index',\n            '--txt_file', self.test_file,\n            '--batch_size', '4'\n        ])\n        self.query_args = set_client_cli_parser().parse_args([\n            '--mode', 'query',\n            '--txt_file', self.test_file,\n            '--batch_size', '4'\n        ])\n        os.unsetenv('http_proxy')\n        os.unsetenv('https_proxy')\n\n    def test_cli(self):\n        args = set_frontend_parser().parse_args([])\n\n        p_args = set_router_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', str(args.port_in),\n            '--socket_in', str(SocketType.PULL_CONNECT),\n            '--socket_out', str(SocketType.PUSH_CONNECT),\n            '--yaml_path', 'BaseRouter',\n        ])\n\n        with RouterService(p_args), FrontendService(args):\n            CLIClient(self.train_args)\n            CLIClient(self.index_args)\n            CLIClient(self.query_args)\n"""
tests/test_compose.py,0,"b""import os\nimport unittest\n\nfrom gnes.cli.parser import set_composer_parser, set_composer_flask_parser\nfrom gnes.composer.base import YamlComposer\nfrom gnes.composer.flask import YamlComposerFlask\nfrom gnes.composer.http import YamlComposerHttp\n\n\nclass TestCompose(unittest.TestCase):\n    def setUp(self):\n        self.dirname = os.path.dirname(__file__)\n        self.html_path = os.path.join(self.dirname, 'test.html')\n\n    def test_all(self):\n        paths = [os.path.join(self.dirname, 'yaml', 'topology%d.yml' % j) for j in range(1, 8)]\n        b_a = [(3, 3), (4, 4), (4, 5), (4, 7), (4, 6), (4, 8), (4, 8)]\n        for p, j in zip(paths, b_a):\n            self._test_topology(p, *j)\n\n    def _test_topology(self, yaml_path: str, num_layer_before: int, num_layer_after: int):\n        args = set_composer_parser().parse_args([\n            '--yaml_path', yaml_path,\n            '--html_path', self.html_path\n        ])\n        a = YamlComposer(args)\n        self.assertEqual(len(a._layers), num_layer_before)\n        r = a.build_layers()\n        self.assertEqual(len(r), num_layer_after)\n        for c in r:\n            print(c)\n        a.build_all()\n        print(a.build_shell(r))\n        os.path.exists(self.html_path)\n        print(a.build_dockerswarm(r))\n\n    @unittest.SkipTest\n    def test_http_local(self):\n        args = set_composer_flask_parser().parse_args(['--serve'])\n        YamlComposerHttp(args).run()\n\n    @unittest.SkipTest\n    def test_flask_local(self):\n        args = set_composer_flask_parser().parse_args(['--flask'])\n        YamlComposerFlask(args).run()\n\n    def test_flask(self):\n        yaml_path = os.path.join(self.dirname, 'yaml', 'topology1.yml')\n        args = set_composer_flask_parser().parse_args([\n            '--flask',\n            '--yaml_path', yaml_path,\n            '--html_path', self.html_path\n        ])\n        app = YamlComposerFlask(args)._create_flask_app().test_client()\n        response = app.get('/', follow_redirects=True)\n        self.assertEqual(response.status_code, 200)\n\n        response = app.post('/generate', follow_redirects=True)\n        self.assertEqual(response.status_code, 406)\n\n        response = app.post('/generate', data={'yaml-config': ''},\n                            follow_redirects=True)\n        self.assertEqual(response.status_code, 400)\n\n        response = app.post('/generate',\n                            data={'yaml-config': 'port: 5566\\nservices:\\n- name: Preprocessor\\n- name: Encoder'},\n                            follow_redirects=True)\n        self.assertEqual(response.status_code, 200)\n\n        response = app.get('/', follow_redirects=True)\n        self.assertEqual(response.status_code, 200)\n\n    def tearDown(self):\n        if os.path.exists(self.html_path):\n            os.remove(self.html_path)\n"""
tests/test_contrib_module.py,0,"b""import os\nimport unittest.mock\n\ndirname = os.path.dirname(__file__)\nmodule_path = os.path.join(dirname, 'contrib', 'dummy_contrib.py')\n\n\n@unittest.SkipTest\nclass TestContribModule(unittest.TestCase):\n    def setUp(self):\n        self.yaml_path = os.path.join(os.path.dirname(__file__),\n                                      'contrib', 'dummy.yml')\n        self.dump_yaml_path = os.path.join(os.path.dirname(__file__),\n                                           'dummy-dump.yml')\n\n    def tearDown(self):\n        # reload gnes module on every unit test\n        if os.path.exists(self.dump_yaml_path):\n            os.remove(self.dump_yaml_path)\n\n    def test_load_contrib(self):\n        os.environ['GNES_CONTRIB_MODULE'] = module_path\n        from gnes.encoder.base import BaseEncoder, BaseTextEncoder\n        a = BaseEncoder.load_yaml(self.yaml_path)\n        self.assertIsInstance(a, BaseTextEncoder)\n        self.assertEqual(a.encode([]), 'hello 531')\n        a.dump()\n        a.dump_yaml(self.dump_yaml_path)\n        b = BaseEncoder.load_yaml(self.dump_yaml_path)\n        self.assertIsInstance(b, BaseTextEncoder)\n        self.assertEqual(b.encode([]), 'hello 531')\n\n    def test_bad_name(self):\n        os.environ['GNES_CONTRIB_MODULE'] = module_path\n        try:\n            from gnes.encoder.base import BaseEncoder\n        except AttributeError:\n            pass\n\n    def test_bad_path(self):\n        os.environ['GNES_CONTRIB_MODULE'] = 'blah'\n        try:\n            from gnes.encoder.base import BaseEncoder\n        except AttributeError:\n            pass\n"""
tests/test_dict_indexer.py,0,"b""import os\nimport unittest\nfrom shutil import rmtree\n\nimport grpc\n\nfrom gnes.cli.parser import set_frontend_parser, set_preprocessor_parser, set_indexer_parser\nfrom gnes.indexer.base import BaseIndexer\nfrom gnes.indexer.doc.filesys import DirectoryIndexer\nfrom gnes.preprocessor.base import BasePreprocessor\nfrom gnes.proto import gnes_pb2, gnes_pb2_grpc, RequestGenerator\nfrom gnes.service.base import SocketType, ServiceManager\nfrom gnes.service.frontend import FrontendService\nfrom gnes.service.indexer import IndexerService\nfrom gnes.service.preprocessor import PreprocessorService\n\n\nclass TestDictIndexer(unittest.TestCase):\n    def setUp(self):\n        self.dirname = os.path.dirname(__file__)\n\n        self.video_path = os.path.join(self.dirname, 'videos')\n        self.video_bytes = [open(os.path.join(self.video_path, _), 'rb').read()\n                            for _ in os.listdir(self.video_path)]\n\n        self.pipeline_name = 'pipe-gif'\n        self.pipeline_yml_path = os.path.join(self.dirname, 'yaml/%s.yml' % self.pipeline_name)\n        self.data_path = './test_chunkleveldb'\n        self.dump_path = os.path.join(self.dirname, 'indexer.bin')\n\n        self.init_db()\n\n    def test_pymode(self):\n        os.unsetenv('http_proxy')\n        os.unsetenv('https_proxy')\n        args = set_frontend_parser().parse_args([\n            '--dump_route', 'test.json'\n        ])\n\n        p_args = set_preprocessor_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', '5531',\n            '--socket_in', str(SocketType.PULL_CONNECT),\n            '--socket_out', str(SocketType.PUSH_BIND),\n            '--yaml_path', 'SentSplitPreprocessor'\n        ])\n\n        e_args = set_indexer_parser().parse_args([\n            '--port_in', str(p_args.port_out),\n            '--port_out', str(args.port_in),\n            '--socket_in', str(SocketType.PULL_CONNECT),\n            '--socket_out', str(SocketType.PUSH_CONNECT),\n            '--yaml_path', '!DictIndexer {gnes_config: {name: dummy_dict_indexer}}',\n        ])\n\n        with ServiceManager(IndexerService, e_args), \\\n             ServiceManager(PreprocessorService, p_args), \\\n             FrontendService(args), \\\n             grpc.insecure_channel('%s:%s' % (args.grpc_host, args.grpc_port),\n                                   options=[('grpc.max_send_message_length', 70 * 1024 * 1024),\n                                            ('grpc.max_receive_message_length', 70 * 1024 * 1024)]) as channel:\n            stub = gnes_pb2_grpc.GnesRPCStub(channel)\n            all_bytes = []\n            with open(os.path.join(self.dirname, '26-doc-chinese.txt'), 'r', encoding='utf8') as fp:\n                for v in fp:\n                    if v.strip():\n                        all_bytes.append(v.encode())\n            for r in stub.StreamCall(RequestGenerator.index(all_bytes)):\n                print(r)\n\n        bi = BaseIndexer.load('dummy_dict_indexer.bin')\n        self.assertEqual(bi.num_docs, 26)\n        print(bi.query([0]))\n\n    def tearDown(self):\n        if os.path.exists(self.data_path):\n            rmtree(self.data_path)\n        if os.path.exists('dummy_dict_indexer.bin'):\n            os.remove('dummy_dict_indexer.bin')\n\n    def init_db(self):\n        self.db = DirectoryIndexer(self.data_path)\n\n        self.d = gnes_pb2.Document()\n        self.d.doc_id = 0\n        self.d.raw_bytes = self.video_bytes[0]\n\n        preprocess = BasePreprocessor.load_yaml(self.pipeline_yml_path)\n        preprocess.apply(self.d)\n\n        self.db.add(list(range(len(self.video_bytes))), [self.d])\n        self.assertEqual(self.db.num_docs, len(self.video_bytes))\n\n    def test_add_docs(self):\n        # self.init_db()\n        self.assertTrue(os.path.exists(os.path.join(self.data_path, str(self.d.doc_id))))\n        self.assertEqual(len(self.d.chunks), len(os.listdir(os.path.join(self.data_path, str(self.d.doc_id)))) - 1)\n\n    def test_query_docs(self):\n        # self.init_db()\n\n        query_list = [0, 1, 2]\n        res = self.db.query(query_list)\n        num_non_empty = sum(1 for d in res if d)\n        self.assertEqual(num_non_empty, 1)\n"""
tests/test_dummy_transformer.py,0,"b""import os\nimport unittest\n\nimport grpc\n\nfrom gnes.cli.parser import set_frontend_parser, _set_loadable_service_parser, set_preprocessor_parser\nfrom gnes.proto import gnes_pb2_grpc, RequestGenerator\nfrom gnes.service.base import ServiceManager, SocketType\nfrom gnes.service.encoder import EncoderService\nfrom gnes.service.frontend import FrontendService\nfrom gnes.service.preprocessor import PreprocessorService\n\n\n@unittest.SkipTest\nclass TestEncoder(unittest.TestCase):\n\n    def setUp(self):\n        self.dirname = os.path.dirname(__file__)\n        self.transformer_yml = os.path.join(self.dirname, 'contrib', 'transformer.yml')\n        self.transformer_py = os.path.join(self.dirname, 'contrib', 'transformer.py')\n        os.environ['GNES_CONTRIB_MODULE'] = self.transformer_py\n        os.unsetenv('http_proxy')\n        os.unsetenv('https_proxy')\n\n    def test_pymode(self):\n        args = set_frontend_parser().parse_args([])\n\n        p_args = set_preprocessor_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', '5531',\n            '--socket_in', str(SocketType.PULL_CONNECT),\n            '--socket_out', str(SocketType.PUSH_BIND),\n            '--yaml_path', '!UnaryPreprocessor {parameters: {doc_type: 1}}'\n        ])\n\n        e_args = _set_loadable_service_parser().parse_args([\n            '--port_in', str(p_args.port_out),\n            '--port_out', str(args.port_in),\n            '--socket_in', str(SocketType.PULL_CONNECT),\n            '--socket_out', str(SocketType.PUSH_CONNECT),\n            '--yaml_path', self.transformer_yml,\n        ])\n\n        with ServiceManager(EncoderService, e_args), \\\n             ServiceManager(PreprocessorService, p_args), \\\n             ServiceManager(FrontendService, args), \\\n             grpc.insecure_channel('%s:%d' % (args.grpc_host, args.grpc_port),\n                                   options=[('grpc.max_send_message_length', 70 * 1024 * 1024),\n                                            ('grpc.max_receive_message_length', 70 * 1024 * 1024)]) as channel:\n            stub = gnes_pb2_grpc.GnesRPCStub(channel)\n            resp = stub.Call(list(RequestGenerator.index([b'hello world', b'goodbye!'], 1))[0])\n            self.assertEqual(resp.request_id, 0)\n"""
tests/test_dump_loads.py,0,"b""import os\nimport unittest\n\nimport numpy as np\n\nfrom gnes.encoder.base import PipelineEncoder\n\n\nclass TestDumpAndLoad(unittest.TestCase):\n    def setUp(self):\n        dirname = os.path.dirname(__file__)\n        self.lopq_yaml_np = os.path.join(dirname, 'yaml', 'lopq-encoder-2-np.yml')\n        self.dump_path = os.path.join(dirname, 'encoder.bin')\n        self.test_vecs = np.random.random([1000, 100]).astype('float32')\n\n    def tearDown(self):\n        if os.path.exists(self.dump_path):\n            os.remove(self.dump_path)\n\n    def _test(self, yaml_fp):\n        lopq = PipelineEncoder.load_yaml(yaml_fp)\n        lopq.train(self.test_vecs)\n        out = lopq.encode(self.test_vecs)\n        lopq.dump(self.dump_path)\n        self.assertTrue(os.path.exists(self.dump_path))\n        lopq2 = PipelineEncoder.load(self.dump_path)\n        out2 = lopq2.encode(self.test_vecs)\n        self.assertEqual(out, out2)\n\n    def test_dumpload_np(self):\n        self._test(self.lopq_yaml_np)\n"""
tests/test_encoder.py,1,"b""import os\nimport unittest\n\nimport numpy as np\nfrom numpy.testing import assert_allclose\n\nfrom gnes.encoder.base import PipelineEncoder\nfrom gnes.encoder.numeric.pca import PCALocalEncoder\nfrom gnes.encoder.numeric.pq import PQEncoder\nfrom gnes.encoder.numeric.tf_pq import TFPQEncoder\n\n\nclass TestPCA(unittest.TestCase):\n    def setUp(self):\n        self.test_vecs = np.random.random([1000, 100]).astype('float32')\n        dirname = os.path.dirname(__file__)\n        self.lopq_yaml_np = os.path.join(dirname, 'yaml', 'lopq-encoder-2-np.yml')\n        self.lopq_yaml_tf = os.path.join(dirname, 'yaml', 'lopq-encoder-2-tf.yml')\n        self.lopq_yaml_np2 = os.path.join(dirname, 'yaml', 'lopq-encoder-3.yml')\n\n    def test_pq_assert(self):\n        self._test_pq_assert(PQEncoder)\n        self._test_pq_assert(TFPQEncoder)\n\n    def test_pq_tfpq_identity(self):\n        def _test_pq_tfpq_identity(pq1, pq2):\n            pq1.train(self.test_vecs)\n            out1 = pq1.encode(self.test_vecs)\n            pq2._copy_from(pq1)\n            out2 = pq2.encode(self.test_vecs)\n            assert_allclose(out1, out2)\n\n        _test_pq_tfpq_identity(PQEncoder(10), TFPQEncoder(10))\n        _test_pq_tfpq_identity(TFPQEncoder(10), PQEncoder(10))\n\n    def _test_pq_assert(self, cls):\n        self.assertRaises(AssertionError, cls, 100, 0)\n        self.assertRaises(AssertionError, cls, 100, 256)\n\n        pq = cls(8)\n        self.assertRaises(AssertionError, pq.train, self.test_vecs)\n\n        pq = cls(101)\n        self.assertRaises(AssertionError, pq.train, self.test_vecs)\n\n    def _simple_assert(self, out, num_bytes, num_clusters):\n        self.assertEqual(bytes, type(out))\n        self.assertEqual(self.test_vecs.shape[0] * num_bytes, len(out))\n        self.assertTrue(np.all(np.frombuffer(out, np.uint8) <= num_clusters))\n\n    def test_assert_pca(self):\n        self.assertRaises(AssertionError, PCALocalEncoder, 8, 3)\n        self.assertRaises(AssertionError, PCALocalEncoder, 2, 3)\n\n        pca = PCALocalEncoder(100, 2)\n        self.assertRaises(AssertionError, pca.train, self.test_vecs)\n\n        pca = PCALocalEncoder(8, 2)\n        self.assertRaises(AssertionError, pca.train, self.test_vecs[:7])\n\n        pca.train(self.test_vecs)\n        out = pca.encode(self.test_vecs)\n        self.assertEqual(out.shape[1], 8)\n        self.assertEqual(out.shape[0], self.test_vecs.shape[0])\n\n    def test_train_pca(self):\n        num_bytes = 8\n        num_clusters = 11\n        lopq = PipelineEncoder.load_yaml(self.lopq_yaml_np2)\n        lopq.train(self.test_vecs)\n        out = lopq.encode(self.test_vecs)\n        self._simple_assert(out, num_bytes, num_clusters)\n\n    # def test_train_pca_assert(self):\n    #     # from PCA\n    #     self.assertRaises(AssertionError, LOPQEncoder, num_bytes=100, pca_output_dim=20)\n    #     # from PCA\n    #     self.assertRaises(AssertionError, LOPQEncoder, num_bytes=7, pca_output_dim=20)\n    #     # from LOPQ, cluster too large\n    #     self.assertRaises(AssertionError, LOPQEncoder, num_bytes=4, pca_output_dim=20, cluster_per_byte=256)\n\n    def test_encode_backend(self):\n        num_bytes = 8\n        lopq = PipelineEncoder.load_yaml(self.lopq_yaml_tf)\n        lopq.train(self.test_vecs)\n        out = lopq.encode(self.test_vecs)\n        self._simple_assert(out, num_bytes, 255)\n\n        lopq2 = PipelineEncoder.load_yaml(self.lopq_yaml_np)\n        lopq2.train(self.test_vecs)\n        out = lopq2.encode(self.test_vecs)\n        self._simple_assert(out, num_bytes, 255)\n\n        # copy from lopq\n        lopq2._copy_from(lopq)\n        out2 = lopq2.encode(self.test_vecs)\n        self._simple_assert(out, num_bytes, 255)\n\n        self.assertEqual(out, out2)\n\n    def test_encode_batching(self):\n        num_bytes = 8\n        lopq = PipelineEncoder.load_yaml(self.lopq_yaml_tf)\n        lopq.train(self.test_vecs)\n        out = lopq.encode(self.test_vecs, batch_size=32)\n        self._simple_assert(out, num_bytes, 255)\n        out2 = lopq.encode(self.test_vecs, batch_size=64)\n        self.assertEqual(out, out2)\n\n    # def test_num_cluster(self):\n    #     def _test_num_cluster(num_bytes, num_cluster, backend):\n    #         lopq = LOPQEncoder(num_bytes,\n    #                            cluster_per_byte=num_cluster,\n    #                            pca_output_dim=20, pq_backend=backend)\n    #         lopq.train(self.test_vecs)\n    #         out = lopq.encode(self.test_vecs)\n    #         self._simple_assert(out, num_bytes, num_cluster)\n    #\n    #     _test_num_cluster(10, 3, 'numpy')\n    #     _test_num_cluster(10, 3, 'tensorflow')\n    #     _test_num_cluster(10, 5, 'numpy')\n    #     _test_num_cluster(10, 5, 'tensorflow')\n"""
tests/test_encoder_service.py,0,"b""import os\nimport unittest\n\nimport numpy as np\n\nfrom gnes.cli.parser import set_encoder_parser, _set_client_parser\nfrom gnes.client.base import ZmqClient\nfrom gnes.proto import gnes_pb2, array2blob\nfrom gnes.service.base import ServiceManager\nfrom gnes.service.encoder import EncoderService\nfrom gnes.encoder.base import BaseEncoder\n\n\nclass TestEncoder(BaseEncoder):\n\n    def encode(self, x):\n        return np.array(x)\n\n\nclass TestEncoderService(unittest.TestCase):\n\n    def setUp(self):\n        self.test_numeric = np.random.randint(0, 255, (1000, 1024)).astype('float32')\n\n    def test_empty_service(self):\n        args = set_encoder_parser().parse_args(['--yaml_path', '!TestEncoder {gnes_config: {name: EncoderService, is_trained: true}}'])\n        c_args = _set_client_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', str(args.port_in)])\n\n        with ServiceManager(EncoderService, args), ZmqClient(c_args) as client:\n            msg = gnes_pb2.Message()\n            d = msg.request.index.docs.add()\n            d.doc_type = gnes_pb2.Document.IMAGE\n\n            c = d.chunks.add()\n            c.blob.CopyFrom(array2blob(self.test_numeric))\n\n            client.send_message(msg)\n            r = client.recv_message()\n            self.assertEqual(len(r.request.index.docs), 1)\n            self.assertEqual(r.response.index.status, gnes_pb2.Response.SUCCESS)\n\n    def tearDown(self):\n        if os.path.exists('EncoderService.bin'):\n            os.remove('EncoderService.bin')\n"""
tests/test_euclidean_indexer.py,0,"b""import os\nimport unittest\n\nimport numpy as np\n\nfrom gnes.indexer.chunk.faiss import FaissIndexer\n\n\nclass TestEUIndexer(unittest.TestCase):\n    def setUp(self):\n        self.toy_query = np.random.random([1000, 20]).astype(np.float32)\n        self.toy_label = np.random.randint(0, 1e9, [1000, 2]).tolist()\n        self.add_query = np.random.random([1000, 20]).astype(np.float32)\n        self.add_label = np.random.randint(0, 1e9, [1000, 2]).tolist()\n\n        self.sub_query = self.toy_query[:10]\n        dirname = os.path.dirname(__file__)\n        self.dump_path = os.path.join(dirname, 'test_eu_faiss')\n\n    def tearDown(self):\n        if os.path.exists(self.dump_path):\n            os.remove(self.dump_path)\n\n    def test_add(self):\n        fd = FaissIndexer(20, 'HNSW32', self.dump_path)\n        fd.add(self.toy_label, self.toy_query, [1.] * len(self.toy_label))\n        self.assertEqual(fd.num_chunks, self.toy_query.shape[0])\n        fd.add(self.add_label, self.add_query, [1.] * len(self.add_label))\n        self.assertEqual(fd.num_chunks,\n                         self.toy_query.shape[0] + self.add_query.shape[0])\n\n    def test_query(self):\n        fd = FaissIndexer(20, 'HNSW32', self.dump_path)\n        fd.add(self.toy_label, self.toy_query, [1.] * len(self.toy_label))\n        ret = fd.query(self.sub_query, top_k=5)\n        self.assertEqual(len(ret), self.sub_query.shape[0])\n        self.assertEqual(len(ret[0]), 5)\n\n    def test_dump_load(self):\n        with FaissIndexer(20, 'HNSW32', self.dump_path) as tmp:\n            tmp.add(self.toy_label, self.toy_query, [1.] * len(self.toy_label))\n            tmp.dump()\n\n        with FaissIndexer.load(tmp.dump_full_path) as fd:\n            ret = fd.query(self.sub_query, top_k=2)\n            self.assertEqual(len(ret), self.sub_query.shape[0])\n            self.assertEqual(len(ret[0]), 2)\n"""
tests/test_ffmpeg_tools.py,0,"b""import copy\nimport os\nimport unittest\n\nfrom gnes.preprocessor.io_utils import ffmpeg\nfrom gnes.preprocessor.io_utils import video\nfrom gnes.preprocessor.io_utils import gif\nfrom gnes.preprocessor.io_utils import audio\n\n\nclass TestFFmpeg(unittest.TestCase):\n    def setUp(self):\n        self.dirname = os.path.dirname(__file__)\n\n        self.video_path = os.path.join(self.dirname, 'videos', 'test.mp4')\n        self.frames = video.capture_frames(input_fn=self.video_path, fps=10, scale='640:360')\n\n    def test_probe(self):\n        probe = ffmpeg.probe(self.video_path)\n        self.assertEqual(probe['height'], 720)\n        self.assertEqual(probe['width'], 1280)\n        self.assertEqual(probe['fps'], 25.0)\n\n\n    def test_get_media_meta(self):\n        meta1 = ffmpeg.get_media_meta(input_fn=self.video_path)\n        with open(self.video_path, 'rb') as f:\n            data = f.read()\n            meta2 = ffmpeg.get_media_meta(input_data=data)\n        self.assertEqual(meta1['frame_width'], meta2['frame_width'])\n        self.assertEqual(meta1['frame_height'], meta2['frame_height'])\n\n    def test_capture_frames(self):\n        frames1 = video.capture_frames(input_fn=self.video_path, fps=10, scale='640:-2')\n        sub_frames = video.capture_frames(input_fn=self.video_path, fps=10, scale='640:-2', vframes=5)\n        self.assertEqual((sub_frames == frames1[:5]).all(), True)\n\n        with open(self.video_path, 'rb') as f:\n            data = f.read()\n            frames2 = video.capture_frames(input_data=data, fps=10, scale='-1:360')\n\n        self.assertEqual(frames1.shape, frames2.shape)\n\n    def test_scale_video(self):\n        out = video.scale_video(input_fn=self.video_path, scale='640:360')\n        meta = ffmpeg.get_media_meta(input_data=out, input_options={'format': 'mp4'})\n        self.assertEqual(meta['frame_width'], 640)\n        self.assertEqual(meta['frame_height'], 360)\n\n    def test_encode_video(self):\n        video_data = video.encode_video(images=self.frames)\n        meta = ffmpeg.get_media_meta(input_data=video_data, input_options={'format': 'mp4'})\n        self.assertEqual(meta['frame_width'], 640)\n        self.assertEqual(meta['frame_height'], 360)\n\n    def test_gif_encode(self):\n        gif_data = gif.encode_video(images=self.frames, frame_rate=10)\n        frames = gif.capture_frames(input_data=gif_data)\n        sub_frames = gif.capture_frames(input_data=gif_data, vframes=5)\n        self.assertEqual(self.frames.shape, frames.shape)\n        self.assertEqual((sub_frames == frames[:5]).all(), True)\n\n    def test_capture_audio(self):\n        audio_data1 = audio.capture_audio(input_fn=self.video_path)\n        with open(self.video_path, 'rb') as f:\n            data = f.read()\n            audio_data2 = audio.capture_audio(input_data=data)\n\n            self.assertEqual(audio_data1.shape, audio_data2.shape)\n\n    def test_split_audio(self):\n        chunks1 = audio.split_audio(input_fn=self.video_path)\n        with open(self.video_path, 'rb') as f:\n            data = f.read()\n            chunks2 = audio.split_audio(input_data=data)\n            self.assertEqual(len(chunks1), len(chunks2))\n            self.assertEqual(chunks1[0].shape, chunks2[0].shape)\n"""
tests/test_flair_encoder.py,0,"b""import os\nimport unittest\n\nfrom gnes.encoder.text.flair import FlairEncoder\n\n\nclass TestFlairEncoder(unittest.TestCase):\n\n    def setUp(self):\n        dirname = os.path.dirname(__file__)\n        self.dump_path = os.path.join(dirname, 'flair_encoder.bin')\n\n        self.test_str = []\n        with open(os.path.join(dirname, 'sonnets.txt')) as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    self.test_str.append(line)\n\n        self.flair_encoder = FlairEncoder(model_name=os.environ.get('FLAIR_CI_MODEL'))\n\n    @unittest.SkipTest\n    def test_encoding(self):\n        vec = self.flair_encoder.encode(self.test_str[:2])\n        print(vec.shape)\n        self.assertEqual(vec.shape[0], 2)\n        self.assertEqual(vec.shape[1], 4196)\n\n    @unittest.SkipTest\n    def test_dump_load(self):\n        self.flair_encoder.dump(self.dump_path)\n\n        flair_encoder2 = FlairEncoder.load(self.dump_path)\n        vec = flair_encoder2.encode(self.test_str)\n        self.assertEqual(vec.shape[0], len(self.test_str))\n        self.assertEqual(vec.shape[1], 512)\n\n    def tearDown(self):\n        if os.path.exists(self.dump_path):\n            os.remove(self.dump_path)\n"""
tests/test_frame_selector.py,0,"b'import unittest\nfrom gnes.proto import gnes_pb2, array2blob, blob2array\nfrom gnes.preprocessor.video.frame_select import FrameSelectPreprocessor\nimport numpy as np\nimport copy\n\n\nclass TestFrameSelector(unittest.TestCase):\n    def setUp(self) -> None:\n        self.doc = gnes_pb2.Document()\n\n        c1 = self.doc.chunks.add()\n        c1.blob.CopyFrom(array2blob(np.array([[1,2,3], [2,3,4]])))\n\n        c2 = self.doc.chunks.add()\n        c2.blob.CopyFrom(array2blob(np.array([[1,2,3], [2,3,4], [1,2,3]])))\n\n        c3 = self.doc.chunks.add()\n        c3.blob.CopyFrom(array2blob(np.array([[1,2,3], [2,3,4], [1,2,3], [2,3,4]])))\n\n    def test_emtpy_document(self):\n        frame_selector = FrameSelectPreprocessor(sframes=-1)\n        frame_selector.apply(gnes_pb2.Document())\n\n    def test_big_sframe(self):\n        doc = copy.deepcopy(self.doc)\n        frame_selector = FrameSelectPreprocessor(sframes=100)\n        frame_selector.apply(doc)\n\n    def test_get_frames(self):\n        doc = copy.deepcopy(self.doc)\n        frame_selector = FrameSelectPreprocessor(sframes=3)\n        frame_selector.apply(doc)\n        for idx, chunk in enumerate(doc.chunks):\n            if idx == 0:\n                self.assertEqual(blob2array(chunk.blob).shape[0], 2)\n            else:\n                self.assertEqual(blob2array(chunk.blob).shape[0], 3)\n\n    def test_get_one_frame(self):\n        doc = copy.deepcopy(self.doc)\n        frame_selector = FrameSelectPreprocessor(sframes=1)\n        frame_selector.apply(doc)\n        for chunk in doc.chunks:\n            self.assertEqual(blob2array(chunk.blob).shape[0], 1)\n\n\n\n\n'"
tests/test_gif.py,0,"b""import copy\nimport os\nimport unittest\n\nfrom gnes.preprocessor.base import BasePreprocessor\nfrom gnes.preprocessor.video.ffmpeg import FFmpegVideoSegmentor\nfrom gnes.proto import gnes_pb2\n\n\nclass TestPartition(unittest.TestCase):\n    def setUp(self):\n        self.dirname = os.path.dirname(__file__)\n        self.p3_name = 'pipe-gif'\n        self.pipeline_path = os.path.join(self.dirname, 'yaml/%s.yml' % self.p3_name)\n        self.ffmpeg_yaml_path = os.path.join(self.dirname, 'yaml/preprocessor-ffmpeg2.yml')\n        self.video_path = os.path.join(self.dirname, 'videos')\n        self.video_bytes = [open(os.path.join(self.video_path, _), 'rb').read()\n                            for _ in os.listdir(self.video_path)]\n\n    def test_gif_pipelinepreproces(self):\n        d = gnes_pb2.Document()\n        d.raw_bytes = self.video_bytes[0]\n        d_ = copy.deepcopy(d)\n\n        p3 = FFmpegVideoSegmentor.load_yaml(self.ffmpeg_yaml_path)\n        p3.apply(d)\n\n        p4 = BasePreprocessor.load_yaml(self.pipeline_path)\n        p4.apply(d_)\n\n        self.assertEqual(len(d.chunks), len(d_.chunks))\n"""
tests/test_gnes_flow.py,0,"b""import os\nimport unittest\n\nfrom gnes.cli.parser import set_client_cli_parser\nfrom gnes.flow import Flow, FlowBuildLevelMismatch\nfrom gnes.flow.base import BaseIndexFlow, BaseQueryFlow\n\n\nclass TestGNESFlow(unittest.TestCase):\n\n    def setUp(self):\n        self.dirname = os.path.dirname(__file__)\n        self.test_file = os.path.join(self.dirname, 'sonnets_small.txt')\n        self.yamldir = os.path.join(self.dirname, 'yaml')\n        self.dump_flow_path = os.path.join(self.dirname, 'test-flow.bin')\n        self.index_args = set_client_cli_parser().parse_args([\n            '--mode', 'index',\n            '--txt_file', self.test_file,\n            '--batch_size', '4'\n        ])\n        os.unsetenv('http_proxy')\n        os.unsetenv('https_proxy')\n        self.test_dir = os.path.join(self.dirname, 'test_flow')\n        self.indexer1_bin = os.path.join(self.test_dir, 'my_faiss_indexer.bin')\n        self.indexer2_bin = os.path.join(self.test_dir, 'my_fulltext_indexer.bin')\n        self.encoder_bin = os.path.join(self.test_dir, 'my_transformer.bin')\n        if os.path.exists(self.test_dir):\n            self.tearDown()\n        os.mkdir(self.test_dir)\n\n        os.environ['TEST_WORKDIR'] = self.test_dir\n\n    def tearDown(self):\n        for k in [self.indexer1_bin, self.indexer2_bin, self.encoder_bin, self.dump_flow_path]:\n            if os.path.exists(k):\n                os.remove(k)\n        os.rmdir(self.test_dir)\n\n    def test_flow1(self):\n        f = (Flow(check_version=False, route_table=True)\n             .add_router(yaml_path='BaseRouter'))\n        g = f.add_router(yaml_path='BaseRouter')\n\n        print('f: %r g: %r' % (f, g))\n        g.build()\n        print(g.to_mermaid())\n\n        f = f.add_router(yaml_path='BaseRouter')\n        g = g.add_router(yaml_path='BaseRouter')\n\n        print('f: %r g: %r' % (f, g))\n        f.build()\n        print(f.to_mermaid())\n        g.build()\n\n    def test_flow1_ctx_empty(self):\n        f = (Flow(check_version=False, route_table=True)\n             .add_router(yaml_path='BaseRouter'))\n        with f(backend='process'):\n            pass\n\n    def test_flow1_ctx(self):\n        flow = (Flow(check_version=False, route_table=False)\n                .add_router(yaml_path='BaseRouter'))\n        with flow(backend='process', copy_flow=True) as f, open(self.test_file) as fp:\n            f.index(txt_file=self.test_file, batch_size=4)\n            f.train(txt_file=self.test_file, batch_size=4)\n\n        with flow(backend='process', copy_flow=True) as f:\n            # change the flow inside build shall fail\n            f = f.add_router(yaml_path='BaseRouter')\n            self.assertRaises(FlowBuildLevelMismatch, f.index, txt_file=self.test_file, batch_size=4)\n\n        print(flow.build(backend=None).to_mermaid())\n\n    def test_flow2(self):\n        f = (Flow(check_version=False, route_table=True)\n             .add_router(yaml_path='BaseRouter')\n             .add_router(yaml_path='BaseRouter')\n             .add_router(yaml_path='BaseRouter')\n             .add_router(yaml_path='BaseRouter')\n             .add_router(yaml_path='BaseRouter')\n             .add_router(yaml_path='BaseRouter')\n             .add_router(yaml_path='BaseRouter')\n             .add_router(yaml_path='BaseRouter')\n             .build(backend=None))\n        print(f._service_edges)\n        print(f.to_mermaid())\n\n    def test_flow3(self):\n        f = (Flow(check_version=False, route_table=True)\n             .add_router(name='r0', send_to=Flow.Frontend, yaml_path='BaseRouter')\n             .add_router(name='r1', recv_from=Flow.Frontend, yaml_path='BaseRouter')\n             .build(backend=None))\n        print(f._service_edges)\n        print(f.to_mermaid())\n\n    def test_flow4(self):\n        f = (Flow(check_version=False, route_table=True)\n             .add_router(name='r0', yaml_path='BaseRouter')\n             .add_router(name='r1', recv_from=Flow.Frontend, yaml_path='BaseRouter')\n             .add_router(name='reduce', recv_from=['r0', 'r1'], yaml_path='BaseRouter')\n             .build(backend=None))\n        print(f._service_edges)\n        print(f.to_mermaid())\n\n    def test_flow5(self):\n        f = (Flow(check_version=False, route_table=True)\n             .add_preprocessor(name='prep', yaml_path='SentSplitPreprocessor')\n             .add_encoder(yaml_path='PyTorchTransformers')\n             .add_indexer(name='vec_idx', yaml_path='NumpyIndexer')\n             .add_indexer(name='doc_idx', yaml_path='DictIndexer', recv_from='prep')\n             .add_router(name='sync_barrier', yaml_path='BaseReduceRouter',\n                         num_part=2, recv_from=['vec_idx', 'doc_idx'])\n             .build(backend=None))\n        print(f._service_edges)\n        print(f.to_mermaid())\n        # f.to_jpg()\n\n    def test_flow_replica_pot(self):\n        f = (Flow(check_version=False, route_table=True)\n             .add_preprocessor(name='prep', yaml_path='SentSplitPreprocessor', replicas=4)\n             .add_encoder(yaml_path='PyTorchTransformers', replicas=3)\n             .add_indexer(name='vec_idx', yaml_path='NumpyIndexer', replicas=2)\n             .add_indexer(name='doc_idx', yaml_path='DictIndexer', recv_from='prep', replicas=2)\n             .add_router(name='sync_barrier', yaml_path='BaseReduceRouter',\n                         num_part=2, recv_from=['vec_idx', 'doc_idx'])\n             .build(backend=None))\n        print(f.to_mermaid())\n        print(f.to_url(left_right=False))\n        print(f.to_url(left_right=True))\n\n    def _test_index_flow(self, backend):\n        for k in [self.indexer1_bin, self.indexer2_bin, self.encoder_bin]:\n            self.assertFalse(os.path.exists(k))\n\n        flow = (Flow(check_version=False, route_table=False)\n                .add_preprocessor(name='prep', yaml_path='SentSplitPreprocessor')\n                .add_encoder(yaml_path=os.path.join(self.dirname, 'yaml/flow-transformer.yml'), replicas=3)\n                .add_indexer(name='vec_idx', yaml_path=os.path.join(self.dirname, 'yaml/flow-vecindex.yml'))\n                .add_indexer(name='doc_idx', yaml_path=os.path.join(self.dirname, 'yaml/flow-dictindex.yml'),\n                             recv_from='prep')\n                .add_router(name='sync_barrier', yaml_path='BaseReduceRouter',\n                            num_part=2, recv_from=['vec_idx', 'doc_idx']))\n\n        with flow.build(backend=backend) as f:\n            f.index(txt_file=self.test_file, batch_size=20)\n\n        for k in [self.indexer1_bin, self.indexer2_bin]:\n            self.assertTrue(os.path.exists(k))\n\n    def _test_query_flow(self, backend):\n        flow = (Flow(check_version=False, route_table=False)\n                .add_preprocessor(name='prep', yaml_path='SentSplitPreprocessor')\n                .add_encoder(yaml_path=os.path.join(self.dirname, 'yaml/flow-transformer.yml'), replicas=3)\n                .add_indexer(name='vec_idx', yaml_path=os.path.join(self.dirname, 'yaml/flow-vecindex.yml'))\n                .add_router(name='scorer', yaml_path=os.path.join(self.dirname, 'yaml/flow-score.yml'))\n                .add_indexer(name='doc_idx', yaml_path=os.path.join(self.dirname, 'yaml/flow-dictindex.yml')))\n\n        with flow.build(backend=backend) as f, open(self.test_file, encoding='utf8') as fp:\n            f.query(bytes_gen=[v.encode() for v in fp][:3])\n\n    # @unittest.SkipTest\n    def test_index_query_flow(self):\n        self._test_index_flow('thread')\n        self._test_query_flow('thread')\n\n    def test_indexe_query_flow_proc(self):\n        self._test_index_flow('process')\n        self._test_query_flow('process')\n\n    def test_query_flow_plot(self):\n        flow = (Flow(check_version=False, route_table=False)\n                .add_preprocessor(name='prep', yaml_path='SentSplitPreprocessor', replicas=2)\n                .add_encoder(yaml_path=os.path.join(self.dirname, 'yaml/flow-transformer.yml'), replicas=3)\n                .add_indexer(name='vec_idx', yaml_path=os.path.join(self.dirname, 'yaml/flow-vecindex.yml'),\n                             replicas=4)\n                .add_router(name='scorer', yaml_path=os.path.join(self.dirname, 'yaml/flow-score.yml'))\n                .add_indexer(name='doc_idx', yaml_path=os.path.join(self.dirname, 'yaml/flow-dictindex.yml')))\n        print(flow.build(backend=None).to_url())\n\n    def test_flow_add_set(self):\n        f = (Flow(check_version=False, route_table=True)\n             .add_preprocessor(name='prep', yaml_path='SentSplitPreprocessor', replicas=4)\n             .add_encoder(yaml_path='PyTorchTransformers', replicas=3)\n             .add_indexer(name='vec_idx', yaml_path='NumpyIndexer', replicas=2)\n             .add_indexer(name='doc_idx', yaml_path='DictIndexer', recv_from='prep', replicas=2)\n             .add_router(name='sync_barrier', yaml_path='BaseReduceRouter',\n                         num_part=2, recv_from=['vec_idx', 'doc_idx'])\n             .build(backend=None))\n\n        print(f.to_url())\n        print(f.set('prep', replicas=1).build(backend=None).to_url())\n        # make it as query flow\n\n        f1 = (f\n              .remove('sync_barrier')\n              .remove('doc_idx')\n              .set_last_service('vec_idx')\n              .add_router('scorer', yaml_path=os.path.join(self.dirname, 'yaml/flow-score.yml'))\n              .add_indexer('doc_idx', yaml_path='DictIndexer', replicas=2)\n              .build(backend=None))\n\n        print(f1.to_url())\n\n        # another way to convert f to an index flow\n\n        f2 = (f\n              .set_last_service('vec_idx')\n              .add_router('scorer', yaml_path=os.path.join(self.dirname, 'yaml/flow-score.yml'))\n              .set('doc_idx', recv_from='scorer', yaml_path='DictIndexer', replicas=2, clear_old_attr=True)\n              .remove('sync_barrier')\n              .set_last_service('doc_idx')\n              .build(backend=None))\n\n        print(f2.to_url())\n\n        self.assertEqual(f1, f2)\n\n        self.assertNotEqual(f1, f2.add_router('dummy', yaml_path='BaseRouter'))\n\n        print(f1.to_python_code())\n        print(f.to_python_code())\n\n        f1.dump(self.dump_flow_path)\n        f3 = Flow.load(self.dump_flow_path)\n        self.assertEqual(f1, f3)\n\n        print(f1.to_swarm_yaml())\n\n    def test_common_flow(self):\n        print(BaseIndexFlow().build(backend=None).to_url())\n        print(BaseQueryFlow().build(backend=None).to_url())\n"""
tests/test_grpc_service.py,0,"b""import os\nimport unittest\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport grpc\n\nfrom gnes.cli.parser import set_grpc_service_parser, set_frontend_parser\nfrom gnes.proto import gnes_pb2_grpc, RequestGenerator\nfrom gnes.service.frontend import FrontendService\nfrom gnes.service.grpc import GRPCService\nfrom tests.proto_s import dummy_pb2_grpc\n\n\nclass DummyServer:\n    def __init__(self, bind_address):\n        self.server = grpc.server(\n            ThreadPoolExecutor(max_workers=1),\n            options=[('grpc.max_send_message_length', 1 * 1024 * 1024),\n                     ('grpc.max_receive_message_length', 1 * 1024 * 1024)])\n        dummy_pb2_grpc.add_DummyGRPCServiceServicer_to_server(self.GNESServicer(), self.server)\n        self.bind_address = bind_address\n        self.server.add_insecure_port(self.bind_address)\n\n    def __enter__(self):\n        self.server.start()\n        print('dummy server is listening at: %s' % self.bind_address)\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.server.stop(None)\n\n    class GNESServicer(dummy_pb2_grpc.DummyGRPCServiceServicer):\n\n        def dummyAPI(self, request, context):\n            print('the dummy server received something: %s' % request)\n            return request\n\n\nclass TestGRPCService(unittest.TestCase):\n    def setUp(self):\n        os.unsetenv('http_proxy')\n        os.unsetenv('https_proxy')\n        self.dirname = os.path.dirname(__file__)\n\n        self.s_args = set_grpc_service_parser().parse_args([\n            '--grpc_port', '5678',\n            '--grpc_host', '127.0.0.1',\n            '--pb2_path', os.path.join(self.dirname, 'proto', 'dummy_pb2.py'),\n            '--pb2_grpc_path', os.path.join(self.dirname, 'proto', 'dummy_pb2_grpc.py'),\n            '--stub_name', 'DummyGRPCServiceStub',\n            '--api_name', 'dummyAPI'\n        ])\n\n        self.args = set_frontend_parser().parse_args([\n            '--grpc_host', '127.0.0.1',\n            '--grpc_port', '9999',\n            '--port_in', str(self.s_args.port_out),\n            '--port_out', str(self.s_args.port_in),\n\n        ])\n\n    def test_grpc_empty_service(self):\n        with DummyServer('%s:%d' % (self.s_args.grpc_host, self.s_args.grpc_port)), GRPCService(self.s_args):\n            pass\n\n    @unittest.SkipTest\n    def test_grpc_real_service(self):\n        # to fix\n\n        with DummyServer('%s:%d' % (self.s_args.grpc_host, self.s_args.grpc_port)), GRPCService(\n                self.s_args), FrontendService(self.args), grpc.insecure_channel(\n            '%s:%s' % (self.args.grpc_host, self.args.grpc_port),\n            options=[('grpc.max_send_message_length', 70 * 1024 * 1024),\n                     ('grpc.max_receive_message_length', 70 * 1024 * 1024)]) as channel:\n            stub = gnes_pb2_grpc.GnesRPCStub(channel)\n            resp = stub.Call(list(RequestGenerator.query(b'abc', 1))[0])\n            self.assertEqual(resp.request_id, 0)  # idx start with 0, but +1 for final FLUSH\n"""
tests/test_hash_encoder.py,0,"b""import os\nimport unittest\n\nimport numpy as np\n\nfrom gnes.encoder.base import PipelineEncoder\nfrom gnes.encoder.numeric.hash import HashEncoder\n\n\nclass TestHash(unittest.TestCase):\n    def setUp(self):\n        self.num_bytes = 16\n        self.num_bits = 8\n        self.num_idx = 2\n        self.kmeans_clusters = 10\n        self.x = 1000\n        self.y = 128\n        self.test_data = np.random.random([self.x, self.y]).astype(np.float32)\n        dirname = os.path.dirname(__file__)\n        self.hash_yaml = os.path.join(dirname, 'yaml', 'hash-encoder.yml')\n\n    def test_train_pred(self):\n        m = HashEncoder(self.num_bytes, self.num_bits,\n                        self.num_idx, self.kmeans_clusters)\n        m.train(self.test_data)\n        self.assertEquals(self.num_idx, m.centroids.shape[1])\n        self.assertEquals(self.kmeans_clusters, m.centroids.shape[2])\n        self.assertEqual(self.y, m.centroids.shape[3])\n\n        self.assertEqual(self.num_bytes, len(m.hash_cores))\n\n        out = m.encode(self.test_data)\n        self.assertEqual(self.x, out.shape[0])\n        self.assertEqual(self.num_idx + self.num_bytes, out.shape[1])\n        self.assertEqual(np.uint32, out.dtype)\n\n    def test_yaml_load(self):\n        pca_hash = PipelineEncoder.load_yaml(self.hash_yaml)\n        pca_hash.train(self.test_data)\n        out = pca_hash.encode(self.test_data)\n        self.assertEqual(self.x, out.shape[0])\n        self.assertEqual(self.num_idx + self.num_bytes, out.shape[1])\n"""
tests/test_hash_indexer.py,0,"b""import os\nimport unittest\n\nimport numpy as np\n\nfrom gnes.indexer.chunk.hbindexer import HBIndexer\n\n\nclass TestMHIndexer(unittest.TestCase):\n\n    def setUp(self):\n        self.num_clusters = 100\n        self.num_bytes = 16\n        self.n_idx = 1\n        self.n = 100\n\n        self.test_label = [(_, 1) for _ in range(self.n)]\n        t = np.random.randint(0, 100, size=[self.n, self.n_idx + self.num_bytes])\n        self.test_data = t.astype(np.uint32)\n        self.weights = [1.] * len(self.test_label)\n        self.data_path = 'test_path'\n        self.dump_path = './dump.bin'\n\n        self.query = self.test_data\n\n    def tearDown(self):\n        if os.path.exists(self.dump_path):\n            os.remove(self.dump_path)\n\n    def test_add_query(self):\n        m = HBIndexer(self.num_clusters, self.num_bytes, self.n_idx, self.data_path)\n        m.add(self.test_label, self.test_data, self.weights)\n        res = m.query(self.query, 1)\n        self.assertEqual(len(res), self.n)\n        s = sum([1 for i in range(self.n) if i in [_[0][0] for _ in res[i]]])\n        self.assertEqual(s, self.n)\n        m.close()\n\n    def test_dump_load(self):\n        m = HBIndexer(self.num_clusters, self.num_bytes, self.n_idx, self.data_path)\n        m.add(self.test_label, self.test_data, self.weights)\n        m.dump(self.dump_path)\n        m.close()\n        self.assertTrue(os.path.exists(self.dump_path))\n        m2 = HBIndexer.load(self.dump_path)\n        res = m2.query(self.query, 1)\n        s = sum([1 for i in range(self.n) if i in [_[0][0] for _ in res[i]]])\n        self.assertEqual(s, self.n)\n"""
tests/test_healthcheck.py,0,"b""import copy\nimport os\nimport time\nimport unittest\n\nfrom gnes.cli.api import healthcheck\nfrom gnes.cli.parser import set_router_parser, set_healthcheck_parser\nfrom gnes.service.base import ServiceManager\nfrom gnes.service.router import RouterService\n\n\nclass TestHealthCheck(unittest.TestCase):\n\n    def test_health_check(self):\n        a = set_router_parser().parse_args([\n            '--yaml_path', 'BaseRouter',\n        ])\n        a1 = copy.deepcopy(a)\n        b = set_healthcheck_parser().parse_args([\n            '--port', str(a.port_ctrl)\n        ])\n\n        # before - fail\n        with self.assertRaises(SystemExit) as cm:\n            healthcheck(b)\n\n        self.assertEqual(cm.exception.code, 1)\n\n        # running - success\n        with self.assertRaises(SystemExit) as cm:\n            with RouterService(a):\n                time.sleep(2)\n                healthcheck(b)\n        self.assertEqual(cm.exception.code, 0)\n\n        # running - managerservice - success\n        with self.assertRaises(SystemExit) as cm:\n            with ServiceManager(RouterService, a1):\n                time.sleep(2)\n                healthcheck(b)\n        self.assertEqual(cm.exception.code, 0)\n\n        # after - fail\n        with self.assertRaises(SystemExit) as cm:\n            healthcheck(b)\n\n        self.assertEqual(cm.exception.code, 1)\n\n    @unittest.SkipTest\n    def test_hc_os_env(self):\n        os.environ['GNES_CONTROL_PORT'] = str(56789)\n        a = set_router_parser().parse_args([\n            '--yaml_path', 'BaseRouter',\n        ])\n        a1 = copy.deepcopy(a)\n        b = set_healthcheck_parser().parse_args([\n            '--port', os.environ.get('GNES_CONTROL_PORT')\n        ])\n\n        # before - fail\n        with self.assertRaises(SystemExit) as cm:\n            healthcheck(b)\n\n        self.assertEqual(cm.exception.code, 1)\n\n        # running - success\n        with self.assertRaises(SystemExit) as cm:\n            with RouterService(a):\n                time.sleep(2)\n                healthcheck(b)\n        self.assertEqual(cm.exception.code, 0)\n\n        # running - managerservice - success\n        with self.assertRaises(SystemExit) as cm:\n            with ServiceManager(RouterService, a1):\n                time.sleep(2)\n                healthcheck(b)\n        self.assertEqual(cm.exception.code, 0)\n\n        # after - fail\n        with self.assertRaises(SystemExit) as cm:\n            healthcheck(b)\n\n        self.assertEqual(cm.exception.code, 1)\n        os.unsetenv('GNES_CONTROL_PORT')\n"""
tests/test_image_encoder.py,0,"b'import copy\nimport os\nimport unittest\nimport zipfile\n\nfrom gnes.encoder.base import BaseEncoder\nfrom gnes.preprocessor.base import UnaryPreprocessor, PipelinePreprocessor\nfrom gnes.preprocessor.image.resize import ResizeChunkPreprocessor\nfrom gnes.preprocessor.image.sliding_window import VanillaSlidingPreprocessor\nfrom gnes.proto import gnes_pb2, blob2array\n\n\ndef img_process_for_test(dirname):\n    zipfile_ = zipfile.ZipFile(os.path.join(dirname, \'imgs/test.zip\'))\n    all_bytes = [zipfile_.open(v).read() for v in zipfile_.namelist()]\n    test_img = []\n    for raw_bytes in all_bytes:\n        d = gnes_pb2.Document()\n        d.raw_bytes = raw_bytes\n        test_img.append(d)\n\n    test_img_all_preprocessor = []\n    pipline_prep1 = PipelinePreprocessor()\n    pipline_prep1.components = lambda: [UnaryPreprocessor(doc_type=gnes_pb2.Document.IMAGE),\n                                        ResizeChunkPreprocessor()]\n    pipline_prep2 = PipelinePreprocessor()\n    pipline_prep2.components = lambda: [VanillaSlidingPreprocessor(),\n                                        ResizeChunkPreprocessor()]\n\n    for preprocessor in [pipline_prep1,\n                         pipline_prep2]:\n        test_img_copy = copy.deepcopy(test_img)\n        for img in test_img_copy:\n            preprocessor.apply(img)\n        test_img_all_preprocessor.append([blob2array(chunk.blob)\n                                          for img in test_img_copy for chunk in img.chunks])\n    return test_img_all_preprocessor\n\n\nclass TestImageEncoder(unittest.TestCase):\n\n    def setUp(self):\n        dirname = os.path.dirname(__file__)\n        self.dump_path = os.path.join(dirname, \'model.bin\')\n        self.test_img = img_process_for_test(dirname)\n        self.vgg_yaml = os.path.join(dirname, \'yaml\', \'vgg-encoder.yml\')\n        self.res_yaml = os.path.join(dirname, \'yaml\', \'resnet-encoder.yml\')\n        self.inception_yaml = os.path.join(dirname, \'yaml\', \'inception-encoder.yml\')\n        self.mobilenet_yaml = os.path.join(dirname, \'yaml\', \'mobilenet-encoder.yml\')\n\n    def test_vgg_encoding(self):\n        self.encoder = BaseEncoder.load_yaml(self.vgg_yaml)\n        for test_img in self.test_img:\n            vec = self.encoder.encode(test_img)\n            print(""the length of data now is:"", len(test_img))\n            self.assertEqual(vec.shape[0], len(test_img))\n            self.assertEqual(vec.shape[1], 4096)\n\n    def test_resnet_encoding(self):\n        self.encoder = BaseEncoder.load_yaml(self.res_yaml)\n        for test_img in self.test_img:\n            vec = self.encoder.encode(test_img)\n            print(""the length of data now is:"", len(test_img))\n            self.assertEqual(vec.shape[0], len(test_img))\n            self.assertEqual(vec.shape[1], 2048)\n\n    def test_inception_encoding(self):\n        self.encoder = BaseEncoder.load_yaml(self.inception_yaml)\n        for test_img in self.test_img:\n            vec = self.encoder.encode(test_img)\n            print(""the length of data now is:"", len(test_img))\n            self.assertEqual(vec.shape[0], len(test_img))\n            self.assertEqual(vec.shape[1], 2048)\n\n    def test_mobilenet_encoding(self):\n        self.encoder = BaseEncoder.load_yaml(self.mobilenet_yaml)\n        for test_img in self.test_img:\n            vec = self.encoder.encode(test_img)\n            print(""the length of data now is:"", len(test_img))\n            self.assertEqual(vec.shape[0], len(test_img))\n            self.assertEqual(vec.shape[1], 1280)\n\n    def test_dump_load(self):\n        self.encoder = BaseEncoder.load_yaml(self.vgg_yaml)\n\n        self.encoder.dump(self.dump_path)\n\n        vgg_encoder2 = BaseEncoder.load(self.dump_path)\n\n        for test_img in self.test_img:\n            vec = vgg_encoder2.encode(test_img)\n            self.assertEqual(vec.shape[0], len(test_img))\n            self.assertEqual(vec.shape[1], 4096)\n\n    def tearDown(self):\n        if os.path.exists(self.dump_path):\n            os.remove(self.dump_path)\n'"
tests/test_image_preprocessor.py,0,"b""import os\nimport unittest\nimport zipfile\n\nfrom gnes.cli.parser import set_preprocessor_parser, _set_client_parser\nfrom gnes.client.base import ZmqClient\nfrom gnes.proto import gnes_pb2, RequestGenerator, blob2array\nfrom gnes.service.preprocessor import PreprocessorService\n\n\nclass TestProto(unittest.TestCase):\n\n    def setUp(self):\n        self.dirname = os.path.dirname(__file__)\n        self.unary_img_pre_yaml = os.path.join(self.dirname, 'yaml', 'base-unary-image-prep.yml')\n        self.slidingwindow_img_pre_yaml = os.path.join(self.dirname, 'yaml', 'base-vanilla_sldwin-image-prep.yml')\n        self.segmentation_img_pre_yaml = os.path.join(self.dirname, 'yaml', 'base-segmentation-image-prep.yml')\n        self.resize_img_pre_yaml = os.path.join(self.dirname, 'yaml', 'resize-image-prep.yml')\n\n    def test_unary_preprocessor_service_empty(self):\n        args = set_preprocessor_parser().parse_args([\n            '--yaml_path', self.unary_img_pre_yaml\n        ])\n        with PreprocessorService(args):\n            pass\n\n    def test_slidingwindow_preprocessor_service_empty(self):\n        args = set_preprocessor_parser().parse_args([\n            '--yaml_path', self.slidingwindow_img_pre_yaml\n        ])\n        with PreprocessorService(args):\n            pass\n\n    def test_segmentation_preprocessor_service_empty(self):\n        args = set_preprocessor_parser().parse_args([\n            '--yaml_path', self.segmentation_img_pre_yaml\n        ])\n        with PreprocessorService(args):\n            pass\n\n    def test_unary_preprocessor_service_echo(self):\n        args = set_preprocessor_parser().parse_args([\n            '--yaml_path', self.unary_img_pre_yaml\n        ])\n        c_args = _set_client_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', str(args.port_in)\n        ])\n        with PreprocessorService(args), ZmqClient(c_args) as client:\n            msg = gnes_pb2.Message()\n            msg.request.index.docs.extend([gnes_pb2.Document() for _ in range(5)])\n            client.send_message(msg)\n            r = client.recv_message()\n            # print(r)\n            msg.request.train.docs.extend([gnes_pb2.Document() for _ in range(5)])\n            client.send_message(msg)\n            r = client.recv_message()\n            # print(r)\n\n    def test_slidingwindow_preprocessor_service_echo(self):\n        args = set_preprocessor_parser().parse_args([\n            '--yaml_path', self.slidingwindow_img_pre_yaml\n        ])\n        c_args = _set_client_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', str(args.port_in)\n        ])\n        with PreprocessorService(args), ZmqClient(c_args) as client:\n            msg = gnes_pb2.Message()\n            msg.request.index.docs.extend([gnes_pb2.Document() for _ in range(5)])\n            client.send_message(msg)\n            r = client.recv_message()\n            # print(r)\n            msg.request.train.docs.extend([gnes_pb2.Document() for _ in range(5)])\n            client.send_message(msg)\n            r = client.recv_message()\n            # print(r)\n\n    def test_segmentation_preprocessor_service_echo(self):\n        args = set_preprocessor_parser().parse_args([\n            '--yaml_path', self.segmentation_img_pre_yaml\n        ])\n        c_args = _set_client_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', str(args.port_in)\n        ])\n        with PreprocessorService(args), ZmqClient(c_args) as client:\n            msg = gnes_pb2.Message()\n            msg.request.index.docs.extend([gnes_pb2.Document() for _ in range(5)])\n            client.send_message(msg)\n            r = client.recv_message()\n            # print(r)\n            msg.request.train.docs.extend([gnes_pb2.Document() for _ in range(5)])\n            client.send_message(msg)\n            r = client.recv_message()\n            # print(r)\n\n    def test_unary_preprocessor_service_realdata(self):\n        args = set_preprocessor_parser().parse_args([\n            '--yaml_path', self.unary_img_pre_yaml\n        ])\n        c_args = _set_client_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', str(args.port_in)\n        ])\n        all_zips = zipfile.ZipFile(os.path.join(self.dirname, 'imgs/test.zip'))\n        all_bytes = [all_zips.open(v).read() for v in all_zips.namelist()]\n\n        with PreprocessorService(args), ZmqClient(c_args) as client:\n            for req in RequestGenerator.index(all_bytes):\n                msg = gnes_pb2.Message()\n                msg.request.index.CopyFrom(req.index)\n                client.send_message(msg)\n                r = client.recv_message()\n                self.assertEqual(r.envelope.routes[0].service, 'UnaryPreprocessor')\n                for d in r.request.index.docs:\n                    self.assertEqual(len(d.chunks), 1)\n                    self.assertEqual(len(blob2array(d.chunks[0].blob).shape), 3)\n                    self.assertEqual(blob2array(d.chunks[0].blob).shape[-1], 3)\n\n    def test_resize_preprocessor_service_realdata(self):\n        args = set_preprocessor_parser().parse_args([\n            '--yaml_path', self.resize_img_pre_yaml\n        ])\n        c_args = _set_client_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', str(args.port_in)\n        ])\n        all_zips = zipfile.ZipFile(os.path.join(self.dirname, 'imgs/test.zip'))\n        all_bytes = [all_zips.open(v).read() for v in all_zips.namelist()]\n\n        with PreprocessorService(args), ZmqClient(c_args) as client:\n            for req in RequestGenerator.index(all_bytes):\n                msg = gnes_pb2.Message()\n                msg.request.index.CopyFrom(req.index)\n                client.send_message(msg)\n                r = client.recv_message()\n                self.assertEqual(r.envelope.routes[0].service, 'PipelinePreprocessor')\n                for d in r.request.index.docs:\n                    self.assertEqual(len(d.chunks), 1)\n                    self.assertEqual(len(blob2array(d.chunks[0].blob).shape), 3)\n                    self.assertEqual(blob2array(d.chunks[0].blob).shape[-1], 3)\n                    self.assertEqual(blob2array(d.chunks[0].blob).shape[0], 224)\n                    self.assertEqual(blob2array(d.chunks[0].blob).shape[1], 224)\n\n    def test_slidingwindow_preprocessor_service_realdata(self):\n        args = set_preprocessor_parser().parse_args([\n            '--yaml_path', self.slidingwindow_img_pre_yaml\n        ])\n\n        c_args = _set_client_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', str(args.port_in)\n        ])\n        all_zips = zipfile.ZipFile(os.path.join(self.dirname, 'imgs/test.zip'))\n        all_bytes = [all_zips.open(v).read() for v in all_zips.namelist()]\n\n        with PreprocessorService(args), ZmqClient(c_args) as client:\n            for req in RequestGenerator.index(all_bytes):\n                msg = gnes_pb2.Message()\n                msg.request.index.CopyFrom(req.index)\n                client.send_message(msg)\n                r = client.recv_message()\n                self.assertEqual(r.envelope.routes[0].service, 'PipelinePreprocessor')\n                for d in r.request.index.docs:\n                    self.assertEqual(len(blob2array(d.chunks[0].blob).shape), 3)\n                    self.assertEqual(blob2array(d.chunks[0].blob).shape[-1], 3)\n                    self.assertEqual(blob2array(d.chunks[0].blob).shape[0], 224)\n                    self.assertEqual(blob2array(d.chunks[0].blob).shape[1], 224)\n\n    def test_segmentation_preprocessor_service_realdata(self):\n        args = set_preprocessor_parser().parse_args([\n            '--yaml_path', self.segmentation_img_pre_yaml\n        ])\n\n        c_args = _set_client_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', str(args.port_in)\n        ])\n        all_zips = zipfile.ZipFile(os.path.join(self.dirname, 'imgs/test.zip'))\n        all_bytes = [all_zips.open(v).read() for v in all_zips.namelist()]\n\n        with PreprocessorService(args), ZmqClient(c_args) as client:\n            for req in RequestGenerator.index(all_bytes):\n                msg = gnes_pb2.Message()\n                msg.request.index.CopyFrom(req.index)\n                client.send_message(msg)\n                r = client.recv_message()\n                self.assertEqual(r.envelope.routes[0].service, 'PipelinePreprocessor')\n                for d in r.request.index.docs:\n                    self.assertEqual(len(blob2array(d.chunks[0].blob).shape), 3)\n                    self.assertEqual(blob2array(d.chunks[0].blob).shape[-1], 3)\n                    self.assertEqual(blob2array(d.chunks[0].blob).shape[0], 224)\n                    self.assertEqual(blob2array(d.chunks[0].blob).shape[1], 224)\n                    print(blob2array(d.chunks[0].blob).dtype)\n"""
tests/test_indexer_service.py,0,"b""import os\nimport unittest\n\nimport numpy as np\n\nfrom gnes.cli.parser import set_indexer_parser, _set_client_parser\nfrom gnes.client.base import ZmqClient\nfrom gnes.proto import gnes_pb2, array2blob\nfrom gnes.service.base import ServiceManager\nfrom gnes.service.indexer import IndexerService\n\n\nclass TestIndexerService(unittest.TestCase):\n\n    def setUp(self):\n        self.test_numeric = np.random.randint(0, 255, (1000, 1024)).astype('float32')\n\n    def test_empty_service(self):\n        args = set_indexer_parser().parse_args(['--yaml_path', '!BaseChunkIndexer {gnes_config: {name: IndexerService}}'])\n        c_args = _set_client_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', str(args.port_in)])\n\n        with ServiceManager(IndexerService, args), ZmqClient(c_args) as client:\n            msg = gnes_pb2.Message()\n            d = msg.request.index.docs.add()\n\n            c = d.chunks.add()\n            c.doc_id = 0\n            c.embedding.CopyFrom(array2blob(self.test_numeric))\n            c.offset = 0\n            c.weight = 1.0\n\n            client.send_message(msg)\n            r = client.recv_message()\n            self.assertEqual(r.response.index.status, gnes_pb2.Response.SUCCESS)\n\n    def tearDown(self):\n        if os.path.exists('IndexerService.bin'):\n            os.remove('IndexerService.bin')\n"""
tests/test_joint_indexer.py,0,"b""import os\nimport unittest\n\nimport numpy as np\n\nfrom gnes.indexer.base import JointIndexer\nfrom tests import txt_file2pb_docs\n\n\n@unittest.SkipTest\nclass TestJointIndexer(unittest.TestCase):\n\n    def setUp(self):\n        dirname = os.path.dirname(__file__)\n        self.dump_path = os.path.join(dirname, 'indexer.bin')\n        self.yaml_path = os.path.join(dirname, 'yaml', 'base-indexer.yml')\n\n        self.n_bytes = 8\n        self.query_num = 3\n\n        self.docs = []\n        self.querys = []\n\n        self.pb_docs = txt_file2pb_docs(open(os.path.join(dirname, 'tangshi.txt')))\n\n    def test_add(self):\n        mhi = JointIndexer.load_yaml(self.yaml_path)\n\n        for doc in self.pb_docs:\n            if len(doc.chunks) == 0:\n                continue\n            mhi.add([doc.doc_id], [doc], [1.])\n            vecs = np.random.randint(\n                0, 255, [len(doc.chunks), self.n_bytes]).astype(np.uint8)\n            mhi.add([(doc.doc_id, j) for j in range(len(doc.chunks))], vecs, [1.] * len(doc.chunks))\n            if len(self.querys) < self.query_num:\n                self.querys.append(vecs)\n\n        for q in self.querys:\n            results = mhi.query(q, top_k=1)\n            for topk in results:\n                print(topk)\n                d, o, w, s, *_ = topk[0]\n                self.assertEqual(1.0, s)\n        mhi.close()\n"""
tests/test_leveldbindexer.py,0,"b""import os\nimport unittest\nfrom shutil import rmtree\n\nfrom gnes.indexer.doc.dict import DictIndexer\nfrom gnes.indexer.doc.leveldb import LVDBIndexer\nfrom gnes.proto import gnes_pb2\nfrom tests import txt_file2pb_docs\n\n\nclass TestBaseLVDB(unittest.TestCase):\n    def setUp(self):\n        dirname = os.path.dirname(__file__)\n\n        self.test_docs = txt_file2pb_docs(open(os.path.join(dirname, 'tangshi.txt'), encoding='utf8'))\n\n        self.db_path = './test_leveldb'\n        self.dump_path = os.path.join(dirname, 'indexer.bin')\n        self.dump_yaml_path = os.path.join(dirname, 'indexer.yaml')\n\n    def tearDown(self):\n        if os.path.exists(self.db_path):\n            rmtree(self.db_path)\n        if os.path.exists(self.dump_path):\n            os.remove(self.dump_path)\n        if os.path.exists(self.dump_yaml_path):\n            os.remove(self.dump_yaml_path)\n        if os.path.exists('my-indexer-531.bin'):\n            os.remove('my-indexer-531.bin')\n        if os.path.exists('my-indexer-531.yml'):\n            os.remove('my-indexer-531.yml')\n\n    def test_dict_indexer(self):\n        db = DictIndexer()\n        db.add(range(len(self.test_docs)), self.test_docs)\n        db.dump(self.dump_path)\n        self.assertEqual(len(self.test_docs), db.num_docs)\n        db2 = DictIndexer.load(self.dump_path)\n        self.assertEqual(len(self.test_docs), db2.num_docs)\n        db.name = 'my-indexer-531'\n        db.dump()\n        db.dump_yaml()\n        db3 = DictIndexer.load_yaml(db.yaml_full_path)\n        for k in db3.query([1, 2, 3]):\n            self.assertIsInstance(k, gnes_pb2.Document)\n        self.assertEqual(len(self.test_docs), db3.num_docs)\n\n    def test_add_docs(self):\n        db = LVDBIndexer(self.db_path)\n        db.add(range(len(self.test_docs)), self.test_docs)\n        self.assertTrue(os.path.exists(self.db_path))\n        self.assertLess(0, len(os.listdir(self.db_path)))\n        db.close()\n\n    def test_query(self):\n        db = LVDBIndexer(self.db_path)\n        db.add(range(len(self.test_docs)), self.test_docs)\n\n        res1 = db.query(range(len(self.test_docs)))\n        num_non_empty = sum(1 for d in res1 if d)\n        self.assertEqual(num_non_empty, len(self.test_docs))\n\n        res2 = db.query(range(len(self.test_docs) + 1, len(self.test_docs) + 100))\n        num_non_empty = sum(1 for d in res2 if d)\n        self.assertEqual(num_non_empty, 0)\n        db.close()\n\n    def dump_load(self):\n        tmp = LVDBIndexer(self.db_path)\n        tmp.add(range(len(self.test_docs)), self.test_docs)\n        tmp.dump(self.dump_path)\n        tmp.close()\n\n        db = LVDBIndexer.load(self.db_path)\n        res1 = db.query(range(len(self.test_docs)))\n        num_non_empty = sum(1 for d in res1 if d)\n        self.assertEqual(num_non_empty, self.test_data1.length)\n\n        res2 = db.query(range(len(self.test_docs) + 1, len(self.test_docs) + 100))\n        num_non_empty = sum(1 for d in res2 if d)\n        self.assertEqual(num_non_empty, 0)\n        db.close()\n"""
tests/test_leveldbindexerasync.py,0,"b""import os\nimport unittest\nfrom shutil import rmtree\n\nfrom gnes.indexer.doc.leveldb import AsyncLVDBIndexer\nfrom tests import txt_file2pb_docs\n\n\nclass TestBaseLVDB(unittest.TestCase):\n    def setUp(self):\n        dirname = os.path.dirname(__file__)\n\n        self.test_docs = txt_file2pb_docs(open(os.path.join(dirname, 'tangshi.txt')))\n\n        self.query_hit_id = list(range(len(self.test_docs)))\n        self.query_miss_id = list(range(len(self.test_docs) + 1, len(self.test_docs) + 100))\n\n        self.db_path = './test_leveldb'\n        self.dump_path = os.path.join(dirname, 'indexer.bin')\n\n    def tearDown(self):\n        if os.path.exists(self.db_path):\n            rmtree(self.db_path)\n\n    def test_add_uni(self):\n        db = AsyncLVDBIndexer(self.db_path)\n        db.add(range(len(self.test_docs)), self.test_docs)\n        self.assertTrue(os.path.exists(self.db_path))\n        self.assertLess(0, len(os.listdir(self.db_path)))\n        db.close()\n\n    def test_add_multi(self):\n        db = AsyncLVDBIndexer(self.db_path)\n        db.add(range(len(self.test_docs)), self.test_docs)\n        self.assertTrue(os.path.exists(self.db_path))\n        self.assertLess(0, len(os.listdir(self.db_path)))\n        db.close()\n\n    @unittest.SkipTest\n    def test_query(self):\n        db = AsyncLVDBIndexer(self.db_path)\n        db.add(range(len(self.test_docs)), self.test_docs)\n        res1 = db.query(self.query_hit_id)\n        num_non_empty = sum(1 for d in res1 if d)\n        self.assertEqual(num_non_empty, len(self.test_docs))\n\n        res2 = db.query(self.query_miss_id)\n        num_non_empty = sum(1 for d in res2 if d)\n        self.assertEqual(num_non_empty, 0)\n        db.close()\n\n    def dump_load(self):\n        tmp = AsyncLVDBIndexer(self.db_path)\n        tmp.add(range(len(self.test_docs)), self.test_docs)\n        tmp.dump(self.dump_path)\n        tmp.close()\n\n        db = AsyncLVDBIndexer.load(self.db_path)\n        res1 = db.query(self.query_hit_id)\n        num_non_empty = sum(1 for d in res1 if d)\n        self.assertEqual(num_non_empty, len(self.test_docs))\n\n        res2 = db.query(self.query_miss_id)\n        num_non_empty = sum(1 for d in res2 if d)\n        self.assertEqual(num_non_empty, 0)\n        db.close()\n"""
tests/test_load_dump_pipeline.py,3,"b""import os\nimport unittest\n\nfrom gnes.encoder.base import BaseEncoder, PipelineEncoder\n\n\nclass DummyTFEncoder(BaseEncoder):\n    is_trained = True\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def post_init(self):\n        import tensorflow as tf\n        with tf.Graph().as_default():\n            self.a = tf.get_variable(name='a', shape=[])\n            self.sess = tf.Session()\n\n    def encode(self, a, *args):\n        return self.sess.run(self.a + 1, feed_dict={self.a: a})\n\n\nclass TestLoadDumpPipeline(unittest.TestCase):\n    def setUp(self):\n        self.dirname = os.path.dirname(__file__)\n        self.yaml_path = os.path.join(self.dirname, 'yaml', 'dummy-pipeline.yml')\n        self.dump_path = os.path.join(self.dirname, 'dummy-pipeline.bin')\n\n    def test_base(self):\n        a = BaseEncoder.load_yaml(self.yaml_path)\n        self.assertFalse(a.is_trained)\n\n        for c in a.components:\n            c.is_trained = True\n        a.dump()\n        os.path.exists(self.dump_path)\n\n        # load the dump from yaml\n        b = BaseEncoder.load_yaml(self.yaml_path)\n        self.assertTrue(b.is_trained)\n\n    def test_name_warning(self):\n        d1 = DummyTFEncoder()\n        d2 = DummyTFEncoder()\n        d1.name = ''\n        d2.name = ''\n        d3 = PipelineEncoder()\n        d3.components = lambda: [d1, d2]\n        d3.name = 'dummy-pipeline'\n        d3.work_dir = './'\n        d3.dump()\n        d3.dump_yaml()\n        print('there should not be any warning after this line')\n        BaseEncoder.load_yaml(d3.yaml_full_path)\n\n    def test_dummytf(self):\n        d1 = DummyTFEncoder()\n        self.assertEqual(d1.encode(1), 2)\n        self.assertTrue(d1.is_trained)\n        d1.dump()\n        d11 = BaseEncoder.load(d1.dump_full_path)\n        self.assertTrue(d11.is_trained)\n\n        d2 = DummyTFEncoder()\n        self.assertEqual(d2.encode(2), 3)\n        self.assertTrue(d2.is_trained)\n\n        d3 = PipelineEncoder()\n        d3.components = lambda: [d1, d2]\n        self.assertEqual(d3.encode(1), 3)\n        self.assertTrue(d3.is_trained)\n        self.assertTrue(d3.components[0].is_trained)\n        self.assertTrue(d3.components[1].is_trained)\n\n        d3.dump()\n        d31 = BaseEncoder.load(d3.dump_full_path)\n        self.assertTrue(d3.is_trained)\n        self.assertTrue(d31.components[0].is_trained)\n        self.assertTrue(d31.components[1].is_trained)\n\n        d3.work_dir = self.dirname\n        d3.name = 'dummy-pipeline'\n        d3.dump_yaml()\n        d3.dump()\n\n        d4 = PipelineEncoder.load(d3.dump_full_path)\n        self.assertTrue(d4.is_trained)\n        self.assertTrue(d4.components[0].is_trained)\n        self.assertTrue(d4.components[1].is_trained)\n\n        d4 = PipelineEncoder.load_yaml(d3.yaml_full_path)\n        self.assertTrue(d4.is_trained)\n        self.assertTrue(d4.components[0].is_trained)\n        self.assertTrue(d4.components[1].is_trained)\n\n        self.assertEqual(d4.encode(4), 6)\n\n    def tearDown(self):\n        if os.path.exists(self.dump_path):\n            os.remove(self.dump_path)\n"""
tests/test_mfcc_encoder.py,0,"b""import os\nimport unittest\n\nfrom gnes.encoder.audio.mfcc import MfccEncoder\nfrom gnes.preprocessor.helper import get_audio, get_video_length_from_raw\n\n\ndef extract_audio(video_bytes):\n    sample_rate = 6500\n    audio_interval = 1\n    audios = []\n    for raw_bytes in video_bytes:\n        audio = get_audio(raw_bytes, sample_rate, audio_interval, get_video_length_from_raw(raw_bytes))\n        audios.append(audio)\n    audios = [slice for audio in audios for slice in audio]\n    return audios\n\n\nclass TestMfccEncoder(unittest.TestCase):\n\n    def setUp(self):\n        self.dirname = os.path.dirname(__file__)\n        self.video_path = os.path.join(self.dirname, 'videos')\n        self.video_bytes = [open(os.path.join(self.video_path, _), 'rb').read()\n                            for _ in os.listdir(self.video_path)]\n        self.audios = extract_audio(self.video_bytes)\n        self.mfcc_yaml = os.path.join(self.dirname, 'yaml', 'mfcc-encoder.yml')\n\n    def test_mfcc_encoding(self):\n        self.encoder = MfccEncoder.load_yaml(self.mfcc_yaml)\n        vec = self.encoder.encode(self.audios)\n        self.assertEqual(len(vec.shape), 2)\n        self.assertEqual(vec.shape[0], len(self.audios))\n        self.assertEqual(vec.shape[1] % self.encoder.n_mfcc, 0)\n"""
tests/test_mh_indexer.py,0,"b""import os\nimport unittest\n\nfrom gnes.indexer.base import BaseIndexer\n\n\nclass TestMHIndexer(unittest.TestCase):\n\n    def setUp(self):\n        dirname = os.path.dirname(__file__)\n        self.yaml_path1 = os.path.join(dirname, 'yaml', 'base-indexer2.yml')\n        self.yaml_path2 = os.path.join(dirname, 'yaml', 'base-indexer3.yml')\n        self.yaml_path3 = os.path.join(dirname, 'yaml', 'base-indexer4.yml')\n\n    def test_ind1(self):\n        self.assertRaises(ValueError, BaseIndexer.load_yaml, self.yaml_path1)\n\n    def test_ind3(self):\n        self.assertRaises(ValueError, BaseIndexer.load_yaml, self.yaml_path1)\n\n    def test_ind2(self):\n        mhi = BaseIndexer.load_yaml(self.yaml_path2)\n        print(mhi)\n"""
tests/test_onnx_image_encoder.py,0,"b'import copy\nimport os\nimport unittest\nimport zipfile\n\nfrom gnes.encoder.image.onnx import BaseONNXImageEncoder\nfrom gnes.preprocessor.base import UnaryPreprocessor, PipelinePreprocessor\nfrom gnes.preprocessor.image.resize import ResizeChunkPreprocessor\nfrom gnes.preprocessor.image.sliding_window import VanillaSlidingPreprocessor\nfrom gnes.proto import gnes_pb2, blob2array\n\n\ndef img_process_for_test(dirname):\n    zipfile_ = zipfile.ZipFile(os.path.join(dirname, \'imgs/test.zip\'))\n    all_bytes = [zipfile_.open(v).read() for v in zipfile_.namelist()]\n    test_img = []\n    for raw_bytes in all_bytes:\n        d = gnes_pb2.Document()\n        d.raw_bytes = raw_bytes\n        test_img.append(d)\n\n    test_img_all_preprocessor = []\n    pipline_prep1 = PipelinePreprocessor()\n    pipline_prep1.components = lambda: [UnaryPreprocessor(doc_type=gnes_pb2.Document.IMAGE),\n                                        ResizeChunkPreprocessor()]\n    pipline_prep2 = PipelinePreprocessor()\n    pipline_prep2.components = lambda: [VanillaSlidingPreprocessor(),\n                                        ResizeChunkPreprocessor()]\n\n    for preprocessor in [pipline_prep1,\n                         pipline_prep2]:\n        test_img_copy = copy.deepcopy(test_img)\n        for img in test_img_copy:\n            preprocessor.apply(img)\n        test_img_all_preprocessor.append([blob2array(chunk.blob)\n                                          for img in test_img_copy for chunk in img.chunks])\n    return test_img_all_preprocessor\n\n\nclass TestONNXImageEncoder(unittest.TestCase):\n\n    def setUp(self):\n        dirname = os.path.dirname(__file__)\n        self.dump_path = os.path.join(dirname, \'model.bin\')\n        self.test_img = img_process_for_test(dirname)\n        self.vgg_yaml = os.path.join(dirname, \'yaml\', \'onnx-vgg-encoder.yml\')\n        self.res_yaml = os.path.join(dirname, \'yaml\', \'onnx-resnet-encoder.yml\')\n        self.inception_yaml = os.path.join(dirname, \'yaml\', \'onnx-inception-encoder.yml\')\n        self.mobilenet_yaml = os.path.join(dirname, \'yaml\', \'onnx-mobilenet-encoder.yml\')\n\n    def test_vgg_encoding(self):\n        self.encoder = BaseONNXImageEncoder.load_yaml(self.vgg_yaml)\n        for test_img in self.test_img:\n            vec = self.encoder.encode(test_img)\n            print(""the length of data now is:"", len(test_img))\n            self.assertEqual(vec.shape[0], len(test_img))\n            self.assertEqual(vec.shape[1], 1000)\n\n    def test_resnet_encoding(self):\n        self.encoder = BaseONNXImageEncoder.load_yaml(self.res_yaml)\n        for test_img in self.test_img:\n            vec = self.encoder.encode(test_img)\n            print(""the length of data now is:"", len(test_img))\n            self.assertEqual(vec.shape[0], len(test_img))\n            self.assertEqual(vec.shape[1], 1000)\n\n    def test_inception_encoding(self):\n        self.encoder = BaseONNXImageEncoder.load_yaml(self.inception_yaml)\n        for test_img in self.test_img:\n            vec = self.encoder.encode(test_img)\n            print(""the length of data now is:"", len(test_img))\n            self.assertEqual(vec.shape[0], len(test_img))\n            self.assertEqual(vec.shape[1], 1000)\n\n    def test_mobilenet_encoding(self):\n        self.encoder = BaseONNXImageEncoder.load_yaml(self.mobilenet_yaml)\n        for test_img in self.test_img:\n            vec = self.encoder.encode(test_img)\n            print(""the length of data now is:"", len(test_img))\n            self.assertEqual(vec.shape[0], len(test_img))\n            self.assertEqual(vec.shape[1], 1000)\n\n    def test_dump_load(self):\n        self.encoder = BaseONNXImageEncoder.load_yaml(self.inception_yaml)\n\n        self.encoder.dump(self.dump_path)\n\n        vgg_encoder2 = BaseONNXImageEncoder.load(self.dump_path)\n\n        for test_img in self.test_img:\n            vec = vgg_encoder2.encode(test_img)\n            self.assertEqual(vec.shape[0], len(test_img))\n            self.assertEqual(vec.shape[1], 1000)\n\n    def tearDown(self):\n        if os.path.exists(self.dump_path):\n            os.remove(self.dump_path)\n'"
tests/test_parser.py,0,"b'import unittest\n\nfrom gnes.cli.parser import set_frontend_parser\n\n\nclass TestParser(unittest.TestCase):\n    def test_service_parser(self):\n        args1 = set_frontend_parser().parse_args([])\n        args2 = set_frontend_parser().parse_args([])\n        self.assertNotEqual(args1.port_in, args2.port_in)\n        self.assertNotEqual(args1.port_out, args2.port_out)\n'"
tests/test_partition.py,0,"b'import random\nimport unittest\n\nimport numpy as np\n\nfrom gnes.helper import get_perm\n\n\nclass TestPartition(unittest.TestCase):\n\n    def test_greedy_partition(self):\n        l = 6\n        # 6 must be dividable by 2 and 3, otherwise this unit test will fail as there is no\n        # perfect partition\n        x = np.array([random.random()] * l + [random.random()] * l)\n        a = get_perm(x, 2)\n        self.assertEqual(sum(x[a[:l]]), sum(x[a[l:]]))\n\n        x = np.array([random.random()] * l + [random.random()] * l + [random.random()] * l)\n        b = get_perm(x, 3)\n        self.assertEqual(sum(x[b[:l]]), sum(x[b[l:(2 * l)]]))\n        self.assertEqual(sum(x[b[:l]]), sum(x[b[(2 * l):]]))\n'"
tests/test_pca_encoder.py,0,"b""import os\nimport unittest\nimport numpy as np\n\nfrom gnes.encoder.base import BaseEncoder\n\n\nclass TestPCAEncoder(unittest.TestCase):\n\n    def setUp(self):\n        dirname = os.path.dirname(__file__)\n        self.dump_path = os.path.join(dirname, 'pca_encoder.bin')\n        self.yaml_path = os.path.join(dirname, 'yaml', 'pca.yml')\n        self.test_numeric = np.random.randint(0, 255, (1000, 1024)).astype('float32')\n\n    def test_encoding(self):\n        self.encoder = BaseEncoder.load_yaml(self.yaml_path)\n        # train before encode to create pca_components\n        self.encoder.train(self.test_numeric)\n        vec = self.encoder.encode(self.test_numeric)\n        self.assertEqual(vec.shape, (1000, 300))\n        # dump after train with valied pca_components\n        self.encoder.dump(self.dump_path)\n        encoder2 = BaseEncoder.load(self.dump_path)\n        vec = encoder2.encode(self.test_numeric)\n        self.assertEqual(vec.shape, (1000, 300))\n\n    def tearDown(self):\n        if os.path.exists(self.dump_path):\n            os.remove(self.dump_path)\n"""
tests/test_pipeline_train.py,0,"b""import os\nimport unittest\n\nfrom gnes.encoder.base import BaseEncoder, PipelineEncoder\nfrom gnes.helper import train_required\n\n\nclass DummyEncoder(BaseEncoder):\n\n    def train(self, *args, **kwargs):\n        self.logger.info('you just trained me!')\n        pass\n\n    @train_required\n    def encode(self, x):\n        return x + 1\n\n\nclass TestPipeTrain(unittest.TestCase):\n    def setUp(self):\n        self.dirname = os.path.dirname(__file__)\n\n    def test_train(self):\n        de = DummyEncoder()\n        self.assertRaises(RuntimeError, de.encode, 1)\n        de.train()\n        self.assertEqual(2, de.encode(1))\n\n    def tearDown(self):\n        if os.path.exists('dummy-pipeline.bin'):\n            os.remove('dummy-pipeline.bin')\n        if os.path.exists('dummy-pipeline.yml'):\n            os.remove('dummy-pipeline.yml')\n\n    def test_pipeline_train(self):\n        p = PipelineEncoder()\n        p.components = lambda: [DummyEncoder(), DummyEncoder(), DummyEncoder()]\n        self.assertRaises(RuntimeError, p.encode, 1)\n        p.train(1)\n        self.assertEqual(4, p.encode(1))\n        p.name = 'dummy-pipeline'\n        p.dump()\n        p.dump_yaml()\n        a = BaseEncoder.load_yaml(p.yaml_full_path)\n        self.assertEqual(4, a.encode(1))\n\n    @unittest.SkipTest\n    def test_load_yaml(self):\n        p = BaseEncoder.load_yaml(os.path.join(self.dirname, 'yaml', 'pipeline-multi-encoder.yml'))\n        self.assertRaises(RuntimeError, p.encode, 1)\n        p.train(1)\n        self.assertEqual(5, p.encode(1))\n        p = BaseEncoder.load_yaml(os.path.join(self.dirname, 'yaml', 'pipeline-multi-encoder.yml'))\n        self.assertRaises(RuntimeError, p.encode, 1)\n"""
tests/test_pipeline_train_ext.py,0,"b""import os\nimport unittest\n\nfrom gnes.encoder.base import BaseEncoder\nfrom gnes.helper import PathImporter\n\n\nclass TestPipeTrain(unittest.TestCase):\n    def setUp(self):\n        self.dirname = os.path.dirname(__file__)\n        PathImporter.add_modules(*('{0}/contrib/dummy2.py,{0}/contrib/dummy3.py'.format(self.dirname).split(',')))\n\n    def tearDown(self):\n        if os.path.exists('dummy-pipeline.bin'):\n            os.remove('dummy-pipeline.bin')\n        if os.path.exists('dummy-pipeline.yml'):\n            os.remove('dummy-pipeline.yml')\n\n    def test_load_yaml(self):\n        p = BaseEncoder.load_yaml(os.path.join(self.dirname, 'yaml', 'pipeline-multi-encoder2.yml'))\n        self.assertFalse(p.is_trained)\n        self.assertRaises(RuntimeError, p.encode, 1)\n        p.train(1)\n        self.assertTrue(p.is_trained)\n        self.assertEqual(5, p.encode(1))\n        p = BaseEncoder.load_yaml(os.path.join(self.dirname, 'yaml', 'pipeline-multi-encoder2.yml'))\n        self.assertRaises(RuntimeError, p.encode, 1)\n"""
tests/test_pipelinepreprocess.py,0,"b""import os\nimport unittest\n\nfrom gnes.preprocessor.base import BasePreprocessor, PipelinePreprocessor\nfrom gnes.proto import gnes_pb2\n\n\nclass P1(BasePreprocessor):\n    def apply(self, doc: 'gnes_pb2.Document'):\n        doc.doc_id += 1\n\n\nclass P2(BasePreprocessor):\n    def apply(self, doc: 'gnes_pb2.Document'):\n        doc.doc_id *= 3\n\n\nclass TestPartition(unittest.TestCase):\n    def setUp(self):\n        self.dirname = os.path.dirname(__file__)\n        self.p3_name = 'pipe-p12'\n        self.yml_dump_path = os.path.join(self.dirname, '%s.yml' % self.p3_name)\n        self.bin_dump_path = os.path.join(self.dirname, '%s.bin' % self.p3_name)\n\n    def tearDown(self):\n        if os.path.exists(self.yml_dump_path):\n            os.remove(self.yml_dump_path)\n        if os.path.exists(self.bin_dump_path):\n            os.remove(self.bin_dump_path)\n\n    def test_pipelinepreproces(self):\n        p3 = PipelinePreprocessor()\n        p3.components = lambda: [P1(), P2()]\n        d = gnes_pb2.Document()\n        d.doc_id = 1\n        p3.apply(d)\n        self.assertEqual(d.doc_id, 6)\n\n        p3.name = self.p3_name\n        p3.dump_yaml()\n        p3.dump()\n\n        p4 = BasePreprocessor.load_yaml(p3.yaml_full_path)\n        p4.apply(d)\n        self.assertEqual(d.doc_id, 21)\n"""
tests/test_pooling_encoder.py,0,"b""import unittest\n\nimport numpy as np\nimport torch\nfrom numpy.testing import assert_allclose\n\nfrom gnes.encoder.numeric.pooling import PoolingEncoder\n\n\nclass TestEncoder(unittest.TestCase):\n    def setUp(self):\n        self.seq_data = np.random.random([5, 10])\n        self.seq_embed_data = np.random.random([5, 10, 32])\n        self.mask_data = np.array(self.seq_data > 0.5, np.float32)\n        self.data = [\n            (torch.tensor(self.seq_embed_data, dtype=torch.float32), torch.tensor(self.mask_data, dtype=torch.float32)),\n            (self.seq_embed_data, self.mask_data),\n            (self.seq_embed_data, self.mask_data)]\n\n    def _test_strategy(self, strategy):\n        pe_to = PoolingEncoder(strategy, 'torch')\n        pe_tf = PoolingEncoder(strategy, 'tensorflow')\n        pe_np = PoolingEncoder(strategy, 'numpy')\n        return [pe.encode(self.data[idx]) for idx, pe in enumerate([pe_to, pe_tf, pe_np])]\n\n    def test_all(self):\n        for s in {'REDUCE_MEAN', 'REDUCE_MAX', 'REDUCE_MEAN_MAX'}:\n            with self.subTest(strategy=s):\n                r = self._test_strategy(s)\n                for rr in r:\n                    print(type(rr))\n                    print(rr)\n                    print('---')\n                assert_allclose(r[0], r[1], rtol=1e-5)\n                assert_allclose(r[1], r[2], rtol=1e-5)\n"""
tests/test_preprocessor.py,0,"b""import os\nimport unittest\n\nfrom gnes.cli.parser import set_preprocessor_parser, _set_client_parser\nfrom gnes.client.base import ZmqClient\nfrom gnes.proto import gnes_pb2\nfrom gnes.service.preprocessor import PreprocessorService\n\n\nclass TestProto(unittest.TestCase):\n\n    def setUp(self):\n        self.single_cn = '\xe7\x9f\xab\xe7\x9f\xab\xe7\x8f\x8d\xe6\x9c\xa8\xe5\xb7\x85\xef\xbc\x8c\xe5\xbe\x97\xe6\x97\xa0\xe9\x87\x91\xe4\xb8\xb8\xe6\x83\xa7\xe3\x80\x82'\n        self.single_en = 'When forty winters shall besiege thy brow. And dig deep trenches in thy beautys field.'\n        self.dirname = os.path.dirname(__file__)\n        self.yaml_path = os.path.join(self.dirname, 'yaml', 'test-preprocessor.yml')\n\n    def test_preprocessor_service_empty(self):\n        args = set_preprocessor_parser().parse_args(['--yaml_path', 'BasePreprocessor'])\n        with PreprocessorService(args):\n            pass\n\n    def test_preprocessor_service_echo(self):\n        args = set_preprocessor_parser().parse_args(['--yaml_path', 'BasePreprocessor'])\n        c_args = _set_client_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', str(args.port_in)\n        ])\n        with PreprocessorService(args), ZmqClient(c_args) as client:\n            msg = gnes_pb2.Message()\n            msg.request.index.docs.extend([gnes_pb2.Document() for _ in range(5)])\n            client.send_message(msg)\n            r = client.recv_message()\n            print(r)\n            msg.request.train.docs.extend([gnes_pb2.Document() for _ in range(5)])\n            client.send_message(msg)\n            r = client.recv_message()\n            print(r)\n\n    def test_preprocessor_service_realdata(self):\n        args = set_preprocessor_parser().parse_args(['--yaml_path', self.yaml_path])\n        c_args = _set_client_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', str(args.port_in)\n        ])\n        with open(os.path.join(self.dirname, '26-doc-chinese.txt'), 'r', encoding='utf8') as fp:\n            msg = gnes_pb2.Message()\n            all_text = ''\n            for v in fp:\n                if v.strip():\n                    d = msg.request.train.docs.add()\n                    d.raw_bytes = v.encode()\n                    all_text += v\n            with PreprocessorService(args), ZmqClient(c_args) as client:\n                client.send_message(msg)\n                r = client.recv_message()\n                print(r)\n\n                msg1 = gnes_pb2.Message()\n                msg1.request.index.docs.extend(msg.request.train.docs)\n\n                client.send_message(msg1)\n                r = client.recv_message()\n                print(r)\n\n                msg2 = gnes_pb2.Message()\n                msg2.request.search.query.raw_text = all_text\n\n                client.send_message(msg2)\n                r = client.recv_message()\n                print(r)\n"""
tests/test_pretrain_encoder.py,0,"b""import unittest\n\nfrom gnes.encoder.base import BaseEncoder as BE, PipelineEncoder\nfrom gnes.helper import train_required\n\n\nclass _DummyTrainEncoder(BE):\n    @train_required\n    def encode(self, *args, **kwargs):\n        pass\n\n    def train(self, *args, **kwargs):\n        pass\n\n\nclass _BertEncoder(BE):\n    def encode(self, data, *args, **kwargs):\n        print('bert-encode!')\n        return data * 7\n\n    def train(self, data, *args, **kwargs):\n        print('bert-train!')\n\n\nclass _PQEncoder(BE):\n    def encode(self, data, *args, **kwargs):\n        print('pq-encode!')\n        return data * 2\n\n    def train(self, data, *args, **kwargs):\n        print('pq-train!')\n\n\nclass _PCAEncoder(BE):\n    @train_required\n    def encode(self, data, *args, **kwargs):\n        print('pca-encode!')\n        return data + 3\n\n    def train(self, data, *args, **kwargs):\n        print('pca-train!')\n\n\nclass _LOPQEncoder(PipelineEncoder):\n    def __init__(self):\n        super().__init__()\n        self.components = lambda: [_PCAEncoder(), _PQEncoder()]\n\n\nclass _BertBinaryEncoder(PipelineEncoder):\n    def __init__(self):\n        super().__init__()\n        self.components = lambda: [_BertEncoder(), _LOPQEncoder()]\n\n\nclass TestDocument(unittest.TestCase):\n    def test_no_pretrain(self):\n        a = _DummyTrainEncoder()\n        self.assertRaises(RuntimeError, a.encode)\n\n    def test_with_pretrain(self):\n        a = _DummyTrainEncoder()\n        a.train()\n        a.encode()\n\n    def test_hierachy_encoder(self):\n        le = _LOPQEncoder()\n        self.assertRaises(RuntimeError, le.encode, 1)\n        le.train(data=1)\n        self.assertEqual(le.encode(data=1), 8)\n        self.assertEqual(le.encode(data=2), 10)\n\n    def test_hierachy_encoder2(self):\n        print('___')\n        le2 = _BertBinaryEncoder()\n        le2.train(data=1)\n        le2.encode(data=1)\n"""
tests/test_progressbar.py,0,"b""import time\nimport unittest\n\nfrom gnes.client.cli import ProgressBar\n\n\nclass TestProgessbar(unittest.TestCase):\n    def setUp(self):\n        self.bar_len = 20\n\n    def test_progressbar5(self):\n        # should be 5 line\n        with ProgressBar(task_name='test', bar_len=self.bar_len) as pb:\n            for j in range(5 * self.bar_len):\n                pb.update()\n                time.sleep(.05)\n\n    def test_progressbar1(self):\n        # should be single line\n        with ProgressBar(task_name='test', bar_len=self.bar_len) as pb:\n            for j in range(self.bar_len):\n                pb.update()\n                time.sleep(.05)\n"""
tests/test_proto.py,0,"b""import unittest\n\nimport numpy as np\nfrom numpy.testing import assert_array_equal\n\nfrom gnes.proto import gnes_pb2, array2blob, blob2array\n\n\nclass TestProto(unittest.TestCase):\n\n    def test_array_proto(self):\n        x = np.random.random([5, 4])\n        blob = array2blob(x)\n        x1 = blob2array(blob)\n        assert_array_equal(x, x1)\n\n    def test_new_msg(self):\n        a = gnes_pb2.Message()\n        a.response.index.status = gnes_pb2.Response.SUCCESS\n        print(a)\n        a.request.train.docs.extend([gnes_pb2.Document() for _ in range(2)])\n        print(a)\n        a.request.train.ClearField('docs')\n        a.request.train.docs.extend([gnes_pb2.Document() for _ in range(3)])\n        print(a)\n"""
tests/test_pytorch_transformers_encoder.py,0,"b""import os\nimport unittest\n\nfrom gnes.encoder.base import BaseEncoder\n\n\nclass TestTorchTransformersEncoder(unittest.TestCase):\n\n    def setUp(self):\n        dirname = os.path.dirname(__file__)\n        self.dump_path = os.path.join(dirname, 'model.bin')\n        self.text_yaml = os.path.join(dirname, 'yaml', 'torch-transformers-encoder.yml')\n        self.tt_encoder = BaseEncoder.load_yaml(self.text_yaml)\n\n        self.test_str = []\n        with open(os.path.join(dirname, 'sonnets_small.txt')) as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    self.test_str.append(line)\n\n    def test_encoding(self):\n        vec = self.tt_encoder.encode(self.test_str)\n        self.assertEqual(vec.shape[0], len(self.test_str))\n        self.assertEqual(vec.shape[1], 768)\n\n    def test_dump_load(self):\n        self.tt_encoder.dump(self.dump_path)\n\n        tt_encoder2 = BaseEncoder.load(self.dump_path)\n\n        vec = tt_encoder2.encode(self.test_str)\n        self.assertEqual(vec.shape[0], len(self.test_str))\n        self.assertEqual(vec.shape[1], 768)\n\n    def tearDown(self):\n        if os.path.exists(self.dump_path):\n            os.remove(self.dump_path)\n"""
tests/test_quantizer_encoder.py,0,"b""import os\nimport unittest\nimport numpy as np\n\nfrom gnes.encoder.base import BaseNumericEncoder\n\n\nclass TestQuantizerEncoder(unittest.TestCase):\n    def setUp(self):\n        dirname = os.path.dirname(__file__)\n        self.vanilla_quantizer_yaml = os.path.join(dirname, 'yaml', 'quantizer_encoder.yml')\n\n    def test_vanilla_quantizer(self):\n        encoder = BaseNumericEncoder.load_yaml(self.vanilla_quantizer_yaml)\n        encoder.train()\n\n        vecs_1 = np.random.uniform(-150, 150, size=[1000, 160]).astype('float32')\n        out = encoder.encode(vecs_1)\n        self.assertEqual(len(out.shape), 2)\n        self.assertEqual(out.shape[0], 1000)\n        self.assertEqual(out.shape[1], 16)\n\n        vecs_2 = np.random.uniform(-1, 1, size=[1000, 160]).astype('float32')\n        self.assertRaises(Warning, encoder.encode, vecs_2)\n\n        vecs_3 = np.random.uniform(-1, 1000, size=[1000, 160]).astype('float32')\n        self.assertRaises(Warning, encoder.encode, vecs_3)\n\n"""
tests/test_raw_bytes_send.py,0,"b""import copy\nimport random\nimport unittest\n\nimport numpy as np\n\nfrom gnes.cli.parser import _set_client_parser\nfrom gnes.client.base import ZmqClient\nfrom gnes.helper import TimeContext\nfrom gnes.proto import gnes_pb2, array2blob, blob2array\nfrom gnes.service.base import SocketType\n\n\nclass TestSqueezedSendRecv(unittest.TestCase):\n    def setUp(self):\n        self.c1_args = _set_client_parser().parse_args([\n            '--port_in', str(5678),\n            '--port_out', str(5679),\n            '--socket_out', str(SocketType.PUSH_BIND),\n            '--no-check_version'\n        ])\n        self.c2_args = _set_client_parser().parse_args([\n            '--port_in', str(self.c1_args.port_out),\n            '--port_out', str(self.c1_args.port_in),\n            '--socket_in', str(SocketType.PULL_CONNECT),\n            '--no-check_version'\n        ])\n\n    def test_send_recv(self):\n        with ZmqClient(self.c1_args) as c1, ZmqClient(self.c2_args) as c2:\n            msg = gnes_pb2.Message()\n            msg.envelope.client_id = c1.args.identity\n            d = msg.request.index.docs.add()\n            d.raw_bytes = b'aa'\n            c1.send_message(msg)\n            r_msg = c2.recv_message()\n            self.assertEqual(r_msg.request.index.docs[0].raw_bytes, d.raw_bytes)\n\n    def test_send_recv_raw_bytes(self):\n        with ZmqClient(self.c1_args) as c1, ZmqClient(self.c2_args) as c2:\n            msg = gnes_pb2.Message()\n            msg.envelope.client_id = c1.args.identity\n            for j in range(random.randint(10, 20)):\n                d = msg.request.index.docs.add()\n                d.raw_bytes = b'a' * random.randint(100, 1000)\n            raw_bytes = copy.deepcopy([d.raw_bytes for d in msg.request.index.docs])\n            c1.send_message(msg, squeeze_pb=True)\n            r_msg = c2.recv_message()\n            for d, o_d, r_d in zip(msg.request.index.docs, raw_bytes, r_msg.request.index.docs):\n                self.assertEqual(d.raw_bytes, b'')\n                self.assertEqual(o_d, r_d.raw_bytes)\n                print('.', end='')\n            print('checked %d docs' % len(msg.request.index.docs))\n\n    def test_send_recv_response(self):\n        with ZmqClient(self.c1_args) as c1, ZmqClient(self.c2_args) as c2:\n            msg = gnes_pb2.Message()\n            msg.envelope.client_id = c1.args.identity\n            msg.response.train.status = 2\n            c1.send_message(msg, squeeze_pb=True)\n            r_msg = c2.recv_message()\n            self.assertEqual(msg.response.train.status, r_msg.response.train.status)\n\n    def build_msgs(self):\n        all_msgs = []\n        num_msg = 20\n        for j in range(num_msg):\n            msg = gnes_pb2.Message()\n            msg.envelope.client_id = 'abc'\n            for j in range(random.randint(10, 20)):\n                d = msg.request.index.docs.add()\n                # each doc is about 1MB to 10MB\n                d.raw_bytes = b'a' * random.randint(1000000, 10000000)\n            all_msgs.append(msg)\n        return all_msgs\n\n    def build_msgs2(self, seed=0):\n        all_msgs = []\n        num_msg = 20\n        random.seed(seed)\n        np.random.seed(seed)\n        for j in range(num_msg):\n            msg = gnes_pb2.Message()\n            msg.envelope.client_id = 'abc'\n            for _ in range(random.randint(10, 20)):\n                d = msg.request.index.docs.add()\n                # each doc is about 1MB to 10MB\n                for _ in range(random.randint(10, 20)):\n                    c = d.chunks.add()\n                    c.embedding.CopyFrom(array2blob(np.random.random([10, 20, 30])))\n                    c.blob.CopyFrom(array2blob(np.random.random([10, 20, 30])))\n            all_msgs.append(msg)\n        return all_msgs\n\n    def test_benchmark(self):\n        all_msgs = self.build_msgs()\n\n        with ZmqClient(self.c1_args) as c1, ZmqClient(self.c2_args) as c2:\n            with TimeContext('send, squeeze_pb=False'):\n                for m in all_msgs:\n                    c1.send_message(m)\n            with TimeContext('recv, squeeze_pb=False'):\n                for _ in all_msgs:\n                    c2.recv_message()\n\n        with ZmqClient(self.c1_args) as c1, ZmqClient(self.c2_args) as c2:\n            with TimeContext('send, squeeze_pb=True'):\n                for m in all_msgs:\n                    c1.send_message(m, squeeze_pb=True)\n            with TimeContext('recv, squeeze_pb=True'):\n                for _ in all_msgs:\n                    c2.recv_message()\n\n    def test_benchmark2(self):\n        all_msgs = self.build_msgs()\n\n        with ZmqClient(self.c1_args) as c1, ZmqClient(self.c2_args) as c2:\n            with TimeContext('send->recv, squeeze_pb=False'):\n                for m in all_msgs:\n                    c1.send_message(m, squeeze_pb=False)\n                    c2.recv_message()\n\n        with ZmqClient(self.c1_args) as c1, ZmqClient(self.c2_args) as c2:\n            with TimeContext('send->recv, squeeze_pb=True'):\n                for m in all_msgs:\n                    c1.send_message(m, squeeze_pb=True)\n                    c2.recv_message()\n\n    def test_benchmark3(self):\n        all_msgs = self.build_msgs()\n        all_msgs_bak = copy.deepcopy(all_msgs)\n\n        with ZmqClient(self.c1_args) as c1, ZmqClient(self.c2_args) as c2:\n            for m, m1 in zip(all_msgs, all_msgs_bak):\n                c1.send_message(m, squeeze_pb=True)\n                r_m = c2.recv_message()\n                for d, o_d, r_d in zip(m.request.index.docs, m1.request.index.docs, r_m.request.index.docs):\n                    self.assertEqual(d.raw_bytes, b'')\n                    self.assertEqual(o_d.raw_bytes, r_d.raw_bytes)\n\n    def test_benchmark4(self):\n        all_msgs = self.build_msgs2()\n\n        with ZmqClient(self.c1_args) as c1, ZmqClient(self.c2_args) as c2:\n            with TimeContext('send->recv, squeeze_pb=False'):\n                for m in all_msgs:\n                    c1.send_message(m, squeeze_pb=False)\n                    c2.recv_message()\n\n        with ZmqClient(self.c1_args) as c1, ZmqClient(self.c2_args) as c2:\n            with TimeContext('send->recv, squeeze_pb=True'):\n                for m in all_msgs:\n                    c1.send_message(m, squeeze_pb=True)\n                    c2.recv_message()\n\n    def test_benchmark5(self):\n        all_msgs = self.build_msgs2()\n        all_msgs_bak = copy.deepcopy(all_msgs)\n\n        with ZmqClient(self.c1_args) as c1, ZmqClient(self.c2_args) as c2:\n            with TimeContext('send->recv, squeeze_pb=True'):\n                for m, m1 in zip(all_msgs, all_msgs_bak):\n                    c1.send_message(m, squeeze_pb=True)\n                    r_m = c2.recv_message()\n\n                    for d, r_d in zip(m1.request.index.docs, r_m.request.index.docs):\n                        for c, r_c in zip(d.chunks, r_d.chunks):\n                            np.allclose(blob2array(c.embedding), blob2array(r_c.embedding))\n                            np.allclose(blob2array(c.blob), blob2array(r_c.blob))"""
tests/test_router.py,0,"b'import json\nimport os\nimport unittest\n\nimport numpy as np\n\nfrom gnes.cli.parser import set_router_parser, _set_client_parser\nfrom gnes.client.base import ZmqClient\nfrom gnes.proto import gnes_pb2, array2blob\nfrom gnes.service.base import SocketType\nfrom gnes.service.router import RouterService\n\n\nclass TestProto(unittest.TestCase):\n\n    def setUp(self):\n        self.dirname = os.path.dirname(__file__)\n        self.publish_router_yaml = \'!PublishRouter {parameters: {num_part: 2}}\'\n        self.batch_router_yaml = \'!DocBatchRouter {gnes_config: {batch_size: 2}}\'\n        self.reduce_router_yaml = \'BaseReduceRouter\'\n        self.chunk_router_yaml = \'Chunk2DocTopkReducer\'\n        self.chunk_sum_yaml = \'ChunkTopkReducer\'\n        self.doc_router_yaml = \'DocFillReducer\'\n        self.doc_sum_yaml = \'DocSumRouter\'\n        self.concat_router_yaml = \'ConcatEmbedRouter\'\n        self.avg_router_yaml = \'AvgEmbedRouter\'\n\n    def test_service_empty(self):\n        args = set_router_parser().parse_args([\'--yaml_path\', \'BaseRouter\'])\n        with RouterService(args):\n            pass\n\n    def test_map_router(self):\n        args = set_router_parser().parse_args([\n            \'--yaml_path\', self.batch_router_yaml,\n        ])\n        c_args = _set_client_parser().parse_args([\n            \'--port_in\', str(args.port_out),\n            \'--port_out\', str(args.port_in),\n        ])\n        with RouterService(args), ZmqClient(c_args) as c1:\n            msg = gnes_pb2.Message()\n            msg.request.index.docs.extend([gnes_pb2.Document() for _ in range(5)])\n            c1.send_message(msg)\n            r = c1.recv_message()\n            self.assertEqual(len(r.request.index.docs), 2)\n            r = c1.recv_message()\n            self.assertEqual(len(r.request.index.docs), 2)\n            r = c1.recv_message()\n            self.assertEqual(len(r.request.index.docs), 1)\n\n    def test_publish_router(self):\n        args = set_router_parser().parse_args([\n            \'--yaml_path\', self.publish_router_yaml,\n            \'--socket_out\', str(SocketType.PUB_BIND)\n        ])\n        c_args = _set_client_parser().parse_args([\n            \'--port_in\', str(args.port_out),\n            \'--port_out\', str(args.port_in),\n            \'--socket_in\', str(SocketType.SUB_CONNECT)\n        ])\n        with RouterService(args), ZmqClient(c_args) as c1, ZmqClient(c_args) as c2:\n            msg = gnes_pb2.Message()\n            msg.request.index.docs.extend([gnes_pb2.Document() for _ in range(5)])\n            msg.envelope.num_part.append(1)\n            c1.send_message(msg)\n            r = c1.recv_message()\n            self.assertSequenceEqual(r.envelope.num_part, [1, 2])\n            r = c2.recv_message()\n            self.assertSequenceEqual(r.envelope.num_part, [1, 2])\n\n    def test_reduce_router(self):\n        args = set_router_parser().parse_args([\n            \'--yaml_path\', self.reduce_router_yaml,\n            \'--socket_out\', str(SocketType.PUB_BIND)\n        ])\n        c_args = _set_client_parser().parse_args([\n            \'--port_in\', str(args.port_out),\n            \'--port_out\', str(args.port_in),\n            \'--socket_in\', str(SocketType.SUB_CONNECT)\n        ])\n        with RouterService(args), ZmqClient(c_args) as c1, ZmqClient(c_args) as c2:\n            msg = gnes_pb2.Message()\n            msg.request.index.docs.extend([gnes_pb2.Document() for _ in range(5)])\n            msg.envelope.num_part.extend([1, 3])\n            c1.send_message(msg)\n            c1.send_message(msg)\n            c1.send_message(msg)\n            r = c1.recv_message()\n            self.assertSequenceEqual(r.envelope.num_part, [1])\n            print(r.envelope.routes)\n\n    def test_chunk_reduce_router(self):\n        args = set_router_parser().parse_args([\n            \'--yaml_path\', self.chunk_router_yaml,\n            \'--socket_out\', str(SocketType.PUB_BIND)\n        ])\n        c_args = _set_client_parser().parse_args([\n            \'--port_in\', str(args.port_out),\n            \'--port_out\', str(args.port_in),\n            \'--socket_in\', str(SocketType.SUB_CONNECT)\n        ])\n        with RouterService(args), ZmqClient(c_args) as c1:\n            msg = gnes_pb2.Message()\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.1\n            s.score.explained = \'""1-c1""\'\n            s.chunk.doc_id = 1\n\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.2\n            s.score.explained = \'""1-c2""\'\n            s.chunk.doc_id = 2\n\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.3\n            s.score.explained = \'""1-c3""\'\n            s.chunk.doc_id = 1\n\n            msg.envelope.num_part.extend([1, 2])\n            c1.send_message(msg)\n\n            msg.response.search.ClearField(\'topk_results\')\n\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.2\n            s.score.explained = \'""2-c1""\'\n            s.chunk.doc_id = 1\n\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.2\n            s.score.explained = \'""2-c2""\'\n            s.chunk.doc_id = 2\n\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.3\n            s.score.explained = \'""2-c3""\'\n            s.chunk.doc_id = 3\n            c1.send_message(msg)\n            r = c1.recv_message()\n            self.assertSequenceEqual(r.envelope.num_part, [1])\n            self.assertEqual(len(r.response.search.topk_results), 3)\n            self.assertGreaterEqual(r.response.search.topk_results[0].score.value,\n                                    r.response.search.topk_results[-1].score.value)\n            print(r.response.search.topk_results)\n            self.assertEqual(json.loads(r.response.search.topk_results[0].score.explained)[\'operands\'],\n                             [\'1-c1\', \'1-c3\', \'2-c1\'])\n            self.assertEqual(json.loads(r.response.search.topk_results[1].score.explained)[\'operands\'],\n                             [\'1-c2\', \'2-c2\'])\n            self.assertEqual(json.loads(r.response.search.topk_results[2].score.explained)[\'operands\'], [\'2-c3\'])\n\n            self.assertAlmostEqual(r.response.search.topk_results[0].score.value, 0.6)\n            self.assertAlmostEqual(r.response.search.topk_results[1].score.value, 0.4)\n            self.assertAlmostEqual(r.response.search.topk_results[2].score.value, 0.3)\n\n    def test_doc_reduce_router(self):\n        args = set_router_parser().parse_args([\n            \'--yaml_path\', self.doc_router_yaml,\n            \'--socket_out\', str(SocketType.PUB_BIND)\n        ])\n        c_args = _set_client_parser().parse_args([\n            \'--port_in\', str(args.port_out),\n            \'--port_out\', str(args.port_in),\n            \'--socket_in\', str(SocketType.SUB_CONNECT)\n        ])\n        with RouterService(args), ZmqClient(c_args) as c1:\n            msg = gnes_pb2.Message()\n\n            # shard1 only has d1\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.1\n            s.doc.doc_id = 1\n            s.doc.raw_text = \'d1\'\n\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.2\n            s.doc.doc_id = 2\n\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.3\n            s.doc.doc_id = 3\n\n            msg.envelope.num_part.extend([1, 2])\n            c1.send_message(msg)\n\n            msg.response.search.ClearField(\'topk_results\')\n\n            # shard2 has d2 and d3\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.1\n            s.doc.doc_id = 1\n\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.2\n            s.doc.doc_id = 2\n            s.doc.raw_text = \'d2\'\n\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.3\n            s.doc.doc_id = 3\n            s.doc.raw_text = \'d3\'\n\n            msg.response.search.top_k = 5\n            c1.send_message(msg)\n            r = c1.recv_message()\n\n            print(r.response.search.topk_results)\n            self.assertSequenceEqual(r.envelope.num_part, [1])\n            self.assertEqual(len(r.response.search.topk_results), 3)\n\n    @unittest.SkipTest\n    def test_chunk_sum_reduce_router(self):\n        args = set_router_parser().parse_args([\n            \'--yaml_path\', self.chunk_sum_yaml,\n            \'--socket_out\', str(SocketType.PUB_BIND)\n        ])\n        c_args = _set_client_parser().parse_args([\n            \'--port_in\', str(args.port_out),\n            \'--port_out\', str(args.port_in),\n            \'--socket_in\', str(SocketType.SUB_CONNECT)\n        ])\n        with RouterService(args), ZmqClient(c_args) as c1:\n            msg = gnes_pb2.Message()\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.6\n            s.score.explained = json.dumps([\'1-c1\', \'1-c3\', \'2-c1\'])\n            s.doc.doc_id = 1\n\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.4\n            s.score.explained = json.dumps([\'1-c2\', \'2-c2\'])\n            s.doc.doc_id = 2\n\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.3\n            s.score.explained = json.dumps([\'2-c3\'])\n            s.doc.doc_id = 3\n\n            msg.envelope.num_part.extend([1, 2])\n            c1.send_message(msg)\n\n            msg.response.search.ClearField(\'topk_results\')\n\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.5\n            s.score.explained = json.dumps([\'2-c1\', \'1-c2\', \'1-c1\'])\n            s.doc.doc_id = 2\n\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.3\n            s.score.explained = json.dumps([\'1-c3\', \'2-c2\'])\n            s.doc.doc_id = 3\n\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.1\n            s.score.explained = json.dumps([\'2-c3\'])\n            s.doc.doc_id = 1\n            c1.send_message(msg)\n            r = c1.recv_message()\n            self.assertSequenceEqual(r.envelope.num_part, [1])\n            self.assertEqual(len(r.response.search.topk_results), 3)\n            self.assertGreaterEqual(r.response.search.topk_results[0].score.value,\n                                    r.response.search.topk_results[-1].score.value)\n            print(r.response.search.topk_results)\n            self.assertEqual(r.response.search.topk_results[0].score.explained, \'1-c2\\n2-c2\\n\\n2-c1\\n1-c2\\n1-c1\\n\\n\')\n            self.assertEqual(r.response.search.topk_results[1].score.explained, \'1-c1\\n1-c3\\n2-c1\\n\\n2-c3\\n\\n\')\n            self.assertEqual(r.response.search.topk_results[2].score.explained, \'2-c3\\n\\n1-c3\\n2-c2\\n\\n\')\n\n            self.assertAlmostEqual(r.response.search.topk_results[0].score.value, 0.9)\n            self.assertAlmostEqual(r.response.search.topk_results[1].score.value, 0.7)\n            self.assertAlmostEqual(r.response.search.topk_results[2].score.value, 0.6)\n\n    @unittest.SkipTest\n    def test_doc_sum_reduce_router(self):\n        args = set_router_parser().parse_args([\n            \'--yaml_path\', self.doc_sum_yaml,\n            \'--socket_out\', str(SocketType.PUB_BIND)\n        ])\n        c_args = _set_client_parser().parse_args([\n            \'--port_in\', str(args.port_out),\n            \'--port_out\', str(args.port_in),\n            \'--socket_in\', str(SocketType.SUB_CONNECT)\n        ])\n        with RouterService(args), ZmqClient(c_args) as c1:\n            msg = gnes_pb2.Message()\n\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.4\n            s.doc.doc_id = 1\n            s.doc.raw_text = \'d3\'\n            s.score.explained = \'1-d3\\n\'\n\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.3\n            s.doc.doc_id = 2\n            s.doc.raw_text = \'d2\'\n            s.score.explained = \'1-d2\\n\'\n\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.2\n            s.doc.doc_id = 3\n            s.doc.raw_text = \'d1\'\n            s.score.explained = \'1-d3\\n\'\n\n            msg.envelope.num_part.extend([1, 2])\n            c1.send_message(msg)\n\n            msg.response.search.ClearField(\'topk_results\')\n\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.5\n            s.doc.doc_id = 1\n            s.doc.raw_text = \'d2\'\n            s.score.explained = \'2-d2\\n\'\n\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.2\n            s.doc.doc_id = 2\n            s.doc.raw_text = \'d1\'\n            s.score.explained = \'2-d1\\n\'\n\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.1\n            s.doc.doc_id = 3\n            s.doc.raw_text = \'d3\'\n            s.score.explained = \'2-d3\\n\'\n\n            msg.response.search.top_k = 5\n            c1.send_message(msg)\n            r = c1.recv_message()\n\n            print(r.response.search.topk_results)\n            self.assertSequenceEqual(r.envelope.num_part, [1])\n            self.assertEqual(len(r.response.search.topk_results), 3)\n            self.assertGreaterEqual(r.response.search.topk_results[0].score.value,\n                                    r.response.search.topk_results[-1].score.value)\n\n    # @unittest.SkipTest\n    def test_concat_router(self):\n        args = set_router_parser().parse_args([\n            \'--yaml_path\', self.concat_router_yaml,\n            \'--socket_out\', str(SocketType.PUSH_BIND)\n        ])\n        c_args = _set_client_parser().parse_args([\n            \'--port_in\', str(args.port_out),\n            \'--port_out\', str(args.port_in),\n            \'--socket_in\', str(SocketType.PULL_CONNECT)\n        ])\n        # 10 chunks in each doc, dimension of chunk embedding is (5, 2)\n        with RouterService(args), ZmqClient(c_args) as c1:\n            msg = gnes_pb2.Message()\n            for i in range(10):\n                c = msg.request.search.query.chunks.add()\n                c.embedding.CopyFrom(array2blob(np.random.random([5, 2])))\n            msg.envelope.num_part.extend([1, 3])\n            c1.send_message(msg)\n            c1.send_message(msg)\n            c1.send_message(msg)\n            r = c1.recv_message()\n            self.assertSequenceEqual(r.envelope.num_part, [1])\n            print(r.envelope.routes)\n            for i in range(10):\n                self.assertEqual(r.request.search.query.chunks[i].embedding.shape, [5, 6])\n\n            for j in range(1, 4):\n                d = msg.request.index.docs.add()\n                for k in range(10):\n                    c = d.chunks.add()\n                    c.embedding.CopyFrom(array2blob(np.random.random([5, 2])))\n\n            c1.send_message(msg)\n            c1.send_message(msg)\n            c1.send_message(msg)\n            r = c1.recv_message()\n            self.assertSequenceEqual(r.envelope.num_part, [1])\n            for j in range(1, 4):\n                for i in range(10):\n                    self.assertEqual(r.request.index.docs[j - 1].chunks[i].embedding.shape, [5, 6])\n    \n    def test_avg_router(self):\n        args = set_router_parser().parse_args([\n            \'--yaml_path\', self.avg_router_yaml,\n            \'--socket_out\', str(SocketType.PUSH_BIND)\n        ])\n        c_args = _set_client_parser().parse_args([\n            \'--port_in\', str(args.port_out),\n            \'--port_out\', str(args.port_in),\n            \'--socket_in\', str(SocketType.PULL_CONNECT)\n        ])\n        # 10 chunks in each doc, dimension of chunk embedding is (5, 2)\n        with RouterService(args), ZmqClient(c_args) as c1:\n            msg = gnes_pb2.Message()\n            for i in range(10):\n                c = msg.request.search.query.chunks.add()\n                c.embedding.CopyFrom(array2blob(np.random.random([5, 2])))\n            msg.envelope.num_part.extend([1, 3])\n            c1.send_message(msg)\n            c1.send_message(msg)\n            c1.send_message(msg)\n            r = c1.recv_message()\n            self.assertSequenceEqual(r.envelope.num_part, [1])\n            print(r.envelope.routes)\n            for i in range(10):\n                self.assertEqual(r.request.search.query.chunks[i].embedding.shape, [5, 2])\n\n            for j in range(1, 4):\n                d = msg.request.index.docs.add()\n                for k in range(10):\n                    c = d.chunks.add()\n                    c.embedding.CopyFrom(array2blob(np.random.random([5, 2])))\n\n            c1.send_message(msg)\n            c1.send_message(msg)\n            c1.send_message(msg)\n            r = c1.recv_message()\n            self.assertSequenceEqual(r.envelope.num_part, [1])\n            for j in range(1, 4):\n                for i in range(10):\n                    self.assertEqual(r.request.index.docs[j - 1].chunks[i].embedding.shape, [5, 2])\n                    \n    def test_multimap_multireduce(self):\n        # p1 ->\n        #      p21 ->\n        #              r311\n        #              r312\n        #                   ->  r41\n        #                             -> r5\n        #      p22 ->\n        #              r321\n        #              r322\n        #                   -> r42\n        #                             -> r5\n        #                                       -> client\n        p1 = set_router_parser().parse_args([\n            \'--yaml_path\', self.publish_router_yaml,\n            \'--socket_in\', str(SocketType.PULL_CONNECT),\n            \'--socket_out\', str(SocketType.PUB_BIND),\n        ])\n        r5 = set_router_parser().parse_args([\n            \'--yaml_path\', self.reduce_router_yaml,\n            \'--socket_in\', str(SocketType.PULL_BIND),\n            \'--socket_out\', str(SocketType.PUSH_CONNECT),\n        ])\n        r41 = set_router_parser().parse_args([\n            \'--yaml_path\', self.reduce_router_yaml,\n            \'--socket_in\', str(SocketType.PULL_BIND),\n            \'--socket_out\', str(SocketType.PUSH_CONNECT),\n            \'--port_out\', str(r5.port_in)\n        ])\n        r42 = set_router_parser().parse_args([\n            \'--yaml_path\', self.reduce_router_yaml,\n            \'--socket_in\', str(SocketType.PULL_BIND),\n            \'--socket_out\', str(SocketType.PUSH_CONNECT),\n            \'--port_out\', str(r5.port_in)\n        ])\n        p21 = set_router_parser().parse_args([\n            \'--yaml_path\', self.publish_router_yaml,\n            \'--socket_in\', str(SocketType.SUB_CONNECT),\n            \'--socket_out\', str(SocketType.PUB_BIND),\n            \'--port_in\', str(p1.port_out),\n            \'--identity\', \'\'\n        ])\n        p22 = set_router_parser().parse_args([\n            \'--yaml_path\', self.publish_router_yaml,\n            \'--socket_in\', str(SocketType.SUB_CONNECT),\n            \'--socket_out\', str(SocketType.PUB_BIND),\n            \'--port_in\', str(p1.port_out),\n            \'--identity\', \'\'\n        ])\n        r311 = set_router_parser().parse_args([\n            \'--socket_in\', str(SocketType.SUB_CONNECT),\n            \'--socket_out\', str(SocketType.PUSH_CONNECT),\n            \'--port_in\', str(p21.port_out),\n            \'--port_out\', str(r41.port_in),\n            \'--yaml_path\', \'BaseRouter\',\n            \'--identity\', \'\'\n        ])\n        r312 = set_router_parser().parse_args([\n            \'--socket_in\', str(SocketType.SUB_CONNECT),\n            \'--socket_out\', str(SocketType.PUSH_CONNECT),\n            \'--port_in\', str(p21.port_out),\n            \'--port_out\', str(r41.port_in),\n            \'--yaml_path\', \'BaseRouter\',\n            \'--identity\', \'\'\n        ])\n        r321 = set_router_parser().parse_args([\n            \'--socket_in\', str(SocketType.SUB_CONNECT),\n            \'--socket_out\', str(SocketType.PUSH_CONNECT),\n            \'--port_in\', str(p22.port_out),\n            \'--port_out\', str(r42.port_in),\n            \'--yaml_path\', \'BaseRouter\',\n            \'--identity\', \'\'\n        ])\n        r322 = set_router_parser().parse_args([\n            \'--socket_in\', str(SocketType.SUB_CONNECT),\n            \'--socket_out\', str(SocketType.PUSH_CONNECT),\n            \'--port_in\', str(p22.port_out),\n            \'--port_out\', str(r42.port_in),\n            \'--yaml_path\', \'BaseRouter\',\n            \'--identity\', \'\'\n        ])\n\n        c_args = _set_client_parser().parse_args([\n            \'--port_in\', str(r5.port_out),\n            \'--port_out\', str(p1.port_in),\n            \'--socket_in\', str(SocketType.PULL_BIND),\n            \'--socket_out\', str(SocketType.PUSH_BIND),\n        ])\n        with RouterService(p1), RouterService(r5), \\\n             RouterService(p21), RouterService(p22), \\\n             RouterService(r311), RouterService(r312), RouterService(r321), RouterService(r322), \\\n             RouterService(r41), RouterService(r42), \\\n             ZmqClient(c_args) as c1:\n            msg = gnes_pb2.Message()\n            msg.envelope.num_part.append(1)\n            c1.send_message(msg)\n            r = c1.recv_message()\n            self.assertSequenceEqual(r.envelope.num_part, [1])\n            print(r.envelope.routes)\n'"
tests/test_score_fn.py,0,"b'import json\nimport unittest\nfrom pprint import pprint\n\nfrom gnes.proto import gnes_pb2\nfrom gnes.score_fn.base import get_unary_score, CombinedScoreFn, ModifierScoreFn\nfrom gnes.score_fn.chunk import WeightedChunkScoreFn, WeightedChunkOffsetScoreFn, CoordChunkScoreFn, TFIDFChunkScoreFn, BM25ChunkScoreFn\nfrom gnes.score_fn.doc import CoordDocScoreFn\nfrom gnes.score_fn.normalize import Normalizer1, Normalizer2, Normalizer3, Normalizer4\n\nfrom gnes.cli.parser import set_router_parser, _set_client_parser\nfrom gnes.service.base import SocketType\nfrom gnes.client.base import ZmqClient\nfrom gnes.service.router import RouterService\n\n\nclass TestScoreFn(unittest.TestCase):\n    def test_basic(self):\n        a = get_unary_score(0.5)\n        b = get_unary_score(0.7)\n        print(a)\n        print(b.explained)\n\n    def test_op(self):\n        a = get_unary_score(0.5)\n        b = get_unary_score(0.7)\n        sum_op = CombinedScoreFn(score_mode=\'sum\')\n        c = sum_op(a, b)\n        self.assertAlmostEqual(c.value, 1.2)\n\n        sq_op = ModifierScoreFn(modifier=\'square\')\n        c = sum_op(a, sq_op(b))\n        self.assertAlmostEqual(c.value, 0.99)\n        print(c)\n\n    def test_combine_score_fn(self):\n        from gnes.indexer.chunk.helper import ListKeyIndexer\n        from gnes.indexer.chunk.numpy import NumpyIndexer\n        from gnes.proto import array2blob\n        import numpy as np\n\n        q_chunk = gnes_pb2.Chunk()\n        q_chunk.doc_id = 2\n        q_chunk.weight = 0.3\n        q_chunk.offset = 0\n        q_chunk.embedding.CopyFrom(array2blob(np.array([3, 3, 3])))\n\n        for _fn in [WeightedChunkOffsetScoreFn, CoordChunkScoreFn, TFIDFChunkScoreFn, BM25ChunkScoreFn]:\n            indexer = NumpyIndexer(helper_indexer=ListKeyIndexer(), score_fn=_fn())\n            indexer.add(keys=[(0, 1), (1, 2)], vectors=np.array([[1, 1, 1], [2, 2, 2]]), weights=[0.5, 0.8])\n            queried_result = indexer.query_and_score(q_chunks=[q_chunk], top_k=2)\n\n    def test_doc_combine_score_fn(self):\n        from gnes.indexer.doc.dict import DictIndexer\n\n        document_list = []\n        document_id_list = []\n\n        for j in range(1, 4):\n            d = gnes_pb2.Document()\n            for i in range(1, 4):\n                c = d.chunks.add()\n                c.doc_id = j\n                c.offset = i\n                c.weight = 1 / 3\n            document_id_list.append(j)\n            document_list.append(d)\n\n        self.chunk_router_yaml = \'Chunk2DocTopkReducer\'\n\n        args = set_router_parser().parse_args([\n            \'--yaml_path\', self.chunk_router_yaml,\n            \'--socket_out\', str(SocketType.PUB_BIND)\n        ])\n        c_args = _set_client_parser().parse_args([\n            \'--port_in\', str(args.port_out),\n            \'--port_out\', str(args.port_in),\n            \'--socket_in\', str(SocketType.SUB_CONNECT)\n        ])\n        with RouterService(args), ZmqClient(c_args) as c1:\n            msg = gnes_pb2.Message()\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.1\n            s.score.explained = \'""1-c1""\'\n            s.chunk.doc_id = 1\n\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.2\n            s.score.explained = \'""1-c2""\'\n            s.chunk.doc_id = 2\n\n            s = msg.response.search.topk_results.add()\n\n            s.score.value = 0.3\n            s.score.explained = \'""1-c3""\'\n            s.chunk.doc_id = 1\n\n            msg.envelope.num_part.extend([1, 2])\n            c1.send_message(msg)\n\n            msg.response.search.ClearField(\'topk_results\')\n\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.2\n            s.score.explained = \'""2-c1""\'\n            s.chunk.doc_id = 1\n\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.2\n            s.score.explained = \'""2-c2""\'\n            s.chunk.doc_id = 2\n\n            s = msg.response.search.topk_results.add()\n            s.score.value = 0.3\n            s.score.explained = \'""2-c3""\'\n            s.chunk.doc_id = 3\n            c1.send_message(msg)\n            r = c1.recv_message()\n            doc_indexer = DictIndexer(score_fn=CoordDocScoreFn())\n            doc_indexer.add(keys=document_id_list, docs=document_list)\n\n            queried_result = doc_indexer.query_and_score(docs=r.response.search.topk_results, top_k=2)\n\n    def test_normalizer(self):\n        a = get_unary_score(0.5)\n        norm_op = Normalizer1()\n        b = norm_op(a)\n        pprint(json.loads(b.explained))\n\n        a = get_unary_score(0.5)\n        norm_op = Normalizer2(2)\n        b = norm_op(a)\n        pprint(json.loads(b.explained))\n        self.assertAlmostEqual(b.value, 0.8)\n\n        a = get_unary_score(0.5)\n        norm_op = Normalizer3(2)\n        b = norm_op(a)\n        pprint(json.loads(b.explained))\n        self.assertAlmostEqual(b.value, 0.7387961283389092)\n\n        a = get_unary_score(0.5)\n        norm_op = Normalizer4(2)\n        b = norm_op(a)\n        pprint(json.loads(b.explained))\n        self.assertEqual(b.value, 0.75)\n\n        norm_op = ModifierScoreFn(\'none\')\n        b = norm_op(a)\n        pprint(json.loads(b.explained))\n        self.assertEqual(b.value, 0.5)\n\n        q_chunk = gnes_pb2.Chunk()\n        q_chunk.weight = 0.5\n        q_chunk.offset = 1\n        d_chunk = gnes_pb2.Chunk()\n        d_chunk.weight = 0.7\n        d_chunk.offset = 2\n        rel_score = get_unary_score(2)\n        _op = WeightedChunkScoreFn()\n        c = _op(rel_score, q_chunk, d_chunk)\n        pprint(json.loads(c.explained))\n        self.assertAlmostEqual(c.value, 0.7)\n'"
tests/test_service_mgr.py,0,"b""import os\nimport unittest.mock\n\nimport grpc\n\nfrom gnes.cli.parser import set_router_parser, set_frontend_parser, set_encoder_parser, set_indexer_parser\nfrom gnes.proto import gnes_pb2_grpc, RequestGenerator\nfrom gnes.service.base import ServiceManager, SocketType, ParallelType\nfrom gnes.service.frontend import FrontendService\nfrom gnes.service.indexer import IndexerService\nfrom gnes.service.router import RouterService\n\n\nclass TestServiceManager(unittest.TestCase):\n    def setUp(self):\n        self.all_bytes = [b'abc', b'def', b'cde'] * 10\n        self.all_bytes2 = [b'abc', b'def', b'cde']\n        self.dir_path = os.path.dirname(__file__)\n        os.unsetenv('http_proxy')\n        os.unsetenv('https_proxy')\n\n    def test_frontend_alone(self):\n        args = set_frontend_parser().parse_args([\n            '--grpc_host', '127.0.0.1',\n\n        ])\n\n        with FrontendService(args):\n            pass\n\n        with ServiceManager(FrontendService, args):\n            pass\n\n    def _test_multiple_router(self, backend='thread', num_parallel=5):\n        a = set_router_parser().parse_args([\n            '--yaml_path', 'BaseRouter',\n            '--num_parallel', str(num_parallel),\n            '--parallel_backend', backend\n        ])\n        with ServiceManager(RouterService, a):\n            pass\n\n    def _test_grpc_multiple_router(self, backend='thread', num_parallel=5):\n        args = set_frontend_parser().parse_args([\n            '--grpc_host', '127.0.0.1',\n\n        ])\n\n        p_args = set_router_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', str(args.port_in),\n            '--socket_in', str(SocketType.PULL_CONNECT),\n            '--socket_out', str(SocketType.PUSH_CONNECT),\n            '--yaml_path', 'BaseRouter',\n            '--num_parallel', str(num_parallel),\n            '--parallel_backend', backend\n        ])\n\n        with ServiceManager(RouterService, p_args), FrontendService(args), grpc.insecure_channel(\n                '%s:%d' % (args.grpc_host, args.grpc_port),\n                options=[('grpc.max_send_message_length', 70 * 1024 * 1024),\n                         ('grpc.max_receive_message_length', 70 * 1024 * 1024)]) as channel:\n            stub = gnes_pb2_grpc.GnesRPCStub(channel)\n            resp = stub.Call(list(RequestGenerator.query(b'abc', 1))[0])\n            self.assertEqual(resp.request_id, 0)\n\n    def _test_grpc_multiple_pub(self, backend='thread', num_parallel=5):\n        args = set_frontend_parser().parse_args([\n            '--grpc_host', '127.0.0.1',\n\n        ])\n\n        p_args = set_router_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', str(args.port_in),\n            '--socket_in', str(SocketType.PULL_CONNECT),\n            '--socket_out', str(SocketType.PUSH_CONNECT),\n            '--yaml_path', 'BaseRouter',\n            '--num_parallel', str(num_parallel),\n            '--parallel_backend', backend,\n            '--parallel_type', str(ParallelType.PUB_BLOCK)\n        ])\n\n        with ServiceManager(RouterService, p_args), FrontendService(args), grpc.insecure_channel(\n                '%s:%d' % (args.grpc_host, args.grpc_port),\n                options=[('grpc.max_send_message_length', 70 * 1024 * 1024),\n                         ('grpc.max_receive_message_length', 70 * 1024 * 1024)]) as channel:\n            stub = gnes_pb2_grpc.GnesRPCStub(channel)\n            resp = stub.Call(list(RequestGenerator.query(b'abc', 1))[0])\n            self.assertEqual(resp.request_id, 0)\n\n    def test_external_module(self):\n        args = set_encoder_parser().parse_args([\n            '--yaml_path', os.path.join(self.dir_path, 'contrib', 'dummy.yml'),\n            '--py_path', os.path.join(self.dir_path, 'contrib', 'dummy_contrib.py'),\n        ])\n\n        with ServiceManager(RouterService, args):\n            pass\n\n    def test_override_module(self):\n        args = set_indexer_parser().parse_args([\n            '--yaml_path', os.path.join(self.dir_path, 'contrib', 'fake_faiss.yml'),\n            '--py_path', os.path.join(self.dir_path, 'contrib', 'fake_faiss.py'),\n        ])\n\n        with ServiceManager(IndexerService, args):\n            pass\n\n    def test_override_twice_module(self):\n        args = set_indexer_parser().parse_args([\n            '--yaml_path', os.path.join(self.dir_path, 'contrib', 'fake_faiss.yml'),\n            '--py_path', os.path.join(self.dir_path, 'contrib', 'fake_faiss.py'),\n            os.path.join(self.dir_path, 'contrib', 'fake_faiss2.py')\n        ])\n\n        with ServiceManager(IndexerService, args):\n            pass\n\n    def test_grpc_with_pub(self):\n        self._test_grpc_multiple_pub('thread', 1)\n        self._test_grpc_multiple_pub('process', 1)\n        self._test_grpc_multiple_pub('thread', 5)\n        self._test_grpc_multiple_pub('process', 5)\n\n    def test_grpc_with_multi_service(self):\n        self._test_grpc_multiple_router('thread', 1)\n        self._test_grpc_multiple_router('process', 1)\n        self._test_grpc_multiple_router('thread', 5)\n        self._test_grpc_multiple_router('process', 5)\n\n    def test_multiple_router(self):\n        self._test_multiple_router('thread', 1)\n        self._test_multiple_router('process', 1)\n        self._test_multiple_router('thread', 5)\n        self._test_multiple_router('process', 5)\n"""
tests/test_simple_indexer.py,0,"b""import os\nimport unittest\n\nimport numpy as np\n\nfrom gnes.helper import batch_iterator, TimeContext\nfrom gnes.indexer.chunk.helper import DictKeyIndexer, NumpyKeyIndexer, ListKeyIndexer, ListNumpyKeyIndexer\n\n\nclass TestProto(unittest.TestCase):\n    def setUp(self):\n        self.num_sample = 1000000\n        self.num_query = 10000\n        self.keys = np.array([j for j in range(self.num_sample)])\n        self.key_offset = np.stack([self.keys, np.random.randint(0, 255, size=[self.num_sample])],\n                                   axis=1).tolist()\n        self.weights = np.random.random(size=[self.num_sample]).tolist()\n        self.query = np.random.randint(0, self.num_sample, size=[self.num_query]).tolist()\n        self.dump_path = './dump.bin'\n\n    def tearDown(self):\n        if os.path.exists(self.dump_path):\n            os.remove(self.dump_path)\n\n    def _test_any(self, cls):\n        a = cls()\n        self.assertEqual(a.num_chunks, 0)\n        a.add(self.key_offset, self.weights)\n        self.assertEqual(a.num_chunks, self.num_sample)\n\n        res1 = a.query(self.query)\n        res2 = [(*self.key_offset[q], self.weights[q]) for q in self.query]\n        self.assertListEqual(res1, res2)\n\n        # test dump and reload\n        a.dump(self.dump_path)\n        b = cls.load(self.dump_path)\n        res1 = a.query(self.query)\n        res2 = b.query(self.query)\n        self.assertListEqual(res1, res2)\n\n    def test_numpy(self):\n        self._test_any(NumpyKeyIndexer)\n\n    def test_list(self):\n        self._test_any(ListKeyIndexer)\n\n    def test_fast_list(self):\n        self._test_any(ListNumpyKeyIndexer)\n\n    def test_dict(self):\n        self._test_any(DictKeyIndexer)\n\n    def test_bench_numpy_list(self):\n        for cls in [ListKeyIndexer, NumpyKeyIndexer, ListNumpyKeyIndexer, DictKeyIndexer]:\n            a = cls()\n            b_size = 1000\n            with TimeContext('%s:add()' % cls.__name__):\n                for k, w in zip(batch_iterator(self.key_offset, b_size), batch_iterator(self.weights, b_size)):\n                    a.add(k, w)\n                self.assertEqual(a.num_docs, self.num_sample)\n                self.assertEqual(a.num_chunks, self.num_sample)\n\n            with TimeContext('%s:query()' % cls.__name__):\n                for k in batch_iterator(self.query, b_size):\n                    a.query(k)\n"""
tests/test_stream_grpc.py,0,"b""import os\nimport time\nimport unittest.mock\n\nimport grpc\n\nfrom gnes.cli.parser import set_frontend_parser, set_router_parser\nfrom gnes.helper import TimeContext\nfrom gnes.proto import RequestGenerator, gnes_pb2_grpc\nfrom gnes.service.base import SocketType, MessageHandler, BaseService as BS\nfrom gnes.service.frontend import FrontendService\nfrom gnes.service.router import RouterService\n\n\nclass Router1(RouterService):\n    handler = MessageHandler(BS.handler)\n\n    @handler.register(NotImplementedError)\n    def _handler_default(self, msg: 'gnes_pb2.Message'):\n        self.logger.info('im doing fancy jobs...')\n        time.sleep(2)\n        super()._handler_default(msg)\n\n\nclass Router2(RouterService):\n    handler = MessageHandler(BS.handler)\n\n    @handler.register(NotImplementedError)\n    def _handler_default(self, msg: 'gnes_pb2.Message'):\n        self.logger.info('im doing stupid jobs...')\n        time.sleep(6)\n        super()._handler_default(msg)\n\n\nclass TestStreamgRPC(unittest.TestCase):\n\n    def setUp(self):\n        self.all_bytes = [b'abc', b'def', b'cde'] * 10\n        self.all_bytes2 = [b'abc', b'def', b'cde']\n        os.unsetenv('http_proxy')\n        os.unsetenv('https_proxy')\n\n    def test_bm_frontend(self):\n        args = set_frontend_parser().parse_args([\n            '--grpc_host', '127.0.0.1',\n\n        ])\n\n        p_args = set_router_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', str(args.port_in),\n            '--socket_in', str(SocketType.PULL_CONNECT),\n            '--socket_out', str(SocketType.PUSH_CONNECT),\n            '--yaml_path', 'BaseRouter'\n        ])\n\n    def test_grpc_frontend(self):\n        args = set_frontend_parser().parse_args([\n            '--grpc_host', '127.0.0.1',\n\n        ])\n\n        p_args = set_router_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', str(args.port_in),\n            '--socket_in', str(SocketType.PULL_CONNECT),\n            '--socket_out', str(SocketType.PUSH_CONNECT),\n            '--yaml_path', 'BaseRouter'\n        ])\n\n        with RouterService(p_args), FrontendService(args), grpc.insecure_channel(\n                '%s:%d' % (args.grpc_host, args.grpc_port),\n                options=[('grpc.max_send_message_length', 70 * 1024 * 1024),\n                         ('grpc.max_receive_message_length', 70 * 1024 * 1024)]) as channel:\n            stub = gnes_pb2_grpc.GnesRPCStub(channel)\n            with TimeContext('sync call'):  # about 5s\n                resp = list(stub.StreamCall(RequestGenerator.train(self.all_bytes, batch_size=1)))[-1]\n\n            self.assertEqual(resp.request_id, len(self.all_bytes))  # idx start with 0, but +1 for final FLUSH\n\n    def test_async_block(self):\n        args = set_frontend_parser().parse_args([\n            '--grpc_host', '127.0.0.1',\n        ])\n\n        p1_args = set_router_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', '8899',\n            '--socket_in', str(SocketType.PULL_CONNECT),\n            '--socket_out', str(SocketType.PUSH_CONNECT),\n            '--yaml_path', 'BaseRouter'\n        ])\n\n        p2_args = set_router_parser().parse_args([\n            '--port_in', str(p1_args.port_out),\n            '--port_out', str(args.port_in),\n            '--socket_in', str(SocketType.PULL_BIND),\n            '--socket_out', str(SocketType.PUSH_CONNECT),\n            '--yaml_path', 'BaseRouter'\n        ])\n\n        with FrontendService(args), Router1(p1_args), Router2(p2_args), grpc.insecure_channel(\n                '%s:%d' % (args.grpc_host, args.grpc_port),\n                options=[('grpc.max_send_message_length', 70 * 1024 * 1024),\n                         ('grpc.max_receive_message_length', 70 * 1024 * 1024)]) as channel:\n            stub = gnes_pb2_grpc.GnesRPCStub(channel)\n            id = 0\n            with TimeContext('non-blocking call'):  # about 26s = 32s (total) - 3*2s (overlap)\n                resp = stub.StreamCall(RequestGenerator.train(self.all_bytes2, batch_size=1))\n                for r in resp:\n                    self.assertEqual(r.request_id, id)\n                    id += 1\n\n            id = 0\n            with TimeContext('blocking call'):  # should be 32 s\n                for r in RequestGenerator.train(self.all_bytes2, batch_size=1):\n                    resp = stub.Call(r)\n                    self.assertEqual(resp.request_id, id)\n                    id += 1\n            # self.assertEqual(resp.result().request_id, str(len(self.all_bytes)))\n\n            # self.assertEqual(resp.request_id, str(len(self.all_bytes2)))  # idx start with 0, but +1 for final FLUSH\n"""
tests/test_uuid.py,0,"b'import unittest\n\nfrom gnes.uuid import BaseIDGenerator, SnowflakeIDGenerator\n\nclass TestUUID(unittest.TestCase):\n    def test_base_uuid(self):\n        uuid_generator = BaseIDGenerator()\n        last = -1\n        for _ in range(10000):\n            nid = uuid_generator.next()\n            self.assertGreater(nid, last)\n            last = nid\n\n\n    def test_snoflake(self):\n        uuid_generator = SnowflakeIDGenerator()\n        last = -1\n        for _ in range(10000):\n            nid = uuid_generator.next()\n            self.assertGreater(nid, last)\n            last = nid\n'"
tests/test_vggish.py,0,"b""import os\nimport unittest\nimport numpy as np\n\nfrom gnes.encoder.audio.vggish import VggishEncoder\n\n\nclass TestVggishEncoder(unittest.TestCase):\n    @unittest.skip\n    def setUp(self):\n        self.dirname = os.path.dirname(__file__)\n        self.video_path = os.path.join(self.dirname, 'videos')\n        self.video_bytes = [open(os.path.join(self.video_path, _), 'rb').read()\n                            for _ in os.listdir(self.video_path)]\n        self.audios = [np.random.rand(10, 96, 64),\n                       np.random.rand(15, 96, 64),\n                       np.random.rand(5, 96, 64)]\n        self.vggish_yaml = os.path.join(self.dirname, 'yaml', 'vggish-encoder.yml')\n\n    @unittest.skip\n    def test_vggish_encoding(self):\n        self.encoder = VggishEncoder.load_yaml(self.vggish_yaml)\n        vec = self.encoder.encode(self.audios)\n        self.assertEqual(len(vec.shape), 2)\n        self.assertEqual(vec.shape[0], len(self.audios))\n        self.assertEqual(vec.shape[1], 128)"""
tests/test_vggish_example.py,0,"b""import unittest\nfrom gnes.cli.parser import set_preprocessor_parser, _set_client_parser\nfrom gnes.client.base import ZmqClient\nfrom gnes.proto import gnes_pb2, RequestGenerator, blob2array\nfrom gnes.service.preprocessor import PreprocessorService\nimport os\n\n\nclass TestVggishPreprocessor(unittest.TestCase):\n    def setUp(self):\n        self.dirname = os.path.dirname(__file__)\n        self.yml_path = os.path.join(self.dirname, 'yaml', 'vggish.yml')\n        self.video_path = os.path.join(self.dirname, 'videos')\n        self.video_bytes = [open(os.path.join(self.video_path, _), 'rb').read()\n                            for _ in os.listdir(self.video_path)]\n\n    def test_vggish_preprocessor_service_realdata(self):\n        args = set_preprocessor_parser().parse_args([\n            '--yaml_path', self.yml_path\n        ])\n\n        c_args = _set_client_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', str(args.port_in)\n        ])\n\n        with PreprocessorService(args), ZmqClient(c_args) as client:\n            for req in RequestGenerator.index(self.video_bytes):\n                msg = gnes_pb2.Message()\n                msg.request.index.CopyFrom(req.index)\n                client.send_message(msg)\n                r = client.recv_message()\n                for d in r.request.index.docs:\n                    self.assertGreater(len(d.chunks), 0)\n                    for _ in range(len(d.chunks)):\n                        shape = blob2array(d.chunks[_].blob).shape\n                        self.assertEqual(len(shape), 3)"""
tests/test_video_decode_preprocessor.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport os\nimport unittest\n\nfrom gnes.cli.parser import set_preprocessor_parser, _set_client_parser\nfrom gnes.client.base import ZmqClient\nfrom gnes.proto import gnes_pb2, RequestGenerator, blob2array\nfrom gnes.service.base import ServiceManager\nfrom gnes.service.preprocessor import PreprocessorService\n\n\nclass TestVideoDecode(unittest.TestCase):\n\n    def setUp(self):\n        self.dirname = os.path.dirname(__file__)\n        self.yml_path = os.path.join(self.dirname, \'yaml\', \'preprocessor-video_decode.yml\')\n        self.video_path = os.path.join(self.dirname, \'videos\')\n\n    def test_video_decode_preprocessor(self):\n        args = set_preprocessor_parser().parse_args([\'--yaml_path\', self.yml_path])\n        c_args = _set_client_parser().parse_args([\n            \'--port_in\', str(args.port_out),\n            \'--port_out\', str(args.port_in)])\n        video_bytes = [\n            open(os.path.join(self.video_path, _), \'rb\').read()\n            for _ in os.listdir(self.video_path)\n        ]\n\n        with ServiceManager(PreprocessorService, args), ZmqClient(c_args) as client:\n            for req in RequestGenerator.index(video_bytes):\n                msg = gnes_pb2.Message()\n                msg.request.index.CopyFrom(req.index)\n                client.send_message(msg)\n                r = client.recv_message()\n                for d in r.request.index.docs:\n                    self.assertGreater(len(d.chunks), 0)\n                    for _ in range(len(d.chunks)):\n                        shape = blob2array(d.chunks[_].blob).shape\n                        self.assertEqual(shape, (299, 299, 3))\n'"
tests/test_video_encoder_preprocessor.py,0,"b""import copy\nimport os\nimport unittest\n\nimport numpy as np\nfrom gnes.proto import gnes_pb2, array2blob\n\nfrom gnes.preprocessor.base import BaseVideoPreprocessor\n\n\nclass TestVideoEncoder(unittest.TestCase):\n    def setUp(self):\n        self.dirname = os.path.dirname(__file__)\n        self.mp4_yaml_path = os.path.join(self.dirname, 'yaml', 'mp4-encoder.yml')\n        self.gif_yaml_path = os.path.join(self.dirname, 'yaml', 'gif-encoder.yml')\n        self.dump_path = os.path.join(self.dirname, 'video_encoder.bin')\n        self.frames_path = os.path.join(self.dirname, 'frames', 'frames.npy')\n        self.mp4_encoder = BaseVideoPreprocessor.load_yaml(self.mp4_yaml_path)\n        self.gif_encoder = BaseVideoPreprocessor.load_yaml(self.gif_yaml_path)\n        self.video_frames = np.load(self.frames_path)\n\n\n    def test_mp4_encoder(self):\n        raw_data = array2blob(self.video_frames)\n\n        doc = gnes_pb2.Document()\n        doc.doc_type = gnes_pb2.Document.VIDEO\n        doc.raw_video.CopyFrom(raw_data)\n        self.mp4_encoder.apply(doc)\n        doc1 = copy.deepcopy(doc)\n\n        doc = gnes_pb2.Document()\n        doc.doc_type = gnes_pb2.Document.VIDEO\n        chunk = doc.chunks.add()\n        chunk.blob.CopyFrom(raw_data)\n        self.mp4_encoder.apply(doc)\n        doc2 = copy.deepcopy(doc)\n\n        self.assertEqual(doc1.raw_bytes, doc2.chunks[0].raw)\n\n    def test_gif_encoder(self):\n        raw_data = array2blob(self.video_frames)\n\n        doc = gnes_pb2.Document()\n        doc.doc_type = gnes_pb2.Document.VIDEO\n        doc.raw_video.CopyFrom(raw_data)\n        self.gif_encoder.apply(doc)\n        doc1 = copy.deepcopy(doc)\n\n        doc = gnes_pb2.Document()\n        doc.doc_type = gnes_pb2.Document.VIDEO\n        chunk = doc.chunks.add()\n        chunk.blob.CopyFrom(raw_data)\n        self.gif_encoder.apply(doc)\n        doc2 = copy.deepcopy(doc)\n\n        self.assertEqual(doc1.raw_bytes, doc2.chunks[0].raw)\n\n    def test_empty_doc(self):\n        doc = gnes_pb2.Document()\n        doc.doc_type = gnes_pb2.Document.VIDEO\n        self.mp4_encoder.apply(doc)\n\n    def test_dump_load(self):\n        raw_data = array2blob(self.video_frames)\n\n        doc = gnes_pb2.Document()\n        doc.doc_type = gnes_pb2.Document.VIDEO\n        doc.raw_video.CopyFrom(raw_data)\n        self.mp4_encoder.apply(doc)\n        doc1 = copy.deepcopy(doc)\n\n        self.mp4_encoder.dump(self.dump_path)\n\n        encoder = BaseVideoPreprocessor.load(self.dump_path)\n\n        doc = gnes_pb2.Document()\n        doc.doc_type = gnes_pb2.Document.VIDEO\n        chunk = doc.chunks.add()\n        chunk.blob.CopyFrom(raw_data)\n        encoder.apply(doc)\n        doc2 = copy.deepcopy(doc)\n\n        self.assertEqual(doc1.raw_bytes, doc2.chunks[0].raw)\n\n\n    def tearDown(self):\n        if os.path.exists(self.dump_path):\n            os.remove(self.dump_path)\n"""
tests/test_video_preprocessor.py,0,"b""import os\nimport unittest\n\nfrom gnes.cli.parser import set_preprocessor_parser, _set_client_parser\nfrom gnes.client.base import ZmqClient\nfrom gnes.proto import gnes_pb2, RequestGenerator, blob2array\nfrom gnes.service.preprocessor import PreprocessorService\n\n\nclass TestFFmpeg(unittest.TestCase):\n\n    def setUp(self):\n        self.dirname = os.path.dirname(__file__)\n        self.yml_path = os.path.join(self.dirname, 'yaml', 'preprocessor-ffmpeg.yml')\n        self.yml_path_2 = os.path.join(self.dirname, 'yaml', 'preprocessor-ffmpeg2.yml')\n        self.yml_path_3 = os.path.join(self.dirname, 'yaml', 'preprocessor-ffmpeg3.yml')\n        self.yml_path_4 = os.path.join(self.dirname, 'yaml', 'preprocessor-ffmpeg4.yml')\n        self.video_path = os.path.join(self.dirname, 'videos')\n        self.video_bytes = [open(os.path.join(self.video_path, _), 'rb').read()\n                            for _ in os.listdir(self.video_path)]\n    def test_video_preprocessor_service_empty(self):\n        args = set_preprocessor_parser().parse_args([\n            '--yaml_path', self.yml_path\n        ])\n        with PreprocessorService(args):\n            pass\n\n    def test_video_preprocessor_service_realdata(self):\n        args = set_preprocessor_parser().parse_args([\n            '--yaml_path', self.yml_path\n        ])\n\n        c_args = _set_client_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', str(args.port_in)\n        ])\n\n        with PreprocessorService(args), ZmqClient(c_args) as client:\n            for req in RequestGenerator.index(self.video_bytes):\n                msg = gnes_pb2.Message()\n                msg.request.index.CopyFrom(req.index)\n                client.send_message(msg)\n                r = client.recv_message()\n                for d in r.request.index.docs:\n                    self.assertGreater(len(d.chunks), 0)\n                    for _ in range(len(d.chunks)):\n                        shape = blob2array(d.chunks[_].blob).shape\n                        self.assertEqual(shape, (168, 192, 3))\n\n    def test_video_cut_by_frame(self):\n        args = set_preprocessor_parser().parse_args([\n            '--yaml_path', self.yml_path_2,\n        ])\n        c_args = _set_client_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', str(args.port_in)\n        ])\n\n        with PreprocessorService(args), ZmqClient(c_args) as client:\n            for req in RequestGenerator.index(self.video_bytes):\n                msg = gnes_pb2.Message()\n                msg.request.index.CopyFrom(req.index)\n                client.send_message(msg)\n                r = client.recv_message()\n                for d in r.request.index.docs:\n                    self.assertGreater(len(d.chunks), 0)\n                    for _ in range(len(d.chunks) - 1):\n                        shape = blob2array(d.chunks[_].blob).shape\n                        self.assertEqual(shape, (30, 168, 192, 3))\n                    shape = blob2array(d.chunks[-1].blob).shape\n                    self.assertLessEqual(shape[0], 30)\n\n    def test_video_cut_by_num(self):\n        args = set_preprocessor_parser().parse_args([\n            '--yaml_path', self.yml_path_3\n        ])\n        c_args = _set_client_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', str(args.port_in)\n        ])\n\n        with PreprocessorService(args), ZmqClient(c_args) as client:\n            for req in RequestGenerator.index(self.video_bytes):\n                msg = gnes_pb2.Message()\n                msg.request.index.CopyFrom(req.index)\n                client.send_message(msg)\n                r = client.recv_message()\n                for d in r.request.index.docs:\n                    self.assertEqual(len(d.chunks), 6)\n\n    def test_video_cut_by_clustering(self):\n        args = set_preprocessor_parser().parse_args([\n            '--yaml_path', self.yml_path_4\n        ])\n        c_args = _set_client_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', str(args.port_in)\n        ])\n\n        with PreprocessorService(args), ZmqClient(c_args) as client:\n            for req in RequestGenerator.index(self.video_bytes):\n                msg = gnes_pb2.Message()\n                msg.request.index.CopyFrom(req.index)\n                client.send_message(msg)\n                r = client.recv_message()\n                for d in r.request.index.docs:\n                    self.assertEqual(len(d.chunks), 6)\n"""
tests/test_video_shotdetect_preprocessor.py,0,"b""import os\nimport unittest\n\nfrom gnes.cli.parser import set_preprocessor_parser, _set_client_parser\nfrom gnes.client.base import ZmqClient\nfrom gnes.proto import gnes_pb2, RequestGenerator, blob2array\nfrom gnes.service.preprocessor import PreprocessorService\n\n\nclass TestShotDetector(unittest.TestCase):\n\n    def setUp(self):\n        self.dirname = os.path.dirname(__file__)\n        self.histogram_yml_path = os.path.join(self.dirname, 'yaml', 'preprocessor-shotdetect_histogram.yml')\n        self.edge_yml_path = os.path.join(self.dirname, 'yaml', 'preprocessor-shotdetect_edge.yml')\n        self.video_path = os.path.join(self.dirname, 'videos')\n\n    def test_video_preprocessor_service_empty(self):\n        args = set_preprocessor_parser().parse_args([\n            '--yaml_path', self.histogram_yml_path\n        ])\n        with PreprocessorService(args):\n            pass\n\n    def test_video_preprocessor_service_realdata_histogram(self):\n        args = set_preprocessor_parser().parse_args([\n            '--yaml_path', self.histogram_yml_path\n        ])\n        c_args = _set_client_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', str(args.port_in)\n        ])\n        video_bytes = [open(os.path.join(self.video_path, _), 'rb').read()\n                       for _ in os.listdir(self.video_path)]\n\n        with PreprocessorService(args), ZmqClient(c_args) as client:\n            for req in RequestGenerator.index(video_bytes):\n                msg = gnes_pb2.Message()\n                msg.request.index.CopyFrom(req.index)\n                client.send_message(msg)\n                r = client.recv_message()\n                for d in r.request.index.docs:\n                    self.assertGreater(len(d.chunks), 0)\n                    for _ in range(len(d.chunks)):\n                        shape = blob2array(d.chunks[_].blob).shape\n                        self.assertEqual(shape[1:], (168, 192, 3))\n\n    def test_video_preprocessor_service_realdata_edge(self):\n        args = set_preprocessor_parser().parse_args([\n            '--yaml_path', self.edge_yml_path\n        ])\n        c_args = _set_client_parser().parse_args([\n            '--port_in', str(args.port_out),\n            '--port_out', str(args.port_in)\n        ])\n        video_bytes = [open(os.path.join(self.video_path, _), 'rb').read()\n                       for _ in os.listdir(self.video_path)]\n\n        with PreprocessorService(args), ZmqClient(c_args) as client:\n            for req in RequestGenerator.index(video_bytes):\n                msg = gnes_pb2.Message()\n                msg.request.index.CopyFrom(req.index)\n                client.send_message(msg)\n                r = client.recv_message()\n                for d in r.request.index.docs:\n                    self.assertGreater(len(d.chunks), 0)\n                    for _ in range(len(d.chunks)):\n                        shape = blob2array(d.chunks[_].blob).shape\n                        self.assertEqual(shape[1:], (168, 192, 3))\n"""
tests/test_vlad.py,0,"b""import os\nimport unittest\nimport numpy as np\nfrom gnes.encoder.numeric.vlad import VladEncoder\n\n\nclass TestVladEncoder(unittest.TestCase):\n    def setUp(self):\n        self.mock_train_data = np.random.random([1, 200, 128]).astype(np.float32)\n        self.mock_eval_data = np.random.random([2, 2, 128]).astype(np.float32)\n        self.dump_path = os.path.join(os.path.dirname(__file__), 'vlad.bin')\n\n    def tearDown(self):\n        if os.path.exists(self.dump_path):\n            os.remove(self.dump_path)\n\n    def test_vlad_train(self):\n        model = VladEncoder(20)\n        model.train(self.mock_train_data)\n        self.assertEqual(model.centroids.shape, (20, 128))\n        v = model.encode(self.mock_eval_data)\n        self.assertEqual(v.shape, (2, 2560))\n\n    def test_vlad_dump_load(self):\n        model = VladEncoder(20)\n        model.train(self.mock_train_data)\n        model.dump(self.dump_path)\n        model_new = VladEncoder.load(self.dump_path)\n        self.assertEqual(model_new.centroids.shape, (20, 128))\n"""
tests/test_w2v_encoder.py,0,"b'import os\nimport unittest\n\nfrom gnes.encoder.text.w2v import Word2VecEncoder\n\n\nclass TestW2vEncoder(unittest.TestCase):\n\n    def setUp(self):\n        dirname = os.path.dirname(__file__)\n        self.dump_path = os.path.join(dirname, \'w2v_encoder.bin\')\n        self.test_str = []\n        with open(os.path.join(dirname, \'tangshi.txt\')) as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    self.test_str.append(line)\n\n    def test_encoding(self):\n        w2v_encoder = Word2VecEncoder(\n            model_dir=os.environ[\'WORD2VEC_MODEL\'],\n            pooling_strategy=""REDUCE_MEAN"")\n        vec = w2v_encoder.encode(self.test_str)\n        self.assertEqual(vec.shape[0], len(self.test_str))\n        self.assertEqual(vec.shape[1], 300)\n\n    def test_dump_load(self):\n        w2v_encoder = Word2VecEncoder(\n            model_dir=os.environ[\'WORD2VEC_MODEL\'],\n            pooling_strategy=""REDUCE_MEAN"")\n        w2v_encoder.dump(self.dump_path)\n        w2v_encoder2 = Word2VecEncoder.load(self.dump_path)\n        vec = w2v_encoder2.encode(self.test_str)\n        self.assertEqual(vec.shape[0], len(self.test_str))\n        self.assertEqual(vec.shape[1], 300)\n\n    def tearDown(self):\n        if os.path.exists(self.dump_path):\n            os.remove(self.dump_path)\n'"
tests/test_yaml.py,0,"b'import os\nimport unittest\nfrom shutil import rmtree\n\nfrom gnes.base import TrainableType\nfrom gnes.encoder.base import PipelineEncoder\nfrom gnes.encoder.numeric.pca import PCALocalEncoder\nfrom gnes.encoder.numeric.pq import PQEncoder\nfrom gnes.encoder.numeric.tf_pq import TFPQEncoder\n\n\nclass foo(metaclass=TrainableType):\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def close(self):\n        pass\n\n\nclass foo1(foo):\n    store_args_kwargs = False\n\n    def __init__(self, a, b=1, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        pass\n\n\nclass foo2(foo1):\n    def __init__(self, c, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        pass\n\n\nclass dummyPipeline(PipelineEncoder):\n    store_args_kwargs = True\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.components = lambda: [foo1(*args, **kwargs),\n                                   foo2(*args, **kwargs), ]\n\n\nclass TestYaml(unittest.TestCase):\n    def setUp(self):\n        dirname = os.path.dirname(__file__)\n        self.dump_path = os.path.join(dirname, \'dump.yml\')\n        self.db_path = \'./test_leveldb\'\n\n    def tearDown(self):\n        if os.path.exists(self.db_path):\n            rmtree(self.db_path)\n        if os.path.exists(self.dump_path):\n            os.remove(self.dump_path)\n\n    def test_signature(self):\n        a = foo1(2)\n        self.assertEqual(a._init_kwargs_dict, {\'a\': 2, \'b\': 1})\n        a = foo2(2, 3, 3)\n        self.assertEqual(a._init_kwargs_dict, {\'c\': 2, \'a\': 3, \'b\': 3})\n        self.assertRaises(TypeError, foo2, 2, 3, c=2)\n        a = foo2(2, 3, 4, 5, 6, 7)\n        self.assertEqual(a._init_kwargs_dict, {\'c\': 2, \'a\': 3, \'b\': 4})\n        a = foo2(2, 3, wee=4)\n        self.assertEqual(a._init_kwargs_dict, {\'c\': 2, \'a\': 3, \'b\': 1})\n        a = foo2(b=1, wee=4, a=3, c=2)\n        self.assertEqual(a._init_kwargs_dict, {\'c\': 2, \'a\': 3, \'b\': 1})\n\n    def test_dump(self):\n        pe0 = dummyPipeline(a=23, b=32, c=[\'123\', \'456\'])\n        pe0.dump_yaml(self.dump_path)\n        self.assertTrue(os.path.exists(self.dump_path))\n        pe = dummyPipeline.load_yaml(self.dump_path)\n        self.assertEqual(type(pe), dummyPipeline)\n        self.assertEqual(pe0._init_kwargs_dict, pe._init_kwargs_dict)\n\n    def test_dump2(self):\n        dummyPipeline.store_args_kwargs = False\n        pe0 = dummyPipeline(a=23, b=32, c=[\'123\', \'456\'])\n        self.assertEqual(pe0._init_kwargs_dict, {})\n        dummyPipeline.store_args_kwargs = True\n        pe0 = dummyPipeline(a=23, b=32, c=[\'123\', \'456\'])\n        self.assertEqual(pe0._init_kwargs_dict, {\'kwargs\': dict(a=23, b=32, c=[\'123\', \'456\'])})\n        pe0.dump_yaml(self.dump_path)\n        self.assertTrue(os.path.exists(self.dump_path))\n        dummyPipeline.store_args_kwargs = False\n        pe = dummyPipeline.load_yaml(self.dump_path)\n        self.assertEqual(type(pe), dummyPipeline)\n        self.assertEqual(pe._init_kwargs_dict, {})\n        dummyPipeline.store_args_kwargs = True\n\n    def test_load(self):\n        with open(self.dump_path, \'w\') as fp:\n            fp.write(""!PipelineEncoder\\n\\\n                                parameters:\\n\\\n                                  kwargs:\\n\\\n                                    a: 23\\n\\\n                                    b: \'32\'\\n\\\n                                    c: [\'123\', \'456\']"")\n        PipelineEncoder.store_args_kwargs = True\n        pe = dummyPipeline.load_yaml(self.dump_path)\n        self.assertEqual(type(pe), PipelineEncoder)\n        self.assertEqual(pe._init_kwargs_dict, {\'kwargs\': dict(a=23, b=32, c=[\'123\', \'456\'])})\n        PipelineEncoder.store_args_kwargs = False\n        pe = dummyPipeline.load_yaml(self.dump_path)\n        self.assertEqual(type(pe), PipelineEncoder)\n        self.assertEqual(pe._init_kwargs_dict, {})\n\n        with open(self.dump_path, \'w\') as fp:\n            fp.write(""!PipelineEncoder\\n\\\n                                parameters:\\n\\\n                                  args:\\n\\\n                                    - 23\\n\\\n                                    - \'32\'\\n\\\n                                    - [\'123\', \'456\']"")\n        PipelineEncoder.store_args_kwargs = True\n        pe = dummyPipeline.load_yaml(self.dump_path)\n        self.assertEqual(type(pe), PipelineEncoder)\n        self.assertEqual(pe._init_kwargs_dict, {\'args\': (23, 32, [\'123\', \'456\'])})\n\n        PipelineEncoder.store_args_kwargs = False\n        pe = dummyPipeline.load_yaml(self.dump_path)\n        self.assertEqual(type(pe), PipelineEncoder)\n        self.assertEqual(pe._init_kwargs_dict, {})\n\n    def test_nest_pipeline(self):\n        self._test_different_encoder_yamlize(dummyPipeline, a=1, b=2, c=3, wee=4)\n        self._test_different_encoder_yamlize(PQEncoder, 10)\n        self._test_different_encoder_yamlize(TFPQEncoder, 10)\n        self._test_different_encoder_yamlize(PCALocalEncoder, 20, 10)\n\n    def _test_different_encoder_yamlize(self, cls, *args, **kwargs):\n        a = cls(*args, **kwargs)\n        a.dump_yaml(self.dump_path)\n        a.close()\n        self.assertTrue(os.path.exists(self.dump_path))\n        b = cls.load_yaml(self.dump_path)\n        self.assertEqual(type(b), cls)\n        self.assertEqual(a._init_kwargs_dict, b._init_kwargs_dict)\n        b.close()\n\n    @unittest.SkipTest\n    def test_NES_yaml_dump(self):\n        self._test_different_encoder_yamlize(GNES, num_bytes=8,\n                                             pca_output_dim=32,\n                                             cluster_per_byte=8,\n                                             port=1,\n                                             port_out=2,\n                                             data_path=self.db_path,\n                                             ignore_all_checks=True)\n\n    @unittest.SkipTest\n    def test_double_dump(self):\n        a = GNES(num_bytes=8,\n                 pca_output_dim=32,\n                 cluster_per_byte=8,\n                 port=1,\n                 port_out=2,\n                 data_path=self.db_path,\n                 ignore_all_checks=True)\n        a.dump_yaml(self.dump_path)\n        a.close()\n        with open(self.dump_path) as fp:\n            content_a = fp.readlines()\n        b = GNES.load_yaml(self.dump_path)\n        b.dump_yaml(self.dump_path)\n        b.close()\n        with open(self.dump_path) as fp:\n            content_b = fp.readlines()\n        self.assertEqual(content_a, content_b)\n'"
tests/test_yt8m_encoder.py,0,"b""import os\nimport unittest\nimport numpy as np\n\nfrom gnes.encoder.base import BaseEncoder\n\n\nclass TestYT8MEncoder(unittest.TestCase):\n    @unittest.skip\n    def setUp(self):\n        dirname = os.path.dirname(__file__)\n        self.test_video = [np.random.rand(10, 1152).astype(np.uint8),\n                         np.random.rand(15, 1152).astype(np.uint8),\n                         np.random.rand(20, 1152).astype(np.uint8)]\n        self.yt8m_yaml = os.path.join(dirname, 'yaml', 'yt8m_encoder.yml')\n\n    @unittest.skip\n    def test_yt8m_encoding(self):\n        self.encoder = BaseEncoder.load_yaml(self.yt8m_yaml)\n        vec = self.encoder.encode(self.test_video)\n\n        self.assertEqual(vec.shape[0], len(self.test_video))\n        self.assertEqual(vec.shape[1], 19310)"""
tests/test_yt8m_feature_extractor.py,0,"b""import os\nimport unittest\nimport numpy as np\n\nfrom gnes.encoder.base import BaseEncoder\n\n\nclass TestYT8MFeatureExtractor(unittest.TestCase):\n    @unittest.skip\n    def setUp(self):\n        dirname = os.path.dirname(__file__)\n        self.test_img = [np.random.rand(10, 299, 299, 3).astype(np.uint8),\n                         np.random.rand(6, 299, 299, 3).astype(np.uint8),\n                         np.random.rand(8, 299, 299, 3).astype(np.uint8)]\n        self.inception_yaml = os.path.join(dirname, 'yaml', 'yt8m_feature.yml')\n\n    @unittest.skip\n    def test_inception_encoding(self):\n        self.encoder = BaseEncoder.load_yaml(self.inception_yaml)\n        vec = self.encoder.encode(self.test_img)\n\n        self.assertEqual(len(vec), len(self.test_img))\n        self.assertEqual(len(vec[0].shape), 2)\n        self.assertEqual(vec[0].shape[0], self.test_img[0].shape[0])\n        self.assertEqual(vec[0].shape[1], 1152)\n"""
gnes/base/__init__.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nimport inspect\nimport os\nimport pickle\nimport tempfile\nimport uuid\nfrom functools import wraps\nfrom typing import Dict, Any, Union, TextIO, TypeVar, Type, List, Callable\n\nimport ruamel.yaml.constructor\n\nfrom ..helper import set_logger, profiling, yaml, parse_arg, load_contrib_module\n\n__all__ = [\'TrainableBase\', \'CompositionalTrainableBase\']\n\nT = TypeVar(\'T\', bound=\'TrainableBase\')\n\n\ndef register_all_class(cls2file_map: Dict, module_name: str):\n    import importlib\n    for k, v in cls2file_map.items():\n        try:\n            getattr(importlib.import_module(\'gnes.%s.%s\' % (module_name, v)), k)\n        except ImportError as ex:\n            default_logger = set_logger(\'GNES\')\n            default_logger.warning(\'fail to register %s, due to ""%s"", you will not be able to use this model\' % (k, ex))\n    load_contrib_module()\n\n\ndef import_class_by_str(name: str):\n    def _import(module_name, class_name):\n        import importlib\n\n        cls2file = getattr(importlib.import_module(\'gnes.%s\' % module_name), \'_cls2file_map\')\n        if class_name in cls2file:\n            return getattr(importlib.import_module(\'gnes.%s.%s\' % (module_name, cls2file[class_name])), class_name)\n\n    search_modules = [\'encoder\', \'indexer\', \'preprocessor\', \'router\', \'score_fn\']\n\n    for m in search_modules:\n        r = _import(m, name)\n        if r:\n            return r\n    else:\n        raise ImportError(\'Can not locate any class with name: %s, misspelling?\' % name)\n\n\nclass TrainableType(type):\n    default_gnes_config = {\n        \'is_trained\': False,\n        \'batch_size\': None,\n        \'work_dir\': os.environ.get(\'GNES_VOLUME\', os.getcwd()),\n        \'name\': None,\n        \'on_gpu\': False,\n        \'warn_unnamed\': True\n    }\n\n    def __new__(cls, *args, **kwargs):\n        _cls = super().__new__(cls, *args, **kwargs)\n        return cls.register_class(_cls)\n\n    def __call__(cls, *args, **kwargs):\n        # do _preload_package\n        getattr(cls, \'_pre_init\', lambda *x: None)()\n\n        if \'gnes_config\' in kwargs:\n            gnes_config = kwargs.pop(\'gnes_config\')\n        else:\n            gnes_config = {}\n\n        obj = type.__call__(cls, *args, **kwargs)\n\n        # set attribute with priority\n        # gnes_config in YAML > class attribute > default_gnes_config\n        for k, v in TrainableType.default_gnes_config.items():\n            if k in gnes_config:\n                v = gnes_config[k]\n            v = _expand_env_var(v)\n            if not hasattr(obj, k):\n                if k == \'is_trained\' and isinstance(obj, CompositionalTrainableBase):\n                    continue\n                setattr(obj, k, v)\n\n        getattr(obj, \'_post_init_wrapper\', lambda *x: None)()\n        return obj\n\n    @staticmethod\n    def register_class(cls):\n        # print(\'try to register class: %s\' % cls.__name__)\n        reg_cls_set = getattr(cls, \'_registered_class\', set())\n        if cls.__name__ not in reg_cls_set:\n            # print(\'reg class: %s\' % cls.__name__)\n            cls.__init__ = TrainableType._store_init_kwargs(cls.__init__)\n            if os.environ.get(\'GNES_PROFILING\', False):\n                for f_name in [\'train\', \'encode\', \'add\', \'query\', \'index\']:\n                    if getattr(cls, f_name, None):\n                        setattr(cls, f_name, profiling(getattr(cls, f_name)))\n\n            if getattr(cls, \'train\', None):\n                # print(\'registered train func of %s\'%cls)\n                setattr(cls, \'train\', TrainableType._as_train_func(getattr(cls, \'train\')))\n\n            reg_cls_set.add(cls.__name__)\n            setattr(cls, \'_registered_class\', reg_cls_set)\n        yaml.register_class(cls)\n        return cls\n\n    @staticmethod\n    def _as_train_func(func):\n        @wraps(func)\n        def arg_wrapper(self, *args, **kwargs):\n            if self.is_trained:\n                self.logger.warning(\'""%s"" has been trained already, \'\n                                    \'training it again will override the previous training\' % self.__class__.__name__)\n            f = func(self, *args, **kwargs)\n            if not isinstance(self, CompositionalTrainableBase):\n                self.is_trained = True\n            return f\n\n        return arg_wrapper\n\n    @staticmethod\n    def _store_init_kwargs(func):\n        @wraps(func)\n        def arg_wrapper(self, *args, **kwargs):\n            taboo = {\'self\', \'args\', \'kwargs\'}\n            taboo.update(TrainableType.default_gnes_config.keys())\n            all_pars = inspect.signature(func).parameters\n            tmp = {k: v.default for k, v in all_pars.items() if k not in taboo}\n            tmp_list = [k for k in all_pars.keys() if k not in taboo]\n            # set args by aligning tmp_list with arg values\n            for k, v in zip(tmp_list, args):\n                tmp[k] = v\n            # set kwargs\n            for k, v in kwargs.items():\n                if k in tmp:\n                    tmp[k] = v\n\n            if self.store_args_kwargs:\n                if args: tmp[\'args\'] = args\n                if kwargs: tmp[\'kwargs\'] = {k: v for k, v in kwargs.items() if k not in taboo}\n\n            if getattr(self, \'_init_kwargs_dict\', None):\n                self._init_kwargs_dict.update(tmp)\n            else:\n                self._init_kwargs_dict = tmp\n            f = func(self, *args, **kwargs)\n            return f\n\n        return arg_wrapper\n\n\nclass TrainableBase(metaclass=TrainableType):\n    """"""\n    The base class for preprocessor, encoder, indexer and router\n\n    """"""\n    store_args_kwargs = False\n\n    def __init__(self, *args, **kwargs):\n        self.verbose = \'verbose\' in kwargs and kwargs[\'verbose\']\n        self.logger = set_logger(self.__class__.__name__, self.verbose)\n        self._post_init_vars = set()\n\n    def _post_init_wrapper(self):\n        if not getattr(self, \'name\', None) and os.environ.get(\'GNES_WARN_UNNAMED_COMPONENT\', \'1\') == \'1\':\n            _id = str(uuid.uuid4()).split(\'-\')[0]\n            _name = \'%s-%s\' % (self.__class__.__name__, _id)\n            if self.warn_unnamed:\n                self.logger.warning(\n                    \'this object is not named (""name"" is not found under ""gnes_config"" in YAML config), \'\n                    \'i will call it ""%s"". \'\n                    \'naming the object is important as it provides an unique identifier when \'\n                    \'serializing/deserializing this object.\' % _name)\n            setattr(self, \'name\', _name)\n\n        _before = set(list(self.__dict__.keys()))\n        self.post_init()\n        self._post_init_vars = {k for k in self.__dict__ if k not in _before}\n\n    def post_init(self):\n        """"""\n        Declare class attributes/members that can not be serialized in standard way\n\n        """"""\n        pass\n\n    @classmethod\n    def pre_init(cls):\n        pass\n\n    @property\n    def dump_full_path(self):\n        """"""\n        Get the binary dump path\n\n        :return:\n        """"""\n        return os.path.join(self.work_dir, \'%s.bin\' % self.name)\n\n    @property\n    def yaml_full_path(self):\n        """"""\n        Get the file path of the yaml config\n\n        :return:\n        """"""\n        return os.path.join(self.work_dir, \'%s.yml\' % self.name)\n\n    def __getstate__(self):\n        d = dict(self.__dict__)\n        del d[\'logger\']\n        for k in self._post_init_vars:\n            del d[k]\n        return d\n\n    def __setstate__(self, d):\n        self.__dict__.update(d)\n        self.logger = set_logger(self.__class__.__name__, self.verbose)\n        try:\n            self._post_init_wrapper()\n        except ImportError as ex:\n            self.logger.warning(\'ImportError is often caused by a missing component, \'\n                                \'which often can be solved by ""pip install"" relevant package. %s\' % ex, exc_info=True)\n\n    def train(self, *args, **kwargs):\n        """"""\n        Train the model, need to be overrided\n        """"""\n        pass\n\n    @profiling\n    def dump(self, filename: str = None) -> None:\n        """"""\n        Serialize the object to a binary file\n\n        :param filename: file path of the serialized file, if not given then :py:attr:`dump_full_path` is used\n        """"""\n        f = filename or self.dump_full_path\n        if not f:\n            f = tempfile.NamedTemporaryFile(\'w\', delete=False, dir=os.environ.get(\'GNES_VOLUME\', None)).name\n        with open(f, \'wb\') as fp:\n            pickle.dump(self, fp)\n        self.logger.critical(\'model is serialized to %s\' % f)\n\n    @profiling\n    def dump_yaml(self, filename: str = None) -> None:\n        """"""\n        Serialize the object to a yaml file\n\n        :param filename: file path of the yaml file, if not given then :py:attr:`dump_yaml_path` is used\n        """"""\n        f = filename or self.yaml_full_path\n        if not f:\n            f = tempfile.NamedTemporaryFile(\'w\', delete=False, dir=os.environ.get(\'GNES_VOLUME\', None)).name\n        with open(f, \'w\', encoding=\'utf8\') as fp:\n            yaml.dump(self, fp)\n        self.logger.info(\'model\\\'s yaml config is dump to %s\' % f)\n\n    @classmethod\n    def load_yaml(cls: Type[T], filename: Union[str, TextIO]) -> T:\n        if not filename: raise FileNotFoundError\n        if isinstance(filename, str):\n            with open(filename, encoding=\'utf8\') as fp:\n                return yaml.load(fp)\n        else:\n            with filename:\n                return yaml.load(filename)\n\n    @staticmethod\n    @profiling\n    def load(filename: str = None) -> T:\n        if not filename: raise FileNotFoundError\n        with open(filename, \'rb\') as fp:\n            return pickle.load(fp)\n\n    def close(self):\n        """"""\n        Release the resources as model is destroyed\n        """"""\n        pass\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    @staticmethod\n    def _get_tags_from_node(node):\n        def node_recurse_generator(n):\n            if n.tag.startswith(\'!\'):\n                yield n.tag.lstrip(\'!\')\n            for nn in n.value:\n                if isinstance(nn, tuple):\n                    for k in nn:\n                        yield from node_recurse_generator(k)\n                elif isinstance(nn, ruamel.yaml.nodes.Node):\n                    yield from node_recurse_generator(nn)\n\n        return list(set(list(node_recurse_generator(node))))\n\n    @classmethod\n    def to_yaml(cls, representer, data):\n        tmp = data._dump_instance_to_yaml(data)\n        return representer.represent_mapping(\'!\' + cls.__name__, tmp)\n\n    @classmethod\n    def from_yaml(cls, constructor, node, stop_on_import_error=False):\n        return cls._get_instance_from_yaml(constructor, node, stop_on_import_error)[0]\n\n    @classmethod\n    def _get_instance_from_yaml(cls, constructor, node, stop_on_import_error=False):\n        try:\n            for c in cls._get_tags_from_node(node):\n                import_class_by_str(c)\n        except ImportError as ex:\n            if stop_on_import_error:\n                raise RuntimeError(\'Cannot import module, pip install may required\') from ex\n\n        if node.tag in {\'!PipelineEncoder\', \'!CompositionalTrainableBase\'}:\n            os.environ[\'GNES_WARN_UNNAMED_COMPONENT\'] = \'0\'\n\n        data = ruamel.yaml.constructor.SafeConstructor.construct_mapping(\n            constructor, node, deep=True)\n\n        _gnes_config = data.get(\'gnes_config\', {})\n        for k, v in _gnes_config.items():\n            _gnes_config[k] = _expand_env_var(v)\n        if _gnes_config:\n            data[\'gnes_config\'] = _gnes_config\n\n        dump_path = cls._get_dump_path_from_config(data.get(\'gnes_config\', {}))\n        load_from_dump = False\n        if dump_path:\n            obj = cls.load(dump_path)\n            obj.logger.critical(\'restore %s from %s\' % (cls.__name__, dump_path))\n            load_from_dump = True\n        else:\n            cls.init_from_yaml = True\n\n            if cls.store_args_kwargs:\n                p = data.get(\'parameters\', {})  # type: Dict[str, Any]\n                a = p.pop(\'args\') if \'args\' in p else ()\n                k = p.pop(\'kwargs\') if \'kwargs\' in p else {}\n                # maybe there are some hanging kwargs in ""parameters""\n                tmp_a = (_expand_env_var(v) for v in a)\n                tmp_p = {kk: _expand_env_var(vv) for kk, vv in {**k, **p}.items()}\n                obj = cls(*tmp_a, **tmp_p, gnes_config=data.get(\'gnes_config\', {}))\n            else:\n                tmp_p = {kk: _expand_env_var(vv) for kk, vv in data.get(\'parameters\', {}).items()}\n                obj = cls(**tmp_p, gnes_config=data.get(\'gnes_config\', {}))\n\n            obj.logger.critical(\'initialize %s from a yaml config\' % cls.__name__)\n            cls.init_from_yaml = False\n\n        if node.tag in {\'!PipelineEncoder\', \'!CompositionalTrainableBase\'}:\n            os.environ[\'GNES_WARN_UNNAMED_COMPONENT\'] = \'1\'\n\n        return obj, data, load_from_dump\n\n    @staticmethod\n    def _get_dump_path_from_config(gnes_config: Dict):\n        if \'name\' in gnes_config:\n            dump_path = os.path.join(gnes_config.get(\'work_dir\', os.getcwd()), \'%s.bin\' % gnes_config[\'name\'])\n            if os.path.exists(dump_path):\n                return dump_path\n\n    @staticmethod\n    def _dump_instance_to_yaml(data):\n        # note: we only dump non-default property for the sake of clarity\n        p = {k: getattr(data, k) for k, v in TrainableType.default_gnes_config.items() if getattr(data, k) != v}\n        a = {k: v for k, v in data._init_kwargs_dict.items() if k not in TrainableType.default_gnes_config}\n        r = {}\n        if a:\n            r[\'parameters\'] = a\n        if p:\n            r[\'gnes_config\'] = p\n        return r\n\n    def _copy_from(self, x: \'TrainableBase\') -> None:\n        pass\n\n\nclass CompositionalTrainableBase(TrainableBase):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._components = None  # type: List[T]\n\n    @property\n    def is_trained(self):\n        return self.components and all(c.is_trained for c in self.components)\n\n    @property\n    def components(self) -> Union[List[T], Dict[str, T]]:\n        return self._components\n\n    @property\n    def is_pipeline(self):\n        return isinstance(self.components, list)\n\n    @components.setter\n    def components(self, comps: Callable[[], Union[list, dict]]):\n        if not callable(comps):\n            raise TypeError(\'components must be a callable function that returns \'\n                            \'a List[BaseEncoder]\')\n        if not getattr(self, \'init_from_yaml\', False):\n            self._components = comps()\n        else:\n            self.logger.info(\'components is omitted from construction, \'\n                             \'as it is initialized from yaml config\')\n\n    def close(self):\n        super().close()\n        # pipeline\n        if isinstance(self.components, list):\n            for be in self.components:\n                be.close()\n        # no typology\n        elif isinstance(self.components, dict):\n            for be in self.components.values():\n                be.close()\n        elif self.components is None:\n            pass\n        else:\n            raise TypeError(\'components must be dict or list, received %s\' % type(self.components))\n\n    def _copy_from(self, x: T):\n        if isinstance(self.components, list):\n            for be1, be2 in zip(self.components, x.components):\n                be1._copy_from(be2)\n        elif isinstance(self.components, dict):\n            for k, v in self.components.items():\n                v._copy_from(x.components[k])\n        else:\n            raise TypeError(\'components must be dict or list, received %s\' % type(self.components))\n\n    @classmethod\n    def to_yaml(cls, representer, data):\n        tmp = super()._dump_instance_to_yaml(data)\n        tmp[\'components\'] = data.components\n        return representer.represent_mapping(\'!\' + cls.__name__, tmp)\n\n    @classmethod\n    def from_yaml(cls, constructor, node):\n        obj, data, from_dump = super()._get_instance_from_yaml(constructor, node)\n        if not from_dump and \'components\' in data:\n            obj.components = lambda: data[\'components\']\n        return obj\n\n\ndef _expand_env_var(v: str) -> str:\n    if isinstance(v, str):\n        return parse_arg(os.path.expandvars(v))\n    else:\n        return v\n'"
gnes/cli/__init__.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nimport sys\n\nfrom termcolor import colored\n\nfrom . import api\nfrom .parser import get_main_parser\n\n__all__ = [\'main\']\n\n\ndef get_run_args(parser_fn=get_main_parser, printed=True):\n    parser = parser_fn()\n    if len(sys.argv) > 1:\n        args = parser.parse_args()\n        if printed:\n            param_str = \'\\n\'.join([\'%20s = %s\' % (colored(k, \'yellow\'), v) for k, v in sorted(vars(args).items())])\n            print(\'usage: %s\\n%s\\n%s\\n\' % (\' \'.join(sys.argv), \'_\' * 50, param_str))\n        return args\n    else:\n        parser.print_help()\n        exit()\n\n\ndef main():\n    args = get_run_args()\n    getattr(api, args.cli, no_cli_error)(args)\n\n\ndef no_cli_error(*args, **kwargs):\n    get_main_parser().print_help()\n    exit()\n'"
gnes/cli/api.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\ndef _start_service(cls, args):\n    from ..service.base import ServiceManager\n    with ServiceManager(cls, args) as es:\n        es.join()\n\n\ndef grpc(args):\n    from ..service.grpc import GRPCService\n    _start_service(GRPCService, args)\n\n\ndef preprocess(args):\n    from ..service.preprocessor import PreprocessorService\n    _start_service(PreprocessorService, args)\n\n\ndef encode(args):\n    from ..service.encoder import EncoderService\n    _start_service(EncoderService, args)\n\n\ndef index(args):\n    from ..service.indexer import IndexerService\n    _start_service(IndexerService, args)\n\n\ndef route(args):\n    from ..service.router import RouterService\n    _start_service(RouterService, args)\n\n\ndef frontend(args):\n    from ..service.frontend import FrontendService\n    _start_service(FrontendService, args)\n\n\ndef client(args):\n    if args.client == \'http\':\n        return _client_http(args)\n    elif args.client == \'cli\':\n        return _client_cli(args)\n    else:\n        raise ValueError(\'gnes client must follow with a client type from {http, cli, benchmark...}\\n\'\n                         \'see ""gnes client --help"" for details\')\n\n\ndef healthcheck(args):\n    from ..service.base import send_ctrl_message\n    from ..proto import gnes_pb2, add_version\n    import time\n    ctrl_addr = \'tcp://%s:%d\' % (args.host, args.port)\n    msg = gnes_pb2.Message()\n    add_version(msg.envelope)\n    msg.request.control.command = gnes_pb2.Request.ControlRequest.STATUS\n    for j in range(args.retries):\n        r = send_ctrl_message(ctrl_addr, msg, timeout=args.timeout)\n        if not r:\n            print(\'%s is not responding, retry (%d/%d) in 1s\' % (ctrl_addr, j + 1, args.retries))\n        else:\n            print(\'%s returns %s\' % (ctrl_addr, r))\n            exit(0)\n        time.sleep(1)\n    exit(1)\n\n\ndef _client_http(args):\n    from ..client.http import HttpClient\n    HttpClient(args).start()\n\n\ndef _client_cli(args):\n    from ..client.cli import CLIClient\n    CLIClient(args)\n\n\ndef compose(args):\n    from ..composer.base import YamlComposer\n    from ..composer.flask import YamlComposerFlask\n    from ..composer.http import YamlComposerHttp\n\n    if args.flask:\n        YamlComposerFlask(args).run()\n    elif args.serve:\n        YamlComposerHttp(args).run()\n    else:\n        YamlComposer(args).build_all()\n'"
gnes/cli/parser.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nimport argparse\n\n\nclass ActionNoYes(argparse.Action):\n    def __init__(self, option_strings, dest, default=None, required=False, help=None):\n\n        if default is None:\n            raise ValueError(\'you must provide a default with yes/no action\')\n        if len(option_strings) != 1:\n            raise ValueError(\'only single argument is allowed with yes/no action\')\n        opt = option_strings[0]\n        if not opt.startswith(\'--\'):\n            raise ValueError(\'yes/no arguments must be prefixed with --\')\n\n        opt = opt[2:]\n        opts = [\'--\' + opt, \'--no-\' + opt, \'--no_\' + opt]\n        super(ActionNoYes, self).__init__(opts, dest, nargs=0, const=None,\n                                          default=default, required=required, help=help)\n\n    def __call__(self, parser, namespace, values, option_strings=None):\n        if option_strings.startswith(\'--no-\') or option_strings.startswith(\'--no_\'):\n            setattr(namespace, self.dest, False)\n        else:\n            setattr(namespace, self.dest, True)\n\n\ndef resolve_py_path(path):\n    import os\n    if not os.path.exists(path):\n        raise argparse.ArgumentTypeError(\'%s is not a valid file path\' % path)\n    return path\n\n\ndef random_port(port):\n    if not port or int(port) <= 0:\n        import random\n        min_port, max_port = 49152, 65536\n        return random.randrange(min_port, max_port)\n    else:\n        return int(port)\n\n\ndef resolve_yaml_path(path, to_stream=False):\n    # priority, filepath > classname > default\n    import os\n    import io\n    if hasattr(path, \'read\'):\n        # already a readable stream\n        return path\n    elif os.path.exists(path):\n        if to_stream:\n            return open(path, encoding=\'utf8\')\n        else:\n            return path\n    elif path.isidentifier():\n        # possible class name\n        return io.StringIO(\'!%s {}\' % path)\n    elif path.startswith(\'!\'):\n        # possible YAML content\n        return io.StringIO(path)\n    else:\n        raise argparse.ArgumentTypeError(\'%s can not be resolved, it should be a readable stream,\'\n                                         \' or a valid file path, or a supported class name.\' % path)\n\n\ndef set_base_parser():\n    from .. import __version__, __proto_version__\n    from termcolor import colored\n    import os\n    # create the top-level parser\n    parser = argparse.ArgumentParser(\n        description=\'%s, a cloud-native semantic search system \'\n                    \'based on deep neural network. \'\n                    \'It enables large-scale index and semantic search for text-to-text, image-to-image, \'\n                    \'video-to-video and any content form. Visit %s for tutorials and documentations.\' % (\n                        colored(\'GNES v%s: Generic Neural Elastic Search\' % __version__, \'green\'),\n                        colored(\'https://gnes.ai\', \'cyan\', attrs=[\'underline\'])),\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'-v\', \'--version\', action=\'version\',\n                        version=\'%(prog)s\' + \': %s\\nprotobuf: %s\\nvcs_version: %s\' %\n                                (__version__, __proto_version__, os.environ.get(\'GNES_VCS_VERSION\', \'unknown\')))\n    parser.add_argument(\'--verbose\', action=\'store_true\', default=False,\n                        help=\'turn on detailed logging for debug\')\n    return parser\n\n\ndef set_composer_parser(parser=None):\n    from pkg_resources import resource_stream\n\n    if not parser:\n        parser = set_base_parser()\n    parser.add_argument(\'--port\',\n                        type=int,\n                        default=8800,\n                        help=\'host port of the grpc service\')\n    parser.add_argument(\'--name\',\n                        type=str,\n                        default=\'GNES app\',\n                        help=\'name of the instance\')\n    parser.add_argument(\'--yaml_path\', type=lambda x: resolve_yaml_path(x, True),\n                        default=resource_stream(\n                            \'gnes\', \'/\'.join((\'resources\', \'compose\', \'gnes-example.yml\'))),\n                        help=\'yaml config of the service\')\n    parser.add_argument(\'--html_path\', type=argparse.FileType(\'w\', encoding=\'utf8\'),\n                        help=\'output path of the HTML file, will contain all possible generations\')\n    parser.add_argument(\'--shell_path\', type=argparse.FileType(\'w\', encoding=\'utf8\'),\n                        help=\'output path of the shell-based starting script\')\n    parser.add_argument(\'--swarm_path\', type=argparse.FileType(\'w\', encoding=\'utf8\'),\n                        help=\'output path of the docker-compose file for Docker Swarm\')\n    parser.add_argument(\'--k8s_path\', type=argparse.FileType(\'w\', encoding=\'utf8\'),\n                        help=\'output path of the docker-compose file for Docker Swarm\')\n    parser.add_argument(\'--graph_path\', type=argparse.FileType(\'w\', encoding=\'utf8\'),\n                        help=\'output path of the mermaid graph file\')\n    parser.add_argument(\'--shell_log_redirect\', type=str,\n                        help=\'the file path for redirecting shell output. \'\n                             \'when not given, the output will be flushed to stdout\')\n    parser.add_argument(\'--mermaid_leftright\', action=\'store_true\', default=False,\n                        help=\'showing the flow in left-to-right manner rather than top down\')\n    parser.add_argument(\'--docker_img\', type=str,\n                        default=\'gnes/gnes:latest-alpine\',\n                        help=\'the docker image used in Docker Swarm & Kubernetes\')\n    return parser\n\n\ndef set_composer_flask_parser(parser=None):\n    if not parser:\n        parser = set_base_parser()\n    set_composer_parser(parser)\n    group = parser.add_mutually_exclusive_group()\n    group.add_argument(\'--flask\', action=\'store_true\', default=False,\n                       help=\'start a Flask server and serve the composer in interactive mode, aka GNES board\')\n    group.add_argument(\'--serve\', action=\'store_true\', default=False,\n                       help=\'start a basic HTTP server and serve the composer in interactive mode, aka GNES board\')\n    parser.add_argument(\'--http_port\', type=int, default=8080,\n                        help=\'server port for receiving HTTP requests\')\n    return parser\n\n\ndef set_service_parser(parser=None):\n    from ..service.base import SocketType, BaseService, ParallelType\n\n    import os\n    if not parser:\n        parser = set_base_parser()\n\n    parser.add_argument(\'--port_in\', type=int, default=random_port(-1),\n                        help=\'port for input data, default a random port between [49152, 65536]\')\n    parser.add_argument(\'--port_out\', type=int, default=random_port(-1),\n                        help=\'port for output data, default a random port between [49152, 65536]\')\n    parser.add_argument(\'--host_in\', type=str, default=BaseService.default_host,\n                        help=\'host address for input\')\n    parser.add_argument(\'--host_out\', type=str, default=BaseService.default_host,\n                        help=\'host address for output\')\n    parser.add_argument(\'--socket_in\', type=SocketType.from_string, choices=list(SocketType),\n                        default=SocketType.PULL_BIND,\n                        help=\'socket type for input port\')\n    parser.add_argument(\'--socket_out\', type=SocketType.from_string, choices=list(SocketType),\n                        default=SocketType.PUSH_BIND,\n                        help=\'socket type for output port\')\n    parser.add_argument(\'--port_ctrl\', type=int, default=os.environ.get(\'GNES_CONTROL_PORT\', random_port(-1)),\n                        help=\'port for controlling the service, default a random port between [49152, 65536]\')\n    parser.add_argument(\'--timeout\', type=int, default=-1,\n                        help=\'timeout (ms) of all communication, -1 for waiting forever\')\n    parser.add_argument(\'--dump_interval\', type=int, default=5,\n                        help=\'serialize the model in the service every n seconds if model changes. \'\n                             \'-1 means --read_only. \')\n    parser.add_argument(\'--read_only\', action=\'store_true\', default=False,\n                        help=\'do not allow the service to modify the model, \'\n                             \'dump_interval will be ignored\')\n    parser.add_argument(\'--parallel_backend\', type=str, choices=[\'thread\', \'process\'], default=\'thread\',\n                        help=\'parallel backend of the service\')\n    parser.add_argument(\'--num_parallel\', \'--replicas\', type=int, default=1,\n                        help=\'number of parallel services running at the same time (i.e. replicas), \'\n                             \'`port_in` and `port_out` will be set to random, \'\n                             \'and routers will be added automatically when necessary\')\n    parser.add_argument(\'--parallel_type\', \'--replica_type\', type=ParallelType.from_string, choices=list(ParallelType),\n                        default=ParallelType.PUSH_NONBLOCK,\n                        help=\'parallel type of the concurrent services\')\n    parser.add_argument(\'--check_version\', action=ActionNoYes, default=True,\n                        help=\'comparing the GNES and proto version of incoming message with local setup, \'\n                             \'mismatch raise an exception\')\n    parser.add_argument(\'--identity\', type=str, default=\'\',\n                        help=\'identity of the service, empty by default\')\n    parser.add_argument(\'--route_table\', action=ActionNoYes, default=False,\n                        help=\'showing a route table with time cost after receiving the result\')\n    parser.add_argument(\'--squeeze_pb\', action=ActionNoYes, default=True,\n                        help=\'sending bytes and ndarray separately apart from the protobuf message, \'\n                             \'usually yields better network efficiency\')\n    parser.add_argument(\'--ctrl_with_ipc\', action=\'store_true\', default=False,\n                        help=\'use ipc protocol for control socket\')\n    return parser\n\n\ndef _set_client_parser(parser=None):\n    from ..service.base import SocketType\n    if not parser:\n        parser = set_base_parser()\n    set_service_parser(parser)\n    parser.set_defaults(\n        port_in=parser.get_default(\'port_out\'),\n        port_out=parser.get_default(\'port_in\'),\n        socket_in=SocketType.PULL_CONNECT,\n        socket_out=SocketType.PUSH_CONNECT,\n        read_only=True)\n    return parser\n\n\ndef _set_loadable_service_parser(parser=None):\n    if not parser:\n        parser = set_base_parser()\n    from ..service.base import SocketType\n    set_service_parser(parser)\n\n    parser.add_argument(\'--yaml_path\', type=resolve_yaml_path, required=True,\n                        help=\'yaml config of the service, it should be a readable stream,\'\n                             \' or a valid file path, or a supported class name.\')\n    parser.add_argument(\'--py_path\', type=resolve_py_path, nargs=\'+\',\n                        help=\'the file path(s) of an external python module(s).\')\n\n    parser.set_defaults(socket_in=SocketType.PULL_BIND,\n                        socket_out=SocketType.PUSH_BIND)\n    return parser\n\n\ndef _set_sortable_service_parser(parser=None):\n    if not parser:\n        parser = set_base_parser()\n    _set_loadable_service_parser(parser)\n\n    parser.add_argument(\'--sorted_response\', action=\'store_true\', default=False,\n                        help=\'sort the response (if exist) by the score\')\n    return parser\n\n\n# shortcut to keep consistent\nset_encoder_parser = _set_loadable_service_parser\n\n\ndef set_preprocessor_parser(parser=None):\n    if not parser:\n        parser = set_base_parser()\n    _set_loadable_service_parser(parser)\n    parser.set_defaults(read_only=True)\n    return parser\n\n\ndef set_healthcheck_parser(parser=None):\n    if not parser:\n        parser = set_base_parser()\n\n    parser.add_argument(\'--host\', type=str, default=\'127.0.0.1\',\n                        help=\'host address of the checked service\')\n    parser.add_argument(\'--port\', type=int, required=True,\n                        help=\'control port of the checked service\')\n    parser.add_argument(\'--timeout\', type=int, default=1000,\n                        help=\'timeout (ms) of one check, -1 for waiting forever\')\n    parser.add_argument(\'--retries\', type=int, default=3,\n                        help=\'max number of tried health checks before exit 1\')\n    return parser\n\n\ndef set_router_parser(parser=None):\n    if not parser:\n        parser = set_base_parser()\n    _set_sortable_service_parser(parser)\n\n    parser.add_argument(\'--num_part\', type=int, default=None,\n                        help=\'explicitly set the number of parts of message\')\n    parser.set_defaults(read_only=True)\n    return parser\n\n\ndef set_indexer_parser(parser=None):\n    if not parser:\n        parser = set_base_parser()\n    _set_sortable_service_parser(parser)\n    parser.add_argument(\'--as_response\', type=ActionNoYes, default=True,\n                        help=\'convert the message type from request to response after indexing. \'\n                             \'turn it off if you want to chain other services after this index service.\')\n\n    return parser\n\n\ndef _set_grpc_parser(parser=None):\n    if not parser:\n        parser = set_base_parser()\n    parser.add_argument(\'--grpc_host\',\n                        type=str,\n                        default=\'0.0.0.0\',\n                        help=\'host address of the grpc service\')\n    parser.add_argument(\'--grpc_port\',\n                        type=int,\n                        default=8800,\n                        help=\'host port of the grpc service\')\n    parser.add_argument(\'--max_message_size\', type=int, default=-1,\n                        help=\'maximum send and receive size for grpc server in bytes, -1 means unlimited\')\n    parser.add_argument(\'--proxy\', action=ActionNoYes, default=False,\n                        help=\'respect the http_proxy and https_proxy environment variables. \'\n                             \'otherwise, it will unset these proxy variables before start. \'\n                             \'gRPC seems perfer --no_proxy\')\n    return parser\n\n\ndef set_grpc_service_parser(parser=None):\n    if not parser:\n        parser = set_base_parser()\n    set_service_parser(parser)\n    _set_grpc_parser(parser)\n    parser.add_argument(\'--pb2_path\',\n                        type=str,\n                        required=True,\n                        help=\'the path of the python file protocol buffer compiler\')\n    parser.add_argument(\'--pb2_grpc_path\',\n                        type=str,\n                        required=True,\n                        help=\'the path of the python file generated by the gRPC Python protocol compiler plugin\')\n    parser.add_argument(\'--stub_name\',\n                        type=str,\n                        required=True,\n                        help=\'the name of the gRPC Stub\')\n    parser.add_argument(\'--api_name\',\n                        type=str,\n                        required=True,\n                        help=\'the api name for calling the stub\')\n    return parser\n\n\ndef set_frontend_parser(parser=None):\n    from ..service.base import SocketType\n    if not parser:\n        parser = set_base_parser()\n    set_service_parser(parser)\n    _set_grpc_parser(parser)\n    parser.set_defaults(socket_in=SocketType.PULL_BIND,\n                        socket_out=SocketType.PUSH_BIND,\n                        read_only=True)\n    parser.add_argument(\'--max_concurrency\', type=int, default=10,\n                        help=\'maximum concurrent connections allowed\')\n    parser.add_argument(\'--dump_route\', type=argparse.FileType(\'w\', encoding=\'utf8\'),\n                        help=\'dumping route information to a file\')\n    parser.add_argument(\'--max_pending_request\', type=int, default=100,\n                        help=\'maximum number of pending requests allowed, when exceed wait until we receive the response\')\n    return parser\n\n\ndef set_client_cli_parser(parser=None):\n    import sys\n    if not parser:\n        parser = set_base_parser()\n    _set_grpc_parser(parser)\n\n    group = parser.add_mutually_exclusive_group()\n\n    group.add_argument(\'--txt_file\', type=argparse.FileType(\'r\'),\n                       default=sys.stdin,\n                       help=\'text file to be used, each line is a doc/query\')\n    group.add_argument(\'--image_zip_file\', type=str,\n                       help=\'image zip file to be used, consists of multiple images\')\n    group.add_argument(\'--video_zip_file\', type=str,\n                       help=\'video zip file to be used, consists of multiple videos\')\n\n    parser.add_argument(\'--batch_size\', type=int, default=100,\n                        help=\'the size of the request to split\')\n    parser.add_argument(\'--mode\', choices=[\'index\', \'query\', \'train\'], type=str,\n                        required=True,\n                        help=\'the mode of the client and the server\')\n    parser.add_argument(\'--top_k\', type=int,\n                        default=10,\n                        help=\'top_k results returned in the query mode\')\n    parser.add_argument(\'--start_doc_id\', type=int,\n                        default=0,\n                        help=\'the start number of doc id\')\n    parser.add_argument(\'--max_concurrency\', type=int, default=10,\n                        help=\'maximum concurrent connections allowed\')\n    return parser\n\n\ndef set_client_http_parser(parser=None):\n    if not parser:\n        parser = set_base_parser()\n    _set_grpc_parser(parser)\n    parser.add_argument(\'--http_port\', type=int, default=80,\n                        help=\'http port to deploy the service\')\n    parser.add_argument(\'--http_host\', type=str, default=\'0.0.0.0\',\n                        help=\'http host to deploy the service\')\n    parser.add_argument(\'--max_workers\', type=int, default=100,\n                        help=\'max workers to deal with the message\')\n    parser.add_argument(\'--top_k\', type=int, default=10,\n                        help=\'default top_k for query mode\')\n    parser.add_argument(\'--batch_size\', type=int, default=2560,\n                        help=\'batch size for feed data for train mode\')\n    return parser\n\n\ndef get_main_parser():\n    # create the top-level parser\n    parser = set_base_parser()\n    adf = argparse.ArgumentDefaultsHelpFormatter\n    sp = parser.add_subparsers(dest=\'cli\', title=\'GNES sub-commands\',\n                               description=\'use ""gnes [sub-command] --help"" \'\n                                           \'to get detailed information about each sub-command\')\n\n    # microservices\n    set_frontend_parser(sp.add_parser(\'frontend\', help=\'start a frontend service\', formatter_class=adf))\n    set_encoder_parser(sp.add_parser(\'encode\', help=\'start an encoder service\', formatter_class=adf))\n    set_indexer_parser(sp.add_parser(\'index\', help=\'start an indexer service\', formatter_class=adf))\n    set_router_parser(sp.add_parser(\'route\', help=\'start a router service\', formatter_class=adf))\n    set_preprocessor_parser(sp.add_parser(\'preprocess\', help=\'start a preprocessor service\', formatter_class=adf))\n    set_grpc_service_parser(sp.add_parser(\'grpc\', help=\'start a general purpose grpc service\', formatter_class=adf))\n\n    pp = sp.add_parser(\'client\', help=\'start a GNES client of the selected type\')\n    spp = pp.add_subparsers(dest=\'client\', title=\'GNES client sub-commands\',\n                            description=\'use ""gnes client [sub-command] --help"" \'\n                                        \'to get detailed information about each client sub-command\')\n    spp.required = True\n    # clients\n    set_client_http_parser(\n        spp.add_parser(\'http\', help=\'start a client that allows HTTP requests as input\', formatter_class=adf))\n    set_client_cli_parser(spp.add_parser(\'cli\', help=\'start a client that allows stdin as input\', formatter_class=adf))\n\n    # others\n    set_composer_flask_parser(\n        sp.add_parser(\'compose\', help=\'start a GNES Board to visualize YAML configs\', formatter_class=adf))\n    set_healthcheck_parser(\n        sp.add_parser(\'healthcheck\', help=\'do health check on any GNES microservice\', formatter_class=adf))\n    return parser\n'"
gnes/client/__init__.py,0,b''
gnes/client/base.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport os\nfrom typing import Tuple, List, Union\n\nimport grpc\nimport zmq\nfrom termcolor import colored\n\nfrom ..helper import set_logger\nfrom ..proto import gnes_pb2_grpc\nfrom ..proto import send_message as _send_message, gnes_pb2, recv_message as _recv_message\nfrom ..service.base import build_socket\n\n\nclass ResponseHandler:\n\n    def __init__(self, h: \'ResponseHandler\' = None):\n        self.routes = {k: v for k, v in h.routes.items()} if h else {}\n        self.logger = set_logger(self.__class__.__name__)\n        self._context = None\n\n    def register(self, resp_type: Union[List, Tuple, type]):\n\n        def decorator(f):\n            if isinstance(resp_type, list) or isinstance(resp_type, tuple):\n                for t in resp_type:\n                    self.routes[t] = f\n            else:\n                self.routes[resp_type] = f\n            return f\n\n        return decorator\n\n    def call_routes(self, resp: \'gnes_pb2.Response\'):\n\n        def get_default_fn(r_type):\n            self.logger.warning(\n                \'cant find handler for response type: %s, fall back to the default handler\'\n                % r_type)\n            f = self.routes.get(r_type, self.routes[NotImplementedError])\n            return f\n\n        self.logger.info(\n            \'received a response for request %d\' % resp.request_id)\n        if resp.WhichOneof(\'body\'):\n            body = getattr(resp, resp.WhichOneof(\'body\'))\n            resp_type = type(body)\n\n            if resp_type in self.routes:\n                fn = self.routes.get(resp_type)\n            else:\n                fn = get_default_fn(type(resp))\n            self.logger.info(\'handling response with %s\' % fn.__name__)\n            return fn(self._context, resp)\n        else:\n            self.logger.warning(\'the received message is not response\')\n            return None\n\n\nclass ZmqClient:\n\n    def __init__(self, args):\n        self.args = args\n        self.logger = set_logger(self.__class__.__name__, self.args.verbose)\n        self.ctx = zmq.Context()\n        self.ctx.setsockopt(zmq.LINGER, 0)\n        self.logger.info(\'current libzmq version is %s,  pyzmq version is %s\' % (zmq.zmq_version(), zmq.__version__))\n\n        self.receiver, recv_addr = build_socket(\n            self.ctx, self.args.host_in, self.args.port_in,\n            self.args.socket_in, self.args.identity)\n        self.sender, send_addr = build_socket(self.ctx, self.args.host_out,\n                                              self.args.port_out,\n                                              self.args.socket_out,\n                                              self.args.identity)\n        self.logger.info(\n            \'input %s:%s\\t output %s:%s\' %\n            (self.args.host_in, colored(self.args.port_in, \'yellow\'),\n             self.args.host_out, colored(self.args.port_out, \'yellow\')))\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def close(self):\n        self.sender.close()\n        self.receiver.close()\n        self.ctx.term()\n\n    def send_message(self, message: ""gnes_pb2.Message"", **kwargs):\n        self.logger.debug(\'send message: %s\' % message.envelope)\n        _send_message(self.sender, message, **kwargs)\n\n    def recv_message(self, **kwargs) -> gnes_pb2.Message:\n        r = _recv_message(self.receiver, **kwargs)\n        self.logger.debug(\'recv a message: %s\' % r.envelope)\n        return r\n\n\nclass GrpcClient:\n    """"""\n    A Base Unary gRPC client which the other client application can build from.\n\n    """"""\n\n    handler = ResponseHandler()\n\n    def __init__(self, args):\n        self.args = args\n        if not args.proxy:\n            os.unsetenv(\'http_proxy\')\n            os.unsetenv(\'https_proxy\')\n        self.logger = set_logger(self.__class__.__name__, self.args.verbose)\n        self.logger.info(\'setting up grpc insecure channel...\')\n        # A gRPC channel provides a connection to a remote gRPC server.\n        self._channel = grpc.insecure_channel(\n            \'%s:%d\' % (self.args.grpc_host, self.args.grpc_port),\n            options={\n                \'grpc.max_send_message_length\': -1,\n                \'grpc.max_receive_message_length\': -1,\n            }.items(),\n        )\n        self.logger.info(\'waiting channel to be ready...\')\n        grpc.channel_ready_future(self._channel).result()\n\n        # create new stub\n        self.logger.info(\'create new stub...\')\n        self._stub = gnes_pb2_grpc.GnesRPCStub(self._channel)\n\n        # attache response handler\n        self.handler._context = self\n        self.logger.critical(\'gnes client ready at %s:%d!\' % (self.args.grpc_host, self.args.grpc_port))\n\n    def call(self, request):\n        resp = self._stub.call(request)\n        self.handler.call_routes(resp)\n        return resp\n\n    def stream_call(self, request_iterator):\n        response_stream = self._stub.StreamCall(request_iterator)\n        for resp in response_stream:\n            self.handler.call_routes(resp)\n\n    @handler.register(NotImplementedError)\n    def _handler_default(self, msg: \'gnes_pb2.Response\'):\n        raise NotImplementedError\n\n    @handler.register(gnes_pb2.Response)\n    def _handler_response_default(self, msg: \'gnes_pb2.Response\'):\n        pass\n\n    def __enter__(self):\n        self.start()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n    def start(self):\n        pass\n\n    def close(self):\n        self._channel.close()\n        self._stub = None\n'"
gnes/client/cli.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nimport sys\nimport time\nimport zipfile\nfrom typing import Iterator, Tuple\n\nfrom termcolor import colored\n\nfrom .base import GrpcClient\nfrom ..proto import RequestGenerator\n\n\nclass CLIClient(GrpcClient):\n    def __init__(self, args, start_at_init: bool = True):\n        super().__init__(args)\n        self._bytes_generator = self._get_bytes_generator_from_args(args)\n        if start_at_init:\n            self.start()\n\n    @staticmethod\n    def _get_bytes_generator_from_args(args):\n        if args.txt_file:\n            all_bytes = (v.encode() for v in args.txt_file)\n        elif args.image_zip_file:\n            zipfile_ = zipfile.ZipFile(args.image_zip_file)\n            all_bytes = (zipfile_.open(v).read() for v in zipfile_.namelist())\n        elif args.video_zip_file:\n            zipfile_ = zipfile.ZipFile(args.video_zip_file)\n            all_bytes = (zipfile_.open(v).read() for v in zipfile_.namelist())\n        else:\n            all_bytes = None\n        return all_bytes\n\n    def start(self):\n        try:\n            getattr(self, self.args.mode)()\n        except Exception as ex:\n            self.logger.error(ex)\n        finally:\n            self.close()\n\n    def train(self) -> None:\n        with ProgressBar(task_name=self.args.mode) as p_bar:\n            for _ in self._stub.StreamCall(RequestGenerator.train(self.bytes_generator,\n                                                                  doc_id_start=self.args.start_doc_id,\n                                                                  batch_size=self.args.batch_size)):\n                p_bar.update()\n\n    def index(self) -> None:\n        with ProgressBar(task_name=self.args.mode) as p_bar:\n            for _ in self._stub.StreamCall(RequestGenerator.index(self.bytes_generator,\n                                                                  doc_id_start=self.args.start_doc_id,\n                                                                  batch_size=self.args.batch_size)):\n                p_bar.update()\n\n    def query(self) -> Iterator[Tuple]:\n        for idx, q in enumerate(self.bytes_generator):\n            for req in RequestGenerator.query(q, request_id_start=idx, top_k=self.args.top_k):\n                resp = self._stub.Call(req)\n                yield (req, resp)\n\n    @property\n    def bytes_generator(self) -> Iterator[bytes]:\n        if self._bytes_generator:\n            return self._bytes_generator\n        else:\n            raise ValueError(\'bytes_generator is empty or not set\')\n\n    @bytes_generator.setter\n    def bytes_generator(self, bytes_gen: Iterator[bytes]):\n        if self._bytes_generator:\n            self.logger.warning(\'bytes_generator is not empty, overrided\')\n        self._bytes_generator = bytes_gen\n\n\nclass ProgressBar:\n    def __init__(self, bar_len: int = 20, task_name: str = \'\'):\n        self.bar_len = bar_len\n        self.task_name = task_name\n\n    def update(self):\n        self.num_bars += 1\n        sys.stdout.write(\'\\r\')\n        elapsed = time.perf_counter() - self.start_time\n        elapsed_str = colored(\'elapsed\', \'yellow\')\n        speed_str = colored(\'speed\', \'yellow\')\n        num_bars = self.num_bars % self.bar_len\n        num_bars = self.bar_len if not num_bars and self.num_bars else max(num_bars, 1)\n\n        sys.stdout.write(\n            \'{:>10} [{:<{}}]  {:>8}: {:3.1f}s   {:>8}: {:3.1f} batch/s\'.format(\n                colored(self.task_name, \'cyan\'),\n                colored(\'=\' * num_bars, \'green\'),\n                self.bar_len + 9,\n                elapsed_str,\n                elapsed,\n                speed_str,\n                self.num_bars / elapsed,\n            ))\n        if num_bars == self.bar_len:\n            sys.stdout.write(\'\\n\')\n        sys.stdout.flush()\n\n    def __enter__(self):\n        self.start_time = time.perf_counter()\n        self.num_bars = -1\n        self.update()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        sys.stdout.write(\'\\t%s\\n\' % colored(\'done!\', \'green\'))\n'"
gnes/client/http.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nimport asyncio\nimport json\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport grpc\nfrom google.protobuf.json_format import MessageToJson\n\nfrom ..helper import set_logger\nfrom ..proto import gnes_pb2_grpc, RequestGenerator\n\n\nclass HttpClient:\n    def __init__(self, args=None):\n        self.args = args\n        self.logger = set_logger(self.__class__.__name__, self.args.verbose)\n\n    def start(self):\n        try:\n            from aiohttp import web\n        except ImportError:\n            self.logger.error(\'can not import aiohttp, it is not installed correctly. please do \'\n                              \'""pip install gnes[aiohttp]""\')\n            return\n        loop = asyncio.get_event_loop()\n        executor = ThreadPoolExecutor(max_workers=self.args.max_workers)\n\n        async def general_handler(request, parser, *args, **kwargs):\n            try:\n                data = dict()\n                # # Option 1: uploading streaming chunk data\n                # data = b""""\n                # async for chunk in request.content.iter_any():\n                #     data += chunk\n                # self.logger.info(""received %d content"" % len(data))\n\n                # Option 2: uploading via Multipart-Encoded File\n                post_data = await request.post()\n                if \'query\' in post_data.keys():\n                    _file = post_data.get(\'query\')\n                    self.logger.info(\'query request from input file: %s\' % _file.filename)\n                    data[\'query\'] = _file.file.read()\n                elif \'docs\' in post_data.keys():\n                    files = post_data.getall(\'docs\')\n                    self.logger.info(\'index request from input files: %d files\' % len(files))\n                    data[\'docs\'] = [_file.file.read() for _file in files]\n\n                self.logger.info(\'data received\')\n                resp = await loop.run_in_executor(\n                    executor,\n                    stub_call,\n                    parser([d for d in data.get(\'docs\')] if hasattr(data, \'docs\')\n                           else data.get(\'query\'), *args, **kwargs))\n                self.logger.info(\'send back to user\')\n                return web.Response(body=json.dumps({\'result\': resp, \'meta\': None}, ensure_ascii=False),\n                                    status=200,\n                                    content_type=\'application/json\')\n            except Exception as ex:\n                return web.Response(body=json.dumps({\'message\': str(ex), \'type\': type(ex)}),\n                                    status=400,\n                                    content_type=\'application/json\')\n\n        async def init(loop):\n            # persistant connection or non-persistant connection\n            handler_args = {\'tcp_keepalive\': False, \'keepalive_timeout\': 25}\n            app = web.Application(loop=loop,\n                                  client_max_size=10 ** 10,\n                                  handler_args=handler_args)\n            app.router.add_route(\'post\', \'/train\',\n                                 lambda x: general_handler(x, RequestGenerator.train, batch_size=self.args.batch_size))\n            app.router.add_route(\'post\', \'/index\',\n                                 lambda x: general_handler(x, RequestGenerator.index, batch_size=self.args.batch_size))\n            app.router.add_route(\'post\', \'/query\',\n                                 lambda x: general_handler(x, RequestGenerator.query, top_k=self.args.top_k))\n            srv = await loop.create_server(app.make_handler(),\n                                           self.args.http_host,\n                                           self.args.http_port)\n            self.logger.info(\'http server listens at %s:%d\' % (self.args.http_host, self.args.http_port))\n            return srv\n\n        def stub_call(req):\n            res_f = list(stub.StreamCall(req))[-1]\n            return json.loads(MessageToJson(res_f))\n\n        with grpc.insecure_channel(\n                \'%s:%s\' % (self.args.grpc_host, self.args.grpc_port),\n                options=[(\'grpc.max_send_message_length\', self.args.max_message_size),\n                         (\'grpc.max_receive_message_length\', self.args.max_message_size),\n                         (\'grpc.keepalive_timeout_ms\', 100 * 1000)]) as channel:\n            stub = gnes_pb2_grpc.GnesRPCStub(channel)\n            loop.run_until_complete(init(loop))\n            loop.run_forever()\n'"
gnes/client/stream.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport time\nimport threading\nimport queue\nfrom concurrent import futures\n\nfrom .base import GrpcClient, ResponseHandler\n\n\nclass SyncClient(GrpcClient):\n    handler = ResponseHandler(GrpcClient.handler)\n\n    def __init__(self, args):\n        super().__init__(args)\n        self._pool = futures.ThreadPoolExecutor(\n            max_workers=self.args.max_concurrency)\n\n    def send_request(self, request):\n        # Send requests in seperate threads to support multiple outstanding rpcs\n        self._pool.submit(self.call, request)\n\n    def close(self):\n        self._pool.shutdown(wait=True)\n        super().close()\n\n\nclass StreamingClient(GrpcClient):\n    handler = ResponseHandler(GrpcClient.handler)\n\n    def __init__(self, args):\n        super().__init__(args)\n\n        self._request_queue = queue.Queue(maxsize=10)\n        self._is_streaming = threading.Event()\n\n        self._dispatch_thread = threading.Thread(target=self._start)\n        self._dispatch_thread.setDaemon(True)\n\n    def send_request(self, request):\n        self._request_queue.put(request, block=True)\n\n        # create a new streaming call\n        if not self._is_streaming.is_set():\n            self._dispatch_thread.start()\n\n    def _start(self):\n        self._is_streaming.set()\n        self.stream_call(self._request_generator())\n        self._is_streaming.clear()\n\n    def _request_generator(self):\n        while True:\n            try:\n                request = self._request_queue.get(block=True, timeout=5.0)\n                if request is None:\n                    break\n                yield request\n            except queue.Empty:\n                break\n            except Exception as e:\n                self.logger.error(\'exception: %s\' % str(e))\n                break\n\n    @handler.register(NotImplementedError)\n    def _handler_default(self, resp: \'gnes_pb2.Response\'):\n        raise NotImplementedError\n\n    def close(self):\n        self._is_streaming.clear()\n        self._dispatch_thread.join()\n        super().close()\n'"
gnes/composer/__init__.py,0,b'# COMPOSER WILL BE RETIRED IN THE FUTURE VERSION!!!\n# COMPOSER WILL BE RETIRED IN THE FUTURE VERSION!!!\n# COMPOSER WILL BE RETIRED IN THE FUTURE VERSION!!!'
gnes/composer/base.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport copy\nimport random\nimport time\nfrom collections import defaultdict\nfrom typing import Dict, List\n\nfrom pkg_resources import resource_stream\nfrom ruamel.yaml import YAML, StringIO\nfrom ruamel.yaml.comments import CommentedMap\n\nfrom .. import __version__\nfrom ..cli.parser import set_frontend_parser, \\\n    set_router_parser, _set_loadable_service_parser, set_preprocessor_parser, \\\n    set_indexer_parser\nfrom ..helper import set_logger\nfrom ..service.base import SocketType\n\n_yaml = YAML()\n\n\nclass YamlComposer:\n    comp2file = {\n        \'Encoder\': \'encode\',\n        \'Router\': \'route\',\n        \'Indexer\': \'index\',\n        \'Frontend\': \'frontend\',\n        \'Preprocessor\': \'preprocess\'\n    }\n\n    comp2args = {\n        \'Encoder\': _set_loadable_service_parser().parse_args([\'--yaml_path\', \'BaseEncoder\']),\n        \'Router\': set_router_parser().parse_args([\'--yaml_path\', \'BaseRouter\']),\n        \'Indexer\': set_indexer_parser().parse_args([\'--yaml_path\', \'BaseIndexer\']),\n        \'Frontend\': set_frontend_parser().parse_args([]),\n        \'Preprocessor\': set_preprocessor_parser().parse_args([\'--yaml_path\', \'BasePreprocessor\'])\n    }\n\n    class Layer:\n        default_values = {\n            \'name\': None,\n            \'yaml_path\': None,\n            \'py_path\': None,\n            \'image\': None,\n            \'replicas\': 1,\n            \'income\': \'pull\',\n        }\n\n        def __init__(self, layer_id: int = 0):\n            self.layer_id = layer_id\n            self.components = []\n\n        @staticmethod\n        def get_value(comp: Dict, key: str):\n            return comp.get(key, YamlComposer.Layer.default_values[key])\n\n        @property\n        def is_homogenous(self):\n            return len(self.components) == 1\n\n        @property\n        def is_single_component(self):\n            return self.is_homogenous and self.get_value(self.components[0], \'replicas\') == 1\n\n        @property\n        def is_homo_multi_component(self):\n            return self.is_homogenous and not self.is_single_component\n\n        @property\n        def is_heto_single_component(self):\n            return not self.is_homogenous and all(self.get_value(c, \'replicas\') == 1 for c in self.components)\n\n        @property\n        def get_component_name(self, unique: bool = False):\n            r = [c[\'name\'] for c in self.components]\n            if unique:\n                r = list(set(r))\n            return r\n\n        def append(self, comp):\n            self.components.append(comp)\n\n        def __repr__(self):\n            return str(self.components)\n\n    def __init__(self, args):\n        self.args = args\n        self.logger = set_logger(self.__class__.__name__, self.args.verbose)\n\n        with args.yaml_path:\n            tmp = _yaml.load(args.yaml_path)\n            stream = StringIO()\n            _yaml.dump(tmp, stream)\n            self.original_yaml = stream.getvalue().strip()\n\n        self._name = tmp.get(\'name\', args.name)\n        self._port = tmp.get(\'port\', args.port)\n        self._networks = tmp.get(\'networks\', {})\n        self._volumes = tmp.get(\'volumes\', {})\n\n        self._layers = []  # type: List[\'YamlComposer.Layer\']\n        self._num_layer = 0\n\n        if \'services\' in tmp:\n            self.add_layer()\n            c = CommentedMap({\'name\': \'Frontend\', \'grpc_port\': self._port})\n            if self.check_fields(c):\n                self.add_comp(c)\n            for comp in tmp[\'services\']:\n                self.add_layer()\n                if isinstance(comp, list):\n                    for c in comp:\n                        if self.check_fields(c):\n                            self.add_comp(c)\n                elif self.check_fields(comp):\n                    self.add_comp(comp)\n                else:\n                    raise ValueError(comp)\n        else:\n            self.logger.error(\'yaml file defines an empty graph! no ""services"" field exists!\')\n\n    def check_fields(self, comp: Dict) -> bool:\n        if \'name\' not in comp:\n            raise AttributeError(\'a component must have a name (choices: %s)\' % \', \'.join(self.comp2file.keys()))\n        if comp[\'name\'] not in self.comp2file:\n            raise AttributeError(\n                \'a component must be one of: %s, but given %s\' % (\', \'.join(self.comp2file.keys()), comp[\'name\']))\n        for k in comp:\n            if k not in self.Layer.default_values:\n                self.logger.warning(\'your yaml contains an unrecognized key named ""%s""\' % k)\n        for k, v in self.Layer.default_values.items():\n            if k not in comp:\n                comp[k] = v\n        return True\n\n    def add_layer(self, layer: \'Layer\' = None) -> None:\n        self._layers.append(copy.deepcopy(layer) or self.Layer(layer_id=self._num_layer))\n        self._num_layer += 1\n\n    def add_comp(self, comp: Dict) -> None:\n        self._layers[-1].append(comp)\n\n    def build_layers(self) -> List[\'YamlComposer.Layer\']:\n        all_layers = []  # type: List[\'YamlComposer.Layer\']\n        for idx, layer in enumerate(self._layers[1:] + [self._layers[0]], 1):\n            last_layer = self._layers[idx - 1]\n            for l in self._add_router(last_layer, layer):\n                all_layers.append(copy.deepcopy(l))\n        all_layers[0] = copy.deepcopy(self._layers[0])\n\n        # Frontend should always on the bind role\n        assert all_layers[0].is_single_component\n        assert all_layers[0].components[0][\'name\'] == \'Frontend\'\n\n        if all_layers[0].components[0][\'socket_in\'] == str(SocketType.SUB_CONNECT):\n            # change to sub bind\n            all_layers[0].components[0][\'socket_in\'] = str(SocketType.SUB_BIND)\n            for c in all_layers[-1].components:\n                c[\'socket_out\'] = str(SocketType.PUB_CONNECT)\n\n        if all_layers[0].components[0][\'socket_in\'] == str(SocketType.PULL_CONNECT):\n            # change to sub bind\n            all_layers[0].components[0][\'socket_in\'] = str(SocketType.PULL_BIND)\n            for c in all_layers[-1].components:\n                c[\'socket_out\'] = str(SocketType.PUSH_CONNECT)\n\n        return all_layers\n\n    @staticmethod\n    def build_dockerswarm(all_layers: List[\'YamlComposer.Layer\'], docker_img: str = \'gnes/gnes:latest-alpine\',\n                          volumes: Dict = None, networks: Dict = None) -> str:\n        with resource_stream(\'gnes\', \'/\'.join((\'resources\', \'compose\', \'gnes-swarm.yml\'))) as r:\n            swarm_lines = _yaml.load(r)\n        config_dict = {}\n        for l_idx, layer in enumerate(all_layers):\n            for c_idx, c in enumerate(layer.components):\n                c_name = \'%s%d%d\' % (c[\'name\'], l_idx, c_idx)\n                args = [\'--%s %s\' % (a, str(v) if \' \' not in str(v) else (\'""%s""\' % str(v))) for a, v in c.items() if\n                        a in YamlComposer.comp2args[c[\'name\']] and a != \'yaml_path\' and v]\n                if \'yaml_path\' in c and c[\'yaml_path\'] is not None:\n                    if c[\'yaml_path\'].endswith(\'.yml\') or c[\'yaml_path\'].endswith(\'.yaml\'):\n                        args.append(\'--yaml_path /%s_yaml\' % c_name)\n                        config_dict[\'%s_yaml\' % c_name] = {\'file\': c[\'yaml_path\']}\n                    else:\n                        args.append(\'--yaml_path %s\' % c[\'yaml_path\'])\n\n                if l_idx + 1 < len(all_layers):\n                    next_layer = all_layers[l_idx + 1]\n                    _l_idx = l_idx + 1\n                else:\n                    next_layer = all_layers[0]\n                    _l_idx = 0\n                host_out_name = \'\'\n                for _c_idx, _c in enumerate(next_layer.components):\n                    if _c[\'port_in\'] == c[\'port_out\']:\n                        host_out_name = \'%s%d%d\' % (_c[\'name\'], _l_idx, _c_idx)\n                        break\n\n                if l_idx - 1 >= 0:\n                    last_layer = all_layers[l_idx - 1]\n                    _l_idx = l_idx - 1\n                else:\n                    last_layer = all_layers[-1]\n                    _l_idx = len(all_layers) - 1\n\n                host_in_name = \'\'\n                for _c_idx, _c in enumerate(last_layer.components):\n                    if _c[\'port_out\'] == c[\'port_in\']:\n                        host_in_name = \'%s%d%d\' % (_c[\'name\'], _l_idx, _c_idx)\n                        break\n\n                if \'BIND\' not in c[\'socket_out\']:\n                    args.append(\'--host_out %s\' % host_out_name)\n                if \'BIND\' not in c[\'socket_in\']:\n                    args.append(\'--host_in %s\' % host_in_name)\n\n                cmd = \'%s %s\' % (YamlComposer.comp2file[c[\'name\']], \' \'.join(args))\n                swarm_lines[\'services\'][c_name] = CommentedMap({\n                    \'image\': c[\'image\'] or docker_img,\n                    \'command\': cmd,\n                })\n\n                rep_c = YamlComposer.Layer.get_value(c, \'replicas\')\n                if rep_c > 1:\n                    swarm_lines[\'services\'][c_name][\'deploy\'] = CommentedMap({\n                        \'replicas\': YamlComposer.Layer.get_value(c, \'replicas\'),\n                        \'restart_policy\': {\n                            \'condition\': \'on-failure\',\n                            \'max_attempts\': 3,\n                        }\n                    })\n\n                if \'yaml_path\' in c and c[\'yaml_path\'] is not None \\\n                        and (c[\'yaml_path\'].endswith(\'.yml\') or c[\'yaml_path\'].endswith(\'.yaml\')):\n                    swarm_lines[\'services\'][c_name][\'configs\'] = [\'%s_yaml\' % c_name]\n\n                if c[\'name\'] == \'Frontend\':\n                    swarm_lines[\'services\'][c_name][\'ports\'] = [\'%d:%d\' % (c[\'grpc_port\'], c[\'grpc_port\'])]\n\n        if volumes:\n            swarm_lines[\'volumes\'] = volumes\n        if networks:\n            swarm_lines[\'networks\'] = volumes\n        swarm_lines[\'configs\'] = config_dict\n        stream = StringIO()\n        _yaml.dump(swarm_lines, stream)\n        return stream.getvalue().strip()\n\n    @staticmethod\n    def build_kubernetes(all_layers: List[\'YamlComposer.Layer\'], *args, **kwargs):\n        pass\n\n    @staticmethod\n    def build_shell(all_layers: List[\'YamlComposer.Layer\'], log_redirect: str = None) -> str:\n        shell_lines = []\n        for layer in all_layers:\n            for c in layer.components:\n                rep_c = YamlComposer.Layer.get_value(c, \'replicas\')\n                shell_lines.append(\'printf ""starting service \\\\e[1;33m%s\\\\e[0m with \\e[1;33m%s\\e[0m replicas...\\\\n""\' % (\n                    c[\'name\'], rep_c))\n                for _ in range(rep_c):\n                    cmd = YamlComposer.comp2file[c[\'name\']]\n                    args = \' \'.join(\n                        [\'--%s %s\' % (a, str(v) if \' \' not in str(v) else (\'""%s""\' % str(v))) for a, v in c.items() if\n                         a in YamlComposer.comp2args[c[\'name\']] and v])\n                    shell_lines.append(\'gnes %s %s %s &\' % (\n                        cmd, args, \'>> %s 2>&1\' % log_redirect if log_redirect else \'\'))\n\n        with resource_stream(\'gnes\', \'/\'.join((\'resources\', \'compose\', \'gnes-shell.sh\'))) as r:\n            return r.read().decode().replace(\'{{gnes-template}}\', \'\\n\'.join(shell_lines)).strip()\n\n    @staticmethod\n    def build_mermaid(all_layers: List[\'YamlComposer.Layer\'], mermaid_leftright: bool = False) -> str:\n        mermaid_graph = []\n        cls_dict = defaultdict(set)\n        for l_idx, layer in enumerate(all_layers[1:] + [all_layers[0]], 1):\n            last_layer = all_layers[l_idx - 1]\n\n            for c_idx, c in enumerate(last_layer.components):\n                # if len(last_layer.components) > 1:\n                #     self.mermaid_graph.append(\'\\tsubgraph %s%d\' % (c[\'name\'], c_idx))\n                for j in range(YamlComposer.Layer.get_value(c, \'replicas\')):\n                    for c1_idx, c1 in enumerate(layer.components):\n                        if c1[\'port_in\'] == c[\'port_out\']:\n                            p = \'((%s%s))\' if c[\'name\'] == \'Router\' else \'(%s%s)\'\n                            p1 = \'((%s%s))\' if c1[\'name\'] == \'Router\' else \'(%s%s)\'\n                            for j1 in range(YamlComposer.Layer.get_value(c1, \'replicas\')):\n                                _id, _id1 = \'%s%s%s\' % (last_layer.layer_id, c_idx, j), \'%s%s%s\' % (\n                                    layer.layer_id, c1_idx, j1)\n                                conn_type = (\n                                        c[\'socket_out\'].split(\'_\')[0] + \'/\' + c1[\'socket_in\'].split(\'_\')[0]).lower()\n                                s_id = \'%s%s\' % (c_idx if len(last_layer.components) > 1 else \'\',\n                                                 j if YamlComposer.Layer.get_value(c, \'replicas\') > 1 else \'\')\n                                s1_id = \'%s%s\' % (c1_idx if len(layer.components) > 1 else \'\',\n                                                  j1 if YamlComposer.Layer.get_value(c1, \'replicas\') > 1 else \'\')\n                                mermaid_graph.append(\n                                    \'\\t%s%s%s-- %s -->%s%s%s\' % (\n                                        c[\'name\'], _id, p % (c[\'name\'], s_id), conn_type, c1[\'name\'], _id1,\n                                        p1 % (c1[\'name\'], s1_id)))\n                                cls_dict[c[\'name\'] + \'CLS\'].add(\'%s%s\' % (c[\'name\'], _id))\n                                cls_dict[c1[\'name\'] + \'CLS\'].add(\'%s%s\' % (c1[\'name\'], _id1))\n                # if len(last_layer.components) > 1:\n                #     self.mermaid_graph.append(\'\\tend\')\n\n        style = [\'classDef FrontendCLS fill:#FFE0E0,stroke:#FFE0E0,stroke-width:1px;\',\n                 \'classDef EncoderCLS fill:#FFDAAF,stroke:#FFDAAF,stroke-width:1px;\',\n                 \'classDef IndexerCLS fill:#FFFBC1,stroke:#FFFBC1,stroke-width:1px;\',\n                 \'classDef RouterCLS fill:#C9E8D2,stroke:#C9E8D2,stroke-width:1px;\',\n                 \'classDef PreprocessorCLS fill:#CEEEEF,stroke:#CEEEEF,stroke-width:1px;\']\n        class_def = [\'class %s %s;\' % (\',\'.join(v), k) for k, v in cls_dict.items()]\n        mermaid_str = \'\\n\'.join(\n            [\'graph %s\' % (\'LR\' if mermaid_leftright else \'TD\')] + mermaid_graph + style + class_def)\n        return mermaid_str.strip()\n\n    @staticmethod\n    def build_html(generate_dict: Dict[str, str]) -> str:\n        with resource_stream(\'gnes\', \'/\'.join((\'resources\', \'compose\', \'gnes-board.html\'))) as r:\n            html = r.read().decode()\n            for k, v in generate_dict.items():\n                if v:\n                    html = html.replace(\'{{gnes-%s}}\' % k, v)\n        return html.strip()\n\n    def build_all(self):\n        def std_or_print(f, content):\n            if content and f:\n                with f as fp:\n                    fp.write(content)\n                    self.logger.info(\'generated content is written to %s\' % f)\n\n        all_layers = self.build_layers()\n        cmds = {\n            \'mermaid\': self.build_mermaid(all_layers, self.args.mermaid_leftright),\n            \'shell\': self.build_shell(all_layers, self.args.shell_log_redirect),\n            \'yaml\': self.original_yaml,\n            \'image\': self.args.docker_img,\n            \'docker\': self.build_dockerswarm(all_layers, self.args.docker_img,\n                                             volumes=self._volumes, networks=self._networks),\n            \'k8s\': self.build_kubernetes(all_layers),\n            \'timestamp\': time.strftime(""%a, %d %b %Y %H:%M:%S""),\n            \'version\': __version__\n        }\n\n        cmds[\'html\'] = self.build_html(cmds)\n\n        std_or_print(self.args.graph_path, cmds[\'mermaid\'])\n        std_or_print(self.args.shell_path, cmds[\'shell\'])\n        std_or_print(self.args.swarm_path, cmds[\'docker\'])\n        std_or_print(self.args.k8s_path, cmds[\'k8s\'])\n        std_or_print(self.args.html_path, cmds[\'html\'])\n        return cmds\n\n    @staticmethod\n    def _get_random_port(min_port: int = 49152, max_port: int = 65536) -> str:\n        return str(random.randrange(min_port, max_port))\n\n    @staticmethod\n    def _get_random_host(comp_name: str) -> str:\n        return str(comp_name + str(random.randrange(0, 100)))\n\n    def _add_router(self, last_layer: \'YamlComposer.Layer\', layer: \'YamlComposer.Layer\') -> List[\'YamlComposer.Layer\']:\n        def rule1():\n            # a shortcut fn: push connect the last and current\n            last_layer.components[0][\'socket_out\'] = str(SocketType.PUSH_BIND)\n            layer.components[0][\'socket_in\'] = str(SocketType.PULL_CONNECT)\n\n        def rule2():\n            # a shortcut fn: pub connect the last and the current\n            last_layer.components[0][\'socket_out\'] = str(SocketType.PUB_BIND)\n            layer.components[0][\'socket_in\'] = str(SocketType.SUB_CONNECT)\n\n        def rule3():\n            # a shortcut fn: (N)-2-(N) with push pull connection\n            router_layer = YamlComposer.Layer(layer_id=self._num_layer)\n            self._num_layer += 1\n            last_layer.components[0][\'socket_out\'] = str(SocketType.PUSH_CONNECT)\n            r = CommentedMap({\'name\': \'Router\',\n                              \'yaml_path\': \'BaseRouter\',\n                              \'socket_in\': str(SocketType.PULL_BIND),\n                              \'socket_out\': str(SocketType.PUSH_BIND),\n                              \'port_in\': last_layer.components[0][\'port_out\'],\n                              \'port_out\': self._get_random_port()})\n            self.check_fields(r)\n            for c in layer.components:\n                c[\'socket_in\'] = str(SocketType.PULL_CONNECT)\n                c[\'port_in\'] = r[\'port_out\']\n            router_layer.append(r)\n            router_layers.append(router_layer)\n\n        def rule4():\n            # a shortcut fn: (N)-to-(1)&(1)&(1)\n            last_layer.components[0][\'socket_out\'] = str(SocketType.PUB_BIND)\n            for c in layer.components:\n                c[\'socket_in\'] = str(SocketType.SUB_CONNECT)\n\n        def rule5():\n            # a shortcut fn: based on c3(): (N)-2-(N) with pub sub connection\n            rule3()\n            router_layers[0].components[0][\'socket_out\'] = str(SocketType.PUB_BIND)\n            router_layers[0].components[0][\'yaml_path\'] = \'""!PublishRouter {parameters: {num_part: %d}}""\' \\\n                                                          % len(layer.components)\n            for c in layer.components:\n                c[\'socket_in\'] = str(SocketType.SUB_CONNECT)\n\n        def rule6():\n            last_layer.components[0][\'socket_out\'] = str(SocketType.PUB_BIND)\n            router_layer = YamlComposer.Layer(layer_id=self._num_layer)\n            self._num_layer += 1\n            for c in layer.components:\n                income = self.Layer.get_value(c, \'income\')\n                r = CommentedMap({\'name\': \'Router\',\n                                  \'yaml_path\': \'BaseReduceRouter\',\n                                  \'socket_in\': str(SocketType.SUB_CONNECT),\n                                  \'socket_out\': str(SocketType.PUSH_BIND) if income == \'pull\' else str(\n                                      SocketType.PUB_BIND),\n                                  \'port_in\': last_layer.components[0][\'port_out\'],\n                                  \'port_out\': self._get_random_port()})\n                self.check_fields(r)\n                c[\'socket_in\'] = str(SocketType.PULL_CONNECT) if income == \'pull\' else str(SocketType.SUB_CONNECT)\n                c[\'port_in\'] = r[\'port_out\']\n                router_layer.append(r)\n            router_layers.append(router_layer)\n\n        def rule7():\n            last_layer.components[0][\'socket_out\'] = str(SocketType.PUSH_CONNECT)\n\n            router_layer = YamlComposer.Layer(layer_id=self._num_layer)\n            self._num_layer += 1\n            r0 = CommentedMap({\'name\': \'Router\',\n                               \'yaml_path\': \'""!PublishRouter {parameters: {num_part: %d}}""\' % len(layer.components),\n                               \'socket_in\': str(SocketType.PULL_BIND),\n                               \'socket_out\': str(SocketType.PUB_BIND),\n                               \'port_in\': self._get_random_port(),\n                               \'port_out\': self._get_random_port()})\n            self.check_fields(r0)\n            router_layer.append(r0)\n            router_layers.append(router_layer)\n            last_layer.components[0][\'port_out\'] = r0[\'port_in\']\n\n            router_layer = YamlComposer.Layer(layer_id=self._num_layer)\n            self._num_layer += 1\n            for c in layer.components:\n                r = CommentedMap({\'name\': \'Router\',\n                                  \'yaml_path\': \'BaseRouter\',\n                                  \'socket_in\': str(SocketType.SUB_CONNECT),\n                                  \'socket_out\': str(SocketType.PUSH_BIND),\n                                  \'port_in\': r0[\'port_out\'],\n                                  \'port_out\': self._get_random_port()})\n                c[\'socket_in\'] = str(SocketType.PULL_CONNECT)\n                c[\'port_in\'] = r[\'port_out\']\n                self.check_fields(r)\n                router_layer.append(r)\n            router_layers.append(router_layer)\n\n        def rule10():\n            last_layer.components[0][\'socket_out\'] = str(SocketType.PUSH_CONNECT)\n\n            router_layer = YamlComposer.Layer(layer_id=self._num_layer)\n            self._num_layer += 1\n            r0 = CommentedMap({\'name\': \'Router\',\n                               \'yaml_path\': \'""!PublishRouter {parameters: {num_part: %d}}""\' % len(layer.components),\n                               \'socket_in\': str(SocketType.PULL_BIND),\n                               \'socket_out\': str(SocketType.PUB_BIND),\n                               \'port_in\': self._get_random_port(),\n                               \'port_out\': self._get_random_port()})\n            self.check_fields(r0)\n            router_layer.append(r0)\n            router_layers.append(router_layer)\n            last_layer.components[0][\'port_out\'] = r0[\'port_in\']\n\n            for c in layer.components:\n                c[\'socket_in\'] = str(SocketType.SUB_CONNECT)\n                c[\'port_in\'] = r0[\'port_out\']\n\n        def rule8():\n            router_layer = YamlComposer.Layer(layer_id=self._num_layer)\n            self._num_layer += 1\n            r = CommentedMap({\'name\': \'Router\',\n                              \'yaml_path\': \'BaseReduceRouter\',\n                              \'socket_in\': str(SocketType.PULL_BIND),\n                              \'socket_out\': str(SocketType.PUSH_BIND),\n                              \'port_in\': self._get_random_port(),\n                              \'port_out\': self._get_random_port()})\n            self.check_fields(r)\n\n            for c in last_layer.components:\n                last_income = self.Layer.get_value(c, \'income\')\n                if last_income == \'sub\':\n                    c[\'socket_out\'] = str(SocketType.PUSH_CONNECT)\n                    r_c = CommentedMap({\'name\': \'Router\',\n                                        \'yaml_path\': \'BaseReduceRouter\',\n                                        \'socket_in\': str(SocketType.PULL_BIND),\n                                        \'socket_out\': str(SocketType.PUSH_CONNECT),\n                                        \'port_in\': self._get_random_port(),\n                                        \'port_out\': r[\'port_in\']})\n                    c[\'port_out\'] = r_c[\'port_in\']\n                    self.check_fields(r_c)\n                    router_layer.append(r_c)\n                elif last_income == \'pull\':\n                    c[\'socket_out\'] = str(SocketType.PUSH_CONNECT)\n                    c[\'port_out\'] = r[\'port_in\']\n\n            for c in layer.components:\n                c[\'socket_in\'] = str(SocketType.PULL_CONNECT)\n                c[\'port_in\'] = r[\'port_out\']\n\n            if router_layer.components:\n                router_layers.append(router_layer)\n            else:\n                self._num_layer -= 1\n\n            router_layer = YamlComposer.Layer(layer_id=self._num_layer)\n            self._num_layer += 1\n            router_layer.append(r)\n            router_layers.append(router_layer)\n\n        def rule9():\n            # a shortcut fn: push connect the last and current\n            last_layer.components[0][\'socket_out\'] = str(SocketType.PUSH_CONNECT)\n            layer.components[0][\'socket_in\'] = str(SocketType.PULL_BIND)\n\n        router_layers = []  # type: List[\'self.Layer\']\n        # bind the last out to current in\n\n        if last_layer.is_single_component:\n            last_layer.components[0][\'port_out\'] = self._get_random_port()\n            for c in layer.components:\n                c[\'port_in\'] = last_layer.components[0][\'port_out\']\n            # 1-to-?\n            if layer.is_single_component:\n                # 1-to-(1)\n                # no router is needed\n                rule1()\n            elif layer.is_homo_multi_component:\n                # 1-to-(N)\n                income = self.Layer.get_value(layer.components[0], \'income\')\n                if income == \'pull\':\n                    rule1()\n                elif income == \'sub\':\n                    rule2()\n                else:\n                    raise NotImplementedError(\'replica type: %s is not recognized!\' % income)\n            elif layer.is_heto_single_component:\n                # 1-to-(1)&(1)&(1)\n                rule4()\n            else:\n                # 1-to-(N)&(N)&(N)\n                rule6()\n        elif last_layer.is_homo_multi_component:\n            # (N)-to-?\n            last_layer.components[0][\'port_out\'] = self._get_random_port()\n\n            last_income = self.Layer.get_value(last_layer.components[0], \'income\')\n\n            for c in layer.components:\n                c[\'port_in\'] = last_layer.components[0][\'port_out\']\n\n            if layer.is_single_component:\n                if last_income == \'pull\':\n                    # (N)-to-1\n                    rule9()\n                elif last_income == \'sub\':\n                    # (N)-to-1 with a sync barrier\n                    rule3()\n                else:\n                    raise NotImplementedError(\'replica type: %s is not recognized!\' % last_income)\n            elif layer.is_homo_multi_component:\n                # (N)-to-(N)\n                # need a router anyway\n                if self.Layer.get_value(layer.components[0], \'income\') == \'sub\':\n                    rule5()\n                else:\n                    rule3()\n            elif layer.is_heto_single_component:\n                # (N)-to-(1)&(1)&(1)\n                rule5()\n            else:\n                income = self.Layer.get_value(layer.components[0], \'income\')\n                if income == \'pull\':\n                    rule7()\n                elif income == \'sub\':\n                    rule10()\n                else:\n                    raise NotImplementedError(\'replica type: %s is not recognized!\' % last_income)\n        elif last_layer.is_heto_single_component:\n            rule8()\n        else:\n            rule8()\n        return [last_layer, *router_layers]\n\n\ndef parse_http_data(data, args):\n    import io\n    if not data or \'yaml-config\' not in data:\n        return \'<h1>Bad POST request</h1> your POST request does not contain ""yaml-config"" field!\', 406\n    try:\n        args.yaml_path = io.StringIO(data[\'yaml-config\'])\n        if data.get(\'mermaid_direction\', \'top-down\').lower() == \'left-right\':\n            args.mermaid_leftright = True\n        else:\n            args.mermaid_leftright = False\n        if \'docker-image\' in data:\n            args.docker_img = data[\'docker-image\']\n        else:\n            args.docker_img = \'gnes/gnes:latest-alpine\'\n\n        return YamlComposer(args).build_all()[\'html\'], 200\n    except Exception as e:\n        return \'<h1>Bad YAML input</h1> please kindly check the format, \' \\\n               \'indent and content of your YAML file! <h3>Traceback: </h3><p><code>%s</code></p>\' % e, 400\n'"
gnes/composer/flask.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nfrom .base import YamlComposer, parse_http_data\nfrom ..cli.parser import set_composer_parser\nfrom ..helper import set_logger\n\n\nclass YamlComposerFlask:\n    def __init__(self, args):\n        self.args = args\n        self.logger = set_logger(self.__class__.__name__, self.args.verbose)\n\n    def _create_flask_app(self):\n        try:\n            from flask import Flask, request\n        except ImportError:\n            raise ImportError(\'Flask or its dependencies are not fully installed, \'\n                              \'they are required for serving HTTP requests.\'\n                              \'Please use ""pip install gnes[flask]"" to install it.\')\n\n        app = Flask(__name__)\n        args = set_composer_parser().parse_args([])\n        default_html = YamlComposer(args).build_all()[\'html\']\n\n        @app.errorhandler(500)\n        def exception_handler(error):\n            self.logger.error(\'unhandled error, i better quit and restart! %s\' % error)\n            return \'<h1>500 Internal Error</h1> \' \\\n                   \'While we are fixing the issue, do you know you can deploy GNES board locally on your machine? \' \\\n                   \'Simply run <pre>docker run -d -p 0.0.0.0:80:8080/tcp gnes/gnes compose --flask</pre>\', 500\n\n        @app.route(\'/\', methods=[\'GET\'])\n        def _get_homepage():\n            return default_html\n\n        @app.route(\'/generate\', methods=[\'POST\'])\n        def _regenerate():\n            data = request.form if request.form else request.json\n            return parse_http_data(data, args)\n\n        return app\n\n    def run(self):\n        app = self._create_flask_app()\n        app.run(port=self.args.http_port, threaded=True, host=\'0.0.0.0\')\n'"
gnes/composer/http.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nfrom http.server import BaseHTTPRequestHandler, HTTPServer\nfrom urllib.parse import parse_qs\n\nfrom .base import YamlComposer, parse_http_data\nfrom ..cli.parser import set_composer_parser\nfrom ..helper import set_logger\n\n\nclass YamlComposerHttp:\n    def __init__(self, args):\n        self.args = args\n        self.logger = set_logger(self.__class__.__name__, self.args.verbose)\n\n    class _HttpServer(BaseHTTPRequestHandler):\n        args = set_composer_parser().parse_args([])\n        default_html = YamlComposer(args).build_all()[\'html\']\n\n        def _set_response(self, msg: str, code: int = 200):\n            self.send_response(code)\n            self.send_header(\'Content-type\', \'text/html\')\n            self.end_headers()\n            self.wfile.write(msg.encode(\'utf-8\'))\n\n        def do_GET(self):\n            if str(self.path) != \'/\':\n                self._set_response(\'<h1>""%s"" is not a valid entrypoint</h1>\' % self.path, 400)\n                return\n            self._set_response(self.default_html)\n\n        def do_POST(self):\n            if str(self.path) != \'/generate\':\n                self._set_response(\'<h1>""%s"" is not a valid entrypoint</h1>\' % self.path, 400)\n                return\n            content_length = int(self.headers[\'Content-Length\'])  # <--- Gets the size of data\n            data = self.rfile.read(content_length)  # <--- Gets the data itself\n\n            data = {k: v[0] for k, v in parse_qs(data.decode(\'utf-8\')).items()}\n            self._set_response(*parse_http_data(data, self.args))\n\n    def run(self):\n        httpd = HTTPServer((\'0.0.0.0\', self.args.http_port), self._HttpServer)\n        try:\n            httpd.serve_forever()\n        except KeyboardInterrupt:\n            pass\n        finally:\n            httpd.server_close()\n'"
gnes/encoder/__init__.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\n# A key-value map for Class to the (module)file it located in\nfrom ..base import register_all_class\n\n_cls2file_map = {\n    \'BertEncoder\': \'text.bert\',\n    \'BertEncoderWithServer\': \'text.bert\',\n    \'BertEncoderServer\': \'text.bert\',\n    \'FlairEncoder\': \'text.flair\',\n    \'PCALocalEncoder\': \'numeric.pca\',\n    \'PQEncoder\': \'numeric.pq\',\n    \'TFPQEncoder\': \'numeric.tf_pq\',\n    \'Word2VecEncoder\': \'text.w2v\',\n    \'BaseEncoder\': \'base\',\n    \'BaseBinaryEncoder\': \'base\',\n    \'BaseTextEncoder\': \'base\',\n    \'BaseVideoEncoder\': \'base\',\n    \'BaseNumericEncoder\': \'base\',\n    \'BaseAudioEncoder\': \'base\',\n    \'PipelineEncoder\': \'base\',\n    \'HashEncoder\': \'numeric.hash\',\n    \'TorchvisionEncoder\': \'image.torchvision\',\n    \'TFInceptionEncoder\': \'image.inception\',\n    \'CVAEEncoder\': \'image.cvae\',\n    \'IncepMixtureEncoder\': \'video.incep_mixture\',\n    \'VladEncoder\': \'numeric.vlad\',\n    \'MfccEncoder\': \'audio.mfcc\',\n    \'PoolingEncoder\': \'numeric.pooling\',\n    \'PyTorchTransformers\': \'text.transformer\',\n    \'VggishEncoder\': \'audio.vggish\',\n    \'YouTube8MFeatureExtractor\': \'video.yt8m_feature_extractor\',\n    \'YouTube8MEncoder\': \'video.yt8m_model\',\n    \'InceptionVideoEncoder\': \'video.inception\',\n    \'QuantizerEncoder\': \'numeric.quantizer\',\n    \'CharEmbeddingEncoder\': \'text.char\'\n}\n\nregister_all_class(_cls2file_map, \'encoder\')\n'"
gnes/encoder/base.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nfrom typing import List, Any, Tuple, Union\n\nimport numpy as np\n\nfrom ..base import TrainableBase, CompositionalTrainableBase\n\n\nclass BaseEncoder(TrainableBase):\n\n    def encode(self, data: Any, *args, **kwargs) -> Any:\n        pass\n\n    def _copy_from(self, x: \'BaseEncoder\') -> None:\n        pass\n\n\nclass BaseImageEncoder(BaseEncoder):\n\n    def encode(self, img: List[\'np.ndarray\'], *args, **kwargs) -> np.ndarray:\n        pass\n\n\nclass BaseVideoEncoder(BaseEncoder):\n\n    def encode(self, data: List[\'np.ndarray\'], *args, **kwargs) -> Union[np.ndarray, List[\'np.ndarray\']]:\n        pass\n\n\nclass BaseTextEncoder(BaseEncoder):\n\n    def encode(self, text: List[str], *args, **kwargs) -> Union[Tuple, np.ndarray]:\n        pass\n\n\nclass BaseNumericEncoder(BaseEncoder):\n    """"""Note that all NumericEncoder can not be used as the first encoder of the pipeline""""""\n\n    def encode(self, data: np.ndarray, *args, **kwargs) -> np.ndarray:\n        pass\n\n\nclass BaseAudioEncoder(BaseEncoder):\n\n    def encode(self, data: List[\'np.ndarray\'], *args, **kwargs) -> np.ndarray:\n        pass\n\n\nclass BaseBinaryEncoder(BaseEncoder):\n\n    def encode(self, data: np.ndarray, *args, **kwargs) -> bytes:\n        if data.dtype != np.uint8:\n            raise ValueError(\'data must be np.uint8 but received %s\' % data.dtype)\n        return data.tobytes()\n\n\nclass PipelineEncoder(CompositionalTrainableBase):\n    def encode(self, data: Any, *args, **kwargs) -> Any:\n        if not self.components:\n            raise NotImplementedError\n        for be in self.components:\n            data = be.encode(data, *args, **kwargs)\n        return data\n\n    def train(self, data, *args, **kwargs):\n        if not self.components:\n            raise NotImplementedError\n        for idx, be in enumerate(self.components):\n            if not be.is_trained:\n                be.train(data, *args, **kwargs)\n\n            if idx + 1 < len(self.components):\n                data = be.encode(data, *args, **kwargs)\n'"
gnes/flow/__init__.py,0,"b'import copy\nfrom collections import OrderedDict, defaultdict\nfrom contextlib import ExitStack\nfrom typing import Union, Tuple, List, Optional, Iterator\n\nfrom .helper import *\nfrom ..base import TrainableBase\nfrom ..helper import set_logger\nfrom ..service.base import SocketType, BaseService\n\n\nclass Flow(TrainableBase):\n    """"""\n    GNES Flow: an intuitive way to build workflow for GNES.\n\n    You can use :py:meth:`.add()` then :py:meth:`.build()` to customize your own workflow.\n    For example:\n\n    .. highlight:: python\n    .. code-block:: python\n\n        from gnes.flow import Flow\n\n        f = (Flow(check_version=False, route_table=True)\n             .add_preprocessor(yaml_path=\'BasePreprocessor\')\n             .add_encoder(yaml_path=\'BaseEncoder\')\n             .add_router(yaml_path=\'BaseRouter\'))\n\n        with f.build(backend=\'thread\') as flow:\n            flow.index()\n            ...\n\n    You can also use `add(\'Encoder\', ...)` or `add(Service.Encoder, ...)` to add service to the flow.\n    The generic :py:meth:`add` provides a convenient way to build the flow.\n\n    As shown above, it is recommend to use flow in the context manner as showed above,\n    as it manages all opened sockets/processes/threads automatically when exit from the context.\n\n    Note the different copy behaviors in :py:meth:`.add()` and :py:meth:`.build()`:\n    :py:meth:`.add()` always copy the flow by default, whereas :py:meth:`.build()` modify the flow in place.\n    You can change this behavior by specifying th argument `copy_flow=False`.\n\n    """"""\n\n    # a shortcut to the service frontend, removing one extra import\n    Frontend = Service.Frontend\n\n    def __init__(self, with_frontend: bool = True, is_trained: bool = True, *args, **kwargs):\n        """"""\n        Create a new Flow object.\n\n        :param with_frontend: adding frontend service to the flow\n        :param is_trained: indicating whether this flow is trained or not. if set to False then :py:meth:`index`\n                            and :py:meth:`query` can not be called before :py:meth:`train`\n        :param kwargs: keyword-value arguments that will be shared by all services\n        """"""\n        super().__init__(*args, **kwargs)\n        self.logger = set_logger(self.__class__.__name__)\n        self._service_nodes = OrderedDict()\n        self._service_edges = {}\n        self._service_name_counter = {k: 0 for k in service_map.keys()}\n        self._service_contexts = []\n        self._last_changed_service = []\n        self._common_kwargs = kwargs\n        self._frontend = None\n        self._client = None\n        self._build_level = BuildLevel.EMPTY\n        self._backend = None\n        self._init_with_frontend = False\n        self.is_trained = is_trained\n        if with_frontend:\n            self.add_frontend(copy_flow=False)\n            self._init_with_frontend = True\n        else:\n            self.logger.warning(\'with_frontend is set to False, you need to add_frontend() by yourself\')\n\n    @build_required(BuildLevel.GRAPH)\n    def to_k8s_yaml(self) -> str:\n        raise NotImplementedError\n\n    @build_required(BuildLevel.GRAPH)\n    def to_shell_script(self) -> str:\n        raise NotImplementedError\n\n    @build_required(BuildLevel.GRAPH)\n    def to_swarm_yaml(self, image: str = \'gnes/gnes:latest-alpine\') -> str:\n        """"""\n        Generate the docker swarm YAML compose file\n\n        :param image: the default GNES docker image\n        :return: the generated YAML compose file\n        """"""\n        from ruamel.yaml import YAML, StringIO\n        _yaml = YAML()\n        swarm_yml = {\'version\': \'3.4\',\n                     \'services\': {}}\n\n        for k, v in self._service_nodes.items():\n            defaults_kwargs, _ = service_map[v[\'service\']][\'parser\']().parse_known_args(\n                [\'--yaml_path\', \'TrainableBase\'])\n            non_default_kwargs = {k: v for k, v in vars(v[\'parsed_args\']).items() if getattr(defaults_kwargs, k) != v}\n            if not isinstance(non_default_kwargs.get(\'yaml_path\', \'\'), str):\n                non_default_kwargs[\'yaml_path\'] = v[\'kwargs\'][\'yaml_path\']\n\n            num_replicas = None\n            if \'num_parallel\' in non_default_kwargs:\n                num_replicas = non_default_kwargs.pop(\'num_parallel\')\n\n            swarm_yml[\'services\'][k] = {\n                \'image\': v[\'kwargs\'].get(\'image\', image),\n                \'command\': \'%s %s\' % (\n                    service_map[v[\'service\']][\'cmd\'],\n                    \' \'.join([\'--%s %s\' % (k, v) for k, v in non_default_kwargs.items()]))\n            }\n            if num_replicas and num_replicas > 1:\n                swarm_yml[\'services\'][k][\'deploy\'] = {\'replicas\': num_replicas}\n\n        stream = StringIO()\n        _yaml.dump(swarm_yml, stream)\n        return stream.getvalue().strip()\n\n    def to_python_code(self, indent: int = 4) -> str:\n        """"""\n        Generate the python code of this flow\n\n        :param indent: the number of whitespaces of indent\n        :return: the generated python code\n        """"""\n        py_code = [\'from gnes.flow import Flow\', \'\']\n        kwargs = []\n        if not self._init_with_frontend:\n            kwargs.append(\'with_frontend=False\')\n        if self._common_kwargs:\n            kwargs.extend(\'%s=%s\' % (k, v) for k, v in self._common_kwargs.items())\n        py_code.append(\'f = (Flow(%s)\' % (\', \'.join(kwargs)))\n\n        known_service = set()\n        last_add_name = \'\'\n\n        for k, v in self._service_nodes.items():\n            kwargs = OrderedDict()\n            kwargs[\'service\'] = str(v[\'service\'])\n            kwargs[\'name\'] = k\n            kwargs[\'recv_from\'] = \'[%s]\' % (\n                \',\'.join({\'\\\'%s\\\'\' % k for k in v[\'incomes\'] if k in known_service}))\n            if kwargs[\'recv_from\'] == \'[\\\'%s\\\']\' % last_add_name:\n                kwargs.pop(\'recv_from\')\n            kwargs[\'send_to\'] = \'[%s]\' % (\',\'.join({\'\\\'%s\\\'\' % k for k in v[\'outgoings\'] if k in known_service}))\n\n            known_service.add(k)\n            last_add_name = k\n\n            py_code.append(\'%s.add(%s)\' % (\n                \' \' * indent,\n                \', \'.join(\n                    \'%s=%s\' % (kk, \'\\\'%s\\\'\' % vv if isinstance(vv, str)\n                                                    and not vv.startswith(\'\\\'\') and not vv.startswith(\'[\')\n                    else vv) for kk, vv\n                    in\n                    (list(kwargs.items()) + list(v[\'kwargs\'].items())) if\n                    vv and vv != \'[]\' and kk not in self._common_kwargs)))\n\n        py_code[-1] += \')\'\n\n        py_code.extend([\'\',\n                        \'# build the flow and visualize it\',\n                        \'f.build(backend=None).to_url()\'\n                        ])\n        py_code.extend([\'\',\n                        \'# use this flow in multi-thread mode for indexing\',\n                        \'with f.build(backend=\\\'thread\\\') as fl:\',\n                        \'%sfl.index(txt_file=\\\'test.txt\\\')\' % (\' \' * indent)\n                        ])\n        py_code.append(\'\')\n\n        return \'\\n\'.join(py_code)\n\n    @build_required(BuildLevel.GRAPH)\n    def to_mermaid(self, left_right: bool = True) -> str:\n        """"""\n        Output the mermaid graph for visualization\n\n        :param left_right: render the flow in left-to-right manner, otherwise top-down manner.\n        :return: a mermaid-formatted string\n        """"""\n\n        # fill, stroke\n        service_color = {\n            Service.Frontend: (\'#FFE0E0\', \'#000\'),\n            Service.Router: (\'#C9E8D2\', \'#000\'),\n            Service.Encoder: (\'#FFDAAF\', \'#000\'),\n            Service.Preprocessor: (\'#CED7EF\', \'#000\'),\n            Service.Indexer: (\'#FFFBC1\', \'#000\'),\n        }\n\n        mermaid_graph = OrderedDict()\n        cls_dict = defaultdict(set)\n        replicas_dict = {}\n\n        for k, v in self._service_nodes.items():\n            mermaid_graph[k] = []\n            num_replicas = getattr(v[\'parsed_args\'], \'num_parallel\', 1)\n            if num_replicas > 1:\n                head_router = k + \'_HEAD\'\n                tail_router = k + \'_TAIL\'\n                replicas_dict[k] = (head_router, tail_router)\n                cls_dict[Service.Router].add(head_router)\n                cls_dict[Service.Router].add(tail_router)\n                p_r = \'((%s))\'\n                k_service = v[\'service\']\n                p_e = \'((%s))\' if k_service == Service.Router else \'(%s)\'\n\n                mermaid_graph[k].append(\'subgraph %s[""%s (replias=%d)""]\' % (k, k, num_replicas))\n                for j in range(num_replicas):\n                    r = k + \'_%d\' % j\n                    cls_dict[k_service].add(r)\n                    mermaid_graph[k].append(\'\\t%s%s-->%s%s\' % (head_router, p_r % \'router\', r, p_e % r))\n                    mermaid_graph[k].append(\'\\t%s%s-->%s%s\' % (r, p_e % r, tail_router, p_r % \'router\'))\n                mermaid_graph[k].append(\'end\')\n                mermaid_graph[k].append(\n                    \'style %s fill:%s,stroke:%s,stroke-width:2px,stroke-dasharray:5,stroke-opacity:0.3,fill-opacity:0.5\' % (\n                        k, service_color[k_service][0], service_color[k_service][1]))\n\n        for k, ed_type in self._service_edges.items():\n            start_node, end_node = k.split(\'-\')\n            cur_node = mermaid_graph[start_node]\n\n            s_service = self._service_nodes[start_node][\'service\']\n            e_service = self._service_nodes[end_node][\'service\']\n\n            start_node_text = start_node\n            end_node_text = end_node\n\n            # check if is in replicas\n            if start_node in replicas_dict:\n                start_node = replicas_dict[start_node][1]  # outgoing\n                s_service = Service.Router\n                start_node_text = \'router\'\n            if end_node in replicas_dict:\n                end_node = replicas_dict[end_node][0]  # incoming\n                e_service = Service.Router\n                end_node_text = \'router\'\n\n            # always plot frontend at the start and the end\n            if e_service == Service.Frontend:\n                end_node_text = end_node\n                end_node += \'_END\'\n\n            cls_dict[s_service].add(start_node)\n            cls_dict[e_service].add(end_node)\n            p_s = \'((%s))\' if s_service == Service.Router else \'(%s)\'\n            p_e = \'((%s))\' if e_service == Service.Router else \'(%s)\'\n            cur_node.append(\'\\t%s%s-- %s -->%s%s\' % (\n                start_node, p_s % start_node_text, ed_type,\n                end_node, p_e % end_node_text))\n\n        style = [\'classDef %sCLS fill:%s,stroke:%s,stroke-width:1px;\' % (k, v[0], v[1]) for k, v in\n                 service_color.items()]\n        class_def = [\'class %s %sCLS;\' % (\',\'.join(v), k) for k, v in cls_dict.items()]\n        mermaid_str = \'\\n\'.join(\n            [\'graph %s\' % (\'LR\' if left_right else \'TD\')] + [ss for s in mermaid_graph.values() for ss in\n                                                             s] + style + class_def)\n\n        return mermaid_str\n\n    @build_required(BuildLevel.GRAPH)\n    def to_url(self, **kwargs) -> str:\n        """"""\n        Rendering the current flow as a url points to a SVG, it needs internet connection\n\n        :param kwargs: keyword arguments of :py:meth:`to_mermaid`\n        :return: the url points to a SVG\n        """"""\n        import base64\n        mermaid_str = self.to_mermaid(**kwargs)\n        encoded_str = base64.b64encode(bytes(mermaid_str, \'utf-8\')).decode(\'utf-8\')\n        return \'https://mermaidjs.github.io/mermaid-live-editor/#/view/%s\' % encoded_str\n\n    @build_required(BuildLevel.GRAPH)\n    def to_jpg(self, path: str = \'flow.jpg\', **kwargs) -> None:\n        """"""\n        Rendering the current flow as a jpg image, this will call :py:meth:`to_mermaid` and it needs internet connection\n\n        :param path: the file path of the image\n        :param kwargs: keyword arguments of :py:meth:`to_mermaid`\n        :return:\n        """"""\n\n        from urllib.request import Request, urlopen\n        encoded_str = self.to_url().replace(\'https://mermaidjs.github.io/mermaid-live-editor/#/view/\', \'\')\n        self.logger.warning(\'jpg exporting relies on https://mermaid.ink/, but it is not very stable. \'\n                            \'some syntax are not supported, please use with caution.\')\n        self.logger.info(\'downloading as jpg...\')\n        req = Request(\'https://mermaid.ink/img/%s\' % encoded_str, headers={\'User-Agent\': \'Mozilla/5.0\'})\n        with open(path, \'wb\') as fp:\n            fp.write(urlopen(req).read())\n        self.logger.info(\'done\')\n\n    def train(self, bytes_gen: Iterator[bytes] = None, **kwargs):\n        """"""Do training on the current flow\n\n        It will start a :py:class:`CLIClient` and call :py:func:`train`.\n\n        Example,\n\n        .. highlight:: python\n        .. code-block:: python\n\n            with f.build(backend=\'thread\') as flow:\n                flow.train(txt_file=\'aa.txt\')\n                flow.train(image_zip_file=\'aa.zip\', batch_size=64)\n                flow.train(video_zip_file=\'aa.zip\')\n                ...\n\n\n        This will call the pre-built reader to read files into an iterator of bytes and feed to the flow.\n\n        One may also build a reader/generator on your own.\n\n        Example,\n\n        .. highlight:: python\n        .. code-block:: python\n\n            def my_reader():\n                for _ in range(10):\n                    yield b\'abcdfeg\'   # each yield generates a document for training\n\n            with f.build(backend=\'thread\') as flow:\n                flow.train(bytes_gen=my_reader())\n\n        :param bytes_gen: An iterator of bytes. If not given, then you have to specify it in `kwargs`.\n        :param kwargs: accepts all keyword arguments of `gnes client` CLI\n        """"""\n        self._get_client(bytes_gen, mode=\'train\', **kwargs).start()\n\n    def index(self, bytes_gen: Iterator[bytes] = None, **kwargs):\n        """"""Do indexing on the current flow\n\n        Example,\n\n        .. highlight:: python\n        .. code-block:: python\n\n            with f.build(backend=\'thread\') as flow:\n                flow.index(txt_file=\'aa.txt\')\n                flow.index(image_zip_file=\'aa.zip\', batch_size=64)\n                flow.index(video_zip_file=\'aa.zip\')\n                ...\n\n\n        This will call the pre-built reader to read files into an iterator of bytes and feed to the flow.\n\n        One may also build a reader/generator on your own.\n\n        Example,\n\n        .. highlight:: python\n        .. code-block:: python\n\n            def my_reader():\n                for _ in range(10):\n                    yield b\'abcdfeg\'  # each yield generates a document to index\n\n            with f.build(backend=\'thread\') as flow:\n                flow.index(bytes_gen=my_reader())\n\n        It will start a :py:class:`CLIClient` and call :py:func:`index`.\n\n        :param bytes_gen: An iterator of bytes. If not given, then you have to specify it in `kwargs`.\n        :param kwargs: accepts all keyword arguments of `gnes client` CLI\n        """"""\n        self._get_client(bytes_gen, mode=\'index\', **kwargs).start()\n\n    def query(self, bytes_gen: Iterator[bytes] = None, **kwargs):\n        """"""Do indexing on the current flow\n\n        It will start a :py:class:`CLIClient` and call :py:func:`query`.\n\n\n        Example,\n\n        .. highlight:: python\n        .. code-block:: python\n\n            with f.build(backend=\'thread\') as flow:\n                flow.query(txt_file=\'aa.txt\')\n                flow.query(image_zip_file=\'aa.zip\', batch_size=64)\n                flow.query(video_zip_file=\'aa.zip\')\n                ...\n\n\n        This will call the pre-built reader to read files into an iterator of bytes and feed to the flow.\n\n        One may also build a reader/generator on your own.\n\n        Example,\n\n        .. highlight:: python\n        .. code-block:: python\n\n            def my_reader():\n                for _ in range(10):\n                    yield b\'abcdfeg\'   # each yield generates a query for searching\n\n            with f.build(backend=\'thread\') as flow:\n                flow.query(bytes_gen=my_reader())\n\n        :param bytes_gen: An iterator of bytes. If not given, then you have to specify it in `kwargs`.\n        :param kwargs: accepts all keyword arguments of `gnes client` CLI\n        """"""\n        yield from self._get_client(bytes_gen, mode=\'query\', **kwargs).query()\n\n    @build_required(BuildLevel.RUNTIME)\n    def _get_client(self, bytes_gen: Iterator[bytes] = None, **kwargs):\n        from ..cli.parser import set_client_cli_parser\n        from ..client.cli import CLIClient\n\n        _, p_args, _ = self._get_parsed_args(self, CLIClient.__name__, set_client_cli_parser, kwargs)\n        p_args.grpc_port = self._service_nodes[self._frontend][\'parsed_args\'].grpc_port\n        p_args.grpc_host = self._service_nodes[self._frontend][\'parsed_args\'].grpc_host\n        c = CLIClient(p_args, start_at_init=False)\n        if bytes_gen:\n            c.bytes_generator = bytes_gen\n        return c\n\n    def add_frontend(self, *args, **kwargs) -> \'Flow\':\n        """"""Add a frontend to the current flow, a shortcut of :py:meth:`add(Service.Frontend)`.\n        Usually you dont need to call this function explicitly, a flow object contains a frontend service by default.\n        This function is useful when you build a flow without the frontend and want to customize the frontend later.\n        """"""\n        return self.add(Service.Frontend, *args, **kwargs)\n\n    def add_encoder(self, *args, **kwargs) -> \'Flow\':\n        """"""Add an encoder to the current flow, a shortcut of :py:meth:`add(Service.Encoder)`""""""\n        return self.add(Service.Encoder, *args, **kwargs)\n\n    def add_indexer(self, *args, **kwargs) -> \'Flow\':\n        """"""Add an indexer to the current flow, a shortcut of :py:meth:`add(Service.Indexer)`""""""\n        return self.add(Service.Indexer, *args, **kwargs)\n\n    def add_preprocessor(self, *args, **kwargs) -> \'Flow\':\n        """"""Add a preprocessor to the current flow, a shortcut of :py:meth:`add(Service.Preprocessor)`""""""\n        return self.add(Service.Preprocessor, *args, **kwargs)\n\n    def add_router(self, *args, **kwargs) -> \'Flow\':\n        """"""Add a router to the current flow, a shortcut of :py:meth:`add(Service.Router)`""""""\n        return self.add(Service.Router, *args, **kwargs)\n\n    def set_last_service(self, name: str, copy_flow: bool = True) -> \'Flow\':\n        """"""\n        Set a service as the last service in the flow, useful when modifying the flow.\n\n        :param name: the name of the existing service\n        :param copy_flow: when set to true, then always copy the current flow and do the modification on top of it then return, otherwise, do in-line modification\n        :return: a (new) flow object with modification\n        """"""\n        op_flow = copy.deepcopy(self) if copy_flow else self\n\n        if name not in op_flow._service_nodes:\n            raise FlowMissingNode(\'recv_from: %s can not be found in this Flow\' % name)\n\n        if op_flow._last_changed_service and name == op_flow._last_changed_service[-1]:\n            pass\n        else:\n            op_flow._last_changed_service.append(name)\n\n        # graph is now changed so we need to\n        # reset the build level to the lowest\n        op_flow._build_level = BuildLevel.EMPTY\n\n        return op_flow\n\n    def set(self, name: str, recv_from: Union[str, Tuple[str], List[str], \'Service\'] = None,\n            send_to: Union[str, Tuple[str], List[str], \'Service\'] = None,\n            copy_flow: bool = True,\n            clear_old_attr: bool = False,\n            as_last_service: bool = False,\n            **kwargs) -> \'Flow\':\n        """"""\n        Set the attribute of an existing service (added by :py:meth:`add`) in the flow.\n        For the attributes or kwargs that aren\'t given, they will remain unchanged as before.\n\n        :param name: the name of the existing service\n        :param recv_from: the name of the service(s) that this service receives data from.\n                           One can also use \'Service.Frontend\' to indicate the connection with the frontend.\n        :param send_to:  the name of the service(s) that this service sends data to.\n                           One can also use \'Service.Frontend\' to indicate the connection with the frontend.\n        :param copy_flow: when set to true, then always copy the current flow and do the modification on top of it then return, otherwise, do in-line modification\n        :param clear_old_attr: remove old attribute value before setting the new one\n        :param as_last_service: whether setting the changed service as the last service in the graph\n        :param kwargs: other keyword-value arguments that the service CLI supports\n        :return: a (new) flow object with modification\n        """"""\n        op_flow = copy.deepcopy(self) if copy_flow else self\n\n        if name not in op_flow._service_nodes:\n            raise FlowMissingNode(\'recv_from: %s can not be found in this Flow\' % name)\n\n        node = op_flow._service_nodes[name]\n        service = node[\'service\']\n\n        if recv_from:\n            recv_from = op_flow._parse_service_endpoints(op_flow, name, recv_from, connect_to_last_service=True)\n\n            if clear_old_attr:\n                # remove all edges point to this service\n                for n in op_flow._service_nodes.values():\n                    if name in n[\'outgoings\']:\n                        n[\'outgoings\'].remove(name)\n                node[\'incomes\'] = recv_from\n            else:\n                node[\'incomes\'] = node[\'incomes\'].union(recv_from)\n\n            # add it the new edge back\n            for s in recv_from:\n                op_flow._service_nodes[s][\'outgoings\'].add(name)\n\n        if send_to:\n            send_to = op_flow._parse_service_endpoints(op_flow, name, send_to, connect_to_last_service=False)\n            if clear_old_attr:\n                # remove all edges this service point to\n                for n in op_flow._service_nodes.values():\n                    if name in n[\'incomes\']:\n                        n[\'incomes\'].remove(name)\n                node[\'outgoings\'] = send_to\n            else:\n                node[\'outgoings\'] = node[\'outgoings\'].union(send_to)\n\n            for s in send_to:\n                op_flow._service_nodes[s][\'incomes\'].add(name)\n\n        if kwargs:\n            if not clear_old_attr:\n                node[\'kwargs\'].update(kwargs)\n                kwargs = node[\'kwargs\']\n            args, p_args, unk_args = op_flow._get_parsed_args(op_flow, name, service_map[service][\'parser\'], kwargs)\n            node.update({\n                \'args\': args,\n                \'parsed_args\': p_args,\n                \'kwargs\': kwargs,\n                \'unk_args\': unk_args\n            })\n\n        if as_last_service:\n            op_flow.set_last_service(name, False)\n\n        # graph is now changed so we need to\n        # reset the build level to the lowest\n        op_flow._build_level = BuildLevel.EMPTY\n\n        return op_flow\n\n    def remove(self, name: str = None, copy_flow: bool = True) -> \'Flow\':\n        """"""\n        Remove a service from the flow.\n\n        :param name: the name of the existing service\n        :param copy_flow: when set to true, then always copy the current flow and do the modification on top of it then return, otherwise, do in-line modification\n        :return: a (new) flow object with modification\n        """"""\n\n        op_flow = copy.deepcopy(self) if copy_flow else self\n\n        if name not in op_flow._service_nodes:\n            raise FlowMissingNode(\'recv_from: %s can not be found in this Flow\' % name)\n\n        op_flow._service_nodes.pop(name)\n\n        # remove all edges point to this service\n        for n in op_flow._service_nodes.values():\n            if name in n[\'outgoings\']:\n                n[\'outgoings\'].remove(name)\n            if name in n[\'incomes\']:\n                n[\'incomes\'].remove(name)\n\n        if op_flow._service_nodes:\n            op_flow._last_changed_service = [v for v in op_flow._last_changed_service if v != name]\n        else:\n            op_flow._last_changed_service = []\n\n        # graph is now changed so we need to\n        # reset the build level to the lowest\n        op_flow._build_level = BuildLevel.EMPTY\n\n        return op_flow\n\n    def add(self, service: Union[\'Service\', str],\n            name: str = None,\n            recv_from: Union[str, Tuple[str], List[str], \'Service\'] = None,\n            send_to: Union[str, Tuple[str], List[str], \'Service\'] = None,\n            copy_flow: bool = True,\n            **kwargs) -> \'Flow\':\n        """"""\n        Add a service to the current flow object and return the new modified flow object.\n        The attribute of the service can be later changed with :py:meth:`set` or deleted with :py:meth:`remove`\n\n        Note there are shortcut versions of this method.\n        Recommend to use :py:meth:`add_encoder`, :py:meth:`add_preprocessor`,\n        :py:meth:`add_router`, :py:meth:`add_indexer` whenever possible.\n\n        :param service: a \'Service\' enum or string, possible choices: Encoder, Router, Preprocessor, Indexer, Frontend\n        :param name: the name identifier of the service, can be used in \'recv_from\',\n                    \'send_to\', :py:meth:`set` and :py:meth:`remove`.\n        :param recv_from: the name of the service(s) that this service receives data from.\n                           One can also use \'Service.Frontend\' to indicate the connection with the frontend.\n        :param send_to:  the name of the service(s) that this service sends data to.\n                           One can also use \'Service.Frontend\' to indicate the connection with the frontend.\n        :param copy_flow: when set to true, then always copy the current flow and do the modification on top of it then return, otherwise, do in-line modification\n        :param kwargs: other keyword-value arguments that the service CLI supports\n        :return: a (new) flow object with modification\n        """"""\n\n        op_flow = copy.deepcopy(self) if copy_flow else self\n\n        if isinstance(service, str):\n            service = Service.from_string(service)\n\n        if service not in service_map:\n            raise ValueError(\'service: %s is not supported, should be one of %s\' % (service, service_map.keys()))\n\n        if name in op_flow._service_nodes:\n            raise FlowTopologyError(\'name: %s is used in this Flow already!\' % name)\n        if not name:\n            name = \'%s%d\' % (service, op_flow._service_name_counter[service])\n            op_flow._service_name_counter[service] += 1\n        if not name.isidentifier():\n            raise ValueError(\'name: %s is invalid, please follow the python variable name conventions\' % name)\n\n        if service == Service.Frontend:\n            if op_flow._frontend:\n                raise FlowTopologyError(\'frontend is already in this Flow\')\n            op_flow._frontend = name\n\n        recv_from = op_flow._parse_service_endpoints(op_flow, name, recv_from, connect_to_last_service=True)\n        send_to = op_flow._parse_service_endpoints(op_flow, name, send_to, connect_to_last_service=False)\n\n        args, p_args, unk_args = op_flow._get_parsed_args(op_flow, name, service_map[service][\'parser\'], kwargs)\n\n        op_flow._service_nodes[name] = {\n            \'service\': service,\n            \'parsed_args\': p_args,\n            \'args\': args,\n            \'incomes\': recv_from,\n            \'outgoings\': send_to,\n            \'kwargs\': kwargs,\n            \'unk_args\': unk_args\n        }\n\n        # direct all income services\' output to the current service\n        for s in recv_from:\n            op_flow._service_nodes[s][\'outgoings\'].add(name)\n        for s in send_to:\n            op_flow._service_nodes[s][\'incomes\'].add(name)\n\n        op_flow.set_last_service(name, False)\n\n        # graph is now changed so we need to\n        # reset the build level to the lowest\n        op_flow._build_level = BuildLevel.EMPTY\n\n        return op_flow\n\n    @staticmethod\n    def _parse_service_endpoints(op_flow, cur_service_name, service_endpoint, connect_to_last_service=False,\n                                 check_name_exist=True):\n        # parsing recv_from\n        if isinstance(service_endpoint, str):\n            service_endpoint = [service_endpoint]\n        elif service_endpoint == Service.Frontend:\n            service_endpoint = [op_flow._frontend]\n        elif not service_endpoint:\n            if op_flow._last_changed_service and connect_to_last_service:\n                service_endpoint = [op_flow._last_changed_service[-1]]\n            else:\n                service_endpoint = []\n        if isinstance(service_endpoint, list) or isinstance(service_endpoint, tuple):\n            for s in service_endpoint:\n                if s == cur_service_name:\n                    raise FlowTopologyError(\'the income of a service can not be itself\')\n                if s not in op_flow._service_nodes:\n                    if check_name_exist:\n                        raise FlowMissingNode(\'recv_from: %s can not be found in this Flow\' % s)\n        else:\n            raise ValueError(\'recv_from=%s is not parsable\' % service_endpoint)\n        return set(service_endpoint)\n\n    @staticmethod\n    def _get_parsed_args(op_flow, name, service_arg_parser, kwargs):\n        kwargs.update(op_flow._common_kwargs)\n        args = []\n        for k, v in kwargs.items():\n            if isinstance(v, bool):\n                if v:\n                    if not k.startswith(\'no_\') and not k.startswith(\'no-\'):\n                        args.append(\'--%s\' % k)\n                    else:\n                        args.append(\'--%s\' % k[3:])\n                else:\n                    if k.startswith(\'no_\') or k.startswith(\'no-\'):\n                        args.append(\'--%s\' % k)\n                    else:\n                        args.append(\'--no_%s\' % k)\n            else:\n                args.extend([\'--%s\' % k, str(v)])\n        try:\n            p_args, unknown_args = service_arg_parser().parse_known_args(args)\n            if unknown_args:\n                op_flow.logger.warning(\'not sure what these arguments are: %s\' % unknown_args)\n        except SystemExit:\n            raise ValueError(\'bad arguments for service ""%s"", \'\n                             \'you may want to double check your args ""%s""\' % (name, args))\n        return args, p_args, unknown_args\n\n    def _build_graph(self, copy_flow: bool) -> \'Flow\':\n        op_flow = copy.deepcopy(self) if copy_flow else self\n\n        op_flow._service_edges.clear()\n\n        if not op_flow._frontend:\n            raise FlowIncompleteError(\'frontend does not exist, you may need to add_frontend()\')\n\n        if not op_flow._last_changed_service or not op_flow._service_nodes:\n            raise FlowTopologyError(\'flow is empty?\')\n\n        # close the loop\n        op_flow._service_nodes[op_flow._frontend][\'incomes\'] = {op_flow._last_changed_service[-1]}\n\n        # build all edges\n        for k, v in op_flow._service_nodes.items():\n            for s in v[\'incomes\']:\n                op_flow._service_edges[\'%s-%s\' % (s, k)] = \'\'\n            for t in v[\'outgoings\']:\n                op_flow._service_edges[\'%s-%s\' % (k, t)] = \'\'\n\n        for k in op_flow._service_edges.keys():\n            start_node, end_node = k.split(\'-\')\n            edges_with_same_start = [ed for ed in op_flow._service_edges.keys() if ed.startswith(start_node)]\n            edges_with_same_end = [ed for ed in op_flow._service_edges.keys() if ed.endswith(end_node)]\n\n            s_pargs = op_flow._service_nodes[start_node][\'parsed_args\']\n            e_pargs = op_flow._service_nodes[end_node][\'parsed_args\']\n\n            # Rule\n            # if a node has multiple income/outgoing services,\n            # then its socket_in/out must be PULL_BIND or PUB_BIND\n            # otherwise it should be different than its income\n            # i.e. income=BIND => this=CONNECT, income=CONNECT => this = BIND\n            #\n            # when a socket is BIND, then host must NOT be set, aka default host 0.0.0.0\n            # host_in and host_out is only set when corresponding socket is CONNECT\n\n            if len(edges_with_same_start) > 1 and len(edges_with_same_end) == 1:\n                s_pargs.socket_out = SocketType.PUB_BIND\n                s_pargs.host_out = BaseService.default_host\n                e_pargs.socket_in = SocketType.SUB_CONNECT\n                e_pargs.host_in = start_node\n                e_pargs.port_in = s_pargs.port_out\n                op_flow._service_edges[k] = \'PUB-sub\'\n            elif len(edges_with_same_end) > 1 and len(edges_with_same_start) == 1:\n                s_pargs.socket_out = SocketType.PUSH_CONNECT\n                s_pargs.host_out = end_node\n                e_pargs.socket_in = SocketType.PULL_BIND\n                e_pargs.host_in = BaseService.default_host\n                s_pargs.port_out = e_pargs.port_in\n                op_flow._service_edges[k] = \'push-PULL\'\n            elif len(edges_with_same_start) == 1 and len(edges_with_same_end) == 1:\n                # in this case, either side can be BIND\n                # we prefer frontend to be always BIND\n                # check if either node is frontend\n                if start_node == op_flow._frontend:\n                    s_pargs.socket_out = SocketType.PUSH_BIND\n                    e_pargs.socket_in = SocketType.PULL_CONNECT\n                elif end_node == op_flow._frontend:\n                    s_pargs.socket_out = SocketType.PUSH_CONNECT\n                    e_pargs.socket_in = SocketType.PULL_BIND\n                else:\n                    e_pargs.socket_in = s_pargs.socket_out.paired\n\n                if s_pargs.socket_out.is_bind:\n                    s_pargs.host_out = BaseService.default_host\n                    e_pargs.host_in = start_node\n                    e_pargs.port_in = s_pargs.port_out\n                    op_flow._service_edges[k] = \'PUSH-pull\'\n                elif e_pargs.socket_in.is_bind:\n                    s_pargs.host_out = end_node\n                    e_pargs.host_in = BaseService.default_host\n                    s_pargs.port_out = e_pargs.port_in\n                    op_flow._service_edges[k] = \'push-PULL\'\n                else:\n                    raise FlowTopologyError(\'edge %s -> %s is ambiguous, at least one socket should be BIND\')\n            else:\n                raise FlowTopologyError(\'found %d edges start with %s and %d edges end with %s, \'\n                                        \'this type of topology is ambiguous and should not exist, \'\n                                        \'i can not determine the socket type\' % (\n                                            len(edges_with_same_start), start_node, len(edges_with_same_end), end_node))\n\n        op_flow._build_level = BuildLevel.GRAPH\n        return op_flow\n\n    def build(self, backend: Optional[str] = \'process\', copy_flow: bool = False, *args, **kwargs) -> \'Flow\':\n        """"""\n        Build the current flow and make it ready to use\n\n        :param backend: supported \'thread\', \'process\', \'swarm\', \'k8s\', \'shell\', if None then only build graph only\n        :param copy_flow: return the copy of the current flow\n        :return: the current flow (by default)\n        """"""\n\n        op_flow = self._build_graph(copy_flow)\n\n        if not backend:\n            op_flow.logger.warning(\'no specified backend, build_level stays at %s, \'\n                                   \'and you can not run this flow.\' % op_flow._build_level)\n        elif backend in {\'thread\', \'process\'}:\n            op_flow._service_contexts.clear()\n            for v in op_flow._service_nodes.values():\n                p_args = v[\'parsed_args\']\n                p_args.parallel_backend = backend\n                # for thread and process backend which runs locally, host_in and host_out should not be set\n                p_args.host_in = BaseService.default_host\n                p_args.host_out = BaseService.default_host\n                op_flow._service_contexts.append((service_map[v[\'service\']][\'builder\'], p_args))\n            op_flow._build_level = BuildLevel.RUNTIME\n        else:\n            raise NotImplementedError(\'backend=%s is not supported yet\' % backend)\n\n        return op_flow\n\n    def __call__(self, *args, **kwargs):\n        return self.build(*args, **kwargs)\n\n    def __enter__(self):\n        if self._build_level.value < BuildLevel.RUNTIME.value:\n            self.logger.warning(\n                \'current build_level=%s, lower than required. \'\n                \'build the flow now via build() with default parameters\' % self._build_level)\n            self.build(copy_flow=False)\n        self._service_stack = ExitStack()\n        for k, v in self._service_contexts:\n            self._service_stack.enter_context(k(v))\n\n        self.logger.critical(\'flow is built and ready, current build level is %s\' % self._build_level)\n        return self\n\n    def close(self):\n        if hasattr(self, \'_service_stack\'):\n            self._service_stack.close()\n        self._build_level = BuildLevel.EMPTY\n        self.logger.critical(\n            \'flow is closed and all resources should be released already, current build level is %s\' % self._build_level)\n\n    def __eq__(self, other):\n        """"""\n        Comparing the topology of a flow with another flow.\n        Identification is defined by whether two flows share the same set of edges.\n\n        :param other: the second flow object\n        :return:\n        """"""\n\n        if self._build_level.value < BuildLevel.GRAPH.value:\n            a = self.build(backend=None, copy_flow=True)\n        else:\n            a = self\n\n        if other._build_level.value < BuildLevel.GRAPH.value:\n            b = other.build(backend=None, copy_flow=True)\n        else:\n            b = other\n\n        return a._service_edges == b._service_edges\n'"
gnes/flow/base.py,0,"b'from . import Flow\n\n\nclass BaseIndexFlow(Flow):\n    """"""\n    BaseIndexFlow defines a common service pipeline when indexing.\n\n    It can not be directly used as all services are using the base module by default.\n    You have to use :py:meth:`set` to change the `yaml_path` of each service.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        (self.add_preprocessor(name=\'prep\', yaml_path=\'BasePreprocessor\', copy_flow=False)\n         .add_encoder(name=\'enc\', yaml_path=\'BaseEncoder\', copy_flow=False)\n         .add_indexer(name=\'vec_idx\', yaml_path=\'BaseIndexer\', copy_flow=False)\n         .add_indexer(name=\'doc_idx\', yaml_path=\'BaseIndexer\', recv_from=\'prep\', copy_flow=False)\n         .add_router(name=\'sync\', yaml_path=\'BaseReduceRouter\',\n                     num_part=2, recv_from=[\'vec_idx\', \'doc_idx\'], copy_flow=False))\n\n\nclass BaseQueryFlow(Flow):\n    """"""\n    BaseIndexFlow defines a common service pipeline when indexing.\n\n    It can not be directly used as all services are using the base module by default.\n    You have to use :py:meth:`set` to change the `yaml_path` of each service.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        (self.add_preprocessor(name=\'prep\', yaml_path=\'BasePreprocessor\', copy_flow=False)\n         .add_encoder(name=\'enc\', yaml_path=\'BaseEncoder\', copy_flow=False)\n         .add_indexer(name=\'vec_idx\', yaml_path=\'BaseIndexer\', copy_flow=False)\n         .add_router(name=\'scorer\', yaml_path=\'Chunk2DocTopkReducer\', copy_flow=False)\n         .add_indexer(name=\'doc_idx\', yaml_path=\'BaseIndexer\', copy_flow=False))\n'"
gnes/flow/helper.py,0,"b'from functools import wraps\n\nfrom ..cli.parser import set_router_parser, set_indexer_parser, \\\n    set_frontend_parser, set_preprocessor_parser, \\\n    set_encoder_parser\nfrom ..service.base import BetterEnum, ServiceManager\nfrom ..service.encoder import EncoderService\nfrom ..service.frontend import FrontendService\nfrom ..service.indexer import IndexerService\nfrom ..service.preprocessor import PreprocessorService\nfrom ..service.router import RouterService\n\n\nclass BuildLevel(BetterEnum):\n    EMPTY = 0\n    GRAPH = 1\n    RUNTIME = 2\n\n\nclass Service(BetterEnum):\n    Frontend = 0\n    Encoder = 1\n    Router = 2\n    Indexer = 3\n    Preprocessor = 4\n\n\nclass FlowIncompleteError(ValueError):\n    """"""Exception when the flow missing some important component to run""""""\n\n\nclass FlowTopologyError(ValueError):\n    """"""Exception when the topology is ambiguous""""""\n\n\nclass FlowMissingNode(ValueError):\n    """"""Exception when the topology is ambiguous""""""\n\n\nclass FlowBuildLevelMismatch(ValueError):\n    """"""Exception when required level is higher than the current build level""""""\n\n\ndef build_required(required_level: \'BuildLevel\'):\n    def __build_level(func):\n        @wraps(func)\n        def arg_wrapper(self, *args, **kwargs):\n            if hasattr(self, \'_build_level\'):\n                if self._build_level.value >= required_level.value:\n                    return func(self, *args, **kwargs)\n                else:\n                    raise FlowBuildLevelMismatch(\n                        \'build_level check failed for %r, required level: %s, actual level: %s\' % (\n                            func, required_level, self._build_level))\n            else:\n                raise AttributeError(\'%r has no attribute ""_build_level""\' % self)\n\n        return arg_wrapper\n\n    return __build_level\n\n\nservice_map = {\n    Service.Encoder: {\n        \'parser\': set_encoder_parser,\n        \'builder\': lambda x: ServiceManager(EncoderService, x),\n        \'cmd\': \'encode\'},\n    Service.Router: {\n        \'parser\': set_router_parser,\n        \'builder\': lambda x: ServiceManager(RouterService, x),\n        \'cmd\': \'route\',\n    },\n    Service.Indexer: {\n        \'parser\': set_indexer_parser,\n        \'builder\': lambda x: ServiceManager(IndexerService, x),\n        \'cmd\': \'index\'\n    },\n    Service.Frontend: {\n        \'parser\': set_frontend_parser,\n        \'builder\': FrontendService,\n        \'cmd\': \'frontend\'\n    },\n    Service.Preprocessor: {\n        \'parser\': set_preprocessor_parser,\n        \'builder\': lambda x: ServiceManager(PreprocessorService, x),\n        \'cmd\': \'preprocess\'\n    }\n}\n'"
gnes/indexer/__init__.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\n# A key-value map for Class to the (module)file it located in\nfrom ..base import register_all_class\n\n_cls2file_map = {\n    \'FaissIndexer\': \'chunk.faiss\',\n    \'LVDBIndexer\': \'doc.leveldb\',\n    \'RocksDBIndexer\': \'doc.rocksdb\',\n    \'AsyncLVDBIndexer\': \'doc.leveldb\',\n    \'NumpyIndexer\': \'chunk.numpy\',\n    \'BIndexer\': \'chunk.bindexer\',\n    \'HBIndexer\': \'chunk.hbindexer\',\n    \'JointIndexer\': \'base\',\n    \'BaseIndexer\': \'base\',\n    \'BaseDocIndexer\': \'base\',\n    \'AnnoyIndexer\': \'chunk.annoy\',\n    \'DirectoryIndexer\': \'doc.filesys\',\n    \'DictIndexer\': \'doc.dict\',\n    \'DictKeyIndexer\': \'chunk.helper\',\n    \'ListKeyIndexer\': \'chunk.helper\',\n    \'ListNumpyKeyIndexer\': \'chunk.helper\',\n    \'NumpyKeyIndexer\': \'chunk.helper\',\n}\n\nregister_all_class(_cls2file_map, \'indexer\')\n'"
gnes/indexer/base.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\nfrom collections import defaultdict\nfrom functools import wraps\nfrom typing import List, Any, Union, Callable, Tuple\n\nimport numpy as np\n\nfrom ..base import TrainableBase, CompositionalTrainableBase\nfrom ..proto import gnes_pb2, blob2array\nfrom ..score_fn.base import get_unary_score, ModifierScoreFn\n\n\nclass BaseIndexer(TrainableBase):\n    def __init__(self,\n                 normalize_fn: \'BaseScoreFn\' = None,\n                 score_fn: \'BaseScoreFn\' = None,\n                 is_big_score_similar: bool = False,\n                 *args, **kwargs):\n        """"""\n        Base indexer, a valid indexer must implement :py:meth:`add` and :py:meth:`query` methods\n\n        :type score_fn: advanced score function\n        :type normalize_fn: normalizing score function\n        :type is_big_score_similar: when set to true, then larger score means more similar\n        """"""\n        super().__init__(*args, **kwargs)\n        self.normalize_fn = normalize_fn if normalize_fn else ModifierScoreFn()\n        self.score_fn = score_fn if score_fn else ModifierScoreFn()\n        self.normalize_fn._context = self\n        self.score_fn._context = self\n        self.is_big_score_similar = is_big_score_similar\n        self._num_docs = 0\n        self._num_chunks = 0\n        self._num_chunks_in_doc = defaultdict(int)\n\n    def add(self, keys: Any, docs: Any, weights: List[float], *args, **kwargs):\n        pass\n\n    def query(self, keys: Any, *args, **kwargs) -> List[Any]:\n        pass\n\n    def query_and_score(self, q_chunks: List[Union[\'gnes_pb2.Chunk\', \'gnes_pb2.Document\']], top_k: int) -> List[\n        \'gnes_pb2.Response.QueryResponse.ScoredResult\']:\n        raise NotImplementedError\n\n    @property\n    def num_docs(self):\n        return self._num_docs\n\n    @property\n    def num_chunks(self):\n        return self._num_chunks\n\n\nclass BaseChunkIndexer(BaseIndexer):\n    """"""Storing chunks and their vector representations """"""\n\n    def __init__(self, helper_indexer: \'BaseChunkIndexerHelper\' = None, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.helper_indexer = helper_indexer\n\n    def add(self, keys: List[Tuple[int, int]], vectors: np.ndarray, weights: List[float], *args, **kwargs):\n        """"""\n        adding new chunks and their vector representations\n\n        :param keys: list of (doc_id, offset) tuple\n        :param vectors: vector representations\n        :param weights: weight of the chunks\n        """"""\n        pass\n\n    def query(self, keys: np.ndarray, top_k: int, *args, **kwargs) -> List[List[Tuple]]:\n        pass\n\n    def query_and_score(self, q_chunks: List[\'gnes_pb2.Chunk\'], top_k: int, *args, **kwargs) -> List[\n        \'gnes_pb2.Response.QueryResponse.ScoredResult\']:\n        vecs = [blob2array(c.embedding) for c in q_chunks]\n        queried_results = self.query(np.stack(vecs), top_k=top_k)\n        results = []\n        for q_chunk, topk_chunks in zip(q_chunks, queried_results):\n            for _doc_id, _offset, _weight, _relevance in topk_chunks:\n                r = gnes_pb2.Response.QueryResponse.ScoredResult()\n                r.chunk.doc_id = _doc_id\n                r.chunk.offset = _offset\n                r.chunk.weight = _weight\n                _score = get_unary_score(value=_relevance,\n                                         name=self.__class__.__name__,\n                                         operands=[\n                                             dict(name=\'doc_chunk\',\n                                                  doc_id=_doc_id,\n                                                  offset=_offset),\n                                             dict(name=\'query_chunk\',\n                                                  offset=q_chunk.offset)])\n                _score = self.normalize_fn(_score)\n                _score = self.score_fn(_score, q_chunk, r.chunk, queried_results)\n                r.score.CopyFrom(_score)\n                results.append(r)\n        return results\n\n    @staticmethod\n    def update_counter(func):\n        @wraps(func)\n        def arg_wrapper(self, keys: List[Tuple[int, int]], *args, **kwargs):\n            doc_ids, _ = zip(*keys)\n            self._num_docs += len(set(doc_ids))\n            self._num_chunks += len(keys)\n            for doc_id in doc_ids:\n                self._num_chunks_in_doc[doc_id] += 1\n            return func(self, keys, *args, **kwargs)\n\n        return arg_wrapper\n\n    @staticmethod\n    def update_helper_indexer(func):\n        @wraps(func)\n        def arg_wrapper(self, keys: List[Tuple[int, int]], vectors: np.ndarray, weights: List[float], *args, **kwargs):\n            r = func(self, keys, vectors, weights, *args, **kwargs)\n            if self.helper_indexer:\n                self.helper_indexer.add(keys, weights, *args, **kwargs)\n            return r\n\n        return arg_wrapper\n\n    @property\n    def num_docs(self):\n        if self.helper_indexer:\n            return self.helper_indexer._num_docs\n        else:\n            return self._num_docs\n\n    @property\n    def num_chunks(self):\n        if self.helper_indexer:\n            return self.helper_indexer._num_chunks\n        else:\n            return self._num_chunks\n\n    def num_chunks_in_doc(self, doc_id: int):\n        if self.helper_indexer:\n            return self.helper_indexer._num_chunks_in_doc[doc_id]\n        else:\n            self.logger.warning(\'enable helper_indexer to track num_chunks_in_doc\')\n\n\nclass BaseDocIndexer(BaseIndexer):\n    """"""Storing documents and contents """"""\n\n    def add(self, keys: List[int], docs: List[\'gnes_pb2.Document\'], *args, **kwargs):\n        """"""\n        adding new docs and their protobuf representation\n\n        :param keys: list of doc_id\n        :param docs: list of protobuf Document objects\n        """"""\n        pass\n\n    def query(self, keys: List[int], *args, **kwargs) -> List[\'gnes_pb2.Document\']:\n        pass\n\n    def query_and_score(self, docs: List[\'gnes_pb2.Response.QueryResponse.ScoredResult\'], *args, **kwargs) -> List[\n        \'gnes_pb2.Response.QueryResponse.ScoredResult\']:\n        keys = [r.doc.doc_id for r in docs]\n        results = []\n        queried_results = self.query(keys, *args, **kwargs)\n        for d, r in zip(queried_results, docs):\n            if d:\n                r.doc.CopyFrom(d)\n                _score = self.normalize_fn(r.score)\n                _score = self.score_fn(_score, d)\n                r.score.CopyFrom(_score)\n            results.append(r)\n        return results\n\n    @staticmethod\n    def update_counter(func):\n        @wraps(func)\n        def arg_wrapper(self, keys: List[int], docs: List[\'gnes_pb2.Document\'], *args, **kwargs):\n            self._num_docs += len(keys)\n            self._num_chunks += sum(len(d.chunks) for d in docs)\n            return func(self, keys, docs, *args, **kwargs)\n\n        return arg_wrapper\n\n\nclass BaseChunkIndexerHelper(BaseChunkIndexer):\n    """"""A helper class for storing chunk info, doc mapping, weights.\n    This is especially useful when ChunkIndexer can not store these information by itself\n    """"""\n\n    def add(self, keys: List[Tuple[int, int]], weights: List[float], *args, **kwargs) -> int:\n        pass\n\n    def query(self, keys: List[int], *args, **kwargs) -> List[Tuple[int, int, float]]:\n        pass\n\n\nclass JointIndexer(CompositionalTrainableBase):\n\n    @property\n    def components(self):\n        return self._component\n\n    @components.setter\n    def components(self, comps: Callable[[], Union[list, dict]]):\n        if not callable(comps):\n            raise TypeError(\'component must be a callable function that returns \'\n                            \'a List[BaseIndexer]\')\n        if not getattr(self, \'init_from_yaml\', False):\n            self._component = comps()\n        else:\n            self.logger.info(\'component is omitted from construction, \'\n                             \'as it is initialized from yaml config\')\n\n        self._binary_indexer = None\n        self._doc_indexer = None\n        for c in self.components:\n            if isinstance(c, BaseChunkIndexer):\n                self._binary_indexer = c\n            elif isinstance(c, BaseDocIndexer):\n                self._doc_indexer = c\n        if not self._binary_indexer or not self._doc_indexer:\n            raise ValueError(\'""JointIndexer"" requires a valid pair of ""BaseBinaryIndexer"" and ""BaseTextIndexer""\')\n\n    def add(self, keys: Any, docs: Any, *args,\n            **kwargs) -> None:\n        if isinstance(docs, np.ndarray):\n            self._binary_indexer.add(keys, docs, *args, **kwargs)\n        elif isinstance(docs, list):\n            self._doc_indexer.add(keys, docs, *args, **kwargs)\n        else:\n            raise TypeError(\'can not find an indexer for doc type: %s\' % type(docs))\n\n    def query(self,\n              keys: Any,\n              top_k: int,\n              *args,\n              **kwargs) -> List[List[Tuple]]:\n        topk_results = self._binary_indexer.query(keys, top_k, *args, **kwargs)\n        doc_caches = dict()\n        topk_results_with_docs = []\n        for topk in topk_results:\n            topk_wd = []\n            for doc_id, offset, weight, score in topk:\n                doc = doc_caches.get(doc_id, self._doc_indexer.query([doc_id])[0])\n                doc_caches[doc_id] = doc\n                topk_wd.append((doc_id, offset, weight, score, doc.chunks[offset]))\n            topk_results_with_docs.append(topk_wd)\n        return topk_results_with_docs\n'"
gnes/preprocessor/__init__.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\n# A key-value map for Class to the (module)file it located in\nfrom ..base import register_all_class\n\n_cls2file_map = {\n    \'BasePreprocessor\': \'base\',\n    \'PipelinePreprocessor\': \'base\',\n    \'SentSplitPreprocessor\': \'text.split\',\n    \'BaseImagePreprocessor\': \'base\',\n    \'BaseTextPreprocessor\': \'base\',\n    \'VanillaSlidingPreprocessor\': \'image.sliding_window\',\n    \'WeightedSlidingPreprocessor\': \'image.sliding_window\',\n    \'SegmentPreprocessor\': \'image.segmentation\',\n    \'UnaryPreprocessor\': \'base\',\n    \'ResizeChunkPreprocessor\': \'image.resize\',\n    \'BaseVideoPreprocessor\': \'base\',\n    \'FFmpegPreprocessor\': \'video.ffmpeg\',\n    \'FFmpegVideoSegmentor\': \'video.ffmpeg\',\n    \'ShotDetectorPreprocessor\': \'video.shot_detector\',\n    \'VideoEncoderPreprocessor\': \'video.video_encoder\',\n    \'VideoDecoderPreprocessor\': \'video.video_decoder\',\n    \'AudioVanilla\': \'audio.audio_vanilla\',\n    \'BaseAudioPreprocessor\': \'base\',\n    \'RawChunkPreprocessor\': \'base\',\n    \'GifChunkPreprocessor\': \'video.ffmpeg\',\n    \'VggishPreprocessor\': \'audio.vggish_example\',\n    \'FrameSelectPreprocessor\': \'video.frame_select\'\n}\n\nregister_all_class(_cls2file_map, \'preprocessor\')\n'"
gnes/preprocessor/base.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nimport io\n\nimport numpy as np\n\nfrom ..base import TrainableBase, CompositionalTrainableBase\nfrom ..proto import gnes_pb2, array2blob\n\n\nclass BasePreprocessor(TrainableBase):\n    doc_type = gnes_pb2.Document.UNKNOWN\n\n    def __init__(self,\n                 uniform_doc_weight: bool = True,\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self.uniform_doc_weight = uniform_doc_weight\n\n    def apply(self, doc: \'gnes_pb2.Document\') -> None:\n        doc.doc_type = self.doc_type\n        if not doc.weight and self.uniform_doc_weight:\n            doc.weight = 1.0\n\n\nclass BaseTextPreprocessor(BasePreprocessor):\n    doc_type = gnes_pb2.Document.TEXT\n\n\nclass BaseAudioPreprocessor(BasePreprocessor):\n    doc_type = gnes_pb2.Document.AUDIO\n\n\nclass BaseImagePreprocessor(BasePreprocessor):\n    doc_type = gnes_pb2.Document.IMAGE\n\n\nclass BaseVideoPreprocessor(BasePreprocessor):\n    doc_type = gnes_pb2.Document.VIDEO\n\n\nclass PipelinePreprocessor(CompositionalTrainableBase):\n    def apply(self, doc: \'gnes_pb2.Document\') -> None:\n        if not self.components:\n            raise NotImplementedError\n        for be in self.components:\n            be.apply(doc)\n\n    def train(self, data, *args, **kwargs):\n        if not self.components:\n            raise NotImplementedError\n        for idx, be in enumerate(self.components):\n            be.train(data, *args, **kwargs)\n            if idx + 1 < len(self.components):\n                data = be.apply(data, *args, **kwargs)\n\n\nclass UnaryPreprocessor(BasePreprocessor):\n    is_trained = True\n\n    def __init__(self, doc_type: int, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.doc_type = doc_type\n\n    def apply(self, doc: \'gnes_pb2.Document\'):\n        super().apply(doc)\n        c = doc.chunks.add()\n        c.doc_id = doc.doc_id\n        c.offset = 0\n        c.weight = 1.\n        if doc.raw_bytes:\n            self.raw_to_chunk(c, doc.raw_bytes)\n        else:\n            self.logger.error(\'bad document: ""raw_bytes"" is empty!\')\n\n    def raw_to_chunk(self, chunk: \'gnes_pb2.Chunk\', raw_bytes: bytes):\n        if self.doc_type == gnes_pb2.Document.TEXT:\n            chunk.text = raw_bytes.decode()\n        elif self.doc_type == gnes_pb2.Document.IMAGE:\n            from PIL import Image\n            img = np.array(Image.open(io.BytesIO(raw_bytes)))\n            chunk.blob.CopyFrom(array2blob(img))\n        elif self.doc_type == gnes_pb2.Document.VIDEO:\n            raise NotImplementedError\n        else:\n            raise NotImplementedError\n\n\nclass RawChunkPreprocessor(BasePreprocessor):\n    \n    @staticmethod\n    def _parse_chunk(chunk: \'gnes_pb2.Chunk\', *args, **kwargs):\n        raise NotImplementedError\n\n    def apply(self, doc: \'gnes_pb2.Document\') -> None:\n        if doc.raw_bytes:\n            for chunk in doc.chunks:\n                chunk.raw = self._parse_chunk(chunk)\n        else:\n            self.logger.error(\'bad document: ""raw_bytes"" is empty!\')\n\n'"
gnes/preprocessor/helper.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the \'License\');\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \'AS IS\' BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nimport datetime\nimport io\nimport os\nimport subprocess as sp\nfrom datetime import timedelta\nfrom itertools import product\nfrom typing import List, Callable\n\nimport numpy as np\n\nfrom ..helper import set_logger\n\nlogger = set_logger(__name__, True)\n\n\ndef get_video_length(video_path):\n    import re\n    process = sp.Popen([\'ffmpeg\', \'-i\', video_path],\n                       stdout=sp.PIPE,\n                       stderr=sp.STDOUT)\n    stdout, _ = process.communicate()\n    stdout = str(stdout)\n    matches = re.search(r""Duration:\\s{1}(?P<hours>\\d+?):(?P<minutes>\\d+?):(?P<seconds>\\d+\\.\\d+?),"", stdout,\n                        re.DOTALL).groupdict()\n    h = float(matches[\'hours\'])\n    m = float(matches[\'minutes\'])\n    s = float(matches[\'seconds\'])\n\n    return 3600 * h + 60 * m + s\n\n\ndef get_video_length_from_raw(buffer_data):\n    import re\n    ffmpeg_cmd = [\'ffmpeg\', \'-i\', \'-\', \'-\']\n    with sp.Popen(ffmpeg_cmd, stdin=sp.PIPE, stdout=sp.PIPE, stderr=sp.PIPE,\n                  bufsize=-1, shell=False) as pipe:\n        _, stdout = pipe.communicate(buffer_data)\n        stdout = stdout.decode()\n        matches = re.search(r""Duration:\\s{1}(?P<hours>\\d+?):(?P<minutes>\\d+?):(?P<seconds>\\d+\\.\\d+?),"", stdout,\n                            re.DOTALL).groupdict()\n        h = str(int(matches[\'hours\']))\n        m = str(int(matches[\'minutes\']))\n        s = str(round(float(matches[\'seconds\'])))\n        duration = datetime.datetime.strptime(h + \':\' + m + \':\' + s, \'%H:%M:%S\')\n    return duration\n\n\ndef get_audio(buffer_data, sample_rate, interval,\n              duration) -> List[\'np.ndarray\']:\n    import soundfile as sf\n\n    audio_list = []\n    start_time = datetime.datetime.strptime(\'00:00:00\', \'%H:%M:%S\')\n    while True:\n        if start_time == duration:\n            break\n\n        end_time = start_time + timedelta(seconds=interval)\n        if end_time > duration:\n            end_time = duration\n        ffmpeg_cmd = [\'ffmpeg\', \'-i\', \'-\',\n                      \'-f\', \'wav\',\n                      \'-ar\', str(sample_rate),\n                      \'-ss\', str(start_time).split(\' \')[1],\n                      \'-to\', str(end_time).split(\' \')[1],\n                      \'-\']\n\n        # (-f, wav) output bytes in wav format\n        # (-ar) sample rate\n        # (-) output to stdout pipeline\n\n        with sp.Popen(\n                ffmpeg_cmd, stdin=sp.PIPE, stdout=sp.PIPE, bufsize=-1,\n                shell=False) as pipe:\n            raw_audio, _ = pipe.communicate(buffer_data)\n            tmp_stream = io.BytesIO(raw_audio)\n            data, sample_rate = sf.read(tmp_stream)\n            # has multiple channels, do average\n            if len(data.shape) == 2:\n                data = np.mean(data, axis=1)\n            if data.shape[0] != 0:\n                audio_list.append(data)\n\n        start_time = end_time\n\n    return audio_list\n\n\ndef split_mp4_random(video_path, avg_length, max_clip_second=10):\n    import random\n    l = get_video_length(video_path)\n    s = []\n    num_part = max(int(l / avg_length), 2)\n\n    while sum(s) < l:\n        s.append(random.randint(3, max_clip_second))\n    s[-1] = int(l - sum(s[:-1]))\n    start = [sum(s[:i]) for i in range(len(s))]\n\n    ts_group = [[] for _ in range(num_part)]\n\n    for i, (_start, _du) in enumerate(zip(start, s)):\n        ts_group[i % num_part].append(\' -ss {} -t {} -i {} \'.format(_start, _du, video_path))\n\n    prefix = os.path.basename(video_path).replace(\'.mp4\', \'\')\n    for i in range(num_part):\n        i_len = len(ts_group[i])\n        cmd = \'ffmpeg\' + \'\'.join(\n            ts_group[i]) + \'-filter_complex ""{}concat=n={}:v=1:a=1"" -strict -2 {}_{}.mp4 -y\'.format(\n            \'\'.join([\'[{}]\'.format(k) for k in range(i_len)]), i_len, prefix, i)\n        os.system(cmd)\n\n\ndef split_video_frames(buffer_data: bytes,\n                       splitter: str = \'__split__\'):\n    from PIL import Image\n\n    chunks = buffer_data.split(splitter.encode())\n    return [np.array(Image.open(io.BytesIO(chunk))) for chunk in chunks]\n\n\ndef get_gif(images: \'np.ndarray\', fps=10):\n    cmd = [\'ffmpeg\', \'-y\',\n           \'-f\', \'rawvideo\',\n           \'-vcodec\', \'rawvideo\',\n           \'-r\', \'%.02f\' % fps,\n           \'-s\', \'%dx%d\' % (images[0].shape[1], images[0].shape[0]),\n           \'-pix_fmt\', \'rgb24\',\n           \'-i\', \'-\',\n           \'-filter_complex\', \'[0:v]split[x][z];[z]palettegen[y];[x]fifo[x];[x][y]paletteuse\',\n           \'-r\', \'%.02f\' % fps,\n           \'-f\', \'gif\',\n           \'-\']\n    with sp.Popen(cmd, stdin=sp.PIPE, stdout=sp.PIPE, stderr=sp.PIPE, bufsize=-1, shell=False) as pipe:\n        for image in images:\n            pipe.stdin.write(image.tostring())\n        out, _ = pipe.communicate()\n    return out\n\n\ndef block_descriptor(image: \'np.ndarray\',\n                     descriptor_fn: Callable,\n                     num_blocks: int = 3) -> \'np.ndarray\':\n    h, w, _ = image.shape  # find shape of image and channel\n    block_h = int(np.ceil(h / num_blocks))\n    block_w = int(np.ceil(w / num_blocks))\n\n    descriptors = []\n    for i in range(0, h, block_h):\n        for j in range(0, w, block_w):\n            block = image[i:i + block_h, j:j + block_w]\n            descriptors.extend(descriptor_fn(block))\n\n    return np.array(descriptors)\n\n\ndef pyramid_descriptor(image: \'np.ndarray\',\n                       descriptor_fn: Callable,\n                       max_level: int = 2) -> \'np.ndarray\':\n    descriptors = []\n    for level in range(max_level + 1):\n        num_blocks = 2 ** level\n        descriptors.extend(block_descriptor(image, descriptor_fn, num_blocks))\n\n    return np.array(descriptors)\n\n\ndef rgb_histogram(image: \'np.ndarray\') -> \'np.ndarray\':\n    import cv2\n\n    _, _, c = image.shape\n    hist = [\n        cv2.calcHist([image], [i], None, [256], [0, 256]) for i in range(c)\n    ]\n    # normalize hist\n    hist = np.array([h / np.sum(h) for h in hist]).flatten()\n    return hist\n\n\ndef hsv_histogram(image: \'np.ndarray\') -> \'np.ndarray\':\n    import cv2\n\n    _, _, c = image.shape\n    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n\n    # sizes = [180, 256, 256]\n    # ranges = [(0, 180), (0, 256), (0, 256)]\n\n    # hist = [\n    #     cv2.calcHist([hsv], [i], None, [sizes[i]], ranges[i]) for i in range(c)\n    # ]\n\n    hist = [cv2.calcHist([hsv], [i], None, [256], [0, 256]) for i in range(c)]\n    # normalize hist\n    hist = np.array([h / np.sum(h) for h in hist]).flatten()\n    return hist\n\n\ndef canny_edge(image: \'np.ndarray\', **kwargs) -> \'np.ndarray\':\n    import cv2\n\n    sigma = kwargs.get(\'sigma\', 0.5)\n    gauss_kernel = kwargs.get(\'gauss_kernel\', (9, 9))\n    l2_gradient = kwargs.get(\'l2_gradient\', True)\n\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    # apply automatic Canny edge detection using the computed median\n    v = np.median(image)\n    low_threshold = ((1.0 - sigma) * v).astype(""float32"")\n    high_threshold = ((1.0 + sigma) * v).astype(""float32"")\n    tmp_image = cv2.GaussianBlur(image, gauss_kernel, 1.2)\n    edge_image = cv2.Canny(tmp_image, low_threshold, high_threshold, L2gradient=l2_gradient)\n    return edge_image\n\n\ndef phash_descriptor(image: \'np.ndarray\'):\n    from PIL import Image\n    import imagehash\n\n    image = Image.fromarray(image)\n    return imagehash.phash(image)\n\n\ndef compute_descriptor(image: \'np.ndarray\',\n                       method: str = \'rgb_histogram\',\n                       **kwargs) -> \'np.array\':\n    funcs = {\n        \'rgb_histogram\': rgb_histogram,\n        \'hsv_histogram\': hsv_histogram,\n        \'canny_edge\': lambda image: canny_edge(image, **kwargs),\n        \'block_rgb_histogram\': lambda image: block_descriptor(image, rgb_histogram, kwargs.get(\'num_blocks\', 3)),\n        \'block_hsv_histogram\': lambda image: block_descriptor(image, hsv_histogram, kwargs.get(\'num_blocks\', 3)),\n        \'pyramid_rgb_histogram\': lambda image: pyramid_descriptor(image, rgb_histogram, kwargs.get(\'max_level\', 2)),\n        \'pyramid_hsv_histogram\': lambda image: pyramid_descriptor(image, hsv_histogram, kwargs.get(\'max_level\', 2)),\n    }\n    return funcs[method](image)\n\n\ndef compare_ecr(descriptors: List[\'np.ndarray\'], **kwargs) -> List[float]:\n    import cv2\n\n    """""" Apply the Edge Change Ratio Algorithm""""""\n    dilate_rate = kwargs.get(\'dilate_rate\', 5)\n    neigh_avg = kwargs.get(\'neigh_avg\', 2)\n    divd = lambda x, y: 0 if y == 0 else x / y\n\n    dicts = []\n    inv_dilate = []\n    sum_disc = []\n    for descriptor in descriptors:\n        sum_disc.append(np.sum(descriptor))\n        inv_dilate.append(255 - cv2.dilate(descriptor, np.ones((dilate_rate, dilate_rate))))\n\n    for i in range(1, len(descriptors)):\n        dict_0 = divd(float(np.sum(descriptors[i - 1] & inv_dilate[i])), float(sum_disc[i - 1]))\n        dict_1 = divd(float(np.sum(descriptors[i] & inv_dilate[i - 1])), float(sum_disc[i]))\n        tmp_dict = max(dict_0, dict_1)\n        if i > 10:\n            dict_0 = divd(float(np.sum(descriptors[i - 10] & inv_dilate[i])), float(sum_disc[i - 10]))\n            dict_1 = divd(float(np.sum(descriptors[i] & inv_dilate[i - 10])), float(sum_disc[i]))\n            tmp_dict *= (1 + max(dict_0, dict_1))\n        dicts.append(tmp_dict)\n\n    for _ in range(neigh_avg):\n        tmp_dict = []\n        for i in range(1, len(dicts) - 1):\n            tmp_dict.append(max(dicts[i - 1], dicts[i], dicts[i + 1]))\n        dicts = tmp_dict.copy()\n\n    return dicts\n\n\ndef compare_descriptor(descriptor1: \'np.ndarray\',\n                       descriptor2: \'np.ndarray\',\n                       metric: str = \'chisqr\') -> float:\n    import cv2\n\n    dist_metric = {\n        \'correlation\': cv2.HISTCMP_CORREL,\n        \'chisqr\': cv2.HISTCMP_CHISQR,\n        \'chisqr_alt\': cv2.HISTCMP_CHISQR_ALT,\n        \'intersection\': cv2.HISTCMP_INTERSECT,\n        \'bhattacharya\': cv2.HISTCMP_BHATTACHARYYA,\n        \'hellinguer\': cv2.HISTCMP_HELLINGER,\n        \'kl_div\': cv2.HISTCMP_KL_DIV\n    }\n\n    return cv2.compareHist(descriptor1, descriptor2, dist_metric[metric])\n\n\ndef kmeans_algo(distances: List[float], **kwargs) -> List[int]:\n    from sklearn.cluster import KMeans\n    clt = KMeans(n_clusters=2)\n    clt.fit(distances)\n\n    num_frames = len(distances) + 1\n    # select which cluster includes shot frames\n    big_center = np.argmax(clt.cluster_centers_)\n\n    shots = []\n    shots.append(0)\n    for i in range(0, len(clt.labels_)):\n        if big_center == clt.labels_[i]:\n            shots.append((i + 1))\n    if shots[-1] < num_frames:\n        shots.append(num_frames)\n    else:\n        shots[-1] = num_frames\n    return shots\n\n\ndef check_motion(prev_dists: List[float], cur_dist: float, motion_threshold: float = 0.75):\n    """""" Returns a boolean value to decide if the peak is due to a motion""""""\n    close_peaks = 0\n    # We observe the a defined number of frames before the peak\n    for dist in prev_dists:\n        if dist > cur_dist * motion_threshold:\n            close_peaks += 1\n    if close_peaks >= len(prev_dists) / 2:\n        return True\n    else:\n        return False\n\n\ndef thre_algo(distances: List[float], **kwargs) -> List[int]:\n    # now threshold algo not support motion\n    kwargs[\'motion_step\'] = 0\n    return motion_algo(distances, **kwargs)\n\n\ndef motion_algo(distances: List[float], **kwargs) -> List[int]:\n    import peakutils\n\n    threshold = kwargs.get(\'threshold\', 0.6)\n    min_dist = kwargs.get(\'min_dist\', 10)\n    motion_step = kwargs.get(\'motion_step\', 15)\n    neigh_avg = kwargs.get(\'neigh_avg\', 2)\n    max_shot_num = kwargs.get(\'max_shot_num\', 30) - 1\n\n    shots = []\n    num_frames = len(distances) + 2 * neigh_avg + 1\n    p = peakutils.indexes(np.array(distances).astype(\'float32\'), thres=threshold, min_dist=min_dist) if len(distances) else []\n    if len(p) == 0:\n        return [0, num_frames]\n    if len(p) > max_shot_num:\n        max_distances = np.array(distances)[p]\n        top = np.argsort(-max_distances)[:max_shot_num]\n        p = p[np.sort(top)]\n    shots.append(0)\n    shots.append(p[0] + neigh_avg + 1)\n    for i in range(1, len(p)):\n        # We check that the peak is not due to a motion in the image\n        valid_dist = not motion_step or not check_motion(distances[p[i]-motion_step:p[i]], distances[p[i]])\n        if valid_dist:\n            shots.append(p[i] + neigh_avg + 1)\n    if shots[-1] < num_frames - min_dist:\n        shots.append(num_frames)\n    elif shots[-1] > num_frames:\n        shots[-1] = num_frames\n    return shots\n\n\ndef detect_peak_boundary(distances: List[float],\n                         method: str = \'kmeans\',\n                         **kwargs) -> List[int]:\n    detect_method = {\n        \'kmeans\': kmeans_algo,\n        \'threshold\': thre_algo,\n        \'motion\': motion_algo\n    }\n\n    if method in detect_method.keys():\n        return detect_method[method](distances, **kwargs)\n    else:\n        logger.error(""detect video shot by [%s] not implemented! Please use threshold, kmeans or motion!"" % method)\n\n\ndef torch_transform(img):\n    try:\n        import torchvision.transforms as transforms\n        return transforms.Compose([transforms.ToTensor(),\n                                   transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])(img)\n    except (ImportError, ModuleNotFoundError):\n        return np.asarray(img) * 2 / 255. - 1.\n\n\ndef get_all_subarea(img):\n    x_list = [0, img.size[0] / 3, 2 * img.size[0] / 3, img.size[0]]\n    y_list = [0, img.size[1] / 3, 2 * img.size[1] / 3, img.size[1]]\n\n    index = [[x, y, x + 1, y + 1] for [x, y] in product(range(len(x_list) - 1), range(len(y_list) - 1))]\n    all_subareas = [[x_list[idx[0]], y_list[idx[1]], x_list[idx[2]], y_list[idx[3]]] for idx in index]\n    return all_subareas, index\n'"
gnes/proto/__init__.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport ctypes\nimport os\nimport random\nfrom typing import List, Iterator, Tuple\nfrom typing import Optional\n\nimport numpy as np\nimport zmq\nfrom termcolor import colored\n\nfrom . import gnes_pb2\nfrom ..helper import batch_iterator, default_logger\n\n__all__ = [\'RequestGenerator\', \'send_message\', \'recv_message\',\n           \'blob2array\', \'array2blob\', \'gnes_pb2\', \'add_route\', \'add_version\']\n\n\nclass RequestGenerator:\n    @staticmethod\n    def index(data: Iterator[bytes], batch_size: int = 0, doc_type: int = gnes_pb2.Document.TEXT,\n              doc_id_start: int = 0, request_id_start: int = 0,\n              random_doc_id: bool = False,\n              *args, **kwargs):\n\n        for pi in batch_iterator(data, batch_size):\n            req = gnes_pb2.Request()\n            req.request_id = request_id_start\n            for raw_bytes in pi:\n                d = req.index.docs.add()\n                d.doc_id = doc_id_start if not random_doc_id else random.randint(0, ctypes.c_uint(-1).value)\n                d.raw_bytes = raw_bytes\n                d.weight = 1.0\n                d.doc_type = doc_type\n                doc_id_start += 1\n            yield req\n            request_id_start += 1\n\n    @staticmethod\n    def train(data: Iterator[bytes], batch_size: int = 0, doc_type: int = gnes_pb2.Document.TEXT,\n              doc_id_start: int = 0, request_id_start: int = 0,\n              random_doc_id: bool = False,\n              *args, **kwargs):\n        for pi in batch_iterator(data, batch_size):\n            req = gnes_pb2.Request()\n            req.request_id = request_id_start\n            for raw_bytes in pi:\n                d = req.train.docs.add()\n                d.doc_id = doc_id_start if not random_doc_id else random.randint(0, ctypes.c_uint(-1).value)\n                d.raw_bytes = raw_bytes\n                d.doc_type = doc_type\n                if not random_doc_id:\n                    doc_id_start += 1\n            yield req\n            request_id_start += 1\n        req = gnes_pb2.Request()\n        req.request_id = request_id_start\n        req.train.flush = True\n        yield req\n        request_id_start += 1\n\n    @staticmethod\n    def query(query: bytes, top_k: int, doc_type: int = gnes_pb2.Document.TEXT, request_id_start: int = 0, *args,\n              **kwargs):\n        if top_k <= 0:\n            raise ValueError(\'""top_k: %d"" is not a valid number\' % top_k)\n\n        req = gnes_pb2.Request()\n        req.request_id = request_id_start\n        req.search.query.raw_bytes = query\n        req.search.query.doc_type = doc_type\n        req.search.top_k = top_k\n        yield req\n\n\ndef blob2array(blob: \'gnes_pb2.NdArray\') -> np.ndarray:\n    """"""\n    Convert a blob proto to an array.\n    """"""\n    x = np.frombuffer(blob.data, dtype=blob.dtype).copy()\n    return x.reshape(blob.shape)\n\n\ndef array2blob(x: np.ndarray) -> \'gnes_pb2.NdArray\':\n    """"""Converts a N-dimensional array to blob proto.\n    """"""\n    blob = gnes_pb2.NdArray()\n    blob.data = x.tobytes()\n    blob.shape.extend(list(x.shape))\n    blob.dtype = x.dtype.name\n    return blob\n\n\ndef router2str(m: \'gnes_pb2.Message\') -> str:\n    route_str = [r.service for r in m.envelope.routes]\n    return colored(\'\xe2\x96\xb8\', \'green\').join(route_str)\n\n\ndef add_route(evlp: \'gnes_pb2.Envelope\', name: str, identity: str):\n    r = evlp.routes.add()\n    r.service = name\n    r.start_time.GetCurrentTime()\n    r.service_identity = identity\n\n\ndef add_version(evlp: \'gnes_pb2.Envelope\'):\n    from .. import __version__, __proto_version__\n    evlp.gnes_version = __version__\n    evlp.proto_version = __proto_version__\n    evlp.vcs_version = os.environ.get(\'GNES_VCS_VERSION\', \'\')\n\n\ndef merge_routes(msg: \'gnes_pb2.Message\', prev_msgs: List[\'gnes_pb2.Message\']):\n    # take unique routes by service identity\n    routes = {(r.service + r.service_identity): r for m in prev_msgs for r in m.envelope.routes}\n    msg.envelope.ClearField(\'routes\')\n    msg.envelope.routes.extend(sorted(routes.values(), key=lambda x: (x.start_time.seconds, x.start_time.nanos)))\n\n\ndef check_msg_version(msg: \'gnes_pb2.Message\'):\n    from .. import __version__, __proto_version__\n    if hasattr(msg.envelope, \'gnes_version\'):\n        if not msg.envelope.gnes_version:\n            # only happen in unittest\n            default_logger.warning(\'incoming message contains empty ""gnes_version"", \'\n                                   \'you may ignore it in debug/unittest mode. \'\n                                   \'otherwise please check if frontend service set correct version\')\n        elif __version__ != msg.envelope.gnes_version:\n            raise AttributeError(\'mismatched GNES version! \'\n                                 \'incoming message has GNES version %s, whereas local GNES version %s\' % (\n                                     msg.envelope.gnes_version, __version__))\n\n    if hasattr(msg.envelope, \'proto_version\'):\n        if not msg.envelope.proto_version:\n            # only happen in unittest\n            default_logger.warning(\'incoming message contains empty ""proto_version"", \'\n                                   \'you may ignore it in debug/unittest mode. \'\n                                   \'otherwise please check if frontend service set correct version\')\n        elif __proto_version__ != msg.envelope.proto_version:\n            raise AttributeError(\'mismatched protobuf version! \'\n                                 \'incoming message has protobuf version %s, whereas local protobuf version %s\' % (\n                                     msg.envelope.proto_version, __proto_version__))\n\n    if hasattr(msg.envelope, \'vcs_version\'):\n        if not msg.envelope.vcs_version or not os.environ.get(\'GNES_VCS_VERSION\'):\n            default_logger.warning(\'incoming message contains empty ""vcs_version"", \'\n                                   \'you may ignore it in debug/unittest mode, \'\n                                   \'or if you run gnes OUTSIDE docker container where GNES_VCS_VERSION is unset\'\n                                   \'otherwise please check if frontend service set correct version\')\n        elif os.environ.get(\'GNES_VCS_VERSION\') != msg.envelope.vcs_version:\n            raise AttributeError(\'mismatched vcs version! \'\n                                 \'incoming message has vcs_version %s, whereas local environment vcs_version is %s\' % (\n                                     msg.envelope.vcs_version, os.environ.get(\'GNES_VCS_VERSION\')))\n\n    if not hasattr(msg.envelope, \'proto_version\') and not hasattr(msg.envelope, \'gnes_version\'):\n        raise AttributeError(\'version_check=True locally, \'\n                             \'but incoming message contains no version info in its envelope. \'\n                             \'the message is probably sent from a very outdated GNES version\')\n\n\ndef extract_bytes_from_msg(msg: \'gnes_pb2.Message\') -> Tuple:\n    doc_bytes = []\n    chunk_bytes = []\n    doc_byte_type = b\'\'\n    chunk_byte_type = b\'\'\n\n    docs = msg.request.train.docs or msg.request.index.docs or [msg.request.search.query]\n    # for train request\n    for d in docs:\n        # oneof raw_data {\n        #     string raw_text = 5;\n        #       NdArray raw_image = 6;\n        #       NdArray raw_video = 7;\n        #       bytes raw_bytes = 8; // for other types\n        # }\n        dtype = d.WhichOneof(\'raw_data\') or \'\'\n        doc_byte_type = dtype.encode()\n        if dtype == \'raw_bytes\':\n            doc_bytes.append(d.raw_bytes)\n            d.ClearField(\'raw_bytes\')\n        elif dtype == \'raw_image\':\n            doc_bytes.append(d.raw_image.data)\n            d.raw_image.ClearField(\'data\')\n        elif dtype == \'raw_video\':\n            doc_bytes.append(d.raw_video.data)\n            d.raw_video.ClearField(\'data\')\n        elif dtype == \'raw_text\':\n            doc_bytes.append(d.raw_text.encode())\n            d.ClearField(\'raw_text\')\n\n        for c in d.chunks:\n            # oneof content {\n            # string text = 2;\n            # NdArray blob = 3;\n            # bytes raw = 7;\n            # }\n            chunk_bytes.append(c.embedding.data)\n            c.embedding.ClearField(\'data\')\n\n            ctype = c.WhichOneof(\'content\') or \'\'\n            chunk_byte_type = ctype.encode()\n            if ctype == \'raw\':\n                chunk_bytes.append(c.raw)\n                c.ClearField(\'raw\')\n            elif ctype == \'blob\':\n                chunk_bytes.append(c.blob.data)\n                c.blob.ClearField(\'data\')\n            elif ctype == \'text\':\n                chunk_bytes.append(c.text.encode())\n                c.ClearField(\'text\')\n\n    return doc_bytes, doc_byte_type, chunk_bytes, chunk_byte_type\n\n\ndef fill_raw_bytes_to_msg(msg: \'gnes_pb2.Message\', msg_data: List[bytes]):\n    doc_byte_type = msg_data[2].decode()\n    chunk_byte_type = msg_data[3].decode()\n    doc_bytes_len = int(msg_data[4])\n    chunk_bytes_len = int(msg_data[5])\n\n    doc_bytes = msg_data[6:(6 + doc_bytes_len)]\n    chunk_bytes = msg_data[(6 + doc_bytes_len):]\n\n    if len(chunk_bytes) != chunk_bytes_len:\n        raise ValueError(\'""chunk_bytes_len""=%d in message, but the actual length is %d\' % (\n            chunk_bytes_len, len(chunk_bytes)))\n\n    c_idx = 0\n    d_idx = 0\n    docs = msg.request.train.docs or msg.request.index.docs or [msg.request.search.query]\n    for d in docs:\n        if doc_bytes and doc_bytes[d_idx]:\n            if doc_byte_type == \'raw_bytes\':\n                d.raw_bytes = doc_bytes[d_idx]\n                d_idx += 1\n            elif doc_byte_type == \'raw_image\':\n                d.raw_image.data = doc_bytes[d_idx]\n                d_idx += 1\n            elif doc_byte_type == \'raw_video\':\n                d.raw_video.data = doc_bytes[d_idx]\n                d_idx += 1\n            elif doc_byte_type == \'raw_text\':\n                d.raw_text = doc_bytes[d_idx].decode()\n                d_idx += 1\n\n        for c in d.chunks:\n            if chunk_bytes and chunk_bytes[c_idx]:\n                c.embedding.data = chunk_bytes[c_idx]\n            c_idx += 1\n\n            if chunk_byte_type == \'raw\':\n                c.raw = chunk_bytes[c_idx]\n                c_idx += 1\n            elif chunk_byte_type == \'blob\':\n                c.blob.data = chunk_bytes[c_idx]\n                c_idx += 1\n            elif chunk_byte_type == \'text\':\n                c.text = chunk_bytes[c_idx].decode()\n                c_idx += 1\n\n\ndef send_message(sock: \'zmq.Socket\', msg: \'gnes_pb2.Message\', timeout: int = -1,\n                 squeeze_pb: bool = False, **kwargs) -> None:\n    try:\n        if timeout > 0:\n            sock.setsockopt(zmq.SNDTIMEO, timeout)\n        else:\n            sock.setsockopt(zmq.SNDTIMEO, -1)\n\n        if not squeeze_pb:\n            sock.send_multipart([msg.envelope.client_id.encode(), msg.SerializeToString()])\n        else:\n            doc_bytes, doc_byte_type, chunk_bytes, chunk_byte_type = extract_bytes_from_msg(msg)\n            # now raw_bytes are removed from message, hoping for faster de/serialization\n            sock.send_multipart(\n                [msg.envelope.client_id.encode(),  # 0\n                 msg.SerializeToString(),  # 1\n                 doc_byte_type, chunk_byte_type,  # 2, 3\n                 b\'%d\' % len(doc_bytes), b\'%d\' % len(chunk_bytes),  # 4, 5\n                 *doc_bytes, *chunk_bytes])  # 6, 7\n    except zmq.error.Again:\n        raise TimeoutError(\n            \'cannot send message to sock %s after timeout=%dms, please check the following:\'\n            \'is the server still online? is the network broken? are ""port"" correct? \' % (\n                sock, timeout))\n    except Exception as ex:\n        raise ex\n    finally:\n        sock.setsockopt(zmq.SNDTIMEO, -1)\n\n\ndef recv_message(sock: \'zmq.Socket\', timeout: int = -1, check_version: bool = False, **kwargs) -> Optional[\n    \'gnes_pb2.Message\']:\n    try:\n        if timeout > 0:\n            sock.setsockopt(zmq.RCVTIMEO, timeout)\n        else:\n            sock.setsockopt(zmq.RCVTIMEO, -1)\n\n        msg = gnes_pb2.Message()\n        msg_data = sock.recv_multipart()\n        msg.ParseFromString(msg_data[1])\n        if check_version:\n            check_msg_version(msg)\n\n        # now we have a barebone msg, we need to fill in data\n        if len(msg_data) > 2:\n            fill_raw_bytes_to_msg(msg, msg_data)\n        return msg\n\n    except zmq.error.Again:\n        raise TimeoutError(\n            \'no response from sock %s after timeout=%dms, please check the following:\'\n            \'is the server still online? is the network broken? are ""port"" correct? \' % (\n                sock, timeout))\n    except Exception as ex:\n        raise ex\n    finally:\n        sock.setsockopt(zmq.RCVTIMEO, -1)\n'"
gnes/proto/gnes_pb2.py,0,"b'# -*- coding: utf-8 -*-\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: gnes.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom google.protobuf import timestamp_pb2 as google_dot_protobuf_dot_timestamp__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'gnes.proto\',\n  package=\'gnes\',\n  syntax=\'proto3\',\n  serialized_options=None,\n  serialized_pb=_b(\'\\n\\ngnes.proto\\x12\\x04gnes\\x1a\\x1fgoogle/protobuf/timestamp.proto\\""9\\n\\x07NdArray\\x12\\x0c\\n\\x04\\x64\\x61ta\\x18\\x01 \\x01(\\x0c\\x12\\x11\\n\\x05shape\\x18\\x02 \\x03(\\rB\\x02\\x10\\x01\\x12\\r\\n\\x05\\x64type\\x18\\x03 \\x01(\\t\\""\\xb9\\x01\\n\\x05\\x43hunk\\x12\\x0e\\n\\x06\\x64oc_id\\x18\\x01 \\x01(\\x04\\x12\\x0e\\n\\x04text\\x18\\x02 \\x01(\\tH\\x00\\x12\\x1d\\n\\x04\\x62lob\\x18\\x03 \\x01(\\x0b\\x32\\r.gnes.NdArrayH\\x00\\x12\\r\\n\\x03raw\\x18\\x07 \\x01(\\x0cH\\x00\\x12\\x0e\\n\\x06offset\\x18\\x04 \\x01(\\r\\x12\\x15\\n\\toffset_nd\\x18\\x05 \\x03(\\rB\\x02\\x10\\x01\\x12\\x0e\\n\\x06weight\\x18\\x06 \\x01(\\x02\\x12 \\n\\tembedding\\x18\\x08 \\x01(\\x0b\\x32\\r.gnes.NdArrayB\\t\\n\\x07\\x63ontent\\""\\xc4\\x02\\n\\x08\\x44ocument\\x12\\x0e\\n\\x06\\x64oc_id\\x18\\x01 \\x01(\\x04\\x12\\x1b\\n\\x06\\x63hunks\\x18\\x02 \\x03(\\x0b\\x32\\x0b.gnes.Chunk\\x12(\\n\\x08\\x64oc_type\\x18\\x03 \\x01(\\x0e\\x32\\x16.gnes.Document.DocType\\x12\\x11\\n\\tmeta_info\\x18\\x04 \\x01(\\x0c\\x12\\x12\\n\\x08raw_text\\x18\\x05 \\x01(\\tH\\x00\\x12\\""\\n\\traw_image\\x18\\x06 \\x01(\\x0b\\x32\\r.gnes.NdArrayH\\x00\\x12\\""\\n\\traw_video\\x18\\x07 \\x01(\\x0b\\x32\\r.gnes.NdArrayH\\x00\\x12\\x13\\n\\traw_bytes\\x18\\x08 \\x01(\\x0cH\\x00\\x12\\x0e\\n\\x06weight\\x18\\n \\x01(\\x02\\""A\\n\\x07\\x44ocType\\x12\\x0b\\n\\x07UNKNOWN\\x10\\x00\\x12\\x08\\n\\x04TEXT\\x10\\x01\\x12\\t\\n\\x05IMAGE\\x10\\x02\\x12\\t\\n\\x05VIDEO\\x10\\x03\\x12\\t\\n\\x05\\x41UDIO\\x10\\x04\\x42\\n\\n\\x08raw_data\\""\\xc9\\x03\\n\\x08\\x45nvelope\\x12\\x11\\n\\tclient_id\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\nrequest_id\\x18\\x02 \\x01(\\r\\x12\\x0f\\n\\x07part_id\\x18\\x03 \\x01(\\r\\x12\\x10\\n\\x08num_part\\x18\\x04 \\x03(\\r\\x12\\x0f\\n\\x07timeout\\x18\\x05 \\x01(\\r\\x12$\\n\\x06routes\\x18\\x06 \\x03(\\x0b\\x32\\x14.gnes.Envelope.route\\x12\\x14\\n\\x0cgnes_version\\x18\\x07 \\x01(\\t\\x12\\x15\\n\\rproto_version\\x18\\x08 \\x01(\\t\\x12\\x13\\n\\x0bvcs_version\\x18\\t \\x01(\\t\\x1a\\xf9\\x01\\n\\x05route\\x12\\x0f\\n\\x07service\\x18\\x01 \\x01(\\t\\x12.\\n\\nstart_time\\x18\\x02 \\x01(\\x0b\\x32\\x1a.google.protobuf.Timestamp\\x12,\\n\\x08\\x65nd_time\\x18\\x03 \\x01(\\x0b\\x32\\x1a.google.protobuf.Timestamp\\x12\\x34\\n\\x10\\x66irst_start_time\\x18\\x04 \\x01(\\x0b\\x32\\x1a.google.protobuf.Timestamp\\x12\\x31\\n\\rlast_end_time\\x18\\x05 \\x01(\\x0b\\x32\\x1a.google.protobuf.Timestamp\\x12\\x18\\n\\x10service_identity\\x18\\x06 \\x01(\\t\\""y\\n\\x07Message\\x12 \\n\\x08\\x65nvelope\\x18\\x01 \\x01(\\x0b\\x32\\x0e.gnes.Envelope\\x12 \\n\\x07request\\x18\\x02 \\x01(\\x0b\\x32\\r.gnes.RequestH\\x00\\x12\\""\\n\\x08response\\x18\\x03 \\x01(\\x0b\\x32\\x0e.gnes.ResponseH\\x00\\x42\\x06\\n\\x04\\x62ody\\""\\xf6\\x03\\n\\x07Request\\x12\\x12\\n\\nrequest_id\\x18\\x01 \\x01(\\r\\x12+\\n\\x05train\\x18\\x02 \\x01(\\x0b\\x32\\x1a.gnes.Request.TrainRequestH\\x00\\x12+\\n\\x05index\\x18\\x03 \\x01(\\x0b\\x32\\x1a.gnes.Request.IndexRequestH\\x00\\x12,\\n\\x06search\\x18\\x04 \\x01(\\x0b\\x32\\x1a.gnes.Request.QueryRequestH\\x00\\x12/\\n\\x07\\x63ontrol\\x18\\x05 \\x01(\\x0b\\x32\\x1c.gnes.Request.ControlRequestH\\x00\\x1a;\\n\\x0cTrainRequest\\x12\\x1c\\n\\x04\\x64ocs\\x18\\x01 \\x03(\\x0b\\x32\\x0e.gnes.Document\\x12\\r\\n\\x05\\x66lush\\x18\\x02 \\x01(\\x08\\x1a,\\n\\x0cIndexRequest\\x12\\x1c\\n\\x04\\x64ocs\\x18\\x01 \\x03(\\x0b\\x32\\x0e.gnes.Document\\x1a<\\n\\x0cQueryRequest\\x12\\x1d\\n\\x05query\\x18\\x01 \\x01(\\x0b\\x32\\x0e.gnes.Document\\x12\\r\\n\\x05top_k\\x18\\x02 \\x01(\\r\\x1am\\n\\x0e\\x43ontrolRequest\\x12\\x35\\n\\x07\\x63ommand\\x18\\x01 \\x01(\\x0e\\x32$.gnes.Request.ControlRequest.Command\\""$\\n\\x07\\x43ommand\\x12\\r\\n\\tTERMINATE\\x10\\x00\\x12\\n\\n\\x06STATUS\\x10\\x01\\x42\\x06\\n\\x04\\x62ody\\""\\xc6\\x06\\n\\x08Response\\x12\\x12\\n\\nrequest_id\\x18\\x01 \\x01(\\r\\x12-\\n\\x05train\\x18\\x02 \\x01(\\x0b\\x32\\x1c.gnes.Response.TrainResponseH\\x00\\x12-\\n\\x05index\\x18\\x03 \\x01(\\x0b\\x32\\x1c.gnes.Response.IndexResponseH\\x00\\x12.\\n\\x06search\\x18\\x04 \\x01(\\x0b\\x32\\x1c.gnes.Response.QueryResponseH\\x00\\x12\\x31\\n\\x07\\x63ontrol\\x18\\x05 \\x01(\\x0b\\x32\\x1e.gnes.Response.ControlResponseH\\x00\\x1a\\x36\\n\\rTrainResponse\\x12%\\n\\x06status\\x18\\x01 \\x01(\\x0e\\x32\\x15.gnes.Response.Status\\x1a\\x36\\n\\rIndexResponse\\x12%\\n\\x06status\\x18\\x01 \\x01(\\x0e\\x32\\x15.gnes.Response.Status\\x1a\\x38\\n\\x0f\\x43ontrolResponse\\x12%\\n\\x06status\\x18\\x01 \\x01(\\x0e\\x32\\x15.gnes.Response.Status\\x1a\\xf8\\x02\\n\\rQueryResponse\\x12%\\n\\x06status\\x18\\x01 \\x01(\\x0e\\x32\\x15.gnes.Response.Status\\x12\\r\\n\\x05top_k\\x18\\x02 \\x01(\\r\\x12?\\n\\x0ctopk_results\\x18\\x03 \\x03(\\x0b\\x32).gnes.Response.QueryResponse.ScoredResult\\x12\\x1c\\n\\x14is_big_score_similar\\x18\\x04 \\x01(\\x08\\x12\\x11\\n\\tis_sorted\\x18\\x05 \\x01(\\x08\\x1a\\xbe\\x01\\n\\x0cScoredResult\\x12\\x1c\\n\\x05\\x63hunk\\x18\\x01 \\x01(\\x0b\\x32\\x0b.gnes.ChunkH\\x00\\x12\\x1d\\n\\x03\\x64oc\\x18\\x02 \\x01(\\x0b\\x32\\x0e.gnes.DocumentH\\x00\\x12>\\n\\x05score\\x18\\x03 \\x01(\\x0b\\x32/.gnes.Response.QueryResponse.ScoredResult.Score\\x1a)\\n\\x05Score\\x12\\r\\n\\x05value\\x18\\x01 \\x01(\\x02\\x12\\x11\\n\\texplained\\x18\\x02 \\x01(\\tB\\x06\\n\\x04\\x62ody\\""8\\n\\x06Status\\x12\\x0b\\n\\x07SUCCESS\\x10\\x00\\x12\\t\\n\\x05\\x45RROR\\x10\\x01\\x12\\x0b\\n\\x07PENDING\\x10\\x02\\x12\\t\\n\\x05READY\\x10\\x03\\x42\\x06\\n\\x04\\x62ody2\\xe3\\x01\\n\\x07GnesRPC\\x12(\\n\\x05Train\\x12\\r.gnes.Request\\x1a\\x0e.gnes.Response\\""\\x00\\x12(\\n\\x05Index\\x12\\r.gnes.Request\\x1a\\x0e.gnes.Response\\""\\x00\\x12(\\n\\x05Query\\x12\\r.gnes.Request\\x1a\\x0e.gnes.Response\\""\\x00\\x12\\\'\\n\\x04\\x43\\x61ll\\x12\\r.gnes.Request\\x1a\\x0e.gnes.Response\\""\\x00\\x12\\x31\\n\\nStreamCall\\x12\\r.gnes.Request\\x1a\\x0e.gnes.Response\\""\\x00(\\x01\\x30\\x01\\x62\\x06proto3\')\n  ,\n  dependencies=[google_dot_protobuf_dot_timestamp__pb2.DESCRIPTOR,])\n\n\n\n_DOCUMENT_DOCTYPE = _descriptor.EnumDescriptor(\n  name=\'DocType\',\n  full_name=\'gnes.Document.DocType\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'UNKNOWN\', index=0, number=0,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'TEXT\', index=1, number=1,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'IMAGE\', index=2, number=2,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'VIDEO\', index=3, number=3,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'AUDIO\', index=4, number=4,\n      serialized_options=None,\n      type=None),\n  ],\n  containing_type=None,\n  serialized_options=None,\n  serialized_start=548,\n  serialized_end=613,\n)\n_sym_db.RegisterEnumDescriptor(_DOCUMENT_DOCTYPE)\n\n_REQUEST_CONTROLREQUEST_COMMAND = _descriptor.EnumDescriptor(\n  name=\'Command\',\n  full_name=\'gnes.Request.ControlRequest.Command\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'TERMINATE\', index=0, number=0,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'STATUS\', index=1, number=1,\n      serialized_options=None,\n      type=None),\n  ],\n  containing_type=None,\n  serialized_options=None,\n  serialized_start=1669,\n  serialized_end=1705,\n)\n_sym_db.RegisterEnumDescriptor(_REQUEST_CONTROLREQUEST_COMMAND)\n\n_RESPONSE_STATUS = _descriptor.EnumDescriptor(\n  name=\'Status\',\n  full_name=\'gnes.Response.Status\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'SUCCESS\', index=0, number=0,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'ERROR\', index=1, number=1,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'PENDING\', index=2, number=2,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'READY\', index=3, number=3,\n      serialized_options=None,\n      type=None),\n  ],\n  containing_type=None,\n  serialized_options=None,\n  serialized_start=2490,\n  serialized_end=2546,\n)\n_sym_db.RegisterEnumDescriptor(_RESPONSE_STATUS)\n\n\n_NDARRAY = _descriptor.Descriptor(\n  name=\'NdArray\',\n  full_name=\'gnes.NdArray\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'data\', full_name=\'gnes.NdArray.data\', index=0,\n      number=1, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'shape\', full_name=\'gnes.NdArray.shape\', index=1,\n      number=2, type=13, cpp_type=3, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=_b(\'\\020\\001\'), file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'dtype\', full_name=\'gnes.NdArray.dtype\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=53,\n  serialized_end=110,\n)\n\n\n_CHUNK = _descriptor.Descriptor(\n  name=\'Chunk\',\n  full_name=\'gnes.Chunk\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'doc_id\', full_name=\'gnes.Chunk.doc_id\', index=0,\n      number=1, type=4, cpp_type=4, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'text\', full_name=\'gnes.Chunk.text\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'blob\', full_name=\'gnes.Chunk.blob\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'raw\', full_name=\'gnes.Chunk.raw\', index=3,\n      number=7, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'offset\', full_name=\'gnes.Chunk.offset\', index=4,\n      number=4, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'offset_nd\', full_name=\'gnes.Chunk.offset_nd\', index=5,\n      number=5, type=13, cpp_type=3, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=_b(\'\\020\\001\'), file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'weight\', full_name=\'gnes.Chunk.weight\', index=6,\n      number=6, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'embedding\', full_name=\'gnes.Chunk.embedding\', index=7,\n      number=8, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'content\', full_name=\'gnes.Chunk.content\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=113,\n  serialized_end=298,\n)\n\n\n_DOCUMENT = _descriptor.Descriptor(\n  name=\'Document\',\n  full_name=\'gnes.Document\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'doc_id\', full_name=\'gnes.Document.doc_id\', index=0,\n      number=1, type=4, cpp_type=4, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'chunks\', full_name=\'gnes.Document.chunks\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'doc_type\', full_name=\'gnes.Document.doc_type\', index=2,\n      number=3, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'meta_info\', full_name=\'gnes.Document.meta_info\', index=3,\n      number=4, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'raw_text\', full_name=\'gnes.Document.raw_text\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'raw_image\', full_name=\'gnes.Document.raw_image\', index=5,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'raw_video\', full_name=\'gnes.Document.raw_video\', index=6,\n      number=7, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'raw_bytes\', full_name=\'gnes.Document.raw_bytes\', index=7,\n      number=8, type=12, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b(""""),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'weight\', full_name=\'gnes.Document.weight\', index=8,\n      number=10, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _DOCUMENT_DOCTYPE,\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'raw_data\', full_name=\'gnes.Document.raw_data\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=301,\n  serialized_end=625,\n)\n\n\n_ENVELOPE_ROUTE = _descriptor.Descriptor(\n  name=\'route\',\n  full_name=\'gnes.Envelope.route\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'service\', full_name=\'gnes.Envelope.route.service\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'start_time\', full_name=\'gnes.Envelope.route.start_time\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'end_time\', full_name=\'gnes.Envelope.route.end_time\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'first_start_time\', full_name=\'gnes.Envelope.route.first_start_time\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'last_end_time\', full_name=\'gnes.Envelope.route.last_end_time\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'service_identity\', full_name=\'gnes.Envelope.route.service_identity\', index=5,\n      number=6, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=836,\n  serialized_end=1085,\n)\n\n_ENVELOPE = _descriptor.Descriptor(\n  name=\'Envelope\',\n  full_name=\'gnes.Envelope\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'client_id\', full_name=\'gnes.Envelope.client_id\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'request_id\', full_name=\'gnes.Envelope.request_id\', index=1,\n      number=2, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'part_id\', full_name=\'gnes.Envelope.part_id\', index=2,\n      number=3, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'num_part\', full_name=\'gnes.Envelope.num_part\', index=3,\n      number=4, type=13, cpp_type=3, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'timeout\', full_name=\'gnes.Envelope.timeout\', index=4,\n      number=5, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'routes\', full_name=\'gnes.Envelope.routes\', index=5,\n      number=6, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'gnes_version\', full_name=\'gnes.Envelope.gnes_version\', index=6,\n      number=7, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'proto_version\', full_name=\'gnes.Envelope.proto_version\', index=7,\n      number=8, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'vcs_version\', full_name=\'gnes.Envelope.vcs_version\', index=8,\n      number=9, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_ENVELOPE_ROUTE, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=628,\n  serialized_end=1085,\n)\n\n\n_MESSAGE = _descriptor.Descriptor(\n  name=\'Message\',\n  full_name=\'gnes.Message\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'envelope\', full_name=\'gnes.Message.envelope\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'request\', full_name=\'gnes.Message.request\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'response\', full_name=\'gnes.Message.response\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'body\', full_name=\'gnes.Message.body\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=1087,\n  serialized_end=1208,\n)\n\n\n_REQUEST_TRAINREQUEST = _descriptor.Descriptor(\n  name=\'TrainRequest\',\n  full_name=\'gnes.Request.TrainRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'docs\', full_name=\'gnes.Request.TrainRequest.docs\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'flush\', full_name=\'gnes.Request.TrainRequest.flush\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1427,\n  serialized_end=1486,\n)\n\n_REQUEST_INDEXREQUEST = _descriptor.Descriptor(\n  name=\'IndexRequest\',\n  full_name=\'gnes.Request.IndexRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'docs\', full_name=\'gnes.Request.IndexRequest.docs\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1488,\n  serialized_end=1532,\n)\n\n_REQUEST_QUERYREQUEST = _descriptor.Descriptor(\n  name=\'QueryRequest\',\n  full_name=\'gnes.Request.QueryRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'query\', full_name=\'gnes.Request.QueryRequest.query\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'top_k\', full_name=\'gnes.Request.QueryRequest.top_k\', index=1,\n      number=2, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1534,\n  serialized_end=1594,\n)\n\n_REQUEST_CONTROLREQUEST = _descriptor.Descriptor(\n  name=\'ControlRequest\',\n  full_name=\'gnes.Request.ControlRequest\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'command\', full_name=\'gnes.Request.ControlRequest.command\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _REQUEST_CONTROLREQUEST_COMMAND,\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1596,\n  serialized_end=1705,\n)\n\n_REQUEST = _descriptor.Descriptor(\n  name=\'Request\',\n  full_name=\'gnes.Request\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'request_id\', full_name=\'gnes.Request.request_id\', index=0,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'train\', full_name=\'gnes.Request.train\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'index\', full_name=\'gnes.Request.index\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'search\', full_name=\'gnes.Request.search\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'control\', full_name=\'gnes.Request.control\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_REQUEST_TRAINREQUEST, _REQUEST_INDEXREQUEST, _REQUEST_QUERYREQUEST, _REQUEST_CONTROLREQUEST, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'body\', full_name=\'gnes.Request.body\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=1211,\n  serialized_end=1713,\n)\n\n\n_RESPONSE_TRAINRESPONSE = _descriptor.Descriptor(\n  name=\'TrainResponse\',\n  full_name=\'gnes.Response.TrainResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'status\', full_name=\'gnes.Response.TrainResponse.status\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1941,\n  serialized_end=1995,\n)\n\n_RESPONSE_INDEXRESPONSE = _descriptor.Descriptor(\n  name=\'IndexResponse\',\n  full_name=\'gnes.Response.IndexResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'status\', full_name=\'gnes.Response.IndexResponse.status\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1997,\n  serialized_end=2051,\n)\n\n_RESPONSE_CONTROLRESPONSE = _descriptor.Descriptor(\n  name=\'ControlResponse\',\n  full_name=\'gnes.Response.ControlResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'status\', full_name=\'gnes.Response.ControlResponse.status\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2053,\n  serialized_end=2109,\n)\n\n_RESPONSE_QUERYRESPONSE_SCOREDRESULT_SCORE = _descriptor.Descriptor(\n  name=\'Score\',\n  full_name=\'gnes.Response.QueryResponse.ScoredResult.Score\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'gnes.Response.QueryResponse.ScoredResult.Score.value\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'explained\', full_name=\'gnes.Response.QueryResponse.ScoredResult.Score.explained\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2439,\n  serialized_end=2480,\n)\n\n_RESPONSE_QUERYRESPONSE_SCOREDRESULT = _descriptor.Descriptor(\n  name=\'ScoredResult\',\n  full_name=\'gnes.Response.QueryResponse.ScoredResult\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'chunk\', full_name=\'gnes.Response.QueryResponse.ScoredResult.chunk\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'doc\', full_name=\'gnes.Response.QueryResponse.ScoredResult.doc\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'score\', full_name=\'gnes.Response.QueryResponse.ScoredResult.score\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_RESPONSE_QUERYRESPONSE_SCOREDRESULT_SCORE, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'body\', full_name=\'gnes.Response.QueryResponse.ScoredResult.body\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=2298,\n  serialized_end=2488,\n)\n\n_RESPONSE_QUERYRESPONSE = _descriptor.Descriptor(\n  name=\'QueryResponse\',\n  full_name=\'gnes.Response.QueryResponse\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'status\', full_name=\'gnes.Response.QueryResponse.status\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'top_k\', full_name=\'gnes.Response.QueryResponse.top_k\', index=1,\n      number=2, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'topk_results\', full_name=\'gnes.Response.QueryResponse.topk_results\', index=2,\n      number=3, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'is_big_score_similar\', full_name=\'gnes.Response.QueryResponse.is_big_score_similar\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'is_sorted\', full_name=\'gnes.Response.QueryResponse.is_sorted\', index=4,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_RESPONSE_QUERYRESPONSE_SCOREDRESULT, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2112,\n  serialized_end=2488,\n)\n\n_RESPONSE = _descriptor.Descriptor(\n  name=\'Response\',\n  full_name=\'gnes.Response\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'request_id\', full_name=\'gnes.Response.request_id\', index=0,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'train\', full_name=\'gnes.Response.train\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'index\', full_name=\'gnes.Response.index\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'search\', full_name=\'gnes.Response.search\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'control\', full_name=\'gnes.Response.control\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_RESPONSE_TRAINRESPONSE, _RESPONSE_INDEXRESPONSE, _RESPONSE_CONTROLRESPONSE, _RESPONSE_QUERYRESPONSE, ],\n  enum_types=[\n    _RESPONSE_STATUS,\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'body\', full_name=\'gnes.Response.body\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=1716,\n  serialized_end=2554,\n)\n\n_CHUNK.fields_by_name[\'blob\'].message_type = _NDARRAY\n_CHUNK.fields_by_name[\'embedding\'].message_type = _NDARRAY\n_CHUNK.oneofs_by_name[\'content\'].fields.append(\n  _CHUNK.fields_by_name[\'text\'])\n_CHUNK.fields_by_name[\'text\'].containing_oneof = _CHUNK.oneofs_by_name[\'content\']\n_CHUNK.oneofs_by_name[\'content\'].fields.append(\n  _CHUNK.fields_by_name[\'blob\'])\n_CHUNK.fields_by_name[\'blob\'].containing_oneof = _CHUNK.oneofs_by_name[\'content\']\n_CHUNK.oneofs_by_name[\'content\'].fields.append(\n  _CHUNK.fields_by_name[\'raw\'])\n_CHUNK.fields_by_name[\'raw\'].containing_oneof = _CHUNK.oneofs_by_name[\'content\']\n_DOCUMENT.fields_by_name[\'chunks\'].message_type = _CHUNK\n_DOCUMENT.fields_by_name[\'doc_type\'].enum_type = _DOCUMENT_DOCTYPE\n_DOCUMENT.fields_by_name[\'raw_image\'].message_type = _NDARRAY\n_DOCUMENT.fields_by_name[\'raw_video\'].message_type = _NDARRAY\n_DOCUMENT_DOCTYPE.containing_type = _DOCUMENT\n_DOCUMENT.oneofs_by_name[\'raw_data\'].fields.append(\n  _DOCUMENT.fields_by_name[\'raw_text\'])\n_DOCUMENT.fields_by_name[\'raw_text\'].containing_oneof = _DOCUMENT.oneofs_by_name[\'raw_data\']\n_DOCUMENT.oneofs_by_name[\'raw_data\'].fields.append(\n  _DOCUMENT.fields_by_name[\'raw_image\'])\n_DOCUMENT.fields_by_name[\'raw_image\'].containing_oneof = _DOCUMENT.oneofs_by_name[\'raw_data\']\n_DOCUMENT.oneofs_by_name[\'raw_data\'].fields.append(\n  _DOCUMENT.fields_by_name[\'raw_video\'])\n_DOCUMENT.fields_by_name[\'raw_video\'].containing_oneof = _DOCUMENT.oneofs_by_name[\'raw_data\']\n_DOCUMENT.oneofs_by_name[\'raw_data\'].fields.append(\n  _DOCUMENT.fields_by_name[\'raw_bytes\'])\n_DOCUMENT.fields_by_name[\'raw_bytes\'].containing_oneof = _DOCUMENT.oneofs_by_name[\'raw_data\']\n_ENVELOPE_ROUTE.fields_by_name[\'start_time\'].message_type = google_dot_protobuf_dot_timestamp__pb2._TIMESTAMP\n_ENVELOPE_ROUTE.fields_by_name[\'end_time\'].message_type = google_dot_protobuf_dot_timestamp__pb2._TIMESTAMP\n_ENVELOPE_ROUTE.fields_by_name[\'first_start_time\'].message_type = google_dot_protobuf_dot_timestamp__pb2._TIMESTAMP\n_ENVELOPE_ROUTE.fields_by_name[\'last_end_time\'].message_type = google_dot_protobuf_dot_timestamp__pb2._TIMESTAMP\n_ENVELOPE_ROUTE.containing_type = _ENVELOPE\n_ENVELOPE.fields_by_name[\'routes\'].message_type = _ENVELOPE_ROUTE\n_MESSAGE.fields_by_name[\'envelope\'].message_type = _ENVELOPE\n_MESSAGE.fields_by_name[\'request\'].message_type = _REQUEST\n_MESSAGE.fields_by_name[\'response\'].message_type = _RESPONSE\n_MESSAGE.oneofs_by_name[\'body\'].fields.append(\n  _MESSAGE.fields_by_name[\'request\'])\n_MESSAGE.fields_by_name[\'request\'].containing_oneof = _MESSAGE.oneofs_by_name[\'body\']\n_MESSAGE.oneofs_by_name[\'body\'].fields.append(\n  _MESSAGE.fields_by_name[\'response\'])\n_MESSAGE.fields_by_name[\'response\'].containing_oneof = _MESSAGE.oneofs_by_name[\'body\']\n_REQUEST_TRAINREQUEST.fields_by_name[\'docs\'].message_type = _DOCUMENT\n_REQUEST_TRAINREQUEST.containing_type = _REQUEST\n_REQUEST_INDEXREQUEST.fields_by_name[\'docs\'].message_type = _DOCUMENT\n_REQUEST_INDEXREQUEST.containing_type = _REQUEST\n_REQUEST_QUERYREQUEST.fields_by_name[\'query\'].message_type = _DOCUMENT\n_REQUEST_QUERYREQUEST.containing_type = _REQUEST\n_REQUEST_CONTROLREQUEST.fields_by_name[\'command\'].enum_type = _REQUEST_CONTROLREQUEST_COMMAND\n_REQUEST_CONTROLREQUEST.containing_type = _REQUEST\n_REQUEST_CONTROLREQUEST_COMMAND.containing_type = _REQUEST_CONTROLREQUEST\n_REQUEST.fields_by_name[\'train\'].message_type = _REQUEST_TRAINREQUEST\n_REQUEST.fields_by_name[\'index\'].message_type = _REQUEST_INDEXREQUEST\n_REQUEST.fields_by_name[\'search\'].message_type = _REQUEST_QUERYREQUEST\n_REQUEST.fields_by_name[\'control\'].message_type = _REQUEST_CONTROLREQUEST\n_REQUEST.oneofs_by_name[\'body\'].fields.append(\n  _REQUEST.fields_by_name[\'train\'])\n_REQUEST.fields_by_name[\'train\'].containing_oneof = _REQUEST.oneofs_by_name[\'body\']\n_REQUEST.oneofs_by_name[\'body\'].fields.append(\n  _REQUEST.fields_by_name[\'index\'])\n_REQUEST.fields_by_name[\'index\'].containing_oneof = _REQUEST.oneofs_by_name[\'body\']\n_REQUEST.oneofs_by_name[\'body\'].fields.append(\n  _REQUEST.fields_by_name[\'search\'])\n_REQUEST.fields_by_name[\'search\'].containing_oneof = _REQUEST.oneofs_by_name[\'body\']\n_REQUEST.oneofs_by_name[\'body\'].fields.append(\n  _REQUEST.fields_by_name[\'control\'])\n_REQUEST.fields_by_name[\'control\'].containing_oneof = _REQUEST.oneofs_by_name[\'body\']\n_RESPONSE_TRAINRESPONSE.fields_by_name[\'status\'].enum_type = _RESPONSE_STATUS\n_RESPONSE_TRAINRESPONSE.containing_type = _RESPONSE\n_RESPONSE_INDEXRESPONSE.fields_by_name[\'status\'].enum_type = _RESPONSE_STATUS\n_RESPONSE_INDEXRESPONSE.containing_type = _RESPONSE\n_RESPONSE_CONTROLRESPONSE.fields_by_name[\'status\'].enum_type = _RESPONSE_STATUS\n_RESPONSE_CONTROLRESPONSE.containing_type = _RESPONSE\n_RESPONSE_QUERYRESPONSE_SCOREDRESULT_SCORE.containing_type = _RESPONSE_QUERYRESPONSE_SCOREDRESULT\n_RESPONSE_QUERYRESPONSE_SCOREDRESULT.fields_by_name[\'chunk\'].message_type = _CHUNK\n_RESPONSE_QUERYRESPONSE_SCOREDRESULT.fields_by_name[\'doc\'].message_type = _DOCUMENT\n_RESPONSE_QUERYRESPONSE_SCOREDRESULT.fields_by_name[\'score\'].message_type = _RESPONSE_QUERYRESPONSE_SCOREDRESULT_SCORE\n_RESPONSE_QUERYRESPONSE_SCOREDRESULT.containing_type = _RESPONSE_QUERYRESPONSE\n_RESPONSE_QUERYRESPONSE_SCOREDRESULT.oneofs_by_name[\'body\'].fields.append(\n  _RESPONSE_QUERYRESPONSE_SCOREDRESULT.fields_by_name[\'chunk\'])\n_RESPONSE_QUERYRESPONSE_SCOREDRESULT.fields_by_name[\'chunk\'].containing_oneof = _RESPONSE_QUERYRESPONSE_SCOREDRESULT.oneofs_by_name[\'body\']\n_RESPONSE_QUERYRESPONSE_SCOREDRESULT.oneofs_by_name[\'body\'].fields.append(\n  _RESPONSE_QUERYRESPONSE_SCOREDRESULT.fields_by_name[\'doc\'])\n_RESPONSE_QUERYRESPONSE_SCOREDRESULT.fields_by_name[\'doc\'].containing_oneof = _RESPONSE_QUERYRESPONSE_SCOREDRESULT.oneofs_by_name[\'body\']\n_RESPONSE_QUERYRESPONSE.fields_by_name[\'status\'].enum_type = _RESPONSE_STATUS\n_RESPONSE_QUERYRESPONSE.fields_by_name[\'topk_results\'].message_type = _RESPONSE_QUERYRESPONSE_SCOREDRESULT\n_RESPONSE_QUERYRESPONSE.containing_type = _RESPONSE\n_RESPONSE.fields_by_name[\'train\'].message_type = _RESPONSE_TRAINRESPONSE\n_RESPONSE.fields_by_name[\'index\'].message_type = _RESPONSE_INDEXRESPONSE\n_RESPONSE.fields_by_name[\'search\'].message_type = _RESPONSE_QUERYRESPONSE\n_RESPONSE.fields_by_name[\'control\'].message_type = _RESPONSE_CONTROLRESPONSE\n_RESPONSE_STATUS.containing_type = _RESPONSE\n_RESPONSE.oneofs_by_name[\'body\'].fields.append(\n  _RESPONSE.fields_by_name[\'train\'])\n_RESPONSE.fields_by_name[\'train\'].containing_oneof = _RESPONSE.oneofs_by_name[\'body\']\n_RESPONSE.oneofs_by_name[\'body\'].fields.append(\n  _RESPONSE.fields_by_name[\'index\'])\n_RESPONSE.fields_by_name[\'index\'].containing_oneof = _RESPONSE.oneofs_by_name[\'body\']\n_RESPONSE.oneofs_by_name[\'body\'].fields.append(\n  _RESPONSE.fields_by_name[\'search\'])\n_RESPONSE.fields_by_name[\'search\'].containing_oneof = _RESPONSE.oneofs_by_name[\'body\']\n_RESPONSE.oneofs_by_name[\'body\'].fields.append(\n  _RESPONSE.fields_by_name[\'control\'])\n_RESPONSE.fields_by_name[\'control\'].containing_oneof = _RESPONSE.oneofs_by_name[\'body\']\nDESCRIPTOR.message_types_by_name[\'NdArray\'] = _NDARRAY\nDESCRIPTOR.message_types_by_name[\'Chunk\'] = _CHUNK\nDESCRIPTOR.message_types_by_name[\'Document\'] = _DOCUMENT\nDESCRIPTOR.message_types_by_name[\'Envelope\'] = _ENVELOPE\nDESCRIPTOR.message_types_by_name[\'Message\'] = _MESSAGE\nDESCRIPTOR.message_types_by_name[\'Request\'] = _REQUEST\nDESCRIPTOR.message_types_by_name[\'Response\'] = _RESPONSE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nNdArray = _reflection.GeneratedProtocolMessageType(\'NdArray\', (_message.Message,), dict(\n  DESCRIPTOR = _NDARRAY,\n  __module__ = \'gnes_pb2\'\n  # @@protoc_insertion_point(class_scope:gnes.NdArray)\n  ))\n_sym_db.RegisterMessage(NdArray)\n\nChunk = _reflection.GeneratedProtocolMessageType(\'Chunk\', (_message.Message,), dict(\n  DESCRIPTOR = _CHUNK,\n  __module__ = \'gnes_pb2\'\n  # @@protoc_insertion_point(class_scope:gnes.Chunk)\n  ))\n_sym_db.RegisterMessage(Chunk)\n\nDocument = _reflection.GeneratedProtocolMessageType(\'Document\', (_message.Message,), dict(\n  DESCRIPTOR = _DOCUMENT,\n  __module__ = \'gnes_pb2\'\n  # @@protoc_insertion_point(class_scope:gnes.Document)\n  ))\n_sym_db.RegisterMessage(Document)\n\nEnvelope = _reflection.GeneratedProtocolMessageType(\'Envelope\', (_message.Message,), dict(\n\n  route = _reflection.GeneratedProtocolMessageType(\'route\', (_message.Message,), dict(\n    DESCRIPTOR = _ENVELOPE_ROUTE,\n    __module__ = \'gnes_pb2\'\n    # @@protoc_insertion_point(class_scope:gnes.Envelope.route)\n    ))\n  ,\n  DESCRIPTOR = _ENVELOPE,\n  __module__ = \'gnes_pb2\'\n  # @@protoc_insertion_point(class_scope:gnes.Envelope)\n  ))\n_sym_db.RegisterMessage(Envelope)\n_sym_db.RegisterMessage(Envelope.route)\n\nMessage = _reflection.GeneratedProtocolMessageType(\'Message\', (_message.Message,), dict(\n  DESCRIPTOR = _MESSAGE,\n  __module__ = \'gnes_pb2\'\n  # @@protoc_insertion_point(class_scope:gnes.Message)\n  ))\n_sym_db.RegisterMessage(Message)\n\nRequest = _reflection.GeneratedProtocolMessageType(\'Request\', (_message.Message,), dict(\n\n  TrainRequest = _reflection.GeneratedProtocolMessageType(\'TrainRequest\', (_message.Message,), dict(\n    DESCRIPTOR = _REQUEST_TRAINREQUEST,\n    __module__ = \'gnes_pb2\'\n    # @@protoc_insertion_point(class_scope:gnes.Request.TrainRequest)\n    ))\n  ,\n\n  IndexRequest = _reflection.GeneratedProtocolMessageType(\'IndexRequest\', (_message.Message,), dict(\n    DESCRIPTOR = _REQUEST_INDEXREQUEST,\n    __module__ = \'gnes_pb2\'\n    # @@protoc_insertion_point(class_scope:gnes.Request.IndexRequest)\n    ))\n  ,\n\n  QueryRequest = _reflection.GeneratedProtocolMessageType(\'QueryRequest\', (_message.Message,), dict(\n    DESCRIPTOR = _REQUEST_QUERYREQUEST,\n    __module__ = \'gnes_pb2\'\n    # @@protoc_insertion_point(class_scope:gnes.Request.QueryRequest)\n    ))\n  ,\n\n  ControlRequest = _reflection.GeneratedProtocolMessageType(\'ControlRequest\', (_message.Message,), dict(\n    DESCRIPTOR = _REQUEST_CONTROLREQUEST,\n    __module__ = \'gnes_pb2\'\n    # @@protoc_insertion_point(class_scope:gnes.Request.ControlRequest)\n    ))\n  ,\n  DESCRIPTOR = _REQUEST,\n  __module__ = \'gnes_pb2\'\n  # @@protoc_insertion_point(class_scope:gnes.Request)\n  ))\n_sym_db.RegisterMessage(Request)\n_sym_db.RegisterMessage(Request.TrainRequest)\n_sym_db.RegisterMessage(Request.IndexRequest)\n_sym_db.RegisterMessage(Request.QueryRequest)\n_sym_db.RegisterMessage(Request.ControlRequest)\n\nResponse = _reflection.GeneratedProtocolMessageType(\'Response\', (_message.Message,), dict(\n\n  TrainResponse = _reflection.GeneratedProtocolMessageType(\'TrainResponse\', (_message.Message,), dict(\n    DESCRIPTOR = _RESPONSE_TRAINRESPONSE,\n    __module__ = \'gnes_pb2\'\n    # @@protoc_insertion_point(class_scope:gnes.Response.TrainResponse)\n    ))\n  ,\n\n  IndexResponse = _reflection.GeneratedProtocolMessageType(\'IndexResponse\', (_message.Message,), dict(\n    DESCRIPTOR = _RESPONSE_INDEXRESPONSE,\n    __module__ = \'gnes_pb2\'\n    # @@protoc_insertion_point(class_scope:gnes.Response.IndexResponse)\n    ))\n  ,\n\n  ControlResponse = _reflection.GeneratedProtocolMessageType(\'ControlResponse\', (_message.Message,), dict(\n    DESCRIPTOR = _RESPONSE_CONTROLRESPONSE,\n    __module__ = \'gnes_pb2\'\n    # @@protoc_insertion_point(class_scope:gnes.Response.ControlResponse)\n    ))\n  ,\n\n  QueryResponse = _reflection.GeneratedProtocolMessageType(\'QueryResponse\', (_message.Message,), dict(\n\n    ScoredResult = _reflection.GeneratedProtocolMessageType(\'ScoredResult\', (_message.Message,), dict(\n\n      Score = _reflection.GeneratedProtocolMessageType(\'Score\', (_message.Message,), dict(\n        DESCRIPTOR = _RESPONSE_QUERYRESPONSE_SCOREDRESULT_SCORE,\n        __module__ = \'gnes_pb2\'\n        # @@protoc_insertion_point(class_scope:gnes.Response.QueryResponse.ScoredResult.Score)\n        ))\n      ,\n      DESCRIPTOR = _RESPONSE_QUERYRESPONSE_SCOREDRESULT,\n      __module__ = \'gnes_pb2\'\n      # @@protoc_insertion_point(class_scope:gnes.Response.QueryResponse.ScoredResult)\n      ))\n    ,\n    DESCRIPTOR = _RESPONSE_QUERYRESPONSE,\n    __module__ = \'gnes_pb2\'\n    # @@protoc_insertion_point(class_scope:gnes.Response.QueryResponse)\n    ))\n  ,\n  DESCRIPTOR = _RESPONSE,\n  __module__ = \'gnes_pb2\'\n  # @@protoc_insertion_point(class_scope:gnes.Response)\n  ))\n_sym_db.RegisterMessage(Response)\n_sym_db.RegisterMessage(Response.TrainResponse)\n_sym_db.RegisterMessage(Response.IndexResponse)\n_sym_db.RegisterMessage(Response.ControlResponse)\n_sym_db.RegisterMessage(Response.QueryResponse)\n_sym_db.RegisterMessage(Response.QueryResponse.ScoredResult)\n_sym_db.RegisterMessage(Response.QueryResponse.ScoredResult.Score)\n\n\n_NDARRAY.fields_by_name[\'shape\']._options = None\n_CHUNK.fields_by_name[\'offset_nd\']._options = None\n\n_GNESRPC = _descriptor.ServiceDescriptor(\n  name=\'GnesRPC\',\n  full_name=\'gnes.GnesRPC\',\n  file=DESCRIPTOR,\n  index=0,\n  serialized_options=None,\n  serialized_start=2557,\n  serialized_end=2784,\n  methods=[\n  _descriptor.MethodDescriptor(\n    name=\'Train\',\n    full_name=\'gnes.GnesRPC.Train\',\n    index=0,\n    containing_service=None,\n    input_type=_REQUEST,\n    output_type=_RESPONSE,\n    serialized_options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'Index\',\n    full_name=\'gnes.GnesRPC.Index\',\n    index=1,\n    containing_service=None,\n    input_type=_REQUEST,\n    output_type=_RESPONSE,\n    serialized_options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'Query\',\n    full_name=\'gnes.GnesRPC.Query\',\n    index=2,\n    containing_service=None,\n    input_type=_REQUEST,\n    output_type=_RESPONSE,\n    serialized_options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'Call\',\n    full_name=\'gnes.GnesRPC.Call\',\n    index=3,\n    containing_service=None,\n    input_type=_REQUEST,\n    output_type=_RESPONSE,\n    serialized_options=None,\n  ),\n  _descriptor.MethodDescriptor(\n    name=\'StreamCall\',\n    full_name=\'gnes.GnesRPC.StreamCall\',\n    index=4,\n    containing_service=None,\n    input_type=_REQUEST,\n    output_type=_RESPONSE,\n    serialized_options=None,\n  ),\n])\n_sym_db.RegisterServiceDescriptor(_GNESRPC)\n\nDESCRIPTOR.services_by_name[\'GnesRPC\'] = _GNESRPC\n\n# @@protoc_insertion_point(module_scope)\n'"
gnes/proto/gnes_pb2_grpc.py,0,"b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\nfrom . import gnes_pb2 as gnes__pb2\n\n\nclass GnesRPCStub(object):\n  # missing associated documentation comment in .proto file\n  pass\n\n  def __init__(self, channel):\n    """"""Constructor.\n\n    Args:\n      channel: A grpc.Channel.\n    """"""\n    self.Train = channel.unary_unary(\n        \'/gnes.GnesRPC/Train\',\n        request_serializer=gnes__pb2.Request.SerializeToString,\n        response_deserializer=gnes__pb2.Response.FromString,\n        )\n    self.Index = channel.unary_unary(\n        \'/gnes.GnesRPC/Index\',\n        request_serializer=gnes__pb2.Request.SerializeToString,\n        response_deserializer=gnes__pb2.Response.FromString,\n        )\n    self.Query = channel.unary_unary(\n        \'/gnes.GnesRPC/Query\',\n        request_serializer=gnes__pb2.Request.SerializeToString,\n        response_deserializer=gnes__pb2.Response.FromString,\n        )\n    self.Call = channel.unary_unary(\n        \'/gnes.GnesRPC/Call\',\n        request_serializer=gnes__pb2.Request.SerializeToString,\n        response_deserializer=gnes__pb2.Response.FromString,\n        )\n    self.StreamCall = channel.stream_stream(\n        \'/gnes.GnesRPC/StreamCall\',\n        request_serializer=gnes__pb2.Request.SerializeToString,\n        response_deserializer=gnes__pb2.Response.FromString,\n        )\n\n\nclass GnesRPCServicer(object):\n  # missing associated documentation comment in .proto file\n  pass\n\n  def Train(self, request, context):\n    """"""option (rpc_core.method_no_deadline) = true;\n    option (rpc_core.service_default_deadline_ms) = 5000;\n    """"""\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def Index(self, request, context):\n    # missing associated documentation comment in .proto file\n    pass\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def Query(self, request, context):\n    # missing associated documentation comment in .proto file\n    pass\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def Call(self, request, context):\n    # missing associated documentation comment in .proto file\n    pass\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n  def StreamCall(self, request_iterator, context):\n    # missing associated documentation comment in .proto file\n    pass\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n\ndef add_GnesRPCServicer_to_server(servicer, server):\n  rpc_method_handlers = {\n      \'Train\': grpc.unary_unary_rpc_method_handler(\n          servicer.Train,\n          request_deserializer=gnes__pb2.Request.FromString,\n          response_serializer=gnes__pb2.Response.SerializeToString,\n      ),\n      \'Index\': grpc.unary_unary_rpc_method_handler(\n          servicer.Index,\n          request_deserializer=gnes__pb2.Request.FromString,\n          response_serializer=gnes__pb2.Response.SerializeToString,\n      ),\n      \'Query\': grpc.unary_unary_rpc_method_handler(\n          servicer.Query,\n          request_deserializer=gnes__pb2.Request.FromString,\n          response_serializer=gnes__pb2.Response.SerializeToString,\n      ),\n      \'Call\': grpc.unary_unary_rpc_method_handler(\n          servicer.Call,\n          request_deserializer=gnes__pb2.Request.FromString,\n          response_serializer=gnes__pb2.Response.SerializeToString,\n      ),\n      \'StreamCall\': grpc.stream_stream_rpc_method_handler(\n          servicer.StreamCall,\n          request_deserializer=gnes__pb2.Request.FromString,\n          response_serializer=gnes__pb2.Response.SerializeToString,\n      ),\n  }\n  generic_handler = grpc.method_handlers_generic_handler(\n      \'gnes.GnesRPC\', rpc_method_handlers)\n  server.add_generic_rpc_handlers((generic_handler,))\n'"
gnes/router/__init__.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\n# A key-value map for Class to the (module)file it located in\nfrom ..base import register_all_class\n\n_cls2file_map = {\n    \'BaseRouter\': \'base\',\n    \'BaseMapRouter\': \'base\',\n    \'BaseReduceRouter\': \'base\',\n    \'BaseTopkReduceRouter\': \'base\',\n    \'BaseEmbedReduceRouter\': \'base\',\n    \'DocTopkReducer\': \'reduce\',\n    \'ChunkTopkReducer\': \'reduce\',\n    \'DocFillReducer\': \'reduce\',\n    \'PublishRouter\': \'map\',\n    \'DocBatchRouter\': \'map\',\n    \'ConcatEmbedRouter\': \'reduce\',\n    \'AvgEmbedRouter\': \'reduce\'\n}\n\nregister_all_class(_cls2file_map, \'router\')\n'"
gnes/router/base.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\nfrom collections import defaultdict\nfrom typing import List, Generator\n\nfrom gnes.score_fn.base import CombinedScoreFn\nfrom ..base import TrainableBase, CompositionalTrainableBase\nfrom ..proto import gnes_pb2, merge_routes, array2blob\n\n\nclass BaseRouter(TrainableBase):\n    """""" Base class for the router. Inherit from this class to create a new router.\n\n    Router forwards messages between services. Essentially, it receives a \'gnes_pb2.Message\'\n    and call `apply()` method on it.\n    """"""\n\n    def apply(self, msg: \'gnes_pb2.Message\', *args, **kwargs):\n        """"""\n        Modify the incoming message\n\n        :param msg: incoming message\n        """"""\n        pass\n\n\nclass BaseMapRouter(BaseRouter):\n    def apply(self, msg: \'gnes_pb2.Message\', *args, **kwargs) -> Generator:\n        pass\n\n\nclass BaseReduceRouter(BaseRouter):\n    def apply(self, msg: \'gnes_pb2.Message\', accum_msgs: List[\'gnes_pb2.Message\'], *args, **kwargs) -> None:\n        """"""\n        Modify the current message based on accumulated messages\n\n        :param msg: the current message\n        :param accum_msgs: accumulated messages\n        """"""\n        merge_routes(msg, accum_msgs)\n        if len(msg.envelope.num_part) > 1:\n            msg.envelope.num_part.pop()\n        else:\n            self.logger.warning(\n                \'message envelope says num_part=%s, means no further message reducing. \'\n                \'ignore this if you explicitly set ""num_part"" in RouterService\' % msg.envelope.num_part)\n\n\nclass BaseTopkReduceRouter(BaseReduceRouter):\n    def __init__(self, reduce_op: str = \'sum\', *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._reduce_op = reduce_op\n\n    def post_init(self):\n        self.reduce_op = CombinedScoreFn(score_mode=self._reduce_op)\n\n    def get_key(self, x: \'gnes_pb2.Response.QueryResponse.ScoredResult\') -> str:\n        raise NotImplementedError\n\n    def set_key(self, x: \'gnes_pb2.Response.QueryResponse.ScoredResult\', k: str) -> None:\n        raise NotImplementedError\n\n    def apply(self, msg: \'gnes_pb2.Message\', accum_msgs: List[\'gnes_pb2.Message\'], *args, **kwargs):\n        # now convert chunk results to doc results\n        all_scored_results = [sr for m in accum_msgs for sr in m.response.search.topk_results]\n        score_dict = defaultdict(list)\n\n        # count score by iterating over chunks\n        for c in all_scored_results:\n            score_dict[self.get_key(c)].append(c.score)\n\n        for k, v in score_dict.items():\n            score_dict[k] = self.reduce_op(*v)\n\n        msg.response.search.ClearField(\'topk_results\')\n\n        for k, v in score_dict.items():\n            r = msg.response.search.topk_results.add()\n            r.score.CopyFrom(v)\n            self.set_key(r, k)\n\n        super().apply(msg, accum_msgs)\n\n\nclass BaseEmbedReduceRouter(BaseReduceRouter):\n    def reduce_embedding(self, accum_msgs: List[\'gnes_pb2.Message\'], msg_type: str, chunk_idx: int, doc_idx: int):\n        raise NotImplementedError\n\n    def apply(self, msg: \'gnes_pb2.Message\', accum_msgs: List[\'gnes_pb2.Message\'], *args, **kwargs) -> None:\n        """"""\n        reduce embeddings from encoders (means, concat ....)\n\n        :param msg: the current message\n        :param accum_msgs: accumulated messages\n        """"""\n        body = getattr(msg, msg.WhichOneof(\'body\'))\n        msg_type = type(getattr(body, body.WhichOneof(\'body\')))\n        if msg_type == gnes_pb2.Request.QueryRequest:\n            for i in range(len(msg.request.search.query.chunks)):\n                reduced_embedding = array2blob(self.reduce_embedding(accum_msgs, \'query\', chunk_idx=i, doc_idx=-1))\n                msg.request.search.query.chunks[i].embedding.CopyFrom(reduced_embedding)\n        elif msg_type == gnes_pb2.Request.IndexRequest:\n            for i in range(len(msg.request.index.docs)):\n                for j in range(len(msg.request.index.docs[i].chunks)):\n                    reduced_embedding = array2blob(self.reduce_embedding(accum_msgs, \'index\', chunk_idx=j, doc_idx=i))\n                    msg.request.index.docs[i].chunks[j].embedding.CopyFrom(reduced_embedding)\n        else:\n            self.logger.error(\'dont know how to handle %s\' % msg_type)\n\n        super().apply(msg, accum_msgs)\n\n\nclass PipelineRouter(CompositionalTrainableBase):\n    def apply(self, *args, **kwargs) -> None:\n        if not self.components:\n            raise NotImplementedError\n        for be in self.components:\n            be.apply(*args, **kwargs)\n'"
gnes/router/map.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nfrom typing import Generator\n\nfrom .base import BaseMapRouter\nfrom ..helper import batch_iterator\nfrom ..proto import gnes_pb2\n\n\nclass BlockRouter(BaseMapRouter):\n    """"""Wait for \'sleep_sec\' seconds and forward messages, useful for benchmark""""""\n\n    def __init__(self, sleep_sec: int = 5, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.sleep_sec = sleep_sec\n\n    def apply(self, msg: \'gnes_pb2.Message\', *args, **kwargs):\n        import time\n        time.sleep(self.sleep_sec)\n\n\nclass PublishRouter(BaseMapRouter):\n    """"""Copy a message \'num_part\' time and forward it, useful for PUB-SUB sockets.\n    \'num_part\' is an indicator for downstream sync-barrier, e.g. a ReduceRouter\n    """"""\n\n    def __init__(self, num_part: int, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.num_part = num_part\n\n    def apply(self, msg: \'gnes_pb2.Message\', *args, **kwargs) -> Generator:\n        msg.envelope.num_part.append(self.num_part)\n\n\nclass DocBatchRouter(BaseMapRouter):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def apply(self, msg: \'gnes_pb2.Message\', *args, **kwargs) -> Generator:\n        if self.batch_size and self.batch_size > 0:\n            batches = [b for b in batch_iterator(msg.request.index.docs, self.batch_size)]\n            num_part = len(batches)\n            for p_idx, b in enumerate(batches, start=1):\n                _msg = gnes_pb2.Message()\n                _msg.CopyFrom(msg)\n                _msg.request.index.ClearField(\'docs\')\n                _msg.request.index.docs.extend(b)\n                _msg.envelope.part_id = p_idx\n                _msg.envelope.num_part.append(num_part)\n                yield _msg\n'"
gnes/router/reduce.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nfrom typing import List\nimport numpy as np\n\nfrom .base import BaseReduceRouter, BaseTopkReduceRouter, BaseEmbedReduceRouter\nfrom ..proto import blob2array\n\n\nclass DocFillReducer(BaseReduceRouter):\n    """"""\n    Gather all documents raw content from multiple shards.\n    This is only useful when you have\n    - multiple doc-indexer and docs are spreaded over multiple shards.\n    - require full-doc retrieval with the original content, not just an doc id\n    Ideally, only each doc can only belong to one shard.\n    """"""\n    def apply(self, msg: \'gnes_pb2.Message\', accum_msgs: List[\'gnes_pb2.Message\'], *args, **kwargs):\n        final_docs = []\n        for idx in range(len(msg.response.search.topk_results)):\n            # get result from all shards, some may return None, we only take the first non-None doc\n            final_docs.append([m.response.search.topk_results[idx] for m in accum_msgs if\n                               m.response.search.topk_results[idx].doc.WhichOneof(\'raw_data\') is not None][0])\n        msg.response.search.ClearField(\'topk_results\')\n        msg.response.search.topk_results.extend(final_docs)\n\n        super().apply(msg, accum_msgs)\n\n\nclass DocTopkReducer(BaseTopkReduceRouter):\n    """"""\n    Gather all docs by their doc_id, result in a topk doc list\n    """"""\n\n    def get_key(self, x: \'gnes_pb2.Response.QueryResponse.ScoredResult\') -> str:\n        return x.doc.doc_id\n\n    def set_key(self, x: \'gnes_pb2.Response.QueryResponse.ScoredResult\', k: str):\n        x.doc.doc_id = k\n\n\nclass Chunk2DocTopkReducer(BaseTopkReduceRouter):\n    """"""\n    Gather all chunks by their doc_id, result in a topk doc list.\n    This is almost always useful, as the final result should be group by doc_id\n    not chunk\n    """"""\n\n    def get_key(self, x: \'gnes_pb2.Response.QueryResponse.ScoredResult\') -> str:\n        return x.chunk.doc_id\n\n    def set_key(self, x: \'gnes_pb2.Response.QueryResponse.ScoredResult\', k: str):\n        x.doc.doc_id = k\n\n\nclass ChunkTopkReducer(BaseTopkReduceRouter):\n    """"""\n    Gather all chunks by their chunk_id from all shards, aka doc_id-offset, result in a topk chunk list\n    """"""\n\n    def get_key(self, x: \'gnes_pb2.Response.QueryResponse.ScoredResult\') -> str:\n        return \'%d-%d\' % (x.chunk.doc_id, x.chunk.offset)\n\n    def set_key(self, x: \'gnes_pb2.Response.QueryResponse.ScoredResult\', k: str):\n        x.chunk.doc_id, x.chunk.offset = map(int, k.split(\'-\'))\n\n\nclass ConcatEmbedRouter(BaseEmbedReduceRouter):\n    """"""\n    Gather all embeddings from multiple encoders and concat them on a specific axis.\n    In default, concat will happen on the last axis.\n    chunk_idx, doc_idx denote index in for loop used in BaseEmbedReduceRouter\n    """"""\n\n    def reduce_embedding(self, accum_msgs: List[\'gnes_pb2.Message\'], msg_type: str, chunk_idx: int, doc_idx: int):\n        if msg_type == \'query\':\n            return np.concatenate([blob2array(m.request.search.query.chunks[chunk_idx].embedding)\n                                   for m in accum_msgs], axis=1)\n        elif msg_type == \'index\':\n            return np.concatenate([blob2array(m.request.index.docs[doc_idx].chunks[chunk_idx].embedding)\n                                   for m in accum_msgs], axis=1)\n        else:\n            self.logger.error(\'dont know how to handle %s\' % msg_type)\n\n\nclass AvgEmbedRouter(BaseEmbedReduceRouter):\n    """"""\n    Gather all embeddings from multiple encoders and do average on a specific axis.\n    In default, average will happen on the first axis.\n    chunk_idx, doc_idx denote index in for loop used in BaseEmbedReduceRouter\n    """"""\n\n    def reduce_embedding(self, accum_msgs: List[\'gnes_pb2.Message\'], msg_type: str, chunk_idx: int, doc_idx: int):\n        if msg_type == \'query\':\n            return np.mean([blob2array(m.request.search.query.chunks[chunk_idx].embedding)\n                                   for m in accum_msgs], axis=0)\n        elif msg_type == \'index\':\n            return np.mean([blob2array(m.request.index.docs[doc_idx].chunks[chunk_idx].embedding)\n                                   for m in accum_msgs], axis=0)\n        else:\n            self.logger.error(\'dont know how to handle %s\' % msg_type)'"
gnes/score_fn/__init__.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n# A key-value map for Class to the (module)file it located in\nfrom ..base import register_all_class\n\n_cls2file_map = {\n    \'BaseScoreFn\': \'base\',\n    \'CombinedScoreFn\': \'base\',\n    \'ModifierScoreFn\': \'base\',\n    \'WeightedChunkScoreFn\': \'chunk\',\n    \'WeightedDocScoreFn\': \'doc\',\n    \'Normalizer1\': \'normalize\',\n    \'Normalizer2\': \'normalize\',\n    \'Normalizer3\': \'normalize\',\n    \'Normalizer4\': \'normalize\',\n    \'Normalizer5\': \'normalize\',\n}\n\nregister_all_class(_cls2file_map, \'score_fn\')\n'"
gnes/score_fn/base.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport json\nfrom typing import Sequence\n\nimport numpy as np\n\nfrom ..base import TrainableBase\nfrom ..proto import gnes_pb2\n\n\ndef get_unary_score(value: float, **kwargs):\n    score = gnes_pb2.Response.QueryResponse.ScoredResult.Score()\n    score.value = value\n    score.explained = json.dumps(\n        dict(value=float(value),\n             **kwargs))\n    return score\n\n\nclass BaseScoreFn(TrainableBase):\n    """"""Base score function. A score function must implement __call__ method""""""\n\n    warn_unnamed = False\n\n    def __init__(self, context=None, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._context = context\n\n    def __call__(self, *args, **kwargs) -> \'gnes_pb2.Response.QueryResponse.ScoredResult.Score\':\n        raise NotImplementedError\n\n    def new_score(self, *, operands: Sequence[\'gnes_pb2.Response.QueryResponse.ScoredResult.Score\'] = (), **kwargs):\n        if not self.__doc__:\n            raise NotImplementedError(\'%s dont have docstring. For the sake of interpretability, \'\n                                      \'please write docstring for this class\')\n        return get_unary_score(name=self.__class__.__name__,\n                               docstring=\' \'.join(self.__doc__.split()).strip(),\n                               operands=[json.loads(s.explained) for s in operands],\n                               **kwargs)\n\n\nclass CombinedScoreFn(BaseScoreFn):\n    """"""Combine multiple scores into one score, defaults to \'multiply\'""""""\n\n    def __init__(self, score_mode: str = \'multiply\', *args, **kwargs):\n        """"""\n        :param score_mode: specifies how the computed scores are combined\n        """"""\n        super().__init__(*args, **kwargs)\n        if score_mode not in self.supported_ops:\n            raise AttributeError(\n                \'score_mode=%s is not supported! must be one of %s\' % (score_mode, self.supported_ops.keys()))\n        self.score_mode = score_mode\n\n    @property\n    def supported_ops(self):\n        return {\n            \'multiply\': np.prod,\n            \'sum\': np.sum,\n            \'max\': np.max,\n            \'min\': np.min,\n            \'avg\': np.mean,\n        }\n\n    def post_init(self):\n        self.op = self.supported_ops[self.score_mode]\n\n    def __call__(self, *last_scores, **kwargs) -> \'gnes_pb2.Response.QueryResponse.ScoredResult.Score\':\n        return self.new_score(\n            value=self.op([s.value for s in last_scores]),\n            operands=last_scores,\n            score_mode=self.score_mode)\n\n\nclass ModifierScoreFn(BaseScoreFn):\n    """"""Modifier to apply to the value\n    score = modifier(factor * value)\n    """"""\n\n    def __init__(self, modifier: str = \'none\', factor: float = 1.0, factor_name: str = \'GivenConstant\',\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if modifier not in self.supported_ops:\n            raise AttributeError(\n                \'modifier=%s is not supported! must be one of %s\' % (modifier, self.supported_ops.keys()))\n        self._modifier = modifier\n        self._factor = factor\n        self._factor_name = factor_name\n\n    @property\n    def supported_ops(self):\n        return {\n            \'none\': lambda x: x,\n            \'log\': np.log10,\n            \'log1p\': lambda x: np.log10(x + 1),\n            \'log2p\': lambda x: np.log10(x + 2),\n            \'ln\': np.log,\n            \'ln1p\': np.log1p,\n            \'ln2p\': lambda x: np.log(x + 2),\n            \'square\': np.square,\n            \'sqrt\': np.sqrt,\n            \'reciprocal\': np.reciprocal,\n            \'reciprocal1p\': lambda x: np.reciprocal(1 + x),\n            \'abs\': np.abs,\n            \'invert\': lambda x: - x,\n            \'invert1p\': lambda x: 1 - x\n        }\n\n    def post_init(self):\n        self.factor = get_unary_score(value=self._factor, name=self._factor_name)\n        self.op = self.supported_ops[self._modifier]\n\n    def __call__(self,\n                 last_score: \'gnes_pb2.Response.QueryResponse.ScoredResult.Score\',\n                 *args, **kwargs) -> \\\n            \'gnes_pb2.Response.QueryResponse.ScoredResult.Score\':\n        if self._modifier == \'none\' and self._factor == 1.0:\n            return last_score\n        else:\n            return self.new_score(\n                value=self.op(self.factor.value * last_score.value),\n                operands=[last_score],\n                modifier=self._modifier,\n                factor=json.loads(self.factor.explained))\n\n\nclass ScoreOps:\n    multiply = CombinedScoreFn(\'multiply\')\n    sum = CombinedScoreFn(\'sum\')\n    max = CombinedScoreFn(\'max\')\n    min = CombinedScoreFn(\'min\')\n    avg = CombinedScoreFn(\'avg\')\n    none = ModifierScoreFn(\'none\')\n    log = ModifierScoreFn(\'log\')\n    log1p = ModifierScoreFn(\'log1p\')\n    log2p = ModifierScoreFn(\'log2p\')\n    ln = ModifierScoreFn(\'ln\')\n    ln1p = ModifierScoreFn(\'ln1p\')\n    ln2p = ModifierScoreFn(\'ln2p\')\n    square = ModifierScoreFn(\'square\')\n    sqrt = ModifierScoreFn(\'sqrt\')\n    abs = ModifierScoreFn(\'abs\')\n    reciprocal = ModifierScoreFn(\'reciprocal\')\n    reciprocal1p = ModifierScoreFn(\'reciprocal1p\')\n'"
gnes/score_fn/chunk.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nfrom .base import get_unary_score, CombinedScoreFn\nfrom typing import List, Tuple\nimport numpy as np\n\n\nclass WeightedChunkScoreFn(CombinedScoreFn):\n    """"""score = d_chunk.weight * relevance * q_chunk.weight""""""\n\n    def __call__(self, last_score: \'gnes_pb2.Response.QueryResponse.ScoredResult.Score\',\n                 q_chunk: \'gnes_pb2.Chunk\',\n                 d_chunk: \'gnes_pb2.Chunk\', *args, **kwargs):\n        q_chunk_weight = get_unary_score(value=q_chunk.weight,\n                                         name=\'query chunk weight\',\n                                         offset=q_chunk.offset)\n        d_chunk_weight = get_unary_score(value=d_chunk.weight,\n                                         name=\'document chunk weight\',\n                                         doc_id=d_chunk.doc_id,\n                                         offset=d_chunk.offset)\n\n        return super().__call__(last_score, q_chunk_weight, d_chunk_weight)\n\n\nclass WeightedChunkOffsetScoreFn(CombinedScoreFn):\n    """"""\n    score = d_chunk.weight * relevance * offset_divergence * q_chunk.weight\n    offset_divergence is calculated based on doc_type:\n        TEXT && VIDEO && AUDIO: offset is 1-D\n        IMAGE: offset is 2-D\n    """"""\n\n    def __call__(self, last_score: \'gnes_pb2.Response.QueryResponse.ScoredResult.Score\',\n                 q_chunk: \'gnes_pb2.Chunk\',\n                 d_chunk: \'gnes_pb2.Chunk\', *args, **kwargs):\n        q_chunk_weight = get_unary_score(value=q_chunk.weight,\n                                         name=\'query chunk weight\',\n                                         offset=str(q_chunk.offset))\n        d_chunk_weight = get_unary_score(value=d_chunk.weight,\n                                         name=\'document chunk weight\',\n                                         doc_id=d_chunk.doc_id,\n                                         offset=str(d_chunk.offset))\n        offset_divergence = get_unary_score(value=self._cal_divergence(q_chunk, d_chunk),\n                                            name=\'offset divergence\')\n        return super().__call__(last_score, q_chunk_weight, d_chunk_weight, offset_divergence)\n\n    @staticmethod\n    def _cal_divergence(q_chunk: \'gnes_pb2.Chunk\', d_chunk: \'gnes_pb2.Chunk\'):\n        if q_chunk.offset_nd and d_chunk.offset_nd:\n            return 1 / (1 + np.sqrt((q_chunk.offset_nd[0] - d_chunk.offset_nd[0]) ** 2 +\n                                    (q_chunk.offset_nd[1] - d_chunk.offset_nd[1]) ** 2))\n        else:\n            return np.abs(q_chunk.offset - d_chunk.offset)\n\n\nclass CoordChunkScoreFn(CombinedScoreFn):\n    """"""\n    score = relevance * query_coordination\n    query_coordination: #chunks return / #chunks in this doc(query doc)\n    """"""\n\n    def __call__(self, last_score: \'gnes_pb2.Response.QueryResponse.ScoredResult.Score\',\n                 q_chunk: \'gnes_pb2.Chunk\',\n                 d_chunk: \'gnes_pb2.Chunk\',\n                 queried_results: List[List[Tuple]],\n                 *args, **kwargs):\n        query_coordination = get_unary_score(value=self._cal_query_coord(d_chunk, queried_results),\n                                             name=\'query coordination\')\n        return super().__call__(last_score, query_coordination)\n\n    def _cal_query_coord(self, d_chunk: \'gnes_pb2.Chunk\', queried_results: List[List[Tuple]]):\n        doc_id = d_chunk.doc_id\n        total_chunks = self._context.num_chunks_in_doc(doc_id)\n        queried_doc_id, _, _, _ = zip(*(queried_results[0]))\n        recall_chunks = queried_doc_id.count(doc_id)\n        return recall_chunks / total_chunks\n\n\nclass TFIDFChunkScoreFn(CombinedScoreFn):\n    """"""\n    score = relevance * tf(q_chunk) * (idf(q_chunk)**2)\n    tf(q_chunk) is calculated based on the relevance of query result.\n    tf(q_chunk) = number of queried chunks where relevance >= threshold\n    idf(q_chunk) = log(total_chunks / tf(q_chunk) + 1)\n    """"""\n\n    def __init__(self, threshold: float = 0.8, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.threshold = threshold\n\n    def __call__(self, last_score: \'gnes_pb2.Response.QueryResponse.ScoredResult.Score\',\n                 q_chunk: \'gnes_pb2.Chunk\',\n                 d_chunk: \'gnes_pb2.Chunk\',\n                 queried_results: List[List[Tuple]],\n                 *args, **kwargs):\n        tf_idf = get_unary_score(value=self._cal_tf_idf(queried_results),\n                                             name=\'query tf-idf\')\n        return super().__call__(last_score, tf_idf)\n\n    def _cal_tf_idf(self, queried_results: List[List[Tuple]]):\n        _, _, _, queried_relevance = zip(*(queried_results[0]))\n        tf = len(list(filter(lambda x: x >= self.threshold, queried_relevance)))\n\n        total_chunks = self._context.num_chunks\n        idf = np.log10(total_chunks / (tf + 1))\n        return tf * (idf ** 2)\n\n\nclass BM25ChunkScoreFn(CombinedScoreFn):\n    """"""\n    score = relevance * idf(q_chunk) * tf(q_chunk) * (k1 + 1) / (tf(q_chunk) +\n                            k1 * (1 - b + b * (chunk_in_doc / avg_chunk_in_doc)))\n\n    in bm25 algorithm:\n             idf(q_chunk) = log(1 + (doc_count - f(q_chunk) +0.5) / (f(q_chunk) + 0.5)),\n    where f(q_chunk) is number of docs that contains q_chunk. In our system, this denotes number of docs\n    appearing in query results.\n\n    In elastic search, b = 0.75, k1 = 1.2\n    """"""\n\n    def __init__(self, threshold: float = 0.8, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.threshold = threshold\n        self.k1 = 1.2\n        self.b = 0.75\n\n    def __call__(self, last_score: \'gnes_pb2.Response.QueryResponse.ScoredResult.Score\',\n                 q_chunk: \'gnes_pb2.Chunk\',\n                 d_chunk: \'gnes_pb2.Chunk\',\n                 queried_results: List[List[Tuple]],\n                 *args, **kwargs):\n        bm25 = get_unary_score(value=self._cal_bm25(d_chunk, queried_results),\n                                             name=\'query bm25\')\n        return super().__call__(last_score, bm25)\n\n    def _cal_bm25(self, d_chunk: \'gnes_pb2.Chunk\', queried_results: List[List[Tuple]]):\n        doc_id = d_chunk.doc_id\n        _, _, _, queried_relevance = zip(*(queried_results[0]))\n        tf = len(list(filter(lambda x: x >= self.threshold, queried_relevance)))\n\n        total_chunks = self._context.num_chunks\n        idf = np.log10(1 + (total_chunks - tf + 0.5) / (tf + 0.5))\n        return idf * tf * (self.k1 + 1) / (tf + self.k1 * (1 - self.b + self.b *\n                                (self._context.num_chunks_in_doc(doc_id) * self._context.num_docs / self._context.num_chunks)))\n'"
gnes/score_fn/doc.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nfrom .base import get_unary_score, CombinedScoreFn\nimport json\n\n\nclass WeightedDocScoreFn(CombinedScoreFn):\n    def __call__(self, last_score: \'gnes_pb2.Response.QueryResponse.ScoredResult.Score\',\n                 doc: \'gnes_pb2.Document\', *args, **kwargs):\n        d_weight = get_unary_score(value=doc.weight,\n                                   name=\'doc weight\',\n                                   doc_id=doc.doc_id)\n        return super().__call__(last_score, d_weight)\n\n\nclass CoordDocScoreFn(CombinedScoreFn):\n    """"""\n    score = score * query_coordination\n    query_coordination: #chunks recalled / #chunks in this doc\n    """"""\n\n    def __call__(self, last_score: \'gnes_pb2.Response.QueryResponse.ScoredResult.Score\',\n                 doc: \'gnes_pb2.Document\',\n                 *args, **kwargs):\n        total_chunks = len(doc.chunks)\n        recall_chunks = len(json.loads(last_score.explained)[\'operands\'])\n        query_coord = 1 if total_chunks == 0 else recall_chunks / total_chunks\n        d_weight = get_unary_score(value=query_coord,\n                                   name=\'query coordination\')\n        return super().__call__(last_score, d_weight)\n\n'"
gnes/score_fn/normalize.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nfrom .base import ModifierScoreFn, ScoreOps as so\n\n\nclass Normalizer1(ModifierScoreFn):\n    """"""Do normalizing: score = 1 / (1 + sqrt(score))""""""\n\n    def __init__(self):\n        super().__init__(modifier=\'reciprocal1p\')\n\n    def __call__(self, last_score, *args, **kwargs):\n        return super().__call__(so.sqrt(last_score))\n\n\nclass Normalizer2(ModifierScoreFn):\n    """"""Do normalizing: score = 1 / (1 + score / num_dim)""""""\n\n    def __init__(self, num_dim: int):\n        super().__init__(modifier=\'reciprocal1p\', factor=1.0 / num_dim, factor_name=\'1/num_dim\')\n\n\nclass Normalizer3(Normalizer2):\n    """"""Do normalizing: score = 1 / (1 + sqrt(score) / num_dim)""""""\n\n    def __call__(self, last_score, *args, **kwargs):\n        return super().__call__(so.sqrt(last_score))\n\n\nclass Normalizer4(ModifierScoreFn):\n    """"""Do normalizing: score = 1 - score / num_bytes """"""\n\n    def __init__(self, num_bytes: int):\n        super().__init__(modifier=\'invert1p\', factor=1.0 / num_bytes, factor_name=\'1/num_bytes\')\n\n\nclass Normalizer5(ModifierScoreFn):\n    """"""Do normalizing: score = 1 / (1 + sqrt(abs(score)))""""""\n\n    def __init__(self):\n        super().__init__(modifier=\'reciprocal1p\')\n\n    def __call__(self, last_score, *args, **kwargs):\n        return super().__call__(so.sqrt(so.abs(last_score)))\n'"
gnes/service/__init__.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n'"
gnes/service/base.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport copy\nimport multiprocessing\nimport os\nimport random\nimport tempfile\nimport threading\nimport time\nimport types\nimport uuid\nfrom contextlib import ExitStack\nfrom enum import Enum\nfrom typing import Tuple, List, Union, Type\n\nimport zmq\nimport zmq.decorators as zmqd\nfrom termcolor import colored\n\nfrom ..base import TrainableBase, T\nfrom ..cli.parser import resolve_yaml_path\nfrom ..helper import set_logger, PathImporter, TimeContext, make_route_table\nfrom ..proto import gnes_pb2, add_route, send_message, recv_message, router2str\n\n\nclass BetterEnum(Enum):\n    def __str__(self):\n        return self.name\n\n    @classmethod\n    def from_string(cls, s):\n        try:\n            return cls[s]\n        except KeyError:\n            raise ValueError(\'%s is not a valid enum for %s\' % (s, cls))\n\n\nclass ReduceOp(BetterEnum):\n    CONCAT = 0\n    ALWAYS_ONE = 1\n\n\nclass ParallelType(BetterEnum):\n    PUSH_BLOCK = 0\n    PUSH_NONBLOCK = 1\n    PUB_BLOCK = 2\n    PUB_NONBLOCK = 3\n\n    @property\n    def is_push(self):\n        return self.value == 0 or self.value == 1\n\n    @property\n    def is_block(self):\n        return self.value == 0 or self.value == 2\n\n\nclass SocketType(BetterEnum):\n    PULL_BIND = 0\n    PULL_CONNECT = 1\n    PUSH_BIND = 2\n    PUSH_CONNECT = 3\n    SUB_BIND = 4\n    SUB_CONNECT = 5\n    PUB_BIND = 6\n    PUB_CONNECT = 7\n    PAIR_BIND = 8\n    PAIR_CONNECT = 9\n\n    @property\n    def is_bind(self):\n        return self.value % 2 == 0\n\n    @property\n    def paired(self):\n        return {\n            SocketType.PULL_BIND: SocketType.PUSH_CONNECT,\n            SocketType.PULL_CONNECT: SocketType.PUSH_BIND,\n            SocketType.SUB_BIND: SocketType.PUB_CONNECT,\n            SocketType.SUB_CONNECT: SocketType.PUB_BIND,\n            SocketType.PAIR_BIND: SocketType.PAIR_CONNECT,\n            SocketType.PUSH_CONNECT: SocketType.PULL_BIND,\n            SocketType.PUSH_BIND: SocketType.PULL_CONNECT,\n            SocketType.PUB_CONNECT: SocketType.SUB_BIND,\n            SocketType.PUB_BIND: SocketType.SUB_CONNECT,\n            SocketType.PAIR_CONNECT: SocketType.PAIR_BIND\n        }[self]\n\n\nclass BlockMessage(Exception):\n    pass\n\n\nclass ComponentNotLoad(Exception):\n    pass\n\n\nclass ServiceError(Exception):\n    pass\n\n\nclass EventLoopEnd(Exception):\n    pass\n\n\ndef get_random_ipc() -> str:\n    try:\n        tmp = os.environ[\'GNES_IPC_SOCK_TMP\']\n        if not os.path.exists(tmp):\n            raise ValueError(\'This directory for sockets ({}) does not seems to exist.\'.format(tmp))\n        tmp = os.path.join(tmp, str(uuid.uuid1())[:8])\n    except KeyError:\n        tmp = tempfile.NamedTemporaryFile().name\n    return \'ipc://%s\' % tmp\n\n\ndef build_socket(ctx: \'zmq.Context\', host: str, port: int,\n                 socket_type: \'SocketType\', identity: \'str\' = None, use_ipc: bool = False) -> Tuple[\'zmq.Socket\', str]:\n    sock = {\n        SocketType.PULL_BIND: lambda: ctx.socket(zmq.PULL),\n        SocketType.PULL_CONNECT: lambda: ctx.socket(zmq.PULL),\n        SocketType.SUB_BIND: lambda: ctx.socket(zmq.SUB),\n        SocketType.SUB_CONNECT: lambda: ctx.socket(zmq.SUB),\n        SocketType.PUB_BIND: lambda: ctx.socket(zmq.PUB),\n        SocketType.PUB_CONNECT: lambda: ctx.socket(zmq.PUB),\n        SocketType.PUSH_BIND: lambda: ctx.socket(zmq.PUSH),\n        SocketType.PUSH_CONNECT: lambda: ctx.socket(zmq.PUSH),\n        SocketType.PAIR_BIND: lambda: ctx.socket(zmq.PAIR),\n        SocketType.PAIR_CONNECT: lambda: ctx.socket(zmq.PAIR)\n    }[socket_type]()\n    sock.setsockopt(zmq.LINGER, 0)\n\n    if socket_type.is_bind:\n        if use_ipc:\n            sock.bind(host)\n        else:\n            host = BaseService.default_host\n            if port is None:\n                sock.bind_to_random_port(\'tcp://%s\' % host)\n            else:\n                sock.bind(\'tcp://%s:%d\' % (host, port))\n    else:\n        if port is None:\n            sock.connect(host)\n        else:\n            sock.connect(\'tcp://%s:%d\' % (host, port))\n\n    if socket_type in {SocketType.SUB_CONNECT, SocketType.SUB_BIND}:\n        sock.setsockopt(zmq.SUBSCRIBE, identity.encode(\'ascii\') if identity else b\'\')\n        # sock.setsockopt(zmq.SUBSCRIBE, b\'\')\n\n    # Note: the following very dangerous for pub-sub socketc\n    sock.setsockopt(zmq.RCVHWM, 10)\n    sock.setsockopt(zmq.RCVBUF, 10 * 1024 * 1024)  # limit of network buffer 100M\n\n    sock.setsockopt(zmq.SNDHWM, 10)\n    sock.setsockopt(zmq.SNDBUF, 10 * 1024 * 1024)  # limit of network buffer 100M\n\n    return sock, sock.getsockopt_string(zmq.LAST_ENDPOINT)\n\n\nclass MessageHandler:\n    def __init__(self, mh: \'MessageHandler\' = None):\n        self.routes = {}\n        self.hooks = {\'pre\': [], \'post\': []}\n\n        if mh:\n            self.routes = copy.deepcopy(mh.routes)\n            self.hooks = copy.deepcopy(mh.hooks)\n\n        self.logger = set_logger(self.__class__.__name__)\n        self.service_context = None\n\n    def register(self, msg_type: Union[List, Tuple, type]):\n        def decorator(f):\n            if isinstance(msg_type, list) or isinstance(msg_type, tuple):\n                for m in msg_type:\n                    self.routes[m] = f.__name__\n            else:\n                self.routes[msg_type] = f.__name__\n            return f\n\n        return decorator\n\n    def register_hook(self, hook_type: Union[str, Tuple[str]], only_when_verbose: bool = False):\n        """"""\n        Register a function as a pre/post hook\n\n        :param only_when_verbose: only call the hook when verbose is true\n        :param hook_type: possible values \'pre\' or \'post\' or (\'pre\', \'post\')\n        """"""\n\n        def decorator(f):\n            if isinstance(hook_type, str) and hook_type in self.hooks:\n                self.hooks[hook_type].append((f.__name__, only_when_verbose))\n                return f\n            elif isinstance(hook_type, list) or isinstance(hook_type, tuple):\n                for h in set(hook_type):\n                    if h in self.hooks:\n                        self.hooks[h].append((f.__name__, only_when_verbose))\n                    else:\n                        raise AttributeError(\'hook type: %s is not supported\' % h)\n                return f\n            else:\n                raise TypeError(\'hook_type is in bad type: %s\' % type(hook_type))\n\n        return decorator\n\n    def call_hooks(self, msg: \'gnes_pb2.Message\', hook_type: Union[str, Tuple[str]], *args, **kwargs):\n        """"""\n        All post handler hooks are called after the handler is done but before\n        sending out the message to the next service.\n        All pre handler hooks are called after the service received a message\n        and before calling the message handler\n        """"""\n        hooks = []\n        if isinstance(hook_type, str) and hook_type in self.hooks:\n            hooks.extend(self.hooks[hook_type])\n        elif isinstance(hook_type, list) or isinstance(hook_type, tuple):\n            for h in set(hook_type):\n                if h in self.hooks:\n                    hooks.extend(self.hooks[h])\n                else:\n                    raise AttributeError(\'hook type: %s is not supported\' % h)\n        else:\n            raise TypeError(\'hook_type is in bad type: %s\' % type(hook_type))\n\n        for fn, only_verbose in hooks:\n            if (only_verbose and self.service_context.args.verbose) or (not only_verbose):\n                try:\n                    fn(msg, *args, **kwargs)\n                except Exception as ex:\n                    self.logger.warning(\'hook %s throws an exception, \'\n                                        \'this wont affect the server but you may want to pay attention\' % fn)\n                    self.logger.error(ex, exc_info=True)\n\n    def call_routes(self, msg: \'gnes_pb2.Message\'):\n        def get_default_fn(m_type):\n            self.logger.warning(\'cant find handler for message type: %s, fall back to the default handler\' % m_type)\n            f = self.routes.get(m_type, self.routes[NotImplementedError])\n            return f\n\n        if msg.WhichOneof(\'body\'):\n            body = getattr(msg, msg.WhichOneof(\'body\'))\n            if body.WhichOneof(\'body\'):\n                msg_type = type(getattr(body, body.WhichOneof(\'body\')))\n                if msg_type in self.routes:\n                    self.logger.info(\'received a %r message\' % msg_type.__name__)\n                    fn = self.routes.get(msg_type)\n                else:\n                    fn = get_default_fn(msg_type)\n            else:\n                fn = get_default_fn(type(body))\n        else:\n            fn = get_default_fn(type(msg))\n\n        self.logger.info(\'handling message with %s\' % fn.__name__)\n        return fn(msg)\n\n    def call_routes_send_back(self, msg: \'gnes_pb2.Message\', out_sock):\n        try:\n            # NOTE that msg is mutable object, it may be modified in fn()\n            ret = self.call_routes(msg)\n            if ret is None:\n                # assume \'msg\' is modified inside fn()\n                self.call_hooks(msg, hook_type=\'post\', verbose=self.service_context.args.verbose)\n                send_message(out_sock, msg, **self.service_context.send_recv_kwargs)\n            elif isinstance(ret, types.GeneratorType):\n                for r_msg in ret:\n                    self.call_hooks(msg, hook_type=\'post\', verbose=self.service_context.args.verbose)\n                    send_message(out_sock, r_msg, **self.service_context.send_recv_kwargs)\n            else:\n                raise ServiceError(\'unknown return type from the handler\')\n\n        except BlockMessage:\n            pass\n        except EventLoopEnd:\n            send_message(out_sock, msg, **self.service_context.send_recv_kwargs)\n            raise EventLoopEnd\n        except ServiceError as ex:\n            self.logger.error(ex, exc_info=True)\n\n\nclass ConcurrentService(type):\n    _dct = {}\n\n    def __new__(cls, name, bases, dct):\n        _cls = super().__new__(cls, name, bases, dct)\n        ConcurrentService._dct.update({name: {\'cls\': cls,\n                                              \'name\': name,\n                                              \'bases\': bases,\n                                              \'dct\': dct}})\n        return _cls\n\n    def __call__(cls, *args, **kwargs):\n        # switch to the new backend\n        _cls = {\n            \'thread\': threading.Thread,\n            \'process\': multiprocessing.Process\n        }[args[0].parallel_backend]\n\n        # rebuild the class according to mro\n        for c in cls.mro()[-2::-1]:\n            arg_cls = ConcurrentService._dct[c.__name__][\'cls\']\n            arg_name = ConcurrentService._dct[c.__name__][\'name\']\n            arg_dct = ConcurrentService._dct[c.__name__][\'dct\']\n            _cls = super().__new__(arg_cls, arg_name, (_cls,), arg_dct)\n\n        return type.__call__(_cls, *args, **kwargs)\n\n\nclass BaseService(metaclass=ConcurrentService):\n    handler = MessageHandler()\n    default_host = \'0.0.0.0\'\n\n    def _get_event(self):\n        if isinstance(self, threading.Thread):\n            return threading.Event()\n        elif isinstance(self, multiprocessing.Process):\n            return multiprocessing.Event()\n        else:\n            raise NotImplementedError\n\n    def __init__(self, args):\n        super().__init__()\n        if \'py_path\' in args and args.py_path:\n            PathImporter.add_modules(*args.py_path)\n        self.args = args\n        self.logger = set_logger(self.__class__.__name__, args.verbose)\n        self.is_ready = self._get_event()\n        self.is_event_loop = self._get_event()\n        self.is_model_changed = self._get_event()\n        self.is_handler_done = self._get_event()\n        self.last_dump_time = time.perf_counter()\n        self._model = None\n        self.use_event_loop = True\n        self.ctrl_with_ipc = (os.name != \'nt\') and self.args.ctrl_with_ipc\n        if self.ctrl_with_ipc:\n            self.ctrl_addr = get_random_ipc()\n        else:\n            self.ctrl_addr = \'tcp://%s:%d\' % (self.default_host, self.args.port_ctrl)\n\n        self.send_recv_kwargs = dict(\n            check_version=self.args.check_version,\n            timeout=self.args.timeout,\n            squeeze_pb=self.args.squeeze_pb)\n        self._override_handler()\n\n    def _override_handler(self):\n        # replace the function name by the function itself\n        mh = MessageHandler()\n        mh.routes = {k: getattr(self, v) for k, v in self.handler.routes.items()}\n        mh.hooks = {k: [(getattr(self, vv[0]), vv[1]) for vv in v] for k, v in self.handler.hooks.items()}\n        self.handler = mh\n\n    def run(self):\n        try:\n            self._run()\n        except Exception as ex:\n            self.logger.error(ex, exc_info=True)\n\n    def dump(self, respect_dump_interval: bool = True):\n        if (not self.args.read_only\n                and self.args.dump_interval > 0\n                and self._model\n                and self.is_model_changed.is_set()\n                and ((respect_dump_interval\n                      and (time.perf_counter() - self.last_dump_time) > self.args.dump_interval)\n                     or not respect_dump_interval)):\n            self.is_model_changed.clear()\n            self.logger.info(\'dumping changes to the model, %3.0fs since last the dump\'\n                             % (time.perf_counter() - self.last_dump_time))\n            self._model.dump()\n            self.last_dump_time = time.perf_counter()\n            self.logger.info(\'dumping finished! next dump will start in at least %3.0fs\' % self.args.dump_interval)\n\n    @handler.register_hook(hook_type=\'post\')\n    def _hook_warn_body_type_change(self, msg: \'gnes_pb2.Message\', *args, **kwargs):\n        new_type = msg.WhichOneof(\'body\')\n        if new_type != self._msg_old_type:\n            self.logger.warning(\'message body type has changed from ""%s"" to ""%s""\' % (self._msg_old_type, new_type))\n\n    @handler.register_hook(hook_type=\'post\')\n    def _hook_sort_response(self, msg: \'gnes_pb2.Message\', *args, **kwargs):\n        if \'sorted_response\' in self.args and self.args.sorted_response and msg.response.search.topk_results:\n            msg.response.search.topk_results.sort(key=lambda x: x.score.value,\n                                                  reverse=msg.response.search.is_big_score_similar)\n\n            msg.response.search.is_sorted = True\n            self.logger.info(\'sorted %d results in %s order\' %\n                             (len(msg.response.search.topk_results),\n                              \'descending\' if msg.response.search.is_big_score_similar else \'ascending\'))\n\n    @handler.register_hook(hook_type=\'pre\')\n    def _hook_add_route(self, msg: \'gnes_pb2.Message\', *args, **kwargs):\n        add_route(msg.envelope, self._model.__class__.__name__, self.args.identity)\n        self._msg_old_type = msg.WhichOneof(\'body\')\n        self.logger.info(\'a message in type: %s with route: %s\' % (self._msg_old_type, router2str(msg)))\n\n    @handler.register_hook(hook_type=\'post\')\n    def _hook_update_route_timestamp(self, msg: \'gnes_pb2.Message\', *args, **kwargs):\n        msg.envelope.routes[-1].end_time.GetCurrentTime()\n        if self.args.route_table:\n            self.logger.info(\'route: %s\' % router2str(msg))\n            self.logger.info(\'route table: \\n%s\' % make_route_table(msg.envelope.routes))\n\n    @zmqd.context()\n    def _run(self, ctx):\n        ctx.setsockopt(zmq.LINGER, 0)\n        self.handler.service_context = self\n        # print(\'!!!! t_id: %d service_context: %r\' % (threading.get_ident(), self.handler.service_context))\n        self.logger.info(\'bind sockets...\')\n        if self.ctrl_with_ipc:\n            ctrl_sock, ctrl_addr = build_socket(ctx, self.ctrl_addr, None, SocketType.PAIR_BIND,\n                                                use_ipc=self.ctrl_with_ipc)\n        else:\n            ctrl_sock, ctrl_addr = build_socket(ctx, self.default_host, self.args.port_ctrl, SocketType.PAIR_BIND)\n\n        self.logger.info(\'control over %s\' % (colored(ctrl_addr, \'yellow\')))\n\n        in_sock, _ = build_socket(ctx, self.args.host_in, self.args.port_in, self.args.socket_in,\n                                  self.args.identity)\n        self.logger.info(\'input %s:%s\' % (self.args.host_in, colored(self.args.port_in, \'yellow\')))\n\n        out_sock, _ = build_socket(ctx, self.args.host_out, self.args.port_out, self.args.socket_out,\n                                   self.args.identity)\n        self.logger.info(\'output %s:%s\' % (self.args.host_out, colored(self.args.port_out, \'yellow\')))\n\n        self.logger.info(\n            \'input %s:%s\\t output %s:%s\\t control over %s\' % (\n                self.args.host_in, colored(self.args.port_in, \'yellow\'),\n                self.args.host_out, colored(self.args.port_out, \'yellow\'),\n                colored(ctrl_addr, \'yellow\')))\n\n        poller = zmq.Poller()\n        poller.register(in_sock, zmq.POLLIN)\n        poller.register(ctrl_sock, zmq.POLLIN)\n\n        try:\n            self.post_init()\n            self.is_ready.set()\n            self.is_event_loop.set()\n            self.logger.critical(\'ready and listening\')\n            while self.is_event_loop.is_set():\n                socks = dict(poller.poll(1))\n                if socks.get(in_sock) == zmq.POLLIN:\n                    pull_sock = in_sock\n                elif socks.get(ctrl_sock) == zmq.POLLIN:\n                    pull_sock = ctrl_sock\n                else:\n                    # no message received, pass\n                    continue\n\n                if self.use_event_loop or pull_sock == ctrl_sock:\n                    with TimeContext(\'handling message\', self.logger):\n                        self.is_handler_done.clear()\n\n                        # receive message\n                        msg = recv_message(pull_sock, **self.send_recv_kwargs)\n\n                        # choose output sock\n                        if msg.request and msg.request.WhichOneof(\'body\') and \\\n                                isinstance(getattr(msg.request, msg.request.WhichOneof(\'body\')),\n                                           gnes_pb2.Request.ControlRequest):\n                            o_sock = ctrl_sock\n                        else:\n                            o_sock = out_sock\n\n                        # call pre-hooks\n                        self.handler.call_hooks(msg, hook_type=\'pre\')\n                        # call main handler and send result back\n                        self.handler.call_routes_send_back(msg, o_sock)\n\n                        self.is_handler_done.set()\n                else:\n                    self.logger.warning(\n                        \'received a new message but since ""use_event_loop=False"" I will not handle it. \'\n                        \'I will just block the thread until ""is_handler_done"" is set!\')\n                    # wait until some one else call is_handler_done.set()\n                    self.is_handler_done.wait()\n                    # clear the handler status\n                    self.is_handler_done.clear()\n\n                # block the event loop if a dump is needed\n                self.dump()\n        except EventLoopEnd:\n            self.logger.info(\'break from the event loop\')\n        except ComponentNotLoad:\n            self.logger.error(\'component can not be correctly loaded, terminated\')\n        except Exception as ex:\n            self.logger.error(\'unknown exception: %s\' % str(ex), exc_info=True)\n        finally:\n            self.is_ready.set()\n            self.is_event_loop.clear()\n            in_sock.close()\n            out_sock.close()\n            ctrl_sock.close()\n            # do not check dump_interval constraint as the last dump before close\n            self.dump(respect_dump_interval=False)\n        self.logger.critical(\'terminated\')\n\n    def post_init(self):\n        pass\n\n    def load_model(self, base_class: Type[TrainableBase], yaml_path=None) -> T:\n        try:\n            return base_class.load_yaml(self.args.yaml_path if not yaml_path else yaml_path)\n        except FileNotFoundError:\n            raise ComponentNotLoad\n\n    @handler.register(NotImplementedError)\n    def _handler_default(self, msg: \'gnes_pb2.Message\'):\n        raise NotImplementedError\n\n    @handler.register(gnes_pb2.Request.ControlRequest)\n    def _handler_control(self, msg: \'gnes_pb2.Message\'):\n        if msg.request.control.command == gnes_pb2.Request.ControlRequest.TERMINATE:\n            self.is_event_loop.clear()\n            msg.response.control.status = gnes_pb2.Response.SUCCESS\n            raise EventLoopEnd\n        elif msg.request.control.command == gnes_pb2.Request.ControlRequest.STATUS:\n            msg.response.control.status = gnes_pb2.Response.READY\n        else:\n            raise ServiceError(\'dont know how to handle %s\' % msg.request.control)\n\n    def close(self):\n        if self._model:\n            self.dump()\n            self._model.close()\n\n        if self.is_event_loop.is_set():\n            msg = gnes_pb2.Message()\n            msg.request.control.command = gnes_pb2.Request.ControlRequest.TERMINATE\n            return send_ctrl_message(self.ctrl_addr, msg, timeout=self.args.timeout)\n\n    @property\n    def status(self):\n        msg = gnes_pb2.Message()\n        msg.request.control.command = gnes_pb2.Request.ControlRequest.STATUS\n        return send_ctrl_message(self.ctrl_addr, msg, timeout=self.args.timeout)\n\n    def __enter__(self):\n        self.start()\n        self.is_ready.wait()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n\ndef send_ctrl_message(address: str, msg: \'gnes_pb2.Message\', timeout: int):\n    # control message is short, set a timeout and ask for quick response\n    with zmq.Context() as ctx:\n        ctx.setsockopt(zmq.LINGER, 0)\n        sock, _ = build_socket(ctx, address, None, SocketType.PAIR_CONNECT)\n        send_message(sock, msg, timeout)\n        r = None\n        try:\n            r = recv_message(sock, timeout)\n        except TimeoutError:\n            pass\n        finally:\n            sock.close()\n        return r\n\n\nclass ServiceManager:\n    def __init__(self, service_cls, args):\n        self.logger = set_logger(self.__class__.__name__, args.verbose)\n\n        self.services = []  # type: List[\'BaseService\']\n        if args.num_parallel > 1:\n            from .router import RouterService\n            _head_router = copy.deepcopy(args)\n            _head_router.yaml_path = resolve_yaml_path(\'BaseRouter\')\n            _head_router.port_ctrl = self._get_random_port()\n            port_out = self._get_random_port()\n            _head_router.port_out = port_out\n\n            _tail_router = copy.deepcopy(args)\n            _tail_router.yaml_path = resolve_yaml_path(\'BaseRouter\')\n            port_in = self._get_random_port()\n            _tail_router.port_in = port_in\n            _tail_router.port_ctrl = self._get_random_port()\n\n            _tail_router.socket_in = SocketType.PULL_BIND\n\n            if args.parallel_type.is_push:\n                _head_router.socket_out = SocketType.PUSH_BIND\n            else:\n                _head_router.socket_out = SocketType.PUB_BIND\n                _head_router.yaml_path = resolve_yaml_path(\n                    \'!PublishRouter {parameters: {num_part: %d}}\' % args.num_parallel)\n\n            if args.parallel_type.is_block:\n                _tail_router.yaml_path = resolve_yaml_path(\'BaseReduceRouter\')\n                _tail_router.num_part = args.num_parallel\n\n            self.services.append(RouterService(_head_router))\n            self.services.append(RouterService(_tail_router))\n\n            for _ in range(args.num_parallel):\n                _args = copy.deepcopy(args)\n                _args.port_in = port_out\n                _args.port_out = port_in\n                _args.port_ctrl = self._get_random_port()\n                _args.socket_out = SocketType.PUSH_CONNECT\n                if args.parallel_type.is_push:\n                    _args.socket_in = SocketType.PULL_CONNECT\n                else:\n                    _args.socket_in = SocketType.SUB_CONNECT\n                self.services.append(service_cls(_args))\n            self.logger.info(\'num_parallel=%d, add a router with port_in=%d and a router with port_out=%d\' % (\n                args.num_parallel, _head_router.port_in, _tail_router.port_out))\n        else:\n            self.services.append(service_cls(args))\n\n    @staticmethod\n    def _get_random_port(min_port: int = 49152, max_port: int = 65536) -> int:\n        return random.randrange(min_port, max_port)\n\n    def __enter__(self):\n        self.stack = ExitStack()\n        for s in self.services:\n            self.stack.enter_context(s)\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.stack.close()\n\n    def join(self):\n        for s in self.services:\n            s.join()\n'"
gnes/service/encoder.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nfrom typing import List, Union\n\nfrom .base import BaseService as BS, MessageHandler\nfrom ..proto import gnes_pb2, array2blob, blob2array\n\n\nclass EncoderService(BS):\n    handler = MessageHandler(BS.handler)\n\n    def post_init(self):\n        from ..encoder.base import BaseEncoder\n        self._model = self.load_model(BaseEncoder)\n        self.train_data = []\n\n    def embed_chunks_in_docs(self, docs: Union[List[\'gnes_pb2.Document\'], \'gnes_pb2.Document\'],\n                             do_encoding: bool = True,\n                             is_input_list: bool = True):\n        if not is_input_list:\n            docs = [docs]\n\n        contents = []\n        chunks = []\n\n        for d in docs:\n            if not d.chunks:\n                self.logger.warning(\'document (doc_id=%s) contains no chunks!\' % d.doc_id)\n                continue\n\n            for c in d.chunks:\n                if d.doc_type == gnes_pb2.Document.TEXT:\n                    contents.append(c.text)\n                elif d.doc_type in {gnes_pb2.Document.IMAGE, gnes_pb2.Document.VIDEO}:\n                    contents.append(blob2array(c.blob))\n                else:\n                    self.logger.warning(\n                        \'chunk content is in type: %s, dont kow how to handle that, ignored\' % c.WhichOneof(\'content\'))\n                chunks.append(c)\n\n        if do_encoding and contents:\n            try:\n                embeds = self._model.encode(contents)\n                if len(chunks) != embeds.shape[0]:\n                    self.logger.error(\n                        \'mismatched %d chunks and a %s shape embedding, \'\n                        \'the first dimension must be the same\' % (len(chunks), embeds.shape))\n                for idx, c in enumerate(chunks):\n                    c.embedding.CopyFrom(array2blob(embeds[idx]))\n            except Exception as ex:\n                self.logger.error(ex, exc_info=True)\n                self.logger.warning(\'encoder service throws an exception, \'\n                                    \'the sequel pipeline may not work properly\')\n\n        return contents\n\n    @handler.register(gnes_pb2.Request.IndexRequest)\n    def _handler_index(self, msg: \'gnes_pb2.Message\'):\n        self.embed_chunks_in_docs(msg.request.index.docs)\n\n    @handler.register(gnes_pb2.Request.TrainRequest)\n    def _handler_train(self, msg: \'gnes_pb2.Message\'):\n        if msg.request.train.docs:\n            contents = self.embed_chunks_in_docs(msg.request.train.docs, do_encoding=False)\n            self.train_data.extend(contents)\n            msg.response.train.status = gnes_pb2.Response.PENDING\n            # raise BlockMessage\n        if msg.request.train.flush:\n            self._model.train(self.train_data)\n            self.logger.info(\'%d samples is flushed for training\' % len(self.train_data))\n            self.is_model_changed.set()\n            self.train_data.clear()\n            msg.response.control.status = gnes_pb2.Response.SUCCESS\n\n    @handler.register(gnes_pb2.Request.QueryRequest)\n    def _handler_search(self, msg: \'gnes_pb2.Message\'):\n        self.embed_chunks_in_docs(msg.request.search.query, is_input_list=False)\n\n    @handler.register_hook(hook_type=(\'pre\', \'post\'), only_when_verbose=True)\n    def _hook_debug_msg(self, msg: \'gnes_pb2.Message\', *args, **kwargs):\n        from pprint import pformat\n\n        debug_kv = {\n            \'envelope\': lambda: msg.envelope,\n            \'num_docs\': lambda: len(msg.request.index.docs),\n            \'num_chunks in doc[0]\': lambda: len(msg.request.index.docs[0].chunks),\n            \'docs[0].chunks[0].content_type\': lambda: msg.request.index.docs[0].chunks[0].WhichOneof(\'content\'),\n            \'docs[0].chunks[0].weight\': lambda: msg.request.index.docs[0].chunks[0].weight,\n            \'docs[0].chunks[0].embedding\': lambda: blob2array(msg.request.index.docs[0].chunks[0].embedding),\n            \'docs[0].chunks[0].embedding[0]\': lambda: blob2array(msg.request.index.docs[0].chunks[0].embedding)[0]\n        }\n        debug_info = {}\n        for k, v in debug_kv.items():\n            try:\n                r = v()\n            except Exception as ex:\n                r = \'fail to get the value, reason: %s\' % ex\n            debug_info[k] = r\n        self.logger.info(pformat(debug_info))\n'"
gnes/service/frontend.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nimport os\nimport threading\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport grpc\nfrom google.protobuf.json_format import MessageToJson\n\nfrom ..client.base import ZmqClient\nfrom ..helper import set_logger, make_route_table\nfrom ..proto import gnes_pb2_grpc, gnes_pb2, router2str, add_route, add_version\n\n\nclass FrontendService:\n\n    def __init__(self, args):\n        if not args.proxy:\n            os.unsetenv(\'http_proxy\')\n            os.unsetenv(\'https_proxy\')\n        self.logger = set_logger(self.__class__.__name__, args.verbose)\n        self.server = grpc.server(\n            ThreadPoolExecutor(max_workers=args.max_concurrency),\n            options=[(\'grpc.max_send_message_length\', args.max_message_size),\n                     (\'grpc.max_receive_message_length\', args.max_message_size)])\n        self.logger.info(\'start a frontend with %d workers\' % args.max_concurrency)\n        gnes_pb2_grpc.add_GnesRPCServicer_to_server(self._Servicer(args), self.server)\n\n        self.bind_address = \'{0}:{1}\'.format(args.grpc_host, args.grpc_port)\n        self.server.add_insecure_port(self.bind_address)\n        self._stop_event = threading.Event()\n\n    def __enter__(self):\n        self.server.start()\n        self.logger.critical(\'listening at: %s\' % self.bind_address)\n        self._stop_event.clear()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.server.stop(None)\n        self.stop()\n\n    def stop(self):\n        self._stop_event.set()\n\n    def join(self):\n        self._stop_event.wait()\n\n    class _Servicer(gnes_pb2_grpc.GnesRPCServicer):\n\n        def __init__(self, args):\n            self.args = args\n            self.logger = set_logger(FrontendService.__name__, args.verbose)\n            self.zmq_context = self.ZmqContext(args)\n            self.request_id_cnt = 0\n            self.send_recv_kwargs = dict(\n                check_version=self.args.check_version,\n                timeout=self.args.timeout,\n                squeeze_pb=self.args.squeeze_pb)\n            self.pending_request = 0\n\n        def add_envelope(self, body: \'gnes_pb2.Request\', zmq_client: \'ZmqClient\'):\n            msg = gnes_pb2.Message()\n            msg.envelope.client_id = zmq_client.args.identity\n            if body.request_id is not None:\n                msg.envelope.request_id = body.request_id\n            else:\n                msg.envelope.request_id = self.request_id_cnt\n                self.request_id_cnt += 1\n                self.logger.warning(\'request_id is missing, filled it with an internal counter!\')\n            msg.envelope.part_id = 1\n            msg.envelope.num_part.append(1)\n            msg.envelope.timeout = 5000\n            add_version(msg.envelope)\n            add_route(msg.envelope, FrontendService.__name__, self.args.identity)\n            msg.request.CopyFrom(body)\n            return msg\n\n        def remove_envelope(self, m: \'gnes_pb2.Message\'):\n            resp = m.response\n            resp.request_id = m.envelope.request_id\n            m.envelope.routes[0].end_time.GetCurrentTime()\n            if self.args.route_table:\n                self.logger.info(\'route: %s\' % router2str(m))\n                self.logger.info(\'route table: \\n%s\' % make_route_table(m.envelope.routes, include_frontend=True))\n            if self.args.dump_route:\n                self.args.dump_route.write(MessageToJson(m.envelope, indent=0).replace(\'\\n\', \'\') + \'\\n\')\n                self.args.dump_route.flush()\n            return resp\n\n        def Call(self, request, context):\n            with self.zmq_context as zmq_client:\n                zmq_client.send_message(self.add_envelope(request, zmq_client), **self.send_recv_kwargs)\n                return self.remove_envelope(zmq_client.recv_message(**self.send_recv_kwargs))\n\n        def Train(self, request, context):\n            return self.Call(request, context)\n\n        def Index(self, request, context):\n            return self.Call(request, context)\n\n        def Search(self, request, context):\n            return self.Call(request, context)\n\n        def StreamCall(self, request_iterator, context):\n            self.pending_request = 0\n\n            def get_response(num_recv, blocked=False):\n                if blocked:\n                    self.logger.info(\'waiting for %d responses ...\' % (num_recv))\n                for _ in range(num_recv):\n                    if blocked or zmq_client.receiver.poll(1):\n                        msg = zmq_client.recv_message(**self.send_recv_kwargs)\n                        self.pending_request -= 1\n                        yield self.remove_envelope(msg)\n\n                while zmq_client.receiver.poll(1):\n                    msg = zmq_client.recv_message(**self.send_recv_kwargs)\n                    self.pending_request -= 1\n                    yield self.remove_envelope(msg)\n\n            with self.zmq_context as zmq_client:\n\n                for request in request_iterator:\n                    self.logger.info(\'receive request: %s\' % request.request_id)\n                    num_recv = max(self.pending_request - self.args.max_pending_request, 1)\n                    yield from get_response(num_recv, num_recv > 1)\n                    self.logger.info(\'send new request into %d appending tasks\' % (self.pending_request))\n                    zmq_client.send_message(self.add_envelope(request, zmq_client), **self.send_recv_kwargs)\n                    self.pending_request += 1\n\n                self.logger.info(\'all requests are sent, waiting for the responses...\')\n                yield from get_response(self.pending_request, blocked=True)\n\n        class ZmqContext:\n            """"""The zmq context class.""""""\n\n            def __init__(self, args):\n                self.args = args\n                self.tlocal = threading.local()\n                self.tlocal.client = None\n\n            def __enter__(self):\n                """"""Enter the context.""""""\n                client = ZmqClient(self.args)\n                self.tlocal.client = client\n                return client\n\n            def __exit__(self, exc_type, exc_value, exc_traceback):\n                """"""Exit the context.""""""\n                self.tlocal.client.close()\n                self.tlocal.client = None\n'"
gnes/service/grpc.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nimport grpc\n\nfrom .base import BaseService as BS, MessageHandler\nfrom ..helper import PathImporter\nfrom ..proto import gnes_pb2\n\n\nclass GRPCService(BS):\n    handler = MessageHandler(BS.handler)\n\n    def post_init(self):\n        self.channel = grpc.insecure_channel(\n            \'%s:%s\' % (self.args.grpc_host, self.args.grpc_port),\n            options=[(\'grpc.max_send_message_length\', self.args.max_message_size),\n                     (\'grpc.max_receive_message_length\', self.args.max_message_size)])\n\n        m = PathImporter.add_modules(self.args.pb2_path, self.args.pb2_grpc_path)\n\n        # build stub\n        self.stub = getattr(m, self.args.stub_name)(self.channel)\n\n    def close(self):\n        self.channel.close()\n        super().close()\n\n    @handler.register(NotImplementedError)\n    def _handler_default(self, msg: \'gnes_pb2.Message\'):\n        yield getattr(self.stub, self.args.api_name)(msg)\n'"
gnes/service/indexer.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport numpy as np\n\nfrom .base import BaseService as BS, MessageHandler, ServiceError\nfrom ..proto import gnes_pb2, blob2array\n\n\nclass IndexerService(BS):\n    handler = MessageHandler(BS.handler)\n\n    def post_init(self):\n        from ..indexer.base import BaseIndexer\n        # print(\'id: %s, before: %r\' % (threading.get_ident(), self._model))\n        self._model = self.load_model(BaseIndexer)\n        # self._tmp_a = threading.get_ident()\n        # print(\'id: %s, after: %r, self._tmp_a: %r\' % (threading.get_ident(), self._model, self._tmp_a))\n\n    @handler.register(gnes_pb2.Request.IndexRequest)\n    def _handler_index(self, msg: \'gnes_pb2.Message\'):\n        # print(\'tid: %s, model: %r, self._tmp_a: %r\' % (threading.get_ident(), self._model, self._tmp_a))\n        # if self._tmp_a != threading.get_ident():\n        #     print(\'!!! tid: %s, tmp_a: %r %r\' % (threading.get_ident(), self._tmp_a, self._handler_index))\n        from ..indexer.base import BaseChunkIndexer, BaseDocIndexer\n        if isinstance(self._model, BaseChunkIndexer):\n            is_changed = self._handler_chunk_index(msg)\n        elif isinstance(self._model, BaseDocIndexer):\n            is_changed = self._handler_doc_index(msg)\n        else:\n            raise ServiceError(\n                \'unsupported indexer, dont know how to use %s to handle this message\' % self._model.__bases__)\n\n        if self.args.as_response:\n            msg.response.index.status = gnes_pb2.Response.SUCCESS\n\n        if is_changed:\n            self.is_model_changed.set()\n\n    def _handler_chunk_index(self, msg: \'gnes_pb2.Message\') -> bool:\n        embed_info = []\n\n        for d in msg.request.index.docs:\n            if not d.chunks:\n                self.logger.warning(\'document (doc_id=%s) contains no chunks!\' % d.doc_id)\n                continue\n\n            embed_info += [(blob2array(c.embedding), d.doc_id, c.offset, c.weight) for c in d.chunks if\n                           c.embedding.data]\n\n        if embed_info:\n            vecs, doc_ids, offsets, weights = zip(*embed_info)\n            self._model.add(list(zip(doc_ids, offsets)), np.stack(vecs), weights)\n            return True\n        else:\n            self.logger.warning(\'chunks contain no embedded vectors, the indexer will do nothing\')\n            return False\n\n    def _handler_doc_index(self, msg: \'gnes_pb2.Message\') -> bool:\n        if msg.request.index.docs:\n            self._model.add([d.doc_id for d in msg.request.index.docs],\n                            [d for d in msg.request.index.docs],\n                            [d.weight for d in msg.request.index.docs])\n            return True\n        else:\n            return False\n\n    def _put_result_into_message(self, results, msg: \'gnes_pb2.Message\'):\n        msg.response.search.ClearField(\'topk_results\')\n        msg.response.search.topk_results.extend(results)\n        msg.response.search.top_k = len(results)\n        msg.response.search.is_big_score_similar = self._model.is_big_score_similar\n\n    @handler.register(gnes_pb2.Request.QueryRequest)\n    def _handler_chunk_search(self, msg: \'gnes_pb2.Message\'):\n        from ..indexer.base import BaseChunkIndexer\n        if not isinstance(self._model, BaseChunkIndexer):\n            raise ServiceError(\n                \'unsupported indexer, dont know how to use %s to handle this message\' % self._model.__bases__)\n\n        results = []\n        if not msg.request.search.query.chunks:\n            self.logger.warning(\'query contains no chunks!\')\n        else:\n            results = self._model.query_and_score(msg.request.search.query.chunks, top_k=msg.request.search.top_k)\n\n        self._put_result_into_message(results, msg)\n\n    @handler.register(gnes_pb2.Response.QueryResponse)\n    def _handler_doc_search(self, msg: \'gnes_pb2.Message\'):\n        from ..indexer.base import BaseDocIndexer\n        if not isinstance(self._model, BaseDocIndexer):\n            raise ServiceError(\n                \'unsupported indexer, dont know how to use %s to handle this message\' % self._model.__bases__)\n\n        # check if chunk_indexer and doc_indexer has the same sorting order\n        if msg.response.search.is_big_score_similar is not None and \\\n                msg.response.search.is_big_score_similar != self._model.is_big_score_similar:\n            raise ServiceError(\n                \'is_big_score_similar is inconsistent. last topk-list: is_big_score_similar=%s, but \'\n                \'this indexer: is_big_score_similar=%s\' % (\n                    msg.response.search.is_big_score_similar, self._model.is_big_score_similar))\n\n        # assume the doc search will change the whatever sort order the message has\n        msg.response.search.is_sorted = False\n        results = self._model.query_and_score(msg.response.search.topk_results)\n        self._put_result_into_message(results, msg)\n'"
gnes/service/preprocessor.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nfrom .base import BaseService as BS, MessageHandler\nfrom ..proto import gnes_pb2\n\n\nclass PreprocessorService(BS):\n    handler = MessageHandler(BS.handler)\n\n    def post_init(self):\n        from ..preprocessor.base import BasePreprocessor\n        self._model = self.load_model(BasePreprocessor)\n\n    @handler.register(gnes_pb2.Request.TrainRequest)\n    def _handler_train(self, msg: \'gnes_pb2.Message\'):\n        for d in msg.request.train.docs:\n            self._apply(d)\n\n    @handler.register(gnes_pb2.Request.IndexRequest)\n    def _handler_index(self, msg: \'gnes_pb2.Message\'):\n        for d in msg.request.index.docs:\n            self._apply(d)\n\n    @handler.register(gnes_pb2.Request.QueryRequest)\n    def _handler_query(self, msg: \'gnes_pb2.Message\'):\n        self._apply(msg.request.search.query)\n\n    def _apply(self, d: \'gnes_pb2.Document\'):\n        self._model.apply(d)\n        if not d.chunks:\n            self.logger.warning(\'document (doc_id=%s) contains no chunks!\' % d.doc_id)\n'"
gnes/service/router.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nfrom collections import defaultdict\nfrom typing import Dict, List\n\nfrom .base import BaseService as BS, MessageHandler, BlockMessage\nfrom ..proto import gnes_pb2\nfrom ..router.base import BaseReduceRouter\n\n\nclass RouterService(BS):\n    handler = MessageHandler(BS.handler)\n\n    def post_init(self):\n        from ..router.base import BaseRouter\n        self._model = self.load_model(BaseRouter)\n        self._pending = defaultdict(list)  # type: Dict[str, List]\n\n    def _is_msg_complete(self, msg: \'gnes_pb2.Message\', num_req: int) -> bool:\n        return (self.args.num_part is None and num_req == msg.envelope.num_part[-1]) or \\\n               (num_req == self.args.num_part)\n\n    @handler.register(NotImplementedError)\n    def _handler_default(self, msg: \'gnes_pb2.Message\'):\n        if isinstance(self._model, BaseReduceRouter):\n            req_id = msg.envelope.request_id\n            self._pending[req_id].append(msg)\n            num_req = len(self._pending[req_id])\n\n            if self._is_msg_complete(msg, num_req):\n                prev_msgs = self._pending.pop(req_id)\n                return self._model.apply(msg, prev_msgs)\n            else:\n                raise BlockMessage\n        else:\n            return self._model.apply(msg)\n'"
tests/contrib/dummy2.py,0,"b""from gnes.component import BaseEncoder\nfrom gnes.helper import train_required\n\n\nclass DummyEncoder2(BaseEncoder):\n\n    def train(self, *args, **kwargs):\n        self.logger.info('you just trained me!')\n        pass\n\n    @train_required\n    def encode(self, x):\n        return x + 1\n"""
tests/contrib/dummy3.py,0,"b""from gnes.component import BaseEncoder\nfrom gnes.helper import train_required\n\n\nclass DummyEncoder3(BaseEncoder):\n\n    def train(self, *args, **kwargs):\n        self.logger.info('you just trained me!')\n        pass\n\n    @train_required\n    def encode(self, x):\n        return x + 1\n"""
tests/contrib/dummy_contrib.py,0,"b""from gnes.encoder.base import BaseTextEncoder\n\n\nclass FooContribEncoder(BaseTextEncoder):\n\n    def __init__(self, bar: int, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.is_trained = True\n        self.bar = bar\n\n    def encode(self, text, **kwargs):\n        return 'hello %d' % self.bar\n"""
tests/contrib/fake_faiss.py,0,"b""from gnes.indexer.base import BaseChunkIndexer\n\n\nclass FaissIndexer(BaseChunkIndexer):\n\n    def __init__(self, bar: int, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.is_trained = True\n        self.bar = bar\n        self.logger.info('look at me, I override the original GNES faiss indexer')\n"""
tests/contrib/fake_faiss2.py,0,"b""from gnes.indexer.base import BaseChunkIndexer\n\n\nclass FaissIndexer(BaseChunkIndexer):\n\n    def __init__(self, bar: int, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.is_trained = True\n        self.bar = bar\n        self.logger.info('look at me, I override the overrided faiss indexer!!!')\n"""
tests/contrib/transformer.py,0,"b""from typing import List\n\nimport numpy as np\n\nfrom gnes.encoder.base import BaseTextEncoder\n\n\nclass PyTorchTransformers(BaseTextEncoder):\n\n    def __init__(self, model_name: str = 'bert-base-uncased', *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.model_name = model_name\n\n    def encode(self, text: List[str], *args, **kwargs):\n        return np.random.random([5, 128])\n"""
tests/proto/dummy_pb2.py,0,"b'# -*- coding: utf-8 -*-\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: dummy.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom google.protobuf import timestamp_pb2 as google_dot_protobuf_dot_timestamp__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'dummy.proto\',\n  package=\'dummy\',\n  syntax=\'proto3\',\n  serialized_options=None,\n  serialized_pb=_b(\'\\n\\x0b\\x64ummy.proto\\x12\\x05\\x64ummy\\x1a\\x1fgoogle/protobuf/timestamp.proto\\""\\xd5\\x01\\n\\x08\\x45nvelope\\x12\\x11\\n\\tclient_id\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\nrequest_id\\x18\\x02 \\x01(\\t\\x12\\x0f\\n\\x07part_id\\x18\\x03 \\x01(\\r\\x12\\x10\\n\\x08num_part\\x18\\x04 \\x03(\\r\\x12\\x0f\\n\\x07timeout\\x18\\x05 \\x01(\\r\\x12%\\n\\x06routes\\x18\\x06 \\x03(\\x0b\\x32\\x15.dummy.Envelope.route\\x1aG\\n\\x05route\\x12\\x0f\\n\\x07service\\x18\\x01 \\x01(\\t\\x12-\\n\\ttimestamp\\x18\\x02 \\x01(\\x0b\\x32\\x1a.google.protobuf.Timestamp\\"",\\n\\x07Message\\x12!\\n\\x08\\x65nvelope\\x18\\x01 \\x01(\\x0b\\x32\\x0f.dummy.Envelope2@\\n\\x10\\x44ummyGRPCService\\x12,\\n\\x08\\x64ummyAPI\\x12\\x0e.dummy.Message\\x1a\\x0e.dummy.Message\\""\\x00\\x62\\x06proto3\')\n  ,\n  dependencies=[google_dot_protobuf_dot_timestamp__pb2.DESCRIPTOR,])\n\n\n\n\n_ENVELOPE_ROUTE = _descriptor.Descriptor(\n  name=\'route\',\n  full_name=\'dummy.Envelope.route\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'service\', full_name=\'dummy.Envelope.route.service\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'timestamp\', full_name=\'dummy.Envelope.route.timestamp\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=198,\n  serialized_end=269,\n)\n\n_ENVELOPE = _descriptor.Descriptor(\n  name=\'Envelope\',\n  full_name=\'dummy.Envelope\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'client_id\', full_name=\'dummy.Envelope.client_id\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'request_id\', full_name=\'dummy.Envelope.request_id\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'part_id\', full_name=\'dummy.Envelope.part_id\', index=2,\n      number=3, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'num_part\', full_name=\'dummy.Envelope.num_part\', index=3,\n      number=4, type=13, cpp_type=3, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'timeout\', full_name=\'dummy.Envelope.timeout\', index=4,\n      number=5, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'routes\', full_name=\'dummy.Envelope.routes\', index=5,\n      number=6, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_ENVELOPE_ROUTE, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=56,\n  serialized_end=269,\n)\n\n\n_MESSAGE = _descriptor.Descriptor(\n  name=\'Message\',\n  full_name=\'dummy.Message\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'envelope\', full_name=\'dummy.Message.envelope\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto3\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=271,\n  serialized_end=315,\n)\n\n_ENVELOPE_ROUTE.fields_by_name[\'timestamp\'].message_type = google_dot_protobuf_dot_timestamp__pb2._TIMESTAMP\n_ENVELOPE_ROUTE.containing_type = _ENVELOPE\n_ENVELOPE.fields_by_name[\'routes\'].message_type = _ENVELOPE_ROUTE\n_MESSAGE.fields_by_name[\'envelope\'].message_type = _ENVELOPE\nDESCRIPTOR.message_types_by_name[\'Envelope\'] = _ENVELOPE\nDESCRIPTOR.message_types_by_name[\'Message\'] = _MESSAGE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nEnvelope = _reflection.GeneratedProtocolMessageType(\'Envelope\', (_message.Message,), dict(\n\n  route = _reflection.GeneratedProtocolMessageType(\'route\', (_message.Message,), dict(\n    DESCRIPTOR = _ENVELOPE_ROUTE,\n    __module__ = \'dummy_pb2\'\n    # @@protoc_insertion_point(class_scope:dummy.Envelope.route)\n    ))\n  ,\n  DESCRIPTOR = _ENVELOPE,\n  __module__ = \'dummy_pb2\'\n  # @@protoc_insertion_point(class_scope:dummy.Envelope)\n  ))\n_sym_db.RegisterMessage(Envelope)\n_sym_db.RegisterMessage(Envelope.route)\n\nMessage = _reflection.GeneratedProtocolMessageType(\'Message\', (_message.Message,), dict(\n  DESCRIPTOR = _MESSAGE,\n  __module__ = \'dummy_pb2\'\n  # @@protoc_insertion_point(class_scope:dummy.Message)\n  ))\n_sym_db.RegisterMessage(Message)\n\n\n\n_DUMMYGRPCSERVICE = _descriptor.ServiceDescriptor(\n  name=\'DummyGRPCService\',\n  full_name=\'dummy.DummyGRPCService\',\n  file=DESCRIPTOR,\n  index=0,\n  serialized_options=None,\n  serialized_start=317,\n  serialized_end=381,\n  methods=[\n  _descriptor.MethodDescriptor(\n    name=\'dummyAPI\',\n    full_name=\'dummy.DummyGRPCService.dummyAPI\',\n    index=0,\n    containing_service=None,\n    input_type=_MESSAGE,\n    output_type=_MESSAGE,\n    serialized_options=None,\n  ),\n])\n_sym_db.RegisterServiceDescriptor(_DUMMYGRPCSERVICE)\n\nDESCRIPTOR.services_by_name[\'DummyGRPCService\'] = _DUMMYGRPCSERVICE\n\n# @@protoc_insertion_point(module_scope)\n'"
tests/proto/dummy_pb2_grpc.py,0,"b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\nimport dummy_pb2 as dummy__pb2\n\nclass DummyGRPCServiceStub(object):\n  # missing associated documentation comment in .proto file\n  pass\n\n  def __init__(self, channel):\n    """"""Constructor.\n\n    Args:\n      channel: A grpc.Channel.\n    """"""\n    self.dummyAPI = channel.unary_unary(\n        \'/dummy.DummyGRPCService/dummyAPI\',\n        request_serializer=dummy__pb2.Message.SerializeToString,\n        response_deserializer=dummy__pb2.Message.FromString,\n        )\n\n\nclass DummyGRPCServiceServicer(object):\n  # missing associated documentation comment in .proto file\n  pass\n\n  def dummyAPI(self, request, context):\n    # missing associated documentation comment in .proto file\n    pass\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details(\'Method not implemented!\')\n    raise NotImplementedError(\'Method not implemented!\')\n\n\ndef add_DummyGRPCServiceServicer_to_server(servicer, server):\n  rpc_method_handlers = {\n      \'dummyAPI\': grpc.unary_unary_rpc_method_handler(\n          servicer.dummyAPI,\n          request_deserializer=dummy__pb2.Message.FromString,\n          response_serializer=dummy__pb2.Message.SerializeToString,\n      ),\n  }\n  generic_handler = grpc.method_handlers_generic_handler(\n      \'dummy.DummyGRPCService\', rpc_method_handlers)\n  server.add_generic_rpc_handlers((generic_handler,))\n'"
tests/proto_s/dummy_pb2.py,0,"b'# -*- coding: utf-8 -*-\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: dummy.proto\n\nimport sys\n\n_b = sys.version_info[0] < 3 and (lambda x: x) or (lambda x: x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\n\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\nfrom google.protobuf import timestamp_pb2 as google_dot_protobuf_dot_timestamp__pb2\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n    name=\'dummy.proto\',\n    package=\'dummy\',\n    syntax=\'proto3\',\n    serialized_options=None,\n    serialized_pb=_b(\n        \'\\n\\x0b\\x64ummy.proto\\x12\\x05\\x64ummy\\x1a\\x1fgoogle/protobuf/timestamp.proto\\""\\xd5\\x01\\n\\x08\\x45nvelope\\x12\\x11\\n\\tclient_id\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\nrequest_id\\x18\\x02 \\x01(\\t\\x12\\x0f\\n\\x07part_id\\x18\\x03 \\x01(\\r\\x12\\x10\\n\\x08num_part\\x18\\x04 \\x03(\\r\\x12\\x0f\\n\\x07timeout\\x18\\x05 \\x01(\\r\\x12%\\n\\x06routes\\x18\\x06 \\x03(\\x0b\\x32\\x15.dummy.Envelope.route\\x1aG\\n\\x05route\\x12\\x0f\\n\\x07service\\x18\\x01 \\x01(\\t\\x12-\\n\\ttimestamp\\x18\\x02 \\x01(\\x0b\\x32\\x1a.google.protobuf.Timestamp\\"",\\n\\x07Message\\x12!\\n\\x08\\x65nvelope\\x18\\x01 \\x01(\\x0b\\x32\\x0f.dummy.Envelope2@\\n\\x10\\x44ummyGRPCService\\x12,\\n\\x08\\x64ummyAPI\\x12\\x0e.dummy.Message\\x1a\\x0e.dummy.Message\\""\\x00\\x62\\x06proto3\')\n    ,\n    dependencies=[google_dot_protobuf_dot_timestamp__pb2.DESCRIPTOR, ])\n\n_ENVELOPE_ROUTE = _descriptor.Descriptor(\n    name=\'route\',\n    full_name=\'dummy.Envelope.route\',\n    filename=None,\n    file=DESCRIPTOR,\n    containing_type=None,\n    fields=[\n        _descriptor.FieldDescriptor(\n            name=\'service\', full_name=\'dummy.Envelope.route.service\', index=0,\n            number=1, type=9, cpp_type=9, label=1,\n            has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n            message_type=None, enum_type=None, containing_type=None,\n            is_extension=False, extension_scope=None,\n            serialized_options=None, file=DESCRIPTOR),\n        _descriptor.FieldDescriptor(\n            name=\'timestamp\', full_name=\'dummy.Envelope.route.timestamp\', index=1,\n            number=2, type=11, cpp_type=10, label=1,\n            has_default_value=False, default_value=None,\n            message_type=None, enum_type=None, containing_type=None,\n            is_extension=False, extension_scope=None,\n            serialized_options=None, file=DESCRIPTOR),\n    ],\n    extensions=[\n    ],\n    nested_types=[],\n    enum_types=[\n    ],\n    serialized_options=None,\n    is_extendable=False,\n    syntax=\'proto3\',\n    extension_ranges=[],\n    oneofs=[\n    ],\n    serialized_start=198,\n    serialized_end=269,\n)\n\n_ENVELOPE = _descriptor.Descriptor(\n    name=\'Envelope\',\n    full_name=\'dummy.Envelope\',\n    filename=None,\n    file=DESCRIPTOR,\n    containing_type=None,\n    fields=[\n        _descriptor.FieldDescriptor(\n            name=\'client_id\', full_name=\'dummy.Envelope.client_id\', index=0,\n            number=1, type=9, cpp_type=9, label=1,\n            has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n            message_type=None, enum_type=None, containing_type=None,\n            is_extension=False, extension_scope=None,\n            serialized_options=None, file=DESCRIPTOR),\n        _descriptor.FieldDescriptor(\n            name=\'request_id\', full_name=\'dummy.Envelope.request_id\', index=1,\n            number=2, type=9, cpp_type=9, label=1,\n            has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n            message_type=None, enum_type=None, containing_type=None,\n            is_extension=False, extension_scope=None,\n            serialized_options=None, file=DESCRIPTOR),\n        _descriptor.FieldDescriptor(\n            name=\'part_id\', full_name=\'dummy.Envelope.part_id\', index=2,\n            number=3, type=13, cpp_type=3, label=1,\n            has_default_value=False, default_value=0,\n            message_type=None, enum_type=None, containing_type=None,\n            is_extension=False, extension_scope=None,\n            serialized_options=None, file=DESCRIPTOR),\n        _descriptor.FieldDescriptor(\n            name=\'num_part\', full_name=\'dummy.Envelope.num_part\', index=3,\n            number=4, type=13, cpp_type=3, label=3,\n            has_default_value=False, default_value=[],\n            message_type=None, enum_type=None, containing_type=None,\n            is_extension=False, extension_scope=None,\n            serialized_options=None, file=DESCRIPTOR),\n        _descriptor.FieldDescriptor(\n            name=\'timeout\', full_name=\'dummy.Envelope.timeout\', index=4,\n            number=5, type=13, cpp_type=3, label=1,\n            has_default_value=False, default_value=0,\n            message_type=None, enum_type=None, containing_type=None,\n            is_extension=False, extension_scope=None,\n            serialized_options=None, file=DESCRIPTOR),\n        _descriptor.FieldDescriptor(\n            name=\'routes\', full_name=\'dummy.Envelope.routes\', index=5,\n            number=6, type=11, cpp_type=10, label=3,\n            has_default_value=False, default_value=[],\n            message_type=None, enum_type=None, containing_type=None,\n            is_extension=False, extension_scope=None,\n            serialized_options=None, file=DESCRIPTOR),\n    ],\n    extensions=[\n    ],\n    nested_types=[_ENVELOPE_ROUTE, ],\n    enum_types=[\n    ],\n    serialized_options=None,\n    is_extendable=False,\n    syntax=\'proto3\',\n    extension_ranges=[],\n    oneofs=[\n    ],\n    serialized_start=56,\n    serialized_end=269,\n)\n\n_MESSAGE = _descriptor.Descriptor(\n    name=\'Message\',\n    full_name=\'dummy.Message\',\n    filename=None,\n    file=DESCRIPTOR,\n    containing_type=None,\n    fields=[\n        _descriptor.FieldDescriptor(\n            name=\'envelope\', full_name=\'dummy.Message.envelope\', index=0,\n            number=1, type=11, cpp_type=10, label=1,\n            has_default_value=False, default_value=None,\n            message_type=None, enum_type=None, containing_type=None,\n            is_extension=False, extension_scope=None,\n            serialized_options=None, file=DESCRIPTOR),\n    ],\n    extensions=[\n    ],\n    nested_types=[],\n    enum_types=[\n    ],\n    serialized_options=None,\n    is_extendable=False,\n    syntax=\'proto3\',\n    extension_ranges=[],\n    oneofs=[\n    ],\n    serialized_start=271,\n    serialized_end=315,\n)\n\n_ENVELOPE_ROUTE.fields_by_name[\'timestamp\'].message_type = google_dot_protobuf_dot_timestamp__pb2._TIMESTAMP\n_ENVELOPE_ROUTE.containing_type = _ENVELOPE\n_ENVELOPE.fields_by_name[\'routes\'].message_type = _ENVELOPE_ROUTE\n_MESSAGE.fields_by_name[\'envelope\'].message_type = _ENVELOPE\nDESCRIPTOR.message_types_by_name[\'Envelope\'] = _ENVELOPE\nDESCRIPTOR.message_types_by_name[\'Message\'] = _MESSAGE\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nEnvelope = _reflection.GeneratedProtocolMessageType(\'Envelope\', (_message.Message,), dict(\n\n    route=_reflection.GeneratedProtocolMessageType(\'route\', (_message.Message,), dict(\n        DESCRIPTOR=_ENVELOPE_ROUTE,\n        __module__=\'dummy_pb2\'\n        # @@protoc_insertion_point(class_scope:dummy.Envelope.route)\n    ))\n    ,\n    DESCRIPTOR=_ENVELOPE,\n    __module__=\'dummy_pb2\'\n    # @@protoc_insertion_point(class_scope:dummy.Envelope)\n))\n_sym_db.RegisterMessage(Envelope)\n_sym_db.RegisterMessage(Envelope.route)\n\nMessage = _reflection.GeneratedProtocolMessageType(\'Message\', (_message.Message,), dict(\n    DESCRIPTOR=_MESSAGE,\n    __module__=\'dummy_pb2\'\n    # @@protoc_insertion_point(class_scope:dummy.Message)\n))\n_sym_db.RegisterMessage(Message)\n\n_DUMMYGRPCSERVICE = _descriptor.ServiceDescriptor(\n    name=\'DummyGRPCService\',\n    full_name=\'dummy.DummyGRPCService\',\n    file=DESCRIPTOR,\n    index=0,\n    serialized_options=None,\n    serialized_start=317,\n    serialized_end=381,\n    methods=[\n        _descriptor.MethodDescriptor(\n            name=\'dummyAPI\',\n            full_name=\'dummy.DummyGRPCService.dummyAPI\',\n            index=0,\n            containing_service=None,\n            input_type=_MESSAGE,\n            output_type=_MESSAGE,\n            serialized_options=None,\n        ),\n    ])\n_sym_db.RegisterServiceDescriptor(_DUMMYGRPCSERVICE)\n\nDESCRIPTOR.services_by_name[\'DummyGRPCService\'] = _DUMMYGRPCSERVICE\n\n# @@protoc_insertion_point(module_scope)\n'"
tests/proto_s/dummy_pb2_grpc.py,0,"b'# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\nimport grpc\n\nfrom . import dummy_pb2 as dummy__pb2\n\n\nclass DummyGRPCServiceStub(object):\n    # missing associated documentation comment in .proto file\n    pass\n\n    def __init__(self, channel):\n        """"""Constructor.\n\n        Args:\n          channel: A grpc.Channel.\n        """"""\n        self.dummyAPI = channel.unary_unary(\n            \'/dummy.DummyGRPCService/dummyAPI\',\n            request_serializer=dummy__pb2.Message.SerializeToString,\n            response_deserializer=dummy__pb2.Message.FromString,\n        )\n\n\nclass DummyGRPCServiceServicer(object):\n    # missing associated documentation comment in .proto file\n    pass\n\n    def dummyAPI(self, request, context):\n        # missing associated documentation comment in .proto file\n        pass\n        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n        context.set_details(\'Method not implemented!\')\n        raise NotImplementedError(\'Method not implemented!\')\n\n\ndef add_DummyGRPCServiceServicer_to_server(servicer, server):\n    rpc_method_handlers = {\n        \'dummyAPI\': grpc.unary_unary_rpc_method_handler(\n            servicer.dummyAPI,\n            request_deserializer=dummy__pb2.Message.FromString,\n            response_serializer=dummy__pb2.Message.SerializeToString,\n        ),\n    }\n    generic_handler = grpc.method_handlers_generic_handler(\n        \'dummy.DummyGRPCService\', rpc_method_handlers)\n    server.add_generic_rpc_handlers((generic_handler,))\n'"
gnes/encoder/audio/__init__.py,0,b''
gnes/encoder/audio/mfcc.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nfrom typing import List\n\nimport numpy as np\n\nfrom ..base import BaseAudioEncoder\nfrom ...helper import batching\n\n\nclass MfccEncoder(BaseAudioEncoder):\n    batch_size = 64\n\n    def __init__(self, n_mfcc: int = 13, sample_rate: int = 16000, max_length: int = 100, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.n_mfcc = n_mfcc\n        self.sample_rate = sample_rate\n        self.max_length = max_length\n\n    @batching\n    def encode(self, data: List[\'np.array\'], *args, **kwargs) -> np.ndarray:\n        import librosa\n\n        mfccs = [np.array(librosa.feature.mfcc(y=audio, sr=self.sample_rate, n_mfcc=self.n_mfcc).T)\n                 for audio in data]\n\n        mfccs = [np.concatenate((mf, np.zeros((self.max_length - mf.shape[0], self.n_mfcc), dtype=np.float32)), axis=0)\n                 if mf.shape[0] < self.max_length else mf[:self.max_length] for mf in mfccs]\n        mfccs = [mfcc.reshape((1, -1)) for mfcc in mfccs]\n        mfccs = np.squeeze(np.array(mfccs), axis=1)\n        return mfccs\n'"
gnes/encoder/audio/vggish.py,3,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport numpy as np\nfrom typing import List\nfrom ..base import BaseAudioEncoder\nfrom ...helper import batching, get_first_available_gpu\n\n\nclass VggishEncoder(BaseAudioEncoder):\n    def __init__(self, model_dir: str,\n                 max_length: int = 10, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.model_dir = model_dir\n        self.max_length = max_length\n\n    def post_init(self):\n        import os\n        import tensorflow as tf\n        from .vggish_cores import vggish_slim\n        from .vggish_cores import vggish_params\n        from .vggish_cores import vggish_postprocess\n\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = str(get_first_available_gpu())\n\n        self.graph = tf.Graph()\n\n        with tf.Graph().as_default():\n            self._sess = tf.Session()\n            vggish_slim.define_vggish_slim(training=False)\n            vggish_slim.load_vggish_slim_checkpoint(self._sess, self.model_dir + ""/vggish_model.ckpt"")\n            self._audio_inputs = self._sess.graph.get_tensor_by_name(vggish_params.INPUT_TENSOR_NAME)\n            self._embedding = self._sess.graph.get_tensor_by_name(vggish_params.OUTPUT_TENSOR_NAME)\n        self._pproc = vggish_postprocess.Postprocessor(self.model_dir + ""/vggish_pca_params.npz"")\n\n\n    @batching\n    def encode(self, audio: List[\'np.ndarray\'], *args, **kwargs) -> np.ndarray:\n        # used for np.split()\n        audio_length = [len(au) for au in audio]\n        for i in range(1, len(audio_length)):\n            audio_length[i] = audio_length[i] + audio_length[i - 1]\n\n        # concat at axis = 0 to make sure the dimension is: (#vggish_example, D)\n        audio_ = np.concatenate((list(audio[i] for i in range(len(audio)))), axis=0)\n\n        [embedding_batch] = self._sess.run([self._embedding], feed_dict={self._audio_inputs: audio_})\n        postprocessed_batch = self._pproc.postprocess(embedding_batch)\n\n        audio_features = np.split(postprocessed_batch, audio_length[:-1])\n        return np.array([features.mean(axis=0) for features in audio_features])'"
gnes/encoder/image/__init__.py,0,b''
gnes/encoder/image/cvae.py,5,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nfrom typing import List\n\nimport numpy as np\nfrom PIL import Image\n\nfrom ..base import BaseImageEncoder\n\n\nclass CVAEEncoder(BaseImageEncoder):\n    batch_size = 64\n\n    def __init__(self, model_dir: str,\n                 latent_dim: int = 300,\n                 select_method: str = \'MEAN\',\n                 l2_normalize: bool = False,\n                 use_gpu: bool = True,\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self.model_dir = model_dir\n        self.latent_dim = latent_dim\n        self.select_method = select_method\n        self.l2_normalize = l2_normalize\n        self.use_gpu = use_gpu\n\n    def post_init(self):\n        import tensorflow as tf\n        from .cvae_cores.model import CVAE\n        g = tf.Graph()\n        with g.as_default():\n            self._model = CVAE(self.latent_dim)\n            self.inputs = tf.placeholder(tf.float32,\n                                         (None, 120, 120, 3))\n\n            self.mean, self.var = self._model.encode(self.inputs)\n\n            config = tf.ConfigProto(log_device_placement=False)\n            if self.use_gpu:\n                config.gpu_options.allow_growth = True\n            self.sess = tf.Session(config=config)\n            self.saver = tf.train.Saver()\n            self.saver.restore(self.sess, self.model_dir)\n\n    def encode(self, img: List[\'np.ndarray\'], *args, **kwargs) -> np.ndarray:\n        img = [(np.array(Image.fromarray(im).resize((120, 120)),\n                         dtype=np.float32) / 255) for im in img]\n\n        def _encode(_, data):\n            _mean, _var = self.sess.run((self.mean, self.var),\n                                        feed_dict={self.inputs: data})\n            if self.select_method == \'MEAN\':\n                return _mean\n            elif self.select_method == \'VAR\':\n                return _var\n            elif self.select_method == \'MEAN_VAR\':\n                return np.concatenate([_mean, _var], axis=1)\n            else:\n                raise NotImplementedError\n\n        v = _encode(None, img).astype(np.float32)\n        if self.l2_normalize:\n            v = v / (v ** 2).sum(axis=1, keepdims=True) ** 0.5\n        return v\n'"
gnes/encoder/image/inception.py,6,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nfrom typing import List\n\nimport numpy as np\n\nfrom ..base import BaseImageEncoder\nfrom ...helper import batching, get_first_available_gpu\n\n\nclass TFInceptionEncoder(BaseImageEncoder):\n    batch_size = 64\n\n    def __init__(self, model_dir: str,\n                 select_layer: str = \'PreLogitsFlatten\',\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self.model_dir = model_dir\n        self.select_layer = select_layer\n        self.inception_size_x = 299\n        self.inception_size_y = 299\n\n    def post_init(self):\n        import os\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = str(get_first_available_gpu())\n\n        import tensorflow as tf\n        from .inception_cores.inception_v4 import inception_v4\n        from .inception_cores.inception_utils import inception_arg_scope\n        g = tf.Graph()\n        with g.as_default():\n            arg_scope = inception_arg_scope()\n            inception_v4.default_image_size = self.inception_size_x\n            self.inputs = tf.placeholder(tf.float32, (None,\n                                                      self.inception_size_x,\n                                                      self.inception_size_y, 3))\n\n            with tf.contrib.slim.arg_scope(arg_scope):\n                self.logits, self.end_points = inception_v4(self.inputs,\n                                                            is_training=False,\n                                                            dropout_keep_prob=1.0)\n\n            config = tf.ConfigProto(log_device_placement=False)\n            if self.on_gpu:\n                config.gpu_options.allow_growth = True\n            self.sess = tf.Session(config=config)\n            self.saver = tf.train.Saver()\n            self.saver.restore(self.sess, self.model_dir)\n\n    def encode(self, img: List[\'np.ndarray\'], *args, **kwargs) -> np.ndarray:\n        img = [(im * 2 / 255. - 1.) for im in img]\n\n        @batching\n        def _encode(_, data):\n            _, end_points_ = self.sess.run((self.logits, self.end_points),\n                                           feed_dict={self.inputs: data})\n            return end_points_[self.select_layer]\n\n        return _encode(self, img).astype(np.float32)\n'"
gnes/encoder/image/onnx.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nfrom typing import List\n\nimport numpy as np\n\nfrom ..base import BaseImageEncoder\nfrom ...helper import batching\n\n\nclass BaseONNXImageEncoder(BaseImageEncoder):\n    batch_size = 64\n\n    def __init__(self, model_name: str,\n                 model_dir: str,\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self.model_dir = model_dir\n        self.model_name = model_name\n\n    def post_init(self):\n        import onnxruntime as ort\n\n        self.sess = ort.InferenceSession(self.model_dir + \'/\' + self.model_name)\n        inputs_info = self.sess.get_inputs()\n\n        if len(inputs_info) != 1:\n            raise ValueError(\'Now only support encoder with one input\')\n        else:\n            self.input_name = inputs_info[0].name\n            self.input_shape = inputs_info[0].shape\n            self.input_type = inputs_info[0].type\n            self.batch_size = self.input_shape[0]\n\n    @batching\n    def encode(self, img: List[\'np.ndarray\'], *args, **kwargs) -> np.ndarray:\n        pad_batch = 0\n        if len(img) != self.input_shape[0]:\n            pad_batch = self.input_shape[0] - len(img)\n            for _ in range(pad_batch):\n                img.append(np.zeros_like(img[0]))\n\n        img_ = np.array(img, dtype=np.float32).transpose(0, 3, 1, 2)\n        if list(img_.shape) != self.input_shape:\n            raise ValueError(\'Map size not match net, expect\', self.input_shape, \',got\', img_.shape)\n\n        result_npy = self.sess.run(None, {self.input_name: img_})\n        if pad_batch != 0:\n            return result_npy[0][0:len(img)]\n        else:\n            return result_npy[0]\n'"
gnes/encoder/image/torchvision.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport os\nfrom typing import List, Callable\n\nimport numpy as np\n\nfrom ..base import BaseImageEncoder\nfrom ...helper import batching, as_numpy_array\n\n\nclass TorchvisionEncoder(BaseImageEncoder):\n    batch_size = 64\n\n    def __init__(self, model_name: str,\n                 layers: List[str],\n                 model_dir: str,\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self.model_dir = model_dir\n        self.model_name = model_name\n        self.layers = layers\n\n    def post_init(self):\n        import torch\n        import torchvision.models as models\n\n        class _Model(torch.nn.Module):\n            def __init__(self, model_name: str, layers: List[str]):\n                super().__init__()\n\n                self.m = getattr(models, model_name)(pretrained=True)\n                self.layers = [self.fn_parser(l) for l in layers]\n\n            def fn_parser(self, layer: str) -> Callable:\n\n                if \'(\' not in layer and \')\' not in layer:\n                    # this is a shorthand syntax we need to add ""(x)"" at the end\n                    layer = \'m.%s(x)\' % layer\n                else:\n                    pass\n\n                def layer_fn(x, l, m, torch):\n                    return eval(l)\n\n                return lambda x: layer_fn(x, layer, self.m, torch)\n\n            def forward(self, x):\n                for l in self.layers:\n                    x = l(x)\n                return x\n\n        os.environ[\'TORCH_HOME\'] = self.model_dir\n        self._model = _Model(self.model_name, self.layers)\n        self._model = self._model.eval()\n        if self.on_gpu:\n            # self._model.cuda()\n            self._device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n            self._model = self._model.to(self._device)\n\n    def encode(self, img: List[\'np.ndarray\'], *args, **kwargs) -> np.ndarray:\n        import torch\n        self._model.eval()\n\n        # padding to ensure that every chunk has the same number of frame\n        def _padding(img: List[\'np.ndarray\']):\n            max_lenth = max([len(x) for x in img])\n            img = [np.concatenate((im, np.zeros((max_lenth - im.shape[0], im.shape[1], im.shape[2], 3), dtype=np.uint8))\n                                  , axis=0)\n                   if im.shape[0] < max_lenth else im for im in img]\n            return img, max_lenth\n\n        # for video\n        if len(img[0].shape) == 4:\n            img, max_lenth = _padding(img)\n        # for image\n        else:\n            max_lenth = -1\n\n        @batching(chunk_dim=max_lenth)\n        @as_numpy_array\n        def _encode(_, img: List[\'np.ndarray\']):\n            import copy\n\n            if len(img[0].shape) == 4:\n                img_ = copy.deepcopy(img)\n                img_ = np.concatenate((list(img_[i] for i in range(len(img_)))), axis=0)\n\n                img_for_torch = np.array(img_, dtype=np.float32).transpose(0, 3, 1, 2)\n            else:\n                img_for_torch = np.array(img, dtype=np.float32).transpose(0, 3, 1, 2)\n\n            img_tensor = torch.from_numpy(img_for_torch)\n            if self.on_gpu:\n                img_tensor = img_tensor.cuda()\n\n            encodes = self._model(img_tensor)\n\n            return encodes.data.cpu()\n\n        return _encode(self, img)\n'"
gnes/encoder/numeric/__init__.py,0,b''
gnes/encoder/numeric/hash.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nimport numpy as np\n\nfrom ..base import BaseNumericEncoder\nfrom ...helper import batching, train_required\n\n\nclass HashEncoder(BaseNumericEncoder):\n    batch_size = 2048\n\n    def __init__(self, num_bytes: int,\n                 num_bits: int = 8,\n                 num_idx: int = 3,\n                 kmeans_clusters: int = 100,\n                 method: str = \'product_uniform\',\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert 1 <= num_bits <= 8, \'maximum 8 hash functions in a byte\'\n        self.num_bytes = num_bytes\n        self.num_bits = num_bits\n        self.num_idx = num_idx\n        self.kmeans_clusters = kmeans_clusters\n        self.method = method\n        self.centroids = None\n        self.x = None\n        self.vec_dim = None\n        self.hash_cores = None\n        self.mean = None\n        self.var = None\n\n    def train(self, vecs: np.ndarray, *args, **kwargs):\n        self.vec_dim = vecs.shape[1]\n        self.centroids = [self.train_kmeans(vecs) for _ in range(self.num_idx)]\n        self.centroids = np.reshape(\n            self.centroids, [1, self.num_idx, self.kmeans_clusters, self.vec_dim]).astype(np.float32)\n\n        if self.vec_dim % self.num_bytes != 0:\n            raise ValueError(\'vec dim should be divided by x\')\n        self.x = int(self.vec_dim / self.num_bytes)\n        self.mean = np.mean(vecs, axis=0)\n        self.var = np.var(vecs, axis=0)\n        self.hash_cores = [self.ran_gen() for _ in range(self.num_bytes)]\n        self.proj = np.array([2 ** i for i in range(self.num_bits)]).astype(np.int32)\n\n    def train_kmeans(self, vecs):\n        import faiss\n        kmeans_instance = faiss.Kmeans(self.vec_dim,\n                                       self.kmeans_clusters,\n                                       niter=10)\n        if vecs.dtype != np.float32:\n            vecs = vecs.astype(np.float32)\n        kmeans_instance.train(vecs)\n        centroids = kmeans_instance.centroids\n        return centroids\n\n    def pred_kmeans(self, vecs):\n        vecs = np.reshape(vecs, [vecs.shape[0], 1, 1, vecs.shape[1]])\n        dist = np.sum(np.square(vecs - self.centroids), -1)\n        return np.argmax(-dist, axis=-1).astype(np.uint32)\n\n    def ran_gen(self):\n        self.logger.info(\'hash functions with %s\' % self.method)\n        if self.method == \'product_uniform\':\n            return np.random.uniform(-1, 1, size=(self.x, self.num_bits)\n                                     ).astype(np.float32)\n        elif self.method == \'uniform\':\n            return np.random.uniform(-1, 1, size=(self.vec_dim, self.num_bits)\n                                     ).astype(np.float32)\n        elif self.method == \'ortho_uniform\':\n            from scipy.stats import ortho_group\n            m = ortho_group.rvs(dim=max(self.vec_dim, self.num_bits)\n                                ).astype(np.float32)\n            if self.vec_dim >= self.num_bits:\n                return m[:, :self.num_bits]\n            else:\n                return m[:self.vec_dim, :]\n\n    def hash(self, vecs):\n        ret = []\n        if self.method == \'product_uniform\':\n            vecs = np.reshape(vecs, [vecs.shape[0], self.num_bytes, self.x])\n            for i in range(self.num_bytes):\n                out = np.greater(np.matmul(vecs[:, i, :], self.hash_cores[i]), 0)\n                ret.append(np.sum(out * self.proj, axis=1, keepdims=1))\n            return np.concatenate(ret, axis=1).astype(np.uint32)\n        elif self.method == \'uniform\' or self.method == \'ortho_uniform\':\n            for i in range(self.num_bytes):\n                out = np.greater(np.matmul(vecs, self.hash_cores[i]), 0)\n                ret.append(np.sum(out * self.proj, axis=1, keepdims=1))\n            return np.concatenate(ret, axis=1).astype(np.uint32)\n\n    @train_required\n    @batching\n    def encode(self, vecs: np.ndarray, *args, **kwargs) -> np.ndarray:\n        if vecs.shape[1] != self.vec_dim:\n            raise ValueError(\'input dimension error\')\n        clusters = self.pred_kmeans(vecs)\n        vecs = (vecs - self.mean) / self.var\n        outcome = self.hash(vecs)\n        return np.concatenate([clusters, outcome], axis=1)\n\n    def _copy_from(self, x: \'HashEncoder\') -> None:\n        self.num_bytes = x.num_bytes\n        self.num_bits = x.num_bits\n        self.num_idx = x.num_idx\n        self.kmeans_clusters = x.kmeans_clusters\n        self.centroids = x.centroids\n        self.method = x.method\n        self.x = x.x\n        self.vec_dim = x.vec_dim\n        self.hash_cores = x.hash_cores\n        self.mean = x.mean\n        self.var = x.var\n        self.is_trained = x.is_trained\n'"
gnes/encoder/numeric/pca.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nimport numpy as np\n\nfrom ..base import BaseNumericEncoder\nfrom ...helper import get_perm, batching, get_optimal_sample_size, train_required\n\n\nclass PCAEncoder(BaseNumericEncoder):\n    batch_size = 2048\n\n    def __init__(self, output_dim: int, whiten: bool=False, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.output_dim = output_dim\n        self.whiten = whiten\n        self.pca_components = None\n        self.mean = None\n\n\n    def post_init(self):\n        from sklearn.decomposition import IncrementalPCA\n        self.pca = IncrementalPCA(n_components=self.output_dim)\n\n\n    @batching\n    def train(self, vecs: np.ndarray, *args, **kwargs) -> None:\n        num_samples, num_dim = vecs.shape\n        if self.output_dim > num_samples:\n            if self.mean.size:\n                return\n            else:\n                raise ValueError(\'training PCA requires at least %d points, but %d was given\' % (self.output_dim, num_samples))\n\n        assert self.output_dim < num_dim, \'PCA output dimension should < data dimension, received (%d, %d)\' % (\n            self.output_dim, num_dim)\n\n        self.pca.partial_fit(vecs)\n\n        self.pca_components = np.transpose(self.pca.components_)\n        self.mean = self.pca.mean_.astype(\'float32\')\n        self.explained_variance = self.pca.explained_variance_.astype(\'float32\')\n\n\n    @train_required\n    @batching\n    def encode(self, vecs: np.ndarray, *args, **kwargs) -> np.ndarray:\n        X_transformed = np.matmul(vecs - self.mean, self.pca_components)\n        if self.whiten:\n            X_transformed /= np.sqrt(self.explained_variance)\n        return X_transformed\n\n\nclass PCALocalEncoder(BaseNumericEncoder):\n    batch_size = 2048\n\n    def __init__(self, output_dim: int, num_locals: int,\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert output_dim >= num_locals and output_dim % num_locals == 0, \\\n            \'output_dim should >= num_locals and can be divided by num_locals!\'\n        self.output_dim = output_dim\n        self.num_locals = num_locals\n        self.pca_components = None\n        self.mean = None\n\n    @batching(batch_size=get_optimal_sample_size, num_batch=1)\n    def train(self, vecs: np.ndarray, *args, **kwargs) -> None:\n        import faiss\n        num_samples, num_dim = vecs.shape\n        assert self.output_dim <= num_samples, \'training PCA requires at least %d points, but %d was given\' % (\n            self.output_dim, num_samples)\n        assert self.output_dim < num_dim, \'PCA output dimension should < data dimension, received (%d, %d)\' % (\n            self.output_dim, num_dim)\n\n        pca = faiss.PCAMatrix(num_dim, self.output_dim)\n        self.mean = np.mean(vecs, axis=0)  # 1 x 768\n        pca.train(vecs)\n        explained_variance_ratio = faiss.vector_to_array(pca.eigenvalues)[:self.output_dim]\n        components = faiss.vector_to_array(pca.PCAMat).reshape([-1, num_dim])[:self.output_dim]\n\n        # permutate engive according to variance\n        opt_order = get_perm(explained_variance_ratio, self.num_locals)\n        comp_tmp = np.reshape(components[opt_order], [self.output_dim, num_dim])\n\n        self.pca_components = np.transpose(comp_tmp)  # 768 x 200\n\n    @train_required\n    @batching\n    def encode(self, vecs: np.ndarray, *args, **kwargs) -> np.ndarray:\n        return np.matmul(vecs - self.mean, self.pca_components)\n\n    def _copy_from(self, x: \'PCALocalEncoder\') -> None:\n        self.output_dim = x.output_dim\n        self.pca_components = x.pca_components\n        self.mean = x.mean\n        self.num_locals = x.num_locals\n        self.is_trained = x.is_trained\n'"
gnes/encoder/numeric/pooling.py,9,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport os\nfrom typing import Tuple\n\nimport numpy as np\n\nfrom ..base import BaseNumericEncoder\nfrom ...helper import as_numpy_array\n\n\nclass PoolingEncoder(BaseNumericEncoder):\n    def __init__(self, pooling_strategy: str = \'REDUCE_MEAN\',\n                 backend: str = \'numpy\',\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        valid_poolings = {\'REDUCE_MEAN\', \'REDUCE_MAX\', \'REDUCE_MEAN_MAX\'}\n        valid_backends = {\'tensorflow\', \'numpy\', \'pytorch\', \'torch\'}\n\n        if pooling_strategy not in valid_poolings:\n            raise ValueError(\'""pooling_strategy"" must be one of %s\' % valid_poolings)\n        if backend not in valid_backends:\n            raise ValueError(\'""backend"" must be one of %s\' % valid_backends)\n        self.pooling_strategy = pooling_strategy\n        self.backend = backend\n\n    def post_init(self):\n        if self.backend in {\'pytorch\', \'torch\'}:\n            import torch\n            self.torch = torch\n        elif self.backend == \'tensorflow\':\n            os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'0\' if self.on_gpu else \'-1\'\n            import tensorflow as tf\n            self._tf_graph = tf.Graph()\n            config = tf.ConfigProto(device_count={\'GPU\': 1 if self.on_gpu else 0})\n            config.gpu_options.allow_growth = True\n            config.log_device_placement = False\n            self._sess = tf.Session(graph=self._tf_graph, config=config)\n            self.tf = tf\n\n    def mul_mask(self, x, m):\n        if self.backend in {\'pytorch\', \'torch\'}:\n            return self.torch.mul(x, m.unsqueeze(2))\n        elif self.backend == \'tensorflow\':\n            with self._tf_graph.as_default():\n                return x * self.tf.expand_dims(m, axis=-1)\n        elif self.backend == \'numpy\':\n            return x * np.expand_dims(m, axis=-1)\n\n    def minus_mask(self, x, m, offset: int = 1e30):\n        if self.backend in {\'pytorch\', \'torch\'}:\n            return x - (1.0 - m).unsqueeze(2) * offset\n        elif self.backend == \'tensorflow\':\n            with self._tf_graph.as_default():\n                return x - self.tf.expand_dims(1.0 - m, axis=-1) * offset\n        elif self.backend == \'numpy\':\n            return x - np.expand_dims(1.0 - m, axis=-1) * offset\n\n    def masked_reduce_mean(self, x, m, jitter: float = 1e-10):\n        if self.backend in {\'pytorch\', \'torch\'}:\n            return self.torch.div(self.torch.sum(self.mul_mask(x, m), dim=1),\n                                  self.torch.sum(m.unsqueeze(2), dim=1) + jitter)\n        elif self.backend == \'tensorflow\':\n            with self._tf_graph.as_default():\n                return self.tf.reduce_sum(self.mul_mask(x, m), axis=1) / (\n                        self.tf.reduce_sum(m, axis=1, keepdims=True) + jitter)\n        elif self.backend == \'numpy\':\n            return np.sum(self.mul_mask(x, m), axis=1) / (np.sum(m, axis=1, keepdims=True) + jitter)\n\n    def masked_reduce_max(self, x, m):\n        if self.backend in {\'pytorch\', \'torch\'}:\n            return self.torch.max(self.minus_mask(x, m), 1)[0]\n        elif self.backend == \'tensorflow\':\n            with self._tf_graph.as_default():\n                return self.tf.reduce_max(self.minus_mask(x, m), axis=1)\n        elif self.backend == \'numpy\':\n            return np.max(self.minus_mask(x, m), axis=1)\n\n    @as_numpy_array\n    def encode(self, data: Tuple, *args, **kwargs):\n        seq_tensor, mask_tensor = data\n\n        if self.pooling_strategy == \'REDUCE_MEAN\':\n            r = self.masked_reduce_mean(seq_tensor, mask_tensor)\n        elif self.pooling_strategy == \'REDUCE_MAX\':\n            r = self.masked_reduce_max(seq_tensor, mask_tensor)\n        elif self.pooling_strategy == \'REDUCE_MEAN_MAX\':\n            if self.backend in {\'pytorch\', \'torch\'}:\n                r = self.torch.cat((self.masked_reduce_mean(seq_tensor, mask_tensor),\n                                    self.masked_reduce_max(seq_tensor, mask_tensor)), dim=1)\n            elif self.backend == \'tensorflow\':\n                with self._tf_graph.as_default():\n                    r = self.tf.concat([self.masked_reduce_mean(seq_tensor, mask_tensor),\n                                        self.masked_reduce_max(seq_tensor, mask_tensor)], axis=1)\n            elif self.backend == \'numpy\':\n                r = np.concatenate([self.masked_reduce_mean(seq_tensor, mask_tensor),\n                                    self.masked_reduce_max(seq_tensor, mask_tensor)], axis=1)\n\n        if self.backend == \'tensorflow\':\n            r = self._sess.run(r)\n        return r\n'"
gnes/encoder/numeric/pq.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nimport numpy as np\n\nfrom ..base import BaseBinaryEncoder\nfrom ...helper import batching, train_required\n\n\nclass PQEncoder(BaseBinaryEncoder):\n    batch_size = 2048\n\n    def __init__(self, num_bytes: int, cluster_per_byte: int = 255, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert 1 < cluster_per_byte <= 255, \'cluster number should >1 and <= 255 (0 is reserved for NOP)\'\n        self.num_bytes = num_bytes\n        self.num_clusters = cluster_per_byte\n        self.centroids = None\n\n    def train(self, vecs: np.ndarray, *args, **kwargs):\n        import faiss\n\n        dim_per_byte = self._get_dim_per_byte(vecs)\n\n        # use faiss ProductQuantizer directly\n        vecs = vecs.astype(np.float32)\n        model = faiss.ProductQuantizer(vecs.shape[1], self.num_bytes, 8)\n        model.ksub = self.num_clusters\n        model.byte_per_idx = 1\n\n        model.train(vecs)\n        centroids = faiss.vector_to_array(model.centroids)\n        centroids = centroids[: self.num_clusters * vecs.shape[1]]\n        self.centroids = np.reshape(centroids, [1, self.num_bytes,\n                                                self.num_clusters,\n                                                dim_per_byte])\n\n    @train_required\n    @batching\n    def encode(self, vecs: np.ndarray, *args, **kwargs) -> np.ndarray:\n        dim_per_byte = self._get_dim_per_byte(vecs)\n\n        x = np.reshape(vecs, [vecs.shape[0], self.num_bytes, 1, dim_per_byte])\n        x = np.sum(np.square(x - self.centroids), -1)\n        # start from 1\n        x = np.argmax(-x, 2) + 1\n\n        return np.array(x, dtype=np.uint8)\n\n    def _get_dim_per_byte(self, vecs: np.ndarray):\n        num_dim = vecs.shape[1]\n        assert num_dim >= self.num_bytes and num_dim % self.num_bytes == 0, \\\n            \'input dimension (=%d) should >= num_bytes (=%d) and can be divided by num_bytes!\' % (\n                num_dim, self.num_bytes)\n        return int(num_dim / self.num_bytes)\n\n    def _copy_from(self, x: \'PQEncoder\') -> None:\n        self.num_bytes = x.num_bytes\n        self.num_clusters = x.num_clusters\n        self.centroids = x.centroids\n        self.is_trained = x.is_trained\n'"
gnes/encoder/numeric/quantizer.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nimport numpy as np\nfrom itertools import product\n\nfrom ..base import BaseBinaryEncoder\nfrom ...helper import batching\n\n\nclass QuantizerEncoder(BaseBinaryEncoder):\n    batch_size = 2048\n\n    def __init__(self, dim_per_byte: int, cluster_per_byte: int = 255,\n                 upper_bound: int = 10000,\n                 lower_bound: int = -10000,\n                 partition_method: str = \'average\',\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert 1 < cluster_per_byte <= 255, \'cluster number should >1 and <= 255 (0 is reserved for NOP)\'\n        self.dim_per_byte = dim_per_byte\n        self.num_clusters = cluster_per_byte\n        self.upper_bound = upper_bound\n        self.lower_bound = lower_bound\n        self.partition_method = partition_method\n        self.centroids = self._get_centroids()\n\n    def _get_centroids(self):\n        """"""\n        calculate centroids for quantizer\n        two kinds of divide methods are supported now: average, random\n        average: split the space averagely and centroids of clusters lie on the corner of sub-space\n        random: randomly pick points and treat them as centroids of clusters\n        Variable Explaination:\n            num_sample_per_dim: number of points to be sample on each dimension\n        """"""\n\n        if self.upper_bound < self.lower_bound:\n            raise ValueError(""upper bound is smaller than lower bound"")\n\n        centroids = []\n        num_sample_per_dim = np.ceil(pow(self.num_clusters, 1 / self.dim_per_byte)).astype(np.uint8)\n        if self.partition_method == \'average\':\n            axis_point = np.linspace(self.lower_bound, self.upper_bound, num=num_sample_per_dim+1,\n                                     endpoint=False, retstep=False, dtype=None)[1:]\n            coordinates = np.tile(axis_point, (self.dim_per_byte, 1))\n        elif self.partition_method == \'random\':\n            coordinates = np.random.uniform(self.lower_bound, self.upper_bound,\n                                                    size=[self.dim_per_byte, num_sample_per_dim])\n        else:\n            raise NotImplementedError\n\n        for item in product(*coordinates):\n            centroids.append(list(item))\n        return centroids[:self.num_clusters]\n\n    @batching\n    def encode(self, vecs: np.ndarray, *args, **kwargs) -> np.ndarray:\n        self._check_bound(vecs)\n        num_bytes = self._get_num_bytes(vecs)\n        x = np.reshape(vecs, [vecs.shape[0], num_bytes, 1, self.dim_per_byte])\n        x = np.sum(np.square(x - self.centroids), -1)\n        # start from 1\n        x = np.argmax(-x, 2) + 1\n\n        return np.array(x, dtype=np.uint8)\n\n    def _get_num_bytes(self, vecs: np.ndarray):\n        num_dim = vecs.shape[1]\n        assert num_dim % self.dim_per_byte == 0 and num_dim >= (num_dim % self.dim_per_byte), \\\n            \'input dimension (=%d) should be divided by dim_per_byte (=%d)!\' % (\n                num_dim, self.dim_per_byte)\n        return int(num_dim / self.dim_per_byte)\n\n    @staticmethod\n    def _get_max_min_value(vecs):\n        return np.amax(vecs, axis=None), np.amin(vecs, axis=None)\n\n    def _check_bound(self, vecs):\n        max_value, min_value = self._get_max_min_value(vecs)\n        if self.upper_bound < max_value:\n            raise Warning(""upper bound (=%.3f) is smaller than max value of input data (=%.3f), you should choose""\n                                ""a bigger value for upper bound"" % (self.upper_bound, max_value))\n        if self.lower_bound > min_value:\n            raise Warning(""lower bound (=%.3f) is bigger than min value of input data (=%.3f), you should choose""\n                                ""a smaller value for lower bound"" % (self.lower_bound, min_value))\n        if (self.upper_bound-self.lower_bound) >= 10*(max_value - min_value):\n            raise Warning(""(upper bound - lower_bound) (=%.3f) is 10 times larger than (max value - min value) ""\n                                ""(=%.3f) of data, maybe you should choose a suitable bound"" %\n                                ((self.upper_bound-self.lower_bound), (max_value - min_value)))\n'"
gnes/encoder/numeric/standarder.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nimport numpy as np\n\nfrom ..base import BaseNumericEncoder\nfrom ...helper import batching, train_required\n\n\nclass StandarderEncoder(BaseNumericEncoder):\n    batch_size = 2048\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.mean = None\n        self.scale = None\n\n    def post_init(self):\n        from sklearn.preprocessing import StandardScaler\n        self.standarder = StandardScaler()\n\n    @batching\n    def train(self, vecs: np.ndarray, *args, **kwargs) -> None:\n        self.standarder.partial_fit(vecs)\n\n        self.mean = self.standarder.mean_.astype(\'float32\')\n        self.scale = self.standarder.scale_.astype(\'float32\')\n\n    @train_required\n    @batching\n    def encode(self, vecs: np.ndarray, *args, **kwargs) -> np.ndarray:\n        return (vecs - self.mean) / self.scale'"
gnes/encoder/numeric/tf_pq.py,12,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nfrom typing import Dict, Any\n\nimport numpy as np\n\nfrom .pq import PQEncoder\nfrom ...helper import batching, train_required, get_first_available_gpu\n\n\nclass TFPQEncoder(PQEncoder):\n    batch_size = 8192\n\n    @classmethod\n    def pre_init(cls):\n        import os\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = str(get_first_available_gpu())\n\n    def post_init(self):\n        import tensorflow as tf\n        self._graph = self._get_graph()\n        self._sess = tf.Session()\n        self._sess.run(tf.global_variables_initializer())\n\n    def _get_graph(self) -> Dict[str, Any]:\n        import tensorflow as tf\n        ph_x = tf.placeholder(tf.float32, [None, self.num_bytes, None])\n        ph_centroids = tf.placeholder(tf.float32, [1, self.num_bytes, self.num_clusters, None])\n        centroids_squeezed = tf.squeeze(ph_centroids, 0)\n        # [self.num_bytes, None, self.m]\n        x = tf.transpose(ph_x, [1, 0, 2])\n        ty = tf.reduce_sum(tf.square(centroids_squeezed), axis=2, keepdims=True)\n        ty = tf.transpose(ty, [0, 2, 1])\n        tx = tf.reduce_sum(tf.square(x), axis=2, keepdims=True)\n        diff = tf.matmul(x, tf.transpose(centroids_squeezed, [0, 2, 1]))\n        diff = tx + ty - 2 * diff\n        # start from 1\n        p = tf.argmax(-diff, axis=2) + 1\n        p = tf.transpose(p, [1, 0])\n        return {\n            \'out\': p,\n            \'ph_x\': ph_x,\n            \'ph_centroids\': ph_centroids\n        }\n\n    @train_required\n    @batching\n    def encode(self, vecs: np.ndarray, *args, **kwargs) -> np.ndarray:\n        vecs = np.reshape(vecs, [vecs.shape[0], self.num_bytes, -1])\n        tmp = self._sess.run(self._graph[\'out\'],\n                             feed_dict={self._graph[\'ph_x\']: vecs,\n                                        self._graph[\'ph_centroids\']: self.centroids})\n        return tmp.astype(np.uint8)\n\n    def close(self):\n        self._sess.close()\n'"
gnes/encoder/numeric/vlad.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nimport numpy as np\n\nfrom ..base import BaseNumericEncoder\nfrom ...helper import batching, train_required\n\n\nclass VladEncoder(BaseNumericEncoder):\n    batch_size = 2048\n\n    def __init__(self, num_clusters: int,\n                 using_faiss_pred: bool = False,\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.num_clusters = num_clusters\n        self.using_faiss_pred = using_faiss_pred\n        self.centroids = None\n        self.index_flat = None\n\n    def kmeans_train(self, vecs):\n        import faiss\n        kmeans = faiss.Kmeans(vecs.shape[1], self.num_clusters, niter=5, verbose=False)\n        kmeans.train(vecs)\n        self.centroids = kmeans.centroids\n        self.centroids_l2 = np.sum(self.centroids**2, axis=1).reshape([1, -1])\n        self.centroids_trans = np.transpose(self.centroids)\n        if self.using_faiss_pred:\n            self.faiss_index()\n\n    def faiss_index(self):\n        import faiss\n        self.index_flat = faiss.IndexFlatL2(self.centroids.shape[1])\n        self.index_flat.add(self.centroids)\n\n    def kmeans_pred(self, vecs):\n        if self.using_faiss_pred:\n            _, pred = self.index_flat.search(vecs.astype(np.float32), 1)\n            return np.reshape(pred, [-1])\n        else:\n            vecs_l2 = np.sum(vecs**2, axis=1).reshape([-1, 1])\n            dist = vecs_l2 + self.centroids_l2 - 2 * np.matmul(vecs, self.centroids_trans)\n            return np.argmax(dist, axis=-1).reshape([-1]).astype(np.int32)\n\n    @batching\n    def train(self, vecs: np.ndarray, *args, **kwargs):\n        vecs = vecs.reshape([-1, vecs.shape[-1]])\n        assert len(vecs) > self.num_clusters, \'number of data should be larger than number of clusters\'\n        self.kmeans_train(vecs)\n\n    @train_required\n    @batching\n    def encode(self, vecs: np.ndarray, *args, **kwargs) -> np.ndarray:\n        knn_output = [self.kmeans_pred(vecs_) for vecs_ in vecs]\n\n        output = []\n        for chunk_count, chunk in enumerate(vecs):\n            res = np.zeros((self.centroids.shape[0], self.centroids.shape[1]))\n            for frame_count, frame in enumerate(chunk):\n                center_index = knn_output[chunk_count][frame_count]\n                res[center_index] += (frame - self.centroids[center_index])\n            res = res.reshape([-1])\n            output.append(res / np.sum(res**2)**0.5)\n\n        return np.array(output, dtype=np.float32)\n\n    def _copy_from(self, x: \'VladEncoder\') -> None:\n        self.num_clusters = x.num_clusters\n        self.centroids = x.centroids\n        self.centroids_l2 = x.centroids_l2\n        self.centroids_trans = np.transpose(self.centroids)\n        self.using_faiss_pred = x.using_faiss_pred\n        if self.using_faiss_pred:\n            self.faiss_index()\n\n    def __setstate__(self, state):\n        super().__setstate__(state)\n        if self.using_faiss_pred:\n            self.faiss_index()\n\n    def __getstate__(self):\n        state = super().__getstate__()\n        del state[\'index_flat\']\n        return state\n'"
gnes/encoder/text/__init__.py,0,b''
gnes/encoder/text/bert.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nfrom typing import List\n\nimport numpy as np\n\nfrom ..base import CompositionalTrainableBase, BaseTextEncoder\nfrom ...helper import batching\n\n\nclass BertEncoder(BaseTextEncoder):\n    store_args_kwargs = True\n    is_trained = True\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._bc_encoder_args = args\n        self._bc_encoder_kwargs = kwargs\n\n    def post_init(self):\n        from bert_serving.client import BertClient\n        self.bc_encoder = BertClient(*self._bc_encoder_args, **self._bc_encoder_kwargs)\n\n    @batching\n    def encode(self, text: List[str], *args, **kwargs) -> np.ndarray:\n        return self.bc_encoder.encode(text, *args, **kwargs)  # type: np.ndarray\n\n    def close(self):\n        self.bc_encoder.close()\n\n\nclass BertEncoderWithServer(CompositionalTrainableBase):\n    def encode(self, text: List[str], *args, **kwargs) -> np.ndarray:\n        return self.components[\'bert_client\'].encode(text, *args, **kwargs)\n\n\nclass BertEncoderServer(BaseTextEncoder):\n    store_args_kwargs = True\n    is_trained = True\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        bert_args = [\'-%s\' % v for v in args]\n        for k, v in kwargs.items():\n            bert_args.append(\'-%s\' % k)\n            bert_args.append(str(v))\n        self._bert_args = bert_args\n\n    def post_init(self):\n        from bert_serving.server import BertServer\n        from bert_serving.server import get_args_parser\n        self.bert_server = BertServer(get_args_parser().parse_args(self._bert_args))\n        self.bert_server.start()\n        self.bert_server.is_ready.wait()\n\n    def close(self):\n        self.bert_server.close()\n'"
gnes/encoder/text/char.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nfrom typing import List\n\nimport numpy as np\n\nfrom ..base import BaseTextEncoder\nfrom ...helper import batching, as_numpy_array\n\n\nclass CharEmbeddingEncoder(BaseTextEncoder):\n    """"""A random character embedding model. Only useful for testing""""""\n    is_trained = True\n\n    def __init__(self, dim: int = 128, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.dim = dim\n        self.offset = 32\n        self.unknown_idx = 96\n        # in total 96 printable chars and 2 special chars = 98\n        self._char_embedding = np.random.random([97, dim])\n\n    @batching\n    @as_numpy_array\n    def encode(self, text: List[str], *args, **kwargs) -> List[np.ndarray]:\n        # tokenize text\n        sent_embed = []\n        for sent in text:\n            ids = [ord(c) - 32 if 32 <= ord(c) <= 127 else self.unknown_idx for c in sent]\n            sent_embed.append(np.mean(self._char_embedding[ids], axis=0))\n        return sent_embed\n'"
gnes/encoder/text/flair.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nfrom typing import List, Tuple\n\nimport numpy as np\n\nfrom ..base import BaseTextEncoder\nfrom ...helper import batching, as_numpy_array\n\n\nclass FlairEncoder(BaseTextEncoder):\n    is_trained = True\n\n    def __init__(self,\n                 word_embedding: str = \'glove\',\n                 flair_embeddings: Tuple[str] = (\'news-forward\', \'news-backward\'),\n                 pooling_strategy: str = \'mean\', *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self.word_embedding = word_embedding\n        self.flair_embeddings = flair_embeddings\n        self.pooling_strategy = pooling_strategy\n\n    def post_init(self):\n        from flair.embeddings import DocumentPoolEmbeddings, WordEmbeddings, FlairEmbeddings\n        self._flair = DocumentPoolEmbeddings(\n            [WordEmbeddings(self.word_embedding),\n             FlairEmbeddings(self.flair_embeddings[0]),\n             FlairEmbeddings(self.flair_embeddings[1])],\n            pooling=self.pooling_strategy)\n\n    @batching\n    @as_numpy_array\n    def encode(self, text: List[str], *args, **kwargs) -> np.ndarray:\n        from flair.data import Sentence\n        import torch\n        # tokenize text\n        batch_tokens = [Sentence(v) for v in text]\n        self._flair.embed(batch_tokens)\n        return torch.stack([v.embedding for v in batch_tokens]).detach()\n'"
gnes/encoder/text/transformer.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nfrom typing import List, Tuple\n\nimport torch\n\nfrom ..base import BaseTextEncoder\nfrom ...helper import batching\n\n\nclass PyTorchTransformers(BaseTextEncoder):\n    is_trained = True\n\n    def __init__(self,\n                 model_name: str = \'bert-base-uncased\',\n                 pooling_layer: int = 0,\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.model_name = model_name\n        self.pooling_layer = pooling_layer\n\n    def post_init(self):\n        from pytorch_transformers import BertModel, BertTokenizer, \\\n            OpenAIGPTModel, OpenAIGPTTokenizer, GPT2Model, GPT2Tokenizer, \\\n            TransfoXLModel, TransfoXLTokenizer, XLNetModel, XLNetTokenizer, \\\n            XLMModel, XLMTokenizer, RobertaModel, RobertaTokenizer\n        # select the model, tokenizer & weight accordingly\n        model_class, tokenizer_class, pretrained_weights = \\\n            {k[-1]: k for k in\n             [(BertModel, BertTokenizer, \'bert-base-uncased\'),\n              (OpenAIGPTModel, OpenAIGPTTokenizer, \'openai-gpt\'),\n              (GPT2Model, GPT2Tokenizer, \'gpt2\'),\n              (TransfoXLModel, TransfoXLTokenizer, \'transfo-xl-wt103\'),\n              (XLNetModel, XLNetTokenizer, \'xlnet-base-cased\'),\n              (XLMModel, XLMTokenizer, \'xlm-mlm-enfr-1024\'),\n              (RobertaModel, RobertaTokenizer, \'roberta-base\')]}[self.model_name]\n\n        def load_model_tokenizer(x):\n            return model_class.from_pretrained(x).eval(), tokenizer_class.from_pretrained(x)\n\n        try:\n            self.model, self.tokenizer = load_model_tokenizer(self.work_dir)\n        except Exception:\n            self.logger.warning(\'cannot deserialize model/tokenizer from %s, will download from web\' % self.work_dir)\n            self.model, self.tokenizer = load_model_tokenizer(pretrained_weights)\n\n    @batching\n    def encode(self, text: List[str], *args, **kwargs) -> Tuple:\n        # encoding and padding\n        ids = [self.tokenizer.encode(t) for t in text]\n        max_len = max(len(t) for t in ids)\n        ids = [t + [0] * (max_len - len(t)) for t in ids]\n        m_ids = [[1] * len(t) + [0] * (max_len - len(t)) for t in ids]\n        seq_ids = torch.tensor(ids)\n        mask_ids = torch.tensor(m_ids, dtype=torch.float32)\n\n        if self.on_gpu:\n            seq_ids = seq_ids.cuda()\n\n        with torch.no_grad():\n            last_hidden_states = self.model(seq_ids)[self.pooling_layer]\n\n        return last_hidden_states.cpu(), mask_ids\n\n    def __getstate__(self):\n        self.model.save_pretrained(self.work_dir)\n        self.tokenizer.save_pretrained(self.work_dir)\n        return super().__getstate__()\n'"
gnes/encoder/text/w2v.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nfrom typing import List\n\nimport numpy as np\n\nfrom ..base import BaseTextEncoder\nfrom ...helper import batching, pooling_simple, as_numpy_array\n\n\nclass Word2VecEncoder(BaseTextEncoder):\n    is_trained = True\n\n    def __init__(self, model_dir: str,\n                 skiprows: int = 1,\n                 dimension: int = 300,\n                 pooling_strategy: str = \'REDUCE_MEAN\', *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.model_dir = model_dir\n        self.skiprows = skiprows\n        self.pooling_strategy = pooling_strategy\n        self.dimension = dimension\n\n    def post_init(self):\n        from ...helper import Tokenizer\n        count = 0\n        self.word2vec_df = {}\n        with open(self.model_dir, \'r\') as f:\n            for line in f.readlines():\n                line = line.strip().split(\' \')\n                if count < self.skiprows:\n                    count += 1\n                    continue\n                if len(line) > self.dimension:\n                    self.word2vec_df[line[0]] = np.array([float(i) for i in line[1:]], dtype=np.float32)\n\n        self.empty = np.zeros([self.dimension], dtype=np.float32)\n        self.cn_tokenizer = Tokenizer()\n\n    @batching\n    @as_numpy_array\n    def encode(self, text: List[str], *args, **kwargs) -> np.ndarray:\n        # tokenize text\n        batch_tokens = [self.cn_tokenizer.tokenize(sent) for sent in text]\n        pooled_data = []\n\n        for tokens in batch_tokens:\n            _layer_data = [self.word2vec_df.get(token, self.empty) for token in tokens]\n            pooled_data.append(pooling_simple(_layer_data, self.pooling_strategy))\n\n        return pooled_data\n'"
gnes/encoder/video/__init__.py,0,b''
gnes/encoder/video/incep_mixture.py,11,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nfrom typing import List\n\nimport numpy as np\nfrom PIL import Image\n\nfrom ..base import BaseVideoEncoder\nfrom ...helper import batching, get_first_available_gpu\n\n\nclass IncepMixtureEncoder(BaseVideoEncoder):\n    batch_size = 64\n\n    def __init__(self, model_dir_inception: str,\n                 model_dir_mixture: str,\n                 select_layer: str = \'PreLogitsFlatten\',\n                 feature_size: int = 300,\n                 vocab_size: int = 28,\n                 cluster_size: int = 256,\n                 method: str = \'fvnet\',\n                 input_size: int = 1536,\n                 vocab_size_2: int = 174,\n                 max_frames: int = 30,\n                 multitask_method: str = \'Attention\',\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.model_dir_inception = model_dir_inception\n        self.model_dir_mixture = model_dir_mixture\n        self.select_layer = select_layer\n        self.cluster_size = cluster_size\n        self.feature_size = feature_size\n        self.vocab_size = vocab_size\n        self.method = method\n        self.input_size = input_size\n        self.multitask_method = multitask_method\n        self.inception_size_x = 299\n        self.inception_size_y = 299\n        self.max_frames = max_frames\n        self.vocab_size_2 = vocab_size_2\n\n    def post_init(self):\n        import tensorflow as tf\n        from ..image.inception_cores.inception_v4 import inception_v4\n        from ..image.inception_cores.inception_utils import inception_arg_scope\n        from .mixture_core.model import NetFV\n        import os\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = str(get_first_available_gpu())\n\n        g = tf.Graph()\n        with g.as_default():\n            arg_scope = inception_arg_scope()\n            inception_v4.default_image_size = self.inception_size_x\n            self.inputs = tf.placeholder(tf.float32, (None,\n                                                      self.inception_size_x,\n                                                      self.inception_size_y, 3))\n\n            with tf.contrib.slim.arg_scope(arg_scope):\n                self.logits, self.end_points = inception_v4(self.inputs,\n                                                            is_training=False,\n                                                            dropout_keep_prob=1.0)\n\n            config = tf.ConfigProto(log_device_placement=False)\n            if self.on_gpu:\n                config.gpu_options.allow_growth = True\n            self.sess = tf.Session(config=config)\n            self.saver = tf.train.Saver()\n            self.saver.restore(self.sess, self.model_dir_inception)\n\n        g2 = tf.Graph()\n        with g2.as_default():\n            config = tf.ConfigProto(log_device_placement=False)\n            if self.on_gpu:\n                config.gpu_options.allow_growth = True\n            self.sess2 = tf.Session(config=config)\n            self.mix_model = NetFV(feature_size=self.feature_size,\n                                   cluster_size=self.cluster_size,\n                                   vocab_size=self.vocab_size,\n                                   input_size=self.input_size,\n                                   use_2nd_label=True,\n                                   vocab_size_2=self.vocab_size_2,\n                                   multitask_method=self.multitask_method,\n                                   method=self.method,\n                                   is_training=False)\n            saver = tf.train.Saver(max_to_keep=1)\n            self.sess2.run(tf.global_variables_initializer())\n            saver.restore(self.sess2, self.model_dir_mixture)\n\n    def encode(self, data: List[\'np.ndarray\'], *args, **kwargs) -> np.ndarray:\n        v_len = [len(v) for v in data]\n        pos_start = [0] + [sum(v_len[:i + 1]) for i in range(len(v_len) - 1)]\n        pos_end = [sum(v_len[:i + 1]) for i in range(len(v_len))]\n        max_len = min(max(v_len), self.max_frames)\n\n        img = [im for v in data for im in v]\n        img = [(np.array(Image.fromarray(im).resize((self.inception_size_x,\n                                                     self.inception_size_y)), dtype=np.float32) * 2 / 255. - 1.) for im\n               in img]\n\n        @batching(concat_axis=None)\n        def _encode1(self, data):\n            _, end_points_ = self.sess.run((self.logits, self.end_points),\n                                           feed_dict={self.inputs: data})\n            return end_points_[self.select_layer]\n\n        if len(img) <= self.batch_size:\n            v = [_ for _ in _encode1(self, img)]\n        else:\n            v = [_ for vi in _encode1(self, img) for _ in vi]\n\n        v_input = [v[s:e] for s, e in zip(pos_start, pos_end)]\n        v_input = [(vi + [[0.0] * self.input_size] * (max_len - len(vi)))[:max_len] for vi in v_input]\n        v_input = [np.array(vi, dtype=np.float32) for vi in v_input]\n\n        @batching\n        def _encode2(self, data):\n            return self.sess2.run(self.mix_model.repre,\n                                  feed_dict={self.mix_model.feeds: data})\n\n        return _encode2(self, v_input).astype(np.float32)\n'"
gnes/encoder/video/inception.py,7,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nfrom typing import List\n\nimport numpy as np\nfrom PIL import Image\n\nfrom ..base import BaseVideoEncoder\nfrom ...helper import batching, get_first_available_gpu\n\n\nclass InceptionVideoEncoder(BaseVideoEncoder):\n    batch_size = 64\n\n    def __init__(self,\n                 model_dir: str,\n                 select_layer: str = \'PreLogitsFlatten\',\n                 *args,\n                 **kwargs):\n        super().__init__(*args, **kwargs)\n        self.model_dir = model_dir\n        self.select_layer = select_layer\n        self.inception_size_x = 299\n        self.inception_size_y = 299\n\n    def post_init(self):\n        import tensorflow as tf\n        from ..image.inception_cores.inception_v4 import inception_v4\n        from ..image.inception_cores.inception_utils import inception_arg_scope\n        import os\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = str(get_first_available_gpu())\n\n        g = tf.Graph()\n        with g.as_default():\n            arg_scope = inception_arg_scope()\n            inception_v4.default_image_size = self.inception_size_x\n            self.inputs = tf.placeholder(\n                tf.float32,\n                (None, self.inception_size_x, self.inception_size_y, 3))\n\n            with tf.contrib.slim.arg_scope(arg_scope):\n                self.logits, self.end_points = inception_v4(\n                    self.inputs, is_training=False, dropout_keep_prob=1.0)\n\n            config = tf.ConfigProto(log_device_placement=False)\n            if self.on_gpu:\n                config.gpu_options.allow_growth = True\n            self.sess = tf.Session(config=config)\n            self.saver = tf.train.Saver()\n            self.saver.restore(self.sess, self.model_dir)\n\n    def encode(self, data: List[\'np.ndarray\'], *args,\n               **kwargs) -> List[\'np.ndarray\']:\n        v_len = [len(v) for v in data]\n        pos_start = [0] + [sum(v_len[:i + 1]) for i in range(len(v_len) - 1)]\n        pos_end = [sum(v_len[:i + 1]) for i in range(len(v_len))]\n\n        _resize = lambda x: np.array(Image.fromarray(x).resize((self.inception_size_x, self.inception_size_y)), dtype=np.float32) * 2 / 255. - 1.\n\n        images = [_resize(im) for v in data for im in v]\n\n        @batching\n        def _encode(self, data):\n            _, end_points_ = self.sess.run((self.logits, self.end_points),\n                                           feed_dict={self.inputs: data})\n            return end_points_[self.select_layer]\n\n        encodes = _encode(self, images).astype(np.float32)\n\n        return [encodes[s:e].copy() for s, e in zip(pos_start, pos_end)]\n'"
gnes/encoder/video/yt8m_feature_extractor.py,6,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nfrom typing import List\nimport numpy as np\nfrom PIL import Image\nfrom ..base import BaseVideoEncoder\nfrom ...helper import batching, get_first_available_gpu\n\n\nclass YouTube8MFeatureExtractor(BaseVideoEncoder):\n    """"""Extracts YouTube8M features for RGB frames.\n\n    First time constructing this class will create directory `yt8m` inside your\n    home directory, and will download inception model (85 MB) and YouTube8M PCA\n    matrix (15 MB). If you want to use another directory, then pass it to argument\n    `model_dir` of constructor.\n\n    If the model_dir exist and contains the necessary files, then files will be\n    re-used without download.\n\n    Usage Example:\n\n        from PIL import Image\n        import numpy\n\n        # Instantiate extractor. Slow if called first time on your machine, as it\n        # needs to download 100 MB.\n        extractor = YouTube8MFeatureExtractor()\n\n        image_file = os.path.join(extractor._model_dir, \'cropped_panda.jpg\')\n\n        im = numpy.array(Image.open(image_file))\n        features = extractor.extract_rgb_frame_features(im)\n\n    ** Note: OpenCV reverses the order of channels (i.e. orders channels as BGR\n    instead of RGB). If you are using OpenCV, then you must do:\n\n        im = im[:, :, ::-1]  # Reverses order on last (i.e. channel) dimension.\n\n    then call `extractor.extract_rgb_frame_features(im)`\n    """"""\n    batch_size = 64\n\n    def __init__(self, model_dir: str,\n                 pca_dir: str,\n                 select_layer: str = \'PreLogits\',\n                 ignore_audio_feature: bool = True,\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self.model_dir = model_dir\n        self.pca_dir = pca_dir\n        self.select_layer = select_layer\n        self.ignore_audio_feature = ignore_audio_feature\n        self.audio_dim = 128\n        self.incep_dim = 2048\n        self.pca_dim = 1024\n        self.inception_size_x = 299\n        self.inception_size_y = 299\n\n    def post_init(self):\n        import tensorflow as tf\n        from .yt8m_feature_extractor_cores.inception_v3 import inception_v3\n        from .yt8m_feature_extractor_cores.inception_utils import inception_arg_scope\n        import os\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = str(get_first_available_gpu())\n\n        self.pca_mean = np.load(os.path.join(self.pca_dir, \'mean.npy\'))[:, 0]\n        self.pca_eigenvals = np.load(os.path.join(self.pca_dir, \'eigenvals.npy\'))[:self.pca_dim, 0]\n        self.pca_eigenvecs = np.load(os.path.join(self.pca_dir, \'eigenvecs.npy\')).T[:, :self.pca_dim]\n\n        g = tf.Graph()\n        with g.as_default():\n            arg_scope = inception_arg_scope()\n            inception_v3.default_image_size = self.inception_size_x\n            self.inputs = tf.placeholder(tf.float32, (None,\n                                                      self.inception_size_x,\n                                                      self.inception_size_y, 3))\n\n            with tf.contrib.slim.arg_scope(arg_scope):\n                self.logits, self.end_points = inception_v3(self.inputs,\n                                                            num_classes=1001,\n                                                            is_training=False,\n                                                            dropout_keep_prob=1.0)\n\n            config = tf.ConfigProto(log_device_placement=False)\n            if self.on_gpu:\n                config.gpu_options.allow_growth = True\n            self.sess = tf.Session(config=config)\n            self.saver = tf.train.Saver()\n            self.saver.restore(self.sess, self.model_dir)\n\n    def encode(self, data: List[\'np.ndarray\'], *args, **kwargs) -> List[\'np.ndarray\']:\n        video_length = [len(d) for d in data]\n        for i in range(1, len(video_length)):\n            video_length[i] = video_length[i] + video_length[i - 1]\n\n        data = [(np.array(Image.fromarray(im).resize((self.inception_size_x,\n                                                      self.inception_size_y)), dtype=np.float32) * 2 / 255. - 1.)\n                for video in data for im in video]\n\n        data = np.stack((list(data[i] for i in range(len(data)))), axis=0)\n\n        @batching\n        def _encode(_, data):\n            def _pca(data):\n                data = np.squeeze(data, axis=(1, 2))\n                data = (data - self.pca_mean).reshape((len(data), self.incep_dim))\n                data = np.matmul(data, self.pca_eigenvecs)\n                data = data / np.sqrt(self.pca_eigenvals + 1e-4)\n                return data\n\n            _, end_points_ = self.sess.run((self.logits, self.end_points),\n                                           feed_dict={self.inputs: data})\n            data = _pca(end_points_[self.select_layer])\n            return data\n\n        def _fill_audio_feature(data):\n            return list(map(lambda x: np.concatenate((x, np.zeros(shape=(x.shape[0], self.audio_dim))), axis=1), data))\n\n        data = _encode(self, data)\n\n        data = np.split(data, video_length[:-1])\n\n        if self.ignore_audio_feature:\n            return _fill_audio_feature(data)\n        else:\n            return data.astype(np.float32)\n\n'"
gnes/encoder/video/yt8m_model.py,6,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nfrom typing import List\nimport numpy as np\nfrom ..base import BaseVideoEncoder\nfrom ...helper import batching, get_first_available_gpu\n\n\nclass YouTube8MEncoder(BaseVideoEncoder):\n    batch_size = 64\n\n    def __init__(self, model_dir: str,\n                 model_name: str,\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self.model_dir = model_dir\n        self.model_name = model_name\n        self.max_num_frames = 300\n        self.embedding_dim = 1152\n\n    def post_init(self):\n        import tensorflow as tf\n        import os\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = str(get_first_available_gpu())\n\n        g = tf.Graph()\n        with g.as_default():\n            checkpoint_file = os.path.join(self.model_dir, self.model_name,\n                                           ""inference_model"")\n            meta_graph_location = checkpoint_file + "".meta""\n            saver = tf.train.import_meta_graph(meta_graph_location, clear_devices=True)\n\n            config = tf.ConfigProto(log_device_placement=False)\n            if self.on_gpu:\n                config.gpu_options.allow_growth = True\n            self.sess = tf.Session(config=config)\n            saver.restore(self.sess, checkpoint_file)\n\n            self.input_tensor = tf.get_collection(""input_batch_raw"")[0]\n            self.num_frames_tensor = tf.get_collection(""num_frames"")[0]\n            self.predictions_tensor = self.sess.graph.get_tensor_by_name(""tower/gates/MatMul:0"")\n\n    def encode(self, data: List[\'np.ndarray\'], *args, **kwargs) -> np.ndarray:\n        def _padding(data):\n            _data = np.array([np.concatenate((d, np.zeros((self.max_num_frames - d.shape[0], self.embedding_dim), dtype=np.float32)),\n                            axis=0) if d.shape[0] < self.max_num_frames else d[:self.max_num_frames] for d in data])\n            return _data.reshape((-1, self.max_num_frames, self.embedding_dim))\n\n        @batching\n        def _encode(_, data):\n            num_frames_list = list(map(lambda x: x.shape[0], data))\n            predictions_val, = self.sess.run([self.predictions_tensor],\n                        feed_dict={self.input_tensor: _padding(data),\n                                   self.num_frames_tensor: np.array(num_frames_list)})\n            return np.array(predictions_val).astype(np.float32)\n\n        return _encode(self, _padding(data))\n\n\n'"
gnes/indexer/chunk/__init__.py,0,b''
gnes/indexer/chunk/annoy.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport os\nfrom typing import List, Tuple, Any\n\nimport numpy as np\n\nfrom .helper import ListKeyIndexer\nfrom ..base import BaseChunkIndexer as BCI\n\n\nclass AnnoyIndexer(BCI):\n\n    def __init__(self, num_dim: int, data_path: str, metric: str = \'angular\', n_trees: int = 10, *args, **kwargs):\n        """"""\n        Initialize an AnnoyIndexer\n\n        :param num_dim: when set to -1, then num_dim is auto decided on first .add()\n        :param data_path: index data file managed by the annoy indexer\n        :param metric:\n        :param n_trees:\n        :param args:\n        :param kwargs:\n        """"""\n        super().__init__(*args, **kwargs)\n        self.num_dim = num_dim\n        self.data_path = data_path\n        self.metric = metric\n        self.n_trees = n_trees\n        self.helper_indexer = ListKeyIndexer()\n\n    def post_init(self):\n        from annoy import AnnoyIndex\n        self._index = AnnoyIndex(self.num_dim, self.metric) if self.num_dim >= 0 else None\n        try:\n            if not os.path.exists(self.data_path):\n                raise FileNotFoundError(\'""data_path"" is not exist\')\n            if os.path.isdir(self.data_path):\n                raise IsADirectoryError(\'""data_path"" must be a file path, not a directory\')\n            self._index.load(self.data_path)\n        except:\n            self.logger.warning(\'fail to load model from %s, will create an empty one\' % self.data_path)\n\n    @BCI.update_helper_indexer\n    def add(self, keys: List[Tuple[int, Any]], vectors: np.ndarray, weights: List[float], *args, **kwargs):\n        last_idx = self.helper_indexer.num_chunks\n\n        if len(vectors) != len(keys):\n            raise ValueError(\'vectors length should be equal to doc_ids\')\n\n        if vectors.dtype != np.float32:\n            raise ValueError(""vectors should be ndarray of float32"")\n\n        if self._index is None:\n            from annoy import AnnoyIndex\n            # means num_dim in unknown during init\n            self.num_dim = vectors.shape[1]\n            self._index = AnnoyIndex(self.num_dim, self.metric)\n\n        for idx, vec in enumerate(vectors):\n            self._index.add_item(last_idx + idx, vec)\n\n    def query(self, keys: \'np.ndarray\', top_k: int, *args, **kwargs) -> List[List[Tuple]]:\n        self._index.build(self.n_trees)\n        if keys.dtype != np.float32:\n            raise ValueError(\'vectors should be ndarray of float32\')\n        res = []\n        for k in keys:\n            ret, relevance_score = self._index.get_nns_by_vector(k, top_k, include_distances=True)\n            chunk_info = self.helper_indexer.query(ret)\n            res.append([(*r, s) for r, s in zip(chunk_info, relevance_score)])\n        return res\n\n    def __getstate__(self):\n        d = super().__getstate__()\n        self._index.save(self.data_path)\n        return d\n'"
gnes/indexer/chunk/faiss.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nimport os\nfrom typing import List, Tuple, Any\n\nimport numpy as np\n\nfrom .helper import ListKeyIndexer\nfrom ..base import BaseChunkIndexer as BCI\n\n\nclass FaissIndexer(BCI):\n\n    def __init__(self, num_dim: int, index_key: str, data_path: str, *args, **kwargs):\n        """"""\n        Initialize an FaissIndexer\n\n        :param num_dim: when set to -1, then num_dim is auto decided on first .add()\n        :param data_path: index data file managed by the faiss indexer\n        """"""\n        super().__init__(*args, **kwargs)\n        self.data_path = data_path\n        self.num_dim = num_dim\n        self.index_key = index_key\n        self.helper_indexer = ListKeyIndexer()\n\n    def post_init(self):\n        import faiss\n        try:\n            if not os.path.exists(self.data_path):\n                raise FileNotFoundError(\'""data_path"" is not exist\')\n            if os.path.isdir(self.data_path):\n                raise IsADirectoryError(\'""data_path"" must be a file path, not a directory\')\n            self._faiss_index = faiss.read_index(self.data_path)\n        except (RuntimeError, FileNotFoundError, IsADirectoryError):\n            self.logger.warning(\'fail to load model from %s, will init an empty one\' % self.data_path)\n            self._faiss_index = faiss.index_factory(self.num_dim, self.index_key) if self.num_dim > 0 else None\n\n    @BCI.update_helper_indexer\n    def add(self, keys: List[Tuple[int, Any]], vectors: np.ndarray, weights: List[float], *args, **kwargs):\n        if len(vectors) != len(keys):\n            raise ValueError(\'vectors length should be equal to doc_ids\')\n\n        if vectors.dtype != np.float32:\n            raise ValueError(\'vectors should be ndarray of float32\')\n\n        if self._faiss_index is None:\n            import faiss\n            # means num_dim in unknown during init\n            self.num_dim = vectors.shape[1]\n            self._faiss_index = faiss.index_factory(self.num_dim, self.index_key)\n\n        self._faiss_index.add(vectors)\n\n    def query(self, keys: np.ndarray, top_k: int, *args, **kwargs) -> List[List[Tuple]]:\n        if keys.dtype != np.float32:\n            raise ValueError(\'vectors should be ndarray of float32\')\n\n        score, ids = self._faiss_index.search(keys, top_k)\n        ret = []\n        for _id, _score in zip(ids, score):\n            ret_i = []\n            chunk_info = self.helper_indexer.query(_id)\n            for c_info, _score_i in zip(chunk_info, _score):\n                ret_i.append((*c_info, _score_i))\n            ret.append(ret_i)\n\n        return ret\n\n    def __getstate__(self):\n        import faiss\n        d = super().__getstate__()\n        faiss.write_index(self._faiss_index, self.data_path)\n        return d\n'"
gnes/indexer/chunk/helper.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nfrom typing import List, Tuple\n\nimport numpy as np\n\nfrom ..base import BaseChunkIndexerHelper as CIH\n\n\nclass DictKeyIndexer(CIH):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._key_info = {}\n\n    @CIH.update_counter\n    def add(self, keys: List[Tuple[int, int]], weights: List[float], *args, **kwargs) -> int:\n        for (k, o), w in zip(keys, weights):\n            self._key_info[k] = o, w\n        return len(self._key_info)\n\n    def query(self, keys: List[int], *args, **kwargs) -> List[Tuple[int, int, float]]:\n        return [(k, *self._key_info[k]) for k in keys]\n\n\nclass ListKeyIndexer(CIH):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._int2key = []  # type: List[Tuple[int, int]]\n        self._int2key_weight = []  # type: List[float]\n\n    @CIH.update_counter\n    def add(self, keys: List[Tuple[int, int]], weights: List[float], *args, **kwargs) -> int:\n        if len(keys) != len(weights):\n            raise ValueError(\'""keys"" and ""weights"" must have the same length\')\n        self._int2key.extend(keys)\n        self._int2key_weight.extend(weights)\n        return len(self._int2key)\n\n    def query(self, keys: List[int], *args, **kwargs) -> List[Tuple[int, int, float]]:\n        return [(*self._int2key[k], self._int2key_weight[k]) for k in keys]\n\n\nclass ListNumpyKeyIndexer(ListKeyIndexer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._data_updated = False\n        self._np_int2key = None\n        self._np_int2key_weight = None\n\n    def _build_np_buffer(self):\n        if self._data_updated or not self._np_int2key or not self._np_int2key_weight:\n            self._np_int2key = np.array(self._int2key, int)\n            self._np_int2key_weight = np.array(self._int2key_weight, float)\n\n    def add(self, *args, **kwargs) -> int:\n        self._data_updated = True\n        return super().add(*args, **kwargs)\n\n    def query(self, keys: List[int], *args, **kwargs) -> List[Tuple[int, int, float]]:\n        self._build_np_buffer()\n        key_offset = self._np_int2key[keys, 0:2].astype(int).tolist()\n        weights = self._np_int2key_weight[keys].astype(float).tolist()\n        return [(*ko, w) for ko, w in zip(key_offset, weights)]\n\n    def __getstate__(self):\n        d = super().__getstate__()\n        del d[\'_np_int2key_weight\']\n        del d[\'_np_int2key\']\n        return d\n\n\nclass NumpyKeyIndexer(CIH):\n    def __init__(self, buffer_size: int = 10000, col_size: int = 3, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._int2key_info = np.zeros([buffer_size, col_size])\n        self._buffer_size = 10000\n        self._col_size = col_size\n        self._size = 0\n        self._max_size = self._buffer_size\n        self._all_docs = []\n\n    @CIH.update_counter\n    def add(self, keys: List[Tuple[int, int]], weights: List[float], *args, **kwargs) -> int:\n        l = len(keys)\n        if self._size + l > self._max_size:\n            extend_size = max(l, self._buffer_size)\n            self._int2key_info = np.concatenate([self._int2key_info, np.zeros([extend_size, self._col_size])])\n            self._max_size += extend_size\n\n        self._int2key_info[self._size:(self._size + l), 0:(self._col_size - 1)] = np.array(keys)\n        self._int2key_info[self._size:(self._size + l), self._col_size - 1] = np.array(weights)\n        self._size += l\n        return self._size\n\n    def query(self, keys: List[int], *args, **kwargs) -> List[Tuple[int, int, float]]:\n        key_offset = self._int2key_info[keys, 0:(self._col_size - 1)].astype(int).tolist()\n        weights = self._int2key_info[keys, self._col_size - 1].astype(float).tolist()\n        return [(*ko, w) for ko, w in zip(key_offset, weights)]\n\n    @property\n    def capacity(self):\n        return self._max_size\n'"
gnes/indexer/chunk/numpy.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nfrom typing import List, Tuple, Any\n\nimport numpy as np\n\nfrom .helper import ListKeyIndexer\nfrom ..base import BaseChunkIndexer as BCI\n\n\nclass NumpyIndexer(BCI):\n    """"""An exhaustive search indexer using numpy\n    The distance is computed as L1 distance normalized by the number of dimension\n    """"""\n\n    def __init__(self, is_binary: bool = False, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._num_dim = None\n        self._vectors = None  # type: np.ndarray\n        self._is_binary = is_binary\n        self.helper_indexer = ListKeyIndexer()\n\n    @BCI.update_helper_indexer\n    def add(self, keys: List[Tuple[int, Any]], vectors: np.ndarray, weights: List[float], *args,\n            **kwargs):\n        if len(vectors) % len(keys) != 0:\n            raise ValueError(\'vectors bytes should be divided by doc_ids\')\n\n        if not self._num_dim:\n            self._num_dim = vectors.shape[1]\n        elif self._num_dim != vectors.shape[1]:\n            raise ValueError(\n                ""vectors\' shape [%d, %d] does not match with indexer\'s dim: %d"" %\n                (vectors.shape[0], vectors.shape[1], self._num_dim))\n\n        if self._vectors is not None:\n            self._vectors = np.concatenate([self._vectors, vectors], axis=0)\n        else:\n            self._vectors = vectors\n\n    def query(self, keys: np.ndarray, top_k: int, *args, **kwargs) -> List[List[Tuple]]:\n        dist = np.abs(np.expand_dims(keys, axis=1) - np.expand_dims(self._vectors, axis=0))\n\n        if self._is_binary:\n            dist = np.minimum(dist, 1)\n\n        score = np.sum(dist, -1) / self._num_dim\n\n        ret = []\n        for ids in score:\n            rk = sorted(enumerate(ids), key=lambda x: x[1])[:top_k]\n            chunk_info = self.helper_indexer.query([j[0] for j in rk])\n            ret.append([(*r, s) for r, s in zip(chunk_info, [j[1] for j in rk])])\n        return ret\n'"
gnes/indexer/doc/__init__.py,0,b''
gnes/indexer/doc/dict.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nfrom typing import List\n\nfrom google.protobuf.json_format import MessageToJson, Parse\n\nfrom ..base import BaseDocIndexer as BDI\nfrom ...proto import gnes_pb2\n\n\nclass DictIndexer(BDI):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._content = {}\n\n    @BDI.update_counter\n    def add(self, keys: List[int], docs: List[\'gnes_pb2.Document\'], *args, **kwargs):\n        self._content.update({k: MessageToJson(d) for (k, d) in zip(keys, docs)})\n\n    def query(self, keys: List[int], *args, **kwargs) -> List[\'gnes_pb2.Document\']:\n        return [Parse(self._content[k], gnes_pb2.Document()) for k in keys]\n'"
gnes/indexer/doc/filesys.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nimport os\nfrom typing import List\n\nfrom ..base import BaseDocIndexer as BDI\nfrom ...proto import gnes_pb2\n\n\nclass DirectoryIndexer(BDI):\n\n    def __init__(self, data_path: str,\n                 keep_na_doc: bool = True,\n                 file_suffix: str = \'gif\',\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.data_path = data_path\n        self.file_suffix = file_suffix\n        self.keep_na_doc = keep_na_doc\n        self._NOT_FOUND = None\n\n    @BDI.update_counter\n    def add(self, keys: List[int], docs: List[\'gnes_pb2.Document\'], *args, **kwargs):\n        """"""\n        write GIFs of each document into disk\n        folder structure: /data_path/doc_id/0.gif, 1.gif...\n\n        :param keys: list of doc id\n        :param docs: list of docs\n        """"""\n        for k, d in zip(keys, docs):\n            dirs = os.path.join(self.data_path, str(k))\n            if not os.path.exists(dirs):\n                os.makedirs(dirs)\n            # keep doc meta in .meta file\n            with open(os.path.join(dirs, \'.meta\'), \'wb\') as f:\n                f.write(d.meta_info or b\'\')\n\n            for i, chunk in enumerate(d.chunks):\n                with open(os.path.join(dirs, \'%d.%s\' % (i, self.file_suffix)), \'wb\') as f:\n                    f.write(chunk.raw)\n\n    def query(self, keys: List[int], *args, **kwargs) -> List[\'gnes_pb2.Document\']:\n        """"""\n        Find the doc according to the keys\n\n        :param keys: list of doc id\n        :return: list of documents whose chunks field contain all the GIFs of this doc(one GIF per chunk)\n        """"""\n        res = []\n        for k in keys:\n            doc = gnes_pb2.Document()\n            target_dirs = os.path.join(self.data_path, str(k))\n\n            if not os.path.exists(target_dirs):\n                if self.keep_na_doc:\n                    res.append(self._NOT_FOUND)\n            else:\n                with open(os.path.join(target_dirs, \'.meta\'), \'rb\') as f:\n                    doc.meta_info = f.read()\n                for raw_file in os.listdir(target_dirs):\n                    if not os.path.isdir(raw_file):\n                        c = doc.chunks.add()\n                        c.doc_id = k\n                        with open(os.path.join(target_dirs, raw_file), \'rb\') as raw:\n                            c.raw = raw.read()\n                res.append(doc)\n        return res\n\n'"
gnes/indexer/doc/leveldb.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nimport pickle\nfrom threading import Thread, Event\nfrom typing import List, Any\n\nfrom ..base import BaseDocIndexer as BDI\nfrom ...proto import gnes_pb2\n\n\nclass LVDBIndexer(BDI):\n\n    def __init__(self, data_path: str,\n                 keep_na_doc: bool = True,\n                 drop_raw_bytes: bool = False,\n                 drop_chunk_blob: bool = False,\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.data_path = data_path\n        self.keep_na_doc = keep_na_doc\n        self.drop_raw_bytes = drop_raw_bytes\n        self.drop_chunk_blob = drop_chunk_blob\n        self._NOT_FOUND = None\n\n    def post_init(self):\n        import plyvel\n        self._db = plyvel.DB(self.data_path, create_if_missing=True)\n\n    @BDI.update_counter\n    def add(self, keys: List[int], docs: List[\'gnes_pb2.Document\'], *args, **kwargs):\n        with self._db.write_batch() as wb:\n            for k, d in zip(keys, docs):\n                doc_id = pickle.dumps(k)\n                if self.drop_raw_bytes:\n                    d.raw_bytes = b\'\'\n                if self.drop_chunk_blob:\n                    for c in d.chunks:\n                        c.ClearField(\'blob\')\n                doc = d.SerializeToString()\n                wb.put(doc_id, doc)\n\n    def query(self, keys: List[int], *args, **kwargs) -> List[\'gnes_pb2.Document\']:\n        res = []\n        for k in keys:\n            doc_id = pickle.dumps(k)\n            v = self._db.get(doc_id)\n            doc = gnes_pb2.Document()\n            if v is not None:\n                doc.ParseFromString(v)\n                res.append(doc)\n            elif self.keep_na_doc:\n                res.append(self._NOT_FOUND)\n        return res\n\n\n    def close(self):\n        super().close()\n        self._db.close()\n\n\nclass AsyncLVDBIndexer(LVDBIndexer):\n    def post_init(self):\n        super().post_init()\n        self._is_listening = Event()\n        self._is_listening.set()\n        self._is_idle = Event()\n        self._is_idle.set()\n        self._jobs = []\n        self._thread = Thread(target=self._db_write)\n        self._thread.setDaemon(1)\n        self._thread.start()\n\n    def add(self, keys: List[int], docs: List[\'gnes_pb2.Document\'], *args, **kwargs):\n        self._jobs.append((keys, docs))\n\n    def query(self, *args, **kwargs) -> List[Any]:\n        self._is_idle.wait()\n        return super().query(*args, **kwargs)\n\n    def _db_write(self):\n        while self._is_listening.is_set():\n            while self._jobs:\n                self._is_idle.clear()\n                keys, docs = self._jobs.pop()\n                super().add(keys, docs)\n            self._is_idle.set()\n\n    def close(self):\n        self._jobs.clear()\n        self._is_listening.clear()\n        self._is_idle.wait()\n        self._thread.join()\n        super().close()\n'"
gnes/indexer/doc/rocksdb.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nimport gc\nimport pickle\nfrom typing import List, Any\n\nfrom ..base import BaseDocIndexer as BDI\nfrom ...proto import gnes_pb2\n\n\nclass RocksDBIndexer(BDI):\n\n    def __init__(self, data_path: str,\n                 drop_raw_data: bool = False,\n                 drop_chunk_blob: bool = False,\n                 read_only: bool = False,\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.data_path = data_path\n        self.drop_raw_data = drop_raw_data\n        self.drop_chunk_blob = drop_chunk_blob\n        self.read_only = read_only\n        self.kwargs = kwargs\n\n    def post_init(self):\n        import rocksdb\n        \n        opts = rocksdb.Options()\n        opts.create_if_missing = True\n        opts.max_open_files = 300000\n        opts.write_buffer_size = 67108864\n        opts.max_write_buffer_number = 3\n        opts.target_file_size_base = 67108864\n\n        opts.table_factory = rocksdb.BlockBasedTableFactory(\n            filter_policy=rocksdb.BloomFilterPolicy(10),\n            block_cache=rocksdb.LRUCache(2 * (1024 ** 3)),\n            block_cache_compressed=rocksdb.LRUCache(500 * (1024 ** 2)))\n\n        for key, value in self.kwargs.items():\n            setattr(opts, key, value)\n\n        self._db = rocksdb.DB(self.data_path, opts, read_only=self.read_only)\n\n\n    @BDI.update_counter\n    def add(self, keys: List[int], docs: List[\'gnes_pb2.Document\'], *args, **kwargs):\n        import rocksdb\n\n        write_batch = rocksdb.WriteBatch()\n        for k, d in zip(keys, docs):\n            key_bytes = pickle.dumps(k)\n            if self.drop_raw_data:\n                d.ClearField(\'raw_data\')\n            if self.drop_chunk_blob:\n                for c in d.chunks:\n                    c.ClearField(\'blob\')\n            value_bytes = d.SerializeToString()\n            write_batch.put(key_bytes, value_bytes)\n        self._db.write(write_batch, sync=True)\n\n\n    def query(self, keys: List[int], *args, **kwargs) -> List[\'gnes_pb2.Document\']:\n        query_keys = []\n        for k in keys:\n            key_value = pickle.dumps(k)\n            query_keys.append(key_value)\n\n        values = self._db.multi_get(query_keys)\n        \n        docs = []\n        for k in query_keys:\n            v = values[k]\n            if v is not None:\n                _doc = gnes_pb2.Document()\n                _doc.ParseFromString(v)\n                docs.append(_doc)\n            else:\n                docs.append(None)\n        return docs\n\n\n    def scan(self, reversed_scan: bool=False):\n        iterator = self._db.iterkeys()\n\n        if reversed_scan:\n            iterator.seek_to_last()\n        else:\n            iterator.seek_to_first()\n\n        if reversed_scan:\n            iterator = reversed(iterator)\n    \n        for key_bytes in iterator:\n            doc_id = pickle.loads(key_bytes)\n            value_bytes = self._db.get(key_bytes)\n            pb_doc = gnes_pb2.Document()\n            pb_doc.ParseFromString(value_bytes)\n\n            yield doc_id, pb_doc\n\n\n    def close(self):\n        super().close()\n        try:\n            del self._db\n        except AttributeError:\n            pass\n        gc.collect()\n'"
gnes/preprocessor/audio/__init__.py,0,b''
gnes/preprocessor/audio/audio_vanilla.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport numpy as np\n\nfrom ..base import BaseAudioPreprocessor\nfrom ...proto import array2blob\nfrom ..io_utils.audio import split_audio\n\n\nclass AudioVanilla(BaseAudioPreprocessor):\n\n    def __init__(self,\n                 sample_rate: int, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.sample_rate = sample_rate\n\n    def apply(self, doc: \'gnes_pb2.Document\') -> None:\n        super().apply(doc)\n\n        if doc.raw_bytes:\n            audio = split_audio(input_data=doc.raw_bytes, sample_rate=self.sample_rate)\n            if len(audio) >= 1:\n                for ci, chunks in enumerate(audio):\n                    c = doc.chunks.add()\n                    c.doc_id = doc.doc_id\n                    c.blob.CopyFrom(array2blob(np.array(chunks, dtype=np.float32)))\n                    c.offset = ci\n                    c.weight = 1 / len(audio)\n            else:\n                self.logger.warning(\'bad document: no audio extracted\')\n        else:\n            self.logger.error(\'bad document: ""raw_bytes"" is empty!\')\n'"
gnes/preprocessor/audio/vggish_example.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport numpy as np\n\nfrom ..base import BaseAudioPreprocessor\nfrom ...proto import array2blob, blob2array\n\n\nclass VggishPreprocessor(BaseAudioPreprocessor):\n\n    def __init__(self, num_frames: int = 96,\n                 num_bands: int = 64,\n                 sample_rate: int = 16000,\n                 log_offset: float = 0.01,\n                 example_window_seconds: float = 0.96,\n                 example_hop_seconds: float = 0.96,\n                 stft_window_length_seconds: float = 0.025,\n                 stft_hop_length_seconds: float = 0.01,\n                 mel_min_hz: int = 125,\n                 mel_max_hz: int = 7500,\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.num_frames = num_frames\n        self.num_bands = num_bands\n        self.sample_rate = sample_rate\n        self.log_offset = log_offset\n        self.example_window_seconds = example_window_seconds\n        self.example_hop_seconds = example_hop_seconds\n        self.stft_window_length_seconds = stft_window_length_seconds\n        self.stft_hop_length_seconds = stft_hop_length_seconds\n        self.mel_min_hz = mel_min_hz\n        self.mel_max_hz = mel_max_hz\n        self.num_mel_binds = num_bands\n\n    def apply(self, doc: \'gnes_pb2.Document\') -> None:\n        super().apply(doc)\n\n        if doc.raw_bytes:\n            for chunks in doc.chunks:\n                chunks.blob.CopyFrom(array2blob(np.array(self.waveform_to_examples(blob2array(chunks.blob),\n                                            sample_rate=self.sample_rate), dtype=np.float32)))\n        else:\n            self.logger.error(\'bad document: ""raw_bytes"" is empty!\')\n\n    def waveform_to_examples(self, data, sample_rate):\n        """"""Converts audio waveform into an array of examples for VGGish.\n\n        Args:\n          data: np.array of either one dimension (mono) or two dimensions\n            (multi-channel, with the outer dimension representing channels).\n            Each sample is generally expected to lie in the range [-1.0, +1.0],\n            although this is not required.\n          sample_rate: Sample rate of data.\n\n        Returns:\n          3-D np.array of shape [num_examples, num_frames, num_bands] which represents\n          a sequence of examples, each of which contains a patch of log mel\n          spectrogram, covering num_frames frames of audio and num_bands mel frequency\n          bands, where the frame length is vggish_params.STFT_HOP_LENGTH_SECONDS.\n        """"""\n        from .vggish_example_helper import mel_features\n        import resampy\n\n        # Convert to mono.\n        print(type(data))\n        if len(data.shape) > 1:\n            data = np.mean(data, axis=1)\n        # Resample to the rate assumed by VGGish.\n        if sample_rate != self.sample_rate:\n            data = resampy.resample(data, sample_rate, self.sample_rate)\n\n        # Compute log mel spectrogram features.\n        log_mel = mel_features.log_mel_spectrogram(\n            data,\n            audio_sample_rate=self.sample_rate,\n            log_offset=self.log_offset,\n            window_length_secs=self.stft_window_length_seconds,\n            hop_length_secs=self.stft_hop_length_seconds,\n            num_mel_bins=self.num_mel_binds,\n            lower_edge_hertz=self.mel_min_hz,\n            upper_edge_hertz=self.mel_max_hz)\n\n        # Frame features into examples.\n        features_sample_rate = 1.0 / self.stft_hop_length_seconds\n        example_window_length = int(round(\n            self.example_window_seconds * features_sample_rate))\n        example_hop_length = int(round(\n            self.example_hop_seconds * features_sample_rate))\n        log_mel_examples = mel_features.frame(\n            log_mel,\n            window_length=example_window_length,\n            hop_length=example_hop_length)\n        return log_mel_examples'"
gnes/preprocessor/image/__init__.py,0,b''
gnes/preprocessor/image/resize.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport numpy as np\nfrom PIL import Image\n\nfrom ..base import BaseImagePreprocessor\nfrom ...proto import gnes_pb2, blob2array, array2blob\n\n\nclass SizedPreprocessor(BaseImagePreprocessor):\n    def __init__(self,\n                 target_width: int = 224,\n                 target_height: int = 224,\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.target_width = target_width\n        self.target_height = target_height\n\n\nclass ResizeChunkPreprocessor(SizedPreprocessor):\n\n    def apply(self, doc: \'gnes_pb2.Document\') -> None:\n        super().apply(doc)\n        for c in doc.chunks:\n            img = blob2array(c.blob)\n            img = np.array(Image.fromarray(img.astype(\'uint8\')).resize((self.target_width, self.target_height)))\n            c.blob.CopyFrom(array2blob(img))\n'"
gnes/preprocessor/image/segmentation.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport io\nimport os\nfrom typing import List\n\nimport numpy as np\nfrom PIL import Image\n\nfrom .resize import SizedPreprocessor\nfrom ..helper import torch_transform, get_all_subarea\nfrom ...proto import array2blob\n\n\nclass SegmentPreprocessor(SizedPreprocessor):\n\n    def __init__(self, model_name: str,\n                 model_dir: str,\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.model_name = model_name\n        self.model_dir = model_dir\n\n    def post_init(self):\n        import torch\n        import torchvision.models as models\n\n        os.environ[\'TORCH_HOME\'] = self.model_dir\n        self._model = getattr(models.detection, self.model_name)(pretrained=True)\n        self._model = self._model.eval()\n        if self.on_gpu:\n            # self._model.cuda()\n            self._device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n            self._model = self._model.to(self._device)\n\n    def apply(self, doc: \'gnes_pb2.Document\'):\n        super().apply(doc)\n        if doc.raw_bytes:\n            original_image = Image.open(io.BytesIO(doc.raw_bytes))\n            all_subareas, index = get_all_subarea(original_image)\n            image_tensor = torch_transform(original_image)\n            if self.on_gpu:\n                image_tensor = image_tensor.cuda()\n\n            seg_output = self._model([image_tensor])\n\n            weight = seg_output[0][\'scores\'].tolist()\n            length = len(list(filter(lambda x: x >= 0.5, weight)))\n            chunks = seg_output[0][\'boxes\'].tolist()[:length]\n            weight = weight[:length]\n\n            for ci, ele in enumerate(zip(chunks, weight)):\n                c = doc.chunks.add()\n                c.doc_id = doc.doc_id\n                c.blob.CopyFrom(array2blob(self._crop(original_image, ele[0])))\n                c.offset = ci\n                c.offset_nd.extend(self._get_seg_offset_nd(all_subareas, index, ele[0]))\n                c.weight = self._cal_area(ele[0]) / (original_image.size[0] * original_image.size[1])\n\n            c = doc.chunks.add()\n            c.doc_id = doc.doc_id\n            c.blob.CopyFrom(array2blob(np.array(original_image)))\n            c.offset = len(chunks)\n            c.offset_nd.extend([100, 100])\n            c.weight = 1.\n        else:\n            self.logger.error(\'bad document: ""raw_bytes"" is empty!\')\n\n    def _get_seg_offset_nd(self, all_subareas: List[List[int]], index: List[List[int]], chunk: List[int]) -> List[int]:\n        iou_list = [self._cal_iou(area, chunk) for area in all_subareas]\n        return index[int(np.argmax(iou_list))][:2]\n\n    @staticmethod\n    def _crop(original_image, coordinates):\n        return np.array(original_image.crop(coordinates))\n\n    @staticmethod\n    def _cal_area(coordinate: List[int]):\n        return (coordinate[2] - coordinate[0]) * (coordinate[3] - coordinate[1])\n\n    def _cal_iou(self, image: List[int], chunk: List[int]) -> float:\n        chunk_area = self._cal_area(chunk)\n        image_area = self._cal_area(image)\n\n        x1 = max(chunk[0], image[0])\n        y1 = max(chunk[1], image[1])\n        x2 = min(chunk[2], image[2])\n        y2 = min(chunk[3], image[3])\n\n        overlap_area = max(0, x2 - x1) * max(0, y2 - y1)\n        iou = overlap_area / (chunk_area + image_area - overlap_area)\n        return iou\n'"
gnes/preprocessor/image/sliding_window.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport io\nfrom typing import List\n\nimport numpy as np\nfrom PIL import Image\n\nfrom .resize import SizedPreprocessor\nfrom ..helper import get_all_subarea, torch_transform\nfrom ...proto import gnes_pb2, array2blob\n\n\nclass _SlidingPreprocessor(SizedPreprocessor):\n\n    def __init__(self, window_size: int = 64,\n                 stride_height: int = 64,\n                 stride_wide: int = 64,\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.window_size = window_size\n        self.stride_height = stride_height\n        self.stride_wide = stride_wide\n\n    def apply(self, doc: \'gnes_pb2.Document\'):\n        super().apply(doc)\n        if doc.raw_bytes:\n            original_image = Image.open(io.BytesIO(doc.raw_bytes))\n            all_subareas, index = get_all_subarea(original_image)\n            image_set, center_point_list = self._get_all_sliding_window(np.array(original_image))\n            normalized_img_set = [np.array(torch_transform(img)).transpose(1, 2, 0)\n                                  for img in image_set]\n            weight = self._get_all_chunks_weight(normalized_img_set)\n\n            for ci, ele in enumerate(zip(normalized_img_set, weight)):\n                c = doc.chunks.add()\n                c.doc_id = doc.doc_id\n                c.blob.CopyFrom(array2blob(ele[0]))\n                c.offset = ci\n                c.offset_nd.extend(self._get_slid_offset_nd(all_subareas, index, center_point_list[ci]))\n                c.weight = ele[1]\n        else:\n            self.logger.error(\'bad document: ""raw_bytes"" is empty!\')\n\n    def _get_all_sliding_window(self, img: \'np.ndarray\'):\n        extend_height = self.window_size - (img.shape[0]) % self.stride_height\n        extend_wide = self.window_size - (img.shape[1]) % self.stride_wide\n\n        input = np.pad(img, ((0, extend_height),\n                             (0, extend_wide),\n                             (0, 0)),\n                       mode=\'constant\', constant_values=0)\n        expanded_input = np.lib.stride_tricks.as_strided(\n            input,\n            shape=(\n                1 + int((input.shape[0] - self.window_size) / self.stride_height),\n                1 + int((input.shape[1] - self.window_size) / self.stride_wide),\n                self.window_size,\n                self.window_size,\n                3\n            ),\n            strides=(\n                input.strides[0] * self.stride_height,\n                input.strides[1] * self.stride_wide,\n                input.strides[0],\n                input.strides[1],\n                1\n            ),\n            writeable=False\n        )\n        center_point_list = [\n            [self.window_size / 2 + x * self.stride_wide, self.window_size / 2 + y * self.stride_height]\n            for x in range(expanded_input.shape[0])\n            for y in range(expanded_input.shape[1])]\n\n        expanded_input = expanded_input.reshape((-1, self.window_size, self.window_size, 3))\n        return [np.array(Image.fromarray(img)) for img in expanded_input], center_point_list\n\n    def _get_slid_offset_nd(self, all_subareas: List[List[int]], index: List[List[int]], center_point: List[float]) -> \\\n            List[int]:\n        location_list = self._get_location(all_subareas, center_point)\n        location = [i for i in range(len(location_list)) if location_list[i] is True][0]\n        return index[location][:2]\n\n    @staticmethod\n    def _get_location(all_subareas: List[List[int]], center_point: List[float]) -> List[bool]:\n        location_list = []\n        x_boundary = max([x[2] for x in all_subareas])\n        y_boundary = max([y[3] for y in all_subareas])\n        for area in all_subareas:\n            if center_point[0] in range(int(area[0]), int(area[2])) and center_point[1] in range(int(area[1]),\n                                                                                                 int(area[3])):\n                location_list.append(True)\n            elif center_point[0] in range(int(area[0]), int(area[2])) and y_boundary == area[3] and center_point[\n                1] > y_boundary:\n                location_list.append(True)\n            elif center_point[1] in range(int(area[1]), int(area[3])) and x_boundary == area[2] and center_point[\n                0] > x_boundary:\n                location_list.append(True)\n            else:\n                location_list.append(False)\n        if True not in location_list:\n            location_list[-1] = True\n        return location_list\n\n    def _get_all_chunks_weight(self, normalizaed_image_set):\n        raise NotImplementedError\n\n\nclass VanillaSlidingPreprocessor(_SlidingPreprocessor):\n\n    def _get_all_chunks_weight(self, image_set: List[\'np.ndarray\']) -> List[float]:\n        return [1 / len(image_set) for _ in range(len(image_set))]\n\n\nclass WeightedSlidingPreprocessor(_SlidingPreprocessor):\n    def _get_all_chunks_weight(self, image_set: List[\'np.ndarray\']) -> List[float]:\n        from ..video.ffmpeg import FFmpegPreprocessor\n\n        return FFmpegPreprocessor.pic_weight(image_set)\n'"
gnes/preprocessor/io_utils/__init__.py,0,b''
gnes/preprocessor/io_utils/audio.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the \'License\');\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \'AS IS\' BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport io\nimport re\nfrom typing import List\n\nimport numpy as np\nimport soundfile as sf\n\nfrom .ffmpeg import compile_args\nfrom .helper import _check_input, run_command\n\nDEFAULT_SILENCE_DURATION = 0.3\nDEFAULT_SILENCE_THRESHOLD = -60\n\n\ndef capture_audio(input_fn: str = \'pipe:\',\n                  input_data: bytes = None,\n                  bits_per_raw_sample: int = 16,\n                  sample_rate: int = 16000,\n                  start_time: float = None,\n                  end_time: float = None,\n                  **kwargs) -> List[\'np.ndarray\']:\n    _check_input(input_fn, input_data)\n\n    input_kwargs = {}\n    if start_time is not None:\n        input_kwargs[\'ss\'] = str(start_time)\n    else:\n        start_time = 0.\n    if end_time is not None:\n        input_kwargs[\'t\'] = str(end_time - start_time)\n\n    output_kwargs = {\n        \'format\': \'wav\',\n        \'bits_per_raw_sample\': bits_per_raw_sample,\n        \'ac\': 1,\n        \'ar\': sample_rate\n    }\n\n    cmd_args = compile_args(\n        input_fn=input_fn,\n        input_options=input_kwargs,\n        output_options=output_kwargs,\n        overwrite_output=True)\n\n    stdout, _ = run_command(\n        cmd_args, input=input_data, pipe_stdout=True, pipe_stderr=True)\n\n    audio_stream = io.BytesIO(stdout)\n    audio_data, sample_rate = sf.read(audio_stream)\n    # has multiple channels, do average\n    if len(audio_data.shape) == 2:\n        audio_data = np.mean(audio_data, axis=1)\n\n    return audio_data\n\n\ndef get_chunk_times(input_fn: str = \'pipe:\',\n                    input_data: bytes = None,\n                    silence_threshold: float = DEFAULT_SILENCE_THRESHOLD,\n                    silence_duration: float = DEFAULT_SILENCE_DURATION,\n                    start_time: float = None,\n                    end_time: float = None):\n    _check_input(input_fn, input_data)\n\n    silence_start_re = re.compile(r\' silence_start: (?P<start>[0-9]+(\\.?[0-9]*))$\')\n    silence_end_re = re.compile(r\' silence_end: (?P<end>[0-9]+(\\.?[0-9]*)) \')\n    total_duration_re = re.compile(\n        r\'size=[^ ]+ time=(?P<hours>[0-9]{2}):(?P<minutes>[0-9]{2}):(?P<seconds>[0-9\\.]{5}) bitrate=\')\n\n    input_kwargs = {}\n    if start_time is not None:\n        input_kwargs[\'ss\'] = start_time\n    else:\n        start_time = 0.\n    if end_time is not None:\n        input_kwargs[\'t\'] = end_time - start_time\n\n    au_filters = [\n        \'silencedetect=noise={}dB:d={}\'.format(silence_threshold,\n                                               silence_duration)\n    ]\n\n    output_kwargs = {\'format\': \'null\'}\n    cmd_args = compile_args(\n        input_fn=input_fn,\n        input_options=input_kwargs,\n        audio_filters=au_filters,\n        output_options=output_kwargs)\n\n    stdout, stderr = run_command(\n        cmd_args, input=input_data, pipe_stdout=True, pipe_stderr=True)\n\n    lines = stderr.decode().splitlines()\n\n    # Chunks start when silence ends, and chunks end when silence starts.\n    chunk_starts = []\n    chunk_ends = []\n    for line in lines:\n        silence_start_match = silence_start_re.search(line)\n        silence_end_match = silence_end_re.search(line)\n        total_duration_match = total_duration_re.search(line)\n        if silence_start_match:\n            chunk_ends.append(float(silence_start_match.group(\'start\')))\n            if len(chunk_starts) == 0:\n                # Started with non-silence.\n                chunk_starts.append(start_time)\n        elif silence_end_match:\n            chunk_starts.append(float(silence_end_match.group(\'end\')))\n        elif total_duration_match:\n            hours = int(total_duration_match.group(\'hours\'))\n            minutes = int(total_duration_match.group(\'minutes\'))\n            seconds = float(total_duration_match.group(\'seconds\'))\n            end_time = hours * 3600 + minutes * 60 + seconds\n\n    if len(chunk_starts) == 0:\n        # No silence found.\n        chunk_starts.append(start_time)\n\n    if len(chunk_starts) > len(chunk_ends):\n        # Finished with non-silence.\n        chunk_ends.append(end_time or 10000000.)\n\n    return list(zip(chunk_starts, chunk_ends))\n\n\ndef split_audio(input_fn: str = \'pipe:\',\n                input_data: bytes = None,\n                silence_threshold=DEFAULT_SILENCE_THRESHOLD,\n                silence_duration=DEFAULT_SILENCE_DURATION,\n                start_time: float = None,\n                end_time: float = None,\n                sample_rate: int = 16000,\n                verbose=False):\n    _check_input(input_fn, input_data)\n    chunk_times = get_chunk_times(\n        input_fn,\n        input_data=input_data,\n        silence_threshold=silence_threshold,\n        silence_duration=silence_duration,\n        start_time=start_time,\n        end_time=end_time)\n    audio_chunks = list()\n    # print(""chunk_times"", chunk_times)\n    for i, (start_time, end_time) in enumerate(chunk_times):\n        time = end_time - start_time\n        if time < 0:\n            continue\n        input_kwargs = {\n            \'ss\': start_time,\n            \'t\': time\n        }\n\n        output_kwargs = {\n            \'format\': \'wav\',\n            \'ar\': sample_rate\n        }\n\n        cmd_args = compile_args(\n            input_fn=input_fn,\n            input_options=input_kwargs,\n            output_options=output_kwargs)\n\n        stdout, stderr = run_command(\n            cmd_args, input=input_data, pipe_stdout=True, pipe_stderr=True)\n\n        audio_stream = io.BytesIO(stdout)\n        audio_data, sample_rate = sf.read(audio_stream)\n        # has multiple channels, do average\n        if len(audio_data.shape) == 2:\n            audio_data = np.mean(audio_data, axis=1)\n        audio_chunks.append(audio_data)\n    return audio_chunks\n'"
gnes/preprocessor/io_utils/ffmpeg.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the \'License\');\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \'AS IS\' BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport re\nfrom .helper import _check_input, kwargs_to_cmd_args, run_command\n\nVIDEO_DUR_PATTERN = re.compile(r"".*Duration: (\\d+):(\\d+):(\\d+)"", re.DOTALL)\nVIDEO_INFO_PATTERN = re.compile(\n    r\'.*Stream #0:(\\d+)(?:\\(\\w+\\))?: Video: (\\w+).*, (\\w+)[(,].* (\\d+)x(\\d+).* (\\d+)(\\.\\d.)? fps\',\n    re.DOTALL)\nAUDIO_INFO_PATTERN = re.compile(\n    r\'^\\s+Stream #0:(?P<stream>\\d+)(\\((?P<lang>\\w+)\\))?: Audio: (?P<format>\\w+).*?(?P<default>\\(default\\))?$\',\n    re.MULTILINE)\nSTREAM_SUBTITLE_PATTERN = re.compile(\n    r\'^\\s+Stream #0:(?P<stream>\\d+)(\\((?P<lang>\\w+)\\))?: Subtitle:\',\n    re.MULTILINE)\n\n\ndef parse_media_details(infos):\n    video_dur_match = VIDEO_DUR_PATTERN.match(infos)\n    dur_hrs, dur_mins, dur_secs = video_dur_match.group(1, 2, 3)\n\n    video_info_match = VIDEO_INFO_PATTERN.match(infos)\n    codec, pix_fmt, res_width, res_height, fps = video_info_match.group(\n        2, 3, 4, 5, 6)\n\n    audio_tracks = list()\n    for audio_match in AUDIO_INFO_PATTERN.finditer(infos):\n        ainfo = audio_match.groupdict()\n        if ainfo[\'lang\'] is None:\n            ainfo[\'lang\'] = \'und\'\n        audio_tracks.append(ainfo)\n\n    medio_info = {\n        \'vcodec\': codec,\n        \'frame_width\': int(res_width),\n        \'frame_height\': int(res_height),\n        \'duration\': (int(dur_hrs) * 3600 + int(dur_mins) * 60 + int(dur_secs)),\n        \'fps\': int(fps),\n        \'pix_fmt\': pix_fmt,\n        \'audio\': audio_tracks,\n    }\n    return medio_info\n\n\ndef extract_frame_size(ffmpeg_parse_info: str):\n\n    possible_patterns = [\n        re.compile(r\'Stream.*Video.*([0-9]{4,})x([0-9]{4,})\'),\n        re.compile(r\'Stream.*Video.*([0-9]{4,})x([0-9]{3,})\'),\n        re.compile(r\'Stream.*Video.*([0-9]{3,})x([0-9]{3,})\')\n    ]\n\n    for pattern in possible_patterns:\n        match = pattern.search(ffmpeg_parse_info)\n        if match is not None:\n            x, y = map(int, match.groups()[0:2])\n            break\n\n    if match is None:\n        raise ValueError(""could not get video frame size"")\n\n    return (x, y)\n\n\ndef compile_args(input_fn: str = \'pipe:\',\n                 output_fn: str = \'pipe:\',\n                 video_filters: str = [],\n                 audio_filters: str = [],\n                 input_options=dict(),\n                 output_options=dict(),\n                 overwrite_output: bool = True):\n    """"""Wrapper for various `FFmpeg <https://www.ffmpeg.org/>`_ related applications (ffmpeg,\n    ffprobe).\n    """"""\n    args = [\'ffmpeg\', \'-threads\', \'1\']\n\n    input_args = []\n    fmt = input_options.pop(\'format\', None)\n    if fmt:\n        input_args += [\'-f\', fmt]\n\n    start_time = input_options.pop(\'ss\', None)\n    duration = input_options.pop(\'t\', None)\n\n    input_args += kwargs_to_cmd_args(input_options)\n    input_args += [\'-i\', input_fn]\n    if start_time is not None:\n        input_args += [\'-ss\', str(start_time)]\n    if duration is not None:\n        input_args += [\'-t\', str(duration)]\n\n    vf_args = []\n    if len(video_filters) > 0:\n        vf_args = [\'-vf\', \',\'.join(video_filters)]\n\n    af_args = []\n    if len(audio_filters) > 0:\n        af_args = [\'-af\', \',\'.join(audio_filters)]\n\n    output_args = []\n\n    fmt = output_options.pop(\'format\', None)\n    if fmt:\n        output_args += [\'-f\', fmt]\n    video_bitrate = output_options.pop(\'video_bitrate\', None)\n    if video_bitrate:\n        output_args += [\'-b:v\', str(video_bitrate)]\n    audio_bitrate = output_options.pop(\'audio_bitrate\', None)\n    if audio_bitrate:\n        output_args += [\'-b:a\', str(audio_bitrate)]\n    output_args += kwargs_to_cmd_args(output_options)\n\n    output_args += [output_fn]\n\n    args += input_args + vf_args + af_args + output_args\n\n    if overwrite_output:\n        args += [\'-y\']\n\n    return args\n\n\ndef probe(input_fn: str):\n    command = [\n        \'ffprobe\', \'-v\', \'fatal\', \'-show_entries\',\n        \'stream=width,height,r_frame_rate,duration\', \'-of\',\n        \'default=noprint_wrappers=1:nokey=1\', input_fn, \'-sexagesimal\'\n    ]\n    out, err = run_command(command, pipe_stdout=True, pipe_stderr=True)\n\n    out = out.decode().split(\'\\n\')\n    return {\n        \'file\': input_fn,\n        \'width\': int(out[0]),\n        \'height\': int(out[1]),\n        \'fps\': float(out[2].split(\'/\')[0]) / float(out[2].split(\'/\')[1]),\n        \'duration\': out[3]\n    }\n\n\ndef get_media_meta(input_fn: str = \'pipe:\',\n                   input_data: bytes = None,\n                   input_options=dict()):\n    _check_input(input_fn, input_data)\n    cmd_args = [\'ffmpeg\']\n\n    fmt = input_options.pop(\'format\', None)\n    if fmt:\n        cmd_args += [\'-f\', fmt]\n    cmd_args += [\'-i\', input_fn]\n\n    cmd_args += [\'-f\', \'ffmetadata\', \'pipe:\']\n    out, err = run_command(\n        cmd_args, input=input_data, pipe_stdout=True, pipe_stderr=True)\n    return parse_media_details(err.decode())\n'"
gnes/preprocessor/io_utils/gif.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the \'License\');\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \'AS IS\' BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nfrom typing import List\nimport numpy as np\nimport subprocess as sp\nimport tempfile\n\nfrom .ffmpeg import compile_args, extract_frame_size\nfrom .helper import _check_input, run_command\n\n\ndef capture_frames(input_fn: str = \'pipe:\',\n                   input_data: bytes = None,\n                   fps: int = None,\n                   pix_fmt: str = \'rgb24\',\n                   vframes: int = -1) -> \'np.ndarray\':\n    _check_input(input_fn, input_data)\n\n\n\n    with tempfile.NamedTemporaryFile(suffix="".gif"") as f:\n        if input_data:\n            f.write(input_data)\n            f.flush()\n            input_fn = f.name\n\n        video_filters = []\n        if fps:\n            video_filters += [\'fps=%d\' % fps]\n\n        output_kwargs = {\'format\': \'rawvideo\', \'pix_fmt\': pix_fmt}\n        if vframes > 0:\n            output_kwargs[\'vframes\'] = vframes\n\n        cmd_args = compile_args(\n            input_fn=input_fn,\n            video_filters=video_filters,\n            output_options=output_kwargs)\n\n        out, err = run_command(cmd_args, pipe_stdout=True, pipe_stderr=True)\n\n        width, height = extract_frame_size(err.decode())\n\n        depth = 3\n        if pix_fmt == \'rgba\':\n            depth = 4\n\n        frames = np.frombuffer(out, np.uint8).copy()\n        frames = frames.reshape([-1, height, width, depth])\n        return frames\n\n\ndef encode_video(images: \'np.ndarray\', frame_rate: int, pix_fmt: str = \'rgb24\'):\n\n    cmd = [\n        \'ffmpeg\', \'-y\', \'-f\', \'rawvideo\', \'-vcodec\', \'rawvideo\', \'-r\',\n        \'%.02f\' % frame_rate, \'-s\',\n        \'%dx%d\' % (images[0].shape[1], images[0].shape[0]), \'-pix_fmt\',\n        \'rgb24\', \'-i\', \'-\', \'-filter_complex\',\n        \'[0:v]split[x][z];[z]palettegen[y];[x]fifo[x];[x][y]paletteuse\', \'-r\',\n        \'%.02f\' % frame_rate, \'-f\', \'gif\', \'-\'\n    ]\n    proc = sp.Popen(cmd, stdin=sp.PIPE, stdout=sp.PIPE, stderr=sp.PIPE)\n    for image in images:\n        proc.stdin.write(image.tostring())\n    out, err = proc.communicate()\n    if proc.returncode:\n        err = \'\\n\'.join([\' \'.join(cmd), err.decode(\'utf8\')])\n        raise IOError(err)\n    del proc\n    return out\n'"
gnes/preprocessor/io_utils/helper.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the \'License\');\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \'AS IS\' BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport subprocess as sp\n\n\ndef kwargs_to_cmd_args(kwargs):\n    args = []\n    for k, v in kwargs.items():\n        args.append(\'-%s\' % k)\n        if v is not None:\n            args.append(\'%s\' % str(v))\n    return args\n\n\ndef _check_input(input_fn: str, input_data: bytes):\n    capture_stdin = (input_fn == \'pipe:\')\n    if capture_stdin and input_data is None:\n        raise ValueError(\n            ""the buffered video data for stdin should not be empty"")\n\n\ndef run_command_async(cmd_args,\n                      pipe_stdin=True,\n                      pipe_stdout=False,\n                      pipe_stderr=False,\n                      quiet=False):\n    stdin_stream = sp.PIPE if pipe_stdin else None\n    stdout_stream = sp.PIPE if pipe_stdout or quiet else None\n    stderr_stream = sp.PIPE if pipe_stderr or quiet else None\n\n    return sp.Popen(\n        cmd_args,\n        stdin=stdin_stream,\n        stdout=stdout_stream,\n        stderr=stderr_stream,\n        close_fds=True)\n\n\ndef wait(process):\n    while True:\n        output = process.stdout.readline()\n        if output == \'\' and process.poll() is not None:\n            break\n        if output:\n            print(output.strip())\n    rc = process.poll()\n    return (output, rc)\n\n\ndef run_command(cmd_args,\n                input=None,\n                pipe_stdin=True,\n                pipe_stdout=False,\n                pipe_stderr=False,\n                quiet=False):\n    with run_command_async(\n            cmd_args,\n            pipe_stdin=pipe_stdin,\n            pipe_stdout=pipe_stdout,\n            pipe_stderr=pipe_stderr,\n            quiet=quiet) as proc:\n        stdout, stderr = proc.communicate(input)\n        retcode = proc.poll()\n\n        if retcode:\n            raise Exception(\'ffmpeg error: %s\' % stderr)\n\n        if proc.stdout is not None:\n            proc.stdout.close()\n        if proc.stderr is not None:\n            proc.stderr.close()\n\n        return stdout, stderr\n'"
gnes/preprocessor/io_utils/video.py,0,"b""#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the 'License');\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an 'AS IS' BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport io\nimport numpy as np\n\nfrom typing import List\n\nfrom .ffmpeg import compile_args, probe\nfrom .helper import _check_input, run_command, run_command_async\n\n\ndef scale_video(input_fn: str = 'pipe:',\n                output_fn: str = 'pipe:',\n                input_data: bytes = None,\n                start_time: float = None,\n                end_time: float = None,\n                scale: str = None,\n                frame_rate: int = 15,\n                crf: int = 16,\n                vcodec: str = 'libx264',\n                format: str = 'mp4',\n                pix_fmt: str = 'yuv420p',\n                **kwargs):\n\n    _check_input(input_fn, input_data)\n\n    capture_stdout = (output_fn == 'pipe:')\n\n    input_kwargs = {}\n    if start_time is not None:\n        input_kwargs['ss'] = start_time\n    else:\n        start_time = 0.\n    if end_time is not None:\n        input_kwargs['t'] = end_time - start_time\n\n    out_kwargs = {\n        'vcodec': vcodec,\n        'pix_fmt': pix_fmt,\n        'crf': crf,\n        'framerate': frame_rate,\n        'acodec': 'aac',\n        'strict': 'experimental',    # AAC audio encoder is experimental\n    }\n\n    if scale:\n        out_kwargs['s'] = scale\n\n    if capture_stdout:\n        out_kwargs['format'] = format\n        # an empty moov means it doesn't need to seek and thus works with a pipe.\n        out_kwargs['movflags'] = 'frag_keyframe+empty_moov'\n\n    cmd_args = compile_args(\n        input_fn=input_fn,\n        output_fn=output_fn,\n        input_options=input_kwargs,\n        output_options=out_kwargs,\n        overwrite_output=True)\n    stdout, _ = run_command(\n        cmd_args, input=input_data, pipe_stdout=True, pipe_stderr=True)\n\n    if capture_stdout:\n        return stdout\n    return None\n\n\ndef encode_video(images: 'np.ndarray',\n                 pix_fmt: str = 'rgb24',\n                 frame_rate: int = 15,\n                 output_fn: str = 'pipe:',\n                 vcodec: str = 'libx264',\n                 format: str = 'mp4',\n                 **kwargs):\n    packet_size = 4096\n\n    n = len(images)\n    height, width, channels = images[0].shape\n\n    capture_stdout = (output_fn == 'pipe:')\n\n    input_kwargs = {\n        'format': 'rawvideo',\n        'pix_fmt': pix_fmt,\n        'framerate': frame_rate,\n        's': '{}x{}'.format(width, height),\n    }\n\n    output_kwargs = {\n        'vcodec': vcodec,\n        'r': frame_rate,\n        'pix_fmt': 'yuv420p',\n        'format': format,\n        'movflags': 'frag_keyframe+empty_moov',\n    }\n\n    cmd_args = compile_args(\n        input_fn='pipe:',\n        output_fn=output_fn,\n        input_options=input_kwargs,\n        output_options=output_kwargs)\n\n    with run_command_async(\n            cmd_args,\n            pipe_stdin=True,\n            pipe_stdout=capture_stdout,\n            pipe_stderr=True) as proc:\n\n        input_stream = io.BytesIO(b'')\n        for frame in images:\n            input_stream.write(frame.astype(np.uint8).tobytes())\n\n        output, err = proc.communicate(input_stream.getvalue())\n\n        if proc.returncode:\n            err = '\\n'.join([' '.join(cmd_args), err.decode('utf8')])\n            raise IOError(err)\n\n        if proc.stdout is not None:\n            proc.stdout.close()\n        if proc.stderr is not None:\n            proc.stderr.close()\n\n        return output\n\n\ndef capture_frames(input_fn: str = 'pipe:',\n                   input_data: bytes = None,\n                   pix_fmt: str = 'rgb24',\n                   fps: int = -1,\n                   scale: str = None,\n                   start_time: float = None,\n                   end_time: float = None,\n                   vframes: int = -1,\n                   **kwargs) -> List['np.ndarray']:\n    _check_input(input_fn, input_data)\n\n    import tempfile\n\n    with tempfile.NamedTemporaryFile() as f:\n        if input_data:\n            f.write(input_data)\n            f.flush()\n            input_fn = f.name\n\n        video_meta = probe(input_fn)\n        width = video_meta['width']\n        height = video_meta['height']\n\n        if scale is not None:\n            _width, _height = map(int, scale.split(':'))\n            if _width * _height < 0:\n                if _width > 0:\n                    ratio = _width / width\n                    height = int(ratio * height)\n                    if _height == -2:\n                        height += height % 2\n                    width = _width\n                else:\n                    ratio = _height / height\n                    width = int(ratio * width)\n                    if _width == -2:\n                        width += width % 2\n\n                    height = _height\n\n                scale = '%d:%d' % (width, height)\n            else:\n                width = _width\n                height = _height\n\n        input_kwargs = {\n            'err_detect': 'aggressive',\n            'fflags': 'discardcorrupt'    # discard corrupted frames\n        }\n        if start_time is not None:\n            input_kwargs['ss'] = str(start_time)\n        else:\n            start_time = 0.\n        if end_time is not None:\n            input_kwargs['t'] = str(end_time - start_time)\n\n        video_filters = []\n        if fps:\n            video_filters += ['fps=%d' % fps]\n        if scale:\n            video_filters += ['scale=%s' % scale]\n\n        output_kwargs = {\n            'format': 'image2pipe',\n            'pix_fmt': pix_fmt,\n            'vcodec': 'rawvideo',\n            'movflags': 'faststart',\n        }\n        if vframes > 0:\n            output_kwargs['vframes'] = vframes\n\n        cmd_args = compile_args(\n            input_fn=input_fn,\n            input_options=input_kwargs,\n            video_filters=video_filters,\n            output_options=output_kwargs)\n        out, _ = run_command(cmd_args, pipe_stdout=True, pipe_stderr=True)\n\n        depth = 3\n        if pix_fmt == 'rgba':\n            depth = 4\n\n        frames = np.frombuffer(out, np.uint8).copy()\n        frames = frames.reshape([-1, height, width, depth])\n        return frames\n\n\n# def read_frame_as_jpg(in_filename, frame_num):\n#     out, err = (\n#         ffmpeg\n#         .input(in_filename)\n#         .filter_('select', 'gte(n,{})'.format(frame_num))\n#         .output('pipe:', vframes=1, format='image2', vcodec='mjpeg')\n#         .run(capture_stdout=True)\n#     )\n#     return out\n"""
gnes/preprocessor/io_utils/webp.py,0,"b""#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the 'License');\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an 'AS IS' BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nfrom typing import List\n\ndef encode_video(images: List['np.ndarray'], frame_rate: int, pix_fmt: str = 'rgb24'):\n    import webp\n    from PIL import Image\n\n    height, width, _ = images[0].shape\n    if pix_fmt == 'rgb24':\n        pix_fmt = 'RGB'\n    # Save an animation\n    enc = webp.WebPAnimEncoder.new(width, height)\n    timestamp_ms = 0\n    duration = 1000 // frame_rate\n    for x in images:\n        img = Image.fromarray(x.copy(), pix_fmt)\n        pic = webp.WebPPicture.from_pil(img)\n        enc.encode_frame(pic, timestamp_ms)\n        timestamp_ms += duration\n    anim_data = enc.assemble(timestamp_ms)\n    return bytes(anim_data.buffer())\n"""
gnes/preprocessor/text/__init__.py,0,b''
gnes/preprocessor/text/split.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport json\nimport re\nimport string\n\nfrom ..base import BaseTextPreprocessor\nfrom ...proto import gnes_pb2\n\n\nclass SentSplitPreprocessor(BaseTextPreprocessor):\n    def __init__(self,\n                 min_sent_len: int = 1,\n                 max_sent_len: int = 256,\n                 deliminator: str = \'.!?\xe3\x80\x82\xef\xbc\x81\xef\xbc\x9f\',\n                 is_json: bool = False,\n                 *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.min_sent_len = min_sent_len\n        self.max_sent_len = max_sent_len\n        self.deliminator = deliminator\n        self.is_json = is_json\n\n    def apply(self, doc: \'gnes_pb2.Document\') -> None:\n        super().apply(doc)\n        d = doc.raw_bytes.decode()\n        if self.is_json:\n            d = json.loads(d)\n            doc.raw_text = d.pop(\'Content\')\n            doc.meta_info = json.dumps(d).encode()\n        else:\n            doc.raw_text = d\n\n        ret = [(m.group(0), m.start(), m.end()) for m in\n               re.finditer(r\'[^{0}]+[{0}]\'.format(self.deliminator), doc.raw_text)]\n        if not ret:\n            ret = [(doc.raw_text, 0, len(doc.raw_text))]\n        for ci, (r, s, e) in enumerate(ret):\n            f = \'\'.join(filter(lambda x: x in string.printable, r))\n            f = re.sub(\'\\n+\', \' \', f).strip()\n            if len(f) > self.min_sent_len:\n                c = doc.chunks.add()\n                c.doc_id = doc.doc_id\n                c.text = f[:self.max_sent_len]\n                c.offset = ci\n                c.weight = len(c.text) / len(doc.raw_text)\n                c.offset_nd.extend([s, e])\n'"
gnes/preprocessor/video/__init__.py,0,b''
gnes/preprocessor/video/ffmpeg.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport random\nfrom typing import List\n\nimport numpy as np\n\nfrom ..base import BaseVideoPreprocessor, RawChunkPreprocessor\nfrom ..helper import split_video_frames, phash_descriptor\nfrom ..io_utils import video as video_util\nfrom ...proto import gnes_pb2, array2blob, blob2array\n\n\nclass FFmpegPreprocessor(BaseVideoPreprocessor):\n\n    def __init__(self,\n                 frame_size: str = \'192:168\',\n                 frame_rate: int = 10,\n                 frame_num: int = -1,\n                 duplicate_rm: bool = True,\n                 use_phash_weight: bool = False,\n                 phash_thresh: int = 5,\n                 *args,\n                 **kwargs):\n        super().__init__(*args, **kwargs)\n        self.frame_size = frame_size\n        self.frame_rate = frame_rate\n        self.frame_num = frame_num\n        self.phash_thresh = phash_thresh\n        self.duplicate_rm = duplicate_rm\n        self.use_phash_weight = use_phash_weight\n\n    def apply(self, doc: \'gnes_pb2.Document\') -> None:\n        super().apply(doc)\n\n        # video could\'t be processed from ndarray!\n        # only bytes can be passed into ffmpeg pipeline\n        if doc.raw_bytes:\n            frames = video_util.capture_frames(input_data=doc.raw_bytes, scale=self.frame_size,\n                                               fps=self.frame_rate, vframes=self.frame_num)\n            # remove dupliated key frames by phash value\n            if self.duplicate_rm:\n                frames = self.duplicate_rm_hash(frames)\n\n            if self.use_phash_weight:\n                weight = FFmpegPreprocessor.pic_weight(frames)\n            else:\n                weight = [1 / len(frames)] * len(frames)\n\n            for ci, chunk in enumerate(frames):\n                c = doc.chunks.add()\n                c.doc_id = doc.doc_id\n                c.blob.CopyFrom(array2blob(chunk))\n                c.offset = ci\n                c.weight = weight[ci]\n\n        else:\n            self.logger.error(\'bad document: ""raw_bytes"" is empty!\')\n\n    @staticmethod\n    def pic_weight(images: List[\'np.ndarray\']) -> List[float]:\n        import cv2\n        weight = np.zeros([len(images)])\n        # n_channel is usually 3 for RGB images\n        n_channel = images[0].shape[-1]\n        for i, image in enumerate(images):\n            weight[i] = sum([\n                cv2.calcHist([image], [_], None, [256], [0, 256]).var()\n                for _ in range(n_channel)\n            ])\n        weight = weight / weight.sum()\n\n        # normalized result\n        weight = np.exp(-weight * 10)\n        return weight / weight.sum()\n\n    def duplicate_rm_hash(self,\n                          images: List[\'np.ndarray\']) -> List[\'np.ndarray\']:\n        hash_list = [phash_descriptor(_) for _ in images]\n        ret = []\n        for i, h in enumerate(hash_list):\n            flag = 1\n            if len(ret) >= 1:\n                # only keep images with high phash diff\n                # comparing only last kept 9 pics\n                for j in range(1, min(len(ret) + 1, 9)):\n                    dist = abs(ret[-j][1] - h)\n                    if dist < self.phash_thresh:\n                        flag = 0\n                        break\n\n            if flag:\n                ret.append((i, h))\n\n        return [images[_[0]] for _ in ret]\n\n\nclass FFmpegVideoSegmentor(BaseVideoPreprocessor):\n    def __init__(self,\n                 frame_size: str = \'192:168\',\n                 frame_rate: int = 10,\n                 frame_num: int = -1,\n                 segment_method: str = \'cut_by_frame\',\n                 segment_interval: int = -1,\n                 segment_num: int = 3,\n                 max_frames_per_doc: int = -1,\n                 use_image_input: bool = False,\n                 splitter: str = \'__split__\',\n                 *args,\n                 **kwargs):\n        super().__init__(*args, **kwargs)\n        self.frame_size = frame_size\n        self.frame_rate = frame_rate\n        self.frame_num = frame_num\n        self.segment_method = segment_method\n        self.segment_interval = segment_interval\n        self.segment_num = segment_num\n        self.max_frames_per_doc = max_frames_per_doc\n        self.splitter = splitter\n        self.use_image_input = use_image_input\n\n    def apply(self, doc: \'gnes_pb2.Document\') -> None:\n        super().apply(doc)\n        from sklearn.cluster import KMeans\n        if doc.raw_bytes:\n            if self.use_image_input:\n                frames = split_video_frames(doc.raw_bytes, self.splitter)\n            else:\n                frames = video_util.capture_frames(input_data=doc.raw_bytes, scale=self.frame_size,\n                                                   fps=self.frame_rate, vframes=self.frame_num)\n            if self.max_frames_per_doc > 0:\n                random_id = random.sample(range(len(frames)),\n                                          k=min(self.max_frames_per_doc, len(frames)))\n                frames = [frames[i] for i in sorted(random_id)]\n\n            sub_videos = []\n            if len(frames) >= 1:\n                # cut by frame: should specify how many frames to cut\n                if self.segment_method == \'cut_by_frame\':\n                    if self.segment_interval == -1:\n                        sub_videos = [frames]\n                    else:\n                        sub_videos = [frames[_: _ + self.segment_interval]\n                                      for _ in range(0, len(frames), self.segment_interval)]\n                # cut by num: should specify how many chunks for each doc\n                elif self.segment_method == \'cut_by_num\':\n                    if self.segment_num >= 2:\n                        _interval = len(frames) // (self.segment_num - 1)\n                        sub_videos = [frames[_: _ + _interval]\n                                      for _ in range(0, len(frames), _interval)]\n                    else:\n                        sub_videos = [frames]\n\n                # cut by clustering: params required\n                #   segment_num\n                elif self.segment_method == \'cut_by_clustering\':\n                    if self.segment_num >= 2:\n                        hash_v = [phash_descriptor(_).hash for _ in frames]\n                        hash_v = np.array(hash_v, dtype=np.int32).reshape([len(hash_v), -1])\n                        label_v = KMeans(n_clusters=self.segment_num).fit_predict(hash_v)\n                        sub_videos = [[frames[i] for i, j in enumerate(label_v) if j == _] for _ in\n                                      range(self.segment_num)]\n                    else:\n                        sub_videos = [frames]\n\n                for ci, chunk in enumerate(sub_videos):\n                    c = doc.chunks.add()\n                    c.doc_id = doc.doc_id\n                    c.blob.CopyFrom(array2blob(np.array(chunk, dtype=np.uint8)))\n                    c.offset = ci\n                    c.weight = 1 / len(sub_videos)\n\n            else:\n                self.logger.warning(\'bad document: no key frames extracted\')\n        else:\n            self.logger.error(\'bad document: ""raw_bytes"" is empty!\')\n\n\nclass GifChunkPreprocessor(RawChunkPreprocessor, BaseVideoPreprocessor):\n    @staticmethod\n    def _parse_chunk(chunk: \'gnes_pb2.Chunk\', *args, **kwargs):\n        from ..io_utils import gif as gif_util\n\n        return gif_util.encode_video(blob2array(chunk.blob), frame_rate=10)\n'"
gnes/preprocessor/video/frame_select.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport numpy as np\nimport math\n\nfrom gnes.preprocessor.base import BaseVideoPreprocessor\nfrom gnes.proto import gnes_pb2, array2blob, blob2array\n\n\nclass FrameSelectPreprocessor(BaseVideoPreprocessor):\n\n    def __init__(self,\n                 sframes: int = 1,\n                 *args,\n                 **kwargs):\n        super().__init__(*args, **kwargs)\n        self.sframes = sframes\n\n    def apply(self, doc: \'gnes_pb2.Document\') -> None:\n        super().apply(doc)\n        if len(doc.chunks) > 0:\n            for chunk in doc.chunks:\n                images = blob2array(chunk.blob)\n                if len(images) == 0:\n                    self.logger.warning(""this chunk has no frame!"")\n                elif self.sframes == 1:\n                    idx = [int(len(images) / 2)]\n                    chunk.blob.CopyFrom(array2blob(images[idx]))\n                elif self.sframes > 0 and len(images) > self.sframes:\n                    if len(images) >= 2 * self.sframes:\n                        step = math.ceil(len(images) / self.sframes)\n                        chunk.blob.CopyFrom(array2blob(images[::step]))\n                    else:\n                        idx = np.sort(np.random.choice(len(images), self.sframes, replace=False))\n                        chunk.blob.CopyFrom(array2blob(images[idx]))\n                del images\n        else:\n            self.logger.error(\n                \'bad document: ""doc.chunks"" is empty!\')\n'"
gnes/preprocessor/video/shot_detector.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport numpy as np\nimport math\nfrom typing import List\n\nfrom gnes.preprocessor.base import BaseVideoPreprocessor\nfrom gnes.proto import gnes_pb2, array2blob, blob2array\nfrom gnes.preprocessor.io_utils import video\nfrom gnes.preprocessor.helper import compute_descriptor, compare_descriptor, detect_peak_boundary, compare_ecr\n\n\nclass ShotDetectorPreprocessor(BaseVideoPreprocessor):\n    store_args_kwargs = True\n\n    def __init__(self,\n                 descriptor: str = \'block_hsv_histogram\',\n                 distance_metric: str = \'bhattacharya\',\n                 detect_method: str = \'threshold\',\n                 frame_size: str = None,\n                 frame_rate: int = 10,\n                 vframes: int = -1,\n                 sframes: int = -1,\n                 drop_raw_data: bool = False,\n                 *args,\n                 **kwargs):\n        super().__init__(*args, **kwargs)\n        self.frame_size = frame_size\n        self.descriptor = descriptor\n        self.distance_metric = distance_metric\n        self.detect_method = detect_method\n        self.frame_rate = frame_rate\n        self.vframes = vframes\n        self.sframes = sframes\n        self.drop_raw_data = drop_raw_data\n        self._detector_kwargs = kwargs\n\n    def detect_shots(self, frames: \'np.ndarray\') -> List[List[\'np.ndarray\']]:\n        descriptors = []\n        for frame in frames:\n            descriptor = compute_descriptor(\n                frame, method=self.descriptor, **self._detector_kwargs)\n            descriptors.append(descriptor)\n\n        # compute distances between frames\n        if self.distance_metric == \'edge_change_ration\':\n            dists = compare_ecr(descriptors, **self._detector_kwargs)\n        else:\n            dists = [\n                compare_descriptor(pair[0], pair[1], self.distance_metric)\n                for pair in zip(descriptors[:-1], descriptors[1:])\n            ]\n            self._detector_kwargs[\'neigh_avg\'] = 0\n\n        shot_bounds = detect_peak_boundary(dists, self.detect_method, **self._detector_kwargs)\n\n        shots = []\n        for ci in range(0, len(shot_bounds) - 1):\n            shots.append(frames[shot_bounds[ci]:shot_bounds[ci + 1]].copy())\n\n        return shots\n\n    def apply(self, doc: \'gnes_pb2.Document\') -> None:\n        super().apply(doc)\n\n        video_frames = []\n\n        if doc.WhichOneof(\'raw_data\'):\n            raw_type = type(getattr(doc, doc.WhichOneof(\'raw_data\')))\n            if doc.raw_bytes:\n                video_frames = video.capture_frames(\n                    input_data=doc.raw_bytes,\n                    scale=self.frame_size,\n                    fps=self.frame_rate,\n                    vframes=self.vframes)\n            elif raw_type == gnes_pb2.NdArray:\n                video_frames = blob2array(doc.raw_video)\n                if self.vframes > 0:\n                    video_frames = video_frames[0:self.vframes, :].copy()\n\n            num_frames = len(video_frames)\n            if num_frames > 0:\n                shots = self.detect_shots(video_frames)\n                for ci, frames in enumerate(shots):\n                    c = doc.chunks.add()\n                    c.doc_id = doc.doc_id\n                    c.offset = ci\n                    shot_len = len(frames)\n                    c.weight = shot_len / num_frames\n                    if self.sframes > 0 and shot_len > self.sframes:\n                        if shot_len >= 2 * self.sframes:\n                            step = math.ceil(shot_len / self.sframes)\n                            frames = frames[::step]\n                        else:\n                            idx = np.sort(np.random.choice(shot_len, self.sframes, replace=False))\n                            frames = [frames[idx_] for idx_ in idx]\n\n                    chunk_data = np.array(frames)\n                    c.blob.CopyFrom(array2blob(chunk_data))\n            else:\n                self.logger.error(\n                    \'bad document: ""raw_bytes"" or ""raw_video"" is empty!\')\n        else:\n            self.logger.error(\'bad document: ""raw_data"" is empty!\')\n\n        if self.drop_raw_data:\n            self.logger.info(""document raw data will be cleaned!"")\n            doc.ClearField(\'raw_data\')\n'"
gnes/preprocessor/video/video_decoder.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nfrom ...proto import array2blob\n\nfrom ..base import BaseVideoPreprocessor\nfrom ..io_utils import video as video_util\n\n\nclass VideoDecoderPreprocessor(BaseVideoPreprocessor):\n    store_args_kwargs = True\n\n    def __init__(self,\n                 frame_rate: int = 10,\n                 frame_size: str = None,\n                 vframes: int = -1,\n                 drop_raw_data: bool = False,\n                 chunk_spliter: str = None,\n                 *args,\n                 **kwargs):\n        super().__init__(*args, **kwargs)\n        self.frame_rate = frame_rate\n        self.frame_size = frame_size\n        self.vframes = vframes\n        self.drop_raw_data = drop_raw_data\n        self.chunk_spliter = chunk_spliter\n\n    def apply(self, doc: \'gnes_pb2.Document\') -> None:\n        super().apply(doc)\n        if doc.WhichOneof(\'raw_data\'):\n            video_frames = []\n            # raw_type = type(getattr(doc, doc.WhichOneof(\'raw_data\')))\n            if doc.raw_bytes:\n                video_frames = video_util.capture_frames(\n                    input_data=doc.raw_bytes,\n                    scale=self.frame_size,\n                    fps=self.frame_rate,\n                    vframes=self.vframes)\n                if not self.drop_raw_data:\n                    doc.raw_video.CopyFrom(array2blob(video_frames))\n            else:\n                self.logger.error(\'the document ""raw_bytes"" is empty!\')\n\n            if self.chunk_spliter == \'base\':\n                for i, frame in enumerate(video_frames):\n                    c = doc.chunks.add()\n                    c.doc_id = doc.doc_id\n                    c.blob.CopyFrom(array2blob(frame))\n                    c.offset = i\n                    c.weight = 1.0\n            elif self.chunk_spliter == \'none\':\n                pass\n            elif self.chunk_spliter == \'shot\':\n                raise NotImplementedError\n            else:\n                chunk = doc.chunks.add()\n                chunk.doc_id = doc.doc_id\n                chunk.blob.CopyFrom(array2blob(video_frames))\n                chunk.offset = 0\n                chunk.weight = 1.0\n\n        else:\n            self.logger.error(\'bad document: ""raw_data"" is empty!\')\n'"
gnes/preprocessor/video/video_encoder.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nfrom ...proto import gnes_pb2, blob2array\nfrom ..base import BaseVideoPreprocessor\nfrom ..io_utils import video, gif, webp\n\n\nclass VideoEncoderPreprocessor(BaseVideoPreprocessor):\n    def __init__(self, frame_rate: int = 10, pix_fmt: str = \'rgb24\', video_format: str = ""mp4"", *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pix_fmt = pix_fmt\n        self.frame_rate = frame_rate\n        self.video_format = video_format\n\n        if self.video_format not in [\'mp4\', \'gif\', \'webp\']:\n            raise ValueError(""%s encoder has not been supported!"" % (self.video_format))\n\n    def _encode(self, images: \'np.ndarray\'):\n        encoder = None\n        if self.video_format == \'mp4\':\n            encoder = video\n        elif self.video_format == \'gif\':\n            encoder = gif\n        elif self.video_format == \'webp\':\n            encoder = webp\n\n        return encoder.encode_video(images, pix_fmt=self.pix_fmt, frame_rate=self.frame_rate)\n\n    def apply(self, doc: \'gnes_pb2.Document\') -> None:\n        super().apply(doc)\n        if len(doc.chunks) > 0:\n            for chunk in doc.chunks:\n                images = blob2array(chunk.blob)\n                chunk.raw = self._encode(images)\n        elif doc.WhichOneof(\'raw_data\'):\n            raw_type = type(getattr(doc, doc.WhichOneof(\'raw_data\')))\n            if raw_type == gnes_pb2.NdArray:\n                images = blob2array(doc.raw_video)\n                doc.raw_bytes = self._encode(images)\n            else:\n                self.logger.error(\'bad document: ""doc.raw_video"" is empty!\')\n        else:\n            self.logger.error(\n                \'bad document: ""doc.chunks"" and ""doc.raw_video"" is empty!\')\n'"
gnes/encoder/audio/vggish_cores/__init__.py,0,b''
gnes/encoder/audio/vggish_cores/vggish_params.py,0,"b'# ==============================================================================\n\n""""""Global parameters for the VGGish model.\n\nSee vggish_slim.py for more information.\n""""""\n\n# Architectural constants.\nNUM_FRAMES = 96  # Frames in input mel-spectrogram patch.\nNUM_BANDS = 64  # Frequency bands in input mel-spectrogram patch.\nEMBEDDING_SIZE = 128  # Size of embedding layer.\n\n# Parameters used for embedding postprocessing.\nPCA_EIGEN_VECTORS_NAME = \'pca_eigen_vectors\'\nPCA_MEANS_NAME = \'pca_means\'\nQUANTIZE_MIN_VAL = -2.0\nQUANTIZE_MAX_VAL = +2.0\n\n# Hyperparameters used in training.\nINIT_STDDEV = 0.01  # Standard deviation used to initialize weights.\nLEARNING_RATE = 1e-4  # Learning rate for the Adam optimizer.\nADAM_EPSILON = 1e-8  # Epsilon for the Adam optimizer.\n\n# Names of ops, tensors, and features.\nINPUT_OP_NAME = \'vggish/input_features\'\nINPUT_TENSOR_NAME = INPUT_OP_NAME + \':0\'\nOUTPUT_OP_NAME = \'vggish/embedding\'\nOUTPUT_TENSOR_NAME = OUTPUT_OP_NAME + \':0\'\nAUDIO_EMBEDDING_FEATURE_NAME = \'audio_embedding\''"
gnes/encoder/audio/vggish_cores/vggish_postprocess.py,0,"b'# ==============================================================================\n\n""""""Post-process embeddings from VGGish.""""""\n\nimport numpy as np\nfrom ..vggish_cores import vggish_params\n\n\nclass Postprocessor(object):\n    """"""Post-processes VGGish embeddings.\n\n    The initial release of AudioSet included 128-D VGGish embeddings for each\n    segment of AudioSet. These released embeddings were produced by applying\n    a PCA transformation (technically, a whitening transform is included as well)\n    and 8-bit quantization to the raw embedding output from VGGish, in order to\n    stay compatible with the YouTube-8M project which provides visual embeddings\n    in the same format for a large set of YouTube videos. This class implements\n    the same PCA (with whitening) and quantization transformations.\n    """"""\n\n    def __init__(self, pca_params_npz_path):\n        """"""Constructs a postprocessor.\n\n        Args:\n          pca_params_npz_path: Path to a NumPy-format .npz file that\n            contains the PCA parameters used in postprocessing.\n        """"""\n        params = np.load(pca_params_npz_path)\n        self._pca_matrix = params[vggish_params.PCA_EIGEN_VECTORS_NAME]\n        # Load means into a column vector for easier broadcasting later.\n        self._pca_means = params[vggish_params.PCA_MEANS_NAME].reshape(-1, 1)\n        assert self._pca_matrix.shape == (\n            vggish_params.EMBEDDING_SIZE, vggish_params.EMBEDDING_SIZE), (\n                \'Bad PCA matrix shape: %r\' % (self._pca_matrix.shape,))\n        assert self._pca_means.shape == (vggish_params.EMBEDDING_SIZE, 1), (\n                \'Bad PCA means shape: %r\' % (self._pca_means.shape,))\n\n    def postprocess(self, embeddings_batch):\n        """"""Applies postprocessing to a batch of embeddings.\n\n        Args:\n          embeddings_batch: An nparray of shape [batch_size, embedding_size]\n            containing output from the embedding layer of VGGish.\n\n        Returns:\n          An nparray of the same shape as the input but of type uint8,\n          containing the PCA-transformed and quantized version of the input.\n        """"""\n        assert len(embeddings_batch.shape) == 2, (\n                \'Expected 2-d batch, got %r\' % (embeddings_batch.shape,))\n        assert embeddings_batch.shape[1] == vggish_params.EMBEDDING_SIZE, (\n                \'Bad batch shape: %r\' % (embeddings_batch.shape,))\n\n        # Apply PCA.\n        # - Embeddings come in as [batch_size, embedding_size].\n        # - Transpose to [embedding_size, batch_size].\n        # - Subtract pca_means column vector from each column.\n        # - Premultiply by PCA matrix of shape [output_dims, input_dims]\n        #   where both are are equal to embedding_size in our case.\n        # - Transpose result back to [batch_size, embedding_size].\n        pca_applied = np.dot(self._pca_matrix,\n                             (embeddings_batch.T - self._pca_means)).T\n\n        # Quantize by:\n        # - clipping to [min, max] range\n        clipped_embeddings = np.clip(\n            pca_applied, vggish_params.QUANTIZE_MIN_VAL,\n            vggish_params.QUANTIZE_MAX_VAL)\n        # - convert to 8-bit in range [0.0, 255.0]\n        quantized_embeddings = (\n                (clipped_embeddings - vggish_params.QUANTIZE_MIN_VAL) *\n                (255.0 /\n                 (vggish_params.QUANTIZE_MAX_VAL - vggish_params.QUANTIZE_MIN_VAL)))\n\n        quantized_embeddings = quantized_embeddings.astype(np.float32)\n\n        return quantized_embeddings'"
gnes/encoder/audio/vggish_cores/vggish_slim.py,14,"b'# ==============================================================================\n\n""""""Defines the \'VGGish\' model used to generate AudioSet embedding features.\n\nThe public AudioSet release (https://research.google.com/audioset/download.html)\nincludes 128-D features extracted from the embedding layer of a VGG-like model\nthat was trained on a large Google-internal YouTube dataset. Here we provide\na TF-Slim definition of the same model, without any dependences on libraries\ninternal to Google. We call it \'VGGish\'.\n\nNote that we only define the model up to the embedding layer, which is the\npenultimate layer before the final classifier layer. We also provide various\nhyperparameter values (in vggish_params.py) that were used to train this model\ninternally.\n\nFor comparison, here is TF-Slim\'s VGG definition:\nhttps://github.com/tensorflow/models/blob/master/research/slim/nets/vgg.py\n""""""\n\nimport tensorflow as tf\nfrom ..vggish_cores import vggish_params as params\n\nslim = tf.contrib.slim\n\n\ndef define_vggish_slim(training=False):\n    """"""Defines the VGGish TensorFlow model.\n\n    All ops are created in the current default graph, under the scope \'vggish/\'.\n\n    The input is a placeholder named \'vggish/input_features\' of type float32 and\n    shape [batch_size, num_frames, num_bands] where batch_size is variable and\n    num_frames and num_bands are constants, and [num_frames, num_bands] represents\n    a log-mel-scale spectrogram patch covering num_bands frequency bands and\n    num_frames time frames (where each frame step is usually 10ms). This is\n    produced by computing the stabilized log(mel-spectrogram + params.LOG_OFFSET).\n    The output is an op named \'vggish/embedding\' which produces the activations of\n    a 128-D embedding layer, which is usually the penultimate layer when used as\n    part of a full model with a final classifier layer.\n\n    Args:\n      training: If true, all parameters are marked trainable.\n\n    Returns:\n      The op \'vggish/embeddings\'.\n    """"""\n    # Defaults:\n    # - All weights are initialized to N(0, INIT_STDDEV).\n    # - All biases are initialized to 0.\n    # - All activations are ReLU.\n    # - All convolutions are 3x3 with stride 1 and SAME padding.\n    # - All max-pools are 2x2 with stride 2 and SAME padding.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                        weights_initializer=tf.truncated_normal_initializer(\n                            stddev=params.INIT_STDDEV),\n                        biases_initializer=tf.zeros_initializer(),\n                        activation_fn=tf.nn.relu,\n                        trainable=training), \\\n         slim.arg_scope([slim.conv2d],\n                        kernel_size=[3, 3], stride=1, padding=\'SAME\'), \\\n         slim.arg_scope([slim.max_pool2d],\n                        kernel_size=[2, 2], stride=2, padding=\'SAME\'), \\\n         tf.variable_scope(\'vggish\'):\n        # Input: a batch of 2-D log-mel-spectrogram patches.\n        features = tf.placeholder(\n            tf.float32, shape=(None, params.NUM_FRAMES, params.NUM_BANDS),\n            name=\'input_features\')\n        # Reshape to 4-D so that we can convolve a batch with conv2d().\n        net = tf.reshape(features, [-1, params.NUM_FRAMES, params.NUM_BANDS, 1])\n\n        # The VGG stack of alternating convolutions and max-pools.\n        net = slim.conv2d(net, 64, scope=\'conv1\')\n        net = slim.max_pool2d(net, scope=\'pool1\')\n        net = slim.conv2d(net, 128, scope=\'conv2\')\n        net = slim.max_pool2d(net, scope=\'pool2\')\n        net = slim.repeat(net, 2, slim.conv2d, 256, scope=\'conv3\')\n        net = slim.max_pool2d(net, scope=\'pool3\')\n        net = slim.repeat(net, 2, slim.conv2d, 512, scope=\'conv4\')\n        net = slim.max_pool2d(net, scope=\'pool4\')\n\n        # Flatten before entering fully-connected layers\n        net = slim.flatten(net)\n        net = slim.repeat(net, 2, slim.fully_connected, 4096, scope=\'fc1\')\n        # The embedding layer.\n        net = slim.fully_connected(net, params.EMBEDDING_SIZE, scope=\'fc2\')\n        return tf.identity(net, name=\'embedding\')\n\n\ndef load_vggish_slim_checkpoint(session, checkpoint_path):\n    """"""Loads a pre-trained VGGish-compatible checkpoint.\n\n    This function can be used as an initialization function (referred to as\n    init_fn in TensorFlow documentation) which is called in a Session after\n    initializating all variables. When used as an init_fn, this will load\n    a pre-trained checkpoint that is compatible with the VGGish model\n    definition. Only variables defined by VGGish will be loaded.\n\n    Args:\n      session: an active TensorFlow session.\n      checkpoint_path: path to a file containing a checkpoint that is\n        compatible with the VGGish model definition.\n    """"""\n    # Get the list of names of all VGGish variables that exist in\n    # the checkpoint (i.e., all inference-mode VGGish variables).\n    with tf.Graph().as_default():\n        define_vggish_slim(training=False)\n        vggish_var_names = [v.name for v in tf.global_variables()]\n\n    # Get the list of all currently existing variables that match\n    # the list of variable names we just computed.\n    vggish_vars = [v for v in tf.global_variables() if v.name in vggish_var_names]\n\n    # Use a Saver to restore just the variables selected above.\n    saver = tf.train.Saver(vggish_vars, name=\'vggish_load_pretrained\',\n                           write_version=1)\n    saver.restore(session, checkpoint_path)\n\n    tvs = [v for v in tf.trainable_variables()]\n    for v in tvs:\n        print(v.name)'"
gnes/encoder/image/cvae_cores/__init__.py,0,b''
gnes/encoder/image/cvae_cores/model.py,28,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport numpy as np\nimport tensorflow as tf\n\n\nclass CVAE(tf.keras.Model):\n    def __init__(self, latent_dim):\n        super(CVAE, self).__init__()\n        self.latent_dim = latent_dim\n        self.inference_net = tf.keras.Sequential(\n            [\n                tf.keras.layers.InputLayer(input_shape=(120, 120, 3)),\n                tf.keras.layers.Conv2D(\n                    filters=32, kernel_size=3, strides=(2, 2),\n                    padding=\'SAME\',\n                    activation=\'relu\'),\n                tf.keras.layers.Conv2D(\n                    filters=32, kernel_size=3, strides=(2, 2),\n                    padding=\'SAME\',\n                    activation=\'relu\'),\n                tf.keras.layers.Conv2D(\n                    filters=32, kernel_size=3, strides=(2, 2),\n                    padding=\'SAME\',\n                    activation=\'relu\'),\n                tf.keras.layers.Flatten(),\n                # No activation\n                tf.keras.layers.Dense(latent_dim + latent_dim),\n            ]\n        )\n\n        self.generative_net = tf.keras.Sequential(\n            [\n                tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n                tf.keras.layers.Dense(units=15 * 15 * 32,\n                                      activation=tf.nn.relu),\n                tf.keras.layers.Reshape(target_shape=(15, 15, 32)),\n                tf.keras.layers.Conv2DTranspose(\n                    filters=32,\n                    kernel_size=3,\n                    strides=(2, 2),\n                    padding=""SAME"",\n                    activation=\'relu\'),\n                tf.keras.layers.Conv2DTranspose(\n                    filters=32,\n                    kernel_size=3,\n                    strides=(2, 2),\n                    padding=""SAME"",\n                    activation=\'relu\'),\n                tf.keras.layers.Conv2DTranspose(\n                    filters=32,\n                    kernel_size=3,\n                    strides=(2, 2),\n                    padding=""SAME"",\n                    activation=\'relu\'),\n                # No activation\n                tf.keras.layers.Conv2DTranspose(\n                    filters=3, kernel_size=3, strides=(1, 1), padding=""SAME""),\n            ]\n        )\n\n    def sample(self, eps=None):\n        if eps is None:\n            eps = tf.random_normal(shape=(100, self.latent_dim))\n        return self.decode(eps, apply_sigmoid=True)\n\n    def encode(self, x):\n        mean, logvar = tf.split(self.inference_net(x), num_or_size_splits=2, axis=1)\n        return mean, logvar\n\n    @staticmethod\n    def reparameterize(mean, logvar):\n        eps = tf.random_normal(shape=tf.shape(mean))\n        return eps * tf.exp(logvar * .5) + mean\n\n    def decode(self, z, apply_sigmoid=False):\n        logits = self.generative_net(z)\n        if apply_sigmoid:\n            probs = tf.sigmoid(logits)\n            return probs\n\n        return logits\n\n    def compute_loss(self, x):\n        mean, logvar = self.encode(x)\n        z = CVAE.reparameterize(mean, logvar)\n        x_logit = self.decode(z)\n\n        cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit,\n                                                            labels=x)\n        logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n        logpz = CVAE.log_normal_pdf(z, 0., 0.)\n        logqz_x = CVAE.log_normal_pdf(z, mean, logvar)\n\n        return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n\n    @staticmethod\n    def log_normal_pdf(sample, mean, logvar, raxis=1):\n        log2pi = tf.math.log(2. * np.pi)\n        return tf.reduce_sum(\n            -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n            axis=raxis)\n'"
gnes/encoder/image/inception_cores/__init__.py,0,b''
gnes/encoder/image/inception_cores/inception_utils.py,3,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains common code shared by all inception models.\n\nUsage of arg scope:\n  with slim.arg_scope(inception_arg_scope()):\n    logits, end_points = inception.inception_v3(images, num_classes,\n                                                is_training=is_training)\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef inception_arg_scope(weight_decay=0.00004,\n                        use_batch_norm=True,\n                        batch_norm_decay=0.9997,\n                        batch_norm_epsilon=0.001,\n                        activation_fn=tf.nn.relu,\n                        batch_norm_updates_collections=tf.GraphKeys.UPDATE_OPS,\n                        batch_norm_scale=False):\n    """"""Defines the default arg scope for inception models.\n\n    Args:\n      weight_decay: The weight decay to use for regularizing the model.\n      use_batch_norm: ""If `True`, batch_norm is applied after each convolution.\n      batch_norm_decay: Decay for batch norm moving average.\n      batch_norm_epsilon: Small float added to variance to avoid dividing by zero\n        in batch norm.\n      activation_fn: Activation function for conv2d.\n      batch_norm_updates_collections: Collection for the update ops for\n        batch norm.\n      batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the\n        activations in the batch normalization layer.\n\n    Returns:\n      An `arg_scope` to use for the inception models.\n    """"""\n    batch_norm_params = {\n        # Decay for the moving averages.\n        \'decay\': batch_norm_decay,\n        # epsilon to prevent 0s in variance.\n        \'epsilon\': batch_norm_epsilon,\n        # collection containing update_ops.\n        \'updates_collections\': batch_norm_updates_collections,\n        # use fused batch norm if possible.\n        \'fused\': None,\n        \'scale\': batch_norm_scale,\n    }\n    if use_batch_norm:\n        normalizer_fn = slim.batch_norm\n        normalizer_params = batch_norm_params\n    else:\n        normalizer_fn = None\n        normalizer_params = {}\n    # Set weight_decay for weights in Conv and FC layers.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                        weights_regularizer=slim.l2_regularizer(weight_decay)):\n        with slim.arg_scope(\n                [slim.conv2d],\n                weights_initializer=slim.variance_scaling_initializer(),\n                activation_fn=activation_fn,\n                normalizer_fn=normalizer_fn,\n                normalizer_params=normalizer_params) as sc:\n            return sc\n'"
gnes/encoder/image/inception_cores/inception_v4.py,49,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition of the Inception V4 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom . import inception_utils\n\nslim = tf.contrib.slim\n\n\ndef block_inception_a(inputs, scope=None, reuse=None):\n    """"""Builds Inception-A block for Inception v4 network.""""""\n    # By default use stride=1 and SAME padding\n    with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                        stride=1, padding=\'SAME\'):\n        with tf.variable_scope(scope, \'BlockInceptionA\', [inputs], reuse=reuse):\n            with tf.variable_scope(\'Branch_0\'):\n                branch_0 = slim.conv2d(inputs, 96, [1, 1], scope=\'Conv2d_0a_1x1\')\n            with tf.variable_scope(\'Branch_1\'):\n                branch_1 = slim.conv2d(inputs, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n                branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n            with tf.variable_scope(\'Branch_2\'):\n                branch_2 = slim.conv2d(inputs, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n                branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0b_3x3\')\n                branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope=\'Conv2d_0c_3x3\')\n            with tf.variable_scope(\'Branch_3\'):\n                branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n                branch_3 = slim.conv2d(branch_3, 96, [1, 1], scope=\'Conv2d_0b_1x1\')\n            return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef block_reduction_a(inputs, scope=None, reuse=None):\n    """"""Builds Reduction-A block for Inception v4 network.""""""\n    # By default use stride=1 and SAME padding\n    with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                        stride=1, padding=\'SAME\'):\n        with tf.variable_scope(scope, \'BlockReductionA\', [inputs], reuse=reuse):\n            with tf.variable_scope(\'Branch_0\'):\n                branch_0 = slim.conv2d(inputs, 384, [3, 3], stride=2, padding=\'VALID\',\n                                       scope=\'Conv2d_1a_3x3\')\n            with tf.variable_scope(\'Branch_1\'):\n                branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n                branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope=\'Conv2d_0b_3x3\')\n                branch_1 = slim.conv2d(branch_1, 256, [3, 3], stride=2,\n                                       padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n            with tf.variable_scope(\'Branch_2\'):\n                branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding=\'VALID\',\n                                           scope=\'MaxPool_1a_3x3\')\n            return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n\n\ndef block_inception_b(inputs, scope=None, reuse=None):\n    """"""Builds Inception-B block for Inception v4 network.""""""\n    # By default use stride=1 and SAME padding\n    with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                        stride=1, padding=\'SAME\'):\n        with tf.variable_scope(scope, \'BlockInceptionB\', [inputs], reuse=reuse):\n            with tf.variable_scope(\'Branch_0\'):\n                branch_0 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n            with tf.variable_scope(\'Branch_1\'):\n                branch_1 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n                branch_1 = slim.conv2d(branch_1, 224, [1, 7], scope=\'Conv2d_0b_1x7\')\n                branch_1 = slim.conv2d(branch_1, 256, [7, 1], scope=\'Conv2d_0c_7x1\')\n            with tf.variable_scope(\'Branch_2\'):\n                branch_2 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n                branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope=\'Conv2d_0b_7x1\')\n                branch_2 = slim.conv2d(branch_2, 224, [1, 7], scope=\'Conv2d_0c_1x7\')\n                branch_2 = slim.conv2d(branch_2, 224, [7, 1], scope=\'Conv2d_0d_7x1\')\n                branch_2 = slim.conv2d(branch_2, 256, [1, 7], scope=\'Conv2d_0e_1x7\')\n            with tf.variable_scope(\'Branch_3\'):\n                branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n                branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope=\'Conv2d_0b_1x1\')\n            return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef block_reduction_b(inputs, scope=None, reuse=None):\n    """"""Builds Reduction-B block for Inception v4 network.""""""\n    # By default use stride=1 and SAME padding\n    with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                        stride=1, padding=\'SAME\'):\n        with tf.variable_scope(scope, \'BlockReductionB\', [inputs], reuse=reuse):\n            with tf.variable_scope(\'Branch_0\'):\n                branch_0 = slim.conv2d(inputs, 192, [1, 1], scope=\'Conv2d_0a_1x1\')\n                branch_0 = slim.conv2d(branch_0, 192, [3, 3], stride=2,\n                                       padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n            with tf.variable_scope(\'Branch_1\'):\n                branch_1 = slim.conv2d(inputs, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n                branch_1 = slim.conv2d(branch_1, 256, [1, 7], scope=\'Conv2d_0b_1x7\')\n                branch_1 = slim.conv2d(branch_1, 320, [7, 1], scope=\'Conv2d_0c_7x1\')\n                branch_1 = slim.conv2d(branch_1, 320, [3, 3], stride=2,\n                                       padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n            with tf.variable_scope(\'Branch_2\'):\n                branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding=\'VALID\',\n                                           scope=\'MaxPool_1a_3x3\')\n            return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n\n\ndef block_inception_c(inputs, scope=None, reuse=None):\n    """"""Builds Inception-C block for Inception v4 network.""""""\n    # By default use stride=1 and SAME padding\n    with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                        stride=1, padding=\'SAME\'):\n        with tf.variable_scope(scope, \'BlockInceptionC\', [inputs], reuse=reuse):\n            with tf.variable_scope(\'Branch_0\'):\n                branch_0 = slim.conv2d(inputs, 256, [1, 1], scope=\'Conv2d_0a_1x1\')\n            with tf.variable_scope(\'Branch_1\'):\n                branch_1 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n                branch_1 = tf.concat(axis=3, values=[\n                    slim.conv2d(branch_1, 256, [1, 3], scope=\'Conv2d_0b_1x3\'),\n                    slim.conv2d(branch_1, 256, [3, 1], scope=\'Conv2d_0c_3x1\')])\n            with tf.variable_scope(\'Branch_2\'):\n                branch_2 = slim.conv2d(inputs, 384, [1, 1], scope=\'Conv2d_0a_1x1\')\n                branch_2 = slim.conv2d(branch_2, 448, [3, 1], scope=\'Conv2d_0b_3x1\')\n                branch_2 = slim.conv2d(branch_2, 512, [1, 3], scope=\'Conv2d_0c_1x3\')\n                branch_2 = tf.concat(axis=3, values=[\n                    slim.conv2d(branch_2, 256, [1, 3], scope=\'Conv2d_0d_1x3\'),\n                    slim.conv2d(branch_2, 256, [3, 1], scope=\'Conv2d_0e_3x1\')])\n            with tf.variable_scope(\'Branch_3\'):\n                branch_3 = slim.avg_pool2d(inputs, [3, 3], scope=\'AvgPool_0a_3x3\')\n                branch_3 = slim.conv2d(branch_3, 256, [1, 1], scope=\'Conv2d_0b_1x1\')\n            return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef inception_v4_base(inputs, final_endpoint=\'Mixed_7d\', scope=None):\n    """"""Creates the Inception V4 network up to the given final endpoint.\n\n    Args:\n      inputs: a 4-D tensor of size [batch_size, height, width, 3].\n      final_endpoint: specifies the endpoint to construct the network up to.\n        It can be one of [ \'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n        \'Mixed_3a\', \'Mixed_4a\', \'Mixed_5a\', \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\',\n        \'Mixed_5e\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\', \'Mixed_6d\', \'Mixed_6e\',\n        \'Mixed_6f\', \'Mixed_6g\', \'Mixed_6h\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\',\n        \'Mixed_7d\']\n      scope: Optional variable_scope.\n\n    Returns:\n      logits: the logits outputs of the model.\n      end_points: the set of end_points from the inception model.\n\n    Raises:\n      ValueError: if final_endpoint is not set to one of the predefined values,\n    """"""\n    end_points = {}\n\n    def add_and_check_final(name, net):\n        end_points[name] = net\n        return name == final_endpoint\n\n    with tf.variable_scope(scope, \'InceptionV4\', [inputs]):\n        with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                            stride=1, padding=\'SAME\'):\n            # 299 x 299 x 3\n            net = slim.conv2d(inputs, 32, [3, 3], stride=2,\n                              padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n            if add_and_check_final(\'Conv2d_1a_3x3\', net): return net, end_points\n            # 149 x 149 x 32\n            net = slim.conv2d(net, 32, [3, 3], padding=\'VALID\',\n                              scope=\'Conv2d_2a_3x3\')\n            if add_and_check_final(\'Conv2d_2a_3x3\', net): return net, end_points\n            # 147 x 147 x 32\n            net = slim.conv2d(net, 64, [3, 3], scope=\'Conv2d_2b_3x3\')\n            if add_and_check_final(\'Conv2d_2b_3x3\', net): return net, end_points\n            # 147 x 147 x 64\n            with tf.variable_scope(\'Mixed_3a\'):\n                with tf.variable_scope(\'Branch_0\'):\n                    branch_0 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                               scope=\'MaxPool_0a_3x3\')\n                with tf.variable_scope(\'Branch_1\'):\n                    branch_1 = slim.conv2d(net, 96, [3, 3], stride=2, padding=\'VALID\',\n                                           scope=\'Conv2d_0a_3x3\')\n                net = tf.concat(axis=3, values=[branch_0, branch_1])\n                if add_and_check_final(\'Mixed_3a\', net): return net, end_points\n\n            # 73 x 73 x 160\n            with tf.variable_scope(\'Mixed_4a\'):\n                with tf.variable_scope(\'Branch_0\'):\n                    branch_0 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n                    branch_0 = slim.conv2d(branch_0, 96, [3, 3], padding=\'VALID\',\n                                           scope=\'Conv2d_1a_3x3\')\n                with tf.variable_scope(\'Branch_1\'):\n                    branch_1 = slim.conv2d(net, 64, [1, 1], scope=\'Conv2d_0a_1x1\')\n                    branch_1 = slim.conv2d(branch_1, 64, [1, 7], scope=\'Conv2d_0b_1x7\')\n                    branch_1 = slim.conv2d(branch_1, 64, [7, 1], scope=\'Conv2d_0c_7x1\')\n                    branch_1 = slim.conv2d(branch_1, 96, [3, 3], padding=\'VALID\',\n                                           scope=\'Conv2d_1a_3x3\')\n                net = tf.concat(axis=3, values=[branch_0, branch_1])\n                if add_and_check_final(\'Mixed_4a\', net): return net, end_points\n\n            # 71 x 71 x 192\n            with tf.variable_scope(\'Mixed_5a\'):\n                with tf.variable_scope(\'Branch_0\'):\n                    branch_0 = slim.conv2d(net, 192, [3, 3], stride=2, padding=\'VALID\',\n                                           scope=\'Conv2d_1a_3x3\')\n                with tf.variable_scope(\'Branch_1\'):\n                    branch_1 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                               scope=\'MaxPool_1a_3x3\')\n                net = tf.concat(axis=3, values=[branch_0, branch_1])\n                if add_and_check_final(\'Mixed_5a\', net): return net, end_points\n\n            # 35 x 35 x 384\n            # 4 x Inception-A blocks\n            for idx in range(4):\n                block_scope = \'Mixed_5\' + chr(ord(\'b\') + idx)\n                net = block_inception_a(net, block_scope)\n                if add_and_check_final(block_scope, net): return net, end_points\n\n            # 35 x 35 x 384\n            # Reduction-A block\n            net = block_reduction_a(net, \'Mixed_6a\')\n            if add_and_check_final(\'Mixed_6a\', net): return net, end_points\n\n            # 17 x 17 x 1024\n            # 7 x Inception-B blocks\n            for idx in range(7):\n                block_scope = \'Mixed_6\' + chr(ord(\'b\') + idx)\n                net = block_inception_b(net, block_scope)\n                if add_and_check_final(block_scope, net): return net, end_points\n\n            # 17 x 17 x 1024\n            # Reduction-B block\n            net = block_reduction_b(net, \'Mixed_7a\')\n            if add_and_check_final(\'Mixed_7a\', net): return net, end_points\n\n            # 8 x 8 x 1536\n            # 3 x Inception-C blocks\n            for idx in range(3):\n                block_scope = \'Mixed_7\' + chr(ord(\'b\') + idx)\n                net = block_inception_c(net, block_scope)\n                if add_and_check_final(block_scope, net): return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v4(inputs, num_classes=1001, is_training=True,\n                 dropout_keep_prob=0.8,\n                 reuse=None,\n                 scope=\'InceptionV4\',\n                 create_aux_logits=True):\n    """"""Creates the Inception V4 model.\n\n    Args:\n      inputs: a 4-D tensor of size [batch_size, height, width, 3].\n      num_classes: number of predicted classes. If 0 or None, the logits layer\n        is omitted and the input features to the logits layer (before dropout)\n        are returned instead.\n      is_training: whether is training or not.\n      dropout_keep_prob: float, the fraction to keep before final layer.\n      reuse: whether or not the network and its variables should be reused. To be\n        able to reuse \'scope\' must be given.\n      scope: Optional variable_scope.\n      create_aux_logits: Whether to include the auxiliary logits.\n\n    Returns:\n      net: a Tensor with the logits (pre-softmax activations) if num_classes\n        is a non-zero integer, or the non-dropped input to the logits layer\n        if num_classes is 0 or None.\n      end_points: the set of end_points from the inception model.\n    """"""\n    end_points = {}\n    with tf.variable_scope(scope, \'InceptionV4\', [inputs], reuse=reuse) as scope:\n        with slim.arg_scope([slim.batch_norm, slim.dropout],\n                            is_training=is_training):\n            net, end_points = inception_v4_base(inputs, scope=scope)\n\n            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                                stride=1, padding=\'SAME\'):\n                # Auxiliary Head logits\n                if create_aux_logits and num_classes:\n                    with tf.variable_scope(\'AuxLogits\'):\n                        # 17 x 17 x 1024\n                        aux_logits = end_points[\'Mixed_6h\']\n                        aux_logits = slim.avg_pool2d(aux_logits, [5, 5], stride=3,\n                                                     padding=\'VALID\',\n                                                     scope=\'AvgPool_1a_5x5\')\n                        aux_logits = slim.conv2d(aux_logits, 128, [1, 1],\n                                                 scope=\'Conv2d_1b_1x1\')\n                        aux_logits = slim.conv2d(aux_logits, 768,\n                                                 aux_logits.get_shape()[1:3],\n                                                 padding=\'VALID\', scope=\'Conv2d_2a\')\n                        aux_logits = slim.flatten(aux_logits)\n                        aux_logits = slim.fully_connected(aux_logits, num_classes,\n                                                          activation_fn=None,\n                                                          scope=\'Aux_logits\')\n                        end_points[\'AuxLogits\'] = aux_logits\n\n                # Final pooling and prediction\n                # TODO(sguada,arnoegw): Consider adding a parameter global_pool which\n                # can be set to False to disable pooling here (as in resnet_*()).\n                with tf.variable_scope(\'Logits\'):\n                    # 8 x 8 x 1536\n                    kernel_size = net.get_shape()[1:3]\n                    if kernel_size.is_fully_defined():\n                        net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                                              scope=\'AvgPool_1a\')\n                    else:\n                        net = tf.reduce_mean(net, [1, 2], keep_dims=True,\n                                             name=\'global_pool\')\n                    end_points[\'global_pool\'] = net\n                    if not num_classes:\n                        return net, end_points\n                    # 1 x 1 x 1536\n                    net = slim.dropout(net, dropout_keep_prob, scope=\'Dropout_1b\')\n                    net = slim.flatten(net, scope=\'PreLogitsFlatten\')\n                    end_points[\'PreLogitsFlatten\'] = net\n                    # 1536\n                    logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                                  scope=\'Logits\')\n                    end_points[\'Logits\'] = logits\n                    end_points[\'Predictions\'] = tf.nn.softmax(logits, name=\'Predictions\')\n        return logits, end_points\n\n\ninception_v4.default_image_size = 299\n\ninception_v4_arg_scope = inception_utils.inception_arg_scope\n'"
gnes/encoder/video/mixture_core/__init__.py,0,b''
gnes/encoder/video/mixture_core/model.py,99,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport math\n\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\n\nclass NetFV:\n    def __init__(self, feature_size,\n                 cluster_size,\n                 vocab_size,\n                 method=\'netvlad\',\n                 use_length=False,\n                 input_size=None,\n                 use_2nd_label=False,\n                 vocab_size_2=None,\n                 add_batch_norm=True,\n                 is_training=False,\n                 use_weights=True,\n                 save_dir=None,\n                 multitask_method=None,\n                 l2_penalty=1e-6):\n        if input_size == None:\n            self.input_size = feature_size\n        else:\n            self.input_size = input_size\n        self.feature_size = feature_size\n        self.use_length = use_length\n        self.is_training = is_training\n        self.vocab_size = vocab_size\n        self.use_2nd_label = use_2nd_label\n        self.vocab_size_2 = vocab_size_2\n        self.add_batch_norm = add_batch_norm\n        self.cluster_size = cluster_size\n        self.use_weights = use_weights\n        self.l2_penalty = l2_penalty\n        self.method = method\n        self.multitask_method = multitask_method\n        self.build_model()\n        self.build_loss()\n\n    @staticmethod\n    def rand_init(feature_size):\n        return tf.random_normal_initializer(stddev=1 / math.sqrt(feature_size))\n\n    def build_model(self):\n        self.feeds = tf.placeholder(tf.float32, [None, None, self.input_size])\n        self.feeds_length = tf.placeholder(tf.int32, [None])\n\n        self.inputs = tf.layers.dense(self.feeds, self.feature_size)\n\n        self.weights = tf.placeholder(tf.float32, [None, self.vocab_size])\n        self.max_frames = tf.shape(self.inputs)[1]\n        self.seq_length = tf.cast(tf.sequence_mask(self.feeds_length,\n                                                   self.max_frames), tf.float32)\n        if self.method == \'fvnet\':\n            self.build_fvnet()\n        elif self.method == \'netvlad\':\n            self.build_netvlad()\n        elif self.method == \'pooling\':\n            self.build_pooling()\n\n    def build_pooling(self):\n        self.repre = tf.layers.dense(self.inputs, self.feature_size)\n        self.repre = tf.reduce_max(self.repre, axis=1)\n\n    def build_fvnet(self):\n        reshaped_input = tf.reshape(self.inputs, [-1, self.feature_size])\n        cluster_weights = tf.get_variable(""cluster_weights"",\n                                          [self.feature_size, self.cluster_size],\n                                          initializer=NetFV.rand_init(self.feature_size))\n\n        covar_weights = tf.get_variable(""covar_weights"",\n                                        [self.feature_size, self.cluster_size],\n                                        initializer=NetFV.rand_init(self.feature_size))\n\n        covar_weights = tf.square(covar_weights)\n        eps = tf.constant([1e-6])\n        covar_weights = tf.add(covar_weights, eps)\n\n        tf.summary.histogram(""cluster_weights"", cluster_weights)\n        activation = tf.matmul(reshaped_input, cluster_weights)\n        if self.add_batch_norm:\n            activation = slim.batch_norm(activation,\n                                         center=True,\n                                         scale=True,\n                                         is_training=self.is_training,\n                                         scope=""cluster_bn"")\n        else:\n            cluster_biases = tf.get_variable(""cluster_biases"",\n                                             [self.cluster_size],\n                                             initializer=NetFV.rand_init(self.feature_size))\n            tf.summary.histogram(""cluster_biases"", cluster_biases)\n            activation += cluster_biases\n\n        activation = tf.nn.softmax(activation)\n        tf.summary.histogram(""cluster_output"", activation)\n\n        activation = tf.reshape(activation, [-1, self.max_frames, self.cluster_size])\n\n        a_sum = tf.reduce_sum(activation, -2, keepdims=True)\n\n        cluster_weights2 = tf.scalar_mul(0.01, cluster_weights)\n\n        a = tf.multiply(a_sum, cluster_weights2)\n\n        activation = tf.transpose(activation, perm=[0, 2, 1])\n\n        reshaped_input = tf.reshape(reshaped_input,\n                                    [-1, self.max_frames, self.feature_size])\n        fv1 = tf.matmul(activation, reshaped_input)\n\n        fv1 = tf.transpose(fv1, perm=[0, 2, 1])\n\n        # computing second order FV\n        a2 = tf.multiply(a_sum, tf.square(cluster_weights2))\n\n        b2 = tf.multiply(fv1, cluster_weights2)\n        fv2 = tf.matmul(activation, tf.square(reshaped_input))\n\n        fv2 = tf.transpose(fv2, perm=[0, 2, 1])\n        fv2 = tf.add_n([a2, fv2, tf.scalar_mul(-2, b2)])\n\n        fv2 = tf.divide(fv2, tf.square(covar_weights))\n        fv2 = tf.subtract(fv2, a_sum)\n\n        fv2 = tf.reshape(fv2, [-1, self.cluster_size * self.feature_size])\n        fv2 = tf.nn.l2_normalize(fv2, 1)\n        fv2 = tf.reshape(fv2, [-1, self.cluster_size * self.feature_size])\n        fv2 = tf.nn.l2_normalize(fv2, 1)\n\n        fv1 = tf.subtract(fv1, a)\n        fv1 = tf.divide(fv1, covar_weights)\n        fv1 = tf.nn.l2_normalize(fv1, 1)\n        fv1 = tf.reshape(fv1, [-1, self.cluster_size * self.feature_size])\n        fv1 = tf.nn.l2_normalize(fv1, 1)\n\n        self.repre = tf.concat([fv1, fv2], 1)\n        self.repre = tf.layers.dense(self.repre, self.feature_size)\n\n    def build_netvlad(self):\n        reshaped_input = tf.reshape(self.inputs, [-1, self.feature_size])\n        cluster_weights = tf.get_variable(""cluster_weights"",\n                                          [self.feature_size, self.cluster_size],\n                                          initializer=NetFV.rand_init(self.feature_size))\n        activation = tf.matmul(reshaped_input, cluster_weights)\n        if self.add_batch_norm:\n            activation = slim.batch_norm(activation,\n                                         center=True,\n                                         scale=True,\n                                         is_training=self.is_training,\n                                         scope=""cluster_bn"")\n        else:\n            cluster_biases = tf.get_variable(""cluster_biases"",\n                                             [self.cluster_size],\n                                             initializer=NetFV.rand_init(self.feature_size))\n            activation += cluster_biases\n        activation = tf.nn.softmax(activation)\n        activation = tf.reshape(activation, [-1, self.max_frames, self.cluster_size])\n        if self.use_length:\n            activation *= tf.reshape(self.seq_length, [-1, self.max_frames, 1])\n\n        a_sum = tf.reduce_sum(activation, -2, keep_dims=True)\n\n        if self.use_length:\n            a_sum = a_sum / tf.cast(tf.reshape(self.feeds_length, [-1, 1, 1]), tf.float32)\n\n        cluster_weights2 = tf.get_variable(""cluster_weights2"",\n                                           [1, self.feature_size, self.cluster_size],\n                                           initializer=NetFV.rand_init(self.feature_size))\n\n        a = tf.multiply(a_sum, cluster_weights2)\n        activation = tf.transpose(activation, perm=[0, 2, 1])\n\n        reshaped_input = tf.reshape(reshaped_input,\n                                    [-1, self.max_frames, self.feature_size])\n        vlad = tf.matmul(activation, reshaped_input)\n        if self.use_length:\n            vlad = vlad / tf.cast(tf.reshape(self.feeds_length, [-1, 1, 1]), tf.float32)\n        vlad = tf.transpose(vlad, perm=[0, 2, 1])\n        vlad = tf.subtract(vlad, a)\n\n        vlad = tf.nn.l2_normalize(vlad, 1)\n\n        vlad = tf.reshape(vlad, [-1, self.cluster_size * self.feature_size])\n        vlad = tf.nn.l2_normalize(vlad, 1)\n        self.repre = vlad\n\n    def build_loss(self):\n        self.probabilities = tf.layers.dense(self.repre,\n                                             self.vocab_size,\n                                             activation=tf.nn.tanh)\n        self.probabilities = tf.layers.dense(self.probabilities, self.vocab_size)\n        self.probabilities = tf.nn.softmax(self.probabilities)\n\n        self.label = tf.placeholder(tf.int32, [None, self.vocab_size])\n        logits = tf.cast(self.label, tf.float32)\n        if self.use_weights:\n            logits = logits * self.weights\n        self.loss = - tf.log(tf.reduce_sum(logits * self.probabilities, axis=1) + 1e-9)\n        self.loss = tf.reduce_mean(self.loss)\n        self.pred = tf.argmax(self.probabilities, 1)\n        self.avg_diff = tf.cast(tf.equal(tf.argmax(self.label, 1), self.pred), tf.float32)\n        self.avg_diff = tf.reduce_mean(self.avg_diff)\n\n        # add 2nd layer labels\n        if self.use_2nd_label:\n            self.label_2 = tf.placeholder(tf.int32, [None, self.vocab_size_2])\n            logits2 = tf.cast(self.label_2, tf.float32)\n\n            if self.multitask_method is None:\n                self.probabilities2 = tf.layers.dense(self.repre,\n                                                      self.vocab_size_2,\n                                                      activation=tf.nn.tanh)\n                self.probabilities2 = tf.layers.dense(self.probabilities2, self.vocab_size_2)\n                self.probabilities2 = tf.nn.softmax(self.probabilities2)\n\n            elif self.multitask_method == \'Attention\':\n                self.x = tf.get_variable(\'emb\',\n                                         shape=[self.vocab_size, self.feature_size],\n                                         dtype=tf.float32,\n                                         initializer=NetFV.rand_init(self.feature_size))\n                self.emb_label = tf.matmul(self.probabilities, self.x)\n                self.emb_concat = tf.concat([self.emb_label, self.repre], axis=1)\n                self.probabilities2 = tf.layers.dense(self.emb_concat,\n                                                      self.vocab_size_2,\n                                                      activation=tf.nn.tanh)\n                self.probabilities2 = tf.layers.dense(self.probabilities2,\n                                                      self.vocab_size_2)\n                self.probabilities2 = tf.nn.softmax(self.probabilities2)\n\n            self.loss += tf.reduce_mean(-tf.log(\n                tf.reduce_sum(logits2 * self.probabilities2, axis=1) + 1e-9))\n            self.pred2 = tf.argmax(self.probabilities2, 1)\n            self.avg_diff2 = tf.cast(tf.equal(tf.argmax(self.label_2, 1), self.pred2), tf.float32)\n            self.avg_diff2 = tf.reduce_mean(self.avg_diff2)\n\n        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.0005,\n                                                epsilon=1e-08,\n                                                name=\'adam\')\n        self.train_op = slim.learning.create_train_op(self.loss, self.optimizer)\n        self.eval_res = {\'loss\': self.loss, \'avg_diff\': self.avg_diff}\n        if self.use_2nd_label:\n            self.eval_res[\'avg_diff2\'] = self.avg_diff2\n'"
gnes/encoder/video/yt8m_feature_extractor_cores/__init__.py,0,b''
gnes/encoder/video/yt8m_feature_extractor_cores/inception_utils.py,3,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains common code shared by all inception models.\n\nUsage of arg scope:\n  with slim.arg_scope(inception_arg_scope()):\n    logits, end_points = inception.inception_v3(images, num_classes,\n                                                is_training=is_training)\n\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nslim = tf.contrib.slim\n\n\ndef inception_arg_scope(weight_decay=0.00004,\n                        use_batch_norm=True,\n                        batch_norm_decay=0.9997,\n                        batch_norm_epsilon=0.001,\n                        activation_fn=tf.nn.relu,\n                        batch_norm_updates_collections=tf.GraphKeys.UPDATE_OPS,\n                        batch_norm_scale=False):\n    """"""Defines the default arg scope for inception models.\n\n    Args:\n      weight_decay: The weight decay to use for regularizing the model.\n      use_batch_norm: ""If `True`, batch_norm is applied after each convolution.\n      batch_norm_decay: Decay for batch norm moving average.\n      batch_norm_epsilon: Small float added to variance to avoid dividing by zero\n        in batch norm.\n      activation_fn: Activation function for conv2d.\n      batch_norm_updates_collections: Collection for the update ops for\n        batch norm.\n      batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the\n        activations in the batch normalization layer.\n\n    Returns:\n      An `arg_scope` to use for the inception models.\n    """"""\n    batch_norm_params = {\n        # Decay for the moving averages.\n        \'decay\': batch_norm_decay,\n        # epsilon to prevent 0s in variance.\n        \'epsilon\': batch_norm_epsilon,\n        # collection containing update_ops.\n        \'updates_collections\': batch_norm_updates_collections,\n        # use fused batch norm if possible.\n        \'fused\': None,\n        \'scale\': batch_norm_scale,\n    }\n    if use_batch_norm:\n        normalizer_fn = slim.batch_norm\n        normalizer_params = batch_norm_params\n    else:\n        normalizer_fn = None\n        normalizer_params = {}\n    # Set weight_decay for weights in Conv and FC layers.\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                        weights_regularizer=slim.l2_regularizer(weight_decay)):\n        with slim.arg_scope(\n                [slim.conv2d],\n                weights_initializer=slim.variance_scaling_initializer(),\n                activation_fn=activation_fn,\n                normalizer_fn=normalizer_fn,\n                normalizer_params=normalizer_params) as sc:\n            return sc\n'"
gnes/encoder/video/yt8m_feature_extractor_cores/inception_v3.py,80,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Contains the definition for inception v3 classification network.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom . import inception_utils\n\nslim = tf.contrib.slim\ntrunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n\n\ndef inception_v3_base(inputs,\n                      final_endpoint=\'Mixed_7c\',\n                      min_depth=16,\n                      depth_multiplier=1.0,\n                      scope=None):\n  """"""Inception model from http://arxiv.org/abs/1512.00567.\n  Constructs an Inception v3 network from inputs to the given final endpoint.\n  This method can construct the network up to the final inception block\n  Mixed_7c.\n  Note that the names of the layers in the paper do not correspond to the names\n  of the endpoints registered by this function although they build the same\n  network.\n  Here is a mapping from the old_names to the new names:\n  Old name          | New name\n  =======================================\n  conv0             | Conv2d_1a_3x3\n  conv1             | Conv2d_2a_3x3\n  conv2             | Conv2d_2b_3x3\n  pool1             | MaxPool_3a_3x3\n  conv3             | Conv2d_3b_1x1\n  conv4             | Conv2d_4a_3x3\n  pool2             | MaxPool_5a_3x3\n  mixed_35x35x256a  | Mixed_5b\n  mixed_35x35x288a  | Mixed_5c\n  mixed_35x35x288b  | Mixed_5d\n  mixed_17x17x768a  | Mixed_6a\n  mixed_17x17x768b  | Mixed_6b\n  mixed_17x17x768c  | Mixed_6c\n  mixed_17x17x768d  | Mixed_6d\n  mixed_17x17x768e  | Mixed_6e\n  mixed_8x8x1280a   | Mixed_7a\n  mixed_8x8x2048a   | Mixed_7b\n  mixed_8x8x2048b   | Mixed_7c\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    final_endpoint: specifies the endpoint to construct the network up to. It\n      can be one of [\'Conv2d_1a_3x3\', \'Conv2d_2a_3x3\', \'Conv2d_2b_3x3\',\n      \'MaxPool_3a_3x3\', \'Conv2d_3b_1x1\', \'Conv2d_4a_3x3\', \'MaxPool_5a_3x3\',\n      \'Mixed_5b\', \'Mixed_5c\', \'Mixed_5d\', \'Mixed_6a\', \'Mixed_6b\', \'Mixed_6c\',\n      \'Mixed_6d\', \'Mixed_6e\', \'Mixed_7a\', \'Mixed_7b\', \'Mixed_7c\'].\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    scope: Optional variable_scope.\n  Returns:\n    tensor_out: output tensor corresponding to the final_endpoint.\n    end_points: a set of activations for external use, for example summaries or\n                losses.\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n                or depth_multiplier <= 0\n  """"""\n  # end_points will collect relevant activations for external use, for example\n  # summaries or losses.\n  end_points = {}\n\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV3\', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'VALID\'):\n      # 299 x 299 x 3\n      end_point = \'Conv2d_1a_3x3\'\n      net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 149 x 149 x 32\n      end_point = \'Conv2d_2a_3x3\'\n      net = slim.conv2d(net, depth(32), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 147 x 147 x 32\n      end_point = \'Conv2d_2b_3x3\'\n      net = slim.conv2d(net, depth(64), [3, 3], padding=\'SAME\', scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 147 x 147 x 64\n      end_point = \'MaxPool_3a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 73 x 73 x 64\n      end_point = \'Conv2d_3b_1x1\'\n      net = slim.conv2d(net, depth(80), [1, 1], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 73 x 73 x 80.\n      end_point = \'Conv2d_4a_3x3\'\n      net = slim.conv2d(net, depth(192), [3, 3], scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 71 x 71 x 192.\n      end_point = \'MaxPool_5a_3x3\'\n      net = slim.max_pool2d(net, [3, 3], stride=2, scope=end_point)\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # 35 x 35 x 192.\n\n    # Inception blocks\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding=\'SAME\'):\n      # mixed: 35 x 35 x 256.\n      end_point = \'Mixed_5b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(32), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_1: 35 x 35 x 288.\n      end_point = \'Mixed_5c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0b_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv_1_0c_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1],\n                                 scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_2: 35 x 35 x 288.\n      end_point = \'Mixed_5d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(48), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],\n                                 scope=\'Conv2d_0b_5x5\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],\n                                 scope=\'Conv2d_0c_3x3\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_3: 17 x 17 x 768.\n      end_point = \'Mixed_6a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(384), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(64), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3],\n                                 scope=\'Conv2d_0b_3x3\')\n          branch_1 = slim.conv2d(branch_1, depth(96), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_1x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed4: 17 x 17 x 768.\n      end_point = \'Mixed_6b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(128), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(128), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(128), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(128), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_5: 17 x 17 x 768.\n      end_point = \'Mixed_6c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # mixed_6: 17 x 17 x 768.\n      end_point = \'Mixed_6d\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(160), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(160), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_7: 17 x 17 x 768.\n      end_point = \'Mixed_6e\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [7, 1],\n                                 scope=\'Conv2d_0b_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0c_1x7\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [7, 1],\n                                 scope=\'Conv2d_0d_7x1\')\n          branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],\n                                 scope=\'Conv2d_0e_1x7\')\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],\n                                 scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_8: 8 x 8 x 1280.\n      end_point = \'Mixed_7a\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_0 = slim.conv2d(branch_0, depth(320), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(192), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [1, 7],\n                                 scope=\'Conv2d_0b_1x7\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],\n                                 scope=\'Conv2d_0c_7x1\')\n          branch_1 = slim.conv2d(branch_1, depth(192), [3, 3], stride=2,\n                                 padding=\'VALID\', scope=\'Conv2d_1a_3x3\')\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding=\'VALID\',\n                                     scope=\'MaxPool_1a_3x3\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n      # mixed_9: 8 x 8 x 2048.\n      end_point = \'Mixed_7b\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(320), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(384), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_1, depth(384), [1, 3], scope=\'Conv2d_0b_1x3\'),\n              slim.conv2d(branch_1, depth(384), [3, 1], scope=\'Conv2d_0b_3x1\')])\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(448), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(\n              branch_2, depth(384), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_2, depth(384), [1, 3], scope=\'Conv2d_0c_1x3\'),\n              slim.conv2d(branch_2, depth(384), [3, 1], scope=\'Conv2d_0d_3x1\')])\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(192), [1, 1], scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n\n      # mixed_10: 8 x 8 x 2048.\n      end_point = \'Mixed_7c\'\n      with tf.variable_scope(end_point):\n        with tf.variable_scope(\'Branch_0\'):\n          branch_0 = slim.conv2d(net, depth(320), [1, 1], scope=\'Conv2d_0a_1x1\')\n        with tf.variable_scope(\'Branch_1\'):\n          branch_1 = slim.conv2d(net, depth(384), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_1 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_1, depth(384), [1, 3], scope=\'Conv2d_0b_1x3\'),\n              slim.conv2d(branch_1, depth(384), [3, 1], scope=\'Conv2d_0c_3x1\')])\n        with tf.variable_scope(\'Branch_2\'):\n          branch_2 = slim.conv2d(net, depth(448), [1, 1], scope=\'Conv2d_0a_1x1\')\n          branch_2 = slim.conv2d(\n              branch_2, depth(384), [3, 3], scope=\'Conv2d_0b_3x3\')\n          branch_2 = tf.concat(axis=3, values=[\n              slim.conv2d(branch_2, depth(384), [1, 3], scope=\'Conv2d_0c_1x3\'),\n              slim.conv2d(branch_2, depth(384), [3, 1], scope=\'Conv2d_0d_3x1\')])\n        with tf.variable_scope(\'Branch_3\'):\n          branch_3 = slim.avg_pool2d(net, [3, 3], scope=\'AvgPool_0a_3x3\')\n          branch_3 = slim.conv2d(\n              branch_3, depth(192), [1, 1], scope=\'Conv2d_0b_1x1\')\n        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n      end_points[end_point] = net\n      if end_point == final_endpoint: return net, end_points\n    raise ValueError(\'Unknown final endpoint %s\' % final_endpoint)\n\n\ndef inception_v3(inputs,\n                 num_classes=1000,\n                 is_training=True,\n                 dropout_keep_prob=0.8,\n                 min_depth=16,\n                 depth_multiplier=1.0,\n                 prediction_fn=slim.softmax,\n                 spatial_squeeze=True,\n                 reuse=None,\n                 create_aux_logits=True,\n                 scope=\'InceptionV3\',\n                 global_pool=False):\n  """"""Inception model from http://arxiv.org/abs/1512.00567.\n  ""Rethinking the Inception Architecture for Computer Vision""\n  Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens,\n  Zbigniew Wojna.\n  With the default arguments this method constructs the exact model defined in\n  the paper. However, one can experiment with variations of the inception_v3\n  network by changing arguments dropout_keep_prob, min_depth and\n  depth_multiplier.\n  The default image size used to train this network is 299x299.\n  Args:\n    inputs: a tensor of size [batch_size, height, width, channels].\n    num_classes: number of predicted classes. If 0 or None, the logits layer\n      is omitted and the input features to the logits layer (before dropout)\n      are returned instead.\n    is_training: whether is training or not.\n    dropout_keep_prob: the percentage of activation values that are retained.\n    min_depth: Minimum depth value (number of channels) for all convolution ops.\n      Enforced when depth_multiplier < 1, and not an active constraint when\n      depth_multiplier >= 1.\n    depth_multiplier: Float multiplier for the depth (number of channels)\n      for all convolution ops. The value must be greater than zero. Typical\n      usage will be to set this value in (0, 1) to reduce the number of\n      parameters or computation cost of the model.\n    prediction_fn: a function to get predictions out of logits.\n    spatial_squeeze: if True, logits is of shape [B, C], if false logits is of\n        shape [B, 1, 1, C], where B is batch_size and C is number of classes.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse \'scope\' must be given.\n    create_aux_logits: Whether to create the auxiliary logits.\n    scope: Optional variable_scope.\n    global_pool: Optional boolean flag to control the avgpooling before the\n      logits layer. If false or unset, pooling is done with a fixed window\n      that reduces default-sized inputs to 1x1, while larger inputs lead to\n      larger outputs. If true, any input size is pooled down to 1x1.\n  Returns:\n    net: a Tensor with the logits (pre-softmax activations) if num_classes\n      is a non-zero integer, or the non-dropped-out input to the logits layer\n      if num_classes is 0 or None.\n    end_points: a dictionary from components of the network to the corresponding\n      activation.\n  Raises:\n    ValueError: if \'depth_multiplier\' is less than or equal to zero.\n  """"""\n  if depth_multiplier <= 0:\n    raise ValueError(\'depth_multiplier is not greater than zero.\')\n  depth = lambda d: max(int(d * depth_multiplier), min_depth)\n\n  with tf.variable_scope(scope, \'InceptionV3\', [inputs], reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v3_base(\n          inputs, scope=scope, min_depth=min_depth,\n          depth_multiplier=depth_multiplier)\n\n      # Auxiliary Head logits\n      if create_aux_logits and num_classes:\n        with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                            stride=1, padding=\'SAME\'):\n          aux_logits = end_points[\'Mixed_6e\']\n          with tf.variable_scope(\'AuxLogits\'):\n            aux_logits = slim.avg_pool2d(\n                aux_logits, [5, 5], stride=3, padding=\'VALID\',\n                scope=\'AvgPool_1a_5x5\')\n            aux_logits = slim.conv2d(aux_logits, depth(128), [1, 1],\n                                     scope=\'Conv2d_1b_1x1\')\n\n            # Shape of feature map before the final layer.\n            kernel_size = _reduced_kernel_size_for_small_input(\n                aux_logits, [5, 5])\n            aux_logits = slim.conv2d(\n                aux_logits, depth(768), kernel_size,\n                weights_initializer=trunc_normal(0.01),\n                padding=\'VALID\', scope=\'Conv2d_2a_{}x{}\'.format(*kernel_size))\n            aux_logits = slim.conv2d(\n                aux_logits, num_classes, [1, 1], activation_fn=None,\n                normalizer_fn=None, weights_initializer=trunc_normal(0.001),\n                scope=\'Conv2d_2b_1x1\')\n            if spatial_squeeze:\n              aux_logits = tf.squeeze(aux_logits, [1, 2], name=\'SpatialSqueeze\')\n            end_points[\'AuxLogits\'] = aux_logits\n\n      # Final pooling and prediction\n      with tf.variable_scope(\'Logits\'):\n        if global_pool:\n          # Global average pooling.\n          net = tf.reduce_mean(net, [1, 2], keep_dims=True, name=\'GlobalPool\')\n          end_points[\'global_pool\'] = net\n        else:\n          # Pooling with a fixed kernel size.\n          kernel_size = _reduced_kernel_size_for_small_input(net, [8, 8])\n          net = slim.avg_pool2d(net, kernel_size, padding=\'VALID\',\n                                scope=\'AvgPool_1a_{}x{}\'.format(*kernel_size))\n          end_points[\'AvgPool_1a\'] = net\n        if not num_classes:\n          return net, end_points\n        # 1 x 1 x 2048\n        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope=\'Dropout_1b\')\n        end_points[\'PreLogits\'] = net\n        # 2048\n        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n                             normalizer_fn=None, scope=\'Conv2d_1c_1x1\')\n        if spatial_squeeze:\n          logits = tf.squeeze(logits, [1, 2], name=\'SpatialSqueeze\')\n        # 1000\n      end_points[\'Logits\'] = logits\n      end_points[\'Predictions\'] = prediction_fn(logits, scope=\'Predictions\')\n  return logits, end_points\ninception_v3.default_image_size = 299\n\n\ndef _reduced_kernel_size_for_small_input(input_tensor, kernel_size):\n  """"""Define kernel size which is automatically reduced for small input.\n  If the shape of the input images is unknown at graph construction time this\n  function assumes that the input images are is large enough.\n  Args:\n    input_tensor: input tensor of size [batch_size, height, width, channels].\n    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]\n  Returns:\n    a tensor with the kernel size.\n  TODO(jrru): Make this function work with unknown shapes. Theoretically, this\n  can be done with the code below. Problems are two-fold: (1) If the shape was\n  known, it will be lost. (2) inception.slim.ops._two_element_tuple cannot\n  handle tensors that define the kernel size.\n      shape = tf.shape(input_tensor)\n      return = tf.stack([tf.minimum(shape[1], kernel_size[0]),\n                         tf.minimum(shape[2], kernel_size[1])])\n  """"""\n  shape = input_tensor.get_shape().as_list()\n  if shape[1] is None or shape[2] is None:\n    kernel_size_out = kernel_size\n  else:\n    kernel_size_out = [min(shape[1], kernel_size[0]),\n                       min(shape[2], kernel_size[1])]\n  return kernel_size_out\n\n\ninception_v3_arg_scope = inception_utils.inception_arg_scope'"
gnes/indexer/chunk/bindexer/__init__.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nimport os\nfrom typing import List, Tuple, Any\n\nimport numpy as np\n\nfrom .cython import IndexCore\nfrom ..helper import ListKeyIndexer\nfrom ...base import BaseChunkIndexer as BCI\n\n\nclass BIndexer(BCI):\n\n    def __init__(self,\n                 num_bytes: int = None,\n                 ef: int = 20,\n                 insert_iterations: int = 200,\n                 query_iterations: int = 500,\n                 data_path: str = \'./bindexer_data\',\n                 *args,\n                 **kwargs):\n        super().__init__(*args, **kwargs)\n        self.num_bytes = num_bytes\n        self.ef = ef\n        self.insert_iterations = insert_iterations\n        self.query_iterations = query_iterations\n        self.data_path = data_path\n        self.helper_indexer = ListKeyIndexer()\n\n    def post_init(self):\n        self.bindexer = IndexCore(self.num_bytes, 4, self.ef,\n                                  self.insert_iterations,\n                                  self.query_iterations)\n        try:\n            if not os.path.exists(self.data_path):\n                raise FileNotFoundError(\'""data_path"" is not exist\')\n            if os.path.isdir(self.data_path):\n                raise IsADirectoryError(\'""data_path"" must be a file path, not a directory\')\n            self.bindexer.load(self.data_path)\n        except (FileNotFoundError, IsADirectoryError):\n            self.logger.warning(\'fail to load model from %s, will create an empty one\' % self.data_path)\n\n    @BCI.update_helper_indexer\n    def add(self, keys: List[Tuple[int, Any]], vectors: np.ndarray, weights: List[float], *args,\n            **kwargs):\n        if len(vectors) != len(keys):\n            raise ValueError(\'vectors length should be equal to doc_ids\')\n\n        if vectors.dtype != np.uint8:\n            raise ValueError(\'vectors should be ndarray of uint8\')\n\n        num_rows = len(keys)\n        keys, offsets = zip(*keys)\n        keys = np.array(keys, dtype=np.uint32).tobytes()\n        offsets = np.array(offsets, dtype=np.uint16).tobytes()\n        weights = self.float2uint_weight(weights).tobytes()\n        self.bindexer.index_trie(vectors.tobytes(), num_rows, keys, offsets, weights)\n\n    @staticmethod\n    def float2uint_weight(weights: List[float], norm: int = 2 ** 16 - 1):\n        weights = norm * np.array(weights)\n        return np.array(weights, dtype=np.uint16)\n\n    @staticmethod\n    def uint2float_weight(weight: int, norm: int = 2 ** 16 - 1):\n        return weight / norm\n\n    def query(self,\n              keys: np.ndarray,\n              top_k: int,\n              method: str = \'nsw\',\n              *args,\n              **kwargs) -> List[List[Tuple]]:\n\n        if keys.dtype != np.uint8:\n            raise ValueError(""vectors should be ndarray of uint8"")\n\n        # num_rows = int(len(keys) / self.num_bytes)\n        num_rows = keys.shape[0]\n        keys = keys.tobytes()\n\n        result = [[] for _ in range(num_rows)]\n\n        if method == \'nsw\':\n            # find the indexed items with same value\n            q_idx, doc_ids, offsets, weights = self.bindexer.find_batch_trie(\n                keys, num_rows)\n            for (i, q, o, w) in zip(doc_ids, q_idx, offsets, weights):\n                result[q].append((i, o, self.uint2float_weight(w), 0))\n\n            # search the indexed items with similar value\n            doc_ids, offsets, weights, dists, q_idx = self.bindexer.nsw_search(\n                keys, num_rows, top_k)\n            for (i, o, w, d, q) in zip(doc_ids, offsets, weights, dists, q_idx):\n                if d == 0:\n                    continue\n                result[q].append((i, o, self.uint2float_weight(w), d))\n\n            # get the top-k\n            for q in range(num_rows):\n                result[q] = result[q][:top_k]\n        elif method == \'force\':\n            doc_ids, offsets, weights, dists, q_idx = self.bindexer.force_search(\n                keys, num_rows, top_k)\n            for (i, o, w, d, q) in zip(doc_ids, offsets, weights, dists, q_idx):\n                result[q].append((i, o, self.uint2float_weight(w), d))\n        return result\n\n    def __getstate__(self):\n        self.bindexer.save(self.data_path)\n        d = super().__getstate__()\n        return d\n'"
gnes/indexer/chunk/hbindexer/__init__.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\n\nimport os\nfrom typing import List, Tuple, Any\n\nimport numpy as np\n\nfrom .cython import IndexCore\nfrom ..helper import ListKeyIndexer\nfrom ...base import BaseChunkIndexer as BCI\n\n\nclass HBIndexer(BCI):\n\n    def __init__(self,\n                 num_clusters: int = 100,\n                 num_bytes: int = 8,\n                 n_idx: int = 1,\n                 data_path: str = \'./hbindexer_data\',\n                 *args,\n                 **kwargs):\n        super().__init__(*args, **kwargs)\n        self.n_bytes = num_bytes\n        self.n_clusters = num_clusters\n        self.n_idx = n_idx\n        self.data_path = data_path\n        if self.n_idx <= 0:\n            raise ValueError(\'There should be at least 1 clustering slot\')\n        self.helper_indexer = ListKeyIndexer()\n\n    def post_init(self):\n        self.hbindexer = IndexCore(self.n_clusters, self.n_bytes, self.n_idx)\n        try:\n            if not os.path.exists(self.data_path):\n                raise FileNotFoundError(\'""data_path"" is not exist\')\n            if os.path.isdir(self.data_path):\n                raise IsADirectoryError(\'""data_path"" must be a file path, not a directory\')\n            self.hbindexer.load(self.data_path)\n        except (FileNotFoundError, IsADirectoryError):\n            self.logger.warning(\'fail to load model from %s, will create an empty one\' % self.data_path)\n\n    @BCI.update_helper_indexer\n    def add(self, keys: List[Tuple[int, Any]], vectors: np.ndarray, weights: List[float], *args, **kwargs):\n        if len(vectors) != len(keys):\n            raise ValueError(""vectors length should be equal to doc_ids"")\n\n        if vectors.dtype != np.uint32:\n            raise ValueError(""vectors should be ndarray of uint32"")\n\n        n = len(keys)\n        keys, offsets = zip(*keys)\n        keys = np.array(keys, dtype=np.uint32).tobytes()\n        offsets = np.array(offsets, dtype=np.uint16).tobytes()\n        weights = self.float2uint_weight(weights).tobytes()\n        clusters = vectors[:, :self.n_idx].tobytes()\n        vectors = vectors[:, self.n_idx:].astype(np.uint8).tobytes()\n        self.hbindexer.index_trie(vectors, clusters, keys, offsets, weights, n)\n\n    @staticmethod\n    def float2uint_weight(weights: List[float], norm: int = 2 ** 16 - 1):\n        weights = norm * np.array(weights)\n        return np.array(weights, dtype=np.uint16)\n\n    @staticmethod\n    def uint2float_weight(weight: int, norm: int = 2 ** 16 - 1):\n        return weight / norm\n\n    def query(self,\n              vectors: np.ndarray,\n              top_k: int,\n              *args,\n              **kwargs) -> List[List[Tuple]]:\n\n        if vectors.dtype != np.uint32:\n            raise ValueError(""vectors should be ndarray of uint32"")\n\n        # num_rows = int(len(keys) / self.num_bytes)\n        n = vectors.shape[0]\n        clusters = vectors[:, :self.n_idx].tobytes()\n        vectors = vectors[:, self.n_idx:].astype(np.uint8).tobytes()\n\n        result = [{} for _ in range(n)]\n\n        doc_ids, offsets, weights, dists, q_idx = self.hbindexer.query(\n            vectors, clusters, n, top_k * self.n_idx)\n        for (i, o, w, d, q) in zip(doc_ids, offsets, weights, dists, q_idx):\n            result[q][(i, o, self.uint2float_weight(w))] = d\n\n        return [list(ret.items()) for ret in result]\n\n    def __getstate__(self):\n        self.hbindexer.save(self.data_path)\n        d = super().__getstate__()\n        return d\n'"
gnes/preprocessor/audio/vggish_example_helper/__init__.py,0,b''
gnes/preprocessor/audio/vggish_example_helper/mel_features.py,0,"b'#  Tencent is pleased to support the open source community by making GNES available.\n#\n#  Copyright (C) 2019 THL A29 Limited, a Tencent company. All rights reserved.\n#  Licensed under the Apache License, Version 2.0 (the ""License"");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an ""AS IS"" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nimport numpy as np\n\n\ndef frame(data, window_length, hop_length):\n    """"""Convert array into a sequence of successive possibly overlapping frames.\n\n    An n-dimensional array of shape (num_samples, ...) is converted into an\n    (n+1)-D array of shape (num_frames, window_length, ...), where each frame\n    starts hop_length points after the preceding one.\n\n    This is accomplished using stride_tricks, so the original data is not\n    copied.  However, there is no zero-padding, so any incomplete frames at the\n    end are not included.\n\n    Args:\n      data: np.array of dimension N >= 1.\n      window_length: Number of samples in each frame.\n      hop_length: Advance (in samples) between each window.\n\n    Returns:\n      (N+1)-D np.array with as many rows as there are complete frames that can be\n      extracted.\n    """"""\n    num_samples = data.shape[0]\n    num_frames = 1 + int(np.floor((num_samples - window_length) / hop_length))\n    shape = (num_frames, window_length) + data.shape[1:]\n    strides = (data.strides[0] * hop_length,) + data.strides\n    return np.lib.stride_tricks.as_strided(data, shape=shape, strides=strides)\n\n\ndef periodic_hann(window_length):\n    """"""Calculate a ""periodic"" Hann window.\n\n    The classic Hann window is defined as a raised cosine that starts and\n    ends on zero, and where every value appears twice, except the middle\n    point for an odd-length window.  Matlab calls this a ""symmetric"" window\n    and np.hanning() returns it.  However, for Fourier analysis, this\n    actually represents just over one cycle of a period N-1 cosine, and\n    thus is not compactly expressed on a length-N Fourier basis.  Instead,\n    it\'s better to use a raised cosine that ends just before the final\n    zero value - i.e. a complete cycle of a period-N cosine.  Matlab\n    calls this a ""periodic"" window. This routine calculates it.\n\n    Args:\n      window_length: The number of points in the returned window.\n\n    Returns:\n      A 1D np.array containing the periodic hann window.\n    """"""\n    return 0.5 - (0.5 * np.cos(2 * np.pi / window_length *\n                               np.arange(window_length)))\n\n\ndef stft_magnitude(signal, fft_length,\n                   hop_length=None,\n                   window_length=None):\n    """"""Calculate the short-time Fourier transform magnitude.\n\n    Args:\n      signal: 1D np.array of the input time-domain signal.\n      fft_length: Size of the FFT to apply.\n      hop_length: Advance (in samples) between each frame passed to FFT.\n      window_length: Length of each block of samples to pass to FFT.\n\n    Returns:\n      2D np.array where each row contains the magnitudes of the fft_length/2+1\n      unique values of the FFT for the corresponding frame of input samples.\n    """"""\n    frames = frame(signal, window_length, hop_length)\n    # Apply frame window to each frame. We use a periodic Hann (cosine of period\n    # window_length) instead of the symmetric Hann of np.hanning (period\n    # window_length-1).\n    window = periodic_hann(window_length)\n    windowed_frames = frames * window\n    return np.abs(np.fft.rfft(windowed_frames, int(fft_length)))\n\n\n# Mel spectrum constants and functions.\n_MEL_BREAK_FREQUENCY_HERTZ = 700.0\n_MEL_HIGH_FREQUENCY_Q = 1127.0\n\n\ndef hertz_to_mel(frequencies_hertz):\n    """"""Convert frequencies to mel scale using HTK formula.\n\n    Args:\n      frequencies_hertz: Scalar or np.array of frequencies in hertz.\n\n    Returns:\n      Object of same size as frequencies_hertz containing corresponding values\n      on the mel scale.\n    """"""\n    return _MEL_HIGH_FREQUENCY_Q * np.log(\n        1.0 + (frequencies_hertz / _MEL_BREAK_FREQUENCY_HERTZ))\n\n\ndef spectrogram_to_mel_matrix(num_mel_bins=20,\n                              num_spectrogram_bins=129,\n                              audio_sample_rate=8000,\n                              lower_edge_hertz=125.0,\n                              upper_edge_hertz=3800.0):\n    """"""Return a matrix that can post-multiply spectrogram rows to make mel.\n\n    Returns a np.array matrix A that can be used to post-multiply a matrix S of\n    spectrogram values (STFT magnitudes) arranged as frames x bins to generate a\n    ""mel spectrogram"" M of frames x num_mel_bins.  M = S A.\n\n    The classic HTK algorithm exploits the complementarity of adjacent mel bands\n    to multiply each FFT bin by only one mel weight, then add it, with positive\n    and negative signs, to the two adjacent mel bands to which that bin\n    contributes.  Here, by expressing this operation as a matrix multiply, we go\n    from num_fft multiplies per frame (plus around 2*num_fft adds) to around\n    num_fft^2 multiplies and adds.  However, because these are all presumably\n    accomplished in a single call to np.dot(), it\'s not clear which approach is\n    faster in Python.  The matrix multiplication has the attraction of being more\n    general and flexible, and much easier to read.\n\n    Args:\n      num_mel_bins: How many bands in the resulting mel spectrum.  This is\n        the number of columns in the output matrix.\n      num_spectrogram_bins: How many bins there are in the source spectrogram\n        data, which is understood to be fft_size/2 + 1, i.e. the spectrogram\n        only contains the nonredundant FFT bins.\n      audio_sample_rate: Samples per second of the audio at the input to the\n        spectrogram. We need this to figure out the actual frequencies for\n        each spectrogram bin, which dictates how they are mapped into mel.\n      lower_edge_hertz: Lower bound on the frequencies to be included in the mel\n        spectrum.  This corresponds to the lower edge of the lowest triangular\n        band.\n      upper_edge_hertz: The desired top edge of the highest frequency band.\n\n    Returns:\n      An np.array with shape (num_spectrogram_bins, num_mel_bins).\n\n    Raises:\n      ValueError: if frequency edges are incorrectly ordered or out of range.\n    """"""\n    nyquist_hertz = audio_sample_rate / 2.\n    if lower_edge_hertz < 0.0:\n        raise ValueError(""lower_edge_hertz %.1f must be >= 0"" % lower_edge_hertz)\n    if lower_edge_hertz >= upper_edge_hertz:\n        raise ValueError(""lower_edge_hertz %.1f >= upper_edge_hertz %.1f"" %\n                         (lower_edge_hertz, upper_edge_hertz))\n    if upper_edge_hertz > nyquist_hertz:\n        raise ValueError(""upper_edge_hertz %.1f is greater than Nyquist %.1f"" %\n                         (upper_edge_hertz, nyquist_hertz))\n    spectrogram_bins_hertz = np.linspace(0.0, nyquist_hertz, num_spectrogram_bins)\n    spectrogram_bins_mel = hertz_to_mel(spectrogram_bins_hertz)\n    # The i\'th mel band (starting from i=1) has center frequency\n    # band_edges_mel[i], lower edge band_edges_mel[i-1], and higher edge\n    # band_edges_mel[i+1].  Thus, we need num_mel_bins + 2 values in\n    # the band_edges_mel arrays.\n    band_edges_mel = np.linspace(hertz_to_mel(lower_edge_hertz),\n                                 hertz_to_mel(upper_edge_hertz), num_mel_bins + 2)\n    # Matrix to post-multiply feature arrays whose rows are num_spectrogram_bins\n    # of spectrogram values.\n    mel_weights_matrix = np.empty((num_spectrogram_bins, num_mel_bins))\n    for i in range(num_mel_bins):\n        lower_edge_mel, center_mel, upper_edge_mel = band_edges_mel[i:i + 3]\n        # Calculate lower and upper slopes for every spectrogram bin.\n        # Line segments are linear in the *mel* domain, not hertz.\n        lower_slope = ((spectrogram_bins_mel - lower_edge_mel) /\n                       (center_mel - lower_edge_mel))\n        upper_slope = ((upper_edge_mel - spectrogram_bins_mel) /\n                       (upper_edge_mel - center_mel))\n        # .. then intersect them with each other and zero.\n        mel_weights_matrix[:, i] = np.maximum(0.0, np.minimum(lower_slope,\n                                                              upper_slope))\n    # HTK excludes the spectrogram DC bin; make sure it always gets a zero\n    # coefficient.\n    mel_weights_matrix[0, :] = 0.0\n    return mel_weights_matrix\n\n\ndef log_mel_spectrogram(data,\n                        audio_sample_rate=8000,\n                        log_offset=0.0,\n                        window_length_secs=0.025,\n                        hop_length_secs=0.010,\n                        **kwargs):\n    """"""Convert waveform to a log magnitude mel-frequency spectrogram.\n\n    Args:\n      data: 1D np.array of waveform data.\n      audio_sample_rate: The sampling rate of data.\n      log_offset: Add this to values when taking log to avoid -Infs.\n      window_length_secs: Duration of each window to analyze.\n      hop_length_secs: Advance between successive analysis windows.\n      **kwargs: Additional arguments to pass to spectrogram_to_mel_matrix.\n\n    Returns:\n      2D np.array of (num_frames, num_mel_bins) consisting of log mel filterbank\n      magnitudes for successive frames.\n    """"""\n    window_length_samples = int(round(audio_sample_rate * window_length_secs))\n    hop_length_samples = int(round(audio_sample_rate * hop_length_secs))\n    fft_length = 2 ** int(np.ceil(np.log(window_length_samples) / np.log(2.0)))\n    spectrogram = stft_magnitude(\n        data,\n        fft_length=fft_length,\n        hop_length=hop_length_samples,\n        window_length=window_length_samples)\n    mel_spectrogram = np.dot(spectrogram, spectrogram_to_mel_matrix(\n        num_spectrogram_bins=spectrogram.shape[1],\n        audio_sample_rate=audio_sample_rate, **kwargs))\n    return np.log(mel_spectrogram + log_offset)'"
